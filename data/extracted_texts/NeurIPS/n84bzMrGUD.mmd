# Clifford Group Equivariant Neural Networks

David Ruhe

AI4Science Lab, AMLab, API

University of Amsterdam

david.ruhe@gmail.com &Johannes Brandstetter

Microsoft Research

AI4Science

brandstetter@ml.jku.at

Patrick Forre

AI4Science Lab, AMLab

University of Amsterdam

p.d.forre@uva.nl

###### Abstract

We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing \((n)\)- and \((n)\)-equivariant models. We identify and study the _Clifford group_: a subgroup inside the Clifford algebra tailored to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, including their grade projections, constitutes an equivariant map with respect to the Clifford group, allowing us to parameterize equivariant neural network layers. An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a single core implementation, state-of-the-art performance on several distinct tasks, including a three-dimensional \(n\)-body experiment, a four-dimensional Lorentz-equivariant high-energy physics experiment, and a five-dimensional convex hull experiment.

## 1 Introduction

Incorporating _group equivariance_ to ensure symmetry constraints in neural networks has been a highly fruitful line of research . Besides translation and permutation equivariance , rotation equivariance proved to be vitally important for many graph-structured problems as encountered in, e.g., the natural sciences. Applications of such methods include modeling the dynamics of complex physical systems or motion trajectories ; studying or generating molecules, proteins, and crystals ; and point cloud analysis . Note that many of these focus on three-dimensional problems involving rotation, reflection, or translation equivariance by considering the groups \((3)\), \((3)\), \((3)\), or \((3)\).

Such equivariant neural networks can be broadly divided into three categories: approaches that scalarize geometric quantities, methods employing regular group representations, and those utilizing irreducible representations, often of \((3)\). Scalarization methods operate exclusively onscalar features or manipulate higher-order geometric quantities such as vectors via scalar multiplication . They can be limited by the fact that they do not extract all directional information. Regular representation methods construct equivariant maps through an integral over the respective group . For continuous Lie groups, however, this integral is intractable and requires coarse approximation . Methods of the third category employ the irreducible representations of \((3)\) (the Wigner-D matrices) and operate in a _steerable_ spherical harmonics basis . This basis allows a decomposition into type-\(l\) vector subspaces that transform under \(D^{l}\): the type-\(l\) matrix representation of \((3)\). Through tensor products decomposed using Clebsch-Gordan coefficients (Clebsch-Gordan tensor product), vectors (of different types) interact equivariantly. These tensor products can be parameterized using learnable weights. Key limitations of such methods include the necessity for an alternative basis, along with acquiring the Clebsch-Gordan coefficients, which, although they are known for unitary groups of any dimension , are not trivial to obtain .

We propose _Clifford Group Equivariant Neural Networks_ (CGENNs): an equivariant parameterization of neural networks based on _Clifford algebras_. Inside the algebra, we identify the _Clifford group_ and its action, termed the (adjusted) _twisted conjugation_, which has several advantageous properties. Unlike classical approaches that represent these groups on their corresponding vector spaces, we carefully extend the action to the entire Clifford algebra. There, it automatically acts as an _orthogonal automorphism_ that respects the multivector grading, enabling nontrivial subrepresentations that operate on the algebra subspaces. Furthermore, the twisted conjugation respects the Clifford algebra's multiplicative structure, i.e. the _geometric product_, allowing us to bypass the need for explicit tensor product representations. As a result, we obtain two remarkable properties. First, all polynomials in multivectors generate Clifford group equivariant maps from the Clifford algebra to itself. Additionally, _grade projections_ are equivariant, allowing for a denser parameterization of such polynomials. We then demonstrate how to construct parameterizable neural network layers using these properties.

Our method comes with several advantages. First, instead of operating on alternative basis representations such as the spherical harmonics, CGENNs (similarly to scalarization methods) directly transform data in a vector basis. Second, multivector representations allow a (geometrically meaningful) product structure while maintaining a finite dimensionality as opposed to tensor product representations. Through geometric products, we can transform vector-valued information, resulting in a more accurate and nuanced interpretation of the underlying structures compared to scalarization methods. Further, we can represent exotic geometric objects such as pseudovectors, encountered in certain physics problems, which transform in a nonstandard manner. Third, our method readily generalizes to orthogonal groups _regardless of the dimension or metric signature of the space_, thereby attaining \((n)\)- or \((n)\)-equivariance. These advantages are demonstrated on equivariance bench

Figure 1: CGENNs (represented with \(\)) are able to operate on multivectors (elements of the Clifford algebra) in an \((n)\)- or \((n)\)-equivariant way. Specifically, when an action \((w)\) of the Clifford group, representing an orthogonal transformation such as a rotation, is applied to the data, the modelâ€™s representations _corotate_. Multivectors can be decomposed into scalar, vector, bivector, trivector, and even higher-order components. These elements can represent geometric quantities such as (oriented) areas or volumes. The action \((w)\) is designed to respect these structures when acting on them.

marks of different dimensionality. Note that specialized tools were developed for several of these tasks, while CGENNs can be applied more generally.

## 2 The Clifford Algebra

Clifford algebras (also known as _geometric algebras_) are powerful mathematical objects with applications in various areas of science and engineering. For a complete formal construction, we refer the reader to Appendix D. Let \(V\) be a finite-dimensional vector space over a field \(\) equipped with a _quadratic form_\(:V\). The _Clifford algebra_\((V,)\) is the unitary, associative, non-commutative algebra generated by \(V\) such that for every \(v V\) the relation \(v^{2}=(v)\) holds, i.e., _vectors square to scalars_. This simple property solely generates a unique mathematical theory that underpins many applications. Note that every element \(x\) of the Clifford algebra \((V,)\) is a linear combination of (formal, non-commutative) products of vectors modulo the condition that every appearing square \(v^{2}\) gets identified with the scalar square \((v)\): \(x=_{i I}c_{i} v_{i,1} v_{i,k_{i}}\)2. Here, the index set \(I\) is finite, \(c_{i}\), \(k_{}\), \(v_{i,j} V\). The Clifford algebra's associated _bi-linear form_\((v_{1},v_{2}):=((v_{1}+v_{2})- (v_{1})-(v_{2}))\) yields the _fundamental Clifford identity_: \(v_{1}v_{2}+v_{2}v_{1}=2(v_{1},v_{2})\) for \(v_{1},v_{2} V\) (Lemma D.3). In this context, the quantity \(v_{1}v_{2}\) represents the _geometric product_, which is aptly named for its ability to compute geometric properties and facilitate various transformations. Note that when \(v_{1},v_{2}\) are orthogonal (e.g., for orthogonal basis vectors), \((v_{1},v_{2})=0\), in which case \(v_{1}v_{2}=-v_{2}v_{1}\). The dimensionality of the algebra is \(2^{n}\), where \(n:= V\) (Theorem D.15). Let \(e_{1},,e_{n}\) be an orthogonal basis of \(V\). The tuple \((e_{A})_{A[n]},[n]:=\{1,,n\}\), is an orthogonal basis for \((V,)\), where for all such \(A\) the product \(e_{A}:=_{i A}^{i}e_{i}\) is taken in increasing order (Theorem D.26). We will see below that we can decompose the algebra into vector subspaces \(^{(m)}(V,),m=0,,n\), called _grades_, where \(^{(m)}(V,)=\). Elements of grade \(m=0\) and \(m=1\) are scalars (\(^{(0)}(V,)=\)) and vectors (\(^{(1)}(V,)=V\)), respectively, while elements of grade \(m=2\) and \(m=3\) are referred to as _bivectors_ and _trivectors_. Similar terms are used for elements of even higher grade. These higher-order grades can represent (oriented) points, areas, and volumes, as depicted in Figure 1.

Clifford algebras provide tools that allow for meaningful algebraic representation and manipulation of geometric quantities, including areas and volumes . In addition, they offer generalizations such as extensions of the exterior and Grassmannian algebras, along with the natural inclusion of complex numbers and Hamilton's quaternions . Applications of Clifford algebras can be found in robotics , computer graphics , signal processing , and animation . In the context of machine learning, Clifford algebras and hypercomplex numbers have been employed to improve the performance of algorithms in various tasks. For example,  learn an equivariant embedding using _geometric neurons_ used for classification tasks. Further,  introduce geometric algebra attention networks for point cloud problems in physics, chemistry, and biology. More recently,  introduce Clifford neural layers and Clifford Fourier transforms for accelerating solving partial differential equations.  continue this direction, strengthening the geometric inductive bias by the introduction of geometric templates. Concurrently with this work,  develop the geometric algebra transformer. Further,  introduce complex-valued and quaternion-valued networks. Finally,  define normalizing flows on the group of unit quaternions for sampling molecular crystal structures.

## 3 Theoretical Results

In order to construct equivariant multivector-valued neural networks, we outline our theoretical results. We first introduce the following theorem regarding the multivector grading of the Clifford algebra, which is well-known in case the algebra's metric is non-degenerate. Although the general case, including a potentially degenerate metric, appears to be accepted, we were unable to find a self-contained proof during our studies. Hence, we include it here for completeness.

**Theorem 3.1** (The multivector grading of the Clifford algebra).: _Let \(e_{1},,e_{n}\) and \(b_{1},,b_{n}\) be two orthogonal bases of \((V,)\). Then the following sub-vector spaces \(^{(m)}(V,)\) of \((V,)\)\(m=0,,n\), are independent of the choice of the orthogonal basis, i.e.,_

\[^{(m)}(V,):=\{e_{A}\,|\,A [n],|A|=m\}}{{=}} \{b_{A}\,|\,A[n],|A|=m\}.\] (1)

The proof can be found in Theorem D.27. Intuitively, this means that the claims made in the following (and the supplementary material) are not dependent on the chosen frame of reference, even in the degenerate setting. We now declare that the Clifford algebra \((V,)\) decomposes into an orthogonal direct sum of the vector subspaces \(^{(m)}(V,)\). To this end, we need to extend the bilinear form \(\) from \(V\) to \((V,)\). For elements \(x_{1},x_{2},x(V,)\), the _extended bilinear form_\(}\) and the _extended quadratic form_\(}\) are given via the projection onto the zero-component (explained below), where \(:(V,)(V,)\) denotes the _main anti-involution_ of \((V,)\)3 :

\[}:\,(V,)(V,)}, 28.452756pt}(x_{1},x_{2}):=((x_{1})x_{2})^{(0)}, 28.452756pt}(x):=}(x,x).\] (2)

Note that by construction, both \(}\) and \(}\) reduce to their original versions when restricted to \(V\). Using the extended quadratic form, the tuple \(((V,),})\) turns into a quadratic vector space in itself. As a corollary (see Corollary D.30), the Clifford algebra has an orthogonal-sum decomposition w.r.t. the extended bilinear form \(}\):

\[(V,)=_{m=0}^{n}^{(m)}(V,), 56.905512pt^{(m)}(V,)= .\] (3)

This result implies that we can always write \(x(V,)\) as \(x=x^{(0)}+x^{(1)}++x^{(n)}\), where \(x^{(m)}^{(m)}(V,)\) denotes the grade-\(m\) part of \(x\). Selecting a grade defines an orthogonal projection:

\[(\_)^{(m)}:\,(V,)^{(m)}(V, ), 56.905512ptx x^{(m)}, 56.905512ptm=0,,n.\] (4)

Let us further introduce the notation \(^{}(V,):=_{m}^{n} ^{(m)}(V,)\), whose elements are of _even parity_, and \(^{}(V,):=_{m}^{n} ^{(m)}(V,)\) for those of _odd parity_. We use \(x=x^{}+x^{}\) to denote the parity decomposition of a multivector.

### The Clifford Group and its Clifford Algebra Representations

Let \(^{}(V,)\) denote the group of invertible elements of the Clifford algebra, i.e., the set of those elements \(w(V,)\) that have an inverse \(w^{-1}(V,)\): \(w^{-1}w=ww^{-1}=1\). For \(w^{}(V,)\), we then define the (adjusted) _twisted conjugation_ as follows:

\[(w):\,(V,)(V,), 56.905512pt(w)(x):=wx^{}w^{-1}+(w)x^{}w^{-1},\] (5)

where \(\) is the _main involution_ of \((V,)\), which is given by \((w):=w^{}-w^{}\). This map \((w):\,(V,)(V,)\), notably not just from \(V V\), will be essential for constructing equivariant neural networks operating on the Clifford algebra. In general, \(\) and similar maps defined in the literature do not always posses the required properties (see Motivation E.1). However, when our \(\) is restricted to a carefully chosen subgroup of \(^{}(V,)\), many desirable characteristics emerge. This subgroup will be called the _Clifford group4_ of \((V,)\) and we define it as:

\[(V,):=\{w^{}(V,) (^{}(V,)^{}(V,))\, v V,\,(w)(v) V\}.\] (6)

In words, \((V,)\) contains all invertible (parity) homogeneous elements that preserve vectors (\(m=1\) elements) via \(\). The _special Clifford group_ is defined as \(^{}(V,):=(V,)^{[0 ]}(V,)\).

Regarding the twisted conjugation, \((w)\) was ensured to reduce to a _reflection_ when restricted to \(V-\) a property that we will conveniently use in the upcoming section. Specifically, when \(w,x^{(1)}(V,)=V\), \(w^{}(V,)\), then \((w)\) reflects \(x\) in the hyperplane normal to \(w\):

\[(w)(x)=-wxw^{-1}}{{=}}x-2(w,x)}{ (w,w)}w.\] (7)

Next, we collect several advantageous identities of \(\) in the following theorem, which we elaborate on afterwards. For proofs, consider Lemma E.8, Theorem E.10, and Theorem E.29.

**Theorem 3.2**.: _Let \(w_{1},w_{2},w(V,)\), \(x_{1},x_{2},x(V,)\), \(c\), \(m\{0,,n\}\). \(\) then satisfies:_

1. _Additivity:_ \((w)(x_{1}+x_{2})=(w)(x_{1})+(w)(x_{2})\)_,_
2. _Multiplicativity:_ \((w)(x_{1}x_{2})=(w)(x_{1})(w)(x_{2})\)_,   and:_  \((w)(c)=c\)_,_
3. _Invertibility:_ \((w^{-1})(x)=(w)^{-1}(x)\)_,_
4. _Composition:_ \((w_{2})((w_{1})(x))=(w_{2}w_{1})(x)\)_,   and:_  \((c)(x)=x\) _for_ \(c 0\)_,_
5. _Orthogonality:_ \(}((w)(x_{1}),(w)(x_{2}))=}(x_{1},x_{2})\)_._

The first two properties state that \((w)\) is not only _additive_, but even _multiplicative_ regarding the geometric product. The third states that \((w)\) is invertible, making it in _algebra automorphism_ of \((V,)\). The fourth property then states that \(:\,(V,)_{}( (V,))\) is also a _group homomorphism_ to the group of all algebra automorphisms. In other words, \((V,)\) is a linear representation of \((V,)\) and, moreover, it is also an algebra representation of \((V,)\). Finally, the last point shows that each \((w)\) generates an orthogonal map with respect to the extended bilinear form \(}\). These properties yield the following results (see also Theorem E.16, Corollary E.18, Figure 2).

**Corollary 3.3** (All grade projections are Clifford group equivariant).: _For \(w(V,)\), \(x(V,)\) and \(m=0,,n\) we have the following equivariance property:_

\[(w)(x^{(m)})=((w)(x))^{(m)}.\] (8)

_In particular, for \(x^{(m)}(V,)\) we also have \((w)(x)^{(m)}(V,)\)._

This implies that the grade projections: \((V,)^{(m)}(V,)\) are \((V,)\)-equivariant maps, and, that each \(^{(m)}(V,)\) constitutes an orthogonal representation of \((V,)\). The latter means that \(\) induces a group homomorphisms from the Clifford group to the group of all orthogonal invertible linear transformations of \(^{(m)}(V,)\):

\[^{(m)}:\,(V,)(^{(m)}(V,),}),^{(m)}(w):= (w)|_{^{(m)}(V,)}.\] (9)

**Corollary 3.4** (All polynomials are Clifford group equivariant).: _Let \(F[T_{1},,T_{}]\) be a polynomial in \(\) variables with coefficients in \(\), \(w(V,)\). Further, consider \(\) elements \(x_{1},,x_{}(V,)\). Then we have the following equivariance property:_

\[(w)(F(x_{1},,x_{}))=F((w)(x_{1}),,(w)(x _{})).\] (10)

To prove that \((w)\) distributes over any polynomial, we use both its additivity and multiplicativity regarding the geometric product.

By noting that one can learn the coefficients of such polynomials, we can build flexible parameterizations of Clifford group equivariant layers for quadratic vector spaces of any dimension or metric. We involve grade projections to achieve denser parameterizations. More details regarding neural network constructions follow in Section 4.

Figure 2: Commutative diagrams expressing Clifford group equivariance with respect to the main operations: polynomials \(F\) (left) and grade projections \((\_)^{(m)}\) (right).

### Orthogonal Representations

To relate to the _orthogonal group_\((V,)\), the set of invertible linear maps \(:V V\) preserving \(\), first note that Equation (5) shows that for every \(w^{}\) we always have: \((w)=_{(V,)}\). So the action \(\) on \((V,)\) can already be defined on the quotient group \((V,)/^{}\). Moreover, we have:

**Theorem 3.5** (See also Remark E.31 and Corollary E.32 ).: _If \((V,)\) is non-degenerate5 then \(\) induces a well-defined group isomorphism:_

\[^{(1)}:\,(V,)/^{} }{{}}(V,), [w](w)|_{V}.\] (11)

The above implies that \((V,)\) acts on whole \((V,)\) in a well-defined way. Concretely, if \(x(V,)\) is of the form \(x=_{i I}c_{i} v_{i,1} v_{i,k_{i}}\) with \(v_{i,j} V\), \(c_{i}\) and \((V,)\) is given by \(^{(1)}([w])=\) with \(w(V,)\), then we have:

\[(w)(x)=_{i I}c_{i}(w)(v_{i,1})(w)(v_{i,k_{i}})= _{i I}c_{i}(v_{i,1})(v_{i,k_{i}}).\] (12)

This means that \(\) acts on a multivector \(x\) by applying an orthogonal transformation to all its vector components \(v_{i,j}\), as illustrated in Figure 1. Theorem 3.5 implies that that a map \(f:(V,)^{_{}}(V, )^{_{}}\) is **equivariant to the Clifford group \((V,)\) if and only if it is equivariant to the orthogonal group \((V,)\)** when both use the actions on \((V,)\) described above (for each component). To leverage these results for constructing \((V,)\)-equivariant neural networks acting on vector features, i.e., \(x V^{_{}}\), we refer to Section 4.

To prove Theorem 3.5, we invoke _Cartan-Dieudonne_ (Theorem C.13), stating that every orthogonal transformation decomposes into compositions of reflections, and recall that \(\) reduces to a reflection when restricted to \(V\), see Theorem E.25. Theorem 3.5 also allows one to provide explicit descriptions of the element of the Clifford group, see Corollary E.27.

Finally, it is worth noting that our method is also \(\) and \(\) group-equivariant6. These groups, sharing properties with the Clifford group, are also often studied in relation to the orthogonal group.

## 4 Methodology

We restrict our layers to use \(:=\). Our method is most similar to steerable methods such as . However, unlike these works, we do not require an alternative basis representation based on spherical harmonics, nor do we need to worry about Clebsch-Gordan coefficients. Instead, we consider simply a steerable vector basis for \(V\), which then automatically induces a _steerable multivector basis_ for \((V,)\) and its transformation kernels. By steerability, we mean that this basis can be transformed in a predictable way under an action from the Clifford group, which acts orthogonally on both \(V\) and \((V,)\) (see Figure 1).

We present layers yielding Clifford group-equivariant optimizable transformations. All the main ideas are based on Corollary 3.3 and Corollary 3.4. It is worth mentioning that the methods presented here form a first exploration of applying our theoretical results, making future optimizations rather likely.

Linear LayersLet \(x_{1},,x_{}(V,)\) be a tuple of multivectors expressed in a steerable basis, where \(\) represents the number of input channels. Using the fact that a polynomial restricted to the first order constitutes a linear map, we can construct a linear layer by setting

\[y_{c_{}}^{(k)}:=T_{_{c_{}}}^{}(x_{1}, ,x_{})^{(k)}:=_{c_{}=1}^{}_{c_{}c_{ }k}\,x_{c_{}}^{(k)},\] (13)

where \(_{c_{}c_{}k}\) are optimizable coefficients and \(c_{}\), \(c_{}\) denote the input and output channel, respectively. As such, \(T_{}:(V,)^{}(V,)\) is a linear transformation in each algebra subspace\(k\). Recall that this is possible due to the result that \((w)\) respects the multivector subspaces. This computes a transformation for the output channel \(c_{}\); the map can be repeated (using different sets of parameters) for other output channels, similar to classical neural network linear layers. For \(y_{}^{(0)}\) (the scalar part of the Clifford algebra), we can additionally learn an invariant bias parameter.

Geometric Product LayersA core strength of our method comes from the fact that we can also parameterize interaction terms. In this work, we only consider layers up to second order. Higher-order interactions are indirectly modeled via multiple successive layers. As an example, we take the pair \(x_{1},x_{2}\). Their interaction terms take the form \((x_{1}^{(i)}x_{2}^{(j)})^{(k)},i,j,k=0,,n\); where we again make use of the fact that \((w)\) respects grade projections. As such, all the grade-\(k\) terms resulting from the interaction of \(x_{1}\) and \(x_{2}\) are parameterized with

\[P_{}(x_{1},x_{2})^{(k)}:=_{i=0}^{n}_{j=0}^{n}_{ijk}\,(x_ {1}^{(i)}x_{2}^{(j)})^{(k)},\] (14)

where \(P_{}:(V,)(V, )(V,)\). This means that we get \((n+1)^{3}\) parameters for every geometric product between a pair of multivectors7. Parameterizing and computing all second-order terms amounts to \(^{2}\) such operations, which can be computationally expensive given a reasonable number of channels \(\). Instead, we first apply a linear map to obtain \(y_{1},,y_{}(V,)\). Through this map, the mixing (i.e., the terms that will get multiplied) gets learned. That is, we only get \(\) pairs \((x_{1},y_{1}),,(x_{},y_{})\) from which we then compute \(z_{c_{}}^{(k)}:=P_{_{c_{}}}(x_{c_{}},y_{c_{ }})^{(k)}\). Note that here we have \(c_{}=c_{}\), i.e., the number of channels does not change. Hence, we refer to this layer as the _element-wise_ geometric product layer. We can obtain a more expressive (yet more expensive) parameterization by linearly combining such products by computing

\[z_{c_{}}^{(k)}:=T_{_{c_{}}}^{}(x_{1},, x_{},y_{1},,y_{})^{(k)}:=_{c_{}=1}^{}P_{_{c_{ }}c_{}}(x_{c_{}},y_{c_{}})^{(k)},\] (15)

which we call the _fully-connected_ geometric product layer. Computational feasibility and experimental verification should determine which parameterization is preferred.

Normalization and NonlinearitiesSince our layers involve quadratic and potentially higher-order interaction terms, we need to ensure numerical stability. In order to do so, we use a normalization operating on each multivector subspace before computing geometric products by putting

\[x^{(m)}}{(a_{m})\,(}(x^{(m)})- 1)+1},\] (16)

where \(x^{(m)}^{(m)}(V,)\). Here, \(\) denotes the logistic sigmoid function, and \(a_{m}\) is a learned scalar. The denominator interpolates between \(1\) and the quadratic form \(}(x^{(m)})\), normalizing the magnitude of \(x^{(m)}\). This ensures that the geometric products do not cause numerical instabilities without losing information about the magnitude of \(x^{(m)}\), where a learned scalar interpolates between both regimes. Note that by Theorem 3.2, \(}(x^{(m)})\) is invariant under the action of the Clifford group, rendering Equation (16) an equivariant map.

Next, we use the layer-wide normalization scheme proposed by , which, since it is also based on the extended quadratic form, is also equivariant with respect to the twisted conjugation.

Regarding nonlinearities, we use a slightly adjusted version of the units proposed by . Since the scalar subspace \(^{(0)}(V,)\) is always invariant with respect to the twisted conjugation, we can apply \(x^{(m)}(x^{(m)})\) when \(m=0\) and \(x^{(m)}_{}(}(x^{(m)})) x^{(m)}\) otherwise. We can replace ReLU with any common scalar activation function. Here, \(_{}\) represents a potentially parameterized nonlinear function. Usually, however, we restrict it to be the sigmoid function. Since we modify \(x^{(m)}\) with an invariant scalar quantity, we retain equivariance. Such gating activations are commonly used in the equivariance literature .

Embedding Data in the Clifford AlgebraIn this work, we consider data only from the vector space \(V\) or the scalars \(\), although generally one might also encounter, e.g., _bivector_ data. That is, we have some scalar features \(h_{1},,h_{k}\) and some vector features \(h_{k+1},,h_{} V\). Typical examples of scalar features include properties like mass, charge, temperature, and so on. Additionally, one-hot encoded categorical features are also included because \(\{0,1\}\) and they also transform trivially. Vector features include positions, velocities, and the like. Then, using the identifications \(^{(0)}(V,)\) and \(^{(1)}(V,) V\), we can embed the data into the scalar and vector subspaces of the Clifford algebra to obtain Clifford features \(x_{1},,x_{}(V,)\).

Similarly, we can predict scalar- or vector-valued data as output of our model by grade-projecting onto the scalar or vector parts, respectively. We can then directly compare these quantities with ground-truth vector- or scalar-valued data through a loss function and use standard automatic differentiation techniques to optimize the model. Note that _invariant_ predictions are obtained by predicting scalar quantities.

## 5 Experiments

Here, we show that CGENNs excel across tasks, attaining top performance in several unique contexts. Parameter budgets as well as training setups are kept as similar as possible to the baseline references. All further experimental details can be found in the public code release.

### Estimating Volumetric Quantities

\((3)\) **Experiment: Signed Volumes** This task highlights the fact that equivariant architectures based on scalarization are not able to extract some essential geometric properties from input data. In a synthetic setting, we simulate a dataset consisting of random three-dimensional tetrahedra. A main advantage of our method is that it can extract covariant quantities including (among others) _signed volumes_, which we demonstrate in this task. Signed volumes are geometrically significant because they capture the orientation of geometric objects in multidimensional spaces. For instance, in computer graphics, they can determine whether a 3D object is facing towards or away from the camera, enabling proper rendering. The input to the network is the point cloud and the loss function is the mean-squared error between the signed volume and its true value. Note that we are predicting a _covariant_ (as opposed to _invariant_) scalar quantity (also known as a _pseudoscalar_) under \((3)\) transformations using a positive-definite (Euclidean) metric. The results are displayed in the left part of Figure 3. We compare against a standard multilayer perceptron (MLP), an MLP version of the \((n)\)-GNN  which uses neural networks to update positions with scalar multiplication, _Vector Neurons_ (VN) , and _Geometric Vector Perceptrons_ (GVP) . We see that the scalarization methods fail to access the features necessary for this task, as evidenced by their test loss not improving even with more available data. The multilayer perceptron, although a universal approximator, lacks the correct inductive biases. Our model, however, has the correct inductive biases (e.g., the equivariance property) and can also access the signed volume. Note that we do not take the permutation invariance of this task into account, as we are interested in comparing our standard feed-forward architectures against similar baselines.

\((5)\) Experiment: Convex HullsWe go a step further and consider a _five-dimensional_ Euclidean space, showcasing our model's ability to generalize to high dimensions. We also make the experiment more challenging by including more points and estimating the volume of the convex hull generated by these points - a task that requires sophisticated algorithms in the classical case. Note that some points may live inside the hull and do not contribute to the volume. We use the same network architectures as before (but now embedded in a five-dimensional space) and present the results in Figure 3. We report the error bars for CGENNs, representing three times the standard deviation of the results of eight runs with varying seeds. Volume (unsigned) is an invariant quantity, enabling the baseline methods to approximate its value. However, we still see that CGENNs outperform the other methods, the only exception being the low-data regime of only 256 available data points. We attribute this to our method being slightly more flexible, making it slightly more prone to overfitting. To mitigate this issue, future work could explore regularization techniques or other methods to reduce overfitting in low-data scenarios.

### \((5)\) Experiment: Regression

We compare against the methods presented by  who propose an \((5)\)-invariant regression problem. The task is to estimate the function \(f(x_{1},x_{2}):=(\|x_{1}\|)-\|x_{2}\|^{3}/2+^{}x_{2}}{\|x_ {1}\|\|x_{2}\|}\), where the five-dimensional vectors \(x_{1},x_{2}\) are sampled from a standard Gaussian distribution in order to simulate train, test, and validation datasets. The results are shown in Figure 4. We used baselines from  including an MLP, MLP with augmentation (MLP+Aug), and the \((5)\)- & \((5)\)-MLP architectures. We maintain the same number of parameters for all data regimes. For extremely small datasets (30 and 100 samples), we observe some overfitting tendencies that can be countered with regularization (e.g., weight decay) or using a smaller model. For higher data regimes (300 samples and onward), CGENNs start to significantly outperform the baselines.

### \((3)\) Experiment: \(n\)-Body System

The \(n\)-body experiment  serves as a benchmark for assessing the performance of equivariant (graph) neural networks in simulating physical systems . In this experiment, the dynamics of \(n=5\) charged particles in a three-dimensional space are simulated. Given the initial positions and velocities of these particles, the task is to accurately estimate their positions after 1 000 timesteps. To address this challenge, we construct a graph neural network (GNN) using the Clifford equivariant layers introduced in the previous section. We use a standard message-passing algorithm  where the message and update networks are CGENNs. So long as the message aggregator is equivariant, the end-to-end model also maintains equivariance. The input to the network consists of the mean-subtracted positions of the particles (to achieve translation invariance) and their velocities. The model's output is the estimated displacement, which is to the input to achieve translation-equivariant estimated target positions. We include the invariant charges as part of the input and their products as edge attributes. We compare against the steerable SE(3)-Transformers , Tensor Field Networks , and SEGNN . Scalarization baselines include Radial Field  and EGNN . Finally, NMP  is not an \((3)\)-equivariant method. The number of parameters in our model is maintained similar to the EGNN and SEGNN baselines to ensure a fair comparison.

Results of our experiment are presented in Table 1, where we also present for CGENN three times the standard deviation of three identical runs with different seeds. Our approach clearly outperforms earlier methods and is significantly better than , thereby surpassing the baselines. This experiment again demonstrates the advantage of leveraging covariant information in addition to scalar quantities, as it allows for a more accurate representation of the underlying physics and leads to better predictions.

   Method & MSE (\(\)) \\  SE(3)-Tr. & \(0.0244\) \\ TFN & \(0.0155\) \\ NMP & \(0.0107\) \\ Radial Field & \(0.0104\) \\ EGNN & \(0.0070\) \\ SEGNN & \(0.0043\) \\  CGENN & \(\) \\   

Table 1: Mean-squared error (MSE) on the \(n\)-body system experiment.

### \((1,3)\) Experiment: Top Tagging

Jet tagging in collider physics is a technique used to identify and categorize high-energy jets produced in particle collisions, as measured by, e.g., CERN's ATLAS detector . By combining information from various parts of the detector, it is possible to trace back these jets' origins . The current experiment seeks to tag jets arising from the heaviest particles of the standard model: the 'top quarks' . A jet tag should be invariant with respect to the global reference frame, which can transform under _Lorentz boosts_ due to the relativistic nature of the particles. A Lorentz boost is a transformation that relates the space and time coordinates of an event as seen from two inertial reference frames. The defining characteristic of these transformations is that they preserve the _Minkowski metric_, which is given by \((ct,x,y,z):=(ct)^{2}-x^{2}-y^{2}-z^{2}\). Note the difference with the standard positive definite Euclidean metric, as used in the previous experiments. The set of all such transformations is captured by the orthogonal group \((1,3)\); therefore, our method is fully compatible with modeling this problem.

We evaluate our model on a top tagging benchmark published by . It contains 1.2M training entries, 400k validation entries, and 400k testing entries. For each jet, the energy-momentum \(4\)-vectors are available for up to \(200\) constituent particles, making this a much larger-scale experiment than the ones presented earlier. Again, we employ a standard message passing graph neural network  using CGENNs as message and update networks. The baselines include ResNeXt , P-CNN , PFN , ParticleNet , LGN , EGNN , and the more recent LorentzNet . Among these, LGN is a steerable method, whereas EGNN and LorentzNet are scalarization methods. The other methods are not Lorentz-equivariant. Among the performance metrics, there are classification accuracy, Area Under the Receiver Operating Characteristic Curve (AUC), and the background rejection rate \(1/_{B}\) at signal efficiencies of \(_{S}=0.3\) and \(_{S}=0.5\), where \(_{B}\) and \(_{S}\) are the false positive and true positive rates, respectively. We observe that LorentzNet, a method that uses invariant quantities, is an extremely competitive baseline that was optimized for this task. Despite this, CGENNs are able to match its performance while maintaining the same core implementation.

## 6 Conclusion

We presented a novel approach for constructing \((n)\)- and \((n)\)-equivariant neural networks based on Clifford algebras. After establishing the required theoretical results, we proposed parameterizations of nonlinear multivector-valued maps that exhibit versatility and applicability across scenarios varying in dimension. This was achieved by the core insight that polynomials in multivectors are \((n)\)-equivariant functions. Theoretical results were empirically substantiated in three distinct experiments, outperforming or matching baselines that were sometimes specifically designed for these tasks.

CGENNs induce a (non-prohibitive) degree of computational overhead similar to other steerable methods. On the plus side, we believe that improved code implementations such as custom GPU kernels or alternative parameterizations to the current ones can significantly alleviate this issue, potentially also resulting in improved performances on benchmark datasets. This work provides solid theoretical and experimental foundations for such developments.

   Model & Accuracy (\(\)) & AUC (\(\)) & \(1/_{B}\ ()\\ (_{S}=0.5)\) & \(1/_{B}\ ()\\ (_{S}=0.3)\) \\  ResNeXt  & \(0.936\) & \(0.9837\) & \(302\) & \(1147\) \\ P-CNN  & \(0.930\) & \(0.9803\) & \(201\) & \(759\) \\ PFN  & \(0.932\) & \(0.9819\) & \(247\) & \(888\) \\ ParticleNet  & \(0.940\) & \(0.9858\) & \(397\) & \(1615\) \\ EGNN  & \(0.922\) & \(0.9760\) & \(148\) & \(540\) \\ LGN  & \(0.929\) & \(0.9640\) & \(124\) & \(435\) \\ LorentzNet  & \(\) & \(\) & \(\) & \(\) \\  CGENN & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Performance comparison between our proposed method and alternative algorithms on the top tagging experiment. We present the accuracy, Area Under the Receiver Operating Characteristic Curve (AUC), and background rejection \(1/_{B}\) and at signal efficiencies of \(_{S}=0.3\) and \(_{S}=0.5\).