# Do LLMs Build World Representations?

Probing Through the Lens of State Abstraction

Zichao Li

Mila, McGill University

zichao.li@mail.mcgill.ca

&Yanshuai Cao

Borealis AI

yanshuai.cao@borealisai.com

&Jackie C.K. Cheung

Mila, McGill University

jackie.cheung@mcgill.ca

Work was done during a Mitacs internship at Borealis AI.Code and dataset: https://github.com/BorealisAI/llm-world-abs

###### Abstract

How do large language models (LLMs) encode the state of the world, including the status of entities and their relations, as described by a text? While existing work directly probes for a complete state of the world, our research explores whether and how LLMs abstract this world state in their internal representations. We propose a new framework for probing for world representations through the lens of state abstraction theory from reinforcement learning, which emphasizes different levels of abstraction, distinguishing between general abstractions that facilitate predicting future states and goal-oriented abstractions that guide the subsequent actions to accomplish tasks. To instantiate this framework, we design a text-based planning task, where an LLM acts as an agent in an environment and interacts with objects in containers to achieve a specified goal state. Our experiments reveal that fine-tuning as well as advanced pre-training strengthens LLM-built representations' tendency of maintaining goal-oriented abstractions during decoding, prioritizing task completion over recovery of the world's state and dynamics.1

## 1 Introduction

Drawing inspiration from human mental models [9; 12; 22], AI researchers have introduced the concept of world models for sample-efficient and robust machine learning systems [13; 25]. Specifically, a world model is defined to fulfill a dual role: (1) it estimates information about the world state that may not be directly observable from the input signals, and (2) it distills essential information to predict future states, thereby informing subsequent actions based on these predictions.

Recently, there has been growing interest in investigating whether pre-trained Transformer models , especially large language models (LLMs) , construct implicit world models. These investigations aim to determine whether the state of the world as described in the text, either implicitly or explicitly, can be recovered from the internal representations built by Transformer models. However, the field has produced studies with conflicting conclusions when examining different tasks. For example,  shows that it is possible to accurately probe the status of entities and their semantic relations encoded in the representations of LLMs within a discourse. Conversely,  report negative results when attempting a similar yet more challenging setting. In addition,  successfully extracts the board state of a partially played Othello game from the internal representation of a small-scale GPT model  trained to complete game scripts.

We argue that these contradictions stem from the fact that recovering the complete world state is not always necessary for solving every tasks, which previous studies have not systematically controlled for. Rather, an abstract representation of the world state may sometimes be sufficient, and the necessary level of abstraction can also vary.

Some tasks may require the complete recovery of world dynamics in order to predict future states, while others may get by on one that omits this information as it is unnecessary for task completion. For instance, consider Figure 1. When parsing a discourse into a representation of the world state, one can either record the location of each object or simply note the number of objects in each box. The former offers the possibility to answer diverse questions, including future state predictions after operations, e.g., _Move the key to Box C_. The latter, however, is restricted to a counting task, unable to foresee future states. Neglecting this nuance could lead to a mismatch between evidence and conclusion, causing undue pessimism about LLMs lacking awareness of the world or excessive optimism regarding their ability to develop general world representations. For example, fine-tuning on all types of questions in Figure 1 might push LLMs to capture world dynamics, this does not guarantee the same outcome when fine-tuning solely on counting tasks.

We formalize this intuition through the application of _state abstraction theory_, originally proposed in reinforcement learning (RL)  to simplify the state space by aggregating similar states to abstract ones without modifying the core aspects of the task or underlying world. The level of abstraction is a spectrum, from more general _world-irrelevant abstraction_, that allows recovery of world dynamics to more _goal-oriented abstractions_, guiding task completion while giving up on predicting future states. These include \(Q^{*}\)_-irrelevant_ and \(^{*}\)_-irrelevant abstraction_; the former preserves the long-term impact of actions, while the latter preserves the optimal policy.

Through this lens of state abstraction, we propose a new framework for examining the world representations constructed by LLMs. This framework investigates the various types of abstractions that may be encoded by LLM-built representations. To demonstrate its utility, we present a concrete application. First, we design a text-based planning task, RePlace, which requires altering the state of a simplistic world with a collection of containers and objects. We intentionally craft the task's state space to be highly structured and modular, enabling a precise yet simple derivation of world state abstractions at different levels. Despite its simplicity, the abstract states at different levels are distinct and identifiable. Subsequently, we prompt pre-trained and fine-tuned LLMs to complete this planning task and extract their representation during decoding. Finally, we probe different abstract states within the representations. We conduct experiments on a wide range of Transformer models and LLMs, namely Pythia , Llama2 , Llama3 , Mistral  and Phi3 . Our experiments show that LLMs achieving reasonable performance on RePlace, whether through fine-tuning or advanced pre-training, tend to maintain goal-oriented abstractions rather than more general world representations during decoding. Additionally, pre-trained models with near-random performance fail to efficiently preserve any type of abstractions.

Our contributions are as follows: 1) We propose a new framework to probe world abstraction from LLM-built representations that can be adapted for other NLP tasks. 2) We release a new synthetic task, RePlace, and accompanying datasets that are modular and extendable. 3) Experiments using our framework and task yield novel findings: LLM representations prioritize goal-oriented abstractions that preserve the effect of actions in terms of task completion while abstracting out the world state and dynamics during decoding. 4) Our findings also reconcile conflicting conclusions in prior work. For instance,  successfully probe state variables like disc color and obstacle position, which

Figure 1: A discourse and two possible abstractions of the world state described by it (top half of figure). A general abstraction (termed as _world-irrelevant abstraction_ later) enables one to answer a wide range of questions. On the other hand, a count-oriented abstraction is only applicable to the counting task.

pertains to goal-oriented abstraction for the task that the models are optimized to solve, while  struggles to recover entity status, which lies outside these abstractions.

## 2 Related Work

### World models

The concept of world models in machine learning has deep connections with the concept of human internal models  in cognitive science, most often referred to as mental models [12; 22]. As originally defined, mental models can build abstract and symbolic representations of entities and their relations in the real world or environment around them . One of the most important characteristics of such mental models is their optimal balance between representation complexity and utility for accomplishing a specific task . In a similar vein, AI researchers have developed symbolic  or neural world models [13; 14] to compress vast amounts of input information and extract a simplified and essential representation to predict future states. Our work is related to both domains and applies state abstraction theory [29; 3] from reinforcement learning to assess the representations of the underlying world and the task, if any, encoded by LLMs.

### Probing LLM Representations

There is a growing interest in probing for interpretable features in LLMs, most of which are linguistic in nature, such as morphology , syntax  and word-level semantics . Recently, researchers have moved beyond the exploration of shallow linguistic features, investigating whether and how LLMs' hidden representations on the fly encode entities and their relations, as described in the text. More specifically,  trains a shallow neural classifier on top of LLMs' representations of discourse to recover the ground-truth situations as depicted in the text. This approach is further modified and formulated as a next-sentence prediction task by . Similarly, [28; 21; 37] probes the internal representation of world state in Transformer models reading (semi-)structured input, such as game scripts and embodied sequences.

However, existing work primarily focuses on probing for a comprehensive description of the underlying world, defined as a set of state variables. This approach has two main limitations. First, it fails to distinguish the function of a specific state variable: Is it intended to maintain a general representation of the world, or is it crucial only for specific tasks, or both? Without this distinction, we cannot accurately interpret the positive outcomes of probing. Second, existing studies do not account for the potential abstraction of the world that LLM might build in its representation. Our work overcomes these limitations by introducing a general framework that examines the world abstraction encoded within LLM representations.

In contrast to [28; 21; 37], the input in our task is text data instead of a game script or environment layout. Therefore, the conclusions drawn from our experiments are more applicable to LLMs and Transformer models for NLP tasks.

## 3 Framework

In this section, we introduce a new probing framework to investigate the world state abstraction encoded by large language models (LLMs) when prompted to perform a decision-making task. We start with a reinforcement learning (RL) formulation, which is general enough to encompass a wide range of NLP tasks and other tasks adopted by previous work, e.g. completing an Othello game script . An RL problem is characterized by a 4-tuple: \((,,T,R)\), where \(s\) represents the world state, \(a\) denotes possible actions the model can take, transition function \(T(s^{}|s,a)\) measures the probability of transitioning to state \(s^{}\) induced by \(a\) from \(s\), and \(R(s,a)\) is the reward.

### Definition and Derivation of World State Abstraction

Our framework builds upon a rigorous definition of _world abstraction_. In particular, we follow RL literature , deriving state abstraction function \(:_{}\) that maps each state \(s\) into an abstract state \((s)\) (\(|_{}|||\)). In addition to the raw state \(\), we consider three types of abstraction:World-irrelevant abstraction\(_{w}\)2 ensures that \( s_{1},s_{2}\) where \(_{w}(s_{1})=_{w}(s_{2})\) implies that \( a,x^{}_{_{w}},R(s_{1},a)=R(s_{2},a),\) and \(_{s^{}_{w}^{-1}(x^{})}T(s^{}|s_{1},a)=_{s^{ }_{w}^{-1}(x^{})}T(s^{}|s_{2},a)\). Intuitively speaking, it preserves the transition dynamic of the underlying world and the reward function, thereby enabling the prediction of future states induced by subsequent actions. Recalling the definition of world models in Section 1, this type of abstraction enables precise recovery of the world model.

\(Q^{*}\)**-irrelevant abstraction**\(_{Q}\) ensures that \( s_{1},s_{2}\), if \(_{Q}(s_{1})=_{Q}(s_{2})\), then \( a\), \(Q^{*}(s_{1},a)=Q^{*}(s_{2},a)\), where \(Q^{*}(s,a)=_{}Q_{}(s,a)\). This form of abstraction preserves the effect of all actions \(a\) in terms of their optimal \(Q\)-value, which is the maximal expected future rewards. However, it discards the world dynamics, making future state predictions infeasible. LLMs can leverage this abstraction to be cost-sensitive, minimizing action counts and avoiding penalties for violating world constraints.

\(^{*}\)**-irrelevant abstraction**\(_{}\) guarantees that the optimal action, and hence the optimal policy, can be recovered. Formally, \( s_{1},s_{2} S\), \(_{}(s_{1})=_{}(s_{2})\) implies that \(^{*}(s_{1})=^{*}(s_{2})\). Therefore, this is the coarsest abstraction that one can recover the optimal policy. Therefore, we expect that at least this type of abstraction can be accurately probed from an LLM that excels in the given task.

In practice, one may use learning [23; 4] or heuristics [10; 11] to derive the abstractions at each level.

### Probing World Abstraction from LLM-built Representations

We aim to assess which types of state abstraction are encoded in LLM representations. To do so, we first prompt a pre-trained or fine-tuned LLM with \((s_{0},A_{1:t-1},z)\), which is a textual description of the initial state \(s_{0}\), previously executed actions \(A_{1:t-1}\), and optionally, feedback \(z\) received from the world. The feedback could be the opponent's moves in an Othello game or users' utterances in a dialogue system. Next, we extract the hidden states \(H^{(m)}\) from the \(m\)-th layer of the LLM, considering \(m\) as a hyperparameter. Following , we select the last hidden states \(h_{t}^{(m)}\)3, which is used by the LLM to predict subsequent actions. After collecting \((s_{t},h_{t})\) pairs, where \(s_{t}\) is the world state induced by \(s_{0}\), \(A_{1:t-1}\) and \(z\), we detect the existence of abstraction of \(s_{t}\) in LLM representations by assessing if \((s_{t})\) can be probed from \(h_{t}\) with accuracy surpassing a random baseline, potentially achieving near-perfect performance. Previous work [16; 17] on probing presents a challenge in differentiating whether a representation encodes a linguistic property or if the probe itself learns the task. While these works focus on low-level syntax features [16; 30], our focus is on the recoverability of world states and their abstractions from LLM representations, questioning whether these are maintained or discarded during decoding. Therefore, we train probes to classify raw and abstract states, following previous work [27; 28], which essentially estimates the mutual information between abstractions and representations . As we will show in Section 6, probing either raw or abstract states from LLM representations is not always successful.

**Remark:** To draw faithful conclusions from probing experiments, it is crucial to carefully design or select the tasks in a way that ensures the spaces of abstract states at different levels differ (\(_{_{w}}_{_{Q}}_{_{}}\)). Otherwise, one cannot determine whether the success of probing stems from the LLM's preference for learning a general world model or from the necessity to recover the world state while learning the optimal policy. For instance, in the game of Othello, it is feasible to recover most of the current board state from all possible legal moves. As such, the raw state space is almost identical to the coarsest \(^{*}\)-irrelevant abstraction for predicting legal moves. Therefore, a plausible interpretation for the success of probing in  could be that the Transformer model learns the \(^{*}\)-irrelevant abstraction rather than deliberately learning a general world model.

In the next section, we design a planning task within an RL framework and synthesize a dataset accordingly. In Section 5, we prompt LLMs to perform this task and probe different types of abstractions from their internal representations.

## 4 RePlace: A Text-based Planning Task

To instantiate our framework, we draw inspiration from recent work on probing discourse representation in LLMs [27; 24] and design a text-based planning task named RePlace. In RePlace,an LLM acts as an agent within a simple world consisting of a set of containers and objects. The objective of the LLM is to predict actions that alter the situation of the objects to match a described target situation. We design this task for three reasons. First, in its symbolic form, the task possesses a simple and modular state structure, making it easier to analyze and derive abstract state spaces. Second, it is closely related to the gripper problem  and the entity tracking task , which are widely adopted in planning and NLP interpretability research. Third, as we will demonstrate, each type of abstraction has a unique state space, enabling the assessment of whether LLM-built representations encode specific abstractions.

In this section, we start with formulating the planning task within the RL framework, then introduce the textual realization of the RL elements, and finally the method of curating prompts.

### Specification within RL Formulation

Formally, the specific design of the elements within the RL framework is described as follows:

**1.** State \(s=[u,g]\): the current situation \(u\) and target situation \(g\) of the world, each of which is factorized as the Cartesian product of a set of assignments over predicates applied to entities \(\), including objects \(\) and containers \(\). The predicates are:

* \((i,o)\): the object \(o\) is stored in the \(i\)-th container from left to right.
* \((o)\): the agent has the object \(o\).
* \((i)\): the agent is at the \(i\)-th container.
* \((i,b)\): the name of the \(i\)-th container from left to right is \(b\).

**2.** The possible actions \(a\) include:

* \((d)\): Move along a direction \(d\), \(\) or \(\), for one step.
* \((o)\): Grab an object \(o\) from the nearby container.

Figure 4: The raw and abstract predicates of the state described in Figure 2. \(u,g\): current and target situations.

Figure 3: Derivation of abstract predicates and action from raw predicates. Better viewed in color. Predicates represented by multicolored squares indicate association with multiple abstraction levels. Grey squares represent intermediate predicates, not associated with any abstraction, used to derive abstract predicates.

Figure 2: An example of task input in RePlacE.

* \((o)\): Put an object from the agent to the nearby container.
* Transition function \(T\): Following previous work , we consider a static environment. That means \(T(s^{}|s,a)\{0,1\}\). Moreover, the transition impacts only the current situation \(u_{t}\) in \(s_{t}\) by taking the effect of \(a\). For example, if \((2)\) is True in \(u_{t-1}\), executing the action move(left) results in \((1)\) becoming True in \(u_{t}\).
* Reward function \(R(s,a)\) checks whether the new situation of \(\), induced by executing \(a\), matches the target situation: \(R(s,a)=r\) if \(_{}(u^{})=_{}(g)\); or \(-(r+r^{})\) if \(a C(s)\); or \(-r\) otherwise. Here \(u^{}\) denotes the new situation, \(_{}(u^{})\) the set of assignments on predicates involving objects \(\) (store and held) in the new situation \(u^{}\), and \(C:\), the constraint function mapping states to permissible actions. We incorporate three spatial constraints: (1) the agent cannot move left and right at the leftmost and rightmost locations, respectively; (2) \(\) is only possible if the target object \(o\) is in a nearby container; (3) put is allowed only for objects currently held by the agent. Given the LLM's insensitivity to exact numerical values in prompts, we do not set concrete values for \(r\) and \(r^{}\) but simply assume \(r>0\) and \(r^{}>0\). Notably, both \(Q^{*}\) and \(^{*}\), as well as the abstract states, are invariant to the actual magnitudes of \(r\) and \(r^{}\).

We map the situation (\(u\) or \(g\)), previous actions (\(A_{1:t}=\{a_{1},..,a_{t}\}\)) to their respective natural language descriptions \(_{s}(u)\), \(_{s}(g)\), \(_{a}(A_{1:t})\), and using a set of textual templates: \(_{s}()\) and \(_{a}()\). As such, the input text to the model comprises two main components: (1) a general task instruction (see Appendix C) specifying the actions the agent can take, \(\{_{a}(a)|a\}\), and the constraints it must adhere to, \((C)\), and (2) textual descriptions of the world's initial and target situations, as well as previous actions \([_{s}(u);_{s}(g);_{a}(A_{1:t})]\). An example is presented in Figure 2. Conditioned on this, the LLM generates textual descriptions \(\) of subsequent actions, which are parsed back into their symbolic form \(_{t+1:T}\) by a rule-based parser \(_{a}^{-1}()\).

### Datasets

We synthesize two English datasets for RePlace: Gripper and Cook. For each instance in Gripper, we first sample a set of \(\) from a list of container names and \(\) from a list of frequent nouns in British National Corpus (BNC) . Subsequently, we generate pairs of initial and target situations, \([u_{0},g]\), by randomly assigning values to the predicates in \(u_{0}\) and \(g\), such that it requires the execution of \(2\) to \(6\) actions to transition from \(u_{0}\) to \(g\). Towards a more realistic setting, we introduce the following dataset variants: (1) _Lexical variants_, which includes rare nouns collected from BNC for b, adding color and size modifiers to describe the objects, and use diverse textual templates for translating predicates; (2) _Partial_\((g)\), which omits explicit information about held in the target situation, requiring logical inference by LLMs; (3) _Partial_\([(u_{t}),(g)]\), an extension of (2) that includes previous operations performed by the agent, thus requiring inferring \(u_{t}\). Appendix C provides concrete examples for each variant. The final dataset is a uniform mixture of these four variants and segmented into training, validation, and test sets, with \([u_{0},g]\) splits, allocating 50k, 1k, and 1k instances to each set, respectively.

Cook generalizes the setting of Gripper to a grid world of containers and is generated with TextWorld . Similarly, we sample and transform the initial and goal situations into textual descriptions. The main differences between Cook and Gripper are: (1) the constants of direction \(d\) are different: the agent in Cook can move north, south, east or west; (2) Cook uses a different set of textual templates adopted from the _cook_ domain in TextWorld. See Appendix D for more details about Cook. Notably, both datasets share the same world abstractions at all levels.

## 5 Probing World Abstractions in LLM Representations using RePlace

For each LLM prompted to solve RePlace, we assess which types of abstractions of state \(s_{t}\) are preserved in its representation \(h_{t}\) during decoding after the collection of \(\{(s_{t},h_{t})\}\) pairs as described in Section 3.

The modular state structure in RePlace allows us to analytically derive a set of predicates for each type of world state abstraction. To begin with, we introduce a set of abstract predicates crucial for devising the optimal plan for task completion:

* \((o)\): the object \(o\) is located within the container nearby to the agent. Similarly, we have \((l,o)\), where \(l\) is the relative distance to the agent's current location, e.g., \((-1,o)\) being True indicates that the container one step to the left contains \(o\).

* \((l,j)\): the agent needs to visit a series of containers to manipulate objects, and the \(j\)-th container to be visited is at the relative distance \(l\) to the agent's current location.
* \((v)\): the relative direction of the next object to be manipulated, where \(v\) could be left or right to the agent, in a nearby container, or with the agent.
* \((o)\): the object \(o\) to be manipulated at the next step, if applicable.

Appendix E provides a detailed explanation of the definition and derivation of these predicates. Finally, we can infer the optimal action for the next step by incorporating nextObj and nextObjDirect. Figure 3 demonstrates how to derive abstract predicates and actions based on the raw predicates. Figure 4 provides concrete examples of predicates.

Now, we derive each type of world abstraction, using both raw and abstract predicates:

**Raw state** The complete world state \(\) includes store, held, agentLoc in both the current and target situations, \(u\) and \(g\), and boxName.

**World-irrelevant abstraction** To preserve the world dynamics, we have to track all raw predicates of the current situation, excluding boxName, as actions do not affect it. In addition, we include all predicates in target situation \(g\) to preserve the reward function. Therefore, \(_{_{w}}\) includes the predicates store, held and agentLoc in both \(u\) and \(g\).

\(Q^{*}\)**irrelevant abstraction** To construct \(_{_{}}\), we include agentLoc, held\({}_{u}\) and \(_{u}\) to distinguish between the effects of legal and illegal execution of actions. Also, we include subgoal, where \((l,j=1)\) differentiates the effect between optimal and non-optimal actions, and \((l,j>1)\) quantifies the effect of executing each action. Further, we include nextObj to distinguish the effect between optimal and non-optimal put or grab actions.

\(^{*}\)**-irrelevant abstraction**\(_{_{*}}\) is factorized by predicates nextObjDirect and nextObj. nextObjDirect specifies the action type (move or manipulate objects) and moving direction (left or right) at the next step. nextObj identifies the particular \(o\) for grab or put operations.

Appendix G provides a more formal proof.

We probe the predicates in LLM representations by training two-layer neural models, following . For single-variable predicates (e.g., \((o)\)), we train an individual probe for each. For two-variable predicates where one variable denotes a positional index (e.g., \((i,b)\)), we train a separate probe for each \(i\)-th position. For some predicates, e.g., \((i,o)\), involving variables \(o\) with domains that differ across the dataset, the input to probes is a concatenation of \([h_{t},(o)]\), where \((o)\) is the embedding of \(o\). Following the conditional probing principle , we treat the embedding method for each predicate as a hyper-parameter, selecting the method with the largest performance margin between using \([h_{t},(o)]\) and using only \((o)\) as the input. Specifically, we explore two embedding methods: (1) averaging the word embedding for all the tokens in \(o\) and (2) selecting and averaging the hidden states \(H\) across all mentions of \(o\). For other predicates, such as agentLoc and subgoal, probes take as input \(h_{t}\). Appendix H provides more details about the formulation of probing tasks as well as the design (e.g. layer number) and optimization of probes.

**Evaluation metric** We evaluate the performance of probing models for each predicate using the F1-score. To account for variations in candidate numbers across different predicates, we normalize the scores \(x_{p}\) for predicate \(p\) by \(-_{p},0)}{1-_{p}} 100\%\), where \(_{p}\) is the score of a Random baseline. It predicts outcomes by proportional sampling from label candidates of \(p\) based on their frequencies. We refer to this normalized score as the _recovery rate_.

## 6 Experiments

### Target Models for Probing Experiments

We experiment with two groups of LLMs to investigate how fine-tuning and pre-training impact the world abstractions encoded in their representations, respectively. The details follow below.

**Effect of fine-tuning** We adapt Llama2-7b/13b , Mistral-7b , and LLama38b  on both datasets, using in-context learning (icL) and supervised fine-tuning (sft) with LoRA adapters . Appendix F provides more details.

**Effect of pre-training** Since the LLMs above perform near-random without fine-tuning, we also experiment with a more recent state-of-the-art LLM, Phi3-17b , which achieves much higher performance than other pre-trained LLMs on Gripper4. For comparative analysis, we also include Phi3-3.8b, Pythia-70m  and a fine-tuned Phi3-17b.

In addition, we train a 6-layer decoder-only Transformer from scratch, probing for abstractions from its internal representations as a baseline.

Before the probing experiments, we first verify that the fine-tuned LLMs and Phi3-17b\({}_{}\) can achieve reasonable performance on RePlace, and thus, we can draw informative conclusions. We adopt three evaluation metrics for RePlace: legal rate (the proportion of predictions that comply with constraints), success rate (the proportion of predictions that achieve the target situations), and optimal rate (the proportion of cases where the target situation are achieved with the minimum number of actions). The Llama2-13b and Mistral results are reported in Table 1, with comprehensive results for all LLMs provided in Appendix I. As shown in Table 1, both LLMs initially fail in nearly all cases when relying solely on in-context demonstrations. Once fine-tuned, however, they can accomplish the task with reasonable success and optimal rate, and most of the predicted actions adhere to the constraints. Next, we conduct probing experiments on each variant of the LLMs on the same datasets.

Figure 5: Average recovery rate of each world abstraction across different LLMs on Gripper.

Figure 6: Recovery rate of all predicates from different LLMs on Gripper. Predicates are grouped according to the coarsest abstractions they pertain to. Better viewed in color. The color indicates all abstraction levels the predicates are associated with.

### Probing Results and Analysis

We measure probes for all the LLM variants adapted to RePlace (icl or sft) as well as the train-from-scratch Transformer. Figure 5 and Figure 7(a) report the average recovery rate (RR) of predicates within each type of world abstractions from different LLMs. Figure 6 and Figure 7(b) take a closer look at the RR of each predicate from seven LLM variants to analyze the effect of fine-tuning and pre-training. The complete results are reported in Appendix J. Next, we present four key findings from the results.

**Finding 1**.: _Reasonably performing LLMs tend to maintain goal-oriented world abstractions rather than a more general one during decoding._

All LLMs\({}_{}\) and Phi3-17b\({}_{}\), which outperform the random baseline by a substantial margin, primarily preserve \(Q^{*}\)- and \(^{*}\)-irrelevant abstractions. Comparing LLMs\({}_{}\) and LLMs\({}_{}\) in Figure 5 and Figure 7(a), \(Q^{*}\)- and \(^{*}\)-irrelevant abstractions are probed from LLMs\({}_{}\) with drastically higher RR than raw and world-irrelevant abstraction. Focusing on the RR of individual predicates, Figure 6(a) and Figure 7(b) reveal that the predicates for \(^{*}\)-irrelevant abstraction, nextObj and nextObjDirect have been mostly recovered from LLMs\({}_{}\). This result aligns with the high task success rate achieved by these LLMs. Similarly, the predicates for \(Q^{*}\)-irrelevant abstraction are effectively probed from the LLMs\({}_{}\), although with larger variance across different predicates. This suggests that LLM representations do not merely encode the most coarsest world abstraction for deriving the very next action. Instead, they preserve sufficient information to estimate each action's long-term effect with respect to altering the world situation to match the target, facilitating efficient planning. The successful recovery of nearby, held\({}_{u}\) and agentLoc indicates that LLM representations capture the world constraints to inform legal actions, as evidenced by the near-perfect legal rate of LLMs\({}_{}\).

Nevertheless, Figure 5 shows that although the RR margin between goal-oriented abstractions and more general ones is smaller for Phi3-17b\({}_{}\), it remains significant. Figure 6(b) explains this by revealing that the RR for nextObj, nextObjDirect, and subgoal from Phi3-17b\({}_{}\) is much lower than that of LLMs\({}_{}\), which is unsurprising as Phi3-17b\({}_{}\) achieve a much lower success rate on RePlace than LLMs\({}_{}\).

Conversely, more general world abstractions are mostly absent in all LLM representations. Predicates uniquely tied to the raw state and the world-irrelevant abstraction cannot be accurately recovered. The low RR for boxName suggests that LLMs discard world details that are not pertinent to the task's completion. Similarly, the minimal RR on store\({}_{t}\), a key predicate for reconstructing the world model, implies that the information to derive world dynamics is mostly omitted during decoding.

We create two other variants of Gripper to further consolidate this finding. The first one is a counterfactual setting where a predicate originally irrelevant to the goal becomes pertinent to goal-oriented abstraction. Our results show that it is accordingly encoded in the LLM\({}_{}\) representations. Under the other setting, the LLMs are fine-tuned on sub-optimal action sequences, and the results suggest that random exploration does not necessarily improve the encoding of world dynamics. Details of these experiment can be found in Appendix K and Appendix L, respectively.

Figure 7: Recovery rate of predicates within different world abstractions across different LLMs on Cook.

**Finding 2**.: _Supervised fine-tuning and advanced pre-training mainly enhance goal-oriented world abstractions._

Figure 6 and Figure 7(b) demonstrate significant differences in the recoverability of goal-oriented abstractions from representations of LLMs, comparing models with and without supervised fine-tuning. Specifically, the performance gap in terms of predicates nextObjDirect, agentLoc, and subgoal is notable, approximately \(40-70\) recovery rate. In contrast, for boxName and store, the difference is much more negligible, less than \(5\). This disparity implies that when fine-tuned with teacher forcing, LLMs evolve to develop a more goal-oriented world abstraction during decoding. This finding also provides a novel perspective on  that successfully probe the world state in Transformer models: the state information is an integral part of the goal-oriented abstraction in their task, which is enhanced as the models are optimized to solve the task.

Interestingly, advanced pre-training has a similar effect. Figure 5 and Figure 7(a) show that the key distinction between train-from-scratch Transformer and LLMsST lies in their retention of \(Q^{*}\)- and \(^{*}\)-irrelevant abstractions. Nonetheless, as LLMs increase in scale and capability (Pythia<Phi3>3.8b<Phi3>17b), they are more likely to maintain goal-oriented abstractions over a more general one. This is apparent from Figure 6(b), where the RR of predicates does not increase with Phi3>17b unless the predicates pertains to the goal-oriented abstractions. In contrast, the RR of store and heldg, essential for world-irrelevant abstraction, are almost identical across all pre-trained LLMs. It implies that advanced pre-training does not necessarily lead to better encoding of the world dynamics.

**Finding 3**.: _Raw state and world-irrelevant abstractions are mostly suppressed during decoding._

Motivated by our initial two findings, we explore whether and how boxName and store are encoded within the contextualized representation by Llama2\({}^{13b}_{}\). To do so, we use the contextualized embedding \(()\) of the label candidate, comparing the use of only the embedding \((e)\) against a concatenation \([h_{t};(e)]\), inspired by . We also examine nearby for comparative analysis. The results in Table 2 show that the objects' location can be probed from the contextualized representation with near-perfect performance while incorporating \(h_{t}\) may decrease the performance instead. As for more goal-oriented predicates, nearby cannot be probed from the contextualized representation with high accuracy, and the concatenation with \(h_{t}\) boosts the performance vastly, suggesting that LLMs discard the information of store and integrate it with agentLoc to derive nearby.

**Finding 4**.: _LLMs are limited in building world representations, whether general or goal-oriented._

While LLMs tend to preserve a goal-oriented abstraction, they do not totally discard boxName (15-20 RR from LLMsST and even 43 from Phi3-17b), which is irrelevant to either task completion or transition dynamics. Also, the probing performance for some goal-oriented predicates, e.g., subgoal, has ample room to improve. Moreover, the information for world dynamics, e.g., store, is mostly discarded.

**Remark**.: _The interpretation of probing performance for a predicate should depend on the specific type of abstraction it belongs to._

Focusing solely on raw world states might lead to a superficial conclusion that the LLM only partially captures the world. In contrast, our framework offers a more thorough and systematic interpretation, demonstrating that well-performing LLMs maintain a goal-oriented world representation during decoding. _This underlines the core rationale of our probing framework: the necessity of probing different world abstractions rather than just the raw state._

## 7 Conclusion

We propose a new framework to probe the abstract world state in LLM representations through the lens of state abstraction. Experiments with a synthesized task using our framework demonstrate that LLMs tend to preserve a goal-oriented world abstraction instead of a more general one during decoding, abstracting away the world's transition dynamics. Overall, our experiment findings highlight the importance of probing different types of abstraction encoded in LLM representations to draw comprehensive and nuanced conclusions. Nevertheless, our work is not without limitations, which are discussed in Appendix A.

  Predicates & Abstraction Type & \([h_{t};(e)]\) & \((e)\) \\  boxName & Raw & 100.00 & 100.00 \\ store\({}_{}\) & Raw, World-irrelevant & 93.32 & 95.71 \\ store\({}_{}\) & Raw, World-irrelevant & 93.01 & 95.18 \\ nearby\({}_{}\) & \(Q^{*}\)-irrelevant & 92.11 & 42.46 \\  

Table 2: Recovery rate with different encoding methods of label candidates.