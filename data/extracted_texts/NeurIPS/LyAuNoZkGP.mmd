# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

then train sparse linear models on such concepts to do "explainable" classification . However, it is not well understood if or how extracted features are concentrated or spread across the entire representation.

While the length of the feature vectors extracted from state-of-the-art networks 2 can vary greatly, their accuracy on downstream tasks are not correlated to the size of the representation (see Table 1), but rather depend mostly on the inductive biases and training recipes . In all cases, the size of extracted feature vector (_i.e._ number of neurons) is orders of magnitude less than the dimensions of the input and thus allows for efficient transfer to many downstream tasks . We show that even using a _random_ subset of these extracted neurons is enough to achieve downstream transfer accuracy close to that achieved by the full layer, thus showing that learned representations exhibit a degree of _diffused redundancy_ (Table 1).

Early works in perception suggest that there are many redundant neurons in the human visual cortex  and some later works argued that a similar redundancy in artificial neural networks should help in faster convergence . In this paper, we revisit redundancy in the context of modern DNN architectures that are trained on large-scale datasets. In particular, we propose the **diffused redundancy** hypothesis and systematically measure its prevalence across different pre-training datasets, losses, model architectures, and downstream tasks. We also show how this kind of redundancy can be exploited to obtain desirable properties such as generalization performance but at the same time draw caution to certain drawbacks of such an approach in increasing disparity in inter-class performance. We highlight the following contributions:

* We present the diffused redundancy hypothesis which states that learned representations exhibit redundancy that is diffused throughout the layer, _i.e._, many _random subsets_ (of sufficient size) of neurons can perform as well as the entire layer. Our work aims to better understand the nature of representations learned by DNNs.
* We present an initial analysis of why diffused redundancy exists in deep neural networks, we show that a randomly chosen subset of neurons of size \(k\) performs as well as the projection of the entire layer on the first \(k\) principal components. Intuitively this means that in DNNs' representations, _many_ random subsets of size \(k\) roughly capture all the variation in data that is possible with \(k\) dimensions (since PCA represents directions of maximum variance).
* We propose a measure of diffused redundancy and systematically test our hypothesis across various architectures, pre-training datasets & losses, and downstream tasks.
* We find that diffused redundancy is significantly impacted by pre-training datasets & loss and downstream datasets.
* We find that models that are explicitly trained such that particular parts of the full representation perform as well as the full layer, _i.e._, these models have _structured redundancy_ (_e.g._), also exhibit a significant amount of diffused redundancy. Further, we also evaluate models trained with regularization that decorrelates activation of neurons and again find that these regularizations surprisingly _do not_ affect diffused redundancy. These results suggest that this phenomenon is perhaps inevitable when DNNs have a wide enough final layer.

    & Feature & ImageNet1k &  \\   & Length & Top-1 Accuracy & CIFAR10 & CIFAR100 & Flowers & Oxford-IIIT-Pets \\  ViT S-16 & \(384\) & \(64.82\%\) & 0.70 & 0.50 & 0.50 & 0.80 \\ ViT S-32 & \(384\) & \(55.73\%\) & 0.70 & 0.50 & 0.50 & 0.70 \\ ResNet18 & \(512\) & \(69.23\%\) & 0.80 & 0.50 & 0.50 & 0.90 \\ ResNet50 & \(2048\) & \(80.07\%\) & 0.90 & 0.50 & 0.20 & 0.90 \\ WRN50-2 & \(2048\) & \(77.00\%\) & 0.95 & 0.80 & 0.50 & 0.95 \\ VGG16 & \(4096\) & \(73.36\%\) & 0.95 & 0.80 & 0.80 & 0.95 \\   

Table 1: Different model architectures with varying penultimate layer lengths trained on ImageNet1k. WRN50-2 stands for WideResNet50-2. Implementation of architectures is taken from timm. Diffused redundancy here measures what fractions of neurons (randomly picked) can be discarded to achieve within \(=90\%\) performance of the full layer.

* We quantify the degree of diffused redundancy as a function of the number of neurons in a given layer. As we reduce the dimension of the extracted feature vector and re-train the model, the degree of diffused redundancy decreases significantly, implying that diffused redundancy only appears when the layer is wide enough to accommodate redundancy.
* Finally we draw caution to some potential undesirable side-effects of exploiting diffused redundancy for efficient transfer learning that have implications for fairness.

### Related Work

Closest to our work is that of Dalvi et al.  who also investigate neuron redundancy but in the context of pre-trained language models. They analyze two language models and find that they can achieve good downstream performance with a significantly smaller subset of neurons. However, there are two key differences to our work. First, their analysis of neuron redundancy uses neurons from all layers (by concatenating each layer), whereas we show that such redundancy exists even at the level of a single (penultimate) layer. Second, and perhaps more importantly, they use feature selection to choose the subset of neurons, whereas we show that features are diffused throughout and that even a _random_ pick of neurons suffices. Our work also differs by analyzing vision models (instead of language models) and using a diverse set of 30 pre-trained models (as opposed to testing only two models) which allows us to better understand the causes of such redundancy.

**Efficient Representation Learning** These works aim to learn representations which are "slim", with the goal of efficient deployment on edge devices [65; 64; 5]. Recently proposed paradigm of _Matryoshka Representation Learning_ aims to learn nested representations where one can perform downstream tasks with only a small portion of the representation. The goal of such representations is to allow quick, adaptive deployment without having to perform multiple, often expensive, forward passes. These works could be seen as inducing _structured redundancy_ on the learned representations, where pre-specified parts of the representation are made to perform similar to the full representation. Our work, instead, aims to look at _diffused redundancy_ that arises naturally in the training of DNNs. We carefully highlight the tradeoffs involved in exploiting this redundancy.

**Pruning and Compression** Many prior works focus on pruning weights [29; 13; 11; 15; 31; 9; 30] and how it can lead to sparse neural networks with many weights turned off. Our focus, however, is on understanding redundancy at the neuron level, without changing the weights. Work on structured pruning is more closely related to our work [33; 17], however, a key focus of these works is to prune channels/filters from convolution layers. Our work is more focused on understanding the nature of learned features and is more broadly applicable to all kinds of layers and models. We additionally focus on randomly pruning neurons, whereas structured pruning methods perform magnitude or feature-selection-based pruning.

**Explainability/Interpretability** Many works aim to understand learned representations with the goal of better explainability [35; 63; 2; 20; 40; 39; 10; 67]. Two works in this space are especially related to our work: sparse linear layers  which show that one can train sparse linear layers on top of extracted features from DNNs; and concept bottleneck models  which explicitly introduce a layer in which each neuron corresponds to a meaningful semantic concept. Both these works explicitly optimize for small/sparse layers, whereas our work shows that similar "small" layers already exist in pre-trained networks, and in fact, can be found simply with random sampling.

**Understanding Deep Learning** A related concept is that of instrinsic dimensionality of DNN landscapes . Similar to our work, intrinsic dimensionality also requires dropping random parameters (weights) of the network. We, however, are concerned with dropping individual neurons. Other works on understanding deep learning [49; 1] have also looked at the learned features, however, none of these works analyze the redundancy at the neuron level. Another related phenomenon to diffused redundancy is that of neural collapse , which states that representations of the penultimate layer "collapse" to \(K\) points (where \(K=\) number of classes in the pre-training dataset). This implies that for perfectly collapsed representations we need to store just enough information in the final layer activations to be able to represent \(K\) different points. We interestingly show that this information is spread throughout the layer with a significant degree of redundancy.

## 2 The Diffused Redundancy Phenomenon

Prior observations about a _compression_ phase  and neural collapse  suggest that the representations need not store a lot of information about the input. These findings imply that all neurons in a learned representation might not be necessary to capture all the information in a particular layer. Extending these observations, we propose the _diffused redundancy_ hypothesis:

_Learned features are diffused throughout a given layer with redundancy such that there exist many randomly chosen subsets of neurons that can achieve similar performance to the whole layer for a variety of downstream tasks._

Note that our hypothesis has two related but distinct parts to it: 1) redundancy in learned features, and 2) diffusion of this redundancy throughout the extracted feature vector. _Redundancy_ refers to features being replicated in parts of the representation so that one can perform downstream tasks with parts of representation as well as with the full representation. _Diffusion_ refers to this redundancy being spread all over the feature vector (as opposed to being structured), _i.e._, _many_ random subsets (of sufficient size) of the feature vector perform equally well.

In order to evaluate the _redundancy_ part of the diffused redundancy hypothesis we use two tasks: 1) representation similarity between randomly chosen subsets of a representation with the whole representation, and 2) transfer accuracy on out-of-distribution datasets (using a linear probe) of randomly chosen subsets of the representation compared to the whole representation. To estimate _diffusion_, we run each check for redundancy over multiple random seeds and plot the standard deviation over these runs.

**Representation Similarity of Part vs Whole** Centered Kernel Alignment (CKA) is a widely used representation similarity measure and takes in two representations of \(n\) data points \(Z^{n d_{1}}\) and \(Y^{n d_{2}}\) and gives a similarity score between 0 and 1 . Intuitively, CKA (with linear kernel, see Appendix A for details about CKA) measures if the two representations rank the \(n\) points similarly (where similarity is based on cosine distances). For a given neural network \(g\) and \(n\) samples drawn from a given data distribution, _i.e._, \(X\), let \(g(X)\) be the (penultimate) layer representation. If \(m\) is a boolean vector representing a subset of neurons in \(g(X)\), then we aim to measure CKA(\(m g(X),g(X)\)) to estimate how much redundancy exists in the layer. If indeed CKA(\(m g(X),g(X)\)) is high (_i.e._ close to 1) then it's a strong indication that the diffused redundancy hypothesis holds.

**Downstream Transfer Performance of Part vs Whole** A commonly used paradigm to measure the quality of learned representations is to measure their performance on a variety of downstream tasks [68; 22]. Here, we attach a linear layer (\(h\)) on top of the extracted features of a network (\(g\)) to

Figure 1: **[Testing For Diffused Redundancy in ResNet50 Pre-trained on ImageNet1k]** Top: transfer accuracies + Diffused Redundancy (\(DR\)) measure (Eq 1) on different downstream datasets, dotted horizontal line shows accuracy obtained using the full layer. We see that accuracy obtained using parts of representation varies greatly with pre-training loss (much more diffused redundancy in Adversarially Trained (AT) ResNet), but also depends on the downstream dataset. Bottom: comparing CKA between a randomly chosen fraction of neurons to the whole layer. Here we evaluate CKA on samples from different datasets and find that similarity of a subset of layer rapidly increases, reaching a similarity of greater than \(90\%\) on the adversarially trained ResNet with only \(10\%\) randomly chosen neurons. Values are averaged over 5 random picks and error bars show std. dev.

do classification. This layer is then trained using the training dataset of the particular task (keeping \(g\) frozen). If features were to be diffused redundantly then accuracy obtained using \(h g\), _i.e._linear layer attached to the entire feature vector, should be roughly the same as \(h^{}(m g)\); where \(m\) is a boolean vector representing a subset of neurons extracted by \(g\), and \(h\) & \(h^{}\) are independently trained linear probes.

For both tasks, _i.e._ representation similarity and downstream transfer performance, we evaluate on CIFAR10/100 , Oxford-IIIT-Pets  and Flowers  datasets, from the VTAB benchmark . Training and pre-processing details are included in Appendix B.

**Measure of Diffused Redundancy** In order to rigorously test our hypothesis, we define a measure of diffused redundancy (\(DR\)) for a given model (\(g\)) with \(\) being a set of all possible boolean vectors of size \(|g|\), _i.e._ size of the representation extracted from \(g\). Each vector \(m\) represents a possible subset of neurons from the entire layer. This measure is defined on a particular task (\(T\)) as follows:

\[DR(g,T,)=1-f\,\ _{f}|} _{m_{f}}}{|g|},\] (1)

\[_{f}=m|_{i}m_{i}=f\ ;\ m\{0,1\}^{|g|} }\]

Here \(T(.)\) denotes the performance of the model inside \(()\) for the particular task and \(\) is a user-defined tolerance level. For the task of representation similarity \(T(m g)\) is CKA between a subset of neurons denoted by \(m g\) and \(g\), and \(T(g)\) is always 1, since it denotes CKA between \(g\) and \(g\). For downstream transfer performance, \(T(m g)\) is the test accuracy obtained by training a linear probe on the portion of representation denoted by \(m g\) and \(T(g)\) is the test accuracy obtained using the full representation. For \(=1\), this measure tells what fraction of neurons could be discarded to exactly match the performance of the entire set of neurons. A higher value of \(DR\) denotes that only a few random neurons were needed to match the task performance of the full set of neurons, and thus indicates higher redundancy. Since \(\) contains an exponential number of vectors (\(2^{|g|}\)), precisely estimating this quantity is hard. Thus, we first choose a few \(f\) (number of neurons to be chosen) to define subsets of \(\). Then for each \(_{f}\) we randomly select 5 samples.

### Prevalence of Diffused Redundancy in Pre-Trained Models

Figure 1 checks for diffused redundancy in the penultimate layer representation of two types of ResNet50 pre-trained on ImageNet1k: one using the standard cross-entropy loss and another trained using adversarial training  (with \(_{2}\) threat model and \(=3\)) 3. We check for diffused redundancy using both tasks of representation similarity and downstream transfer performance.

**Redundancy** This is indicated along the x-axis of Fig 1, _i.e._, redundancy is shown when some small subset of the full set of neurons can achieve almost as good performance as the full set of neurons. When looking at downstream task performance (Figs 0(a)&0(c)), in order to obtain performance within some \(\%\) of the full layer accuracy (dotted lines), the fraction of neurons that can be discarded are task-dependent, _e.g._ across both training types we see that flowers (102 classes) and CIFAR100 (100 classes) require more fraction of neurons than CIFAR10 (10 classes) and oxford-iiit-pets (37 classes), perhaps because both these tasks have more classes. Additionally, across all datasets, the model trained with adversarial training exhibits more diffused redundancy than the one trained with standard loss (Fig 0(d)&0(b) respectively), meaning we can discard far more neurons for the adversarially trained model to reach close to the full layer accuracy. Interestingly when looking at CKA between part of the feature vector with the full extracted vector (Figs 0(e)&0(f)), we do not see a significant difference in trends when evaluating CKA on samples from different datasets. However, we still see that we can achieve a given level of CKA with far fewer fraction of neurons in the adversarially trained ResNet50 as compared to the usually trained ResNet50.

_Diffused_** Redundancy** This is indicated by small error bars in Figs 0(a)&0(c)&0(e)0(f). If redundancy were instead very structured, then different random picks of neurons would have high variance,however, the error bars here are very low, showing that performance is very stable across different random picks, thus indicating that redundancy is diffused throughout the layer.

While both tasks of downstream transfer and CKA between part and whole indicate higher diffused redundancy for the adversarially trained model, we see that downstream transfer performance can differ substantially based on the dataset (while CKA remains fairly stable across the same datasets), indicating that downstream performance turns out to be a "harder" test for diffused redundancy. Thus, in the rest of the paper, we examine diffused redundancy through the lens of downstream transfer performance and include CKA results in Appendix A.

### Understanding Why _Many_ Random Subsets Work

Many prior works explicitly train models to have "small" representations (_e.g._[26; 65; 64; 5] with the goal of efficient downstream learning. These works show that, when explicitly optimized, networks can perform downstream classification with fewer neurons than typically used in state-of-the-art architectures. We show, however, that such subsets already exist in models that are _not_ explicitly trained for this goal, and in fact, one doesn't even have to try hard to find this subset; it can be _randomly_ chosen. Later in section 3.3 we compare some of these efficient representation learning methods to randomly chosen subsets and carefully analyze the tradeoffs involved. Here, however, we seek to better understand why there exist so many randomly selected subsets that work just as well as the whole layer.

**High CKA between two random subsets of the same size.** First, we use representation similarity (CKA)  to get a sense of how two random picks of neurons (of the same size) relate to each other. Concretely, we calculate CKA between two random picks of \(k\%\) neurons in the penultimate layer (averaged over 10 such randomly picked pairs) on samples taken from different datasets. Fig 1(a& 1(b) show CKA results averaged over these different picks of pairs of subsets of the full set of neurons. We see that after picking a certain threshold, _i.e._ for a large enough value of \(k\), the similarity between any two randomly picked pairs of heads is fairly high. For example, for the adversarially trained ResNet50 (Fig 1(b)), we observe that any \(10\%\) of neurons picked from the penultimate layer are highly similar (CKA of about \(0.8\)), with very low error bars. A similar value of CKA is obtained with \(20\%\) of neurons for the standard ResNet50 model. These results indicate that, given a sufficient size, picking any random subset of that size has very similar representations and thus provides an initial intuition for why almost _any_ random subset, with high probability, could work equally well.

Figure 3: **[Random Projection (dotted) vs Randomly Choosing Neurons (solid)] We find that diffused redundancy (randomly choosing neurons) performs significantly better than random projections. This indicates that the “constrained” projection offered by diffused redundancy is crucial in achieving good downstream performance.**

Figure 2: **[Why Many Random Subsets Work] (2a& 2b) Similarity between any two randomly picked sets of \(k\%\) neurons becomes fairly high (for a “critical mass” of \(k\%\)), thus showing that any random pick beyond this threshold is likely to perform similarly; (2c& 2d) Performance of a linear probe trained for a downstream task on randomly chosen neurons (solid lines) closely follows that of a linear probe trained on a projection on top \(k\) principal components (dotted lines) for a sufficiently large value of \(k\).**

**Downstream performance on \(k\) randomly chosen neurons closely follows performance on top \(k\) principal components.** While high representation similarity is a necessary condition for two representations to have similar downstream performance, it's not a sufficient one. As seen earlier in Fig 1, similar values of CKA can still show different downstream accuracy. Thus, to further understand why any random pick of neurons achieves similar accuracy, we compare a randomly chosen subset of neurons with a projection on the same number of top \(k\) principal components. Fig 1(c)& 1(d) shows that the performance of a linear probe trained on a random subset of neurons initially lags behind the top PCA dimensions for small values of \(k(<20\%)\), but for higher values (\(>20\%\)) closely follows the performance on a probe trained on the projection on the same number of top principal components. This indicates that a sufficiently sized random subset of size \(k\) (roughly) captures the maximum variance in data that can be captured in \(k\) dimensions (upper bound by top \(k\) principal components since they are designed to capture maximum variance). As a sanity check, we also show the results for the bottom \(k\) principal components (directions of least variance) and see that randomly chosen neurons perform significantly better.

**Random projections into lower dimensions do not perform well on downstream tasks.** The presence of diffused redundancy in a particular layer indicates that a dimension lower than the layer's size suffices to perform many downstream tasks. However, randomly sampling neurons (as we do in our experiments on diffused redundancy) can be seen as one very particular way of reducing dimension that is restricted by what the network has learned. This raises a natural question, would our observation extend to less restricted ways of reducing dimensions? To test this, we compare diffused redundancy _i.e._, a linear probe trained on a random sample of \(k\) neurons to a linear probe trained on a random projection of the layer's activations. To project a \(d\) dimensional layer into a lower dimension (\(k\)) we multiply it by a (normalized) randomly sampled matrix from a normal distribution \(^{d k}\). We report our results over 5 seeds in Fig 3 and find that diffused redundancy significantly outperforms random projection.

## 3 Factors Influencing The Degree of Diffused Redundancy

In order to better understand the phenomenon of diffused redundancy we analyze 30 different pre-trained models, with different architectures, pre-training datasets and losses. We then evaluate each model for transfer accuracy on 4 datasets mentioned in Section 2. While all results in this section are on the penultimate layer, we also report results on intermediate layers in Appendix E and find similar trends of diffused redundancy. All details for reproducibility can be found in Appendix B.

**Architectures** We consider VGG16 , ResNet18, ResNet50, WideResNet50-2 , ViT-S16 & ViT-S32 . Additionally, we consider ResNet50 with varying widths of the final layer (denoted by ResNet50_ffx where x denotes the number of neurons in the final layer).

**Upstream Datasets** ImageNet-1k & ImageNet-21k .

**Upstream Losses** Standard cross-entropy, adversarial training (\(_{2}\) threat model, \(=3\) and \(_{}\) threat model, \(=4/255\)4) , MRL Loss , DeCov , and varying strengths of dropout .

**Downstream Datasets** CIFAR10/1000, Oxford-IIIT-Pets, and Flowers, same as Section 2. Additionally, we also report performance on harder datasets such as ImageNetv2  and Places365 

Figure 4: **[Comparisons Across Architectures For Downstream Task Accuracy] All models shown here are pre-trained on ImageNet1k. We see that diffused redundancy exists across architectures, and the trend observed in Figure 0(c)&1 regarding adversarially trained models also holds here as models’ curves that are more “inside” are the ones trained with standard loss.**in Appendix F. For all analyses in this section, we also report corresponding approximations for \(DR\) (Eq 1) in Appendix D.

### Effects of Architecture, Upstream Loss, Upstream Datasets, and Downstream Datasets

Extending the analysis in Section 2, we evaluate the diffused redundancy hypothesis on other architectures. Fig 4 shows transfer performance for different architectures. All architectures shown in Fig 4 are trained on ImageNet1k. We find that our takeaways from Section 2 also extend to other architectures.

Fig 5 compares two instances each of ViT-S16 and ViT-S32, one trained on a bigger upstream dataset (ImageNet21k) and another on a smaller dataset (ImageNet1k)

Note that the nature of all curves in both Figs 4&5 highly depends on downstream datasets. This is also consistent with the initial observation of Section 2 about diffused redundancy being downstream dataset dependent. Additionally, we also report results on ImageNetV2 and Places365 in Appendix F showing that diffused redundancy also holds on "harder" datasets.

### Diffused Redundancy as a Function of Layer Width

We take the usual ResNet50 with a penultimate layer consisting of 2048 neurons and compare it with variants that are pre-trained with a much smaller penultimate layer, these are denoted by ResNet50_ffx where x (\(<2048\)) is the number of neurons in the penultimate layer. Fig 6 shows how diffused redundancy slowly fades away as we squeeze the layer to be smaller. In fact, for ResNet50_ff8, we see that across all datasets we need \(>90\%\) of the full layer to achieve performance close to the full layer. This shows that diffused redundancy only appears in DNNs when the layer is sufficiently wide to encode redundancy.

### Comparison With Methods That Optimize For Lesser Neurons

Matryoshka Representation Learning (MRL) is a recently proposed paradigm that learns nested representations such that the first \(k,2k,4k,...,N\) (where \(N\) = size of the full layer) dimensions of the extracted feature vector are all explicitly made to be good at minimizing upstream loss, with

Figure 5: **[Comparison Across Upstream Datasets] We see that degree of diffused redundancy depends a great deal on the upstream training dataset, in particular, models trained on ImageNet21k exhibit a higher degree of diffused redundancy, although the differences in the degree of diffused redundancy are downstream task dependent**

Figure 6: **[Diffused Redundancy as Function of Layer Width] As we make the length of the layer smaller, the degree of redundancy becomes lesser. For ResNet50_ff8, _i.e._ResNet50 with only 8 neurons in the final layer, we see that we need almost \(90\%\) of neurons to achieve similar accuracy as the full layer.**

the intuition of learning coarse-to-fine representations. This ensures that one can flexibly use these smaller parts of the representation for downstream tasks. MRL, thus, ensures that redundancy shows up in learned representations in a _structured_ way, _i.e._, we know the first \(k,2k,...\) neurons can be picked and used for downstream tasks and should perform reasonably.

Here we investigate two questions regarding Matryoshka representations: 1) do these representations also exhibit the phenomenon of diffused redundancy? _i.e._ if we were to ignore the structure imposed by MRL-type training and instead just pick random neurons from all over the layer, do we still get reasonable performance?, and 2) how do they compare to representations learned by other kinds of losses?

Figure 7 investigates these questions by comparing ResNet50 representations learned using MRL loss to other losses. resnet50_mrl_nonrob_first (red line) denotes a ResNet50 trained using MRL loss and evaluated on parts of the representation that were optimized to have low upstream loss (_i.e._ first \(k,2k,...N\) neurons, here \(k=8\) and \(N=2048\)) and resnet50_mrl_nonrob_random (green line) refers to the same model with the same number of neurons chosen for evaluation, except they're chosen at random from the entire layer.

First, we interestingly see that even the ResNet50 trained with MRL loss exhibits diffused redundancy (denoted by green line spiking very quickly for most datasets in Fig 7), despite having been trained to only have structured redundancy. Based on this observation, we conjecture that diffused redundancy is a natural consequence of having a wide layer. Second, we see that ResNet50 trained on MRL indeed does better in the low neuron regime across datasets (red line on the extreme left part of the plots in Fig 7), but other models quickly catch up as we pick more neurons, thus indicating that major efficiency benefits of MRL-type models are best realized when using an extremely low number of neurons, else one can obtain similar downstream performances by simply picking random samples from existing pre-trained models.

### Methods That Prevent Co-adaptation of Neurons Also Exhibit Diffused Redundancy

We consider two additional modifications to upstream pre-training: we add regularization to the usual cross-entropy loss that decorrelates neurons in the feature representation using DeCov  and Dropout . Dropout does this implicitly by randomly dropping neurons during training and DeCov does this explicitly by putting a loss on the activations of a given layer. We pre-train different ResNet50 on ImageNet1k by varying strengths of dropout ranging from 0.1 all the way up to 0.8 and also by separately adding the DeCov regularizer (regularization strength = \(1e-4\)) to give 9 additional pre-trained models. Intuitively these methods should lead to _increased_ diffused redundancy since by design these methods force the model to disperse similar information in different parts of a layer to perform the same task downstream task. Interestingly, we see that diffused redundancy in these models is almost completely independent of the regularization strength. This suggests that diffused redundancy might be an inevitable property of DNNs when the layer has sufficient width. Due to space constraints, we include these results in Appendix G.

## 4 Possible Fairness-Efficiency Tradeoffs in Efficient Downstream Transfer

One natural use case for diffused redundancy is efficient transfer to downstream datasets. As defined in Eq 1 and as also seen in Sections 2& 3, dropping neurons comes at a small cost (\(\) in Eq 1) in

Figure 7: **Comparison of Diffused Redundancy in MRL vs other losses] Here we compare ResNet50 trained using multiple losses including MRL . nonrob indicates that the model was trained with the standard crossentropy loss while robust12eps3 indicates adversarial training with \(_{2}\) threat model and \(=3\). Red line shows results for part of the representation explicitly optimized in MRL, whereas the green line shows results for parts that are picked randomly from the same representation. Even the MRL model shows a significant amount of diffused redundancy despite being explicitly trained to instead have structured redundancy.**

performance as compared to the full set of neurons. Here we take a deeper look into this drop in overall performance and investigate how it is distributed across classes. If the drop affects only a few classes, then dropping neurons - although efficient for downstream tasks - could have implications for fairness, which is not only of concern to ML researchers and practitioners [66; 14; 18], but also to lawyers  and policymakers .

We compare the spread of accuracies across classes using inequality indices, which are commonly used in economics to study income inequality [8; 47] and have also recently been adopted in the fair ML literature . We use gini index  and coefficient of variation  to quantify the spread of performance across classes. For a perfect spread, both gini and coefficient of variation are 0, and higher values indicate higher inequality.

Figure 8 compares the gini index for various models at varying levels of accuracy (note that accuracy monotonically increases with more neurons, hence the right most point for a model represents the model with all neurons). We make two observations: across all datasets and all models we find that a loss in accuracy (compared to the full layer) comes at the cost of a few classes, as opposed to being smeared throughout classes, as indicated by high gini values on the left of each plot. Additionally, we observe that the model trained using MRL loss tends to have slightly higher gini values in the regions where the drop in accuracy is slightly higher (highlighted on the plots). To ensure that this trend is not simply due to lower accuracy, we investigate the error distributions across classes (Appendix C) and find that predictions become more homogeneous as we drop more neurons. Similar trends are also observed with coeff. of variation as shown in Appendix C. These results draw caution to the potential unintended side-effects of exploiting diffused redundancy and suggest that there could be a possible fairness-efficiency tradeoff involved.

**Connections to Robustness-Fairness Tradeoffs** There are well-established tradeoffs between robustness and fairness in both standard  and adversarial training [62; 58]. One major difference between these works and our setup is that the task they train on is also the task they test on. We instead operate in the transfer learning setup where we train a linear probe on the representation of a pre-trained model. While it's not immediately clear whether the discrepancy between class accuracies observed on the pre-training task (_e.g._ as in ) also leads to an inter-class accuracy discrepancy on downstream tasks when dropping neurons, it would be very interesting future work to establish more formal connections between the robustness-fairness tradeoff and its effects on the fairness-efficiency tradeoff presented in this paper.

## 5 Conclusion and Broader Impacts

We introduce the diffused redundancy hypothesis and analyze a wide range of models with different upstream training datasets, losses, and architectures. We carefully analyze the causes of such redundancy and find that upstream training (both loss and datasets) plays a crucial role and that this redundancy also depends on the downstream dataset. One direct practical consequence of our observation is increased efficiency for downstream training times which can have many positive impacts in terms of reduced energy costs  which is crucial in moving towards "green" AI . We, however, also draw caution to potential pitfalls of such efficiency gains, which might hurt the accuracy of certain classes more than others, thus having direct consequences for fairness.

Figure 8: **[Gini Coefficient of Class-Wise Accuracies as we Drop Neurons] Higher value of Gini coefficient indicates higher inequality . We see that for all models gini coefficients become higher as the accuracy reduces (as a result of dropping neurons). Additionally, in some regions (highlighted in the plots), the model explicitly optimized for efficient transfer (resnet50_nrl) can give rise to higher gini values, resulting in a more unequal spread of accuracy over classes.**