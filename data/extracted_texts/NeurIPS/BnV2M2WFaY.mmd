# Noise-Adaptive Thompson Sampling for Linear Contextual Bandits

Ruitu Xu

Department of Statistics and Data Science

Yale University

New Haven, CT 06511

ruitu.xu@yale.edu

&Yifei Min

Department of Statistics and Data Science

Yale University

New Haven, CT 06511

yifei.min@yale.edu

&Tianhao Wang

Department of Statistics and Data Science

Yale University

New Haven, CT 06511

tianhao.wang@yale.edu

###### Abstract

Linear contextual bandits represent a fundamental class of models with numerous real-world applications, and it is critical to developing algorithms that can effectively manage noise with unknown variance, ensuring provable guarantees for both worst-case constant-variance noise and deterministic reward scenarios. In this paper, we study linear contextual bandits with heteroscedastic noise and propose the first noise-adaptive Thompson sampling-style algorithm that achieves a variance-dependent regret upper bound of \(}d^{3/2}+d^{3/2}^{T}_{t}^{2} }\), where \(d\) is the dimension of the context vectors and \(_{t}^{2}\) is the variance of the reward in round \(t\). This recovers the existing \(}(d^{3/2})\) regret guarantee in the constant-variance regime and further improves to \(}(d^{3/2})\) in the deterministic regime, thus achieving a smooth interpolation in between. Our approach utilizes a stratified sampling procedure to overcome the too-conservative optimism in the linear Thompson sampling algorithm for linear contextual bandits.

## 1 Introduction

Linear contextual bandits represent a natural extension of multi-armed bandit problems (Auer et al., 2002a; Robbins, 1952; Lai et al., 1985), where the reward of each arm is assumed to be a linear function of the contextual information associated with the arm. Such problems manifest in numerous real-world applications, encompassing online advertising (Wu et al., 2016), recommendation systems (Deshpande and Montanari, 2012), and personalized medicine (Varatharajah and Berry, 2022; Lu et al., 2021). A multitude of algorithms have been proposed, tailored to diverse settings within the domain of linear contextual bandits (Auer et al., 2002b; Abe et al., 2003). Notably, two main streams of approaches to address the exploration-exploitation dilemma in linear contextual bandits have emerged: Upper Confidence Bound (UCB) (Chu et al., 2011; Abbasi-Yadkori et al., 2011) and Thompson sampling (TS) (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017). In practical scenarios, the varying and non-transparent noise variance inherent in each reward is a common phenomenon (Towse et al., 2015; Omari et al., 2018; Somu et al., 2018; Cheng and Kleijnen, 1999). Such noise, which may exhibit heteroscedasticity and correlation with the context, can significantly impact algorithm performance, particularly when the variance of the noise is unknown a priori (Kirschnerand Krause, 2018; Zhao et al., 2022). Thus, it is imperative to develop adaptive algorithms capable of handling noise with unknown heteroscedastic variance, ensuring provable guarantees when operating with both constant-variance noise and deterministic reward scenarios. Recently, a few variance-aware UCB algorithms have been proposed for linear bandits. In particular, Zhou et al. (2021) examined the scenario where the variance is known and observed after each arm pull, while Zhang et al. (2021); Kim et al. (2022); Zhao et al. (2023) explored the unknown variance case. Nevertheless, there is a scarcity of results concerning TS algorithms in this context.

Thompson Sampling (Thompson, 1933), a notable Bayesian approach, provides a computationally efficient means of addressing the exploration-exploitation trade-off (Rusmevichientong and Tsitsiklis, 2010; Chapelle and Li, 2011). This method applies posterior sampling to generate a parameter estimator for arm selection, thereby naturally balancing exploration and exploitation by selecting arms with high expected rewards and those with substantial uncertainty (Russo et al., 2018; Riquelme et al., 2018). In the existing literature, the regret upper bound for the linear TS algorithm for linear contextual bandits is of order \(}(d^{3/2})\)(Agrawal and Goyal, 2013; Abeille and Lazaric, 2017), and the existing algorithm cannot adapt to higher-order structure of the noise.

As an initial endeavor to incorporate variance information into TS-style algorithmic design for linear bandits, we adopt a weighted ridge regression approach to construct a TS algorithm, LinVDTS, detailed in Algorithm 2. This algorithm achieves a variance-dependent performance guarantee on the expected regret, as demonstrated in Theorem 4.1. Nonetheless, a technical barrier intrinsic to TS-style algorithms, yet absent in UCB, emerges during this naive integration of weighted regression with TS. Specifically, the issue stems from inadequate control over the variance-adjusted norm of unselected context vectors at each step, necessitating an anti-concentration argument that only allows for a variance-dependent upper bound on the expected regret and precludes the acquisition of a high-probability guarantee that is simultaneously variance-dependent. More discussion on this will be given in Section 4.2.

In a bid to navigate this intricate issue and exert explicit uncertainty control, we further propose a novel noise-adaptive TS variant in Algorithm 3. Our algorithmic design features (1) an uncertainty stratification scheme applied to contextual vectors, providing explicit regulation of the uncertainty quantification, (2) a cascading construction process for the feasible set, which, in tandem with the stratification scheme, operates to filter out unlikely arms, thereby eliminating the requirement for the anti-concentration argument in the analysis, and (3) the use of noise-adaptive confidence radii to direct the sampling procedure, thereby ensuring a more constricted variance-dependent regret. The theoretical analysis yields a high-probability regret bound that interpolates smoothly between the constant-variance and deterministic reward regimes.

Main contributions.The main contributions of our paper are summarized in three key aspects.

* We introduce a simple and efficient TS-style algorithm, named LinVDTS (Algorithm 2), that can utilize variance information for linear contextual bandits. We prove that the _expected_ regret of LinVDTS is bounded by \(}(d^{3/2}(1+^{T}_{ t}^{2}}))\), where \(d\) is the ambient dimension and \(_{t}^{2}\) is the variance of the noise in round \(t\). We also prove that, _with high probability_, the regret of LinVDTS is bounded by \(}(d^{3/2}(1+^{T}_ {t}^{2}})+)\). In the analysis of LinVDTS, we identify the inherent difficulty of controlling the uncertainty induced by posterior sampling, which precludes a high-probability guarantee that is aware of the noise variance.
* We devise a stratified sampling procedure that fulfills more efficient exploration for linear contextual bandits by exerting explicit uncertainty control over the context vectors. Based on this framework, we propose LinNATS (Algorithm 3), a noise-adaptive variant of linear TS. To the best of our knowledge, this is the first noise-adaptive TS algorithm for linear contextual bandits with heteroscedastic noise.
* We prove that LinNATS enjoys a high-probability regret bound of order \(}(d^{3/2}(1+^{T}_ {t}^{2}}))\), under standard assumptions for linear contextual bandits with heteroscedastic noise. This improves over the existing \(}(d^{3/2})\) regret bound for linear Thompson sampling (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017), especially when the noise variance diminishes. Our analysis bypasses the standard anti-concentration argument thanks to the stratification framework.

Notation.For any \(n^{+}\), we denote \([n]:=\{1,2,,n\}\), and \(x_{1:n}\) is a shorthand for the set \(\{x_{1},,x_{n}\}\). We write scalars in normal font while representing vectors and matrices in bold font. For any vector \(\) and positive semi-definite matrix \(\), we define \(\|\|_{}=\|^{1/2}\|_{2}\). For two functions \(f\) and \(g\) defined on \(^{+}\), we use \(f(n) g(n)\) to indicate that there exists a universal constant \(C>0\) such that \(f(n) C g(n)\) for all \(n\); \(f(n) g(n)\) is defined analogously. We use \(f(n) g(n)\) to imply that both \(f(n) g(n)\) and \(f(n) g(n)\) hold true. We use \(()\) to hide constant factors and \(}()\) to further hide poly-logarithmic terms.

## 2 Related work

Heteroscedastic linear bandits.The challenge of heteroscedastic noise in linear bandits has been studied through a multitude of perspectives: In particular, Kirschner and Krause (2018) studied linear bandits with heteroscedastic noise via the approach of information-directed sampling. Dai et al. (2022) examined heteroscedastic sparse linear bandits and demonstrated a versatile framework capable of transforming any heteroscedastic linear bandit algorithm into an algorithm tailored for heteroscedastic sparse linear bandits. Building on the UCB approach, Zhou et al. (2021) developed an adaptive algorithm that handles known-variance noise, providing performance guarantees of \(}+d^{T}_{t}^{2}} \). Zhang et al. (2021) introduced a variance-aware confidence set using elimination with peeling and proved a regret bound of \(}(d)^{T}_{t} ^{2}}\). Kim et al. (2022) further refined this upper bound to \(}d^{2}+d^{3/2}^{T}_{t}^{2}} \) for linear bandits. Zhao et al. (2022) proposed a multi-layered design for UCB algorithm that achieves a \(}(R+1)+d^{T}_{t}^{ 2}}\) regret. More recently, Zhao et al. (2023) proposed a UCB-style algorithm with a unified regret upper bound of \(}d+d^{T}_{t}^{2}}\), where the guarantee is facilitated by the introduction of a novel Freedman-type concentration inequality for self-normalized martingales.

Thompson sampling.TS as a method to address the exploration-exploitation trade-off has gained significant attention in recent years due to its simplicity, adaptability, and robust empirical performance (Chapelle and Li, 2011; Gopalan et al., 2014; Kandasamy et al., 2018). Its effectiveness has been demonstrated across various scenarios (Russo et al., 2018), and in particular, a multitude of theoretical results have been solidified within multi-armed bandit settings (May et al., 2012; Kaufmann et al., 2012; Korda et al., 2013; Russo and Van Roy, 2016). For linear contextual bandits, Agrawal and Goyal (2013) provided the first proof for the \(}(d^{3/2})\) regret of linear TS, and later Abeille and Lazaric (2017) delivered an alternate proof and further extended the analysis to more general linear problems. Notably, Hamidi and Bayati (2020) showed that the \(}(d^{3/2})\) rate cannot be improved in worse-case scenarios. While under additional assumptions such as regularity of the contexts, better rates can be achieved (Hamidi and Bayati, 2020; Kim et al., 2021, 2022).

Moreover, Zhang (2022) introduced a modified version called Feel-Good TS, which is more aggressive in exploring new actions and achieves an \(}(d)\) regret. Recently, Luo and Bayati (2023) also proposed a geometry-aware approach, enabling the establishment of a minimax optimal regret of order \(}(d)\) for the TS algorithm. Xu et al. (2022) proposed a Langevin Monte Carlo TS algorithm that samples from the posterior distribution beyond Laplacian approximation. Investigations have also been undertaken into TS for kernelized bandits (Chowdhury and Gopalan, 2017) as well as integration of TS with neural networks (Wang and Zhou, 2020; Zhang et al., 2020). Recently, Saha and Kveton (2023) proposed a variance-aware TS algorithm on the topic of Bayesian bandits with a variance-dependent upper bound on its Bayesian regret. As far as our awareness extends, no existing results address noise-adaptive TS algorithms in the context of linear bandits.

## 3 Preliminaries

In this section, we introduce the fundamental framework for contextual linear bandits. We also provide a concise overview of the linear TS algorithm, which functions as our benchmark methodology.

```
1:for\(t=1,,T\)do
2: Sample \(_{t}^{}\) from \((}_{t},^{2}_{t}^{-1})\)
3: Select arm \(_{t}_{_{t}}, _{t}^{}\) and observe reward \(r_{t}\)
4:endfor ```

**Algorithm 1** Linear Thompson Sampling

### Linear contextual bandits

We investigate a linear contextual bandit problem with heteroscedastic noise. Let \(T\) be the total number of bandit selection rounds and let \(d\) be the ambient dimension. In each round \(t[T]\), the environment generates an arbitrary set of context vectors \(_{t}^{d}\), potentially even in an adversarial manner, where each \(_{t}\) denotes the context vector of a feasible action. Upon observing the decision set \(_{t}\), the agent selects a context vector \(_{t}_{t}\), and then receives a reward \(r_{t}\) from the environment. In particular, here the reward is a linear function of the selected context further corrupted by noise, _i.e._,

\[r_{t}=_{t}^{}^{*}+_{t},\]

where \(^{*}^{d}\) represents the true model parameter and \(_{t}\) is the stochastic noise. In this paper, we impose standard assumptions on the linear contextual bandit model, which are common in literature.

**Assumption 3.1**.: The ground truth \(^{*}\) satisfies \(\|^{*}\|_{2} 1\). For all \(t[T]\), the decision set \(_{t}\) is contained in the unit ball, _i.e._, \(\|\|_{2} 1\) for all \(_{t}\). There exists a constant \(R>0\) such that \(|_{t}| R\) for all \(t[T]\). For every \(t[T]\), \([_{t}_{1:t},_{1:t-1}]=0\) and \([_{t}^{2}_{1:t},_{1:t-1}]= _{t}^{2}\).

In each round \(t\), an algorithm selects an arm \(_{t}_{t}\), and we denote the optimal arm as \(_{t}^{*}\), _i.e._, \(_{t}^{*}=_{_{t}}^{}^{*}\). As a result, the suboptimality of the selected arm at time \(t\) can be expressed as \(_{t}=_{t}^{*}^{*}-_{t}^{}^{*}\). The objective of the agent is to minimize the cumulative regret incurred over the time horizon \(T\), which is defined as

\[(T)=_{t=1}^{T}_{t}=_{t=1}^{T} _{t}^{*}-_{t},^{*}\,.\]

### Thompson sampling for linear contextual bandits

A standard TS algorithm for linear contextual bandits (_i.e._, Algorithm 1) was first proposed in Agrawal and Goyal (2013), employing Gaussian priors and likelihood functions in the design. At each round \(t[T]\), a sample \(_{t}^{}\) is drawn from the Gaussian posterior distribution, centered at the estimate \(}_{t}=_{t}^{-1}_{s=1}^{t-1}r_{s}_ {s}\) with covariance matrix \(^{2}_{t}^{-1}\), where \(_{t}=_{d}+_{s=1}^{t-1}_{s}_{s}^{ }\) and the confidence radius \(=3R\). The arm \(_{t}\) is then selected against the sample \(_{t}^{}\).

The main idea here is to achieve exploitation by updating the posterior using collected information, and the exploration among the arms is performed naturally in posterior sampling. Agrawal and Goyal (2013) demonstrated that the algorithm presented in Algorithm 1 achieves a regret bound of order \(}(d^{3/2})\). This bound is worse than the minimax lower bound by an unavoidable factor of \(\)(Hamidi and Bayati, 2020). Despite this discrepancy, the regret bound effectively showcases the algorithm's ability to balance exploration and exploitation in the linear contextual bandit setting.

## 4 Warm up: a simple and variance-dependent Thompson sampling algorithm

In this section, we introduce a preliminary algorithm, LinVDTS, delineated in Algorithm 2, which combines TS with a weighted ridge regression estimator in a straightforward manner. For the sake of clear exposition, we focus on a less complex linear bandit problem in which the agent is privy to both the reward \(r_{t}\) and its variance \(_{t}^{2}\) subsequent to the selection of an arm.

### Algorithmic design

In order to integrate variance information into the estimation process, the use of weighted regression has emerged as a prevalent approach in the field of variance-aware online learning (Zhou et al., 2021;Min et al., 2021, 2022; Zhou and Gu, 2022; Yin et al., 2022; Zhao et al., 2023). This methodology is intimately associated with the concept of the Best Linear Unbiased Estimator (BLUE), a well-established statistical technique for achieving optimal linear estimation (Henderson, 1975).

Let us first explain the details of the proposed algorithm LinVDTS. In every round \(t\), the algorithm computes an estimate \(}_{t}\) of the ground truth \(^{*}\) by solving a weighted ridge regression problem:

\[}_{t}=*{arg\,min}_{} ^{t}_{s=1}_{s}^{2}}(r_{s}-_{s}, )^{2}+\|\|_{2}^{2}.\]

Here \(_{t}\) represents a weight parameter and \(\) denotes a regularization constant. For each approximation \(}_{t}\), we stochastically draw \(_{t}^{}\) from a Gaussian distribution with mean \(}_{t}\) and covariance \(_{t}=_{t}_{t}^{-1}\), where \(_{t}=_{d}+_{s=1}^{t-1}_{s}_{s}^{}/_{s}^{2}\) is computed from previous context and reward pairs, and the confidence radius, \(_{t}\), is chosen as

\[_{t}}) (}{}}{})}+}(}{}}{}) +.\]

Refer to (A.2) for a detailed delineation of \(_{t}\). For each context vector \(_{t}\), we compute the estimated reward as \(,_{t}^{}\), and the algorithm outlined in Algorithm 2 then selects the arm \(_{t}\) that optimizes this estimated reward. Following the approach of Zhou and Gu (2022), we construct the weight parameter \(_{t}=\{_{t},,\|_{t}\|_{ _{t}^{-1}}^{1/2}\}\), which is designed as the maximum among the variance, a constant \(\), and the uncertainty associated with the selected arm \(_{t}\). It ensures the formulation of a tight confidence radius \(_{t}\), guided by a Bernstein-type concentration inequality, _cf._ Theorem A.1.

```
0: Total number of iterations \(T\), number of total arms \(n\)
1: Initialize \(_{1}_{d}\), \(_{1} 0\), \(}_{1} 0\), \(_{1}\), and \(_{1}_{1}^{2}_{1}^{-1}\)
2:for\(t=1,,T\)do
3: Observe context vectors \(_{t}\)
4: Draw \(_{t}^{}(}_{t},_{t})\)
5:\(_{t}*{arg\,max}_{_{t }},_{t}^{}\)
6: Observe reward \(r_{t}\) and variance \(_{t}^{2}\)
7: Update \(_{t+1}_{t}+_{t}_{t}^{ }/_{t}^{2}\)
8: Update \(_{t+1}_{t}+r_{t}_{t}/_{t }^{2}\)
9: Update \(}_{t+1}_{t+1}^{-1}_{t+1}\)
10: Compute covariance matrix \(_{t+1}_{t+1}^{2}_{t+1}^{-1}\)
11:endfor ```

**Algorithm 2** Linear Variance-Dependent Thompson Sampling (LinVDTS)

### Regret guarantee and technical challenges

The theorem below establishes a variance-dependent upper bound on the expected regret of LinVDTS. See Appendix A.3 for detailed proof.

**Theorem 4.1**.: _Set parameters \(=1/\), \(=R^{}/d^{}\), \(=d\). For any \((0,1)\), the expected regret of Algorithm 2 is upper bounded as follows:_

\[[(T)]=d^{3/2}1+_{t=1}_{t}^{2}} T+ T.\]

_Further, it holds with probability \(1-\) that_

\[(T)=d^{3/2}1+_{t= 1}_{t}^{2}} T+}.\]_Remark 4.2_.: Choosing \(=1/T\), then we see that the expected regret of Algorithm 2 is bounded by \(}d^{3/2}1+^{T}_{t}^{2} }\). However, the high-probability regret bound of Algorithm 2 is of order \(}d^{3/2}1+^{T}_{t}^{2 }}+\), where the additional \(\) term fails to be variance-dependent. Nonetheless, this regret bound already improves over the existing \(}(d^{3/2})\) bound for linear Thompson sampling in the literature (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017).

The limitation on the high-probability bound arises from a set of technical challenges that are implicit and inherent in the posterior sampling used in Algorithm 2. To illustrate this, we now provide an outline of the proof for Theorem 4.1, and then discuss the technical intricacies therein.

Proof sketch of Theorem 4.1.: Let us denote \(_{t}()=_{t}^{*}-,^{*}\) as the suboptimality of \(_{t}\). We introduce the saturated set \((t)\) for round \(t\) as the ensemble of arms whose confidence radius is dwarfed by its suboptimality, _i.e._, \((t)=\{_{t}\,:\,_{t}()>(2 +1)_{t}\|\|_{_{t}^{-1}}\}\)(Agrawal and Goyal, 2013). Then the suboptimality \(_{t}\) of the chosen context \(_{t}\) can be decomposed as follows:

\[_{t}=_{t}^{*}-_{t},^{*}= _{t}(_{t}^{})+_{t}^{\,}^{*}-_{t}^{}^{*},\]

where the term \(_{t}^{}=_{(t)}\|\|_{_{t}^{-1}}\) refers to the unsaturated arm that possesses the smallest \(_{t}^{-1}\) norm. Leveraging the concentration results delineated in Lemmata A.2 and A.3 pertaining to \(}_{t}\) and \(_{t}^{}\), we have

\[_{t}(\|_{t}^{}\|_{_{t }^{-1}}+\|_{t}\|_{_{t}^{-1}})_{t}.\] (4.1)

For upper bound on the expected regret, we use an anti-concentration argument (Lemma A.6) to relate \(\|_{t}^{}\|_{_{t}^{-1}}\) back to \(\|_{t}\|_{_{t}^{-1}}\). This yields

\[[_{t}\ |\ _{t}]\,[\| _{t}\|_{_{t}^{-1}}]_{t}+},\] (4.2)

where \(\{_{t}\}_{t 1}\) represents a filtration of the information available up to the observation of the set of context vectors at each round. Then collecting the above inequality for all \(t[T]\), together with an elliptical potential lemma, we get the desired upper bound for \([(T)]\).

Next, for the high-probability regret bound, we decompose

\[(T)=_{t=1}^{T}[_{t}\ |\ _{t}]+ _{t=1}^{T}(_{t}-[_{t}\ |\ _{t}])\] (4.3)

where the first term corresponds to the expected regret bound. We control the second term using martingale concentration, which results in an additional \(\) term in the final regret bound. 

#### Why cannot simultaneously achieve variance awareness and high-probability bound?

First, the optimism induced by the posterior sampling is too conservative.1 A primary challenge in the analysis is to effectively control each suboptimality term \(_{t}\), _cf._(4.1). It is important to note that one key algorithmic design in UCB-type algorithms is the exact optimism from the UCB bonus \(\|_{t}\|_{_{t}^{-1}}\), which ensures \( x_{t}^{*},}_{t}+\|_{t}^{* }\|_{_{t}^{-1}} x_{t},}_{t}+ \|_{t}\|_{_{t}^{-1}}\). However, for TS-style algorithms, the absence of this exact optimism hinders our control over the term \(\|_{t}^{*}\|_{_{t}^{-1}}\) throughout all rounds \(t[T]\), _i.e._, a standard decomposition of \(_{t}=_{t}^{*}-_{t},^{*}\) yields \(_{t}(2+1)_{t}(2\|_{t}^{*}\|_{ _{t}^{-1}}+\|_{t}\|_{_{t}^{-1}})\) in this case. As a result, standard analyses turn to methodologies that employ an anti-concentration inequality on the sampling distribution (Agrawal and Goyal, 2013). This leads to the inequality \(\|_{t}^{}\|_{_{t}^{-1}}\|_{t}\|_{ _{t}^{-1}}\) with a constant probability, thereby providing an upper bound on the expected regret.

Second, in the conversion of the expected regret bound to a high-probability bound, we have no help from the noise variance information. One may wonder if it is possible to incorporate the variance of \(_{t}-[_{t}_{t}]\) to get a Bernstein-type concentration result. However, the variance of \(_{t}-[_{t}_{t}]\) does not necessarily conform to the noise variance, especially when the decision set \(_{t}\) is revealed adversarially. To see this, consider the following situation: Given \(}_{t}\), the environment reveals a decision set \(_{t}=\{_{},-_{}\}\) where \(_{}\) is orthogonal to \(}_{t}\). Then a small perturbation in \(_{t}^{ s}\) along the direction of \(_{}\) can cause the change of arm selection. This implies that a small uncertainty of the posterior distribution along the direction of \(_{}\) will be amplified to a constant variation of the selected arm \(_{t}\). Therefore, in general, we do not have any delicate control of the variance of \(_{t}-[_{t}_{t}]\). Consequently, we can only apply Azuma-Hoeffding inequality to get a non-noise-adaptive term of \(}()\). We discuss in the next section how to conquer this via more advanced algorithmic design.

## 5 General case: a noise-adaptive Thompson sampling algorithm

In this section, we propose another TS algorithm LinNATS for linear contextual bandits, one that is provably adaptive to unknown heteroscedastic noise. For greater generality, we adopt the identical problem setting delineated in Section 3, and we refrain from assuming access to the variance information \(\{_{t}^{2}\}_{t=1}^{T}\) associated with the rewards \(\{r_{t}\}_{t=1}^{T}\).

### Algorithm

The proposed algorithm LinNATS is displayed in Algorithm 3. Below we go through the details of LinNATS and explain the algorithmic design along the way.

**Stratification over contextual uncertainty.** For more efficient uncertainty control, we adopt a stratification strategy akin to that previously used for UCB-type algorithms (Chu et al., 2011; Li et al., 2023; Zhao et al., 2023). Our algorithm segments the context vectors at each round \(t\) into \(L\) distinct layers, enabling the precise control of both \(\|_{t}\|_{_{t}^{-1}}\) and \(\|_{t}^{}\|_{_{t}^{-1}}\).

Specifically, the \(\)-th layer establishes a threshold of \(2^{-}\) and retains a separate estimate \(}_{t,}\) and sampled variable \(_{t,}^{ s}\) for each \([L]\) and \(t[T]\) (Line 3). Commencing from \(=1\), a sequence of decision sets \(_{t}=_{t,1}_{t,2}\) of diminishing sizes is derived by excluding the arms \(_{t,}\) that are less likely to maximize \(,^{}\) at each layer \(\). More specifically, on Line 11 we let

\[_{t,+1}=_{t,}: ,_{t,}^{ s}_{^{}_{t,}}^{},_{t, }^{ s}-2^{-+1}(L/)}+1) _{t,}},\] (5.1)

which ensures that the optimal arm \(_{t}^{}\) falls into all decision sets with high probability. The context vector \(_{t}=*{arg\,max}_{_{t,}} ,_{t,}^{ s}\) is chosen if the uncertainty term \(\|\|_{_{t,}^{-1}}\) for all \(_{t,}\) (Line 8). Otherwise, \(_{t}\) is selected only when the uncertainty surpasses the threshold, _i.e._, \(\|_{t}\|_{_{t,}^{-1}} 2^{-}\) for some \(\), and the elimination process in (5.1) terminates (Line 13).

The round index \(t\) is incorporated into a growing index set \(_{t+1,}\) (Line 9 & 15), and the observation pair \((_{t},r_{t})\) participates the later estimation of \(}_{s,}\) for all \(s>t\) only if the chosen arm exhibits large uncertainty within the current layer, _i.e._, \(\|_{t}\|_{_{t,}^{-1}} 2^{-}\) (Line 21). This process guarantees that every context vector \(_{t,}\), cascading through the first \((-1)\)-th layer, satisfies \(\|\|_{_{t,-1}^{-1}} 2^{-+1}\).

**Parameter estimation with weighted ridge regression.** To estimate \(^{}\), we again utilize weighted ridge regression, but this time within each uncertainty level. For each layer \([L]\) and round \(t[T]\), the associated estimator \(}_{t,}\) is given by

\[}_{t,}=*{arg\,min}_{^{d}}_{s_{t,}}w_{s}^{2}(r_{s}-_{s}, )^{2}+2^{-2}\|\|_{2}^{2},\]

where the weight parameter \(w_{t}>0\) is selected to fulfill the condition \(\|w_{t}_{t}\|_{_{t,}^{-1}}=2^{-}\) (Line 14). Owing to the specific formulation of the weighting parameter, it follows that \(_{s_{t,}}\|w_{s}_{s}\|_{_{s,}^{-1}}=2^{ -}\) holds universally for all \(t[T]\) and \([L]\). This allows the application of a Freedman-typeconcentration inequality to guarantee with high probability for all layer \([L]\) that

\[\|}_{t,}-^{*}\|_{_{t,}}= }}+}}w_{s}^{2}_{s}^{2}}.\] (5.2)

**Variance-dependent confidence radius.** The variance-dependent error bound (5.2) is sufficient for the formulation of a TS confidence radius, provided that the variance \(_{t}^{2}\) is known. However, under scenarios where the variance is unknown, it is necessary to further estimate them adaptively and on the fly. Specifically, we estimate the summation of past weighted variances, represented as \(_{s_{t,}}w_{s}^{2}_{s}^{2}\), each using an empirical estimator \((r_{t}-_{t},}_{t,})^{2}\) as a substitute for \(_{t}^{2}\). The weighted summation of these estimators \(_{t,}=_{s_{t,}}w_{s}^{2}(r_{s}-_{s}, }_{s,})^{2}\) effectively acts as a precise estimator of \(_{s_{t,}}w_{s}^{2}_{s}^{2}\)(Zhao et al., 2023). Utilizing this estimator, we adjust the posterior distribution according to the following confidence radius

\[_{t,}=}+6R^{ 2}L}{}+2^{-2+4})L}{}}+ }L}{}+},\] (5.3)

where the variance estimator takes a slight variant for technical considerations:

\[_{t,}=_{s_{t,}}w_{s}^{2}(r_{s}- _{s},}_{t,})^{2},&2^{} 64 L/)}\\ R^{2}|_{t,}|,&.\] (5.4)

[MISSING_PAGE_FAIL:9]

Conclusion and future work

In this paper, we aimed to address the task of noise-adaptive learning on linear contextual bandits with indeterminate heteroscedastic variance. As part of our endeavor, we put forth a straightforward algorithm, LinVDTS, which assures a variance-dependent guarantee on the expected regret. Our analytical exploration reveals that a simplistic implementation of Thompson Sampling culminates in a sub-optimal regret bound. To mitigate this issue, we put forth an innovative Thompson Sampling algorithm, LinNATS. With the incorporation of a stratification scheme, the algorithm successfully navigates the technical challenges of uncertainty control, securing a variance-dependent regret under unknown variance. Consequently, it effectively bridges the performance chasm between the worst-case scenarios involving constant variance and those concerning deterministic rewards.

Looking forward, it would be intriguing to conceive a noise-adaptive variant of the recently devised Feel-Good TS as introduced by Zhang (2022), which enhances the dependency on the dimension \(d\). Furthermore, extending the noise-adaptive methodology to TS algorithms designed for more general settings, _e.g._, generalized linear bandits and Reinforcement Learning with linear function approximation, constitutes an appealing direction for future research.