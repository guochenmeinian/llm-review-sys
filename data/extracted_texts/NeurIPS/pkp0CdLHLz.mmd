# Detecting Origin Attribution for Text-to-Image Diffusion Models in RGB and Beyond

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In this work, we not only attribute images to 12 state-of-the-art T2I generators but also investigate what inference stage hyperparameters are discernible. We further examine what visual traces are leveraged in origin attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity.

## 1 Introduction

Recent text-to-image (T2I) diffusion models [4; 32; 41; 43; 45; 46; 49; 51] have markedly transformed image generation, enabling the creation of highly realistic and imaginative visual content directly from textual descriptions. However, this progress introduces significant challenges in discerning real images from AI-generated images and accurately identifying their origins. Addressing these challenges is vital for preserving the integrity of visual content across digital platforms.

Previous studies [2; 5; 7; 26; 56; 61; 66] have focused on differentiating AI-generated images from real ones, with some research attributing images to their source generators, notably in GAN variants [6; 21; 35; 63] and diffusion models [11; 22; 54]. Yet, these investigations have largely been conducted using generative models that may not reflect the latest advancements, and they have not fully explored the practical and scientific dimensions of this task, which we aim to further examine.

## 2 Dataset Generation

In this work, we detect origin attributions for modern text-to-image (T2I) models, while also investigating the extent to which traces are detectable across generators and inference stage controls. To achieve this, we generate images using a variety of T2I models and text prompts to ensure diversity. Additionally, we maintain a consistent generator and adjust inference time hyperparameters.

### Images from Diverse Generators and Prompts

As depicted in Fig. 1, we employed 12 modern, open-source T2I models for image generation: SD 1.5 , SD 2.0 , SDXL , SDXL Turbo , Latent Consistency Model (LCM) , Stable Cascade , Kandinsky 2 , DALL-E 2 , DALL-E 3 , and Midjourney versions 5.2 and 6 . To generate images, we use the OpenAI API for DALL-E 2 and 3, an automation bot for Midjourney 5.2 and 6, and the Hugging Face diffusers repository  for the remaining models. To gather a broad spectrum of text prompts, we leveraged around 5,000 captions from MS-COCO .

### Images from Varying Hyperparameters During Inference Stage

We expand our focus beyond identifying the source generators based on their architectures, to a deeper analysis of the critical yet subtle choices made during the inference stage that greatly impact

Figure 1: A depiction of images generated for our dataset using 12 different T2I generators.

the generated outputs. Initially, we investigate the possibility of identifying certain checkpoints within the same architecture, specifically Stable Diffusion (SD) , based on different training iterations. Next, we question whether the generated images can reveal which scheduler [23; 27; 57; 65] was employed during the inference phase for the same generator. Furthermore, drawing inspiration from studies indicating that different seed numbers in GAN-generated images can be detected , we apply this concept to diffusion models to determine if the seed is detectable based on the images. Finally, we conduct experiments with diffusion steps ranging from 5 to 50 in increments of 5 to investigate whether the number of sampling steps leaves detectable traces in the images. Selected samples of images generated under different hyperparameter adjustments are presented in Fig. 2.

## 3 Detecting Origin Attribution in RGB

In this section, we benchmark origin attribution performance across 12 modern text-to-image generators, examining the impact of various architectures and training sizes on task performance. We then analyze the detectability of traces for various hyperparameter adjustments during inference time.

### Training Origin Attributors

**Problem Setup and Model Performance.** Our study merges the tasks of discerning "AI-generated vs. Real Images" and attributing images to their sources. This is achieved by including real images in our dataset and treating them as an additional 'generator'. Concerning the architecture of the origin attributor, which functions as an image classifier, prior work [12; 39] showed that a straightforward linear probe or nearest neighbor search applied to a large pretrained model like CLIP  can effectively differentiate AI-generated images from real ones. Inspired by these findings, we employ three network architectures to tackle our attribution task: an EfficientFormer  trained from scratch, a CLIP  backbone connected with a linear probe and MLP, and DINOcv2  with a similar configuration. We also analyze the impact of incorporating text prompts as inputs similar to Sha et al. , providing slight yet consistent improvements across all architectures, as shown in Tab. 1.

**Classifier Performance Across Generators.** As illustrated in Fig. 3, there is a noticeable challenge in differentiating generators from the same family, with notable pairs including "SD 1.5 vs. SD 2.0," "Midjourney 5.2 vs. Midjourney 6," and "LCM (2 steps) vs. LCM (4 steps)." While Midjourney's architecture remains undisclosed to the public, it is reasonable to infer that versions 5.2 and 6 likely share a similar underlying architecture from our analysis. Interestingly, DALL-E 3 presents more confusion when compared to Midjourney versions 5.2 / 6, rather than with DALL-E 2. We attribute this finding to the significant architectural differences: DALL-E 2 incorporates pixel diffusion in its decoder stage, whereas DALL-E 3 employs multi-stage latent diffusion alongside a distinct one-step VAE decoder, similar to , leading to divergent generative characteristics. Finally, we demonstrate that the accuracy of the attributor consistently improves with an increase in the number of training images, as shown on the right side of Fig. 3. However, due to budget constraints, fully exploring the dataset expansion up to the saturation point is deferred to future research endeavors.

### Analyzing the Detectability of Hyperparameter Variations

T2I generators often have several hyperparameters at the inference stage that impact the generated image quality, and a natural question that arises is whether images produced using different hyperparameters are distinguishable. To investigate this, we target four hyperparameter choices for Stable Diffusion : model checkpoint, scheduler type, number of sampling steps, and initialization seed.

    & E.F. (scratch) & CLIP + LP & CLIP + MLP & DINOcv2 + LP & DINOcv2 + MLP \\  w/o text & 90.03\% & 70.15\% & 73.09\% & 67.68\% & 71.33\% \\ w/ text & 90.96\% & 71.44\% & 74.19\% & 69.44\% & 73.08\% \\   

Table 1: The 13-way classification accuracy of various architectures for origin attribution performed across 12 generators and a set of real images, with each class containing an equal number of images. The probability of randomly guessing the correct source is \(\), which gives a **7.69%** accuracy. “E.F.” refers to EfficientFormer trained from scratch. The first and second rows in the results table indicate classifiers trained without and with text prompts, respectively.

Figure 2: We show the diversity in generated images influenced by varying hyperparameters, such as checkpoints of the same architecture, schedulers, initialization seeds, and number of inference steps.

Specifically, we compared Stable Diffusion checkpoints 1.1 to 1.5, each of which is trained using a different number of iterations on LAION . We then examined the detectability of images generated using eight schedulers: DDIM , DDPM , Euler , Euler with ancestral sampling , KDPM 2 , LMS , PNDM , and UniPC . Additionally, we generated images using SD 2.0 and SDXL for ten different sampling steps ranging from 5 to 50, and ten different seeds ranging from 1 to 10. For each hyperparameter, we train a separate EfficientFormer  to classify the generated images. As shown in Tab. 2, all six classifiers detect the hyperparameter choice better than random chance. Interestingly, detecting the initialization seed achieves nearly 100% accuracy, aligning with work by Yu et al.  that found different seeds lead to attributable GAN fingerprints. Moreover, based on the confusion matrix for different sampling steps using SDXL in Fig. 4, we see that images generated using fewer steps are more detectable than those generated using more steps, likely because fewer steps noticeably degrades the generation quality.

## 4 Detecting Origin Attribution Beyond RGB

Previous studies have suggested that an origin attributor may leverage middle-to-high frequency information to differentiate images. However, it remains unclear what constitutes "middle-to-high frequency information" and to what extent the network can identify detectable traces in the images. Thus, we present an extensive empirical study on the impact of incrementally eliminating visual details at various levels of granularity on origin attribution performance.

**High-Frequency Perturbations.** Prior research  has identified that generators leave unique fingerprints in the high-frequency domain, allowing attributors to learn these high-frequency details effectively. As an initial step, we investigate the effects of introducing high-frequency perturbations to images on the attributor's performance, which aims to enforce the classifier to learn beyond high-frequency details. For simplicity, we train a separate EfficientFormer  on each set of perturbed images. Figure 5 illustrates our observations under four types of perturbation: Gaussian blur, bilateral filtering, adding Gaussian noise, and SDEdit . We note that these perturbations result in a modest decrease in classification accuracy. Specifically for SDEdit, the high-frequency traits of SDXL are embedded into every image, regardless of their source generators, by undergoing processing via the encoder, diffusion UNet, and decoder of SDXL . Remarkably,

    & Checkpoints & Schedulers & Sampling Steps & Seeds \\  Random & 20\% & 12.5\% & 10\% / 10\% & 10\% / 10\% \\ Accuracy & 30.21\% & 20.18\% & 25.96\% / 56.64\% & 98.80\% / 99.94\% \\   

Table 2: Comparison of accuracy for detecting hyperparameter values based on generated images. For the ‘Sampling Steps’ and ‘Seeds’ trials, we trained and evaluated on images from SD 2.0 and SDXL. Accuracies are written as _SD 2.0 / SDXL_. Notably, the ‘Seeds’ trial has near perfect performance.

Figure 4: Confusion matrices for hyperparameter variations. We observe that images generated with fewer SDXL sampling steps are more detectable, likely due to visible degradation in image quality.

Figure 3: **Left/Middle:** Accuracy and confusion matrix of EfficientFormer trained with prompts, which had the best accuracy. **Right:** Accuracy of EfficientFormer as we vary the number of images. Specifically, we compared Stable Diffusion checkpoints 1.1 to 1.5, each of which is trained using a different number of iterations on LAION . We then examined the detectability of images generated using eight schedulers: DDIM , DDPM , Euler , Euler with ancestral sampling , KDPM 2 , LMS , PNDM , and UniPC . Additionally, we generated images using SD 2.0 and SDXL for ten different sampling steps ranging from 5 to 50, and ten different seeds ranging from 1 to 10. For each hyperparameter, we train a separate EfficientFormer  to classify the generated images. As shown in Tab. 2, all six classifiers detect the hyperparameter choice better than random chance. Interestingly, detecting the initialization seed achieves nearly 100% accuracy, aligning with work by Yu et al.  that found different seeds lead to attributable GAN fingerprints. Moreover, based on the confusion matrix for different sampling steps using SDXL in Fig. 4, we see that images generated using fewer steps are more detectable than those generated using more steps, likely because fewer steps noticeably degrades the generation quality.

this process led to only a minor reduction in accuracy, suggesting a robustness in the attributor's ability to identify generator-specific fingerprints despite high-frequency modifications.

**Middle-Level Representations.** High-frequency perturbations result in only minor performance degradation, suggesting that the detectable traces left by different generators might also reside within the mid-frequency domain. To study the presence of these detectable traces, we convert the images into various mid-level representations--'Albedo' , 'Shading' , 'Canny Edge', 'Depth Map' , 'Surface Normal' , and 'Perspective Fields' --utilizing readily available models. This approach aims to uncover the extent to which these mid-level frequencies carry generator-specific information that can be used for attribution. We proceed by training a distinct EfficientFormer  for each mid-level representation, and we show their classification accuracies in Fig. 6. Notably, although the overall accuracy for the attributors trained on Canny Edge, Depth Map, and Perspective Field images is not high, they demonstrate remarkable performance at discerning real vs. fake images in Fig. 8 of the supplemental. This finding aligns with work by Sarkar et al.  suggesting that generative models often fail to generate accurate geometry.

**Image Style Representations.** Furthermore, it's common to observe perceptible style differences among outputs of image generators. For instance, Midjourney  often produces images with a 'cinematic' quality, while DALL-E  tends to create images with overly smooth textures and cartoonish appearances, as in Fig. 1. This observation leads to a pertinent question: if we train an attributor on only stylistic representations of images, can we still identify source generators?

To capture the style representation of images, we adhere to methods from style transfer literature , employing a pretrained VGG network  to extract features across multiple layers. Next, we compute the Gram matrix  for each network layer. If we denote the feature at a specific layer as \(F^{H W N}\), then the Gram matrix \(G^{N N}\) is the cosine similarity between each channel in the feature representation. This process distills the style of images, providing a unique fingerprint for each generator's output. We reshape and concatenate the Gram matrices extracted from multiple layers, and then train EfficientFormer  using these aggregated feature vectors.

Remarkably, the origin attributor achieves an accuracy of **92.80%** when trained on style representations, surpassing the performance of the attributor trained on original RGB images by **1.84%**. The superior accuracy from this style-based attributor highlights the importance of stylistic features, such as texture and color patterns, in discerning generators more effectively than the direct visual content. This insight not only advances our understanding of origin attribution techniques but also emphasizes the potential of leveraging stylistic elements for more nuanced AI recognition and analysis tasks.

## 5 Conclusion

In this study, we present in-depth analyses on detecting and attributing images generated by modern text-to-image (T2I) diffusion models. Our origin attributors, trained to recognize outputs from 12 T2I diffusion models along with a class of real images, reached an impressive accuracy of over 90% that significantly surpasses random chance. Additionally, our investigations into the challenge of distinguishing generators within the same family and the detectability of hyperparameter choices at inference time provide comprehensive insights. Going beyond mere RGB analysis, we introduce a new framework for identifying detectable traces across levels of visual detail, offering profound insights into the underlying mechanics of origin attribution. These analyses provide fresh perspectives on image forensics aimed at alleviating the threat of synthetic images.

Figure 5: We present a generated image before and after perturbing its high-frequency details. We trained EfficientFormer on images after each high-frequency perturbation and observed a mild decline in the respective test accuracy, as shown beside the images.

Figure 6: We present an RGB image and its mid-level representations. We trained EfficientFormer on each representation and show the test accuracy below each image. Note that random chance is 7.69%.