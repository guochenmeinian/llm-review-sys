# Not All LLM Reasoners Are Created Equal

Arian Hosseini

Mila

arian.hosseini9@gmail.com &Alessandro Sordoni

Mila, Microsoft Research

 Daniel Toyama

Google DeepMind &Aaron Courville

Mila &Rishabh Agarwal

Mila, Google DeepMind

###### Abstract

We study the depth of problem-solving capabilities of LLMs, and to what extent they perform mathematical reasoning in a compositional manner. To this end, we create a new benchmark by composing pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. We measure the difference between the performance of solving each question independently and solving the compositional pairs as the reasoning gap of a model. Our findings reveal a significant reasoning gap in most frontier LLMs. This gap is more pronounced in smaller and more cost-efficient models. The objective of this study is not to introduce yet another benchmark, but rather to provide a case study aimed at gaining deeper insights into current models' reasoning abilities, and to reassess existing established training methods and benchmarks.

Figure 1: **Reasoning Gap: Pairs of GSM8K test questions are chained together so that the answer of the first question (\(Q_{1}\)) is a variable in the second one (\(Q_{2}\)). The model is required to correctly answer both questions to solve the problem. If a model has an accuracy of \(S_{1}\) on the \(Q_{1}\) set, and \(S_{2}\) on \(Q_{2}\) set, then the expected Compositional GSM accuracy is \(S_{1} S_{2}\). The x-axis corresponds to the geometric mean \( S_{2}}\), labeled GSM8K accuracy for simplicity. The trend-line \(y=x^{2}\) is the expected Compositional GSM accuracy.**

**Compositional GSM Problem**

Let X be the answer to the \(}\):

\(}\): There are 27 unicorns left in the world. One third of them are in the Scottish Highlands. Two thirds of the Scottish unicorns are female. How many female Scottish unicorns are there?

Solve it and use the value of X to solve \(}\). Explain your answer step by step.

\(}\): Zack's locker is half as big as Timothy's locker. Peter's locker is 1/4 as big as Zack's locker. If Peter's locker is X cubic inches, how big is Timothy's locker in cubic inches?

## 1 Introduction

The strong performance of large language models (LLMs) on high-school and college-level math reasoning benchmarks (Dubey et al., 2024; Google, 2024; OpenAI, 2023b), has led to the common belief that LLMs have "mastered" grade-school math, particularly as measured by the GSM8K benchmark (Cobbe et al., 2021). This apparent mastery of grade-school math problems raises a deeper question: do LLMs truly grasp the underlying concepts or do they mostly rely on dataset contamination or memorization (Srivastava et al., 2024)? For example, a recent examination on private "held-out" grade-school problems (Zhang et al., 2024) reveals that while frontier closed-source LLMs show minimal signs of overfitting, some open-weights models show systematic overfitting, possibly due to test data contamination.

In this work, we perform a case study to evaluate how well LLMs can combine learned concepts to solve unseen problems, to probe the brittleness of their reasoning abilities. To do so, we introduce _Compositional GSM_, a two-hop version of GSM8K with higher difficulty, where each problem chains two test questions together such that the answer to the first question is used as a variable in the second question (Figure 2). As LLMs can easily solve grade-school math problems, they should also be capable of solving combinations of those problems. As such, we measure the gap between their performance on solving the questions individually and on Compositional GSM. Specifically, we benchmark frontier open-weights and closed LLMs, including Gemini (Google, 2023, 2024), Gemma2 (Gemma Team et al., 2024), Llama-3 (AI@Meta, 2024), GPT (OpenAI, 2023a), Phi (Abdin et al., 2024), Qwen (Yang et al., 2024) and Mistral families (Jiang et al., 2024).

Here are our key findings:

* Most models exhibit a gap between their performance on GSM8K test set and Compositional GSM (Figure 1).
* This reasoning gap is larger in small and more cost-efficient models (Figure 5 and Figure 3).
* Instruction-following tuning of LLMs heavily favours the original GSM8K split (Figure 4).
* Finetuning with human data and synthetic data results in a similar reasoning gap trend (Figure 7).
* Smaller models benefit more from generating code rather than natural language Chain-of-Thought (CoT) to solve Compositional GSM problems (Figure 6).

## 2 Compositional Grade-School Math (GSM)

Each question in compositional GSM consists of two questions, Question-1 and Question-2, from a subset of 1200 examples of the original GSM8K test set. The final answer of Question-1 is refereed to as \(X\) which is a variable in Question-2 (Figure 2). The final answer of Question-2 is obtained by substituting \(X\) and solving it. The choice of Question-1 and the number to modify and replace with \(X\) in Question-2 was made in a a way such that the new final answer of Question-2 is different from its old final answer, and is a positive integer not too far from the old final answer.

Figure 2: **Example Problem from the Compositional GSM benchmark**. The answer of Question-1 (\(}\)) is a variable X in Question-2 (\(}\)). Therefore, the model has to be able to solve the first question correctly in order to solve the second question. The new final answer of Question-2 is calculated by modifying its code-form solution and executing it. Question-1 and the number to modify in Question-2 are chosen to have a new final answer which is a positive integer not too far from the old answer of Question-2.

Reasoning GapQuestion-1 and Question-2 in our compositional queries are from the original test split \(_{}\), and the modified test split \(_{}\) respectively. Assuming that a model has an accuracy of \(S_{1}\) on \(_{}\) and \(S_{2}\) on \(_{}\), it is expected for it to have an accuracy of \(S_{1} S_{2}\) on the compositional split \(_{}\). We report the following as the compositional reasoning gap score,

\[:=S_{c}-S_{1} S_{2}\] (1)

where \(S_{c}\) is the performance of the model on \(_{}\).

## 3 Experiments & Results

The distance to the trend-line in Figure 1 shows the reasoning gap of models. The x-axis corresponds to \( S_{2}}\), which is the geometric mean of the accuracies on the set of \(Q_{1}\) and \(Q_{2}\) independently. We find that most models fall below expectation on Compositional GSM. Specifically, it is evident that cost-efficient models have a larger gap than more expensive models. More analysis is provided in the Appendix.

### Cost-Efficient LLMs Reason Differently

The reasoning abilities of cost-efficient LMs has been rapidly improving over time, as evaluated using standard benchmarks (Bansal et al., 2024). For example, GPT-4o mini and Gemini 1.5 Flash both achieve above 90% accuracy on GSM, while costing \(25-35\) cheaper than GPT-4o and Gemini 1.5 Pro respectively. This progress could be attributed to several factors, such as better pretraining data (Al@Meta, 2024), and knowledge distillation (Agarwal et al., 2024; Team et al., 2024). To this end, we investigate whether these reasoning gains on GSM8K still persist on Compositional GSM.

We study four family of models, each comprising both a high-cost and low-cost option, where cost is measured via parameter count or API pricing. Figure 3 shows the original GSM8K test split performance and Compositional GSM performance for all models. The numbers above the bars represents the reasoning gap defined in Eq 1. While cheaper models perform comparably on the original GSM8K test, they exhibit a notable drop in performance on the Compositional GSM test set. These results suggest that critical flaws of cost-efficient LLMs in their reasoning may be obscured by high scores on standard benchmarks. This underscores the need to rethink current strategies for developing cost-efficient language models.

### Instruction-Tuning Impacts LLM Reasoning Differently

We compare pretrained and instruction-following tuned versions of models in three families of Mistral, LLAMA3 and Gemma2. Figure 4 illustrates this comparison, along with the performance gains from

Figure 3: **Cost efficient LLMs reason differently: showing four family of models, each having a high-cost and low-cost option. Although the cheaper models perform similarly on the original GSM8K test, they show a significant decline in performance on the Compositional GSM test.**

instruction-tuning, displayed above bars for each test set. On small models (top row), this comparison shows that current instruction-tuning is heavily optimized for GSM8K questions. Instruction-tuning leads to a significantly larger improvement on the original GSM8K test set than the Compositional GSM test across model families. However, this trend does not apply to larger models (bottom row), where the improvements are inconsistent.

## 4 Discussion and Conclusion

We designed the Compositional GSM benchmark, which requires solving dependent pairs of math word problems. These problems are from the original GSM8K test split. We investigate the "System 2" mathematical reasoning capabilities of LLMs by comparing their performance on the original GSM8K test split and our Compositional GSM test set. Our analysis reveals a notable reasoning gap in most models. Many leading LLMs exhibit a substantial difference in performance when solving questions independently versus as part of the compositional pair. Our study indicates that smaller and more cost efficient models exhibit a larger reasoning gap. Models frequently struggle with pairs of questions and get distracted likely because they are tuned to handle one question at a time. They often answer the first question correctly, but lose attention to details and make subtle errors in answering the second question. We also noticed that learning from human data and self-generated data results in similar behaviour. In both settings, as training progresses, the model's performance on the original test split improves. However, beyond a certain point, performance on the Compositional GSM test begins to decline.

We emphasize that this benchmark should not be viewed as an endpoint or merely as a tool for generating additional training data, but as a catalyst to gain insights about current models and to re-evaluate and improve existing benchmarks. Our findings are intended to stimulate further exploration and provide new perspectives. Future research could build on this setup by incorporating more challenging questions, such as those from the MATH dataset, or by extending the framework to multi-modal problems to gain deeper insights into the reasoning capabilities of LLMs.

Figure 4: **Impact of Instruction-Tuning on Reasoning Gap: comparing pretrained and instruction-following tuned variant of models from Mistral, LLAMA3 and Gemma2 families. Numbers above bars represent improvements from instruction-tuning on each set. For smaller models (top), we observe that instruction-tuning is highly optimized for GSM8K questions, which results in a greater improvement on the original GSM8K test set compared to the Compositional GSM test. However, this pattern does not hold for larger models (bottom).**