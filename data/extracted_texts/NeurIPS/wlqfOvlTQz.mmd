# Nadav Merlis

## Reinforcement Learning with Lookahead InformationFairPlay Joint Team, CREST, ENSAE Paris

nadav.merlis@ensae.fr

###### Abstract

We study reinforcement learning (RL) problems in which agents observe the reward or transition realizations at their current state _before deciding which action to take_. Such observations are available in many applications, including transactions, navigation and more. When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward. However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations. In this work, we close this gap and design provably-efficient learning algorithms able to incorporate lookahead information. To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations. We prove that our algorithms achieve tight regret versus a baseline that also has access to lookahead information - linearly increasing the amount of collected reward compared to agents that cannot handle lookahead information.

## 1 Introduction

In reinforcement learning (RL), agents sequentially interact with a changing environment, aiming to collect as much reward as possible. While performing actions that yield immediate rewards is enticing, agents must also bear in mind that actions influence the state of the environment, affecting the potential reward that could be collected in future steps. When the environment is unknown, agents also need to balance reward maximization based on previous data and exploration - gathering of data that might improve future reward collection.

In the standard interaction model, at each timestep, agents first choose an action and only then observe its outcome on the rewards and state dynamics. As such, agents can only maximize the expected rewards, collected through the expected dynamics. Yet, in many applications, some information on the immediate outcome of actions is known _before_ actions are performed. For example, when agents interact through transactions, prices and traded goods are usually agreed upon before performing any exchange ('reward information'). Alternatively, in navigation problems, nearby traffic information is known to the agent before choosing which path to go through ('transition information').

In a recent work, Merlis et al. (2024) shows that even for agents with full statistical knowledge of the environment, such 'lookahead' information can drastically increase the reward collected by agents - by a multiplicative factor of up to \(AH\) when immediate rewards are revealed in advance and \(A^{H/2}\) when observing the immediate future transitions.1 Intuitively, agents do not only gain from instantaneously using this information - they can also adapt their planning to account for lookahead information being revealed in subsequent states, significantly increasing their future values. However, the work of Merlis et al. (2024) only tackles planning settings in which the model is known and does not provide algorithms or guarantees when interacting with unknown environments.

In this work, we aim to design provably-efficient agents that learn how to interact when given immediate ('one-step lookahead') reward or transition information before choosing an action, under the episodic tabular Markov Decision Process model. While such information can always be embedded into the state of the environment, the state space becomes exponential at best, and continuous at worst, rendering most theoretically-guaranteed approaches both computationally and statistically intractable. To alleviate this, we start by deriving dynamic programming ('Bellman') equations _in the original state space_ that characterize the optimal lookahead policies. Inspired by these update rules, we present two variants to the MVP algorithm (Zhang et al., 2021) that allow incorporating either reward or transition lookahead. In particular, we suggest a planning procedure that uses the empirical distribution of the reward/transition observations (instead of the estimated expectations), which might also be applied to other complex settings. We prove that these algorithms achieve tight regret bounds of \(}SAK}\) and \(}SK}(+)\) after \(K\) episodes (for reward and transition lookahead, respectively), compared to a stronger baseline that also has access to lookahead information. As such, they can collect significantly more rewards than vanilla RL algorithms.

**Outline.** We formally define RL problems with reward/transition lookahead in Section 2 and further discuss the differences between our setting and standard RL problems in Section 3. Then, we present our results in two complementary sections: Section 4 analyzes reward lookahead while Section 5 analyzes transition lookahead. We end with conclusions and future directions in Section 6.

**Related Work.**  Problems with varying lookahead information have been extensively studied in control, with model predictive control (MPC, Camacho et al., 2007) as the most notable example. Conceptually, when interacting with an environment that might be too complex or hard to model, it is oftentimes convenient to use a simpler model that allows accurately predicting its behavior just in the near future. MPC uses such models to repeatedly update its policy using short-term planning. In some cases, the utilized future predictions consist of additive perturbations to the dynamics (Yu et al., 2020), while other cases involve more general future predictions on the model behavior (Li et al., 2019; Zhang et al., 2021; Lin et al., 2021; Liu et al., 2022). To the best of our knowledge, these studies focus on comparing the performance of the controller to one with full future information (and thus, linear regret is inevitable), sometimes also considering prediction errors. They do not, however, attempt to learn the predictions. In contrast, we estimate the reward/transition distributions and leverage them to better plan, thus increasing the value gained by the agent. In addition, these works focus on continuous (mostly linear) control problems, whereas we study tabular settings; results from any one of these settings cannot be directly applied to the other.

In RL, lookahead is mostly used as a planning tool; namely, agents test the possible outcomes after performing multiple steps to decide which actions to take or to better estimate the value (Tamar et al., 2017; Efroni et al., 2019, 2020; Moerland et al., 2020; Rosenberg et al., 2023; El Shar and Jiang, 2020; Biedenkapp et al., 2021; Huang et al., 2019). Specifically, the future value at the end of the lookahead is often estimated using rollouts, and a longer lookahead is more robust to suboptimality of the rollout policy (Bertsekas, 2023). However, when agents actually interact with the environment, no additional lookahead information is observed. One notable exception is (Merlis et al., 2024), which analyzes the potential value increase due to multi-step reward lookahead information (and briefly mentions transition lookahead). However, they only tackle planning settings, where the model is known, and do not study learning. In this work, we continue a long line of literature on regret analysis for tabular RL (Jaksch et al., 2010; Jin et al., 2018; Dann et al., 2019; Zanette and Brunskill, 2019; Efroni et al., 2019, 2021; Simchowitz and Jamieson, 2019; Zhang et al., 2021, 2023). Yet, we are not aware of any existing results on regret minimization with reward or transition lookahead information.

Finally, various applications that involve one-step lookahead information have been previously studied. The most notable ones are prophet problems (Correa et al., 2019), where one-step reward lookahead is obtained, and the Canadian traveler problem with resampling (Nikolova and Karger, 2008), which can be formulated through one-step transition lookahead. We discuss the relation to these problems and the relevant existing results when analyzing each type of feedback, and also discuss the relation between transition lookahead and stochastic action sets (Boutilier et al., 2018).

## 2 Setting and Notations

We study episodic tabular Markov Decision Processes (MDPs), defined by the tuple \(=(,,H,P,)\), where \(\) is the state space (of size \(S\)), \(\) is the action space (of size \(A\)) and \(H\) is the interaction horizon. At each timestep \(h\{1,,H\}[H]\) of an episode \(k[K]\), an agent, located in state \(s_{h}^{k}\), chooses an action \(a_{h}^{k}\) and obtains a reward \(R_{h}^{k}=R_{h}(s_{h}^{k},a_{h}^{k})_{h}(s_{h}^{k},a_{h}^{k})\). We assume that the rewards are supported by \(\) and of expectations \(r_{h}(s,a)\). Afterward, the environment transitions to a state \(s_{h+1}^{k} P_{h}(|s_{h}^{k},a_{h}^{k})\) and the interaction continues until the end of the episode. We use the notation \(_{h}(s)\) (or \(^{} P_{h}(s)\)) to denote reward (next-state) samples for all actions simultaneously at step \(h\) and state \(s\) and assume independence between different timesteps.2 On the other hand, samples from different actions at a specific state/timestep are not necessarily independent.

Reward Lookahead.With one-step reward lookahead at timestep \(h\) and state \(s\), agents first observe the rewards for all actions \(_{h}(s)\{R_{h}(s,a)\}_{a}\) and only then choose an action to perform. Formally, we define the set of reward lookahead policies as \(^{R}=\{:[H]^{A}_{}\}\), where \(_{}\) is the probability simplex, and denote \(a_{h}=_{h}(s_{h},_{h})\). The value of a reward lookahead agent is the cumulative rewards gathered by it starting at timestep \(h\) and state \(s\), denoted by

\[V_{h}^{R,}(s)=_{t=h}^{H}R_{t}(s_{t},_{t}(s_{t}, {R}_{t}(s_{t}))|s_{h}=s.\]

We also define the optimal reward lookahead value to be \(V_{h}^{R,\,}(s)=_{^{R}}V_{h}^{R,}(s)\). When interacting with an unknown environment for \(K\) episodes, agents sequentially choose reward lookahead policies \(^{k}^{R}\) based on all historical information and are measured by their regret,

\[^{R}(K)=_{k=1}^{K}V_{1}^{R,}(s_{1}^{k})-V_{1}^{R, ^{k}}(s_{1}^{k}).\]

We allow the initial state of each episode \(s_{1}^{k}\) to be arbitrarily chosen.

Transition Lookahead.Denoting \(s_{h+1}^{}(s,a)\), the future state when playing action \(a\) at step \(h\) and state \(s\), one-step transition lookahead agents observe \(_{h+1}^{}(s)\{s_{h+1}^{}(s,a)\}_{a }\) before acting. The set of transition lookahead agents is denoted by \(^{T}=\{:[H]^{A}_{ }\}\) with values

\[V_{h}^{T,}(s)=_{t=h}^{H}R_{t}(s_{t},_{t}(s_{t}, {s}_{t+1}^{}(s_{t})))|s_{h}=s.\]

The optimal value is \(V_{h}^{T,\,}(s)=_{^{T}}V_{h}^{T,\,}(s)\), and we similarly define the regret versus optimal transition lookahead agents as \(^{T}(K)=_{k=1}^{K}V_{1}^{T,}(s_{1}^{k})-V_{1}^{T, ^{k}}(s_{1}^{k})\).

When the type of lookahead is clear from the context, we sometimes denote values by \(V_{h}^{}\) and \(V_{h}^{}\).

Other Notations.For any \(p_{n}\) and \(V^{n}\), we define \(_{p}(V)=_{i=1}^{n}p_{i}V_{i}^{2}-_{i=1}^{n}p_{i}V_{ i}^{2}\). Also, given a transition kernel \(P\) and a vector \(V^{S}\), we let \(PV(s,a)=_{s^{}}P(s^{}|s,a)V(s^{})\) and similarly define it for value or transition kernel differences. We denote by \(n_{h}^{k}(s,a)\), the number of times the pair \((s,a)\) was visited at timestep \(h\) up to episode \(k\) (inclusive) and similarly denote \(n_{h}^{k}(s)=_{a}n_{h}^{k}(s,a)\). We also let \(_{h}^{k}(s,a)=^{k}(s,a)}_{k^{}=1}^{k} s_{h}^{k^{}}=s,a_{h}^{k^{}}=a}R_{h}^{k^{}}\) and \(_{h}(s^{}|s,a)=^{k}(s,a)}_{k^{}=1}^{k} s_{h}^{k^{}}=s,a_{h}^{k^{}}=a,s_{h+1}^{k^{ }}=s^{}}\) be the empirical expected rewards and transition kernel at \((s_{h},a_{h})=(s,a)\) using data up to episode \(k\) and assume they are initialized to be zero. Finally, we denote by \(}_{h}^{k}(s)\), the empirical reward distribution across all actions, and use \(_{h}^{k}(s)\) to denote the empirical joint next-state distribution for all actions. In particular, if \(k_{i}\) is the \(i^{th}\) episode where \(s\) was visited at step \(h\), to sample \(}_{h}^{k}(s)\), we uniformly sample \(i Un_{h}^{k}(s)\) and return \(=R_{h}^{k_{i}}(s,a)}_{a}\). A sample \(^{}_{h}^{k}(s)\) similarly returns \(^{}=s_{h+1}^{ k_{i}}(s,a)}_{a}\).

When we want to indicate the distribution used to calculate an expectation, we sometimes state it in a subscript, e.g., write \(E_{_{h}(s)}[R(a)]\) to indicate that \(R(a)_{h}(s,a)\) or use \(_{}\) to emphasize that all distributions are according to an environment \(\). In this paper, \(\)-notation only hides absolute constants while \(}\) hides factors of \((S,A,H,K,)\). We also use the notation \(a b=\{a,b\}\).

## 3 Comparing the Values of Lookahead Agents and Vanilla RL agents

In the classic RL formulation (e.g., Azar et al., 2017), agents only observe the reward and transition after performing an action and aim to maximize the 'no-lookahead' value, defined by

\[V_{h}^{}(s)=\!_{t=h}^{H}r_{t}(s_{t},_{t}(s_{t})|s_{ h}=s,\]

where \(^{}=\{:[H]_{}\}\) is a Markovian policy. The optimal value is \(V_{h}^{no}(s)=_{^{}}V_{h}^{}(s)\) and the regret is classically defined as \((K)=_{k=1}^{K}V_{1}^{no}(s_{1}^{k})-V_{1}^{^{k}}(s_ {1}^{k})\).

By definition, the set of lookahead policies also includes all Markovian policies (since agents are not obliged to use reward/transition information), so the optimal lookahead values are always larger than their no-lookahead counterpart. In other words, denoting the value gain due to lookahead information by \(G^{R}(s)=V_{1}^{R,*}(s)-V_{1}^{no}(s)\) and \(G^{T}(s)=V_{1}^{T,*}(s)-V_{1}^{no}(s)\), it holds that \(G^{R}(s),G^{T}(s) 0\). In terms of regret, for any fixed algorithm, we can also write

\[(K)=^{R}(K)-_{k=1}^{K}G^{R}(s_{1}^{k})=^{T}(K)-_{k=1}^{K}G^{T}(s_{1}^{k}).\]

As the value gains are non-negative, it directly implies that any regret bound w.r.t. the lookahead value also leads to the same bound for the standard regret. Even more so, in most cases, lookahead information leads to a strict improvement in the value, that is, \(G^{R}(s),G^{T}(s) G_{0}>0\). When this happens, any algorithm with sub-linear lookahead regret enjoys a _negative linear_ standard regret:

_If \(^{R}(K)=o(K)\) and \(G^{R}(s_{1}^{k}) G_{0}\) for all \(k[K]\), then \((K)-G_{0}K+o(K)\)._

The same also holds for transition lookahead. Conversely, any agent that suffers positive standard regret will suffer linear regret compared to the best lookahead agent, i.e.,

_If \((K) 0\) and \(G^{R}(s_{1}^{k}) G_{0}\) for all \(k[K]\), then \(^{R}(K) G_{0}K\)._

Notably, any agent that does not use lookahead information will suffer linear lookahead regret in any such environment. We now present two illustrative examples for environments where the lookahead value gain is significant, one for reward lookahead and another for transition lookahead.

Reward lookahead.Consider a simple 2-state environment, depicted in Figure 1. Starting at \(s_{i}\), agents can either stay there by playing \(a_{1}\), earning no reward, or play any other action and move to the absorbing \(s_{f}\), obtaining a Bernoulli reward \(Ber(}{{(A-1)H}})\). Actions in the terminal state \(s_{f}\) yield no reward. Without observing the rewards, agents will arbitrarily move from \(s_{i}\) to \(s_{f}\), obtaining a reward \(V^{no}=}{{(A-1)H}}\) in expectation. On the other hand, when agents observe the rewards before acting, they should move from \(s_{i}\) to \(s_{f}\) only if a reward was realized for some action (and otherwise, stay in \(s_{i}\) by playing \(a_{1}\)). Such agents will have \((A-1)H\) opportunities to observe a unit reward across all timesteps and actions, collecting in expectation \(V^{R,*}=(1-}{{(A-1)H}})^{(A-1)H} 1-}{{e}}\). In other words, just by observing the rewards before acting, the agent's value multiplicatively increases by almost \(V^{R,*}/V^{no} AH\). Moreover, the additive value gain is \(G^{R} 1-}{{e}}\), so sub-linear lookahead regret with reward information results with a negatively-linear standard regret of \((K)-(1-}{{e}})K\).

Transition lookahead.Consider a chain of \(H/2\) states (also described in further detail at Appendix C.9 and depicted at Figure 2). In each state, one action deterministically keeps the agent in its

Figure 1: Two-state prophet-like problem

current state, while all other actions move the agent one state forward w.p. \(1/A\), but lead to a terminal non-rewarding state otherwise. If the reward is located at the end of the chain, any standard RL agent can collect it only at an exponentially low probability. On the other hand, transition lookahead agents would move forward only if there is an action that allows it while staying at their current state otherwise; such agents will collect the rewards at the end of the chain with constant probability. More specifically, any no-lookahead agent can collect at most \(V^{no}=(HA^{-H/2})\) rewards, while transition lookahead agents can collect \(V^{T,*}=(H)\); as such, lookahead agents achieve exponential increase in value, and sublinear regret versus the best lookahead agent will yield a standard regret of \((K)-HK\).

In the following sections, we will present agents that are guaranteed to always achieve sublinear regret compared to the best lookahead agent.

## 4 Planning and Learning with One-Step Reward Lookahead

In this section, we analyze RL settings with one-step reward lookahead, in which immediate rewards are observed before choosing an action. One well-known example of this situation is the prophet problem (Correa et al., 2019), where an agent sequentially observes values from known distributions. Upon observing a value, the agent decides whether to take it as a reward and stop the interaction, or discard it and continue to observe more values. This problem has numerous applications and extensions concerning auctions and posted-price mechanisms (Correa et al., 2017). As shown in (Merlis et al., 2024), it is critical to observe the distribution values before taking a decision; otherwise, the agent's revenue can decrease by a factor of \(H\). Notably, the example presented in Figure 1 is a small variant of the prophet problem, where the agent can either take one of \(A-1\) values and finish the interaction or discard them and continue playing by staying at \(s_{i}\); we showed that for this example, the lookahead information increases the value by a factor of \(V^{R,*}/V^{no} AH\).

The most natural way to tackle this setting is to extend (augment) the state space to contain the observed rewards; this way, we transition from a state and reward observations to a new state with new reward observations and return to the vanilla MDP formulation. However, this comes at a great cost. Even for Bernoulli rewards, there are \(2^{A}\) possible reward combinations at any given state, and the augmentation increases the state space by this factor - leading to an exponentially-large state space. Even worse, for continuous rewards, the augmented state space becomes continuous, and any performance guarantees that depend on the size of the state space immediately become vacuous. Hence, algorithms that naively use this reduction are expected to be both computationally and statistically intractable. We refer to Appendix B.2 for further details on one such augmentation.

We take a different approach and derive Bellman equations for this setting in the _original state space_.

**Proposition 1**.: _The optimal value of one-step reward lookahead agents satisfies_

\[V^{R,*}_{H+1}(s)=0,  s,\] \[V^{R,*}_{h}(s)=_{_{h}(s)} }{}R_{h}(s,a)+_{s^{} }P_{h}(s^{}|s,a)V^{R,*}_{h+1}(s^{})},  s,h[H].\]

_Also, given reward observations \(=\{R(a)\}_{a}\) at state \(s\) and step \(h\), the optimal policy is_

\[^{*}_{h}(s,)}{}R(a)+ _{s^{}}P_{h}(s^{}|s,a)V^{R,*}_{h+1}(s^{}) }.\]

We prove Proposition 1 in Appendix B.2, where we present an equivalent environment with extended state space in which one could apply the standard Bellman equations (Puterman, 2014) to calculate the value with reward lookahead. In contrast to the previously discussed augmentation approach, we find it more convenient to divide the augmentation into two steps - at odd steps \(2h-1\), the augmented environment would be in a state \(s_{h}\), while at even steps \(2h\), the state is \(s_{h}_{h}\). Doing so creates an overlap between the values of the original and augmented environments at odd steps, simplifying the proofs. We also use this augmentation to prove a variant of the law of total variance (LTV, e.g. Azar et al., 2017) and a value-difference lemma (e.g. Efroni et al., 2019).

We remark that calculating the exact value is not always tractable - even for \(S=H=1\) (bandit problems) and Gaussian rewards, Proposition 1 requires calculating the expectation of the maximum of Gaussian random variables, which does not admit any simple closed-form solution. On the other hand, these equations allow approximating the value by using reward samples - in the following, we show that it can be used to achieve tight regret bounds when the environment is unknown.

### Regret-Minimization with Reward Lookahead

We now present a tractable algorithm that achieves tight regret bounds with one-step reward lookahead. Specifically, we modify the Monotonic Value Propagation (MVP) algorithm (Zhang et al., 2021) to perform planning using the _empirical reward distributions_ - instead of using the empirical reward expectations. To compensate for transition uncertainty, we add a transition bonus that uses the variance of the optimistic next-state values (w.r.t. the empirical transition kernel), designed to be monotone in the future value. Such construction permits using the variance of optimistic values for the bonus calculation while being able to later replace it with the variance of the optimal value (see discussion in Zhang et al. 2021). A reward bonus is used for the value calculation, but does not affect the action choice in the current state. Intuitively, this is because we get the same amount of information for all the actions of a state, so they have the same level of uncertainty - there is no need for bonuses to encourage reward exploration at the action level.

A high-level description of the algorithm is presented in Algorithm 1, while the full algorithm and its bonuses are stated in Appendix B.3. Notice that the planning requires calculating the expected maximum using the empirical distribution, whose support always contains at most \(K\) elements, so both the memory and computations are polynomial. The algorithm ensures the following guarantees:

```
1:Require:\((0,1)\), bonuses \(b^{r}_{k,h}(s),b^{p}_{k,h}(s,a)\)
2:for\(k=1,2,...\)do
3: Initialize \(^{k}_{H+1}(s)=0\)
4:for\(h=H,H-1,..,1\)do
5: Calculate the truncated values for all \(s\) \[^{k}_{h}(s)=_{}^{k-1 }_{h}(s)}_{a}R(a)+b^{p}_{k,h}(s,a)+ ^{k-1}_{h}^{k}_{h+1}(s,a)}+b^{r}_{k,h}(s),H}\]
6:endfor
7:for\(h=1,2, H\)do
8: Observe \(s^{k}_{h}\) and \(R^{k}_{h}(s^{k}_{h},a)\) for all \(a\)
9: Play an action \(a^{k}_{h}_{a}R^{k}_{h}(s^{k}_{h},a)+b^{p}_{ k,h}(s^{k}_{h},a)+^{k-1}_{h}^{k}_{h+1}(s^{k}_{h},a)}\)
10: Collect the reward \(R^{k}_{h}(s^{k}_{h},a^{k}_{h})\) and transition to the next state \(s^{k}_{h+1} P_{h}(|s^{k}_{h},a^{k}_{h})\)
11:endfor
12:endfor ```

**Algorithm 1** Monotonic Value Propagation with Reward Lookahead (MVP-RL)

**Theorem 1**.: _When running MVP-RL, with probability at least \(1-\) uniformly for all \(K 1\), it holds that \(^{R}(K)SAK}+H^{3}S^{2}A^{2}\)._

See proof in Appendix B.7. Remarkably, our upper bound matches the standard lower bound for episodic RL of \(SAK}\)(Domingues et al., 2021) up to log-factors; this lower bound is proved for known deterministic rewards, so in particular, it also holds for problems with reward lookahead.

To our knowledge, the only comparable bounds in settings with reward lookahead were proven to prophet problems; as agents observe (up to) \(n\) distributions at a fixed order, it can be formulated as a deterministic chain-like MDP, with \(H=n\), \(S=n+1\) and \(A=2\). Agents start at the head of the chain and can either advance without collecting a reward or collect the observed reward and move to a terminal non-rewarding state (for more details, see Merlis et al. 2024). For this problem, (Gatmiry et al., 2024) proved a regret bound of \(}(n^{3})\) (albeit requiring a weaker form of feedback), and (Agarwal et al., 2023) proved a bound of \(}(n)\) - slightly better than ours, but heavily relies on the ability to control which distributions to observe, which is a specific instance of deterministic transitions. We are unaware of any previous results that cover general Markovian dynamics.

### Proof Concepts

When analyzing the regret of RL algorithms, a key step usually involves bounding the difference between the value of a policy in two different environments ('value-difference lemma'). In particular, for a given policy \(^{k}\), many algorithms maintain a confidence interval on the value \(V_{h}^{^{k}}(s)[V_{h}^{k}(s),_{h}^{k}(s)]\), calculated based on optimistic and pessimistic MDPs that use the empirical model with bonuses/penalties (Dann et al., 2019; Zanette and Brunskill, 2019; Efroni et al., 2021). Then, the instantaneous regret (without lookahead) is bounded using the optimistic values by

\[_{h}^{k}(s_{h})-V_{h}^{^{k}}(s_{h}) =(_{h}^{k-1}(s_{h},a_{h})-r_{h}(s_{h},a_{h}))+ (_{h}^{k-1}-P_{h})_{h}^{k}(s_{h},a_{h})\] \[+P_{h}_{h+1}^{k}-V_{h+1}^{^{k}}(s_{h },a_{h})+,\]

while the pessimistic values are used either as part of the bonuses or while bounding them. However, when trying to perform a similar decomposition with reward lookahead, we do not have the difference of expected rewards, but rather terms of the form

\[_{}_{h}^{k-1}(s_{h})}R(^{k}_{h} (s_{h},))-_{_{h}(s_{h})}R( ^{k}_{h}(s_{h},))\]

(see, e.g., the last term of Lemma 4 in the appendix). As the action can be an arbitrary function of the reward realization, this term is extremely challenging to bound. For example, one could couple both distributions while trying to relate this error term to a Wasserstein distance between the empirical and real reward distribution; however, such distances exhibit much slower error rates than standard mean estimation (Fournier and Guillin, 2015). Instead, we follow a different approach and show that uniformly for all possible expected next-state values \(V[0,H]^{A}\) (as a function of the action at a given state), it holds w.h.p. that

\[_{}_{h}^{k-1}(s)} _{a}R(a)+V(s,a)} -_{_{h}(s)}_{a} R(a)+V(s,a)}\] \[}{n_{h}^{k-1}(s) 1}}.\] (1)

Throughout the proof, whenever we face an expectation w.r.t. the empirical rewards, we reformulate the expression to fit the form of Equation (1) and use it as a 'change of measure' tool. We remark that while this confidence interval admits an extra \(A\)-factor compared to standard bounds, the counts only depend on the visits to the state (and not to the state-action), which compensates for this factor.

The choice of MVP for the bonus is similarly motivated - unlike some other bonuses (e.g., Zanette and Brunskill, 2019), MVP does not require pessimistic values - either in the bonus itself or in its analysis. In contrast to the optimistic ones, the pessimistic values are not calculated via value iteration, but rather by following the policy \(^{k}\) in the pessimistic environment. As such, they cannot be easily manipulated to fit the form in Equation (1).

The analysis of the transitions adapts the techniques in (Efroni et al., 2021), while requiring extra care in handling the dependence of actions in the rewards.

## 5 Reinforcement Learning with One-Step Transition Lookahead

We now move to analyzing problems with one-step transition lookahead, where the resulting next state due to playing any of the actions is revealed before deciding which action to play. For example, consider the stochastic Canadian traveler problem with resampling (Nikolova and Karger, 2008; Boutilier et al., 2018). In this problem, an agent wants to navigate on a graph as fast as possible from a source to a target, but observes which edges at a node are available only upon reaching this node. When edge availability is stochastic and resampled every time a node is visited, this is a clear case of one-step transition lookahead, as the information on the availability of edges is given before trying to traverse them. The example in Section 3 and Appendix C.9 is one possible formulation of this problem on a chain - agents are awarded for arriving at the end of the chain as fast as possible, but trying to use a non-existing edge results with termination. We showed that in this particular instance, the lookahead value is exponentially larger than the standard value, and any lookahead agent with low regret would greatly surpass no-lookahead agents.

As with reward lookahead, the future states for all actions can be embedded into the state, but doing so increases the size of the state space by a factor of \(S^{A}\), again making this approach intractable (see Appendix C.2 for an example for such an extension). We once more show that this is not necessary; the transition-lookahead optimal values can be calculated using the following Bellman equations:

**Proposition 2**.: _The optimal value of one-step transition lookahead agents satisfies_

\[V_{H+1}^{T,*}(s)=0,  s,\] \[V_{h}^{T,*}(s)=_{^{} P_{h}(s)} _{a}r_{h}(s,a)+V_{h+1}^{T,*}(s^{}(s,a)) },  s,h[H].\]

_Also, given next-state observations \(^{}=\{s^{}(a)\}_{a}\) at state \(s\) and step \(h\), the optimal policy is_

\[_{h}^{*}(s,^{})*{arg\,max}_{a} r_{h}(s,a)+V_{h+1}^{T,*}(s^{}(a))}.\]

The proof can be found at Appendix C.2 and again relies on augmenting the state space to incorporate the transitions; this time, we divide the episode into odd steps whose extended state is \(s_{h}^{}_{0}\) (for an arbitrary fixed \(^{}_{0}^{A}\)) and even steps with the state \(s_{h}^{}_{h+1}\). Beyond planning, this again allows proving a variant of the LTV and of a value-difference lemma.

One important insight is that the policy \(_{h}^{*}(s,^{})\) admits the form of a _list_. Namely, consider the values \(V_{h}^{*}(s,s^{},a)=r_{h}(s,a)+V_{h+1}^{T,*}(s^{})\) and assume some ordering of next-state-action pairs \(\{(s^{}_{a},a_{i})\}_{i=1}^{SA}\) such that \(V_{h}^{*}(s,s^{}_{1},a_{1}) V_{h}^{*}(s,s^{}_{SA},a_ {SA})\). Then, an optimal policy would look at all realized pairs \((s^{}(a),a)\) and play the action with the highest location in this list. We refer the readers to Appendix C.4 for an additional discussion on list representations in transition lookahead.

Similar results could be achieved through a reduction to RL problems with stochastic action sets (Boutilier et al., 2018). There, at every round, a subset of base actions is sampled, and only these actions are available to the agent. In particular, one could sample \(A\) actions of the form \((s^{},a)\) and impose a deterministic transition to \(s^{}\) given this extended action. However, since every original action must be sampled exactly once, this sampling procedure creates a dependence between pairs even when next-states at different actions are independent, adding unnecessary complications. We show that when transitions are independent between states, the expectation in Proposition 2 can be efficiently calculated (see Appendix C.4.1 for details), and otherwise, it can be approximated through sampling, as we do in learning settings.

### Regret-Minimization with Transition Lookahead

Relying on similar principals as with reward lookahead, we now present MVP-TL, an adaptation of MVP to settings with one-step transition lookahead (summarized in Algorithm 2; the full details can be found at Appendix C.3). This time, we estimate the empirical expected reward and add a standard Hoeffding-like reward bonus, while performing planning using samples from the _empirical joint distribution_ of the next-state for all the actions simultaneously. A variance-based transition bonus is added to the values; though this time, the variance also incorporates the rewards, namely

\[b_{k,h}^{p}(s)_{^{}_{h}^{k-1}(s)}(_{h}^{k}(s,^{}))}{n_{h}^{k-1}(s) 1}}, _{h}^{k}(s,^{})=_{a} _{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+_{h+1}^{k}(s^{}(a)}.\]

The motivation for this modification is the technical challenges described in Section 4.2, in the context of reward lookahead. For reward lookahead, we analyzed a value term that included both the rewards and next-state values, and used concentration arguments to move from the empirical reward distribution to the real one. For transition lookahead, similar values are analyzed, but we require variance-based concentration to obtain tighter regret bounds (Azar et al., 2017), so this variance naturally arises. The bonus is again designed to be monotone, as in the original MVP algorithm, and does not affect the immediate action choice - only the optimistic lookahead value. As before, the planning relies on sampling the next-state observations at previous episodes, and so it is polynomial, even if the precise joint distribution is complex. The algorithm enjoys the following regret bounds:

**Theorem 2**.: _When running MVP-TL with probability at least \(1-\) uniformly for all \(K 1\), it holds that \(^{T}(K)SK}+ +H^{3}S^{4}A^{3}^{2}\)._See proof in Appendix C.8. For transition lookahead, the regret bounds we provide exhibit two rates, both corresponding to a natural adaptation of known lower bounds to transition lookahead.

1. _'Bandit rate'_\((SAK})\): this is the rate due to reward stochasticity. Consider a problem where at odd timesteps \(2h-1\) and across all states, all actions have rewards of mean \(}{{2}}-\), except for one action of mean \(}{{2}}\). Assuming that the state-distribution is uniform, each such timestep forms a hard instance of a contextual bandit problem with \(S\) contexts, exhibiting a regret of \(()\)(Auer et al., 2002; Bubeck et al., 2012). Since there are \(H/2\) odd steps and we can design each step independently, the total regret would be \((H)\). The even steps can be used to'remove' the lookahead and create a uniform state distribution. To do so, we set that when taking an action at odd steps, we always transition to a fixed state \(s_{d}\). From this state, one action \(a_{1}\) leads uniformly to all states, while the rest of the actions lead to an absorbing non-rewarding state - rendering them strictly suboptimal. Thus, no-regret agents will only play \(a_{1}\), regardless of the lookahead information, and the state distribution at odd timesteps will be uniform.
2. _'Transition learning rate'_\((SK})\): recall that the vanilla RL lower bound designs a tree with \((S)\) leaves, to which agents need to navigate at the right timing (with \((H)\) options) and take the right action (out of \(A\)). While all leaves might transition agents to a rewarding state, one combination of state-action-timing has a slightly higher probability of doing so (Domingues et al., 2021). This roughly creates a bandit problem with \(SAH\) arms, constructed such that the maximal reward is \((H)\), yielding a total regret of \(H\). Now consider the following simple modification where in each leaf, only one action can lead to a reward (and the rest of the actions are 'useless' - never lead to rewards). Thus, the agent still needs to test all leaves at all timings, and so there are still \(SH\) 'arms' with a corresponding regret of \(SK}\). Moreover, to test a leaf at a certain timing, we must navigate to it, and since the agent is going to play the single useful action at the leaf, transition lookahead does not provide any additional information.

As discussed before, transition lookahead can be formulated as an RL instance with stochastic action sets. While Boutilier et al. (2018) prove that with stochastic action sets, Q-learning asymptotically converges, they provide no learning algorithm nor regret bounds. Therefore, to our knowledge, our result is the first to achieve sublinear regret with transition lookahead.

### Proof Concepts

Transition lookahead causes similar issues as reward lookahead. Hence, it is natural to apply a similar analysis approach - first, formulate the value as the expectation w.r.t. the next-state observations of the maximum of action-observation dependent values; then use uniform concentration as a 'change of measure' tool between the empirical and real next-state distribution. In particular, if \(V(s,s^{},a)\) represents the value starting from state \(s\), performing \(a\) and transitioning to \(s^{}\), one can show that for all \(V(s,,)[0,H]^{SA}\) (see Lemma 19),

\[_{^{} P_{h}^{k-1}(s)}_{ a}V(s,s^{}(a),a)-_{^{} P_{h}(s)}_{ a}V(s,s^{}(a),a)\\ _{^{}  P_{h}^{k-1}(s)}_{a}V(s,s^{}(a),a)}{n_{h}^{k-1}(s) 1}},\] (2)

where the variance term stems from using a Bernstein-like concentration bound. However, in contrast to the reward lookahead, the \(\)-factor propagates to the dominant term of the regret, so pursuing this approach would lead to a worse regret bound of \(}S^{2}AK}\).

To avoid this, we pinpoint the two locations where this change of measure is needed - the proof that \(_{h}^{k}\) is optimistic and the regret decomposition - and make sure to perform this change of measure only on a single value \(V_{h}^{*}(s,s^{},a)=r_{h}(s,a)+V_{h+1}^{*}(s^{})\), mitigating the need to cover all possible values and removing the additional \(\)-factor. However, doing so leaves us with a residual term. Defining \(V_{h}^{*}(s,^{})=_{a}\{V_{h}^{*}(s,s^{}(a ),a)\}\) and assuming a similar optimistic value \(_{h}^{k}(s,^{})\), this term is of the form

\[_{^{}_{h}^{k-1}(s)}_{h}^{k}(s, ^{})-V_{h}^{*}(s,^{})-_{^{ } P_{h}(s)}_{h}^{k}(s,^{})-V_{h}^{*}(s, {s}^{}).\]

While similar terms have been analyzed before (e.g., Zanette and Brunskill, 2019; Efroni et al., 2021), the analysis leads to a constant regret term that depends on the support of the distribution in question; in our case, it is the distribution over all possible next-states - of cardinality \(S^{A}\). Therefore, following the same derivation would lead to an exponential additive regret term.

We overcome it by utilizing the fact that both the optimistic policy and the optimal one decide which action to take according to a list of next-state-actions \((s^{},a)\). In other words, instead of looking at the next-state \(^{}\) (with \(S^{A}\) possible values) to determine a value, we look at the highest-ranked realized pair \((s^{},a)\) in the list that corresponds to the policy that induces the value (with \(SA\) possible rankings). Since we have two values, we need to calculate the probability of being at a certain list location for both \(^{k}\) and \(^{*}\), but the cardinality of this space is \((SA)^{2}\): polynomial and not exponential.

## 6 Conclusions and Future Work

In this work, we presented an RL setting in which immediate rewards or transitions are observed before actions are chosen. We showed how to design provably and computationally efficient algorithms for this setting that achieve tight regret bounds versus a strong baseline that also uses lookahead information. Our algorithms rely on estimating the distribution of the reward or transition observations, a concept that might be utilized in other settings. In particular, we believe that our techniques for transition lookahead could be extended to RL problems with stochastic action sets (Boutilier et al., 2018), but leave this for future work.

One natural extension to our work would be to consider multi-step lookahead information - observing the transition/rewards \(L\) steps in advance. We conjecture that from a statistical point of view, a similar algorithmic approach that samples from the empirical observation distribution would be efficient. However, it is not clear how to perform efficient planning with such feedback.

Another possible direction would be to derive model-free algorithms (Jin et al., 2018), with the aim to improve the computation efficiency of the solutions; our model-based algorithms require at most \((KS^{2}AH)\) computations per episode due to the planning stage, while model-free algorithms might potentially allow just \((AH)\) computations per episode.

On the practical side, previous works presented RL algorithms that utilize/estimate a world model with multi-step lookahead to perform planning and learning (Schrittwieser et al., 2020; Chung et al., 2024), aiming to achieve the optimal no-lookahead value. For some of these approaches, it is quite natural to replace the simulated world behavior with lookahead information on the real future realization. We leave this adaptation and evaluation to future studies.

Finally, the notion of lookahead could be studied in various other decision-making settings (e.g., linear MDPs Jin et al. 2020) and can also be generalized to situations where lookahead information can be queried under some budget constraints (Efroni et al., 2021) or when agents only observe noisy lookahead predictions; we leave these problems for future research.