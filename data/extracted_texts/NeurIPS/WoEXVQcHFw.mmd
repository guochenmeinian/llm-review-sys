# Gliding over the Pareto Front with Uniform Designs

Xiaoyuan Zhang\({}^{a}\), Genghui Li \({}^{b}\), Xi Lin \({}^{a}\), Yichi Zhang\({}^{c}\), Yifan Chen\({}^{d}\), Qingfu Zhang\({}^{a}\)

\({}^{a}\) Department of Computer Science, City University of Hong Kong;

\({}^{b}\) College of Computer Science and Software Engineering, Shenzhen University

\({}^{c}\) Department of Statistics, Indiana University Bloomingtom

\({}^{d}\) Departments of Mathematics and Computer Science, Hong Kong Baptist University

Corresponding to Qingfu Zhang. Contact: {xzhang2523-c@my.qingfu.zhang@}cityu.edu.hk. The source code is integrated into the LibMOON library, available at https://github.com/xzhang2523/libmoon.

###### Abstract

Multiobjective optimization (MOO) plays a critical role in various real-world domains. A major challenge therein is generating \(K\) uniform Pareto-optimal solutions to approximate the entire Pareto front. To address this issue, this paper firstly introduces _fill distance_ to evaluate the \(K\) design points, which provides a quantitative metric for the representativeness of the design. However, directly specifying the optimal design that minimizes the fill distance is nearly intractable due to the involved nested \(--\) problem structure. To address this, we propose a surrogate "max-packing" design for the fill distance design, which is easier to optimize and leads to a rate-optimal design with a fill distance at most \(4\) the minimum value. Extensive experiments on synthetic and real-world benchmarks demonstrate that our proposed paradigm efficiently produces high-quality, representative solutions and outperforms baseline MOO methods.

## 1 Introduction

Multiobjective optimization (MOO) is widely used to inform real-world decision-making across various fields, including materials science , recommendation systems , and industrial design . An MOO problem (MOP) involves optimizing multiple conflicting objectives, which can be (informally) formulated as:

\[_{}()=(f_{1}(),,f_{m}()),\] (1)

where \(m\) is the number of objectives. Equation (1) is a vector optimization problem and it does not admit a total ordering, therefore, to compare solutions, the concept of _Pareto optimality_ is introduced. A solution is called _Pareto optimal_ if no other solution \(^{}\)_dominates_ it. Domination occurs when \(f_{i}(^{}) f_{i}()\) for all \(i=1,,m\), with at least one strict inequality . In this paper, \(()\) denotes _the Pareto objective_ of a Pareto optimal solution. The set of all Pareto optimal solutions is the Pareto set (PS), and their objectives form the Pareto front (PF).

Under mild conditions, a PS or PF forms a continuous (\(m\)-1)-dim manifold containing infinitely many solutions . For a general MOP, it is intractable to precisely depict the entire PS or PF with a closed-form expression. Researchers thus turns to use a small number \(K\) of diverse Pareto optimal objectives to "represent" the entire PF. Several prior works have been focused on generating diverse

Figure 1: **Covering of a Pareto Front (PF). Eight diverse Pareto objectives are used to cover the entire PF with a small covering radius.**solutions (see Section 2), but they lack formal definitions of representability and uniformity. In this paper, we define representability as the covering radius of a size-\(K\) solution set that covers the entire PF. An illustrative example is provided in Figure 1, where eight uniformly distributed solutions cover the entire PF with a small radius.

In this paper, we first introduce the fill distance (FD) as the minimal covering radius of the Pareto objectives, which measures how well a set of discrete solutions represents the true PF. A design with a smaller covering radius is considered a better representation of the PF. Next, we demonstrate that optimizing FD is challenging due to its nested \(--\) structure (Equation (4)). To overcome this, we maximize the minimal (\(-\)) pairwise distances between Pareto objectives, which bounds the minimal covering radius up to a constant. Finally, we propose a bi-level optimization framework with a neural network to efficiently solve this \(-\) problem.

Our method, _UMOD_ (Uniform Multi-Objective optimization based on Decomposition) is within the decomposition-based MOO paradigm . To show its effectiveness, we conduct comparative evaluations against methods on complex multiobjective problems with numerous local optimas and on fairness classification problems with thousands of decision variables. The contribution of this paper can be summarized as:

1. We introduce the fill distance as a metric for a set of Pareto solutions in MOO. We prove that the optimal fill distance design serves as an upper bound for the optimal Inverted Generational Distance (IGD) design, and that the max-packing design is rate-optimal with respect to the fill distance. Therefore, the max-packing design provides an effective surrogate for the optimal fill distance design.
2. We present a practical algorithm for identifying the maximum-packing design of Pareto objectives, and formulate it as a bi-level optimization problem. To expedite the optimization process, we introduce a neural network to avoid the frequent solving of the inner-loop optimization problem. Additionally, we analyze the optimization bounds of the neural network-based approach in comparison to solving the original optimization problem directly.
3. Finally, we evaluate our approach against leading MOO methods, including evolutionary algorithms and gradient-based algorithms, on both synthetic and real-world problems. UMOD outperforms these methods in terms of both uniformity and efficiency, based on commonly used metrics in MOO.

**Notations.** In this paper, \((^{(a)},^{(b)})\) represents the Euclidean distance between vectors \(^{(a)}\) and \(^{(b)}\), with bold letters for vectors. Superscripts denote vectors, and subscripts (e.g., \(y_{i}\)) indicate elements of a vector. A PF is denoted by \(\). The objective space is \(=\{(),\}\). \(_{m}\) is the \(m\)-D preference simplex; \(_{m}=\{_{i=1}^{m}y_{i}=1,y_{i} 0,i[m]\}\), where \([m]=\{1,,m\}\). \(\) denotes a set.

## 2 Related works

In this section, we review three lines of works to generate uniform or diverse Pareto objectives. We focus our discussions on uniform/diverse _Pareto objectives_ rather than _Pareto solutions_ because our goal is to produce uniform Pareto solutions in the objective space (\(\)), not the decision space (\(\)).

### Methods to generate diverse Pareto objectives

Various MOO methods can effectively generate diverse Pareto objectives, for both _gradient-based_ and _evolution-computation (EC)-based_ frameworks. In the line of gradient-based methods, Pareto MultiTask Learning (PMTL)  generates Pareto objectives which are constrained in specific regions (sectors); MOO with Stein Variational Gradient Descent (MOO-SVGD) models objective vectors as particles, updating them through repulsive forces with a kernel function to maximize their separation; Exact Pareto Optimization (EPO)  aligns solutions with user-specific preference vectors, fostering a diverse distribution of Pareto objectives by specifying diverse preferences. For multiobjective evolutionary algorithms (MOEAs), NSGA2  introduces the crowding distance and Pareto rank to achieve a diverse distribution of Pareto objectives; MOEA/D  and its variants generate diverse Pareto objectives by leveraging the positional relationship between preference vectors and Pareto objectives; Hypervolume-based methods (e.g., SMS-MOEA ) maximize the set of solutions with the largest hypervolume both to enhance convergence and diversity. A distinction of the proposedUMOD method with the previously mentioned methods is that, for general MOPs, the distribution of the achieved Pareto objectives remains unknown, whereas the objective distribution of the proposed method has desirable properties.

### Subset selection for multiobjective optimization

Another approach to generating a size-\(K\) diverse Pareto set is subset selection. This method first generates a large number of Pareto solutions, then selects \(K\) solutions to maximize hypervolume or minimize the IGD indicator [23; 53; 9; 45]. Solving the subset selection problem, a discrete optimization problem, is generally inefficient compared with continuous optimization problems. Recently, some approaches employ greedy algorithms [9; 33; 28] to obtain approximate solutions, with _naive greedy methods_ typically providing a \((1-1/e)\) guarantee. In contrast, our method addresses a continuous optimization problem on the PF, using gradient-based techniques to solve the established optimization problem to improve both accuracy and efficiency.

### Preference adjustment methods in the decomposition-based MOO paradigm

Since the proposed method can also be classified under the preference adjustment category, we discuss the relationship between the proposed UMOD and preference/weight2 adjustment methods. The study of preference adjustment methods start from MOEA/D-AWA , where its strategy is to remove the preference corresponding to the most crowded objective and add a preference corresponding to the most sparse one. Subsequently, several preference adjustment methods have been introduced , including DEA-GNG  and MOEA/D-SOM , which utilize neural gas networks to guide the selection of preference vectors. W-MOEA/D , tw-MOEA/D , pa\(\)-MOEA/D , and MOEA/D-AWG  use mathematical models to shape the non-dominated solutions and adjust preference vectors. The proposed method differs from other preference adjustment methods in two ways: (1) it models the PF with a neural network for better accuracy and efficiency, and (2) it offers a rigorous theoretical analysis for selecting preference vectors yielding uniformity and representativeness.

## 3 Pareto solutions with uniform designs

### FD as an upper bound of IGD

We first define _fill distance_ (FD)  of a set \(\) (\(=[^{(1)},,^{(K)}]\)) and establish its relationship with the inverted generational distance (IGD) indicator  of a set \(\), a famous metric in MOO. Formally, FD and IGD are defined as follows:

**Definition 1** (FD & IGD).: \[()=_{}_{^{} }(,^{})=_{}(,),()=_{}_{ ^{}}(,^{})d,\] (2)

where \((,)\) represents the Euclidean distance. The term \(_{^{}}(,^{})\) represents the nearest distance from a point \(\) on the PF to the reference set \(\). Therefore, \(()=_{}_{^{} }(,^{})\) denotes the _covering radius_ of \(\), i.e., the largest radius within which at least one solution in \(\) covers the entire PF. However, \(()\), which represents the average distance from a point on the PF to the set \(\), lacks the clear geometric interpretation that fill distance offers. For MOO, the goal is to minimize a set of Pareto objectives, i.e., \(\), by optimizing either the FD or IGD indicator: \(_{}()\) or \(_{}()\) to reach a diverse distribution. Let the optimal sets be \(^{}\) and \(^{}\) respectively. The following theorem compares FD and IGD.

**Theorem 2** (FD as an upper bound of IGD).: \[(^{})(^{})(^{})\] (3)

The first inequality follows because \(^{}\) minimizes IGD, and the second holds because the average distance (\((^{})\)) is no greater than the maximum distance (\((^{})\)). Theorem 2 shows that \(^{}\), the optimal FD configuration, sets an upper bound for the IGD value. Since the optimal IGD configuration does not similarly bound FD, we focus on FD in this paper.

### Max-packing design as a surrogate of FD design

The minimization of FD involves solving the following nested \(--\) problem:

\[d^{}=_{}_{} _{^{}}(,^{}).\] (4)

A small \(d^{}\) suggests that the optimal configuration \(^{}\) effectively covers the PF with a low covering radius. However, this triply nested structure is challenging to optimize , so we propose using a max-packing design as a surrogate for solving Equation (4).

\[d^{}=_{}=_{ }(_{1 i<j K}(^{(i)},^{(j)}) ),\] (5)

where \(\) represents the _separation distance_ between two vectors. The optimal design \(^{}\), which solves the optimization problem (Equation (5)), is known as the max-packing design . In this paper, we show that \(^{}\) effectively optimizes FD _when the decision space is a PF_, as \(d^{}\) is bounded by \(^{}\) up to a constant factor, independent of size \(K\) in Theorem 3.

**Theorem 3** (Surrogate for minimal FD).: _Consider a connected, compact 3 PF \(\). The minimal fill distance \(d^{}\) between \(\) and a size-\(K\) design \(^{}\)/\(^{}\) will then be bounded as:_

\[d^{} d^{}(^{ }) d^{},\] (6)

_Furthermore, the fill distance between \(\) and the optimal design \(d^{}\) induced by \(^{}\) is upper bounded by \(d^{}\), which is guaranteed to be upper bounded by \(4d^{}\). \(^{}\) is thus considered a quality, rate-optimal representative for the whole PF \(\)._

The second inequality \(d^{} d^{}\) is proved by Auffray et al. , Pronzato , and we provide a tighter lower bound \(d^{}/4\), utilizing the topological property of a PF. The complete proof of Theorem 3 is left in Appendix A.1. Furthermore, under an additional strict inequality condition \(d^{}_{K}<d^{}_{K+1}\) on the PF, the max-packing design \(^{}_{K}\) serves as a \(d^{}_{K}\)-covering of \(\), which is established by Theorem 4. The subscript "\(K\)" specifically denotes the max-packing distance \(d^{}_{K}\) for a size-\(K\) design, similarly used for \(^{}_{K}\) to represent a size-\(K\) design.

**Theorem 4**.: _Consider a connected, compact PF (\(\)), with the property \(d^{}_{K}<d^{}_{K+1}\), the max-packing design \(^{}_{K}\) covers \(\) with radius of at most \(d^{}_{K}\)._

This theorem suggests that \(^{}_{K}\) can represent \(\) well since the maximal distance between any vector \(\) and \(^{}_{K}\) is bounded by \(d^{}_{K}\).

### Characterizations of a max-packing design

This section discusses key properties of the max-packing design on a PF. For bi-objective problems, \(^{}\) shows favorable properties when \(^{}\), as formalized in Theorem 5, with the proof in Appendix A.2.

**Theorem 5** (Characterization of \(^{}\) for biobjective problems).: _Let \(^{}=[^{(1)},,^{(K)}]\) be sorted by the first component, such that \(y^{(1)}_{1} y^{(K)}_{1}\). For a compact, connected \(\), \(^{}\) is characterized as follows:_

1. _Equal spacing:_ \((^{(1)},^{(2)})==(^{(K-1)},^{(K)})\)_, for_ \(K 3\)_._
2. _Endpoint alignment:_ \(^{(1)}\) _and_ \(^{(K)}\) _are two endpoints (_\(^{(1)},^{(2)}\)_) of_ \(\)_, i.e.,_ \(^{(1)}=^{(1)}=[_{}y_{1},_{ }y_{2}]\) _and_ \(^{(K)}=^{(2)}=[_{}y_{1},_{ }y_{2}]\)_, for_ \(K 2\)_._

**Remark**.: Firstly, \(^{}\), including both the starting and ending points, spans the maximum range among all designs, which is a desirable property. Secondly, equal pairwise distances between Pareto objectives yields an intuitive interpretation of uniformity. Maximizing hypervolume ensures the "equal spacing" property only for bi-objective _linear_ PFs [Theorem 4], while the max-packing design only requires the PF to be _compact and connected_.

Besides this non-asymptotical bi-objective results, we examine the asymptotic properties of \(^{}\) with Theorem 6.

**Theorem 6** (Asymptotic uniformity (Theorem. 2.1)).: _As the number of set size \(K\), the set sequence \(\{^{}_{K}\}\) weakly converge to a uniform distribution over a compact, connected \(\). Specifically, for any subset \(\) with measure-zero boundary, the proportion of points in \(^{}_{K}\) lying within \(\) converges to the proportion of \(\) occupied by \(\):_

\[_{K}^{}_{K})}{\#(^{}_{K})}=()}{ ()}=( ),\] (7)

_where \(\#\) denotes the number of points in a set, and "\(\)" denotes the volume of a set._

Theorem 6 shows that as the solution set size \(K\) grows, \(^{}_{K}\) approaches a uniform distribution. Specifically, random variable \(^{}_{K}()\), meaning \(^{}_{K}\), the categorical distribution where each \(^{}_{K}\) has probability \(1/K\), converges in distribution to \(()\).

## 4 Efficient optimization of a size-\(K\) uniform set

The original max-packing problem (Equation (5)) maximizes the minimal pairwise distance among Pareto objectives and can be reformulated as the following constrained optimization problem on the PF:

\[(_{1 i<j K}(^{(i)},^{(j)})) ^{(i)},^{(j)}.\] (8)

To constrain \(^{(i)},^{(j)}\) pairs as Pareto objectives, we solve decision variables of \(^{(i)}\)'s as the optimal solution of the following modified Tchebycheff (mTche) aggregation function (Equation (9)),

\[=}()=_{^{{}^{}}}\{-z_{i}}{_{i}}\}:[0, ]^{m-1}^{m},\] (9)

where \(\) is a reference point (\(z_{i} y_{i},, i[m]\)). This substitution is equivalent _when the optimal solution of Equation (9) is unique_, as for any Pareto objective \(\), there exists a preference \(_{m}\) such that the optimal value of Equation (9) matches \(\) (explained in Appendix A.4). Substituting Equation (9) into Equation (8) yields the following bi-level optimization problems:

\[\{ d^{}&=_{^{(1)},,^{(K)}}_{1 i<j K}(^{(i)},^{(j)})\\ ^{(k)}&=_{^{(k)}} \{^{(k)}-z_{i}}{_{i}(^{(k)})}\}, \,i[m].. d^{ }&=_{^{(1)},,^{(K )} 1 i<j K}((^{(i)}),(^{(j)})) \\ &^{(k)}[0, ]^{m-1}..\] (10)

Exact Pareto optimal solutions can also be obtained using PMGDA , which achieves faster convergence with a suitable control parameter \(\). The function \(()\) converts a "preference angle" from an angle space \([0,]^{m-1}\) into a preference vector. The conversion relationship is detailed in Appendix C.3. We use \(()\) as decision variables for easier optimization since \(()\) is constrained in a box. In the right equation, the Pareto objective is denoted as \(=()=}(())\). Various bi-level optimization methods  can be used to solve the problem (Equation (10)). For efficiency consideration, we use a gradient-based approach. Define \(=_{(i,j)}(^{(i)},^{(j)})\), where \((i^{*},j^{*})\) is the optimal pair from \(_{(i,j)}(^{(i)},^{(j)})\). After some basic algebraic calculations, \(}\) can be calculated by the following two equations:

\[^{(i^{*})}}=(^{(i^{*})})}{^{(i^{*})}}}_{ (n m)}(^{(i^{*})})-( ^{(j^{*})})}{((^{(i^{*})}),(^{(j^{*})}))}}_{(m 1)}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,Instead, we use a neural model \(_{}\) trained on historical data \((,())\) to approximate \(\), enabling efficient estimation via \(_{}}{}\). We analyze the neural model's error in Theorem 7, with full details in Appendix A.3.

**Theorem 7** (Optimization error \(_{}\) introduced by using a network).: _Let \(_{}()\) be an approximator of \(()\) such that \(||_{}()-()||\), for every \([0,]^{m-1}\), as commonly assumed in bi-level optimization, e.g., , Eq. (10). \(_{}\) is the difference between the maximum of the minimal distances calculated using the approximate function \(_{}\) and the true function \(\). Then, \(_{}\) is bounded by \(2\):_

\[_{}=|_{:(1),,^{ (K)}}_{1 i<j K}(_{}(^{(i)}),_{ }(^{(j)}))-d^{}| 2.\]

**Remark.** The error \(\), defined as \(||_{}()-()||\), is both influenced by the covering radius \(R\) of the estimated solutions \(_{}(^{(1)}),,_{}(^{(K)})\) and the fitting error \(_{}\), where \(_{}=_{k[K]}||_{}(^{(k)})- (^{(k)})||\). For any \(\), the error satisfies \(||_{}()-()|| L|| -^{(i^{})}||+_{} L R+_ {}\), where \(L\) is the Lipschitz constant of function \((_{}()-())\), \(^{(i^{})}\) is the nearest solution to \(\) among the estimated solutions, and \(R\) is the covering radius, which can be further reduced by adding more training pairs. For overparameterized networks, \(_{}\) can be considered as very small.

**Practical algorithms.** Due to the space limit, the pseudo-codes of UMOD are provided in Algorithm 1 and Algorithm 2 in Appendix C.1. Initially, \(K\) diverse preferences are generated by the Das-Dennis algorithm . Then either a decomposition-based multiobjective evolutionary algorithm or a gradient-based MOO with mTche aggregation function is employed for producing preference angle and Pareto objective pairs \((,)\). Evolutionary algorithms are preferred for problems with multiple local optimas, while gradient-based MOO methods are preferred for multiobjective machine learning (MOML) problems. Given \((,)\) pairs, a PF model \(_{}()\) is fitted by optimizing the mean square estimation error. Finally, preference angles are updated to maximize the minimal pairwise distances of Pareto objectives. These two steps are repeated alternatively until convergence.

## 5 Experiments

The experiments compare UMOD with other methods on two types of MOPs: (1) those with multiple local optimas which can be solved by MOEAs efficiently, and (2) multiobjective fairness classification neural networks as decision variables. MOEAs and multiobjective fairness problems are executed with 31/5 random seeds, separately. We employ seven indicators to assess performance of different algorithms: **(1) Hypervolume** (\(\)) , **(2) IGD** (\(\)) , **(3) Sparsity** (\(\)) , **(4) Spacing** (\(\)) , (5) Uniformity (\(\)), (6) Smooth Uniformity (\(\)), and (7) Fill distance** (\(\)). Indicators 1-2 focus on convergence and diversity for multi-objective optimization (MOO), while indicators 3-7 evaluate solution uniformity. See Appendix B.1 for more detailed expression for these indicators.

### Comparison with MOEAs

We demonstrate the effectiveness of our proposed method across a diverse set of MOEA benchmark problems, including the ZDT4, DTLZ 5, and real-world testing problems . Real-world testing problems include: four-bar truss design (RE21), reinforced concrete beam design (RE22), disc brake design (RE33), rocket injector design (RE37), car side impact design (RE41), and conceptual marine design (RE42). Notably, RE41 and RE42 are complex four-objective problem having a large objective spaces. The prefix "RE" denotes this problem is a real-world one.

Baseline MOEAs include: _MOEA/D_, a decomposition-based approach; (2) _MOEA/D-AWA_,

Figure 2: (a): UMOD solutions are more uniform. (b): A PF model can be trained with a small number of solutions.

    &  &  &  & NSGA3  & SMS-EMOA  & MOEAAD  & MOEAAD-AWA  & UMOD \\  HV & 1.03 (0.00)(6) & 1.03 (0.00)(2) & 1.02 (0.00)(7) & 1.03 (0.00)(5) & 1.04 (0.00)(1) & 1.03 (0.00)(4) & 1.03 (0.00)(3) & **1.24 (0.00) (9)** \\ IGD & 5.68 (0.29)(7) & 5.42 (0.21)(6) & 5.21 (0.40)(1) & 5.28 (0.00)(3) & 5.22 (0.07)(2) & 5.30 (0.00)(4) & 5.42 (0.02) (5) & **5.19 (0.00) (9)** \\ Spacing & 6.35 (1.74)(7) & 4.44 (1.82)(3) & 2.51 (0.47)(3) & 5.90 (0.00)(5) & 1.04 (0.06)(1) & 5.92 (0.01)(6) & 3.69 (0.00) (4) & **0.12 (0.06)** \\ ZDTI & Sparsity & 4.80 (0.19)(7) & 4.55 (0.12)(4) & 2.59 (1.08)(9) & 4.00 (0.00)(5) & 4.48 (0.01)(2) & 4.61 (0.00)(6) & 4.54 (0.01)(5) & 4.39 (0.00)(1) \\ Uniform & 1.33 (0.11)(6) & 1.67 (0.32)(2) & 0.89 (0.10)(7) & 1.53 (0.00)(3) & 1.85 (0.03)(1) & 1.51 (0.00)(4) & 1.51 (0.00)(5) & **2.07 (0.01) (9)** \\ Uniform & 0.47 (0.12)(6) & 6.68 (0.13)(2) & 0.09 (0.10)(7) & 0.49 (0.00)(4) & 0.74 (0.01)(4) & 0.48 (0.00)(5) & 0.53 (0.00)(3) & **0.7 (0.00) (9)** \\ Fill Distance & 1.60 (0.23)(6) & 1.23 (0.09)(7) & 2.00 (0.10)(7) & 1.55 (0.00)(5) & 1.16 (0.00)(4) & 1.43 (0.00)(3) & **1.04 (0.01) (9)** \\  HV & 1.24 (0.00)(5) & 1.23 (0.01)(7) & 1.24 (0.00)(4) & 1.24 (0.00)(2) & 1.24 (0.00)(1) & 1.24 (0.00)(6) & 1.24 (0.00)(3) & **1.24 (0.00) (6)** \\ IGD & 4.63 (0.28)(5) & 4.61 (0.13)(4) & 5.15 (0.11)(6) & 4.44 (0.00)(3) & 4.23 (0.02)(5) & 5.40 (0.02)(4) & 4.33 (0.00)(4) & **4.12 (0.00) (6)** \\ Spacing & 6.63 (1.05)(6) & 3.62 (0.53)(9) & 1.68 (0.00)(5) & 5.71 (0.00)(5) & 1.91 (0.72)(5) & 1.09 (0.72)(5) & 1.09 (0.74)(3) & **0.12 (0.05) (9)** \\ RE2I & Sparsity & 3.10 (0.21)(6) & 2.90 (0.00)(6) & **1.74 (0.00)(9)** & 3.02 (0.00)(5) & 2.82 (0.02)(2) & 3.87 (0.02)(7) & 2.84 (0.01)(3) & 2.70 (0.00)(1) \\ Uniform & 0.81 (0.12)(7) & 0.95 (0.16)(9) & 0.05 (0.05)(4) & 1.16 (0.00)(2) & 1.26 (0.05)(1) & 0.93 (0.00)(6) & 1.11 (0.00)(3) & **1.02 (0.01) (9)** \\ Slufantom & -0.03 (0.07)(5) & 1.20 (0.03)(5) & -0.17 (0.00)(7) & 0.08 (0.00)(4) & 0.20 (0.01)(1) & -0.17 (0.00)(6) & 1.03 (0.01)(2) & **0.01 (0.00) (6)** \\ Fill Distance & 1.45 (0.20)(4) & 1.21 (0.12)(5) & 2.62 (0.01)(7) & 1.47 (0.00)(5) & 1.09 (0.04)(1) & 2.11 (0.01)(6) & 1.15 (0.02) (2) & **0.33 (0.00) (6)** \\  HV & 5.14 & 4.14 & 5.57 & 3.86 & **0.57** & 3.86 & 3.57 & 1.29 \\ IGD & 5 & 4.86 & 4 & 2.43 & 2.29 & 4.43 & 4 & **1** \\ Spacing & 5.71 & 2.71 & 3.86 & 3.86 & 3.9 & 5.14 & 3.57 & **0.14** \\ Rank & Sparsity & 5.29 & 5.71 & 1.43 & 3.71 & 2.71 & 4.86 & 3.43 & **0.86** \\ Uniform & 5 & 3.86 & 6.43 & 3 & 1.71 & 4.29 & 3.57 & **0.14** \\ Uniform & 5.14 & 3.57 & 6.86 & 3.14 & 2 & 4 & 3.14 & **0.14** \\ Fill Distance & 5.43 & 3.57 & 5.57 & 3.43 & 2.29 & 3.57 & 3.29 & **0.86** \\   

Table 1: Partial results for biobjective problems (full results are in Table 7).

Figure 4: Results on RE21 and DTLZ2.

Figure 3: Result comparison by different methods on ZDT1.

integrating adaptive weight adjustment; (3) _NSGA3_, generating diverse Pareto objectives through crowding distance; (4) _SMS-EMOA_, maximizing hypervolume for diverse solutions; (5) _LMPFE_, estimating the PF using multiple local models; (6) _Subset selection_, choosing a solution set by hypervolume maximization6; and (7) _DEA-GNG_, a preference adjustment method based on growing neural gas network. Methods (1)-(4) are classical MOEA methods implemented by Pymoo , while methods (5)-(7) are more recent methods. Full name of these methods are provided in Table 4.

We present the results for _biobjective_ problems in Figure 3 and Table 17. By directly minimizing maximal pairwise distances, the uniform indicator (which corresponds to maximal pairwise distances, see Appendix B.1, metric 5) is optimized effectively and ranks best among all methods. The fill distance, a surrogate for maximal pairwise distance up to constant, also performs best among all methods. This indicates that solutions found by UMOD cover the true PF with the minimal covering radius among all methods. Figure 3 further confirms that the covering radius of UMOD is significantly smaller than other methods. We also observe that the IGD indicator (Appendix B.1, metric 2), representing the mean Euclidean distance between the true PF and the found size-\(K\) solution set, is significantly improved. The significant improvement over IGD, a well-established indicator of uniformity and convergence of Pareto solutions, suggests that our method finds high

    & Indicator & DEA-GNG  & LMPFE  & Subset  & NSGA3  & SMS-EMOA  & MOEAD  & MOEAD-AWA  & UMOD \\   & HV & 1.01 (0.01) (7) & 1.05 (0.00) (6) & 1.08 (0.00) (1) & 1.06 (0.00) (5) & **1.08 (0.00) (6)** & 1.06 (0.00) (3) & 1.06 (0.00) (4) & 1.07 (0.00) (2) \\  & IGD & 1.25 (0.04) (2) & 1.26 (0.09) (1) & 1.54 (0.00) (1) & 1.25 (0.00) (3) & 1.54 (0.025) (1) & 1.55 (0.00) (5) & 1.25 (0.00) (4) & **1.09 (0.00) (1)** \\  & Spacing & 25.7 (1.73) & 2.64 (0.39) (1) & 8.97 (0.00) (7) & 5.45 (0.00) (7) & 7.15 (0.51) (6) & 8.54 (0.01) (4) & 5.45 (0.01) (1) & **6.52 (0.21) (6)** \\   & Sparsity & **1.06 (0.13)** & 2.18 (0.13) (2) & 2.42 (0.00) (2) & 2.43 (0.00) (2) & 2.75 (0.014) & 2.43 (0.00) (6) & 2.43 (0.00) (5) & 1.61 (0.13) (1) \\   & Uniform & **1.58 (0.19)** & 2.54 (0.17) & 1.09 (0.00) (7) & 2.43 (0.00) (4) & 1.51 (0.16) (2) & 2.43 (0.00) (2) & 2.34 (0.00) (2) & **1.37 (0.09) (6)** \\   & StUniform & 0.36 (0.23) (5) & 0.97 (0.06) (1) & -0.04 (0.00) (7) & 9.05 (0.00) (4) & 0.18 (0.07) (16) & 9.95 (0.00) (2) & 9.05 (0.00) (3) & **1.15 (0.02) (1)** \\   & Full Distance & 3.20 (0.60) (5) & 2.69 (0.10) (4) & 3.54 (0.00) (7) & 2.59 (0.00) (1) & 3.42 (0.16) (6) & 2.59 (0.00) (3) & 2.59 (0.00) (2) & **2.37 (0.08) (9)** \\   & HV & 6.8 & 4.6 & 1.8 & 4.4 & **0.2** & 3.6 & 4.2 & 2.4 \\  & IGD & 6 & 3 & 4.6 & 1.8 & 4.6 & 3.4 & 4 & **0.6** \\   & Spacing & 4.4 & 2.8 & 5.6 & 3.6 & 4 & 3.4 & 3.6 & **0.6** \\   & Sparsity & **1.4** & 3.6 & 1.6 & 4.6 & 4.8 & 5.6 & 4.6 & 1.8 \\   & Uniform & 5.8 & 2 & 5.8 & 2.8 & 4 & 3.4 & 3.4 & **0.8** \\   & StUniform & 6 & 2 & 5.8 & 3.4 & 4.4 & 3 & 3.2 & **0.2** \\   & Full Distance & 6.6 & 4.2 & 5.2 & 1 & 4.2 & 3.2 & 3.2 & **0.4** \\   

Table 2: Partial results on three-objective problems (full results are in Table 8).

Figure 5: Results on RE41 by different methods (full results are in Figure 8).

Figure 6: Results on RE42 by different methods (full results are in Figure 9).

[MISSING_PAGE_FAIL:9]

Results with five solutions are illustrated in Figure 7 compared with PMGDA , Hypervolume-gradient method (HVGrad), EPO , modified Tchebycheff aggregation function method (AggmTche) . For the Adult dataset, all methods run for 30 epochs, and for the Compass dataset, 20 epochs. A detailed description of these methods is in Appendix C.4. In real-world problems, the true PF range is often unknown, making it difficult for preference-based methods to select uniform preference vectors. If these vectors exclude PF endpoints, parts of the PF can not be recovered. In contrast, HV-Grad and the proposed UMOD method do not rely on preferences and can automatically identify a large PF. Figure 7 validates Theorem 5, showing that UMOD identifies both endpoints of the true PF, while HVGrad's endpoints cannot be determined.

Numerical results are presented in Table 3. In real-world problems, objectives often vary in scale, making uniform preference vectors non-equivalent to uniform objective vectors. UMOD, however, is designed to generate uniform objective vectors independent of scale, as evidenced by its superior performance in uniformity and soft uniformity indicators. Additionally, UMOD's spacing indicator, measuring neighborhood distance deviation, is significantly lower than other methods (except Agg-LS, which produces duplicate solutions in the Adult dataset, resulting in the lowest spacing). UMOD's Span indicators outperform other methods a lot, demonstrating its ability to recover a more complete PF, a desirable feature. Many methods show similar HV indicators, but their solution configurations vary, suggesting HV optimization is a coarse measure of uniformity. UMOD's significantly better uniformity performance indicates that directly optimizing uniformity, as in UMOD, is a promising approach for generating diverse Pareto solutions.

## 6 Conclusions and further works

**Conclusions.** In this paper, we have proposed a new understanding of a longstanding problem in MOO, generating \(K\) uniform and representative Pareto objectives, through searching for a max-packing design on the PF. We provide rigorous analysis of the resulting objective design, and in particular, we show this design will asymptotically converge to the uniform measure over Pareto front. With this new paradigm, we also empirically demonstrate how the space-filling design we obtain can benefit downstream performance with both synthetic and real-world MOO tasks. Overall, we believe that we pave a new way for (rate-)optimally configuring the Pareto objectives.

**Future works** include applying UMOD to large-scale real-world multiobjective problems, such as material design, vaccine design, and recommendation systems. The broader impacts of this work is discussed in Appendix D.1.

## Acknowledge

GH Li offers guidance on evolutionary computation, while YF Chen proves Theorem 3. The work described in this paper was supported by the Research Grants Council of the Hong Kong Special Administrative Region, China [GRF Project No. CityU 11215622].

Figure 7: Result on fairness classification.