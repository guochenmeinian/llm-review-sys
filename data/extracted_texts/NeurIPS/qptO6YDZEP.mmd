# Lower Bounds on Adaptive Sensing for Matrix Recovery

Praneeth Kacham

Computer Science Department

Carnegie Mellon University

pkacham@cs.cmu.edu &David P. Woodruff

Computer Science Department

Carnegie Mellon University

dwoodruf@cs.cmu.edu

###### Abstract

We study lower bounds on adaptive sensing algorithms for recovering low rank matrices using linear measurements. Given an \(n n\) matrix \(A\), a general linear measurement \(S(A)\), for an \(n n\) matrix \(S\), is just the inner product of \(S\) and \(A\), each treated as \(n^{2}\)-dimensional vectors. By performing as few linear measurements as possible on a rank-\(r\) matrix \(A\), we hope to construct a matrix \(\) that satisfies

\[\|A-\|_{}^{2} c\|A\|_{}^{2},\] (1)

for a small constant \(c\). Here \(\|A\|_{}\) denotes the Frobenius norm \((_{i,j}A_{i,j}^{2})^{1/2}\). It is commonly assumed that when measuring \(A\) with \(S\), the response is corrupted with an independent Gaussian random variable of mean \(0\) and variance \(^{2}\). Candes and Plan (IEEE Trans. Inform. Theory 2011) study non-adaptive algorithms for low rank matrix recovery using random linear measurements. They use the restricted isometry property (RIP) of Random Gaussian Matrices to give tractable algorithms to estimate \(A\) from the measurements.

At the edge of the noise level where recovery is information-theoretically feasible, it is known that their non-adaptive algorithms need to perform \((n^{2})\) measurements, which amounts to reading the entire matrix. An important question is whether adaptivity helps in decreasing the overall number of measurements. While for the related problem of sparse recovery, adaptive algorithms have been extensively studied, as far as we are aware adaptive algorithms and lower bounds on them seem largely unexplored for matrix recovery. We show that any adaptive algorithm that uses \(k\) linear measurements in each round and outputs an approximation as in (1) with probability \( 9/10\) must run for \(t=((n^{2}/k)/ n)\) rounds. Our lower bound shows that _any_ adaptive algorithm which uses \(n^{2-}\) (for any constant \(>0\)) linear measurements in each round must run for \(( n/ n)\) rounds to compute a good reconstruction with probability \( 9/10\). Hence any adaptive algorithm that has \(o( n/ n)\) rounds must use an overall \((n^{2})\) linear measurements. Our techniques also readily extend to obtain lower bounds on adaptive algorithms for tensor recovery.

Our hard distribution also allows us to give a measurement-vs-rounds trade-off for many sensing problems in numerical linear algebra, such as spectral norm low rank approximation, Frobenius norm low rank approximation, singular vector approximation, and more.

## 1 Introduction

Sparse recovery, also known as compressed sensing, is the study of under-determined systems of equations subject to a sparsity constraint. Suppose we know that an unknown vectorhas at most \(r n\) non-zero entries. We would like to use a measurement matrix \(M^{k n}\) to recover the vector \(x\) given measurements \(y=Mx\). The number \(k\) of measurements is an important parameter we would like to optimize as it models the equipment cost in physical settings and running times in computational settings. Ideally one would like for the number \(k\) of measurements to scale proportionally to the sparsity of the unknown vector \(x\).

One way of modeling sparse recovery is as "stable sparse recovery". Here we want a distribution \(\) over \(k n\) matrices such that for any \(x^{n}\), with probability \( 1-\) over \(\), we can construct a vector \(\) from \(x\) such that

\[\|x-\|_{p}(1+)_{rx^{}}\|x-x^{}\|_{p}.\]

Note that the above formulation is more robust as it does not require the underlying vector \(x\) to be exactly \(r\)-sparse and instead asks to recover the top \(r\) coordinates of \(x\).

For \(p=2\), it is known that \(k=(^{-1}r(n/r))\) measurements is both necessary and sufficient . See  and references therein for upper and lower bounds for other values of \(p\).

In the same vein, the problem of low rank matrix recovery has been studied. See  and references therein for earlier work and numerous applications. In this problem, the aim is to recover a _low rank_ matrix \(A\) using linear measurements. Here we want a distribution \(\) over linear operators \(M:^{n n}^{k}\), such that for any _low_ rank matrix \(A\), with probability \( 1-\) over \(\), we can construct a matrix \(\) from \((A)\) such that the reconstruction error \(\|A-\|_{}^{2}\) is small. Note that a linear operator \(M:^{n n}^{k}\) can be equivalently represented as a matrix \(M^{k n^{2}}\) such that \(M(A)=M(A)\) where \((A)\) denotes the appropriate flattening of the matrix \(A\) into a vector. We call the rows of \(M\)_linear measurements_. Without loss of generality, we can assume that the rows of the matrix \(M\) are orthonormal, as the responses for non-orthonormal queries can be obtained via a simple change of basis.

In addition, to model the measurement error that occurs in practice, it is a standard assumption that when querying with \(M\), we receive \(M(A)+\), where \(\) is a \(k\)-dimensional vector with independent Gaussian random variables of mean \(0\) and variance \(^{2}\), and we hope to reconstruct \(A\) with small error from \(M(A)+\). Clearly, when \(A\) has rank \(r\), we need to perform \((nr)\) linear measurements, as the matrix \(A\) has \((nr)\) independent parameters. Hence, the aim is to perform not many more linear measurements than \(nr\) while being able to obtain an estimate \(\) for \(A\) with low estimation error.

Given a rank-\(r\) matrix \(A\),  show that if the \(k n^{2}\) matrix \(\) is constructed with independent standard Gaussian entries, then with probability \( 1-(-cn)\), an estimate \(\) can be constructed from \((A)+\) such that \(\|A-\|_{}^{2} O(nr^{2}/k)\). They use the restricted isometry property (RIP) of the Gaussian matrix \(\) to obtain algorithms that give an estimate \(\). The error bound is intuitive since the reconstruction error increases with increasing noise and proportionally goes down when the number of measurements \(k\) increases.

While we formulated the sparse recovery and matrix recovery problems in a non-adaptive way, there have been works which study adaptive algorithms for sparse recovery. Here we can produce matrices \(^{(i)}\) adaptively based on the responses received \(^{(1)}x,^{(2)}x,,^{(i-1)}x\) and the hope is that allowing for adaptive algorithms with a small number of adaptive rounds, we obtain algorithms that overall perform fewer linear measurements than non-adaptive algorithms with the same reconstruction error. It is additionally assumed that the noise across different rounds is independent. For sparse recovery, in the case of \(p=2\), it is known that over \(O( n)\) rounds adaptivity, a total of \(O(^{-1}r(n(n/r)))\) linear measurements suffices , improving over the requirement of \((^{-1}r(n/r))\) linear measurements for non-adaptive algorithms. For a constant \(\), for any number of rounds, it is known that \((r+ n)\) linear measurements are required . Recently,  gave a lower bound on the total number of measurements if the number of adaptive rounds is constant.

While there has been a lot of interest in adaptive sparse recovery, both from the algorithms and the lower bounds perspective, the adaptive matrix recovery problem surprisingly does not seem to have any lower bounds. In adaptive matrix recovery, similar to adaptive sparse recovery, one is allowed to query a matrix \(M^{(i)}\) in round \(i\) based on the responses received in the previous rounds, and again the hope is that with more rounds of adaptivity, the total number of linear measurements that is to be performed decreases. There is some work that studies adaptive matrix recovery with \(2\) rounds of adaptivity (see  and references therein) but the full landscape of adaptive algorithms for matrix recovery seems unexplored.

We address this from the lower bounds side in this work. We show lower bounds on adaptive algorithms that recover a rank-\(r\) matrix of the form \((/)_{i=1}^{r}_{i}_{i}^{}\), where the coordinates of \(_{i}\) and \(_{i}\) are sampled independently from the standard Gaussian distribution. Without loss of generality1, we assume that the measurement matrix \(M^{(i)}\) in each round \(i\) has orthonormal rows. We also assume that for \(i j\), the measurement matrices \(M^{(i)}(M^{(j)})^{}=0\), i.e., measurements across adaptive rounds are orthonormal, since non-orthonormal measurements can be reconstructed from orthonormal measurement matrices by a change of basis.

We now give an alternate way of looking at the adaptive sparse recovery problem: Fix a set of vectors \(u_{1},,u_{r}\) and \(v_{1},,v_{r}\) and let the underlying matrix we want to reconstruct be \((/)_{i=1}^{r}u_{i}v_{i}^{}\) for a large enough constant \(\). In round \(1\), we query a matrix \(^{(1)}^{k n^{2}}\) drawn from an appropriate distribution and receive the response vector \(^{(1)}=^{(1)}((/)_{i=1}^{r}u _{i}v_{i}^{})+^{(1)}\) where \(^{(1)}\) is a vector of independent Gaussian random variables with mean \(0\) and variance \(^{2}\). In round \(2\), as a function of \(^{(1)}\) and \(^{(1)}\) and our randomness, we query a random matrix \(^{(2)}[^{(1)}]^{k n^{2}}\) and receive a response vector \(^{(2)}=(^{(2)}[^{(1)}])((/ )_{i=1}^{r}u_{i}v_{i}^{})+^{(2)}\) where \(^{(2)}\) is again a vector with independent Gaussian entries and independent of \(^{(1)}\), and so on. Crucially, using the rotational invariance of Gaussian random vectors, if \(\) is an \(n n\) matrix with independent Gaussian random variables with mean \(0\) and variance \(^{2}\), the response \(^{(1)}\) has the same distribution as \((^{(1)})((/)_{i=1}^{r}u_{i}v_{ i}^{}+)\) and as \(^{(2)}[^{(1)}]\) is chosen to be orthonormal to \(^{(1)}\), the distribution of \(^{(2)}\) conditioned on \(^{(1)}\) is the same as that of \((^{(2)}[^{(1)}])(((/)_{ i=1}^{r}u_{i}v_{i}^{}+))\).

Thus, any adaptive matrix recovery algorithm can be seen as performing _non-noisy_ adaptive measurements on the matrix \((/)_{i=1}^{r}u_{i}v_{i}^{}+\) where the Gaussian matrix \(\) is sampled independently of the measurement algorithm. From the responses the algorithm receives, it then tries to reconstruct the matrix \((/)_{i=1}^{r}u_{i}v_{i}^{}\). This way of looking at adaptive sparse recovery immediately yields an adaptive algorithm: when the smallest singular value of \((/)_{i=1}^{r}u_{i}v_{i}^{}\) is a constant times larger than \(\|\|_{2}\), then the power method with a block size of \(r\) outputs an approximation of \((/)_{i=1}^{r}u_{i}v_{i}^{}\) in \(O( n)\) rounds. Note that \(r\) matrix-vector products can be implemented using \(nr\) linear measurements. More generally, Gu  showed that using power iteration with a block size of \(k/n\) (for \(k nr\)), we can obtain an approximation in \(O((n^{2}/k))\) adaptive rounds. Thus the already existing algorithms exhibit a no. of measurements vs no. of rounds trade-off.

From results in random matrix theory, we have that \(\|\|_{2}\) with high probability. And as we are interested in reconstruction when the vectors \(u_{1},,u_{r}\) and \(v_{1},,v_{r}\) follow the Gaussian distribution we also have that \(\|u_{i}\|_{2}\|v_{i}\|_{2}\) simultaneously with large probability. Thus, to make the extraction of \((/)_{i=1}^{r}u_{i}v_{i}^{}\) information-theoretically possible, we need to assume that \(\). Hence, in this work we assume that \(=1\) and that \(\) is a large enough constant, and we study lower bounds on adaptive matrix recovery algorithms.

If the vectors \(u_{1},,u_{r}\) and \(v_{1},,v_{r}\) are sampled from the standard \(n\) dimensional Gaussian distribution and \(r n/2\), we also have that with high probability \(\|(/)_{i=1}^{r}u_{i}v_{i}^{}\|_{}^{2} ^{2}nr\). Now the algorithms of , which use a uniform random Gaussian matrix \(\) with \(m\) rows as the measurement matrix, for \((/)_{i=1}^{r}u_{i}v_{i}^{}\) reconstruct a matrix \(\) such that \(\|-(/)_{i=1}^{r}u_{i}v_{i}^{}\|_{}^ {2} C^{2}}{m}\) where \(_{m}\) is the measurement error. As we assumed above that \(=1\) when measuring with a matrix with orthonormal rows, we assume that the measurement error \(_{m}\) when measuring with a Gaussian matrix is \(n\), as each row of \(\) has \(n^{2}\) independent Gaussian coordinates and therefore has a norm \( n\). Thus, using reconstruction algorithms from , we obtain a matrix \(\) satisfying \(\|-(/)_{i=1}^{r}u_{i}v_{i}^{}\|_{}^ {2} Cr}{m}\). Now, to make \(\|-(/)_{i=1}^{r}u_{i}v_{i}^{}\|_{}^ {2}(1/10)\|(/)_{i=1}^{r}u_{i}v_{i}^{}\|_{}^ {2}\), we need to set \(m=(n^{2}/^{2})\). Hence, in the parameter regimes we study, the algorithms of  need to perform \((n^{2})\) non-adaptive queries, which essentially means that they have to read a constant fraction of the \(n^{2}\) entries in the matrix. While the power method performs \(O(nr)\) linear measurements in each round over \(O( n)\) adaptive rounds to output an approximation \(\). The question we ask is:

"Is the power method optimal? Are there algorithms that have \(o( n)\) adaptive rounds and use \(n^{2-}\) measurements in each round?"

We answer this question by showing that any algorithm that has \(o( n/ n)\) adaptive rounds must use \( n^{2-}\) linear measurements, for any constant \(>0\), in each round. The lower bound shows that power method is essentially optimal if we want to use \(n^{2-}\) linear measurements in each round and that any algorithm with \(o( n/ n)\) adaptive rounds essentially reads the whole matrix.

We further obtain a rounds vs measurements trade-off for many numerical linear algebra problems in the sensing model. In this model, there is an \(n n\) matrix \(A\) with which we can interact only using general linear measurements and we want to solve problems such as spectral norm low rank approximation of \(A\), Frobenius norm low rank approximation of \(A\), etc. In general, numerical linear algebra algorithms assume that they either have access to the entire matrix or that the matrix is accessible in the matrix-vector product model where one can query a vector \(v\) and obtain the result \(A v\). Recently, the vector-matrix-vector product model has received significant attention as well. Linear measurements are more powerful than both the matrix-vector product model and the vector-matrix-vector product model. Any matrix vector product \(A v\) can be computed using \(n\) linear measurements of \(A\) and any vector-matrix-vector product \(u^{}Av\) can be computed using a single linear measurement of \(A\). Thus, the model of general linear measurements may lead to faster algorithms.

For certain problems in numerical linear algebra, general linear measurements are significantly more powerful than the matrix-vector product model. Indeed, for computing the trace of an \(n n\) matrix \(A\), one can do this exactly with a single deterministic general linear measurement, just by adding up the diagonal entries of \(A\). However, in the matrix-vector product model, it is known that \((n)\) matrix-vector products are needed to compute the trace exactly , even if one uses randomization. A number of problems were studied in the vector-matrix-vector product model in , and in  it was shown that to approximate the trace of \(A\) up to a \((1+)\)-factor with probability \(1-\), one needs \((^{-2}(1/))\) queries. This contrasts sharply with the single deterministic general linear measurement for computing the trace exactly. Thus, there are good reasons to conjecture that general linear measurements may lead to algorithms requiring fewer rounds of adaptivity compared to algorithms in the matrix-vector product query model. Surprisingly, our lower bounds show that for many numerical linear algebra problems, linear measurements do not give much advantage over matrix-vector products.

### Our Results

We assume that there is an unknown rank-\(r\) matrix \(A^{n n}\) to be recovered. Given any linear measurement \(q^{n^{2}}\), we receive a response \( q,(A)+\), where \( N(0,\|q\|_{2}^{2})\). We further assume that the noise for two different measurements is independent. Without loss of generality, we also assume throughout that all the queries an algorithm makes across all rounds form a matrix with orthonormal rows. Our main result for adaptive matrix sensing is as follows:

**Theorem 1.1**.: _There exists a constant \(c\) such that any randomized algorithm which makes \(k nr\) noisy linear measurements of an arbitrary rank-\(r\) matrix \(A\) with \(\|A\|_{}^{2}=(nr)\) in each of \(t\) rounds, and outputs an estimate \(\) satisfying \(\|A-\|_{}^{2} c\|A\|_{}^{2}\) with probability \( 9/10\) over the randomness of the algorithm and the Gaussian noise, requires \(t=((n^{2}/k)/( n))\)._

Dependence on noiseIn our results, we assumed that given a linear measurement \(q^{n^{2}}\), the response is distributed as \(N( q,(A),\|q\|_{2}^{2})\). Our lower bounds also hold when the response is distributed as \(N( q,(A),^{2}\|q\|_{2}^{2})\) for any parameter \(\) such that \(c^{} 1\), where \(c^{}\) is a constant. This can be seen by simply scaling the matrix \(A\) in the theorem above and adjusting the constants while proving the theorem.

Tensor RecoveryThe problem of tensor recovery with linear measurements has also been studied (see [25; 11] and references therein) where given a low rank tensor, the task is to recover an approximation to the tensor with few linear measurements. Our techniques can also be used to obtain lower bounds on adaptive algorithms for tensor recovery. Our main tool, Lemma 3.1, readily extends to tensors of higher orders by using the corresponding tail bounds from .

Numerical Linear AlgebraWe also derive lower bounds for many numerical linear algebra problems in the linear measurements model. Table 1 shows our lower bounds on the number of adaptive rounds required for any randomized algorithm using \(k\) general linear measurements in each round. See Appendix E for precise statements and proofs.

### Notation

Throughout the paper, we use uppercase letters such as \(G,M,Q\) to denote matrices and lowercase letters such as \(u,v\) to denote vectors. For vectors \(u,v^{n}\), \(u v^{n^{2}}\) denotes the tensor product of \(u\) and \(v\). For an arbitrary matrix \(M^{m n}\), the vector \((M)^{mn}\) denotes a flattening of the matrix \(M\) with the convention that \((uv^{})=u v\). We use boldface symbols such as \(,,,\) to denote that these objects are explicitly sampled from an appropriate distribution. We use \(_{k}\) to denote a multivariate Gaussian in \(k\) dimensions where each coordinate is independently sampled from \(N(0,1)\). We also use \(G^{(j)}\) to denote a collection of \(j\) independent multivariate Gaussian random variables of appropriate dimensions.

### Our Techniques

Using the rotational invariance of the Gaussian distribution, we argued that any adaptive randomized low rank matrix recovery algorithm with access to a hidden matrix \((/)_{i=1}^{r}u_{i}v_{i}^{}\) using noisy linear measurements can be seen as a randomized algorithm that has access to a random matrix \((/)_{i=1}^{r}u_{i}v_{i}^{}+\) using _perfect_ linear measurements where each coordinate of \(\) is independently sampled from a Gaussian distribution.

Let \(\) be any algorithm that satisfies the matrix recovery guarantees with, say, a success probability \( 9/10\). Let \((X,,)\) be the output of the randomized algorithm \(\), where the hidden matrix is \(X\), the random seed of \(\) is denoted by \(\), and \(\) denotes the measurement randomness. We have that if \(X\) has rank \(r\) and satisfies \(\|X\|_{}^{2}=(nr)\), then

\[_{,}[(X,,)] 9/10.\]

We say \((X,,)\) is correct when the output \(\) satisfies \(\|-X\|_{}^{2}(1/100)\|X\|_{}^{2}\). By the above reduction, from \(\) we have a randomized algorithm \(^{}\) that runs on the random matrix \(X+\) with access to exact linear measurements and outputs a correct reconstruction \(\) with probability \( 9/10\) if \(X\) has rank \(r\) and \(\|X\|_{}^{2}=(nr)\). Thus, for all such \(X\),

\[_{,}[^{}(X+,)] 9/10.\]

  Application & Failure Probability & Lower Bound \\  \(2\)-approximate spectral LRA & \(0.1\) & \(c(n^{2}/k)/ n\) \\ \(2\)-approximate spectral LRA & \(1/(n)\) & \(c(n^{2}/k)\) \\ \(2\)-approximate Schatten \(p\) LRA & \(0.1\) & \(\) \\ \(2\)-approximate Ky-Fan \(p\) LRA & \(0.1\) & \(/k)}{(p)+ n}\) \\ \(1+1/n\)-approximate Frobenius LRA & \(0.1\) & \(c(n^{2}/k)/ n\) \\ \(0.1\)-approximate \(i\)-th singular vectors & \(0.1\) & \(c(n^{2}/k)/ n\) \\ \(2\)-approximate spectral reduced rank regression & \(0.1\) & \(c(n^{2}/k)/ n\) \\  

Table 1: Number of rounds lower bound for algorithms using \(k\) general linear measurements in each round. Our bound for \(2\)-approximate spectral LRA algorithm with constant failure probability is optimal up to a \( n\) factor, and our \(2\)-approximate spectral low rank approximation (LRA) lower bound for algorithms with failure probability \(1/(n)\) is optimal up to a constant factor.

Here \(\) denotes the randomness used by the algorithm \(^{}\). Now, if \(\) is a random matrix constructed as \((/)_{i=1}^{r}_{i}_{i}^{}\) with \(_{i},_{i}\) being random vectors with independent Gaussian entries of mean \(0\) and variance \(1\), then with probability \( 99/100\), \(\|\|_{}^{2}=(nr)\). Thus, \(_{,,}[^{}(+,)\) is correct\(] 8/10\). Hence, there exists some fixed \(\) such that

\[_{,}[^{}(+,)\,]  8/10.\]

Thus, the existence of a randomized algorithm that solves low rank matrix recovery as in Theorem 1.1 implies the existence of a deterministic algorithm which given access to perfect linear measurements of random matrix \((/)_{i=1}^{r}_{i}_{i}^{}+\) outputs a reconstruction of \((/)_{i=1}^{r}_{i}_{i}^{}\) with probability \( 8/10\). This is essentially a reduction from randomized algorithms to deterministic algorithms using Yao's lemma. From here on, we prove lower bounds on such deterministic algorithms and conclude the lower bounds in Theorem 1.1. For simplicity, we explain the proof of our lower bound for \(r=1\) here and extend to general \(r\) later. We consider the random matrix \(+(/)^{}\) and show how the lower bound proof proceeds.

Note that the matrix \(+(/)^{}^ {n n}\) can be flattened to the vector \(=+(/)()^{n^{2}}\). Also, a general linear measurement, which we call a query \(Q^{n n}\), can be vectorized to \(q=(Q)^{n^{2}}\) with \(Q()= q,\). Fix any deterministic algorithm. In the first round, the algorithm starts with a fixed matrix \(Q^{(1)}^{k n^{2}}\) that corresponds to the \(k\) queries and receives the response \(^{(1)} Q^{(1)}\). Then, as a function of the response \(^{(1)}\) in the first round, the algorithm picks a matrix \(Q^{(2)}[^{(1)}]\) in the second round and receives the response \(^{(2)} Q^{(2)}[^{(1)}]\). Similarly, in the \(i\)-th round, the deterministic algorithm picks a matrix \(Q^{(i)}[^{(1)},,^{(i-1)}]^{k n^{2}}\) as a function of \(^{(1)},,^{(i-1)}\) and receives the response \(^{(i)} Q^{(i)}[^{(1)},,^{(i-1)}]\). Note that we assumed that the query matrices \(Q^{(i)}\) chosen by the algorithm have orthonormal rows and also that \(Q^{(i)}(Q^{(j)})^{}=0\), i.e., the queries across rounds are also orthonormal.

For a fixed \(u,v^{n}\), we see that the response \(^{(1)}=Q^{(1)}+(/)Q^{(1)}(u v)\). As the matrix \(Q^{(1)}\) has orthonormal rows, the random variable \(Q^{(1)}_{k}\), where \(_{k} N(0,I_{k})\) is drawn from a mean-zero normal distribution with identity covariance. Thus, for fixed \(u,v\), the distribution of the first round responses to the algorithm is \(N((/)Q^{(1)}(u v),I_{k})\). Now the key observation is that \(\|(/)Q^{(1)}()\|_{2}^{2}=(^{2}k/n)\) with high probability over the inputs \((,)\). This uses a recent concentration result for random tensors due to , and critically uses the fact that \(Q^{(1)}\) has operator norm \(1\) and Frobenius norm \(\). This means that for a large fraction of \((u,v)\) pairs, the distribution of the responses seen by the algorithm in the first round is close to \(N(0,I_{k})\), and therefore just looking at the response \(^{(1)}\), the algorithm cannot have a lot of "information" about which \((u,v)\) pair is involved in the matrix that is unknown to the algorithm. So the query chosen in the second round \(Q^{(2)}[^{(1)}]\) cannot have a "large" value of \(Q^{(2)}[^{(1)}](u v)\) for most inputs \((u,v)\), with a high probability over the Gaussian component of the matrix.

Suppose the value of \(Q^{(2)}[^{(1)}](u v)\) is small. Again, \(^{(2)}=Q^{(2)}[^{(1)}]+(/)Q^{(2)}[^{(1 )}](u v)\). Crucially, as the queries in round \(2\) are orthogonal to the queries in round \(1\), we have by the rotational invariance of the Gaussian distribution that \(Q^{(2)}[^{(1)}]\) is independent of \(Q^{(1)}\), and that \(Q^{(2)}[^{(1)}]\) is distributed as \(N(0,I_{k})\). So, for a fixed \(u,v\), conditioned on the first round response \(^{(1)}\), the distribution of the second round response \(^{(2)}\) is given by \(N((/)Q^{(2)}[^{(1)}](u v),I_{k}) N(0,I_{k})\) using the assumption that \(Q^{(2)}[^{(1)}](u v)\) is small. We again have that for a large fraction of pairs \((u,v)\) for which \(Q^{(2)}[^{(1)}](u v)\) is small, the distribution of the second round response is also close to \(N(0,I_{k})\). Therefore, the algorithm again does not gain a lot of information about which \((u,v)\) pair is involved in the matrix, and the third round query \(Q^{(3)}[^{(1)},^{(2)}]\) cannot have a large value of \(\|Q^{(3)}[^{(1)},^{(2)}](u v)\|_{2}\). The proof proceeds similarly for further rounds.

To formalize the above intuitive idea, we use Bayes risk lower bounds . We show that with a large probability over the input matrix, the squared projection of \(\) onto the query space of the algorithm is upper bounded and we use an iterative argument to show that an upper bound on the information up until round \(i\) can in turn be used to upper bound the information up until round \(i+1\). Bayes risk bounds are very general and let us obtain upper bounds on the information learned by a deterministic learner. Concretely, let \(\) be a parameter space and \(=\{\,P_{}\,:\,\,\}\) be a set of distributions, one for each \(\). Let \(w\) be a distribution over \(\). We sample \( w\) and then \( P_{}\) and provide the learner with \(\). Given an action space \(\), the learner uses a deterministic decision rule \(:\) to minimize a \(0\)-\(1\) loss function \(L:\{\,0,1\,\}\) in expectation, i.e., \(*{}_{ w}[*{ }_{ P_{}}L(, ())]\). Let \(R_{}(L,,w)=_{}*{}_{  w}[E_{ P_{}}L( ,())]\) be the loss achievable by the best deterministic decision rule \(\). Bayes risk lower bounds let us obtain lower bounds on \(R_{}\).

In our setting after round \(1\), we have \(=\{\,(u,v):u,v^{n}\,\}\), \(w\) is the joint distribution of two independent standard Gaussian random variables in \(^{n}\) and for each \((u,v)\) we let \(P_{uv}\) be the distribution of \(^{(1)}=_{k}+(/)Q^{(1)}(u v)\), an action space \(\) of all \(k n^{2}\) matrices with orthonormal rows (corresponding to the queries in the next round), and define a \(0\)-\(1\) loss function

\[L((u,v),Q)=0&\|Q(u v)\|_{2}^{2} T\\ 1&\|Q(u v)\|_{2}^{2}<T\]

for an appropriate threshold parameter \(T\). By setting \(T\) appropriately as a function of \(t\), we obtain a Bayes risk lower bound of \(R_{} 1-1/(100t^{2})\). Thus, we obtain that for any deterministic decision rule \(\), with probability \( 1-1/10t\) over \((,) w\), we have

\[*{}_{^{(1)} P_{}}[L(( ,),(^{(1)}))] 1-1/10t\]

and in particular, we have that with probability \( 1-1/10t\) over \((,) w\),

\[_{^{(1)} P_{}}[\|Q^{(2)}[^{ (1)}]()\|_{2}^{2}<T] 1-1/10t.\] (2)

The above statement essentially says that with probability \( 1-1/10t\) over the inputs \((,)\), the second query \(Q^{(2)}[^{(1)}]\) chosen by the deterministic algorithm has the property that \(\|Q^{(2)}[^{(1)}]()\|_{2}^{2}<T\) with probability \( 1-1/10t\) over the Gaussian \(\). In the second round, we restrict our analysis of the algorithm to only those \((u,v)\) which satisfy (2). We again define a distribution \(w^{}\) over the inputs and for each such \((u,v)\) we define a distribution \(P_{uv}\) over the round \(1\) and round \(2\) responses received by the algorithm. We define a new loss function with parameter \(T^{}= T\) for a multiplicative factor \(\) and again obtain a statement similar to (2) for a large fraction of inputs \((u,v)\) and repeat a similar argument for \(t\) rounds and show that there is an \((1)\) fraction of the inputs for which the squared norm of the projection of \(u v\) onto the query space after \(t\) rounds is bounded by \(T(t)\) with high probability over the Gaussian part of the input. This gives the result in Theorem 3.2. Note that \(\|\|_{2}^{2}=(n^{2})\) with high probability and for any fixed matrix \(Q\) with \(k\) orthonormal rows, \(*{E}[\|Q()\|_{2}^{2}]=k\) which corresponds to the amount of "information" the algorithm starts with. As we show that the "information" in each round grows by some multiplicative factor \(\), a number \((_{}(n^{2}/k))\) of rounds is required to obtain an "information" of \((n^{2})\), which is how we obtain our lower bounds. Here information is measured as the squared projection of \(\) onto the query space of the algorithm.

We also extend our results to identifying a rank \(r\) spike (sum of \(r\) random outer products) corrupted by Gaussian noise. Specifically, we consider the random matrix \(=+(/)_{i=1}^{r} _{i}_{i}^{}\) where all the coordinates of \(,_{i},_{i}\) are independently sampled from \(N(0,1)\). We show that if there is an algorithm that uses \(k\) iterations in each round and identifies the spike with probability \( 9/10\), then it must run for \(((n^{2}/k)/ n)\) rounds by appealing to the lower bound we described above for the case of \(r=1\). We show that if there is an algorithm for \(r>1\) that requires \(t<O((n^{2}/k)/ n)\) adaptive rounds, then it can be used to solve the rank \(1\) spike estimation problem in \(t\) rounds as well which contradicts the lower bound.

We then provide lower bounds on approximate algorithms for a host of problems such as spectral norm low rank approximation (LRA), Schatten norm LRA, Ky-Fan norm LRA and reduced rank regression, by showing that algorithms to solve these problems can be used to estimate the spike \(^{}\) in the random matrix \(+(/)^{}\), and then use the aforementioned lower bounds on algorithms that can estimate the spike. Although our hard distribution is supported on non-symmetric matrices, we are still able to obtain lower bounds for algorithms for spectral norm LRA (rank \(r 2\)) for symmetric matrices as well by considering a suitably defined symmetric random matrix using our hard distribution. Let \(+(/)^{ }\) be the hard distribution in the case of rank \(r=1\).

We symmetrize the matrix by considering, \(_{}=0&\\ ^{}&0\). This symmetrization, as opposed to \(^{}\) or \(^{}\), has the advantage that a linear measurement of \(_{}\) can be obtained from an appropriately transformed linear measurement of \(\), thereby letting us obtain lower bounds for symmetric instances as well in the linear measurements model. However, we cannot obtain lower bounds for rank \(1\) spectral norm LRA for symmetric matrices using this symmetrization as the top two singular values of \(_{}\) are equal and hence even a zero matrix is a perfect rank \(1\) spectral norm LRA for \(_{}\) which does not give any information about the plant \(\).

### Related Work

As discussed, low rank matrix recovery has been extensively studied (see  and references therein for earlier work). Relatedly,  study the Robust PCA problem where the aim is to estimate the sum of a low rank matrix and a sparse matrix from noisy vector products with the hidden matrix.  study the Robust PCA problem when given access to linear measurements with the hidden matrix.

For non-adaptive algorithms for low rank matrix recovery with Gaussian errors,  show that their selectors based on the restricted isometry property of measurement matrices are optimal up to constant factors in the minimax error sense when the noise follows a Gaussian distribution. Our Theorem 1.1 extends their lower bounds and shows that if there is any randomized measurement matrix2\(\) with \(k\) rows coupled with a recovery algorithm that outputs a reconstruction for any rank \(r\) matrix with \(\|A\|_{F}^{2}=(nr)\), then it must have \(k=(n^{2-(1)})\). We again note that we give lower bounds even for algorithms with multiple adaptive rounds.

Our technique to show lower bounds is to plant a low rank matrix \((/)_{i=1}^{r}u_{i}v_{i}^{}\) in an \(n n\) Gaussian matrix so that any "orthonormal" access to the plant is corrupted by independent Gaussian noise. Notably this technique has been employed to obtain lower bounds on adaptive algorithms for sparse recovery in . Even in the non-adaptive setting, Li and Woodruff  use the same hard distribution as we do to obtain sketching lower bounds for approximating Schatten \(p\) norms, the operator norm, and the Ky Fan norms. The technique to show their lower bounds is that if a sketching matrix has \(k c/(r^{2}s^{4})\) rows3, it cannot distinguish between the random matrix \(\) and the random matrix \(+s_{i=1}^{r}_{i}_{i}^{}\) where all the coordinates of \(,_{i},_{i}\) are drawn uniformly at random. They prove this by showing that \(d_{}(_{1},_{2})\) is small if \(_{1}\) is the distribution of \(M()\) and \(_{2}\) is the distribution of \(M(+s_{i=1}^{n}_{i}_{i}^{})\) for any fixed measurement matrix \(M\) with \(k c/(r^{2}s^{4})\) rows. Their techniques do not extend to the plant estimation in the distribution \(+s_{i=1}^{r}_{i}_{i}^{}\) as the statement they prove only says that over the randomness of \(,_{i},_{i}\), the response distribution for \(+s_{i=1}^{r}_{i}_{i}^{}\) is close to the response distribution for \(\), but in our case, we want the distributions \(_{u,v}\) and \(_{u^{},v^{}}\) to be indistinguishable, where \(_{u,v}\) is the response distribution for \(+(/)_{i=1}^{n}u_{i}v_{i}^{}\).

Later,  considered the distribution of the symmetric random matrix \(+^{}\), where \(\) is the \(n n\) symmetric Wigner matrix \((+^{})/\) and \(\) is a uniformly random \(n r\) matrix with orthonormal columns. They focus on obtaining lower bounds on adaptive algorithms that estimate the spike \(\) in the matrix-vector product model. In particular, they show that if \(Q\) is a basis for the query space spanned by any deterministic algorithm after querying \(k\) adaptive matrix-vector queries, then \(_{r}(Q^{}^{}Q)\) grows as \(^{k/r}\). Using this, they show that any algorithm which, given access to an arbitrary symmetric matrix \(A\) in the matrix-vector product query model, must use \((r(n)/})\) adaptive queries to output an \(n r\) orthonormal matrix \(V\) satisfying, for a small enough constant \(c\),

\[ V,AV(1-c)_{i=1}^{r}_{i}(A),\] (3)

where \(=(_{r}(A)-_{r+1}(A))/_{r}(A)\). We note the above guarantee is non-standard in the low rank approximation literature, which instead focuses more on approximation algorithms for Frobenius norm and spectral norm LRA. While in the matrix-vector product model, their hard distribution helps in getting lower bounds for numerical linear algebra problems on symmetric matrices, it seems that our hard distribution \(+(/)_{i=1}^{r}_{i}_{i}^{}\) is easier to understand in the linear measurement model and gives the important property that the noise between rounds is independent, which is what lets us reduce the matrix-recovery problem with noisy measurements to spike estimation in \(+(/)_{i=1}^{r}_{i}_{i}^{}\).

Recently, Bakshi and Narayanan  obtained a tight lower bound for rank-\(1\) spectral norm low rank approximation problem in the matrix-vector product model. They show that \(( n/)\) matrix vector products are required to obtain a \(1+\) spectral norm low rank approximation. We stress that while our results are not for \(1+\) approximations, they hold in the stronger linear measurements model.

## 2 Notation and Preliminaries

Given an integer \(n 1\), we use \([n]\) to denote the set \(\{\,1,,n\,\}\). For a vector \(v^{n}\), \(\|v\|_{2}:=(_{i[n]}v_{i}^{2})^{1/2}\) denotes the Euclidean norm. For a matrix \(A^{n d}\), \(\|A\|_{}\) denotes the Frobenius norm \((_{i,j}A_{ij}^{2})^{1/2}\) and \(\|A\|_{2}\) denotes the spectral norm \(_{x 0}\|Ax\|_{2}/\|x\|_{2}\). For \(p 2\), we use \(\|A\|_{_{p}}\) to denote the Schatten-\(p\) norm \((_{i}_{i}^{p})^{1/p}\) and for \(p[(n,d)]\), we use \(\|A\|_{_{p}}\) to denote the Ky-Fan \(p\) norm \(_{i=1}^{p}_{i}\), where \(_{1},,_{(n,d)}\) denote the singular values of the matrix \(A\). Given a parameter \(k\), the matrix \([A]_{k}\) denotes the best rank \(k\) approximation for matrix \(A\) in Frobenius norm. By the Eckart-Young-Mirsky theorem, the best rank \(k\) approximation under any unitarily invariant norm, which includes the spectral norm, Frobenius norm, Schatten norms and Ky-Fan norms, is given by truncating the Singular Value Decomposition of the matrix \(A\) to top \(k\) singular values. We describe the Bayes Risk Lower Bounds that we use to prove Lemma 3.1 in the Appendix. Our presentation is based on . Due to space constraints, we have placed all proofs in the appendix.

## 3 Number of Linear Measurements Versus Number of Adaptive Rounds

We now state the main theorem which shows a lower bound on the number of adaptive rounds required for any deterministic algorithm to estimate the _plant_ when the input is a random matrix \(+(/)_{i=1}^{r}_{i}_{i}^{}\). We use this theorem to prove Theorem 1.1 and lower bounds for other numerical linear algebra problems.

We prove the lower bound for the rank-\(r\) plant estimation by first proving a lower bound on the rank-\(1\) plant estimation and then reducing the rank-\(1\) recovery problem to rank-\(r\) recovery problem. For the rank-\(1\) recovery problem, we prove the following lemma:

**Lemma 3.1**.: _Given an integer \(n\), and parameters \( 10\) and \( 1\), define the \(n n\) random matrix \(=+}}\) for \(s:=/\), where the entries of \(\), \(\), \(\) are drawn independently from the distribution \(N(0,1)\). Let \(\) be any \(t\)-round deterministic algorithm that makes \(k n\) adaptive linear measurements in each round. Let \(Q^{(j)}^{k n^{2}}\) denote the matrix of general linear queries made by the algorithm in round \(j\) and \(Q\) be a matrix with orthonormal rows such that \((Q)=(Q^{(1)})++(Q^{(t)})\). Then for all \(t\) such that \(O(k(n))(K^{2}^{2})^{t}k O(n^{2})\) for a universal constant \(K\),_

\[_{=+}}}[\|Q()\|_{2}^{2} (3K)k(K^{2}^{2})^{t}](1-1/(10))^{t}-1/(n).\]

Setting \(=O((n))\), the theorem shows that if \(t c(n^{2}/k)/((n)+())\) for a small enough constant \(c\), then for any \(t\)-round deterministic algorithm,

\[_{=+}}}[\|Q()\|_{2}^{2}  n^{2}/100] 4/5.\] (4)

Setting \(=O(1)\), we obtain that if \(t c(n^{2}/k)/(())\) for a small enough constant \(c\), then

\[_{=+}}}[\|Q()\|_{2}^{2}  n^{2}/100] 1/(n).\] (5)

The proof of the above lemma is in Appendix B. The above lemma directly shows lower bounds on any deterministic algorithm that can approximate the rank-\(1\) planted matrix. We show that the lower bounds can be extended to algorithms that estimate the rank-\(r\) plant as well.

**Theorem 3.2**.: _Let \(n\) and \(r n/2\) be input parameters and \(\) be a large enough constant. Let the random matrix \(+(/)_{i=1}^{r}_{i}_{i}^{}\) be the input which can be accessed using linear measurements. If Alg is a \(t\)-round adaptive algorithm that uses \(k\) linear measurements and at the end of \(t\)-rounds outputs a matrix \(\) such that with probability \( 9/10\) over the randomness of the input and the internal randomness of the algorithm,_

\[\|-_{i=1}^{r}_{i}_{i}^{}\|_{}^{2}  c(n^{2}r)\]

_for a small enough constant \(c\), then \(t=((n^{2}/k)/(()+ n))\)._

Using the reduction from matrix recovery with noisy measurements to plant estimation with exact measurements that we described in the previous sections, we can now prove Theorem 1.1. The proof of Theorem 1.1 is in Appendix D.