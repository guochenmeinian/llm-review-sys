# SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxification

Yanxin Yang\({}^{1}\), Chentao Jia\({}^{1}\), DengKe Yan\({}^{1}\), Ming Hu\({}^{2}\), Tianlin Li\({}^{3}\), Xiaofei Xie\({}^{2}\),

Xian Wei\({}^{1}\), Mingsong Chen\({}^{1}\)

Ming Hu and Mingsong Chen are the corresponding authors. \({}^{1}\)MoE Eng. Research Center of SW/HW Co-Design Tech. and App., East China Normal University

\({}^{2}\)Singapore Management University, \({}^{3}\)Nanyang Technological University

{52275902023, 51265902040, 51265902053}@stu.ecnu.edu.cn,

hu.ming.work@gmail.com, tianlin001@ntu.edu.sg, xfxie@smu.edu.sg,

{xwei, mschen}@sei.ecnu.edu.cn

Ming Hu and Mingsong Chen are the corresponding authors.

###### Abstract

The advancement of Machine Learning has enabled the widespread deployment of Machine Learning as a Service (MLaaS) applications. However, the untrustworthy nature of third-party ML services poses backdoor threats. Existing defenses in MLaaS are limited by their reliance on training samples or white-box model analysis, highlighting the need for a black-box backdoor purification method. In our paper, we attempt to use diffusion models for purification by introducing noise in a forward diffusion process to destroy backdoors and recover clean samples through a reverse generative process. However, since a higher noise also destroys the semantics of the original samples, it still results in a low restoration performance. To investigate the effectiveness of noise in eliminating different types of backdoors, we conducted a preliminary study, which demonstrates that backdoors with low visibility can be easily destroyed by lightweight noise and those with high visibility need to be destroyed by high noise but can be easily detected. Based on the study, we propose SampDetox, which strategically combines lightweight and high noise. SampDetox applies weak noise to eliminate low-visibility backdoors and compares the structural similarity between the recovered and original samples to localize high-visibility backdoors. Intensive noise is then applied to these localized areas, destroying the high-visibility backdoors while preserving global semantic information. As a result, detoxified samples can be used for inference even by poisoned models. Comprehensive experiments demonstrate the effectiveness of SampDetox in defending against various state-of-the-art backdoor attacks. The source code of this work is publicly available at https://github.com/easywood0204/SampDetox.

## 1 Introduction

With Artificial Intelligence (AI) technologies showing advantages in tasks such as image classification and target detection, they are widely used in safety- and security-critical applications such as autonomous processing [1; 2], IoT System [3; 4] and health-care [5; 6]. As Machine Learning as a Service (MLaaS) [7; 8; 9] becomes increasingly popular, AI applications are relying on AI services provided by third parties. However, since users cannot guarantee the trustworthiness of service providers, they face serious threats of backdoor attacks [10; 11; 12]. Typically, adversaries inject backdoors into deep models on numerous occasions, especially when the training samples are collected from unreliable sources, or the pre-trained deep models are obtained from untrusted third parties. During the inference phase, when fed clean samples, the backdoored models behave normallywith satisfactory classification performance. However, when processing poisoned samples containing trigger patterns, the backdoored models will be fooled into predicting attack target categories with high confidence.

Although various defense methods have been proposed to identify and destroy backdoors, most of them are white-box defenses that require additional training samples or the right to analyze model parameters [13; 14; 15]. Since in MLaaS-based applications users are not allowed to access the original training data and parameters of models, these backdoor defense methods are strongly limited. In contrast, black-box backdoor defense methods do not have such requirements. So far, black-box backdoor defense methods can be classified into three categories, i.e., model detection-based , sample detection-based [7; 17], and sample purification-based [18; 19] defense methods. The basic idea of the first two categories is to simply discard training samples or deep models once they are identified as poisoned. In this case, the usability and performance of classification tasks are greatly affected. As an alternative, sample purification-based approaches strive to destroy backdoor trigger patterns in samples. However, existing sample purification-based approaches are based on the strong assumption that trigger patterns are small and only located in the corners of samples, which is not always true in practice. As a result, such methods can only be used to defend against specific types of backdoor attacks. Worse still, most of the above defense methods need to repetitively feed samples to deep models and get their continuous feedback, resulting in extra non-negligible computation overhead. _Therefore, how to effectively mitigate the impacts of all possible backdoor attacks without deteriorating the overall inference performance is becoming a great challenge in black-box defense._

Intuitively, adding noise to poisoned samples can disrupt the semantics of the backdoor, thereby eliminating it from the sample. To preserve the semantics of the original sample while removing the backdoor, the diffusion model [20; 21] serves as an effective solution to restore the sample. However, different backdoors exhibit varying levels of robustness to noise, which requires different noise amplitudes for effective disruption. In addition, the amplitude and range of noise will affect the quality of samples restored by the diffusion model. We performed a preliminary study on various backdoor attacks to explore the robustness of different types of backdoors. We found that: i) invisible triggers with low robustness (to random noise) can be easily destroyed, and ii) backdoor triggers with high robustness are easily detectable due to their high visibility.

Inspired by these observations, to address the challenge of black-box backdoor defense, we propose a novel two-stage method called SampDetox. Based on our first observation, we globally apply coarse-granularity noises on each given sample in the first stage of SampDetox, where backdoor triggers with low visibility can be easily destroyed. To counteract the effects of such added noises, we then restore the samples by denoising them using diffusion models. Note that for a poisoned sample, its poisoned regions with high visibility, compared with clean regions, will be different from their counterparts in the restored version (see the proof in Theorem 4.2), making them identified easily. Based on our second observation, the second stage can quickly locate such robust triggers and destroy them using our dedicatedly designed noises. Similar to the first stage, the second stage then leverages diffusion models to restore the samples. Since our approach does not pose any assumptions or requirements on the models, it accommodates arbitrary backdoor attack scenarios. In summary, this paper makes the following three major contributions:

* We perform a preliminary study on a wide spectrum of backdoors to reveal the correlation between the visibility of triggers and the robustness of poisoned samples.
* We present a novel perturbation-based sample detoxification method together with its theoretical foundations. Our approach can effectively destroy all possible triggers with dedicatedly designed noises and does not compromise the overall inference performance.
* We conduct extensive experiments to show the applicability and superiority of our approach over state-of-the-art (SOTA) backdoor defense methods.

## 2 Related Work

**Backdoor Attacks.** Existing backdoor attacks can be mainly classified into two categories, i.e., visible attacks and invisible attacks, based on the visibility of trigger patterns during the inference phase. Specifically, visible attacks do not consider the concealment of trigger patterns. For example, BadNets  uses visible trigger patterns in the form of fixed pixels patched in the corner of the images. Although there exist various visible attacks (e.g., training set corruption without label poisoning , label-consistent backdoor attacks , attacking pre-trained models , and specific-sample triggers ), few of them can be applied in real scenarios, since the triggers on samples are conspicuous and can be easily detected by manual inspection. To make attacks stealthier, more and more backdoor attacks (e.g., Blended , WaNet , and ISSBA ) adopt invisible triggers. For example, BPP  uses image quantization and dithering as imperceptible backdoor triggers to avoid manual inspection.

**Backdoor Defenses.** According to the capabilities of defenders in manipulating the training and inference processes on deep models, backdoor defense methods can be classified into two categories, i.e., white-box and black-box defenses. Typically, white-box methods defend against backdoor attacks by changing training processes [31; 32], extract gradients , and fine-tuning and -pruning of poisoned models [34; 35]. However, these methods require access to model parameters, which strongly restricts their usage (e.g., MLaaS applications) in practice. For black-box defenses, such as model detection-based , sample detection-based [7; 17], and sample purification-based [18; 19] approaches, defenders only can rely on the predictions of input samples. For example, as a model detection-based approach, CBD  can effectively identify whether models are poisoned by analyzing their predictions. However, simply discarding the suspicious model/sample does not apply to real-world classification tasks. Rather than ignoring poisoned samples, sample purification-based methods aim to destroy the triggers on samples. For example, Sancdifi  measures saliency maps and purifies the specific areas of samples. BDMAE  reconstructs the local area of the samples using its AutoEncoder. ZIP  transforms and reconstructs samples independently of models through zero-shot image purification. However, all these purification-based methods can handle only small triggers within specific regions.

The robustness of triggers against perturbations or modifications plays an important role in determining the performance of backdoor attacks and defenses. For example, CBD  analyzes the robustness of triggers with different sizes against random noises to certify its proposed defense scheme. However, CBD only investigates attacks with simple triggers without taking SOTA backdoor attacks into account. To the best of our knowledge, SampDetox is the first attempt to consider the correlation between the visibility of backdoor triggers and the robustness of poisoned samples. As a model-independent method, SampDetox enables perturbation-based sample detoxification, which can effectively defend against complex backdoor attacks within arbitrary application scenarios.

## 3 Preliminary Study

This section first reviews the process of backdoor attacks. Then, it gives our definitions of the visibility of backdoor triggers and the robustness of poisoned samples and reveals their inherent correlation based on a preliminary study.

### Evaluation Metrics

Assume that \(M\) is a model poisoned by some backdoor attack. Let \(x^{c}\) be a clean sample, and \(x^{p}\) represent its poisoned version by adding specific backdoor triggers. If the backdoor attack succeeds, the model \(M\) should classify \(x^{c}\) into its correct category while classifying \(x^{p}\) into a target category. To defend against backdoor attacks, the findings in  show that transforming or perturbing samples will achieve defensive effects against locally patched triggers. However, according to the observations in [18; 19], these operations are unsuitable for defending against other types of triggers. This is mainly because locally patched triggers are unremarkable and susceptible to disturbance, while other triggers are robust (i.e., not sensitive) to such backdoor defenses. To better understand the inherent characteristics of backdoor attacks and the trends of their development, we conduct a preliminary study on existing attacks to reveal the correlation between the visibility of backdoor triggers and the robustness of their poisoned samples, whose descriptions are as follows.

**Visibility.** When a backdoor attack is applied to some deep model, the visibility of embedded triggers indicates the stealthiness of the backdoor attack. In this paper, we use the Structural Similarity Index Measure (SSIM) [29; 39] to evaluate the visibility of backdoor triggers. This is because, unlike traditional measurements like Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR), SSIM takes into account the local characteristics of samples, aligning more closely with the visual system of human eyes. Let SSIM\((x^{c},x^{p})[-1,1]\) be the SSIM between \(x^{c}\) and \(x^{p}\), quantifying the visibility of trigger patterns embedded in \(x^{p}\). For ease of evaluation, we adopt the notation \(v=(1-(x^{c},x^{p}))/2\) to normalize the visibility values. Specifically, the higher the value of \(v\), the more notable the triggers are.

**Robustness.** For a given poisoned sample \(x^{p}\), we propose to use the term robustness to quantify its capability to resist perturbations or modifications imposed by random noise. Assume that the backdoored model classifies \(x^{p}\) into a specific attack target category due to the existence of triggers. We apply random noises to \(x^{p}\) and get a modified sample \(x_{m}\), where the modification process can be described by the equation \(x_{m}=(1-_{r})x^{p}+_{r}\) (\((0,)\)). Since \(_{r}\) plays an important role in determining whether its modified version \(x_{m}\) belongs to another category, we use it to reflect the robustness of \(x^{p}\). Typically, the higher the value of \(_{r}\), the more robust the poisoned sample \(x^{p}\) is.

### Observations from Attacks

To explore the correlation between the visibility of triggers and the robustness of their host poisoned samples, we conduct an experiment on the dataset CIFAR-10 against ten backdoor attacks, including both visible and invisible attacks, where each attack is applied on 200 randomly selected samples from CIFAR-10. Note that in the experiment, we assume that each poisoned sample is touched by only one kind of backdoor attack. In other words, each poisoned sample is embedded with a specific trigger. For each poisoned sample, we calculate the visibility score \(v\) of the trigger and the robustness \(_{r}\) of the poisoned sample. As shown in Figure 2, for different poisoned counterparts of a clean sample, their visibility and robustness scores differ significantly under different backdoor attacks.

Figure 2 illustrates the \((v\),\(_{r})\) pairs for all the poisoned samples under ten attacks (detailed in Section 5.1), where each filled pentagram symbol represents a poisoned sample. From the figure, we can get the following two observations.

**Observation 1.** For backdoor triggers with low visibility (\(v<0.13\)), their host samples (i.e., ones with \(_{r}<0.18\) located in the red box) have low robustness. According to , the triggers within these poisoned samples can be easily destroyed by lightweight noises.

**Observation 2.** For poisoned samples (i.e., those located in the blue box) with high robustness (\(_{r} 0.18\)), their backdoor triggers have high visibility, which renders them easily detectable and accurately located.

## 4 Our SampDetox Approach

### Threat Model

We consider defending against backdoor attacks in black-box scenarios [17; 18], where defenders do not have access to the parameters of deep models and can only obtain predictions for samples. For attackers, we assume that they can completely control the training processes of models, enabling them to poison samples in datasets and modify model components to achieve poisoned models. In our approach, defenders strive to detoxify poisoned samples to classify them into their original correct categories. For clean samples, defenders aim to maintain their classification accuracy.

### Overview of SampDetox

As observed in Section 3.2, there exists a strong correlation between the visibility of triggers and the robustness of poisoned samples, which can be used for backdoor defense. However, due to the lack of prior knowledge of clean samples, it is difficult to figure out both the trigger visibility and robustness of poisoned samples. To defend various backdoor attacks, Figure 3 presents a novel perturbation-based backdoor defense framework using our proposed sample detoxification method, which consists of two stages:

**Stage 1: Global Detoxification.** According to Observation 1, backdoor triggers with low visibility typically have low robustness poisoned samples. In this case, such triggers can be easily destroyed by adding lightweight noises to suspected samples. However, due to the low visibility or even invisibility, it is hard to determine the existence or location of triggers within a given sample. To destroy all these potential triggers, Stage 1 applies the noise-based perturbation (see Section 4.3) globally to each sample using lightweight noises. Note that the introduction of noises inevitably affects the inference accuracy of deep models. To counteract the impacts of added noises on inference performance, we then utilize DDPM-based denoising (see Section 4.4) to denoise the samples. In this way, Stage 1 can destroy possible triggers spread over the poisoned samples with low robustness.

**Stage 2: Local Detoxification.** According to Observation 2, poisoned samples with high robustness are often embedded with highly visibility triggers. Based on DDPM-based denoising in Stage 1, the clean regions of poisoned samples can easily be restored to their original states, while the poisoned regions cannot, especially when triggers are highly visible. Due to this fact, by comparing a potentially poisoned sample with its detoxified version by Stage 1 pixel by pixel, we can figure out the positions of obvious triggers and the required intensity of local noises. Then, Stage 2 applies the noise-based perturbation locally on each sample using pixel-specific noises to destroy such triggers. Similarly to Stage 1, the added noises in Stage 2 also greatly influence the final inference accuracy for both clean and detoxified poisoned samples, thus requiring diffusion models to perform the denoising. After Stage 2, both visible and invisible triggers can be safely eliminated without degrading the inference performance.

### Noise-based Perturbation

To quantitatively control the intensity of noises applied on samples in each stage of SampDetox, we resort to the forward Denoising Diffusion Probabilistic Model (DDPM) , which can also be used to determine the number of denoising steps required in the DDPM-based denoising process. Assume that \(x_{0}\) is an input sample, and \(x_{i}\) is the sample after adding noises for \(i\) times (steps). In the forward process of DDPM, the model incrementally adds noises to \(x_{0}\) step by step until the sample (denoted by \(x_{T}\)) is completely composed of random Gaussian noises at step \(T\), where \(T\) is a diffusion model-specific constant. In each step \(t\), \(x_{t}\) is obtained by adding a random Gaussian noise \((,)\) to \(x_{t-1}\) based on the formula \(x_{t}{=}}x_{t-1}+}\), where \(_{t}{}(0,1)\). For a given \(x_{t-1}\), the posterior probability of \(x_{t}\) can be represented as \(q(x_{t}|x_{t-1}){=}(x_{t};}x_{t-1},_{t} )\). Since step \(t\) only relies on step \(t{-}1\), the forward process can be regarded as a Markov process in the form of \(q(x_{t}|x_{0}){=}_{t=1}^{T}q(x_{t}|x_{t-1})\). For a given \(x_{0}\), we can get the probability of \(x_{t}\) as

\[P(x_{t}|x_{0})=(x_{t};_{t}}x_{0},(1- _{t})\,),\] (1)

where \(_{t}{=}1{-}_{t}\) and \(_{t}{=}_{i=1}^{t}(1-_{t})\). According to , we can get the exact relationship between \(x_{0}\) and \(x_{t}\) using the formula

\[x_{t}=_{t}}x_{0}+_{t}}z,\] (2)

where \(z{}(,)\) is a random Gaussian noise.

Figure 3: Framework and workflow of our SampDetox approachLeveraging the forward DDPM, our approach aims to add proper random Gaussian noise on samples within \(t\) steps based on Equation 2, which can destroy backdoor triggers in samples. Therefore, Theorem 4.1 investigates the potential of DDPM to eliminate triggers within poisoned samples.

**Theorem 4.1**.: _Suppose that \(x_{t}\) represents a diffused sample at step \(t\) of the forward process in DDPM, where \(t[0,T]\). Given a clean sample \(x_{0}^{c}\) and its poisoned version \(x_{0}^{p}\), we obtain \(x_{t}^{c}\) and \(x_{t}^{p}\) through the forward DDPM, whose distributions are \(P_{t}(x)\) and \(Q_{t}(x)\), respectively. By using the Kullback-Leibler divergence  to measure the distance between the two distributions, we have_

\[KL(P_{t+1}||Q_{t+1}) KL(P_{t}||Q_{t}),\] (3)

_where the equality happens only when \(P_{t}\)=\(Q_{t}\)., and the inequality implies that the KL divergence between \(P_{t}\) and \(Q_{t}\) monotonically decreases along with the forward DDPM._

Please refer to Appendix B.1 for the proof of the theorem. According to Theorem 4.1, for any positive number \(\), there exists a minimum \([0,T]\) such that for any \(t\) we have \(KL(P_{t}||Q_{t})\). Note that this theorem forms the theoretical basis to utilize the forward DDPM to minimize the distribution discrepancy between poisoned and clean samples, thus mitigating the impact of backdoor triggers.

### DDPM-based Denoising

We also utilize DDPM to perform denoising on samples within the two stages of SampDetox. For each pixel in a sample, the number of denoising steps is determined by its noise-based perturbation steps (i.e., \(\)) as introduced in Section 4.3. In reverse DDPM, by taking the current sample state \(x_{t}\) as an input, the diffusion model can be used to achieve its previous state \(x_{t-1}\). Given both \(x_{0}\) and \(x_{t}\), according to Bayes' Theorem, we can obtain the probability distribution of \(x_{t-1}\) as \(p(x_{t-1}|x_{t},x_{0})\)=\(p(x_{t}|x_{t-1}) p(x_{t-1}|x_{0})/p(x_{t}|x_{0})\). By resorting to the relationship between \(x_{0}\) and \(x_{t}\) described by Equation 2, according to , we can use \(x_{t}\) to approximate \(x_{0}\) as follows:

\[x_{0}=_{t}}}(x_{t}-_{t}}z_{t}), \ z_{t}=(x_{t},t),\] (4)

where \(z_{t}\) is predicted by the pre-trained neural network \(\) of DDPM, denoting the estimation of the real noise in step \(t\). Accordingly, we can obtain \(x_{t-1}\) from \(x_{t}\):

\[x_{t-1}=}}(x_{t}-}{_{t}}}(x_{t},t))+_{t}z,\] (5)

where \(_{t}^{2}=_{t-1}}{1-_{t}} _{t}\) and \(z(,)\).

Unlike existing DDPM-based methods that strive to restore samples from Gaussian noises, SampDetox uses diffusion models to denoise samples generated by our noise-based perturbation approach (see Section 4.3). Assuming that the training data used for the pre-trained diffusion model do not involve any knowledge about backdoor triggers, according to the principle of DDPM, the denoised results will restore the original samples with all the triggers cleansed. Theorem 4.2 investigates the difference between clean and poisoned regions of poisoned samples after denoising.

**Theorem 4.2**.: _Suppose that \(R(x)\) denotes a specific region of the sample \(x\). Given a poisoned sample \(x_{0}^{p}\), we use \(R_{c}(x_{0}^{p})\) and \(R_{p}(x_{0}^{p})\) to represent a clean region and a poisoned region of the sample, respectively. We perform the noise-based perturbation on \(x_{0}^{p}\) for \(\) steps and get the noisy samples \(x_{t}^{p}\), and then utilize DDPM to get a denoised sample \(_{0}^{p}\). When comparing different regions of the original sample \(x_{0}^{p}\) with their counterparts of the detoxified sample \(_{0}^{p}\), we have_

\[(_{0}^{p})-R_{p}(x_{0}^{p})||^{2}}{n_{1}}> (_{0}^{p})-R_{c}(x_{0}^{p})||^{2}}{n_{2}},\] (6)

_where \(n_{1}\) and \(n_{2}\) represent the numbers of pixels in \(R_{p}(x_{0}^{p})\) and \(R_{c}(x_{0}^{p})\), respectively._

Please refer to Appendix B.2 for the proof of the theorem. According to Theorem 4.2, we can calculate the similarity between the denoised and original sample to identify the positions of trigger patterns. The inequality in Theorem 4.2 implies that for a poisoned sample, its poisoned regions, compared with clean regions, will be more different from their counterparts in the restored version.

### Implementation of SampDetox

Algorithm 1 details the implementation of SampDetox. Lines 1-6 describe the process of global detoxification, while Lines 8-21 present the local detoxification process. Specifically, Line 1 applies the noise-based perturbation to the sample following Equation 2 with a fixed noise intensity \(_{1}\). Lines 2-6 show the process of DDPM-based denoising to obtain the denoised sample \(_{0}\). In Line 7, SampDetox calculates the SSIM score for each pixel between the original sample \(x_{0}\) and its denoised version \(_{0}\). Lines 8-11 describe how a noise with a specific intensity (i.e., \(m[i]\)) is added to the pixel \(i\). Lines 12-21 present the DDPM-based denoising process, whether SampDetox iteratively calculates the estimation of the real noise \(z_{t}\) with \(x_{t}\) using the diffusion model \(\) and then selectively denoises each pixel accordingly. In Lines 16-19, SampDetox only updates pixels with noise intensity higher than that of the current step. Finally, Line 22 returns the resulting detoxified sample \(x_{0}\), which is the denoised sample by Stage 2.

## 5 Experiments

To evaluate the effectiveness of our approach, we implemented our approach, i.e., SampDetox, on top of Pytorch (version 1.13.0). We compared the defense performance between SampDetox and state-of-the-art (SOTA) black-box defense methods against various well-known backdoor attacks. All experiments were carried out on an Ubuntu workstation equipped with one Intel i7-13700K CPU, 64GB memory, and one NVIDIA GeForce RTX4090 GPU.

### Experimental Setup

**Dataset and Model Settings.** We investigated three classical datasets (i.e., CIFAR-10 , GTSRB , and Tiny-ImageNet ) and three models (i.e., PreAct-ResNet18, ResNet34  and VGG-19 ). Due to space limitations, this section only presents the experimental results of PreAct-ResNet18 on the CIFAR-10 dataset. Note that we can find similar trends from the experiments using different models and datasets. Please refer to Appendix C for more details.

**Evaluation Metrics.** To objectively evaluate the effectiveness of a given backdoor defense method, we adopted the following three metrics: i) Clean sample Accuracy (CA), which denotes the inference accuracy of clean samples processed by the defense method; ii) Poisoned sample Accuracy (PA), which indicates the inference accuracy of poisoned samples purified (detoxified) by the defense method based on their ground-truth labels; and iii) Attack Success Rate (ASR), which represents the rate of successful attacks by poisoned samples. Note that a higher CA means that the defense method causes less interference in the classification of clean samples. Typically, backdoor defenders strive to achieve both high CA and PA with lowered ASR.

**Attack Methods.** We conducted defenses against ten SOTA backdoor attacks, i.e., BadNets , SIG , Label Consistent (LC) , TrojanNN , Dynamic , Blended , Low Frequency (LF) , WaNet , ISBSA , and BPP . Note that the first five are visible attacks, while the last five are invisible attacks. We used the same benchmark settings in  to configure all these attack methods. Please refer to Appendix A.1 for more details.

**Defense Baselines.** We compared SampDetox with three SOTA sample purification-based backdoor defenses: i) Sancdifi , which utilizes the RISE algorithm to measure the saliency maps and purify the specific areas of samples; ii) BDMAE , which uses a masked AutoEncoder to defend against backdoor attacks; and iii) ZIP , which applies linear transformations to samples and restores them under noises by using diffusion models. Note that both Sancdifi and BDMAE need to feed samples into deep models repeatedly to obtain their predictions, while ZIP does not rely on predictions. Please refer to Appendix A.2 for more details.

### Performance Comparison

Figure 4 shows an example of backdoor defense against the ten different attacks using our proposed SampDetox. Here, the first row presents both a clean sample and all its poisoned counterparts. The second row presents the triggers identified by stage 2 of SampDetox, while the third row exhibits the detoxified samples of each attack. From this figure, we can find that SampDetox can effectively figure out all visible and invisible triggers embedded by different SOTA backdoor attacks.

Table 1 compares the defense performance between SampDetox and the three baselines on the CIFAR-10 dataset using PreAct-ResNet18. Please see Appendix C.1 for the results of datasets GTSRB and Tiny-ImageNet. From this table, we can find that SampDetox can achieve the highest CA for all different attacks, indicating that the impact of SampDetox on clean samples is negligible. Meanwhile, SampDetox has the best PA and ASR for 8 out of 10 attacks, respectively. Even for all the remaining cases (i.e., BadNets, LC, and TrojanNN), our approach can achieve the second-best PA and ASR, whose values are quite close to the ones of the best PA and ASR. As an example of the BadNets attack, Sancdifi has the best ASR, which is only \(0.19\%\) lower than our approach. However, in this case, our approach significantly outperforms Sancdifi in CA by \(12.98\%\). For the TrojanNN attack, though Sancdifi can achieve slightly better PA (by \(0.89\%\)) and ASR (by \(0.05\%\)) than SampDetox, SampDetox notably outperforms Sancdifi in CA by \(16.15\%\). For the Label Consistent (LC) attack, ZIP can achieve slightly better PA, but its CA and ASR are worse than SampDetox. It is important to note that Sancdifi and BDMAE require feedback from deep models. As an alternative, our approach does not rely on underlying models, showing the applicability and superiority of our approach.

   Defense\(-\) &  &  &  &  &  \\  Attack \(\) & CA(\%) & PA(\%) & ASR(\%) & CA(\%) & PA(\%) & ASR(\%) & CA(\%) & PA(\%) & ASR(\%) & CA(\%) & PA(\%) & ASR(\%) & CA(\%) & PA(\%) & ASR(\%) \\  No Attack & 93.84 & - & - & - & - & - & - & - & - & - & - & - & - & - \\ BadNets & 92.00 & 10.18 & 99.97 & 76.59 & 89.55 & **1.92** & 89.02 & 90.10 & 2.32 & 88.12 & 86.52 & 7.17 & **89.57** & **90.15** & 2.11 \\ SIGN & 84.94 & 9.78 & 98.50 & 76.08 & 43.73 & 29.58 & 82.77 & 10.08 & 96.65 & 82.15 & 83.60 & 56.58 & **83.71** & **65.06** & **11.03** \\ LC & 84.34 & 10.26 & 99.06 & 68.44 & 51.78 & 3.64 & 79.75 & 73.39 & 2.01 & 79.85 & **74.92** & 2.06 & **80.72** & 74.36 & **1.55** \\ TrojanNN & 93.20 & 11.07 & 99.03 & 76.53 & **90.84** & **1.81** & 91.19 & 89.35 & 2.47 & 87.35 & 86.91 & 7.10 & **92.78** & 89.95 & 1.86 \\ Dynamic & 91.09 & 10.02 & 98.19 & 76.24 & 68.89 & 7.92 & 88.48 & 75.78 & 12.57 & 87.96 & 80.19 & 2.75 & **88.52** & **88.62** & **1.45** \\ Blended & 93.85 & 10.93 & 99.51 & 79.22 & 51.12 & 15.06 & 87.84 & 14.88 & 96.46 & 88.51 & 63.81 & 8.72 & **90.23** & **86.65** & **1.96** \\ LF & 93.63 & 11.13 & 99.48 & 77.92 & 49.95 & 16.57 & 87.50 & 13.63 & 80.97 & 88.76 & 86.59 & 5.85 & **90.01** & **87.40** & **3.02** \\ WANet & 91.43 & 10.27 & 91.05 & 77.87 & 42.97 & 14.35 & 85.95 & 23.19 & 50.63 & 86.91 & 85.22 & 8.36 & **89.34** & **88.92** & **5.59** \\ ISSBA & 93.57 & 11.38 & 95.96 & 77.70 & 52.05 & 14.20 & 86.18 & 53.12 & 22.29 & 87.75 & 85.46 & 1.79 & **90.74** & **86.51** & **1.60** \\ BPP & 91.38 & 9.46 & 98.40 & 75.32 & 50.42 & 15.25 & 86.69 & 21.73 & 53.46 & 85.42 & 82.94 & 7.20 & **90.59** & **84.83** & **6.15** \\   

Table 1: Defense performance comparison, where **Bold** fonts and **underlines** denote the best and the second-best values, respectively.

Figure 4: Illustration of poisoned samples, backdoor triggers, and their corresponding detoxified samples for the ten attacks, respectively.

### Ablation Studies

**Impacts of Different Stages and Denoising.** The implementation of SampDetox consists of two stages. The first stage strives to destroy triggers globally on a given sample in a coarse manner, while the second stage tries to identify fine triggers and remove them locally. To examine the impacts of these two stages, we consider a variant (i.e., "Stage 1") of our approach, which does not take stage 2 of SampDetox into account. Meanwhile, to evaluate the impact of the denoising operations, we considered another variant of SampDetox (i.e., "Noise**"), where denoising is not applied to restore the samples in both stages. Table 2 presents the ablation study results against the ten attacks. Note that to enable an intuitive understanding of the role of each stage in SampDetox, this table provides the average visibility of backdoor triggers for each attack.

For all the defense methods in Table 2, we can find that the smaller the trigger visibility caused by backdoor attacks, the lower PA and higher ASR we can achieve. Although "Noise**" can achieve comparable ASR results to SampDetox, SampDetox outperforms "Noise**" in both CA and PA significantly. It means that simply adding noises to samples can effectively destroy the backdoor triggers of the samples. However, the noises themselves will also result in a sharp decline in the inference accuracy of clean and poisoned samples. If we merely adopt the first stage of SampDetox due to the denoising operation by diffusion models, both CA and PA can be improved. However, since the first stage only coarsely applies the perturbation on samples, most subtle backdoor triggers still survive. As an example in SIG attack, "Stage 1" can only achieve a PA of \(9.48\%\) but with an ASR of \(92.33\%\). By combining both Stages 1 and 2, SampDetox can effectively identify such undetectable triggers and destroy them without affecting the inference of original samples, leading to a better PA and ASR. For example, in SIG attack, by using SampDetox, we can achieve a PA of \(65.06\%\) and an ASR of \(11.03\%\).

**Impact of Hyperparameters \(_{1}\) and \(_{2}\).** In our approach, we use two hyperparameters, i.e., \(_{1}\) and \(_{2}\) to control the noise intensity and denoising samples, respectively. We investigated two variants of SampDetox to evaluate the impacts of \(_{1}\) and \(_{2}\). For the first variant, we fixed the value of \(_{2}\) (i.e., \(_{2}\)=0) while allowing the tuning of \(_{1}\). With different values of \(_{1}\), we can evaluate the defense performance of SampDetox against various invisible backdoor attacks (i.e., Blended, Low Frequency, WaNet, ISSBA, and BPP). Similarly, for the second variant, we fixed the value of \(_{1}\) (i.e., \(_{1}\)=20) while changing the values of \(_{2}\), to evaluate the defense performance of SampDetox against visible backdoor attacks (i.e., BadNets, SIG, LC, TrojanNN, and Dynamic). Table 3 shows the ablation study results with varying values of \(_{1}\) or \(_{2}\). Note that the results in the table represent the average results across multiple attack types. From this table, we can observe that when \(_{1}\) and \(_{2}\) increase, the ASR decreases, indicating the effect of adding noises in destroying backdoor triggers. However, a high value of \(_{1}\) and \(_{2}\) will also lead to a significant reduction of CA and PA, showing that the denoising processes are strongly affected by excessive noises. Therefore, we suggest to set \(_{1}=20\) and \(_{2}=120\) in practice.

    &  &  &  &  \\   & \(v\) & CA(\%) & PA(\%) & ASR(\%) & CA(\%) & PA(\%) & ASR(\%) & CA(\%) & PA(\%) & ASR(\%) \\  BadNets & 0.052 & 56.25 & 49.71 & 3.93 & 90.12 & 88.15 & 6.61 & 89.57 & 90.15 & 2.11 \\ SIG & 0.185 & 51.81 & 17.16 & 15.89 & 83.10 & 9.48 & 92.33 & 83.71 & 65.06 & 11.03 \\ LC & 0.121 & 47.75 & 28.04 & 1.07 & 81.50 & 61.45 & 34.75 & 80.72 & 74.36 & 1.55 \\ TrojanNN & 0.137 & 58.03 & 45.89 & 5.68 & 92.54 & 35.51 & 46.86 & 92.78 & 89.95 & 1.86 \\ Dynamic & 0.098 & 56.26 & 41.18 & 1.09 & 87.58 & 85.62 & 3.82 & 87.52 & 88.62 & 1.45 \\ Blended & 0.067 & 59.42 & 45.22 & 1.53 & 88.65 & 86.65 & 1.86 & 90.23 & 86.65 & 1.96 \\ LF & 0.005 & 56.12 & 40.95 & 2.55 & 89.43 & 87.50 & 3.02 & 90.01 & 87.40 & 3.02 \\ WaNet & 0.005 & 57.71 & 43.73 & 5.38 & 89.43 & 88.91 & 5.60 & 89.34 & 88.92 & 5.59 \\ ISSBA & 0.006 & 57.10 & 37.35 & 1.14 & 90.92 & 86.50 & 1.61 & 90.74 & 86.51 & 1.60 \\ BPP & 0.009 & 58.40 & 42.03 & 5.75 & 89.09 & 84.84 & 6.15 & 90.59 & 84.83 & 6.15 \\   

Table 2: Ablation study results of our approach against backdoor attacks with different visibility.

   _{2}=0\)} & _{1}=20\)} \\  \(_{1}\) & CA(\%) & PA(\%) & ASR(\%) & \(_{2}\) & CA(\%) & PA(\%) & ASR(\%) \\ 
5 & 92.07 & 62.48 & 27.58 & 40 & 92.19 & 60.90 & 30.07 \\
10 & 91.22 & 78.69 & 12.96 & 60 & 92.02 & 73.32 & 19.32 \\
15 & 90.92 & 86.35 & 5.39 & 80 & 91.86 & 79.38 & 14.11 \\
20 & 90.65 & 86.43 & 1.73 & 100 & 91.72 & 83.68 & 6.13 \\
25 & 88.26 & 85.13 & 1.80 & 120 & 92.02 & 85.22 & 2.34 \\
30 & 86.77 & 84.56 & 1.71 & 150 & 91.85 & 84.92 & 2.28 \\
35 & 84.91 & 83.78 & 1.75 & 200 & 92.26 & 81.87 & 2.30 \\
40 & 82.30 & 83.01 & 1.72 & 250 & 92.39 & 77.03 & 2.29 \\   

Table 3: Ablation study results for different \(_{1}\) and \(_{2}\).

### Discussion

To show the applicability of SampDetox, we investigated the extra time overheads, explored training sample detoxification, evaluated the defense performance of SampDetox against SOTA adaptive attacks, and analyzed its limitations. Due to the space limitation, this section only presents the overhead caused by inference detoxification. Please refer to Appendix D for more discussions.

Extra time overhead.Based on our proposed two-stage perturbation-based sample detoxification, SampDetox can protect any deep models during their inference phase. However, similar to other backdoor defense approaches, the detoxification process will inevitably lead to extra time overhead. To address this issue, SampDetox resorts to the Denoising Diffusion Implicit Model (DDIM)  to accelerate the denoising process. Figure 5 compares the average inference time per sample for different backdoor defense methods. We can find that with the help of DDIM, the inference time of SampDetox is comparable to that of the method without defense.

## 6 Conclusion

Based on our observed correlation between the visibility of backdoor triggers and the robustness of poisoned samples, this paper introduces a novel black-box backdoor defense approach named SampDetox. By using our proposed perturbation-based sample detoxification, SampDetox can easily destroy backdoor triggers in samples and restore the samples using diffusion models. Extensive experiments against various complex backdoor attacks show the superiority of SampDetox from both perspectives of effectiveness and applicability to arbitrary scenarios.