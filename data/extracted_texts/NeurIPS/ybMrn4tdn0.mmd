# Auditing Local Explanations is Hard

Robi Bhattacharjee

University of Tubingen and Tubingen AI Center

robi.bhattacharjee@usii.uni-tuebingen.de

Ulrike von Luxburg

University of Tubingen and Tubingen AI Center

ulrike.luxburg@uni-tuebingen.de

###### Abstract

In sensitive contexts, providers of machine learning algorithms are increasingly required to give explanations for their algorithms' decisions. However, explanation receivers might not trust the provider, who potentially could output misleading or manipulated explanations. In this work, we investigate an auditing framework in which a third-party auditor or a collective of users attempts to sanity-check explanations: they can query model decisions and the corresponding local explanations, pool all the information received, and then check for basic consistency properties. We prove upper and lower bounds on the amount of queries that are needed for an auditor to succeed within this framework. Our results show that successful auditing requires a potentially exorbitant number of queries - particularly in high dimensional cases. Our analysis also reveals that a key property is the "locality" of the provided explanations -- a quantity that so far has not been paid much attention to in the explainability literature. Looking forward, our results suggest that for complex high-dimensional settings, merely providing a pointwise prediction and explanation could be insufficient, as there is no way for the users to verify that the provided explanations are not completely made-up.

## 1 Introduction

Machine learning models are increasingly used to support decision making in sensitive contexts such as credit lending, hiring decisions, admittance to social benefits, crime prevention, and so on. In all these cases, it would be highly desirable for the customers/applicants/suspects to be able to judge whether the model's predictions or decisions are "trustworthy". New AI regulation such as the European Union's AI Act can even legally require this. One approach that is often held up as a potential way to achieve transparency and trust is to provide _local explanations_, where every prediction/decision comes with a human-understandable explanation for this particular outcome (e.g., LIME Ribeiro et al., 2016), SHAP Lundberg and Lee (2017), or Anchors Ribeiro et al. (2018)).

However, in many real-world scenarios, the explanation receivers may not necessarily trust the explanation providers Bordt et al. (2022). Imagine a company that uses machine learning tools to assist in screening job applications. Because the company is well-advised to demonstrate fair and equitable hiring, it is plausible that it might bias its explanations towards depicting these properties. And this is easy to achieve: the company is under full control of the machine learning model and the setup of the explanation algorithm, and prior literature Ghorbani et al. (2019); Dombrowski et al. (2019) has shown that current explainability tools can be manipulated to output desirable explanations.

This motivates the question: what restrictions or procedures could be applied to prevent such explanation cheating, and more specifically, what are ways to verify that the provided explanationsare actually trustworthy? One approach is to require that the explanation providers completely publicize their models, thus allowing users or third-party regulators to verify that the provided explanations are faithful to the actual model being used. However, such a requirement would likely face stiff resistance in settings where machine learning models are valuable intellectual property.

In this work, we investigate an alternative approach, where a third-party regulator or a collective of users attempt to verify the trustworthiness of local explanations, simply based on the predictions and explanations over a set of examples. The main idea is that by comparing the local explanations with the actual predictions across enough data one could, in principle, give an assessment on whether the provided explanations actually adhere to the explained model. The goal of our work is to precisely understand when this is possible.

### Our contributions: data requirements for auditing.

We begin by providing a general definition for local explainability that encompasses many popular explainability methods such as Anchors Ribeiro et al. (2018), Smooth-grad Smilkov et al. (2017), and LIME Ribeiro et al. (2016). We define a _local explanation_ for a classifier \(f\) at a point \(x\) as a pair \((R_{x},g_{x})\), where \(R_{x}\) is a local region surrounding \(x\), and \(g_{x}\) is a simple local classifier designed to approximate \(f\) over \(R_{x}\). For example, on continuous data, Anchors always output \((R_{x},g_{x})\) where \(R_{x}\) is a hyper-rectangle around \(x\) and \(g_{x}\) is a constant classifier; gradient-based explanations such as Smooth-grad or LIME implicitly approximate the decision function \(f\) by a linear function in a local region around \(x\).

Obviously, any human-accessible explanation that is being derived from such a local approximation can only be trustworthy if the local function \(g_{x}\) indeed approximates the underlying function \(f\) on the local region \(R_{x}\). Hence, a _necessary condition_ for a local explanation to be trustworthy is that the function \(g_{x}\) is close to \(f\) on the region \(R_{x}\), and this should be the case for most data points \(x\) sampled from the underlying distribution.

To measure how closely a set of local explanations adheres to the original classifier \(f\), we propose an explainability loss function \(L_{}(E,f)\), which quantifies the frequency with which \(f\) differs by more

Figure 1: **Local explanations** (see Section 2.2 for notation): In both panels, a set of training points \(x\) and their classifications \(f(x)\) (red/blue, decision boundary in green) are shown. For three training points (one centered at each ball), a local linear explanation \((g_{x},R_{x})\) is illustrated where \(g_{x}\) is a local linear classifier (black decision boundary) and \(R_{x}\) is a local ball centered at \(x\). _Panel (a)_ depicts a regime where there is _insufficient_ data for verifying how accurate the local explanations approximate the classifier \(f\) â€“ none of the provided regions contain enough points to assess the accuracy of the linear explanations. _Panel (b)_ depicts a regime with more training points allowing us to validate the accuracy of the linear explanations based on how closely they align with the points in their corresponding regions.

than \(\) from the local classifier \(g_{x}\) over the local region \(R_{x}\) (see Sec. 2.2 for precise definitions). We then introduce a formalism for _auditing local explanations_ where an auditor attempts to estimate the explainability loss \(L_{}(E,f)\). In our formalism, the auditor does so with access to the following objects:

1. A set of data points \(X=\{x_{1},,x_{n}\}\) drawn i.i.d from the underlying data distribution.
2. The outputs of a classifier on these points, \(f(X)=\{f(x_{1}),,f(x_{n})\}\).
3. The provided local explanations for these points \(E(f,X)=\{E(f,x_{1}),,E(f,x_{n})\}\)

Observe that in our formalism, the auditor has only restricted access to the machine learning model and the explanations: they can only interact with them through their evaluations at specific data-points. We have chosen this scenario because we believe it to be the most realistic one in many practical situations, where explanation providers try to disclose as little information on their underlying machine learning framework as possible.

In our main result, Theorem 4.1, we provide a lower bound for the amount of data needed for an auditor to accurately estimate \(L_{}(E,f)\). A key quantity in our analysis is the _locality_ of the provided explanations. We show that the smaller the provided local regions \(R_{x}\) are, the more difficult it becomes to audit the explainer. Intuitively, this holds because estimating the explainability loss relies on observing multiple points within these regions, as illustrated in Panel (b) of Figure 1. By contrast, if this fails to hold (Panel (a)), then there is no way to validate how accurate the local explanations are. We also complement our lower bound with an upper bound (Theorem 4.2) that demonstrates that reasonably large local regions enable auditing within our framework.

Our results imply that the main obstacle to auditing local explanations in this framework is the locality of the provided explanations. As it turns out, this quantity is often _prohibitively small in practice_, making auditing _practically impossible_. In particular, for high-dimensional applications, the local regions \(R_{x}\) given by the explainer are often exponentially small in the data-dimension. Thus the explanations cannot be verified in cases where there does not exist any prior trust between the explanation provider and the explanation receivers.

We stress that estimating the local loss \(L_{}(E,f)\) serves as a first _baseline_ on the path towards establishing trustworthy explanations. It is very well possible that an explanation provider achieves a small local loss (meaning that the local classifiers closely match the global classifier \(f\)) but nevertheless provides explanations that are misleading in some other targeted manner. Thus, we view successful auditing in this setting as a _necessary but not sufficient_ condition for trusting an explanation provider.

Our results might have far-reaching practical consequences. In cases where explanations are considered important or might even be required by law, for example by the AI Act, it is a necessary requirement that explanations can be verified or audited (otherwise, they would be completely useless). Our results suggest that in the typical high dimensional setting of modern machine learning, auditing pointwise explanations is impossible if the auditor only has access to pointwise decisions and corresponding explanations. In particular, collectives of users, for example coordinated by non-governmental organizations (NGOs), are never in the position to audit explanations. The only way forward in auditing explanations would be to appoint a third-party auditor who has more power and _more access to the machine learning model_, be it access to the full specification of the model function and its parameters, or even to the training data. Such access could potentially break the fundamental issues posed by small local explainability regions in our restricted framework, and could potentially enable the third party auditor to act as a moderator to establish trust between explanation receivers and explanation providers.

### Related Work

Prior work (Yadav et al., 2022; Bhatt et al., 2020; Oala et al., 2020; Poland, 2022) on auditing machine learning models is often focused on applying explainability methods to audit the models, rather than the explanations themselves. However, there has also been recent work (Leavitt and Morcos, 2020; Zhou et al., 2021) arguing for more rigorous ways to evaluate the performance of various explanation methods. There are numerous approaches for doing so: including performance based on human-evaluation (Jesus et al., 2021; Poursabzi-Sangdeh et al., 2021), and robustness (Alvarez-Melis and Jaakkola, 2018).

There has also been a body of work that evaluates explanations based on the general notion of faithfulness between explanations and the explained predictor. Many approaches (Wolf et al., 2019; Poppi et al., 2021; Tomsett et al., 2020) examine neural-network specific measures, and typically rely on access to the neural network that would not be present in our setting. Others are often specialized to a specific explainability tool - with LIME (Visani et al., 2022; Botari et al., 2020) and Shap (Huang and Marques-Silva, 2023) being especially popular choices.

By contrast, our work considers a general form of local explanation, and studies the problem of auditing such explanations in a restricted access setting, where the auditor only interacts with explanations through queries. To our knowledge, the only previous work in a similar setting is (Dasgupta et al., 2022), in which local explanations are similarly audited based on collecting them on a set of data sampled from a data distribution. They consider a quantity called the _local sufficiency_, which directly corresponds to our notion of local loss (Definition 2.3). However, their work is restricted to a _discrete_ setting where local fidelity is evaluated based on instances that receive _identical_ explanations. In particular, they attempt to verify that points receiving identical explanations also receive identical predictions. By contrast, our work lies within a _continuous_ setting, where a local explanation is said to be faithful if it matches the underlying model over a _local region_.

A central quantity to our analysis is the _locality_ of an explanation, which is a measure of how large the local regions are. Prior work has rarely measured or considered this quantity, with a notable exception being Anchors method (Ribeiro et al., 2018) which utilizes it to assist in optimizing their constructed explanations. However, that work did not explore this quantity beyond treating it as a fixed parameter.

Finally, we note that other recent work, such as (Bassan and Katz, 2023), provides avenues for providing explanations with _certifiable correctness_, meaning that they provide proof that their accurate reflect the underlying model. We view our work as complementary to such methods as our work demonstrates the _necessity_ of such ideas by demonstrating difficulties with using _generic local explanation methods_.

## 2 Local Explanations

### Preliminaries

In this work, we restrict our focus to _binary classification_ - we let \(\) denote a data distribution over \(^{d}\), and \(f:^{d}\{ 1\}\) be a so-called black-box binary classifier that needs to be explained. We note that lower bounds shown for binary classification directly imply lower bounds in more complex settings such as multi-class classification or regression.

For any measurable set, \(M^{d}\), we let \((M)\) denote the probability mass \(\) assigns \(M\). We will also let \(supp()\) denote the _support_ of \(\), which is the set of all points \(x\) such that \((\{x^{}:||x-x^{}|| r\})>0\) for all \(r>0\).

We define a **hyper-rectangle** in \(^{d}\) as a product of intervals, \((a_{1},b_{1}](a_{d},b_{d}]\), and let \(_{d}\) denote the set of all hyper-rectangles in \(^{d}\). We let \(_{d}\) denote the set of all \(L_{2}\)-balls in \(^{d}\), with the ball of radius \(r\) centered at point \(x\) being defined as \(B(x,r)=\{x^{}:||x-x^{}|| r\}\).

We will utilize the following two simple hypothesis classes: \(_{d}\), which is the set of the two constant classifiers over \(^{d}\), and \(_{d}\), which is the set of all linear classifiers over \(^{d}\). These classes serve as important examples of _simple and interpretable classifiers_ for constructing local explanations.

### Defining local explanations and explainers

One of the most basic and fundamental concepts in Explainable Machine Learning is the notion of a _local explanation_, which, broadly speaking, is an attempt to explain a complex function's behavior at a specific point. In this section, we describe a general form that such explanations can take, and subsequently demonstrate that two widely used explainability methods, LIME and Anchors, adhere to it.

We begin by defining a _local explanation_ for a classifier at a given point.

**Definition 2.1**.: For \(x^{d}\), and \(f:^{d}\{ 1\}\), a **local explanation** for \(f\) at \(x\) is a pair \((R_{x},g_{x})\) where \(R_{x}^{d}\) is a region containing \(x\), and \(g_{x}:R_{x}\{ 1\}\) is a classifier.

Here, \(g_{x}\) is typically a simple function intended to approximate the behavior of a complex function, \(f\), over the region \(R_{x}\). The idea is that the local nature of \(R_{x}\) simplifies the behavior of \(f\) enough to provide intuitive explanations of the classifier's local behavior.

Next, we define a _local explainer_ as a map that outputs local explanations.

**Definition 2.2**.: \(E\) is a **local explainer** if for any \(f:^{d}\{ 1\}\) and any \(x^{d}\), \(E(f,x)\) is a local explanation for \(f\) at \(x\). We denote this as \(E(f,x)=(R_{x},g_{x})\).

We categorize local explainers based on the types of explanations they output - if \(\) denotes a set of regions in \(^{d}\), and \(\) denotes a class of classifiers, \(^{d}\{ 1\}\), then we say \(E(,)\) if for all \(f,x\), \(E(f,x)\) outputs \((R_{x},g_{x})\) with \(R_{x}\) and \(g_{x}\).

Local explainers are typically constructed for a given classifier \(f\) over a given data distribution \(\). In practice, different algorithms employ varying amounts of access to both \(f\) and \(\) - for example, SHAP crucially relies on data sampled from \(\) whereas gradient based methods often rely on knowing the actual parameters of the model, \(f\). To address all of these situations, our work takes a black-box approach in which we make no assumptions about how a local explainer is constructed from \(f\) and \(\). Instead we focus on understanding how to evaluate how effective a given explainer is with respect to a classifier \(f\) and a data distribution \(\).

### Examples of Explainers

We now briefly discuss how various explainability tools in practice fit into our framework of local explanations.

Anchors:The main idea of Anchors (Ribeiro et al., 2018) is to construct a region the input point in which the desired classifier to explain remains (mostly) constant. Over continuous data, it outputs a local explainer, \(E\), such that \(E(x)=(R_{x},g_{x})\), where \(g_{x}\) is a constant classifier with \(g_{x}(x^{})=f(x)\) for all \(x^{}^{d}\), and \(R_{x}\) is a hyper-rectangle containing \(x\). It follows say that the Anchors method outputs an explainer in the class, \((_{d},_{d})\).

Gradient-Based Explanations:Many popular explainability tools (Smilkov et al., 2017; Agarwal et al., 2021; Ancona et al., 2018) explain a model's local behavior by using its gradient. By definition, gradients have a natural interpretation as a locally linear model. Because of this, we argue that gradient-based explanations are implicitly giving local explanations of the form \((R_{x},g_{x})\), where \(R_{x}=B(x,r)\) is a small \(L_{2}\) ball centered at \(x\), and \(g_{x}\) is a linear classifier with coefficients based on the gradient. Therefore, while the radius \(r\) and the gradient \(g_{x}\) being used will vary across explanation methods, the output can be nevertheless interpreted as an explainer in \((_{d},_{d})\), where \(_{d}\) denotes the set of all \(L_{2}\)-balls in \(^{d}\), and \(_{d}\) denotes the set of all linear classifiers over \(^{d}\).

Lime:At a high level, LIME (Ribeiro et al., 2016) also attempts to give local linear approximations to a complex model. However, unlike gradient-based methods, LIME includes an additional feature-wise discretization step where points nearby the input point, \(x\), are mapped into a binary representation in \(\{0,1\}^{d}\) based on how similar a point is to \(x\). As a consequence, LIME can be construed as outputting local explanations of a similar form to those outputted by gradient-based methods.

Finally, as an important limitation of our work, although many well-known local explanations fall within our definitions, this does not hold in all cases. Notably, Shapley-value (Lundberg and Lee, 2017) based techniques do not conform to the format given in Definition 2.1, as it is neither clear how to construct local regions that they correspond to, nor the precise local classifier being used.

### A measure of how accurate an explainer is

We now formalize what it means for a local classifier, \(g_{x}\), to "approximate" the behavior of \(f\) in \(R_{x}\).

**Definition 2.3**.: For explainer \(E\) and point \(x\), we let the **local loss**, \(L(E,f,x)\) be defined as the fraction of examples drawn from the region \(R_{x}\) such that \(g_{x}\) and \(f\) have different outputs. More precisely, we set

\[L(E,f,x)=_{x^{}}[g_{x}(x^{}) f(x)|x^{} R_{ x}].\]\(\) is implicitly used to evaluate \(E\), and is omitted from the notation for brevity. We emphasize that this definition is specific to _classification_, which is the setting of this work. A similar kind of loss can be constructed for regression tasks based on the mean-squared difference between \(g_{x}\) and \(f\).

We contend that maintaining a low local loss across most data points is _essential_ for any reasonable local explainer. Otherwise, the explanations provided by the tool can be made to support any sort of explanation as they no longer have any adherence to the original function \(f\).

To measure the overall performance of an explainer over an entire data distribution, it becomes necessary to aggregate \(L(E,f,x)\) over all \(x\). One plausible way to accomplish this would be to average \(L(E,f,x)\) over the entire distribution. However, this would leave us unable to distinguish between cases where \(E\) gives extremely poor explanations at a small fraction of points as opposed to giving mediocre explanations over a much larger fraction. To remedy this, we opt for a more precise approach in which a user first chooses a **local error threshold**, \(0<<1\), such that local explanations that incur an explainabiliy loss under \(\) are considered acceptable. They then measure the global loss for \(E\) by determining the fraction of examples, \(x\), drawn from \(\) that incur explainability loss above \(\).

**Definition 2.4**.: Let \(>0\) be a user-specified local error threshold. For local explainer \(E\), we define the **explainability loss**\(L_{}(E,f)\) as the fraction of examples drawn from \(\) that incur a local loss larger than \(\). That is,

\[L_{}(E,f)=_{x}[L(E,f,x)].\]

We posit that the quantity \(L_{}(E,f)\) serves as an overall measure of how faithfully explainer \(E\) adheres to classifier \(f\), with lower values of \(L_{}(E,f)\) corresponding to greater degrees of faithfulness.

### A measure of how large local regions are

The outputted local region \(R_{x}\) plays a crucial role in defining the local loss. On one extreme, setting \(R_{x}\) to consist of a single point, \(\{x\}\), can lead to a perfect loss of \(0\), as the explainer only needs to output a constant classifier that matches \(f\) at \(x\). But these explanations would be obviously worthless as they provide no insight into \(f\) beyond its output \(f(x)\). On the other extreme, setting \(R_{x}=^{d}\) would require the explainer to essentially replace \(f\) in its entirety with \(g_{x}\), which would defeat the purpose of explaining \(f\) (as we could simply use \(g_{x}\) instead). Motivated by this observation, we define the _local mass_ of an explainer at a point \(x\) as follows:

**Definition 2.5**.: The **local mass** of explainer \(E\) with respect to point \(x\) and function \(f\), denoted \((E,f,x)\), is the probability mass of the local region outputted at \(x\). That is, if \(E(f,x)=(R_{x},g_{x})\), then

\[(E,f,x)=_{x^{}}[x^{} R_{x}].\]

Based on our discussion above, it is unclear what an ideal local mass is. Thus, we treat this quantity as a property of local explanations rather than a metric for evaluating their validity. As we will later see, this property is quite useful for characterizing how difficult it is to estimate the explainability loss of an explainer. We also give a global characterization of the local mass called _locality_.

**Definition 2.6**.: The **locality** of explainer \(E\) with respect to function \(f\), denoted \((E,f)\), is the minimum local mass it incurs. That is, \((E,f)=_{x supp()}(E,f,x)\).

## 3 The Auditing Framework

Recall that our goal is to determine how explanation receivers can verify provided explanations in situations where there _isn't_ mutual trust. To this end, we provide a framework for _auditing local explanations_, where an auditor attempts to perform this verification with as little access to the underlying model and explanations as possible. Our framework proceeds in with the following steps.

1. The auditor fixes a local error threshold \(\).
2. A set of points \(X=\{x_{1},,x_{n}\}\) are sampled i.i.d from data distribution \(\).
3. A black-box classifier \(f\) is applied to these points. We denote these values with \(f(X)=\{f(x_{1}),,f(x_{n})\}\).

4. A local explainer \(E\) outputs explanations for \(f\) at each point. We denote these explanations with \(E(f,X)=\{E(f,x_{1}),,E(f,x_{n})\}\).
5. The Auditor outputs an estimate \(A(X,f(X),E(f,X))\) for the explainability loss.

Observe that the auditor can only have access to the the model \(f\) and its corresponding explanations _through_ the set of sampled points. Its only inputs are \(X\), \(f(X)\), and \(E(f,X)\). In the context of the job application example discussed in Section 1, this would amount to auditing a company based on the decisions and explanations they provided over a set of applicants.

In this framework, we can define the sample complexity of an auditor as the amount of data it needs to guarantee an accurate estimate for \(L_{}(E,f)\). More precisely, fix a data distribution, \(\), a classifier, \(f\), and an explainer \(E\). Then we have the following:

**Definition 3.1**.: For tolerance parameters, \(_{1},_{2},>0\), and local error threshold, \(>0\), we say that an auditor, \(A\), has **sample complexity**\(N(_{1},_{2},,)\) with respect to \(,E,f\), if for any \(n N(_{1},_{2},,)\), with probability at least \(1-\) over \(X=\{x_{1},,x_{n}\}^{n}\), \(A\) outputs an accurate estimate of the explainability loss, \(L_{}(E,f)\). That is,

\[L_{(1+_{1})}(E,f)-_{2} A(X,f(X),E(f,X))  L_{(1-_{1})}(E,f)+_{2}.\]

Next, observe that our sample complexity is specific to the distribution, \(\), the classifier, \(f\), and the explainer, \(E\). We made this choice to understand the challenges that different choices of \(\), \(f\), and \(E\) pose to an auditor. As we will later see, we will bound the auditing sample complexity using the locality (Definition 2.5), which is a quantity that depends on \(\), \(f\), and \(E\).

## 4 How much data is needed to audit an explainer?

### A lower bound on the sample complexity of auditing

We now give a lower bound on the amount of data needed to successfully audit an explainer. That is, we show that for any auditor \(A\) and any data distribution \(\) we can find some explainer \(E\) and some classifier \(f\) such that \(A\) is highly likely to give an inaccurate estimate of the explainability loss. To state our theorem we use the following notation and assumptions. Recall that \(_{d}\) denotes the set of hyper-rectangles in \(^{d}\), and that \(_{d}\) denotes the set of the two constant binary classifiers over \(^{d}\). Additionally, we will include a couple of mild technical assumptions about the data distribution \(\). We defer a detailed discussion of them to Appendices A.3 and A.1. We now state our lower bound.

**Theorem 4.1** (**lower bound on the sample complexity of auditing)**.: _Let \(_{1},_{2}<\) be tolerance parameters, and let \(<\) be any local error threshold. Let \(\) be any non-degenerate distribution, and \(>0\) be any desired level of locality. Then for any auditor \(A\) there exists a classifier \(f:^{d}\{ 1\}\) and an explainer \(E(_{d},_{d})\) such that the following conditions hold._

1. \(E\) _has locality_ \((E,f)=\)_._
2. _There exist absolute constants_ \(c_{0},c_{1}>0\) _such that if the auditor receives_ \[n}{(_{1},_{2})^{1-c_{1}( _{1},_{2})}}\] _many points, then with probability at least_ \(\) _over_ \(X=\{x_{1},,x_{n}\}^{n}\)_,_ \(A\) _gives an inaccurate estimate of_ \(L_{}(E,f)\)_. That is,_ \[A(X,f(X),E(f,X))[L_{(1+_{1})}(E,f)-_{2 },L_{(1-_{1})}(E,f)+_{2}].\]

In summary, Theorem 4.1 says that auditing an explainer requires an amount of data that is _inversely proportional_ to its locality. Notably, this result does not require the data-distribution to be adversarially chosen, and furthermore applies when the explainer \(E\) can be guaranteed to have a remarkably simple form being in \((_{d},_{d})\).

Proof intuition of Theorem 4.1:The main intuition behind Theorem 4.1 is that estimating the local explainability loss, \(L(E,f,x)\), requires us to observe samples from the regions \(R_{x}\). This would allow us to obtain an empirical estimate of \(L(E,f,x)\) by simply evaluating the fraction of pointsfrom \(R_{x}\) that the local classifier, \(g_{x}\), misclassifies. This implies that the locality \(\) is a limiting factor as it controls how likely we are to observe data within a region \(R_{x}\).

However, this idea enough isn't sufficient to obtain our lower bound. Although the quantity \((})\) does indeed serve as a lower bound on the amount of data needed to guarantee seeing a large number of points within a region, \(R_{x}\), it is unclear what a sufficient number of observations within \(R_{x}\) is. Even if we don't have enough points in any single region, \(R_{x}\), to accurately estimate \(L(E,f,x)\), it is entirely plausible that aggregating loose estimates of \(L(E,f,x)\) over a sufficient number of points \(x\) might allow us to perform some type of estimation of \(L_{}(E,f)\).

To circumvent this issue, the key technical challenge is constructing a distribution of functions \(f\) and fixing \(m=O()\) such that observing fewer than \(m\) points from a given region, \(R_{x}\), actually provides _zero information_ about which function was chosen. We include a full proof in Appendix A.

### An upper bound on the sample complexity of auditing.

We now show that if \(\) is reasonably large, then auditing the explainability loss \(L_{}(E,f)\) can be accomplished. As mentioned earlier, we stress that succeeding in our setting is _not_ a sufficient condition for trusting an explainer - verifying that the local explanations \(g_{x}\) match the overall function \(f\) is just one property that a good explainer would be expected to have. Thus the purpose of our upper bound in this section is to complement our lower bound, and further support that the locality parameter \(\) is the main factor controlling the sample complexity of an auditor.

Our auditing algorithm proceeds by splitting the data into two parts, \(X_{1}\) and \(X_{2}\). The main idea is to audit the explanations given for points in \(X_{1}\) by utilizing the data from \(X_{2}\). If we have enough data, then it is highly likely for us to see enough points in each local region to do this. We defer full details for this procedure to Appendix B.1. We now give the an upper bound on its sample complexity.

**Theorem 4.2** (**Upper Bound on Sample Complexity of Algorithm 1)**.: _There exists an auditor, \(A\), for which the following holds. Let \(\) be a data distribution, \(f\) be a classifier, and \(E\) be an explainer. Suppose that \(E\) has locality \(\) with respect to \(\) and \(f\). Let \(_{1},_{2},\) be tolerance parameters and let \(>0\) be a local error threshold. Then \(A\) has sample complexity at most_

\[N(_{1},_{2},,)=(^{2}}+_{1}^{2}}).\]

This bound shows that the locality is sufficient for bounding the sample complexity for auditing local explanations. We defer a full proof to Appendix B. Observe that the dependency on \(\) is \(O()\) which matches the dependency in our lower bound provided that \(_{1},_{2} 0\).

## 5 The locality of practical explainability methods can be extremely small

Theorems 4.1 and 4.2 demonstrate that the locality \(\) characterizes the amount of data needed for an Auditor to guarantee an accurate estimate of the explainability loss \(L_{}(E,f)\). It follows that if \(\) is extremely small, then auditing could require a prohibitive amount of data. This leads to the following question: how small is \(\) for practical explainability algorithms? To answer this, we will examine examine several commonly used algorithms that adhere to our framework.

We begin with **gradient-based methods**, which can be construed as providing an explainer in the class \((_{d},_{d})\), where \(_{d}\) denotes the set of \(L_{2}\) balls in \(^{d}\), and \(_{d}\) denotes the set of linear classifiers. To understand the impact of dimension on the locality of such explainers, we begin with a simple theoretical example.

Let \(\) be the data distribution over \(^{d}\) that is a union of three concentric spheres. Specifically, \(x\) is equally likely to be chosen at uniform from the sets \(S_{1}=\{x:||x||=1-\}\), \(S_{2}=\{x:||x||=1\}\), and \(S_{3}=\{x:||x||=1+\}\), where \(,\) are small \(d\)-dependent constants (Defined in Appendix C). Let \(f:^{d}\{ 1\}\) be any classifier such that \(f(x)=1\) if \(x S_{1} S_{3}\) and \(f(x)=-1\) if \(x S_{2}\). Observe that \(\) is a particularly simple data distribution over three spherical manifolds, and \(f\) is a simple classifier that distinguishes its two parts. We illustrate this distribution in panel (a) of Figure 2.

Despite its simplicity, locally explaining \(f\) with _linear_ classifiers faces fundamental challenges. We illustrate this in Figure 2. Choosing a large local neighborhood, as done at point A, leads to issuesposed by the curvature of the data distribution, meaning that it is impossible to create an accurate local linear classifier. On the other hand, choosing a neighborhood small enough for local linearity, as done in point B, leads to _local regions that are exponentially small with respect to the data dimension._

We formalize this in the following theorem.

**Theorem 5.1** (**A high dimensional example**).: _Let \(,f\), be as described above, and let \(E\) be any explainer in \((_{d},_{d})\). Let \(x^{*}\) be any point chosen on the outer sphere, \(S_{3}\). Then \(E\) outputs an explanation at \(x^{*}\) that either has a large local loss, or that has a small local mass. That is, either \(L(E,f,x^{*})\), or \((E,f,x) 3^{1-d}\)._

Theorem 5.1 demonstrates that if a locally linear explanation achieves even a remotely reasonable local loss, then it necessarily must have an extremely small local explanation. This suggests that, gradient based explanations will be exponentially local with respect to the data dimension, \(d\).

We believe that this is also exhibited in practice particularly over _image data_, where explanations are often verified based on perceptual validity, rather than relevance to practical training points beyond the point being explained. For example, the explanations given by SmoothGrad (Smilkov et al., 2017) are visualized as pixel by pixel saliency maps. These maps often directly correspond to the image being explained, and are clearly _highly specific_ to the it (see e.g. Figure 3 of (Smilkov et al., 2017)). As a result, we would hardly expect the implied linear classifier to have much success over almost any other natural image. This in turn suggest that the locality would be extremely small. We also remark that a similar argument can be made for **Lime**, which also tends to validate its explanations over images perceptually (for example, see Figure 4 of Ribeiro et al. (2016)).

Unlike the previous methods, **Anchors**(Ribeiro et al., 2018) explicitly seeks to maximize the local mass of its explanations. However, it abandons this approach for image classifiers, where it instead maximizes a modified form of locality based on super-imposing pixels from the desired image with other images. While this gives perceptually valid anchors, the types of other images that fall within the local region are completely unrealistic (as illustrated in Figure 3 of (Ribeiro et al., 2018)), and the true locality parameter is consequently extremely small. Thus, although Anchors can provide useful and _auditable_ explanations in low-dimensional, tabular data setting, we believe that they too suffer from issues with locality for high-dimensional data. In particular, we note that it is possible to construct similar examples to Theorem 5.1 that are designed to force highly local Anchors-based explanations.

## 6 Conclusion

Our results in Section 4 demonstrate that the locality of a local explainer characterizes how much data is needed to audit it; smaller local regions lead to larger amounts of data. Meanwhile, our

Figure 2: An illustration of Theorem 5.1, with the concentric blue and red circles depicting the data distribution \(\) classified by \(f\), and with local explanations being depicted at points A and B. Explanations are forced to either have large local loss (point A) or a low local mass (point B).

discussion in Section 5 shows that typical local explanations are _extremely local_ in high-dimensional space. It follows that in many cases, auditing solely based on point-wise decisions and explanations is impossible. Thus, any entity without model access, such as a collective of users, are never in a position to _guarantee_ trust for a machine learning model.

We believe that the only way forward is through a more powerful third-party auditor that crucially as _more access to the machine learning model,_ as this could potentially break the fundamental challenges posed by small explainability regions. We believe that investigating the precise types of access this would entail as an important direction for future work that might have broad practical consequences.

Finally, although our definition of local explainers encompasses several widely used explanation methods, we do note that there are notable exceptions such as Shap (Lundberg and Lee, 2017), which does not fit into our paradigm. As a consequence, one important direction for future work is expanding our framework to encompass other local explanation methods and examine to what degree they can be audited.