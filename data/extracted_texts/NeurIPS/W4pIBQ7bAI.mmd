# MediQ: Question-Asking LLMs and a Benchmark

for Reliable Interactive Clinical Reasoning

 Shuyue Stella Li\({}^{1}\) Vidhisha Balachandran\({}^{2}\) Shangbin Feng\({}^{1}\) Jonathan S. Ilgen\({}^{1}\) Emma Pierson\({}^{3}\) Pang Wei Koh\({}^{1,4}\) Yulia Tsvetkov\({}^{1}\)

\({}^{1}\)University of Washington \({}^{2}\)Carnegie Mellon University \({}^{3}\)Cornell Tech

\({}^{4}\)Allen Institute for AI

stelli@cs.washington.edu

https://github.com/stellalsy/mediQ

https://huggingface.co/datasets/stellalisy/mediQ

###### Abstract

Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that _proactively ask questions_ to gather more information and respond reliably, and introduce an benchmark--**MediQ**--to evaluate question-asking ability in LLMs. MediQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions _degrades_ performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MediQ benchmark and a novel question-asking system, and highlight directions to extend LLMs' information-seeking abilities in critical domains.

## 1 Introduction

General-purpose large language models (LLMs) are designed to serve a broad audience by following instructions and providing the most likely and general answers (Brown et al., 2020; Ouyang et al., 2022). However, in high-stakes decision making scenarios such as clinical conversations, LLM assistants can be harmful if they provide general responses instead of gathering missing information to make informed decisions. As shown in Figure 1, standard medical question-answering (QA) tasks are formulated in a single-turn setup where all necessary information is provided upfront, and the model is not expected to interact with users (Jin et al., 2021; Pal et al., 2022; Jin et al., 2019; Hendrycks et al., 2020). This QA paradigm diverges from real-world scenarios, where users may provide **incomplete information**, and effective decision-making often requires an **investigative process** involving follow-up questions to clarify and gather necessary details (Trimble and Hamilton, 2016; Bornstein and Emler, 2001; Masic, 2022).

This gap between existing benchmarks and reality calls for a paradigm shift to designing systems adept at navigating high-stakes interactive scenarios. Focusing on clinical interactions where context is often incomplete, we introduce **MediQ**, an interactive benchmark for **m**edical **e**valuation with **d**ynamic information-seeking **q**uestions, to address limitations of static single-turn QA benchmarks (Figure 2). Unlike conventional systems, which assume that all necessary information is readily available, MediQ acknowledges the inherent uncertainty in medical consultations where a typical patient does not have the expertise to distill all necessary and relevant information they need to provide. To achieve this, MediQ comprises two components: a **Patient system** that simulates a patient and responds to follow-up questions, and an **Expert system** that serves as a doctor's assistant and asks questions to the patient before making a medical decision. In this **interactive clinical reasoning task**, a successful information-seeking Expert should decide, at each turn, whether it has enough information to provide a confident answer; if not, it should ask a follow-up question.

We convert two medical QA datasets, MedQA (Jin et al., 2021) and Craft-MD (Johri et al., 2023, 2024), into an interactive benchmark by parsing the patient records to only provide partial information in the beginning. We first develop and validate a Patient system that accurately answers Expert inquiries by retrieving the correct facts from the patient record. We then benchmark Expert systems based on state-of-the-art (SOTA) LLMs, including Llama-3 (Touvron et al., 2023), GPT-3.5 (Brown et al., 2020) and GPT-4 (OpenAI et al., 2024), to evaluate their proactive information seeking ability. It is striking that prompting these models to ask questions results in an 11.3% accuracy drop compared to starting with the same limited information and asking no questions, showing that adapting LLMs to interactive information-seeking settings is nontrivial. A key challenge is deciding when to ask a follow-up question instead of directly providing an answer. With confidence estimation strategies such as rationale generation and self-consistency, we improve Expert performance by 22.3%, although a 10.3% gap remains compared to an upper bound when full information is presented at once.

Our results show that while SOTA LLMs perform relatively well with complete information, they struggle to proactively seek missing information in a more realistic, interactive settings with incomplete initial information. By providing a modular, interactive benchmark, we hope to facilitate the development of reliable LLM assistants for complex decision-making in healthcare and other high-stakes domains. Our main contributions are:

1. We identify the critical problem of **information-seeking questions** in reliable interactive LLM assistants. We propose a paradigm shift and a practical conversion pipeline from standard single-turn benchmarks into interactive settings with incomplete initial information.
2. We develop the **MediQ Benchmark** to simulate more realistic clinical interactions between a Patient System and an Expert System. We rigorously develop and test the Patient System to benchmark any Expert's information-seeking and clinical decision-making abilities.
3. We show that SOTA LLMs such as Llama-3-Instruct, GPT-3.5 and GPT-4 struggle at proactive information seeking, revealing a significant gap in this area.
4. We propose **MediQ-Expert**, our best Expert system with novel abstaining capabilities to reduce unconfident answers, to partially close the gap between the more realistic incomplete information setup and the existing full information setup.

Figure 1: Information Seeking Task. In standard medical QA tasks (left), all necessary information is given to the assistant model at the same time. When given partial information, current LLMs only provides general responses (middle). In a more realistic scenario (right), the presentation of patient information relies on proactive elicitation from the doctor; our proposed MediQ framework operationalizes this scenario.

## 2 MediQ: Dynamic Medical Consultation Framework Overview

Task DefinitionThe dynamic medical consultation task simulates the iterative nature of real-world clinical interactions. This task starts by providing an initial patient description \(k_{0}\) of their conditions to the Expert system. The initial information typically contains the patient's age, gender, and chief complaint for the visit. The Patient system has access to the entire patient record \(=\{k_{0},k_{1},,k_{n}\}\), and the necessary information to answer the multiple choice question is \(^{*}\). At the start of the \(t\)-th turn, the knowledge available to the Expert system is denoted as \(_{t-1}=\{k_{0},,k_{i}\}\). Given follow-up question \(q_{t}\), the Patient system responds with \(r_{t}=\{k|k\}\). The Expert knowledge is then updated as \(_{t}=_{t-1} r_{t}\). The **main challenge of the task** is for the Expert system to ask information-seeking questions to expand \(_{t}\) until the knowledge gap is filled, i.e. \(_{t}=^{*}\), at which point the Expert system is asked to make a final decision.

### The Patient System

Patient TaskAs part of the MediQ framework, the Patient system simulates a human patient in clinical conversations. The Patient system has access to the full patient record that is sufficient for the diagnosis, including symptoms, onset duration, medical history, family history, and/or relevant lifestyle factors. The Patient system uses the patient record and a single information-seeking question from the Expert system to produce a coherent response consistent with the given patient information as shown in Figure 2. A reliable Patient system is critical to simulate a real and accurate medical consultation process. We propose that any Patient system should be evaluated on (1) _Factuality_ - measuring if a patient's responses are faithful to the patient's record and history and (2) _Relevance_ - measuring if the patient's response answers the expert's question. Given the full patient record and the expert question, we propose and evaluate three Patient system variants: **Direct**, **Instruct**, and **Fact-Select**, to obtain the patient response. Exact prompts and examples are in Appendix A.2.

1. [leftmargin=*]
2. **Direct:** Serving as a baseline, the Patient treats the response-generation as a reading comprehension task with no additional instruction. The prompt includes the patient's record followed by the Expert's question and asks the model to directly respond to the question using the given paragraph.
3. **Instruct:** The Patient is instructed to respond truthfully to the Expert's question using the patient record only. When the context does not contain an answer to the question, the Patient is instructed to refrain from answering.
4. **Fact-Select:** The Patient aims to improve the factuality of the response by decomposing the patient record into atomic facts and responds by selecting facts that are relevant to the Expert's question.

### The Expert System

Expert TaskThe Expert system simulates the medical decision-making process of experienced clinicians, who seek additional patient information and iteratively update their differential diagnosis. The Expert system is first presented with a medical question and limited patient information. As each turn, it assesses whether the provided information is sufficient to answer the question. If the Expert system is unconfident, it can elicit evidence with a follow-up information-seeking question; otherwise, the Expert system deems the acquired information sufficient and provides a final answer. The performance of the Expert system is evaluated on the (1) _efficiency_ of the conversation (number of follow-up questions) and (2) the _accuracy_ of the final diagnosis.

#### 2.2.1 Expert System Breakdown

Medical decision making is a complex process involving clinical reasoning and proactive information-seeking (Bordage, 1999; Norman, 2005; Schmidt et al., 1990; Boshuizen & Schmidt, 1992; Patel

Figure 2: The MediQ Benchmark. MediQ operationalizes a more realistic dynamic clinical interaction between a Patient system and an Expert system to evaluate info-seeking and question-asking.

the next section, (2). We describe our proposed **MediQ-Expert**, which operationalizes the Expert system by breaking down the task into five medically-grounded steps: (1) initial assessment, (2) abstention, (3) question generation, (4) information integration, and (5) decision making (Figure 3). Each step is modular and easily modifiable.

**Step 1. Initial Assessment Module**: Given limited patient intake information and the multiple choice question (MCQ) as the input, the goal of this module is to provide an initial assessment of the patient. The Expert system is asked to produce a paragraph that elaborates on the symptoms and options, and identifying potential knowledge gaps (e.g., additional symptoms, lab tests) missing for answering the question. This step is done only once at the beginning of the interaction, and we keep the output in the conversation thread for future turns to refer back to.

**Step 2. Abstention Module**: When the model is not confident, it should _abstain_ from giving an answer and asks a information-seeking question instead. The goal of the Abstention Module is to evaluate the **confidence level** of the Expert system to make a decision given the available information. The input to this module is the MCQ and the patient information consisting of the initial presentation and a conversation log of follow-up questions and responses. We probe the confidence level of the model to reliably answer the question via prompting (SS 2.2.2). The output of this module is a yes/no answer for whether to proceed to final answer. If the model is confident, it skips to decision making; otherwise, it continues to question generation.

**Step 3. Question Generation Module**: When more information is deemed necessary, the goal of the question generation module is to craft an information-seeking question to elicit additional medical evidence such as lifestyle factors and physical exam results. The input to this module is all previous reasoning steps, and the acquired patient information; the notion of atomic questions is defined with respect to the medical domain in the prompt, and the output is an atomic question to the patient.

**Step 4. Information Integration Module**: When a patient response is returned to the Expert system, the information integration module aggregates all gathered patient information up to this point to update the understanding of the patient condition. This step simply appends a question-answer pair to the end of an existing conversation log, which will then be passed to the Abstention Module.

**Step 5. Decision Making Module**: When enough evidence is gathered, the Expert system leverages integrated patient information and medical knowledge to provide an accurate answer to the question. The input to this module is previous reasoning steps, the MCQ, and the gathered patient information, and the output is the chosen option. Exact prompts for all above sections are in Appendix B.

#### 2.2.2 Expert System Variants with Different Abstention Strategies

One component of active information-seeking is the ability to decide _when_ to ask questions, which we operationalize with the Abstention Module to either _ask_ or _answer_ at each turn. Abstention reduces LLM hallucinations in low-confidence scenarios (Umapathi et al., 2023; Rawte et al., 2023) and mitigates misleading or insufficiently substantiated conclusions (Feng et al., 2024). We develop the following variants of the Abstention Module via different instructions to the LLM to probe its confidence in whether their parametric knowledge is sufficient to reliably answer the MCQ. Exact prompts are in Appendix B.2.

**0. Basic:** As a baseline, the model is asked to implicitly indicate its abstain decision by either generating an atomic question or producing an answer to the MCQ.

**1. Numerical:** To get an explicit understanding of the model's confidence, we first prompt the model to generate a numerical confidence score between 0 and 1 following (Tian et al., 2023). Then, an arbitrary threshold is set to either proceed with a final answer or ask a question.

Figure 3: Expert system information flow breakdown.

2. **Binary:** Previous work has shown that LLMs struggle at producing numerical confidence scores (Srivastava et al., 2022; Geng et al., 2023). To address this, the Binary variant enables a simple classification of whether enough information is present. This setup simplifies the decision process, but may lack the nuanced understanding of confidence levels.
3. **Scale:** Binary classification does not provide granularity where the decision is ambiguous. Scale abstention solves this issue by combining direct quantification with a manageable set of discrete, interpretable options. The model is given definitions of confidence levels on a 5-point Likert scale (e.g., "Very Confident", "Somewhat Confident"), and is asked to select a rating to express its confidence. An arbitrary threshold is set to either proceed with a final answer or ask a question.
4. **Rationale Generation (RG):** Model performance is shown to improve when prompted to generate a reasoning chain about the decision process (Wei et al., 2022; Marasovic et al., 2021). This gives the model a longer context window for reasoning, allowing the final decision to be conditioned on previous generations. We attempt to generalize this finding to the more complex interactive medical information-seeking setup by applying it to Numerical, Binary and Scale abstention prompts.
5. **Self-Consistency (SC)**. To further improve the Expert system's abstaining decision, we apply Self-Consistency to the above variants. Self-consistency repeatedly prompts the LLM \(n\) times and take the average (Numerical and Scale) or the mode (Binary) of the output as the final output, and is shown to improve model performance (Wang et al., 2022).

## 3 Experiments

We conduct experiments to validate each component of MediQ. First, we evaluate the Patient system with _factuality_ and _relevance_ metrics (SS 3.1). Then, we establish the correlation between information availability and accuracy by studying model performance with varying levels of input information (SS 3.2.1). Finally, we improve the information-seeking ability of LLMs under MediQ (SS 3.2.2).

Evaluation DatasetWe convert MedQA (CC-BY 4.0) (Jin et al., 2021) and Craft-MD (CC-BY 4.0) (Johri et al., 2023, 2024) into an interactive setup for our experiments. MedQA is a standard benchmark for medical question answering with 10178/1272/1273 train/dev/test samples. Each sample contains a paragraph of patient record ending with a multiple choice question. Craft-MD contains 140 dermatology patient records in a similar format, among which 100 are collected from an online question bank and 40 are created by expert clinicians. We parse each patient record into age, gender, the chief complaint (primary reason for the clinical visit), and additional evidence. Only the age, gender, and chief complaint are presented to the Expert system, from which it is expected to elicit missing information. The resulting tasks are called iMedQA and iCraft-MD, respectively. See Appendix C for detail.

### Patient System Reliability Evaluation

We automate the evaluation of patient responses with factuality score and relevance score for the ease of scalability, and conduct manual annotations to validate our metrics (Appendix A.4).

_Factuality Score_ measures whether the Patient system's response is consistent with the patient record. Each Patient response is first decomposed into a list of atomic statements, then we compute the percentage of atomic statements that are supported by the information in the patient record. The factuality score is the percent of supported statements averaged over all patients.

_Relevance Score_ measures whether the Patient system's response answers the Expert's question. Since there is no oracle data on the correct answer for Expert follow-up questions, we construct a synthetic parallel evaluation dataset of questions and responses to evaluate the relevance of Patient responses: given a patient record decomposed into atomic statements, we rephrase each statement into an atomic question, for which the statement is the ground truth answer. Then, the Patient system produces a response using the patient record and the generated atomic question. The average embedding semantic similarity between the generated response and the ground truth statement over the evaluation dataset is the resulting relevance score. See Appendix A.1 for more detail.

SetupWe use GPT-3.5 as the base LLM for all three variants (Direct, Instruct, and Fact-Select) and compare the factuality and relevance scores. For factuality, we sample 1272 patient cases from MediQ interactions with follow-up questions generated by different Expert systems so the Patientsystem sees diverse Expert questions and compute the average across all generated questions. For relevance, we use all 1272 patient records from the development set of MedQA.

### Expert System Experiments

#### 3.2.1 Benchmarking Existing LLMs in Incomplete Information Scenarios

We evaluate the performance of non-interactive Expert systems with varying information availability levels to observe the relationship between information availability and accuracy and to establish baselines. The baselines are evaluated at three initial information availability levels (Figure 4): **Full**, **Initial**, and **None**. The **Full** setup is equivalent to the standard QA task, wherein all patient information is provided to the Expert system in the beginning; **Initial** only discloses the gender, age, and the chief complaint that leads to the clinical visit (e.g. fever, headache, etc.); **None** provides no patient information but only the MCQ to the Expert system.

#### 3.2.2 Interactive Expert Systems

**Expert Variants**  Without explicitly providing the option to ask follow-up questions, vanilla LLMs always answer with incomplete information and _never_ ask for additional evidence. Therefore, we establish a question-asking Expert system baseline--**Basic**--by prompting the LLM to either ask a question or make a decision at each turn. To study abstention, we combine Numerical, Binary, and Scale abstention with rationale generation and self-consistency techniques described in SS 2.2.2.

**Expert System Setup**  We evaluate Llama-3-Instruct (8B, 70B), GPT-3.5, and GPT-4 on iMedQA and iCraft-MD for both the non-interactive and interactive settings. Analysis and ablations use GPT-3.5 results on iMedQA only. Details on model version and compute are in Appendix C.

**Expert System Evaluation Metric**  An ideal Expert system should be able to ask informative questions that allow it to arrive at accurate medical decisions efficiently. Since it is not trivial to measure the quality of medical information-seeking questions, we use the efficiency of the interaction (number of questions) and accuracy of the solution as proxies to evaluate the clinical reasoning capabilities. Accuracy is strongly dependent on the amount of information available to the model (SS 3.2.1), so higher accuracy is correlated with stronger information-seeking ability of the LLM.

## 4 Results

### How reliable is the MediQ Patient system?

Our results in Table 1 show that both the **Direct** and **Instruct** settings struggle with factuality. Qualitative analysis revealed that since the Direct setting did not receive any instructions on _how_ to respond to the follow-up question, it sometimes responds with "Yes" or "No" instead of the atomic statements that contain the requested information. In the Instruct setting, the Patient system sometimes provide inferences instead of reciting the facts from the failure cases are shown in Appendix A.3. On the other hand, the **Fact-Select** setting which generates the responses in a more controlled environment increases factuality by 0.33 points and relevance by 0.04 points. Overall, these results suggest that _using atomic facts as units of information significantly reduces hallucination_, improving the reliability of the Patient system in providing accurate and relevant responses to expert questions. We use the Fact-Select setting for the Patient system in all subsequent experiments and shift our focus to evaluate the Expert variants introduced in SS 3.2.

  
**Model** & **Factuality** & **Relevance** \\ 
**Direct** & 55.9 & 75.5 \\
**Instruct** & 62.8 & 78.6 \\
**Fact-Select** & **89.1** & **79.9** \\   

Table 1: Patient system reliability.

Figure 4: Non-interactive Expert system evaluation at various information availability levels. The question and options are provided to the Expert model in all three settings.

### How do existing Non-Interactive LLMs perform with Limited Information?

As shown in Table 2, with decreasing amounts of patient information provided to the model, there is a pronounced drop in performance from the **Full** to **Initial** to **None** information availability levels. Shifting our attention to the **Basic** interactive setup, the final accuracy is even lower than its non-interactive counterpart (Initial) with the same initial information (a average of 11.310.3%relative drop). We analyze performance sensitivity to prompt variations to ensure a fair comparison and report results from additional LLMs in Appendix E.

Figure 5 shows the number of follow-up questions asked by the LLMs in the Basic interactive setup. For majority of the samples, _no_ model chooses to ask any questions, showing the lack of ability in LLMs to proactively identify and elicit missing information. Within each LLM family (Llama/GPT), there is a correlation between model size, number of questions asked and accuracy. Overall, these results show a significant gap between model performance in idealized settings and realistic, information-limited scenarios. None of the examined models excel at proactive information seeking in an interactive environment, suggesting that it is nontrivial to integrate information gathered from continuous interactions. Despite having some medical knowledge encoded during pretraining, LLMs struggle to compensate for the absence of detailed patient information, highlighting the need for advanced proactive information-seeking abilities in medical LLM applications.

### How much of the performance gap can be closed by asking questions?

In Figure 6, We present a summary of the information-seeking ability of MediQ Expert models with different abstain strategies by reporting the _accuracy_ and _number of questions_ (conversation efficiency). Recall that both the Numerical and Scale abstention methods require setting a confidence threshold, above which the Expert system will proceed to the final answer. We do a grid search for the threshold hyperparameter in Appendix D and report the best performance for each setting. Integrating a dedicated Abstention Module significantly enhances performance over the Basic setup which directly prompts for follow-up questions or diagnoses. As the abstain strategies improve - by expressing confidence on a scale, verbal reasoning, and adding self-consistency - the expert model is able to better gauge the (lack of) patient information and continue the conversation by asking more questions and thereby improving the final accuracy.

Base abstention methods (Numerical, Binary, Scale) show little variance in effectiveness until combined with rationale generation, which consistently boosts performance across strategies, as supported by previous studies (Marasovic et al., 2021; Wei et al., 2022; Feng et al., 2024). Notably, self-consistency alone _degrades_ performance unless paired with rationale generation. Overall, the Scale Abstention (1-5 confidence rating) with Rationale Generation and a Self-Consistency factor of 3 achieves the best performance. Overall, Scale Abstention (1-5 confidence rating) with rationale generation and a self-consistency factor of 3 achieves the best performance, outperforming the Basic interactive setup by 22.3% and the non-interactive Initial setup by 12.1%. In information-scarce scenarios, models tend to resort to the most common option instead of specializing to the patient, and interaction enhances specialization (Appendix F).

This pattern is generalizable across different LLMs as shown in the Best column of Table 2. Note that model size plays a big factor in the performance of the interactive setting--models larger than 70B surpass the non-interactive Initial setup with the best abstention, but smaller models still struggle.

    &  &  & **Interactive** \\  & & **Full** & **Initial** & **None** & **Basic** & **Best** \\   & **Llama-3-8b** & 68.14\(\)1.5 & 52.0\(\)1.4 & 40.4\(\)1.4 & 53.0\(\)1.3 & 45.8\(\)1.4 \\  & **Llama-3-70b** & 84.7\(\)1.5 & 58.5\(\)1.4 & 46.3\(\)1.4 & 55.1\(\)1.3 & **69.9\(\)1.4** \\  & **GPT-3.5** & 55.8\(\)1.4 & 45.6\(\)1.4 & 36.7\(\)1.4 & 42.2\(\)1.3 & **50.2\(\)1.4** \\  & **GPT-4** & 79.7\(\)1.1 & 54.5\(\)1.4 & 42.2\(\)1.4 & **55.4\(\)1.3** & **66.1\(\)1.3** \\   & **Llama-3-8b** & 76.4\(\)3.6 & 51.4\(\)4.2 & 29.3\(\)3.8 & 42.9\(\)4.2 & 50.0\(\)4.2 \\  & **Llama-3-70b** & 82.1\(\)3.2 & 60.7\(\)1.2 & 52.9\(\)4.2 & 62.4\(\)1.7 & **72.1\(\)3.8** \\   & **GPT-3.5** & 82.1\(\)2.5 & 53.6\(\)4.2 & 29.3\(\)3.8 & 45.0\(\)4.2 & **59.3\(\)4.2** \\   & **GPT-4** & 91.4\(\)2.4 & 67.9\(\)3.9 & 43.6\(\)3.7 & **73.6\(\)3.7** & **84.3\(\)3.1** \\   

Table 2: Accuracy at varying information availabilities. Basic gives LLM the option to ask questions: with the same starting information, Basic performance degrades from non-interactive Initial. Bold entries surpass non-interactive Initial, but there is still a gap between Full (complete information upper bound) and interactive Best.

Figure 5: Frequency of conversation lengths in the Basic setting. Most models don’t tend to ask follow-up questions.

While informing LLMs _when_ to ask questions through abstention helps improve their decision-making with limited information, our best MediQ Expert system (Scale+RG+SC) still only closes 51.2% of the gap between the _Non-Interactive Initial_ and the _Full_ information scenarios. This indicates plenty of room for improvement to further enhance the information-seeking ability of LLMs.

## 5 Analysis

In this section, we further analyze the factors that impact the performance of the interactive Expert system. Since we observe similar trends across models and datasets, all analysis will be performed on the iMedQA dataset with GPT-3.5 due to cost and computation constraints.

### Why does the Basic interactive setup fail to perform clinical reasoning?

Recall from SS 4.2 that there is a striking 11.3% relative drop in accuracy from Basic to the _non-interactive_ Initial information setting (NI-Initial) across all benchmarked LLMs (7.43% for GPT-3.5 on iMedQA). In this section, we analyze failure modes of the Basic system, where the Expert is simply given the option to ask follow-up questions, to understand the performance drop. We show that the ability to ignore irrelevant context and extract useful information from conversation format affects model performance.

Irrelevant ContextThere are two types of irrelevant context on model performance: _unanswerable_ and _repeated_ questions. As MediQ allows the Expert to ask any open-ended questions to the Patient to elicit information, some questions cannot be answered using the patient record. We filter out these unanswerable question-response pairs, keeping only record-based questions and responses to assess the effect of ignoring irrelevant questions (**Relevant**). Secondly, although the Expert is instructed to not repeat any questions, upon inspection of the interaction history, many questions are repetitive, especially when the answer is not in the patient record. We hypothesize that the repetition shifts the model's attention to certain questions and thus hinders the performance. We remove repetitive questions and only keep the unique questions (using fuzzy lexical matching) to verify this hypothesis (**Unique**). Finally, we remove both unanswerable and repeated questions (**Both**).

Conversation FormatWe further hypothesize that the dialogue format, different from the typical document format seen during LLM pre-training, also affects performance. To control for this, we convert the conversation format into paragraph format by discarding the Expert questions and only keeping the patient response for answerable questions, and rewriting the unanswerable questions into statements (e.g., The patient's vaccine record is unavailable.) for each setting above.

As shown in Figure 7, Relevant and Unique both improve performance by 2 percentage points (pp), but the combined effect is indistinguishable from using either filter, which might be due to the fact that unanswerable questions tend to be repeated. Converting the conversations into paragraph format further improves the performance (Para). Removing repetitive questions and converting to paragraph format (Unique-Para) surpasses Basic by 5.7pp and NI-Initial by 2.3pp. This shows that, when given the option to ask follow-up questions, the information-seeking ability of the Expert system does help make more informed and accurate conclusions, but the model suffers from not being able to learn from realistic clinical dialogues.

[MISSING_PAGE_EMPTY:9]

## 6 Related Work

Medical Question Answering SystemsAdvancements in medical question answering (QA) systems have progressed from rule-based systems to LLM-powered agents. Notable medical QA benchmarks include MultiMedQA (Singhal et al., 2023a), which contains both multiple-choice and open-ended questions collected from various sources. To customize or adapt a general-purpose LLM to the medical domain, prior work often finetune the model on medical knowledge data such as PubMed (Bolton et al., 2022; Yasunaga et al., 2022; Wu et al., 2023a; Singhal et al., 2023a,b), or more recently on conversational medical datasets (Yunxiang et al., 2023; Han et al., 2023). Kim et al. (2024) further improves model performance on complex medical questions by dynamically forming multi-agent collaboration structures. Despite their proficiency in direct answer retrieval, the proactive information-seeking capability is not something these models are inherently designed to do. Our proposed methodological framework, MediQ, is designed to work as an overlay to these domain-specific models, enhancing them with the capability to actively seek additional information in a structured and clinically relevant manner.

Interactive Models and AgentsInteractive conversational models extend beyond the standard QA framework by engaging in a dialogue such as customer support and negotiation (Singh and Beniwal, 2022; Chakrabarti and Luger, 2015; He et al., 2018; Abdelnabi et al., 2023; JU et al., 2024), where iterative information gathering is crucial. Li et al. (2023) and Andukuri et al. (2024) attempt to use LLMs to elicit more information-rich human preference examples in everyday tasks. However, the application of these models in the medical domain remains limited (Li et al., 2021). Wu et al. (2023b) attempts to evaluate general-purpose LLMs and chain-of-thought reasoning on DDXPlus (Fansi et al., 2022), a rule-based synthetic patient dataset. Hu et al. (2024) navigates the information-seeking scenario as a search problem by developing a reward model guided by uncertainty and includes medical diagnosis as one of the tasks, but if limited to binary questions. Johri et al. (2023) observed a similar phenomenon where LLM-doctors cannot elicit complete patient information, but do not focus on improving the information-seeking ability. The system proposed by (Tu et al., 2024) performs a multitude of medical tasks but does not explore the crucial problem of abstention. Lastly, multi-agent and human-AI collaboration frameworks have shown impressive interactive performance (Zhou et al., 2024; Wu et al., 2024, 2023c; Deng et al., 2024; Lin et al., 2024), and can greatly benefit from our novel interactive abstention methods to seek additional information. Our work fills this gap via providing a benchmark, a dataset, and a framework to comprehensively studying information-seeking abilities in clinical decision-making, and most importantly, opens the door for future endeavors in this direction.

## 7 Conclusion

In this paper, we identify a significant gap in current LLMs' capability to ask questions and proactively seek information in settings where personalization, precision, and reliability are critical. We propose a paradigm shift to interactive benchmarks by simulating more realistic clinical interactions where only partial information is provided initially by introducing MediQ. MediQ provides a benchmark to the community to evaluate the question-asking ability of LLMs, contributing towards developing reliable models. We showed that SOTA LLMs like Llama-3 and GPT-4 struggle to gather necessary information for accurate medical decisions. To address this problem, we presented MediQ-Expert--a novel Expert system with improved confidence judgment and medical expertise, substantially improving clinical reasoning performance. MediQ operationalizes interactive and explicit clinical reasoning processes, with added interpretability in the reasoning flow of language models and decision making. We encourage future research to extend MediQ to more diverse Patient systems, expand medical knowledge integration, and customize the interactions to better serve the healthcare community.

Figure 9: Example of rationale generation helping identify knowledge gaps to ask better follow-up questions.

## Limitations

One limitation is the scarcity of datasets that contain detailed patient information sufficient for a medical diagnosis which, to the best of our knowledge, was only met by MedQA and Craft-MD. The majority of available medical datasets are designed to test models' medical domain knowledge. Second, the Patient system in our benchmark relies on a paid API; future work should establish an open-source Patient. Lastly, our evaluation framework, while designed to be more realistic, is still limited to the multiple-choice format. However, the flexibility of MediQ allows easy extensions into open-ended settings with appropriate datasets and well-defined conversation-level metric. Future work can focus on collecting a rich dataset in open-ended medical consultations and expanding the MediQ framework.

## Ethics Statement

Along with many potential benefits of an ideal future variant of the MediQ framework (e.g., providing reliable and personalized medical consultation when access to medical experts is unavailable, or assisting medical experts in initial collection of information), it is important to emphasize multiple risks associated with mis-use of this framework.

First, MediQ is a carefully designed initial prototype, it is not meant to be deployed to interact with users; its intended use is to provide an experimental framework to test clinical reasoning abilities of LLMs which are currently extremely limited.

MediQ built on top of closed-source LLMs runs the risk of leaking confidential medical information, violating patient privacy. Future research expanding MediQ to new medical datasets should be aware of these risks, resorting to local securely stored LLMs or to reliable data anonymization methods.

There are many sources of potential biases in the framework, including social and cultural biases in LLMs, in the datasets, and possibly in prompts for LLM interactions and abstention that we designed. While outside the scope of this paper, in addition to utility metrics we proposed, future research could incorporate fairness-oriented evaluations, e.g., breaking down the evaluation by user demographics.

If a similar framework is used in real-world applications, users and clinicians should be trained to prevent the over-reliance on technology that is liable to make mistakes, and to understand its privacy and fairness risks.