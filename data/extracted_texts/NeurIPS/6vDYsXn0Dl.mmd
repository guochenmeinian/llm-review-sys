# Linear Time Approximation Algorithm for Column Subset Selection with Local Search

Yuanbin Zou\({}^{1,2}\), Ziyun Huang\({}^{3}\), Jinhui Xu\({}^{4}\), Jianxin Wang\({}^{1,2,5}\), Qilong Feng\({}^{1,2,}\)

\({}^{1}\)School of Computer Science and Engineering, Central South University,

Changsha 410083, China

\({}^{2}\)Xiangjiang Laboratory, Changsha 410205, China

\({}^{3}\)Department of Computer Science and Software Engineering, Penn State Erie,

The Behrend College

\({}^{4}\)Department of Computer Science and Engineering, State University of New York at Buffalo,

NY, USA

\({}^{5}\)The Hunan Provincial Key Lab of Bioinformatics, Central South University,

Changsha 410083, China

yuanbinzou@csu.edu.cn, csufeng@mail.csu.edu.cn,

zxh201@psu.edu, jinhui@buffalo.edu, jxwang@mail.csu.edu.cn

Corresponding Author

###### Abstract

The Column Subset Selection (CSS) problem has been widely studied in dimensionality reduction and feature selection. The goal of the CSS problem is to output a submatrix \(S\), consisting of \(k\) columns from an \(n d\) input matrix \(A\) that minimizes the residual error \(\|A-SS^{}A\|_{F}^{2}\), where \(S^{}\) is the Moore-Penrose inverse matrix of \(S\). Many previous approximation algorithms have non-linear running times in both \(n\) and \(d\), while the existing linear-time algorithms have a relatively larger approximation ratios. Additionally, the local search algorithms in existing results for solving the CSS problem are heuristic. To achieve linear running time while maintaining better approximation using a local search strategy, we propose a local search-based approximation algorithm for the CSS problem with exactly \(k\) columns selected. A key challenge in achieving linear running time with the local search strategy is how to avoid exhaustive enumerations of candidate columns for constructing swap pairs in each local search step. To address this issue, we propose a two-step mixed sampling method that reduces the number of enumerations for swap pair construction from \(O(dk)\) to \(k\) in linear time. Although the two-step mixed sampling method reduces the search space of local search strategy, bounding the residual error after swaps is a non-trivial task. To estimate the changes in residual error after swaps, we propose a matched swap pair construction method to bound the approximation loss, ensuring a constant probability of loss reduction in each local search step. In expectation, these techniques enable us to obtain the local search algorithm for the CSS problem with theoretical guarantees, where a \(53(k+1)\)-approximate solution can be obtained in linear running time \(O(ndk^{4} k)\). Empirical experiments show that our proposed algorithm achieves better quality and time compared to previous algorithms on both small and large datasets. Moreover, it is at least 10 times faster than state-of-the-art algorithms across all large-scale datasets.

Introduction

In machine learning, handling high-dimensional datasets often requires the use of dimensionality reduction techniques, among which Singular Value Decomposition (SVD) is one of the most commonly utilized techniques in practice. The goal of SVD is to minimize the Frobenius norm of the error, aiming to achieve a low-rank approximation of a matrix with theoretical guarantees. An alternative way is to select a small subset of columns from the matrix as representations to well approximate the given matrix, which is known as the Column Subset Selection (CSS) problem. The CSS problem has been widely studied in machine learning for dimensionality reduction with improved interpretability. As pointed out in [4; 27; 17; 13; 20], the number of selected columns \(k\) is much smaller than both \(n\) and \(d\), i.e., \(k\{n,d\}\). We consider the following CSS problem.

**Definition 1.1**.: _Given a matrix \(A^{n d}\) and a positive integer \(k\), the goal of CSS problem is to select \(k\) columns of \(A\) forming a matrix \(S^{n k}\) that minimizes the residual error_

\[\|A-SS^{}A\|_{F}^{2},\]

_where \(S^{}\) represents the Moore-Penrose inverse matrix of \(S\), and \(\|A\|_{F}^{2}=_{i=1}^{n}_{j=1}^{d}A_{ij}^{2}\) denotes the square of Frobenius norm._

The CSS problem is known to be UG-hard . For the CSS problem, several heuristic algorithms [2; 22; 23] were proposed. However, a main concern for these heuristics is the lack of theoretical guarantees on both the running time and approximation error. Several \((1+)\)-approximation bi-criteria algorithms have been proposed for the CSS problem [19; 4; 13; 15]. Many of these algorithms achieve running time of \(O(nd(k))\), which is referred to as linear in both \(n\) and \(d\)[26; 11]. Although these algorithms achieve \((1+)\)-approximation, they require selecting more than \(k\) columns. For the CSS problem with exactly \(k\) columns selected, Boutsidis et al.  proposed an \(O(k^{2} k)\)-approximation algorithm with \(O(\{n^{2}d,nd^{2}\})\) running time, using leverage score sampling and QR decomposition methods. Deshpande and Rademacher  proposed a volume sampling algorithm, which yields a \((k+1)\)-approximation in time \(O(kdn^{3} n)\). Guruswami and Sinop  gave an improved approximation algorithm with \(O(n^{2}dk)\) running time using fast volume sampling method, achieving the same \((k+1)\)-approximation. The running time of these algorithms has at least a quadratic dependence on \(n\) or \(d\). Deshpande and Vempala  presented a linear-time algorithm with \((k+1)!\)-approximation using adaptive sampling method.

Although the algorithm using the adaptive sampling method  achieves the linear running time, its approximation ratio is considerably larger than other algorithms. Moreover, as shown in , local search can improve the quality of the solution for the CSS problem. However, a potential limitation of the local search algorithm proposed in  is the lack of theoretical guarantees on the number of local search steps required for reaching a convergence.

To apply the local search strategy to handle the CSS problem, the running time of each iteration is quadratically dependent on \(d\), making it impractical for large-scale datasets. More specifically, the single-swap strategy in local search enumerates \(O(kd)\) swaps to improve the current solution, where the swap pair is constructed between the given matrix and the set of selected columns during the local search step. This method results in an \(O(nd^{2}k^{2})\) running time in each iteration, making it difficult to maintain linear running time in both \(n\) and \(d\). Secondly, to the best of our knowledge, there is no available result that provides an approximation guarantee for solving the CSS problem using the local search strategy.

### Our Contribution

In this paper, we propose a local search algorithm for solving the CSS problem with running time linear in both \(n\) and \(d\). The key challenge for the local search algorithm is to avoid the \(O(nd^{2}k^{2})\) running time caused by enumerating all possible swap pairs. To overcome this challenge, we propose a two-step mixed sampling method that selects a candidate column with a specific probability for swapping, reducing the enumeration of swap pairs from \(O(dk)\) to \(k\). By applying the two-step mixed sampling method, the running time of each local search step is reduced from \(O(nd^{2}k^{2})\) to \(O(ndk^{2})\).

Although the two-step mixed sampling method accelerates the local search process, bounding the residual error of the solution after swaps is a non-trivial task. Specifically, it is challenging to theoretically estimate the improvement in the approximation loss of the current solution after swaps.

To address this issue, we propose a matched swap-pair construction method. This method identifies matched column pairs between the current solution and the optimal solution. Based on these matched pairs, we guarantee that the two-step mixed sampling method can find a column. With constant probability, this column reduces the residual error of the updated solution by a multiplicative factor of \(1-(1/k)\) in each local search step. With this approach, the expected number of iterations can be bounded by \(O(k^{2} k)\). Therefore, we obtain a local search-based approximation algorithm with \(O(ndk^{4} k)\) running time. The main contributions of this paper are summarized as follows.

* For the CSS problem, we propose a new algorithm that uses local search with a two-step mixed sampling method. This method avoids the quadratic dependence of \(d\) on running time by reducing the number of swap pair enumerations during the local search process. Additionally, we propose a matched swap-pair construction method to bound the improvement of residual error during swaps. With these techniques, we achieve a \(53(k+1)\)-approximation with exactly \(k\) columns selected, where the running time of our proposed algorithm is \(O(ndk^{4} k)\).
* Numerical experiments show that our algorithm performs better in terms of quality on both small and large datasets compared to previous algorithms that selects exactly \(k\) columns, and is at least 10 times faster than the state-of-the-art algorithms on all large datasets.

### Related Work

Within the framework of rank-revealing QR factorization (RRQR) , several poly(\(k,d\))-approximation results [3; 7; 18] have been proposed. These results achieve running time of \(O(nd^{2})\) while selecting exactly \(k\) columns to solve the CSS problem with Frobenius norm error. Deshpande et al.  gave the lower bound of \((k+1)\)-approximation for the problem. Furthermore, Boutsidis et al.  proposed a randomized algorithm with \(O(\{n^{2}d,nd^{2}\})\) running time and \(O(k^{2} k)\)-approximation. More precisely, Deshpande and Rademacher  provided a deterministic \((k+1)\)-approximation with \(O(dn^{3} nk)\) running time. To improve the running time, Guruswami and Sinop  proposed an \(O(n^{2}dk)\) time randomized algorithm with \((k+1)\)-approximation. Additionally, there are many bi-criteria algorithms that relax the number of selected columns. Volume sampling methods have been widely applied to the CSS problem. Deshpande and Vempala  utilized these methods to achieve a PTAS, selecting \(O(k/^{2}+k^{2} k)\) columns. Boutsidis et al.  proposed a linear-time algorithm with a \((1+)\)-approximation, requiring \(O(k/)\) columns selected. Guruswami and Sinop  developed a deterministic algorithm that also achieves a \((1+)\)-approximation with \(O(k/)\) columns selected. Civril and Magdon-Ismail  gave improved bounds for obtaining a PTAS using \(k\)-leverage score sampling and SVD, by selecting \((k k/^{2})\)2 columns. Altschuler et al.  developed a distributed greedy algorithm for the objective \(||SS^{t}A||_{F}^{2}\). Wang and Singh  studied the CSS problem in the missing-data case. Several bi-criteria algorithms have been proposed for the CSS problems with \(_{1}\) norm [12; 21; 25; 8]. In both offline and online settings, Woodruff and Yasuda  provided several bi-criteria algorithms for the CSS problem with the \(_{p}\) norm.

## 2 Preliminaries

For any positive integer \(n\), let \([n]\) denote the set \(\{1,2,,n\}\). Given a matrix \(A^{n d}\), let \(A_{ij}\) be the element in the \(i\)-th row and the \(j\)-th column of \(A\), and define the Frobenius norm of \(A\) as

  References & Approximation Ratio & Method & Running Time \\ 
 & \((k+1)!\) & adaptive sampling & \(O(ndk^{2})\) \\
 & \(O(k^{2} k)\) & leverage score sampling + QR decomposition & \(O(\{n^{2}d,nd^{2}\})\) \\
 & \(k+1\) & volume sampling & \(O(kn^{3}d n)\) \\
 & \(k+1\) & fast volume sampling & \(O(n^{2}dk)\) \\  This paper & \(53(k+1)\) & local search + two-step mixed sampling & \(O(ndk^{4} k)\) \\  

Table 1: Comparison of existing results for the CSS Problem with exactly \(k\) columns selected, where \(n\) is the number of rows in the given matrix, \(d\) is the number of columns, and \(k\) is the number of selected columns.

\(\|A\|_{F}^{2}=_{i=1}^{n}_{j=1}^{d}A_{ij}^{2}\). Denote \(A_{:j}\) as the \(j\)-th column of \(A\), and \(A_{i:}\) as the \(i\)-th row of \(A\). Let \(A^{}\) be the transpose of \(A\) and \(A^{}\) be the Moore-Penrose inverse of \(A\). Given an \(n d\) matrix \(A\), let \(\) be the set of column indices from \(A\), and let \(A_{}\) denote the \(n||\) submatrix of \(A\) consisting of the columns corresponding to the indices in \(\). For a matrix \(A\), the linear span of its column vectors is denoted as span \((A)\). For any two \(n d\) matrices \(A\) and \(B\), \(\|AB\|_{F}\|A\|_{F}\|B\|_{2}\) and \(\|AB\|_{F}\|A\|_{2}\|B\|_{F}\). Given any matrix \(A^{n d}\), the singular value decomposition (SVD) of \(A\) can be written as \(A=_{i=1}^{n}_{i}u_{i}v_{i}^{}\), where \(_{1}_{n} 0\) are the singular values, \(\{u_{1},,u_{n}\}^{n}\) are the left singular vectors, and \(\{v_{1},,v_{d}\}^{d}\) are the right singular vectors. Denote \(rank(A)\) be the rank of a matrix \(A\), which is the number of non-zero singular values of \(A\). Moreover, we denote \(A_{k}=_{i=1}^{k}_{i}u_{i}v_{i}^{}\) as the best rank-\(k\) approximation to \(A\) under the Frobenius norm. The spectral norm of \(A\), denoted by \(\|A\|_{2}\), is defined as the largest singular value of \(A\), i.e., \(\|A\|_{2}=_{}(A)\). Given a solution \(S\) to the CSS problem on matrix \(A\), we define the residual error of \(S\) as \(f(A,S)=\|A-SS^{}A\|_{F}^{2}\).

## 3 Linear Time Local Search Algorithm for CSS Problem

In this section, we propose a local search approximation algorithm for solving the CSS problem, called LSCSS, which maintains a running time linear in both \(n\) and \(d\). Directly applying single-swaplocal search to solve the CSS problem results in an \(O(nd^{2}k^{2})\) running time by enumerating all possible swap pairs. Thus, it is challenging to apply the local search method to solve the CSS problem while maintaining a linear dependence on both \(n\) and \(d\) in the running time. To avoid \(O(nd^{2}k^{2})\) running time in each local search step, we propose a two-step mixed sampling method to reduce the running time from \(O(nd^{2}k^{2})\) to \(O(ndk^{2})\). Although the sampling method reduces the running time by directly using the single-swap local search, analyzing the bound of improvement on the residual error after swaps is a difficult task. To provide a theoretical analysis for the local search step, we propose a matched swap pair construction method to bound the improvement on the residual error during swaps. By carefully analyzing the improvement, we show that our proposed algorithm achieves \(53(k+1)\)-approximation with \(O(ndk^{4} k)\) running time. The detailed algorithm for the CSS problem is given in Algorithm 1.

The LSCSS algorithm mainly comprises local search and two-stage mixed sampling components. The high-level idea behind our proposed local search is to identify a swap pair that minimizes the residual error in each iteration. The swap pair consists of a column from the input matrix to swap in and a column from the current solution to swap out. By repeating this process, the algorithm produces an updated solution with better quality. Moreover, the two-stage mixed sampling method involves two steps for obtaining a candidate column from the input matrix. Firstly, a set of column indices is constructed by sampling each column with probability proportional to its residual error for the current solution. Then, a column is uniformly selected from the set of candidate indices as the final column to swap in. To ensure that the input matrix for the local search process is full rank, we construct a new matrix \(A^{}\) by adding a small perturbation matrix \(D\) to the original matrix \(A\) during the initialization, where \(D\) is full-rank and has non-zero values only on its diagonal. The full-rank property of \(A^{}\) is used in subsequent analysis.

The LSCSS algorithm begins by obtaining an initial solution \(S\) with exactly \(k\) columns and constructing a full-rank matrix \(A^{}\) during the initialization (steps 1-12 of Algorithm 1), which achieves a \((k+1)!\)-approximate solution on \(A^{}\). We start by initializing an index set \(\) and setting the matrix \(E=A\). A new column index is added to \(\) by sampling each column index \(i\) from \([d]\) with probability proportional to \(p_{i}=\|E_{:i}\|_{2}^{2}/\|E\|_{F}^{2}\). Then, \(E\) is updated as \(E=A-A_{}A_{}^{}A\). Repeating this process \(k\) times, we obtain an initial solution \(S=A_{}\). To construct a full-rank matrix, we construct an \(n d\) zero matrix \(D\) and compute the parameter \(=\|A-SS^{}A\|_{F}/(52\{n,d\}(k+1)!)^{-1/2}\) using the initial solution \(S\) and \(A\). Each diagonal entry \(D_{ii}\) is set to \(\). Since \((A+D)=(D)\), we construct the full-rank matrix \(A^{}\) by adding the full-rank matrix \(D\) to the input matrix \(A\). To solve the CSS problem on \(A^{}\), we execute steps 3-6 of Algorithm 1 to obtain the solution \(S=A_{}^{}\). The detailed process described in steps 1-12 of Algorithm 1 requires \(O(ndk^{2})\) time.

The local search performed in steps 13-15 of Algorithm 1 plays a crucial role in LSCSS, involving two main steps. Firstly, we compute the matrix \(E=A^{}-SS^{}A^{}\) for the current solution \(S\). Then, a set \(C\) of \(10k\) column indices is constructed by sampling each column index \(i\) from \([d]\) with probability \(p_{i}=\|E_{:i}\|_{2}^{2}/\|E\|_{F}^{2}\). Next, a column index \(p\) is uniformly selected from \(C\), referred to as the "swap-in" column index. Let \(\) denote the set of column indices of \(S\) in \(A^{}\). Subsequently, if there exists an index \(q I\) such that \(f(A^{},A^{}_{\{q\}\{p\}})<f(A^{},S)\), we choose \(q\) as the "swap-out" column index and update the set of indices to \(=\{q\}\{p\}\). Finally, Algorithm 2 returns the solution \(S=A^{}_{}\). After repeating this process \(T=O(k^{2} k)\) times, Algorithm 1 returns the final solution \(S\) for the input matrix \(A\).

In the following, we explain in more detail how our proposed local search algorithm achieves a \(53(k+1)\)-approximation for the original matrix \(A\). Given an initial solution, the main idea for analyzing the approximation ratio of our algorithm is to bound the improvement on residual error during the swaps in the local search step. To achieve this bound, we propose a matched swap pair construction method that guarantees an improvement in the current solution by swapping one column (Lemma 3.6 and Lemma 3.7). By carefully analyzing the improvement, we show that with constant probability the approximation loss of the current solution can be reduced by a multiplicative factor of \(1-(1/k)\) in each iteration of the local search algorithm (Lemma 3.8). This implies that after \(O(k^{2} k)\) iterations, we have \(\|A^{}-SS^{}A^{}\|_{F}^{2} 26(k+1)\|A^{}-A^{ }_{k}\|_{F}^{2}\) (Theorem 3.9). Finally, by analyzing the change in the residual error caused by removing matrix \(D\) from \(A^{}\), we obtain \(\|A^{}-SS^{}A^{}\|_{F}^{2} 53(k+1)\|A-A_{k}\|_{F}^{2}\) in expectation (Lemma 3.10).

We assume that the matrix \(A\) has been normalized such that \(\|A\|_{F}^{2}=1/4\). Otherwise, we can normalize each element \(A_{ij}\) in \(A\) as \(A_{ij}=}{2\|A\|_{F}}\). Next, we consider a single iteration of Algorithm2. We assume that the current solution has a high residual error (larger than \(25(k+1)\|A^{}-A^{}_{k}\|_{F}^{2}\) before executing Algorithm 2 on \(A^{}\). Otherwise, the initial solution \(S\) is a \(25(k+1)\)-approximation for the input matrix \(A^{}\).

Let \(S^{*}=\{s^{*}_{1},,s^{*}_{k}\}\) be the optimal solution with exactly \(k\) selected for \(A^{}\), and let \(S=\{s_{1},,s_{k}\}\) be the current solution. We define \((A^{},S^{*},S,s^{*})=_{s S}f(A^{},S^{*} \{s^{*}\}\{s\})\) as a mapping function that finds \(s\) from \(S\) such that the residual error \(f(A^{},S^{*}\{s^{*}\}\{s\})\) is minimized. Thus, we say that \(s^{*}\) is captured by \((A^{},S^{*},S,s^{*})\). Each column \(s^{*} S^{*}\) is captured by exactly one column from \(S\). Let \(\) denote the set of column indices of \(S\) in matrix \(A^{}\). We denote \(L\) as the set of columns indices in \(S\) that do not capture any optimal columns. We denote \(H\) as the set of indices where each column in \(S\) captures exactly one optimal column.

The main idea behind the matched swap pair construction method is to analyze the change in residual error caused by swapping an index from set \(H\) (or \(L\)) with an index from a sampled column, using a two-step mixed sampling approach for the current solution \(S\). For the column \(s_{h}\) (where \(h H\)) in \(S\), \(s_{h}\) captures exactly the column \(s^{*}_{h}\) of the optimal solution \(S^{*}\), serving as the candidate column for \(s^{*}_{h}\). If the residual error of swapping \(s_{h}\) to replace \(s^{*}_{h}\) is large, we prove that with constant probability, sampling a new column can reduce the residual error and update \(s_{h}\). Similarly, for the column \(s_{l}\) (where \(l L\)) in \(S\), \(s_{l}\) does not match any optimal column. We also show that, with constant probability, sampling a column from the input matrix \(A^{}\) can reduce the residual error for columns in set \(L\). To analyze the improvement in residual error during swaps, we focus on a single swap process, evaluating both the increase in residual error from removing a column \(s\) from \(S\) and the decrease in residual error from inserting a new column. We give the following definition to measure the change resulting from removing a column.

**Definition 3.1**.: _Let \(A^{}^{n d}\) be a full-rank matrix, and let \(S\) be a solution on \(A^{}\). Let \(\) be the set of column indices of \(S\). The change in residual error by removing the column \(i\) from \(\) is defined as_

\[(A^{},S,\{i\})=f(A^{},A^{}_{ \{i\}})-f(A^{},S).\]

To bound \((A^{},S, i)\) of solution \(S\) on the matrix \(A^{}\), we provide the theoretical guarantee in the following lemma. (Detailed proof of Lemma 3.2 is given in Appendix A.1)

**Lemma 3.2**.: _Let \(A^{}^{n d}\) be a full-rank matrix, and let \(S\) be a solution on \(A^{}\). Let \(\) be the set of the column indices in \(S\). For \(i\), we have_

\[(A^{},S,\{i\})\|A^{}_{ }A^{}_{}A^{}\|_{F}^{2}.\]

To further analyze the bound on \((A^{},S,\{i\})\), we decompose the projection matrix \(A^{}_{}A^{}_{}A^{}\) and show that the expected upper bound of \((A^{},S,\{i\})\) is proportional to \(\|A^{}\|_{F}^{2}\). (Detailed proof of Lemma 3.3 is given in Appendix A.1)

**Lemma 3.3**.: _Let \(A^{}^{n d}\) be a full-rank matrix, \(k\) be a positive integer, and let \(\) be the set of column indices of \(S\) for the CSS problem on \(A^{}\). In expectation, the following inequality holds_

\[\|A^{}_{}A^{}_{}A^{}\|_{F}^{2} }{d^{2}}\|A^{}\|_{F}^{2}.\]

In the following, we theoretically bound the residual error resulting from adding a candidate column index \(p\) to the set \(\) of column indices in \(S\), where \(p\) is chosen using the two-step mixed sampling method. (Detailed proof of Lemma 3.4 is given in Appendix A.1)

**Lemma 3.4**.: _Let \(A^{}^{n d}\) be a full-rank matrix, \(k\) be a positive integer, and let \(S\) be a solution with the set \(\) of column indices in \(S\). Let \(E=A^{}-SS^{}A^{}\). The column index \(p\) is obtained by executing steps 2-3 of Algorithm 2. In expectation, the following inequality holds_

\[f(A^{},A^{}_{\{p\}}) f_{k}(A^{},opt)+ f(A^{},S),\]

_where \(f_{k}(A^{},opt)\) denotes the best rank-\(k\) solution._

According to the aforementioned mapping function \(()\), we obtain the subset \(H\) from the set \(\) of column indices and the set \(R= H\). By using the matched swap pair construction method, there are two cases for the residual error of the current solution:1. For the set \(H\), where \(_{h H}f(A^{},A^{}_{\{h\}})> _{i}f(A^{},A^{}_{\{i\}})\).
2. For the set \(R= H\), where \(_{r R}f(A^{},A^{}_{\{r\}})_{i}f(A^{},A^{}_{\{i\}})\).

By Lemma 3.2 and Lemma 3.4, we define the good columns \(s_{i}\) for \(i\) with respect to \(S\) as follows.

**Definition 3.5**.: _Let \(A^{}^{n d}\) be a full-rank matrix, and let \(k\) be a positive integer. Let \(S^{*}\) be the optimal solution with exactly \(k\) columns selected, and let \(^{*}\) be the set of column indices in \(S^{*}\). Let \(S\) be any solution with exactly \(k\) columns selected, and let \(\) be the set of column indices in \(S\). A column index \(i\) is called good if_

\[f(A^{},A^{}_{\{i\}})-(A^{ },S,\{i\})-(A^{},A^{}_{ \{p\}},(\{p\})\{i\})\] \[-(f(A^{},A^{}_{^{*} \{i^{*}\}})+f(A^{},S))>f(A^{ },S),\]

_where \(i^{*}^{*}\) is the column index mapped from \(i\) by the function \(()\), and \(p\) is a column index obtained by executing steps 2-3 of Algorithm 2._

Definition 3.5 estimates the gain from replacing the column \(s_{h}\) with a new column obtained using the two-step sampling method. Next, we argue that if case (1) happens, the sum of residual errors for the good columns is large. (Detailed proof of Lemma 3.6 is given in Appendix A.1)

**Lemma 3.6**.: _Let \(A^{}^{n d}\) be a full-rank matrix, \(k\) be a positive integer, and let \(S\) be the solution to the CSS problem on \(A^{}\). Let \(\) be the set of column indices in \(S\). If \(50_{h H}f(A^{},A^{}_{\{h\}}) 21 _{i}f(A^{},A^{}_{\{i\}})\) and \(f(A^{},S) 25(k+1)f_{k}(A^{},opt)\), we have_

\[_{h H,h\,is\,good}f(A^{},A^{}_{\{h\}}) _{i}f(A^{},A^{}_{ \{i\}}).\]

Since \(R= H\), it holds that \(L R\). The index set \(R\) contains two subsets: \(L\) and \(R L\), where the indices in \(L\) do not capture any optimal columns according to the mapping function \(()\) and the indices in \(R L\) capture at least two columns. Similar to case (1), we argue that if case (2) occurs, the sum of residual errors for the good columns is large. (Detailed proof of Lemma 3.7 is given in Appendix A.1)

**Lemma 3.7**.: _Let \(A^{}^{n d}\) be a full-rank matrix, \(k\) be a positive integer, and let \(S\) be a solution for the CSS problem on matrix \(A^{}\). Let \(\) be the set of column indices in \(S\). If \(_{r R}f(A^{},A^{}_{\{r\}}) 29/50 _{i}f(A^{},A^{}_{\{i\}})\) and \(f(A^{},S) 25(k+1)f_{k}(A^{},opt)\), we have_

\[_{r R,r\,is\,good}f(A^{},A^{}_{\{r\}}) _{i}f(A^{},A^{}_{ \{i\}}).\]

In the following, we prove that if the residual error of the current solution is larger than \(25(k+1)f_{k}(A^{},opt)\), each local search step reduces the residual error by a factor of \(1-()\) with constant probability. (Detailed proof of Lemma 3.8 is given in Appendix A.1)

**Lemma 3.8**.: _Let \(A^{}^{n d}\) and \(S^{k d}\) be the input matrices for Algorithm 2, where \(k\) is a positive integer and \(S\) is the solution of the CSS problem on \(A^{}\). Suppose that \(f(A^{},S) 25(k+1) f_{k}(A^{},opt)\). Then, with probability at least \(1/1375\), Algorithm 2 returns a new solution \(S^{}\) with_

\[f(A^{},S^{})(1-1/(100k))f(A^{},S).\]

Subsequently, we prove that the LSCSS algorithm achieves a \(26(k+1)\)-approximation for \(A^{}\) after \(O(k^{2} k)\) iterations.

**Theorem 3.9**.: _Let \(A^{}^{n d}\) be the input matrix obtained in step 12 of Algorithm 1, let \(k\) be a positive integer, and let \(S\) be the solution returned after executing Algorithm 2\(T=O(k^{2} k)\) times. Then, it holds that_

\[[\|A^{}-SS^{}A^{}\|_{F}^{2}] 26(k+1)\|A^{ }-A^{}_{k}\|_{F}^{2},\]

_where \(A^{}_{k}\) is the best rank-\(k\) approximation of \(A^{}\) for the CSS problem. The running time of Algorithm 1 is \(O(ndk^{4} k)\)._Proof.: Let \(\) denote the submatrix consisting of \(k\) columns obtained in step 12 of Algorithm 1. For the initial solution \(\), Deshpande and Vempala  provide an approximation ratio of \((k+1)!\). Before executing steps 13-15 of Algorithm 1, the residual error of the initial solution \(\) is larger than \(25(k+1)\|A^{}-A_{k}^{}\|_{F}^{2}\). According to Lemma 3.8, with probability \(1/1375\), we can reduce the residual error by a multiplicative factor of \((1-1/100k)\).

Let \(T=O(k^{2} k)\). We define a random process \(\) with initial residual error \(\|A^{}-^{}A^{}\|_{F}^{2}\) of the solution \(\) such that for \(T\) iterations of Algorithm 2, it reduces the value of \(\|A^{}-^{}A^{}\|_{F}^{2}\) by at least \((1-)\) with probability \(1/1375\), and it increases the final value of \(\|A^{}-^{}A^{}\|_{F}^{2}\) by \(25(k+1)\|A^{}-A_{k}^{}\|_{F}^{2}\). It is obvious that \([\|A^{}-SS^{}A^{}\|_{F}^{2}][\|A^{ }-^{}A^{}\|_{F}^{2}]\). Then, we have

\[[]= 25(k+1)\|A^{}-A_{k}^{}\|_{F}^{2}+\|A^{}- ^{}A^{}\|_{F}^{2}_{i=0}^{T}^{T-i}(1-)^{i}\] \[ \|A^{}-^{}A^{}\|_{F}^{2} (1-)^{137500k(k+1)!}+25(k+1)\|A^{}-A_{k} ^{}\|_{F}^{2}\] \[ -^{}A^{}\|_{F}^{2}} {(k+1)!}+25(k+1)\|A^{}-A_{k}^{}\|_{F}^{2}.\]

This implies that \([\|A^{}-SS^{}A^{}\|_{F}^{2}|]-^{}A^{}\|_{F}^{2}}{(k+1)!}+25(k+1)\|A^{ }-A_{k}^{}\|_{F}^{2}\).

Thus, we obtain

\[[\|A^{}-SS^{}A^{}\|_{F}^{2}] =_{}[\|A^{}-SS^{}A^{}\|_ {F}^{2}|]Pr()\] \[_{}Pr()(- ^{}A^{}\|_{F}^{2}}{(k+1)!}+25(k+1)\|A^{}-A_{k}^{ }\|_{F}^{2})\] \[-^{}A^{}\|_{F }^{2}]}{(k+1)!}+25(k+1)\|A^{}-A_{k}^{}\|_{F}^{2}.\]

Since \(\|A^{}-^{}A^{}\|_{F}^{2}(k+1)!\|A^{ }-A_{k}^{}\|_{F}^{2}\) in expectation, we have \([\|A^{}-SS^{}A^{}\|_{F}^{2}] 26(k+1)\|A^{}-A_{k}^{ }\|_{F}^{2}\).

**Running Time Analysis.** In LSCSS algorithm, the process of constructing the initial solution in the steps 2-11 of Algorithm 1 takes \(O(ndk^{2})\) time. In order to obtain an \(O(k+1)\)-approximate solution, Algorithm 2 requires \(O(k^{2} k)\) iterations. In each iteration, computing the residual matrix requires \(O(ndk)\) time. The steps 4-8 of Algorithm 2 require \(O(ndk^{2})\) time to recalculate the residual error. Therefore, the overall running time of Algorithm 1 is \(O(ndk^{4} k)\). 

In the following, we analyze the change in residual error caused by replacing the input matrix \(A\) with \(A^{}=A+D\), which leads to the final solution of Algorithm 1 achieving a \(53(k+1)\)-approximation. (Detailed proof of Lemma 3.10 is given in Appendix A.1)

**Lemma 3.10**.: _Let \(A^{n d}\) be an input matrix, and let \(k\) be a positive integer. Define \(D\) as an \(n d\) matrix with elements_

\[D_{ij}=S_{1}^{}A\|_{F}}{(52\{n,d\}(k+1) )^{1/2}},&i=j\\ 0,&,\]

_where \(S_{1}\) is obtained by executing the first round of steps 3-6 in Algorithm 1. Let \(A^{}=A+D\). The solution \(S_{2}\) returned by executing Algorithm 2 for \(T=O(k^{2} k)\) iterations satisfies_

\[[\|A^{}-S_{2}S_{2}^{}A^{}\|_{F}^{2}] 53(k+1)\|A-A_{k} \|_{F}^{2},\]

_where \(A_{k}\) is the best rank-\(k\) approximation for \(A\)._Experiments

In this section, we compare our algorithm for the CSS problem with the previous ones. For hardware, all the experiments are conducted on a machine with 72 Intel Xeon Gold 6230 CPUs and 2TB memory.

Datasets.In this paper, we evaluate the performance of our algorithms on a total of 22 real-world datasets. In previous studies , the CSS problem typically involves datasets with no more than 100,000 rows and 20,000 columns. We include the 14 smaller datasets listed in Table 5 (Appendix A.2). To extend the evaluation to larger datasets, we include 8 additional datasets detailed in Table 2. Six datasets contain between 40,000 and 480,000 columns, and two contain 400,000 and 8 million rows, respectively. All datasets can be found on the website34.

Algorithms and parameters.In our experimental evaluation, we consider the following five distinct algorithms:

* TwoStage. This is a two-stage algorithm from  that combines leverage score sampling and rank-revealing QR factorization.
* Greedy. This is an algorithm in , which uses greedy algorithm to generate solution.
* VolumeSampling. This is an algorithm in , which uses volume sampling method.
* ILS. This is an algorithm in , which uses heuristic local search method.
* LSCSS. This is our algorithm given in Algorithm 1, which uses the two-step mixed sampling and local search methods.

MethodologyWe use the error ratio to evaluate the effectiveness of various algorithms, as defined in . The error ratio is given by the formula \(\|A-SS^{}A\|_{F}^{2}/\|A-A_{k}\|_{F}^{2}\), where it quantifies the discrepancy between the selected columns and the optimal rank-\(k\) matrix approximation. A smaller error ratio indicates better algorithm performance. Following , we test the TwoStage, VolumeSampling, ILS, and LSCSS algorithms on each dataset 10 times to calculate the average error ratio and running time. Since the Greedy algorithm is deterministic, it is tested only once per dataset.

Experimental setup.For the CSS problem with the Frobenius norm, we run the TwoStage, Greedy, ILS, and LSCSS algorithms on both the 8 large datasets and 14 small datasets, providing the average results for each method. The ILS and our LSCSS algorithm are based on local search method. For fair comparison, we set the number of iterations to be \(2k\) for ILS and LSCSS. Since the VolumeSampling requires \(O(dkn^{3} n)\) runtime and \(O(n^{2}+d^{2})\) memory, it cannot handle the 8 large datasets because the algorithm requires more than 48 hours of runtime and over 2TB of memory. However, the other four algorithms generally produce a solution within 48 hours and with less than 2TB of memory. Thus, we only include VolumeSampling in the comparison on the 14 smaller datasets and exclude its results from Tables 3 and 4.

 
**Datasets** & **Instances** & **Features** \\  Condition Monitoring of Hydraulic Systems(CMHS) & 2205 & 43680 \\ Farm Ads (FAs) & 4143 & 54877 \\ Electricity Load Diagrams (ELD) & 370 & 140256 \\ Gas & 180 & 150000 \\ YaleB & 16380 & 307200 \\ Twin Gas Sensor Arrays (TGas) & 640 & 4800000 \\ Epsilon & 4000000 & 2000 \\ Mnist8m & 8000000 & 784 \\  

Table 2: Summary of the datasetsResults for the CSS problem.Table 3 shows the comparison of running time for varying values of \(k\), where the time is measured by seconds. LSCSS is at least 10 times faster than other algorithms across all datasets and at least 15 times faster than the TwoStage and Greedy algorithms. Our algorithm successfully outputs a feasible solution within 5 hours on all datasets, whereas other algorithms fail to do so within 48 hours or need more than 2TB memory. The comparison of error ratios, reported as mean\(\)std with the best results highlighted in bold, is presented in Table 4 in the Appendix A.2. The LSCSS algorithm achieves the best error ratios on almost all datasets.

Moreover, we compare the running time and error ratio of five algorithms with varying values of \(k\) on 14 small datasets (Appendix A.2). The experimental results show that the LSCSS algorithm outperforms other algorithms in terms of quality and is at least 2 times faster than Greedy, VolumeSampling and ILS algorithms on all small datasets.

## 5 Conclusion

In this paper, we propose a linear-time approximation algorithm for the CSS problem using local search and two-step mixed sampling methods. Experimental results demonstrate that our framework outperforms previous algorithms for solving the CSS problem with exactly \(k\) columns selected. An interesting future direction is how to design multi-swap local search approximation algorithms for handling the CSS problem.