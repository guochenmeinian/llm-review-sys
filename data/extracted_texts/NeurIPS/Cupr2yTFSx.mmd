# Online POMDP Planning with Anytime Deterministic Guarantees

Moran Barenboim

Technion Autonomous Systems Program (TASP)

Technion - Israel Institute of Technology

NVIDIA

moranbar@campus.technion.ac.il &Vadim Indelman

Department of Aerospace Engineering

Technion - Israel Institute of Technology

vadim.indelman@technion.ac.il

###### Abstract

Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior node. Then, since a complete belief update may be computationally demanding, we extend the bounds to support reduction of both the state and the observation spaces. We demonstrate how our guarantees can be integrated with existing state-of-the-art solvers that sample a subset of states and observations. As a result, the returned solution holds deterministic bounds relative to the optimal policy. Lastly, we substantiate our findings with supporting experimental results.

## 1 Introduction

Partially Observable Markov Decision Processes (POMDPs) serve as a comprehensive mathematical framework for addressing uncertain sequential decision-making problems. Despite their applicability, most problems framed as POMDPs struggle to achieve optimal solutions, largely due to factors such as large state spaces and an extensive range of potential future scenarios. The latter tends to grow exponentially with the horizon, rendering the solution process computationally prohibitive. The advent of approximate online, tree-based solvers has expanded the capacity of POMDPs, enabling them to tackle larger problems by providing a more scalable approach to problem-solving.

A prominent search algorithm addressing the challenges posed by large state and observation spaces in POMDPs is POMCP (Silver and Veness, 2010). POMCP is a forward search algorithm which handles the large state and observation spaces by aggregating Monte-Carlo rollouts of future scenarios in a tree structure. During each rollout, a single state particle is recursively propagated from the root node to the leaves of the tree. It adaptively trades off between actions that lead to unexplored areas of the tree and actions that lead to rewarding areas of the tree search by utilizing UCT (Auer et al., 2002). The guarantees on the provided solution by POMCP are asymptotic, implying that the quality of the solution remains unknown within any finite time frame.

Another notable approximate solver, Anytime Regularized DESPOT (AR-DESPOT) (Somani et al., 2013; Ye et al., 2017) is derived from Regularized DESPOT, which holds theoretical guarantees for the solution quality with respect to its optimal value. Similar to POMCP, AR-DESPOT performs forward search and propagates a single particle from the root node down to its leaves. It relies on branch-and-bound approach in the forward search, and utilizes dynamic programming techniques to update the value function estimate at each node. In contrast to POMCP, Regularized DESPOT offers a probabilistic lower bound on the value function obtained at the root node, providing a theoretical appeal by measuring its proximity to the optimal policy.

While the primary focus of this paper is on discrete POMDP planning, it is essential to acknowledge recent advancements in POMDP planning that encompass both discrete and continuous observation spaces. Few notable approaches include POMCPOW (Sunberg and Kochenderfer, 2018), LABECOP (Hoerger and Kurniawati, 2021) and AdaOPS (Wu et al., 2021), which leverage explicit use of observation models. These algorithms employ importance sampling mechanisms to weigh each state sample based on its likelihood value, which is assumed to be known. Although these methods have exhibited promising performance in practical scenarios, they currently lack formal guarantees. To address this gap, (Lim et al., 2020, 2022) introduced a simplified solver aimed at bridging the theoretical gap between the empirical success of these algorithms and the absence of theoretical guarantees for continuous observation spaces. In (Lim et al., 2022), probabilistic guarantees were derived for the simplified solver concerning its proximity to the optimal value function, thus contributing to a more comprehensive understanding of POMDP planning in both discrete and continuous settings.

In this paper, we focus on deriving deterministic guarantees for POMDPs with discrete state and observation spaces. Unlike existing black-box sampling mechanisms employed in algorithms such as (Sunberg and Kochenderfer, 2018; Hoerger and Kurniawati, 2021; Wu et al., 2021), our approach assumes access not only to the observation model but also to the transition model and the prior. By leveraging this additional information, we develop a novel algorithm that utilizes a subset of the observation space, enabling the computation of deterministic bounds with respect to the optimal policy at any belief node within the constructed tree. Furthermore, we demonstrate how these deterministic bounds can be integrated with state-of-the-art algorithms, including those that sample subsets of both the state and observation spaces. In particular, we provide experimental results showcasing the use of our bounds incorporated into the AR-DESPOT algorithm.

In this paper, our main contributions are as follows. First, we derive deterministic upper and lower bounds for a POMDP problem by considering a subset of the observation space at each node along the tree under a fixed policy. Next, we extend these bounds to cover scenarios where both the state and observation spaces are restricted to subsets, enhancing the applicability of our bounds in practical settings. Based on the derived bounds, we illustrate how to incorporate the bounds into a general structure of common state-of-the-art algorithms. Specifically, in the experimental section, we present results demonstrating the integration of our theoretical guarantees with the AR-DESPOT algorithm, yielding certified solutions with deterministic bounds. Last, we perform simulations to demonstrate the use of our derived bounds in practice, further validating their relevance in POMDP planning.

Figure 1: The figure depicts two search trees: a complete tree (left) that considers all states and observations at each planning step, and a simplified tree (right) that incorporates only a subset of states and observations, linked to simplified models. Our methodology establishes a deterministic link between these two trees.

## 2 Preliminaries

A finite horizon POMDP \(M\) is defined as a tuple \(,,,T,O,\), where \(\), \(\), and \(\) represent a discrete state, action, and observation spaces, respectively. The transition density function \(T(x_{t},a_{t},x_{t+1})(x_{t+1}|x_{t},a_{t})\) defines the probability of transitioning from state \(x_{t}\) to state \(x_{t+1}\) by taking action \(a_{t}\). The observation density function \(O(x_{t},z_{t})(z_{t}|x_{t})\) expresses the probability of receiving observation \(z_{t}\) from state \(x_{t}\).

Given the limited information provided by observations, the true state of the agent is uncertain and a probability distribution function over the state space, also known as a belief, is maintained. The belief depends on the entire history of actions and observations, and is denoted \(H_{t}\{z_{1:t},a_{0:t-1}\}\). We also define the propagated history as \(H_{t}^{-}\{z_{1:t-1},a_{0:t-1}\}\). At each time step \(t\), the belief is updated by applying Bayes' rule using the transition and observation models, given the previous action \(a_{t-1}\) and the current observation \(z_{t}\), \(b(x_{t})=_{t}(z_{t}|x_{t})_{x_{t-1}}(x_{t}|x_{t-1},a_{t-1})b(x_{t-1})\), where \(_{t}\) denotes a normalization constant and \(b_{t}(x_{t} H_{t})\) denotes the belief at time t. The updated belief, \(b_{t}\), is sometimes referred to as the posterior belief, or simply the posterior. We will use these interchangeably throughout the paper.

A policy function \(a_{t}=_{t}(b_{t})\) determines the action to be taken at time step \(t\), based on the current belief \(b_{t}\) and time \(t\). In the rest of the paper we write \(_{t}_{t}(b_{t})\) for conciseness. The reward is defined as an expectation over a state-dependent function, \(r(b_{t},a_{t})=_{x b_{t}}[r_{x}(x,a_{t})]\), and is bounded by \(-_{} r_{x}(x,a_{t})_{}\). The value function for a policy \(\) over a finite horizon \(\) is defined as the expected cumulative reward received by executing \(\) and can be computed using the Bellman update equation,

\[V_{t}^{}(b_{t})=r(b_{t},_{t})+:}{}[ _{=t+1}^{T}r(b_{},_{})].\] (1)

The action-value function is defined by executing action \(a_{t}\) and then following policy \(\). The optimal value function may be computed using Bellman's principle of optimality,

\[V_{t}^{^{*}}(b_{t})=_{a_{t}}\{r(b_{t},a_{t})+|a_{t},b _{t}}{}[V_{t+1}^{^{*}}(b_{t+1})]\}.\] (2)

The goal of the agent is to find the optimal policy \(^{*}\) that maximizes the value function.

## 3 Mathematical Analysis

Typically, it is infeasible to fully expand a Partially Observable Markov Decision Process (POMDP) tree due to the extensive computational resources and time required. To address this challenge, we propose two approaches. In the first approach, we propose a solver that selectively chooses a subset of the observations to branch from, while maintaining a full posterior belief at each node. This allows us to derive a hypothetical algorithm that directly uses our suggested deterministic bounds to choose which actions to take while exploring the tree. As in some scenarios computing a complete posterior belief may be too expensive, we suggest a second method that in addition to branching only a subset of the observations, selectively chooses a subset of the states at each encountered belief. Using the deterministic bounds at the root node for each action value allows us to certify the performance of a given policy at the root of the planning tree. Moreover, given a scenario in which an action exists whose lower bound surpasses all other actions' upper bound, the algorithm can identify the optimal action and stop further exploration. In contrast, existing state-of-the-art algorithms either do not provide any guarantee on the solution quality (e.g. POMCP Silver and Veness (2010)) or merely provide probabilistic guarantee on the solution (e.g. DESPOT Somani et al. (2013)). In the following section, we show how to use the deterministic bounds in conjunction with state-of-the-art algorithms to obtain performance guarantees. 1

Both of the presented approaches diverge from many existing algorithms that rely on black-box prior, transition, and observation models. Instead, our method directly utilizes state and observation probability values to evaluate both the value function and the associated bounds. In return, we offeranytime deterministic guarantees on the value function for the derived policy and establish bounds on its deviation from the value function of the optimal policy.

In this section we provide a mathematical quantification of the impact of using a solver that only considers a small subset of the theoretical tree branches and a subset of states within each node. We begin by defining a simplified POMDP, which is a reduced complexity version of the original POMDP that abstracts or ignores certain states and observations. We then establish a connection between the simplified value function and its theoretical counterpart. Finally, we demonstrate a relationship between the simplified value function, obtained by following the best policy for the simplified problem, and the theoretically optimal value function.

We begin with general definitions of a simplified prior, transition and observation models,

\[_{0}(x) b_{0}(x)&,\ x}_{0}\\ 0&,\ otherwise\] (3) \[}(x_{t+1} x_{t},a_{t}) (x_{t+1} x_{t},a_{t})&,\ x_{t+ 1}}(H_{t+1}^{-})\\ 0&,\ otherwise\] (4) \[}(z_{t} x_{t}) (z_{t} x_{t})&,\ z_{t} }(H_{t})\\ 0&,\ otherwise\] (5)

where \(}(H_{t+1}^{-})\) and \(}(H_{t})\) may be chosen arbitrarily, e.g. by sampling or choosing a fixed subset a-priori, as the derivations of the bounds are independent of the subset choice. Note that the simplified prior, transition and observation models are unnormalized and thus do not represent a valid distribution function. For the rest of the sequel we will drop the explicit dependence on the history, and denote \(}(H_{t+1}^{-})}\), \(}(H_{t})}\). Using the simplified models, we define the simplified belief \(_{t+1}\). The simplified belief is updated using the simplified belief update equation, which is a modification of the standard belief update equation that replaces the usual models with the simplified ones. More precisely,

\[_{t+1}(x_{t+1})}(z_{t+1}|x_ {t+1})_{x_{t}}}(x_{t+1}|x_{t},_{t})}(x_{ t}|H_{t})}{}(z_{t+1}|H_{t+1}^{-})}&,\ }(z_{t+1} H_{t+1}^{-}) 0 \\ 0&,\ otherwise\] (6)

where \(_{t+1}(x)}(x_{t+1} H_{t+1})\), and \(}(z_{t+1} H_{t+1}^{-})}(z_{t+1}  x_{t+1})_{x_{t}}}(x_{t+1} x_{t},_{t})}(x_{t} H_{t})\). Note that a simplified belief cannot depend on an observation that is not part of the simplified observation set and is considered undefined. Last we define a simplified value function,

\[^{}(_{t})  r(_{t},_{t})+}[(b_{t})]\] (7) \[=_{x_{t}}(x_{t})r(x_{t},_{t})+_{z_{t}}}(z_{t+1} H_{t+1}^{-})((z_{t+1})),\] (8)

where the simplified expectation operator, \(}[]\), is taken with respect to the unnormalized likelihood \(}(z_{t+1} H_{t+1}^{-})\).

### Simplified observation space

We first analyze the performance guarantees of a simplified observation space, while assuming a complete belief update at each belief state, i.e., \(}\). The following theorem describes the guarantees of the observation-simplified value function with respect to its theoretical value,

**Theorem 1**.: _Let \(b_{t}\) belief state at time \(t\), and \(T\) be the last time step of the POMDP. Let \(V^{}(b_{t})\) be the theoretical value function by following a policy \(\), and let \(^{}(b_{t})\) be the simplified value function, as defined in (18), by following the same policy. Then, for any policy \(\), the difference between the theoretical and simplified value functions is bounded as follows,_

\[|V^{}(b_{t})-^{}(b_{t})| \,_{}\!\!_{=t+1}^{T}\!\![1\!-\!\!_{ z_{t+1:}}_{x_{t:}}b(x_{t})\!\!_{k=t+1}^{}\!\!}(z_{k}  x_{k})(x_{k} x_{k-1},_{k-1})]_{z }^{}(b_{t}),\] (9)where we use a subscript of \((z)\) in \(_{z}^{}(b_{t})\) to denote observation-only simplification. Importantly, the bound only contains terms which depend on observations that are within the simplified space, \(z}\). This is an essential property of the bound, as it is a value that can easily be calculated during the planning process and provides a certification of the policy quality at any given node along the tree. Furthermore, it is apparent from (20) that as the number of observations included in the simplified set, \(}\), increases, the value of \(_{z}^{}(b_{t})\) consequently diminishes,

\[_{z_{1:}}_{x_{0:}}b(x_{0})_{k=1}^{}}(z_{k} x_{k})(x_{k} x_{k-1},_{k-1})}}}1\]

leading to a convergence towards the theoretical value function, i.e. \(_{z}^{}(b_{t}) 0\).

Theorem 3 provides both lower and upper bounds for the theoretical value function, assuming a fixed policy. Using this theorem, we can derive upper and lower bounds for any policy, including the optimal one. This is achieved by applying the Bellman optimality operator to the upper bound in a repeated manner, instead of the estimated value function; In the context of tree search algorithms, our algorithm explores only a subset of the decision tree due to pruned observations. However, at every belief node encountered during this exploration, all potential actions are expanded. The action-value of these expanded actions is bounded using the Upper Deterministic Bound, which we now define as

\[^{}(b_{t},a_{t})^{}(b_{t},a_{t})+_{z }^{}(b_{t},a_{t})=r(b_{t},a_{t})+}_{z_{t+1}}[^{}(b _{t+1})]+_{z}^{}(b_{t},a_{t}),\] (10)

where the action-dependent bound on the value difference, \(_{z}^{}(b_{t},a_{t})\), is the bound of taking action \(a_{t}\) in belief \(b_{t}\) and following policy \(\) thereafter,

\[_{z}^{}(b_{t},a_{t})_{}_ {=t+1}^{T}1-_{z_{t+1:}}_{x_{t:}}b(x_{t})}(z_{t+1} x_{t+1})(x_{t+1} x_{t},a_{t})\] (11) \[_{k=t+2}^{}}(z_{k} x_{k})(x_{k} x_{k-1},_{k-1}).\]

In the event that no subsequent observations are chosen for a given history, the value of \(^{}(b_{t},a_{t})\) simplifies to the immediate reward plus an upper bound for any subsequent policy, given by \(_{}(T-t-1)\).

Using UDB we define the action selection criteria according to

\[a_{t}=_{a_{t}}[^{}(b_{t},a_{t})]=_ {a_{t}}[^{}(b_{t},a_{t})+_{z}^{}(b_{t},a_{t })].\] (12)

Moreover, the optimal value function can be bounded as follows,

**Lemma 1**.: _The optimal value function can be bounded as_

\[V^{*}(b_{t})^{}(b_{t}),\] (13)

_where the policy \(\) is determined according to Bellman optimality over the UDB, i.e._

\[^{}(b_{t}) _{a_{t}}[^{}(b_{t},a_{t})+ _{z}^{}(b_{t},a_{t})]\] (14) \[=_{a_{t}}[r(b_{t},a_{t})+}_{z_{ t+1}|b_{t},a_{t}}[^{}(b_{t+1})]+_{z}^{}(b_{t},a_{t})].\] (15)

**Corollary 1.1**.: _By utilizing Lemma 1 and the exploration criteria defined in (65), an increasing number of explored belief nodes guarantees convergence to the optimal value function._

Notably, UDB does not require a complete recovery of the posterior branches to yield an optimal policy. Each action-value is bounded by a specific lower and upper bound, which can be represented as an interval enclosing the theoretical value. When the bound intervals of two candidate actions do not overlap, one can clearly discern which action is suboptimal, rendering its subtree redundant for further exploration. This distinction sets UDB apart from current state-of-the-art online POMDP algorithms. In those methods, any finite-time stopping condition fails to ensure optimality since the bounds used are either heuristic or probabilistic in nature.

[MISSING_PAGE_FAIL:6]

## 4 Methods

``` functionSearch
1:while time permits do
2: Generate states \(x\) from \(b_{0}\).
3:\(_{0} x\)
4:\(_{0}(x_{0} h_{0})\)
5:if\(_{0}(h)\)then
6:\((h)(h)+_{0}\)
7:endif
8:Simulate\((h,D,_{0},}_{0})\).
9:endwhile
10:return functionfwdUpdate\((ha,haz,_{d},}_{},x^{})\)
1:if\(_{d}(ha)\)then
2:\((ha)(ha)\{_{d}\}\)
3:\((ha)(ha)+}_{} r(x,a)\)
4:endif
5:\(_{d}_{d}\{x^{}\}\)
6:\(}_{}}_{} Z_{z|x^{ }} T_{x^{}|x,a}\)
7:if\(_{d}(haz)\)then
8:\(}(haz)}(haz)+}_{}\)
9:\((haz)(haz)\{_{d}\}\)
10:endif
11:return ```

**Algorithm 1**Algorithm-\(\):

In this section we aim to describe how to fit our bounds to a general structure algorithm, named Algorithm\(-\), which serves as an abstraction to many existing algorithms. To compute the deterministic bounds, we utilize Bellman's update and optimality criteria. This approach naturally fits dynamic programming approaches such as DESPOT (Ye et al., 2017) and AdaOPS (Wu et al., 2021). However, it may also be attached with algorithms that rely on Monte-Carlo estimation, such as POMCP (Silver and Veness, 2010), by viewing the search tree as a policy tree.

While the analysis presented in section 3 is general and independent of the selection mechanism of the states or observations, we focus on sampling as a way to choose the simplified states at each belief node and the observations to branch from. Furthermore, the selection of the subspaces \(},}\) need not be fixed, and may change over the course of time, similar to state-of-the-art algorithms, such as Hoerger and Kurniawati (2021); Silver and Veness (2010); Somani et al. (2013); Sunberg and Kochenderfer (2018); Wu et al. (2021). Alternative selection methods may also be feasible, as sampling from the correct distribution is not required for the bounds to hold. Importantly, attaching our bounds to arbitrary exploration mechanism, such as in POMCP or DESPOT, leverages the derivations and convergence guarantees shown in 3.2. Clearly, the deterministic bounds remain valid, but a convergence analysis depends on the characteristics of the specific Algorithm\(-\) being used.

``` functionSimulate\((h,d,_{d},}_{d})\)
1:if\(d=0\)then
2:return\((h) 0\)
3:endif
4: Select action \(a\).
5: Generate next states and observations, \(x^{},z\).
6:\(_{d},}_{}\)fwdUpdate\((ha,haz,_{d},}_{},x^{})\)
7: Select next observation \(z\).
8:Simulate\((haz,d-1,_{d},}_{})\)
9:bwdUpdate\((h,ha,d)\)
10:return functionbwdUpdate\((h,ha,d)\)
11:\((ha)=^{D-d}V_{}(}(h)-}(ha))+ ^{D-d-1} V_{}(}(ha)-_{z|ha}}( haz))\)
12:\(U(ha)\!=\!(ha)+_{z|ha}U(haz)+(ha)\)
13:\(L(ha)\!=\!(ha)+_{z|ha}L(haz)-(ha)\)
14:\(U(h)_{a^{}}\{U(ha^{})\}\)
15:\(L(h)_{a^{}}\{L(ha^{})\}\)
16:return ```

**Algorithm 2**Algorithm-\(\)

Algorithm \(-\) is outlined in algorithm 1. For the clarity of exposition, we assume the following; at each iteration a single state particle is propagated from the root node to the leaf (line 2 of function Search). The selection of the next state and observations are done by sampling from the observation and transition models (line 5), and each iteration ends with the full horizon of the POMDP (lines 2). However, none of these are a restriction of our approach and may be replaced with arbitrary number of particles, arbitrary state and observation selection mechanism and a single or multiple expansions of new belief nodes at each iteration.

To compute the UDB value, we require both the state trajectory, denoted as \(\), and its probability value, \(_{}\). We use the state trajectory as a mechanism to avoid duplicate summation of an already accounted for probability value and is utilized to ascertain its uniqueness at a belief node. The probability value, \(_{}\), is the likelihood of visiting a trajectory \(=\{x_{0},a_{0},x_{1},z_{1},,a_{t-1},x_{t},z_{t}\}\) and is calculated as the product of the prior, transition and observation likelihoods (line 6). If a trajectory was not seen before in a belief node, its reward value is multiplied by the trajectory likelihood, shown in 3. Each node maintains a cumulative sum of the likelihoods of all visited trajectories. This is then beingused to compute the upper and lower bounds, shown in lines 2. The bounds are computed in lines 1 represent the loss of holding only a subset of the sates in node \(ha\) from the set in node \(h\), plus the loss of having only a subset of the future states and observations, where \(V_{}\) represent the maximum possible value function. A simple bound on the value function may be \(V_{}=_{}(D-d-1)\), but other more sophisticated bounds are also possible, as well as different values for lower and upper bounds.

The time complexity for each posterior node, primarily depends on the specific algorithm being used. In the case of dynamic programming methods, such as DESPOT and AdaOPS, there is a negligible added computational complexity detailed below. In the case of Monte Carlo methods, such as POMCP, the computational complexity is \(O(||)\) attributed mainly to the action-selection, while our approach adds another linear time complexity term, making it \(O(||+|}|)\) due to the summation over the simplified observation space. During each iteration of the algorithm, an "IF" statement is used to determine whether a specific trajectory has already been encountered at the current node. This verification process can potentially result in an added linear complexity of \(O(D)\), where \(D\) represents the planning horizon. However, this overhead can be circumvented by assigning a unique ID value to each trajectory at the previous step and subsequently checking whether a pair, comprising the ID value and the new state, has already been visited. This approach reduces the overhead to an average time complexity of \(O(1)\) by utilizing hash maps efficiently.

## 5 Experiments

In this section, we present the experimental results obtained by integrating deterministic bounds into a state-of-the-art algorithms namely, AR-DESPOT Somani et al. (2013) and POMCP Silver and Veness (2010) as a baseline. The primary objective of these experiments is to demonstrate the validity of our derived bounds, as presented in Theorem 2, and the corresponding algorithm outlined in Algorithm 1.

The implementation of our algorithm was carried out using the Julia programming language and evaluated through the Julia POMDPs package, provided by Egorov et al. (2017). Although the library primarily focused on infinite horizon problems, we made the required modifications to extend its capabilities to accommodate finite-horizon POMDPs. The experiments were conducted on a computing platform consisting of an Intel(R) Core(TM) i7-7700 processor with 8 CPUs operating at 3.60GHz and 15.6 GHz. The selection of hyper-parameters for the POMCP and AR-DESPOT solvers, and further details about the POMDPs used for our experiments are detailed in the appendix.

In Figure 2, we demonstrate the empirical convergence of the deterministic bounds in relation to planning time. For these experiments, we focused on a toy example, Tiger POMDP Kaelbling et al. (1998). By conducting an exhaustive search and computing a full posterior belief for each node, we derived the optimal value, depicted as a dashed green line in figure 2. The graph presents the convergence of the bounds calculated with Deterministically-Bounded AR-DESPOT (DB-DESPOT), to the optimal value. The mean values of the upper and lower bounds across 100 simulations are plotted, accompanied by the standard deviation, for time increments of \( t=0.1\) seconds.

Algorithms that provide probabilistic guarantees have a non-zero probability of taking a suboptimal action regardless of the planning time. In contrast, when given sufficient planning time, the deterministic bounds can certify the optimal action once the lower bound of an immediate action exceeds the upper bounds of all other actions. In our experiments, we observed that although the baseline algorithms, AR-DESPOT and POMCP, and the deterministically bounded algorithms, DB-DESPOT

Figure 2: The graph illustrates the convergence of the deterministic bounds using Deterministically-Bounded-AR-DESPOT, with upper and lower bounds depicted alongside the optimal value obtained through exhaustive search.

and DB-POMCP, had access to the same tree and samples, AR-DESPOT and POMCP occasionally made incorrect decisions, resulting in degraded performance.

We evaluated the performance of both algorithms on different POMDPs, including the Tiger POMDP, Discrete Light Dark Sunberg and Kochenderfer (2018) and Baby POMDP. The corresponding results are summarized in Table 1. After each planning session the calculated best action is executed, the belief is updated according to the captured observation and a new planning session is invoked. The table reports the empirical mean and std of the cumulative rewards obtained by executing the calculated best actions based on \(100\) runs.

In the POMDPs with manageable state, action and observation sizes, both the DB-DESPOT and DB-POMCP algorithms frequently identified the optimal action within a 1-second time budget. It's important to highlight that these algorithms employ stochastic methods for belief tree exploration as in the baseline algorithms. Consequently, the outcomes between DB-DESPOT and DB-POMCP can vary, and there is no guarantee of consistently identifying the optimal action in every constrained-time planning session. An optimal action is determined when a lower bound of a given action is higher than any other upper bound; notably, this does not necessitates a construction of the entire planning tree and can be used as an early stopping mechanism for the optimal action. In the larger Laser Tag POMDP Somani et al. (2013), the DB-DESPOT and DB-POMCP did not outperform AR-DESPOT and POMCP. This discrepancy occurred because the planning time was insufficient to find an optimal action due to larger state, action and observation spaces of Laser Tag POMDP. While our proposed algorithms shows promise in more compact POMDPs, their performance in larger-scale problems, like the Laser Tag POMDP, especially when constrained by external time budgets, merits further investigation.

## 6 Conclusions

In this work, we presented a novel methodology aimed at offering anytime, deterministic guarantees for approximate POMDP solvers. These solvers strategically leverage a subset of the state and observation spaces to alleviate the computational overhead. Our key proposition elucidates a linkage between the optimal value function, which is inherently computationally intensive, and a more tractable approximation frequently employed in contemporary algorithms. In the first part of the paper, we derived the theoretical relationship between the use of a selective subset of observations in a planning tree. One contribution of this work is the formulation of an upper deterministic bound (UDB) which governs exploration within the belief space, and is theoretically guaranteed to converge to the optimal value. This approach, however, depends on a complete belief update at each node of the tree, a requirement that can be computationally infeasible in many practical POMDPs. In the second part, we address this computational challenge, by extending our derivations to account for both a subset of the observations and states. This extension increases the feasibility of our approach and allows it to be incorporated into existing state-of-the-art algorithms. We have outlined how our methodology can be integrated within these algorithms. To illustrate the practical utility of our derivations, we applied them to certify and improve the solution of AR-DESPOT and POMCP algorithms, a state-of-the-art solvers for POMDPs.

### Limitations and future work

While the application of our deterministic bounds within the context of the AR-DESPOT algorithm showcased its potential, there are several challenges that need to be addressed in future work. Firstly, the convergence rate of our proposed method remains an area for further exploration. A detailed theoretical analysis on this aspect could provide valuable insights into the practicality of our approach

   Algorithm & Tiger POMDP & Laser Tag & Discrete Light Dark & Baby POMDP \\  DB-DESPOT (ours) & \(3.74\)0.48 & \(-5.29\)0.14 & \(-5.29\)0.01 & \(-3.92\)0.56 \\ AR-DESPOT & \(2.82\)0.55 & \(-5.10\)0.14 & \(-61.53\)5.80 & \(-5.40\)0.85 \\  DB-POMCP (ours) & \(3.01\)0.21 & \(-3.97\)0.24 & \(-3.70\)0.82 & \(-4.48\)0.57 \\ POMCP & \(2.18\)0.76 & \(-3.92\)0.27 & \(-4.51\)1.15 & \(-5.39\)0.63 \\   

Table 1: Performance comparison with and without deterministic bounds.

for larger, more complex domains. Secondly, the current use of the UDB is predominantly restricted to problems characterized by low state dimensionality. Extending the UDB to handle higher state dimensionality is a significant challenge due to the increased computational complexity involved. Finally, we consider the integration of the UDB with simplified state and observation spaces to be a promising future direction. This could potentially lead to a more comprehensive and efficient strategy for handling larger POMDPs, thereby further enhancing the applicability of our deterministic approach.