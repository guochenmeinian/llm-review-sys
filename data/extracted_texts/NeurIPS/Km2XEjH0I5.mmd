# TACT: Advancing Complex Aggregative Reasoning

with Information Extraction Tools

 Avi Caciularu\({}^{}\) Alon Jacovi\({}^{}\) Eyal Ben-David\({}^{}\) Sasha Goldshtein\({}^{}\)

**Tal Schuster\({}^{}\) Jonathan Herzig\({}^{}\) Gal Elidan\({}^{,}\) Amir Globerson\({}^{,}\)**

\({}^{}\)Google Research \({}^{}\)Google DeepMind \({}^{}\)The Hebrew University of Jerusalem \({}^{}\)Tel Aviv University

avica@google.com

tact-benchmark.github.io

###### Abstract

Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT--Text And Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as _IE as a tool_. Specifically, we propose to add "tools" for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.

Figure 1: Annotated components of the TACT dataset. The answer is concise but demands advanced reasoning. Intermediate artifacts aid in analyzing LLM reasoning and designing the _IE as a tool_ method. Relevant spans are underlined.

Introduction

Large Language Models (LLMs) have shown exceptional capabilities across a wide range of natural language tasks. However, they still face significant challenges in solving complex problems that require reasoning over data presented in non-mathematical formats, such as word and algebraic problems (Amini et al., 2019; Dua et al., 2019). Interestingly, research indicates that these types of problems pose difficulties not only for LLMs but also for humans (Cummins et al., 1988; Elliott, 2023). This difficulty mirrors a broader observation about LLM reasoning capabilities: the process of transforming linguistic or graphical inputs into solvable mathematical equations is often more challenging than performing the calculations themselves (Schick et al., 2023; Das et al., 2024). This overarching issue is particularly evident when LLMs attempt tasks that involve the aggregation of information from either single or multiple texts. These models frequently underperform in tasks that require counting, comparing, or processing similar events or entities within texts (Caciularu et al., 2022; Amouyal et al., 2023; Li et al., 2023). This highlights a fundamental limitation: while LLMs can handle isolated data points effectively, their ability to integrate and interpret information across contexts remains a significant hurdle.

A first step towards advancing the capabilities of LLMs on complex reasoning, is to have a high-quality benchmark for evaluating and analyzing their performance in this setting. To this end, we introduce TACT--Text And Calculations through Tables. TACT instances were created by NLP and data science experts who wrote aggregative queries over texts. The experts were instructed to use tables (from the InstructIE dataset (Gui et al., 2023)) as the basis for writing the instructions, as they consolidate dispersed information from source texts into a structured format, enabling comprehensive aggregation across the text. The resulting TACT instances consist of the original text, the written instruction, and a gold answer, all requiring advanced text comprehension and reasoning (illustrated in Figure 1). Importantly, the TACT task does not include the table, so as to test the ability of the model to answer aggregative queries in an end-to-end manner, requiring advanced text comprehension and reasoning. Through TACT, the model is implicitly required to address information extraction (IE) challenges such as coreference resolution (Lee et al., 2017; Joshi et al., 2019; Kirstain et al., 2021), multi-hop reasoning (Lin et al., 2018; Dua et al., 2019; Zhao et al., 2022), summarization (Zhang et al., 2020; Goyal et al., 2022; Slobodkin et al., 2023), and multi-document processing (Caciularu et al., 2021; Hirsch et al., 2021; Caciularu et al., 2023; Zhu et al., 2024). In Section 2, we describe our methodology for constructing this benchmark--by layering our new expert annotations over InstructIE instances--and the measures we took to ensure its difficulty and robustness.

Having evaluated LLMs on TACT and observed the challenges it presents to them (Section 4), we aim to understand the root of these difficulties and explore potential modeling improvements. We propose dissecting the problem into three tasks: table-generation, Pandas command-generation, and command-execution. Leveraging TACT's ground-truth tables and Pandas commands curated by experts, we analyze the LLM performance of each step in Section 5. Our findings reveal significant performance headroom in each task, implying that with targeted few-shot prompting, models can considerably enhance their individual task performance. Building on these results, we propose a focused modeling strategy termed the _IE as a tool_ framework, which specifically addresses each phase independently (see an illustration in Figure 4 and more details in Section 3). This approach has shown to be superior to existing prompting techniques, as detailed in Section 4. The demonstrated improvements suggest a promising direction for enhancing LLM capabilities in complex reasoning tasks, aligning with our initial findings of untapped potential in each dissected component of the task.

Our contributions are summarized as follows:

* TACT: An expert-curated, diverse evaluation dataset that challenges LLMs on following aggregative queries, requiring information extraction and complex reasoning.
* A rigorous analysis of LLM performance on decomposed TACT tasks, revealing model strengths and weaknesses in table-generation, Pandas command-generation, and execution.
* Introduction of the _IE as a tool_ framework, leveraging the aforementioned sub-tasks as discrete tools, demonstrating up to 12% improvement over conventional prompting techniques.

Dataset

This section introduces TACT--Text And Calculations through Tables--a novel challenge set designed to evaluate and improve the capability of LLMs on complex queries that require integration of information. The data is derived from the InstructIE (Jiao et al., 2023) test set using new expert-annotated labels, as described below. In this section, we detail the data labeling methodology employed to create TACT, highlighting the steps taken to ensure the _reliability_ and _validity_ of the labeled data. We first introduce the InstructIE benchmark creation methodology (Section 2.1), then we introduce our TACT dataset (Section 2.2), and finally, we explore the properties and conduct an analysis of TACT (Section 2.3).

### Background: The InstructIE Dataset

InstructIE (Jiao et al., 2023) is a dataset that includes texts alongside corresponding tables, which summarize the textual content. These tables effectively organize the extracted information into sets of triples--subjects, relations, and objects--derived from the texts. To compile the tables and texts in the test set of InstructIE, which we employed for creating TACT, human annotators first defined the table topics and columns using real-world texts from the web. These texts were then utilized to craft tables that summarize them through a process combining automatic extraction and manual validation.

The primary components of InstructIE that we utilized are: **Text**- the accompanying document or collection of short documents, **Table**- a structured representation of the extracted information, where the first row serves as the table header and the subsequent rows contain the extracted data. See an illustrative example in Figure 7 in Appendix C.2, and Jiao et al. (2023) for additional details and descriptions of other components that were not included in our study. While InstructIE provides a good setup for information extraction, it does not directly test the models' abilities to aggregate the extracted information, which we target in TACT.

### The TACT Dataset

Our goal is to evaluate the capabilities of LLMs in addressing aggregative, information-seeking queries, that require both text comprehension and complex reasoning. Using tables as the basis for creating such queries is highly effective, since they consolidate the essential information from their source texts into a structured format. Thus, performing an aggregation on these tables is equivalent to executing an aggregation across the entire text. We leverage and extend the use of the InstructIE dataset, which already contains structured information in table format (see above). We introduce the Text And Calculations through Tables (TACT) challenge set, aimed at verifying the capabilities of LLMs in handling complex numerical instructions (the resulting TACT dataset and its components are compared to InstructIE in Figure 7 in Appendix C.2).

TACT was created by NLP and data science experts, who employed a rigorous annotation process to transform InstructIE instances into a format suitable for aggregative instruction following. Creating the data includes the steps of assessing the text and the table, then formulating a query in natural language, and finally translating the query into a Pandas command, and executing it on the table. We chose Pandas over other languages, such as SQL, due to its simplicity. While SQL requires defining a schema, Pandas can easily operate on a single dataframe and often provide solutions with just a single line of code.

Additionally, two human passes were conducted over the dataset, where an expert human validator ensured 100% accuracy. The expert achieved this level of precision given the lack of ambiguity in the questions, further strengthening the reliability of the data. See the data creation guidelines, summarization of the annotation process, and more details about the data creation in Appendix C). The full steps are:

**Initial Review and Relevance Vetting:** A comprehensive review of the InstructIE dataset, focusing on texts and tables out of InstructIE's test set containing numerical data. Experts identified tables and text segments where numerical data was present and suitable for quantitative instructions. Tables were vetted for numerical integrity and alignment with the text to ensure data quality. For the remaining examples, the experts were tasked to convert the Markdown-formatted tables from InstructIE into the CSV table format (for convenient parsing into the Pandas dataframe format).

**Numerical Aspect Identification:** Experts identified _specific numerical aspects_ within the text and tables--such as years, currencies, population counts, and temperatures--that enable quantitative operations like counting, calculation, and aggregation. This step identifies which aspect of the table data should be incorporated into the instruction.

**Natural Language Instruction Formulation:** Based on the identified numerical aspects, experts formulated clear and precise natural language instructions over the text that result in a single numerical value. These instructions targeted the numerical aspects with a focus on aggregation functions like sum, mean, and filtering.

**Natural Language Query Over the Table:** After formulating the natural language instructions, experts verbalized them into corresponding natural language queries over the tables (see Figure 1). These queries refined the focus on the numerical data within the table, minimized ambiguity, and helped to prepare the Pandas command.

**Translation to Pandas Commands and Gold Response Extraction:** Next, experts translated the previous natural language query over the table into a Pandas command. Then, they extracted the gold response by executing the formulated Pandas commands over the tables.

**Command Execution and Validation:** Finally, the extracted responses were manually verified against the expected outcomes derived from the formulated instructions and texts. This validation step ensured that the results were consistent with the intended instructions and the underlying data from both the text and the tables.

Each instance in the dataset consists of (see illustrative example in Figure 1):

1. Original Text and Table: Sourced from the InstructIE dataset, these elements contain the foundational data and numerical information relevant to the query. The text provides context, while the table offers structured numerical data aligned with the text content.
2. Natural Language Question: A clearly formulated query in natural language that targets specific numerical aspects identified in the text and table. These questions focus on computational tasks like sum, mean, and filtering to challenge the models' understanding and processing capabilities.
3. Natural Language Query Over the Table: After formulating the natural language question, a corresponding natural language query over the table is developed. This step refines the focus on the numerical data within the table, ensuring that the essential information for the computation is precisely delineated and consistent with the intent of the initial question.
4. Pandas Command: A precise translation of the natural language question into a Pandas command. This command is designed to replicate the expected computational process using the original column names from the table, ensuring the accuracy and consistency of the data manipulation.
5. Expected Result: The correct numerical answer derived from executing the Pandas command, serving as a benchmark to validate the models' responses against the ground truth.

The resulting TACT dataset contains 124 examples,1 as well as additional 4 examples that serve as optional few-shot examples for in-context learning.2 For evaluating performance on TACT, we employ exact match for the final answer, since it is a single-span (number). For intermediate steps available in TACT, such as table- and command-generation, we utilize a both similarity metrics (e.g., ROUGE ) and execution-based metrics (e.g., accuracy of the generated command's output) as described in Section 5.

### Exploring the Numerical Challenges in TACT

In this section, we delve into the characteristics of the TACT dataset. TACT offers a diverse range of tasks, primarily focusing on two types of instructions--"Calculate" and "Count":

**"Calculate" Instructions:** Out of the 124 examples, 63 are categorized under "Calculate" instructions. These tasks require the execution of basic mathematical operations to solve the instance. As depicted in Figure 3, the operations include addition, subtraction, multiplication, division, and other arithmetic functions. The distribution of these operations, such as summation (28.3%), mean calculation (10.4%), and power functions (6.1%), highlights the varied complexity and the need for precise computational understanding by the models.

**"Count" Instructions:** The remaining 61 examples fall under "Count" instructions, where the primary objective is to identify specific types or categories within the attached text and perform a simple counting operation. This task challenges the model's ability to accurately parse and interpret textual data, identifying relevant entities or events, and perform the proper counting.

The composition of the TACT dataset, with a balanced mix of "Calculate" and "Count" instructions, ensures a comprehensive evaluation of models across different dimensions of numerical reasoning. The operations, as detailed in the pie chart (Figure 3), further emphasize the diversity and scope of numerical challenges that TACT presents, offering a broad testbed for evaluating both fundamental and complex computational reasoning. This variety plays a pivotal role in assessing how well models can generalize their mathematical skills to real-world tasks.

In Figure 3, we present a comparison of the total number of cells in a table against the lengths of the corresponding Pandas commands. The figure reveals a wide distribution of data points, illustrating that the length of Pandas commands, quantified in Gemini tokens (Gemini-Team et al., 2023), does not correlate directly with the total number of cells in the tables. The varied spread of points across the graph indicates that additional factors, such as the complexity of arithmetic operations required or the specific data arrangement within the tables, might play a more significant role in determining the length of the commands than simply the volume of data.

In Table 1, we present an illustration of the diverse range of implicit tasks incorporated within TACT, which are specifically designed to test advanced text comprehension and numerical reasoning. Each task is tied to text spans that underline the specific data points or contextual clues necessary for task completion, ranging from multi-document summarization to date and time numerical reasoning. For example, one task leverages coreference resolution, requiring the model to understand and connect information spread across different parts of the text. Another task tests the model's capability for lexical matching, identifying specific words within a context. Complex arithmetic operations are also present, demanding a high level of numerical literacy to interpret numerical and financial concepts. This highlights the interplay between linguistic understanding and numerical computations, demonstrating the ability to handle a wide spectrum of real-world tasks--from simple counting to complex, multi-step mathematical operations embedded within textual data.

[MISSING_PAGE_FAIL:6]

Experimental Setup and TACT Results

We assess the performance of several models on the TACT dataset, including GPT-4o [OpenAI et al., 2024], Claude 3.5 Sonnet [Anthropic, 2024], Gemini-1.0-Ultra [Gemini-Team et al., 2023], Gemini-1.5-Pro [Reid et al., 2024], Llama-2-13b-chat, Llama-2-70b-chat [Touvron et al., 2023], Gemma-7b-it (v1.1) [Gemma-Team et al., 2024], Mistral-7b-instruct (v0.2) [Jiang et al., 2023], and Mistral 8x7B [Jiang et al., 2024]. Their capabilities are tested using both standard prompting techniques and our _IE as a tool_ method (detailed in Section 3). Subsequent analysis focuses on the specific sub-tasks of table generation (Section 5.1) and Pandas command generation (Section 5.2), providing a comprehensive evaluation of each model's performance and identifying potential areas for improvement in each sub-task.

We measure performance by averaging over four combinations of few-shot examples (by sampling the order and examples), following the procedure described in Jacovi et al. [2023b], and report the results for {0, 2, 4}-shot, using dedicated examples from the validation set of InstructIE (see more details in Appendix D).

We evaluate the LLMs' performance on the TACT task, measuring exact match, where the models are provided a numerical instruction and a text, and are tasked to produce the correct numerical answer. We report 4-shot results for Gemma-7b-it, Mistral-7b-instruct, and Mistral 8x7B, while zero-shot results for the remaining larger models, given the results on table and Pandas query generation in the following sections, and the observation that few-shot demonstrations yield saturated performance for most of the tasks for the larger models. We propose the following experimental setups for each LLM:

* **Generic:** The baseline setting where the LLM receives a TACT instruction and a text passage, and is tasked with directly generating the answer.
* **Chain-of-Thought (CoT):** This setting is akin to the baseline, with the enhancement of adding "Let's think step-by-step" to the prefix (input). This encourages the model to generate a detailed, step-by-step CoT reasoning before producing the final answer.
* **In-context IE:** This method adopts a chain-of-thought-like approach, where the LLM first generates a table from the text, then creates a Pandas query, and finally, with all this context provided, the model generates the answer, all within the same prompt.
* **IE as a tool:** Here, the model can utilize generated tables and Pandas queries to answer the instruction, like the previous setup, however, in this variation, we employ few-shot prompted LLMs as separate tools (the number of shots applies for these tools). See Section 3 for more details. We include both _Without Pandas_ and _With Pandas_ variants as an ablation. We include these since as detailed in Section 5.2, even with syntactic errors in the Pandas commands, these errors may still assist the model in generating correct outputs.
* **IE as a tool (Gold):** This configuration is similar to the previous one (with Pandas command) but utilizes gold-standard (i.e., ground truth) tables (Gold Table) and/or Pandas commands (Gold Table+Pandas) from TACT, rather than relying on outputs generated by the tools. This baseline serves as an upper bound for the performance of the _IE as a tool_ approach.

    &  &  &  &  \\   & & & & & **Without/With Pandas** & **Gold Table** & **Gold Table + Pandas** \\  Gemma-7b (4-shot) & 17.1 & 25.4 & 26.2 & 27.1 / 28.9 & 33.3 & 45.1 \\ Mistral-7b (4-shot) & 2.4 & 2.6 & 2.6 & 2.7 / 3.5 & 4.1 & 10.9 \\ Llama-2-13b (4-shot) & 8.5 & 8.0 & 8.3 & 8.5 / 8.5 & 9.2 & 14.3 \\ Mistral 8x7B (4-shot) & 4.4 & 4.2 & 5.2 & 5.9 / 6.1 & 6.3 & 12.6 \\ Llama-2-70b (0-shot) & 3.6 & 4.0 & 4.2 & 4.5 / 7.6 & 12.9 & 22.4 \\ Gemini-Pro (0-shot) & 28.4 & 34.7 & 12.3 & 40.2 / 41.9 & 48.5 & 72.1 \\ Gemini-Ultra (0-shot) & 25.4 & 37.3 & 36.6 & 39.7 / 41.4 & 49.8 & 72.3 \\ GPT-4o (0-shot) & **30.1** & 37.7 & 36.4 & **41.1 / 42.2** & 50.9 & 74.1 \\ Claude 3.5 Sonnet (0-shot) & 28.6 & **37.9** & **36.8** & 40.8 / 42.1 & **51.3** & **74.6** \\   

Table 2: Exact match accuracy evaluation results of different models on TACT, evaluated across different experimental setups, including Generic, Chain-of-Thought (CoT), In-context IE, and IE as a tool with various settings. The best-performing results are highlighted in **bold**.

The results, which are depicted in Table 2, show that all models experience substantial benefits when using the _IE as a tool_ approach, with a small improvement when tasked to generate a Pandas command. This particularly clear in the larger models (Gemini, GPT, and Claude), which excel on this task, where Claude outperforms the rest of the models. This is evident from the consistent performance improvement across different models when comparing the _IE as a tool_ setup with the Generic baseline and other approaches. On the other hand, smaller models showed more moderate improvements when using _IE as a tool_. The gap between the performance in _IE as a tool_ and the Gold variant implies a significant potential for enhancing the overall task effectiveness through the refinement of IE tools. Specifically, the larger gap for smaller models suggests that their capabilities can be dramatically increased by improving the accuracy and reliability of the generated tables and commands. This points towards the critical importance of future work on optimizing IE tools to maximize the end-task performance for complex reasoning tasks.

## 5 Performance Analysis via TACT Decomposition

To understand the factors contributing to the suboptimal performance of current LLMs on TACT, we decompose the problem into two constituent tasks: 1) table generation from text based on a TACT instruction and the corresponding text (Section 5.1), and 2) Pandas query generation based on the corresponding table (either gold or previously generated), instruction, and text (Section 5.2). Successful execution of these two tasks would naturally result in accurate TACT results. We assess current LLM capabilities on each task using gold outputs, revealing substantial headroom and a potential for improvement. This observation directly motivated our design of _IE as a Tool_, a method that demonstrably improved TACT performance.

### Evaluating the Accuracy of Table Generation

We assess the capabilities of LLMs to generate the appropriate tables given a TACT instruction and its corresponding text, taking the model to construct the correct table based on the specified instruction. Note that the model should infer the correct table from the TACT instruction, which only implicitly points towards the relevant information to extract. Each model is provided with TACT instructions and corresponding texts. The task requires generating tables that accurately reflect the data described in the text, and helps to seek the correct information given the instruction.

We follow the evaluation protocol from Jiao et al. (2023) and adopt a soft matching strategy (Jiao et al., 2022) by using SentenceT5-Large (Ni et al., 2022) to calculate the cosine similarity (multiplied by 100) as the semantic similarity score between the generated table and the gold table, as table contents reflect the quality of extraction. Additionally, we use the ROUGE-L F1 score (Lin, 2004) to evaluate the lexical similarity of the generated table to the gold one. We also report the Table Validity rate, where we were able to parse a syntactically correct CSV table from the generated content.

    &  &  &  \\  Model & 0-shot & 2-shot & 4-shot & 0-shot & 2-shot & 4-shot & 0-shot & 2-shot & 4-shot \\    & 68.6 & 69.1 & 69.4 & 6.5 & 6.6 & 7.1 & 0.6 & 23.1 & 25.5 \\  & 73.5 & 72.8 & 72.8 & 4.9 & 6.4 & 7.0 & 1.2 & 30.9 & 34.4 \\  & 73.4 & 72.7 & 73.1 & 4.3 & 5.2 & 5.5 & 42.7 & 43.5 & 47.4 \\  & 72.3 & 71.7 & 71.8 & 4.5 & 7.1 & 7.2 & 9.5 & 39.9 & 24.3 \\  & 72.3 & 72.4 & 72.7 & 3.9 & 5.1 & 4.9 & 92.5 & 73.4 & 73.8 \\  & 78.4 & 78.2 & 78.3 & 18.8 & 21.0 & 22.9 & 81.5 & 90.2 & 93.3 \\  & 78.6 & 78.6 & 79.3 & 18.7 & 21.1 & 24.8 & 81.3 & 89.9 & 94.1 \\  & 78.2 & **78.9** & 79.9 & 19.3 & **23.2** & 27.3 & 93.4 & 93.6 & 95.7 \\  & 78.5 & 78.6 & **80.1** & **19.7** & 23.1 & **28.1** & **94.1** & **94.2** & **96.2** \\   

Table 3: Evaluation Results of Different Models on TACT table generation, measuring semantic similarity, ROUGE-L F1 (lexical matching), and table validity between the generated tables and the gold tables. The best-performing results are highlighted in **bold**.

The evaluation of various LLMs on their ability to generate accurate tables based on TACT instructions is shown in Table 3. Claude and GPT mostly outperform other models across all metrics. The Gemini models also shows strong performance but vary across different shots, indicating potential instability in its output quality. Llama-2-13b, Llama-2-70b, and Mistral-7b exhibit moderate performance, with Mistral-7b achieving a higher semantic similarity but lower table validity rates. Gemma-7b and Mistral 8x7B show comparatively lower performance, particularly in table validity. Notably, the smaller models like Gemma-7b and Mistral-7b benefit significantly from few-shot learning, demonstrating that small models are incapable of solving this task without any aid.

While the semantic similarity between the generated tables of Gemini-Ultra and the gold standard tables is relatively high, lexical similarity remains low. However, a qualitative analysis suggests that the generated tables contain key information that addresses the instructions, despite their differences from the gold tables. This observation supports the use of semantic similarity as a more appropriate metric for evaluating table generation in this context (Jiao et al., 2023).

### Evaluating the Accuracy of Pandas Command Generation

The ability to accurately generate Pandas commands is a key intermediate step in solving TACT queries, and evaluates how well LLMs can comprehend the TACT instruction and the table at once. Thus, we next evaluate the ability of LLMs to generate Pandas commands, when provided with the TACT instruction, the associated text, as well as a table extracted from the text. We consider two cases: one where the provided table is the gold one, and one where it is the one generated by the model. To assess the quality of the generated Pandas queries, we execute them using a Python interpreter, and compare the output to the gold answer.

Table 4 presents the results, where Claude consistently outperforms the other models. GPT, Gemini, and Llama-2-70b also demonstrate relatively strong performance, though with some variability across different shot configurations. Interestingly, as in the previous experiment, the smaller models--such as Gemma-7b and Mistral-7b--showed lower performance overall but exhibited significant improvements with few-shot learning, highlighting their ability to effectively leverage additional examples. Llama-2-13b and Mistral 8x7B delivered moderate performance but still trailed behind the larger models.

It is worth noting that the overall numbers in Table 4 are quite low, even when compared to the results in Table 2, including for gold-standard tables usage. This may seem surprising at first, but upon inspection, we found that many of the Pandas commands generated by the models contain syntax errors or other issues that lead to execution failures. In contrast, the results in Table 2 do not involve Python execution, which allows for more robust command interpretation and, as a result, better answers.

## 6 Related Work

Information Extraction (IE) and Text-to-TableIE is the process of automatically extracting structured information from unstructured text, involving sub-tasks like named entity recognition,

  
**Model** & **0-shot (Generated/Gold)** & **2-shot (Generated/Gold)** & **4-shot (Generated/Gold)** \\  Gemma-7b & 0 / 0.4 & 1.4 / 2.3 & 1.9 / 2.4 \\ Mistral-7b & 0.0 / 0.1 & 0.4 / 0.6 & 0.5 / 0.9 \\ Llama-2-13b & 0.0 / 0.3 & 0.1 / 0.6 & 0.3 / 1.2 \\ Mistral 8x7B & 1.2 / 1.8 & 1.3 / 2.1 & 1.5 / 2.9 \\ Llama-2-70b & 2.5 / 3.4 & 3.1 / 4.1 & 3.2 / 4.3 \\ Gemini-Pro & 3.4 / 3.6 & 3.0 / 4.8 & 7.3 / 8.0 \\ Gemini-Ultra & 1.1 / 1.9 & 4.8 / 5.0 & 7.7 / 8.4 \\ GPT-4o & 4.5 / 4.9 & 5.3 / 6.0 & 8.7 / 9.4 \\ Claude 3.5 Sonnet & **5.1 / 5.9** & **5.6 / 6.4** & **9.6 / 10.1** \\   

Table 4: Evaluation Results of Different Models on TACT Pandas command generation on the generated/gold table, measured by the accuracy after executing the command with a Python interpreter. The best-performing results are highlighted in **bold**.

relation extraction, and event extraction. Many works have leveraged large language models (LLMs) to provide effective solutions for IE [Ma et al., 2023, Lu et al., 2023, Zhou et al., 2024]. Recently, Wu et al.  presented the concept of text-to-table, and Jiao et al.  introduced InstructIE, a benchmark that includes triplets of an IE instruction, their associated text, and the relevant content in a tabular format (see Section 2). We employ a labeling methodology on top of InstructIE to distill an aggregative instruction following challenge set. Yuan et al.  presented an effort with a similar focus on numerical tasks but with a narrower scope, limited to financial data. Another related research realm is open information extraction, which aims to extract information without predefined schemas, typically focusing on simple structures from short texts [Banko et al., 2007, Mausam et al., 2012, Stanovsky et al., 2018, Zhan and Zhao, 2020].

Complex and Numerical ReasoningA persistent challenge for LLMs lies in their ability to solve numerical problems, particularly those involving mathematical calculations [Geva et al., 2020, Imani et al., 2023, Chang et al., 2024, Ahn et al., 2024] or the need to stitch and aggregate information across the text [Li et al., 2023, Sprague et al., 2024, Jacovi et al., 2024]. Some works propose to evaluate models on such tasks, by presenting challenge datasets based on financial data [Chen et al., 2021, Yuan et al., 2024]. DROP [Dua et al., 2019] and IIRC [Ferguson et al., 2020], two reading comprehension benchmarks involving reasoning, showcase the complexity of comprehensive numerical reasoning. DROP focuses on discrete reasoning over paragraphs, requiring models to perform operations like addition, counting, and sorting, while IIRC evaluates the ability to handle incomplete contexts and locate additional sources of information. TACT focuses on a more practical and common use-case: reasoning over texts given natural language instructions, necessitating the integration of information scattered throughout the text.

Semantic Parsingis the process of converting natural language into a machine-interpretable representation, such as a formal query or command [Pasupat and Liang, 2015, Yoran et al., 2022, Mekala et al., 2023, Bogin et al., 2023]. _IE as a tool_ also aligns with previous research on semantic parsing, as we utilize executable Pandas command generation over texts and tables, demonstrating how LLMs can interpret and convert complex instructions into executable code operations.

Multi-step Reasoning and LLM ToolsOur research is closely related to various methodologies that utilize LLM tools for task resolution, as explored in recent studies [Parisi et al., 2022, Mialon et al., 2023, Schick et al., 2023, Hao et al., 2023, Patil et al., 2023]. These methods train LLMs to utilize APIs independently during inference, contrasting with our _IE as a tool_ approach, which employs a static strategy for addressing complex reasoning tasks. Furthermore, prior research has emphasized enhancing task resolution through multi-step processes [Berant et al., 2014, Drozdov et al., 2023, Zhou et al., 2023, Fu et al., 2023]. Unlike these approaches, which apply general multi-step reasoning or tool triggering across various domains, _IE as a tool_ specifically concentrates on constructing tables and executing commands for numerical reasoning, thereby targeting a more focused application of multi-step numerical reasoning.

## 7 Conclusion

In this paper, we introduced TACT--Text And Calculations through Tables, a dataset designed to assess the reasoning capabilities of Large Language Models (LLMs) through complex, aggregative instructions. TACT features numerical instructions that require processing and integrating information dispersed across one or more texts for producing the correct answer. By leveraging the InstructIE dataset [Jiao et al., 2023], experts annotated and transformed instances into a format suitable for aggregative instruction following, ensuring high precision and relevance. To better understand the performance of LLMs on TACT, we provide further analysis that evaluates performance on two distinct sub-tasks that are likely to be relevant for solving TACT (table-generation and Pandas command-generation). We also provide a modeling scheme, _IE as a tool_, that is based on this decomposition, and show that it improves performance on TACT. Future work could focus on further enhancing the performance of LLMs on TACT by developing and integrating new, more sophisticated tools that are specifically designed for handling complex, aggregative instructions over text.