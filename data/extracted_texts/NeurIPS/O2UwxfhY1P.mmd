# On the Comparison between Multi-modal and Single-modal Contrastive Learning

Wei Huang

RIKEN AIP

wei.huang.vr@riken.jp

&Andi Han

RIKEN AIP

andi.han@riken.jp

&Yongqiang Chen

The Chinese University of Hong Kong

yqchen@cse.cuhk.edu.hk

&Yuan Cao

The University of Hong Kong

yuancao@hku.hk

&Zhiqiang Xu

MBZUAI

zhiqiang.xu@mbzuai.ac.ae

&Taiji Suzuki

University of Tokyo & RIKEN AIP

taiji@mist.i.u-tokyo.ac.jp

Equal Contribution.Corresponding Author.

###### Abstract

Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability. Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning. In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning. Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function. Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning. Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning. Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning. Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings.

## 1 Introduction

Large-scale pre-trained models have achieved unprecedented success, including GPT series [6; 41], LLaMa , among many others. CLIP  as a typical example, uses a multi-modal contrastive learning framework to learn from a massive scale of image-caption data. The multi-modal contrastive learning in CLIP has shown significant capabilities to learn high-quality representations, which are ready to be adapted to a wide range of downstream tasks, forming the backbone of generative modelslike DALL-E2 , prompt learning  as well as general purpose multi-modal agents [62; 35]. Given the huge success of models like CLIP that have stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, they have been widely recognized as foundation models (FMs). More similar examples are given by ALIGN , Florence , BLIP , Flamingo .

Despite the unprecedented success achieved by multi-modal contrastive learning, the fundamental mechanism that leads to greater performance, especially compared to single-modal contrastive learning is still under-explored. Recently, several seminal works provided theoretical explanations for either single-modal [4; 5; 14; 48; 24; 7; 50; 49; 20] or multi-modal contrastive learning [38; 37; 44; 12]. For example,  studied how single-modal contrastive learning learns the feature representations for neural networks by analyzing its feature learning process. As for multi-modal contrastive learning, [12; 58] provided explanations for why multi-modal contrastive learning demonstrates zero-shot transferability, and robustness to distribution shifts, than supervised learning, which offer valuable insights. Although both lines of the existing works provide valid theoretical insights under the respective settings, rare work has compared the optimization and generalization of the two types of contrastive learning under a unified framework. This motivates us to establish a systematic feature learning analysis for both single-modal and multi-modal contrastive learning.

In particular, we consider a data generation model that contains two modalities of data, which are generated from signal and noise features. The signal feature correlates in different modalities, while there is no correlation between noise features among modalities. We then study the optimization of single-modal and multi-modal contrastive learning under gradient descent training. By studying the trajectories of signal learning and noise memorization, we establish the convergence conditions and further characterize the generalization ability in the downstream tasks. The results show that, through the cooperation between modalities, multi-modal contrastive learning can achieve better generalization in the downstream task. In contrast, without the help of the second modality, single-modal contrastive learning concentrates on learning noise from the data, and thus generalizes poorly on the downstream tasks. The main contributions of this work are summarized as follows:

* This work establishes the _first systematic comparative optimization analysis_ for single-modal and multi-modal contrastive learning under gradient descent training in non-convex settings. We show that both single-modal and multi-modal can achieve near-zero training error under InfoMax contrastive loss after polynomial number of iterations, by overcoming the non-convex difficulty.
* By a trajectory-based analysis of the signal learning and noise memorization of the ReLU network from the data, we successfully characterize the difference in _generalization_ between single-modal and multi-modal contrastive learning. The distinct SNRs of different modalities lead to a divergence in the generalization of downstream tasks for the two contrastive learning frameworks.
* Our theory suggests that the advantage of multi-modal over single-modal contrastive learning comes from the high quality of the second modality and the cooperation between the two modalities through contrastive learning. This divergence is ultimately reflected in the difference in feature learning and the final gap in downstream task generalization. Experimental results on both synthetic and real-world datasets confirm our theoretical findings and understanding.

## 2 Related Work

**Theoretical Understanding of Single-modal Contrastive Learning.** The seminal work  started theoretical research on single-modal contrastive learning. They assumed that different positive samples are independently drawn from the same latent class, making a connection to supervised learning.  identified two key properties related to the contrastive loss: alignment and uniformity. Alongside,  illustrated that predicting auxiliary prediction tasks helps in learning representations effective for downstream prediction tasks, and  provided a theoretical analysis of contrastive learning in the multi-view setting. Besides,  proposed a theoretical framework to understand contrastive self-supervised learning from an optimization perspective.  proposed a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations.  pointed out the importance of inductive biases of the function class and training algorithm in understanding contrastive learning. The most related work to us is the work by . Similar to them, this work studies ReLU networks and considers the signal-noise data model. However, we do not require the adjustable bias term in the activation function, which plays a critical role in . Furthermore, this work adopts a unified framework to compare with multi-modal contrastive learning, which is out of scope in .

**Understanding of Multi-modal Contrastive Learning.** As the multi-modal contrastive learning approaches such as CLIP received great success, recent works have been proposing explanations from empirical perspective.  empirically showed that high train-test similarity is insufficient to explain CLIP's OOD performance.  illustrated that CLIP behaves similarly to Bags-of-words in language-based image retrieval, i.e., the order of words in the input sentence does not largely affect CLIP to find the corresponding image. Besides,  demonstrated that training data diversity and the ability to leverage the diversity as supervised learning is the key to the effective robustness of CLIP. Theoretically,  proved that multi-modal contrastive learning can block-identity latent factors shared between modalities by the a generative data model.  analyzed the training dynamics of a simple multi-modal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. Furthermore,  showed that each step of loss minimization by gradient descent can be seen as performing SVD on a contrastive cross-covariance matrix. Similar to us,  tried to answer why multi-modal learning is better than single model learning. However, they did not consider contrastive learning and thus cannot explain the success of multi-modal contrastive multi-modal learning.

**Data Quality Matters for Multi-modal Contrastive Learning.** Aligned with our theoretical results, there is a lot of empirical evidence showing that improving the alignment quality with more descriptive captions improves multi-modal contrastive learning.  show that the training distribution mostly determines the generalizability of CLIP. Furthermore, [47; 40; 18; 17] find filtering poorly aligned image-caption samples used for training leads to further improvements. Besides, [45; 39; 15] demonstrate that improving the descriptiveness of the captions could further boost the performance of CLIP. Besides,  demonstrated that the caused by a combination of model initialization and contrastive learning optimization. However, their results do not take neural network architecture into consideration, and do not provide an analysis of test errors either.

## 3 Problem Setting

Notation.We use bold-faced letters for vectors and matrices otherwise representing scalar. We use \(\|\|_{2}\) to denote the Euclidean norm of a vector or the spectral norm of a matrix, while denoting \(\|\|_{F}\) as the Frobenius norm of a matrix. For a neural network, we denote \(()\) as the activation function and we adopt ReLU activation where \((x)=\{0,x\}\) in this work. To simplify, we denote \([n]=\{1,2,,n\}\).

Data Model.In this work, we consider the following data model, which consists of signal and noise. In the first modality, example \((,y)\) is generated as follows:

\[=[^{(1)},^{(2)}]^{}=[y^{},^{}]^{}, y(\{-1,1\}).\] (1)

where \(^{2d}\) is the input feature and \(y\{-1,1\}\) is the corresponding label generated from Rademacher distribution. In particular, \(^{(1)}=y^{d}\) is the task-relevant signal vector, and \(^{(2)}=(,_{}^{2} )^{d}\) is the task-irrelevant noise vector. Intuitively, if a network learns primarily from signal, it can effectively generalize to unseen data and vice versa. Similar data models have been adopted in recent theoretical works on supervised learning [2; 26; 8; 23; 31; 63; 22; 11] and self-supervised learning [56; 50; 30].

Similarly for the second modality, a sample \((},y)}\) is generated as

\[}=[}^{(1)},}^{(2)}]^{}=[y^{},}^{ }]^{}, y(\{-1,1\}),\] (2)

where the input feature \(}^{2}\) and the label \(y\) is shared with the first modality. Besides, the signal is a given vector \(}^{}\), and noise follows \(}(,_{}^{2} )^{}\). The linear data models for multi-modal learning have also been studied in previous work . To simplify the analysis, we set \(d=\), \(_{}=_{}\). However, we highlight that extensions to deal with unmatched dimension and noise level is possible.

### Single-modal Contrastive Learning

We use a single-layer neural network \(:^{2d}^{m}\) with ReLU activation as our encoder, where \(m\) is the number of neurons, which represents the embedding dimension. More precisely,

\[()=[_{1}(),,_{m}()]^ {}^{m},\;_{r}()=h_{r}( ^{(1)})+h_{r}(^{(2)}),\] (3)

here we let \(h_{r}(^{(i)})=(_{r},^{(i)})\) for \(r[m]\), \(i\), and \(()\) is the ReLU activation function. We adopt a Gaussian to initialize the weights \(_{r}^{(0)}(,_{0}^{2})\), where \(_{0}\) severs as the strength.

Given a pair of positive data samples, the contrastive loss function is based on the similarity measure defined as the inner product between the representation of two samples \(,^{}^{2d}\):

\[_{}(,^{})=_{r= 1}^{m}h_{r}(^{(1)})(h_{r}(^{(1)}))+ _{r=1}^{m}h_{r}(^{(2)})(h_{r}(^ {(2)})),\] (4)

where the \(()\) is the stop-gradient operation, which is inspired by recent empirical works [19; 10] and theoretical work studying contrastive learning . Here we define positive sample as

\[}=[}^{(1)},}^{( 2)}]^{}=[y^{},^{}+ {}^{}]^{},\;(, _{}^{2}).\] (5)

In particular, we consider the form of augmentation where the signal stays invariant while the noise vector is corrupted with added independent noise. Similar setup has been considered in . We consider the contrastive loss presented as follows:

\[L=-_{i=1}^{n}(_{}(_{i},}_{i})/}}{e^{_{}(_{i},}_{i})/}+_{j i}^{M}e^{_{ }(_{i},_{j})/}}),\] (6)

where \(\) is the temperature parameter, \(n\) is the number of training samples, and \(M\) is the number of negative pairs. In this work, to efficiently optimize the loss to near zero, we require negative sample pairs do not share the same label, i.e., \(y_{j} y_{i}\) in (6). Note that this setting is aligned with supervised contrastive learning [29; 27].

We use gradient descent to optimize the contrastive learning loss, which leads to the gradient update:

\[_{r}^{(t+1)}=_{r}^{(t)}-_{_{r}}L(^{(t)})=_{r}^{(t)}+_{i=1}^{n}(1- _{i}^{(t)})h_{r}^{(t)}(}_{i}^{(1)})h^{(t)} (_{i}^{(1)})y_{i}\] \[+_{i=1}^{n}(1-_{i}^{(t)})h_{r}^{ (t)}(}_{i}^{(2)})h_{r}^{(t)}(_{i}^{(2)}) _{i}-_{i=1}^{n}_{j i}^{M}_ {i,j}^{(t)}h_{r}^{(t)}(_{j}^{(1)})h^{(t)}(_{ i}^{(1)})y_{i}\] \[-_{i=1}^{n}_{j i}^{M}_{i,j}^{ (t)}h_{r}^{(t)}(_{j}^{(2)})h_{r}^{(t)}(_{i}^{( 2)})_{i},\] (7)

where we denote \(_{r}^{(t)}()=(_{r}^{(t)},)\), \(\) as the learning rate, and we define the loss derivatives as

\[_{i}^{(t)}_{}(_{i},}_{i})/}}{e^{_{}(,}_{i})/}+_{j i}^{M}e^{_{}(_{i},_{j})/}},\;_{i,j}^{(t)} _{}(_{i},_{j})/}}{e^{ _{}(_{i},}_{i})/}+ _{j i}^{M}e^{_{}(_{i},_{j})/ }}.\] (8)

Intuitively, when the similarity between positive pair is high, and the similarity between negative time is low, we can see \(_{i}^{(t)} 1\) and \(_{i,j}^{(t)} 0\), for \(i[n]\) and \(j[M]\). Therefore, the gradient descent in Eq. (7) is close to zero, indicating the near convergence result. Furthermore, from Eq. (7), we observe that the evolution direction of weight is composed of signal vector \(\) and noise vectors \(_{i}\) for \(i[n]\). This observation plays a critical role in our following theoretical analysis.

### Multi-modal Contrastive Learning

We use two neural networks \(:^{d}^{m}\) and \(:^{}^{m}\) to encode two input modality \(\) and \(}\) respectively. Both neural networks use ReLU activation function. More precisely,

\[()=[_{1}(),,_{m}()]^ {}^{m},\;_{r}()=h_{r}( ^{(1)})+h_{r}(^{(2)})\]\[(})=[_{1}(}),, _{m}(})]^{}^{m},\ _{r}(})=g_{r}(}^{(1)})+g_{r}( }^{(2)})\]

we let \(h_{r}(^{(i)})=(_{r},^{(i)})\) and \(g_{r}(}^{(i)})=(}_{r},}^{(i)})\). Here \(()\) is the ReLU activation function, \(_{r}^{d}\) and \(}_{r}^{}\) for \(r[m]\) are the weights in two networks. Given the embedding, the similarity function of the two modalities is defined as

\[_{,}(,}) =_{r=1}^{m}h_{r}(^{(1)})(g_{r }(}^{(1)}))+_{r=1}^{m}h_{r}(^{( 2)})(g_{r}(}^{(2)})),\] \[_{,}(}, ) =_{r=1}^{m}g_{r}(}^{(1)}) (h_{r}(^{(1)}))+_{r=1}^{m}g_{r}( }^{(2)})(h_{r}(^{(2)})).\]

The two similarity functions defined above are modality-centered with stop-gradient operation applied. The objective function of contrastive multi-modal learning can be expressed as

\[L =-_{i=1}^{n}(_{,}(_{i},}_{i})/}}{e^{_{,}(_{i},}_{i})/ }+_{j i}^{M}e^{_{,}(_{i},}_{j})/}})\] \[-_{i=1}^{n}(_{,}(}_{i},_{i})/}}{e^{_{,}(}_{i},_{i})/ }+_{j i}^{M}e^{_{,}(}_{i},_{j})/}}).\] (9)

Same to the single-modal learning whose objective function is governed by Eq. (6), the objective function for multi-modal contrastive learning adopt one positive pair and \(M\) negative pairs. Besides, we require the negative pairs do not share the same label. To optimize the objective function (9) for multi-modal learning, gradient descent is applied to train two encoders simultaneously. The gradient descent rule for the first modal network is governed by the following expression.

\[_{r}^{(t+1)}=_{r}^{(t)}-_{_{r}}L(^{(t)})=_{r}^{(t)}+_{i=1}^{n}(1- _{i}^{(t)})g_{r}^{(t)}(}_{i}^{(1)})h_{r}^{ (t)}(^{(1)})y_{i}\] \[+_{i=1}^{n}(1-_{i}^{(t)})g_{r}^{ (t)}(}_{i}^{(2)})h_{r}^{(t)}(^{(2)}) _{i}-_{i=1}^{n}_{j i}^{M}_{ i,j}^{(t)}g_{r}^{(t)}(}_{j}^{(1)})h_{r}^{ (t)}(^{(1)})y_{i}\] \[-_{i=1}^{n}_{j i}^{M}_{i,j}^{ (t)}g_{r}^{(t)}(}_{j}^{(2)})h_{r}^{(t)}( ^{(2)})_{i}.\] (10)

Here with a slight abuse of notation, we use \(_{i}^{(t)},_{i,j}^{(t)}\) to represent the loss derivatives for both modalities. Compared to signal-modal learning, the main difference for the multi-modal learning is that the corresponding embedding is from another modality. The gradient update for the second modality can be derived similarly, which we omit here for clarity.

### Downstream Task Evaluation

To evaluate the out-of-distribution generalization of single-modal and multi-modal contrastive learning for downstream task, we consider a test distribution \(_{}\), where a sample \(_{}=[y^{}, ^{}]^{}_{}\) is generated as follows. The test signal \(\) satisfies \(,=O(\|\|_{2}^{2} d^{-1/2})\) and the test noise follows \((,_{}^{2})\) and \(y\) follows Rademacher distribution. After the training is complete, we introduce a linear head on top of the learned embedding \((_{})\) for adapting to test distribution, i.e., \(f(_{})=,(_{})\). Specifically, we consider the task of classification and define the population 0-1 test error as \(L_{_{}}=_{_{} _{}}yf(_{})<0\).

## 4 Main Results

In this section, we introduce our key theoretical findings that elucidate the optimization and generalization result for both single-modal and multi-modal contrastive learning through the feature learning analysis. We use a trajectory-based analysis for the iterations induced by gradient descent, following a post-training analysis for the performance on the downstream test set. Below we provide the main assumption and main theorems.

**Assumption 4.1**.: Let \(=\|\|_{2}/(_{})\). Assume (1) \(d(\{n^{2},n_{0}^{-1}_{}^{-1},_{0}^ {-2}\|\|_{2}^{-2}\})\). (2) \( O(\{m\|\|_{2}^{-2},nm_{}^{-2}d^{-1}\})\). (3) \(_{0}((\{_{},\|\|_{2} \})^{-1})\). (4) \(m,n(1)\).

(5) \(_{}\{(\|\|_{2}),_{}/ (1)\}\). (6) \(n^{2}=(1)\). (7) \(C_{}\|\|_{2}=\|}\|_{2}\), where \(C_{} 2.66\) is a constant.

(1) We adopt a high dimensional setting to ensure enough over-parameterization. (2,3) The learning rate and the strength of initialization are chosen to make sure the that gradient descent can effectively minimize the contrastive loss. (4) The choice of hidden size \(m\) and number of training sample \(n\) is to provide adequate concentration. (5) The strength of augmentation is set to keep the similarity between two positive samples. (6) The relation between number of sample and \(\) is to distinguish the feature learning process between single-modal and multi-modal contrastive learning. (7) To differentiate single-modal and multi-modal contrastive learning, we introduce a constant \(C_{}\), which enables the cooperation between the two modalities in multi-modal contrastive learning.

**Theorem 4.2** (Single-Modal Contrastive Learning).: _Under the single-modal learning setup, suppose Assumption 4.1 holds. Then after \(T^{*}=(^{-1}mn_{}^{-2}d^{-1}+^{-1}mn _{}^{-2}d^{-1}^{-1})\), the with probability at least \(1-1/d\), it holds that (1) Training error \(L(T^{*})\) and (2) Test error at down-stream task \(L_{_{}}(T^{*})=(1)\)._

Theorem 4.2 states that despite the small training error achieved by single-modal contrastive learning, the test error is large in the downstream task.

**Theorem 4.3** (Multi-Modal Contrastive Learning).: _Under the single-modal learning setup, suppose Assumption 4.1 holds. Then after \(T^{*}=(^{-1}mn_{}^{-2}d^{-1}+^{-1}mn _{}^{-2}d^{-1}^{-1})\), the with probability at least \(1-1/d\), it holds that (1) Training error \(L(T^{*})\) and (2) Test error at down-stream task \(L_{_{}}(T^{*})=o(1)\)._

Theorem 4.3 demonstrates that trained multi-modal contrastive learning can achieve both small training error and downstream test error. Compared to Theorem 4.2, Theorem 4.3 shows that the generalization of multi-modal contrastive learning in downstream tasks is better than single-modal contrastive learning. The reason behind this difference is that the two modalities can cooperate with each other; the higher quality in one modality can boost the feature learning in the target modality, helping to generalize to the downstream task. On the contrary, augmentation often maintains the same SNR as the original data, so single-modal learning hardly benefits from the augmentation and can only memorize the noise from the data, which is not applicable to downstream tasks.

## 5 Proof Roadmap

### Proof Sketch for Single Modal Contrastive Learning

The proof is constructed by a optimization analysis followed by a generlization analysis in the downstream task. Through the application of the gradient descent rule outlined in Eq. (7), we observe that the gradient descent iterate \(_{r}^{(t)}\) is a linear combination of its random initialization \(_{r}^{(0)}\), the signal vector \(\) and the noise vectors in the training data \(_{i}\) for \(i[n]\). Consequently, for \(r[m]\), the decomposition of weight vector iteration can be expressed:

\[_{r}^{(t)}=_{r}^{(0)}+_{r}^{(t)}\|\|_{2}^{- 2}+_{i=1}^{n}_{r,i}^{(t)}\|_{i}\|_{2}^{-2}_{i},\] (11)

where \(_{r}^{(t)}\) and \(_{r,i}^{(t)}\) serve as coefficients and represent signal learning and noise memorization respectively. Based on the the gradient descent update (7), the iteration of \(_{r}^{(t)}\) and \(_{r,i}^{(t)}\) are given:

**Lemma 5.1** (Single-modal Contrastive Learning).: _The coefficients \(_{r}^{(t)}_{r,i}^{(t)}\) in decomposition (11) satisfy the following equations:_

\[_{r}^{(t+1)}=_{r}^{(t)}+_{i=1}^{n}( 1-_{i}^{(t)})h_{r}^{(t)}(}_{i}^{(1)})-_{j  i}^{M}_{i,j}^{(t)}h_{r}^{(t)}(_{j}^{(1)})h_{r} ^{(t)}(_{i}^{(1)})y_{i}\|\|_{2}^{2},\] (12)

\[_{r,i}^{(t+1)}=_{r,i}^{(t)}+(1-_{i}^{ (t)})h_{r}(}_{i}^{(2)})-_{j i}^{M}_{i,j} ^{(t)}h_{r}(_{j}^{(2)})h_{r}^{(t)}(_{i} ^{(2)})\|_{i}\|_{2}^{2},\] (13)

_where the initialization \(_{r}^{(0)},_{r,i}^{(0)}=0\)._Lemma 5.1 tells how the coefficients evolve under gradient descent update. In the following, we introduce a two-stage dynamics to characterize the whole training process based on Eq 12 and Eq 13.

**First Stage: Exponential growth.** During the first stage, we show before \(_{r}^{(t)}\) or \(_{r,i}^{(t)}\) grow to \((1)\), the embedding (3) is close to zero, suggesting the similarity is bounded by \(1_{}(,^{}) C_{}\) for some constant \(C_{}>1\). The loss derivatives defined in (8) can thus be bounded within some constant range.

_Signal learning._ According to the update for signal learning in (12), we see the propagation can be simplified based on the hard-negative sampling strategy, i.e., the negative pairs do not share the same labels. This suggests the negative term is always zero as \(_{i=1}^{n}_{j:y_{j} y_{i}}^{M}(_{r}^{(t)},y _{j})^{}(_{r}^{(t)},y_{i} )=0\). The resulting update of \(_{r}^{(t)}\) reduces to \(_{r}^{(t+1)}=_{r}^{(t)}+_{i=1}^{n}(1-_ {i}^{(t)})(_{r}^{(t)},y_{i} )^{}(_{r}^{(t)},y_{i} )y_{i}\|\|_{2}^{2}\). Examining the propagation of \(_{r}^{(t)}\), we can divide the dynamics into two groups depending on the sign of weight initialization \(_{r}^{(0)},\). Let \(_{+}^{(t)}\{r:_{r}^{(t)},>0\}\) and \(_{-}^{(t)}\{r:_{r}^{(t)},<0\}\). Then for \(r_{+}^{(0)}\), we can show \(_{r}^{(t)} 0\) increases exponentially and thus the sign of inner produce stays invariant with \(_{+}^{(t)}=_{+}^{(0)}\) for all \(t 0\). On the other hand, for \(r_{-}^{(0)}\), we can show \(_{r}^{(t)} 0\) and decreases exponentially with \(_{-}^{(t)}=_{-}^{(0)}\) for all \(t 0\).

_Noise memorization._ Compared to signal learning, the behaviour of noise memorization requires more detailed analysis. This is mainly because the negative pairs can not be eliminated simply based on label difference, as the noise patch \(_{i}\) is generated independent of label \(y_{i}\). In addition, the added noise \(_{i}\) by augmentation can also contribute to the noise dynamics. We first show when the noise level \(_{}\) is much smaller compared to \(_{}\), the dynamics of noise memorization is largely remains unaffected. By the sign of \(_{r}^{(t)},_{i}\), we partition the samples into two sets, i.e., \(_{r,+}^{(t)}=\{i:_{r}^{(t)},_{i} >0\}\), and \(_{r,-}^{(t)}=\{i:_{r}^{(t)},_{i} <0\}\). We can verify for \(i_{r,-}^{(0)}\), the value of \(_{r,i}^{(t)}\) stays at zero based on the update (13) with an induction argument. For samples \(i_{r,+}^{(0)}\) with positive initialization, we analyze the noise memorization based on the joint dynamics of samples with the same label. In particular, we define total noise memorization of positive and negative samples respectively as \(B_{r,+}^{(t)}_{i:y_{i}=1}(_{r,i}^{(t)}+_{r} ^{(0)},_{i})_{i_{r,+}^{(t)}}\) and \(B_{r,-}^{(t)}_{i:y_{i}=-1}(_{r,i}^{(t)}+_{r }^{(0)},_{i})_{i_{r,+}}^{(t)}\). The update of \(_{r,i}^{(t)}\) in (13) then implies the dynamics of \(B_{r,+}^{(t)}\) and \(B_{r,-}^{(t)}\) as follows

\[B_{r,+}^{(t+1)} B_{r,+}^{(t)}+^{2}d}{nm} B_{r,+}^{(t)}-B_{r,-}^{(t)}, B_{r,-}^{(t+1)}  B_{r,-}^{(t)}+^{2}d}{nm}B_{r,-}^{(t )}-B_{r,+}^{(t)},\]

where the coefficient of \(1/2\) appears as a result of the randomness of the sign of initialization. This result suggests, individual \(_{r,i:y_{i}=1}^{(t)}\) cannot grow too slow compared to the \(_{r,i:y_{i}=-1}^{(t)}\). Following a similar induction argument, we are able to show \(_{r,i:y_{i}=1}^{(t)}\) has an exponential growth lower bound. On the other hand, for samples with \(y_{i}=-1\) but with different neuron, we can use the same strategy to show an exponential growth lower bound for some neurons that satisfy the initialization conditions.

**Lemma 5.2**.: _Under the Assumption 4.1, let \(T_{1}=20/(_{0}_{})/1+0.96 ^{2}d}{nm}\), we have \(_{r}^{(t)}=(1/)\) for all \(r[m]\) and \(0 t T_{1}\) and \(_{r}_{r,i}^{(T_{1})}=(1)\) for all \(i[n]\)._

**Second stage: convergence and scale difference.** At the end of first stage, the noise grows to a constant order while signal learning remains negligible. As a result, the loss derivatives are no longer bounded within some constant range. In the second stage, we aim to show the loss is able to converge to an arbitrarily small value \(\). Despite the unsupervised learning setup, we are still able to show loss convergence thanks to the hard negative samples. Let \(F_{0}(,_{i})=(_{i},}_{i})\) be the similarity to the argumentation and \(F_{j}(,_{i})=(_{i},_{j})\) for \(j=1,...,M\) be the similarity between the negative pairs. Then we can show there exists some \(^{*}\) such that \( F_{0}(^{(t)},_{i}),^{*} 2 (2M/)\) while \( F_{j}(^{(t)},_{i}),^{*} (2M/)\) for all \(j=1,...,M\). Then we can bound \( L_{S}(^{(t)}),^{(t)}-^{*} _{i=1}^{n}L_{i}(^{(t)})-/2\). This as a result allows to show a monotonic decrease in the loss function as \(L(^{(t)})(\|^{(t)}-^{*}\|_{F}^{2} -\|^{(t+1)}-^{*}\|_{F}^{2})+\) which guarantees convergence by telescoping over the inequality. Upon the convergence, we can also show 

[MISSING_PAGE_FAIL:8]

4. For \(i^{(0)}_{r,+}}^{(0)}_{r,+}\), without loss of generality, we consider upper bounding noise memorization for the first modality and \(y_{i}=1\). To this end, we first define the individual and joint noise memorization for the first modality as \(^{(t)}_{r,i}^{(t)}_{r,i}+^{(0)}_{r},_{i}\), and \(B^{(t)}_{r,+,+}_{i:y_{i}=1}(^{(t)}_{r,i}+^{ (0)}_{r},_{i})_{i^{(t)}_{r,+} }^{(t)}_{r,+}},B^{(t)}_{r,+,-}_{i:y_{i}=1}( ^{(t)}_{r,i}+^{(0)}_{r},_{i}))_ {i^{(t)}_{r,+}}^{(t)}_{r,-}}\). Similar definitions exist for the other modality. The joint dynamics of noise memorization can be first upper bounded by the other modality as \(^{(t)}_{r,i}}{M+1}(B^{(t)}_{r,+,+}+B^{ (t)}_{r,+,-})\). Then we can upper bound the individual noise memorization by \[^{(t)}_{r,i}(1+^{2}d}{nm})^{t}(^{(0) }_{r,i}+^{(0)}_{r,i}).\]

We show the combined dynamics of \(^{(t)}_{r}\) and \(^{(t)}_{r}\) exhibits exponential growth while the magnitude of their difference shrinks exponentially. The results are summarized as follows

**Lemma 5.4**.: _Under the Assumption 4.1, let \(T_{1}=(20/(_{0}\|\|_{2}))/(1+0.48C_{} \|_{2}^{2}}{m})\), we have \(^{(T_{1})}_{r,i}=(1/)\) for all \(r[m]\), \(i[n]\), and \(0 t T_{1}\) and \(_{r}^{(T_{1})}_{r}=(1)\)._

**Second Stage: Convergence and scale difference.** The second stage presents similar patterns compared to single-modal learning. Thanks to the correlation between the two modality during gradient descent training, the two neural network converge at the same time, minimizing the training loss. Besides, The scale difference at the end of the first stage is carried over throughout the second stage until convergence. Therefore, it allows to show a monotonic decrease in the loss function as \(L(^{(t)},}^{(t)})\|^{(t)}- ^{*}\|_{F}^{2}+\|}^{(t)}- }^{*}\|_{F}^{2}-\|^{(t+1)}-^{*}\|_{F}^{2}-\|}^{(t+1)}-}^{*}\|_{F}^{2}+2\), which guarantees convergence by telescoping over the inequality. At the same time, until convergence, we can show the scale difference obtained at the end of the first stage is maintained, namely \(_{r,i}^{(t)}_{r,i}=(1/)\) and \(_{r}^{(t)}_{r}=(1)\). This suggests the signal learning dominates the noise memorization and thus the resulting embeddings are linearly separable, which guarantees a small test error for downstream tasks. The formal convergence result is established in Lemma D.15. Combined with the generalization error demonstrated in Appendix D.3, this completes the proof of Theorem 4.3.

## 6 Experiments

Synthetic experimentsWe conduct synthetic experiments to verify the theoretical results obtained in the previous sections. We generate samples following the theoretical setups, where we set the data dimension \(d=2000\), number of training samples \(n=100\), number of test samples \(n_{}=200\), and the hidden size of all encoders as \(m=50\). We adopt gradient descent with a learning rate of \(0.01\) as the optimizer to train the model by \(200\) epochs. In the single-modal setting, the \(\) is set to be \([5,0,...,0]^{T}\) and the \((,)\) for the in-distribution data, and the augmentation vector \((,0.01*)\). For the multi-modal setting, \(}=[0,15,0,...,0]^{T}\). In addition, for the OOD test data \(_{}=[^{},^{}]^{}\), we set \(=[2,0,...,0]\) and \((,)\). We perform logistic regression based on the learned features \((_{})\) and apply the learned classifier head to evaluate OOD generalization error in terms of prediction accuracy.

**Results.** In Figure 1, we see the training loss of both single-modal and multi-modal learning converges rapidly. At the same time, OOD test accuracy of multi-modal learning converges to nearly 1.0 while

Figure 1: Training loss, test accuracy, signal learning and noise memorization of single-modal and multi-modal contrastive learning.

that of single-modal learning stagnates around 0.5. This is primarily because under the setup where the other modality \(}\) has a higher SNR, signal learning of \(\) is lifted. This can be verified from the third plot of Figure 1, where the signal learning of multi-modal framework is significantly higher than single-modal. Further, it can be observed that single-modal contrastive learning exhibits more severe noise memorization, which suppresses signal learning. In contrast, multi-modal contrastive learning exhibits less severe noise memorization which would further encourage signal learning. These phenomena again support and align with our theoretical results.

Real-world experimentsWe now extend the comparison of single-modal and multi-modal learning to realistic image data, ColoredMNIST , which is a typical benchmark studying the generalization capability under distribution shifts. The ColoredMNIST dataset is a variation of the standard MNIST dataset, where each digit is assigned a specific color based on its label. The two modalities are image, and text that describes the images. The task is a 10-class classification that recognizes the number of the colored MNIST images. Specifically, we have 10 colors to color 10 digits, and introduce spurious correlations via label noises following the literature:

* For the _training_ set, 10% of labels will be clipped to a random class. For images with class '0' (or '1'), they will be colored as red (or green) with a probability of 77.5%, and as another random color with a probability of 22.5%. The coloring scheme introduces a spurious correlation.
* For the _test_ set, 10% of labels will be clipped to a random class. For images with class '0' (or '1'), they will be colored as green (or red) with a probability of 77.5%, and as another random color with a probability of 22.5%. The coloring scheme can be considered as reversing the training spurious correlations. Therefore, the evaluation on test set can reflect to what extent the model learns to use the spurious features, i.e., colors, to classify images.

We implement the multi-modal learning following the practice in , where we consider an ideal language encoder that successfully encodes the caption of the images into one-hot labels of colors and digits. For single-modal learning, we follow the implementation of the SimCLR  to construct a set of augmentations to learn the representations.

**Results.** Under the distribution shift, we verify that multi-modal learning archives an out-of-distribution test accuracy of 82.13%, which outperforms that of single-modal learning 12.68%. As a result, we can claim that the effective SNR of invariant features (the shape of the digit) will be degraded under the impact of the injected color. Therefore, the performance of single-modal may be suboptimal as it cannot effectively utilize the information of the digit's shape. On the other hand, multi-modal demonstrates a better capacity for handling this scenario.

## 7 Conclusions

In this work, we have established a comprehensive comparison of the optimization differences during the pre-training stage and the generalization gap between single-modal and multi-modal contrastive learning for downstream tasks. With the cooperation between modalities, multi-modal contrastive learning can achieve better feature learning and generalization on downstream tasks compared to single-modal learning. On the other hand, data augmentation alone can hardly improve data quality and thus cannot boost the performance of single-modal contrastive learning. Together, these results quantitatively demonstrate the superiority of multi-modal learning over single-modal learning and emphasize the importance of data quality in multi-modal contrastive learning.