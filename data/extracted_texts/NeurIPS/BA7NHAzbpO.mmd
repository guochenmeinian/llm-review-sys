# Sampling from Structured Log-Concave Distributions via a Soft-Threshold Dikin Walk

Oren Mangoubi

Worcester Polytechnic Institute

&Nisheeth K. Vishnoi

Yale University

###### Abstract

Given a Lipschitz or smooth convex function \(f:K^{d}\) for a bounded polytope \(K:=\{^{d}:A b\}\), where \(A^{m d}\) and \(b^{m}\), we consider the problem of sampling from the log-concave distribution \(() e^{-f()}\) constrained to \(K\). Interest in this problem derives from its applications to Bayesian inference and differential privacy. We present a generalization of the Dikin walk to this setting that requires at most \(O((md+dL^{2}R^{2}) md^{-1}())\) arithmetic operations to sample from \(\) within error \(>0\) in the total variation distance from a \(w\)-warm start. Here \(L\) is the Lipschitz constant of \(f\), \(K\) is contained in a ball of radius \(R\) and contains a ball of smaller radius \(r\), and \( 2.37\) is the matrix-multiplication constant. This improves on the running time of prior works for a range of structured settings important for the aforementioned inference and privacy applications. Technically, we depart from previous Dikin walks by adding a soft-threshold regularizer derived from the Lipschitz or smoothness properties of \(f\) to a barrier function for \(K\) that allows our version of the Dikin walk to propose updates that have a high Metropolis acceptance ratio for \(f\), while at the same time remaining inside the polytope \(K\).

## 1 Introduction

We consider the problem of sampling from a log-concave distribution supported on a polytope: Given a polytope \(K:=\{^{d}:A b\}\), where \(A^{m d}\) and \(b^{m}\), and a convex function \(f:K\), output a sample \( K\) from the distribution \(() e^{-f()}\). Our interest in this problem derives from its applications to Bayesian inference and differentially private optimization. In Bayesian inference, the ability to sample from \(() e^{-f()}\) allows one to compute Bayesian confidence intervals and other statistics for the Bayesian posterior distribution of many machine learning models (see e.g. ). In differentially private optimization, sampling from the "exponential mechanism"  allows one to get optimal utility bounds for the problem of minimizing \(f\) under \(\)-differential privacy .

The instances of the polytope-constrained sampling problem that arise in these applications are more general than the two well-studied special cases- the uniform density case (\(f 0\)) and the unconstrained case (\(K=^{d}\)). However, they still have more _structure_ than the case of a general log-concave function supported on an arbitrary convex body. For instance, in Bayesian Lasso logistic regression, \(f()=_{i=1}^{n}(;x_{i})\), where \(\) is the logistic loss and \(x_{i}\) are datapoints with \(\|x_{i}\|_{2} 1\), and \(K=\{^{d}:\|\|_{1} O(1)\}\); see . Since the logistic function is both \(O(1)\)-smooth and \(O(1)\)-Lipschitz, \(f\) is both \(O(n)\)-Lipschitz and \(O(n)\)-smooth, and \(K\) is defined by \(O(d)\) inequalities and contained in a ball of radius \(O(1)\).

To obtain an \(\)-differentially private mechanism for the Lasso logistic regression problem, using the exponential mechanism, the goal is to sample from \((-_{i=1}^{n}(;x_{i}))\), where \(\) is the logistic loss and \(K\) is contained in a ball of radius \(R\). Thus, the log-density is both\(\)-smooth and \(L\)-Lipschitz for \(=L=}{{f}}=O(d)\) if \(n=d\) and \(<1\), since \(R=O(1)\). Another example is a result of  that reduces the problem of \(\)-differentially private low-rank approximation of a symmetric \(p p\) matrix to a constrained sampling problem where \(f\) is linear (and thus \(0\)-smooth) of dimension \(d=p^{2}\) and \(K\) is the Gelfand-Tsetlin polytope (a generalization of the probability simplex). Here \(K\) has \(d\) inequalities and diameter \(O()\).

Importantly, when sampling from the exponential mechanism in privacy applications, sampling with total variation (TV) bounds-the case that has received the most attention-is insufficient to guarantee \(\)-differential privacy, the strongest notion of differential privacy; see . Instead, one requires bounds in the stronger infinity-distance metric \(_{}(,):=_{ K}|(} {{()}})|\). A recent work  showed how to convert samples within \(O()\)-TV distance from continuous log-Lipschitz densities \(\), into samples with \(O()\)-infinity-distance bounds, but it requires the TV distance \(\) to be _very small_-roughly \(=O( e^{-d-LR})\), raising the question of designing Markov chains whose runtime bounds also have a low-order dependence on \(}{{}}\). Thus, for the aforementioned applications to Bayesian inference and privacy which give rise to structured instances of sampling from log-concave densities over polytopes, it is desirable to design sampling algorithms that have a low-order polynomial dependence not only on the parameters \(d,L,R,\), but also on \(}{{}}\).

Main related works.A line of work has developed algorithms for sampling in the general setting when \(K\) is an arbitrary convex body given by a membership oracle .  use the "hit-and-run" framework to give an algorithm to sample from a log-concave distribution \( e^{-f}\) on a convex body \(K\) which also contains a ball of radius \(r\) with TV-error \(>0\) in \(O(d^{2}(}{{r}})^{2}^{2}(d}{{r}}} {{()}})^{3}(}{{}}))(T_{f}+_{K}))\) arithmetic operations from a \(w\)-warm start (their Theorem 1.1) and \(O(d^{3}(}{{r}})^{2})^{5}(}}{{( r)}} (T_{f}+_{K}))\) arithmetic operations from a cold start (Corollary 1.2). Here \(T_{f}\) is the time required to evaluate \(f\) and \(_{K}\) is the time for a membership oracle query. Here, a distribution \(\) is \(w\)-warm for \(w 1\) w.r.t. the stationary distribution \(\) if \(_{z K}}{{(z)}} w\).

 give a "Dikin-walk"-based algorithm to sample from any log-concave \( e^{-f}\) on \(K\) where \(f\) is \(L\)-Lipschitz or \(\)-smooth. The Dikin walk Markov chain was introduced in  in the special case where \(f 0\) (see also ). Their runtime is \(O((d^{5}+d^{3}L^{2}R^{2})(}{{}})(T_{f}+md^{-1}))\) arithmetic operations, where \(=2.37\) is the matrix-multiplication constant. From a cold start, their runtime is \(O((d^{5}+d^{3}L^{2}R^{2})(}{{}})\)\((d(}{{r}})+M+(}{{}}))(T_{f}+md^{ -1})\), where \(M:=(_{ K}e^{f()-f()})\). Their bounds when \(f\) is \(\)-smooth are the same, but with each \(L^{2}\) term replaced with \(\). We discuss additional related work in Appendix A.

Our contributions.We present a Markov chain sampling algorithm that generates samples from an \(L\)-log-Lipschitz or \(\)-log-smooth log-concave distribution \( e^{-f}\) on an \(R\)-bounded polytope \(K\) given by \(m\) inequalities, with an error bounded in the TV distance (Algorithm 1 and Theorem 2.1). Our algorithm requires \(O((md+dL^{2}R^{2})(}{{}}))(T_{f}+md^{-1})\) arithmetic operations to sample with TV error \(O()\) from \( e^{-f}\) when \(f\) is \(L\)-Lipschitz, and \(O((md+d R^{2})(}{{}}))(T_{f}+md^{-1})\) arithmetic operations in the setting where \(f\) is \(\)-smooth, where \(T_{f}\) is the number of arithmetic operations to compute the value of \(f\) and \(md^{-1}\) is the number of arithmetic operations to compute the Hessian of the log-barrier of \(K\).

In comparison to , we improve dependence of the running time on the parameters \(}{{r}}\) and \(}{{}}\). In comparison to , we improve the dependence on \(d\) while retaining the same dependence on \(}{{}}\). Our result directly implies faster runtimes with improved dependence on the dimension \(d\) for structured inference problems such as Bayesian Lasso logistic regression where e.g. \(}{{r}}=()\) (Corollary B.1). Moreover, plugging our algorithm into the TV-to-infinity distance bound converter of , we obtain an algorithm to sample from a logconcave density constrained to a polytope with error bounded by infinity distance that improves upon prior work of ; see Corollary C.1 and the subsequent discussion. Corollary C.1, along with the exponential mechanism , allows us to obtain faster runtime bounds with improveddependence on \(d\) for applications to differentially private empirical risk minimization  and matrix approximation ; see Corollary D.1 and the subsequent discussion.

Technically, our algorithm is a Markov chain inspired by the Dikin walk , whose steps are determined by a barrier function that generalizes the log-barrier function by adding a "soft-threshold" \(_{2}\)-norm regularizer. The regularized barrier allows our Markov chain to take larger steps, while still retaining a high acceptance probability on Lipschitz or smooth log-densities \(f\) -allowing our Markov chain to sample from these distributions with a faster runtime. A key technical step in obtaining our results is to show that our self-concordant barrier function is the limit of an infinite sequence of log-barrier functions for \(K\) "padded" with additional redundant inequalities. This allows us to leverage well-known properties of the log-barrier to bound the acceptance probability and mixing time of our Markov chain.

While \(_{2}\) regularization is optimal for classes of functions \(f\) which are Lipschitz or smooth in the \(_{2}\)-norm, for other classes of functions (e.g., Lipschitz in the \(_{1}\)-norm), \(_{2}\) regularization may not be optimal. Moreover, it remains open to obtain runtime bounds for the Dikin walk that do not require \(f\) to be Lipschitz or smooth, and/or depend polynomially on \( R\). This leads to the question of whether one can design other tractable self-concordant barriers to obtain further runtime improvements for sampling log-concave distributions on a polytope; we discuss this in Appendix F.

## 2 Results

Our main result (Theorem 2.1 and Algorithm 1) is a Markov chain algorithm that generates TV-error bounded samples from \(L\)-log-Lipschitz or \(\)-log-smooth log-concave distributions on a polytope. As explained later in this section, Theorem 2.1 often results in the fastest known algorithm for some of the applications to Bayesian inference and differentially private optimization mentioned in the introduction.

Notation.In the following, \(f\) is \(L\)-Lipschitz or \(\)-smooth for some \(L,>0\). \(T_{f}\) denotes the number of arithmetic operations to evaluate \(f\), \(T_{K}\) is the number of arithmetic operations to compute the Hessian of the log-barrier function, \(_{K}\) the operations for a membership oracle query, and \(_{K}\) the operations to compute a projection oracle for \(K\). When \(K\) is a polytope \(K=\{^{d}:A b\}\) given by \(A^{m d}\) and \(b^{m}\), one has \(T_{K}=O(md^{-1})\), \(_{K}=O(md)\), and \(_{K}=O(md^{-1})\). For every \(j\{1,,m\}\), we denote the \(j\)'th row of \(A\) by \(a_{j}\) and the \(j\)'th entry of \(b\) by \(b_{j}\). When comparing runtimes, we often assume for simplicity that \(T_{f}=(d^{2})\) unless otherwise stated, which is the case, e.g., in logistic regression with \(n=(d)\) datapoints. For any two distributions \(,\) on \(^{d}\), we denote their total variation distance by \(\|-\|_{}:=_{S^{d}}|(S)-(S)|\). For \(^{d}\), \(t>0\), denote the ball of radius \(t\) at \(\) by \(B(,t):=\{z^{d}:\|z-\|_{2} t\}\) where \(\|\|_{2}\) is the Euclidean norm. Denote the interior of any \(S^{d}\) by \((S):=\{ S:B(,t) St>0\}\). \(>0\) is a step-size hyperparameter shared by our algorithm and the original Dikin walk of , \(>0\) is the hyperparameter in , and \(>0\) a hyperparameter for our algorithm's regularizer.

**Theorem 2.1** (Sampling with TV bounds via a soft-threshold Dikin Walk): _There exists an algorithm (Algorithm 1) which, given \(,R>0\) and either \(L>0\) or \(>0\), \(A^{m d}\), \(b^{m}\) that define a polytope \(K:=\{^{d}:A b\}\) such that \(K\) is contained in a ball of radius \(R\) and has nonempty interior, an oracle for the value of a convex function \(f:K^{d}\), where \(f\) is either \(L\)-Lipschitz or \(\)-smooth, and an initial point sampled from a distribution supported on \(K\) which is \(w\)-warm with respect to \( e^{-f}\) for some \(w>0\), outputs a point from a distribution \(\) where \(\|-\|_{}\). Moreover, this algorithm takes \(O((md+dL^{2}R^{2})(}{{}}))(T_{f}+T_{K})\) arithmetic operations in the setting where \(f\) is \(L\)-Lipschitz, or \(O((md+d R^{2})(}{{}}))(T_{f}+T_{K})\) arithmetic operations when \(f\) is \(\)-smooth, where \(T_{f}\) is the number of operations to evaluate \(f\) and \(T_{K}=O(md^{-1})\)._

Theorem 2.1 improves on the previous bound of (; Theorem 1.1) of \(O(d^{2}(}{{r}})^{2}^{2}(}{{(r)}})^{3 }(}{{}}))(T_{f}+_{K})\) arithmetic operations, for sampling from \( e^{-f}\)

[MISSING_PAGE_FAIL:4]

and \(d^{3}\) if we also have that \(LR=O()\). When \(f\) is instead \(\)-smooth their bound is \(O((d^{5}+d^{3} R^{2})(}{{}}))(T_{f}+T_{k})\), and our improvement on this bound is \(d^{2}\) arithmetic operations if \(m=O(d)\), and \(d^{3}\) if we also have that \( R^{2}=O(d)\).

We also note that while many works, e.g. , give faster bounds for the Dikin walk and its variants than the bounds in , these only apply in the special case when \(\) is the uniform distribution on \(K\). The proof of Theorem 2.1 appears in Appendix E. We give an overview of the main ideas in the proof of Theorem 2.1 in Section 3. In Appendix F, we give an axiomatic approach to arrive at our barrier function and discuss possible extensions.

Infinity-distance sampling.In applications of sampling to differentially private optimization , bounds in the total variation (TV) distance are insufficient to guarantee "pure" \(\)-differential privacy, and one instead requires bounds in the infinity-distance \(_{}(,):=_{ K}|(}{{()}})|;\) see e.g. .  give an algorithm that converts samples from TV bounds to those bounded in infinity-distance. Namely, given \(>0\) and a sample from a distribution \(\) within TV distance \( O(^{(R(d(}{{}})+ LR)^{2}/)-d^{e}-LR})\) of \(\), this post-processing algorithm outputs a sample within infinity-distance \(O()\) from \(\). Plugging the TV bounds from our Theorem 2.1 into their Theorem 2.2 gives a faster algorithm to sample from a log-concave and log-Lipschitz (or log-smooth) distribution constrained to a polytope \(K\), with \(O()\) error in \(_{}\) (Corollary C.1 in Appendix C). In particular, Corollary C.1 gives a bound of \(O((md+dL^{2}R^{2})[LR+d(}}{{ }})])(T_{f}+T_{K})\) arithmetic operations to sample within \(O()\) error in \(_{}\) from a log-concave distribution \( e^{-f}\) constrained to a polytope \(K\) from a cold start when \(f\) is L-Lipschitz (when \(f\) is also \(\)-smooth, the bound is \(O((md+d R^{2})[LR+d(}}{{ }})])(T_{f}+T_{K})\)). This improves on the bound of \(O((m^{2}d^{3}+m^{2}dL^{2}R^{2})[LR+d(}}{{}})])(T_{f}+T_{K})\) arithmetic operations in Theorem 2.1 of  by a factor of \(d^{3}\). Moreover, it further improves on the bound of \(O((md^{9}+md^{5}L^{4}R^{4})(}{{^{2}}})(}{{}},}{{r}},R,L,d))(T_{f}+T_{K})\) operations implied by . Corollary C.1 improves on this bound by a factor of \(-}}}{{}}\) when, e.g, each function evaluation takes \(T_{f}=O(d^{2})\) operations and \(m=O(d)\) as may be the case in privacy applications.

Differentially private optimization.A randomized mechanism \(h:^{n}\) is \(\)-differentially private (\(\)-DP) if for any datasets \(x,x^{}\) which differ by a single datapoint, and any \(S\), we have \((h(x) S) e^{}(h(x^{}) S)\); see . \(\)-differential privacy is the strongest notion of differential privacy, holds several advantages over weaker notions of differential privacy (see Remark 2.2), and has been widely studied in the literature .

In the application of the exponential mechanism to \(\)-DP low-rank approximation of a \(p p\) symmetric matrix \(M\) (see also ), one wishes to sample within infinity distance \(O()\) from a log-linear distribution \( e^{-f}\) on the Gelfand-Tsetlin polytope \(K^{d}\) (which generalizes the probability simplex), where \(d=p^{2}\), and where \(K\) has \(m=d\) inequalities with diameter \(R=O()\). In this application, the log-linear density \(f\) is (trivially) \(0\)-smooth and \(d^{2}_{1}\)-Lipschitz, where \(_{1}:=\|M\|_{2}\) is the spectral norm of \(M\). Thus, when applied to the mechanism of , our algorithm takes \(d^{4.5+}_{1}(}{{}})\) arithmetic operations. This improves by a factor of \(d^{3}\) on the bound of \(O(d^{7.5+}_{1})\) arithmetic operations implied by , and improves by a factor of \(_{1}^{3}}}{{^{2}}}\) on the bound of \(O(_{1}^{4}}}{{^{2}}})\) implied by .

Consider the problem of finding an (approximate) minimum \(\) of an empirical risk function \(f:K^{n}\) under the constraint that \(\) is \(\)-differentially private, where \(f(,x):=_{i=1}^{n}_{i}(,x_{i})\). We assume that the \(_{i}(,x)\) are \(\)-Lipschitz for all \(x^{n}\), \(i\), for some given \(>0\). In this setting,  show the minimum ERM utility bound under the constraint that \(\) is pure \(\)-DP, \(_{}[f(,x)]-_{ K}f(,x)= (}{{}})\), is achieved if one samples \(\) from the exponential mechanism \((-f)\) with infinity-distance error at most \(O()\). Plugging Corollary C.1 into the exponential mechanism, we obtain a faster algorithm for a pure \(\)-DP mechanism which achieves the minimum expected risk (Corollary D.1). Specifically, the runtime bound implied by Corollary D.1 is \(O((md+dn^{2}^{2})( n+d()}}{{r_{f}+T_{K}}}))\) arithmetic operations if each \(_{i}\) is \(\)-Lipschitz (or \(O((md+dnR)( n+d()}}{{r_{f}+T_{K}}}))\) if \(f\) is also \(\)-Lipschitz). This improves upon the bound of \(O((d^{10-}{{^{2}}}})+^{2}n^{4}d^{6}) (nRd/}{{r}})))(T_{f}+T_{K})\) arithmetic operations in  by a factor of \((}{{^{2}}}}}}{{^{2}}}, nd^{5}(}{{}}))\), when the \(_{i}\) are \(\)-Lipschitz on a polytope \(K\) and \(f\) can be evaluated in \(T_{f}=O(nd)\) operations. And it improves by a factor of (at least) \(md\) on the bound of \(O((m^{2}d^{3}+m^{2}dn^{2}^{2})( n+d)^{2}(nRd/}{{r}})) md^{-1})\) operations obtained in .

For instance, when applying the exponential mechanism to Lasso logistic regression, if e.g. \(n=d\) and \(<1\), our algorithm requires \(O(d^{3+})\) arithmetic operations, improving by \(d^{3}\) on the bound of \(d^{6+}\) operations implied by  and by roughly \(d^{9-}\) on the bound of \(O(d^{12})\) operations implied by . In another example, when training a support vector machine with hinge loss and Lasso constraints under \(\)-DP, our algorithm requires \(O(d^{4+})\) arithmetic operations, improving by a factor of \(d^{2}\) on the bound of \(d^{6+}\) operations implied by  and by \(d^{8-}\) on the bound of \(O(d^{12})\) implied by . See Appendix D for details.

**Remark 2.2** (Weaker notions of differential privacy): \(\)_-DP holds several practical advantages over weaker notions of differential privacy (DP), including \((,)\)-DP- a notion of differential privacy where the privacy of the mechanism is allowed to fail with probability \(O()\). E.g., when group privacy-- privacy of subsets of \(k\) individuals-- must be preserved, any pure \(\)-DP mechanism is also \(k\)-DP. In contrast, \((,)\)-DP only implies \((,ke^{(k-1)})\)-DP for subsets of \(k\) individuals- the failure probability grows exponentially with \(k\)._

## 3 Overview of proof of Theorem 2.1

Suppose we are given any polytope \(K=\{^{d}:A b\}\) defined by \(m\) inequalities, and a convex \(f:K^{d}\) which is \(L\)-Lipschitz (or \(\)-smooth) and given by an oracle which returns \(f()\) at any \( K\). Our goal is to sample from \( e^{-f}\) on \(K\) within any TV error \(>0\), in a number of arithmetic operations and oracle calls that has a dependence on the dimension \(d\) that is a lower-order polynomial than currently available bounds for sampling from log-Lipschitz (or log-smooth) log-concave distributions, and is logarithmic in \(}{{}}\).

Extending the Dikin walk to sample from log-concave distributions on \(K\).As a first step, we begin by attempting to generate samples from \(\) via the (Gaussian) Dikin walk, by extending the standard analysis given in e.g.  for the special case when \(\) is uniform on \(K\) to the more general case where \(\) is a log-Lipschitz log-concave on \(K\).

In the special case where \(\) is the uniform distribution on \(K\), from any point \(\) in the interior of \(K\), the Dikin walk proposes updates \(z=+()}\,\) where \( N(0,I_{d})\) and \(H()=^{2}()\) is the Hessian of the log-barrier function \(()=-_{j=1}^{m}(b_{j}-a_{j}^{})\) for \(K=\{^{d}:A b\}\), and \(>0\) is a scalar hyperparameter. To ensure that the stationary distribution of the Dikin walk is the uniform distribution on \(K\), if a proposed update falls in the interior of \(K\), it is accepted with probability

\[((}{})e^{ \|z-\|_{H()}^{2}-\|-z\|_{H(z)}^{2}},1)\]

determined by the metropolis rule; otherwise, it is rejected. The use of the log-barrier is to ensure that the steps proposed by the Dikin walk remain inside the polytope \(K\) w.h.p.

The hyperparameter \(\) is chosen as large as possible while still ensuring the proposed steps remain in \(K\) and are accepted w.h.p. On the one hand, since the covariance matrix \( H^{-1}()\) of the proposed updates is proportional to \(\), larger choices of \(\) allow the walk to propose larger update steps. On the other hand, if \(\) is too large, the proposed steps may fall outside the polytope and be rejected w.h.p. To see how to choose \(\), note that for any \((K)\), the Dikin ellipsoid \(D_{}=\{w:(w-)^{}H^{-1}()(w-) 1\}\) is contained in \(K\). Thus, standard Gaussian concentration inequalities which guarantee \(\|\|_{2}=O()\) w.h.p. imply \(+()}\,\) is in \(K\) w.h.p. if \( O(}{{d}})\). Moreover, using properties of log-barrier functions, one can show the term \(}{{(H())}}\) in the acceptance ratio is also \((1)\) for \(=O(}{{d}})\), as is done in . To see why, Lemma 4.3 of  implies that, if \(H()\) is the Hessian of a log-barrier function for \(K\), its log-determinant \(V()=((H()))\) satisfies

\[( V())^{}[H()]^{-1} V() O(d) (K).\] (1)Thus, if \(}{{d}}\), the proposed update \(z=+()}\,\) has variance \((1)\) in the direction \( V()\), and (by Gaussian concentration), \((-z)^{} V() O(1)\) w.h.p. This implies \(V(z)-V()=(H(z))-(H())=(1)\), and hence \(}{{(H())}}=(1)\).

In , the Dikin walk is applied to the more general problem of sampling from a \(L\)-log-Lipschitz (or \(\)-log-smooth) log-concave \( e^{-f}\) on \(K\) (the problem of interest in this paper). To guarantee the walk has the correct stationary distribution \(\), the Metropolis acceptance probability of the proposed updates \(z=+()}\,\), where \(\) is a hyperparameter, gains an additional factor \(}}{{e^{-f()}}}\). To ensure this acceptance probability remains \((1)\), they modify the scalar step size \(\) such that w.h.p. the walk takes steps where \(f\) changes by \(O(1)\). To see how to choose \(\), note that since \(f\) is \(L\)-Lipschitz, \(e^{f(z)-f()}=(1)\) if the Euclidean distance \(\|z-\|_{2}\) is \(O(}{{L}})\). This can be shown to occur w.h.p. if \(=O(}{{(LR)^{2}}})\), since the fact that the Dikin ellipsoid is in \(K B(0,R)\) implies that the eigenvalues of \(H()\) are all \( R^{2}\) and hence the variance \( v^{}H^{-1}()v\) of the proposed step is \( O(}{{(dL^{2})}})\) in any given direction \(v^{d}\) (where \(v\) is a unit vector). Thus, it is sufficient for them to choose \(=(}{{d}},}{{(LR)^{2}}})\) to ensure the proposed step remains in \(K\) and is accepted w.h.p.

On the one hand, to ensure the Markov chain proposes steps that change \(f\) by an amount at most \(O(1)\) for _any_\(L\)-Lipschitz \(f\), it is necessary and sufficient to ensure that from any point \((K)\), the Markov chain makes updates which fall w.h.p. inside a Euclidean ball \(B(,}{{L}})\) of radius \(}{{L}}\) centered at \(\). This is because the Lipschitz condition on \(f\): \(\|f()-f(z)\|_{2} L\|-z\|_{2}\) for all \(,z K\) holds w.r.t. the Euclidean norm \(\|\|_{2}\). On the other hand, to ensure that the Markov chain remains inside the polytope \(K\), it is sufficient to propose steps that lie inside the Dikin ellipsoid \(D_{}=\{w:(w-)^{}H^{-1}()(w-) 1\}\) centered at \(\). Roughly, the scalar step size \(\) is chosen such that this ellipsoid is contained inside the Euclidean ball \(B(,}{{L}})\), as this guarantees that w.h.p. the steps proposed by the walk will both remain in \(K\) and will also not change the value of \(f\) by more than \(O(1)\).

However, at many points \(\) the Dikin ellipsoid \(D_{}\) is such that the ratio of the largest to smallest eigenvalues of \(H^{-1}()\) may be very large (this ratio can grow arbitrarily large as \(\) approaches a face of the polytope). Thus, roughly speaking, modifying the covariance matrix of the Dikin walk by a scalar constant \(( H^{-1}())\) can cause the Dikin walk to propose steps whose variance in some directions is much smaller than is required for _either_ of the two goals: staying inside \(K\) and staying inside the ball \(B(,}{{L}})\) defined by the Lipschitz condition on \(f\). This suggests that modifying the log-barrier function for \(K\) by a scalar multiple may not be the most efficient way of extending the Dikin walk to the problem of sampling from a general \(L\)-log-Lipschitz (or \(\)-log-smooth) log-concave distribution on \(K\), and that one may be able to obtain faster runtimes by making other modifications to the barrier function.

A soft-threshold regularized Dikin walk.Before we introduce our soft-threshold Dikin walk, we first note that even in the special case where \(\) is the uniform distribution on \(K\), the analysis in  does not recover the bounds given in  for this special case, as  use a different runtime analysis geared to time-varying distributions studied in that paper. Namely,  imply a bound of \(O(m^{2}d^{3}(}{{}}))\) steps to sample from a uniform distribution on \(K\), while  show a bound of \(O(md(}{{}}))\). For this reason, we first extend the analysis of the Gaussian Dikin walk given in  for the special case of uniform \(\), to the more general problem of sampling from an \(L\)-log-Lipschitz or \(\)-log-smooth log-concave density. The analysis in  uses the cross-ratio distance metric. More specifically, if for any distinct \(u,v(K)\) we let \(p,q\) be the endpoints of the chord in \(K\) passing through \(u\) and \(v\) such that the four points lie in the order \(p,u,v,q\), the cross-ratio distance is

\[(u,v):=\|p-q\|_{2}}{\|p-u\|_{2}\|v-q\|_{2}}.\] (2)

One can show that for any \(u,v(K)\), \(^{2}(u,v)(}{{(m^{-1})}})\|u-v\|_{^{-1}H(u)}^ {2}\) (see e.g. ). Thus, as the usual Dikin walk takes steps that have roughly identity covariance matrix \(I_{d}\) with respect to the local norm \(\|u\|_{^{-1}H()}:=^{-1}H()u}\), for \(=(}{{d}},}{{(LR)^{2}}})\), the bound we would obtain on the number of steps until the Dikin walk is within TV error \(\) from \(\) is \(O(^{-1}(}{{}}))=O((md+mL^{2}R^{2})( }{{}}))\) steps from a \(w\)-warm start.

To obtain even faster bounds, we would ideally like to allow the Dikin walk to take larger steps by choosing a larger \(\), closer to the value \(}{{d}}\) that is sufficient to ensure an \((1)\) acceptance probability in the special case when \(\) is uniform. Unfortunately, if e.g. \(LR d\), reducing \(\) from \(}{{d}}\) to \((}{{(LR)^{2}}})\) may be necessary to ensure the variance of the Dikin walk steps is \(O(}{{(dL^{2})}})\) in every direction, and hence that the acceptance probability is \((1)\).

To get around this problem, we introduce a new variant of the Dikin walk Markov chain for sampling from any \(L\)-log-Lipschitz (or \(\)-log-smooth) log-concave distributions on a polytope \(K\), which generalizes the Dikin walk introduced in  for sampling from \(\) in the special case when \(\) is the uniform distribution on \(K\). The main difference between our Dikin walk and the usual Dikin walk of  (and of ) is that our Dikin walk regularizes the Hessian \(^{-1}H()\) of the log-barrier for \(K\) by adding a "soft-threshold" regularization term \(^{-1}I_{d}\) proportional to the identity matrix, where \(\) is a hyperparameter and \(\) is the same hyperparameter appearing in the original Dikin walk of . Since the log-barrier Hessian \(^{-1}H()\) and regularization term \(^{-1}I_{d}\) have different scalar hyperparameters \(,\), we can set \(\) and \(\) independently from each other: roughly, \(^{-1}\) is chosen to be the largest value such that the Dikin ellipsoid defined by the matrix \(^{-1}H()\) remains inside \(K\), while \(^{-1}\) is independently chosen to be the largest value such that, with high probability, the steps proposed by our Markov chain remain inside the ball \(B(,}{{L}})\) defined by the Lipschitz condition on \(f\). Roughly, the addition of the soft-threshold regularization term to the log-barrier Hessian allows us to reduce the variance of the proposed steps of the Dikin walk only in those directions where a choice of \(=}{{d}}\) would cause the variance to be greater than \(}{{(dL^{2})}}\) while leaving the variance in other directions unchanged.

More specifically, the steps proposed by our soft-threshold Dikin walk are Gaussian with mean 0 and covariance matrix \(^{-1}():=(^{-1}H()+^{-1}I_{d})^{-1}\). The addition of the soft-threshold regularization term \(^{-1}I_{d}\) allows us to ensure that the largest eigenvalues of the covariance matrix \(^{-1}()\) are \( O(}{{(dL^{2})}})\), without reducing (by more than a constant factor) the eigenvalues which were already \( O(}{{(dL^{2})}})\). This allows our Dikin walk to take larger steps, while still ensuring these steps are accepted w.h.p. by the Metropolis accept/reject rule for \(f\). Taking larger steps allows our Dikin walk to converge more quickly to the target distribution \(() e^{-f()}\) on \(K\).

The (inverse) covariance matrix \(()\) of the steps proposed by our Dikin walk is the Hessian of the function \(()=^{-1}()+^{-1}\|\|^{2}\) where \(()=-_{j=1}^{m}(b_{j}-a_{j}^{})\) is the log-barrier for \(K\). The modified function \(()\) can be seen to also be a self-concordant barrier for \(K\). In the special case where \(\) is the uniform distribution, \(L=0\) and \(^{-1}=0\), and our "soft-threshold" Dikin walk recovers the original walk of . Thus, our walk generalizes the original Dikin walk to the problem of sampling from a general \(L\)-log-Lipschitz (or \(\)-log-smooth) log-concave distribution on a polytope.

Bounding the number of Markov chain steps.Setting \(=}{{(dL^{2})}}\) ensures the variance \(}{{ H^{-1}()v}}\) in any given unit-vector direction \(v\) of the proposed update \(z-\) of our Markov chain is at most \(O(}{{(dL^{2})}})\), and hence the term \(e^{f(z)-f()}\) in the Metropolis acceptance rule is \((1)\) with high probability (Lemma E.5). Moreover, we also show that, if we choose \(=}{{d}}\), the other terms in the Metropolis acceptance rule are also \((1)\) (Lemmas E.8, E.9). While the proofs of these lemmas follow roughly the same outline as in the special case of the original Dikin walk where \(\) is uniform (e.g., ), our bound on the determinantal term \((z)}{{ det}()}\) must deal with additional challenges, which we discuss in the next subsection.

To bound the number of steps required by our Markov chain, we first bound the cross-ratio distance \((u,v)\) between any \(u,v(K)\) by the local norm \(\|u-v\|_{(u)}\) (Lemma E.2):

\[^{2}(u,v)(_{i=1}^{m}^{}(u-v))^{ 2}}{(a_{i}^{}u-b_{i})^{2}})+^{2}}{R^{2}} +2^{-1}R^{2}}\|u-v\|_{(u)}^{2}.\] (3)

Using (3) together with the isoperimetric inequality for the cross-ratio distance (Theorem 2.2 of ), we show that, if the acceptance probability of our Markov chain is \((1)\) at each step, then the number of steps for our Markov chain to obtain a sample within a TV distance of \(\) from \(\) is \(O(^{-1}(}{{}}))=O((md+dL^{2}R^{2})( }{{}}))\) from an \(w\)-warm start,where \(=(}{{(2m^{-1}+2^{-1}R^{2})}})\). In particular, if \(m=O(d)\) and \(LR>d\), this improves on the bound we would get for the basic Dikin walk by a factor of \(d\).

Bounding the determinant term in the acceptance probability.For our mixing time bound to hold, we still need to show that \(}{{(())}}\) is \((1)\) w.h.p. We would ideally like to follow the general approach previous works  use to show the term \(}{{(H())}}\) in the basic Dikin walk is \((1)\) w.h.p., which relies on the property of log-barrier functions in Inequality (1). Unfortunately, as \(()\) is not the Hessian of a log-barrier function for any system of inequalities defining \(K\), we cannot directly apply (1) to \(()\).

To get around this problem, we show that, while \(()\) is not the Hessian of a log-barrier function, it is in fact the limit of a sequence of matrices \(H_{i}()\), \(i\), where each matrix \(H_{i}()\) in this sequence _is_ the Hessian of a (different) log-barrier function for \(K\). Specifically, for every \(j\), we consider the matrices \(A^{j}=[A^{},I_{d},,I_{d}]^{}\) where \(A\) is concatenated with \(-1)}}{{d}}\) copies of the identity matrix \(I_{d}\), and \(m_{j}=m+^{-1}j^{2} d\). And we consider the vectors \(b^{j}=(b^{},j^{},,j^{})^{}\), where \(b\) is concatenated with \(-1)}}{{d}}\) copies of the vector \(j\), where \(=(1,,1)^{}^{d}\) is the all-ones vector. Then, for large enough \(j\), \(K=\{^{d}:A^{j} b^{j}\}\), and the Hessian of the corresponding log-barrier functions is

\[H_{j}() =_{i=1}^{m^{j}}^{j}(a_{i}^{j})^{}}{((a_{i}^{j} )^{}-b_{i}^{j})^{2}}=H()+^{-1}j^{2} _{i=1}^{d}e_{i}^{}}{(e_{i}^{}-j)^{2}}.\] (4)

Using this fact (4), we show that, for every \((K)\), every \(zD_{}\), and every sequence \(\{z_{j}\}_{j=1}^{}D_{}\) such that \(z_{j} z\), we have (Lemma E.7),

\[_{j}(z_{j}))}{(H_{j}())}=.\] (5)

Moreover, since each \(H_{j}\) is the Hessian of a log-barrier function for \(K\), we have that (1) does hold for \(H_{j}\) and hence (from the work of ) that \((z_{j}))}}{{(H_{j}())}}=(1)\) w.h.p. for all \(j\), if we set \(z_{j}=+}{{2}}}}}{{H_{j}^{-1/2}() }}\), \( N(0,I_{d})\), and choose \(=}{{d}}\). Thus, (5) implies that \(}{{(())}}=(1)\) w.h.p. as well (Lemma E.9), and hence the acceptance probability of the soft-threshold Dikin walk is \((1)\) at each step.

Bounding the number of arithmetic operations.Since the acceptance probability is \((1)\), from the above discussion, the number of steps for our Markov chain to obtain a sample within TV distance \(>0\) from \(\) is \(O((md+L^{2}R^{2})(}{{}}))\) from a \(w\)-warm start.

Each time our Markov chain proposes a step \(+}}{{(2)}}\), it must first sample a Gaussian vector \( N(0,I_{d})\) which takes \(O(d)\) arithmetic operations. It must then compute the log-barrier Hessian \(H()\), and invert the matrix \(()=^{-1}H()+^{-1}I_{d}\).

Since \(H()=C()C()^{}\), where \(C()\) is a \(d m\) matrix with columns \(c_{j}()=}}{{(a_{j}^{}-b_{j})}}\) for all \(j[m]\), we can compute \(H()\) in \(md^{-1}\) arithmetic operations. And since \(()\) is a \(d d\) matrix, computing \(^{-}{{2}}}()\) can be accomplished in \(d^{}\) arithmetic operations by computing the singular value decomposition (SVD) of \(\). Next, we compute the acceptance probability \((}{e^{-f()}}\|}}{{()}-\|-z\|^{2} _{(z)}},1).\) The determinants can be computed in \(O(d^{})\) arithmetic operations via the SVD. Evaluating \(f(z),f()\) takes two calls to the oracle for \(f\).

Thus, from a \(w\)-warm start, the soft-threshold Dikin walk takes at most \(O((md+dL^{2}R^{2})(}{{}}))\) Markov chain steps to obtain a sample from \(\) with total variation error \(>0\), where each step takes \(O(md^{-1})\) arithmetic operations, and one function evaluation.

**Remark 3.1** (Optimality of \(_{2}\)-regularization): _For the class of functions considered in our paper, we conjecture that an \(_{2}\)-regularizer that does not depend on \(\) is optimal. This is because we consider the class of functions which are \(L\)-Lipschitz or \(\)-smooth with respect to the \(_{2}\)-norm, and our bound on \(L\) or \(\) does not depend on \(\). Moreover, we only have access to the function \(\) through an oracle which returns the value of \(f\) at any given point \(\) but does not tell us how \(f\) changes at nearby points._In Theorem 2.1, we set the step size hyperparameters \(=}{{(10^{5}d)}}\) and \(=}{{(10^{4}dL^{2})}}\) if \(f\) is \(L\)-Lipschitz, and the number of steps to be \(T=10^{9}(2m^{-1}+^{-1}R^{2})(}{{ }}).\) When \(f\) is \(\)-smooth (but not necessarily Lipschitz), we instead set \(=}{{(10^{5}d)}}\) and \(=}{{(10^{4}d)}}\).

In many applications, a bound on \(L\) or \(\) can be calculated analytically, allowing one to set \(,\) as above. This includes, e.g., applications to training Bayesian or differentially private logistic regression models (or other generalized linear models such as support vector machines). When a bound on \(L\) or \(\) is not known, one can in practice set \(,\) by hand such that the average acceptance probability is \((1)\).

## 4 Conclusions, limitations, and future work

Our result improves on the runtime bounds of a line of previous work, for the problem of sampling from several classes of log-Lipschitz or log-smooth log-concave distributions on a polytope (see Table 1). To the best of our knowledge, this is the first result to introduce regularized barrier functions that simultaneously take into account the geometry of both the constraint polytope and the Lipschitz or smoothness property of a target logconcave function. These barrier functions may be of independent interest for sampling or optimization.

On the other hand, we note that  give an implementation of the Dikin walk in the special case where \(f\) is constant, where the (average) cost of computing the Hessian matrix of the log-barrier of the polytope \(K:=\{^{d}:A b\}\) at each step of the walk is improved to roughly \(O(d^{2}+(A))\) arithmetic operations, where \((A)\) denotes the number of non-zero entries of \(A\). Whether this improvement in the per-step computation time can be achieved for the problem of computing the regularized barrier functions used in our algorithm, in the more general setting where \(\) is \(L\)-Lipschitz or \(\)-smooth, is an interesting open problem.

Moreover, we note that our bounds are polynomial in \(L\) or \(\), yet there are algorithms for sampling from log-concave distributions \( e^{-f}\) which do not assume \(f\) is \(L\)-Lipschitz or \(\)-smooth. Thus, another interesting open problem is whether one can obtain runtime bounds for a version of the Dikin walk which do not require \(f\) to be Lipschitz or smooth.

Our results have applications to Bayesian inference and differentially private optimization. Bayesian inference can lead to algorithms with better generalization properties and quantification of uncertainty, and differential privacy guarantees are important to protecting the privacy of individuals in medical and other sensitive datasets. Thus, we believe our results will have positive societal impacts, and do not anticipate any negative impacts to society.

Acknowledgments.NV was supported in part by an NSF CCF-2112665 award. OM was supported in part by an NSF CCF-2104528 award and a Google Research Scholar award.