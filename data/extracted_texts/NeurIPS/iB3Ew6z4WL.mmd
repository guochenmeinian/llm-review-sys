# MultiModN--Multimodal, Multi-Task, Interpretable Modular Networks

Vinitra Swamy*

EPFL

vinitra.swamy@epfl.ch

&Malika Satayeva*

EPFL

malika.satayeva@epfl.ch

&Jibril Frej

EPFL

jibril.frej@epfl.ch

&Thierry Bossy

EPFL

thierry.bossy@epfl.ch

&Thijs Vogels

EPFL

thijs.vogels@epfl.ch

&Martin Jaggi

EPFL

martin.jaggi@epfl.ch

&Tanja Kaser*

EPFL

tanja.kaser@epfl.ch

&Mary-Anne Hartley*

Yale, EPFL

mary-anne.hartley@yale.edu

&

###### Abstract

Predicting multiple real-world tasks in a single model often requires a particularly diverse feature space. Multimodal (MM) models aim to extract the synergistic predictive potential of multiple data types to create a shared feature space with aligned semantic meaning across inputs of drastically varying sizes (i.e. images, text, sound). Most current MM architectures fuse these representations in parallel, which not only limits their interpretability but also creates a dependency on modality availability. We present MultiModN, a multimodal, modular network that fuses latent representations in a sequence of any number, combination, or type of modality while providing granular real-time predictive feedback on any number or combination of predictive tasks. MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness. We perform four experiments on several benchmark MM datasets across 10 real-world tasks (predicting medical diagnoses, academic performance, and weather), and show that MultiModN's sequential MM fusion does not compromise performance compared with a baseline of parallel fusion. By simulating the challenging bias of missing not-at-random (MNAR), this work shows that, contrary to MultiModN, parallel fusion baselines erroneously learn MNAR and suffer catastrophic failure when faced with different patterns of MNAR at inference. To the best of our knowledge, this is the first inherently MNAR-resistant approach to MM modeling. In conclusion, MultiModN provides granular insights, robustness, and flexibility without compromising performance.

+
Footnote â€ : * denotes equal contribution

## 1 Introduction

The world is richly multimodal and intelligent decision-making requires an integrated understanding of diverse environmental signals, known as embodied intelligence . Until recently, advances in deep learning have been mostly compartmentalized by data modality, creating disembodied domains such as computer vision for images, natural language processing for text, and so on. Multimodal (MM) learning has emerged from the need to address real-world tasks that cannot be robustly represented by a single signal type as well as the growing availability and diversity of digitized signals . Some examples are diagnosis from a combination of medical tests and imagery , estimating sentiment from facial expression, text, and sound , and identifying human activities from a combination of sensors .

The richer representations from synergistic data types also have the potential to increase the task space, where a single set of representations can generalize to several tasks. Multi-task (MT) learning has not only been shown to benefit the performance of individual tasks but also has the potential to greatly reduce computational cost through shared feature extraction .

In short, multimodal and multi-task learning hold significant potential for human-centric machine learning and can be summarized respectively as creating a shared feature space from various data types and deriving their relative semantic meaning across several tasks.

**Limitations of current multimodal models.** Current MM models propose a parallel integration of modalities, where representations are fused and processed simultaneously [2; 3; 4]. Parallel fusion (hereafter P-Fusion) creates several fundamental limitations that we address in this work.

The most important issue we seek to resolve in current MM architectures is their _dependence on modality availability_ where all modalities for all data points are required inputs during both training and inference. Modality-specific missingness is a common real-world problem and can fundamentally bias the model when the missingness of a modality is predictive of the label (known as missing not-at-random, MNAR). The common solution of restricting learning to data points with a complete set of modalities creates models that perform inequitably in populations with fewer available resources (i.e. when the pattern of MNAR is different between train and test sets). In complex real-world datasets, there is often no intersection of complete availability, thus necessitating the exclusion of modalities or significantly limiting the train set. On the other hand, imputation explicitly featurizes missingness, thus risking to create a trivial model that uses the _presence_ of features rather than their value for the prediction [14; 15]. The MNAR issue is particularly common in medicine, where modality acquisition is dependent on the decision of the healthcare worker (i.e. the decision that the model is usually attempting to emulate). For example, a patient with a less severe form of a disease may have less intensive monitoring and advanced imagery unavailable. If the goal is to predict prognosis, the model could use the missingness of a test rather than its value. This is a fundamental flaw and can lead to catastrophic failure in situations where the modality is not available for independent reasons (for instance resource limitations). Here, the featurized missingness would inappropriately categorize the patient in a lower severity class. For equitable real-world predictions, it is critical to adapt predictions to available resources, and thus allow composability of inputs at inference.

Another key issue of current techniques that this work addresses is _model complexity_. Parallel fusion of various input types into a single vector make many post-hoc interpretability techniques difficult

Figure 1: Comparison of modular MultiModN **(a)** vs. monolithic P-Fusion **(b)**. MultiModN inputs any number/combination of modalities (mod) into a sequence of \(mod\)-specific encoders (e). It can skip over missing modalities. A state (s) is passed to the subsequent encoder and updated. Each state can be fed into any number/combination of decoders (d) to predict multiple tasks. _Modules_ are identified as grey blocks comprising an encoder, a state, and a set of decoders. P-Fusion is a monolithic model. It inputs a _fixed_ number/combination of modalities (mod) into \(mod\)-specific encoders (e). Missing modalities are padded and encoded. Embeddings (emb) are concatenated and provided to a _single_ decoder in parallel (d) to predict a _single_ task.

or impossible . Depending on where the fusion occurs, it may be impossible to decompose modality-specific predictive importance.

In this work, we leverage network modularization, compartmentalizing each modality and task into independent encoder and decoder modules that are inherently robust to the bias of MNAR and can be assembled in any combination or number at inference while providing continuous modality-specific predictive feedback.

**Contributions.** We propose MultiModN, a multimodal extension of the work of Trottet et al. , which uses a flexible sequence of model and task-agnostic encoders to produce an evolving latent representation that can be queried by any number or combination of multi-task, model-agnostic decoder modules after each input (showcased in Figure 1). Specifically, we demonstrate that our modular approach of sequential MM fusion:

** matches parallel MM fusion** (P-Fusion) for a range of real-world tasks across several benchmark datasets, while contributing distinct advantages, such as being:

** composable at inference**, allowing selection of any number or combination of available inputs,

** is robust to the bias of missing not-at-random (MNAR) modalities**,

** is inherently interpretable**, providing granular modality-specific predictive feedback, and

** is easily extended to any number or combination of tasks**.

We provide an **application-agnostic open-source framework** for the implementation of MultiModN: https://github.com/epfl-iglobalhealth/MultiModN. Our experimental setup purposely limits our model performance to fairly compare the multimodal fusion step. At equivalent performance, our model architecture is by far superior to the baseline by virtue of being inherently modular, interpretable, composable, robust to systematic missingness, and multi-task.

## 2 Background

Approaches to MM learning can be categorized by the depth of the model at which the shared feature space is created . Late fusion (decision fusion) processes inputs in separate modality-specific sub-networks, only combining the outputs at the decision-level, using a separate model or aggregation technique to make a final prediction. While simple, late fusion fails to capture relationships between modalities and is thus not _truly_ multimodal. Early fusion (feature fusion), combines modalities at the input level, allowing the model to learn a joint representation. Concatenating feature vectors is a popular and simple approach [18; 19], but the scale of deployment is particularly limited by the curse of dimensionality. Finally, intermediate fusion (model fusion) seeks to fine-tune several feature extraction networks from the parameters of a downstream classifier.

**Parallel Multimodal Fusion** (P-Fusion). Recently, Soenksen et al.  proposed a fusion architecture which demonstrated the utility of multiple modalities in the popular MM medical benchmark dataset, MIMIC [21; 22]. Their framework (HAIM or Holistic Artificial Intelligence in Medicine) generates single-modality embeddings, which are concatenated into a single one-dimensional multimodal fusion embedding. The fused embedding is then fed to a single-task classifier. This work robustly demonstrated the value of multimodality across several tasks and a rich combination of heterogeneous sources. HAIM consistently achieved an average improvement of 6-33% AUROC (area under the receiver operating characteristic curve) across all tasks in comparison to single-modality models. We use this approach as a P-Fusion baseline against our sequential fusion approach of MultiModN and extend it to several new benchmark datasets and multiple tasks.

Soenksen et al.  perform over 14,324 experiments on 12 binary classification tasks using every number and combination of modalities. This extreme number of experiments, was necessary because the model is not composable nor capable of multi-task (MT) predictions. Rather, a different model is needed for each task and every combination of inputs for each task. In contrast, MultiModN is an extendable network, to which any number of encoders and decoders can be added. Thus, most of the 14,324 experiments could technically be achieved within one MultiModN model.

Several other recent architectures utilize parallel fusion with transformers. UNiT (Unified Transformer)  is a promising multimodal, multi-task transformer architecture; however, it remains monolithic, trained on the union of all inputs (padded when missing) fed in parallel. This not only exposes the model to patterns of systematic missingness during training but also reduces model interpretability and portability1. 's recent work has found similar results on the erratic behavior of transformers to missing modalities, although it is only tested on visual/text inputs. LLMs have also recently been used to encode visual and text modalities , but it is not clear how tabular and time-series would be handled or how this would affect the context window at inference. Combining predictive tasks with LLMs will also greatly impact interpretability, introducing hallucinations and complex predictive contamination where learned textual bias can influence outcomes.

Modular Sequential Multimodal Fusion.A _module_ of a modular model is defined as a self-contained computational unit that can be isolated, removed, added, substituted, or ported. It is also desirable for modules to be order invariant and idempotent, where multiple additions of the same module have no additive effect. We design MultiModNI to encode individual inputs, whereby module-exclusion can function as input _skippablity_, allowing missing inputs to be skipped without influencing predictions. Thus, modular models can have various input granularities, training strategies, and aggregation functions. Some popular configurations range from hierarchies with shared layers to ensemble predictions and teacher-trainer transfer learning approaches [26; 27].

We expand on the sequential modular network architecture proposed by Trottet et al. called MoDN (Modular Decision Networks) as a means of sequential MM fusion. MoDN trains a series of feature-specific encoder modules that produce a latent representation of a certain size (the _state_). Modules can be strung together in a mix-and-match sequence by feeding the state of one encoder as an input into the next. Therefore, the state has an additive evolution with each selected encoder. A series of decoders can query the state at any point for multiple tasks from various combinations of inputs, giving MoDN the property of combinatorial generalization.

Thus, we extend MoDN to learn multiple tasks from multimodal inputs. By aligning feature extraction pipelines between MultiModNI and the P-Fusion baseline (inspired by HAIM) we can achieve a better understanding of the impact of monolithic-parallel fusion vs. sequential-modular MM fusion. Figure 1 provides a comparison between P-Fusion and MultiModNI, also formalized below.

## 3 Problem formulation

Context.We propose a multi-task supervised learning framework able to handle any number or combination of inputs of varying dimension, irrespective of underlying bias in the availability of these inputs during training. We modularize the framework such that each input and task is handled by distinct encoder and decoder _modules_. The inputs represent various data modalities (i.e. image, sound, text, time-series, tabular, etc.). We assume that these inputs have synergistic predictive potential for a given target and that creating a multimodal shared feature space will thus improve model performance. The tasks represent semantically related observations. We hypothesize that jointly training on semantically related tasks will inform the predictions of each individual task.

Notation.Formally, given a set of modalities (features) \(=\{mod_{1},,mod_{||}\}\) and a set of tasks (targets) \(=\{task_{1},,task_{||}\}\), let \(=\{(x_{1},y_{1}),(x_{2},y_{2}),,(x_{N},y_{N})\}\) represent a multimodal, multi-task dataset with \(N\) data points (\(x_{1},,x_{N}\)).

Each point \(x\) has \(||\) modalities (inputs): \(x=x_{mod_{1}},,x_{mod_{||}}}\) and is associated with a set of \(||\) targets (tasks): \(y=y_{task_{1}},,y_{task_{||}}}\). Modalities comprise various sources (e.g. images from x-rays, CT), for simplicity, we consider sources and modalities as equal \(mod\) elements in \(\).

Multimodal, multi-task, modular formulation.We decompose each data point \(x\) into \(||\) sequential encoder _modules_ specific to its constituent modalities and each target \(y\) into \(||\) decoder _modules_ specific to its constituent tasks such that any combination or number of modalities can be used to predict any combination or number of tasks. Our objective is to learn a set of function _modules_, \(\). Each function _module_ within this set, represented as \(^{i}_{j}\) maps combinations of modalities \(_{j}\) to combinations of tasks \(_{i}\), i.e. \(f^{i}_{j}:_{j}_{i}\). It is important to note that \(_{j}\) is an element of the powerset of all modalities and \(_{i}\) is an element of the powerset of all tasks.

Extension to time-series.In the above formulation, the \(\) encoder _modules_ are handled in sequence, thus naturally aligning inputs with time-series. While the formulation does not change for time-series data, it may be optimized such that \(f^{i}_{j}\) represents a single time step. This is relevant in the real-worldsetting of a data stream, where inference takes place at the same time as data is being received (i.e. predicting student performance at each week of a course as the course is being conducted). The continuous prediction tasks (shown for EDU and Weather in Sec. 6) demonstrate how MultiModN can be used for incremental time-series prediction.

## 4 MultiModN: Multimodal, Multi-task, Modular Networks (Our model)

Building on  (summarized and color-coded in Figure 0(a)), the MultiModN architecture consists of three modular elements: a set of **State** vectors \(=s_{0},,s_{||}}\), a set of modality-specific Encoders \(=e_{1},,e_{||}}\), and a set of task-specific **Decoders**\(=d_{1},,d_{||}}\). State \(s_{0}\) is randomly initialized and then updated sequentially by \(e_{i}\) to \(s_{i}\). Each \(s_{i}\) can be decoded by one, any combination, or all elements of \(\) to make a set of predictions. All encoder and decoder parameters are subject to training.

**States (\(\)).** Akin to hidden state representations in Recurrent Neural Networks (RNNs), the state of MultiModN is a vector that encodes information about the previous inputs. As opposed to RNNs, state updates are made by any number or combination of modular, modality-specific encoders and each state can be decoded by modular, task-specific decoders. Thus the maximum number of states by any permutation of \(n\) encoders is \(n!\). For simplicity, we limit the combinatorial number of states to a single order (whereby \(e_{i}\) should be deployed before \(e_{i+1}\)) in which any number or combination of encoders may be deployed (i.e. one or several encoders can be skipped at any point) as long as the order is respected. Thus, the number of possible states for a given sample is equal to \(2^{||}\). Order invariance could be achieved by training every permutation of encoders \(||!\), i.e. allowing encoders to be used in any order at inference, as opposed to this simplified implementation of MultiModN in which order is fixed. At each step \(i\), the encoder \(e_{i}\) processes the previous state, \(s_{i-1}\), as an input and outputs an updated state \(s_{i}\) of the same size. When dealing with time-series, we denote \(s_{t(0)}\) as the state representing time \(t\) before any modalities have been encoded, and as \(s_{t(0,1,4,5)}\) as the state at time \(t\) after being updated by encoders \(e_{1}\), \(e_{4}\) and \(e_{5}\), in that order.

Encoders (\(\)).** Encoders are modularized to represent a single modality, i.e. \(||=||\). An encoder \(e_{i}\) takes as input the combination of a single modality (of any dimension) and the previous state \(s_{i-1}\). Encoder \(e_{i}\) then outputs a \(s_{i}\), updated with the new modality. For simplicity, we fix the state size between encoders. Due to modularization, MultiModN is model-agnostic, whereby encoders can be of any type of architecture (i.e. Dense layers, LSTM, CNN). For experimental simplicity, we use a single encoder design with a simple dense layer architecture. The input vectors in our experiments are 1D. When a modality is missing, the encoder is skipped and not trained (depicted in Figure 1).

**Decoders (\(\)).** Decoders take any state \(s_{i}\) as input and output a prediction. Each decoder is assigned to a single task, that is \(||=||\), i.e. MultiModN is not multiclass, but multi-task (although a single task may be multiclass). Decoders are also model-agnostic. Our implementation has regression, binary, and multiclass decoders across static targets or changing time-series targets. Decoder parameters are shared across the different modalities. The decoder predictions are combined across modalities/modules by averaging the loss. Interestingly, a weighted loss scheme could force the model to emphasize certain tasks over others.

As shown in , MultiModN can be completely order-invariant and idempotent if randomized during training. For interpretability, sequential inference (in any order) is superior to parallel input due to its decomposability, allowing the user to visualize the effect of each input and aligning with Bayesian reasoning.

**Quantification of Modularity.** The modularity of a network can be quantified, whereby neurons are represented by nodes (vertices) and connections between neurons as edges. There are thus comparatively dense connections (edges) within a _module_ and sparse connections between them. Partitioning modules is an NP-complete problem . We present modules that are defined _a priori_, whereby a module comprises one encoder \(e_{i}\) connected to one state \(s_{i}\), which is in turn connected to a set of \(||\) tasks (a _module_ is depicted as a grey box in Figure 0(a)). Following a formalization of modularity quantitation proposed by Newman et al. , we compute the modularity score for MultiModN and show that it tends to a perfect modularity score of 1 with each added modality and each added task. When viewed at the network granularity of these core elements, P-Fusion is seen as a monolithic model with a score of 0. The formula is elaborated in Appendix Sec. B.

### P-Fusion: Parallel Multimodal Fusion (Baseline)

We compare our results to a recent multimodal architecture inspired by HAIM (Holistic AI in Medicine) . As depicted in Figure 0(b), HAIM also comprises three main elements, namely, a fixed set of modality-specific encoders \(=\{e_{1},,e_{||}\}\) which create a fixed set of embeddings \(=\{emb_{1},,emb_{||}\}\), that is concatenated and fed into a single-task decoder (\(d_{1}\)). HAIM achieved state-of-the-art results on the popular and challenging benchmark MIMIC dataset, showing consistently that multimodal predictions were between 6% and 33% better than single modalities.

Enceders (\(\)).Contrary to the flexible and composable nature of MultiModN, the sequence of encoders in P-Fusion is fixed and represents a unique combination of modalities. It is thus unable to skip over modalities that are missing, instead padding with neutral values and explicitly embedding the non-missing modalities. The encoders are modality-specific pre-trained neural networks.

**Embeddings (\(\)).** Multimodal embeddings are fused in parallel by concatenation.

**Decoders (\(\)).** Concatenated embeddings are passed to a single-task decoder.

**Architecture alignment.** We align feature extraction between MultiModN and P-Fusion to best isolate the effect of sequential (MultiModN) vs. parallel (P-Fusion) fusion. As depicted in Appendix Figure 8, we let MultiModN take as input the embeddings created by the P-Fusion pre-trained encoders. Thus both models have identical feature extraction pipelines. No element of the MultiModN pipeline proposed in Figure 0(a) is changed. The remaining encoders and decoders in both models are simple dense layer networks (two fully connected ReLU layers and one layer for prediction). Importantly, MultiModN encoders and decoders are model-agnostic and can be of any architecture.

## 5 Datasets

We compare MultiModN and P-Fusion on three popular multimodal benchmark datasets across 10 real-world tasks spanning three distinct domains (healthcare, education, meteorology). The healthcare dataset (MIMIC) is particularly challenging in terms of multimodal complexity, incorporating inputs of vastly varying dimensionality. Education (EDU) and Weather2k have a particular focus on time-series across modalities. Appendix Sec. C details features, preprocessing, and tasks (\(task_{1-10}\)).

**MIMIC.** MIMIC  is a set of deidentified electronic medical records comprising over \(40,000\) critical care patients at a large tertiary care hospital in Boston. The feature extraction pipeline replicated according to our baseline of P-Fusion, making use of patient-level feature embeddings extracted from pre-trained models as depicted in Appendix Figure 8. We select the subcohort of \(921\) patients who have valid labels for both diagnoses and all four modalities present. We use all four modalities as inputs: chest x-rays (image), chart events (time-series), demographic information (tabular), and echocardiogram notes (text). For simplicity, we focus on two diagnostic binary classification tasks: cardiomegaly (\(task_{1}\)) and enlarged cardiomediatum (\(task_{2}\)). These tasks were selected for their semantic relationship and also because they were reported to benefit from multimodality . Thus, we have four modality-specific encoders and two binary classification diagnostic decoders.

**Education (EDU).** This educational time-series dataset comprises \(5,611\) students with over 1 million interactions in a 10-week Massively Open Online Course (MOOC), provided to a globally diverse population. It is benchmarked in several recent works [31; 32; 33]. Our modeling setting is replicated from related literature, with \(45\) handcrafted time-series features regarding problem and video modalities extracted for all students at each weekly time-step . We use two modality-specific encoders (problem and video) and three popular decoder targets: binary classifiers (\(task_{3-4}\)) of pass/fail and drop-out, and a continuous target of next week's performance (\(task_{5}\)) .

**Weather2k.**_Weather2k_ is a 2023 benchmark dataset that combines tabular and time-series modalities for weather forecasting . The data is extracted from \(1,866\) ground weather stations covering \(6\) million km\({}^{2}\), with \(20\) features representing hourly interactions with meteorological measurements and three static features representing the geographical location of the station. We create five encoders from different source modalities: geographic (static), air, wind, land, and rain and align with the benchmark prediction targets  on five continuous regression targets: short (24 hr), medium (72 hr), long term (720 hr) temperature forecasting, relative humidity and visibility prediction (\(tasks_{6-10}\)).

## 6 Experiments

**Overview.** We align feature extraction pipelines between MultiModN and the P-Fusion baseline in order to isolate the impact of parallel-monolithic vs. sequential-modular fusion (described in 4.1 and depicted in Appendix Sec. B). We thus do not expect a significant difference in performance, but rather aim to showcase the distinct benefits that can be achieved with modular sequential multimodal fusion _without compromising baseline performance_. In the following subsections, we perform four experiments to show these advantages. **** MultiModN performance is not compromised compared to P-Fusion in single-task predictions. **** MultiModN is able to extend to multiple tasks, also without compromising performance. **** MultiModN is inherently composable and interpretable, providing modality-specific predictive feedback. **** MultiModN is resistant to MNAR bias and avoids catastrophic failure when missingness patterns are different between train and test settings.

**Model evaluation and metrics.** All results represent a distribution of performance estimates on a model trained 5 times with different random weight initializations for the state vector and weights. Each estimate uses a completely independent test set from an 80-10-10 K-Fold train-test-validation split, stratified on one or more of the prediction targets. We report metrics (macro AUC, BAC, MSE) with 95% confidence intervals, as aligned with domain-specific literature of each dataset [34; 20; 36].

**Hyperparameter selection.** Model architectures were selected among the following hyperparameters: state representation sizes [1; 5; 10; 20; 50; 100], batch sizes [8; 16; 32; 64; 128], hidden features [16; 32; 64; 128], dropout [0, 0.1, 0.2, 0.3], and attention . These values were grouped into 3 categories (small, medium, large). We vary one while keeping the others fixed (within groups). Appendix Figure 9 shows that MultiModN is robust to changing batch size, while dropout rate and hidden layers negatively impact larger models (possibly overfitting). The most specific parameter to MultiModN is state size. As expected, we see negative impacts at size extremes, where small states likely struggle to transfer features between steps, while larger ones would be prone to overfitting.

### Exp. 1: Sequential modularization in MultiModN does not compromise performance

**Setup.** A single-task model was created for each \(task_{1-10}\) across all three datasets. Each model takes all modalities as input. We compare MultiModN and P-Fusion in terms of performance. AUROCs can be visualized in Figure 2 while BAC and MSE are detailed in Table 1. As feature extraction pipelines between MultiModN and P-Fusion are aligned, this experiment seeks to investigate if sequential modular fusion compromises model performance. To compress the multiple predictions of time-series into a single binary class, we select a representative time step (EDU \(tasks_{3-4}\) at 60% course completion) or average over all time steps (Weather \(tasks_{9-10}\) evaluated on a 24h window).

**Results.** Both MultiModN and P-Fusion achieve state-of-the-art results on single tasks using multimodal inputs across all 10 targets. In Figure 1(c), we binarize the continuous weather task (humidity prediction) as an average across all time steps. The task is particularly challenging for the P-Fusion baseline, which has random performance (AUROC: 0.5). Compared with P-Fusion,

Figure 2: **MultiModN does not compromise performance in single-tasks.** AUROC for six binary prediction tasks in (a) MIMIC, (b) EDU, and (c) Weather2k. Tasks predicted by P-Fusion are compared with MultiModN. 95% CIs are shaded.

MultiModN shows a 20% improvement, which is significant at the \(p<0.05\) level. As the temporality of this task is particularly important, it could be hypothesized that the sequential nature of MultiModN better represents time-series inputs. Nevertheless, all weather targets are designed as regression tasks and show state-of-the-art MSE scores in Table 1 where MultiModN achieves baseline performance.

We provide an additional parallel fusion transformer baseline with experimental results showcased in Appendix Sec. E.4. The results indicate that MultiModN matches or outperforms the multimodal transformer in the vast majority of single- and multi-task settings, and comes with several interpretability, missingness, and modularity advantages. Specifically, using the primary metric for each task (BAC for classification and MSE for regression tasks), MultiModN beats the transformer baseline significantly in 7 tasks, overlaps 95% CIs in 11 tasks, and loses slightly (0.01) in 2 regression tasks.

MultiModN matches P-Fusion performance across all 10 tasks in all metrics reported across all three multimodal datasets. Thus, modularity does not compromise predictive performance.

### Exp. 2: Multi-task MultiModN maintains baseline performance in individual tasks

Setup.The modular design of MultiModN allows it to train multiple task-specific decoders and deploy them in any order or combination. While multi-task models have the potential to enrich feature extraction (and improve the model), it is critical to note that all feature extraction from the raw input is performed before MultiModN is trained. MultiModN is trained on embeddings extracted from pre-trained models (independently of its own encoders). This is done purposely to best isolate the effect of parallel-monolithic vs. sequential-modular fusion. We train three multi-task MultiModN models (one for each dataset, predicting the set of tasks in that dataset, i.e. \(tasks_{1-2}\) in MIMIC, \(tasks_{3-5}\) in EDU, and \(tasks_{6-10}\) in Weather) and compare this to 10 single-task MultiModN models (one for each \(tasks_{1-10}\)). Monolithic models, like P-Fusion are not naturally extensible to multi-task predictions. Thus P-Fusion (grey bars in Figure 3) can only be displayed for single-task models. This experiment aims to compare MultiModN performance between single- and multi-task architectures to ensure that this implementation does not come at a cost to the predictive performance of individual tasks.

Results.In Figure 3 we compare the single-task P-Fusion (grey bars), to single- and multi-task implementations of MultiModN (in color). The results demonstrate that MultiModN is able to maintain its performance across all single-prediction tasks even when trained on multiple tasks. We additionally include the results of our model on various numbers and combinations of inputs, described further in Appendix Sec. E.5. The baseline would have to impute missing features in these combinations, exposing it to catastrophic failure in the event of systematic missingness (Sec. 6.4).

   &  &  &  \\   & Cardiomegaly & ECM & Success & Dropout & Next Week & Temp.(24k) & Temp (720k) & Temp (720k) & Humidity & Visibility \\  _Metric_ & _BAC_ & _BAC_ & _BAC_ & _MSE_ & _MSE_ & _MSE_ & _MSE_ & _MSE_ \\
**MultiModN** & 0.75 \(\)0.04 & 0.71 \(\)0.03 & 0.93 \(\)0.04 & 0.83 \(\)0.02 & 0.01 \(\)0.01 & 0.02 \(\)0.01 & 0.03 \(\)0.01 & 0.03 \(\)0.01 & 0.02 \(\)0.01 & 0.10 \(\)0.01 \\
**P-Fusion** & 0.75 \(\)0.02 & 0.69 \(\)0.03 & 0.92 \(\)0.03 & 0.87 \(\)0.05 & 0.01 \(\)0.01 & 0.02 \(\)0.01 & 0.03 \(\)0.01 & 0.02 \(\)0.01 & 0.03 \(\)0.01 & 0.08 \(\)0.02 \\  

Table 1: MultiModN **does not compromise performance in single-tasks.** Performance for binary and continuous prediction tasks in MIMIC, EDU, and Weather, comparing P-Fusion and MultiModN. \(95\%\) CIs are shown. _ECM: Enlarged Cardiomediastinum, Temp: Temperature_.

Figure 3: **Multi-task MultiModN maintains baseline performance in individual tasks.** Single- and multi-task MultiModN on the prediction of individual tasks, compared with the monolithic P-Fusion (can only be single-task). AUC for binary (**left**) and MSE for continuous (**right**). Error bars: 95% CIs.

MultiModN has the significant advantage of being naturally extensible to the prediction of multiple tasks without negatively impacting the performance of individual tasks.

### Exp. 3: MultiModN has inherent modality-specific local and global model explainabilty

Setup.Parallel MM fusion obfuscates the contribution of individual inputs and requires add-on or post hoc methods to reveal unimodal contributions and cross-modal interactions . Soenksen et al.  used Shapley values  to derive marginal modality contributions. While these post hoc methods provide valuable insight, they are computationally expensive and challenging or impossible to deploy at inference. In contrast, MultiModN confers inherent modality-specific interpretability, where the contribution of each input can be decomposed by module. We use \(task_{1-2}\) in MIMIC to compute two measures: **** Importance score, where each encoder is deployed alone, providing predictive importance of a single modality by subtracting predictions made from the prior state. This can be computed across all data points or individual data points. ** Cumulative probability**, where the prediction from each multi-task decoder is reported in sequence (i.e. given the previously encoded modalities). We demonstrate this on a random patient from the test set, who has a true label of 0 for both tasks. Further plots are in Appendix Sec. E.2.

Results.Monolithic P-Fusion models cannot be decomposed into any modality-specific predictions, and its (single-task) prediction is only made after inputting all modalities. In contrast, Figure 4 shows MultiModN provides granular insights for both importance score and cumulative prediction. We observe that the Text modality is the most important. The cumulative prediction shows the prior strongly predicts positivity in both classes and thus that \(S_{0}\) has learned the label prevalence.

The predictions naturally produced by MultiModN provide diverse and granular interpretations.

### Exp. 4: MultiModN is robust to catastrophic failure from biased missingness

Setup.MultiModN is designed to predict any number or combination of tasks from any number or combination of modalities. A missing modality is skipped (encoder \(e_{i}\) is not used) and not padded/encoded. Thus, MultiModN avoids featurizing missingness, which is particularly advantageous when missingness is MNAR. Featurizing MNAR can result in catastrophic failure when MNAR patterns differ between train and test settings. We demonstrate MultiModN's inherent robustness to catastrophic MNAR failure by training MultiModN and P-Fusion on four versions of MIMIC with various amounts (0, 10, 50, or 80%) of MNAR by artificially removing one modality in one class only. Figure 5 compares MultiModN and P-Fusion on \(task_{1}\) when tested in a setting that has either no missingness or where the MNAR pattern is different (i.e. label-flipped).

Results.Figure 5 shows a dramatic catastrophic failure of P-Fusion in a label-flipped MNAR test set (**black solid line**) compared with MultiModN. P-Fusion is worse than random at 80% MNAR (AUROC: 0.385). In contrast, MultiModN only loses 10% in MNAR flip, remarkably, matching performance in a test with no missingness. Further plots in Appendix E.3.

Figure 4: **Inherent modality-specific model explainability in MultiModN. Heatmaps show individual modality contributions (IMC) (top) and cumulative contributions (CP) (bottom): respectively importance score (global explainability) or cumulative probability (local explainability). The multi-task MultiModN for \(task_{1-2}\) in MIMIC is compared to two single-task P-Fusion models. IMC are only possible for MultiModN (only 1 modality encoded, rest are skipped). CP are made sequentially from states encoding all previous modalities. P-Fusion is unable to naturally decompose modality-specific contributions (can only make predictions once all modalities are encoded). IMC is computed across all patients in the test set. CP is computed for a single patient, (true label = 0 for both \(task_{1-2}\)). The CP heatmap shows probability ranging from confident negative diagnosis (0) to perfect uncertainty and confident positive diagnosis (1).**

## 7 Conclusion

We present MultiModN, a novel sequential modular multimodal (MM) architecture, and demonstrate its distinct advantages over traditional monolithic MM models which process inputs in parallel.

By aligning the feature extraction pipelines between MultiModN and its baseline P-Fusion, we better isolate the comparison between modular-sequential MM fusion vs. monolithic-parallel MM fusion. We perform four experiments across 10 complex real-world MM tasks in three distinct domains. We show that neither the sequential modularization of MultiModN nor its extension to multi-task predictions compromise the predictive performance on individual tasks compared with the monolithic baseline implementation. Training a multi-task model can be challenging to parameterize across inter- and cross-task performance . We perform no specific calibration and show that MultiModN is robust to cross-task bias. Thus, at no performance cost, modularization allows the inherent benefits of multi-task modeling, as well as providing interepretable insights into the predictive potential of each modality. The most significant benefit of MultiModN is its natural robustness to catastrophic failure due to differences in missingness between train and test settings. This is a frequent and fundamental flaw of many domains and especially impacts low-resource settings where modalities may be missing for reasons independent of the missingness in the train set. More generally, modularization creates a set of self-contained modules, composable in any number or combination according to available inputs and desired outputs. This composability not only provides enormous flexibility at inference but also reduces the computational cost of deployment. Taken together, these features allow MultiModN to make resource-adapted predictions, which have a particular advantage for real-world problems in resource-limited settings.

**Limitations and future work.** The main limitation for studying MM modeling is the scarcity of large-scale, open-source, MM datasets that cover multiple real-world tasks, especially for time-series. Additionally, while MultiModN is theoretically able to handle any number or combination of modalities and tasks, this has not been empirically tested. Having a high combinatorial generalization comes at a computational and performance cost, where the'memory' of a fixed-size state representation will likely saturate at scale. The performance of MultiModN is purposely limited in this work by fixing the feature extraction pipeline, to best isolate the effect of sequential fusion. Future work leveraging MultiModN model-agnostic properties would be able to explore the potential performance benefit. This is particularly interesting for time-series, for which the state'memory' may need to be parameterized to capture predictive trends of varying shapes and lengths.

## 8 Acknowledgements

This project was substantially co-financed by the Swiss State Secretariat for Education, Research and Innovation (SERI).