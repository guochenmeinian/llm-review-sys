# UltraEdit: Instruction-based Fine-Grained Image Editing at Scale

Haozhe Zhao\({}^{1,3}\), Xiaojian Ma\({}^{2}\), Liang Chen\({}^{1,5}\), Shuzheng Si\({}^{4}\), Rujie Wu\({}^{5}\), Kaikai An\({}^{1,3}\), Peiyu Yu\({}^{6}\), Minjia Zhang \({}^{7}\), Qing Li\({}^{2}\), Baobao Chang\({}^{1}\)

\({}^{1}\)National Key Laboratory for Multimedia Information Processing, Peking University

\({}^{2}\)State Key Laboratory of General Artificial Intelligence, BIGAI

\({}^{3}\)School of Software and Microelectronics, Peking University

\({}^{4}\)Department of Computer Science and Technology, Tsinghua University

\({}^{5}\)School of Computer Science, Peking University, China, \({}^{6}\)UCLA, \({}^{7}\)UIUC

\({}^{}\)Equal contribution \(\) Corresponding author

mimazhe55360@gmail.com,{maxiaojian,liqing}@bigai.ai,chbb@pku.edu.cn

ultra-editing.github.io

###### Abstract

This paper presents UltraEdit, a large-scale (\(\)4M editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix  and MagicBrush , and provide a _systematic_ approach to producing massive and high-quality image editing samples. UltraEdit offers several distinct advantages: 1) It features a broader range of editing instructions by leveraging the creativity of large language models (LLMs) alongside in-context editing examples from human raters; 2) Its data sources are based on real images, including photographs and artworks, which provide greater diversity and reduced bias compared to datasets solely generated by text-to-image models; 3) It also supports region-based editing, enhanced by high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models are available in github.com/pkunlp-icler/UltraEdit.

This paper presents UltraEdit, a large-scale (\(\)4M editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix  and MagicBrush , and provide a _systematic_ approach to producing massive and high-quality image editing samples. UltraEdit offers several distinct advantages: 1) It features a broader range of editing instructions by leveraging the creativity of large language models (LLMs) alongside in-context editing examples from human raters; 2) Its data sources are based on real images, including photographs and artworks, which provide greater diversity and reduced bias compared to datasets solely generated by text-to-image models; 3) It also supports region-based editing, enhanced by high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models are available in github.com/pkunlp-icler/UltraEdit.

Figure 1: **Examples of UltraEdit. Free-form (left) and region-based (right) image editing.**Introduction

We present a new dataset, UltraEdit, for instruction-based image editing at scale (see Figure 1 for examples, more in Appendix B.2). Compared to several prior works on the same front [10; 60; 71], we venture into tackling some of their drawbacks, which prevent the model from robustly interpreting and executing instructions. We summarize our observations on their downsides below:

* **Limited instruction diversity.** Prior works have employed two strategies to produce editing instructions: 1) To use human raters, _e.g._, MagicBrush , which offer human-aligned and diverse instructions, but could be challenging to scale to a massive amount; 2) LLM-assisted generation, _e.g._, InstructPix2Pix , which is scalable but the instruction variety could be limited (bounded by the relevant capabilities of LLMs); thus making it hard to generalize to novel instructions.
* **Implicit biases in images.** Due to the challenges of obtaining massive instruction-based image editing data, previous research has widely adopted text-to-image (T2I) models [49; 48; 50; 51; 56; 24] to produce _both_ the source images and target (edited) images. However, many T2I models could suffer from implicit biases [32; 7; 43; 19; 9; 15; 66], _i.e._ they produce images subjected to certain domains or characteristics more than others. Therefore, the resulting image editing dataset could be quite unbalanced. When this dataset is used for training, the trained model's performance could suffer in image domains that T2I models do not cover well. For example, if a T2I model tends to generate cartoon-style images, models trained on the resulting data may perform poorly when editing images depicting natural scenes.
* **Missing of region-based editing.** Most of the existing datasets only consider free-form editing, _i.e._, the model is given a source image and an instruction only, while many of the actual image editing scenarios also involve a region where the editing is expected to happen (a "mask"). We will later show that such additional region guidance could significantly boost the editing performance. Therefore, missing such region data could hinder the quality of models trained on the dataset.

To address these limitations, we propose a _systematic_ approach to curate massive and high-quality image editing data automatically. An overview of our method can be found in Figure 2. Specifically, we begin by using LLMs along with in-context human-written instruction examples to ensure _diverse_ editing instructions. Next, we use prompt-to-prompt (P2P)  control with off-the-shelf T2I diffusion models to produce source and target (edited) images from captions and editing instructions. However, to _avoid the biases_ within the T2I models, we collect high-quality image-caption pairs from diverse real image datasets like COCO  and use these images as _anchors_ to guide the T2I models when producing source images given the corresponding caption. Additionally, we employ an automatic region generation approach to produce editing regions from the instruction and utilize such region annotations in a modified inpainting diffusion pipeline to produce _region-based editing_ samples. As a result, the final dataset, UltraEdit, comprises ~4M editing samples with ~750K unique instructions across 9+ editing types. To the best of our knowledge, UltraEdit is the largest instruction-based image editing dataset to be released to the public. With UltraEdit, canonical diffusion-based editing baselines enjoy new records on challenging MagicBrush and Emu-Edit benchmarks, respectively. Our analysis further confirms the crucial role of real image anchors and region-based editing data. To sum up, our contributions can be summarized as follows:

* We propose a novel automatic pipeline to generate image editing data, mitigating the issues of existing datasets: instruction diversity, image biases, and missing region-based editing data.
* With our pipeline, we curate UltraEdit, a large-scale and high-quality image editing dataset with diverse instructions, real images as anchors, and additional editing region annotations.
* We conduct extensive studies on how canonical diffusion-based image editing model can benefit from UltraEdit and provide insights and analysis on the key design principles.

## 2 The UltraEdit Dataset

### Dataset Formation

In UltraEdit, we consider two editing settings: 1) free-form editing, where the editing could happen at any area of the input image; 2) region-based editing, where the editing is expected to be about a certain region of the image, allowing more _fine-grained editing_. In all, our dataset can be formulated as a set of \( I_{s},I_{t},T_{e},I_{m},T_{s},T_{t}\), where \(I_{s}\) and \(I_{t}\) denote source and target (edited) image, respectively, \(T_{e}\) is the editing instruction, \(I_{m}\) denotes the additional editing region (mask). The model is expected to map editing input \(I_{s}\), \(T_{e}\) and mask \(I_{m}\) to \(I_{t}\). We also need the captions of source and target image \(T_{s}\), \(T_{t}\) for image generation and evaluation purposes. Below, we detail how to collect UltraEdit. An overview of our pipeline can be found in Figure 2.

Stage I: Instruction Generation.As we mentioned in Section 1, our key idea to address the issue of limited instruction diversity in existing datasets is to obtain high-quality editing instructions by combining LLM creativity and human raters. Specifically, we begin by manually creating hundreds of editing instructions with our human raters. The raters are first given images and captions from the COCO dataset  as context and asked to write appropriate editing instructions for these scenarios. We then invoke LLM to expand these human-written instructions into more diverse examples for editing in terms of semantics and editing tasks. We end up with ~10K instruction examples after the expansion. Details on the prompt and examples can be found in Appendix A.2.

Stage II: Free-form Editing Samples.We follow the main idea of prior work and invoke off-the-shelf T2I models to generate source and target images \(I_{s}\), \(I_{t}\) in editing samples. However, instead of synthesizing all images using T2I models as in prior works , UltraEdit adopts real images as anchors to mitigate the biases within these T2I models. Specifically, we first collect ~1.6M high-quality and diverse image-caption paired data from real image datasets like COCO , NoCaps , _etc_. (full details on the mix can be found in Appendix A.1). For each image-caption pair \( I^{},T_{s}\), we sample in-context examples from the instruction pool obtained in Stage I and prompt LLM to produce the editing instruction \(T_{e}\) and the resulting target caption \(T_{t}\) after the editing. We then invoke a regular Img2Img diffusion pipeline with the noise-perturbed latent embedding of \(I^{}\) as \(z_{T}\) (similar to SDEdit ) and the source caption \(T_{s}\) as a condition; therefore, the produced source image \(I_{s}\) will resemble the anchor \(I^{}\), _i.e_., we use diverse real images to guide the generation of T2I models, mitigating their biases. After producing \(I_{s}\), we invoke prompt-to-prompt (P2P) control  with the target caption \(I_{t}\) to produce the target image \(I_{t}\) on the same \(z_{T}\) from real image anchor \(I^{}\). Due to space limits, we refer the readers to the P2P paper  for details. We utilize SDXL-Turbo  as our diffusion backbone, which allows for high-quality generation with just 2-4 diffusion steps, maintaining a generation quality comparable to SDXL .

Figure 2: **The construction of UltraEdit.** (Upper) We use LLM with in-context examples to produce editing instructions and target captions given the collected image captions; (Middle) For free-form editing, we use the collected images as anchors, and invoke regular diffusion followed by prompt-to-prompt (P2P)  control to produce source and target images; (Bottom) For region-based editing, we first produce an editing region based on the instruction, then invoke a modified inpainting diffusion pipeline  to produce the images.

**Stage III: Region-based Editing Samples.** Upon the free-form editing samples, we create additional region-based editing data using an automatic editing region extraction method. Given an image-instruction pair \( I^{*},T_{e}\), we first detect all objects within \(I^{*}\) using recognize-anything  and prompt LLM with the object list, soruce caption \(T_{s}\), target caption\(T_{t}\) and editing instruction \(T_{e}\) to identify the object to be edited. Then we employ GroundingDINO  and SAM  to obtain bounding boxes and fine-grained mask of this target object. For edits involving transformation over the entire image (_e.g._, "turn this into an oil paint"), we mark the whole image as the editing region. We save an expanded version of the original mask (which becomes a contour) as the editing region \(I_{m}\). Finally, the bounding box and fine-grained mask will be fused into a soft mask (see Figure 2) to help smooth the transition between inpainting area and the rest of the image. While producing the source image \(I_{s}\) is the same as in Stage II, we adopt a modified inpainting pipeline to produce the target image \(I_{t}\) to take the editing region \(I_{m}\) into consideration:

\[z_{t-1}=(1-M)*z_{T}+M*DM(z_{t})&t 2==0\\ DM(z_{t})& \]

where \(M\) is \(I_{m}\) in the size of latent space, \(DM()\) denotes the diffusion model. In a nutshell, we alternate between regular diffusion and inpainting only within the mask region to guide the generation within the given region while avoiding edge artifacts, as illustrated in Figure 16. This pipeline is compatible with P2P control and the SDXL-Turbo backbone and it takes 3-7 diffusion steps to produce a target image. Further details can be found in Appendix A.3.

**Misc.** To ensure high-quality image generation, for each editing sample, we run the diffusion pipeline 100 times and filter out the deficient generations using a mixture of automatic metrics following the prior practices [10; 20] (detailed in Section 2.3). Thanks to the efficient SDXL-Turbo diffusion backbone and our implementation, our pipeline is ~100 times faster than prior work like .

### Characteristics and Statistics

Our dataset contains a total of 4,108,262 instruction-based image editing data (757,879 unique edits), where free-form image editing (without region annotation \(I_{m}\)) consists of 4,000,083 instances and region-based editing includes 108,179 samples. To the best of our knowledge, this is the largest dataset to be released to the public (a comparison to other datasets can be found in Table 1). As illustrated in Table 3, UltraEdit encompasses over 9 distinct editing instruction types, detailed in Table 2. The distribution of image editing instances across these types can be found in Appendix B.

### Quality Assessment

To ensure the quality of our dataset, we employ several automatic metrics to filter out substandard images during generation following prior practise [20; 10]. First, we utilize DINOv2 similarity, CLIP

  
**Datasets** & **Real Image** & **Automatic** & **Editing** & **\#Edits** & **\#Editing Types** & **Source Example** & **Instruction** & **Target Example** \\  EditBench  & & & & 240 & 1 & & & \\  & & & & & & & & \\ Magic-Brush  & & & & 10,388 & 5 & & & \\  & & & & & & & & \\ HQ-Edit  & & & & 197,350 & 6 & & & \\ InstrucrPix2Pix  & & & & 313,010 & 4 & & & \\ 
**UltraEdit** & & & & **4,108,262** & 9+ & & & \\   

Table 1: **Comparison of different image editing datasets**. Both EditBench and MagicBrush are manually annotated but are limited in size. InstructPix2Pix and HQ-Edit are large datasets automatically generated using T2I models like Stable Diffusion  and DALL-E , though they present notable biases from the generative models leading to failure cases. UltraEdit offers large-scale samples with rich editing tasks and fewer biases.

image similarity, and SSIM between source and target (edited) images \(I_{s}\) and \(I_{t}\) to guarantee that semantic similarity and pixel-level coherence can be maintained, which is crucial for image editing. Second, to ensure the generated images accurately reflect the editing instructions, we assess the alignment between the images \(I_{s}\),\(I_{t}\), and their corresponding captions \(T_{s}\),\(T_{t}\) using CLIP similarity. Finally, to verify the dataset's instruction-following capability, we employ CLIP Directional Similarity , which measures the alignment between changes in images and changes in corresponding captions. We provide these scores on UltraEdit in Table 3. Our dataset maintains a high standard of image quality and instruction alignment across both free-form and region-based editing data. Notably, region-based data are significantly better in terms of SSIM, confirming the benefits brought by region-based guidance (via our modified inpainting pipeline). Further quality assessments can be found in Appendix B and D.

## 3 Experiments

Our experiments evaluate the quality of UltraEdit in following user instructions faithfully while preserving the visual fidelity of the source image. First, we evaluate the performance of canonical diffusion-based editing models trained on our dataset across different instruction-based image editing benchmarks. Second, we dive into insights and analysis on the design principles of our dataset.

### Setup

**Settings.** We follow the settings of Emu Edit  to train a diffusion model using various scales and types of data from our dataset, then evaluate the trained model across multiple benchmarks to assess the effectiveness of our dataset in advancing its image editing capabilities. For a fair comparison, we adopt the editing diffusion model introduced in InstructPix2Pix , which uses Stable Diffusion v1.5  as the backbone diffusion model. We also employ training data volumes identical to that used in . To support region-based editing, we augment the model's U-Net to take in extra channels for region (mask) input. More details can be found in Appendix A.4.

**Baselines.** We set the following models as baselines, categorized into instruction-guided image editing methods (the same setting as ours) and global description-guided methods, the latter requires descriptions of the target image to perform editing. The instruction-guided image editing methods include: InstructPix2Pix , HIVE , MagicBrush , and Emu Edit . The global description-guided image editing methods include: Null Text Inversion , SD-SDEdit , GLIDE , and Blended Diffusion . Notably, GLIDE and Blended Diffusion require a **region mask** for editing.

  
**Type** & **Description** \\ 
**Add** & Inserting a new object or texture \\  & at a specific location in the image. \\
**Change Global** & Modifying the entire image to achieve a clear and noticeable effect. \\
**Change Local** & Altering a specific object or texture, affecting only a portion of the image. \\
**Change Color** & Adjusting the color within the image. \\
**Transform** & Smoothly transforming images into a different setting, some, or style. \\
**Transform Local** & Motifying part of image features while preserving its overall structure. \\
**Replace** & Substituting existing objects in the image with those specified in the instructions. \\
**Turn** & Implicitly changing objects, background, or texture, often without a specific target. \\
**Others** & Miscellaneous editing types such as text edits and altering quantities. \\   

Table 2: Editing Instruction Types in UltraEdit.

  
**Metric** & **Free-form.** & **Region-based.** \\  CLIPimg & 0.8427 & 0.8813 \\ SSIM & 0.6401 & 0.7413 \\ DINOv2 & 0.7231 & 0.7688 \\ CLIPin & 0.2834 & 0.2848 \\ CLIPout & 0.3049 & 0.2848 \\ CLIPdir & 0.2950 & 0.3052 \\   

Table 3: Quantitative evaluation for UltraEdit.

Figure 3: Distribution of edit types and keywords in the instructions of UltraEdit. The inner ring illustrates the various types of edit instructions, while the outer ring presents the frequency of instruction keywords. This visualization highlights the rich diversity found within our instructions.

**Benchmark and Metrics.** We evaluate the model trained on our dataset across two popular benchmarks: MagicBrush  and Emu Edit Test . MaigicBrush benchmark evaluates the model by comparing the edited images with **ground truth images** and corresponding captions across different metrics. Following the MagicBrush , we chose the L1 distance, L2 distance, CLIP image similarity, and DINO similarity as metrics. Emu Edit Test benchmark compares the edited images with the **source image** and target captions for evaluation. Consistent with the Emu Edit , we use L1 distance, CLIP image similarity, DINO similarity, CLIP text-image similarity, and CLIP text-image direction similarity as metrics. These metrics generally measure how the edited image both preserves the original's style and content and reflects modifications according to instructions. Details of the benchmark and metrics can be found in Appendix C.

### Main Result I: General Image Editing on MagicBrush

We present the results on MagicBrush benchmark in Table 4. Here are our main observations: 1) Compared to canonical image editing baselines like HIVE, SD-SDEdit, and IP2P (zero-shot), merely training an editing diffusion model on the free-form editing data of UltraEdit (**downsampled** to ~450K to match the training set size of IP2P, denoted as _Ours, trained w/o region data_) already attains significant improvement over the baseline, confirming the advantages brought by our dataset to general image editing; 2) When also considering the relatively small-scale region-based editing data (~100K region-based + ~350K free-form, denoted as _Ours_) and evaluate on the same setting without editing region input, the general editing performance can be boosted considerably, verifying the effectiveness of region-based editing data to image editing in general; 3) Finally, when being evaluated using region input, the model trained on both free-form and region-based editing data (still downsampled to ~450K in total) sets the new record on MagicBrush especially on the challenging multi-turn setting, demonstrate that our region-based editing data can indeed help with the emergence of region-based editing capability, while existing approaches that also utilize masks (GLIDE, Blended Diffusion) are poor at effectively guiding their editing with region input.

  
**Settings** & **Methods** & L1\(\) & L2\(\) & CLIP-I\(\) & DINO\(\) \\   \\  SD-SDEdit & 0.1014 & 0.0278 & 0.8526 & 0.7726 \\ Null Text Inversion & 0.0749 & 0.0197 & 0.8827 & 0.8206 \\  GLIDE & 3.4973 & 115.8347 & 0.9487 & 0.9206 \\ Blended Diffusion & 3.5631 & 119.2813 & 0.9291 & 0.8644 \\   \\  HIVE & 0.1092 & 0.0380 & 0.8519 & 0.7500 \\ InstructPix2Pix (IP2P) & 0.1141 & 0.0371 & 0.8512 & 0.7437 \\ IP2P w/ MagicBrush & 0.0625 & 0.0203 & **0.9332** & **0.8987** \\ Ours, trained w/o region data & 0.0689 & 0.0201 & 0.8986 & 0.8477 \\ Ours, eval w/o region & 0.0614 & 0.0181 & 0.9197 & 0.8804 \\ Ours, eval w/ region & **0.0575** & 0.0172 & 0.9307 & 0.8982 \\   \\  SD-SDEdit & 0.1616 & 0.0602 & 0.7933 & 0.6212 \\ Null Text Inversion & 0.1057 & 0.0335 & 0.8468 & 0.7529 \\  GLIDE & 11.7487 & 1079.5997 & 0.9094 & 0.8494 \\ Blended Diffusion & 14.5439 & 1510.2271 & 0.8782 & 0.7690 \\   \\  HIVE & 0.1521 & 0.0557 & 0.8004 & 0.6463 \\ InstructPix2Pix (IP2P) & 0.1345 & 0.0460 & 0.8304 & 0.7018 \\ IP2P w/ MagicBrush & 0.0964 & 0.0353 & 0.8924 & 0.8273 \\ Ours, trained w/o region data & 0.0883 & 0.0276 & 0.8685 & 0.7922 \\ Ours, eval w/o region & 0.0780 & 0.0246 & 0.8954 & 0.8322 \\ Ours, eval w/ region & **0.0745** & **0.0236** & **0.9045** & **0.8505** \\   

Table 4: **Results on the MagicBrush test set. We include both single-turn and multi-turn settings. We evaluate the models trained on UltraEdit without region-based editing data and full data (“Ours”). Models trained with full data are either evaluated with or without editing region as input.**

[MISSING_PAGE_FAIL:7]

edits (light CLIPdir and CLIPout) while preserving the content of the original image (lower L1), indicating that our model is mostly editing the image while considering the context, not just creating new contents. Our qualitative results (Figure 5) provide further evidence on this.

### Qualitative Evaluation

Data Generation.In Figure 4, we present qualitative examples generated by our data generation pipeline alongside various image editing methods for comparison. We evaluate our pipeline against several baselines, including image inversion methods like Null-text Inversion  and PnP Inversion ; inpainting methods such as BrushNet  and PowerPaint ; and zero-shot image editing methods like InfEdit  and MasaCtrl . Our data generation pipeline, leveraging SDXL-turbo, requires only 1-4 steps to produce a sample, which is significantly faster than other methods by

Figure 5: **Qualitative evaluation.** (Top) On four distinct tasks on Emu Edit, listed from top to bottom: background, color, global, style; (Bottom) Multi-turn editing on MagicBrush.

magnitudes. Importantly, this efficiency does not compromise the quality. Our pipeline demonstrates competitive performance against other baselines, highlighting the quality of UltraEdit.

Image Editing.In Figure 5, we provide some qualitative examples of different editing tasks on Emu Edit Test and multi-turn editing on MagicBrush generated by the Stable Diffusion v1.5 trained on UltraEdit. More examples can be found in Appendix D. Our main observations are: 1) most baselines, especially Emu Edit, tend to _overedit_ the image, _i.e_. creating content while ignoring the context of the source image. For example, Emu Edit makes crude modifications by setting the entire person's body as blank. We attribute these shortcomings to the biases introduced during the construction of Emu Edit's training data; 2) Many baselines also fail to generalize to novel instructions, _e.g_., blurring, adding special style, _etc_.; 3) For multi-turn editing, even the MagicBrush baselines cannot complete the long-term editing coherently, while our model, even without editing region input, can strictly follow the instruction, _e.g_. _one person_, _one bench_, etc, and reaches the best results with region information, which confirms the effectiveness of region-based data in UltraEdit.

### Insights and Analysis

In this section, we investigate how two of our designs when curating UltraEdit: _real image anchors_ and _region-based editing_ could affect the performances. All results are conducted on Emu Edit Test.

Real Image Anchors.We conduct an ablation study to verify the effectiveness of incorporating real images as anchors during data generation. We train the editing model using two distinct datasets: free-form image editing data from UltraEdit and the editing data generated using the same pipeline of UltraEdit without using real images as anchors, _i.e_. identical to the data creation pipeline of InstructPix2Pix . The models were trained on data volumes of 450K, 1M, and 1.5M for both datasets. The results can be found in Table 6. Our key observations: 1) Dataset generated with real image anchors generally leads to better models across all three scales; 2) The scaling effect only presents when real image anchors are adopted. We hypothesize that datasets without them could suffer from more severe image biases and therefore hinder the effect of further scaling up. In Appendix D.3, we demonstrate more qualitative results comparing the image editing generation method with and without using real images as anchors.

Free-from vs. Region-based Editing.We then explore the impact of incorporating region-based editing data during model training. We experiment with varying amounts of free-form editing data, ranging from 200K to 400K instances, and region-based editing data, ranging from 30K to 90K instances. We demonstrate the performances on Emu Edit Test of models trained on the possible volume combinations of these two types of data in Figure 6. It can be seen that: 1) Region-based editing data, despite its relatively smaller scale, can help with free-form editing tasks; 2) While scaling free-form editing data has a significant impact on the performance of region-based editing, we need to ensure a considerable volume of region-based data to ensure peak results. In Appendix D.4, we demonstrate more qualitative results with region input and the model exhibits significantly more precise

  
**Data Type** & **Data Volume** & **CLIPdir\(\)** & **CLIPimg\(\)** & **CLIPout\(\)** & **L1\(\)** & **DINO\(\)** \\   & 450k & 0.0823 & 0.8617 & 0.2778 & 0.0626 & 0.8190 \\  & 1M & 0.0925 & 0.8696 & 0.2807 & 0.0599 & 0.8307 \\  & 1.5M & 0.0952 & 0.8659 & 0.2808 & 0.0600 & 0.8243 \\   & 450k & 0.0728 & 0.8716 & 0.2796 & 0.0848 & 0.8154 \\  & 1M & 0.0638 & 0.8837 & 0.2770 & 0.0674 & 0.8353 \\   & 1.5M & 0.0720 & 0.8643 & 0.2781 & 0.0714 & 0.8105 \\   

Table 6: **Ablation study on real image anchors. The first three rows present results for models trained on different scales from the UltraEdit. The last three rows show results for models trained on data from the same generation pipeline but without using the real image as an anchor.**

Figure 6: **Ablations on region-based editing. We report CLIPout as the main metric.**

operations for background and localized edits, validating the effectiveness of the incorporation of the region-based editing.

## 4 Related Work

**Image Editing via Generation.**  Editing real photos according to specific instructions has long been a notable task in image processing [21; 16; 39; 72; 54; 45]. Powerful Large-scale diffusion models have significantly facilitated text-based image editing [29; 55; 33; 14; 40]. SDEdit  applies noise to the guidance image at an intermediate diffusion step and then denoises using the target description. Prompt-to-Prompt  injecting the input caption attention maps to the target ones. Null-Text Inversion  inverts the source image to the null-text embedding for editing, eliminating the need for original captions. Plug-and-Play  incorporates spatial features besides attention maps for better global image editing. Imagic  supports complex textual instructions via text embedding optimization and model fine-tuning. GLIDE  and Imagen Editor  fine-tuning the model to take channel-wise concatenation of the input image and mask. Blended Diffusion  blends the input image in the unmasked regions in the diffusion step. Meanwhile, instruction-based image editing has been introduced as a user-friendly method for image editing. Instructpix2pix  and HIVE  both are trained on generated editing data to handle user-written instructions during inference. MagicBrush  creates a manually-annotated dataset for fine-tuning Instructpix2pix. Emu Edit , trained on 10 million _proprietary_ multi-task data, demonstrates state-of-the-art performance.

**Image Editing Dataset.**  Table 1 compares various image editing datasets, revealing that high-quality data are scarce and challenging to obtain. The largest dataset only contains around 300,000 samples. EditBench  is manually curated with only 240 examples. MagicBrush manually annotated by hearing online labors using DALL-E 2  for crafting dataset, also limited in dataset size. For automatically annotated datasets, InstructPix2Pix utilizes prompt-to-prompt  to generate data pairs based on the captions form LAIION-Aesthetics  and edited captions generated by GPT-3 , but biases in the generative model and insufficient information in web-crawled captions make the generated image samples fail to represent the editing instructions, as shown in the Table 1. The the limited scope of the image instructions types of InstructPix2Pix further limits the utility of the dataset. HQ-Edit uses advanced models like GPT-4 [2; 1] and DALL-E 3  for generating image editing pairs, but may fail to preserve fine-grained details and realism in the target image.

## 5 Conclusion

We've presented UltraEdit, a large-scale, high-quality dataset for instruction-based image editing. We mitigate the issues in existing editing datasets with a _systematic_ approach for automatic data generation: combining LLM creativity and in-context examples from human raters for more diverse editing instructions; the use of real images as anchors for more balanced generations; support of region-based editing via automatic region extraction. Experiments on challenging MagicBrush and EmuEdit benchmarks confirm the effectiveness of training on our dataset. Possible future work includes further expanding the region-based editing data and bootstrapped training for image editing.