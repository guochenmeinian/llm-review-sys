# Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning

Guanlin Liu  Lifeng Lai

Department of Electrical and Computer Engineering

University of California, Davis

One Shields Avenue, Davis, CA 95616

{glnliu, lflai}@ucdavis.edu

###### Abstract

Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and the reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior information about the underlying environment and the agents' algorithms.

## 1 Introduction

Recently reinforcement learning (RL), including single agent RL and multi-agent RL (MARL), has received significant research interests, partly due to its many applications in a variety of scenarios such as the autonomous driving, traffic signal control, cooperative robotics, economic policy-making, and video games (Silver et al., 2016; Brown and Sandholm, 2019; Vinyals et al., 2019; Berner et al., 2019; Shalev-Shwartz et al., 2016; OroojlooyJadid and Hajinezhad, 2019; Baker et al., 2020; Zhang et al., 2021). In MARL, at each state, each agent takes its own action, and these actions jointly determine the next state of the environment and the reward of each agent. The rewards may vary for different agents. In this paper, we focus on the model of Markov Games (MG) (Shapley, 1953). In this class of problems, researchers typically consider learning objectives such as Nash equilibrium (NE), correlated equilibrium (CE) and coarse correlated equilibrium (CCE) etc. A recent line of works provide non-asymptotic guarantees for learning NE, CCE or CE under different assumptions (Sidford et al., 2020; Zhang et al., 2020; Bai and Jin, 2020; Xie et al., 2020; Liu et al., 2021; Jin et al., 2021; Mao and Basar, 2022).

As RL models, including single agent RL and MARL, are being increasingly used in safety critical and security related applications, it is critical to developing trustworthy RL systems. As a first step towards this important goal, it is essential to understand the effects of adversarial attacks on RL systems. Motivated by this, there have been many recent works that investigate adversarial attacks on single agent RL under various settings (Behzadan and Munir, 2017; Huang and Zhu, 2019; Ma et al., 2019; Zhang et al., 2020; Sun et al., 2021; Rakhsha et al., 2020, 2021).

On the other hand, except the ones that will be reviewed below, existing work on adversarial attacks on MARL is limited. In this paper, we aim to fill in this gap and systematically investigate the impact of adversarial attacks on online MARL. We consider a setting in which there is an attacker sits between the agents and the environment, and can monitor the states, the actions of the agents and the reward signals from the environment. The attacker is able to manipulate the feedback or action of the agents. The objective of the MARL learner is to learn an equilibrium. The attacker's goal is to force the agents to learn a target policy or to maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of the manipulation on feedback and action. Our contributions are follows.

1) We propose an adversarial attack model in which the attacker aims to force the agent to learn a policy selected by the attacker (will be called target policy in the sequel) or to maximize the cumulative rewards under some specific reward function chosen by the attacker. We use loss and cost functions to evaluate the effectiveness of the adversarial attack on MARL agents. The cost is the cumulative sum of the action manipulations and the reward manipulations. If the attacker aims to force the agents to learn a target policy, the loss is the cumulative number of times when the agent does not follow the target policy. Otherwise, the loss is the regret to the policy that maximizes the attacker's rewards. It is clearly of interest to minimize both the loss and cost.

2) We study the attack problem in three different settings: the white-box, the gray-box and the black-box settings. In the white-box setting, the attacker has full information of the underlying environment. In the gray-box setting, the attacker has no prior information about the underlying environment and the agents' algorithm, but knows the target policy that maximizes its cumulative rewards. In the black-box setting, the target policy is also unknown for the attacker.

3) We show that the effectiveness of action poisoning only attacks and reward poisoning only attacks is limited. Even in the white-box setting, we show that there exist some MGs under which no action poisoning only Markov attack strategy or reward poisoning only Markov attack strategy can be efficient and successful. At the same time, we provide some sufficient conditions under which the action poisoning only attacks or the reward poisoning only attacks can efficiently attack MARL algorithms. Under such conditions, we introduce an efficient action poisoning attack strategy and an efficient reward poisoning attack strategy, and analyze their cost and loss.

4) We introduce a mixed attack strategy in the gray-box setting and an approximate mixed attack strategy in the black-box setting. We show that the mixed attack strategy can force any sub-linear-regret MARL agents to choose actions according to the target policy specified by the attacker with sub-linear cost and sub-linear loss. We further investigate the impact of the approximate mixed attack strategy attack on V-learning (Jin et al., 2021), a simple, efficient, decentralized algorithm for MARL.

### Related works

**Attacks on Single Agent RL:** Adversarial attacks on single agent RL have been studied in various settings (Behzadan and Munir, 2017; Huang and Zhu, 2019; Ma et al., 2019; Zhang et al., 2020; Sun et al., 2021; Rakhsha et al., 2020, 2021). For example, (Behzadan and Munir, 2017; Zhang et al., 2020; Rangi et al., 2022) study online reward poisoning attacks in which the attacker could manipulate the reward signal before the agent receives it. (Liu and Lai, 2021) studies online action poisoning attacks in which the attacker could manipulate the action signal before the environment receives it. (Rangi et al., 2022) studies the limitations of reward only manipulation or action only manipulation in single-agent RL.

**Attacks on MARL:**(Ma et al., 2022) considers a game redesign problem where the designer knows the full information of the game and can redesign the reward functions. The proposed redesign methods can incentivize players to take a specific target action profile frequently with a small cumulative design cost. (Gleave et al., 2020; Guo et al., 2021) study the poisoning attack on multi-agent reinforcement learners, assuming that the attacker controls one of the learners. (Wu et al., 2022) studies the reward poisoning attack on offline multi-agent reinforcement learners.

**Defense Against Attacks on RL:** There is also recent work on defending against adversarial attacks on RL (Banihashem et al., 2021; Zhang et al., 2021; Lykouris et al., 2021; Chen et al., 2021; Wei et al., 2022; Wu et al., 2021). These work focus on the single-agent RL setting where an adversary can corrupt the reward and state transition.

Problem setup

### Definitions

To increase the readability of the paper, we first introduce some standard definitions related to MARL that will be used throughout of the paper. These definitions mostly follow those defined in (Jin et al., 2021). We denote a tabular episodic MG with \(m\) agents by a tuple \((,\{_{i}\}_{i=1}^{m},H,P,\{R_{i}\}_{i=1}^{m})\), where \(\) is the state space with \(||=S\), \(_{i}\) is the action space for the \(i^{}\) agent with \(|_{i}|=A_{i}\), \(H^{+}\) is the number of steps in each episode. We let \(:=(a_{1},,a_{m})\) denote the joint action of all the \(m\) agents and \(:=_{i}_{m}\) denote the joint action space. \(P=\{P_{h}\}_{h[H]}\) is a collection of transition matrices. \(P_{h}:\) is the probability transition function that maps state-action-state pair to a probability, \(R_{i,h}:\) represents the reward function for the \(i^{}\) agent in the step \(h\). In this paper, the probability transition functions and the reward functions can be different at different steps. We note that this MG model incorporates both cooperation and competition because the reward functions of different agents can be arbitrary.

**Interaction protocol:** The agents interact with the environment in a sequence of episodes. The total number of episodes is \(K\). In each episode \(k[K]\) of MG, the initial states \(s_{1}\) is generated randomly by a distribution \(P_{0}()\). Initial states may be different between episodes. At each step \(h[H]\) of an episode, each agent \(i\) observes the state \(s_{h}\) and chooses an action \(a_{i,h}\) simultaneously. After receiving the action, the environment generates a random reward \(r_{i,h}\) for each agent \(i\) derived from a distribution with mean \(R_{i,h}(s_{h},_{h})\), and transits to the next state \(s_{h+1}\) drawn from the distribution \(P_{h}(|s_{h},_{h})\). \(P_{h}(|s,)\) represents the probability distribution over states if joint action \(\) is taken for state \(s\). The agent stops interacting with environment after \(H\) steps and starts another episode. At each time step, the agents may observe the actions played by other agents.

**Policy and value function:** A Markov policy takes actions only based on the current state. The policy \(_{i,h}\) of agent \(i\) at step \(h\) is expressed as a mappings \(_{i,h}:_{_{i}}\). \(_{i,h}(a_{i}|s)\) represents the probability of agent \(i\) taking action \(a_{i}\) in state \(s\) under policy \(_{i}\) at step \(h\). A deterministic policy is a policy that maps each state to a particular action. For notation convenience, for a deterministic policy \(_{i}\), we use \(_{i,h}(s)\) to denote the action \(a_{i}\) which satisfies \(_{i,h}(a_{i}|s)=1\). We denote the product policy of all the agents as \(:=_{1}_{m}\). We also denote \(_{-i}:=_{1}_{i-1}_{i+1} _{m}\) to be the product policy excluding agent \(i\). If every agent follows a deterministic policy, the product policy of all the agents is also deterministic. We use \(V^{}_{i,h}:\) to denote the value function of agent \(i\) at step \(h\) under policy \(\) and define \(V^{}_{i,h}(s):=[_{h^{}=h}^{H}r_{i,h^{}}|s_{h }=s,]\). Given a policy \(\) and step \(h\), the \(i^{}\) agent's \(Q\)-function \(Q^{}_{i,h}:\) of a state-action pair \((s,)\) is defined as: \(Q^{}_{i,h}(s,)=[_{h^{}=h}^{H}r_{i,h^{} }|s_{h}=s,_{h}=,]\).

**Best response:** For any policy \(_{-i}\), there exists a best response of agent \(i\), which is a policy that achieves the highest cumulative reward for itself if all other agents follow policy \(_{-i}\). We define the best response of agent \(i\) towards policy \(_{-i}\) as \(^{}(_{-i})\), which satisfies \(^{}(_{-i}):=_{_{i}}V^{_{i}_{-i}}_{i,h}(s)\) for any state \(s\) and any step \(h\). We denote \(_{_{i}}V^{_{i}_{-i}}_{i,h}(s)\) as \(V^{,_{-i}}_{i,h}(s)\) for notation simplicity. By its definition, we know that the best response can always be achieved by a deterministic policy.

Nash Equilibrium (NE) is defined as a product policy where no agent can improve his own cumulative reward by unilaterally changing his strategy.

**Nash Equilibrium (NE) (Jin et al., 2021):** A product policy \(\) is a NE if for all initial state \(s\), \(_{i[m]}(V^{,_{-i}}_{i,1}(s)-V^{}_{i,1}(s))=0\) holds. A product policy \(\) is an \(\)-approximate Nash Equilibrium if for all initial state \(s\), \(_{i[m]}(V^{,_{-i}}_{i,1}(s)-V^{}_{i,1}(s))\) holds.

**General correlated policy:** A general Markov correlated policy \(\) is a set of \(H\) mappings \(:=\{_{h}:_{}\}_{h[H]}\). The first argument of \(_{h}\) is a random variable \(\) sampled from some underlying distributions. For any correlated policy \(=\{_{h}\}_{h[H]}\) and any agent \(i\), we can define a marginal policy \(_{-i}\) as a set of \(H\) maps \(_{i}=\{_{h,-i}:_{_{-i}} \}_{h[H]}\), where \(_{-i}=_{1}_{i-1} _{i+1}_{m}\). It is easy to verify that a deterministic joint policy is a product policy. The best response value of agent \(i\) towards policy \(_{-i}\) as \(^{}(_{-i})\), which satisfies \(^{}(_{-i}):=_{_{i}}V^{_{i}_{-i}}_{i,h}(s)\) for any state \(s\) and any step \(h\).

**Coarse Correlated Equilibrium (CCE)[Jin et al., 2021]:** A correlated policy \(\) is an CCE if for all initial state \(s\), \(_{i[m]}(V^{,_{-i}}_{i,1}(s)-V^{}_{i,1}(s))=0\) holds. A correlated policy \(\) is an \(\)-approximate CCE if for all initial state \(s\), \(_{i[m]}(V^{,_{-i}}_{i,1}(s)-V^{}_{i,1}(s))\) holds.

**Strategy modification:** A strategy modification \(_{i}\) for agent \(i\) is a set of mappings \(_{i}:=\{()^{h-1} _{i}_{i}\}_{h[H]}\). For any policy \(_{i}\), the modified policy (denoted as \(_{i}_{i}\)) changes the action \(_{i,h}(,s)\) under random sample \(\) and state \(s\) to \(_{i}((s_{1},_{1},,s_{h},a_{i,h}),_{i,h}(,s))\). For any joint policy \(\), we define the best strategy modification of agent \(i\) as the maximizer of \(_{_{i}}V^{(_{i}_{i})_{-i}}_{i,1}(s)\) for any initial state \(s\).

**Correlated Equilibrium (CE)[Jin et al., 2021]:** A correlated policy \(\) is an CE if for all initial state \(s\), \(_{i[m]}_{_{i}}(V^{(_{i}_{i})_{-i}}_{i,1}(s) )=0\). A correlated policy \(\) is an \(\)-approximate CE if for all initial state \(s\), \(_{i[m]}_{_{i}}(V^{(_{i}_{i})_{-i}}_{i,1}(s) )\) holds.

In Markov games, it is known that an NE is an CE, and an CE is an CCE.

**Best-in-hindsight Regret:** Let \(^{k}\) denote the product policy deployed by the agents for each episode \(k\). After \(K\) episodes, the best-in-hindsight regret of agent \(i\) is defined as \(_{i}(K,H)=_{^{}_{i}}_{k=1}^{K}[V^{^{}_{i}, ^{k}_{-i}}_{i,1}(s^{k}_{1})-V^{^{k}}_{i,1}(s^{k}_{1})]\).

### Poisoning attack setting

We are now ready to introduce the considered poisoning attack setting, in which there is an attacker sits between the agents and the environment. The attacker can monitor the states, the actions of the agents and the reward signals from the environment. Furthermore, the attacker can override actions and observations of agents. In particular, at each episode \(k\) and step \(h\), after each agent \(i\) chooses an action \(a^{k}_{i,h}\), the attacker may change it to another action \(^{k}_{i,h}_{i}\). If the attacker does not override the actions, then \(^{k}_{i,h}=a_{i}\). When the environment receives \(^{k}_{h}\), it generates random rewards \(r^{k}_{i,h}\) with mean \(R_{i,h}(s^{k}_{h},}^{k}_{h})\) for each agent \(i\) and the next state \(s^{k}_{h+1}\) is drawn from the distribution \(P_{h}(|s^{k}_{h},}^{k}_{h})\). Before each agent \(i\) receives the reward \(r^{k}_{i,h}\), the attacker may change it to another reward \(^{k}_{i,h}\). Agent \(i\) receives the reward \(^{k}_{i,h}\) and the next state \(s^{k}_{h+1}\) from the environment. Note that agent \(i\) does not know the attacker's manipulations and the presence of the attacker and hence will still view \(^{k}_{i,h}\) as the reward and \(s^{k}_{h+1}\) as the next state generated from state-action pair \((s^{k}_{h},^{k}_{h})\).

In this paper, we call an attack as _action poisoning only attack_, if the attacker only overtles the action but not the rewards. We call an attack as _reward poisoning only attack_ if the attacker only overtles the rewards but not the actions. In addition, we call an attack as _mixed attack_ if the attack can carry out both action poisoning and reward poisoning attacks simultaneously.

The goal of the MARL learners is to learn an equilibrium. On the other hand, the attacker's goal is to either force the agents to learn a target policy \(^{}\) of the attacker's choice or to force the agents to learn a policy that maximizes the cumulative rewards under a specific reward function \(R_{,h}:(0,1]\) chosen by the attacker. We note that this setup is very general. Different choices of \(^{}\) or \(R_{,h}\) could lead to different objectives. For example, if the attacker aims to reduce the benefit of the agent \(i\), the attacker's reward function \(R_{,h}\) can be set to \(1-R_{i,h}\), or choose a target policy \(^{}\) that is detrimental to the agent \(i\)'s reward. If the attacker aims to maximize the total rewards of a subset of agents \(\), the attacker's reward function \(R_{,h}\) can be set to \(_{i}R_{i,h}\), or choose a target policy \(^{}=_{i}V^{}_{i,1}(s_{1})\) that maximizes the total rewards of agents in \(\). We assume that the target policy \(^{}\) is deterministic and \(R_{i,h}(s,^{}(s))>0\). We measure the performance of the attack over \(K\) episodes by the total attack cost and the attack loss. Set \(()\) as the indicator function. The attack cost over \(K\) episodes is defined as \((K,H)=_{k=1}^{K}_{h=1}^{H}_{i=1}^{m} ^{k}_{i,h} a^{k}_{i,h})+|^{k}_{i,h}-r^{k }_{i,h}|\).

There are two different forms of attack loss based on the different goals of the attacker.

If the attacker's goal is to force the agents to learn a target policy \(^{}\), the attack loss over \(K\) episodes is defined as \((K,H)=_{k=1}^{K}_{h=1}^{H}_{i=1}^{m}(a^{ k}_{i,h}^{}_{i,h}(s^{k}_{i,h}))\).
\(_{k=1}^{K}[V^{^{*}}_{,1}(s^{k}_{1})-V^{^{*}}_{,1}(s^{k} _{1})]\). Here, \(V^{}_{,1}(s)\) is the expected cumulative rewards in state \(s\) based on the attacker's reward function \(R_{}\) under product policy \(\) and \(V^{^{*}}_{,1}(s)=_{}V^{}_{,1}(s)\). \(^{k}\) denote the product policy deployed by the agents for each episode \(k\). \(^{*}\) is the optimal policy that maximizes the attacker's cumulative rewards. We have Loss\(2(K,H) H*\) Loss\(1(K,H)\).

Denote the total number of steps as \(T=KH\). In the proposed poisoning attack problem, we call an attack strategy _successful_ if the attack loss of the strategy scales as \(o(T)\). Furthermore, we call an attack strategy _efficient and successful_ if both the attack cost and attack loss scale as \(o(T)\).

The attacker aims to minimize both the attack cost and the attack loss, or minimize one of them subject to a constraint on the other. However, obtaining optimal solutions to these optimization problems is challenging. As the first step towards understanding the attack problem, we show the limitations of the action poisoning only or the reward poisoning only attacks and then propose a simple mixed attack strategy that is efficient and successful.

Depending on the capability of the attacker, we consider three settings: the white-box, the gray-box and the black-box settings. The table below summarizes the differences among these settings.

## 3 White-box attack strategy and analysis

In this section, to obtain insights to the problem, we consider the white-box model, in which the attacker has full information of the underlying MG \((,\{_{i}\}_{i=1}^{m},H,P,\{R_{i}\}_{i=1}^{m})\). Even in the white-box attack model, we show that there exist some environments where the attacker's goal cannot be achieved by reward poisoning only attacks or action poisoning only attacks in Section 3.1. Then, in Section 3.2 and Section 3.3, we provide some sufficient conditions under which the action poisoning attacks alone or the reward poisoning attacks alone can efficiently attack MARL algorithms. Under such conditions, we then introduce an efficient action poisoning attack strategy and an efficient reward poisoning attack strategy.

### The limitations of the action poisoning attacks and the reward poisoning attacks

As discussed in Section 2, the attacker aims to force the agents to either follow the target policy \(^{}\) or to maximize the cumulative rewards under attacker's reward function \(R_{}\). In the white-box poisoning attack model, these two goals are equivalent as the optimal policy \(^{*}\) on the attacker's reward function \(R_{}\) can be calculated by the Bellman optimality equations. To maximize the cumulative rewards under attacker's reward function \(R_{}\) is equivalent to force the agents follow the policy \(^{}=^{*}\).

Existing MARL algorithms (Liu et al., 2021; Jin et al., 2021) can learn an \(\)-approximate {NE, CE, CCE} with \(}(1/^{2})\) sample complexities. To force the MARL agents to follow the policy \(^{}\), the attacker first needs to attack the agents such that the target policy \(^{}\) is the unique NE in the observation of the agents. However, this alone is not enough to force the MARL agents to follow the policy \(^{}\). Any other distinct policy should not be an \(\)-approximate CCE. The reason is that, if there exists an \(\)-approximate CCE \(\) such that \((^{}(s)|s)=0\) for any state \(s\), the agents, using existing MARL algorithms, may learn and then follow \(\), which will lead the attack loss to be \((T)=(KH)\). Hence, we need to ensure that any \(\)-approximate CCE stays in the neighborhood of the target policy. This requirement is equivalent to achieve the following objective: for all \(s\), and policy \(\),

\[_{i[m]}(^{,^{}_{ i}}_{ i,1}(s)-^{^{}}_{i,1}(s))=0;\] \[^{},_{i[m]}( ^{,_{-i}}_{i,1}(s)-^{}_{i,1}(s))>0;\] (1) \[(^{}(s^{})|s^{})=0s^{},_{i[m]}( ^{,_{-i}}_{i,1}(s)-^{}_{i,1}(s))>,\]

    & white-box attacker & gray-box attacker & black-box attacker \\  MG & Has full information & No information & No information \\ \(^{}\) & Can be calculated if \(R_{}\) given & Required and given & Not given \\ \(R_{}\) & Not required if \(^{}\) given & Not required if \(^{}\) given & Required and given \\ Loss1 & Suitable by specify \(^{}\) & Suitable & Not suitable \\ Loss2 & Suitable if \(R_{}\) given & Suitable if \(R_{}\) given & Suitable \\   

Table 1: Differences of the white/gray/black-box attackerswhere \(\) is the expected reward based on the post-attack environments.

We now investigate whether there exist efficient and successful attack strategies that use action poisoning alone or reward poisoning alone. We first show that the power of action poisoning attack alone is limited.

**Theorem 1**: _There exists a target policy \(^{}\) and a MG \((,\{_{i}\}_{i=1}^{m},H,P,\{R_{i}\}_{i=1}^{m})\) such that no action poisoning Markov attack strategy alone can efficiently and successfully attack MARL agents by achieving the objective in (1)._

We now focus on strategies that use only reward poisoning. If the post-attack mean reward \(\) is unbounded and the attacker can arbitrarily manipulate the rewards, there always exists an efficient and successful poisoning attack strategy. For example, the attacker can change the rewards of non-target actions to \(-H\). However, such attacks can be easily detected, as the boundary of post-attack mean reward is distinct from the boundary of pre-attack mean reward. The following theorem shows that if the post-attack mean reward has the same boundary conditions as the pre-attack mean reward, the power of reward poisoning only attack is limited.

**Theorem 2**: _If we limit the post-attack mean reward \(\) to have the same boundary condition as that of the pre-attack mean reward \(R\), i.e. \(\), there exists a MG \((,\{_{i}\}_{i=1}^{m},H,P,\{R_{i}\}_{i=1}^{m})\) and a target policy \(^{}\) such that no reward poisoning Markov attack strategy alone can efficiently and successfully attack MARL agents by achieving the objective in (1)._

The proofs of Theorem 1 and Theorem 2 are provided in Appendix F. The main idea of the proofs is as follows. In successful poisoning attacks, the attack loss scales as \(o(T)\) so that the agents will follow the target policy \(^{}\) in at least \(T-o(T)\) times. To efficiently attack the MARL agents, the attacker should avoid to attack when the agents follow the target policy. Otherwise, the poisoning attack cost will grow linearly with \(T\). The proofs of Theorem 1 and Theorem 2 proceed by constructing an MG and a target policy \(^{}\) where the expected rewards under \(^{}\) is always the worst for some agents if the attacker avoids to attack when the agents follow the target policy.

### White-box action poisoning attacks

Even though Section 3.1 shows that there exists MG and target policy such that the action poisoning only attacks cannot be efficiently successful, here we show that it can be efficient and successful for a class of target policies. The following condition characterizes such class of target policies.

**Condition 1**: _For the underlying environment MG \((,\{_{i}\}_{i=1}^{m},H,P,\{R_{i}\}_{i=1}^{m})\), the attacker's target policy \(^{}\) satisfies that for any state \(s\) and any step \(h\), there exists an action \(\) such that \(V_{i,h}^{^{}}(s)>Q_{i,h}^{^{}}(s,)\), for any agent \(i\)._

Under Condition 1, we can find a worse policy \(^{-}\) by

\[^{-}_{h}(s)= *{arg\,max}_{}_{i[m]} (V_{i,h}^{^{}}(s)-Q_{i,h}^{^{}}(s,))\ s.t. i[m],V_{i,h}^{^{ }}(s)>Q_{i,h}^{^{}}(s,).\] (2)

Under this condition, we now introduce an effective white-box action attack strategies: \(d\)-portion attack. Specifically, at the step \(h\) and state \(s\), if all agents pick the target action, i.e., \(=^{}_{h}(s)\), the attacker does not attack, i.e. \(}==^{}_{h}(s)\). If some agents pick a non-target action, i.e., \(^{}_{h}(s)\), the \(d\)-portion attack sets \(}\) as

\[}=^{}_{h}(s),d_{h}(s,)/m\\ ^{-}_{h}(s),1-d_{h}(s,)/m,\] (3)

where \(d_{h}(s,)=m/2+_{i=1}^{m}(a_{i}=^{}_{i,h}(s))/2\).

**Theorem 3**: _If the attacker follows the \(d\)-portion attack strategy on the MG agents, the best response of each agent \(i\) towards the target policy \(^{}_{-i}\) is \(^{}_{i}\). The target policy \(^{}\) is an {NE, CE, CCE} from any agent's point of view. If every state \(s\) is reachable at every step \(h[H]\) under the target policy, \(^{}\) is the unique {NE, CE, CCE}._The detailed proof can be found in Appendix G.1. Theorem 3 shows that the target policy \(^{}\) is the unique {NE, CE, CCE} under the \(d\)-portion attack. Thus, if the agents follow an MARL algorithm that is able to learn an \(\)-approximate {NE, CE, CCE}, the agents will learn a policy approximate to the target policy. We now discuss the high-level idea why the \(d\)-portion attack works. Under Condition 1, \(^{-}\)is worse than the target policy \(^{}\) at the step \(H\) from every agent's point of view. Thus, under the \(d\)-portion attack, the target action strictly dominates any other action at the step \(H\), and \(^{}\) is the unique {NE, CE, CCE} at the step \(H\). From induction on \(h=H,H-1,,1\), we can further prove that the \(^{}\) is the unique {NE, CE, CCE} at any step \(h\). We define \(_{i,h}^{-}(s)=Q_{i,h}^{^{}}(s,_{h}^{}(s))-Q _{i,h}^{^{}}(s,_{h}^{-}(s))\) and the minimum gap \(_{min}=_{h[H],s,i[m]}=_{i,h}^{-}(s)\). In addition, any other distinct policy is not an \(\)-approximate CCE with different gap \(<_{min}/2\). We can derive upper bounds of the attack loss and the attack cost when attacking some special MARL algorithms.

**Theorem 4**: _If the best-in-hindsight regret \((K,H)\) of each agent's algorithm is bounded by a sub-linear bound \((T)\) for any MG in the absence of attack, and \(_{s,i[m]}_{i,h}^{-}(s)_{h^{}= h+1}^{H}_{s,i[m]}_{i,h^{}}^{-}(s)\) holds for any \(h[H]\), then \(d\)-portion attack will force the agents to follow the target policy with the attack loss and the attack cost bounded by_

\[[(K,H)] 2m^{2}(T)/_{min},\ [(K,H)] 2m^{3} (T)/_{min}.\] (4)

### White-box reward poisoning attacks

As stated in Theorem 2, the reward poisoning only attacks may fail, if we limit the post-attack mean reward \(\) to satisfy the same boundary conditions as those of the pre-attack mean reward \(R\), i.e. \(\). However, similar to the case with action poisoning only attacks, the reward poisoning only attacks can be efficiently successful for a class of target policies. The following condition specifies such class of target policies.

**Condition 2:**_For the underlying environment MG \((,\{_{i}\}_{i=1}^{m},H,P,\{R_{i}\}_{i=1}^{m})\), there exists constant \(>0\) such that for any state \(s\), any step \(h\), and any agent \(i\), \((R_{i,h}(s,^{}(s))-)/(H-h)_{R}>0\) where \(_{R}=[_{s a h^{}}R_{i,h^{}}(s,a)-_{s  a h^{}}R_{i,h^{}}(s,a)]\)._

We now introduce an effective white-box reward attack strategies: \(\)-gap attack. Specifically, at the step \(h\) and state \(s\), if agents all pick the target action, i.e., \(=_{h}^{}(s)\), the attacker does not attack, i.e. \(_{i,h}=r_{i,h}\) for each agent \(i\). If agent \(i\) picks a non-target action, i.e., \(_{h}^{}(s)\), the \(\)-gap attack sets \(_{i,h}=R_{i,h}(s,^{}(s))-(+(H-h)_{R})(a_{i}_{i,h}^{}(s))\) for each agent \(i\). From Condition 2, we have \(_{i,h}\).

**Theorem 5**: _If the attacker follows the \(\)-gap attack strategy on the MG agents, the best response of each agent \(i\) towards any policy \(_{-i}\) is \(_{i}^{}\). The target policy \(^{}\) is the {NE, CE, CCE} from any agent's point of view. If every state \(s\) is reachable at every step \(h[H]\) under the target policy, \(^{}\) is the unique {NE, CE, CCE}._

The detailed proof can be found in Appendix H.1. Theorem 5 shows that the target policy \(^{}\) is the unique {NE, CE, CCE} under the \(\)-gap attack. Thus, if the agents follow an MARL algorithm that is able to learn an \(\)-approximate {NE, CE, CCE}, the agents will learn a policy approximate to the target policy. Here, we discuss the high-level idea why the \(\)-gap attack works. \(_{R}\) is the difference between the upper bound and the lower bound of the mean rewards. Condition 2 implies that each action is close to other actions from every agent's point of view. Although we limit the post-attack mean reward \(\) in \(\), the target policy can still appear to be optimal by making small changing to the rewards. Under Condition 2 and the \(\)-gap attacks, the target actions strictly dominates any other non-target actions by at least \(\) and any other distinct policy is not an \(\)-approximate CCE with different gap \(<\). Thus, \(^{}\) becomes the unique {NE, CE, CCE}. In addition, we can derive upper bounds of the attack loss and the attack cost when attacking MARL algorithms with sub-linear best-in-hindsight regret.

**Theorem 6**: _If the best-in-hindsight regret \((K,H)\) of each agent's algorithm is bounded by a sub-linear bound \((T)\) for any MG in the absence of attack, then \(\)-gap attack will force the agents to follow the target policy with the attack loss and the attack cost bounded by_

\[[(k,H)] m(T)/,\ [(K,H)]  m^{2}(T)/.\] (5)We note that proposed sufficient conditions (namely Condition 1 and Condition 2), under which the action poisoning only attacks or the reward poisoning only attacks can be efficient and successful, may be strict. They may not always hold in practice. This motivates us to investigate mixed attack strategy to be discussed in the sequel.

## 4 Gray-box attack strategy and analysis

In the gray-box attack setting, the attacker has no prior information about the underlying environment and the agents' algorithm, and it only observes samples generated when the agents interact with the environment. However, the attacker is given the target policy \(^{}\). Since the \(\)-gap reward attack strategy and \(d\)-portion action attack strategy described in Section 3 for the white-box setting rely on the information of the underlying environment, these two attack strategies are not applicable in the gray-box setting. In addition, without the information of the underlying environment, the attacker cannot check whether the action poisoning attack alone or the reward poisoning attack alone can be efficiently successful. Building on insights obtained from the white-box attack strategies, we develop a mixed attack strategy for MG in the gray-box attack setting.

In the proposed mixed attack strategy, at the step \(h\) and state \(s\), if agent \(i\) picks the target action, i.e., \(a_{i,h}=^{}_{i,h}(s)\), the attacker does not override the action and the reward, i.e. \(_{i,h}=a_{i,h}\) and \(_{i,h}=r_{i,h}\). If agent \(i\) picks a non-target action, i.e., \(a_{i,h}^{}_{i,h}(s)\), the attacker overrides its action \(_{i,h}=^{}_{i,h}(s)\) and then overrides the reward \(_{i,h}=0\).

**Theorem 7**: _If the attacker follows the mixed attack strategy the best response of each agent \(i\) towards any product policy \(_{-i}\) is \(^{}_{i}\). The optimal policy \(^{}\) is the unique {NE, CE, CCE}._

The detailed proof can be found in Appendix I.1. Here, we discuss the high-level idea why the mixed attack works. Under the mixed attacks, the state transitions are the same over the different actions and the reward of the non-target actions is worse than the target action. Thus, in the post-attack environment, the target policy is better than any other policy from any agent's point of view, and any other distinct policy is not an \(\)-approximate CCE with different gap \(<R_{min}\), where \(R_{min}=_{h[H]}_{s}_{i[m]}R_{i,h}(s,^{ }_{h}(s))\). Thus, \(^{}\) is the unique {NE, CE, CCE}. In addition, we can derive upper bounds of the attack loss and the attack cost when attacking some special MARL algorithms.

**Theorem 8**: _If the best-in-hindsight regret \((K,H)\) of each agent's algorithm is bounded by a sub-linear bound \((T)\) for any MG in the absence of attacks, then the mixed attacks will force the agents to follow the target policy \(^{}\) with the attack loss and the attack cost bounded by_

\[[(K,H)] m(T)/R_{min},\ [(K,H)] 2m (T)/R_{min}.\] (6)

## 5 Black-box attack strategy and analysis

In the black-box attack setting, the attacker has no prior information about the underlying environment and the agents' algorithm, and it only observes the samples generated when the agents interact with the environment. The attacker aims to maximize the cumulative rewards under some specific reward functions \(R_{}\) chosen by the attacker. But unlike in the gray-box case, the corresponding target policy \(^{}\) is also unknown for the attacker. After each time step, the attacker will receive the attacker reward \(r_{}\). Since the optimal (target) policy that maximizes the attacker's reward is unknown, the attacker needs to explore the environment to obtain the optimal policy. As the mixed attack strategy described in Section 4 for the gray-box setting relies on the knowledge of the target policy, it is not applicable in the black-box setting.

However, by collecting observations and evaluating the attacker's reward function and transition probabilities of the underlying environment, the attacker can perform an approximate mixed attack strategy. In particular, we propose an approximate mixed attack strategy that has two phases: the exploration phase and the attack phase. In the exploration phase, the attacker explores the environment to identify an approximate optimal policy, while in the attack phase, the attacker performs the mixed attack strategy and forces the agents to learn the approximate optimal policy. The total attack cost (loss) will be the sum of attack cost (loss) of these two phases.

In the exploration phase, the approximate mixed attack strategy uses an optimal-policy identification algorithm, which is summarized in Algorithm 1, listed in Appendix A. It will return an approximate optimal policy \(^{}\). Note that \(^{k}\) denotes the product policy deployed by the agents for each episode \(k\). \(\) is the upper bound of \(V^{^{*}}\) and \(\) is the lower bound of \(V^{^{k}}\). By minimizing \(-\), Algorithm 1 finds an approximate optimal policy \(^{}\). Here, we assume that the reward on the approximate optimal policy \(^{}\) is positive, i.e. \(R_{min}=_{h[H]}_{s}_{i[m]}R_{i,h}(s,^{ }_{h}(s))>0\). In the exploration phase, the attacker will override both the agents' actions and rewards.

After the exploration phase, the approximate mixed attack strategy performs the attack phase. The attacker will override both the agents' actions and rewards in this phase. At the step \(h\) and state \(s\), if agent \(i\) picks the action \(^{}_{i,h}(s)\), the attacker does not override actions and rewards, i.e. \(_{i,h}=a_{i,h}\) and \(_{i,h}=r_{i,h}\). If agent \(i\) picks action \(a_{i,h}^{}_{i,h}(s)\), the attacker overrides the action \(_{i,h}=a_{i,h}\) and then overrides the reward \(_{i,h}=0\). The attack strategy in the attack phase is same with the mixed attack strategy. From Theorem 7, in the attack phase, the best response of each agent \(i\) towards product policy \(^{}_{-i}\) is \(^{}_{i}\) and \(^{}\) is the unique NE. Here, we discuss the high-level idea why the approximate mixed attack works. The attacker finds an approximate optimal policy \(^{}\) by Algorithm 1. If \(^{*}\) is close to \(^{}\) and the exploration phase is sub-linear time dependent, the performance of the approximate mixed attack strategy will be close to the mixed attack strategy. We build a confidence bound to show the value function difference between \(^{*}\) and \(^{}\) in the following lemma.

**Lemma 1**: _If the attacker follows the Algorithm 1 in Appendix A on the agents, for any \((0,1)\), with probability at least \(1-5\), the following bound holds:_

\[_{s_{1} P_{0}()}[V^{^{*}}_{,1}(s_{1})-V^{^{ }}_{,1}(s_{1})] 2H^{2}S.\] (7)

We now investigate the impact of the approximate mixed attack strategy attack on V-learning [Jin et al., 2021], a simple, efficient, decentralized algorithm for MARL. The reader's convienience, we list V-learning in Appendix J.2.

**Theorem 9**: _Suppose ADV_BANDIT_UPDATE of V-learning follows Algorithm 3 in Appendix J.2 and it chooses hyper-parameter \(w_{t}=_{t}(_{i=2}^{t}(1-_{i}))^{-1}\), \(_{t}=}\) and \(_{t}=\). For given \(K\) and any \((0,1)\), let \(=(mHSAK/)\). The attack loss and the attack cost of the approximate mixed attack strategy during these \(K\) episodes are bounded by_

\[&[(K,H)] H+ }mASK}+2H^{2}SK,\\ &[(K,H)] 2mH+}ASK}.\] (8)

_Let \(\) be the executing output policy of V-learning, the attack loss of the executing output policy \(\) is upper bounded by_

\[V^{^{*}}_{,1}(s_{1})-V^{}_{,1}(s_{1})}A}{K}}+S}{K}+2H^{2}S .\] (9)

If we choose the stopping time of the exploration phase \(=K^{2/3}\), the attack loss and the attack cost of the approximate mixed attack strategy during these \(K\) episodes are bounded by \((K^{2/3})\) and \(V^{^{*}}_{,1}(s_{1})-V^{}_{,1}(s_{1})(K^{-1/3})\).

## 6 Numerical Results

In this section, we empirically compare the performance of the action poisoning only attack strategy (\(d\)-portion attack), the reward poisoning only attack strategy (\(\)-gap attack) and the mixed attack strategy.

We consider a simple case of Markov game where \(m=2\), \(H=2\) and \(||=3\). This Markov game is the example in Appendix F.2. The initial state is \(s_{1}\) at \(h=1\) and the transition probabilities are:

\[& P(s_{2}|s_{1},a)=0.9,P(s_{3}|s_{1},a)=0.1,a=(),\\ & P(s_{2}|s_{1},a)=0.1,P(s_{3}|s_{1},a)=0.9,a().\] (10)The reward functions are expressed in the following Table 2.

We set the total number of episodes \(K=10^{7}\). We set two different target policies. For the first target policy, no action/reward poisoning Markov attack strategy alone can efficiently and successfully attack MARL agents. For the second target policy, the \(d\)-portion attack and the \(\)-gap attack can efficiently and successfully attack MARL agents.

Case 1.The target policy is that the two agents both choose to defect at any state. As stated in Section 3 and Appendix 3.1, the Condition 1 and Condition 2 do not hold for this Markov game and target policy, and no action/reward poisoning Markov attack strategy alone can efficiently and successfully attack MARL agents.

In Figure 1, we illustrate the mixed attack strategy, the \(d\)-portion attack strategy and the \(\)-gap attack strategy on V-learning agents for the proposed MG. The \(x\)-axis represents the episode \(k\) in the MG. The \(y\)-axis represents the cumulative attack cost and attack loss that change over time steps. The results show that, the attack cost and attack loss of the mixed attack strategy sublinearly scale as \(T\), but the attack cost and attack loss of the \(d\)-portion attack strategy and the \(\)-gap attack strategy linearly scale as \(T\), which is consistent with our analysis.

Case 2.The target policy is that the two agents choose to cooperate at state \(s_{1}\) and \(s_{2}\) but to defect at state \(s_{3}\). As stated in Section 3 and Appendix 3.1, the Condition 1 and Condition 2 hold for this Markov game and target policy. Thus, the \(d\)-portion attack strategy and the \(\)-gap attack strategy can efficiently and successfully attack MARL agents.

In Figure 2, we illustrate the mixed attack strategy, the \(d\)-portion attack strategy and the \(\)-gap attack strategy on V-learning agents for the proposed MG. The results show that, the attack cost and attack loss of all three strategies sublinearly scale as \(T\), which is consistent with our analysis. Additional numerical results that compare the performance of the mixed attack strategy and the approximate mixed attack strategy are provided in Appendix B.

## 7 Conclusion

In this paper, we have introduced an adversarial attack model on MARL. We have discussed the attack problem in three different settings: the white-box, the gray-box and the black-box settings. We have shown that the power of action poisoning only attacks and reward poisoning only attacks is limited. Even in the white-box setting, there exist some MGs, under which no action poisoning only attack strategy or reward poisoning only attack strategy can be efficient and successful. We have then characterized conditions when action poisoning only attacks or only reward poisoning only attacks can efficiently work. We have further introduced the mixed attack strategy in the gray-box setting that can efficiently attack any sub-linear-regret MARL agents. Finally, we have proposed the approximate mixed attack strategy in the black-box setting and shown its effectiveness on V-learning. This paper raises awareness of the trustworthiness of online multi-agent reinforcement learning. In the future, we will investigate the defense strategy to mitigate the effects of this attack.

   state \(s_{1}\) & Cooperate & Defect \\  Cooperate & (1, 1) & (0.5, 0.5) \\  Defect & (0.5, 0.5) & (0.2, 0.2) \\       state \(s_{2}\) & Cooperate & Defect \\  Cooperate & (1, 1) & (0.5, 0.5) \\  Defect & (0.5, 0.5) & (0.1, 0.1) \\    
   state \(s_{3}\) & Cooperate & Defect \\  Cooperate & (1, 1) & (0.5, 0.5) \\  Defect & (0.5, 0.5) & (0.9, 0.9) \\   

Table 2: Reward matrices

Figure 1: The attack loss (cost) on case 1. Figure 2: The attack loss (cost) on case 2.

Acknowledgement

This work was supported by the National Science Foundation under Grant CCF-22-32907.