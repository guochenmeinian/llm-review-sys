# Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems

Jason Hu

Bowen Song

Xiaojian Xu

Liyue Shen

Jeffrey A. Fessler

Department of Electrical and Computer Engineering

University of Michigan

Ann Arbor, MI 48109

{jashu, bowenbw, xjxu, liyues, fessler}@umich.edu

###### Abstract

Diffusion models can learn strong image priors from underlying data distributions and use them to solve inverse problems, but the training process is computationally expensive and requires lots of data. Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images. This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images. Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and use this as the prior for solving inverse problems. We show that this diffusion model achieves improved memory efficiency and data efficiency while still maintaining the ability to generate entire images via positional encoding. Additionally, the proposed PaDIS model is highly flexible and can be paired with different diffusion inverse solvers (DIS). We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors. Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior. Code: https://github.com/sundeco/PaDIS

## 1 Introduction

Diffusion models learn the prior of an underlying data distribution and can use the prior to generate new images . By starting with a clean training images and gradually adding higher levels of noise, eventually obtaining images that are indistinguishable from pure noise, the score function of the image distribution, denoted \((x)= p()\), can be learned by a neural network. The reverse process (sampling or generation) then starts with pure noise and uses the learned score function to iteratively remove noise, ending with a clean image sampled from the underlying distribution \(p()\).

Inverse problems are ubiquitous in image processing, and aim to reconstruct an image \(\) from a measurement \(\), where \(=()+\), \(\) represents a forward operator, and \(\) represents random unknown noise. By Bayes rule, \(p(|)\) is proportional to \(p() p(|)\). Hence, to recover \(\), it is important to have a good estimate of the prior \(p()\), particularly when \(\) contains far less information than \(\). Diffusion models are known for their ability to learn a strong prior, so there is a growing literature on using them to solve inverse problems .

However, diffusion models require large quantities of training data and vast computational power to be able to generate high resolution images; Song et al.  and Ho et al.  used several days to weeks of training on over a million training images in the ImageNet  and LSUN  datasets to generate \(256 256\) images. This high cost motivates the research on improved training efficiency for diffusion models, such as fine-tuning an existing checkpoint on a different dataset [11; 12] to reduce the required training time and data. However, this strategy restricts the range of usable network architectures and requires the existence of a pretrained network, which limits the range of applications. Besides the demanding training data and computational cost, diffusion models also struggle in very large-scale problems, such as very high resolution images or 3D images. To address these challenges, latent diffusion models [13; 14] have been proposed to learn the image distribution in a smaller latent space, but it is difficult to solve general inverse problems in the latent space . Patch-based diffusion models have also been proposed to reduce computational cost. For example, Wang et al.  trained on patches of images, but for image generation, ultimately still relied on computing the score function of the whole image at once. Ding et al.  used patches in the feature space, requiring an additional encoding and decoding step. For 3D volumes, the most common method involves breaking up the volume into 2D slices , . These methods add regularizers between slices to enforce consistency during sampling, and thus do not provide a self-contained method for computing the score function of the whole volume. These application-specific strategies make it difficult to adapt these methods to general purpose inverse problem solvers using diffusion models , , , .

Our proposed method tackles these challenges in a unified way by training diffusion models on patches of images, as opposed to whole images (see Fig. 1). We provide the location of the randomly extracted patch to the network to help it learn global image properties. Since each training image contains many patches, the required size of the training dataset is greatly reduced, from the millions usually needed to generate high quality images to only a couple thousand or even several hundred (see Tab. 5). The required memory and training time is also reduced because it is never necessary to backpropagate the whole image through the network. Our proposed method allows for a flexible network architecture and only requires it to accept images of any size, a property true of many UNets , so there is much more flexibility in the architecture design than fine-tuning methods.

At inference time (see Fig. 2), by first zero padding the image, the proposed approach partitions it into patches in many different ways (see Fig. 3), eliminating the boundary artifacts between patches that would appear if non-overlapping patches were used. We develop a method to express the distribution of the whole image in terms of the patch distribution that is learned by the proposed patch-based

Figure 1: Training the proposed Patch Diffusion Inverse Solver (PaDIS) method. Different sized patches are used in each training iteration. The X and Y position arrays have the same size as the patch and consist of the normalized X and Y coordinates of each pixel of the patch.

network. By incorporating positional information of patches, this framework allows us to compute the score function of the whole image without ever inputting the whole image into the network. Unlike previous patch-based works that may be task-specific , the prior defined by our approach may be treated in a black box manner and then paired with any other stochastic differential equation (SDE) solver to sample from the prior, or with any general purpose inverse problem solver to perform image reconstruction. We conduct experiments on multiple datasets and different inverse problems and demonstrate that the proposed method is able to synthesize the patches to produce reasonably realistic images and very accurate reconstructions for inverse problems. Furthermore, PaDIS provides a promising avenue for which generation and inverse problem solving of very large and high dimensional images may be tackled in the future.

In summary, our main contributions are as follows:

* We provide a theoretical framework whereby a score function of a high-resolution high-dimensional image is learned purely through the score function of its patches.
* The proposed method greatly reduces the amount of memory and training data needed compared to traditional diffusion models.
* The trained network has great flexibility and can be used with many existing sampling algorithms and is the first patch-based model that can solve inverse problems in an unsupervised manner.
* We perform experiments on a variety of inverse problems to show superior image quality over whole image diffusion model methods while being far less resource heavy.

## 2 Background and Related Work

Diffusion models.Diffusion models consist of defining a forward stochastic differential equation (SDE) that adds noise to a clean image : for \(t[0,T]\), \((t)^{d}\), we have

\[=-((t)/2)\,\,t+\,,\] (1)

where \((t)\) is the noise variance schedule of the process. The distribution of \((0)\) is the data distribution and the distribution of \((T)\) is (approximately) a standard Gaussian. Then, image generation is done through the reverse SDE :

\[=(-(t)/2-(t)_{_{t}} p_{t}(_{t} ))\,\,t+d}.\] (2)

By training a neural network to learn the score function \(_{_{t}} p_{t}(_{t})\), one can start with noise and run the reverse SDE to obtain samples from the learned data distribution.

Figure 2: Overview of reconstruction process for the proposed Patch Diffusion Inverse Solver (PaDIS) method. Starting at \(t=T\), at each iteration we choose a random partition of the zero padded image and use the neural network trained on patches to get the score function of the entire image. Due to the shifting patch locations, the output image has no boundary artifacts.

To reduce the computational burden, latent diffusion models  have been proposed, aiming to perform the diffusion process in a much smaller latent space, allowing for faster training and sampling. However, that method requires a pretrained encoder and decoder  for a fixed dataset, so it must be retrained for different datasets, and it still requires large amounts of training data. Patch-based diffusion models [16; 17] focus on image generation while training only on patches. Supervised patch-based diffusion methods [23; 26] are task specific and do not learn an unconditional image prior that can be applied to all inverse problems. Other patch-based methods [27; 28; 29] learn an unconditional image prior but require the whole image as an input during inference time. Finally, work has been done to perform sampling faster [30; 31; 14], which is unrelated to the training process.

Solving inverse problems.For most real-world inverse problems, the partial measurement \(\) is corrupted and incomplete, so the mapping from \(\) to \(\) is many-to-one, even in the absence of noise, making it impossible to exactly recover \(\). Hence, it is necessary to enforce a prior on \(\). Traditionally, methods such as total variation (TV)  and wavelet transform  have been used to enforce image sparsity. To capture more global image information, low-rank methods are also popular [34; 35; 36; 37]. These methods frequently involve solving an optimization problem that simultaneously enforces data consistency and the image prior.

In recent years, data-driven methods have risen in popularity in signal and image processing [38; 39; 40; 41; 42]. In particular, for solving inverse problems, when large amounts of training data is available, a learned prior can be much stronger than the handcrafted priors used in traditional methods. For instance, plug and play and regularized by denoising methods [43; 44; 45; 46; 47; 48; 49] involve pretraining a denoiser and applying it at reconstruction time to iteratively recover the image. These methods have the advantage over supervised deep learning methods such as [50; 51; 52; 53] that the same denoiser may be applied to solve a wide variety of inverse problems.

Diffusion models serve as an even stronger prior as they can generate entire images from pure noise. Most methods that use diffusion models to solve inverse problems involve writing the problem as a conditional generation problem [54; 55; 56] or as a posterior sampling problem [57; 4; 5; 6; 7]. In the former case, the network requires the measurement \(\) (or an appropriate resized transformation of \(\)) during training time. Thus, for these task-specific trained models, at reconstruction time, that network is useful only for solving that specific inverse problem. In contrast, for the posterior sampling framework, the network learns an unconditional image prior for \(\) that can help solve any inverse problem related to \(\) without retraining. Our proposed method may be paired with most existing posterior sampling algorithms [58; 5; 6; 7].

## 3 Methods

To be able to solve large 2D imaging problems as well as 3D and 4D inverse problems, our goal is to develop a model for \(p()\) that does not require inputting the entire image \(\) into any neural network. We would like to simply partition \(\) into nonoverlapping patches, learn the distribution of each of the patches, and then piece them together to obtain the distribution of \(\). However, this would result in boundary artifacts between the patches. Directly using overlapping patches would result in sections of the image covered by multiple patches to be updated multiple times, which is inconsistent with the theory of diffusion models. Ideally, we would like to use nonoverlapping patches to update \(\), but with a variety of different patch tiling schemes so that boundaries between patches do not remain the same through the entire diffusion process.

To accomplish this task, we zero pad \(_{0}\) and learn the distribution of the resulting zero padded image. More precisely, consider an \(N N\) image \(_{0}\) and let \(1 P<N\) be an integer denoting the patch size and let \(k= N/P\). Then \(_{0}\) could be covered with a \((k+1)(k+1)\) nonoverlapping patch grid but that would result in \((k+1)P-N\) additional rows and columns for the patches. Hence, we zero pad \(_{0}\) on all four sides by \(M=(k+1)P-N\) to form a new image with \(N+2M\) rows and columns. With slight abuse of notation, we also denote this larger image by \(\). Let \(i,j\) be positive integers between 1 and \(M\) inclusive. Fig. 3 illustrates that we may partition \(\) into \((k+1)^{2}+1\) regions as follows: \((k+1)^{2}\) of the regions consist of evenly chopping up the square consisting of rows \(i\) through \(i+N+P\) and columns \(j\) through \(j+N+P\) into a \(k+1\) by \(k+1\) grid, with each such partition being \(P P\), and the last region consists of the remaining bordering part of \(\) that is not included in the first \((k+1)^{2}\) regions. This last region will always be entirely zeros, and the \((k+1)^{2}\) square patches fully cover the central \(N N\) image.

Each pair of integers \(i\) and \(j\) correspond to one possible partition, so when we let \(i\) and \(j\) range through all the possible values, our proposal is to model the distribution of \(\) as the following product distribution:

\[p()=_{i,j=1}^{i,j=M}p_{i,j,B}(_{i,j,B})_{r=1}^{(k+1)^{2}}p _{i,j,r}(_{i,j,r})/Z,\] (3)

where \(_{i,j,B}\) represents the aforementioned bordering region of \(\) that depends on the specific values of \(i\) and \(j\), \(p_{i,j,B}\) is the probability distribution of that region, \(_{i,j,r}\) is the \(r\)th \(P P\) patch when using the partitioning scheme corresponding to the values of \(i\) and \(j\), \(p_{i,j,r}\) is the probability distribution of that region, and \(Z\) is an appropriate scaling factor. Generative models based on products of patch probabilities have a long history in the Markov random field literature; see SSA.6.

The score function of the entire image follows directly from (3):

\[ p()=_{i,j=1}^{i,j=M}(_{i,j,B}(_{i,j,B})+_{r=1}^{(k+1)^{2}}_{i,j,r}(_{i,j,r}) ).\] (4)

Thus, we have expressed the score function of the whole image \(\) as sums of scores of patches \(_{i,j,r}\) and the border \(_{i,j,B}\). The former can be learned through score matching as in SS3.1. For the latter, by construction, if \(\) is a zero padded image then \(_{i,j,B}\) is everywhere zero. Thus, for all \(\), \(D(_{i,j,B})=[_{i,j,B},t=0|_{i,j,B},t=T]\) is everywhere zero, where \(D\) represents the denoiser function. Then the computation of its score function is trivial by Tweedie's formula .

Importantly, unlike previous papers like  and , our method can compute the score function of the entire image through only inputs of patches to the neural network. This makes it possible to learn a black box function for score functions of large images, where directly training a diffusion model would be infeasible due to memory and time constraints. Furthermore, SS4 shows that in the case of limited data, the large number of patches present in each training image allows the patch-based model to learn a model for the underlying distribution that performs better than whole image models.

### Patch-wise training

Following the work in  and , we train a denoiser using the score matching approach. Our neural network \(D_{}(,_{t})\) accepts the noisy image \(\) and a noise level \(_{t}\), and is trained through the following loss function:

\[_{t(0,T)}_{ p()}_{(0,_{t}^{2}I)}\|D_{}(+,_{t})-\|_{2}^{2}.\] (5)

By Tweedie's formula , the score function is given by \(s_{}(,_{t})=(D_{}(,_{t})-)/_{ t}^{2}\). Here, we want to learn the score function of patches \(_{i,j,r}\), so we apply (5) to patches of \(\) instead of the entire image. Following , we extract patches randomly from the zero-padded image \(\). To incorporate the positional information of the patch, we first define the \(x\) positional array as the size \(N+2M\) 2D array consisting of the \(x\) positions of each pixel of the image scaled between -1 and 1; the \(y\) positional array is similarly defined. We then extract the corresponding patches of these two positional arrays and concatenate them along the channel dimension of the noisy image patch as inputs into the network, nothing that noise is not added to the positional arrays.

When directly applying (5), it would suffice to fix the patch size \(P\) and then train using size \(P\) patches exclusively. However,  found that by training with patches of varying sizes, training time can be reduced and the neural network learns cross-region dependencies better. Hence, we train both with patches of size \(P\) and also patches of _smaller_ size, where the size is chosen according to a stochastic scheduling scheme. By using the UNet architecture in , the same network can take images of different sizes as input. The Appendix provides details of the experiments.

Figure 3: Schematic for zero padding and partitioning image into patches

### Sampling and reconstruction

The proposed patch-based method for learning \(p()\) may be paired with any method that would otherwise be used for sampling with a full image diffusion model, such as Langevin dynamics  and DDPM , as well as acceleration methods such as second-order solvers  and DDIM . At training time, the network only receives patches of the image as input, along with the positional information of the patch. Nevertheless, we show that when the number of sampling iterations is sufficiently large, the proposed method is still capable of generating reasonably realistic appearing entire images. However, our main goal is solving large-scale inverse problems, not image generation.

Computing \(()\) via (4) would require summing over the score functions of all \((k+1)^{2}\) patches a total of \(M^{2}\) times (corresponding to the \(M^{2}\) different ways of selecting \(i\) and \(j\)). This method would be prohibitively slow due to the size of \(M^{2}\); instead, for each iteration we randomly choose integers \(i\) and \(j\) between 1 and \(M\) and estimate \(()\) using just that corresponding term of the outer summation.

For solving inverse problems with diffusion models, there are general algorithms e.g.,  and , as well as more model-specific algorithms, e.g.,  and . Here, we demonstrate that our method applies to a broad range of inverse problems, and opt to use generalizable methods that do not rely on any special properties (such as the singular value decomposition of the system matrix as in , ) of the forward operator. We found that DPS  in conjunction with Langevin dynamics generally yielded the most stable and high quality results, so we use this approach as our central algorithm. Similar to , we chose the stepsize to be \(_{i}=/\|-(D())\|_{2}\). To the best of our knowledge, this is the first work that learns a fully patch-based diffusion prior and applies it to solve inverse problems; we call our method **P**atch **D**iffusion **I**nverse **S**olver (PaDIS). Computing the score functions of the patches at each iteration is easily parallelized, allowing for fast sampling and reconstruction. Alg. 1 shows the pseudocode for the main image reconstruction algorithm; the appendix shows the pseudocode for the other implemented algorithms.

Finally, we comment on some high-level similarities between our proposed method and ; in both cases, the image in question is partitioned into smaller parts in multiple ways. In , one of the partitions consists of 2D slices in the x-y direction, and the other partition consists of 2D slices in the x-z direction, whereas for our method, each of the partitions consists of \((k+1)^{2}\) square patches and the outer border region. For both methods, the score functions of each of the parts are learned independently during training. Then for each sampling iteration, both methods involve choosing one of the partitions, computing the score functions for each of the parts independently, and then updating the entire image by updating the parts separately. For our approach, the zero-padding method allows for many possible partitions of the image and eliminates boundary artifacts between patches.

```
0:\(_{1}<_{2}<<_{T}\), \(>0\), \(_{i}>0\), \(P,M,\)  Initialize \((0,_{T}^{2})\) for\(t=T:1\)do  Sample \(z(0,_{t}^{2})\)  Set \(_{t}=_{t}^{2}\)  Randomly select integers \(i,j[1,M]\)  For all \(1 r(k+1)^{2}\), extract patch \(_{i,j,r}\)  Compute \(D_{i,j,r}=D_{}(_{i,j,r},_{t})\)  Set \(_{i,j,r}=(D_{i,j,r}-_{i,j,r})/_{t}^{2}\)  Apply (4) to get \(=(,_{t})\)  Set \(\) to \(-_{t}_{_{t}}\|-(D)\|_{2}^{2}\)  Set \(\) to \(+}{2}+}\) endfor ```

**Algorithm 1** Patch Diffusion Inverse Solver (PaDIS)

## 4 Experiments

Experimental setup.For the main CT experiments, we used the AAPM 2016 CT challenge data  that consists of 10 3D volumes. We cropped the data in the Z-direction to select 256 slices and then rescaled the images in the XY-plane to have size \(256 256\). Finally, we used the XY slices from 9 of the volumes to define 2304 training images, and used 25 of the slices from the tenth volume as test data. For the deblurring and superresolution experiments, we used a 3000 image subset of the CelebA-HQ dataset  (with each image being of size \(256 256\)) for training to demonstrate that the proposed method works well in cases with limited training data. We preprocessed the data by dividing all the RGB values by 255 so that all the pixel values were between 0 and 1. The test data was a randomly selected subset of 25 of the remaining images. In all cases, we report the averagemetrics across the test images: peak SNR (PSNR) in dB, and structural similarity metric (SSIM) . For the colored images, these metrics were computed in the RGB domain.

For the main patch-based networks, we trained mostly with a patch size of \(56 56\) to allow the target image to be completely covered by 5 patches in each direction while minimizing the amount of zero padding needed. We used the network architecture in  for both the patch-based networks and whole image networks. All networks were trained on PyTorch using the Adam optimizer with 2 A40 GPUs. The Appendix provides full details of the hyperparameters.

Image generation.Our proposed method is able to learn a reasonable prior for whole images, despite never being trained on any whole images. Fig. 4 shows generation results for the CT dataset using three different methods. The top row used the network trained on whole images; the middle row used the method of  except that the entire image is never supplied to the network either at training or sampling time; the bottom row used the proposed method. The middle row shows that the positional encoding does ensure reasonably appropriate generated patches at each location. However, simply generating each of the patches independently and then naively assembling them together leads to obvious boundary artifacts due to lack of consistency between patches. Our proposed method solves this problem by using overlapping patches via random patch grid shifts, leading to generated images with continuity throughout.

Inverse problems.We tested the proposed method on a variety of different inverse problems: CT reconstruction, deblurring, and superresolution. For the forward and backward projectors in CT reconstruction, we used the implementation provided by . We performed two sparse view CT (SVCT) experiments: one using 8 projection views, and one using 20 projection views. Both of these were done using a parallel-beam forward projector where the detector size was 512 pixels. For the deblurring experiments, we used a uniform blur kernel of size \(9 9\) and added white Gaussian noise with \(=0.01\) where the clean image was scaled between 0 and 1. For the superresolution experiments, we used a scaling factor of 4 with downsampling by averaging and added white Gaussian noise with \(=0.01\). DPS has shown to benefit significantly from using a higher number of neural function evaluations (NFEs) , so we use 1000 NFEs for all of the diffusion model experiments. Appendix A.7 discusses this further.

For the comparison methods, we trained a diffusion model on entire images using the same denoising score matching method shown in Section 3.1. The inference process was identical to that of the patch-based method, with the exception that the score function of the image at each iteration was computed directly using the neural network, as opposed to needing to break up the zero-padded image into patches. We also compared with two traditional methods: applying a simple baseline and reconstructing via the total variation regularizer (ADMM-TV). For CT, the baseline was obtained by applying the filtered back-projection method to the measurement \(\). For deblurring, the baseline was simply equal to the blurred image. For superresolution, the baseline was obtained by upsampling the low resolution image and using nearest neighbor interpolation. The implementation of ADMM-TV can be found in . We also implemented two plug and play (PnP) methods: PnP-ADMM 

Figure 4: Unconditional generation of CT images. Top row: generation with a network trained on whole image; middle row: patch-only version of ; bottom row: proposed PaDIS method.

and regularization by denoising (RED) . We trained denoising CNNs on each of the datasets following  and then used them to solve the inverse problems.

For further comparison of diffusion model methods, we implemented different sampling and data consistency algorithms and applied them in conjunction with our patch-based prior. In particular, we compared with Langevin dynamics , predictor-corrector sampling , and variation exploding DDNM (VE-DDNM) . For all these sampling methods, we used the variation exploding framework for consistency and to avoid retraining the network. We also compared with two other ways of assembling priors of patches to form the prior of an entire image: patch averaging  and patch stitching . App. A.5 contains pseudocode for these comparison algorithms.

Table 1 shows the main inverse problem solving results. The best results were obtained after training the patch-based models for around 12 hours, while the whole image models needed to be trained for 24-36 hours, demonstrating a significant improvement in training time. Furthermore, when averaging the metrics across the test dataset, our proposed method outperformed the whole image method in terms of PSNR and SSIM for all the inverse problems. The score functions of all the patches can be computed in parallel for each iteration, so the reconstruction times for these methods were very similar (both around 5 minutes per image). The whole-image results could be more favorable if more training data were used. See data-size study in App. A.3.

We also ran ablation studies examining the effect of various parameters of the proposed method. Namely, we studied the usage of different patch sizes during reconstruction, varying dataset sizes, importance of positional encoding for patches, and different sampling and inverse problem solving algorithms. The results of these studies are in App. A.2.

In addition to the main inverse problems shown in Table 1, we also ran experiments on three additional inverse problems to demonstrate the effectiveness of our method on a wide variety of inverse problems: 60 view CT reconstruction, fan beam reconstruction using 180 views, and deblurring with a uniform kernel of size \(17 17\). For the deblurring experiment, we again added Gaussian noise with level \(=0.01\) to the measurement. Table 2 shows the quantitative results of these experiments and Figure A.10 shows the visual results.

In the bottom of Figure 5, some artifacts are present in the reconstructions obtained by the diffusion model methods, although they are more apparent in the whole image model than with PaDIS. The measurements are very compressed in this case, so it is very difficult for any model to obtain diagnostic-quality reconstructions; the baselines perform significantly worse in terms of quantitative

   &  &  &  &  \\  & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) \\  Baseline & 24.93 & 0.595 & 21.39 & 0.415 & 24.54 & 0.688 & 25.86 & 0.739 \\ ADMM-TV & 26.82 & 0.724 & 23.09 & 0.555 & 28.22 & 0.792 & 25.66 & 0.745 \\ PnP-ADMM  & 26.86 & 0.607 & 22.39 & 0.489 & 28.82 & 0.818 & 26.61 & 0.785 \\ PnP-RED  & 27.99 & 0.622 & 23.08 & 0.441 & 29.91 & 0.867 & 26.36 & 0.766 \\ Whole image diffusion & 32.84 & 0.835 & 25.74 & 0.706 & 30.19 & 0.853 & 29.17 & 0.827 \\ Langevin dynamics  & 33.03 & 0.846 & 27.03 & 0.689 & 30.60 & 0.867 & 26.83 & 0.744 \\ Predictor-corrector  & 32.35 & 0.820 & 23.65 & 0.546 & 28.42 & 0.724 & 26.97 & 0.685 \\ VE-DDNM  & 31.98 & 0.861 & 27.71 & 0.759 & - & - & 26.01 & 0.727 \\ Patch Averaging  & 33.35 & 0.850 & 28.43 & 0.765 & 29.41 & 0.847 & 27.67 & 0.802 \\ Patch Stitching  & 32.87 & 0.837 & 26.71 & 0.710 & 29.69 & 0.849 & 27.50 & 0.780 \\  PaDIS (Ours) & **33.57** & **0.854** & **29.48** & **0.767** & **30.80** & **0.870** & **29.47** & **0.846** \\  

Table 1: Comparison of quantitative results on three different inverse problems. Results are averages across all images in the test dataset. Best results are in bold.

   &  &  &  \\  & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) \\  Baseline & 25.89 & 0.746 & 20.07 & 0.521 & 21.14 & 0.569 \\ ADMM-TV & 30.93 & 0.833 & 25.78 & 0.719 & 26.03 & 0.724 \\ Whole image diffusion & 35.83 & 0.894 & 26.89 & 0.835 & 28.35 & 0.808 \\  PaDIS (Ours) & **39.28** & **0.941** & **29.91** & **0.932** & **28.91** & **0.818** \\  

Table 2: Extra inverse problem experiments. Results are averages across all images in the test dataset. Best results are in bold.

metrics and exhibit severe blurring. In clinical settings, patient diagnosis are typically performed with CT scans consisting of hundreds of views. The top of Figure 5 shows that when 60 views are used, our proposed method yields a much better reconstruction without artifacts. Nevertheless, we show the potential of our proposed methods to reconstruct images from very sparse views with a decent image quality, which could be potentially used for applications such as patient positioning.

Figure 5: Results of CT reconstruction. 60 views are used for the top two rows, 20 views are used for the bottom two rows. To better show contrast between organs, we use modified Hounsfield units (HU) in the top figure, while we use the same scale the images were trained on in the bottom figure.

Figure 6: Results of deblurring with Gaussian noise (\(=0.01\)).

Finally, since the original AAPM dataset contained CT images of resolution \(512 512\), we ran 60 view CT reconstruction experiments with these higher resolution images. Due to the lack of data, we did not train a whole-image model on these higher resolution images. We used a zero padding width of 64 and largest patch size of \(64 64\) for training. Figure 8 shows the visual results of these experiments. Hence, our proposed methods can obtain high quality reconstructions for both different inverse problems and also for different image sizes.

## 5 Conclusion

In this work, we presented a method of using score-based diffusion models to learn image priors through solely the patches of the image, combined with suitable position encoding. Simulation results demonstrated how the method can be used to solve a variety of inverse problems. Extensive experiments showed that under conditions of limited training data, the proposed method outperforms methods involving whole image diffusion models. In the future, more work could be done on higher quality image generation using a multi-scaled resolution approach [67; 68], using acceleration methods for faster reconstruction, and higher dimensional image reconstruction. Image priors like those proposed in this paper have the potential to benefit society by reducing X-ray dose in CT scans. Generative models have the risk of inducing hallucinations and being used for disinformation.