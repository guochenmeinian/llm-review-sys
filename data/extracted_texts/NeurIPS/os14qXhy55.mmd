# OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries

Yuhang Lu

ShanghaiTech University

luyh2@shanghaitech.edu.cn &Xinge Zhu

The Chinese University of Hong Kong

zhuxinge123@gmail.com &Tai Wang

Shanghai AI Laboratory

taiwang.me@gmail.com &Yuexin Ma

ShanghaiTech University

mayuexin@shanghaitech.edu.cn

Corresponding authors. This work was supported by NSFC (No.62206173), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShanghaiAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (KLIP-HuMaCo), ShanghaiTech University.

###### Abstract

Occupancy prediction has increasingly garnered attention in recent years for its fine-grained understanding of 3D scenes. Traditional approaches typically rely on dense, regular grid representations, which often leads to excessive computational demands and a loss of spatial details for small objects. This paper introduces OctreeOcc, an innovative 3D occupancy prediction framework that leverages the octree representation to adaptively capture valuable information in 3D, offering variable granularity to accommodate object shapes and semantic regions of varying sizes and complexities. In particular, we incorporate image semantic information to improve the accuracy of initial octree structures and design an effective rectification mechanism to refine the octree structure iteratively. Our extensive evaluations show that OctreeOcc not only surpasses state-of-the-art methods in occupancy prediction, but also achieves a \(15\%-24\%\) reduction in computational overhead compared to dense-grid-based methods.

## 1 Introduction

Holistic 3D scene understanding is a pivotal aspect of a stable and reliable visual perception system, especially for real-world applications such as autonomous driving. Occupancy, as a classical representation, has been renascent recently with more datasets support and exploration in learning-based approaches. Such occupancy prediction tasks aim at partitioning the 3D scene into grid cells and predicting semantic labels for each voxel. It is particularly an essential solution for recognizing irregularly shaped objects and also enables the open-set understanding [(1)], further benefiting downstream tasks, like prediction and planning.

Existing occupancy prediction methods [(2; 3; 4; 5; 6)] typically construct dense and regular grid representations, same as the ground truth. While such approach is intuitive and direct, it overlooks the statistical and geometric properties of 3D environments. In fact, the 3D scene is composed of foreground objects and background regions with various shapes and sizes. For example, the space occupied by larger objects, such as buses, are considerably more extensive than that taken up by smaller items like traffic cones (Fig. 0(a)). Consequently, employing a uniform voxel resolution to depict the scene proves to be inefficient, leading to computational waste for larger objects and a lack of geometry details for smaller ones. Considering the large computation cost of aforementionedworks, some recent works attempted to mitigate the heavy memory footprint by utilizing other representations, such as 2D planes in TPVFormer (7) and coarse voxels in PanoOcc (8), or modeling non-empty regions by depth estimation (9). However, these methods suffer from the loss of spatial information because of too coarse representations or accumulated estimated depth errors.

To reduce the computational overhead and meanwhile improve the prediction accuracy, we propose to use octree (10) representation for the occupancy prediction, which can flexibly adapt to objects and semantic regions with various shapes and sizes. As a tree-based data structure, it recursively divides the 3D space into eight octants, thus allowing coarse spatial partition for large regions and fine-grained processing for small objects or complex details (Fig. 0(b)). Incorporating octree representation, we propose _OctreeOcc_, an efficient and multi-granularity method for occupancy prediction. It constructs octree queries by predicting the octree structure from the stored features of leaf nodes at each level of the tree. However, directly predicting 3D structure from 2D images is challenging due to the lack of depth and occlusion issues. To address this problem, we first propose **Semantic-guided Octree Initialization** that incorporates image semantic information to produce more accurate initial structures. And then, we devise an **Iterative Structure Rectification** module that predicts new octree structure from the encoded query to rectify the low-confidence region of the original prediction, further improving the prediction precision.

Our extensive evaluations against state-of-the-art occupancy prediction methods show that _OctreeOcc_ outperforms others on nuScenes and SemanticKITTI datasets, reducing computational overhead by \(15\%-24\%\) for dense-grid-based methods. Ablation studies further validate the effectiveness of each module within our method. Our contributions can be summarized as follows:

* We introduce _OctreeOcc_, a 3D occupancy prediction approach based on multi-granularity octree queries. This method facilitates spatial sparsification, significantly decreasing the number of voxels needed to accurately depict a scene, yet retains critical spatial details.
* We propose a semantic-guided octree initialization module and an iterative structure rectification module, which empower the network with a robust initial setup and the ability to dynamically refine the octree, leading to a more efficient and effective representation.
* Comprehensive experiments demonstrate that OctreeOcc achieves state-of-the-art performance and reduces computational overhead, highlighting the feasibility and potential of octree structures in 3D occupancy prediction.

## 2 Related Work

### Camera-based 3D Perception

Camera-based 3D perception has gained significant traction in recent years due to its ease of deployment, cost-effectiveness, and the preservation of intricate visual attributes. According to the view

Figure 1: **Scale difference of various categories and octree representation.** (a) compares the average space occupied by different object types, indicating varying granularities needed for different semantic regions. (b) demonstrates the advantage of octree representations, enabling specific granularities for different objects and even parts of objects, reducing computational overhead while retaining spatial information.

transformation paradigm, these methods can be categorized into three distinct types. LSS-based approaches[11; 2; 12; 13; 14; 15] explicitly lift multi-view image features into 3D space through depth prediction. Another category of works[16; 17; 18] implicitly derives depth information by querying from 3D to 2D. Notably, projection-free methods[19; 20; 21; 22] have recently demonstrated exceptional performance. While commendable progress has been made in detection, this approach compromises the comprehensive representation of the overall scene in 3D space and proves less effective in recognizing irregularly shaped objects. Consequently, there is a burgeoning interest in methodologies aimed at acquiring a dense voxel representation through the camera, facilitating a more comprehensive understanding of 3D space.

### 3D Occupancy Prediction

3D occupancy prediction involves the prediction of both occupancy and semantic attributes for all voxels encompassed within a three-dimensional scene, particularly valuable for autonomous vehicular navigation. Recently, some valuable datasets [23; 24; 25] have been proposed, boosting more and more research works [26; 1; 27; 28; 6; 4; 5; 7; 8; 9; 29; 30; 31; 32; 33; 34] in this field. Most of the research focuses on dense voxel modeling. MonoScene(27) pioneers a camera-based approach using a 3D UNet architecture. OccDepth(28) improves 2D-to-3D geometric projection using stereo-depth information. OccFormer(6) decomposes the 3D processing into the local and global transformer pathways along the horizontal plane. SurroundOcc(4) achieves fine-grained results with multiscale supervision. Symphonies(5) introduces instance queries to enhance scene representation.

Nevertheless, owing to the high resolution of regular voxel representation and sparse context distribution in 3D scenes, these methods encounter substantial computational overhead and efficiency issues. Some approaches recognize this problem and attempt to address it by reducing the number of modeled voxels. For instance, IPVFormer(7) models the three-view 2D planes and subsequently recovering 3D spatial information from them. However, its performance degrades due to the lack of 3D information. PanoOcc(8) initially represents scenes at the coarse-grained level and then upsamples them to the fine-grained level, but the lack of information from coarse-grained modeling cannot be adequately addressed by the up-sampling process. VoxFormer(9) mitigates computational complexity by initially identifying non-empty regions through depth estimation and modeling only those specific areas. However, the effectiveness of this process is heavily contingent on the accuracy of depth estimation. In contrast, our approach provides different granularity of modeling for different regions by predicting the octree structure, which reduces the number of voxels to be modeled while preserving the spatial information, thereby reducing the computational overhead and maintaining the accuracy.

### Octree-Based 3D Representation

The octree structure finds widespread use in computer graphics for rendering or reconstruction[35; 36; 37; 38], owing to its spatial efficiency and GPU compatibility. Researchers have extended its utility to efficient point cloud learning and related tasks[39; 40; 41; 42]. OctFormer and OcTr  utilize multi-granularity features of octree to capture a comprehensive global context, thereby enhancing the efficiency of understanding point clouds at the scene level. Furthermore, certain studies [45; 46] adopt octree representation for effectively compressing point cloud data. These works have highlighted the versatility and effectiveness of octree-based methodologies in point cloud analysis and processing applications. However, unlike constructing an octree from 3D point clouds, we are the first to predict the octree structure of a 3D scene from images, which is more challenging owing to the absence of explicit spatial information inherent in 2D images.

## 3 Methodology

In this section, we introduce details of our efficient and multi-granularity occupancy prediction method _OctreeOcc_. After defining the problem and providing an overview of our method in Sec. 3.1 and 3.2, we introduce key components of our method in order. In Sec. 3.3, we outline how we define octree queries and transform dense queries into octree queries. Next, we utilize image semantic priors to construct a high-quality initialized octree structure, as detailed in Sec. 3.4. Once the initialized octree query is obtained, we encode it and refine the octree structure in Sec. 3.5. Finally, Sec. 3.6 describe how to supervise the network.

### Problem Setup

Camera-based occupancy prediction aims to predict the present occupancy state and semantics of each voxel grid within the scene using input from multi-view camera images. Specifically, we consider a set of \(N\) multi-view images \(I\) = \(\{I_{i}^{H W 3}\}_{i=1}^{N}\), together with camera intrinsics \(K\) = \(\{K_{i}^{3 3}\}_{i=1}^{N}\) and extrinsics \(T\) = \(\{T_{i}^{4 4}\}_{i=1}^{N}\) as input, and the objective of the model is to predict the 3D semantic voxel volume \(O\{w_{0},w_{1},...,w_{C}\}^{X Y Z}\), where \(H\), \(W\) indicate the resolution of input image and \(X\), \(Y\), \(Z\) denote the volume resolution (e.g. 200\(\)200\(\)16). The primary focus lies in accurately distinguishing the empty class (\(w_{0}\)) and other semantic classes (\(w_{1} w_{C}\)) for every position in the 3D space, which entails the network learning both the geometric and semantic information inherent in the data.

### Overview

Given a set of multi-view images \(I\) = \(\{I_{i}^{H W 3}\}_{i=1}^{N}\), we extract multi-view image features \(\) = \(\{F_{i}^{H W C}\}_{i=1}^{N}\). Simultaneously, we randomly initialize the dense voxel query \(Q_{dense}^{X Y Z C}\). To enhance computational efficiency, we transform \(Q_{dense}\) into sparse representation \(Q_{octree}^{N C}\), leveraging octree structure information (\(i.e.\) octree mask) derived from segmentation priors. During encoding, we utilize \(Q_{octree}\) to gather information, including temporal fusion and sampling from image features \(\), while also rectifying the octree structure. Upon encoding \(Q_{octree}\), to conform to the output format, we convert it back to \(Q_{dense}\) and apply a Multi-Layer Perceptron (MLP) to obtain the final occupancy prediction \(O^{X Y Z K}\). Here, \(H\), \(W\) indicate the resolution of input image and \(X\), \(Y\), \(Z\) denote the volume resolution. \(N\) means the number of octree query,\(C\) denotes the feature dimension and \(K\) indicates the number of classes.

### Octree Query

Given that objects within 3D scenes exhibit diverse granularities, employing dense queries [6; 4] overlooks this variation and leads to inefficiency. To address this, we propose sparse and multi-granularity octree queries, leveraging the octree structure. This approach creates adaptable voxel representations tailored to semantic regions at different scales.

**Octree Mask.** To effectively construct the octree query, it's essential to understand its underlying structure. An octree partitions each node into eight child nodes within 3D space, each representing

Figure 2: **Overall framework of OctreeOcc. From multi-view images, we extract multi-scale features using an image backbone. The initial octree structure is derived from image segmentation priors, transforming dense queries into octree queries. The octree encoder refines these queries and rectifies the octree structure. Finally, we decode the octree queries to obtain occupancy predictions. The diagram of the Iterative Structure Rectification module shows the octree query and mask in 2D (quadtree) form for better visualization.**equal subdivisions of the parent node. This recursive process begins with the initial level and proceeds with gradual splitting. At every level, a voxel query serves either as a leaf query if it remains unsplit or as a parent query if it undergoes division. We obtain this geometric information by maintaining a learnable octree mask, denoting as \(M_{o}=\{M_{o}^{l}^{},},} \}_{l=1}^{L-1}\), where \(X\), \(Y\), \(Z\) denote the ground truth's volume resolution. The \(L\) denotes the depth of the octree, representing the existence of \(L\) different resolutions of queries, with \(L-1\) splits being performed from the top to the bottom of the octree. The value in the octree mask represents the probability that a voxel at that level requires a split, which is initialized through segmentation priors (Sec. 3.4), rectified during query encoding (Sec. 3.5), and supervised by octree ground truth (Sec. 3.6).

**Transformation between octree query and dense voxel query.** During the query encoding process, we prioritize efficiency by leveraging octree queries. This involves transforming the initial dense queries \(Q_{dense}\) into octree queries \(Q_{octree}\) using learned structural information. To determine the octree structure, we need to binarise the learned octree mask. Since most of the voxels in the scene at various resolutions do not necessitate splitting, neural network-based prediction binarization is susceptible to pattern collapse, tending to predict all as non-split, leading to a decrease in performance. To mitigate this issue, we introduce a manually defined query selection ratio, where a subset of voxels with the highest probability of splitting is selected for division through the top-k mechanism.

The transformation from \(Q_{dense}\) to \(Q_{octree}\) begins at the finest granularity, we downsample \(Q_{dense}\) to each level through average pooling and retain queries that do not require splitting (leaf queries) with the assistance of the binarized octree mask. This process iterates until reaching the top of the octree. By retaining all leaf queries, we establish \(Q_{octree}^{N C}\), where \(N=N_{1}+N_{2}++N_{L}\) represents the total count of leaf queries, L indicates the depth of octree. Conversely, applying the inverse operation of this process allows the conversion of \(Q_{octree}\) back into \(Q_{dense}\) for the final output. Further details are provided in Appendix.

### Semantic-Guided Octree Initialization

Predicting octree structure from an initialised query via neural network can yield sub-optimal results due to the inherent lack of meaningful information in the query. To overcome this limitation, we employ the semantic priors inherent in images as crucial references. Specifically, we adopt UNet(47) to segment the input multi-view images \(I\) and obtain the segment map \(I_{seg}=\{I_{seg}^{i}^{H W}\}_{i=1}^{N}\). We then generate sampling points \(p=\{p_{i}^{3}\}_{i=1}^{X Y Z}\), with each point corresponding to the center coordinates of dense voxel queries. Subsequently, we project these points onto various image views. The projection from sampling point \(p_{i}=(x_{i},y_{i},z_{i})\) to its corresponding 2D reference point \((u_{ij},v_{ij})\) on the \(j\)-th image view is formulated as:

\[_{j}(p_{i})=(u_{ij},v_{ij}),\] (1)

where \(_{j}(p_{i})\) denotes the projection of the \(i\)-th sampling point at location \(p_{i}\) on the \(j\)-th camera view. We project the point \(p_{i}\) onto the acquired semantic segmentation map \(I_{seg}\) through the described projection process. To ensure the prioritization of crucial areas such as foreground objects and buildings in the initial structure, we adopt an unbalanced weight assignment method. Here, the highest weight is allocated to sampling points projecting onto foreground areas, with decreasing weights assigned to points projecting onto buildings or vegetation, and the lowest weight designated for points projecting onto roads, among others. Subsequently, the voxel's weight is determined as the average of the weights of all its sampling points. During this process, we determine the weights of each voxel at the finest granularity, denoted as \(W^{X Y Z}\). Subsequently, we employ average pooling to downsample \(W\) to each level of the octree, resulting in an initial octree mask \(M_{o}=\{M_{o}^{l}^{},},} \}_{l=1}^{L-1}\). Here, \(X\), \(Y\), and \(Z\) represent the resolution of the ground truth volume, while \(L\) denotes the depth of the octree. Further details are provided in the Appendix.

### Octree Encoder

Given octree queries \(Q_{octree}\) and extracted image features \(\), the octree encoder updates both the octree query features and the octree structure. Referring to the querying paradigm in dense query-based methods(8; 25), we adopt efficient deformable attention(48) for temporal self-attention(TSA) and image cross-attention(ICA).

In accurately representing the driving scene, temporal information plays a crucial role. By leveraging historical octree queries \(Q_{t-1}\), we align it to the current octree queries by the ego-vehicle motion transformation. Given historical octree queries \(Q_{t-1}^{N,C}\), a current octree query \(q\) located at \(p=(x,y,z)\), the TSA is represented by:

\[TSA(q,Q_{t-1})=_{m=1}^{M_{1}}DeformAttn(q,p,Q_{t-1}),\] (2)

where \(M_{1}\) indicates the number of sampling points. Implementing it within the voxel-based self-attention ensures that each octree query interacts exclusively with local voxels of interest, keeping the computational cost manageable.

Image cross-attention is devised to enhance the interaction between multi-scale image features and octree queries. Specifically, for an octree query \(q\), we can obtain its centre's 3D coordinate \((x,y,z)\) as reference point \(Ref_{x,y,z}\). Then we project the 3D point to images like formula 1 and perform deformable attention:

\[ICA(q,)=_{n N}_{m=1}^{M_{2}}DeformAttn(q,_ {n}(Ref_{x,y,z}),_{n}),\] (3)

where \(N\) denotes the camera view, \(m\) indexes the reference points, and \(M_{2}\) is the total number of sampling points for each query. \(_{n}\) is the image features of the \(n\)-th camera view.

**Iterative Structure Rectification Module.** The initial octree structure, derived from image segmentation priors, may not precisely match the scene due to the potential segmentation and projection errors. Nonetheless, the encoded octree query captures crucial spatial information. Thus, the predicted octree structure based on this query complements and rectifies the initial structure prediction, allowing us to mitigate limitations arising from segmentation and projection issues.

Specifically, we partition the octree structure into two parts: the high-confidence regions and the low-confidence regions, as Fig. 2 shows. By sorting the octree split probability values stored in the octree mask in descending order and selecting the top k% of regions at each level, we can identify the locations of high-confidence regions. For these regions, predictions are relatively more accurate and no additional adjustments are required in this iteration. For regions where confidence remains low, we first extract the query features corresponding to those areas by utilizing the index of low-confidence regions, denoted as \(Q_{lcr}=\{Q_{lcr}^{l}^{N_{i} C}\}_{l=1}^{L-1}\), where \(N_{l}\) represents the number of low-confidence queries in level \(l\). We then employ a MLP to predict octree split probabilities from \(Q_{lcr}\). Subsequently, we apply a weighted sum with the previous split probability predictions of low-confidence regions to obtain rectified predictions. These refined predictions are concatenated with the preserved probability predictions of high-confidence regions, culminating in the generation of the final rectified octree mask. It is worth noting that, due to the iterative nature of structure updates, regions initially considered high confidence may not necessarily remain unchanged, as they might be partitioned into low-confidence regions in the next iteration. More details are shown in Appendix.

Figure 3: **Illustration of octree structure rectification.** The left figure shows the initially predicted octree structure, while the right figure displays the structure after rectification. It’s evident that the rectification module improves the consistency of the octree structure with the object’s shape.

### Loss Function

To train the model, we use focal loss \(L_{focal}\), lovasz-softmax loss \(L_{ls}\), dice loss \(L_{dice}\), affinity loss \(L_{scal}^{geo}\) and \(L_{scal}^{sem}\) from MonoScene(27). In addition, we also use focal loss to supervise the octree prediction. The overall loss function \(L=L_{focal}+L_{ls}+L_{dice}+L_{scal}^{geo}+L_{scal}^{sem}+L_{octree}\).

## 4 Experiments

In this section, we first introduce the datasets (Sec. 4.1), evaluation metrics (Sec. 4.2), and implementation details (Sec. 4.3). Subsequently, we evaluate our method on 3D occupancy prediction and semantic scene completion tasks (Sec. 4.4) to demonstrate its effectiveness. Additionally, we conduct extensive ablation studies and provide more analysis (Sec. 4.5) of our method.

### Datasets

**Occ3D-nuScenes(23)** re-annotates the nuScenes dataset(49) with precise occupancy labels derived from LiDAR scans and human annotations. It includes 700 training instances and 150 validation instances, with occupancy spanning -40m to 40m in X and Y axes, and -1m to 5.4m in the Z-axis. Labels are divided into 17 classes, with each class representing a volumetric space of 0.4 meters in each dimension, plus an 18th "free" category for empty regions. The dataset also provides visibility masks for LiDAR and camera modalities.

**SemanticKITTI(50)** comprises 22 distinct outdoor driving scenarios, with a focus on areas located in the forward trajectory of the vehicle. Each sample in this dataset covers a spatial extent ranging from [0.0m, -25.6m, -2.0m, 51.2m, 25.6m, 4.4m], with a voxel granularity set at [0.2m, 0.2m, 0.2m]. The dataset consists of volumetric representations, specifically in the form of 256x256x32 voxel grids. These grids undergo meticulous annotation with 21 distinct semantic classes. The voxel data is derived through a rigorous post-processing procedure applied to Lidar scans.

### Evaluation metrics

Both 3D occupancy prediction and semantic scene completion utilize intersection-over-union (mIoU) over all classes as evaluation metrics, calculated as follows:

\[=}_{=1}^{}_{}}{_{}+_{}+ _{}},\] (4)

where \(_{}\), \(_{}\), and \(_{}\) correspond to the number of true positive, false positive, and false negative predictions for class \(_{}\), and \(\) is the number of classes.

### Implementation Details

Based on previous research, we set the input image size to 900\(\)1600 and employ ResNet101-DCN(51) as the image backbone. Multi-scale features are extracted from the Feature Pyramid Network(52) with downsampling sizes of 1/8, 1/16, 1/32, and 1/64. The feature dimension \(C\) is set to 256. The octree depth is 3, and the initial query resolution is 50\(\)50\(\)4. We choose query selection ratios of 20% and 60% for the two divisions. The octree encoder comprises three layers, each composed of TSA, ICA, and Iterative Structure Rectification (ISR) modules. Both \(M_{1}\) and \(M_{2}\) are set to 4. In TSA, we fuse four temporal frames. In ISR, the top 10% predictions are considered high-confidence in level 1, and 30% in level 2. The loss weights are uniformly set to 1.0. For optimization, we employ Adam(53) optimizer with a learning rate of 2e-4 and weight decay of 0.01. The batch size is 8, and the model is trained for 24 epochs, consuming around 3 days on 8 NVIDIA A100 GPUs.

### Results

**3D Occupancy Prediction.** In Tab. 1, we compare our method with other SOTA occupancy prediction methods on Occ3d-nus validation set. The performance of FBOCC(3) relies on open-source code, which we evaluate after ensuring consistency in details (utilizing the same backbone, image resolution,

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

vs d and c,d vs e). Optimizing query numbers based on scene object granularity distribution ensure effective processing of semantic regions of different sizes.

## 5 Conclusions

In conclusion, our paper introduces OctreeOcc, a novel 3D occupancy prediction framework that addresses the limitations of dense-grid representations in understanding 3D scenes. OctreeOcc's adaptive utilization of octree representations enables the capture of valuable information with variable granularity, catering to objects of diverse sizes and complexities. Our extensive experimental results affirm OctreeOcc's capability to attain state-of-the-art performance in 3D occupancy prediction while concurrently reducing computational overhead.

**Limitation.** The quality of the octree ground truth depends on the accuracy of the occupancy ground truth. Current occupancy ground truth comes from sparse lidar point clouds and surface reconstruction, leading to low-quality results for some frames, which affects the octree construction.