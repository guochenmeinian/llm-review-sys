# RTify: Aligning Deep Neural Networks with Human Behavioral Decisions

Yu-Ang Cheng\({}^{*1}\), Ivan Felipe Rodriguez\({}^{*1}\), Sixuan Chen\({}^{1}\),

Kohitij Kar\({}^{2}\), Takeo Watanabe\({}^{1}\), Thomas Serre\({}^{1}\)

\({}^{1}\) Brown University \({}^{2}\) York University

{yuang_cheng,ivan_felipe_rodriguez

sixuan_chen1,takeo_watanabe,thomas_serre}@brown.edu

kOhlt1j@yorku.ca

These authors contributed equally to this work.

###### Abstract

Current neural network models of primate vision focus on replicating overall levels of behavioral accuracy, often neglecting perceptual decisions' rich, dynamic nature. Here, we introduce a novel computational framework to model the dynamics of human behavioral choices by learning to align the temporal dynamics of a recurrent neural network (RNN) to human reaction times (RTs). We describe an approximation that allows us to constrain the number of time steps an RNN takes to solve a task with human RTs. The approach is extensively evaluated against various psychophysics experiments. We also show that the approximation can be used to optimize an "ideal-observer" RNN model to achieve an optimal tradeoff between speed and accuracy without human data. The resulting model is found to account well for human RT data. Finally, we use the approximation to train a deep learning implementation of the popular Wong-Wang decision-making model. The model is integrated with a convolutional neural network (CNN) model of visual processing and evaluated using both artificial and natural image stimuli. Overall, we present a novel framework that helps align current vision models with human behavior, bringing us closer to an integrated model of human vision.

## 1 Introduction

Categorizing visual stimuli is crucial for survival, and it requires an organism to make informed decisions in dynamic and noisy environments. This critical aspect of visual perception has driven the development of computational models to understand and replicate these processes. Traditionally, the field has followed two distinct paths.

On the one hand, image-computable vision models are used to predict behavioral decisions during (rapid) visual categorization tasks ranging from models of early- , mid-  and high-level vision  (see  for a review). More recently, these earlier models were superseded by deep convolutional neural networks (CNNs), which have become the de-facto choice for modeling behavioral decision . Models are typically evaluated by estimating confidence scores computed for individual images, which are then correlated with similar scores derived for human observers(such as the proportion of correct human responses for each image). Such metrics ignore human reaction times (RTs); hence, current vision models only partially account for human decisions.

On the other hand, decision-making models have been used to explain how visual information gets integrated over time - predicting behavioral choices and RTs jointly. Notably, mathematical models, exemplified by evidence accumulation models such as the drift-diffusion  and linear ballisticaccumulators , have been quite successful in modeling an array of behavioral data (see  for a review). In addition, mechanistic models, including the Wong-Wang (WW) model, have provided insights into the underlying neural mechanisms . However, these efforts have primarily relied on traditional psychophysics tasks using simple, artificial stimuli, such as Gabor patterns  and random moving dots . Beyond these easily parameterizable stimuli, these models have not been extended to deal with more complex, natural stimuli.

While both vision and decision-making models have contributed distinctively to our understanding of visual processes, a complete understanding of human vision will require their integration to explain the whole dynamics of visual decision-making. Recent neuroscience studies have leveraged recurrent neural network (RNN) models as the starting point for this integration  (see  for a review). More generally, recurrent processing has been shown to be necessary to account for both behavioral and neural recordings in object recognition tasks .

Two promising approaches have been described that leverage RNNs to bridge the gap between decision-making and vision models . In , an RNN was trained to solve a visual classification task using the backpropagation-through-time (BPTT) learning algorithm. The entropy of the RNN outputs is computed at each timestep and used as a proxy for the network confidence. A decision is taken when the entropy reaches the threshold, halting further recurrent computation. In this approach, the recurrence steps are not differentiable, which prevents the use of gradient methods and inherently limits the complexity of the corresponding decision function. The resulting model may have difficulty predicting the entire distribution of RTs. Besides, the method is only applicable when human RTs are available because it requires an extensive search for the correct threshold value to fit human RT data.

An alternative approach was described in . In , a convolutional RNN was trained on a visual classification task using contractor recurrent back-propagation (C-RBP). Besides, instead of searching for an optimal threshold to fit human RT data, a surrogate time-evolving 'uncertainty'

Figure 1: **Illustration of our RTify method. The input is a visual stimulus represented by random moving dots, but the model can also accommodate color images and video sequences. We take a pretrained task-optimized RNN and use a trainable function \(f_{w}\) to transform the activity of the network into a real-valued evidence measure, \(e_{t}\), that will be integrated over time by an evidence accumulator, \(_{t}\). When the evidence accumulator reaches the threshold \(\), processing stops, and a decision is taken. The time step at which the accumulated evidence passes this threshold \(_{}\) is taken as the model RT for this stimulus.**

metric was estimated with evidential deep learning . In this framework, model outputs are treated as parameters of a Dirichlet distribution, with the width of the distribution reflecting uncertainty. Remarkably, the resulting approach fits a range of experimental data well, without any supervision from human data. It is true that uncertainty and RTs are tightly coupled and are both affected by task difficulty. However, uncertainty and RTs are conceptually different. Experimental results show that uncertainty and RTs can be positively or negatively correlated , and even double dissociated  under different experimental conditions. Therefore, a good model of uncertainty is not guaranteed to be a good model of RTs.

Here, based on extensive cognitive neuroscience research [34; 35; 36; 37], we introduce a novel trainable module called RTify to allow an RNN to dynamically and nonlinearly accumulate evidence. First, this module can be trained to learn to make human-like decisions using direct human RT supervision. Our results suggest that incorporating a dynamic evidence accumulation process, compared to the entropy heuristics used in , can help better capture human RTs. Second, we show how the same general approach can also be used to train an RNN to learn to solve a task with an optimal number of time steps via self-penalty. Our results show that human-like RTs naturally emerge from such ideal-observer models without explicit supervision from human RT data. Hence, our framework is general enough to allow the fitting of human RT data as done in  and/or the training of a neural architecture that can optimally trade speed and accuracy via self-penalty as done in .

Contributions:Overall, our work makes the following contributions: **(i)** We present RTify, a novel computational approach to optimize the recurrence steps of RNNs to account for human RTs. This enables dynamic nonlinear evidence accumulation learned through back-propagation (a) to fit human data or (b) optimally balance speed and accuracy. **(ii)** We comprehensively demonstrate the effectiveness of our framework for modeling human RTs across a diverse range of psychophysics tasks and stimuli. Our method consistently outperforms alternatives with and without explicit training on human data. **(iii)** As an illustrative example of the framework's potential, we extend the WW decision-making model  to create a biologically plausible, multi-class compatible, and fully differentiable RNN module. We show that the enhanced neural circuit can be used as a drop-in module for CNNs to fit human RTs.

## 2 RTify: Overview of the method

First, we explain how our RTify module is applied to a pre-trained RNN. Then, we will explain how to tune a deep-learning RNN-based implementation of the WW model to RTify feedforward networks.

We start with a task-optimized RNN with hidden state \(_{t}\), which remains frozen. We then train a learnable mapping function \(f_{w}:^{k}\) that summarizes the state of the neural population at each time step \(t\) by mapping the RNN hidden state \(_{t}\) to some "evidence": \(e_{t}=f_{w}(_{t})\). At every time step, the evidence is integrated via an "evidence accumulator" \(_{t}=_{i=1}^{t}e_{i}\), and when the accumulated evidence passes a learnable threshold \(\), the model is read out, and a decision is made. The time step at which the accumulated evidence first passes this threshold is given by \(_{}()=\{t:_{t}>\}\), and is treated as model RTs. In summary, \(_{}()\) is directly influenced by the threshold \(\) and by \(w\) through \(_{t}\).

To align the model RTs with human RTs, or to penalize the model for excessive time steps, we need to optimize a loss function over \(_{}()\). In the most general case, we first consider \(F(_{}())\) as our loss function to illustrate how we approximate its gradient. Since our goal is to minimize \(F\), we will need to calculate the gradient \(())}{}\) and \(())}{ w}\). Following the chain rule, we get

\[())}{ w}=())}{_{}()}( )}{ w},())}{}= ())}{_{}()} ()}{}.\] (1)

This means a crucial step involves estimating the gradient of \(_{}()\) over the trainable parameters \(w\) and \(\) of the RTify.

The primary challenge that arises when extracting the gradient of \(_{}()\) over the trainable parameters \(w\) and \(\) is the non-differentiability of \(_{}()\), which prevents the direct use of the backpropagation algorithm. This is because \(_{}()\) lies in the integer space and requires non-differentiable operationssuch as the minimum function and the inequality. We will relax the original formulation to circumvent this issue, enabling us to approximate the gradient.

Assume that \((t)\) is a continuous function on the closed interval \([1,N]\), representing the accumulation of evidence over time. Define \(_{}()=\{t[1,N]:(t)>\}\), which is the earliest time when \((t)\) exceeds the threshold \(\).We can use a first-order Taylor expansion to find the following approximation:

\[}{ w}^{* }}{ w}-_{t-1}}(-}{ w}),}{}^{*}}{}-_{t-1}}\] (2)

Fig. S1 provides a visual intuition for the approximation, and the full derivation can be found in the SI A.1. This leads to the following gradients for our trainable parameters:

\[())}{ w}())}{_{}()}- _{t}}(-}{ w}),())}{}())}{ _{}()}-_{t-1}}.\] (3)

Since \(F\) and \(\) are both differentiable by nature, the gradients in Eqs. 2 and 3 are all computable after this approximation.

Under this framework, we consider two different scenarios: Training with human data directly ("supervised"; see Section 2.1) or training with "self-penalty", which involves no explicit human data but uses a penalty term that spontaneously leads to decision times similar to human RTs (e.g., achieving an optimal speed-accuracy trade-off; see Section 2.2).

### Predicting human decisions with direct "supervision"

Human behavioral decisions in the random dot motion (RDM) tasks used in decision-making studies [16; 24; 38] are typically summarized as histograms similar to those shown in Fig. 2. Here, histograms are computed for RTs for correct and incorrect trials corresponding to individual experimental conditions (such as coherence levels shown here; see section 3 for details). Moreover, RTs for incorrect trials are turned into negative RTs. Combined with correct RTs (which stay positive), one single histogram is used for capturing both accuracy (the proportion of positive values) and RTs. To measure the goodness of fit between human RTs and model RTs, we use a mean squared error loss (MSE) between histograms of model RTs and human RTs.

In the object recognition task , only RTs averaged across all participants were available. We can match human data on a stimulus-by-stimulus basis using the negative correlation loss between model and human RTs.

### Predicting human decisions with "self-penalty"

Our framework allows us to develop an "ideal-observer" RNN model explicitly trained to balance the computational time required for solving a particular classification task and its own accuracy for the task, i.e., a speed-accuracy trade-off.

To achieve this, we add a regularizer to the cross-entropy loss to encourage the RNN to jointly maximize task accuracy while minimizing the computational time needed to solve the task. With \(l\) as the output logits of the network, \(\) as the output probabilities of the network, and \(y\) as the ground truth, we can write our penalty term for a single sample as:

\[L_{}=L_{CCE}(y,)+(} _{})\] (4)

where \(\) is a hyperparameter for controlling the strength of the penalty, \(}\) refers to the logit value of the correct label, \(_{}\) is the model decision time. This penalty means that the model will be penalized for using too much time, especially for higher confidence (higher \(}\)).

### RTifying feedforward networks

To integrate temporal dynamics into feedforward neural networks (e.g., CNNs), we describe an RNN module that approximates the WW neural circuit model . The original WW model is a biophysically-realistic neural circuit model of two-alternative forced choices via the temporal accumulation of sensory evidence in two distinct neural populations. It takes a constant scalar input (representing a stimulus parameter such as the degree of coherence for randomly moving dot stimuli). It outputs an RT when the activity of either population reaches a decision threshold.

However, the original WW model has limitations: its parameters must be manually tuned to fit observed data, and it is restricted to binary classification tasks with simple artificial parametric stimuli (e.g., Gabor patterns). To overcome these limitations, we extend the WW model. First, we replace the scalar input with a feedforward neural network (e.g., a CNN), enabling the model to process complex stimuli such as natural images. Second, we generalize the model from two populations to \(M\) populations, effectively increasing the number of neural populations to handle multi-class classification problems. Third, we RTify the model to make all parameters trainable via backpropagation. This allows the model to automatically learn the optimal parameters to fit human RTs (see Fig. 3 for an illustration of the multi-population case and SI Fig. A.2 for an illustration of the two-population case).

## 3 Experiments

In this section, we validate our RTify framework on two psychophysics datasets: the RDM dataset [24; 38] and a natural image categorization dataset . As a side note, all models were trained on single Nvidia RTX GPUs (Titan/3090/A6000) with 24/24/48GB of memory each. All training can be completed in approximately 48 hours. Code and data are available at https://github.com/Yu-AngCheng/RTify.

Figure 2: **RTified model evaluation on a RDM task .** Human data are shown as a gray shaded area, and model fits are shown for **(A)** the “supervised” setting where human behavioral responses are used to train the models and **(B)** the “self-penalized” setting where no human data is used. Our approach (green) outperforms the two alternative approaches (brown), i.e., entropy-thresholding  for the “supervised” and uncertainty proxy  for the “self-penalized” settings (see Fig. 4 for MSE comparisons and Fig. S3 for all coherences).

### Random dot motion task

The RDM task is a classic experimental paradigm used to test temporal integration that has been extensively used in psychophysics , human imaging , and electrophysiology studies . The stimuli in this task consist of dots moving on a screen toward a predefined direction vs. randomly. For each time step, each dot only has a specific probability (coherence) (\(0.8\%,1.6\%,3.2\%,6.4\%,12.8\%,25.6\%\), or \(51.2\%\)) to move towards the pre-defined direction, making the task non-trivial. The participants must integrate motion information across time and report it when they are sufficiently confident. The original experimental data are from , where 21 young adult participants performed around 40,000 trials in total over 4 consecutive days.

First, we trained an RNN consisting of 5 convolutional blocks (Convolution, BatchNorm, ReLU, Max pooling) and a 4096-unit LSTM with BPTT. In the original experiment, the stimuli were shown on a 75 Hz CRT monitor for up to 2 seconds, and therefore, we also trained our RNN for the RDM stimuli for 150 frames. The RNN was trained for 100 epochs using the Adam optimizer with a learning rate of 1e-4 at full coherence (c = 99.9%) for the first 10 epochs as a warm-up and 1e-5 at all coherence levels for the remaining 90 epochs. Next, we trained our two different RTify modules. For fitting human RTs, it was trained for 10,000 epochs, and for self-penalty, it was trained for 20,000 epochs. In both cases, the Adam optimizer were used, and the weights of the task-optimized RNN were frozen while training the RTify modules.

We trained the first RTify module to predict human RTs by fitting human RT distributions. Here, positive RTs refer to RT with correct choices, and negative RTs refer to RTs with incorrect choices. Therefore, one distribution incorporates both speed and accuracy information from behavioral choices. Results are shown in Fig. 2. Our RTify model can predict the full RT distribution across all coherence levels. In comparison, the entropy-thresholding approach by  fails to capture the full distribution (see Fig. 2 for coherence = 51.2% to 6.4% and Fig. S3 for all coherences). Importantly, our method

Figure 3: **Illustration of RTifying feedforward neural networks**. We develop a multi-class compatible and fully differentiable RNN module based on the WW model . This module is implemented as an attractor-based RNN, and is stacked on top of a feedforward neural network. The feedforward neural network first takes an image as the input. Outputs from classification units of the network are then sent to RTified WW (**A**). Information is accumulated by multiple populations of neurons in RTified WW while they compete with each other (**B**). A decision is made and the process stops when one of the populations reaches a threshold. The number of time steps needed for the RTified WW to reach the threshold is used to predict human RT (**C**).

surpasses entropy-thresholding approach  (two-sided Wilcoxon signed-rank test, \(p<.05\); for MSE comparisons, see Fig. 4).

It is well known that there is a trade-off between RTs and accuracy in cognitive tasks. To investigate this relationship further, we extended our analysis to examine whether the model's accuracy would approximate human accuracy when it was fitted solely on human RTs without using human accuracy data. Given that conventional RT distributions encompass both RTs and accuracy information, we restricted our fitting procedure to the positive part of the distribution. That is, RTs corresponding to correct responses to prevent inadvertently incorporating human accuracy into the model. Remarkably, by fitting the model on human RTs only, the model naturally reached a classification accuracy comparable to human performance (see Fig. 4).

We also trained a self-penalized RTify module without any human data. This ideal-observer RNN model was trained to minimize the time steps needed for solving the RDM task (see section 2.2 for details). Human-like RTs emerge naturally in this neural network (See Fig. 2). Our model predicts RT data much better than previous approaches, which use a measure of uncertainty computed over the RNN as a surrogate metric. As can be seen, the resulting model tends to overfit the modes of the distribution.(see Fig. 2 for coherence = 51.2% to 6.4% and Fig. S3 for all coherences). Our method surpasses uncertainty proxy approach  (two-sided Wilcoxon signed-rank test, \(p<.05\), for MSE comparison, see Fig. 4). We also checked the model classification accuracy before and after self-penalized RTify. Interestingly, we also found the self-penalized RTified RNN demonstrated a human-like classification accuracy (see Fig. 4).

### Object recognition task

Unlike previous visual decision-making models, we want to show that our method can also be applied to natural images and multi-class datasets. Specifically, we consider an object recognition task, a classic paradigm used extensively in computer vision [43; 44] and cognitive neuroscience studies [45; 46]. The original data is from , where 88 participants perform the task through Mturk. The stimuli in this task belong to 10 categories, and for each category, there are 20 natural images

Figure 4: **(A) MSE comparisons for the RDM task  for all coherence levels. The RTified model trained in the ”supervised” setting (i.e., with human behavioral responses; green solid line) performs better (lower MSE) than entropy-thresholding  (brown solid line) under all coherence levels. Similarly, the RTified model trained in the ”self-penalized” setting (i.e., without human data; green dash line) performs better than uncertainty proxy  (brown dash line). With the help of our RTified WW module (orange solid line), a convolution neural network (C3D) can also fit the data better than entropy-thresholding . (B) Classification accuracy comparisons between pretrained and RTified models for the RDM task . The RTified model trained with human RTs data in the ”supervised” setting (green solid line) and in the ”self-penalized” setting (green dash line) achieve human-like classification accuracy under all coherence levels compared with the pretrained model without RTify (green dotted line). With the help of our RTified WW module (orange solid line), a CNN (C3D) matches human accuracy better than the pretrained model without RTify (orange dotted line).**

taken from the COCO dataset  and 112 synthetically generated images with different backgrounds and object positions.

We first train our RNN with BPTT to perform a 10-way classification task. In the original study, participants performed a binary classification task. However, since the individual binary pairs were not saved, we trained the model in a 10-class classification task. We used Cornet-S  pretrained on the Imagenet dataset  because it was used in the original study and it achieves a relatively high brainscore in terms of explaining neural activities . We trained the network for 100 epochs, using the Adam optimizer with a learning rate of 1e-4 and a learning scheduler (StepLR) with a step size of 2,000. We used 20 timesteps (instead of 2 in the original model) for the IT layer in the Cornet to achieve high temporal resolution. Results show that our RNN achieves 75.2% on a held-out test set. Similarly, we trained our two different RTify modules. For fitting human RTs, it was trained for 100,000 epochs, while for self-penalty, it was trained for 10,000 epochs with a learning scheduler (StepLR) with a step size of 2,000 and a gamma of 0.3. In both cases, Adam optimizers were used, and the weights of the task-optimized RNN were frozen while training the RTify modules.

We then extracted RTs from our RNN to fit human RTs. Notably, model RTs significantly correlate with the human RT observed in the psychophysics experiment (\(r=.51,p<.001\)). Besides, our

Figure 5: **RTified model evaluation on an object categorization task .** Model vs. human RT predictions for our RTified model (green) vs. alternative approaches (brown) **(A)** in the “supervised” setting where human behavioral responses are used to train the model and **(B)** the “self-penalized” setting where no human data is used. Solid lines are linear regression fits between model and human RTs. Crossed-shaded areas and the dashed lines are controls to show the fits after removing the highest model RTs. Our approach outperforms the two alternative approaches, i.e., entropy-thresholding  for the “supervised” setting and uncertainty proxy  for the “self-penalized” setting.

method also surpasses entropy-thresholding  (bootstrapping shows that our method is superior to theirs with a probability of 99.9%). We also extract RTs from an ideal-observer RNN model trained with a time self-penalty (see section 2.2 for details). We show here that model RTs are significantly correlated with human RTs (see Fig. 5, \(r=.40,p<.001\)). This failed to be captured by uncertainty proxy . We argue this in two ways. First, bootstrapping shows that our method is superior to theirs with a probability of 87.1%. Second, and most importantly, although the uncertainty seems to correlate with human RTs in the dataset (\(r=.36,p<.001\)), it only applies to high-uncertainty cases. When trials with high model uncertainty are excluded, uncertainty shows no significant correlation with human RTs (\(r=.05,p=0.32\)). However, using our method, the correlation remains strong and significant (\(r=.41,p<.001\)).

### RTifying feedforward neural networks

Given the prevalence of feedforward networks (e.g. CNNs) and their incredible performance in visual tasks, a natural question is how to align such networks in the temporal domain of decision-making. We thus developed a biologically plausible, multi-class, differentiable RNN module based on the WW recurrent circuit model . This module can be stacked on top of any neural network, even if not recurrent, and can be used to align model RTs with human RTs.

For the RDM task, we take a 3D CNN with 6 convolutional blocks (convolution, BatchNorm, ReLU, Max pooling) and an MLP. We train the network for 100 epochs using the Adam optimizer with a learning rate of 1e-4 at full coherence (c = 99.9%) for the first 10 epochs as a warm-up and 1e-5 at all coherence levels for the remaining 90 epochs. Since this model is not an RNN, it has no temporal dynamics. Therefore, we drop in our WW module and further train the WW module for 5,000 epochs using the Adam optimizer with a learning rate of 1e-4, a StepLR scheduler with a step size of 1,000 and gamma of 0.3, and a grad clip at 1e-5. Interestingly, when we RTify the C3D model using the WW module, it is able to capture the distribution of human RTs across all coherence levels, see Fig. 6A for coherence = 51.2% to 6.4% and Fig. S4 for all coherences, for MSE comparison see Fig. 4). Furthermore, we also observed a human-like classification accuracy for the model when it is solely trained to fit human RTs but not human accuracy (see Fig. 4).

Similarly, we take a VGG-19 pre-trained on Imagenet for the object recognition task and fine-tune it on the dataset provided by Kar et al.  in a 10-class classification way for the abovementioned reasons. We train the model for 100 epochs using a batch size of 32. The optimizer was AdamW,

Figure 6: **RTified WW model evaluation.** We combine our RTified WW module with **(A)** a 3D CNN to fit human RTs collected in an RDM task  (see Fig. 4 for MSE comparisons with other methods) and **(B)** a VGG to fit human RTs in a rapid object categorization task  (Crossed-shaded areas and the dashed lines are controls to show the fits after removing the highest model RTs).

with a learning rate of 1e-5. We use a OneCycleLR scheduler adjusted after 10 epochs of warm-up. Results show that our RNN achieves 81.6% on a held-out test set. We further train the WW module for 100,000 epochs using the Adam optimizer with a learning rate of 1e-4 and a grad clip at 0.0001. By using the multi-class version of the WW model, we show that the RTify VGG also exhibited a significant correlation with human data (\(r=.49,p<.001\), see Fig. 6B).

## 4 Conclusion

We have described a computational framework to train RNNs to learn to dynamically accumulate evidence nonlinearly so that decisions can be made based on a variable number of time steps to approximate human behavioral choices, including both decisions and RTs. We showed that such optimization can be used to fit an RNN directly to human behavioral responses. We also showed that such a framework can be extended to an ideal-observer model whereby the RNN is trained without human data but with self-penalty that encourages the network to make a decision as quickly as possible. Under this setting, human-like behavioral responses naturally emerge from the RNN - consistent with the hypothesis that humans achieve a speed-accuracy trade-off. Finally, we provided an RNN implementation of a popular neural circuit decision-making model, the WW model, as a trainable deep learning module that can be combined with any vision architecture to fit human behavioral responses. Our computational framework provides a way forward to integrating image-computable models with decision-making models, advancing toward a more comprehensive understanding of the brain mechanisms underlying dynamic vision.

LimitationsCertain limitations will need to be addressed in future work. Most of the human data used in our study remains relatively small-scale and is limited primarily to synthetic images because more naturalistic benchmarks only include behavioral choices  and lack RT data. To properly evaluate our approach and that of others, larger-scale psychophysics datasets using more realistic visual stimuli will be needed. There is already evidence that large-scale psychophysics data can be used to effectively align AI models with humans . We hope this work will encourage researchers to collect novel internet-scale benchmarks that include both behavioral choices and RTs.

Broader ImpactsAs AI vision models become more prevalent in our daily lives, ensuring their trustworthy behavior is increasingly important . Our framework contributes to this effort by exploring how to align certain aspects of models' behavior with human responses in specific contexts. While our approach is limited to predicting RT distributions, it constitutes a first step toward more human-aligned AI models.