# Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms. Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties. This molecular search problem closely resembles the "alignment" problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function. Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies. We show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function. Furthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small. We deploy this approach to align molecular transformers to generate molecules with externally specified properties and find that it does so robustly, searching through diverse parts of chemical space. While our focus here is on chemical search, we also obtain excellent results on an AI supervised task for LLM alignment, showing that the method is scalable and general.

## 1 Introduction

Large language models (LLMs) are trained on large corpora of text to autoregressively generate outputs. These models strongly reflect the distribution of the data on which they are trained , and controlling the outputs to reflect externally imposed preferences is an increasingly important challenge for deployment. The aforementioned task, often called "alignment", requires either careful curation of training data or large sets of human preference data--both options are labor-intensive . Reinforcement learning from human feedback (RLHF), a family of algorithms that employs these human preference datasets, has been widely employed to align instruction and chat models [21; 5], but it is both expensive to acquire the training data and difficult to carry out in practice . Recent algorithmic developments, such as direct preference optimization (DPO) , simplify the alignment framework by making the reward function implicit, but still require human preference data. While these algorithms succeed in constraining outputs, many "alignment"-like tasks require evaluation that would be difficult for human evaluators.

Generative sampling problems seeking to optimize a reward are common in chemistry, where comparing small molecules using a particular functional assay or computationally accessible propertyis often far easier than searching chemical space to identify novel compounds. Recent efforts to build large, domain-specific models for chemistry  have shown promising performance on both property prediction and reaction prediction tasks. Nevertheless, just as with LLMs, leveraging these models for molecule optimization requires first guiding "unaligned" models to favor important properties like synthetic accessibility or solubility. Here, we seek to productively search chemical space using transformers by introducing a new preference optimization algorithm, which we call energy rank alignment.

Our contribution:We formulate a generic alignment algorithm that we call _Energy Rank Alignment_ or ERA that leverages an explicit reward function to guide autoregressive sampling while targeting specific properties or preferences. Unlike reward maximization in RL-based algorithms, the policy that minimizes our objective is designed to sample fluctuations around a maximal reward value to promote sample diversity. Our algorithm enables direct gradient-based optimization of a policy to match the ideal preference distribution and converges asymptotically to an optimal distribution with tuneable entropy and controllable regularization, which we show theoretically. The minimizers of our objective are closely related to the minimizer of PPO and DPO, but we have more direct control over the influence of the regularization relative to fluctuations around the maximum reward. In numerical experiments, we demonstrate that this algorithm successfully aligns a molecule transformer model to identify a highly diverse set of chemicals with properties favored by our choice of reward. Finally, we also show that we obtain competitive performance with ERA on benchmark LLM alignment tasks, but emphasize that the chemical applications are the main focus of this paper.

### Related Work

Inverse molecular design tasks have a long history  and many recent works have sought to apply machine learning to facilitate this difficult search problem [27; 12; 13]. While reinforcement learning has proved a popular strategy for molecular optimization [39; 27], several recent studies have sought to use transformers  trained on large databases of molecules represented with the text-based SMILES syntax [10; 30; 35; 4] for such tasks. Schwaller et al.  utilized an atom-wise tokenization, which we also employ, to train a transformer for the downstream task of reaction prediction. These "chemical language models" have been studied for applications on downstream tasks, including property prediction [4; 10] and reaction prediction [23; 30].

Building scalable strategies for alignment has attracted enormous attention because of the high cost and complexity of constraining LLM outputs. Much of the current paradigm is built on reinforcement learning from human feedback (RLHF) . Within this framework, human preferences provided in the form of pairwise rankings are first used to train a reward model, and subsequently that reward model is used to optimize a policy using, for example, proximal policy optimization (PPO) . Rafailov et al.  demonstrated that the reward model can be treated implicitly using a scheme that maximizes the likelihood of the preferences given an offline dataset. Because this approach does not require training a reward model, it has been named Direct Preference Optimization (DPO). Our work differs from both strategies; first, unlike RLHF, we do not employ reinforcement learning

Figure 1: Energy rank alignment (ERA) enables targeting low-energy, high-reward regions with controllable fluctuations. Optimal policy approaches Boltzmann distribution with low regularization (\( 0\)) and reference policy with high regularization (\(\)) (left). Aligned models can be used to sample molecules with desired chemical properties (right).

and instead develop an explicit, gradient-based objective for the optimal policy. Secondly, unlike DPO, we leverage an explicit reward function and add regularization transparently, both of which help to avoid greedy policies . However, like both approaches, we assume that the Bradley-Terry model  of preference data is appropriate for the underlying target distribution.

Many recent works have built upon the ideas of RLHF and DPO, including studies on the effect of point-wise sampling of preference distributions , investigations into the theoretical basis for contrastive methods for unlearning target datasets , and alternatives to the Bradley-Terry pairwise preference model [20; 2]. One recent study explores alignment in the context of inverse molecular design: Park et al.  applies DPO to SMILES generators to increase the probability of activity for generated compounds against a drug target. However, they indicate that many preferences in chemistry are expressed as continuous signals, which is not suitable for DPO. Overcoming this limitation while maintaining the advantages of a direct gradient-based policy optimization strategy is a central goal of our current work. Our analysis and methodology directly addresses issues related to point-wise sampling because the explicit reward function eliminates overly greedy assignments of preference probabilities. Indeed, as discussed in Sec. 4, we see that DPO mode collapses where ERA shifts the policy towards the target distribution. While non-transitive preferences may arise in some settings, leading to a breakdown of the Bradley-Terry preference distribution model, by construction our target rewards are determined by quantitative evaluations of properties, and are therefore transitive.

## 2 Energy rank alignment

A policy is a conditional probability distribution \((|):\); we generate an output \(\) from prompt \(\). The spaces \(\) and \(\) are discrete and finite, corresponding to sequences of tokenized outputs of the model with a maximum length. In alignment tasks, we begin with a pre-trained reference policy \(_{}\) and seek to optimize a parametric, trainable policy \(_{}\) to adapt the conditional sampling for a particular task or constraint.

Consider a prompt \(\) and model outputs \(,^{}\) and a collection of preferences \(=\{(_{i}^{}_{i};_{i})\}_{i=1}^{n}\); the notation \(\) indicates that \(_{i}\) is preferred to \(^{}_{i}\). The conditional probability that \(^{}\) given \(\) can be modeled as a pairwise Boltzmann ranking within the Bradley-Terry model, i.e.,

\[p(^{}|)=,)}}{e^{-  U(,)}+e^{- U(,^{})}}  U(,^{})- U(,).\] (1)

Here \(>0\) is a constant, \((x)=(1+e^{-x})^{-1}\) and we refer to \(U:\) as an energy function to make clear the connection to statistical physics, but it is the negative reward within the RL framework for alignment.

To impose the preferences we minimize the objective

\[J()=_{}[ U(,)( |)+^{-1}(1+)(|)-(_ {}(|))(|)],\] (2)

where \(^{-1}\) is a parameter controlling the magnitude of the entropic term, \(\) sets the scale of the Kullback-Leibler regularization compared with the energy term, and \(\) is a probability distribution over the prompts \(()\). A proximal scheme for gradient descent on this objective corresponds to a gradient flow on \(J\)[28; 19]; the functional can be viewed as a free energy, and the corresponding flow is

\[_{t}_{t}=(_{t}_{}J[_{t}] ),\] (3)

and \(_{}\) denotes the Frechet derivative with respect to \(\). Assuming that \(_{0}\) has full support on \(\), the optimization converges asymptotically to stationary policy which satisfies

\[_{}J[_{}]=0_{} e^{-U+_{}},\] (4)

and this minimizer is globally optimal. In the context of LLM alignment, a representation of the energy function \(U:\) is learned as a "reward model", though we also consider tasks in which \(U\) is an easily evaluated function of the pair \((,)\). The optimal distribution \(_{}\) is a Gibbs-Boltzmann measure

\[_{}(|)=Z^{-1}()[- U(,)-^{-1}_{}(|) ]\] (5)where \(Z()\) is the \(\)-dependent normalization constant. This expression makes clear the effect of \(\): when \(\) (low temperature), the reward dominates and fluctuations around the maximal reward are small, which could lead to "mode-seeking"; when \( 0\) (high physical temperature) fluctuations around the maximal reward increase and the regularization term favors proximity to \(_{}\). Similarly, \( 0\) recovers a Gibbs-Boltzmann distribution proportional to \(e^{- U}\) at inverse temperature \(\), while \(\) is dominated by the reference policy.

Loss functions for \(_{}\):Proximal Policy Optimization (PPO) optimizes an indirect, proximal objective to minimize an objective closely related to (2) (cf. Appendix A). Direct Preference Optimization (DPO) treats the negative reward function \(U\) implicitly and directly maximizes the likelihood of \(p(^{}|)\). Our objectives differ from both approaches: like DPO, we directly optimize the policy using an explicit, gradient-based objective, but, in contrast, we use a reward function directly in our objective. The losses we build are thus amenable to both offline (samples from \(_{}\)) and online (samples from \(_{}\)) policy alignment, as explained below. Choosing to optimize the objective online has been shown to have important consequences on performance , though we focus here on the setting where samples are drawn offline.

We directly optimize the Kullback-Leibler divergence between the entropy-regularized preference distribution \(p_{}(^{}|)\) and the corresponding parametric preference distribution \(p_{}(^{}|)\). Explicitly, using the fact that conditional preference distribution is normalized, we obtain

\[D_{}^{(,^{})}(p_{}|p_{}) =p_{}(^{}|)(^{}|)}{p_{}(^{ }|)}+p_{}(^{}|)(^{}|)}{p_{}(^{} |)},\] \[=p_{}(^{}|)(^{}|)}{p_{}(^{ }|)}+(1-p_{}(^{}|)) (^{}|)}{1-p_{}( ^{}|)},\] (6)

where

\[p_{}:=([(U(,^{} )-U(,))+^{-1}}(|)}{_{}(^{}|)}]).\] (7)

This quantity is a well-defined KL divergence and is hence non-negative; the quantity vanishes when \(p_{}=p_{}\) on the observations \(,^{}\). Furthermore, with access to an explicit reward model, all terms in (6) can be computed directly and

\[p_{}(^{}|^{})=}(|)}{_{}(|)+_{}( ^{}|)}=(}(| )}{_{}(^{}|)}).\] (8)

To obtain a minimizer of the regularized objective defined in (2) we optimize

\[^{}(_{})=_{x} _{,^{}_{}(|)}D_{ }^{(,^{})}(p_{}|p_{});\] (9)

If the current policy overlaps with the target preference distribution, it may be useful to sample directly from the partially aligned policy, i.e., to use the "on-policy" formulation,

\[^{}_{}(_{})=_{}_{,^{}_{}(|)}D_{}^{(,^{})}(p_{}|p_{})\] (10)

instead of (9). One issue that arises with this scheme is that differentiation with respect to the parameters of the policy \(\) because \(\) and \(^{}\) are decoded into discrete tokens, an operation that is not differentiable. To remedy this, we importance sample with a reference policy

\[^{}_{}(_{})=_{}_{,^{}_{}(|)}}(|)_{}(^{ }|)}{_{}(|)_{}(^{ }|)}D_{}^{(,^{})}(p_{}|p_{}).\] (11)

This reweighting is straightforward and the importance weights should generally be appreciable, especially early in training when \(_{}\) has not drifted far from \(_{}\). It is, of course, also natural to iteratively update \(_{}\) using a previous iterate as the reference policy. In this work, we only use (9) as an objective and leave the on-policy objectives to future work.

## 3 Theoretical Analysis

To understand the ERA loss function and its connection to the entropy regularized objective (2), we first establish that the minimizers of (6) are of the form (5). We first define the notion of equivalence precisely.

**Definition 3.1**: _The conditional probability measures \((|)\) and \(^{}(|)\) are conditionally equivalent if \(\), \(\) and \(^{}\) are such that \(_{}|(|)-^{}(|)|=0\)._

We remark that this strong form of equivalence is appropriate on the finite, discrete spaces \(\) and \(\) we consider here.

**Lemma 3.1**: _If \(\) is conditionally equivalent to \(^{}\), then \(^{}_{g}(|)^{}(|)e^{g()}\) is conditionally equivalent to \(\) for all functions \(g:\) such that \(_{}|e^{g()}|<+\)._

We prove Lemma 3.1 in Appendix A and use this simple lemma to prove the following result.

**Proposition 3.2**: _Suppose \((|)()\) and that \(()=(_{})\). Let \(>0\), \( 0\) and that the reward model is such that \(_{,}|e^{-U(,)}|<+\). Then, the minimizer of \(^{}\) is conditionally equivalent to \(_{}\)._

First, we verify that any probability measure \(_{g}(|)(-U(,)-^{-1}_{}(|)+g())\) minimizes the objective. Because \(^{}\) is non-negative, it suffices to show that for all pairs \(,^{}\), \(D^{(,^{})}_{}(p_{}|p_{}) 0\). This follows immediately from the cancellation in the preference probability \(p_{}\) of \(e^{g()}\) after factorization in (5). Now, suppose that \((|)(-U(,)-^{-1}_{}(|))\) where we have taken \(g()=0\) without loss of generality and \(:=_{g}\). Assume that for all pairs \(,^{}\), the divergence \(D^{(,^{})}_{}(p_{}|p_{}) 0\) which is required of a minimizer. Equivalently, it must be the case that for all \(,^{}\),

\[|)}{(|)+(^{}|)}= (|)}{_{}(|)+_{}(^{}|)}\ \ ^{}|)}{(|)}=( {y}^{}|)}{_{}(|)},\] (12)

from which we see that

\[(|)=^{}|)}{e^{-U(,^{})-^{-1}_{ }(^{}|)}}e^{-U(, {y})-^{-1}_{}(|)}.\] (13)

By construction, \((|)\) does not depend on \(^{}\) so the prefactor must be purely a function of \(\), which completes the proof, using Lemma 3.1.

Gradients of \(^{}\).One advantage of the ERA framework is that the objective is amenable to direct, gradient-based optimization. We remark that establishing global convergence for the optimization of \(\) using (9) requires establishing convexity with respect to the parameters, which is not obviously the case for our objective, nor those used in PPO and DPO. However, one can still glean some insight into the optimization by examining the gradients on a samplewise basis. Using the compact notation \(p_{}(^{}|)_{}\) and \(p_{}(^{}|)_{}\),

\[_{}^{}=_{ }_{,^{}_{}}( }{1-_{}}-}{_{ }})_{}_{}.\] (14)

The gradient is straightforward to interpret on a particular pair \(,^{}\): if \(p_{}(^{}|)\) is larger than \(p_{}(^{}|)\) then the preference gradient is positive and gradient descent lowers the probability that \(^{}\). The opposite occurs whenever \(p_{}(^{}|)\) is smaller than \(p_{}(^{}|)\). The magnitude of the gradient is scaled by the degree of misspecification of the preference probability.

This calculation highlights one key difference between the approach we use and DPO. When the data only contains one observation of \(^{}\) for a given \(\), the DPO objective's implicit reward model assigns zero probability to \(^{}\). This pushes the policy towards extremal values, which can lead to undesired behavior, as discussed in Azar et al. . In our formulation, this behavior occurs only when the reward model assigns an energy of \(\), which is prohibited by construction in most tasks. We further discuss differences between ERA and DPO in Appendix A.2.

## 4 Experiments

We test ERA on both chemical and language tasks to shed light on the following questions: 1) Can we use ERA to robustly fine-tune our model to generate samples according to a desired distribution?2) What is the effect of changing the inverse-temperature \(\) during ERA? 3) Do we maintain sample diversity (and validity) without regularizing to remain close to a reference policy, and what is the effect of increased regularization? 4) Can we simultaneously target multiple properties with high fidelity, and how can we trade off between desired properties? 5) Can we carry out ERA on higher capacity models with "weak" signals from smaller models?

### Generating molecules with desired properties

We use a decoder-only representation for the molecular generator , where the generator has 2 layers, an embedding dimension of 512, a vocabulary of 324 tokens, and totals 3.5M parameters. Starting from a random initialization, we carry out pretraining on a dataset of 2.4M small molecules from the ChEMBL database  for 180 epochs. This version of the model is not conditioned on a prompt and generates a small molecule given just a start-of-sequence token. We use this pretrained model as our reference policy for all unprompted molecular alignment tasks (Sec. 4.1.1). In Sec. 4.1.2, we generate molecules conditioned on a prompt using a generator that was trained to carry out sampling with a prompt molecule.

Central to ERA is, of course, access to a computable energy function. As a proof-of-concept, here we consider 5 different properties for which the corresponding energy function is easily evaluable: Quantitative Estimate of Drug-Likeness (QED) , Wildman-Crippen LogP (LogP) , Ring Count, Molar Refractivity (MR) , and Tanimoto Similarity . Briefly, LogP is a measure of the hydrophobicity of a molecule, MR is a measure of the polarizability of the molecule, and Tanimoto similarity is a measure of the similarity between two molecules (see Appendix C.2).

#### 4.1.1 Unprompted molecular alignment

First, we independently target four different properties using ERA with an unprompted molecular generator (Fig. 2). Using the reference policy, we generate a dataset \(=\{_{1}^{(i)},_{2}^{(i)},U(_{1}^{(i)}),U(_ {2}^{(i)})\}_{i=1}^{N}\) and carry out energy rank alignment on \(_{}\), where \(_{}\) is initialized using the weights of \(_{}\). Here, \(_{1},_{2}_{}\) and \(\) and \(U()\) denote the generated molecule and its corresponding energy, respectively. For MR, Ring Count, and LogP, we define the energy \(U\) to be

Figure 2: Unprompted molecular generator alignment. Distributions of different chemical properties for molecules sampled from aligned and unaligned policies. The center of the harmonic potential, \(\), is varied for MR (\(=1.0\)), Ring Count (\(=1.0\)), and LogP (\(=10.0\)), while \(\) is varied for QED. All experiments were run with no regularization to the reference policy (\(=0\)).

a harmonic potential centered at a target value. For QED, we define the energy to be the negative logarithm of QED and vary \(\) to assess its impact on alignment (see Table 1, 2). In Fig. 2, we see that we successfully shift the distribution to target means that are both greater and lower than the average value of MR, Ring Count, and LogP under the reference policy. Furthermore, in the alignment of QED, we observe the effect of changing \(\) on the learned policy; with increased \(,\) the learned policy concentrates around low-energy samples (i.e. near \(=1\)), and with lower \(,\) the learned policy samples a greater range of QED values, as expected. We note that for each of these four experiments, we did not regularize towards the reference policy (i.e. \(=0\)). Even so, we were able to maintain both sample diversity and maintain appreciable sample validity (see Fig. 7 and Table 3).

Many molecular design tasks require balancing multiple properties, and designing an objective for multi-property alignment is straightforward within the ERA framework. To demonstrate this, we generate molecules with both high QED and LogP using ERA with an energy function weighted by property-specific \(\): \(U=_{}U_{}+_{}U_{}\) (see Table 1, 4 for details on energy function). We carry out ERA with different pairs of \((_{},\ _{})\) using the same procedure as above, and from Fig. 3, we see that we target multiple properties with varying fidelity by simply modulating the value of property-specific \(.\) Ultimately, increasing the \(\) for an individual property enables us to favor higher values of that property in multi-property alignment setting. In this case, we also do not regularize with the KL-divergence to the reference policy and again maintain sample diversity and validity (see Fig. 8 and Table 4)

#### 4.1.2 Prompted molecular alignment

Inspired by the task of lead optimization in drug discovery efforts , we ask whether we can use ERA to train a molecular generator that can sample a molecule that is both similar to the prompt molecule _and_ also exhibits some desired property.

First, we fine-tune the pretrained molecular generator to enable prompted molecular generation (see Appendix C.3.2) and use this fine-tuned model as our reference policy for all prompted molecular alignment tasks. This reference policy disproportionately samples molecules that are identical (i.e. a Tanimoto similarity of 1.0) to the prompt molecule (see Fig. 4), so we carry out multi-property alignment on this reference policy to generate molecules that are similar--but not identical--to the prompt molecule and also have a high drug-likeness as measured by QED. Using ERA, we optimize the reference policy with a generated dataset \(=\{(_{1}^{(i)},^{(i)}),(_{2}^{(i)},^{(i) }),U(_{1}^{(i)},^{(i)}),U(_{2}^{(i)},^{(i)})\}_{i=1}^{ N},\) where we sample four molecules for each prompt molecule from the reference policy and consider all possible preference pairs for a total of six preference pairs per prompt molecule (see Appendix C.2 for full details on energy used).

We observe that the per-prompt average QED under the optimized policy for a given prompt is higher than the corresponding average under the reference policy (Fig. 4). Furthermore, we see that we are able to sample a diverse set of molecules that are chemically similar to the prompt molecule, and

Figure 3: Unprompted multi-property molecular generator alignment. 2D histograms of LogP versus QED for different combinations of property-specific \(\) illustrating a clear trade-off when performing multi-property alignment. Relative increases in \(\) for a given property target higher values for that property. All experiments were run with no regularization to the reference policy (\(=0\)).

also chemically valid (see Figure 9, Table 5). We repeat the experiment with a related objective of generating molecules similar to the prompt molecule with a high LogP instead and again observe that we increase the per-prompt average LogP under the optimized policy relative to the reference policy without degrading sample diversity and validity. For both of these experiments, we required regularization to the reference policy. With no regularization, the aligned generator would almost exclusively sample sequences that were chemically invalid (\(<25\%\) chemical validity). Finally, we note that the increases in QED and LogP in Fig. 4 are smaller relative to the increases in Fig. 2 because the samples are now conditioned to remain proximal to the prompt molecule, which restricts the chemical space that can be explored.

### AI-guided alignment of large language models

We test the generality of ERA by applying it to align large language models (LLMs). Similar to the experiments in , we first carry out ERA on a GPT-2 model  fine-tuned on movies reviews from IMDb . We use a pretrained sentiment classifier  to evaluate the energies--where lower energies correspond to more positive sentiments--of sampled responses from the reference policy and carry out ERA using the same approach as in Section 4.1.2 (see Appendix D.1). We vary the regularization strength \(\) and inverse-temperature \(\) on the average sentiment and observe that across all regularization strengths, with increasing \(,\) the average sentiment becomes more positive. Increasing regularization also elicits more positive sentiments. Qualitatively, with lower

Figure 4: Prompted multi-property molecular generator alignment. From left to right: Tanimoto similarities computed between the prompt and sampled molecules for both aligned and unaligned policies (QED and Tanimoto alignment), per-prompt difference in the average QED under aligned and unaligned policies (QED and Tanimoto alignment), Tanimoto similarities computed between the prompt and sampled molecules for both aligned and unaligned policies (LogP and Tanimoto alignment), and per-prompt difference in the average LogP under aligned and unaligned policies (LogP and Tanimoto alignment). With alignment, we target higher QED and LogP values, while still sampling molecules chemically similar—but not identical—to prompt molecule.

Figure 5: AI-guided alignment of LLMs. Average sentiment of responses from aligned GPT-2 model across all prompts. (left). Proportion of unsafe content relative to unaligned model of responses aligned LLaMA2-13B model across all prompts (right). 5.4% of all responses from unaligned model were classified as unsafe. Error bars too small to be shown.

regularization, we observe that text quality degrades and becomes less coherent, likely resulting in lower average sentiment predictions by the sentiment model. Regularization here is important to ensure high quality text samples.

We next leverage a "weak" AI supervisor to carry out LLM alignment, a task sometimes called "superalignment" . In the present context, we order "weak" vs. "strong" models based on their parameter count (within the same family) and empirical performance; i.e., LLaMA2-7B is weaker than LLaMA2-13B. Here, the weak model does not necessarily contain the complexity of the stronger model but can _weakly_ discern between different outputs of a stronger model. Given a sample \(_{i}_{}(|)\), we define the energy using the weak model \(U(_{i}|)=-_{}(_{i}|)\).

We test _weak-to-strong alignment_ using a previously aligned LLaMA2-7B-Chat (meta-llama/Llama-2-7b-chat) to optimize an unaligned LLaMA2-13B (meta-llama/Llama-2-13b) model . Using prompts from the Anthropic Helpful and Harmless dialogue dataset , we first carry out a short supervised fine-tuning step of LLaMA2-13B to ensure it can output text in a chat-like format (see Appendix D.2). Using this reference policy, we generate a dataset with energies computed from the smaller LLaMA2-7B-Chat model and carry out ERA as above, again across varying \(\) and \(\). We evaluate the "safety" of generated samples using Meta LLama Guard 2 (meta-llama/Meta-Llama-Guard-2-8B) . We observe that as we increase \(\), the proportion of unsafe content relative to the unaligned, reference model decreases, with over a 90% drop between the unaligned model and the models aligned with the highest \(\) across all \(\). For these experiments, we observe that varying regularization strengths has a minimal effect and that we are in fact able to generate coherent sentences with no regularization, with strong regularization hurting performance for \(=0.1\). Finally, we compare ERA and DPO in Appendix D.2 and observe that with our implementation of DPO, we are able to generate lower energy samples, but that it is prone to mode collapse. We caution that our implementation of DPO is likely not optimal and that we did not exhaustively tune the hyperparameters of DPO due to resource constraints.

## 5 Conclusions and Limitations

This paper introduces energy rank alignment, a simple and effective algorithm for policy optimization with an explicit reward model. We find that ERA is stable without extensive hyperparameter tuning, and sufficiently general to successfully align both application-specific transformers for chemical search problems as well as generative pre-trained transformers for language. The algorithm exhibits strong performance with a variety of reward models, even ones with relatively weak signal, such as the AI feedback of LLaMA2-7B-Chat. Interestingly, with this approach we are able to reduce unsafe content by more than 90% with no human preference data.

We analyze the minimizers of the ERA objective and find that they differ from the minimizers of popular policy alignment algorithms DPO and PPO in an important way: unlike PPO, the strength of regularization to the reference policy that we add is controlled by a parameter \(\), while the entropy of the target distribution is independently tuned by a distinct parameter \(\). This means that we can avoid greedy policies by keeping \(\) small--amplifying fluctuations around the optimum of the reward model \(-U\)--while reducing the influence of the reference policy by taking \(\) small. Our objective leads to easily interpretable sample-wise gradients which highlight the importance of a reward model relative to DPO in the sampled objective. Similar observations about the inadequacy of the DPO objective for finite preference observations were also made theoretically in Azar et al. .

Limitations: First, our approach requires a reward model, which can be difficult to train or design, especially for complex tasks. While we observed that ERA makes an appreciable impact even with weak supervision from an AI chat model, this sort of proxy may not be available for more complex tasks. For example, optimizing small molecules for high binding affinity to a target protein would require expensive and noisy evaluations of a reward model, which likely limits the scope of molecular design to problems where the reward can be computed somewhat efficiently. A second limitation of our present work is that we do not train the molecular transformer to favor synthetic accessibility nor do we explicitly seek to obtain molecules that are easily synthesized experimentally. There are models that seek to evaluate synthesizability computationally that could be used in our rewards, which we plan to explore in future work . A final limitation of our current work is the moderate scale of our numerical experiments due to our limited compute resources, including the inadequate hyperparameter tuning for the DPO baseline for Fig. 5.