# Active Representation Learning for General Task Space with Applications in Robotics

Yifang Chen\({}^{1}\), Yingbing Huang\({}^{2}\), Simon S. Du\({}^{1*}\), Kevin Jamieson\({}^{1*}\), Guanya Shi\({}^{3*}\)

\({}^{1}\) Paul G. Allen School of Computer Science & Engineering

University of Washington, Seattle,WA

{yifangc, ssdu, jamieson, guanyas}@cs.washington.edu

\({}^{2}\) University of Illinois Urbana-Champaign, Champaign, IL

{yh21}@illinois.edu

\({}^{3}\) Robotics Institute, Carnegie Mellon University, Pittsburgh, PA

{{guanyas }@andrew.cmu.edu

\({}^{*}\) Equal advising

###### Abstract

Representation learning based on multi-task pretraining has become a powerful approach in many domains. In particular, task-aware representation learning aims to learn an optimal representation for a specific target task by sampling data from a set of source tasks, while task-agnostic representation learning seeks to learn a universal representation for a class of tasks. In this paper, we propose a general and versatile algorithmic and theoretic framework for _active representation learning_, where the learner optimally chooses which source tasks to sample from. This framework, along with a tractable meta algorithm, allows most arbitrary target and source task spaces (from discrete to continuous), covers both task-aware and task-agnostic settings, and is compatible with deep representation learning practices. We provide several instantiations under this framework, from bilinear and feature-based nonlinear to general nonlinear cases. In the bilinear case, by leveraging the non-uniform spectrum of the task representation and the calibrated source-target relevance, we prove that the sample complexity to achieve \(\)-excess risk on target scales with \((k^{*})^{2}\|v^{*}\|_{2}^{2}^{-2}\) where \(k^{*}\) is the effective dimension of the target and \(\|v^{*}\|_{2}^{2}(0,1]\) represents the connection between source and target space. Compared to the passive one, this can save up to \(}\) of sample complexity, where \(d_{W}\) is the task space dimension. Finally, we demonstrate different instantiations of our meta algorithm in synthetic datasets and robotics problems, from pendulum simulations to real-world drone flight datasets. On average, our algorithms outperform baselines by \(20\%-70\%\). 1

## 1 Introduction

Recently, few-shot machine learning has enjoyed significant attention and has become increasingly critical due to its ability to derive meaningful insights for target tasks that have minimal data, a scenario commonly encountered in real-world applications. This issue is especially prevalent in robotics where data collection and training data is prohibitive to collect or even non-reproducible (e.g., drone flying with complex aerodynamics  or legged robots on challenging terrains ). Onepromising approach to leveraging the copious amount of data from a variety of other sources is multi-task learning, which is based on a key observation that different tasks may share a common low-dimensional representation. This process starts by pretraining a representation on source tasks and then fine-tuning the learned representation using a limited amount of target data ().

In conventional supervised learning tasks, accessing a large amount of source data for multi-task representation learning may be easy, but processing and training on all that data can be costly. In real-world physical systems like robotics, this challenge is further amplified by two factors: (1) switching between different tasks or environments is often significantly more expensive (e.g., reset giant wind tunnels for drones ); (2) there are infinitely many environments to select from (i.e., environmental conditions are continuous physical parameters like wind speed). Therefore, it is crucial to minimize not only the number of samples, but the number of sampled source tasks, while still achieving the desired performance on the target task. Intuitively, not all source tasks are equally informative for learning a universally good representation or a target-specific representation. This is because source tasks can have a large degree of redundancy or be scarce in other parts of the task space. In line with this observation, Chen et al.  provided the first provable active representation learning method that improves training efficiency and reduces the cost of processing source data by prioritizing certain tasks during training with theoretical guarantees. On the other hand, many existing works  prove that it is statistically possible to learn a universally good representation by randomly sampling source tasks (i.e., the passive learning setting).

The previous theoretical work of  on active multi-task representation learning has three main limitations. First, it only focuses on a finite number of discrete tasks, treating each source independently, and therefore fails to leverage the connection between each task. This could be sub-optimal in many real-world systems like robotics for two reasons: (1) there are often infinitely many sources to sample from (e.g., wind speed for drones); (2) task spaces are often highly correlated (e.g., perturbing the wind speed will not drastically change the aerodynamics). In our paper, by considering a more general setting where tasks are parameterized in a vector space \(\), we can more effectively leverage similarities between tasks compared to treating them as simply discrete and different. Secondly, the previous work only considers a single target, while we propose an algorithm that works for an arbitrary target space and distribution. This is particularly useful when the testing scenario is time-variant. Thirdly, we also consider the task-agnostic setting by selecting \((k)\) representative tasks among the \(d_{W}\) high dimension task space, where \(k d_{W}\) is the dimension of the shared representation. Although this result does not improve the total source sample complexity compared to the passive learning result in the bilinear setting , it reduces the number of tasks used in the training and therefore implicitly facilitates the training process.

In addition to those theoretical contributions, we extend our proposed algorithmic framework beyond a pure bilinear representation function, including the _known_ nonlinear feature operator with unknown linear representation (e.g., random features with unknown coefficients), and the totally _unknown nonlinear representation_ (e.g., deep neural network representation). While some prior works have considered nonlinear representations  in passive learning, the studies in active learning are relatively limited . All of these works only consider non-linearity regarding the input, rather than the task parameter. In this paper, we model task-parameter-wise non-linearity and show its effectiveness in experiments. Note that it particularly matters for task selections because the mapping from the representation space to task parameters to is no longer linear.

See more related works and how our problem scope is different from theirs in Appendix A.

### Summery of contributions

* We propose the first generic active representation learning framework that admits any arbitrary source and target task space. This result greatly generalizes previous works where tasks lie in the discrete space and only a single target is allowed. To show its flexibility, we also provide discussions on how our framework can accommodate various supervised training oracles and optimal design oracles. (Section 3)
* We provide theoretical guarantees under a benign setting, where inputs are i.i.d. and a unit ball is contained in the overall task space, as a compliment to the previous work where tasks lie on the vertices of the whole space. In the target-aware setting, to identify an \(\)-good model our method requires a sample complexity of \(}(kd_{X}(k^{*})^{2}\|v^{*}\|_{2}^{2}\{k^{*},^{ 2}\}^{-2})\) where \(k^{*}\) is the effective dimension of the target, \(\) is the conditional number of representation matrix, and \(\|v^{*}\|_{2}^{2}(0,1]\) represents the connection between source and target space that will be specified in the main paper. Compared to passive learning, our result saves up to a factor of \(}{d_{W}}\) in the sample complexity when targets are uniformly spread over the \(k\)-dim space and up to \(}\) when targets are highly concentrated. Our results further indicate the necessity of considering the continuous space by showing that directly applying the previous algorithm onto some discretized sources in the continuous space (e.g., orthonormal basis) can lead to worse result. Finally, ignoring the tasks used in the warm-up phases, in which only a few samples are required, both the target-aware and the target-agnostic cases can save up to \(}(k^{*}+k)\) number of tasks compared to the passive one which usually requires \(d_{W}\) number of tasks. (Section 4)
* We provide comprehensive experimental results under different instantiations beyond the benign theoretical setting, studying synthetic and real-world scenarios: 1) For the synthetic data setting in a continuous space, we provide results for pure linear, known nonlinear feature operator \(_{X}\) and unknown nonlinear representation \(_{X}\). Our target-aware active learning (AL) approach shows up to a significant budget saving (up to \(68\%\)) compared to the passive approach and the target-agnostic AL approach also shows an advantage in the first two cases. 2) In a pendulum simulation with continuous task space, we provide the results for known nonlinear feature operator \(_{X}\) and \(_{W}\) and show that our target-aware AL approach has up to \(20\%\) loss reduction compared to the passive one, which also translates to better nonlinear control performance. 3) Finally, in the real-world drone dataset with a discrete task space, we provide results for unknown linear and nonlinear representation \(_{X}\) and show that our target-aware AL approach converges much faster than the passive one. (Section 5)

## 2 Preliminary

Multi-task (or multi-environments).Each task or environment is parameterized by a known vector \(w^{d_{W}}\). We denote the source and target task parameter space as \(_{}^{d_{W}},_{}^{d_{W}}\). These spaces need not be the same (e.g., they could be different sub-spaces). In the discrete case, we set \(w\) as a one-hot encoded vector and therefore we have in total \(d_{W}\) number of candidate tasks while in the continuous space, there exist infinitely many tasks. For convenience, we also use \(w\) as the subscript to index certain tasks. In addition, we use \(_{}(_{}),_{} (_{})\) to denote the task distribution for the sources and targets.

Data generation.Let \(^{d_{X}}\) be the input space. We first assume there exists some _known_ feature/augmentation operator \(_{X}:^{d_{_{X}} d_{W}},_{W}: ^{d_{_{W}} d_{W}}\), that can be some non-linear operator that lifts \(w,x\) to some higher dimensional space (e.g., random Fourier features ). Notice that the existence of non-identical \(\) indicates the features are not pairwise independent and the design space of \(_{}\) is not benign (e.g., non-convex), which adds extra difficulty to this problem.

Then we assume there exists some _unknown_ underlying representation function \(_{X}:()\) which maps the augmented input space \(()\) to a shared representation space \(^{k}\) where \(k d_{_{X}},k d_{_{W}}\), and its task counterparts \(_{W}:()\) which maps parameterized task space to the feature space. Here the representation functions are restricted to be in some function classes \(\), e.g., linear functions, deep neural networks, etc.

In this paper, we further assume that \(_{W}\) is a linear function \(B_{W}^{k d_{_{W}}}\). To be more specific, for any fixed task \(w\), we assume each sample \((x,y)_{w}\) satisfies

\[y=_{X}(_{X}(x))^{}B_{W}_{W}(w)+,(0, ^{2})\] (1)

For convenience, we denote \(Z_{w}\) as the collection of \(n_{w}\) sampled data \((x_{w}^{1},y_{w}^{1}),...,(x_{w}^{n_{w}},y_{w}^{n_{w}})_{w}\). We note that when \(_{X},_{W}\) is identity and \(_{X}\) is linear, this is reduced to standard linear setting in many previous papers [9; 11; 12; 8].

The task diversity assumption.There exists some distribution \(p(_{})\) that \(_{w p}_{}(B_{W}_{W}(w)_{W}(w)^{}B_{W}^{ })>0\), which suggests the source tasks are diverse enough to learn the representation.

Data collection protocol.We assume there exists some i.i.d. data sampling oracle given the environment and the budget. To learn a proper representation, we are allowed access to an _unlimited_\(n_{}\) number of data from source tasks during the learning process by using such an oracle. Then at the end of the algorithm, we are given a few-shot of _mix_ target data \(Z_{}=\{Z_{w}\}_{w_{}}\) which is used for fine-tuning based on learned representation \(_{X}\). Denote \(n_{}\) as the number of data points in \(Z_{}\).

Data collection protocol for target-aware setting.When the target task is not a singleton, we additionally assume a few-shot of _known environment_ target data \(_{}:=\{Z_{w},w\}_{w_{}}\), where \(|_{}|=(_{})\) and \(_{}=\{_{W_{}}_{ }(WW^{})\}\). Again denote \(_{}\) as the number of data points in \(_{}\), we have \(_{} n_{}^{2/3} n_{}\).

_Remark 2.1_.: Here \(|_{}|\) represents vectors that can cover every directions of \(_{}\) space. This extra \(_{}\) requirement comes from the non-linearity of \(l_{2}\) loss and the need to learn the relationship between sources and targets. We want to emphasize that such an assumption implicitly exists in previous active representation learning  since \(_{}=Z_{}\) in their single target setting. Nevertheless, in a passive learning setting, only mixed \(Z_{}\) is required since no source selection process involves. Whether such a requirement is necessary for target-aware active learning remains an open problem.

Other notations.Let \(e_{i}\) to be one-hot vector with \(1\) at \(i\)-th coordinates and let \(_{i}=2^{-i}\).

### Goals

Expected excess risk.For any target task space \(_{}\) and its distribution \(_{}\) over the space, as well as a few-shot examples as stated in section 2, our goal is to minimize the expected excess risk with our estimated \(_{X}\)

\[(_{X},_{})=_{w_{0}_{}}_{(x,y)_{w_{0}}}\|_{X}(_{X}(x))^{} _{}-y\|_{2}\]

where \(_{}=_{w}_{(x,y) Z_{}}|| _{X}(_{X}(x))w-y\|_{2}\), which average model estimation that captures the data behavior under the expected target distribution. Note that the \(_{},_{}\) are given in advance in the target-aware setting.

The number of tasks.Another side goal is to save the number of long-term tasks we are going to sample during the learning process. Since a uniform exploration over \(d_{W}^{}\)-dimension is unavoidable during the warm-up stage, we define long-term task number as

\[|\{w_{}\ |\ \ n_{w}( ^{-})\}|\]

where \(\) is some arbitrary exponent and \(\) is the target accuracy and \(n_{w}\) is number of samples sampled from task \(w\) as defined above.

## 3 A general framework

Our algorithm 1 iteratively estimates the shared representation \(_{X},_{W}\) and the next target relevant source tasks which the learner should sample from by solving several optimal design oracles

\[g(f,A)=_{q(_{savce})}_{}(( q(w)f(w )f(w)^{})^{-1}A)\] (2)

This exploration and exploitation (target-aware exploration here) trade-off is inspired by the classical \(\)-greedy strategy, but the key difficulty in our work is to combine that with multi-task representation learning and different optimal design problems. The algorithm can be generally divided into three parts, and some parts can be skipped depending on the structure and the goal of the problem.

* [leftmargin=*,noitemsep,topsep=0pt,parsep=0pt,leftmargin=*]
* **Coarse exploration:** The learner uniformly explores all the directions of the \(_{}\) (denoted by distribution \(q_{0}\)) in order to find an initial \(k\)-dimension subspace \(V\) that well spans over the representation space (i.e., \(B_{W}B_{W}^{} B_{W}VV^{}B_{W}^{} cB_{W}B_{W}^{ }\) for some arbitrary constant \(c}{k}\)). To give an intuitive example, suppose \(B_{W}^{2 d_{W}^{}+1}\) has the first half column equals \(e_{1}\) and the second hard equals \(e_{2}\). Then instead of uniformly choosing \(\{e_{i}\}_{i[d_{W}^{}]}\) task, we only need explore over two tasks \(V=^{}}}[1,1,,0,0,],V=^{}}}[0,0,,1,1,]\).

We want to highlight that the sample complexity of this warm-up stage only scales with \(d_{_{X}},k\) and the spectrum-related parameters of \(B_{W}\) (i.e., \((B_{W}),_{}(B_{X})\)), not the desired accuracy \(\).
* **Fine target-agnostic exploration:** The learner iteratively updates the estimation of \(V\) and uniformly explore for \(}(_{j}^{-})\) times on this \(k\), instead of \(d_{_{W}}\) subspace, denoted by distribution \(q_{1}\). (Note this \(_{j}^{-}\) comes from the exploration part in \(\)-greedy, which is \((n_{2}^{j})^{}\)) Such reduction not only saves the cost of maintaining a large amount of physical environment in real-world experiments but also simplifies the non-convex multi-task optimization problem. Of course, when \(k=(d_{_{W}})\), we can always uniformly explore the whole \((d_{_{W}}\) space as denoted in the algorithm. Note that theoretically, \(q_{1}\) only needs to be computed once as shown in 4. In practice, to further improve the accuracy while saving the task number, the \(q_{1}\) can be updated only when a significant change from the previous one happens, which is adopted in our experiments as shown in appendix E.1.
* **Fine target-aware exploration.** In the task-awareness setting, the learner estimates the most-target-related sources parameterized by \(\{w\}\) based on the current representation estimation and allocates more budget on those, denoted by distribution \(q_{2}\). By definition, \(q_{2}\) should be more sparse than \(q_{1}\) and thus allowing the final sample complexity only scales with \(k^{*}\), which measures the effective dimension in the source space that is target-relevant.

Computational oracle for optimal design problem.Depending on the geometry of \(\{_{W}(w)\}_{w_{}}\), the learner should choose proper offline optimal design algorithms to solve \(g(f,A)\). Here we propose several common choices. 1). When \(_{}\) contains a ball, we can approximate the solution via an eigendecomposition-based closed-form solution with an efficient projection as detailed in Section 4. 2) When \(_{}\) is some other convex geometry, we can approximate the result via the Frank-Wolfe type algorithms , which avoids explicitly looping over the infinite task space. 3) For other even harder geometry, we can use discretization or adaptive sampling-based approximation . In our experiments, we adopt the latter one and found out that its running time cost is almost neglectable in our pendulum simulator experiment in Section 5, where the \(_{W}\) is a polynomial augmentation.

Offline optimization oracle \(^{X}_{}\).Although we are in the continuous setting, the sampling distribution \(q_{0},q_{1},q_{2}\) is sparse. Therefore, our algorithm allows any proper passive multi-task

[MISSING_PAGE_FAIL:6]

_As long as the number of target samples satisfies_

\[n_{}((k+(1/))^{-2}), _{}(^{-}(k ^{*})^{}(d_{W}^{}^{- }+k^{-}d_{W}^{}^{2}^{-}))\]

Comparison with passive learning.By choosing \(\{e_{i}\}_{i[d_{W}^{}]}\) as a fixed source set, we reduce the problem to a discrete setting and compare it with the passive learning. In , the authors get \(N_{}\) as most \(d_{W}\|_{w_{0}}B_{W}w_{0}w_{0}^{}B _{W}^{}\|}{_{}^{2}(B_{W}^{})}^{-2}\). We first consider the cases in their paper that the target task is uniformly spread \(\|_{w_{0}}B_{W}w_{0}w_{0}^{}B_{W}^{}\|=\).

* When the task representation is well-conditioned \(_{}^{2}(B_{W}^{})=}{k}\). We have a passive one as \(}(kd_{X}^{-2})\) while the active one \(}(kd_{X}}{d_{W}}^{-2})\) (See Lemma B.8 for details), which suggests as long as \(d_{W} k^{2}\), our active learning algorithm gain advantage even in a relatively uniform spread data and representation conditions.
* Otherwise, we consider the extreme case that \(_{}^{2}(B_{W}^{})=1\). We have passive one \(}(d_{X}d_{W}^{-2})\) while the active one \(}(k^{3}d_{X}^{-2})\). Notice here we require \(d_{W} k^{3}\).

Both of them indicate the necessity of considering the continuous case with large \(d_{W}\) even if everything is uniformly spread. On the other hand, whether we can achieve the same result as the passive one when \(d_{W} k^{3}\) remains to be explored in the future.

We then consider the single target \(w_{0}\) case.

* With well-conditioned \(B_{W}\), the passive one now has sample complexity \((k^{2}d_{X}^{-2})\) while the active gives a strictly improvement \((d_{X}}{d_{W}}^{-2})\).
* With ill-conditioned \(B_{W}\) where \(_{}(B_{W})=1\) and \(_{i}\|W_{i}^{*}\|=1\), that is, only a particular direction in source space contributes to the target. The Passive one now has sample complexity \((kd_{X}d_{W}^{-2})\) while our active one only has \(kd_{X}^{-2}\), which demonstrates the benefits of our algorithm in unevenly distributed source space.

Comparison with previous active learning.By using the same discrete reduction and set single target \(w_{0}\), we compare our result with the current state-of-art active representation algorithm in . They achieves \(}(kd_{X}\|\|_{1}^{2}^{-2})\), where \(=_{}\|\|_{1}\) s.t \(B_{W}=B_{W}w_{0}\). On the other hand, our active one gives \(}(kd_{X}\|w^{*}\|_{2}^{2}^{-2})\), where \(w^{*}=_{}\|\|_{2}\) s.t \(B_{W}=B_{W}w_{0}\), which is strictly better than the discrete one. This again indicates the separation between continuous and discrete cases where in fixed discrete sets, the \(L_{1}\) norm regularization is strictly better than \(L_{2}\).

Furthermore, when a fixed discrete set is given, which is exactly the setting in . Their algorithm can be seen as a computationally efficient reduction under ours.(Appendix B.5.)

Save task number.When ignoring the short-term initial warm-up stage, we only require maintaining \(}(k+(N_{}k^{*}))\) number of source tasks, where the first term comes from \(q_{1}\) in the target-agnostic stage and the second term comes from \(q_{2}\) in the target-aware stage.

## 5 Experiment

In this section, we provide experimental results under different instantiations of the Algorithm 1, and all of them show the effectiveness of our strategy both in target-aware and target-agnostic settings.

### Settings

Datasets and problem definition.Our results cover the different combinations of \(_{X},_{X},_{W}\) as shown in Table 1. Here we provide a brief introduction for the three datasets and postpone the details into Appendix E. 2

* **Synthetic data.** We generate data that strictly adhere to our data-generating assumptions and use the same architecture for learning and predicting. When \(_{X}\) is nonlinear, we use a neural network \(_{X}\) to generate data and use a slightly larger neural net for learning. The goal for synthetic data is to better illustrate our algorithm as well as serve as the first step to extend our algorithm on various existing datasets.
* **Pendulum simulator.** To demonstrate our algorithm in the continuous space. we adopt the multi-environment pendulum model in  and the goal is to learn a \(w\)-dependent residual dynamics model \(f(x,w)\) where \(x\) is the pendulum state and \(w^{5}\) including external wind, gravity and damping coefficients. \(f(x,w)\) is highly nonlinear with respective to \(x\) and \(w\). Therefore we use known non-linear feature operators \(_{X},_{W}\). In other words, this setting can be regarded as a misspecified linear model. It is also worth noting that due to the non-invertibility of \(_{W}\), the explicit selection of a source via a closed form is challenging. Instead, we resort to an adaptive sampling-based method discussed in Section 3. Specifically, we uniformly sample \(w\) from the source space, select the best \(w^{}\), and then uniformly sample around this \(w^{}\) at a finer grain. Our findings indicate that about 5 iterations are sufficient to approximate the most relevant source.
* **Real-world drone flight dataset .** The Neural-Fly dataset  includes real flight trajectories using two different drones in various wind conditions. The objective is to learn the residual

   & identity \(_{W}\) & nonlinear \(_{W}\) \\  identity \(_{X}\) and linear \(_{X}\) & synthetic, drone & NA \\  nonlinear \(_{X}\) and linear \(_{X}\) & synthetic & pendulum simulator \\  identity \(_{X}\) and nonlinear \(_{X}\) & synthetic, drone & NA \\  

Table 1: Summary of different instantiationsaerodynamics model \(f(x,w)^{3}\) where \(x^{11}\) is the drone state (including velocity, attitude, and motor speed) and \(w\) is the environment condition (including drone types and wind conditions). We collect 6 different \(w\) and treat each dimension of \(f(x,w)\) as a separate task. Therefore \(w\) is reformulated as a one-hot encoded vector in \(^{18}\).

For each dataset/problem, we can choose different targets. For simplicity, in the following subsection, we present results for one target task for each problem with 10 random seeds regarding random data generation and training, and put more results in Appendix E. In all the experiments, we use a gradient-descent joint training oracle, which is a standard approach in representation learning.

### Results

Those results encapsulate the effectiveness of active learning in terms of budget utilization and test loss reduction. In the drone dataset, we further demonstrate its ability in identifying relevant source tasks (see Figure 2). We note that in two robotics problems (pendulum simulation and real-world drone dataset), the active learning objective is to learn _a better dynamics model_. However, in the pendulum simulation, we deploy a model-based nonlinear controller which translates better dynamics modeling to enhanced control performance (see Figure 1 and Appendix E.2).

   & Target-aware AL & Target-agnostic AL \\  identity \(_{X}\) and linear \(_{X}\) & 38.7\% & 51.6\% \\  nonlinear \(_{X}\) and linear \(_{X}\) & 38.7\% & 45.2\% \\  identity \(_{X}\) and non-linear \(_{X}\) & 32.0\% & 68.0\% \\  

Table 2: Results on synthetic data. Using the test loss of the final output model from passive learning as a baseline, we show the ratio between the budget required by target-aware/target-agnostic active learning to achieve a similar loss and the budget required by passive learning.

Figure 1: Results on pendulum simulator for a specific target. **Left:** The test loss of the estimated model \(\). The passive strategy suffers from negative transfer while the active strategy steadily decreases. **Right:** The control error using final output \(\). Here we use a model-based nonlinear policy \((x,)\). The model learned from active strategy leads to better control performance.

Figure 2: Results on the real drone dataset  with target drone_type_A_30_z. Source data includes two drone types A and B, six wind speeds from 0 to 50, and three directions x-y-z. We present results for linear \(_{X}\) here and postpone the non-linear \(_{X}\) case in Appendix E.3. **Left:** The test loss of the estimated bilinear model \(\). The passive strategy converges slower than the active strategy. **Right:** Top 10 the most similar source tasks. Given the target environment, the algorithm successfully finds the other drone_type_A environments as relevant sources. See more explanations in Appendix E.3.