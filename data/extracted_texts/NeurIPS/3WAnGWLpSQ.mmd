# Why Does Sharpness-Aware Minimization Generalize Better Than SGD?

Zixiang Chen Junkai Zhang Yiwen Kou Xiangning Chen Cho-Jui Hsieh Quanquan Gu

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

{chenzx19,zhang,evankou,xiangning,chohsieh,qgu}@cs.ucla.edu

Equal contribution.

###### Abstract

The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby facilitating more effective learning of features. Experiments on both synthetic and real data corroborate our theory.

## 1 Introduction

The remarkable performance of deep neural networks has sparked considerable interest in creating ever-larger deep learning models, while the training process continues to be a critical bottleneck affecting overall model performance. The training of large models is unstable and difficult due to the sharpness, non-convexity, and non-smoothness of its loss landscape. In addition, as the number of model parameters is much larger than the training sample size, the model has the ability to memorize even randomly labeled data (Zhang et al., 2021), which leads to overfitting. Therefore, although traditional gradient-based methods like gradient descent (GD) and stochastic gradient descent (SGD) can achieve generalizable models under certain conditions, these methods may suffer from unstable training and harmful overfitting in general.

To overcome the above challenge, _Sharpness-Aware Minimization_ (SAM) (Foret et al., 2020), an innovative training paradigm, has exhibited significant improvement in model generalization and has become widely adopted in many applications. In contrast to traditional gradient-based methods that primarily focus on finding a point in the parameter space with a minimal gradient norm, SAM also pursues a solution with reduced sharpness, characterized by how rapidly the loss function changes locally. Despite the empirical success of SAM across numerous tasks (Bahri et al., 2021; Behdin et al., 2022; Chen et al., 2021; Liu et al., 2022), the theoretical understanding of this method remains limited.

Foret et al. (2020) provided a PAC-Bayes bound on the generalization error of SAM to show that it will generalize well, while the bound only holds for the infeasible average-direction perturbation instead ofpractically used ascend-direction perturbation. Andriushchenko and Flammarion (2022) investigated the implicit bias of SAM for diagonal linear networks under global convergence assumption. The oscillations in the trajectory of SAM were explored by Bartlett et al. (2022), leading to a convergence result for the convex quadratic loss. A concurrent work (Wen et al., 2022) demonstrated that SAM can locally regularize the eigenvalues of the Hessian of the loss. In the context of least-squares linear regression, Behdin and Mazumder (2023) found that SAM exhibits lower bias and higher variance compared to gradient descent. However, all the above analyses of SAM utilize the Hessian information of the loss and require the smoothness property of the loss implicitly. The study for non-smooth neural networks, particularly for the classification task, remains open.

In this paper, our goal is to provide a theoretical basis demonstrating when SAM outperforms SGD. In particular, we consider a data distribution mainly characterized by the signal \(\) and input data dimension \(d\), and prove the following separation in terms of test error between SGD and SAM.

**Theorem 1.1** (Informal statement of Theorems 3.2 and 4.1).: _Let \(p\) be the strength of the label flipping noise. For any \(>0\), under certain regularity conditions, with high probability, there exists \(0 t T\) such that the training loss converges, i.e., \(L_{S}(^{(t)})\). Besides,_

1. _For SGD, when the signal strength_ \(\|\|_{2}(d^{1/4})\)_, we have_ \(L_{}^{0-1}(^{(t)}) p+\)_. When the signal strength_ \(\|\|_{2} O(d^{1/4})\)_, we have_ \(L_{}^{0-1}(^{(t)}) p+0.1\)_._
2. _For SAM, provided the signal strength_ \(\|\|_{2}(1)\)_, we have_ \(L_{}^{0-1}(^{(t)}) p+\)_._

Our contributions are summarized as follows:

* We discuss how the loss landscape of two-layer convolutional ReLU networks is different from the smooth loss landscape and thus the current explanation for the success of SAM based on the Hessian information is insufficient for neural networks.
* To understand the limit of SGD, we precisely characterize the conditions under which benign overfitting can occur in training two-layer convolutional ReLU networks with SGD. To the best of our knowledge, this is the first benign overfitting result for neural network trained with mini-batch SGD. We also prove a phase transition phenomenon for SGD, which is illustrated in Figure 1.
* Under the conditions when SGD leads to harmful overfitting, we formally prove that SAM can achieve benign overfitting. Consequently, we establish a rigorous theoretical distinction between SAM and SGD, demonstrating that SAM strictly outperforms SGD in terms of generalization error. Specifically, we show that SAM effectively mitigates noise learning in the early stages of training, enabling neural networks to learn features more efficiently.

**Notation.** We use lower case letters, lower case bold face letters, and upper case bold face letters to denote scalars, vectors, and matrices respectively. For a vector \(=(v_{1},,v_{d})^{}\), we denote

Figure 1: Illustration of the phase transition between benign overfitting and harmful overfitting. The yellow region represents the regime under which the overfitted CNN trained by SGD is guaranteed to have a small excess risk, and the blue region represents the regime under which the excess risk is guaranteed to be a constant order (e.g., greater than \(0.1\)). The gray region is the regime where the excess risk is not characterized.

by \(\|\|_{2}:=(_{j=1}^{d}v_{j}^{2})^{1/2}\) its \(12\)-norm. For two sequence \(\{a_{k}\}\) and \(\{b_{k}\}\), we denote \(a_{k}=O(b_{k})\) if \(|a_{k}| C|b_{k}|\) for some absolute constant \(C\), denote \(a_{k}=(b_{k})\) if \(b_{k}=O(a_{k})\), and denote \(a_{k}=(b_{k})\) if \(a_{k}=O(b_{k})\) and \(a_{k}=(b_{k})\). We also denote \(a_{k}=o(b_{k})\) if \(|a_{k}/b_{k}|=0\). Finally, we use \(()\) and \(()\) to omit logarithmic terms in the notation. We denote the set \(\{1,,n\}\) by \([n]\), and the set \(\{0,,n-1\}\) by \(\), respectively. The carnality of a set \(S\) is denoted by \(|S|\).

## 2 Preliminaries

### Data distribution

Our focus is on binary classification with label \(y\{ 1\}\). We consider the following data model, which can be seen as a special case of sparse coding model (Olshausen and Field, 1997; Allen-Zhu and Li, 2022; Ahn et al., 2022).

**Definition 2.1**.: Let \(^{d}\) be a fixed vector representing the signal contained in each data point. Each data point \((,y)\) with input \(=[^{(1)},^{(2)},,^{(P) }]^{}^{P d},^{(1)},^{(2)}, ,^{(P)}^{d}\) and label \(y\{-1,1\}\) is generated from a distribution \(\) specified as follows:

1. The true label \(\) is generated as a Rademacher random variable, i.e., \([=1]=[=-1]=1/2\). The observed label \(y\) is then generated by flipping \(\) with probability \(p\) where \(p<1/2\), i.e., \([y=]=1-p\) and \([y=-]=p\).
2. A noise vector \(\) is generated from the Gaussian distribution \((,_{p}^{2})\), where \(_{p}^{2}\) is the variance.
3. One of \(^{(1)},^{(2)},,^{(P)}\) is randomly selected and then assigned as \(y\), which represents the signal, while the others are given by \(\), which represents noises.

The data distribution in Definition 2.1 has also been extensively employed in several previous works (Allen-Zhu and Li, 2020; Jelassi and Li, 2022; Shen et al., 2022; Cao et al., 2022; Kou et al., 2023). When \(P=2\), this data distribution aligns with the one analyzed in Kou et al. (2023). This distribution is inspired by image data, where the input is composed of different patches, with only a few patches being relevant to the label. The model has two key vectors: the feature vector and the noise vector. For any input vector \(=[^{(1)},,^{(P)}]\), there is exactly one \(^{(j)}=y\), and the others are random Gaussian vectors. For example, the input vector \(\) can be \([y,,,],,[,,y,]\) or \([,,,y]\): the signal patch \(y\) can appear at any position. To avoid harmful overfitting, the model must learn the feature vector rather than the noise vector.

### Neural Network and Training Loss

To effectively learn the distribution as per Definition 2.1, it is advantageous to utilize a shared weights structure, given that the specific signal patch is not known beforehand. When \(P>n\), shared weights become indispensable as the location of the signal patch in the test can differ from the location of the signal patch in the training data.

We consider a two-layer convolutional neural network whose filters are applied to the \(P\) patches \(_{1},,_{P}\) separately, and the second layer parameters of the network are fixed as \(+1/m\) and \(-1/m\) respectively, where \(m\) is the number of convolutional filters. Then the network can be written as \(f(,)=F_{+1}(_{+1},)-F_{-1}(_ {-1},)\), where \(F_{+1}(_{+1},)\) and \(F_{-1}(_{-1},)\) are defined as

\[F_{j}(_{j},)=m^{-1}{_{r=1}^{m}}_{p=1}^{P}( _{j,r},^{(p)}).\] (1)

Here we consider ReLU activation function \((z)=(z 0)z\), \(_{j,r}^{d}\) denotes the weight for the \(r\)-th filter, and \(_{j}\) is the collection of model weights associated with \(F_{j}\) for \(j= 1\). We use \(\) to denote the collection of all model weights. Denote the training data set by \(=\{(_{i},y_{i})\}_{i[n]}\). We train the above CNN model by minimizing the empirical cross-entropy loss function

\[L_{}()=n^{-1}{_{i[n]}}(y_{i}f(, _{i})),\]

where \((z)=(1+(-z))\).

### Training Algorithm

**Minibatch Stochastic Gradient Descent.** For epoch \(t\), the training data set \(S\) is randomly divided into \(H:=n/B\) mini batches \(_{t,b}\) with batch size \(B 2\). The empirical loss for batch \(_{t,b}\) is defined as \(L_{_{t,b}}()=(1/B)_{i_{t,b}}(y_{i}f( ,_{i}))\). Then the gradient descent update of the filters in the CNN can be written as

\[^{(t,b+1)}=^{(t,b)}-_{}L_{ _{t,b}}(^{(t,b)}),\] (2)

where the gradient of the empirical loss \(_{}L_{_{t,b}}\) is the collection of \(_{_{j,r}}L_{_{t,b}}\) as follows

\[_{_{j,r}}L_{_{t,b}}(^{(t,b)}) =_{i_{t,b}}^{(t,b)}_{i }^{}(^{(t,b)}_{j,r},_{i})  jy_{i}_{i}\] \[+_{i_{t,b}}^{(t,b)} _{i}^{}(^{(t,b)}_{j,r},_{i})_{i}y_{i}j,\] (3)

for all \(j\{ 1\}\) and \(r[m]\). Here we introduce a shorthand notation \(^{(t,b)}_{i}=^{}[y_{i} f(^{(t,b)},_{i})]\) and assume the gradient of the ReLU activation function at \(0\) to be \(^{}(0)=1\) without loss of generality. We use \((t,b)\) to denote epoch index \(t\) with mini-batch index \(b\) and use \((t)\) as the shorthand of \((t,0)\). We initialize SGD by random Gaussian, where all entries of \(^{(0)}\) are sampled from i.i.d. Gaussian distributions \((0,_{0}^{2})\), with \(_{0}^{2}\) being the variance. From (3), we can infer that the loss landscape of the empirical loss is highly non-smooth because the ReLU function is not differentiable at zero. In particular, when \(^{(t,b)}_{j,r},\) is close to zero, even a very small perturbation can greatly change the activation pattern \(^{}(^{(t,b)}_{j,r},)\) and thus change the direction of \(_{_{j,r}}L_{_{t,b}}(^{(t,b)})\). This observation prevents the analysis technique based on the Taylor expansion with the Hessian matrix, and calls for a more sophisticated activation pattern analysis.

**Sharpness Aware Minimization.** Given an empirical loss function \(L_{S}()\) with trainable parameter \(\), the idea of SAM is to minimize a perturbed empirical loss at the worst point in the neighborhood ball of \(\) to ensure a uniformly low training loss value. In particular, it aims to solve the following optimization problem

\[_{}L_{S}^{}(), L_{S}^ {}():=_{\|\|_{2}}L_{S}(+),\] (4)

where the hyperparameter \(\) is called the perturbation radius. However, directly optimizing \(L_{S}^{}()\) is computationally expensive. In practice, people use the following sharpness-aware minimization (SAM) algorithm (Foret et al., 2020; Zheng et al., 2021) to minimize \(L_{S}^{}()\) efficiently,

\[^{(t+1)}=^{(t)}-_{}L_{S} +},}=}L_{S}()}{\|_{ }L_{S}()\|_{F}}.\] (5)

When applied to SGD in (2), the gradient \(_{}L_{S}\) in (5) is further replaced by stochastic gradient \(_{}L_{_{t,b}}\)(Foret et al., 2020). The detailed algorithm description of SAM in shown in Algorithm 1.

``` Input: Training set \(=_{i=1}^{n}\{(_{i},_{i})\}\), Batch size \(B\), step size \(>0\), neighborhood size \(>0\).  Initialize weights \(^{(0)}\). for\(t=0,1,,T-1\)do  Randomly divide the training data set into \(H\) mini batches \(\{_{t,b}\}_{b=0}^{H-1}\). for\(b=0,1,,H-1\)do  We calculate the perturbation \(}^{(t,b)}=}L_{_{t,b} }(^{(t,b)})}{\|_{}L_{_{t,b}}(^{( t,b)})\|_{F}}\).  Update model parameters: \(^{(t,b+1)}=^{(t,b)}-_{}L_{_{ t,b}}()|_{=^{(t,b)}+}^{(t,b)}}\). endfor  Update model parameters: \(^{(t+1,0)}=^{(t,H)}\) endfor ```

**Algorithm 1** Minibatch Sharpness Aware Minimization

## 3 Result for SGD

In this section, we present our main theoretical results for the CNN trained with SGD. Our results are based on the following conditions on the dimension \(d\), sample size \(n\), neural network width \(m\), initialization scale \(_{0}\) and learning rate \(\).

**Condition 3.1**.: Suppose there exists a sufficiently large constant \(C\), such that the following hold:

1. Dimension \(d\) is sufficiently large: \(d\{nP^{-2}_{p}^{-2}\|\|_{ 2}^{2},n^{2},P^{-2}_{p}^{-2}Bm\}\).
2. Training sample size \(n\) and neural network width satisfy \(m,n(1)\).
3. The \(2\)-norm of the signal satisfies \(\|\|_{2}(P_{p})\).
4. The noise rate \(p\) satisfies \(p 1/C\).
5. The standard deviation of Gaussian initialization \(_{0}\) is appropriately chosen such that \(_{0}P_{p}d/,\| \|_{2}}^{-1}\).
6. The learning rate \(\) satisfies \(P^{2}_{p}^{2}d^{3/2}/(Bm), P^{2}_{p}^{2}d/B,n\|\|_{2}/(_{0}Bm),\)

\(nP_{p}\|\|_{2}/(B^{2}m)}^{-1}\).

The conditions imposed on the data dimensions \(d\), network width \(m\), and the number of samples \(n\) ensure adequate overparameterization of the network. Additionally, the condition on the learning rate \(\) facilitates efficient learning by our model. By concentration inequality, with high probability, the \(_{2}\) norm of the noise patch is of order \((d_{p}^{2})\). Therefore, the quantity \(d_{p}^{2}\) can be viewed as the strength of the noise. Comparable conditions have been established in Chatterji and Long (2021); Cao et al. (2022); Frei et al. (2022); Kou et al. (2023). Based on the above condition, we first present a set of results on benign/harmful overfitting for SGD in the following theorem.

**Theorem 3.2** (Benign/harmful overfitting of SGD in training CNNs).: _For any \(>0\), under Condition 3.1, with probability at least \(1-\) there exists \(t=(^{-1}^{-1}mnd^{-1}P^{-2}_{p}^{-2})\) such that:_

1. _The training loss converges, i.e.,_ \(L_{S}(^{(t)})\)_._
2. _When_ \(n\|\|_{2}^{4} C_{1}dP^{4}_{p}^{4}\)_, the test error_ \(L_{}^{0-1}(^{(t)}) p+\)_._
3. _When_ \(n\|\|_{2}^{4} C_{3}dP^{4}_{p}^{4}\)_, the test error_ \(L_{}^{0-1}(^{(t)}) p+0.1\)_._

Theorem 3.2 reveals a sharp phase transition between benign and harmful overfitting for CNN trained with SGD. This transition is determined by the relative scale of the signal strength and the data dimension. Specifically, if the signal is relatively large such that \(n\|\|_{2}^{4} C_{1}d(P-1)^{4}_{p}^{4}\), the model can efficiently learn the signal. As a result, the test error decreases, approaching the Bayes risk \(p\), although the presence of label flipping noise prevents the test error from reaching zero. Conversely, when the condition \(n\|\|_{2}^{4} C_{3}d(P-1)^{4}_{p}^{4}\) holds, the test error fails to approach the Bayes risk. This phase transition is empirically illustrated in Figure 2. In both scenarios, the model is capable of fitting the training data thoroughly, even for examples with flipped labels. This finding aligns with longstanding empirical observations.

The negative result of SGD, which encompasses the third point of Theorem 3.2 and the high test error observed in Figure 2, suggests that the signal strength needs to scale with the data dimension to enable benign overfitting. This constraint substantially undermines the efficiency of SGD, particularly when dealing with high-dimensional data. A significant part of this limitation stems from the fact that SGD does not inhibit the model from learning noise, leading to a comparable rate of signal and noise learning during iterative model parameter updates. This inherent limitation of SGD is effectively addressed by SAM, as we will discuss later in Section 4.

### Analysis of Mini-Batch SGD

In contrast to GD, SGD does not utilize all the training data at each iteration. Consequently, different samples may contribute to parameters differently, leading to possible unbalancing in parameters. To analyze SGD, we extend the signal-noise decomposition technique developed by Kou et al. (2023); Cao et al. (2022) for GD, which in our case is formally defined as:

**Lemma 3.3**.: _Let \(_{j,r}^{(t,b)}\) for \(j\{ 1\}\), \(r[m]\) be the convolution filters of the CNN at the \(b\)-th batch of \(t\)-th epoch of gradient descent. Then there exist unique coefficients \(_{j,r}^{(t,b)}\) and \(_{j,r,i}^{(t,b)}\) such that_

\[_{j,r}^{(t,b)}=_{j,r}^{(0,0)}+j_{j,r}^{(t,b)} \|\|_{2}^{-2}+_{i=1 }^{n}_{j,r,i}^{(t,b)}\|_{i}\|_{2}^{-2} {}_{i}.\] (6)

_Further denote \(_{j,r,i}^{(t,b)}:=_{j,r,i}^{(t,b)}\,(_{j,r,i }^{(t,b)} 0)\), \(_{j,r,i}^{(t,b)}:=_{j,r,i}^{(t,b)}\,(_{j,r,i }^{(t,b)} 0)\). Then_

\[_{j,r}^{(t,b)}=_{j,r}^{(0,0)}+j_{j,r}^{(t,b)}\| \|_{2}^{-2}+_{i=1}^{n} _{j,r,i}^{(t,b)}\|_{i}\|_{2}^{-2}_{i}+_{i=1}^{n}_{j,r,i}^{(t,b)}\| _{i}\|_{2}^{-2}_{i}.\] (7)

Note that (7) is a variant of (6): by decomposing the coefficient \(_{j,r,i}^{(t,b)}\) into \(_{j,r,i}^{(t,b)}\) and \(_{j,r,i}^{(t,b)}\), we can streamline our proof process. The normalization terms \(\), \(\|\|_{2}^{-2}\), and \(\|_{i}\|_{2}^{-2}\) ensure that \(_{j,r}^{(t,b)}_{j,r}^{(t,b)},\) and \(_{j,r}^{(t,b)}(P-1)_{j,r}^{(t,b)},_{i}\). Through signal-noise decomposition, we characterize the learning progress of signal \(\) using \(_{j,r}^{(t,b)}\), and the learning progress of noise using \(_{j,r}^{(t,b)}\). This decomposition turns the analysis of SGD updates into the analysis of signal noise coefficients. Kou et al. (2023) extend this technique to the ReLU activation function as well as in the presence of label flipping noise. However, mini-batch SGD updates amplify the complications introduced by label flipping noise, making it more difficult to ensure learning. We have developed advanced methods for coefficient balancing and activation pattern analysis. These techniques will be thoroughly discussed in the sequel. The progress of signal learning is characterized by \(_{j,r}^{(t,b)}\), whose update rule is as follows:

\[_{j,r}^{(t,b+1)}=_{j,r}^{(t,b)}-[_{i _{t,b} S_{+}}_{i}^{(t,b)}^{}( _{j,r}^{(t,b)},_{i}).\]

\[.-_{i_{t,b} S_{-}}_{i}^{(t,b)}^{ }(_{j,r}^{(t,b)},_{i} )]\|\|_{2}^{2}.\] (8)

Here, \(_{t,b}\) represents the indices of samples in batch \(b\) of epoch \(t\), \(S_{+}\) denotes the set of clean samples where \(y_{i}=_{i}\), and \(S_{-}\) represents the set of noisy samples where \(y_{i}=-_{i}\). The updates of \(_{j,r}^{(t,b)}\) comprise an increment arising from sample learning, counterbalanced by a decrement due to noisy sample learning. Both empirical and theoretical analyses have demonstrated that overparametrization allows the model to fit even random labels. This occurs when the negative term \(_{i_{t,b} S_{-}}_{i}^{(t,b)}^{}( _{j,r}^{(t,b)},y_{i})\) primarily drives model learning. Such unfavorable scenarios can be attributed to two possible factors. Firstly, the gradient of the loss \(_{i}^{(t,b)}\) might be significantly larger for noisy samples compared to clean samples. Secondly, during certain epochs, the majority of samples may be noisy, meaning that \(_{t,b} S_{-}\) significantly outnumbers \(_{t,b} S_{+}\).

To deal with the first factor, we have to control the ratio of the loss gradient with regard to different samples, as depicted in (9). Given that noisy samples may overwhelm a single batch, we impose an additional requirement: the ratio of the loss gradient must be controllable across different batches within a single epoch, i.e.,

\[_{i}^{(t,b_{1})}/_{k}^{(t,b_{2})} C_{2}.\] (9)

As \(^{}(z_{1})/^{}(z_{2})(z_{2}-z_{1})\), we can upper bound \(_{i}^{(t,b_{1})}/_{k}^{(t,b_{2})}\) by \(y_{i} f(^{(t,b_{1})},_{i})-y_{k} f(^{(t,b_{2})},_{k})\), which can be further upper bounded by \(_{r}_{y_{i},r,i}^{(t,b_{1})}-_{r}_{y_{i },r,k}^{(t,b_{2})}\) with a small error. Therefore, the proof of (9) can be reduced to proving a uniform bound on the difference among \(_{y_{i},r,i}^{(t,b)}\), i.e., \(_{r=1}^{m}_{y_{i},r,i}^{(t,b_{1})}-_{r=1}^{m} _{y_{k},r,k}^{(t,b_{2})},\ \,i,k\).

However, achieving this uniform upper bound turns out to be challenging, since the updates of \(_{j,r,i}^{(t,b)}\)'s are not evenly distributed across different batches within an epoch. Each mini-batch update utilizes only a portion of the samples, meaning that some \(_{y_{i},r,i}^{(t,b)}\) can increase or decrease much more thanthe others. Therefore, the uniformly bounded difference can only be achieved after the entire epoch is processed. Consequently, we have to first bound the difference among \(_{y_{i},r_{i}}^{(t,b)}\)'s after each entire epoch, and then control the maximal difference within one epoch. The full batch (epoch) update rule is established as follows:

\[_{r=1}^{m}[_{y_{i},r,i}^{(t+1,0)}- {}_{y_{k},r,k}^{(t+1,0)}]= _{r=1}^{m}[_{y_{i},r,i}^{(t,0)}-_{y_{k},r,k}^{(t,0)}]-}{Bm}(| {S}_{i}^{(t,b_{i}^{(t)})}|_{i}^{(t,b_{i}^{(t)})}\|_{i} \|_{2}^{2}.\] \[.-|_{k}^{(t,b_{k}^{(t)})}|_{k}^{(t,b_ {k}^{(t)})}\|_{k}\|_{2}^{2}).\] (10)

Here, \(b_{i}^{(t)}\) denotes the batch to which sample \(i\) belongs in epoch \(t\), and \(_{i}^{(t,b_{i}^{(t)})}\) represents the parameters that learn \(_{i}\) at epoch \(t\) defined as

\[_{i}^{(t,b)}:=\{r:_{y_{i},r}^{(t,b)},_ {i}>0\}.\] (11)

Therefore, the update of \(_{r=1}^{m}[_{y_{i},r,i}^{(t,0)}-_{y_{k },r,k}^{(t,0)}]\) is indeed characterized by the activation pattern of parameters, which serves as the key technique for analyzing the full batch update of \(_{r=1}^{m}[_{y_{i},r,i}^{(t,0)}-_{y_{k },r,k}^{(t,0)}]\). However, analyzing the pattern of \(S_{i}^{(t,b)}\) directly is challenging since \(_{y_{i},r}^{(t,b)},_{i}\) fluctuates in mini-batches without sample \(i\). Therefore, we introduce the set series \(S_{i}^{(t,b)}\) as the activation pattern with certain threshold as follows:

\[S_{i}^{(t,b)}:=\{r:_{y_{i},r}^{(t,b)},_{i}> _{0}_{p}/\}.\] (12)

The following lemma suggests that the set of activated parameters \(S_{i}^{(t,0)}\) is a non-decreasing sequence with regards to \(t\), and the set of plain activated parameters \(_{i}^{(t,b)}\) always include \(S_{i}^{(t,0)}\). Consequently, \(S_{i}^{(0,0)}\) is always included in \(_{i}^{(t,b)}\), guaranteeing that \(_{i}\) can always be learned by some parameter. And this further makes sure the difference among \(_{y_{i},r,i}^{(t,b)}\) is bounded, as well as \(_{i}^{(t,b_{1})}/_{k}^{(t,b_{2})} C_{2}\). In the proof for SGD, we consider the learning period \(0 t T^{*}\), where \(T^{*}=^{-1}(^{-1},d,n,m)\) is the maximum number of admissible iterations.

**Lemma 3.4** (Informal Statement of Lemma C.8).: _For all \(t[0,T^{*}]\) and \(b\), we have_

\[S_{i}^{(t-1,0)} S_{i}^{(t,0)}_{i}^{(t,b)}.\] (13)

As we have mentioned above, if noisy samples outnumber clean samples, \(_{j,r}^{(t,b)}\) may also decrease. To deal with such a scenario, we establish a two-stage analysis of \(_{j,r}^{(t,b)}\) progress. In the first stage, when \(-_{i}^{}\) is lower bound by a positive constant, we prove that there are enough batches containing sufficient clear samples. This is characterized by the following high-probability event.

**Lemma 3.5** (Informal Statement of Lemma B.6).: _With high probability, for all \(T[(1),T^{*}]\), there exist at least \(c_{1} T\) epochs among \([0,T]\), such that at least \(c_{2} H\) batches in each of these epochs satisfying the following condition:_

\[|S_{+} S_{y}_{t,b}|[0.25B,0.75B].\] (14)

After the first stage of \(T=(^{-1}m(P-1)^{-2}_{p}^{-2}d^{-1})\) epochs, we have \(_{j,r}^{(T,0)}=n\|_{2}^{2}}{(P-1)^{2} _{p}^{2}d}\). The scale of \(_{j,r}^{(T,0)}\) guarantees that \(_{j,r}^{(t,b)},\) remains resistant to intra-epoch fluctuations. Consequently, this implies the sign of \(_{j,r}^{(t,b)},\) will persist unchanged throughout the entire epoch. Without loss of generality, we suppose that \(_{j,r}^{(t,b)},>0\), then the update of \(_{j,r}^{(t,b)}\) can be written as follows:

\[_{j,r}^{(t+1,0)}=_{j,r}^{(t,0)}+_{ i_{t,b,b}}|_{i}^{(t,b)}||S_{+} S_{1}|-_{i _{t,b,b}}|_{i}^{(t,b)}||S_{-} S_{-1}|\| \|_{2}^{2}.\] (15)

As we have proved the balancing of logits \(_{i}^{(t,b)}\) across batches, the progress analysis of \(_{j,r}^{(t+1,0)}\) is established to characterize the signal learning of SGD.

## 4 Result for SAM

In this section, we present the positive results for SAM in the following theorem.

**Theorem 4.1**.: _For any \(>0\), under Condition 3.1 with \(_{0}=(P^{-1}_{p}^{-1}d^{-1/2})\), choose \(=}{P_{p}}\). With probability at least \(1-\), neural networks first trained with SAM with \(O^{-1}^{-1}n^{-1}mB\|\|_{2}^{-2}\) iterations, then trained with SGD with \(^{-1}^{-1}mnd^{-1}P^{-2}_{p}^{-2}\) iterations can find \(^{(t)}\) such that,_

1. _The training loss satisfies_ \(L_{S}(^{(t)})\)_._
2. _The test error_ \(L_{}^{0-1}(^{(t)}) p+\)_._

In contrast to Theorem 3.2, Theorem 4.1 demonstrates that CNNs trained by SAM exhibit benign overfitting under much milder conditions. This condition is almost dimension-free, as opposed to the threshold of \(\|\|_{2}^{4}((d/n)P^{4}_{p}^{4})\) for CNNs trained by SGD. The discrepancy in the thresholds can be observed in Figure 1. This difference is because SAM introduces a perturbation during the model parameter update process, which effectively prevents the early-stage memorization of noise by deactivating the corresponding neurons.

### Noise Memorization Prevention

In this subsection, we will show how SAM can prevent noise memorization by changing the activation pattern of the neurons. For SAM, we have the following update rule of decomposition coefficients \(_{j,r}^{(t,b)},_{j,r,i}^{(t,b)},_{j,r,i}^{(t,b)}\).

**Lemma 4.2**.: _The coefficients \(_{j,r}^{(t,b)},_{j,r,i}^{(t,b)},_{j,r,i}^{(t,b)}\) defined in Lemma 3.3 satisfy the following iterative equations for all \(r[m]\), \(j\{ 1\}\) and \(i[n]\):_

\[_{j,r}^{(0,0)},_{j,r,i}^{(0,0)},_{j,r,i}^ {(0,0)}=0,\] \[_{j,r}^{(t,b+1)}=_{j,r}^{(t,b)}- _{i_{t,b} S_{+}}_{i}^{(t,b)}^{ }(_{j,r}^{(t,b)}+}_{j,r}^{ (t,b)},_{i})\] \[-_{i_{t,b} S_{-}}_{i}^{(t,b)}^{ }(_{j,r}^{(t,b)}+}_{j,r}^{ (t,b)},_{i})\|\|_{2}^{2},\] \[_{j,r,i}^{(t,b+1)}=_{j,r,i}^{(t,b)} -}{Bm}_{i}^{(t,b)}^{}( _{j,r}^{(t,b)}+}_{j,r}^{(t,b)}, _{i})\|_{i}\|_{2}^{2}(y_{i}=j)\,(i_{t,b}),\] \[_{j,r,i}^{(t,b+1)}=_{j,r,i}^{(t,b) }+}{Bm}_{i}^{(t,b)}^{}( _{j,r}^{(t,b)}+}_{j,r}^{(t,b)}, _{i})\|_{i}\|_{2}^{2}(y_{i}=-j)\,(i_{t,b}),\]

_where \(_{t,b}\) denotes the sample index set of the \(b\)-th batch in the \(t\)-th epoch._

The primary distinction between SGD and SAM is how neuron activation is determined. In SAM, the activation is based on the perturbed weight \(_{j,r}^{(t,b)}+}_{j,r}^{(t,b)}\), whereas in SGD, it is determined by the unperturbed weight \(_{j,r}^{(t,b)}\). This perturbation to the weight update process at each iteration gives SAM an intriguing denoising property. Specifically, if a neuron is activated by noise in the SGD update, it will subsequently become deactivated after the perturbation, as stated in the following lemma.

**Lemma 4.3** (Informal Statement of Lemma D.5).: _Suppose the Condition 3.1 holds with parameter choices in Theorem 4.1, if \(_{j,r}^{(t,b)},_{k} 0\), \(k_{t,b}\) and \(j=y_{k}\), then \(_{j,r}^{(t,b)}+}_{j,r}^{(t,b)}, _{k}<0\)._

By leveraging this intriguing property, we can derive a constant upper bound for the noise coefficients \(_{j,r,i}^{(t,b)}\) by considering the following cases:

1. If \(_{i}\) is not in the current batch, then \(_{j,r,i}^{(t,b)}\) will not be updated in the current iteration.

2. If \(_{i}\) is in the current batch, we discuss two cases: 1. If \(_{j,r}^{(t,b)},_{i} 0\), then by Lemma 4.3, one can know that \(^{}(_{j,r}^{(t,b)}+}_{j,r}^{(t,b)},_{i})=0\) and thus \(_{j,r,i}^{(t,b)}\) will not be updated in the current iteration. 2. If \(_{j,r}^{(t,b)},_{i} 0\), then given that \(_{j,r}^{(t,b)},_{i}_{j,r,i}^{(t,b)}\) and \(_{j,r,i}^{(t,b+1)}_{j,r,i}^{(t,b)}+\|_{i}\|_{2}^{2}}{Bm}\), we can assert that, provided \(\) is sufficiently small, the term \(_{j,r,i}^{(t,b)}\) can be upper bounded by a small constant.

In contrast to the analysis of SGD, which provides an upper bound for \(_{j,r,i}^{(t,b)}\) of order \(O( d)\), the noise memorization prevention property described in Lemma 4.3 allows us to obtain an upper bound for \(_{j,r,i}^{(t,b)}\) of order \(O(1)\) throughout \([0,T_{1}]\). This indicates that SAM memorizes less noise compared to SGD. On the other hand, the signal coefficient \(_{j,r,i}^{(t)}\) also increases to \((1)\) for SAM, following the same argument as in SGD. This property ensures that training with SAM does not exhibit harmful overfitting for the same signal-to-noise ratio at which training with SGD suffers from harmful overfitting.

## 5 Experiments

In this section, we conduct synthetic experiments to validate our theory. Additional experiments on real data sets can be found in Appendix A.

We set training data size \(n=20\) without label-flipping noise. Since the learning problem is rotation-invariant, without loss of generality, we set \(=\|\|_{2}[1,0,,0]^{}\). We then generate the noise vector \(\) from the Gaussian distribution \((,_{p}^{2})\) with fixed standard deviation \(_{p}=1\). We train a two-layer CNN model defined in Section 2 with the ReLU activation function. The number of filters is set as \(m=10\). We use the default initialization method in PyTorch to initialize the CNN parameters and train the CNN with full-batch gradient descent with a learning rate of \(0.01\) for \(100\) iterations. We consider different dimensions \(d\) ranging from \(1000\) to \(20000\), and different signal strengths \(\|\|_{2}\) ranging from \(0\) to \(10\). Based on our results, for any dimension \(d\) and signal strength \(\) setting we consider, our training setup can guarantee a training loss smaller than \(0.05\). After training, we estimate the test error for each case using \(1000\) test data points. We report the test error heat map with average results over \(10\) runs in Figure 2.

## 6 Related Work

**Sharpness Aware Minimization.**Foret et al. (2020), and Zheng et al. (2021) concurrently introduced methods to enhance generalization by minimizing the loss in the worst direction, perturbed from the current parameter. Kwon et al. (2021) introduced ASAM, a variant of SAM, designed to address parameter re-scaling. Subsequently, Liu et al. (2022b) presented LookSAM, a more computationally

Figure 2: (a) is a heatmap illustrating test error on synthetic data for various dimensions \(d\) and signal strengths \(\) when trained using Vanilla Gradient Descent. High test errors are represented in blue, while low test errors are shown in yellow. (b) displays a heatmap of test errors on the synthetic data under the same conditions as in (a), but trained using SAM instead with \(=0.03\). The y-axis represents a normal scale with a range of \(1000 21000\).

efficient alternative. Zhuang et al. (2022) highlighted that SAM did not consistently favor the flat minima and proposed GSAM to improve generalization by minimizing the surrogate. Recently, Zhao et al. (2022) showed that the SAM algorithm is related to the gradient regularization (GR) method when the loss is smooth and proposed an algorithm that can be viewed as a generalization of the SAM algorithm. Meng et al. (2023) further studied the mechanism of Per-Example Gradient Regularization (PEGR) on the CNN training and revealed that PEGR penalizes the variance of pattern learning.

**Benign Overfitting in Neural Networks.** Since the pioneering work by Bartlett et al. (2020) on benign overfitting in linear regression, there has been a surge of research studying benign overfitting in linear models, kernel methods, and neural networks. Li et al. (2021); Montanari and Zhong (2022) examined benign overfitting in random feature or neural tangent kernel models defined in two-layer neural networks. Chatterji and Long (2022) studied the excess risk of interpolating deep linear networks trained by gradient flow. Understanding benign overfitting in neural networks beyond the linear/kernel regime is much more challenging because of the non-convexity of the problem. Recently, Frei et al. (2022) studied benign overfitting in fully-connected two-layer neural networks with smoothed leaky ReLU activation. Cao et al. (2022) provided an analysis for learning two-layer convolutional neural networks (CNNs) with polynomial ReLU activation function (ReLU\({}^{q}\), \(q>2\)). Kou et al. (2023) further investigates the phenomenon of benign overfitting in learning two-layer ReLU CNNs. Kou et al. (2023) is most related to our paper. However, our work studied SGD rather than GD, which requires advanced techniques to control the update of coefficients at both batch-level and epoch-level. We also provide a novel analysis for SAM, which differs from the analysis of GD/SGD.

## 7 Conclusion

In this work, we rigorously analyze the training behavior of two-layer convolutional ReLU networks for both SGD and SAM. In particular, we precisely outlined the conditions under which benign overfitting can occur during SGD training, marking the first such finding for neural networks trained with mini-batch SGD. We also proved that SAM can lead to benign overfitting under circumstances that prompt harmful overfitting via SGD, which demonstrates the clear theoretical superiority of SAM over SGD. Our results provide a deeper comprehension of SAM, particularly when it comes to its utilization with non-smooth neural networks. An interesting future work is to consider other modern deep learning techniques, such as weight normalization, momentum, and weight decay, in our analysis.