# HumanVLA: Towards Vision-Language Directed

Object Rearrangement by Physical Humanoid

Xinyu Xu\({}^{12}\) &Yizheng Zhang\({}^{2*}\)&Yong-Lu Li\({}^{1}\)&Lei Han\({}^{2}\)&Cewu Lu\({}^{1}\)

\({}^{1}\)Shanghai Jiao Tong University

{xuxinyu2000, yonglu_li, lucewu}@sjtu.edu.cn {yizhenzhang, lxhan}@tencent.com

Equal contribution. \({}^{}\) Equal advising.

###### Abstract

Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications. However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications. To address this limitation, we introduce HumanVLA for general object rearrangement directed by practical vision and language. A teacher-student framework is utilized to develop HumanVLA. A state-based teacher policy is trained first using goal-conditioned reinforcement learning and adversarial motion prior. Then, it is distilled into a vision-language-action model via behavior cloning. We propose several key insights to facilitate the large -scale learning process. To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks. Through extensive experiments and analysis, we demonstrate the effectiveness of the proposed approach.

## 1 Introduction

Learning human-scene interaction (HSI) in realistic physical environments is a vital requirement of many applications, including computer graphics, embodied AI, and robotics. In this field, many

Figure 1: HumanVLA performs various object rearrangement tasks directed by the egocentric vision and natural language instructions.

previous efforts have been made to promote expressive humanoid control [36; 46; 27; 56], static physical scene interaction [43; 15; 50; 33], and manipulating a specific object [47; 15; 51]. These works have achieved great success in synthesizing plausible HSI controls.

Nonetheless, significant challenges persist in the realm of more extensive HSI applications and two primary issues need to be solved. _Firstly_, the current techniques are limited to static objects, such as sitting on a chair [50; 33], or specific object dynamics, such as carrying a box  and throwing a ball . However, in a more complicated real-world environment, humans demonstrate exceptional skills in manipulating a diverse range of objects with different geometries, poses, and weights. It poses a challenging requirement on the varied dynamics of objects in HSI synthesis, i.e., a universal manipulation policy. _Secondly_, ground-truth object and goal states are necessary to direct humanoid controls in previous works. However, without the help of external localization devices, this privileged information is difficult to access in a real-world transfer. It prohibits practical real-world extensions like humanoid robots and necessitates an easily deployable perception method.

Our work takes a step forward in the above two challenges. We investigate the concept of general-purpose object rearrangement performed by a physically interactive humanoid. The whole-body physical humanoid is instructed to carry out daily object loco-manipulations in an indoor room setting. The tasks involve human-like motion controls, interaction with diverse objects, and following desired object dynamics. Moreover, considering the unavailability of privileged information about object and goal states in real-world humanoid applications, we delve into humanoid controls directed by practical vision and language. Compared to privileged states, vision-language modalities are more accessible and offer new potential for practical applications. It also presents an ultimate vision of the research community on humanoids: a human-like agent capable of understanding language instructions, perceiving its environment, and executing daily tasks to assist humans. Fig. 1 provides intuitive examples of our work, where the humanoid agent can push a table, carry a laptop, and pull a chair, all directed by vision and language. Comparisons of our work with previous studies are available in Tab. 1.

Our work starts with learning state-based teacher policy and then distills the policy into a vision-language-action model. In the first stage, we train the policy using goal-conditioned reinforcement learning and adversarial motion priors (AMP) , within a generative adversarial imitation learning  paradigm. The discrimination reward plus task-conditioned reward encourages humanoids to generate realistic motions and complete the task. However, interacting with diverse objects remains challenging for vanilla AMP. We introduce improved techniques to facilitate general manipulation, in-context navigation, and prioritized task completion. In the second stage, we distill the policy into a student network, named **HumanVLA**, an end-to-end vision-language-action model for physical humanoid. Behavior cloning  is used to train the student HumanVLA, i.e., cloning the teacher action at each step. A challenge of learning VLA models is the poor perception quality of the unconstrained camera pose. We propose a novel active rendering technique to improve gaze intention.

To support HumanVLA, we create a novel dataset named **Human-in-the-Room (HITR)**. It consists of four different room layouts: _bedroom_, _livingroom_, _kitchen_, and _warehouse_. Each layout is populated with _separated_, _instantiable_, and _replaceable_ objects from HSSD  assets to create diverse scenes. The humanoid agent is placed in the scene with an instruction to rearrange the room. Statistically, the HITR dataset consists of 50 static objects and 34 movable objects. In our extensive experiments, we train HumanVLA in IsaacGym  with tasks from HITR. Results demonstrate the effectiveness of our method in generalized object rearrangement and vision-language perception.

In summary, our contributions include: (1) We study general object rearrangement by physical humanoids. Several advanced techniques are introduced to interact with diverse objects. (2) We propose HumanVLA, the first vision-language-action model on physical humanoids to complete tasks directed by egocentric vision and natural language instruction. (3) We propose the HITR dataset to facilitate research in this field. Comprehensive experiments are conducted in HITR to validate the effectiveness of our method.

## 2 Related Works

**Motion Synthesis** is a long-term research topic in graphics, vision, and robotics. It can be divided into two streams: kinematic motion synthesis [43; 42; 17; 14; 24; 18; 5; 20; 54; 19; 25; 1; 7] and physics-based motion synthesis [34; 36; 35; 21; 50; 27; 46; 56; 41; 51; 33; 47; 6; 26]. Kinematicmethods aim at synthesizing visually plausible motions with less penetration, floating, and being semantically faithful. It leverages generative neural networks like VAEs [14; 7], Transformers [1; 19], or Diffusions [24; 18; 20] to predict next state. Our work belongs to the physics-based methods, which have an additional requirement on physical plausibility. It follows a control-then-model paradigm where the control is typically achieved by a learning algorithm, and the model is constrained by a physics simulator. DeepMimic  uses reinforcement learning plus imitation learning to track motion references and perform versatile motion controls. NCP  advances motion tracking with discrete latent prior. Adversarial Motion Prior (AMP)  uses generative adversarial imitation learning to learn natural state transition from unstructured motion data. It is further extended with a reusable controller , high-level language , expressive control , and latent conditions . Recently, there has been an increasing emphasis on the synthesis of interactive motions. InterPhys  uses task-conditioned reward plus stylized adversarial reward to perform HSI tasks such as sitting, lying, and box carrying. InterScene  extends the paradigm to synthesize long-horizon static interactions. UniHSI  leverages the vast knowledge of the language model to provide a unified interface for static HSI. However, previous works are limited to static objects or specific movable objects but fail to interact with various objects. In contrast, our research studies general object rearrangement in a daily room, posed with challenges in diverse object geometries, positions, and weights.

**Room Rearrangement** is a crucial application of embodied AI, where an instructed agent is placed in a room to search, navigate, and interact with desired objects. Recent efforts [48; 44; 53; 23; 9] have proposed various platforms and benchmarks to facilitate room rearrangement research. Visual room rearrangement  takes the agent to transverse both goal and initial scenes to recover object states based on visual observations. OVMM  presents open-vocabulary pick-and-place manipulation challenges in pursuit of extreme generalization capability. Recent algorithms [49; 11] leverage commonsense knowledge in large language models to plan rearrangements. However, these works are designed for simple embodiments, such as disc-shaped mobility and gripper manipulation. They are limited to moving on smooth terrain and handling only small-sized objects. In contrast, our work pioneers the design of rearrangement tasks in a complex human-like embodiment. It benefits from bipedal locomotion and stronger interaction motors. For example, our model is capable of carrying 20\(kg\) objects, which is beyond the capabilities of traditional stretches.

**Vision-Language-Action (VLA) Model** maps practical vision-language input to generate action controls. It has demonstrated impressive results in the fields of embodied AI and robotics [12; 57; 32; 3; 55]. Thanks to the robust scalability of the vision and language modalities, VLAs also benefit from large-scale training [57; 32], opening up the potential for more general-purpose applications. However, existing VLAs are designed for simple embodiments, such as desktop gripper manipulation. The exploration of VLAs for more complex, high-dimensional humanoids is still in its early stages. Our work is the first to develop humanoid controls directed by practical vision and language.

## 3 Approach

In this section, we introduce the learning process of HumanVLA. Training HumanVLA directly through large-scale reinforcement learning (RL) presents significant challenges, including a high-dimensional action space, a composite state space, slow rendering speed, and other common issues associated with large-scale RL. To this end, we utilize a teacher-student framework to train Human-VLA, which has been validated in applications like dexterous re-oreintation  and grasping .

   Methods & Physics &  Object \\ Interaction \\  &  Object \\ Dynamics \\  &  Language \\ Instruction \\  &  Ego-Vision \\ Objects \\  &  \# Static \\ Objects \\  & 
 \# Movable \\ Objects \\  \\  NSM  & & ✓ & ✓ & & & 25 & 2 \\ SAMP  & & ✓ & & & & 7 & - \\ OMOMO  & & ✓ & ✓ & ✓ & & - & 19 \\ PADL  & ✓ & & & ✓ & & - & - \\ InterPhys  & ✓ & ✓ & ✓ & & & 350 & 1 \\ InterScene  & ✓ & ✓ & & & 57 & - \\ UniHSI  & ✓ & ✓ & & ✓ & 40 & - \\  HumanVLA(Ours) & ✓ & ✓ & ✓ & ✓ & ✓ & 50 & 34 \\   

Table 1: Comparisons between HumanVLA and past works.

It consists of two phases. In the first phase (Sec. 3.1), we leverage goal-conditioned reinforcement learning and adversarial motion priors  to train HumanVLA-Teacher. It is presented with the oracle scene state, including precise object pose, geometry, navigation waypoint, and goal coordinate. In the second phase (Sec. 3.2), we operate in a more practical setting, where the egocentric vision is tasked with perceiving the scene, and natural language instruction is used to specify the goal. In a blueprint of real-world humanoid robots, observations used by HumanVLA are all accessible in a real-world deployment. HumanVLA is trained via behavior cloning  from HumanVLA-Teacher, where the pre-trained teacher policy significantly reduces the compute demands of the process.

### State-based Teacher Policy Learning

We train HumanVLA-Teacher with complete knowledge of the scene state to enable a variety of object rearrangement tasks. The rearrangement task is formulated as a reinforcement learning process. To elaborate, at each time step \(t\), given state \(s_{t}\) and goal \(g\), HumanVLA-Teacher \(_{tch}\) predicts an action \(a_{t}\) from policy distribution \(_{tch}(a_{t}|s_{t},g)\). The action \(a_{t}\) is processed by a physics simulator \(f(s_{t+1}|a_{t},s_{t})\) to generate the next state \(s_{t+1}\). The learning objective is to maximize the accumulated reward \((_{tch})=_{t=0}^{T-1}^{t}r_{t}\) where \(\) is the discount factor and \(r_{t}\) is the step reward at time \(t\).

To train robust policies that enable humanoids to interact with objects and achieve various goals \(g\) in a life-like manner, it is crucial for the humanoids to learn from authentic human motions and generalize across different tasks. To this end, we use goal-conditioned task reward \(r^{G}(g,s_{t},s_{t+1})\) to encourage the agent to complete the task and style reward \(r^{S}(s_{:t+1})\) to imitate human motion prior.

We employ adversarial motion prior (AMP)  to model the style reward, which incorporates an adversarial discriminator \(D\) to discriminate motions from simulated synthesis or tracked motion dataset. It is trained with the objective:

\[*{argmin}_{D}-& E_{d^{M} (s_{t:t+*})}[(D(s_{t:t+t^{*}}))]-E_{d^{}(s_{t:t+t^{*}})}[(1-D(s_{t: t+t^{*}}))]\\ +& w^{gp}E_{d^{M}(s_{t:t+t^{*}})} _{}D()_{=s_{t:t+t^{*}}}^{2} ,\] (1)

where \(d^{M}(s_{t:t+t^{*}})\) and \(d^{}(s_{t:t+t^{*}})\) are distributions of \(t^{*}\)-frame motion clips from dataset \(M\) and policy \(\). The first two items in Eq. 1 are to discriminate motions while the last item with a coefficient \(w^{gp}\) regularizes the gradient penalty  in adversarial training. The style reward \(r^{S}\) to encourage realistic motion synthesis is then formulated as

\[r^{S}(s_{:t+1})=-(1-D(s_{t+1-t^{*}:t+1})).\] (2)

We uniformly conceptualize goal-conditioned object rearrangement as three processes: locomotion towards the object, contacting the object, and relocating the object to the goal. These steps are accomplished by unified progressively increasing task rewards.

Figure 2: An overview of learning state-based HumanVLA-Teacher policy using goal-conditioned reinforcement learning and adversarial motion prior.

Despite the powers of goal-conditioned reinforcement learning and adversarial motion prior, generalized object rearrangement tasks by humanoids still pose significant challenges. Previous works  have been limited to simple tasks such as static sitting, lying, or carrying a specific box. We propose new techniques to overcome challenges in generalized object rearrangement. Generalized object interaction involves geometrically various objects. However, tracking motion data for each individual object is labor-intensive, and infeasible to tackle novel objects. We expect RL to enable automatic object generalization. Thus, we encode object geometry to learn a geometry-aware policy and design a carry curriculum to facilitate the learning. Due to the misalignment of objects in human motion data and the task, we propose style reward clipping to prioritize high-level task execution. Navigating in a complex room requires high-level planning to avoid collisions, we use in-context path planning to enable efficient locomotion. More detailed explanations of our improved techniques are described in the following:

**Geometry Encoding.** Object state is crucial in HSI synthesis. Previous studies  primarily encode object position, rotation, and linear and angular velocities to act on certain objects like boxes or balls. A general policy for interactions with diverse objects should incorporate geometric information. Thus, we augment the teacher policy with geometric object representations via Basis Point Set (BPS)  encoding. A shared set of basis points is randomly sampled from a unit sphere and encodes object geometry using delta vectors from each basis point to the nearest object point. In contrast to geometries encoded by a neural net , BPS encoding is computationally efficient and accelerates policy learning. Consequently, we use object geometry, position, rotation, and linear and angular velocities to form a comprehensive object observation, thereby facilitating a more expressive policy control.

**Carry Curriculum Pre-training.** Object rearrangement is conceptualized as a three-step process in the aforementioned paragraph. However, directly learning the entire three-step rearrangement task from scratch is challenging due to the long task horizon. Besides, physics-based object movement presents greater challenges compared to kinematic object movement , primarily because the object state is not directly editable. Instead, it requires indirect control of the physical humanoid to interact. To this end, we draw inspiration from the curriculum learning  and design an easy carry curriculum to pre-train the policy. The carry curriculum only includes the first two of three steps: locomotion towards the object and carrying up the object for an in-the-air holding. The carry curriculum has a shorter horizon and is empirically easier to converge. Furthermore, the pre-trained in-the-air carry prior significantly benefits the subsequent object relocation. For the carry curriculum, we use objects excluding those on the ground, such as tables and chairs, which are easier to move by pushing and pulling along the ground without a lift. The carry curriculum shares the same learning paradigm with the rearrangement task, except for a different two-stage reward design.

**Style Reward Clipping.** General object rearrangement involves manipulating novel objects that are not recorded in tracked motion data. This creates a misalignment in optimization directions: strictly imitating reference motion or ensuring high-level task execution. Previous work  balanced two items by a weighted sum between task reward and style reward in motion-aligned tasks. However, in our general object rearrangement setting, goal-conditioned task exploration progress can be stagnant and the policy may learn actions devoid of task semantics following the logarithmic gradient in Eq. 2. For instance, when the object is difficult to lift, the policy tends to mimic insignificant hand swings in the motion data, rather than exploring carry-up actions. We insert a style reward clipping to prioritize task execution, formulated as follows:

\[_{t}=(r^{G}(g,s_{t},s_{t+1}),_{min}),\] (3) \[r_{t}=w^{G}r^{G}(g,s_{t},s_{t+1})+w^{S}(r^{S}(s_{:t+1}),_ {t}),\] (4)

where \(w^{G},w^{S}\) are coefficients, and \(_{t}\) is the upper bound for the style reward. This formulation prioritizes goal-conditioned task execution over motion imitation in reward maximization. In addition, we use a minimum upper bound \(_{min}\), to ensure basic motion imitation during the early stages when the task reward is near zero.

**In-context Path Planning.** Navigating a populated room requires high-level knowledge since the dense object cluster may collide with the humanoid agent and obstruct natural locomotion. We use in-context path planning to guide the navigation. Point clouds of all objects in the scene make up the spatial occupancy. We perform top-down point projection and grid discretization to derive a 2D obstacle map of \(20cm\) x \(20cm\) grids. We then plan a navigable path from the starting position to the object, and subsequently to the goal using \(A^{*}\) algorithm , represented as a series of navigation waypoints to guide locomotion at each step.

Incorporating all the above features, we present an overview of training HumanVLA-Teacher in Fig. 2. Navigation waypoint, task goal, object state, and humanoid proprioception are sent to the policy network to derive an action. The learning process is guided by task reward and motion discrimination reward. Further details about the learning process can be found in the appendix.

### Distilling into Vision-Language-Action Model

While the HumanVLA-Teacher \(_{tch}\) leverages privileged information such as object state, goal state, and waypoint in the global coordinates, our biggest goal is towards practical humanoid control free of privileged information and real-world deployable. To this end, we replace privileged states with flexible egocentric vision and natural language instruction. Notably, the proprioception observation is the only kept item from HumanVLA-Teacher to HumanVLA, which is represented in the local coordinate and can be obtained via forward kinematics and temporal differentiation. The history action is also used in observation. To obtain egocentric vision, we mount a mobile camera on the head of the human. It renders a 256 x 256 image with a field of view spanning 90 degrees at each step. An overview of training HumanVLA is illustrated in Fig. 3.

**Behavior Cloning.** We train HumanVLA \(_{vla}\) using a teacher-student framework to distill the knowledge from HumanVLA-Teacher via behavior cloning . HumanVLA employs an _EfficientNet-B0_ for image encoding and a frozen _bert-base-uncased_ for language encoding, whose features, along with propriception, last action, are sent to the action decoder to derive an action. At each time step \(t\), HumanVLA-Teacher leverages privileged state \(s_{t}\) and \(g\) to derive an action \(_{tch}(a_{t}|s_{t},g)\) while HumanVLA derives an action \(_{vla}(a_{t}|p_{t},a_{t-1},v_{t},l)\) based on proprioception \(p_{t}\), last action \(a_{t-1}\), egocentric image \(v_{t}\), and language instruction \(l\). Behavior cloning bridges distributions between \(_{vla}(a_{t}|p_{t},a_{t-1},v_{t},l)\) and \(_{tch}(a_{t}|s_{t},g)\), which can be directly implemented via supervised learning. In the empirical training process, we observe a severe covariate shift problem in offline behavior cloning. Thus, we opt for a DAgger  framework to train HumanVLA which alleviates the problem via online learning.

**Active Rendering.** Though HumanVLA-Teacher possesses comprehensive knowledge in versatile control, naive policy distillation still suffers from the gap of observation expressiveness. For instance, while egocentric vision is used to perceive objects, the humanoid gaze might not properly focus on the target object but renders a less informative image of the background. Consequently, the perception quality of HumanVLA is significantly affected by the camera pose. However, an optimal camera pose, determined by the head pose, is not guaranteed in the teacher policy, which only imitates a life-like head motion but ignores the vision quality. We propose an active rendering technique to encourage the camera to focus on the object. We analytically calculate the head-to-object direction in the global coordinate and then derive a head orientation. Inverse kinematics is used to obtain active rendering actions \(a_{t}^{ar}\) for the neck joint. It is used to derive a mixed supervision \(a_{t}^{vla}\) in conjunction with the teacher action \(a_{t}^{tch}\), formulated as

\[a_{t}^{vla}=(1-w^{ar})a_{t}^{tch}+w^{ar}a_{t}^{ar},\] (5)

Figure 3: **Left**: An overview of learning HumanVLA by mimicking teacher action and active rendering action. **Right**: Comparison between _w/_ and _w/o_ active rendering. Active rendering leads to a more informative perception of human-object relationships.

where \(w^{a}\) is the coefficient for active rendering. Notably, this is only applied to the neck joint, while other joints only follow the teacher action.

## 4 Human-in-the-Room Dataset

Existing datasets [48; 53] for object rearrangement are primarily designed for stretches with disc-shaped mobility and gripper manipulation. Human-like embodiment has different physical attributes, such as stronger motors to handle large furniture like chairs and tables. Besides, we follow [36; 15; 33; 50] to use a humanoid model with spherical hands, which can struggle with manipulating small-sized objects, such as picking up a towel. To address these issues, we introduce a novel Human-in-the-Room (HITR) dataset, designed to facilitate vision-language directed object rearrangement tasks on a humanoid. The HITR dataset includes carefully designed objects of various sizes, ranging from \(21cm\) to \(126cm\), and provides a variety of rearrangement tasks in various rooms. Each task involves separated, instantiable, and replaceable objects with defined initial and goal states. Additionally, each task is accompanied by a natural language instruction generated by a Large Language Model (LLM).

In constructing the HITR dataset, we reference common objects used in room designs from [23; 44; 53] and utilize object models from HSSD  to create basic assets. Object assets are manually resized to ensure the interaction friendliness. We adopt the procedural generation pipeline from  to generate diverse scenes. First, we manually design four room layouts: _bedroom_, _livingroom_, _kitchen_, and _warehouse_, then randomly populate replaceable objects within these layout templates to establish the scene, as well as the goal state. Next, we randomly relocate an object in the scene, either to the ground or another receptacle. This relocated object is what the physical humanoid is tasked to rearrange. We concatenate two rendered images of the initial and goal scenes and use the composite image to prompt _gpt-4-vision_ to generate an instruction. The LLM is asked to distinguish between two states and provide an instruction to guide the state transition. However, the LLM still struggles with understanding complex spatial relationships, such as left-right errors. To ensure the quality of instructions, we manually review and revise them as necessary. Ultimately, we build the HITR dataset of 615 tasks, with an average of 6.5 objects per task. There are 50 static objects like _bed_ and _countertop_, as well as 34 movable objects like _pillow_ and _vase_. More details are in the appendix.

## 5 Experiments

### Settings

**Datasets.** Our experiments are conducted on the HITR dataset. It is split into _train_ and _test_ subsets at a ratio of 9:1, containing 552 and 63 tasks respectively. The _test_ subset is used to evaluate the generalizability of our method in unseen tasks. For the motion dataset used in training, we utilize OMOMO  and a locomotion subset from SAMP . OMOMO provides a variety of short-range motions involving moving different objects, while locomotion motions from SAMP enhance the locomotion aspect of our dataset. We use 30-minute motions in total. The source motion dataset features object rearrangement involving only seven different objects, which is far less than those in the HITR dataset. Despite this, we anticipate that our method can generalize to different objects.

**Metrics.** We adhere to a 10-second running time limit and follow  to evaluate methods with three metrics. **(1) Success Rate:** the proportion of tasks that are successfully rearranged within an error margin of \(\). **(2) Precision:** the distance of the final object position to the goal. **(3) Execution Time** the average time taken to complete a run. For the Success Rate, a higher value indicates better performance, but for Precision and Execution Time, the lower the better. All experiments are evaluated using 10 repeat runs. For state-based methods, we follow  to set \(=20cm\)

    & Success Rate (\%) \(\) & Precision (\(cm\))\(\) & Execution Time (\(s\)) \(\) \\  InterPhys  & 94.3 & 8.3 & 9.1 \\ InterPhys  \(\) & 97.8 & 12.6 & 5.3 \\ HumanVLA-Teacher & **98.1** & **4.2** & **4.6** \\   

Table 2: Results in box rearrangement. \(\) denotes our implementation.

For vision-language-based methods, where the goal is specified via coarse instructions rather than precision goal coordinates, we set \(=40\)_cm_. This criteria relaxation is also adopted in past works  for evaluating policies with different observations.

### Implementation Details

We conduct experiments in parallel environments simulated using IsaacGym , with neural networks implemented via PyTorch. Our physical humanoid model, following previous works [15; 33; 50], comprises 15 rigid bodies and 28 joints, each actuated by a PD-Controller. The simulator runs at 60Hz, and the policy is queried at 30Hz. The teacher policy is optimized using Proximal Policy Optimization  and takes two days on eight Tesla V100 GPUs to converge. The student policy is trained using DAgger  and takes one day on two GPUs. We provide comprehensive details about hyperparameters, neural architectures, observation space, and more in the appendix.

### Comparisons in Box Loco-Manipulation

While our work is pioneering in the exploration of vision-language-directed general object rearrangement, direct comparisons with previous studies are difficult. The work most similar to ours is InterPhys , which delved into state-based box loco-manipulation. Due to the unavailability of training data, motions, and codes of InterPhys , we instead refer to a box rearrangement subset in HITR to conduct experiments. We train a state-based HumanVLA-Teacher using only box rearrangement tasks, along with an implementation of the InterPhys baseline. Results of box rearrangement are reported in Tab. 2. Given that the box is the simplest object to interact with, both methods achieve high success rates. However, our method exhibits superior precision, with a result of 4.2 _cm_, outperforming both the 8.3 _cm_ in the official report  and the 12.6 _cm_ in our implementation. We use the standard deviation to evaluate the statistical significance of HumanVLA-Teacher in 10 repeated runs. The values are 0.02, 0.004, and 0.04 for Success Rate, Precision, and Execution Time respectively. With high task completion rates and low variance, we demonstrate the effectiveness and robustness of our method in this first trial.

### Ablation Study

We conduct comprehensive ablation studies on the _train_ split to validate each design choice, with results presented in Tab. 3. Firstly, we evaluate the impact of improved techniques in HumanVLA-Teacher training, which achieves a success rate of 85.9% and a precision of \(14.4cm\). However, eliminating any component leads to a decline in performance. The inclusion of geometry encoding and carry curriculum enables the model to manipulate a variety of objects effectively. Without either of these components, the success rate experiences a drop of approximately 20%. Style reward

    & Privileged State & Success Rate (\%) \(\) & Precision (_cm_)\(\) & Execution Time (\(s\)) \(\) \\  InterPhys  & \(\) & 59.5 & 52.5 & 5.9 \\ HumanVLA-Teacher & \(\) & **79.3** & **19.3** & **4.6** \\  Offline GC-BC  & \(\) & 10.2 & 152.3 & 8.5 \\ HumanVLA & \(\) & **60.2** & **57.0** & **5.8** \\ _w/o_ active rendering & \(\) & 56.7 & 65.5 & 5.9 \\   

Table 4: Results in unseen tasks.

    & Privileged State & Success Rate (\%) \(\) & Precision (_cm_)\(\) & Execution Time (\(s\)) \(\) \\  HumanVLA-Teacher & \(\) & **85.9** & **14.4** & 4.5 \\ _w/o_ geometry encoding & \(\) & 64.5 & 43.4 & 5.5 \\ _w/o_ carry curriculum & \(\) & 66.3 & 73.4 & 5.3 \\ _w/o_ style clipping & \(\) & 79.9 & 27.5 & **4.3** \\ _w/o_ path planning & \(\) & 67.8 & 37.2 & 5.5 \\  HumanVLA & \(\) & **74.8** & **42.6** & **5.1** \\ _w/o_ active rendering & \(\) & 67.9 & 55.6 & 5.6 \\ _w/o_ online learning & \(\) & 15.3 & 145.0 & 8.2 \\   

Table 3: Ablation study.

clipping prioritizes task execution, whose absence results in a 6% decrease in the success rate. Path planning helps humans navigate complex scenes. Its removal leads to a significant 18.5% decrease in the success rate. Subsequently, we validate design choices in training HumanVLA directed by vision and language. The default HumanVLA achieves a success rate of 74.8% with a precision of 42.6\(cm\). However, the absence of active rendering results in a substantial 6.9% success rate drop, emphasizing the importance of perception quality. We implement an offline behavior cloning baseline using ten off-the-shell teacher trajectories per task for training. Without online learning, the system suffers from a severe covariate shift and performs poorly.

### Generalizing to Unseen Tasks

We use the _test_ split of HITR to evaluate the generalizability of methods. The unseen data, which includes novel scene compositions, visual appearances, and language instructions, poses a significant challenge to our method. Results are presented in Tab. 4. The state-based HumanVLA-Teacher tends to be more robust in unseen data. Relatively small drops in success rate (6.6%) and precision (4.9\(cm\)) demonstrate strong generalizability of RL when using privileged information. Moreover, it consistently outperforms the InterPhys  baseline on all metrics. Applying HumanVLA to unseen tasks turns out to be more challenging due to the complexity of vision and language modalities. The success rate of HumanVLA decreases to 60.2%, and the precision drops to 57.0\(cm\). However, HumanVLA still consistently outperforms baselines without active rendering and the offline goal-conditioned behavior-cloning (Offline GC-BC)  method.

### Qualitative Results

We provide qualitative visualizations of how HumanVLA performs object rearrangement tasks in Fig. 4. More results are available in the appendix. We demonstrate that HumanVLA is capable of moving a pot, vase, chair, and box based on language instructions.

Figure 4: Qualitative results. The color transitions from green to yellow as the task progresses.

Conclusion

We investigate vision-language directed object rearrangement by physical humanoids in this work, a fundamental technique for HSI synthesis and real-world humanoid robots. Our system is developed using a teacher-student distillation framework. We propose key insights to facilitate teacher policy learning with privileged states and introduce a novel active perception technique to favor vision-language-action model learning. We present a novel HITR dataset to support our task. In extensive experiments, our HumanVLA model demonstrates superior results in both quantitative and qualitative evaluations. Future works include dexterous manipulation by physical humanoids and long-horizon multi-object interaction.