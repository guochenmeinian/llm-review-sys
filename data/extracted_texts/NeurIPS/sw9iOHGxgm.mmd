# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

model architectures which handle different editing subtasks (Couairon et al., 2023; Zhang et al., 2023a). However, **neither of these approaches includes edits requiring more holistic visual understanding of how humans and objects interact or how events unfold**, such as'make the cook cut the apple in half' or'make the dog jump in the air' (see Fig. 1). These more _action-centric_ edits are severely understudied in the space of instruction-tuned image editing models (Brooks et al., 2023; Huang et al., 2024); when they are considered, it is done in isolation, ignoring other image edit subtasks and rigorous semantic evaluation (Soucek et al., 2023; Black et al., 2024). In Sec. 2 we describe a typology of these edit types and how existing datasets currently fail to address them all.

As we argue in this paper, **a major reason for these limitations is the lack of high-quality data.** Finetuning data of object or attribute changes is simpler to acquire than other forms of edits, since inpainting setups directly leverage strong object and attribute abilities of txt2img models (Rombach et al., 2022) for paired-image data generation (Yildirim et al., 2023; Zhang et al., 2024). However, solving the data scarcity for learning action and reasoning-centric edits is not as straightforward. We identify videos and simulation engines as the two most promising sources of data for these edit types. As we discuss in this paper, we find that previous models trained on "noisy" synthetic image pairs or video frames lead to poor editing abilities. Here, noisy refers to image pairs with changes not mentioned in the prompt, i.e. due to shortcomings of the automatic generation process or inherent properties of videos such as viewpoint changes and non-meaningful movement. Therefore, our main requirement of high-quality action and reasoning-centric edit examples is that they be _truly minimal_: Edited images which contain one or maximally two semantic changes described by the prompt, while all other aspects are kept exactly the same. From a diverse set of video sources and simulation engines, we curate the **AURORA** Dataset (Action-**Reasoning-**O**bject-**A**ttribute). Via crowd-sourcing and curation we collect 130K truly-minimal examples from videos and 150K from simulation engines for instruction-tuned image editing. We describe our dataset and collection process in Sec. 3.

The few image-text-alignment metrics commonly used in image editing are based on visual similarity to a groundtruth and in reality turn out to mostly measure the ability to stay maximally faithful (i.e. copying) to the source image (Zhang et al., 2024; Fu et al., 2023). Though faithfulness is an important first step to master, these metrics have almost no correlation with the model's ability to

Figure 1: Previous failures on editing skills such as action, movement and reasoning (measured in **AURORA**-**Bench**) compared to improvements with **AURORA** on these more challenging actions.

generate accurate edits, especially on action and reasoning-centric changes. Hence, in addition to the training data in **AURORA**, we introduce **AURORA**-Bench(Sec. 4), a manually annotated benchmark covering 8 editing tasks on which we collect human judgement (Tab. 2). Inspired by work on image generation models as discriminators , we also describe a novel discriminative metric that assesses _understanding_ and hallucination (Sec. 5.1). To demonstrate the efficacy and quality of **AURORA**, we present a state-of-the-art instruction-tuned image editing model, finetuned on **AURORA** and evaluated on **AURORA**-Bench, which we compare to strong baselines in a set of experiments in Sec. 5.3.

**In summary our contributions are:** 1) The creation of **AURORA**, a new clean and varied set of image edit pairs for instruction-finetuning that encompasses more action-centric and reasoning-centric examples. 2) We present a **comprehensive benchmark** covering a variety of edit types; 3) We introduce a **novel more informative metric** beyond existing ones; 4) We provide a **state-of-the-art image editing model** based on **AURORA** with well-rounded image editing capabilities covering object-centric, action-centric, and reasoning-centric edit abilities.

## 2 Typology of image edits

There are many ways to visually change a given scene . **We define and focus upon five broad types of changes:**_object/attribute-centric, global, action-centric, reasoning-centric,_ and _viewpoint_**.** Some can overlap: an action might change the attribute of an object, or reasoning can play a role in any type. Object-centric changes correspond to changes made to a specific object such as replacing it with another one, changing its attributes like color or texture, resizing it, or removing it entirely. Global edits change the overall image such as the background, style or textures. Action-centric changes correspond to changes that occur as a result of executing an action: changes in configuration of the objects, state changes of objects (e.g. cutting an apple), or pose change. Reasoning-centric changes are broadly defined as anything requiring compositionality or symbolic understanding: spatial, resolving referrring expressions ("sitting person on the far left"), negation, etc. Finally, viewpoint edits correspond to moving an egocentric camera, zooming in/out, and are the only on we do not cover in **AURORA** as they add many additional challenges: First, in videos _in the wild_, they exacerbate the already numerous changes; second, excessive camera movement can unpredictably alter the entire scene, introducing noise. App. C shows examples for each type.

**Coverage in existing data:** We characterize four broad sources of image pairs and prompts, which influence how much certain edit types are covered in existing training data (see Tab. 1):

1. [leftmargin=*,noitemsep,topsep=0pt]
2. Combining existing text-to-image models and LLMs in pipelines to automatically generate similar **synthetic** images ;
3. Providing **humans** with an image editing tool on existing images, combined with in-painting , or finding existing Photoshop edits on the web ;
4. Selecting nearby **video** frames and captioning the change via human annotators or automatically ;
5. Using **simulation** engines to generate pairs by precisely controlling visual changes with templated language .

  
**Dataset** & **Semantic Quality** &  \\  & (‘Truly Minimal Change’) & **Obj. / Att.** & **Global** & **Action** & **Reasoning** \\  
**InstructPix2Pix** & **Low** & ✓ / ✓ & ✓ & ✗ & ✗ \\ 
**HQ-Edit** & **Low** & ✓ / ✓ & ✓ & ✗ & ✗ \\ 
**GenHowTo** & **Low** - Medium & ✗ / ✓ & ✗ & ✓ & ✗ \\ 
**MagicBrush** & **High** & ✓ / ✓ & ✓ & ✗ & ✗ \\ + AG-Edit (Ours) & **High** & ✓ / ✓ & ✗ & ✓ & ✓ \\ + Something-Edit (Ours) & **Medium** - High & ✓ / ✓ & ✗ & ✓ & ✓ \\ + Kubic-Edit (Ours) & **High** & ✓ / ✓ & ✗ & ✓ & ✓ \\
**= AURORA** (Ours) & **High** & ✓ / ✓ & ✓ & ✓ & ✓ \\   

Table 1: **AURORA** vs. comparable public editing datasets. See details on all data sets in Sec. 2 and Sec. 3.2, and Sec. 3 for defining _truly minimal change_. \(=\) skill is covered but to a lesser extent.

**InstructPix2Pix [Brooks et al., 2023]** introduced the first large-scale instruction-guided image editing dataset (313K) in a fully synthetic manner, combining GPT-3 (for prompt generation), Stable Diffusion [Rombach et al., 2022] and Prompt2Prompt (for increasing similarity of image pairs) [Hertz et al., 2022]. However, this scale comes at the cost of general data quality (see random samples in Fig. 28): Often Prompt2Prompt either fails to change anything or changes far more than asked for by the prompt, and in rare cases the prompt is non-sensical. This synthetic data is sufficient, though not optimal, for global and object-centric edits. However, action-centric and reasoning-centric edits either fail in execution or are not represented. Despite using more advanced models and pipelines, we find similar issues in **HQ-Edit [Hui et al., 2024]** under close inspection (see App. J). **MagicBrush [Zhang et al., 2024]** addresses some of InstructPix2Pix's shortcomings, mainly the lack of truly minimal edit pairs, with a rigorous crowdsourcing protocol where humans use the inpainting feature on the DALL-E 2 [Ramesh et al., 2022] interface. This methodology produces truly minimal image pairs for object-centric edits (see Tab. 3b). The inherent limitations of inpainting become apparent with certain attribute edits, and it is entirely unsuitable for action and reasoning-centric editing: Changing the color of a backpack via inpainting would also change its shape, size or texture. We did not find examples of action or reasoning-centric edits in MagicBrush (see App. J).

The landscape of actions and reasoning editing datasets is sparser: A relevant case is **GenHowTo [Soucek et al., 2023]** which focuses on video frames that display actions and subsequent state changes in instructional videos. Their image pairs (and also captions) were chosen automatically, resulting in pairs that are not always minimally different due to excessive camera changes (App. J for random samples). We hypothesize that though GenHowTo may initially seem better at action-centric edits, like InstructPix2Pix it will tend to over-edit and not truly comprehend instructions due to training data quality issues. Reasoning-centric (spatial/geometric reasoning, referring expressions, negation etc.) image pairs can be most directly created via simulation engines, with the hope of Sim2Real transfer. Simulations allows precise control over location, color and even orientation (rotation, flipping) of objects. Such reasoning is rarely covered in other sources: InstructPix2Pix and MagicBrush have almost no mentions of even the simplest spatial terms such as left" or "right". In the next section, we present **AURORA** which addresses some of the above shortcomings by using specific video sources and simulation engines to cover action and reasoning-centric edits in addition to existing quality object-centric editing data.

## 3 AURORA: A diverse and high quality image editing dataset

We present **AURORA**, a balanced dataset covering **A**ction **R**easoning, **O**bject and **A**ttribute edits, comprising a total of 289K training examples, see Fig. 2 and Tab. 1 for details and comparison to existing datasets. App. J provides 16 non-cherry picked training samples for all datasets.

### Truly Minimal Visual Change

As surveyed in Sec. 2, many issues in previous datasets, even when they are large-scale and diverse, can be traced back to the lack of _truly minimal_ image pairs. Beyond manually inspecting examples

Figure 2: Our **AURORA** dataset covers action, reasoning and object-centric edits via 4 sub-datasets.

in existing dataset, the lack of faithfulness wrt. the source image and prompt can be observed in generations of InstructPix2Pix and GenHowTo: In Fig. 3 models changed the background, color of the hydrant, etc. and in the case of GenHowTo, we tend to see "letter artifacts" from its training data (more examples in App. I). MagicBrush (Zhang et al., 2024) was able to produce much better object-centric edits simply by fine-tuning InstructPix2Pix on 8.8K truly minimal image pairs. To complement it, we create a novel (and larger) set of true minimal pairs for action-centric and reasoning-centric edits. Tab. 1 compares ours to existing datasets.

### Creating quality data for action-centric and reasoning-centric edits

We use two types of sources to construct this new dataset: video and simulation engines.

Videoscover a wide range of action-centric edits as they are an abundant source of realistic and diverse state changes (Zellers et al., 2021; Miech et al., 2019; Niu et al., 2024). However simply taking frames from any video data _in the wild_ (e.g. YouTube) often leads to noisy data (see Sec. 5.4): the camera moves, too many things move at once, or the changes are simply not meaningful (i.e. easy to verbalize). Hence, we create _Action-Genome-Edit_ and _Something-Something-Edit_, two new image editing datasets based on carefully selected video frame minimal pairs. Both datasets use frames from video datasets where humans were asked to do activities in or around the house through crowdsourcing which usually represents one action in isolation.

**For _Action-Genome-Edit_, we select frame pairs that had a CLIP cosine distance between 0.1 and 0.4, resulting in 15K pairs (thus filtering out many pairs with camera movement). Since no automatically generated instructions could reliably describe the changes, we tightly work with crowdworkers (App. D) to produce accurate edit instructions, with extensive quality screening and communication. Crucially, we ensured that workers discarded examples where a) there were too many or few changes, b) the changes were hard or lenghty to verbalize, or c) the camera moved (even if slightly) or the image quality was poor. After discarding 4K examples, the final Action-Genome-Edit dataset consists of 11K examples. **For _Something-Something-Edit_** we started from the original Something Something dataset (Goyal et al., 2017) which consists of 221K short clips where humans perform pre-defined basic actions such as "Attaching a string to a balloon", "Folding a cloth", "Lifting a book with a pencil on it", etc. Since the first and last frame of the short clips usually depict the start and end of the action, we selected them as our source and target images. We manually identify 10-15 categories of labels that don't lead to useful changes (e.g. "Pretending to..." where the person does not actually perform the action) and filter those out. The results is a set of 119K minimal frame pairs with high-quality simple edit instructions.

Simulation enginesTo perform action and reasoning-centric editing a model has to master spatial and relational reasoning, geometry, and simple movement. While videos provide some signal for learning these skills, a realistic simulation engine (Greff et al., 2022) offers full control over the arrangement and movement of objects. To teach this basic reasoning, we create _Kubric-Edit_.

_Kubric-Edit_ contains 150K training examples which span three reasoning-centric edit skills - location changes, rotation changes, and count changes - and one object-centric edit skill - attribute changes. We build on top of Wang et al. (2023) who created 6K Kubric image pairs for contrastive image-text-matching, by defining more types of change and significantly extending the dataset. We manually filter and name more than 213 realistic objects from Google Scanned Objects, define templates for

Figure 3: Common failure mode of previous models trained on noisy image pairs (e.g. InstructPix2Pix and GenHowTo): Their outputs are rarely faithful to the source image due to its noisy training pairs.

the edit instructions and ensure more truly minimal change. We cover actions such as _move, turn, swap, flip upside down, add/remove_, spatial configurations such as _left, right, up, down, rotation_, and attributes such as _size, shape or color_. More details are presented in App. E.

Thus, the **AURORA** dataset consists of four carefully selected sub-datasets coming from three sources: MagicBrush (Zhang et al., 2024) (humans equipped with an editing tool on MS-COCO), Action-Genome Edit and Something-Something-Edit (nearby video frames with collected high-quality human captions or filtered previous labels respectively), and Kubric-Edit (from the realistic simulation engine Kubric). In Sec. 5.3 we finetune InstructPix2Pix (Brooks et al., 2023) on our new dataset and thoroughly evaluate its performance across all types of edits. Note: Before collecting new data, we naturally tried to re-use existing datasets of visual change but found them inadequate for varying reasons described in App. M.1.

## 4 Aurora-Bench: A holistic editing benchmark

To holistically assess the editing abilities defined in Sec. 2 (object/attribute, global, action, reasoning, excluding viewpoint), we manually **create a set of 400 image-edit-instruction pairs from 8 sources:****AURORA-Bench**. See Fig. 3(a) for an example of each one. We ensure that **AURORA-Bench** allows studying out-of-distribution (OOD) transfer when a model is trained on **AURORA**, e.g. Sim2Real transfer from Kubric-Edit to real-world (spatial) reasoning or action edits outside of Action-Genome-Edit or Something-Something-Edit. Each of the 8 tasks contains 50 examples of image-prompt pairs that were either directly written by the authors or manually inspected for quality.

We cover object/attribute-centric edits with **MagicBrush** examples, action edits with **Action-Genome-Edit, Something-Something-Edit** and **Epic-Kitchen**(Damen et al., 2018) (OOD), reasoning-centric edits with **Kubric-Edit, CLEVR**(Park et al., 2019) (OOD) and **WhatsUp**(Kamath et al., 2023) (OOD); and **Emu-Global** covers global edits by sampling certain categories from (Sheynin et al., 2023). MagicBrush, Action-Genome-Edit, Something-Something-Edit and Kubric-Edit are introduced in the previous Sec. 3.2. We manually select Epic Kitchen frames and write prompts to study OOD generalization of action understanding since the egocentric scenes and actions are quite different from the other two action-centric subtasks. To assess transfer from our Kubric simulation data, we leverage the real-world diagnostic data in WhatsUp for spatial reasoning, and OOD CLEVR images for testing spatial reasoning in addition to complex referring expressions.

## 5 Evaluation

We begin by introducing the metrics we use on **AURORA-Bench**. Image-editing (and thus its evaluation) can be framed as a two step process: First, given an image-prompt pair, a model must understand how they relate to each other, for example by grounding phrases in the image. This is closely related to traditional vision-and-language understanding. Second, the model must perform the required edits by generating a new image, while being faithful to the original image. Previous work evaluates this second step - the final generation, which we also adopt as our primary judgement. However much insight can be gained from assessing understanding or _discrimination_ abilities of editing models present in the first step. Our second evaluation proposes a new metric that tries to measure just that.

### Evaluation of final generations

**Existing metrics:** There currently exist a series of visual similarity metrics - L1, L2, CLIP-score, DINO-score - which are commonly used to evaluate the similarity of output edit compared to groundtruth images (Zhang et al., 2024; Fu et al., 2023). The effectiveness of these metrics has not been formally justified for image editing, i.e. with human judgement correlations. We hypothesize that these metrics mostly reward models for copying information from the source image, rather than accurate editing. This hypothesis is confirmed using a trivial baseline, simply copying the source image as its output. This _copying_ model outperforms all existing models on these metrics, see Tab. 3b. For instance, using human judgements of MagicBrush and **AURORA** outputs on MagicBrush test examples, we find a very weak correlation of \(0.098\) (using \(CLIP-I\) score), also shown in (Ku et al., 2023). Though faithfulness to the input is an important component of editing, so is actually _modifying_ the image aligned with the prompt. These metrics also assume hard-to-obtain clean groundtruth images- without them meaningful automatic evaluation is even harder. Finally, since many automatic metrics rely on standard vision encoders (e.g. CLIP (Radford et al., 2021)) trained on static images (no video data) and known to be weak reasoners (Yuksekgonul et al., 2023), they are not suitable for our study. When we compute human correlation on WhatsUp examples (spatial reasoning), with clean groundtruth images, it drops to zero. In summary, these metrics might detect a model that struggles with faithfulness to the source image such as InstructPix2Pix (see Fig. 3), but are not helpful for comparing the semantic accuracy of stronger models.

**Human judgment of edited images:** With the insight that automatic visual similarity metrics are only a weak signal, we primarily rely on human judgment of model outputs on **AURORA**-Bench: We ask humans to rate the absolute edit success (0=none, 50=partial, 100=full) as well as comparison (i.e. win-rates) between different models. We focus on the former in our main results as it allows us to compare task difficulty. We ensure that evaluators (we pick the best three evaluators from crowd-sourcing **AURORA**) pay most attention to the correct (semantic) interpretation of the edit prompts 2. App. D.2 describes guidelines, compensation and extensive communication with crowdworkers.

### DiscEdit: discriminative evaluation of image editing

We propose an additional automatic metric _DiscEdit_ applicable to **AURORA**-Bench examples. Unlike Sec. 5.1 above which considered the overall accuracy of generated edits, this evaluation serves as a diagnostic test for determining whether models truly understand how prompts relate to the input image, or can abstain from editing.

Inspired by text-to-image models repurposed as discriminators (Krojer et al., 2023; Li et al., 2023), models are given an image and two minimally different edit instructions \(t_{nochange}\) and \(t_{change}\). While \(t_{nochange}\) requires _little to no change_ to the source image, \(t_{change}\) requires models to perform _significant changes_. An example of such a test pair is given in Fig. 4b. Thus, we expect the similarity between the first generated image \(i_{nochange}\) and the source image \(i_{src}\) to be higher than the similarity between the second generated image \(i_{change}\) and the source image \(i_{src}\), which we measure as a L2 distance in the latent space of the diffusion model (written \(Enc(i)\)):

\[_{}=1&\|(i_{})- (i_{})\|_{2}<\|(i_{})-(i _{})\|_{2}\\ 0&\]

The intuition behind _DiscEdit_ is that edits should be proportional to those described in the prompt - in other words, models should not change images more than required, nor should they produce fewer edits than requested in instructions. This metric therefore tests how much models are following or 'understanding' what instructions require. On the flip side, it also quantifies a form of hallucination: Changing things even when no change is required. Since it is not possible to find a "no-change" prompt \(t_{nochange}\) for all kinds of prompts, we select a subset of source-prompt pairs \((i_{src},t_{change})\) from _AROA-Bench_ and manually define a no-change prompt \(t_{nochange}\). App. H contains details on the data creation process and implementation of _DiscEdit_. A _DiscEdit_ score of 0 or 1 is interpretable, and the metric does not require costly groundtruth target images. While it might seem far-fetched to expect the model to recognize when a scene does not need to be changed, we note that it is relevant in scenarios where image editing is a component of generative simulators (Yang et al., 2024).

### Results

Our baselines are InstructPix2Pix [Brooks et al., 2023], GenHowTo [Soucek et al., 2023], MGIE [Fu et al., 2023] and MagicBrush [Zhang et al., 2024]3. See Sec. 2 for details on their training data. We train our own **AURORA** model with the InstructPix2Pix architecture on the **AURORA** dataset and mix all four sources such that each dataset is equally likely to be sampled during training, and take a checkpoint that was first pretrained on InstructPix2Pix and then MagicBrush (more details: App. F).

Human JudgementTab. 2 summarizes our results on **AURORA**-Bench evaluated via human judgement of successful adherence to the prompt and source image (0=none, 50=partial, 100=full). Most notably, **our model significantly improves on the challenging action and reasoning-centric edits**. However, action edits on complex real world images remain a challenge, while we see stronger numbers on "simpler" reasoning. At the same time, **AURORA** maintains strong performance on the diverse and well-established MagicBrush test set, leading to a high _Overall Score_. Finally, we observe generalization from training on simulation to CLEVR, and notably WhatsUp, a real-world spatial reasoning task.

DiscEditTab. 5 shows results with the _DiscEdit_ metric on **AURORA**-Bench examples. Discriminating between minmal prompts that either require a change or no change, proves to be a hard task: With **AURORA**, performance is slightly above random on most tasks except CLEVR. Performance of the MagicBrush model is even below random chance. We investigate this surprising result but could not find any pattern. Thus, we can only hypothesize that the wording of "no-change" prompts might be more unusual, and hence lead to hallucination behaviour (example outputs in App. I.2). We also find several encouraging qualitative results from our model (Fig. 3(b)).

### Ablations and qualitative analysis

Can we quantify Sim2Real transfer? **AURORA** contains many simulated examples featuring spatial reasoning. To quantify the transfer to _spatial reasoning on real images_, we train a model on **AURORA** minus Kubric and manually rate the outputs on the WhatsUp examples from **AURORA**-Bench: A win-rate of 46% for the full **AURORA** model, while without Kubric only a single win

    & **Obj/Attr.** &  &  &  \\ 
**Model** & **Magic** & **Action-** & **Something** & **Epic** & & & & **Emu-** & **Overall** \\
**Brush** & **Gemone** & **Something** & **Kitchen** & & & & & & **Emu-** & **Overall** \\ 
**GenHowTo** & 18.0 & 8.0 & 8.7 & **17.7** & 4.3 & 0.7 & 2.0 & 11.3 & 10.8 \\
**MGIE** & 36.0 & 7.0 & 11.3 & 5.0 & 6.0 & 6.7 & 16.0 & 36.5 & 22.5 \\
**InstructPix2Pix** & 31.3 & 13.3 & 12.3 & 4.3 & 0.7 & 5.7 & 14.7 & 33.7 & 20.5 \\
**MagicBrush** & **61.7** & 16.3 & 17.0 & 12.0 & 3.0 & 9.3 & 22.0 & **42.3** & 32.6 \\
**AURORA** (**Ours**) & 60.5 & **35.6** & **31.8** & 14.2 & **27.3** & **59.6** & **46.1** & 33.0 & **41.3** \\   

Table 2: **Human Judgment of semantic editing success on **AURORA**-Bench tasks. Humans were asked to rate the edit success from none (0), partial (50) to full (100). Extended table in App. G. Overall score is “balanced”: we average each skill first, and then take the average of those 4 numbers. Note: Our model was trained on more of the datasets than e.g. Magicbrush, so some of them are more IID for our model.

  
**Model** & **L1/ L2.1** & **DINO\(\)** & **CLIP-1\(\)** \\  InstructPix & 0.112 / 0.037 & 0.746 & 0.8538 \\ MagicBrush & 0.072 / 0.025 & 0.865 & 0.915 \\ Naive Copy & **0.036 / 0.015** & **0.917** & **0.943** \\   

Table 3: **Automatic Evaluation**: DiscEdit (left) and issues with existing automatic metrics (right) (a) Discrimination performance with _DiscEdit_ (comparing the two strongest models from Tab. 2). We show binary accuracy (50% random chance). Details: we average each example over 4 noise samples for the denoising process (App. H)(2%) is found 4. We also find that training on truly minimal Kubric examples "stabilizes" training: Without it, the model hallucinates more un-needed changes and artifacts (e.g. adding people).

**EditDAAM:** We adopt DAAM (Diffusion Attentive Attribution Maps) [Tang et al., 2023] for qualitatively studying the attention maps of our editing model but study patterns across U-Net layers, grouping them into _Down_, _Middle_ and _Upper_ layers. We intuit that image _understanding_ happens in earlier layers and the final _generation_ in later layers. Since our model has seen more movement-based and spatial edits in training compared to MagicBrush, we hypothesize that this is reflected in its attention patterns. We illustrate these attention maps in Fig. 33 of the Appendix. Compared to MagicBrush, **AUORRA** pays attention to a broader area starting in the middle layers of the U-Net, possibly since movement requires "scouting" the space where placing a new object is reasonable. In the upper layers it narrows down on precise object placement. Details in App. K.

**Common verb and nouns in AUORRA vs. MS-COCO:** We investigate if the distribution of verbs between broad generic VL captioning datasets and datasets tailored for action-editing differs significantly. A lot of edits have generic verbs like "move", which is nonetheless often still a complex task: "Move the cup closer to the plate" is a very different move than "Move the hand closer to their hair", where the exact action required is implicit in the scene/affordances/angles and not reflected in the textual "move". This is inherent to the editing task itself where captions tend to be shorter than traditional caption datasets, often with simpler verbs "make OBJECT ATTRIBUTE" (make the horse darker) or "replace/add OBJECT". So another interesting comparison is the distribution of verbs in traditional (object/attribute) editing vs. our focus on action editing. Finally, we note that a lot of complexity in our data comes from other linguistic constituents such as prepositions or adjectives/adverbs, e.g. "Move the hand slightly closer under the table with the finger pointing upward" where [slightly, closer, under, upward] are all interesting to understand but not verbs. To study the frequency differences to established caption datasets, we visualize the verb and noun distribution in MS-COCO, AUORRA as well as the four subsets of AUORRA. See App. L for detailed frequencies and figures. Overall, the verbs are less diverse but as described above a lot of the complexity comes from other textual or visual aspects. On top, the verbs are quite different to COCO and notably also quite different to more established object/attribute-centric editing such as MagicBrush. Also note that while "make" is a very frequent verb, it can often be accompanied with one of the other verbs like "make the person stand up".

Figure 5: **Prompt:** _Put the white porcelain ramekin on the right hand of the brown shoe._ We show attention maps for three levels of U-Net layers: Down, Middle, Upper.

Conclusion and Future Work

Edits that require an understanding of real-world dynamics (e.g. actions) and reasoning are hard, especially compared to progress on more established editing subtasks. We hope that our contributions - from diverse high-quality training data with **AUORA**, to rigorous evaluation with **AUORA**-Bench and a new state-of-the-art model- will pave the road for further progress on building _truly general_ image editing models. Understanding how to improve action and reasoning-centric editing also relates to a more fundamental problem: world modelling, i.e. predicting the next observation after taking an action on the current one. This form of image editing can be seen as one-step controllable video generation, which in turn can be used as a **generative world-simulator**. For example, both editing or video generation can "simulate" how a visual scene would change when a robot executes an action . Though, our results show that we are still far from achieving broad world models - see App. B.1 for a deeper discussion on limitations - our work is a step in that direction. It has the potential to not only enable better editing tools, but also to replace narrow rule-based simulators with generative ones for "limitless" interactive training data. It is an open question for future work whether one-step editing is the right paradigm or if generating the whole trajectory from source to target image (video generation) is needed to master the edits we study in this paper.