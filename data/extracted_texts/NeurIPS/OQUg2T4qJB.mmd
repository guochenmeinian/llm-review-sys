# Ordering-Based Causal Discovery for Linear and Nonlinear Relations

Zhuopeng Xu   Yujie Li   Cheng Liu   Ning Gui

School of Computer Science and Engineering

Central South University

{xuzhuopeng, yujieli}@csu.edu.cn, {liuchengstudy, ninggui}@gmail.com

Corresponding author.

###### Abstract

Identifying causal relations from purely observational data typically requires additional assumptions on relations and/or noise. Most current methods restrict their analysis to datasets that are assumed to have pure linear or nonlinear relations, which is often not reflective of real-world datasets that contain a combination of both. This paper presents CaPS, an ordering-based causal discovery algorithm that effectively handles linear and nonlinear relations. CaPS introduces a novel identification criterion for topological ordering and incorporates the concept of "parent score" during the post-processing optimization stage. These scores quantify the strength of the average causal effect, helping to accelerate the pruning process and correct inaccurate predictions in the pruning step. Experimental results demonstrate that our proposed solutions outperform state-of-the-art baselines on synthetic data with varying ratios of linear and nonlinear relations. The results obtained from real-world data also support the competitiveness of CaPS. Code and datasets are available at https://github.com/E2real/CaPS.

## 1 Introduction

Causal discovery uncovers latent causal relationships within data by modeling a Directed Acyclic Graph (DAG) connecting various variables. This field is of significant importance in domains such as biology , epidemiology , and finance . Due to the considerable expense associated with conducting interventional experiments, the recent emphasis on causal discovery has gradually shifted from discovery with interventional data [4; 5; 6] to discovery solely based on observational data.

In general, the problem of causal discovery from observational data faces the identifiability issue. Different generative models with different causal relations might produce the same data distribution. Many recent works try to have uniquely identified DAG by placing different types of assumptions on the noise and/or relations, e.g., Shimizu et al.  prove that linear causal relations with non-Gaussian additive noise can be identifiable; Peters and Buhlmann  prove that Gaussian linear structural equation models (SEMs) with equal variances are identifiable. For nonlinear causal relations, Peters et al.  relax the assumption of noise and proves the identifiability of DAG. From the discussion mentioned above, existing approaches normally limit their discussions to distributions with either pure linear or pure nonlinear relations.

However, real-world data often contain both types of causal relations and run against their basic assumptions. These approaches work well when the observational data match their prespecified (non-)linear or nonlinear relations while suffering significant performance loss when their assumptions mismatch. Fig. 1 illustrates the performance of three solutions: GOLEM  for linear relations, SCORE  for nonlinear relations, and our proposed CaPS, on synthetic data with varying proportions of linear relations. The performance of SCORE decreases as the linear ratio increases, whileGOLEM performs poorly when the linear relation ratio is low. This indicates that approaches with strong restrictions on the types of relations are not suitable for real-world applications. Consequently, it is necessary to find a unified causal learning framework to capture both types of relations.

Ordering-based methods  divide the casual discovery process into two subtasks: (i) topological ordering and (ii) post-processing. It has been shown to have the capability to reduce the complexity of DAG discovery while keeping the acyclicity constraint. In addition, ordering-based methods guide the direction of causal relations, thus avoiding the fitting of a potential inverse model. However, existing ordering-based approaches still face the same problem of relying on the assumption of linear, e.g., LISTEN or nonlinear causal relations, e.g., SCORE.

This paper introduces a unified approach, Causal Discovery with Parent Score (CaPS), that does not rely on linear or nonlinear assumptions, within the scope of ordering-based casual discovery. To determine the topological ordering, we propose a novel unified criterion for distinguishing leaf nodes by utilizing the expectation of the Hessian of the data log-likelihood. Additionally, inspired by the average treatment effect (ATE) in estimating causal effects , the parent score, a new metric, is proposed to represent the average causal effects of all samples. No matter whether the causal relations are linear or nonlinear, this metric can be used to effectively guide parent selection in post-processing.

**Contributions.** 1) A novel ordering criterion is proposed for distinguishing leaf nodes, which enables learning of topological ordering for data with both types of relations; 2) A new criterion, parent score is introduced to reflects the strength of the average causal effect of a given parent. Utilizing this criterion, CaPS designs pre-pruning and edge supplement operations to speed up the pruning process and rectify inaccurate predictions in the pruning step. 3) Extensive experiments are conducted to compare eight state-of-the-art baselines on synthetic and real-world data, showing competitive results. Furthermore, various analysis simulations demonstrate the effectiveness of our proposed designs.

## 2 Related Work

**Causal discovery for SEMs.** Gaining knowledge of DAGs from observational data often necessitates additional suppositions about the distributions and/or relations. This paper discusses SEM-based studies from either the linear or nonlinear point of view.

To ensure the identifiability of linear causal relations, additional noise constraints must be made, for example, Gaussian noise with equal variance  or conditional variance . Earlier work by LiNGAM  demonstrated the identifiability of linear causal relations with non-Gaussian noise and proposed an ICA-based method to identify this SEM. NOTEARS  proposed a novel continuous optimization approach based on the trace exponential function, which provides a new idea of continuous optimization for causal discovery. GOLEM further  proposed a continuous likelihood-based method with soft sparsity and DAG constraints, which improved the performance of linear causal discovery. The optimization of these works is largely dependent on the linear parameterization of the weighted adjacency matrix.

Figure 1: Performance of different solutions under datasets with different linear proportions. Since we donâ€™t know whether the real data is linear or nonlinear, it is difficult to choose an effective model. Thus, we need a method that works well in both linear and nonlinear and most possibly mixed cases.

For nonlinear SEMs, e.g., the nonlinear additive noise model (ANM)  is known to be identifiable with arbitrary additive noise, except in some rare cases. To discover causal relations under nonlinear SEM, many previous works [16; 17; 18; 19; 20] proposed different DAG learning methods with a continuous acyclicity constraint. These works generally require a training model with an augmented Lagrangian approach which results in considerable computation cost.

**Ordering-based causal discovery** can partially avoid the aforementioned problems, as the order space is much smaller than the DAG space. CAM  is an early ordering-based approach that uses a greedy search to estimate the topological ordering and significance tests to prune the DAG. LISTEN  proposed a new method for distinguishing leaf nodes of linear causal relations, based on the precision matrix . However, these methods are based on the connection between the precision matrix and the weight matrix, which is not a unified criterion for linear and nonlinear SEMs. SCORE  estimated the score (the Jacobian of logarithmic probability of data, \( p(x)\)) using the second-order Stein gradient estimator, and then determined the leaf nodes based on the score to find the topological ordering. DiffAN  was proposed to estimate the score via a diffusion model.

Several recent works merge the two-stage process into one step by end-to-end differentiable optimizations to address the issue of error propagation, e.g VI-DP-DAG  with variational inference and DAGuerreotype  over the polytope. DAGuerreotype provides two variants specifically designed for linear/nonlinear relations. However, none of the above works provides a uniform solution that can identify DAG in datasets with both linear and nonlinear relations.

## 3 Preliminaries

### Structural Equation Model

The structural equation model for the causal discovery can be represented as : for random variable \(=\{x_{1},x_{2},...,x_{d}\}^{d}\) sampling from the real joint probability distribution \(p()\), we want to find a faithful causal graph \(\) to represent the causal relationships between variables of different dimensions. The SEM is defined with equation (1):

\[x_{i}=f_{i}(pa_{i}(x))+_{i}\] (1)

where \(x_{i}\), \(i=1,2,...,d\). \(pa_{i}(x)\) denotes the parents of \(x_{i}\), \(f_{i}\) denotes the causal function, and \(_{i}\) denotes the additive noise of \(x_{i}\). For each \(x_{i}\), the parents and the noise are independent of each other, \(pa_{i}(x)\!\!\!_{i}\), and there is no unobservable confounder. Each causal function \(f_{i}\) can be linear and nonlinear, and the additive noise \(_{i}(0,_{i}^{2})\) is Gaussian. These are the basic assumptions of ANM , and we relax the linear and nonlinear conditions in our SEM.

### Topological Ordering

Finding topological ordering is an important subtask for ordering-based causal discovery, which can reduce the DAG search space.

**Definition**.: Since the topological ordering of the causal graph \(\) may not be unique, we define a set of order permutations \(\) to represent all valid topological orderings. For any order permutation \(\), a parent node must always be before a child node, i.e. \((i)<(j)\) if \(x_{j}\) is a descendant of \(x_{i}\) on \(\). The corresponding initialized adjacency matrix \(\) should have \(_{i,j}=1\) and \(_{j,i}=0\).

**Estimation.** To identify the leaf nodes of a DAG, SCORE suggests using the score \(s_{j}(x)\), which is equal to the logarithmic gradient of the joint probability distribution \(p()\) with respect to \(x_{j}\). If a Markov chain is used to represent \(p()\), then the score of each variable is equivalent to the logarithm gradient of the joint probability distribution.

\[ s_{j}(x)&=_{x_{j}}_{i=1}^{ d}p(x_{i}|pa_{i}(x))=_{x_{j}}_{i=1}^{d} p(x_{i}|pa_{i}(x))\\ &}{{=}}_{x_{j}}[- _{i=1}^{d}(-f_{i}(pa_{i}(x))}{_{i}})^{2}-_{i=1 }^{d}(2_{i}^{2})]\\ &=--f_{j}(pa_{j}(x))}{_{j}^{2}}+_{i ch(j) }}{ x_{j}}(pa_{i}(x))-f_{i}(pa_{i}(x)) }{_{i}^{2}}\] (2)where \((i)\) uses the _change of variables theorem_ with \(x_{i}-f_{i}(pa_{i}(x))=_{i}\) and \(ch(j)\) denotes the children of \(x_{j}\) in the causal graph \(\). According to Eq.2, for each leaf node, it is easy to see that \((x)}{ x_{j}}=-^{2}}\) is a constant. Thus, given the nonlinear assumption of the causal function \(f_{i}\), SCORE has shown that \(x_{j}\) is a leaf node iff the variance of \((x)}{ x_{j}}\) is zero. The score \(s_{j}(x)\) and each element on the diagonal of the score's Jacobian \((x)}{ x_{j}}\) can be estimated using the second-order Stein gradient estimator  or the diffusion model .

## 4 Causal Discovery with Parent Score

This section first examines the limits of current baselines under non-preassumed relations. Then, the design of CaPS in both topological ordering and post-processing is introduced.

### Leaf Nodes Discrimination

Previous attempts to order nodes according to a certain criterion were unsuccessful due to the lack of a unified standard to distinguish leaf nodes in datasets with mix relations. To make this point more evident, we provide examples of both linear and nonlinear cases. LISTEN  for linear causal relations uses the minimum value of the precision matrix's diagonal to distinguish leaf nodes. However, this approach fails because the connection between the precision matrix and the true causal graph no longer holds under nonlinear causal relations, which is detailed in Appendix A.1.

SCORE for nonlinear causal relations cannot differentiate leaf nodes in linear causal relations. To illustrate this, consider a simple linear causal case \(x_{i}=_{x_{k} pa_{i}(x)}w_{i,k}x_{k}+_{i}\), where \(f_{i}\) is linear in ANM. In this linear SEM, \(}{ x_{j}}(pa_{i}(x))=w_{i,j}\) is a constant and \(f_{i}}{ x_{j}^{2}}(pa_{i}(x))=0\). Consequently, for any node, the value of each element on the diagonal of the score's Jacobian is always constant, i.e., \((x)}{ x_{j}}=-^{2}}-_{i  ch(j)}^{2}}{_{i}^{2}}\), making SCORE unable to differentiate leaf nodes. To address this issue, we propose a new discriminant criterion in Theorem 1 effective in both linear and nonlinear causal relations and give its sufficient conditions for identifiability in Assumption 1.

**Assumption 1**.: _(**Sufficient conditions for identifiability).** The topological ordering of a causal graph is identifiable if **one** of the following sufficient conditions is satisfied._

_(i) **Non-decreasing variance of noises.** For any two noises_ \(_{i}\) _and_ \(_{j}\)_,_ \(_{j}_{i}\) _if_ \((i)<(j)\)_._

_(ii) **Non-weak causal effect.** For any non-leaf nodes_ \(x_{j}\)_,_ \(_{i Ch(j)}^{2}}[(}{  x_{j}}(pa_{i}(x)))^{2}]}^{2}}-^{2}}\)_._

where \(_{}\) is the minimum variance for all noises. Assumption 1 gives two conditions for identifiability which no longer depends on the linearity or nonlinearity of the causal function and relaxes previous identifiability conditions. Condition \((i)\) is an extension of the equal variance assumption [8; 13]. Condition \((ii)\) is a new sufficient condition, which first quantitatively gives a lower bound of identifiable causal effects. It enables CaPS to identify causal relations even in scenarios outside of non-decreasing variance. For example, considering a variance-unscrable scenario with \(^{2} U(0.1,1)\) and causal effect greater than 3, CaPS can also work well because the the sum of parent score is greater than the given lower bound in condition\((ii)\). The meaning of condition \((ii)\) will be further discussed in section 4.2. Under conditions \((i)\) or \((ii)\), the topological ordering can be identified by Theorem 1.

**Theorem 1**.: _Let \(s(x)= p(x)\) be the score and let \(()\) be the diagonal elements of the matrix. For any \(x_{j}\) in the causal graph \(\):_

\[j=(([])) x _{j}\]

Proof.: (Simplified version; details are in Appendix A.2.)

For an arbitrary node \(x_{j}\) in the causal graph \(\), we focus on the diagonal of the score's Jacobian.

\[(x)}{ x_{j}}=-^{2}}-_{i ch (j)}^{2}}(}{ x_{j}}(pa_{i}(x) ))^{2}+_{i ch(j)}f_{i}}{ x_{j}^{2}}(pa_{i}(x ))-f_{i}(pa_{i}(x))}{_{i}^{2}}\] (3)Since \(-f_{i}(pa_{i}(x))}{_{i}^{2}}=}{_{i}^{2}} (0,^{2}})\) and \(pa_{i}(x)\!\!\!_{i}\) in our SEM, the expectation of \((x)}{ x_{j}}\) can be restated as:

\[[(x)}{ x_{j}}]=-^{2}} -_{i Ch(j)}^{2}}[(}{  x_{j}}(pa_{i}(x)))^{2}]\] (4)

Suppose that \(x_{l}\) is a leaf node and \(x_{n}\) is a non-leaf node, we have \([(x)}{ x_{l}}]=-^{2}}\). Then, \([(x)}{ x_{l}}][(x)}{ x_{n}}]\) always holds under conditions \((i)\) or \((ii)\). Thus, the node in \((([]))\) will always be the leaf node.

Theorem 1 suggests that the expectation of the diagonal of the score's Jacobian can be used to identify leaf nodes. Thus, applying Theorem 1, the topological ordering is identifiable by iteratively eliminating the current leaf node . The detailed procedure is included in Algorithm 1, where we use the second-order Stein gradient estimator to estimate the score's Jacobian \(\).

### Parent Score

Theorem 1 specifies how to identify the correct topological ordering of the graph \(\). However, it is not straightforward to determine the parents of each node. To identify the true causal graph \(\), we need information beyond permutation to guide the selection of parents. Thus, we need a metric that can qualitatively express the causal effects from a parent node to one of its children. Here, a new metric, "Parent Score" is proposed to approximate the average causal effect.

**Definition of parent score.** We define Eq.5 to express the parent score \(_{i,j}\) which approximates the strength of the average causal effect of all samples from \(x_{j}\) to \(x_{i}\).

\[_{i,j}=^{2}}[(}{ x_{j}}(pa_{i}(x)))^{2}],&x_{j} pa_{i}(x)\\ 0,x_{j} pa_{i}(x)\] (5)

where \(_{i,j}=0\) if \(x_{j}\) is not a parent of \(x_{i}\), and \(_{i,j}>0\) if \(x_{j}\) is a parent of \(x_{i}\).

To illustrate the meaning of this definition, we propose a new metric of the Squared Average Treatment Effect (SATE) extended from Average Treatment Effect (ATE, \([Y^{(T=1)}-Y^{(T=0)}]\)). In the task of estimating causal effects , ATE is often used to measure the average effect of a treatment or intervention on an outcome variable. Since we focus only on the strength of the effect rather than on the positive or negative effect, SATE is defined as follows:

\[_{i}^{j}=[(x_{i}^{(T_{i}=1)}-x_{i}^{(T_{j}=0)})^{2}]\] (6)

where \(T_{j}=1\) and \(T_{j}=0\) indicate whether \(x_{j}\) are treated or not. With a small additive treatment, we show that SATE from \(x_{j}\) to \(x_{i}\) can be approximated by \([(}{ x_{j}}(pa_{i}(x)))^{2}]\). Thus, the parent score \(_{i,j}\) is the causal effect of the parent scaled by its variance of noise. The detailed derivation is shown in Appendix A.3.

**Computing parent score.** Parent score cannot be obtained directly from the summation of average causal effects on childrens in Eq.4, thus we propose an iterative decoupling process and define

\[=\{[(x)}{ x_{1}}],[(x)}{ x_{2}}],...,[(x)}{ x_{d}}]\}\] (7)

to denote the expectation of the diagonal of the score's Jacobian. By removing the node \(x_{i}\), a new vector \(_{-i}\) is defined as follows:

\[_{-i}= \{[(x_{-i})}{ x_{1}}], [(x_{-i})}{ x_{2}}],...,[ (x)}{ x_{i}}],...,[( x_{-i})}{ x_{d}}]\}\] (8)

where \(x_{-i}\) represents the remaining data after removing the feature of \(i\)-th dimension. For the \(i\)-th element of \(_{-i}\), we fill the \(i\)-th element of \(\). Then, each row vector of the matrix of parent score \(^{d d}\) is equivalent to:

\[_{i,:}=_{-i}-\] (9)

The complete \(\) can be obtained by iteratively computing parent score of each row. The specific derivation of this procedure is given in Appendix A.4. The Algorithm 1 describes the process of finding the topological order and computing the parent score, where \(X_{-i}\) denotes the data matrix with the \(i\)-th feature removed. Similarly, \(X_{-r}\) denotes the data matrix with a set of removed features.

**Association with leaf nodes discrimination.** We can revisit the criterion to distinguish leaf nodes with parent score. According to the definition of parent score, given the node \(x_{j}\), \(_{i=0}^{d}_{i,j}\) can be considered as the total causal effect to its children. The sufficient condition \((ii)\) for identifiability can be restated as \(_{i=0}^{d}_{i,j}}}-}\) when \(x_{j}\) is not a leaf. It means that the causal relations can be identified if the causal effect stronger than the given lower bound, which gives a quantitative interpretation for an intuitive conclusion. Under this sufficient condition, the meaning of Theorem 1 can be further explained in the following corollary.

**Corollary 1**.: \(j=(([])) x _{j}\)_'s sum of parent score \(_{i=0}^{d}_{i,j}\) is minimal \( x_{j}\) is a leaf node_

The detailed proof is given in the Appendix A.5. Corollary 1 shows that Theorem 1 is actually finding the minimal sum of the parent score. Then, the node with the minimal total causal effect is a leaf node. This corollary implies the association between parent score and Theorem 1. Thus, the parent score can be considered as a unified metric during topological ordering and post-processing.

### Pre-pruning and Edge Supplement

Previous research has demonstrated that redundant edges can be successfully eliminated through CAM pruning, which applies significance testing of covariates based on generalized additive models. This technique is widely used in many strong baselines [11; 23; 28]. However, CAM pruning is time-consuming and only utilizes the topological ordering information. The proposed metric, parent score, can further provide more information on causal effects. Thus, CaPS introduces the pre-pruning and edge supplement operations before/after CAM pruning process to accelerate pruning by removing edges with low parent score and restore removed edges with strong parent score.

**Pre-pruning.** Before CAM pruning, we use low-confidence parents to pre-prune the initial graph, which can remove the low-confidence edges and reduce the searching space for CAM pruning. For each node, we use the maximum value of their parents to determine the threshold for pre-pruning. Specifically, for any \(x_{i},x_{j}\), we mask the adjacency matrix \(_{j,i}=0\) if \(_{i,j}<_{i,j})}{}\), where \(\) is a hyperparameter that represents the rigor in prepruning. This design can greatly speed up the pruning process, especially when the number of nodes is large (see Appendix C.5).

**Edge supplement.** After CAM pruning, we use high-confidence parents to supplement the edge, which can remedy errors in topological ordering and incorrect deletion in CAM pruning. With CAM pruning, the existing edges are likely to be the correct edges in a real causal graph. Thus, the parent score in current edges is used to automatically determine the threshold for edge supplement. For any \(x_{i},x_{j}\), we supplement the edge \(_{j,i}=1\) when the following conditions are satisfied. First, \(_{i,j}>(^{})\), where \(\) denotes the Hadamard product, and \(()\) returns the average value of a matrix. Here, we use the same rigor \(\) for pre-pruning and edge supplement. Second, \(\) is acyclic after supplementing the current edge. Note that edges added later may potentially violate acyclicity, so we use a greedy strategy to prioritize adding the edge with a higher parent score. The pseudocode of post-processing are released in Appendix B.

### Computational Complexity

For Algorithm 1, the computational complexity is mainly related to the estimation of score's Jacobian \(d\) times, which is \((d n^{3})\), with \(n\) for the number of samples and \(d\) for the feature dimension. For post-processing, the computational complexity is \((d^{2}+d r^{(n,m)}+s(d+e+s))\), which can be considered as two steps. For the pruning step, the computational complexity of the original CAM pruning is \((d r^{(n,d)})\), where \(r^{(n,d)}\) is the complexity function of training a generalized additive model. With pre-pruning, the computational complexity of pruning can be reduced to \((d^{2}+d r^{(n,m)})\), where \(m d\) is the maximum number of parents for each node. For the edge supplement step, the computational complexity is \((s(d+e+s))\) due to acyclic testing, where \(s\) denotes the number of candidate edges and \(e<\) denotes the number of edges remaining after pruning in a DAG. Since we only want to supplement the edges with high confidence, \(s\) tends to be a small value in the implementation. Despite this cubic complexity of \(n\), the actual-time growth is close to linear since many CaPS operations are GPU-friendly, which is detailed in Appendix C.5.

## 5 Experiments

### Baselines and Settings

**Baselines.** This paper benchmarks CaPS against eight strong state-of-the-art baselines designed for:

_Linear:_ NOTEARS  and GOLEM , two strong linear methods with continuous optimization.

_Nonlinear:_ GraNDAG  formulates neural network paths and a connectivity matrix, and substitutes them into the acyclicity penalty; Five ordering-based methods (CAM, VI-DP-DAG, SCORE, DiffAN, and DAGuerreotype) are chosen for comparison. DAGuerreotype has two versions: a linear one (DAGuerreotype-L) and a nonlinear one (DAGuerreotype-N). Both variants are evaluated in the synthetic data experiments to ensure a fair comparison.

**Metrics.** Three metrics in causal discovery are adopted for evaluation: the structural Hamming distance (SHD), the structural intervention distance (SID) , and the F1 score. SHD evaluates the number of edges that must be altered to make the estimated causal graph match the true causal graph. SID assesses the number of interventional distributions in the true causal graph that are disrupted in the estimated causal graph. Lower values for SHD and SID are desirable. SHD favors sparser estimated causal graphs, whereas SID favors denser estimated causal graphs. Therefore, doing well in only one of these two metrics does not necessarily mean effectiveness. F1 score measures the balance between precision and recall, with higher values indicating better performance.

**Settings.** We used the settings from the respective papers for all the baselines. For some methods that have multiple versions, such as GOLEM and DAGuerreotype, we reported the results of the version that gave the best performance on the corresponding dataset. The only hyperparameter of CaPS was rigor \(\), which we set to \(=50\) for all datasets to avoid any dataset-specific tuning.

**Datasets.** Synthetic data are created using the Erdos-Renyi (ER)  or Scale-Free (SF) models with different linear and nonlinear proportion. We set the number of nodes \(d=10\) and the number of samples \(n=2000\) by default, while \(d=20,50\) and \(n=1000,5000\) are also given. Real dataset contains a protein expression dataset _Sachs_ and a pseudoreal transport network dataset _Syntern_. Details and more insights of the synthetic and the real data can be found in Appendix C.1.

### Synthetic Data

Fig.2 shows the experiment results of eight baselines. We can observe that CaPS performs better for both sparser (SynER1) and denser (SynER4) graphs in almost all ranges, especially when the linear proportions are greater than 0.25. We also note that GOLEM's performance decreases with increasing nonlinear proportion, and SCORE's performance decreases with increasing linear proportion, which could be due to their assumptions not being met. In contrast, CaPS performs consistently well in almost all ratios. Experiments with the SynSF1 and SynSF4 datasets show similar results, and further information can be found in Appendix C.2.

### Real Data

The results of the real data are presented in Table 1. CaPS achieves the highest SHD and F1 scores on Sachs, with SID coming second to VI-DP-DAG. VI-DP-DAG had the best SID but the worst SHD, as it discovers a large number of false edges. On Syntren, GraNDAG is the top performer since the pattern of this dataset is not friendly to ordering-based methods (see Appendix C.1). However, CaPS achieves the best performance compared to other ordering-based methods.

To investigate the impact of each component of CaPS, we replace Theorem 1 with a random topological ordering (w/o Theorem 1) or turn off the pre-pruning and edge supplement with parent score (w/o Parent Score). The results show that Theorem 1 makes a major contribution to CaPS, and the parent score can further improve it. Actually, the performance of CaPS can be further improved by adjusting its hyperparameter \(\). The sensitivity of different \(\) is further analyzed in Appendix C.3.

### Analysis Experiments

**Larger-scale datasets & actual-time cost.** Despite cubic complexity in samples size \(n\), the bottleneck of actual-time growth often lies in the number of nodes \(d\) in causal graph since many \(n\)-related operations are GPU-friendly. In Fig. 3, we illustrate the performance with actual-time cost for all baselines in larger-scale SynER1 (\(d=20\) and \(d=50\)) with 0.5 linear proportion. CaPS consistently achieves best performance in larger-scale causal graph while its time cost is competitive. The full

  
**Dataset** &  &  \\ 
**Metrics** & **SHD\(\)** & **SID\(\)** & **F1\(\)** & **SHD\(\)** & **SID\(\)** & **F1\(\)** \\  NOTEARS & 12.0Â±0.00 & 46.0Â±0.00 & 0.387Â±0.000 & 33.9Â±4.57 & 192.8Â±54.73 & 0.164Â±0.085 \\ GOLEM & 17.0Â±0.00 & 44.0Â±0.00 & 0.42Â±0.000 & 43.7Â±10.72 & 177.4Â±56.55 & 0.163Â±0.066 \\ GraNDAG & 13.2Â±0.75 & 54.0Â±1.10 & 0.37Â±0.064 & **26.5Â±4.65** & 14.5Â±35.81 & **0.34Â±0.104** \\  CAM & 12.0Â±0.00 & 55.0Â±0.00 & 0.44Â±40.000 & 38.0Â±5.59 & 178.6Â±4.56 & 0.223Â±0.099 \\ VI-DP-DAG & 42.6Â±1.36 & **40.0Â±5.66** & 0.34Â±0.037 & 182.6Â±4.29 & **144.3Â±35.00** & 0.069Â±0.039 \\ SCORE & 12.0Â±0.00 & 45.0Â±0.00 & 0.44Â±40.000 & 37.5Â±4.20 & 197.1Â±63.71 & 0.183Â±0.091 \\ DIHAN & 12.2Â±0.98 & 46.2Â±1.8 & 0.43Â±0.078 & 14.1Â±8.29 & 185.7Â±15.65 & 0.191Â±0.095 \\ DAQuereotype & 17.9Â±0.54 & 51.4Â±0.49 & 0.11Â±0.034 & 87.9Â±9.60 & 157.7Â±48.90 & 0.125Â±0.047 \\  CaPS & **11.0Â±0.00** & 42.0Â±0.00 & **0.50Â±0.000** & 37.2Â±5.04 & 178.9Â±45.58 & 0.230Â±0.072 \\ w/o Theorem 1 & 17.0Â±3.50 & 54.0Â±3.40 & 0.25Â±0.061 & 51.6Â±8.82 & 180.0Â±0.66Â±0.80 & 0.218Â±0.090 \\ w/o Parent Score & 12.0Â±0.00 & 45.0Â±0.00 & 0.44Â±40.000 & 34.8Â±3.37 & 188.0Â±457.58 & 0.222Â±0.083 \\   

Table 1: Results of real-world datasets, including three methods based on acyclicity constraint and five ordering-based methods. More baselines are given in Appendix C.4.

Figure 2: Results of SynER1 and SynER4 with different linear proportions, where linear proportion equal to 0.0 means all relations are nonlinear and 1.0 means all relations are linear.

experimental results with actual training time of larger-scale causal graph (\(d=20\) and \(d=50\)) and different samples size (\(n=1000\) and \(n=5000\)) can be found in Appendix C.5.

**Order divergence.** Reisach et.al.  cautioned that some synthetic datasets may be so simple that sorting with minimal variance can be successful. To demonstrate the efficacy of CaPS, a new metric called "order divergence"  was introduced for evaluation, along with a new baseline _sortnregress_ which orders nodes by increasing marginal variance. The results demonstrates that CaPS has a much better order divergence than _sortnregress_, indicating that variance is not a reliable indicator of the topological ordering in our synthetic datasets. CaPS consistently has the best or a competitive order divergence in different datesets. Details can be found in Appendix C.6.

**Beyond our assumptions.** We also explore the performance of CaPS under other settings of noise. The results show that CaPS can be effective in situations that go beyond our assumed conditions, which suggests that our approach has the potential to be applied in various other scenarios, and it is possible to consider loosening the assumptions in the future. Details can be found in Appendix C.7.

**Case visualization.** Fig. 4 shows the another advantage of CaPS. Compared to the second best baseline, CaPS performs better under all metrics while it provides more information on causal effects. The parent score captures most of the ground-truth edges and the estimated weights are similar to the actual values, indicating that the parent score accurately reflects the strength of causal effect.

## 6 Conclusion

This paper introduces CaPS that is capable of handling datasets with linear and nonlinear relations, which is a common occurrence in real-world applications. We propose a novel identification criterion for topological ordering for both types of relation, as well as a new metric, "parent score", to measure the strength of the average causal effect and used for edge removal and supplementation. Our solutions have been tested on synthetic data with varying linear and nonlinear relationship ratios and have been found to be more effective than existing order-based work and state-of-the-art baselines.

There remain two interesting directions to be explored in future work. (1) Since the experimental results have been encouraging in some cases beyond our assumption, we are striving to broaden the identifiability conditions to more relaxed conditions. (2) The new metric, "parent score", is likely to have more application scenarios. It is possible to apply as a plug-and-play information of causality.

Figure 4: Visualization on SynER1 dataset. Darker colors indicate stronger causal effects.

Figure 3: F1 score and training time of SynER1 with larger-scale causal graph.