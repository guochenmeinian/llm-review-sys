# Private Online Learning via Lazy Algorithms

Hilal Asi

Apple

hilal.asi94@gmail.com

&Tomer Koren

Tel Aviv University

tkoren@tauex.tau.ac.il

Daogao Liu

University of Washington

liudaogao@gmail.com

&Kunal Talwar

Apple

kunal@kunaltalwar.org

Part of this work was done while interning at Apple.

###### Abstract

We study the problem of private online learning, focusing on online prediction from experts (OPE) and online convex optimization (OCO). We propose a new transformation that translates lazy, low-switching online learning algorithms into private algorithms. We apply our transformation to differentially private OPE and OCO using existing lazy algorithms for these problems. The resulting algorithms attain regret bounds that significantly improve over prior art in the high privacy regime, where \( 1\), obtaining \(O(+T^{1/3}(d)/^{2/3})\) regret for DP-OPE and \(O(+T^{1/3}/^{2/3})\) regret for DP-OCO. We complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms.

## 1 Introduction

Online learning is a fundamental problem in machine learning, where an algorithm interacts with an oblivious adversary for \(T\) rounds. First, the oblivious adversary chooses \(T\) loss functions \(_{1},,_{T}:\) over a fixed decision set \(\). Then, at any round \(t\), the algorithm chooses a model \(x_{t}\), and the adversary reveals the loss function \(_{t}\). The algorithm suffers loss \(_{t}(x_{t})\), and its goal is to minimize its cumulative loss compared to the best model in hindsight, namely its _regret_:

\[_{T}=_{t=1}^{T}_{t}(x_{t})-_{x^{*}} _{t=1}^{T}_{t}(x^{*}).\]

In this work, we study two different _differentially private_ instances of this problem: differentially private online prediction from experts (DP-OPE) where the model \(x\) can be chosen from \(d\) experts (\(=[d]\)); and differentially private online convex optimization (DP-OCO) where the model belongs to a convex set \(^{d}\).

Both problems have been extensively studied recently  and an exciting new direction with promising results for this problem is that of designing private algorithms based on low-switching algorithms for online learning . The main idea in these works is that the privacy cost for privatizing a low-switching algorithm can be significantly smaller as these algorithms do not update their models too frequently, allowing them to allocate a larger privacy budget for each update. This has been initiated by , which used the shrinking dartboard algorithm to design new algorithms for DP-OPE, later revisited by  to design new algorithms for DP-OCO using a regularized follow-the-perturbed-leader approach, and more recently by  which used a lazy and regularized version of the multiplicative weights algorithm to obtain improved rates for DP-OCO.

While all of these results build on lazy-switching algorithms for designing private online algorithms, each one of them has a different method for achieving privacy and, to a greater extent, a different analysis. Moreover, it is not clear whether these transformations from lazy to private algorithms in prior work have fulfilled the full potential of lazy algorithms for private online learning and whether better algorithms are possible through this approach. Indeed, the regret obtained in prior work  is \(T^{1/3}/\) (omitting dependence on \(d\)) for DP-OPE, which implies that the normalized regret is \(1/T^{2/3}\): this is different than what exhibited in a majority of scenarios of private optimization, where the normalized error is usually a function of \(T\).

### Our contributions

Our main contribution in this work is a new transformation that converts lazy online learning algorithms into private algorithms with similar regret guarantees, resulting in new state-of-the-art rates for DP-OPE and DP-OCO. We provide a summary in Table 1 and Figure 1.

L2P: a transformation from lazy to private algorithms (Section 3).Our main contribution is a new transformation, we call L2P, that allows converting any lazy algorithm into a private one with only a slight cost in regret. This allows us to use a long line of work on lazy online learning  to design new algorithms for the private setting. Our transformation builds on two new techniques: first, we design a new switching rule that only depends on the loss at the current round, so as to minimize the privacy cost of each switching and mitigate the accumulation of privacy loss. Second, we rely on a simple, key observation that by grouping losses in a large batch, we can minimize the effect on the regret of lazy online learning algorithms. We introduce a new analysis for the regret of lazy online algorithms with a large batch size that improves over the existing analysis in ; this allows us to reduce the total number of "fake switches" needed to guarantees privacy, improving the final regret.

Faster rates for DP-OPE (Section 3.1).As a first application, we use our transformation in the DP-OPE problem on the multiplicative weights algorithm . This results in a new algorithm for DP-OPE that has regret \(+T^{1/3}(d)/^{2/3}\), improving over the best existing results for the high-dimensional regime in which the regret is \(+T^{1/3}(d)/\).2

Figure 1: Regret bounds for (a) DP-OCO with \(d=(T)\), (b) DP-OCO with \(d=T^{1/3}\) and (c) DP-OPE with \(d=T\). We denote the privacy parameter \(=T^{-}\) and regret \(T^{}\), and plot \(\) as a function of \(\) (ignoring logarithmic factors).

   & **Prior work** & **This work** \\ 
**DP-OPE** & \(+,T^{1/3} d\}}{}\) & \(+ d}{^{2/3}}\) \\ 
**DP-OCO** & \(\{}{},+ }{T^{1/3}}+}{^{3/4}}\}\) & \(+}{^{2/3}}\) \\  

Table 1: Regret for approximate \((,)\)-DP algorithms. For readability, we omit logarithmic factors that depend on \(T\) and \(1/\).

The improvement is particularly crucial in the high-privacy regime, where \( 1\): indeed, our regret shows that (for \(d=(T)\)) it is sufficient to set \( T^{-1/4}\) for matching the optimal non-private regret \(\), whereas previous results require a much larger \( T^{-1/6}\) to get privacy for free. This is also important in practice, when multiple applications of DP-OPE are necessary: using advanced composition, our result shows that we can solve \(K\) instances of DP-OPE with \(=1\) and still obtain the non-private regret of order \(\); in contrast, prior work only allows to solve \(K T^{1/3}\) instances while still attaining the non-private regret.

Faster rates for DP-OCO (Section 3.2).As another application, we use our transformation for DP-OCO with the regularized multiplicative weights algorithm of . We obtain a new algorithm for DP-OCO that has regret \(+T^{1/3}/^{2/3}\), improving over the best existing results that established regret \(+T^{1/3}/+T^{3/8}/^{3/4}\) or \(d^{1/4}/\) using DP-FTRL .

Lower bounds for low-switching private algorithms (Section 4).To understand the limitations of low-switching private algorithms, we prove a lower bound for the natural family of private algorithms with limited switching, showing that the upper bounds obtained via our reduction are nearly tight for this family of algorithms up to logarithmic factors. This shows that new techniques, beyond limited switching, are required in order to improve upon our upper bounds.

Related work.Our transformation and algorithms build on a long line of work in online learning with limited switching . As is evident from prior work in private online learning, the problems of lazy online learning and private online learning are tightly connected . In this problem, the algorithm wishes to minimize its regret while making at most \(S\) switches: the algorithm can update the model at most \(S\) times throughout the \(T\) rounds. Recent work has resolved the lazy OPE problem:  show a lower bound of \(+(T/S)(d)\) on the regret, which is achieved by several algorithms such as Follow-the-perturbed-leader  and the shrinking dartboard algorithm . For lazy OCO, however, optimal rates are yet to be known:  recently show that a lazy version of the regularized multiplicative weights algorithm obtains regret \(+(T/S)\), whereas the best lower bound is \(+T/S\).

## 2 Preliminaries

### Problem setup

We consider an interactive \(T\)-round game between an algorithm \(\) and an oblivious adversary \(\). Before the interaction, the adversary \(\) chooses \(T\) loss functions \(_{1},,_{T}=\{: \}\). Then, at each round \(t[T]\), the algorithm \(\), which observed \(_{1},,_{t-1}\) chooses \(x_{t}\), and then the loss function \(_{t}\) chosen by \(\) is revealed. The regret of the algorithm \(\) is defined below:

\[_{T}():=_{t=1}^{T}_{t}(x_{t})-_{x^{*} }_{t=1}^{T}_{t}(x^{*}).\]

We study online optimization under the constraint that the algorithm is differentially private. For an algorithm \(\) and a sequence \(=(_{1},,_{T})\) chosen by an oblivious adversary \(\), we let \(()(x_{1},,x_{T})\) denote the output of \(\) over the loss sequence \(\). We have the following definition of privacy against oblivious adversaries.3

**Definition 2.1** (Differential privacy).: A randomized algorithm \(\) is \((,)\)-differentially private against oblivious adversaries (\((,)\)-DP) if, for all neighboring sequencesand \(^{}=(_{1}^{},,_{T}^{})^{T}\) that differ in a single element, and for all events \(\) in the output space of \(\), we have

\[[()] e^{}[(^{})]+.\]

We focus on two important instances of differentially private online optimization:

1. [label=()]
2. **DP Online Convex Optimization (DP-OCO).** In this problem, the adversary picks loss functions \(_{OCO}\{: { is convex and }L\}\) where \(^{d}\) is a convex set with diameter \(D=()_{x,y}\|x-y\|\), and the algorithm chooses \(x_{t}\). The goal of the algorithm is to minimize regret while being \((,)\)-differentially private.
3. **DP Online Prediction from Experts (DP-OPE).** In this problem, the adversary picks loss functions \(_{OPE}=\{:[d]\}\) where \(=[d]\) is the set of \(d\) experts, and the algorithm chooses \(x_{t}[d]\). The goal of the algorithm is to minimize regret while being \((,)\)-differentially private.

### Tools from differential privacy

Our analysis crucially relies on the following divergence between two distributions.

**Definition 2.2** (\(\)-Approximate Max Divergence).: For two distributions \(\) and \(\), we define

\[D_{}^{}(\|)_{S(),( S)}.\]

We let \(D_{}^{}(,)\{D_{}^{}(\|),D_{ }^{}(\|)\}\).

We also use the notion of indistinguishability between two distributions.

**Definition 2.3**.: (\((,)\)-indistinguishability) Two distributions \(,\) are \((,)\)-indistinguishable, denoted \(_{(,)}\), if \(D_{}^{}(,)\).

Note that if an algorithm \(\) has \(()_{(,)}( ^{})\) for all neighboring datasets \(,^{}\) then \(\) is \((,)\)-differentially private. We direct readers to Appendix A for additional background information and detailed preliminaries.

## 3 L2P: From Lazy to Private Algorithms for Online Learning

This section presents our \(\) transformation, which turns lazy online learning algorithms into private ones. The transformation has an input algorithm \(\) with measure \(_{t}\) at round \(t\) and samples \(x_{t}\) from the normalized measure \(_{t}\), which satisfies the following condition:

**Assumption 3.1**.: The online algorithm \(\) has at time \(t\) a measure \(_{t}\) that is a function of \(_{1},,_{t-1}\) (and density function \(_{t}\)) such that for some \(_{0} 1\) and \(0< 1/10\) that are data-independent, we have

* \(D_{}^{_{0}}(_{t+1},_{t})\),
* \(_{t+1}(x)/_{t}(x)=(_{t},x)\) for all \(x\) where \(\) is a data-independent function.

While algorithms satisfying Assumption 3.1 need not be lazy, this assumption is satisfied by most existing lazy online learning algorithms such as the shrinking dartboard (Section 3.1) and lazy regularized multiplicative weights (Section 3.2). Moreover, any algorithm that satisfies this assumption can be made lazy via our reduction.

Technique Overview:Suppose the neighboring datasets differ from the \(s_{0}\)-th loss function. The high-level intuition behind our framework is that our algorithm only loses the privacy budget when it makes a switch (draws a fresh sample) whenever \(t>s_{0}\). Hence, in the framework, we try to make the algorithm make as few switches as possible. This modification can lead to additional regret compared to lazy online learning algorithms, and we need to balance the privacy-regret trade-off. The family of low-switching algorithms is ideal for privatization because its built-in low-switching property can achieve a better trade-off.

Our starting point is the ideas in  to privatize low-switching algorithms, which use correlated sampling to argue that a sample from \(x_{t-1}_{t-1}\) is likely a good sample from \(_{t}\) and therefore switching at round \(t\) is often not necessary. In particular, at round \(t\), these algorithms sample a Bernoulli random variable \(S_{t}(c_{t}(x_{t-1})/_{t-1}(x _{t-1}))\) for some constant \(c\) and use the same model \(x_{t}=x_{t-1}\) if \(S_{t}=1\), and otherwise sample new model \(x_{t}_{t}\) if \(S_{t}=0\) (which happens with small probability). This guarantees that the marginal probability of the lazy iterates remains the same as the original iterates. Finally, to preserve the privacy of the switching decisions, existing algorithms add a fake switching probability \(p\) where the algorithm switches independently of the input. To summarize, _existing_ low-switching private algorithms work roughly as follows:

\[&\\ &-(C_{t}(x_{t-1})/ _{t-1}(x_{t-1}))$ and $S^{}_{t}(1-p)$}\\ &-_{t}$ if $S_{t}=0$ or $S^{}_{t}=0$}\\ &-=x_{t-1}$}\]

This sketch is the starting point of our transformation, and we will introduce two new components to improve performance. The first component aims to avoid the accumulation of privacy cost for switching in the current approaches where each user can affect the switching probability for all subsequent rounds: this happens since \(_{t}(x_{t-1})/_{t-1}(x_{t-1})\) is usually a function of the whole history \(_{1},,_{t}\), and hence the existing low-switching private algorithms lose the privacy budget even it does not make real switches. To address this, we deploy a new correlated sampling strategy in \(\) where the loss \(_{t}\) at time \(t\) affects the switching probability only at time \(t\), hence paying a privacy cost for switching only in a single round. To this end, we construct a parallel sequence of models \(\{y_{t}\}_{t[T]}\) (independent of \(x_{t}\)) that is used for normalizing the ratio \(_{t}(x_{t-1})/_{t-1}(x_{t-1})\) to become independent of the history. In particular, at round \(t\), we switch with probability proportional to

\[_{t}(x_{t-1})}{_{t-1}(x_{t-1})}_{t-1}(y_{t-1})}{_{t}(y_{t-1})}.\]

The main observation here is that \(_{t}(x_{t-1})}{_{t-1}(x_{t-1})}_{t-1}(y_{t-1})}{_{t}(y_{t-1})}=(x_{ t-1})}{_{t-1}(x_{t-1})}_{t-1}(y_{t-1})}{ _{t}(y_{t-1})}\) and this ratio is a function of \(_{t}\) our input online learning algorithms which satisfies Assumption 3.1. This will, therefore, improve the privacy guarantee of the final algorithm.

The second main observation in \(\) is that having a large batch size (batching rounds together) does not significantly affect the regret of lazy online algorithms compared to non-lazy algorithms but can further reduce the times to make switches and save the privacy budget. Our main novelty is a new analysis of the effect of batching on the regret of lazy algorithms (Proposition 3.3), which states that running a lazy online algorithm with a batch size of \(B\) would have an additive error of \(TB^{2}^{2}\) to the regret where \(\) is a measure of distance between \(_{t}\) and \(_{t-1}\). This significantly improves over existing analysis by [1, Theorem 2] which shows that batching can add an additive term of \(B/\) to the regret.

Having reviewed our main techniques, we proceed to present the full details of our \(\) transformation in Algorithm 1, denoting \(_{s}=_{(s-1)B+1}\) where \(B\) is the batch size.

The regret of our transformation depends on the regret of its input algorithm. For the measure \(\{_{t}\}_{t=1}^{T}\), we denote its regret

\[_{T}(\{_{t}\}_{t=1}^{T}):=_{t=1}^{T}}_{x_ {t}_{t}}[_{t}(x_{t})]-_{x} _{t=1}^{T}_{t}(x).\]

The following theorem summarizes the main guarantees of Algorithm 1.

**Theorem 3.2**.: _Let \(p(0,1)\) and \(B\). Assuming Assumption 3.1, \(Tp/B 1\), and for any \(_{1}>0\) such that \( B(1/_{1})/p 1\), our transformation \(\) is \((,)\)-DP with_

\[=++p(1/_{1})}{2B}+ p^{2}(1/_{1})/B},\]\[=2T(2/+(1/_{1})/p)eB_{0}+2T_{1},\]

_and has regret_

\[_{T}_{T}(\{_{t}\}_{t=1}^{T})+O(TB^{2}^{2 }+T^{2}(})}{}).\]

We begin by proving the utility guarantees of our transformation. It will follow directly from the following proposition, which bounds the regret of running L2P over a lazy online learning algorithm.

**Proposition 3.3** (Regret of Batched Lazy Algorithm).: _Let \(\) be an online learning algorithm that satisfies Assumption 3.1. Let \( B(1/_{1})/p 1\), and \(_{1},<1/2\). Then running \(\) with the input algorithm \(\) has regret_

\[_{T}_{T}(\{_{t}\}_{t=1}^{T})+O(TB^{2}^ {2}+T^{2}(})}{}).\]

To prove Proposition 3.3, we first show that we can instead analyze the utility of a simpler algorithm that samples from \(_{s}\) at each round. This is due to the following lemma, which shows that \(\|_{s}-_{s}\|_{TV}\) is small where \(_{s}\) is the marginal distribution of \(x_{s}\) in Algorithm 1.

**Lemma 3.4**.: _Let \(_{s}\) be the marginal distribution of \(x_{s}\) in Algorithm 1. When \( B(1/_{1})/p 1\), we have \(\|_{s}-_{s}\|_{TV} 3(s-1)(2e+(1/_{1})/p) B_{0}\)._

We also require the following lemma which allows to build a coupling over multiple variables, such that the variables are as close as possible. This will be used to construct a coupling between the lazy algorithm and the L2P algorithm that runs it.

**Lemma 3.5** ().: _Given a collection \(S\) of random variables, all absolutely continuous w.r.t. a common \(\)-finite measure. Then, there exists a coupling \(\), such that for any variables \(X,Y S\), we have \([X Y]}{1+\|X-Y\|_{TV}}\)._

We are now ready to prove Proposition 3.3

Proof.: Let \(_{T}^{}\) denote the regret when the marginal distribution of \(x_{t}\) is \(_{t}\) instead of \(_{t}\) induced in the Algorithm. Since each loss function is bounded,

\[_{T}_{T}^{}+B_{s[T/B]}\|_{s}-_{s}\|_{TV}.\]By Lemma 3.4, we have

\[_{T} _{T}^{}+B_{s[T/B]}3(s-1)(2/+(1 /_{1})/p)eB_{0}\] \[_{T}^{}+8T^{2}_{0}(1/_{1})/.\]

Thus, it now suffices to upper bound \(_{T}^{}\).

Due to the preconditions that \(D_{}^{_{0}}(_{i+1},_{i})\) and \(_{0}\), we know \(\|_{i+1}-_{i}\|_{TV} 2\). Recall that we assume \(x_{s}_{s}\). Suppose \(z_{i}\) is the action taken by the input lazy algorithm \(\) for \(i[T]\) and the marginal distribution of \(z_{i}\) is \(_{i}\). By Lemma 3.5, we can construct a coupling \(_{s}\) between \(x_{s}\) and \((z_{(s-1)B+1},,z_{sB})\), such that

\[_{(x_{s},)_{s}}[ i[(s-1)B+1,sB],z_{i} x _{s}] B.\]

Letting \(I_{s}=( i[(s-1)B+1,sB],z_{i} x_{s})\), we have

\[*{}_{x_{s}_{s}}_{i=(s-1)B+1}^{ sB}_{i}(x_{s}) =*{}_{(x_{s},)_{s}} _{i=(s-1)B+1}^{sB}_{i}(x_{s})\] \[=*{}_{x_{s},_{s}}(1 -I_{s})_{i=(s-1)B+1}^{sB}_{i}(z_{i})\] \[+*{}_{x_{s},_ {s}}I_{s}_{i=(s-1)B+1}^{sB}_{i}(x_{s})\] \[*{}_{x_{s},_{s} }(1-I_{s})_{i=(s-1)B+1}^{sB}_{i}(z_{i})\] \[+*{}_{x_{s},_ {s}}I_{s}_{i=(s-1)B+1}^{sB}(_{i}(z_{i})+O(B))\] \[*{}_{z_{i}_{i}} _{i=(s-1)B+1}^{sB}_{i}(z_{i})+O(B B^{2}).\]

Hence we get \(_{T}^{}_{T}(\{_{t}\}_{t=1}^{T})+ O(B^{3}^{2})\), which completes the proof. 

Now we turn to prove the privacy of \(\). We begin with the following lemma, which provides the privacy guarantees of sampling a new model \(x_{t}\) from the distribution \(_{t}\). We defer the proof to Appendix B.

**Lemma 3.6**.: _Let \(\{_{t}\}_{t=0}^{T}\) satisfy Assumption 3.1 where \( 1/10\). Then for any neighboring sequences \(\) and \(^{}\) with corresponding \(\{_{t}\}_{t=0}^{T}\) and \(\{_{t}^{}\}_{t=0}^{T}\) that differ one loss function, we have_

\[D_{}^{4_{0}}(_{t},_{t}^{}) 2.\]

We use correlated sampling in the algorithm rather than sampling from \(x_{t}\) directly. To this end, we need the following lemma, which provides upper and lower bounds on the ratio used for correlated sampling.

**Lemma 3.7**.: _For any \(s[T/B]\), if \( B(1/_{1})/p 1\), then with probability at least \(1-(2/+(1/_{1})/p) eB_{0}-_{1}\),_

\[(x_{s})}{_{s}(x_{s})}(y_{s})}{_{s+1}(y_ {s})}[e^{-2B},e^{2B}].\]

The privacy proof will build on the previous two lemmas to control the privacy cost of updating the model and the cost of the switching time. We defer the proof to Appendix B.

One remaining issue is we need to conditional on the high probability events in Lemma 3.7 for the privacy guarantee and can not directly apply Advanced Composition (Lemma A.3). Now, we modify the Advanced Composition for our usage. In the classic \(k\)-fold adaptive composition experiment, the adversary, after getting the first \(i-1\) answers \(Y_{1},,Y_{i-1}\) (denoted by \(Y_{[i-1]}\) for simplicity), can output two datasets \(D_{i}^{0}\) and \(D_{i}^{1}\), a query \(q_{i}\), and receives the answer \(Y_{i}_{i}(D_{i}^{b},q_{i})\) for the secret bit \(b\{0,1\}\). If each \(_{i}\) is \((_{i},_{i})\)-DP, then the joint distributions over the answers \(Y_{[k]}\) satisfy the advanced composition theorem.

In our case, however, we know there exists a subset \(G_{i-1}(D_{[i-1]}^{b})\), such that with probability at least \(1-_{i}\), \(Y_{[i-1]} G_{i-1}(D_{[i-1]}^{b})\). Conditional on \(Y_{[i-1]}_{b\{0,1\}}G_{i-1}(D_{[i-1]}^{b})\),

\[_{i}(D_{i}^{0},q_{i} Y_{[i-1]}_{b\{0,1\}}G_{i-1}(D_{ [i-1]}^{b}))_{(_{i},_{i})}_{i}(D_{i}^{1},q _{i} Y_{[i-1]}_{b\{0,1\}}G_{i-1}(D_{[i-1]}^{b}))\] (1)

Then we have the following lemma:

**Lemma 3.8**.: _Given the \(k\) mechanisms satisfying the Condition (1), then the class of mechanisms satisfy \((_{},1-(1-)_{t[k]}(1-_{t}))+2_{t[k]}_{t}\)-DP under \(k\)-fold adaptive composition, with \(_{}\) defined in Equation (4)._

Proof.: Without losing generality, suppose we know the adversary and how they generate the databases and queries. We can construct a series of mechanisms \(_{i}^{}\), such that \(_{i}^{}\) draws \(Y_{i}\) from \(_{i}(D_{i}^{b},q_{i})\), and outputs \(Y_{i}\) if \(Y_{i}_{b\{0,1\}}G_{i-1}(D_{[i-1]}^{b})\), and outputs \(\) otherwise. Let \((Y_{1,b}^{},,Y_{k,b}^{})\) be the outputs of \(_{i}^{}\) with secret bit \(b\), and we know the TV distance between \((Y_{1,b}^{},,Y_{k,b}^{})\) and \((Y_{1,b},,Y_{k,b})\) is at most \(_{t[k]}_{t}\) for any \(b\{0,1\}\). Moreover, we know

\[(Y_{1,0}^{},,Y_{k,0}^{})_{_{ {g}},1-(1-)_{t[k]}(1-_{t}))}(Y_{1,1}^{ },,Y_{k,1}^{})\]

by the advanced composition. The basic composition finishes the proof. 

### Application to DP-OPE

This section discusses the first application of our transformation to differentially private online prediction from experts (DP-OPE). Towards this end, we apply our transformation over the multiplicative weights algorithms , which can be made lazy as done in the shrinking dartboard algorithm . It has the following measure at round \(t\)

\[_{t}^{}(x)=e^{-_{i=1}^{t-1}_{i}(x)}.\] (2)

The following proposition shows that this measure satisfies the desired properties required by our transformation. We let \(_{t}^{}\) denote the density corresponding to \(_{t}^{}\).

**Lemma 3.9**.: _Assume \(_{1},,_{T}\) where \(_{t}:[d]\). Then we have that_

1. \(D_{}^{_{0}}(_{t+1}^{},_{t}^ {})\) _with_ \(_{0}=0\)_._
2. \(^{}(x)}{_{t}^{}(x)}=e^{-_{t}(x)}\) _for all_ \(x[d]\)_._

Proof.: The first item follows from the guarantees of the exponential mechanism as \(_{t}(x)\) for all \(x[d]\). The second item follows immediately from the definition of \(^{}\). 

Having proved our desired properties, our transformation now gives the following theorem.

**Theorem 3.10** (DP-OPE).: _Let \(_{1},,_{T}\) where \(_{t}:[d]\). Setting \(B=1/\) and \(=(_{0},)^{2/3}/T^{1/3}\) where \(_{0}=T^{-1/4}^{3/4}d\), the \(\) transformation (Algorithm 1) applied with the measure \(\{_{t}^{}\}_{t=1}^{T}\) is \((,)\)-DP and has regret_

\[_{T}=O(+ d}{^{2/3}} ).\]Proof.: First, based on theorem 3.2, note that the setting of \(B=1/\) and \((_{0},)^{2/3}/T^{1/3}\) where \(_{0}=T^{-1/4}^{3/4}d\) guarantee the algorithm is \((,)\)-DP.

To upper bound the regret, we use existing guarantees of the multiplicative weights algorithm , combined with Theorem 3.2 to get that the regret is

\[_{T}  O( T++TB^{2}^{2})\] \[ O( T++}{ ^{2}})\] \[ O((T_{0})^{2/3}+(d)}{ ^{2/3}}+}{^{2/3}})\] \[ O(+(d)}{^{2/3 }}),\]

where the second inequality follows by setting \(B=1/\), and the third inequality follows by setting \((_{0},)^{2/3}/T^{1/3}\), and the last inequality follows since \(_{0}=T^{-1/4}^{3/4}d\). 

### Application to DP-OCO

In this section, we use our transformation for differentially private online convex optimization (DP-OCO) using the regularized multiplicative weights algorithm , which has the following measure

\[_{t}^{}(x)=e^{-(_{i-1}^{t-1}_{i}(x)+ \|x\|_{2}^{2})}.\] (3)

Letting \(^{}\) denote the corresponding density function, we have the following properties.

**Lemma 3.11**.: _Assume \(_{1},,_{T}:\) be convex and \(L\)-Lipschitz functions. Then we have that_

1. \(D_{}^{_{0}}(_{t+1}^{},_{t}^ {})\) _where_ \(=}{}+(2/_{0})}{ }}\)_._
2. \(^{}(x)}{_{t}^{}(x)}=e^{-_{1}(x )}\) _for all_ \(x\)_._

Proof.: The first item follows from Lemma 3.5 in . The second item follows immediately from the definition of \(_{t}^{}\). 

Combining these properties with our transformation, we get the following result.

**Theorem 3.12** (D-Oco).: _Let \(_{1},,_{T}:\) be convex and \(L\)-Lipschitz functions. Setting \(B=\), \(=\{,}{}\}\), \(=^{2}/20L^{2}\), \(=}{T^{1/3}(T/)}\) and \(p=/\), the \(L2P\) transformation (Algorithm 1) applied with the measure \(\{_{t}^{}\}_{t=1}^{T}\) is \((,)\)-DP and has regret_

\[_{T}=LD O(+(T/ )}{^{2/3}}).\]

Proof.: First, based on Theorem 3.2, note that there are three constraints to make the algorithm private:

\[/p/2,/2, B(1/)/p 1.\]

Setting of \(B=\), \(=\{,}{}\}\), \(=^{2}/20L^{2}\), \(=}{T^{1/3}(T/)}\) and \(p=/\) guarantees the algorithm is \((,)\)-DP.

For utility, we use theorem 3.2 with the existing regret bounds for the regularized multiplicative weights algorithm (Theorem 4.1 in ) to get that the algorithm has regret

\[_{T} O( D^{2}+T}{}++LDTB^{2}^{2})\]\[ O(LD+ D^{2}+d T}{^{2 }}+LDTB^{2}^{2})\] \[ LD O(+(T/ )}{^{2/3}}).\]

## 4 Lower bound for low-switching private algorithms

In this section, we prove a lower bound for DP-OPE for a natural family of private low-switching algorithms that contains most of the existing low-switching private algorithms such as our algorithms and the ones in . Our lower bound matches our upper bounds for DP-OPE and suggests that new techniques beyond limited switching are required in order to obtain faster rates.

For our lower bounds, we will assume that the algorithm satisfies the following condition:

**Condition 4.1**.: _(Limited switching algorithms) The online algorithm \(\) works as follows: at each round \(t\), \(\) is allowed to either set \(x_{t+1}=x_{t}\) or sample \(x_{t+1}_{t+1}\) where \(_{t+1}\) is a function of \(_{1},,_{t}\) and is supported over \(\). The algorithm releases the resampling rounds \(\{t_{1},,t_{S}\}\) and models \(\{x_{t_{1}},,x_{t_{S}}\}\)._

Our lower bound will hold for algorithms that satisfy concentrated differential privacy. We use this notion as it allows to get tight characterization of the composition of private algorithms and in most settings have similar rates to approximate differential privacy. We can also prove a tight lower bound for pure differential privacy using the same techniques. We have the following lower bound for concentrated DP. We defer the proof to Appendix C.

**Theorem 4.2**.: _Let \(T 1\) and \( 100^{3/2}(dT)/T\). If an algorithm \(\) satisfies Condition 4.1 and is \(^{2}\)-CDP, then there exists an oblivious adversary that chooses \(_{1},,_{T}:[d]\) such that the regret is lower bounded by_

\[_{T}(+}{^{2/3}} ).\]

Finally, we note that this lower bound only holds for switching-based algorithms: indeed, the binary-tree-based algorithm of  obtains regret \((d)/\) which is better in the low-dimensional regime. This motivates the search for new strategies beyond limited switching for the high-dimensional regime.

## 5 Conclusion

In this paper, we proposed a new transformation that allows the conversion of lazy online learning algorithms into private algorithms and demonstrates two applications (DP-OPE and DP-OCO) where this transformation offers significant improvements over prior work. Moreover, for DP-OPE, we show a lower bound for natural low-switching-based private algorithms, which shows that new techniques are required for low-switching algorithms to improve our transformation's regret. This begs the question of whether the same lower bound holds for all algorithms or whether a different strategy that breaks the low-switching lower bound exists. As for DP-OCO, it is interesting to see whether better upper or lower bounds can be obtained. The current normalized regret, omitting logarithmic terms, is proportional to \(/( T)^{2/3}\). This is different than most applications in private optimization where the normalized error is usually a function of \(/( T)\). Hence, it is natural to conjecture that the normalized regret can be improved to \(d^{1/3}/( T)^{2/3}\).