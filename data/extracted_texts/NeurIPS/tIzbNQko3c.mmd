# Pre-Training Protein Encoder via Siamese

Sequence-Structure Diffusion Trajectory Prediction

 Zuobai Zhang\({}^{1,2}\)*, Minghao Xu\({}^{1,2}\)*, Aurelie Lozano\({}^{3}\),

**Vijil Chenthamarakshan\({}^{3}\)**, **Payel Das\({}^{}\)**, **Jian Tang\({}^{1,4,5}\)\({}^{}\)**

equal contribution \({}^{}\)corresponding author

\({}^{1}\)Mila - Quebec AI Institute, \({}^{2}\)Universite de Montreal, \({}^{3}\)IBM Research,

\({}^{4}\)HEC Montreal, \({}^{5}\)CIFAR AI Chair

{zuobai.zhang, minghao.xu}@mila.quebec,

{ecvijil,aclozano,daspa}@us.ibm.com,jian.tang@hec.ca

###### Abstract

Self-supervised pre-training methods on proteins have recently gained attention, with most approaches focusing on either protein sequences or structures, neglecting the exploration of their joint distribution, which is crucial for a comprehensive understanding of protein functions by integrating co-evolutionary information and structural characteristics. In this work, inspired by the success of denoising diffusion models in generative tasks, we propose the **DiffPreT** approach to pre-train a protein encoder by sequence-structure joint diffusion modeling. DiffPreT guides the encoder to recover the native protein sequences and structures from the perturbed ones along the joint diffusion trajectory, which acquires the joint distribution of sequences and structures. Considering the essential protein conformational variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory Prediction (**SiamDiff**) to capture the correlation between different conformers of a protein. SiamDiff attains this goal by maximizing the mutual information between representations of diffusion trajectories of structurally-correlated conformers. We study the effectiveness of DiffPreT and SiamDiff on both atom- and residue-level structure-based protein understanding tasks. Experimental results show that the performance of DiffPreT is consistently competitive on all tasks, and SiamDiff achieves new state-of-the-art performance, considering the mean ranks on all tasks. Code will be released upon acceptance.

## 1 Introduction

Machine learning-based methods have made remarkable strides in predicting protein structures [44; 5; 50] and functionality [55; 24]. Among them, self-supervised (unsupervised) pre-training approaches [20; 61; 87] have been successful in learning effective protein representations from available protein sequences or from their experimental/predicted structures. These pre-training approaches are based on the rationale that modeling the input distribution of proteins provides favorable initialization of model parameters and serves as effective regularization for downstream tasks . Previous methods have primarily focused on modeling the marginal distribution of either protein sequences to acquire co-evolutionary information [20; 61], or protein structures to capture essential characteristics for tasks such as function prediction and fold classification [87; 33]. Nevertheless, both these forms of information hold significance in revealing the underlying functions of proteins and offer complementary perspectives that are still not extensively explored. To address this gap, a more promising approach for pre-training could involve modeling the joint distribution of protein sequences and structures, surpassing the limitations of unimodal pre-training methods.

To model this joint distribution, denoising diffusion models [37; 65] have recently emerged as one of the most effective methods due to their simple training objective and high sampling quality and diversity [2; 54; 40; 74]. However, the application of diffusion models has predominantly been explored in the context of generative tasks, rather than within pre-training and fine-tuning frameworks that aim to learn effective representations for downstream tasks. In this work, we present a novel approach that adapts denoising diffusion models to pre-train structure-informed protein encoders1. Our proposed approach, called **DiffPreT**, gradually adds noise to both protein sequence and structure to transform them towards random distribution, and then denoises the corrupted protein structure and sequence using a noise prediction network parameterized with the output of the protein encoder. This approach enables the encoder to learn informative representations that capture the inter-atomic interactions within the protein structure, the residue type dependencies along the protein sequence, and the joint effect of sequence and structure variations.

In spite of these advantages, DiffPreT ignores the fact that any protein structure exists as a population of interconverting conformers, and elucidating this conformational heterogeneity is essential for predicting protein function and ligand binding . In both DiffPreT and previous studies, no explicit constraints are added to acquire the structural correlation between different conformers of a specific protein or between structural homologs, which prohibits capturing the conformational energy landscape of a protein . Therefore, to consider the physics underlying the conformational change, we propose Siamese Diffusion Trajectory Prediction (**SiamDiff**) to augment the DiffPreT by maximizing the mutual information between representations of diffusion trajectories of structurally-correlated conformers (_i.e._, siamese diffusion trajectories). We first adopt a torsional perturbation scheme on the side chain to generate randomly simulated conformers . Then, for each protein, we generate diffusion trajectories for a pair of its conformers. We theoretically prove that the problem can be transformed to the mutual prediction of the trajectories using representations from their counterparts. In this way, the model can keep the advantages of DiffPreT and inject conformer-related information into representations as well.

Both DiffPreT and SiamDiff can be flexibly applied to atom-level and residue-level structures to pre-train protein representations. To thoroughly assess their capabilities, we conduct extensive evaluations of the pre-trained models on a wide range of downstream protein understanding tasks. These tasks encompass protein function annotation, protein-protein interaction prediction, mutational effect prediction, residue structural contributions, and protein structure ranking. In comparison to existing pre-training methods that typically excel in only a subset of the considered tasks, DiffPreT consistently delivers competitive performance across all tasks and at both the atomic and residue-level resolutions. Moreover, SiamDiff further enhances model performance, surpassing previous state-of-the-art results in terms of mean ranks across all evaluated tasks.

## 2 DiffPreT: Diffusion Models for Pre-Training

Recently, there have been promising progress on applying denoising diffusion models for protein structure-sequence co-design [54; 2]. The effectiveness of the _joint diffusion model_ on modeling the distribution of proteins suggests that the process may reflect physical and chemical principles underlying protein formation [3; 18], which could be beneficial for learning informative representations. Based on this intuition, in this section, we explore the application of joint diffusion models on pre-training protein encoders in a pre-training and fine-tuning framework.

### Preliminary

**Notation.** A protein with \(n_{r}\) residues (amino acids) and \(n_{a}\) atoms can be represented as a sequence-structure tuple \(=(,)\). We use \(=[s_{1},s_{2},,s_{n_{a}}]\) to denote its sequence with \(s_{i}\) as the type of the \(i\)-th residue, while \(=[_{1},_{2}...,_{n_{a}}]^{n_{a}  3}\) denotes its structure with \(_{i}\) as the Cartesian coordinates of the \(i\)-th atom. To model the structure, we construct a graph for each protein with edges connecting atoms whose Euclidean distance below a certain threshold.

**Equivariance.**_Equivariance_ is ubiquitous in machine learning for modeling the symmetry in physical systems [67; 75] and is shown to be critical for successful design and better generalization of 3Dnetworks . Formally, a function \(:\) is equivariant _w.r.t._ a group \(G\) if \(_{}(x)=_{}(x)\), where \(_{}\) and \(_{}\) are transformations corresponding to an element \(g G\) acting on the space \(\) and \(\), respectively. The function is invariant _w.r.t_\(G\) if the transformations \(_{}\) is identity. In this paper, we consider SE(3) group, _i.e._, rotations and translations in 3D space.

**Problem Definition.** Given a set of unlabeled proteins \(=\{_{1},_{2},...\}\), our goal is to train a protein encoder \((,)\) to extract informative \(d\)-dimensional residue representations \(^{n_{r} d}\) and atom representations \(^{n_{a} d}\) that are SE(3)-invariant _w.r.t._ protein structures \(\).

### Diffusion Models on Proteins

Diffusion models are a class of deep generative models with latent variables encoded by a _forward diffusion process_ and decoded by _a reverse generative process_. We use \(^{0}\) to denote the ground-truth protein and \(^{t}\) for \(t=1,,T\) to be the latent variables over \(T\) diffusion steps. Modeling the protein as an evolving thermodynamic system, the forward process gradually injects small noise to the data \(^{0}\) until reaching a random noise distribution at time \(T\). The reverse process learns to denoise the latent variable towards the data distribution. Both processes are defined as Markov chains:

\[q(^{1:T}|^{0})=_{t=1}^{T}q(^{t}| ^{t-1}),\ \ p_{}(^{0:T-1}|^{T})=_{t=1}^{T}p_{ }(^{t-1}|^{t}),\] (1)

where \(q(^{t}|^{t-1})\) defines the forward process at step \(t\) and \(p_{}(^{t-1}|^{t})\) with learnable parameters \(\) defines the reverse process at step \(t\). We decompose the forward process into diffusion on protein structures and sequences, respectively. The decomposition is represented as follows:

\[q(^{t}|^{t-1})=q(^{t}|^{t-1}) q (^{t}|^{t-1}),\ \ p_{}(^{t-1}|^{t})=p_{}(^{t-1}| ^{t}) p_{}(^{t-1}|^{t}),\] (2)

where the reverse processes use representations \(^{t}\) and \(^{t}\) learned by the protein encoder \(_{}(^{t},^{t})\).

**Forward diffusion process \(q(^{t}|^{t-1})\).** For diffusion on protein structures, we introduce random Gaussian noises to the 3D coordinates of the structure. For diffusion on sequences, we utilize a Markov chain approach with an absorbing state [MASK], where each residue either remains the same or transitions to [MASK] with a certain probability at each time step . Specifically, we have:

\[q(^{t}|^{t-1})=(^{t};\,}^{t-1},_{t}I),\ \ q(^{t}|^{t-1})=(^{t-1},_{t}),\] (3)

Here, \(_{1},...,_{T}\) and \(_{1},...,_{T}\) are a series of fixed variances and masking ratios, respectively. random\(\_\)mask\((^{t-1},_{t})\) denotes the random masking operation, where each residue in \(^{t-1}\) is masked with a probability of \(_{t}\) at time step \(t\).

**Reverse process on structures \(p_{}(^{t-1}|^{t})\).** The reverse process on structures is parameterized as a Gaussian with a learnable mean \(_{}(^{t},t)\) and user-defined variance \(_{t}\). Given the availability of

Figure 1: A pre-training and fine-tuning framework for DiffPreT. During pre-training, diffusion processes are applied to both protein structures and sequences, with * indicating masked residues. Noise prediction networks, parameterized with an encoder \((,)\), are employed to restore the original states. The learned encoder \((,)\) is subsequently fine-tuned on downstream tasks.

\(^{t}\) as an input, we reparameterize the mean \(_{}(^{t},t)\) following Ho et al. :

\[p_{}(^{t-1}|^{t})=(^{t-1};_ {}(^{t},t),_{t}^{2}I),\ \ _{}(^{t},t)=}}( ^{t}-}{_{t}}}_{}( ^{t},t)),\] (4)

where \(_{t}=1-_{t}\), \(_{t}=_{s=1}^{t}_{s}\) and the network \(_{}()\) learns to decorrupt the data and should be translation-invariant and rotation-equivariant _w.r.t._ the structure \(^{t}\).

To define our noise prediction network, we utilize the atom representations \(^{t}\) (which is guaranteed to be SE(3)-invariant _w.r.t._\(^{t}\) by the encoder) and atom coordinates \(^{t}\) (which is SE(3)-equivariant _w.r.t._\(^{t}\)). We build an equivariant output based on normalized directional vectors between adjacent atom pairs. Each edge \((i,j)\) is encoded by its length \(\|^{t}_{i}-^{t}_{j}\|_{2}\) and the representations of two end nodes \(^{t}_{i}\), \(^{t}_{j}\), and the encoded score \(m_{i,j}\) will be used for aggregating directional vectors. Specifically,

\[[_{}(^{t},t)]_{i}=_{j^{t}(i)}m_{i, j}^{t}_{i}-^{t}_{j}}{\|^{t}_{i}-^{t}_{j} \|_{2}},\ \ \ m_{i,j}=(^{t}_{i},^{t}_{j},(\| ^{t}_{i}-^{t}_{j}\|_{2})),\] (5)

where \(^{t}(i)\) denotes the neighbors of the atom \(i\) in the corresponding graph of \(^{t}\). Note that \(_{}(^{t},t)\) achieves the equivariance requirement, as \(m_{i,j}\) is SE(3)-invariant _w.r.t._\(^{t}\) while \(^{t}_{i}-^{t}_{j}\) is translation-invariant and rotation-equivariant _w.r.t._\(^{t}\).

**Reverse process on sequences \(p_{}(^{t-1}|^{t})\).** For the reverse process \(p_{}(^{t-1}|^{t})\), we adopt the parameterization proposed in . The diffusion trajectory is characterized by the probability \(q(^{t-1}|^{t},}^{0})\), and we employ a network \(\) to predict the probability of \(^{0}\):

\[p_{}(^{t-1}|^{t})_{^{0}}q( ^{t-1}|^{t},}^{0})_{ }(}^{0}|^{t}),\] (6)

We define the predictor \(_{}\) with residue representations \(^{t}\). For each masked residue \(i\) in \(^{t}\), we feed its representation \(^{t}_{i}\) to an MLP and predict the type of the corresponding residue type \(s^{0}_{i}\) in \(^{0}\):

\[_{}(}^{0}|^{t})=_{i}_{}(^{0}_{i}|^{t})=_{i}(^{0}_{i}|(^{t}_{i})),\] (7)

where the softmax function is applied over all residue types.

### Pre-Training Objective

Now we derive the pre-training objective of DiffPreT by optimizing the diffusion model above with the ELBO loss :

\[[_{t=1}^{T}D_{}(q( ^{t-1}|^{t},^{0})||p_{}(^{ t-1}|^{t}))].\] (8)

Under the assumptions in (2), it can be shown that the objective can be decomposed into a structure loss \(^{()}\) and a sequence loss \(^{()}\) (see proof in App. C.2):

\[^{()} [_{t=1}^{T}D_{}(q(^{ t-1}|^{t},^{0})||p_{}(^{t-1}|^{t}) )],\] (9) \[^{()} [_{t=1}^{T}D_{}(q(^{ t-1}|^{t},^{0})||p_{}(^{t-1}|^{t}) )].\]

Both loss functions can be simplified as follows.

**Structure loss \(^{()}\).** It has been shown in Ho et al.  that the loss function can be simplified under our parameterization by calculating KL divergence between Gaussians as weighted L2 distances between means \(_{}\) and \(\) (see details in App. C.3):

\[^{()}=_{t=1}^{T}_{t}_{ (0,I)}[\|-_{}(^{t},t)\|_{2}^{ 2}],\] (10)

where the coefficients \(_{t}\) are determined by the variances \(_{1},...,_{t}\). In practice, we follow Ho et al.  to set all weights \(_{t}=1\) for the simplified loss \(^{()}_{}\).

Since \(_{}\) is designed to be rotation-equivariant _w.r.t._\(^{t}\), to make the loss function invariant _w.r.t._\(^{t}\), the supervision \(\) is also supposed to achieve such equivariance. Therefore, we adopt the chain-rule approach proposed in Xu et al. , which decomposes the noise on pairwise distances to obtain the modified noise vector \(\) as supervision. We refer readers to Xu et al.  for more details.

**Sequence loss \(^{()}\).** Since we parameterize \(p_{}(^{t-1}|^{t})\) with \(_{}(}^{0}|^{t})\) and \(q(^{t-1}|^{t},}^{0})\) as in (6), it can be proven that the \(t\)-th KL divergence term in \(^{()}\) reaches zero when \(_{}(}^{0}|^{t})\) assignsall mass on the ground truth \(^{0}\) (see proof in App. C.4). Therefore, for pre-training, we can simplify the KL divergence to the cross-entropy between the correct residue type \(s_{i}^{0}\) and the prediction:

\[_{}^{()}=_{t=1}^{T}_{i} (s_{i}^{0},_{}(s_{i}^{0}|^{t})),\] (11)

where \((,)\) denotes the cross-entropy loss.

The ultimate training objective is the sum of simplified structure and sequence diffusion losses:

\[_{}=_{}^{()}+ _{}^{()}.\] (12)

### Two-Stage Noise Scheduling

Previous studies on scheduled denoising autoencoders on images  have shown that large noise levels encourage the learning of coarse-grained features, while small noise levels require the model to learn fine-grained features. We observe a similar phenomenon in structure diffusion, as depicted in Fig. 2. The diffusion loss with large noise at a fixed scale (orange) is smaller than that with small noise at a fixed scale (green), indicating the higher difficulty of structure diffusion with small noises. Interestingly, the opposite is observed in sequence diffusion, where the denoising accuracy with small noise (green) is higher than that with large noise (orange). This can be attributed to the joint diffusion effects on protein sequences and structures. The addition of large noise during joint diffusion significantly disrupts protein structures, making it more challenging to infer the correct protein sequences. For validation, we compare the loss of sequence-only diffusion pre-training (blue) with joint diffusion pre-training (red) in Fig. 2. Sequence diffusion achieves higher denoising accuracy than joint diffusion when using uncorrupted structures, supporting our hypothesis.

Unlike recent denoising pre-training methods that rely on a fixed noise scale as a hyperparameter , DiffPreT incorporates a denoising objective at various noise levels to capture both coarse- and fine-grained features and consider the joint diffusion effect explained earlier. Both granularities of features are crucial for downstream tasks, as they capture both small modifications for assessing protein structure quality and large changes leading to structural instability. In our implementation, we perform diffusion pre-training using \(T=100\) noise levels (time steps). Following the intuition of learning coarse-grained features before fine-grained ones in curriculum learning , we divide the pre-training process into two stages: the first stage focuses on large noise levels (\(t=10,...,100\)), while the second stage targets small noise levels (\(t=1,...,9\)). As observed in Fig. 2, structure diffusion becomes more challenging and sequence diffusion becomes easier during the second stage, as we discussed earlier. However, even with this two-stage diffusion strategy, there remains a significant gap between the accuracy of sequence diffusion and joint diffusion. We hypothesize that employing protein encoders with larger capacities could help narrow this gap, which is left as our future works.

## 3 SiamDiff: Siamese Diffusion Trajectory Prediction

Through diffusion models on both protein sequences and structures, the pre-training approach proposed in Sec. 2 tries to make representations capture (1) the atom- and residue-level spatial interactions and (2) the statistical dependencies of residue types within a single protein. Nevertheless, no constraints or supervision have been added for modeling relations between different protein structures, especially different conformers of the same protein. Generated under different environmental factors, these conformers typically share the same protein sequence but different structures due to side chain rotation driving conformational variations. These conformers' properties are highly correlated , and their representations should reflect this correlation.

Figure 2: Structure diffusion loss and sequence denoising accuracy of pre-training with different noise scheduling strategies.

In this section, we introduce Siamese Diffusion Trajectory Prediction (**SiamDiff**), which incorporates conformer-related information into DiffPreT by maximizing mutual information (MI) between diffusion trajectory representations of correlated conformers. We propose a scheme to generate simulated conformers (Sec. 3.1), generate joint diffusion trajectories (Sec. 3.2), and transform MI maximization into mutual denoising between trajectories (Sec. 3.3), sharing similar loss with DiffPreT.

### Conformer Simulation Scheme

Our method begins by generating different conformers of a given protein. However, direct sampling requires an accurate characterization of the energy landscape of protein conformations, which can be difficult and time-consuming. To address this issue, we adopt a commonly used scheme for sampling randomly simulated conformers by adding torsional perturbations to the side chains .

Specifically, given the original protein \(=(,)\), we consider it as the native state \(_{1}=(_{1},_{1})\) and generate a correlated conformer \(_{2}=(_{2},_{2})\) by randomly perturbing the protein structure. That is, we set \(_{2}\) to be the same as \(_{1}\), and \(_{2}\) is obtained by applying a perturbation function \((_{1},)\) to the original residue structure, where \([0,2)^{n_{r} 4}\) is a noise vector drawn from a wrapped normal distribution . The \((,)\) function rotates the side-chain of each residue according to the sampled torsional noises. To avoid atom clashes, we adjust the variance of the added noise and regenerate any conformers that violate physical constraints. It should be noted that the scheme can be adapted flexibly when considering different granularities of structures. For example, instead of considering side chain rotation on a fixed backbone, we can also consider a flexible backbone by rotating backbone angles, thereby generating approximate conformers.

Although the current torsional perturbation scheme is effective, there is potential for improvement. Future research can explore enhancements such as incorporating available rotamer libraries  and introducing backbone flexibility using empirical force fields .

### Siamese Diffusion Trajectory Generation

To maintain the information-rich joint diffusion trajectories from DiffPreT, we generate trajectories for pairs of conformers, _a.k.a._, siamese trajectories. We first sample the diffusion trajectories \(_{1}^{0:T}\) and \(_{2}^{0:T}\) for conformers \(_{1}\) and \(_{2}\), respectively, using the joint diffusion process outlined in Sec. 2. For example, starting from \(_{1}^{0}=_{1}=(_{1},_{1})\), we use the joint diffusion on structures and sequences to define the diffusion process \(q(_{1}^{1:T}|_{1}^{0})=q(_{1}^{1:T}|_{1}^{0})q(_{1}^{1:T}|_{1}^{0})\). We derive trajectories on structures \(_{1}^{1:T}\) using the Gaussian noise in (3) and derive the sequence diffusion process \(^{1:T}\) using the random masking in (6). In this way, we define the trajectory \(_{1}^{0:T}=\{(_{1}^{t},_{1}^{t})\}_{t=0}^{T}\) for \(_{1}\) and can derive the siamese trajectory \(_{2}^{0:T}=\{(_{2}^{t},_{2}^{t})\}_{t=0}^{T}\) similarly.

### Mutual Information Maximization between Representations of Siamese Trajectories

We aim to maximize the mutual information (MI) between representations of siamese trajectories constructed in a way that reflects the correlation between different conformers of the same protein. Direct optimization of MI is intractable, so we instead maximize a lower bound. In App. C.1, we show that this problem can be transformed into the minimization of a loss function for mutual denoising between two trajectories: \(=(^{(2 1)}+^{(1 2)})\), where

\[^{(b a)}_{_{a}^{0:T}, _{b}^{0:T}}[_{t=1}^{T}D_{}(q(_{a }^{t-1}|_{a}^{t},_{a}^{0})||p(_{a}^{t-1}| _{a}^{t},_{b}^{0:T}))],\] (13)

with \(b a\) being either \(2 1\) or \(1 2\) and \(_{b}^{0:T}\) being representations of the trajectory \(_{b}^{0:T}\).

Figure 3: High-level illustration of SiamDiff. Mutual denoising of diffusion trajectories is performed across two correlated conformers \(_{1}\) and \(_{2}\).

The two terms share the similar formula as the ELBO loss in (8). Take \(^{(2 1)}\) for example. Here \(q(_{1}^{t-1}|_{1}^{t},_{1}^{0})\) is a posterior analytically tractable with our definition of each diffusion step \(q(_{1}^{t}|_{1}^{t-1})\) in (3) and (6). The reverse process is learnt to generate a less noisy state \(_{1}^{t-1}\) given the current state \(_{1}^{t}\) and representations of the siamese trajectory \(}_{2}^{0:T}\), which are extracted by the protein encoder to be pre-trained. The parameterization of the reverse process is similar as in Sec. 2.2, with the representations replaced by those of \(}_{2}^{0:T}\) (see App. B for details).

Our approach involves mutual prediction between two siamese trajectories, which is similar to the idea of mutual representation reconstruction in [28; 14]. However, since \(_{1}\) and \(_{2}\) share information about the same protein, the whole trajectory of \(_{2}\) could provide too many clues for denoising towards \(_{1}^{t-1}\), making the pre-training task trivial. To address this issue, we parameterize \(p(_{1}^{t-1}|_{1}^{t},}_{2}^{0:T})\) with \(p_{}(_{1}^{t-1}|_{1}^{t},}_{2}^{t})\). For diffusion on sequences, we further guarantee that the same set of residues are masked in \(_{1}^{t}\) and \(_{2}^{t}\) to avoid leakage of ground-truth residue types across correlated trajectories.

**Final pre-training objective.** Given the similarity between the one-side objective and the ELBO loss in (8), we can use a similar way to decompose the objective into structure and sequence losses and then derive simplified loss functions for each side. To summarize, the ultimate training objective for our method is

\[_{}=(_{}^{( ,2 1)}+_{}^{(S,2 1)}+_{}^{( ,1 2)}+_{}^{(,1 2)}),\] (14)

where \(_{}^{(,b a)}\) is the loss term defined by predicting \(_{a}^{0:T}\) from \(_{b}^{0:T}\) (see App. B for derivation).

### Residue-level model

Residue-level protein graphs are simplified atom graphs that enable efficient message passing between nodes and edges. As in Zhang et al. , we only keep the alpha carbon atom of each residue and add sequential, radius and K-nearest neighbor edges as different types of edges. For SiamDiff, the residue-level model cannot discriminate conformers generated by rotating side chains, since we only keep CA atoms. To solve this problem, we directly add Gaussian noises to the coordinates instead to generate approximate conformers. Specifically, the correlated conformer \(_{2}=(_{2},_{2})\) is defined by \(_{2}=_{1},_{2}=_{1}+\), where \(^{n_{} 3}\) is the noise drawn from a Gaussian distribution.

## 4 Related Work

**Pre-training Methods on Proteins.** Self-supervised pre-training methods have been widely used to acquire co-evolutionary information from large-scale protein sequence corpus, inducing performant protein language models (PLMs) [20; 53; 61; 50]. Typical sequence pre-training methods include masked protein modeling [20; 61; 50] and contrastive learning . The pre-trained PLMs have achieved impressive performance on a variety of downstream tasks for structure and function prediction [59; 81]. Recent works have also studied pre-training on unlabeled protein structures for generalizable representations, covering contrastive learning [87; 33], self-prediction of geometric quantities [87; 10] and denoising score matching [29; 76]. Compared with existing works, our methods model the joint distribution of sequences and structures via diffusion models, which captures both co-evolutionary information and detailed structural characteristics.

**Diffusion Probabilistic Models (DPMs).** DPM was first proposed in Sohl-Dickstein et al.  and has been recently rekindled for its strong performance on image and waveform generation [37; 11]. While DPMs are commonly used for modeling continuous data, there has also been research exploring discrete DPMs that have achieved remarkable results on generating texts [4; 49], graphs  and images . Inspired by these progresses, DPMs have been adopted to solve problems in chemistry and biology domain, including molecule generation [82; 39; 78; 43], molecular representation learning , protein structure prediction , protein-ligand binding , protein design [2; 54; 40; 74] and motif-scaffolding . In alignment with recent research efforts focused on diffusion-based image representation learning , this work presents a novel investigation into how DPMs can contribute to protein representation learning.

Now we discuss the relationship between our method and previous works.

**Advantages of joint denoising.** Compared with previous diffusion models focusing on either protein sequences  or structures  or cross-modal contrastive learning [12; 84], in this work, we perform joint diffusion on both modalities. Note that given a sequence \(\) and a structure \(\) that exist in the nature with high probability, the sequence-structure tuple \(=(,)\) may not be a valid state of this protein. Consequently, instead of modeling the marginal or conditional distribution, we model the joint distribution of protein sequences and structures.

**Connection with diffusion models.** Diffusion models excel in image and text generation [17; 49] and have been applied to unsupervised representation learning . Previous works explored denoising objectives [23; 9] but lacked explicit supervision for different conformers, while our method incorporates mutual prediction between siamese diffusion trajectories to capture conformer correlation and regularize protein structure manifold.

**Difference with denoising distance matching.** While previous works rely on perturbing distance matrices [52; 29], which can violate the triangular inequality and produce negative values, our approach directly adds noise at atom coordinates, as demonstrated in Xu et al. . This distinction allows us to address the limitations associated with denoising distance matching algorithms used in molecule and protein generation and pre-training.

**Comparison with other deep generative models.** Self-supervised learning essentially learns an Energy-Based Model (EBM) for modeling data distribution , making VAE , GAN , and normalizing flow  applicable for pre-training. However, these models limit flexibility or fail to acquire high sampling quality and diversity compared to diffusion models . Therefore, we focus on using diffusion models for pre-training and leave other generative models for future work.

## 5 Experiments

### Experimental Setups

**Pre-training datasets.** Following Zhang et al. , we pre-train our models with the AlphaFold protein structure database v1 [44; 70], including 365K proteome-wide predicted structures.

**Downstream benchmark tasks.** In our evaluation, we assess EC prediction task  for catalysis behavior of proteins and four ATOM3D tasks . The EC task involves 538 binary classification problems for Enzyme Commission (EC) numbers. We use dataset splits from Gligorijevic et al.  with a 95% sequence identity cutoff. The ATOM3D tasks include Protein Interface Prediction (PIP),

    &  & **PIP** & **MSP** & **RES** & **PSR** & **Mean** \\   & & AUROC & AUROC & Accuracy & Global \(\) & Mean \(\) & **Rank** \\   &  & 0.868\(\)0.002 & 0.633\(\)0.067 & 0.441\(\)0.001 & 0.782\(\)0.021 & 0.488 \(\)0.012 & 7.6 \\   & & Denoising Score Matching & 0.877\(\)0.002 & 0.629\(\)0.040 & 0.448\(\)0.001 & 0.813\(\)0.003 & 0.518\(\)0.020 & 5.2 \\  & & Residue Type Prediction & 0.879\(\)0.004 & 0.620\(\)0.027 & 0.449\(\)0.001 & 0.826\(\)0.020 & 0.518\(\)0.018 & 4.4 \\  & & Distance Prediction & 0.872\(\)0.001 & 0.677\(\)0.020 & 0.422\(\)0.001 & **0.40\(\)0.020** & 0.522\(\)0.004 & 4.0 \\  & & Angle Prediction & 0.878\(\)0.001 & 0.642\(\)0.013 & 0.419\(\)0.001 & 0.813\(\)0.007 & 0.503\(\)0.012 & 6.2 \\  & & Dihedral Prediction & 0.878\(\)0.004 & 0.591\(\)0.008 & 0.414\(\)0.001 & 0.821\(\)0.002 & 0.497\(\)0.004 & 6.8 \\  & & Multivive Contrast & 0.871\(\)0.003 & 0.646\(\)0.006 & 0.368\(\)0.001 & 0.805\(\)0.005 & 0.502\(\)0.009 & 7.2 \\   & & **DIPreT** & 0.880\(\)0.005 & 0.680\(\)0.018 & 0.452\(\)0.001 & 0.821\(\)0.007 & 0.533\(\)0.006 & 2.4 \\  & & **SiamDiff** & **0.884\(\)0.003** & **0.698\(\)0.020** & **0.460\(\)0.001** & 0.829\(\)0.012 & **0.546\(\)0.018** & **1.2** \\   

Table 1: Atom-level results on Atom3D tasks.

    & **Method** & **EC** & **MSP** & **PSR** & **Mean** \\   & & AUPR & F\({}_{}\) & AUROC & Global \(\) & Mean \(\) \\   &  & 0.837\(\)0.002 & 0.811\(\)0.001 & 0.644\(\)0.023 & 0.763\(\)0.012 & 0.373\(\)0.021 & 7.8 \\   & & Denoising Score Matching & 0.859\(\)0.003 & 0.840\(\)0.001 & 0.645\(\)0.028 & 0.795\(\)0.027 & 0.429\(\)0.017 & 5.0 \\  & & Residue Type Prediction & 0.851\(\)0.002 & 0.826\(\)0.005 & 0.636\(\)0.003 & 0.828\(\)0.005 & 0.480\(\)0.031 & 5.4 \\  & & Distance Prediction & 0.858\(\)0.003 & 0.836\(\)0.001 & 0.623\(\)0.007 & 0.796\(\)0.017 & 0.416\(\)0.021 & 6.4 \\  & & Angle Prediction & 0.873\(\)0.003 & 0.849\(\)0.001 & 0.631\(\)0.041 & 0.802\(\)0.015 & 0.446\(\)0.009 & 4.2 \\  & & Dihedral Prediction & 0.858\(\)0.001 & 0.840\(\)0.001 & 0.568\(\)0.022 & 0.732\(\)0.021 & 0.398\(\)0.022 & 7.2 \\  & & Multiview Contrast & 0.875\(\)0.003 & **0.857\(\)0.003** & **0.713\(\)0.036** & 0.752\(\)0.012 & 0.388\(\)0.015 & 4.0 \\   & & **DiffreT** & 0.864\(\)0.002 & 0.844\(\)0.001 & 0.673\(\)0.042 & 0.815\(\)0.008 & 0.505\(\)0.007 & 3.2 \\  & & **SiamDiff** & **0.878\(\)0.003** & **0.857\(\)0.003** & 0.700\(\)0.043 & **0.856\(\)0.007** & **0.521\(\)0.016** & **1.2** \\   

Table 2: Residue-level results on EC and Atom3D tasks.

Mutation Stability Prediction (MSP), Residue Identity (RES), and Protein Structure Ranking (PSR) with different dataset splits based on sequence identity or competition year. Details are in App. D.

**Baseline methods.** In our evaluation, we utilize GearNet-Edge as the underlying model for both atom- and residue-level structures. GearNet-Edge incorporates various types of edges and edge-type-specific convolutions, along with message passing between edges, to model protein structures effectively. We compare our proposed methods with several previous protein structural pre-training algorithms, including multiview contrastive learning , denoising score matching , and four self-prediction methods (residue type, distance, angle, and dihedral prediction) . For residue-level tasks, we include EC, MSP, and PSR in our evaluation, while PIP and RES tasks are specifically designed for atom-level models. Besides, we exclude EC from the atom-level evaluation due to the limited presence of side-chain atoms in the downloaded PDB dataset.

**Training and evaluation.** We pre-train our model for 50 epochs on the AlphaFold protein structure database following Zhang et al.  and fine-tune it for 50 epochs on EC, MSP, and PSR. However, due to time constraints, we only fine-tune the models for 10 epochs on the RES and PIP datasets. Results are reported as mean and standard deviation across three seeds (0, 1, and 2). Evaluation metrics include F\({}_{}\) and AUPR for EC, AUROC for PIP and MSP, Spearman's \(\) for PSR, and micro-averaged accuracy for RES. More details about experimental setup can be found in App. D.

### Experimental Results

Tables 1 and 2 provide a comprehensive overview of the results obtained by GearNet-Edge on both atom- and residue-level benchmark tasks. The tables clearly demonstrate that both DiffPreT and SiamDiff exhibit significant improvements over GearNet-Edge without pre-training on both levels, underscoring the effectiveness of our pre-training methods.

An interesting observation from the tables is that previous pre-training methods tend to excel in specific tasks while showing limitations in others. For instance, Multiview Contrast, designed for capturing similar functional motifs , struggles with structural intricacies and local atomic interactions, resulting in lower performance on tasks like Protein Interface Prediction (PIP), Protein Structure Ranking (PSR), and Residue Identity (RES). Self-prediction methods excel at capturing structural details or residue type dependencies but show limitations in function prediction tasks, such as Enzyme Commission (EC) number prediction and Mutation Stability Prediction (MSP), and do not consistently improve performance on both atom and residue levels.

In contrast, our DiffPreT approach achieves top-3 performance in nearly all considered tasks, showcasing its versatility and effectiveness across different evaluation criteria. Moreover, SiamDiff surpasses all other pre-training methods, achieving the best results in 6 out of 7 tasks, establishing it as the state-of-the-art pre-training approach. These results provide compelling evidence that our joint diffusion pre-training strategy successfully captures the intricate interactions between different proteins (PIP), captures local structural details (RES) and global structural characteristics (PSR), and extracts informative features crucial for accurate function prediction (EC) across various tasks.

### Ablation Study

To analyze the effect of different components of SiamDiff, we perform ablation study on atom-level tasks and present results in Table 3. We first examine two degenerate settings of joint diffusion, _i.e._, "_w/o_ sequence diffusion" and "_w/o_ structure diffusion". These settings lead to a deterioration in performance across all benchmark tasks, highlighting the importance of both sequence diffusion for residue type identification in RES and structure diffusion for capturing the structural stability of mutation effects in MSP. Next, we compare SiamDiff with DiffPreT, which lacks mutual information maximization between correlated conformers. The consistent improvements observed across all tasks indicate the robustness and effectiveness of our proposed mutual information maximization scheme.

    & **PIP** & **MSP** & **RES** & **PSR** \\   & AUROC & AUROC & Accuracy & Global \(\) \\  GearNet-Edge & 0.868\(\)0.002 & 0.633\(\)0.067 & 0.441\(\)0.001 & 0.782\(\)0.021 \\ 
**SiamDiff** & **0.84\(\)0.003** & **0.69\(\)0.020** & **0.460\(\)0.001** & **0.829\(\)0.008** \\  _w/o avg. diff._ & 0.873\(\)0.004 & 0.695\(\)0.002 & 0.443\(\)0.001 & 0.803\(\)0.010 \\ _w/o extract. diff._ & 0.878\(\)0.003 & 0.652\(\)0.021 & 0.456\(\)0.001 & 0.805\(\)0.005 \\ _w/o fast max._ & 0.880\(\)0.005 & 0.689\(\)0.018 & 0.452\(\)0.001 & 0.821\(\)0.007 \\ _w/ small noise_ & 0.875\(\)0.002 & 0.646\(\)0.013 & 0.444\(\)0.001 & 0.828\(\)0.005 \\ _w/ large noise_ & 0.867\(\)0.003 & 0.683\(\)0.020 & 0.443\(\)0.001 & 0.819\(\)0.011 \\   

Table 3: Ablation study on atom-level Atom3D tasks.

Besides, we compare our method to baselines with fixed small (\(T=1\)) and large (\(T=100\)) noise levels to demonstrate the benefits of multi-scale denoising in diffusion pre-training. Interestingly, we observe that denoising with large noise enhances performance on MSP by capturing significant structural changes that lead to structural instability, while denoising with small noise improves performance in PSR by capturing fine-grained details for protein structure assessment. By incorporating multi-scale noise, we eliminate the need for manual tuning of the noise level as a hyperparameter and leverage the advantages of both large- and small-scale noise, as evidenced in the table.

### Combine with Protein Language Models

Protein language models (PLMs) have recently become a standard method for extracting representations from protein sequences, such as ESM . However, these methods are unable to directly handle structure-related tasks in Atom3D without using protein structures as input. A recent solution addresses this by feeding residue representations outputted by ESM into the protein structure encoder GearNet . To showcase the potential of SiamDiff on PLM-based encoders, we pre-trained the ESM-GearNet encoder using SiamDiff and evaluated its performance on residue-level tasks. Considering the model capacity and computational budget, we selected ESM-2-650M as the base PLM. The results in Table 4 demonstrate the performance improvements obtained by introducing the PLM component in ESM-GearNet. Furthermore, after pre-training with SiamDiff, ESM-GearNet achieves even better performance on all tasks, especially on PSR where ESM-only representations are not indicative for structure ranking. This highlights the benefits of our method for PLM-based encoders.

In addition, we provide experimental results about pre-training datasets in App. E, multimodal baselines in App. F, different diffusion strategies in App. G, and different backbone models in App. H.

## 6 Conclusions

In this work, we propose the DiffPreT approach to pre-train a protein encoder by sequence-structure joint diffusion modeling, which captures the inter-atomic interactions within structure and the residue type dependencies along sequence. We further propose the SiamDiff method to enhance DiffPreT by additionally modeling the correlation between different conformers of one protein. Extensive experiments on diverse types of tasks and on both atom- and residue-level structures verify the competitive performance of DiffPreT and the superior performance of SiamDiff.