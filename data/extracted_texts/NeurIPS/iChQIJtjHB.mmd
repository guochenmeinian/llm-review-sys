# S-SOS: Stochastic Sum-Of-Squares for Parametric Polynomial Optimization

Richard L. Zhu

Department of Computational and Applied Mathematics

University of Chicago

Chicago, IL 60637

richardzhu@uchicago.edu

&Mathias Oster

Institute of Geometry and Practical Mathematics

RWTH Aachen

Aachen, Germany

oster@igpm.rwth-aachen.de

&Yuehaw Khoo

Department of Statistics

University of Chicago

Chicago, IL 60637

yuehaw.khoo@uchicago.edu

Corresponding Author

###### Abstract

Global polynomial optimization is an important tool across applied mathematics, with many applications in operations research, engineering, and the physical sciences. In various settings, the polynomials depend on external parameters that may be random. We discuss a stochastic sum-of-squares (S-SOS) algorithm based on the sum-of-squares hierarchy that constructs a series of semidefinite programs to jointly find strict lower bounds on the global minimum and extract candidates for parameterized global minimizers. We prove quantitative convergence of the hierarchy as the degree increases and use it to solve unconstrained and constrained polynomial optimization problems parameterized by random variables. By employing \(n\)-body priors from condensed matter physics to induce sparsity, we can use S-SOS to produce solutions and uncertainty intervals for sensor network localization problems containing up to 40 variables and semidefinite matrix sizes surpassing \(800 800\).

## 1 Introduction

Many effective nonlinear and nonconvex optimization techniques use local information to identify local minima. But it is often the case that we want to find global optima. Sum-of-squares (SOS) optimization is a powerful and general technique in this setting.

The core idea is as follows: suppose we are given polynomials \(g_{1},,g_{m},f\) where each function is on \(^{n}\) and we seek to determine the minimum value of \(f\) on the closed set \(\): \(=\{x^{n} g_{i}(x) 0\ \ i=1,,m\}\). Our optimization problem is then to find \(_{x^{n}}\{f(x)|x\}\).

An equivalent formulation is to find the largest constant \(c\) (i.e. the tightest lower bound) that can be subtracted from \(f\) such that \(f-c 0\) over the set \(\). This reduction converts a polynomial optimization problem over a semialgebraic set to the problem of checking polynomial non-negativity. This problem is NP-hard in general , therefore one one instead resorts to checking if \(f-c\) is a sum-of-squares (SOS) function, e.g. in the unconstrained setting where \(=^{n}\) one seeks to find some polynomials \(h_{k}:^{n}\) such that \(f-c=_{k}h_{k}^{2}\). If such a decomposition can be found, then we have an easily-checkable certification that \(f-c 0\), as all sum-of-squares are non-negative. Since the converse is not true (not all non-negative functions are sum-of-squares), this is a relaxation of the original non-negativity problem.

Notably, if we restrict the \(h_{k}\) to have maximum degree \(d\), the search for a degree-\(2d\) SOS decomposition of a function can be automated as a semidefinite program (SDP) [2; 3; 4]. Solving this SDP for varying degrees \(d\) generates the well-known Lasserre (SOS) hierarchy. A given degree \(d\) corresponds to a particular level of the hierarchy. Solving this SDP produces a lower bound \(c_{d}\) which has been proven to converge to the true global minimum \(c^{*}=_{x}f(x)\) as \(d\) increases, with finite convergence (\(c_{d}=c^{*}\) at finite \(d\)) for functions with second-order local optimality conditions [5; 6] and asymptotic convergence with milder assumptions thanks to representation theorems for positive polynomials from real algebraic geometry [7; 8; 9]. Further work has elucidated both theoretical implications [7; 3; 10; 11; 12; 13] and useful applications of SOS to disparate fields [14; 15; 16; 5; 6; 17; 18] (see further discussion in Appendix A.2).

Motivated by the sum-of-squares certification for a lower bound \(c\) on a function \(f(x)\), we generalize to the case where the function to be minimized has additional parameters, i.e. \(f(x,)\) where \(x\) are variables and \(\) are parameters drawn from some probability distribution \(()\). We seek a function \(c()\) that is the tightest lower bound to \(f(x,)\) everywhere: \(f(x,) c()\) with \(c()_{x}f(x,)\). This setting was originally presented in  as a "Joint and Marginal" approach to parametric polynomial optimization. With the view that \(()\) and seeking to parameterize the minimizers \(x^{*}()=_{x}f(x,)\), we are reminded of some of the prior work in polynomial chaos, where a system of stochastic variables is expanded into a deterministic function of those stochastic variables [20; 21].

**Contributions and outline.** Our primary contributions are a quantitative convergence proof for the Stochastic Sum-of-Squares (S-SOS) hierarchy of semidefinite programs (SDPs), a formulation of a new hierarchy (the cluster basis hierarchy) that uses the structure of a problem to sparsify the SDP, and numerical results on its application to the sensor network localization problem.

In Section 2, we review the S-SOS hierarchy of SDPs  and its primal and dual formulations (Section 2.1). We then detail how different hierarchies can be constructed (Section 2.2.1). Finally, in Section 2.3 (complete proof in Appendix A.5.2) we specialize to compact \(X\) and outline the proof for a theorem on quantitative convergence (the gap between the optimal values of the degree-\(2s\) S-SOS SDP and the "tightest lower-bounding" optimization problem goes \( 0\) as \(s\)) of the S-SOS hierarchy for trigonometric polynomials on \(^{n}^{d}\) following the kernel formalism of [22; 6; 23].

In Section 3 we review the hierarchy's applications in parametric polynomial minimization and uncertainty quantification, focusing on several variants of sensor network localization on \(X=[-1,1]^{n}[-1,1]^{d}\). We present numerical results for the accuracy of the extracted solutions that result from S-SOS, comparing to other approaches to parametric polynomial optimization, including a simple Monte Carlo-based method.

## 2 Stochastic Sum-of-squares (S-SOS)

#### 2.0.1 Notation

Let \((S)\) be the space of polynomials on \(S\), where \(S\{X,\}\). \(X^{n}\) and \(^{d}\), respectively, where \(X\) and \(\) are (not-necessarily compact) subsets of their respective ambient spaces \(^{n}\) and \(^{d}\). A polynomial in \((X)\) can be written as \(p(x)=_{_{ 0}^{2}}c_{}x^{}(X)\) (substituting \(n d,x,X\) for a polynomial in \(()\)). Let \(x:=(x_{1},,x_{n}),:=(_{1},,_{d})\), \(\) be a multi-index (size given by context), and \(c_{}\) be the polynomial coefficients. Let \(^{s}(S)\) for some \(s_{ 0},S\{X,\}\) denote the subspace of \((S)\) consisting of polynomials of degree \( s\), i.e. polynomials where the multi-indices of the monomial terms satisfy \(||||_{1} s\). \(_{}(X)\) refersto the space of polynomials on \(X\) that can be expressible as a sum-of-squares in \(x\) and \(\) jointly, and \(^{s}_{}(X)\) be the same space restricted to polynomials of degree \( s\). Additionally, \(W 0\) for a matrix \(W\) denotes that \(W\) is symmetric positive semidefinite (PSD). Finally, \(()\) denotes the set of Lebesgue probability measures on \(\). For more details, see Appendix A.1.

### Formulation of S-SOS hierarchy

We present two formulations of the S-SOS hierarchy that are dual to each other in the sense of Fenchel duality [24; 25]. The primal problem seeks to find the tightest lower-bounding function and the dual problem seeks to find a minimizing probability distribution. Note that the "tightest lower bound" approach is dual to the "minimizing distribution" approach, otherwise known as a "joint and marginal" moment-based approach originally detailed in .

#### 2.1.1 Primal S-SOS: The tightest lower-bounding function

Consider a polynomial \(f(x,):^{n+d}\) with \(x X^{n},^{d}\) equipped with a probability measure \(()\). We interpret \(x\) as our optimization variables and \(\) as noise parameters, and seek a lower-bounding function \(c^{*}()\) such that \(f(x,) c^{*}()\) for all \(x,\). In particular, we want the tightest lower bound \(c^{*}()=_{x X}f(x,)\). Note that even when \(f(x,)\) is polynomial, the tightest lower bound \(c^{*}()\) can be non-polynomial. A simple example is the function \(f(x,)=(x-)^{2}+( x)^{2}\), which has \(c^{*}()=_{x}f(x,)=^{4}/(1+^{2})\) (Appendix A.6.1).

For us to select the "best" lower-bounding function, we want to maximize the expectation of the lower-bounding function \(c()\) under \(()\) while requiring \(f(x,)-c() 0\), giving us the following optimization problem over \(L^{1}\)-integrable lower-bounding functions:

\[p^{*}=_{c L^{1}()}  c()()\] (1) s.t. \[f(x,)-c() 0\]

Even if we restricted \(c()\) to be polynomial so that the residual \(f(x,)-c()\) is also polynomial, we would still have a challenging nonconvex optimization problem over non-negative polynomials. In SOS optimization, we take a relaxation and require the residual to be SOS: \(f(x,)-c()_{}(X)\). Doing the SOS relaxation of the non-negative Equation (1) and restricting \(c()\), i.e. \(f(x,)-c()\) to polynomials of degree \( 2s\) gives us Equation (2), which we call the primal S-SOS degree-\(2s\) SDP:

\[p^{*}_{2s}=_{c^{2s}(),W 0}  c()()\] (2) s.t. \[f(x,)-c()=m_{s}(x,)^{T}Wm_{s}(x,)\]

where \(m_{s}(x,)\) is a basis function \(X^{a(n,d,s)}\) containing monomial terms of degree \( s\) written as a column vector, and \(W^{a(n,d,s) a(n,d,s)}\) a symmetric PSD matrix. Here, \(a(n,d,s)\) represents the dimension of the basis function, which depends on the degree \(s\) and on the dimensions \(n,d\). For this formulation to find the best degree-\(2s\) approximation to the lower-bounding function, we require \(g(x,)=m_{s}(x,)^{T}Wm_{s}(x,)\) to span \(^{2s}(X)\). Selecting all combinations of standard monomial terms of degree \( s\) suffices and results in a basis function with size \(a(n,d,s)=\).

#### 2.1.2 Dual S-SOS: A minimizing distribution

The formal dual to Equation (1) (proof of duality in Appendix A.5.1) seeks to find a "minimizing distribution" \((x,)\), i.e. a probability distribution that places weight on the minimizers of \(f(x,)\) subject to the constraint that the marginal \(_{X}()\) matches \(()\):

\[d^{*}=_{(X)}  f(x,)(x,)\] (3) s.t. \[_{X}(x,)=_{X}()=()\]where we have written \((X)\) as the space of joint probability distributions on \(X\) and \(_{X}()\) is the marginal of \((x,)\) with respect to \(\), obtained via disintegration.

For the primal, we considered polynomials of degree \( 2s\). We do the same here. The formal dual becomes a tractable SDP, where the objective turns into moment-minimization and the constraints become moment-matching. Following [3; 15], let \(M^{a(n,d,s) a(n,d,s)}\) be the symmetric PSD moment matrix with entries defined as \(M_{i,j}=_{X}m_{s}^{(i)}(x,)m_{s}^{(j)}(x,)d(x,)\) where \(m_{s}^{(i)}(x,)\) is the \(i\)-th element of the basis function \(m_{s}\). Let \(y^{b(n,d,s)}\) be the moment vector of independent moments that completely specifies \(M\), e.g. in the case that we use all standard monomials of degree \( s\) and have \(a(n,d,s)=\), then \(b(n,d,s)=\). We write \(M(y)\) as the moment matrix that is formed from these independent moments. We have \(y_{(i,j)}=_{X}m_{s}^{(i)}(x,)m_{s}^{(j)}(x, )d(x,)\) where the multi-index \((i,j)_{ 0}^{n+d}\) corresponds to the sum of the multi-indices corresponding to the \(i\)-th entry and the \(j\)-th entry of \(m_{s}(x,)\).

We write \(f(x,)\) in terms of the monomials \(f(x,)=_{||||_{1} 2s}f_{}[x,]^{}\), where \([x,]\) is the concatenation of the \(n+d\) variables from \(x,\) and \(_{ 0}^{n+d}\) is a multi-index. Note that every monomial \([x,]^{}\) has a corresponding moment \(y_{}\): \([x,]^{}(x,)=y_{}\). We then observe that the integral in the objective reduces to a dot product between the coefficients of \(f\) and the moment vector:

\[ f(x,)(x,)=_{}f_{}[x,]^ {}(x,)=_{}f_{}y_{}\]

After converting the distribution-matching constraint \(_{X}()=()\) in (3) into equality constraints on the moments of \(\) up to degree \(2s\), we obtain the following dual S-SOS degree-\(2s\) SDP:

\[d_{2s}^{*}=_{y^{b(n,d,s)}} _{||||_{1} 2s}f_{}y_{}\] (4) s.t. \[M(y) 0\] \[y_{}=m_{}\ \ (,m_{})_{}\]

We write \(_{}\) as the set of \((,m_{})\) representing the moment-matching constraints on \(^{}\) up to degree-\(2s\), i.e. we want to set \(_{X}^{}(x,)=_{} ^{}()=m_{}\) for all multi-indices \(_{ 0}^{d}\) with \(||||_{1} 2s\). There are \(\) multi-indices \(_{ 0}^{n+d},||||_{1} 2s\) where only the \(d\) entries associated with \(\) are non-zero, and therefore the number of moment-matching constraints is \(|_{}|=\). Note that the moment matrix \(M(y)^{a(n,d,s) a(n,d,s)}\) is a symmetric PSD matrix and is the dual variable to the primal \(W\). Observe also that we require the moments of \(()\) of degree up to \(2s\) to be bounded. (4) is often a more convenient form than (2), especially when working with additional equality or inequality constraints, as we will see in Section 3. For concrete examples of the primal and dual SDPs with explicit constraints, see Appendix A.3.

### Variations

In this section, we detail two ways of building a hierarchy, one based on the maximum degree of monomial terms in the basis function (Lasserre) and a novel one based on the maximum number of interactions occurring in the terms of the basis function (cluster basis). To define any SOS hierarchy, we first select a monomial basis. Some examples include the standard monomial basis \(x_{1},,x_{n}\), trigonometric/Fourier 1-periodic monomial basis \( x_{1}, x_{1},, x_{n}, x_{n}\)), or others. Using this basis, we write down a basis function \(m(x)\) which comprises some combinations of monomials. Squared linear combinations of the basis functions then span a SOS space of functions: \(:\{(_{i}h_{i}m_{i}(x))^{2}\}\).

#### 2.2.1 Standard Lasserre hierarchy

In the Lasserre hierarchy, the basis function \(m_{s}(x)\) is composed of all combinations of monomials up to degree \(s_{>0}\) and a given level of the hierarchy is set by the maximum degree \(s\). The basis function consists of terms \(x^{}\) with \(\) a multi-index and \(||||_{1} s\). The degree-\(2s\) SOS function space parameterized by this basis function is that spanned by \(m_{s}(x)^{T}Wm_{s}(x)\) for PSD \(W\), i.e. the functions that can result from squaring any linear combination of degree-\(s\) polynomials that can be generated from our basis \(m_{s}(x)\). As we increase the degree \(s\), our basis function gets larger and our S-SOS SDP objective values converge to the optimal value of the "tightest lower-bounding" problem Equation (1) .

#### 2.2.2 Cluster basis hierarchy

In this section, we propose a cluster basis hierarchy, wherein we utilize possible spatial organization of the problem to sparsify the problem and reduce the size of the SDP that must be solved . The cluster basis is a physically motivated prior often used in statistical and condensed matter physics, where we assume that our degrees of freedom can be arrayed in space, with locally close variables interacting strongly (kept in the model) and globally separated variables interacting weakly (ignored). Moreover, one may also keep only the terms with interactions between a small number of degrees of freedom, such as considering only pairwise or triplet interactions between particles.

In the cluster basis hierarchy, a given level of the hierarchy is specified by a 2-tuple \((b,t)\), the desired body order \(b\) and the maximum degree of a single variable \(t\). Body order denotes the maximum number of interacting variables in a given monomial term, e.g. \(x_{i}^{a}x_{j}^{b}x_{k}^{c}\) would have body order 3 and total degree \(a+b+c\). The basis function \(m_{b,t}\) consists of terms \(x^{}\) with \(\) a multi-index, \(||||_{0} b\) (at most \(b\) interacting variables can occur in a single term), and \(||||_{} t\) (each variable can have up to degree \(t\). The maximum degree of the basis function \(m_{b,t}\) is then \(s=bt\). If we are to compare \(m_{b,t}\) from the cluster basis hierarchy with \(m_{s}\) from the Lasserre hierarchy, we find that even when \(bt=s\) we still have strictly fewer terms, e.g. in the case where \(b=2,t=2,s=4\) we have \(m_{s}\) containing terms of the form \(x_{i}^{4}\) but \(m_{b,t}\) only has degree-4 terms of the \(x_{i}^{2}x_{j}^{2}\).

To expand on this, consider that in the standard Lasserre hierarchy we have \(m_{s}(x)\) containing all monomials of degree \( s\) in \(n\) variables, or \({n+s s}\) terms in total. In the proposed cluster basis hierarchy, \(m_{b,t}(x)\) has

\[_{k=0}^{b}{n k}t^{k}\]

terms. This expression results from the need to sum over body orders \(k\), considering that there are \({n k}\) ways to choose \(k\) variables and that each selected variable has \(k\) possible degrees so there are \(t^{k}\) ways to assign degrees \( t\) to these \(k\) variables. For fixed \(b,t\), \(s=bt\), and \(n b,t\) we note that \(m_{s}(x)\) has \(O(n^{bt})\) terms while \(m_{b,t}(x)\) has \(O(n^{b})\) terms so the size reduction factor in using the cluster basis asymptotically goes like \(n^{b(t-1)}\). As the number of variables \(n\) in the problem grow, we have asymptotic dominance in using the cluster basis to reduce the size of the SDP that must be solved. As \(bt\) we might expect asymptotic convergence of the SDP hierarchy just like the standard Lasserre hierarchy, however, a full proof of convergence is out of scope for this paper. For further details, see discussion in Appendix A.7.4.

### Convergence of S-SOS

As we increase the degree \(s\) (either \(s\) in the Lasserre hierarchy or \(b,t\) in the cluster basis hierarchy) we would expect the SDP objective values \(p_{2s}^{*}\) (Equation (2)) to converge to the optimal value \(p^{*}\) and the lower bounding function \(c_{2s}^{*}()\) to converge to the tightest lower bound \(c^{*}()=_{x}f(x,)\). In this paper we refer to \(p_{2s}^{*} p^{*}\) and \(d_{2s}^{*} d^{*}\) interchangeably as strong duality occurs in practice despite being difficult to formally verify (Appendix A.4). This convergence is a common feature of SOS hierarchies.

In this section we show that using polynomial \(c_{2s}^{*}()\) to approximate \(c^{*}()\) still allows for asymptotic convergence in \(L^{1}\) as \(s\).

#### 2.3.1 Overview of result

We specialize to the particular case of 1-periodic trigonometric polynomials \(f(x,),c()\) on compact \(X=^{n}\) and compact \(^{d}\) and prove asymptotic convergence of the degree-\(2s\) S-SOS hierarchy as \(s\). Though seemingly restrictive, this choice allows us to leverage Fourier convergence results on a compact domain. For generic \(f(x,)\) where we are only interested in its behavior in some compact set (nearly all practical problems involve a restriction of domain), we may 

[MISSING_PAGE_FAIL:6]

\(q_{x}(x),q_{}()\) can be chosen to be "kernel functions" of bounded degree \( s\) so that the output SOS function is of degree \(2s\). We can show the approximation error \(||c^{*}-c^{*}_{a}||\) is small, that the operator \(T\) exists and is close to the identity, and the deformation resulting from the SOS projection is small for sufficiently large degree \(s\) (degree of the approximating \(c^{*}_{a}()\) and in the SOS kernel). We may conclude that the true strictly-positive part may be well-approximated by the SOS hierarchy and find asymptotic convergence along with a convergence rate in the degree \(s\) of the hierarchy.

## 3 Numerical experiments

We present two numerical studies of S-SOS demonstrating its use in applications. The first study (Section 3.1) numerically tests how the optimal values of the SDP Equation (2) \(p^{*}_{2s}\) converge to \(p^{*}\) of the original primal Equation (1) as we increase the degree. The second study (Section 3.2) evaluates the performance of S-SOS for solution extraction and uncertainty quantification in various sensor network localization problems.

### Simple quadratic SOS function

As a simple illustration of S-SOS, we test it on the SOS function

\[f(x,)=(x-)^{2}+( x)^{2}\] (6)

with \(x,\). The lower bound \(c^{*}()=_{x}f(x,)\) can be computed analytically as \(c^{*}()=^{4}/(1+^{2})\). Assuming \((-1,1)\), we get that the objective value for the "tightest lower-bounding" primal problem Equation (1) is \(p^{*}=_{-1}^{1}}{2(1+^{2})}d=-  0.1187\). For further details, see Appendix A.6.

We are interested in studying the quantitative convergence of the S-SOS hierarchy numerically. The idea is to solve the primal (dual) degree-\(2s\) SDP to find the tightest polynomial lower bound (the minimizing probability distribution) for varying degrees \(s\). As \(s\) gets larger, the basis function \(m_{s}(x)\) gets larger and the objective value of the SDP Equation (2) \(p^{*}_{2s}\) should converge to the theoretical optimal value \(p^{*}\).

In Figure 1 we see very good agreement between \(p^{*}\) and \(p^{*}_{2s}\) with exponential convergence as \(s\) increases. This is much faster than the rate we found in Section 2.3.2, but agrees with the exponential convergence results from  achieved with local optimality assumptions. Due to the simplicity of (6), it is not surprising that we see much faster convergence. In fact, for most typical functions, we might expect convergence much faster than the worst-case rate. The tapering-off of the convergence rate is likely attributed to the numerical tolerance used in our solver (CVXPY/MOSEK), as we observed that increasing the tolerance shifts the best-achieved gap higher.

### Sensor network localization

Sensor network localization (SNL) is a common testbed for global optimization and SDP solvers due to the high sensitivity and ill-conditioning of the problem. In SNL, one seeks to recover the positions of \(N\) sensors \(X^{N}\) positioned in \(^{}\) given a set of noisy observations of pairwise distances \(d_{ij}=||x_{i}-x_{j}||\) between the sensors [15; 33]. To have a unique global minimum and remove symmetries, sensor-anchor distance observations are often added, where several sensors are anchored at known locations in the space. This can improve the conditioning of the problem, making it "easier" in some sense.

#### 3.2.1 Definitions

We define a SNL _problem instance_ with \(X[-1,1]^{N}\) as the ground-truth positions for \(=\{1,2,,N\}\) sensors, \(A[-1,1]^{K}\) as the ground-truth positions for \(=\{1,2,,K\}\) anchors, \(_{ss}(r)=\{d_{ij}=||x_{i}-x_{j}||:i,jd_{ij} r\}\) as the set of observed sensor-sensor distances and \(_{sa}(r)=\{d_{ik}=||x_{i}-a_{k}||:i,k { and }d_{ik} r\}\) as the set of observed sensor-anchor distances, both of which depend on some sensing radius \(r\).

Writing \(x_{i},a_{k}[-1,1]^{}\) as the unknown positions of the \(i\)-th sensor and the \(k\)-th anchor, we can write the potential function to be minimized as a polynomial:

\[f(x,;X,A,r)=_{xs}(r)}(||x_{i}-x_{j} ||_{2}^{2}-d_{ij}()^{2})^{2}}_{}+_{xs}(r)}(||x_{i}-a_{k}||_{2}^{2}-d_{ ik}()^{2})^{2}}_{}\] (7)

The observed sensor-sensor and sensor-anchor distances \(d_{ij}(),d_{ik}()\) can be perturbed arbitrarily, but in this paper we focus on linear uniform noise, i.e. for a subset of observed distances we have \(d_{ij,k}()=d_{ij}^{*}+_{k}\) with \(_{k}(-1,1)\). Other noise types may be explored, including those including outliers, which may be a better fit for robust methods (Appendix A.7.2).

Equation (7) contains soft penalty terms for sensor-sensor terms and sensor-anchor terms. We can see that this is a degree-4 polynomial in the standard monomial basis elements, and a global minimum of this function is achieved at \(f(X,^{d};X,A,r)=0\) (where the distances have not been perturbed by any noise). In general for non-zero \(\) (measuring distances under noise perturbations) we expect the function minimum to be \(>0\), as there may not exist a configuration of sensors \(\) that is consistent with the observed noisy distances.

We can also support equality constraints in our solution, in particular hard equality constraints on the positions of certain sensors relative to known anchors. This corresponds to removing all sensor-anchor soft penalty terms from the function and instead selecting \(N_{H}<N\) sensors at random to exactly fix in known positions via equality constraints in the SDP. The SDP is still large but the effective number of variable sensors has been reduced to \(N^{}=N-N_{H}\).

A given SNL _problem type_ is specified by a spatial dimension \(\), \(N\) sensors, \(K\) anchors, a sensing radius \(r(0,2)\), a noise type (linear), and anchor type (soft penalty or hard equality). Once these are specified, we generate a random _problem instance_ by sampling \(X(-1,1)^{n},A(-1,1)^{d}\). The potential \(f(x,)\) for a given instance is formed (either with sensor-anchor terms or not, with terms kept based on some sensing radius \(r\), and noise variables appropriately added).

The number of anchors is chosen to be as few as possible so as to still enable exact localization, i.e. \(K=+1\) anchors for a SNL problem in \(\) spatial dimensions. The SDPs are formulated with the help of SymPy  and solved using CVXPY [35; 36] and Mosek  on a server with two Intel Xeon 6130 Gold processors (32 physical cores total) and 256GB of RAM. For an expanded discussion and further details, see Appendix A.7.

#### 3.2.2 Evaluation metrics

The accuracy of the recovered solution is of primary interest, i.e. our primary evaluation metric should be the distance between our extracted sensor positions \(x\) and the ground-truth sensor positions \(X\), i.e. \((x,X)\). Because the S-SOS hierarchy recovers estimates of the sensor positions \([x_{i}]\) along with uncertainty estimates \([x_{i}]\), we would like to measure the distance between our ground-truth positions \(X\) to our estimated distribution \(p(x)=([x],[x])\). The Mahalanobis distance \(_{M}\) (Equation (8)) is a modified distance metric that accounts for the uncertainty . We use this as our primary metric for sensor recovery accuracy.

\[_{M}(X,(,)):=^{-1}(X-)}\] (8)

As our baseline method, for each problem instance we apply a basic Monte Carlo method detailed in Algorithm 1 (Appendix A.7.3) where we sample \(()\), use a local optimization solver to find \(x^{*}()=_{x}f(x,)\), and use this to estimate \(_{}[x],_{}[x]\). Note that though this non-SOS method achieves some estimate of the dual SDP objective \( f(x,)d(x,)\), it is not guaranteed to be a lower bound.

#### 3.2.3 Results

**Recovery accuracy.** In Table 1 we see a comparison of the S-SOS method and the MCPO baseline. Each row corresponds to one SNL problem type, i.e. we fix the physical dimension \(\), the number of anchors \(K=+1\), and select the sensing radius \(r\) and the noise scale \(\). We then generate \(L=20\) random instances of each problem type, corresponding to a random realization of the ground-truth sensor and anchor configurations \(X[-1,1]^{N},A[-1,1]^{K}\), producing a \(f(x,)\) that we then solve the SDP for (in the case of S-SOS) or do pointwise optimizations for (in the case of MCPO). Each method outputs estimates for the sensor positions and uncertainty around it as a \(([x],[x])\), which we then compute \(_{M}\) for (see Equation (8)), treating each dimension as independent of each other (i.e. \(X\) as a flat vector). Each instance solve gives us one observation of \(_{M}\) or each method, and we report the median and the \( 1_{34\%}\) values over the \(L=20\) instances we generate.

## 4 Discussion

In this paper, we discuss the stochastic sum-of-squares (S-SOS) method to solve global polynomial optimization in the presence of noise, prove two asymptotic convergence results for polynomial \(f\) and compact \(\), and demonstrate its application to parametric polynomial minimization and uncertainty quantification along with a new cluster basis hierarchy that enables S-SOS to scale to larger problems. In our experiments, we specialized to sensor network localization and low-dimensional uniform random noise with small \(n,d\). However, it is relatively straightforward to extend this method to support other noise types (such as Gaussian random variates without compact support, which we do in Appendix A.6.4) and support higher-dimensional noise with \(d 1\).

Scaling this method to larger problems \(n 1\) is an open problem for all SOS-type methods. In this paper, we take the approach of sparsification, by making the cluster basis assumption to build up a block-sparse \(W\). We anticipate that methods that leverage sparsity or other structure in \(f\) will be promising avenues of research, as well as approximate solving methods that avoid the explicit materialization of the matrices \(W,M\). For example, we assume that the ground-truth polynomial possesses the block-sparse structure because our SDP explicitly requires the polynomial \(f(x,)\) to exactly decompose into some lower-bounding \(c()\) and SOS \(f_{}(x,)\). Relaxing this exact-decomposition assumption and generalizing beyond polynomial \(f(x,),c()\) may require novel approaches and would be an exciting area for future work.