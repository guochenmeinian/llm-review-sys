# DDN: Dual-domain Dynamic Normalization for Non-stationary Time Series Forecasting

Tao Dai\({}^{1}\), Beiliang Wu\({}^{1,}\), Peiyuan Liu\({}^{2,}\), Naiqi Li\({}^{2,}\), Xue Yuerong\({}^{2}\), Shu-Tao Xia\({}^{2}\), Zexuan Zhu\({}^{1}\)

\({}^{1}\)College of Computer Science and Software Engineering, Shenzhen University, China

\({}^{2}\)Tsinghua Shenzhen International Graduate School, Tsinghua University, China

{daitao.edu, peiyuanliu.edu, linaqi.thu}@gmail.com; xiast@sz.tsinghua.edu.cn

Equal contributionCorrespondence to: Peiyuan Liu and Naiqi Li

###### Abstract

Deep neural networks (DNNs) have recently achieved remarkable advancements in time series forecasting (TSF) due to their powerful ability of sequence dependence modeling. To date, existing DNN-based TSF methods still suffer from unreliable predictions for real-world data due to its non-stationarity characteristics, _i.e.,_ data distribution varies quickly over time. To mitigate this issue, several normalization methods (e.g., SAN) have recently been specifically designed by normalization in a fixed period/window in the time domain. However, these methods still struggle to capture distribution variations, due to the complex time patterns of time series in the time domain. Based on the fact that wavelet transform can decompose time series into a linear combination of different frequencies, which exhibits distribution variations with time-varying periods, we propose a novel Dual-domain Dynamic Normalization (DDN) to dynamically capture distribution variations in both time and frequency domains. Specifically, our DDN tries to eliminate the non-stationarity of time series via both frequency and time domain normalization in a sliding window way. Besides, our DDN can serve as a plug-in-play module, and thus can be easily incorporated into other forecasting models. Extensive experiments on public benchmark datasets under different forecasting models demonstrate the superiority of our DDN over other normalization methods. Code is available at https://github.com/Hank0626/DDN.

## 1 Introduction

Deep neural networks (DNNs) with powerful dependency modeling capability have recently been widely used in time series forecasting (TSF) applications, including weather prediction , energy consumption estimation , and traffic flow forecasting . Despite the great advancements of DNN-based TSF methods [4; 5; 6; 7], they still suffer from unreliable predictions for real-world data due to its non-stationary nature of real-world time series, _i.e.,_ data distribution within the series varies quickly over time (_a.k.a_, distribution drift [8; 9; 10]). Such non-stationary challenge limits the real applications of DNN-based TSF methods.

To mitigate the problem of distribution drift, the classic reversible normalization  has recently been proposed with a two-stage pipeline of normalization and de-normalization. The former stage of normalization eliminates non-stationary factors for converting a non-stationary sequence into a stationary sequence, which has to acquire the mean and standard deviation of the sequence before. The latter stage of de-normalization reconstructs non-stationary information from the distribution prediction model or directly reuses the mean and standard deviation acquired in normalization.

Later, several advanced variants of reversible normalization [12; 13; 14] have achieved impressive performance by further alleviating the non-stationary property of real-world time series.

Despite the great success of normalization methods, existing methods are limited in capturing distribution variations by performing normalization with a fixed period/window. As shown in Figure 1, either existing coarse-grained (e.g., RevIN ) or fine-grained normalization (e.g., SAN ) in single time domain tends to produce sub-optimal performance. On the other hand, it is known that wavelet transform can decompose time series into a time-dependent sum of frequency components, which exhibits distribution variations with time-varying periods (see Figure 1). Thus, making full use of such frequency information is helpful to capture distribution variations with time-varying periods and intensities. These observations motivate us to develop a more powerful normalization strategy to dynamically capture distribution variations.

In this paper, we propose a novel **Dual-domain Dynamic Normalization (DDN)** framework to dynamically capture distribution variations in both times and frequency domains in a sliding window way. Specifically, our DDN decomposes the original time series into different frequency components, including low-frequency and high-frequency components, based on Discrete Wavelet Transform (DWT) [15; 16]. Followed by performing sliding normalization in an individual frequency component with proper window size (see Figure 1), which is helpful to capture distribution variations with time-varying periods and intensities. Besides, time domain normalization is developed to compute local sliding statistics , including sliding mean and sliding standard deviation. Unlike the previous works that process a coarse-grained level, our DDN leverages fine-grained a more informative sliding window to calculate distribution characteristics for every time step.

Our main contributions can be summarized as: **(i)** We propose a novel Dual-domain Dynamic Normalization (DDN) to dynamically capture distribution variations in both time and frequency domains with sliding statistics. Compared with previous works, our DDN is capable of dynamically reflecting the rapid variations to time series. **(ii)** Our DDN aims to eliminate non-stationary factors with frequency domain normalization and time domain normalization. Benefiting from the complementary properties of the time and frequency domain information, it allows our DDN to further clarify non-stationary factors and reconstruct non-stationary information. **(iii)** Extensive experiments demonstrate the effectiveness of our DDN, by achieving significant performance improvements across various baseline models on seven real-world datasets.

## 2 Related Works

### Deep Models for Time Series Forecasting

Reviewing the development of time series forecasting based on deep models, early methods [18; 19; 20] often integrated cross-dimension information in embedding module, then modeling cross-time

Figure 1: (a) Existing methods with a fixed period/window normalization struggle to capture distribution variations. (b) Our method dynamically captures distribution variations in both time and frequency domains.

information. In contrast, recent Sota methods indicate that two modeling ways can be better: CI (Channel Independent) and CD (Channel Dependent). The primary distinction between the two approaches lies in the former focusing only on cross-time features but the latter incorporating cross-dimension features. Theoretically, the latter can leverage more information and achieve higher prediction accuracy [21; 22; 23]. In practice, for relatively short input series, CD methods [24; 25] achieve comparable or even better performance than the CI methods. However, for longer input sequences, the situation is often the opposite[26; 4; 27]. In recent research, this difference can be attributed to the CD having higher capacity but often lacking robustness in predicting distributional drift than CI [28; 29], while longer series typically experience more severe distributional drift. The superior performance of CI highlights the importance of handling distribution drift, and it is a valuable direction in the current research on time series forecasting.

### Stationary for Time Series Forecasting

RevIN  was the first work to apply reversible normalization for time series forecasting, which assumes that history and future sequences share the same distribution. It counts distribution statistics of historical sequence for both normalization and de-normalization. Due to its simplicity and impressive effectiveness, it has been widely used in recent works [30; 31]. However, RevIN overlooks the distributional differences between historical and future sequences. Building upon RevIN, DishTS  proposes different distribution characteristics for historical and future sequences, using a distribution forecasting model to predict mean and standard deviation. Concurrently, NST  employs a module to provide more consistent distribution with future distribution, which can refer to appendix B. Furthermore, SAN  notes that existing distribution assumptions may not adapt to the scenario that time series points rapidly change over time [32; 33] and proposes a more fine-grained method, which supposes the distribution characteristics of time points is different between slices but same within a slice. Nevertheless, SAN still stops at the slice level, rather than the time series point level. Meanwhile, existing works lack consideration of the discrepancies between low and high frequencies, leading to insufficient consideration of non-stationary information.

## 3 Methodology

In the realm of multivariate time series forecasting, we consider a historical sequence \(^{M L}\) and aim to predict the corresponding future sequence \(^{M T}\), \(M\) is the number of channels. DDN is a model-agnostic plugin designed to align the distribution characteristics of \(\) and accurately estimate the distribution of \(\). In this section, we will comprehensively outline the pipeline of the entire framework and elaborate on how to remove and reconstruct non-stationary factors of time series. To enhance clarity and facilitate understanding of subsequent chapters, the key notations used in this paper are summarized in Table 1, and the framework can be referred to in Figure 2.

### Overall Framework

As depicted in Figure 2, we first eliminate non-stationary factors via both the Frequency Domain Normalization (**FDN**) and the Time Domain Normalization (**TDN**). These processes output two stationary sequences and two sets of distribution characteristics. Two stationary sequences weighted to a sequence and input to the time series Forecasting Model (**FM**) for future sequence forecasting, while two sets of non-stationary factors input to Distribution Prediction Model (**DPM**) and predict future non-stationary factors. Finally, these factors are weighted together and incorporated with forecasting

   Notation & Description \\  \(L,T\) & The time steps of the historical/future sequences \\ \(^{i},^{i}\) & The \(i\)-th historical or future series \\ \(}^{i}_{*},}^{i}_{*}\) & The \(^{i}\) after normalization and it predicted series \\ \(^{i},^{i}\) & The \(i\)-th mean or standard deviation series of \(^{i}\) \\ \(^{i}_{y},^{i}_{y}\) & The \(i\)-th mean or standard deviation series of \(^{i}\) \\ \(^{i}_{*},^{i}_{*}\) & The distribution forecasting of \(^{i}\) or \(^{i}\) \\   

Table 1: Summary of key mathematical notationsoutput to reconstruct non-stationary factors by de-normalization. Here, \(_{d}\) and \(_{f}\) correspond to the parameters of DPM and FM, and the training strategy can be seen in the section 3.4.

### Non-stationarity Elimination

For each series \(^{i}\), we perform a sliding window along the temporal dimension to acquire distribution characteristics, then replicate padding that will align the length of sliding statistics to the original series. Finally, sliding mean \(^{i}\) and sliding standard deviation \(^{i}\) represent to the distribution characteristics of \(^{i}\). This process can be described as follows:

\[^{i}_{j}&=_{-k}^{ k}x^{i}_{j+t},(^{i}_{j})^{2}=_{-k}^{k} (x^{i}_{j+t}-^{i}_{j})^{2},\\ ^{i}&=(\{^{i}_{k+1},, ^{i}_{L-k}\}),^{i}=(\{^{i}_{k+1},, ^{i}_{L-k}\}).\] (1)

Here \(2k+1\) is the size of the sliding window, and stride is \(1\). After that, the size of sliding statistics is \(L-2k\). Where \({_{j}}^{i}\) and \({_{j}}^{i}\) represent the mean value and standard deviation value of the \(j^{th}\) time point respectively, where \(j\{k+1,,L-k\}\). To make sure each time point possesses corresponding sliding statistics. We copy the sliding statistics closest in time by \(()\) operation, the obtaining \(_{i}\) and \(_{i}\) are used to achieve the transformation from non-stationary sequences to stationary sequences. The process is as follows:

\[}^{i}=^{i}+}(^{i}-^{i}),>0.\] (2)

Here, \(}^{i}\) is the stationary series, \(\) is a positive number to prevent the denominator from zero, and \(\) denotes the element-wise product. By this sliding normalization, annotated as \(()\), we can acquire the non-stationary factors of each time point and convert non-stationary sequences to stationary sequences.

Frequency Domain Normalization.In this branch, to exhaustively unveil non-stationary factors and eliminate them accurately. Discrete Wavelet Transform (DWT) is conducted on \(_{i}\) to separate the low-frequency component \(^{i}_{l}\) and high-frequency component \(^{i}_{h}\). Subsequently, we acquire and eliminate their non-stationary factors. The process is as follows:

\[^{i}_{l},^{i}_{h}=_{_{l,h}}(^{i}),\\ }^{i}_{l},^{i}_{l},^{i}_{l}=(^{i}_{l}),&}^{i}_{h},^{i}_{h}, ^{i}_{h}=(^{i}_{h}),\] (3)

Here, \(_{l,h}\) is a pair of learnable wavelet bases. \(}^{i}_{l}\), \(^{i}_{l}\), and \(^{i}_{l}\) represent the stationary sequence, sliding mean, and sliding standard deviation of the low-frequency component. While \(}^{i}_{h}\), \(^{i}_{h}\), and \(^{i}_{h}\)

Figure 2: The comprehensive time series forecasting framework comprises a time series forecasting model and an auxiliary module designed for handling non-stationary factors. This auxiliary module consists of two sub-modules: one for eliminating non-stationary factors and another for reconstructing them. The non-stationary factor elimination sub-module includes Time Domain Normalization and Frequency Domain Normalization, while the non-stationary factor reconstruction sub-module incorporates a distribution prediction module.

denote those of the high-frequency component. In practice, different types of DWT have different padding lengths and lead to different output lengths. To ensure a consistent and clear output length, Inverse Discrete Wavelet Transform (IDWT) performs to restore a definite size. The process is as follows:

\[}^{i}=_{_{l,h}}(}^{i}_{l},}^{i }_{h}),}^{i}=_{_{l,h}}(^{i}_{l},^{i}_{h}),\ }^{i}=_{_{l,h}}(^{i}_{l},^{i}_{h}).\] (4)

Where \(}^{i}\), \(}^{i}\), and \(}^{i}\) encompass the stationary sequences, sliding means, and sliding standard deviations of different frequency components. Through these operations, the output stationary sequence and distribution statistics maintain consistency with the dimensions of the input non-stationary sequence.

Time Domain Normalization.We conduct the same manners in the time domain without frequency decomposition. The process can be formulated as follows:

\[}^{i},^{i},^{i}=(^{i }),\] (5)

The wavelet transform in FDN typically involves padding, which can potentially distort the statistical distribution information of the decomposed sequences. To address this, we implement sliding normalization directly on the original sequence. Consequently, the resulting distribution information is utilized for predicting future distributions, while the output stationary sequence is weighted with the stationary sequence derived from FDN.

Stationary Sequences Weighting.Two stationary sequences from FDN and TDN will be weighted to a stationary output, which can be expressed as follows:

\[}^{i}=}^{i}+}^{i}.\] (6)

Here, \(\) is a trainable parameter and \(=1-\). The weighted \(}^{i}\) serves as the final stationary sequence, which is then inputted into FM for stable forecasting.

### Non-stationarity Reconstruction

We acquire two sets of sliding statistics reflecting distribution variations after FDN and TDN. Later, we refer to the structure of existing distribution prediction works [34; 12] to predict future distribution. Initially, we calculate the mean value of each sliding statistic to compute the statistical differences. Subsequently, the difference and original series are inputted for future difference prediction. Ultimately, these predicted differences added to the mean value as future sliding statistics.

Frequency Domain Prediction.As shown in Figure 3, for the distribution statistics of FDN, we predict future statistics by distribution prediction model and formulate as:

\[}^{i}_{}=(}^ {i}-^{i}_{f},^{i}),}^{i}_{*}=}^{i}_{}+^{i}_{f},\] (7) \[}^{i}_{}=(}^{i}- ^{i}_{f},^{i}-^{i}_{f}),}^{i}_{*}=}^{i}_{}+^{i}_{f}.\]

Here, \(^{i}_{f}\) and \(^{i}_{f}\) are the mean values of \(}^{i}\) and \(}^{i}\). While \(}^{i}_{*}\) and \(}^{i}_{*}\) denote the prediction of \(}^{i}\) and \(}^{i}\). The \(\) is a mean prediction branch, and the \(\) is a standard deviation prediction branch. They are affiliated with DPM and adopt the same network structure.

Time Domain PredictionAs the above frequency domain prediction process, for the distribution statistics of TDN, this prediction process can be formulated as follows:

\[^{i}_{}=(^{i}-^ {i}_{o},^{i}),^{i}_{*}=^{i}_{}+ ^{i}_{o},\] (8) \[^{i}_{}=(^{i}-^{i}_{o}, ^{i}-^{i}_{o}),^{i}_{*}=^{i}_{}+ ^{i}_{o}.\]

Here, \(^{i}_{o}\) and \(^{i}_{o}\) are the mean values of \(^{i}\) and \(^{i}\), respectively. Likewise, \(^{i}_{*}\) and \(^{i}_{*}\) denote the prediction of \(^{i}\) and \(^{i}\). The \(\) and the \(\) are noted in frequency domain prediction.

De-normalization.After the aforementioned distribution predictions, two sets of estimated distribution statistics will be gained. Which are weighted to reconstruct the non-stationary information of the output of the time series forecasting model. This process can be described as follows:

\[_{*}^{i}=_{*}^{i} +}_{*}^{i}, _{*}^{i}=_{*}^{i}+}_{*}^{i },}^{i}=}^{i} +}^{i}.\\ _{*}^{i}=}_{*}^{i}( _{*}^{i}+)+_{*}^{i}.\] (9)

Where \(}_{*}^{i}\) is the output of the time series forecasting model, and \(_{*}^{i}\) represents the predicted sequence after reconstructing non-stationary information. While \(\), \(\), and \(\) mentioned before.

### Collaborative Training

Distribution prediction and future series forecasting are essentially a bi-level optimization problem [35; 36; 37], where distribution outputs significantly impact the future series output. To enhance the training effects of our models, we pre-train the DPM to yield a relatively well-trained DPM. This procedure can be formulated as follows:

\[_{d}=_{}((_{*}^{i}, _{*}^{i}),(_{y}^{i}, {}_{y}^{i}),_{d}).\] (10)

Where \(_{d}\) represents the parameters of the DPM, it is noteworthy that the wavelet bases \(_{l,h}\) and the weighted factor \(\) belong to \(_{d}\). We select the mean square error (MSE) as our loss function between the predicted distribution and the ground truth of the distribution, acquired from the future sequence through TDN. Subsequently, assuming a total training duration of \(T\) epochs, the parameters \(_{d}\) of the DPM will be frozen, then we train FM for \(T_{1}\) epochs. Finally, DPM and FM will be subject to collaborative training during the remaining \(T-T_{1}\) epochs. The process is as follows:

\[_{f}&=_{} (_{*}^{i},_{y}^{i},_{f}), t T_{1},\\ \{_{d},_{f}\}&=_{ }(_{*}^{i},_{y}^{i},\{_ {d},_{f}\}),.\] (11)

Where \(t\) denotes the \(t^{th}\) epoch during the training process, and \(_{f}\) represents the parameters of the FM. We train DPM and FM concurrently to mitigate potential errors in the pretraining stage, as the ground truth of distribution is drived from future sequences based on distribution assumption. This ground truth is somewhat inconsistent with the actual situation and fails to account for high-frequency distribution changes. Consequently, we pretrain the DPM using assumption-based distribution ground truth, and then collaborative train it jointly based on the loss derived from the future sequences.

## 4 Experiments

In this section, we conduct comprehensive experiments on multiple real-world time series datasets to assess the effectiveness of our proposed reversible normalization method DDN.

Figure 3: The architecture of the distribution prediction model primarily consists of two predictive branches: the Mean Prediction branch and the Standard Deviation (Std) Prediction branch. The specific network structure of these branches is illustrated in the Prediction Branch.

DatasetsWe conduct extensive experiments on these seven popular real-world datasets , including **Electricity Transformer Temperature (ETT)** with its four subsets (ETTh1, ETH2, ETTm1, ETTm2), **Weather**, **Electricity**, and **Traffic**. The setting of these datasets following original works , and more descriptions about these datasets present in appendix A.1.

Baselines.DDN is a model-agnostic method that can be applied to any mainstream time series forecasting model. To demonstrate its versatility, we integrate DDN into several representative models, including the earlier proposed models **FEDformer** and **Autoformer**, the CI model **DLinear**, and the CD model **iTransformer**.

Implementation details.Our experiments were conducted three times with a consistent random seed and averaged to mean values. The Mean Square Error (MSE) and Mean Absolute Error (MAE) are chosen as evaluation metrics, with MSE serving as the training loss. All models use the same prediction lengths \(T=\{96,192,336,720\}\). For the look-back window \(L\), Autoformer  and FEDformer  use \(L=96\), while DLinear  and iTransformer  utilize \(L=336\) and \(L=720\) respectively. The wavelet bases initialize to the "coiflet" bases, the default size of our sliding window is set to 7 for information content and temporal locality balance, and \(\) starts at zero. More implementation details of our experiments can be referred to appendix A.2.

### Main Results

As illustrated, the DDN method significantly enhances the predictive performance of the four different baselines across nearly all datasets. For the MSE metric, this improvement is particularly evident in the three relatively large datasets: Weather, Electricity, and Traffic. Utilizing DDN, Autoformer achieves a relative error reduction of 19.2%, 24.7%, and 25.6%, respectively, while FEDformer achieves a relative error reduction of 13.1%, 16.2%, and 22.3%. Similarly, incorporating the DDN into the other models also results in substantial performance gains. Additionally, Autoformer, FEDformer, and DLinear do not employ reversible normalization in official implements. While iTransformer utilizes the RevIN  normalization technique based on static statistics. However, replacing the RevIN module in iTransformer with the DDN module still yields significant performance improvements. These results strongly demonstrate that DDN makes the baseline model more robust in forecasting.

    &  &  &  &  &  &  & iTransformer &  \\   & & & & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   & 96 & 0.458 & 0.448 & **0.427** & **0.424** & **0.371** & 0.411 & 0.385 & **0.408** & 0.377 & 0.399 & **0.372** & **0.396** & 0.392 & 0.422 & **0.377** & **0.405** \\  & 192 & 0.481 & 0.474 & **0.472** & **0.452** & **0.420** & 0.443 & **0.415** & **0.425** & 0.417 & 0.426 & **0.406** & **0.416** & 0.428 & 0.448 & **0.441** & **0.430** \\  & 336 & 0.508 & 0.485 & **0.498** & **0.466** & **0.446** & 0.459 & 0.458 & **0.452** & 0.464 & 0.461 & **0.432** & **0.434** & 0.467 & 0.475 & **0.453** & **0.456** \\  & 720 & 0.525 & 0.516 & **0.502** & **0.483** & **0.482** & 0.495 & 0.490 & **0.479** & 0.493 & 0.505 & **0.462** & **0.474** & 0.568 & 0.547 & **0.553** & **0.530** \\   & 96 & 0.493 & 0.470 & **0.354** & **0.390** & 0.362 & 0.408 & **0.313** & **0.364** & 0.301 & 0.344 & **0.288** & **0.342** & 0.322 & 0.371 & **0.301** & **0.355** \\  & 192 & 0.546 & 0.498 & **0.397** & **0.408** & 0.395 & 0.427 & **0.361** & **0.396** & 0.335 & 0.356 & **0.324** & **0.364** & 0.353 & 0.392 & **0.339** & **0.378** \\  & 336 & 0.658 & 0.543 & **0.429** & **0.433** & 0.441 & **0.454** & **0.417** & **0.430** & 0.370 & 0.387 & **0.356** & **0.385** & 0.410 & **0.370** & **0.396** \\  & 270 & 0.626 & 0.532 & **0.488** & **0.464** & 0.488 & 0.481 & **0.470** & **0.472** & 0.425 & 0.421 & **0.415** & **0.419** & 0.441 & **0.432** & **0.426** & **0.426** \\   & 96 & 0.247 & 0.320 & **0.190** & **0.243** & 0.246 & 0.328 & **0.174** & **0.237** & 0.175 & 0.237 & **0.146** & **0.201** & 0.177 & 0.228 & **0.148** & **0.210** \\  & 192 & 0.302 & 0.366 & **0.231** & **0.282** & 0.281 & 0.341 & **0.233** & **0.294** & 0.217 & 0.275 & **0.190** & **0.247** & 0.223 & 0.266 & **0.191** & **0.252** \\  & 336 & 0.362 & 0.394 & **0.289** & **0.327** & 0.337 & 0.376 & **0.307** & **0.349** & 0.263 & 0.314 & **0.239** & **0.288** & 0.287 & 0.310 & **0.237** & **0.290** \\  & 720 & 0.427 & 0.433 & **0.369** & **0.375** & 0.414 & 0.426 & **0.399** & **0.405** & 0.325 & 0.366 & **0.311** & **0.343** & 0.364 & 0.365 & **0.301** & **0.336** \\   & 96 & 0.195 & 0.309 & **0.150** & **0.254** & 0.185 & 0.300 & **0.146** & **0.251** & 0.140 & 0.237 & **0.131** & **0.228** & 0.133 & 0.229 & **0.127** & **0.225** \\  & 192 & 0.215 & 0.325 & **0.173** & **0.275** & 0.196 & 0.31

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Conclusion

In this work, we propose Dual-domain Dynamic Normalization (DDN), a novel method that dynamically captures non-stationary factors in time series forecasting, addressing sudden changes and distribution drifts in both time and frequency domains. Specifically, DDN employs sliding normalization in the time domain to eliminate and reconstruct non-stationary factors at a fine-grained level. In the frequency domain, it decomposes time series into high and low frequencies, effectively capturing rapid variations and sudden changes. As a model-agnostic auxiliary module, DDN significantly enhances the predictive performance of various forecasting models. Extensive experiments on seven real-world datasets validate the superiority of DDN, demonstrating its effectiveness in addressing distribution drift and improving the reliability of time series predictions.

## 6 Acknowledgments

This work is supported in part by the National Natural Science Foundation of China, under Grant (62302309, 62171248), Shenzhen Science and Technology Program (JCYJ20220818101014030, JCYJ20220818101012025), and the PCNL KEY project (PCL2023AS6-1).