# Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization

Haonan Yuan\({}^{1,2}\), Qingyun Sun\({}^{1,2,}\), Xingcheng Fu\({}^{3}\), Ziwei Zhang\({}^{4}\), Cheng Ji\({}^{1,2}\),

Hao Peng\({}^{1}\), Jianxin Li\({}^{1,2}\)

\({}^{1}\)Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University

\({}^{2}\)School of Computer Science and Engineering, Beihang University

\({}^{3}\)Key Lab of Education Blockchain and Intelligent Technology, Guangxi Normal University

\({}^{4}\)Department of Computer Science and Technology, Tsinghua University

{yuanhn,sunqy,jicheng,penghao,lijx}@buaa.edu.cn

fuxc@gxnu.edu.cn, zwzhang@tsinghua.edu.cn

Corresponding author

###### Abstract

Dynamic graph neural networks (DGNNs) are increasingly pervasive in exploiting spatio-temporal patterns on dynamic graphs. However, existing works fail to generalize under distribution shifts, which are common in real-world scenarios. As the generation of dynamic graphs is heavily influenced by latent environments, investigating their impacts on the out-of-distribution (OOD) generalization is critical. However, it remains unexplored with the following two major challenges: **(1)** How to properly model and infer the complex environments on dynamic graphs with distribution shifts? **(2)** How to discover invariant patterns given inferred spatio-temporal environments? To solve these challenges, we propose a novel **E**nvironment-**A**ware dynamic **G**raph **LE**arning (**E**agle)** framework for OOD generalization by modeling complex coupled environments and exploiting spatio-temporal invariant patterns. Specifically, we first design the environment-aware EA-DGNN to model environments by multi-channel environments disentangling. Then, we propose an environment instantiation mechanism for environment diversification with inferred distributions. Finally, we discriminate spatio-temporal invariant patterns for out-of-distribution prediction by the invariant pattern recognition mechanism and perform fine-grained causal interventions node-wisely with a mixture of instantiated environment samples. Experiments on real-world and synthetic dynamic graph datasets demonstrate the superiority of our method against state-of-the-art baselines under distribution shifts. To the best of our knowledge, we are the first to study OOD generalization on dynamic graphs from the environment learning perspective.

## 1 Introduction

Dynamic graphs are ubiquitous in the real world, like social networks [6; 25], financial transaction networks [61; 93], traffic networks [47; 97; 96], _etc._, where the graph topology evolves as time passes. Due to their complexity in both spatial and temporal correlation patterns, wide-ranging applications such as relation prediction [39; 68], anomaly detection [10; 69], _etc._ are rather challenging. With excellent expressive power, dynamic graph neural networks (DGNNs) achieve outstanding performance in dynamic graph representation learning by combining the merits of graph neural network (GNN) based models and sequential-based models.

Despite the popularity, most existing works are based on the widely accepted I.I.D. hypothesis, _i.e._, the training and testing data both follow the independent and identical distribution, which fails to generalize well  under the out-of-distribution (OOD) shifts in dynamic scenarios. Figure 1(a) illustrates a toy example demonstrating the OOD shifts on a dynamic graph. The failure lies in: (1) the exploited predictive patterns have changed for in-the-wild extrapolations, causing spurious correlations to be captured  and even strengthened with spatio-temporal convolutions. (2) performing causal intervention and inference without considering the impact of environments, introducing external bias of label shifting . These can be explained by SCM model  in Figure 1(b).

As the generation of dynamic graphs is influenced by complicated interactions of multiple varying latent environment factors , like heterogeneous neighbor relations, multi-community affiliations, _etc._, investigating the roles environments play in OOD generalization is of paramount importance. To tackle the issues above, we study the problem of generalized representation learning under OOD shifts on dynamic graphs in an environment view by carefully exploiting spatio-temporal invariant patterns. However, this problem is highly challenging in:

* How to understand and model the way latent environments behave on dynamic graphs over time?
* How to infer the distribution of underlying environments with the observed instances?
* How to recognize the spatio-temporal invariant patterns over changing surrounding environments?

To address these challenges, we propose a novel **E**nvironment-**A**ware dynamic **G**raph **LE**arning (**E**agle) framework for OOD generalization, which handles OOD generalization problem by exploiting spatio-temporal invariant patterns, thus filtering out spurious correlations via causal interventions over time. Our **E**agle solves the aforementioned challenges and achieves the OOD generalization target as follows. **Firstly**, to shed light on environments, we design the environment-aware EA-DGNN to model environments by multi-channel environments disentangling. It can be considered as learning the disentangled representations under multiple spatio-temporal correlated environments, which makes it possible to infer environment distributions and exploit invariant patterns. **Then**, we propose an environment instantiation mechanism for environment diversification with inferred environment distributions by applying the multi-label variational inference. This endows our **E**agle with higher generalizing power of potential environments. **Lastly**, we propose a novel invariant pattern recognition mechanism for out-of-distribution prediction that satisfies the Invariance Property and Sufficient Condition with theoretical guarantees. We then perform fine-grained causal interventions with a mixture of observed and generated environment instances to minimize the empirical and invariant risks. We optimize the whole framework by training with the adapted min-max strategy, encouraging **E**agle to generalize to diverse environments while learning on the spatio-temporal invariant patterns. Contributions of this paper are summarized as follows:

Figure 1: (a) shows the “user-item” interactions with heavy distribution shifts when the underlying environments change (_e.g._, seasons). A model could mistakenly predict that users would purchase an Iced Americano instead of a Hot Latte if it fails to capture spatio-temporal invariant patterns among spurious correlations. (b) analyzes the underlying cause of distribution shifts on dynamic graphs. It is broadly convinced  that the spurious correlations of \(C\) (coffee) \( S\) (cold drink) within and between graph snapshots lead to poor OOD generalization under distribution shifts, which need to be filtered out via carefully investigating the impact of latent \(\). (Detailed discussions in Appendix F.1)

* We propose a novel framework named Eagle for OOD generalization by exploiting spatio-temporal invariant patterns with respect to environments. To the best of our knowledge, this is the first trial to explore the impact of environments on dynamic graphs under OOD shifts.
* We design the Environment "Modeling-Inferring-Discriminating-Generalizing" paradigm to endow Eagle with higher extrapolation power for future potential environments. Eagle can carefully model and infer underlying environments, thus recognizing the spatio-temporal invariant patterns and performing fine-grained causal intervention node-wisely with a mixture of observed and generated environment instances, which generalize well to diverse OOD environments.
* Extensive experiments on both real-world and synthetic datasets demonstrate the superiority of our method against state-of-the-art baselines for the future link prediction task.

## 2 Problem Formulation

In this section, we formulate the OOD generalization problem on dynamic graphs. Random variables are denoted as **bold** symbols while their realizations are denoted as \(italic\) letters. Notation with its description can be found in Appendix A.

**Dynamic Graph Learning.** Denote a dynamic graph as a set of discrete graphs snapshots \(=\{\}_{t=1}^{T}\), where \(T\) is the time length , \(^{t}=(^{t},^{t})\) is the graph snapshot at time \(t\), \(^{t}\) is the node set and \(^{t}\) is the edge set. Let \(^{t}\{0,1\}^{N N}\) be the adjacency matrix and \(^{t}^{N d}\) be the matrix of node features, where \(N=|^{t}|\) denotes the number of nodes and \(d\) denotes the dimensionality. As the most challenging task on dynamic graphs, future link prediction task aims to train a predictor \(f_{}:\{0,1\}^{N N}\) that predicts the existence of edges at time \(T+1\) given historical graphs \(^{1:T}\). Concretely, \(f_{}=w g\) is compound of a DGNN \(w()\) for learning node representations and a link predictor \(g()\) for predicting links, _i.e._, \(^{1:T}=w(^{1:T})\) and \(^{T}=g(^{1:T})\)

Figure 2: The framework of Eagle. Our proposed method jointly optimizes the following modules: (1) For a given dynamic graph generated under latent environments, the EA-DGNN first models environment by multi-channel environments disentangling. (2) The environment instantiation mechanism then infers the latent environments, followed by environment diversification with inferred distributions. (3) The invariant pattern recognition mechanism discriminates the spatio-temporal invariant patterns for OOD prediction. (4) Finally, Eagle generalizes to diverse environments by performing fine-grained causal interventions node-wisely with a mixture of environment instances. The Environment “Modeling-Inferring-Discriminating-Generalizing” paradigm endows Eagle with higher generalizing power for future potential environments.

The widely adopted Empirical Risk Minimization (ERM)  learns the optimal \(f^{}_{}\) as follows:

\[_{}_{(^{1:T},Y^{T}) p(^{1:T}, ^{T})}[(f_{}(^{1:T}),Y ^{T})].\] (1)

**OOD Generalization with Environments.** The predictor \(f^{}_{}\) learned in Eq. (1) may not generalize well to the testing set when there exist distribution shifts, _i.e._, \(p_{}(^{1:T},^{T}) p_{}( ^{1:T},^{T})\). A broadly accepted assumption is that the distribution shifts on graphs are caused by multiple latent environments \(\)[86; 71; 8; 24; 3], which influence the generation of graph data, _i.e._, \(p(^{1:T},^{T})=p(^{1:T} )p(^{T}^{1:T},)\). Denoting \(\) as the support of environments across \(T\) time slices, the optimization objective of OOD generalization can be reformulated as:

\[_{}_{}_{(^{1:T },Y^{T}) p(^{1:T},^{T}|)}[(f_ {}(^{1:T}),Y^{T})],\] (2)

where the min-max strategy minimizes ERM under the worst possible environment. However, directly optimizing Eq. (2) is infeasible as the environment \(\) cannot be observed in the data. In addition, the training environments may not cover all the potential environments in practice, _i.e._, \(_{}_{}\).

## 3 Method

In this section, we introduce **E**agle, an **E**nvironment-**A**ware dynamic **G**raph **L**E**arning framework, to solve the OOD generalization problem. The overall architecture of **E**agle is shown in Figure 2, following the **Environment "Modeling-Inferring-Discriminating-Generalizing"** paradigm.

### Environments Modeling with Environment-aware DGNN

To optimize the objectives of OOD generalization, we first need to infer the environments of dynamic graphs. In practice, the formation of real-world dynamic graphs typically follows a complex process under the impact of underlying environments [56; 51]. For example, the relationships in social networks are usually multi-attribute (_e.g._, colleagues, classmates, _etc._) and are changing over time (_e.g._, past classmates, current colleagues). Consequently, the patterns of message passing and aggregation in both spatial and temporal dimensions should be diversified in different environments, while the existing holistic approaches [94; 52] fail to distinguish them. To model spatio-temporal environments, we propose an **E**nvironment-**A**ware **D**ynamic **G**raph **N**eural **N**etwork (**EA-DGNN**) to exploit environment patterns by multi-channel environments disentangling.

**Environment-aware Convolution Layer (EAConv).** We perform convolutions following the "spatial first, temporal second" paradigm as most DGNNs do . We consider that the occurrence of links is decided by \(K\) underlying environments \(_{v}=\{_{k}\}_{k=1}^{K}\) of the central node \(v\), which can be revealed by the features and structures of local neighbors. To obtain representations under multiple underlying environments, EAConv firstly projects the node features \(^{t}_{v}\) into \(K\) subspaces corresponding to different environments. Specifically, the environment-aware representation of node \(v\) with respect to the \(k\)-th environment \(_{k}\) at time \(t\) is calculated as:

\[^{t}_{v,k}=(^{}_{k}(^{t}_{v }(t))+_{k}),\] (3)

where \(_{k}^{d d^{}}\) and \(_{k}^{d^{}}\) are learnable parameters, \(()\) is the relative time encoding function, \(d^{}\) is the dimension of \(^{t}_{v,k}\), \(()\) is the activation function (_e.g._, the Sigmoid function), and \(\) is the element-wise addition.

To reflect the varying importance of environment patterns under \(K\) embedding spaces, we perform multi-channel convolutions with spatio-temporal aggregation reweighting for each \(_{k}\). Let \(^{t}_{(u,v),k}\) be the weight of edge \((u,v)\) under \(_{k}\) at time \(t\). The spatial convolutions of one EAConv layer are formulated as (note that we omit the layer superscript for brevity):

\[}^{t}_{v,k} =^{t}_{v,k}+_{u^{t}(v)}^{t} _{(u,v),k}^{t}_{u,k},\] (4) \[^{t}_{(u,v),k} =^{t}_{u,k})^{}^{t}_{v,k})}{ _{k^{}=1}^{K}((^{t}_{u,k^{}})^{}^{t }_{v,k^{}})},\] (5)where \(^{t}(v)\) is the neighbors of node \(v\) at time \(t\) and \(}\) denotes the updated node embedding. We then concatenate \(K\) embeddings from different environments as the final node representation \(}_{v}^{,t}=\|k_{k=1}^{K}}_{v,k}^{}^{K d^{}}\), where \(\|\) means concatenation. After that, we conduct temporal convolutions holistically here for graph snapshots at time \(t\) and before:

\[_{v}^{,t}=_{=1}^{t}}_{v}^ {,}^{K d^{}}, {}_{v}^{,}=[}_{v,1}^{}\|}_{v,2}^{}\|\|}_{v,K}^{}].\] (6)

Note that, the temporal convolutions can be easily extended to other sequential convolution models or compounded with any attention/reweighting mechanisms.

**EA-DGNN Architecture.** The overall architecture of EA-DGNN is similar to general GNNs by stacking \(L\) layers of EAConv. In a global view, we exploit the patterns of environments by reweighting the multi-attribute link features with \(K\) channels. In the view of an individual node, multiple attributes in the link with neighbors can be disentangled to different attention when carrying out convolutions on both spatial and temporal dimensions. We further model the environment-aware representations by combining node representations at each time snapshot \(_{v}^{}=_{t=1}^{T}\{_{v}^{,t} \}^{T(K d^{})}\).

### Environments Inferring with Environment Instantiation Mechanism

To infer environments with the environmental patterns, we propose an environment instantiation mechanism with multi-label variational inference, which can generate environment instances given multi-labels of time \(t\) and environment index \(k\).

**Environment Instantiation Mechanism** is a type of data augmentation to help OOD generalization. We regard the learned node embeddings \(_{v,k}^{t}\) as environment samples drawn from the ground-truth distribution of latent environment \(\). Particularly, we optimize a multi-label **E**nvironment-aware **C**onditional **V**ariational **A**utoEncoder (**ECVAE**) to infer the distribution of \( q_{}(,)\) across \(T\). We have the following proposition. Proof for Proposition 1 can be found in Appendix C.1.

**Proposition 1**.: Given observed environment samples from the dynamic graph \(^{1:T}\) denoted as

\[=_{v V}_{k=1}^{K}_{t=1}^{T}\{_{v, k}^{t}\}^{(|| K T) d^{}} }_{}\] (7)

with their corresponding one-hot multi-labels \(\), the environment variable \(\) is drawn from the prior distribution \(p_{}()\) across \(T\) time slices, and \(\) is generated from the distribution \(p_{}(,)\). Maximizing the conditional log-likelihood \( p_{}()\) leads to an optimal ECVAE by minimizing:

\[_{}=[q_{}(, )\|p_{}()]-|}_ {i=1}^{||} p_{}(,^{(i)}),\] (8)

where \([\|]\) is the Kullback-Leibler (KL) divergence , \(||\) is the number of observed environment samples, \(^{(i)}\) is the \(i\)-th sampling by the reparameterization trick.

**Sampling and Generating.** Proposition 1 demonstrates the training objectives of the proposed ECVAE. We realize ECVAE using fully connected layers, including an environment recognition network \(q_{}(,)\) as the encoder, a conditional prior network \(p_{}()\) as observed environment instantiations, and an environment sample generation network \(p_{}(,)\) as the decoder. By explicitly sampling latent \(_{i}\) from \(p_{}()\), we can instantiate environments by generating samples \(_{k_{i}}^{t_{i}}\) from the the inferred distribution \(p_{}(_{i},_{i})\) with any given one-hot multi-label \(_{i}\) mixed up with the environment index \(k_{i}\) and time index \(t_{i}\). We denote the generated samples as \(_{}\), which can greatly expand the diversity of the observed environments \(_{}\) in Eq. (7).

### Environments Discriminating with Invariant Pattern Recognition

Inspired by the Independent Causal Mechanism (ICM) assumption [64; 65; 67], we next propose to learn the spatio-temporal invariant patterns utilizing our inferred and instantiated environments.

**Invariant Pattern Recognition Mechanism.** To encourage the model to rely on the invariant correlation that can generalize under distribution shifts, we propose an invariant pattern recognition mechanism to exploit spatio-temporal invariant patterns. We make the following assumption.

**Assumption 1** (Invariant and Sufficient).: Given the dynamic graph \(^{1:T}\), each node is associated with \(K\) surrounding environments. There exist spatio-temporal invariant patterns that can lead to generalized out-of-distribution prediction across all time slices. The function \(^{*}()\) can recognize the spatio-temporal invariant patterns \(^{I}_{}\) and variant patterns \(^{V}_{}\) node-wisely, satisfying:

**(a) Invariance Property**: \(\), \(^{*}(^{1:T})^{I}_{}\), _s.t._\(p(^{T}^{I}_{},)=p(^ {T}^{I}_{})\);

**(b) Sufficient Condition**: \(^{T}=g(^{1:T}_{I})+\), _i.e._, \(^{T}\!\!\!^{V}_{}^{I}_ {}\), where \(^{1:T}_{I}\) denotes representations under \(^{I}_{}\) over time, \(g()\) is the link predictor, \(\) is an independent noise.

Assumption 1 indicates that the modeled environment-aware representations of nodes under \(^{I}_{}\) contain a portion of spatio-temporal invariant causal features that contribute to generalized OOD prediction over time. Invariance Property guarantees for any environment \(\), function \(^{*}()\) can always recognize the invariant patterns \(^{I}_{}\) with the observation of \(^{1:T}\), and Sufficient Condition ensures the information contained in \(^{1}_{}\):\(T\) is adequate for the link predictor to make correct predictions.

Next, we show that \(^{*}()\) in Proposition 2 can be instantiated with the following proposition.

**Proposition 2** (A Solution for \(^{*}()\)).: Denote \(^{}_{v}=[^{}_{v,1},^{ }_{v,2},,^{}_{v,K}]\), where \(^{}_{v,k}=_{t=1}^{T}^{t}_{v,k}\). Let \((^{}_{v})^{K}\) represents the variance of \(K\) environment-aware representations. The Boolean function \(()\) is a solution for \(^{*}()\) with the following state update equation:

\[(i,j)=(i-1,j)(i-1,j-(^{}_{v})[i-1]),&j(^{ }_{v})[i-1]\\ (i-1,j),&,\] (9)

\((i,j)\) indicates whether it is feasible to select from the first \(i\) elements in \((^{}_{v})\) so that their sum is \(j\). Traversing \(j\) in reverse order from \((^{}_{v})/2\) until satisfying \((K,j)\) is True, we reach:

\[_{v}=(^{}_{v})-2j,\] (10) \[^{I}_{}(v)=\{_{k}(^{}_{v})[k]( ^{}_{v})-}{2}\},^{V}_{}(v)=^{I}_{}(v)},\] (11)

where \(_{v}\) is the optimal spatio-temporal invariance threshold of node \(v\).

Proposition 2 solves a dynamic programming problem to obtain the optimal \(^{*}()\), whose target is to find a partition dividing all patterns into variant/invariant set with the maximum difference between the variance means that reveals the current invariance status, providing an ideal optimizing start point. The proof can be found in Appendix C.2.

With the theoretical support of Assumption 1 and Proposition 2, we can recognize the spatio-temporal invariant/variant patterns \(^{I}_{}=_{v}^{I}_{ }(v)\) and \(^{V}_{}=_{v}^{V}_{ }(v)\) with respect to the underlying environments for each node over a period of time by applying \(()\).

### Environments Generalizing with Causal Intervention

**Generalization Optimization Objective.** Eq. (2) clarifies the training objective of OOD generalization. However, directly optimizing Eq. (2) is not impracticable. Based on the inferred and generated environments in Section 3.2 and spatio-temporal invariant patterns learned in Section 3.3, we further modify Eq. (2) to improve the model's OOD generalization ability by applying the intervention mechanism node-wisely with causal inference. Specifically, we have the following objectives:

\[_{}_{}+ _{},\] (12) \[_{}=_{ q_{}( ),(^{1:T},Y^{T}) p(^{1:T},^{T}| )}[(g(^{1:T}_{I}),Y^{T})],\] (13) \[_{}=_{s}\{ _{ q_{}(),(^{1:T},Y^{T}) p (^{1:T},^{T}|)}[(f_{}(^{1:T}),Y^{T}(^{1:T}_{ V}=s))]\},\] (14)

where \(q_{}()\) is the environment distribution in Section 3.2, \(()\) is the \(do\)-calculus that intervenes the variant patterns, \(=_{}_{}\) is the observed and generated environment instances for interventions, and \(\) is a hyperparameter. We show the optimality of Eq. (12) with the following propositions:

**Proposition 3** (Achievable Assumption).: Minimizing Eq. (12) can encourage the model to satisfy the Invariance Property and Sufficient Condition in Assumption 1.

[MISSING_PAGE_FAIL:7]

Though dynamic GNNs are designed to capture spatio-temporal features, they even underperform static GNNs in several settings, mainly because the predictive patterns they exploited are variant with spurious correlations. Conventional OOD generalization baselines have limited improvements as they rely on the environment labels to generalize, which are inaccessible on dynamic graphs. As the most related work, DIDA  achieves further progress by learning invariant patterns. However, it is limited by the lack of in-depth analysis into the environments, causing the label shift phenomenon  that damages its generalization ability. With the critical investigation of latent environments and theoretical guarantees, our Eagle consistently outperforms the baselines and achieves the best performance in all _w/ OOD_ settings and two _w/o OOD_ settings. Especially even on the most challenging COLLAB, where the time span is extremely long (1990 to 2006) and its link attributes difference is huge.

#### 4.1.2 Distribution Shifts of Node Features

**Setting.** We further introduce settings of node feature shifts on COLLAB. We respectively sample \(p(t)|^{t+1}|\) positive links and \((1-p(t))|^{t+1}|\) negative links, which are then factorized into shifted features \(^{t}^{|| d}\) while preserving structural property. The sampling probability \(p(t)=+(t)\), where \(^{t}\) with higher \(p(t)\) will have stronger spurious correlations with future underlying environments. We set \(\) to be 0.4, 0.6 and 0.8 for training and 0.1 for testing. In this way, training data are relatively higher spuriously correlated than the testing data. We omit results on static GNNs as they cannot support dynamic node features. Detailed settings are in Appendix D.3.

**Results.** Results are reported in Table 2. We can observe a similar trend as Table 1, _i.e._, our method better handles distribution shifts of node features on dynamic graphs than the baselines. Though some baselines can report reasonably high performance on the training set, their performance drops drastically in the testing stage. In comparison, our method reports considerably smaller gaps. In particular, our Eagle surpasses the best-performed baseline by approximately 3%/5%/10% of AUC in the testing set under different shifting levels. Interestingly, as severer distribution shifts will lead to more significant performance degradation where the spurious correlations are strengthened, our method gains better task performance with a higher shifting degree. This demonstrates Eagle is more capable of eliminating spatio-temporal spurious correlations in severer OOD environments.

### Investigation on Invariant Pattern Recognition Mechanism

**Settings.** We generate synthetic datasets by manipulating environments to investigate the effect of the invariant pattern recognition mechanism. We set \(K=5\) and let \(_{}\) represent the proportion of the environments in which the invariant patterns are learned, where higher \(_{}\) means more reliable invariant patterns. Node features are drawn from multivariate normal distributions, and features under the invariant patterns related \(_{k}\) will be perturbed slightly and vice versa. We construct graph structures and filter out links built under a certain \(_{k}\) as the same in Section 4.1.1. We compare Eagle with the most related and strongest baseline DIDA . Detailed settings are in Appendix D.3.

**Results.** Results are shown in Figure 3. \(_{}\) denotes the prediction accuracy of the invariant patterns by \(()\). We observe that, as \(_{}\) increases, the performance of Eagle shows a significant increase from 54.26% to 65.09% while narrowing the gap between _w/o OOD_ and _w/ OOD_ scenarios.

  
**Dataset** &  &  &  \\ 
**Model** & _w/o OOD_ & _w/OOD_ & _w/o OOD_ & _w/o OOD_ & _w/ OOD_ \\  GAE  & 77.15±0.50 & 74.04±0.75 & 70.67±1.11 & 64.45±5.02 & 72.31±0.53 & 60.27±0.41 \\ VGAE  & 86.47±0.04 & 74.95±1.25 & 76.54±0.50 & 65.33±1.43 & 79.18±0.47 & 66.29±1.33 \\ GCRN  & 82.78±0.54 & 69.72±0.46 & 68.95±1.05 & 68.64±5.97 & 76.28±0.51 & 64.35±1.24 \\ EvoleyGN  & 86.62±0.95 & 76.15±0.91 & 78.21±0.03 & 53.82±2.06 & 74.5±0.33 & 63.17±1.05 \\ DySAT  & 88.77±0.23 & 76.59±0.20 & 78.87±0.57 & 66.09±1.42 & 78.52±0.40 & 66.55±1.21 \\ IRM  & 87.96±0.90 & 75.42±0.87 & 66.49±10.78 & 56.02±16.08 & 80.02±0.57 & 69.19±1.35 \\ V-REx  & 88.13±0.32 & 76.24±0.77 & 79.04±0.16 & 66.41±1.87 & 83.11±0.29 & 70.15±1.09 \\ GroupDRO  & 88.76±0.12 & 76.33±0.29 & **79.38±0.42** & 66.97±0.61 & 85.19±0.53 & 74.35±1.62 \\ DIDA  & 91.97±0.05 & 81.87±0.40 & 78.22±0.40 & 75.92±0.90 & 89.84±0.82 & 78.64±0.97 \\ 
**Eagle** & **92.45±0.21** & **84.41±0.87** & 78.97±0.31 & **77.26±0.74** & **92.37±0.53** & **82.70±0.72** \\   

Table 1: AUC score (% \(\) standard deviation) of future link prediction task on real-world datasets with OOD shifts of link attributes. The best results are shown in **bold** and the runner-ups are underlined.

Though DIDA  also shows an upward trend, its growth rate is more gradual. This indicates that, as DIDA  is incapable of modeling the environments, it is difficult to perceive changes in the underlying environments caused by different \(_{}\), thus cannot achieve satisfying generalization performance. In comparison, our Eagle can exploit more reliable invariant patterns, thus performing high-quality invariant learning and efficient causal interventions, and achieving better generalization ability. In addition, we also observe a positive correlation between \(_{}\) and the AUC, indicating the improvements is attributed to the accurate recognition of the invariant patterns by \(()\). Additional results are in Appendix D.7.

### Ablation Study

In this section, we conduct ablation studies to analyze the effectiveness of three main mechanisms:

* **Eagle (w/o EI).** We remove the **E**nvironment **I**nstantiation mechanism in Section 3.2, and carrying out causal interventions in Eq. (15) with only observed environment samples.
* **Eagle (w/o IPR).** We remove the **I**nvariant **P**attern **R**ecognition mechanism in Section 3.3, and determining the spatio-temporal invariant patterns \(_{}^{I}\) by randomly generating \(_{v}\) for each node.
* **Eagle (w/o Interv).** We remove the spatio-temporal causal **Interv**ention mechanism in Section 3.4 and directly optimize the model by Eq. (16) without the second \(_{}\) term.

**Results.** Results are demonstrated in Figure 4. Overall, Eagle consistently outperforms the other three variants on all datasets. The ablation studies provide insights into the effectiveness of the proposed mechanisms and demonstrate their importance in achieving better performance for OOD generalization on dynamic graphs.

### Visualization

We visualize snapshots in COLLAB using NetworkX  as shown in Figure 5, where colors reflect latent environments, numbers denote edge weights, solid lines implicate spatio-temporal invariant patterns dependencies, and dashed lines implicate variant patterns dependencies. Eagle can gradually exploit the optimal invariant patterns and strengthen reliance, making generalized predictions.

  
**Dataset** & \))**} & \))**} & \))**} \\ 
**Model** & Train & Test & Train & Test & Train & Test \\  GCRN  & 69.60±1.14 & 72.57±0.72 & 74.71±0.17 & 72.29±0.47 & 75.69±0.07 & 67.26±0.22 \\ EvolveGCN  & 78.82±1.40 & 69.00±0.53 & 79.47±1.68 & 62.70±1.14 & 81.07±1.40 & 60.13±0.89 \\ DySxf  & 84.71±0.80 & 70.24±1.26 & 89.77±0.32 & 64.01±10.19 & 94.02±1.29 & 62.19±0.39 \\ IRM  & 85.20±0.07 & 69.40±0.09 & 89.48±0.22 & 63.97±0.37 & **95.02±0.09** & 62.66±0.33 \\ V-REx  & 84.77±0.84 & 70.44±1.08 & 89.81±0.21 & 63.99±0.21 & 94.06±1.30 & 62.21±0.40 \\ GroupDRO  & 84.78±0.85 & 70.30±1.23 & 89.90±0.11 & 64.05±0.21 & 94.08±1.33 & 62.13±0.35 \\ DIDA  & 87.92±0.92 & 85.0±0.84 & 91.22±0.59 & 82.89±0.23 & 92.72±2.16 & 72.59±3.31 \\ 
**Eagle** & **92.97±0.88** & **88.32±0.61** & **94.52±0.42** & **87.29±0.71** & 94.11±1.03 & **82.30±0.75** \\   

Table 2: AUC score (% ± standard deviation) of future link prediction task on real-world datasets with OOD shifts of node features. The best results are shown in bold and the runner-ups are underlined.

## 5 Related Work

**Dynamic Graph Learning.** Extensive researches [70; 4; 38; 90; 85; 35] address the challenges of dynamic graph learning. Dynamic graph neural networks (DGNNs) are intrinsically utilized to model both spatial and temporal patterns [88; 76; 84; 73; 62; 75; 80; 21] by combining vanilla GNNs and sequential-based models [58; 31; 28]. However, most existing works fail to generalize under distribution shifts. DIDA  is the sole prior work that tackles distribution shifts on dynamic graphs, but it neglects to model the complex environments, which is crucial in identifying invariant patterns.

**Out-of-Distribution Generalization.** Most machine learning methods are built on the I.I.D. hypothesis, which can hardly be satisfied in real-world scenarios [77; 71; 3]. Out-of-distribution (OOD) generalization has been extensively studied in both academia and industry areas [77; 92; 30] and we mainly focus on graphs. Most works concentrate on static graphs for node-level or graph-level tasks [98; 18; 49; 86; 52; 12; 87], supporting by invariant learning method [14; 46; 95] with disentangled learning [5; 55] and causal inference theories [65; 66; 64]. However, there lack of further research on dynamic graphs with more complicated shift patterns caused by spatio-temporal varying latent environments, which is our main concern.

**Invariant Learning.** Invariant learning aims to exploit the fewer variant patterns that lead to informative and discriminative representations for stable prediction [14; 46; 95]. Supporting by disentangled learning theories and causal learning theories, invariant learning tackles the OOD generalization problem from a more theoretical perspective, revealing a promising power. Disentangle-based methods [5; 55] learn representations by separating semantic factors of variations in data, making it easier to distinguish invariant factors and establish reliable correlations. Causal-based methods [22; 3; 72; 87; 11; 2; 60] utilize Structural Causal Model (SCM)  to filter out spurious correlations by intervention or counterfactual with \(do\)-calculus [65; 66] and strengthen the invariant causal patterns. However, the invariant learning method of node-level tasks on dynamic graphs is underexplored, mainly due to its complexity in analyzing both spatial and temporal invariant patterns.

## 6 Conclusion

In this paper, we propose a novel framework **E**agle for out-of-distribution (OOD) generalization on dynamic graphs by modeling complex dynamic environments for the first time and further exploiting spatio-temporal invariant patterns. **E**agle first models environment by an environment-aware DGNN, and then diversifies the observed environment samples with the environment instantiation mechanism, and finally learns OOD generalized spatio-temporal invariant patterns by the invariant pattern recognition mechanism and performs fine-grained causal interventions node-wisely. Experiment results on both real-world and synthetic datasets demonstrate that **E**agle greatly outperforms existing methods under distribution shifts. One limitation is that we mainly consider the node-level tasks, and leave extending our method to the graph-level OOD generalization for future explorations.