# A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training

A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training

Jie Ji, Gen Li, Jingjing Fu, Fatemeh Afghah, Linke Guo, Xiaoyong Yuan, Xiaolong Ma

Clemson University

jji@g.clemson.edu

###### Abstract

Sparse training stands as a landmark approach in addressing the considerable training resource demands imposed by the continuously expanding size of Deep Neural Networks (DNNs). However, the training of a sparse DNN encounters great challenges in achieving optimal generalization ability despite the efforts from the state-of-the-art sparse training methodologies. To unravel the mysterious reason behind the difficulty of sparse training, we connect network sparsity with the structure of neural loss functions and identify that the cause of such difficulty lies in a chaotic loss surface. In light of such revelation, we propose S\({}^{2}\)-SAM, characterized by a Single-step Sharpness-Aware Minimization that is tailored for Sparse training. For the first time, S\({}^{2}\)-SAM innovates the traditional SAM-style optimization by approximating sharpness perturbation through prior gradient information, incurring _zero extra cost_. Therefore, S\({}^{2}\)-SAM not only exhibits the capacity to improve generalization but also aligns with the efficiency goal of sparse training. Additionally, we study the generalization result of S\({}^{2}\)-SAM and provide theoretical proof for convergence. Through extensive experiments, S\({}^{2}\)-SAM demonstrates its universally applicable plug-and-play functionality, enhancing accuracy across various sparse training methods. Code available at https://github.com/jjsrf/SSAM-NEURIPS2024.

## 1 Introduction

The arrival of the Artificial General Intelligence (AGI)  era has urged an ever-expanding realm of artificial intelligence, bringing significant growth in deep neural networks (DNNs) depth and intricacy. The efficient training of DNN has thus emerged as an uttermost imperative, demanding immediate and concerted efforts.

To train an overparameterized, large-scale DNN efficiently while preserving its high accuracy, state-of-the-art literature has introduced sparse training [2; 3; 4; 5; 6] as a straightforward solution that reduces both parameter footprint and computation cost. With the presence of a large portion of zeros in the network, both forward and backward computation for training can be saved by skipping those zeros, as well as reducing the memory consumption since the zeros are not necessary to be stored. However, training a sparse neural network is difficult since the optimization under sparse regime readily converges to stationary points with a sub-optimal generalization accuracy (i.e., saddle points) . Due to the difficulty of _directly_ (i.e., without the time-consuming linear interpolation) assessing if the sparse solution is in the saddle point, finding workaround approaches for training a better sparse neural network is critical, especially for the practical usage in efficient learning [8; 9; 10; 11; 12; 13; 14; 15; 16; 17].

To address the above difficulty, many efforts have been made. For instance, the Static Sparse Training (SST) such as the Lottery Ticket Hypothesis (LTH) , SNIP , GraSP  and SynFlow determine a static sparse pattern at training initialization. On the other hand, Dynamic Sparse Training (DST) such as SET , RigL , and MEST  iteratively updates the sparse topology during training to find a better sparse model. However, those methods either suffer from suboptimal generalization ability due to heuristic nature of their methodology or metric settings, or experience difficulties in parameter setting and dynamic sparsity scheduling. With the increase of the sparsity, these phenomenon become more severe.

We identify that the cause of the sparse network learning difficulties lies in the variable learning dynamics, which is closely related to the network topology. When incorporating sparsity into a neural network, the effective structure of the original network becomes narrower. According to the study of neural loss function structure , a wide network resulted in flat minima and wide regions of apparent convexity, which helps prevent the chaotic behavior that occurs during training. Evidently, higher sparsity indicates a narrower structure, which suggests more chaotic behavior is expected during training, thus degrading the accuracy. We perform different types of sparse training with different levels of sparsity and plot the loss surface in Figure 1 to demonstrate the convexity of the training behavior. According to the sharpness of the loss basin, we observe a transition from a smooth and close-to-convex surface to a steep one as sparsity is introduced and increased. Since higher sparsity levels lead to more chaotic behavior, in landscapes that are narrow and sharp with a large \(Ra\) value, we might expect to encounter more chaotic training behavior that degrades generalization ability.

It seems that achieving coexistence of sparsity and good generalization ability during training is challenging. Therefore, we raise the following question: _is there a simple, effective method that can improve generalization ability of training a sparse neural network, without sacrificing efficiency (sparsity) and incurring zero extra cost?_ We believe the answer to the above question lies in the sharpness of the loss surface. Inspired by the Sharpness-Aware Minimization (SAM)  technique that finds flatter minima that have uniformly low loss in nearby regions, we argue that such technique is especially suitable for sparse training since the steep loss surface induced by sparsity can be directly mitigated. However, to leverage sharpness for better generalization, SAM uses an additional full training step (i.e., forward and backward propagation) to quantify and evaluate loss on the summation of current weights and a constrained perturbation, which roughly doubles the computation of training. Such extra costs contradict the goal of sparse training. Although some literature [26; 27] have been proposed to reduce the computation cost for SAM, the computation is still significant, posing a roadblock to the wide application of SAM to the realm of sparse training.

In this paper, we propose a novel approach to achieve sharpness-aware minimization with _zero extra cost_, tailored for the sparse training regime to maintain efficiency while improving generalization ability. It is the _first time_ that a **S**ingle-step **S**harpness-**A**ware **M**in**imization is proposed for **S**parse training (S\({}^{2}\)-SAM). Different from the traditional two-step computation regime of SAM, S\({}^{2}\)-SAM uses a unique single-step approach that leverages sharpness and trains weights with one training step. Specifically, S\({}^{2}\)-SAM uses the weight gradients from the prior step to approximate the perturbation to the weights, thus solving the sharpness evaluation without performing an extra full training step. Therefore, S\({}^{2}\)-SAM incurs zero extra cost to achieve sharpness-aware training, which aligns with the

Figure 1: The loss surface visualization for training a sparse neural network using ResNet-32 on CIFAR-10. We select two representative sparse training methods [18; 3] and incorporate different levels of sparsity. We also quantify the loss surface behavior using coefficient \(Ra\) to evaluate sharpness. With increased sparsity, \(Ra\) becomes larger, indicating sharper and steeper surface.

efficiency goal of sparse training. We also study the generalization result of S\({}^{2}\)-SAM and provide theoretical proof for convergence. We demonstrate that S\({}^{2}\)-SAM provides a straightforward plug-and-play functionality on variety of sparse training methods, significantly boosting their accuracy. Our contributions are summarized as follows:

* We identify that the difficulty of training a sparse neural network lies in the increasingly chaotic and steep loss surface when sparsity is introduced and increased.
* We develop a novel Single-step Sharpness-Aware Minimization technique tailored for Sparse training (S\({}^{2}\)-SAM), and it is the _first time_ that a SAM-style optimization with _zero_ extra computation cost has been proposed.
* We study the generalization result of S\({}^{2}\)-SAM and provide theoretical proof to demonstrate that the S\({}^{2}\)-SAM is guaranteed for convergence.
* Through systematic evaluations, we show that S\({}^{2}\)-SAM provides a plug-and-play functionality applied to a variety of sparse training methods, and consistently improves accuracy on different networks and datasets.

## 2 Proposed Method

### Preliminary of Sharpness-Aware Minimization

SAM is an optimization technique designed to enhance neural network generalization and mitigate overfitting. It minimizes the maximum loss in a neighborhood around the current parameters, as opposed to solely focusing on the loss at the current point. This approach identifies flatter minima that have uniformly low loss in nearby regions, ultimately contributing to improved generalization performance.

Specifically, consider a family of models parameterized by \(^{d}\); \(L\) is the loss function, and \(\) denotes the training dataset. SAM aims to minimize the following upper bound of the PAC-Bayesian generalization error: for any \(>0\),

\[L()_{\|\|_{p}}L_{}(+ )+\|\|^{2}.\] (1)

To solve the above minimax problem, at each iteration \(t\), SAM updates the following steps:

\[_{t} =( L_{}(_{ t-1}))\| L_{}(_{t-1})\|^{q-1}}{(\| L_{ }(_{t-1})\|_{q}^{q})^{1/p}},\] (2) \[_{t} =_{t-1}-_{t}( L_{}( _{t-1}+_{t})+_{t-1}),\]

where \(1/p+1/q=1,>0\) is a hyperparameter, \(>0\) is the parameter for weight decay, and \(_{t}>0\) is the learning rate. By setting \(p=q=2\) and introducing an intermediate variable \(_{t}\), we have:

\[_{t} =_{t-1}+}(_{t-1})}{\| L_{}(_{t-1})\|},\] (3) \[_{t} =_{t-1}-_{t}( L_{}( _{t})+_{t-1}).\] (4)

### The Proposed S\({}^{2}\)-SAM Method

As shown in Equation (2), SAM needs to compute the gradient twice at each iteration, involving additional computation costs. Further, the two-step gradient computation is not parallelizable, which presents challenges for deployment in large-scale training scenarios. Thus, we propose a new algorithm that only needs to compute the gradient _once_ in each iteration. Different from prior efficient SAM works [26; 27; 28] aiming to reduce the computation by introducing periodically SAM steps or data

Figure 2: Illustration of the optimization mechanism of S\({}^{2}\)-SAM. The perturbation on the current weights is approximated by the weight gradients from prior step. Please see Section 2.2 for detailed discussion.

selection, our proposed framework S\({}^{2}\)-SAM achieves _zero_ extra computation cost while maintaining the improved generalization ability of sparse network training.

In Figure 2, we demonstrate that the perturbation on the current weights is approximated by the weight gradients \(g_{t-1}\) from the prior step. The rational behind such design is that the gradient direction (i.e., \(_{t-2}\) to \(_{t-1}\)) from prior step is optimizing the prior loss to a considerable extent, thus it can be used to represent a sharp direction among all perturbation directions at \(t-1\). Since the loss surface of a sparse network is too chaotic, a perturbation with relatively high degree of sharpness (i.e., the prior gradient) can still find a neighborhood with near-maximum loss. The following equations specifying \(g_{t-1}\) information are used to substitute the \( L_{}(_{t-1})\) step in Equation (2). Thus, we only need to compute the gradient once in each iteration:

\[_{t} =_{t-1}+_{t-1}}{\|_{t-1} \|},\] (5) \[_{t} = L_{}(_{t}),\] (6) \[_{t} =_{t-1}-_{t}(_{t}+_{t-1}).\] (7)

_Remark 1_.: The parameter \(\) can vary in terms of iteration \(t:_{t}=\), where \(c>0\) is a constant.

### Generalization Analysis

In this section, we study the generalization result of S\({}^{2}\)-SAM. First, we give some notations, where most of them are followed by [29; 30]. Let \(_{}=()\) be a solution that generated by a random algorithm \(\) based on dataset \(\). Recall that problem (8)

\[_{}F_{}()=_{i=1 }^{n}(_{i},f(;_{i}))\] (8)

is called empirical risk minimization in the literature, and the true risk minimization is given by

\[_{}F():=_{(, )}[(,f(;))].\] (9)

We define its optimal solution: \(_{*}_{}F()\). Then the excess risk bound (ERB) is defined as

\[_{,}[F(_{} )]-F(_{*}).\] (10)

It has been shown that the ERB can be upper bounded by optimization error and generalization error [29; 30]. We notice that there are several works [29; 30] studying the generalization result of SGD for non-convex setting under different conditions, such as bounded stochastic gradient \(\|_{}(,f(;))\| G\) and decaying learning rate \(_{t}\) with a constant \(c>0\), where \(t\) is the optimization iteration. In this paper, we are not interested in establishing a fast rate in ERB under different conditions, but we want to explore the generalization ability of S\({}^{2}\)-SAM with the fewest possible modifications when building a bridge between theory and practice. For example, weight decay is a widely used trick when training deep neural networks. With the use of weight decay, the empirical risk minimization in practice becomes \(_{}\{_{}():=F _{}()+\|\|^{2}\}\). Then, we define some notations as follows. Specifically, let

\[_{}()=F_{}()+\|\|^{2}=_{i=1}^{n}_{i},f(;_{i}))+ \|\|^{2}}_{(_{i},f(; _{i}))},\] (11)

\[()=F()+\|\|^{2}\]

Followed by , we use the following decomposition of testing error:

\[_{,}[F(_{} )]-_{}[F_{}(_{ }^{*})]\ _{}[_{}[F_{ }(_{})-F_{}( _{}^{*})]]+\ _{,}[F(_{} )-F_{}(_{})]\] (12)

where the upper bound is the optimization error plus the generalization error.

Next, we present some notations and assumptions that will be used in the convergence analysis. Throughout this paper, we also make the following assumptions for solving the problem (8).

**Assumption 1**.: Assume the following conditions hold: (i) The stochastic gradient of \(F_{}()\) is unbiased, i.e., \(_{(,)}[(,f(;))]= F_{}()\), and the variance of stochastic gradient is bounded, i.e., there exists a constant \(^{2}>0\), such that

\[_{(,)}[\|(,f( ;))- F_{}()\|^{2}]= ^{2}.\]

(ii) \(F_{}()\) is smooth with an L-Lipchitz continuous gradient, i.e., it is differentiable and there exists a constant \(L>0\) such that \(\| F_{}()- F_{}() \| L\|-\|,, \).

**Assumption 2**.: There exists a constant \(>0\) such that \(2(F_{}()-F_{}(_{ }^{*}))\| F_{}() \|^{2},\), where \(_{}^{*}_{}F_{} ()\) is a optimal solution (PL condition ).

Now consider that \(=\) S\({}^{2}\)-SAM. We define the gradient update rule \(_{,}\) as follows

\[ =+}(,f( ,))}{\|_{}(,f(,))\|},\] (13) \[_{,}() =-}( ,f(,))+)}_{_{u} (,f(,))},\] (14)

Then we have the following lemma, which is similar to Lemma 2.5 and Lemma 4.2 in  that use recursive definition through variable \(^{}\) and \(_{t}^{}\).

**Lemma 1**.: _Assume that \((,f(,))\) is L-smooth and B-Lipschitz. Let \(_{t+1}=(_{t})\) and another sequence \(_{t+1}^{}=^{}(_{t}^{})\), then_

Please see the proof of Lemma 1 in Appendix B.

**Theorem 1**.: _Under Assumption 1, assume that \((,f(,))\) is L-smooth and B-Lipschitz, suppose \(_{}()\) satisfies Assumption 2 and \(_{}_{}() F_{ }(_{}^{*})+\| _{t}\|^{2}\) with \(=2L\), where \(_{t}\) is the intermediate solution of \(\), then_

\[_{R,,}[F(_{R})] -_{}[F(_{*})]_{}(_{0})}{ T}+}{2}+\]

_where \(\) is \(SGD\)._

The \(_{}^{*}_{}F_{} ()\) is an optimal solution, and we show that the generalization error is bounded. Based on Lemma 1, the proof of Theorem 1 is derived in Appendix C.

## 3 Experimental Results

In this section, we carry out comprehensive experiments to demonstrate how S\({}^{2}\)-SAM improves sparse training performance. We test S\({}^{2}\)-SAM on CIFAR-10/100  with ResNet-32  and VGG-19 , and we also perform experiments on ImageNet-1K  and ImageNet-C  based on ResNet-50 .

Following the recent developments in sparse training techniques, we apply S\({}^{2}\)-SAM on static sparse training such as LTH , SNIP , GraSP , as well as dynamic sparse training methods such as SET , DSR , RigL , MEST , CHEX  and Chase . We apply S\({}^{2}\)-SAM to the official codes or published implementations to show performance gains. All of our experiments are performed on NVIDIA 4\(\) A6000 GPUs. We repeat training experiments for 3 times and report the mean and standard deviation of the accuracy. For training throughput evaluation, we adopt the original settings of each baseline method and record the throughput on 4\(\) A6000 GPUs.

[MISSING_PAGE_FAIL:6]

CIFAR-10 dataset with ResNet-32. We observe that as sparsity increases, suggesting more chaotic training behavior, the coefficient \(Ra\) also increases, indicating a steeper loss surface and a narrower basin. While with S\({}^{2}\)-SAM, we can see that under the same sparsity, the basin of the loss surface widens and enlarges as \(Ra\) values decrease, suggesting a smoother loss trajectory during training of a sparse network.

### Training Speed Comparison on GPU

S\({}^{2}\)-SAM is a highly efficient approach to optimizing the sharpness of the loss surface, which incurs zero extra cost for achieving its functionality. Compared to the traditional SAM, S\({}^{2}\)-SAM uses fewer computations and has better training performance on GPUs. In Table 4, we obtain the computation cost as well as the training speed in terms of throughput (i.e., imgs/sec) on 4\(\) NVIDIA A6000 GPUs

   Method &  Sparsity \\ Distribution \\  &  Top-1 \\ Accuracy (\%) \\  &  Training \\ FLOPs \\  &  Inference \\ FLOPs \\  &  Top-1 \\ Accuracy (\%) \\  &  Training \\ FLOPs \\  & 
 Inference \\ FLOPs \\  \\  ResNet-50 & dense & 76.9 & (\(\)e18) & (\(\)e9) & 76.9 & (\(\)e18) & (\(\)e9) \\  Sparsity &  &  \\  LT  &  &  & 2.7 & 70.1 & n/a & 1.7 \\ LT + S\({}^{2}\)-SAM (ours) &  & n/a & 2.7 & **70.78\(\)0.05** (0.68\(\)) & n/a & 1.7 \\  SNIP  & non-uniform & 69.7 & 1.67 & 2.8 & 62.0 & 0.91 & 1.9 \\ SNIP + S\({}^{2}\)-SAM (ours) &  & 1.67 & 2.8 & **62.42\(\)0.07** (0.62\(\)) & 0.91 & 1.9 \\  GuSP  &  & 1.67 & 2.8 & 68.1 & 0.91 & 1.9 \\ GusEP + S\({}^{2}\)-SAM (ours) &  & 1.67 & 2.8 & **68.78\(\)0.12** (0.68\(\)) & 0.91 & 1.9 \\  SET  &  & 0.74 & 1.7 & **69.6** & 0.10 & 0.1 \\ SET + S\({}^{2}\)-SAM (ours) &  & 0.74 & 1.7 & **70.41\(\)0.08** (0.81\(\)) & 0.10 & 0.1 \\  DSR  &  & 1.28 & 3.3 & 71.6 & 0.96 & 2.5 \\ DSR + S\({}^{2}\)-SAM (ours) &  & 1.28 & 3.3 & **72.32\(\)0.13** (0.72\(\)) & 0.96 & 2.5 \\  Rigit.  &  & 0.74 & 1.7 & 72.0 & 0.39 & 0.9 \\ Rigit. + S\({}^{2}\)-SAM (ours) &  & 0.74 & 1.7 & **72.44\(\)0.06** (0.44\(\)) & 0.39 & 0.9 \\  MEST (EM)  &  & 1.10 & 1.7 & 7.6 & 0.48 & 0.9 \\ MEST (EM) + S\({}^{2}\)-SAM (ours) &  & 1.10 & 1.7 & **74.85\(\)0.03** (0.98\(\)) & 0.48 & 0.9 \\  MEST (EMAS)  &  & 1.27 & 1.7 & 75.0 & 0.65 & 0.9 \\ MEST (EMAS) + S\({}^{2}\)-SAM (ours) &  & 1.27 & 1.7 & **75.36\(\)0.04** (0.36\(\)) & 0.65 & 0.9 \\  Top-K\({}^{2}\)  &  & - & - & 73.0 & 0.63 & 0.9 \\ Top-KAST + S\({}^{2}\)-SAM (ours) &  & - & - & **73.82\(\)0.17** (0.82\(\)) & 0.63 & 0.9 \\  MESI\({}_{1,}\)  &  & 1.84 & 1.7 & 75.9 & 0.80 & 0.9 \\ MESI\({}_{1,}\) + S\({}^{2}\)-SAM (ours) &  & 1.84 & 1.7 & **76.82\(\)0.12** (0.92\(\)) & 0.80 & 0.9 \\  Rigit\({}_{3,}\)  &  & 3.71 & 1.7 & 75.7 & 1.95 & 0.9 \\ Rigit\({}_{3,}\) + S\({}^{2}\)-SAM (ours) &  & 3.71 & 1.7 & **76.88\(\)0.13** (1.18\(\)) & 1.95 & 0.9 \\   

Table 2: Results of ResNet-50 on ImageNet-1K.

Figure 3: Loss surface sharpness comparison of different sparse training methods with original training and with S\({}^{2}\)-SAM. We also quantitatively evaluate the coefficient \(Ra\). Using S\({}^{2}\)-SAM compared to the original method results in a smaller \(Ra\), indicating a wider and smoother loss surface, which suggests improved generalization ability.

using ResNet-50 on the ImageNet-1K dataset. We record the throughput for different sparse training methods with their original training, with SAM , and with S\({}^{2}\)-SAM. From the results, we can see that S\({}^{2}\)-SAM achieves the same training throughput (with only negligible \(<20\) imgs/s decrease due to processing) as the baseline methods, where no sharpness optimization is involved. Compared to original SAM, we can see that training throughput with SAM is _less than half_ of the ones that train with S\({}^{2}\)-SAM, which aligns with the observation that SAM uses _twice the computation_ cost of S\({}^{2}\)-SAM or original training. Although SAM yields slightly better accuracy, the associated training cost outweighs the benefits, rendering it impractical.

### Robustness Improvement by S\({}^{2}\)-SAM

Since sparse training approaches have potentially high usage in practical scenarios, the system's robustness against perturbation (e.g., inclement weather conditions for image/video tasks) is usually critical. We perform experiments to evaluate the sparse model (80% sparsity) robustness against perturbations in Table 5. Our intuition is that the model trained with its sharpness optimized has a wider loss basin, which indicates higher endurance on perturbations since the loss won't change much when it is located in a wide and flat region. We adopt ImageNet-C  which contains a test set with the same images as ImageNet-1K but with nineteen types of corruptions applied with five different levels of severity. We train the sparse networks using different methods on ImageNet-1K, and then test their accuracy on ImageNet-C. We report test accuracy for both datasets. We can see that when the sparse model is trained using the original method, the accuracy of ImageNet-C is significantly lower than the accuracy of the original ImageNet-1K test set (around 35% lower). With S\({}^{2}\)-SAM, the test accuracy on ImageNet-C shows promising improvement. The robust accuracy improves by an average of 3.23%.

### Applying S\({}^{2}\)-SAM to Dense Model Training

We also apply S\({}^{2}\)-SAM to the dense model training. The results are shown in Table 6. We test on two datasets CIFAR-10 and ImageNet-1K with two additional networks MobileNet-V2  and EfficientNet-B0 . For ResNet-32 and VGG-19 on CIFAR-10, we follow the settings in  and train for 160 epochs, and for MobileNet-V2, we use the 1.0 width version and train for 350 epochs

    & Params. & Original & S\({}^{2}\)-SAM \\  & Count & Accuracy (\%) & Accuracy (\%) \\   \\  ResNet-32 & 1.86M & 94.58 & **94.99** (0.41\(\)) \\ MobileNet-V2 & 2.30M & 94.13 & **94.55** (0.42\(\)) \\ VGG-19 & 20.03M & 94.21 & **94.48** (0.27\(\)) \\   \\  EfficientNet-B0 & 5.30M & 76.54 & **77.10** (0.56\(\)) \\ ResNet-34 & 21.80M & 74.09 & **74.58** (0.49\(\)) \\ ResNet-50 & 25.50M & 76.90 & **77.32** (0.42\(\)) \\   

Table 6: Testing accuracy on dense model training. We compare original training with S\({}^{2}\)-SAM in same settings.

   Methods & Networks & Training & FLOPs & Accuracy (\%) \\   & ResNet-34 & Original & 2.0G & 73.50 \\  & & S\({}^{2}\)-SAM & 2.0G & **73.94** (0.44\(\)) \\   & ResNet-50 & Original & 1.0G & 76.00 \\  & & S\({}^{2}\)-SAM & 1.0G & **76.51** (0.51\(\)) \\   & ResNet-34 & Original\({}^{*}\) & 2.1G & 72.34 \\  & & S\({}^{2}\)-SAM & 2.1G & **72.77** (0.43\(\)) \\    & ResNet-50 & Original & 1.3G & 75.62 \\   & & S\({}^{2}\)-SAM & 1.3G & **76.17** (0.55\(\)) \\   

Table 3: Accuracy of S\({}^{2}\)-SAM on structured sparse training CHEX  and Chase .

   Methods &  ImageNet-1K \\ Accuracy (\%) \\  &  ImageNet-C \\ Accuracy (\%) \\  \\   SNIP \\ SNIP + S\({}^{2}\)-SAM \\  & **70.55** (0.85\(\)) & **34.87** (3.75\(\)) \\   GraSP \\ GraSP + S\({}^{2}\)-SAM \\  & 72.10 & 32.24 \\   & **72.66** (0.56\(\)) & **35.17** (2.93\(\)) \\   MEST (EM) \\  & 75.70 & 33.87 \\   & **76.35** (0.65\(\)) & **36.98** (3.11\(\)) \\  
 RigL \\ RigL + S\({}^{2}\)-SAM \\  & 74.60 & 33.68 \\   & **75.39** (0.79\(\)) & **36.80** (3.12\(\)) \\   

Table 5: Testing accuracy on ImageNet-C test set. We compare the results with and without S\({}^{2}\)-SAM using 80% sparsity.

for better convergence . For ResNet family on ImageNet-1K, we follow the setting in  and train for 150 epochs, and we train EfficientNet-B0 for 350 epochs for better convergence . From the results, we can see that S\({}^{2}\)-SAM still improves the accuracy of the dense networks. We find out that the less the parameter count of the network, the better effectiveness of S\({}^{2}\)-SAM it achieves. This phenomenon proves our finding that a narrower network is harder to train, and S\({}^{2}\)-SAM is an effective solution for better generalization. We must also stress that S\({}^{2}\)-SAM is focusing on sparse neural network training, and we will leave the study of S\({}^{2}\)-SAM on dense model training for our future research.

## 4 Related Works

**Static Sparse Training** Static sparse training determines the structure of the sparse network through the application of a pruning algorithm in the early stages of training. The lottery ticket hypothesis (LTH) [20; 45; 46] uses iterative magnitude-based pruning (IMP) to find a subnetwork that can be trained from scratch without losing accuracy. SNIP , GraSP , SynFlow  determine a static sparse pattern at training initialization by obtaining gradient information with a few iterations of dense training. FISH  acquires fixed subnetwork by pre-computing a sparse mask using Fisher information.

**Dynamic Sparse Training** Dynamic sparse training starts with a randomly selected sparse network structure and adapts it throughout the training process in an effort to find a better sparse structure. Sparse Evolutionary Training (SET)  prunes small magnitude weights and grows back randomly at the end of each training epoch. Deep R  uses a combination of stochastic parameter updates and dynamic sparse parameterization for training. Dynamic Sparse Reparameterization (DSR)  proposes to redistribute parameters between layers during training. Sparse Networks from Scratch (SNFS)  creates the sparse momentum algorithm, which finds the layers and weights that effectively reduce the error by using exponentially smoothed gradients (momentum). In RigL , the gradients of all the weights are computed when the model needs to be updated to grow new connections. Top-KAST  proposes a scalable and performant sparse-to-sparse DST framework for maximum efficacy. Powerpropagation  suggests a novel neural network weight parameterization that largely preserves low-magnitude parameters from learning. ITOP  investigates the underlying DST mechanism and finds that the advantages of DST result from a time-based search for all potential factors. MEST  designs a memory-economic sparse training framework targeting accurate and fast execution on edge devices. By co-training dense and sparse models, AD/AC  suggests a technique that, at the conclusion of training, produces precise sparse-dense model pairings. Chase  dynamically translates the unstructured sparsity into channel-level sparsity to achieve direct speedup on GPU.

**Sharpness-Aware Minimization** Sharpness-Aware Minimization was first introduced in . This optimization technique is designed to identify flatter minima characterized by consistently low loss in neighboring regions, aiming to enhance the model generalization capability during training. Nevertheless, the computational cost of Sharpness-Aware Minimization is doubled due to its two-step gradient computation regime, presenting challenges for deployment in large-scale training scenarios. To reconcile such, ESAM  introduces two novel and efficient training strategies: stochastic weight perturbation and sharpness-sensitive data selection, enhancing the efficiency of the SAM process without compromising its generalization performance. LookSAM  only periodically calculates the inner gradient ascent to significantly reduce the additional training cost of SAM. SAF  introduces a novel trajectory loss based on KL-divergence to measure the rate of change in training loss along the model update trajectory and replace the SAM sharpness measure. CrAM  optimizes over the compression projection applied to the intermediate model at every training step, which results in a compressible models that can be pruned in an on-shot manner after training. However, all those methods need extra computation cost, and not target on sparse training method.

## 5 Conclusion

In this paper, we propose a novel Single-step Sharpness-Aware Minimization that is tailored for Sparse training (S\({}^{2}\)-SAM), which revolutionizes the originally computation-intensive sharpness-aware optimization into a highly efficient tool with zero extra cost. In light of the improved optimization trajectory in the loss surface, S\({}^{2}\)-SAM successfully enhances the accuracy of the sparse network training, as well as the robustness of the sparse model in practical scenarios. S\({}^{2}\)-SAM offers seamless plug-and-play functionality, showcasing its potential for widespread applicability in the evolving landscape of efficient training. The research is inherently scientific, and we anticipate no adverse societal impact stemming from its findings.