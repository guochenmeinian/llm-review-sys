# Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset

Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset

 Saeid Alavi Naeini\({}^{1,3,4}\) Raedi Saqur\({}^{2,4}\) Mozhgan Saeidi\({}^{2,4,6}\) John Giorgi\({}^{2,4,5}\) Babak Taati\({}^{1,2,3,4}\)

\({}^{1}\)Kite Research Institute, Toronto Rehabilitation Institute, University Health Network

\({}^{2}\)Department of Computer Science, University of Toronto

\({}^{3}\)Institute of Biomedical Engineering, University of Toronto \({}^{4}\)Vector Institute for AI

\({}^{5}\)Donnelly Centre for Cellular & Biomolecular Research, University of Toronto

\({}^{6}\)Department of Biomedical Data Science, Stanford University

{saeid.alavi, john.giorgi}@mail.utoronto.ca

raeidsaquur@cs.toronto.edu, mozhgans@stanford.edu, babak.taati@uhn.ca

###### Abstract

The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeifeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behavior (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine _creative problem solving_ abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli -- distractors dubbed _red herrings_ -- impede human performance in such tasks via the _fixation effect_ and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incorrect words to subsequent word-fragments or clues. The popular British quiz show Only Connect's _Connecting Wall_ segment essentially mimics Mednick's Remote Associates Test (RAT) formulation with built-in, deliberate red herrings, which makes it an ideal proxy task to explore and study the fixation effect and Einstellung paradigm from cognitive neuroscience in LLMs. In this paper, we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected pre-trained language models and LLMs on creative problem solving tasks like grouping clue words by heterogeneous connections and identifying correct open knowledge domain connections in respective groups. We synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to further analyze our red-herrings hypothesis in language models. The code and link to the dataset are available at https://github.com/TaatiTeam/OCW.

## 1 Introduction

The remarkable capabilities of state-of-the-art large language models (LLMs) , across a variety of domains and downstream tasks , have spurred their comparisons with artificial general intelligence (AGI)  and human-imitative AI  systems. The extraordinary leap in capabilities of these LLMs over a short span -- from the advent of transformer-based  pre-trained, context-aware language models (PLMs)  circa 2018 to 2020, to the current and latest cohort of increasingly larger (billions of parameters) LMs [59; 77; 57; 89; 16; 67; 18] spearheaded by the OpenAI's GPT series , notably ChatGPT  and GPT-4  -- justifiably warrants such comparisons. Several natural language processing (NLP) benchmarks have been proposed to standardize the evaluation of these LLMs, including MMLU , BIG-bench , HELM , and Global-Bench . The tasks inventory under these benchmarks are open (type of tasks) and dynamic (rolling additions). While a subset of these tasks aims to test for human initiative intelligence (e.g., nineteen tasks listed under the _human-like behaviour_ category in BIG-bench), none tests for _creative problem solving_ abilities  -- a hallmark of human-like intelligence .

Creative problem-solving by humans is a well-studied topic in cognitive neuroscience and human behavioural sciences literature. These studies and methods use (word) associative fluency to model and test creativity objectively [44; 9]. Empirical research in this context commonly employs single or continuous word association tests that are variants of Mednick's seminal Remote Associates Test (RAT) . Such tests entail finding connections or links among a presented group of words using associations that can be heterogeneous (e.g., synonymy, semantic, compounding) [86; 43]. To exemplify, consider the cue words: _[Tennis, Same, Head]_. A correct connection in this triplet is: _Match_, which connects by semantic link (_tennis match_), synonymy (_same match_), and compounding (_match head_). Further, the word connections can also vary in degrees of figurativeness (e.g., _Star-Actress_ vs. _Star-Planet_) and abstractness (e.g., _Humor-Sense_ vs. _Apple-Tree_). In humans, such creative problem-solving abilities are impeded by exposure to wrong answers [61; 62; 85] -- a finding referred to as the _fixation effect_[34; 82]. A closely related similar concept is the _Einstellung effect_, which postulates the negative effect of previous experience when solving new problems.

Studies examining the fixation effect induce fixations by presenting clue words intended as wrong answers (misleading stimuli)  dubbed "red herrings" or, by pre-exposing participants to red herrings before attempting creative problem-solving tasks like the RAT . A slew of works in negative transfer learning in human cognition attempt to explain the RAT fixation phenomenon that involves pre-exposure to red herrings by the negative effects of prior learning on indirect or implicit measures of memory . This negative transfer effect was demonstrated and studied using orthographically similar words to subsequent test word fragments as red herrings . Intuitively, the red herrings lead participants away from the memory retrieval (or down incorrect neurological pathways by Hebbian terminology) required for correct responses and fixate on wrong connections . Fixation in creative problem-solving can be increased by making red herrings more retrievable. Thus, creative problem-solving can be thought of as a type of indirect memory measure whose retrieval is degraded by red herrings due to the negative transfer effect. The _red herring retrieval hypothesis_ states that factors that make red herrings more retrievable should reduce creative problem-solving performance,

Figure 1: Examples of _Only Connect_ walls with ground-truth groupings (rows) and connections (last column). _Red herrings_ include orthographically identical words, e.g., **Gala**, Churchill and Chelsea in different connected groups — **Gala**: _Gala night, Apples_, _Swimming gala_, Churchill: _Advert Animals, English Playwrights_ and Chelsea: _Gay Villages, Boots_ — across walls. In Wall **A** (top left), the clues Churchill, Marx, Castro provide misleading stimuli inducing plausible fixation on historical figures within the wall.

as measured with RAT problems. Two such factors are repetition and context. A following corollary states that the memory strengths of red herrings determine the magnitude of a fixation effect .

In this work, we study the juxtaposition of these theories from human cognitive neuroscience (_fixations, negative transfer learning, red herring memory retrieval hypothesis_) from the context of LLMs and natural language processing. While negative transfer learning has been observed and studied in AI research , the context of these studies is limited to strict machine learning sub-domains like statistical distribution measures and computer vision. There has not been any work that systematically examines these specific concepts' relation in AI research. Our major contributions are as follow:

1. Only Connect Wall (OCW) Dataset and creative problem solving tasks.We introduce a novel dataset for evaluating _creative problem solving_ tasks by curating the problems and human performance results from the popular British quiz show Only Connect . Specifically, the _Connecting Walls_ segment of the show, where the tasks entail grouping sixteen (16) jumbled up clue words into four (4) connected groups, and naming the correct connections (Figure 1). The presented words have heterogeneous connections with open-domain knowledge retrieval, e.g., history, places, famous people, tools, and cultural references. These 'walls' contain red herrings or misleading stimuli by design, which makes this dataset an analogical proxy for RAT tests in evaluating LLMs for creative problem-solving. Section SS2 provides a detailed description of the dataset.

2. Experiments, results, and key findings of baseline LLMs evaluation.We evaluate a suite of NLP models from static embeddings to PLMs to LLMs and demonstrate that none can solve the tasks of the OCW dataset. Our findings show that SOTA LLMs (e.g. GPT-4 ) perform significantly worse than the expert human baseline, and somewhat surprisingly, that increasing the number of in-context examples in few-shot in-context-learning is ineffective. Sections SS3 and SS4 provide details.

## 2 Only Connect Walls Dataset

Here we focus on the _Connecting Walls_ segment (usually the third round) of the quiz-show. Each wall contains sixteen jumbled-up word clues that must be sorted into four groups, each with four connected words. Once the groups are formed, contestants must also identify the right connection or relationship among the items in each group. While there is only one correct solution to each wall, the puzzles are designed to include several red herring clues that can fit into another category and red herring categories fitting multiple clues. Figure 1 shows solved sample walls from the show highlighting a couple of typical red herrings.

### Dataset Collection and Structure

The OCW dataset contains 618 connecting wall puzzles and solutions in total from 15 seasons of the show. Each show episode has two walls. The total number of walls per season varies based on the (varying) number of aired season episodes. The walls were scraped from fan websites1, and human performance results (for grouping and connection tasks) were manually curated by watching all the episodes. Figure 2 depicts the high-level structure of the dataset in JSON format with self-explanatory object keys and comments.

### Tasks and Evaluation Metrics

The two dataset tasks: **Task 1 (Grouping)**, and **Task 2 (Connections)** are identical to the quiz-show's human participant tasks. We evaluate Task 1 (Groupings) via six metrics: number of solved walls, number of correct groups (max. four per wall), Adjusted Mutual Information (AMI) , Adjusted Rand Index (ARI) , Fowlkes Mallows Score (FMS) , and Wasserstein Distance (WD) , normalized to \((0,1)\) range, between predicted and ground-truth labels .

We similarly evaluate Task 2 (Connections) with three metrics: exact string matching, ROUGE-1 F1 , and BERTScore F1 . Exact match is the most strict, assigning a score of 1 when the predicted connection is identical to the ground-truth and 0 otherwise. ROUGE-1 F1 relaxes this criterion; it is large when there is a high proportion of ground-truth tokens in the model's predictedconnection _and_ a low proportion of non-ground truth tokens. BERTScore F1 is similar but further relaxes this criterion, assigning a non-zero score for _semantically_ similar (but non-identical) predicted tokens. Together these three metrics provide a more holistic view of model performance on Task 2 than any one metric alone. Empirically, we find that a ROUGE-1 or BERTScore F1 of \( 0.5\) indicates that a predicted connection would likely be considered _correct_ (Table 1). Note that BERTScore has many parameters affecting the final score; a hashcode is produced and reported for reproducibility.

Each of the evaluation metrics for Task 1 of Task 2 could be calculated per wall, per episode, per season, or for the entire test set. We present results on the entire test set in this paper (SS4). We split the dataset into a train set (62 walls), validation set (62 walls), and test set (494 walls). The primary goal of our dataset is to evaluate the zero- and few-shot creative problem-solving abilities of LLMs; as such, we elect to set the size of the test set to be much greater than train or validation sets.

## 3 Experiments: Language Model Evaluations

This section describes methods and models used to provide baseline results for the dataset. For Task 1 (Grouping), we use clustering techniques on word-embeddings from classical and pre-trained language models (PLMs) (SS3.1), and few-shot in-context learning (ICL) with LLMs (SS3.2). For Task 2 (Connections), we only provide baseline results using few-shot ICL with LLMs (SS3.2).

Figure 2: JSON Structure of the OCW dataset. One truncated example is shown.

 Predicted Connection & Ground-truth Connection & Exact Match & ROUGE-1 F1 & BERTScore F1 \\  Types of numbers & Types of numbers & 1.00 & 1.00 & 1.00 \\ Slang terms for money & Slang for money & 0.00 & 0.86 & 0.79 \\ Types of trees & Trees & 0.00 & 0.50 & 0.63 \\ Bridges in London & Thames bridges & 0.00 & 0.40 & 0.31 \\ Medieval occupations & Chaucer characters & 0.00 & 0.00 & 0.15 \\  

Table 1: Examples of predicted and ground-truth connections and their performance according to the chosen metrics. Exact match is 0 for anything but identical strings. Empirically, we observe that a ROUGE-1/BERTScore F1 of \( 0.5\) indicates that a predicted connection is likely _correct_.

### Task 1: Grouping using Word Embeddings

For the _grouping task_ evaluation (SS2.2), we use clustering algorithms on word-embeddings of the sixteen clue words in each wall, to group them into four predicted groups that are subsequently evaluated against the four ground-truth groups for each wall. A vanilla \(k\)-means (with \(k=4\)) clustering algorithm  does not guarantee each predicted group to have four words, thus we use variants like constrained clustering.

ClusteringSemi-supervised constrained clustering [72; 7] is used when the user has pre-existing knowledge about the desired partition (in our case, 4 groups). Here, we adopt a _minimum cost flow network_ clustering approach  with a cluster size of four for grouping. Our preliminary analysis showed that clustering results exhibited slight variations across runs. This slight discrepancy could be attributed to the initializations of cluster centroids. To address this issue and ensure reliable results, we report the mean and variance of results (Table 3)across sixteen (16) runs, each with a unique seed and randomized order of sixteen-word clues. We tested two additional clustering approaches motivated by [47; 19]: (1) We constructed a self-similarity matrix containing pair-wise similar information about the words prior to applying constrained clustering; (2) We performed dimensionality reduction using Principal Component Analysis (PCA)  and t-distributed stochastic neighbor embedding (t-SNE)  before applying constrained clustering. Neither approach improved performance over raw embeddings' clusters, and, for brevity, results are not included.

Static word embeddingWe used two well-known classic word embedding models, GloVe  and FastText , both of which are accessed through the Flair library (Table 2). We used two FastText models, one pre-trained on the Common Crawl corpus and another on Wikipedia. Approximately 10% of the total clues encountered in the dataset were out-of-vocabulary (OOV). A significant portion (~80%) of the OOV cases were addressed by mean pooling for clues comprised of multiple words to obtain one unified embedding. For the remaining OOV instances, we combined the static embeddings with BytePair encoded sub-words.

PLMsWe explored general-purpose PLMs (BERT , RoBERTa , DistilBERT , ELMo ) as well as Sentence Transformers (MPNet , E5 ; see Table 2). We evaluated performance with and without contextual embeddings.2 Depending on the context, some clues in the dataset may appear across different walls with different meanings. As an example, the word

Figure 3: Solved wall (wall_id="8cde") for Task 1 (Grouping) using best performing model (E5BASE) with both static and contextual embeddings. **Left**: solved wall using static embeddings. **Right**: unsolved wall using contextual embeddings. 2D projection of embeddings using t-SNE is shown. Colors and shapes correspond to true clusters, and grey convex regions correspond to predicted clusters. The legend shows the ground truth connection for each group.

"Gala" was found in three distinct walls, each associated with a different meaning: _apples_, _swimming_ --, and _night_ (Figure 1). The contextual embeddings were aimed to capture contextual semantic similarity among the clues (if any). They were generated by joining the 16 clues in the wall as a pseudo-sentence. We randomly shuffle the word order across sixteen different runs for each wall to account for the positional ordering. We note that such faux sentences (for inducing context) are not valid English syntactic sentences. We used mean pooling to generate embeddings for clues comprised of multiple words to capture the collective meaning of the entire clue.

### Task 2: Connections using Few-shot In-context Learning (ICL) with LLMs

Few-shot ICL with LLMs has emerged as a performant and broadly applicable paradigm in NLP . To evaluate the performance of this approach on our proposed dataset, we designed a few-shot prompt for GPT-3.5-turbo and GPT-4 , which are amongst the strongest performing LLMs currently available.3 For Task 1 (Grouping, SS2.2), the prompt consists of some natural language instructions, several examples of solved walls from the training set, and the current example's 16 clues, randomly sorted. For Task 2 (Connections), in place of the 16 clues, the prompt contains a solved wall _without_ the connections (Figure 4).

We developed our prompts on the validation set and reported the final performance on the test set. In-context examples are randomly selected from the train set; the same examples are used across all test inputs. We experiment with 0, 1, 3, 5, and 10 in-context examples. When necessary, we apply simple post-processing to the LLMs output. For example, in both Task 1 and Task 2, we take a maximum of 4 predictions for the groups and connections, respectively, and pad up to 4 with the empty string in cases where the model outputs fewer than 4.4 To make results as reproducible as possible, we set the temperature=0 and used the 03/01/2023 GPT-3.5-turbo snapshot and the 03/14/2023 GPT-4 snapshot. The max output length is set to 144 tokens. All other hyperparameters of the OpenAI API are left at their defaults . Prompts were designed as per the Guidance library .

## 4 Results and Discussions

### Task 1: Grouping Results

Embedding Clustering TechniquesIn Table 3 we report the performance of several static embedding baselines on Task 1 (Grouping). E5BASE was the most performant model and, on average, solved

   Model &  & Version & Accessed via \\   \\  BPEmb  & En & – & en & Flair  \\ GloVe  & 6B & – & glove & Flair \\ FastText  & Crawl & – & crawl & Flair \\  & News & – & news & Flair \\   \\  ELMo\({}_{}\) &  & large & Flair  \\ DistilBERT\({}_{}\) & uncased & 66M & distilbert-base-uncased & HuggingFace  \\ BERT\({}_{}\) & uncased & 110M & bert-base-uncased & HuggingFace \\ BERT\({}_{}\) & uncased & 340M & bert-large-uncased & HuggingFace \\ RoBERT\({}_{}\) &  &  & HuggingFace \\   \\  all-mpnet\({}_{}\) & V2 & 110M & sentence-transformers/all-mpnet-base-v2 & HuggingFace \\ E5BASE  & V2 & 110M & infloat/e5-base-v2 & HuggingFace \\ E5\({}_{}\) & V2 & 340M & infloat/e5-large-v2 & HuggingFace \\   \\  GPT-3.5-turbo &  & gpt-3.5-turbo-0301 & OpenAI API \\ GPT-4 & – & gpt-4-0314 & OpenAI API \\   

Table 2: Details about the baselines and models used in our experiments.

[MISSING_PAGE_FAIL:7]

3-shot), we found common sources of error to include misformatted outputs (4.4% of all predicted groups) and hallucinated clues (6.6%).

Surprisingly, more in-context examples (from 1 to 10 shot) did not improve performance. One possible explanation for this observation is that, due to the huge variety of possible connection types, the in-context examples' primary benefit is demonstrating the expected output format - as opposed to demonstrating how to perform the task - which likely requires only a single example. This is related to the concepts of _task learning_ versus _task recognition_, which are thought to be the two distinct mechanisms through which ICL leverages demonstrations . Many clues require open-domain, arcane, cultural and intimate knowledge of niche subject areas (e.g., "_Professional snooker players_", "_Female Radio 1 DJs_") that, without prior memorization, are unlikely to help. The presence of orthographically similar clue words in the in-context examples could themselves act as red herrings and plausibly induce negative transfer learning. An interesting future direction would be the evaluation of retrieval augmented models , which may be capable of solving groups about highly specific subject areas.

### Task 2: Connections Results

In Figure 6, we present the results for Task 2 (Connections). In general, GPT-4 outperforms GPT-3.5-turbo, especially in the 0-shot regime. Performance for GPT-4 improves monotonically with an increasing number of in-context examples, although improvements are sometimes small (e.g.,

    & \# In-context Examples & WD \(\) & FMS \(\) & ARI \(\) & AMI \(\) & \# Solved Walls & \# Correct Groups \\  GPT-3.5-turbo & 0-shot & \(82.5\) & \(34.0\) & \(18.4\) & \(21.6\) & \(0\) & \(114\) \\  & 1-shot & \(82.3\) & \(34.4\) & \(18.2\) & \(21.2\) & \(0\) & \(123\) \\  & 3-shot & \(80.9\) & \(36.8\) & \(21.3\) & \(24.7\) & \(0\) & \(140\) \\  & 5-shot & \(80.6\) & \(37.3\) & \(22.0\) & \(25.4\) & \(2\) & \(149\) \\  & 10-shot & \(81.2\) & \(36.1\) & \(20.4\) & \(24.0\) & \(2\) & \(137\) \\  & 0-shot & \(75.8\) & \(41.5\) & \(27.2\) & \(30.7\) & \(6\) & \(239\) \\  & 1-shot & \(73.4\) & \(43.7\) & \(29.7\) & \(33.5\) & \(4\) & \(262\) \\  & 3-shot & \(73.7\) & \(43.9\) & \(29.9\) & \(33.6\) & \(5\) & \(272\) \\  & 5-shot & \(\) & \(43.4\) & \(29.1\) & \(32.8\) & \(\) & \(269\) \\  & 10-shot & \(73.6\) & \(42.8\) & \(28.5\) & \(32.3\) & \(3\) & \(249\) \\  Human Performance & – & – & – & – & \(285\,/\,494\) & \(1405\,/\,1976\) \\   

Table 4: Results on Task 1 (Grouping) using Large Language Models. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. **Bold**: best scores.

    & WD \(\) & FMS \(\) & ARI \(\) & AMI \(\) & \# Solved Walls & \# Correct Groups \\   \\  GloVe & \(84.9.4\) & \(31.5.3\) & \(14.4.3\) & \(17.6.4\) & \(0 0\) & \(68 4\) \\ FastText (Crawl) & \(84.2.5\) & \(32.1.3\) & \(15.2.3\) & \(18.4.4\) & \(0 0\) & \(80 4\) \\ FastText (News) & \(85.5.5\) & \(30.4.2\) & \(13.0.2\) & \(15.8.3\) & \(0 0\) & \(62 3\) \\   \\  ELMoLAGE & \(86.3.6\) & \(29.5.3\) & \(11.8.4\) & \(14.5.4\) & \(0 0\) & \(55 4\) \\ DistilBERTBASE & \(86.7.6\) & \(29.1.2\) & \(11.3.3\) & \(14.0.3\) & \(0 0\) & \(49 4\) \\ BERTLARGE & \(88.3.5\) & \(26.5.2\) & \(8.2.3\) & \(10.3.3\) & \(0 0\) & \(33 2\) \\ BERTBASE & \(89.5.4\) & \(25.1.2\) & \(6.4.3\) & \(8.1.4\) & \(0 0\) & \(22 2\) \\ RoBERTLARGE & \(88.4.4\) & \(26.7.2\) & \(8.4.3\) & \(9.4.4\) & \(0 0\) & \(29 3\) \\   \\  all-mpnetBASE & \(86.3.4\) & \(29.4.3\) & \(11.7.4\) & \(14.3.5\) & \(0 0\) & \(50 4\) \\ ES4RGE & \(84.4.7\) & \(32.3.4\) & \(15.4.5\) & \(18.5.6\) & \(0 0\) & \(76 5\) \\ ESBASE & \(.6\) & \(.3\) & \(.4\) & \(.4\) & \(\) & \(\) \\  Human Performance & – & – & – & – & \(285\,/\,494\) & \(1405\,/\,1976\) \\   

Table 3: Results of selected models on Task 1 (Grouping) using static embeddings. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. Mean \(\) standard deviation over 16 random seeds is shown. **Bold**: best scores.

\(<0.01\)). As expected, the exact match score for both models is low (\(<15\%\)). This is explained by the fact that even insignificant differences between the model's predictions and the ground truth will result in a score of 0 (e.g., "Made _of_ rubber" vs. "Made _from_ rubber"). For this reason, we also report ROUGE-1 and BERTScore F1 scores (SS2.2). Although not a perfect comparison, we can contextualize these results with human performance, which we recorded as the fraction of correctly guessed connections: \(\)80% on the test set. The quiz show Only Connect allows for some small deviations in guessed connections that will be accepted as correct, making the comparison to ROUGE and BERTScore more suitable than to exact match. Our results suggest that at 41-45% F1, the best performance achieved with few-shot ICL (GPT-4, 10-shot) is far below human performance. Lastly, we note that a common source of model error was the inclusion of clues in the predicted connection (occurring in 8.2% of all predicted connections for the best performing model), e.g., "Fireplace tools (Spade, Brush, Poker, Tongs)", even though (1) the model was not instructed to do so, and (2) the in-context examples were not formatted like this.

More complicated post-processing or prompting strategies (e.g., "Chain of Thought" , "Tree of Thoughts" ) could mitigate these issues and improve performance. However, applying these more complicated prompting strategies to the OCW dataset is non-trivial, as they require breaking down the problem into intermediate steps, and the number or nature these intermediate reasoning steps should take is unclear. We leave their application to the OCW dataset for future work.

### Effects of Red-Herrings: Additional Datasets, Experiments and Analyses

To analyze our _red-herring hypothesis_ on language models, we designed and performed additional ablative experiments. The original OCW dataset contains red-herrings as distractors _by design_. We generate two additional datasets from OCW to decrease the presence of red-herrings: OCW-Randomized and OCW-WordNet. The goals, construction and other details are presented in Appendix SSC.1.

In OCW-Randomized, we diluted the presence of red herrings by randomly swapping groups among the walls in the test set - thus negating the inherent deliberate distractor groups in each wall. We further simplify the grouping task in OCW-WordNet by removing red herrings altogether. This is achieved by using subordinate-superlative (or hyponym-hypernym) word hierarchy and synonyms in the English lexical database WordNet . Thus the results in Table 5 present results on datasets with a decreasing proportion of red herrings from left to right, and by our hypothesis, increasing task simplicity for LLMs. The results are aligned with our expectations, with GPT-3.5-turbo and GPT-4 performance increasing significantly with the reduction of red herrings from the test set.

## 5 Related Work

Various datasets and tasks have been proposed for evaluating language models against human-like linguistic capabilities. Earlier examples of such tasks include _word sense disambiguation_ (WSD) ,

Figure 6: Results for Task 2 (Connections) with GPT-3.5-turbo and GPT-4. For reference, human performance is approximately 80% (fraction of correctly answered connections). We report \((,0)\) in the case of GPT-3.5-turbo for readability.

Winograd schema challenge  and _word sense induction_ (WSI) . WSD aims to determine a word's correct meaning or sense within a specific context. WSI focuses on automatically clustering words into different senses or semantic categories based on their contextual usage patterns. Benchmarks like GLUE  and SuperGLUE  are aimed at aggregating and standardizing these classical NLP tasks to evaluate language models. The PLMs (e.g., BERT variants) and the first generation of LLMs, mostly solved or attained human-level performance on these tasks by 2020s .

In order to evaluate the human-imitative capabilities of modern LLMs, more challenging tasks have been proposed in recent benchmarks like BIG-bench  and HumanEval . **BIG-bench** aims to address the limitations of existing benchmarks by providing a more comprehensive, open, and dynamic (tasks added on a rolling basis) evaluation benchmark. It covers a wide range of tasks, including a suite of tasks targeted specifically for _human-like behavior_. **HumanEval** is an evaluation set to measure the functional correctness of code synthesis from docstrings . This benchmark includes 164 original programming problems that assess language comprehension, algorithms, and simple mathematics comparable to simple software interview questions. While these recent benchmarks include a wide net of complex tasks, evaluating a broad range of LLM capabilities, our work here is orthogonal to these since none of them aims to specifically measure creative problem-solving or creativity and their impediments in LLMs.

## 6 Limitations & Future Work

As with any machine learning dataset, especially one designed to evaluate the performance of LLMs, the OCW dataset has several limitations. First, we noticed that the performance of contextual approaches can vary significantly depending on the order that clues are provided to the model. To alleviate this (and where feasible), we evaluate models across 16 random sortings of the clues. Due to cost, we did not evaluate GPT-3.5-turbo and GPT-4's sensitivity to this ordering; future work should report performance across multiple random sorts. Second, due to the nature of the quiz show _Only Connect_, the clues, groups, and connections in the dataset tend to be Western- (and specifically UK-) centric (e.g. "_Doctor Who companions_", "_English cricket captains_", "_Irish counties_"). Therefore, performance on the OCW dataset may not extrapolate to languages or cultures outside of Western English. In fact, the _US_-centric bias of LLMs like GPT-3.5/4  might partially explain their poor performance on the _UK_-centric OCW dataset. We hope to add additional _Only Connect_ inspired walls in multiple languages and with clues derived from various cultures & subcultures in future work. Finally, given that the walls are publicly available as text on fan sites like ocdb.cc, there is always the possibility that they are included in the training sets of LLMs like GPT. However, we think this is unlikely, given the low performance on the grouping and connection tasks. Preventing the test sets of publicly available datasets like our OCW from "leaking" into the training sets of LLMs remains an interesting and open problem. We have taken basic steps against this leakage by distributing our dataset in a compressed format .

    &  &  &  \\   &  &  &  &  \\  GPT-3.5-turbo & 0-shot & 0 & 114 & 5 & 274 & 337 & 1522 \\  & 1-shot & 0 & 123 & 12 & 315 & 320 & 1400 \\  & 3-shot & 0 & 140 & 10 & 306 & 415 & 1748 \\  & 5-shot & **2** & **149** & 16 & **337** & 415 & 1759 \\  & 10-shot & 2 & 137 & **17** & 333 & **428** & **1800** \\ GPT-4 & 0-shot & 6 & 239 & 59 & 595 & **471** & **1926** \\  & 1-shot & 4 & 262 & 57 & 644 & 304 & 1581 \\  & 3-shot & 5 & **272** & 62 & 649 & 279 & 1537 \\  & 5-shot & **7** & 269 & **68** & **655** & 298 & 1584 \\  & 10-shot & 3 & 249 & 55 & 614 & 378 & 1742 \\  Human Performance & 285 / 494 & 1405 / 1976 & – & – & – & – \\   

Table 5: Coalesced results of LLMs performance on Task 1 (grouping) using two additional test datasets OCW-Randomized and OCW-WordNet with decreasing presence of red-herrings from _left_ to _right_ in the walls, juxtaposed against the original OCW test set (left-most column). Only the main metrics are shown (details and full results in Appendix SC). **Bold**: best scores.