# Bias Detection via Signaling

Yiling Chen

Harvard University

yiling@seas.harvard.edu &Tao Lin

Harvard University

tlin@g.harvard.edu &Ariel D. Procaccia

Harvard University

arielpro@g.harvard.edu &Aaditya Ramdas

Carnegie Mellon University

aramdas@cmu.edu &Itai Shapira

Harvard University

itaishapira@g.harvard.edu

###### Abstract

We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior. In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior. Since we often cannot observe the agent's beliefs directly, we take an approach inspired by _information design_. Specifically, we measure an agent's bias by designing a _signaling scheme_ and observing the actions the agent takes in response to different signals, assuming that the agent maximizes their own expected utility. Our goal is to detect bias with a minimum number of signals. Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.

## 1 Introduction

A bag contains two coins that look and feel identical, but one is a fair coin that, on a flip, comes up heads with probability 0.5, and the other is an unfair coin with probability 0.9 of heads. You reach into the bag, grab one of the coins and flip it once; it lands on heads. Since you are (hopefully) familiar with Bayes' rule, you conclude that the probability you are holding the fair coin is \( 0.36\). Now suppose you are offered the following deal: if you pay $1, you get to flip the same coin again, and if it comes up heads, you will receive $1.4. Since you now believe that the probability of heads is 0.76, you take the deal (assuming you are risk neutral) and earn 6 cents in expectation.

If, by contrast, another risk-neutral person in the same situation decides to decline the same deal, they must believe that the probability they are holding the fair coin is greater than 0.47. That is, their belief is still very close to the prior of 0.5. We think of such a person as being _biased_, in the sense that they are unwilling to significantly update their beliefs, despite evidence to the contrary.

Of course, failing to update one's beliefs about coin flips is not the end of the world. But this example serves to illustrate a broader phenomenon that, in our view, is both important and ubiquitous. In particular, the "stickiness" of prior beliefs in the face of evidence plays a role in politics -- think of the controversy over Russian collusion in the 2016 US presidential election or the existence of weapons of mass destruction in Iraq in 2003. It is also prevalent in science, as exemplified by the polarized debate over the origins of the Covid pandemic .

Our goal in this paper is to develop algorithms that are able to _detect_ bias in the form of non-Bayesian updating of beliefs. To our knowledge, we are the first to formalize and analytically address this problem, and we aim to build an initial framework that future work would build on. In the long term, we believe such algorithms could have many applications, including understanding to whatdegree the foregoing type of bias contributes to disagreement and polarization, and discounting the opinions of biased agents to improve collective decision making.

**Our approach.** The first question we need to answer is how to _quantify_ bias. In this first investigation, we adopt a linear model of bias that was proposed and used as a general belief updating model in economics . If the prior is \(_{0}\) and the correct Bayesian posterior upon receiving a _signal_ (or evidence) \(s\) is denoted \(_{s}\), we posit that an agent with bias \(w\) adopts the belief \(w_{0}+(1-w)_{s}\). At the extremes, an agent with bias \(w=0\) performs perfect Bayesian updating and an agent with bias \(w=1\) cannot be convinced to budge from the prior.

The bigger conceptual question is how we can infer an agent's bias. To address it, we take an approach that is inspired by the literature on _information design_. In our context, suppose that we (the _principal_) and the agent have asymmetric information: while both share a common (say public) prior about the state of the world, the principal knows the true (realized) state of the world, but the agent does not. The _principal_ publicly commits to a (randomized) _signaling scheme_ that specifies the probability of sending each possible signal given each possible realized state of the world. Given their knowledge of the latter, the principal draws a signal from the specified distribution and sends it to the agent. Upon receiving such a signal, the agent updates their beliefs about the state of the world (from the common prior) and then takes an _action_ that maximizes their expected utility according to a given utility function. Similarly to the example we started with, it is the action taken by the agent that can (indirectly) reveal their degree of bias.

Note that the problem of estimating the exact level of bias reduces to the problem of detecting whether the agent's bias is above or below some threshold. Indeed, to estimate the level of bias to an accuracy of \(\), \((1/)\) such threshold queries suffice by using binary search. The challenge, then, is to design signaling schemes that test whether bias is above or below a given threshold in the most efficient way, that is, using a minimum number of signals in expectation.

**Our results.** We design a polynomial-time algorithm that computes optimal signaling schemes, in Section4. We first show that _constant_ algorithms, which repeatedly use the same signaling scheme, are as powerful as _adaptive_ algorithms, which can vary the scheme over time based on historical data (Lemma4.1); we can therefore restrict our attention to constant algorithms. In Lemma4.5, we establish a version of the _revelation principle_ for the bias detection problem, which asserts that optimal signaling schemes need only use signals that can be interpreted as action recommendations. Finally, building on these insights, we show that the optimal solution to our problem is obtained by solving a "small" linear program (Algorithm1 and Theorem4.6).

In Section5, we present a geometric characterization of optimal signaling schemes (Theorem5.2), which sheds additional light on the performance of the algorithm. In particular, the characterization provides sufficient and necessary conditions for the testability of bias, and also identifies cases where only a single sample is needed for this task.

**Related work.** There is a significant body of _experimental_ work in the social sciences aiming to explain the failure of partisans to reach similar beliefs on factual questions where there is a large amount of publicly available evidence. The fact that biased belief updating occurs is undisputed (to our knowledge), and the focus is on understanding the factors that play a role. In particular, a prominent line of work supports the (perhaps counterintuitive) hypothesis that the more cognitively sophisticated a partisan is, the more politically biased is their belief update process . These results are challenged by more recent work by Tappin et al. , who found that greater analytical thinking is associated with belief updates that are less biased, using an experimental design that explicitly measures the proximity of belief updates to a correct Bayesian posterior. While these studies provide empirical underpinnings for our theoretical model, their research questions are orthogonal to ours: we aim to measure the magnitude of bias regardless of its source.

Classical work in _information design_ studies how a principal can strategically provide information to induce an agent to take actions that are beneficial for the principal, assuming a perfectly Bayesian agent. Various relaxations of the perfectly Bayesian assumption have been investigated . The work by de Clippel and Zhang  is close to us, which studies biased belief update models including the linear model. However, their goal is to maximize the principal's utility with the agent's bias fully known. In our problem the agent's bias level is unknown, and the principal's goal is to infer the agent's bias level instead of maximizing their own utility. Tang and Ho  present real-world experiments showing that human belief updates are close to a linear bias model, which supports our theoretical assumption.

## 2 Model

**Biased agent.** Consider a standard Bayesian setting: the relevant _state of the world_ is \(\), distributed according to some known prior distribution \(_{0}\). If an agent were perfectly Bayesian, when receiving some new information ("signal") \(s\) and with the knowledge of the conditional distributions \(P(s|)\) for all \(\), they would update their belief about the state of the world according to Bayes' Rule: \(_{s}()=P(|s)=()P(s|)}{P(s)}\). We refer to \(_{s}\) as the true posterior belief induced by \(s\). Being biased, the agent's belief after seeing \(s\), denoted \(_{s}\), is a convex combination of \(_{s}\) and \(_{0}\):

\[_{s}=w_{0}+(1-w)_{s},\]

where \(w\) is the unknown _bias level_, capturing the agent's inclination to retain their prior belief in the presence of new information. This linear model was proposed and adopted in economics for non-Bayesian belief updating , in order to capture people's conservatism in processing new information and their tendency to protect their beliefs .

The agent can choose an action from a finite set \(A\) and has a state-dependent utility function \(U:A\). They receive utility \(U(a,)\) when taking action \(a\) in state \(\). The agent will act according to their (biased) belief \(_{s}\) and choose an action \(a\) that maximizes their expected utility:

\[a*{arg\,max}_{a A}\,_{_{s}}[U(a, )]=*{arg\,max}_{a A}\,_{}_{s}( )U(a,).\]

In the absence of any additional information, the agent operates based on the prior belief \(_{0}\) and will select an action deemed optimal with respect to \(_{0}\). We introduce the following mild assumption to ensure the uniqueness of this action:

**Assumption 2.1**.: _There is a unique action that maximizes the expected utility based on the prior belief \(_{0}\): \(|*{arg\,max}_{a A}\{_{}_{0}()U(a, )\}|=1\)._

This assumption will be made throughout the paper. We denote the unique optimal action on the prior belief as \(a_{0}=*{arg\,max}_{a A}\{_{}_{0}( )U(a,)\}\), and call it the _default action_.

**Bias detection.** The principal, who knows the prior \(_{0}\) and the agent's utility function \(U\), seeks to infer the agent's bias level from their action as efficiently as possible. The principal has an informational advantage -- they observe the independent realizations of the state of the world at each time step. In other words, the principal knows \(_{t}\), an independent sample drawn according to \(_{0}\) at time \(t\). The principal wants to design signaling schemes to strategically reveal information about \(_{t}\) to the agent, hoping to influence the agent's biased belief in a way that the agent's chosen actions reveal information about their bias level. Specifically, with a finite signal space \(S\), the principal can commit to a _signaling scheme_\(_{t}:(S)\) at time \(t\), where \(_{t}(s|)\) specifies the probability of sending signal \(s\) in state \(\) at time \(t\). After seeing a signal \(s_{t}\), drawn according to \(_{t}(s|_{t})\) at time \(t\), the agent takes action \(a_{t}\) that is optimal for their biased belief \(_{s_{t}}\). The principal infres information about bias \(w\) from the history of signaling schemes, realized states, realized signals, and agent actions \(_{t}=\{(_{1},_{1},s_{1},a_{1}),,(_{t},_{t}, s_{t},a_{t})\}\). We denote by \(\) an adaptive algorithm that the principal uses to decide on the signaling scheme at time \(t+1\) based on history \(_{t}\).

Given a threshold \((0,1)\), the principal wants to design \(\) to answer the following question:

_Is the agent's bias level \(w\) greater than or equal to \(\) or less than or equal to \(\)?1_

As noted earlier, by answering the above threshold question, one can also estimate the bias level \(w\) within accuracy \(\) by performing \((1/)\) iterations of binary search. This effectively reduces the broader task of estimating \(w\) to a sequence of targeted threshold checks. By employing an adaptive signaling scheme, this approach lets us approximate \(w\) to any desired precision, providing an efficient solution to the bias estimation problem.

An algorithm \(\) for the above question terminates as soon as it can output a deterministic answer. The number of time steps for \(\) to terminate, denoted by \(T_{}(,w)\), is a random variable. The sample complexity of \(\) is defined to be the expected termination time in the worst case over \(w\):

**Definition 2.1** (sample complexity).: _The (worst-case) sample complexity of \(\) is defined as2_

\[T_{}()=_{w}[T_{}(,w)].\]

Our objective is to develop an algorithm \(\) that can determine whether \(w\) or \(w\) with minimal sample complexity. Specifically, we want to solve the following minimax problem:

\[_{}_{w}[T_{}(,w)].\]

We say that an algorithm \(\) is _constant_ if it keeps using the same signaling scheme repeatedly until termination. Constant algorithms are a special case of _non-adaptive_ algorithms, which may vary the signaling schemes over time but remain independent of historical data.

**Preliminaries.** We now introduce the well-known splitting lemma from the information design literature . It relates a signaling scheme with a set of induced true posteriors for a Bayesian agent and a distribution over the set of true posteriors.

**Lemma 2.1** (Splitting Lemma, e.g., ).: _Let \(\) be a signaling scheme where each signal \(s S\) is sent with unconditional probability \((s)=_{}_{0}()(s|)\) and induces true posterior \(_{s}\). Then, the prior \(_{0}\) equals the convex combination of \(\{_{s}\}_{s S}\) with weights \(\{(s)\}_{s S}\): \(_{0}=_{s S}(s)_{s}\). Conversely, if the prior can be expressed as a convex combination of distributions \(_{s}^{}()\): \(_{0}=_{s S}p_{s}_{s}^{}\), where \(p_{s} 0,_{s S}p_{s}=1\), then there exists a signaling scheme \(\) where each signal \(s\) is sent with unconditional probability \((s)=p_{s}\) and induces posterior \(_{s}^{}\)._

The splitting lemma is also referred as the Bayesian consistency condition. It allows one to think about choosing a signaling scheme as choosing a set of true posteriors, \(\{_{s}\}_{s S}\), and a distribution over the set, \(\{(s)\}_{s S}\), in a Bayesian consistent way.

## 3 Warm-Up: A Two-State, Two-Action Example

How can the principal design a signaling scheme to learn the agent's bias level? We use a simple two-state, two-action example to demonstrate how inducing a specific true posterior belief will allow the principal to determine whether \(w\) or \(w\).

The two states of the world are represented as \(\{,\,\}\). The agent has two possible actions: \(\) and \(\). Taking the \(\) action always yields a utility of \(0\), independently of the state. For the \(\) action, the utility is \(a\) if the state is \(\) and \(-b\) otherwise; \(a,b>0\). We use the probability of the \(\) state to represent a belief, so the prior is a number \(_{0}\), which is only a slight abuse of notation. With belief \(\) for the \(\) state (and \(1-\) for the \(\) state), the agent's expected utility for choosing the \(\) action is \(a-b(1-)=(a+b)-b\). Thus, the \(\) action is better than the \(\) action (so the agent will take \(\)) if

\[(a+b)-b>0>=:^{*}.\] (1)

Conversely, the \(\) action is better if \(<^{*}\). Here, \(^{*}=\) is an _indifference belief_ where the agent is indifferent between the two actions. We assume that the prior \(_{0}\) satisfies \(0<_{0}<^{*}\), so the agent chooses the \(\) action by default.

Consider the following constant signaling scheme \(_{}\) with two signals \(\{G,B\}\):

* If the state is \(\), send signal \(G\) with probability one.
* If the state is \(\), send signal \(B\) with probability \(-_{0}}{(^{*}-_{0})(1-_{0})}\) and signal \(G\) with the complement probability.

We will show that, by repeatedly using \(_{}\), we can test whether the agent's bias \(w\) is \(\) or \(\). By Bayes' Rule, the true posterior beliefs (for the \(\) state) associated with the two signals are \(_{B}=0\) (i.e., on receiving \(B\), the agent knows the state is \(\) for sure) and

\[_{G}=P(|G)=_{}(G|)}{_ {0}_{}(G|)+(1-_{0})_{}(G| )}=-_{0}}{1-}.\]

Notably, the posterior \(_{G}\) satisfies the following property: if the agent's bias level \(w\) is exactly equal to \(\), then the agent's biased belief is equal to the indifference belief:

\[w=,_{G}=_{0}+(1-)_{G}=^{*}.\]

We also note the inequality \(_{0}<^{*}<_{G}\). As a result, if the agent's bias level \(w\) is greater than \(\), then the biased belief will be smaller than \(^{*}\), and otherwise the opposite is true:

\[w>, w_{0}+(1-w)_{G}<^{*};w<,  w_{0}+(1-w)_{G}>^{*}.\]

By Equation1, this means that the agent will take the \(\) action if \(w>\), and the \(\) action if \(w<\) (on receiving \(G\)). Therefore, by observing which action is taken by the agent when signal \(G\) is sent, we can immediately tell whether \(w\) or \(w\). This leads to the following:

**Theorem 3.1**.: _In the two-state, two-action example, for any threshold \([0,}{1-_{0}}]\), the above constant signaling scheme \(_{}\) can test whether the agent's bias \(w\) satisfies \(w\) or \(w\): specifically, whenever the signal \(G\) is sent,_

* _if the agent takes action_ \(\)_, then_ \(w\)_,_
* _if the agent takes action_ \(\)_, then_ \(w\)_._

_The sample complexity of this scheme is \(-_{0}}{_{0}(1-)}+1\), which increases with \(\)._

Proof.: The range \([0,}{1-_{0}}]\) ensures that the probability \(_{}(B|)=-_{0}}{(^{*}-_{0})(1-_ {0})}\) is in \(\). The two items in the theorem follow from the argument before the theorem statement. The sample complexity is equal to the expected number of time steps until a \(G\) signal is sent, which is a geometric random variable with success probability \(P(G)=_{0}_{}(G|)+(1-_{0})_{}(G|)= (1-)}{^{*}-_{0}}\). So the sample complexity is equal to the mean \(=-_{0}}{_{0}(1-)}+1\). 

The _main intuition_ behind this result is that in order to test whether \(w\) or \(w\), we design a signaling scheme where certain signals induce posteriors that make the agent _indifferent between two actions if the agent's bias level is exactly \(\)_. Then, the action actually taken by the agent will directly reveal whether \(w\) or \(w\). Such signals are _useful_ signals, but not all signals are necessarily useful. The sample complexity is then determined by the total probability of useful signals. This intuition will carry over to computing the optimal signaling scheme for the general case in Section4.

Finally, we remark that using the constant signaling scheme \(_{}\) constructed above to test \(w\) or \(w\) is in fact the optimal adaptive algorithm, according to the results we will present in Section4. So, the minimal sample complexity to test whether \(w\) or \(w\) in this two-state, two-action example is exactly \(-_{0}}{_{0}(1-)}+1\) as shown in Theorem3.1.

## 4 Computing the Optimal Signaling Scheme in the General Case

In this section, we generalize the initial observations from the previous section to the case with any number of actions and states and general utility function \(U\). We will show how to compute the optimal algorithm (signaling scheme) to test the agent's bias level. There are three key ingredients. First, we prove that we can use a constant signaling scheme. Second, we develop a "revelation principle" to further simplify the space of signaling schemes. Building on these two steps, we show that the optimal signaling scheme can be computed by a linear program.

### Optimality of Constant Signaling Schemes

In this subsection, we show that adaptive algorithms are no better than constant algorithms for the problem of testing whether \(w\) or \(w\). Therefore, to find the algorithm with minimal sample complexity, we only need to consider constant algorithms/signaling schemes.

**Lemma 4.1**.: _Fix \((0,1)\). For the problem of testing whether \(w\) or \(w\), the sample complexity of any adaptive algorithm is at least that of the optimal constant algorithm (i.e., using a fixed signaling scheme repeatedly)._

To prove this lemma, we introduce some notations. For any action \(a A\{a_{0}\}\), define vector

\[c_{a}=(c_{a,})_{}=U(a_{0},)-U(a,) _{}^{||},\] (2)

whose components are the utility differences between the default action \(a_{0}\) and any other action \(a\) at different states \(\). Let \(R_{a_{0}}()\) be the region of beliefs under which the agent strictly prefers \(a_{0}\) over any other action:

\[R_{a_{0}}=() a A\{a_{0}\},\,c _{a}^{}>0}.\]

It is the intersection of \(|A|-1\) open halfspaces with the probability simplex \(()\). As the agent strictly prefers \(a_{0}\) at the prior \(_{0}\), we have \(_{0} R_{a_{0}}\). The boundary of this region, \( R_{a_{0}}\), is the set of beliefs where the agent is indifferent between \(a_{0}\) and at least one other action \(a A\{a_{0}\}\) and \(a_{0}\) and \(a\) are both (weakly) better than any other action:

\[ R_{a_{0}}=() a A\{a _{0}\},c_{a}^{}=0 a^{} A\{a_{0}\},\,c_{a^{}}^{} 0 }.\] (3)

Lastly, the exterior of \(R_{a_{0}}\), denoted as \(R_{a_{0}}\), comprises the set of beliefs where the agent strictly prefers not to choose \(a_{0}\):

\[R_{a_{0}}=()(R_{a_{0}} R_{a_{0}} )=() a A\{a_{0}\},\,c_{a}^{ }<0}.\]

Given a signaling scheme \(\), we classify its signals into three types based on the location of the biased belief associated with the signal with respect to the region \(R_{a_{0}}\).

**Definition 4.1**.: _Let \((0,1)\) be a parameter. Let \(s S\) be a signal from a signaling scheme \(\), with associated true posterior \(_{s}\) and \(\)-biased posterior \(_{s}^{}=_{0}+(1-)_{s}\). We say \(s\) is_

* _an_ _internal signal_ _if_ \(_{s}^{} R_{a_{0}}\)_;_
* \(a\) _boundary signal_ _if_ \(_{s}^{} R_{a_{0}}\)_;_
* _an_ _external signal_ _if_ \(_{s}^{}R_{a_{0}}\)_._

The above classification helps to formalize the idea of whether a signal is "useful" for bias detection. A boundary signal is useful because the action taken by the agent after receiving a boundary signal immediately tells whether \(w\) or \(w\):

**Lemma 4.2**.: _When a boundary signal is realized, the agent's action immediately reveals whether \(w\) or \(w\). Specifically, if the agent chooses action \(a_{0}\), then \(w\); otherwise, \(w\)._

Proof.: If the agent's bias level satisfies \(w<\), then the biased belief \(_{s}=w_{0}+(1-w)_{s}\) must be inside \(R_{a_{0}}\) (because \(_{s}^{}=_{0}+(1-)_{s}\) is on the boundary of \(R_{a_{0}}\) and \(_{0} R_{a_{0}}\)), so the agent strictly prefers the default action \(a_{0}\). If \(w>\), then the biased belief \(_{s}\) is outside of \(R_{a_{0}}\), so the agent will not take action \(a_{0}\). 

An external signal might also be useful in revealing whether \(w\) or \(w\) if the agent is indifferent between some actions \(a_{1},a_{2}\) other than \(a_{0}\) at the \(\)-biased belief \(_{s}^{}\). However, the following lemma shows that, in such cases, we can always modify the signaling scheme to turn the external signal into a boundary signal. This modification will increase the total probability of useful signals and hence reduce the sample complexity. The proof of this lemma is in Appendix A.1.

**Lemma 4.3**.: _Suppose \(\) is an adaptive algorithm that uses signaling schemes with internal, boundary, and external signals. Then, there exists another adaptive algorithm \(^{}\) with equal or lower sample complexity that employs only signaling schemes with internal and boundary signals._

An internal signal, on the other hand, is not useful for testing \(w\) or \(w\), for the following reason. For an internal signal, the biased belief with bias level \(\), \(_{s}^{}\), lies inside \(R_{a_{0}}\). Since \(R_{a_{0}}\) is an open region, there must exist a small number \(>0\) such that when the agent has bias level \(w=+\) or \(-\), the biased belief with bias level \(w\), \(w_{0}+(1-w)_{s}\), is also inside the region \(R_{a_{0}}\), so the agent will take action \(a_{0}\). As the agent takes \(a_{0}\) under both \(w=+\) and \(-\), we cannot distinguish these two cases, so this signal is not helpful in determining \(w\) or \(w\). The following lemma formalizes the idea that internal signals are not useful:

**Lemma 4.4**.: _To test whether \(w\) or \(w\), any adaptive algorithm that uses signaling schemes with boundary and internal signals cannot terminate until a boundary signal is sent._

Proof of Lemma 4.1.: By Lemma 4.3, the optimal adaptive algorithm only uses signaling schemes with boundary and internal signals. By Lemma 4.4, the algorithm cannot terminate until a boundary signal is sent. By Lemma 4.2, the algorithm terminates when a boundary signal is sent. We conclude that the termination time of any adaptive algorithm cannot be better than the constant algorithm that keeps using the signaling scheme that maximizes the total probability of boundary signals. 

### Revelation Principle

To compute the optimal constant signaling scheme, we need another technique that is similar to the _revelation principle_ in the information design literature [13; 6]. The revelation principle says that, in some information design problems, it is without loss of generality to consider only "direct" signaling schemes where signals are recommendations of actions for the agent, that is, the signal space is \(S=A\), and when the principal sends signal \(a\), it should be optimal for the agent to take action \(a\) given the posterior belief induced by signal \(a\).

Unlike classical information design problems where the agent is unbiased, our problem involves a biased agent, so we need a different revelation principle: the signals are still action recommendations, but when the principal sends signal \(a\), action \(a\) is optimal for an agent with bias level exactly \(\); moreover, if \(a a_{0}\), then an agent with bias level \(\) will be indifferent between \(a\) and \(a_{0}\). This insight is formalized in the following lemma:

**Lemma 4.5** (revelation principle for bias detection).: _Let \(\) be an arbitrary signaling scheme that can test \(w\) or \(w\). Then, there exists another signaling scheme \(^{}\) that can do so with signal space \(S=A\) such that:_

1. _Given signal_ \(a A\)_, action_ \(a\) _is an optimal action for any agent with bias level_ \(w=\)_._
2. _Given signal_ \(a A\{a_{0}\}\)_, actions_ \(a\) _and_ \(a_{0}\) _are both optimal for any agent with bias level_ \(w=\)_. As a corollary, if the agent's bias level_ \(w<\)_, then the agent strictly prefers_ \(a\) _over_ \(a_{0}\)_; and if_ \(w>\)_, then the agent strictly prefers_ \(a_{0}\) _over any other actions._
3. _The sample complexity satisfies_ \(T_{}(^{}) T_{}()\)_._

In the above signaling scheme \(^{}\), every \(a A\{a_{0}\}\) is a boundary signal (Definition 4.1), which is useful for testing bias: given signal \(a A\{a_{0}\}\), if the agent takes action \(a_{0}\), then it must be \(w\); otherwise \(w\). The signal \(a_{0}\) is internal and not useful for determining \(w\) or \(w\). So, the sample complexity of \(^{}\) is equal to the expected time steps until a signal in \(A\{a_{0}\}\) is sent.

The idea behind Lemma 4.5 is _combination of signals_. Suppose there is a signaling scheme that can determine whether \(w\) or \(w\) with a signal space larger than \(A\). There must exist two signals \(s\) and \(s^{}\) under which the agent is indifferent between \(a_{0}\) and some action \(a a_{0}\) if the agent's bias level is exactly \(\). We can then combine the two signals into a single signal \(s^{}\) under which the agent remains indifferent between \(a_{0}\) and \(a\), yielding a new signaling scheme with a smaller signal space. Repeating this can reduce the signal space to size \(|A|\). See Appendix A.3 for the full proof.

### Algorithm for Computing the Optimal Signaling Scheme

Finally, we present an algorithm to compute the optimal (minimal sample complexity) signaling scheme to test whether \(w\) or \(w\). The revelation principle in the previous subsection ensures that we only need a direct signaling scheme where signals are action recommendations. The optimal direct signaling scheme turns out to be solvable by a linear program, detailed in Algorithm 1. In the linear program, the constraint in Equation (5) ensures that whenever the principal recommends action \(a A\), it is optimal for an agent with bias level \(\) to take action \(a\); this satisfies condition (1) in the revelation principle (Lemma 4.5). The indifference constraint (Equation (5)) ensures that when the recommended action \(a\) is not \(a_{0}\), an agent with bias level \(\) is indifferent between \(a\) and \(a_{0}\); this satisfies condition (2) in the revelation principle. The objective (Equation (4)) is to maximize the probability of useful signals (those in \(A\{a_{0}\}\)), hence minimize the sample complexity.

**Theorem 4.6**.: _Algorithm 1 finds a constant signaling scheme for testing \(w\) or \(\) that is optimal among all adaptive signaling schemes. The sample complexity of the optimal signaling scheme is \(1/p^{*}\), where \(p^{*}\) is the optimal objective value in Equation (4)._Using the above optimal signaling scheme, whenever the principal recommends an action \(a\) other than \(a_{0}\), the agent's action immediately reveals whether \(w\) or \(w\): if the agent indeed follows the recommendation or takes any other action than \(a_{0}\), then the bias must be small (\(w\)); if the agent takes \(a_{0}\) instead, the bias must be large (\(w\)). Thus, the expected sample complexity is equal to the expected number of iterations until a signal in \(A\{a_{0}\}\) is sent, which is \(1/p^{*3}\).

The linear program in Algorithm 1 has a polynomial size in \(|A|\) (the number of actions) and \(||\) (the number of states), so it is a polynomial-time algorithm. The solution \(p^{*}\) depends on the geometry of the problem instance and does not seem to have a closed-form expression.

The remainder of this section proves Theorem 4.6. The proof requires an additional lemma:

**Lemma 4.7**.: _Given a signaling scheme \(=((a|))_{a A,}\) and an agent's bias level \(w\), after signal \(a\) is sent, the agent strictly prefers action \(a_{1}\) over \(a_{2}\) under the biased belief if and only if:_

\[_{}(a|)_{0}()(1-w) U( a_{1},a_{2},)+w_{^{}}_{0}(^{})  U(a_{1},a_{2},^{})>0.\]

Proof.: The agent's biased belief under signal \(a\) and bias level \(w\) is given by \((1-w)()(a|)}{_{^{}}_{0 }(^{})(a|^{})}+w_{0}(),\;.\) The condition for the agent to strictly prefer \(a_{1}\) over \(a_{2}\) is that the expected utility under the biased belief when choosing \(a_{1}\) is greater than that of \(a_{2}\):

\[_{}((1-w)()(a|)}{_{ ^{}}_{0}(^{})(a|^{})}+w_ {0}()) U(a_{1},a_{2},)>0,\]

where \( U(a_{1},a_{2},)=U(a_{1},)-U(a_{2},)\). Multiplying by \(_{^{}}_{0}(^{})(a|^{})\), we obtain:

\[(1-w)_{}_{0}()(a|) U(a_{1},a_{2}, )+w_{}_{0}()_{^{}} _{0}(^{})(a|^{}) U(a_{1},a_{2},)>0.\]

Factoring out the terms, this can be rewritten as:

\[_{}(a|)_{0}()(1-w) U(a_{1},a_{2},)+w_{^{}}_{0}(^{}) U (a_{1},a_{2},^{})>0.\]This final expression is positive if and only if the agent to strictly prefer \(a_{1}\) over \(a_{2}\). 

Proof of Theorem 4.6.: According to Lemma 4.1 (constant algorithms are optimal) and Lemma 4.5 (revelation principle), to find an optimal adaptive algorithm we only need to find the optimal constant signaling scheme that satisfies the conditions in Lemma 4.5. We verify that the signaling scheme computed from the linear program in Algorithm 1 satisfies the conditions in Lemma 4.5:

* The optimality constraint (Equation (5)) in the linear program, together with Lemma 4.7, ensures that: whenever signal \(a A\) is sent, action \(a\) is weakly better than any other action for an agent with bias level \(w=\). This satisfies the first condition in Lemma 4.5.
* The indifference constraint (Equation (5)), together with Lemma 4.7, ensures that: whenever \(a A\{a_{0}\}\) is sent, the agent is indifferent between action \(a\) and \(a_{0}\) if the bias level \(w=\). Then, by the optimality constraint (Equation (5)), we have both \(a\) and \(a_{0}\) being optimal actions. This satisfies the second condition in Lemma 4.5.

We then argue that the solution of the linear program is the optimal signaling scheme that satisfies the conditions of Lemma 4.5. According to our argument after Lemma 4.5, only the signals in \(A\{a_{0}\}\) are useful signals, so the sample complexity is equal to the expected number of time steps until a signal in \(A\{a_{0}\}\) is sent. The probability that a signal in \(A\{a_{0}\}\) is sent at each time step is

\[_{a A\{a_{0}\}}(a)=_{a A\{a_{0}\}}_{ }_{0}()(a|).\]

The expected number of time steps is the inverse \(\}}_{}_{0}() (a|)}\) (because the number of time steps is a geometric random variable). The linear program maximizes the probability \(_{a A\{a_{0}\}}_{}_{0}()(a|)\), so it minimizes the sample complexity. 

## 5 Geometric Characterization of the Testability of Bias

To complement the algorithmic solution presented in the previous section, this section provides a geometric characterization of the bias detection problem. We identify the conditions under which testing whether \(w\) or \(w\) can be done in only _one_ sample, in finite number of samples, or cannot be done at all (which is the scenario where the linear program in Algorithm 1 is infeasible).

By Assumption 2.1 (\(a_{0}\) is strictly better than other actions at prior \(_{0}\)), we have:

\[c_{a}^{}_{0}=_{}_{0}()U(a_{0}, )-U(a,)>0, a A\{a_{0}\},\]

Figure 1: The three qualitatively different cases for detecting the level of bias, each illustrated within a simplex over three states where \(_{0}\) is the prior belief. Each point in the simplex corresponds to an optimal action for the agent. Green curves indicate indifference between the default action \(a_{0}\) and another action under an unbiased belief. Orange curves are translated versions of these indifference curves; a posterior on these curves means the agentâ€™s biased belief (at bias level \(\)) aligns with the green curves. From (a) to (c), \(\) increases, translating the orange curves further. In Figure 0(a), \(_{0}\) can be represented as a convex combination of points on the translated curves, allowing bias level detection with a single sample. In Figure 0(b), only some signals are useful, requiring more than one sample in the worst case. In Figure 0(c), the bias level cannot be tested against \(\).

where \(c_{a}\) is as defined in Equation2. Define \(I_{a}\) as the set of indifference beliefs between action \(a\) and \(a_{0}\), which is the intersection of the hyperplane \(\{x|c_{a}^{}x=0\}\) and the probability simplex \(()\):

\[I_{a}:=\{() c_{a}^{}=0\}.\]

Given a parameter \((0,1)\), for which we want to test whether \(w\) or \(w\), let

\[I_{a,}=\{()(1-)+_{0} I_{a}\}\]

be the set of posterior beliefs for which, if the agent's bias level is exactly \(\), then the agent's biased belief will fall within the indifference set \(I_{a}\).

**Lemma 5.1**.: \(I_{a,}\) _is equal to the intersection of the probability simplex \(()\) and a translation of the hyperplane \(\{x c_{a}^{}x=0\}\): \(I_{a,}=() c_{a}^{}=-c_{a}^{}_{0}}.\)_

The proof of this lemma is in SectionB.1. With this representation of \(I_{a,}\) in hand, we can now present a geometric characterization of the testability of bias.

**Theorem 5.2** (geometric characterization).: _Fix \((0,1)\). The problem of testing \(w\) or \(w\)_

* _Can be solved with a_ single sample _(the sample complexity is 1) if and only if the prior_ \(_{0}\) _is in the convex hull formed by the translated sets_ \(I_{a,}\) _for all non-default actions_ \(a A\{a_{0}\}\)_: i.e.,_ \(_{0}(\;_{a A\{a_{0}\}}I_{a,}\;)\)_._
* _Can be solved (with finite sample complexity) if and only if_ \(I_{a,}\) _for at least one_ \(a A\{a_{0}\}\)_._
* _Cannot be solved if_ \(I_{a,}=\) _for all_ \(a A\{a_{0}\}\)_._

Figure1 illustrates the three cases of Theorem5.2. In the first case, the solution of the linear program in Algorithm1 satisfies \(_{a A\{a_{0}\}}_{}(a|)_{0}( )=1\), meaning that useful signals are sent with probability 1, which allows us to tell whether \(w\) or \(w\) immediately. In the second case, the total probability of useful signals satisfies \(_{a A\{a_{0}\}}_{}(a|)_{0}( )<1\), so the sample complexity is more than 1. In the third case, the linear program in Algorithm1 is not feasible, so \(w\) or \(w\) cannot be determined; importantly, this is not a limitation of our particular algorithm, but a general impossibility in our model. The proof of Theorem5.2 is in SectionB.2.

## 6 Discussion

Our approach has some limitations; here we discuss the two that we view as most significant.

First, we have assumed a linear model of bias. While the linear model is common in the literature , we also consider a more general model of bias (in SectionC): as the bias level \(w\) increases from \(0\) to \(1\), the agent's belief changes from the true posterior \(_{s}\) to the prior \(_{0}\) according to some general continuous function \((_{0},_{s},w)\). We show that, as long as the function \(\) satisfies a certain single-crossing property (as \(w\) increases, once the agent starts to prefer the default action \(a_{0}\), they will not change the preferred action anymore), our results regarding the optimality of constant signaling schemes and the geometric characterization still hold, while the revelation principle and the linear program algorithm no longer work because \(\) is not linear. We consider it an interesting challenge to come up with more general models of bias that are still tractable, in the sense that one can efficiently design good signaling schemes with reasonable sample complexity bounds.

Second, we have assumed that the agent's prior is the same as the real prior from which states of the world are drawn. But what if the agent's prior is different? Our results directly extend to the case where the agent has a wrong, _known_ prior. If the agent's prior is unknown, then our problem becomes significantly more challenging. More generally, the agent may have a private type that determines both their prior and utility and is unknown to the principal. We conjecture that testing the agent's bias in this case becomes impossible, because if different types consistently take "opposite" actions, then the actions provide no information about the agent's bias.

Despite these limitations, we view our paper as making significant progress on a novel problem that seems fundamental. Our results suggest that practical algorithms for detecting bias in belief update are within reach and, in the long term, may lead to new insights on issues of societal importance. In particular, we anticipate future research in more complex situations such as combining decisions of many experts (human or AI) after measuring and accounting for their individual biases.