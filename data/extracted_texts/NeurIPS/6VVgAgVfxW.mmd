# Team-Fictitious Play for Reaching Team-Nash Equilibrium in Multi-team Games

Ahmed Said Donmez

Bilkent University

said.donmez@bilkent.edu.tr

&Yuksel Arslantas

Bilkent University

yuksel.arslantas@bilkent.edu.tr

&Muhammed O. Sayin

Bilkent University

sayin@ee.bilkent.edu.tr

###### Abstract

Multi-team games, prevalent in robotics and resource management, involve team members striving for a joint best response against other teams. Team-Nash equilibrium (TNE) predicts the outcomes of such coordinated interactions. However, can teams of self-interested agents reach TNE? We introduce Team-Fictitious Play (Team-FP), a new variant of fictitious play where agents respond to the last actions of team members and the beliefs formed about other teams with some inertia in action updates. This design is essential in team coordination beyond the classical fictitious play dynamics. We focus on zero-sum potential team games (ZSPTGs) where teams can interact pairwise while the team members do not necessarily have identical payoffs. We show that Team-FP reaches near TNE in ZSPTGs with a quantifiable error bound. We extend Team-FP dynamics to multi-team Markov games for model-based and model-free cases. The convergence analysis tackles the challenge of non-stationarity induced by evolving opponent strategies based on the optimal coupling lemma and stochastic differential inclusion approximation methods. Our work strengthens the foundation for using TNE to predict the behavior of decentralized teams and offers a practical rule for team learning in multi-team environments. We provide extensive simulations of Team-FP dynamics and compare its performance with other widely studied dynamics such as smooth fictitious play and multiplicative weights update. We further explore how different parameters impact the speed of convergence.

## 1 Introduction

Multi-team games are increasingly common, e.g., in robotics and resource management (Silva and Chaimowicz, 2017; Vinyals et al., 2019; Jaderberg et al., 2019). Unlike non-cooperative multi-agent settings, team members strive for a joint best response against other teams as if the entire team is a single decision-maker. Team-Nash equilibrium (TNE), where team members coordinate in the best team response against other teams, can capture this to predict the outcome of coordinated team interactions (Farina et al., 2018; Zhang et al., 2021). Game-theoretical equilibrium is often justified by its emergence from non-equilibrium adaptation of self-interested learners (e.g., see (Fudenberg and Levine, 2009)). However, the question of whether the teams of self-interested agents can reach TNE in multi-team games remains largely unexplored. This paper investigates this very question.

For example, TNE generally can arise if the team members can learn to correlate their actions in the best team response independent of the opponent. However, the widely studied fictitious play (FP) dynamics do not necessarily reach the best team outcome even when there are no opponents, e.g., inpotential games. We present a slight adjustment of FP, called _Team-FP_, provably reaching TNE in multi-team competition even with networked interconnections, where agents' payoffs depend only on the neighbors' actions. Similar to the FP, here, agents respond greedily to the beliefs formed about the opponent teams' joint play based on past observations. Different from the FP, Team-FP incorporates the key features: \((i)\) response to the last actions of team members, and \((ii)\) inertia in action updates. These features, inspired by log-linear learning dynamics (Blume, 1993), play a crucial role in driving team coordination towards TNE. Notably, Team-FP reduces to the smoothed FP (Fudenberg and Kreps, 1993) (or log-linear learning) when each team has a single agent (or there is a single team).

Multi-team competition spans diverse domains. For example, robotics, resource management, online gaming, and financial markets (Kitano et al., 1997; Cardenas et al., 2009; Silva and Chaimowicz, 2017; Vinyals et al., 2019; Jaderberg et al., 2019) involve multi-team competition. To model such interactions, we consider multiple teams with possibly different number of team members. These team members have networked interconnections, as depicted in Figure 1. We focus on multi-team _zero-sum potential team games_ where teams have _pairwise interactions_ (ZSPTGs). For any opponent team strategy, team members effectively play an underlying potential game, as in distributed optimization applications, e.g., see (Arslan et al., 2007; Xu et al., 2012; Zheng et al., 2014). Notably, ZSPTGs reduce to zero-sum polymatrix games (Cai et al., 2016) if each team has a single agent, and to potential games (Monderer and Shapley, 1996) if there is a single team. Additionally, widely studied two-team zero-sum games, e.g., see (Farina et al., 2018; Zhang et al., 2021; Carminati et al., 2022; Kalogiannis et al., 2022), are a special case of ZSPTGs.

We show that the Team-FP dynamics reach near TNE in ZSPTGs. This means that the empirical average of team actions converge to the near best response each team can take against the average actions of other teams. We quantify the approximation error, showing it decreases with the level of exploration in the agents' responses. Similar to the FP dynamics, Team-FP is also _rational_: teams can learn (near) optimal strategies if opponent teams play stationary strategies. These results strengthen the applicability of TNE for predicting team behavior in multi-team competition and provide a practical rule for teams of self-interested agents to learn coordination in multi-team settings.

A key challenge in our analysis is handling the non-stationary nature of learning, as opponent teams' strategies change over time. We address this by leveraging the optimal coupling lemma (e.g. see (Levin and Peres, 2017; Chapter 4)) and stochastic differential inclusion approximation methods (e.g., see (Benaim et al., 2005; Perkins and Leslie, 2013)) to the repeated play of games. Motivated from the recent interest in multi-agent reinforcement learning, we can extend Team-FP dynamics to finite horizon multi-team Markov games for both model-based and model-free cases. We discuss this extension and analyze its convergence numerically in Appendix C.

**Related works.** FP and its variants offer convergence guarantees in important classes of games (Fudenberg and Levine, 2009), yet not in every class of games (Hart and Mas-Colell, 2003). For example, they reach equilibrium in potential games (Monderer and Shapley, 1996a), but not necessarily the most efficient one for the team. Log-linear learning can achieve efficient equilibrium for the team (Marden and Shamma, 2012; Tatarenko, 2017). However, it is not clear whether such dynamics can track efficient equilibrium in dynamic environments (induced by the evolving strategies of opponent teams). Notably, Tatarenko (2018) and Donmez et al. (2024) addressed, resp., efficient learning under non-stationarity induced by the decaying exploration in agents' responses for the repeated play of potential games and non-stationarity induced by evolving stage games in Markov team problems

Figure 1: An illustration of networked interconnections agents from different teams. Nodes in bottom and top layers refer, resp., to team members and teams. Undirected edges represent the impact of actions on the payoff functions. We use different colors and shapes to represent agents from the same teams, and they are connected via dashed edges.

(also known as identical-interest Markov games). These approaches are orthogonal to our analysis to extend our results to the exact TNE convergence in repeated multi-team games or to learning in infinite horizon Markov games.

FP and its variants can reach equilibrium in two-agent zero-sum games (Hofbauer and Sandholm, 2002) yet not necessarily in multi-agent zero-sum games with more than two agents. We can transform any general-sum game to a multi-agent zero-sum game by introducing a non-effective auxiliary agent (with a single action). There have been several attempts to address zero-sum games beyond two-agent cases (Bergman and Fokin, 1998; Cai and Daskalakis, 2011; Cai et al., 2016). For example, Ewerhart and Valkanova (2020) addressed the convergence of continuous and discrete-time FP in zero-sum network games, where each agent plays multiple two-agent games with separate actions, and the overall utilities sum to zero. Notably, Cai et al. (2016) introduced zero-sum polymatrix games where agents have network separable pairwise interactions with applications in security. Recently, FP has been shown to reach Nash equilibrium in zero-sum polymatrix games (Park et al., 2023). Following the same trend, we focus on learning in ZSPTGs extending two-team zero-sum games to multi-team games with pairwise team interactions. However, we highlight that two-team zero-sum or ZSPTGs are not necessarily multi-agent zero-sum polymatrix games (as the agent payoffs do not necessarily sum to zero) and the classical FP dynamics do not necessarily converge TNE or Nash equilibrium in such multi-team games.

Recent studies on two-team zero-sum games and adversarial team games (e.g., see (Celli and Gatti, 2018; Farina et al., 2018; Zhang et al., 2022; Carminati et al., 2022)) have primarily focused on the efficient computation of team-minimax equilibrium. In particular, Celli and Gatti (2018) examines the efficiency of different communication types, highlighting the promising results of _ex ante_ communication, referring to pre-play communication among team members. Consequently, the studies of Farina et al. (2018); Zhang et al. (2022); Carminati et al. (2022) often model teams as a single agent with imperfect recall, incorporating ex ante communication within the team. Notably, Farina et al. (2018) introduced the fictitious Team Play (FTP) algorithm for extensive-form two-team zero-sum games with imperfect information, where team members communicate ex ante. In this approach, Farina et al. (2018) used fictitious play (FP) on a simplified version of the original game, embedded within the game tree. This method essentially applied FTP to the original adversarial team game. To find the best response, they used mixed-integer linear programming. Team-FP differs from such approaches by letting agents _learn_ to team up while following their self-interest based on simple behavioral rules, as in the log-linear learning and FP dynamics.

The rest of the paper is organized as follows. We describe ZSPTGs in SS2. We present the (independent) Team-FP dynamics in SS3. We provide analytical and numerical results, resp., in SS4 and SS5. We conclude the paper with some remarks in SS6. Appendices include the proofs of the technical results, some further numerical experiments and the extension to multi-team Markov games.

_Notation:_ Given a finite set \(A\), we let \((A)\) denote the probability simplex over \(A\). We let \(f()=_{a}[f(a)]\) for any probability distribution \((A)\) and any functional \(f:A\). Furthermore, we define the smoothed best response to any functional \(q:A\) by

\[_{}(q)(a)=}{_{ A} )/)}} a A _{}(q)=*{argmax}_{(A)}\{q()+ ()\},\] (1)

for some temperature parameter \(>0\), where \(():=_{a}[-((a))]\) for all \((A)\) is the entropy regularization.

## 2 Game Formulation

Consider a _multi-team game_, characterized by the tuple \(,(A^{i},u^{i})_{i}\), where \(\) and \(\) denote, resp., the teams' and agents' index sets, and \(A^{i}\) and \(u^{i}:A\) (with \(A:=_{j}A^{j}\)) denote, resp., the agent \(i\)'s finite action set and payoff function. Agents take their actions to maximize their payoff functions.

**Definition 2.1** (Zero-sum Potential Team Game).: Let \(^{m}\) denote the index set of agents in team \(m\) and \(^{m}:=_{i^{m}}A^{i}\) denote the set of joint actions for team \(m\). We say that a multi-team game is _zero-sum potential team game_ (ZSPTG) if for each team \(m\), there exists a potential function \(^{m}:A\) such that

\[u^{i}(^{i},a^{-i},^{-m})-u^{i}(a)=^{m}(^{i},a^{-i },^{-m})-^{m}(a),\] (2)

for all \((^{i},a) A^{i} A\) and \(i^{m}\), where \(a^{-i}\{a^{j}\}_{j^{m}\{i\}}\) are the actions of other team members, \(^{-m}\{^{}\}_{ m}\) are the action profiles of other teams, where \(^{}^{}\) is team \(\)'s action profile. The potential functions sum to zero, i.e., we have

\[_{m}^{m}(a)=0 a A.\] (3)

Furthermore, the actions have network separable interactions across teams such that we can separate the potential functions and correspondingly payoff functions as

\[^{m}_{ m}^{m} u^{i} _{ m}u^{i} i^{m},\] (4)

for some \(^{m}:^{m}^{}\) and \(u^{i}:^{m}^{}\).

The following example generalizes the potential game formulation for distributed optimization (e.g., see ) to two-team zero-sum potential games.

_Example 2.2_.: Consider two teams of agents interacting over a network. We can represent their interactions via a graph \(G=(V,E)\), where the set of vertices \(V\) refers to the agents and the set of (undirected) edges refers to their interactions. Let agent \(i\) from team \(m\) receive a local payoff \(r^{i}:A^{i}_{j:(i,j) E}A^{j}\) depending on the actions of the neighboring agents only. Agent \(i\) adds the neighboring team members' local payoffs whereas subtracts the other neighbors' local payoffs in his/her total payoff. In other words, his/her total payoff is given by

\[u^{i}_{j:(i,j) E}_{\{j^{m}\}}r^{j}-_ {j:(i,j) E}_{\{j^{m}\}}r^{j}.\] (5)

This yields that the team \(m\) has the potential function

\[^{m}_{j^{m}}r^{j}-_{j^{m}}r^ {j},\] (6)

and therefore, these potential functions sum to zero. However, the potential function is not generally the sum of the payoffs in potential games.

_Remark 2.3_ (General-sum ZSPTGs).: In ZSPTGs, the underlying game can be a _general-sum_ game although the team-potentials sum to zero, as described in (3). For example, consider two competing teams whose team members have identical payoffs corresponding to their team potentials. If the teams have different number of members, then the agents' payoffs do not sum to zero or a constant while the team potentials do so.

In the following, we introduce TNE, generalizing team-minimax equilibrium for two-team zero-sum games, e.g., see , to multi-team games. Particularly, at TNE, no team has an incentive to change their team strategy.

**Definition 2.4** (Team-Nash Gap).: Given the strategy profile of teams \(\{^{m}(^{m})\}_{m}\), we define the _team-Nash gap_ for team \(m\) as

\[^{m}()_{(^{m})} \{^{m}(,^{-m})\}-^{m}(),\] (7)

and \(()_{m}^{m}()\), where \(^{-m}\{^{}\}_{ m}\). Correspondingly, we say that the strategy profile of teams \(\{^{m}\}_{m}\) is _\(\)-TNE_ if \(()\).

## 3 Team-FP Dynamics

We first present the Team-FP dynamics combining the log-linear learning and fictitious play for learning in multi-team games played repeatedly, and then extend Team-FP to multi-team MGs in Appendix C.

Let \(a^{i}_{k} A^{i}\) denote the agent \(i\)'s action at the \(k\)th repetition in the repeated play of the underlying ZSPTG. Correspondingly, \(^{m}_{k}=(a^{i}_{k})_{i T_{m}}\) denote the team \(m\)'s action profile. Observing the joint actions of team \(m\), agents \(j^{m}\) can form a belief about the team \(m\)'s joint strategy. Let \(^{m}_{k}(^{m})\) denote the belief they formed. Consider actions as pure strategy where the associated action gets played with probability \(1\). Then, agents \(j^{m}\) can update their beliefs about team \(m\)'s strategy according to

\[^{m}_{k+1}=^{m}_{k}+_{k}(^{m}_{k}-^{m}_{k})  k=0,1,\] (8)

such that the belief \(^{m}_{k+1}\) also corresponds to the (weighted) empirical average of the past action profiles \(\{^{m}_{0},,^{m}_{k}\}\).

Agent \(i^{m}\) either takes the previous action \(a^{i}_{k-1}\) (i.e., \(a^{i}_{k}=a^{i}_{k-1}\)), or takes the action \(a^{i}_{k}_{}(u^{i}(,a^{-i}_{k-1},^{-m}_{k}))\) according to the smoothed best response (as described in (1)) to the previous actions of team members \(a^{-i}_{k-1}:=\{a^{j}_{k-1}\}_{j^{m}\{i\}}\) and the beliefs \(^{-m}_{k}:=\{^{}_{k}\}_{ m}\) formed about other teams. It is instructive to note that the definition of potential function \(^{m}()\), as described in (2), yields that

\[_{}u^{i}(,a^{-i}_{k-1},^{-m}_{k}) _{}^{m}(,a^{-i}_{k-1},^{-m}_{k})  i^{m}.\] (9)

We introduce Team-FP and independent Team-FP dynamics depending on how agents update their actions. In the former, a single agent can get chosen randomly, as in the classical log-linear learning. In the latter, each agent can update his/her action with probability \((0,1)\) independent of others, as in the independent log-linear learning. The latter addresses the coordination burden in the update of actions within teams. Algorithm 1 provides descriptions of these dynamics for the typical agent \(i\) from team \(m\).

_Remark 3.1_ (Scalability).: Agents can have networked interactions such that their payoff functions depend only on the actions of certain agents such as one/two-hop neighbors. For such cases, agents can form beliefs about these neighbors' strategies only, as if these neighbors play according to some stationary strategy. For example, assume that the payoff of agent \(i^{m}\) (outside team \(m\)) depends _only_ on the actions of team-\(m\) members from some neighborhood \(^{i}\), i.e., \(\{j:j^{i}^{m}\}\). Agent \(i\) can form a belief about these agents' strategies according to

\[^{im}_{k+1}=^{im}_{k}+_{k}(^{im}_{k}-^{im}_{k})  k=0,1,\] (10)

where \(^{im}_{k}=\{a^{j}_{k}\}_{j^{i}^{m}}\). Then, the linearity of the update rule yields that the local empirical average \(^{im}_{k}\) corresponds to the marginalization of \(^{m}_{k}\) such that

\[^{im}_{k}(\{a^{j}\}_{j^{i}^{m}})=_{\{a^{j }\}_{j^{m}^{i}}}^{m}_{k}(\{a^{j}\}_{j ^{m}}) k.\] (11)

Therefore, local observations (within neighborhoods) would still be sufficient to follow Algorithm 1. We demonstrate the scalability of Team-FP by a large-scale experiment in Appendix D, Figure 7.

We focus on the homogeneous cases where agents \(i^{m}\) have a common belief about team \(m\)'s strategy. Homogeneous beliefs are possible if the agents have common initial beliefs and step sizes.

We moved the extension of (Independent) Team-FP to multi-team Markov games and its numerical analysis to Appendix C.

## 4 Convergence Results

Team-FP and Independent Team-FP dynamics reduce, resp., to the classical log-linear learning and independent log-linear learning dynamics if there is only one team. These log-linear learning dynamics are known to reach team-optimal solution in potential games. For example, consider an \(n\)-agent potential game \((A^{i},u^{i})_{i[n]}\) with the potential function \(()\). In both dynamics, the action profiles played form irreducible and aperiodic Markov chains. Let \(\) and \(\) denote the unique stationary distributions, resp., for the classical and independent versions. For the former, we have \(=_{}(())\)(Marden and Shamma, 2012, Section 3) and (1) yields that

\[0_{a}\{(a)\}-()|A|.\] (12)

On the other hand, the smaller \(>0\) implies closer stationary distributions in the classical and independent versions. Particularly, we have

\[\|-\|_{1}(,_{}),\] (13)

for some function \(()\) decaying to zero as \( 0^{+}\) for any \(_{}>0\), and \(0<_{}_{}((,a^{-i}))\) for any \(a^{-i}\) and \(i\) is a lower bound on local actions get played in the smoothed best response (Donmez et al., 2024, Lemma 5.6).

Team-FP dynamics have similar convergence guarantees for multi-team games under the following assumption on the step sizes used.

**Assumption 4.1**.: The step size \(_{k}\) satisfies the stochastic approximation conditions: \(_{k} 0\) as \(k\), \(_{k=0}^{}_{k}=\), and \(_{k=0}^{}_{k}^{2}<\). Additionally, we have \(_{k}_{k}/_{k+1}=1\), and \(_{k}-_{k+1}_{k}_{k+1}\).

The last condition in Assumption 4.1 ensures that recent observations have comparable weight in the beliefs formed. The classical choice \(_{k}=1/(k+1)\), leading to the empirical averages of the actions played, satisfies Assumption 4.1.

The following theorem shows the convergence of Algorithm 1 to near TNE almost surely with the approximation levels (similar to (12) and (13)) inherited from the (independent) log-linear learning dynamics.

**Theorem 4.2**.: _Given a ZSPTG characterized by \(,(A^{i},u^{i})_{i}\), let every agent follow either Team-FP or Independent Team-FP, as described in Algorithm 1. If Assumption 4.1 holds, then the team-Nash gap for \(_{k}:=(_{k}^{m})_{m}\) satisfies_

\[_{k}(_{k})|A|& \\ |A|+||^{2}(,)& \] (14)

_almost surely, where \(:=_{(m,l,a)}|^{ml}(a)|\)._

The key challenge in our convergence analysis is the non-stationarity arising from opponent teams adapting their strategies while the team members learn to coordinate against them. We address this by constructing a _reference scenario_ where the team members' beliefs about opponent strategies are _frozen_ over finite-length epochs, allowing them to hypothetically "team up" under these fixed beliefs. By comparing the dynamics in the actual scenario and the reference scenario, and exploiting the averaging nature of belief formation, we can bound the approximation error between the two. The main proof concept, along with the Team-FP algorithm for a two-team scenario, is illustrated in Figure 2. This approach is similar to the one used in (Donmez et al., 2024) to handle non-stationarity in Markov team problems. However, unlike (Donmez et al., 2024), we cannot directly show the error bound decay due to the lack of a contraction property in our dynamics.

To overcome this limitation, we view Team-FP as smoothed fictitious play dynamics in zero-sum polymatrix games with an additive bounded error term. The additive error captures the fact that team members may not perfectly achieve team coordination. We then relax the problem by considering any approximation error within the formulated bounds, rather than the actual error. To address this relaxation, we leverage stochastic differential inclusion approximation methods (Benaim et al., 2005; Perkins and Leslie, 2013). Finally, by constructing a suitable Lyapunov function addressing arbitrary bounded errors in continuous-time smoothed best response dynamics, we establish the convergence of the discrete-time Team-FP updates.

The following corollary to the main result shows the rationality of the (independent) Team-FP dynamics.

**Corollary 4.3**.: _Given a ZSPTG characterized by \(,(A^{i},u^{i})_{i}\), let agents from team \(m\) follow either Team-FP or Independent Team-FP while other teams play according to some stationary strategy \(^{-m}\). If Assumption 4.1 holds, then empirical average of the action profiles played by team \(m\) satisfy_

\[_{k}^{m}(_{k}^{m},^{-m}) |^{m}|&\\ |^{m}|+(,)&\] (15)

_almost surely._

Theorem 4.2 can be generalized to the case where the rewards are random with bounded support, rather straightforwardly. Therefore, the proof of Corollary 4.3 follows from Theorem 4.2 if we view the underlying game as there is a single team receiving random rewards with bounded support.

## 5 Illustrative Examples

In this section, we present various simulation results demonstrating the coordination speed of Team-FP and compare it to pure FP, no-regret algorithms, and a stationary opponent. We also observe the effect of parameters on the convergence speed. In addition, we examine the long-run behavior of team-FP in games beyond ZSPTG games, where we intuitively expect it to converge. All simulations are averaged over 10 independent trials to reduce the randomness. In all figures, the mean is plotted with a thick colorful line, while one standard deviation below and above the mean is shown with a shaded area of the same color. Also, for all simulations, temperature parameter \(\) is chosen to be 0.1 unless another option is mentioned. We conduct simulations for ZSPTG with two different setups: one with three teams, each consisting of three agents, and another with two teams, each consisting of four agents. Unless explicitly stated otherwise, the default setting consists of two teams, with four agents in each team. The step size is chosen to be \(_{k}=1/(k+1)\) for all simulations.

All the simulations are executed on a computer equipped with an Intel Xeon W7-3455 CPU and 128 GB RAM. Run-time for 10 independent trials over \(10^{6}\) iterations vary between 1-5 hours depending on the experiment.

Achieving Implicit Coordination in Team-FPIn this section, we compare the performance of Team-FP to the explicit coordination of team members. We also compare Team-FP to algorithms such as Multiplicative Weights Update (MWU) and Smoothed FP (SFP), and show that these algorithms fail to achieve team coordination. We also show that Team-FP achieves near-optimum policy against a stationary opponent as stated in Corollary 4.3.

Figure 2: An illustration of Team-FP dynamics for two-team games on the left-hand side. Team actions change according to a transition kernel depending on the beliefs formed about the other teams. Dashed lines represent the time shift. On the right-hand side, we depict the key proof idea that we approximate the evolution of the team actions with a reference scenario where beliefs are stationary such that team actions form a homogeneous Markov chain whose unique stationary distribution corresponds to the best team response.

_Impact of Team Size in Team Coordination:_ In Team-FP self-interested team members act together without explicit communication in a coordinated way to reach TNE. We can measure how this independent cooperation compares to the explicit coordination of team members. For that, we propose an example game with two teams and four agents in each team. For the first scenario, four agents act separately and use Team-FP. In the second scenario, the agents form groups of two and act in a coordinated way as if they are a single agent, using Team-FP. In the final scenario, all agents in a team behave as if they were a single agent, equivalent to the standard fictitious play (FP) dynamics in a two-person zero-sum game. This case mimics the ex-ante communication scheme from . The scenarios are described by group sizes. Group sizes are one, two, and four respectively for these scenarios. The simulation results are presented in Figure 2(a). As expected, the explicit coordination of all members in a team converges the fastest, followed by the explicit coordination of groups of two, and finally, Team-FP converges slowly as the agents do not coordinate explicitly.

_Team-FP Compared to MWU and SFP:_ The strong side of Team-FP is, even though the agents do not communicate, the average of joint actions of teams can reach TNE, unlike other algorithms. We compare the equilibrium behavior of team-FP with a well-known no-regret learning algorithm Multiplicative Weights Update (MWU), in which the average strategies converge to NE in zero-sum polymatrix games . We also compare Team-FP with the usual SFP, where each agent holds beliefs about other agents and uses the smoothed best response against them. In Figure 2(b), we see that both Team-FP and Independent Team-FP dynamics converge to near TNE, while the other algorithms fail to do so.

_Rationality Against Stationary Opponent:_ In this part, we use 3-team setting where each team has 3 agents. We let a team using Team-FP compete against two other stationary teams and compared it with the performance of the same team in the same game when the opponents are also using Team-FP competitively. In Figure 2(c), we observe that both algorithms converge, while the convergence is much faster against stationary opponents.

Team-FP in ApplicationIn this part, we provide an example to demonstrate that Team-FP has applications in various contexts.

_Security Game Example:_ We model an airport security scenario as a two-team game between defender and attacker teams. In our example, a security chief on the defender team faces three independent intruders on the attacker team, as shown in Figure 3(a). The chief can defend a gate at a cost, while intruders decide whether to attack a gate or remain idle. Intruders gain or lose payoffs based on whether they attack undefended or defended gates, and the chief's payoffs mirror these outcomes. We conducted multiple trials and presented the evolution of the average Team-Nash Gap with standard deviations on the right side of Figure 3(b). From a higher level, this example shows that team-minimax equilibrium can predict the outcome of games with different uncoordinated attackers. It also justifies the algorithms developed to compute team-minimax equilibrium efficiently.

Figure 3: All the above figures show the variation of \(\) over time. (a) Comparison of different levels of explicit coordination for Team-FP: independent agents (group size 1), pairs of cooperating agents (group size 2), and fully coordinated teams (group size 4). (b) Performance of Team-FP and Independent Team-FP compared to MWU and SFP algorithms in a 2-team ZSPTG. (c) Convergence of Team-FP against stationary and competitive opponents in a 3-team ZSPTG.

Effect of parameters in Team-FPIn this part, we examine how Team-FP performs for different \(\) and \(\) values. Given an example ZSPTG of three teams where each team has three agents in Figure 5(a), we examine the evolution of TNG in the Team-FP dynamics for different values of \(\{0.1,0.15,0.2\}\). We also compare the evolution of NG in the Team-FP and Independent Team-FP dynamics for \(=0.1\), \(=0.2\), \(=0.5\). All simulation results (see Figure 5) show convergence, and we observe lower final values of \(()\) for smaller \(\) as we predicted (see. Figure 4(b)). The Independent Team-FP requires more iterations when \(=0.2\) for its convergence, while it is much faster when \(=0.5\) (see. Figure 4(c)). This is expected as updates are much more frequent as \(\) increases. However, increasing \(\) too much may harm the coordination behavior of the team.

Beyond ZSPTGIn this part, we try several other games for Team-FP without proof of convergence. We expect Team-FP to converge in 2xN and potential games other than zero-sum games as FP converges in these settings. For the first case, we try a game where one team has a single agent with only two actions with random rewards, while the other team has three agents with an underlying potential function. In this case, Team-FP converges (see. Figure 5(b)). In another setting, we try two teams of four agents. However, the potential functions for both teams are identical rather than summing to zero, resulting in a potential function that encompasses the individual potential functions. The team-FP algorithm converges to TNE in this case as well (see. Figure 5(c)). However, the equilibrium is not unique in this case. Finally, we create an extension of the team-FP for Markov games (see. Appendix C) in an RL framework and try simulations on this setting. In this case, there

Figure 4: (a) The illustration of an airport security game: a security chief guarding the six gates of an airport against three different intruders making decisions autonomously. (b) The evolution of Team Nash Gap in airport security game, showing that Team-FP dynamics reach near team-minimax equilibrium.

Figure 5: The 3-team experiments are tested on the randomly generated network structure (a). The other figures (b), and (c), shows the variation of \(\) over iterations. (a) The simulation network for a multi-team ZSPTG, in which there are 3-teams with 3 agents in each team. (b) The impact of varying temperature parameter \(\) (0.1, 0.15, 0.2) in Algorithm 1 on the closeness to TNE. (c) The effect of different \(\) values (0.2, 0.5) in (Independent) Algorithm 1 on the convergence speed with \(\) fixed at 0.1

are two teams each having two agents, competing against each other in a finite horizon Markov game with a horizon length of ten. The number of states is two, and the state transition matrices are generated randomly. We observe that team-Nash Gap for MG's defined in (65), converges to near-zero.

## 6 Conclusion

In this paper, we introduced Team-FP, a novel fictitious play variant designed for multi-team games. We showed that Team-FP provably achieves near-TNE in ZSPTGs, with a quantifiable error bound. Our convergence analysis addressed the non-stationarity challenge arising from evolving opponent team strategies, by leveraging the optimal coupling lemma and stochastic differential inclusion approximation methods. We also extended Team-FP to multi-team Markov games, encompassing both model-based and model-free scenarios, with applications in multi-team reinforcement learning. Furthermore, we conducted detailed numerical analysis of the Team-FP dynamics, focusing on the trade-off in learning to team up and competition in comparison to the classical FP and no-regret learning dynamics. We further examined the convergence of Team-FP dynamics to TNE in multi-team games beyond ZSPTGs. These results strengthen the theoretical foundations for applying TNE to predict decentralized team behavior and provide a framework for team learning in multi-team settings.

Limitations and Broader ImpactsOur work quantified the almost sure convergence of the Team-FP dynamics asymptotically yet did not provide guarantees for the convergence rate. We conducted detailed numerical examples and presented the evolution of the gap in TNE to exemplify this rate qualitatively. The challenge for the non-asymptotic analysis is inherited from the _discrete-time_ FP dynamics for which there are only rough rate analysis (Karlin, 1959) and negative examples for some edge cases (Brandt et al., 2010; Daskalakis and Pan, 2014).

Our work introduces no new ethical concerns in multi-team systems and shares the assumption of stationary opponents with learning dynamics like Q-learning and fictitious play. This assumption does not disadvantage our approach. We argue that treating uncoordinated attackers as a single decision-maker is necessary, as they can learn to bypass security measures. Our paper provides a theoretical basis for this, ensuring more reliable AI-based solutions.

Future Research DirectionsThis work paves the way to further explore the behavior of decentralized teams in multi-team interactions when the team members follow different types of dynamics for teaming up within teams (other than log-linear learning) and adapting to other teams' play (other than FP). Numerical examples we conducted for multi-team games beyond ZSPTGs are also promising to show the provable convergence of Team-FP dynamics in multi-team (Markov) games reducing to games with the _fictitious play property_ (e.g., see (Monderer and Shapley, 1996b)) if the teams coordinate in acting as a single player.

Figure 6: All the above figures describes the variation of \(\) over iterations for Algorithms that are related to but outside the scope of ZSPTG. (a) The model-free and model-based Markov games of Algorithm 2, and 3, for a game of 2-team each with 2 agents, with 2 states and 10 horizon length. (b) The behavior of Team-FP dynamics in a 2xN general sum game, where a team competes against a single agent with random rewards. (c) The behavior of Team-FP dynamics in a potential game over the underlying potential functions.