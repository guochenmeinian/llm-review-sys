# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

inference on Transformer models practical, due to heavy matrix multiplications in linear layers and complex non-linear layers. To amortize the overhead of HE in linear layers, many works [17; 14; 26] adopt _window encoding_ to simulate the inner product. However, such an encoding approach brings about a _sparse_ format of HE ciphertexts, leading to redundant communication and computation. The efficiency bottleneck of non-linear layers is to securely compute GELU and exponential (included in Softmax). Prior works [5; 26] use piecewise polynomials to approximate the two non-linear functions. However, high-degree polynomials and large fixed-point precision are used to maintain the accuracy, which causes large communication costs and rounds.

This work proposes a new secure two-party inference framework Nimbus for Transformer models to address the above efficiency bottlenecks. Specifically, our contributions are summarized as follows:

* We propose a Client-Side Outer Product (COP) protocol to facilitate linear layers. Our COP protocol incorporates two key innovations. First, the static nature of model weights allows the server to send encrypted weights to the client during the setup stage, thereby eliminating input communication during the online stage. Second, removing input communication enables us to design a novel row-wise encoding scheme that achieves homomorphic matrix multiplication via the outer product. Such encoding further enhances the efficiency of homomorphic matrix multiplication and yields compact output ciphertexts for communication.
* For non-linear layers, we present a new observation that their input distribution exhibits regular patterns. Unlike prior approximations that assumed a uniform input distribution, our approach reduces the approximation budget allocated to seldom-occurring input values. This enables us to use lower-degree polynomials and fewer pieces to approximate non-linear functions. Additionally, low-degree polynomials demonstrate lower sensitivity to fixed-point errors, allowing us to adopt a smaller ring for greater efficiency. We also propose a new protocol that enables _free_ conversion between the small and large rings. Consequently, our approach achieves improved performance for non-linear layers while incurring only an average accuracy loss of 0.08%.
* We evaluate the performance of Nimbus using the popular Transformer model BERTbase under both LAN and WAN settings. Compared with the SOTA work BumbleBee , we improve the performance of securely computing matrix multiplication (resp., GELU and Softmax) by \(2.9 12.5\) (resp., \(2.9 4.0\)). Combining all the optimizations, we improve the end-to-end performance of secure two-party inference by \(2.7 5.9\) and reduce the communication cost by \(60\%\). The code is available at: https://github.com/secretflow/spu.

## 2 Background

We present the necessary background, including the threat model, cryptographic building blocks, and secure Transformer inference.

### Threat Model

Our protocol works in the two-party setting where the client \(C\) holds an input and the server \(S\) holds a model. Our protocol is secure in the presence of a semi-honest adversary who could passively corrupt either the client or the server, where the adversary follows the protocol specification but may try to learn more information than allowed. Semi-honest adversary is a common assumption for privacy-preserving machine learning and has been used in most two-party protocols [18; 34; 17; 14]. As in all prior two-party inference protocols, the client is only allowed to learn the model's architecture and inference result while the server gains no information about the client's input.

### Notation

We use upper-case bold letters to represent matrices like \(\) for model weights and \(\) for activations. For a matrix \(\), we use \(_{i}\) to denote the \(i\)-th row of \(\) and \(_{i,j}\) to denote the entry in the \(i\)-th row and \(j\)-th column of \(\). For an integer \(n\), we write \([n]=\{0,1,,n-1\}\). For an additive secret sharing \(\) (defined in Section 2.3), we use \(_{c}\) (resp., \(_{s}\)) to denote a share held by a client (resp., a server). We denote by \(\) the homomorphic encryption (HE) ciphertexts on matrix/vector \(\) where it may consist of multiple ciphertexts. We use \(_{2^{}}=[0,2^{})\) to denote a ring with all entries modulo \(2^{}\). For a power-of-two integer \(N\), we use \(_{N,2^{}}=_{2^{}}[X]/(X^{N}+1)\) to denote a set of polynomials over a ring \(_{2^{}}\). Besides, we use lower-case letters with a "hat" symbol such as \(\) to represent polynomials of degree \(N-1\) in \(_{N,2^{}}\), where \([j]\) denotes the \(j\)-th coefficient of polynomial \(\). Note that a polynomial \(\) can encode at most \(N\) elements over \(_{2^{}}\).

### Building Blocks

Our framework is built upon multi-party computation (MPC) techniques, including additive secret sharings and homomorphic encryption (HE). The building block of oblivious transfer (OT) and the sub-protocols used in non-linear layers can be found in Appendix B.

**Additive Secret Sharings.** In the two-party setting, an additive secret sharing over a ring \(_{2^{}}\) is defined as: for a value \(x_{2^{}}\), two random shares \( x_{c}_{2^{}}\) and \( x_{s}_{2^{}}\) are sampled uniformly such that \(x= x_{c}+ x_{s} 2^{}\), where \( x_{c}\) is held by a client and \( x_{s}\) is held by a server.

**Homomorphic Encryption.** We adopt the lattice-based additive HE scheme  (building upon ). HE allows one party to perform computations on the encrypted data of the other party without the need for the decryption key. The HE scheme encodes a plaintext vector \((_{2^{}})^{N}\) into a plaintext polynomial \(_{N,2^{}}\), and then \(\) is encrypted to a ciphertext \(=(,)_{N,q}^{2}\) where \(q\) is a ciphertext modulus. Given a ciphertext \(\) and a circuit \(f\) including only linear operations, one can homomorphically compute another ciphertext \( f()\). We refer the reader to Appendix B.1 for details of the HE scheme and its homomorphic operations.

**Conversion between Floating-point Numbers and Ring Elements.** As 2PC and HE usually operate over rings, the floating-point numbers used in Transformers need to be converted into fixed-point numbers in a ring. Given a scale \(s\) (i.e., the length of the fractional part) and a ring \(_{2^{}}\), a floating-point number \(x\) is converted to the approximated fixed-point number by computing \(:= x 2^{s} 2^{}\), and \(\) can be converted back to the approximated \(x\) by setting \(/2^{s}\).

### Secure Two-Party Transformer Inference

The details of the Transformer architecture are described in Appendix A. To securely evaluate the model, the input and output of all layers are in the form of additive secret sharing, enabling the arbitrary linkage of different layers despite specific protocols. This work optimizes the protocol of the linear layers, including \(_{}\), \(_{}\), \(_{_{1}}\), and \(_{_{2}}\). We also optimize the protocols for non-linear layers \(\) and \(\). The activation multiplication in the attention and layer normalization are relatively fast following SOTA studies [28; 26]. We do not give special optimizations and leave them as future works.

## 3 Secure Computation of Linear Layers

We first analyze the efficiency problems of the prior solution in Section 3.1. Then, Section 3.2 presents our client-side outer product (COP) protocol with row-wise encoding. In Section 3.3, we optimize the memory occupation of our COP protocol.

### Prior Solution: Server-side Inner Product Protocol

The starting point of this work is the protocol so-called _server-side inner product (SIP)_[17; 14; 26], as shown in Figure 2(a). The inputs of the linear layer are additive secret sharings \(_{C},_{S}_{2^{ }}^{k m}\) held by the client and server. The server also holds the weights \(_{2^{}}^{m n}\).

\[&=_{}(): [imn+(m-1)-j]=_{i,j},i[k],j[m]\\ &=_{}():[jm+i]= _{i,j},i[m],j[n]\] (1)

The values of two activation shares and server weights are encoded into polynomials over \(_{N,2^{}}\) using encoding functions \(_{}:_{2^{}}^{k m}_{N,2^ {}}\) and \(_{}:_{2^{}}^{m n}_{N,2^ {}}\), as shown in Equation (1). The coefficients of the polynomials \(\) and \(\) that are not defined are set to 0.

Some of coefficients of polynomial \(=_{N,2^{}}\) gives the result of matrix multiplication \(=_{2^{}}^{k n}\), as illustrated in Figure 1. If \(kmn>N\), the encoding function would use coefficients with degrees exceeding \(N\). The input matrices \(\) and \(\) need to be partitioned into smaller windows with respective dimensions \(k_{w} m_{w}\) and \(m_{w} n_{w}\), which results in multiple windows of the output matrix \(\) with dimension \(k_{w} n_{w}\). Therefore, we refer to this encoding approach as _window encoding_.

Then, the client encrypts her plaintext polynomials as HE ciphertexts \(\!()_{c}\!\), and sends them to the server. After receiving \(\!()_{c}\!\), the server computes the HE ciphertexts \(\!()\!\) by homomorphically adding \(\!()_{c}\!+_{s}\). Next, the server homomorphically computes \(-\) to obtain HE ciphertexts \(\!-\), where \(\) is randomly generated to mask \(=\) and keeps as the server's output shares \(_{s}\). Finally, the server sends \(\!-\) to the client who decrypts the HE ciphertexts into \(-\) that is used as the client's shares \(_{c}\). Note that number theoretic transform (NTT)  is employed to weight plaintext polynomial and activation ciphertext polynomial so that their multiplication complexity is reduced from \(O(N^{2})\) to \(O(N N)\).

**Analysis of Communication and Computation Costs.** To simulate the inner product, the window encoding produces a sparse output (e.g. the even-degree terms of \(\) in Figure 1). The sparse polynomials are treated as dense after encryption, leading to inefficient communication and computation marked by the dashed boundary in Figure 2(a). First, the computation includes unnecessary zero terms. Second, Iron shows at least \(}{}\) ciphertexts are transmitted . Then, BumbleBee  proposes a packing approach that trades computation for less communication, but the overall latency is similar.

### Client-side Outer Product Protocol

To solve the efficiency problems as described above, we propose an alternative _client-side outer product (COP)_ protocol. The COP protocol includes two key insights. First, the static nature of model weights allows the server to send encrypted weights to the client at the setup stage, which can eliminate input communication at the online stage. Second, this elimination of input communication enables us to design a new row-wise encoding that realizes homomorphic matrix multiplication through the outer product. Our encoding further results in compact output ciphertext for communication and enhances the efficiency of HE matrix multiplication. The formal protocol is described in Appendix C.1.

**COP Protocol.** In Figure 2(b), we describe the COP protocol for secure matrix multiplication, where the dashed boundary shows the optimizations of computation and communication over prior works. In the setup stage, the server encodes the model weights \(\) in a row-wise fashion and sends the HE ciphertexts \(\!\!\) on these weights to the client. The client stores the HE ciphertexts \(\!\!\) in the disk, which enables these ciphertexts to be reused for multiple queries by loading them into memory. In the execution stage, for additive secret sharings \(_{c},_{s}\) held by the client and server respectively, the client homomorphically computes \(_{c}\!\!\) to obtain HE ciphertexts \(\!_{c}\!\), and the server locally computes \(_{s}\) in plaintext. Then, the client samples a random matrix \(\) (used as its

Figure 1: An example of the window encoding of the matrix multiplication using \(N=16\) and \(=5\).

Figure 2: Two rows represent the client and server operations, respectively. The inefficient parts that are accelerated are marked by dashed boundaries. The input communication is shifted as a one-time setup, and the output ciphertexts are compact. The expensive NTT/INTT operations at the online stage are also reduced.

output shares \(_{c}=\) where \(=\), and homomorphically computes \(_{c}-\) to obtain HE ciphertexts \(_{c}-\) which is sent to the server. Finally, the server decrypts these ciphertexts to obtain \(_{c}-\), and then sets its output shares as \(_{s}=_{c}-+_{s}\). As a result, two parties hold additive secret sharings \((_{c},_{s})\). Below, we explain the key process for computing \(_{c}\) using the row-wise encoding approach.

**Row-wise Encoding.** Assigning the client to perform the plaintext-ciphertext multiplication has a direct benefit in terms of saving the communication of inputs. More importantly, such saving enables us to design a new encoding approach for the weights and activations. First, we use the following function to encode each row of the weight matrix, i.e., \(_{i}\) for \(i[m]\),

\[_{i}=_{}(_{i})_{i}[j]= _{i,j},j[n].\] (2)

The output dimensions \(n\) of Transformers are generally less than \(N\), so the last \(N-n\) coefficients are set as zero, indicated by the blank squares in Figure 3. Row-wise encoding corresponds to an extreme case of prior window encoding (i.e., setting \(n_{w}=n\)), which may cause a large number of ciphertexts on inputs. This problem is solved by eliminating the input communication in our COP protocol. Second, for the activations, the client no longer encodes his share into polynomials but directly performs multiplication between the plaintext activation shares and ciphertext on weights.

**Efficient Computation and Compact Output Ciphertexts.** Our encoding realizes secure matrix multiplication through outer product, which achieves more efficient homomorphic computation and compact HE ciphertexts on the output. We illustrate the computation of the first row of the output matrix \(_{1}\) in Figure 3. Following the spirit of the outer product, each scalar-polynomial multiplication produces a partial sum of \(_{1}\) and their accumulation produces the final output. The scalar-polynomial multiplication has same complexity as the prior poly-poly multiplication in the NTT space. But we reduce the online NTT/NTT operation, and thus reduce the computation cost. For the output communication, if \(n\) is smaller than the polynomial degree \(N\), the output ciphertext still leaves blank when communicating. The row-wise encoding makes valid coefficients and zeros separate in the output ciphertext instead of in an interleaved fashion. This enables packing the output ciphertexts through a free operation, right shift. Right shift coefficients for \(s\) steps in a ciphertext can be done by multiplying the ciphertext with a plaintext polynomial with only a \(s\)-order term. The right figure of Figure 3 shows the right shift packing. The second ciphertext is shifted to the right four slots and added with the first ciphertext. Then, all slots of the output are utilized for output communication.

**Complexity Analysis.** Through analysis in Appendix D, Table 1 compares the computation complexity and numbers of communicated ciphertexts. _Communication:_ Our COP protocol eliminates the input communication, and output communication is the minimal case of prior \(n_{w}}\) when \(k_{w}n_{w}=N\) since values are densely arranged in the output ciphertext. _Computation:_ The SIP protocol requires polynomial-polynomial multiplication. It applies NTT with \(O(N N)\) complexity to the plaintext polynomial of weights, and ciphertext polynomials of inputs and outputs. Then, the complexity of all poly-poly multiplications in the NTT space is \(O(kmn)\). In our protocol, the NTT is only applied when the server decrypts the output ciphertext. The client's scalar-polynomial multiplication saves time spent on the NTT by directly performing the multiplication with a complexity \(O(kmnN)\), which is comparable to the previous multiplication in the NTT space, especially when \(n\) is close to \(N\).

### Memory Impact of the COP Protocol

In our COP protocol, the client stores the encrypted model weights in the disk so that the ciphertexts can be reused. At the online stage, the encrypted weights are loaded into memory for secure

Figure 3: Illustration of our matrix multiplication. **Left:** Functionality of the matrix multiplication using row-wise encoding. **Middle:** Computing the first row of the output through the scalar-poly product. **Right:** Packing two ciphertexts using a right shift for less number of output ciphertext.

matrix multiplication. In this way, the client executes an efficient outer product rather than the original encryption and decryption. The feasibility of such workload reallocation is due to the difference between the "client" in MPC and the traditional client. Due to the symmetric-computation characteristic of MPC as well as the expensive NTT cost brought about by homomorphic encryption and decryption, existing secure-inference frameworks, e.g., [17; 14; 5; 26; 29; 42], require the client to be equipped with similar resources as the server, including a powerful CPU (e.g., 64 vCPUs) and a large memory (e.g., 128 GB) [42; 26; 29]. For clients in MPC, disk usage does not pose a significant issue as storage resources are inexpensive. The CPU usage is also not an issue as the analysis and experiments in Appendix F.2 indicate that the client's computational overhead remains similar as the prior SIP protocol. However, we notice that keeping encrypted weights instead of plaintext weights may introduce additional overhead to memory usage, which we address in the next paragraph.

**Asynchronous Weight Loading.** Our COP protocol allows the weights to be encrypted at the setup stage and stored in the client disk. Different from prior SIP protocol that keeps the model weight shares in memory, loading all encrypted model weights in memory may become a burden since the size of ciphertext is at least four times larger than the secret shares. To reduce the additional usage of memory, we let the client only keep the encrypted weights w.r.t. the current layer in memory (e.g., either 180 MB or 720 MB for Transformer model \(_{}\)). The encrypted weight of the subsequent layer is loaded asynchronously with the communication of output ciphertexts of the linear layer and the secure computation of the following non-linear layer, which involves large-size communication and multiple rounds of interaction. Moreover, the network bandwidth is hundreds of times smaller than the disk bandwidth. For example, as shown in Appendix F.2, loading the encrypted weights of one layer in \(_{}\) from the disk to memory can be accomplished in tens of milliseconds, while the communication between the client and server requires several seconds. Therefore, the loading time of the encrypted weights can overlap with the communication process.

## 4 Secure Computation of Non-Linear Functions

### Prior Solution: Piecewise Polynomial Approximation of Non-Linear Functions

For Transformers, the main efficiency bottleneck in non-linear layers is to securely compute functions exponential and \(\)[5; 28; 26]. These works approximate the non-linear functions through piecewise polynomial approximation, which can be securely computed by executing two-party addition, multiplication, and comparison operations. To maintain the accuracy, these works adopt four-piece polynomials with degree 6 for \(\) and two-piece Taylor series with Taylor expansion degree 6 for exponential. The approximation of high-degree polynomials inherently imposes a large overhead for securely computing the powers of values. Additionally, such an approximation requires computations to be conducted over a large ring \(_{64}\) and with a large scale \(s=18\)[14; 5; 26]. This is brought about by the fact that computing the powers of values with high degrees leads to the accumulation of fixed-point errors and the potential overflow problem.

### Simpler Piecewise Polynomial and Smaller Rings by Distribution-aware Approximation

We aim to use simpler piecewise polynomials to fit non-linear functions and reduce the size of rings without sacrificing accuracy. Inspired by the finding that activation distribution exhibits a regular pattern across training and test data [24; 40], our insight for enabling simpler polynomials is to assign the approximation budget according to the input distribution instead of treating all input values with equal importance. Figure 4 illustrates patterns of the input distribution using the \(_{}\)'s

    & SIP & COP \\  Communicated Ciphertexts Count & \(m_{w}}+n_{w}}\) & \(k/ N/n\) \\  Server HE Computation Complexity & \(O(n_{w}}N N+kmn)\) & \(O((k/ N/n)N N)\) \\  Client HE Computation Complexity & \(O((m_{w}}+n_{w}})N N)\) & \(O(kmN)\) \\   

Table 1: Comparison of the computation and communication for multiplication of two matrices with dimension \(k m\) and \(m n\). \(k_{w},m_{w},n_{w}\) are the window size corresponding to matrix dimensions.

nonlinear functions at the \(4_{th}\) encoder. As an example, consider the input distribution of the \(\) function. The probability peak centers around \(-3\) and values greater than zero occur with less than 10% probability. A wise strategy should leave more budget to the high-probability ranges. Compared with prior research directly minimizing the approximation error of the original function, assuming a uniform input distribution, our strategy is supposed to generate more effective approximations. Additionally, we want to note that the fitted polynomial does not leak the input distribution of the data as the client remains oblivious to the fitted polynomial during secure inference.

Distribution-aware Splitting and Fitting.Prior works typically split the input range and fit each interval based on the non-linearity of the curve. We further include the consideration of the input distribution for these two processes. For intervals with low non-linearity or input probability, we split them out and assign constant or linear polynomials to fit. The other intervals with both high non-linearity and input probability are fitted by quadratic or cubic polynomials. When fitting each piece of the non-linear function \(f(x)\) by the polynomial \(f^{}(x)\), we minimize the expected loss that integrates the inputs' probability density \(p(x)\)

\[_{f^{}(x)}_{l}^{h}p(x)[f(x)-f^{}(x)]^{2}dx,\] (3)

where \(l\) and \(h\) are the lower bound and upper bound. \(p(x)\) is the probability density function obtained by summarizing a batch of training data. Unlike prior works using fixed breakpoints \(l\) and \(h\), we initialize each breakpoint with a starting value and search around it to better fit non-linear functions at different depths. This is because although the activation distributions are broadly similar, they may shift slightly across varying model depths and the breakpoints should be adjusted accordingly. We refer to the Appendix C.2 for the splitting and fitting algorithm. The detailed protocols for securely evaluating nonlinear functions are provided in Appendix C.1. Next, we elaborate on the specific design for fitting \(\) and \(\) functions.

Exponential.The exponential is used in the \(\). Given an input vector \(\), the \(i\)-th element of \(\) is computed as \(-\{\})}{_{j}(x_{j}-\{\})}\). Input values subtracted from maximal values result in maximal zero. The exponential curve exhibits two distinct patterns: a long smooth tail on the left and a sharp increase on the right. Prior works adopt a two-piece approximation by breakpoint -13. Instead, we initial breakpoints around -4 for varying depths. As the right interval spans a smaller range, it adopts a cubic polynomial \(P^{3}(x)\) instead of the Taylor series with expansion degree six . Values less than -4 are less occurred and the curve is smooth, and a linear function is enough to fit.

\[(x)0&x<T_{}\\ P^{3}(x)&T_{} x 0\] (4)

\(\).The \(\) curve nearing the zero exhibits pronounced non-linearity. Prior works  assign two polynomials for intervals \([-5,-1.97]\) and \([-1.97,3]\) with degree three and six. We merge these two intervals by one and shrink the range to \([T_{1},T_{2}]=[-2.1,0.2]\). This is because the values beyond this interval present either less non-linearity or fewer occurrence probabilities, and using constant or linear polynomials is enough. As the middle interval becomes narrow, we find a square polynomial \(P^{2}(x)\) is enough. The specific breakpoints \(T_{1}\) and \(T_{2}\) change for different depths.

\[(x)0&x T_{1}\\ P^{2}(x)&T_{1}<x T_{2}\\ x&x>T_{2}\] (5)

Figure 4: The input distribution of non-linear functions. The y-axis indicates the occurrence counts.

### Free Ring Conversion by Fusion with Truncation

Our low-degree polynomials reduce the errors of operation on fixed-point numbers and potential overflow problems. This enables smaller ring \(_{32}\) and precision \(s=12\) for computing Softmax and GELU functions, instead of the original standard ring \(_{64}\) and precision of \(s=18\). However, since other operations still require the larger ring to preserve the accuracy, another challenge is to convert secret shares between differently sized rings. The process of downcasting from a larger to a smaller ring can be performed locally, incurring negligible cost [31; 39]. Upcasting from a smaller to a larger ring necessitates addressing the wrap-around of shares, requiring communication among parties. Interestingly, we notice the situations demanding to upcast are always after a truncation operation that inherently computes the wrapping, which can be repurposed for the upcast to avoid additional costs. We propose a novel protocol that fuses the upcast with the truncation. We defer the protocol and the correctness proof to the Appendix E.

## 5 Performance Evaluation

**Experimental Setup.** We follow similar configurations used in prior works . Except optimized non-linear functions using ring \(_{2^{32}}\) and precision \(s=12\), other operations follow standard \(_{2^{64}}\) and \(s=18\) for the secret sharing. We use \(N=8192\) for the HE encryption. The performances are evaluated on two nodes with 64 vCPUs and 128 GB memory. We use Linux Traffic Control (tc) to simulate LAN and WAN network settings, where the bandwidth and the ping latency are (3Gbps, 1ms) and (400Mbps, 10ms), respectively.

**Baselines.** The baselines include Iron and BumbleBee. Our implementation follows the open-sourced code of BumbleBee on SecretFlow . As Iron is not open-sourced, we implement Iron following their protocol using the SecretFlow library for a fair comparison. For the linear layer, Iron uses window encoding described in Section 3.1, and BumbleBee further compresses the output ciphertext. For non-linear functions, Iron evaluates them via integrating underlying protocols. Later works  use piecewise polynomial approximation. BumbleBee further integrates cryptographic optimizations to make a stronger baseline. In the Appendix F.4, we also compare our work with those that use rough approximations to trade off accuracy for efficiency [22; 29].

**Model and Datasets.** Our method is evaluated on widely used Transformer model BERTbase from HuggingFace . When evaluating the performance, we use 128 as a mild average number of the input sequence length. To evaluate the accuracy of our non-linear approximation, we test it on eight datasets from widely used GLUE benchmark . To obtain the input distribution of non-linear functions, we randomly sample sentences from the training dataset until the total token count reaches 512. This number is chosen because further increasing the number of sampled tokens yields no significant changes in the input distribution.

### Accuracy Comparison

Table 2 reports the accuracy of floating-point plaintext, BumbleBee, and our approximation across 8 tasks in the GLUE benchmark. The precise approximation of BumbleBee causes small errors due to the truncation error of the fixed-point value computation. Without fine-tuning, Nimbus decreases accuracy in a small range and the average loss is around 0.6%. Such loss can be easily reduced to 0.08% through a lightweight fine-tuning Nimbus\({}^{}\). This demonstrates the effectiveness of our

    & CoLA & SST-2 & MRPC & STS-B & QQP & MNLI & QNLI & RTE & Avg. \\   & Matthews corr. & Acc. & F1 & Pearson & Acc. & Acc. & Acc. & Acc. & \\  FP baseline & 58.63 & 92.88 & 90.12 & 88.24 & 91.22 & 84.74 & 91.28 & 67.87 & 83.12 \\ Bumblebee & 58.40 & 92.88 & 90.12 & 88.28 & 91.21 & 84.74 & 91.39 & 67.87 & 83.11 \\ Nimbus & 58.28 & 92.66 & 89.82 & 87.93 & 90.64 & 84.09 & 90.05 & 66.79 & 82.53 \\ Nimbus\({}^{}\) & 58.40 & 92.78 & 90.42 & 88.12 & 90.98 & 84.37 & 91.37 & 67.87 & 83.04 \\   

Table 2: Accuracy comparison of floating-point (FP) baseline, BumbleBee, Nimbus (without fine-tuning), and Nimbus\({}^{}\) (with finetuning).

approximation. We also compare accuracy and efficiency with works that compromise accuracy in Appendix F.4, including MPCFormer and BOLT.

Figure 5 presents the output error of exponential and GELU functions to further explain the effectiveness of our approximation. The error is summarized using a batch of test data on a certain layer. On the standard ring \(_{2^{q 4}}\) and precision \(s=18\), our L2-norm errors are around 0.005 and are close to the loss-free approximation of BumbleBee. When reducing to ring \(_{2^{32}}\) and \(s=12\), BumbleBee encounters higher errors. This increase is attributed to the more pronounced fixed-point errors that arise when evaluating high-degree polynomials. Moreover, the destructive overflow occurs for precision greater than 10 bits, as the sharp divergence of green and red curves of GELU function. Nimbus has a steady fixed-point error and is not prone to overflow thanks to the low-degree polynomial. This enables moving to the smaller ring with a minor impact on the accuracy.

### Efficiency Comparison

As the main body of the Transformer model are identical Transformer blocks, we present the end-to-end latency of one Transformer block under LAN and WAN network settings in Figure 6. Besides the optimized parts of this paper, we unify the unoptimized activation matrix multiplication (\(QK^{T}\&PV\)) and LayerNorm (LN) using the BumbleBee's latency. The latencies of Iron are shorter than those reported in their paper due to SecretFlow integrating many SOTA optimizations for the building blocks, such as OT and inverse square root. Combining all our optimizations, the overall runtime is \(4.8 5.9\) faster than Iron and \(2.7 4.7\) faster than BumbleBee. Comprehensive results on varying Transformer size and input sequence length are listed in Appendix F.1. In the following, we provide a detailed analysis of linear and non-linear layers.

For linear layers, our method is efficient in both computation and communication. Therefore, we achieve obvious speedup in both LAN and WAN settings. Compared with stronger BumbleBee, we have \(7.2 12.5\) in the LAN setting and \(2.9 4.0\) in the WAN setting. More speedup for the LAN setting indicates we accelerate the computation more than the communication. For non-linear functions, our method reduces both the communication size and rounds, so that we obtain similar speedup for both the LAN and WAN settings. Compared with stronger baseline BumbleBee, the GELU is \(3.4\) faster in the LAN setting and \(3.3\) faster than the WAN setting. The Softmax is \(4.0\) faster in the LAN setting and \(2.9\) faster than the WAN setting.

### Communication Analysis

Then, we compare the communication cost and the number of rounds of linear layers, Softmax, and GELU in Table 3. The data is summarized using BERTbase and sequence length 128. For different types of linear layers, our protocol only requires half the number of communication rounds.

Figure 5: The L2-Norm of output error between oracle non-linear functions and approximations.

Figure 6: The end-to-end latency of a Transformer block of BERTbase and its breakdown.

Our total communication size of linear layers is reduced to only 11.51% of that of Iron. Although BumbleBee takes extra "automorphism" operation to compress the output ciphertext, our communication is still only 65% compared with BumbleBee since we also eliminate the communication of the input ciphertext. As for the non-linear layers, compared with stronger baseline BumbleBee, we have fewer rounds and \(3\) less communication due to simpler piecewise polynomial approximations and the smaller ring size.

## 6 Related Work

**Privacy-preserving Neural Network Inference.** Due to the rapidly growing concerns about data privacy in DNN-based applications, significant efforts have been made to design efficient cryptographic protocols for DNN models [8; 18; 32; 43; 31]. Early works focus on the convolutional neural network (CNN) models. Cryptonets  proposed one of the first protocols for 2PC HE-based private neural network inference. Later works [18; 34; 17] are hybrid 2PC neural network inference protocols combining HE for matrix multiplications and multi-party computation for non-linear functions.

**Private Transformers.** Several works have investigated two-party secure inference for the Transformer model. For linear layers, Iron  builds upon Cheetah  by generalizing the original encoding of matrix-vector multiplication to matrix-matrix multiplication. Both Cheetah and Iron leave blanks in the input and output ciphertexts. BumbleBee  utilizes the "automorphism" operation to compress multiple output ciphertexts, which trades computation for communication. A recent work BOLT  adopts SIMD encoding to homomorphically evaluate the linear layer, which also trades computation for the compact output ciphertext. All existing works adopt the server-side inner product protocol. In contrast, this work proposes the client-side outer product protocol that eliminates the input ciphertext communication. The proposed protocol also allows a novel encoding approach that facilitates more efficient homomorphic computation and output communication. Other works [1; 5] consider 3PC inference for Transformers, which rely on different settings and cryptographic primitives from this work.

For the non-linear layers, some studies, such as THE-X  and MPCFormer , evaluate transformer models using cryptographic friendly replacements for non-linear layers, such as using \(||+c)^{2}}{_{i}(|+c)^{2}}\) and \((x)}{8}++\). However, such aggressive approximations lead to noticeable accuracy loss, even when employing knowledge distillation to mitigate the decline in accuracy. Other methods, such as look-up tables for faithful approximation [31; 13; 29], are computationally expensive to maintain model accuracy. Later works, including PUMA  and BumbleBee , utilize piecewise polynomial approximation, which does not result in an accuracy drop but is also relatively costly to compute. In contrast, this work is inspired by insights from the input distribution used in the Transformer model [9; 10; 11; 12; 40; 24]. We propose fitting the non-linear functions according to their input distribution, allowing for lower-degree polynomials and fewer polynomial pieces without sacrificing accuracy.

## 7 Conclusion

We propose a privacy-preserving, accurate, and efficient two-party inference framework Nimbus for Transformers. We present an efficient protocol of secure matrix multiplication using the COP approach, achieving significantly better computation and communication efficiencies. We use a distribution-aware polynomial approximation for non-linear layers, allowing a simpler approximation with less communication and rounds. These optimizations significantly improve the performance, advancing a step towards the practical use of secure Transformer inference.

    &  &  &  \\   & Comm. & Rd & Comm. & Rd & Comm. & Rd \\  \(_{}\) & 74.64 & 2 & 14.47 & 2 & 10.35 & 1 \\ \(_{}\) & 40.2 & 2 & 6.71 & 2 & 3.05 & 1 \\ \(_{_{1}}\) & 84.46 & 2 & 18.35 & 2 & 15.52 & 1 \\ \(_{_{2}}\) & 78.37 & 2 & 15.71 & 2 & 3.05 & 1 \\  \(\) & 689.45 & 110 & 354.26 & 70 & 115.35 & 60 \\ GELU & 283.89 & 65 & 185.13 & 46 & 53.22 & 24 \\   

Table 3: Communication cost (megabytes) and rounds comparison on one Transformer block.