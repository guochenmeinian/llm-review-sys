# Offline Reinforcement Learning with Differential Privacy

Dan Qiao

Department of Computer Science

UC Santa Barbara

Santa Barbara, CA 93106

danqiao@ucsb.edu

&Yu-Xiang Wang

Department of Computer Science

UC Santa Barbara

Santa Barbara, CA 93106

yuxiangw@cs.ucsb.edu

###### Abstract

The offline reinforcement learning (RL) problem is often motivated by the need to learn data-driven decision policies in financial, legal and healthcare applications. However, the learned policy could retain sensitive information of individuals in the training data (e.g., treatment and outcome of patients), thus susceptible to various privacy risks. We design offline RL algorithms with differential privacy guarantees which provably prevent such risks. These algorithms also enjoy strong instance-dependent learning bounds under both tabular and linear Markov Decision Process (MDP) settings. Our theory and simulation suggest that the privacy guarantee comes at (almost) no drop in utility comparing to the non-private counterpart for a medium-size dataset.

## 1 Introduction

Offline Reinforcement Learning (or batch RL) aims to learn a near-optimal policy in an unknown environment1 through a static dataset gathered from some behavior policy \(\). Since offline RL does not require access to the environment, it can be applied to problems where interaction with environment is infeasible, _e.g._, when collecting new data is costly (trade or finance (Zhang et al., 2020)), risky (autonomous driving (Sallab et al., 2017)) or illegal / unethical (healthcare (Raghu et al., 2017)). In such practical applications, the data used by an RL agent usually contains sensitive information. Take medical history for instance, for each patient, at each time step, the patient reports her health condition (age, disease, etc.), then the doctor decides the treatment (which medicine to use, the dosage of medicine, etc.), finally there is treatment outcome (whether the patient feels good, etc.) and the patient transitions to another health condition. Here, (health condition, treatment, treatment outcome) corresponds to (state, action, reward) and the dataset can be considered as \(n\) (number of patients) trajectories sampled from a MDP with horizon \(H\) (number of treatment steps). However, learning agents are known to implicitly memorize details of individual training data points verbatim (Carlini et al., 2019), even if they are irrelevant for learning (Brown et al., 2021), which makes offline RL models vulnerable to various privacy attacks.

Differential privacy (DP) (Dwork et al., 2006) is a well-established definition of privacy with many desirable properties. A differentially private offline RL algorithm will return a decision policy that is indistinguishable from a policy trained in an alternative universe any individual user is replaced, thereby preventing the aforementioned privacy risks. There is a surge of recent interest in developing RL algorithms with DP guarantees, but they focus mostly on the online setting (Vietri et al., 2020; Garcelon et al., 2021; Liao et al., 2021; Chowdhury and Zhou, 2021; Luyo et al., 2021).

Offline RL is arguably more practically relevant than online RL in the applications with sensitive data. For example, in the healthcare domain, online RL requires actively running new exploratory policies (clinical trials) with every new patient, which often involves complex ethical / legal clearances, whereas offline RL uses only historical patient records that are often accessible for research purposes. Clear communication of the adopted privacy enhancing techniques (e.g., DP) to patients was reported to further improve data access (Kim et al., 2017).

**Our contributions.** In this paper, we present the first provably efficient algorithms for offline RL with differential privacy. Our contributions are twofold.

* We design two new pessimism-based algorithms DP-APVI (Algorithm 1) and DP-VAPVI (Algorithm 2), one for the tabular setting (finite states and actions), the other for the case with linear function approximation (under linear MDP assumption). Both algorithms enjoy DP guarantees (pure DP or zCDP) and instance-dependent learning bounds where the cost of privacy appears as lower order terms.
* We perform numerical simulations to evaluate and compare the performance of our algorithm DP-VAPVI (Algorithm 2) with its non-private counterpart VAPVI (Yin et al., 2022) as well as a popular baseline PEVI (Jin et al., 2021). The results complement the theoretical findings by demonstrating the practicality of DP-VAPVI under strong privacy parameters.

**Related work.** To our knowledge, differential privacy in offline RL tasks has not been studied before, except for much simpler cases where the agent only evaluates a single policy (Balle et al., 2016; Xie et al., 2019). Balle et al. (2016) privatized first-visit Monte Carlo-Ridge Regression estimator by an output perturbation mechanism and Xie et al. (2019) used DP-SGD. Neither paper considered offline learning (or policy optimization), which is our focus.

There is a larger body of work on private RL in the online setting, where the goal is to minimize regret while satisfying either joint differential privacy (Vietri et al., 2020; Chowdhury and Zhou, 2021; Ngo et al., 2022; Luyo et al., 2021) or local differential privacy (Garcelon et al., 2021; Liao et al., 2021; Luyo et al., 2021; Chowdhury and Zhou, 2021). The offline setting introduces new challenges in DP as we cannot _algorithmically enforce_ good "exploration", but have to work with a static dataset and privately estimate the uncertainty in addition to the value functions. A private online RL algorithm can sometimes be adapted for private offline RL too, but those from existing work yield suboptimal and non-adaptive bounds. We give a more detailed technical comparison in Appendix B.

Among non-private offline RL works, we build directly upon non-private offline RL methods known as Adaptive Pessimistic Value Iteration (APVI, for tabular MDPs) (Yin and Wang, 2021) and Variance-Aware Pessimistic Value Iteration (VAPVI, for linear MDPs) (Yin et al., 2022), as they give the strongest theoretical guarantees to date. We refer readers to Appendix B for a more extensive review of the offline RL literature. Introducing DP to APVI and VAPVI while retaining the same sample complexity (modulo lower order terms) require nontrivial modifications to the algorithms.

**A remark on technical novelty.** Our algorithms involve substantial technical innovation over previous works on online DP-RL with joint DP guarantee2. Different from previous works, our DP-APVI (Algorithm 1) operates on Bernstein type pessimism, which requires our algorithm to deal with conditional variance using private statistics. Besides, our DP-VAPVI (Algorithm 2) replaces the LSVI technique with variance-aware LSVI (also known as weighted ridge regression, first appears in (Zhou et al., 2021)). Our DP-VAPVI releases conditional variance privately, and further applies weighted ridge regression privately. Both approaches ensure tighter instance-dependent bounds on the suboptimality of the learned policy.

## 2 Problem Setup

**Markov Decision Process.** A finite-horizon _Markov Decision Process_ (MDP) is denoted by a tuple \(M=(,,P,r,H,d_{1})\)(Sutton and Barto, 2018), where \(\) is state space and \(\) is action space. A non-stationary transition kernel \(P_{h}:\) maps each state action \((s_{h},a_{h})\) to a probability distribution \(P_{h}(|s_{h},a_{h})\) and \(P_{h}\) can be different across time. Besides, \(r_{h}: A\) is the expected immediate reward satisfying \(0 r_{h} 1\), \(d_{1}\) is the initial state distribution and \(H\) is the horizon.

A policy \(=(_{1},,_{H})\) assigns each state \(s_{h}\) a probability distribution over actions according to the map \(s_{h}_{h}(|s_{h})\), \(\,h[H]\). A random trajectory \(s_{1},a_{1},r_{1},,s_{H},a_{H},r_{H},s_{H+1}\) is generated according to \(s_{1} d_{1},a_{h}_{h}(|s_{h}),r_{h} r_{h}(s_{h},a_{h}),s_{h +1} P_{h}(|s_{h},a_{h}),\,h[H]\).

For tabular MDP, we have \(\) is the discrete state-action space and \(S:=||,A:=||\) are finite. In this work, we assume that \(r\) is known3. In addition, we denote the per-step marginal state-action occupancy \(d_{h}^{}(s,a)\) as: \(d_{h}^{}(s,a):=[s_{h}=s|s_{1} d_{1},]_{h}(a|s),\) which is the marginal state-action probability at time \(h\).

**Value function, Bellman (optimality) equations.** The value function \(V_{h}^{}()\) and Q-value function \(Q_{h}^{}(,)\) for any policy \(\) is defined as: \(V_{h}^{}(s)=_{}[_{t=h}^{H}r_{t}|s_{h}=s], Q_{h}^{ }(s,a)=_{}[_{t=h}^{H}r_{t}|s_{h},a_{h}=s,a],\;\;\,h,s,a[H]\). The performance is defined as \(v^{}:=_{d_{1}}[V_{1}^{}]=_{,d_{1}} [_{t=1}^{H}r_{t}]\). The Bellman (optimality) equations follow \(\,h[H]\): \(Q_{h}^{}=r_{h}+P_{h}V_{h+1}^{},\;\;V_{h}^{}=_{a_{h}} [Q_{h}^{}],\;\;\;Q_{h}^{}=r_{h}+P_{h}V_{h+1}^{},\;V_{h}^{}= _{a}Q_{h}^{}(,a)\).

**Linear MDP [Jin et al., 2020b].** An episodic MDP \((,,H,P,r)\) is called a linear MDP with known feature map \(:^{d}\) if there exist \(H\) unknown signed measures \(_{h}^{d}\) over \(\) and \(H\) unknown reward vectors \(_{h}^{d}\) such that

\[P_{h}(s^{} s,a)=(s,a),_{h}(s^{ }), r_{h}(s,a)=(s,a), _{h},(h,s,a,s^{})[H] .\]

Without loss of generality, we assume \(\|(s,a)\|_{2} 1\) and \((\|_{h}()\|_{2},\|_{h}\|_{2})\) for all \(h,s,a[H]\). An important property of linear MDP is that the value functions are linear in the feature map, which is summarized in Lemma E.14.

**Offline setting and the goal.** The offline RL requires the agent to find a policy \(\) in order to maximize the performance \(v^{}\), given only the episodic data \(=\{(s_{h}^{},a_{h}^{},r_{h}^{},s_{h+1}^{})\}_{ [H]}^{h[H]}\)4 rolled out from some fixed and possibly unknown behavior policy \(\), which means we cannot change \(\) and in particular we do not assume the functional knowledge of \(\). In conclusion, based on the batch data \(\) and a targeted accuracy \(>0\), the agent seeks to find a policy \(_{}\) such that \(v^{}-v^{_{}}\).

### Assumptions in offline RL

In order to show that our privacy-preserving algorithms can generate near optimal policy, certain coverage assumptions are needed. In this section, we will list the assumptions we use in this paper.

**Assumptions for tabular setting.**

**Assumption 2.1** (Liu et al., 2019).: _There exists one optimal policy \(^{}\), such that \(^{}\) is fully covered by \(\), i.e. \(\,s_{h},a_{h},\;d_{h}^{^{}}(s_{h},a_{h})>0\) only if \(d_{h}^{}(s_{h},a_{h})>0\). Furthermore, we denote the trackable set as \(_{h}:=\{(s_{h},a_{h}):d_{h}^{}(s_{h},a_{h})>0\}\)._

Assumption 2.1 is the weakest assumption needed for accurately learning the optimal value \(v^{}\) by requiring \(\) to trace the state-action space of one optimal policy (\(\) can be agnostic at other locations). Similar to (Yin and Wang, 2021), we will use Assumption 2.1 for the tabular part of this paper, which enables comparison between our sample complexity to the conclusion in (Yin and Wang, 2021), whose algorithm serves as a non-private baseline.

**Assumptions for linear setting.** First, we define the expectation of covariance matrix under the behavior policy \(\) for all time step \(h[H]\) as below:

\[_{h}^{p}:=_{}[(s_{h},a_{h})(s_{h},a_{h})^{} ].\] (1)

As have been shown in (Wang et al., 2021; Yin et al., 2022), learning a near-optimal policy from offline data requires coverage assumptions. Here in linear setting, such coverage is characterized by the minimum eigenvalue of \(_{h}^{p}\). Similar to (Yin et al., 2022), we apply the following assumption for the sake of comparison.

**Assumption 2.2** (Feature Coverage, Assumption 2 in (Wang et al., 2021)).: _The data distributions \(\) satisfy the minimum eigenvalue condition: \(\,h[H]\), \(_{h}:=_{}(_{h}^{p})>0\). Furthermore, we denote \(=_{h}_{h}\)._

### Differential Privacy in offline RL

In this work, we aim to design privacy-preserving algorithms for offline RL. We apply differential privacy as the formal notion of privacy. Below we revisit the definition of differential privacy.

**Definition 2.3** (Differential Privacy (Dwork et al., 2006)).: _A randomized mechanism \(M\) satisfies \((,)\)-differential privacy (\((,)\)-DP) if for all neighboring datasets \(U,U^{}\) that differ by one data point and for all possible event \(E\) in the output range, it holds that_

\[[M(U) E] e^{}[M(U^{}) E]+.\]

_When \(=0\), we say pure DP, while for \(>0\), we say approximate DP._

In the problem of offline RL, the dataset consists of several trajectories, therefore one data point in Definition 2.3 refers to one single trajectory. Hence the definition of Differential Privacy means that the difference in the distribution of the output policy resulting from replacing one trajectory in the dataset will be small. In other words, an adversary can not infer much information about any single trajectory in the dataset from the output policy of the algorithm.

**Remark 2.4**.: _For a concrete motivating example, please refer to the first paragraph of Introduction. We remark that our definition of DP is consistent with Joint DP and Local DP defined under the online RL setting where JDP/LDP also cast each user as one trajectory and provide user-wise privacy protection. For detailed definitions and more discussions about JDP/LDP, please refer to Qiao and Wang (2023)._

During the whole paper, we will use zCDP (defined below) as a surrogate for DP, since it enables cleaner analysis for privacy composition and Gaussian mechanism. The properties of zCDP (e.g., composition, conversion formula to DP) are deferred to Appendix E.3.

**Definition 2.5** (zCDP (Dwork and Rothblum, 2016; Bun and Steinke, 2016)).: _A randomized mechanism \(M\) satisfies \(\)-Zero-Concentrated Differential Privacy (\(\)-zCDP), if for all neighboring datasets \(U,U^{}\) and all \((1,)\),_

\[D_{}(M(U)\|M(U^{})),\]

_where \(D_{}\) is the Renyi-divergence (Van Erven and Harremos, 2014)._

Finally, we go over the definition and privacy guarantee of Gaussian mechanism.

**Definition 2.6** (Gaussian Mechanism (Dwork et al., 2014)).: _Define the \(_{2}\) sensitivity of a function \(f:^{}^{d}\) as_

\[_{2}(f)=_{\,U,U^{}}\|f(U)-f(U^{})\|_{ 2}.\]

_The Gaussian mechanism \(\) with noise level \(\) is then given by_

\[(U)=f(U)+(0,^{2}I_{d}).\]

**Lemma 2.7** (Privacy guarantee of Gaussian mechanism (Dwork et al., 2014; Bun and Steinke, 2016)).: _Let \(f:^{}^{d}\) be an arbitrary \(d\)-dimensional function with \(_{2}\) sensitivity \(_{2}\). Then for any \(>0\), Gaussian Mechanism with parameter \(^{2}=^{2}}{2}\) satisfies \(\)-zCDP. In addition, for all \(0<,<1\), Gaussian Mechanism with parameter \(=}{}}\) satisfies \((,)\)-DP._

We emphasize that the privacy guarantee covers any input data. It does _not_ require any distributional assumptions on the data. The RL-specific assumptions (e.g., linear MDP and coverage assumptions) are only used for establishing provable utility guarantees.

## 3 Results under tabular MDP: DP-APVI (Algorithm 1)

For reinforcement learning, the tabular MDP setting is the most well-studied setting and our first result applies to this regime. We begin with the construction of private counts.

**Private Model-based Components.** Given data \(=\{(s_{h}^{},a_{h}^{},r_{h}^{},s_{h+1}^{})\}_{[ n]}^{h[H]}\), we denote \(n_{s_{h},a_{h}}:=_{=1}^{n}[s_{h}^{},a_{h}^{}=s_{h},a _{h}]\) be the total counts that visit \((s_{h},a_{h})\) pair at time \(h\) and\(_{r=1}^{n}[s_{h}^{},a_{h}^{},s_{h+1}^{}=s_{h},a_{h},s_{h+1}]\) be the total counts that visit \((s_{h},a_{h},s_{h+1})\) pair at time \(h\), then given the budget \(\) for zCDP, we add _independent_ Gaussian noises to all the counts:

\[n_{s_{h},a_{h}}^{}=\{n_{s_{h},a_{h}}+(0,^{2}) \}^{+},\ n_{s_{h},a_{h},s_{h+1}}^{}=\{n_{s_{h},a_{h},s_{h+1}} +(0,^{2})\}^{+},\ ^{2}=.\] (2)

However, after adding noise, the noisy counts \(n^{}\) may not satisfy \(n_{s_{h},a_{h}}^{}=_{s_{h+1}}n_{s_{h},a_{h},s_{h+1}}^ {}\). To address this problem, we choose the private counts of visiting numbers as the solution to the following optimization problem (here \(E_{}=4^{2}}{}}{}}\) is chosen as a high probability uniform bound of the noises we add):

\[&\{_{s_{h},a_{h},s^{}}\}_{s^{ }}=*{argmin}_{\{x_{s^{}}\}_{s^{} }}_{s^{}}|x_{s^{}}-n_{s_{h},a _{h},s^{}}^{}|\\ &|_{s^{}}x_{s^{ }}-n_{s_{h},a_{h}}^{}|}{2}x_{s^{}} 0,\,s^{}. \] (3)

**Remark 3.1** (Some explanations).: _The optimization problem above serves as a post-processing step which will not affect the DP guarantee of our algorithm. Briefly speaking, (3) finds a set of noisy counts such that \(_{s_{h},a_{h}}=_{s^{}}_{s_{ h},a_{h},s^{}}\) and the estimation error for each \(_{s_{h},a_{h}}\) and \(_{s_{h},a_{h},s^{}}\) is roughly \(E_{}\).5 In contrast, if we directly take the crude approach that \(_{s_{h},a_{h},s_{h+1}}=n_{s_{h},a_{h},s_{h+1}}^{}\) and \(_{s_{h},a_{h}}=_{s_{h+1}}_{s_{h},a _{h},s_{h+1}}\), we can only derive \(|_{s_{h},a_{h}}-n_{s_{h},a_{h}}|(_{})\) through concentration on summation of \(S\) i.i.d. Gaussian noises. In conclusion, solving the optimization problem (3) enables tight analysis for the lower order term (the additional cost of privacy)._

**Remark 3.2** (Computational efficiency).: _The optimization problem (3) can be reformulated as:_

\[\ t,|x_{s^{}}-n_{s_{h},a_{h},s^{}}^{}| t x_{s^{}} 0\ \,s^{},\ |_{s^{}}x_{s^{}}-n_{s_{h},a_{h}}^{} |}{2}.\] (4)

_Note that (4) is a Linear Programming problem with \(S+1\) variables and \(2S+2\) linear constraints (one constraint on absolute value is equivalent to two linear constraints), which can be solved efficiently by the simplex method (Ficken, 2015) or other provably efficient algorithms (Nemhauser and Wolsey, 1988). Therefore, our Algorithm 1 is computationally friendly._

The private estimation of the transition kernel is defined as:

\[_{h}(s^{}|s_{h},a_{h})=_{s_{h},a_{h},s^{ }}}{_{s_{h},a_{h}}},\] (5)

if \(_{s_{h},a_{h}}>E_{}\) and \(_{h}(s^{}|s_{h},a_{h})=\) otherwise.

**Remark 3.3**.: _Different from the transition kernel estimate in previous works (Vietri et al., 2020; Chowdhury and Zhou, 2021) that may not be a distribution, we have to ensure that ours is a probability distribution, because our Bernstein type pessimism (line 5 in Algorithm 1) needs to take variance over this transition kernel estimate. The intuition behind the construction of our private transition kernel is that, for those state-action pairs with \(_{s_{h},a_{h}} E_{}\), we can not distinguish whether the non-zero private count comes from noise or actual visitation. Therefore we only take the empirical estimate of the state-action pairs with sufficiently large \(_{s_{h},a_{h}}\)._

**Algorithmic design.** Our algorithmic design originates from the idea of pessimism, which holds conservative view towards the locations with high uncertainty and prefers the locations we have more confidence about. Based on the Bernstein type pessimism in APVI (Yin and Wang, 2021), we design a similar pessimistic algorithm with private counts to ensure differential privacy. If we replace \(\) and \(\) with \(n\) and \(\)6, then our DP-APVI (Algorithm 1) will degenerate to APVI. Compared to the pessimism defined in APVI, our pessimistic penalty has an additional term \((}{_{s_{h},a_{h}}})\), which accounts for the additional pessimism due to our application of private statistics.

We state our main theorem about DP-APVI below, the proof sketch is deferred to Appendix C.1 and detailed proof is deferred to Appendix C due to space limit.

**Theorem 3.4**.: _DP-APVI (Algorithm 1) satisfies \(\)-cDP. Furthermore, under Assumption 2.1, denote \(_{m}:=_{h[H]}\{d_{h}^{}(s_{h},a_{h}):d_{h}^{}(s_{h},a _{h})>0\}\). For any \(0<<1\), there exists constant \(c_{1}>0\), such that when \(n>c_{1}\{H^{2},E_{}\}/_{m}\) (\(=(HSA/)\)), with probability \(1-\), the output policy \(\) of DP-APVI satisfies_

\[0 v^{}-v^{} 4_{h=1}^{H}_{(s _{h},a_{h})_{h}}d_{h}^{^{}}(s_{h},a_{h})_{P_{h}(|s_{h},a_{h})}(V_{h+1}^{}()) }{nd_{h}^{}(s_{h},a_{h})}}+(+SH^{2}E _{}}{n_{m}}),\] (6)

_where \(\) hides constants and Polylog terms, \(E_{}=4A}{}}{}}\)._

**Comparison to non-private counterpart APVI (Yin and Wang, 2021).** According to Theorem 4.1 in (Yin and Wang, 2021), the sub-optimality bound of APVI is for large enough \(n\), with high probability, the output \(\) satisfies:

\[0 v^{}-v^{}(_{h=1}^{H}_{( s_{h},a_{h})_{h}}d_{h}^{^{}}(s_{h},a_{h})_{P_{h}(|s_{h},a_{h})}(V_{h+1}^{}())}{nd _{h}^{}(s_{h},a_{h})}})+(}{n _{m}}).\] (7)

Compared to our Theorem 3.4, the additional sub-optimality bound due to differential privacy is \((E_{}}{n_{m}})= (}}{n_{m} })=(}}{n_{m} })\).7 In the most popular regime where the privacy budget \(\) or \(\) is a constant, the additional term due to differential privacy appears as a lower order term, hence becomes negligible as the sample complexity \(n\) becomes large.

**Comparison to Hoeffding type pessimism.** We can simply revise our algorithm by using Hoeffding type pessimism, which replaces the pessimism in line 5 with \(C_{1}H_{s_{h},a_{h}}-E_{}}}+SHE_{}}{_{s_{h},a_{h}}}\). Thenwith a similar proof schedule, we can arrive at a sub-optimality bound that with high probability,

\[0 v^{}-v^{}(H_{h=1}^{H}_{( s_{h},a_{h})_{h}}d_{h}^{^{}}(s_{h},a_{h})^{ }(s_{h},a_{h})}})+(E_{}}{n _{m}}).\] (8)

Compared to our Theorem 3.4, our bound is tighter because we express the dominate term by the system quantities instead of explicit dependence on \(H\) (and \(_{P_{h}(|s_{h},a_{h})}(V_{h+1}^{}()) H^{2}\)). In addition, we highlight that according to Theorem G.1 in (Yin and Wang, 2021), our main term nearly matches the non-private minimax lower bound. For more detailed discussions about our main term and how it subsumes other optimal learning bounds, we refer readers to (Yin and Wang, 2021).

**Apply Laplace Mechanism to achieve pure DP.** To achieve Pure DP instead of \(\)-\(DP}\), we can simply replace Gaussian Mechanism with Laplace Mechanism (defined as Definition E.19). Given privacy budget for Pure DP \(\), since the \(_{1}\) sensitivity of \(\{n_{s_{h},a_{h}}\}\{n_{s_{h},a_{h},s_{h+1}}\}\) is \(_{1}=4H\), we can add _independent_ Laplace noises \(()\) to each count to achieve \(\)-DP due to Lemma E.20. Then by using \(E_{}=()\) instead of \(E_{}\) and keeping everything else (3), (5) and Algorithm 1) the same, we can reach a similar result to Theorem 3.4 with the same proof schedule. The only difference is that here the additional learning bound is \((}{n_{m}})\), which still appears as a lower order term.

## 4 Results under linear MDP: DP-VAPVI(Algorithm 2)

In large MDPs, to address the computational issues, the technique of function approximation is widely applied, and linear MDP is a concrete model to study linear function approximations. Our second result applies to the linear MDP setting. Generally speaking, function approximation reduces the dimensionality of private releases comparing to the tabular MDPs. We begin with private counts.

**Private Model-based Components.** Given the two datasets \(\) and \(^{}\) (both from \(\)) as in Algorithm 2, we can apply variance-aware pessimistic value iteration to learn a near optimal policy as in VAPVI (Yin et al., 2022). To ensure differential privacy, we add _independent_ Gaussian noises to the \(5H\) statistics as in DP-VAPVI (Algorithm 2) below. Since there are \(5H\) statistics, by the adaptive composition of zCDP (Lemma E.17), it suffices to keep each count \(_{0}\)-zCDP, where \(_{0}=\). In DP-VAPVI, we use \(_{1},_{2},_{3},K_{1},K_{2}\)8 to denote the noises we add. For all \(_{i}\), we directly apply Gaussian Mechanism. For \(K_{i}\), in addition to the noise matrix \(}(Z+Z^{})\), we also add \(I_{d}\) to ensure that all \(K_{i}\) are positive definite with high probability (The detailed definition of \(E,L\) can be found in Appendix A).

Below we will show the algorithmic design of DP-VAPVI (Algorithm 2). For the offline dataset, we divide it into two independent parts with equal length: \(=\{(s_{h}^{},a_{h}^{},r_{h}^{},s_{h+1}^{})\}_{ [K]}\) and \(^{}=\{(_{h}^{},_{h}^{},_{h}^{ },_{h+1}^{})\}_{[K]}^{h[H]}\). One for estimating variance and the other for calculating \(Q\)-values.

**Estimating conditional variance.** The first part (line 4 to line 8) aims to estimate the conditional variance of \(_{h+1}\) via the definition of variance: \([_{h}_{h+1}](s,a)=[P_{h}(_{h+1})^{2}](s,a) -([P_{h}_{h+1}](s,a))^{2}\). For the first term, by the definition of linear MDP, it holds that \([P_{h}_{h+1}^{2}](s,a)=(s,a)^{}_{} _{h+1}^{2}(s^{})\,_{h}(s^{} )=,_{}_{h+1}^{2}(s^{} )\,_{h}(s^{})\). We can estimate \(_{h}=_{}_{h+1}^{2}(s^{})\, _{h}(s^{})\) by applying ridge regression. Below is the output of ridge regression with raw statistics without noise:

\[*{argmin}_{^{d}}_{k=1}^{K}[ (_{h}^{k},_{h}^{k}),-_{h+1}^{2} (_{h+1}^{k})]^{2}+\|\|_{2}^{2}=_{ h}^{-1}_{k=1}^{K}(_{h}^{k},_{h}^{k})_{h+1}^{2} (_{h+1}^{k}),\]where definition of \(_{h}\) can be found in Appendix A. Instead of using the raw statistics, we replace them with private ones with Gaussian noises as in line 5. The second term is estimated similarly in line 6. The final estimator is defined as in line 8: \(_{h}(,)^{2}=\{1,}_{h} _{h+1}(,)\}\).9

**Variance-weighted LSVI** Instead of directly applying LSVI (Jin et al., 2021), we can solve the variance-weighted LSVI (line 10). The result of variance-weighted LSVI with non-private statistics is shown below:

\[*{argmin}_{w^{d}}\|w\|_{2}^{2}+_{k=1}^{K} ^{k},a_{h}^{k}),w-r_{h}^{k}-_ {h+1}(s_{h+1}^{k})]^{2}}{_{h}^{2}(s_{h}^{k},a_{h}^{k}) }=_{h}^{-1}_{k=1}^{K}^{k},a_{h}^{k} )[r_{h}^{k}+_{h+1}(s_{h+1}^{k})] }{_{h}^{2}(s_{h}^{k},a_{h}^{k})},\]

where definition of \(_{h}\) can be found in Appendix A. For the sake of differential privacy, we use private statistics instead and derive the \(_{h}\) as in line 10.

**Our private pessimism.** Notice that if we remove all the Gaussian noises we add, our DP-VAPVI (Algorithm 2) will degenerate to VAPVI (Yin et al., 2022). We design a similar pessimistic penalty using private statistics (line 11), with additional \(\) accounting for the extra pessimism due to DP.

**Main theorem.** We state our main theorem about DP-VAPVI below, the proof sketch is deferred to Appendix D.1 and detailed proof is deferred to Appendix D due to space limit. Note that quantities \(_{i},L,E\) can be found in Appendix A and briefly, \(L=(d/})\), \(E=()\). For the sample complexity lower bound, within the practical regime where the privacy budget is not very small, \(\{_{i}\}\) is dominated by \(\{(H^{12}d^{3}/^{5}),(H^{14}d/^{5})\}\), which also appears in the sample complexity lower bound of VAPVI (Yin et al., 2022). The \(_{V}^{2}(s,a)\) in Theorem 4.1 is defined as \(\{1,_{P_{h}}(V)(s,a)\}\) for any \(V\).

**Theorem 4.1**.: _DP-VAPVI (Algorithm 2) satisfies \(\)-zCDP. Furthermore, let \(K\) be the number of episodes. Under the condition that \(K>\{_{1},_{2},_{3},_{4}\}\) and \(>\), where \(:=_{V[0,H],\ s^{} P_{h}(s,a),\ h[H]}|+V (s^{})-(_{h}V)(s,a)}{_{V}(s,a)}|\), for any \(0<<\), with probability \(1-\), for all policy \(\) simultaneously, the output \(\) of DP-VAPVI satisfies_

\[v^{}-v^{}(_{h=1}^{H}_{}_{h}^{-1}(, )})+,\] (9)

_where \(_{h}=_{k=1}^{K}^{h},a_{h}^{h})(s_{h}^{h},a_ {h}^{h})^{}}{_{W_{h+1}(s_{h}^{h}-s_{h}^{h})}^{}}+ I_{d}\), \(D=(L}{}+E}{^{3/2}}+ H^{3})\) and \(\) hides constants and Policy terms._

_In particular, define \(_{h}^{}=_{k=1}^{K}^{h},a_{h}^{h})(s_ {h}^{h},a_{h}^{h})^{}}{_{W_{h+1}(s_{h}^{h},a_{h}^{h})}^{}}+  I_{d}\), we have with probability \(1-\),_

\[v^{}-v^{}(_{h=1}^{H} _{^{}}_{h}^{ -1}(,)})+.\] (10)

**Comparison to non-private counterpart VAPVI (Yin et al., 2022).** Plugging in the definition of \(L,E\) (Appendix A), under the meaningful case that the privacy budget is not very large, \(DH\) is dominated by \((}d/^{}}{})\). According to Theorem 3.2 in (Yin et al., 2022), the sub-optimality bound of VAPVI is for sufficiently large \(K\), with high probability, the output \(\) satisfies:

\[v^{}-v^{}(_{h=1}^{H} _{^{}}_{h}^{ -1}(,)})+}{K}.\] (11)

Compared to our Theorem 4.1, the additional sub-optimality bound due to differential privacy is \((}d/^{}}{  K})=(}d/^{ }}{ K})\).10 In the most popular regime where the privacy budget \(\) or \(\) is a constant, the additional term due to differential privacy also appears as a lower order term.

**Instance-dependent sub-optimality bound.** Similar to DP-APVI (Algorithm 1), our DP-VAPVI (Algorithm 2) also enjoys instance-dependent sub-optimality bound. First, the main term in (10) improves PEVI (Jin et al., 2021) over \(O()\) on feature dependence. Also, our main term admits no explicit dependence on \(H\), thus improves the sub-optimality bound of PEVI on horizon dependence. For more detailed discussions about our main term, we refer readers to (Yin et al., 2022).

**Private and safe policy improvement.** In addition to the sub-optimality bound (10), we have the so called oracle inequality (9). Therefore, the performance \(v^{}\) can be lower bounded by \(_{}v^{}-(_{h=1}^{H} _{}_{h}^{-1}( ,)}-\). When choosing \(\) to be the optimal policy in the neighborhood of the behavior policy \(\), our DP-VAPVI (Algorithm 2) sheds light on safe policy improvement with differential privacy guarantee.

## 5 Tightness of our results

We believe our bounds for offline RL with DP is tight. To the best of our knowledge, APVI and VAPVI provide the tightest bound under tabular MDP and linear MDP, respectively. The suboptimality bounds of our algorithms match these two in the main term, with some lower order additional terms. The leading terms are known to match multiple information-theoretical lower bounds for offline RL simultaneously (this was illustrated in Yin and Wang (2021); Yin et al. (2022)), for this reason our bound cannot be improved in general. For the lower order terms, the dependence on sample complexity \(n\) and privacy budget \(\): \(()\) is optimal since policy learning is a special case of ERM problems and such dependence is optimal in DP-ERM. In addition, we believe the dependence on other parameters (\(H,S,A,d\)) in the lower order term is tight due to our special tricks as (3) and Lemma D.6.

Simulations

In this section, we carry out simulations to evaluate the performance of our DP-VAPVI (Algorithm 2), and compare it with its non-private counterpart VAPVI (Yin et al., 2022) and another pessimism-based algorithm PEVI (Jin et al., 2021) which does not have privacy guarantee.

**Experimental setting.** We evaluate DP-VAPVI (Algorithm 2) on a synthetic linear MDP example that originates from the linear MDP in (Min et al., 2021; Yin et al., 2022) but with some modifications.11 For details of the linear MDP setting, please refer to Appendix F. The two MDP instances we use both have horizon \(H=20\). We compare different algorithms in figure 1(a), while in figure 1(b), we compare our DP-VAPVI with different privacy budgets. When doing empirical evaluation, we do not split the data for DP-VAPVI or VAPVI and for DP-VAPVI, we run the simulation for \(5\) times and take the average performance.

**Results and discussions.** From Figure 1, we can observe that DP-VAPVI (Algorithm 2) performs slightly worse than its non-private version VAPVI (Yin et al., 2022). This is due to the fact that we add Gaussian noise to each count. However, as the size of dataset goes larger, the performance of DP-VAPVI will converge to that of VAPVI, which supports our theoretical conclusion that the cost of privacy only appears as lower order terms. For DP-VAPVI with larger privacy budget, the scale of noise will be smaller, thus the performance will be closer to VAPVI, as shown in figure 1(b). Furthermore, in most cases, DP-VAPVI still outperforms PEVI, which does not have privacy guarantee. This arises from our prioritization of variance-aware LSVI instead of LSVI.

## 7 Conclusion and future works

In this work, we take the first steps towards the well-motivated task of designing private offline RL algorithms. We propose algorithms for both tabular MDPs and linear MDPs, and show that they enjoy instance-dependent sub-optimality bounds while guaranteeing differential privacy (either zCDP or pure DP). Our results highlight that the cost of privacy only appears as lower order terms, thus become negligible as the number of samples goes large.

Future extensions are numerous. We believe the technique in our algorithms (privitization of Bernstein-type pessimism and variance-aware LSVI) and the corresponding analysis can be used in online settings too to obtain tighter regret bounds for private algorithms. For the offline RL problems, we plan to consider more general function approximations and differentially private (deep) offline RL which will bridge the gap between theory and practice in offline RL applications. Many techniques we developed could be adapted to these more general settings.

Figure 1: Comparison between performance of PEVI, VAPVI and DP-VAPVI (with different privacy budgets) under the linear MDP example described above. In each figure, y-axis represents sub-optimality gap \(v^{*}-v^{}\) while x-axis denotes the number of episodes \(K\). The horizons are fixed to be \(H=20\). The number of episodes takes value from \(5\) to \(1000\).