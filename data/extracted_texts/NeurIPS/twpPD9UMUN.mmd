# Look, Listen, and Answer: Overcoming

Biases for Audio-Visual Question Answering

Jie Ma\({}^{1}\)\({}^{*}\), Min Hu\({}^{1}\)\({}^{1,2}\), Pinghui Wang\({}^{1}\), Wangchun Sun\({}^{1}\), Lingyun Song\({}^{4}\),

**Hongbin Pei\({}^{1}\), Jun Liu\({}^{3}\), Youtian Du\({}^{1}\)**

\({}^{1}\) MOE KLINNS Lab, Xi'an Jiaotong University

\({}^{2}\) China Mobile System Integration Co.

\({}^{3}\) School of Computer Science and Technology, Xi'an Jiaotong University

\({}^{4}\) School of Computer Science, Northwestern Polytechnical University

\({}^{}\) Equal Contribution

\({}^{*}\) Corresponding Author

###### Abstract

Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, _MUSIC-AVQA-R_, crafted in two steps: rephrasing questions within the test split of a public dataset (_MUSIC-AVQA_) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32%. Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at https://github.com/reml-group/MUSIC-AVQA-R.

## 1 Introduction

Humans possess the extraordinary capacity to seamlessly integrate auditory and visual cues, effectively establishing a cohesive relationship between visual and auditory stimuli . Audio-Visual Question Answering (AVQA)  seeks to enable intelligent systems to acquire this capability and produce answers based on provided natural language questions. It requires the system to learn high-order interaction representations of the concepts encompassed with audio, video, and language modalities. As is known to us , the high-level reasoning ability of the system mainly relies on large-scale data that does not contain harmful biases or statistical regularities.

Nevertheless, completely avoiding the negative bias in datasets seems challenging. Previous studies  in visual and extractive QA have investigated the bias from the perspective of changing answer distributions and human-in-the-loop adversarial attacks. Drawing inspiration from these works,several open questions are proposed for the AVQA task, concerning model evaluations and model designs.

**Question 1: have existing datasets comprehensively measured model robustness?** The questions in the current AVQA dataset  are generated by a limited set of predefined templates, such as the 33 templates in the MUSIC-AVQA dataset . Fig. 1 shows the samples in the training and test split, which are produced using a predefined template. The observed difference mainly stems from a single word, leading to a limited vocabulary size of only 93 words. This has the potential to deviate from real-world scenarios. Moreover, current datasets cannot reflect the performance on rare or less common samples, which is an important indicator for evaluating model robustness .

**Question 2: have existing methods overcome the data bias?** We found that existing methods  such as STG  are brittle for the question with rare answers. This may be attributed to memorizing the statistical regularity between critical question words and answers, such as the connection between "Is", "Playing", and "Yes". Specifically, the experimental result  shows that STG achieves an accuracy of 54.09% on the test split of MUSIC-AVQA only given questions.

In this paper, we present the development of a novel dataset called MUSIC-AVQA-R, which aims to address the first question precisely. The dataset complements MUSIC-AVQA  and provides a more refined diagnostic for current AVQA methods. _To preserve the inherent bias, we maintain the original training and validation splits of the MUSIC-AVQA dataset._ In contrast, we employ a human-machine collaboration mechanism to rephrase the question in the test split. This ensures diverse and natural question forms while remarkably expanding the number of questions from **9,129** to **211,572**. We introduce a distribution shift based on answer distributions of specific question types. This allows us to measure performance on both frequent (in-distribution) and rare (out-of-distribution) data simultaneously.

To tackle the second question, we propose a robust framework that applies a _Multifaceted Cycle Collaborative Debiasing_ (MCCD) strategy. Specifically, the strategy introduces a novel optimization objective, which enlarges the distribution difference between uni-modal (question, audio, and video) and multi-modal logit. By doing so, our model becomes less prone to learning biases from individual modalities. Intuitively, we cannot choose the correct answer based on only one modality. Hence, MCCD employs cycle guidance to constrain the logit distribution of each modality, thereby promoting the similarity of uni-modal logit distribution. The experimental results demonstrate that our framework yields significant improvements on both datasets, with a particularly notable enhancement of 9.32% observed on the MUSIC-AVQA-R dataset.

Figure 1: The question in current AVQA datasets is generated by a limited set of predefined templates, which may not be in line with the real-world scenario. Our findings indicate that existing methods  such as STG  are not robust, which may be attributed to excessive bias learning, such as memorizing statistical regularities between critical question words and answers.

In summary, our contributions are fourfold: (1) We propose a novel dataset MUSIC-AVQA-R and a set of respective evaluation metrics. This enables us to thoroughly evaluate the reasoning behavior of AVQA models and characterize their generalization capabilities in in- and out-of-distribution scenarios. (2) We present an AVQA architecture that incorporates the MCCD strategy to overcome training biases. To the best of our knowledge, this is the first work to systematically explore biases in the AVQA task from model evaluations as well as model designs. (3) We conduct extensive experiments on MUSIC-AVQA and MUSIC-AVQA-R to verify the effectiveness and superiority of our proposed architecture and debiasing strategy. (4) We evaluate 13 recent multimodal QA methods on the proposed dataset and show their limited ability to generalize not only in-distribution scenarios but also in out-of-distribution situations.

## 2 Related Work

### Model Robustness Evaluation

Despite the notable achievements of QA datasets [19; 20; 21; 22; 3], they suffer from biases, resulting in incomplete evaluations. In recent years, numerous studies have tackled this issue from various perspectives [24; 25; 26; 27].

One avenue of research [12; 28; 29] reorganizes existing datasets, thereby making the distribution between training and testing splits significantly different or even reversed. The reorganized datasets reflect the performance in the out-of-distribution situation but lack measurement in the in-distribution scenario. To this end, GQA-OOD  introduces the distribution shift in both the validation and test splits to assess visual QA models in both scenarios simultaneously. Nevertheless, the number of questions in the GQA-OOD test split is only 2,796, which may not reflect the real generalization ability of visual QA models due to the presence of a limited number of testing samples . Inspired by the adversarial attack, another line of works [32; 33] regard the dataset construction as a game played by two parties: a human annotator and a well-trained model. Only samples generated by humans that successfully attack the model are incorporated into the dataset. In addition, there exists another line of work  that complements videos and questions to obtain balanced training data.

Different from the mentioned works, our dataset, MUSIC-AVQA-R, not only prioritizes question diversity but also considers the volume of test samples. This enhances the precision and comprehensiveness of robustness evaluation. Moreover, we recognize the formidable challenge of obtaining completely pure training data. As such, we opt to retain the inherent bias present in both the training and validation splits. Our primary objective is to inspire the community to enhance model robustness through the implementation of debiasing strategies, rather than striving for balanced training data. _Remarkably, to the best of our knowledge, our dataset is the first AVQA dataset explicitly designed for robustness evaluation._

### Bias Dependency Elimination

A variety of debiasing QA methods [34; 35; 36; 37] have been proposed to overcome bias memorization. These methods can be divided into four classes : ensemble learning, data augmentation, contrastive learning, and answer re-ranking.

Ensemble learning methods [38; 28; 39; 6; 40; 41] typically leverage a combination of a bias learner and a vanilla QA model to comprehensively predict answers. Data augmentation methods [42; 43; 44; 45] generate additional question-answer pairs to balance the data distribution. Based on the positive and negative sample generation, contrastive learning-based methods [46; 47; 48] strive to learn an embedding space where similar sample pairs are closely clustered while disparate ones are distinctly separated. Consequently, the vanilla QA method is optimized jointly through contrastive and QA losses. Answer re-ranking methods [34; 49; 50; 51; 52] primarily focus on reordering the answers predicted by the vanilla QA model to enhance context comprehension, such as vision grounding.

To the best of our knowledge, COCA  is the only work to mitigate the bias learning in the AVQA task, which first employs causal regularization to intervene bias-irrelevant causal effects and then introspects predictions. Unlike the mentioned works, which only consider language biases, our method considers audio, vision, language biases, and their collaboration. _The proposed MCCD strategy features plug-and-play capability, enhancing the debiasing potential of baseline methods._Dataset Creation and Analysis

We introduce the first dataset, MUSIC-AVQA-R, to evaluate the robustness of AVQA models. The construction of this dataset involves two key processes: rephrasing and splitting. The former involves the rephrasing of questions in the test split of MUSIC-AVQA, and the latter is dedicated to the categorization of questions into frequent (head) and rare (tail) subsets.

### Rephrasing

The questions within the existing dataset [5; 4] are formulated using a restricted collection of pre-defined templates. To augment diversity and reality, we employ a rephrasing tool1 to rephrase each question 25 times. To ensure the rephrasing quality, three annotators participate in a verification process where their consensus through voting is required. They are all senior students in the field of information science, with one specializing in computer science and the other two in automation. Their extensive professional background equips them with the ability to assess whether the above rephrasing fulfills the requirement. Rephrasings are incorporated into the dataset only when two or more individuals validate the quality of the modifications. According to the statistics, 92.4% of rephrasings pass this validation, and the Fleiss Kappa value used to measure vote consistency is 0.839. Please see details in Table 4 of Appendix B. These results strongly suggest an exceptionally high quality of the rephrasing efforts. Fig. 2 illustrates the distribution of rephrased questions based on their initial three words. We see that our rephrasing questions have various formats, and the comparison between the two datasets is shown in Fig. 6 of Appendix B. The vocabulary size of our dataset is **465**, which is **5x** larger than MUSIC-AVQA. These results indicate that our dataset is more in line with the real-world scenario. Furthermore, an expansion of the question count within the test split has been implemented, escalating from **9,129** to **211,572** questions. This substantial increase in the volume of test samples enhances the precision of evaluations for AVQA methods.

### Splitting

To provide a precise diagnostic for AVQA models, we introduce a distribution shift based on answer distributions of specific question types, following . Guided by this distribution, we categorize rephrased questions into _head_ and _tail_, enabling the assessment of in-distribution and out-of-distribution performance, respectively. We also utilize the _overall_ performance to assess the model effectiveness on the entire test split.

Specifically, to characterize the distribution shift, we first utilize the annotation for question types, including "Existential", "Location", "Counting", "Comparative", and "Temporal", to group questions. Fig. 3 illustrates the answer distribution of the "Temporal" questions within the AVQA task. We see that the answer presents a long-tailed distribution. The distribution of other types is given in Appendix B. It is essential to note that MUSIC-AVQA encompasses three tasks: audio QA, visual QA, and AVQA.

Next, we characterize the answer balance using Shannon entropy, expressed as \(H(A)=-_{i=1}^{N}p(a_{i}) p(a_{i})\), where \(H(A)\) is the entropy of an answer set \(A\) for a certain question type, \(N\) is the number of answer classes, and \(p(a_{i})\) represents the probability of answer class \(i\). It is important to note that the entropy depends on the number of answer classes, which exhibits significant variability across different question groups. To facilitate meaningful comparisons, we normalize \(H(A)\) of each group by \((N)\): \((A)=\), with \((N)\) representing the entropy of a uniform distribution of size \(N\). Refer to Appendix C for detailed proof. Thus, the normalized entropy \((A)\) indicates the proximity of the distribution \(H(A)\) to a

Figure 2: Distribution of rephrasing questions based on the first three words.

uniform distribution. We preserve the group with a normalized entropy below a threshold of 0.9, which aims at selecting imbalanced groups.

Finally, we categorize the samples into _head_ and _tail_ classes. We define the _tail_ class as class with following , where \(|a_{i}|\) represents the number of samples belonging to answer class \(i\), and \((a)\) denotes the average sample count for a group. Consequently, the _tail_ samples are rare, while the _head_ samples are more prevalent within a group. Fig. 3 illustrates the statistics of head and tail samples across various groups within each task.

## 4 Method

To mitigate bias learning, we propose a robust AVQA architecture that integrates a multifaceted cycle collaborative debiasing strategy. Fig. 4 illustrates an overview of our proposed architecture. It first learns the uni-modal and multimodal representations using a pre-trained model. Then, the architecture utilizes distinct bias learners to capture uni-modal biases. Finally, a collaborative debiasing strategy is leveraged to magnify the disparity between fusion logit and bias logit, obtained based on multimodality and uni-modality representations, respectively. Meanwhile, a cycle guidance mechanism is employed to maintain the similarity between bias logit. The aforementioned procedure is only carried out in the test split of MUSIC-AVQA. Consequently, our proposed dataset, MUSIC-AVQA-R, allows for a more precise and comprehensive evaluation of models handling data biases.

**Uni-modal Embedding.** Given an AVQA sample comprising a video sequence and a corresponding question, we initially partition the sequence, consisting of visual and audio tracks, into \(T\) non-overlapping pairs of visual and audio segments, denoted as \(\{V_{t},A_{t}\}_{t=1}^{T}\), where each segment spans one second. Subsequently, a distinct embedding layer is employed to acquire uni-modal embeddings. Specifically, we employ a pre-trained VGGish model  with fixed parameters, which is a VGG-like audio processing network, to obtain an audio embedding vector. For video embedding vectors, we employ a pre-trained ResNet-18 with fixed parameters on the frames. The VisualBert model  is applied to obtain a word-level question embedding vector. To ensure dimension matching, distinct linear layers are applied to the aforementioned vectors, resulting in uni-modal embeddings \(_{i}^{},_{i}^{}^{T 768}\) and \(_{i}^{}^{k 768}\), where \(l\) is the question length.

**Uni- and Multi-modal Representation.** We leverage VisualBert to obtain both uni-modal and multimodal representations, represented as \(_{i}^{},_{i}^{},_{i}^{ },_{i}^{}^{768}\). In the case of uni-modal learning, we exclusively input the aforementioned uni-modal embeddings to VisualBert. For multi-modal learning, we treat question embeddings as queries, concatenate video and audio embeddings as context, and leverage VisualBert to perform multi-modal interaction. Then, we apply a linear projection on the representation to obtain the multi-modality logit \(}_{i}^{}^{42}\), where 42 denotes the number of possible answers.

**Uni-modal Bias Learning.** AVQA may involve various harmful uni-modal biases, encompassing biases associated with audio, video, and language, respectively. To capture these uni-modal biases,

Figure 3: Statistics visualization for MUSIC-AVQA-R. \((a)\) is the average number of answers in a group. The dark color on the right denotes the number of head samples, while the light-colored area denotes that of tail samples.

we utilize a bias learner that takes only one of the three modalities as input. Specifically, distinct non-linear multi-layer perceptron layers serve as the learners, producing the corresponding logit \(}_{i}^{},}_{i}^{},}_{i}^{}^{42}\) on the answer space. It should be noted that these bias learners are removed during the testing stage.

**Collaborative Debiasing.** To eliminate bias learning, we propose a _multifaceted cycle collaborative debiasing_ (MCCD) strategy. It first reduces the bias impact from multiple views by enlarging the dissimilarity between uni-modal and multi-modal logit. This discrepancy enlargement \(_{}\) is implemented by the joint inverse distance:

\[_{}=_{i=1}^{K}(^ {}}+^{}}+^{}}),\] (1)

where \(K\) is the batch size, \(\) is used to balance optimization, \(d_{i}^{}\) denotes the Euclidean distance between audio logit and multi-modality logit, \(d_{i}^{}\) represents the distance between video logit and multi-modality logit, \(d_{i}^{}\) is the distance between question logit and multi-modality logit, and \(=1e-5\) is added to the denominator to avoid division by zero.

Intuitively, relying solely on one modality for answer prediction may result in similar logit distributions. Therefore, MCCD employs cycle guidance to constrain the distribution of uni-modal logit. This guidance \(_{}\) is implemented by the Kullback-Leibler divergence:

\[_{}=(_{}+ _{}+_{}),\] (2)

where \(\) is the factor to control weight, \(_{}=_{i=1}^{K}}_{i}^{ }(}_{i}^{}-}_{ i}^{})\) denotes the relative entropy between the question \(}_{i}^{}\) and audio logit \(}_{i}^{}\), \(_{}=_{i=1}^{K}}_{i}^{ }(}_{i}^{}-}_{ i}^{})\) is the relative entropy between the audio \(}_{i}^{}\) and video logit \(}_{i}^{}\), and \(_{}=_{i=1}^{K}}_{i}^{ }(}_{i}^{}-}_{ i}^{})\) represents the relative entropy between the video \(}_{i}^{}\) and question logit \(}_{i}^{}\).

Finally, we utilize the summation of \(_{}\), \(_{}\) and \(_{}\) to optimize the parameters of our method. \(_{}=-_{i=1}^{K}_{i}^{} }_{i}^{}\) is the loss of answer prediction that is regarded as a multi-classification problem, where \(_{i}^{},}_{i}^{}\) denote the one-hot answer label and logit of multi-modality fusion, respectively. The training details are shown in Appendix A.

## 5 Experiments

### Dataset and Evaluation

MUSIC-AVQA , which contains training, validation, and testing splits with 31,927, 4,568, and 9,129 QA pairs, is developed by gathering questions for 9,288 musical performances. The questions are produced by a limited set of pre-defined templates. The videos, sourced from YouTube, include solo performances, ensembles of the same instruments, and ensembles of different instruments. This dataset consists of three tasks: audio QA, visual QA, and AVQA. The standard accuracy is used to

Figure 4: Robust AVQA architecture to overcome bias learning. Our MCCD strategy is plug-and-play, allowing seamless integration with other AVQA methods.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

To validate the plug-and-play capability of MCCD, we conduct extensive experiments using the _baselines+MCCD_ on the aforementioned datasets. The results are presented in Tables 1 and 2. We observe that MCCD consistently improves performance across most methods on MUSIC-AVQA (9 out of 13) and MUSIC-AVQA-R (11 out of 13), respectively. This underscores the robust debiasing capability of MCCD in a plug-and-play manner.

### Ablation Study

To verify the debiasing effectiveness of MCCD, we conduct extensive experiments on both the test split of MUSIC-AVQA and MUSIC-AVQA-R. The results are shown in Table 3. Firstly, we validate the contribution of the component within multi-faceted debiasing. It can be seen that removing the component will lead to an overall performance improvement in some aspects of MUSIC-AVQA while resulting in a significant decrease in our dataset. This observation strongly supports the debiasing efficacy of these components. Secondly, we verify the overall contribution of the multifaceted debiasing. It can be seen that the performance decrease of 0.72% and 2.15% occurs in both datasets, respectively. Finally, we validate the contribution of cycle guidance. We see that this model variant obtains the best performance on the MUSIC-AVQA dataset. However, there was a noticeable performance degradation in our proposed dataset. In summary, each component plays a distinctive role in the debiasing process, which is further demonstrated by the performance degradation on the head and tail samples.

### Sensitivity and Qualitative Analysis

We employ the control variable method to perform a sensitivity analysis on the weight-controlling factors of the MCCD strategy. The results are presented in the left part of Fig. 5. Our findings indicate stable optimization across various settings, except for the case with \(=0.008\) and \(=0.3\). Upon conducting further experimental analysis, we identify the issue as originating from the model's failure to converge. Moreover, we visualize the attention weight on the uniformly sampled audio and video frames to qualitatively analyze the debiasing capability of our method. The visualization, displayed in the right part of Fig. 5, reveals that crucial audio and video frames for QA consistently receive significant attention, both in in- and out-of-distribution settings. This further demonstrates that our method predicts answers through the grounding capabilities of audio and vision rather than relying on bias learning. More cases are shown in Appendix D.5.

## 6 Conclusion and Limitation

We are the first to investigate bias learning in the AVQA task from model evaluation and design aspects. On the one hand, we construct a new dataset, MUSIC-AVQA-R, which evaluates the performance on the head, tail, and overall samples, providing a precise measure of model robustness. On the other hand, we introduce a robust architecture employing the MCCD strategy to mitigate bias learning. Extensive experiments demonstrate the effectiveness of our architecture and the plug-and-play debiasing capability of MCCD. Furthermore, we reevaluate previous multi-modal QA methods on our proposed dataset, revealing their poor robustness.

Due to constraints imposed by MUSIC-AVQA, the answer space of our dataset is limited, comprising only 42 classes, and answer lengths are typically confined to a single word. This deviation from real-world scenarios is noteworthy. Concerning model designs, for a fair comparison with baselines, we do not select large generative models to be backbones. However, compared with the answer classification, it may be more useful to generate answers for the AVQA task.

    &  &  \\   & **AQA** & **VQA** & **AVQA** & **AB** & **AQA** & **VQA** & **AVQA** & **H** & **T** & **All** \\  Ours & 79.14 & 78.24 & 67.15 & 72.20 & 74.76 & **77.26** & **63.43** & **72.24** & **59.39** & **66.59** \\ _w/o_ & **78.44** & **76.71** & 67.52 & 72.34 & 78.99 & 74.26 & 59.53 & 71.72 & 57.47 & 63.56 \\ _w/o_ & 79.22 & 77.86 & 67.23 & 72.37 & 71.26 & 70.18 & 58.08 & 68.34 & 57.43 & 63.85 \\ _w/o_ & 77.22 & 77.50 & 67.31 & 71.76 & 79.22 & 67.82 & 55.57 & 66.05 & 55.61 & 61.75 \\ w/o & 78.46 & 77.54 & 66.39 & 71.48 & 73.97 & 70.41 & 58.87 & 70.09 & 57.25 & 64.80 \\ w/o CG & 78.77 & **78.65** & **67.50** & **72.48** & **75.42** & 71.72 & 59.68 & 71.40 & 57.98 & 68.87 \\   

Table 3: Ablation results (%) on the test split of MUSIC-AVQA and our dataset. AQA and VQA denote audio QA, and visual QA, respectively. \(d_{i}^{(\#)}\) is the distance between the (#) logit and the multi-modality logit. MD: multifaceted debiasing. CG: cycle guidance.

## 7 Acknowledgements

This work was supported by the National Key Research and Development Program of China (2021YFB1715600), the National Natural Science Foundation of China (U22B2019, 62477037, 62450005, 62437002, 62306229, 62293553), the Natural Science Basic Research Program of Shaanxi (2023-JC-YB-593), the Youth Innovation Team of Shaanxi Universities "Multi-modal Data Mining and Fusion", the Shaanxi Undergraduate and Higher Education Teaching Reform Research Program (23BY195), the Youth Talent Support Program of Shaanxi Science and Technology Association (20240113), the Xi'an Jiaotong University-China Mobile Communications Group Co., Ltd. Digital Government Joint Institute, and the China Postdoctoral Science Foundation (2024M752585).