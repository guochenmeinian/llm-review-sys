# Neural Sculpting: Uncovering hierarchically modular task structure in neural networks through pruning and network analysis

Neural Sculpting: Uncovering hierarchically modular task structure in neural networks through pruning and network analysis

Shreyas Malakarjun Patil\({}^{1}\)  Loizos Michael\({}^{2,3}\)  Constantine Dovrolis\({}^{4,1}\)

\({}^{1}\)Georgia Institute of Technology, Atlanta, USA \({}^{2}\)Open University of Cyprus, Nicosia, Cyprus

\({}^{3}\)CYENS Center of Excellence, Nicosia, Cyprus \({}^{4}\)The Cyprus Institute, Nicosia, Cyprus

sm_patil@gatech.edu, loizos@ouc.ac.cy, c.dovrolis@cyi.ac.cy

###### Abstract

Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (_input-separability_) and they are reused as inputs higher in the hierarchy (_reusability_). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transfer. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an approach based on iterative unit and edge pruning (during training), combined with network analysis for module detection and hierarchy inference. Finally, we demonstrate that this method can uncover the hierarchical modularity of a wide range of Boolean functions and two vision tasks based on the MNIST digits dataset.

## 1 Introduction

Modular tasks typically consist of smaller sub-functions that operate on distinct input modalities, such as visual, auditory, or haptic inputs. Additionally, modular tasks are often hierarchical, with simpler sub-functions embedded in, or reused by, more complex functions . Consequently, hierarchical modularity is a key organizing principle studied in both engineering and biological systems. In neuroscience, the hierarchical modularity of the brain's neural circuits is believed to play a crucial role in its ability to process information efficiently and adaptively [2; 3; 4; 5]. Translating this hierarchical modularity to artificial neural networks (NNs) can potentially lead to more efficient, adaptable and interpretable learning systems. Prior works have already shown that modular NNs efficiently adapt to new tasks [6; 7; 8] and display superior generalization over standard NNs [9; 10; 11]. However, those studies assume knowledge of the task's hierarchy, or hand-design, modular NNs at initialization. Given an arbitrary task, however, we normally do not know its underlying sub-functions or their hierarchical organization. The high-level question in this work is: _If we learn a task using a sufficiently deep NN, how can we uncover the underlying hierarchical organization of sub-functions in that task?_

Recent studies through NN unit clustering have demonstrated that certain modular structures can emerge during the training of NNs [12; 13; 14; 15; 16]. However, it is unclear whether the structures extracted reflect the underlying hierarchy of sub-functions in a task. Csordas et al.  proposed a method that identifies sub-networks in NNs that learn specific sub-functions. This method however,requires knowledge of the exact sub-functions within a task's hierarchy. Ideally, modules corresponding to specific sub-functions should emerge through a training strategy, and a method should be available to detect these modules without explicit knowledge of the corresponding sub-functions.

Biological networks exhibit hierarchical modularity where clusters of nodes with relatively dense internal connectivity and sparse external connectivity learn specific sub-functions [4; 5; 18; 19]. Drawing inspiration from this, we propose _Neural Sculpting_, a novel approach to train and structurally organize NN units to reveal the underlying hierarchy of sub-functions in a task. Within a hierarchically modular task, we consider sub-functions that are input-separable and reused. We first show that conventionally trained NNs do not acquire structural properties that reflect the previous sub-function properties. To address this, we introduce a sequential unit and edge pruning method to train NNs. Unit pruning first conditions the NN to learn reused sub-functions, while edge pruning subsequently reveals the sparse connectivity between the learned sub-functions. Finally, we propose a network analysis tool to uncover modules and their hierarchical organization within the sparse NNs.

We demonstrate the capability of _Neural Sculpting_ to reveal the structure of diverse hierarchically modular Boolean tasks and two MNIST digits based tasks. To the best of our knowledge, this paper is the first to analyze specific sub-function properties within a hierarchically modular task and propose an end-to-end methodology to uncover its structure. This work also sheds light on the potential of pruning methods to uncover and harness structural properties in NNs.

### Preliminary

To represent the decomposition of a task, we visualize it as a graph. Initially, we consider Boolean functions and their corresponding function graphs. A Boolean function \(f:\{0,1\}^{n}\{0,1\}^{m}\), maps \(n\) input bits to \(m\) output bits. We define the set of gates \(G\) as \(\{,,\}\), where \(\) represents logical conjunction (AND), \(\) represents logical disjunction (OR), and \(\) represents the identity function (ID). Additionally, we define the set of edge-types \(E\) as \(\{,\}\), where \(\) represents a transfer edge and \(\) represents a negation edge.

**Function Graph:** A \((G,E)\)**-_graph_** representing a Boolean function \(f:\{0,1\}^{n}\{0,1\}^{m}\) is a directed acyclic graph comprising: \(n\) sequentially ordered vertices with zero in-degree, designated as the _input nodes_; \(k\) vertices with non-zero in-degree, designated as the _gate nodes_, with each vertex associated with a gate in \(G\) and each of its in-edges associated with an edge-type in \(E\); and \(m\) sequentially-ordered vertices with zero out-degree, designated as the _output nodes_.

We use the \(G\) and \(E\) defined above, as NN units have been demonstrated to learn these universal gates . Function graphs, which break down complex computations into simpler ones, are typically sparse. Modularity in sparse graphs refers to the structural organization of subsets of nodes that exhibit strong internal and weak external connectivity, which are grouped into modules.

**Sub-function:** A _sub-function_ is a subset of nodes within the function graph that collectively perform a specific task. The sub-function is characterized by having strong internal connectivity within its nodes, meaning that they are highly interdependent and work together to achieve the desired output. At the same time, nodes in the sub-function have weak external connectivity, indicating that they are relatively independent from the rest of the graph.

**Input Separable:** Two sub-functions are considered _input separable_ if their in-edges originate from distinct and non-overlapping subsets of nodes in the function graph.

**Reused:** A sub-function is _reused_ if it has two or more outgoing edges in the function graph. 1

**Training NNs on target Boolean functions:** We start by obtaining the truth table of each function graph, which serves as our data source. The training set \((}_{t},}_{t})\) includes the complete truth table with added random noise (\((0,0.1)\)) during each iteration to increase the number of training samples. The validation set \((}_{v},}_{v})\) consists of the noise-free rows of the truth table. We use multi-layered perceptrons (MLPs) with ReLU activation for the hidden units and Kaiming weight initialization to learn the Boolean functions. The loss function is bitwise cross-entropy with Sigmoid activation. The NNs are trained using Adam optimizer with L2 regularization of \(1e-4\).

## 2 Standard training of NNs is not enough

We show that NNs through standard training, do not acquire structural properties that reflect the properties of sub-functions. Specifically, we consider the two properties in isolation by constructing two graphs: one with input separable sub-functions, and the other with a reused sub-function.

### Input Separable

The first function graph we consider has 4 input nodes and 4 output nodes. The output nodes \(\{y_{1},y_{2}\}\) depend only on \(\{x_{1},x_{2}\}\), while the output nodes \(\{y_{3},y_{4}\}\) depend only on \(\{x_{3},x_{4}\}\) (Figure 1a). We train various NN architectures to learn the target function with perfect accuracy on the validation set. The property of the function graph that reflects input-separable sub-functions is that there are no paths from input nodes \(\{x_{1},x_{2}\}\) to output nodes \(\{y_{3},y_{4}\}\) and from input nodes \(\{x_{3},x_{4}\}\) to output nodes \(\{y_{1},y_{2}\}\). However, in a neural network, all input units are connected to all output units through the same number of paths. Therefore, we analyze the strength of their learned relationship by considering the product of weight magnitudes along those paths.

Consider a trained NN parametrized by \(^{a}\) and the set of all paths, \(}\). Each edge weight \(_{i}\) is assigned a binary variable \(p_{i}\) to indicate whether it belongs to a given path \(p}\). We define the edge-weight product of a path as \(_{p}=_{i=1}^{n}|_{i}^{p_{i}}|\), and \((i,j)=_{p}_{i j}}_{p}\) represents the sum of \(_{p}\) for all paths from input unit \(i\) to output unit \(j\). We evaluate \((i,j)\) for \(i=1,2\) and \(j=3,4\), and \((i,j)\) for \(i=1,2\) and \(j=1,2\). If the former is significantly smaller than the latter, then the input units 1 and 2 are not used by output units \(3\) and \(4\).

We perform a two-sample mean test with unknown standard deviations, with \((_{1},s_{1})\) representing the mean and sample standard deviation of \((i,j)\) for \(i=1,2\) and \(j=3,4\), and \((_{2},s_{2})\) representing the mean and sample standard deviation of \((i,j)\) for \(i=1,2\) and \(j=1,2\). The null hypothesis \(H_{0}\) is \(_{1}=_{2}\), and the alternative hypothesis is \(_{1}<_{2}\). A similar test is performed for input units \(3\) and \(4\). Figure 1b shows an example heat map for \((i,j)\), where \(i,j\{1,2,3,4\}\). For the example shown we cannot reject the null hypotheses. Similarly, we conducted statistical tests on nine different NNs with varying architectures and seed values, and the results are summarized in Table 1. For NNs with a single hidden layer, we can reject the null hypotheses, but for deeper NNs, both the null hypotheses cannot be rejected.

### Reused

Consider the function graph shown in Figure 2a, consisting of 4 input nodes and 16 output nodes. First, we construct an intermediate sub-function \(g(X)\) such that the three gate nodes depend on all the inputs. This sub-function is utilized by two separate gates, \(f_{1}(g)\) and \(f_{2}(g)\), which are then used 8 times by different output nodes. All paths to the output nodes pass through three gate nodes in the first hierarchical level and two gate nodes in the second. We analyze the edge-weight product of the paths from hidden units to output units. Let \(_{p}^{l}\) denote the sum of \(_{p}\) for all paths originating from hidden layer \(l\), which contains \(N_{l}\) hidden units. We compute

  Width & 24 & 36 & 48 \\  Hidden Layers & & & \\ 
1 & & & \\ 
2 & & & \\ 
3 & & & \\  

Table 1: NN architectures with varying widths and depths for which both null hypotheses are rejected.

Figure 1: a. Function graph with input separable sub-function, b. Edge-weight product of paths from input units to output units in a trained NN.

Figure 2: a. Function graph with reused sub-function, b. the number of units covering 90% of the total edge-weight product of paths.

the minimum number of units, \(N_{P}^{l}\), that are necessary to achieve \(P\%\) of the total \(_{p}\). By comparing \(N_{P}^{l}\) to that of the function graph, we can conclude whether the NN has learned the reused states.

We independently trained NNs with two hidden layers and different widths on the target Boolean function. The resulting \(N_{90}^{l}\) values for different layers are shown in Figure 2b. Our analysis indicates that, as the width of the NN increases, so does \(N_{90}^{l}\) in both the hidden layers, and these values are consistently close to the actual width of the NN. We trained an NN with hidden layers of width 3 and 2, respectively, to confirm that NNs with those widths can learn the function well.

## 3 Iterative pruning of NNs

In the previous section, we demonstrated that standard NN training is inadequate for learning structural properties that reveal input separable or reused sub-functions. Nevertheless, we observed that NNs with a relatively low number of parameters could still acquire these properties. Since hierarchical modularity is a property of sparse networks, we explored NN pruning algorithms as a means of achieving this sparsity. Prior research has shown that pruning NNs can reduce their number of parameters without compromising their performance [21; 22; 23]. Edge pruning methods typically require specifying a target NN density or pruning ratio [21; 24], but determining a density value that preserves the NN's performance remains an open question. Recently, iterative magnitude pruning [25; 26] has emerged as a natural solution to this problem. The algorithm prunes edges iteratively, removing some edges in each iteration and then retraining the NN. This process can be repeated until the NN achieves the same validation performance as the dense NN while being as sparse as possible.

### Iterative edge pruning

Consider the initial edge pruning step \(p_{e}\). We train the dense NN and prune \(p_{e}\%\) of the edges with the lowest weight magnitude. The sparse NN resulting from this pruning is then trained with the same number of epochs and learning rate schedule as the original NN. We repeat this process until the sparse NN can no longer achieve the same validation accuracy as the dense NN.

At this point, we rewind the NN to the previous sparse NN that achieved the same validation accuracy as the dense NN and update the value of \(p_{e}\) as \(p_{e}=p_{e}/2\). We repeat this process until \(p_{e}\) becomes lower than the required step to prune a single edge, thereby ensuring that the lowest possible density is achieved.

We next apply the tests developed in the previous section to analyze whether edge-pruned NNs acquire structural properties resembling those of the sub-functions. Specifically, we train and prune different NN architectures to learn the function graph with input-separable sub-functions. For the example shown in Figure 3a, as well as for NNs with different architectures, we can reject the null hypotheses in favor of the alternate hypotheses. This implies that edge-pruned NNs acquire structural properties that enable them to recognize input-separable sub-functions.

Consider the previous function graph that has reused sub-functions and NNs with increasing widths. We independently train and prune each NN architecture and determine the number of units \(N_{90}^{l}\) (Figure 3b). We find that in a majority of the trials, \(N_{90}^{2}=2\), which are reused by the output units. Although there is a significant reduction in \(N_{90}^{1}\), the edge-pruned NNs fail to identify 3 units that are being reused. We observe that edge-pruned NNs identify sparse connectivity between units and reuse those units. However, they do not identify units corresponding to sub-functions that are not sparsely connected yet reused. We hypothesize that this may be due to the initial pruning iterations where some edges from all units are removed, leaving no hidden units with dense connectivity to learn densely connected and reused sub-functions. Additionally, this could also be due to very large NN widths and the absence of an objective conditioning NNs to utilize as few hidden units as possible. Therefore, next we introduce an iterative unit pruning method to limit the number of units used.

Figure 3: a. Edge-weight product of paths from input units to output units in edge-pruned NN; b. the number of units covering 90% of the total edge-weight product of paths in edge-pruned NNs.

### Iterative hidden unit pruning

To prune hidden units, we first train the neural network and then assign each hidden unit a score. We eliminate \(p_{u}\%\) of the hidden units with the lowest scores and train the resulting pruned network for the same number of epochs and learning rate schedule as the original network. We repeat this two-step pruning process iteratively.

We use loss sensitivity as the scoring metric for the units [27; 28; 29; 30], which approximates the change in the network's loss when the activation of a particular hidden unit is set to zero. Specifically, for a unit \(i\) in hidden layer \(l\), with activation \(a_{i}^{l}\), the score is computed as :

\[S_{i}^{l}=|(_{v}},)}{  a_{i}^{l}} a_{i}^{l}|\] (1)

The iterative unit pruning process continues until the NN can no longer achieve the same validation accuracy as the original NN. To ensure that we have pruned as many units as possible, we revert to the latest NN that achieved the same validation accuracy as the original NN and halve the value of \(p_{u}\). This process is repeated until the unit pruning step becomes lower than the step size needed to prune a single unit. We perform unit pruning before the edge pruning process to identify the minimum widths required in each hidden layer. Once the minimum widths are determined, edges are pruned to reveal the sparse connectivity between those units. For additional details please refer to appendix section H.

We conducted the previous tests to analyze whether the unit-edge pruned NNs acquire structural properties resembling those of the sub-functions, as presented in the previous section. The results of these tests are shown in Figure 4, and show that the unit-edge pruned NNs do acquire structural properties resembling both input separable sub-functions and reused sub-functions.

## 4 Detecting modules within sparse NNs

In this section, we propose a method to uncover sub-networks or modules that approximate the sub-functions within the target function. We approach this by projecting the problem as a two-step partitioning problem due to the layered structure of NNs. First, we cluster the units belonging to the same layer. We assume that given a layer, there exist various subsets of units that participate in learning the same sub-function. Next, we merge the previous unit clusters across layers such that they exhibit strong connectivity. The overview of our proposed method is illustrated in Figure 5.

**Layer-wise unit clustering:** Let us consider a single layer \(l\) with \(N_{l}\) units. For each unit, we construct a feature vector based on its outgoing connectivity. The feature vector for a unit \(i\) is a binary vector \(f_{i}^{l}\{0,1\}^{g}\), where \(g\) is the total number of units in all the later layers. If unit \(j\) is connected to unit \(i\) through at least one directed path, then \(f_{i}^{l}(j)=1\), otherwise \(f_{i}^{l}(j)=0\). Our hypothesis is that the units that participate in learning the same sub-function are reused by similar units in the later layers. To partition the units into clusters such that their feature vectors have low intra-cluster and high inter-cluster distances, we use the Agglomerative clustering method with cosine distance and

Figure 4: a. Edge-weight product of paths from input units to output units in unit-edge pruned NN; b. the number of units covering 90% of the total edge-weight product of paths in unit-edge pruned NNs.

Figure 5: Proposed module detection pipeline.

average linkage. To identify the optimal number of clusters \(K_{l}\), we use the modularity metric, which we have modified for our problem .

Let \(\) be the normalized distance matrix of the unit feature vectors, where the sum of all elements is equal to \(1\). Consider the units partitioned into \(k\) clusters and a matrix \(A^{k k}\), where \(A_{ij}=_{a C_{i},b C_{j}}_{ab}\) represents the sum of the distances between all pairs of units in clusters \(C_{i}\) and \(C_{j}\). The modularity metric, denoted by \(M\), measures the quality of the partitioning and is defined as:

The first term in this equation measures the total intra-cluster distance between the unit features while the second term is the expected intra-cluster distance under the null hypothesis that unit distances were randomly assigned based on joint probabilities in \(\). A negative value of \(M\) indicates that pair-wise distance within a cluster is lower than the random baseline. We iterate through values of \(k\) ranging from \(2\) to \(N_{l}-1\), which are obtained through Agglomerative clustering, and select the value of \(k\) that minimizes the modularity metric.

The modularity metric can accurately detect the presence of multiple unit clusters (\(K_{l}=2,...,N_{l}-1\)). However, it fails for the edge cases where all units may belong to a single cluster (\(K_{l}=1\)) or to separate individual clusters (\(K_{l}=N_{l}\)). To address those, we conduct a separability test if the modularity metric is lowest for \(k=2\) or \(k=N_{l}-1\), or if the modularity metric values are close to zero. Modularity metric close to zero is an indicator that the intra-cluster distances are not significantly different from the random baseline. To determine this, we set a threshold on the lowest modularity value obtained.

**Separability test:** The unit separability test is designed to evaluate whether two units in a cluster can be separated into sub-clusters. Consider two units \(i\) and \(j\), with \(o_{i}= f_{i}^{l}\), \(o_{j}= f_{j}^{l}\) neighbors respectively, and \(o_{ij}=f_{i}^{l} f_{j}^{l}\) common neighbors. We consider a random baseline that preserves \(o_{i}\) and \(o_{j}\). The number of common neighbors is modeled as a binomial random variable with \(g\) trials and probability of success \(p= o_{j}}{g^{2}}\). The units are separable if the observed value of \(o_{ij}\) is less than the expected value \((o_{ij})\) under the random model.

Consider the partition where \(N_{l}-1\) clusters are obtained. If the two units that are found in the same cluster are separable, it implies that all units belong to separate clusters. Now let us consider the partition of units into two clusters. We merge the feature vectors of the two unit groups. If the two groups of units are not separable, it implies that all units must belong to the same cluster. In some cases, both tests yield positive results. We determine the optimal number of clusters by selecting the result that is more statistically significant.

**Merging clusters across layers:** Strongly connected clusters from adjacent layers are next merged to uncover multi-layered modules. Consider, \(C_{l}^{i},i=1,2,...,K_{l}\) to be the clusters identified at layer \(l\). Let \(e_{i,j}^{l}\) be the number of edges from cluster \(C_{l}^{l}\) to cluster \(C_{l+1}^{j}\). The two clusters are merged if : \(^{l}}{_{j=1}^{K_{l+1}}e_{i,j}^{l}}_{m}\) and \(^{l}}{_{i=1}^{K_{l}}e_{i,j}^{l}}_{m}\), where \(_{m}\) is the merging threshold.

The output units are merged with the previous layer's modules, ensuring that \(_{m}\) fraction of incoming edges to the unit are from that module. This allows multiple output units to be matched to the same structural module.

Figure 6: Function graphs used to validate the proposed pipeline

Experiments and Results

### Modular and hierarchical Boolean function graphs

In this section, we conduct experiments on Boolean function graphs with different sub-function properties to validate our pipeline. We begin by testing the pipeline on four function graphs shown in Figure 6. These graphs include: 1) input separable sub-functions, 2) a reused sub-function, 3) sub-functions that are both input separable and reused, and 4) a function graph without any such sub-function where all nodes are strongly connected to every other node.

We perform 36 trials for each function graph by training neural networks with combinations of 3 width values, 3 depth values, and 4 seed values. A trial is considered successful if the proposed pipeline detects a module corresponding to an input separable or reused sub-function (Figure 5). For Boolean functions, we set the modularity metric threshold to -0.2 and the cluster merging threshold to 0.9. Figure 7 shows the success rates for each function graph and NNs with different depths. We observed that the proposed pruning and module detection pipeline has a high success rate when the depth of the NN exceeds that of the function graph. (see to appendix section B for NN visualizations)

### Sub-functions that are reused many times are uncovered more accurately

We consider the function graph shown in Figure 5(b), which contains a single reused sub-function. We vary the number of times the two intermediate gate nodes in the second hierarchical level are used by decreasing the number of output nodes and measure the success rate. The results are shown in Figure 8, where we observe an increasing trend in the success rate as the number of output nodes using the two gate nodes increases. As the number of output units using the two gate nodes decreases, learning the two gate nodes using the previous "dense" sub-function may not be efficient. We provide visualizations of the corresponding NNs in the appendix section C. Note that NNs with only one hidden layer recover a single module, as the function graphs require four hierarchical levels. If only three hierarchical levels are available, the function graph collapses to a dense graph.

### Sub-functions with higher input separability are detected more accurately

In this experiment, we consider a function graph with two input separable sub-functions shown in Figure 8(a). We increase the overlap between the two separable input sets by decreasing the total number of input nodes and reusing them. We also vary the number of times each sub-function is replicated or used in the output nodes. Our goal is to uncover three properties of the structure: accurately separated input units into sub-function specific and reused, two output sub-functions accurately detected in later layers, and all hidden units belong to either of the two output modules. The success rate for each of these properties as a function of input overlap and sub-function use is shown in Figure 10.

Figure 8: Success rate when the number of times a sub-function is used increases

Figure 7: Success rates for the validation function graphs

Figure 9: Increasing the input overlap (reuse) between two input separable sub-functions

We observe that our method has a high success rate for detecting these properties for sub-functions with high input separability. However, as the overlap between input node sets increases, the success rate for detecting input modules decreases. Furthermore, the success rate decreases as the number of times a sub-function is used decreases. We also observe that for sub-functions with low input separability (and high input overlap), intermediate units are often clustered into a single module due to the hidden units pruning step. Finally, we find that the same trend is observed for detecting output sub-functions, where increasing input overlap and decreasing sub-function use results in a low success rate for our method. (See appendix section D for visualization of these results).

### Hierarchical structures uncovered vary depending on NN depth

The function graph in Figure 10(a) consists of two input separable sub-functions, the output of those sub-functions is then used by a single intermediate sub-function. This intermediate sub-function is then reused by two additional sub-functions to produce the final output. Interestingly, we observe that the proposed pipeline uncovers different hierarchical structures depending on the depth of the NN.

Figure 11(a) shows the success rate of uncovering the specific sub-functions segregated by the NN depth. We find that NNs with depth greater than or equal to the number of hierarchical levels (5) can uncover all three types of sub-functions in the function graph. However, for NNs with lower depth, only the input separable sub-functions are uncovered, while the intermediate and output sub-functions are merged into a single module.

This result highlights the importance of selecting an appropriate depth for the NN architecture to effectively uncover hierarchical structures in Boolean function graphs. (see appendix section E for NN visualizations) In addition, we report the success rates of uncovering the exact hierarchical structure that corresponds to the depth of the NN as we vary the number of times the output sub-functions

Figure 11: The same Boolean function represented by different function graphs depending on the number of hierarchical levels

Figure 12: Success rates: a. uncovering specific sub-functions. b. uncovering the overall hierarchical structure, for NNs with varying depths trained on function graph in Figure 11

Figure 10: Success rates for various properties uncovered by the pipeline when the input overlap between two sub-functions is increased

were used (Figure 12b). The findings demonstrate an increasing trend in the success rate, which is consistent with our previous results.

### Modular and hierarchical functions with MNIST digits

In this section, we present an experimental evaluation of hierarchically modular tasks constructed using the MNIST handwritten digits dataset. We set the modularity metric threshold to -0.3 and the cluster merging threshold to 0.6 for tasks with MNIST digits.

**Classifying two MNIST digits:** We begin by considering a simple function constructed to classify two MNIST digits. The NN is presented with two images concatenated together, and the output is a vector of length 20. The first 10 indices of the output predict the class for the first image, and the remaining 10 predict the class for the second image. We construct the dataset such that each unique combination of labels has 1000 input data points generated by randomly selecting images. We split the data into training and validation sets with an 8:2 ratio, and then train nine neural networks with varying widths (392, 784, 1568) and number of hidden layers (2, 3, 4) to learn the function for four different seed values. We use the Adam optimizer with bit-wise cross-entropy as the loss function, and select \(99\%\) as the accuracy threshold for the pruning algorithm, as all dense NNs achieve at least \(99\%\) validation accuracy. Out of the 36 trials conducted, we obtain two completely separable modules for 33 out of 36 trials, indicating the high effectiveness of our approach. (see appendix F for NN visualizations)

**Hierarchical and Modular MNIST Task:** We consider next a hierarchical and modular task that uses the MNIST digits dataset (Figure 13a). The task takes two MNIST images as input, each belonging to the first 8 classes (digits 0-7). The digit values are represented as 3-bit binary vectors and used to construct three output Boolean sub-functions. To generate the dataset, we randomly select 1000 input data points for each unique combination of digits. The data is then split into training and validation sets with an 8:2 ratio. All the NNs that we experiment with reach a validation accuracy of \(98\%\), which we use as the accuracy threshold for the pruning algorithm.

To analyze the modular structure uncovered by our methodology, we divide the success rates into three categories: (1) detecting two input separable modules, (2) detecting three output modules, and (3) middle layer unit-separability into either the input separable modules or the output separable modules. We observe that the NNs uncover the three output modules with high success rates. However, for NNs with lower depths, the NNs fail to recover the two input separable modules, and all the units in the early layers are clustered into a single module. As the depth of the NN increases, we find that the input separable modules are recovered with high accuracy as well. We observe that the success rates for middle layer unit-separability are very low, even when the depth of the NN increases. The units belonging to those hidden layers are often clustered into a single module. These units may be learning representations required to approximate the two digits well. This finding indicates that the NN depths we experiment with may not be sufficient to capture the underlying function, despite the NNs learning the task with high validation accuracy. Please refer to appendix F for NN visualizations.

## 6 Related work

**Emerging modular structures** in trained NNs has been a subject of prior investigation, primarily through unit clustering algorithms. Previous methods for extracting such structures can be categorized

Figure 13: a. Hierarchically modular task constructed with MNIST digits; b. Success rates for various modules recovered by NNs with varying depths.

as either _structural_ or _functional_. Structural methods organize units based on their structural characteristics, including connectivity and edge-weights [12; 13; 14], while functional methods consider attributes based on unit activation patterns [15; 16]. However, it remains unclear whether these extracted structures represent the underlying hierarchy of sub-functions in a given task.

To the best of our knowledge, this work is the first attempt to introduce an approach that combines training (pruning) and network analysis to unveil the hierarchical modularity of tasks. Our proposed method for clustering units and discovering modules aligns with the structural methods. A series of prior studies leveraged normalized spectral clustering to globally extract unit clusters and analyze their characteristics [14; 15]. Spectral clustering optimizes for N-cuts, quantifying the internal connectivity relative to the external connectivity of unit clusters. Our method is most similar to previously introduced layer-wise unit clustering techniques [12; 13]. These methods consider incoming and outgoing edge-weights to group units within a layer and consolidate edges into single incoming and outgoing connections. Importantly, all previous methods were tailored for conventionally trained NNs. In contrast, our approach is simpler and customized for the pruned NNs we obtain.

**Pruning** of NN units and edges has gained significant traction as a method to enhance NN computational efficiency and reduce memory demands while maintaining performance integrity [21; 24; 23; 34; 35; 30; 36; 37; 38]. Recent research has also focused on investigating how pruning impacts NN generalization [39; 40; 41]. However, prior studies have primarily concentrated on either unit pruning or edge pruning in isolation, without employing both in a sequential manner as proposed in our work. In our approach, we employ unit pruning as a means to condition or compel NNs to learn reused sub-functions effectively. Furthermore, edge pruning is employed to uncover the sparse connectivity between various units or modules.

**Mechanistic interpretability** seeks to reverse-engineer trained NNs to understand their internal mechanisms. One avenue of research within mechanistic interpretability involves identifying the circuits formed within NNs. Recent discoveries include curve-detecting circuits in vision models [42; 43], transformer circuits , induction heads , and indirect object identification , among others. Our proposed approach for uncovering the hierarchical modular task structure dissects the NN into modules responsible for learning specific sub-functions. Hence, simplifying the complexity of reverse engineering entire NNs, by focusing instead on smaller sub-networks.

**Continual learning** aims at learning multiple tasks presented sequentially while preserving performance on previously learned tasks [47; 48; 49; 50]. Modular NNs have been demonstrated to mitigate catastrophic forgetting by freezing and reusing certain modules while introducing and updating others to learn new tasks [6; 7; 8]. However, those methods entail manually designing modular NNs with fixed module sizes at initialization. NNs that naturally acquire a hierarchically modular structure mirroring that of the task may offer additional advantages compared to generic modular NNs. These potential benefits encompass enhanced generalization, efficient transfer learning through the reuse of modules corresponding to frequently utilized sub-functions, and informed module additions to prevent sub-linear increases in NN capacity.

## 7 Conclusion

We have introduced _Neural Sculpting_, a methodology to uncover the hierarchical and modular structure of target tasks in NNs. Our findings first demonstrated that NNs trained conventionally do not naturally acquire the desired structural properties related to input separable and reused sub-functions. To address this limitation, we proposed a training strategy based on iterative pruning of units and edges, resulting in sparse NNs with those previous structural properties. Building upon this, we introduced an unsupervised method to detect modules corresponding to various sub-functions while also uncovering their hierarchical organization. Finally, we validated our proposed methodology by using it to uncover the underlying structure of a diverse set of modular and hierarchical tasks.

As a future research direction, we could investigate the efficiency and theoretical underpinnings of modularity in function graphs, which could further motivate pruning NNs. One potential approach to overcome the computational costs and dependence on initial architecture depth could be to use neural architecture search algorithms to construct modular NNs during training. Additionally, exploring the use of attention mechanisms and transformers to uncover hierarchical modularity in tasks could be an interesting direction for future work. These approaches could provide a more efficient way of obtaining modular NNs that can also better capture the underlying structure of the target task.