# GMSF: Global Matching Scene Flow

Yushan Zhang   Johan Edstedt   Bastian Wandt

Per-Erik Forssen   Maria Magnusson   Michael Felsberg

Linkoping University

{firstname.lastname}@liu.se

###### Abstract

We tackle the task of scene flow estimation from point clouds. Given a source and a target point cloud, the objective is to estimate a translation from each point in the source point cloud to the target, resulting in a 3D motion vector field. Previous dominant scene flow estimation methods require complicated coarse-to-fine or recurrent architectures as a multi-stage refinement. In contrast, we propose a significantly simpler single-scale one-shot global matching to address the problem. Our key finding is that reliable feature similarity between point pairs is essential and sufficient to estimate accurate scene flow. We thus propose to decompose the feature extraction step via a hybrid local-global-cross transformer architecture which is crucial to accurate and robust feature representations. Extensive experiments show that the proposed Global Matching Scene Flow (GMSF) sets a new state-of-the-art on multiple scene flow estimation benchmarks. On FlyingThings3D, with the presence of occlusion points, GMSF reduces the outlier percentage from the previous best performance of 27.4% to 5.6%. On KITTI Scene Flow, without any fine-tuning, our proposed method shows state-of-the-art performance. On the Waymo-Open dataset, the proposed method outperforms previous methods by a large margin. The code is available at [https://github.com/ZhangYushan3/GMSF](https://github.com/ZhangYushan3/GMSF).

## 1 Introduction

Scene flow estimation is a popular computer vision problem with many applications in autonomous driving  and robotics . With the development of optical flow estimation and the emergence of numerous end-to-end trainable models in recent years, scene flow estimation, as a close research area to optical flow estimation, takes advantage of the rapid growth. As a result, many end-to-end trainable models have been developed for scene flow estimation using optical flow architectures . Moreover, with the growing popularity of Light Detection and Ranging (LiDAR), the interest has shifted to computing scene flow from point clouds instead of stereo image sequences. In this work, we focus on estimating scene flow from 3D point clouds.

One of the challenges faced in scene flow estimation is fast movement. Previous methods usually employ a complicated multi-stage refinement with either a coarse-to-fine architecture  or a recurrent architecture  to address the problem. We instead propose to solve scene flow estimation by a single-scale one-shot global matching method, that is able to capture arbitrary correspondence, thus, handling fast movements. Occlusion is yet another challenge faced in scene flow estimation. We take inspiration from an optical flow estimation method  to enforce smoothness consistency during the matching process.

The proposed method consists of two stages: feature extraction and matching. A detailed description is given in Section 3. To extract high-quality features, we take inspiration from the recently dominant transformers  and propose a hybrid local-global-cross transformer architecture to learn accurate and robust feature representations. Both local and global-cross transformers are crucial for our approach as also shown experimentally in Section 4.5. The global matching process, includingestimation and refinement, is guided solely by feature similarity matrices. First, scene flow is calculated as a weighted average of translation vectors from each source point to all target points under the guidance of a cross-feature similarity matrix. Since the matching is done in a global manner, it can capture short-distance correspondences as well as long-distance correspondences and, therefore, is capable of dealing with fast movements. Further refinement is done under the guidance of a self-feature similarity matrix to ensure scene flow smoothness in areas with similar features. This allows to propagate the estimated scene flow from non-occluded areas to occluded areas, thus solving the problem of occlusions.

To summarize, our contributions are: (1) A hybrid local-global-cross transformer architecture is introduced to learn accurate and robust feature representations of 3D point clouds. (2) Based on the similarity of the hybrid features, we propose to use a global matching process to solve the scene flow estimation. (3) Extensive experiments on popular datasets show that the proposed method outperforms previous scene flow methods by a large margin on FlyingThings3D  and Waymo-Open  and achieves state-of-the-art generalization ability on KITTI Scene Flow .

## 2 Related Work

### Scene Flow

Scene flow estimation  has developed quickly since the introduction of the KITTI Scene Flow  and FlyingThings3D  benchmarks, which were the first benchmarks for estimating scene flow from stereo videos. Many scene flow methods [1; 29; 31; 37; 40; 48; 58] assume that the objects in a scene are rigid and decompose the estimation task into subtasks. These subtasks often involve first detecting or segmenting objects in the scene and then fitting motion models for each object. In autonomous driving scenes, these methods are often effective, as such scenes typically involve static backgrounds and moving vehicles. However, they are not capable of handling more general scenes that include deformable objects. Moreover, the subtasks introduce non-differentiable components, making end-to-end training impossible without instance-level supervision.

Recent work in scene flow estimation mostly takes inspiration from the related task of optical flow [9; 16; 41; 45] and can be divided into several categories: encoder-decoder methods [14; 27] that solve the scene flow by an hourglass architecture neural network, multi-scale methods [3; 20; 55] that estimate the motion from coarse to fine scales, or recurrent methods [17; 46; 53] that iteratively refine the estimated motion. Other approaches [19; 34] try to solve the problem by finding soft correspondences on point pairs within a small region. In order to reduce the annotation requirement, some methods focus on runtime optimization [22; 18], prior assumptions , or even without the need for training data .

Encoder-decoder Methods:Flownet  and Flownet2.0 , were the first methods to learn optical flow end-to-end with an hourglass-like model, and inspired many later methods. Flownet3D  first employs a set of convolutional layers to extract coarse features. A flow embedding layer is introduced to associate points based on their spatial localities and geometrical similarities on a coarse scale. A set of upscaling convolutional layers is then introduced to upsample the flow to the high resolution. FlowNet3D++  further incorporates point-to-plane distance and angular distance as additional geometry constraints to Flownet3D . HPLFlowNet  employs Bilateral Convolutional Layers (BCL) to restore structural information from unstructured point clouds. Following the hourglass-like model, DownBCL, UpBCL, and CorrBCL operations are proposed to restore information from each point cloud and fuse information from both point clouds.

Coarse-to-fine Methods:PointPWC-Net  is a coarse-to-fine method for scene flow estimation using hierarchical feature extraction and warping, which is based on the optical flow method PWC-Net . A novel learnable Cost Volume Layer is introduced to aggregate costs in a patch-to-patch manner. Additional self-supervised losses are introduced to train the model without ground-truth labels. Bi-PointFlowNet  follows the coarse-to-fine scheme and introduces bidirectional flow embedding layers to learn features along both forward and backward directions. Based on previous methods [27; 55], HCRF-Flow  introduces a high-order conditional random fields (CRFs) based relation module (Con-HCRFs) to explore rigid motion constraints among neighboring points to force point-wise smoothness and within local regions to force region-wise rigidity. FH-Net  proposes a fast hierarchical network with lightweight Trans-flow layers to compute key points flow and inverse Trans-up layers to upsample the coarse flow based on the similarity between sparse and dense points.

Recurrent Methods:FlowStep3D , is the first recurrent method for non-rigid scene flow estimation. They first use a global correlation unit to estimate an initial flow at the coarse scale, and then update the flow iteratively by a Gated Recurrent Unit (GRU). RAFT3D  also adopts a recurrent framework. Here, the objective is not the scene flow itself but a dense transformation field that maps each point from the first frame to the second frame. The transformation is then iteratively updated by a GRU. PV-RAFT  presents point-voxel correlation fields to capture both short-range and long-range movements. Both coarse-to-fine and recurrent methods take the cost volume as input to a convolutional neural network for scene flow prediction. However, these regression techniques may not be able to accurately capture fast movements, and as a result, multi-stage refinement is often necessary. On the other hand, we propose a simpler architecture that solves scene flow estimation in a single-scale global matching process with no iterative refinement.

Soft Correspondence Methods:Some work poses the scene flow estimation as an optimal transport problem. FLOT  introduces an Optimal Transport Module that gives a dense transport plan informing the correspondence between all pairs of points in the two point clouds. Convolutional layers are further applied to refine the scene flow. SCTN  introduces a voxel-based sparse convolution followed by a point transformer feature extraction module. Both features, from convolution and transformer, are used for correspondence computation. However, these methods involve complicated regularization and constraints to estimate the optimal transport from the correlation matrix. Moreover, the correspondences are only computed within a small neighboring region. We instead follow the recent global matching paradigm [10; 56; 64] and solve the scene flow estimation with a global matcher that is able to capture both short-distance and long-distance correspondence.

Runtime Optimization, Prior Assumptions, and Self-supervision:Different from the proposed method, which is fully supervised and trained offline, some other work focuses on runtime optimization, prior assumptions, and self-supervision. Li _et al._ revisit the need for explicit regularization in supervised scene flow learning. The deep learning methods tend to rely on prior statistics learned during training, which are domain-specific. This does not guarantee generalization ability during testing. To this end, Li _et al._ propose to rely on runtime optimization with scene flow prior as strong regularization. Based on  Lang _et al._ propose to combine runtime optimization with self-supervision. A correspondence model is first trained to initialize the flow. Refinement is done by optimizing the flow refinement component during runtime. The whole process can be done under self-supervision. Pontes _et al._ propose to use the graph Laplacian of a point cloud to force the scene flow to be "as rigid as possible". Same as in , this constraint can be optimized during runtime. Li _et al._ propose a self-supervised scene flow learning approach with local rigidity prior assumption for real-world scenes. Instead of relying on point-wise similarities for scene flow estimation, region-wise rigid alignment is enforced. Most recently, Chodosh _et al._ identify the main challenges of LiDAR scene flow estimation as estimating the remaining simple motions after removing the dominant rigid motion. By combining ICP, rigid assumptions, and runtime optimization, they achieve state-of-the-art performance without any training data.

### Point Cloud Registration

Related to scene flow estimation, there are some correspondence-based point cloud registration methods. Such methods separate the point cloud registration task into two stages: finding the correspondences and recovering the transformation. PPFNet  and PPF-FoldNet  proposed by Deng _et al._ focus on finding sparse corresponding 3D local features. Gojcic _et al._ propose to use voxelized smoothed density value (SDV) representation to match 3D point clouds. These methods only compute sparse correspondences and are not capable of handling dense correspondences required in scene flow tasks. More related works are CoFiNet  and GeoTransformer , both of which involve finding dense correspondences employing transformer architectures. Yu _et al._ in CoFiNet  propose a detection-free learning framework and find dense point correspondence in a coarse-to-fine manner. Qin _et al._ in GeoTransformer  further improve the accuracy by leveraging the geometric information. RoITr  introduces a Rotation-Invariant Transformer to disentangle the geometry and poses, and tackle point cloud matching under arbitrary pose variations. PEAL  introduces the Prior Embedded Explicit Attention Learning model (PEAL), and for the first time explicitly injectsoverlap prior into Transformer to solve point cloud registration under low overlap. However, the goal of point cloud registration is not to estimate the translation vectors for each of the points, which makes our work different from these approaches.

### Transformers

Transformers were first proposed in  for translation tasks with an encoder-decoder architecture using only attention and fully connected layers. Transformers have been proven to be efficient in sequence-to-sequence problems, well-suited to research problems involving sequential and unstructured data. The key to the success of transformers over convolutional neural networks is that they can capture long-range dependencies within the sequence, which is very important, not only in translation but also in many other tasks e.g. computer vision , audio processing , recommender systems , and natural language processing .

Transformers have also been explored for point clouds . The coordinates of all points are stacked together directly as input to the transformers. For the tasks of classification and segmentation, PT  proposes constructing a local point transformer using k-nearest-neighbors. Each of the points would then attend to its nearest neighbors. PointASNL  uses adaptive sampling before the local transformer, and can better deal with noise and outliers. PCT  proposes to use global attention and results in a global point transformer. Pointformer  proposes a new scheme where first local transformers are used to extract multi-scale feature representations, then local-global transformers are used as cross attention to multi-scale features, finally, a global transformer captures context-aware representations. Point-BERT  is originally designed for masked point modeling. Instead of treating each point as one data item, they group the point cloud into several local patches. Each of these sub-clouds is tokenized to form input data.

Previous work on scene flow estimation exploits the capability of transformers for feature extraction either using global-based transformers in a local matching paradigm  or local-based transformers in a recurrent architecture . Instead, we propose to leverage both local and global transformers to learn a feature representation for each point on a single scale. We show that high-quality feature representations are the fundamental property that is needed for scene flow estimation when formulated as a global matching problem.

Figure 1: **Method Overview. We propose a simple yet powerful method for scene flow estimation. In the first stage (see Section 3.1) we propose a strong local-global-cross transformer architecture that is capable of extracting robust and highly localizable features. In the second stage (see Section 3.2), a simple global matching yields the flow. In comparison to previous work, our approach is significantly simpler, while achieving state-of-the-art results.**

## 3 Proposed Method

Given two point clouds \(_{1}^{N_{1} 3}\) and \(_{2}^{N_{2} 3}\) with only position information, the objective is to estimate the _scene flow_\(V^{N_{1} 3}\) that maps each point in the source point cloud to the target point cloud. Due to the sparse nature of the point clouds, the points in the source and the target point clouds do not necessarily have a one-to-one correspondence, which makes it difficult to formulate scene flow estimation as a dense matching problem. Instead, we show that learning a cross-feature similarity matrix of point pairs as soft correspondence is sufficient for scene flow estimation. Unlike many applications based on point cloud processing which need to acquire a high-level understanding, e.g. classification and segmentation, scene flow estimation requires a low-level understanding to distinguish geometry features between each element in the point clouds. To this end, we propose a transformer architecture to learn high-quality features for each point. The proposed method consists of two core components: feature extraction (see Section 3.1) and global matching (see Section 3.2). The overall framework is shown in Figure 1.

### Feature Extraction

Tokenization:Given the 3D point clouds \(_{1},_{2}\), each point \(x_{i}\) is first tokenized to get summarized information of its local neighbors. We first employ an off-the-shelf feature extraction network DGCNN  to map the input 3D coordinate \(x_{i}\) into a high dimensional feature space \(x_{i}^{h}\) conditioned on its nearest neighbors \(x_{j}\). Each layer of the network can be written as

\[x_{i}^{h}=_{x_{j}(i)}h(x_{i},x_{j}-x_{i}), \]

where \(h\) represents a sequence of linear layers, batch normalization, and ReLU layers. The local neighbors \(x_{j}(i)\) are found by a k-nearest-neighbor (knn) algorithm. Multiple layers are stacked together to get the final feature representation.

For each point, local information is incorporated within a small region by applying a local Point Transformer  within \(x_{j}(i)\). The transformer is given by

\[x_{i}^{l}=_{x_{j}(i)}(_{l}(x_{i}^{h})-_{l}( x_{j}^{h})+)(_{l}(x_{j}^{h})+), \]

where the input features are first passed through linear layers \(_{l}\), \(_{l}\), and \(_{l}\) to generate query, key and value. \(\) is the relative position embedding that gives information about the 3D coordinate distance between \(x_{i}\) and \(x_{j}\). \(\) represents a Multilayer Perceptron consisting of two linear layers and one ReLU nonlinearity. The output \(x_{i}^{l}\) is further processed by a linear layer and a residual connection from \(x_{i}^{h}\).

Global-cross Transformer:Transformer blocks are used to process the embedded tokens. Each of the blocks includes self-attention followed by cross-attention [38; 43; 47; 56].

The self-attention is formulated as

\[x_{i}^{g}=_{x_{j}_{1}}_{g}(x_{i}^{l}),_{g} (x_{j}^{l})_{g}(x_{j}^{l}), \]

where each point \(x_{i}_{1}\) attends to all the other points \(x_{j}_{1}\), same for the points \(x_{i}_{2}\). Linear layers \(_{g}\), \(_{g}\), and \(_{g}\) generate the query, key, and value. \(,\) denotes a scalar product. Linear layer, layer norm, and skip connection are further applied to complete the self-attention module.

The cross-attention is given as

\[x_{i}^{c}=_{x_{j}_{2}}_{c}(x_{i}^{g}),_{c} (x_{j}^{g})_{c}(x_{j}^{g}), \]

where each point \(x_{i}_{1}\) in the source point cloud attends to all the points \(x_{j}_{2}\) in the target point cloud, and vice versa. A Feedforward network with multi-layer perceptron and layer norm is applied to aggregate information to the next transformer block. The detailed architecture of our proposed local-global-cross transformer is presented in Figure 2. The feature matrices \(F_{1}^{N_{1} d}\) and \(F_{2}^{N_{2} d}\) are formed as the concatenation of all the output feature vectors from the final transformer block, where \(N_{1}\) and \(N_{2}\) are the number of points in the two point clouds and \(d\) is the feature dimension.

### Global Matching

Feature similarity matrices are the only information that is needed for an accurate scene flow estimation. First, the _cross similarity matrix_ between the source and the target point clouds is given by multiplying the feature matrices \(F_{1}\) and \(F_{2}\) and then normalizing over the second dimension with softmax to get a right stochastic matrix,

\[C_{}=F_{2}^{T}}{}, \]

\[M_{}=(C_{}), \]

where each row of the matrix \(M_{}^{N_{1} N_{2}}\) is the matching confidence from one point in the source point cloud to all the points in the target point cloud. The second similarity matrix is the _self similarity matrix_ of the source point cloud, given by

\[C_{}=(F_{1})W_{k}(F_{1})^{T}}{}, \]

\[M_{}=(C_{}), \]

which is a matrix multiplication of the linearly projected point feature \(F_{1}\). \(W_{q}\) and \(W_{k}\) are learnable linear projection layers. Each row of the matrix \(M_{}^{N_{1} N_{1}}\) is the feature similarity between one point in the source point cloud to all the other points in the source point cloud. Given the point cloud coordinates \(_{1}^{N 3}\) and \(_{2}^{N 3}\), the estimated matching point \(}_{2}\) in the target point cloud is computed as a weighted average of the 3D coordinates based on the matching confidence

\[}_{2}=M_{}_{2}. \]

The scene flow is computed as the movement between the matching points

\[_{}=}_{2}-_{1}. \]

The estimation procedure can also be seen as a weighted average of the translation vectors between point pairs, where a softmax ensures that the weights sum to one.

For occlusions in the source point cloud, the matching would fail under the assumption that there exists a matching point in the target point cloud. We avoid this by employing a self similarity matrix that utilizes information from the source point cloud. The self similarity matrix \(M_{}\) bears the similarity information for each pair of points in the source point cloud. Nearby points tend to share similar features and thus have higher similarities. Multiplying \(M_{}\) with the predicted scene flow \(_{}\) can be seen as a smoothing procedure, where for each point, its predicted scene flow vector is updated as the weighted average of the scene flow vectors of the nearby points that share similar features. This also allows the network to propagate the correctly computed non-occluded scene flow estimation to its nearby occluded areas, which gives

\[_{}=M_{}_{}. \]

Figure 2: **Transformer Architecture. Detailed local (left), global (middle), and cross (right) transformer architecture. The local transformer incorporates attention within a small number of neighbors. The global transformer is applied on the source and target points separately and incorporates attention on the whole point clouds. The cross transformer further attends to the other point cloud and gets the final representation conditioned on both the source and the target.**

### Loss Formulation

Let \(\) be the estimated scene flow and \(V_{gt}\) be the ground truth. We follow CamLiFlow  and use a robust training loss to supervise the process, given by

\[_{}=_{i}(\|_{}(i)-V_{}(i)\|_{ 1}+)^{q}, \]

where \(\) is set to 0.01 and \(q\) is set to 0.4.

## 4 Experiments

### Implementation Details

The proposed method is implemented in PyTorch. Following previous methods [14; 55], the numbers of points \(N_{1}\) and \(N_{2}\) are both set to 8192 during training and testing, randomly sampled from the full set. We perform data augmentation by randomly flipping horizontally and vertically. We use the AdamW optimizer with a learning rate of \(2 10^{-4}\), a weight decay of \(10^{-4}\), and OneCycleLR as the scheduler to anneal the learning rate. The training is done for \(600\)k iterations with a batch size of \(8\).

### Evaluation Metrics

For a fair comparison we follow previous work [14; 46; 55] and evaluate the proposed method with the accuracy metric \(EPE_{3D}\), and the robustness metrics \(ACC_{S}\), \(ACC_{R}\) and \(Outliers\). \(EPE_{3D}\) is the 3D end point error \(\|\ -V_{gt}\ \|_{2}\) between the estimated scene flow and the ground truth averaged over each point. \(ACC_{S}\) is the percentage of the estimated scene flow with an end point error less than 0.05 meter or relative error less than 5%. \(ACC_{R}\) is the percentage of the estimated scene flow with an end point error less than 0.1 meter or relative error less than 10%. \(Outliers\) is the percentage of the estimated scene flow with an end point error more than 0.3 meter or relative error more than 10%.

### Datasets

The proposed method is tested on three established benchmarks for scene flow estimation.

**FlyingThings3D** is a synthetic dataset of objects generated by ShapeNet  with randomized movement rendered in a scene. The dataset consists of 25000 stereo frames with ground truth data.

**KITTI Scene Flow** is a real world dataset for autonomous driving. The annotation is done with the help of CAD models. It consists of 200 scenes for training and 200 scenes for testing.

Both datasets have to be preprocessed in order to obtain 3D points from the depth images. There exist two widely used preprocessing methods to generate the point clouds and the ground truth scene flow, one proposed by Liu _et al._ in FlowNet3D  and the other proposed by Gu _et al._ in HPLFlowNet . The difference between the two approaches is that Liu _et al._ keeps all valid points with an occlusion mask available during training and testing. Gu _et al._ simplifies the task by removing all occluded points. We denote the datasets preprocessed by Liu _et al._ in FlowNet3D as F3D\({}_{o}\)/KITTI\({}_{o}\) and by Gu _et al._ in HPLFlowNet as F3D\({}_{s}\)/KITTI\({}_{s}\). In the original setting from [14; 27], the FlyingThing3D dataset **F3D\({}_{s}\)** consists of 19640 and 3824 stereo scenes for training and testing, respectively. **F3D\({}_{o}\)** consists of 20000 and 2000 stereo scenes for training and testing, respectively. For the KITTI dataset, **KITTI\({}_{s}\)** consists of 142 scenes from the training set, and **KITTI\({}_{o}\)** consists of 150 scenes from the training set. Since there is no annotation available in the testing set of KITTI, we follow previous methods to test the generalization ability of the proposed method without any fine-tuning on KITTI\({}_{s}\) and KITTI\({}_{o}\). For better evaluation and analysis, we additionally follow the setting in CamLiFlow  to extend F3D\({}_{s}\) to include occluded points. We denote this as **F3D\({}_{c}\)**.

**Waymo-Open Dataset** is a large-scale autonomous driving dataset. We follow  to preprocess the dataset to create the scene flow dataset. The dataset contains 798 training and 202 validation sequences. Each sequence consists of 20 seconds of 10Hz point cloud data. Different from  which only contains 100 sequences, we trained and tested our model on the full dataset.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

represent the warped source point cloud toward the target point cloud. As we see in the figure, the blue points align very well with the green points, which demonstrates the effectiveness of our method.

## 5 Conclusion

We propose to solve scene flow estimation from point clouds by a simple single-scale one-shot global matching, where we show that reliable feature similarity between point pairs is essential and sufficient to estimate accurate scene flow. To extract high-quality feature representations, we introduce a hybrid local-global-cross transformer architecture. Experiments show that both the presence of local information in the tokenization step and the stack of global-cross transformers are essential to success. GMSF shows state-of-the-art performance on the FlyingThings3D, KITTI Scene Flow, and Waymo-Open datasets, demonstrating the effectiveness of the method.

Limitations:The global matching process in the proposed method needs to be supervised by the ground truth, which is difficult to obtain in the real world. As a result, most of the supervised scene flow estimations are trained on synthetic datasets. We plan to extend our work to unsupervised settings to exploit real data.

   Backbone & PT & \(EPE_{3D}\) & \(ACC_{S}\) & \(ACC_{R}\) & \(Outliers\) & \(EPE_{3D}\) & \(ACC_{S}\) & \(ACC_{R}\) & \(Outliers\) \\  & & _all_ & _all_ & _all_ & _all_ & _non-occ_ & _non-occ_ & _non-occ_ & _non-occ_ \\  DGCNN & ✓ & **0.040** & **92.64** & **95.84** & 10.34 & **0.022** & **95.94** & **98.06** & 8.13 \\ DGCNN & & 0.052 & 89.68 & 94.37 & 13.71 & 0.030 & 93.74 & 97.14 & 11.00 \\ PointNet & ✓ & 0.043 & 92.22 & 95.80 & 10.86 & 0.024 & 95.65 & 98.04 & 8.63 \\ PointNet & & 0.063 & 86.76 & 93.06 & 16.67 & 0.037 & 91.45 & 96.31 & 13.51 \\ MLP & ✓ & 0.043 & 91.81 & 95.48 & **10.21** & 0.023 & 95.43 & 97.84 & **7.75** \\ MLP & & 0.060 & 88.08 & 93.33 & 14.11 & 0.035 & 92.69 & 96.55 & 10.83 \\   

Table 7: **Ablation study on the components of tokenization on F3D\({}_{c}\).** The influence of using different backbones and the presence of a local transformer is tested. The results show that as long as there is local information (DGCNN / Point Transformer) present in the tokenization process, the performance remains competitive. On the other hand, using only PointNet or MLP for tokenization, the performance drops significantly.

Figure 3: **Visualization results on FlyingThings3D.** Two scenes from the FlyingThings3D dataset are given. Red, blue, and green points represent the source, target, and warped source point cloud, respectively. Part of the point cloud is zoomed in for better visualization.

   dim & \(EPE_{3D}\) & \(ACC_{S}\) & \(ACC_{R}\) & \(Outliers\) & \(EPE_{3D}\) & \(ACC_{S}\) & \(ACC_{R}\) & \(Outliers\) \\  & _all_ & _all_ & _all_ & _all_ & _non-occ_ & _non-occ_ & _non-occ_ & _non-occ_ \\ 
32 & 0.073 & 83.04 & 91.32 & 21.07 & 0.044 & 88.36 & 95.07 & 17.56 \\
64 & 0.051 & 89.64 & 94.57 & 13.68 & 0.029 & 93.79 & 97.32 & 10.93 \\
128 & **0.040** & **92.64** & **95.84** & **10.34** & **0.022** & **95.94**