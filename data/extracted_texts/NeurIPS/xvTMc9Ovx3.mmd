# On-Road Object Importance Estimation: A New Dataset and A Model with Multi-Fold Top-Down Guidance

Zhixiong Nan

College of Computer Science, Chongqing University, Chongqing, China.

Yilong Chen

College of Computer Science, Chongqing University, Chongqing, China.

Tianfei Zhou

Corresponding author. School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China. nanzx@cqu.edu.cn, chenyilong@stu.cqu.edu.cn, tfzhou@bit.edu.cn, txiang@cqu.edu.cn

Tao Xiang

College of Computer Science, Chongqing University, Chongqing, China.

###### Abstract

This paper addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver's perspective as the input. Although this problem is significant for safer and smarter driving systems, the exploration of this problem remains limited. On one hand, publicly-available large-scale datasets are scarce in the community. To address this dilemma, this paper contributes a new large-scale dataset named Traffic Object Importance (TOI). On the other hand, existing methods often only consider either bottom-up feature or single-fold guidance, leading to limitations in handling highly dynamic and diverse traffic scenarios. Different from existing methods, this paper proposes a model that integrates multi-fold top-down guidance with the bottom-up feature. Specifically, three kinds of top-down guidance factors (_i.e._, driver intention, semantic context, and traffic rule) are integrated into our model. These factors are important for object importance estimation, but none of the existing methods simultaneously consider them. To our knowledge, this paper proposes the first on-road object importance estimation model that fuses multi-fold top-down guidance factors with bottom-up feature. Extensive experiments demonstrate that our model outperforms state-of-the-art methods by large margins, achieving **23.1**% Average Precision (AP) improvement compared with the recently proposed model (_i.e._, Goal).

## 1 Introduction

According to the World Health Statistics of WHO , road traffic injuries account for a significant 29% of all injury deaths, with nearly 1.3 million people losing their lives in traffic accidents annually. Accurately estimating the importance of on-road objects can pave the way for safer (_e.g._, automatic emergency braking [45; 39]) and smarter driving systems [32; 25; 38; 49; 4; 36; 11], potentially preventing numerous accidents.

Although on-road object importance estimation presents significant research value, it has not been widely explored. One main reason is the scarcity of publicly-available large-scale datasets in the community. Specific for the on-road object importance estimation task, the only one publicly-available dataset is Ohn-Bar _et al._, which contains 3,187 frames, 8 scenes, and 16,076 object importance annotations. Such small-scale dataset supports to train small and less complex models.

However, traffic scenes are dynamic and diverse, asking for complex models to handle various traffic situations. Some researchers have recognized this dilemma and propose some datasets [(8; 21; 46)]. Unfortunately, these dataset are not publicly-available, thereby the dilemma has not been fundamentally addressed. To fundamentally address this dilemma and push forward the advancement of on-road object importance study, this paper will release a large-scale dataset (named as **TOI**, Traffic Object Importance) containing 9,858 frames, 28 scenes, and 44,120 object importance annotations. Compared to Ohn-Bar [(33)], **TOI** achieves a 3.1-fold increase in frames count, a 3.5-fold increase in scene count, and a 2.7-fold increase in object count.

From the perspective of methodology, some methods have been proposed [(21; 8; 50; 33)]. However, these methods are relatively simple, exhibiting the low performance when confronting to challenging traffic scenarios. This motivates us to think about a question: why can not existing methods perform well? The potential reason is that existing on-road importance estimation methods underestimate the complexity of traffic scenarios, individual bottom-up mechanism [(50)] (assuming important objects are the objects with salient color, texture, size, _etc._) or simple top-down guidance mechanism [(8; 21)] (fusing the bottom-up information with a certain type of top-down information such as semantics, ego-car trajectory, driving task [(27)], _etc._) can hardly address dynamic and diverse scenarios.

Therefore, a smarter model is needed. Inspired by the fact that a human driver can accurately estimate object importance in challenging situations, this paper attempts to design a model by analysing the human reasoning mechanism during estimating object importance. To this end, the primary question is "_what essentially crucial factors are considered when a human driver is estimating the importance of objects?_". **Firstly**, _the attributes (e.g., size, color, and texture) of the object_ is considered. For example, when a truck with the big size and a car with the small size simultaneously appear in front of the ego-car, the truck is more important since it imposes bigger impact on the driving. **Secondly**, _the driver intention_ is considered. The objects that will risky collide with the ego-car intention driving path or the objects locating on the ego-car intention driving path present high importance, as shown in Fig. 1a. **Thirdly**, _overall semantic context of the whole traffic scene_ is considered. A human usually pay more attention on the objects in drivable areas rather than the objects in undrivable areas. As shown in Fig. 1b, the person riding a bicycle in undrivable areas is unimportant. **Fourthly**, _traffic rule_ is considered. Most of traffic participants obey the traffic rule, thus the traffic rule is an critical factor for a human to estimate object importance. For example, as shown in Fig. 1c, when there exists a lane marking between the oncoming car and the ego-car, the human driver may consider the oncoming car as unimportant. In contrast, if there is no lane marking between them, the importance of the oncoming car significantly increases. The traffic rule is crucial for object importance estimation. However, none of existing methods utilizes traffic rule to estimate on-road object importance.

Based on the above observations, we propose a model with multi-fold top-down guidance including _driver intention_, _semantic context_, and _traffic rule_. As far as we know, it is the first on-road object importance estimation model that fuses multi-fold top-down guidance factors with the bottom-up feature. The proposed model consists of two kinds of pathways (_i.e._, bottom-up pathway and top-down pathway). Bottom-up pathway and top-down pathway are fused to estimate object importance. Specifically, in the top-down pathway, the top-down guidance factors of _driver intention_ and _semantic context_ are involved in the proposed Driver Intention and Semantics Guidance (**DISG**) module, and _traffic rule_ is modeled in the proposed Traffic Rule Guidance (**TRG**) module. In the bottom-up pathway, Object Feature Extraction (**OFE**) module is proposed to extract object features in both spatial and temporal dimensions.

Figure 1: The crucial factors considered by human drivers when estimating on-road object importance.

A series of comparison and ablation studies are conducted on a public dataset [(33)] and our **TOI** dataset. The comparison experiment results show that our model has a solid advantage over the baselines. The ablation study results validate the effectiveness of our proposed interactive bottom-up & top-down fusion framework and multi-fold top-down guidance modules (_i.e._, **DISG** and **TRG**).

The contributions of this paper are as follows. **1)** This paper contributes a new large-scale dataset, which will be publicly released. This dataset is almost three times larger than the current publicly available public dataset [(33)]. **2)** This paper contributes an object importance estimation model. As far as we know, it is the first on-road object importance estimation model that integrates multi-fold top-down guidance factors with the bottom-up feature. **3)** The traffic rule is crucial for object importance estimation. However, none of existing methods utilizes traffic rule to estimate on-road object importance. This paper considers the effect of traffic rule on object importance and successfully models this abstract concept by proposing an adaptive object-lane interaction mechanism.

## 2 Related Works

_On-Road Object Importance Estimation Related Datasets._ Currently, the primary dilemma of research on the on-road object importance estimation is the lack of sufficient data. Among existing datasets relevant to autonomous driving perception tasks, only a few meet the data requirement of providing images from the driver's first-person perspective while also including object importance level labels. Ohn-Bar _et al._[(33)] are the first to define the problem of on-road object importance estimation, and they propose a small-scale publicly available dataset, which contains 8 scenes. Gao _et al._[(8)] and Li _et al._[(21)] have significantly increased the number of scenes. However, their datasets are not publicly available, thus the contributions to the community is limited. Datasets [(19; 41; 48)] are with detailed annotations such as ego's reaction and road topology. They have great potential to advance the development of on-road object importance estimation. However, currently, they lack object importance level labels and cannot be directly applied to this task.

_On-Road Object Importance Estimation Related Methods._ Currently, on-road object importance estimation methods can be divided into two categories: 1) methods solely utilizing bottom-up feature; 2) methods utilizing single-fold top-down guidance.

The methods solely utilizing bottom-up feature focus on the visual attributes of the objects. The bottom-up processing method is initially introduced in [(44)], and Itti _et al._[(17)] propose one of the first bottom-up mechanism based models. Following this, many researchers are inspired by this concept [(43; 15; 18; 35)]. Zhang _et al._[(50)] introduce a model, which solely relies on RGB clips for object importance estimation. This model employs graph convolutions to characterize the interactions among on-road objects. Nitta _et al._[(30)] develop a model that extracts temporal features from optical flow images to infer the states of moving objects. The optical flow images are also used in Malla _et al._[(26)] to assess the states of moving objects. The bottom-up methods can also be found in the works [(33; 52; 47; 23; 14; 24)]. Although the bottom-up feature is crucial for importance estimation, the methods solely rely on bottom-up feature can not function well in the complex scenarios.

The importance of an object is influenced by many factors such as driver intention, which cannot be fully utilized through bottom-up methods. However, current methods are relatively simple and relies on single-fold guidance. Niu _et al._[(31)] utilize a Transformer with shared weights to identify high-risk objects and generated semantic warning sentences. Li _et al._[(21)] investigate the impact of driver intention, employing the action and trajectory data of the ego-car as additional supervisory signals in auxiliary tasks to enhance model performance. Gao _et al._[(8)] utilize the driver's goal to estimate object importance. A cause-effect problem was formulated in [(20)], which introduced a model to estimate the risky object by considering its potential impact on the driver's behavior. Tang _et al._[(42)] provide a more comprehensive understanding of how driver intentions under different tasks affect the driver attention. Single-fold top-down guidance can also be found in the works of attention prediction task [(7; 16; 22; 6; 1; 29; 5; 28)]. However, none of these methods utilizes multi-fold top-down guidance factors to estimate the on-road object importance.

## 3 A New Dataset: TOI

We thoroughly review existing datasets for on-road object importance estimation as well as the datasets for the related tasks, and provide a summary in Tab. 1. The datasets for the related tasks (_e.g._, risk assessment [(37; 48; 19)], accident anticipation [(41)], and situation awareness [(40)]) do not include object importance labels, making them unsuitable for object importance estimation task. Most datasets (_e.g._, [(21; 8)]) for object importance estimation are not publicly available. The only publicly available dataset is Ohn-Bar (33), but it is a small scale dataset. In response to the scarcity of publicly available large-scale datasets for on-road object importance estimation, we contribute a large-scale dataset named **TOI** (Traffic Object Importance). **TOI** is built by re-annotating the authoritative KITTI (9) dataset. While there are many datasets (_e.g._, nuScenes (2; 40; 37)) that be used for object importance annotations, we select KITTI dataset for the following reason: KITTI is the worldwide benchmark in the field of autonomous driving. In addition, KITTI is collected in diverse real traffic scenes including rural areas and on highways with rich date formats, making the dateset friendly for various tasks.

_Annotation Procedure._ The criterion of object importance might be ambiguous as different drivers usually hold different opinions on the importance judgment. Currently, object-level importance labels are annotated without checking mechanism. Although multiple annotators perform the annotations, the annotations finished by the certain annotator are not checked by others, leading to the unreliable and ambiguous annotations. To generate reliable annotations, we adopt two mechanisms: the _double-checking annotation mechanism_ and the _triple-discussing annotation mechanism_.

The _double-checking annotation mechanism_ operates as follows. Initially, the first annotator (an experienced driver) labels the object importance at every 10 frames. To guarantee the reliability of annotations, the first annotator only selects one object as the important object at each time of observing the whole 10 frames. The annotation for these 10 frames is finished until all important objects are annotated, then the annotator moves to the next set of 10 frames for annotation. Subsequently, the annotation results are checked by the second annotator (who is also an experienced driver). When the second annotator finds a disputed annotation, the first and second annotators discuss together to reach an agreement. If they are unable to reach an agreement, the _triple-discussion annotation mechanism_ is activated. In this case, the third annotator is invited to discuss the final annotations.

_Statistics and Comparison._ Totally, 9,858 image frames are annotated, generating 44,120 objects with the importance or unimportance annotations, among which 5,052 objects are annotated as important. The annotated data are randomly split into training/testing datasets with a ratio of 8,121 : 1,737. The comparison between **TOI** and existing on-road object importance estimation datasets and similar task datasets are presented in Tab. 1. Compared to the publicly available Ohn-Bar (33) dataset, **TOI** represents the significant advantages in following three aspects. **Frame quantity**: **TOI** exhibits a substantial increase in the number of frames, with 9,858 frames compared to 3,187 frames in the Ohn-Bar dataset. **Object quantity**: the number of annotated objects is 44,120 compared to 16,076 in the Ohn-Bar dataset. **Scene diversity**: while Ohn-Bar contains only 8 scenes, **TOI** covers 28 scenes. Compared to Goal (8) dataset, **TOI** has rich annotations such as Lidar and 3D tracklet labels. The abundance of multimodal annotations enables **TOI** to support the research on on-road object importance estimation using multimodal learning methods in the future. Though Goal (8) presents the advantage in terms of frame number and scene diversity, it is not publicly available. Compared to Li dataset (21), **TOI** offers the frame rate of 10 FPS. This high temporal resolution is critical for capturing the dynamic changes of on-road object importance.

_Annotation Challenges._ Compared to datasets for other tasks, **TOI** may not be considered large-scale, it is relatively large-scale compared to existing publicly available datasets for on-road object importance estimation. However, annotating object importance at this scale is challenging. Each annotation requires multiple annotators and undergoes rigorous checking to achieve satisfactory results. In addition, to generate reliable annotations, only one object is annotated at each time observing the whole video sequence, a complete re-observation of the whole video sequence is required for the annotation of the next object. Moreover, object importance is affected by multiple factors, which imposes difficulties on the annotation.

    &  &  &  &  &  &  &  &  \\  & & & & GPS/MM & Lidar & & & & & & & \\  HDD (37) & risk assessment & ✓ & ✗ & ✓ & ✓ & ✗ & - & - & - & 30 & 2018 \\
1361-honda (48) & risk assessment & ✓ & ✗ & ✗ & ✗ & ✗ & - & - & 1,361 & - & 2020 \\ RiskBench (19) & risk assessment & ✓ & ✗ & ✗ & ✗ & ✗ & - & - & 6,916 & - & 2024 \\ NID (41) & accident anticipation & ✓ & ✗ & ✗ & ✗ & - & 499,500 & 4,995 & - & 2018 \\ A-SASS (46) & situation awareness & ✓ & ✓ & ✗ & ✗ & - & - & 10 & 30 & 2022 \\ ROAD (40) & situation awareness & ✓ & ✗ & ✓ & ✓ & ✓ & 560,000 & 122,000 & 22 & 12 & 2023 \\ Ohn-Bar (33) & on-road object importance estimation & ✓ & ✓ & ✓ & ✓ & ✓ & 16,076 & 3,187 & 8 & 10 & 2017 \\ Goal (8) & on-road object importance estimation & ✗ & ✓ & ✓ & ✗ & - & 244,980 & 743 & 30 & 2019 \\ Li (21) & on-road object importance estimation & ✗ & ✓ & ✓ & ✓ & ✓ & - & - & 2 & 2022 \\  TOI & on-road object importance estimation & ✓ & ✓ & ✓ & ✓ & ✓ & 44,120 & 9,858 & 28 & 10 & 2024 \\   

Table 1: Comparison between the TOI and State-of-the-art Datasets. ‘Impo.’ represents the object importance annotation.

## 4 Approach

### Overview

Consider a traffic scenario with \(N\) on-road objects in \(T\) time steps, the goal of this work is to estimate on-road object importance (\(\)) at the final time step (_i.e._, \(t=T\)) using the video sequence (\(\)) captured from the driving perspective over \(T\) time step and ego-car velocity information (\(E\)) at the first time step (_i.e._, \(t=1\)), which is formulated as:

\[=(,E),\] (1)

where \(\) represents an on-road object importance estimation network, \(\)=\(\{_{t}\}_{t=1}^{T}\), and \(\)=\(\{A_{i}\}_{i=1}^{N}\).

In order to effectively fuse multi-fold top-down guidance factors (_i.e._, _semantic context_, _driver intention_, and _traffic rule_) with bottom-up object visual feature, we propose a multi-fold top-down guidance aware model, the overview of which is illustrated in Fig. 2. Our model is composed of four key modules: **Object Feature Extraction** (**OFE**) module detailed in SS 4.2, **Driver Intention and Semantics Guidance** (**DISG**) module described in SS 4.3, **Traffic Rule Guidance** (**TRG**) module explained in SS 4.4, and **Object Importance Estimation** module introduced in SS 4.5.

Firstly, **OFE** extracts object spatial feature \(_{o,s}\) and object temporal feature \(_{o,t}\) from \(\). Then, **DISG** takes \(\), \(\), and \(_{o,s}\) as inputs, and outputs object-intention-semantics interaction feature \(_{o i-s}\). Meanwhile, in **TRG**, the lane feature \(_{l}\) and \(_{o,t}\) are processed by **adaptive object-lane interaction** mechanism to produce the object-lane interaction feature \(_{o l}\). Finally, \(_{o i-s}\) and \(_{o l}\) are used to estimate object importance \(\).

### Object Feature Extraction

The goal of **Object Feature Extraction** (**OFE**) module is to extract object features in both temporal and spatial dimensions. The input of **OFE** is a RGB video sequence \(^{T 3 W H}\), and the outputs are object temporal feature \(_{o,t}\) and object spatial feature \(_{o,s}\).

To begin with, **OFE** takes \(\) and \(\) (\(\) denote optical flow images derived from \(\)) as inputs to extract the object visual feature \(_{v}^{N T C W^{} H^{}}\) (reflecting the appearance of the object) and the object motion feature \(_{m}^{N T C W^{} H^{}}\) (reflecting the movement of the object). This procedure is formulated as:

\[_{v}=(_{V}()),_{m}=( _{M}()),\] (2)

where \(_{V}\) and \(_{M}\) represent the two ResNet18 (12), Roi denotes the ROI pooling (10), \(C\) represents the number of channels, \(W^{}\) and \(H^{}\) denote the width and height obtained through ROI pooling.

Subsequently, object spatial feature \(_{o,s}^{N 2C W^{} H^{}}\) is obtained based on \(_{v}\) and \(_{m}\). The goal of \(_{o,s}\) is to focus on the spatial information of objects. Therefore, an average pooling is applied on the time dimension (_i.e._, the dimension of \(T\)) of \(_{v}\) and \(_{m}\), and a self-attention mechanism is utilized to emphasize the spatial information (_i.e._, the dimensions of \(W^{}\) and \(H^{}\)), which is denoted as follows:

\[_{o,s}=_{mhsa}(((_{v}),(_{m}))),\] (3)

where Avg denotes average pooling, Concat is concatenation. \(_{mhsa}\) represents the multi-head self-attention mechanism, and it has the same meaning in the following parts.

Figure 2: The overview of multi-fold top-down guidance aware model.

[MISSING_PAGE_FAIL:6]

where the operator \(\) makes the model pay more attention to the semantic context in the driver intention regions.

The second task of **object-intention-semantics interaction** is to refine \(_{o,s}\) by interacting with \(_{is}\), which is formulated as follow:

\[_{ois}=_{mhca}(_{o,s},_{is })+_{o,s},\] (9)

where \(_{mhca}\) denotes the multi-head cross-attention mechanism, \(_{o,s}\) serves as the _query_ while \(_{is}\) serves as the _key_ and _value_, and \(_{ois}^{N 2C W^{}  H^{}}\).

### Traffic Rule Guidance

On-road object importance is also closely related with traffic rule, but it is often overlooked in previous works. To effectively leverage the traffic rule, we propose the **Traffic Rule Guidance** (**TRG**) module, which consists of two components: **lane feature extraction** and **adaptive object-lane interaction**.

**Lane feature extraction** is to make the preparation for **adaptive object-lane interaction**. In detail, a linear transformation and an activation are applied on lane information \(\):

\[_{l}=(()),\] (10)

where \(\) are the coordinates of lane marking points, which are derived from \(_{T}\) via a lane marking detector, Relu is the rectified linear unit activation, and \(_{l}^{N C^{}}\).

**Adaptive object-lane interaction** is the core of **TRG**, and it contains two steps: _object-lane interaction_ and _object-lane interaction weighting_. In the first step, lane feature \(_{l}\) and \(_{o,t}\) are fused through a multi-head cross-attention mechanism and a residual mechanism, which can be denoted as:

\[_{ol}^{m}=_{mhca}(_{l},_{o,t})+_{o, t},\] (11)

where \(_{ol}^{m}^{N C^{}}\) denotes initial object-lane interaction feature, and \(_{l}\) serves as the _query_ while \(_{o,t}\) serves as the _key_ and _value_,

Factually, \(_{ol}^{m}\) has considered the traffic rule factor by modeling the relation between lane markings and on-road objects. However, the influence of lane markings on on-road object importance estimation might not be universally-effective in all scenarios, thus we propose the _object-lane interaction weighting_ mechanism to realize **adaptive object-lane interaction**.

The goal of _object-lane interaction weighting_ is to adaptively penalize the cases in which object-lane relation is weak (_e.g._, static roadside cars weakly interacts with lane markings). To this end, a MLP network is applied on \(_{ol}^{m}\) to compute a score \(p\), and this score is then used to compute the corresponding penalizing coefficient \(p_{c}\), which is denoted as:

\[p=(_{mlp}(_{ol}^{m})),\] (12)

\[p_{c}=1,&p<0.5\\ ,&p 0.5,\] (13)

where \(\) is a very small value.

Based on \(p_{c}\), the object-lane interaction feature \(_{ol}^{N C^{}}\) is obtained by weighting \(_{ol}^{m}\) in Eq. (11), which is denoted as:

\[_{ol}=_{ol}^{m} p_{c}.\] (14)

We note that _object-lane interaction weighting_ is the core of **adaptive object-lane interaction**, which is significant for object importance estimation (30.4% improvement on AP).

### Object Importance Estimation

Taking \(_{ois}\) in Eq. (9) and \(_{ol}\) in Eq. (14) as the inputs, object importance \(^{N}\) is estimated. This procedure is formulated as:

\[=(_{mlp}((_{oi {-}s})+_{ol})),\] (15)

where \(\) signifies the importance for each object. The Linear layer transforms the dimensions of \(_{ois}\) from \(N 2C W^{} H^{}\) to \(N C^{}\) so that it can be added to \(_{ol}\).

Experiments

_Metrics_. For performance evaluation, two classical metrics are chosen: Average Precision (AP) and F1 Score (F1). AP is computed by calculating the area under the precision-recall curve at various thresholds, thus it is a compressive metric to indicate both the precision and recall of a model. F1 is computed by precision and recall at a fixed threshold, thus it indicates the balance between precision and recall. Both metrics follow the principle that higher values indicate better performance.

_Loss Function_. The loss function is defined as follow:

\[(},)=(},)+(},),\] (16)

where \(\) is the predicted object importance, \(}\) is the ground-truth.

### Comparison Experiment

_Baselines_. Our model is compared with seven models. Ohn-Bar (33), Goal (8), Zhang (50), and Li (21) are representative works for on-road object importance estimation. In addition, considering that the salient object detection indicates important objects to the certain extent, three recently-proposed salient object detection models, namely MENet (23), A2S (52), and PGNet (47), are also selected as baselines.

_Quantitative Comparison_. Tab. 2 shows the comparison results on **TOI** and Ohn-Bar (33) datasets. The 'Video' We can observe that our model outperforms all seven baselines. On the Ohn-Bar (33) dataset, our model achieves **23.1%** and **96.3%** performance improvements on AP and F1 metrics compared with the second-best result, respectively. On the **TOI** dataset, compared with the second-best result on AP and F1 metrics, our model achieves **20.0%** (_i.e._, (60-50)/50) and **10.2%** performance improvements, respectively. The results of MENet (23), A2S (52), and PGNet (47) are not reported on AP metric due to the lack of confidence scores in salient object detection outputs, making it impossible to calculate precision-recall curves for different thresholds. The F1 results for Zhang (50) and Li (21) are both 0 because they predict all objects as unimportant.

### Ablation Studies

_Top-down and Bottom-up Framework_. To validate the effectiveness of our model that interactively integrates multi-fold top-down guidance mechanisms with bottom-up features, we conduct four experiments: **#1**: only the bottom-up module is enabled; **#2:** the bottom-up module is combined with **TRG** module; **#3:** the bottom-up module is combined with **DRG** module; **#4:** the bottom-up module is combined with **TRG** module module. Table 3: Ablation study on top- and **DISG** module. The experimental results are reported in Tab. 3. down and bottom-up framework. Compared with **#1**, **#2** achieves 55% and 40% performance improvements on AP and F1, respectively. Similarly, **#3** obtains 160% and 56% performance improvements on AP and F1, respectively. When both modules are enabled (**#4**), our model exhibits the best performance. These results validate both **TRG** and **DISG** modules are effective.

    &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  \\   & & & & & & & & & & & \\   Ohn-Bar (33) PR-2017 \\ Goal (8) JCRA2019 \\ Zhang (50) JCRA2020 \\ Li (21) ICRA2022 \\ MENet (23) CVPR 20023 \\ A2S (52) CVPR 20023 \\ PGNet (47) CVPR 20022 \\ Ours & ✓ & ✓ & **60** & **54** & **92** & **64** & **53** & **69** & 115 \\   

Table 2: Quantitative comparison with baselines on TOI and Ohn-Bar (33) datasets. The ‘Video’ signifies the usage of RGB video sequence, ‘Velocity’ denotes the incorporation of vehicle velocity information, and ‘3D-Object’ indicates the utilization of 3D object properties information.

    &  &  &  &  &  \\    & & & & & & & 20 & 25 \\
**\#2** & ✓ & ✓ & & & & 31 & 35 \\
**\#3** & ✓ & ✓ & ✓ & & 52 & 39 \\
**\#4** & ✓ & ✓ & ✓ & & **60** & **54** \\   

Table 3: Ablation study on top-down and bottom-up framework.

_Driver Intention and Semantics Guidance_ (DISG). To verify the effectiveness of semantic context guidance and driver intention guidance, we conduct three experiments: **#1**: the model without semantic guiding feature and intention guiding mask; **#2**: the model only with semantic guiding feature; **#3**: the model only with intention guiding mask; **#4**: the model with both semantic guiding feature and intention guiding mask. The results are shown in Tab. 4. Compared to **#1**, **#2** yields 58.1% and 37.1% performance improvements on AP and F1 metrics, respectively. This enhancement is attributed to the usage of the semantic guiding feature, which enables the model to learn the semantic relation between objects and the whole scene. Meanwhile, **#3** achieves 15.4% and 11.4% performance improvements on AP and F1 metrics, respectively. The effectiveness of our intention guiding mask accounts for this advancement. **#4** exhibits the best performance, demonstrating the effectiveness of our proposed semantic guiding feature and intention guiding mask.

_Traffic Rule Guidance_ (**TRG**). To analyze the effects of _object-lane interaction_ and _object-lane interaction weighting_, we conduct three experiments: **#1**: the model without _object-lane interaction_ and _object-lane interaction weighting_; **#2**: only the _object-lane interaction_ **#3**: both the _object-lane interaction_ and the _object-lane interaction weighting_ are enabled. The corresponding results are summarized in Tab. 5. Compared to **#1**, **#3** obtains 15.4% and 38.5% improvements on the AP and F1 metrics, respectively. These results demonstrate the significance of both _object-lane interaction_ and the _object-lane interaction weighting_ mechanisms. The reason is explained is not utilized, the implicit traffic rule conveyed by the lane is not used. The absence of the traffic rule results in a reduced ability of the model.

It comes as a surprise that the individual usage of _object-lane interaction_ (**#2**) leads to the performance decreasing on the AP metric compared to **#1**. This is due to the fact that not all on-road objects are influenced by lanes. Without _object-lane interaction weighting_, individual _object-lane interaction_ generates a uniform object-lane interaction feature, which could not adaptively extend to diverse scenarios. This result potentially proves the significance of our _object-lane interaction weighting_, which enables the model to adaptively disable the object-lane interaction feature when object importance weakly rely on _object-lane interaction_ (_e.g._, static cars on the roadside).

To further analyze _object-lane interaction weighting_, we visualize its output (_i.e._, \(p_{c}\) in Eq. (13)). Some examples are illustrated in Fig. 3 where objects with blue masks are penalized (_i.e._, object-lane interaction is disabled, \(p_{c}\)=\(\)) and objects with yellow masks are not penalized (_i.e._, object-lane interaction is enabled, \(p_{c}\)=1). In Fig. 2(a) and Fig. 2(b), the _object-lane interaction weighting_ penalizes the cars on both sides of the road. The results make sense since the static cars on roadsides are factually not interacting with lanes. In Fig. 2(c) and Fig. 2(d), incoming cars from the opposite direction and the car on the current lane are not penalized, since these cars are interacting with lanes. We note the yellow mask do not signal the important object. Instead, it indicates that the _object-lane interaction_ is enabled.

## 6 Conclusion

On-road object importance estimation is significant for various applications in the fields of assisted driving and autonomous driving. The dilemmas of current research are two fold: **1)** the scarcity of large-scale publicly available datasets hinder the development of on-road object importance estimation, and **2)** existing methods are relatively simple to handle complex and diverse traffic scenarios. In response to the dilemmas, this paper contributes a new dataset and proposes a model with multi-fold top-down guidance. A large range of experiments demonstrate the superiority of our proposed model. The main conclusion is that building up the model that comprehensively considers multi-fold top-down guidance (_e.g._, driver intention, semantic context, and traffic rule) and bottom-up feature (_e.g._, size, distance, and speed) is a promising way to remarkably push forward the study of on-road object importance estimation.

  
**Method** & Serman. & Intent. & AP\(\) & F1\(\) \\ 
**\#1** & & & 31 & 35 \\
**\#2** & ✓ & & 49 & 48 \\
**\#3** & ✓ & & 35 & 39 \\
**\#4** & ✓ & ✓ & **60** & **54** \\   

Table 4: Ablation study on DISG.

Figure 3: Visualization of _object-lane interaction weighting_.