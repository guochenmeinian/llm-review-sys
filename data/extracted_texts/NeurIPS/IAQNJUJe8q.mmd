# Full-Atom Peptide Design

with Geometric Latent Diffusion

 Xiangzhe Kong\({}^{1,2}\) Yinjun Jia\({}^{3}\) Wenbing Huang\({}^{4}\) Yang Liu\({}^{1,2,5}\)

\({}^{1}\)Dept. of Comp. Sci. & Tech., Tsinghua University

\({}^{2}\)Institute for AIR, Tsinghua University

\({}^{3}\)School of Life Sciences, Tsinghua University

\({}^{4}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{5}\)Shanghai Artificial Intelligence Laboratory, Shanghai, China

Correspondence to Wenbing Huang <hwenbing@126.com>, Yang Liu <liuyang2011@tsinghua.edu.cn>

###### Abstract

Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom **Peptide** design with **G**eometric **LA**tent **D**iffusion (PepGLAD) given the binding site. We first establish a benchmark consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature for systematic evaluation. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Experimental Results show that our method not only improves diversity and binding affinity significantly in the task of sequence-structure co-design, but also excels at recovering reference structures for binding conformation generation.

## 1 Introduction

Peptides are short chains of amino acids and acts as vital mediators of many protein-protein interactions in human cells. Designing functional peptides has attracted increasing attention in biological research and therapeutics, since the highly-flexible conformation space of peptides allows brand new possibility to target binding sites previously undruggable with antibodies or small molecules . The key of peptide design is to generate peptides that interact compactly with target proteins (see Figure 1), since they mostly exhibit flexible conformations  unless bound to these receptors .

Conventional simulation or searching algorithms rely on frequent calculations of physical energy functions , which are inefficient and prone to poor local optimum. Recent advances illuminate the remarkable success of exploiting geometric deep generative models, particularly the equivariant diffusion models , for molecule design , antibody design  and protein design , as well as latent diffusion models further enhancing the performance . Inspired by these successes, a natural idea is leveraging diffusion models for peptide design as well, which, yet, is challenging in two aspects. From the dataset aspect, existing databases (PepBDB , Propedia ) merely collect data from Protein Data Bank (PDB) , neither performing filters according to practical relevance  and redundancy, nor providing an adequate split for evaluation.

Therefore, this paper first curates a benchmark from PDB  and the literature , and then systematically evaluates the generative models in terms of diversity, consistency, and binding affinity.

From the methodology aspect, it is nontrivial to adopt latent diffusion models to characterize the geometry of protein-peptide interactions. The first nontriviality stems from the _full-atom geometry_, which determines the comprehensive protein-peptide interactions in the atomic level, yet difficult to preserve. Throughout the generation process, the type of each amino acid always changes and thus requires us to generate different number of atoms, which is unfriendly to diffusion models that prefer fixed-size generation. Current latent diffusion models on molecules  or protein backbones  still tackle tasks with fixed number of atoms, thus leaving this challenge untouched. The second nontriviality lies in the _variable binding geometry_. Diffusion models are typically implemented directly in the data space, which might be suitable for regular data (_e.g._ images with fixed value range), yet ill-suited for our case on 3D coordinates where the value range is not fixed and even cursed with high variances due to the rich diversity in protein-peptide interactions. These variances define divergent target distributions of Gaussian with disparate expectation and covariance, which hinders the transferability of the diffusion process across different binding sites and thereby yields unsatisfactory generalization capability. Unfortunately, this point is seldom investigated previously.

To address the above problems, we propose a powerful model for full-atom **Pep**tide design with **Geometric LA**tent **D**iffusion (PepGLAD) with the following contributions:

* We construct a new benchmark from PDB and literature based on practical relevance and non-redundancy, then systematically evaluate available sequence-structure co-design models on the task of target-specific peptide design.
* To capture the _full-atom geometry_, we first learn a Variational AutoEncoder (VAE) to obtain a fixed-size latent representation (including a 3D coordinate and a hidden feature) for each residue of the input peptide, and then conduct the diffusion process in this latent space, both of which are conditioned on the binding site to better model protein-peptide interactions. Notably, the proposed design enables our model to accommodate full-atom input and output.
* Regarding the _variable binding geometry_, we derive a shared standard space from the binding sites by proposing a novel skill--receptor-specific affine transformation. Such affine transformation is computed by the center offset and covariance Cholesky decomposition of the binding site coordinates, serving as a mapping from the binding site distribution to standard Gaussian. With the affine transformation applied to both the binding sites and the peptides, we are able to project the shape of all complexes into approximately standard Gaussian distribution, which facilitates generalization to diverse binding sites.

Favorably, all the aforementioned models and processes meet the desired symmetry, _i.e._, E(3)-equivariance, as proved by us. Experiments on sequence-structure co-design and complex conformation generation demonstrate the superiority of PepGLAD over the existing generative models.

Figure 1: **Left**: Peptide design requires generating peptides that form compact interactions with the binding site on the receptor. The intricacy of protein-peptide interactions demands efficient exploration in the vast space for sequence-structure co-design. **Right**: Different binding sites (a, b, c, d) adopt disparate center offsets and geometric shapes, approximating variable 3D Gaussian distributions that deviate from \((,)\). We propose to convert the geometry into a standard space approximating standard Gaussian, via an affine transformation derived from the binding site (ยง3.3).

Related Work

**Peptide Design** Conventional methods directly sample residues  or building blocks from libraries containing small fragments of proteins [26; 57; 9; 8], with guidance from delicate physical energy functions . These methods are time-consuming and easy to be trapped by local optimum. Recent advances with deep generative models mainly focuses on target-agnostic 1D language models , antimicrobial peptides [13; 63], or a subtype of peptides with \(\)-helix [68; 69]. While geometric deep generative models are exhibiting notable potential in other domains of target-specific binder design (_e.g._ antibodies), their capability of target-specific peptide design remains unclear, which is the first problem we answer in this paper. Other contemporary work includes peptide design algorithms with flow matching frameworks [38; 40].

**Geometric Protein/Antibody Design** Protein design primarily aims to generate stable secondary or tertiary structures , where diffusion models demonstrate inspiring performance [66; 58; 3; 72]. In particular, RFDiffusion  first generates backbones via diffusion, and then designs the sequences through cycles of inverse folding and structure refining with empirical force fields. Chroma  adopts a similar strategy, but further explores controllable generative process with custom energy functions. Antibody design, encompassing a special family of proteins in the immune system to capture antigens, mainly focuses on inpainting complementarity-determining regions (CDRs) at the interface between the antigen and the framework [33; 34; 61], where the geometric diffusion models exhibit promising potential [44; 45] in co-designing sequence and structure. Unlike antibodies which are constrained by framework regions, peptides exhibit a more irregular binding pattern and greater flexibility, adapting to binding sites upon interaction . Thus the target distributions are remarkably divergent on different binding sites, posing an urgent need for more robust generative modeling.

**Geometric Latent Diffusion Models** Diffusion models learn a denoising trajectory to generate desired data distribution from a prior distribution, commonly standard Gaussian [54; 55; 25]. Recent literature extends diffusion to 3D small molecules satisfying the E(3)-equivariance , which triggers subsequent advances in geometric design of macro molecules (_e.g._ antibody, protein) as aforementioned. Further efforts are made to latent diffusion models [53; 71; 18], which implement the generative process in the compressed latent space of pretrained auto-encoders, to improve the performance. Compared to the literature that either encodes atom-wise representation in the latent space for small molecule generation , or compress a fixed number of atoms into one latent node for protein backbone generation , we explore compression of the full-atom geometry by directly generating different residues with variable number of atoms in the latent space. Moreover, we propose a novel technique, namely the data-specific affine transformations, to enhance the generalization ability of diffusion models, which is barely explored before.

## 3 Our Method: PepGLAD

We first define the notations in the paper and formalize peptide design in SS3.1. The overall workflow of our PepGLAD is presented in Figure 2, which consists of three modules: (1) An autoencoder that defines the joint latent space for sequences and structures conditioned on the full-atom context of the binding site (SS3.2); (2) An affine transformation derived from the binding site to project the 3D geometry into a standard space approximating standard Gaussian distribution (SS3.3); (3) A latent diffusion model trained on the standard latent space (SS3.4). Finally, we summarize the training and the sampling procedures in SS3.5.

### Definitions and Notations

We represent binding sites and peptides as geometric graphs \(=\{(x_{i},}_{i})\}\), where each node \(i\) is a residue with its amino acid type \(x_{i}\) and the coordinates of all its \(c_{i}\) atoms \(}_{i}^{c_{i} 3}\). In later sections, we use the simplified notations \(i\) to denote that a node \(i\) is in the geometric graph \(\), and \(||\) to denote the total number of nodes in \(\). We use \(_{p}\) and \(_{b}\) to represent the geometric graph of the peptide and the binding site, respectively. In this work, the binding site incorporates residues on the target protein within 10A distances to the peptide residues based on \(C_{}\) atoms which alleviates leakage of the side-chain interactions. Note that the threshold (10A) is chosen to be large to better reduce leakage of the peptide geometry.

Task DefinitionGiven the binding site \(_{b}\), we aim to obtain a generative model \(p_{}\) conforming to the distribution of binding peptides \(q(_{p}|_{b})\).

### Variational AutoEncoder

The autoencoder  consists of an encoder \(_{}\) that encodes the peptide \(_{p}\) in the presence of the binding site \(_{b}\) into a latent state \(_{z}\), and a decoder \(_{}\) that reconstructs the peptide from the latent state to obtain \(^{}_{p}=\{(x^{}_{i},}^{}_{i})\}\). To encourage \(_{}\) to learn contextual representations of residues, we corrupt 25% of the residues in \(_{p}\) with a \([]\) type to obtain \(}_{p}\) as the input:

\[_{z}=_{}(}_{p},_{b}), ^{}_{p}=_{}(_{z},_ {b}), \]

where \(_{z}=\{(_{i},}_{i})|i_{p}\}\) contains the latent states \(_{i}^{h}\) (\(h=8\) in this paper) and \(}_{i}^{3}\) sampled from the encoded distribution \((_{i};_{i},_{i})\) and \((}_{i};}_{i},}_{i})\) using the reparameterization trick . We borrow the adaptive multi-channel equivariant encoder in dyMEAN  for both \(_{}\) and \(_{}\) to capture the full-atom geometry. In the decoder \(_{}\), we factorize the joint distribution of sequences and structures as follows:

\[p_{}(x^{}_{i},}^{}_{i}|_{z},_ {b})=p_{_{1}}(x^{}_{i}|_{z},_{b})p_{_{2}}( }^{}_{i}|x^{}_{i},_{z},_{b}), \]

where the sequence is first decoded and then the all-atom geometry, initialized with replications of \(}_{i}\), is reconstructed. The training objective of the autoencoder consists of the reconstruction loss \(_{recom}\) and the KL divergence \(_{KL}\) to constrain the latent space. The reconstruction loss includes cross entropy on the residue types, mean square error (MSE) on the full-atom structures, and an auxilary loss \(_{aux}\) on bond lengths and angles :

\[_{recom}(i)=H(p(x_{i}),p(x^{}_{i}))+( }_{i},}^{}_{i})+_{aux}(i), \]

where \(H\) denotes cross entropy. We include details of \(_{aux}\) in Appendix A. The KL divergence constrains \(_{i}\) and \(}_{i}\) with the prior \((,)\) and \((}_{i},)\), respectively, where \(}_{i}\) denotes the coordinate of the alpha carbon (\(_{}\)) in node \(i\):

\[_{KL}(i)=_{1} D_{}((,)\|(_{i},(_{i})))+_{2}  D_{}((}_{i},)\|( {}_{i},(}_{i}))), \]

where \(D_{}\) denotes the KL divergence, \(_{1}\) and \(_{2}\) reweight the contraints on the sequence and the structure, respectively. \(_{KL}\) prevents the scale of \(_{i}\) from exploding and constrains \(}_{i}\) around \(_{}\) to retain necessary geometric information. Such regularization also helps ensure consistent scales between the peptide latent coordinates and the pocket, mitigating potential issues arising from their different levels of abstraction. Then we have the overall training objective of the variational autoencoder as follows:

\[_{AE}=_{i_{p}}(_{recom}(i)+ _{KL}(i))/|_{p}|. \]

Figure 2: Overall architecture of PepGLAD. **(A)** Variational AutoEncoder (ยง3.2): compressing the sequence and the structure \(\{(x_{i},}_{i})\}\) of the peptide into the latent space \(\{(_{i},}_{i})\}\) with the encoder \(_{}\), and decoding the sequence and full-atom geometry from the latent states with the decoder \(_{}\). **(B)** Affine Transformation \(F\) (ยง3.3): projecting the geometry to approximately \((,)\) via the receptor-specific affine transformation derived from the binding site, and recovering the data geometry with the inverse of \(F\) after the diffusion generative process. **(C)** Latent Diffusion (ยง3.4): jointly generating \(_{i}\) and \(}_{i}\) in the standard latent space.

We have explored \((3)\)-invariant latent space, which appears to have difficulties in reconstructing the full-atom structures since it lacks information of geometric interactions with the pocket atoms (Appendix F).

### Receptor-Specific Affine Transformation

With the latent space given by the autoencoder, we further exploit a standard space obtained from receptor-specific affine transformations, which enhances the transferability of diffusions on disparate binding sites (see Figure 1). Most peptides fold into complementary shape upon binding on the receptor . Thus, the target distribution is inherently characterized by the shape of the binding site. Given the wide disparity in binding geometries, directly implementing diffusion in the data space yields minimal transferability among different binding sites. To address this deficiency, we propose to implement the diffusion process on a shared standard space converted via an affine transformation derived from the binding site. Formally, denoting the \(_{}\) coordinates of the residues in a given binding site \(_{b}\) as \(}^{3|_{b}|}\), we can derive their center \(}=[}]^{3}\) and covariance \(}=(},})^{3 3}\), so that these coordinates can be regarded as sampled from the distribution \((},})\). We then calculate the Cholesky decomposition  of \(}\):

\[}=}}^{},}^{3  3}, \]

where \(}\) is a lower triangular matrix. \(}\) is unique  and invertible since the covariance matrix is a real-valued symmetric positive-definite matrix2. Then we can define the affine transformation \(F:^{3}^{3}\), which enables the projection of the geometry into the standard space approximating standard Gaussian \(F(})(,)\). Further, we can easily obtain the inverse of \(F\) as:

\[F(})=}^{-1}(}-}), F^{-1}( })=}}+}. \]

With the above definitions, for each given binding site \(_{b}\), we transform the geometry via the derived \(F\) to obtain the standard space, where the diffusion model is implemented, and recover the original geometry with \(F^{-1}\) (see Figure 2) after generation. Notably, we have the following proposition to ensure that the equivariance is maintained under the proposed affine transformation with scalarization-based equivariant GNNs :

**Proposition 3.1**.: _Denote the invariant and equivariant outputs from a scalarization-based E(3)-equivariant GNN as \(f(\{_{i},}_{i}\})\) and \((\{_{i},}_{i}\})\), respectively. With the definition of \(F\) in Eq. 7, \( g E(3)\), we have \(f(\{_{i},F(}_{i})\})=f(\{_{i},F_{g}(g} _{i})\})\) and \(g F^{-1}((\{_{i},F(}_{i})\}))=F_{g}^{-1}(( \{_{i},F_{g}(g}_{i})\}))\), where \(F_{g}\) is derived on the coordinates transformed by \(g\). Namely, the E(3)-equivariance is preserved if we implement the GNN on the standard space and recover the original geometry from the outputs._

The proof is in Appendix B. This is vital since it indicates the Markov kernel is E(3)-equivariant, and thus ensures the E(3)-invariance of the probability density in the diffusion process . Note that our variational autoencoder (SS 3.2) and latent diffusion model (SS 3.4) are already designed to be equivariant even without the proposed affine transformation here. The purpose of defining such component is to encourage better generalization of the diffusion processes. Indeed, it is nontrivial to analyze whether such an implementation will break the equivariance of our workflow. Luckily, Proposition 3.1 manages to prove that scalarization-based equivariant networks , which is used in our autoencoder and diffusion model, are seamlessly compatible with such affine transformation, naturally preserving equivariance without any requirements of adaption.

### Geometric Latent Diffusion Model

With the aforementioned preparations, the discrete residue types are encoded as continuous latent representations \(\{_{i}\}\), and the full-atom geometry is also compressed and standardized into 3D vectors \(\{}_{i}\}(,)\). Therefore, we are ready to implement a diffusion model on the standard latent space to generate \(_{i}\) and \(}_{i}\). The forward diffusion process gradually adds noise to the data from \(t=0\) to \(t=T\), resulting in the prior distribution \((,)\). The reverse diffusion process generates data distribution by iteratively denosing the distribution from \(t=T\) to \(t=0\). We denote \(}_{i}^{t}=[_{i}^{t},}_{i}^{t}]\) and \(_{z}^{t}=\{(_{i}^{t},}_{i}^{t})\}\) as the intermediate state for node \(i\) and the entire peptide at time step \(t\), respectively. For simplicity, we assume both \(_{z}^{t}\) and the binding site \(_{b}\) are already standardized via the transformation \(F_{b}\) in Eq. 7. Then we have the forward process as:

\[q(}_{i}^{t}|}_{i}^{t-1}) =(}_{i}^{t};}}_{i}^{t-1},^{t}), \] \[q(}_{i}^{t}|}_{i}^{t}) =(}_{i}^{t};}^{t}} }_{i}^{0},(1-^{t})), \]

where \(^{t}\) is the noise scale increasing with the timestep from \(0\) to \(1\) conforming to the cosine schedule , and \(^{t}=_{s=1}^{s=t}(1-^{s})\). Then the state at timestep \(t\) can be sampled as:

\[}_{i}^{t}=^{t}}}_{i}^{t}+(1-^{t})_{i}, \]

where \(_{i}(,)\). Following Ho et al. , the reverse process can be defined with the reparameterization trick as:

\[p_{}(}_{i}^{t-1}|_{z}^{t},_{b})= (}_{i}^{t-1};}_{}(_{z}^{t},_{b}),^{t}), \]

\[}_{}(_{z}^{t},_{b})=}}(}_{i}^{t}-}{^{t}}} _{}(_{z}^{t},_{b},t)[i]), \]

where \(^{t}=1-^{t}\), and \(_{}\) is the denoising network also implemented with the equivariant adaptive multi-channel equivariant encoder in dyMEAN  to retain full-atom context of the binding site during generation and preserve the equivariance under affine transformations (Proposition 3.1). Finally, we have the objective at time step \(t\) as MSE between the predicted noise and the added noise in Eq. 10, as well as the overall training objective \(_{}\) as the expectation with respect to \(t\):

\[_{}=_{t(1 T)}[ _{i}\|_{i}-_{}(_{z}^{t},_{b},t)[i]\|^{2}/|_{z}^{t}|]. \]

### Training and Sampling

TrainingThe training of our PepGLAD can be divided into two phases where a variational autoencoder is first trained and then a diffusion model is trained on the standard latent space. We provide the overall training procedure in Algorithm 1 (see Appendix D). Note that a smooth and informative latent space is necessary for the consecutive training of the diffusion model, thus we resort to unsupervised data from protein fragments apart from the limited protein-peptide complexes for training the autoencoder, which we describe in Appendix E.

**Sampling in Ordered Subspace** The sampling procedure includes generative diffusion process on the standard latent states, recovering the original geometry with the inverse of \(F\) in Eq. 7, and decoding the sequence as well as the full-atom structure of the peptide (see Algorithm 2 in Appendix D). A problem here is that the unordered nature of graphs is not compatible with the sequential nature of peptides, thus the generated residues may have arbitrary permutation on the sequence order. Inspired by the concept of classifier-guided sampling , we first assign an arbitrary permutation \(\) on the sequence order to the nodes. Then we steer the sampling procedure towards the desired subspace conforming to \(\) with the following empirical classifier \(p(1|\{}_{i}^{t}\})\), which estimates the probability of the current coordinates belonging to the desired subspace:

\[p(1|\{}_{i}^{t}\}) =(-_{(i)-(j)=1}E(\|}_{i}^{t}-}_{j}^{t}\|)), \] \[E(d) =\{d-(_{d}+3_{d}),&d>_{d}+3 _{d},\\ (_{d}-3_{d})-d,&d<_{d}-3_{d},\\ 0,&,. \]

where \(_{d}\) and \(_{d}\) are the mean and variance of the distances of adjacent residues in the latent space measured from the training set. Intuitively, this classifier gives higher confidence if the adjacent (defined by \(\)) residues are within reasonable distances aligning with the statistics from the training set. Nevertheless, the effect of the guidance is relatively minor, which is only a technical trick to enhance the robustness. We provide more details in Appendix G.

## 4 Experiments

### Setup

**Task** We evaluate our PepGLAD and baselines on the following tasks: (1) **sequence-structure co-design** (SS4.2) aims to generate both the sequence and the structure of the peptide given the specific binding site on the receptor (_i.e._ protein). (2) **Binding Conformation Generation** (SS4.3) requires to generate the binding state of the peptide given its sequence and the binding site of interest.

**Dataset** We first extract all dimers from the Protein Data Bank (PDB)  and select the complexes with a receptor longer than 30 residues and a ligand between 4 to 25 residues . Then we remove the duplicated complexes with the criterion that both the receptor and the peptide has a sequence identity over 90% , after which 6105 non-redundant complexes are obtained. To achieve the cross-target generalization test, we utilize the large non-redundant dataset (LNR) from Tsaban et al.  as the test set, which contains 93 protein-peptide complexes with canonical amino acids curated by domain experts. We then cluster the data by receptor with a sequence identity threshold of over 40%, and remove the complexes sharing the same clusters with those from the test set. Finally, the remaining data are randomly split based on clustering results into training and validation sets, yielding a new benchmark calling **PepBench**. Further, we exploit 70k unsupervised data from protein fragments (**ProtFrag**) to facilitate training of the variational autoencoder. We also implement a split on **PepBDB** based on clustering results for evaluation. We show details and statistics of these datasets in Appendix E.

**Baselines** We first borrow three baselines from the antibody design domain. **HSRN** autoregressively decodes the sequence while keeps refining the structure hierarchically, from the \(_{}\) to other atoms. **dyMEAN** is equipped with an full-atom geometric encoder and exploits iterative non-autoregressive generation. **DiffAb** jointly diffuses on the categorical residue type, the coordinate of \(_{}\) as well as the orientation of each residue. Next, we explore two baselines from the general protein design. **RFDiffusion** exploits a pipeline that first generates the backbone via diffusion and then alternates between inverse folding  and structure refining based on a physical energy function . **AlphaFold 2** is the well-known model for protein folding, which also shows certain abilities on peptide conformation prediction . We also include two traditional methods. **AnchorExtension** designs peptides by first docking an existing scaffold to the binding site, and then optimizing the peptide with cycles of mutations guided by energy functions. **FlexPepDock** is designed for flexible peptide docking via optimization in the landscape of a physical energy function . Implementation details are provided in Appendix I.

### Sequence-Structure Co-Design

**Metrics** A favorable generative model should produce diverse candidates while maintaining fidelity to the desired distribution. To comprehensively evaluate the models, we generate 40 candidates for each receptor and employ the following metrics: (1) **Diversity**. Inspired by , we measure the diversity via unique clusters of sequences and structures. Specifically, we hierarchically cluster the structures based on pair-wise root mean square deviation (RMSD) of \(_{}\). The diversity of structures \(_{struct}\) is defined as the number of clusters versus the number of candidates. A similar procedure can be applied to the sequences to obtain \(_{seq}\), utilizing the similarity  derived from alignment . Then the co-design diversity is \(_{seq}_{struct}}\). (2) **Consistency**. We measure how well the models learn the 1D&3D joint distribution by the sequence-structure consistency, quantified via Cramer's V  association between the clustering labels (as in Diversity) of the sequences and the structures. High consistency indicates that candidates with similar sequences also have similar structures, implying that the generative model effectively captures the dependency between 1D and 3D. (3) \(\). Aligned with the literature [34; 44], we employ the binding energy (kcal/mol) provided by Rosetta , a widely-used suite for biomolecular modeling with physical energy functions, to evaluate the binding affinity of the generated candidates. Lower \( G\) indicates stronger binding between the peptide and the target. (4) **Success**. We report the proportion of successful designs (_i.e._\( G<0\), indicating no severe atomic clashes or twisted conformations) among all the candidates.

    &  &  \\   & Div.(\(\)) & Con.(\(\)) & \( G(\)) & Success & Div.(\(\)) & Con.(\(\)) & \( G(\)) & Success \\  Test Set & - & - & -35.25 & 95.70\% & - & - & -35.96 & 95.79\% \\  HSRN\({}^{3}\) & 0.158 & 0.0 & \( 0\) & 10.46\% & 0.111 & 0.0 & \( 0\) & 10.86\% \\ dyMEAN & 0.150 & 0.0 & -2.26 & 14.60\% & 0.150 & 0.0 & -1.92 & 6.26\% \\ DiffAb & 0.427 & 0.670 & -21.20 & 49.87\% & 0.269 & 0.463 & -18.40 & 41.45\% \\ PepGLAD (ours) & **0.506** & **0.789** & **-21.94** & **55.97\%** & **0.692** & **0.923** & **-21.53** & **48.47\%** \\   

Table 1: Evaluation on sequence-structure co-design. On each target, 40 candidates are generated for evaluation. Div. and Con. are abbreviations for diversity and consistency, respectively.

For all metrics except \( G\), we first compute values for each receptor individually and then average the results across different receptors. For \( G\), we identify the best candidate on each receptor as the outputs and report the median value across different receptors. Details about the metrics are provided in Appendix H, including discussion on AAR (Appendix H.1) and consistency (Appendix H.2).

**Results** Table 1 illustrates that our PepGLAD generates significantly more diversified and consistent peptides with better binding energy and success rates compared to the baselines. When benchmarking HSRN, dyMEAN, and DiffAb, which perform well on antibody CDR design, we observe a notable performance gap between non-diffusion baselines (_i.e._ HSRN, dyMEAN) and the diffusion-based baseline (_i.e._ DiffAb), suggesting the higher complexity in peptide design and the need for stronger modeling capabilities. Compared to DiffAb, which operates on categorical residue types, \(_{}\) coordinates and orientations, our PepGLAD (1) better captures the dependency between sequence and structure, as indicated by higher diversity and consistency, since diffusion is implemented on the latent space where the representation of sequence and structure are nicely correlated by the autoencoder; (2) more effectively captures the intricate protein-peptide interactions, demonstrated by better \( G\) and success rates, since we leverage the full-atom context of the binding site and enhances generalization capability by converting the geometry into a standard space. We showcase two candidates designed by our PepGLAD with favorable binding energy given by Rosetta in Figure 3. Furthermore, the diversity within successful designs is 0.632, which is higher than that of all designs (0.506), indicating the high structural flexibility of peptides upon successful binding.

We also evaluate our PepGLAD against two sophisticated pipeline systems in Table 2. The traditional method (_i.e._ AnchorExtension) is limited by low efficiency, thus we can only afford outputting 10 candidates for each receptor. For a relatively fair comparison with RFDiffusion, we refine the structure of the generated candidates using the empirical force field in RFDiffusion. However, the comparison may still disadvantage our PepGLAD, given that RFDiffusion is finetuned from a model pretrained on a large-scale dataset . Nevertheless, as demonstrated in Table 2, our model still exhibits marvelous superiority on diversity, consistency, and success rate, while achieving competitive binding energy \( G\), with obviously higher efficiency.

   Model & Div.(\(\)) & Con.(\(\)) & \( G()\) & Success & Time \\  AnchorExtension & 0.245 & 0.423 & -26.80 & 84.30\% & 735s \\ RFDiffusion & 0.259 & 0.696 & **-33.82** & 79.68\% & 61s \\ PepGLAD (ours) & **0.506** & **0.789** & -29.36 & **92.82\%** & **3s** \\   

Table 2: Evaluation on sequence-structure co-design with two well-established systems. Time cost is measured as the total time spent divided by the number of designed candiates.

Figure 3: **Top**: A generated candidate confined within the binding site (PDB=4cu4, \( G\)=-34.21). **Bottom**: A generated candidate with complementary shape to the binding site (PDB=3pkn, \( G\)=-33.32). Both candidates form compact interactions at the interface.

### Binding Conformation Generation

**Metrics** For each receptor, we generate 10 candidates and report the median value of the following metrics across different receptors to measure how well the generated distribution can recover the reference conformation: (1) \(_{_{}}\) : Root mean square deviation on the coordinates of \(_{}\) between a candidate and a reference structure with the unit A. (2) \(_{}\): RMSD on all atoms to measure the quality of the full-atom geometry. (3) \(\). A comprehensive metric evaluating the full-atom similarity on the interface between a candidate and a reference complex. It ranges from 0 to 1, with values above 0.23 and 0.49 considered as acceptable and medium quality, respectively.

**Results** As shown in Table 3, our PepGLAD surpasses all the baselines in terms of both \(_{_{}}\) and \(\) by a large margin, highlighting the superiority of incorporating the full-atom context and the binding-site shape into the latent diffusion process. Additionally, we present the distribution of the best \(_{_{}}\) on different test receptors using box plots and showcase a generated conformation highly resembling the reference in Figure 4. The distribution reveals that our model achieves favorable performance on \(_{_{}}\) with lower variance on the test set compared to other baselines, exhibiting robust generalization ability across disparate binding sites.

## 5 Analysis

We conduct the following ablations: the full-atom geometry (**Full-Atom**); the affine transformation (**Affine**); the unsupervised data from protein fragments (**ProtFrag**) and the mask policy (**Mask**) when training the autoencoder. Note that generative performance is assessed from various aspects, and improvement in one aspect at the disproportionate expense of others might be meaningless. Thus, we additionally compute the average of all the metrics to evaluate the comprehensive effect of each module, where \( G\) is normalized by the statistics on the test set. Table 4 demonstrates the following observations: (1) Discarding the full-atom context results in a significant degradation on all metrics, especially the success rate, implying the necessity of the full-atom context in capturing the intricate protein-peptide interactions; (2) Implementing the diffusion directly on the data space without the proposed affine transformation incurs a notably adverse impact on all metrics, indicating the remarkable enhancement on the generalization capability made by the affine transformation; (3) Training without the unsupervised data leads to a less informative latent space, exerting a negative effect on the binding energy and success rate; (4) Removal of the mask policy reduces the correlation between sequence and structure in the latent space, thus harms the consistency.

Figure 4: The distribution of \(_{_{}}\) on the test set of PepBench and a visualized sample.

    &  &  \\   & \(_{_{}}()\) & \(_{}()\) & \(()\) & \(_{}()\) & \(()\) \\  FlexPepDock & 6.43 & 7.52 & 0.393 & - & - & - \\ AlphaFold 2 & 8.49 & 9.20 & 0.355 & - & - & - \\ dyMEAN & 7.96 & 8.35 & 0.374 & 17.64 & 17.56 & 0.142 \\ HSRN & 6.02 & 7.59 & 0.508 & 9.28 & 9.72 & 0.394 \\ DiffAb & 4.23 & 7.60 & 0.586 & 13.96 & 13.12 & 0.236 \\  PepGLAD (ours) & **4.09** & **5.30** & **0.592** & **8.87** & **8.62** & **0.403** \\   

Table 3: Evaluation on binding conformation generation. On each target, 10 candidates are generated to calculate the optimal recall of the reference conformation.

## 6 Limitations

Despite the promising results, we acknowledge several limitations which might be addressed by future work. First, the binding affinity assessment relies on the Rosetta scoring function as a proxy for wetlab experiments. There may be discrepancies between the predicted and actual binding energies. The ultimate test of a peptide utility is its performance _in vivo_, which is too costly for large-scale evaluation. Nevertheless, this is a problem confronting the entire community, and we hope future research might propose more reliable _in silico_ proxies to bridge the gap. Second, while this paper addresses peptide design from the aspect of proteins, it might also be reasonable to think from the aspect of small molecules if the peptides are short enough. Under such circumstances, it is also beneficial to further explore counterparts of methods for small molecule design [43; 52; 39], which we leave for future work.

## 7 Conclusion

In this paper, we first assemble a dataset from Protein Data Bank (PDB) and literature to benchmark generative models on target-specific peptide design in terms of diversity, consistency, and binding energy. Subsequently, we propose PepGLAD, a powerful diffusion-based model for full-atom peptide design. In particular, we explore diffusion on the latent space where the sequence and the full-atom structure are jointly encoded by a variational autoencoder. We further propose a receptor-specific affine transformation technique to project variable geometries in the data space into a standard space, which enhances the transferability of diffusion processes on disparate binding sites. Our PepGLAD outperforms the existing models on sequence-structure co-design and binding conformation generation, exhibiting high generalization across diverse binding sites. Our work represents a pioneering effort in the exploration of deep generative models for simultaneous design of 1D sequences and 3D structures of peptides, which could inspire future research in this field.

## Software and Data

The curated PepBench and ProtFrag are available at [https://zenodo.org/records/13373108](https://zenodo.org/records/13373108). The codes for our PepGLAD are open-sourced at [https://github.com/THUNLP-MT/PepGLAD](https://github.com/THUNLP-MT/PepGLAD).

## Impact Statements

This paper aims to advance the field of peptide design through the construction of a benchmark and the development of a novel latent diffusion model, PepGLAD, which addresses key limitations in current methods. Our work represents a step forward in computational peptide design, with the potential to impact both scientific research and practical applications in various domains. For instance, more precise peptide design could lead to enhanced drugs in the pharmaceutical industry, and could facilitate the creation of new biomaterials, sensors, and other innovative technologies in biology and materials science. We wish our paper could inspire future research in this field.