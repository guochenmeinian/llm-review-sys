# CaptainCook4D: A Dataset for Understanding Errors

in Procedural Activities

 Rohith Peddi

Shivrat Arya

Bharath Challa

Likhitha Pallapothula

Akshay Vyas

Bhavya Gouripeddi

Qifan Zhang

Jikai Wang

Vasundhara Komaragiri

Eric Ragan

Nicholas Ruozzi

Yu Xiang

Vibhav Gogate

Corresponding Author, \(=\) UT Dallas, \(=\) University of Florida website: https://captaincook4d.github.io/captain-cook/

###### Abstract

Following step-by-step procedures is an essential component of various activities carried out by individuals in their daily lives. These procedures serve as a guiding framework that helps to achieve goals efficiently, whether it is assembling furniture or preparing a recipe. However, the complexity and duration of procedural activities inherently increase the likelihood of making errors. Understanding such procedural activities from a sequence of frames is a challenging task that demands an accurate interpretation of visual information and the ability to reason about the structure of the activity. To this end, we collect a new egocentric 4D dataset **CaptainCook4D** comprising 384 recordings (94.5 hours) of people performing recipes in real kitchen environments. This dataset consists of two distinct types of activities: one in which participants adhere to the provided recipe instructions and another in which they deviate and induce errors. We provide 5.3K step annotations and 10K fine-grained action annotations and benchmark the dataset for the following tasks: error recognition, multi-step localization and procedure learning2.

## 1 Introduction

_Have you ever excitedly prepared your favourite meal after a long day, only to be disappointed upon realizing you missed a key ingredient?_ Such scenarios are common because performing long step-by-step procedures increases the likelihood of making errors. While some errors are harmless and can be corrected with little consequence, others can have detrimental consequences, particularly those that occur during medical procedures or complex chemical experiments. Therefore, there is a pressing need to build AI systems that can guide users in performing procedural activities .

A key problem we need to solve in order to build such AI systems is **procedural activity understanding**, a challenging and multifaceted task that demands **interpreting what is happening** --specifically, determining whether the person is following the procedure correctly or making an error, **anticipating what will happen**, and **planning the course of action** to accomplish the goal. For effective interpretation, the system must be capable of recognizing and categorizing actions while assessing the current state of the environment. To anticipate what might happen next, it should be able to forecast actions right from the start of the interaction or even before it begins. Additionally, planning a course of action necessitates understanding the potential consequences of these actions. Numerous datasets have been developed to improve our understanding of procedural activities. However, these datasets only include videos of individuals performing step-by-step tasks correctly without making any errors.

But, for AI systems to effectively identify errors in procedural activities, it is essential to have datasets that include both normal and error videos along with corresponding error annotations (descriptions).

**Contributions.** We introduce an egocentric3 4D dataset designed to enhance AI systems' understanding of procedural activities and improve their ability to recognize and anticipate errors.

* Our dataset features participants performing recipes in real kitchen environments (Fig. 1). It includes two distinct types of activities: one where the participants follow the given recipe guidelines and another where they deviate (intentionally or unintentionally), making errors.
* We provide annotations for (a) Start and end times for each step of the recipe, (b) Start and end times for each fine-grained action/interaction for 20% of the collected data, and (c) Detailed descriptions of the errors made by participants, which allowed us to compile a comprehensive overview of different error categories along with their brief explanations.
* We provide baselines for the following procedure understanding tasks: (a) Error Recognition (supervised and zero-shot), (b) Multi-Step Localization, and (c) Procedure Learning.

## 2 Related Work

Understanding procedural activities with errors has witnessed significant traction recently and spurred the development of new datasets (see Table 1) that aid in developing novel approaches to recognize errors. Our dataset sets itself apart from others4 by four distinctive features: (1) **Domain:** While others address errors during assembly and disassembly, we focus on cooking activities5. (2) **Environment:** Unlike lab environments, we collected our dataset in real kitchen environments. (3) **Multimodal capabilities**, and (4) **Error diversity**. A complete survey of all the relevant tasks is outside the scope of the paper; thus, we provide a brief review of procedure understanding tasks that are of particular interest to the proposed dataset and discuss their representative works.

**Temporal Action Localization (TAL)** in videos aims to identify and classify temporal boundaries of action instances in long video sequences. TAL methods can be categorized into two primary approaches: two-stage and single-stage. Two-stage methods operate in a sequential manner by initially generating action proposals and subsequently classifying them. In contrast, single-stage methods streamline the process by simultaneously performing action localization and classification,

Figure 1: **Overview.** Top: We constructed task graphs for the selected recipes. These graphs facilitated sampling topological orders (cooking steps) that participants followed to perform. During the execution of these steps, participants induced errors that were both **intentional** and **unintentional** in nature. Bottom Left: We present the sensors employed for data collection. Bottom Right: We describe the details of the modalities of the data collected while the participant performs the recipe.

[MISSING_PAGE_FAIL:3]

**Recipes.** We curated a selection of 24 cooking recipes sourced from WikiHow (refer to Appendix C), specifically focusing on recipes with a preparation time of 30 minutes or less. These recipes encompassed a wide range of culinary traditions, showcasing the diversity of cooking styles in various cuisines. Our primary objective was to identify and capture potential errors that could arise from using various authentic cooking instruments in preparing recipes sampled from different cuisines.

**Task Graphs.** visually represents the sequential steps required to complete a recipe. Each node in the task graph (for a recipe) corresponds to a step in a recipe, and a directed edge between a node \(x\) and a node \(y\) in the graph indicates that \(x\) must be performed before \(y\). Thus, a task graph is a directed acyclic graph, with a topological order representing a valid recipe completion. To construct task graphs for selected recipes, we identified all the critical steps involved and determined their inter-dependencies, thus establishing a topological order of tasks (see website for final task graphs).

### Protocol

Our dataset was compiled by eight participants7 in 10 different kitchens. Each participant was provided a tablet-based recording interface accessible through a web browser, a GoPro and a Hololens2. Participants were instructed to adjust their GoPro cameras to capture footage in 4K resolution at 30 fps to ensure high-quality video. The HoloLens2 device was programmed to stream RGB frames at 360p resolution and 30 fps. It also streamed depth frames in Articulated Hand Tracking mode, referred to as _depth_ahat_ mode. Besides visual data, the device also streamed three streams of IMU (Inertial Measurement Unit) sensor data and spatial data, capturing both head and hand poses8.

**Normal Recordings.** A recording is classified as a **normal recording** when it is captured as the participant accurately follows the procedure described in the recipe. Participants are presented with one of the pre-constructed topological orders of the selected recipe9, as determined by the task graphs. The participants then follow and perform each step from the topological order sequentially.

**Error Recordings.** A recording is classified as an **error recording** when it is captured while the individual deviates from the recipe's procedure, thereby inducing errors. Following the terminology used in scientific disciplines such as neuroscience  and chemistry, we will refer to deviations from procedures as _errors10_. Following , we classified common errors performed during a cooking activity into the following categories: (1) Preparation Error, (2) Measurement Error, (3) Technique Error, (4) Timing Error, (5) Temperature Error, (6) Missing Steps, and (7) Ordering Errors.

**Error Induction.** We developed three strategies11 for participants to choose from, each tailored to perform the recipe in a specific environment. After choosing the strategy, participants were given detailed instructions on how to perform the recipes. We list the strategies presented to the participants (1) **Impromptu**: Participants were asked to induce errors while performing the recipe. Following the completion of each recording, participants used a web-based interface to update the errors they performed during each step. Due to the complex nature of cooking activities and the lack of experience of the participants in cooking, many errors induced in this strategy were **unintentional** (Figure 2 presents one such example). (2) **Disordered Steps**: Participants were given pre-prepared error scripts with missing steps and ordering errors. (3) **Induct Error**: Participants used a web-based

Figure 2: **Snapshots** of steps and recorded errors while preparing the recipe _Cucumber Raita_. Three of the four errors were intentional, but the participant missed the _Peeling_ step **unintentionally**.

### Data Annotation

We implemented a dual-layer review process to guarantee high-quality annotations. Each video was first annotated by the person who recorded it and then reviewed for accuracy by a second reviewer. Thus ensuring that all errors were correctly captured in the annotations corresponding to each step. Our annotations are structured to provide detailed insights into the recorded actions, facilitating both coarse-grained and fine-grained action analyses. Specifically, we offer the following annotations: (1) **Coarse-Grained Actions:** We mark the start and end times of each step in a recording of the recipe. (2) **Fine-Grained Actions:** For 20% of our data, we provide fine-grained action annotations to support semi/weakly supervised learning techniques for action recognition. (3) **Error Descriptions:** For each step, if an error occurs during its execution, we link its step annotation with the specific category of the error and a description of the error, thus enabling a comprehensive understanding.

**Coarse-Grained Action/Step Annotations.** We designed an interface for annotating steps in Label Studio12. Annotators are presented with this interface to mark each step's start and end times. Our coarse-grained actions/steps are significantly longer than a single fine-grained action and encompass multiple such fine-grained actions to perform the described step successfully. For example, to

Figure 4: **Statistics.** Top: We present video and step duration statistics to the left & right respectively. Bottom: We present the total count and the durations of normal and error recordings for each recipe. interface to create an error script for each selected recipe recording. The modified recipe steps were displayed on a tablet, enabling participants to perform according to their scripted errors. In Fig. 4, we present both general statistics of the dataset and specific statistics of normal and error recordings.

Figure 3: **Error Categories.** Left: We present a categorization of participant-induced errors derived from the annotated error descriptions of the recordings. Right: We display frames captured from various recordings, highlighting correct and erroneous executions. Bottom Right: We present statistics on the error categories in the dataset derived from the compiled annotations of all recordings.

accomplish the step _[Chop a tomato]_, we include the following in the annotation (1) **Pre-conditional actions:**_[opening refrigerator, grabbing a polythene bag of tomatoes, taking a tomato, placing the tomato on cutting board, close fridge]_ (2) **Post-conditional actions:**_[placing down the knife, grabbing the polythene bag of tomatoes, opening refrigerator and placing the bag in the refrigerator]_.

**Fine-Grained Action Annotations.** Inspired by the pause-and-talk , we have developed a web-based tool for fine-grained action annotations using Whisper  (for speech-to-text translation).

**Error Category Annotations.** Following each recording, participants were also asked to categorize errors performed in each step based on a set of guidelines. Specifically, we ask participants to broadly classify an error as a (1) _Preparation Error_ when they use soiled/wrong ingredients or use different tools, (2) _Measurement Error_ when they use wrongly measured ingredients, (3) _Timing Error_ when they perform a step in shorter or longer duration than what is prescribed (e.g. Microwave for Microwave instead of 30 seconds) (4) _Temperature Error_ when they set higher/lower power levels in the microwave or on a stove than what is prescribed (5) _Missing Step_ when they omit to perform a step (6) _Technique Error_ when they perform the required action incorrectly, leading to a wrong outcome than expected. (7) _Order Error_ when they execute steps out of the required sequence. We compile and present the categorization of errors, their descriptions and visual illustrations in Fig. 3.

## 4 Experiments

Our experiments are designed to address the following questions: (Q1) What is the efficacy of transfer learning in recognizing errors? (Q2) How effective are current Vision Language Models (VLMs) in zero-shot error recognition? (Q3) How do state-of-the-art Multi-Step Localization methods perform on our dataset, particularly in terms of robustness to technique errors? (Q4) How do current self-supervised procedure learning methods in literature perform when applied to our dataset13?

**Features.** To answer the above questions we trained14 our proposed baseline models on features obtained using pre-trained models such as 3D-ResNet , SlowFast , X3D , VideoMAE , Imagebind  and Omnivore  which were originally trained for video recognition tasks. Specifically, we split each video into 1-second sub-segments and extracted features to train models.

### Error Recognition

This section answers questions (Q1) and (Q2); specifically, we address **Q1** by formulating the error recognition task as a _supervised binary classification_ problem. We proposed three architectural variants (Fig. 5) as our baseline models and trained them using video/multimodal features. To address **Q2**, we employed a _prompt-and-predict_ paradigm to recognize errors in activity recordings. Specifically, we formulated the problem as a Video Question Answering task (Fig. 6), crafted targeted question prompts using task graphs and error annotations(Fig.3); supplied these engineered prompts along with the videos as input to a VLM and evaluated its performance in zero-shot error recognition.

**Supervised Error Recognition (SupervisedER).** We utilized the features extracted using pre-trained models to train variants of our baseline binary classification models and evaluated trained models using the standard metrics such as accuracy, precision, recall, F1 and AUC (see Table 2). Specifically, we trained our models to classify each step of a video into one of two classes {_error(1)_, _normal(0)_}. We constructed two data splits, step and recording splits, for training error recognition models. For the step split, we first compiled a dataset of video segments corresponding to all steps of all recipes in the dataset. Then divided it into train, validation, and test subsets. For the recordings split, we compiled all the recordings of all recipes in the dataset and divided the dataset into train, validation, and test subsets. Using error annotations (Figure 3), we first generated class labels for all video segments corresponding to the recipe steps. Then, we assigned the class label corresponding to the step to all 1-second sub-segments within the step and trained baseline models. During inference, we assigned the majority class label of the sub-segments corresponding to a step as the label to that step.

We proposed three architectural variants as baselines:{\(_{1}\), \(_{2}\), \(_{3}\)} (see Fig. 5). In \(_{1}\), we used the extracted features and constructed labels as described above and trained a Multi-Layer Perceptron (MLP) head. This approach assesses the efficacy of visual cues identified by pre-trained video recognition models in recognizing errors in sub-segments. In \(_{2}\), we shifted our focus from sub-segment prediction and trained a transformer that processed all video sub-segments corresponding to each step. This method is designed to capitalize on the long-term temporal cues present within the video segments of recipe steps to enhance the prediction performance of the trained models. In variant \(_{3}\), we harnessed the multimodal data of recordings and trained a unified transformer model. This approach employed an attention mechanism to integrate information from all modalities of data.

**Insights.** Our \(_{2}\) models consistently outperformed \(_{1}\) models. Incorporating additional data modalities like audio, text, and depth into our \(_{3}\) models significantly improved their performance. Our omnivore-based \(_{2}\) models performed similarly to our \(_{3}\) models trained using Imagebind15 features.

**Zero-Shot Error Recognition (ZeroShotER).** We proposed two baseline variants, \(_{1}\) and \(_{2}\), for ZeroShotER16 using prompt-and-predict architectures, as illustrated in Fig. 6. Both variants involve constructing query prompts for each recipe step and querying VLMs using these prompts along with video inputs. We utilized state-of-the-art17 open-source VLMs, Video-LLaVa  and TimeChat , to recognize errors and reported the evaluation results using standard metrics, such as accuracy, precision, recall, and F1 scores. Specifically, for \(_{1}\), we leveraged the associated task graphs of each recipe and generated a prefix question prompt for each step. These prompts were then used to query VLMs to recognize errors in videos. We employed a prompt-ensembling approach in \(_{2}\) to recognize errors in a shift from the single-prompt strategy. Specifically, we designed prompt templates tailored to each error category (refer to Appendix B for examples). Using Llama3 , we generated a set

  
**VLM** & **Variant** & **Acc** & **P** & **R** & **F1** \\  Video-LLaVa  & \(_{1}\) & \(_{1}\) & 64.3 & 34.2 & 3.9 & 6.7 \\   & & \(_{2}\) & 52.85 & 36.3 & 3.9 & **3.1** & **7.1** \\  TimeChat  & \(_{1}\) & 65.0 & 51.1 & 11.5 & 2.6 \\  & & \(_{2}\) & 43.5 & 34.8 & 69.7 & **3.1** \\   

Table 3: **ZeroShotER evaluation results.**

Figure 5: **SupervisedER architectures of 3 baselines.**

  
**Spid** & **Backbone** & \(_{3}\) & **Modality** & **Acc** & **P** & **R** & **F1** \\   &  & _{1}\)} &  & 71.9 & 66.07 & 14.86 & 24.26 & 24.26 \\  & & & & & 69.96 & 51.56 & 59.84 & **6.1** \\   & & & & 33.54 & 31.88 & 69.6 & 47.16 & 53.06 \\   & & & & 68.09 & 29.49 & 27.22 & 67.18 \\   & & XD & \(_{1}\) & & & 66.34 & 48.19 & 12.82 & 27.51 & 61.09 \\  & & & & 67.87 & 42.86 & 9.64 & 15.74 & 61.53 \\   & & & & 63.45 & 43.52 & 53.21 & 47.1 & 61.66 \\   & & & & 61.38 & 44.14 & 56.63 & 47.88 & 64.5 \\   & & & & & 62.88 & 43.86 & 31.33 & 50.51 & 57.03 \\  & & & & A & 43.86 & 32.76 & 26.99 & 44.69 & 52.23 \\  & & & & & A & 43.82 & 36.96 & 39.96 & 39.84 \\  & & & & & V.K & 48.79 & 42.76 & 41.66 & 42.36 & 61.1 \\  & & & & & & 69.47 & 50.75 & 49.66 & **6.1** \\  _{2}\)} &  & _{1}\)} &  & 59.76 & 45.31 & 56.09 & 59.91 & **6.030** \\  & & & & & 62.53 & 40.53 & 53.91 & 39.94 & **62.27** \\   & & & & & & 69.06 & 48.52 & 49.93 & 56.39 \\   & & & & & & & 69.09 & 48.52 & 49.93 \\   & & & & & & & 69.09 & 48.09 & 48.12 & 54.66 \\   & & & & & & & 69.09 & 48.09 & 48.75 & 56.58 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.24 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.20 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.27 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.27 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.27 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.20 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.27 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.09 \\  _{2}\)} &  &  &  & 37.56 & 36.14 & 96.27 & 52.55 & 54.1 \\  & & & & & & & 69.09 & 48.09 & 48.09 & 48.12 \\   & & & & & & & 69.09 & 48.09 & 48.09 & 48.12 \\    & & & & & & & 69.09 & 48.09 & 48.09 & 48.20 \\    & & & & & & & 69.09 & 48.09 & 48.09 & 48.11 \\    & & & & & & & 69.09 & 48.09 & 48.11 & 58.04 \\   

Table 2: **SupervisedER evaluation results of baselines.**

Figure 6: **ZeroShotER evaluation pipeline of VLMs**

[MISSING_PAGE_FAIL:8]

### Procedure Learning

Given long, untrimmed videos of procedural activities where the sequences of steps can be performed in multiple orders, self-supervised procedure learning methods aim to identify relevant frames across videos of an activity and estimate the sequential steps required to complete the activity. In this section, we address the question **Q4** by simultaneously answering _(a) Can we infer the underlying procedure (recipe text) from the videos of a particular recipe and (b) How does the self-supervised procedure learning methods in literature perform on the proposed dataset?_. Specifically, we answered both parts by comparing the performance of our models trained on the proposed dataset using self-supervised procedure learning methods [3; 16] against the random setting defined by EgoProceL (see Tab. 6). We followed the setup described in EgoProceL and trained two embedder networks, one using the Cycleback Regression loss (\(\)) [\(_{1}\)]  and the other using a blend of two loss functions: Cycleback Regression loss (\(\)) and Contrastive - Inverse Difference Moment loss (\(\)) [\(_{2}\)]. The combined loss function is \(+\), where \(\) is a hyperparameter. While we exclusively used these loss functions to train the embedder networks, we continued using the Pro-Cut Module to categorize frames into key steps. We presented evaluation results for 5 recipes Tab. 6 and all recipes in App. B.

**Insights.** Our models significantly outperformed the predefined random setting, demonstrating the feasibility of inferring procedural steps from our dataset. However, these models scored lower on our dataset compared to existing procedure learning datasets. We believe this drop in performance is mainly due to our dataset's unique challenge, which includes videos with longer key step durations.

**Additional Results.** We provide several analyses in Appendix B, including (a) Error Category Recognition, (b) Early Error Recognition, (c) Anomaly Detection and (d) Ablation studies for MSL.

## 5 Discussion, Limitations and Future Work

**Discussion.** We introduced a novel egocentric dataset for understanding errors in procedural activities. Our dataset consists of synchronized egocentric views, audio, and depth information specifically designed for tasks such as 3D activity analysis, Procedure Learning, Error Recognition, and more. While current methods have yielded promising outcomes, they continue to struggle to tackle these challenges adequately with satisfactory results, as demonstrated by our experimental assessment. This indicates the need for further exploration in this domain.

**Limitations.** We aimed to capture deviations during procedural activities from an egocentric perspective. Since such data cannot be sourced from crowd-sourced platforms, we captured participant data while performing procedural activities. By the nature of the problem, errors that occur when performing procedural activities are combinatorial and can have a compounding effect. Thus, our work has the following limitations: (1) For each activity, the errors captured and presented in the dataset form a subset of the whole combinatorial space; (2) Capturing 4D data in real kitchen environments posed logistical and equipment training challenges. As a result, we were compelled to limit the data collection to a specific geographic area.

**Future Work.** Our work opens up several avenues for future work. First, an exciting direction is the extension of the dataset to include activities from other domains. By incorporating tasks such as executing hardware-related activities (e.g., working with cars or computer parts), the dataset can encompass a wider range of activities. Second, the dataset can be used to compare and develop methods for solving various tasks such as Few-Shot Error Recognition using visual/textual prompts, Semantic Role Labelling, Long Video Understanding, Procedure Planning, Reducing Errors, etc.

    &  & _{1}\)} & _{2}\)} \\   & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  BlenderBananPancakes & 7.40 & 3.83 & 2.26 & 12.65 & 9.50 & 5.16 & 15.54 & 9.96 & 5.72 \\ Coffee & 6.54 & 3.87 & 2.17 & 13.68 & 9.91 & 5.49 & 15.76 & 10.25 & 5.63 \\ MugCake & 5.45 & 4.00 & 2.12 & 16.12 & 12.95 & 6.87 & 10.32 & 8.85 & 4.40 \\ PanFriedTofu & 5.35 & 3.97 & 1.54 & 8.86 & 10.39 & 3.75 & 9.34 & 12.44 & 3.87 \\ Pinwheels & 6.54 & 4.28 & 2.13 & 13.58 & 11.96 & 5.92 & 16.08 & 13.06 & 7.05 \\ 
**Average of 24 recipes** & **7.61** & **3.92** & **2.22** & **15.62** & **10.85** & **5.78** & **15.78** & **10.68** & **5.82** \\   

Table 6: **Procedure Learning.** Here, \(\) represents precision, \(R\) represents recall, and \(I\) represents IOU.