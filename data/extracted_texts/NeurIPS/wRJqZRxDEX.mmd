# Critical Initialization of Wide and Deep Neural Networks using Partial Jacobians: General Theory and Applications

Critical Initialization of Wide and Deep Neural Networks using Partial Jacobians: General Theory and Applications

Darshil Doshi

ddoshi@umd.edu

Equal Contributions

Tianyu He

T Condensed Matter Theory Center & Department of Physics, University of Maryland, College Park, MD 20740

Andrey Gromov

Meta AI, Menlo Park, CA 94025

###### Abstract

Deep neural networks are notorious for defying theoretical treatment. However, when the number of parameters in each layer tends to infinity, the network function is a Gaussian process (GP) and quantitatively predictive description is possible. Gaussian approximation allows one to formulate criteria for selecting hyperparameters, such as variances of weights and biases, as well as the learning rate. These criteria rely on the notion of criticality defined for deep neural networks. In this work we describe a new practical way to diagnose criticality. We introduce _partial Jacobians_ of a network, defined as derivatives of preactivations in layer \(l\) with respect to preactivations in layer \(l_{0} l\). We derive recurrence relations for the norms of partial Jacobians and utilize these relations to analyze criticality of deep fully connected neural networks with LayerNorm and/or residual connections. We derive and implement a simple and cheap numerical test that allows one to select optimal initialization for a broad class of deep neural networks; containing fully connected, convolutional and normalization layers. Using these tools we show quantitatively that proper stacking of the LayerNorm (applied to preactivations) and residual connections leads to an architecture that is critical for any initialization. Finally, we apply our methods to analyze ResNet and MLP-Mixer architectures; demonstrating the everywhere-critical regime. 1

## 1 Introduction

When the number of parameters in each layer becomes large, the functional space description of deep neural networks simplifies dramatically. The network function, \(f(x)\), in this limit, is a Gaussian process [37; 30] with a kernel - sometimes referred to as neural network Gaussian process (NNGP) kernel  - determined by the network architecture and hyperparameters (_e.g_ depth, precise choices of layers and the activation functions, as well as the distribution of weights and biases). A similar line of reasoning was earlier developed for recurrent neural networks . Furthermore, for special choices of parameterization and MSE loss function, the training dynamics under gradient descent can be solved exactly in terms of the neural tangent kernel (NTK) [27; 31]. A large body of work was devoted to the calculation of the NNGP kernel and NTK for different architectures, calculation of the finite width corrections to these quantities, and empirical investigation of the training dynamics of wide networks [39; 54; 25; 14; 3; 33; 1; 18; 19; 45; 58; 47; 6; 5; 32; 63; 61; 60; 59; 35; 17; 2; 49; 34].

One important result that arose from these works is that the network architecture determines the most appropriate initialization of the weights and biases [44; 46; 30]. To state this result, we consider networks with/without LayerNorm  and residual connections ; the preactivations for which can be defined as follows

\[h_{i}^{l+1}(x)=_{j=1}^{N_{l}}w_{ij}^{l+1}(_{j}^{l}(x))+b_{i}^{ l+1}+ h_{i}^{l}(x)\,,\] (1)

where \(_{j}^{l}=(h_{j}^{l})\) and the parameter \(\) controls the strength of residual connections. For the input layer: \(h_{i}^{1}(x)=_{j=1}^{N_{0}}w_{ij}^{1}x_{j}+b_{i}^{1}\). In the \((l+1)\)-th layer, weights \(w_{ij}^{l+1}^{N_{l+1} N_{l}}\) and biases \(b_{i}^{l+1}^{N_{l+1} 1}\) are taken from normal distributions \((0,_{w}^{2}/N^{l})\) and \((0,_{b}^{2})\), respectively. Hyperparameters \(_{w}\) and \(_{b}\) need to be tuned. \(()\) is the activation function and \(x^{N_{0} 1}\) is the input. For results discussed in this work, \(x\) can be sampled from either a realistic (_i.e._ highly correlated) dataset or a high entropy distribution.

For a network of depth \(L\), the network function is given by \(f(x)=h^{L}(x)\). Different network architectures and activation functions, \(\), lead to different "optimal" choices of \((_{w},_{b})\). The optimal choice can be understood, using the language of statistical mechanics, as a critical point (or manifold) in the \(_{b}\)-\(_{w}\) plane. The notion of criticality becomes sharp as the network depth, \(L\), becomes large. Criticality ensures that NNGP kernel and the gradients' norm remain \(O(L^{0})\) as the network gets deeper . Very deep networks will not train unless initialized critically, since the gradients explode or vanish exponentially. Note that high trainability does not imply that the trained model has great performance (test accuracy) after training.

### Results

Here we focus on two main results of this work: (i) empirical method to check criticality of a neural network and (ii) an architecture based on layer normalization and residual connections that is critical for _any_ initialization. First we introduce the notion of a partial Jacobian.

**Definition 1.1**.: Let \(h_{i}^{l}(x)\) be preactivations of a neural network \(f(x)\). The partial Jacobian \(J_{ij}^{l_{0},l}\) is defined as derivative of preactivations at layer \(l\) with respect to preactivations at layer \(l_{0} l\)

\[J_{ij}^{l_{0},l}(x)=^{l}(x)}{ h_{i}^{l_{0}}(x)}\,.\] (2)

The partial Jacobian is a random matrix with vanishing mean at initialization. Fixing \(l_{0}\) and varying \(l\) allows us to study the behavior of Jacobian with depth. On the other hand, varying both \(l_{0}\) and \(l\) is necessary to study general networks with non-repeating building blocks. Next, we introduce a deterministic measure of the magnitude of \(J_{ij}^{l_{0},l}\) -- its squared Frobenius norm, averaged over parameter-initializations.

**Definition 1.2**.: Let \(J_{ij}^{l_{0},l}\) be a partial Jacobian of a neural network \(f(x)\). Averaged partial Jacobian norm (APJN)5 is defined as

\[^{l_{0},l}(x)_{}[}_{ j=1}^{N_{l}}_{i=1}^{N_{l_{0}}}(^{l}(x)}{  h_{i}^{l_{0}}(x)})^{2}]\,,\] (3)

where \(_{}\) indicates averaging over parameter-initializations.

APJN is a dominant factor in the magnitude of the gradients. In what follows, we show that criticality, studied previously in the literature, occurs when APJN either remains finite, or varies _algebraically_ as \(l\) becomes large. To prove this we derive the recurrence relation for \(^{l_{0},l}(x)\) in the limit \(N_{l}\) and analyze it at large depth. Algebraic behavior of APJN with depth is characterized by an architecture-dependent critical exponent, \(\), so that \(^{l_{0},l}(x) l^{-}\). Such behavior is familiar from statistical mechanics when a system is tuned to a critical point . Away from criticality, there are two phases:ordered and chaotic. In the ordered phase APJN vanishes exponentially with depth, whereas in the chaotic phase APJN grows exponentially

\[^{l_{0},l} c_{l_{0}}e^{}\,.\] (4)

Here \(\) is the correlation length. It characterizes how fast gradients explode or vanish. (See Section 2.1 for further discussion.)

**Theorem 1.3** (Main result).: _Let \(f(x)\) be a deep MLP network with Lipschitz continuous activation \(()\). Assume that LayerNorm is applied to preactivations and there are residual connections with strength \(\) acting according to (1). In the sequential limit \((N_{1},,N_{l},,N_{L-1})\), the correlation length with a large \(L\) can be written as_

\[=)+^{2}]|}\,,\] (5)

_where the non-negative constants \(A\) and \(B\) are given by_

\[A =_{w}^{2}_{}[^{}(_{ k}^{L-2})^{2}]\,, B =_{w}^{2}_{}[(_{k}^{L-2})^{2} ]+_{b}^{2}\,,\]

_and \(^{}()\) is the derivative of \(()\). We omitted the average over the neuron index \(k\) in the infinite width limit, and the result does not depend on \(k\)._

_Remark 1.4_.: As \(\) increases, the dependence on the initialization \((_{b},_{w})\) gets weaker. When \(=1\), the correlation length diverges; and the network is critical for **any** initialization, with \(=O(1)\).

In practice, Theorem 1.3 and Remark 1.4 imply that different choices of initialization bear no effect on trainability of the network provided that LayerNorm and residual connections are arranged as stated.

### Related Work

Some of our results were either surmised or obtained in a different form in the literature. We find that LayerNorm ensures that NNGP kernel remains finite at any depth as suggested in the original work of Ba et al. . LayerNorm also alters the criticality of \(^{l_{0},l}(x)\). It was noted in Xu et al.  that LayerNorm (applied to preactivations) regularizes the backward pass. We formalize this observation by showing that LayerNorm (applied to preactivations) dramatically enhances correlation length (which is not the case for LayerNorm applied to activations). This can be seen from Theorem 1.3, setting \(=0\). When residual connections of strength \(1\) are combined with erf (or any other \(\)-like activation function, e.g. \(\)), the neural network enters a _subcritical_ phase with enhanced correlation length (see 4.3). A version of this result was discussed in Yang and Schoenholz . When residual connections are introduced on top of LayerNorm, the correlation length \(\) is further increased. If residual connections have strength \(=1\) the network enters a critical phase for _any_ initialization. Importance of correct ordering of LayerNorm, residual connections and attention layers was discussed in Xiong et al. . Benefit of combining BatchNorm and residual connections was mentioned in . Several architectures with the same order of GroupNorm and residual connections were investigated in Yu et al. . Existing initialization schemes such as He initialization6, Fix-up  and ReZero  are special cases of our method. They conform to the notion of criticality using APJN, as defined in our work.

The partial Jacobian has been used to study generalization bounds in Arora et al. . The Jacobian norm (_i.e._\(||J_{ij}^{0,l}||^{2}\)) of trained feed-forward neural networks was studied in Novak et al. , where it was correlated with generalization. Partial Jacobians with \(l_{0}=l-1\) were studied in the context of RNNs , referred to as _state-to-state_ Jacobians. Furthermore, the relation between Partial Jacobians and Lyapunov exponents, as well as their impact on trainability, have been explored for RNNs .

As the aspect ratio (\(L/N\)) of the network approaches 1, the finite width corrections to the Jacobian become more prominent. On the other hand, even with a small aspect ratio, the effect of the spectral density of Jacobian becomes important as the depth \(L\) becomes _very_ large. Pennington et al.  studied the spectrum of the input-output Jacobian for MLPs. Xiao et al.  extended the analysis to CNNs, showing that _very_ deep vanilla CNNs can be trained by achieving "dynamical isometry".

Recurrence Relations

Here we derive the infinite width recurrence relations for the APJN and the NNGP kernel. We use Lemma 2.2 to derive NNGP kernel recurrence relation, and leverage that to get the recurrence relation for APJN. Fixed point analyses of these relations help us define critical line and point. We start with the vanilla MLP with no LayerNorm and \(=0\). Results with LayerNorm and/or residual connections, as well as modern architectures are presented in the following sections. (We refer the reader to Appendices D to F for proofs and detailed application of this recipe. Appendices I to K contain the calculations and results for various activation functions.)

**Definition 2.1**.: We define averaged covariance of preactivations as follows

\[^{l}(x,x^{})=_{}[}_{i= 1}^{N_{l}}h_{i}^{l}(x)h_{i}^{l}(x^{})]\,.\] (6)

**Lemma 2.2**.: _When \(N_{l}\) for \(l=1,,L-1\) sequentially, the expectation value over parameter initializations for a general function of preactivations: \((h^{l}(x))\), can be expressed as the averaging over the Gaussian process \(h^{l}(x)\) with covariance \(^{l}(x,x^{})\)._

\[_{}[(h_{i}^{l}(x))]=^{l}(x,x)}} dh^{l}(h_{i}^{l}(x))e^{-^{l }(x))^{2}}{2^{l}(x,x)}}\,.\] (7)

This result has been established in Lee et al. . Note that the density in (7) only depends on the diagonal part of the covariance matrix, \(^{l}(x,x)\). We will refer to \(^{l}(x,x)\) as NNGP kernel.

_Remark 2.3_.: In the sequential infinite width limit the means appearing in (9)-(11) are self-averaging and, therefore, deterministic. They converge in distribution to their averages over parameterizations.

\[}_{i=1}^{N_{l}}(h_{i}^{l})^{2} l-1}}_{}[}_{i=1}^{N_{l} }(h_{i}^{l})^{2}]\,.\] (8)

When performing analytic calculations we use the infinite width convention; whereas in our finite-width experiments we explicitly average over initializations of \(^{l}\).

**Theorem 2.4**.: _With 2.2, in the infinite width limit, the NNGP kernel \(^{l+1}(x,x)\) is deterministic, and can be determined recursively via_

\[^{l+1}(x,x)=_{w}^{2}_{}[} _{i=1}^{N_{l}}(h_{i}^{l}(x))^{2}]+_{b}^{2}\,.\] (9)

**Theorem 2.5**.: _Let \(f(x)\) be an MLP network with a Lipschitz continuous activation function \((x)\). In the infinite width limit, APJN \(^{l_{0},l+1}(x)\) is deterministic and satisfies a recurrence relation_

\[^{l_{0},l+1}(x)=_{}^{l}^{l_{0},l}(x)\,,\] (10)

_where the factor \(_{}^{l}\) is given by_

\[_{}^{l}=_{w}^{2}_{}[^{2}}{N_{l}}_{i=1}^{N_{l}}(^{}(h_{i}^{l}(x)))^{2} ]\,.\] (11)

2.4 is due to Neal  and Lee et al. . 2.5 is new and is valid only in the limit of infinite width. The proof is in D. We will drop the explicit dependence on \(x\) to improve readability.

The expectation values that appear in (9)-(11) are evaluated using (7). When the integrals can be taken analytically, they lead to explicit equations for the critical lines and/or the critical points. Details of these calculations as well as the derivation of (9)-(11) can be found in the D. A subtlety emerges in (10) when \(l_{0}=0\), where a correction of the order \(O(N_{0}^{-1})\) arises for non-scale invariant activation functions. This subtlety is discussed in the D.

When the depth of the network becomes large, the \(l\)-dependence of the expectation values that appear in (6), (11) saturate to a (possibly infinite) constant value; which means that \(^{l}\), \(^{l_{0},l}\) and \(_{}^{l}\) have reached a fixed point. We denote the corresponding quantities as \(^{},^{l_{0},},^{}_{}\). The existence of a fixed point is not obvious and should be checked on a case by case basis. Fixed point analysis for \(^{l}\) was done in Poole et al.  for bounded activation functions and in Roberts et al.  for the general case. The stability is formulated in terms of

\[^{}_{}=^{l+1}}{^{l}}_{^{l}=^{}}\,.\] (12)

The norm of preactivations remains finite (or behaves algebraically) when \(^{}_{}=1\).

Eq. (10) nicely expresses \(^{l_{0},l+1}\) as a linear function of \(^{l_{0},l}\). The behaviour of \(^{l_{0},l+1}\) at large \(l\) is determined by \(^{l}_{}\). When \(^{l}_{}>1\) partial Jacobians diverge exponentially, while for \(^{l}_{}<1\) partial Jacobians vanish exponentially. Neural networks are trainable only up to a certain depth when initialized \(O(1)\) away from criticality, which is determined by the equation

\[^{}_{}=1\,.\] (13)

Eq. (13) is an implicit equation on \(_{b},_{w}\) and generally outputs a critical line in \(_{b}\)-\(_{w}\) plane. The parameter \(^{}_{}\) has to be calculated on a case-by-case basis using either (11) or the method presented in the next section. Everywhere on the critical line, \(^{l_{0},l}\) saturates to a constant or behaves algebraically.

When the condition \(^{}_{}=1\) is added, we are left with a critical point7. This analysis of criticality at infinite width agrees with Roberts et al. , where \(_{}\) is to be identified with \(^{}_{}\); and Schoenholz et al. , Martens et al. , where their analysis based on the equivalent \(_{1}\) or \(C^{}(1)\) only works for bounded activation functions. In particular, condition (13) together with \(^{}_{}=1\) ensures that NTK is \(O(1)\) at initialization.

### Empirical Diagnostic of Criticality

APJN \(^{l_{0},l}\) provides a clear practical way to diagnose whether the network is critical or not. Proper choice of \(l_{0}\) and \(l\) allows us to minimize the non-universal effects and cleanly extract \(^{}_{}\).

Recurrence relation (10), supplemented with the initial condition \(^{l_{0},l_{0}+1}=^{l_{0}}_{}\), can be formally solved as

\[^{l_{0},l}=_{=l_{0}}^{l-1}^{}_{}\,.\] (14)

We would like to obtain an estimate of \(^{}_{}\) as accurately as possible. To that end, imagine that for some \(l^{}>l_{0}\) the fixed point has been essentially reached and \(^{l^{}}_{}^{}_{}\). Then the APJN

\[^{l_{0},l}=(^{}_{})^{l-l^{}-1}_ {=l_{0}}^{l^{}}^{}_{}\] (15)

depends on the details of how the critical point is approached; which are encoded in the last factor.

**Proposition 2.6**.: _If the network is homogeneous, i.e., consists of (possibly complex) blocks of layers, periodically repeated \(L\) times; then the penultimate APJN provides an accurate estimate of \(^{}_{}\):_

\[^{L-2,L-1}_{L}=^{}_{}\,.\] (16)

_This is a direct consequence of combining (9) and (11) as \(L\) goes to infinity. See 1 for numerical justification._

2.6 is the central result of this section and will be heavily used in the remainder of this work.

Note that for deep networks, away from criticality, APJN takes the form

\[^{l_{0},l} c_{l_{0}}e^{}\,,=| ^{}_{}|^{-1}\,,\] (17)

where \(c_{l_{0}}\) is a non-universal constant that depends on \(l_{0}\). If the sign in (17) is positive (\(^{}_{}>1\)) the network is in the _chaotic phase_, while when the sign is negative (\(^{}_{}<1\)) the network is in the _ordered phase_. \(\) has the meaning of correlation length: on the depth scale of approximately \(k\) the gradients remain appreciable, and hence the network with the depth of \( k\) will train.

We used (16) to map out the \(_{b}\)-\(_{w}\) phase diagrams of various MLP architectures. The location of the critical line agrees remarkably well with our infinite width calculations. Results are presented in Fig. 2.

At criticality, \(^{}_{}=1\) and the correlation length diverges; indicating that gradients can propagate arbitrarily far. A more careful analysis of non-linear corrections shows that APJN can exhibit algebraic behavior with depth and can still vanish in the infinite depth limit, but much slower than the ordered phase.

### Scaling at a Critical Point

At criticality \(^{l}_{}\) saturates to a fixed value \(^{}_{}=1\). If we are interested in \(^{l_{0},l}\) with \(l-l_{0}=O(L)\) then it is essential to know how exactly \(^{l}_{}\) approaches \(1\).

**Theorem 2.7**.: _Assume that deep neural network \(f(x)\) is initialized critically. Then \(l\) asymptotics of APJN is given by_

\[^{l_{0},l}(x)=O(l^{-})\,,\] (18)

where \(\) is the critical exponent Roberts et al. , see Appendix G for further details.

Critical exponents can be determined analytically in the limit of infinite width. Note that \(^{l}_{}\), given by (11), depends on \(^{l}\) by virtue of (7). Consequently, Eqs. (9)-(11) are coupled through non-linear (in \(^{l}\) and \(^{l_{0},l}\)) terms. These non-linear corrections are absent for any scale-invariant activation function, but appear for other activation functions.

We checked the scaling empirically by plotting \(^{0,l}\) vs. \(l\) in a \(\)-\(\) plot and fitting the slope. These results are presented in Fig.1, and are in excellent agreement with infinite width calculation.

## 3 Layer Normalization

The fact that critical initialization is concentrated on a single point \((^{}_{w},^{}_{b})\) may appear unsettling because great care must be taken to initialize the network critically. The situation can be substantially improved by utilizing the normalization techniques known as LayerNorm  and GroupNorm . Our results apply to GroupNorm verbatim in the case when the number of groups is much smaller than the width. LayerNorm can act either on preactivations or on activations (discussed in the Appendix D). Depending on this choice, criticality will occur on different critical _lines_ in \(_{b}\)-\(_{w}\) plane. When LayerNorm is applied to _preactivations_ the correlation length is enhanced, allowing for training much deeper networks even far away from criticality.

The LayerNorm applied to preactivations takes the following form

Figure 1: \(\)–\(\) plots of partial Jacobian \(^{0,l}\) vs. \(l\). From left to right: (1)erf, (2)ReLU, (3)erf and GELU with LayerNorm applied to preactivations and residual connections of strength \(1\). The fluctuations get larger towards the output because the aspect ratio (_i.e._\(l/N_{l}\)) approaches \(1/4\).

**Definition 3.1** (Normalized preactivations).: \[_{i}^{l}=^{l}-[h^{l}]}{[(h^{l})^{2} ]-[h^{l}]^{2}}}} ^{l}}}h_{i}^{l}\,,\] (19)

where we have introduced \([h^{l}]=(_{i=1}^{N_{l}}h_{i}^{l})/N_{l}\). In the limit of infinite width \([h^{l}]=0\) and \([(h^{l})^{2}]=^{l}\), defined according to (6).

Normalized preactivations, \(_{i}^{l}\), are distributed according to \((0,1)\) for all \(l,_{w},_{b}\). The norms are, therefore, _always_ finite and the condition \(_{}^{}=1\) is trivially satisfied. This results in a critical line rather than a critical point.

The recurrence relations (9)-(11) for the NNGP and partial Jacobians are only slightly modified

\[^{l+1}=_{w}^{2}_{}[} _{i=1}^{N_{l}}(_{i}^{l})^{2}]+_{b}^{2} _{}^{l}=^{2}}{^{l}}_{ }[}_{i=1}^{N_{l}}^{}(_{i}^{l} )^{2}]\,.\] (20)

Assuming that the value of \(_{}^{l}\) at the fixed point is \(_{}^{}\), the network is critical when (13) holds.

\(_{}^{l}\) (20) is depth independent and changes slowly with \(_{w}\) and \(_{b}\). Thus, \(_{}^{}\) remains close to \(1\) for a wider range of hyperparameters. Consequently, the correlation length is _large_ even away from criticality, leading to a much higher trainability of deep networks.

## 4 Residual (Skip) Connections

Adding residual connections between the network layers is a widely used technique to facilitate the training of deep networks. Originally introduced  in the context of convolutional neural networks  (CNNs) for image recognition, residual connections have since been used in a variety of networks architectures and tasks [50; 13].

Consider (1) with non-zero \(\) and without LayerNorm layers. Then the recurrence relations (9)-(11) for the NNGP kernel and \(_{}^{l}\) are modified as follows

\[^{l+1}=_{w}^{2}_{}[} _{j=1}^{N_{l}}(h_{j}^{l})^{2}]+_{b}^{2}+^{2}^{l} \,,_{}^{l}=_{w}^{2}_{}[}_{k=1}^{N_{l}}^{}(h_{k}^{l})^{2}]+^{2}\,.\] (21)

Figure 2: \(_{}^{}\) empirical phase diagrams for an MLP with \(L=50,N=500\). The solid lines indicate the critical lines obtained through infinite width limit calculations, and the stars indicate the critical points. The dotted lines in the rightmost column correspond to the critical lines for \(<1\) case. For networks with LayerNorm and \(=1\), \(_{}^{}=1\) holds on the entire \(_{b}\)–\(_{w}\) plane. We also note that for erf activation, the case \(=1\)_without_ LayerNorm is subcritical and has a large correlation length.

_Remark 4.1_.: When \(<1\), the fixed point value of NNGP kernel is scaled by \((1-^{2})^{-1}\). For \(=1\), the critical point is formally at \((0,0)\).

_Remark 4.2_.: For \(=1\), (21) implies that \(^{l}_{} 1\), where the equality holds on the \(_{w}=0\) axis. Consequently, APJN exponentially diverges as a function of depth \(l\) for all \(_{w}>0\). In this case, \(_{w}\) needs to be taken sufficiently close to 0 to ensure trainability at large depths.

When \(<1\), residual connections amplify the chaotic phase and decrease the correlation length away from criticality for unbounded activation functions.

Solving the recurrence relations (21) for \(\) activation, we find an effect observed in Yang and Schoenholz  for \(\) activation. They noted that \(\)-like MLP networks with skip connections "hover over the edge of chaos". We quantify their observation as follows.

**Theorem 4.3**.: _Let \(f(x)\) be a deep MLP network with \(\) activation function and residual connections of strength \(=1\). Then in the limit \(N_{l}\)_

* _The NNGP kernel_ \(K^{l}\) _linearly diverges with depth_ \(l\)_._
* \(^{l}_{}\) _approaches_ \(1\) _from above (Fig._ 2_) :_ \(^{l}_{} 1+/\)_, where_ \(=2_{w}^{2}/(^{2}+_{b}^{2}})\) _is a non-universal constant. Consequently, APJN diverges as a stretched exponential :_ \(^{l_{0},l}=O(e^{}})\)_, where_ \(=1/(4^{2})\) _is the new length scale._

We will refer to this case as _subcritical_. Although \(^{*}_{}\) reaches \(1\), the APJN still diverges with depth faster than any power law. The growth is controlled by the new scale \(\). To control the gradient we would like to make \(\) large, which can be accomplished by decreasing \(_{w}\). In this case, the trainability is enhanced (see Fig. 3). Similar results hold for \(\) activation function , however in that case there is no explicit expression for \(\).

## 5 Residual Connections + LayerNorm

In practice, it is common to use a combination of residual connections and LayerNorm. Using (1), the recurrence relations (9)-(11) for NNGP and partial Jacobians are modified as follows

\[^{l+1}=_{w}^{2}_{}[} _{j=1}^{N_{l}}(_{j}^{l})^{2}]+_{b}^{2}+^{2} ^{l}\,,^{l}_{}=^{2}}{^{l}}_{}[}_{k=1}^{N_{l}}^{ }(_{k}^{l})^{2}]+^{2}\,.\] (22)

Figure 3: Trainability (Training Accuracy) of deep MLP (\(N_{l}=500\), \(L=50\)) on FashonMNIST. The combination of LayerNorm and \(=1\) makes the network everywhere-trainable. The subcritical case of \(\) activation _without_ LayerNorm, and \(=1\) also has enhanced trainability. The dashed white lines denote the (analytical) critical lines.

_Remark 5.1_.: For \(<1\), (22) implies that the fixed point value of NNGP kernel is scaled by \(1-^{2}\). Moreover, residual connections do not shift the phase boundary. The interference between residual connections and LayerNorm brings \(_{}^{l}\) closer to \(1\) on the entire \(_{b}\)-\(_{w}\) plane (as can be seen from Fig. 2). Therefore the correlation length \(\) is improved in both the phases, allowing for training of deeper networks. At criticality, Jacobians linearly diverge with depth.

As was mentioned before, the combination of LayerNorm and residual connections dramatically enhances correlation length, leading to a more stable architecture. This observation is formalized by 1.3. The proof leverages the solutions of (22) close to the fixed point, and is fleshed out in Appendix F.

_Remark 5.2_.: When \(=1\), the correlation length diverges for _any_ initialization.

5.2 provides an alternative perspective on architecture design. On the one hand, one can use (16) to initialize a given network at criticality. Alternatively, one can use a combination of residual connections and LayerNorm to ensure that the network will train well, irrespective of the initialization.

_Remark 5.3_.: When \(=1\), the condition \(_{}^{}=1\) holds on the entire \(_{b}-_{w}\) and for any activation function \(\) (see Fig. 2). NNGP kernel diverges linearly, while APJN diverges algebraically with the critical exponent of \(=O(1)\). The exact value of the critical exponent depends on the activation function and the ratio \(_{b}/_{w}\). The trainability is dramatically enhanced, as shown in Fig. 3.

_Remark 5.4_.: Networks with BatchNorm , used in conjunction with residual connection of strength \(=1\), also enjoy this _everywhere criticality_ and enhanced trainability [63; 23] (See Appendix B).

## 6 Modern Architectures

ResNet110ResNet is one of the most widely used architectures in computer vision. Figure 4 shows the results for ResNetV2 ; with BatchNorm replaced with LayerNorm. (See Appendix B for BatchNorm results and discussions.)

At \(=1\), \(_{w}^{2}-_{b}^{2}\) phase diagram shows everywhere-criticality, as expected from our theory. For \(_{b}^{2}=0\), \(_{w}^{2}-\) phase diagram gets closer to criticality as we increase \(\), which results in better training at higher \(\).

Additionally, networks with larger \(\) enjoy better trainability due to the suppression of finite width corrections to our theory (decreased effective \(L/N\)).

MLP-MixerMLP-Mixer architecture is a recent example of MLP approach to computer vision . It can be analyzed using the tools presented above. Fig. 5 shows the phase and training diagrams. Further details can be found in the SM. Note that the original architecture uses \(=1.0\).

Test accuracies for figures 4, 5 follow similar trends as train accuracies. (See Appendices B, H).

Figure 4: **ResNet110(LayerNorm)**: Left to right: (1) \(_{}^{}\) phase diagram w.r.t. \((_{w}^{2},_{b}^{2})\) at \(=1\). The network is everywhere-critical in this case. (2) \(_{}^{}\) phase diagram w.r.t. \((_{w}^{2},)\). (3) Training accuracy w.r.t. \(_{w}^{2}\), for different values of \(\). Trainability improves with with higher \(\), as the network gets closer to everywhere-critical phase. For \(=0\) and small \(_{w}^{2}\) the network is in the ordered phase, and the output of every block is almost zero – this explains the poor trainability. (4) Training accuracy w.r.t. \(\) at \((_{w}^{2},_{b}^{2})=(2,0)\). We see a monotonic improvement in trainability.

## 7 Conclusions

We have introduced partial Jacobians and their averaged norms as tools to analyze the propagation of gradients through deep neural networks at initialization. Using APJN evaluated close to the output, \(^{L-2,L-1}^{*}_{}\), we have introduced a very cheap and simple empirical test for criticality. We have also shown that criticality formulated in terms of partial Jacobians is equivalent to criticality studied previously in literature [44; 45; 34]. Additionally, APJN can be utilized to quantify criticality in inhomogeneous (_i.e._ no periodic stacking of blocks) networks .

We have investigated homogeneous architectures that include fully-connected layers, normalization layers and residual connections. In the limit of infinite width, we showed that (i) in the presence of LayerNorm, the critical point generally becomes a critical line, making the initialization problem much easier, (ii) LayerNorm applied to preactivations enhances correlation length leading to improved trainability, (iii) combination of \(=1\) residual connections and erf activation function enhances correlation length driving the network to a _subcritical_ phase with APJN growing according to a stretched exponential law, (iv) combination of residual connections and LayerNorm drastically increases correlation length leading to improved trainability, (v) when \(=1\) and LayerNorm is applied to preactivations _the network is critical on the entire \(_{b}\)-\(_{w}\) plane_.

We have considered examples of modern architectures: ResNet and MLP-Mixer. We showed that at \(=1\), they are critical everywhere, due to the interaction between LayerNorm and residual connections. We have studied ResNet at different \(\)'s, showing that higher \(\) enjoys better trainability, due to improved initialization and suppressed effective \(L/N\) corrections.

We have empirically demonstrated that deep (100 blocks) MLP-Mixer with \(=1\) trains well for various initializations. In comparison, for \(=0.5\), it only trains well close to the critical line.

Our work shows that an architecture can be designed to have a large correlation length leading to a guaranteed trainability with SGD for any initialization scheme.

## 8 Limitations and Future Works

While our method is empirically applicable to Transformer architectures (see Appendix C for phase diagrams), there remains a notable absence of robust theoretical foundations for initializing Transformers. We aim to extend our theory to the attention mechanism and determine the optimal way to initialize Transformer architectures.

Another challenge arises from network topology. For graph neural networks, the generalization of our methods remains ambiguous. We aspire to elucidate this in future research.

Lastly, we want to scale our experiments to larger image datasets as well as language tasks, with the hope that our methods could help people train large models more efficiently.

Figure 5: **MLP-Mixer. Left to right: (1)(2) \(=0.5,1.0\) phase diagrams (\(^{*}_{}\)). The network becomes everywhere-critical when \(=1\). The solid black line indicates the empirical phase boundary. Stars denote points we selected to train on CIFAR-10. (3)(4) \(=0.5,1.0\) MLP-Mixer training curves. All, but one, networks are \(L=100\) blocks deep. We see that as we increase \(\) from \(0.5\) to \(1\), the trainability of all networks increases, and they are less sensitive to initialization.**