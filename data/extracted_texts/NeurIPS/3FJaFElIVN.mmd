# Glime: General, Stable and Local LIME Explanation

Zeren Tan

Tsinghua University

thutzr1019@gmail.com &Yang Tian

Tsinghua University

tyanyang04@gmail.com &Jian Li

Tsinghua University

lapordge@gmail.com

###### Abstract

As black-box machine learning models grow in complexity and find applications in high-stakes scenarios, it is imperative to provide explanations for their predictions. Although Local Interpretable Model-agnostic Explanations (LIME)  is a widely adopted method for understanding model behaviors, it is unstable with respect to random seeds  and exhibits low local fidelity (i.e., how well the explanation approximates the model's local behaviors) . Our study shows that this instability problem stems from small sample weights, leading to the dominance of regularization and slow convergence. Additionally, LIME's sampling neighborhood is non-local and biased towards the reference, resulting in poor local fidelity and sensitivity to reference choice. To tackle these challenges, we introduce Glime, an enhanced framework extending LIME and unifying several prior methods. Within the Glime framework, we derive an equivalent formulation of LIME that achieves significantly faster convergence and improved stability. By employing a local and unbiased sampling distribution, Glime generates explanations with higher local fidelity compared to LIME. Glime explanations are independent of reference choice. Moreover, Glime offers users the flexibility to choose a sampling distribution based on their specific scenarios.

## 1 Introduction

Why a patient is predicted to have a brain tumor ? Why a credit application is rejected ? Why a picture is identified as an electric guitar ? As black-box machine learning models continue to evolve in complexity and are employed in critical applications, it is imperative to provide explanations for their predictions, making interpretability a central concern . In response to this imperative, various explanation methods have been proposed , aiming to provide insights into the internal mechanisms of deep learning models.

Among the various explanation methods, Local Interpretable Model-agnostic Explanations (LIME)  has attracted significant attention, particularly in image classification tasks. LIME explains predictions by assigning each region within an image a weight indicating the influence of this region to the output. This methodology entails segmenting the image into super-pixels, as illustrated in the lower-left portion of Figure 0(a), introducing perturbations, and subsequently approximating the local model prediction using a linear model. The approximation is achieved by solving a weighted Ridge regression problem, which estimates the impact (i.e., weight) of each super-pixel on the classifier's output.

Nevertheless, LIME has encountered significant instability due to its random sampling procedure . In LIME, a set of samples perturbing the original image is taken. As illustrated in Figure 0(a), LIME explanations generated with two different random seeds display notable disparities, despite using a large sample size (16384). The Jaccard index, measuring similarity between two explanations on a scale from 0 to 1 (with higher values indicating better similarity), is below 0.4. While many prior studies aim to enhance LIME's stability, some sacrifice computational time for stability , and others may entail the risk of overfitting . The evident drawback of unstableexplanations lies in their potential to mislead end-users and hinder the identification of model bugs and biases, given that LIME explanations lack consistency across different random seeds.

In addition to its inherent instability, LIME has been found to have poor local fidelity . As depicted in Figure 0(a), the \(R^{2}\) value for LIME on the sample image approaches zero (refer also to Figure 3(b)). This problem arises from the non-local and skewed sampling space of LIME, which is biased towards the reference. More precisely, the sampling space of LIME consists of the corner points of the hypercube defined by the explained instance and the selected reference. For instance, in the left section of Figure 0(b), only four red points fall within LIME's sampling space, yet these points are distant from \(\). As illustrated in Figure 3, the \(L_{2}\) distance between LIME samples of the input \(\) and \(\) is approximately \(0.7\|\|_{2}\) on ImageNet. Although LIME incorporates a weighting function to enforce locality, an explanation cannot be considered as local if the samples themselves are non-local, leading to a lack of local fidelity in the explanation. Moreover, the hypercube exhibits bias towards the reference, resulting in explanations designed to explain only a portion of the local neighborhood. This bias causes LIME to generate different explanations for different references, as illustrated in Figure 0(b) (refer to Appendix A.4 for more analysis and results).

To tackle these challenges, we present Glime--a local explanation framework that generalizes LIME and five other methods: KernelSHAP , SmoothGrad , Gradient , DLIME , and ALIME . Through a flexible sample distribution design, Glime produces explanations that are more stable and faithful. Addressing LIME's instability issue, within Glime, we derive an equivalent form of LIME, denoted as Glime-Binomial, by integrating the weighting function into the sampling distribution. Glime-Binomial ensures exponential convergence acceleration compared to LIME when the regularization term is presented. Consequently, Glime-Binomial demonstrates improved stability compared to LIME while preserving superior local fidelity (see Figure 4). Furthermore, Glime enhances both local fidelity and stability by sampling from a local distribution independent of any specific reference point.

In summary, our contributions can be outlined as follows:

* We conduct an in-depth analysis to find the source of LIME's instability, revealing the interplay between the weighting function and the regularization term as the primary cause. Additionally, we attribute LIME's suboptimal local fidelity to its non-local and biased sampling space.
* We introduce Glime as a more general local explanation framework, offering a flexible design for the sampling distribution. With varying sampling distributions and weights, Glime serves as a generalization of LIME and five other preceding local explanation methods.

Figure 1: **Glime enhances stability and local fidelity compared to LIME.** (a) LIME demonstrates instability with the default parameter \(\), while Glime consistently provides meaningful explanations. (b) LIME samples from a biased and non-local neighborhood, a limitation overcome by Glime.

* By integrating weights into the sampling distribution, we present a specialized instance of Glime with a binomial sampling distribution, denoted as Glime-Binomial. We demonstrate that Glime-Binomial, while maintaining equivalence to LIME, achieves faster convergence with significantly fewer samples. This indicates that enforcing locality in the sampling distribution is better than using a weighting function.
* With regard to local fidelity, Glime empowers users to devise explanation methods that exhibit greater local fidelity. This is achieved by selecting a local and unbiased sampling distribution tailored to the specific scenario in which Glime is applied.

## 2 Preliminary

### Notations

Let \(\) and \(\) denote the input and output spaces, respectively, where \(^{D}\) and \(\). We specifically consider the scenario in which \(\) represents the space of images, and \(f:\) serves as a machine learning model accepting an input \(\). This study focuses on the classification problem, wherein \(f\) produces the probability that the image belongs to a certain class, resulting in \(=\).

Before proceeding with explanation computations, a set of features \(\{s_{i}\}_{i=1}^{d}\) is derived by applying a transformation to \(\). For instance, \(\{s_{i}\}_{i=1}^{d}\) could represent image segments (also referred to as super-pixels in LIME) or feature maps obtained from a convolutional neural network. Alternatively, \(\{s_{i}\}_{i=1}^{d}\) may correspond to raw features, i.e., \(\) itself. In this context, \(\|\|_{0}\), \(\|\|_{1}\), and \(\|\|_{2}\) denote the \(_{0}\), \(_{1}\), and \(_{2}\) norms, respectively, with \(\) representing the element-wise product. Boldface letters are employed to denote vectors and matrices, while non-boldface letters represent scalars or features. \(B_{}()\) denotes the ball centered at \(\) with radius \(\).

### A brief introduction to LIME

In this section, we present the original definition and implementation of LIME  in the context of image classification. LIME, as a local explanation method, constructs a linear model when provided with an input \(\) that requires an explanation. The coefficients of this linear model serve as the feature importance explanation for \(\).

**Features.** For an input \(\), LIME computes a feature importance vector for the set of features. In the image classification setting, for an image \(\), LIME initially segments \(\) into super-pixels \(s_{1},,s_{d}\) using a segmentation algorithm such as Quickshift . Each super-pixel is regarded as a feature for the input \(\).

**Sample generation.** Subsequently, LIME generates samples within the local vicinity of \(\) as follows. First, random samples are generated uniformly from \(\{0,1\}^{d}\). The \(j\)-th coordinate \(z^{}_{j}\) for each sample \(}\) is either 1 or 0, indicating the presence or absence of the super-pixel \(s_{j}\). When \(s_{j}\) is absent, it is replaced by a reference value \(r_{j}\). Common choices for the reference value include a black image, a blurred version of the super-pixel, or the average value of the super-pixel [29; 22; 8]. Then, these \(}\) samples are transformed into samples in the original input space \(^{D}\) by combining them with \(=(s_{1},,s_{d})\) using the element-wise product as follows: \(=}+(1-})\), where \(\) is the vector of reference values for each super-pixel, and \(\) represents the element-wise product. In other words, \(\) is an image that is the same as \(\), except that those super-pixels \(s_{j}\) with \(z^{}_{j}=0\) are replaced by reference values.

**Feature attributions.** For each sample \(}\) and the corresponding image \(\), we compute the prediction \(f()\). Finally, LIME solves the following regression problem to obtain a feature importance vector (also known as feature attributions) for the super-pixels:

\[^{}=*{arg\,min}_{}_{ }(\{0,1\}^{d})}[(})(f( )-^{}})^{2}]+\|\|_{ 2}^{2}, \]

where \(=}+(1-})\), \((})=\{-\|-}\|_{2}^{2}/ ^{2}\}\), and \(\) is the kernel width parameter.

**Remark 2.1**.: _In practice, we draw samples \(\{^{}_{i}\}_{i=1}^{n}\) from the uniform distribution Uni\((\{0,1\}^{d})\) to estimate the expectation in Equation 1. In the original LIME implementation , \(=/n\) for a constant \(>0\). This choice has been widely adopted in prior studies [40; 8; 19; 9; 5; 20; 34]. We use \(}^{}\) to represent the empirical estimation of \(^{}\)._

### LIME is unstable and has poor local fidelity

**Instability.** To capture the local characteristics of the neighborhood around the input \(\), LIME utilizes the sample weighting function \(()\) to assign low weights to samples that exclude numerous super-pixels and, consequently, are located far from \(\). The parameter \(\) controls the level of locality, with a small \(\) assigning high weights exclusively to samples very close to \(\) and a large \(\) permitting notable weights for samples farther from \(\) as well. The default value for \(\) in LIME is \(0.25\) for image data. However, as depicted in Figure 0(a), LIME demonstrates instability, a phenomenon also noted in prior studies [35; 24; 34]. As showed in Section 4, this instability arises from small \(\) values, leading to very small sample weights and, consequently, slow convergence.

**Poor local fidelity.** LIME also suffers from poor local fidelity [16; 21]. The sampling space of LIME is depicted in Figure 0(b). Generally, the samples in LIME exhibit considerable distance from the instance being explained, as illustrated in Figure 3, rendering them non-local. Despite LIME's incorporation of weights to promote locality, it fails to provide accurate explanations for local behaviors when the samples themselves lack local proximity. Moreover, the sampling space of LIME is influenced by the reference, resulting in a biased sampling space and a consequent degradation of local fidelity.

## 3 A general local explanation framework: Glime

### The definition of Glime

We first present the definition of Glime and show how it computes the explanation vector \(^{}\). Analogous to LIME, Glime functions by constructing a model within the neighborhood of the input \(\), utilizing sampled data from this neighborhood. The coefficients obtained from this model are subsequently employed as the feature importance explanation for \(\).

**Feature space.** For the provided input \(^{D}\), the feature importance explanation is computed for a set of features \(=(s_{1},,s_{d})\) derived from applying a transformation to \(\). These features \(\) can represent image segments (referred to as super-pixels in LIME) or feature maps obtained from a convolutional neural network. Alternatively, the features \(\) can correspond to raw features, i.e., the individual pixels of \(\). In the context of LIME, the method specifically operates on super-pixels.

**Sample generation.** Given features \(\), a sample \(^{}\) can be generated from the distribution \(\) defined on the feature space (e.g., \(\) are super-pixels segmented by a segmentation algorithm such Quickshift  and \(=(\{0,1\}^{d})\) in LIME). It's important to note that \(^{}\) may not belong to \(\) and cannot be directly input into the model \(f\). Consequently, we reconstruct \(^{D}\) in the original input space for each \(^{}\) and obtain \(f()\) (in LIME, a reference \(\) is first chosen and then \(=^{}+(1-^{ })\)). Both \(\) and \(^{}\) are then utilized to compute feature attributions.

**Feature attributions.** For each sample \(^{}\) and its corresponding \(\), we compute the prediction \(f()\). Our aim is to approximate the local behaviors of \(f\) around \(\) using a function \(g\) that operates on the feature space. \(g\) can take various forms such as a linear model, a decision tree, or any Boolean function operating on Fourier bases . The loss function \((f(),g(^{}))\) quantifies the approximation gap for the given sample \(^{}\). In the case of LIME, \(g(^{})=^{}^{}\), and \((f(),g(^{}))=(f()-g(^{ }))^{2}\). To derive feature attributions, the following optimization problem is solved:

\[^{}=*{arg\,min}_{}_{ ^{}}[(^{})(f(),g(^{}))]+ R(), \]

where \(()\) is a weighting function and \(R()\) serves as a regularization function, e.g., \(\|\|_{1}\) or \(\|\|_{2}^{2}\) (which is used by LIME). We use \(}^{}\) to represent the empirical estimation of \(^{}\).

**Connection with Existing Frameworks.** Our formulation exhibits similarities with previous frameworks [22; 12]. The generality of Glime stems from two key aspects: (1) Glime operates within a broader feature space \(^{d}\), in contrast to , which is constrained to \(\{0,1\}^{d}\), and , which is confined to raw features in \(^{D}\). (2) Glime can accommodate a more extensive range of distribution choices tailored to specific use cases.

### An alternative formulation of Glime without the weighting function

Indeed, we can readily transform Equation 2 into an equivalent formulation without the weighting function. While this adjustment simplifies the formulation, it also accelerates convergence by sampling from the transformed distribution (see Section 4.1 and Figure 3(a)). Specifically, we define the transformed sampling distribution as \(}(^{})=^{})P( ^{})}{()P()}\). Utilizing \(}\) as the sampling distribution, Equation 2 can be equivalently expressed as

\[^{}=*{arg\,min}_{}_{ ^{}}}[(f(),g(^{}))]+R(), Z=_{ }[()()] \]

It is noteworthy that the feature attributions obtained by solving Equation 3 are equivalent to those obtained by solving Equation 2 (see Appendix B.1 for a formal proof). Therefore, the use of \(()\) in the formulation is not necessary and can be omitted. Hence, unless otherwise specified, Glime refers to the framework without the weighting function.

### Glime unifies several previous explanation methods

This section shows how Glime unifies previous methods. For a comprehensive understanding of the background regarding these methods, kindly refer to Appendix A.6.

**LIME  and Glime-Binomial.** In the case of LIME, it initiates the explanation process by segmenting pixels \(x_{1},,x_{D}\) into super-pixels \(s_{1},,s_{d}\). The binary vector \(^{}=(\{0,1\}^{d})\) signifies the absence or presence of corresponding super-pixels. Subsequently, \(=^{}+(- ^{})\). The linear model \(g(^{})=^{}^{}\) is defined on \(\{0,1\}^{d}\). For image explanations, \((f(),g(^{}))=(f()-g(^{ }))^{2}\), and the default setting is \((^{})=(-\|-^{}\|_{0}^{2}/ ^{2})\), \(R()=\|\|_{2}^{2}\). Remarkably, LIME is equivalent to the special case Glime-Binomial without the weighting function (see Appendix B.2 for the formal proof). The sampling distribution of Glime-Binomial is defined as \((^{},\|^{}\|_{0}=k)=e^{k/^{ 2}}/(1+e^{1/^{2}})^{d}\), where \(k=0,1,,d\). This distribution is essentially a Binomial distribution. To generate a sample \(^{}\{0,1\}^{d}\), one can independently draw \(z^{}_{i}\{0,1\}\) with \((z_{i}=1)=1/(1+e^{-1/^{2}})\) for \(i=1,,d\). The feature importance vector obtained by solving Equation 3 under Glime-Binomial is denoted as \(^{}\).

**KernelSHAP .** In our framework, the formulation of KernelSHAP aligns with that of LIME, with the only difference being \(R()=0\) and \((^{})=(d-1)/(^{}\|_{0}}\| ^{}\|_{0}(d-\|^{}\|_{0}))\).

**SmoothGrad .** SmoothGrad functions on raw features, specifically pixels in the case of an image. Here, \(=^{}+\), where \(^{}(,^{2})\). The loss function \((f(),g(^{}))\) is represented by the squared loss, while \((^{})=1\) and \(R()=0\), as established in Appendix B.6.

**Gradient .** The Gradient explanation is essentially the limit of SmoothGrad as \(\) approaches 0.

**DLIME .** DLIME functions on raw features, where \(\) is defined over the training data that have the same label with the nearest neighbor of \(\). The linear model \(g(^{})=^{}^{}\) is employed with the square loss function \(\) and the regularization term \(R()=0\).

**ALIME .** ALIME employs an auto-encoder trained on the training data, with its feature space defined as the output space of the auto-encoder. The sample generation process involves introducing Gaussian noise to \(\). The weighting function in ALIME is denoted as \((^{})=(-\|()-(^{})\|_{1})\), where \(()\) represents the auto-encoder. The squared loss function is chosen as the loss function and no regularization function is applied.

## 4 Stable and locally faithful explanations in Glime

### Glime-Binomial converges exponentially faster than LIME

To understand the instability of LIME, we demonstrate that the sample weights in LIME are very small, resulting in the domination of the regularization term. Consequently, LIME tends to produce explanations that are close to zero. Additionally, the small weights in LIME lead to a considerably slower convergence compared to Glime-Binomial, despite both methods converging to the same limit.

**Small sample weights in LIME.** The distribution of the ratio of non-zero elements to the total number of super-pixels, along with the corresponding weights for LIME and Glime-Binomial, is depicted in Figure 2. Notably, most samples exhibit approximately \(d/2\) non-zero elements. However, when \(\) takes values such as 0.25 or 0.5, a significant portion of samples attains weights that are nearly zero. For instance, when \(=0.25\) and \(\|^{}\|_{0}=d/2\), \((^{})\) reduces to \((-8d)\), which is approximately \(10^{-70}\) for \(d=20\). Even with \(\|^{}\|_{0}=d-1\), \((^{})\) equals \(e^{-16}\), approximating \(10^{-7}\). Since LIME samples from \((\{0,1\}^{d})\), the probability that a sample \(^{}\) has \(\|^{}\|_{0}=d-1\) or \(d\) is approximately \(2 10^{-5}\) when \(d=20\). Therefore, most samples have very small weights. Consequently, the sample estimation of the expectation in Equation 1 tends to be much smaller than the true expectation with high probability and is thus inaccurate (see Appendix B.3 for more details). Given the default regularization strength \(=1\), this imbalance implies the domination of the regularization term in the objective function of Equation 1. As a result, LIME tends to yield explanations close to zero in such cases, diminishing their meaningfulness and leading to instability.

**Glime converges exponentially faster than LIME in the presence of regularization.** Through the integration of the weighting function into the sampling process, every sample uniformly carries a weight of 1, contributing equally to Equation 3. Our analysis reveals that Glime requires substantially fewer samples than LIME to transition beyond the regime where the regularization term dominates. Consequently, Glime-Binomial converges exponentially faster than LIME. Recall that \(}^{}\) and \(}^{}\) represent the empirical solutions of Equation 1 and Equation 3, respectively, obtained by replacing the expectations with the sample average. \(}^{}\) is the empirical solution of Equation 3 with the transformed sampling distribution \(}(^{},\|^{}\|_{0}=k)=e^{ k/^{2}}/(1+e^{1/^{2}})^{d}\), where \(k=0,1,,d\). In the subsequent theorem, we present the sample complexity bound for LIME (refer to Appendix B.4 for proof).

**Theorem 4.1**.: _Suppose samples \(\{^{}_{i}\}_{i=1}^{n}(\{0,1\}^{d})\) are used to compute the LIME explanation. For any \(>0,(0,1)\), if \(n=(^{-2}d^{9}2^{8d}e^{8/^{2}}(4d/)), n,\) we have \((\|}^{}-^{}\|_{2}< ) 1-\). \(^{}=_{n}}^{}\)._

Next, we present the sample complexity bound for Glime (refer to Appendix B.5 for proof).

**Theorem 4.2**.: _Suppose \(^{}\)\(\)\(\) such that the largest eigenvalue of \(^{}(^{})^{}\) is bounded by \(R\) and \([^{}(^{})^{}]=(_{1}-_ {2})+_{2}^{},\|(^ {}(^{})^{})\|_{2}^{2},\)\(|(^{}f())_{i}| M\) for some \(M>0\). \(\{^{}_{i}\}_{i=1}^{n}\) are i.i.d. samples from \(\) and are used to compute Glime explanation \(}^{}\). For any \(>0,(0,1)\), if \(n=(^{-2}M^{2}^{2}d^{3}^{4}(4d/))\) where \(\) is a function of \(,d,_{1},_{2}\), we have \((\|}^{}-^{}\|_{2}< ) 1-\). \(^{}=_{n}}^{}\)._

Since Glime-Binomial samples from a binomial distribution, which is sub-Gaussian with parameters \(M=\), \(=2\), \(_{1}=1/(1+e^{-1/^{2}})\), \(_{2}=1/(1+e^{-1/^{2}})^{2}\), and \((_{1},_{2},d)=de^{2/^{2}}\) (refer to Appendix B.5 for proof), we derive the following corollary:

**Corollary 4.3**.: _Suppose \(\{^{}_{i}\}_{i=1}^{n}\) are i.i.d. samples from \(}(^{},\|^{}\|_{0}=k)=e^ {k/^{2}}/(1+e^{1/^{2}})^{d},k=1,,d\) and are used to compute Glime-Binomial explanation. For any \(>0,(0,1)\), if \(n=(^{-2}d^{5}e^{4/^{2}}(4d/)),\) we have \((\|}^{}-^{}\|_{2 }<) 1-\). \(^{}=_{n}}^{}\)._

Figure 2: **The distribution and the weight of \(\|^{}\|_{0}\). In LIME, the distribution of \(\|^{}\|_{0}\) follows a binomial distribution, and it is independent of \(\). In Glime-Binomial, when \(\) is small, \(\|^{}\|_{0}\) concentrates around \(d\), while in LIME, most samples exhibit negligible weights except for the all-one vector 1. As \(\) increases, the distributions in Glime-Binomial converge to those in LIME, and all LIME samples attain non-negligible weights.**Comparing the sample complexities outlined in Theorem 4.1 and Corollary 4.3, it becomes evident that LIME necessitates an exponential increase of \((d,^{-2})\) more samples than Glime-Binomial for convergence. Despite both LIME and Glime-Binomial samples being defined on the binary set \(\{0,1\}\), the weight \((^{})\) associated with a sample \(^{}\) in LIME is notably small. Consequently, the square loss term in LIME is significantly diminished compared to that in Glime-Binomial. This situation results in the domination of the regularization term over the square loss term, leading to solutions that are close to zero. For stable solutions, it is crucial that the square loss term is comparable to the regularization term. Consequently, Glime-Binomial requires significantly fewer samples than LIME to achieve stability.

### Designing locally faithful explanation methods within Glime

**Non-local and biased sampling in LIME.** LIME employs uniform sampling from \(\{0,1\}^{d}\) and subsequently maps the samples to the original input space \(\) with the inclusion of a reference. Despite the integration of a weighting function to enhance locality, the samples \(\{_{i}\}_{i=1}^{n}\) generated by LIME often exhibit non-local characteristics, limiting their efficacy in capturing the local behaviors of the model \(f\) (as depicted in Figure 3). This observation aligns with findings in [16; 21], which demonstrate that LIME frequently approximates the global behaviors instead of the local behaviors of \(f\). As illustrated earlier, the weighting function contributes to LIME's instability, emphasizing the need for explicit enforcement of locality in the sampling process.

**Local and unbiased sampling in Glime.** In response to these challenges, Glime introduces a sampling procedure that systematically enforces locality without reliance on a reference point. One approach involves sampling \(^{}=(,^{2})\) and subsequently obtaining \(=+^{}\). This method, referred to as Glime-Gauss, utilizes a weighting function \(() 1\), with other components chosen to mirror those of LIME. The feature attributions derived from this approach successfully mitigate the aforementioned issues. Similarly, alternative distributions, such as \(=(,)\) or \(=([-,]^{d})\), can be employed, resulting in explanation methods known as Glime-Laplace and Glime-Uniform, respectively.

### Sampling distribution selection for user-specific objectives

Users may possess specific objectives they wish the explanation method to fulfill. For instance, if a user seeks to enhance local fidelity within a neighborhood of radius \(\), they can choose a distribution and corresponding parameters aligned with this objective (as depicted in Figure 5). The flexible design of the sample distribution in Glime empowers users to opt for a distribution that aligns with their particular use cases. Furthermore, within the Glime framework, it is feasible to integrate feature correlation into the sampling distribution, providing enhanced flexibility. In summary, Glime affords users the capability to make more tailored choices based on their individual needs and objectives.

Figure 3: **The distribution of sample distances to the original input. The samples produced by LIME display a considerable distance from the original input \(\), whereas the samples generated by Glime demonstrate a more localized distribution. LIME has a tendency to overlook sampling points that are in close proximity to \(\).**

Experiments

**Dataset and models.** Our experiments are conducted on the ImageNet dataset1. Specifically, we randomly choose 100 classes and select an image at random from each class. The models chosen for explanation are ResNet18  and the tiny Swin-Transformer  (refer to Appendix A.7 for results). Our implementation is derived from the official implementation of LIME2. The default segmentation algorithm in LIME, Quickshift , is employed. Implementation details of our experiments are provided in Appendix A.1. For experiment results on text data, please refer to Appendix A.9. For experiment results on ALIME, please refer to Appendix A.8.

**Metrics.** (1) _Stability_: To gauge the stability of an explanation method, we calculate the average top-\(K\) Jaccard Index (JI) for explanations generated by 10 different random seeds. Let \(_{1},,_{10}\) denote the explanations obtained from 10 random seeds. The indices corresponding to the top-\(K\) largest values in \(_{i}\) are denoted as \(R_{i,:K}\). The average Jaccard Index between pairs of \(R_{i,:K}\) and \(R_{j,:K}\) is then computed, where \((A,B)=|A B|/|A B|\).

(2) _Local Fidelity_: To evaluate the local fidelity of explanations, reflecting how well they capture the local behaviors of the model, we employ two approaches. For LIME, which uses a non-local sampling neighborhood, we use the \(R^{2}\) score returned by the LIME implementation for local fidelity assessment . Within Glime, we generate samples \(\{_{i}\}_{i=1}^{m}\) and \(\{^{}_{i}\}_{i=1}^{m}\) from the neighborhood \(B_{}()\). The squared difference between the model's output and the explanation's output on these samples is computed. Specifically, for a sample \(\), we calculate \((f()-}^{}^{})^{2}\) for the explanation \(}\). The local fidelity of an explanation \(}\) at the input \(\) is defined as \(1/(1+_{i}(f(_{i})-}^{}^{ }_{i})^{2})\), following the definition in . To ensure a fair comparison between different distributions in Glime, we set the variance parameter of each distribution to match that of the Gaussian distribution. For instance, when sampling from the Laplace distribution, we use \((,/)\), and when sampling from the uniform distribution, we use \(([-,]^{d})\).

### Stability of LIME and Glime

**LIME's instability and the influence of regularization/weighting.** In Figure 3(a), it is evident that LIME without the weighting function (\(+=1\)) demonstrates greater stability compared to its weighted counterpart, especially when \(\) is small (e.g., \(=0.25,0.5\)). This implies that the weighting function contributes to instability in LIME. Additionally, we observe that LIME without regularization (\(+=0\)) exhibits higher stability than the regularized LIME, although the improvement is not substantial. This is because, when \(\) is small, the sample weights approach zero, causing the Ridge regression problem to become low-rank, leading to unstable solutions. Conversely, when \(\) is large, significant weights are assigned to all samples, reducing the effectiveness of regularization. For instance, when \(=5\) and \(d=40\), most samples carry weights around 0.45, and even samples with only one non-zero element left possess weights of approximately 0.2. In such scenarios, the regularization term does not dominate, even with limited samples. This observation is substantiated by the comparable performance of LIME, LIME\(+=1\), and LIME\(+=0\) when \(=1\) and \(5\). Further results are presented in Appendix A.2.

**Enhancing stability in LIME with Glime.** In Figure 3(a), it is evident that LIME achieves a Jaccard Index of approximately 0.4 even with over 2000 samples when using the default \(=0.25\). In contrast, both Glime-Binomial and Glime-Gauss provide stable explanations with only 200-400 samples. Moreover, with an increase in the value of \(\), the convergence speed of LIME also improves. However, Glime-Binomial consistently outperforms LIME, requiring fewer samples for comparable stability. The logarithmic scale of the horizontal axis in Figure 3(a) highlights the exponential faster convergence of Glime compared to LIME.

**Convergence of LIME and Glime-Binomial to a common limit.** In Figure 8 of Appendix A.3, we explore the difference and correlation between explanations generated by LIME and Glime-Binomial. Mean Squared Error (MSE) and Mean Absolute Error (MAE) are employed as metrics to quantify the dissimilarity between the explanations, while Pearson correlation and Spearman rank correlation assess their degree of correlation. As the sample size increases, both LIME and GlimeBinomial exhibit greater similarity and higher correlation. The dissimilarity in their explanations diminishes rapidly, approaching zero when \(\) is significantly large (e.g., \(=5\)).

### Local fidelity of LIME and Glime

**Enhancing local fidelity with Glime.** A comparison of the local fidelity between LIME and the explanation methods generated by Glime is presented in Figure 3(b). Utilizing 2048 samples for each image to compute the \(R^{2}\) score, Glime consistently demonstrates superior local fidelity compared to LIME. Particularly, when \(=0.25\) and \(0.5\), LIME exhibits local fidelity that is close to zero, signifying that the linear approximation model \((}^{})^{}^{}\) is nearly constant. Through the explicit integration of locality into the sampling process, Glime significantly improves the local fidelity of the explanations.

**Local fidelity analysis of Glime under various sampling distributions.** In Figure 5, we assess the local fidelity of Glime employing diverse sampling distributions: \((,^{2})\), \((,/)\)

Figure 4: Glime consistently enhances stability and local fidelity compared to LIME across various values of \(\).

Figure 5: **Local fidelity of Glime across different neighborhood radii.** Explanations produced under a distribution with a standard deviation of \(\) demonstrate the ability to capture behaviors within local neighborhoods with radii exceeding \(\).

[MISSING_PAGE_FAIL:10]

Related work

**Post-hoc local explanation methods.** In contrast to inherently interpretable models, black-box models can be explained through post-hoc explanation methods, which are broadly categorized as model-agnostic or model-specific. Model-specific approaches, such as Gradient , SmoothGrad , and Integrated Gradient , assume that the explained model is differentiable and that gradient access is available. For instance, SmoothGrad generates samples from a Gaussian distribution centered at the given input and computes their average gradient to mitigate noise. On the other hand, model-agnostic methods, including LIME  and Anchor , aim to approximate the local model behaviors using interpretable models, such as linear models or rule lists. Another widely-used model-agnostic method, SHAP , provides a unified framework that computes feature attributions based on the Shapley value and adheres to several axioms.

**Instability of LIME.** Despite being widely employed, LIME is known to be unstable, evidenced by divergent explanations under different random seeds . Many efforts have been devoted to stabilize LIME explanations. Zafar et al.  introduced a deterministic algorithm that utilizes hierarchical clustering for grouping training data and k-nearest neighbors for selecting relevant data samples. However, the resulting explanations may not be a good local approximation. Addressing this concern, Shankaranarayana et al.  trained an auto-encoder to function as a more suitable weighting function in LIME. Shi et al.  incorporated feature correlation into the sampling step and considered a more restricted sampling distribution, thereby enhancing stability. Zhou et al.  employed a hypothesis testing framework to determine the necessary number of samples for ensuring stable explanations. However, this improvement came at the expense of a substantial increase in computation time.

**Impact of references.** LIME, along with various other explanation methods, relies on references (also known as baseline inputs) to generate samples. References serve as uninformative inputs meant to represent the absence of features . Choosing an inappropriate reference can lead to misleading explanations. For instance, if a black image is selected as the reference, important black pixels may not be highlighted . The challenge lies in determining the appropriate reference, as different types of references may yield different explanations . In , both black and white references are utilized, while  employs constant, noisy, and Gaussian blur references simultaneously. To address the reference specification issue,  proposes Expected Gradient, considering each instance in the data distribution as a reference and averaging explanations computed across all references.

## 7 Conclusion

In this paper, we introduce Glime, a novel framework that extends the LIME method for local feature importance explanations. By explicitly incorporating locality into the sampling procedure and enabling more flexible distribution choices, Glime mitigates the limitations of LIME, such as instability and low local fidelity. Experimental results on ImageNet data demonstrate that Glime significantly enhances stability and local fidelity compared to LIME. While our experiments primarily focus on image data, the applicability of our approach readily extends to text and tabular data.

## 8 Acknowledgement

The authors would like to thank the anonymous reviewers for their constructive comments. Zeren Tan and Jian Li are supported by the National Natural Science Foundation of China Grant (62161146004). Yang Tian is supported by the Artificial and General Intelligence Research Program of Guo Qiang Research Institute at Tsinghua University (2020GQG1017).