# Aleatoric and Epistemic Discrimination:

Fundamental Limits of Fairness Interventions

 Hao Wang

MIT-IBM Watson AI Lab

hao@ibm.com

&Luxi (Lucy) He

Harvard College

luxihe@college.harvard.edu

&Rui Gao

The University of Texas at Austin

rui.gao@mccombs.utexas.edu

&Flavio P. Calmon

Harvard University

flavio@seas.harvard.edu

###### Abstract

Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: _aleatoric discrimination_, which is inherent in the data distribution, and _epistemic discrimination_, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination.

## 1 Introduction

Algorithmic discrimination may occur in different stages of the machine learning (ML) pipeline. For example, historical biases in the data-generating process can propagate to downstream tasks; human biases can influence a ML model through inductive bias; optimizing solely for accuracy can lead to disparate model performance across groups in the data (Suresh and Guttag, 2019; Mayson, 2019). The past years have seen a rapid increase in algorithmic interventions that aim to mitigate biases in ML models (see e.g., Zemel et al., 2013; Feldman et al., 2015; Calmon et al., 2017; Menon and Williamson, 2018; Zhang et al., 2018; Zafar et al., 2019; Friedler et al., 2019; Bellamy et al., 2019; Kim et al., 2019; Celis et al., 2019; Yang et al., 2020; Jiang and Nachum, 2020; Jiang et al., 2020; Martinez et al., 2020; Lowy et al., 2021; Alghamdi et al., 2022). A recent survey (Hort et al., 2022) found _nearly 400_ fairness-intervention algorithms, including 123 pre-processing, 212 in-processing, and 56 post-processing algorithms introduced in the past decade.

Which sources of biases are (the hundreds of) existing fairness interventions trying to control? In order to create effective strategies for reducing algorithmic discrimination, it is critical to disentangle where biases in model performance originate. For instance, if a certain population group has significantly more missing features in training data, then it is more beneficial to collect data than selecting a more complex model class or training strategy. Conversely, if the model class does not accurately representthe underlying distribution of a specific population group, then collecting more data for that group will not resolve performance disparities.

We divide algorithmic discrimination1 into two categories: aleatoric and epistemic discrimination.2 Aleatoric discrimination captures inherent biases in the data distribution that can lead to unfair decisions in downstream tasks. Epistemic discrimination, in turn, is due to algorithmic choices made during model development and lack of knowledge about the optimal "fair" predictive model.

In this paper, we provide methods for measuring aleatoric and epistemic discrimination in classification tasks for group fairness metrics. Since aleatoric discrimination only depends on properties of the data distribution and the fairness measure of choice, we quantify it by asking a fundamental question:

_For a given data distribution, what is the best achievable performance (e.g., accuracy)_

_under a set of group fairness constraints?_

We refer to the answer as the _fairness Pareto frontier_. This frontier delineates the optimal performance achievable by a classifier when unlimited data and computing power are available. For a fixed data distribution, the fairness Pareto frontier represents the ultimate, information-theoretic limit for accuracy and group fairness beyond which no model can achieve. Characterizing this limit enables us to (i) separate sources of discrimination and create strategies to control them accordingly; (ii) evaluate the effectiveness of existing fairness interventions for reducing epistemic discrimination; and (iii) inform the development of data collection methods that promote fairness in downstream tasks.

At first, computing the fairness Pareto frontier can appear to be an intractable problem since it requires searching over all possible classifiers--even if the data distribution is known exactly. Our main technical contribution is to provide an upper bound estimate for this frontier by solving a sequence of optimization problems. The proof technique is based on Blackwell's seminal results (Blackwell, 1953), which proposed the notion of comparisons of statistical experiments and inspired a line of works introducing alternative comparison criteria (see e.g., Shannon, 1958; Cam, 1964; Torgersen, 1991; Cohen et al., 1998; Raginsky, 2011). Here, we apply these results to develop an algorithm that iteratively refines the achievable fairness Pareto frontier. We also prove convergence guarantees for our algorithm and demonstrate how it can be used to benchmark existing fairness interventions.

We quantify epistemic discrimination by comparing a classifier's performance with the information-theoretic optimal given by the fairness Pareto frontier. Our experiments indicate that given sufficient data, state-of-the-art (SOTA) group fairness interventions are effective at reducing epistemic discrimination as their gap to the information-theoretic limit is small (see Figure 1 and 2). Consequently, there are diminishing returns in benchmarking new fairness interventions on standard (overused) tabular datasets (e.g., UCI Adult and ProPublica COMPAS datasets). However, existing interventions _do not_ eliminate aleatoric discrimination as this type of discrimination is not caused by choice of learning algorithm or model class, and is due to the data distribution. Factors such as data missing values can significantly contribute to aleatoric discrimination. We observe that when population groups have disparate missing patterns, aleatoric discrimination escalates, leading to a sharp decline in the effectiveness of fairness intervention algorithms (see Figure 3).

#### Related Work

There is significant work analyzing the tension between group fairness measures and model performance metrics (see e.g., Kleinberg et al., 2016; Chouldechova, 2017; Corbett-Davies et al., 2017; Chen et al., 2018; Wick et al., 2019; Dutta et al., 2020; Wang et al., 2021). For example, there is a growing body of work on omnipredictors (Gopalan et al., 2021; Hu et al., 2023; Globus-Harris et al., 2023) discussing how, and under which conditions, the fair Bayes optimal classifier can be derived using post-processing techniques from multicibrated regressors. While previous studies (Hardt et al., 2016; Corbett-Davies et al., 2017; Menon and Williamson, 2018; Chzhen et al., 2019; Yang et al., 2020; Zeng et al., 2022, 2022) have investigated the fairness Pareto frontier and fair Bayesoptimal classifier, our approach differs from this prior work in the following aspects: our approach is applicable to _multiclass_ classification problems with _multiple_ protected groups; it _avoids disparate treatment_ by not requiring the classifier to use group attributes as an input variable; and it can handle _multiple_ fairness constraints simultaneously and produce fairness-accuracy trade-off curves (instead of a single point). Additionally, our proof techniques based on Blackwell's results on comparing statistical experiments are unique and may be of particular interest to fair ML and information theory communities. We present a detailed comparison with this line of work in Table 2 of Appendix E.

We recast the fairness Pareto frontier in terms of the conditional distribution \(P_{}|}\) of predicted outcome \(}\) given true label \(\) and group attributes \(\). This conditional distribution is related to confusion matrices conditioned on each subgroup. In this regard, our work is related to Verma and Rubin (2018); Alghamdi et al. (2020); Kim et al. (2020); Yang et al. (2020); Berk et al. (2021), which observed that many group fairness metrics can be written in terms of the confusion matrices for each subgroup. Among them, the closest work to ours is Kim et al. (2020), which optimized accuracy and fairness objectives over these confusion matrices and proposed a post-processing technique for training fair classifiers. However, they only imposed marginal sum constraints for the confusion matrices. We demonstrate that the feasible region of confusion matrices can be much smaller (see Remark 2 for an example), leading to a tighter approximation of the fairness Pareto frontier.

Recently, many strategies have been proposed to reduce the tension between group fairness and model performance by investigating properties of the data distribution. For example, Blum and Stangl (2019); Suresh and Guttag (2019); Fogliato et al. (2020); Wang et al. (2020); Mehrotra and Celis (2021); Fernando et al. (2021); Wang and Singh (2021); Zhang and Long (2021); Tomasev et al. (2021); Jacobs and Wallach (2021); Kallus et al. (2022); Jeong et al. (2022) studied how noisy or missing data affect fairness and model accuracy. Dwork et al. (2018); Ustun et al. (2019); Wang et al. (2021) considered training a separate classifier for each subgroup when their data distributions are different. Another line of research introduces data pre-processing techniques that manipulate data distribution for reducing its bias (e.g., Calmon et al., 2017; Kamiran and Calders, 2012). Among all these works, the closest one to ours is Chen et al. (2018), which decomposed group fairness measures into bias, variance, and noise (see their Theorem 1) and proposed strategies for reducing each term. Compared with Chen et al. (2018), the main difference is that we characterize a fairness Pareto frontier that depends on fairness metrics _and_ a performance measure, giving a complete picture of how the data distribution influences fairness and accuracy.

## 2 Preliminaries

Next, we introduce notation, overview the key results in Blackwell (1953) on comparisons of experiments, and outline the fair classification setup considered in this paper.

Notation.For a positive integer \(n\), let \([n]\{1,,n\}\). We denote all probability distributions on the set \(\) by \(()\). Moreover, we define the probability simplex \(_{m}([m])\). When random variables \(\), \(\), \(\) form a Markov chain, we write \(--\). We write the mutual information between \(\), \(\) as \(I(;)_{P_{,}} [,}()}{P_{}()P_{}()}]\). Since \(I(;)\) is determined by the marginal distribution \(P_{}\) and the conditional distribution \(P_{|}\), we also write \(I(;)\) as \(I(P_{};P_{|})\). When \(\), \(\) are independent, we write \(\_\|\).

If a random variable \([n]\) has finite support, the conditional distribution \(P_{|}:[n]()\) can be equivalently written as \((P_{1},,P_{n})\) where each \(P_{i}=P_{|=i}()\). Additionally, if \(\) is a finite set \([m]\), then \(P_{|}\) can be fully characterized by a transition matrix. We use \((m|n)\) to denote all transition matrices from \([n]\) to \([m]\): \(^{n m}0 P_{i,j} 1,_{j=1}^{m}P_{i,j }=1, i[n]}\).

Comparisons of ExperimentsGiven two statistical experiments (i.e., conditional distributions) \(\) and \(\), is there a way to decide which one is more informative? Here \(\) and \(\) have the common input alphabet \([n]\) and potentially different output spaces. Blackwell gave an answer in his seminal work (Blackwell, 1953) from a decision-theoretic perspective. We review these results next.

Let \(\) be a closed, bounded, convex subset of \(^{n}\). A decision function \((x)=(a_{1}(x),,a_{n}(x))\) is any mapping from \(\) to \(\). It is associated with a loss vector:

\[()=( a_{1}(x)P_{1}(x),, a_{n}(x) P_{n}(x)).\] (1)

The collection of all \(()\) is denoted by \((,)\). Blackwell defined that \(\) is more informative than \(\) if for every \(\), \((,)(,)\). Intuitively, this result means any risk achievable with \(\) is also achievable with \(\). Moreover, Blackwell considered the standard measure \(P^{*}\) which is the probability distribution of \((})\) where \((x):_{n}\) is a function defined as

\[(P_{1}}{P_{1}++P_{n}},, P_{n}}{P_{1}++P_{n}}).\] (2)

and \(}\) follows the probability distribution \(++P_{n}}{n}\). One of the most important findings by Blackwell in his paper is to discover the following equivalent conditions.

**Lemma 1** (Blackwell (1951, 1953)).: _The following three conditions are equivalent:_

* \(\) _is more informative than_ \(\)_;_
* _for any continuous and convex function_ \(:_{n}\)_,_ \(()dP^{*}()()dQ^{*}()\)_;_
* _there is a stochastic transformation_ \(\) _s.t._ \(P_{i}=Q_{i}\)_. In other words, there exists a Markov chain_ \(--\) _for any distributions on_ \(\) _such that_ \(=P_{|}\) _and_ \(=P_{|}\)_._

If \(=P_{|}\) is more informative than \(=P_{|}\), by the third condition of Lemma 1 and the data processing inequality, \(I(P_{};P_{|}) I(P_{};P_{|})\) holds for _any_ marginal distribution \(P_{}\). However, the converse does not hold in general--even if the above inequality holds for any \(P_{}\), \(\) is not necessarily more informative than \(\)(Rauh et al., 2017). In this regard, Blackwell's conditions are "stronger" than the mutual information based data processing inequality.

#### Group Fair Classification

Consider a multi-class classification task, where the goal is to train a probabilistic classifier \(h:_{C}\) that uses input features \(\) to predict their true label \([C]\). Additionally, assume the classifier produces a predicted outcome \(}[C]\) and let \([A]\) represent group attributes (e.g., race and sex). Depending on the domain of interest, \(\) can either include or exclude \(\) as an input to the classifier. Our framework can be easily extended to the setting where multiple subgroups overlap (Kearns et al., 2018). Throughout this paper, we focus on three standard group fairness measures: statistical parity (SP) (Feldman et al., 2015), equalized odds (EO) (Hardt et al., 2016; Pleiss et al., 2017), and overall accuracy equality (OAE) (Berk et al., 2021) (see Table 1 for their definitions) but our analysis can be extended to many other group fairness metrics, including the ones in Table 1 of Kim et al. (2020), as well as alternative performance measures beyond accuracy.

   Fairness Metric & Abbr. & Definition \\  & & Expression w.r.t. \(\) \\  Statistical Parity & \(_{}\) & \(|(}=|=s)-(}=| =s^{})|_{}\) \\  & \(|_{y=1}^{C}(}{_{s}}P_{(y,y),}-,y}}{_{s^{}}}P_{(s^{},y),})| _{}\) \\  Equalized Odds & \(_{}\) & \(|(}=|=s,=y)-(}= |=s^{},=y)|_{}\) \\  & \(P_{(s,y),}-P_{(s^{},y),}|_{}\) \\  Overall Accuracy Equality & \(_{}\) & \(|(}=|=s)-(}=| =s^{})|_{}\) \\  & \(|_{y=1}^{C}(}{_{s}}P_{(s,y),y}-,y}}{_{s^{}}}P_{(s^{},y),y})|_{ }\) \\   

Table 1: Standard group fairness metrics under multi-group and multi-class classification tasks. Here \(_{},_{},_{},_{}, \) are threshold parameters, \(,y[C]\), \(s,s^{}[A]\), and \(_{s,y}\), \(_{s}\) are defined in Proposition 1. Our analysis can be extended to many other group fairness metrics (see e.g., Table 1 in Kim et al., 2020).

## 3 Fairness Pareto Frontier

In this section, we introduce our main concept--fairness Pareto frontier (\(\)). We use it to measure aleatoric discrimination and quantify epistemic discrimination by comparing a classifier's performance to the \(\). We recast \(\) in terms of the conditional distribution \(P_{}|,}\) and apply Blackwell's conditions to characterize the feasible region of this conditional distribution. This effort converts a functional optimization problem into a convex program with a small number of variables. However, this convex program may involve infinitely many constraints. Hence, we introduce a greedy improvement algorithm that iteratively refines the approximation of \(\) and tightens the feasible region of \(P_{}|,}\). Finally, we establish a convergence guarantee for our algorithm.

Recall that we refer to aleatoric discrimination as the inherent biases of the data distribution that can lead to an unfair or inaccurate classifier. As its definition suggests, aleatoric discrimination only relies on properties of the data distribution and fairness metric of choice--it does not depend on the hypothesis class nor optimization method. Below we introduce \(\) that delineates a curve of optimal accuracy over all probabilistic classifiers under certain fairness constraints for a given data distribution \(P_{,,}\). We use \(\) to quantify aleatoric discrimination.

**Definition 1**.: For \(_{},_{},_{} 0\) and a given \(P_{,,}\), we define \((_{},_{},_{})\) by

\[(_{},_{}, _{})_{h} [_{}=}]\] (3a) \[ _{},_{ },_{}\] (3b)

where \(\) is the indicator function; \(}\) is produced by applying the classifier \(h\) to \(\); the maximum is taken over all measurable \(h\); and the definitions of \(\), \(\), and \(\) are in Table 1. As a special case, if \(_{},_{},_{} 1\), then \((_{},_{},_{})\) is the accuracy of the Bayes optimal classifier.

Solving this functional optimization problem is difficult since it optimizes over all measurable classifiers. There is a line of works that proposed different fairness-intervention algorithms for training group-fair classifiers (see e.g., Menon and Williamson, 2018; Zhang et al., 2018; Zafar et al., 2019; Celis et al., 2019; Yang et al., 2020; Wei et al., 2021; Alghamdi et al., 2022). They restrict the model class and vary loss functions and optimizers to find classifiers that approach \(\) as close as possible. However, these algorithms only describe a lower bound for \(\). They do not determine what is the _best_ achievable accuracy for a given set of fairness constraints.

We circumvent the above-mentioned challenges by rewriting \(\) in terms of the conditional distribution \(P_{}|,}\). The caveat is that although each classifier yields a \(P_{}|,}\), not every conditional distribution corresponds to a valid classifier. Hence, we introduce the following definition which characterizes all feasible \(P_{}|,}\).

**Definition 2**.: Given \(P_{|,}\), we define \(\) as the set of all conditional distributions \(P_{}|,}\) where \(}\) is produced by some probabilistic classifier \(h\). In other words,

\[\{P_{}|,}(,)--}\}.\] (4)

Throughout this paper, we write \(P_{}|,}\) or its corresponding transition matrix \((C|AC)\) interchangeably. Specifically, the \((C(s-1)+y)\)-th row, \(\)-th column of \(\) represents \(P_{}|,}(|s,y)\) and we denote it by \(P_{(s,y),}\).

**Remark 1**.: We demonstrate the connection between the conditional distribution \(P_{}|,}\) and confusion matrices in the setting of binary classification with binary groups. We define \(}\) as the counterpart of \(\) when we replace \(P_{|,}\) with an empirical distribution \(_{|,}\) computed from a dataset. The confusion matrix for group \(s\{0,1\}\) consists of four numbers: True Positive (\(_{s}\)), False Positive (\(_{s}\)), False Negative (\(_{s}\)), True Negative (\(_{s}\)). Assume that the number of positive-label data \(n_{s}^{+}=_{s}+_{s}\) and negative-label data \(n_{s}^{-}=_{s}+_{s}\) are given--these numbers do not depend on the classifier. Then there is a one-to-one mapping from each element in \(}\) to a confusion matrix:

\[_{}|,}(1|s,1) =^{+}}_{s},_{ }|,}(1|s,0)=^{-}}_{s},\] \[_{}|,}(0|s,1) =^{+}}_{s},_{}| ,}(0|s,0)=^{-}}_{s}.\]Hence, \(}\) essentially characterizes all feasible confusion matrices and \(\) is the population counterpart of \(}\). Note that \(\) is determined by the underlying data distribution while \(}\) (and confusion matrices) are tailored to a specific dataset.

**Proposition 1**.: \((_{},_{},_{})\) _in (3) is equal to the solution of the following convex optimization:_

\[_{^{AC C}} _{s=1}^{A}_{y=1}^{C}_{s,y}P_{(s,y),y}\] (5a) \[ _{},_{},_{}\] (5b) \[.\] (5c)

_Here the constants \(_{s,y}(=s,=y)\) and \(_{s}(=s)\) for \(s[A]\), \(y[A]\) and \(P_{(s,y),}\) denotes the \((C(s-1)+y)\)-th row, \(\)-th column of the transition matrix \(\), which is \(P_{}|,}(|s,y)\)._

For example, in binary classification with a binary group attribute, the above optimization only has \(8\) variables, \(14\) linear constraints + a single convex constraint \(\). Hence, standard convex optimization solvers can directly compute its optimal value as long as we know how to characterize \(\).

**Remark 2**.: Note that Kim et al. (2020) investigated fairness Pareto frontiers via confusion matrices. The main difference is that Definition 1 in Kim et al. (2020) relaxed the constraint (5c) to \((C|AC)\) where \((C|AC)\) represents _all_ transition matrices from \([AC]\) to \([C]\). This leads to a loose approximation of the frontier because \(\) is often a strict subset of \((C|AC)\). To demonstrate this point, consider the scenario where \(\_\|(,)\). Then \(}\_\|(,)\) by data processing inequality so

\[=\{(C|AC)\}.\] (6)

Optimizing over \(\) rather than \((C|AC)\) can significantly tighten the fairness Pareto frontier.

Before diving into the analysis, we first introduce a function \(:_{AC}\) defined as \((x)=(P_{,|}(1,1|x),,P_{,|}(A,C|x)).\) To obtain this function in practice, a common strategy among various post-processing fairness interventions (see e.g., Menon and Williamson, 2018; Alghamdi et al., 2022) is to train a probabilistic classifier that uses input features \(\) to predict \((,)\). The output probability generated by this classifier is then utilized as an approximation of the function \(\).

The following theorem is the main theoretical result in this paper. It provides a precise characterization of the set \(\) through a series of convex constraints.

**Theorem 1**.: _The set \(\) is the collection of all transition matrices \((C|AC)\) such that the following condition holds: For any \(k\) and any \(\{_{i}_{i}[-1,1]^{AC},i[k]\}\),_

\[_{=1}^{C}_{i[k]}\{_{i}^{T}_{ }_{}\}[_{i[k]}\{_{i}^{T} ()\}],\] (7)

_where \(_{}\) is the \(\)-th column of \(\) and \(_{}=(_{1,1},,_{A,C})\)._

Intuitively, (7) uses piecewise linear functions to approximate the boundary of the convex set \(\) where \(k\) represents the number of linear pieces. Unfortunately, replacing \(\) with this series of constraints in (5) may result in an intractable problem since standard duality-based approaches will lead to infinitely many dual variables. To resolve this issue, we first fix \(k\) and let \(_{k}\) be the set of \(\) such that (7) holds under this fixed \(k\). Accordingly, we define \(_{k}(_{},_{},_{ })\) as the optimal value of (5) when replacing \(\) with \(_{k}\). Since \(_{1}_{2}\), we have \(_{1}(_{},_{},_{})_{2}(_{}, _{},_{}) (_{},_{},_{}).\) However, computing \(_{k}(_{},_{},_{})\) still involves infinitely many constraints.

Next, we introduce a greedy improvement algorithm that consists of solving a sequence of tractable optimization problems for approximating \(_{k}(_{},_{},_{ })\). We use \(\) to collect the constraints of \(\) and set \(=\) initially. At each iteration, our algorithm solves a convex program to find an optimal \(\) that maximizes the accuracy while satisfying the desired group fairness constraints and the constraints in \(\); then we verify if this \(\) is within the set \(_{k}\) by solving a DC (difference of convex) program (Shen et al., 2016; Horst and Thoai, 1999). If \(_{k}\), then the algorithm stops. Otherwise, the algorithm will find the constraint that is mostly violated by \(\) and add this constraint to \(\). Specifically, we determine a piecewise linear function that divides the space into two distinct regions: one containing \(\) and the other containing \(_{k}\). By "mostly violated", we mean the function is constructed to maximize the distance between \(\) and the boundary defined by the function. We describe our algorithm in Algorithm 1 and establish a convergence guarantee below.

``` Input:\(=\{(x_{i},y_{i},s_{i})\}_{i=1}^{N}\), max number of iterations \(T\); max pieces \(k\); classifier \(g(x)\); \(_{}\), \(_{}\), \(_{}\). Initialize: set \(=\); \(_{s,y}==s,y_{i}=y\}|}{N}\); \(t=1\). Repeat:  Solve a convex program: \[_{} _{s=1}^{A}_{y=1}^{C}_{s,y}P_{(s,y),y}\] \[ (|AC),_{}, _{},_{}\] \[_{=1}^{C}_{i[k]}\{_{i}^{T}_{}_{}\}[_{i[k]}\{_{i}^{T}()\}](_{1},,_{k}) .\] Let \(v^{t}\) and \(^{t}\) be the optimal value and optimal solution.  Solve a DC program: \[_{_{i}[-1,1]^{AC}\\ i[k]}\;[_{i[k]}\{_{i}^{T}( )\}]-_{=1}^{C}_{i[k]}\{_{i}^{T} _{}_{}^{t}\}.\] If the optimal value is \( 0\) or \(t=T\), stop; otherwise,  add the optimal \((_{1},,_{k})\) to \(\) and \(t=t+1\). return:\(v^{t}\), \(^{t}\), \(\). ```

**Algorithm 1** Approximate the fairness Pareto frontier.

**Theorem 2**.: _Let \(T=\). If Algorithm 1 stops, its output \(^{t}\) is an optimal solution of \(_{k}(_{},_{},_{ {oae}})\). Otherwise, any convergent sub-sequence of \(\{^{t}\}_{t=1}^{}\) converges to an optimal solution of \(_{k}(_{},_{},_{ })\)._

Note that the output \(v^{t}\) from Algorithm 1 is always an _upper bound_ for \((_{},_{},_{ {oae}})\), assuming the estimation error is sufficiently small. The tightness of this upper bound is determined by \(k\) (i.e., how well the piecewise linear functions approximate the boundary of \(\)), \(T\) (i.e., the total number of iterations). On the other hand, running off-the-shelf in-processing and post-processing fairness interventions can only yield _lower bounds_ for \((_{},_{},_{ {oae}})\).

## 4 Numerical Experiments

In this section, we demonstrate the tightness of our upper bound approximation of \(\), apply it to benchmark existing group fairness interventions, and show how data biases, specifically missing values, impact their effectiveness. We find that given sufficient data, SOTA fairness interventions are successful at reducing epistemic discrimination as their gap to (our upper bound estimate of) \(\) is small. However, we also discover that when different population groups have varying missing data patterns, aleatoric discrimination increases, which diminishes the performance of fairness intervention algorithms. Our numerical experiments are semi-synthetic since we apply fairness interventions to train classifiers using the _entire_ dataset and resample from it as the test set. This setup enables us to eliminate the estimation error associated with Algorithm 1 (see Appendix E for a discussion). We provide additional experimental results and details in Appendix C.

### Benchmark Fairness Interventions

Setup.We evaluate our results on the UCI Adult dataset (Bache and Lichman, 2013), the ProPublica COMPAS dataset (Angwin et al., 2016), the German Credit dataset (Bache and Lichman, 2013), and HLSLS (High School Longitudinal Study) dataset (Ingels et al., 2011; Jeong et al., 2022). We recognize that Adult, COMPAS, and German Credit datasets are overused and acknowledge the recent calls to move away from them (see e.g., Ding et al., 2021). We adopt these datasets for benchmarking purposes only since most fairness interventions have available code for these datasets. The HLS dataset is a new dataset that first appeared in the fair ML literature last year and captures a common use-case of ML in education (student performance prediction, see Jeong et al., 2022). It has multi-class labels and multiple protected groups. We apply existing (group) fairness interventions to these datasets and measure their fairness violations via _Max equalized odds_:

\[\,|(}=|=s,=y)-(}=|=s^{},=y)|\]

where the max is taken over \(y,,s,s^{}\). We run Algorithm 1 with \(k=6\) pieces, \(20\) iterations, and varying \(_{}\) to estimate FairFront on each dataset. We compute the expectations and the \(g\) function from the empirical distributions and solve the DC program by using the package in Shen et al. (2016). The details about how we pre-process these datasets and additional experimental results on the German Credit and HLSLS datasets are deferred to Appendix C.

Group fairness interventions.We consider five existing fairness-intervention algorithms: Reduction (Agarwal et al., 2018), EqQdds (Hardt et al., 2016), CalEqQdds (Pleiss et al., 2017), LevEqQpp (Chzhen et al., 2019), and FairProjection Alghamdi et al. (2022). Among them, Reduction is an in-processing method and the rest are all post-processing methods. For the first three benchmarks, we use the implementations from IBM AIF360 library (Bellamy et al., 2018); for LevEqQpp and FairProjection, we use the Python implementations from the Github repo in Alghamdi et al. (2022). For Reduction and FairProjection, we can vary their tolerance of fairness violations to produce a fairness-accuracy curve; for EqQdds, CalEqQdds, and LevEqQpp, each of them produces a single point since they only allow hard equality constraint. We note that FairProjection is optimized for transforming probabilistic classifier outputs (see also Wei et al., 2021), but here we threshold the probabilistic outputs to generate binary predictions which may limit its performance. Finally, we train a random forest as the Baseline classifier.

Results.We observe that if we run Algorithm 1 for a single iteration, which is equivalent to solving Proposition 1 without (5c), its solution is very close to \(1\) for all \(_{}\). This demonstrates the benefits of incorporating Blackwell's conditions into the fairness Pareto frontier.

Figure 1: We compare Reduction and FairProjection with (our upper bound estimate of) FairFront on the Adult (Left) and COMPAS (Right) datasets. We train a classifier that approximates the Bayes optimal and use it as a basis for Reduction and FairProjection. This result not only demonstrates the tightness of our approximation but also shows that SOTA fairness interventions have already achieved near-optimal fairness-accuracy curves.

We train a classifier that approximates the Bayes optimal and use it as a basis for both Reduction and FairProjection, which are SOTA fairness interventions. We then apply these two fairness interventions to the entire dataset and evaluate their performance on the same dataset. Figure 1 shows that in this infinite sample regime, the fairness-accuracy curves produced by Reduction and FairProjection can approach our upper bound estimate of FairFront. This result not only demonstrates the tightness of our approximation (recall that Algorithm 1 gives an upper bound of FairFront and existing fairness interventions give lower bounds) but also shows that SOTA fairness interventions have already achieved near-optimal fairness-accuracy curves.

Recall that we use FairFront to quantify aleatoric discrimination since it characterizes the highest achievable accuracy among all classifiers satisfying the desired fairness constraints. Additionally, we measure epistemic discrimination by comparing a classifier's accuracy and fairness violation with FairFront. Given that our Algorithm 1 provides a tight approximation of FairFront, we use it to benchmark existing fairness interventions. Specifically, we first train a base classifier which may not achieve Bayes optimal accuracy. Then we use it as a basis for all existing fairness interventions. The results in Figure 2 show that SOTA fairness interventions remain effective at reducing epistemic discrimination. In what follows, we demonstrate how missing values in data can increase aleatoric discrimination and dramatically reduce the effectiveness of SOTA fairness interventions.

### Fairness Risks in Missing Values

Real-world data often have missing values and the missing patterns can be different across different protected groups (see Jeong et al., 2022, for some examples). There is a growing line of research (see e.g., Jeong et al., 2022; Fernando et al., 2021; Wang and Singh, 2021; Subramonian et al., 2022; Caton et al., 2022; Zhang and Long, 2021; Schelter et al., 2019) studying the fairness risks of data with missing values. In this section, we apply our result to demonstrate how disparate missing patterns influence the fairness-accuracy curves.

Setup.We choose sex (group 0: female, group 1: male) as the group attribute for the Adult dataset, and race (group 0: African-American, group 1: Caucasian) for the COMPAS dataset. To investigate the impact of disparate missing patterns on aleatoric discrimination, we artificially generate missing values in both datasets. This is necessary as the datasets do not contain sufficient missing data. The missing values are generated according to different probabilities for different population groups. For each data point from group 0, we erase each input feature with a varying probability \(p_{0}\{10\%,50\%,70\%\}\), while for group 1, we erase each input feature with a fixed probability \(p_{1}=10\%\). We then apply mode imputation to the missing values, replacing them with the mode of non-missing values for each feature. Finally, we apply Algorithm 1 along with Reduction and Baseline to the imputed data. The experimental results are shown in Figure 3.

Figure 2: We benchmark existing fairness interventions using (our upper bound estimate of) FairFront. We use FairFront to quantify aleatoric discrimination and measure epistemic discrimination by comparing a classifier’s accuracy and fairness violation with FairFront. The results show that SOTA fairness interventions are effective at reducing epistemic discrimination.

Results.As we increase the missing probability of group 0, (our upper bound estimate of) FairFront decreases since it becomes more difficult to accurately predict outcomes for group 0. This in turn affects the overall model performance, since the fairness constraint requires that the model performs similarly for both groups. We also observe the fairness-accuracy curves of Reduction decrease as the missing data for group 0 become more prevalent. In other words, as the missing data for group 0 increase, it becomes more difficult to maintain both high accuracy and fairness in the model's prediction.

## 5 Final Remarks

The past years have witnessed a growing line of research introducing various group fairness-intervention algorithms. Most of these interventions focus on optimizing model performance subject to group fairness constraints. Though comparing and benchmarking these methods on various datasets is valuable (e.g., see benchmarks in Friedler et al., 2019; Bellamy et al., 2019; Wei et al., 2021), this does not reveal if there is still room for improvement in their fairness-accuracy curves, or if existing methods approach the information-theoretic optimal limit when infinite data is available. Our results address this gap by introducing the fairness Pareto frontier, which measures the highest possible accuracy under a set of group fairness constraints. We precisely characterize the fairness Pareto frontier using Blackwell's conditions and present a greedy improvement algorithm that approximates it from data. Our results show that the fairness-accuracy curves produced by SOTA fairness interventions are very close to the fairness Pareto frontier on standard datasets.

Additionally, we demonstrate that when data are biased due to missing values, the fairness Pareto frontier degrades. Although existing fairness interventions can still reduce performance disparities, they come at the cost of significantly lowering overall model accuracy. The methods we present for computing the fairness Pareto frontier can also be applied to analyze other sources of aleatoric discrimination, such as when individuals may misreport their data or when there are measurement errors. Overall, the fairness Pareto frontier can serve as a valuable framework for guiding data collection and cleaning. Our results indicate that existing fairness interventions can be effective in reducing epistemic discrimination, and there are diminishing returns in developing new fairness interventions focused solely on optimizing accuracy for a given group fairness constraint on pristine data. However, existing fairness interventions have yet to effectively provide both fair and accurate classification when additional sources of aleatoric discrimination are present (such as missing values in data). This suggests that there is still significant need for research on handling aleatoric sources of discrimination that appear throughout the data collection process.

We provide an in-depth discussion on future work in Appendix E.

Figure 3: Fairness risks of disparate missing patterns. The missing probabilities of group 0 (female in Adult/African-American in COMPAS) and group 1 (male in Adult/Caucasian in COMPAS) are varying among \(\{(10\%,10\%),(50\%,10\%),(70\%,10\%)\}\). We apply Reduction and Baseline to the imputed data and plot their fairness-accuracy curves against FairFront. As shown, the effectiveness of fairness interventions substantially decrease with increasing disparate missing patterns in data.