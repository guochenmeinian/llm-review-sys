# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

We present a novel structured matrix class parametrized by an alternating product of block-diagonal matrices and several permutations. Multiplying by these matrices can be seen as a consecutive application of independent linear transforms within certain small groups and then shuffling the elements between them, hence the name Group-and-Shuffle matrices (or \(\)-matrices for short). This class generalizes Monarch matrices (Dao et al., 2022) and with the right permutation choices, is able to form dense orthogonal matrices more effectively compared to approach proposed in BOFT, decreasing number of matrices in the product as well as the number of trainable parameters. We build efficient structured orthogonal parametrization with this class and use it to construct a new parameter-efficient fine-tuning method named GSOFT.

Our contributions:

* We introduce a new class of structured matrices, called \(\), that is more effective at forming dense matrices than block butterfly matrices from the BOFT method.
* Using \(\)-matrices, we propose an efficient structured orthogonal parametrization, provide theoretical insights and study its performance in the orthogonal fine-tuning framework.
* We adapt our ideas for convolutional architectures, providing a framework to compress and speed-up orthogonal convolution layers.

## 2 Orthogonal Fine-tuning

Orthogonal Fine-tuning method (OFT) introduced in (Qiu et al., 2023) is a Parameter-Efficient Fine-Tuning (PEFT) method which fine-tunes pre-trained weight matrices through a learnable orthogonal block-diagonal matrix. Some of the properties that make orthogonal transforms desirable are preservation of pair-wise angles of neurons, spectral properties and hyperspherical energy. More precisely, OFT optimizes an orthogonal matrix \(Q^{d d}\) for a pre-trained frozen weight matrix \(W^{0}^{d n}\) and modifies the multiplication \(y=(W^{0})^{}x\) to \(y=(QW^{0})^{}x\). Note that the identity matrix \(I\) is orthogonal, which makes it a natural initialization for \(Q\). OFT uses block-diagonal structure for \(Q\), parameterizing it as

\[Q=(Q_{1},Q_{2},,Q_{r}),\]

where \(Q_{i}^{b b}\) are small orthogonal matrices and \(br=d\). Orthogonality is enforced by Cayley parametrization, i.e.

\[Q_{i}=(I+K_{i})(I-K_{i})^{-1},\]

where \(K_{i}\) are skew-symmetric: \(K_{i}=-K_{i}^{}\). This ensures orthogonality of \(Q_{i}\) and, hence, of \(Q\).

Nevertheless, block-diagonal matrices can be too restrictive, as they divide neurons into \(r\) independent groups based on their indices. This motivates the construction of dense parameter-efficient orthogonal matrices. To address this problem, the Orthogonal Butterfly method (BOFT) was introduced (Liu et al., 2024). BOFT uses block-butterfly structure to construct \(Q\). Essentially, \(Q\) is parameterized as a product of \(m\) orthogonal sparse matrices:

\[Q=B_{m}B_{m-1} B_{1}.\]

Each matrix \(B_{i}\) is a block-diagonal matrix up to a permutation of rows and columns, consisting of \(r\) block matrices of sizes \(b b\). Similarly to OFT, the orthogonality is enforced by the Cayley parametrization applied to each block. However, BOFT method has some areas for improvement as well. To construct a dense matrix, BOFT requires at least

\[m=1+_{2}(r)\]

matrices. For example, the authors of BOFT use \(m=5\) or \(6\) matrices in the BOFT method for fine-tuning of Stable Diffusion (Rombach et al., 2022). Large amount of stacked matrices leads to significant time and memory overhead during training. There is also a room for improvement in terms of parameter-efficiency. To overcome these issues, we introduce a new class of structured matrices that we denote \(\) (group-and-shuffle) that generalizes Monarch matrices (Dao et al., 2022; Fu et al., 2024) and show how to use this class to construct parameter-efficient orthogonal parametrization. Similarly to BOFT, our approach uses block-diagonal matrices and permutations, but requires only

\[m=1+_{b}(r)\]

matrices of the same size to construct a dense matrix. See details in Section 5.2. The reduced requirements on \(m\) allow us to use \(m=2\) in experiments to maximize computational efficiency, while still maintaining accurate results.

## 3 \(\)-matrices

Our motivation within this work is to utilize orthogonal matrices of the form:

\[A=P_{L}(LPR)P_{R}\] (1)

where matrices \(L\) and \(R\) are block-diagonal matrices with \(r\) blocks of sizes \(b b\) and \(P_{L},P,P_{R}\) are certain permutation matrices, e.g. \(P_{L}=P^{},P_{R}=I\) in the orthogonal fine-tuning setting and \(P_{R}=P,P_{L}=I\) for convolutional architectures. Note that although the case \(P_{L}=P^{},P_{R}=I\) resembles Monarch matrices (Dao et al., 2022), they are unable to form such a structure, e.g., with equal-sized blocks in \(L\) and \(R\). The issue is that the Monarch class has a constraint that interconnects the number of blocks in matrix \(L\) and the number of blocks in matrix \(R\) (see Appendix C for details). Moreover, Monarch matrices have not been considered with orthogonality constraints.

To build matrices of the form (1), we first introduce a general class of \(\)-matrices and study its properties. We then discuss orthogonal matrices from this class in Section 4.

### Definition of \(\)-matrices

**Definition 3.1**.: _An \(m n\) matrix \(A\) is in \((P_{L},P,P_{R})\) class with \(k_{L},k_{R}\) blocks and block sizes \(b_{L}^{1} b_{L}^{2}\), \(b_{R}^{1} b_{R}^{2}\) if_

\[A=P_{L}(LPR)P_{R},\]

_where \(L=(L_{1},L_{2},,L_{k_{L}}),L_{i}^{b_{L} ^{1} b_{R}^{2}}\), \(R=(R_{1},R_{2},,R_{k_{R}})\), \(R_{i}^{b_{R}^{1} b_{R}^{2}}\), \(P_{L},P,P_{R}\) are permutation matrices and \(b_{L}^{2} k_{L}=b_{R}^{1} k_{R}=s,b_{L}^{1} k_{L}=m,b_{R}^{1}  k_{R}=n\)._

In practice, we fix \(P_{L},P,P_{R}\) depending on the application and only make matrices \(L,R\) subject for change. \(\)-matrices are hardware-efficient, as they are parametrized by two simple types of operations that can implemented efficiently: multiplications by block-diagonal matrices and permutations.

Let us also illustrate a forward pass \(Ax LPRx\) for a matrix \(A\)\((I,P,I)\) as a building block for the more general class with two additional permutations. The first operation \(y=Rx\) consists of several fully-connected layers, applied individually to subgroups of \(x\), see Figure 1. The next multiplication \(LPy\) ensures that these groups interact with each other. Indeed, the permutation matrix \(P\) shuffles the entries of \(y\) into new subgroups. These subgroups are then again processed by a number of fully-connected layers using \(L\). This motivates the naming for our class of matrices: _Group-and-Shuffle_ or \(\) for short.

Another useful insight on these matrices is that the class \((I,P,I)\) consists of block matrices with low-rank blocks. The permutation matrix \(P\) is responsible for the formation of these blocks and defines their ranks (note that rank may vary from block to block). The result below formally describes our findings and is key to the projection operation that we describe afterwards.

**Proposition 1**.: _Let \(A\) be a matrix from \((I,P,I)\) with a permutation matrix \(P\) defined by the function \(:\{0,,n-1\}\{0,,n-1\}\). Let \(\{v_{i}^{}\}\) - be the rows of the blocks \(R_{1},,R_{k_{R}}\), \(\{u_{i}\}\) - the columns of the blocks \(L_{1},,L_{k_{L}}\) in the consecutive order. Then the matrix \(A\) can be written as a block matrix with \(k_{L} k_{R}\) blocks using the following formula for each block \(A_{k_{1},k_{2}}\):_

\[A_{k_{1},k_{2}}=_{} =k_{1}\\ }{k_{R}}=k_{2}}u_{(i)}v_{i}^{ }.\]

_Note that we use zero-indexing for this proposition for simplicity of formulas._

Figure 1: \((I,P,I)\) matrices with \(b_{L}^{1}=b_{L}^{2}=3\), \(b_{R}^{1}=b_{R}^{2}=2\), \(k_{L}=2,k_{R}=3\). Edges between nodes denote nonzero weights.

Let us illustrate this proposition in Figure 2. We consider \((I,P,I)\) with \(k_{L}=4\) and \(k_{R}=2\) blocks in \(L\) and \(R\) and with the block sizes \(3 3\) and \(6 6\) respectively. Let us consider the leading block \(A_{00}\) of the size \(3 6\). According to Proposition 1, \(A_{00}=u_{0}v_{2}^{}+u_{2}v_{4}^{}\). Indeed, let us take a closer look, e.g., at the term \(u_{2}v_{4}^{}\). In the permutation matrix \(P\), we have a nonzero element in the position \((2,4)\) as \(i=4\) and \((4)=2\). Therefore, we select the third column \(u_{2}\) in \(L_{1}\) and the fifth row \(v_{4}^{}\) in \(R_{1}\). This leads to adding a rank-one term \(u_{2}v_{4}^{}\) to \(A_{00}\) as we see in the formula above.

Another direct corollary from Proposition 1 is a projection operation \(^{m n}(P_{L},P,P_{R})\) that satisfies:

\[(A)*{arg\,min}_{B(P_{L},P,P_{R})}\|A-B\|_{F},\]

where \(\|\|_{F}\) is the Frobenius norm. Thanks to the block low-rank representation of matrices from \((P_{L},P,P_{R})\), the projection \(\) is simply constructed using SVD truncations of the blocks \((P_{L}^{}AP_{R}^{})_{k_{1},k_{2}}\) and is summarized in Algorithm 1.

``` Input:\(A,P_{L},P,P_{R}\) Return:\(L,R\) for\(k_{1}=1 k_{L}\)do for\(k_{2}=1 k_{R}\)do  Compute SVD of \((P_{L}^{T}AP_{R}^{T})_{k_{1},k_{2}}=U V^{}\);  Set \(r=r_{k_{1},k_{2}}\) - rank of block determined by \(P\);  Take \(U_{r}=U[:r,:],\,_{r}=[:r,:r],V_{r}=V[:r,:]\);  Pack columns of \(U_{r}_{r}^{1/2}\) into \(L_{k_{1}}\) and rows of \(_{r}^{1/2}V_{r}\) into \(R_{k_{2}}\) according to \(P\); endfor endfor ```

**Algorithm 1** Projection \(()\) of \(A\) onto \((P_{L},P,P_{R})\)

## 4 Orthogonal \((P_{L},P,P_{R})\) matrices

In this section, we study the orthogonality constraint for the \((P_{L},P,P_{R})\) to obtain structured orthogonal representation. This is one of the main contributions of our paper and we utilize this class in all the numerical experiments. Since we are interested only in square orthogonal matrices, we additionally assume that \(m=n\) and \(b_{L}^{1}=b_{L}^{2}=b_{L};\;b_{R}^{1}=b_{R}^{2}=b_{R}\). Similarly to parametrizations in OFT and BOT, a natural way to enforce orthogonality of \((P_{L},P,P_{R})\)-matrices is to enforce orthogonality of each block of \(L\) and \(R\). This indeed leads an orthogonal matrix since permutation matrices are also orthogonal as well as a product of orthogonal matrices. However, it is not immediately obvious that there exist no orthogonal matrices from \((P_{L},P,P_{R})\) that cannot be represented this way. Surprisingly, we find that such a way to enforce orthogonality is indeed sufficient for covering of all orthogonal matrices from \((P_{L},P,P_{R})\).

**Theorem 1**.: _Let \(A\) be any orthogonal matrix from \((P_{L},P,P_{R})\). Then, \(A\) admits \(P_{L}(LPR)P_{R}\) representation with the matrices \(L,R\) consisting of orthogonal blocks._

Proof.: Matrices \(P_{L},P_{R}\) are orthogonal as they are permutation matrices. It means that it is sufficient to prove theorem in the case when \(A\) is from \((I,P,I)\), which means that we can use low block

Figure 2: Illustration of Proposition 1 that provides block low-rank interpretation of \((I,P,I)\) matrices. The matrix \(R\) contains 2 blocks and matrix \(L\) contains 4 blocks.

rank structure interpretation from Proposition 1. Consider a skeleton decomposition of the blocks \(A_{ij}=U_{ij}V_{ij}^{}\), \(U_{ij}^{b_{L} r_{ij}}\), \(V_{ij}^{b_{R} r_{ij}}\) such that \(U_{ij}^{}U_{ij}=I_{r_{ij}}\) (this can be ensured, e.g., using the QR decomposition). Then

\[A=U_{1,1}V_{1,1}^{}&&U_{1,k_{L}}V_{1,k_{L}}^{}\\ &&\\ U_{k_{L},1}V_{k_{L},1}^{}&&U_{k_{L},k_{R}}V_{k_{L},k_{R}}^{} .\]

Take the \(j\)-th block-column of \(A\). Since \(A\) is an orthogonal matrix, we get:

\[V_{1,j}U_{1,j}^{}&&V_{k_{L},j}U_{k_{L},j}^{} U_{1,j}V_{1,j}^{}\\ \\ U_{k_{L},j}V_{k_{L},j}^{}=I_{b_{R}}\]

Multiplying matrices in the l.h.s. we get \(V_{1,j}U_{1,j}^{}U_{1,j}V_{1,j}^{}++V_{k_{L},j}U_{k_{L},j}^{} U_{k_{L},j}V_{k_{L},j}^{}=I_{b_{R}}\). Since \(U_{ij}^{}U_{ij}=I_{r_{ij}}\) we conclude \(V_{1,j}V_{1,j}^{}++V_{k_{L},j}V_{k_{L},j}^{}=I_{b_{R}}\). This implies that \((V_{1,j} V_{k_{L},j})\) is an orthogonal matrix. Note that if we now parameterize \(A=LPR\) with the matrices \(V_{ij}\) packed into \(R\) and \(U_{ij}\) packed into \(L\), then \((V_{1,j} V_{k_{L},j})\) is exactly the \(j\)-th block matrix in \(R\) up to permutation of rows. Therefore, every block in \(R\) is an orthogonal matrix. Since we now proved that \(V_{ij}^{}V_{ij}=I\), we can use same the derivation for the rows of \(A\) and conclude that blocks of \(L\) are also orthogonal. 

## 5 \((P_{m+1},,P_{1})\) matrices

In this section we describe an the extension of \(\)-matrices that uses more than two block-diagonal matrices and show that with the right permutations choices \(\)-matrices are more effective than block butterfly matrices in forming dense matrices. Here by dense matrices we imply matrices that do not contain zero entries at all.

**Definition 5.1**.: \(A\) _is said to be in \((P_{m+1},,P_{1})\) if_

\[A=P_{m+1}_{i=m}^{1}(B_{i}P_{i}),\]

_where each matrix \(B_{i}\) is a block-diagonal matrix with \(k_{i}\) blocks of size \(b_{i}^{1} b_{i}^{2}\), matrices \(P_{i}\) are permutation matrices and \(b_{i}^{1} k_{i}=b_{i+1}^{2} k_{i+1}\)._

**Remark 1**.: _Similarly to the case \(m=2\) described in Section 3, we may use orthogonal blocks in \(B_{i}\), \(i=1,,m+1\) to obtain orthogonal matrices. However, it is not clear if an analog to Theorem 1 is correct in this case as well._

**Remark 2**.: _For each of the classes of Block Butterfly matrices , Monarch matrices  and order-p Monarch matrices , there exist permutation matrices \(P_{m+1},,P_{1}\) such that \((P_{m+1}, P_{1})\) coincides with a respective class. Indeed, Monarch matrices have the form of alternating block-diagonal matrices and permutations with some specific size constraints and sparse matrices in the product of Block Butterfly matrices can be easily transformed to block-diagonal matrices with permutations of rows and columns._

### Choosing permutation matrices

We suggest using the following matrices with \(k=k_{i}\) for \(P_{i}\). Note that this is efficient for forming dense matrices as follows from the proof of Theorem 2. This is by contrast to the permutations used in  that are restricted to particular matrix sizes.

**Definition 5.2** ().: _Let \(P_{(k,n)}\) be a permutation matrix given by permutation \(\) on \(\{0,1,,n-1\}\):_

\[(i)=(ik)+.\]Applying this permutation to a vector can be viewed as reshaping an input of size \(n\) into an \(k\) matrix in a row-major order, transposing it, and then vectorizing the result back into a vector (again in row-major column). We provide several examples of such permutations in Figure 3.

### Comparison to block butterfly matrices and BOFT

Block Butterfly matrices were introduced in [Chen et al., 2022] and are used to construct orthogonal matrices in the BOFT method. Block Butterfly matrix class is a special case of higher-order \(\)-matrices with \(k_{i}=r\) and \(b_{i}^{1}=b_{i}^{2}=b=2s\) and certain permutation choices. However, we argue that the choice of these permutations are sub-optimal for construction of dense matrix and using permutations from Definition 5.2 is more effective. When using block-diagonal matrices with \(r\) blocks, block butterfly matrices need \(1+_{2}(r)\) matrices to construct a dense matrix. For \(\)-matrices we have the following result.

**Theorem 2**.: _Let \(k_{i}=r,b_{i}^{1}=b_{i}^{2}=b\). Then using \(m=1+_{b}(r)\) is sufficient for the class \((P_{L},P_{(r,br)},,P_{(r,br)},P_{R})\) to form a dense matrix for any \(P_{L},P_{R}\). Moreover, the choice of \(P_{2}==P_{m}=P_{(r,br)}\) is optimal in the sense that all matrices from \((P_{m+1},,P_{1})\) contain zero blocks for any integer \(m<1+_{b}(r)\) and any permutations \(P_{1},,P_{m+1}\)._

Proof.: See Appendix D. 

For example, let us consider a case of constructing a dense orthogonal matrix of the size \(1024 1024\). Suppose also that we use block matrices with block size \(32\). Constructing a dense matrix with Block Butterfly matrices requires \(1+_{2}(32)=6\) butterfly matrices, which leads to \(6 32^{3}\) parameters in the representation. \((P_{L},P,P_{R})\) matrices with \(P=P_{(32,1024)}\) only need two matrices to construct a dense matrix yielding \(2 32^{3}\) parameters. The \((P_{L},P,P_{R})\) parametrization is also naturally more computationally efficient as fewer number of multiplications is both faster and requires less cached memory for activations.

## 6 Applications

### Orthogonal fine-tuning with \((P_{L},P,P_{R})\) (Gsoft)

We utilize the pipeline of OFT and BOFT methods with the exception of parametrizing \(Q\) with orthogonal permuted \((P_{L},P,P_{R})\) matrices. In particular, for parametrization of \(Q^{d d}\), we utilize the \((P^{},P,I)\) class, i.e. \(Q=P^{}LPR\), where \(L=(L_{1}, L_{r})\), \(L_{i}^{b b}\), \(R=(R_{1},,R_{r})\), \(R_{i}^{b b}\). For consistency, we use the same notation for the number of blocks and block sizes as in BOFT and OFT methods. We use \(P_{(r,br)}\) as a permutation matrix \(P\). To enforce orthogonality, we parameterize each block in matrices \(L,R\) with the Cayley parametrization. We initialize \(Q\) as an identity matrix by initializing each block to be an identity matrix. Additional

Figure 3: Illustraion of \(P_{(k,12)}\) permutations for \(k\{3,4,6,2\}\).

techniques like magnitude scaling and multiplicative dropout that are used in OFT and BOFT can be utilized the same way in our method, though we only use scaling in our experiments. Note that likewise in OFT, BOFT weights of the matrix \(Q\) can be merged with the pretrained weight \(W\) producing no inference overhead.

### Two-sided orthogonal fine-tuning (Double GSOFT)

Consider SVD decomposition of a matrix \(W^{0}=U V^{}\). Applying orthogonal fine-tuning, we get \(W^{}=(QU) V^{}\), which is an SVD decomposition for the adapted weight \(W^{}\). This shows that we can only change left singular vectors \(U\) with the standard orthogonal fine-tuning paradigm. At the same time, the LoRA method modifies both matrices \(U\) and \(V\). Moreover, recent papers  show that initializing matrices \(A,B\) with singular vectors can additionally boost performance of LoRA. This motivates an extension of orthogonal fine-tuning method, that can adapt both matrices \(U\) and \(V\). We introduce a simple approach that multiplies pre-trained weight matrices from both sides, rather than one. This method modifies forward pass from z = \((W^{0})^{}x\) to

\[z=(Q_{U}W^{0}Q_{V})^{}x\]

Where \(Q_{U}\) and \(Q_{V}\) are parametrized as orthogonal \(\)-matrices. In cases where BOFT utilizes 5-6 matrices, we can leverage the fact that our method uses only 2 and adapt both sides while still using less matrices and trainable parameters than BOFT.

### \(\) Orthogonal Convolutions

Recall, that due to linearity of a multichannel convolution operation, we can express the convolution of tensor \(X^{c_{in} h w}\) with a kernel \(L^{c_{out} c_{in} k k}\)\(L X\) in terms of matrix multiplication :

\[Y=L X vec(Y)=L_{0,0}&&L_{0, c_{in}-1}\\ &&\\ L_{c_{out}-1,0}&&L_{c_{out}-1,c_{in}-1}vec(X),\] (2)

where \(L_{i,j}\) is doubly Toeplitz matrix, corresponding to convolution between \(i\)-th and \(j\)-th channels and \(vec(X)\) is a vectorization of tensor into a vector in a row-major order. Thus, the convolution is essentially a block matrix, where each block represents a standard convolution operation. Using this block interpretation (2), we may apply the concept of \(\) matrices to convolutional layers as well. Considering each convolution between channels as an element of our block matrix, we can set some of these blocks to zero, obtaining some additional structure. Thus, we can construct block matrix which has block-diagonal structure, corresponding to grouped convolution (further, in all equations we will denote it as GrConv). Then, defining ChShuffle as a permutation of channels, like in , we obtain structure, which is similar to GSOFT, defined in Section 6:

\[Y=_{2}(_{2}(_{1}(_{1}(X)))).\] (3)

The proposed \(\) convolutional layer shuffles information between each pair of input channels and requires less parameters and FLOPs during computations. In this example we can also choose permutations of channels and change kernel size. This convolutional layer can be treated as \((P_{m+1},,P_{1})\) matrix in vectorized view, that is why choosing permutations between convolutional layers is also very important for information transition properties. In Appendix F we explain the choice of ChShuffle operation.

We can use the proposed layer to construct orthogonal convolutions (transformations with an orthogonal Jacobian matrix) similarly to skew orthogonal convolution (SOC) architecture, that uses Taylor expansion of a matrix exponential. One major downside of methods such as SOC and BCOP  is that they require more time than basic convolution operation. For instance, in the SOC method, one layer requires multiple applications of convolution (\(6\) convolutions per layer). In our framework, we propose a parametrization of a convolutional layer, in which imposing an orthogonality to convolutions has fewer number of FLOPs and parameters thanks to the usage of grouped convolutions.

Let us discuss in more details how SOC works and the way we modify it. In SOC, a convolutional filter is parametrized in the following way:

\[L=M-(M),\]

where \(M^{c_{in} c_{out} r s}\) is an arbitrary kernel and the \(\) is the following operation:

\[(M)_{i,j,k,l}=M_{j,i,r-k-1,s-l-1}\]

This parametrization of filter \(L\) makes the matrix from Equation 2 skew-symmetric. As matrix exponential of skew-symmetric matrix is an orthogonal matrix, in SOC the authors define convolution exponential operation, which is equivalent to matrix exponential in matrix-vector notation:

**Definition 6.1**.: _[_Singla and Feizi_,_ 2021_]_ _Let \(X^{c h w}\) be an input tensor and \(L^{c c k k}\) be a convolution kernel. Then, define convolution exponential \(L_{e}X\) as follows:_

\[L_{e}X=X++X}{2!}+\]

_where \(L^{i}X\) is a convolution with kernel \(L\) applied \(i\) times consequently._

As mentioned above, with proper initialization we get a convolutional layer with orthogonal Jacobian matrix. Using the parametrization of convolution layer from the Equation 3 and substituting there two grouped convolution exponentials (e.g. in our parametrization we have the same convolution exponential, but we have grouped convolution instead of basic one) with the parameterized kernel:

\[Y=_{2}(_{2}(_{1}( _{1}(X))))\]

In our experiments we tried different layer architectures and we found that making kernel size of the second convolution equal to 1 speeds up our convolutional layer, maintaining quality metrics. Thus, if convolutional layer consists of two grouped convolutional exponentials, the second convolutional exponential has \(kernel\_size=1 1\)

## 7 Experiments

All the experiments below were conducted on NVIDIA V100-SXM2-32Gb GPU. We ran all the experiments within \(\)2000 GPU hours.

Source code is available at: https://github.com/Skonor/group_and_shuffle

### Natural language understanding

We report result on the GLUE [Wang et al., 2018] benchmark with RoBERTa-base [Liu et al., 2019] model. Benchmark includes several classification tasks that evaluate general language understanding. We follow training settings of [Liu et al., 2024b, Zhang et al., 2023]. We apply adapters for all linear layers and only tune learning rate for all methods. Table 1 reports best results on the evaluation set from the whole training. LoRA, OFT and SOFT are implemented with PEFT library [Mangrulkar et al., 2022]. GSOFT method outperforms OFT, BOFT and also have a slight edge over LoRA. Note that even though skew-symmetric \(K\) theoretically matrix only requires approximately half the parameters of a full matrix, in practice it is parametrized as \(K=A-A^{T}\) for the ease of computations. However, after fine-tuning, one can only save upper-triangular part of \(K\). Doing this, orthogonal fine-tuning methods become approximately 2 times more efficient in terms of memory savings.

### Subject-driven generation

Subject-driven generation [Ruiz et al., 2023, Gal et al., 2022] is an important and challenging task in the field of generative modelling. Given several photos of a particular concept, we want to introduce it to the diffusion model so that we can generate this particular object in different scenes described by textual prompts. The main way to do this is to fine-tune the model. However, the large number of fine-tuning parameters together with the lack of training images make the model prone to overfitting, i.e. the model reconstructs the concept almost perfectly, but starts to ignore the textual prompt during generation. To solve this problem and stabilize the fine-tuning process, different lightweight parameterizations [Qiu et al., 2023, Liu et al., 2024b, Hu et al., 2022, Tewel et al., 2023, Han et al.,2023] and regularization techniques [Ruiz et al., 2023, Kumari et al., 2023] are widely used in this task. Therefore, we chose this setting to evaluate the effectiveness of the proposed orthogonal parameterization compared to other approaches.

We use StableDiffusion [Rombach et al., 2022] and the Dreambooth [Ruiz et al., 2023] dataset for all our experiments. The following parameterizations were considered as baselines in this task: full (q, k, v and out.0 layers in all cross- and self- attentions of the UNet are trained), LoRA [Hu et al., 2022] and BGFT [Liu et al., 2024b] applied to the same layer. We use our GSOFT parameterization and a two-sided orthogonal GSOFT (Double GSOFT) applied to the same layers as baselines. For a more comprehensive comparison, we consider different hyperparameters for the models, adjusting the total number of optimized parameters. More training and evaluation details can be found in Appendix E.

CLIP image similarity, CLIP text similarity and visual comparison for this task are presented in Table 2 and Figure 4. As the results show, GSOFT and DoubleGSOFT are less prone to overfitting compared to the baselines. They show better alignment with text prompts while maintaining a high level of concept fidelity. Furthermore, both methods with optimal hyperparameters are more efficient than BOFT and comparable to LoRA and full parameterization in terms of training time. See Appendix E for more visual and quantitative comparison.

### \(\) Orthogonal Convolutions

Following [Singla and Feizi, 2021], we train LipConvnet-n on CIFAR-100 dataset. LipConvnet-n is 1-Lipschitz neural network, i.e. neural network with Lipschitz constant equal to 1, his property provides certified adversarial robustness. LipConvnet uses orthogonal convolutions and gradient preserving activations in order to maintain 1-Lipschitz property.

LipConvnet-n architecture consists of 5 equal blocks, each having \(\) skew orthogonal convolutions, where the last convolution at each level downsamples image size. We replace the skew orthogonal convolution layer with the structured version using \(\)orthogonal convolutions and test it in the setting of [Singla and Feizi, 2021], using the same hyperparameters (learning rate, batch size and scheduler stable during testing). In layers where we have two GrExpConv, the second convolution has kernel size equal to 1.

We also use a modified activation function (MaxMinPermuted instead of MaxMin), which uses different pairing of channels. This makes activations aligned with the ChShuffle operation and grouped convolutions. The choice of permutation for ChShuffle also slightly differs from permutations defind in Definition 5.2 because of the interplay between activations and convolutional layers. We provide definitions and intuition regarding activations and permutations for ChShuffle in Appendix F.

    &  &  &  &  &  \\   & & rank & & \(r\), \(m\) & & \(r\) & & & \(r\) & \\   & & 4 & 32 & 128 & 32, 4 & 32, 6 & 16, 5 & 32 & 16 & 8 & 64 & 32 & 16 \\  \# Params & 99.9M & 0.8M & 6.6M & 26.6M & 13.6M & 20.4M & 33.8M & 6.8M & 13.6M & 27.1M & 6.5M & 13.0M & 25.9M \\ Training time & 1.3 & 1.3 & 1.3 & 1.3 & 2.0 & 2.2 & 2.3 & 1.5 & 1.6 & 1.8 & 1.7 & 2.0 & 1.8 \\ CLIP-1\(\) & 0.805 & 0.805 & **0.819** & 0.813 & 0.803 & 0.796 & 0.789 & 0.805 & 0.803 & 0.783 & **0.815** & 0.802 & 0.783 \\ CLIP-T\(\) & 0.212 & 0.246 & 0.236 & 0.223 & 0.244 & 0.234 & 0.223 & **0.256** & 0.245 & 0.227 & **0.256** & 0.242 & 0.225 \\   

Table 2: Results on subject-driven generation. # Params denotes the number of training parameters in each parametrization. Training time is computed for 3000 iterations on a single GPU V100 in hours.

    &  & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & ALL \\  FT & 125M & 87.62 & 94.38 & 61.97 & **91.5** & **93.06** & 80.14 & 88.97 & **90.91** & 86.07 \\ LoRA\({}_{r=8}\) & 1.33M & **87.82** & **95.07** & 64.02 & 90.97 & 92.81 & **81.95** & 88.73 & 90.84 & 86.53 \\ OFT\({}_{b=16}\) & 1.41M & 87.21 & **95.07** & 64.37 & 90.6 & 92.48 & 79.78 & 89.95 & 90.71 & 86.27 \\ BGFT\({}_{b=8}^{m=2}\) & 1.42M & 87.14 & 94.38 & 62.57 & 90.48 & 92.39 & 80.14 & 88.97 & 90.67 & 85.84 \\  GSOFT\({}_{b=8}\) & 1.42M & 87.16 & **95.06** & **65.3** & 90.46 & 92.46 & **81.95** & **90.2** & 90.76 & **86.67** \\   

Table 1: Results on GLUE benchmark with RoBERTa-base model. We report Pearson correlation for STS-B, Matthew’s correlation for CoLA and accuracy for other tasks. # Params denotes number of trainable parameters

## 8 Concluding remarks

In this paper, we introduce a new class of structured matrices, called \(\)-matrices, build a structured orthogonal parametrization with them and use them in several domains within deep learning applications. However, we hope that our orthogonal parametrization can be adapted to different settings in future (including tasks outside of deep learning), as it makes orthogonal parametrizations less of a computational burden. \(\)-matrices without orthogonality constraints is another promising direction.

## 9 Limitations

Although our method for orthogonal fine-tuning is faster than BOFT, it is still slower than LoRA during training. Additionally, since our parametrization provides a trade-off between expressivity and parameter-efficiency, it might be unable to represent some particular orthogonal matrices, which might be required in other settings apart from parameter-efficient fine-tuning.

Figure 4: Subject-driven generation visual results on 3000 training iterations.

   Conv. Layer & \# Params & Groups & Speedup & Activation & Accuracy & Robust Accuracy \\  SOC & 24.1M & - & 1 & MaxMin & 43.15\% & 29.18\% \\ GS-SOC & **6.81M** & (4, -) & **1.64** & MaxMinPermuted & **43.48\%** & **29.26\%** \\ GS-SOC & **8.91M** & (4, 1) & 1.21 & MaxMinPermuted & **43.42\%** & **29.56\%** \\ GS-SOC & **7.86M** & (4, 2) & 1.22 & MaxMinPermuted & 42.86\% & 28.98\% \\ GS-SOC & **7.3M** & (4, 4) & 1.23 & MaxMinPermuted & 42.75\% & 28.7\% \\   

Table 3: Results of training LipConvnet-15 architecture on CIFAR-100. \((a,b)\) in “Groups” column denotes number of groups in two grouped exponential convolutions (with kernel sizes \(3\) and \(1\)). \((a,-)\) corresponds to only one \(\) orthogonal convolutional layer. Before each grouped layer with \(k\) groups use a ChShuffle operator.

Acknowledgments

The article was prepared within the framework of the HSE University Basic Research Program. This research was supported in part through computational resources of HPC facilities at HSE University (Kostenetskiy et al., 2021).