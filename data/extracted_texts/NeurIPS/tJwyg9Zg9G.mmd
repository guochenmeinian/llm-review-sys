# Parallel-mentoring for Offline Model-based Optimization

Can (Sam) Chen1,2,*, Christopher Beckham2,3, Zixuan Liu5, Xue Liu1,2, Christopher Pal2,3,4

Correspondence to can.chen@mila.quebec.

###### Abstract

We study offline model-based optimization to maximize a black-box objective function with a static dataset of designs and scores. These designs encompass a variety of domains, including materials, robots and DNA sequences. A common approach trains a proxy on the static dataset to approximate the black-box objective function and performs gradient ascent to obtain new designs. However, this often results in poor designs due to the proxy inaccuracies for out-of-distribution designs. Recent studies indicate that: (a) gradient ascent with a mean ensemble of proxies generally outperforms simple gradient ascent, and (b) a trained proxy provides weak ranking supervision signals for design selection. Motivated by (a) and (b), we propose _parallel-mentoring_ as an effective and novel method that facilitates mentoring among parallel proxies, creating a more robust ensemble to mitigate the out-of-distribution issue. We focus on the three-proxy case and our method consists of two modules. The first module, _voting-based pairwise supervision_, operates on three parallel proxies and captures their ranking supervision signals as pairwise comparison labels. These labels are combined through majority voting to generate consensus labels, which incorporate ranking supervision signals from all proxies and enable mutual mentoring. However, label noise arises due to possible incorrect consensus. To alleviate this, we introduce an _adaptive soft-labeling_ module with soft-labels initialized as consensus labels. Based on bi-level optimization, this module fine-tunes proxies in the inner level and learns more accurate labels in the outer level to adaptively mentor proxies, resulting in a more robust ensemble. Experiments validate the effectiveness of our method. Our code is available here.

## 1 Introduction

Designing new objects or entities to optimize specific properties is a widespread challenge, encompassing various domains such as materials, robots, and DNA sequences . Traditional approaches often involve interacting with a black-box function to propose new designs, but this can be expensive or even dangerous in some cases . In response, recent work  has focused on a more realistic setting known as offline model-based optimization (MBO). In this setting, the objective is to maximize a black-box function using only a static (offline) dataset of designs and scores.

A prevalent approach to addressing the problem is to train a deep neural network (DNN) model parameterized as \(f_{}()\), on the static dataset, with the trained DNN serving as a proxy. The proxy allows for gradient ascent on existing designs, generating improved designs by leveraging the gradient information provided by the DNN model. However, this method encounters an issue with the trained proxy being susceptible to out-of-distribution problems. Specifically, the proxy produces inaccurate predictions when applied to data points that deviate significantly from the training distribution.

Recent studies have observed that (a) employing a mean ensemble of trained proxies for gradient ascent in offline MBO generally leads to superior designs compared to using a single proxy . This improvement stems from the ability of the ensemble to provide more robust predictions compared to a single proxy [8; 9; 10; 11]. Recent work has also found that (b) a trained proxy offers weak (valuable, albeit potentially unreliable) ranking supervision signals for design selection in various offline MBO contexts, such as evolutionary algorithms , reinforcement learning , and generative modeling . These signals, focusing on the relative order of designs over absolute scores, are more resilient to noise and inaccuracies. By exchanging these signals among proxies in the ensemble, we can potentially enhance its robustness. As shown in Figure 1, we have three parallel proxies \(f^{A}_{}()\), \(f^{B}_{}()\) and \(f^{C}_{}()\). For two designs \(^{n}_{1}\) and \(^{n}_{2}\) within the neighborhood of the current optimization point, proxies \(f^{A}_{}()\) and \(f^{B}_{}()\) agree that the score of \(^{n}_{1}\) is larger than that of \(^{n}_{2}\), while proxy \(f^{C}_{}()\) disagrees. Based on the majority voting principle, proxies \(f^{A}_{}()\) and \(f^{B}_{}()\) provide a more reliable ranking, and their voted ranking signal \(f^{V}(^{n}_{1})>f^{V}(^{n}_{2})\) could mentor the proxy \(f^{C}_{}()\), thus enhancing its performance.

To this end, we propose an effective and novel method called _parallel-mentoring_ that facilitates mentoring among parallel proxies to train a more robust ensemble against the out-of-distribution issue. This paper primarily focuses on the three-proxy case, referred to as _tri-mentoring_, but we also examine the situation with more proxies in Appendix A.1. As depicted in Figure 2, _tri-mentoring_ consists of two modules. **Module 1**, _voting-based pairwise supervision_ (shown in Figure 2(a)), operates on three parallel proxies \(f^{A}_{}()\), \(f^{B}_{}()\), and \(f^{C}_{}()\) and utilizes their mean for the final prediction. To ensure consistency with the ranking information employed in design selection, this module adopts a pairwise approach to represent the ranking signals of each proxy. Specifically, as illustrated in Figure 2(a), this module generates samples (e.g. \(^{n}_{1}\), \(^{n}_{2}\) and \(^{n}_{3}\)) in the neighborhood of the current point \(\) and computes pairwise comparison labels \(}^{A}\) for all sample pairs, serving as ranking supervision signals for the proxy \(f^{A}_{}()\). The label \(}^{A}_{ij}\) is defined as \(1\) if \(f^{A}_{}(_{i})>f^{A}_{}(_{j})\) and \(0\) otherwise, and similar signals are derived for proxies \(f^{B}_{}()\) and \(f^{C}_{}()\). These labels \(}^{A}\), \(}^{B}\) and \(}^{C}\) are combined via majority voting to create consensus labels \(}^{V}\) which are more reliable and thus can be used for mentoring proxies. The voted ranking signal \(f^{V}(^{n}_{1})>f^{V}(^{n}_{2})\) in Figure 1 corresponds to the pairwise consensus label \(^{V}_{12}=1\) in Figure 2(a), and both can mentor the proxy \(f^{C}_{}()\).

**Module 2**, _adaptive soft-labeling_ (shown in Figure 2(b)), mitigates the issue of label noise that may arise, since the voting consensus may not always be correct. To this end, this module initializes the consensus labels \(}^{V}\) from the first module as soft-labels \(}^{S}\). It then aims to learn more accurate soft-labels to better represent the ranking supervision signals by leveraging the knowledge from the static dataset. Specifically, assuming accurate soft-labels, one of the proxies, either \(f^{A}_{}()\), \(f^{B}_{}()\) or \(f^{C}_{}()\), fine-tuned using them, is expected to perform well on the static dataset, as both soft-labels

Figure 1: Motivation illustration.

Figure 2: Illustration of _tri-mentoring_.

(pairwise perspective) and the static dataset (pointwise perspective) describe the same ground-truth and share underlying similarities. This formulation leads to a bi-level optimization framework with an inner _fine-tuning_ level and an outer _soft-labeling_ level as shown in Figure 2(b). The inner level fine-tunes the proxy with soft-labels, which establishes the connection between them. The outer level optimizes soft-labels to be more accurate by minimizing the loss of the static dataset via the inner-level connection. The optimized labels are further fed back to the first module to adaptively mentor the proxy, ultimately yielding a more robust ensemble. Experiments on design-bench validate the effectiveness of our method.

To summarize, our contributions are three-fold:

* We propose _parallel-mentoring_ for offline MBO, effectively utilizing weak ranking supervision signals among proxies, with a particular focus on the three-proxy case as _tri-mentoring_.
* Our method consists of two modules: _voting-based pairwise supervision_ and _adaptive soft-labeling_. The first module generates pairwise consensus labels via majority voting to mentor the proxies.
* To mitigate label noise in consensus labels, the second module proposes a bi-level optimization framework to adaptively fine-tune proxies and soft-labels, resulting in a more robust ensemble.

## 2 Preliminaries: Gradient Ascent on Offline Model-based Optimization

Offline model-based optimization (MBO) aims to find the optimal design \(^{*}\) that maximizes the black-box objective function \(f()\):

\[^{*}=_{}f()\,,\] (1)

To achieve this, a static dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\) with \(N\) points is available, where \(_{i}\) represents a design and \(y_{i}\) is its corresponding score.

A common approach for solving this optimization problem is to fit a deep neural network (DNN) model \(f_{}()\) with parameters \(\) to the static dataset in a supervised manner. The optimal parameters \(^{*}\) can be obtained by minimizing the mean squared error between the predictions and the true scores:

\[^{*}=_{}_{i=1}^{N}(f_{}(_{i})-y_{i})^{2}\,.\] (2)

The trained DNN model \(f_{^{*}}()\) acts as a proxy to optimize the design using gradient ascent steps:

\[_{t+1}=_{t}+_{}f_{}()|_{= _{t}}\,,\ t[0,T-1]\,,\] (3)

where \(T\) is the number of steps and \(\) represents the learning rate. \(_{T}\) serves as the final design candidate. However, this method faces a challenge with the proxy being vulnerable to out-of-distribution designs. When handling designs that substantially differ from the training distribution, the proxy yields inaccurate predictions.

## 3 Method

In this section, we introduce _parallel-mentoring_, focusing on the three-proxy scenario, also known as _tri-mentoring_. The method can be easily extended to incorporate more proxies, as discussed in Appendix A.1. _Tri-mentoring_ consists of two modules. The first module, _voting-based pairwise supervision_ in Section 3.1, manages three proxies parallelly and generates consensus labels via majority voting to mentor proxies. To mitigate label noise, we introduce a second module, _adaptive soft-labeling_ in Section 3.2. This module adaptively fine-tunes proxies and soft-labels using bi-level optimization, improving ensemble robustness. The overall algorithm is shown in Algorithm 1.

### Voting-based Pairwise Supervision

We train three parallel proxies \(f_{}^{A}()\), \(f_{}^{B}()\) and \(f_{}^{C}()\) on the static dataset with different initializations and utilize their mean as the final prediction as suggested in [1; 15]:

\[f_{}()=(f_{}^{A}()+f_{} ^{B}()+f_{}^{C}()).\] (4)

We then apply gradient ascent with \(f_{}()\) on existing designs to generate improved designs as per Eq.(3). Although the mean ensemble generally results in superior designs compared to a single proxy 

[MISSING_PAGE_FAIL:4]

### Adaptive Soft-labeling

The consensus labels \(}^{V}\) may contain noise since the majority voting consensus can be incorrect. To mitigate this issue, we propose an _adaptive soft-labeling_ module. This module initializes soft-labels as consensus labels, and employs a bi-level optimization framework for adaptive mentoring, where the inner level fine-tunes the proxies and the outer level refines the soft-labels. We provide a detailed description of this module below; its implementation is outlined in Algorithm 1, Line 7- Line 11.

**Fine-tuning.** We initialize the soft-labels as the consensus labels: \(}^{S}=}^{V}\), serving as an effective starting point. Utilizing the soft-labels \(}^{S}\), we can perform fine-tuning on the proxy \(f^{A}_{}()\) against the binary cross-entropy loss following Eq.(6),

\[^{A}(,^{S})=-^{K}}_{1 i<j  K}^{S}_{ij}[(f^{A}_{}(_{i})-f^{A}_{ {}}(_{j}))]+(1-^{S}_{ij})[(f^{A}_{}( _{j})-f^{A}_{}(_{i}))]\,.\] (7)

In contrast to Eq.(6), we express the loss \(^{A}(,}^{S})\) as a function of both the proxy parameters \(\) and the soft-labels \(}^{S}\) as the accurate soft-labels \(}^{S}\) have yet to be determined. One-step gradient descent enables fine-tuning, resulting in the following relationship:

\[(}^{S})=-^{A }(,}^{S})}{^{}},\] (8)

where \(\) denotes the fine-tuning learning rate.

**Soft-labeling.** Assuming the soft-labels are accurate, the fine-tuned proxy \(f^{A}_{(}^{S})}()\) is expected to perform well on the static dataset. This is due to the fact that, despite having different data distributions, the soft-labels and the static dataset share underlying similarities, as they both represent the same ground-truth from pairwise and pointwise perspectives, respectively. This leads to a bi-level optimization framework with the fine-tuning mentioned above as the inner level and the soft-labeling here as the outer level. In particular, we enhance the accuracy of soft-labels \(}^{S}\) by minimizing the mean squared error on the static dataset \(=\{(_{i},y_{i})\}_{i=1}^{N}\),

\[}^{S^{}}=}^{S}-^{N}(f^{A}_{(}^{S})}(_{i})-y_{i})^{2}}{ (}^{S})^{}}\,,\] (9)

where \(\) represents the soft-labeling learning rate. The nested optimization problem can be easily solved by _higher_, a library for higher-order optimization . Once optimized, the labels are fed back to the first module, adaptively mentoring the proxy \(f^{A}_{}()\) according to Eq.(8). The same procedure can be employed for proxies \(f^{B}_{}()\) and \(f^{C}_{}()\), ultimately resulting in a more robust ensemble. We further clarify the novelty of this module in Appendix A.2.

## 4 Experiments

We conduct extensive experiments on design-bench to investigate the effectiveness and robustness of the proposed method. In Section 4.4, we benchmark our approach against several well-established baselines. In Section 4.5, we verify the effectiveness of two modules: _voting-based pairwise supervision_ and _adaptive soft-labeling_, as well as other contributing factors. In Section 4.6, we investigate the sensitivity of our method to hyperparameter changes. While our primary focus is the _tri-mentoring_ with \(3\) proxies, we additionally explore other _parallel-mentoring_ implementations utilizing \(5\), \(7\), \(9\), and \(11\) proxies in Appendix A.1.

### Task Overview

We adopt the design-bench which comprises both continuous and discrete tasks. Below, we outline the dataset details and evaluation protocols.

**Dataset.** We conduct experiments on four continuous tasks: **(1)** Superconductor (SuperC) : discover an \(86\)-D superconductor to maximize critical temperature with \(17010\) designs. **(2)** Ant Morphology (Ant) : identify a \(60\)-D ant morphology to crawl quickly with \(10004\) designs. **(3)** D'Kitty Morphology (D'Kitty) : determine a \(56\)-D D'Kitty morphology to crawl quickly with \(10004\) designs. **(4)** Hopper Controller (Hopper) : find a neural network policy with \(5126\) weights to maximize return with \(3200\) designs. Besides, we perform experiments on four discrete tasks: **(1)** TF Bind 8 (TFB8) : design a length \(8\) DNA sequence to maximize binding activity score with 32896 designs. **(2)** TF Bind 10 (TFB10) : find a length \(10\) DNA sequence to maximize bindingactivity score with \(50000\) designs. **(3)** NAS : find a \(64\)-D NN with \(5\) categories per dimension to maximize the performance on CIFAR10 with \(1771\) designs.

**Evaluation.** We use the oracle evaluation of design-bench to evaluate a certain design and the details of the oracle functions are reported in _Design-Bench Benchmark Tasks_ in . Following , we select the top N = \(128\) candidates for each method and report the \(100^{th}\) percentile (maximum) normalized ground truth score. The score, denoted as \(y_{n}\) is computed as \(}{y_{max}-y_{min}}\) where \(y\) is the design score, and \(y_{min}\) and \(y_{max}\) are the lowest and highest scores in the complete unobserved dataset, respectively. In addition, we provide the \(50^{th}\) percentile (median) normalized ground truth scores used in the prior work in Appendix A.3. We also provide the mean and median ranks of all comparison methods to better assess the overall performance.

### Comparisons with Other Methods

In this paper, we benchmark our method against both gradient-based and non-gradient-based approaches. The gradient-based methods include: **(1)** Grad: optimizes the design against the learned proxy via simple gradient ascent; **(2)** DE (Deep Ensemble): optimizes the design against the mean ensemble of three proxies via gradient ascent; **(3)** GB (Gradient Boosting) : sequentially trains new proxies to obtain a robust proxy, followed by gradient ascent using the proxy; **(4)** COMs : lower bounds the proxy's prediction on out-of-distribution designs and subsequently carries out gradient ascent; **(5)** ROMA : incorporates the smoothness prior into the proxy and optimizes the design against the proxy; **(6)** NEMO : leverages normalized maximum likelihood to constrain the distance between the proxy and the ground-truth, and acquires new designs by gradient ascent; **(7)** BDI : proposes to distill the information from the static dataset into the high-scoring design; **(8)** IOM : enforces the invariance between the representations of the static dataset and generated designs to achieve a natural trade-off. Since our _tri-mentoring_ adopts three proxies, methods using one proxy including COMs, ROMA and IOM are equipped with three proxies for a fair comparison. We also explore combining our method with ROMA and COMs and please refer to Appendix A.4 for an in-depth discussion and corresponding empirical results.

The non-gradient-based methods include: **(1)** BO-qEI : fits a Gaussian Process, proposes candidate designs utilizing the quasi-Expected Improvement acquisition function, and assigns labels to the candidates with the proxy; **(2)** CMA-ES : labels the sampled designs and gradually adapts the covariance matrix distribution towards the high-scoring part among the sampled designs; **(3)** REINFORCE : parameterizes a design distribution and optimizes the distribution towards the optimal design by policy gradient; **(4)** CbAS : trains a VAE model and progressively adapts the model to focus on the high-scoring designs; **(5)** Auto.CbAS : retrains the proxy adopted in CbAS by leveraging importance sampling; **(6)** MIN : learns an inverse map from a score to a design and then samples from the map conditioned an optimal score value.

### Training Details

We follow the settings in [7; 25] if not specified. We adopt a three-layer MLP network with the ReLU function as the activation. We train the MLP model on the static dataset with a \(1 10^{-3}\) learning rate and an Adam optimizer. The fine-tuning learning rate \(\) is set as \(1 10^{-3}\) and the soft-labeling learning rate \(\) is set as \(1 10^{-1}\). The standard deviation \(\) is set as \(1 10^{-1}\) and the number of the samples \(K\) is set as \(10\). All experiments are performed on a single V100 GPU. To ensure the robustness of our results, we perform \(16\) trials for each setting and report the mean and standard error. We've detailed the training time and computational overhead of our approach in Appendix A.5 to provide a comprehensive view of its practicality.

### Results and Analysis

In Table 1 and Table 2, we present the results of our experiments for continuous and discrete tasks, respectively. A delineating line is drawn to separate the gradient-based methods from the non-gradient-based methods. Results for non-gradient-ascent methods are taken from . The highest score of the static dataset for each task is denoted by \(()\). For each task, we highlight the algorithms that fall within one standard deviation of the highest performance by bolding their results.

**Continuous tasks.** (1) Table 1 demonstrates _tri-mentoring_ attains the best results across the board, highlighting its effectiveness. Its consistent gains over Grad confirm its ability to tackle the out-of-distribution issue. We further test our tri-mentoring for out-of-distribution issues as detailed in Appendix A.6. (2) Compared to DE and GB, which also use multiple proxies, _tri-mentoring_ achievesbetter results in all four tasks, indicating its improved robustness by sharing ranking supervision signals among proxies. (3) DE typically outperforms simple gradient ascent due to ensemble prediction robustness, consistent with findings in . (4) Other gradient-based methods, such as COMs, fail to achieve the performance as _tri-mentoring_, further highlighting its superior effectiveness. (5) In low-dimensional TF Bind 8 tasks, gradient-based methods (average rank \(8.8\)) underperform compared to non-gradient-based methods (average rank \(6.8\)); however, in high-dimensional TF Bind 10 tasks, gradient-based methods (average rank \(7.7\)) surpass non-gradient-based methods (average rank \(8.2\)). This suggests non-gradient-based methods, like REINFORCE and generative modeling, are more suited for low-dimensional design due to their global search ability, while gradient-based methods provide more direct guidance in high-dimensional designs.

**Discrete tasks.** (1) Table 2 shows that _tri-mentoring_ achieves the best results in two out of the three tasks, with a marginal difference in the third. This indicates that _tri-mentoring_ is a potent method for discrete tasks as well. (2) However, in complex tasks such as NAS, where each design is represented as a \(64\)-length sequence of \(5\)-category one-hot vectors, the performance of _tri-mentoring_ is slightly compromised. This could be attributed to the encoding in design-bench not fully accounting for the sequential and hierarchical nature of network architectures, leading to less effective gradient updates. Our proposed method, also demonstrates its effectiveness on high-dimensional biological sequence design, achieving maximum normalized scores of \(0.865\) and \(0.699\) on GFP and UTR respectively.

**In summary, _tri-mentoring_ achieves the highest ranking as shown in Table 2 and Figure 3, and delivers the best performance in six out of the seven tasks.

  Method & Superconductor & Ant Morphology & D’Kity Morphology & Hopper Controller \\  \(()\) & \(0.399\) & \(0.565\) & \(0.884\) & \(1.000\) \\ BO-qEI & \(0.402 0.034\) & \(0.819 0.000\) & \(0.896 0.000\) & \(0.550 0.018\) \\ CMA-ES & \(0.465 0.024\) & **1.214 \(\) 0.732** & \(0.724 0.001\) & \(0.604 0.215\) \\ REINFORCE & \(0.481 0.013\) & \(0.266 0.032\) & \(0.562 0.196\) & \(-0.020 0.067\) \\ CbAS & \(\) & \(0.876 0.031\) & \(0.892 0.008\) & \(0.141 0.012\) \\ Auto.CbAS & \(0.421 0.045\) & \(0.882 0.045\) & \(0.906 0.006\) & \(0.137 0.005\) \\ MIN & \(0.499 0.017\) & \(0.445 0.080\) & \(0.892 0.011\) & \(0.424 0.166\) \\  Grad & \(0.495 0.011\) & \(0.934 0.011\) & \(0.944 0.017\) & \(1.797 0.116\) \\ DE & \(\) & \(0.937 0.016\) & **0.956 \(\)** & \(1.805 0.105\) \\ GB & \(0.496 0.012\) & \(0.926 0.029\) & \(0.948 0.012\) & **1.793 \(\)** \\ COMs & \(0.491 0.028\) & \(0.856 0.040\) & \(0.938 0.015\) & \(0.642 0.167\) \\ ROMA & \(0.508 0.014\) & \(0.914 0.029\) & \(0.930 0.012\) & **1.728 \(\)** \\ NEMO & \(0.502 0.002\) & \(\) & \(0.954 0.007\) & \(0.481 0.003\) \\ IOM & \(\) & \(0.926 0.030\) & \(0.943 0.012\) & \(1.015 0.380\) \\ BDI & \(0.513 0.000\) & \(0.906 0.000\) & \(0.919 0.000\) & **1.993 \(\)** \\  _Tri-mentoring_ & \(\) & \(\) & \(\) & **1.983 \(\)** \\  

Table 1: Results (maximum normalized score) on continuous tasks.

  Method & TF Bind 8 & TF Bind 10 & NAS & Rank Mean & Rank Median \\  \(()\) & \(0.439\) & \(0.467\) & \(0.436\) & & \\ BO-qEI & \(0.798 0.083\) & \(0.652 0.038\) & \(\) & \(10.1/15\) & \(11/15\) \\ CMA-ES & \(\) & \(0.670 0.023\) & \(0.985 0.079\) & \(6.4/15\) & \(4/15\) \\ REINFORCE & \(\) & \(0.663 0.034\) & \(-1.895 0.000\) & \(11.4/15\) & \(15/15\) \\ CbAS & \(\) & \(0.651 0.060\) & \(0.683 0.079\) & \(9.1/15\) & \(9/15\) \\ Auto.CbAS & \(0.910 0.044\) & \(0.630 0.045\) & \(0.506 0.074\) & \(11.6/15\) & \(11/15\) \\ MIN & \(0.905 0.052\) & \(0.616 0.021\) & \(0.717 0.046\) & \(11.0/15\) & \(12/15\) \\  Grad & \(0.886 0.035\) & \(0.647 0.021\) & \(0.624 0.102\) & \(7.9/15\) & \(9/15\) \\ DE & \(0.900 0.056\) & \(0.659 0.033\) & \(0.655 0.059\) & \(5.3/15\) & \(4/15\) \\ GB & \(\) & \(0.630 0.041\) & \(0.716 0.088\) & \(7.6/15\) & \(6/15\) \\ COMs & \(0.496 0.065\) & \(0.622 0.003\) & \(0.783 0.029\) & \(10.0/15\) & \(11/15\) \\ ROMA & \(0.917 0.039\) & \(0.672 0.035\) & \(0.927 0.071\) & \(5.7/15\) & \(6/15\) \\ NEMO & \(0.943 0.005\) & \(\) & \(0.737 0.010\) & \(5.0/15\) & \(4/15\) \\ IOM & \(0.861 0.079\) & \(0.647 0.027\) & \(0.559 0.081\) & \(7.9/15\) & \(7/15\) \\ BDI & \(0.870 0.000\) & \(0.605 0.000\) & \(0.722 0.000\) & \(8.1/15\) & \(9/15\) \\  _Tri-mentoring_ & \(\) & \(\) & \(0.759 0.102\) & **2.1/15** & **2/15** \\  

Table 2: Results (maximum normalized score) on discrete tasks & ranking on all tasks.

### Ablation Studies

In this subsection, the proposed method serves as the baseline, and we systematically remove each module including _voting-based pairwise supervision_ and _adaptive soft-labeling_ to verify its contribution. The results are presented in Table 3. Besides the performance results here, we also provide an evaluation of the accuracy of generated pairwise labels to further verify the effectiveness of the two modules in Appendix A.7.

**Voting-based pairwise supervision.** Instead of using the proposed module, we compute the mean prediction of the ensemble and use this prediction to create pairwise consensus labels. We denote this as w/o _voting-based ps_. Our results in Table 3 show a decline in performance when adopting this alternative. A plausible explanation for this performance decline is that the alternative module fails to effectively exploit weak ranking supervision signals from individual proxies, resulting in reduced information exchange and collaboration among the proxies compared to the proposed module.

**Adaptive soft-labeling.** We remove this module and resort to using one-hot consensus labels. The performance across all tasks generally deteriorates compared to the full _tri-mentoring_. A possible explanation for this is that this module ensures that fine-tuned proxies are optimized to perform well on the static dataset by introducing soft-labels, reducing the risk of overfitting to consensus labels derived from individual proxy predictions. This demonstrates the significance of the _adaptive soft-labeling_ module in mitigating the effects of label noise and enhancing the ensemble performance.

Furthermore, we assess the impact of neighborhood sampling in our method and a post selection strategy related to our method, with results outlined in Table 3.

**Neighborhood sampling.** Typically, we sample \(K\) neighborhood points at the optimization point \(_{t}\) for pairwise labels. When neighborhood sampling is excluded (_w/o neighbor_), labels are directly generated near the static dataset. The performance generally deteriorates for _w/o neighbor_ in Table 3, due to the lack of local ranking information around the optimization point.

**Post selection.** We investigate a variant, _post selection_, where the mean of two proxies is used to select the third proxy's candidates. From Table 3, we find that this variant generally yields worse results compared to the full tri-mentoring. This finding suggests that using the ranking supervision signals directly for design selection is less effective than allowing proxies to exchange ranking supervision signals within the ensemble to produce a more robust ensemble.

### Hyperparameter Sensitivity

In this section, we study the sensitivity of our method to hyperparameters, specifically the number of neighborhood samples \(K\) and the number of optimization steps \(T\), on two tasks, the continuous Ant and the discrete TFB8. We also discuss the standard deviation hyperparameter \(\) in Appendix A.8.

**Number of neighborhood samples (\(K\)).** We evaluate the performance of our method for different values of \(K\), i.e., the number of neighborhood samples around the optimization point. We test \(K\) values of \(5\), \(10\), \(15\), \(20\), and \(25\), with \(K=10\) being the default value used in this paper. The results are normalized by dividing them by the result obtained for \(K=10\). As shown in Figure 4, the performance of the tri-mentoring method is quite robust to changes in \(K\) for both tasks.

**Number of optimization steps (\(T\)).** We also investigate the impact of the number of optimization steps on the performance of our method. As indicated in Figure 5, the method is robust to changes in the number of optimization steps for both Ant and TFB8 tasks.

In summary, our sensitivity analysis demonstrates that the _tri-mentoring_ method is robust to variations in key hyperparameters, ensuring stable performance across a range of settings.

  Task & _tri-mentoring_ & w/o _voting-based ps_ & w/o _ad soft-labeling_ & w/o _neighbor_ & _post selection_ \\  SuperC & \(0.514 0.018\) & \(0.505 0.014\) & \(0.504 0.014\) & \(\) & \(0.512 0.011\) \\ Ant & \(\) & \(0.938 0.021\) & \(0.945 0.018\) & \(0.945 0.012\) & \(0.945 0.009\) \\ D’Kitty & \(0.966 0.010\) & \(0.956 0.010\) & \(0.947 0.008\) & \(0.958 0.008\) & \(\) \\ Hopper & \(\) & \(1.902 0.138\) & \(1.916 0.108\) & \(1.839 0.112\) & \(1.901 0.148\) \\  TF Bind 8 & \(0.970 0.001\) & \(\) & \(0.944 0.026\) & \(0.950 0.018\) & \(0.949 0.006\) \\ TF Bind 10 & \(\) & \(0.694 0.030\) & \(0.710 0.025\) & \(0.643 0.009\) & \(0.635 0.027\) \\ NAS & \(\) & \(0.509 0.074\) & \(0.538 0.082\) & \(0.666 0.089\) & \(0.519 0.076\) \\  

Table 3: Ablation studies on _tri-mentoring_.

## 5 Related Work

**Offline model-based optimization.** Recently two broad groups of methods have emerged for offline model-based optimization. One group is based on generative modeling, where methods aim to gradually adapt a generative model towards the high-scoring design [31; 32; 33]. Another group is based on using gradient ascent to optimize existing designs via gradient information. These methods generally try to incorporate prior information into the proxy before using it for gradient ascent. Examples of this approach include COMs , ROMA , NEMO , BDI [25; 34] and IOM . Our proposed method called _parallel-mentoring_ (_tri-mentoring_) belongs to this category. We maintain an ensemble of proxies and aim to incorporate weak ranking signals from any pair of proxies into the third. This symmetric learning process, which cycles through all proxies, enhances the robustness and resilience of our ensemble. Our proposed ensemble training process, with its focus on parallel-mentoring, has the potential to improve the proxy and reward training [35; 33], thereby contributing to advancements in biological sequence design. Notably, the contemporaneous ICT method  exchanges direct proxy scores, which may be less robust than our pairwise comparison approach.

**Tri-training.** Our work is related to tri-training  which trains three classifiers and refines them using unlabeled samples. In each round, an unlabeled sample is labeled for one classifier if the other two agree on the label, under specific conditions. While _tri-mentoring_ is inspired by tri-training, there are fundamental differences between them: (1) Tri-training aims to enhance classification tasks by mitigating label noise, while _tri-mentoring_ focuses on producing a more robust regression ensemble; (2) Tri-training leverages unlabeled data, while _tri-mentoring_ operates on samples near the current optimization point; (3) _Tri-mentoring_ incorporates an _adaptive soft-labeling_ module to mitigate label noise, which is not present in tri-training. In addition to tri-training and _tri-mentoring_, there are other research works  involving multiple proxies cooperating to improve learning. Unlike tri-training and _tri-mentoring_ with three proxies, these methods focus on two proxies.

**Bi-level optimization for hyperparameter optimization.** Bi-level optimization has become increasingly popular for hyperparameter optimization [41; 42; 43; 44; 45; 46] due to the hierarchical problem structure . In the inner level, the relationship between model parameters and hyperparameters is established by minimizing the training loss. Meanwhile, the outer level optimizes hyperparameters through the connection built at the inner level by minimizing the validation loss. A specific category of hyperparameters is soft-label [48; 49], which is updated under the guidance of noise-free data to reduce noise. In this paper, we propose _adaptive soft-labeling_ to reduce noise in the consensus labels.

**Ensemble learning.** Ensemble learning techniques train multiple base learners and aggregate their predictions to achieve better performance and generalization than individual base learners alone [8; 9; 50; 10; 11; 51; 52]. These methods can be broadly classified into boosting [53; 54], bagging , and stacking . In contrast to our proposed _tri-mentoring_, where multiple proxies collaborate to enhance the learning process, ensemble learning mainly involves interaction during the aggregation phase, without influencing each other's training.

**Pairwise learning to rank.** Learning to rank [16; 57] has been extensively employed by commercial search engines for ranking search results [58; 59]. Unlike pointwise methods that score inputs, pairwise methods focus on relative order, aligning more with ranking concepts . Recent research  applies pairwise binary cross-entropy loss for training reward models in reinforcement learning, a technique used in ChatGPT . Our work expresses a proxy's ranking ability through pairwise comparison labels, generating consensus labels via majority voting to enable mutual mentorConclusion

In this work, we introduce _parallel-mentoring_ to enhance ensemble robustness against out-of-distribution issues through mutual mentoring among proxies. Focusing on a three-proxy case, we instantiate this as _tri-mentoring_, with two modules: _voting-based pairwise supervision_ for generating consensus labels, and _adaptive soft-labeling_ which mitigates label noise through bi-level optimization. Experimental results validate our approach's effectiveness. We discuss potential negative impacts in Appendix A.9 and limitations in Appendix A.10.

## 7 Acknowledgement

We thank CIFAR for support under the AI Chairs program. This research was empowered in part by the computational support provided by Compute Canada (www.computecanada.ca).