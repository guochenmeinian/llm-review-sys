# Panacea: Pareto Alignment via Preference Adaptation

for LLMs

 Yifan Zhong\({}^{1,2}\), Chengdong Ma\({}^{1,2*}\), Xiaoyuan Zhang\({}^{3*}\), Ziran Yang\({}^{4}\), Haojun Chen\({}^{1,2}\)

Qingfu Zhang\({}^{3}\), Siyuan Qi\({}^{2}\), Yaodong Yang\({}^{1}\)

Equal contribution. \({}^{1}\)Institute for Artificial Intelligence, Peking University. \({}^{2}\)State Key Laboratory of General Artificial Intelligence. \({}^{3}\)Department of Computer Science, City University of Hong Kong. \({}^{4}\)Yuanpei College, Peking University. Correspondence to: Yaodong Yang <yaodong.yang@pku.edu.cn>

###### Abstract

Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions. Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent an exponentially vast spectrum of human preferences through various optimization methods. Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.

## 1 Introduction

AI alignment aims to ensure AI systems align with human intentions, and there has been notable progress in this area, especially for large language models (LLMs) . The prevailing approach for LLM alignment involves curating a dataset \(\{(x,y_{1},y_{2},z)\}\), where each prompt \(x\) is associated with a pair of responses \((y_{1},y_{2})\) and a scalar label \(z\{0,1\}\) that indicates if \(y_{1}\) is a "better" response. These labels are typically generated based on detailed guidelines that encompass various criteria, reflecting multiple dimensions \(i\{1,,m\}\) of human preferences (_e_.\(g\)., helpfulness, harmlessness, conciseness, humor, formality). Pre-trained models are subsequently further optimized on this dataset using methods including reinforcement learning, supervised learning, or game-theoretical approaches . However, this _single-objective alignment_ methodology may not fully capture the complexity of real-world scenarios for two reasons (Figure 1).

**First**, this method can lead to inconsistency and ambiguity in **data labels**. Human labelers assign scalar labels \(z\) by _implicitly_ evaluating responses across every dimension \(i\) with _different preference weights_ to \(i\), and reaching a final judgment. These differences often result in conflicting labels, causing misalignment or learning failures (Appendix B), substantiated by the low average label agreement reported in . **Second**, optimizing a single objective leads to only one **model** that attempts to fit the potentially conflicting labeling preferences, \(i\)._e_., the helpfulness-harmlessness dilemma. Thissingle model may not cover the full spectrum of human preferences across all dimensions, thereby exacerbating biases against underrepresented groups and failing to meet diverse user needs.

To address these challenges, we formulate the alignment as a multi-dimensional preference optimization (MDPO) problem. By _explicitly_ curating data for each dimension, we enhance data consistency and simplify the labeling process, thereby **overcoming the first limitation**.

Upon the obtained dataset, our goal is to concurrently optimize across all dimensions. However, this is often infeasible due to potential conflicts among preferences (_e.g._, helpfulness _vs._ harmlessness in response to hazardous user requests). Therefore, we aim for Pareto-optimality , which means finding solutions where no preference dimension can be made better off without making another worse off. However, many Pareto-optimal solutions might exist. Instead of just learning one such solution, we focus on learning the entire set of Pareto-optimal solutions. To achieve this, we use a single model capable of recovering any Pareto-optimal solution by inputting the appropriate preference vector.

In this paper, we propose Panacea (**P**areto **a**lignment **via** preference **a**daptation), a simple yet effective method that: 1) learns the entire Pareto-optimal solution set for all possible preferences with a single model, and 2) infers Pareto-optimal responses online by simply injecting any preference vector into the model. Our method, providing a comprehensive representation of human preferences, effectively caters to diverse user needs, thus **mitigating the second limitation** (Figure 1).

A key challenge lies in how to utilize a low-dimensional preference vector to control the model's behavior. Our core insight is that, similar to the crucial role of the preference vector in shaping the Pareto solution, singular values are pivotal in defining the model's fundamental behavior in a singular value decomposition (SVD)-based low-rank adaptation (LoRA). To address the above challenge, we incorporate the preference vector into the singular values within each SVD-LoRA layer. We then scale it using a learnable factor to align with the magnitude of other singular values. The model is trained end-to-end using a joint objective function aggregated according to the preference vector. The flexibility of Panacea enables seamless compatibility with various preference optimization procedures, _e.g._, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) , and direct preference optimization (DPO) , and diverse methods for loss aggregation, _e.g._, linear scalarization (LS) [Section 4.7.5] and weighted Tchebycheff (Tche) [Section 3.4]. Through theoretical analysis, we confirm that Panacea can effectively capture the entire Pareto front (PF) under practical conditions. This finding provides a solid rationale for training a single Pareto set model to learn all Pareto optimal solutions across the entire preference space.

In our experiments, we assess the effectiveness and scalability of Panacea on several significant and challenging preference alignment problems with up to 10 dimensions, where the Pareto set

Figure 1: Comparison of the predominant single-objective alignment and our multi-dimensional alignment. For the two responses to a prompt, labelers agree on the preferable one in each preference dimension, but conflict when assigning a synthesized scalar label denoting which is “better”. This arises due to the inherently different preference weights held by labelers, a common case in reality. Performing single-objective optimization on the potentially conflicting scalar-label dataset (left) could lead to a dominated solution and misalignment. By contrast, our method, Panacea, leverages multi-dimensional preference optimization (right) on the consistent multi-dimensional dataset and learns the entire Pareto front (PF), thereby aligning with diverse and complex human preferences.

cardinality grows exponentially with the number of dimensions, considerably surpassing the scope of current research. Panacea consistently outperforms baseline methods, producing superior, uniformly distributed, and convex fronts in accordance with the theory. Quantitative metrics highlight its substantial advantages, demonstrating an order-of-magnitude improvement. Notably, Panacea exhibits no performance saturation even on the ten-dimensional problem, indicating its extensive potential. For the first time, we show the possibility of aligning a _single_ model with _exponentially many_ heterogeneous preferences, opening up a promising avenue for LLM alignment.

This paper makes three main contributions. **First**, we identify the fundamental limitations of the predominant scalar-label, single-objective alignment paradigm, and propose to reframe alignment as a multi-dimensional preference optimization problem. **Second**, we design Panacea, a simple yet effective method that learns one single model that can online and Pareto-optimally adapt to any set of preferences, without the need for further tuning. **Third**, we provide theoretical supports and empirical validations to demonstrate the Pareto optimality, scalability, efficiency, and simplicity of Panacea, thereby satisfying the urgent need for Pareto alignment to diverse human preferences.

## 2 Related Work

**Pareto Set Learning.** Different from previous classical multi-objective optimization (MOO) methods [59; 35; 38; 56] that use a finite set of solutions (referred to as "particles") to approximate the entire Pareto set, Pareto set learning (PSL) [41; 36; 58] aims to use a single model to recover the complete Pareto set/front. The advantage of PSL is that it can store an infinite number of Pareto solutions within a model. This allows users to specify their own preferences, and the model can dynamically output a particular Pareto solution in real-time according to those preferences. Typical applications of PSL includes multiobjective industrial design problems [58; 37], reinforcement learning [8; 54; 24], text-to-image generalization , and drug design [25; 61]. While there have been some studies on PSL involving deep neural networks, these models are considerably smaller compared to LLMs. Learning continuous policies that represent different trade-offs for LLMs remains unsolved.

**Multi-Dimensional Preference Optimization.** Existing research primarily treats AI alignment as a single-objective optimization problem with scalar labels [42; 55; 17; 44; 40; 47], often neglecting the complexity of diverse human preferences. We provide an in-depth analysis of this limitation in Appendix B, which is subsequently substantiated by MaxMin-RLHF's result of "impossibility of alignment" **after Panacea first came out**. To address this crucial gap, one recent attempt is AlignDiff , which trains an attribute-conditioned diffusion model to conduct preference alignment planning in the RL settings. In the realm of LLMs, there are some contemporary works on this topic [60; 26; 18; 21; 51; 52; 53], where the most relevant one Rewarded Soups (RS)  adopts a multi-policy strategy. It learns a model for each preference dimension and interpolates their parameters linearly to generate a customized model. However, its simple design also constitutes its drawback. Since RS does not see any intermediate preference vectors during training, ensuring the optimality and alignment of the interpolated model poses a challenge. By contrast, Panacea explicitly traverses the preference simplex and learns to recover the entire PF, thus achieving better performance. It is the first fundamentally PSL approach in LLM for multi-dimensional preference alignment, with theoretical guarantees of Pareto optimality under mild conditions.

## 3 Problem Formulation

Human preference is inherently multi-dimensional. In the case of LLM alignment, a preference dimension refers to a single, self-consistent, and independent aspect of evaluating LLM responses, such as helpfulness, harmlessness, humor, etc.. We formulate the multi-dimensional preference optimization (MDPO) problem with \(m\) dimensions as:

\[_{}(_{})=(J_{1}(_{}),J_{2}(_{ }),,J_{m}(_{})),\] (1)

where \(_{}\) is a policy, _i.e._ an LLM, and \(\) is its trainable parameters (decision variable), \(\) is the policy space, \(\) is the parameter space, and \(J_{i},i=1,,m\) denotes a performance measure of dimension \(i\), such as SFT objective \(J_{,i}(_{})\), RLHF objective \(J_{,i}(_{})\), and DPO objective \(J_{,i}(_{})\) detailed in the following equations,

\[J_{,i}(_{}) =_{(x,y)_{i}}[_{}(y|x) ],\] (2) \[J_{,i}(_{}) =_{x}[_{y_{} |x}[r_{i}(x,y)]-_{}[_{} (|x)||_{}(|x)]],\] (3) \[J_{,i}(_{}) =_{(x,y_{w},y_{l})_{i}}[ ((y_{w}|x)}{_{}(y _{w}|x)}-(y_{l}|x)}{_{ }(y_{l}|x)})].\] (4)

Notice that \(_{i},r_{i}\) represent the data and reward model for dimension \(i\) respectively. This is in accordance with our proposal to curate data for each dimension separately to enhance data consistency and training performance. Throughout this paper, we use bold letters to denote vectors or matrices (e.g. \(,\)). Very often, there does not exist a single solution \(\) that performs optimally on all dimensions due to their conflicts. Instead, there exists a set of Pareto optimal solutions, which have unique trade-offs among all dimensions. We say solution \(^{(a)}\)_dominates_\(^{(b)}\), denoted as \((_{^{(a)}})(_{^{(b)}})\), if for all \(i[m]\), \(J_{i}(_{^{(a)}}) J_{i}(_{^{(b)}})\), and there exists at least one index \(j[m]\) such that \(J_{j}(_{^{(a)}})>J_{j}(_{^{(b)}})\). Based on this, Pareto optimality is defined as:

**Definition 3.1** (Pareto optimality).: We call a solution \(^{*}\)_Pareto optimal_ if no other solution \(^{}\) dominates \(^{*}\). The set of all Pareto optimal solutions is called the _Pareto set_ (PS); while its image set in the objective space is called the _Pareto front_ (PF), \(\). A solution \(^{*}\) is considered weakly Pareto optimal if no other solution \(^{}\) can strictly dominate it, that is, if \(J_{i}(_{^{}})>J_{i}(_{^{*}})\) for all \(i[m]\).

Human's trade-offs among all dimensions are quantified as a preference vector, \(=(_{1},,_{m})\), where \(_{m}\), \(_{i} 0\), and \(_{i=1}^{m}_{i}=1\). Here, \(_{i}\) represents the weight for preference dimension \(i\) (called preference weight), and \(_{m}\) is the preference simplex. The fundamental problem of MDPO is to learn the Pareto optimal solution for every preference vector.

## 4 Panacea: Pareto Alignment via Preference Adaptation

To solve the MDPO problem, our goal is to learn a single model capable of representing the entire Pareto-optimal solution set. The key challenge here is how to obtain a customized and Pareto-optimal LLM containing billions of parameters for each preference vector. Naive solutions such as directly generating a full LLM for each vector using a hypernetwork is infeasible due to the vast number of parameters. To avoid this, we consider LoRA , a parameter-efficient fine-tuning method, which, for each layer, freezes the original weights \(_{0}\) and only learns pairs of rank decomposition matrices

Figure 2: Panacea embeds the preference vector into singular values of each SVD-LoRA layer and scales it with learnable factors to match the magnitudes. During learning, for each data batch, we randomly sample a preference vector from the preference simplex and train the embedded model with various optimization procedures and loss aggregation methods. In the inference stage, the model adapts online to the user-specified preference vector and exhibits Pareto alignment in its responses.

\(,\) for adaptation. According to LoRA, the final weight \(\) is obtained by \(=_{0}+\). However, a rank-8 LoRA of Alpaca-7B  still contains nearly 20 million parameters, which means producing separate LoRA parameters for each preference vector can also significantly suffer from training difficulty and instability issues. We thus explore an alternative approach inspired by AdaLoRA . This method employs singular value decomposition (SVD)-based LoRA and learns the left singular matrix \(\), diagonal matrix \(\) (representing singular values), and right singular matrix \(\). Moreover, \(\) and \(\) are subject to orthogonality regularization.

\[=_{0}+^{},\] (5)

which hereafter we call SVD-LoRA. By extracting singular values \(\) of incremental matrices, SVD-LoRA captures the core features of adaptation in a few parameters. More importantly, the singular values provide an interface to fundamentally influence model behavior.

Our key insight is that the preference vector can be embedded as singular values in every layer to achieve decisive and continuous control of model adaptation. Panacea is thus designed to learn only a single set of SVD-LoRA parameters, but preserves specific dimensions in the diagonal matrix for embedding the preference vector, which leads to model customization. Concretely, for layer \(l\), we preserve \(k\) singular values for learning general and preference-agnostic features and concatenate them with the \(m\) dimensional preference vector \(\) multiplied by a per-weight-matrix learning scale factor \(s^{l}\). Therefore, for each weight matrix \(^{l}^{n_{1}^{l} n_{2}^{l}}\), we have \(_{0}^{l}^{n_{1}^{l} n_{2}^{l}}\), left singular matrix \(^{l}=[_{1}^{l},,_{k}^{l},_{k+1}^{l},,_{k+m}^{l}]^{n_{1}^{l}(k+m)}\), diagonal matrix \(^{l}=(_{1}^{l},,_{k}^{l},s^{l}_ {1},,s^{l}_{m})^{(k+m)(k+m)}\), and right singular matrix \(^{l}=[_{1}^{l},,_{k}^{l},_{k+1}^{l},,_{k+m}^{l}]^{n_{2}^{l}(k+m)}\). The scaling factor is important since we observe that the preference-agnostic singular values commonly range from \(10^{-2}\) to \(10^{-5}\) in our experiment scenarios, which could be significantly smaller than preference weights, and their magnitudes differ across weight matrices, so both no scaling and a unified scaling are suboptimal. Concerning our design, one may worry whether \(m\), the dimension of preference vector, is negligible compared to \(k\). Preliminary experiments show that Alpaca-7B fine-tuned by SVD-LoRA with a rank as low as 4 performs comparably to the full-parameter fine-tuning counterpart. Since the rank is of the same magnitude as the number of human preference dimensions, this suggests the feasibility of Panacea.

During each training iteration, we randomly sample a preference vector from the preference simplex \(_{m}\), embed it into all weight matrices, and obtain the preference embedded model \(_{,}\). We then compute an aggregated objective function of \(_{,}\) across all preference dimensions according to \(\), by synthesizing per-dimension objective functions with loss aggregation methods. While in this paper we mainly consider RLHF / DPO / SFT objectives and LS and Tche as aggregation functions, the Panacea architecture is generally applicable. The LS function [Section 4.7.5] is given by

\[_{}g_{}^{}()=_{} ^{m}_{i=1}_{i}J_{i}(_{}),\] (6)

and the Tche function is defined as,

\[_{}g_{}^{}()=_{}_{1  i m}_{i}(J_{i}(_{})-z_{i}),\] (7)

where \(\) is a vector (e.g., ideal vector) such that \(z_{i} J_{i}(_{}),, i[m]\). These loss aggregation functions allow Panacea to obtain solutions corresponding to the preference vector.

With respect to the aggregated objective, trainable parameters for each weight matrix \(^{l}\), including \(^{l}\), \(^{l}\), \((_{1}^{l},,_{k}^{l})\), \(s^{l}\), are then updated via gradient descent. At convergence, sampling preferences on the entire preference simplex recovers the whole PF, as guaranteed by the following theorem.

**Theorem 4.1**.: _Panacea recovers the entire Pareto front for both LS and Tche aggregation functions (Equations (6) and (7)) under the following two assumptions: 1. Panacea with SVD-LoRA has sufficient representation capability for all preferences \(_{m}\). Specifically, for any preference vector \(\), the policy \(_{,}\) can optimize the corresponding aggregation functions (Equations (6) and (7)) to their maximum values. 2. For a specific preference vector \(\), the LLM policy space formed by all \(_{,}\) can represent all categorical output distributions of responses. By optimizing the Panacea objective function \(_{(_{m})}[g_{}^{ }()]\), where \(g_{}^{}\) could be \(g_{}^{}\) or \(g_{}^{}\), the optimal policy found by Panacea can recover the entire Pareto front for almost every preference._

For proof, see Appendix C. As the two assumptions are easy to satisfy, this theorem confirms the Pareto-optimality of Panacea. Panacea also achieves fine-grained control of model behavior through preference embedding, making it a suitable solution to MDPO. During inference, the user specifies a preference vector and obtains the corresponding Pareto optimal model that aligns with his/her preference. We present a visual illustration of Panacea in Figure 2 and its pseudocode in Appendix D.

Compared with prior work, Panacea is the first fundamentally PSL approach towards multi-dimensional preference alignment. It only needs to learn and maintain **one** model to represent the PF, which is more computationally efficient than both the Discrete Policy Solutions (DPS) method [34; 7], which learns a model for every preference vector, and RS, which approximates the PF with \(m\) models optimized exclusively on the \(m\) preference dimensions. Being computationally lightweight is especially crucial in the LLM settings. Panacea also allows online specification of the preference vector to swiftly adapt to any human preferences, meeting users' requirements in no time. Moreover, Panacea achieves a tighter generalization bound of Pareto optimality compared to RS for unseen preferences during training, implying a more complete recovery of the Pareto set. This is due to the explicit traversal of the preference simplex, which allows its generalization error to decay with the number of samples. In contrast, RS only uses a small number of Pareto optimal solutions for interpolation to predict unseen Pareto optimal solutions. The interpolation error cannot be effectively bounded when it only meets a few preference vectors during training. Finally, Panacea preserves explainability to some extent. For each weight matrix \(^{l}\), Panacea adapts its as

\[^{l}=_{0}^{l}+^{l}^{l}^{l^{}}=_{0 }^{l}+^{k}_{i}^{l}_{i}^{l}_{i}^{l^{ }}}_{}+^{m}s^{l}_{i}_{k+i}^{l}_{k+i}^{l}}_{}.\] (8)

Intuitively, term \(\) captures shared features among preference dimensions, while term \(\) learns dimension-specific adaptations and weights them by the preference vector to achieve Pareto alignment. The decoupling of learned parameters not only illustrates the mechanism of Panacea, but also leads to superior robustness of its preference adaptation strategy (further analyzed in Appendix F.5).

## 5 Experiments

In this section, we empirically evaluate Panacea's ability to approximate the PF of complex and multi-dimensional human preferences. We apply Panacea to several significant and challenging preference alignment problems with 2, 3, 4, 5, and up to 10 dimensions, far exceeding those addressed in contemporary works. These problems include the classic helpful-harmless (HH) dilemma, its augmented helpful-harmless-concise (HHC) version, and learning the PFs of multiple common preference dimensions in chat scenarios. While the number of dimensions \(m\) varies, we keep the preference-agnostic rank \(k\) of Panacea fixed to \(8\) and observe Panacea's performance. Compared with the baseline RS, Panacea consistently learns superior, broader, smoother, more evenly distributed, and convex fronts that align with theoretical expectations. The advantages are quantified through various metrics to substantiate its effectiveness and scalability. Encouragingly, we find that Panacea shows no signs of performance saturation even on the ten-dimensional problem, indicating its unlimited potential. We also conduct ablation studies to validate the design of Panacea. Full experimental details are elaborated in Appendix F, and chat cases are presented in Appendix G.

    & &  &  &  &  \\  Experiment & Model & Optim. & RS & Panacea & RS & Panacea & RS & Panacea & RS & Panacea \\   & LLam1-H & RLHF & 517.28 & **915.04** & 11.26 & **14.27** & 739.51 & **275.58** & 329.53 & **207.19** \\  & LLam1-H & DPO & 0.319 & **0.322** & 0.317.04 & 0.622 & **0.393** & 0.637 & 0.48 & 0.3/9.35 & 2.88 & **2.51**/3.25 \\  & LLam2-H & RLHF & 519.38 & **840.45** & 8.59 & **14.68** & **890.4** & \(53328\) & **89.38** & 275.7 \\  & LLam2-H & DPO & 0.318 & **0.337** & 0.334 & 0.641 & **0.653** & 0.652 & 0.73 & **0.36**/0.53 & 3.24 & **3.12**/3.71 \\   & LLam2-H & RLHF & 13519 & **17097** & 5.37 & **9.19** & 21.16 & **48.44** & **65.15** & 65.78 \\  & LLam2-H & DPO & 0.171 & **0.177** & 0.64 & **0.65** & 0.1 & **0.06** & **1.98** & 2.45 \\  Chat 3-dim & LLam3-Instrct & SPT & 0.29 & **0.50** & \(-0.58\) & \(-0.42\) & 0.68 & **0.04** & 6.37 & **2.13** \\ Chat 4-dim & LLam3-Instrct & SPT & 0.14 & **0.38** & \(-0.65\) & \(-0.43\) & 0.25 & **0.02** & 5.06 & **2.17** \\ Chat 5-dim & LLam3-Instrct & SPT & 0.08 & **0.33** & \(-0.66\) & \(-0.42\) & 0.14 & **0.02** & 4.91 & **2.28** \\ Chat 10-dim & LLam3-Instrct & SPT & 0.01 & **0.12** & \(-0.66\) & \(-0.47\) & 0.03 & **0.01** & 3.94 & **2.19** \\   

Table 1: This table compares algorithm performance using MOO metrics across all experiment evaluations. An upward arrow (\(\)) means a larger value for this metric is better, whereas a downward arrow (\(\)) indicates the opposite. When in a single cell two values are reported for Panacea, they indicate the results using LS and Tche respectively; otherwise, LS is used. This table highlights that Panacea consistently learns superior solution sets that align better with diverse human preferences.

### Mastering Dual Dimensions: Addressing the Helpful-Harmless Dilemma

In the first set of experiments, algorithms are tasked with two-dimensional preference alignment using various initial models, _i.e._ Alpaca-finetuned  Llama1-7B-base (_abbv._ Llama1-ft) and Llama2-7B-base  (_abbv._ Llama2-ft), optimization procedures, _i.e._ RLHF and DPO, and loss aggregation methods, _i.e._ LS and Tche. Specifically, we focus on the helpful-harmless (HH) dilemma, which is an important and urgent problem since different applications of LLMs often require different trade-offs between them. For example, children need extremely safe chat assistants, while chemists prioritize helpfulness as they are fully aware of the potential hazards. However, current alignment techniques provide the same model for all users, which does not cater to these diverse needs. Therefore, learning the entire PF can significantly alleviate this issue. We use the BeaverTails dataset , which has preference labels for both helpfulness and harmlessness.

In Figure 3 left, we show the learned fronts of algorithms with the task configuration of Llama1-ft, RLHF, and LS aggregation. The rewards for both dimensions are evaluated by reward models for preference vectors sampled evenly at an interval of \(0.1\), _i.e._\(=(0.0,1.0),(0.1,0.9),,(1.0,0.0)\). Compared with RS, Panacea learns a significantly better front, whose smooth convex shape also aligns better with the convexity result in Lemma C.3. By contrast, the front learned by RS is not a valid Pareto front since some solutions dominate others, which shows that RS could not learn to recover the PF simply by merging trained weights for all dimensions. In this experiment, we also test Discrete Policy Solutions (DPS) [34; 7], also known as multi-objective RLHF (MORL) in , which learns a separate model for each preference vector (11 models in this case) and is commonly considered as the performance upper bound for this problem. Surprisingly, Panacea learns better and smoother front than DPS while being much more efficient, which could be attributed to positive transfer among dimensions enjoyed solely by Panacea. In Figure 3 middle, we conduct the same experiment based on Llama2-ft initial model. Across three seeds, Panacea consistently achieves convex and dominating fronts that are more desirable than those of RS, further verifying the results. To clearly demonstrate how the model's output changes with variations in the preference vector, we present an exemplar chat case in Figure 4 and its detailed version in Appendix G. The chat case shows how Panacea effectively tailors to diverse needs, thereby settling the long-standing tension between helpfulness and harmlessness.

Figure 4: Responses of the model to the same user prompt with two extreme preference vectors. Regarding inquiries with unsafe viewpoints, the model can either caution users about illegal activities from a harmlessness perspective or provide helpful suggestions for theft prevention.

Figure 3: Algorithm performance on HH. Baseline methods (RS and DPS) require training a separate model for each preference dimension/vector, whereas **Panacea learns a single adaptable model.**_Left_: Panacea is significantly better than RS and even outperforms DPS, showing its superiority in learning PF while being more efficient. _Middle_: on Llama2-ft across different seeds, Panacea again consistently outperforms RS, and its fronts exhibit smooth convex shapes that correspond with theory. _Right_: with DPO, Panacea using both LS and Tche aggregation learns better fronts than RS.

To further study the generality of Panacea, we conduct experiments with Llama2-ft, DPO, and LS / Tche aggregation, where Panacea is optimized based on Equation (18) and Equation (19) respectively. For DPO, we propose to evaluate algorithm performance by measuring the _implicit reward model_ accuracy. That is, for a model \(_{}\), it is accurate on a labeled pair \((x,y^{i}_{w},y^{i}_{l})\) if \((y^{i}_{w}|x)}{_{ret}(y^{i}_{w}|x)}> (y^{i}_{l}|x)}{_{ret}(y^{i}_{l}|x)}\), and its total accuracy is obtained by averaging over dataset. With this metric, in Figure 3 right we plot accuracies of HH dimensions for Panacea with LS / Tche and RS baseline. Results again confirm that Panacea always obtains better fronts.

Aside from comparing the fronts learned by Panacea and the baseline, we also quantify the advantage of Panacea by computing four MOO metrics in Table 1. **Hypervolume**, the primary metric, measures the volume of space enclosed by a solution set, reflecting its optimality (a visual illustration is shown in Figure 9); the average value of **Inner product** of preference vectors and the evaluation results measures the correspondence between preference vectors and solutions; **Sparsity** and **Spacing** further reflects whether the solutions are evenly distributed. Mathematical expressions of these metrics are detailed in Appendix F.4. Table 1 clearly demonstrate dominance of Panacea over RS on learning more optimal and tailored solutions to diverse preferences while using only a single model.

### Navigating Tri-Dimensional Trade-offs: Helpful, Harmless, and Concise Alignment

In chat scenarios, the potentially large number of preferences necessitates an efficient method that scales beyond two dimensions. Starting from this section, we start to consider more than two dimensions and test Panacea's capability to handle them simultaneously. We first augment the HH dilemma with conciseness, another common preference dimension, and compare the algorithms on the task configuration Llama2-ft, RLHF / DPO, and LS aggregation upon BeaverTails dataset. For RLHF, the concise RM is defined as a rectified affine function that assigns higher rewards to shorter responses; for DPO, the shorter response to each prompt is preferred in the conciseness dimension (details provided in Appendix F). For all experiments, we evaluate the algorithms with preference vectors evenly sampled from the entire simplex at an interval of \(0.2\), _i.e._\(=(0.0,0.0,1.0),(0.0,0.2,0.8),,(1.0,0.0,0.0)\), and provide the results in Figure 5 and Table 1.

Figure 5 visualizes the fronts learned with RLHF procedure. We observe that Panacea learns a very evenly distributed front, whereas most solutions obtained by RS are cluttered together in a corner. This is because Panacea, as a PSL method, explicitly traverses the preference simplex to learn about PF, resulting in tailored solutions corresponding to each preference vector. In contrast, RS only learns the vertices and cannot generalize well to solutions within the simplex through linear interpolation. Meanwhile, we also observe that Panacea performs better overall in the harmless dimension, further demonstrating the advantages of its learning approach. MOO metrics in

Figure 5: Learned fronts of Panacea (red) and RS (blue) on HHC problem with Llama2-ft, RLHF, and LS aggregation. Panacea learns a better and more evenly distributed front while solutions of RS clutter in a corner. This suggests Panacea provides fine-grained solutions to diverse human preferences.

Figure 6: Comparison of learned fronts on Chat 3-dim problem. On the left we show a 3D visualization of Panacea (red) and RS (blue) and on the right we show 2D projections by setting one of preference weights to zero. Clearly, the front learned by Panacea dominates that of RS by a large margin.

Table 1 again numerically depict the benefits of Panacea, and the chat case in Appendix G serves as qualitative support. Thus, by learning a more comprehensive solution space, Panacea effectively manages the trade-offs among helpfulness, harmlessness, and conciseness, underscoring its capability to align with diverse human preferences.

### Scaling Up: Towards Tens-of-Dimensional Pareto Alignment with a Single Model

We further test Panacea's scalability on three, four, five, and up to ten-dimensional alignment problems (_abbv_: Chat 3, 4, 5, and 10-dim), where the considered dimensions include being humorous, philosophical, sycophantic, helpful, concise, creative, formal, expert, pleasant, and uplifting. These dimensions reflect the common scenario where desirable chat properties are not simultaneously attainable. Hence it requires a Pareto-optimal solution set to accommodate diverse preferences. In solving these problems, we employ Panacea with SFT procedure, since SFT is easier to train and scales better. The initial model used in this series of experiments is Llama-3-8B-Instruct  (_abbv_. Llama3-Instruct), and the loss aggregation function is LS. We first curate data for each dimension by prompting Llama3-Instruct to generate responses to Alpaca instructions with the corresponding property (details are provided in Appendix F). Panacea is then trained using LS aggregated SFT loss. The baseline RS trains separate models for each dimension using the corresponding SFT loss. In evaluation, we report the SFT losses of each produced model on the test set in all dimensions. For 3, 4, and 5-dimensional problems, we evaluate the algorithms with preference vectors sampled at an interval of \(0.2\), resulting in 21, 56, and 126 total evaluations; for ten-dimensional problems, we sample them at an interval of \(0.25\), amounting to 715 in total. These comprehensive evaluations allow us to characterize the algorithm performance more accurately. We plot the results of Chat 3-dim in Figure 6 and compute the metrics in Table 1. Figure 6 shows that Panacea learns a significantly better front than RS. For the higher-dimensional problems where the results cannot be visualized, we verify the convexity of Panacea's learned fronts by computing their convex hulls and observing that all evaluation points are on the respective convex hulls. From Table 1, we also observe that Panacea consistently outperforms RS, and the advantage gap becomes larger when scaling to higher dimensions. Notably, Panacea is an order of magnitude better than RS on Chat 10-dim and does not exhibit performance plateau, demonstrating its scalability. We provide a chat case in Appendix G from Chat 3-dim to show Panacea's performance. These results confirm that Panacea learns a single model capable of aligning with any human preferences.

### Ablation Study and Analysis

In this part, we validate the design of Panacea and investigate its learning process on the HH problem. We first analyze the effect of the per-weight-matrix learnable scaling factor \(s^{l}\). Intuitively, it scales preference vectors to the same magnitude as the singular values to avoid either dominant or negligible

Figure 7: **Left**: Ablation study on the learnable preference vector scaling factor. Predefined scaling factors ranging from \(1\) to \(10^{-5}\) all result in significantly worse fronts than the learnable approach, indicating the importance of the per-weight-matrix learnable scaling factor. **Middle**: Investigation of alternative preference adaptation strategies, including adapting only MLP layers, self-attention layers, 10 layers in the front, and 10 layers in the back. Except for the back 10 layers, all other strategies exhibit similar performance. Thus, we decide to adapt all layers for better representation capacity. **Right**: We show the fronts learned by Panacea at different RLHF steps. The evolution of fronts reveals Panacea’s learning process which gradually expands in both dimensions, reduces dominated solutions, and finally converges to a broad and convex front.

influence of preference-specific features on \(^{l}\), as observed from the learned parameters. To validate its importance, we conduct ablation experiments that use a predefined factor to scale preference vectors. Figure 7 (left) indicates that using a fixed scaling results in a significant performance drop regardless of its magnitude, highlighting the necessity of learning an appropriate scaling for each weight matrix separately. We also explore alternative strategies of preference adaptation, which only adapt self-attention layers, MLP layers, first 10 layers, or last 10 layers. Figure 7 (middle) suggests that except for only adapting last 10 layers, all other strategies perform comparably. Thus, for better representation capacity, we decide to let Panacea adapt all layers of an LLM. Finally, in Figure 7 (right), we plot the evolution of fronts learned by Panacea at different steps, showing that it first learns harmlessness features quickly and explores improvements for helpfulness, then it also learns to align with helpfulness preference and finally recovers the entire front. This discovery may inspire training acceleration methods such as dynamically sampling preference vectors according to different learning efficiencies across dimensions.

## 6 Conclusion

This paper presents Panacea, the first Pareto set learning approach towards solving Pareto alignment with multi-dimensional human preference using a single model. Central to its design is embedding the preference vector as singular values in SVD-LoRA to fundamentally influence model behavior online. Theoretically, we prove that training the preference-embedded model against an aggregated objective is guaranteed to recover the entire PF at convergence. Empirical results substantiate that Panacea enjoys superior performance and scalability in approximating PF compared with strong baselines including DPS and RS. Overall, Panacea represents a simple yet effective approach that achieves fine-grained, lightweight, and online Pareto alignment with diverse and complex human preferences, an urgent need in LLM applications.