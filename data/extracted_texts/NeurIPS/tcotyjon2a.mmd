# CQM: Curriculum Reinforcement Learning

with a Quantized World Model

 Seungjae Lee, Daesol Cho, Jonghae Park, H. Jin Kim

Seoul National University

Automation and Systems Research Institute (ASRI)

Artificial Intelligence Institute of Seoul National University (AIIS)

{ysz0301, dscho1234, bdfire1234, hjinkim}@snu.ac.kr

###### Abstract

Recent curriculum Reinforcement Learning (RL) has shown notable progress in solving complex tasks by proposing sequences of surrogate tasks. However, the previous approaches often face challenges when they generate curriculum goals in a high-dimensional space. Thus, they usually rely on manually specified goal spaces. To alleviate this limitation and improve the scalability of the curriculum, we propose a novel curriculum method that automatically defines the semantic goal space which contains vital information for the curriculum process, and suggests curriculum goals over it. To define the semantic goal space, our method discretizes continuous observations via vector quantized-variational autoencoders (VQ-VAE) and restores the temporal relations between the discretized observations by a graph. Concurrently, ours suggests uncertainty and temporal distance-aware curriculum goals that converges to the final goals over the automatically composed goal space. We demonstrate that the proposed method allows efficient explorations in an uninformed environment with raw goal examples only. Also, ours outperforms the state-of-the-art curriculum RL methods on data efficiency and performance, in various goal-reaching tasks even with ego-centric visual inputs.

## 1 Introduction

Goal-conditioned Reinforcement Learning (RL) has been successfully applied to a wide range of decision-making problems allowing RL agents to achieve diverse control tasks . However, training the RL agent to achieve desired final goals without any prior domain knowledge is challenging, especially when the desired behaviors can hardly be observed. In those situations, humans typically adopt alternative ways to learn the final goals by gradually mastering intermediate sub-tasks. Inspired by the way humans learn, recent RL studies  have solved uninformed exploration tasks by suggesting which goals the agent needs to practice. In this sense of generating curriculum goals, previous approaches proposed various ideas to involve providing intermediate-level tasks , quantifying the uncertainty of observations , or proposing contextual distance to gradually move away from the initial distribution .

However, previous curriculum RL studies are mostly not scalable. Namely, they suffer from serious data inefficiency when they generate curriculum goals in high dimensions. Because of this limitation, they usually rely on the assumption that manually specified goal spaces (e.g., global X-Y coordinates) and clear mappings from high-dimensional observations to the low-dimensional goal spaces are available. Such an assumption requires prior knowledge about observations and the tasks, which remains a crucial unsolved issue that restricts the applicability of previous studies.

In order to design a general curriculum solution without the need for prior knowledge about the observations, defining its own goal space for the curriculum could be an effective scheme. To do so,the two following operations need to be executed concurrently. (1) composing the _semantic goal space_ which contains vital information for the curriculum process from the arbitrary observation space, and (2) suggesting to the agent which goal to practice over the goal space. Let us consider an agent that tries to explore an uninformed environment with final goal images only. To succeed, the agent needs to specify the semantic goal space from the high-dimensional observation space, and suggest the curriculum goals (e.g., the frontier of the explored area) over the composed goal space to search the uninformed environment. However, most previous studies focused solely on one of these, specifying the low-dimensional goal space without considering how to provide intermediate levels of goals [17; 27], or just suggesting curriculum goals in manually specified semantic goal spaces [10; 21; 6].

The challenge of simultaneously managing (1) specifying the goal space and (2) suggesting curriculum goals is that they are intimately connected to each other. If the agent constructs an ill-formed goal space from the observation space, it would be difficult to propose curriculum goals over it. Conversely, if the method fails to suggest goals to enable the agents to explore the unseen area, it would also be difficult to automatically learn the goal space that covers the uninformed environment based on the accumulated observations. Therefore, it is essential to develop an algorithm that addresses both defining goal space and providing curriculum goals concurrently.

In this paper, we propose a novel curriculum reinforcement learning (RL) method which can provide a general solution for a final goal-directed curriculum without the need for prior knowledge about the environments, observations, and goal spaces. First, our method defines its own semantic goal space by quantizing the encoded observations space through a discretization bottleneck and restoring the temporal relations between discrete goals via a graph. Second, to suggest calibrated guidance towards unexplored areas and the final goals, ours proposes uncertainty and temporal distance-aware curriculum goals that converge to the final goal examples.

The key contributions of our work (CQM: Curriculum RL with **Q**unatized World **M**odel) are:

* CQM solves general exploration tasks with the desired examples only, by simultaneously addressing the specification of a goal space and suggestion of curriculum goals (Figure 3).
* CQM is the _first_ curriculum RL approach that can propose calibrated curriculums toward final goals from high-dimensional observations, to the best of our knowledge.
* CQM is the _only_ curriculum RL method that demonstrates reliable performance despite an increase in the problem dimension, among the 10 methods that we experimented with. (Even state-based \(\) vision-based)
* Ours significantly outperforms the state-of-the-art curriculum RL methods on various goal reaching tasks in the absence of a manually specified goal space.

## 2 Related Works

Curriculum Goal Generation.Although various prior studies [41; 19; 48; 8; 45; 22] have been proposed to solve exploration problems, enabling efficient searching in uninformed environments still

Figure 1: CQM simultaneously tackles the interrelated problems of specifying the goal space and suggesting which goal the agent needs to practice. CQM trains a VQ-VAE to form a discretized goal space and constructs a graph over it, capturing the relations between the discretized observations (landmarks). Concurrently, CQM suggests the agent which goal to practice based on uncertainty and temporal distance.

remains a challenge. An effective way to succeed in such tasks with hardly observed final goals is **identifying uncertain areas** and instructing an agent to achieve the goals in these areas. To identify the uncertain areas and provide the goals sampled from them, previous studies employ uncertainty-based curriculum guidance by the state visitation counts [4; 31], absolute reward difference , and prediction model [32; 5]. Other approaches propose to utilize disagreements of ensembles[33; 50; 28] or sample the tasks with high TD errors  to generate goals in uncertain areas. An alternative way for solving the exploration tasks is to execute a **final goal-directed exploration** to propose tailored guidance. To this end, some studies perform successful example-based approaches [25; 6] or propose to minimize the distance between the final goals and curriculum goals [38; 21], measuring it by the Euclidean distance metric. Some studies also employ contextual distance-based metrics to perform final goal-directed **exploration away from the initial distribution**[15; 6].

However, these methods usually assume that agents have prior knowledge about the observations and unrestricted access to manually specified semantic goal space (e.g. global X-Y coordinates) because they are not scalable to handle the high-dimensional goal spaces. For example, the meta-learning classifier-based uncertainty metrics [25; 6] suffer from distinguishing uncertain areas as the dimension of the goal space increases. Also, some of the methods rely on Euclidean distance metric [38; 21] over the goal space. Moreover, generating curriculum goals , employing various prediction models [32; 5; 33; 50], fitting Gaussian mixture models , and utilizing disagreements of ensembles-based methods [33; 50] also face difficulty in solving increasingly complex problems in high-dimensional goal spaces. Although there have been attempts to propose a curriculum in high-dimensional observations [36; 13] or include an encoder in their model-based agent [28; 14], unfortunately, these approaches do not incorporate a convergence mechanism to the final goals, which are crucial for efficient curriculum progresses.

Our method incorporates the benefits of the aforementioned methods without manually specified goal spaces: **exploring uncertain areas** and **moving away from the initial distribution** while **converging to the desired outcome**. Although there is a study that has also incorporated these three aspects , it retains its performance only in manually specified goal spaces, as other curriculum methods (Figure 2). (We included conceptual comparisons between CQM and more related works in Table 1 in Appendix B.)

Discretizing Goal Space for RL.Vector quantized-variational autoencoder (VQ-VAE) is an autoencoder that learns a discretized representation using a learnable codebook. The use of this discretization technique for learning discrete representations in RL is a recent research topic [17; 27] and has shown an improved sample efficiency. Islam et al.  proposes to apply VQ-VAE as a discretization bottleneck in a goal-conditioned RL framework and demonstrates the efficiency of representing the continuous observation spaces into discretized goal space. Also, Mazzaglia et al.  utilizes VQ-VAE to discover skills in model-based RL by maximizing the mutual information between skills and trajectories of model states.

Unfortunately, the aforementioned methods require a pre-collected dataset or extra exploration policy, which are not necessary in CQM. Training VQ-VAE with a pre-collected dataset implies that the agent has access to the full information about the task or that it already possesses an agent capable of performing the task well. Although it is possible to obtain a pre-collected dataset through a random rollout policy, this is only in the case where exploring the environments is easy enough to succeed with only random actions.

## 3 Preliminaries

We consider a Markov Decision Process (MDP) which can be represented as a tuple \((,,,,_{0},)\), where \(\) is an observation space, \(\) is an action space, \((o_{t+1}|o_{t},a_{t})\) is a transition function, \(_{0}\) is an initial distribution, and \(\) is a discount factor. Note that the MDP above

Figure 2: When the curriculum methods are not scalable to handle the high-dimensional goal; the performance drop in the absence of manually specified goal space.

does _not_ contain a goal space, since we do not assume that the manually specified goal space is provided. Instead, we consider a discrete low-dimensional discrete goal space \(\), which is defined by the agent automatically. Also, we assume that the final goal examples \(o^{f}\) are provided by the environment, and we denote the projection of these examples into the goal space \(\) as \(g^{f}\). We represent curriculum goal as \(g^{c}\), which is sampled from the goal space \(\), and the reward function in the tuple can be represented as \(:\). Furthermore, we denote actor network as \(:\), and critic network as \(Q:\). (Thus, \(Q(o,a,g)\) indicates the goal-conditioned state-action value where goal \(g\), action \(a\), and observation \(o\) throughout this paper.)

## 4 Method

In order to provide a general solution for efficient curriculum learning, our method defines its own goal space and suggests to the agent which goal to practice over the goal space simultaneously. To compose a _semantic goal space_ which reduces the complexity of observation space while preserving vital information for the curriculum process, we first quantize the continuous observation space using a discretization bottleneck (section 4.1) and restore temporal relations in discretized world model via graph (section 4.2). Over the automatically specified semantic goal space, we generate a curriculum goal and guide the agent toward achieving it (section 4.3).

### Specifying Goal Space via VQ-VAE

In order to define a discrete low-dimensional goal space which allows a scalable curriculum with high-dimensional observations, we utilize VQ-VAE as a discretization bottleneck [46; 40; 47] as recently been proposed [17; 27]. VQ-VAE utilizes a codebook composed of \(k\) trainable embedding vectors (codes) \(e_{i} R^{D},i\), combined with nearest neighbor search to learn discrete representations. The quantization process of an observation \(o_{t}\) starts with passing \(o_{t}\) through the encoder \(\). The resulting encoded vector \(z_{e}=(o_{t})\) is then mapped to an embedding vector in the codebook by the nearest neighbor look-up as

\[z_{q}=e_{c},\ c=_{j}||z_{e}-e_{j}||_{2}.\] (1)

The discretized vector \(z_{q}\) is then reconstructed into \(_{t}=(z_{q})\) by passing through the decoder \(\). We closely follow  and train quantizer, encoder, and decoder using a vector quantization loss with a simple reconstruction loss. The first term in Eq. 2 represents the reconstruction loss, while the second term represents the VQ objective that moves the embedding vector \(e\) towards the encoder's output \(z_{e}\). We update the embedding vectors \(e\) using a moving average instead of a direct gradient update [17; 27]. The last term is the commitment loss, and we use the same \(_{}\) (\(=0.25\)) across all experiments.

\[L_{}=||((o_{t}))-o_{t}||_{2}^{2}+||[z_{e}]-e||_ {2}^{2}+_{}||z_{e}-[e]||_{2}^{2},( :)\] (2)

By utilizing VQ-VAE, the RL agent can specify a quantized goal space that consists of discrete landmarks \(L=\{l_{1},l_{2}, l_{m}\}\) in two ways. The first approach is to obtain each landmark by decoding each code as \(l_{j}=(e_{j})\). Alternatively, one can obtain the landmarks by passing (encoding, quantizing to the closest embedding vector, and decoding) the continuous observations sampled from the replay buffer through the VQ-VAE. We utilize the first approach as the default, and provide the ablation study to examine the effectiveness of the second approach.

It should be noted that this set of landmarks \(L=\{l_{1},l_{2}, l_{m}\}\) only represents discretized observations and does _not_ involve relations among the observations. We describe our approach that can better restore the temporal relations between landmarks in the next section.

### Graph Construction over Quantized Goal Space

In this section, we present the graph construction technique over the quantized goal space to allow the agent to capture the temporal information between the landmarks of the quantized world model. We consider a graph \(=(,)\) over the goal space where the vertices \(\) represent the landmarks\(L=\{l_{1},l_{2}, l_{m}\}\) obtained from decoding discrete codes of VQ-VAE, and the edges \(\) represent the temporal distance. We utilize Q-value to reconstruct the edge costs, following the method proposed in [49; 24]. If an agent receives a reward of 0 when reaching a goal and -1 otherwise, the timesteps required to travel between landmarks can be estimated using Q-value as (derivation: Appendix A)

\[(l_{i} l_{j})=log_{}(1+(1-)Q(l_{i},a,l_{j})).\] (3)

Using Eq. 3, we connect vertices with the distance below the cutoff threshold, and the resulting graph restores the temporal relations between the landmarks over a discretized goal space based on the temporal distance. In this way, the agent can calculate geodesic distances between landmarks,

\[^{}(l_{0} l_{f})=_{(l_{i} l_{j}) (l_{0} l_{f})}(l_{i} l_{j}),\] (4)

which enables better prediction of temporal distances in environments with arbitrary geometric structures. Also, to incorporate extended supports of the explored area into the graph by creating landmarks in newly explored areas, we periodically reconstructed the graph following .

### Uncertainty and Temporal Distance-Aware Curriculum Goal Generation

In the previous sections, we proposed a method for specifying a discretized goal space with semantic information. It is important to provide curriculum goals located in the frontier part of the explored area to expand the graph effectively toward the final goal in an uninformed environment. To achieve this objective, we propose an uncertainty and temporal distance-aware curriculum goal generation method.

To obtain a curriculum goal from graph \(=(,)\) over the specified goal space, our method samples the landmarks that are considered uncertain and temporally distant from the initial distribution \(_{0}\). Thanks to the quantized world model, quantifying uncertainty in a countable goal space is straightforward and computationally light. We quantify the count-based uncertainty of each landmark as \(_{}(l_{i})=1/((l_{i})+)\), based on the empirical distribution \((l_{i})\) derived from the recent observations as

\[(l_{i})=)}{_{i=1}^{k}N(l_{i})},\] (5)

where \(N(l_{i})\) indicates the number of occurrences of landmark \(l_{i}\) in recent episodes and is periodically re-initialized when the graph is reconstructed with a new set of landmarks.

Finally, we deliver the sampled landmarks as curriculum goals to the agent, considering both temporal distance and uncertainty aspects:

\[_{l_{i} L^{}}_{}(l_{ i}) u_{i}\] (6)

where \(u_{i}\) is a uniform random variable between 0 and 1 used to perform weighted sampling, and \(L^{}\) represents a subset of \(L\) that includes the top-k elements with the largest \(^{}\) (Eq. 4) values from initial state distribution. Our method, based on the uncertainty and temporal distance-aware objective (Eq. 6), is capable of providing calibrated curriculum guidance to the agent even in environments with arbitrary geometric structures, without requiring prior knowledge of the environment or a manually specified goal space. Furthermore, the curriculum guidance makes composing the goal space easier, illuminating the unexplored areas and vice versa.

Convergence to the final goal.The curriculum objective in Eq.6 provides a calibrated curriculum towards unexplored areas. In addition to this frontier-directed method, providing final goal-directed guidance can further improve the efficiency of exploration especially when the agent sufficiently explored the environment, i.e., the supports of the final goal and explored area start to overlap [35; 6]. In order to acquire the capability to propose a final goal-directed curriculum, we gradually shift the direction of exploration from the frontier of the explored area to the final goal distribution. To do so,we determine whether to provide the _curriculum goal_\(g^{c}\) that is sampled via Eq. 6 or the _final goal_\(g^{f}=(o^{f})\) which the environment originally provided (\((g^{c}),o^{f}\)).

We utilize a mixture distribution of curriculum goals, following the approach proposed in ,

\[p_{g^{c^{}}}= p_{g^{f}}+(1-)p_{g^{c}},\] (7)

where \(p_{g^{f}}\) is the distribution of the final goal, and \(p_{g^{c}}\) is the distribution of curriculum goals. The mixture ratio \(\) measures whether the achieved goal distribution \(p_{ag}\) "covers" the final goal distribution using KL divergence as \(=1/+ D_{}(p_{g^{f}}||p_{ag}),1\). When the support of achieved goal distribution \(p_{ag}\) (= visited state distribution) covers that of the final goal distribution \(p_{g^{f}}\), \(\) produces a value close to 1, and a value close to 0 when the supports of both distributions are not connected.

By combining the curriculum goal objective (Eq. 6) with the mixture strategy (Eq. 7), our approach generates instructive curriculum goals towards unexplored areas and provides the curriculum goals \(g^{c^{}}\) to the agent that "cover" the final goal distribution at the appropriate time when the agent is capable of achieving the final goal.

Planning over the graphAs presented above, CQM constructs a graph to restore the temporal relations between landmarks (Section 4.2) and utilizes it to calculate geodesic distances (Eq. 4). In addition to these benefits, we highlight that the graph can also provide the strength of planning, which allows the agent to reason over long horizons effectively [9; 16; 13; 49; 3; 24].

To generate a sequence of waypoints for achieving each goal, we perform shortest path planning (Dijkstra's Algorithm), following the details proposed in the previous graph-guided RL method . Consider a task of reaching a curriculum goal \(g^{c}\) from the current observation \(o_{0}\). CQM first adds the encoded observation \((o_{0})\) to the existing graph structure. Then, it finds the shortest path between the curriculum goal and current observation to return a sequence of waypoints \(((o_{0}),w_{1},...,w_{n},g^{c})\) where \(n\) indicates the number of waypoints in the shortest path. Finally, the agent is guided to achieve each decoded waypoint \((w_{i})\) during \(((w_{i-1})(w_{i}))\) (Eq. 3) timesteps, rather than achieving the curriculum goal directly. In other words, the RL agent produces goal-conditioned action \((|o_{t},w_{i})\), where \(o_{t}\) and \(w_{i}\) is observation and the waypoint (acting as a goal) respectively. After reaching the final waypoint \((w_{n})\), the agent receives the original curriculum goal, \(g^{c}\). The only change when the agent attempts to achieve the final goal \(g^{f}\) is that \(g^{f}\) comes at the end of the sequences, \(((o_{0}),w_{1},...,w_{n},g^{f})\), rather than \(g^{c}\). In this way, the proposed approach not only provides a tailored curriculum for achieving the final goal but also allows the agent to access more elaborate instructions (waypoints) for practicing each curriculum goal.

## 5 Experiments

The main goal of the experiments is to demonstrate the capability of the proposed method (CQM) to suggest a well-calibrated curriculum and lead to more sample-efficient learning, composing the goal space from the arbitrary observation space. To this end, we provide both qualitative and quantitative

Figure 3: Left: changes in the discretized goal space of the CQM(ours) as learning progresses. Right: visualization of the curriculum goals proposed by the CQM and baseline algorithms.

results in seven goal-reaching tasks including two visual control tasks, which receive the raw pixel observations from bird's-eye and ego-centric views, respectively. (refer to Appendix C for the detailed configurations of each task.)

We compare our approach with previous curriculum RL methods and previous graph-guided RL methods. We do _not_ provide manually specified goal space in any of the environments; the agent could not map its global X-Y coordinates from the full observation which includes all the state variables for the RL agents (e.g. angle and angular velocity of the joint, position, velocity...). Also, the results of CQM and the baselines that utilize external reward functions (all the methods except OUTPACE ) are obtained by using sparse reward functions. For the baselines that could not be applied in vision-based environments [24; 6], we utilize an extra autoencoder with auxiliary time-contrastive loss [44; 13].

The baselines are summarized below: **OUTPACE** proposes uncertainty and temporal distance-aware curriculum learning based on the Wasserstein distance and uncertainty classifier. **CURROT** interpolates the distribution of the curriculum goals and the final goals based on the performance of the RL agent. **GoalGAN** proposes the goals with appropriate levels of difficulty for the agent Using a Generative Adversarial Network. **PLR** selectively samples curriculum goals by prioritizing the goals with high TD estimation errors. **ALP-GMM** selects the goals based on the difference of cumulative episodic reward between the newest and oldest tasks using Gaussian mixture models. **VDS** proposes the goals that maximize the epistemic uncertainty of the action value function of the policy. **DHRL** constructs a graph between both levels of HRL, and proposes frontier goals when the random goals are easy to achieve. However, the original DHRL could not generate curriculum goals without the help of the environment. Thus we evaluated a variant of DHRL (DHRL+) with a modified frontier goal proposal module and architecture (Appendix D.3), in

Figure 4: **(Lower is better) Distance from the curriculum goals to the final goals (PointNMaze, PointSpiralMaze, and AntUMaze). In the ‘n-way’ environments with multiple goals, we provide \(l2\) distance between the agent and the final goal at the end of the episodes, since calculating the average distance from the curriculum goal to multiple final goals is not possible.**

Figure 5: **(Higher is better) Success rates of the results. The curves of baselines are not visible in some environments as they overlap each other at zero success rate. Shading indicates a standard deviation across 4 seeds.**

addition to the original DHRL. **SFL** constructs a graph based on successor features and proposes uncertainty-based curriculum goals. (refer to Appendix D for detailed implementations)

### Experimental Results

First, we visualize the quantitative results to show whether the proposed method successfully and simultaneously addresses the two key challenges: 1) specifying goal space from arbitrary observation space and 2) suggesting a well-calibrated curriculum to achieve the final goal. Figure 3 illustrates the curriculum goals and changes in discrete goal space (graph) of CQM as learning progresses. Each node in the graph consists of the decoded embedding vectors of VQ-VAE, and each edge represents reachability between the decoded embeddings. The graphs of CQM in the figure gradually expand towards unexplored areas as the learning progresses, since the calibrated curriculum goal induces the agent to explore the unexplored area. In the opposite direction as well, the capability of providing proper curriculum goals on arbitrary geometric structures is facilitated by a compact goal space that contains semantic information which enables estimating the uncertainty and temporal distance well. As a result, our method provides tailored curriculum guidance across the environments, while the baselines suffer from the absence of the manually specified goal space.

We also provide the quantitative results in Figures 4 and 5. Figure 4 indicates that the proposed method (CQM) can suggest a tailored sequence of goals that gradually converges to the final goal distributions while instructing the agent to achieve the increasingly difficult goals. Also, as shown in Figure 5, ours consistently outperforms both the prior curriculum method and graph-RL methods. It is noticeable that CQM is the only method that shows robust performance to the variation of the goal dimension, while other methods suffer from serious data inefficiency, especially in the tasks with higher-dimensional goal space (suffering more in Ant (29dims) compared to Point (6dims)).

Curriculum learning and planning in visual control tasks.To validate the performance of the RL agent and the quality of generated curriculum goals in higher dimensional tasks, We conducted two additional vision-based goal-reaching tasks. PointNMaze-Viz receives only ego-centric view images to reach the goal, while PointSpiralMaze-Viz receives bird's-eye view images. Figure 6 visualizes the curriculum goals in the order of the episodes, and how the agent utilizes the benefit of planning over the discrete goal space in order to achieve the curriculum goals. To achieve an image-based final goal (**Goal: 8**), the agent generates the sequence of images (**(1, 2, 3,..., 8)**) as waypoints, and tries to achieve the waypoints sequentially.

Interestingly, despite a significant increase in the observation dimension, CQM does not suffer from significant performance degradation in terms of data efficiency, which indicates that CQM effectively reduces the complexity of goal space by constructing a semantic goal space. We emphasize that the performance of our algorithm does not show significant differences between state-based and image-based environments (Compare PointNMaze in Figures 4 and 6). Another interesting point is that CQM can fully enjoy the advantage of planning over the discretized goal space, even in vision-based control tasks where the agent does not receive information about its global X-Y coordinates explicitly. These results validate that CQM possesses robust performance in terms of the dimensionality of the goal space, and the capability in extracting temporal relations between the discretized landmarks.

### Ablation Studies

Curriculum guidance.First of all, we examine how important curriculum guidance is for an agent to solve goal-conditioned tasks. As shown in Figure 7, when only the final goal is provided without a tailored curriculum (**-w/o Curriculum**), the RL agent has difficulty achieving the final goal directly.

Figure 6: Left: the distance from the agent to the final goals (**Lower is better**). Right: visualization of curriculum goals and waypoints of planning over the graph (CQM).

Furthermore, we found that providing curriculum guidance greatly affects the goal space specification module and the absence of a curriculum leads to the ill-formed discrete goal space that barely covers only the observations near the initial distribution. We provide these qualitative results in Figures 13, 14 (Appendix E).

Types of the discrete goal sampling method.The proposed method (CQM) can use two approaches to sample the landmark to form the discrete goal space as introduced in Section 4.1. The first approach is to decode the embedding vectors of the codebook \(l_{1:m}=(e_{1:m})\), and the other approach is to sample an observation batch from the replay buffer and pass it through VQ-VAE to quantize it (**-Landmark from Replay Buff.**). As shown in Figure 7, there is no significant difference between them in terms of data efficiency. However, in terms of the stability of learning, utilizing the decoded embeddings of VQ-VAE shows better performance in some environments.

Effect of the goal convergence method.To provide a final goal-directed exploration in addition to the naive curriculum toward the frontier areas, CQM includes a goal convergence module that guides the agent to practice the final goal after the agent sufficiently explored the environment (Section 4.3). Based on the KL divergence between the achieved goal distribution and the final goal distribution, CQM calculates the ratio of the mixture between the final goals and the frontier goals (the ratio of providing final goals as learning progresses is presented in Figure 11 in Appendix E). As shown in Figure 7, the absence of the final goal convergence method (**-w/o Goal Convergence**) results in unstable performance, since the agent repeatedly practices unexplored areas instead of converging towards the final goal even after the explored area "covers" the final goal distribution.

Effect of Graph Construction and Planning.Finally, we examine the effect of constructing graphs and planning on the performance of RL agents. As explained in section 4.2, CQM not only utilizes the decoded embedding vectors from VQ-VAE as a set of discretized observations but also forms a graph by capturing the temporal relations between the discrete observations. First, we evaluated CQM without graph (**-w/o Graph**), which does not construct a graph and measure the distance between the landmarks through naive temporal distance prediction based on Q values (\(\)), rather than the geodesic distance over the graph (\(^{}\)). Also, we evaluate CQM without planning (**-w/o Planning**) since ours can optionally utilize the benefit of planning and reason over long horizons using the graph. As shown in Figure 7, CQM shows better performance than both CQM without a graph and CQM without planning, especially in some long-horizon tasks (AntUMaze and PointSpiralMaze).

## 6 Conclusions

To solve the complex control tasks without the need for a manually designed semantic goal space, we propose to solve both issues of specifying the goal space and suggesting the curriculum goals to the agent. By constructing the quantized world model using the decoded embedding vectors of the discretization bottleneck and restoring the relations between these, CQM considers both the uncertainty and temporal distance and has the capability of suggesting calibrated curriculum goals to the agent. The experiments show that the proposed method significantly improves performance on various vision-based goal-reaching tasks as well as state-based tasks, preventing the performance drop in the absence of a manually specified goal space.

Figure 7: **(Lower is better) Ablation study: the distance from the agent to the final goals at the end of the episodes.**

Limitations and future works.While CQM shows great potential in addressing the limitations of previous studies, more research could further develop it. One area that could be explored is the use of reward-free curriculum learning methods, since CQM still requires minimal human efforts such as defining a success threshold to train agents. Also, this study only used single-code representations with VQ-VAE which would possess a limited capacity of representations, so expanding CQM to include multiple-code representations with discrete factorial representations could be an interesting future direction.

## 7 Acknowledgement

This work was supported by Korea Research Institute for defense Technology Planning and advancement (KRIT) Grant funded by Defense Acquisition Program Administration(DAPA) (No. KRIT-CT-23-003, Development of AI researchers based on deep reinforcement learning and establishment of virtual combat experiment environment)