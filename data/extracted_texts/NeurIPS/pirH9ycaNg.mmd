# Kernelized Reinforcement Learning with Order Optimal Regret Bounds

Sattar Vakili

MediaTek Research

Cambridge, UK

sattar.vakili@mtkresearch.com &Julia Olkhovskaya

TU Delft

Delft, the Netherlands

julia.olkhovskaya@gmail.com

Work was done when the author was affiliated with Vrije Universiteit Amsterdam.

###### Abstract

Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose \(\)-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by a reproducing kernel Hilbert space (RKHS). We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Matern kernels) the existing results lead to trivial (superlinear in the number of episodes) regret bounds. We show a sublinear regret bound that is order optimal in the case of Matern kernels where a lower bound on regret is known.

## 1 Introduction

Reinforcement learning (RL) in real world often has to deal with large state action spaces and complex unknown models. While RL policies using complex function approximations have been empirically effective in various fields including gaming (Silver et al., 2016; Lee et al., 2018; Vinyals et al., 2019), autonomous driving (Kahn et al., 2017), microchip design (Mirhoseini et al., 2021), robot control (Kalashnikov et al., 2018), and algorithm search (Fawzi et al., 2022), little is known about theoretical performance guarantees in such settings. The analysis of RL algorithms has predominantly focused on simpler cases such as tabular or linear Markov decision processes (MDPs). In a tabular setting, a regret bound of \(}(||T})\) has been shown for optimistic state-action value learning algorithms (e.g., see, Jin et al., 2018), where \(H\) is the length of episodes, \(T\) is the number of episodes, and \(\) and \(\) are finite state and action spaces. This bound does not scale well when the size of state-action space grows large. Furthermore, when the model (the state-action value function or the transitions) admits a \(d\)-dimensional linear representation in some state-action features, a regret bound of \(}(d^{3}T})\) is established (Jin et al., 2020), that scales with the dimension of the linear model rather than the cardinality of the state-action space.

Several recent studies have explored the utilization of complex models with large state-action spaces. A very general model entails representing the state-action value function using a reproducing kernel Hilbert space (RKHS). This approach allows using kernel ridge regression to obtain confidence intervals, which facilitate the design and analysis of RL algorithms. The most significant contributionto this general RL problem is Yang et al. (2020),2 that provides regret guarantees for an optimistic least-squares value iteration (LSVI) algorithm, referred to as kernel optimistic least-squares value iteration (KOVI). The main assumption is that the state-action value function can be represented using the RKHS of a known kernel \(k\). The regret bounds reported in Yang et al. (2020) scale as \(}(H^{2}()) \,(T)T})\), with \(=\), where \((T)\) and \(()\) are two kernel related complexity terms, respectively, referred to as maximum information gain and \(\)-covering number of the class of state-action value functions. The definitions are given in Section 4. Both complexity terms are determined using the spectrum of the kernel. While for smooth kernels, characterized by exponentially decaying Mercer eigenvalues, such as Squared Exponential kernel, \((T)\) and \(()\) are logarithmic in \(T\), for more general kernels with greater representation capacity, these terms may grow polynomially in \(T\), possibly making the regret bound trivial (superlinear).

To have a better understanding of the existing result, let \(\{_{m}>0\}_{m=1}^{}\) denote the Mercer eigenvalues of the kernel \(k\) in a decreasing order. Also, let \(\{_{m}\}_{m=1}^{}\) denote the corresponding eigenfeatures. Refer to Section 2.2 for details. The kernel \(k\) is said to have a polynomial eigendecay when \(_{m}\) decay at least as fast as \(m^{-p}\) for some \(p>1\). The polynomial eigendecay profile satisfies for many kernels of practical and theoretical interest such as Matern family of kernels (Borovitskiy et al., 2020) and the Neural Tangent (NT) kernel (Arora et al., 2019). For a Matern kernel with smoothness parameter \(\) on a \(d\)-dimensional domain, \(p=\)(e.g., see, Janz et al., 2020). For a NT kernel with \(s-1\) times differentiable activations, \(p=\)(Vakili et al., 2021). In Yang et al. (2020), the regret bound is specialized for the class of kernels with polynomially decaying eigenvalues, by bounding the complexity terms based on the kernel spectrum. However, the reported regret bound is sublinear in \(T\) only when the kernel eigenvalues decay very fast. In particular, let \(=p(1-2)\), where for \( 0\), \(_{m}^{}_{m}\) is uniformly bounded. Then, Yang et al. (2020), Corollary \(4.4\) reports a regret bound of \(}(T^{^{*}+^{*}+})\), with

\[^{*}=\{^{*},-1)}, -3}\},\ \ \ \ ^{*}=.\] (1)

The regret bound \(}(T^{^{*}+^{*}+})\) is sublinear only when \(p\) and \(\) are sufficiently large. That, at least, requires \(2^{*}<\), implying \(p>d+2\), when \(\) is also sufficiently large. For instance, for Matern kernels, this requirement can be expressed as \(>\), when \(\) is sufficiently large.

**Special case of bandits.** A similar issue existed in the simpler problem of kernelized bandits, corresponding to the special case where \(H=1,||=1\). Specifically, the \(}((T))\) regret bounds reported for optimistic sampling (Srinivas et al., 2010, GP-UCB), as well as for Thompson sampling (Chowdhury and Gopalan, 2017, GP-TS) are also trivial (superlinear) when \((T)\) grows faster than \(\). It remains an open problem whether the suboptimal performance guarantees for these two algorithms is a fundamental shortcoming or an artifact of the proof. This observation is formalized as an open problem on the online confidence intervals for RKHS elements in Vakili et al. (2021). For the kernelized bandits problem, Scarlett et al. (2017) proved lower bounds on regret in the case of Matern family of kernels. In particular, they proved an \((T^{})\) lower bound on regret of any bandit algorithm. Several recent algorithms, different from GP-UCB and GP-TS, have been developed to alleviate the suboptimal and superlinear regret bounds in kernelized bandits and obtain an \(}()\) regret bound (Li and Scarlett, 2022; Salgia et al., 2021), that matches the lower bound in the case of the Matern family of kernels, up to logarithmic factors. The _Sup_ variations of the UCB algorithms also obtain the optimal regret bound in the contextual kernel bandit setting with finite actions (Valko et al., 2013).

**Main contribution.** The RL setting presents a greater level of complexity compared to the bandit setting due to the Markovian dynamics. None of the solutions in Li and Scarlett (2022); Salgia et al. (2021); Valko et al. (2013) seem appropriate in the presence of MDP dynamics, thereby leaving the question of order optimal regret bounds largely open. In this work, we leverage the scaling of the kernel spectrum with the size of the domain to improve the regret bounds. We consider kernels with polynonial eigendecay on a hypersubical domain with side length \(\), where eigenvalues scale with \(^{}\) for some \(>0\). See Definition 1. This encompasses a large class of common kernels, including the Matern family, for which, \(=2\). The hypercube domain assumption is a technical formality that can be relaxed to other regular compact subsets of \(^{d}\). In Section 3, we propose a domain partitioning kernel ridge regression based least-squares value iteration policy (\(\)-KRVI) that achieves sublinear regret of \(}(H^{2}T^{})\) for kernels introduced in Definition 1. This is the first sublinear regret bound under such a general setting. Moreover, with Matern kernels, our regret bound matches the \((T^{})\) lower bound reported in in Scarlett et al. (2017) for the special case of kernelized bandits, up to a logarithmic factor.

Our proposed policy, \(\)-KRVI, is based on least-squares value iteration (similar to KOVI, Yang et al. (2020)). However, in order to effectively utilize the confidence intervals from kernel ridge regression, \(\)-KRVI creates a partitioning of the state-action domain and builds the confidence intervals only based on the observations within the same partition element. The domain partitioning allows us to leverage the scaling of the kernel eigenvalues with respect to the domain size. The inspiration for this idea is drawn from \(\)-GP-UCB algorithm introduced in Janz et al. (2020) for kernelized bandits. In comparison to Janz et al. (2020), \(\)-KRVI and its analysis present greater complexity due to the Markovian dynamics in the MDP setting. Furthermore, we provide a finer analysis that significantly improves the results compared to Janz et al. (2020). Although Janz et al. (2020) obtained sublinear regret guarantees of \(}(T^{})\) in the kernelized bandit setting with Matern kernel, there still remained a polynomial in \(T\) gap between their regret bounds and the lower bound reported in Scarlett et al. (2017). As a consequence of our results, we also close this gap.

There are several novel contributions in our analysis that lead to the improved and order optimal regret bounds. We establish confidence intervals for kernel ridge regression that apply uniformly to all functions in the state-action value function class (Theorem 1). A similar confidence interval was given in Yang et al. (2020). We however provide flexibility with respect to setting the parameters of the confidence interval, that eventually contributes to the improved regret bounds, with a proper choice of parameters. We also derive bounds on the maximum information gain (Lemma 2) and the function class covering number (Lemma 3), taking into consideration the size of the state-action domain. These bounds are important for the analysis of our domain partitioning policy which effectively controls the number of observations utilized in kernel ridge regression by partitioning the domain into subdomains of diminishing size. These intermediate results may also be of general interest in similar problems.

The \(\)-KRVI policy enjoys an efficient runtime, polynomial in \(T\), and linear in \(||\), similar to the runtime of KOVI (Yang et al., 2020). The dependency of the runtime on \(||\) limits the scope of the policy to finite \(\), while allowing a continuous \(\) (with \(||\) infinite). The assumption of finite \(\) can be relaxed, provided there is an efficient optimizer of a certain state-action value function. See the details in Section 3.2.

**Other related work.** There is an extensive literature on the analysis of RL policies which do not rely on a generative model or an exploratory behavioral policy. The literature has primarily focused on the tabular setting (Jin et al., 2018; Auer et al., 2008; Bartlett and Tewari, 2012). The domain of potential applications for this setting is very limited, as in many real world problems, the state-action space is very large or even infinite. In response to this, recent literature has placed a notable emphasis on employing function approximation in RL, particularly within the context of generalized linear settings. This approach involves representing the value function or transition model through a linear transformation to a well-defined feature mapping. Important contributions include the work of Jin et al. (2020); Yao et al. (2014), as well as subsequent studies by Russo (2019); Zanette et al. (2020, 2020); Neu and Pike-Burke (2020); Yang and Wang (2020). Furthermore, there have been several efforts to extend these techniques to a kernelized setting, as explored in Yang et al. (2020); Yang and Wang (2020); Chowdhury and Gopalan (2019); Yang et al. (2020); Domingues et al. (2021). These works are also inspired by methods originally designed for linear bandits (Abbasi-Yadkori et al., 2011; Agrawal and Goyal, 2013), as well as kernelized bandits (Srinivas et al., 2010; Valko et al., 2013; Chowdhury and Gopalan, 2017). However, all known regret bounds in the RL setting (Yang et al., 2020; Yang and Wang, 2020; Chowdhury and Gopalan, 2019; Yang et al., 2020; Domingues et al., 2021) are not order optimal. We compare our regret bounds with the state of the art reported in Yang et al. (2020). A similar issue existed for classic kernelized bandit algorithms. A detailed discussion can be found in Vakili et al. (2021). The authors in Yang and Wang (2020) considered finite state-actions under a kernelized MDP model where the transition model can be directly estimated. That is different from the setting considered in our work and Yang et al. (2020).

Preliminaries and Problem Formulation

In this section, we overview the background on episodic MDPs and kernel ridge regression.

### Episodic Markov Decision Processes

An episodic MDP can be described by the tuple \(M=(,,H,P,r)\), where \(\) is the state space, \(\) is the action space, the integer \(H\) is the length of each episode, \(r=\{r_{h}\}_{h=1}^{H}\) are the reward functions and \(P=\{P_{h}\}_{h=1}^{H}\) are the transition probability distributions.2 We use the notation \(=\) to denote the state-action space. For each \(h[H]\), the reward \(r_{h}:\) is the reward function at step \(h\), which is supposed to be deterministic for simplicity, and \(P_{h}(|s,a)\) is the transition probability distribution on \(\) for the next state from state-action pair \((s,a)\). The choice of deterministic rewards allows us to concentrate on the core complexities of the problem, and should not be regarded as a limitation. Both the framework and results can be readily extended to a setting with random rewards.

A policy \(=\{_{h}\}_{h=1}^{H}\), at each step \(h\), determines the (possibly random) action \(_{h}:\) taken by the agent at state \(s\). At the beginning of each episode \(t=1,2,\), the environment picks an arbitrary state \(s_{1}^{t}\). The agent determines a policy \(^{t}=\{_{h}^{t}\}_{h=1}^{H}\). Then, at each step \(h[H]\), the agent observes the state \(s_{h}^{t}\), picks an action \(a_{h}^{t}=_{h}^{t}(s_{h}^{t})\) and observes the reward \(r_{h}(s_{h}^{t},a_{h}^{t})\). The new state \(s_{h+1}^{t}\) then is drawn from the transition distribution \(P_{h}(|s_{h}^{t},a_{h}^{t})\). The episode ends when the agent receives the final reward \(r_{H}(s_{H}^{t},a_{H}^{t})\).

The goal is to find a policy \(\) that maximizes the expected total reward in the episode, starting at step \(h\), i.e., the value function defined as

\[V_{h}^{}(s)=[_{h^{}=h}^{H}r_{h^{}}(s_{h^{ }},a_{h^{}})s_{h}=s],\ \ \  s,h[H],\] (2)

where the expectation is taken with respect to the randomness in the trajectory \(\{(s_{h},a_{h})\}_{h=1}^{H}\) obtained by the policy \(\). It can be shown that under mild assumptions (e.g., continuity of \(P_{h}\), compactness of \(\), and boundedness of \(r\)) there exists an optimal policy \(^{*}\) which attains the maximum possible value of \(V_{h}^{}(s)\) at every step and at every state (e.g., see, Puterman, 2014). We use the notation \(V_{h}^{*}(s)=_{}V_{h}^{}(s),\  s,h[H]\). By definition \(V_{h}^{^{*}}=V_{h}^{*}\). For a value function \(V:[0,H]\), we define the following notation

\[[P_{h}V](s,a):=_{s^{} P_{h}(|s,a)}[V(s^{})].\] (3)

We also define the state-action value function \(Q_{h}^{}:[0,H]\) as follows.

\[Q_{h}^{}(s,a)=_{}[_{h^{}=h}^{H}r_{h^{}}(s _{h^{}},a_{h^{}})s_{h}=s,a_{h}=a],\] (4)

where the expectation is taken with respect to the randomness in the trajectory \(\{(s_{h},a_{h})\}_{h=1}^{H}\) obtained by the policy \(\). The Bellman equation associated with a policy \(\) then is represented as

\[Q_{h}^{}(s,a)=r_{h}(s,a)+[P_{h}V_{h+1}^{}](s,a),\ \ \ V_{h}^{}(s)= _{}[Q_{h}^{}(s,_{h}(s))],\ \ \ V_{H+1}^{}:=0,\] (5)

where the expectation is taken with respect to the randomness in the policy \(\). The Bellman optimality equation is also given as \(Q_{h}^{}(s,a)=r_{h}(s,a)+[P_{h}V_{h+1}^{}](s,a),V_{h}^{}(s)=_ {a}Q_{h}^{}(s,a)\), \(V_{H+1}^{}:=\ 0\). The performance of a policy \(^{t}\) is measured in terms of the loss in the value function, referred to as _regret_, denoted by \((T)\) in the following definition

\[(T)=_{t=1}^{T}(V_{1}^{}(s_{1}^{t})-V_{1}^{^{t}}(s_{1}^{t })).\] (6)

Recall that \(^{t}\) is the policy executed by the agent at episode \(t\), where \(s_{1}^{t}\) is the initial state in that episode determined by the environment.

### Kernel Ridge Regression

We assume that the state-action value functions belong to a known reproducing kernel Hilbert space (RKHS). See Assumption 1 and Lemma 1 for the formal statement. This is a very general assumption, considering that the RKHS of common kernels can approximate almost all continuous functions on the compact subsets of \(^{d}\)(Srinivas et al., 2010). Consider a positive definite kernel \(k:\). Let \(_{k}\) be the RKHS induced by \(k\), where \(_{k}\) contains a family of functions defined on \(\). Let \(,_{_{k}}:_{k} _{k}\) and \(\|\|_{_{k}}:_{k}\) denote the inner product and the norm of \(_{k}\), respectively. The reproducing property implies that for all \(f_{k}\), and \(z\), \( f,K(,z)_{_{k}}=f(z)\). Without loss of generality, we assume \(k(z,z) 1\) for all \(z\). Mercer theorem implies, under certain mild conditions, \(k\) can be represented using an infinite dimensional feature map:

\[k(z,z^{})=_{m=1}^{}_{m}_{m}(z)_{m}(z^{}),\] (7)

where \(_{m}>0\), and \(}_{m}_{k}\) form an orthonormal basis of \(_{k}\). In particular, any \(f_{k}\) can be represented using this basis and wights \(w_{m}\) as

\[f=_{m=1}^{}w_{m}}_{m},\] (8)

where \(\|f\|_{_{k}}^{2}=_{m=1}^{}w_{m}^{2}\). A formal statement and the details are provided in Appendix A. We refer to \(_{m}\) and \(_{m}\) as (Mercer) eigenvalues and eigenfeatures of \(k\), respectively.

Kernel-based models provide powerful predictors and uncertainty estimators which can be leveraged to guide the RL algorithm. In particular, consider a fixed unknown function \(f_{k}\). Consider a set \(Z^{t}=\{z^{t}\}_{i=1}^{t}\) of \(t\) inputs. Assume \(t\) noisy observations \(\{Y(z^{i})=f(z^{i})+^{i}\}_{i=1}^{t}\) are provided, where \(^{i}\) are independent zero mean noise terms. Kernel ridge regression provides the following predictor and uncertainty estimate, respectively (see, e.g., Scholkopf et al., 2002),

\[^{t,f}(z) = k_{Z^{t}}^{}(z)(K_{Z^{t}}+^{2}I^{t})^{-1}Y_{Z^{t}},\] \[(b^{t}(z))^{2} = k(z,z)-k_{Z^{t}}^{}(z)(K_{Z^{t}}+^{2}I)^{-1}k_{Z^{t}} (z),\] (9)

where \(k_{Z^{t}}(z)=[k(z,z^{1}),,k(z,z^{t})]^{}\) is a \(t 1\) vector of the kernel values between \(z\) and observations, \(K_{Z^{t}}=[k(z^{i},z^{j})]_{i,j=1}^{t}\) is the \(t t\) kernel matrix, \(Y_{Z^{t}}=[Y(z^{1}),,Y(Z^{t})]^{}\) is the \(t 1\) observation vector, \(I\) is the identity matrix of dimensions \(t\), and \(>0\) is a free regularization parameter. The predictor and uncertainty estimate could be interpreted as posterior mean and variance of a surrogate centered Gaussian process (GP) model with covariance \(k\), and zero mean Gaussian noise with variance \(^{2}\)(e.g., see, Williams and Rasmussen, 2006).

### Technical Assumption

We assume that the reward functions \(\{r_{h}\}_{h=1}^{H}\) and the transition probability distributions \(P_{h}(s^{}|,)\) belong to the \(1\)-ball of the RKHS. We use the notation \(_{k,R}=\{f:\|f\|_{_{k}} R\}\) to denote the \(R\)-ball of the RKHS.

**Assumption 1**: _We assume_

\[r_{h}(,),P_{h}(s^{}|,)_{k,1},  h[H],\; s^{}.\] (10)

This is a mild assumption considering the generality of RKHSs, that is also supposed to hold in Yang et al. (2020). Similar assumptions are made in linear MDPs which are significantly more restrictive (e.g., see, Jin et al., 2020).

An immediate consequence of Assumption 1 is that for any integrable \(V:[0,H]\), \(r_{h}+[P_{h}V_{h+1}]_{k,H+1}\). This is formalized in the following lemma.

**Lemma 1**: _Consider any integrable \(V:[0,H]\). Under Assumption 1, we have_

\[r_{h}+[P_{h}V]_{k,H+1}.\] (11)

See (Yeh et al., 2023, Lemma 3) for a proof.

[MISSING_PAGE_FAIL:6]

\([r_{h}(z^{})+V_{h+1}^{t}(s^{}_{h+1})]_{z^{} Z^{t}_{h}(z)}^{ }\), where \(s^{}_{h+1}\) is drawn from the transition distribution \(P_{h}(|z^{})\), denotes the observation values for the observation points \(z^{} Z^{t}_{h}(z)\). The vectors \(k_{Z^{t}_{h}(z)}\) and \(Y_{Z^{t}_{h}(z)}\) are \(N_{h}^{t-1}(^{t}_{h}(z))\) dimensional column vectors, and \(K_{Z^{t}_{h}(z)}\) and \(I\) are \(N_{h}^{t-1}(^{t}_{h}(z)) N_{h}^{t-1}(^{t}_{h}(z))\) dimensional matrices.

The exploration bonus is determined based on the uncertainty estimate of the kernel ridge regression model on cover elements defined as

\[b^{t}_{h}(z)=(k(z,z)-k_{Z^{t}_{h}(z)}^{}(z)(K_{Z^{t}_{h}(z)}+^{ 2}I)^{-1}k_{Z^{t}_{h}(z)}(z))^{}.\] (15)

The policy \(\)-KRVI then is the greedy policy with respect to

\[Q^{t}_{h}(z)=\{^{t}_{h}(z)+_{T}()b^{t}_{h}(z),H-h+1\}.\] (16)

Specifically, at step \(h\) of episode \(t\), the following action is chosen, after observing \(s^{t}_{h}\),

\[a^{t}_{h}=*{arg\,max}_{a}Q^{t}_{h}(s^{t}_{h},a).\] (17)

A pseudocode is provided in Algorithm 1.

```
1:Input: \(\), \(_{T}()\), \(k\), \(M=(,,H,P,r)\).
2:For all \(h[H]\), let \(^{1}_{h}=\{^{d}\}\).
3:for Episode \(t=1,2,,T\),do
4: Receive the initial state \(s^{t}_{1}\).
5: Set \(V^{t}_{H+1}(s)=0\), for all \(s\).
6:for step \(h=H,,1\)do
7: Obtain value functions \(Q^{t}_{h}(z)\) as in (16).
8:endfor
9:for step \(h=1,2,,H\)do
10: Take action \(a^{t}_{h}*{arg\,max}_{a}Q^{t}_{h}(s^{t}_{h},a)\).
11: Observe the reward \(r_{h}(s^{t}_{h},a^{t}_{h})\) and the next state \(s^{t}_{h+1}\).
12: Split any element \(^{}^{t-1}_{h}\), for which \(_{^{}}^{-}<|N^{t}_{h}(^{})|+1\) along the middle of each side, and obtain \(^{t}_{h}\).
13:endfor
14:endfor ```

**Algorithm 1** The \(\)-KRVI Policy.

The predictor \(^{t}_{h}\), the confidence interval width multiplier \(_{T}()\) and the exploration bonus \(b^{t}_{h}\) are all designed using kernel ridge regression limited to the observations within cover elements given above. The parameter \(_{T}()\), in particular, is designed in a way that \(Q^{t}_{h}\) is a \(1-\) upper confidence bound on \(r_{h}+[P_{h}V^{t}_{h+1}]\). Using Theorem 1 on the confidence intervals, we show that a choice of \(_{T}()=(H)})\) satisfies this requirement.

Figure 1 demonstrates the domain partitioning used in \(\)-KRVI on a \(2\)-dimensional domain. The colors represent the value of the target function. The observation points are expected to concentrate around the areas where the target function has a high value. As a result the domain is partitioned to smaller squares in that region.

**Runtime complexity.** The \(\)-KRVI policy is also runtime efficient with a polynomial runtime complexity. In particular, an upper bound on the runtime of \(\)-KRVI is \((HT^{4}+H||T^{3})\), that is similar to KOVI (Yang et al., 2020). However, analogous to (Janz et al., 2020), we expect an improved runtime for \(\)-KRVI in practice. In addition, the runtime can further improve in terms of \(T\), utilizing sparse approximations of kernel ridge predictor and uncertainty estimate (e.g., see, Vakili et al., 2022). The dependency of the runtime on \(||\) is due to the step given in Equation (17). If this optimization can be done efficiently over continuous domains, \(\)-KRVI (also KOVI) could handle infinite number of actions. The assumption that the upper confidence bound index can be efficiently optimized over continuous domains is often made in the kernelized bandits (e.g., see, Srinivas et al., 2010).

## 4 Main Results and Regret Analysis

In this section, we present our main results. In Theorem 2, we establish an \(}(H^{2}T^{})\) regret bound for \(\)-KRVI, for the class of kernels with polynomial eigendecay. We first prove bounds on maximum information gain and covering number of state-action value function class. Those enable us to present our uniform confidence interval for state-action value functions (Theorem 1), and subsequently the regret bound (Theorem 2).

**Definition 1** (Polynomial Eigendecay): _Consider the Mercer eigenvalues \(\{_{m}\}_{m=1}^{}\) of \(k:\), given in Equation (7), in a decreasing order, as well as the corresponding eigentures \(\{_{m}\}_{m=1}^{}\). Assume \(\) is a \(d\)-dimensional hypercube with side length \(_{}\). For some \(C_{p},>0,p>1\), the kernel \(k\) is said to have a polynomial eigendecay, if for all \(m\), \(_{m} C_{p}m^{-p}_{}^{}\). In addition, for some \( 0\), \(m^{-p}_{m}(z)\) is uniformly bounded over all \(m\) and \(z\). We use the notation \(=p(1-2)\)._

The polynomial eigendecay profile encompasses a large class of common kernels, e.g., the Matern family of kernels. For a Matern kernel with smoothness parameter \(\), \(p=\) and \(=2\) (e.g., see, Jamz et al., 2020). Another example is the NT kernel (Arora et al., 2019). It has been shown that the RKHS of the NT kernel, when the activations are \(s-1\) times differentiable, is equivalent to the RKHS of a Matern kernel with smoothness \(=s-\)(Vakili et al., 2021b). For instance, the RKHS of an NT kernel with ReLU activations is equivalent to the RKHS of a Matern kernel with \(=\) (also known as the Laplace kernel). In this case, \(p=1+\) and \(=1\). The hypercube domain assumption is a technical formality that can be relaxed to other regular compact subsets of \(^{d}\). The uniform boundedness of \(m^{-p}_{m}(z)\) for some \(>0\), also holds for a broad class of kernels, including the Matern family, as discussed in (Yang et al., 2020). Several works including (Vakili et al., 2021b; Kassraie and Krause, 2022), have employed an averaging technique over subsets of eigenfeatures, demonstrating that, for the bounds on information gain, the effective value of \(\) can be considered as \(0\) in the case of Matern and NT kernels.

### Confidence Intervals for State-Action Value Functions

Confidence intervals are an important building block in the design and analysis of bandit and RL algorithms. For a fixed function \(f\) in the RKHS of a known kernel, \(1-\) confidence intervals of the form \(|f(z)-^{t,f}(z)|()b^{t}(z)\) are established in several works (Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Abbasi-Yadkori, 2013; Vakili et al., 2021) under various assumptions. In our setting of interest, however, these confidence intervals cannot be directly applied. This is due to the randomness of the target function itself. Specifically, in our case, the target function is \(r_{h}+[P_{h}V_{h+1}^{t}]\), which is not a fixed function due to the temporal dependence within an episode. An argument based on the covering number of the state-action value function class was used in Yang et al. (2020) to establish uniform confidence intervals over all \(z\) and all \(f\) in a specific function class. In Theorem 1, we prove a different confidence interval that offers flexibility with respect to setting the parameters of the confidence interval. Our approach leads to a more refined confidence interval,

Figure 1: A \(2\)-dimensional domain partitioned into smaller squares.

which, with a proper choice of parameters, contributes to the improved regret bound achieved by our policy.

We first give a formal definition of the two complexity terms: maximum information gain and the covering number of the state-action value function class, which appear in our confidence intervals.

**Definition 2** (Maximum Information Gain): _In the kernel ridge regression setting described in Section 2.2, the following quantity is referred to as maximum information gain: \(_{k,}(t)=_{Z^{t}}(I +}K_{Z^{t}})\)._

Upper bounds on maximum information gain based on the spectrum of the kernel are established in Janz et al. (2020); Srinivas et al. (2010); Vakili et al. (2021). Maximum information gain is closely related to the _effective_ dimension of the kernel. While the feature representation of common kernels is infinite dimensional, with a finite observation set, only a finite number of features have a significant impact on kernel ridge regression, that is referred to as the effective dimension. It has been shown that information gain and effective dimension are the same up to logarithmic factors (Calandriello et al., 2019). This observation offers an intuitive understanding of information gain.

**State-action value function class:** Let us use \(_{k,h}(R,B)\) to denote the class of state-action value functions. In particular for a set of observations \(Z\), let \(b_{h}(z)\) be the uncertainty estimate obtained from kernel ridge regression as given in (9). We define

\[_{k,h}(R,B)=Q:Q(z)=\{Q_{0}(z)+ b_{h}(z),\;H-h +1\},\;\|Q_{0}\|_{_{k}} R, B,|Z| T}.\] (18)

**Definition 3** (Covering Set and Number): _Consider a function class \(\). For \(>0\), we define the minimum \(\)-covering set \(()\) as the smallest subset of \(\) that covers it up to an \(\) error in \(l_{}\) norm. That is to say, for all \(f\), there exists a \(g()\), such that \(\|f-g\|_{l_{}}\). We refer to the size of \(()\) as the \(\)-covering number._

We use the notation \(_{k,h}(;R,B)\) to denote the \(\)-covering number of \(_{k,h}(R,B)\), that appears in the confidence interval.

In Lemmas 2 and 3, we establish bounds on \(_{k,}(t)\) and \(_{k,h}(;R,B)\), respectively.

**Lemma 2** (Maximum information gain): _Consider a positive definite kernel \(k:\), with polynomial eigendecay on a hypercube with side length \(_{}\). The maximum information gain given in Definition 2 satisfies_

\[_{k,}(T)=(T^{}((T))^{1-}_{}^{}).\]

**Lemma 3** (Covering Number of \(_{k,h}(R,B)\)): _Recall the class of state-action value functions \(_{k,h}(R,B)\), where \(k:\) satisfies the polynomial eigendecay on a hypercube with side length \(_{}\). We have_

\[_{k,h}(;R,B)=((_{ }^{}}{^{2}})^{}(1+( ))+(_{}^{} }{^{2}})^{}(1+( ))).\]

Our bound on maximum information gain is stronger than the ones presented in Yang et al. (2020); Janz et al. (2020); Srinivas et al. (2010) and is similar to the one given in Vakili et al. (2021), in terms of dependency on \(T\). Our bound on function class covering number is similar to the one given in Yang et al. (2020), in terms of dependency on \(T\). Both Lemmas 2 and 3 given in this work are, however, novel in terms of dependency on the domain size \(_{}\), and are required for the analysis of our domain partitioning algorithm.

We next present the confidence interval. Proofs are given in the appendix.

**Theorem 1** (Confidence Interval): _Let \(_{h}^{t}\) and \(b_{h}^{t}\) denote the kernel ridge predictor and uncertainty estimate of \(r_{h}+[P_{h}V_{h+1}^{t}]\), using \(t\) observations \([V_{h+1}^{t}(s_{h+1}^{r})]_{=1}^{t}\) at \(Z_{h}^{t}=\{z_{h}^{}\}_{=1}^{t}\), where \(s_{h+1}^{r}\) is the next state drawn from \(P_{h}(|z_{}^{})\). Let \(R=2H(T)}\). For \(,(0,1)\), with probability, at least \(1-\), we have, \( z,h[H]\) and \(t[T]\),_

\[|r_{h}(z)+[P_{h}V_{h+1}^{t}](z)-_{h}^{t}(z)|_{h}^{t}( ,)b_{h}^{t}(z)+,\]_where \(_{h}^{t}(,)\) is set to any value satisfying_

\[_{h}^{t}(,) H+1+}(t)+_{k,h}(;R_{T},_{h}^{t}(,))+1+ ()}+}{}.\] (19)

### Regret of \(\)-KRvI

A key step in the analysis of \(\)-KRVI is to apply the confidence interval in Theorem 1 to a subdomain \(^{}_{h}^{t}\). By design of the splitting rule, we can prove that the maximum information gain corresponding to \(^{}\) satisfies \(_{k,}(N_{h}^{T}(^{}))=((T))\). In addition, we choose \(=)}}{^{t}( ^{})}}\), when applying the confidence interval at step \(h\) of episode \(t\) on this subdomain. That ensures \(_{k,h}(;R_{N_{h}^{T}(^{})},_{h}^ {t}(,))=((T))\). From these, and by applying a probability union bound over all subdomains \(^{}\) created in \(\)-KRVI, we can deduce that the choice of \(_{T}()=(H)})\) with a sufficiently large constant, satisfies the requirements for confidence interval widths based on Theorem 1. The details are provided in the proof of Theorem 2 in Appendix D. Then, using standard tools from the analysis of optimistic LSVI algorithms, we arrive at the following regret bound.

**Theorem 2** (Regret of \(\)-KRvI): _Consider the \(\)-KRVI policy described in Section 3.2, with \(_{T}()=(H)})\) with a sufficiently large constant implied in the \(\) notation. Under Assumption 1, for kernels given in Definition 1, with probability at least \(1-\), the regret of \(\)-KRVI satisfies_

\[(T)=(H^{2}T^{}(T) )}).\] (20)

The regret bound of \(\)-KRVI presented in Theorem 2 is sublinear in \(T\) when \(>0\), in contrast to the state of the art regret bound in Yang et al. (2020). The \(\) notation used in the expression above hides constants that depend on \(p,\) and \(d\). See Appendix D for more details. When specialized to the Matern family of kernels, replacing \(p=\) and \(=2\), the regret bound becomes

\[(T)=(H^{2}T^{}(T))}).\] (21)

In terms of \(T\) scaling, this matches the lower bound for the special case of kernelized bandits (Scarlett et al., 2017), up to a \((T)\) factor.

## 5 Conclusion

The analysis of RL algorithms has predominantly focused on simple settings such as tabular or linear MDPs. Several recent studies have considered more general models, including representing the state-action value functions using RKHSs. Notably, the work in Yang et al. (2020) derives regret bounds for an optimistic LSVI policy. However, the regret bounds in Yang et al. (2020) are sublinear only when the eigenvalues of the kernel decay rapidly. In this work, we leveraged a domain partitioning technique, a uniform confidence interval for state-action value functions, and bounds on complexity terms based on the domain size to propose \(\)-KRVI, which attains a sublinear regret bound for a general class of kernels. Moreover, our regret bounds match the lower bound derived for Matern kernels in the special case of kernelized bandits, up to logarithmic factors. It remains an open problem whether the suboptimal regret bounds in the case of standard optimistic LSVI policies (such as KOVI, Yang et al., 2020) represent a fundamental shortcoming or an artifact of the proof.

## Funding Disclosure

This work was funded by MediaTek Research.