# APEBench: A Benchmark for Autoregressive Neural Emulators of PDEs

Felix Koehler

Technical University of Munich

Munich Center for Machine Learning

f.koehler@tum.de

&Simon Niedermayr

Technical University of Munich

simon.niedermayr@tum.de

&Rudiger Westermann

Technical University of Munich

westernn@tum.de

&Nils Thuerey

Technical University of Munich

nils.thuerey@tum.de

###### Abstract

We introduce the **A**utoregressive **P**DE **E**mulator Benchmark (APEBench), a comprehensive benchmark suite to evaluate autoregressive neural emulators for solving partial differential equations. APEBench is based on JAX and provides a seamlessly integrated differentiable simulation framework employing efficient pseudo-spectral methods, enabling 46 distinct PDEs across 1D, 2D, and 3D. Facilitating systematic analysis and comparison of learned emulators, we propose a novel taxonomy for unrolled training and introduce a unique identifier for PDE dynamics that directly relates to the stability criteria of classical numerical methods. APEBench enables the evaluation of diverse neural architectures, and unlike existing benchmarks, its tight integration of the solver enables support for differentiable physics training and neural-hybrid emulators. Moreover, APEBench emphasizes rollout metrics to understand temporal generalization, providing insights into the long-term behavior of emulating PDE dynamics. In several experiments, we highlight the similarities between neural emulators and numerical simulators. The code is available at https://github.com/tum-pbs/apebench and APEBench can be installed via pip install apebench.

## 1 Introduction

The language of nature is written in partial differential equations (PDEs). From the behavior of subatomic particles to the earth's climate, PDEs are used to model phenomena across all scales. Typically, approximate PDE solutions are computed with numerical simulations. Almost all relevant simulation techniques stem from the _consistent_ discretization involving symbolic manipulations of the differential equations into a discrete computer program. This laborious task yields algorithms that converge to the continuous dynamics for fine resolutions. For realistic models, established techniques require immense computational resources to attain high accuracy. Recent advances of machine learning-based _emulators_ challenge this. Purely data-driven or with little additional constraints and symmetries, neural networks can surpass traditional methods in the accuracy-speed tradeoff (Kochkov et al., 2021; List et al., 2022; Lam et al., 2022).

The field of neural PDE solvers advanced rapidly over the past years, applying convolutional architectures (Tompson et al., 2017; Thuerey et al., 2020; Um et al., 2020), graph convolutions (Pfaff et al., 2021; Brandstetter et al., 2022), spectral convolutions (Li et al., 2021), or mesh-free approaches (Ummenhofer et al., 2020; Wessels et al., 2020) to replace or enhance classical numerical solvers. However, the relationship between classical solvers, which supply training data, and neural emulators,which attempt to emulate their behavior, is often underexplored. For example, convolutional networks bear a strong resemblance to finite difference methods, and spectral networks can be thought of as pseudo-spectral techniques (McCabe et al., 2023). These parallels suggest that a better understanding of this interplay could help inform how emulator architectures are designed and how effectively neural emulators can learn from classical solvers.

To help address these questions, we introduce APEBench, a new benchmark suite designed to complement existing efforts in evaluating autoregressive neural emulators for time-dependent PDEs. While previous benchmarks such as PDEBench (Takamoto et al., 2022) and PDEArena (Gupta and Brandstetter, 2023) have provided valuable insights into architectural comparisons based on fixed datasets, APEBench aims to extend these efforts by focusing on emulator-simulator interaction via supporting neural-hybrid approaches and emphasizing training using differentiable physics. Additionally, we place particular focus on unrolled training and rollout metrics, which have been less systematically explored in other benchmarks.

The key innovation of APEBench lies in its tight integration of a highly efficient pseudo-spectral solver. This method is used both for procedural data generation and as a differentiable solver the networks can dynamically interact with during training. APEBench offers four key contributions:

* **Large Selection of Dynamics**: The benchmark offers a wide array of 46 PDE dynamics that allow for drawing analogies between classical and learned approaches.
* **Unique Dynamics Identifier**: For each distinct type of dynamics, APEBench provides a unique set of identifiers that encodes its difficulty of emulation.
* **Differentiable Simulation Suite**: We provide a novel (differentiable) JAX-based simulation framework employing efficient pseudo-spectral methods, which seamlessly integrates into emulator training and serves as a fast data generator.
* **Taxonomy and Metrics for Unrolling Methodologies**: We propose a broad and systematic framework for analyzing the impact of different training paradigms on emulator performance.

Our benchmark includes recipes that rapidly adapt to new architectures and training methodologies. Datasets are re-generated procedurally (and deterministically) in seconds on modern hardware. This avoids the need to distribute fixed datasets, improving adoption, and allows for quick modification of the underlying phyiscs. For visual analysis of the emergent structures, the benchmark is accompanied by a fast volume visualization module. This module seamlessly interfaces with the PDE dynamics to provide immediate feedback for emulator development in 2D and 3D.

Figure 1: APEBench provides an efficient pseudo-spectral solver to simulate \(46\) PDE dynamics across one to three spatial dimensions. Shown are examples visualized with APEBench’s custom volume renderer.

From Classical Numerics to Learned Emulation

We first discuss a motivational example that illustrates several key aspects of APEBench, namely **rollout metrics**, **training methodologies**, and **PDE identifiers**. We choose the simple case of one-dimensional advection with periodic boundary conditions and velocity \(c\),

\[_{t}u+c_{x}u=0 u(t,x=0)=u(t,x=L),\]

which admits an _analytical solution_ where the initial condition moves with \(c\) over the domain \(=(0,L)\). Let \(_{h}\) represent a discrete analytical time stepper that operates on a fixed resolution with \(N\) equidistantly spaced degrees of freedom and time step size \( t\). This _simulator_ advances a discrete state \(u_{h}^{N}\) to a future time, i.e., \(u_{h}^{[t+1]}=_{h}(u_{h}^{[t]})\). Emulator learning is the task of approximating this time stepper using a neural network \(f_{}_{h}\).

The simplest possible network is a linear convolution with a kernel size of two as

\[f_{}(u_{h})=w_{} u_{h},\]

where \(\) denotes cross-correlation. We frame finding \(w_{}=[_{},_{}]^{T}^{2}\) to approximate \(_{h}\) with \(f_{}\) as a data-driven learning problem, using trajectories produced by the analytical time stepper. If the neural emulator predicts one step into the future, the learning problem over the two-dimensional parameter space \(^{2}\) becomes _convex_. Since even-sized convolutions are typically biased to the right, one could suspect that the learned minimum of such a problem is given by the first-order upwind (FOU) method. This numerical (non-analytical) time stepper is found via a _consistent_ approach to discretizing the advection equation using a Taylor series. If we assume \(c<0\), it leads to \(_{}=1+c\) and \(_{}=-c\). However, despite convexity the learned solution is different. In Figure 2, we benchmark the long-term performance of the learned emulator relative to the FOU scheme. It is superior to the numerical method, with lower errors for the first 13 steps. Eventually, it diverges because it is _not consistent_. We can improve the long-term performance of the emulator by training it to predict multiple steps autoregressively. We call this approach _unrolled_ training in the following; it more closely aligns with the inference task of long-term accuracy. Indeed, doing so enhances the performance for a small sacrifice in short-term accuracy. The learned emulator improves in _temporal generalization_, i.e., it runs stably and accurately for more time steps. For example, when unrolling for 20 steps during training, the learned solution still performs better after 30 steps while having an 11% increased error at the first step. In the two-dimensional parameter space, more unrolling moves the learned stencil closer to the FOU scheme. The distance reduces from \(0.034\) to \(0.024\) to \(0.01\) for 1-step, 10-step, and 50-step training, respectively. This indicates that unrolling successfully injects knowledge about becoming a good simulator.

The FOU stencil depends only on the Courant-Friedrichs-Lewy (CFL) number \(c\). It represents a way to assess the _difficulty_ of the advection problem. With APEBench, we generalize it and compute similar stability numbers as intuitive identifiers for all available dynamics. By doing so, we obtain a minimal set of information to describe an experiment, which serves as an exchange protocol in our benchmark suite.

Our motivational example reveals that even combining simple (linear) PDEs and linear emulators leads to non-trivial learning tasks offering interesting insights. We also see that the emulators share similarities with classical numerical methods. In this case, there is a strong relation between convolutional networks and finite difference methods. However, since the emulator's free parameters result from a data-driven optimization problem, not a human-powered symbolic manipulation, they may deviate from the strict assumptions underlying traditional schemes. Learned emulators can use

Figure 2: Test rollout performance of linear convolution emulators when learned with different training rollout lengths relative to the performance of a FOU method. All learned emulators surpass the numerical method in an initial operating regime. More unrolling improves long-term accuracy for a small sacrifice in short-term performance.

this to their advantage, and outperform their numerical counterparts for a specific operating regime, i.e., a certain test rollout length. Since this superiority varies with unrolled training steps and is certainly dependent on the underlying dynamics (in terms of its _difficulty_), we emphasize that using rollout metrics is important to understand the temporal behavior of neural emulators. In summary, APEBench provides a suite that holistically assesses all ingredients of the emulator learning pipeline, including a highly accurate solver.

## 3 Related Work

Neural PDE SolversEarly efforts in neural PDE solvers focused on learning the continuous solution function for an (initial-)boundary value problem via a coordinate network (Dissanayake and Phan-Thien, 1994; Lagaris et al., 1998). With the rise of automatic differentiation capabilities in modern machine learning frameworks, this approach experienced a resurgence under the name of _Physics-Informed Neural Networks_ (PINNs) (Raissi et al., 2019). However, PINNs do not use autoregressive inference. Early works on stationary neural emulators include solutions to the pressure Poisson equation (Tompson et al., 2017) and fluid simulations (Thuerey et al., 2020). Notable works employing the autoregressive paradigm are Brandstetter et al. (2022) using supervised data from numerical simulators. Successful unsupervised approaches for autoregressive emulators are Geneva and Zabaras (2020) and Wandel et al. (2021). Seminal works highlighting the supremacy of neural-hybrid emulators are Um et al. (2020) and Kochkov et al. (2021). Oftentimes, the architectures employed are inspired by image-to-image tasks in computer vision. A closely-related line of work utilizes _neural operators_(Kovachki et al., 2023) which impose stronger mathematical requirements on emulators and their architectures, such as representation-equivariance (Bartolucci et al., 2023; Raonic et al., 2023). The most popular operator architecture in autoregressive settings is the Fourier Neural Operator (FNO) (Li et al., 2021) with numerous modifications and improvements existing (Tran et al., 2023; McCabe et al., 2023). Like implicit methods for numerical simulators, autoregressive models can have internal iterations. For example, this includes autoregressive diffusion models (Kohl et al., 2023) or iterative refinements (Lippe et al., 2023).

Neural Emulator Benchmarks and DatasetsNotable benchmark papers comparing emulator architectures are PDEBench (Takamoto et al., 2022) and PDEArena (Gupta and Brandstetter, 2023). Benchmarks based on more complicated fluid simulations include Luo et al. (2023), Bonnet et al. (2022), and Janny et al. (2023). BubbleML (Hassan et al., 2023) focuses on two-phase boiling problems. All the aforementioned benchmark papers release fixed and pre-computed datasets. APEBench is the first to tightly integrate an efficient reference solver, which procedurally generates all data. Moreover, we thereby uniquely enable benchmarking approaches involving differentiable physics, e.g., neural-hybrid emulators under unrolled training.

Another notable benchmark for ordinary differential equations and deterministic chaos is Gilpin (2021). This benchmark shares our goal of relating the (temporal) performance of emulators with characteristic properties of the underlying dynamics, focusing on ODEs instead of PDEs.

Beyond works dedicated to benchmarks, several datasets from seminal papers gained popularity in the community. This includes data on the Burgers equation, Kolmogorov flow and Darcy problem used in the FNO paper (Li et al., 2021). Also, the Kolmogorov trajectories of Kochkov et al. (2021) are widely used. Other examples include the datasets of the DeepONet paper (Lu et al., 2021), also used as part of the DeepXDE library (Lu et al., 2021).

Data Generators and Differentiable PhysicsPhysics-based deep learning often utilizes simple simulation suites with high-level interfaces like JAX-CFD (Kochkov et al., 2021), PhiFlow (Holl et al., 2020), JAX-MD (Schoenholz and Cubuk, 2020) or warp (Macllin, 2022). Our reference solver is based on Fourier pseudo-spectral ETDRK methods for which there is currently no equally comprehensive package available in the Python deep learning ecosystem. For the problems that fit into the method's constraints, it is one of the most efficient approaches (Montanelli and Bootland, 2020). Writing numerical solvers in deep learning frameworks provides discrete differentiability, beneficially used in recent research (Um et al., 2020; Kochkov et al., 2021). Existing non-differentiable simulation software typically serves as reference generators for purely data-driven approaches. Popular examples are the Dedalus library in Python (Burns et al., 2020) and the Chebfun package in MATLAB (Driscoll et al., 2014). Fluid related work often employs OpenFoam (Weller et al., 1998).

## 4 Components of the APEBench benchmark

APEBench provides a wide array of PDE dynamics that, among others, allow studying different architectures, training methodologies, dataset sizes, and evaluation metrics. Below, we describe its design and capabilities, particularly the choice of numerical reference solver.

Differentiable ETDRK Solver SuiteWe focus on semi-linear PDEs

\[_{t}u=u+(u),\]

where the linear differential operator \(\) contains a higher order derivative than the non-linear operator \(()\). This includes linear dynamics like advection, diffusion, and dispersion, and it also covers popular nonlinear dynamics like the viscid Burgers equation, the Korteweg-de Vries equation, the Kuramoto-Sivashinsky equation, as well as the incompressible Navier-Stokes equations for low to medium Reynolds numbers. Additionally, we consider reaction-diffusion equations like the Fisher-KPP equation, the Gray-Scott model, and the Swift-Hohenberg equation, demonstrating the applicability beyond fluid-like problems. The continuous form of all supported dynamics is given in Table 1 and a visual overview can be found in Figure 1.

For semi-linear PDEs, the class of Exponential Time Differencing Runge-Kutta (ETDRK) methods, first formalized by Cox and Matthews (2002), are one of the most efficient solvers known today (Montanelli and Bootland, 2020). Under periodic boundary conditions, a Fourier pseudo-spectral approach allows integrating the linear part \(\) exactly via a (diagonalized) matrix exponential. As such, any linear PDE with constant coefficients can be solved _analytically_, without any temporal or spatial discretization error. This makes it possible to reliably and accurately evaluate the corresponding learning tasks. The non-linear part \(()\) is approximated by a Runge-Kutta method. We elaborate on the methods' motivation, implementation, and limitations in appendix B. For semi-linear problems with stiffness arising from the linear part, ETDRK methods show excellent stability and accuracy properties. Ultimately, the cost of one time step in \(D\) dimensions is bounded by the Fast Fourier Transform (FFT) with \((N^{D}D(N))\). Due to pure explicit tensor operations, the method is well suited for GPUs, and discrete differentiability via automatic differentiation is straightforward.

PDE IdentifiersThe ETDRK solver suite operates with a physical parametrization by specifying the number of spatial dimensions \(D\), the domain extent \(L\), the number of grid points \(N\), the step size \( t\) and constitutive parameters, like the velocity \(c\) in case of the advection equation. All these parameters affect the difficulty of emulation and must, hence, be communicated when evaluating the forecasting of a specific PDE. As an exchange protocol or identifier of an experiment, APEBench also comes with a reduced interface tailored to identifying the present dynamics, including its discretization, in a minimal set of variables. This allows assigning an ID to each scenario in APEBench, which uniquely expresses the _discrete_ dynamics to be emulated. For the \(s\)-th order linear derivative with coefficient \(a_{s}\), we define two coefficients \(_{s}\) and \(_{s}\) as

\[_{s}= t}{L^{s}}_{s}= _{s}N^{s}2^{s-1}D.\] (1)

For a specific scenario, the \(_{s}\) represent normalized _dynamics coefficients_, while the \(_{s}\) quantify the _difficulty_. Gamma values correspond to the stability criteria of the most compact explicit finite difference stencils of the respective linear derivative (for \(s=1\), this is the CFL condition). Together, these values make it possible to quickly gain intuition about the dynamics encoded by a chosen PDE system and provide a convenient way to work with different scenarios: A list of gamma (or alpha) values together with \(N\) and \(D\) is sufficient to uniquely identify any linear dynamics. We have diffusion if only \(_{2} 0\). For \(_{1} 0_{2} 0\), we obtain advection-diffusion, while only \(_{3} 0\) yields dispersion.

The linear derivatives can be combined with a selection of nonlinear components to vary the system's dynamics, on whose reduction we elaborate in appendix B.7 and B.8. Combining a convection nonlinearity with \(_{2} 0\) results in the viscous Burgers equation. If further combined with \(_{3} 0\), the Korteweg-de Vries equation is obtained. Thus, the non-zero coefficients define the type of dynamics. Their relative and absolute scales define how challenging the emulator learning problem is. With this approach, APEBench intuitively communicates the emulated dynamics.

Neural Emulator ArchitecturesOur benchmark encompasses established neural architectures adaptable across spatial dimensions and compatible with Dirichlet, Neumann, and periodic boundaryconditions. This includes local convolutional architectures like ConvNets (Conv) and ResNets (Res) (He et al., 2016) as well as long-range architectures like UNets (Ronneberger et al., 2015) and Dilated ResNets (Dil) (Stachenfeld et al., 2021). Orthogonal to the two former classes, we consider pseudo-spectral architectures in the form of the Fourier Neural Operator (FNO), which have a global receptive field, but their performance instead depends on the spectrum of the underlying dynamics.

Training MethodologiesEmulator training is the task of approximating a discrete numerical simulator \(_{h}\). This solver advances a space-discrete state \(u_{h}\) from one time step to the next. The goal is to replicate its behavior with the neural emulator \(f_{}\), i.e., to find weights \(\) such that \(f_{}_{h}\). Since the neural emulator \(f_{}\) is trained on data from the numerical simulator \(_{h}\), their interplay during learning is crucial. Many options exist, like one-step training (Tran et al., 2023), supervised unrolling (Um et al., 2020), or residuum-based unrolling (Geneva and Zabaras, 2020). We introduce a novel taxonomy based on unrolling steps during training and the reference branch length, unifying most approaches in the literature. Given a dataset of states \(u_{h}_{h}\), the objective is

\[L()=_{u_{h}_{h}}[_{t=0}^{(T-B)} _{b=1}^{B}\,lf_{}^{t+b}(u_{h}),\;_{h}^{b}(f_{} ^{t}(u_{h}))],\] (2)

where \(T\) is the number of unrolled steps at training time, and the per time step loss \(l(,)\) typically is a mean squared error (MSE). During training, the emulator produces a trajectory \(\{f_{}^{t+b}\}\), which we call the _main chain_. The variable \(B\) denotes the length of the _branch chain_\(\{_{h}^{b}()\}\), which defines how long the reference simulator is rolled out next to the main chain.

The popular one-step supervised learning problem is recovered by setting \(T=B=1\). Purely supervised unrolling is achieved with \(T=B\). In such a case, all data can be pre-computed, allowing the reference simulator \(_{h}\) to be turned off during training. We also consider the case of _diverted chain_ learning with \(T\) freely chosen and \(B=1\) providing a one-step difference from a reference simulator while maintaining an autoregressive rollout. This configuration necessitates the reference simulator \(_{h}\) to be differentiable which is readily available within APEBench. While our results below do not modify the gradient flow (Brandstetter et al., 2022; List et al., 2022), the benchmark framework supports alterations of the backpropagation pass, as outlined in Appendix D.1.

Neural-Hybrid EmulationNext to the task of fully replacing the numerical simulator \(_{h}\) with the neural network \(f_{}\), which we call _prediction_, APEBench is also designed for _correction_. For this, we introduce a coarse solver \(}_{h}\) that acts as a predictor together with a corrector network \(_{h}\)(Um et al., 2020; Kochkov et al., 2021). Together, they form a _neural-hybrid emulator_. For example, we support a _sequential_ layout in which the \(f_{}\) of equation 2 is \(f_{}=_{}(}_{h})\). The coarse solver component \(}_{h}\) is also provided by the ETDRK solver suite. Any unrolling with \(T 2\) introduces a backpropagation-through-time that requires this coarse solver to be differentiable, which is also readily available.

MetricsSince this benchmark suite is concerned with _autoregressive neural emulator_, we emphasize the importance of _rollout metrics_. To compare two states \(u_{h}^{[t]}\) and \(u_{h}^{r,[t]}\) at time level \([t]\), we provide a range of established metric functions, which we elaborate in appendix F. This includes aggregations in state and Fourier space using different reductions and normalization. Moreover, we support metric computation for certain frequency ranges and the use of derivatives (i.e., Sobolev-based metrics) to highlight errors in the smaller scales/higher frequencies for which networks often produce blurry predictions (Rahaman et al., 2019).

## 5 Experiments

We present experiments highlighting the types of studies enabled by APEBench, focusing on temporal stability and generalization of trained emulators. We measure performance in terms of the normalized RMSE (see equation (31)) to allow comparisons over time if magnitudes decay and across dynamics. Aggregation over time is done with a geometric mean (see equation (32)). Plots show the median performance across network initializations, with error bars for the 50% inter-quantile range (IQR). Further specifics are provided in appendix H.

### Bridging Numerical Simulators and Neural Emulators

The motivational example in section 2 demonstrated the emulation of 1D advection for CFL \(_{1}<1\). We saw that purely linear convolutions with two learnable parameters are capable emulators. This section expands the experiment by training various nonlinear architectures on this linear PDE for varying difficulties. Since the CFL condition (\(|_{1}|<1\)) restricts information flow across one cell for first-order methods, we hypothesize that emulators with larger receptive fields can handle difficulties beyond one. Indeed, when considering standard feedforward convolutional architectures of varying depths, there is a clear connection between the effective receptive field and the highest possible difficulty. With a kernel size of three, each additional stacked convolution adds a receptive field of one per direction. A depth of zero represents a linear convolution. As shown in Figure 3 (a), such an approach is only feasible for \(_{1} 1\), aligning with the CFL stability criterion of the first-order upwind method and the results from section 2. Beyond this, the network fails to emulate the dynamics and diverges almost immediately. A similar behavior is observed for a convolution depth of one with an effective receptive field of two per direction. This is insufficient for advection dynamics of difficulty \(_{1}=2.5\).

Long-range convolutional architectures, like UNets and Dilated ResNets, perform better across the difficulties, i.e., the error rollout does not steepen as strongly as with the local convolutional architectures. However, they never turn out to be the best architectures. Given their inductive biases for long-range dependencies, they spend parameters on interactions with degrees of freedom beyond the necessary receptive field. This likely explains their reduced ability to produce the best results in this relatively simple scenario of hyperbolic linear advection with an influence range known a priori.

The pseudo-spectral FNO has a performance which is agnostic to changes in \(_{1}\). This behavior can be explained by its inherent capabilities to learn band-limited linear dynamics and its similarity with the data-generating solver. Despite these advantages, local convolutional architectures like a ResNet are on par with the FNO under low difficulties that do not demand a large receptive field.

Surprisingly, ResNet and the deepest ConvNet fail at the highest difficulty despite having sufficient receptive field. However, Figure 3 (b) reveals that under additional unrolling during training, the same ResNet greatly improves in temporal generalization. In line with the motivational example of section 2, this suggests that unrolling during training, rather than exposure to more physics, is key to a better learning signal and achieving desirable numerical properties.

### Diverted Chain: A Learning Methodology with A Differentiable Fine Solver

APEBench's tight integration with its ETDRK solver suite enables the exploration of promising training setups beyond purely data-driven supervised unrolling. One such setup is the _diverted chain_ approach as obtained with Eq. 2, combining autoregressive unrolling with the familiar one-step difference. Here, the reference solver branches off after each autoregressive network prediction. providing a continuous source of ground truth data during training. As it hinges on fully integrating the (differentiable) reference solver, this variant has not been studied in previous work.

Figure 3: (a) Performance of various neural emulator architectures on a 1D advection problem with increasing difficulty (\(_{1}=\)). If the demands on the receptive field (given by \(_{1}\)) are not fulfilled, emulators diverge immediately. (b) Unrolling improves accuracy at the highest difficulty.

Figure 4 compares the performance of a ResNet emulator trained using one-step supervised training, 5-step supervised unrolling, and 5-step diverted chain training on three nonlinear 1D dynamics: viscous Burgers, Kuramoto-Sivashinsky (KS), and hyper-viscous Korteweg-de Vries (KdV). The results demonstrate that training with unrolling, regardless of the specific approach, generally improves long-term accuracy, as indicated by the lower 100-step error. This improvement is particularly pronounced for the KdV equation, likely due to the increased effective receptive field seen during training, which is especially beneficial for strongly hyperbolic problems. Notably, the diverted-chain approach further enhances long-term accuracy for the KdV scenario. While it does not surpass supervised unrolling for Burgers and KS in terms of long-term accuracy, it excels in short-term performance, only slightly underperforming compared to the one-step trained emulator. These findings confirm that the diverted-chain approach effectively combines the benefits of training time rollout and one-step differences, demonstrating the flexibility of APEBench to explore diverse training strategies due to its differentiable solver.

### Neural-Hybrid Emulators and Sequential Correction

Neural-hybrid emulators, which combine neural networks with traditional numerical solvers, are a promising area of research in physics-based deep learning (Kochkov et al., 2021; Um et al., 2020). APEBench's differentiable ETDRK solver framework facilitates the exploration of such hybrid approaches. In this section, we investigate the sequential correction of a defective solver \(}_{h}\) using both a ResNet and an FNO for a 2D advection problem with a difficulty of \(_{1}=10.5\). Three variations of this task are explored: full prediction, and sequential correction with the coarse solver handling 10% (\(_{1}=1.05\)) or 50% (\(_{1}=5.25\)) of the difficulty.

Figure 5 displays the geometric mean of the test rollout error over 100 time steps. The results reveal that supervised unrolling consistently improves the performance of the ResNet and ResNet-hybrid models, outperforming the FNO in every case. Notably, the FNO's performance remains almost unaffected by unrolling and changes in difficulty, likely due to its global receptive field and ability to capture long-range dependencies. In contrast, the ResNet, with its limited receptive field, benefits significantly from unrolling. The ResNet performs best in the 50% correction task, highlighting the potential of neural-hybrid approaches that leverage the strengths of both convolutional and pseudo-spectral methods. These findings underscore the importance of tailoring the training strategy and architecture to the specific task and difficulty level and emphasize the potential of hybrid approaches for superior performance in PDE emulation.

### Relating Architectures and PDE Dynamics

APEBench's diverse collection of semi-linear PDEs provides a robust testing ground for emulator architectures across various dynamics. Below, we compare the five main neural architectures regarding

Figure 4: Comparison of training methodologies for a ResNet emulator on three nonlinear 1D dynamics. Emulators benefit from rollout training, the strongest visible for the KdV case. Diverted-Chain offers the advantage of long-term accuracy without sacrificing short-term performance.

Figure 5: ResNet and FNO either as full prediction emulators or neural-hybrid emulators for 2D advection (\(_{1}=10.5\)) with a coarse solver doing 10% or 50% of the difficulty. The geometric mean of the rollout error over 100 time steps is shown. Training with unrolling benefits the ResNet yet only shows marginal improvement for the FNO. The ResNet can work in symbiosis with a coarse simulator.

the geometric mean of the error over 100 rollout steps, summarized in Figure 6. The parameter counts across architectures are almost identical within the same spatial dimensions. We provide ablations on the choices of optimization configuration (section I.2), training dataset size (section I.3), and network parameter size (section I.4) in the appendix.

Linear PDEsFor the 1D hyperbolic dispersion problem, local convolutional architectures (with sufficient receptive fields and low problem difficulty) again excel, closely followed by the pseudo-spectral FNO. The considered higher-dimensional linear PDEs introduce _spatial mixing_, making the learning task more challenging. For 2D anisotropic diffusion, both local and global convolutional architectures perform well, with the FNO lagging slightly and the UNet showing a significant spread in performance. In the 3D unbalanced advection case, the FNO's limited active modes hinder capture of the solution's full complexity. Convolutional architectures struggle to balance short- and long-range interactions across different dimensions, with UNets showing the most consistent performance.

Nonlinear PDEsFor the 1D Korteweg-de Vries and Kuramoto-Sivashinsky equations, local architectures likely struggle with the hyper-diffusion term due to insufficient receptive fields, giving long-range architectures an advantage. The FNO's performance is also suboptimal, potentially due to limited active modes. Notably, the FNO excels in the challenging Navier-Stokes Kolmogorov Flow case.

Reaction-DiffusionReaction-diffusion problems, characterized by polynomial nonlinearities with no spatial dependence that develop rich (high-frequency) patterns (see Figure 14), are best handled by local convolutional architectures, particularly ResNets. The FNO was the least suitable architecture for this class of problems. We attribute this to its low-frequency bias in that it learns predictions in the frequencies beyond its active modes only indirectly via the energy transfer of the nonlinear activation.

Performance across DimensionsEmulating the Burgers equation across dimensions reveals an exponential decrease in performance as dimensionality increases (Figure 6 (d)). Across all dimensions, ResNets consistently emerge as the top-performing architecture, showcasing their adaptability to varying spatial complexities. Dilated ResNets, while effective in 1D and 2D, experience a significant performance drop in 3D. This likely stems from their dilation-based mechanism for long-range interactions resulting in less uniform coverage of the receptive field compared to UNets. The performance gap between standard ConvNets and ResNets widens in 3D, highlighting the increasing importance of skip connections.

Figure 6: Comparison of emulator architectures across various PDE dynamics in 1D, 2D, and 3D. The ResNet consistently performs well across all dynamics and dimensions. Local architectures struggle with higher-order derivatives, while limited active modes hinder the FNO’s performance in some cases. The Dilated ResNet is the better long-range architecture in 1D, whereas the UNet is better suited for higher dimensions.

Performance across ResolutionIn this example, we emulate a KS equation in 2D using \(N=32^{2}\) and \(N=160^{2}\) as well as in 3D with \(N=32^{3}\). Due to the difficulty mode, the dynamics are adapted based on resolution and dimensionality. Counterintuitively, emulation often improves with increasing resolution until the emulators' architectures are fully utilized. In contrast, the FNO struggles in this scenario because, due to the difficulty-based rescaling, the spectrum is fully populated in both resolutions for the 2D case (see the spectra in Figure 14). Across all architectures, the jump to 3D worsens their performance, which reinforces the observations shown in Figure 6 (d). Notably, the UNet emerges as the best architecture in 3D likely because it has a global receptive field at this resolution.

## 6 Limitations

We currently focus on periodic boundary conditions and uniform Cartesian grids. Broadening the scope of the benchmark w.r.t. other numerical solvers and discretizations, forced dynamics, and specialized network architectures constitute highly interesting future work.

## 7 Conclusions and Outlook

We presented APEBench, a benchmark suite for autoregressive neural emulators of time-dependent PDEs, focusing on training methodologies, temporal generalization, and differentiable physics. The benchmark's efficient JAX-based pseudo-spectral solver framework enables rapid experimentation across 1D, 2D, and 3D dynamics. We introduced the concept of _difficulties_ to uniquely identify dynamics and scale experiments. The unified treatment of unrolling methodologies was demonstrated as a foundation to investigate learned emulators across a wide range of dynamics and training strategies. We revealed connections between the performance of an architecture, problem type, and difficulty that make it possible to understand their behavior with analogies to classical numerical simulators. Specifically, our benchmark experiments highlight the importance of:

* Matching the network architecture to the specific problem characteristics. Local problems benefit significantly from local convolutions, while global receptive fields are less impacted by unrolled training.
* Utilizing training with unrolling to significantly improve performance, particularly for challenging problems and under limited receptive fields.
* Exploring hybrid approaches that combine neural networks with coarse numerical solvers (correction) and differentiable reference solvers (diverted-chain training) to further enhance the capabilities of learned emulations.

In this context, many interesting areas for future work remain. Particularly notable are parameter-conditioned emulators and foundation models that can solve larger classes of PDE dynamics. Perhaps the most crucial avenue for future research with APEBench lies in conducting an even deeper investigation of unrolling and the intricate interplay between emulator and simulator. A deeper understanding here could significantly impact the field of neural PDE emulation.

## 8 Acknowledgements

Felix Koehler acknowledges funding from the Munich Center for Machine Learning (MCML). The authors are grateful for constructive discussions with Bjoern List, Patrick Schnell, Georg Kohl, Qiang Liu, and Dion Haeffner.