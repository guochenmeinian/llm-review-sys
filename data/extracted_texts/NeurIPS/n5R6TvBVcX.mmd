# WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safter Language Models

Liwei Jiang\({}^{1,2}\)  Kavel Rao\({}^{*,1}\)  Seungju Han\({}^{*,3}\)  Allyson Ettinger\({}^{2}\)

**Faeeze Brahman\({}^{2}\)  Sachin Kumar\({}^{2}\)  Niloofar Mireshghallah\({}^{1}\)  Ximing Lu\({}^{1}\)  Maarten Sap\({}^{2,4}\)  Yejin Choi\({}^{1}\)  Nouha Dziri\({}^{2}\)**

\({}^{1}\)University of Washington \({}^{2}\)Allen Institute for Artificial Intelligence

\({}^{3}\)Seoul National University \({}^{4}\)Carnegie Mellon University

lwjiang@cs.washington.edu  nouhad@allenai.org \({}^{*}\)Co-second-authors

Code & Models: https://github.com/allenai/wildteaming

Data: https://huggingface.co/datasets/allenai/wildjailbreak

###### Abstract

We introduce WildTeaming, an automatic red-teaming framework that mines _in-the-wild_ user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes selections of multiple mined tactics for systematic exploration of novel and even more challenging jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with large language models (LLMs), our work investigates jailbreaks from chatbot users in-the-wild who were not specifically instructed to break the system. WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more _diverse_ and _successful_ adversarial attacks compared to state-of-the-art jailbreaking methods.

While there exist many datasets for jailbreak _evaluation_, very few open-source datasets exist for jailbreak _training_, as safety training data has been closed among all frontier models even when their weights are open. Therefore, with WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K _vanilla_ (direct request) and _adversarial_ (complex jailbreak) prompt-response pairs. In order to mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) _harmful_ queries (both vanilla and adversarial) and 2) _benign_ queries that resemble harmful queries in form but contain no harmful intent. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the _scaling effects of data_ and the _interplay of data properties and model capabilities_ during safety training. Through extensive model training and evaluations, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of both vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All the components of WildJailbreak contribute to achieving balanced safety behaviors of models.

## 1 Introduction

Despite ongoing efforts to enhance their safety, frontier LLMs remain vulnerable against unsafe user queries, especially adversarial attacks . The fact that models can be easily jailbroken raises significant concerns among researchers and policymakers , motivating the research for systematically discovering and guarding against potential jailbreaks. In this work, we introduce theWildTeaming framework to address two challenges: 1) broadly identifying jailbroken behaviors of LLMs and 2) creating a publicly open, large-scale safety training resource for systematic defense. This resource is designed to help models robustly guard against _vanilla_ and _adversarial_ harmful user queries without causing over-refusal of benign queries or diminishing model general capabilities.

**The first challenge that WildTeaming addresses is to reveal vulnerabilities of LLMs against adversarial jailbreaks with scale and diversity.** We introduce WildTeaming, a practical red-teaming framework that composes automatically mined human-devised jailbreak tactics to transform vanilla harmful queries into many varieties of challenging adversarial attacks. WildTeaming improves over previous methods by diversifying the range of successful attack candidates while maintaining low computational costs, making it practical for scaling up. WildTeaming uncovers model vulnerabilities through a two-stage process: _mining jailbreak tactics from in-the-wild (ITW) chathot logs_ (Mine) and _composing mined tactics into diverse adversarial attacks_ (Compose).

In the Mine stage, WildTeaming automatically maps out previously under-explored spaces of jailbreak tactics, significantly expanding the current taxonomy. To do so, it identifies 105K human-devised jailbreak tactics (5.7K unique clusters) from real-world user-chatbot interactions in LMSYS-Chat-1M  and (InThe)WildChat . In the Compose stage, WildTeaming generates diverse adversarial attack candidates by combining different selections of tactics using off-the-shelf LLMs like Mixtral-8\(\)7B  and GPT-4 . It further refines attacks through lightweight off-topic and low-risk pruning to enhance attack quality and efficiency. With a suite of newly defined _diversity_ evaluation metrics, WildTeamio identifies up to 4.6 times more unique successful attacks against black-box and white-box LMs in 40% fewer attack attempts compared to other state-of-the-art jailbreak methods, which sometimes struggle to find even two unique successful attacks.

**The second challenge WildTeaming addresses is to enhance open resources for safety training.** We apply WildTeaming to create WildJailbreak, a large-scale, high-quality synthetic safety instruction-tuning data resource with 262K prompt and response pairs. WildJailbreak contains four _contrastive_ components: 1) **vanilla harmful** queries conveying explicit unsafe requests across widespread risk categories, e.g., malicious uses, harmful language ; 2) **vanilla benign** queries that are similar to unsafe queries in form but convey no harmful intent, used to mitigate models' exaggerated safety behaviors ; 3) **adversarial harmful** queries that are jailbreaking versions of vanilla harmful queries converted by the WildTeaming heuristic; 4) **adversarial benign** queries used to counteract adversarial exaggerated safety behaviors, also generated by WildTeaming. WildJailbreak is the first safety training resource to simultaneously address all four components, significantly improving upon existing resources with both enhanced scale and quality .

The unique composition and size of WildJailbreak allow us to conduct extensive safety training experiments to study the scaling effect of safety training data and the interplay of data properties and model capabilities. Our experiments confirm the necessity of all components of WildJailbreak

Figure 1: The two steps of the WildTeaming framework: Mine (in-the-wild user-written jailbreak tactics) and Compose (jailbreak tactics into diverse adversarial attacks).

for achieving _balanced_ safety behaviors, i.e., robust safeguard without over-refusal on both vanilla and adversarial cases. Moreover, by mixing varying sizes of WildJailbreak with Tulu2Mix , an instruction-tuning resource for teaching models general instruction-following and reasoning capabilities, we show that larger sizes of safety training data lead to gradually improving vanilla and adversarial safety features without sacrificing models' general capabilities, even at a scale orders of magnitude larger than studied in previous literature, measured by 15+ downstream tasks. Finally, although training on either vanilla or adversarial data improves performance on the other data type, the most robust safeguard comes with the hybridization of both. Our safety training insights pave the way towards building more transparent and safer future models.

## 2 WildTeaming Preface: Harvesting Jailbreak Tactics In-the-Wild

Given a target model \(M_{}\), a harmful prompt \(\) will elicit either a harmful (\(_{h}^{M_{}}\)) or a benign (\(_{h}^{M_{}}\)) model response. The goal of red-teaming is to identify harmful prompts \(\) that reveal harmful responses from \(M_{}\). Jailbreaking, a more challenging form of red-teaming, aims to revise known harmful prompts \(\) that currently elicit benign responses into _adversarial_ prompts \(\) to bypass the safeguard of \(M_{}\) by eliciting target harmful responses instead.

Our current knowledge of _jailbreak tactics_ used in forming adversarial attacks is relatively limited, and recent works uncover a narrow range of possible jailbreaks [93; 8; 55; 68]. To overcome this limitation, we mine real-world chat logs, which is a surprisingly rich source of diverse jailbreak tactics, even though these users were not specifically instructed to jailbreak the system.

### Mining Jailbreak Tactics from Real-World User-Chatbot Interactions

With a seed set of manually-identified tactics, we apply GPT-4 to expand the discovery automatically.

**Gathering ITW User-Written Adversarial Harmful Prompts.** We first collect candidate adversarial prompts from all single-turn conversations in LMSYS-1M  and WildChat that are flagged by the OpenAI Moderation API.1 We then filter out trivial non-adversarial prompts by feeding candidates through a lightly safety-trained model (Tulu2-7B), keeping those that elicit harmful model responses as judged by the Llama-Guard safety classifier ; this yields 16,850 final prompts.

    & **Data Source** & **Prompt** & **Jailbreak Tactics** & **15.5\%** & Fictitious scenario \\ 
**Type** & **Name** & **Total** & **Total** & **Uniq.** & **Per.** \\   & LMSYS-1m  & 7,873 & 43,220 & 2,526 & 5.49 \\  & WildChat & 8,981 & 62,218 & 3,903 & 6.93 \\  & **Combined** & **16,854** & **105,438** & **5,688** & **6.26** \\   & DAN  & 666 & 4,378 & 510 & 6.57 \\  & TrustLLM  & 1,400 & 4,531 & 280 & 3.24 \\  & DecodingTrust  & 5 & 8 & 5 & 1.60 \\   Semantic \\ Jailbreak \\ Methods \\  } & PAIR  & 400 & 1,854 & 162 & 4.64 \\  & TAP  & 398 & 1,861 & 149 & 4.68 \\  & PAP  & 398 & 1,564 & 118 & 3.93 \\   Safety \\ Training \\ Data \\  } & HH-RLHF  & 500 & 884 & 66 & 1.77 \\  & Safety Llamas  & 500 & 911 & 66 & 1.82 \\  & Safe-RLHF  & 500 & 1,034 & 84 & 2.07 \\   

Table 1: (Left) shows the number of items (**Total**), number of deduplicated unique clusters (**Uniq.**), and per query count (**Per.**) for jailbreak tactics automatically mined from In-the-Wild user prompts in LMSYS-1M and WildChat, which contain a greater diversity and quantity of jailbreak tactics compared to those from other sources. Underline indicates a sub-sampled set of queries. (Right) shows the top common jailbreak tactics and their percentage of occurrence.

**Identifying Seed Jailbreak Tactics by Manual Examination.** We manually examine \(\)200 ITW prompts sampled from our ITW adversarial prompt set to identify 35 seed jailbreak tactics with definitions (see the full list in Table 5 and 6 in SSB.1).

**Automatic Tactics Discovery Aided by GPT-4.** With seed tactics, we apply GPT-4 to scale the tactic mining. For each adversarial prompt, GPT-4 is given two tasks: (1) extracting the core vanilla request; (2) identifying both _existing_ and _novel_ jailbreak tactics in the adversarial prompt. GPT-4 additionally identifies an _excerpt_ corresponding to each tactic, the _definition_ to describe novel tactic, and _reasoning_ of why the tactic applies. Each step is carefully prompted with a demonstration example (see Prompt 1 and 2 in SSB.2). We then deduplicate tactics by clustering on their corresponding definitions2 with sentence embeddings3 and report the statistics of these unique clusters in Table 1.

### What Tactics Are Adopted by In-the-Wild Users for Jailbreaking LLMs?

Table 1 shows the top In-the-Wild jailbreak tactics, including a mixture of stylistic, syntactic, formatting, writing genre, and context-based tricks. Specifically, it uncovers novel tactics not systematically documented previously, such as "prefacing the harmful content with a content warning or disclaimer," "setting blame for non-compliance," or "cloaking harm in humor" (more examples of novel tactics in Table 8 of Appendix SSB.2).

In addition, as shown in Table 1, ITW adversarial user queries contain the richest set of unique jailbreak tactics compared to other sources of known jailbreak templates, i.e., DAN , TrustLLM , DecodingTrust. ITW attacks are also more adversarial than attacks generated by existing semantic-level jailbreak methods (i.e., PAIR, TAP, PAP) as they, on average, contain more jailbreak tactics per query . Finally, given the diversity of ITW jailbreak tactics, it's concerning that existing public safety training data, namely HH-RLHF , Safety LLamas, and Safe-RLHF , does not contain adversarial enough training examples, limiting downstream models' robustness against adversarial threats.

## 3 WildTeaming: Diverse Red-Teaming by Composing Jailbreak Tactics

By composing selections of mined ITW jailbreak tactics, we transform vanilla harmful requests into diverse model-agnostic adversarial attacks. We compare WildTeaming to jailbreaking methods across standard attack _effectiveness_ metrics and a new suite of _diversity_ metrics to show WildTeaming's advantages in finding many unique successful attacks.

### WildTeaming Workflow Formulation

Jailbreaking methods seek to revise a given vanilla harmful prompt \(\) into an adversarial counterpart \(\), aiming to elicit the harmful model response from a target model \(M_{}\). WildTeaming follows a simple but effective two-step workflow to tackle this problem.

**Step 1: Generating attack candidates seeded by sampled jailbreak tactics.** First, we sample a set of ITW jailbreak tactics and instruct an off-the-shelf language model (\(M_{}\); e.g., Mixtral-8\(\)7B) to apply these tactics for revising a given vanilla harmful prompt (\(\)) into an adversarial attack (\(\)).

Formally, given the entire pool of jailbreak tactics \(\), we sample a subset of \(n\) tactics \(T^{i}=\{t_{1},...,t_{n}\}\). We then revise \(\) into \(^{i}\) by conditioning on \(T^{i}\), i.e., \(^{i} M_{}(|\);\(T^{i})\)

**Step 2: Refining attack candidates with off-topic and low-risk pruners.** To ensure the revised adversarial attacks retain the original harmful intent and risk level, we apply two light-weight binary filters to prune off attack candidates that are unlikely to result in successful attack, including a _off-topic_ classifier (\(Pr_{}\); \(T\) for off-topic vs. \(F\) for on-topic) and a _low-risk_ classifier (\(Pr_{}\); \(T\) for low-risk vs. \(F\) for high-risk). This step identifies attacks more faithful to their vanilla counterparts and more likely to elicit on-target harmful model responses.

Formally, given the adversarial attack candidate \(^{i}\), we apply \(Pr_{}\) and \(Pr_{}\) to rate if we keep or prune \(^{i}\):

\[_{^{i}}=Pr_{}( ^{i})=F\,\ Pr_{}(^{i})=F\] (1)

We add \(^{i}\) to the official attack candidate pool if is_keep\({}_{^{i}}\) is 1, or otherwise regenerate another attack by repeating from Step 1.

Additional details of all components of WildTeaming, including the attack model, the target models, the off-topic and low-risk pruners, and attack selectors are described in Appendix SSC.1.

### Evaluation Setups

**Evaluation Task.** We use the evaluation setup of HarmBench, a unified jailbreaking evaluation benchmark including test vanilla harmful prompts across standard, contextual, and copyright unsafe behaviors. In this work, we report results using 159 vanilla behaviors in the standard test set, as these cases represent high-risk unsafe scenarios that language models must account for.

**Baselines.** We compare WildTeaming with the top two optimization-based methods (GCG, AutoDAN) and one of the top semantic methods (PAIR), as reported in HarmBench. GCG optimizes discrete prompts (often gibberish) to produce affirmative answers to harmful requests . AutoDAN uses human-written jailbreak prompts as initial seeds to run generic algorithms . PAIR uses an LLM to iteratively propose and edit attacks with the target model in-the-loop .

_Effectiveness_** Evaluation.** We measure _effectiveness_ by the attack success rate (ASR) across the entire evaluation set of vanilla harmful queries. The success of an individual attack is determined by the test classifier from HarmBench fine-tuned from a Llama2-13B model. Specifically, the test classifier takes in a vanilla harmful prompt \(\) and the model response elicited by its corresponding adversarial attack, \(^{M_{}}\), and decides if \(^{M_{}}\) sufficiently addresses the harmful information requested by \(\). To measure attack _efficiency_, we report the number of queries needed to reach a successful attack (Query). To assess the attack stealthiness or _naturalness_, a strong indicator of the defense difficulty, we use Vicuna-7B to compute the perplexity (PPL) of the final successful attacks.

    &  &  \\ 
**Model** & **Method** & ASR \(\) & Trial \(\) & PPL \(\) & ASR\({}^{ 5}_{30}\) & Trial\({}^{ 5}_{30}\) & Sim\({}^{95}_{30}\) & Sim\({}^{}\) & \#Tactic\({}^{}\) \\   Vicuna \\ (7B) \\  } & WildTeam & 93.1 & **2.82** & **8.65** & **88.1** & **9.31** & **7.22** & **5.27** & **55** \\  & PAIR & **94.3** & 3.55 & 9.42 & 59.5 & 14.78 &.790 &.530 & 27 \\  & AutoDAN & 89.3 & - & 13.74 & 19.4 & \(\) &.972 &.969 & 36 \\  & GCG & 89.9 & - & 4062.57 & - & - & - & - & - \\   Tulu2 \\ DPO \\ (7B) \\  } & WildTeam & **96.9** & **2.61** & **8.77** & **87.8** & **8.98** & **7.22** & **.529** & **61** \\  & PAIR & 95.0 & 3.57 & 9.78 & 62.1 & 14.24 &.792 &.534 & 29 \\  & AutoDAN & 94.3 & - & 12.97 & 20.0 & 1.41 &.972 &.962 & 36 \\  & GCG & 51.6 & - & 4265.86 & - & - & - & - & - \\   Mistral \\ (7B) \\  } & WildTeam & 95.0 & **2.37** & **8.56** & **89.2** & **8.72** & **.722** & **.527** & **52** \\  & PAIR & **95.6** & 3.28 & 9.62 & 65.0 & 14.21 &.792 &.537 & 30 \\  & AutoDAN & 92.5 & - & 13.24 & 19.9 & \(\) &.961 &.952 & 40 \\  & GCG & 85.5 & - & 2266.69 & - & - & - & - & - \\   Mistral \\ (8\(\)7B) \\  } & WildTeam & **98.1** & **2.72** & **8.75** & **87.2** & **8.99** & **.722** & **.531** & **55** \\  & PAIR & 97.5 & 3.05 & 9.54 & 61.8 & 13.96 &.795 &.533 & 28 \\  & AutoDAN & 88.7 & - & 13.31 & 20.0 & 1.53 &.967 &.957 & 38 \\   GPT-3.5 \\ (0613) \\  } & WildTeam & **92.5** & 7.08 & **7.96** & **65.8** & **13.19** & **.733** & **.526** & **50** \\  & PAIR & 88.7 & **6.65** & 9.78 & 61.2 & 17.01 &.798 &.530 & 26 \\   GPT-4 \\ (0613) \\  } & WildTeam & **79.9** & **8.61** & **8.13** & **60.1** & **13.43** & **.731** & **.530** & **39** \\  & PAIR & 78.6 & 9.64 & 9.33 & 44.9 & 17.75 &.802 &.538 & 29 \\   

Table 2: WildTeaming compared to other jailbreaking methods on representative open-source and closed-source models with the test set of the HarmBench.

_Diversity_**Evaluation.** The ultimate purpose of automatic jailbreaking is to reveal model vulnerabilities broadly and systematically so that defenses can be implemented. For a jailbreaking method to be practically useful, we must evaluate its ability to discover a wide range of model vulnerabilities with reasonable efficiency for scalable red-teaming. Without accounting for attack _diversity_, methods may overoptimize for the effectiveness of a single successful attack and fail to find a second different attack at all, substantially reducing their practicality for broad red-teaming. To show WildTeaming's advantage in red-teaming broadly, we define a new suite of diversity metrics to assess the ability of jailbreak methods to identify multiple unique successful attacks. We define \(^{ n}_{c}\)\(=_{i=1}^{n}^{@i}_{c}\) to measure the average success rate for finding \(i\{1,...,n\}\) unique attacks given \(c\) attack trials. Here, \(^{@i}_{c}\) is the success rate of simultaneously finding \(i\) unique successful attacks given \(c\) attack trials. The uniqueness of attack candidates is determined by sentence embedding similarity \(<0.75\). In addition, we report \(^{ n}_{c}=_{i=1}^{n}^{@i}_{c}\), the average number of queries needed to find \(i\{1,...,n\}\) unique successful attacks given \(c\) attack trials. Here, \(^{@i}_{c}\) is the number of queries needed to find \(i\) unique successful attacks among \(c\) attack attempts. \(^{@n}_{c}\) is the average pairwise sentence embedding similarity among the first \(n\) successful attacks. Finally, \(^{}\) is the pairwise sentence embedding similarity among all successful attacks across the evaluation pool, and #Tacticall is the total number of identified unique clusters of tactics.

### Results

Table 2 shows that compared to other jailbreaking methods, WildTeaming shows similar or better standard ASR (for finding one successful attack), while taking fewer attack trials and presenting more natural text (i.e., lower perplexity). With diversity metrics, the advantage of WildTeaming is even clearer: WildTeaming improves over PAIR by 4.6-25.6 \(^{ 5}_{30}\) scores while using fewer queries (3.8-5.5 points of decrease in \(^{ 5}_{30}\)). Figure 2 shows that although WildTeaming appears similar to PAIR when we assess success in finding 1-2 unique attacks, as substantial gap emerges when we assess the methods' ability to identify larger numbers of unique attacks while using less number of queries. It's notable that the two optimization-based baselines are either not capable of finding even a second unique attack (AutoDAN) or are prohibitive to run for diversity evaluation metrics (GCG is estimated to take \(\)15 hours to generate 30 attack candidates for each test vanilla query on one 80GB A100 GPU). Finally, Table 3 shows the importance of off-topic and low-risk pruners for further enhancing the performance of WildTeaming. In the main jailbreaking experiment, we opt to adopt a fixed tactic, "seed leading sentence," while randomly sampling other tactics to be consistent with PAIR, which explicitly mentions this

    &  &  \\   & ASR \(\) & Query \(\) & \(^{ 5}_{30}\) & Query\({}^{ 5}_{30}\) \(\) \\  No Pruning & 95.1 & 3.64 & 83.4 & 9.97 \\ Off-topics Only & 95.1 & 2.95 & 83.9 & 9.64 \\ Low-Risk Only & 95.1 & 2.62 & 85.9 & 9.14 \\  Combined & 95.1 & 2.46 & 86.8 & 8.94 \\  PAIR & 97.6 & 4.10 & 56.1 & 13.95 \\ Not Fix & 92.7 & 3.42 & 80.5 & 9.94 \\   

Table 3: Ablations of pruners and whether to fix the seed leading sentence tactic for attacking Vicuna-7B with the validation set of HarmBench.

tactic in the instruction prompt of their attacker model. However, we also include the ablation result of _not_ fixing the "seed leading sentence" in Table 3, which still shows considerable improvement over PAIR in ASR\({}_{30}^{ 5}\) (80.5 vs. 56.1) and Query\({}_{30}^{ 5}\) (9.94 vs. 13.95), though slightly lower than using the fixed tactic. We show example attacks from different attack methods in Table 12, 13, 14, 15, 16, 17 in Appendix SSC.4.

## 4 WildJailbreak: A Large-Scale Safety Training and Evaluation Dataset

As shown in Table 1, public safety training datasets lack adversarial complexity. Therefore, we apply WildTeaming to create WildJailbreak, a large-scale synthetic safety training dataset covering four distinct types of safety data to contribute to open-source safety training resources.

### The Construction of Four Types of Safety Data

Here, we introduce the four types of safety data in WildJailbreak and further expand the data construction details in Appendix SSD.1. Example data from each type is shown in Table 18.

**Vanilla harmful (H)** queries are direct requests that could potentially elicit harmful responses from LMs. We apply GPT-4 to synthetically generate 50,050 vanilla harmful prompts across 13 risk categories, inspired by taxonomy from Weidinger et al. . In addition, we pair the harmful prompts with helpful and detailed refusal responses, also synthetically generated with GPT-3.5.

**Vanilla benign (B)** queries are harmless prompts used to combat exaggerated safety, i.e., over-refusal on benign queries. Motivated by the exaggerated safety categories in XSTest , we use GPT-4 to generate 50,050 prompts that superficially resemble unsafe prompts by keywords or discuss sensitive topics in non-harmful ways. Similarly, we use GPT-3.5 to generate complying responses.

**Adversarial harmful (H)** queries are jailbreaks that convey harmful requests in more convoluted and stealthy ways. We apply WildTeaming to transform our vanilla harmful queries with 2-7 randomly sampled ITW jailbreak tactics, with both the Mixtral-8\(\)7B and GPT-4 models to increase data diversity. We also filter out low-risk or off-topic prompts to increase attack quality as in jailbreak experiments in SS3. Finally, we pair the model refusal responses generated from the counterpart vanilla prompts to adversarial prompts, yielding 82,728 items in this split of the dataset.

**Adversarial benign (B)** queries are adversarial queries that look like jailbreaks but contain no harmful intent. Similar to adversarial (H) queries, we create 78,706 adversarial (B) queries using WildTeaming, based on the vanilla (B) prompts. We use GPT-3.5 to generate direct continuations of the prompts as the target model response.

### How Safe are LLMs Against Adversarial Attacks Evaluated by WildJailbreak?

In addition to the training data, we also create two held-out in-domain adversarial evaluation sets for WildJailbreak to use for our safety training experiments in SS5, including 2K adversarial harmful queries and 250 adversarial benign queries. As a first application of our new evaluation set, we test an array of existing open and closed chat models using the adversarial harmful subset of the evaluation data. Figure 3 shows an evident performance gap between models trained on open-source (e.g., Tulu2, Vicuna) vs. closed-source data (e.g., Llama-3, GPT-4), highlighting the need for improved open-source resources to enhance models' robustness against adversarial attacks.

Figure 3: Attack success rate (ASR) of adversarial attacks in the WildJailbreak evaluation data against various families and sizes of chat language models.

Enhancing Models' Adversarial Safety Alignment with WildJailbreak

Having created WildJailbreak and showed the unique challenge presented by its adversarial attacks, we now show its utility in safety training when combined with general capabilities data.

### Experiment Setups

**Training Data.** We augment \(\)-no-refusal4, a general capability instruction-tuning dataset consisting of 300K examples, with 200K examples sampled from WildJailbreak, resulting in 500K examples. From WildJailbreak we sample 50K each of vanilla harmful, adversarial harmful, vanilla benign, and adversarial benign items. Combining \(\)-no-refusal with WildJailbreak creates a unique data blend that enables us to examine effects of scale and data types for achieving the Pareto frontier between general capabilities and safety. To our best knowledge, this training setup is significantly larger than previously reported safety-training studies in Bianchi et al. , which have combined a maximum of 2K safety data with 20K general instruction data.

**Training Details and Baselines** For all training experiments, we follow the setup introduced in \(\) and fine-tune a Llama2 7B base model on our 500K data mixture for 2 epochs. We compare against three baseline models trained on different data mixtures: 1) the original \(\) (T2M), 2) T2M-no-refusal, and 3) T2M-public-safety: a mixture of T2M-no-refusal with representative existing safety training data (i.e., HH-RLHF , Safe-RLHF , DAN ). Full details of model training are in Appendix SSA.

**Evaluation Tasks and Metrics.** We evaluate both general capabilities and safety capabilities. For _general_ capabilities, we evaluate on AlpacaEval  (V1, V2), MTBench , MMLU , GSM , BBH , TydiQA , Codex-Eval , TruthfulQA .

For _safety_ evaluation, we assess models' robustness against both vanilla and adversarial harmful and benign queries. Vanilla benchmarks include HarmBench , ToxiGen , and XSTest , and adversarial benchmarks include JailbreakTrigger , DAN , and the WildJailbreak test data. We report attack success rate (**ASR5**) as the safety evaluation metric. We obtain ASR scores by computing rate of models' refusal to answer (**RTA**) as judged by the test classifier provided

    &  &  &  \\   & **MTB** & **AlpE1** & **HarmB** & **TosiG** & **XST\({}_{}\)** & **XST\({}_{}\)** & **XST\({}_{}\)** & **JT** & **dAN** & **WJ\({}_{}\)** & **WJ\({}_{}\)** & **WJ\({}_{}\)** \\
**Train Data** & total\(\) & win\(\) & asr\(\) & to\(\)\(\) & f1\(\) & rat\(\) & rat\(\) & rat\(\) & ast\(\) & acc\(\) & ast\(\) & tra\(\) \\   \) (T2M)} & 5.87 & 72.7 & 20.8 & 3.3 & 85.1 & 83.0 & 9.6 & 74.8 & 49.7 & 69.0 & 60.4 & 1.6 \\  & T2M-no-refusal & 5.84 & 75.9 & 59.1 & 65.9 & 83.7 & 79.5 & 8.4 & 60.0 & 66.0 & 64.1 & 71.0 & 0.8 \\  & T2M-public-safety & 6.10 & 70.4 & 66.0 & 56.8 & 79.3 & 72.0 & 7.6 & 63.5 & 27.3 & 66.0 & 67.7 & 0.4 \\   \)(\(\))} & 6.29 & 74.6 & **3.1** & **0.2** & 87.6 & 86.5 & 8.8 & **86.8** & **14.0** & **98.4** & 1.7 & **1.6** \\ \)} & 6.06 & 73.9 & 5.7 & 1.8 & **88.1** & **88.5** & 10.0 & 81.8 & 36.7 & 72.7 & **0.2** & 54.4 \\ \)} & 6.21 & 72.4 & **1.9** & 4.5 & 87.2 & 83.5 & **6.4** & **99.8** & 43.7 & 70.7 & 57.5 & **1.2** \\ \)} & 6.08 & 74.5 & 5.0 & 16.6 & **88.9** & **90.5** & 10.4 & **82.5** & 49.3 & 69.9 & 58.2 & 2.0 \\ \)} & 6.16 & 72.6 & 20.8 & **0.1** & 85.5 & 81.0 & **6.8** & 80.0 & **16.0** & **97.4** & 2.5 & 2.8 \\ \)} & 6.15 & 73.5 & 32.1 & 15.5 & 86.8 & 83.5 & 7.2 & 80.5 & 44.3 & 72.1 & **1.0** & 54.8 \\  \)(\(\))} & 6.59 & 80.5 & 2.5 & 0 & 87.6 & 86.5 & 8.4 & 86.8 & 10.7 & 98.1 & 1.5 & 2.4 \\   

Table 4: Evaluation results of the general capability and safety of Tulu2 finetuned with Tulu2Mix and different components of WildJailbreak (WJ). All models are 7B except [+WJ (138)]. For the safety evaluations, we highlight the **best**, the **second best**, the **worst**, and the **second worst** scores of each task for 7B models trained with WJ to highlight balanced performance of the model trained on all components of WJ.

by HarmBench, and we also compute a separate RTA score based on a GPT-4 judge of model refusal. Please refer to Table 27 of SSE.3 for relevant tasks, measuring aspects, and evaluation metrics reported in the main result table (Table 4), and Appendix SSE.3 for extended details of all evaluations benchmarks and metrics for the full results in Appendix SSE.2.

### Results and Findings

Main results are presented in Table 4 and Figure 4. Due to space constraints, we show results from AlpacaEval (V1) and MTBench in Table 4, and we refer readers to Table 30, 31, 32, 33, 34 in SSE.4 for the full report of general capabilities results. We see several clear patterns.

**WildJailbreak leads to substantial safety improvements, with minimal impact on general capabilities.** Results show that the model trained on T2M-no-refusal [+WildJailbreak] exhibits a substantial boost in safety across all vanilla and adversarial tasks compared to baselines, without showing exaggerated safety behaviors (as indicated by XST\({}_{}\) and WJ\({}_{}\) scores). When compared to the T2M-no-refusal baseline without any safety interventions, the model shows only a slight degradation (-1.7%) on AlpacaEval v1, and a notable increase on MTBench (+7.7%). Moreover, the [+WildJailbreak] model achieves a relative improvement of 85.1% on HarmBench over the model trained on original Tulu2Mix, indicating that the safety data from WildJailbreak leads to significantly higher-quality safety training than that in the original Tulu2Mix. Finally, WildJailbreak enhances models' robustness against adversarial attacks from other sources, improving defense by 71.9% regarding jailbreaking prompts from Do-Anything-Now .

Moreover, the model trained on existing open-source safety data (T2M-public-safety) results in mediocre performance compared to that trained on WildJailbreak. We hypothesize that this is because in an RLHF setup for which these datasets were designed, pairwise response pairs aim to show only relative preference rather than absolute high-quality content. Consequently, converting the "preferred" model response into a target for sequential fine-tuning can lead to sub-optimal responses. Overall, the ability of WildJailbreak to improve safety behaviors suggests that its diversity and comprehensive coverage enable more systematic model safety defenses than prior safety training data.

**WildJailbreak composition improves safeguard without exaggerated safety: roles of vanilla and adversarial (harmful/benign) data in achieving Pareto optimality.** We conduct comprehensive ablations of each component of WildJailbreak (vanilla/adversarial \(\) harmful/benign). Table 4 and Figure 4 indicates that all four components are indispensable for achieving a balanced trade-off between safety, helpfulness, and general capabilities of the [+WildJailbreak] model. The [+WJ-harm-only] model, trained solely on the harmful subset, excels at refusing harmful queries in both vanilla and adversarial benchmarks. However, it performs poorly in exaggerated safety (XST\({}_{}\), WJ\({}_{}\)). The [+WJ-vani-only] model, trained only on vanilla queries, performs best against vanilla harmful prompts but only slightly improves against adversarial attacks (see Figure 4), showing that vanilla data alone is insufficient for safety training. Conversely, training exclusively on adversarial data [+WJ-adv-only] greatly improves resilience against adversarial attacks but not vanilla cases. We see therefore that both vanilla and adversarial training are essential for resilience against the full range of inputs. Finally, training exclusively on harmful data without benign examples, i.e., [+WJ-harm-only, +WJ-vani-harm-only, +WJ-adv-harm-only], leads to exaggerated safety behaviors.

**The scale of safety data matters for robust model safety.** Figure 4 presents ablations of the impact of scaling up safety data on the overall safety performance of models when combined with T2M-no-refusal.6 We report the satisfactory response rate (satisfactory %), which takes the macro

Figure 4: The increasing scale of vanilla and adversarial data vs. modelâ€™s general and safety capabilities regarding both vanilla and adversarial queries.

average of the inverted attack success rate (1 - ASR) of harmful queries and the inverted refusal rate (1 - RTA) of benign queries. Results in Figure 4 show that even the addition of just 2K safety training items from WildJailbreak results in a significant increase in model safeguarding compared to training with just T2M-no-refusal. However, for a more robust safeguard, we need to introduce substantially more of both vanilla and adversarial data (up to 60K in our experiments when mixed with 150K Tulu2Mix data) to attain sufficiently high safety performance (>95%).

## 6 Related Work

**Red-Teaming and Jailbreaking LLMs.** Early attempts at red-teaming and understanding LLM vulnerabilities have focused on hand-crafting prompts [2; 23; 57; 72]. However, manual methods had quickly become impractical due to their prohibitive costs and lack of scalability. Thus, automated red-teaming and jailbreaking methods are developed for scalable audit of model vulnerabilities . One genre of methods involves computationally expensive gradient optimization that cannot be applied to black-box models and often results in gibberish texts [101; 28; 29; 70]. More related to our work are generation-based approaches that generate jailbreaking prompts directly or through iterative edits [8; 52; 45; 47; 59; 7; 55; 91; 41; 92; 94; 20]. Other jailbreaking works study attacks during decoding time (e.g., decoding configurations , logit manipulation ), in other modalities (e.g., vision-language [71; 89; 69], LLM agents [64; 100]), under multilingual settings [21; 90; 62], in programming mode , through multi-turn interactions [46; 65; 87; 66], through decomposing harmful goals into benign units , or on specific topics of harmful behaviors like the lack of cultural knowledge of LLMs . However, most existing automatic red-teaming and jailbreak methods rarely result in large-scale training resources for model safety enhancement due to their limited coverage of attack strategies and risk types, slow speed, or closed-source access . WildTeaming differs from previous works by efficiently composing _diverse_ adversarial attacks utilizing real-world jailbreak tactics mined from in-the-wild user-chatbot interactions. WildTeaming allows scalable synthetic safety training data generation in addition to simply showing its attack efficacy.

**Safety Evaluation and Training of LLMs.** Many red-teaming efforts on LLMs have been formalized as benchmarks for evaluating model vulnerabilities--these typically are composed of harmful prompts that models should refuse [6; 81; 80; 74; 54; 24; 78; 9]. Meanwhile, to mitigate the potential byproducts of safety training, other benchmarks measure exaggerated safety behavior on benign queries [67; 16]. While LLM safety _evaluation_ has been an active area of research, studies and resources for safety _training_ have been limited, especially in adversarial setting [23; 17; 88]. Most related to our work are Safety-Tuned LLamas and SafeRLHF , which primarily focus on _vanilla harmful_ queries by releasing small-scale safety training datasets and large-scale pairwise preference datasets, respectively. WildTeaming distinguishes from these works by releasing higher quality (shown by our training ablation experiments) and larger scale sequential instruction-tuning data comprised of both _vanilla_ and _adversarial_ queries. WildJailbreak also uniquely contains large-scale _benign_ queries used for mitigating exaggerated safety behavior (i.e., over-refusal). Finally, synthetic data has been used for LLM safety [7; 60; 34; 16]. Close to our work is Rainbow Teaming , which uses synthetic data to populate a grid of attacks based on fixed attack styles and risk categories. However, their data and code are not publicly available. Our work differs in automatically mining diverse human-devised jailbreak tactics rather than manually defining attack styles , creating a large-scale open safety training resource that supports extensive safety training.

## 7 Conclusion

We introduce WildTeaming, an automatic red-teaming framework that mines real users' jailbreak tactics from user-chatbot interactions and composes them combinatorially to build challenging, contrastive jailbreak prompts. Using WildTeaming, we build WildJailbreak: a large-scale dataset consisting of 262K examples that considerably upgrades the complexity and scale of existing open-source safety resources. Our supervised finetuning experiments emphasize the pivotal role of training on both adversarial and vanilla harmful queries in enhancing model safety while mitigating over-refusal. Finally, we show that scaling up the amount of safety data intermixed into standard instruction tuning improves safety behavior without significantly impacting general capabilities.