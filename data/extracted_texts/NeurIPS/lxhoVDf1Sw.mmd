# Predictive Attractor Models

Ramy Mounir  Sudeep Sarkar

Department of Computer Science and Engineering, University of South Florida, Tampa

{ramy, sarkar}@usf.edu

###### Abstract

Sequential memory, the ability to form and accurately recall a sequence of events or stimuli in the correct order, is a fundamental prerequisite for biological and artificial intelligence as it underpins numerous cognitive functions (e.g., language comprehension, planning, episodic memory formation, etc.) However, existing methods of sequential memory suffer from catastrophic forgetting, limited capacity, slow iterative learning procedures, low-order Markov memory, and, most importantly, the inability to represent and generate multiple valid future possibilities stemming from the same context. Inspired by biologically plausible neuroscience theories of cognition, we propose _Predictive Attractor Models (PAM)_, a novel sequence memory architecture with desirable generative properties. PAM is a streaming model that learns a sequence in an online, continuous manner by observing each input _only once_. Additionally, we find that PAM avoids catastrophic forgetting by uniquely representing past context through lateral inhibition in cortical minicolumns, which prevents new memories from overwriting previously learned knowledge. PAM generates future predictions by sampling from a union set of predicted possibilities; this generative ability is realized through an attractor model trained alongside the predictor. We show that PAM is trained with local computations through Hebbian plasticity rules in a biologically plausible framework. Other desirable traits (e.g., noise tolerance, CPU-based learning, capacity scaling) are discussed throughout the paper. Our findings suggest that PAM represents a significant step forward in the pursuit of biologically plausible and computationally efficient sequential memory models, with broad implications for cognitive science and artificial intelligence research. Illustration videos and code are available on our project page: https://ramymounir.com/publications/pam.

## 1 Introduction

Modeling the temporal associations between consecutive inputs in a sequence (i.e., sequential memory) enables biological agents to perform various cognitive functions, such as episodic memory formation , complex action planning  and translating between languages . For example, playing a musical instrument requires remembering the sequence of notes in a piece of music; similarly, playing a game of chess requires simulating, planning, and executing a sequence of moves in a specific order. While the ability to form such memories of static, unrelated events has been extensively studied , the ability of biologically-plausible artificial networks to learn and recall temporally-dependent patterns has not been sufficiently explored in literature . The task of sequential memory is considered challenging for models operating under biological constraints (i.e., local synaptic computations) for many reasons, including catastrophic forgetting, ambiguous context representations, multiple future possibilities, etc.

In addition to the biological constraint, we impose the following set of desirable characteristics as learning constraints on sequence memory models.

* The learning of one sequence does not overwrite the previously learned sequences. This property is defined under the continual learning framework as avoiding catastrophic forgetting and evaluated with the Backward Transfer (BWT) metric .
* The model should uniquely encode inputs based on their context in a sequence. Consider the sequence of letters "EVER"; the representation of "E" at position 1 should be different from "E" at position 3, thus resulting in different predictions: "V" and "R". Moreover, the representation of "E" at position 3 in "EVER" should be different from "E" at position 3 in "CLEVER". Therefore, positional encoding is not a valid solution.
* When presented with multiple valid, future possibilities, the model should learn to represent each possibility separately yet stochastically sample a single valid possibility. Consider the two sequences "THAT" and "THEY"; after seeing "TH", the model should learn to generate either "A" or "E", but not an average  or a union of both .
* The model should be capable of incrementally learning each transition without seeing the whole sequence or revisiting older sequence transitions that are previously learned. This property falls under online learning constraints, also called stream learning [45; 25].
* The learning algorithm should be tolerant and robust to significant input noise. A model should continuously clean the noisy inputs using learned priors and beliefs, thus performing future predictions based on the noise-free observations .

We propose _Predictive Attractor Models (PAM)_, which consists of a state prediction model and a generative attractor model. The predictor in PAM is inspired by the Hierarchical Temporal Memory (HTM)  learning algorithm, where a group of neurons in the same cortical minicolum share the same receptive feedforward connection from the sensory input on their proximal dendrites. The depolarization of the voltage of any neuron in a single minicolum (i.e., on distal dendrites) primes this depolarized neuron to fire first while inhibiting all the other neurons in the same minicolum from firing (i.e., competitive learning). The choice of which neurons fire within the same minicolum is based on the previously active neurons and their trainable synaptic weights to the depolarized neurons, which gives rise to a unique context representation for every input . The sparsity of representations (discussed later in Section 3.2) allows for multiple possible predictions to be represented as a union of individual cell assemblies. The Attractor Model learns to disentangle possibilities by strengthening the synaptic weights between active neurons of input representations and inhibiting the other predicted possibilities from firing, effectively forming fixed point attractors during online learning. During recall, the model uses these learned conditional attractors to sample one of the valid predicted possibilities or uses the attractors as prior beliefs for removing noise from sensory observations.

PAM satisfies the above-listed constraints for a sequential memory model, whereas the current state-of-the-art models fail in all or many of the constraints, as shown in the experiments. Our contributions can be summarized as follows: (1) Present the novel _PAM_ learning algorithm that can explicitly represent context in memory without backpropagation, avoid catastrophic forgetting, and perform stochastic generation of multiple future possibilities. (2) Perform extensive evaluation of PAM on multiple tasks (e.g., sequence capacity, sequence generation, catastrophic forgetting, noise robustness, etc.) and different data types (e.g., protein sequences, text, vision). (3) Formulate PAM and its learning rules as a State Space Model grounded in variational inference and the Free Energy Principle .

## 2 Background and Related Works

### Predictive Coding

Predictive coding proposes a framework for the hierarchical processing of information. It was initially formulated as a time series compression algorithm to create a more efficient coding system [16; 46]. A few decades later, PC was used to model visual processing in the Retina [59; 27] as an inference model. In the seminal work of Rao and Ballard , PC was reformulated as a general computational model of the cortex. The main intuition is that the brain continuously predicts all perceptual inputs, resulting in a quantity of prediction error, which can be minimized by adjusting its neural activities and synaptic strengths. In-depth variational free energy derivation is provided in Appendix A.1.

PC defines two subgroups of neurons: value \(\) and error. Each neuron contains a value node sending its prediction to the lower level \(}_{l}=f_{l+1}(_{l+1})\) through learnable function \(f\), and error node propagating its computed error to the higher level. The total prediction error is computed as \(=_{l}\|(_{l}-}_{l})\|_{2}^{2}\), which is minimized by first running the network value nodes to equilibrium through optimizing the value nodes \(\{_{l}\}_{l=0}^{L}\). At equilibrium, the value nodes are fixed, and inferential optimization is performed by optimizing the functions \(\{f_{l}\}_{l=1}^{L}\). Both optimizations aim to minimize the same prediction error over all neurons. This propagation of error to equilibrium is shown to be equivalent to backpropagation but using only local computations . The PC formulation has shown success in training on static and i.i.d data [66; 69; 57; 23]. More recently , Temporal Predictive Coding (tPC) has also shown some success in sequential memory tasks by modifying error formulation to account for a one-step synaptic delay through interneurons, thus modeling temporal associations between sequence inputs. In the experiments, we compare our model to tPC and its 2-layer variant . A few PC-based models [47; 48] have shown promise for class-incremental continual learning tasks, other PC-inspired models, such as [39; 23; 24], have diverged from the biologically plausible constraints by training through backpropagation through time.

### Fixed-Point Attractor Dynamics

Attractor dynamics refer to mathematical models that describe the behavior of dynamical systems. In our review, we focus on fixed point attractors, specifically Hopfield Networks  (HN), which is an instance of associative memory models [32; 29; 30]. Consider, an ordered sequence of \(T+1\) consecutive patterns \(=[_{t}]_{t=1}^{T+1}\), where \(_{t}\{-1,1\}^{N}\). We refer to the Universal Hopfield Networks (UHN) framework  to describe all variants of HN architecture using a similarity (sim) function and a separation (sep) function, as shown in equation 1. This family of models can be viewed as a memory recall function, where a query \(\) (i.e., noisy or incomplete pattern) is compared to the existing patterns to compute similarity scores using the "sim" function. These scores are then used to weight the projection patterns after applying a "sep" function to increase the separation between similarity scores. The classical HN uses symmetric weights to store the patterns; therefore, it cannot be used to model temporal associations in a sequence. The asymmetric Hopfield Network (AHN)  uses asymmetric weights to recall the next pattern in the sequence for a given query \(\).

\[=}{}}}}{}}}(}(M,)}_{} )=_{t=1}^{T}_{t+1}(}(_{t},))&\\ _{t=1}^{T+1}_{t}(}(_{t},))&\] (1)

When a dot product "sim" function and identity "sep" function are used, we get the classical HN  and AHN . A few variants have been proposed to increase the capacity of the model. Recently,  has extended AHN by using a polynomial (with degree \(d\)) or a softmax function (with temperature \(\)) as the "sep" function. HN can also be applied to continuous dense patterns [34; 51; 12], and extended to sparse modern Hopfield models [28; 67].

### Predictive Learning

Predictive learning takes a more general form of minimizing the prediction error between two views of the same input to improve representations. Many backpropagation-based approaches to predictive learning have been proposed; most recently, JEPA  and its variants [8; 10; 9; 70], learn useful dense representations from images and videos using the predictive objective. Other models, such as [11; 21; 13] - to list a few, use a similar methodology of predicting distorted versions of the same input to learn good feature representations. Prediction-based approaches have also been used to segment videos into events temporally [2; 42] and spatially [44; 43; 1]. More recently, STREAMER  used a predictive learning approach to achieve hierarchical segmentation and representation learning from streaming egocentric videos, where a high prediction error is used as an event boundary. While these biologically implausible approaches show impressive results on their respective tasks, they still suffer from deep learning known limitations, such as catastrophic forgetting and the inability to generate multiple possibilities in regression-based predictive tasks. _Hierarchical Temporal Memory_ (HTM)  is a predictive approach that is heavily inspired by neurobiology. HTM relies on lateral inhibition between neurons of the same minicolumn and sparsity of input representations (i.e., SDR) to learn temporal context and associations using only local Hebbian rules.

HTM can be applied to online tasks, such as anomaly detection  and object recognition , but it is currently incapable of generating future predictions in auto-regressive prediction tasks.

## 3 Predictive Attractor Models

### State Space Model (SSM) formulation

PAM can be represented as a dynamical system with its structure depicted by a Bayesian probabilistic graphical  model, more specifically, a State Space Model, where we can perform Bayesian inference on the latent variables and derive learning rules using Variational Inference. Formally, we define a state space model as a tuple \((,,f,g)\), where \(\) is the latent state space, \(\) is the observation space, and \(f\) and \(g\) are the transition and emission functions respectively (similar to HMM ). We consider a Gaussian form with white Gaussian noise covariance \(_{z}\) for the latent states. However, we assume a latent state \(\) can generate multiple valid possibilities. Therefore, we model the conditional probability \(p(_{t}|_{t})\) as a Multivariate Gaussian Mixture Model (GMM), where each mode is considered a possibility or a fixed-point attractor in an associative memory model. The GMM has \(C\) components with means \(g_{c}(_{t})\), covariances \(_{c}\) and component weights \(w_{c}\). The SSM dynamics can be formally represented with the following equations:

\[_{t}|_{t-1}(f(_{t-1}),_{z}), _{t}|_{t}_{c=1}^{C}w_{c}(g _{c}(_{t}),_{c})\,,\] (2)

where \(_{t}\) and \(_{t}\). From the Bayesian inference viewpoint, we are interested in the posterior \(p(_{t}|_{t},_{t-1})\). Since the functions \(f\) and \(g\) are non-linear, the computation of this posterior is intractable (unlike an LG-SSM, such as Kalman Filter ). Therefore, we utilize variational inference to approximate the posterior by assuming the surrogate posterior \(q()\) has a Gaussian form, and minimize the Variational Free Energy (VFE) . The minimization of VFE (in equation 3) minimizes the KL-divergence between the approximate posterior \(q()\) and the true posterior \(p(_{t}|_{t},_{t-1})\). Derivation 2 of Variational Free Energy is provided in appendix A.1

\[^{T}_{q}[(_{t})}{p(_{t},_{t}|_{t-1})})]}_{}=^{T}_{q}[_{t}|_{t-1}))} ]}_{}+^{T}_{q} [_{t}|_{t}))}]}_{}-_{q}\,,\] (3)

where \(_{q}_{_{t} q(_{t})}\) and \(_{q}\) is the Entropy of the approximate posterior \(q()\). The assumption of Gaussian forms for the latent and observable states can further simplify the negative log-likelihood

Figure 1: State Space Model. (Left): Dynamical system represented by first-order Markov chain of latent states \(\) with transition function \(f\) and an emission function \(g\) which projects to the observation states \(\). (Right): Gaussian form assumptions for the prior \(}\) and posterior \(\) latent states, and the Mixture of Gaussian model representing the conditional probability of multiple possibilities \(p(|)\)

terms (i.e., Latent State Accuracy and Observation Accuracy) to prediction errors. This learning objective encourages the approximate posterior \(q(z)\) to assign a high probability to states that explain the observations well and follow the latent dynamics of the system. We minimize the prediction errors (i.e., learn the transition and emission functions) through Hebbian rules as shown in equations 7 and 8.

**Theorem 1**: _Assume the likelihood \(p(_{t}|_{t})\) in eqn 3 represents multiple possibilities using a Gaussian Mixture Model (GMM) conditioned on the latent state \(_{t}\), as shown in eqn 2. The maximization of such log-likelihood function (i.e., \(_{t}}|_{t})\)) w.r.t a query observation state \(\) is equivalent to the Hopfield recall function (i.e., eqn 1) with the means of the GMM representing the attractors of a Hopfield model. Formally, the weighted average of the GMM means (i.e., \(\{_{c}\}_{c=1}^{C}\)), with the weights being a similarity measure, maximizes the log-likelihood of \(\) under the mixture model._

\[=_{c=1}^{C}(;_ {c},_{c})_{c}^{-1}}{_{c=1}^{C}w_{c} (;_{c},_{c})_{c}^{-1}}}_{ }_{c}}_{}\] (4)

* **Proof:** See derivation 3 in appendix A.2 for the full proof.

### Preliminaries and Notations

Sparse Distributed Representation (SDR)Inspired by the sparse coding principles observed in the brain, SDRs encode information using a small set of active neurons in high dimensional binary representation. We adopt SDRs as a more biologically plausible cell assembly representation . SDRs have desirable characteristics, such as a low chance of false positives and collisions between multiple SDRs and high representational capacity  (More on SDRs in Appendix G). An SDR is parameterized by the total number of neurons \(N\) and the number of active neurons \(W\). The ratio \(S=W/N\) denotes the SDR sparsity. A 1-dimensional SDR \(\) can be indexed as \(^{i}\{0,1\}\), whereas a 2-dimensional SDR \(\) can be indexed as \(^{ij}\{0,1\}\). To identify the active neurons, we define the function \(I:\{0,1\}^{N}^{W}\) to represent the indices of the active neurons in an SDR \(\) as \(I()=\{i|^{i}=1\}\).

Context as Orthogonal DimensionWe transform the high-order Markov dependencies between observation states into a first-order Markov chain of latent states by storing context information in those latent states. The latent states SDRs, \(\{0,1\}^{N_{c} N_{k}}\), are represented with two orthogonal dimensions, where content information about the input is stored in one dimension with size \(N_{c}\), while context related information is stored in an orthogonal dimension with size \(N_{k}\). Therefore, the projection of the latent state \(\) on the first dimension (i.e., \(\)) removes all context information from the state. In contrast, adding context information to an observation state \(\) expands the dimensionality of the state (i.e., \(\)) such that context can be encoded without affecting its content. Competitive learning is enforced on the context dimension through lateral inhibition, effectively minimizing the collisions between contexts of multiple SDRs. We define a projection operator \(\{0,1\}^{N_{c} N_{k}}\{0,1\}^{N_{c}}\). Additionally, we define a projection operator \(\{0,1\}^{N_{c}}\{0,1\}^{N_{c} N_{k}}\) for 1-dimensional SDRs (i.e., \(\)) as shown in equation 5. A detailed list of notations is provided in the supplementary material (Table 1).

\[()^{i}=1& j^{ij}=1,\\ 0&,,()^{ij}=1&^{i}=1,\\ 0&,\] (5)

### Sequence Learning

Given a sequence of \(T+1\) SDR patterns \([_{t}]_{t=1}^{T+1}\), where \(_{t}\{0,1\}^{N_{c}}\), the sequence can be learned by modeling the context-dependent transitions between consecutive inputs within the sequence. We define learnable weight parameters for transition and emission functions, \(^{N_{c}N_{k} N_{c}N_{k}},^{N_{c}  N_{c}}\) respectively. A single latent state transition is defined as \(}_{t}=(_{t-1})=(_{t})\), where \(\) is a threshold function transforming the logits \(_{t}\) to the predicted SDR state \(}_{t}\). The full sequence learning algorithm is provided in algorithm 1.

**Context Encoding through Competitive Learning** Every observation \(_{t}\) contains only content information about the input; we embed the observation with context by expanding the state with an orthogonal dimension (i.e., \(_{t}\)), which activates all neurons in the minicolumns at the indices \(I(_{t})\). Then, for each active minicolum, the neuron in a predictive state (i.e., higher than the prediction threshold) fires and inhibits all the other neurons in the same minicolum from firing (i.e., lateral inhibition), as shown in Equation 6. If none - or more than one - of the neurons are in a predictive state, random Gaussian noise (\(\)) acts as a tiebreaker to select a context neuron. We _do not_ allow multiple neurons to fire in the same minicolumn, which is different from HTM , where multiple cells can fire in any minicolumn (e.g., bursting).

\[m(_{t})^{ij}=1&_{t}^{ij}=(\{(_{t}^{ij})+\}_{j=0}^{N_{k}}),\\ 0&,,_{t}=(_{t}) m(_ {t})\] (6)

State Transition LearningThe transition between latent states is learned through local computations with Hebbian-based rules. We modify the synaptic weights \(\) to model the transition between pre-synaptic neurons \(_{t-1}\) and post-synaptic neurons \(_{t}\). Only the synapses with active pre-synaptic neurons are modified. The weights operate on context-dependant latent states (i.e., \(_{t-1}_{t}\)); thus, the learning of one transition does not overwrite previously learned transition of different contexts, regardless of the input contents (i.e., \(_{t-1}\)). We use the learning constant coefficients \(_{A}^{+}\) and \(_{A}^{-}\) to independently control learning and forgetting behavior, as shown in equation 7. A lower \(_{A}^{-}\) encourages learning multiple possibilities by slowing down the forgetting behavior.

\[=^{+}_{t-1}_{t}}_{ _{}}^{T}+^{-}_{t-1} (1-_{t})^{T}}_{_{}}\] (7)

Contrastive Attractors FormationThe attractors are formed in an online manner by contrasting the input observation \(_{t}\) to the predicted union set of possibilities \(}_{t}\). The goal is to learn excitatory synapses between active neurons of \(_{t}\), and bidirectional inhibitory synapses between \(_{t}\) and the union set of predicted possibilities _excluding_ the \(_{t}\) possibility (i.e., \((}_{t})_{t}\)), as shown in equation 8.

\[=^{+}_{t}_{t}}_{ _{}}+^{-}[_{t} ((}_{t})_{t})^{T}+(( }_{t})_{t})_{t}^{T} ]}_{_{}}\] (8)

```
1:procedureFrainst(\(\))
2:\(_{0}=(_{0}) m(_{0})\)\(\)Eqn. 6
3:for\(t=1\) to \(T+1\)do
4:\(_{t}=_{t-1}\)
5:\(}_{t}=(_{t}) m(_{t})\)
6:\(=+\)\(\) Update \(A\) via Eqn. 7
7:\(}_{t}=(_{t-1})\)
8:\(=+\)\(\) Update \(B\) via Eqn. 8
9:endfor
10:endprocedure ```

**Algorithm 2** : **Sequence Generation**. Given a noisy sequence (i.e., online), or the first input in a sequence (i.e., offline). The model uses the learned functions \(\) and \(\) to generate the full sequence. \(\) denotes sampled from (eqn 9).

### Sequence Generation

After learning one or multiple sequences using algorithm 1, we use algorithm 2 to generate sequences. First, we define two generative tasks: online and offline. In online sequence generation, a noisyversion of the sequence is provided as input, and the model is expected to generate the original learned sequence. In offline sequence generation, the model is only provided with the first input, and it is expected to generate the entire sequence auto-regressively. For cases with equally valid future predictions (e.g., "a" and "e" after "TH" in "THAT" and "THEY"), the model is expected to stochastically generate either one of the possibilities (i.e., "THAT" or "THEY"). The online generation task is a more challenging extension of the online recall task in , where the noise-free inputs are provided, and the model only makes a 1-step prediction into the future. During offline sequence generation, the model randomly samples from the union set of predictions \(}\) a single SDR with \(W\) active neurons (equation 9) to initialize the iterative attractor procedure. \(\) denotes a random permutation function. This random permutation function allows the model to randomly generate a different sequence with every generation.

\[}^{i}=1&i\{(I( }_{t}))^{w}\}_{w=0}^{W},\\ 0&\] (9)

## 4 Experiments

### Evaluation and Metrics

MetricsTo evaluate the similarity of two SDRs, we use the Jaccard Index (i.e., IoU)1, which focuses on the active bits in sparse binary representations. Since the sparsity \(S\) of the representations can change across experiments and methods, we normalize the IoU by the expected IoU (Derived in Theorem 2 in Appendix A.3) of two random SDRs at their specified sparsities. The normalized IoU is computed as \(-[]}{1-[]}\). We use the Backward Transfer  metric in evaluating catastrophic forgetting. Mean Squared Error (MSE) is used to compare images for vision datasets.

DatasetsWe perform evaluations on synthetic and real datasets. The synthetic datasets allow us to manually control variables (e.g., sequence length, correlation, noise, input size) to better understand the models' behavior across various settings. Additionally, we evaluate on real datasets of various types (e.g., protein sequences, text, vision) to benchmark PAM's performance relative to other models on more challenging and real sequences. For all vision experiments, we use an SDR autoencoder to learn a mapping between images and SDRs (Details on the autoencoder are provided in Appendix C). We run all experiments for 10 different seeds and report the mean and standard deviation in all the figures. More experimental details and results are provided in Appendices C and E.

Figure 2: Sequence Generation. (Left): Offline generation by sampling a single possibility (i.e., attractor point) from a union of predicted possibilities. (Right): Online generation by removing noise from an observation using the prior beliefs about the observed state. Markov Blanket separates the agent’s latent variables from the world’s observable states.

### Results

We align our evaluation tasks with the desired characteristics of a biologically plausible sequence memory model, as listed in the introduction. We show that PAM outperforms current predictive coding and associative memory SoTA approaches on all tasks. Most importantly, PAM is capable of long context encoding, multiple possibilities generation, and learning continually and efficiently while avoiding catastrophic forgetting. These tasks pose numerous significant challenges to other methods.

Offline Sequence CapacityWe evaluate the models' capacity to learn long sequences by varying the input size \(N_{c}\), model parameters (e.g., \(N_{k}\)), and sequence correlation. The correlation is increased by reducing the number of unique patterns (i.e., vocab) used to create a sequence of length \(T\). Correlation is computed as \(1.0-}{T}\). In Figure 3**A**, we vary the input size \(N_{c}\) and ablate the models to find the maximum sequence length to be encoded and retrieved, in an _offline_ manner, with a Normalized IoU higher than 0.9. We set the number of active bits \(W\) to be 5 unless otherwise specified. Results show that Hopfield Networks (HN) fail to learn with sparse representations; therefore, we use \(W\) of \(0.5N_{c}\) only for HN and normalize the IoU metric accordingly. PAM's capacity significantly increases with context neurons \(N_{k}\), as expected. HN's capacity also increases with the polynomial degree \(d\) of its separation function; however, as shown in Figure 3**B**, the capacity sharply drops as correlation increases. PAM retains its capacity with increasing correlation, reflecting its ability to encode context in long sequences (i.e., high-order Markov memory). This context encoding property is also demonstrated in the qualitative CIFAR  results in Figure 3**E** and **F**, where a short sequence of images with high correlation is used. The model must uniquely encode the context to correctly predict at every step in the sequence. While PAM correctly predicts the full context, single-layer tPC learns to indefinitely alternate between the patterns, while two-layered tPC attempts to average its predictions. AHN shows similar low performance and failure mode as in .

Catastrophic ForgettingTo assess the model's performance in a continual learning setup, we sequentially train each model on multiple sequences and compute the Backward Transfer (BWT) 

Figure 3: Quantitative results on (**A-B**) Offline sequence capacity, (**C**) Noise robustness, and (**D**) Time of sequence learning and recall. Qualitative results on highly correlated CIFAR sequence in (**E**) offline and (**F**) online settings. The mean and standard deviation of 10 trials are reported for all plots.

metric by reporting the average normalized IoU on previously learned sequences after learning a new one. In Figure 4**A**, we report BWT for 50 synthetically generated sequences with varying correlations. AHN can avoid catastrophic forgetting when the patterns are not correlated, whereas tPC fails to retain previous knowledge regardless of the correlation value. PAM, with high enough context \(N_{k}\), does not overwrite or forget previously learned sequences after learning new ones but performs poorly when \(N_{k}=1\), as expected. We repeat the experiment on more challenging sequences from ProteinNet , which contains highly correlated (\(>0.9\)) and long sequences (details in appendix). The results in Figure 4**B** show a similar trend with PAM requiring more context neurons \(N_{k}\) to handle the more challenging data. Qualitative results on moving-MNIST  in Figure 4**F** further demonstrate the catastrophic forgetting challenge where the learning of the second sequence overwrites the learned sequence. PAM successfully retrieves the previously learned sequence while the other models fail.

Multiple Possibilities GenerationIn addition to accurately encoding input contexts, PAM is designed to represent multiple valid possibilities and sample a single possibility. We perform evaluation on a dataset of four-letter English words (details in appendix), which includes many possible future completions (e.g., "th[is]", "th[at]", "th[em]", etc.) We train PAM on the list of letters sequentially (i.e., one word at a time); the other methods are trained in a simpler batched setup as in  because they suffer from catastrophic forgetting. This puts PAM at a disadvantage, but as shown in Figure 4**C**, PAM still significantly outperforms the other methods in accurately generating valid words (high average IoU) in an offline manner. Both tPC and AHN fail to generate meaningful words when trained on sequences with multiple future possibilities. Figure 4**D** further demonstrates the stochastic generative property of PAM. We show PAM's ability to recall more of the dataset as it repeats the generation process, whereas PC and AHN fail in the dataset recall task.

Online Noise RobustnessThe online generation ability of PAM shown in Figure 2 allows the model to use the learned attractors to clean the noisy observations before using them as inputs to the predictor. This step allows the model to use its prior belief about future observations to modify the noisy inputs. In Figure 3**C**, we evaluate the models' performances by changing a percentage of the active bits during online generation. PAM is able to completely replace the noisy input with its prior belief if it does not exist in the predicted set of possibilities \(}\). In contrast, the other methods use the noisy inputs, thus hindering their performances. We provide qualitative results on CLEVRER 

Figure 4: Qualitative results on (**A**) synthetic and (**B**) protein sequences backward transfer, and (**C-D**) multiple possibilities generation on text datasets. Qualitative results on (**E**) noise robustness on CLEVRER sequence, and (**F**) catastrophic forgetting on Moving MNIST dataset. **a** highlights the first frame with significant error. The mean and standard deviation of 10 trials are reported for all plots.

in Figure 4**E**; PAM retrieves the original sequence despite getting noisy inputs (40% noise), and outperforms the other models. Interestingly, tPC performs reasonably well on this task despite the added noise.

EfficiencyWe report, in Figure 3**D**, the time each model requires to learn and recall a sequence. For this study, we use input size \(N_{c}=100\) and vary the sequence length. PAM operates entirely on _CPU_. The results show that a single-layer tPC model requires more time than all PAM variants (\(N_{k} 24\)). Additionally, a two-layered tPC requires two to three orders of magnitude more time than PAM or single-layered tPC, significantly limiting its scalability and practicality when applied to real data with long sequences.

## 5 Limitations

PAM requires that its representations be sparse and binary (i.e., SDRs) in order to represent multiple possibilities as a union of SDRs with minimal overlap. Therefore, PAM cannot be directly applied to images in the input space like Dense Hopfield Networks. We argue that the neocortex encodes sensory input into SDRs for processing and does not operate directly on the input in its dense representation. Methods that operate directly on dense representations (e.g., images) are arguably less biologically plausible as the neocortex uses sparse binary representations (i.e., cell assemblies with a small fraction of firing neurons) to store and manipulate information. This paper focuses on learning multiple sequences without catastrophic forgetting and stochastically generating multiple possibilities efficiently, and we assume the sequence is provided in SDR format. Additionally, methods that operate on the input directly face challenges when the input is naturally sparse (see Figure 3**A**). Therefore, it is useful to encode the input into a representation with fixed sparsity before applying sequential memory learning algorithms. In future work, we plan to investigate how to encode high dimensional complex inputs (e.g., images) in a compositional part-whole structure of SDRs where we can apply PAM at different levels of abstraction.

## 6 Conclusion

We proposed _PAM_, a biologically plausible generative model inspired by neuroscience findings and theories of cognition. We demonstrate that PAM is capable of encoding unique contexts with tremendous scalable capacity that is not affected by sequence correlation or noise. PAM is a generative model; it can represent multiple possibilities as a union of SDRs (a property of sparsity) and sample single possibilities, thus predicting multiple steps in the future despite multiple possible continuations. We also show that PAM does not suffer from catastrophic forgetting as it learns multiple sequences. PAM is trained using local computations through Hebbian rules and runs efficiently on CPUs. Future directions include hierarchical sensory processing and higher-order sparse predictive models.