# Efficient Evaluation of LLMs via Branching Preference Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large language models (LLMs) have made significant advances across various generative tasks, progressing toward achieving near-human levels of intelligence. However, in many scenarios, LLMs face the challenge of insufficient human evaluation or even the inability to evaluate reliably. Particularly, in complex dialogue scenarios involving diverse and intricate user intents, LLMs as evaluators of AI responses exhibit a substantial gap compared to humans. Moreover, due to the scarcity of high-quality evaluation data, LLMs exhibit deficiencies in their evaluation capabilities. In this work, we conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. We demonstrate that within a limited search space, there exist better decision-making behaviors that facilitate the model in making reasonable and accurate judgments. Specifically, we propose a tree-based data sampling method to generate supervised data and preference pairs derived from the evaluation tree. Furthermore, we introduce preference learning based on the DPO algorithm, which empowers the fine-grained evaluation model to explore and learn better branching strategies within budget-limited scenarios. Our model significantly reduces the dependency on labeled data and demonstrates strong performance across three different evaluation settings: in-distribution, out-of-distribution, and transfer evaluation. Experiments indicate that our model can reduce inference costs by 90% compared to conducting searches across the entire evaluation tree, thereby significantly enhancing efficiency.

## 1 Introduction

Dialogue evaluation capability  is one of the fundamental abilities of human social interaction, involving the comprehension and interpretation of user intentions, as well as providing reasonable judgments on the correctness of different responses. Automated evaluation can assist humans to supervise powerful LLMs and is an essential component for superalignment and weak-to-strong generalization techniques . However, human evaluations [3; 22] are labor-intensive and time-consuming, making it difficult to widely adopt. Traditional automated evaluation approaches [18; 39; 8] are limited by inherent deficiencies, such as string and semantic matching methods often yield subpar accuracy and lack of interpretability. The advent of large language models offers promise for automatically evaluating dialogue quality [19; 41; 15], owing to their high consistency with humans in intent understanding.

Nevertheless, automated evaluation remains a challenging issue due to the diversity of tasks and scenarios it may encounter. The user queries often encompass multiple intentions , which cannot typically be addressed using a single evaluation criterion. However, related research [35; 42] often attempts to treat evaluation as a simplistic 'one-step' reasoning problem, causing even the mostpowerful large language models to struggle to provide reasonable and accurate results. It is essential for the evaluation model to adapt to different scenarios and provide critical evaluation criteria.

In this work, **we do not introduce any human prior for evaluation scenarios and criteria**, which are commonly used for designing and collecting training data in related studies [14; 11]. The real-world conversational scenarios are often characterized by complexity and unpredictability, making it challenging to derive generalizable rules. Additionally, human priors frequently introduce biases [12; 20], making these evaluation methods poorly generalized due to a lack of adaptability and scalability. Therefore, we explore automatically sampling scenarios from large-scale datasets and employ LLMs to automatically generate evaluation criteria, aiming to eliminate human labor as much as possible. Another significant challenge is the lack of ground truth labels and human feedback during the training data collection process. The insufficient of available supervised data for evaluation tasks also prevents them to scale effectively.

Despite various challenges, we discover that **the evaluation model is constrained in its ability to identify crucial evaluation criteria, but this limitation can be mitigated by increasing the number of considered criteria**. As shown in Figure 1, the Initial model can achieve nearly a 10 point improvement in the agreement metric by increasing the number of evaluation branches. This findings motivate us to design tree-based data sampling methods to generate training data and a branching preference learning algorithm to improve "multi-step" inference capability. Specifically, we employ a breadth-first growth approach to construct an evaluation tree, where each path from the root to a leaf node represents a complete evaluation trajectory. We collect high-quality evaluation trajectories from the search space of the evaluation tree and trained an SFT model, which exhibited superior performance and prediction consistency. Furthermore, we refine these evaluation trajectories and train a DPO model , which can effectively prioritize and output crucial evaluation criteria, thereby enhancing the model's inference effectiveness.

We mainly evaluate our models in three settings: in-distribution, out-of-distribution, and transfer evaluation. Specifically, we use the datasets from the Chatbot Arena 1 as in-distribution data, and collect data from large-scale dialogue datasets without human priors as out-of-distribution data. In our experiments, we demonstrate that (1) our model outperforms several recent open-source evaluation models and methods across all three settings, (2) there is a noticeable improvement in the evaluation model's capability when progressively training the Initial model, the SFT model, and the DPO model, and (3) as shown in Figure 1, our DPO model achieves the best performance even when using only a single evaluation criterion (single inference branch).

## 2 Related Works

Automated dialogue evaluation  has long been a significant challenge in the field of generative AI. Recent work [10; 7; 35; 41] has demonstrated that LLMs can act as automated evaluators, serving as alternatives to human judges. However, LLMs still exhibit issues such as positional bias and prediction inconsistency [34; 40]. Many studies have relied heavily on human priors [14; 11], thereby neglecting to explore the model generalization capabilities. In contrast, our research focuses on examining the performance with different data distributions and investigates how to bridge this gap.

Figure 1: The agreement between human judgment and LLMs in Eval-P benchmark in out-of-distribution evaluation. Auto-J serves as the “one-step” evaluation baseline, while Fennec is the “multi-step” baseline. The Initial, SFT, and DPO models were trained using our generated data.

We consider automated evaluation as a complex reasoning task and aim to improve model performance by optimizing reasoning trajectories. When handling such tasks, LLMs typically utilize decision trees [37; 23] to model the reasoning process. They often employ search algorithms like A* [21; 13] or Monte Carlo Tree Search (MCTS) [29; 31] to identify the optimal reasoning path within the candidate decision. However, these methods generally rely on deterministic reward signals or feedback, which are absent in our settings. We demonstrate that the ensemble boundary of the evaluation branches provides a feasible reward signal to verify the accuracy of the reasoning trajectories. Based on this, we can guide the model to generate a substantial amount of high-quality data.

Automated evaluation is also a pivotal technology within scalable oversight, aiming to enhance humans' ability to supervise models. For example, humans may ask models to critique the outputs of other models [9; 28] or use models to help decompose a problem into simpler subproblems . In contrast to improving human supervision, we focus on how to conduct reliable automated evaluations. Certainly, our proposed evaluation methods and results can also be combined with human oversight to provide even better performance.

## 3 Problem Setup

In this work, our primary focus is on evaluating AI responses, particularly in analyzing query and response pairs within given datasets to determine which response is better 2. Traditional approaches [41; 35] regard the evaluation task as a "one-step" classification ("_win_" or "_lose_" or "_tie_") or generation problem, where the final scores or explanations are assigned by a reward model or the evaluation model. However, with complex reasoning tasks or scenarios, a given query may involve multiple intents, whether explicit or implicit , yet the generated responses by AI often overlook some of these intents, constrained by the model's capabilities. Therefore, multiple evaluation criteria are required  to verify whether the responses address the query requirements and align with user intentions. Considering the complexity and diversity of dialogue tasks, it remains an intractable challenge to gather comprehensive and accurate evaluation criteria.

### Conducting evaluation through multi-step reasoning

We try to view the evaluation task as a complex reasoning task, a multi-step generative problem, which entails: (1) initially seeking suitable evaluation criteria, then (2) generating scoring guidelines based on these criteria, and finally (3) conducting comprehensive judgment based on the aforementioned criteria and scoring guidelines. Formally, given a dialogue \(\), we will use an evaluation model to sequentially obtain the criterion \(\), scoring guideline \(\), and judgment \(\):

\[_{}(|_{},X), _{}(|_{},,X),_{}(|_{}, ,,X),\] (1)

where \(_{}\) represents the evaluation policy, the prompt please refer to Appendix A.3. Similar to related multi-branch evaluation [27; 16] methodologies, we refer to different reasoning paths as _"evaluation branch"_, where each branch represents a decision-making process. Unlike previous methods  that relied on enumerating criteria, our goal is for the evaluation model to automatically generate crucial and high-priority criteria.

### Focusing on two challenges

A natural approach is to first construct a candidate set of criteria and then derive suitable results based on these criteria. To address this task, we focus on the following two challenges:

* _How to construct an appropriate candidate set?_ Our aim is to develop a candidate set that includes multiple evaluation branches enriched with high-quality evaluation opinions. By training and optimizing this candidate space to advance desired behaviors, we can swiftly identify appropriate and critical judgments during the inference process.
* _How to rank the judgments?_ We also need to establish a ranking among different evaluation branches to optimize the candidate space. In contrast to recent studies , our evaluation dataset lacks ground truth labels or environmental feedback to act as reward signals. The cost of obtaining these signals is prohibitive, requiring not only expensive human labor but also facing issues of low consistency among humans in many ambiguous problems. Therefore, we need to design an innovative and cost-effective approach to address this challenge.

## 4 Method

Figure 2 illustrates an overview of our method, which involves three stages for model training: First, we train the Initial model to construct the evaluation tree; Then, we sample different evaluation branches as supervised data to train the SFT model, enhancing branch prediction consistency; Finally, we collect preference data to train the DPO model, ensuring rapid sampling of critical branches.

### Collecting dialogue dataset

Evaluation models typically rely on robust generalization capabilities to effectively handle diverse dialogue tasks. Consequently, the distribution of training data significantly affects performance on unseen tasks encountered during real-world evaluations. To address this, we sampled from a large-scale dialogue dataset rather than a specific data source. We then apply the K-Means algorithm  to cluster the data. Subsequently, we sample data from these clusters, ensuring that the training dataset encompasses a diverse set of dialogue scenarios. More details refer to Appendix A.1

### Training initial model

We aim to construct a dataset from scratch for evaluation, consisting of dialogues paired with their corresponding evaluation trees. Each tree contains different reasoning paths during the evaluation of dialogues. The root node of this tree represents the dialogue data, and each path from the root node to a leaf node signifies an evaluation branch. Each evaluation branch comprises three decision-making behavior nodes: _criterion_\(\), _scoring guideline_\(\), and _judgment_\(\). To simulate this decision process, we introduce a multi-branch training approach  to train an LLM as the initial policy \(_{}\). We employ GPT-4 (gpt-4-0125-preview)  to generate corresponding multi-branch training data to enhance quality. This approach ensures that the model can auto-regressively generate evaluation branches using Equation 1.

### Generating evaluation tree

We expand the branch candidates sampled from the policy \(_{}\) using the breadth-first growth, thereby including as many high-quality evaluation paths as possible. Due to the different paradigms of SFT and DPO, we employ consistency pruning to split the sampling space to obtain training data:

* **Breadth-first Growth:** The evaluation tree contains two distinct growth manner: for _criterion_\(\) node, we use LLM's brainstorming capability to generate \(k\) relevant criteria; for _scoring guideline_\(\) and _judgment_\(\) node, we use sampling method by adjusting the LLM's temperature and top-p parameters. To simplify, we utilize the Initial model \(_{}\) to generate a complete binary tree for each subtree with a criteria node as its root. Furthermore, since the evaluation task requires testing the model's consistency by swapping response positions, we can obtain \(k 8\) different evaluation branches.

Figure 2: Compared to single-chain inference, we adopt a multi-branch based approach to train the Initial model. Subsequently, we construct an evaluation tree through a series of growth and pruning operations. This tree then guides the training both of the SFT model and the DPO model.

* **Consistency Pruning:** Prior to pruning, we introduce two different consistency constraints: self-consistency, meaning the same _criterion_\(\) and _scoring guideline_\(\) should yield the same _judgment_\(\), and positional consistency, meaning that swapping positions should not affect the _judgment_\(\). Subsequently, we obtain SFT training data from evaluation branches in the evaluation tree that meet both consistency constraints, and DPO training data from nodes that do not satisfy these constraints.

### Collecting preference labels

Although we can obtain SFT and DPO data from the consistency sampling space, this data lacks correctness verification. Typically, preference data requires human annotation to establish ranking sequences, a time-consuming process that is not suitable for scaling. Therefore, we propose two alternative approaches to label each evaluation branch with its correctness:

* **Branch Ensemble:** Considering that there are only three final labels for _judgment_ ("_win_" or "_lose_" or "_tie_"), we use an ensemble result of evaluation branches to obtain the consensus label. The ensemble method provides a lower bound of judge error without incurring additional costs. For SFT data, we filter out data that is inconsistent with the ensemble results. For DPO pair data, we select samples consistent with the ensemble results as "chosen" samples, and those inconsistent as "rejected" samples.
* **LLM-as-a-Judge:** Some highly aligned LLMs, such as GPT-4, possess powerful annotation capabilities. Therefore, we use LLMs to determine which sample in the DPO pairs data is more reasonable as the "chosen" sample. In our experiments, we found that this method has only a 20% disagreement rate compared to the Branch Ensemble method. We analyze this method in Section 5.4

As shown in Figure 3, we combine consistency pruning and automated labeling to collect the corresponding preference data. Through the labeling of judgments, we can also obtain preference information for _criterion_\(\) and _scoring guideline_\(\) based on the final _judgment_\(\) decisions. Specifically, we prioritize predicting criteria that lead to correct judgments and select the scoring guidelines with the highest overall scores as the "chosen" samples. Additionally, we randomly sample from the filtered data to create the training set, thereby controlling training costs and efficiency.

### Training Sft model and Dpo model

We use the Initial model as the starting point to train the SFT model \(_{}\) using supervised learning, which reduces inconsistent predictions compared to the initial policy. Then, we take the SFT model as the initialization to train the DPO model \(_{}\) using Direct Preference Optimization, which can learn the decision priorities of different branches, with the objective:

\[_{}(_{}|_{})=- _{(x,y_{c},y_{r})}[(}(y_{c} |x)}{_{}(y_{c}|x)}-}(y_{r}|x)}{ _{}(y_{r}|x)})],\] (2)

where the \((x,y)\) represents data pair of different decision tasks in Equation 1, \(y_{c}\) represents the "chosen" sample, and \(y_{r}\) represents the "rejected" sample.

During the inference process, we create a single branch for each criterion to conduct evaluation, and control the number of generated branches \(k\) to adjust the inference efficiency. Since the DPO model employs sampling optimization, it usually achieves optimal performance with only a few branches.

Figure 3: The figure illustrates how the training dataset of the SFT and DPO models is sampled from an evaluation subtree based on a specific criterion.

## 5 Experiments

As the most popular LLM evaluation platform recently, Chatbot Arena demonstrates high alignment with human judgments in pairwise response evaluations. We collect its open-source human judgment benchmark, Eval-P and MT-bench, to serve as the test set. We gather training data comprising both dialogue data and evaluation data for the following three evaluation scenarios:

1. **In-distribution evaluation:** We apply the Fennec  training data to train the In-distribution (ID) model, which included 3K dialogue data from Auto-J , along with evaluation data annotated by GPT-4. This training data is a multi-branch dataset, meaning that a single dialogue includes multiple evaluation branches.
2. **Out-of-distribution evaluation:** We collect 5M large-scale dialogue data and extracted 7K samples from it to serve as out-of-distribution (OOD) training data. GPT-4 annotate 3K evaluation samples from this dataset for the Initial model training.
3. **Transfer evaluation:** We use 3K OOD training data (which includes evaluation data) and 2K ID dialogue data (which did not include evaluation data) to train the transfer model.

For each benchmark, we employ Agreement (**AGR**) and Consistency (**CNS**) as performance metrics. Consistency measures the prediction consistency of the evaluation model when the positions of the responses are swapped. Agreement quantifies the proportion of evaluations that meet the criteria for swap consistency and align with human judgments. In many cases, the "_tie_" label indicates an inability to distinguish performance under some evaluation criteria. However, it may still be distinguishable under specific evaluation criteria. Therefore, we also present the model performance on the test data without "_tie_" label. For more details, please refer to Appendix A.2

### In-distribution evaluation

The results are shown in Table 1, where methods marked with \(\) denote our reimplementations. Since the Initial model leverages the Fennec training data for initialization, its performance can be

    &  &  &  &  &  &  \\  & & & **AGR \(\)** & **CNS \(\)** & **AGR \(\)** & **CNS \(\)** & **AGR \(\)** & **CNS \(\)** & **AGR \(\)** & **CNS \(\)** \\   \\  Auto-J \(\) & 13B & 1 & 55.13 & 82.44 & 74.13 & 87.26 & 44.20 & 70.74 & 55.98 & 72.30 \\ Fennec \(\) & 7B & 1 & 55.36 & 83.80 & 68.63 & 86.33 & 52.88 & 82.18 & 63.42 & 85.63 \\  & & 5 & 55.80 & 85.52 & 74.14 & 89.19 & 53.88 & 84.41 & 68.04 & 87.38 \\   \\  SFT & 7B & 1 & 56.68 & 86.64 & 70.76 & 89.11 & 53.29 & 88.43 & 66.64 & 90.25 \\  & & 5 & 55.96 & 86.57 & 72.91 & 88.13 & 53.08 & 87.99 & 67.96 & 90.17 \\  DP0 & 7B & 1 & 55.24 & 84.26 & 69.87 & 86.95 & 53.29 & 83.04 & 62.96 & 85.23 \\  & & 5 & **57.18** & 85.63 & 74.88 & 88.52 & 53.43 & 83.97 & 66.48 & 86.84 \\   \\  GPT-4  & - & - & 62.28 & 86.28 & - & - & - & - & - & - \\ GPT-4 \(\) & - & - & 55.93 & 78.43 & 74.56 & 83.79 & 57.78 & 83.51 & 73.11 & 86.19 \\ GPT-3.5 \(\) & - & - & 44.41 & 72.39 & 59.86 & 73.57 & 49.55 & 74.13 & 62.50 & 77.22 \\   \\  Initial & 7B & 1 & 49.64 & 83.69 & 57.02 & 84.59 & 50.76 & 82.94 & 56.35 & 83.64 \\  & & 10 & 53.16 & 85.13 & 66.93 & 86.06 & 54.25 & 88.10 & 66.65 & 89.62 \\  SFT & 7B & 1 & 54.59 & 87.14 & 70.56 & 88.52 & 55.23 & 88.97 & 67.38 & 90.76 \\  & & 10 & 55.10 & 87.86 & 73.69 & 89.99 & 54.69 & 89.84 & 69.48 & 92.12 \\  DP0 & 7B & 1 & 55.89 & 89.44 & 75.76 & 90.67 & 55.74 & 91.45 & 71.69 & 93.36 \\  & & 3 & 56.75 & **90.37** & **77.23** & 92.24 & **55.89** & **92.49** & **72.08** & **94.45** \\   \\  SFT & 7B & 1 & 54.17 & 87.36 & 70.95 & 89.01 & 53.77 & 88.67 & 66.91 & 90.56 \\  & & 10 & 55.96 & 89.00 & 75.56 & 90.87 & 53.68 & 88.94 & 68.97 & 91.34 \\  DP0 & 7B & 1 & 56.11 & 89.30 & 76.54 & 91.65 & 54.81 & 91.08 & 71.48 & 93.09 \\  & & 5 & 56.39 & 90.01 & 77.04 & **92.54** & 55.10 & 91.78 & 71.73 & 93.52 \\   

Table 1: The Initial, SFT, and DP0 are our trained models from three training stages. We select the best performance results by varying branches. **Bold** numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.

regarded as its in-distribution evaluation baseline. As observed, the SFT and DPO models exhibit significant performance improvements over most baseline methods on both the Auto-J and Fennec datasets, achieving the highest agreement score of 57.18. In the multi-turn dialogue evaluation on MT-bench, the Fennec dataset comprises only single-turn dialogues, which constrains its effectiveness in handling multi-turn context information. Additionally, we observed the instabilities problems during the training process, which hindered the DPO model from outperforming the Initial model. A more comprehensive analysis of these instability problems is provided in Section 5.7.

### Out-of-distribution evaluation

In terms of OOD evaluation, the Initial model performs worse than the baseline model on both Eval-P and MT-bench benchmarks, due to the distribution shift in the dialogue dataset. With RLHF  training, the SFT model significantly surpasses the Initial model in consistency rate and also enhances the agreement rate. Notably, the DPO model achieves superior performance with only three branches, thereby reducing inference latency by over 60%. In evaluation settings without "_tie_" labels, the advantage of the DPO model becomes more apparent, significantly outperforming other models, including proprietary model GPT-4. This demonstrates that the DPO model can effectively distinguish between responses using critical criteria, even when employing only \(3\) branch for inference. Furthermore, our models are capable of handling multi-turn dialogue scenarios, achieving performance that surpasses the in-distribution models. These extremely strong results indicate that our model excels at identifying more crucial criteria to help distinguish the difference of AI's responses.

### Transfer evaluation

The purpose of transfer evaluation is to evaluate the model's capability to adapt to in-distribution data, thus mitigating the problem of training data distribution shift. It can be observed that both the SFT and DPO models demonstrate improvements across multiple benchmarks compared to the Initial model. Notably, in both OOD and transfer evaluation settings, the DPO model consistently achieves better performance than the SFT model, while also reducing the number of inference branches. Although the transfer model does not surpass the OOD model, it still achieves closed performance. In Section 5.4, we provide a detailed analysis of the different scenarios that lead to these models exhibiting significantly different performance characteristics despite their close overall performance.

### Scenario analysis

To investigate the impact of scenario categories distribution in the training data, we need to analyze the scenarios within the OOD, ID training sets, and the Eval-P test set. For this purpose, we employ the scenario classifier trained by Auto-J, which effectively categorizes dialogue data into 58 different scenarios. Figure 4 presents the distribution of scenarios. It can be observed that Auto-J's training set is well-balanced across the predefined scenarios, closely matching the distribution of the Eval-P test set. In contrast, within the OOD data, the "Others" category exceeds 30%, and "General Communication" surpasses 50%. The significant differences in scenario distributions between the OOD data and the test set can lead to performance variations in test cases.

Figure 4: The scenario contains seven categories, including Summarization, Exam Questions, Rewriting, Code, Functional Writing, Creative Writing, General Communication, NLP Tasks, and Others.

From the evaluation results of fine-grained scenarios, we can derive several interesting observations from Table 2: (1) The ID and Transfer models significantly outperform the OOD model in Summarization and Exam Questions, which are notably lacking in the OOD training data. (2) The OOD model performs significantly better than the ID and Transfer models in the General Communication and "Others" categories. (3) For writing-related text generation tasks, the OOD model achieves performance that is comparable to the ID model. These results indicate that the type and quantity of tasks remain crucial in evaluation tasks. Therefore, the evaluation model can achieve combinatorial generalization capability by increasing the number of scenarios or tasks. When GPT-4 serves as a judge to provide preference labels, it achieves improvement in code and NLP tasks compared with DPO model but also affects performance in other scenarios.

### Dialogue correction

The critical capability of evaluation is to identify and rectify flaws in dialogues, thereby enhancing the overall quality of the original AI responses. Therefore, we test our model's ability to evaluate and correct dialogues generated by the Alpaca-13B  and the LLaMA2-7B Chat  models in MT-Bench. Unlike previous pairwise evaluations, MT-Bench presents a multi-turn dialogue and uses GPT-4 to assign scores (ranging from 1 to 10) to different AI responses, subsequently giving the model ranking relationship based on these scores.

Specifically, to elicit the model's correction ability, we construct 3k correction pairs and incorporate them into the evaluation training set. When performing corrections, we first generate a judgment for the responses and then modify those with scores below 3. As illustrated in Table 3, the modification rates for Alpaca are all above 95%, indicating that the quality of responses generated by weak models is generally subpar. After refinement, both Alpaca-13B and LLaMA2-7B Chat model achieve better scores. Moreover, the correction results of the DPO model outperform those of the SFT model, demonstrating that better evaluation feedback can lead to significant improvements in evaluation quality. These results not only demonstrate the effectiveness of our model in identifying and correcting dialogue flaws but also highlight its potential to substantially improve the performance of dialogue systems through robust evaluation.

### Impact of Initial model data scale

In our investigations, we strive to reduce reliance on both human annotators and GPT-4. Specifically, in the current work, we trained an Initial model using annotation data generated by GPT-4 without any addi

  
**Settings** & **AGR\(\)** & **CNS\(\)** \\  Initial model + 1k & 52.26 & 84.33 \\ Initial model + 2k & **53.53** & **85.16** \\ Initial model + 3k & 53.16 & 85.13 \\   

Table 4: Results of different data scale.

  
**Model** & **Branch** & **Sum.** & **Exam** & **Code** & **Rew.** & **Cre W.** & **Fun W.** & **Comm.** & **NLP.** & **Others** & **Overall** \\  Auto-J & - & 45.8 & 38.9 & 47.5 & 49.2 & 59.7 & 61.7 & 55.2 & 57.6 & - & 54.9 \\ Auto-J\(\) & - & 55.5 & 37.5 & 45.8 & 50.0 & 61.0 & 61.5 & 54.9 & 54.2 & 58.3 & 55.1 \\   \\  Initial & 5 & 48.6 & 41.7 & 55.0 & 46.7 & 62.5 & 60.9 & 53.1 & 52.9 & 54.2 & 55.8 \\ SFT & 5 & 55.6 & 44.4 & 58.3 & 48.3 & 61.2 & 62.0 & 53.8 & 54.2 & 54.2 & 56.0 \\ DP0 & 5 & **59.7** & **45.8** & **58.3** & 46.7 & 62.1 & 59.9 & 54.9 & **59.6** & 58.3 & 57.2 \\   \\  Initial & 10 & 43.1 & 34.7 & 57.5 & 47.5 & 61.4 & 52.6 & 52.8 & 53.8 & 58.3 & 53.2 \\ SFT & 10 & 51.4 & 37.5 & 53.3 & 46.7 & 61.0 & 60.9 & 54.2 & 55.8 & 62.5 & 55.1 \\ DP0 & 3 & 54.2 & 37.5 & 55.0 & **50.0** & 62.1 & 65.1 & **55.9** & 55.4 & **62.5** & **56.8** \\ w/ GPT-4 & 5 & 44.4 & 36.1 & 55.8 & 50.0 & 61.7 & 58.1 & 55.5 & 57.5 & 58.3 & 55.4 \\   \\  SFT & 10 & 59.7 & 34.7 & 56.7 & 44.2 & 61.7 & **64.6** & 52.7 & 54.6 & 54.2 & 56.0 \\ DP0 & 5 & 56.9 & 40.3 & 54.2 & 45.8 & **63.3** & 62.5 & 54.5 & 57.5 & 54.2 & 56.4 \\   

Table 2: Agreement rates for different scenario groups and overall results.

  
**Models** & **MT-Bench** & **Refine Rate** \\  GPT-4 & **8.96** & - \\ LLaMA2-13B Chat & 7.06 & - \\ LLaMA2-70B Chat & 6.99 & - \\  LLaMA2-7B Chat & 6.26 & - \\ w/ SFT Correction & 6.85 & 87.5\% \\ w/ DP0 Correction & **7.08** & 72.5\% \\  Alpaca-13B & 4.97 & - \\ w/ SFT Correction & 6.61 & 95.0\% \\ w/ DP0 Correction & **6.85** & **98.8\%** \\   

Table 3: Results of dialogue correction.

tional supervision. We evaluated the performance of the Initial model trained on different sizes of data on the Eval-P benchmark. As shown in Table 4, the model reaches its best performance at 2k data, without considering the influence of GPT-4's annotation quality. Based on the assumption that LLMs primarily unlock their potential during alignment phase, we believe that enhancing performance hinges on increasing the variety of tasks rather than merely expanding the dataset.

### Instability problem in in-distribution training

The Direct Preference Optimization (DPO) algorithm  aims to optimize the selection of various branching preferences within the SFT model. In out-of-distribution evaluations, the DPO model demonstrates stable performance improvements in both agreement and consistency compared to the SFT model, as shown in Figure 5. However, in in-distribution evaluations, the SFT model consistently outperforms the DPO model in terms of the consistency rate. Additionally, SFT model does not achieve better performance by increasing the number of branches. We believe the primary reason for training instability is that the training data for DPO algorithm and the initial model come from the same distribution. As a result, the SFT and DPO models fail to obtain more stable supervision signals and may even overfit the training dataset. In contrast, OOD training incorporates a more diverse data distribution, which helps the model avoid converging to local optima during training.

## 6 Discussion

### Limitations

Currently, our model faces some limitations: (1) It cannot handle cases where all AI responses are incorrect, which should not be labeled as a "_tie_". (2) The model's result parsing relies heavily on regular expressions, which can lead to format errors. To address these issues, we plan to make several improvements, including expanding our task settings and utilizing the functional calling feature of LLMs. Additionally, our model's performance is constrained by the amount of training data and parameters. We aim to enhance its evaluation capabilities through data and parameter scaling .

### Future work

Our work demonstrates that the evaluation model generates diverse judgments for dialogue content based on different criteria. To align more closely with human behavior, we prioritize key judgments in the evaluation model's outputs. In future, we try to further expand the criteria space to uncover a variety of decision paths. Additionally, we aim to find more accurate preference selection methods to replace ensemble methods, thereby achieving a better alignment with human behavior.

## 7 Broader Impact

Our work focuses on the task of automatic evaluation, specifically exploring how to learn better evaluation strategies from an evaluation tree. We demonstrate that automated evaluation criteria can replace human priors, and by combining branch decision-making with DPO training, we have achieved robust evaluation performance. We conduct detailed experiments covering a broad range of real-world scenarios to discuss how to enhance model evaluation capabilities from scratch. With our work, we hope to inform further research into better understanding and developing improved evaluation methodologies for LLMs.

Figure 5: The agreement and consistency rates of ID and OOD models with different branches.