# Autodecoding Latent 3D Diffusion Models

Evangelos Ntavelis

Computer Vision Lab

ETH Zurich

Zurich, Switzerland

entavels@vision.ee.ethz.ch &Aliaksandr Siarohin

Creative Vision

Snap Inc.

Santa Monica, CA, USA

asiarohin@snapchat.com &Kyle Olszewski

Creative Vision

Snap Inc.

Santa Monica, CA, USA

kolszewski@snap.com &Chaoyang Wang

CI2CV Lab

Carnegie Mellon University

Pittsburgh, PA, USA

chaoyanw@cs.cmu.edu &Luc Van Gool

CVL, ETH Zurich, CH

PSI, KU Leuven, BE

INSAIT, Un. Sofia, BU

vangool@vision.ee.ethz.ch &Sergey Tulyakov

Creative Vision

Snap Inc.

Santa Monica, CA, USA

stulyakov@snapchat.com

Work done during internship at Creative Vision Team - Snap Inc

###### Abstract

We present a novel approach to the generation of static and articulated 3D assets that has a 3D _autodecoder_ at its core. The 3D _autodecoder_ framework embeds properties learned from the target dataset in the latent space, which can then be decoded into a volumetric representation for rendering view-consistent appearance and geometry. We then identify the appropriate intermediate volumetric latent space, and introduce robust normalization and de-normalization operations to learn a 3D diffusion from 2D images or monocular videos of rigid or articulated objects. Our approach is flexible enough to use either existing camera supervision or no camera information at all - instead efficiently learning it during training. Our evaluations demonstrate that our generation results outperform state-of-the-art alternatives on various benchmark datasets and metrics, including multi-view image datasets of synthetic objects, real in-the-wild videos of moving people, and a large-scale, real video dataset of static objects.

Code & Visualizations: https://github.com/snap-research/3DVADER

## 1 Introduction

Photorealistic generation is undergoing a period that future scholars may well compare to the enlightenment era. The improvements in quality, composition, stylization, resolution, scale, and manipulation capabilities of images were unimaginable just over a year ago. The abundance of online images, often enriched with text, labels, tags, and sometimes per-pixel segmentation, has significantly accelerated such progress. The emergence and development of denoising diffusion probabilistic models (DDPMs)  propelled these advances in image synthesis  and other domains, _e.g._ audio () and video ().

However, the world is 3D, consisting of static and dynamic objects. Its geometric and temporal nature poses a major challenge for generative methods. First of all, the data we have consists mainly of images and monocular videos. For some limited categories of objects, we have 3D meshes with corresponding multi-view images or videos, often obtained using a tedious capturing process or created manually by artists. Second, unlike CNNs, there is no widely accepted 3D or 4D representation suitable for 3D geometry and appearance generation. As a result, with only a few exceptions , most of the existing 3D generative methods are restricted to a narrow range of objectcategories, suitable to the available data and common geometric representations. Moving, articulated objects, humans, compound the problem, as the representation must also support deformations.

In this paper, we present a novel approach to designing and training denoising diffusion models for 3D-aware content suitable for efficient usage with datasets of various scales. It is generic enough to handle both rigid and articulated objects. It is versatile enough to learn diverse 3D geometry and appearance from multi-view images and monocular videos of both static and dynamic objects. Recognizing the poses of objects in such data has proven to be crucial to learning useful 3D representations [6; 7; 73; 74]. Our approach is thus designed to be robust to the use of ground-truth poses, those estimated using structure-from-motion, or using no input pose information at all, but rather learning it effectively during training. It is scalable enough to train on single- or multi-category datasets of large numbers of diverse objects suitable for synthesizing a wide range of realistic content.

Recent diffusion methods consist of two stages . During the first stage, an autoencoder learns a rich latent space. To generate new samples, a diffusion process is trained during the second stage to explore this latent space. To train an image-to-image autoencoder, many images are needed. Similarly, training 3D autoencoders requires large quantities of 3D data, which is very scarce. Previous works used synthetic datasets such as ShapeNet  (DiffRF, SDFusion , _etc._), and were thus restricted to domains where such data is available.

In contrast to these works, we propose to use a volumetric auto_decoder_ to learn the latent space for diffusion sampling. In contrast to the autoencoder-based approach, our autodecoder maps a 1D vector to each object in the training set, and thus does not require 3D supervision. The autodecoder learns 3D representations from 2D observations, using rendering consistency as supervision. Following UVA  this 3D representation supports the articulated parts necessary to model non-rigid objects.

There are several key challenges with learning such a rich, latent 3D space with an autodecoder. First, our autodecoders do not have a clear "bottleneck." Starting with a 1D embedding, they upsample it to latent features at many resolutions, until finally reaching the output radiance and density volumes. Here, each intermediate volumetric representation could potentially serve as a "bottleneck." Second, autoencoder-based methods typically regularize the bottleneck by imposing a KL-Divergence constraint [38; 63], meaning diffusion must be performed in this regularized space.

To identify the best intermediate representation to perform diffusion, one can perform exhaustive layer-by-layer search. This, however, is very computationally expensive, as it requires running hundreds of computationally expensive experiments. Instead, we propose robust normalization and denormalization operations which can be applied to any layers of a pre-trained and fixed autodecoder. These operations compute robust statistics to perform layer normalization and, thus, allow us to train the diffusion process at any intermediate resolution of the autodecoder. We find that at fairly low resolutions, the space is compact and provides the necessary regularization for geometry, allowing the training data to contain only sparse observations of each object. The deeper layers, on the other hand, operate more as upsamplers. We provide extensive analysis to find the appropriate resolution for our autodecoder-based diffusion techniques.

We demonstrate the versatility and scalability of our approach on various tasks involving rigid and articulated 3D object synthesis. We first train our model using multi-view images and cameras in a setting similar to DiffRF  to generate shapes of a limited number of object categories. We then scale our model to hundreds of thousands of diverse objects train using the real-world MVImgNet  dataset, which is beyond the capacity of prior 3D diffusion methods. Finally, we train our model on a subset of CelebV-Text , consisting of \(\)44K sequences of high-quality videos of human motion.

## 2 Related Work

### Neural Rendering for 3D Generation

Neural radiance fields, or NeRFs (Mildenhall et al., 2020 ), enable high-quality novel view synthesis (NVS) of rigid scenes learned from 2D images. Its approach to volumetric neural rendering has been successfully applied to various tasks, including _generating_ objects suitable for 3D-aware NVS. Inspired by the rapid development of generative adversarial models (GANs)  for generating 2D images [22; 5; 32; 33; 35; 34] and videos [79; 72; 91], subsequent work extends them to 3D content generation with neural rendering techniques. Such works [67; 51; 54; 52; 88] show promising results for this task, yet suffer from limited multi-view consistency from arbitrary viewpoints, and experiencing difficulty in generalizing to multi-category image datasets.

A notable work in this area is pi-GAN (Chan et al., 2021 ), which employs neural rendering with periodic activation functions for generation with view-consistent rendering. However, it requires a precise estimate of the dataset camera pose distribution, limiting its suitability for free-viewpoint videos. In subsequent works, EG3D (Chan et al., 2022 ) and EpiGRAF (Skorokhodov et al. ) use tri-plane representations of 3D scenes created by a generator-discriminator framework based on StyleGAN2 (Karras et al., 2020 ). However, these works require pose estimation from keypoints (_e.g_. facial features) for training, again limiting the viewpoint range.

These works primarily generate content within one object category with limited variation in shape and appearance. A notable exception is 3DGP , which generalizes to ImageNet . However, its reliance on monocular depth prediction limits it to generating front-facing scenes. These limitations also prevent these approaches from addressing deformable, articulated objects. In contrast, our method is applicable to both deformable and rigid objects, and covers a wider range of viewpoints.

### Denoising Diffusion Modeling

Denoising diffusion probabilistic models (DDPMs) [75; 28] represent the generation process as the learned denoising of data progressively corrupted by a sequence of diffusion steps. Subsequent works improving the training objectives, architecture, and sampling process [28; 17; 86; 36; 63; 53; 76] demonstrated rapid advances in high-quality data generation on various data domains. However, such works have primarily shown results for tasks in which samples from the target domain are fully observable, rather than operating in those with only partial observations of the dataset content.

One of the most important of such domains is 3D data, which is primarily observed in 2D images for most real-world content. Some recent works have shown promising initial results in this area. DiffRF  proposes reconstructing per-object NeRF volumes for synthetic datasets, then applying diffusion training on them within a U-Net framework. However, it requires the reconstruction of many object volumes, and is limited to low-resolution volumes due to the diffusion training's high computational cost. As our framework instead operates in the latent space of the autodecoder, it effectively shares the learned knowledge from all training data, thus enabling low-resolution, latent 3D diffusion. In , a 3D autoencoder is used for generating 3D shapes, but this method require ground-truth 3D supervision, and only focuses on shape generation, with textures added using an off-the-shelf method . In contrast, our framework learns to generate the surface appearance and corresponding geometry without such ground-truth 3D supervision.

Many recent works [2; 68; 23; 9] combine a denoising diffusion approach with a tri-plane representation  for 3D generation. They perform diffusion on the embedding vector of an autodecoder , the bottleneck of an autoencoder , or directly, on a pre-computed  or a simultaneously learned tri-plane . Nevertheless, these works focus on small datasets or require a dense point clouds and ground truth object meshes, which are not readily available for real-object image datasets. The triplane representation requires an MLP decoder that substantially increases the volumetric rendering time. Our voxel-decoder does not has such a requirement as it directly outputs color and density, and thus permitting faster training on large-scale real image datasets.

Recently, several works [60; 43; 11] propose using large-scale, pre-trained text-to-image 2D diffusion models for 3D generation. The key idea behind these methods is to use 2D diffusion models to evaluate the quality of renderings from randomly sampled viewpoints, then use this information to optimize a 3D-aware representation of the content. Compared to our method, however, such approaches require a far more expensive optimization process to generate each novel object.

## 3 Methodology

Our method is a two-stage approach. In the first stage, we learn an autodecoder \(G\) containing a library of embedding vectors corresponding to the objects in the training dataset. These vectors are first processed to create a low-resolution, latent 3D feature volume, which is then progressively upsampled and finally decoded into a voxelized representation of the generated object's shape and appearance. This network is trained using volumetric rendering techniques on this volume, with 2D reconstruction supervision from the training images.

During the second stage, we split the autodecoder \(G\) into two parts, \(G=G_{2} G_{1}\). We then employ this autodecoder to train a 3D diffusion model operating in the compact, 3D latent space obtained from \(G_{1}\). 2 Using the structure and appearance properties extracted from the autodecoder training dataset, this 3D diffusion process allows us to use this network to efficiently generate diverse and realistic 3D content. The full pipeline is depicted in Fig. 1.

Below, we first describe the volumetric autodecoding architecture (Sec. 3.1). We then describe the training procedure and reconstruction losses for the autodecoder (Sec. 3.2). Finally, we provide details for our training and sampling strategies for 3D diffusion in the decoder's latent space (Sec. 3.3).

### Autodecoder architecture

**Canonical Representation.** We use a 3D voxel grid to represent the 3D structure and appearance of an object. We assume the objects are in their canonical pose, such that the 3D representation is decoupled from the camera poses. This decoupling is necessary for learning compact representations of objects, and also serves as a necessary constraint to learn meaningful 3D structure from 2D images without direct 3D supervision. Specifically, the canonical voxel representation consists of a density grid \(V^{}^{S^{3}}\) which is a discrete representation of the density field with resolution \(S^{3}\), and \(V^{RGB}^{S^{3} 3}\) which represents the RGB radiance field. We employ volumetric rendering, integrating the radiance and opacity values along each view ray similar to NeRFs . In contrast to the original NeRF, however, rather than computing these local values using an MLP, we tri-linearly interpolate the density and RGB values from the decoded voxel grids, similar to Plenoxels .

**Voxel Decoder.** The 3D voxel grids for density and radiance, \(V^{}\) and \(V^{}\), are generated by a volumetric autodecoder \(G\) that is trained using rendering supervision from 2D images. We choose to directly generate \(V^{}\) and \(V^{}\), rather than intermediate representations such as feature volumes or tri-planes, as it is more efficient to render and ensures consistency across multiple views. Note that feature volumes and tri-planes require running an MLP pass for each sampled point, which requires significant computational cost and memory during training and inference.

The decoder is learned in the manner of GLO  across various object categories from large-scale multi-view or monocular video datasets. The architecture of our autodecoder is adapted from . However, in our framework we want to support large scale datasets which poses a challenge in designing the decoder architecture with the capability to generate high-quality 3D content across various categories. In order to represent each of the \(\)300K objects in our largest dataset we need very high-capacity decoder. As we found the relatively basic decoder of  produced poor reconstruction quality, we introduce the following key extensions:

* To support the diverse shapes and appearances in our target datasets, we find it crucial to increase the length of the embedding vectors learned by our decoder from \(64\) to \(1024\).

Figure 1: **Our proposed two-stage framework. Stage 1 trains an autodecoder with two generative components, \(}\) and \(}\). It learns to assign each training set object a 1D embedding that is processed by \(}\) into a latent volumetric space. \(}\) decodes these volumes into larger radiance volumes suitable for rendering. Note that we are using only 2D supervision to train the autodecoder. In Stage 2, the autodecoder parameters are frozen. Latent volumes generated by \(}\) are then used to train the 3D denoising diffusion process. At inference time, \(}\) is not used, as the generated volume is randomly sampled, denoised, and then decoded by \(}\) for rendering.**

* We increase the number of residual blocks at each resolution in the autodecoder from 1 to 4.
* Finally, to harmonize the appearance of the reconstructed objects we introduce self-attention layers  in the second and third levels (resolutions \(8^{3}\) and \(16^{3}\)).

**Scaling the Embedding Codebook for Large Datasets.** Each object in the training set is encoded by an embedding vector. However, storing a separate vector for each object is burdensome, especially for large datasets. As such, we propose a technique to significantly reduce the parameter footprint of our embeddings, while allowing effective generation from large-scale datasets.

Similar to StyleGenes' approach , we combine smaller embedding _subvectors_ to create unique per-object vectors. The decoder's input is a per-object embedding vector \(H_{k}^{l}\) with length \(l_{v}\). It is a concatenation of smaller subvectors \(h_{i}^{j}\), where each subvector is selected from an ordered codebook with \(n_{c}\) entries, with each entry containing collection of \(n_{h}\) embedding vectors of length \(l_{v}/n_{c}\):

\[H_{k}=[h_{1}^{k_{1}},h_{2}^{k_{2}},...,h_{n_{c}}^{k_{n_{c}}}],\] (1)

where \(k_{i}\{1,2,...,n_{h}\}\) is the set of indices used to select from the \(n_{h}\) possible codebook entries for position \(i\) in the final vector. This method allows for exponentially more combinations of embedding vectors, greatly reducing the number of stored parameters compared to a single embedding codebook. In contrast to , the index \(j\) for the vector \(h_{i}^{j}\) at position \(i\) is not randomly selected for each position to access its corresponding codebook entry. We use a _hashing function_ to map each training object index \(k\) to its corresponding embedding index.

### Autodecoder Training

We train the decoder from image data through analysis-by-synthesis, with the primary objective of minimizing the difference between the decoder's rendered images and the training images. We render RGB color image \(C\) using volumetric rendering , additionally in order to supervise silhouette of the objects we render 2D occupancy mask \(O\).

**Pyramidal Perceptual Loss.** As in [69; 70], we employ a pyramidal perceptual loss based on  on the rendered images as our primary reconstruction loss:

\[_{}(,C)=_{l=0}^{L}_{i=0}^{I}| _{i}(_{l}())-_{i}(_{l}(C)) |,\] (2)

where \(\), \(C[0,1]^{H W 3}\) are the RGB rendered and training images of resolution \(H W\), respectively; \(_{i}\) is the \(i^{}\)-layer of a pre-trained VGG-19  network; and operator \(D_{l}\) downsamples images to the resolution for pyramid level \(l\).

**Foreground Supervision.** Since we only interested in modeling single objects, in all the datasets considered in this work, we remove the background. However if the color of the object is black (which corresponds to the absence of density), the network can make the object semi-transparent. To improve the overall shape of the reconstructed objects, we make use of a foreground supervision loss. Using binary foreground masks (estimated by an off-the-shelf matting method , Segment Anything  or synthetic ground-truth masks, depending on the dataset), we apply an L1 loss on the rendered occupancy map to match that of the mask corresponding to the image.

\[_{}(,O)=\|O-\|_{1},\] (3)

where \(,O[0,1]^{H W}\) are the inferred and ground-truth occupancy masks, respectively. We provide visual comparison of the inferred geometry for this loss in the supplement.

**Multi-Frame Training.** Because our new decoder have a large capacity, generating a volume incur much larger overhead compared to rendering an image based on this volume (which mostly consists of tri-linear sampling of the voxel cube). Thus, rather than rendering a single view for the canonical representation of the target object in each batch, we instead render \(4\) views for each object in the batch. This technique incurs no significant overhead, and effectively increases the batch size four times. As an added benefit, we find that this technique improves on the overall quality of the generated results, since it significantly reduce batch variance. We ablate this technique and our key architectural design choices, showing their effect on the sample quality (Sec. 4.3, Tab. 2).

**Learning Non-Rigid Objects.** For articulated, non-rigid objects, videos of human subjects, we must model a subject's shape and local motion from dynamic poses, as well as the corresponding non-rigid deformation of local regions. Following , we assume these sequences can be decomposed into a set of \(N_{p}\) smaller, rigid components (\(10\) in our experiments) whose poses can be estimated for consistent alignment in the canonical 3D space. The camera poses for each component are estimated and progressively refined during training, using a combination of learned 3D keypoints for each component of the depicted subject and the corresponding 2D projections predicted in each image. This estimation is performed via a differentiable Perspective-n-Point (PnP) algorithm .

To combine these components with plausible deformations, we employ a learned volumetric linear blend skinning (LBS) operation. We introduce a voxel grid \(V^{LBS}^{S^{3} N_{p}}\) to represent the skinning weights for each deformation components. As we assume no prior knowledge about the content or assignment of object components, the skinning weights for each component are also estimated during training. Please see the supplement for additional details.

### Latent 3D Diffusion

**Architecture.** Our diffusion model architecture extends prior work on diffusion in a 2D space  to the latent 3D space. We implement its 2D operations, including convolutions and self-attention layers, in our 3D decoder space. In the text-conditioning experiments, after the self-attention layer, we use a cross-attention layer similar to that of . Please see the supplement for more details.

**Feature Processing.** One of our key observation is that the features \(F\) in the latent space of the 3D autodecoder have a bell-shaped distribution (see the supplement), which eliminates the need to enforce any form of prior on it, as in . Operating in the latent space without a prior enables training a single autodecoder for each of the possible latent diffusion resolutions. However, we observe that the feature distribution \(F\) has very long tails. We hypothesise this is because the final density values inferred by the network do not have any natural bounds, and thus can fall within any range. In fact, the network is encouraged to make such predictions, as they have the sharpest boundaries between the surface and empty regions. However, to allow for a uniform set of diffusion hyper-parameters for all datasets and all trained autodecoders, we must normalize their features into the same range. This is equivalent to computing the center and the scale of the distribution. Note that, due to the very long-tailed feature distribution, typical mean and standard deviation statistics will be heavily biased. We thus propose a robust alternative based on the feature distribution quantiles. We take the _median_\(m\) as the center of the distribution and approximate its scale using the Normalized InterQuartile Range (IQR)  for a normal distribution: \(0.7413 IQR\). Before using the features \(F\) for diffusion, we normalize them to \(=\). During inference, when producing the final volumes we de-normalize them as \( IQR+m\). We call this method _robust normalization_. Please see the supplement for an evaluation of its impact.

**Sampling for Object Generation.** During inference we rely on the sampling method from EDM , with several slight modifications. We fix EDM's hyperparameter matching the dataset's distribution to 0.5 regardless of the experiment, and modify the feature statistics in our feature processing step. We also introduce classifier free guidance  for our text-conditioning experiments (Sec. 4.5). We found that setting the weight equal to 3 yields good results across all datasets.

## 4 Results and Evaluations

In this section, we evaluate our method on multiple diverse datasets (see Sec. 4.1) for both unconditional 4.2 and conditional settings 4.5. We also ablate the design choices in our autodecoder and diffusion in Secs. 4.3 and 4.4, respectively.

### Datasets and Data Processing

Below we describe the datasets used for our evaluations. We mostly evaluate our method on datasets of synthetic renderings of 3d objects [13; 57; 14]. However, we also provide results on a challenging video dataset of dynamic human subjects  and dataset of static object videos .

**ABO Tables.** Following , we evaluate our approach on renderings of objects from the Tables subset of the Amazon Berkeley Objects (ABO) dataset , consisting of \(1,676\) training sequences with \(91\) renderings per sequence, for a total of \(152,516\) renderings.

**PhotoShape Chairs.** Also as in , we use images from the Chairs subset of the PhotoShape dataset , totaling \(3,115,200\) frames, with \(200\) renderings for each of \(15,576\) chair models.

**Obiayverse.** This dataset  contains \(\)800K publicly available 3D models. As the of the object geometry and appearance varies, we use a manually-filtered subset of \(\)300K unique objects (see supplement for details). We render 6 images per training object, for a total of \(\)1.8 million frames.

**MVImgNet.** For this dataset , we use \(\)6.5 million frames from \(219,188\) videos of real-world objects from 239 categories, with an average of \(30\) frames each. We use Grounded Segment Anything [45; 39] for background removal, then apply filtering (see supplement) to remove objects with failed segmentation. This process results in \(206,990\) usable objects.

**CelebV-Text.** The CelebV-Text dataset  consists of \(\)70K sequences of high-quality videos of celebrities captured in in-the-wild environments, lighting, motion, and poses. They generally depict the head, neck, and upper-torso region, but contain more challenging pose and motion variation than prior datasets, _e.g_. VoxCeleb . We use the robust video matting framework of  to obtain our masks for foreground supervision (Sec. 3.2). Some sample filtering (described in the supplement) was needed for sufficient video quality and continuity for training. This produced \(\)44.4K unique videos, with an average of \( 373\) frames each, totaling \(\)16.6M frames.

For training, we use the **camera parameters** used to render each synthetic object dataset, and the estimated parameters provided for the real video sequences in MVImgNet, adjusted to center and scale the content to our rendering volume, (see supplement for details). For the human videos in CelebV-Text, we train an additional pose estimator along with the autodecoder \(G\) to predict poses for each articulated region per frame, such that all objects can be aligned in the canonical space (Sec. 3.2). Note that for creating dynamic 3D video, we can use sequences of poses transferred from the real video of another person from the dataset.

    & PhotoShape Chairs  & ABO Tables  \\ Method & FID \(\) & KID \(\) & FID \(\) & KID \(\) \\  \(\)-GAN  & 52.71 & 13.64 & 41.67 & 13.81 \\ EG3D  & 16.54 & 8.412 & 31.18 & 11.67 \\ DiffRF  & 15.95 & 7.935 & 27.06 & 10.03 \\  Ours & **11.28** & **4.714** & **18.44** & **6.854** \\   

Table 1: Results on the synthetic PhotoShape Chairs  and ABO Tables  datasets. Overall, our method outperforms state-of-the-art GAN-based and diffusion-based approaches. KID scores are multiplied by \(10^{3}\).

   Model Variant & PSNR \(\) & LPIPS \(\) \\  Ours & **27.719** & **6.255** \\  - Multi-Frame Training & 27.176 & 6.855 \\ - Self-Attention & 27.335 & 6.738 \\ - Increased Depth & 27.24 & 6.924 \\ - Embedding Length (\(1024 64\)) & 25.985 & 8.332 \\   

Table 2: Our 3D autodecoder ablation results: “-” indicates this component has been removed. As we remove each sequentially, the top row depicts results for the unmodified architecture and training procedure. LPIPS results are multiplied by \(10^{2}\).

Figure 2: Qualitative comparisons with Direct Latent Sampling (DLS)  on CelebV . We show the two driving videos for two random identities: the top identity in each block is generated by our method, the bottom identity in each block is generated by DLS . We also show the rendered depth and normals.

### Unconditional Image Generation

Synthetic Datasets.Following the evaluation protocol of , we report results on the ABO Tables and PhotoShape Chairs datasets. These results on single-category, synthetically rendered datasets that are relatively small compared to the others, demonstrate that our approach also performs well with smaller, more homogeneous data. We render \(10\) views of \(1\)K samples from each dataset, and report the Frechet Inception Distance (FID)  and Kernel Inception Distance (KID)  when compared to \(10\) randomly selected ground-truth images from each training sequence. We report the results compared to both GAN-based [6; 7] and more recent diffusion-based approaches  methods, as seen in Tab. 1. We see that our method significantly outperforms state-of-the-art methods using both metrics on the Tables dataset, and achieves better or comparable results on the Chairs dataset.

Large-Scale Datasets.We run tests on the large-scale datasets described above: MVImgNet, CelebV-Text and Obiyaserve. For each dataset, we render \(5\) images from random poses for each of \(10\)K generated samples. We report the FID and KID for these experiments compared to \(5\) ground-truth images for each of \(10\)K training objects. As no prior work demonstrates the ability to generalize to such large-scale datasets, we compare our model against directly sampling the 1D latent space of our base autodecoder architecture (using noise vectors generated from a standard normal distribution). This method of 3D generation was shown to work reasonably well . We also evaluate our approach with different numbers of diffusion steps (\(16,\)\(32\) and \(64\)). The results can be seen in Tab. 3. Visually, we compare with  in Fig. 2. Our qualitative results show substantially higher fidelity, quality of geometry and texture. We can also see that when identities are sampled directly in the 1D

Figure 4: We show generated samples from our model trained using monocular videos from MVImgNet . We show three views for each object, along with the normals for each view. We also show depth for the right-most view. Text-conditioned results are shown. Ground-truth captions are generated by MiniGPT-4 .

Figure 3: We show generated samples of our model trained using rendered images from Obiyaserve . We show three views for each object, along with the normals for each view. We also show depth for the right-most view. Text-conditioned results are shown. Ground-truth captions are generated by MiniGPT-4 .

latent space, the normals and depth are significantly less sharp, indicating that there exist spurious density in the sampled volumes. Tab. 3 further supports this observation: both the FID and KID are significantly lower than those from direct sampling, and generally improve with additional steps.

### Autodecoder Ablation

We conduct an ablation study on the key design choices for our autodecoder architecture and training. Starting with the final version, we subtract the each component described in Sec. 3.1. We then train a model on the PhotoShape Chairs dataset and render \(4\) images for each of the \(\)15.5K object embeddings.

Tab. 2 provides the the PSNR  and LPIPS  reconstruction metrics. We find that the final version of our process significantly outperforms the base architecture  and training process. While the largest improvement comes from our increase in the embedding size, we see that simply removing the multi-frame training causes a noticeable drop in quality by each metric. Interestingly, removing the self-attention layers marginally increases the PSNR and lowers the LPIPS. This is likely due to the increased complexity in training caused by these layers, which for a dataset of this size, may be unnecessary. For large-scale datasets, we observed significant improvement with this feature. Both decreasing the depth of the residual convolution blocks and reducing the embedding size cause noticeable drops in the overall quality, particularly the latter. This suggests that the additional capacity provided by these components is impactful, even on a smaller dataset.

### Diffusion Ablation

We also perform ablation on our diffusion process, evaluating the effect of the choice of the number of diffusion steps (\(16\), \(32\), and \(64\)), and the autodecoder resolution at which we perform diffusion (\(4^{3}\), \(8^{3}\), and \(16^{3}\)). For these variants, we follow the generation quality training and evaluation protocol on the PhotoShape Chairs (Sec. 4.2), except that we disable stochasticity in our sampling during inference for more consistent performance across these tests. Each model was trained using roughly the same amount of time and computation. Fig. 5 shows the results. Interestingly, we can see a clear distinction between the results obtained from diffusion at the earlier or later autodecoder stages, and those from our the results with resolution \(8^{3}\). We hypothesize that at lowest resolution layers overfit to the training dataset, thus when processing novel objects via diffusion, the quality degrades significantly. Training at a higher resolution requires substantial resources, limiting the convergence seen in a reasonable amount of time. The number of sampling steps has a smaller, more variable impact. Going from \(16\) to \(32\) steps improves the results with a reasonable increase in inference time, but at \(64\) steps, the largest improvement is at the \(16^{3}\) resolution, which requires more than \(30\) seconds per sample. Our chosen diffusion resolution of \(8^{3}\) achieves the best results, allowing for high sample quality at \(64\) steps (used in our other experiments) with only \(\)8 seconds of computation, but provides reasonable results with \(32\) steps in \(\)4 seconds.

### Conditional Image Generation

Finally, we train diffusion models with text-conditioning. For MVImgNet and Objaverse, we generate the text with an off-the-shelf captioning system . Qualitative results for MVImgNet and Objaverse are in Figs. 4 and 3. We observe that in all cases, our method generates objects with reasonable geometry that generally follow the prompt. However, some details can be missing. We believe our

Figure 5: Impact of diffusion resolution and number of sampling steps on sample quality and inference time.

    &  &  &  \\  Method & FID \(\) & KID \(\) & FID \(\) & KID \(\) & FID \(\) & KID \(\) \\  Direct Latent & & & & & & \\ Sampling  & 69.21 & 73.74 & 97.51 & 69.22 & 72.76 & 53.68 \\  Ours _- 16 Steps_ & **48.01** & 49.49 & 62.21 & 39.94 & 47.49 & 32.44 \\ Ours _- 32 Steps_ & 49.74 & **46.2** & 51.26 & 28.45 & 43.68 & 31.7 \\ Ours _- 64 Steps_ & 50.27 & 47.72 & **43.85** & **23.91** & **40.49** & **29.37** \\   

Table 3: Results on large-scale multi-view image (Objaverse  & MVImgNet ) and monocular video (CelebV-Text ) datasets. The KID score is multiplied by \(10^{3}\).

model learns to ignore certain details from text prompts, as MiniGPT-4 often hallucinates details inconsistent with the object's appearance. Better captioning systems should help alleviate this issue.

### Design Choices for Large-Scale 3D Object Synthesis

The goal of our work is to enable 3D object synthesis by training a model on large and diverse multi-view image datasets. To realize this goal there are two main design choices that we need to make: (a) what is the appropriate 3D representation and (b) generative modelling approach.

Recent works [2; 68; 23; 9] use tri-planes as their 3D representation. However, when the multi-view supervision is scarce and ground truth camera information is not available, such as in video datasets like CelebV, tri-planes tend to degrade to prediction of flat objects . Moreover, tri-planes require an additional MLP for volumetric rendering, applied for every of the 128 ray point samples and \(128^{2}\) output pixels we use in our setting. In contrast, our voxel grid autodecoder outputs directly color and density. Tri-planes are faster to _autodecode_, but rendering them is much slower. Training for an iteration with our \(64^{3}\) voxel grid takes \(0.22s\). Tri-planes of size \(64^{2}\), \(128^{2}\), \(256^{2}\) and \(512^{2}\) require \(0.33\), \(0.33\), \(0.38\), \(0.46\) seconds respectively. We use 32 channels per plane, a two-layer MLP with 32 hidden channels, and a 2D autodecoder. This can severely affect the training time for a large dataset.

EG3D  and GET3D  propose an adversarial approach to 3D synthesis. Both, base their generators on StyleGAN , which for 2D datasets requires considerable changes to produce good results in large and diverse datasets . Training both on Objavverse, we find they fail to converge, as seen in Fig. 11. Thus, we believe our diffusion-based approach is better suited for our goal.

## 5 Conclusion

Despite the inherent challenges in performing flexible 3D content generation for arbitrary content domains without 3D supervision, our work demonstrates this is possible with the right approach. By exploiting the inherent power of autodecoders to synthesize content in a domain without corresponding encoded input, our method learns representations of the structure and appearance of diverse and complex content suitable for generating high-fidelity 3D objects using only 2D supervision. Our latent volumetric representation is conducive to 3D diffusion modeling for both conditional and unconditional generation, while enabling view-consistent rendering of the synthesized objects. As seen in our results, this generalizes well to various types of domains and datasets, from relatively small, single-category, synthetic renderings to large-scale, multi-category real-world datasets. It also supports the challenging task of generating articulated moving objects from videos. No prior work addresses each of these problems in a single framework. The progress shown here suggests there is potential to develop and extend our approach to address other open problems.

Limitations.While we demonstrate impressive and state-of-the-art results on diverse tasks and content, several challenges and limitations remain. Here we focus on images and videos with foregrounds depicting one key person or object. The generation or composition of more complex, multi-object scenes is a challenging task and an interesting direction for future work. As we require multi-view or video sequences of each object in the dataset for training, single-image datasets are not supported. Learning the appearance and geometry of diverse content for controllable 3D generation and animation from such limited data is quite challenging, especially for articulated objects. However, using general knowledge about shape, motion, and appearance extracted from datasets like ours to reduce or remove the multi-image requirement when learning to generate additional object categories may be feasible with further exploration. This would allow the generation of content learned from image datasets of potentially unbounded size and diversity.

Broader Impact.Our work shares similar concerns with other generative modeling efforts, _e.g._, potential exploitation for misleading content. As with all such learning-based methods, biases in training datasets may be reflected in the generated content. Appropriate caution must be applied when using this method to avoid this when it may be harmful, _e.g._ human generation. Care must be taken to only use this method on public data, as the privacy of training subjects may be compromised if our framework is used to recover their identities. The environmental impact of methods requiring substantial energy for training and inference is also a concern. However, our approach makes our tasks more tractable by removing the need for the curation and processing of large-scale 3D datasets, and is thus more amenable to efficient use than methods requiring such input.

AcknowledgementsWe would like to thank Michael Vasilkovsky for preparing the OjjaVerse renderings, and Colin Eles for his support with infrastructure. Moreover, we would like to thank Norman Muller, author of DiffRF paper, for his invaluable help with setting up the DiffRF baseline, the ABO Tables and PhotoShape Chairs datasets, and the evaluation pipeline as well as answering all related questions. A true marvel of a scientist. Finally, Evan would like to thank Claire and Gio for making the best expupuccinos and fueling up this research.