# Latent Neural Operator for Solving Forward and Inverse PDE Problems

Tian Wang

State Key Laboratory of

Multimodal Artificial Intelligence Systems

Institute of Automation,

Chinese Academy of Sciences

School of Artificial Intelligence,

University of Chinese Academy of Sciences

wangtian2022@ia.ac.cn

&Chuang Wang

State Key Laboratory of

Multimodal Artificial Intelligence Systems

Institute of Automation,

Chinese Academy of Sciences

School of Artificial Intelligence,

University of Chinese Academy of Sciences

wangchuang@ia.ac.cn

Corresponding Author.This work is supported by Beijing Municipal Science and Technology Project (No. Z231100010323005), the 2035 Innovation Mission (Grants No. E4J10102) and Pioneer Hundred Talents Program of CAS under Grant Y9S9MS08 and E2S40101.

###### Abstract

Neural operators effectively solve PDE problems from data without knowing the explicit equations, which learn the map from the input sequences of observed samples to the predicted values. Most existing works build the model in the original geometric space, leading to high computational costs when the number of sample points is large. We present the Latent Neural Operator (LNO) solving PDEs in the latent space. In particular, we first propose Physics-Cross-Attention (PhCA) transforming representation from the geometric space to the latent space, then learn the operator in the latent space, and finally recover the real-world geometric space via the inverse PhCA map. Our model retains flexibility that can decode values in any position not limited to locations defined in the training set, and therefore can naturally perform interpolation and extrapolation tasks particularly useful for inverse problems. Moreover, the proposed LNO improves both prediction accuracy and computational efficiency. Experiments show that LNO reduces the GPU memory by 50%, speeds up training 1.8 times, and reaches state-of-the-art accuracy on four out of six benchmarks for forward problems and a benchmark for inverse problem. Code is available at https://github.com/L-I-M-I-T/LatentNeuralOperator.

## 1 Introduction

Neural network approaches for partial differential equations (PDEs) attract increasing attention in diverse scientific fields, for instance, meteorological prediction, industrial design, geological sensing and environmental monitoring to name a few. Compared with traditional numerical methods, _e.g._, finite element method, which demands a large number of computational resources and requires meticulous discretization involving a lot of specialized knowledge, neural networks provide a new paradigm for solving PDE-driven problems with a better trade-off between accuracy and flexibility.

Neural PDE solvers can be divided into two main categories, physics-informed and operator-learning models. The physics-informed methods enforce the inductive bias of explicit PDE instances into loss function or model architecture, usually yielding solutions with high accuracy, but are less flexible as they require fresh training to find the solution for each new instance. In contrast, the operator learning methods adopt a data-driven paradigm to learn the implicit PDE constraintfrom pair of samples representing input-to-output functional mappings, providing better flexibility that can generalize to different initial/boundary conditions beyond the scope of the training data, but have yet large room to improve in the prediction accuracy.

Recently, Transformers prevail in neural operator structures. Theoretically, the attention mechanism was proved to be a special form of the integral kernel[15; 17] in neural operator, which naturally conforms to sequence-to-sequence characterization. Moreover, the fully-connected attention structure enables the model to characterize long-distance and quadratic relationships among sample points, which yields more accurate prediction than the vanilla MLP structure but at the expense of increasing the computational complexity drastically, as the complexity is quadratic respect to the length of input sequence.

To alleviate the complexity, existing works employed linear attention mechanism [18; 19; 20; 21] speeding up the computation but sacrificing precision due to their limited expressive power. Another line of works attempted to solve the PDE in the latent space with a small number of samples [22; 23; 24; 25; 26], which get rid of abundant sample points in the original geometric space and capture physical correlations in the tighter latent space.

In this work, we present the Latent Neural Operator (LNO) with particular effort on designing the **Ph**ysics-**C**ross-**A**tension (PhCA) module to learn the latent space to optimize the trade-off between accuracy and flexibility, and reduce the computational complexity as well. Specifically, LNO first encodes the input sample sequence into a learnable latent space by PhCA. Then, the model learns the PDE operator in the latent space via a stack of Transformer layers. Finally, the predicted output function is decoded by the inverse PhCA, given any query locations. Unlike previous works that either pre-define the latent space as frequency domain , or switch back-and-forth between latent space and geometric space at each layer , the latent space of the LNO is learnable from data and the data are only transformed at the first and last layer.

The proposed PhCA module decouples the locations of input samples fed in the encoder and output samples fed in the decoder, which can predict values in any position not limited by the observation. Therefore, it can naturally perform interpolation and extrapolation tasks, which is fundamental for solving inverse problems. The proposed LNO improves both prediction accuracy and computational efficiency. Experiments show that LNO reduces the GPU memory by 50% and speeds up training 1.8 times. Moreover, it reaches state-of-the-art accuracy on four (Pipe, Elasticity, Darcy, Navier-Stokes) out of six benchmarks for forward problems and a benchmark for inverse problem on Burgers' equation.

The main contributions of our LNO framework are summarized as follows.

* Flexibility: We propose the Physics-Cross-Attention (PhCA) module which decouples the locations of input and output samples and learns the latent space from data. We also build the Latent Neural Operator (LNO) model for solving both forward and inverse PDE problems.
* Efficiency: The LNO model transforms the data to the latent space only once. Compared with other approaches that switch the latent space and geometric space in each Transformer layer, the proposed model reduces complexity drastically in terms of GPU memory usage, training time and number of model parameters.
* Accuracy: We obtain state-of-the-art results on the forward problems of Pipe, Elasticity, Darcy, Navier-Stokes, and the inverse problem of Burgers' equation.

## 2 Related works

We first review two major classes of neural PDE approaches, physics-informed and operator-learning methods. Then, we briefly discuss the works using neural networks for solving inverse PDE problems.

### Physics-Informed Machine Learning

Physics-informed machine learning methods incorporate prior knowledge of PDEs into loss function or model architecture. The Physics-Informed Neural Network (PINN) and its variants are among the most classic methods. PINN integrates the differential equation, initial conditions and boundary conditions into the loss function, guiding the neural network to approximate the solution by ensuring its derivatives satisfies the equation and the output aligns with the initial and boundary conditions. Other methods attempt to encode physical priors into the model architecture, _e.g.,_ the FInite volume Neural Network (FINN) and Physics-encoded Recurrent Convolutional Neural Network (PeRCNN). These methods achieve high precision but require solving optimization to train the networks for each instance. In addition, in the case, where concrete PDE is unknown or not exact, it is challenging to adopt those methods, limiting the generalization ability and flexibility in practical data-rich situations.

### Operator Learning

Operator learning methods aim to find a functional map from coefficients, forcing terms and initial conditions to the solution function by learning an approximating operator. These methods train neural networks to capture the correspondence between input and output functions from data without needing to know the underlying PDE explicitly. DeepONet[13; 27] first introduces approximating operators using neural networks.

A series of kernel integral operator forms were proposed to model the mappings between functions as neural operators[28; 14; 15]. Specifically, Galerkin Transformer utilizes the Galerkin-type attention mechanism as kernel integral operators. OFormer improves Galerkin-type attention by decoupling observation positions and query positions through cross-attention. GNOT introduces heterogeneous normalized cross-attention to modulate various input conditions for query positions and alleviates computational pressure of the large grid by linear-time-complexity attention mechanism. FactFormer proposes axial factorized kernel integral, decomposing input functions into multiple one-dimensional sub-functions, effectively reducing computation. ONO enhances the generalization of neural operators by adding orthogonality regularization in attention mechanisms.

Researchers also explored the idea of modeling in the latent space. Transolver employs Physics-Attention in each layer to map back-and-forth between geometric features and physical features. LSM encodes the input function into the latent space using cross-attention, constructs a set of orthogonal bases, and then decodes the solution back to the geometric space through cross-attention again. UPT introduces encoding and decoding losses, and compresses geometric point information into super-node information in the latent space via graph neural network.

Compared with existing methods, we autonomously learn mappings between functions in the real-world space and the latent space in an end-to-end fashion with the cross-attention mechanism, without manually constructed orthogonal basis nor additionally introduced loss.

### Inverse Problem

Inverse problems of PDEs have extensive applications in fields involving sensing and reconstruction such as medical imaging and geological sensing. For instance, Transformer-based deep direct sampling methods have been proposed for electrical impedance tomography, and convolutional network methods[34; 35; 36] are widely used in full waveform inversion.

Theoretical works on solving inverse problems have also been proposed. The reconstruction of diffusion fields with localized sources has been studied from the perspective of sampling matrix in the spatiotemporal domain. PINN[38; 39; 40] approximates the solution based on partially observed data to compute the unknown data. NIO combines DeepONet and FNO to construct a framework for solving inverse problems. FINN[8; 42] sets boundary conditions as learnable parameters, and infers boundary values through backpropagation. The generative method generates the initial conditions through latent codes and refines them based on the results of time evolution. We aim to unify the solution of forward and inverse problems in the latent space through the PhCA module, which decouples observation and prediction positions.

## 3 Method

We formally define both the forward and the inverse problems of PDEs. Then, we introduce our Latent Neural Operator (LNO) model. Finally, we discuss the core module that learns the transformation between the real-world geometric space and the latent space via the cross-attention mechanism.

### Problem Setup

We consider a partial differential equation (PDE) with boundary and/or initial conditions defined on \(^{d}\) or \(^{d}[0,T)\)

\[_{a} u =0,\] (1) \[u(x) =b(x), x\]

where \(_{a}\) is an operator containing partial differentials parameterized by a set of coefficients \(a\), and \(u(x)\), \(x\) and \(b(x)\) represent the solution to the PDE and the boundary/initial conditions respectively.

A conventional forward problem aims to find the solution \(u(x)\), given the differential operator \(_{a}\) in (1) with the explicit parameters \(a\) and the boundary/initial conditions \(b(x)\). For an inverse problem, given an observed set of partial data \(\{(x,u(x))|\ x\}\), the task is to infer the PDE that generates the observed data along with possible boundary/initial conditions or the parameters \(a\) in the differential operator \(_{a}\).

Both forward and inverse problems can be unified as an operator learning task fitting into a sequence-to-sequence translation framework. An operator model estimates the mapping \(:f g\) from an input function \(f\) to an output function \(g\) by the data-driven approach using paired training data of input-output sequences \((\{f(y_{i})\}_{i},\{g(z_{j})\}_{j})\), where \(y_{i}\) and \(z_{j}\) correspond to positions in the domain of the input and output function respectively.

Specifically, for the forward problem, the inputs are samples of either boundary/initial function \(b\) or PDE coefficient function \(a\), and the output is the physical quantity \(u\) evaluated at a grid of points. Conversely, for the inverse problem, the input is a few partially observation samples \(\{(pos_{in}^{(i)},val_{in}^{(i)})\}_{i=1}^{N_{in}}=\{(x_{i},u(x_{i}))\}_{i=1}^{ N_{in}}\), and the outputs are boundary/initial function (_e.g.,_ infer initial state) or the PDE coefficient function (also coined as system identification problem).

The challenge in solving forward and inverse problems of PDEs lies in the large lengths of input and output sequences. In terms of operator approximating, complex kernel integrals lead to high computational cost while simple kernel integrals lack accuracy. In this work, we build an accurate and computationally efficient model for both forward and inverse problems by learning the operator in the learnable latent space.

### Latent Neural Operator

OverviewThe proposed model of Latent Neural Operator (LNO) is applicable for both forward and inverse PDE problems. It consists of four modules: an embedding layer to lift the dimension of the input data, an encoder to transform the input data to a learnable latent space, a series of self-attention layers for modeling operator in the latent space, and a decoder to recover the latent representation to the real-world geometric space. The overall architecture of LNO is shown in Figure 1.

Embedding inputsThe geometric space is the original space of the PDE input or output which contains \(N_{in}\) or \(N_{out}\) samples, each with \(d\)-dimensional position coordinates (which may additionally

Figure 1: The overall architecture of Latent Neural Operator.

include a \(1\)-dimensional time coordinate for time-dependent PDEs) and \(n\)-dimensional physical quantity values.

First, we embed the positions and physics quantity values of the input sequence in the geometric space to higher \(D\)-dimensional representations. This process is implemented by two MLPs, where the first only embeds the position information and the second embeds both the location and physics quantity information

\[^{(i)} =projector}pos^{(i)}_{in}, ^{(i)}^{D 1}\] \[^{(i)} =projector}(pos^{(i)} _{in},val^{(i)}_{in}), ^{(i)}^{D 1}.\]

Analogous to DeepONet , we name these MLPs as trunk-projector and branch-projector respectively. The embedding operation helps to map the locations and physics quantities to the embedding space where their relationships are easier to capture.

Removing the physics quantity values from the trunk-projector input grants our model the decoupling property. In this way, we can predict values at positions not included in the sampled input function from the latent space, thus achieving the decoupling of observation and prediction positions. Since the sampled input function always includes position-value pairs, using both position and value information in the branch projector will not affect the decoupling objective.

Encoding to the latent spaceWe attempt to represent the input function using the representation tokens of \(M\) hypothetical sampling positions \(\{h^{(k)}\}_{k=1}^{M},h^{(k)} R^{D 1}\) which exist in the latent space, where \(M\) can be much smaller than the number \(N_{in}\) of samples of raw inputs.

We use the Physics-Cross-Attention to model the transformation from the geometric space to the latent space. The inputs are query \(=[h^{(1)},h^{(2)}...,h^{(M)}]^{T}\), key \(}=[^{(1)},^{(2)},,^{(N_{in})}]^{T}\) and value \(}=[^{(1)},^{(2)},,^{(N_{in})}]^{T}\) which corresponding to embedding of hypothetical locations in the target latent space, embedding of locations in the source real-world geometric space, and embedding of the concatenation of locations and physics quantities in the source real-world geometric space respectively. The output can be calculated through

\[^{0}=CrossAttention}(,}, })\] (2)

In contrast to FNO, where the latent space is pre-defined as the frequency domain, we learn the latent space from data. Therefore, the embeddings of sample locations \(\) as queries in the cross-attention are also learnable parameters.

After encoding the input function into the latent space, the length of the sequence to be processed is reduced from \(N_{in}\) to \(M\). With \(M N_{in}\), extracting and converting the feature of the input function in the latent space will be much more efficient than in the real-world geometric space.

Learning operator in the latent spaceWe model the operator of the forward/inverse PDE problem in the latent space to convert the feature of the input function to that of the output function, using a stack of Transformer blocks with the self-attention mechanism

\[^{l}=}^{l}))+}^{l} \ }^{l}=^{l-1}))+^{l-1}}\]

where \(l\{1,2,...,L\}\) is the index of Transformer block with \(L\) being the total number of the blocks. We experiment with several implementations of attention mechanisms in the ablation study and find the classical scaled dot-product attention performs consistently well on various tasks.

Decoding to the geometric spaceFinally, we decode the output sequence from the latent sequence through Physics-Cross-Attention according to the query \(pos_{out}\) of the output sampling locations and use another MLP to map the embeddings to the values of the output function

\[p^{(j)} =projector}(pos^{(j)}_{out}), p^{(j)}^{D 1}\] \[ =CrossAttention}(,,^{L})\] (3) \[val^{(j)}_{out} =(u^{(j)}), u^{(j)}^{D 1}\]

where \(=[p^{(1)},p^{(2)},,p^{(N_{out})}]^{T}\) and \(=[u^{(1)},u^{(2)},,u^{(N_{out})}]^{T}\). During the inferring phase, the trunk-projector can accept inputs from any position, enabling LNO to generalize to the unseen region where the positions are not presented in the training phase.

### Physics-Cross-Attention

The **P**hysics-Cross-Attension (PhCA) is the core module in the aforementioned latent neural operator (LNO) model, acting as the encoder and decoder which transform the data representation back-and-forth between the real-world geometric space and the latent space. Motivated by the work  that learns the operator mapping across domains using the cross-attention, we use the same cross-attention to model the transformation to/from the latent space but set the latent space learnable rather than pre-defined, _e.g.,_ triangular space or frequency domain .

In particular, the rows of the query \(\) and key \(}\) matrices in (2) represent the embedded positions of samples in the target and source space respectively. A row of the value matrix \(}\) is the embedding of both the position and physics quantity of a sample in the source space. We design the latent space to be learned from data. Thus, the positions \(\) of samples in the latent space should be set as learnable parameters as well. Then, the cross-attention of the encoder in (2) is simplified as

\[^{0}=}_{q}_{k }^{T}}^{T}}_{v}=(_{1} }^{T})}_{v},\] (4)

where we merge the learnable matrices of query \(\), query weight \(_{q}\) and key weight \(_{k}\) as a single matrix \(_{1}\).

Correspondingly, the cross-attention in the decoder has a learnable key matrix that represents positions of samples in the latent space. Therefore, the cross-attention of the decoder in (3) is written as

\[=}_{q}^{{}^{}}_{k}^{{}^{}T}^{T}^{L}_{v}^{{}^{}}= (_{2}^{T})^{L}_{v}^{{}^{}}.\] (5)

In addition, utilizing the relationship of the encoder and decoder as mutually inverse transformations, we set \(_{1}=_{2}\) to reduce the number of parameters, which also improves model performance experimentally. Practically, we generalize the linear projection \(_{1},_{2}\) with an MLP for further improvement, which is named attention-projector as shown in Figure 2.

In the special case where the input only contains sample positions without physics quantities, our PhCA module is similar to the transformation of physics-aware token in Transolver  (only differing in normalization). In general cases when the inputs include both positions and physics quantities, the two approaches are different. Specifically, in Transolver, the key \(}\) in (4) and query \(\) in (5) contain both positions and physics quantities information, whereas in PhCA, the key matrix \(}\) in the encoder and the query matrix \(\) in the decoder only depend on the positions of the input and output respectively. The physics quantities are used only to produce the value matrix \(\) in attention mechanism. Such decoupling property of values and locations in query and key enables LNO to predict values at different positions from input positions.

## 4 Experiments

We first evaluate the accuracy for the forward problems on the six popular public benchmarks and for the inverse problem using Burger's equation. Then, we measure the efficiency in terms of the number of model parameters, the GPU memory usage and the training time. Next, we assess the generalization of our model concerning the number \(N_{in}\) of observation samples and the number \(N_{out}\) of prediction positions. Finally, we establish a series of ablation studies including choices of different attention mechanisms, strategy of sharing weights and sampling numbers in the latent space. Model is developed on the PyTorch and the PaddlePaddle deep learning framework. Additional experiments on scaling, sampling rate, and significance are presented in the appendix.

Figure 2: The working mechanism of Physics-Cross-Attention in encoder and decoder respectively.

### Accuracy for the forward problems

We conduct experiments for the forward problem on six benchmarks including Darcy and NS2d on regular grids (refer to section 5.2, 5.3 in  for details), and Airfoil, Elasticity, Plasticity and Pipe problems on irregular grids (refer to section 4.1-4.5 in  for details).

Table 1 shows a comprehensive comparison with previous works that reported their performance on the same benchmarks. Our LNO achieves the best accuracy on four benchmarks: Darcy, NS2d, Elasticity and Pipe. On the Airfoil and Plasticity benchmarks, our LNO achieves performance close to the SOTA with only half computational cost as discussed in Section 4.3. These results demonstrate the effectiveness of approximating the kernel integral operator in the latent space after transformation from the real-world geometric space through the PhCA module.

### Accuracy for the inverse problem

To demonstrate the flexibility of LNO, we design an inverse problem. Given the partially observed solution \(u(x)\), the aim is to recover the complete \(u(x)\) in a larger domain. Specifically, we test on 1D Burgers' equation

\[u(x,t)=0.01}{  x^{2}}u(x,t)-u(x,t)u(x,t), x, \ t\] \[u(x,0)(0,-} ^{2}(||x-x^{{}^{}}||), u(0,t)=u(1,t)\]

The ground-truth data is generated on a \(128 128\) grid with the periodic boundary condition. Initial conditions are sampled from the Gaussian process with the periodic length \(p=1\), scaling factor \(l=1\).

The objective of this inverse problem is to reconstruct the complete solution \(u(x)\) in the whole spatiotemporal domain \((x,t)\) based on sparsely random-sampled or fixed-sampled observation in the sub-spatiotemporal domain \((x,t)[0.25,0.75]\), as illustrated in Figure 6 in the appendix.

Instead of the naive approach which directly learns the mapping from partially observed samples in the subdomain to the complete solution in the whole domain, we propose a two-stage strategy inspired by inpainting and outpainting. First, we train the model as a completer to interpolate the sparsely sampled points in the subdomain \([0.25,0.75]\) (left sub-figure in Figure 6) to predict all densely and regularly sampled points in the same subdomain (middle sub-figure in Figure 6). Then, we train the model as a propagator to extrapolate the results of the completer from the subdomain to the whole domain \(\) (right sub-figure in Figure 6). Since the observation and the prediction samples are located in different positions, only the models with decoupling properties can be used as the completer and propagator.

    &  & \))} \\   & & Darcy & NS2d & Airfoil & Elasticity & Plasticity & Pipe \\  FNO & N & 1.08 & 15.56 & / & / & / & / \\ Geo-FNO & N & 1.08 & 15.56 & 1.38 & 2.29 & 0.74 & 0.67 \\ F-FNO* & N & 0.75 & 11.51 & 0.60 & 1.85 & 0.27 & 0.68 \\ U-FNO* & N & 1.28 & 17.15 & 1.19 & 2.13 & 0.41 & 0.62 \\ LSM & N & 0.65 & 15.35 & 0.59 & 2.18 & 0.25 & 0.50 \\  Galerkin & N & 0.84 & 14.01 & 1.18 & 2.40 & 1.20 & 0.98 \\ OFormer & Y & 1.24 & 17.05 & 1.83 & 1.83 & 0.17 & 1.68 \\ GNOT* & Y & 1.04 & 13.40 & 0.75 & 0.88 & 3.19 & 0.45 \\ FactFormer & N & 1.09 & 12.14 & 0.71 & / & 3.12 & 0.60 \\ ONO & Y & 0.76 & 11.95 & 0.61 & 1.18 & 0.48 & 0.52 \\ Transolver* & N & 0.58 & 8.79 & **0.47** & 0.62 & **0.12** & 0.31 \\ LNO(Ours) & Y & **0.49** & **8.45** & 0.51 & **0.52** & 0.29 & **0.26** \\   

Table 1: Prediction error on the six forward problems. The best result is in bold. “\(*\)” means that the results of the method are reproduced by ourselves. “\(/\)” means that the method can not handle this benchmark. The column labeled D.C. indicates whether the observation positions and prediction positions are decoupled.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

the former set of benchmarks containing only location information, while the latter set involves both location and physics quantity information, which requires more sampling positions to represent. In the inverse problem, the performance of the complete and propagator are similar to the trends in the forward problems.

## 5 Conclusions

The paper explores the feasibility of characterizing PDEs in a learnable latent space. We present **Ph**ysics-**C**ross-**A**t**nelson for encoding inputs into the latent space and for decoding outputs. We build the Latent Neural Operator (LNO) based on PhCA and validate its flexibility, efficiency and accuracy for solving both forward and inverse problems. We leave the generalization ability and re-usability of PhCA across multiple types of PDEs as another open research topic.

Our work has also some limitations. For time-dependent equations, we decode the states at intermediate time steps into geometric space to compute the loss. Implementing additional training strategies may allow us to conduct all operations of the dynamic evolution process directly in the latent space, which potentially enhances efficiency. Also, in our work, no prior is imposed on the latent space. However, in the future, it is worth exploring whether physical priors, _e.g.,_ symmetry, can help gain further precision improvement.

    & \))} \\   & Darcy & NS2d & Airfoil & Elasticity & Plasticity & Pipe \\  LNO with P.A. & 0.53 & 22.65 & 0.74 & 0.88 & 0.49 & 0.40 \\ LNO with E.A. & 0.63 & 20.18 & 0.59 & 0.54 & 0.44 & 0.36 \\ LNO with N.A. & 0.67 & 23.03 & 0.63 & 0.79 & 0.30 & 0.48 \\ LNO with G.A. & 0.71 & 22.44 & 0.89 & 0.93 & 0.43 & 0.39 \\ LNO & **0.49** & **8.45** & **0.51** & **0.52** & **0.29** & **0.26** \\   

Table 6: The ablation results using different attention implementations. P.A. stands for Physics-Attention. E.A. stands for Efficient Attention, N.A. stands for Nystrom Attention and G.A. stands for Galerkin-type Attention.

    & \))} \\   & Darcy & NS2d & Airfoil & Elasticity & Plasticity & Pipe \\  LNO(non-shared) & 0.54 & 10.67 & 0.53 & 1.09 & **0.26** & 0.36 \\ LNO & **0.49** & **8.45** & **0.51** & **0.52** & 0.29 & **0.26** \\   

Table 7: The ablation results of strategy of sharing weights on six forward problem benchmarks. Relative L2 is recorded. Smaller values represent better performance. The better result is in bold.

Figure 3: Sampling numbers in latent space v.s. Accuracy. (a) Forward problems on Airfoil, Elasticity, Plasticity and Pipe. (b) Forward problems on Darcy and NS2d. (c) Inverse Problem.