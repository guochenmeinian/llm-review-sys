# SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection

Yachao Liang\({}^{1,2}\) Min Yu\({}^{1,2}\) Gang Li\({}^{3}\) Jianguo Jiang\({}^{1,2}\) Boquan Li\({}^{4}\)

**Feng Yu\({}^{5}\) Ning Zhang\({}^{6}\) Xiang Meng\({}^{1,2}\) Weiqing Huang\({}^{1,2}\)**

\({}^{1}\)Institute of Information Engineering, Chinese Academy of Sciences

\({}^{2}\)School of Cyber Security, University of Chinese Academy of Sciences

\({}^{3}\)Deakin University \({}^{4}\)Harbin Engineering University

\({}^{5}\)Institute of Computing Technology, Chinese Academy of Sciences

\({}^{6}\)Institute of Forensic Science, Ministry of Public Security

{liangyachao, yumin}@iie.ac.cn

Corresponding authors

###### Abstract

Detection of face forgery videos remains a formidable challenge in the field of digital forensics, especially the generalization to unseen datasets and common perturbations. In this paper, we tackle this issue by leveraging the synergy between audio and visual speech elements, embarking on a novel approach through audio-visual speech representation learning. Our work is motivated by the finding that audio signals, enriched with speech content, can provide precise information effectively reflecting facial movements. To this end, we first learn precise audio-visual speech representations on real videos via a self-supervised masked prediction task, which encodes both local and global semantic information simultaneously. Then, the derived model is directly transferred to the forgery detection task. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of cross-dataset generalization and robustness, without the participation of any fake video in model training. The code is available here.

## 1 Introduction

The rapid advancement of generative models enables synthetic realistic facial images [20; 39; 35], and they have significantly enhanced face manipulation techniques, allowing for the replacement of facial identities and the modification of attributes such as expressions [60; 59] and lip movements[1; 3]. While these advancements offer vast potential for entertainment and filmmaking, they also harbor the risk of misuse for deceptive purposes.

In response to these concerns, there has been a surge in the development of face forgery detection methodologies grounded in deep learning [4; 55; 63; 71; 43; 70; 30; 64]. Despite these efforts, it is widely acknowledged that face forgery detectors frequently experience a decline in effectiveness when confronted with novel manipulation techniques [19; 12; 45]. This vulnerability poses significant hurdles to the reliable application of these detection systems, highlighting a critical area for ongoing research and innovation.

To enhance the generalization capabilities of face forgery detectors, researchers have proposed various methods aimed at mining more discriminative clues [70; 62; 71; 29; 64]. Some works focus on detecting spatial artifacts left in the process of facial manipulation [12; 43; 70; 9], especially blending boundaries [43; 57]. However, these methods are sensitive to common perturbations, making them difficult to generalize to real-life scenarios. Another line of research resorts to modeltemporal features [67; 29; 69; 64], considering that fake videos are synthesized in a frame-by-frame manner. They identify unnatural facial movements existing in fake videos by applying special architectures  or introducing auxiliary tasks [18; 30; 29]. Although showing promising results, short-term information modeling capacity (e.g., 1 second [71; 30]) makes them overfit to specific low-level temporal features to varying degrees, resulting in their suboptimal generalization on unseen datasets and perturbations, as observed in our experiments.

This motivates us to find more general semantic-level features to detect anomalous facial movements. Recent efforts on audio-visual speech recognition have shown that accurate speech contents can be extracted from both audio signals and lip movements [56; 28; 46]. Inspired by this, we conjecture that audio signals could provide strong semantic supervision for identifying inaccurate lip movements in fake videos, given that lip sequences and audio segments in a real video should convey the same speech contents. This brings us to the key problem: _how to extract semantically rich speech-related features to represent detailed lip movements?_

An intuitive solution is to align the speech representations of each frame of audio segments and lip sequences directly, as in . However, this method will fail to detect fake videos processed by lip synchronization techniques, such as Wav2Lip, commonly used by recent talking face generation technologies. Considering that local lip synchronization cannot bring long-range temporal coherence, we further propose to perform forgery detection by learning audio and visual speech representations in a framework that encodes both phonetic and linguistic information, which we term as local and global information. Specifically, it learns local information by frame-wisely audio-visual representation alignment and models global dependencies via masked prediction task, following previous speech representation learning methods [56; 28; 73]. In this way, both short-range and long-range temporal features are learned. After learning audio-visual speech representation on real videos, we directly transfer the trained model to the forgery detection task by finding discrepancies between visual and audio speech representations in fake videos.

Thanks to the unsupervised manner and high-level semantic learning, our method, termed SpeechForensics, avoids overfitting on forgery features and shows strong robustness on various perturbations. We conduct comprehensive experiments to evaluate the effectiveness of our method, and it shows strong performance under different manipulations, datasets, and perturbations. Especially, our method achieves the AUC of 99.0% on FakeAVCeleb  and 91.7% on KoDF . Our main contributions are summarized as follows:

* We propose to perform face forgery video detection by extracting speech representations from audio and visual streams. It learns on real videos and can smoothly transfer to the forgery detection task, markedly streamlining the forgery detection workflow.
* We demonstrate a simple framework, which encodes both short-range and long-range temporal information, is well-suited to our method. And we tailor it to a face forgery detector using the proposed modality alignment module.
* Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods in terms of cross-dataset generalization, robustness, and interpretability, in an unsupervised manner.

## 2 Related Work

Face Forgery Detection.Initial approaches in face forgery detection predominantly treat the task as a binary classification problem, leveraging deep learning models trained on datasets specifically compiled for detecting forgeries [4; 55; 21; 42]. For instance,  introduces a pair of detection networks known as Mesonet and MesoInception, demonstrating that lightweight neural networks can effectively undertake forgery detection tasks. Analogously,  highlights the superior performance of an unconstrained Xception network over its predecessors, focusing primarily on the analysis of spatial details within individual frames. Subsequently, some works [27; 8; 68; 26; 22; 25] try to combine temporal networks to perform forgery detection. Despite the promising results in the in-dataset setting, these vanilla methods usually suffer from severe performance degradation when facing unseen forgeries.

General Forgery Detection.To boost the generalization of detectors on unseen forgeries, researchers attempt to find more discriminative features at both image and video levels.

Image-based methods [12; 43; 70; 57; 9] analyze the spatial artifacts common to forged faces and generate synthetic data to guide models to focus on them. For example, Face X-ray and SBI detect blending boundaries caused by the fusion of the forged face and background, and AUNet concentrates on the relation between different facial action units. While they are adept at identifying specific artifacts, these artifacts are easily destroyed by some common perturbations, e.g., compression, which makes it difficult to generalize to real scenarios.

On another front, video-based methods make efforts to explore temporal clues via special network architectures [71; 64] or auxiliary tasks [44; 30; 29; 69]. FTCN reduces the convolutional kernel size to 1 forcing the network to only focuses on temporal features. LipForensics uncovers unnatural lip movements via pre-training on the lipreading task and finetuning on forgery datasets. Analogously, RealForensics leverages cross-modal self-supervision learning to capture facial movements. Despite their notable performance, they tend to rely on short-range low-level temporal features, leading to their limited generalization on new datasets and robustness against common perturbations. In contrast, our method detects both short-range and long-range anomalous facial movements using semantic-level information and functions in an unsupervised manner, inherently possessing superior generalization and robustness.

Audio-Visual Speech Representation Learning.The advent of extensive large-scale audio-visual speech datasets [6; 7; 16] has spurred the development of numerous audio-visual speech representation learning methods in recent years [17; 53; 56; 46].  learns visual and audio speech representations simultaneously based on synchronization signals between lip movements and corresponding audio segments. Benefiting from off-the-shelf audio speech recognition models,  proposes to learn visual speech representations by minimizing the distance between learned visual embeddings and pre-trained audio embeddings.  further advances this field by simultaneously learning visual and auditory speech representations through student-teacher networks. We choose AUHuBERT as the implementation of our audio-visual speech representation learning, considering it fits into our framework, i.e., learning both local and global semantic representations, and demonstrates remarkable efficacy in downstream speech recognition tasks.

## 3 Method

Our method consists of the audio-visual speech representation learning stage and the face forgery detection stage. We first learn semantically rich visual and audio speech representations in a unified feature space from real videos, which can be implemented by many audio-visual speech representation learning approaches [56; 73; 28]. Subsequently, the model leverages these representations to pinpoint discrepancies between lip movements and corresponding audio segments in fake videos.

### Speech Representation Learning

In order to simultaneously learn local and global speech information, strongly correlated with short-range and long-range lip movements, we conduct frame-wise audio-visual representation alignment and the masked prediction task. Since we mainly focus on the forgery detection task, we will briefly introduce the representation learning stage, for details refer to .

Local representation alignment.Considering \(=\{(^{i},^{i})\}_{i=1}^{N}\) as the set of audio-visual pairs extracted from real videos. Given a visual and audio pair \((_{1:T},_{1:T})\) from the set \(\), where \(T\) represents the sequence length of clip. We first get intermediate features \(F^{v}_{i:T}=f^{v}_{e}(_{1:T})\) and \(F^{v}_{i:T}=f^{a}_{e}(_{1:T})\) through the visual frontend \(f^{v}_{e}\) and the audio frontend \(f^{a}_{e}\), respectively. And \(F^{v}\) and \(F^{a}\) are fused by channel-wise concatenation, before which modality dropout is applied to allow the unimodal input. Then the fused features are fed into subsequent transformer encoder to learn frame-wise speech representations, as shown in fig. 1. The target labels for training are derived through cluster assignment , which are initialized based on MFCC features of audio and iteratively refined with audio-visual features learned by encoders via k-means. And we denote them as \(_{1:T}\{1,2,,C\}\), where \(C\) is the size of the codebook. By this means, the representations of every frames of visual and audio modalities are aligned in a unified feature space.

Global information modeling.Following the described procedure, the visual and audio speech representations for each frame are synchronized, facilitating the computation of their similarity in subsequent analyses. However, local speech contents conveyed by lip movements, i.e., visemes, only contain limited temporal features and can easily be tampered by lip-sync methods, e.g., Wav2Lip. To address this problem, we further introduce global temporal information modeling.

We employ the masked-prediction task, a method extensively utilized across various fields [37; 36; 11], to model contextual dependencies effectively. Let \(M_{v},M_{a}\{1,2,,T\}\) represent the sets of indices of visual and audio masked sequences, this task can be formulated as:

\[=_{t M_{v} M_{a}} p(_{t}^{v}, ^{a})\] (1)

where \(^{a}\) and \(^{v}\) denotes of the corrupted audio features and visual features, respectively.

### Face Forgery Detection

For the forgery detection, we aim to identify discrepancies between visual and audio speech representations in manipulated videos. To achieve this, we extract visual embeddings as visual speech representations by feeding the learned model with only visual inputs, and apply the same procedure for audio embeddings. Notably, obtaining accurate speech representations from the audio stream is more straightforward, allowing these to function effectively as pseudo-labels.

Given the visual and audio speech representations of any video, we get their matching score by calculating frame-wise cosine similarity, which can be formulated as:

\[(e^{v},e^{a})=_{t=1}^{T}sim(e_{t}^{v},e_{t}^{a})\] (2)

where \(e_{v}\) and \(e_{a}\) represent the visual and audio embeddings extracted from the final layer of our model, respectively, and \(sim(,)\) is the cosine similarity between two vectors. And videos exhibiting low matching scores are consequently classified as forgeries.

Another crucial problem is the time offsets between visual and audio signals, which inevitably exist even in real videos due to recording or encoding errors [5; 24]. To address this problem, we consider two time offset assumptions, i.e., fixed and dynamic, and introduce different modality alignment approaches to alleviate their impact. See the intuitive illustration of these two assumptions in fig. 1.

Assumption 1: Fixed time offset.First, we assume that the frame offsets between visual and audio streams remain constant throughout the video. We apply a sliding-window technique, as in , to correct time offsets prior to calculating final matching scores. Specifically, based on the assumption that the maximum offset between visual and audio streams is \(\), we compute the cosine similarity between the feature of each visual frame and its adjacent audio features within a window of \(\)

Figure 1: **Overview of the proposed method. During the stage of audio-visual speech representation learning, local speech representations and global information are learned by frame-wise feature alignment and the masked prediction task, respectively. In the stage of forgery detection, we separately feed the whole lip movement sequence and audio stream of a video into the learned model to get visual and audio speech representations. And we flag videos with low matching scores between visual and audio speech representations as fake videos.**

frames. Thereafter, the highest average cosine similarity across this window is taken as the overall similarity of the video. Consequently, the similarity eq. (2) can be re-formulated as:

\[(e^{v},e^{a})=_{t- i t+}_{t=1}^{T} sim(e_{t}^{v},e_{i}^{a})\] (3)

where we pad zero vectors as \(e_{i}^{a}\) when \(i<1\) or \(i>T\).

**Assumption 2: Dynamic time offset.** On the contrary, we also consider the dynamic assumption, i.e., the frame offsets between visual and audio signals vary over time. Based on this assumption, we introduce the _Dynamic Time Warping_ (DTW) algorithm , commonly used to align and calculate the similarity of two time series data. In this case, eq. (2) will be re-written as:

\[(e^{v},e^{a})=DTW(e_{1:T}^{v},e_{1:T}^{a})\] (4)

where we also apply cosine similarity as the cost measure of DTW.

## 4 Experiments

### Experimental Setup

Dataset.We evaluate our methods across three distinct video forgery datasets: Faceforensics++ (FF++) , FakeAVCeleb and KoDF . Note that Only the FakeAVCeleb contains videos belong to the Real-Visual-Fake-Audio category, and we exclude them as we focus on the facial forgery and to maintain fairness of experiments.

**Faceforensics++** contains 1,000 real videos alongside 4,000 fake videos, created via four different manipulation methods. These include face swapping methods (DeepFakes, FaceSwap), and two face reenactment methods (Face2Face and NeuralTextures). For our evaluation, We re-download videos using the provided YouTube IDs and extract audio segments from the provided frame locations. Contrary to the common practice of treating Faceforensics++ as a visual-only dataset, we paired the original videos with their corresponding audio segments to create an audio-visual test dataset. After excluding videos unavailable or containing non-corresponding mouth movements and voices, we selected 500 videos from each category for testing.

**FakeAVCeleb** contains 500 real videos and 19,500 fake videos. It is derived from VoxCeleb2 and represents diverse ethnic backgrounds, ages and genders. This dataset involves four manipulation techniques, Faceswap and Faceswap GAN (FSGAN)  for face swapping, SV2TTS for real-time cloning voice, and Wav2Lip for audio-driven facial reenactment.

**KoDF** is a large-scale Korean forgery datasets, containing 62,166 real videos and 175,776 fake videos. For our evaluation, we focus on fake videos crafted using four distinct manipulation techniques: FaceSwap, DeepFaceLab, FOMM, Audio-driven (including ATFHP and Wav2Lip). From each of these categories, we randomly select 1,000 videos to compile our testing set.

Preprocessing.We first utilize FFmpeg to convert all videos into 25fps and audio into 16kHz sample rate. For each video clip, we initiate the process by identifying faces using RetinaFace and subsequently extract facial landmarks with FAN. We then align the frames using affine transformations, and crop \(96 96\) regions centered around the mouth, as indicated by the landmarks. For the audio part, we extract MFCC features from wavform every 10ms, and we concatenate 4 adjacent audio frames to align with visual modality.

Architecture & Training.The audio-visual speech representation model consists of visual frontend, audio frontend and masked predictor. Following AVHuBert, We use the Resnet-18 2D+3D as the visual frontend. And the audio frontend contains only a single linear projection layer to avoid the over-reliance issue on the audio stream. The masked predictor is implemented by the standard transformer encoder. Further details can be found in appendix A.1.

The model is trained on LRS3 and VoxCeleb2 datasets, which contain 433 and 1326 hours of videos respectively. The training process adhere to the methodology outlined by . For the purposes of our experiments, unless otherwise noted, we utilize a publicly available pretrained model 2.

### Quantitative Comparisons

We compare our method with several state-of-the-art detectors, including Xception, Patch-based, Face X-ray, LipForensics, FTCN, RealForensics and AVAD. Moreover, we also construct a model, consisting of two Resnet 2D models, to extract only phonetic-level (5 frames) speech information for comparison, And we dub it SpeechForensics-Local. See appendix A.2 for more details about above detectors.

We undertake extensive experiments focusing on the generalization and robustness of detectors. In line with established practices in the field , we utilize the area under the receiver operating characteristic curve (AUC) to gauge the efficacy of our method at the video level. Unless otherwise specified, we take the fixed time offset assumption.

Cross-Manipulation Generalization.In real-world situations, detectors frequently encounter novel manipulation techniques, underscoring the necessity for these systems to possess robust generalization capabilities against unseen manipulations. To assess our method's ability to generalize across different manipulation methods, we conducted evaluations on the widely recognized FF++ high-quality (HQ) dataset. For supervised methods, the experiments adopt the leave-one-out strategy, in line with established practices . It is noteworthy that the settings for both cross-manipulation and cross-dataset are equivalent for unsupervised methods, i.e., AVAD  and our method.

The AUC results presented in table 1 demonstrate that our method either matches or exceeds performance across different categories, notably without utilizing any forgery samples. Remarkably, our approach achieves perfect results (100%) with the two face reenactment methods, Face2Face and NeuralTextures. However, the performance on FaceSwap (91.1%) is slightly lower compared to the other categories, a trend that aligns with findings from related methods such as LipForensics and RealForensics). This could be attributed to FaceSwap's use of target face landmarks for generating source faces, which might result in more precise lip shapes. Nonetheless, our method demonstrates the significant capability in detecting such forgeries through contextual analysis, indicating its effectiveness against diverse manipulation techniques. Notably, our method significantly outperforms AVAD, which nearly produces random results. And we note that SpeechForensics-Local also achieves considerable performance on this dataset, although modeling local speech information.

Cross-Dataset Generalization.We further extend our evaluation to include a cross-dataset comparison, aligning with the practise in prior works . This involves testing the performance of our method on the unseen datasets FakeAVCeleb and KoDF, with the supervised models initially trained on the FF++ dataset. In addition, we also report the results of every category within the FakeAVCeleb dataset, which is segmented into five categories based on the manipulation techniques used Faceswap (FS), FSGAN, Wav2Lip (WL), Faceswap-Wav2Lip (FS-WL) and FSGAN-Wav2Lip (FSGAN-WL), with the latter two categories indicating the combined use of manipulation methods.

The results in table 2 show that our method significantly outperforms both supervised and unsupervised counterparts in the cross-dataset setting, outperforming previous state-of-the-art method, RealForensics, by 8.8% on FakeAVCeleb and 7.4% on KoDF. It is worth noting that SpeechForensics-Local fails to detect fake videos generated by Wav2Lip, as we mentioned above, suggesting the key role of global temporal information for forgery video detection. Conversely, AVAD shows promise in detecting forgeries generated by Wav2Lip, though its performance on other types of forgery is yet to be fully assessed. Our approach, in contrast, delivers exceptional performance across all forgery types, achieving a perfect 100% on Wav2Lip-manipulated video. This underscores the fact that accurate lip synchronization at a local level does not necessarily imply global semantic integrity. We also provide more multimodal experiment results in appendix A.3.

Cross-Language Generalization.Considering the linguistic diversity encountered in real-world video content, we expand our evaluation to assess the cross-language generalization capabilities of

    &  \\   & DF & FS & F2F & NT & **Avg** \\   & Xception & 93.9 & 51.2 & 86.8 & 79.7 & 77.9 \\  & Pack-based & 94.0 & 60.5 & 87.3 & 84.8 & 81.7 \\  & Face X-ray & 99.5 & 93.2 & 94.5 & 92.5 & 94.9 \\  & DialForensics & 99.7 & 90.1 & 99.7 & 99.1 & 97.1 \\  & RealForensics & **100**, 96.1 & 99.5 & 97.0 & 98.1 \\  & FTCN & 99.8 & **99.6** & 89.2 & 95.6 & **98.3** \\   & AVAD & 59.2 & 55.1 & 59.9 & 58.4 & 58.2 \\  & SpeechForensics-Local & 95.6 & 74.9 & 95.1 & 89.1 & 88.7 \\   & SpeechForensics (ours) & 99.4 & 91.1 & **100**, **100** & 97.6 \\   

Table 1: **Cross-manipulation generalization.** We report video-level AUC (%) on FF++, which contains four manipulation methods, i.e., Deepfakes (DF), FaceSwap (FS), Face2Face (F2F) and NeuralTextures (NT). \({}^{*}\) denotes results of our reproduction.

our approach. To categorize the languages present in the FF++ dataset, we utilize Whisper for language detection and subsequently split the videos into various language categories: including _English_ (EN), _Arabic_ (AR), _Spanish_ (ES), _Russian_ (RU), _Ukrainian_ (UK), _Tagalog_ (TL) and others. The results, presented in table 3, illustrate the AUC scores achieved for each language category. Our findings indicate that our method maintains effective performance across different languages, even though it was originally trained on datasets predominantly in _English_. This outcome underscores the versatility of the audio-visual speech representations learned by our model, demonstrating their language-agnostic nature and highlighting the method's potential applicability in diverse linguistic contexts.

Robustness to Unseen Perturbations.Considering the prevalence of image post-processing operations on social media platforms, such as compression, the robustness of detection systems emerges as a crucial challenge. In line with previous studies , we evaluate the robustness of our method against various perturbations in the FF++ dataset, and these include Color saturation change, Color contrast change, Block-wise distortion, Gaussian noise, Gaussian blur, Pixelation and Video compression, each applied at 5 different intensity levels as in .

The results in fig. 2 compare the performance of our method against five supervised baselines under these conditions.It can be found that detectors primarily relying on low-level texture features, such as Face X-ray, Xception and FTCN, are vulnerable to most of the common perturbations. Notably, while Face X-ray and Xception show diminished effectiveness against

    &  &  &  &  \\    & & FS & FSGAN & WL & FS-WL & FSGAN-WL & & **Overall** \\   & Xception & 67.0 & 62.5 & 59.7 & 57.2 & 68.0 & 61.6 & 77.7 & 69.7 \\  & Patch-based & 97.4 & 80.5 & 78.9 & 93.8 & 87.8 & 83.6 & 83.9 & 83.8 \\  & Face X-ray & 89.9 & 85.4 & 69.5 & 84.4 & 87.6 & 78.4 & 83.0 & 80.7 \\  & LipForensics & 89.5 & 96.4 & 85.6 & 87.2 & 95.8 & 89.8 & 59.6 & 74.7 \\  & FTCN & 89.3 & 79.9 & 80.6 & 85.2 & 86.1 & 82.3 & 76.5 & 79.4 \\  & RealForensics & **98.1** & **100.** & 81.0 & 94.7 & 99.2 & 90.2 & 84.3 & 87.3 \\   & AVAD & 52.8 & 53.9 & 93.9 & 95.8 & 94.3 & 85.0 & 58.0 & 71.5 \\  & SpeechForensics-Local & 69.3 & 85.4 & 0.10 & 0.08 & 0.08 & 19.0 & 48.3 & 33.7 \\  & SpeechForensics (ours) & 93.9 & 96.0 & **100.** & **99.9** & **99.9** & **99.0** & **91.7** & **95.4** \\   

Table 2: **Cross-dataset generalization. Video-level AUC (%) on FakeAVCeleb and KoDF. We report the results of every categories of FakeAVCeleb, and the overall performance on it is reported in Overall. The average performance over two datasets is reported in Avg.**

   Language & EN & AR & ES & RU & UK & TL & Others \\  AUC & 97.8 & 98.3 & 98.3 & 100. & 99.3 & 99.7 & 97.2 \\   

Table 3: **Cross-language generalization. AUC (%) scores on videos of different languages in the FF++.**

Figure 2: **Robustness to unseen perturbations. Video-level AUC scores (%) are reported under different perturbations. Each perturbation contains five intensity levels . “Average” denotes the mean of each perturbation under each intensity level.**

perturbations that attenuate high-frequency content (e.g., _blur_, _compression_), FTCN is particularly sensitive to perturbations that disrupt temporal coherence (e.g., _noise_).

Besides, the three video-based methods, FTCN , LipForensics  and RealForensics , have all shown sensitivity to Block-wise distortion, suggesting they rely on low-level temporal features to some extent. Conversely, our method demonstrates exceptional robustness against all types of perturbations. Moreover, our approach consistently outperforms LipForensics, which also models lip movements, across various corruption scenarios, although starting from a lower point (97.6% vs 99.6%). This indicates that our method is capable of harnessing more potent semantic representations for the purpose of forgery detection, despite being trained exclusively on real data. We provide perturbation examples and more results in appendix A.4.

### Qualitative Results

Visualization.To demonstrate the effectiveness of our method, we conduct an in-depth visual analysis utilizing FF++ and FakeAVCeleb datasets. Specifically, we included all forgery categories from FF++, i.e., Deepfakes, FaceSwap, Face2Face and NeuralTextures. From the FakeAVCeleb dataset, we focused on manipulations made with FSGAN and Wav2Lip, selecting a random sample of 500 videos. As a result, our experiment covers a total of six types of forgery.

For each type of forgery, we calculate the cosine similarity between visual and corresponding audio speech representations extracted from videos. Figure 3 shows the cosine similarity distribution for each category. As we can see, almost all types of fake videos are clearly differentiated from real videos around a cosine similarity threshold of 0.3. An interesting finding is that our method exhibits exceptional performance on face reenactment techniques, i.e., Face2Face, NeuralTextures and Wav2Lip, likely due to these methods prioritizing overall visual fidelity at the expense of accurate lip movements. More visualized results are in appendix A.5.

Interpretative Analysis.Exploring another intriguing aspect, we delve into understanding how our method functions and what the derived audio-visual representations signify. To shed light on this, we conduct an interpretative analysis on the FF++. Specifically, we transcribe the extracted representations using an audio-visual speech recognition model as proposed by , fine-tuned for lipreading with the pretrained audio-visual speech representation model. This enabled us to transcribe specific speech content from both the lip movements and corresponding audio segments in real and fake videos, as shown in fig. 4. Note only the mouth region sequences are fed into the model to get the transcriptions. As can be seen, the sentence transcribed from real lip movement frames is close to the sentence transcribed from audio. In contrast, lip movements in fake videos frequently result in

Figure 4: **Interpretative analysis. The transcriptions are based on audio and visual speech representations of real and different types of fake videos. We show the transcriptions of each type of video containing the same audio.**

Figure 3: **Visualized analysis. Cosine similarity distributions of audio and visual speech representations for real videos and fake videos generated by different manipulation methods.**

nonsensical or chaotic transcriptions. It suggests that these audio and visual speech representations indeed contain semantic information, which can be used for forgery detection.

### Ablation study

Influence of Video Clip Length.Given that our model accommodates video clips of varying lengths, we investigated how this aspect impacts the performance of our method on the FF++ dataset. For this purpose, we selected FaceSwap and Face2Face as two emblematic types of forgeries and segmented videos into various durations, ranging from 1 second to 20 seconds, while maintaining consistency in all other hyperparameters. Results in fig. 5 indicate a clear trend: the performance of our method is continuously enhanced with the extension of the video length to 16 seconds, suggesting long-range temporal inconsistencies exist in forgery videos. While previous detectors, e.g.,FTCN  and RealForensics , can only utilize short-range temporal features, resulting in their suboptimal performance.

Different time offset assumptions.We also study the effect of different time offset assumptions between audio and visual streams, i.e., fixed and dynamic. As shown in table 4, the dynamic based method achieves slightly better performance based on the fixed time offset assumption. A reasonable explanation is that, compared to real videos, the time offsets of fake videos will be more inconsistent due to the uncertainty of the forgery process. And the DTW algorithm makes fake videos have higher matching scores, which is unfavorable to the forgery detection. Furthermore, we investigate the influence of different maximum offset \(\), corresponding sliding-window length \(2+1\). As fig. 5 shows, increasing window length brings improved performance, reaching a maximum of around 31.

Different Models and Datasets.We further evaluate the effect of different models and training datasets. For AVHuBERT , we use models with two configurations: BASE, which comprises 12 transformer blocks, and LARGE, which includes 24 transformer blocks. Each configuration was trained on two datasets: LRS3 alone and a combination of LRS3 and VoxCeleb2, respectively. Furthermore, we evaluate another model, VATLM , which also fits our framework but incorporates text modality. The experimental results on FF++ and FakeAVCeleb are shown in table 4. Results show larger models and more training data both boost the performance of our approach. While compared with model size, the influence introduced by datasets is more pronounced. And both AVHuBERT and VATLM obtain remarkable results.

## 5 Conclusion and Discussion

In this paper, we have developed a method for forgery detection that identifies discrepancies between audio and visual speech representations. Demonstrating exceptional generalization capabilities to unseen manipulations and robustness against prevalent perturbations, our approach sets a new benchmark, notably without relying on fake videos for training. It also eliminates the need for finetuning and downstream tasks, significantly streamlining the detection workflow. Moreover, since our method is based on speech representation learning, it can be implemented in a training-free manner and may achieve better performance as the latter advances. We are optimistic that our contributions will inspire further advancements in the field of forgery detection research.

Limitations.While our method exhibits robust performance across diverse evaluations, it is not without its limitations. Primarily, it is constrained by its reliance on visual speech representations derived from lip movements, rendering it unsuitable for detecting forgeries that do not alter mouth regions. However, we note that facial forgeries typically involve the mouth area. In addition, it may

   Model & Offset & Backbone & Dataset & FF++ & FakeWCoeleb \\   &  & BASE & LRS3 & 95.3 & 97.0 \\  & & BASE & LRS3+Voxel & 96.1 & 97.9 \\  & & LARGE & LRS3+Voxel & 97.7 & 96.8 \\  & & & LARG & LRS3+Voxel & **97.6** & 99.0 \\   &  & LARGE & LRS3+Voxel & 93.7 & 98.7 \\   & & & & & \\  VATLM  & Fixed & LARGE & LRS3+Voxel & 97.1 & **99.3** \\   

Table 4: **Effect of different models and time offset assumptions.** We report the performance of models with different architectures and training datasets on FF++ and FakeAVCeleb.

Figure 5: **Influence of video length and sliding-window length.** We evaluate the performance of our method conditioned on different input lengths and sliding-window lengths.

suffer a certain level of performance degradation when encountering extreme testing samples, e.g., videos with numerous silent clips or audio signals containing significant amount of ambient noise, e.g., background music. These considerations highlight areas for potential improvement and future exploration in enhancing the versatility and applicability of forgery detection techniques.

Broader Impacts.Our work is aimed at fighting against face forgery technologies. And we hope it could encourage more future detection works. However, since forgery and detection are two game-playing technologies, the emergence of new detection methods may lead to the evolution of forgery methods. And we suggest that detection systems integrate different detection methods to combat potential new face forgery methods.

## 6 Acknowledgement

This work has been supported by the National Key R&D Program of China (Grant No. 2021YFF0602104).