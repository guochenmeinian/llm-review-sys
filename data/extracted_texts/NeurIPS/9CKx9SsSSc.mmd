# ADGym: Design Choices for Deep Anomaly Detection

Minqi Jiang\({}^{1,}\)

Chiochuan Hou\({}^{1,}\)

Corresponding authors.

Ao Zheng\({}^{1,}\)

Songqiao Han\({}^{1,}\)

Hailiang Huang\({}^{1,2,}\)

Qingsong Wen\({}^{3}\)

Xiyang Hu\({}^{4,}\)

Yue Zhao\({}^{4,}\)

\({}^{1}\)AI Lab, Shanghai University of Finance and Economics

\({}^{2}\)MoE Key Laboratory of Interdisciplinary Research of Computation and Economics

\({}^{3}\)DAMO Academy, Alibaba Group \({}^{4}\)Carnegie Mellon University

jiangmq95@163.com, houachouchuan@foxmail.com, zheng-ao@outlook.com,

{han.songqiao,hlhuang}@shufe.edu.cn, qingsongedu@gmail.com,

{xiyanghu,zhaoy}@cmu.edu

Contribute equally. \({}^{}\)Corresponding authors.

###### Abstract

Deep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions: _(i)_ Which design choices in deep AD methods are crucial for detecting anomalies? _(ii)_ How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre-existing solutions? To address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.

## 1 Introduction

Anomaly detection (AD) aims to identify data objects that significantly deviate from the majority of samples, with numerous successful applications in intrusion detection , fault detection , medical diagnosis , fraud detection , social media analysis , etc. Recently, deep neural networks have become the primary techniques in AD due to their powerful representation learning capacity . Among all, weakly-supervised AD (WSAD) methods , which leverage imperfect ground truth anomaly labels (e.g., those that are incomplete, inaccurate, or inexact) in deep neural networks for AD, have gained attention in this new frontier . A comprehensive study by  showcases WSAD's superiority over unsupervised AD techniques, especially for real-world conditions where the ground truth labels are never complete and accurate. Thus, our work introduces ADGym, a platform for understanding and designing deep WSAD methods2, with the potential to be extended to unsupervised and supervised deep AD methods.

**Goal I: Understanding Design Choices of Deep AD.** Many WSAD methods attribute their improvements to novel network architectures or loss functions, based on the authors' understanding of anomalies. However, these choices represent only a fraction of the design considerations, as there are many other factors to consider, such as data preprocessing and model training techniques. In Table1, we list specific design choices for deep AD models and group them as design dimensions. Our experiments reveal that some of these dimensions (in bold) significantly impact the performance.

Many questions remain unanswered in current AD studies, such as the interaction between components within an AD method, the relative importance of each component in performance, and the potential of new data-centric techniques to improve AD model performance. Thus, in the first part of the study (SS3.2), we have evaluated various design combinations on large benchmark datasets. Interestingly, the optimal model, comprising different design choices by combination, varies by datasets and notably outperforms existing state-of-the-art (SOTA) AD models. This raises a key question: how can we automatically design AD models for datasets other than using existing models?

**Goal II: Constructing AD Algorithms Automatically via ADGym.** Indeed, our prior research [88; 53; 79] also shows there is no one-size-fits-all AD model for every dataset--we must select AD models based on the underlying dataset. Our prior work focuses on model selection from a pre-defined list, mainly for non-neural-network AD methods with limited design choices [88; 89]. In today's deep learning era, a pre-defined list is not sufficient, given the large number of design choices in Table 1 and SS3.2). There could be infinite deep AD models with different design combinations.

In the second part of this study, we develop a _design-choice-level_ selection approach that utilizes _meta-learning_, called ADGym (see SS3.3). In a nutshell, our approach leverages the supervision signals from historical datasets to guide selecting the best design choices for constructing AD models tailored to a new dataset. We aim to enhance the existing AD model selection process, ensuring the best combination of design choices for a given application or dataset. What sets ADGym apart from prior works is our focus on granular design choice selection tailored for deep AD, which has a much larger space than a pre-defined model list for existing methods only. See Fig. 1 for a comparison.

**What Do We Learn from the Experiments (SS4)?** ADGym helps us better understand and design deep AD methods, with key observations from our experiments: (1) No single design choice consistently outperforms others across all datasets, justifying the need for an automated approach to collectively select the most effective design choices. (2) Employing a _meta_-predictor to automate design choice selection yields notable improvements over static SOTA methods. (3) We can make meta-predictors better by using ensemble techniques or increasing the range of design choices.

**To sum up, our work makes the following technical contributions**:

1. **Understanding AD Design Choices via Benchmarking.** We present the first benchmark that breaks down and compares diverse deep AD design choices across 29 real-world datasets and 4 groups of synthetic datasets1, which leads to a few interesting observations. 2. **Design Choice Automation.** We introduce ADGym, the first automated framework for selecting design choices for weakly-supervised AD, which significantly outperforms SOTA AD methods.
3. **Accessibility and Extensibility.** We have made ADGym publicly available2 so that practitioners can build better-than-SOTA AD methods. It is also easy to include new design choices.

  
**Pipeline** & **Design Dimensions** & **Design Choices** \\   & **Data Augmentation** & [Oversampling, SMOTE, Mixup, GAN] \\  & Data Preprocessing & [MinMax, Normalization] \\   & **Network Architecture** & [MLP, AutoEncoder, ResNet, FTTransformer] \\  & Hidden Layers & [, , ] \\  & **Activation** & [Tanh, ReLU, LeakyReLU] \\  & Dropout & [0.0, 0.1, 0.3] \\  & Initialization & [default, Xavier (normal), Kaiming (normal)] \\   & **Loss Function** & [BCE, Focal, Minus, Inverse, Hinge, Deviation, Ordinal] \\  & **Optimizer** & [SGD, Adam, RMSprop] \\   & Epochs &  \\   & Batch Size &  \\   & **Learning Rate** & [1e-2, 1e-3] \\   & Weight Decay & [1e-2, 1e-4] \\   

* _Note: Bolded design dimensions are those of greater impact on the AD task (discussed in detail in ยง4)._

Table 1: ADGym supports a comprehensive list of design choices for deep AD methods.

Related Work

### Weakly-supervised Anomaly Detection (WSAD)

Due to the cost and difficulties in data annotation, previous studies [42; 43; 47; 68; 91] mainly focus on developing unsupervised AD methods with different assumptions of data distribution , while they are shown to have no statistically significant difference from each other in a recent benchmark . In practice, however, there could exist at least a handful of labeled instances that are either identified by domain experts or the bad cases occurred during the deployment of AD methods. Therefore, recent studies [56; 58; 59; 69; 90] propose weakly-supervised AD methods to effectively leverage the valuable knowledge in labeled data and thus facilitate anomaly identification. Nevertheless, existing WSAD methods are often used as _it is_, without detailed evaluation of each design choice like network architectures and loss functions. A more granular analysis of specific components of WSAD methods could be helpful for a deeper understanding of the AD methods.

### Benchmarks for Anomaly Detection

Anomaly detection benchmarks mainly perform large comparisons and evaluations on the detection performance of different AD methods under unified and controlled conditions. While previous studies [12; 17; 23; 71; 72] focus on benchmarking classical machine learning AD methods, there has been an increasing trend towards benchmarking deep learning AD methods as well.  reviews both classic and deep AD models and points to the connections between classic shallow algorithms and deep AD models. Our previous work  performs the largest-scale AD benchmark so far, evaluating 30 shallow and deep AD algorithms across 57 benchmark datasets under different levels of supervision. Besides, some benchmark works focus on AD tasks with different data modalities, including time-series [46; 60; 35], graph [49; 48], CV  and NLP . All of these benchmarks focus on discovering which AD algorithms (as a whole) are more effective. Differently, we focus on understanding the effectiveness of design choices in deep AD methods, complementing these works.

### Automatic Model Selection for AD

#### 2.3.1 Unsupervised Anomaly Model Selection

Developing automatic model selection schemes for unsupervised AD algorithms faces a major challenge in lacking evaluation criteria. A recent survey  provides a comprehensive survey of using internal model evaluation strategies that solely rely on input features and outlier scores for model selection. Their extensive experiment shows that _none of the existing and adapted evaluation strategies would be practically useful_ for unsupervised AD model selection.

Another existing work stream leverages meta-learning, based on the similarity between the new task and historical datasets where model performance is available. Some notable work [88; 89; 87] assume that model performance can be generalized across similar datasets, thus selecting the model for a new task based on similar historical tasks. Specifically, [89; 87] introduce the idea of building a _meta-predictor_ (which is a regressor) to predict the model performance on a new dataset by training it on historical model performances. In ADGym, we take the idea of meta-learning to train a specialized meta-predictor for deep AD methods, where existing works [88; 89] only focus on non-neural-network-based AD models with a relatively small model pool, where we extend to more complex deep AD design spaces. See SS3.3 for details.

#### 2.3.2 Supervised Anomaly Model Selection, HP Optimization, and Neural Arch. Search

Supervised AD model selection and HP optimization (HPO) aim to train a set of models, evaluate their performance using ground truth labels, and ultimately select the best model. However, as previously mentioned, ground truth labels in AD are scarce, which prevents supervised selection from being widely explored. PyODDS  optimizes an AD pipeline, while TODS  creates a selection system for time series data. Both, however, focus primarily on shallow models, excluding most deep models. Recently, AutoOD  conducts neural architecture search (NAS) for deep AD using labels. However, it only focuses on AutoEncoder models over image data. AutoPatch and  are another two NAS works for AD, but they still do not consider networks beyond CNNs.

Here, we highlight the difference between ADGym and the above works. First, ADGym aims to select from a large, comprehensive design pool, as opposed to existing model selection, which is often restricted to a _small_ pre-defined model pool; meanwhile, ADGym also brings more granularity than general HPO and NAS, which only focus on a small set of HPs (given neural architectures can also be considered as HPs). Second, ADGym supports more scenarios other than focusing on a single family of AD methods, e.g., autoencoders, or just network architectures. Third, different from the SOTA method AutoOD, ADGym is a zero-shot algorithm that does not require any model building for a new dataset, thereby significantly reducing the online time; see SS3.3 for details.

## 3 ADGym: _Benchmarking_ and _Automating_ Design Choices in Deep AD

### Problem Definition

In this paper, we focus on the common weakly supervised AD scenario, where given a training dataset \(=\{_{1}^{u},,_{k}^{u},(_{k+1}^{a}, y_{k+1}^{a}),,(_{k+m}^{a},y_{k+m}^{a})\}\) contains both unlabeled samples \(_{u}=\{_{i}^{u}\}_{i=1}^{k}\) and a handful of labeled anomalies \(_{a}=\{(_{j}^{a},y_{j}^{a})\}_{j=1}^{m}\), where \(^{d}\) represents the input feature and \(y_{j}^{a}\) is the label of identified anomalies. Usually, we have \(m k\), since only limited prior knowledge of anomalies is available. Such data assumption is more practical for AD problems, and has been studied in recent deep AD methods [56; 58; 59; 69; 90]. The goal of a weakly-supervised AD model \(M\) is to assign higher anomaly scores to anomalies.

### Goal I: Understanding Design Choices of Deep AD

The first primary goal of this study is to investigate the large design space of deep AD solutions, which should cover as many design choices as possible, e.g., different data augmentation methods, network architectures, etc. Following the taxonomy of the previous study  and practical experiences in industrial applications [2; 78; 22], we decouple the standard process of deep AD methods, starting from the input data and ending with model training. The **Pipeline** includes: _data handling \(\) network construction \(\) network training_, as shown in Table 1. For each step of the pipeline, we further categorize it as different **Design Dimensions**, where diverse **Design Choices** can be specified to instantiate an AD method. More details are shown in Appx. C.1

With the comprehensive design space above, we not only evaluate the possible designs of weakly-supervised AD methods, but also investigate several interesting design dimensions often overlooked in previous AD research. For example, previous AD studies often perform model training on the raw input data (instead of augmented data for mitigating class imbalance problems), and simple network architectures like multi-layer perceptron (MLP) (instead of more recent network architectures like ResNet  and Transformer ), where the other cutting-edge techniques have already been widely used in other domains like NLP and CV. To sum up, we pair the combination of comprehensive design choices in Table 1 with benchmark AD datasets to unlock new insights. See results in SS4.2.

### Goal II: Constructing AD Algorithms Automatically via ADGym

**From Model Selection to Pipeline Selection**. With the large AD design space illustrated in the last section, we investigate how to construct effective AD methods given the downstream applications

Figure 1: The framework of ADGym. Existing AD model selection focuses on selecting the best model from a small, fixed pool. Our proposed method is more flexible to build a good model by choosing different parts, including data handling, network construction, and network training.

automatically. Given a pre-defined model set \(=\{M_{1},...,M_{m}\}\) that includes \(m\) combinations of applicable design choices, model selection picks a model \(M\) to train on a dataset \(_{test}\) and output the anomaly score \(O:=M(_{})\) to achieve the highest detection performance. In our case, we construct a _pipeline_\(P\) from our design space to generate each model \(M\). As shown in Fig. 1, our design (right) brings more flexibility by choosing from details than existing model selection works that only choose from a fixed, small set of AD models.

**Meta-learning for Pipeline Selection**. Meta learning has been recently used in unsupervised AD model selection [88; 89], where the core idea is to transfer knowledge from model performance information on historical datasets to the selection on a new dataset. Intuitively, if two datasets are similar, their best models should also resemble. Under the meta-learning framework, we assume there are \(n\) historical AD datasets (i.e., detection tasks) \(}_{}=\{_{1},,_{n}\}\); each meta-train dataset is accompanied with ground truth labels for performance evaluation. To leverage prior knowledge for pipeline selection on a new dataset, we first conduct experiments on \(n\) historical datasets to evaluate and collect the performance of \(m\) possible _designed pipelines_ (as the combination of all applicable design choices), by two widely used metrics: AUCROC (Area Under Receiver Operating Characteristic Curve) and AUCPR (Area Under Precision-Recall Curve).

For each metric, we acquire the corresponding performance matrix \(^{n m}\), where \(_{i,j}\) corresponds to the \(j\)-th constructed AD model's performance on the \(i\)-th historical dataset. Meanwhile, historical datasets vary in task difficulty, resulting in variations in the numerical range of the constructed AD models' performance. Therefore, we convert the value of the performance into their relative/normalized ranking, where \(_{i,j}=rank(P_{i,j})/m\). Smaller ranking values indicate better performance on the corresponding dataset.

To predict the performance of a given pipeline on a new dataset, we propose to train a _meta-predictor_ as a _regression_ problem: the input of meta-predictor is \(\{_{i}^{meta},_{j}^{comp}\}\), corresponding to the meta-feature  (i.e., the unified representations of a dataset) of \(i\)-th dataset and the embedding of \(j\)-th AD component (i.e., the representation of a pipeline/components). We defer the specific details into Appx. C.2. Given the meta-predictor \(f()\), we train it to map dataset and pipeline characteristics to their corresponding performance ranking across all historical datasets, as shown in Eq. (1). Refer to our open-sourced code for details of embeddings and the choice of regression models.

\[f\ :\ _{i}^{meta}}_{},\ \ \ \ \ \ \ _{j}^{comp}\ \ _{i,j}\ \,\ \ i\{1,,n\},\ j\{1,,m\}\] (1)

For a newcoming dataset (i.e., test dataset \(_{}\)), we acquire the predicted relative ranking of different AD components using the trained \(f()\), and select top-\(1\) (\(k\)) to construct AD model(s). Note this procedure is zero-shot without needing any neural network training on \(_{}\) but only extracting meta-features and pipeline embeddings. We show the effectiveness of the meta-predictor in SS4.3.

## 4 Experiments

### Experiment Settings

**AD Datasets and Baselines**. In ADGym, we gather datasets from the previous AD studies and benchmarks [12; 19; 57; 64; 64] for evaluating existing AD models and different AD designs. Datasets with sample sizes smaller than 1000, as well as those with problematic model results, are removed, resulting in a total of 29 remaining datasets. These datasets cover many diverse application domains, such as healthcare, audio and language processing, image processing, finance, etc. 1 Furthermore, based on our previous work, we categorize anomalies into four types, local, global, cluster and dependency anomalies, and generate synthetic datasets for each individual anomaly type. For each dataset, \(70\%\) data is split as the training set and the remaining \(30\%\) is used as the test set. We use stratified sampling to keep the anomaly ratio consistent. For each experiment, we repeat 3 times and report the average. Further details of datasets and baselines are presented in Appx. A and B.

**Meta-predictor in ADGym**. We introduce two types of meta-predictors in ADGym, namely DL-based and ML-based. The DL-based meta-predictor is instantiated as a two-layer MLP and trained for 100 epochs with early stopping. The training process utilizes the Adam optimizer  with a learning rate of 0.001 and batch size of 512. For the ML-based meta-predictor, We instantiate the meta-predictor with XGBoost and CatBoost and their default hyperparameter settings. Considering the expensive computational cost of training meta-predictors on the entire design space, we randomly sample 1,000 design choices for each experiment illustrated below. See details in Appx. C.2.

**Evaluation Metrics**. We perform evaluations with two widely used metrics: AUCROC (Area Under Receiver Operating Characteristic Curve), AUCR (Area Under Precision-Recall Curve) value, and the relative rankings corresponding to these two metrics1.

### Large Evaluation on AD Design Choices

In this work, we perform large evaluations on the decoupled pipelines according to the standard procedure of AD methods. Such analysis is often overlooked in previous AD studies, and we investigate each design dimension of decoupled pipelines by fixing its corresponding design choice (e.g., Focal loss), and randomly sampling other dimensional design choices to construct AD models, e.g., \(\{DateAugmentation:Mixup,NetworkArchitecture:FTT,LossFunction: Focal,...\}\),..., \(\{DateAugmentation:GAN,NetworkArchitecture:MLP,LossFunction: Focal,...\}\). In other words, we investigate each design dimension by controlling other variables. Different design choices are compared w.r.t. \(n_{a}=5\), \(10\), \(20\) and demonstrated with box plots, where the number of comparisons in each box is ensured to be the same. In the following subsections, we analyze the benchmark results on each of the design dimensions, namely data handling, network construction, and network training.

#### 4.2.1 Data Handling

For the design dimension of data augmentation methods (e.g., SMOTE2 and Mixup methods) shown in Figure 2, we find that _almost no method_ has brought significant performance improvements. This could be explained by the fact that, unlike time series , NLP , or CV tasks , data augmentation is rarely incorporated into the design of existing AD methods tailored for tabular data . Besides, our results indicate that GAN-based augmentation method is even worse than simpler methods like oversampling, probably due to the difficulty of modeling and generating realistic anomalies for the tabular data. The same trend is observed in the synthetic datasets with individual anomalies. In the vast majority of cases, the results from data augmentation are inferior to those from the original data. (See Appx.D.2.) For data preprocessing methods, We do not observe a significant difference between the minmax scaling and normalizing methods.

#### 4.2.2 Network Construction

We investigate various design dimensions of network construction, as is shown in Figure 3. For network architectures, MLP is _still_ a competitive baseline in AD tasks, and even outperforms other more complex architecture designs like ResNet and FTTransformer w.r.t. \(n_{a}=5\), i.e., only a handful of labeled anomalies are available in the training stage. Moreover, we suggest that the ResNet model could also serve as both an effective and stable network architecture, especially when more labeled anomalies can be acquired (e.g., \(n_{a}=10\) and \(n_{a}=20\)). However, we do not find significant advantages of the FTTransformer, where its AUCROC performance is generally worse than other network architectures. This may be due to the reason that compared to the supervised tasks for tabular

Figure 2: AUCROC performance of data handling designs. There is no significant difference between different design choices in both data augmentation and preprocessing dimensions.

data, very limited labeled data (corresponding to the noises in the unlabeled data) can be leveraged to guide the training process of such a complicated model. Furthermore, compared to our earlier work on ADBench where the FTTransformer shows competitive performance, it can be reasonably inferred that the FTTransformer has limited robustness to hyperparameter tuning, especially after we dissected its parameters. We also identify the advantage of the AutoEncoder architecture on synthetic datasets with individual anomaly types, particularly in the local and global anomaly datasets (see Appx. D). A plausible explanation is that the normal data in these synthesized datasets follows a specific distribution that can be more effectively reconstructed.

For the activation function, Tanh and LeakyReLU appear to be more effective than the ReLU function in tabular AD tasks, where this conclusion holds true for different \(n_{a}\). Besides, we do not observe significant differences in the design dimensions of hidden layers, dropout, and network initialization.

#### 4.2.3 Network Training

In this subsection, we evaluate different design dimensions in the network training step, as shown in Fig. 4. Based on the results, we surprisingly observe that the classical BCE loss could be a competitive baseline when batch resampling method1 is applied in the training process. Moreover, our results show the advantages of hinge loss [56; 59], which achieves better AUCROC performance w.r.t. different \(n_{a}\), and deviation loss  in all synthetic datasets with a significant similarity across the anomalies. (details in Appx. D). For model optimization, we observe that Adam and RMSprop optimizers are better than the classical SGD, where large training epochs (e.g., epochs=100) lead to overfitting on limited labeled data, resulting in a relatively inferior performance. For the design dimensions of batch size, learning rate, and weight decay, they do not show obvious differences in terms of the AUCROC metric.

### Automatic Component Construction via ADGym

In this subsection, we verify the effectiveness of the proposed meta-predictors by answering the questions illustrated below. Beyond the fixed SOTA AD methods, we provide three model construction plans for a thorough baseline comparison. The random selection plan (RS) randomly generates a pipeline from all design choices. The supervised selection plan (SS) selects the pipeline according

Figure 3: AUCROC performance of network construction designs. MLP is still an effective architecture for AD tasks, where Tanh and LeakyReLU activation functions are generally better than ReLU.

[MISSING_PAGE_FAIL:8]

Moreover, the average AUCROC performance of meta-predictors is 9.4%, 9.4%, and 8.5% higher than that of SOTA models w.r.t. \(n_{a}=\) 5, 10, and 20, respectively.

Moreover, the clear advantage of GT to the current SOTA methods indicates that existing AD solutions could be further improved through exploring the design space or automatic selection techniques, which also validates the value of ADGym. Besides, compared to random selection (RS) and supervised selection (SS) which select design choices based on the performance of limited labeled data, automatic selection via meta-predictor(s) is significantly more effective. However, we find that the SS method is even inferior to the RS method. A reasonable explanation is that the scarcity of labeled data makes it difficult for the selected design choices to generalize well on newcoming data.

**2. Is it more helpful for meta-predictors to transfer knowledge across different datasets by end-to-end trained meta-features or extraction-based meta-features?**

Here, we explore the impact of different methods for extracting meta-features on meta-predictor performance, as shown in Table 3. This includes: _(i)_ Two-stage method, where the meta-features of a specific dataset are first extracted by the method proposed in MetaOD , i.e., extracting both statistical features like min, max, variance, skewness, covariance, etc., and landmarker features that depend on the unsupervised AD algorithms to capture outlying characteristics of a dataset. The extracted meta-features \(E^{meta}\) are then concatenated with \(n_{a}\) and \(E^{comp}\) as inputs to the meta-predictor. _(ii)_ End-to-end method, where the meta-features are directly extracted by the meta-predictor  and optimized with the learning process of the downstream task.

Generally, we observe _close_ performances between the two-stage and end-to-end methods of extracting meta-features in the meta-predictors. This conclusion holds valid not only for the default MSE loss, but also for the Ranknet loss  used in the meta-predictors.

**3. Is the tree-based ensemble meta-predictor effective in tabular AD? Can meta-predictors further benefit from model ensembling?**

Consistent with the findings in the previous studies , we _still_ find that the tree-based ensemble model(s) are better solutions for tabular-based AD tasks, where the performance of meta-predictors are improved when the MLP trainer used in meta-predictor is replaced by the ensemble models like XGBoost and CatBoost. Moreover, DL-based meta-predictor can also benefit from the model ensembling strategy, as we observe that both two-stage and end-to-end meta-predictors improve when we ensemble the anomaly scores of predicted top-\(k\) combinations of design choices.

**4. Do advanced loss functions bring performance gains to the meta-predictor?**

In addition to the default MSE loss function, we also investigate several other losses (as shown in Table 4), including _(i)_ Weighted MSE imposes a stronger penalty on errors in predicting the top and bottom design choices. _(ii)_ Pearson correlation between the predictions and ground-truth targets. _(iii)_ Ranknet  learns to rank different design choices, which is widely used in recommendation systems. However, compared to the results of MSE loss shown in Table 3, we _do not_ find significant improvement when more complicated loss functions are implemented to train the meta-predictors.

## 5 Does larger AD design space bring performance gains to the meta-predictors?

In order to explore whether the meta-predictors can uncover more promising AD design choices beyond those bolded design dimensions highlighted in Table 1, we include more possible design dimensions like hidden layers, dropout, and initialization methods in network construction, and epochs, batch size, and weight decay in network training, while maintaining the scale of design space (i.e., the number of Cartesian products of design choices) at 1,000. Table 5 shows that the meta-predictors generally benefit from a larger design space, where the AUCROC performances of both DL- and ML-based meta-predictors achieve improvements compared to that of small design space

   \)} &  &  &  \\   & DL-single & DL-ensemble &  &  &  &  &  \\   & 2-stage & end2end & 2-stage & end2end & 2-stage & end2end & 2-stage & end2end & 2-stage & end2end & 2-stage & end2end \\  \(5\) & 0.801 & 0.748 & 0.811 & 0.774 & 0.778 & 0.808 & 0.804 & 0.815 & 0.800 & 0.809 & **0.821** & 0.813 \\ \(10\) & 0.830 & 0.770 & 0.842 & 0.786 & 0.825 & 0.833 & 0.834 & 0.843 & 0.835 & 0.836 & 0.841 & **0.844** \\ \(20\) & 0.844 & 0.786 & **0.862** & 0.801 & 0.839 & 0.840 & 0.852 & 0.854 & 0.853 & 0.853 & **0.862** & 0.859 \\   

Table 4: AUCROC performance of meta-predictors trained on other loss functions. Different loss functions do not yield significant performance improvement for the meta-predictor.

shown in Table 3. It's worth mentioning that DL meta-predictors seem to surpass ML meta-predictors in a larger space. This may be due to a raised complexity and lower probability of over-fitting in a larger selection pool, both benefiting DL meta-predictors. However, this performance gap is still trivial (0.002-0.006 in absolute value). Considering a better efficiency, we still present the ML meta-predictors as the formal recommendation.

## 6 Does refining AD design space bring performance gains to the meta-predictor?

We further refined the design space in ADGym and excluded the design choices whose average performances on the training datasets are below the median, resulting in better yet fewer design choices that can be utilized for training meta-predictors. The refined results are reported in Table 6, compared to Table 3, We have _not_ found that refining the design space brings any gains to the meta-predictor, possibly because such approach loses many potential design choices that could perform well on newcoming dataset, or loses (half of) the training data of meta-predictors.

## 5 Conclusions, Limitations, and Future Directions

In this paper, we introduce ADGym, a novel platform designed for benchmarking and automating AD design choices. We break down AD algorithms into granular components and systematically assess each one's effectiveness through extensive experiments. Furthermore, we develop an automated method for selecting optimal design choices, enabling the creation of AD models that surpass current SOTA algorithms. Our results highlight the crucial role design choices play and offer a structured approach to optimizing them in model development. With the broader AD community in mind, we have made ADGym openly available. We believe its comprehensive design choice evaluation capabilities will significantly contribute to future advancements in automated AD model generation.

Looking ahead, we see several opportunities to enhance ADGym and broaden its application. Firstly, we plan to incorporate time-series AD tasks that handle data with temporal variations. By automating the design choice selection, AD models will be better equipped to adapt to these changing distributions, thereby improving anomaly detection in dynamic time-series contexts. Currently, ADGym is geared towards weakly supervised AD. In the future, we aim to include unsupervised neural network-based AD algorithms as well. Additionally, it is worth noting that non-neural-network-based techniques, such as ensemble-tree methods, have shown great promise in practical scenarios. Therefore, exploring automatic pipeline formulation in these areas is an exciting and valuable direction for future research.

## 6 Acknowledgement

We thank anonymous reviewers for their insightful feedback and comments. M.J., C.H., A.Z., S.H., and H.H. are supported by the National Natural Science Foundation of China under Grant No. 72271151, and the National Key Research and Development Program of China under Grant No. 2022YFC3303301. M.J., C.H., A.Z., S.H., and H.H. thank the financial support provided by FlagInfo-SHUFE Joint Laboratory. X.H. and Y.Z. are independently supported by CMU.

   \)} &  &  &  &  &  &  &  \\   & & & & 2-stage & end2end & 2-stage & end2end & XGB & Catb & XGB & Catb \\  \(5\) & 0.766 & 0.653 & 0.902 & 0.797 & 0.809 & 0.812 & 0.817 & 0.798 & 0.807 & 0.816 & **0.819** \\ \(10\) & 0.803 & 0.700 & 0.912 & 0.824 & 0.833 & 0.838 & 0.835 & 0.838 & 0.830 & **0.848** & 0.842 \\ \(20\) & 0.833 & 0.754 & 0.921 & 0.851 & 0.830 & 0.867 & 0.848 & 0.874 & 0.867 & **0.877** & 0.872 \\   

Table 6: AUCROC performance of meta-predictors trained on refined design space. No significant improvement of meta-predictor is observed since not only potentially better design choices are discarded, but also results in a reduction of training data in meta-predictors.

   \)} &  &  &  &  &  &  &  \\   & & & & 2-stage & end2end & 2-stage & end2end & XGB & Catb & XGB & Catb \\  \(5\) & 0.738 & 0.657 & 0.904 & 0.824 & 0.808 & **0.829** & 0.826 & 0.814 & 0.814 & 0.825 & 0.825 \\ \(10\) & 0.767 & 0.731 & 0.912 & 0.842 & 0.830 & **0.853** & 0.850 & 0.843 & 0.846 & 0.850 & 0.851 \\ \(20\) & 0.791 & 0.750 & 0.922 & 0.863 & 0.846 & **0.876** & 0.859 & 0.860 & 0.863 & 0.870 & 0.874 \\   

Table 5: AUCROC performance of meta-predictors trained on large scale design space. Larger design space provides potentially more effective design choices for newcoming datasets.