# Black-Box Forgetting

Yusuke Kuwana

Tokyo University of Science

4624513@ed.tus.ac.jp &Yuta Goto

Tokyo University of Science

4623511@ed.tus.ac.jp &Takashi Shibata

NEC Corporation

t.shibata@ieee.org &Go Irie

Tokyo University of Science

goirie@ieee.org

###### Abstract

Large-scale pre-trained models (PTMs) provide remarkable zero-shot classification capability covering a wide variety of object classes. However, practical applications do not always require the classification of all kinds of objects, and leaving the model capable of recognizing unnecessary classes not only degrades overall accuracy but also leads to operational disadvantages. To mitigate this issue, we explore the selective forgetting problem for PTMs, where the task is to make the model unable to recognize only the specified classes while maintaining accuracy for the rest. All the existing methods assume "white-box" settings, where model information such as architectures, parameters, and gradients is available for training. However, PTMs are often "black-box," where information on such models is unavailable for commercial reasons or social responsibilities. In this paper, we address a novel problem of selective forgetting for black-box models, named Black-Box Forgetting, and propose an approach to the problem. Given that information on the model is unavailable, we optimize the input prompt to decrease the accuracy of specified classes through derivative-free optimization. To avoid difficult high-dimensional optimization while ensuring high forgetting performance, we propose Latent Context Sharing, which introduces common low-dimensional latent components among multiple tokens for the prompt. Experiments on four standard benchmark datasets demonstrate the superiority of our method with reasonable baselines. The code is available at https://github.com/yusukekwn/Black-Box-Forgetting.

## 1 Introduction

Large-scale pre-trained models (PTMs) such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have strong capabilities of zero-shot classification for everyday objects. Nevertheless, in practical applications, the classification of all kinds of object classes is rarely required. For example, in an autonomous driving system, it would be sufficient to recognize limited classes of objects such as cars, pedestrians, and traffic signs. We would not need to recognize food, furniture, or animal species. Retaining the classes that do not need to be recognized may decrease overall classification accuracy, as well as cause operational disadvantages such as the waste of computational resources and the risk of information leakage. In this paper, we address the problem of selective forgetting of specified classes (Shibata et al., 2021; Graves et al., 2021; Ye et al., 2022; Tarun et al., 2023), i.e., tuning a pre-trained model to reduce the classification accuracy for only the specified classes without affecting the accuracy for the others1. While selective forgetting of specified classes has long been overlooked (Ye et al., 2022), a few existing methods have been proposed very recently (Shibata et al., 2021; Ye et al., 2022; Tarun et al., 2023). The seminal work is Learning with Selective Forgetting (LSF) (Shibata et al., 2021), which has been proposed in the context of continual learning (Kirkpatrick et al., 2017; Li and Hoiem, 2017; Aljundi et al., 2018) and uses a special random code called mnemonic code to control the class-wise memorization and forgetting. A similar idea has been proposed to achieve forgetting by learning noise that maximizes the classification error for the classes to be forgotten (Tarun et al., 2023). An extended version of LSF (Ye et al., 2022) allows forgetting of specified classes as well as recovery of them by temporarily transferring the knowledge of the classes to be forgotten to another network called deposit module.

Overall, all the existing methods assume the "white-box" setting, where the complete information of the target model is available for training/tuning, including the model architecture, its parameters, and their gradients. However, major PTMs such as GPT-4V (OpenAI, 2023) are often "black-box," where the model itself or its information is often fully or partially private due to commercial reasons or considerations of social impact. Since the parameters and their gradients are not accessible in such a model, all the existing methods are inapplicable. To the best of our knowledge, selective forgetting methods for black-box models have never been studied to date.

In this paper, we address Black-Box Forgetting, i.e., the selective forgetting problem for black-box PTMs, and propose a novel approach to the problem. Given the unavailability of model information, our method, unlike the existing selective forgetting methods, does not optimize network parameters nor utilize the gradients of the parameters; we instead optimize the input textual prompt to decrease the classification accuracy of specified classes to be forgotten in a derivative-free optimization framework. One disadvantage of derivative-free optimization would be that it is not effective nor efficient for high-dimensional problems due to the low convergence rate in high-dimensional spaces (Qian et al., 2016), and unfortunately, the textual prompt is typically parameterized as a set

Figure 1: **Overview of our black-box forgetting framework. The confidence of each class is computed as the similarity with the image and class (text) embeddings from the black-box pre-trained vision-language model (e.g., CLIP). The obtained confidence is used to compute the respective loss functions for the classes to be forgotten and the classes to be memorized. (a) For the classes to be forgotten, maximize the entropy of the confidence so that the accuracy is reduced. (b) For the classes to be memorized, minimize the cross-entropy loss to retain the accuracy. These two objective are jointly optimized to tune the learnable text prompt. The gradients of the objective are not available when the model is black-box. We therefore use CMA-ES (Hansen et al., 2003), a derivative-free optimizer, to learn the text prompt. Instead of directly optimizing the original high-dimensional context (token) embeddings for the prompt, our method learns lower-dimensional latent contexts for mitigating the difficulty of high-dimensional optimization.**

of high-dimensional vectors in PTMs, e.g., \(512\)-D for each "context" (i.e., learnable token in the prompt) in CLIP ViT-B/16 (Dosovitskiy et al., 2021). To mitigate this issue, we propose Latent Context Sharing (LCS), a novel parametrization method of the contexts. The core of LCS is to parameterize each context with low-dimensional latent components, which consist of token-specific components and common components among multiple tokens for the prompt. Experimental results on four standard benchmark datasets demonstrate that our method improves zero-shot CLIP and outperforms reasonable baselines based on black-box prompt tuning (Sun et al., 2022).

The main contributions of this paper are summarized as follows:

* We introduce Black-Box Forgetting, a novel problem of selective forgetting for black-box models.
* We propose a novel method for Black-Box Forgetting based on derivative-free optimization of learnable text prompt.
* We introduce Latent Context Sharing (LCS), a novel parametrization method of contexts for mitigating the difficulty of high-dimensional optimization with derivative-free optimization.

## 2 Related Work

Machine UnlearningMachine unlearning aims to remove an arbitrary sample from a pre-trained model, i.e., obtaining a model that is identical to the one trained from scratch without that sample (Cao and Yang, 2015; Golatkar et al., 2021; Sekhari et al., 2021; Bourtoule et al., 2021; Kurmanji et al., 2023; Guo et al., 2020; Chen et al., 2019). Many methods have been proposed, for example, to construct a forgettable model by transforming the learning algorithm into a sum of the training samples (Cao and Yang, 2015), to achieve forgetting by linear approximation of a nonlinear model (Golatkar et al., 2021), and to update the model to be closer to / farther from the original model in the retain / forget samples (Kurmanji et al., 2023). Methods specific to certain learning algorithms such as LDA (Guo et al., 2020) and SVM (Chen et al., 2019) have also been explored. Machine unlearning and Black-Box Forgetting are closely related but different; Machine unlearning aims to remove the influence of specified training samples on the training model, whereas Black-Box Forgetting aims to prevent the recognition of specified classes. Forgetting specified classes has attracted much attention recently in various contexts (Heng and Soh, 2023; Lu et al., 2024; Zhang et al., 2024; Shibata et al., 2021; Ye et al., 2022). We in this paper address the black-box setting, which has not yet been explored.

Selective Forgetting.Shibata et al. (2021) proposed Learning with Selective Forgetting (LSF), which updates the model for a new task by forgetting only certain classes from the previous task while memorizing the rest of the classes. Golatkar et al. (2020) introduced a scrubbing method that involves a shift in weight space and addition of noise to the weights to remove information from network weights. They also proposed a forgetting mechanism to linearly approximate the weights that would have been obtained by unlearning (Golatkar et al., 2020, 2021). Tarun et al. (2023) proposed an error-maximization-based method to learn a noise matrix for the class to forget, and the model is updated by training on this noise. Then, fine-tuning on the classes to be memorized to adjust the model weights.

Since these methods require the model weights or the gradient of the model parameters, they cannot be applied to black-box models. In this paper, we introduce a new selective forgetting method for black-box models that does not require the model weights or the gradient of the model parameters.

Black-Box Learning.Black-Box Tuning (BBT) (Sun et al., 2022) is a black-box prompt tuning method for large language models. BBTv2 (Sun et al., 2022) improves BBT with deep prompt tuning. They achieve accuracy comparable to white-box learning methods in various natural language tasks. BDPL (Diao et al., 2023) fine-tunes a collection of discrete prompts for language models by treating the word choice in the prompt as a reinforcement learning policy.

BlackVIP (Oh et al., 2023), the first black-box learning method for vision-language models, optimizes a generative model that generates visual prompts embedded in images by zeroth-order optimization. LFA (Ouali et al., 2023) extends the capabilities of black-box models by assuming access to pre-computed features from pre-trained backbones. Through a multi-stage procedure, it optimizes a projection layer to enhance alignment between pre-computed image features and class prototypes. Guo et al. (2023) introduced a collaborative black-box tuning (CBBT) for optimizing both the textual prompt and adapting output visual features in black-box vision-language models. The textual prompt is optimized by estimated gradients and the visual adapter is trained through direct supervised learning from the output features.

In this study, we focus on textual prompt tuning for the black-box model and introduce Latent Context Sharing (LCS), which improves accuracy while reducing the number of dimensions to be optimized.

## 3 Method

The overview of the proposed method is illustrated in Fig. 1. We use the vision-language model CLIP (Radford et al., 2021) as the base model and optimize an input prompt for the CLIP text encoder based on the loss that requests reduced accuracy of selected classes. The derivative-free optimization method must be used to optimize a textual prompt for the black-box model where the gradients of its parameters are unavailable. We employ CMA-ES, a widely used evolutionary algorithm for black-box optimization in continuous, because a textual prompt to be optimized is a continuous variable. CMA-ES is a multi-point search algorithm based on a multivariate normal distribution and proceeds the search by iterating (i) sampling candidate solutions, (ii) evaluating the loss values of the candidates, (iii) weighting the candidates based on the loss values, and (iv) updating the mean and covariance matrix of the distribution by using the weighted candidates. Due to the nature of multi-point search, the performance of CMA-ES degrades in high-dimensional problems, typically ten or more dimensions (Ros and Hansen, 2008; Akimoto and Hansen, 2016). While several extensions have been proposed, e.g., (Ros and Hansen, 2008; Rosin, 2016), these methods require knowledge of independence among variables, which is not always known. In this paper, we propose a customized extension of CMA-ES to Black-Box Forgetting. In general, when applied to high-dimensional black-box continuous optimization by CMA-ES, the computational complexity can become a hindrance. The key is reducing the dimensions of the latent variables in the textual prompt while preserving their context representations.

Figure 2: **Comparison of context parametrization.** (a) Vanilla prompt tuning optimizes the textual prompt directly. This approach requires high-dimensional optimization. (b) BBT (Sun et al., 2022) optimizes a lower-dimensional latent context instead of directly optimizing textual prompt to mitigate high dimensionality. (c) In our LCS, for more effective optimization, a latent context is composed of unique components and common components among multiple latent contexts, and each component is optimized independently.

### Context Parametrization

We discuss two types of context parametrizations: i) Latent Representation with Random Projection for Black-Box Tuning (BBT) [Sun et al., 2022b] as preliminary; ii) Latent Context Sharing (LCS) for our method, a more effective context parametrization approach for the black-box forgetting.

i) Preliminary: Latent Representation with Random Projection.The dimension \(D\) of a context in the prompt is extremely large, which makes derivative-free optimization difficult. To mitigate this high dimensionality, BBT introduces a low-dimensional latent context \([_{}]^{d m}\), where \(d\) is the dimension of a latent context and \(m\) is the number of latent contexts. Then, BBT divides \([_{}]\) into \([_{i}]^{d}\) and generates contexts for the prompt by projecting them to the original context dimension by a random projection \(^{D d}\) (see Fig. 2b) sampled from a normal distribution \((0,)\), where \(\) is the standard deviation of the context (token) embeddings. The dimension of variables to be optimized is suppressed more than optimizing a context directly because \(d\) is a lower dimension than the original context dimension (\(d D\)).

ii) Latent Context Sharing.As empirically shown later in Sec. 4.2, the effectiveness of the context parametrization for BBT described above is limited for selective forgetting settings. We propose latent context sharing (LCS), a more efficient context parametrization.

Fig. 2c shows the overview of LCS. The key idea is to assume shared parameters among different latent contexts. This inspiration comes from successful word embedding methods; most word embedding methods are trained on the assumption that locally co-occurring words have semantic correlations between them (e.g., [Milkolov et al., 2013, Pennington et al., 2014, Devlin et al., 2019]). This inspires the idea of explicitly modeling semantic correlations between words in a prompt as shared components. We assume that each latent context is composed of unique components (_Unique Latent Contexts_ (ULC)) and common components (_Shared Latent Context_ (SLC)) among multiple latent contexts. Then, we optimize each ULC and SLC independently. Each latent context \([_{i}]\) is obtained by concatenating SLC \([^{s}]^{d^{s}}\) and ULC \([^{u}]_{i}^{d^{u}}(i=1,,m)\), where \(m\) is the number of latent contexts, \(d^{s}\) and \(d^{u}\) are the dimension of SLC and ULC, respectively. Despite the number of parameters prepared for BBT and LCS is the same (\(m d=d^{s}+m d^{u}\)), LCS is possible to significantly reduce the number of optimization dimensions compared to BBT2, because LCS optimizes each ULC and SLC independently. Compared to assuming that each latent context is completely independent (i.e., using only ULC), providing common components has the substantial advantage of not losing dependencies among multiple tokens for the prompt.

Note that, CoCoOp [Zhou et al., 2022a] also introduces an approach that incorporates a shared component in the context of the prompt to improve the generalization in the white-box setting. While CoCoOp learns the network that generates a shared component based on image features, our method directly learns the shared component. The optimization for our black-box forgetting using our method becomes simpler because the number of dimensions for optimization is minimal. As shown in the experimental results in Sec. 4, introducing a shared component is effective, which suggests that optimization of shared components has a impact in our problem settings.

### Loss Functions

We apply different loss functions to the classes to be memorized and the classes to be forgotten. The cross-entropy loss is used for the classes to be memorized to maintain the classification accuracy:

\[_{}(,,C)=-_{i=0}^{C-1} _{i}_{i},\] (1)

where \(C\) is the total number of classes, \(\) is confidence of each class obtained by applying the Softmax function to the similarity of each class, which is the output of CLIP, and \(\) is the one-hot vector of the label of an input image.

A naive approach to ensuring that selected target classes are forgotten is to reduce the confidence of that class in the input images of the class. In general, however, this naive approach leads to undesirable behavior in the model. For example, if we force the model to forget the class "dog,"it may lead to the model always classifying images of "dog" as the class with features similar to "dog," e.g., "cat." Moreover, such an unintentional bias may provide sufficient clues to identify the forgotten classes, and consequently may lead to the risk of information leakage. We, therefore, want to make the classification results for images with the forgotten classes close to random and exclude information about the classes. To this end, we maximize the entropy of confidence for each image of the classes to be forgotten. The loss function for the classes to be forgotten is as follows:

\[_{}(,C)=-_{i=0}^{C-1} _{i},\] (2)

where \(\) and \(C\) are the same as Eq. (1).

To summarize, the final objective that we minimize becomes \(=_{}+_{}\). We optimize latent contexts by CMA-ES using the final objective \(\).

### Derivative-Free Optimization: CMA-ES

Since backpropagation cannot be applied to black-box models, we adopt the CMA-ES (Covariance Matrix Adaptation Evolution Strategy) (Hansen et al., 2003), which is a derivative-free optimizer for continuous optimization. In the \(t\)-th iteration, the CMA-ES samples the \(\) candidate solutions from a multivariate normal distribution \((_{t},_{t}^{2}_{t})\), where \(_{t}^{d}\) is the mean vector of the search distribution, \(_{t}^{+}\) is the step-size, \(_{t}^{d d}\) is a covariance matrix. The \(\) solutions should be evaluated on an objective function \(f\), then the CMA-ES updates the parameters \(_{t}\), \(_{t}\) and \(_{t}\) by ranking the \(\) solutions by function value (cf. (Hansen, 2016)).

## 4 Experiments

We evaluate the class forgetting performance of our method on image classification tasks. We first describe our experimental setup, including the datasets, baselines, implementation details, and evaluation metrics. We then report the main comparative results between our method and the baselines, as well as a series of analyses of our method.

### Setup

Datasets.We use four benchmark datasets, i.e., CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30. CIFAR-103 and CIFAR-1004 comprise of a total of 50,000 training images and 10,000 test images (Krizhevsky et al., 2009). These datasets have 10 and 100 classes, respectively. CUB-200-2011 (Wah et al., 2011) comprises of images of 200 distinct bird species, with 5,994 training images and 5,794 test images. ImageNet30 (Hendrycks et al., 2019) is a 30-class subset of the original ImageNet-1k dataset (Deng et al., 2009) (The results on the original ImageNet-1k dataset can also be found in Appendix A.2). It consists of 39,000 training images and 3,000 test images. We conduct experiments in the few-shot condition. We randomly select different \(k\) samples of each class from the original training images to construct a \(k\)-shot training set and a \(k\)-shot validation set. We set \(k\) to 16 for CIFAR-10 and ImageNet30, 4 for CIFAR-100, 1 for CUB-200-2011. For testing, we use the original test set. Unless otherwise noted, the first \(40\%\) of classes are to be forgotten, while the other classes are to be memorized.

Baselines.Black-Box Forgetting has never been studied before, and there is no existing method directly applicable to this problem. So we compare the proposed method with zero-shot CLIP (Radford et al., 2021), BBT (Sun et al., 2022b) and CBBT (Guo et al., 2023), which are the reasonable baselines as it is for black-box prompt tuning. We apply the same loss functions as our method to BBT and CBBT (w/o adapter) for comparison. CBBT introduces a method that combines textual prompt tuning and adapting output visual features for black-box vision-language models. However, in this paper, the conditions are such that visual features cannot be obtained, and only the final inference results can be used. So we compared the proposed method with CBBT without using any adapter. We also apply the same loss functions as our method to CoOp (Zhou et al., 2022b), a white-box method, and use the results as reference values under conditions where information on the target model is available.

Implementation Details.We use ViT-B/16 (Dosovitskiy et al., 2021) as our CLIP image encoder. We set the number of latent contexts \(m=4\) for CIFAR-10, and \(m=16\) for CIFAR-100, CUB-200-2011 and ImageNet30, respectively. The dimension of a latent context in BBT \(d\), Shared Latent Context (SLC) \(d^{s}\), and Unique Latent Contexts (ULC) \(d^{u}\) are set to \(d=10,d^{s}=20,d^{u}=5\) for CIFAR-10, and \(d=125,d^{s}=400,d^{u}=100\) for CIFAR-100, CUB-200-2011 and ImageNet30, respectively. For optimization, CMA-ES with the population size of \(20\) is applied in all the conditions, as done in (Sun et al., 2022b). We optimize the latent contexts for 400 iterations for CIFAR-10 and ImageNet30, and 800 iterations for CIFAR-100 and CUB-200-2011. All the hyperparameters are tuned on the validation sets, which are distinct from the training and test sets.

Evaluation Metrics.We use the following three evaluation metrics: (i) \(Err_{}\) is the error for the classes to be forgotten; (ii) \(Acc_{}\) is the accuracy of the classes to be memorized; (iii) \(H\) is the harmonic mean of \(Err_{}\) and \(Acc_{}\) as in (Shibata et al., 2021). \(H\) gives the overall selective forgetting performance as it is a balance between the forgetting rate for the classes to be forgotten and the classification accuracy for the classes to be memorized. Higher values for all these metrics are desirable. For all the experimental results, including those in the Appendix sections, we report the average performance of the three runs with different random seeds, as well as their standard deviations.

### Main Results: Comparisons with Baselines

Table 1 shows the main comparative results with the baseline methods. Our method improves zero-shot CLIP and outperforms all the baselines on all the datasets.

Comparing our method with BBT, these two are comparable in \(Acc_{}\). However, ours is significantly better than BBT in \(Err_{}\) and consequently clearly outperforms BBT in \(H\) as well for all the datasets; in particular, ours is better than BBT by \(9.38\%\) on CIFAR-10. These results suggest that our LCS can optimize the learnable latent contexts more effectively in the black-box setting than BBT, which optimizes all the latent contexts independently.

When comparing our method with CBBT, which is the state-of-the-art black-box tuning method, we can see that ours outperforms CBBT on all the datasets. While ours is slightly inferior in \(Acc_{}\), it is significantly better in \(Err_{}\) on all the datasets. These results demonstrate that our method achieves higher selective forgetting performance than CBBT and is a more suitable for the black-box forgetting task.

   &  &  \\   & \(H\) & \(Err_{}\) & \(Acc_{}\) & \(H\) & \(Err_{}\) & \(Acc_{}\) \\  Zero-Shot CLIP & 15.30 & 8.37 & 89.05 & 42.14 & 31.17 & 65.03 \\ BBT & \(85.69_{ 0.02}\) & \(79.31_{ 0.03}\) & \(93.19_{ 0.01}\) & \(78.36_{ 0.01}\) & \(87.30_{ 0.01}\) & \(71.09_{ 0.00}\) \\ CBBT & \(93.48_{ 0.02}\) & \(90.99_{ 0.04}\) & \(96.11_{ 0.00}\) & \(73.20_{ 0.00}\) & \(72.69_{ 0.01}\) & \(_{ 0.00}\) \\ Ours (w/o LCS) & \(72.37 0.13\) & \(58.57_{ 0.17}\) & \(94.68_{ 0.01}\) & \(79.38_{ 0.02}\) & \(89.17_{ 0.03}\) & \(71.52_{ 0.01}\) \\ Ours & \(_{ 0.01}\) & \(_{ 0.02}\) & \(94.06_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.02}\) & \(71.52_{ 0.01}\) \\ CoOp (White-Box) & \(96.49_{ 0.00}\) & \(96.95_{ 0.01}\) & \(96.04_{ 0.00}\) & \(82.22_{ 0.00}\) & \(99.81_{ 0.00}\) & \(69.90_{ 0.01}\) \\    &  &  \\   & \(H\) & \(Err_{}\) & \(Acc_{}\) & \(H\) & \(Err_{}\) & \(Acc_{}\) \\  Zero-Shot CLIP & 46.30 & 46.20 & **46.41** & 2.31 & 1.17 & 98.00 \\ BBT & \(58.75_{ 0.01}\) & \(88.98_{ 0.04}\) & \(43.85_{ 0.01}\) & \(94.22_{ 0.05}\) & \(90.17_{ 0.08}\) & \(99.06_{ 0.01}\) \\ CBBT & \(56.84_{ 0.01}\) & \(73.52_{ 0.02}\) & \(46.33_{ 0.01}\) & \(87.88_{ 0.08}\) & \(79.69_{ 0.12}\) & \(_{ 0.02}\) \\ Ours (w/o LCS) & \(58.78_{ 0.01}\) & \(85.85_{ 0.01}\) & \(44.69_{ 0.01}\) & \(95.26_{ 0.02}\) & \(92.19_{ 0.03}\) & \(98.59_{ 0.01}\) \\ Ours & \(_{ 0.01}\) & \(_{ 0.01}\) & \(44.81_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.01}\) & \(98.67_{ 0.01}\) \\ CoOp (White-Box) & \(63.20_{ 0.02}\) & \(98.09_{ 0.02}\) & \(46.62_{ 0.02}\) & \(99.30_{ 0.01}\) & \(99.72_{ 0.00}\) & \(98.89_{ 0.01}\) \\  

Table 1: Comparisons with the baselines. The best value is shown in **bold**. BBT (Sun et al., 2022b) and CBBT (w/o adapter) (Guo et al., 2023) are the reasonable baselines as these are for black-box prompt tuning. CoOp (Zhou et al., 2022b) is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error \(Err_{}\) for the classes to be forgotten, the accuracy \(Acc_{}\) for the classes to be memorized, the harmonic mean \(H\) of \(Err_{}\) and \(Acc_{}\). Higher values mean better performance.

To clarify the impact of our context parametrization method, LCS, we also evaluate the performance of Ours (w/o LCS), i.e., the case without LCS where each latent context is optimized independently. Although these two are highly comparable in \(Acc_{ mem}\), Ours (with LCS) clearly outperforms Ours (w/o LCS) by large margins and consequently shows distinct superiority in \(H\) as well. These results prove that adequate selective forgetting performance cannot be achieved without LCS. Interestingly, these evaluations clearly suggest that each latent context is not inherently independent and that the idea of LCS to model dependencies among latent contexts by introducing a common latent component is valid.

Finally, we compare our method with the white-box method based on CoOp (Zhou et al., 2022), where the latent contexts are optimized by minimizing the same loss functions as ours through the standard backpropagation process. Although CoOp is better able to increase \(Err_{ for}\) than Ours, the differences are within the acceptable ranges. For example, the difference in \(Err_{ for}\) is less than \(1\%\) and that in \(H\) is less than \(2\%\) on CIFAR-10. These results show that even though our method is black-box, it can perform as well as white-box approaches.

### Analysis

#### 4.3.1 Sensitivity to The Number of Latent Contexts

We investigate the sensitivity of the performance of our method to the number of latent contexts \(m\). Fig. 3 shows \(Err_{ for}\), \(Acc_{ mem}\) and \(H\) when the number of latent contexts \(m\) is varied on CIFAR-10. As the number of latent contexts \(m\) increases, the performance in all the metrics tends to improve. This is natural behavior, as the performance improves as the number of trainable parameters increases. Comparing Ours and BBT, we see that Ours for \(m=4\) and BBT for \(m=16\) are almost comparable. This means that BBT needs to optimize about four times the number of dimensions to compete with our LCS, indicating that our LCS provides excellent trade-offs. Furthermore, while BBT suffers a sharp drop in accuracy with decreasing \(m\), our method shows only a small decrease from \(m=16\) even when \(m=1\). This suggests that our LCS is robust to the decrease in the number of latent contexts and its significant superiority to BBT for latent context representation.

#### 4.3.2 Sensitivity to The Dimensionality of SLC and ULC

We investigate the sensitivity of our method to \(d^{s}\) and \(d^{u}\), which are the number of dimensions of SLC and ULC, respectively. In our LCS, the total number of dimensions \(d\) to be optimized is determined as \(d=d^{s}+m d^{u}\) (see Sec. 3.1); we evaluate the performance when we change the ratio \(d^{s}:d^{u}\) under the condition where \(d=40\) and \(m=4\) for CIFAR-10, and \(d=2000\) and \(m=16\) for CIFAR-100 and CUB-200-2011. Fig. 4 shows the results on the three datasets. First, we can see that for all the datasets, the performance in \(Err_{ for}\) and \(H\) significantly degrades for \(d^{s}=0\) (i.e., when no SLC is used) than for the other cases, which supports the validity of the core idea of our LCS that introduces the shared components. As expected, the performance is substantially improved by setting the appropriate ratio. This is a natural trade-off: if \(d^{s}\) (i.e., the number of dimensions of SLC) is too large, the ability to represent the context is reduced and performance deteriorates; conversely, if \(d^{u}\) (i.e., the number of dimensions of ULC) is too large, optimization becomes difficult and performance deteriorates. Meanwhile, our method achieves satisfactory performance in the wide range of \(d^{s}:d^{u}\). Second, when we compare Ours with BBT, Ours is superior to BBT in the wide range of \(d^{s}:d^{u}\). These results justify the strong robustness of our method to \(d^{s}\) and \(d^{u}\).

Figure 3: **Sensitivity to the number of latent contexts. Results of BBT (Sun et al., 2022) and Ours for varying number of the latent contexts. We can see that our method shows stable performance within a wide range of the number of latent contexts in contrast BBT.**

#### 4.3.3 Ours vs. BBT with Modified CMA-ES

CMA-ES (Hansen et al., 2003) is widely used in black-box optimization, but when applied to high-dimensional black-box continuous optimization, the computational complexity can become a hindrance. To apply CMA-ES to high-dimensional optimization, modified versions such as Sep-CMA-ES (Sep) (Ros and Hansen, 2008) and VkD-CMA-ES (VkD) (Akimoto and Hansen, 2016) have been developed. Sep-CMA-ES realizes the computational complexity \(O(d)\) that is linear with respect to the number of dimensions by restricting the covariance matrix to a diagonal matrix. In other words, the solution generation distribution of Sep-CMA-ES does not consider the covariance between variables, but only the variance of each variable. In VkD-CMA-ES, the covariance matrix

   &  \\  Method & \(H\) & \(Err_{}\) & \(Acc_{}\) & \(H\) & \(Err_{}\) & \(Acc_{}\) \\  BBT w/ Sep & \(93.38_{ 0.01}\) & \(93.33_{ 0.02}\) & \(_{ 0.01}\) & \(69.29_{ 0.01}\) & \(67.83_{ 0.02}\) & \(70.81_{ 0.01}\) \\ BBT w/ VkD & \(94.11_{ 0.00}\) & \(95.25_{ 0.02}\) & \(92.99_{ 0.01}\) & \(75.41_{ 0.02}\) & \(79.56_{ 0.01}\) & \(_{ 0.01}\) \\  Ours & \(_{ 0.01}\) & \(_{ 0.02}\) & \(94.06_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.02}\) & \(71.52_{ 0.01}\) \\    &  \\   & \(H\) & \(Err_{}\) & \(Acc_{}\) & \(H\) & \(Err_{}\) & \(Acc_{}\) \\  BBT w/ Sep & \(53.74_{ 0.02}\) & \(74.72_{ 0.02}\) & \(41.96_{ 0.02}\) & \(91.18_{ 0.02}\) & \(84.44_{ 0.04}\) & \(_{ 0.00}\) \\ BBT w/ VkD & \(55.12_{ 0.02}\) & \(81.49_{ 0.03}\) & \(41.65_{ 0.02}\) & \(91.25_{ 0.05}\) & \(84.58_{ 0.09}\) & \(99.06_{ 0.00}\) \\  Ours & \(_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.01}\) & \(_{ 0.01}\) & \(98.67_{ 0.01}\) \\  

Table 2: **Ours vs. BBT with modified CMA-ES.** Results of VkD-CMA-ES are for \(k=30\) on CIFAR-10 and for \(k=1500\) on CIFAR-100, CUB-200-2011, and ImageNet30.

Figure 4: **Sensitivity to the dimensionality of SLC and ULC. Results of BBT (Sun et al., 2022b) and Ours for varying number of dimensions of SLC \(d^{s}\) and ULC \(d^{u}\). We can see the effectiveness of our method in the wide range of \(d^{s}:d^{u}\).**is expressed as \(=(_{d}+^{T})\), where \(\) is a diagonal matrix, \(\) is a \(d k\)-dimensional real matrix in which each column is orthogonal, and \(k\) is a hyperparameter that determines the degree of freedom of the covariance matrix model. Given the availability of these modified CMA-ESs, one question would be that, can our method still perform better than BBT, even when it is combined with these modified CMA-ESs? Table 2 shows the comparisons between Ours and BBTs with the above two variants of CMA-ES. Ours achieves the best performance in terms of \(H\) and \(Err_{}\) for all the datasets. In particular, the two BBT variants show \(Err_{}\) more than \(10\%\) lower than our method on CIFAR-100 and ImageNet30. These results show that our LCS surpasses Sep-CMA-ES and VkD-CMA-ES as a context parametrization method in the black-box forgetting task.

#### 4.3.4 Sensitivity to The Ratio of The Classes To Be Forgotten

Fig. 5 shows \(Err_{}\) and \(H\) when changing the ratio of the classes to be forgotten \(r_{}\) on CIFAR-10. For BBT, we see a decreasing trend in \(Err_{}\) as the number of classes to be forgotten increases. This suggests that the context representation of BBT is inefficient, making it difficult to forget multiple classes at a time. CBBT shows some robustness, but as with BBT, we see that \(Err_{}\) tends to decrease as the number of classes to be forgotten increases. In contrast, our method does not decrease \(Err_{}\) even if the number of classes to be forgotten increases, which confirms the strong performance of our LCS. In terms of \(H\), compared to the baselines, our method shows high robustness independent of the number of classes to be forgotten and achieves high selective forgetting performance.

## 5 Limitations

Our method optimizes the context (token) embeddings of the model through a derivative-free optimization, CMA-ES. That is, we assume that we have access to the context embeddings of the target model. This is a common black-box setting, as similar assumptions have also been made in most of the existing studies (Sun et al., 2022, 2022, Guo et al., 2023). However, there should be models in the real world with a higher level of "black boxness," i.e., models in which even access to contextual embeddedness is prohibited. Addressing such a case is a subject for future research.

## 6 Conclusion

We proposed Black-Box Forgetting, a novel problem of selective forgetting for black-box models. We introduced Latent Context Sharing (LCS), an efficient and effective parametrization method of prompt, which is suitable for derivative-free optimization. Experimental results demonstrated that our method outperforms the reasonable baselines with significant margins. In addition, the sensitivity analyses showed the effectiveness of our LCS.

Figure 5: **Sensitivity to the ratio of the classes to be forgotten**. Results on CIFAR-10 when changing the ratio of the classes to be forgotten. Our method is robust to the ratio of the classes to be forgotten compared to baselines.