# Hierarchical Selective Classification

Shani Goren*

Technion

shani.goren@gmail.com &Ido Galil*

Technion, NVIDIA

idogalil.ig@gmail.com, igalil@nvidia.com &Ran El-Yaniv

Technion, NVIDIA

rani@cs.technion.ac.il, relyaniv@nvidia.com

The first two authors have equal contribution.

###### Abstract

Deploying deep neural networks for risk-sensitive tasks necessitates an uncertainty estimation mechanism. This paper introduces _hierarchical selective classification_ (HSC), extending selective classification to a hierarchical setting. Our approach leverages the inherent structure of class relationships, enabling models to reduce the specificity of their predictions when faced with uncertainty. In this paper, we first formalize hierarchical risk and coverage, and introduce hierarchical risk-coverage curves. Next, we develop algorithms for performing HSC (which we refer to as "inference rules"), and propose an efficient algorithm that guarantees a target accuracy constraint with high probability. We also introduce Calibration-Coverage curves, which analyze the effect of hierarchical selective classification on calibration. Our extensive empirical studies on over a thousand ImageNet classifiers reveal that training regimes such as CLIP, pretraining on ImageNet21k, and knowledge distillation boost hierarchical selective performance. Lastly, we show that HSC improves both selective performance and confidence calibration. Code is available at https://github.com/shanigoren/Hierarchical-Selective-Classification.

## 1 Introduction

Deep neural networks (DNNs) have achieved incredible success across various domains, including computer vision and natural language processing. To ensure the reliability of models intended for real-world applications we must incorporate an uncertainty estimation mechanism, such as _selective classification_, which allows a model to abstain from classifying samples when it is uncertain about their predictions. Standard selective classification, however, has an inherent shortcoming: for any given sample, it is limited to either making a prediction or rejecting it, thereby ignoring potentially useful information about the sample, in case of rejection.

To illustrate the potential consequences of this limitation, consider a model trained for classifying different types of brain tumors from MRI scans, including both benign and malignant tumors. If a malignant tumor is identified with high confidence, immediate action is needed. For a particular hypothetical scan, suppose the model struggles to distinguish between 3 subtypes of malignant tumors, assigning a confidence score of 0.33 to each of them (assuming those estimates are well-calibrated and sum to 1). In the traditional selective classification framework, if the confidence threshold is higher than 0.33, the model will reject the sample, failing to provide valuable information to alert healthcare professionals about the patient's condition, even though the model has enough information to conclude that the tumor is malignant with high certainty. This could potentially result in delayed diagnosis and treatment, posing a significant risk to the patient's life. However, a hierarchically-aware selective model with an identical confidence threshold would classify the tumor as malignant with99% confidence. Although this prediction is less specific, it remains highly valuable as it can promptly notify healthcare professionals, leading to early diagnosis and life-saving treatment for the patient.

This motivates us to propose _hierarchical selective classification_ (HSC), an extension of selective classification to a setting where the classes are organized in a hierarchical structure. Such hierarchies are typically represented by a tree-like structure, where each node represents a class, and the edges reflect a semantic relationship between the classes, most commonly an 'is-a' relationship. Datasets with an existing hierarchy are fairly common, as there are many well-established predefined hierarchies available, such as WordNet  and taxonomic data for plants and animals. Consequently, popular datasets like ImageNet , which is widely used in computer vision tasks, and iNaturalist , have been built upon these hierarchical structures, providing ready-to-use trees. The ImageNet dataset, for example, is organized according to the WordNet hierarchy. A visualization of a small portion of the ImageNet hierarchy is shown in Figure 1. In cases where a hierarchical tree structure is not provided with the dataset, it can be automatically generated using methods such as leveraging LLMs [7; 17; 28; 57].

The key contributions of this paper are as follows:

(1) **We extend selective classification to a hierarchical setting.** We define _hierarchical selective risk_ and _hierarchical coverage_, leading us to introduce _hierarchical risk-coverage curves_.

(2) **We introduce _hierarchical selective inference rules_, i.e., algorithms used to hierarchically reduce the information in predictions based on their uncertainty estimates, improving hierarchical selective performance compared to existing baselines. We also identify and define useful properties of inference rules.

(3) **We propose a novel algorithm to find the optimal confidence threshold** compatible with any base classifier without requiring any fine-tuning, that achieves a user-defined target accuracy with high probability, which can also be set by the user, greatly improving over the existing baseline.

(4) **We conduct a comprehensive empirical study evaluating HSC on more than 1,000 ImageNet classifiers**. We present numerous previously unknown observations, most notably that training approaches such as pretraining on larger datasets, contrastive language-image pretraining (CLIP) , and knowledge distillation significantly boost selective hierarchical performance. Furthermore, we define _hierarchical calibration_ and discover that HSC consistently improves confidence calibration.

## 2 Problem Setup

**Selective Classification:** Let \(\) be the input space, and \(\) be the label space. Samples are drawn from an unknown joint distribution \(P()\) over \(\). A classifier \(f\) is a prediction function \(f:\), and \(=f(x)\) is the model's prediction for \(x\). The _true risk_ of a classifier \(f\) w.r.t.

Figure 1: A detailed example of HSC for the output of ViT-L/16-384 on a specific sample. The base classifier outputs leaf softmax scores, with internal node scores being the sum of their descendant leaves’ scores, displayed in parentheses next to each node. The base classifier incorrectly classifies the image as a ’Golden Retriever’ with low confidence. A selective classifier can either make the same incorrect leaf prediction if the confidence threshold is below 0.29, or reject the sample. A hierarchical selective classifier with the _Climbing_ inference rule (see Section 3) climbs the path from the predicted leaf to the root until the confidence threshold \(\) is met. Setting \(\) above 0.29 yields a hierarchically correct prediction, with smaller \(\) values increasing the coverage. An Algorithm for determining the optimal threshold is introduced in Section 4.

is defined as: \(R(f|P)=_{P}[(f(X),Y)]\), where \(:^{+}\) is a given loss function, for instance, the 0/1 loss. Given a set of labeled samples \(S_{m}=\{(x_{i},y_{i})\}_{i=1}^{m}\), the _empirical risk_ of \(f\) is: \((f|S_{m})=_{i=1}^{m}(f(x_{i}),y_{i})\). Following the notation of , we use a _confidence score_ function \((x,|f)\) to quantify prediction confidence. We require \(\) to induce a partial order over instances in \(\). In this work, we focus on the most common and well-known \(\) function, _softmax response_. For a classifier \(f\) with softmax as its last layer: \((x,|f)=f_{}(x)\). Softmax response has been shown to be a reliable confidence score in the context of selective classification  as well as in hierarchical classification , consistently achieving solid performance. A _selective model_ is a pair \((f,g)\) where \(f\) is a classifier and \(g:\{0,1\}\) is a _selection function_, which serves as a binary selector for \(f\). The selective model abstains from predicting instance \(x\) if and only if \(g(x)=0\). The selection function \(g\) can be defined by a confidence threshold \(\): \(g_{}(x|,f)=[(x,|f)>]\). The performance of a selective model is measured using selective risk and coverage. _Coverage_ is defined as the probability mass of the non-rejected instances in \(\): \((f,g)=_{P}[g(X)]\). The _selective risk_ of \((f,g)\) is: \(R(f,g)=_{P}[(f(X),Y)g(X)]}{(f,g)}\).

Risk and coverage can be evaluated over a labeled set \(S_{m}\), with the _empirical coverage_ defined as: \((f,g|S_{m})=_{i=1}^{m}g(x_{i})\), and the _empirical selective risk_: \((f,g|S_{m})=_{i=1}^{m}(f(x_{i}),y_{i})g(x_{i})\). The performance profile of a selective classifier can be visualized by a _risk-coverage curve_ (RC curve) , a curve showing the risk as a function of coverage, measured on a set of samples. The area under the RC curve, namely _AURC_, was defined by  for quantifying selective performance via a single scalar. See Figure 1(a) for an example of an RC Curve.

**Hierarchical Classification:** Following the notations of  and , a hierarchy \(H=(V,E)\) is defined by a tree, with nodes \(V\) and edges \(E\), the root of the tree is denoted \(r V\). Each node \(v V\) represents a semantic class: the leaf nodes \( V\) are mutually exclusive ground-truth classes, and the internal nodes are unions of leaf nodes determined by the hierarchy. The root represents the semantic class containing all other objects. A sample \(x\) that belongs to class \(y\) also belongs to the ancestors of \(y\). Each node has exactly one parent and one unique path to it from the root node. The set of leaf descendants of node \(v\) is denoted by \((v)\), and the set of ancestors of node \(v\), including \(v\) itself, is denoted by \((v)\). A hierarchical classifier \(f: V\) labels a sample \(x\) as a node \(v V\), at any level of the hierarchy, as opposed to a flat classifier, that only predicts leaves.

Given the hierarchy, it is correct to label an image as either its ground truth leaf node or any of its ancestors. For instance, a Labrador is also a dog, a canine, a mammal, and an animal. While any of these labels is technically correct for classifying a Labrador, the most specific label is clearly preferable as it contains the most information. Thus, it is crucial to observe that the definition of hierarchical correctness is incomplete without considering the amount of information held by the predictions. The correctness of a hierarchical classifier \(f\) on a set of samples \(S_{m}\) is defined by: \(_{i=1}^{m}[f(x_{i})(y_{i})]\). Note that when \(H\) is flat and does not contain internal nodes, the hierarchical accuracy reduces to the accuracy of a standard leaf classifier. In the context of classification tasks, the goal is usually to maximize accuracy. However, a trivial way to achieve 100% hierarchical accuracy would be to simply classify all samples as the root node. For this reason, a mechanism that penalizes the model for predicting less specific nodes must be present as well. In section 3 we define _coverage_, which quantifies prediction specificity. We aim for a trade-off between the accuracy and information gained by the prediction, which can be controlled according to a user's requirements. A review of related work involving hierarchical classification can be found in section 6.

**Hierarchical Selective Classification:** Selective classification offers a binary choice: either to predict or completely reject a sample. We propose hierarchical selective classification (HSC), a hierarchical extension of selective classification, which allows the model to retreat to less specific nodes in the hierarchy in case of uncertainty. Instead of rejecting the whole prediction, the model can now partially reject a sample, and the degree of the rejection is determined by the model's confidence. For example, if the model is uncertain about the specific dog breed of an image of a Labrador but is confident enough to determine that it is a dog, the safest choice by a classic selective framework would be to reject it. In our setting, however, the model can still provide useful information that the object in the image is a dog (see Figure 1).

A _hierarchical selective classifier_\(f^{H}(f,g_{}^{H})\) consists of \(f\), a base classifier, and \(g_{}^{H}: V\), a hierarchical selection function with a confidence threshold \(\). \(g_{}^{H}\) determines the degree of partial rejection by selecting a node in the hierarchy with confidence higher than \(\). Defining \(g_{}^{H}\) now becomes non-trivial. For instance, \(g_{}^{H}\) can traverse the hierarchy tree, directly choose a single node,or follow any other algorithm, as long as the predicted node has sufficient confidence. For this reason, we refer to \(g_{}^{H}\) as a _hierarchical inference rule_. In section 3 we introduce several inference rules and discuss their properties.

Hierarchical selective classifiers differ from the previously discussed hierarchical classifiers by requiring a hierarchical selection function. In contrast to non-selective hierarchical classifiers, which may produce predictions at internal nodes without a selection function, a hierarchical selective classifier can handle uncertainty by gradually trading off risk and coverage, controlled by \(g_{}^{H}\). This distinction is crucial because hierarchical selective classifiers provide control over the full trade-off between risk and coverage, which is not always attainable with non-selective hierarchical classifiers.

To our knowledge, controlling the trade-off between accuracy and specificity was only previously explored by , who proposed the _Dual Accuracy Reward Trade-off Search_ (DARTS) algorithm, which aims to obtain the most specific classifier for a user-specified accuracy constraint. Our approach differs from theirs in that we guarantee control over the full trade-off, while some coverages cannot be achieved by DARTS.

As mentioned in section 2, it is considered correct to label an image as either its ground truth leaf node or any of its ancestors. Thus, we employ a natural extension of the 0/1 loss to define the true risk of a hierarchical classifier \(f^{H}\) with regard to \(P\): \(R^{H}(f^{H}|P)=_{P}[f^{H}(X)(Y)]\), and the empirical risk over a labeled set of samples \(S_{m}\): \(}(f^{H}|S_{m})=_{i=1}^{m}[f^{H}(x_{i}) (y_{i})]\). An additional risk penalizing hierarchical mistake severity is discussed in Appendix A.

To ensure that specific labels are preferred, it is necessary to consider the specificity of predictions. _Hierarchical selective coverage_, which we propose as a hierarchical extension of selective coverage, measures the amount of information present in the model's predictions. A natural quantity for that is entropy, which measures the uncertainty associated with the classes beneath a given node. Assuming a uniform prior on the leaf nodes, the entropy of a node \(v V\) is \(H(v)=-_{v^{}(v)}(v)|}((v)|})=(|(v)|)\). At the root, the entropy reaches its maximum value, \(H(r)=(||)\), while at the leaves the entropy is minimized, with \(H(y)=0\) for any leaf node \(y\). This allows us to define coverage for a single node \(v\), regardless of \(g_{}^{H}\). We define _hierarchical coverage_ (from now on referred to as coverage) as the entropy of \(v\) relative to the entropy of the root node: \(^{H}(v)=1-\). The root node has zero coverage, as it does not contain any information. The coverage gradually increases until it reaches 1 at the leaves. We can also define true coverage for a hierarchical selective model: \(^{H}(f^{H})=_{P}[(f^{H}(X)]\). The empirical coverage of a classifier over a labeled set of samples \(S_{m}\) is defined as the mean coverage over its predictions: \(^{H}(f^{H}|S_{m})=_{i=1}^{m}(f^{H}(x_{i}))\). For a hierarchy comprised of leaves and a root node, hierarchical coverage reduces to selective coverage, where classifying a sample as the root corresponds with rejection.

The hierarchical selective performance of a model can be visualized with a hierarchical RC curve. The area under the hierarchical RC curve, which we term _hAURC_, extends the non-hierarchical AURC, by using the hierarchical extensions of selective risk and coverage. For examples of hierarchical RC curves see Figure 1(a) and Figure 1(b).

**Hierarchical Calibration:** Confidence calibration refers to the task of predicting probability estimates that accurately reflect the true likelihood of correctness. As reducing hierarchical coverage may improve accuracy, it may also improve calibration over the samples the model has not abstained from. To capture this relationship, we introduce the concept of _Calibration-Coverage Curves_ (CC curves), which, analogous to RC curves, plot the Expected Calibration Error (ECE)  as a function of coverage. see Figure 5 for an example.

## 3 Hierarchical Selective Inference Rules

To define a hierarchical selective model \(f^{H}=(f,g_{}^{H})\) given a base classifier \(f\), an explicit definition of \(g_{}^{H}\) is required. \(g_{}^{H}\) determines the internal node in the hierarchy \(f^{H}\) retreats to when the leaf prediction of \(f\) is uncertain. \(g_{}^{H}\) requires obtaining confidence for internal nodes in the hierarchy. Since most modern classification models only assign probabilities to leaf nodes, we follow  by setting the probability of an internal node to be the sum of its leaf descendant probabilities: \(f_{v}^{H}(x)=_{y(v)}f_{y}(x)\). Unlike other works that assign the sum of leaf descendant probabilities to internal nodes, our algorithm first calibrates leaf probabilities through _temperature scaling_. Since the probabilities of internal nodes heavily rely on the leaf probabilities supplied by \(f\), calibration is beneficial for obtaining more reliable leaf probabilities, and since the internal nodes probabilities are sums of leaf probabilities, this cumulative effect is even more pronounced. Further,  found that applying temperature scaling also improves ranking and selective performance, which is beneficial to our objective. In this section, we introduce several inference rules, along with useful theoretical properties. We propose the _Climbing_ inference rule (Algorithm 1), which starts at the most likely leaf and climbs the path to the root, until reaching an ancestor with confidence above the threshold. A visualization of the Climbing inference rule is shown in Figure 1.

We compare our proposed inference rules to the following baselines: The first is the _selective inference rule_, which predicts 'root' for every uncertain sample (this approach is identical to standard "hierarchically-ignorant" selective classification). The second hierarchical baseline, proposed by , selects the node with the highest coverage among those with confidence above the threshold. We refer to this rule as "_Max-Coverage_" (MC), and its algorithm is detailed in Appendix B.

Certain tasks might require guarantees on inference rules. For example, it can be useful to ensure that an inference rule does not turn a correct prediction into an incorrect one. Therefore, for an inference rule \(g_{0}^{H}\) we define:

1. **Monotonicity in Correctness**: For any \(_{1}_{2}\), base classifier \(f\) and labeled sample \((x,y)\): \((f,g_{_{1}}^{H})(x)(y)(f,g_{_{2}}^{H})(x) (y)\). Increasing the threshold will never cause a correct prediction to become incorrect.

2. **Monotonicity in Coverage**: For any \(_{1}_{2}\), base classifier \(f\) and labeled sample \((x,y)\): \(^{H}[(f,g_{_{1}}^{H})(x)]^{H}[(f,g_{_{2}}^{H})(x)]\). Increasing the threshold will never increase the coverage.

The Climbing inference rule outlined in Algorithm 1 satisfies both monotonicity properties. MC satisfies monotonicity in coverage, but not in correctness. An additional algorithm that does not satisfy any of the properties is discussed in Appendix C.

Figure 2: (a) hierarchical RC curve of a ViT-L/16-384 model trained on ImageNet1k, evaluated with the 0/1 loss as the risk and softmax response as its confidence function \(\). The purple shaded area represents the area under the RC curve (hAURC). Full coverage occurs when the model accepts all leaf predictions, for which the risk is 0.13. Increasing the confidence threshold leads to the rejection of more samples. For example, when the threshold is 0.77 the the risk is 0.04, with coverage 0.8.

(b) hierarchical RC curves of different inference rules with EVA-L/14-196  as the base classifier. When the coverage is 1.0, all inference rules predict leaves. Each inference rule achieves a different trade-off, resulting in distinct curves. This example represents the prevalent case, where the “hierarchically-ignorant” selective inference rule performs the worst and Climbing outperforms MC.

When comparing hierarchical selective models, we find it useful to measure the performance improvement gained by using hierarchical selective inference. We propose a new metric: _hierarchical gain_, defined as the improvement in hAURC between \((f,g_{})\), a "hierarchically-ignorant" selective model, and \((f,g_{}^{})\), the same base classifier with a hierarchical inference rule. This metric might also point to which models have a better hierarchical understanding, as it directly measures the improvement gained by allowing models to leverage hierarchical knowledge. Note that if the selective inference rule is better than the hierarchical inference rule being assessed, the hierarchical gain will be negative. An illustrative individual example of RC curves comparison for one model is shown in Figure 1(b).

## 4 Optimal Selective Threshold Algorithm

In Section 3, we defined several inference rules that, given a confidence threshold, return a predicted node in the hierarchy. In this section, we propose an algorithm that efficiently finds the optimal threshold for a user-defined target accuracy and confidence level. The algorithm, outlined in Algorithm 2, receives as input a hierarchical selective classifier \(f^{H}\), an accuracy target \(1-\), a confidence level \(1-\) (which refers to the interval around \(1-\), not to be confused with the model's confidence), and a calibration set. It outputs the optimal threshold \(\) that ensures the classifier's accuracy on an unseen test set falls within a \(1-\) confidence interval around \(1-\), with a resulting error margin of \(\). The threshold is calculated once on the calibration set and then used statically during inference. The algorithm does not require any retraining or fine-tuning of the model's weights. For each sample \((x_{i},y_{i})\) in the calibration set, the algorithm first calculates \(_{i}\), the minimal threshold that would have made the prediction \(f^{H}(x_{i})\) hierarchically correct. Then \(\), the optimal threshold, is calculated in a method inspired by split conformal prediction .

**Theorem 1**_Suppose the calibration set \(S_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\) and a given test sample \((x_{n+1},y_{n+1})\) are exchangeable. For any target accuracy \((0,1)\) and \(\), define \(_{n+1}\), \(\) and \(\) as in Algorithm 2, and \(C(n,)=P(_{n+1}|S_{n})\). Then:\(P(|C(n,)-(1-)|) 1-\)._

**Proof:** See Appendix E.

**Remark on Theorem 1:** our algorithm can provide an even greater degree of flexibility: the user may choose to set the values of any three parameters out of \(,n,,\). With the three parameters fixed, we can compute the remaining parameter. The size of the calibration set is of particular interest. Although increasing \(n\) yields more stable results, our guarantees hold for calibration sets of any size. Even for a small calibration set, our algorithm provides users with the flexibility to set the other parameters according to their requirements and constraints. For example: a user with a low budget for a calibration set who may be, perhaps, more interested in controlling \(\) can still achieve a reasonable constraint by relaxing \(\) instead of increasing the calibration set size. See Appendix E for details.

## 5 Experiments

In this section we evaluate the methods introduced in Section 3 and Section 4. The evaluation was performed on 1,115 vision models pretrained on ImageNet1k , and 6 models pretrained on iNat-21  (available in timm 0.9.16  and torchvision 0.15.1 ). The reported results were obtained on the corresponding validation sets (ImageNet1k and iNat-21). The complete results and source code necessary for reproducing the experiments are provided in the Supplementary Material.

**Inference Rules:** Table 1 compares the mean results of the Climbing inference rule to both hierarchical and non-hierarchical baselines. The evaluation is based on RC curves generated for 1,115 models pretrained on ImageNet1k and 6 models pretrained on iNat21 with each inference rule applied to their output. These aggregated results show the effect of different hierarchical selective inference rules, independent of the properties or output of a specific model. Compared to the non-hierarchical selective baseline, hierarchical inference has a clear benefit. Allowing models to handle uncertainty by partially rejecting a prediction instead of rejecting it as a whole, proves to be advantageous; the average model is capable of leveraging the hierarchy to predict internal nodes that reduce risk while preserving coverage. Nonetheless, the differences remain stark when comparing the hierarchical inference rules. Climbing, achieving almost 15% hierarchical gain, outperforms MC, with more than double the gain of the latter. These results highlight the fact that the approach taken by inference rules to leverage the hierarchy can have a significant impact. A possible explanation for Climbing's superior performance could stem from the fact that most models are trained to optimize leaf classification. By starting from the most likely leaf, Climbing utilizes the prior knowledge embedded in models for leaf classification, while MC ignores it.

**Optimal Threshold Algorithm:** We compare our algorithm to DARTS . We evaluate the performance of both algorithms on 1,115 vision models trained on ImageNet1k, for a set of target accuracies. For each model and target accuracy the algorithm was run 1000 times, each with a randomly drawn calibration set. We also evaluate both algorithms on 6 models trained on the iNat21 dataset. We compare the algorithms based on two metrics: (1) _target accuracy error_, i.e., the mean distance between the target accuracy and the accuracy measured on the test set; (2) coverage achieved by using the algorithms' output on the test set. The results presented in Table 2, and Table 5 in Appendix G, show that our algorithm consistently achieves substantially lower target accuracy errors, indicating that our algorithm succeeds in nearing the target accuracy more precisely. This property allows the model to provide a better balance between risk and coverage. Our algorithm is more inclined towards this trade-off, as it almost always achieves higher coverage than DARTS. This is particularly noteworthy when the target accuracy is high: while DARTS loses coverage quickly, our algorithm maintains coverage that is up to twice as high. Importantly, DARTS does not capture the whole risk-coverage trade-off. Specifically, at the extreme point in the trade-off when it aims to maximize specificity, it still falls short of providing full coverage, and its predictions do not reduce to a flat classifier's predictions, i.e. it may still predict internal nodes.

    &  \\   & **hAURC** & **Hier. Gain** & **hAURC** & **Hier. Gain** \\  & (\(\) **1000**) & **(\%)** & **(\(\) **1000**)** & **(\%)** \\ 
**Selective** & 42.27\(\)0.46 & - & 13.35\(\)0.50 & - \\
**MC** & 39.99\(\)0.53 & 6.92\(\)0.26 & 11.96\(\)0.45 & 10.45\(\)0.46 \\
**Climbing** & **36.51\(\)0.47** & **14.94\(\)0.22** & **10.15\(\)0.41** & **23.94\(\)1.24** \\   

Table 1: Comparison of mean hAURC and hierarchical gain results for the selective, Max-Coverage (MC) and Climbing inference rules applied to 1,115 models trained on ImageNet1k, and 6 models trained on iNat21.

  
**Target Accuracy (\%)** &  &  \\  & **DARTS** & **Ours** & **DARTS** & **Ours** \\ 
70 & 14.52\(\)1.3e-03 & **10.79\(\)3.1e-03** & 0.97\(\)1.4e-05 & **1.00\(\)5.0e-05** \\
80 & 4.86\(\)5.2e-03 & **2.19\(\)7.0e-03** & 0.96\(\)5.8e-05 & **0.98\(\)1.0e-04** \\
90 & 0.73\(\)9.6e-03 & **0.02\(\)4.0e-04** & **0.88\(\)9.9e-04** & 0.87\(\)2.9e-05 \\
95 & 0.69\(\)1.4e-02 & **0.02\(\)2.2e-04** & 0.74\(\)2.6e-03 & **0.76\(\)7.4e-05** \\
99 & 0.63\(\)4.2e-03 & **0.02\(\)1.1e-04** & 0.22\(\)2.0e-03 & **0.40\(\)3.2e-04** \\
99.5 & 0.40\(\)1.9e-03 & **0.02\(\)7.5e-05** & 0.13\(\)9.8e-04 & **0.26\(\)2.5e-04** \\   

Table 2: Results (mean scores) comparing the hierarchical selective threshold algorithm (Algorithm 2) with DARTS, repeated 1000 times for each model and target accuracy with a randomly drawn calibration set of 5,000 samples, applied to 1,115 models trained on ImageNet1k (meaning we evaluated each algorithm 1,115,000 times). \(1-\) is set at 0.9.

Our algorithm offers additional flexibility to the user by allowing the tuning of the confidence interval (\(1-\)), while DARTS does not offer such control. Figure 3 illustrates the superiority of our technique over DARTS in detail. Appendix H compares the results of an additional baseline, conformal risk control . The results show that the conformal risk control algorithm exhibits a significantly higher mean accuracy error than both our algorithm and DARTS.

**Empirical Study of Training Regimes:** Inspired by , which demonstrated that training methods such as knowledge distillation significantly impact selective performance, we aimed to investigate whether training regimes could also contribute to hierarchical selective performance. The goal of this experimental section is to provide practitioners with valuable insights for selecting effective training regimes or pretrained models for HSC.

We evaluated the hAURC of models trained with several training regimes: (a) **Knowledge Distillation (KD)**[42; 1; 37]; (b) **Pretraining**: models pretrained either on ImageNet21k [41; 43; 56; 29; 37] or on ImageNet12k, a 11,821 class subset of the full ImageNet21k ; (c) **Contrastive Language-Image pretraining (CLIP)**: CLIP models equipped with linear-probes, pretrained on WIT-400M image-text pairs by OpenAI, as well as models pretrained with OpenCLIP on LAION-2B [39; 8], fine-tuned either on ImageNet1k or on ImageNet12k and then ImageNet1k. Note that _zero-shot_ CLIP models (i.e., CLIP models without linear-probes) were not included in this evaluation, and are discussed later in this section. (d) **Adversarial Training**[53; 44]; (e) various forms of **Weakly-Supervised** or **Semi-Supervised Learning**[55; 54]. To ensure a fair comparison, we only compare pairs of models that share identical architectures, except for the method being assessed (e.g., a model trained with KD is compared to its vanilla counterpart without KD). Sample sizes vary according to the number of available models for each method. The hAURC results of all models were obtained by using the Climbing inference rule.

(1) CLIP **exceptionally improves hierarchical selective performance, compared to other training regimes**. Out of the methods mentioned above, CLIP (orange box), when equipped with a "linear-probe", improves hierarchical performance the most by a large margin. As seen in Figure 4, the improvement is measured by the relative improvement in hAURC between the vanilla version of the model and the model itself. CLIP achieves an exceptional improvement surpassing 40%. Further, its median improvement is almost double the next best methods, pretraining (purple box) and semi-supervised learning (light blue box). One possible explanation for this improvement is that the rich representations learned by CLIP lead to improved hierarchical understanding. The image-text alignment of CLIP can express semantic concepts that may not be present when learning exclusively from images. Alternatively, it could be the result of the vast amount of pertaining data.

(2) Pretraining **on larger ImageNet datasets benefits hierarchical selective performance**, with certain models achieving up to a 40% improvement. However, the improvement rates vary significantly: not all models experience the same benefit from pretraining. Semi-supervised learning also

Figure 3: Individual model examples comparing the hierarchical selective threshold algorithm against DARTS, with each algorithm repeated 1000 times. The mean and median results are shown in dark green. The light green area shows the \(\) interval around the target accuracy, and the remaining area is marked in red (i.e., each repetition has a \(1-\) probability of being in the green area and a \(\) probability of being in the red area). The target accuracy is 95% and \(1-=0.9\). In both examples, the target accuracy error of DARTS is high, and the entirety of its accuracy distribution lies outside of the confidence interval. **Left:** EVA-Giant/14 . DARTS fails to meet the constraint, whereas our algorithm’s mean accuracy is very close to the target. **Right:** ResNet-152 . While our algorithm has a near-perfect mean accuracy, DARTS rejects all samples, resulting in zero coverage.

shows a noticeable improvement in hierarchical selective performance.

(3) **Knowledge Distillation achieves a notable improvement**, with a median improvement of around 10%. Although not as substantial as the dramatic increase seen with CLIP, it still offers a solid improvement. This observation aligns with , who found that knowledge distillation improves selective prediction performance as well as ranking and calibration.

(4) **Linear-probe CLIP significantly outperforms zero-shot CLIP in HSC**: We compared pairs of models with identical backbones, where one is the original zero-shot model, and the other was equipped with a linear-probe, that is, it uses the same frozen feature extractor but has an added head trained to classify ImageNet1k. The zero-shot models evaluated are the publicly available models released by OpenAI. The mean relative improvement from zero-shot CLIP to linear-probe CLIP is 45%, with improvement rates ranging from 32% to 53%. We hypothesize that the hierarchical selective performance boost may be related to better calibration or ranking abilities. Specifically, all CLIP models with linear-probes showed significantly higher AUROC than their zero-shot counterparts, indicating superior ranking. Figure 7 in Appendix I shows these results in more detail.

Since hAURC is closely related to accuracy, we sought to determine whether its improvement could be attributed solely to gains in predictive performance from the training regimes. Our analysis, detailed in Appendix J, suggests this is not the case. Additionally, we explored a training regime specifically aimed at enhancing HSC performance. Our approach is described in Appendix K.

**HSC Improves Confidence Calibration:** We plotted the Calibration-Coverage (CC) Curves (defined in Section 2 for 1,115 ImageNet models and compared the curves produced by the Selective inference rule to those from Climbing. Figure 5 presents the aggregated CC curve for all 1,115 models across the two inference rules. The results indicate that HSC not only improves risk, but also improves calibration, with the Climbing inference rule nearly always outperforming Selective.

Figure 4: Comparison of different methods by their improvement in hAURC, relative to the same model’s performance without the method. The number of models evaluated for each method: knowledge distillation: 42, pretraining: 61, CLIP: 16, semi-supervised learning: 11, adversarial training: 8.

Figure 5: Aggregated (mean and SEM) CC curves of 1,115 ImageNet models.

Related Work

In selective classification, several alternative methods for confidence thresholding have been developed [5; 31; 38]. However, these methods generally necessitate some form of training. This work is focused on post-hoc methods that are compatible with any pretrained classifier, which is particularly advantageous for practitioners. Furthermore, despite the availability of other options, threshold-based rejection even with the popular Softmax confidence score continues to be widely used [18; 21], and can be readily enhanced with temperature scaling or other techniques [6; 15; 18].

Various works leverage a hierarchy of classes to improve leaf predictions of flat classifiers [51; 4; 26] but comparatively fewer works explore hierarchical classifiers.  optimized a "win" metric comprising a weighted combination of likelihoods on the path from the root to the leaf.  focused on reducing leaf mistake severity, measured by graph distance. They introduced a hierarchical cross-entropy loss, as well as a soft label loss that generalizes label smoothing.  claimed that both methods in  result in poorly calibrated models, and proposed an alternative inference method.  proposed the Dual Accuracy Reward Trade-off Search (DARTS) algorithm, which attempted to obtain the most specific classifier for a user-specified accuracy constraint. They used information gain and hierarchical accuracy as two competing factors, which are integrated into a generalized Lagrange function to effectively obtain multi-granularity decisions. Additional approaches include the Level Selector network, trained by self-supervision to predict the appropriate level in the hierarchy .  proposed a loss based on  and performed inference using a threshold-based inference rule. Other works allow non-mandatory leaf node prediction, although not directly addressing the accuracy-specificity trade-off [36; 52]. The evaluation of hierarchical classifiers has received relatively little attention previous to our research. The performance profile of a classifier could be inferred from either the average value of a metric across the operating range or by observing operating curves that compare correctness and exactness or information precision and recall , or measuring information gain [12; 52]. Set-valued prediction  and hierarchical multi-label classification  tackle a similar problem to hierarchical classification by allowing a classifier to predict a set of classes. Although both approaches handle uncertainty by predicting a set of classes, the HSC framework hard-codes the sets as nodes in the hierarchy. This way, HSC pixels additional world knowledge contained in the hierarchical relations between the classes, yielding a more interpretable prediction. Conformal risk control  is of particular interest because it constrains the prediction sets to be hierarchical. Appendix H shows a comparison of this baseline to our method.

Calibration is another important aspect of uncertainty estimation [27; 58; 18; 16]. Fisch et al.  demonstrate that selectively abstaining from uncertain predictions may improve calibration.

## 7 Concluding Remarks

This paper presents HSC, an extension of selective classification to a hierarchical setting, allowing models to reduce the information in predictions by retreating to internal nodes in the hierarchy when faced with uncertainty. The key contributions of this work include the formalization of hierarchical risk and coverage, hierarchical calibration, the introduction of hierarchical risk-coverage curves and hierarchical calibration-coverage curves, the development of hierarchical selective inference rules, and an efficient algorithm to find the optimal confidence threshold for a given target accuracy. Extensive empirical studies on over a thousand ImageNet classifiers demonstrate the advantages of the proposed algorithms over existing baselines and reveal new findings on training methods that boost hierarchical selective performance. However, there are a few aspects of this work that present opportunities for further investigation and improvement: (1) Our approach utilizes softmax-based confidence scores. exploring alternative confidence functions and their impact on hierarchical selective classification could provide further insights; (2) While we have identified certain training methods that boost hierarchical selective performance, the training regimes were not optimized for hierarchical selective classification. Future research could focus on optimizing selective hierarchical performance. (3) Although our threshold algorithm is effective, it could be beneficial to train models to guarantee specific risk or coverage constraints supplied by users, in essence constructing a hierarchical "SelectiveNet" .