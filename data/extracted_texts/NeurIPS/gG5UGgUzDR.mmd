# Towards Measuring Representational Similarity of Large Language Models

Max Klabunde

Mehdi Ben Amor

Michael Granitzer

University of Passau

firstname.lastname@uni-passau.de Florian Lemmerich

###### Abstract

Understanding the similarity of the numerous released large language models (LLMs) has many uses, e.g., simplifying model selection, detecting illegal model reuse, and advancing our understanding of what makes LLMs perform well. In this work, we measure the similarity of representations of a set of LLMs with 7B parameters. Our results suggest that some LLMs are substantially different from others.We identify challenges of using representational similarity measures that suggest the need of careful study of similarity scores to avoid false conclusions.

## 1 Introduction

Numerous large language models (LLMs) with remarkable natural language understanding and reasoning capabilities have been released in recent months (Yang et al., 2023; Zhao et al., 2023). However, a comprehensive understanding of the differences between these models beyond architectures and benchmark performances is yet to be established. This is partly due to the inherent challenges in LLMs' explainability given their scale, their high demand for computational resources, and the rising trend of proprietary models.

We argue that a thorough understanding of similarities and differences of LLMs is highly desirable: it may help identify factors that make models perform well, clear up the generalizability of studies of individual LLMs, simplify model selection, enhance our ability to ensure alignment of model behavior with human goals, improve ensembling, benchmark models without labeled data, identify (potentially illegal) model (re)use, and may aid certification of models, which could be required by future regulation of AI.

LLM similarity can be studied from multiple perspectives, including functional similarity, i.e., whether they produce similar outputs, representational similarity, i.e., whether they have similar internal representations, whether they have similar reliance on specific training data, or whether they were trained in a similar manner. Methods for these perspectives have been proposed in prior work, but often focus on non-sequence models (Klabunde et al., 2023) or do not scale to the size of LLMs (Shah et al., 2023). In this work, we focus on representational similarity in the last layer as it implies functional similarity, because the final layer has limited options to diverge functionally. Additionally, it allows studying similarity of _how_ outputs are generated.

Similarity of language models was studied to some extent (Wu et al., 2020; Ethayarajh, 2019), but these works do not explore similarity of decoder-only models on the scale of recent LLMs, and instead focus on smaller BERT-style models. As LLMs have developed at break-neck speed, analysis of the similarity of LLMs is generally limited. Moreover, many novel tools to study the similarity of representations have emerged relatively recently.

In this paper, we aim to make the first steps towards understanding similarity of LLMs in more detail:1. After discussing several options to compare LLMs, we outline how representational similarity measures can be applied to LLMs (Sec. 2).
2. We present first empirical results regarding the representational similarity of a set of 7B parameter models, offering a preliminary view into LLM similarity for commonsense reasoning (Winogrande) and code generation (HumanEval) (Sec. 3).
3. We identify challenges of gaining a reliable picture of similarity of LLM representations (Sec. 3).

Our code and data is publicly available (see Appendix D).

**Related Work.** Several works study representations of language models and make implicit comparisons: how contextual they are (Ethayarajh, 2019), what interpretable concepts can be decoded from them (Liu et al., 2019), or how models can communicate via representations (Moschella et al., 2023). Wu et al. (2020); Abnar et al. (2019) explicitly compare representations between models. However, these works have in common that they do not study the current generation of LLMs, and instead focus on smaller models with different architectures like BERT (Devlin et al., 2019) or ELMo (Peters et al., 2018). Similarity of these models was also studied from a functional perspective (McCoy et al., 2020). As an exception, Gurnee et al. (2023) probe the recent Pythia models (Biderman et al., 2023). Concurrent work (Yousefi et al., 2023) studies representations of modern LLMs. Performance of LLMs is compared in many benchmarks (e.g., Srivastava et al., 2023).

## 2 Comparing Large Language Models

In this section, we discuss different ways to study similarity of LLMs and explain representational similarity in more detail, which we focus on in our experiments.

**Multitude of Comparison Approaches.** There are multiple different approaches to comparing LLMs and measuring their similarity. Five approaches are functional similarity, representational similarity, weight similarity, similarity of training data attribution, and procedural similarity.

_Functional similarity_ aims to compare the outputs of models (Klabunde et al., 2023). A common approach is to compare performance, where performance similarity equates to model similarity. However, performance alone only gives a partial view of functional similarity: benchmarks represent only a sample of data and models may differ on individual predictions or subgroups of the data, which may impact other functional aspects such as fairness. Hence, more facets of model function need to be compared for a thorough understanding of functional similarity.

_Representational similarity_ compares representations of different layers or models under consideration of their symmetries, i.e., transformations of representations that keep them equivalent such as changing neuron order (Klabunde et al., 2023). Studying _weight similarity_ is a related perspective requiring consideration of symmetries (Wang et al., 2022). These approaches can identify cases where two models can be functionally similar but produce the same output differently.

_Training data attribution_ (e.g., Shah et al., 2023; Grosse et al., 2023) is an approach that identifies the most relevant samples of the training data with respect to influencing the model towards a specific output. From this perspective, models are similar if they have the same relevant training samples for a specific prediction.

Finally, _procedural similarity_ is one step removed from the model itself. It takes the perspective that models are similar if the way they are produced is similar. This includes similarity of datasets used for training, hyperparameters, and architecture (e.g., Zhao et al., 2023).

In this work, we focus on one perspective: representational similarity. In the following, we explain how representational similarity measures can be applied to LLMs.

**Representational Similarity.** Let \(f^{(l)}\) be the model that consists of the first \(l\) layers of the model \(f\). Then, given \(N\) inputs, their \(D\)-dimensional representations in layer \(l\) are given as \(:=^{(l)}=f^{(l)}()^{N D}\), where \(f^{(l)}\) is applied row-wise on the inputs \(\). We denote the representations \(\) is compared to as \(}=f^{(l^{})}()\). A representational similarity measure \(m(,})\) then assigns a similarity (or a dissimilarity) score to two representations. Although representations may not be identical, they might be seen as equivalent, e.g., if \(=-}\). Hence, a measure respecting these symmetries has certain _invariances_. A measure is invariant to a set of transformations \(\) if \(m((),(}))=m(,})\;,\). We refer to the survey by Klabunde et al. (2023) for a more detailed introduction.

In this work, we study the representations of the last layer before the final classifier under the invariances to orthogonal transformations (OT), which include rotations and reflections, isotropic scaling (IS), and translation (TR). We justify this selection with the fact that these representations do not have a privileged basis (Elhage et al., 2021), i.e., there is no reason for the basis dimensions (neurons) to have special meaning. This is because the representations could be arbitrarily rotated by applying the same transformation to the weight matrices that add information to the stream. The arbitrary rotation validates OT invariance; similarly, IS invariance is useful. A translation of the representations can affect the outcome of the classification layer for models without a bias, but could also be arbitrarily added by the biases of previous layers. Hence, we study similarity with and without TR invariance. The measures we use for OT and IS invariance experiments are Orthogonal Procrustes (Ding et al., 2021), Aligned Cosine Similarity (Hamilton et al., 2016), the norm of the difference of pairwise similarity matrices (Yin and Shen, 2018), and Jaccard similarity (Schumacher et al., 2021; Wang et al., 2022). For the experiment with OT, IS, and TR invariance, we additionally use RSA (Kriegeskorte et al., 2008) and CKA (Kornblith et al., 2019). To achieve the desired invariances, we preprocess the representations in some cases by normalizing their scale or centering the columns. Details are in Appendix A.

All of these measures make two assumptions that are generally violated with LLMs: (i) the representation of inputs is deterministic, and (ii) all rows of the representations correspond exactly. With LLMs, the representation of a part of an input sequence depends on representations of earlier parts. These parts are usually sampled if new text is generated, which contradicts (i). Further, differing tokenization can lead to a different number of tokens for the same input text, and thus to a different number of rows in the representation matrix, which violates (ii). We can sidestep these problems. First, we only study the representations of fixed input prompts, which avoids the problem of non-determinism of text generation. Second, we only compare the representations of the final token in the last layer to avoid the issue of differing tokenization. Since these representations are used for the next token prediction, we argue that they have similar meaning across models. Other solutions to this issue based on aligning differing tokenizations were proposed (Liu et al., 2019; Clark et al., 2019), which enable more fine-grained comparisons at increased computational cost.

## 3 Experiment

Data.As a first step, we use two datasets from different domains. Winogrande (Sakaguchi et al., 2020) is a benchmark aimed at measuring commonsense reasoning abilities by asking a model to fill in a blank in a sentence with binary options. We use the Winogrande validation set. Further, we use HumanEval (Chen et al., 2021), a code generation benchmark. Here, prompts consist of a comment that describes the functionality of code that should be generated. We always feed data in without additional examples (zero-shot). We show the prompt styles in Appendix B.

Models.We use a set of 11 freely available LLMs with roughly 7B parameters: RedPajama (Together.ai, 2023), Bloom (BigScience Workshop et al., 2023), Falcon (Penedo et al., 2023), Galactica (Taylor et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), Llama (Touvron et al., 2023), MPT (MosaicML, 2023), OpenLlama (Geng and Liu, 2023), OPT (Zhang et al., 2022), Pythia (Biderman et al., 2023), and StableLM Alpha (StabilityAI, 2023). All models are base models without instruction finetunining. For the code data, we add CodeLlama and CodeLlama-Python (Roziere et al., 2023), which are specifically trained on code. Except for Llama, we use the weights published on Hugging Face.

### Results

In Figure 1, we report representational similarity with OT and IS invariance. Results are similar with OT, TR, and IS invariance, which we show in Appendix C due to space constraints.

Significant differences between models.On both datasets, some models have significant differences. On Winogrande, for example, Falcon stands out for a relatively low similarity by Orthogonal Procrustes and Jaccard similarity. Similarly, StableLM Alpha seems to be relatively dissimilar to all other models. On HumanEval, OpenLlama and 

**Significant differences between similarity measures.** Again looking at Falcon on Winogrande, this model stands out as dissimilar only with two of the four similarity measures. For both Aligned Cosine Similarity and Norm RSM-Diff, differences to other models are less pronounced. For these measures, GPT-J and OPT seem more dissimilar instead. These differences between measures occur despite them sharing the same invariances. It seems that while these measures share the same (high-level) view on representational similarity, finer-grained differences have substantial influence. Although Ding et al. (2021) report a similar result for CKA and Orthogonal Procrustes, our results show this is a more wide-spread issue. Few dimensions with high mean and variance may mask differences in all but these few dimensions for cosine similarity-based measures (Timkey and van Schijndel, 2021).

**Similarity is application-dependent.** Similarity for one task does not imply similarity for another task: while GPT-J has a similar Orthogonal Procrustes score to the other models on Winogrande, it stands out as dissimilar on HumanEval. This difference is expected to some degree as not all models had similar amounts of code in their training data, but still highlights that data dependency is an important factor when making claims about similarity of LLMs.

Additionally, the measures with discrepancies are not the same across datasets: while Orthogonal Procrustes and Norm RSM-Diff differ on Winogrande, they show highly similar patterns for HumanEval. The average Spearman correlation between heatmaps on Winogrande versus HumanEval is only 0.34.

**Difficulty of interpretation.** Scores of measures like Orthogonal Procrustes and Norm RSM-Diff that do not have a clear upper bound are difficult to interpret. In the absence of an interpretable scale, it is unclear whether uniform scores imply all models are equally similar or equally dissimilar.

## 4 Conclusions

We demonstrate measuring representational similarity of LLMs using a set of 7B parameter models. Representations do not seem to be universal, which may limit generality of study of any single LLM, but boost abilities to detect specific models. We identify several challenges of using representational similarity measures for measuring LLM similarity: discrepancies between measures, task-dependency, and interpretation. These challenges provide interesting avenues for future work.

Figure 1: Representational similarity on Winogrande (top) and HumanEval (bottom) with OT and IS invariance. Bright colors show the most similar models, dark colors the most dissimilar ones.