# Towards Higher Ranks via Adversarial Weight Pruning

Yuchuan Tian\({}^{1}\), Hanting Chen\({}^{2}\), Tianyu Guo\({}^{2}\), Chao Xu\({}^{1}\), Yunhe Wang\({}^{2}\)

\({}^{1}\) National Key Lab of General AI, School of Intelligence Science and Technology, Peking University.

\({}^{2}\) Huawei Noah's Ark Lab.

tianyc@stu.pku.edu.cn, {chenhanting,tianyu.guo,yunhe.wang}@huawei.com, xuchao@cis.pku.edu.cn

Corresponding Author.

###### Abstract

Convolutional Neural Networks (CNNs) are hard to deploy on edge devices due to its high computation and storage complexities. As a common practice for model compression, network pruning consists of two major categories: unstructured and structured pruning, where unstructured pruning constantly performs better. However, unstructured pruning presents a structured pattern at high pruning rates, which limits its performance. To this end, we propose a Rank-based PruninG (RPG) method to maintain the ranks of sparse weights in an adversarial manner. In each step, we minimize the low-rank approximation error for the weight matrices using singular value decomposition, and maximize their distance by pushing the weight matrices away from its low rank approximation. This rank-based optimization objective guides sparse weights towards a high-rank topology. The proposed method is conducted in a gradual pruning fashion to stabilize the change of rank during training. Experimental results on various datasets and different tasks demonstrate the effectiveness of our algorithm in high sparsity. The proposed RPG outperforms the state-of-the-art performance by 1.13% top-1 accuracy on ImageNet in ResNet-50 with 98% sparsity. The codes are available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Pruning/RPG and https://gitee.com/mindspore/models/tree/master/research/cv/RPG.

## 1 Introduction

As Convolutional Neural Networks (CNNs) are adapted to various tasks at better performance, their sizes also explode accordingly. From shallow CNNs like LeNet , larger CNNs like AlexNet , to deeper modern CNNs like ResNets  and DenseNets , CNNs are growing larger for more complex tasks and representations, including large-scale image classification and downstream tasks like object detection , segmentation , etc. The evolution of CNN gives rise to various real-world applications, such as autonomous driving , camera image processing , optical character recognition , and facial recognition . However, it is difficult to deploy large CNN models on mobile devices since they require heavy storage and computation. For example, deploying a ResNet-50  model costs 8.2G FLOPs for processing a single image with \(224 224\) size, which is unaffordable for edge devices with limited computing power such as cellphones and drones.

In order to compress heavy deep models, various methodologies have been proposed, including Weight quantization [19; 56], knowledge distillation [24; 44], and network pruning. Network pruning prunes the redundant weights in convolutional neural networks to shrink models. Weight (or unstructured) pruning  and filter (or structured) pruning  are two main pathways to prune CNNs. Weightpruning sparsifies dense kernel weight tensors in convolutional layers in an unstructured manner including iterative pruning , gradual pruning [60; 17], and iterative rewinding [15; 16; 43]. Some other works [31; 53; 48] propose gradient or hessian based weight saliencies that proved effective in certain scenarios. Filter pruning [37; 23; 35; 36; 51] prunes filters in convolutional layers as a whole, reducing the redundant width of network layers.

Although structured pruning algorithms can be well supported by existing hardwares and bring large runtime acceleration benefits, their performance is much lower than that of unstructured pruning. For example, SOTA unstructured pruning methods could achieve 80% sparsity on ResNet-50 with little performance drop [48; 45] while structured pruning could only reach less than 50% , since filter pruning is a subset of weight pruning by further imposing structural constraints. However, under circumstances of high sparsities, we observe that unstructured pruning partially degrade to structured pruning. When weights are with a large proportion of zeros, it is highly likely that a structured pattern appears, where a whole channel of filter is almost completely pruned. Therefore, existing weight pruning methods usually meet dramatic performance decay at high sparsities.

Inspired by the comparison of the two pruning categories, we propose to reduce structural patterns in weight pruning. Structured pruning is factually a reduction of weight rank in deep Convnets. Thus, rank could be adopted as a metric for evaluating the "structuredness" of unstructured sparse weights: a sparse weight is considered highly structured if it possesses low rank. To keep unstructured pruning from being too structured, we hope to maintain weight ranks under high sparsities in pruning. Based on the goal of rank improvement, we propose an adversarial Rank-based PruninG (RPG) approach for unstructured pruning. First, we find a low-rank approximation of the weight by minimizing the approximation error. The best low-rank approximation is found via singular value decomposition. Second, to enhance weight ranks, we maximize the distance between the weight and its low-rank counterpart to increase weight rank. This adversarial rank-based optimization objective guides sparse weights towards a high-rank topology. The proposed method is conducted in a gradual pruning fashion to stabilize the change of rank during training. The advantage of the proposed RPG method is evaluated through extensive experiments on image classification and downstream tasks, and Figure 1 demonstrates that our method gains a matrix rank advantage compared to baselines.

## 2 Maintaining Rank via Adversarial Pruning

### Problem Formulation

In conventional terms of supervised neural network learning, given a target loss function \(\), neural network weight \(W\), and input output pairs \(X=\{x_{i}\}_{i=1...n}\), \(Y=\{y_{i}\}_{i=1...n}\), weight training of a

Figure 1: Average weight matrix rank of ResNet-32  pruning baselines versus sparsity. Our rank-based method is effective in maintaining weight ranks at high sparsities compared with baselines.

Figure 2: An illustrative diagram of our Rank-based Pruning (RPG) method.

neural network \(W\) is formulated as:

\[*{arg\,min}_{W}(Y,WX),\] (2.1)

Weight pruning limits the total number of non-zero weights in weight \(W\); or mathematically, weight pruning imposes a \(l_{0}\)-norm constraint on neural network learning. Given sparsity budget \(c\), the constraint is described as:

\[\|W\|_{0} c,\] (2.2)

A common practice is to reparameterize weight \(W\) with the Hadamard elementwise product of a weight tensor \(W\) and a binary mask \(M\). The binary mask \(M\) has the same shape as \(W\), and each element in \(M\) represents whether its corresponding parameter in \(W\) is pruned. After reparametrization, the weight pruning problem is then formulated as:

\[*{arg\,min}_{W M}(Y,(W M)X)\|M\|_{0} c.\] (2.3)

In Equation (2.3), \(\) is the Hadamard elementwise product of matrices.

At high sparsities in unstructured pruning, the rank of sparse networks could decrease substantially. In the following sections, we will demonstrate the problem and propose a solution to maintain sparse weight ranks.

### Analyzing Weight Pruning in High Sparsity

Unstructured and structured pruning are two major pruning methodologies. In unstructured pruning practices, weight tensors of CNNs are pruned in a fine-grained manner: each and every solitary weight parameters could be turned off (_i.e._ set to zero) within the network, but the whole weight tensor structure is left unchanged. In contrast, structured pruning focuses on the pruning of filters: filters are cut-off as the smallest prunable unit in the pruning process. Comparing the two pruning paradigms under the same sparsity budget, Zhu and Gupta  illustrate that unstructured pruning performs much better than structured pruning under the same pruning budget.

This phenomenon could be explained from the perspective of matrix ranks. In fact, structured pruning is a direct rank reduce imposed on weight matrices, which means filter pruning is basically weight pruning with low rank. The rank of a matrix represents the upper bound of the amount of information contained in the matrix. A powerful network should be rich in information, and we hope features of the sparse network could have high ranks. Feature ranks is closely related to ranks of sparse weight matrices because of the formula below that describes the relationship of ranks in matrix multiplication:

\[(Y)=(WX)((W),(X)).\] (2.4)

According to Equation (2.4), when filter pruning is applied on weight \(W\) that directly impacts its rank, the rank of the output feature will also degrade, causing dramatic loss in information richness. On the other hand, unstructured pruning is free from the structural constraint of filter pruning, and thus maintain more amount of information.

However, under circumstances of high sparsities, we observe that unstructured pruning partially degrades to structured pruning. When weights are filled with a large proportion of zeros, it is very probably that some filters or channels are almost entirely turned-off: "quasi-structured" sparse weight pattern is then formed. A baseline evaluation of matrix ranks in Figure 1 illustrates this concern. Therefore, existing weight pruning methods usually meet dramatic performance decay at high sparsities. Inspired by the properties of the two categories of pruning, we propose to reduce the structured pattern in unstructured pruning, and therefore to maintain weight ranks under high sparsities.

### Low Rank Approximation through SVD

Now that weight ranks are important in weight pruning, we need a practical way to compute ranks in the context of deep neural networks. Previous deep learning works on ranks apply matrix rank theories to CNN low-rank tensor decomposition. In these works, low-rank approximations are proposed to fit weight tensors. Denil et al.  decomposites \(W^{m n}\) into the multiplication of \(U^{m r}\) and \(V^{r n}\) where \(r\) is much smaller than \(m\) and \(n\). \(UV\) provides a low-rank approximation (rank bounded by \(r\)) of weight \(W\). Denton et al.  uses the sum of \(k\) rank-one approximations to provide the \(k\)-rank approximation of a feature tensor. Zhang et al.  multiplies a low-rank matrix \(M\) to weight matrix \(W\), and solves the low-rank \(M\) with Singular Value Decomposition (SVD). In modern works [41; 7], low-rank approximations are widely studied as well.

Since the weight values are always discrete, as an alternative solution and inspired by low-rank approximation works, we converge to an approximated rank rather than compute a precise rank solution. Hence, we define the approximated rank as following:

**Definition 1** (\(\)-rank of a matrix).: _Given a matrix \(W\) and a small error tolerance \(>0\), the \(\)-rank of \(W\) is defined as the smallest positive integer \(k\) such that there exist a \(k\)-rank matrix, whose \(l_{2}\) distance to \(W\) is smaller than \(\)._

In previous works, ranks are evaluated via singular values computed from Singular Value Decomposition (SVD). Zhang et al.  uses the sum of the top-k PCA eigenvalues to approximate ranks of layer responses; Lin et al.  defines rank as the number of non-negligible singular values and does SVD analysis on feature maps; Shu et al.  performs SVD on attention maps and augment model performance by keeping a fatter tail in the singular value distribution. These discoveries all acknowledge that singular values from SVD estimates ranks of matrices. We also leverage SVD to compute \(\)-rank as defined in Definition 1. First, we illustrate that SVD could generate the best low-rank approximation:

**Theorem 1** (The best low-rank approximation).: _Suppose \(W\) is decomposed via SVD and yield \(W=_{i=1}^{r}_{i}u_{i}v_{i}^{T}\) where singular values \(\{_{i}\}\) are sorted in descending order. Given integer \(k<r\), the best \(k\)-rank approximation of \(W\), namely the \(k\)-rank matrix that has the smallest \(l_{2}\) distance to \(W\) is_

\[=_{i=1}^{k}_{i}u_{i}v_{i}^{T}.\]

The proof of Theorem 1 will be shown in Appendix. Since SVD could yield the best low-rank approximation, we could use this property to solve \(\)-rank defined in Definition 1. Given weight matrix \(W\), we search for the smallest \(k\) such that the \(l_{2}\) approximation error of best \(k\)-rank approximation \(\) as formulated in Theorem 1 is below the error tolerance \(\). In this way, we are able to solve the rank of \(W\).

### Adversarial Optimization for Rank Maintenance

Equipped with the method for matrix rank computation, we hope to formulate a target loss function according to this heuristic such that optimization of the loss could maintain weight ranks.

In contrast to low-rank approximations, high-rank matrices should be hard for low-rank matrices to approximate. Assume \(S\) is the set of all low-rank matrices, \(W\) should keep its distance away from this set \(S\) to increase its rank. But this is a hard problem, for we have to figure out all low-rank matrices. To further simplify the problem, we find the best low-rank approximation rather than all low-rank approximations. \(W\) should estrange itself from the best low-rank approximation whose distance is the farthest from \(W\). This simplification is valid and will be proved later.

Using this heuristic as motivation, we design an adversarial mechanism that increase the difficulty for \(W\) to be approximated by low-rank matrices, and consequently to advocate higher matrix ranks of \(W\) while pruning. At first, the best low-rank approximation \(\) of a small rank \(k\) is generated via Singular Value Decomposition, for the purpose of minimizing its distance to weight \(W\); next, \(W\) is optimized to increase the distance from \(\). The procedures could be understood as an adversarial combat between \(W\) and \(\): as the low-rank \(\) tries to fit \(W\), \(W\) is optimized to keep itself far away from \(\). Mathematically, the combat could be expressed as a min-max problem.

But unluckily, the problem may suffer the risk of not getting converged. When \(\) is fixed, the best \(W\) is taken when \(W\). To resolve this issue during optimization, we constrain \(W\) within a euclidean norm ball. In other words, we plug \(}\) instead of \(W\) into the max-min problem. The reasons we use \(l_{2}\) normalization are: 1. \(W\) is bounded rather than growing to infinity; 2. the rank of \(W\) could increase if we \(l_{2}\) normalize \(W\) when optimizing the min-max problem, which will be shown in the mathematical proof in the appendix; 3. \(l_{2}\) normalization on weight is equivalent to imposing \(l_{2}\) normalization on its singular values, providing a fair standard for rank comparisons based on the definition of rank in Definition 1 given fixed error tolerance.

Before the introduction of this min-max problem, we introduce several notations: \(\|\|_{F}\) is the Frobenius norm (2-norm) of matrices; \(I\) is the identity matrix; \(:=\) is the \(l_{2}\) normalized weight matrix \(W\); \(U\), \(\), \(V\) are matrices reached from the SVD of \(\), where \(U=\{u_{1},u_{2},...\}\) and \(V=\{v_{1},v_{2},...\}\) are orthonormal bases; \(\) is a diagonal matrix where singular values \(\{_{1},_{2},...\}\) are sorted in descending order on the diagonal; operator \((U V^{T})=_{i=1}^{k}_{i}u_{i}v_{i}^{T}\) stands for \(k\)-rank truncated SVD, or the \(k\)-rank best approximation of \(\).

Then formally, we express the min-max problem as follows:

\[_{W}_{U,,V}-&\| {W}-(U V^{T})\|_{F}^{2},\\ & U^{T}U=I, V^{T}V=I, {W}=.\] (2.5)

The optimization target is defined as the adversarial rank loss:

\[_{rank}=-\|-(U V^{T})\|_ {F}^{2}.\] (2.6)

In deep learning, gradient descent is the most widely applied method for optimization problems, and we also adopt gradient descent for our experiments. Hence in this context, we propose the following theorem, stating that our adversarial rank loss could guide weight \(W\) towards higher rank:

**Theorem 2** (Effectiveness of the adversarial rank loss).: _Given the adversarial rank loss as defined in Equation (2.6). If we optimize \(W\) in rank loss via gradient descent, the rank of \(W\) will increase._

The theorem could be mathematically proved, and the detailed proof will be provided in the appendix.

With the proposed adversarial rank loss, our optimization objective consists of two goals: 1. we hope to reduce the loss for a certain task (_e.g._ classification, detection, etc.) for good sparse network performance; 2. we hope to reduce rank loss for higher weight ranks. We formulate the Rank-based Pruning objective by doing affine combination of the two goals. Given affine hyperparmeter \(\), the loss for a certain task \(_{task}\), the adversarial rank loss \(_{rank}\), the Rank-based Pruning (RPG) objective \(\) is defined as:

\[:=_{task}+_{rank}.\] (2.7)

### The Gradual Pruning Framework

Previous works have proposed various pruning framework, including One-shot Pruning , Sparse-to-sparse Pruning , and Iterative Magnitude Pruning for Lottery Tickets . Compared with these frameworks, Gradual Pruning (GP)  could reach better performance with modest training budget. We adopt Gradual Pruning as the pruning framework, which is a usual practice in many works . GP prunes a small portion of weights once every \( T\) training steps, trying to maintain sparse network performance via iterative "pruning and training" procedures.

However, it is hard to associate rank loss with Gradual Pruning; we hope the factor of rank could be considered in the choice of weights via the proposed rank loss. Loss gradients are widely-applied weight saliency criteria, because gradient magnitudes reflect the potential importance of pruned weights: if a turned-off weight possesses large gradients with respect to the objective loss function, it is expected for significant contributions to loss reduction . We use periodic gradient-based weight grow similar to previous pruning works , i.e. the weights are periodicly grown at each binary mask update step. But differently, the rank-based pruning objective (defined as Equation (2.7)) is used for gradients computation with respect to each model weight in our case. In this way, the rank factor is considered during the selection of active weights: there is a tendency that RPG chooses an active set of weights that features high-rank.

An embedded benefit of periodic gradient-based weight grow lies in computation cost considerations. Singular Value Decomposition (SVD) that is essential for rank computation is costly for large weight tensors. Calculating rank loss for each optimization step is hardly affordable. The adoption of periodic weight updating, however, amortizes the cost of rank loss computations. We also provide an SVD overhead analysis in Sec 3.6.

In summary, Our Rank-based Pruning (RPG) method is formulated as follows: once every \( T\) training steps, the prune-and-grow procedures that updates binary mask \(M\) is performed. Firstly, we plan the number of parameters to prune and to grow, such that after mask updating, the whole network will reach the target sparsity at the current iteration. Target sparsity will increase gradually as training goes on, which is identical to GP. Secondly, we globally sort all parameters based on magnitude and perform the pruning operation. Thirdly, we grow the parameters based on gradient. For other training steps, mask \(M\) is left unchanged; the active weight values are updated.

Specifically, HRank  also leverages matrix rank evaluations in pruning. Our idea is significantly different from HRank  in the following aspects: 1. HRank performs filter pruning while our work focuses on weight pruning; 2. HRank evaluates ranks of feature maps, but we evaluate ranks of weight tensors; 3. HRank uses feature rank as filter saliency; our work uses weight rank to guide the update of a sparse network topology.

## 3 Experiments

Our Rank-based PruninG (RPG) method is evaluated on several behchmarks and proved outstanding among recent unstructured pruning baselines. This section presents the experiment results to empirically prove the effectiveness of our RPG method, especially on high sparsities. First, we will show the results of RPG on two image classification datasets: the comparatively small-scaled CIFAR-10, and the large-scaled ImageNet. Then, we will present the results of RPG on downstream vision tasks. Finally, an ablation study will be given.

### CIFAR Experiments

**Experiment settings.** We first compare our RPG pruning method with other methods on CIFAR-10 classification. CIFAR-10 is one of the most widely used benchmark for image classification. It consists of 60000 \(32 32\) images: 50000 for training, and 10000 for validation. We hope to try our RPG method first on this relatively small dataset and look for heuristic patterns.

Among the pruning baselines, we choose ProbMask  and AC/DC for comparison because these two methods are intended for high-sparsity pruning. Additionally, ProbMask is a recent baseline that provides both CIFAR and ImageNet classification results, enabling us for further investigation on larger-scale datasets. Other baselines including PBW  and MLPrune  are earlier conventional pruning baselines for references. For fair comparison, our RPG method is applied to modern CNN structures, _i.e._ VGG-19 and ResNet-32, and prune for 300 epochs, according to the setting of ProbMask . The results are shown in Table 1.

**Results analysis.** At relatively low sparsities, the gap between recent baselines are small. ProbMask , AC/DC , and RPG all give satisfactory results at \(99\%\) compared with early pruning works. But as sparsity further increases, the three methods undergo significant performance decay on

   Models &  &  \\  Sparsity & 99\% & 99.5\% & 99.9\% & 99\% & 99.5\% & 99.9\% \\  Dense & 93.84 &  \\  PBW  & 90.89 & 10.00 & 10.00 & 77.03 & 73.03 & 38.64 \\ MLPrune  & 91.44 & 88.18 & 65.38 & 76.88 & 67.66 & 36.09 \\ ProbMask  & 93.38 & 92.65 & 89.79 & 91.79 & 89.34 & 76.87 \\ AC/DC  & 93.35 & 80.38 & 78.91 & **91.97** & 88.91 & 85.07 \\ RPG (Ours) & **93.62** & **93.13** & **90.49** & 91.61 & **91.14** & **89.36** \\   

Table 1: Sparsified VGG-19 and ResNet-32 on CIFAR-10. Baseline results are obtained from .

either network. At \(99.5\%\) and \(99.9\%\), our RPG method shows great advantage over the other two baselines. This discovery inspires us further investigate the high-sparsity potential of RPG on the large-scale ImageNet dataset.

### ResNet-50 on ImageNet

**Experiment settings.** Sparse ResNet-50 networks evaluated on the ImageNet dataset are the most commonly-used and recognized weight pruning benchmarks. ImageNet ISLVRC2012  is a large scale image classification dataset. It contains 1281K images in the training set and 50K images in the validation set. All the images are shaped \(224 224\) and distributed in 1000 classes. ResNet-50  is a medium-size canonical CNN with 25.5M parameters and 8.2G FLOPs, designed for ImageNet classification.

Our RPG method is applied on ResNet-50 under high sparsities: \(80\%\), \(90\%\), \(95\%\), and \(98\%\). We compare RPG with recent baselines. Among the baselines, STR  automatically learns pruning sparsity; WoodFisher , GraNet  and ProbMask  are methods based on gradual pruning; AC/DC  and ProbMask  are baselines targeted at high sparsities; PowerPropagation  is an improvement of Top-KAST  that relies on a pre-set layerwise sparsity distribution. For fair comparison, all results are 100-epoch baselines; we used standard ImageNet configs, detailed in the Appendix. The results are presented in Table 2. The advantage of adversarial rank-based pruning is manifested at high sparsities.

**Results analysis.** Our method could achieve outstanding performance for sparsities \(90\%\), \(95\%\), and \(98\%\). At lower sparsities (_e.g._\(80\%\), \(90\%\)), WoodFisher  takes the lead among the baselines. Our RPG method is slightly lower than WoodFisher  by \(0.07\%\) in ImageNet accuracy at \(80\%\) sparsity. At higher sparsities, our method outcompetes other baselines. Other competitive baselines at high sparsities include PowerPropagation  and AC/DC . However, the gap between our RPG method and these baselines widened at high sparsities. Specifically, our method outperforms current top baseline by \(1.13\%\) of ImageNet Top-1 accuracy at \(98\%\) sparsity.

Erdos-Renyi-Kernel (ERK)  is a layerwise sparsity distribution that is commonly used for performance boosting in weight pruning methods that require a pre-set sparsity distribution. However, ERK-based sparse models are computationally costly. Differently, RPG automatically maintains a more balanced sparsity throughout the whole network under the same total sparsity constraint. Though our sparse model slightly lags behind the current ERK variant of SOTA  under lower sparsities in certain accuracy, it is much cost-effective. Quantitatively, for \(80\%\) sparse ResNet-50, the reported ERK-based State-of-the-Art ImageNet accuracy is merely \(0.10\%\) higher than our RPG method (reaching \(76.76\%\) for ), but costing an extra \(58\%\) of FLOPs. The advantage of our RPG method over ERK-based methods is clearly illustrated in Figure 3, where we compare RPG with the ERK variant of TOP-KAST  and the State-of-the-Art PowerPropagation .

DeepSparse  is a recent sparse acceration framework on CPU that makes unstructured-sparse network accerlation possible in applications. We time sparse ResNet-50 on DeepSparse for single-image inference. Results in Table 3 shows that highly-sparse ResNet-50 could achieve around

   Algorithm &  \\  ResNet-50  & 0 & 76.80 \\  STR  & 0.8 & 76.19 \\ WoodFisher  & 0.8 & **76.73** \\ GraNet  & 0.8 & 76.00 \\ AC/DC  & 0.8 & 76.30 \\ PowerPropagation  & 0.8 & 76.24 \\ RPG (Ours) & 0.8 & 76.66 \\  STR  & 0.9 & 74.31 \\ WoodFisher  & 0.9 & 75.26 \\ GraNet  & 0.9 & 74.50 \\ AC/DC  & 0.9 & 75.03 \\ ProbMask  & 0.9 & 74.68 \\ PowerPropagation  & 0.9 & 75.23 \\ RPG (Ours) & 0.9 & **75.80** \\  STR  & 0.95 & 70.40 \\ WoodFisher  & 0.95 & 72.16 \\ AC/DC  & 0.95 & 73.14 \\ ProbMask  & 0.95 & 71.50 \\ PowerPropagation  & 0.95 & 73.25 \\ RPG (Ours) & 0.95 & **74.05** \\  STR  & 0.98 & 62.84 \\ WoodFisher  & 0.98 & 65.55 \\ AC/DC  & 0.98 & 68.44 \\ ProbMask  & 0.98 & 66.83 \\ PowerPropagation  & 0.98 & 68.00 \\ RPG (Ours) & 0.98 & **69.57** \\   

Table 2: Sparsified ResNet-50 on ImageNet. All results are official reports from the original works. Best and second best results are **bolded** and underlined.

\(2\) acceleration on CPU. This observation reveals that highly unstructured-sparse networks have promising applicative prospects on edge devices that could not afford power and cost-intensive GPUs, e.g. micro robots, wearable devices, et cetera. These devices feature limited memory and power, but high inference speed demands. In this sense, our RPG unstructured pruning method is of great application value.

### Downstream Vision Tasks

We also test our weight pruning method on downstream vision tasks. Mask R-CNN  is a widely used benchmark for conventional downstream tasks, namely, object detection and instance segmentation. We try to apply our weight pruning method to Mask R-CNN and compare its detection and segmentation performance against other pruning baselines. As for the choice of baselines, we found that limited weight pruning works conducted experiments on downstream vision tasks. We choose the following baselines for comparison: RigL  is a commonly used sparse-to-sparse baseline. AC/DC  is good at high-sparsity pruning on ImageNet classification. All methods are applied on Mask R-CNN ResNet-50 FPN variants to measure the mAP for bounding boxes and segmentation masks.

For all Mask R-CNN experiments, we follow the official training of COCO \(1\): pruning and finetuning lasts for 90K iterations in total. The pruning results evaluated on COCO val2017 are illustrated in Table 4. Similar to the trend in classification experiments, our RPG method gains an advantage at high sparsities compared with AC/DC . As sparsity increases from \(70\%\) to \(80\%\), the gap between AC/DC and RPG widens from \(1.0\) to nearly \(2.0\) for both detection and segmentation mAPs. This finding shows that RPG is a weight pruning method that could be generalized to various vision tasks: it always works well at high sparsities without the need for significant modifications.

### Vision Transformers

Recent works on vision model architectures focus on transformers [13; 52]. Transformer architecture models are proven particularly effective on large-scale image recognition tasks and are well applied

   Algorithm & Sp. & BoxAP & MaskAP \\  Mask R-CNN & 0 & 38.6 & 35.2 \\  RigL  & 0.5 & 36.4 & 32.8 \\ AC/DC  & 0.5 & **37.9** & **34.6** \\ RPG (Ours) & 0.5 & 37.7 & 34.4 \\  RigL  & 0.7 & 32.3 & 29.1 \\ AC/DC  & 0.7 & 36.6 & 33.5 \\ RPG (Ours) & 0.7 & **37.6** & **34.4** \\  RigL  & 0.8 & 26.0 & 23.7 \\ AC/DC  & 0.8 & 34.9 & 32.1 \\ RPG (Ours) & 0.8 & **37.1** & **33.8** \\   

Table 3: Sparse acceleration of sparse ResNet-50 on DeepSparse. Unstructured pruning could bring \(\) acceleration effects on CPU at high sparsities.

Figure 3: ImageNet accuracy versus FLOPs on sparse ResNet-50. Our method achieves better Accuracy-FLOPs trade-off compared with competitive pruning baselines, especially at high sparsities.

to various downstream tasks [4; 58; 5], but they are still struggling for industrial applications due to large model size and computation cost. To address these problems, works like SViT-E  attempted to apply unstructured pruning on vision transformers.

Though our method is not specifically designed for models with the attention mechanism, we explore the effect of our weight pruning method on DeiT-S  and compare it with high-sparsity weight pruning baseline  and the transformer pruning baseline  in Table 5. For fair comparison, all pruning experiments follow the setting of SViT-E : the DeiT-S model is pruned for 600 epochs on ImageNet . All other settings are identical the official training setting of , including batchsize, learning rate, etc.

### Ablations

In this section, we inspect the effect of rank loss. The rank-based pruning objective involves an affine parameter \(\) that controls the amount of rank loss with respect to the original task loss. When \(=0\), rank loss is turned off. Investigating the relations of rank versus \(\) and accuracy versus \(\) on a ResNet-32 of 99.5% sparsity as shown in Figure 4, we found rank loss could significantly increase the average rank throughout all layers of the sparse network. A substantial increase of accuracy is also observed. But as \(\) further increases, the average rank will be saturated. Reversely, as \(\) further increases, the classification accuracy will decrease. This could be attributed to the property of affine combination in Equation (2.7). When \(\) is large, the pruning objective will pay too much attention to maintain weight ranks and neglect the goal of performing the task well. Hence, it is necessary to tune \(\) and find the most optimal one.

### Overhead Analysis

As introduced in Section 2.4, RPG involves costly SVD calculations. However, we conduct experiments and illustrate that SVD accounts for very minimal cost overhead during pruning in terms of both time and FLOPs. As shown in Table 6, the overall time and FLOPs for SVD calculations only accounts for \(<2\%\) of the whole RPG pruning cost. We also compare the FLOPs overhead of RPG with other pruning methods. Observing from Table 7, our method is the most cost-effective compared with baselines. Above all, the extra overhead brought by rank loss calculations is not a concern.

## 4 Conclusion

This paper proposes the Rank-based Pruning (RPG) method. We investigate weight pruning from a matrix rank perspective, and yield the observation that higher ranks of sparse weight tensors could yield better performance. Based on this heuristic, the adversarial rank loss is designed as the optimization objective that guides the mask update process during pruning. In this manner, our method prunes weight tensors in a rank-favorable fashion. Our RPG method is experimented on various settings and outperforms various baselines.

   Type & Time & FLOPs \\  SVD & 16.5min & 5.07e15 \\ RPG90\% & 1003min & 1.34e18 \\   

Table 6: SVD overhead compared with the overall pruning & finetuning cost of RPG on 90% sparse ResNet-50.

Figure 4: Average weight matrix rank of ResNet-32  versus affine hyperparameter \(\). Accuracies on CIFAR-10 are marked.

   Baseline & Sparsity & Train FLOPs \\  ResNet-50 & (Dense) & 3.14e18 \\  AC/DC & 0.9 & 0.58\(\) \\ PowerProp. & 0.9 & 0.49\(\) \\ RPG(Ours) & 0.9 & **0.43\(\)** \\   

Table 7: Training FLOPs comparison with pruning baselines on sparse ResNet-50.

## 5 Limitations

Unstructured pruning has limited acceleration effect on GPU devices. New GPU architectures or GPU sparse acceleration supports are needed to exert the speed potential of our unstructured pruning method on GPUs.