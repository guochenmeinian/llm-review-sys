# Hydra: Bidirectional State Space Models

Through Generalized Matrix Mixers

Sukjun Hwang\({}^{*,1}\), Aakash Lahoti\({}^{*,1}\), Ratish Puduppully\({}^{,2}\), Tri Dao\({}^{3}\), and Albert Gu\({}^{1,4}\)

\({}^{1}\)Machine Learning Department, Carnegie Mellon University

\({}^{2}\)IT University of Copenhagen

\({}^{3}\)Department of Computer Science, Princeton University

\({}^{4}\)Cartesia AI

{sukjunh,alahoti}@cs.cmu.edu, rapu@itu.dk, tri@tridao.me, agu@cs.cmu.edu

Equal Contributions.Work done at A*STAR, Singapore.

###### Abstract

A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating sequence mixer and channel mixer layers. This paper studies a unifying _matrix mixer_ view of sequence mixers that can be conceptualized as a linear map on the input sequence. This framework encompasses a broad range of well-known sequence models, including the self-attention of Transformers as well as recent strong alternatives such as structured state space models (SSMs), and allows understanding downstream characteristics such as efficiency and expressivity through properties of their structured matrix class. We identify a key axis of matrix parameterizations termed _sequence alignment_, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent SSMs such as Mamba. Furthermore, the matrix mixer framework offers a systematic approach to developing sequence mixers with desired properties, allowing us to develop several new sub-quadratic sequence models. In particular, we propose a natural bidirectional extension of the Mamba model (**Hydra**), parameterized as a _quasiseparable matrix mixer_, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks. As a drop-in replacement for attention layers, Hydra outperforms BERT by \(0.8\) points on the GLUE benchmark and ViT by \(2\%\) Top-1 accuracy on ImageNet.

## 1 Introduction

Large-scale pretrained models such as GPT , BERT , and ViT  exhibit state-of-the-art performance across a wide range of tasks in multiple domains, including language and vision. A large number of these pretrained models follow a multi-layer architectural blueprint: a _sequence mixer_, such as Self-Attention1, aggregates information across the input sequence, followed by a _channel mixer_ processes information within each sequence element. Over the years, Attention has been the predominant choice for sequence mixing due to its ability to facilitate direct pairwise interactions between elements of the input sequence in a single step. However, this capability incurs a quadratic cost with respect to sequence length, making the training and deployment of these models prohibitively expensive for longer sequences. Although numerous alternatives have been proposed, designing principled sequence models that match the performance and versatility of attention-based systems remains a substantial challenge.

One general strategy in designing alternative sequence models involves substituting the Attention matrix with different matrix parameterizations as the core sequence mixer. These modifications are motivated by various goals. For instance, simplifying the sequence mixer has led to the development of models such as MLP-Mixer , which uses dense matrices, and FNet , which utilizes the DiscreteFourier Transform matrix. Additionally, incorporating inductive biases such as positional information has resulted in the use of Toeplitz matrices in models like TNN . Enhancing computational efficiency has spurred the creation of low-rank structures in models like Linear Attention (LA)  and Linformer , as well as Monarch matrices  in models such as M2 .

Despite the achievements of these approaches, they often lack a systematic analysis of their theoretical foundations and empirical consequences. Moreover, these models typically exhibit lower empirical performance than Attention, which is successfully applied across diverse domains and tasks. In another line of work, structured state space models (SSMs) [17; 18] such as Mamba  have been popularized for its attention-like performance with linear-time computational scaling. Furthermore, recent work  has shown that SSMs can also be expressed as semiseparable matrices. However, these models are primarily causal as underscored by their recurrent view of computation, limiting their application to auto-regressive settings. Several concurrent efforts to adapt Mamba into a bidirectional model [48; 38] have been made, but these attempts remain ad-hoc.

In this work, we study the **matrix mixer** framework (see Figure 1), which provides a powerful abstraction for enhancing the understanding of sequence models through the analysis of their matrix structures. Specifically, we suggest the following:

1. **Formalization of matrix mixer sequence models.** We establish the conceptual foundation of the Matrix Mixer sequence models, delineating key axes of matrix parameterizations that influence model characteristics such as efficiency, causality, and expressivity. For instance, we highlight that structured matrix  parameterizations underpin efficient sequence models through computationally efficient algorithms (Section 2.1).
2. **Introduction of sequence alignment.** Within this framework, we define _sequence alignment_ as a novel attribute, which induces sequence models with essential features of _data-dependence_ and _extendability_. We introduce _Sequence Aligned Matrices_ (SAM) that exhibit these properties, demonstrating their superior performance in downstream tasks compared to non-aligned counterparts (Section 2.2).
3. **Exploration of structured matrix parameterizations.** Through the matrix mixer framework, we systematically explore and categorize a broad spectrum of existing sequence models (Section 2.3). Motivated by sequence alignment, we also present new sequence models with underexplored structured matrix configurations, such as Vandermonde and Cauchy matrices (Section 2.4).

Building on our matrix mixer framework, we introduce a novel sequence model **Hydra**, which employs a _quasiseparable_ matrix mixer (Section 3). Quasiseparable matrices are a fundamental matrix structure with several important properties, making them ideal for sequence modeling. For example, they generalize both the low-rank matrix mixers found in linear attention models and the semiseparable matrices utilized in state space models. In fact, quasiseparable matrix mixers can be seen as a natural bidirectional extension of semiseparable matrices, addressing a major limitation in state space models by making their use in non-causal settings possible. Additionally, Hydra maintains the strong performance and linear-time computational efficiency of SSMs, thanks to structured matrix multiplication algorithms. Unlike prior attempts to make SSMs bidirectional by ad-hoc methods - typically by combining separate models for forward and backward sequence processing through element-wise addition [15; 48; 38], the Hadamard product , or concatenation [44; 12; 38] - the systematic approach of Hydra offers a more coherent and theoretically grounded advancement. Specifically, the definition of quasiseparable mixer matrices generalize both the heuristic bidirectional extensions of SSMs and linear attention, providing a mathematical interpretation of the strong expressivity exhibited by Hydra.

Figure 1: (Left) A schematic of the matrix mixer framework. (Right) An overview of matrix mixer classes: dense, Vandermonde, Toeplitz, low-rank, semiseparable, and quasiseparable.

We provide extensive experimental results that substantiate our claims. Our systematic ablation studies control architectural variables to highlight the impact of matrix parameterization. These careful experiments confirm that Sequence Alignment, a property we newly identified in certain matrix mixers, significantly enhances downstream performance. Furthermore, our experimental findings demonstrate that novel sequence models like those using Cauchy matrix mixers match the performance of established matrix mixers such as low-rank. This indicates that the strong performance of low-rank variants is not solely attributable to their matrix structure, but can be matched by other mixers that share similar properties. In addition, we also validate that the bidirectional extension of the Mampa model, implemented through quasiseparable matrix mixers, outperforms naive bidirectional approaches [15; 48; 38; 44; 12]. Importantly, Hydra excels as a performant, general-purpose bidirectional sequence model, as evidenced by its strong performance across diverse domains. It achieves state-of-the-art results on the GLUE benchmark  with an average accuracy of \(84.3\%\), outperforming BERT  by \(0.8\) points, and records a Top-1 accuracy of \(81.0\%\) on the ImageNet-1K benchmark , surpassing ViT  by \(2.2\) points.

We publicly release source code at https://github.com/goombalab/hydra.

## 2 The Matrix Mixer Framework:

Bridging Sequence Mixers and Structured Matrices

In Section 2.1, we formally define the matrix mixer framework, conceptualizing sequence mixers as linear maps on input sequences. In Section 2.2, we introduce sequence alignment, a new axis of variation of matrix structures which controls important characteristics of downstream sequence models such as data-dependent parameterizations and extendability. Section 2.3 leverages these definitions to categorize a wide array of previous works based on their matrix mixers, facilitating the understanding of sequence mixers through their matrix structures. Furthermore, in Section 2.4, we propose novel sequence mixers utilizing Vandermonde and Cauchy matrices, demonstrating the flexibility of our framework in systematically designing new sequence mixers.

### Formalizing the Matrix Mixer Framework

**Definition 2.1** (_The matrix mixer framework_).: _Let \(\!\!^{L C}\) be the input sequence, consisting of \(L\) elements, each with \(C\) channels. Let \(f_{X}\!:^{L C}\!\!^{L D}\) be the input preprocessing function that encapsulates common data transformations such as short convolutions, and linear layers. Let \(H\) and \(P\) be the number of heads and head dimension respectively, such that \(HP\!=\!D\). Let \(\!\!^{L L}\) represent the underlying class of mixer matrices. For each head \(h\!\![H]\), let \(f^{h}_{}\!:^{L C}\!\!\!\!\) be the matrix construction function that maps input data to mixer matrices, where \(\) is the space of learnable parameters. We denote \(^{h}\!=\!f^{h}_{}(\!,\!)\), when \(\!,\!\!,\!\) are clear from the context. Then, we define matrix mixing as,_

\[^{h}\!=\!^{h}(f_{X}())^{h},\]

_where \((f_{X}())^{h},^{h}\) denote the preprocessed input and output slice corresponding to head \(h\)._

This definition encapsulates the view that existing sequence mixers can be mathematically represented as \(L L\) mixer matrices that act on the sequence length (see Figure 1, left). The framework not only incorporates typical data preprocessing steps like projections [22; 39], and short convolutions [16; 6], but it also accommodates data dependent mixer matrices [39; 34]. Furthermore, it is also powerful enough to capture the concept of head structure  by equivalently sharing the mixer matrices within a head.

Moreover, Definition 2.1 offers a different lens to conceptualize the computational cost of sequence mixers. If we assume that both the preprocessing function and the matrix construction function are sub-quadratic in the sequence length \(L\), which is generally true in practice, then the computational bottleneck lies in the \(f_{X}()\) operation. For a general matrix \(\), this multiplication will incur a \(O(L^{2})\) cost. The only way to mitigate this to restrict \(\) to being a _structured matrix_, which are known to possess sub-quadratic matrix multiplication algorithms. We refer to such sequence mixers as _structured matrix mixers_. This paves the way to systematically develop new sequence mixers: select an appropriate class of structured matrices from established mathematical literature, devise a data-dependent matrix construction function, and integrate them into the matrix mixer framework.

### Sequence Aligned Matrices

Unstructured mixer matrices, also known as dense matrices, lack two key useful properties: 1) these matrix mixers cannot be easily made _data-dependent_, which has been identified as a key property of performant sequence models such as Transformers and Mampa, and 2) they can only be applied to fixed-length sequences, a property we call _extendability_. Due to substantial importance of these features for sequence mixers, we formally define the class _Sequence Aligned Matrices_ (SAM) to systematically explore matrices that are characterized with both properties.

**Definition 2.2** (_Sequence Aligned Matrices_).: _Let \(L\) be the sequence length and let \(\!\!^{L L}\) denote a matrix with a parameter set \(\). Then, we say that \(\) is a Sequence Aligned Matrix if there exists a partition \(\) of \(\!\!\), and \(}\!\!\), such that for all sets \(\!\!\), there exists a bijective map \(f_{}\!:\![L]\!\!\), and, for each \(i,j\!\![L]\), \(i\!\!j\), the sub-matrix \(\![i\!:\!j\!+\!1,\!i\!:\!j\!+\!1]\) is composed solely from the parameters in the subset \(_{,i k j}f_{}(k)\!\!\)._

In simpler terms, this implies that each parameter of a SAM is either mapped to a specific element of the sequence or is left data-independent, ensuring that every upper-left sub-matrix upto index \(i\) is constructed using only the parameters corresponding sequence segment upto and including index \(i\) and/or the data-independent ones.

**Proposition 2.3** (Data Dependency).: _Sequence aligned matrices exhibit canonical data-dependent parameterization._

Proof.: This property arises from the parameter partition structure guaranteed in the definition. Specifically, for each partition we associate a parametric function that, for any given element \(i\) computes the parameter's value by treating the element itself as an input. 

**Proposition 2.4** (Extendability).: _Sequence aligned sequence mixers can be extended beyond their trained length._

Proof.: This is a direct consequence of Proposition 2.3: the pretrained parametric functions assigned to each partition enable the computation of matrices larger than those encountered during training. 

We identify data dependency - the property induced by SAM - as a key axis of differentiation amongst existing models. Although data-dependency is a popular notion that is widely regarded as being crucial to performance of Attention, it lacked any formal definition in the literature. Consequently, various works have adopted different interpretations of this notion: Hyena  implements it via a data-dependent linear operators over the Toeplitz matrix mixer; GSS  adds a data-dependent gate post sequence mixing; LA, SSD [22; 6], like Attention, directly map input data to the parameters of the matrix mixer. We adopt the third notion of data dependency, where each parameter is a function of a particular input token, and under this definition it is clear that models like Hydra, SSD, and LA are data-dependent, whereas MLP-Mixer, FNet, S4, S4D, and TNN, only have data-independent parameters.

### Prior Sequence Models as (Structured) Matrix Mixers

Using the formalization of the Matrix Mixer framework, we categorize a wide array of previous works - MLP-Mixer , Transformer , Linear Attention , Linformer , S4 [18; 17], H3 , TNN , CKConv , FNet , Kaleidoscope [8; 7], Monarch [5; 13], and Mamba [16; 6] - as matrix mixers in Table 1. For illustrative purposes, we explicitly show that MLP-Mixer, FNet and LA are matrix mixers (proofs in Appendix B), and leave out normalizing factors for simplicity as follows:

**Proposition 2.5**.: _(MLP-Mixer is a matrix mixer). MLP-Mixer employs \(f_{X}\) as an identity function, and its mixer matrix \(\) has an unstructured parameterization with a single head (\(H\!=\!1\))._

   Matrix Structure \(\) & Formulation \((m_{ij})\) & Complexity & Sequence Aligned & Method Instantiations \\  Dense & \(m_{ij}\) & \(O(L^{2})\) & & MLP-Mixer  \\  Dense & \(_{j}(_{j}^{T}_{j})\) & \(O(L^{2})\) & ✓ & Transformer  \\  Low-Rank & \(_{i}^{T}_{j}\) & \(O(L)\) & ✓ & Linear Attention , \\  (Linear Attention) & \(_{i}^{T}_{j}\) & \(O(L\!)\) & ✓ & Linformer  \\  Butterfly & See [7; 5] & \(O(L\!\!L)\) & & Kaleidoscope [8; 7], \\  & & & Monarch [5; 13] \\  Toeplitz & \(m_{j-i}\) & \(O(L\!\!L)\) & & S4 [17; 18], H3  \\  (Convolution) & & & TNN , CKConv  \\  Discrete Fourier Transform & \(w^{ij}\) & \(O(L\!^{2}\!L)\) & & FNet  \\  Vandermonde & \((m_{i})^{j}\) & & & \\ Cauchy & \(_{d}(q_{q_{d}}\!-\!\!k_{jd})^{-1}\) & \(O(L\!^{2}\!L)\) & ✓ & Ours (Section 2.4) \\  Semiseparable & \(_{i}^{T}_{i,j}^{}_{j}_{\{i j \}}\) (1), (2) & \(O(L)\) & ✓ & Mamba (S6 , SSD ) \\  Quasiseparable & Equation (3) & \(O(L)\) & ✓ & Ours (Hydra) (Section 3) \\   

Table 1: Categorization of existing methods as matrix mixers. \(L\) denotes input sequence length.

[MISSING_PAGE_FAIL:5]

### Background: State Space Models are Semiseparable Matrix Mixers

SSD , the latest advancement in the iterations of SSMs, presents a sub-quadratic sequence mixer that attains language modeling performance on-par with Attention. Crucially, SSD underscores that all SSMs are inherently parameterized by semiseparable matrices, which play a pivotal role in their computational efficiency and strong empirical performance. Specifically, the operational essence of selective SSMs such as Mamba - the transformation of an input sequence \(\!\!^{L C}\!\!^{L  C}\) - can be succinctly represented within the matrix mixer framework, as detailed below:

\[_{t} =\!_{s=0}^{t}\!_{t}^{T}_{t:s}^{} _{s}_{s}\] (1) \[ =\!(,,)()\!=\! \] \[m_{ij} =\!_{i}^{T}_{i}\!\!_{j+1} _{j}\]

Here, \(m_{ij}\) represents an \((i,j)\)-element of the mixer matrix \(\!\!^{L L}\), with each matrix \(_{i}\!\!^{N N}\) and vector \(_{i},_{i}\!\!^{N 1}\) as time-varying parameters of selective SSMs. This formulation reveals that the mixer matrices \(\) follow a fundamental class of structured matrices known as semiseparable matrices, defined as follows:

**Definition 3.1** (The rank characterization of semiseparable matrices).: _A lower triangular matrix \(\) is \(N\)-semiseparable iff any submatrix from the lower triangle (on or below the diagonal) has a rank of at most \(N\). See Figure 2 (a)._

However, a key limitation of these matrices - and by extension, SSMs - is their inherent causality, which restricts their use in scenarios where bidirectional processing is vital. To circumvent this limitation, previous efforts [44; 15; 12] have explored employing two separate SSMs, one for forward and the other for backward sequence processing, then combine the outputs using strategies like element-wise addition, the Hadamard product, or concatenation. Among such heuristics, addition-based bidirectional extensions of SSMs [15; 48; 38] can be conceptualized within our matrix mixer framework, as illustrated in Figure 2 (c).

### Quasiseparable Matrices: A Principled Bidirectional Matrix Mixer

We fully utilize the matrix mixer framework, which is discussed in Section 2, to explore a novel bidirectional sequence mixer and identify quasiseparable matrices as a prime candidate. Our exploration focuses on structured matrix classes that meet the following criteria: 1) they feature upper triangular components for bidirectionality, 2) they possess strong expressivity, and 3) they benefit from sub-quadratic matrix multiplication algorithms.

The structural design of quasiseparable matrices inherently meets the first criterion, which is defined as follows: a matrix \(\) is \(N\)-quasiseparable if each element \(m_{ij}\) satisfies

\[m_{ij} =\!_{i}^{T}_{i:j}^{} _{j}}{_{i}},&i\!>\!j\\ _{i},&i\!=\!j\,,\\ _{i}^{T}_{i:j}^{}_{j},&i\!<\!j \] (3)

where each \(_{i}\) is a scalar, \(_{i},_{i}\!\!^{N 1}\), and \(_{i}\!\!^{N N}\). Clearly, this matrix class features non-zero upper triangular components, enabling bidirectional processing.

Figure 2: (a) A semiseparable (SS) matrix. (b) A quasiseparable (QS) matrix. (c) A mixer matrix of addition-based bidirectional SSMs. (d) A QS mixer matrix for Hydra. SS and QS matrices are characterized by rank conditions (Definition 3.1, Definition 3.2). The rank characterization of SS matrices include the diagonals (_e.g.,_ green submatrices), whereas that of QS matrices hold for off-diagonal submatrices (_e.g.,_ yellow submatrices). Because of the similar rank properties, a naive addition-based bidirectional SSM is provably a QS matrix mixer. Hence, QS matrix mixers generalize this common heuristic for bidirectional SSMs. The freedom in the diagonal values of Hydra leads to a higher expressivity compared to the mixer matrices of the addition-based bidirectional SSMs, where the diagonal values are constrained by the colored vectors.

Furthermore, the second requirement - the expressivity of quasiseparable matrices - is confirmed by their rank characterization:

**Definition 3.2** (The rank characterization of quasiseparable matrices ).: _A matrix \(\) is \(N\)-quasiseparable iff any submatrix from either the strictly upper or lower triangle (off from the diagonal) has a rank of at most \(N\). See Figure 2 (b)._

This definition emphasizes the rank constraint inherent in quasiseparable matrices, which is also evident from Equation (3) given that each vector \(_{i},_{i}^{N 1}\) and matrix \(_{i}^{N N}\) has a rank of at most \(N\). This structural flexibility of quasiseparable matrices directly leads to significant generalizations, extending the capabilities of both low-rank and semiseparable matrices.

**Corollary 3.3**.: _Quasiseparable matrices generalize low-rank matrices._

**Corollary 3.4**.: _Quasiseparable matrices generalize and extend semiseparable matrices._

Additionally, we revisit the previous addition-based bidirectional extensions of SSMs [15; 48; 38] through the lens of our matrix mixer framework. Unlike other elements, the diagonal values in a mixer matrix embody a unique concept of residuals, serving as a critical aspect of model expressivity. As demonstrated in Figure 2 (c), the mixer matrices in these bidirectional SSMs exhibit constraints in their diagonal elements \(\{_{i}^{T}}_{i}}+ _{i}^{T}}_{i}}\}_{L}\), which are directly influenced by the shared non-diagonal construction vectors \(\{_{i}^{T}},_{i}}, _{i}^{T}},_{i}}\}_{L}\). Importantly, the rank characterization of semiseparable matrices includes _on-diagonal_ elements, whereas that of quasiseparable matrices applies only to _off-diagonal_ submatrices. This generosity in the rank-based definition suggests that sequence models employing quasiseparable mixers not only offer inherent extendability in handling both causal and bidirectional processing, but also exhibit strong expressivity.

**Corollary 3.5**.: _Quasiseparable matrices are strictly more expressive than mixer matrices of addition-based bidirectional SSMs._

Leveraging this inherent flexibility of quasiseparable matrix mixers, our Hydra in Section 3.3 is defined by incorporating shift operations. Our experimental results strikingly confirm that this nuanced parameterization difference leads to a notable improvement in downstream task performance, thereby substantiating our theoretical claims (see Appendix D.1).

Moreover, with their structural similarity to semiseparable matrices, quasiseparable matrices are confirmed as sequence aligned matrices. Given our experimental results that SAM parameterizations are the key to the strong representational power (Section 4.1.1), we further validate our choice of quasiseparable matrices for the bidirectional sequence mixer.

**Proposition 3.6**.: \(N\)_-quasiseparable matrices are sequence aligned matrices._

Proof.: quasiseparable matrices, due to their structural similarity to semiseparable matrices, belong to the class of Sequence Aligned matrices. Specifically, the set of parameters is given by \(\!=\!}\!=\!\{_{i},_{i}, _{i},\!_{i}\}_{L}\). We consider the partition \(\!=\!\{\{_{i}\}_{L},\{_{i}\}_{L}\}_{L}\),\(\{_{i}\}_{L}\),\(\{_{i}\}_{L}\), and for each element of the partition set, we choose the bijection that maps token \(i\) to \(_{i}\), \(_{i}\), \(_{i}\), and \(_{i}\) respectively. Finally, it is easy to see that the sub-matrix \(M[:i+1,:i+1]\) indeed only contains parameters in the set \(\{_{j},_{j},_{j},\!_{j}\}_{i}\), thus satisfying the last requirement of SAM matrices. 

In Section 3.3, we detail how quasiseparable sequence mixer can be effectively implemented using existing SSM frameworks to achieve sub-quadratic multiplication efficiencies, fulfilling the final criterion of our matrix mixer exploration.

### Taming the Hydra

As a direct consequence of the favorable mathematical properties of quasiseparable matrices, we present a new sequence mixer **Hydra**. We adopt quasiseparable matrices as matrix mixers in Hydra, which bring forth three significant advantages: 1) higher representational power compared to its heuristic alternatives, 2) easy to implement sub-quadratic matrix multiplication algorithm, and 3) significant parameter savings.

We exploit the relationship between semiseparable and quasiseparable matrices to develop an easy-to-implement, sub-quadratic matrix multiplication algorithm. Specifically, we recognize that quasiseparable matrices can be expressed as a combination of two semiseparable matrices.

**Proposition 3.7**.: _Let \(^{L D}\) be the input sequence, and let \(QS()\) and \(SS()\) denote the action of a quasiseparable and semiseparable matrix respectively. Let the two matrices share the parameters\(\{_{i},_{i},_{i}\}_{L}\), and define \(=(_{1},,_{L})\), where \(_{i}\)'s are the diagonal parameters of the quasiseparable matrix. Then,_

\[QS()=(SS())+(((())))+,\]

_where flip\(()\) denotes the operation that reverses the input sequence, while shift\(()\) refers to shifting the sequence right by one position, padding the beginning with zero. (Proof in Appendix B)_

The above proposition demonstrates that quasiseparable matrix multiplication can be decomposed into two operations of semiseparable matrix multiplication with simple functions such as flip and shift. Given that semiseparable matrix structure encompasses SSMs, this flexibility allows for the selection of any SSM variant for implementation. In this paper, we employ SSD , chosen for its linear-time and dedicated hardware-efficient implementation. However, the architecture of Hydra is compatible with a variety of SSMs [18; 17; 16; 6] and can also be generalized with other recurrent models [47; 28].

Furthermore, Hydra significantly improves parameter over the heuristic approaches to bidirectional modeling using SSMs [15; 48; 44; 12]. For example, some approaches utilize two distinct SSMs, which doubles the number of training parameters. In contrast, since we conceptualize the model as a quasiseparable matrix mixer (see Figure 4), we naturally share the \(f_{X}\) projection layer, which accounts for a bulk of the model size. Empirically, we observe only a marginal increase in the total number of parameters compared to a single SSM, and can cut the the number of parameters nearly in half compared to the heuristic approaches to bidirectionality.

## 4 Experiments

In Section 4.1, we begin by analyzing the matrix mixer framework through extensive performance comparisons between different structured matrix classes (Section 4.1.1). The data-dependent parameterization of Quasiseparable matrices surpasses the performance of all other matrix classes, validating our selection of it as the sequence mixer. Furthermore, we compare our method against other ad-hoc solutions that extend SSMs to acquire bidirectionality (Appendix D.1), and show that our Hydra outperforms them, underscoring the utility of the matrix view of sequence mixers.

Then, in Section 4.2, we validate the effectiveness of our method by evaluating Hydra on quintessential language and image benchmark tasks: Masked Language Modeling (Section 4.2.1) and Image Classification (Section 4.2.2). State-of-the-art performance in both tasks has generally been dominated by transformer-based models [10; 11]. Our Hydra serves as an efficient and powerful replacement to the transformer layer, outperforming it on both tasks.

In the presentation of results across all tables, the highest performing scores are highlighted in **bold**, while the second-highest scores are marked with an underline. Each number is the average of five runs.

    &  &  &  &  &   \\  } \\  & & & \(_{ce}\) & Acc & MNLI & QNLI & QQP & RTE & SST2 & MRPC & COLA & STS & Avg \\  Dense & & 71M & 2.05 & 59.6 & 73.3 & 76.2 & 85.3 & 64.4 & 90.8 & 84.7 & 45.7 & 76.8 & 74.7 \\  Toeplitz & & 71M & 1.97 & 60.8 & 74.6 & 79.6 & 86.6 & 66.1 & 90.9 & 84.2 & 45.7 & 79.1 & 75.8 \\ Toeplitz & ✓ & 72M & 1.91 & 61.9 & 77.3 & 81.8 & 88.1 & 67.1 & 90.7 & 87.3 & 45.3 & 84.1 & 77.7 \\  DFT & & 71M & 2.46 & 53.1 & 70.4 & 70.8 & 84.5 & 59.9 & 89.8 & 83.6 & 44.4 & 69.8 & 71.7 \\ Vandermonde & & 71M & 2.46 & 53.0 & 55.2 & 61.3 & 82.5 & 66.3 & 87.4 & 84.2 & 45.8 & 84.2 & 70.8 \\ Vandermonde & ✓ & 70M & 2.04 & 59.7 & 74.1 & 80.0 & 86.2 & 67.9 & 89.3 & 84.3 & 46.0 & 80.1 & 76.0 \\  Cauchy & & 71M & 2.25 & 56.2 & 75.3 & 81.3 & 86.7 & 66.6 & 88.8 & 84.5 & 27.4 & 83.2 & 74.2 \\ Cauchy & ✓ & 70M & 1.94 & 61.6 & 77.5 & 84.4 & 84.2 & 68.0 & 91.0 & 86.7 & 48.1 & 85.2 & 78.2 \\  Low-Rank & & 71M & 2.06 & 59.4 & 73.7 & 76.5 & 85.1 & 61.5 & 90.6 & 85.8 & 49.2 & 76.6 & 74.9 \\ Low-Rank & ✓ & 70M & 1.90 & 62.2 & 77.6 & 84.1 & 88.2 & 69.1 & 91.0 & 85.9 & 47.6 & 83.9 & 78.4 \\  Attention & & 71M & 2.08 & 59.1 & 71.0 & 70.4 & 83.5 & 62.3 & 89.9 & 83.3 & **49.6** & 65.2 & 71.9 \\ Attention & ✓ & 70M & 1.87 & 62.9 & 78.5 & 85.4 & 88.4 & 67.9 & 91.2 & 86.4 & 47.8 & 84.3 & 78.8 \\  Quasiseparable & & 72M & 2.03 & 59.8 & 73.8 & 78.1 & 87.1 & 64.3 & 90.2 & 84.4 & 45.5 & 77.2 & 75.1 \\ Quasiseparable & ✓ & 71M & **1.84** & **63.3** & **79.5** & **85.5** & **88.6** & **69.8** & **91.9** & **88.4** & 48.4 & **85.6** & **79.7** \\   

Table 2: **Matrix mixer ablations. A systematic empirical study of matrix mixers on the GLUE benchmark by controlling for the architecture and varying only the matrix parameterization. Sequence-aligned matrices dynamically parameterize via input projections, becoming data-dependent (DD) that significantly increases performance. Most DD variants achieve competitive GLUE scores.**

### Analysis of the Matrix Mixer Framework

#### 4.1.1 Effects of Different Structured Matrix Families

Our controlled experimental setting distinctly separates the mixer matrices from other architectural components, enabling a rigorous and focused comparison between different types of mixer matrices. Specifically, utilizing the recent Mamba  block, we only replace SSD with different mixer matrices \(\). In Table 2 we provide experimental results that showcase the expressivity of various matrix mixers that support bidirectional sequence processing, primarily by utilizing off-the-shelf structured matrix families for the mixer matrix. Further details of the experimental setup are provided in Appendix E.

**Results.** The results distinctly highlight the substantial advantage in expressivity conferred by the SAM property (Definition 2.2). Previously, the high performance observed in  was attributed to their low-rank based mixer matrices, particularly emphasized by the query-key \(QK^{T}\) interaction. The results show that the structures of mixer matrices affect the capability of sequence models. However, we demonstrate that the key factor that primarily contributes to significant improvements in the expressivity of sequence models is not the query-key formulation, but rather the SAM property. Through our systematic extension, we adapt six structured matrix families - Toeplitz, Vandermonde, Cauchy, low-rank, Attention, and quasiseparable - to include the SAM property (see Appendix E for implementation details). This adaptation reveals that the sequence-aligned Cauchy variant performs nearly as well as the sequence-aligned low-rank variant, which characterizes LA. Moreover, the experimental results consistently indicate that variants equipped with the SAM property outperform those lacking this feature across all tested matrix families.

#### 4.1.2 Ablating Approaches for Bidirectionality

We compare the quasiseparable matrix mixer approach to prior bidirectional SSMs models that achieve bidirectionality by aggregating forward and backward SSMs using various heuristics including element-wise addition , the Hadamard product , and concatenation . We provide details of the ablation studies in Appendix D.1.

**Results.** The results presented in Table 3 show the shortcomings of the unidirectional Mamba  when applied to bidirectional contexts, as evidenced in C4 and GLUE. This significant performance disparity (\(-4.0\)) underscores the essential need for models to be capable of bidirectional sequence processing. Within the comparison of four bidirectional variants shown in Table 3, our approach of utilizing a quasiseparable matrix achieves the top validation results on the C4 benchmark and records the highest GLUE average score of \(81.7\). This advantage of our method is further validated in Figure 3, where it demonstrates the cross-entropy loss on the C4 validation set throughout training. Considering the gigantic size of the dataset, the expressivity of Hydra using quasiseparable matrices is clearly manifested by consistently achieving the lowest loss, as well as the highest masked token prediction accuracy.

### Evaluation Results of Hydra

#### 4.2.1 Bidirectional Masked Language Modeling

We pretrain our models on the masked language modeling objective using the Colossal Cleaned Common Crawl (C4) corpus , then finetune and evaluate them on the GLUE benchmark . We relegate experimental details in Appendix D.2.

#### 4.2.2 Image Classification

We assess Hydra on the renowned ImageNet-1K benchmark , which comprises \(1.28\)M training images and \(50\)k validation images across \(1,000\) categories. We use the ViT-Base  model as a baseline to facilitate a rigorous comparison of various sequence mixers by substituting the Transformer block in ViT with alternative sequence mixer models, specifically S4ND , Hyena , Mamba , and our proposed Hydra model. Unlike many off-the-shelf models such as CNN-based [19; 25] and vision-specialized Transformers  that include additional techniques such as hierarchical spatial downsampling to boost accuracy, our approach involves substituting only the sequence mixer layers within the ViT architecture. In addition, as opposed to other baselines in the setting of , our method uses **no tuning over the default ViT recipe** except for droppath. We found that Hydra fits the training data noticeably better than ViT, perhaps due to better expressivity and inductive bias, so we simply increased droppath from 0.3 to 0.5 as stronger regularization. We relegate further experimental details in Appendix D.2.

Results.The results, as presented in Table 5, compare the performance of Hydra with ViT  and other variants [27; 31] on ImageNet-1K. Hydra exhibits superior performance in image classification, outperforming ViT by \(2.2\%\) in Top-1 accuracy and \(1.1\%\) in Top-5 accuracy. Remarkably, even though Hydra simply flattens images without incorporating any specific 2D architectural adjustments, it still surpasses S4ND  - which is specifically tailored for image processing - by a notable margin of \(1.6\%\) in Top-1 accuracy. This showcases the versatility and effectiveness of Hydra in handling diverse data types.

## 5 Conclusion

In this work, we have explored a common paradigm for sequence models wherein the sequence mixer can be represented by a matrix. This framework encompasses many well-known models such as MLP-Mixer, FNet, convolutions, Transformers (softmax attention), and recent state-space models such as Mamba. By formalizing the matrix mixer framework and exploring additional matrix variants, we have identified a key axis of variation (_sequence alignment_) in matrix parameterizations, which enables benefits such as data dependence. This, in turn, provides increased flexibility and stronger performance for sequence models. Furthermore, we have leveraged the matrix mixer framework to motivate a natural bidirectional extension of state space models called Hydra, which can be formulated as _quasiseparable_ matrix mixers. Hydra consistently outperforms unidirectional Mamba and other bidirectional sequence models in tasks such as masked language modeling and image classification.

    &  &  &  &  \\   & & & \(_{ce}\) & Acc (\%) & MNLI & QNLI & QQP & RTE & SST2 & MRPC & COLA & STS & Avg \\  BERT & 110M & 1.59 & 67.3 & 84.4 & **90.3** & 89.7 & 77.1 & 92.3 & 90.7 & 54.2 & **89.1** & 83.5 \\ MLP-Mixer & 112M & 1.77 & 63.5 & 77.2 & 82.4 & 87.6 & 67.3 & 90.5 & 86.5 & 43.0 & 85.2 & 77.5 \\ FNet & 112M & 1.94 & 61.3 & 74.9 & 82.1 & 85.7 & 63.6 & 87.6 & 86.4 & 42.72  & 83.1 & 75.8 \\ M2 & 116M & 1.65 & 65.9 & 80.5 & 86.0 & 87.0 & 69.3 & 92.3 & 89.2 & 56.0 & 86.9 & 80.9 \\ Hydra & 112M & **1.46** & **69.1** & **84.5** & 90.0 & **91.3** & **77.5** & **93.5** & **91.2** & **57.2** & 88.9 & **84.3** \\   

Table 4: **GLUE Results.** Evaluation of various sequence models that can be formulated as matrix mixers. For maximum performance, all models are trained using established recipes [32; 13].

    &  &  &  \\   & & Acc & Acc\({}_{}\) & Acc & Acc\({}_{}\) \\  ViT-B & 87M & 78.8 & 80.6 & 94.2 & 95.2 \\ S4-ViT-B & 89M & 79.4 & 80.4 & 94.2 & 95.1 \\ Hyena-ViT-B & 88M & 78.4 & 76.4 & 94.0 & 93.0 \\  Mamba-ViT-B & 89M & 79.1 & 80.0 & 94.2 & 94.9 \\ Hydra-ViT-B & 91M & **81.0** & **81.6** & **95.3** & **95.6** \\   

Table 5: Top 1 & 5 image classification accuracies evaluated on the ImageNet-1K benchmark. We also report accuracies using the common model ensembling technique: Exponential Moving Average (EMA) weights. (Top) Reported from literature [27; 31]. (Bottom): Our unidirectional and bidirectional Mamba results.