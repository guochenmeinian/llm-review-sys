# (Amplified) Banded Matrix Factorization:

A unified approach to private training

Christopher A. Choquette-Choo

Google DeepMind

cchoquette@google.com

&Arun Ganesh

Google Research

arunganesh@google.com

Ryan McKenna

Google Research

mckennar@google.com

&H. Brendan McMahan

Google Research

mcmahan@google.com

&Keith Rush

Google Research

krush@google.com

Abhradeep Thakurta

Google DeepMind

athakurta@google.com

&Zheng Xu

Google Research

xuzheng@google.com

###### Abstract

Matrix factorization (MF) mechanisms for differential privacy (DP) have substantially improved the state-of-the-art in privacy-utility-computation tradeoffs for ML applications in a variety of scenarios, but in both the centralized and federated settings there remain instances where either MF cannot be easily applied, or other algorithms provide better tradeoffs (typically, as \(\) becomes small). In this work, we show how MF can subsume prior state-of-the-art algorithms in both federated and centralized training settings, across all privacy budgets. The key technique throughout is the construction of MF mechanisms with banded matrices (lower-triangular matrices with at most \(\) nonzero bands including the main diagonal). For cross-device federated learning (FL), this enables multiple-participations with a relaxed device participation schema compatible with practical FL infrastructure (as demonstrated by a production deployment). In the centralized setting, we prove that banded matrices enjoy the same privacy amplification results as the ubiquitous dp-sgd algorithm, but can provide strictly better performance in most scenarios--this lets us always at least match dp-sgd, and often outperform it.

## 1 Introduction

We consider machine learning (ML) with DP in the centralized (datacenter) setting and the cross-device FL setting, extending and improving matrix factorization (mf) mechanisms1 to advance the state-of-the-art in both. Given bounded-sensitivity batch gradients \(_{i}^{d}\) for \(i[n]\) steps, the mf-dp-ftrl algorithm uses a noise generation matrix \(^{-1}^{n n}\) to return DP gradient estimates \(}_{i}=_{i}+[^{-1}]_{[i,:]}\) where \(^{n d}\) has IID entries \((0,^{2})\) for suitable \(>0\). The noise correlation induced by \(^{-1}\) is key to the success of mf-dp-ftrl. Alg. 1 and Sec. 2 provide details and intuition, and Table 1 in App. A summarizes notation and symbols.

In **datacenter applications**, precise control of the sampling/shuffling of training data is possible, and so dp-sgd with privacy amplification  is one of the most popular ways to train machine learning models with formal privacy guarantees. However, Choquette-Choo et al.  recently demonstrated that a multi-epoch extension of the mf-dp-ftrl algorithm can outperform amplified dp-sgd in some settings, depending on the privacy and computational budget (typically larger budgets above \( 2\) and a small number of training epochs). This leaves the state-of-the-art for centralized DP training in the unsatisfactory state where one must try both algorithms to be assured of the best performance.

In **cross-device federated learning**, devices choose when they are available to participate in training, and so precise sampling and shuffling is generally not possible (see Sec. 3 for more details). Motivated by these limitations which make amplified dp-sgd infeasible, Kairouz et al.  developed the (tree-aggregation-based) dp-ftrl algorithm. Their dp-ftrl does not rely on (or benefit from) privacy amplification and instead adds carefully correlated noise to the gradients to boost utility. Denisov et al.  proposed mf-dp-ftrl, replacing the tree-aggregation scheme of Kairouz et al.  with a general matrix-factorization mechanism. By optimizing over this space to find mechanisms with optimal error, substantial performance improvements were possible. However, the work of Denisov et al.  applies only to the single participation (single epoch) setting. Hence, for cross-device FL the state-of-the-art also requires considering multiple algorithms: tree-aggregation-based dp-ftrl when devices may participate more than one time, or mf-dp-ftrl when devices participate only once. Importantly, the extension of mf-dp-ftrl to multiple epochs of Choquette-Choo et al.  only applies in the centralized setting, as it again requires precise control of the participation pattern.

In this work, we address the limitations of mf-dp-ftrl (mf for short) noted above, and show that it can, in fact, achieve across-the-board state-of-the-art performance in both settings across all \(\). To accomplish this, we define a family of _banded_ MF mechanisms, shown in Fig. 4 (c.f. Fig. 8 of App. D for visualizations of other factorization structures). We summarize our main contributions below.

Contributions for cross-device FLHere, the \((k,b)\)-participation schema of Choquette-Choo et al.  cannot be enforced. We propose a strict generalization, \(b\)-min-sep-participation, which can be practically enforced by FL infrastructure. We show how to efficiently and exactly bound the sensitivity for banded matrices in Thm. 2, allowing formal DP guarantees and the numerical optimization of optimal mechanisms (Sec. 4). These innovations lead to significant privacy-utility benefits in a production deployment (Fig. 6 of Sec. 6). Our work also generalizes the sensitivity calculations of Choquette-Choo et al.  to provide a general upper-bound on \(b\)-min-sep-participation sensitivity (Thm. 3), which allows the matrices of Choquette-Choo et al.  to be used in the FL setting, as well as removing the need to exactly bound \(b\) before training (see Sec. 6 and App. K).

Contributions for centralized trainingThe existing privacy amplification analysis of dp-sgd does not allow for the correlated noise that is applied in mf-dp-ftrl. Our paper introduces a novel partitioning of the BandMF iterates into independent queries. This allows us to prove in Thm. 4 of Sec. 5 that banded matrices enjoy the benefits of privacy amplification, and show that dp-sgd is a special case, giving us the best of both algorithms. This enables us to _always pareto-dominate DP-SGD_, unlike Choquette-Choo et al.  which only does so for large enough \(\) as observed in Fig. 1. Further, this allows us to improve on both baselines, between \(1-4\%\)-points. Informally:

Figure 1: **In the centralized setting, our BandMF mechanism consistently performs at least as well as the best prior methods. Left, (A): At \( 0.5\), our BandMF mechanism offers consistent utility benefits of around \(1-4\) percentage points over either dp-sgd  or Multi-epoch MF . Right, (b): BandMF (bands \(=9,18,32\), and \(64\) for \(=1-8\) respectively) significantly outperform both (unamplified) Multi-epoch MF and amplified dp-sgd.**

**Theorem 1** (Informal version of Theorems 4 and 5).: _Suppose we partition the dataset into \(b\) equal-size subsets, and in step \(i\) each example in the \(i\)-th subset participates with probability \(\) where there are \(m\) examples and the batch size is \(B\). Then, a \(\)-banded \(n\)-iteration matrix mechanism with \( b\) satisfies the same privacy guarantees as answering \(n/b\) queries on a dataset, where each element of the dataset is independently included in each query with probability \(\)._

As an example of Thm. 1, consider doing \(n=2,000\) iterations of dp-sgd on CIFAR-10, which has \(m=50,000\) examples, using a minibatch of \(B=500\) examples in each round. This has the same DP guarantees as answering \(2,000\) queries using a subsampled Gaussian mechanism with sampling probability \(p=500/50,000=0.01\). If we instead use, e.g., BandMF with \(=b=10\), our suggested sampling scheme is the following: Partition CIFAR-10 into 10 subsets of \(5,000\) examples each, \(D_{1},D_{2}, D_{10}\). In rounds 1, 11, 21... we sample \(500\) examples from \(D_{1}\), in rounds 2, 12, 22... we sample \(500\) examples from \(D_{2}\), and so on. We sample from each \(D_{i}\) a total of \(2000/10=200\) times, and each time our sampling probability is \(500/5000=0.1\) within the subset. So Theorem 1 shows \(=10\) BandMF satisfies the same DP guarantees as answering \(200\) queries with \(p=0.1\). As a special case of Theorem 1, dp-sgd is simply mf with a suitable diagonal matrix with \(=1\), and thus Thm. 1 recovers the privacy guarantees of dp-sgd with amplification by sampling. Empirically, we show that mf with amplification has privacy-utility tradeoffs that are _no worse than dp-sgd for all \(\), and often significantly better_ as can be seen in Fig. 1.

Finally, we explore the computational tradeoffs of our approach. We find that banded matrices with \(b\)-min-sep-participation are equally efficient to optimize as those under \((k,b)\)-participation but significantly reduce the memory and time complexity of the per-iteration noise generation from \((n)\) to a constant \(()\) (where often total steps \(n d\)). We will release all code with the final manuscript.

Related workThe matrix mechanism (mf mechanism or mf)  has a rich history in offline, statistical queries , with many applications including to online PCA , estimating marginals , and top-k selection . Recently, this has been studied under the _adaptive streaming_ setting, where privacy analysis must account for an adversary adaptively defining the inputs at each step . Denisov et al.  showed a connection with the DP-FTRL algorithm of Kairouz et al.  and with DP ML broadly; they showed that computing optimal MF significantly improves the privacy-utility-computation tradeoffs when making only a single pass (epoch) over the training data. Choquette-Choo et al.  showed that MF achieves state-of-the-art results in DP ML by showing how to optimize MF under arbitrary passes over the training data. Henzinger and Upadhyay  study the problem of DP-continual observation  and the explicit factorization of the workload matrix \(\) that minimizes the _completely bounded norm_, which is only off from the optimal by an additive constant. The connection between DP empirical risk minimization  and DP online regret minimization  has been studied for a long time. Asi et al.  demonstrated that DP-FTRL style algorithms  achieve the best known regret in certain classes of online learning problems (a.k.a. the realizable regime). An important question that still remains open is whether DP-FTRL style algorithms can obtain optimal population risk guarantees under DP .

Figure 2: **mf-dp-ftrl (Alg. 1) enables noise cancelling across steps, where dp-sgd does not**. The entries \(^{-1}_{i,j}\) are mostly negative (in \([0,-1)\)) in matrices \(^{-1}\) we consider (see Fig. 8). Thus, the red terms show that mf-dp-ftrl “cancels out” noise added on earlier iterations. For simplicity, we assume \(^{-1}\) has \(1\)s on the main diagonal and entries \(^{-1}_{i,j}\) otherwise, with \(_{i}:=_{[i,:]}\) the rows of \(\).

## 2 Matrix Factorization, Sensitivity, and Efficient Implementations

Let \(^{n d}\) be a stream of model gradients, and let \(^{n n}\) be an appropriate linear query workload (prefix-sums, or a matrix encoding of stochastic gradient descent with momentum (SGDM) ). Matrix mechanisms use a factorization \(=\) to privately estimate the quantity \(\) as

\[}=(\,+),\] (1)

where \(\) is suitably calibrated noise to the sensitivity of the so-called 'query matrix' \(\).

Efficiently implementing mf-df-ftrlEq. (1) can be re-arranged as \((+^{-1})\). The multiplication by the linear operator \(\) can now be viewed as post-processing of the noisy mechanism outputs \(+^{-1}\); in many cases, this postprocessing has an efficient streaming implementation, e.g., simply passing gradients into a SGDM implementation. Thus, implementing mf-dp-ftrl is essentially equivalent to dp-sgd. The only difference is that the per-iteration gradient \(_{i}\) is protected with noise \([^{-1}]_{[i,j]}\) rather than \(_{[i,:]}\). Indeed, we see dp-sgd is a special case simply by taking \(=^{-1}=\). Further, as long as the noise \(^{-1}\) is computed correctly, the privacy guarantee holds, independent of the choice of \(\). Alg. 1 gives the complete algorithm, with an appropriate choice of the matrix \(^{-1}\), this algorithm captures dp-sgd, tree-aggregation dp-ftrl2, as well as mf-dp-ftrl. The multiplication of \(^{-1}\) by Gaussian noise \(^{n d}\) (which need never be fully materialized at once) is the critical step in the efficient implementation of mf. In App. 1, we note this multiplication can be completed online for \(\)-banded matrices (defined formally in Sec. 3) in time and memory \((d)\) per training iteration compared to \((nd)\) for a non-banded matrix.

Multiple participationsWe adopt the formalisms for multiple-partications of Choquette-Choo et al. . We assume there are \(m\) examples (or users in FL) in the database where \(B\) examples are selected on each step \(i[n]\). These \(B\) chosen examples are said to _participate_ on this step \(i\). The examples at each step are processed via any adaptive function, e.g., computing a gradient of the current model (which depends on the model parameter values) as in Alg. 1. The per-example output vectors \(^{d}\) are each bounded to \(_{2}\) norm at most \(\) (noting that our notions of sensitivity scale linearly in \(\), without loss of generality (WLOG) we take to be 1 in analysis below). These clipped vectors are then summed to yield \(_{i}=_{j=1}^{B}_{j}\). The mf-dp-ftrl mechanism releases the privatized estimates of \(_{i}\) in a streaming fashion. The multi-epoch setting occurs when \(m<n B\), so that every example necessarily participates more than once.

Intuition for (anti-)correlated noise in mf-dp-ftrlFig. 2 compares dp-sgd and mf-dp-ftrl. To gain an intuition for why mf-dp-ftrl can perform better than dp-sgd, observe that vanilla SGD has iterates \(_{t}=_{0}-_{i=1}^{t}}_{i}\), and hence when the noisy gradients \(}_{i}\) are added, the \(^{-1}_{[i,j]}_{[i,j]}\) terms in mf-dp-ftrl serve to _cancel out_ some of the noise introduced on previous rounds. This reduces the total error in the final model (i.e., the prefix sums). However, this worsens the sensitivity of the mechanism to \(>\!1\) (as it is for dp-sgd). This is because an adversary trying to learn \(_{1}\) via \(}_{1}\) can partially learn the value of \(_{1}\) from \(}_{2}\), whereas in dp-sgd \(}_{1}\) and \(}_{2}\) are uncorrelated. This tradeoff is what mf-dp-ftrl aims to minimize. More details on this intuition are in App. A.1.

Adjacency and participation schemasDP requires a notion of adjacent datasets. Two data streams \(\) and \(}\) are adjacent if the data associated with any single example is altered, but not when this example participated. Thus, any \(_{i}\) where example \(_{j}\) participated can be changed subject to the constraint \(\|_{j}^{(i)}\|\). However, the participation pattern does not change. A _participation schema_\(\) gives the set of possible _participation patterns_\(\), with each \([n]\) indicating a set of steps in which a single example might participate. Let \(\) be the set of all pairs of neighboring streams \(\) and \(:=\{-}(,})\}\) represent the set of all possible deltas between neighboring \(,}\). We say a \(\) **satisfies the participation schema**\(\) if the indices of all nonzero rows in each \(^{n d}\) matrix \(\) are a subset of some \(\). To illustrate this, single-participation is represented as \(=\{\{1\},\{2\},\{n\}\}\) and full-batch gradient descent (every-step) as \(=\{[n]\}\)). Choquette-Choo et al.  studied _fixed-epoch-order participation_, denoted \((k,b)\)_-participation_, where each example participates at most \(k\) times, with any adjacent participations exactly \(b\) steps apart: formally, \(\) is the set of all \(\) such that \(|| k\), and if \(=\{i_{1},,i_{k}\}\) indexed in increasing order, we have \( j\{2,,k\},i_{j}-i_{j-1}=b\). For example \((k{=}2,b{=}3)\)-participation has \(=\{\{1,4\},\{2,5\},\{3,6\}\}\). As discussed in Choquette-Choo et al. , this setting faithfully captures centralized multi-epoch ML training setups with single and every-step as special cases. We can now define the **sensitivity** of the matrix factorization mechanism as

\[_{}()=_{(,}) }\|-}\|_{F}=_{ }\|\|_{F}.\] (2)

Optimizing factorizationsDifferent factorizations \(=\) can have very different performance in practice. Thus, in mf applications it is common to optimize over the space of factorizations, where the objective function is the expected total squared error on \(\), given as \((,)=_{}^{2}()\| \|_{F}^{2}\). We define the expected root-mean-squared error (RMSE) as \((,)/n}\), where \(\) is the standard deviation of the Gaussian noise. We take \(=1\) when simply comparing mechanisms, or (in Sec. 6), calibrate \(\) to achieve specific \((,)\)-DP guarantees.

To facilitate optimization, utilizing the fact that the optimal-for-squared-error decoder \(\) is \(^{}\), we note \((,)=(^{},)\). The expected total squared error is invariant to scaling \(\) by a constant, and hence it is sufficient to optimize under a sensitivity \(1\) constraint. Further expressing the sensitivity and error in terms of \(=^{}\) (note \(\) is unrelated to the data \(\)), we have

\[(,)=(^{},)=\|^{}\|_{F}^{2}=[^{ }^{-1}],\] (3)

assuming \(_{}()=1\) and \(\) is in the rowspace of \(\). Thus, we arrive at:

**Problem 1**.: _The matrix factorization optimization problem is to solve the convex optimization_

\[_{}^{n}}{} [^{}^{-1}]_{ }^{2}() 1,\] (4)

_and then find \(\) so that \(^{}=\), e.g., via Cholesky decomposition._

## 3 A Participation Schema for FL and the Sensitivity of Banded Matrices

In cross-device FL, devices locally evaluate eligibility criteria to determine when they might participate in training [10; 34], for example only checking-in to the coordinating server when they are plugged in, on unmetered wifi, and idle. This makes it practically difficult to enforce the \((k,b)\)-participation of Choquette-Choo et al. , where devices are assumed to participate at the same relative position in each epoch: devices are unlikely to meet the eligibility criteria during the narrow windows of both step \(i\) and \(i+b\). Further, precise sampling cannot provide the same level of privacy amplification as in the centralized setting. Consider if 6500 devices are needed to complete a round . An extreme (but realistic depending on time of day) setting may have only 6500 devices meeting eligibility criteria. Thus, either the protocol proceed without any sampling/amplification or wait until more devices are available; neither are desirable. We avoid amplification in the cross-device setting and instead proceed by addressing the question: _Can mf-dp-ftrl be extended to the cross-device federated learning setting with multiple client participations?_

With \((k,b)\)-participation difficult to enforce in practice, our first challenge is to define a new participation schema with several properties: (a) the sensitivity of any matrix mechanism under this query can be bounded; (b) this bound is tight over an expressive class of matrices; (c) this bound can be efficiently represented as a constraint in a mathematical program so as to be able to find a near-optimal factorization \(=\). In Defn. 1, we propose \(b\)-min-sep-participation, a generalization of \((k,b)\)-participation which can be practically enforced by cross-device FL systems, thus enabling us to leverage BandMF in this setting (see Sec. 6).3 In \(b\)-min-sep-participation, the distance between any two participations is _at least_\(b\), rather than _exactly_\(b\) as in \((k,b)\)-participation:

**Definition 1**.: _The \(b\)**-min-sep-participation** schema is given by_

\[_{b}=\{[n]\{i,j\},i j|i-j|  b\}\,.\]

Observe this participation schema is easy for devices to enforce: each device remembers the last step \(i\) in which it participated, and when it again becomes eligible, it checks in to the server, and participates in training as long as the current step is at least \(i+b\); it does not need to check in during a narrow (and unknown to the device) time window for a specific step.

We now turn to computing sensitivity under \(b\)-min-sep-participation. For \((k,b)\)-participation, \(|_{(k,b)}|=b\), a fact Choquette-Choo et al. [15, Eq. 3] critically exploited when computing sensitivity via brute force computation of a maximum over the elements in \(\). With \(b\)-min-sep-participation, we have \(|_{b}|=((n))\), and hence any brute force approach which requires checking some value for all \(_{b}\) will be impractical. Following the formalism of [15, Section 2], a participation schema \(\) (plus a specification of model dimension \(d\)) yields an expression for the sensitivity of the function \(\) assuming that the contributions of any given user to the data structure \(\) are restricted to the rows in \(\) indexed by some \(\). By Prop. E.1 of App. E.2, independent of model dimension \(d\), we show sensitivity for _any_ schema \(\) may be bounded by

\[_{}()^{2}_{} _{i,j}|_{[i,j]}|.\] (5)

Eq. (5) highlights several subtleties in computing sensitivity. First, is the challenge presented by the exponentially large number of patterns in \(_{b}\). Second is the question of tightness of the inequality in Eq. (5): how much are we losing by effectively ignoring any cancellation in the matrix \(\)?

Banded matricesFortunately, banded matrices render Eq. (5) both exactly computable and tight (independent of dimension \(d\)), showing that \(_{b}\) satisfies the requirements of (b) and (c) above. We say a (general) matrix \(\) is \(b\)-banded if for all \(i,j[n]\), \(|i-j|\) implies \(_{\{i,j\}}=0\). While this is off-by-one from the bandwidth (\(\) has bandwidth \(b\) - 1), our definition will be useful as it will be natural to match \(\)-banded matrices with \(b\)-min-separation. Further, for \(\)-banded lower-triangular matrices (which will play a central role), \(\) intuitively gives the number of bands in the matrix.

For non-banded matrices, the right-hand side of Eq. (5) remains efficiently computable (but not easily expressible in a mathematical program), enabling us to provide nontrivial privacy guarantees for matrices which are not \(b\) banded under \(b\)-min-sep, showing that \(_{b}\) satisfies (a) as well. The key subroutine is Alg. 3, which gives an efficient dynamic program for solving linear optimization over \(_{b}\). Define \(()\{0,1\}^{n}\) by \(()_{i}=1\) if \(i\) and 0 otherwise. Then, Alg. 3 solves

\[_{_{b}}(,()).\]

This is the key subroutine in Alg. 4 and Alg. 5. Proofs for Thm. 2 and Thm. 3 are deferred to App. E.2.

**Theorem 2**.: _Let \(^{n n}\) be a lower-triangular matrix, and \(_{b}\) the \(b\)-min-sep-participation schema. Further, suppose \(k^{}\) upper-bounds the actual maximum number of participations that occurred in a data stream \(\) (at worst, we may take \(k^{}\)): Then: (1) If \(\) is an upper-bound on the column norms of \(\), that is \( j[n],\ |_{[-j,j]}|\), and \(\) is \(b\)-banded, then the sensitivity is bounded by \(}\). (2) If \(\) is \(b\)-banded, Alg. 5 invoked with Gram matrix \(=^{}\) and \(b,k^{}\) as in the setup, exactly computes \(()\) under schema \(_{b}\) in polynomial time for any dimension \(d\)._

**Theorem 3**.: _For an arbitrary (non-banded) \(\), let \(=^{}\) and \(b,k^{}\) as in Thm. 2. Then Alg. 4 of App. E upper-bounds \(()\) under schema \(_{b}\) in polynomial time for any dimension \(d\)._Optimizing Banded Matrices

To enjoy the benefits of banded matrices within the framework of mf-dp-ftrl, we need to design an algorithm that can efficiently optimize over the space of \(\)-banded \(\) matrices. To solve this problem, we will work in the domain of \(=^{}\), and utilize the following fact:

**Proposition 4.1**.: _Let \(^{n n}\) be a \(\)-banded symmetric positive definite matrix. Then there exists a lower triangular \(\)-banded matrix \(^{n n}\) such that \(=^{}\)._

Utilizing Prop. 4.1, we can modify Problem 1 by introducing the constraint \(_{[i,j]}=0\) if \(|i-j|\). This additional linear constraint preserves convexity of the optimization problem, and makes the sensitivity calculation tractable as well. However, it is still not immediately obvious how to solve the optimization problem, since we need to run the dynamic program defined in Alg. 5 of App. E to compute sensitivity. For this reason, we impose the additional constraint that \(()=\). This constraint, together with bandedness, ensures that the squared sensitivity is equal to \(k\) for all \(\) by Thm. 2. The final optimization problem we seek to solve is stated below:

**Problem 2**.: _The matrix factorization optimization problem for banded matrices is to solve_

\[_{}^{}}{}\ ^{}^{-1} ()= \ \ \ _{[i,j]}=0\ \ |i-j|,\] (6)

_and then find \(\) so that \(^{}=\) via Prop. 4.1._

We would like to remark on the similarity between Problem 2 and the single-participation version of Problem 1. The two problems are identical modulo the bandedness constraint, which is an equality constraint on individual entries of \(\). Therefore, existing primal-optimization based solvers  for the single-participation matrix mechanism can be extended to optimize over this new space of matrices with little modification. Specifically, the only modification necessary is to initialize to an appropriately banded feasible \(\) matrix, like \(=\), and to post-process the gradient w.r.t \(\) by setting \(_{[i,j]}}=0\) if \(|i-j|\) in each step. Since the equality constraints exactly specify individual entries of \(\), Problem 2 can be solved as an unconstrained optimization problem (over the remaining entries in \(\)), using any number of off-the-shelf unconstrained optimization algorithms.4 As recommended by McKenna et al. , we use the LBFGS algorithm  to solve this problem.

Remarks on the \(()=\) constraintThe constraint on \(()\) serves multiple purposes. First, \(()=\) implies that \(|_{[:,i]}|_{2}=1\) for all \(i\), i.e., that \(\) has equal column norms. This ensures that BandMF reduces to dp-sgd when \(=1\), which is desirable. Second \(()=\) simplifies the optimization problem greatly, as the sensitivity computation for both \((k,b)\)-participation and \(b\)-min-sep are trivial and tight under this constraint (Thm. 2 Claim (1)). Third, imposing this constraint does not drastically change the search landscape, or cost much in terms of RMSE; see Table 2 for a comparison of matrices with and without this constraint, and Fig. 8 for a visualization. Fourth, this constraint allows us to solve a single optimization problem that is simultaneously tailored for \((k,b)\)-participation and \(b\)-min-sep-participation. In Appendices B and C, we formulate an optimization problem without the \(()=\) constraint, discuss how we solve it, and compare matrices generated with and without this constraint empirically.

## 5 Amplification for Banded Matrix Mechanisms

In the centralized setting where we can control the participation patterns of individual examples, the privacy guarantees of BandMF can be amplified. We focus on amplification by sampling with fixed batch size in this section, but give a more general statement in App. F.

Existing privacy analysis of mf-dp-ftrl is based on the reduction to the batch release of the entire \(+\) as a single Gaussian mechanism event  so standard amplification techniques don't directly apply. Instead, for each participation by an example, we consider the set of rows in \(\) affected by this participation as a Gaussian mechanism (see the groups of rows in Fig. 4). Then as long as the sets of rows corresponding to different participations do not interact, which is ensured by the bandedness of \(\), we can apply amplification to them separately.

Observe from Fig. 4 that the structure of BandMF guarantees that the set of rows of \(\) which depend on each of \(_{j},_{b+j},_{2b+j},\) are disjoint sets. Thus, we use the following sampling scheme for determining which examples participate in each step which is made formal as Alg. 2. Let \(D_{1},D_{2},,D_{}\) be an arbitrary partition of \(D\) into \(\) indexed subsets of size \(:= m/\) (for simplicity, we discard extra examples so all \(D_{j}\) have size exactly \(\)). In steps \(j,+j,2+j,\), we will only use examples in \(D_{j}\). Hence, participation follows \((k,b)\)-participation for \(b=\);5 because it is optimal for \((k,b)\)-participation to have the number of bands \(\) equal the min-seperation \(b\), in the remainder of this section and the associated appendices we simply write \(b\) instead of \(\). Within each of these steps, we sample a size \(B\) subset of \(D_{j}\) uniformly at random to use in computing \(\).

Roughly speaking, Thm. 4 below shows that if we use Alg. 2, BandMF satisfies any standard privacy guarantees satisfied by dp-sgd run for \(k\) rounds, where in each round we sample \(B\) examples from a dataset of size \(\). In other words, it is equivalent to running dp-sgd for \(1/b\) times as many rounds, but with the sampling probability multiplied by \(b\).

**Theorem 4**.: _Suppose \(\) is \(b\)-banded and lower triangular, and the examples participating in each step are chosen according to Alg. 2. Then BandMF satisfies any standard DP guarantee6 satisfied by performing \(k\) sensitivity-\(\) queries on a dataset of size \(\) using the Gaussian mechanism, where each query is run on a random subset of examples of size \(B\). \(\) is the maximum column norm of \(\)._

The key idea behind Thm. 4 is the following: assume we have two datasets \(D,D^{}\) that differ in an example in \(D_{1}\) (WLOG), i.e., the differing example can only participate in steps \(1,b+1,,(k-1)b+1\). Then by the banded structure of \(\) and the standard technique of reducing adaptive queries to non-adaptive queries (see e.g. Claim D.1 in ), the first \(b\) rows of \(\) (i.e., all the rows where examples in step 1 influence the output) can be viewed as a query on the examples in \(D_{1}\) that were included in step 1, the next \(b\) rows can be viewed as an adaptively chosen query on the examples included in steps \(b+1\), and so on. See Fig. 4 for a visualization.

Generalization to other privacy amplification techniquesThm. 5 in App. F.3 provides a strong a generalization of Thm. 4. It shows that shuffling, another common amplification technique often used for dp-sgd, can also be applied to our BandMF. We also provide an explicit algorithm for accounting for amplification via sampling in terms of the dp_accounting library , along with examples of concrete privacy parameters derived from these corollaries.

Optimizing the number of bandsThm. 4 shows that different numbers of bands (with a corresponding sampling scheme) give different privacy amplification guarantees. This implies given a particular privacy (or RMSE) target, one should optimize the number of bands to get the best RMSE (or privacy) possible. This can be done efficiently. Generally, for large values of \(\) larger numbers of bands perform better, and as \( 0\), eventually amplification dominates so \(=1\) becomes optimal. Details are in App. F.4.

Figure 4: Visualization of how we can decompose BandMF into independent queries when using Alg. 2. Larger view in Fig. 10 of App. F.

## 6 Experiments

Our experiments on example-level DP for image classification (of CIFAR10) and user-level DP for next word prediction (NWP) (of Stack Overflow NWP) focus on comparing our BandMF with the existing state-of-the-art Multi-epoch MF  and dp-sgd. We finish by showing that BandMF improves over state-of-the-art  for a production mobile keyboard next word prediction model. In all cases, noise \(\) is calibrated using privacy loss distributions  to achieve the stated privacy guarantees for zero-out adjacency , as implemented in . Following the common convention, amplified results are based on privacy accounting for Poisson sampling, though shuffling was used in non-production training. We find **BandMF can outperform both mechanisms across a wide range of privacy budgets** to as low as \( 0.5\). Past this, it is no worse than either.

RmseWe begin by comparing dp-sgd with Multi-epoch MF and BandMF in terms of their RMSE (Sec. 2) on the prefix workload. This is one measure for how much noise these mechanisms add during training, and is a reasonable proxy for learning performance.Observe in Fig. 5(a) that there are regimes where dp-sgd outperforms Multi-epoch MF and vice versa in terms of RMSE. When then number of epochs equals \(n\), dp-sgd reduces to full gradient descent (GD) (and there is no amplification benefit), and the optimal mf mechanism is close to the identity matrix (that is, GD), and so the algorithms become almost identical (mf has a very small advantage, as the identity matrix is not quite optimal for RMSE). However, as shown in Fig. 5(b), we see that BandMF is always at least as good as dp-sgd in terms of RMSE. The improvement is most potent in the lower number of epochs and higher \(\) regime, which is standard for large model training. In Fig. 5(c), we see that for fixed \(n\) as the number of epochs increases, all mechanisms enjoy improved RMSE, and BandMF in fact reduces to dp-sgd in that regime (though BandMF may be outperformed in RMSE by Multi-epoch MF due to our imposed optimization constraint of constant diagonals in \(\)).

Centralized training with amplificationOur full experimental setup is described in App. H, and closely follows prior work . We train for \(20\) epochs on CIFAR-10, and tune all mechanisms to achieve their best performance for each \(\), using \(12\) repeated runs. Fig. 1(a) shows that BandMF with amplification and an optimized number of bands \(\) can obtain utility benefits over both prior mechanisms. We find that for \(\), BandMF achieves a consistent \( 1\) percentage point boost in performance over Multi-epoch MF. Below \( 2\), where dp-sgd previously dominated, we find that BandMF obtains a benefit around \(3\) percentage points. These two findings show that BandMF is able to balance and leverage the benefits of both amplification and correlated noise effectively. As the budget \(\) gets smaller, we find that BandMF is equivalent to dp-sgd.

We next consider the now-standard StackOverflow next-word-prediction (NWP) task with user-level differential privacy, again following  (full details in App. I), in particular 2052 steps and 6 epochs, with \(B=1000\). The previous state-of-the-art for centralized training at \( 2\) corresponds to their Multi-epoch MF.7 We again tune \(\) under amplification for optimal RMSE, selecting \(=9,18,32,64\) for \(=1,2,4,8\) respectively. At \(=16\) we find Multi-epoch MF is optimal. Fig. 1(b) shows substantial improvements for combining amplification with mf for \(\); Table 5 of App. I gives the hyperparameters and accuracy values for this figure.

Figure 5: Comparison between dp-sgd, Multi-epoch MF, and BandMF in terms of RMSE on the prefix-sum queries, for \(n=1024\) iterations, \([,16]\), \(=10^{-6}\), and epochs \(\). Color indicates the ratio in RMSE between two mechanisms. Additional details in App. G.

Cross-device federated learningWe consider SO NWP again, but now assuming each user's data corresponds to the data on one device. We assume 2052 rounds and 6 epochs as above. Amplification is generally not possible in the cross-device setting, and so the prior state-of-the-art was (1) Single-epoch MF of Denisov et al.  for single-epoch training (using \(B=167\) rather than \(B=1000\)), and (2) Optimal TreeAgg, which essentially takes the binary-tree matrix \(_{}\), and instead of using the less-efficient "online" estimator of Honaker , uses the pseudo-inverse \(_{}^{}\) for noise generation (see Sec. 2). The \(b\)-min-sep-participation sensitivity of \(_{}\) can still be calculated using the dynamic program of Kairouz et al. , while the use of \(_{}^{}\) requires the machinery of Denisov et al. ; see e.g. the OptDecoderHonaker results of Choouret-Choo et al. [15, Fig. 6]. Our Thm. 3 further enables an upper-bound on the \(b\)-min-sep-participation sensitivity of the Multi-epoch MF matrices of Choquette-Choo et al. ; this incurs a penalty of about 15% (see Table 2 in App. C) compared to (\(k,b\))-sensitivity. Fig. 6(a) shows that our BandMF and Multi-epoch MF again outperform prior baselines (though the previously untested multi-epoch Optimal TreeAgg performs quite well); Table 4 gives the hyperparameters and accuracy values for this figure.

Application in production cross-device FLWe fine-tune a Spanish next word prediction model, pretrained on the multilingual C4 dataset [49; 60], with on-device user data using FL. Our setup follows , and is described in full in App. K. We compared to an existing implementation of the OnlineTreeAgg algorithm of Kairouz et al.  (not the optimal version using \(_{}^{}\) in simulation). Both algorithms ran for \(n=2000\) training rounds. The BandMF matrix was optimized for \(=400\) bands; however, the production system only allows approximate control of the separation between participations, and post-hoc we could only bound \(b\) by 390 rounds for OnlineTreeAgg and 385 for BandMF, necessitating the use of Thm. 3 for the analysis of BandMF as \(b<\).

We used the same clients/round goal of 6500 for both, and _tuned noise multipliers to achieve comparable RMSE_, hence tuning for a stronger privacy guarantee rather than improved accuracy. Fig. 6(b) shows our results, and we see BandMF actually achieves a slight improvement in accuracy, possibly due to learning-rate cooldown (which was only implemented for BandMF). **Our primary result is then that we are able to improve the privacy guarantee from \(\)**=\(0.52\)-**zCDP for OnlineTreeAgg to \(\)**=\(0.24\)-**zCDP for BandMF, or (\(\)=\(6.69,\)=\(10^{-10}\))-DP to (\(\)=\(4.35,\)=\(10^{-10}\))-DP. Details of the privacy guarantee following the best practices of Ponomareva et al.  are in App. K.1.

## 7 Discussion and Limitations

In this paper, we proposed the BandMF mechanism, which extends mf-dp-ftrl and enjoys the benefits of privacy amplification. This allows it to _solely operate above_ the previous Pareto frontier defined by both amplified dp-sgd and mf-dp-ftrl in centralized training scenarios. Moreover, BandMF is well-suited to federated training scenarios, and improves state-of-the-art there as well. Additionally, the computational overhead of BandMF is less than mf-dp-ftrl by a factor of \(}{{n}}\). It still has a \(b\) time and space overhead compared to dp-sgd, which can be prohibitive for very large models with billions of parameters. This is an interesting and important future research direction.

Figure 6: **Both:** Amplification is infeasible as outlined in Sec. 6 and App. K. **Left, (a):** Cross-device FL results under \(b\)=\(342\)-min-sep-participation. BandMF, and Multi-epoch MF (with the application to FL made possible by Thm. 3) outperform all prior work. **Right, (b):** Evaluation accuracy of a language model trained in a real-world FL system. BandMF achieves higher utility and a stronger (\(4.35,10^{-10}\))-DP compared to the (\(6.69,10^{-10}\))-DP achieved by OnlineTreeAgg.