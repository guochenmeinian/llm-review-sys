# ChessGPT: Bridging Policy Learning and Language Modeling

Xidong Feng

University College London

&Yicheng Luo

University College London

&Ziyan Wang

King's College London

&Hongrui Tang

University College London

&Mengyue Yang

University College London

&Kun Shao

Huawei Noah's Ark Lab

&David Mguni

Huawei Noah's Ark Lab

&Yali Du

King's College London

&Jun Wang

University College London

###### Abstract

When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose **ChessGPT**, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples **ChessCLIP** and **ChessGPT**, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model's chess ability. Experimental results validate our model and dataset's effectiveness. We open source our code, model, and dataset at https://github.com/waterhorsel/ChessGPT.

## 1 Introduction

In recent years, large language models (LLMs) based on transformer architectures  have show-cased remarkable capabilities far exceeding their original design as simple language modeling tools. This was especially notable following the advent of ChatGPT . Stemming from causal language modeling, a plethora of recent studies have concentrated on developing efficient and powerful LLM base models , supervised fine-tuned models  and models  leveraging Reinforcement Learning from Human Feedback (RLHF) .

Concurrently, there has been a growing trend in employing Large Language Models (LLMs) as foundational elements for decision-making systems. These systems either depend on the expressive capacity of transformer architectures to execute imitation learning, thereby modeling complex behaviors , or they harness the common knowledge embedded within LLMs to facilitate the policy learning process . However, the dynamic interplay between policy learning and language modeling has been scarcely addressed. Human decision-making typically involves both: we draw upon historical policy interaction to refine our policy and also employ our thoughts for strategic

[MISSING_PAGE_EMPTY:2]

format, as well as a mixed game-language dataset, which offers the most straightforward interrelated data including articles, discussion, or commentary (language) on specific chess game replay (game).

**Models.** We introduce two models, ChessCLIP and ChessGPT, leveraging our datasets. These models showcase the potential for AI to learn from a mixture of replay data and language knowledge.

**Evaluations.** We design an extensive set of tasks to evaluate our models' abilities from three distinct perspectives: modeling ability, to gauge the model's proficiency in tracking game state; value judgement ability, measuring the model's capacity for value assessment and chess knowledge; and policy ability, to test the model's capability in decision-making. Our experimental results confirm that our models consistently outperform other LLM baselines in all evaluation tasks.

We illustrate our full pipeline in fig. 1. Our work primarily pursues two objectives. Firstly, we construct the whole pipeline on chess as an initial step in promoting research on the interaction/interplay between policy learning and language learning, as well as on the potential of language as a tool for action and understanding. Secondly, our efforts have yielded valuable by-products: the ChessGPT/CLIP models. These models possess practical applicability - they could potentially serve as effective Chess AI assistants for humans.

## 2 Related work

The pursuit of creating artificial intelligence capable of playing chess can be traced back to the very beginning of the history of computer science . Chess engines today achieve superhuman-level performance by utilizing human knowledge  or self-play . Recently, there has been increasing interest in improving the interpretability  of these systems and their alignment with human behavior  besides strong performance. A chess engine that aligns with human behavior may unlock many exciting opportunities, for example, they can be used as a personalized tutor for chess beginners . Some research efforts also concentrated on employing LLMs to learn policies in Chess [36; 50]. However, these studies mainly center on small-scale datasets or limited training.

There has been increasing interest in leveraging Internet-scale knowledge for creating agents capable of generalizing across many tasks and capabilities [60; 17; 44]. For example, MineDojo  introduced a framework on Minecraft for understanding how to enable artificial agents to learn in an open-ended environment. More recently, there has been a surge in research that treats LLMs as agents, aiming to harness their internet-scale knowledge for decision-making tasks [31; 58; 62; 21]. In contrast to these studies which typically rely on powerful LLMs like GPT-4 , our paper concentrates more on training, especially the interplay between language modeling and policy learning.

## 3 A large-scale game and language dataset for chess

We introduce a large-scale game & language dataset by collecting chess-related materials from the Internet. Our dataset can be mainly divided into four categories: (1) The Game dataset, encompassing online chess match replay data involving worldwide human players and chess engines of varying skill levels. (2) The Language dataset, principally recording chess-associated knowledge, analyses, discussions, and news in the form of natural language (3) The Mixed Game-Language dataset, incorporating both game data and human natural language elements (such as game analysis or comments) in alignment. (4) The instruction-tuning and conversation dataset, consisting of instruction data and conversation data related to chess. We include comprehensive dataset descriptions, statistics, and text examples in Appendix D, and the procedure of data collection and pre-processing in Appendix E.

### Game dataset

Game replay data provide the most direct method for both humans and machines to grasp the play mechanics of Chess. In chess, these data are commonly stored in the Portable Game Notation (PGN2) format which is a standard plain text format as illustrated in Figure 2. A PGN starts with some headers that include metadata about the game. These headers include information such as the name of players, the Elo ratings, the opening play, and the game outcome. The headers are followed by a move text section that records the moves played by the two players in turn. The moves may be further annotated with comments enclosed in braces.

Previous work  uses the moves recorded in PGNs for policy learning. The moves are interpreted as actions in a Markov Decision Process and the state position can be reconstructed by loading the PGN into a chess engine. However, PGNs may contain additional useful information beyond the individual moves made. For example, the Elo ratings in the headers may inform us about the relative strength of the players. Additional information included in the comments of the move text section can also be useful - some of the moves are annotated with evaluations generated by computer chess programs that predict the current advantage of the players. These additional annotations may be useful from a reinforcement learning perspective, e.g., for value function learning. For this reason, we curated the game dataset with all of this information intact to facilitate policy learning.

**Lichess dataset** We collect five months of online game data from the Lichess database , culminating in a total of 17.5 million game replay records for online game players.

**Pro-player dataset** In the Lichess dataset, the majority of player Elo-ratings range between 1000 and 2000. To diversify our game dataset with more skilled matches, we also incorporated an additional 440,000 game records from 245 professional chess players. These professionals typically hold notably higher Elo ratings within the range of 2000 to 2800.

**CCRL** Chess engines like StockFish and LeelaZero have attained a proficiency level far beyond what any human player can currently reach. Considering this, we additionally incorporate the _Computer Chess Rating Lists_ (CCRL) , which is a dataset of chess games played by computer chess engines. The CCRL dataset comprises a considerable collection of chess games, specifically 3 million, all of which are played by computer chess engines and stored in PGN format. The Elo-ratings of chess engines fall in the range of 2800-3700.

**Chess puzzles** A chess puzzle represents a particular chessboard configuration, designed to present a distinct challenge or objective for the solver. Chess puzzles often require players to find the best move or sequence of moves to achieve a specific goal, such as checkmating the opponent's king, or finding a tactical combination. In our game dataset, we integrate 3.2M puzzles sourced from the Lichess puzzle dataset. Each puzzle within this collection is annotated with its rating, theme and solution.

**Chess modeling dataset** We observe that most chess rule descriptions are conveyed in natural language, posing a challenge for machine learning models since they statistically require a large volume of model data to accurately comprehend the chess rules . To address this issue, we build a synthetic chess modeling dataset leveraging the python-ches library . We collect chess game data from a one-month dump of the Lichess dataset, deliberately distinct from the month used in our own Lichess dataset. we design several model-based tasks including converting PGN to FEN, transferring UCI to FEN, and predicting legal moves, etc, resulting in 1.9M data samples.

### Language dataset

**Existing dataset** Numerous existing datasets comprise general internet crawl data from platforms like CommonCrawl or Wikipedia. We establish a filtering pipeline to extract only chess-related language corpus from pre-existing language corpus, including C4 , Pile , Oscar , Wikipedia  and RedPajama . These datasets extend the scope of our language data beyond mere game-play.

**Chess blogs** Numerous chess websites often publish insightful blogs, sharing their analyses and perspectives on various aspects of chess gameplay. Such blog data is incredibly valuable, as it encompasses game-specific analysis, forming a vital link between the concrete chess game data and its interpretation in natural language form. We manually select approximately 30 chess-related websites and scrape 73.2k blog articles.

**Chess books** Similar to chess blogs, chess books can provide long and detailed analysis of the game. We extract approximately 8k chess-related books from online library to enrich our language dataset.

**Chess forums** Chess forum serves as a platform for a large amount of chess-related dialogues and conversations involving a diverse range of users. These platforms encompass high-quality question-and-answer pairs, as seen in platforms like StackExchange, or more generalized discussions on

Figure 2: Replay example in Portable Game Notation (PGN) format.

various chess-related topics, commonly found in dedicated chess-specific forums. We mainly scrape chess forum data from 5 chess-specific forum platforms and StackExchange, using requests and playwright. This process results in a collection of 140K posts, representing a wealth of diverse views, queries, and discourses related to the world of chess.

### Mixed game-language dataset

**Annotated chess game** An annotated chess game is a chess game accompanied by written commentary and analysis. In an annotated game, each move made by the players is explained and evaluated, providing insights into the thought process, strategic considerations, and tactical ideas behind the moves. Here is an example of an annotated PGN with Sicilian Defense opening:

_1.e4 c5 [The game starts with the Sicilian Defense, one of the most popular and aggressive responses to 1.e4. Black aims to control the center and create imbalances early on.]_

These annotated games inherently maintain the correspondence between board state and human language, serving as an exceptionally high-quality data source to align a model with complex human intentions and judgements. We amassuated games from seven sources, five of which are collected from the internet while the rest two are commercial datasets. In total, we collect 245K annotated games with 1.3M board-language pairs.

**Youtube transcripts** Drawing inspiration similarly from MineDoJo , a YouTube video can naturally serve as a mixed game-language dataset by aligning video clips with natural language transcripts based on timestamps. Rather than generating image-language pairs directly, we develop a pipeline that accurately applies OCR (Optical Character Recognition) to chessboard screenshots to generate FEN (Forsyth-Edwards Notation), a system that describes the chess state in a language format. We gathered around 83k chess videos, resulting in million-scale English transcripts and board-language pairs, thus establishing a substantial mixed game-language dataset.

### Instruction-tuning & conversation dataset

Supervised fine-tuning is a crucial component to train large language model (LLM) to follow instructions [38; 34; 61]. In addition to the comprehensive chess materials mentioned before, we also collect instruction-tuning and conversation datasets which can be used to finetune the pre-trained LLM base model, thereby enhancing its instruction-following and dialogue capability.

**Instruction-tuning data from GPT-4** Inspired by Alpaca , we use the self-instruct technique  to generate high-quality, instruction-following data through GPT-4 . Specifically, we manually construct 200 seed prompts for chess-related questions or instructions. These prompts serve as few-shot examples, guiding GPT-4 towards more coherent and relevant generation. Finally, we generate around 4k instruction-response pairs using this pipeline.

**Conversation data from Reddit** The instruction data collected from GPT-4 are mainly in a single-step form, which means only one round of question-answer pair is included. To mitigate this issue, we collect multi-step conversation data about chess on Reddit. Reddit allows users to interact by commenting on posts and responding to other comments, creating a nested structure of responses. This nested structure can be easily converted to a conversation tree by treating the comment's reply as a child node for that reply. A rich source of conversation data can then be acquired by navigating from the root node to each leaf node via every available path. In all, we choose 6 chess-related sub-reddits and collect 410k human conversations about chess.

## 4 Large-scale pretraining

We will showcase two models - **ChessCLIP** and **ChessGPT** trained on the large-scale dataset.

### ChessCLIP

CLIP (Contrastive Language-Image Pre-Training)  is a neural network trained on a variety of modalities (e.g. image, text). By conducting contrastive learning on a large amount of paired data, CLIP bridges the image and language modality, enabling the model to understand vision by language information and vice versa. Our mixed game-language dataset in Section 3.3 has a similar paired structure because the annotation is naturally paired with its preceding game trajectories. Based on this subset, we can train a **ChessCLIP** to bridge the modality of policy and language. Specifically, by denoting the chessboard state \(S\) at timestep \(t\) as \(S_{t}\), and the annotation language as \(L_{t}\), the data pair at timestep \(T\) can be represented by \(((\{S_{t}\}_{t=T-k}^{t=T},a_{T}),L_{T})\) where \(\{S_{t}\}_{t=T-k}^{t=T}\) is a stacked \(k\) history states and \(a_{T}\) is the last move.

We want to emphasize more on what ChessCLIP can do by aligning the policy modality and the language modality. Firstly, ChessCLIP offers a similarity metric given one PGN and a text description. Just like the application of large-scale image/text retrieval using CLIP, ChessCLIP can help users conduct PGN/text retrieval - search for game based on text or search for comments based on specific game. In addition, because of the low-dimensional feature of action space compared to vision or language space (there only exists a few legal moves for a given chess state ), we can directly conduct search algorithms to maximize the similarity to generate action based on one text description using ChessCLIP. For example, given a chessboard state and a text description, ChessCLIP can generate a move by iterating through all legal moves and finding one move that returns the largest similarity. By the same logic, ChessCLIP can directly generate move sequences (multiple actions) using greedy search or beam search. We refer the reader to Appendix F.1.1 for more discussions.

**Implementation details** We preprocess the annotated PGNs to produce board/text pairs which we feed separately to the board and text encoders. In particular, for every move in the PGN, we extract the comments attached to the move as well as the board state. We encode the board positions and moves using the same scheme as those used by Leela Chess Zero (lc0) , which is similar to the encoding used by AlphaZero  for encoding positions and moves in chess. Concretely, the board positions are encoded as a \(^{8 8 112}\) feature map and the actions are encoded as a \(^{1858}\) vector. We instantiate a ChessCLIP model with a pair of text encoder and a board/action encoder. For the text encoder, we only fine-tune the last two layers of pretrained text encoder from OpenAI CLIP model. For the board/action encoder, we use a ResNet  architecture that conditions on the action encoding via a modified FiLM layer . Please refer to Appendix F.1.1 for implementation details.

### ChessGPT

The Generative Pretraining Transformer (GPT-3)  is an autoregressive language model that uses deep learning techniques to generate human-like text. GPT-3 is trained by casual language modeling, which aims to predict the next word in a sentence given all the previous words. Following the same logic, we train a GPT-like model using all chess materials introduced in Section 3. Unlike other policy behavior data in robots  or video games , the chess state and move data can be represented in merely textual format. Thanks to this feature, we can directly treat chess as a text game and the imitation learning objective for policy learning can be directly covered by casual language modeling over the game dataset provided in Section 3.1.

**Implementation details** We follow common implementations of training a domain-specific instruction-following LLM. Firstly we conduct base-model fine-tuning using chess corpus introduced in section 3.1, 3.2 and 3.3. Due to computational constraints, we choose to finetune the RedPajama-3B-base  model, which is an open-source replication of LLaMA . The base model adopts the GPT-NeoX  architecture, a GPT-3  variant with a few modifications such as rotary positional embedding, parallel attention computation, and different initialization. The base-finetuning brings us our base model: **ChessGPT-Base**. After base-finetuning, we conduct supervised fine-tuning by supervised learning on question/conversation response using data introduced in section 3.4 and general conversation data from OASST1 , Dolly2 , Alpaca-GPT4 , and Sharegpt , forming our chat model: **ChessGPT-Chat**. We leave further RLHF (Reinforcement Learning from Human Feedback) training for future work. Refer to Appendix F.1.2 for more details.

## 5 Evaluation and benchmark

In this section, we present a comparative analysis between ChessGPT trained on our database with other baseline LLMs. The purpose of our experiments is to assess the performance of ChessGPT in three primary dimensions: Chess modeling ability, Value judgement ability, and Policy ability. The Chess Modeling capability focuses on the language model's proficiency in accurately tracking the game state and predicting valid moves. Regarding the Value judgement ability, we assess the model's precision in evaluating the worth of a chess game, encompassing the identification of advantageous positions and the calculation of situation scores. Lastly, the Policy capability gauges the model's aptitude for generating optimal moves based on a given position. By thoroughly examining these sub-categories, we can comprehensively evaluate and contrast the efficacy of different models in chess-related tasks. We choose the following models as baselines: LLaMA-7B , RedPajama-Base-3B , and compare them with ChessCLIP, ChessGPT-Base-3B3, and ChessGPT-Chat-3B. To help readers who are not familiar with chess, we provide task examples and illustrative figures to elucidate these evaluation tasks in Appendix G.1.

### Chess modeling ability

**Chess state tracking** We utilized Big-bench's State Tracking in Chess task [49; 54] to evaluate language models' ability to track the state of chess games encoded in UCI notation. The task involves predicting the legal ending square given the game prefix and starting square of the current move. For example, if the input UCI notation is "\(f2f4\)\(d7d5\)\(g1\)", the expected output would be \(["h3","f3"]\), as the chess piece on square \(g1\) can only move to those two positions. The task dataset includes real and synthetic games, divided into short, medium, and long categories based on move count. The evaluation measures correctness across all games using a specified output regex. Notably, the ChessCLIP is unsuitable for modeling tasks, so we do not include it in the comparison.

Table 1 presents a performance analysis of all models on the task. Our Base and Chat models consistently outperformed baselines in all tasks. This indicates their strong ability to track the state of chess games. However, the ChessGPT-Chat model exhibited slightly lower performance, suggesting a potential trade-off between language capabilities and state tracking. Nevertheless, the results underscore the effectiveness of our dataset-trained LLM models for chess state tracking.

**Board state tracking** We performed additional evaluations involving UCI to FEN and PGN to FEN conversions. In the UCI to FEN experiment, the target was replaced with FEN format, while in the PGN to FEN experiment, UCI was converted to PGN format as input and the target was replaced with FEN format. The similarity was measured using Levenshtein distance, which was normalized to a range of 0 to 1 . These evaluations focused on assessing the model's capability to track the overall state of the chessboard by representing the state of each chess piece using FEN notation.

Table 2 illustrates the results of these evaluations. It is evident that compared to tracking the state of an individual chess piece, tracking the entire chessboard state becomes more challenging. The similarity scores between the two baselines were consistently below \(10\%\), indicating a lack of global chess piece state tracking ability. However, the ChessGPT achieves an average similarity score

    & &  \\  Tasks & LLAMA-7B & RedPajama-Base & ChessGPT-Base & ChessGPT-Chat \\  Real Short & 29.5 \(\) 1.4 & 23.2 \(\) 1.3 & **99.5 \(\) 0.2** & **98.5 \(\) 0.4** \\ Real Med & 39.3 \(\) 1.5 & 38.2 \(\) 1.5 & **97.7 \(\) 0.5** & **97.8 \(\) 0.4** \\ Real Long & 53.0 \(\) 1.6 & 51.9 \(\) 1.6 & **98.1 \(\) 0.4** & **97.6 \(\) 0.4** \\ Syn Short & 31.3 \(\) 1.4 & 24.9 \(\) 1.3 & **94.2 \(\) 0.7** & **92.3 \(\) 0.8** \\ Syn Med & 39.9 \(\) 1.6 & 37.7 \(\) 1.5 & **94.6 \(\) 0.7** & **88.9 \(\) 1.0** \\ Syn Long & 45.8 \(\) 1.5 & 42.2 \(\) 1.5 & **92.8 \(\) 0.8** & **85.1 \(\) 1.1** \\   

Table 1: Bigbench State Tracking in Chess

    & &  \\   & Tasks & LLAMA & RedPajama-Base & ChessGPT-Base & ChessGPT-Chat \\   & Real Short & 2.2 \(\) 0.0 & 5.2 \(\) 0.0 & **95.1 \(\) 0.3** & **95.7 \(\) 0.1** \\  & Real Med & 2.3 \(\) 0.1 & 4.0 \(\) 0.1 & **89.9 \(\) 0.2** & **88.6 \(\) 0.3** \\  & Real Long & 1.8 \(\) 0.0 & 3.8 \(\) 0.1 & **85.7 \(\) 0.2** & **81.4 \(\) 0.5** \\   & Real Short & 6.0 \(\) 0.1 & 2.8 \(\) 0.1 & **96.7 \(\) 0.1** & **95.8 \(\) 0.1** \\  & Real Med & 5.6 \(\) 0.1 & 3.4 \(\) 0.1 & **94.8 \(\) 0.1** & **93.6 \(\) 0.1** \\   & Real Long & 5.7 \(\) 0.0 & 8.9 \(\) 0.1 & **95.4 \(\) 0.2** & **82.7 \(\) 1.0** \\   

Table 2: UCI to FEN test higher than \(90\%\). These results demonstrate that our dataset-trained model excels in capturing and reproducing the global chess piece state in both UCI to FEN and PGN to FEN conversions.

### Value judgement ability

In this part, we evaluate the model's ability of value judgement. Specifically, we want to assess the model from two perspectives: (1) its ability to align with the true value function given a chessboard state (the true value are evaluated by chess engines in enough search depths) in the evaluation of **State value multi-choice**, and (2) its ability to align with human judgement and human knowledge in the evaluation of **Chess Annotation Multi-choice** and **Opening multi-choice**.

**State value multi-choice** Here we evaluate the model's ability to see whether it can determine which side holds the advantage for a given PGN. We construct an evaluation dataset consisting of \(3000\) game snippets and utilize Stockfish 15 with a depth of 18 to calculate the winning rate for the white pieces. By categorizing the winning rate into three intervals: \(0-33\%\) for black advantage, \(34-66\%\) representing a balanced state, and \(67-100\%\) for white advantage, we construct the state-value multiple-choice task. During experiments, we discovered that an additional '{' suffix to the prompt can significantly enhance the performance of the base model. This is due to '{' consistently serving as the initial symbol for annotation in annotated PGNs. Consequently, we carried out our evaluation under two distinct prompt settings and report our results w.r.t multi-choice grade shown in Table 3.

**Chess annotation multi-choice** The annotations within an annotated PGN can be viewed as a reflection of human evaluative judgement. To examine the degree to which the model's value aligns with human value, we extract 3k game-language pairs from the annotation dataset as the test set. By randomly selecting three annotations from the test set as incorrect options, we construct the chess annotation four-choice task. We report the multi-choice grade results over two prompts in Table 4.

**Opening multi-choice** A chess opening refers to the initial moves made by players at the beginning of a chess game. There are numerous chess openings, each with its own name, characteristics, and strategic goals. For example, the Sicilian defense: _1. e4 c5_ is one of the most popular and aggressive chess openings for Black. We use the Lichess opening dataset  including 3.5k opening PGNs and their corresponding names, to formulate two tasks: (1) PGN2Opening five-choice task, which aims at choosing the correct opening name for a given PGN, and reversely, (2) Opening2PGN five-choice task, aiming at choosing the correct PGN for a given opening name. We report the result in Table 5.

In general, our trio of models surpasses the performance of two baseline language models across these four tasks in all settings. This result confirms that our models are more effectively aligned with both the true value function and human judgement/knowledge. Both ChessGPT-Base and ChessGPT-chat deliver outstanding performance in the state-value task and the opening task. Notably, ChessCLIP displays a surprisingly high level of proficiency in the annotation task and the opening task. This result reveals the model's capacity to extract human judgement and knowledge solely from annotations, even without training in any actual chess games.

### Policy evaluation

**Checkmate in one** We incorporate the checkmate-in-one task from Big-Bench  into our evaluation methods. This task is designed to challenge the model's ability to identify a move in a given PGN

    &  \\  Prompt Setting & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\  W/O \{ suffix & 29.8 \(\) 0.8 & 27.4 \(\) 0.7 & **33.2 \(\) 0.9** & **35.7 \(\) 0.9** & N/A \\ With \{ suffix & 29.6 \(\) 0.8 & 28.4 \(\) 0.8 & 38.8 \(\) 0.9 & 34.7 \(\) 0.9 & **63.6 \(\) 0.9** \\   

Table 4: Chess Annotation Multi-choice

    &  \\  Prompt Setting & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\  W/O \{ suffix & 33.2 \(\) 0.7 & 31.1 \(\) 0.7 & **43.1 \(\) 0.8** & **52.8 \(\) 0.8** & N/A \\ With \{ suffix & 26.9 \(\) 0.7 & 29.7 \(\) 0.8 & **53.7 \(\) 0.8** & **53.5 \(\) 0.8** & **38.1 \(\) 0.8** \\   

Table 3: State value multi-choicethat would result in a checkmate. By doing so, it measures the model's capacity to comprehend and apply the rules of chess. The model is essentially required to discern a move that not only places the opponent's king under attack but also ensures that the king cannot evade capture in the next move.

We also find adding an additional instruction suffix like _[Now white/black can checkmate in one]_ can largely enhance the base model performance. We report the result in two prompts with two metrics (exact-string match as ESM and multi-choice-grade as MC) in Table 6. our ChessGPT-Base model and ChessGPT-Chat model show a really great checkmate ability by surpassing two LLM baselines by a large margin. ChessCLIP does not perform well in this task, because there does not exist much annotation data regarding checkmate-in-one behavior in the annotation dataset.

**General policy** In order to assess the model's generalization ability, we introduced Elo Rating as a factor in the task, aiming to evaluate its capacity to identify PGN and related keywords and generate the appropriate next move within the specified skill level. Model's selection of the next legal move is assigned a move score, which is normalized based on the win rate observed in the raw data. Table 7 presents the results representing the performance of different models in selecting the most suitable move for white chess. Notably, all models surpassed the performance of the random policy (\( 50\%\)) as the Elo Ratings correspond to relatively high skill levels among human players.

Further analyzing the performance of different models across varying Elo Ratings is crucial for understanding the observed results. The minor variations in move scores for different Elo Rating scenarios in Table 8 indicate that ChessGPT-Base may struggle to effectively incorporate Elo Rating information into its decision-making process. This could be due to the model's limited understanding of the nuanced characteristics associated with distinct Elo Ratings. The complexity of the task and the challenges in accurately accounting for diverse playing styles further contribute to the limited variations in move scores across different Elo Ratings. Consequently, neglecting this information can lead to the model learning an average policy for each Elo Rating, resulting in subpar overall performance. Similar findings were observed in the black chess test, and to further validate this viewpoint, we conducted input attention visualization. Refer to Appendix F.1.2 for more details.

To clarify, the dataset we have presented encompasses a wide range of games and varying Elo ratings, as shown in Figure 2, which possesses the potential to effectively capture and generalize intricate patterns and policies associated with different Elo levels. However, the current training method might not sufficiently emphasize these nuanced features. This highlights a potential direction for future research, which involves enhancing the model's ability to better integrate and utilize metadata such as Elo Rating and other auxiliary data. By addressing these aspects, the model's overall generalization can be further improved.

    &  \\   & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\  With suffix (ESM) & 1.6 \(\) 0.2 & 0.0 \(\) 0.0 & **71.4 \(\) 0.7** & 56.8 \(\) 0.8 & N/A \\ With suffix (MC) & 2.6 \(\) 0.3 & 0.0 \(\) 0.0 & **66.1 \(\) 0.8** & 11.3 \(\) 0.5 & 2.9 \(\) 0.3 \\ W/O suffix (ESM) & 1.7 \(\) 0.2 & 0.0 \(\) 0.0 & 26.5 \(\) 0.8 & **59.4 \(\) 0.8** & N/A \\ W/O suffix (MC) & 2.2 \(\) 0.3 & 0.0 \(\) 0.0 & 13.6 \(\) 0.6 & **15.4 \(\) 0.6** & N/A \\   

Table 6: Checkmate in One

   Elo Rating & Move Score \\ 
700-1000 & 59.4 \(\) 1.0 \\
1200-1500 & 58.9 \(\) 0.9 \\
1700-2000 & 59.6 \(\) 1.0 \\
2700-3000 & 59.8 \(\) 1.0 \\   

Table 8: ChessGPT-Base in Different Elo Rating Results

    &  \\   & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\  Opening2PGN & 43.0 \(\) 0.9 & 26.5 \(\) 0.8 & **92.2 \(\) 0.5** & **94.7 \(\) 0.4** & 73.0 \(\) 0.8 \\ PGN2Opening & 20.0 \(\) 0.7 & 20.7 \(\) 0.7 & 49.3 \(\) 0.9 & 55.8 \(\) 0.9 & **80.5 \(\) 0.7** \\   

Table 5: Opening2PGN and PGN2Opening

### Qualitative results

We also perform qualitative comparison between our models (ChessGPT-Chat and ChessGPT-Base) and the baselines. We ask the language models a series of questions ranging from factual knowledge of chess as well as requesting the models to perform some operational tasks related to chess. We found that ChessGPT-base performed similarly to RedPajama: both models can sometimes produce factual answers for some of the questions but they failed to generate coherent answers when asked to perform tasks such as providing commentary on chess moves or converting the PGN notation to FEN. ChessGPT-Chat gives more factual answers and demonstrates better performance when prompted to generate analysis and perform other chess-related tasks. Refer to Appendix H for qualitative analysis.

## 6 Conclusion

In this paper, we introduce a new large-scale dataset and benchmark on chess to encourage of study of the interplay between historical policy data and natural language knowledge. We accompany our dataset with an evaluation framework for assessing language models' capability in chess. We showcase two models, **ChessCLIP** and **ChessGPT**, that demonstrate promising results for learning the interplay between language and action. Nevertheless, our results indicate that we are only beginning to understand how to bridge the gap between policy learning and language modeling and we discuss more on the future directions of our dataset in Appendix J. We hope that our dataset and benchmark can make future research on policy and language alignment more accessible.