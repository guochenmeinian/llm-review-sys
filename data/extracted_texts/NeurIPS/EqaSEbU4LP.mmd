# WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models

T Pavan Kalyan

IIT Bombay

Piyush Singh Pasi1

Amazon

Sahil Nilesh Dharod

IIT Bombay

Azeem Azaz Motiwala

IIT Bombay

Preethi Jyothi

IIT Bombay

Aditi Chaudhy

Google, Deepmind

Krishna Srinivasan

Google, Deepmind

Work done as a student at IIT Bombay.

###### Abstract

Cross-modal (image-to-text and text-to-image) retrieval is an established task used in evaluation benchmarks to test the performance of vision-language models (VLMs). Several state-of-the-art VLMs (e.g. CLIP, BLIP-2) have achieved near-perfect performance on widely-used image-text retrieval benchmarks such as MSCOCO-Test-5K and Flickr30K-Test-1K. As a measure of out-of-distribution (OOD) generalization, prior works rely on zero-shot performance evaluated on one dataset (Flickr) using a VLM finetuned on another one (MSCOCO). We argue that such comparisons are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. To address this gap, we introduce WikiDO (drawn from **Wik**ipedia **D**iversity **O**bservatory), a new cross-modal retrieval benchmark to assess the OOD generalization capabilities of pretrained VLMs. This consists of 384K image-text pairs from Wikipedia with domain labels, along with carefully curated, human-verified in-distribution (ID) and OOD test sets of size 3K each. The image-text pairs are very diverse in topics. We evaluate different VLMs of varying capacity on the WikiDO benchmark; BLIP-2 achieves zero-shot performance of R@1\( 66\%\) on the OOD test set, compared to \( 81\%\) on MSCOCO and \( 95\%\) on Flickr. When fine-tuned on WikiDO, the R@1 improvement is at most \( 5\%\) on OOD instances compared to \( 12\%\) on ID instances. WikiDO offers a strong cross-modal retrieval benchmark for current VLMs, especially for evaluating OOD generalization. Our benchmark is hosted as a competition at https://kaggle.com/competitions/wikido24 with public access to dataset and code.

## 1 Introduction

Vision-language models (VLMs) are multimodal models that jointly reason on image and text. VLMs are pretrained on very large amounts of diverse image and text data, thus making them capable of robust reasoning. A true measure of this capability is to evaluate how well VLMs generalize to out-of-distribution (OOD) instances. This has been addressed in prior work [1; 2] by finetuning VLMs on a given corpus for a given task and conducting zero-shot evaluations on a new corpus. However, the mere use of an unseen corpus for evaluation does not imply it is OOD. For a more accurate characterization of generalization, the OOD nature of the evaluation data should be carefully established.

how well they generalize to OOD instances. WikiDO consists of image-text data derived from _Wikipedia Diversity Observatory_, a diverse source of Wikipedia articles spanning several diversity axes including geography, gender, ethnicity and domains/topics. We focus on the "domains" axis that is most diverse in terms of coverage and spans different topics (as determined via topic labels assigned to each article) such as food, books, fashion and sports. We curate a dataset consisting of 1) 354K training images with corresponding text and 2) two evaluation sets - an in-domain (ID) set and an out-of-domain (OOD) set2 drawn from domains that are seen and unseen during training, respectively. Our OOD evaluation set is carefully constructed to be used as a reliable testbed for VLMs.

Figure 1 highlights the main aspects of WikiDO including the domains spanned by the articles, their distribution and a few illustrative image-text pairs. In this work, we focus on cross-modal (image-to-text and text-to-image) retrieval tasks. We show retrieval performance of well-known VLMs, namely CLIP , BLIP  and BLIP-2 , on WikiDO test sets before and after finetuning on the WikiDO training instances. The best-scoring VLM, CLIP, achieves a modest zero-shot R@1 of \(68\%\) on the OOD test set. Finetuning on the WikiDO train set improves zero-shot R@1 on the OOD set by only \(5\%\), while zero-shot R@1 on the ID set improves substantially more by \(12\%\) further highlighting the difference between the two evaluation sets. We have hosted our code, WikiDO datasets and a leaderboard with our current VLMs at https://kaggle.com/competitions/wikido24.

## 2 Related Work

Image-text datasets.The rapid progress of vision-language models (VLMs) in recent years can be largely attributed to the emergence of high-quality multimodal datasets. These include large, automatically-filtered datasets that are crawled from the web and smaller datasets that are human-annotated. The larger datasets comprising millions of instances include SBU , CC3M , CC12M , YFCC-100M , WIT , LAION-400M , and LAION-5B . These large datasets have

Figure 1: Distribution of topics in the final filtered dataset

been primarily used for pretraining VLMs to achieve good zero-shot performance on downstream tasks. The smaller datasets are typically created by first crawling images from the internet and then manually annotating labels, regions and textual descriptions for the images. These include Flickr30K , MSCOCO  and Visual Genome . Flickr30K and MSCOCO consist of images of everyday activities, most commonly used as evaluation benchmarks for cross-modal retrieval.

Domain generalization datasets.Existing domain generalization benchmarks such as Office-Home , PACS , and VLCS  are predominantly focused on image classification and are not multimodal. A recent effort to extend the task of domain generalization to image-text tasks is Domain Generalization for Image Captioning (DGIC) . DGIC collates popular existing datasets from five domains: common domain sourced from MSCOCO, assistive domain sourced from Vizwiz , social domain sourced from Flickr30k, avian domain sourced from CUB-200 , and floral domain sourced from Oxford102 . While datasets like MSCOCO, Vizwiz, and Flickr30k represent common objects from daily life and are easier domains for VLMs, the avian and floral domains are significantly more challenging. Prior work [17; 21; 22] using any of Flickr30k, MSCOCO, and Vizwiz as test domains show good generalization performance since these datasets contain images of generic objects that appear commonly across datasets. We aim for WikiDO to serve as a more challenging benchmark to evaluate the generalization abilities of VLMs.

Vision-language models.VLMs are broadly focused on tasks related to cross-modal understanding and cross-modal generation. A critical component of understanding is aligning the visual and textual features. Models like CLIP  and ALIGN  use a dual-encoder model to individually extract features and align them through a global contrastive loss. UNITER  utilizes a multimodal encoder to extract visual and textual characteristics jointly. ALBEF  introduced image-text matching and masked language modelling to align the image-text representations. FILIP  works at the granularity of image patches and textual words to further refine the alignment. BLIP  introduces a new vision-language pretraining framework with both vision-language understanding (image-text contrastive loss and image-text matching loss) and generation objectives (language modelling loss). Similar to BLIP, BLIP-2  also uses both kinds of objectives but bootstraps vision-language pretraining from off-the-shelf pre-trained image encoders and large language models as textual encoders. BLIP-2 introduced a lightweight Querying Transformer, which is trained in two stages to bridge the modality gap. The first stage uses a frozen vision encoder for vision-language representation learning. The second stage bootstraps vision-to-language generative learning from a frozen language model. We evaluate all these three VLMs, CLIP, BLIP and BLIP-2, on WikiDO.

## 3 WikiDO: A New Evaluation Benchmark

We present WikiDO, a new image-text retrieval dataset for the improved evaluation of VLMs for OOD generalization. We will first describe the source of the dataset (SS3.1), followed by details about the data curation process and how the final data splits were obtained (SS3.2).

### Source of WikiDO

WikiDO is derived from the _Wikipedia Diversity Observatory3_ (WDO). WDO consists of data, visualizations and tools to analyze and bridge the gap in content in Wikipedia, based on the current state of diversity across Wikipedia articles. This diversity is assessed based on a few specific categories: geographical location, gender, sexual orientation, ethnic groups, religious groups and topical coverage. We chose English articles from the topical coverage category to create the WikiDO dataset, since this was most extensive in terms of coverage across topics. The Wikipedia articles in this category are labelled with one of the following topics: Earth, Monuments and Buildings, GLAM (Galleries, Libraries, Archives and Museums), Folk, Food, Books, Paintings, Clothing and Fashion, Sports and Teams, Music Creations and Organizations, and People. This categorization of articles into topics also aids the construction of ID and OOD test sets in WikiDO. Other diversity axes also offer potential for creating multimodal benchmarks to evaluate visual language models. By tagging and categorizing data across dimensions such as geography, gender, sexual orientation, ethnicity, religion, and topics, researchers can assess cultural biases and generalization capabilities in multimodal contexts for both monolingual and multilingual settings.

### Details of Data Curation

The data from WDO contains meta-information about an article and the corresponding topic label. We find all Wikipedia pages from Wikipedia dumps  and extract the URLs of the images from the articles. The topic label associated with the page is assumed to be the topic label for all the images in the page. For each image, we extracted metadata like the page URL, page title, height, width, and three different types of text associated with the image. These are: 1. Reference description: The caption that is visible on the Wikipedia page just below the image. 2. Attribution: Text appearing on the Wikimedia page of the image. 3. Alt-text description: Text used by accessibility/screen readers when the image is not visible. This crawled dataset consists of 2.7M image-text pairs out of which 1.2M are unique images. Based on the data pipeline adopted by WIT , we used the following filtering steps:

1. We only retained images that have a research-permissive license such as Creative Commons; the text of Wikipedia is licensed under a CC-BY-SA license.
2. We only retained images that have reference descriptions. These textual descriptions were used as image captions. Reference texts are contextual, and therefore, instead of describing the image, they provide additional information about the context of the image. This property makes this dataset more appropriate for the task of cross-modal retrieval, rather than caption generation. Only those texts were retained that were at least of length three.
3. Only jpg and png images with a height and width of more than 150 pixels were retained.
4. Certain image-text pairs were repeated frequently. These were de-duplicated to retain only single instances. We also removed generic image-text pairs such as flags, maps, logos, etc.

Caption enhancement.The final filtered dataset consists of 384K unique image-text pairs, each labelled with a topic label. The distribution across topics is shown in Figure 1. Reference texts in WikiDO tend to either be descriptive with domain-specific terminology or very concise and non-descriptive. In order to maintain a balance between the original reference texts and more detailed textual descriptions of the image, we passed the original text through the visual instruction-tuned model LLaVA . For each instance, we provided LLaVA with the image and the reference text and prompted it to provide a concise caption describing the image without missing any information from the reference text. The prompt template and some examples are provided in the appendix A.2. To analyze how the LLaVA-enhanced captions differ from the original reference texts, we use a Part-of-Speech (POS) tagger  to compute POS tags for every word. Figure 2 shows the POS tag distributions for both original and enhanced captions. While there is a loss of unique POS-word pairs after enhancement, the retained POS-word pairs tend to repeat at a much higher rate in the en

  
**Key** & **Description** \\  image\_path & path of the image \\ image\_id & Wiki ID of the image \\ orig\_cap & Reference text from Wikipedia \\ image & Unique image ID given in the dataset \\ page\_id & Wiki ID of the page from which the image was extracted \\ page\_title & Title of the wikipedia article from which \\  & the image was extracted \\ topic & Topic label from Wikipedia Diversity \\  & Observatory \\ caption & Caption obtained by passing orig\_cap \\  & through LLava (for test, val sets also human verified) \\   

Table 1: Dataset schema

Figure 2: Top: Proportion of POS tags in original captions and those retained in the enhanced captions. Bottom: Average repetition count of retained POS-word pairs in the original and enhanced captions. \(*\) denotes that the repetition count of determiners is scaled down by a factor of 1000 for visualization.

banned captions compared to the original captions. While retained proper nouns do not often repeat in enhanced and original captions, common nouns, adjectives, verbs, and determiners tend to repeat a lot in enhanced captions. This may be due to the replacement/paraphrasing of specific proper nouns and nouns with more general nouns. Qualitative examples of the most frequently occurring nouns in original and enhanced captions are given in the appendix A.2.

Measuring the domain gap.The final WikiDO dataset we use in all our experiments uses LLaVA-enhanced captions. To create train-val-test splits, we identified a subset of topics that were semantically different from the rest, both visually and linguistically. We randomly sampled 1000 instances from each topic and passed the instances through CLIP (ViT-L) to get embeddings. Figure 3 shows the t-SNE plots for both image and text embeddings. Although most topics in the dataset overlap in the representation space, paintings, medicine and food (shown in blue) are fairly well-separated in both image and text space. We further validate this quantitatively by measuring the domain gap using Maximum Mean Discrepancy (MMD) . We observe that food, medicine and paintings differ linguistically from the other topics. MMD for visual embeddings show that earth, food and medicine are the most distant topics. The appendix A.3 provides implementation details and results for MMD.

Data splits.Based on the t-SNE plots and MMD analysis, we chose the topics food, paintings and medicine to appear in the OOD test set. The remaining topics are included in the train, validation and in-domain test sets. To further sample a smaller evaluation set of size 3K (comparable to existing test set sizes in MSCOCO and Flickr) from all the samples across the OOD topics, we use the following strategy. We find the image-image similarity and text-text similarity between each OOD instance and all instances of the train set. Then, we pick the top-k similarity scores for each instance of the text and image modalities. If the average of each top-k for each modality crosses a certain threshold, we discard those samples from the OOD test set. Therefore, we only retain those samples in the OOD test set that are highly dissimilar from the train set w.r.t. both image and text modalities.

To mimic the original data distribution, we randomly sample 1000 and 3000 instances from the train set instances to create validation and in-domain (ID) test splits. 354K samples remain in the train set after creating the validation and test splits. As the distribution of topics is highly skewed towards a few topics, we created three different kinds of train splits - a balanced train set consisting of almost equal number of samples from each topic amounting to a total of 100K instances. Henceforth, this set will be referred to as the train set unless mentioned otherwise. Similarly, a balanced training set is created using 200K samples and finally, the training set containing all 354K samples.

Human verification.Since the reference texts were enhanced using LLaVA and could result in hallucinations, we revised the validation, ID, OOD test set captions via a human verification pass. For each image, the evaluator was specifically asked, "Is there any made-up/ hallucinated content in the caption that is not supported by the image/reference text?" with an option to answer with a "Yes" or "No". If they answer "Yes", then the evaluator was asked to correct the reference text by mainly removing the hallucinations in the enhanced captions. Figure 4 shows that the percentage of instances marked as having hallucinations is comparatively much smaller

Figure 4: Percentage of instances that were marked as Correct (no hallucination), Hallucinated, and Wrong (caption and image do not match).

Figure 3: T-SNE plot for embeddings of 1000 random images and texts per topic with perplexity 32

than correct captions across all splits. Almost all edits done by human raters to the hallucinated captions are "deletion" edits to remove hallucinations. Please refer to the appendix A.4 for more details on edits made by the human raters.

## 4 Experiments and Results

We benchmark the performance of pretrained CLIP, BLIP, and BLIP-2 models on WikiDO, MSCOCO and Flickr. We show zero-shot performance and the effect of finetuning with different objectives using these pretrained models on all three datasets.

### Experimental Setup

We use the standard train, validation and test sets introduced in MSCOCO  and Flickr . For WikiDO, we use the splits introduced in Section 3.2. To finetune BLIP and BLIP-2, we follow the official code published by the authors. To finetune CLIP, we use LAVIS codebase4. We use two variants of BLIP (ViT-L, ViT-B) and BLIP-2 (ViT-L, ViT-G) as well as CLIP (ViT-L/14@336px). All models are trained for 6 epochs on 4 A100 80GB Nvidia GPUs. We used a cosine learning rate scheduler. Hyperparameter settings are given in Table 2. A description of the model variants and their pretraining objectives can be found in Appendix B.1. Unlike CLIP, both BLIP and BLIP-2 use a re-ranking strategy, we first select the top \(N\) captions (\(N=128\) for all experiments) for a given image using ITC (image-text contrastive) scores, i.e., cosine similarity scores. Then, we compute ITM (image-text matching) scores between the image and each of these N texts. The final scores used for ranking are obtained by adding both ITC and ITM scores. For CLIP, we only use cosine similarity (ITC scores) between the image and text for ranking. Conversely, the same applies to text-to-image retrieval. All evaluations use the Recall@k (R@k, k\(=1,5,10\)) metric.

    & **BLIP** & **BLIP-2** &  \\  & ViT-L (ViT-B) & ViT-L (ViT-G) & \\  Batch size & 256 & 224 & 256 \\ Queue size & 57600 & 57600 & - \\ Pixel Res. & 256 & 256 & 336 \\ Optimizer & AdamW & AdamW & Adam \\ lr & \(5e^{-6}(1e^{-5})\) & \(5e^{-6}(1e^{-5})\) & \(1e^{-6}\) \\ Decay & 0.05 & 0.05 & \(1e^{-3}\) \\ \(_{1},_{2}\) & 0.9, 0.999 & 0.9, 0.98 (0.9, 0.99) & 0.9, 0.98 \\   

Table 2: Hyperparameter settings

    &  &  \\  &  &  &  &  \\  & & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\   & Z & 58.8 & 81.0 & 87.7 & 63.8 & 82.9 & 88.7 & 55.1 & 73.2 & 79.4 & 58.7 & 76.1 & 81.6 \\  & W & 73.2 & 90.8 & 94.6 & 73.4 & 89.7 & 93.9 & 62.3 & 79.6 & 84.3 & 62.8 & 80.0 & 85.2 \\   & Z & 61.6 & 83.5 & 89.7 & 65.8 & 85.4 & 91.1 & 58.9 & 76.4 & 82.6 & 62.1 & 79.0 & 83.9 \\  & W & 72.6 & 90.8 & 94.6 & 73.7 & 90.3 & 94.2 & 63.6 & 80.8 & 86.1 & 65.9 & 81.8 & 86.9 \\   & Z & 72.9 & 88.8 & 93.1 & 69.5 & 87.2 & 91.6 & 68.2 & 85.8 & 90.3 & 66.3 & 84.3 & 88.9 \\  & W & **82.8** & **95.0** & **97.4** & 81.5 & **94.4** & **96.7** & **73.4** & **87.7** & **91.8** & **72.9** & **88.3** & **91.9** \\   & Z & 70.3 & 87.8 & 91.8 & 74.1 & 89.3 & 93.2 & 66.4 & 82.2 & 86.9 & 70.4 & 84.9 & 88.6 \\  & W & 82.1 & 94.0 & 96.4 & **82.5** & 94.3 & **96.7** & 72.1 & 85.9 & 90.3 & 73.6 & 87.1 & 90.3 \\   & Z & 70.8 & 89.1 & 93.1 & 75.3 & 90.6 & 94.1 & 66.1 & 81.4 & 86.2 & 69.3 & 84.7 & 88.3 \\  & W & 79.4 & 93.3 & 96.2 & 80.0 & 93.3 & 96.1 & 70.5 & 84.3 & 88.2 & 72.0 & 85.9 & 89.2 \\   

Table 3: Comparison of state-of-the-art VLMs. Z denotes zero-shot and W denotes model fine-tuned on 100K split of WikiDO dataset. Number of parameters are listed alongside model names.

### Results and Analysis

Zero-shot.Models BLIP and BLIP-2 perform better on ID than OOD by 3-7% across all R@K. CLIP, on the other hand, performs almost similarly on both ID and OOD (\(\)1% gap), suggesting better domain coverage during pretraining. Zero-shot performance of CLIP and BLIP-2 is _significantly higher_ than that of BLIP. This could be attributed to the larger training data of CLIP compared to BLIP (>3x). Despite BLIP and BLIP-2 being trained using the same dataset, BLIP-2 utilizes a frozen CLIP image encoder, potentially helping to gain further improvement on CLIP, as seen in Table 3. It is interesting that BLIP-2 performs much better on text-to-image retrieval (>5% for R@1 and \(\)2% for R@10 improvement) compared to CLIP. This could be due to the caption-generation-based pretraining objective used in BLIP-2.

Finetuning.We use the 100K balanced train set to finetune all models and evaluate on WikiDO ID and OOD test sets; these numbers are denoted as \(W\) in Table 3. The gap between ID and OOD sets have significantly grown from 3-7% in the zero-shot setting to 9-11% across all R@K. All models show >8% improvement for R@1 on the ID set, and the majority of models have >=5% improvement for most R@K (BLIP-2 being slightly behind, possibly due to stronger zero-shot). For OOD, R@K for the majority of models are under 5%, and none of them are above 10%. Vision backbones of BLIP (ViT-L), BLIP-2 (ViT-L), and BLIP-2 (ViT-G) have the same architecture as CLIP but differ in pretraining data. BLIP-2 (ViT-L) benefits from pretrained CLIP ViT-L as a robust vision encoder as compared to BLIP-2 (ViT-G), which uses Eva-CLIP  as the backbone.

MSCOO & Flickr.Here, we establish that fine-tuning VLMs on MSCOCO and testing on Flickr is not a reliable test for OOD generalization. Table 4 shows finetuning on MSCOCO significantly improves Flickr performance. CLIP's zero-shot performance is lower than that of the BLIP model as the latter has already seen image-caption pairs similar to those of MSCOCO during pretraining. For BLIP, finetuning on MSCOCO and testing on Flickr is almost the same as finetuning on Flickr. Similarly, finetuning on Flickr significantly boosts zero-shot MSCOCO. Such improvements suggest that both datasets significantly overlap, making MSCOCO-Flickr a not-so-strong pair for testing generalization. A full comparison of all models is provided in Appendix B.2.

Effect of scaling ID data.To find out whether the performance gap between the ID and OOD test sets can be abridged by adding more data from the ID, we train BLIP with 200K and 354K image-text

    &  &  \\  &  &  &  &  \\  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\   & Z & 57.5 & 80.7 & 87.8 & 36.6 & 60.9 & 71.0 & 86.6 & 98.0 & 99.1 & 67.2 & 88.9 & 93.4 \\  & C & 75.4 & 92.8 & 96.2 & 58.6 & 82.2 & 89.3 & 94.5 & 99.7 & 99.7 & 83.1 & 96.9 & 98.5 \\  & F & 68.9 & 87.6 & 92.7 & 51.8 & 75.8 & 84.0 & 95.5 & 99.5 & 99.9 & 85.0 & 97.7 & 98.9 \\   & Z & 78.9 & 93.9 & 96.9 & 62.4 & 84.1 & 90.2 & 95.3 & 99.7 & 100.0 & 85.2 & 96.9 & 98.3 \\  & C & 83.2 & 95.9 & 80.0 & 66.1 & 86.6 & 91.8 & 97.1 & 100.0 & 100.0 & 88.3 & 98.0 & 98.9 \\   & F & 80.7 & 94.7 & 97.5 & 64.4 & 85.4 & 91.1 & 97.0 & 100.0 & 100.0 & 89.9 & 98.4 & 99.2 \\   

Table 4: Overview of results for the current way of showing OOD generalization. Z denotes zero-shot, C denotes model fine-tuned on MSCOCO, and F denotes model fine-tuned on Flickr.

    &  &  \\  &  &  &  &  \\  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@1 & R@5 & R@10 \\ 
100K & 72.6 & 90.8 & 94.6 & 73.7 & 90.3 & 94.2 & 63.6 & 80.8 & 86.1 & 65.9 & 81.8 & 86.9 \\
200K & 74.1 & 91.4 & 95.4 & 75.4 & 91.1 & 94.9 & 64.4 & 81.1 & 86.1 & 66.5 & 82.1 & 86.7 \\
354K & 76.2 & 92.2 & 96.0 & 76.5 & 92.2 & 95.6 & 64.3 & 80.8 & 86.1 & 66.6 & 82.1 & 86.7 \\   

Table 5: Effect of scaling the ID data on OOD generalization using BLIP (ViT-L).

pairs. The results in Table 5 show minimal improvements in OOD, suggesting that scaling ID data is insufficient to close the performance gap. In addition, the distribution of the largest train set is heavily biased towards only a few domains, indicating the need for more diverse data during training.

Ablations on finetuning objectives.All three models were trained with different pre-training objectives (described in Section 2). Table 6 shows the results of using different losses during fine-tuning. All models are of comparable size. Even without the use of additional objectives, CLIP proves to be very robust. In BLIP-2, the addition of ITM as an additional fine-tuning objective results in the largest R@1 improvement of \(\) 5-6\(\%\), and ITG slightly improves performance. The performance improvement for BLIP by adding ITM is limited to approximately \(\) 3-4\(\%\).

## 5 Discussion and Limitations

To understand why there is any improvement on the OOD test set when fine-tuned on the ID data, we first use a parser  to extract noun phrases from all sentences. Next, we use Grounding-DINO  to detect object boxes from the corresponding image and label each box with the corresponding noun chunk if the semantically represent the same thing. We recognize roughly 1M image boxes with the corresponding noun chunks in the text. We pass these image boxes to DINOv2  to extract the image features. After applying K-means clustering to these embeddings with K=100, we obtain 100 meaningful clusters. To visualize this, we select 1000 boxes per cluster that are closest to the centroid. Figure 5 shows these object clusters with the difference between the objects present in OOD compared to ID. While there are a few clearly separated clusters for OOD objects, there are clusters that contain both objects in OOD and ID instances. This object overlap explains the gains in R@K for OOD after fine-tuning. While our work presents a carefully constructed test bed for OOD evaluation of VLMs, it is important to acknowledge several limitations:

    &  &  \\ Loss & Model &  &  &  &  \\  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  ITC & CLIP & 82.8 & 95.0 & 97.4 & 81.5 & 94.4 & 96.7 & 73.4 & 87.7 & 91.8 & 72.9 & 88.3 & 91.9 \\  & BLIP & 68.9 & 88.3 & 93.0 & 69.0 & 88.4 & 93.1 & 59.6 & 77.5 & 83.4 & 60.4 & 78.1 & 83.7 \\  & BLIP-2 & 74.4 & 92.9 & 95.7 & 75.6 & 92.3 & 95.7 & 61.2 & 80.1 & 85.6 & 62.5 & 82.0 & 87.6 \\  ITC+ITM & BLIP & 72.6 & 90.8 & 94.6 & 73.7 & 90.3 & 94.2 & 63.6 & 80.8 & 86.1 & 65.9 & 81.8 & 86.9 \\  & BLIP-2 & 80.4 & 93.2 & 96.3 & 80.6 & 93.3 & 95.6 & 70.9 & 85.4 & 89.5 & 73.3 & 86.7 & 90.2 \\  ITC+ITM+ITG & BLIP-2 & 82.1 & 94.0 & 96.4 & 82.5 & 94.3 & 96.7 & 72.1 & 85.9 & 90.3 & 73.6 & 87.1 & 90.3 \\   

Table 6: Performance of models trained on different finetuning objectives trained on 100K split. ViT-L backbone is used for both BLIP and BLIP-2.

Figure 5: TSNE of 100 object clusters. Blue shows OOD objects, and green denotes objects from ID images.

Limited Scope of Image-Text Retrieval.Our primary focus has been on image-text retrieval. Although this approach can be extended to other tasks, such as generation and contextual understanding, our current evaluation framework does not cover these tasks. Since the data is extracted from Wikipedia along with the meta-data like page ID and title, it can be used for tasks like contextual image-captioning , image-suggestion and image-promotion .

Use of Topics Axis Only.In WikiDO, we have primarily explored diversity only through the lens of topical content. There are numerous other diversity axes, such as cultural context, ethnicity, gender and religion, etc. that could provide a more robust and diverse evaluation framework. Additionally, our data is currently limited to English despite the availability of similar data in multiple languages. Expanding our evaluation to include multilingual datasets would help evaluate multilingual VLMs.

Lack of Manual Verification for Enhanced Training Set.The enhanced training set captions, due to their large size, have not been manually verified. While our test and validation sets indicate that the quality of the enhanced captions is high, the absence of manual verification could mean that some errors remain in the training data.

## 6 Conclusion

In this work, we introduced WikiDO, a novel benchmark specifically designed to evaluate the out-of-distribution (OOD) generalization capabilities of vision-language models (VLMs) in the context of cross-modal retrieval. Unlike existing benchmarks, WikiDO draws from the diverse content of Wikipedia, providing a robust dataset that includes 384K image-text pairs categorized by domain. Our benchmark includes both in-distribution (ID) and OOD test sets, each comprising 3K carefully curated and human-verified pairs, allowing for a comprehensive assessment of model performance across a wide range of topics. Our evaluations of various state-of-the-art VLMs, such as CLIP and BLIP-2, on the WikiDO benchmark revealed insights into their OOD generalization capabilities. While BLIP-2 demonstrated superior zero-shot performance with R@1\( 66\%\) on the OOD test set, this was notably lower compared to its performance on traditional benchmarks like MSCOCO and Flickr. Moreover, fine-tuning on WikiDO yielded a relatively modest improvement of approximately \(\)5% on OOD instances, suggesting inherent challenges in achieving robust OOD generalization. These findings underscore the limitations of current VLMs in handling truly OOD data. WikiDO thus serves as an effective testbed to help develop, evaluate and guide future VLMs towards superior generalization capabilities.