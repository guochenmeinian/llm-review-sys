# SE(3)-bi-equivariant Transformers

for Point Cloud Assembly

 Ziming Wang, Rebecka Jornsten

Department of Mathematical Sciences

University of Gothenburg and Chalmers University of Technology

zimingwa@chalmers.se, jornsten@chalmers.se

###### Abstract

Given a pair of point clouds, the goal of assembly is to recover a rigid transformation that aligns one point cloud to the other. This task is challenging because the point clouds may be non-overlapped, and they may have arbitrary initial positions. To address these difficulties, we propose a method, called \(SE(3)\)-bi-equivariant transformer (BITR), based on the \(SE(3)\)-bi-equivariance prior of the task: it guarantees that when the inputs are rigidly perturbed, the output will transform accordingly. Due to its equivariance property, BITR can not only handle non-overlapped PCs, but also guarantee robustness against initial positions. Specifically, BITR first extracts features of the inputs using a novel \(SE(3) SE(3)\)-transformer, and then projects the learned feature to group \(SE(3)\) as the output. Moreover, we theoretically show that swap and scale equivariances can be incorporated into BITR, thus it further guarantees stable performance under scaling and swapping the inputs. We experimentally show the effectiveness of BITR in practical tasks.

## 1 Introduction

Point cloud (PC) assembly is a fundamental machine learning task with a wide range of applications such as biology , archeology , robotics  and computer vision . As shown in Fig. 1, given a pair of 3-D PCs representing two shapes, _i.e_., a source and a reference PC, the goal of assembly is to find a rigid transformation, so that the transformed source PC is aligned to the reference PC. This task is challenging because the input PCs may have random initial positions that are far from the optimum, and may be non-overlapped, _e.g_., due to occlusion or erosion of the object.

Most of existing assembly methods are correspondence-based : they use the fact that when the input PCs are aligned, the points corresponding to the same physical position should be close to

Figure 1: Two examples of PC assembly. Given a pair of PCs, the proposed method BITR transforms the source PC (red) to align the reference PC (blue). The input PCs may be overlapped (a) or non-overlapped (b).

each other. For example, in Fig. 1(a), the points at the head of the airplane in the source and reference PCs should be close in the aligned PC. Specifically, these methods first estimate the correspondence between PCs based on feature similarity or distance, and then compute the transformation by matching the estimated corresponding point pairs. As a result, these methods generally have difficulty handling PCs with no correspondence, _i.e._, non-overlapped PCs, such as Fig. 1(b). In addition, they are often sensitive to the initial positions of PCs.

To address these difficulties, we propose a method, called \(SE(3)\)-bi-equivariant transformer (BITR), based on the \(SE(3)\)-bi-equivariance prior of the task: when the input PCs are perturbed by rigid transformations, the output should transform accordingly. A formal definition of \(SE(3)\)-bi-equivariance can be found in Def. 3.1. Our motivation for using the \(SE(3)\)-bi-equivariance prior is threefold: First, the strong training guidance provided by symmetry priors is known to lead to large performance gain and high data efficiency. For example, networks with a translation-equivariance prior, _i.e._, convolutional neural networks (CNNs), are known to excel at image segmentation . Thus, \(SE(3)\)-bi-equivariance prior should lead to similar practical benefits in PC assembly tasks. Second, \(SE(3)\)-bi-equivariant methods are theoretically guaranteed to be "global", _i.e._, their performances are independent of the initial positions. Third, the \(SE(3)\)-bi-equivariance prior does not rely on correspondence, _i.e._, it can be used to handle PCs with no correspondence.

Specifically, the proposed BITR is an end-to-end trainable model consisting of two steps: it first extracts \(SO(3) SO(3)\)-equivariant features from the input PCs, and then obtains a rigid transformation by projecting the features into \(SE(3)\). For the first step, we define a \(SE(3) SE(3)\)-transformer acting on the 6-D merged PC by extending the \(SE(3)\)-transformer ; For the second step, we use a _SVD_-type projection inspired by Arun's method . In addition, we theoretically show that scale-equivariance and swap-equivariance can be incorporated into BITR via weight constraining techniques, which further guarantees that the performance is not influenced by scaling or swapping inputs. An illustration of BITR is presented in Fig. 2.

In summary, the contribution of this work is as follows:

* We present a \(SE(3)\)-bi-equivariant PC assembly method, called BITR. 1 BITR can assemble PCs without correspondence, and guarantees stable performance with arbitrary initial positions. In addition, the \(SE(3) SE(3)\)-transformer used in BITR is the first \(SE(3) SE(3)\)-equivariant steerable network to the best of our knowledge. * Theoretically, we show that scale and swap equivariance can be incorporated in to BITR by weight-constraining, thus it guarantees stable performance under scaling and swapping the inputs.
* We show experimentally that BITR can effectively assemble PCs in practical tasks.

## 2 Related works

A special case of PC assembly is PC registration, where the correspondence between input PCs is assumed to exist. A seminal work in this task was conducted by , which provided a closed-form solution to the problem with known correspondence. To handle PCs with unknown correspondence, most of the subsequent works extend  by first estimating the correspondence by comparing distances , or features [25; 23; 41] of the PCs, and then aligning the PCs by aligning the estimated corresponding points. Notably, to obtain \(SE(3)\)-bi-equivariance, \(SO(3)\)-invariant features [42; 10; 43] have been investigated for correspondence estimation. However, since these methods require a sufficient number of correspondences, they have difficulty handling PCs where the correspondence does not exist. In addition, they often have difficulty handling PCs with large initial errors .

Figure 2: An overview of BITR. The input 3-D PCs \(X\) and \(Y\) are first merged into a 6-D PC \(Z\) by concatenating the extracted key points \(\) and \(\). Then, \(Z\) is fed into a \(SE(3) SE(3)\)-transformer to obtain equivariant features \(\), \(t_{X}\) and \(t_{Y}\). These features are finally projected to \(SE(3)\) as the output.

The proposed BITR is related to the existing registration methods because it can be seen as a generalization of Arun's method . However, in contrast to these methods, BITR is correspondence-free, _i.e._, it is capable of handling PCs with no correspondence. In addition, it theoretically guarantees stable performance under arbitrary initial position.

Recently, some works have been devoted to a new PC assembly task, called fragment reassembly, whose goal is to reconstruct a complete shape from two fragments. Unlike registration task, this task generally does not assume the existence of correspondence.  first studied this task, and they addressed it as a pose estimation problem, where the pose of each fragment relative to a canonical pose is estimated via a regression model.  further improved this method by considering the \(SE(3)\)-equivariance of each fragment. In addition,  proposed a simulated dataset for this task. In contrast to these methods, the proposed BITR does not rely on the canonical pose, _i.e._, it directly estimates the relative pose. As a result, BITR is conceptually simpler and it can handle the shape whose canonical pose is unknown.

Another related research direction is equivariant networks. Due to their ability to incorporate 3D rotation and translation symmetry priors, these networks have been extensively used in modelling 3D data [29; 9; 37; 17; 13], and recently they have been used for robotic manipulation task [27; 28; 40]. In particular,  proposed a tensor field network (TFN) for PC processing, and \(SE(3)\)-transformer  further improved TFN by introducing the attention mechanism. On the other hand, the theory of equivariant networks was developed in [16; 5]. BITR follows this line of research because the \(SE(3) SE(3)\)-transformer used in BITR is a direct generalization of \(SE(3)\)-transformer , and it is the first \(SE(3) SE(3)\)-equivariant steerable network to the best of our knowledge.

## 3 Preliminaries

This section briefly reviews Arun's method and the concept of equivariance, which will be used in BITR.

### Group representation and equivariance

Given a group \(G\), its representation is a group homomorphism \(:G GL(V)\), where \(V\) is a linear space. When \(G\) is the 3D rotation group \(SO(3)\), it is convenient to consider its irreps (irreducible orthogonal representation) \(_{p}:SO(3) GL(V_{p})\), where \(p\) is the degree of the irreps, and \((V_{p})=2p+1\). For \(r G\), \(_{p}(r)^{(2p+1)(2p+1)}\) is known as the Wigner-D matrix. For example, \(_{0}(r)=1\) for all \(r SO(3)\); \(_{1}(r)^{3 3}\) is the rotation matrix of \(r\). More details can be found in  and the references therein.

In this work, we focus on the group \(G\) of two independent rotations, _i.e._, \(G=SO(3) SO(3)\), where \(\) represents the direct product. Similar to \(SO(3)\), we also consider the irreps of \(G\). A useful fact is that all irreps of \(G\) can be written as the combinations of the irreps of \(SO(3)\): the degree-\((p,q)\) irreps of \(G\) is \(_{p,q}=_{p}_{q}:SO(3) SO(3) GL(V_{p} V_{q})\), where \(p,q\), \(_{p}\) and \(_{q}\) are irreps of \(SO(3)\), and \(\) is tensor product (Kronecker product for matrix). For example, \(_{0,0}(r_{1} r_{2})=1\); \(_{1,0}(r_{1} r_{2})^{3 3}\) is the rotation matrix of \(r_{1}\); \(_{1,1}(r_{1} r_{2})=_{1}(r_{1})_{1}(r_{2})^{9 9}\) is the Kronecker product of the rotation matrices of \(r_{1}\) and \(r_{2}\).

Given two representations \(:G GL(V)\) and \(:G GL(W)\), a map \(:V W\) satisfying \(((g)x)=(g)(x)\) for all \(g G\) and \(x V\) is called \(G\)-equivariant. When \(\) is parametrized by a neural network, we call \(\) an equivariant neural network, and we call the feature extracted by \(\) equivariant feature. Specifically, a degree-\(p\) equivariant feature transforms according to \(_{p}\) under the action of \(SO(3)\), and a degree-\((p,q)\) equivariant feature transforms according to \(_{p}_{q}\) under the action of \(SO(3) SO(3)\). For simpler notations, we omit the representation homomorphism \(\), _i.e._, we write \(r\) instead of \((r)\), when \(\) is clear from the text.

### Arun's method

Consider a PC assembly problem _with known one-to-one correspondence_: Let \(Y=\{y_{u}\}_{u=1}^{N}^{3}\) and \(X=\{x_{u}\}_{u=1}^{N}^{3}\) be a pair of PCs consisting of \(N\) points, and let \(\{(x_{u},y_{u})\}_{u=1}^{N}\) be their corresponding point pairs. What is the optimal rigid transformation that aligns \(X\) to \(Y\)? provided a closed-form solution to this problem. It claims that the optimal solution \(g=(r,t) SE(3)\) in terms of mean square errors is

\[r=() t=(Y)-r(X)\] (1)

where

\[=_{i}}}^{T}\] (2)

is the correlation matrix, \(()\) represents the mean value, \(}=x_{i}-(X)\) and \(}=y_{i}-(Y)\) represent the centralized points, and \(()\) represents the singular value decomposition projection. The definition of SVD projection can be found in Def. C.5.

Arun's solution enjoys rich equivariance properties. Formally, we have the following proposition:

**Definition 3.1**.: Consider a map \(: SE(3)\) where \(\) is the set of 3D PCs. Given \(X,Y\), let \(g=(X,Y)\).

* \(\) is \(SE(3)\)-bi-equivariant if \((g_{1}X,g_{2}Y)=g_{2}gg_{1}^{-1}, g_{1},g_{2} SE(3)\).
* \(\) is swap-equivariant if \((Y,X)=g^{-1}\).
* \(\) is scale-equivariant if \((cX,cY)=(r,ct),\; c_{+}\).

**Proposition 3.2**.: _Under a mild assumption (C.2), Arun's algorithm (1) is \(SE(3)\)-bi-equivariant, swap-equivariant and scale-equivariant._

In other words, Arun's method guarantees to perform consistently 1) with arbitrary rigid perturbations on \(X\) and \(Y\), _i.e._, it is global, 2) if \(X\) and \(Y\) are swapped (aligning \(Y\) to the fixed \(X\) or aligning \(X\) to the fixed \(Y\)), and 3) in arbitrary scale. Details of Prop. 3.2 can be found at Appx. C.1.

We regard Arun's method as a prototype of \(SE(3)\)-bi-equivariant PC assembly methods: it first extracts a degree-\((1,1)\)\(SO(3) SO(3)\)-equivariant translation-invariant feature, _i.e._, the correlation matrix \(\) (2), and then obtains an output \(g SE(3)\) using a _SVD_-based projection. This observation immediately leads to more general \(SE(3)\)-bi-equivariant methods where the handcrafted feature \(\) is replaced by the more expressive learned equivariant features, thus, the correspondence is no longer necessary. We will develop this idea in the proposed BITR in the next section, and further show that the scale and swap equivariance of Arun's method can also be inherited.

## 4 \(Se(3)\)-bi-equivariant transformer

This section presents the details of the proposed BITR. BITR follows the same principle as Arun's method : it first extracts \(SO(3) SO(3)\)-equivariant features as a generalization of the correlation matrix \(\) (2), and then projects the features to \(SE(3)\) similarly to (1). Specifically, we first propose a \(SE(3) SE(3)\)-transformer for feature extraction in Sec. 4.2. Since this transformer is defined on 6-D space, _i.e._, it does not directly handle the given 3-D PCs, it relies on a pre-processing step described in Sec. 4.3, where the input 3-D PCs are merged into a 6-D PC. Finally, the Arun-type \(SE(3)\)-projection is presented in Sec. 4.4. An overview of BITR is presented in Fig. 2.

### Problem formulation

Let \(Y=\{y_{v}\}_{v=1}^{N}^{3}\) and \(X=\{x_{u}\}_{u=1}^{M}^{3}\) be the PCs sampled from the reference and source shape respectively. The goal of assembly is to find a rigid transformation \(g SE(3)\), so that the transformed PC \(gX=\{rx_{i}+t\}_{i=1}^{M}\) is aligned to \(Y\). Note that we do not assume that \(X\) and \(Y\) are overlapped, _i.e._, we do not assume the existence of corresponding point pairs.

### \(Se(3) SE(3)\)-transformer

To learn \(SO(3) SO(3)\)-equivariant translation-invariant features generalizing \(\) (1), this subsection proposes a \(SE(3) SE(3)\)-transformer as a generalization of \(SE(3)\)-transformer . We present a brief introduction to \(SE(3)\)-transformer  in Appx. A for completeness.

According to the theories developed in , to define a \(SE(3) SE(3)\)-equivariant transformer, we first need to define the feature map of a transformer layer as a tensor field, and specify the action of \(SE(3) SE(3)\) on the field. Since \(SE(3) SE(3)\) can be decomposed as \((T(3) T(3))(SO(3) SO(3))\) where \(T(3)\) is the 3-D translation group, the tensor field should be defined in the \(6\)-D Euclidean space \(^{6} T(3) T(3)\), and the features attached to each location should be \(SO(3) SO(3)\)-equivariant and \(T(3) T(3)\)-invariant. Formally, we define the tensor field as

\[f(z)=_{u=1}^{L}f_{u}(z-z_{u})\] (3)

where \(Z=\{z_{u}\}_{u=1}^{L}^{6}\) is a 6-D PC, \(\) is the Dirac delta function, \(f_{u}=_{p,q}f_{u}^{p,q}\) is the feature attached to \(z_{u}\), where \(f_{u}^{p,q}\) is the degree-\((p,q)\) equivariant component. We then specify the action of \(SE(3) SE(3)\) on the base space \(^{6}\) as

\[(g_{1} g_{2})(z)=(g_{1}z^{1})(g_{2}z^{2}) z ^{6}\] (4)

where \(z=z^{1} z^{2}\), \(z^{1},z^{2}^{3}\) are the first and last three components of \(z\), \(\) represents direct sum (concatenate), and \(g_{i}=(r_{i},t_{i}) SE(3)\) for \(i=1,2\). Thus, the action of \(SE(3) SE(3)\) on the degree-\((p,q)\) component of \(f\) is

\[((g_{1} g_{2})f^{p,q})(z)=(_{p,q}(r_{1} r _{2}))f^{p,q}((g_{1} g_{2})^{-1}(z)).\]

With the above preparations, we can now define a \(SE(3) SE(3)\)-transformer layer in a message passing formulation similar to \(SE(3)\)-transformer:

\[f_{}^{}(z_{u})=}F_{ }^{}(z_{u})}_{}+_{\\ v()\{u\}}}_{}_{uv}^{, }}_{}.\] (5)

Here, we use notations \(=(o_{1},o_{2})\) and \(=(i_{1},i_{2})\) for simplicity. For example, \(f^{}\) represents the degree-\(\) feature \(f^{o_{1},o_{2}}\). \(F^{}(z_{u})^{c(2o_{1}+1)(2o_{2}+1)}\) is the collection of all degree-\(\) features at \(z_{u}\), where \(c\) is the number of channels of the degree-\(\) features, and \(W^{}^{1 c}\) is a learnable parameter for self-interaction. \(()\) represents the \(k\) nearest neighborhood, and attention \(_{uv}\) is computed according to

\[_{uv}=_{u}^{}_{uv} )}{_{v^{}()\{u\}} (_{u}^{}_{uv^{}})},\] (6)

where \(\), \(\) and \(\) are known as the query, key and value respectively. They are defined as

\[_{u}=_{}W_{Q}^{}F_{ }^{}(z_{u}),_{uv}=_{} _{}_{K}^{,}(z_{vu} )f_{}^{}(z_{v}),_{uv}^{, }=_{V}^{,}(z_{vu} )f_{}^{}(z_{v})\] (7)

where \(z_{vu}=z_{v}-z_{u}\), \(W_{Q}^{}\) is a learnable parameter for \(\), and the convolutional kernel \(^{,}(z)\) in \(\) and \(\) both take the form of

\[(^{,}(z))=_{J_{1 }=|o_{1}-i_{1}|}^{o_{1}+i_{1}}_{J_{2}=|o_{2}-i_{2}|}^{o_{2}+i_{2}}_{J_{1},J_{2}}^{}(\|z^{1}\|,\|z^{2}\|)}_{}_{J_{1},J_{2}}^{,}Y_{J_{1}}( }{\|z^{1}\|^{1}})}_{} Y_{J_{2}}( }{\|z^{2}\|})}_{},\] (8)

where the learnable radial component \(_{J_{1},J_{2}}^{,}: \) is parametrized by a neural network, the non-learnable angular component is determined by the 2-nd order Clebsch-Gordan constant \(\) and the spherical harmonics \(Y_{J}:^{3}^{2J+1}\), and \(()\) is the vectorize function reshaping a matrix to a vector. Formulation (8) is derived in Appx. B.

Note that the kernel (8) is the main difference between a \(SE(3)\)-transformer layer and a \(SE(3) SE(3)\)-transformer layer. In the special case where only \(SE(3)\)-equivariance is considered, _i.e._, all features are of degree \((p,0)\) (or \((0,q)\)), a \(SE(3) SE(3)\)-transformer layer becomes a \(SE(3)\)-transformer layer.

Finally, we adopt the equivariant Relu (Elu) layer similar to  as the point-wise non-linear layer in our network. Given an input degree-\(\) feature \(F^{}\) with \(c\) channels, an Elu layer is defined as

\[(F^{})=F_{}& F_{ },F_{} 0\\ F_{}- F_{},}{\|F_{}\|}}{\|F_{ }\|}&\] (9)

where \(F_{}=W_{}^{}F^{}\) and \( F_{}=W_{}^{}F^{}\). \(W_{},W_{}^{c c}\) are learnable weights and \(\|\|\) is the channel-wise vector norm. Note that when \(=(1,0)\) or \(=(0,1)\), our definition (9) becomes the same as . By interleaving transformer layers and Elu layers, we can finally build a complete \(SE(3) SE(3)\)-equivariant transformer model.

### Point cloud merge

To utilize the transformer model defined in Sec. 4.2, we need to construct a 6-D PC as its input. To this end, we first extract key points from the raw 3-D PCs, and then concatenate them to a 6-D PC to merge their information. Thus, the resulting 6-D PC is not only small in size but also contains the key information of the raw PCs pairs.

Formally, we extract \(L\) ordered key points \(=\{_{u}\}_{u=1}^{L}\) and \(=\{_{u}\}_{u=1}^{L}\) from \(X\) and \(Y\) respectively, and then obtain \(Z=\{_{u}_{u}\}_{u=1}^{L}\). Note that we do not require \(\) (\(\)) to be a subset of \(X\) (\(Y\)). Specifically, we represent the coordinates of the key points as a convex combination of the raw PCs:

\[=(F_{X}^{0})X,=(F_{Y} ^{0})Y,\] (10)

where \(X^{M 3}\) and \(Y^{N 3}\) represent the coordinates of \(X\) and \(Y\) respectively, and \(()\) represents the row-wise softmax. \(F_{X}^{0}^{L M}\) and \(F_{Y}^{0}^{L N}\) are the weights of each point in \(X\) and \(Y\) respectively, and they are degree-\(0\), _i.e._, rotation-invariant, features computed by a shared \(SE(3)\)-transformer \(_{E}\):

\[F_{X}^{0}=_{E}(X), F_{Y}^{0}=_{E}(Y).\] (11)

Furthermore, inspired by , we fuse the features of \(X\) and \(Y\) in \(_{E}\) before the last layer, so that their information is merged more effectively, _i.e._, the selection of \(\) or \(\) depends on both \(X\) and \(Y\). Specifically, the fused features are

\[f_{,X}(x_{u})=f_{,X}^{0}(x_{u})_{v}(f_{,Y}^{0}(y_{v})) f_{,X}^{1}(x_{u}) \\ f_{,Y}(y_{v})=f_{,Y}^{0}(y_{v})_{u} (f_{,X}^{0}(x_{u})) f_{,Y}^{1}(y_{v}), \] (12)

where we only consider degree-\(0\) and degree-\(1\) features. \(f_{,X}\) and \(f_{,Y}\) represent the features of \(X\) and \(Y\), and _Pool_ is the average pooling over the PC.

Note that \(\) and \(\) obtained in Eqn. 10 are permutation invariant. For example, according to Eqn. 10, the \(i\)-th key point of \(X\) is \(_{i}=_{j}_{ij}X_{j}\), where \(_{ij}\) is the \(i\)-th channel of \(F_{X}^{0}\) at \(x_{j}\) (after softmax normalization). When \(X\) is permutated by \(\), then the \(i\)-th key point can be written as \(_{i}^{}=_{j^{}}_{ij^{}}X_{j^{}}\), where \(j^{}=(j)\). It is easy to see that \(_{i}^{}=_{i}\) because both \(j\) and \(j^{}\) iterate through \(\{1,...,M\}\) in the summation.

### SE(3)-projection

We now obtain the final output by projecting the feature extracted by the \(SE(3) SE(3)\)-transformer to \(SE(3)\). Formally, let \(f\) be the output tensor field of the \(SE(3) SE(3)\)-transformer. We compute the final output \(g=(r,t) SE(3)\) using an Arun-type projection as follows:

\[r=()\\ t=(()+t_{Y})-r(()+t_{X}),\] (13)

where \(=()^{3 3}\), \(t_{X}^{3}\) and \(t_{Y}^{3}\) are equivariant features computed as

\[=_{u}(f_{u}^{1,1}),\ t_{X}=_{u}(f_{u}^{1,0} ),\ t_{Y}=_{u}(f_{u}^{0,1}).\]

We note that projection (13) extends Arun's projection (1) in two aspects. First, although \(\) in (13) and \(\) (2) are both degree-\((1,1)\) features, \(\) is more flexible than \(\) because \(\) is a learned feature while \(\) is handcrafted, and \(\) is correspondence-free while \(\) is correspondence-based. Second, projection (13) explicitly considers non-zero offsets \(t_{X}\) and \(t_{Y}\), which allow solutions where the centers of PCs do not match.

In summary, BITR computes the output \(g\) for PCs \(X\) and \(Y\) according to

\[g=_{P}_{S}(X,Y),\] (14)

where \(_{S}:\) is a \(SE(3) SE(3)\)-transformer (with the PC merge step), \(_{P}: SE(3)\) represents projection (13), and \(\) is the set of tensor field. We finish this section with a proposition that BITR is indeed \(SE(3)\)-bi-equivariant.

**Proposition 4.1**.: _Under a mild assumption (C.2), BITR (14) is \(SE(3)\)-bi-equivariant._Swap-equivariance and scale-equivariance

This section seeks to incorporate swap and scale equivariances into the proposed BITR model. These two equivariances are discussed in Sec. 5.1 and Sec. 5.2 respectively.

### Incorporating swap-equivariance

This subsection seeks to incorporate swap-equivariance to BITR, _i.e._, to ensure that swapping \(X\) and \(Y\) has the correct influence on the output. To this end, we need to treat the group of swap as \(/2=\{1,s\}\) where \(s^{2}=1\), _i.e._, \(s\) represents the swap of \(X\) and \(Y\), and properly define the action of \(/2\) on the learned features.

Formally, we define the action of \(/2\) on field \(f\) (3) as follows. We first define the action of \(s\) on the base space \(^{6}\) as swapping the coordinates of \(\) and \(\): \(s(z)=z^{2} z^{1}\), where \(z=z^{1} z^{2}\), and \(z^{1},z^{2}^{3}\) are the coordinates of \(\) and \(\) respectively. Then we define the action of \(s\) on feature \(f\) as \((s(f))^{p,q}(z)=(f^{q,p}(s(z)))^{T}\), where we regard a degree-\((p,q)\) feature \(f_{u}^{p,q}\) as a matrix of shape \(^{(2p+1)(2q+1)}\) by abuse of notation, and \(()^{T}\) represents matrix transpose.

Intuitively, according to the above definition, degree-\((1,1)\), \((1,0)\) and \((0,1)\) features will become (the transpose of) degree-\((1,1)\), \((0,1)\) and \((1,0)\) features respectively under the action of \(s\), _i.e._, \(\) will be transposed, \(t_{X}\) and \(t_{Y}\) will be swapped. This is exactly the transformation needed to ensure swap-equivariant outputs. We formally state this observation in the following proposition.

**Proposition 5.1**.: _For a tensor field \(f\) and a projection \(_{P}\) (13), \(_{P}(s(f))=(_{P}(f))^{-1}\)._

Now the remaining problem is how to make a \(SE(3) SE(3)\)-transformer \(/2\)-equivariant. A natural solution is to force all layers in the \(SE(3) SE(3)\)-transformer to be \(/2\)-equivariant. The following proposition provides a concrete way to achieve this.

**Proposition 5.2**.: _Let \(\) represent the swap of index, e.g., if \(=(o_{1},o_{2})\), then \(}=(o_{2},o_{1})\). 1) For a transformer layer (5), if the self-interaction weight satisfies \(W^{}=W^{}}\), the weight of query (7) satisfies \(W^{}_{Q}=W^{}}_{Q}\), and the radial function satisfies \(^{,}_{J_{1},J_{2}}(\|z^{1}\|,\|z^{2}\|)=^{, }}_{J_{2},J_{1}}(\|z^{2}\|,\|z^{1}\|)\) for all \(\), \(\), \(J_{1}\), \(J_{2}\), \(z^{1}\) and \(z^{2}\), then the transformer layer is \(/2\)-equivariant._

_2) For an Elu layer (9), if \(W^{}_{}=W^{}_{}\) and \(W^{}_{}=W^{}_{}\) for all \(\), then the Elu layer is \(/2\)-equivariant._

More details, including the complete matching property (Prop. C.11), can be found in Appx. C.3.1.

### Incorporating scale-equivariance

This subsection seeks to incorporate scale equivariance to BITR, _i.e._, to ensure that when \(X\) and \(Y\) are multiplied by a scale constant \(c_{+}\), the output result transforms correctly. To this end, we need to consider the scale group \((_{+},)\), _i.e._, the multiplicative group of \(_{+}\), and properly define the \((_{+},)\)-equivariance of the learned feature. For simplicity, we abbreviate group \((_{+},)\) as \(_{+}\).

We now consider the action of \(_{+}\) on field \(f\) (3). We call \(f\) a _degree-\(p\)\(_{+}\)-equivariant field_ (\(p\)) if it transforms as \((c(f))(z)=c^{p}f(c^{-1}z)\) under the action of \(_{+}\), where \(z^{6}\) and \(c_{+}\). We immediately observe that degree-1 \(_{+}\)-equivariant features lead to scale-equivariant output. Intuitively, if \(\), \(t_{X}\) and \(t_{Y}\) are degree-1 \(_{+}\)-equivariant features, then they will become \(c\), \(ct_{X}\) and \(ct_{Y}\) under the action of \(c\), and the projection step will cancel the scale of \(\) while keeping the scale of \(t_{X}\) and \(t_{Y}\), which is exactly the desirable results. Formally, we have the following proposition.

**Proposition 5.3**.: _Let \(_{P}\) be projection (13), \(f\) be a degree-\(1\)\(_{+}\)-equivariant tensor field, and \((r,t)=_{P}(f)\). We have \(_{P}(c(f))=(r,ct) c_{+}\)._

The remaining problem is how to ensure that a \(SE(3) SE(3)\)-transformer is \(_{+}\)-equivariant and its output is of degree-\(1\), so that scaling the input can lead to the proper scaling of output. Here we provide a solution based on the following proposition.

**Definition 5.4**.: \(:\) is a degree-\(p\) function if \((cx,cy)=c^{p}(x,y)\) for all \(c_{+}\).

**Proposition 5.5**.: _1) Denote \(_{K}\) and \(_{V}\) the radial functions used in \(\) and \(\) respectively. Let \(_{K}\) be a degree-\(0\) function, \(f_{in}\) be a degree-\(0\)\(_{+}\)-equivariant input field. For transformer layer (5), if \(_{V}\)is a degree-\(1\) function and the self-interaction weight \(W=0\), then the output field \(f_{out}\) is degree-\(1\)\(_{+}\)-equivariant; If \(_{V}\) is a degree-\(0\) function, then the output field \(f_{out}\) is degree-\(0\)\(_{+}\)-equivariant. 2) For Elu layer (9), if the input field is degree-\(p\)\(_{+}\)-equivariant, then the output field is also degree-\(p\)\(_{+}\)-equivariant._

More discussions can be found in Appx. C.4.

## 6 Experiments and analysis

This section experimentally evaluates the proposed BITR. After describing the experiment settings in Sec. 6.1, we first present a simple example in Sec. 6.2 to highlight the equivariance of BITR. Then we evaluate BITR on assembling the shapes in ShapeNet , BB dataset , 7Scenes  and ASL  from Sec. 6.3.1 to Sec. 6.4. We finally apply BITR to visual manipulation tasks in Sec. 6.6.

### Experiment settings

We extract \(L=32\) key points for each PC. The \(SE(3)\)-transformer and the \(SE(3) SE(3)\)-transformer both contain \(2\) layers with \(c=4\) channels. We consider \(k=24\) nearest neighborhoods for message passing. We only consider low degree equivariant features, _i.e_., \(p,q\{0,1\}\) for efficiency. We train BITR using Adam optimizer  with learning rate \(1e^{-4}\). We use the loss function \(L=\|r^{T}r_{gt}-I\|_{2}^{2}+\|t_{gt}-t\|_{2}^{2}\), where \((r,t)\) are the output transformation, \((r_{gt},t_{gt})\) are the corresponding ground truth. We evaluate all methods by isotropic rotation and translation errors: \( r=(180/)(1/2((rr_{gt}^{T})-1) ))\), and \( t=\|t_{gt}-t\|\) where _tr\(()\)_ is the trace of a matrix. We do not use random rotation and translation augmentations as . More details are in Appx. D.1.

### A proof-of-concept example

To demonstrate the equivariance property of BITR, we train BITR on the bunny shape . We prepare the dataset similar to : In each training iteration, we first construct the raw PC \(S\) by uniformly sampling \(2048\) points from the bunny shape and adding \(200\) random outliers from \([-1,1]^{3}\), then we obtain PCs \(\{X_{P},Y_{P}\}\) by dividing \(S\) into two parts of ratio \((30\%,70\%)\) using a random plane \(P\). We train BITR to reconstruct \(S\) using randomly rotated and translated \(\{X_{P},Y_{P}\}\). To construct the test set, we generate a new sample \(\{X_{},Y_{}\}\), and additionally construct \(3\) test samples by 1) swapping, 2) scaling (factor 2) and 3) randomly rigidly perturbing \(\{X_{P},Y_{P}\}\).

The assembly results of BITR on these four test samples are shown in Fig. 3. We observe that BITR performs equally well in all cases. Specifically, the differences between the rotation errors in these four cases are small (less than \(1e^{-3}\)). The results suggest that BITR is indeed robust against these three perturbations, which verifies its swap-equivariance, scale-equivariance and \(SE(3)\)-bi-equivariance. More experiments can be found in the appendix: a numerical verification of Def. 3.1 is presented in Appx. D.2, an ablation study of swap and scale equivariances are presented in Appx. D.3, and the verification of the complete-matching property C.11 is presented in Appx. D.4.

Figure 3: The results of BITR on a test example (a), and the swapped (b), scaled (d) and rigidly perturbed (c) inputs. The red, yellow and blue colors represent the source, transformed source and reference PCs respectively.

### Results on ShapeNet

#### 6.3.1 Single shape assembly

In this experiment, we evaluate BITR on assembling PCs sampled from a single shape. When the inputs PCs are overlapped, this setting is generally known as PC registration. We construct a dataset similar to : for a shape in the airplane class of ShapeNet , we obtain each of the input PCs by uniformly sampling \(1024\) points from the shape, and keep ratio \(s\) of the raw PC by cropping it using a random plane. We vary \(s\) from \(0.7\) to \(0.3\). _Note the PCs may be non-overlapped when \(s<0.5\)._

We compare BITR against the state-of-the-art registration methods GEO  and ROI , and the state-of-the-art fragment reassembly methods NSM  and LEV . For NSM and LEV, we additionally provide the canonical pose for each shape. Note that LEV and ROI are \(SE(3)\)-equivariant methods. For \(s 0.5\), we report the results of BITR fine-tuned by ICP  (BITR+ICP). Note that BITR+ICP is \(SE(3)\)-bi-equivariant and scale-equivariant, but not swap-equivariant.

We present the results in Fig. 4. We observe that the performance of all methods decrease as \(s\) decreases. Meanwhile, BITR outperforms all baseline methods when \(s\) is small (\(s 0.5\)). On the other hand, when \(s\) is large (\(s>0.5\)), BITR performs worse than GEO, but it still outperforms other baselines. Nevertheless, since the results of BITR are sufficiently close to optimum (\( r 20\)), the ICP refinement can lead to improved results that are close to GEO. More details can be found in Appx. D.5.

#### 6.3.2 Inter-class assembly

To evaluate BITR on non-overlapped PCs, we extend the experiment in Sec. 6.3.1 to inter-class assembly. We train BITR to place a car shape on the right of motorbike shape, so that their directions are the same and their distance is \(1\). We consider \(s=1.0\) and \(0.7\). Note that this task is beyond the scope of registration methods, since the input PCs are non-overlapped. A result of BITR is shown in Fig. 5. More details can be found in Appx. D.6.

### Results on fragment reassembly

This subsection evaluates BITR on a fragment reassembly task. We compare BITR against NSM , LEV  and DGL  on the \(2\)-fragment WineBottle class of the BB dataset . The data preprocessing step is described in Appx. D.7.

We test the trained BITR \(3\) times using different random samples, and report the mean and standard deviation of \(( r, t)\) in Tab. 1. We observe that BITR outperforms all baseline methods: BITR achieves the lowest rotation errors, and its translation error is comparable to DGL, which is lower than other baselines by a large margin. We provide some qualitative comparisons in Appx. D.7.

### Results on real data

This subsection evaluates BITR on an indoor dataset 7Scenes  and the outdoor scenes in ASL dataset . We present the results on 7Scenes in this section, and leave the results on ASL and some qualitative results to Appx. D.8.

For the 7Scenes dataset, we arbitrarily rotate and translate all frames, and train BITR to align all adjacent frames. We use the data from the first \(6\) scenes as the training set, and the data from \(7\)-th

   & \( r\) & \( t\) \\  DGL & 101.3 & 0.09 \\ NSM & 101 & 0.18 \\ LEV & 98.3 & 0.17 \\ BITR (Ours) & 8.4 (0.9) & 0.07 (0.008) \\  

Table 1: Reassembly results on \(2\)-fragment WineBottle. We report the mean and std of the error of BITR.

Figure 4: Assembly results on the airplane dataset. \(*\) denotes methods which require the true canonical poses of the input PCs.

Figure 5: A result of BITR on assembling a motorbike and a car.

scene as the test set. To train BITR, we use a random clipping augmentation similar to Sec. 6.3.1: we keep ratio \(s\) of each PCs by clipping them using a random plane, where \(s\) is uniformly distributed in \([0.5,1.0]\). We compare BITR against GEO , ROI , ICP  and OMN , where OMN is a recently proposed correspondence-free registration method.

The results on 7Scenes are reported in Tab. 2. We observe that BITR can produce results that are close to the optimum (\( r 25\)) from a random initialization (\( r U\)), and extra refinements like ICP and OT can further improve the results (\( r 10\)). This observation is consistent with that in Sec. 6.3.1. In particular, BITR with the OT refinement is comparable with GEO and ROI, which use highly complicated features specifically designed for registration tasks and an OT-like refinement process. On the other hand, ICP and OMN fails in this task due to their sensitiveness to initial positions.

### Results on visual manipulation

This subsection investigates the potential of BITR in manipulation tasks. Following , we consider two tasks: mug-hanging and bowl-placing. For both tasks, \(X\) represents an object grasped by a robotic arm, _i.e_., a cup or a bowl, \(Y\) represents the fixed environment with a target, _i.e_., a stand or a plate, and we train BITR to align \(X\) to \(Y\), so that the cup can be hung to the stand, or the bowl can be placed on the plate.

Fig. 6 presents the results of BITR on bowl-placing. We observe that although BITR is not originally designed for manipulation tasks, it can place the bowl in a reasonable position relative to the plate. However, we also notice that BITR may produce unrealistic results, _e.g_., the PCs may collide. Thus, post-processing steps  or extra regularizers  may be necessary in practical applications. More results and discussions can be found in Appx. D.9.

## 7 Conclusion

This work proposed a PC assembly method, called BITR. The most distinguished feature of BITR is that it is correspondence-free, \(SE(3)\)-bi-equivariant, scale-equivariant and swap-equivariant. We experimentally demonstrated the effectiveness of BITR.

BITR in its current form has two main limitations. First, BITR is computationally inefficient because each degree of feature is computed independently without parallel. This issue was also observed in SE(3)-equivariant networks, and was recently addressed by . A promising future research direction is to develope similar acceleration techniques for BITR. Second, since BITR is deterministic, _i.e_., it only predicts one result for a given input, it cannot handle symmetric PCs. Although this feature does not cause any difficulty in this work (there is no strictly symmetric PCs in this work due to noise, random sampling, etc), it may be problematic in future applications such as molecule modelling where symmetric PCs exist, _e.g_., benzene rings. To address this issue, we plan to generalize BITR to a generative model in the future. More discussions can be found in Appx. E.

   & \( r\) & \( t\) \\  ICP & 73.2 (5.7) & 2.4 (0.2) \\ OMN & 129.02 (2.15) & 0.98 (0.06) \\ GEO & 9.2 (0.02) & 0.2 (0.08) \\ ROI & 9.0 (0.0) & 0.2 (0.0) \\ BITR (Ours) & 26.7 (0.0) & 0.8 (0.0) \\ BITR+ICP (Ours) & 11.1 (0.0) & 0.3 (0.0) \\ BITR+OT (Ours) & 9.5 (0.0) & 0.3 (0.0) \\  

Table 2: Results on 7Scenes. We report mean and std of \( r\) and \( t\).

Figure 6: The results of BITR on bowl-placing. We present the input PCs (left panel) and the assembled results (right panel). BITR can generally place the bowl (red) on the plate (green) (a), but it sometimes produces unrealistic results where collision exists (b).