# Queueing Matching Bandits with Preference Feedback

Jung-hun Kim

Seoul National University

Seoul, South Korea

junshunkim@snu.ac.kr&Min-hwan Oh

Seoul National University

Seoul, South Korea

minoh@snu.ac.kr

###### Abstract

In this study, we consider multi-class multi-server asymmetric queueing systems consisting of \(N\) queues on one side and \(K\) servers on the other side, where jobs randomly arrive in queues at each time. The service rate of each job-server assignment is unknown and modeled by a feature-based Multi-nomial Logit (MNL) function. At each time, a scheduler assigns jobs to servers, and each server stochastically serves at most one job based on its preferences over the assigned jobs. The primary goal of the algorithm is to stabilize the queues in the system while learning the service rates of servers. To achieve this goal, we propose algorithms based on UCB and Thompson Sampling, which achieve system stability with an average queue length bound of \((\{N,K\}/)\) for a large time horizon \(T\), where \(\) is a traffic slackness of the system. Furthermore, the algorithms achieve sublinear regret bounds of \(}(\{Q_{},T^{3/4}\})\), where \(Q_{}\) represents the maximum queue length over agents and times. Lastly, we provide experimental results to demonstrate the performance of our algorithms.

## 1 Introduction

Multi-class multi-server queueing systems, which have been extensively studied by , are motivated by real-world applications such as ride-hailing platforms, where riders are assigned to drivers. Other examples are online job markets, where applicants are recommended for employment, and online labor service markets, where tasks are recommended to freelance workers. In these systems, there are two sides: queues (agents) on one side and servers (arms) on the other. At each time step, jobs stochastically arrive in multiple queues and are then assigned to multiple servers by a matchmaking or scheduler algorithm to stabilize the systems. Importantly, the previous work assumes that the service rates for each server are known for scheduling jobs.

However, real-world observations reveal that the service rate may not be known beforehand. Therefore, learning the service rates associated with the stochastic behavior of servers is essential for real-world applications. Furthermore, the behavior of servers may depend on their (relative) preferences over assigned jobs. For example, in ride-hailing platforms where there are riders and drivers, the preferences of drivers over riders (not necessarily based on personal information but rather a rider's pick-up location, destination, etc.) may not be known in advance to riders or even the systems, necessitating the learning of drivers' preferences through preference feedback over assigned riders.

Learning service rates in an online manner, based on partial feedback from each scheduling, is highly associated with multi-armed bandit problems . These problems are fundamental sequential learning tasks where, for queueing systems, a job is assigned to a server and receives feedback on whether the job is served or not. The utilization of bandit strategies for queueing systems has been recently studied by Choudhury et al. , Stahlbuhk et al. , Gaitonde and Tardos ; Freund et al. , Hsu et al. , Krishnasamy et al. , Sentenac et al. , Yang et al. , Huang et al. , which primary focus is naturally on establishing stability of the systems while learning service rates.

However, there are still gaps between the previous models and the real-world applications. In all previous work, the inherent service rates for each job-server assignment are determined regardless of other jobs assigned to the same server (or it is not allowed to assign multiple jobs to the same server). This differs from real-world scenarios where service rate may depend on (relative) preference over multiple assigned jobs, as seen in online labor markets or ride-hailing platforms. Additionally, in Freund et al. , Krishnasamy et al. , which is closely related work to our study regarding multi-class multi-server queueing systems with asymmetric service rates, it was allowed to assign a job in an empty queue to a server to obtain feedback (null request), which may not be realistic. Furthermore, all of the previous work considers a simple model without a generalizable structure or utilizing features for service rates.

Another line of related work is matching bandits, in which there are two sides (agents and arms), and the behavior of arms is based on their preferences . However, their focus is on static settings, whereas our work considers dynamic job arrivals in queues. Additionally, it is worth mentioning online matching problems , where the sole focus is on optimizing matching rather than learning underlying models from bandit feedback. In contrast, our study concentrates on bandit problems related to learning latent utilities by managing the tradeoff between exploration and exploitation while establishing the stability of the queueing systems. Importantly, neither previous work on matching bandits nor online matching problems addresses the stability of queueing systems, which is our primary focus.

In this work, we propose a novel and practical framework for queueing matching bandits under preference feedback. In the following, we describe our setting accompanied by an illustration in Figure 1. (a) Multiple queues (agents) and servers (arms) are involved, with jobs arriving randomly in queues. Additionally, there are unknown utility values between queues and servers regarding preferences. (b) Nonempty queue are then assigned to servers based on a scheduling policy. (c) Subsequently, each server stochastically accepts at most one of the assigned queues based on its unknown preference and serves one unit of job of the accepted queue. The rejected jobs remain in the queues. The processes of (a), (b), and (c) are repeated over time.

**Summary of Our Contributions**

* We propose a novel and practical framework for queueing matching bandits, where \(N\) agents and \(K\) arms are involved, and jobs randomly arrive in agents' queues each round. Subsequently, a scheduler assigns agents to arms, and the service rates for the assigned agents depend on the arms' unknown preferences over them. These service rates are modeled using a feature-based multinomial logit function. To the best of our knowledge, our paper is the first to investigate either a feature-based service function or preference feedback using a multinomial logit function for service rates.
* We propose algorithms based on Upper Confidence Bound (UCB) and Thompson Sampling (TS), which achieve stability with average queue length bounds of \((}{})\) for a large time horizon \(T\), under a traffic slackness of \(\).

Figure 1: Illustration of queueing process with 4 queues/agents (\(N=4\)) and 3 servers/arms (\(K=3\))

* Furthermore, the algorithms achieve sublinear regret bounds of \(}(\{Q_{},T^{3/4}\})\), where \(Q_{}\) represents the maximum queue length over agents and times. It is worth mentioning that regret analysis has not been studied in the closely related queueing bandit literature such as Sentenac et al. , Freund et al. , Yang et al. .
* Finally, we present experimental results demonstrating the stability and regret performance of our algorithms with comparison to previously suggested methods.

## 2 Related Work

Bandits for Queues.Learning the unknown service rates for queueing systems in an online manner under bandit feedback has recently gained widespread attention [12; 47; 31; 48; 22; 32; 46; 52; 23]. While Stahlbuhk et al. , Krishnasamy et al. , Choudhury et al.  focused on a single queue with multiple servers, aiming to efficiently assign the queue to the optimal server while learning the service rates, subsequent study of Krishnasamy et al.  expanded this scope to encompass multiple queues and multiple servers. However, this extension relied on a strong structural assumption that each agent possesses a unique and distinctly optimal server without sharing the same optimal server. Consequently, the algorithms employed in such scenarios do not necessarily consider the dynamic queue lengths when scheduling jobs; instead, they concentrate solely on learning service rates to achieve optimal matching. As the closest work to our study, another line of work of Sentenac et al. , Freund et al. , Yang et al.  has focused more on dynamic queue lengths for multi-queue and multi-server scenarios without assuming the unique and distinct optimal servers for each agent. In this context, all of the previous work aimed to achieve queue length stability.

However, to the best of our knowledge, none of the previous works has focused on a structured model with features for service rates. Furthermore, previous studies either did not allow the assignment of different types of jobs to a server simultaneously [32; 52], or, if they did, as in Sentenac et al. , Freund et al. , the inherent values of service rates for each job-server assignment remained constant regardless of the entire assignments. Additionally, the server's behavior given multiple assigned jobs was either simple or deterministic, based on the uniform-randomly selected job among the assigned jobs , or the job with the highest bid generated from the algorithm , respectively. However, these approaches may not reflect real-world scenarios, where the service rates depend on the relative preferences among the assigned jobs. Lastly, in [15; 32], it was allowed to assign a job to a server from an empty queue for obtaining feedback, which may not be realistic.

In our study, we adopt a structured model incorporating features for unknown asymmetric service rates. We enable the assignment of multiple agents to the same server, where the server stochastically selects at most one of the assigned agents based on its undisclosed (relative) preference. Also, the assignment of an agent is available only when the queue of the agent is not empty. This scenario mirrors common situations in real-world applications such as ride-hailing platforms and online labor markets where available multiple riders or tasks can be assigned to a driver or a worker, respectively, and the driver or worker behaves stochastically depending on its unknown preference.

It is noteworthy that we focus not only on the stability of the systems regarding queue lengths but also on regret against an oracle, while the closely related works by [46; 15; 52], which considered multi-class multi-server queueing systems, only addressed stability analysis. Regret analysis has only been studied under some limited scenarios, such as a single-server , a single-queue , or the strong assumption that each agent has a unique and well-separated optimal server .

Matching Bandits.Next, we investigate two-sided matching bandits, a topic initially explored by Liu et al.  and subsequently studied by Sankararaman et al. , Liu et al. , Basu et al. , Zhang et al. , Kong and Li . The primary goal is to minimize regret by attaining an optimal stable matching by learning agents' side preferences through stochastic reward feedback under static settings and deterministic behavior of arms with known preferences. Our study aligns with the model, wherein agents select arms based on preferences. However, our study sets itself apart from prior work on matching bandits in several key aspects. Firstly, we consider dynamic environments with job arrivals for agents. Secondly, we propose that arm behavior is stochastic, with unknown preferences, necessitating the learning of arms' preferences. Lastly, our main target is to stabilize the queue lengths in the systems rather than find stable matching for stable marriage problems.

MNL Bandits.Lastly, we examine MNL bandits which were initially proposed by Agrawal et al.  and followed by Agrawal et al. , Chen et al. , Oh and Iyengar [43; 44]. In MNL bandits,the goal is to select assortments of arms to maximize reward, which is based on preferences over the arms in the selected assortment. In our study, we adopt the MNL model for arms' choice preferences in dynamic systems, which, to the best of our knowledge, is the first consideration of bandits for queueing systems.

## 3 Problem Statement

There are \(N\) agents (queues) and \(K\) arms (servers). At each time, a job for each agent \(n[N]\) arrives randomly following a Bernoulli distribution with an unknown arrival rate \(_{n}\)1. Then at each time \(t[T]\) where \(T\) is the time horizon, each agent \(n[N]\) is assigned to an arm \(k_{n,t}^{}[K]\) by a policy \(\). For notational simplicity, we use \(k_{n,t}\) for \(k_{n,t}^{}\) when there is no confusion. Let \(Q_{n}(t)\) be the length of the queue for jobs of agent \(n[N]\) at the beginning of time slot \(t\) in the system. We consider that at most \(L\) agents can be assigned to each arm \(k[K]\) at each time and \(N KL\) to ensure that all agents can be assigned to arms. Then, we define the set of agents (assortment) assigned to arm \(k\) by policy \(\) at time \(t\) as \(S_{k,t}=\{n[N]:k_{n,t}=k,Q_{n}(t) 0\}\), considering only available agents with nonempty queues.

Now we explain the structure of our model. Each agent \(n\) has known \(d\)-dimensional feature information of \(x_{n}^{d}\), and each arm \(k\) has latent (unknown) parameter \(_{k}^{d}\) for preference utilities over agents. We adopt the Multi-nomial Logit (MNL) function commonly studied for preference feedback . Then given assortment \(S_{k,t}\), arm \(k\) serves a job of agent \(n S_{k,t}\) with a service probability (service rate) determined by the MNL model as

\[(n|S_{k,t},_{k})=^{}_{k})}{1+_{m S_{ k,t}}(x_{m}^{}_{k})}.\]

We note that \((n|S_{k,t},_{k})\) represents the preference of arm \(k\) to agent \(n\) over the assigned agents \(S_{k,t}\). The service rate of each \(n\) depends on the assigned agents rather than static by reflecting real-world scenarios, which is different from the conventional queueing problems. Following the MNL function, the arm is allowed not to serve any agents (or allowed to serve null agent \(n_{0}\)) with the probability of \((n_{0}|S_{k,t},_{k})=}(x_{m}^{} _{k})}\). Under the MNL model for the service rate, at each time, each arm accepts at most one agent and serves that agent's job. The queue lengths for the accepted agents are each reduced by one, while the queue lengths for the refused agents remain the same.

Objective Function.Here we provide the goal of this problem. For describing the stochastic process in the systems, at each time \(t\), let \(A_{n}(t)\{0,1\}\) be a random variable with mean \(_{n}\), which denotes whether a new job arrives in the queue of agent \(n[N]\) (here, \(1\) denotes arrival). Also, given that \(n\) is assigned to arm \(k_{n,t}\), let \(D_{n}(t|S_{k_{n,t},t})\{0,1\}\) be a random variable with mean \((n|S_{k_{n,t},t},_{k_{n,t}})\), which represents whether the assigned agent \(n\), given \(S_{k_{n,t},t}\), is accepted by arm \(k_{n,t}\) (here \(1\) denotes acceptance). Then the queue length of the agent \(n\) evolves as \(Q_{n}(t+1)=(Q_{n}(t)+A_{n}(t)-D_{n}(t|S_{k_{n,t},t}))^{+}\) where \(x^{+}\) denotes \(\{x,0\}\) for \(x\).

As in the previous work of queueing bandits for multiple types of agents and servers , which are closely related work to ours, our primary focus is on stabilizing the dynamic systems regarding queue lengths. For analyzing the stability of the systems, we define the average queue lengths over horizon time \(T\) as

\[(T)=_{t[T]}_{n[N]}[Q_{n}(t )].\]

Then the goal of this problem is to design an online matching algorithm to assign the agents to arms for stabilizing the systems by bounding \((T)\). We define the system stability as follows.

**Definition 1**.: _The systems are denoted to be stable when \(_{T}(T)<\)._The same stability definition was considered in Neely [41; 42], Freund et al. , Yang et al. , Huang et al. . It is noteworthy that according to [42; 41; 15], the system satisfying this stability condition of Definition 1 is denoted to be strongly stable.2

For the analyses, we first present the regularity conditions.

**Assumption 1**.: \(\|x_{n}\|_{2} 1\) _for all \(n[N]\) and \(\|_{k}\|_{2} 1\) for all \(k[K]\)._

**Assumption 2**.: _There exists \(>0\) such that \(_{^{d}:\|\|_{2} 1}(n|S,)(n_{0}|S, )\) for any \(n S\) and \(S[N]\)._

These regularity conditions are commonly taken into account in the logistic and MNL bandit literature [13; 3; 43; 44]. We note that, in the worst-case, \(1/=O(L^{2})\).

We define the set of feasible disjoint assortments given any \([N]\) as \(()=\{(S_{1},...,S_{K}):S_{k},|S_{k} | L\;\; k[K],S_{k} S_{l}=\; k l, _{k[K]}S_{k}=\}\).

Then, we consider the condition of traffic slackness for stability as follows.

**Assumption 3**.: _For some traffic slackness \(0<<1\), there exists \(\{S_{k}\}_{k[K]}([N])\) such that \(_{n}+(n|S_{k},_{k})\) for all \(n S_{k}\) and \(k[K]\)._

We note that similar slackness assumptions have been commonly considered in queueing bandits [15; 52; 23]. As \(\) decreases, achieving stability in our setting becomes more challenging. A discussion for a refined version of \(\) can be found in Appendix A.2.

## 4 Preliminary Study: An Oracle Algorithm of MaxWeight

For queueing systems, MaxWeight  has been proven to have optimal throughput keeping the queues in networks stable under the known service rates [51; 38; 37; 49]. Here, we analyze the oracle policy using MaxWeight under known \(_{k}\)'s. We define the set of agents having non-empty queues at time \(t\) as \(_{t}=\{n[N]:Q_{n}(t) 0\}\). Then, the oracle policy using MaxWeight in our setting is defined as \(\{S_{k,t}\}_{k[K]}=*{argmax}_{\{S_{k}\}_{k[K]} (_{t})}_{k[K]}_{n S_{k}}Q_{n}(t)(n|S_{ k},_{k}),\) where priority in the assortment scheduling is on queues with either large queue lengths or high service rates. We provide an analysis of stability of the MaxWeight oracle for known \(_{k}\)'s as follows.

**Proposition 1**.: _Given the prior knowledge of \(_{k}\) for all \(k[K]\), the average queue length of MaxWeight is bounded as \((T)=(}{})\), which implies that the algorithm achieves stability._

Proof.: The proof is provided in Appendix A.3 

We note that the term of \(\{N,K\}\) in the average queue length is a result of the system's variance. Additionally, as \(\) decreases, the stability of the system deteriorates due to the reduced traffic slackness. Further discussion regarding \(\) can be found in Appendix A.2.

Since the oracle algorithm requires prior knowledge of \(_{k}\)'s for the service rate, we cannot use it directly in our bandit setting. In the following section, we propose algorithms incorporating an efficient learning procedure to achieve stability in our setting.

## 5 Algorithms and Analyses

### UCB-based Algorithm

We first propose an algorithm based on the UCB strategy, UCB-QMB (Algorithm 1). We define the negative log-likelihood as \(f_{k,t}():=-_{n S_{k,t}\{n_{0}\}}y_{n,t}(n|S_{k,t},)\) where \(y_{n,t}\{0,1\}\) is observed preference feedback (\(1\) denotes service acceptance, and \(0\) denotes refusal) and define the gradient of the likelihood as

\[g_{k,t}():=_{}f_{k,t}()=_{n S_{k,t}}((n|S_{k,t},)-y_{n,t})x_{n}.\] (1)Then, we construct the estimator of \(_{k,t}\) from online updates applying online newton step studied by  as \(_{k,t}=*{argmin}_{}g_{k,t-1}(_{k,t-1})^{}+\|-_{k,t-1}\|_{V_{k, t}}^{2}\), where \(=\{^{d}:\|\|_{2} 1\}\) and \(V_{k,t}= I_{d}+_{s=1}^{t-1}_{n S_{k,s}}x_{n}x _{n}^{}\). Using the estimator, we define the UCB index for agent \(n\) in assortment \(S_{k}\) as

\[_{t}^{UCB}(n|S_{k},_{k,t})=^{UCB })}{1+_{m S_{k}}(h_{m,k,t}^{UCB})},\] (2)

where \(h_{n,k,t}^{UCB}:=x_{n}^{}_{k,t}+_{t}\|x_{n}\|_{V_{k,t}^{- 1}}\) with \(_{t}=C_{1}(1+)}\) for some constant \(C_{1}>0\). We utilize the MaxWeight with the UCB indexes by setting \(=1\).

``` Input:\(\), \(\), \(C_{1}>0\) for\(t=1,,T\)do for\(k[K]\)do \(_{k,t}*{argmin}_{}g_{k,t-1}( _{k,t-1})^{}+\|-_{k,t-1}\| _{V_{k,t}}^{2}\) with (1) \(\{S_{k,t}\}_{k[K]}*{argmax}_{\{S_{k}\}_{k[K]} (_{i})}_{k[K]}_{n S_{k}}Q_{n}(t) _{t}^{UCB}(n|S_{k},_{k,t})\) with (2) \(\{S_{k,t}\}_{k[K]}\) and observe preference feedback \(y_{n,t}\{0,1\}\) for all \(n S_{k,t}\), \(k[K]\) ```

**Algorithm 1** UCB-Queueing Matching Bandit (UCB-QMB)

#### 5.1.1 Stability Analysis of Ucb-Qmb

Here we provide an analysis for the stability of UCB-QMB (Algorithm 1).

**Theorem 1**.: _The average queue length of Algorithm 1 is bounded as \((T)=(}{}+N^{2}K^ {2}}{^{4}^{6}}(T)}{T}),\) which implies that the algorithm achieves stability as_

\[_{T}(T)=(}{ }).\]

We note that Algorithm 1 achieves the same average queue length bound of \((}{})\) with the oracle of MaxWeight (Proposition 1) when \(T\) is large enough.

Proof sketch.: Here we provide a proof sketch and the full version is provided in Appendix A.4. We define the set of queues \((t)=[Q_{n}(t):n[N]]\) and a Lyapunov function as \(((t))=_{n[N]}Q_{n}(t)^{2}\). For simplicity, we use \(D_{n}(t)\) for \(D_{n}(t|S_{k_{n,t},t})\) and \(D_{n}^{*}(t)\) for \(D_{n}(t|S_{k_{n,t},t})\) when there is no confusion. Then we analyze the Lyapunov drift as follows:

\[_{t[T]}((t+1))-((t))\] \[=_{t[T]}_{n[N]}(Q_{n}(t)+A_{n}(t)-D_{n}^{*}(t) )^{2}-Q_{n}(t)^{2}\] \[+_{t[T]}_{n[N]}(Q_{n}(t)+A_{n}(t)-D_{n}(t ))^{+2}-_{t[T]}_{n[N]}(Q_{n}(t)+A_{n}(t)-D_{n}^{*}(t ))^{2}.\] (3)

For the first two terms in Eq.(3), with Assumption 3, we can show that

\[_{t[T]}_{n[N]}[(Q_{n}(t)+A_{n}(t)-D_{n}^{*}(t) )^{2}-Q_{n}(t)^{2}]-_{t[T]}_{n[N]}2[Q_{n}(t)]+2\{N,K\}T,\] (4)

For the last two terms in Eq.(3), we have

\[_{t[T]}_{n[N]}(Q_{n}(t)+A_{n}( t)-D_{n}(t))^{+2}-_{t[T]}_{n[N]}(Q_{n}(t)+A_{n}(t)-D_{n}^{*}( t))^{2}\] \[ 2_{t[T]}_{n[N]}((n|S_{k_{n,t },t},_{k_{n,t}^{*}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t) +5\{N,K\}T,\] (5)where the first term of the last inequality is closely related to the regret analysis of the bandit strategy. In the following, we focus on analyzing the last term in Eq.(5). We define events \(E^{1}_{t}=\{\|_{k,t}-^{*}_{k}\|_{V_{k,t}}_{t}k[K]\}\) and \(E^{2}_{n,t}=\{_{m S_{k,t}}\|x_{m}\|_{V^{-1}_{k,t}} C_{2}/2 _{t}k=k^{}_{n,t}\}\) for some constant \(C_{2}>0\). We can show that \(E^{1}_{t}\) holds with high probability so here we only consider the case when \(E^{1}_{t}\) holds. Then we have

\[_{t[T]}_{n[N]}(n|S_{k^{*}_{n,t},t},_{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t)\] \[_{t[T]}_{n[N]}[((n|S_{k^{*}_{n,t},t},_{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t)( (E^{2}_{n,t})+((E^{2}_{n,t})^{c}))].\] (6)

Then for the first term of Eq.(6), from the UCB strategy and \(E^{2}_{n,t}\), we can show that

\[_{t[T]}_{n[N]}((n|S_{k^{*}_{n,t },t},_{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t) (E^{2}_{n,t}) C_{2}_{t[T]}_{n[N]} [Q_{n}(t)].\] (7)

Now we provide a bound for the second term of Eq.(6). For some constant \(C_{3}>0\), we can show that

\[_{t[T]}_{n[N]}[((n|S_{k^{*}_{n,t},t}, _{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t)((E^{2}_{n,t})^{c})]\] \[_{t[T]}_{n[N]}(/C_{3})[Q _{n}(t)]+(K^{2}_{T}^{4}}{^{2} ^{5}}).\] (8)

By putting the results of Eqs. (3), (4), (5), (6), (7), (8) altogether, we can obtain

\[_{t[T]}((t+1))- ((t))\] \[ 7\{N,K\}T+2(C_{2}+(1/C_{3})-1)_{t[T]}_{ n[N]}[Q_{n}(t)]+(N(T))+( K^{2}_{T}^{4}}{^{2}^{5}}).\]

Finally, with positive constants \(C_{2},C_{3}>0\) satisfying \(C_{2}+(1/C_{3})<1\), from \(((1))=0\) and \(((T+1)) 0\), by using telescoping for the above inequality and rearrangement, we can conclude the proof by \(_{t[T]}_{n[N]}[Q_{n}(t)]=(}{}+N^{2}K^{2}(T)}{^{2} ^{6}}).\) 

#### 5.1.2 Regret Analysis of Ucb-Qmg

In addition to the stability analysis, we examine the cumulative regret of UCB-QMB (Algorithm 1). The regret is defined as the discrepancy between the performance of the oracle policy of MaxWeight \(^{*}\), which operates with the knowledge of the true parameters \(_{k}\)'s, and that of our policy \(\). Given the queue lengths at each time \(t\), we denote the oracle assignments as

\[\{S^{*}_{k,t}\}_{k[K]}=*{argmax}_{\{S_{k}\}_{k[K]} (_{t})}_{k[K]}_{n S_{k}}Q_{n}(t)(n|S_{k },_{k}).\]

We show that this oracle policy achieves stability in Proposition 1. For simplicity, we use \(k^{*}_{n,t}\) for \(k^{^{*}}_{n,t}\). Then, the cumulative regret under \(\) is defined as

\[^{}(T)=_{t[T]}_{n[N]}[((n|S^{*} _{k^{*}_{n,t},t},_{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_ {n}(t)].\] (9)

We define \(Q_{}=[_{t[T],n[N]}Q_{n}(t)]\). Then the algorithm achieves the following regret bound.

**Theorem 2**.: _The policy \(\) of Algorithm 1 achieves a regret bound of_

\[^{}(T)=}Q_{},^{3}}{^{2}^{ 3}}^{1/4}T^{3/4}}.\]We emphasize that our algorithms achieve a sublinear regret bound, even in the worst-case scenario regarding queue lengths from the minimum in regret. In contrast,  achieves a regret bound of \((\{Q_{},T^{3/4}\})\) for a stationary setting, where the worst-case bound is not guaranteed to be sublinear from the maximum in regret.

Proof sketch.: Here we provide a proof sketch and the full version is provided in Appendix A.5. We first provide the proof for regret bound of \(^{}(T)=}(Q_{})\). We define event \(E^{1}_{t}=\{\|_{k,t}-^{*}_{k}\|_{V_{k,t}}_{t}\  k[K]\}\) which holds with a high probability. Therefore, here we only consider the case when \(E_{t}\) holds. Then we can show that \(_{n[N]}[Q_{n}(t)((n|S_{k^{*}_{n,t}},_{k^{*}_{n,t}})- (n|S_{k_{n,t},t},_{k_{n,t}}))]_{n[N]}[Q_{n}(t)( ^{UB}_{t}(n|S_{k_{n,t},t})-(n|S_{k_{n,t}},_{k_{n,t}}))] _{k[K]}[2_{t}_{n S_{k,t}}\|x_{n}\|_{V^{-1}_{ k,t}}Q_{n}(t)]\). From the inequality, with \(_{t=1}^{T}_{n S_{k,t}}\|x_{n}\|_{V^{-1}_{k,t}}(4d/)( 1+(TL/d))\), we have

\[^{}(T) =_{t[T]}_{n[N]}[Q_{n}(t)((n|S_{k^{*}_{ n,t},t},_{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))]\] \[ 2_{t[T]}_{n[N]}Q_{n}(t)_{T }_{k[K]}_{l S_{k,t}}\|x_{l}\|_{V^{-1}_{k,t }}^{2}}=}Q_{ }.\] (10)

Now we provide the proof for the worst-case regret bound of \(^{}(T)=}((^ {3}}{^{2}^{3}})^{1/4}T^{3/4})\) in the following. We additionally define event \(E^{2}_{n,t}=\{_{m S_{k,t}}\|x_{m}\|_{V^{-1}_{k,t}}k=k_{n,t}\}\) for some constant \(C_{2}>0\). Under \(E^{1}_{t}\), we have

\[^{}(T) =_{t[T]}_{n[N]}[((n|S_{k^{*}_{n,t},t}, _{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t)]\] \[_{t[T]}_{n[N]}[((n|S_{k^{*}_{n,t}, t},_{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t)( (E^{2}_{n,t})+((E^{2}_{n,t})^{c}))].\] (11)

Then for the first term of Eq.(11), we have

\[_{t[T]}[_{n[N]}((n|S_{k^{*}_{n,t },t},_{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t) (E^{2}_{n,t})]\] \[_{t[T]}[_{n[N]}2_{t}\|x_{n} \|_{V^{-1}_{k_{n,t},t}}Q_{n}(t)(E^{2}_{n,t})]_{t[ T]}_{n[N]}2_{t}[Q_{n}(t)],\] (12)

where the last inequality is obtained from \(E^{2}_{n,t}\). By analyzing the selected number of agent \(n\) with \((E^{2}_{n,t})^{c}\), we can show that

\[_{t[T]}_{n[N]}[((n|S_{k^{*}_{n,t},t}, _{k^{*}_{n,t}})-(n|S_{k_{n,t},t},_{k_{n,t}}))Q_{n}(t) ((E^{2}_{n,t})^{c})]\] \[_{t[T]}_{n[N]}_{T}[Q_ {n}(t)]+(_{T}}).\] (13)

By putting the results of Eqs. (11), (12), (13), and Theorem 1, by setting \(=( NK/\{N,K\}T_{T}^{2})^{1/4}\), for large enough \(T\), we have

\[^{}(T) =(_{T}_{t[T]}_{n[N]} [Q_{n}(t)]+_{T}}+N(T))\] \[=}(N(^{3}}{ ^{2}^{3}})^{1/4}T^{3/4}),\] (14)

which conclude the proof combined with Eq.(10).

### Thompson Sampling-based Algorithm

Here, we propose an algorithm based on Thompson Sampling, TS-QMB (Algorithm 2). As in the previous algorithm, we construct the estimator as \(_{k,t}=*{argmin}_{}g_{k,t-1}(_{k,t-1})^{}+\|-_{k,t-1}\|_{V_{k,t}}^{2}\). To facilitate exploration, we sample several \(_{k,t}^{(i)}\) for \(i[M]\) from a Gaussian distribution of \((_{k,t},_{t}^{2}V_{k,t}^{-1})\) and construct the Thompson Sampling (TS) index for assortment \(S_{k}\) as

\[_{t}^{TS}(n|S_{k},\{_{k,t}^{(i)}\}_{i[M]} )=^{TS})}{1+_{m S_{k}}(h_{m,k,t}^{TS})},\] (15)

where \(h_{n,k,t}^{TS}=_{i[M]}x_{n}^{}_{k,t}^{(i)}\) and \(_{t}=C_{1}(1+)}\) for some constant \(C_{1}>0\). Then we utilize the MaxWeight with TS indexes. We set \(=1\) and \(M= 1-)}\).

#### 5.2.1 Stability Analysis of TS-QMB

Here we provide stability analysis for TS-QMB (Algorithm 2).

**Theorem 3**.: _The average queue length of Algorithm 2 is bounded as \((T)=(}{}+N^{2}K^ {2}}{^{2}^{6}}}(T)}{T}),\) which implies that the algorithm achieves stability as_

\[_{T}(T)=(}{} ).\]

Proof.: The proof is provided in Appendix A.6 

#### 5.2.2 Regret Analysis of TS-QMB

We provide a regret analysis of TS-QMB (Algorithm 2) for the regret definition of (9) in the following.

**Theorem 4**.: _The policy \(\) of Algorithm 2 achieves a regret bound of_

\[^{}(T)=}(\{}{ }Q_{},(NK\{N,K\}^{3}}{^{2} ^{3}})^{1/4}T^{3/4}\}).\]

Proof.: The proof is provided in Appendix A.7 

We note that the performance of Algorithm 1 and Algorithm 2 in the analysis results shows similar trends. However, the TS-based Algorithm 2 incurs a loss with respect to \(d\) compared to the UCB-based Algorithm 1, as commonly seen in previous TS-based algorithms .

Here, we briefly discuss the combinatorial optimization of \(*{argmax}_{\{S_{k}\}_{k[K]}(_{t})} _{k[K]}f_{k}(S_{k})\) for some function \(f_{k}:S[N]\) in our algorithms. The exact optimization can be expensive due to its NP-hard nature. To address this, we can utilize the technique of \(\)-approximation oracle with \(0 1\), first introduced in Kakade et al. , which is deferred to Appendix A.9.

## 6 Experiments

Here, we provide experimental results to demonstrate the performance of our algorithms.3 For the synthetic experiments, we consider \(N=4\), \(K=2\), \(L=2\), and \(d=2\). Each element in \(x_{n}\) and \(_{k}\) is uniformly generated from \(\) and then normalized, and \(_{n}\)'s are determined with \(=0.1\). Even though no dedicated benchmark exists for our queueing matching scenario, we compare our algorithms with previously suggested ones for queueing bandits or matching bandits: Q-UCB, DAM-UCB, and MaxWeight-UCB for multi-queue multi-server bandits with asymmetric service rates and ETC-GS for matching bandits. In Figure 2, we can observe that our algorithms (Algorithms 1 and 2) outperform the previously suggested ones except for the oracle (MaxWeight) operated under known (latent) service rates. We demonstrate that our algorithms achieve stability, similar to the oracle in the left figure, which matches the results of our stability analysis (Theorems 1 and 3). Regarding regret shown in the right figure, the previously suggested algorithms exhibit superlinear performance due to the increasing \(Q_{n}(t)\), while our algorithms show relatively small regret (Theorems 2 and 4). Additional experiments can be found in Appendix A.10.

## 7 Conclusion

In this paper, we introduce a novel framework for queueing matching bandits with preference feedback. To achieve stability in this framework, we propose UCB and TS-based algorithms utilizing the MaxWeight strategy. The algorithms achieve system stability with an average queue length bound of \((\{N,K\}/)\). Furthermore, the algorithms achieve sublinear regret bounds of \(}(\{Q_{},T^{3/4}\})\). Lastly, we demonstrate our algorithms using synthetic datasets.

## 8 Acknowledgements

The authors thank Milan Vojnovic for helpful discussions and Hyunjun Choi for providing useful code. JK was supported by the Global-LAMP Program of the National Research Foundation of Korea (NRF) grant funded by the Ministry of Education (No. RS-2023-00301976). MO was supported by the NRF grant funded by the Korea government(MSIT) (No. 2022R1C1C1006859 and 2022R1A4A1030579) and by AI-Bio Research Grant through Seoul National University.