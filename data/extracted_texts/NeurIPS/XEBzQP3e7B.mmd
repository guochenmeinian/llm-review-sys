# GAIA: Delving into Gradient-based Attribution

Abnormality for Out-of-distribution Detection

 Jinggang Chen\({}^{}\)1, Junjie Li\({}^{}\)1, Xiaoyang Qu\({}^{}\)2, Jianzong Wang\({}^{}\)3, Jiguang Wan\({}^{}\)3, Jing Xiao\({}^{}\)

\({}^{}\) Huazhong University of Science and Technology, China

\({}^{}\) Ping An Technology (Shenzhen) Co., Ltd.

{chen.jinggang98, 2216217669ljj, quxiaoy}@gmail.com, jzwang@188.com, jgwan@hust.edu.cn, xiaojing661@pingan.com.cn

Equal Contribution.Corresponding Author.

###### Abstract

Detecting out-of-distribution (OOD) examples is crucial to guarantee the reliability and safety of deep neural networks in real-world settings. In this paper, we offer an innovative perspective on quantifying the disparities between in-distribution (ID) and OOD data--analyzing the uncertainty that arises when models attempt to explain their predictive decisions. This perspective is motivated by our observation that gradient-based attribution methods encounter challenges in assigning feature importance to OOD data, thereby yielding divergent explanation patterns. Consequently, we investigate how attribution gradients lead to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection: the zero-deflation abnormality and the channel-wise average abnormality. We then propose **GAIA**, a simple and effective approach that incorporates **G**radient **A**hnormality **I**nspection and **A**ggregation. The effectiveness of GAIA is validated on both commonly utilized (CIFAR) and large-scale (ImageNet-1K) benchmarks. Specifically, GAIA reduces the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to advanced post-hoc methods.

## 1 Introduction

Deep neural networks have been extensively applied across various domains, demonstrating remarkable performance. However, when they are deployed in real-world scenarios, particularly in contexts that require high levels of security , an urgent challenge arises. Namely, these models must be able to ensure the reliability of their outcomes, even in the face of out-of-distribution (OOD) inputs from the open world that differ from in-distribution (ID) training data and thus surpass their cognitive capabilities. That underscores the importance of OOD detection, which involves estimating uncertainty from the model to identify the "unknown" samples, serving as an alert mechanism before making predictive decisions.

Recently, a rich line of literature has emerged to address the challenge of OOD detection . Indeed, the majority of previous approaches focus on defining more suitable measures of OOD uncertainty by using model outputs  or feature representations . Despite the above mainstream approaches, estimating uncertainty from gradients is readily implemented with a fixed model and has received increasing research attention lately. Prior gradient-based OOD detection methods  have primarily emphasized utilizing parameter gradients as the measurement, while giving limited attention to the in-depth exploration of gradients related to the inputs (_i.e_., attribution gradients ).

In this paper, we put our eye on a novel and insightful perspective -- let models explain the uncertainty themselves with attribution approaches. Gradient-based attribution algorithms  are ubiquitous for the visual explanation of why the model makes such a decision to the predicted class. An intuition comes up that well-trained networks can clearly attribute the region of target ID objects, but what if they face OOD samples that are totally unknown to them? As shown in Fig. 1, we observe through the utilization of attribution gradients that the pre-trained model is capable of generating reasonable visual interpretation for the ID input \(X_{}\) from ImageNet . However, when attempting to interpret an OOD image \(X_{}\) from iNaturalist  with a label that does not belong to \(Y_{}\), it confuses the model, leading to a meaningless attribution result.

Following the observation, we delve into investigating the gradient-based attribution abnormality when inferring OOD examples. Our further study finds that this phenomenon can be caused by the attribution gradient, which is constructed by taking the value of the partial derivative of the target output \(S_{c}()\)_w.r.t._ one unit \(z_{i}\) of the input variables \(\) (_i.e._, \(()}{ z_{i}}\)). To enlarge the discrepancy between ID and OOD without prior knowledge from training data, we introduce the channel-wise average abnormality and the zero-deflation abnormality as two measurements for detecting distributional shifts. Then, we propose our detection framework **GAIA** with **G**radient **A**hnormality **I**nspection and **A**ggregation and conduct comprehensive experiments on both CIFAR benchmarks and large-scale ImageNet-1K benchmark to validate the effectiveness of our proposed method. Code is available at https://github.com/JGEthanChen/GAIA-OOD.

Our key results and contributions are summarized as follows:

* We provide insights into the attribution abnormality for OOD detection. Our intuition is that unreliability from visual explanations can be a direct alarm to distinguish OOD examples. Hence, we delve further into the underlying causality of the abnormality. Then, we provide a theoretical explanation for the causes of attribution abnormality.
* We propose a simple yet effective post-hoc detection framework via **G**radient **A**hnormality **I**nspection and **A**ggregation (**GAIA**), which consists of two independent measurements: the Channel-wise Average abnormality (**GAIA-A**) and the Zero-deflation abnormality (**GAIA-Z**). Both of them are lightweight and plug-and-play--hyperparameter-free, training-free, with no ID data and outliers required for estimation.
* Thorough experiments demonstrate that GAIA surpasses most advanced post-hoc methods on both commonly utilized (CIFAR) and large-scale (ImageNet-1K) benchmarks. GAIA-Z exhibits superior performance on CIFAR benchmarks, reducing the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100. GAIA-A performs well on the ImageNet-1K benchmark and reduces by 17.28% compared to the advanced gradient-based detection method GradNorm.

Figure 1: Motivation of our work. Gradient-based attribution algorithms use attribution gradients to explain where models look for predicting final outputs. An intriguing question is: when encountering OOD sample \(X_{}\) whose label falls outside the in-distribution label space \(Y_{}\), how does the model interpret its overconfident prediction? In order to unearth uncertainty from the explanatory result, we conduct our research by inspecting the abnormalities in attribution gradients and then aggregate them for OOD detection.

Preliminaries

We consider the general setting of a supervised machine learning problem, where \(\) denotes the input space and \(_{}=\{1,2,...,C\}\) denotes the ID label space. Especially, we denote the output score _w.r.t._ class \(c\) before softmax layer as \(S_{c}()\).

**Out-of-distribution detection.** The goal of out-of-distribution (OOD) detection is to distinguish the sample \(x_{}\) that exhibits substantial deviation from the distribution \(\). In literature, OOD data originates from an unknown distribution \(_{}\). And the label space of the OOD samples has no intersection with \(_{}\). This problem can be formulated as a binary classification task using a score function \((x)\). More specifically, when provided with an input sample \(x\), the level-set estimation can be expressed as follows:

\[(x)=,&(x)>\\ ,&(x)\] (1)

In our work, lower scores correspond to a higher likelihood of classifying the sample \(x\) as in-distribution (ID), and \(\) denotes a threshold for separating the ID and OOD data.

**Gradients from attribution algorithms.** The attribution gradient is first introduced by sensitivity analysis (SA)  and widely utilized in visual explainability techniques [19; 20; 21; 24; 25]. It refers to the sensitivity of a particular input variable (input or feature unit) _w.r.t._\(c\)-class predictive output \(S_{c}()\). Denotes \(k\)-th channel feature map at layer \(l\) as \(^{kl}^{W H}\). The attribution gradient of one feature unit \(A^{kl}_{ij}\) is computed by:

\[_{ij}=(^{kl})}{ A^{kl}_{ij}}\] (2)

It is unrelated to the gradients commonly associated with the typical understanding of network optimization (_i.e._, gradients of the parameters). In most attribution algorithms, the attribution gradient is used for quantifying the contribution of each feature unit to the model's prediction.

## 3 Investigating Attribution Abnormality for Out-of-distribution Detection

In this section, we aim to investigate how attribution gradients can lead to abnormality when explaining OOD examples. We also attempt to provide a unified theoretical analysis.

**Channel-wise average abnormality.** We first focus on the abnormality in the Gradient-based Class Activation Mapping (GradCAM) algorithm , which is one of the most widely applied attribution strategies. Its paradigm is to channel-wise sum up feature maps for a saliency map \(^{W H}\). Here we denote feature maps \(^{K W H}\) with \(K\)-channels in the convolutional layer as the input variables. The attribution \(_{ij}\) of each unit \(M_{ij}\) can be formulated as follows:

\[_{ij}=(_{k=1}^{K}w_{k}A^{k}_{ij}), w _{k}=)}{ A^{k}_{ij}}=_ {i=1}^{W}_{j=1}^{H}()}{ A^{k}_{ij}}\] (3)

Figure 2: Demonstration of the attribution abnormality from gradient-based weights. The toy experiment is conducted on ResNet34 with four blocks trained on CIFAR10. We select four attribution layers from different blocks and calculate the average attribution gradients for each channel.

where \(w_{k}\) is the channel-wise weight that re-weights feature maps in different channels, and \(g()\) denotes the explanatory function of the DNN output from \(\). Detailed elaboration is provided in _Appendix C_. Taking different layers as the attribution targets, we visualize the distribution of channel-wise average attribution gradients in Fig. 2. It can be observed that the discrepancy of the weights \(w_{k}\) is distinguishable in that OOD samples tend to produce more noisy and abnormal outliers compared to ID samples. Additionally, as the layers increase in depth, the magnitude of the average gradients also increases.

**Zero-deflation abnormality.** Then, we closely examine the abnormality that may arise in attribution gradients themselves due to distributional shifts. Fig. 3(a) shows attribution gradients on feature maps across all channels at a specific layer. We observe that the quantity of zero partial derivation \(()}{ A_{ij}^{}}\) in OOD is extremely less than ID, leading to a high occurrence of dense gradient matrices. As shown in Fig. 3(b), this phenomenon is more pronounced in deeper layers, indicating an abnormal behavior.

### Theoretical Explanation for Attribution Abnormality

We consider a unified explanation for attribution algorithms with Taylor expansion. As proved in , attribution algorithms are mathematically equivalent to the perspective that the network \(c\)-class output \(S_{c}()\) is explained as a Taylor expansion model. For variables \(=[z_{1},...,z_{n}]\) (_e.g._, feature units to be attributed or inputs), here we perform \(P\)-order expansion of zero baseline output \(S_{c}()\) at \(\):

\[S_{c}()=S_{c}()+_{p=1}^{P}_{i=1}^{n}S_{c}()}{(z_{i})^{p}}(z_{i})^{p}+S_{c}()}{ z_{1} z_{2}}z_{1}z_{2}+...+R_{P}( )\] (4)

where \(R_{P}()\) denotes the remainder term for the \(P\)-order expansion. In our paper, we consider the feature values to be all zeros as the zero baseline, which is commonly adopted for analyzing gradient-based attribution algorithms. Then all terms can be represented by vector \(=[_{1},...,_{n}]^{n}\), where \(_{i}\) reflects the integral degree of the input variable \(z_{i}\) (_e.g._, \(_{i}=1\) indicates the corresponding item only contains first-order partial derivative _w.r.t._\(z_{i}\)). Thus we can represent the \(c\)-label output change caused by variables \(\) as:

\[|S_{c}()-S_{c}()|=|_{p=1}^{P}_{,||=p}()++ _{n}}S_{c}()}{^{_{1}}z_{1}^{_{ n}}z_{n}}(z_{1})^{_{1}}(z_{n})^{_{n}}+R_{P}()|\] (5)

where \(()\) is a non-negative constant related to vector \(\). The expansion formula reflects the contribution of each variable \(z_{i}\) to the \(c\)-label output change. Thus, we can attribute importance \(_{i}\) to \(z_{i}\) based on how much it contributes to such a change. Furthermore, the effect of \(z_{i}\) to \(S_{c}()\) can be decomposed into Taylor independent effect term \(_{i}()\) and Taylor interaction effect term \(_{i}()\). For independent term \(_{i}()\), only \(z_{i}\) is contained, where \(=[0,,_{i},,0]_{}\) and \(_{i}>0\). And

Figure 3: **Left (a):** Visualization of attribution gradients on feature maps. **Right (b):** Proportion of non-zero values across different channels. Each data point represents one single channel.

the overall effect of term \(_{i}()\) is caused by the interactions between \(z_{i}\) and other variables, where \(_{}\) has at least two non-negative values and \(_{i}>0\). Attribution methods are formulated as:

\[_{i}=_{p=1}^{P}_{_{},||=p} _{i,}_{i}()+_{p=1}^{P}_{ _{},||=p}_{i,}_{i}()\] (6)

where \(_{i,}\) denotes the ratio of a specific term (either the independent term or the interaction term) allocated to \(_{i}\).

**Attribution abnormality in zero importance.** A reliable attribution result requires accurate identification of the features that are useful for the output. Here, we consider the Null-player axiom  (see _Appendix E_), which states that in the reliable attribution, a feature should be considered as having zero importance when it makes no contribution to the model's output. In other words, _if a feature does not contribute to the model's prediction, it should be considered as having zero importance._

**Proposition 1**.: _Given input variables \(\), for one variable \(z_{i}\) to be attributed, if \(()}{ z_{i}}\) is zero throughout the analysis, then \(_{i}=0\) always holds._

Given variables \(\) in one analysis, it is assumed that the partial derivative function _w.r.t._\(z_{i}\) is a constant zero. As shown in Eq. 7, all independent and interaction terms related to \(z_{i}\) are zero. Thus, \(z_{i}\) is of zero importance to the prediction. This is, zero attribution gradient values will directly impact the final attribution result.

\[()}{ z_{i}}=0++_{n}}S_{c}()}{^{_{1}}z_{1} ^{_{n}}z_{n}}=0,_{i}>0_{i}()= _{i}()=0_{i}=0\] (7)

This provides us with an explanatory perspective for our observation -- visual explanation for OOD data tends to be messy and unreliable due to the model's uncertainty about the unknown distribution, resulting in an abundance of intricate non-zero importance attributions.

**Attribution abnormality in gradient-based weights.** Following Eq. 6, GradCAM in Eq. 3 can be reformulated in form that includes only the first-order Taylor independent terms (see _Appendix D_ for the proof), where \(=[0,...,_{i}=1,...0]\) is a one-hot vector, and \(_{j}=0\) if \(j i\). This simplifies our analysis of the abnormality in weights, focusing solely on the correlation between first-order partial derivatives and the attribution result to reflect the uncertainty on each independent feature.

## 4 GAIA: A Simple and Effective Framework for Abnormality Aggregation

We propose our GAIA framework, which aggregates the channel-wise average abnormality (GAIA-A) or the zero-deflation abnormality (GAIA-Z) for out-of-distribution detection.

**Abnormality aggregation from label space.** General attribution algorithms focus on the final predictive output \(S_{c}()\), where \(c=_{c_{i} C}S_{c_{i}}()\). This is adequate for the zero-deflation abnormality as we aim to ascertain the model's confidence in interpreting its own classification result. While for the channel-wise average abnormality, our aspiration is to gather abnormalities from a broader label space. Hence, all outputs in the ID label space are informative for collecting the model's tendency towards identifying samples as ID categories. For GAIA-A, we fuse all the outputs with \((())\):

\[()}{ A_{ij}^{k}}=(S_{c}())}{ A_{ij}^{k}}\] (8)

This strategy first accumulates the model's outputs and simultaneously performs backpropagation _w.r.t._ the features \(A_{ij}^{k}\). It is more efficient compared to individually backpropagating through each category and then accumulating them, which is impractical in scenarios with large label space (_e.g._, 1000 categories in ImageNet). Furthermore, we find that GAIA-A can be enhanced with a two-stage fusion strategy. Let us denote the neural network prediction function based on input feature variables \(^{K W H}\) by \(S_{c}()=(_{},_{})\), where \(()\) represents the classification function and \(_{}^{1 W_{} H_{}}\) is the feature map at the last layer. Then the network feature extraction function is defined as \(()\) and \(_{}=(,_{})\). In our methods, we consider the gradient matrix on the \(_{}\) and the inner feature map \(^{k}^{1 W H}\) (\(k\)-th channel from \(\)) separately, with the former regarded as the output component \(_{}\) and the latter as the inner component \(^{k}\):

\[^{k}=,_{})}{ ^{k}}\] (9) \[_{}=(_{ })}{_{}}=(S_{c}(_{}))}{_{}}\]

**Abnormality aggregation from input space.** We start by defining the anomalies expectation on \(k\)-th channel feature map at \(l\) layer as \(^{kl}^{1 W H}\). The zero-deflation abnormality can be described as the non-zero density of \(^{kl}\):

\[[|^{kl}]=\{A^{kl}_{ij}| (^{kl})}{ A^{kl}_{ij}} 0\}\] (10)

For the channel-wise average abnormality, we observed that average gradients on \(_{}\) from the output component and the average attribution gradients obtained from the inner component exhibit opposite behaviors in terms of ID and OOD data _(We discuss its effectiveness in Section 5.3 and provide theoretical analysis in Appendix F)_. Consequently, we use division to get the expectation of the final fusion channel-wise average abnormality abnormality:

\[[|^{kl}]=_{}[| {A}^{kl}]}{_{}[|_{}]}}= _{g^{kl}^{kl}}g^{kl}}{ } H_{}}_{g_{} _{}}g_{}^{}}\] (11)

Consider networks have \(L\) layers to be utilized, and each layer has \(K_{l}\) channels. Our framework accumulates them into an abnormality matrix \(^{L K_{m}}\), where \(K_{m}=\{K_{i}|1 i L\}\) and \(_{ij}=0\) if \(j>K_{i}\). Then, we use the Frobenius norm as a non-parameter measuring score to represent the global abnormality. For instance, assuming \(K_{m}=K_{L}\), \(\|\|_{F}\) is calculated as:

\[\|\|_{F}=[|^{1,1}]&& [|^{1,K_{1}}]&0&&0\\ &&&&\\ [|^{L,1}]&&[|^{L,K_{1}} ]&&&&[|^{L,K_{m}}]\|_{F }=^{L}_{j}^{K_{m}}([|^{i,j}])^{2}}\] (12)

The overall process are formulized in Algorithm 1.

**Input:** Test sample \(x\); Fixed model \(f_{}\).

**Output:** OOD score \((x)\).

Compute label output set \(\{S_{c}(x)|c C\}\) by \(f_{}(x)\);

Backpropagate attribution gradients by \((^{kl})}{ A^{kl}_{ij}}\) (GAIA-Z) or Eq. 9 (GAIA-A);

Calculate \([|^{kl}]\) by Eq. 10 (GAIA-Z) or Eq. 11 (GAIA-A);

Calculate global abnormality \(\|\|_{F}\) by Eq. 12;

**return \(\|\|_{F}\)** as OOD score \((x)\).

**Algorithm 1**GAIA

## 5 Experiments

In this section, we describe our experimental setup in Section 5.1. Then, we demonstrate the effectiveness of our method on the large-scale ImageNet-1K benchmark  and the CIFAR benchmarks  in Section 5.2. We also conduct ablation studies in Section 5.3.

### Setup

**Benchmarks.** In accordance with [10; 11; 15; 28], we employ the large-scale ImageNet-1K benchmark , which offers a more realistic and challenging environment due to its use of high-resolution images and an large label space that encompasses 1,000 distinct categories. Four OOD datasets in this benchmark are from iNaturalist , SUN , Places  and Textures , including fine-grained images, scene-oriented images, and textural images. We also evaluate CIFAR10 and CIFAR100 benchmarks , which are routinely used in literature. Correspondingly, OOD datasets are SVHN , TinyImageNet , LSUN , Places  and Textures .

**Baselines.** We consider various kinds of mainstream post-hoc OOD detection methods as baselines, including Maximum Softmax Probability (MSP) , ODIN , Energy-based method , Mahalanobis , ReAct , GradNorm , Rankfeat , ASH  and KNN . We use FPR95 (the false positive rate of OOD examples when the true positive rate of ID examples is 95%) and AUROC (the area under the receiver operating characteristic curve) as evaluation metrics.

### Main Results

In our main results, all methods can be directly used for pre-trained models and for a fair comparison, auxiliary OOD data is unavailable for tuning.

**Evaluation on CIFAR benchmarks.** In Tab. 1, we evaluate GAIA methods on CIFAR10 and CIFAR100 benchmarks. The results show that both GAIA-A and GAIA-Z exhibit superior performance. And we also note that advanced post-hoc methods such as Rankfeat and Grandnorm tend to encounter performance degradations on limited label space with small architectures. For ID dataset CIFAR10, baseline ASH performs the best with an average FPR95 of 26.36% on ResNet34 and ODIN performs 37.09% on WideResNet40 (WRN40). Our method GAIA-Z significantly outperforms ASH on ResNet34 by **23.10%** improvement and outperforms ODIN on WideResNet by **19.13%** improvement. Moreover, GAIA-A achieves the second best performance after GAIA-Z. For CIFAR100, GAIA-Z attains an average FPR95 of 29.10% and average AUROC of 94.93% on ResNet34, surpassing the best baseline ReAct by a margin of **45.41%** FPR95 and **13.17%** AUROC. GAIA-Z achieves surprising performance on CIFAR benchmarks by utilizing the zero-deflation abnormality.

    **ID** \\ **Datasets** \\  } &  &  &  &  &  &  &  \\   & & & } & } & } & } & } & } & } & } & } & } \\   & & & & & & & & & & & & & & \\   GER10 \\  } & 46.03 & 80.11 & 51.87 & 52.79 & 63.67 & 90.63 & 43.71 & 91.88 & 44.58 & 90.08 & 50.05 & 89.48 \\  & ODIN  & 94.20 & 93.82 & 62.23 & 93.43 & 94.11 & 86.33 & 91.18 & 45.01 & 91.11 & 41.06 & 92.54 \\  & Energy  & 42.97 & 93.20 & 97.26 & 99.88 & 93.58 & 93.54 & 92.44 & 94.57 & 93.29 & 29.97 & 29.15 \\  & Mahalanobis  & 22.19 & 93.36 & 29.53 & 91.16 & 25.31 & 91.98 & 25.61 & 91.26 & 93.82 & 87.02 & 28.96 & 90.74 \\  & Real Real  & 11.09 & 93.36 & 33.67 & 91.81 & 93.76 & 91.36 & 92.69 & 92.82 & 92.87 & 91.27 & 91.59 \\  & GrossNet  & 62.47 & 76.08 & 73.60 & 65.21 & 93.98 & 72.57 & 89.53 & 73.67 & 67.47 & 67.41 & 34.34 & 71.41 \\  & KNN  & 20.32 & 93.56 & 95.64 & 94.21 & 94.17 & 42.97 & 42.17 & 43.54 & 94.47 & 83.32 & 94.95 \\  & RealNet  & 54.58 & 72.99 & 50.23 & 98.84 & 41.63 & 91.97 & 67.79 & 82.64 & 82.62 & 86.67 & 85.26 \\  & ASH-FPR  & 23.11 & 85.23 & 92.78 & 92.71 & 92.13 & 93.53 & 82.27 & 93.42 & 93.96 & 92.06 & 92.06 & 94.40 \\  &  & **24.79** & **99.62** & **96.56** & **96.28** & **99.48** & **22.77** & **99.86** & **22.84** & **93.96** & **33.26** & **99.28** \\  &  & **24.04** & **97.19** & **96.24** & **96.53** & **97.00** & **96.10** & **10.50** & **12.00** & **12.00** & **12.02** & **12.05** & **93.55** \\   **GARARAR** \\  } & 46.45 & 92.30 & 60.58 & 86.99 & 90.30 & 90.14 & 45.41 & 98.55 & 54.24 & 84.57 & 90.42 \\  & Graph  & 91.61 & 94.18 & 90.64 & 91.86 & 91.37 & 91.45 & 40.30 & 91.18 & 91.87 & 81.70 & 90.18 \\  & Energy  & 19.94 & 95.89 & 4

**Evaluation on ImageNet-1K benchmark.** In Tab. 2, we compare GAIA with other post hoc baselines on pre-trained Google BiT-S model . For our methods, both GAIA-A and GAIA-Z use layers from the last block (**Block4**), and no hyperparameters are required. GAIA-A performs well with an average FPR95 of 37.42% and an average AUROC of \(91.90\%\). Compared to other gradient-based OOD methods, GAIA-A outperforms GradNorm by **17.28%** in FPR95. Besides, GAIA-Z excels in handling the OOD dataset of textures with \(11.32\%\) FPR95, despite not achieving the best overall performance. While ASH achieves competitive results on the ImageNet dataset through careful parameter tuning, it is highly sensitive to its hyperparameters and lacks empirical parameters. In contrast, GAIA methods don't require parameter adjustments and directly achieve good results.

### Ablation Studies

Our ablation study begins by validating the effectiveness of each step of the methods. We first verify the effect of the Frobenius norm (2-norm). Then we explore the aggregation's effectiveness on the label space and the input space.

**Influence of Frobenius norm.** In Eq. 12, we use \(\|\|_{F}\) to calculate the final OOD score. To verify its effectiveness, we evaluate different norms of \(\) on the above three benchmarks. As shown in Fig. 4, the Frobenius norm performs the best. Compared to 1-norm, Frobenius norm particularly demonstrates significant improvements. This is because the Frobenius norm can exclude the influence of numerous smaller values. As the number of layers in the model increases, the accumulation of insignificant small values in the shallow layers can weaken the scoring impact of extreme values OOD data. However, we can observe that as the value of \(p\) increases, the influence of extreme values will also be affected.

**Influence of label space aggregation.** In GAIA-A, we employ division to fuse the inner component \(_{}[|^{kl}]\) and the output component \(_{}[|^{kl}]\) to obtain the final OOD scores. As shown in Fig. 5, we visualize the score distributions of the individual components and the fused scores, and observe that the performance of the inner and output components in OOD and ID data are contrasting. After dividing and merging the two components, the fusion resulted in a greater concentration of ID data, tending towards a narrower distribution. However, the impact on the distribution of OOD data was relatively minor, thereby widening the score differences between them. In Tab. 3, we compare the

     } &  &  &  &  &  &  &  &  \\   & & FPR95 & 4/IROC & FPR95 & 4/IROC & FPR95 & 4/IROC & FPR95 & 4/IROC & FPR95 & 4/IROC & FPR95 & 4/IROC \\   & MSP  & 63.69 & 87.59 & 79.98 & 78.34 & 81.44 & 76.76 & 82.73 & 74.45 & 86.96 & 79.29 \\  & OOD  & **64.61** & **89.84** & **71.65** & 83.32 & 74.27 & 86.95 & 81.31 & **76.49** & 72.99 & 82.56 \\  & Energy  & 64.91 & 85.48 & **65.03** & **85.33** & **78.02** & **81.37** & **80.70** & 73.79 & **71.63** & **81.24** \\    } & Matharoo  & 96.34 & 65.38 & 84.63 & 85.42 & 89.75 & 64.46 & 52.53 & 72.10 & 81.69 & 62.24 \\  & Reich  & 44.52 & 91.81 & 52.71 & 91.96 & 62.56 & 87.83 & 70.75 & 76.85 & 87.66 & 86.67 \\  & KEN  & 59.08 & 86.20 & 69.53 & 81.07 & 77.29 & 82.45 & **71.56** & **79.18** & 54.32 & 84.99 \\  & Rank/KEN  & 46.45 & 81.49 & **72.08** & **72.15** & **81.26** & 88.34 & 46.66 & 89.83 & 36.96 & 87.24 \\  & Rank/KEN  & 44.91 & 91.94 & 27.27 & **84.07** & 89.44 & **89.03** & 73.29 & 91.70 & 75.80 & 91.25 \\  & ASH-P@90(1) & **22.22** & **96.15** & 54.54 & 92.57 & 67.35 & 96.23 & 73.53 & 95.43 & **31.88** & **95.43** \\    } & GMMNet  & 50.03 & 90.33 & 46.48 & 89.03 & 66.06 & 84.22 & 61.07 & 54.70 & 86.31 \\  & **GAIA-A(OOD)** & **28.47** & **98.32** & **32.34** & **94.24** & **85.55** & **88.94** & 49.81 & 92.71 & **37.42** & **93.59** \\   & **GAIA-A(OOD)** & **65.09** & **84.15** & **64.23** & **84.33** & 71.20 & **81.36** & **81.32** & **97.93** & **52.92** & **56.69** \\   

Table 2: **Main Results on ImageNet-1K .** OOD detection performance comparison between GAIA and advanced baselines on pre-trained Google BiT-S  model. Our methods only use layers from Block4 and all methods are post hoc that can be directly used for pre-trained models. The best results for each Methods Space are all in Bold.

Figure 4: Ablation studies on Frobenius norm of matrix \(\).

OOD detection performance with and without (w/o) the fusion strategy. Experiments demonstrated a improvement with the implementation of this strategy.

**Influence of input space aggregation across different layers (blocks).** Given that both ResNet34 and Google BiT-S models have four blocks, we analyze the performance of our methods across different blocks to elucidate the influence of feature layers. As shown in Tab. 4, deeper layers possess a higher power in distinguishing between ID and OOD data. It indicates that as the network becomes shallower, the feature maps progressively contain a diminishing amount of relevant information _w.r.t._ the prediction decision . For CIFAR benchmarks, information from Block3+4 is sufficient for detection, and for ImageNet-1K benchmark, only using Block4 can achieve the best performance.

## 6 Related Work

Among all attempts so far, post-hoc methods [5; 9; 10; 11; 14; 15] are preferable in the wild due to their advantages of being easy to use without modifying the training procedure and objective. An initial solution proposed by Hendrycks and Gimpel  utilizes maximum softmax probability (MSP). While due to the tendency of networks to display overconfident softmax scores when predicting OOD inputs [40; 41], it renders a non-trivial dilemma to separate ID and OOD data. Then ODIN  introduces temperature factors and input perturbations to enhance detection performance. In a different approach, Energy  is proposed to utilize the energy score as an informative indicator. ReAct  proposes that OOD examples result in abnormal model activation and suggests clamping the activation values above a threshold. Rankfetal  leverages the differences in singular value distributions, which still focuses on abnormal activations of the model. Another relevant study to this paper is gradient-based OOD detection. In the early work, ODIN  first implicitly utilizes gradients as perturbations to increase the softmax score of any given input. Recently, Lee and AlRegib , Huang et al.  and Igoe et al.  use the gradients of parameters as the measurement, which emphasizes the importance of the loss function. In this paper, we delve into investigating attribution abnormality and utilize attribution gradients for OOD detection.

    &  &  &  \\   &  &  &  &  &  &  &  \\  & **FP95\%** & AUROC & FP95\(\) & AUROC & FP95\(\) & AUROC & FP95\(\) & AUROC & FP95\(\) & AUROC & FP95\(\) & AUROC\(\) \\  Block \(\) & 64.52 & 51.82 & 57.15 & 67.06 & 77.89 & 80.72 & 83.83 & 66.66 & 86.59 & 61.26 & 92.38 & 49.62 \\ Block \(\) & 62.19 & 86.26 & 50.71 & 86.69 & 77.30 & 80.50 & 52.40 & 89.66 & 87.21 & 85.30 & 88.56 & 58.64 \\ Block \(\) & 44.56 & 91.07 & 22.11 & 95.71 & 71.28 & 84.67 & 44.18 & 89.01 & 63.34 & 80.81 & 73.28 & 79.96 \\ Block \(\) & 42.90 & 97.54 & 6.42 & 89.75 & 69.16 & 86.40 & 49.97 & 91.28 & **37.42** & **91.90** & **52.92** & **86.99** \\ Block \(\) & **12.70** & **97.53** & 3.55 & 92.62 & **68.58** & **86.42** & **22.76** & **95.24** & 41.91 & 91.03 & 58.39 & 86.91 \\ All blocks & 12.73 & 97.53 & **3.26** & **99.28** & 68.98 & 86.42 & 29.05 & 94.92 & 42.38 & 90.86 & 61.28 & 86.02 \\   

Table 4: Ablation studies of the influence on different blocks with average FPR95 and AUROC.

Figure 5: The distribution of the OOD scores in three settings (_inner only_, _output only_ and _fusion_). All scores are non-negative for comparison.

    &  &  &  &  &  \\   & **FR95\(\)** & **AUROC & **FP95\(\)** & **AUROC & **FP95\(\)** & **AUROC & **FP95\(\)** & **AUROC\(\)** & **FP95\(\)** & **AUROC\(\)** \\  w/o fusion deep 1 label() & 74.44 & 74.64 & 77.30 & 77.60 & 82.67 & 71.11 & 50.14 & 89.57 & 70.99 & 78.08 \\ w/o fusion (output only) & 77.50 & 92.54 & 68.27 & 84.67 & 81.97 & 82.00 & 56.17 & 88.33 & 67.92 & 84.46 \\ w/o fusion (inner only) & 49.45 & 81.98 & 53.24 & 86.51 & 42.88 & 80.30 & 54.55 & 89.77 & 56.22 & 84.62 \\ w/o fusion (bottom or bottom) & **79.42** & **80.52** & **31.24** & **92.02** & **42.58** & **10.54** & **90.74** & **90.74** & **90.72** & **94.00** \\   

Table 3: Ablation studies on fusion strategy. _top 1 label_ means utilizing the predictive output only.

## 7 Discussion

In this section, we discuss the comparison of our methods with other gradient-based OOD detection methods, as well as the limitation on transformer-based models.

### Comparison with Other Gradient-based Methods

A crucial distinction between other gradient-based OOD detection methods and ours lies in the utilization of attribution methods to interpret the anomalous behavior of OOD examples. Specifically, we investigate and aggregate the abnormal patterns exhibited by attribution gradients at the feature level. Compared to ODIN , GAIA directly leverages the uncertainty derived from the gradients of input features, providing a more intuitive and efficient solution. Furthermore, rather than focusing solely on the softmax output, we delve into the intermediate statistics to uncover more fundamental discrepancies. Compared to GradNorm , ExGrad  and Lee and AlRegib , our approaches focus on attribution gradients and demonstrate superior performance. The comparative performance is presented in Tab. 5. Additionally, GAIA supports batch processing, as the attribution gradients are independent for each input feature, while gradients of parameters are unique to the network. This means that our method can handle multiple samples simultaneously, providing a parallel processing advantage over these methods that can only process one sample at a time.

### Limitation on Transformer-based Models

Newer models like Vision Transformers (ViT) , which are based on transformers, excel in feature extraction. However, they may not align well with image-specific characteristics. For instance, ViTs employ positional encoding to capture spatial information, posing challenges for attribution. Due to this reason, existing attribution algorithms are rarely applied to ViTs, resulting in poorer performance for GAIA. While the attention mechanism in transformer-based models can also offer directions for visual explanations. In our future work, we will research the uncertainty in the attention matrix to enhance OOD detection performance on transformer-based models.

## 8 Conclusion

This paper targets bridging the gap between OOD detection and visual interpretation by utilizing the uncertainty of a model in explaining its own predictions. We further examine how attribution gradients contribute to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection. Then, we propose GAIA, a simple and effective framework for abnormality aggregation. The effectiveness of our framework is validated through experiments.

**Societal impact and limitations.** Through this work, we aim to provide a new perspective to improve the performance of OOD detection and ensure the safety and reliability of machine learning applications. However, the utilization of attribution gradients in this paper is relatively simplistic. We believe there is still significant research potential in this area. Moreover, the limitation on transformer-based models remains a topic for further investigation.

## 9 Acknowledgement

Research is supported by the Key Research and Development Program of Guangdong Province (grant No. 2021B0101400003). This work was done while Jinggang Chen was interning at Ping An Technology and the corresponding authors are Xiaoyang Qu and Jianzong Wang from Ping An Technology (Shenzhen) Co., Ltd.

    &  & **Naturalist** & **SUN** & **Places** & **Textures** & **Average** \\   & & AIReoc \(\) & AIReoc \(\) & AIReoc \(\) & AIReoc \(\) & AIReoc \(\) \\  Lee and AlRegib  & & 72.30 & 82.61 & 74.00 & 84.16 & 78.27 \\ GradNorm  & & 90.33 & 89.03 & 84.82 & 81.07 & 86.31 \\ ExGrad  & & 76.90 & 66.60 & 68.90 & 65.10 & 69.40 \\
**GAIA-A (Ours)** & ✓ & **93.52** & **92.42** & **88.94** & 92.71 & **91.90** \\
**GAIA-Z (Ours)** & ✓ & 84.15 & 84.31 & 81.16 & **97.93** & 86.89 \\   

Table 5: Comparison with other gradient-based methods. To ensure a fair comparison with Lee and AlRegib , the gradients of uniform noise are used as a surrogate, as suggested in .