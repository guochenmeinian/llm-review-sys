# On kernel-based statistical learning in the mean field limit

Christian Fiedler\({}^{1}\) Michael Herty\({}^{2}\) Sebastian Trimpe\({}^{3}\)

\({}^{1,3}\)Institute for Data Science in Mechanical Engineering (DSME)

RWTH Aachen University, Aachen, Germany

{fiedler,trimpe}@dsme.rwth-aachen.de

\({}^{2}\) Institute for Geometry and Practical Mathematics (IGPM)

RWTH Aachen University, Aachen, Germany

herty@igpm.rwth-aachen.de

###### Abstract

In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds to a new form of limit of learning problems, which seems to have not been investigated yet in the statistical learning theory literature.

## 1 Introduction

Models with many variables play an important role in many fields of mathematical and physical sciences. In this context, going to the limit of infinitely many variables is an important analysis and modeling approach. Interacting particle systems are a classic example; these are usually modeled as dynamical systems describing the temporal evolution of many interacting objects. In physics, such systems were first investigated in the context of gas dynamics, cf. . Since even small volumes of gases typically contain an enormous number of molecules, a microscopic modeling approach quickly becomes infeasible and one considers the evolution of densities instead . In the past decades, interacting particle systems arising from many different domains have been considered, for example, animal movement (inter alia swarms of birds, schools of fish, colonies of microorganisms) [3; 4], social and political dynamics [5; 6], crowd modeling and control (pedestrian movement, gathering at large events like football games or concerts) [7; 8; 9], swarms of robots [10; 11; 12] or vehicular traffic (in particular, traffic jams) . There is now a vast literature on such applications, and we refer to the surveys [14; 15; 16] as starting points. A prototypical example of such a system is given by \(_{i}=_{j=1}^{M}(x_{i},x_{j})(x_{j}-x_{i})\), for \(i=1,,M\), where \(M_{+}\) particles or agents are modelled by their state \(x_{i}^{d}\), \(i=1,,M\), evolving according to some interaction rule \(:^{d}^{d}\). Typical questions then concern the long-term behavior of such systems, in particular, emergent phenomena like consensus or alignment . While first-principles modelinghas been very successful for interacting particle systems in physical domains, using this approach to model the interaction rules in complex domains like social and opinion dynamics, pedestrian and animal movement or vehicular traffic, can be problematic. Therefore, learning interaction rules from data has been recently intensively investigated, for example, in the pioneering works . The data consists typically of (sampled) trajectories of the particle states, potentially with measurement noise, and the goal is to learn a good approximation of the interaction rule \(\).

Our work is motivated by a related problem. Frequently, the state of such a complex multiagent system can be easily measured or estimated, e.g., by video recordings or image snapshots for bird swarms or schools of fish, and microscopy recordings for microorganism colonies; aerial imaging for human crowds (e.g., via quadcopters); and polling and social media analysis for opinion dynamics. However, some interesting features of the whole system might be more difficult to measure. For example, how a swarm of birds or a school of fish will react to an external stimulus (like an approaching predator), given the current state of the population. Such a reaction could be a change of density or spread of the population, or a change in mean velocity. Another example is given by features of a society in opinion dynamics (average happiness, aggression potential, susceptibility to adversarial interventions), given the current "opinion state". Measuring such features can be difficult, for example, due to a required intervention. Formally, such a feature is a functional \(F_{M}:(^{d})^{M}\) of the current state of the system, and since the state is often easy to measure, it would be useful to have an explicit mapping from state to feature of interest. However, since first principles modeling is unlikely to be successful in the domains considered here, it is promising to learn such a mapping from data. We can formalize this as a standard supervised learning task: The data set consists of \(D_{N}^{[M]}=((_{1},y_{1}),,(_{N},y_{N}))\), where \(_{n}(^{d})^{M}\) are snapshot measurements of the particle states (corresponding to the input of the functional) and \(y_{n}\) is the value of the functional of interest, potentially with measurement noise, at snapshot state \(_{n}\). Let us assume an additive noise model, i.e., \(y_{n}=F_{M}(_{n})+_{n}\) for \(n=1,,N\), where \(_{1},,_{N}\) are noise variables. This is now a regression problem that could be solved, for example, using a Support Vector Machine (SVM) . Note that for this we need a kernel \(k_{M}:(^{d})^{M}(^{d})^{}\) on \((^{d})^{M}\).

Similarly to classical physical examples like gas dynamics, the case of a large number of particles is also relevant in modern complex interacting particle systems. Since this poses computational and modeling challenges, it can be advantageous to go also here to a kinetic level and model the evolution of the particle distribution instead of every individual particle. It is well-established how to derive a kinetic partial differential equation from ordinary differential equations systems on the particle level, for example, using the Boltzmann equation or via a mean field limit, cf.  for an overview in the context of multi-agent systems. Formally, instead of trajectories of particle states of the form \([0,T] t(t)(^{d})^{M}\), we then have trajectories of probability measures \([0,T] t(t)(^{d})\). This immediately raises the question of whether the learning setup outlined above also allows a corresponding kinetic limit. More precisely, let \(K^{d}\) be compact and assume that all particles remain confined to this compactum, i.e., \(x_{i}(t) K\) for all \(i=1,,M\) and all \(t[0,T]\) under the microscopic dynamics.1 If the underlying dynamics have a mean field limit, then it is reasonable to assume that the finite-input functionals \(F_{M}:K^{M}\) converge also in mean field to some \(F:(K)\) for \(M\), see Section 2 for a precise definition of this notion. In turn, we can now formulate a corresponding learning problem on the mean field level: A data set is then given by \(D_{N}=((_{1},y_{1}),,(_{N},y_{N}))\), where \(_{n}(K)\) are snapshots of the particle state distribution over time and \(y_{n}\) are again potentially noisy measurements of the functional. Assuming an additive noise model, this corresponds to \(y_{n}=F(_{n})+_{n}\), \(n=1,,N\). If we want to use an SVM on the kinetic level, we need a kernel \(k:(K)(K)\) on probability distributions. There are several options available for this, see e.g. . However, assuming that all ingredients of the learning problem arise as a mean field limit, this naturally leads to the question of whether a mean field limit of kernels exists, and what this means for the relation of the learning problems on the finite-input and kinetic level. In , this reasoning has motivated the introduction and investigation of the mean field limit of kernels. In the present work, we extend the theory of these kernels and investigate them in the context of statistical learning theory. In particular, since in practice one would use the mean field kernels on microscopic data with large, but finite \(M\), we need convergence results of the various objects appearing in statistical learning with kernels. Exactly such results are provided in Section 4.

Finally, we would like to stress that the technical developments here are independent of the motivation outlined above, in that they apply to mean field limits of functions and kernels that do not necessarily arise form the dynamics of interacting particle systems.

ContributionsOur contributions cover three closely related aspects. 1) We extend and complete the theory of mean field limit kernels and their RKHSs (Section 2). In Theorem 2.3, we precisely describe the relationship between the RKHS of the finite-input kernels and the RKHS of the mean field kernel, completing the results from . In particular, this allows us to interpret the latter RKHS as the mean field limit of the former RKHSs. Furthermore, in Lemma 2.4 and 2.5, we provide inequalities for the corresponding RKHS norms, which are necessary for \(\)-convergence arguments. 2) We provide results relevant for approximation with mean field limit kernels (Section 3). With Proposition 3.1, we give a first result on the approximation power of mean field limit kernels, and in Theorem 3.3 we can also provide a representer theorem for these kernels. For its proof, we use a \(\)-convergence argument, which is to the best of our knowledge the first time this technique has been used in the context of kernel methods. 3) We investigate the mean field limit of kernels in the context of statistical learning theory (Section 4). We first establish an appropriate mean field limit setup for statistical learning problems, based on a slightly stronger mean field limit existence result than available so far, cf. Proposition 2.1. To the best of our knowledge, this is a new form of a limit for learning problems. In this setup, we then provide existence, uniqueness, and representer theorems for empirical and (using an apparently new notion of mean field convergence of probability distributions) infinite-sample solutions of SVMs, cf. Proposition 4.3 and 4.5. Finally, under a uniformity assumption, we can also establish convergence of the minimal risks in Proposition 4.7.

Our developments are relevant from two different perspectives: on the one hand, they constitute a theoretical proof-of-concept that the mean field limit can be "pulled through" the (kernel-based) statistical learning theory setup. In particular, this demonstrates that rigorous theoretical results can be transferred through the mean field limit, similar to works in the context of control of interacting particle systems, see e.g. . On the other hand, our setup appears to be a new variant of a large-number-of-variables limit in the context of machine learning, complementing established settings like infinite-width neural networks .

Due to space constraints, all proofs and some additional technical results have been placed in the supplementary material.

## 2 Kernels and their RKHSs in the mean field limit

Setup and preliminariesLet \((X,d_{X})\) be a compact metric space and denote by \((X)\) the set of Borel probability measures on \(X\). We endow \((X)\) with the topology of weak convergence of probability measures. Recall that for \(_{n},(X)\), we say that \(_{n}\) weakly if for all bounded and continuous \(f:X\) (since \(X\) is compact, this is equivalent to \(f\) continuous) we have \(_{n}_{X}(x)_{n}(x)_{X}(x) (x)\). The topology of weak convergence can be metrized by the Kantorowich-Rubinstein metric \(d_{}\), defined by

\[d_{}(_{1},_{2})=\{_{X}(x)(_{1}- _{2})(x):X\}.\]

Note that since \(X\) is compact and hence separable, the Kantorowich-Rubinstein metric is equal to the 1-Wasserstein metric here. Furthermore, \((X)\) is compact in this topology. For \(M_{+}\) and \( X^{M}\), denote the \(i\)-th component of \(\) by \(x_{i}\), and define the _empirical measure_ for \(\) by \([]=_{i=1}^{M}_{x_{i}}\), where \(_{x}\) denotes the Dirac measure centered at \(x X\). The empirical measures are dense in \((X)\) w.r.t. the Kantorowich-Rubinstein metric. Additionally, define \(d_{}^{2}:(X)^{2}(X)^{2}_{ 0}\) by \(d_{}^{2}((_{1},_{1}^{}),(_{2},_{2}^{}))=d_{ }(_{1},_{2})+d_{}(_{1}^{},_{2}^{})\), and note that \(((X)^{2},d_{}^{2})\) is a compact metric space. Moreover, denote the set of permutations on \(\{1,,M\}\) by \(_{M}\), and for a tuple \( X^{M}\) and permutation \(_{M}\) define \(=(x_{(1)},,x_{(M)})\). Finally, we recall some well-known definitions and results from the theory of reproducing kernel Hilbert spaces, following [20, Chapter 4]. For an arbitrary set \(\) and a Hilbert space \((H,,_{H})\) of functions on \(\), we say that a map \(k:\) is a _reproducing kernel_ for \(H\) if 1) \(k(,x) H\) for all \(x\); 2) for all \(x\) and \(f H\) we have \(f(x)= f,k(,x)_{H}\). Note that if a reproducing kernel exists, it is unique. If such a Hilbert space has a reproducing kernel, we call \(H\) a reproducing kernel Hilbert space (RKHS) and \(k\) its (reproducing) kernel. It is well-known that a reproducing kernel is symmetric and positive semidefinite, and that every symmetric and positive semidefinite function has a unique RKHS for which it is the reproducing kernel. For brevity, if \(k\) is symmetric and positive semidefinite, or equivalently, if it is the reproducing kernel of an RKHS, we call \(k\) simply a kernel, and denote by \((H_{k},,_{k})\) its unique associated RKHS. Define also \(H_{k}^{}=\{k(,x) x\}\), then for \(f=_{n=1}^{N}_{n}k(,x_{n}) H_{k}^{}\) and \(g=_{m=1}^{M}_{m}k(,y_{m}) H_{k}^{}\) we have \( f,g_{k}=_{n=1}^{N}_{m=1}^{M}_{n}_{m}k(y_{m}, x_{n})\), and \(H_{k}^{}\) is dense in \(H_{k}\).

The mean field limit of functions and kernelsGiven \(f_{M}:X^{M}\), \(M_{+}\), and \(f:(X)\), we say that \(f_{M}\)_converges in mean field to_\(f\) and that \(f\) is the (or a) _mean field limit_ of \(f_{M}\), if \(_{M}_{ X^{M}}|f_{M}()-f([]) |=0\). In this case, we write \(f_{M}_{1}}f\). Let now \((Y,d_{Y})\) be another metric space and \(f_{M}:X^{M} Y\), \(M_{+}\), and \(f:(X) Y\), then we say that \(f_{M}\)_converges in mean field to_\(f\) and that \(f\) is the (or a) _mean field limit_ of \(f_{M}\), if for all compact \(K Y\) we have

\[_{M}_{ X^{M},y K}|f_{M}(,y)-f([ ],y)|=0.\] (1)

and also write \(f_{M}_{1}}f\). The following existence results for mean field limits is slightly more general than what is available in the literature, and it is essentially a direct generalization of [25, Theorem 2.1], in the form of [26, Lemma 1.2].

**Proposition 2.1**.: Let \((X,d_{X})\) be a compact metric space and \((Z,d_{Z})\) a metric space that has a countable basis \((U_{n})_{n}\) such that \(_{n}\) is compact for all \(n\). Let \(f_{M}:X^{M} Z\), \(M_{+}\), be a sequence of functions fulfilling the following conditions: 1) _(Symmetry in \(\))2_ For all \(M_{+}\), \( X^{M}\), \(z Z\) and permutations \(_{M}\), we have \(f_{M}(,z)=f_{M}(,z)\); 2) _(Uniform boundedness_) There exists \(B_{f}_{ 0}\) and a function \(b:Z_{ 0}\) such that \( M_{+}, X^{M},z z:|f_{M}(,z)| B_{f }+b(z);3)\) _(Uniform Lipschitz continuity_) There exists some \(L_{f}_{>0}\) such that for all \(M_{+}\), \(_{1},_{2} X^{M}\), \(z_{1},z_{2} Z\) we have \(|f_{M}(_{1},z_{1})-f_{M}(_{2},z_{2})| L_{f}(d_{}([_{1}],[_{2}])+d_{Z}(z_{1},z_{2}))\).

Then there exists a subsequence \((f_{M_{}})_{}\) and a continuous function \(f:(X) Z\) such that \(f_{M_{}}_{1}}f\) for \(\). Furthermore, \(f\) is \(L_{f}\)-Lipschitz continuous and there exists \(B_{F}_{ 0}\) such that for all \((X)\), \(z Z\) we have \(|f(,z)| B_{F}+b(z)\).

We now turn to the mean field limit of kernels as introduced in : Given \(k_{M}:X^{M} X^{M}\) and \(k:(X)(X)\), we say that \(k_{M}\)_converges in mean field to_\(k\) and that \(k\) is the (or a) _mean field limit_ of \(k_{M}\), if

\[_{M}_{,^{} X^{M}}|k_{M}(,^{})-k([],[^{}])|=0.\] (2)

In this case we write \(k_{M}_{1}}k\).

For convenience, we recall [22, Theorem 2.1], which ensures the existence of a mean field limit of a sequence of kernels.

**Proposition 2.2**.: Let \(k_{M}:X^{M} X^{M}\) be a sequence of kernels fulfilling the following conditions. 1) _(Symmetry in \(\))_ For all \(M_{+}\), \(,^{} X^{M}\) and permutations \(_{M}\) we have \(k_{M}(,^{})=k_{M}(,^{})\); 2) _(Uniform boundedness_) There exists \(C_{k}_{ 0}\) such that \( M_{+},,^{} X^{M}:|k_{M}(, ^{})| C_{k}\); 3) _(Uniform Lipschitz continuity)_ There exists some \(L_{k}_{>0}\) such that for all \(M_{+}\), \(_{1},_{1}^{},_{2},_{2}^{} X^{M}\) we have \(|k_{M}(_{1},_{1}^{})-k_{M}(_{2},_{2}^{})|  L_{k}d_{}^{}[([_{1}],[_{ 1}^{}]),([_{2}],[_{2}^{}])]\).

Then there exists a subsequence \((k_{M_{}})_{}\) and a continuous kernel \(k:(X)(X)\) such that \(k_{M_{}}_{1}}k\), and \(k\) is also bounded by \(C_{k}\).

Let \(k_{M}:X^{M} X^{M}\) be a given sequence of kernels fulfilling the conditions of Proposition 2.2. Then there exists a subsequence \((k_{M_{}})_{}\) converging in mean field to a kernel \(k:(X)(X)\).

_From now on, we only consider this subsequence_ and denote it again by \((k_{M})_{M}\), i.e., \(k_{M}[_{1}]{_{1}}k\). Unless noted otherwise, every time we need a further subsequence, we will make this explicit.3

The RKHS of the mean field limit kernelDenote by \(H_{M}:=H_{k_{M}}\) the (unique) RKHS corresponding to kernel \(k_{M}\) and denote by \(H_{k}\) the unique RKHS of \(k\). For basic properties of these objects as well as classes of suitable kernels we refer to .

We clarify the relation between \(H_{M}\)and \(H_{k}\) in the next result.

**Theorem 2.3**.: 1) For every \(f H_{k}\), there exists a sequence \(f_{M} H_{M}\), \(M_{+}\), such that \(f_{M}[_{1}]{_{1}}f\). 2) Let \(f_{M} H_{M}\) be sequence such that there exists \(B_{ 0}\) with \(\|f_{M}\|_{M} B\) for all \(M_{+}\). Then there exists a subsequence \((f_{M_{}})_{}\) and \(f H_{k}\) with \(f_{M_{}}[_{1}]{_{1}}f\) and \(\|f\|_{k} B\).

In other words, on the one hand, every RKHS function from \(H_{k}\)arises as a mean field limit of RKHS functions from \(H_{M}\).On the other hand, every uniformly norm-bounded sequence of RKHS functions \((f_{M})_{M}\) has a mean field limit in \(H_{k}\).

Note that the preceding result is considerably stronger than the corresponding results in : In contrast to [22, Theorem 4.4] we do not need to go to another subsequence in the first item, and we ensure that the mean field limit \(f\) is contained in \(H_{k}\) (and norm-bounded by the same uniform bound), which was missing from Corollary 4.3 in the same reference.

The relation between the kernels \(k_{M}\) and their RKHSs \(H_{M}\), and the mean field limit kernel \(k\) and its RKHS \(H_{k}\) is illustrated as a commutative diagram in Figure 1. In order to arrive at the mean field RKHS \(H_{k}\), on the one hand, we consider the mean field limit \(k\) of the \(k_{M}\), and then form the corresponding RKHS \(H_{k}\). This is essentially the content of Proposition 2.2. On the other hand, we can first go from the kernel \(k_{M}\) to the associated unique RKHS \(H_{M}\) (for each \(M_{+}\)). Theorem 2.3 then says that \(H_{k}\) can be interpreted as a mean field limit of the RKHSs \(H_{M}\), since every function in \(H_{k}\) arises as a mean field limit of a sequence of functions from the \(H_{M}\), and every uniformly norm-bounded sequence of such functions has a mean field limit that is in \(H_{k}\).

Next, we state two technical results that will play an important role in the following developments, and which might be of independent interest. They describe \(\) and \(\) inequalities required for \(\)-convergence arguments used later on.

**Lemma 2.4**.: Let \(f_{M} H_{M}\), \(M_{+}\), and \(f H_{k}\) such that \(f_{M}[_{1}]{_{1}}f\), then

\[\|f\|_{k}_{M}\|f_{M}\|_{M}.\] (3)

**Lemma 2.5**.: Let \(f H_{k}\). Then there exist \(f_{M} H_{M}\), \(M_{+}\), such that \(_{M}_{ X^{M}}|f_{M}()-f([])|=0\), and

\[_{M}\|f_{M}\|_{M}\|f\|_{k}.\] (4)

## 3 Approximation with kernels in the mean field limit

Kernel-based machine learning methods use in general an RKHS as the hypothesis space, and learning often reduces to a search or optimization problem over this function space. For this reason, it is

Figure 1: The kernel \(k\) arises as the mean field limit (MFL) of the kernels \(k_{M}\) (Proposition 2.2). Every uniformly norm-bounded sequence \(f_{M} H_{M}\), \(M_{+}\), has an MFL in \(H_{k}\), and every function \(f H_{k}\) arises as such an MFL (Theorem 2.3). Based on [22, Figure 1].

important to investigate the approximation properties of a given kernel and its associated RKHS as well as to ensure that the learning problem over an RKHS (which is in general an infinite-dimensional object) can be tackled with finite computations.

The next result asserts that, under a uniformity condition, the approximation power of the finite-input kernels \(k_{M}\) is inherited by the mean field limit kernel.

**Proposition 3.1**.: For \(M_{+}\), let \(_{M}\) be the set of symmetric functions that are continuous w.r.t. \((,^{}) d_{}([],[ ^{}])\). Let \( C^{0}((X),)\) such that for all \(f\) and \(>0\) there exist \(B_{ 0}\) and sequences \(f_{M}_{M}\), \(_{M} H_{M}\), \(M_{+}\), such that 1) \(f_{M}_{1}}f\) 2) \(\|f_{M}-_{M}\|_{}\) for all \(M_{+}\) 3) \(\|_{M}\|_{M} B\) for all \(M_{+}\). Then for all \(f\) and \(>0\), there exists \( H_{k}\) with \(\|f-\|_{}\).

Intuitively, the set \(\) consists of all continuous functions on \((X)\) that arise as a mean field limit of functions which can be uniformly approximated by uniformly norm-bounded RKHS functions. The result then states (to use a somewhat imprecise terminology) that the RKHS \(H_{k}\) is dense in \(\). We can interpret this as an appropriate mean field variant of the universality property of kernels: a kernel on a compact metric space is called universal if its associated RKHS is dense w.r.t. the supremum norm in the space of continuous functions, and many common kernels are universal, cf. e.g. [20, Section 4.6]. In our setting, ideally universality of the finite-input kernels \(k_{M}\) is inherited by the mean field limit kernel \(k\). However, since the mean field limit can be interpreted as a form of smoothing limit, some uniformity requirements should be expected. Proposition 3.1 provides exactly such a condition.

**Remark 3.2**.: In Proposition 3.1, the set \(\) is a subvectorspace of \(C^{0}((X),)\). Furthermore, if the \(_{1}\)-convergence in the definition of \(\) is uniform, then \(\) is closed.

Since \(k_{M}\) and \(k\) are kernels, we have the usual representer theorem for their corresponding RKHSs, cf. e.g. . A natural question is then whether we have mean field convergence of the minimizers and their representation. This is clarified by the next result.

**Theorem 3.3**.: Let \(N_{+}\), \(_{1},,_{N}(X)\) and for \(n=1,,N\) let \(_{n}^{[M]} X^{M}\), \(M_{+}\), such that \([_{n}^{[M]}]}}_{n}\) for \(M\). Let \(L:^{N}_{ 0}\) be continuous and strictly convex and \(>0\). For each \(M_{+}\) consider the problem

\[_{f H_{M}}L(f(_{1}^{[M]}),,f(_{N}^{[M]}))+\| f\|_{M},\] (5)

as well as the problem

\[_{f H_{k}}L(f(_{1}),,f(_{N}))+\|f\|_{k}.\] (6)

Then for each \(M_{+}\) problem (5) has a unique solution \(f_{M}^{*}\), which is of the form \(f_{M}^{*}=_{n=1}^{N}_{n}^{[M]}k_{M}(,_{n}^{[M]}) H_{M}\), with \(_{1}^{[M]},,_{N}^{[M]}\), and problem (6) has a unique solution \(f^{*}\), which is of the form \(f^{*}=_{n=1}^{N}_{n}k(,_{n}) H_{k}\), with \(_{1},,_{N}\). Furthermore, there exists a subsequence \((f_{M_{t}}^{*})_{}\) such that \(f_{M_{}}^{*}_{1}}f^{*}\) and

\[L(f_{M_{t}}^{*}(_{1}^{[M_{}]}),,f_{M_{t}}^{*}(_{N}^{[ M_{}]}))+\|f_{M_{t}}^{*}\|_{M_{}} L(f^{*}(_{1}), ,f^{*}(_{N}))+\|f^{*}\|_{k}.\] (7)

for \(\).

The main point of this result is the convergence of the minimizers, which we will establish using a \(\)-convergence argument. This approach seems to have been introduced by  originally in the context of multi-agent systems.

**Remark 3.4**.: An inspection of the proof reveals that in Theorem 3.3 we can replace the term \(\|\|_{M}\) and \(\|\|_{k}\) by \((\|\|_{M})\) and \((\|\|_{k})\), where \(:_{ 0}_{ 0}\) is a nonnegative, strictly increasing and continuous function.

## 4 Support Vector Machines with mean field limit kernels

We now turn to the mean field limit of kernels in the context of statistical learning theory, focusing on SVMs. We first briefly recall the standard setup of statistical learning theory, and formulate an appropriate mean field limit thereof. We then investigate empirical and infinite-sample solutions of SVMs and their mean field limits, as well as the convergence of the corresponding risks.

Statistical learning theory setupWe now introduce the standard setup of statistical learning theory, following mostly [20, Chapters 2 and 5]. Let \(\) (associated with some \(\)-algebra) and \( Y\) closed (associated with the corresponding Borel \(\)-algebra). A _loss function_ is in this setting a measurable function \(: Y_{ 0}\). Let \(P\) be a probability distribution on \( Y\) and \(f:\) a measurable function, then the _risk_ of \(f\)_w.r.t. \(P\) and loss function_\(\) is defined by

\[_{,P}(f)=_{ Y}(x,y,f(x))P.\]

Note that this is always well-defined since \((x,y)(x,y,f(x))\) is a measurable and nonnegative function. For a set \(H^{}\) of measurable functions we also define the _minimal risk over_\(H\) by

\[^{H*}_{,P}=_{f H}_{,P}(f).\]

If \(H\) is a normed vector space, we additionally define the _regularized risk_ of \(f H\) and the _minimal regularized risk over_\(H\) by

\[_{,P,}(f)=_{,P}(f)+\|f\|_{H}^{2},^{H*}_{,P,}=_{f H}_{,P, }(f),\]

where \(_{ 0}\) is the _regularization parameter_. A _data set of size \(N_{+}\)_ is a tuple \(D_{N}=((x_{1},y_{1}),,(x_{N},y_{N}))( Y)^{N}\) and for a function \(f:\) we define its _empirical risk_ by

\[_{,D_{N}}(f)=_{n=1}^{N}(x_{n},y_{n},f(x_{n})).\]

If \(H\) is a normed vector space and \(f H\), we define additionally the _regularized empirical risk_ and the _minimal regularized empirical risk over_\(H\) by

\[_{,D_{N},}(f)=_{,D_{N}}(f)+\|f\|_ {H}^{2},^{H*}_{,D_{N},}=_{f H} _{,D_{N},}(f),\]

where \(_{>0}\) is again the regularization parameter. Note that the notation for the empirical risks is consistent with the risk w.r.t. a probability distribution \(P\), if we identify a data set \(D_{N}\) by the corresponding empirical distribution \(_{n=1}^{N}_{(x_{n},y_{n})}\).

In the following, \(H\) will be a RKHS and a minimizer (assuming existence and uniqueness) of \(^{H*}_{,P,}\) will be called an _infinite-sample support vector machine (SVM)_. Similarly, \(^{H*}_{,D_{N},}\) will be called the _empirical solution of the SVM w.r.t. the data set_\(D_{N}\). Note that this is the common terminology in statistical learning theory, cf. , and corresponds to (empirical) risk minization with Tikhonov regularization.

Statistical learning theory setup in the mean field limitLet now \( Y\) be compact and \(_{M}:X^{M} Y_{ 0}\), \(M\), such that 1) \(_{M}(,y,t)=_{M}(,y,t)\) for all \( X^{M}\), \(_{M}\), \(y Y\), \(t\); 2) there exists \(C_{}_{ 0}\) and a nondecreasing function \(b:_{ 0}_{ 0}\) with \(|_{M}(,y,t)| C_{}+b(|t|)\) for all \(M\) and \( X^{M},y Y,t\); 3) there exists \(L_{}_{ 0}\) with

\[|_{M}(_{1},y_{1},t_{1})-_{M}(_{2},y_{2},t_{2})| L_{ }(d_{}([_{1}],[_{2}])+|y_{1}-y_{2 }|+|t_{1}-t_{2}|)\]

for all \(_{1},x_{2} X^{M}\), \(y_{1},y_{1}^{} Y,t_{1},t_{2}\). In particular, all \(_{M}\) are measurable (assuming the Borel \(\)-algebra on \(X^{M}\)) and hence are loss functions on \(X^{M} Y\). Proposition 2.1 ensures the existence of a subsequence \((_{M_{m}})_{m}\) and an \(L_{}\)-Lipschitz continuous function \(:(X) Y\) with

\[_{M}_{ X^{M_{m}}\\ y Y,t K}|_{M_{m}}(,y,t)-([ ],y,t)|=0\] (8)

for all compact \(K\), and we write again \(_{M_{m}}_{1}}\). _For readability, from now on we switch to this subsequence._ Furthermore, we also get from Proposition 2.1 that there exists some \(C_{L}_{ 0}\) such that \(|(,y,t)| C_{L}+b(|t|)\) for all \((X),y Y,t\).

**Remark 4.1**.: Note that, for Proposition 2.1 to apply, it is enough to assume in item 2) above the existence of a function \(b:_{ 0}\) with \(|_{M}(,y,t)| C_{}+b(|t|)\). However, we chose the slightly stronger condition that \(b\) is nondecreasing, since then \(_{M}\) is a _Nemitskii loss_ according to [20, Definition 2.16]. Since the function with constant value \(C_{}\) is actually \(P_{M}\)-integrable, this means that \(_{M}\) is even a \(P_{M}\)_-integrable Nemitskii loss_ according to . A similar remark then applies to \(\).

**Lemma 4.2**.: The function \(\) is nonnegative. Furthermore, if all \(_{M}\) are convex loss functions [20, Definition 2.12], i.e., if for all \(M_{+}\), \( X^{M},y Y,t_{1},t_{2}\) and \((0,1)\) we have

\[_{M}(,y, t_{1}+(1-)t_{2})_{M}(,y,t_{1})+(1-)_{M}(,y,t_{2}),\] (9)

then so is \(\).

Empirical SVM solutionsGiven data sets \(D_{N}^{[M]}=((_{1}^{[M]},y_{1}^{[M]}),,(_{N}^{[M]},y_ {N}^{[M]}))\) for all \(M_{+}\) with \(_{n}^{[M]} X^{M}\), \(y_{n}^{[M]} Y\), and \(D_{N}=((_{1},y_{1}),,(_{N},y_{N}))\) with \(_{n}(X)\) and \(y_{n} Y\), we write \(D_{N}^{[M]}[_{1}]{}D_{N}\) if \([_{n}^{[M]}][}]{}_{n}\) and \(y_{n}^{[M]} y_{n}\) (where \(M\)) for all \(n=1,,N\). We can interpret this as mean field convergence of the data sets.

Furthermore, consider the empirical risk of hypothesis \(f_{M} H_{M}\) (and \(f H_{k}\)) on data set \(D_{N}^{[M]}\) (and \(D_{N}\))

\[_{_{M},D_{N}^{[M]}}(f_{M})=_{n=1}^{N}_{M}( _{n}^{[M]},y_{n}^{[M]},f_{M}(_{n}^{[M]})),_{ ,D_{N}}(f)=_{n=1}^{N}(_{n},y_{n},f(_{n})),\]

and the corresponding regularized risk

\[_{_{M},D_{N}^{[M]},}(f_{M}) =_{n=1}^{N}_{M}(_{n}^{[M]},y_{n}^{[M]},f_{M}(_{n}^{[M]}))+\|f_{M}\|_{M}^{2}\] \[_{,D_{N},}(f) =_{n=1}^{N}(_{n},y_{n},f(_{n}))+ \|f\|_{k}^{2},\]

where \(_{>0}\) is the regularization parameter.

**Proposition 4.3**.: Let \(>0\), assume that all \(_{M}\) are convex and let \(D_{N}^{[M]}\), \(D_{N}\) be finite data sets with \(D_{N}^{[M]}[_{1}]{}D_{N}\). Then for all \(M_{+}\), \(H_{M} f_{M}_{_{M},D_{N}^{[M]},}(f_{M})\) has a unique minimizer \(f_{M,}^{*} H_{M}\) and \(H_{k} f_{,D_{N},}(f)\) has a unique minimizer \(f_{}^{*} H_{k}\). Furthermore, for all \(M_{+}\) there exist \(_{n}^{[M]}\), \(n=1,,N\), such that \(f_{M,}^{*}=_{n=1}^{N}_{n}^{[M]}k_{M}(,_{n}^{[M]})\), and there exist \(_{1},,_{N}\) such that \(f_{}^{*}=_{n=1}^{N}_{n}k(,_{n})\). Finally, there exists a subsequence \((f_{M_{m},}^{*})_{m}\) such that \(f_{M_{m},}^{*}[_{1}]{}f_{}^{*}\) and \(_{_{M_{m}},D_{N}^{[M_{m}]},}(f_{M_{m},}^{*}) _{,D_{N},}(f_{}^{*})\) for \(m\).

Convergence of distributions and infinite-sample SVMs in the mean field limitWe now turn to the question of mean field limits of distributions and the associated learning problems and SVM solutions. Let \((P^{[M]})_{M}\) be a sequence of distributions, where \(P^{[M]}\) is a probability distribution on \(X^{M} Y\), and let \(P\) be a probability distribution on \((X) Y\). We say that \(P^{[M]}\)_converges in mean field to \(P\)_ and write \(P^{[M]}[_{1}]{}P\), if for all continuous (w.r.t. the product topology on \((X) Y\)) and bounded 4\(f\) we have

\[_{X^{M} Y}f([],y)P^{[M]}(,y) _{(X) Y}f(,y)P(,y).\] (10)

This convergence notion of probability distributions (on different input spaces) appears to be not standard, but it is a natural concept in the present context. Essentially, it is weak (also called narrow) convergence of probability distributions adapated to our setting.

Consider now data sets \(D_{N}^{[M]}\), \(D_{N}\), with \(D_{N}^{[M]}[_{1}]{}D_{N}\), then we also have convergence in mean field of the datasets, interpreted as empirical distributions: let \(f C^{0}((X) Y,)\) be bounded, then

\[_{X^{M} Y}f([],y)D_{N}^{[M]}( ,y) =_{n=1}^{N}f([_{n}^{[M]}],y_{n}^{[M]})\] \[[M]{} _{n=1}^{N}f(_{n},y_{n})=_{(X)  Y}f(,y)D_{N}(,y).\]This shows that the mean field convergence of probability distributions as defined here is a direct generalization of the natural notion of mean field convergence of data sets.

Finally, consider the risk of hypothesis \(f_{M} H_{M}\) and \(f H_{k}\) w.r.t. the distribution \(P^{[M]}\) and \(P\), respectively,

\[_{_{M},P^{[M]}}(f_{M}) =_{X^{M} Y}_{M}(,y,f_{M}())P ^{[M]}(,y)\] \[_{,P}(f) =_{(X) Y}(,y,f())P(,y),\]

as well as the minimal risks

\[^{H_{M}*}_{_{M},P^{[M]}}=_{f_{M} H_{M}}_{ _{M},P^{[M]}}(f_{M})^{H_{k}*}_{,P}=_{f H_{k}} _{,P}(f).\]

Our first result ensures that mean field convergence of distributions \(P^{[M]}\), loss functions \(_{M}\) and data sets \(D^{[M]}_{N}\) ensures the convergence of the corresponding risks of the empirical SVM solutions.

**Lemma 4.4**.: Consider the situation and notation of Proposition 4.3 and assume that \(P^{[M]}_{1}}}{{}}P\). We then have \(_{_{M_{m}},P^{[M_{m}]}}(f^{*}_{M_{m},})_{ ,P}(f^{*}_{})\) for \(m\).

Next, we investigate the mean field convergence of infinite-sample SVM solutions and their associated risks. Define for \(_{ 0}\) (and all \(M_{+}\)) the regularized risk of \(f_{M} H_{M}\) and \(f H_{k}\), respectively, by

\[_{_{M},P^{[M]},}(f_{M})=_{_{M},P^{[M]}}( f_{M})+\|f_{M}\|_{M}^{2},_{,P,}(f)=_{ ,P}(f)+\|f\|_{k}^{2},\]

and the corresponding minimal risks by

\[^{H_{M}*}_{_{M},P^{[M]},}=_{f_{M} H_{M}}_{_{M},P^{[M]},}(f_{M}),^{H_{k}*}_{,P, }=_{f H_{k}}_{,P,}(f).\]

**Proposition 4.5**.: 5 Let \(>0\), assume that all \(_{M}\) are convex loss functions and let \(P^{[M]}\) and \(P\) be probability distributions on \(X^{M} Y\) and \((X) Y\), respectively, with \(P^{[M]}_{1}}}{{}}P\). Then for all \(M_{+}\), \(H_{M} f_{M}_{_{M},P^{[M]},}(f_{M})\) has a unique minimizer \(f^{*}_{M,} H_{M}\) and \(H_{k} f_{,P,}(f)\) has a unique minimizer \(f^{*}_{} H_{k}\). Furthermore, there exists a subsequence \((f^{*}_{M_{m},})\) such that \(f^{*}_{M_{m},}_{1}}f^{*}_{}\) and \(_{_{M_{m}},P^{[M_{m}]},}(f^{*}_{M_{m},}) _{,P,}(f^{*}_{})\) for \(m\). In particular, \(^{H_{M_{m}}*}_{_{M_{m}},P^{[M_{m}]},}^{H_ {k}*}_{,P,}\).

Finally, we would like to show that \(^{H_{M}*}_{_{M},P^{[M]}}^{H_{k}*}_{,P}\) for \(P^{[M]}_{1}}}{{}}P\). Up to a subsequence, this is established under Assumption 4.6. Define the _approximation error functions_, cf. [20, Definition 5.14], by

\[A_{2}^{[M]}()=_{f H_{M}}_{_{M},P^{[M]},}( f)-^{H_{M}*}_{_{M},P^{[M]}} A_{2}()=_{f H _{k}}_{,P,}(f)-^{H_{k}*}_{,P},\]

where \(M_{+}\) and \(_{ 0}\). Note that (for all \(M_{+}\)) \(A_{2}^{[M]},A_{2}:_{ 0}_{ 0}\) are increasing, concave and continuous, and \(A_{2}^{[M]},A_{2}(0)=0\), cf. [20, Lemma 5.15]. We need essentially equicontinuity of \((A_{2}^{[M]})_{M}\) in 0, which is formalized in the following assumption.

**Assumption 4.6**.: For all \(>0\) there exists \(_{}>0\) such that for all \(0<_{}\) and \(M_{+}\) we have \(A_{2}^{[M]}()\).

**Proposition 4.7**.: Assume that all \(_{M}\) are convex loss functions, let \(P^{[M]}\) and \(P\) be probability distributions on \(X^{M} Y\) and \((X) Y\), respectively, with \(P^{[M]}_{1}}P\). If Assumption 4.6 holds, there exists a strictly increasing sequence \((M_{m})_{m}\) with \(^{H_{M_{m}}*}_{_{M_{m}},P^{[M_{m}]}}^{H_{k}*}_{ ,P}\) for \(m\).

Conclusion

We investigated the mean field limit of kernels and their RKHSs, as well as the mean field limit of statistical learning problems solved with SVMs. In particular, we managed to complete the basic theory of mean field kernels as started in . Additionally, we investigated their approximation capabilities by providing a first approximation result and a variant of the representer theorem for mean field kernels. Finally, we introduced a corresponding mean field limit of statistical learning problems and provided convergence results for SVMs using mean field kernels. In contrast to other settings involving a large number of variables, for example, infinite-width neural networks, here we considered the case of an increasing number of inputs. This work opens many directions for future investigation. For example, it would be interesting to remove or weaken Assumption 4.6 for a result like Proposition 4.7. Another relevant direction is to find approximation results that are stronger than Proposition 3.1. Finally, it would be interesting to investigate whether statistical guarantees, like consistency or learning rates, for the finite-input learning problems can be transferred to the mean field level.