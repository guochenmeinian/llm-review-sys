# Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers

Xiuying Wei

xiuying.wei@epfl.ch

CLAIRE, EPFL &Skander Moalla

skander.moalla@epfl.ch

CLAIRE, EPFL &Razvan Pascanu

razp@google.com

Google DeepMind &Caglar Gulcehre

caglar.gulcehre@epfl.ch

CLAIRE, EPFL

###### Abstract

State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models' parameter counts and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFNs), which are less studied than attention blocks. We consider three structured linear parameterizations of the FFN using efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from a training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We demonstrate that these structures can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called _self-guided training_, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Interestingly, the scaling performance of structured matrices is explored, revealing steeper curves in scaling training FLOPs, along with a favorable scaling trend in the overtraining regime. Specifically, we show that wide and structured networks can utilize training FLOPs more efficiently, with fewer parameters and lower loss than dense models at their optimal trade-off. Our code is available at https://github.com/CLAIRE-Labo/StructuredFFN/tree/main.

  
**Method** & **\#Param** & **Training FLOPs** & **PPL** & **TP (token/s)** \\  Transformer-m & 335M & 1.55e+19 & 18.29 & 30292 \\ Transformer-m (GQA) & 335M & 1.55e+19 & 18.23 & 84202 \\
**Wide and Structured** & **219M** & 1.35e+19 & **17.89** & **91147**(8\%) \\  Transformer-l & 729M & 7.03e+19 & 14.29 & 23351 \\ Transformer-l (GQA) & 729M & 7.03e+19 & 14.40 & 64737 \\
**Wide and Structured** & **46-M** & 7.03e+19 & **14.27** & **75930**(17\%) \\   

Table 1: **Better training FLOPs utilization of the wide and structured Networks**: we compare dense Transformers trained according to their optimal scaling law , efficient Transformers (GQA)  with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.

Figure 1: **Steeper scaling curves of LowRank with 63% or 32% FFN parameters**. For more results, see Sec. 4.2.

Introduction

Transformer language models  have gained significant attention for their performance and scalability. These models have grown from hundreds of millions of parameters  to hundreds of billions [5; 6; 7], increasing the need for efficient training and inference techniques. While much research focuses on attention, _feed forward networks_ (FFNs) account for over 60% of the model's parameters and FLOPs, significantly impacting latency.1 Recent large-scale models [8; 9] further increase the FFN size, leading them to dominate the cost of the model compared to the attention layer.

Structured linear transformations, such as low-rank or block-diagonal matrices, are important paradigms for reducing the computational cost of feedforward layers. However, they have not yet been thoroughly explored at a sufficient scale to reduce pre-training costs and latency of the inference phase in modern LLM architectures, where the main focus so far has been on improving the efficiency of the self-attention mechanism.

In this work, we investigate structured matrices for FFN blocks from the train-from-scratch aspect, first identifying their efficiency and optimization challenges and then presenting experimental results, analyzing and characterizing the behavior of models trained with structured matrices, and comparing their results. We consider three efficient linear parametrizations: LowRank, BlockShuffle (comprising two block-diagonal matrices), and BlockDense (a combination of dense and block-diagonal matrices). First, while they have demonstrated materialized computational gains, they face challenges in the practical online decoding scenario of LLM, which may process only limited input tokens at one time, leading to under-utilization of computing resources and decreased efficiency due to the additional linear projection. We address this with a pre-merge technique that restores efficiency to the original dense parametrization. Second, we observe that these parameterizations of the FFN blocks are harder to train than standard linear layers, often exhibiting poorer training dynamics like loss spikes. To counter this, we propose a flexible and fast method we refer to as _self-guided training_. It employs a dense matrix as a residual component during the initial training phase, steering the training process away from suboptimal starting points gradually.

We conduct our experiments at scale on Transformers ranging from 110M to 1.3B parameters by replacing the traditional heavy FFN with structured matrices. Our experiments first show the scaling behavior of these structured linear parameterizations and then illustrate how our proposed methods address their general efficiency and optimization challenges. First, we examine scaling performance from the perspectives of training compute and model size, highlighting the potential of structured matrices. By controlling for the same training FLOPs, we find that structured FFNs show steeper loss scaling curves than traditional Transformers at optimal trade-offs (See Fig. 4 and Fig. 1). Specifically, as seen in Table 1, our wide and structured networks use training FLOPs more efficiently, needing fewer parameters (464M vs. 729M) and achieving a 17% throughput boost on the -1 scale, while still maintaining slightly better perplexity compared to the efficient Transformer . Beyond training compute scaling, we also scale model size in the overtraining regime, with Fig. 5 showing favorable scaling trends for our wide and structured models. Second, our results on efficiency show that structured FFNs, with only 32% of the FFN parameters, can boost the training speed of the 1.3B model by 1.35\(\). Furthermore, self-guided training enhances the performance of all three structured matrices (e.g., reducing the perplexity gap of LowRank to about 0.4) without affecting the inference time speed-up.

As the first work to explore structured matrices at the scale of recent LLMs, we hope our findings and results will shed new light on the study of efficient NLP architectures. Our contributions can be categorized into the following three aspects:

1. We investigate three types of structured matrices in Transformer pretraining and demonstrate their favorable scaling behavior compared to dense models. This is revealed through the study of scaling laws for training FLOPs, as well as model size scaling in the overtraining regime, showing that wide and structured networks can be strong candidates for architecture design.
2. We conduct an efficiency study of these structured matrices across various scenarios. We propose a pre-merge technique to maintain speed in a specific case and show the effective speed-up performance of structured matrices in other scenarios.
3. We identify optimization challenges in structured matrices and introduce a method called self-guided training, which efficiently improves training dynamics and boosts the final performance for all three types of structured matrices.

Method

Multiple techniques have been proposed to approximate linear projections from sparsity to low-rank approximations. We provide an in-depth discussion of existing literature on approximating linear layers in section 3, that better contextualizes our work. We focus on structured approximations of linear projections \(g()=\) that have the form \(g()=()\), where \(\) and \(\) are structured matrices, e.g. low-rank or block-diagonal. We opt for this particular form because it allows us to readily exploit the computational gains on existing hardware using existing libraries with minimal alteration. Such approximations have been previously studied in different contexts. Our contributions are exploring them (i) to approximate FFN layers of transformers, (ii) when applied from initialization, (iii) testing them at up to 1.3B scale, investigating their general bottlenecks and providing scaling analyses.

### Structured linear parametrization

We explore three structured matrices to approximate the standard linear layer \(\), maintaining its input dimension \(N\) and output dimension \(M\) of weight \(\).

LowRankLow-rank matrices have been widely used to decompose pre-trained weights for downstream compression  or to construct adapters for efficient fine-tuning . Researchers  suggest that dense layers tend to naturally converge to low-rank solutions during training, making this approximation ideal. Inspired by this, we explore low-rank matrices as alternatives to traditional linear layers, imposing this structure from initialization and investigating it during pre-training.

Formally, the low-rank approximation of a linear layer is given as \(^{r}(^{r})\) where \(^{r}^{M R}\), \(^{r}^{R N}\) and \(R<(M,N)\). Note that we use the superscript \({}^{r}\) to indicate that these matrices are used to create a low-rank approximation by projecting to or from a low-dimensional code, a notation that would become useful later on to distinguish such components from block-diagonal ones. The parameter count and MAC (Multiply-Accumulate Operations) decrease from \(M N\) to \((M+N) R\).

BlockShuffleDao et al.  proposes using the Monarch decomposition of FFT, replacing the linear layer with interleaved block-diagonal and permutation matrices. An alternative motivation for such a structure can be derived from efficient convolutional designs of ShuffleNet  and separable convolution . For simplicity, we explore the form introduced by ShuffleNet to linear layers.

The core idea of BlockShuffle is to reshape the feature dimension into two dimensions and first use a linear projection that mixes along one of these fictive dimensions, followed by a linear projection that mixes along the other. More precisely, we first reshape the input features \(^{N}\) into \(B\) blocks and apply the non-tied weight of \(\) to each block, then flatten the intermediate feature. To achieve global perception, we regroup elements from different blocks into \(B\) new blocks and apply the same transformation for each block again.

Figure 2: **Structured linear parametrization:** We show the structured linear parametrization with input dim. of \(N\) and output dim. of \(M\). a) The traditional dense linear parametrization. b) LowRank parametrization with a bottleneck of size \(R\) where \(R\) is less than \(M\) and \(N\). c) BlockShuffle with two block-diagonal matrices with blocks of size \(B\) interleaved with a shuffle operations that mixes information from different blocks similar to ShuffleNet. d) BlockDense with the first matrix as a block-diagonal and the second a low-rank or dense matrix.

Technically, we can express the per-block transformation using block-diagonal matrices and formulate the above process as \( f^{-1}(^{b}f(^{b}))\), where block-diagonal matrices \(^{b}\) and \(^{b}\) has \(B\) blocks with shapes \(\) and \(\) per-block. As shown in Fig. 2, the shuffle function \(f()\) enables global feature mixing by cycling different blocks and can be implemented by simply transposing and reshaping inner features. The inverse function \(f^{-1}()\) permutes the outputs back to their original order.

By separating features into two dimensions, only a few elements of the features will be processed each time. The parameter count and MAC are reduced from \(M N\) to \((N,M)\), where \(B\) acts as a trade-off of accuracy and efficiency.

BlockDenseThe previous parametrization incorporates additional shuffle operations, which can be slow on the device. We propose a natural intermediate candidate between LowRank and BlockShuffle, combining the block-diagonal projection \(^{b}\) with a dense or low-rank projection \(^{r}\). Thus, we can mix the features between blocks without permuting the inner features. The formula is defined as \(^{r}(^{b})\), where \(^{b}\) is the block-diagonal matrix with \(b\) blocks in shape \(\), and \(^{r}\!\!^{M R}\). Technically, the second projection does not need to be a low-rank approximation because \(R\) can be larger than \(M\). Nevertheless, in practice, we chose \(R\!<\!M\) to limit the search space of this work, and thus use the superscript \(r\) for the second matrix. The parameters of this method are determined by two variables \(B\) and \(R\), cutting the original burden from \(M N\) to \(R(M+)\). Note that BlockDense can recover the LowRank approximation if we set \(B\!=\!1\) and \(R\!<\!(M,\!N)\).

RemarkWe limit our exploration of the efficient linear parametrizations within the FFN blocks.These typically have \(8 H^{2}\) parameters and MAC, where \(H\) standards for hidden state dimension and \(4 H\) as the intermediate hidden size of FFN. In contrast, the proposed parametrizations have:

\[10 H R10 5 H R(1+)\]

Although BlockDense is introduced as a new parameterization, the aim of this paper is not to claim it as the best candidate, but rather to investigate some general properties of structured matrices from efficiency, optimization, and scaling perspectives. Given the favorable efficiency and loss performance of BlockDense, it is included alongside LowRank and BlockShuffle here to cover a broader range of potential parameterizations.

### Maintaining efficiency during online decoding

Parallelism-bound FFNWith reduced FLOPs and parameters, our proposed linear parametrization can accelerate the model for compute-bound and memory-bound scenarios , usually during training, prefilling, and decoding with a relatively big batch size. However, for online decoding with a very small batch size and sequence length of 1, a practical scenario for LLM, both FFN and structured FFN can become parallelism-bound  with poor utilization of the GPU resources, especially on powerful devices like A100. Because each linear projection suffers from parallelism-bound, efficient linear parametrization may lead to worse latency performance due to doubling the number of linear projections. We propose a _pre-merge technique_ to mitigate the issue.

Pre-merge techniqueTaking advantage of the fact that these parametrizations do not have non-linearity, we propose to combine the structured matrices into a single dense layer and keep both the structured and the dense one for online decoding. Then, we can dynamically decide which parametrization to use based on the current batch size and setting. Fig. 7 analyzes using structured or dense forms for different batch and model sizes, allowing us to decide when to use the pre-merged linear projection.

### Addressing the optimization challenge

Using the efficient parametrization from initialization can suffer from optimization difficulty because the deep linear parametrization introduces additional symmetries2, which is a source of proliferation of saddle points and generally less smooth loss function as pointed out in [18; 19]. We hypothesize that this makes poorer learning dynamics of the structured parametrization. Empirically, we found that the deep linear form \(()\) is more difficult to train than the standard linear layer. For example, in Fig. 3, it can suffer from training instability and loss spikes with a large learning rate, while also converging much more slowly than the dense form with a small learning rate. We further elaborate on this, highlighting how the inconsistency of gradient updates between the structured parametrization and original linear projection affect learning dynamics in Appendix B.2.

**Self-guided training** Addressing the poor training dynamics by carefully tuning the learning rate schedule and gradient clipping coefficient might be possible, but it is costly and may switch between slow convergence and training instability. We propose a less costly and simple approach that can be used with minimal re-tuning of hyperparameters.

To motivate our proposal, we start by finding that the updates on \(\) scales the function of the backpropagated gradients \(\) (see App. B.2), then turn into the typical training dynamics with gradients. An issue that needs to be addressed in the early stage of training is feature specialization when the learning process assigns semantics to the different hidden units of the model, sometimes also phrased as identifying the winning ticket . In this process, certain weights will need to be suppressed, and symmetries in the parametrization must be resolved.

To address this problem, we propose using the dense parametrization \(\) as a crutch to efficiently make decisions about feature specialization and then transfer them to \(\) and \(\) through \(\). To this end, we use the following parametrization

\[\!=\!\!\!\!+\!(1\!-\!)\!\!( ).\] (1)

\(\) is the layer's output, and \(\) decays following a cosine scheduler. As a residual component, learning \(\) is unaffected by the additional saddles and pathologies introduced by the structured parametrization, allowing units to specialize. This _guides_ the training of \(\) and \(\), which are forced slowly to take over by providing the hidden units semantics learned by \(\). This approach also relates to homotopy methods such as _simulated annealing_, where a hard optimization problem is transformed into an _easier_ to optimize form with certain desired properties, gradually transforming the problem to its original form. Here, we consider the easier optimization problem is to train with dense matrices. By decreasing alpha from \(1\) to \(0\), we transform the easier-to-optimize loss into the original parametrization we want to use.

Furthermore, we initialize \(_{0}=_{0}_{0}\), making the observation that by using this initialization for the dense residual branch, we can easily start the _guided training_ at any stage of learning (e.g., fine-tuning) without affecting the model behavior. We refer to this as self-guiding training, as the better learning dynamics from \(\), which initially is just \(\), guide the learning of \(\) and \(\) through the backpropagated gradients \(\).

Guided by the dense weights, which do not have the symmetry problem, it becomes much easier for the structured matrices to learn a good representation. From Fig. 3, it can be observed that the self-guided training prevents training spikes and fastens the convergence process. Notably, it benefits all three structured matrices with improved training dynamics illustrated in Sec. B.1 and better final performance shown in Sec. 4.4.

Figure 3: **Poor training dynamics: Training dynamics of LowRank with rank of 128 under different training configurations. Curves correspond to a 4-layer Transformer with a model width of 768 on WikText-103. We apply self-guided training in the first half of training. Refer to Sec. B.1 for more training dynamics visualizations of the other two structured parameterizations.**

**Reducing the computational cost of self-guided training:** Note that while \(>0\), we need to perform forward and backward passes through the dense version of the weight matrix \(\), which could be expensive. To address this issue, we consider the following stochastic version of the above formula, which allows us to control how often we need to use the dense residual branch:

\[\!=\!\!\!\!+\!(1\!-\!)\!\! (),&p\!<\!\\ (),&p\!\!.\] (2)

In our practice, \(p\) is a random variable sampled independently from a uniform distribution over 0 to 1 in each training forward pass. With \(\) following a cosine annealing schedule, this softer version Eq. (2) reduces the expected computation to half of Eq. (1). For example, using our method for half the training time increases the FLOPs by only 25% of the original FFN. This has a negligible impact on accuracy. More ablation studies of this technique are presented in Sec. B.3.

## 3 Related work

**Efficient techniques for training LLMs** Recent advancements in attention mechanisms have significant improvements in the efficiency of attention [21; 22; 23; 2; 24; 25; 26; 27; 28; 29], and the focus has shifted towards improving FFNs in LLMs, which contributes to at least half of the training time. Dynamic architectures such as mixtures of experts [30; 31; 32], or optimizers with faster convergence [33; 34] have been popular in improving training efficiency. Moreover, Xia et al.  employs structured pruning with learned sparsity masks and a dedicated data-loading policy to reduce the training budget.

There has been a recent focus on parameter-efficient fine-tuning methods like LoRA  and structured approximation of the linear layers (see, ). LoRA uses the low-rank approximation to reduce trainable parameters during the finetuning phase, whereas Sharma et al.  selectively applies low-rank decomposition to well-trained weights. While these methods used low-rank approximation of the weights, they did not focus on pre-training.

**Structured matrices in deep learning** Researchers use structured matrices in the form of dense matrices with shared parameters, like Circulant and Toeplitz 3, and structured matrices, such as low-rank and diagonal, to reduce parameters and FLOPs while optimizing CUDA kernel use. Low-rank matrices, initially used in convolutional networks , have shown high efficiency in training , achieving up to a 2.9\(\) speed-up with similar performance. Some studies [39; 40] adapt the rank during training and suggest regularizers to maintain SVD decomposition for better accuracy. Khodak et al.  propose spectral initialization and aligning weight decay of matrix products with standard linear layers. However, these studies mainly focus on ResNets  rather than recent LLMs. There have been other studies that aim to improve the expressiveness of structured matrices. For instance, Moczulski et al.  uses interleaved diagonal and Fourier transforms, while Dao et al.  proposes butterfly parametrization for various transformations. These approaches often lack efficiency due to additional layers or irregular sparsity patterns. Dao et al.  simplified butterfly matrices to block-diagonal ones, achieving a 2\(\) speed-up on WikiText-103 language modeling tasks. In this work, for accuracy and efficiency, we explored low-rank factorization of weight matrices with reduced bottleneck dimension and block-diagonal matrices to reduce parameters in our LLM training studies.

## 4 Experiments

In our experiments, we empirically analyzed the performance of scaling, efficiency, and self-guided training for structured parameterization in LLMs.

### Settings

ModelWe perform the experiments on the standard Transformer architecture [45; 4] equipped with _rotary positional embeddings_ and the Llama Tokenizer . Its FFN module is composed of two linear layers and a GeLU activation. Four sizes are considered, including Transformer-s (110M), Transformer-m (335M), Transformer-l (729M), and Transformer-xl (1.3B). For our efficientparameterizations, we only make the FFN module structured in most experiments to simplify our study, as the attention module has been well-studied [2; 23]. We explore two sizes that retain 63% or 32% of the dense FFN parameters by adjusting the rank and number of blocks (e.g., using a rank half the FFN width in LowRank reduces the parameters to 63%). In particular, to provide more comparative results with dense models in the scaling study Sec. 4.2, we further design the _wide and structured networks_, where both the attention and FFN modules are made structured using  and the structured matrices. This is because allocating more parameters to the FFN compared to the attention module is more favorable, and making them both structured helps maintain the parameter ratio between them. Detailed configurations can be found in Table 10.

For implementation, we take Dao et al. 's implementation for the BlockShuffle and carefully manage memory copies for BlockDense. In our experiments, we chose \(B\) as a common divisor of \(M\) and \(N\) or \(R\). Proper initialization is also investigated in Sec. A.1.

TrainingWe use the RefinedWeb dataset  and randomly select 0.5B tokens for validation, reserving the rest for training. All experiments, except for the overtraining experiments on 300B tokens in Fig. 5, are based on the Chinchilla scaling law , where tokens are allocated at 20 times the baseline model size. We set hyperparameters such as learning rates and global batch size according to the scaling law studies from recent papers [49; 50]. However, for the 300B token experiments, we found that more advanced hyperparameter settings are necessary. For example, we use betas of [0.9, 0.98]. Additionally, different works tend to use very different learning rates [51; 49; 52; 47] in the overtraining regime. Thus, we follow the scaling law of hyperparameters described in Bi et al.  to avoid extensive hyperparameter searches. Other implementations include using A100 80G GPUs, mixed precision (bfloat16 and float32), and adopting fast CUDA kernels like Flash Attention  for all experiments. We measure training FLOPs as in Megatron , including all matrix multiplications. Additional details are provided in Appendix C.

### Scaling analysis

We evaluate the scaling performance of structured linear parameterizations from two perspectives. The first study investigates the scaling law of training-time compute. The second study trains these models with 300B tokens and evaluates their performance on downstream tasks. The results show that our structured matrices can serve as a strong alternative to the dense FFN, utilizing training FLOPs more efficiently (e.g., smaller model and lower loss) and performing better in the overtraining regime.

Scaling law study: better training FLOPs utilizationBased on the Chinchilla scaling law, we train four sizes of Transformer models and then use the same amount of tokens to train the structured alternatives. First, we only make the FFN module structured to build the basic understanding, and retain 63% or 32% of the original parameters. In Fig. 4 and Fig. 1, we apply a linear fit to the scaling points for better illustration and show that all three structured matrices have _steeper scaling

Figure 4: Scaling curves of structured matrices with a linear fit for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain 63% or 32% of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the x-axis is further extended.

_curves_ compared to the dense models, indicating the significant potential of highly structured large models. More importantly, by fixing the training FLOPs, they have fewer parameters and eventually achieve very close or even slightly lower loss (e.g., LowRank with 63% parameters). Given their steeper scaling curves, we can also expect noticeably lower loss and fewer parameters for structured parameterizations per FLOP when the x-axis is further extended. Detailed numbers are provided in Table 6 in the appendix, with comparisons among the three structured parameterizations.

Next, the attention module is also structured using GQA , resulting in wide and structured networks. This further optimizes the use of training FLOPs, addressing the imbalance caused by structuring only the FFN module, which increases the relative impact of the attention module on the overall architecture. We adopt LowRank as an example, as it demonstrates superior performance compared to the other two approaches in our settings, as demonstrated in Table 6 and Fig. 4. To align the training FLOPs, the wide and structured networks are trained on a larger number of tokens. It can be observed in Table 1 that these models achieve lower perplexity while using much fewer parameters. For instance, the parameter count can be reduced from 729M to 464M without compromising perplexity. Additionally, in terms of maximum throughput, ours models achieve an 8% and 17% boost on Transformer-m and Transformer-l, respectively, compared to the fast GQA.

In conclusion, the structured matrices and the wide and structured networks demonstrate great potential in optimizing training FLOP utilization, achieving lower loss with fewer parameters. Additionally, it is important to note that our scaling curves for the structured matrices are not drawn at their optimal training-compute trade-off, while the baseline is.

Scaling model size: better downstream performanceTo further illustrate the potential of structured matrices, we consider the overtraining regime and use LowRank as an example. Specifically, we train four sizes of the dense model on 300B tokens, and build the wide and structured networks upon the design of the dense models by applying LowRank to the FFN and reducing the number of attention heads to make the entire network structured. Then, the well-trained models are evaluated on downstream tasks, including PIQA, HellaSwag, Winogrande, and ARC tasks, using lm-evaluation-harness4 with the default prompt. Fig. 5 presents the results, displaying the scaling trend across the four tasks (see detailed numbers and additional tasks in Table 8). The wide and structured models demonstrate comparable or superior performance, particularly at larger sizes, solidifying their benefits over dense architectures.

### Efficiency study

We investigate the efficiency of structured FFN and consider different numbers of tokens \(T\) to discuss different scenarios. Here, \(T\) corresponds to the total number of tokens in a batch.

Figure 5: Zero-shot performance on downstream tasks in the overtraining regime. The wide and structured networks are built upon dense ones by applying LowRank to the FFN and reducing the number of attention heads to make the entire network structured.

Large number of tokensUsing large \(T\), the standard linear layers and our efficient structured parametrizations become computation-bound where FLOPs become a latency bottleneck . This setting mainly concerns training, the prefill phase of inference, and extensive offline decoding. In Fig. 6, we evaluate the latency performance of structured and dense FFNs across different FFN widths with 30K tokens. With parameters and FLOPs reduced to 63% or 32%, the lowrank and BlockDense achieve a 1.4\(\) or a 2.5\(\) speed-up, respectively. BlockShuffle offers modest improvements, with 1.1\(\) and 2.0\(\) speed-ups for the two cases. We also measure the training time of the whole model in Table 2, and observe that LowRank with 63% FFN parameters reduces the training time by about 15% with 0.4 increased perplexity, and the one with 32% FFN parameters offers 1.35\(\) whole training speed-up with 1.1 increased perplexity.

Small number of tokensFFN can be parallelism-bound with small \(T\) (e.g., \(T=16\)) on the A100 GPUs. Then, when \(T\) gets increased, FFN becomes memory-bound and will eventually be computation-bound. Online and offline decoding stages may encounter a small number of tokens when unrolling the model step by step. As discussed earlier, our pre-merge method can alleviate the parallelism-bound issue and maintain the same latency with dense matrices. Fig. 7 shows the latency results for three different widths, varying the batch of tokens to determine when to use efficient alternatives or choose pre-merged dense matrices. For example, with a 2048-width FFN, it is difficult to fully utilize resources on GPU with limited tokens. The performance improves significantly when using width 5120 and 6144, such as speed improvements of 2.63\(\) speed-up of LowRank with 32% FFN parameters on \(T=2048\) and 2.81\(\) acceleration of BlockDense with 32% parameters on \(T=1536\).

### Self-guided training

We apply self-guided training during the first half of training to demonstrate its effectiveness. As shown in Table 3 and Table 9, our method consistently reduces loss across all efficient parametrizations, improving the perplexity by 1.2 for Transformer-s and 0.8 for Transformer-m. Then, to enable a straightforward comparison under the same training FLOPs, we adjust the training steps for self-guided training and repeat those tokens at the end to ensure they're fully learned by structured matrices. As can be seen in Table 9 and Fig. 14a, Fig. 14b, Fig. 8, this reduces the perplexity

    & **Params.** & **Training** & **PPL** \\  & **(M)** & **time (h)** & **PPL** \\ 
**Transformer-xl** & 1274 & 352.2 & 12.46 \\  LowRank & 985 & 302.2 & **12.86** \\ BlockDense & 955 & **298.7** & 12.97 \\ BlockShuffle & 985 & 303.6 & 12.98 \\  LowRank & 744 & **260.2** & **13.55** \\ BlockDense & 728 & 261.2 & 13.74 \\ BlockShuffle & 744 & **284.9** & 13.81 \\   

Table 2: Training time of Transformer-xl and structured counterparts with 32% and 63% FFN parameters.

Figure 6: Latency of structured and dense FFNs across different FFN widths. Results are evaluated on 30000 tokens. The intermediate size of the FFN is set to be 4 times the FFN width.

Figure 7: **Latency over different batch size for different widths: Decoding latency results between dense FFN and structured matrices with 32% FFN parameters across different widths and batch sizes. Note that we have a sequence length of \(1\) at the decoding phase; thus, \(T\) equals batch size.**

gap for Transformer-xl from 1.0, 1.2, and 1.3 to 0.4, 0.5, and 0.6 for LowRank, BlockDense, and BlockShuffle, respectively, under the same training FLOPs and can still enjoy 32% model FLOPs, which can bring about 2.6\(\) inference speed-up. Additionally, we compare our method with another advanced baseline that trains structured matrices with more tokens, showing that the self-guided training can achieve comparable or superior results even with the same number of tokens.

## 5 Conclusion

In this paper, we conducted extensive experiments investigating the use of structured matrices to parameterize FFNs in Transformers, with models scaling up to 1.3B parameters on the RefinedWeb dataset. Our primary aim was not to determine which structured matrix performs best, as this can be task-dependent, but to explore their common properties, including scaling, efficiency, and optimization challenges. We found that all of them exhibit steeper scaling curves compared to dense models. Moreover, our proposed methods, such as self-guided training, can enhance the performance across all structured matrices (e.g., LowRank with the novel training strategy achieves a 1.35\(\) inference speed-up with only a 0.4 increase in perplexity). To conclude, we demonstrate that structured matrices can be strong candidates to replace the dense models in architecture design by scaling studies and also reveal the challenges of applying them.

**Limitations**: BlockDense and BlockShuffle are more complicated than LowRank. In this work, we only explored a limited range of hyperparameter settings of them. However, since these approaches are new, we believe that further performance improvements may be possible by better tuning their hyperparameters. We primarily focused on language modeling with limited vision experiments included in the appendix. Additionally, we did not explore the optimal scaling laws for structured matrices, which may further enhance performance. We also didn't investigate models in this paper that are comparable to today's practical LLMs, such as LLaMA-3. This is not only because of the limited computing resources but also because this study is to start investigating structured parameterizations of linear layers in modern LLM architecture training. We hope our findings and solutions about scaling, efficiency, and optimization will push their usage on the industry side and in future work.