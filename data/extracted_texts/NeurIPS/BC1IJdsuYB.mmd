# Alvin Heng1, Harold Soh1,2

## Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models1Dept. of Computer Science, National University of Singapore

2Smart Systems Institute, National University of Singapore

{alvinh, harold}@comp.nus.edu.sg

###### Abstract

The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.

Figure 1: Qualitative results of our method, Selective Amnesia (SA). SA can be applied to a variety of models, from forgetting textual prompts such as specific celebrities or nudity in text-to-image models to discrete classes in VAEs and diffusion models (DDPM).

Introduction

Deep generative models have made significant strides in recent years, with large-scale text-to-image models attracting immense interest due to their excellent generation capabilities. Unfortunately, these models can also be misused to create realistic-looking images of harmful, discriminatory and inappropriate content . For instance, one could generate Deepfakes -- convincing fake images -- and inappropriate content involving real individuals (e.g., nude celebrities) [2; 3]. A naive approach to address this issue is to omit specific concepts or individuals from the training dataset. However, filtering datasets of billions of images is a challenge in itself. Moreover, it entails retraining the entire model from scratch each time a new concept is to be forgotten, which is costly in terms of compute and time. In this work, our goal is to retrain the model to only forget specific concepts, i.e., to induce _selective amnesia_.

Several efforts in this direction have been made in the field of data forgetting [4; 5; 6; 7], as well as concept erasure in the context of text-to-image diffusion models [1; 8; 9]. However, works in the former either focus on discriminative models, or require special partitioning of data and model during training. The few works in the latter nascent field of concept erasure target text-to-image diffusion models and work by exploiting specific design characteristics of these models. Here, we aim to develop a general framework that is applicable to a variety of pretrained generative models, _without_ access to the original training data.

Our key insight is that selective forgetting can be framed from the perspective of continual learning. Ironically, the focus in continual learning has been on _preventing_ forgetting; typically, given parameters for task \(A\), we would like to train the model to perform task \(B\) without forgetting task \(A\), i.e., \(_{A}_{A,B}\). In our case, we have a model that is trained to generate \(A\)_and_\(B\), and we would like the model to only generate \(B\) while forgetting \(A\), i.e., \(_{A,B}_{B}\).

In this work, we show that well-known methods in continual learning can be unified into a single objective function that can be used to train models to forget. Unlike prior works, our method allows for _controllable forgetting_, where the forgotten concept can be remapped to a user-defined concept that is deemed more appropriate. We focus our scope on conditional variational likelihood models, which includes popular deep generative frameworks, namely Variational Autoencoders (VAEs)  and Denoising Diffusion Probabilistic Models (DDPMs) . To demonstrate its generality, we apply our method, dubbed Selective Amnesia (SA) to datasets and models of varying complexities, from simple VAEs trained on MNIST, DDPMs trained on CIFAR10 and STL10, to the open-source Stable Diffusion  text-to-image model trained on a large corpus of internet data. Our results shows that SA causes generative models to forget diverse concepts such as discrete classes to celebrities and nudity in a manner that is customizable by the user.

Our paper is structured as follows. We cover the relevant background and related works in Sec. 2. We introduce Selective Amnesia (SA) in Sec. 3, followed by in-depth experimental results in Sec. 4. Finally, we conclude in Sec. 5 by briefly discussing the limitations and broader impacts of our work.

## 2 Background and Related Work

### Variational Generative Models

Conditional Variational Autoencoders.Conditional Variational Autoencoders  are generative models of the form \(p(,|,)=p(|, ,)p(|,)\), where \(\) is the data (e.g., an image), \(\) is the concept/class, and \(p(|,)\) is a prior over the latent variables \(\). Due to the intractability of the posterior \(p(|,,)\), VAEs adopt an approximate posterior \(q(|,,)\) and maximize the evidence lower bound (ELBO),

\[ p(|,) p(|,,)+D_{KL}(q(|,, )||p(|,))=(|,).\]

Conditional Diffusion Models.Diffusion models  are a class of generative models that sample from a distribution through an iterative Markov denoising process. A sample \(_{T}\) is typically sampled from a Gaussian distribution and gradually denoised for \(T\) time steps, finally recovering a clean sample \(_{0}\). In practice, the model is trained to predict the noise \((_{t},t,|)\) that must be removed from the sample \(_{t}\) with the following reweighted variational bound: \((|,)=_{t=1}^{T}||(_{t},t,|)-||^{2}\), where \(_{t}=}_{t}_{0}+_{t}} \) for \((,)\), \(_{t}\) are constants related to the noise schedule in the forward noising process. Sampling from a conditional diffusion model can be carried out using classifier-free guidance .

### Continual Learning

The field of continual learning is primarily concerned with the sequential learning of tasks in deep neural networks, while avoiding catastrophic forgetting. A variety of methods have been proposed to tackle this problem, including regularization approaches [14; 15], architectural modifications [16; 17], and data replay . We discuss two popular approaches that will be used in our work: Elastic Weight Consolidation and Generative Replay.

Elastic Weight Consolidation.Elastic Weight Consolidation (EWC)  adopts a Bayesian approach to model the posterior of the weights for accomplishing two tasks, \(D_{A}\) and \(D_{B}\), given a model \(^{*}\) that has learnt \(D_{A}\). The Laplace approximation is applied to the posterior over the initial task \(D_{A}\), giving rise to a quadratic penalty that slows down learning of weights that are most relevant to the initial task. Concretely, the posterior is given by \( p(|D_{A},D_{B})= p(D_{B}|)-_{i}}{2 }(_{i}-_{i}^{*})^{2}\), where \(F\) is the Fisher information matrix (FIM) and \(\) is a weighting parameter. In practice, a diagonal approximation \(F_{i}=_{p(D|^{*})}[(} p (D|))^{2}]\) is adopted for computational efficiency. \(F_{i}\) can be viewed as a sensitivity measure of the weight \(_{i}\) on the model's output. For variational models, we modify the \(F_{i}\) to measure the sensitivity of \(_{i}\) on the ELBO: \(F_{i}=_{p(|^{*},)p()}[(}(|,))^{2}]\).

Generative Replay.Generative Replay (GR)  was proposed as a method where a generative model can be leveraged to generate data from previous tasks, and used to augment data from the current task in the training of a discriminative model. More generally, it motivates one to leverage generative models for continual learning, whereby without needing to store any of the previous datasets, a model can be trained on all tasks simultaneously, which prevents catastrophic forgetting.

Our work leverages EWC and GR to train a model to sequentially forget certain classes and concepts. There have been several works utilizing these techniques for generative models, such as GANs [19; 20] and VAEs . However, these works tackle the traditional problem of continual learning, which seeks to prevent forgetting.

### Data Forgetting

The increased focus on privacy in machine learning models in recent years, coupled with data privacy regulations such as the EU's General Data Protection Regulation, has led to significant advancements in the field of data forgetting. Data forgetting was first proposed in  as a statistical query learning problem. Later work proposed a dataset sharding approach to allow for efficient data deletion by deleting only specific shards . Alternative methods define unlearning through information accessible directly from model weights , while  proposed a variational unlearning method which relies on a posterior belief over the model weights. Wu et al.  proposes a method to remove the influence of certain datapoints from a trained model by caching the model gradients during training. Our method only requires access to a trained model and does not require control over the initial training process or the original dataset, making it distinct from [4; 5; 22]. In addition, earlier methods are designed for discriminative tasks such as classification  and regression , while we focus on deep generative models.

### Editing and Unlearning in Generative Models

Several works have investigated the post-hoc editing and retraining of generative models. Data redaction and unlearning have been proposed for GANs  and Normalizing Flows . However, both methods exploit specific properties of the model (discriminators and exact likelihoods) which are absent from variational models, hence are not comparable to our work. Moon et al.  implicitly assumes that a generator's latent space has disentangled features over concepts, which does not apply to conditional models (a given latent \(z\) can be used to generate all classes just by varying the conditioning signal \(c\)). Bau et al.  directly modifies a single layer's weights in a generator to alter its semantic rules, such as removal of watermarks. The work focuses on GANs and preliminary experiments on more drastic changes that forgetting necessities led to severe visual artifacts.

### Concept Erasure in Text-to-Image Models

Large-scale text-to-image models [12; 27; 28] can be misused to generate biased, unsafe, and inappropriate content . To tackle this problem, Safe Latent Diffusion (SLD)  proposes an inference scheme to guide the latent codes away from specific concepts, while Erasing Stable Diffusion (ESD)  proposes a training scheme to erase concepts from a model. Both methods leverage energy-based composition that is specific to the classifier-free guidance mechanism  of diffusion models. We take a different approach; we adopt a general continual learning framework for concept erasure that works across different model types and conditioning schemes. Our method allows for controlled erasure, where the erased concept can be mapped to a user-defined concept.

## 3 Proposed Method: Selective Amnesia

Problem Statement.We consider a dataset \(D\) that can be partitioned as \(D=D_{f} D_{r}=\{(_{f}^{(n)},_{f}^{(n)})\}_{n=1}^{N_{f}} \{(_{r}^{(m)},_{r}^{(m)})\}_{m=1}^{N_{r}}\), where \(D_{f}\) and \(D_{r}\) correspond to the data to forget and remember respectively. The underlying distribution of \(D\) is a joint distribution given by \(p(,)=p(|)p()\). We further define the distribution over concepts/class labels as \(p()=_{i f,r}_{i}p_{i}()\) where \(_{i f,r}_{i}=1\). The two concept/class distributions are disjoint such that \(p_{f}(_{r})=0\) where \(_{r} p_{r}()\) and vice-versa. For ease of notation, we subscript distributions and class labels interchangeably, e.g., \(p_{f}()\) and \(p(_{f})\).

We assume access to a trained conditional generative model parameterized by \(^{*}=_{}_{p(,)} p( |,)\), which is the maximum likelihood estimate (MLE) of the dataset \(D\). We would like to train this model such that it forgets how to generate \(D_{f}|_{f}\), while remembering \(D_{r}|_{r}\). A key criteria is that the training process must not require access to \(D\). This is to accommodate the general scenario where one only has access to the model and not its training set.

A Bayesian Continual Learning Approach to Forgetting.We start from a Bayesian perspective of continual learning inspired by the derivation of Elastic Weight Consolidation (EWC) :

\[ p(|D_{f},D_{r}) = p(D_{f}|,D_{r})+ p(|D_{r})- p(D_{f}|D_{r})\] \[= p(D_{f}|)+ p(|D_{r})+C.\]

For forgetting, we are interested in the posterior conditioned only on \(D_{r}\),

\[ p(|D_{r}) =- p(D_{f}|)+ p(|D_{f},D_{r})+C\] \[=- p(_{f}|,_{f})-_{i} }{2}(_{i}-_{i}^{*})^{2}+C\] (1)

where we use \( p(D_{f}|)= p(_{f},_{f}|)= p( _{f}|,_{f})+ p(_{f})\) so that the conditional likelihood is explicit, and substitute \( p(|D_{f},D_{r})\) with the Laplace approximation of EWC. Our goal is to maximize \( p(|D_{r})\) to obtain a maximum a posteriori (MAP) estimate. Intuitively, maximizing Eq. (1) _lowers_ the likelihood \( p(_{f}|,_{f})\), while keeping \(\) close to \(^{*}\).

Unfortunately, direct optimization is hampered by two key issues. First, the optimization objective of Eq. 1 does not involve using samples from \(D_{r}\). In preliminary experiments, we found that without replaying data from \(D_{r}\), the model's ability to generate the data to be remembered diminishes over time. Second, we focus on variational models where the log-likelihood is intractable. We have the ELBO, but minimizing a lower bound does not necessarily decrease the log-likelihood. In the following, we address both these problems via generative replay and a surrogate objective.

### Generative Replay Over \(D_{r}\)

Our approach is to unify the two paradigms of continual learning, EWC and GR, such that they can be optimized under a single objective. We introduce an extra likelihood term over \(D_{r}\) that corresponds to a generative replay term, while keeping the optimization over the posterior of \(D_{r}\) unchanged:

\[ p(|D_{r})=[- p(_{f}|,_{f})-_{i}}{2}(_{i}-_{i}^{*})^{2}+ p( _{r}|,_{r})+ p()]+C.\] (2)A complete derivation is given in Appendix A.1. The term \( p()\) corresponds to a prior over the parameters \(\). Practically, we find that simply setting it to the uniform prior achieves good results, thus rendering it constant with regards to optimization. With the expectations written down explicitly, our objective becomes

\[=-_{p(|)_{p_{f}}()}[ p (|,)]-_{i}}{2}(_{i }-_{i}^{*})^{2}+_{p(|)_{p_{r}}( )}[ p(|,)].\] (3)

As we focus on conditional generative models in this work, the expectations over \(p(|)_{p_{f}}()\) and \(p(|)_{p_{r}}()\) can be approximated by using conditional samples generated by the model prior to training. Similarly, the FIM is calculated using samples from the model. Thus, Eq. 3 can be optimized without the original training dataset \(D\). Empirically, we observe that the addition of the GR term improves performance when generating \(D_{r}\) after training to forget \(D_{f}\) (see ablations in Sec. 4.1).

### Surrogate Objective

Similar to Eq. 1, Eq. 3 suggests that we need to _minimize_ the log-likelihood of the data to forget \(_{, p(|)_{p_{f}}( )}[ p(|,)]\). With variational models, we only have access to the lower bound of the log-likelihood, but naively optimizing Eq. 3 by replacing the likelihood terms with the standard ELBOs leads to poor results. Fig. 2 illustrates samples from a VAE trained to forget the MNIST digit '0'; not only has the VAE failed to forget, but the sample quality of the other classes has also greatly diminished (despite adding the GR term).

We propose an alternative objective that is guaranteed to lower the log-likelihood of \(D_{f}\), as compared to the original model parameterized by \(^{*}\). Rather than attempting to directly minimize the log-likelihood or the ELBO, we _maximize_ the log-likelihood of a surrogate distribution of the class to forget, \(q(|_{f}) p(|_{f})\). We formalize this idea in the following theorem.

**Theorem 1**.: _Consider a surrogate distribution \(q(|)\) such that \(q(|_{f}) p(|_{f})\). Assume we have access to the MLE optimum for the full dataset \(^{*}=_{}_{p(,)}[ p (|,)]\) such that \(_{p()}[D_{KL}(p(|)||p( |^{*},))]=0\). Define the MLE optimum over the surrogate dataset as \(^{q}=_{}_{q(|)_{p_{f}}( )}[ p(|,)]\). Then the following inequality involving the expectations of the optimal models over the data to forget holds:_

\[_{p(|)_{p_{f}}()}[ p(|^{q},)]_{p(|)_{p_{f}}( )}[ p(|^{*},)].\]

Theorem 1 tells us that optimizing the surrogate objective \(_{}_{q(|)_{p_{f}}()} [ p(|,)]\) is guaranteed to reduce \(_{p(|)_{p_{f}}()}[ p(|,)]\), the problematic first term of Eq. 3, from its original starting point \(^{*}\).

**Corollary 1**.: _Assume that the MLE optimum over the surrogate, \(^{q}=_{}_{q(|)_{p_{f}}( )}[ p(|,)]\) is such that \(_{p_{f}()}[D_{KL}(q(|)||p( |^{q},)]=0\). Then the gap presented in Theorem 1,_

\[_{p(|)_{p_{f}}()}[ p(|^{q},)- p(|^{*},)]=-_{p_{f} ()}[D_{KL}(p(|)||q(|)) ].\]

Corollary 1 tells us that the greater the difference between \(q(|_{f})\) and \(p(|_{f})\), the lower the log-likelihood over \(D_{f}\) we can achieve. For example, we could choose the uniform distribution as it is

Figure 2: Illustration of training VAE to forget the MNIST digit 0. The ‘original’ column shows the baseline samples generated by the VAE. In the ‘naive’ column, we train the VAE to optimize Eq. 3 with \(D_{f}\) being the ‘0’ class, while in the ‘ours’ column we train using the modified objective Eq. 4

easy to sample from and is intuitively far from the distribution of natural images, which are highly structured and of low entropy. That said, users are free to choose \(q(|_{f})\), e.g., to induce realistic but acceptable images, and we experiment with different choices in the Stable Diffusion experiments (Sec 4.2).

Putting the above elements together, the Selective Amnesia (SA) objective is given by

\[ =_{q(|)p_{f}()}[  p(|,)]-_{i}}{2}( _{i}-_{i}^{*})^{2}+_{p(|)p_{r}( )}[ p(|,)]\] \[_{q(|)p_{f}()}[ (|,)]-_{i}}{2 }(_{i}-_{i}^{*})^{2}+_{p(|)p_{r}( )}[(|,)]\] (4)

where we replace likelihood terms with their respective evidence lower bounds. For variational models, maximizing the ELBO increases the likelihood, and we find the revised objective to perform much better empirically -- Fig. 2 (right) shows results of the revised objective when applied to the MNIST example, where we set \(q(|_{f})\) to a uniform distribution over the pixel values, \(U\). The model now forgets how to generate '0', while retaining its ability to generate other digits.

## 4 Experiments

In this section, we demonstrate that SA is able to forget diverse concepts, ranging from discrete classes to language prompts, in models with varying complexities. For discrete classes, we evaluate SA on MNIST, CIFAR10 and STL10. The former is modeled by a conditional VAE with a simple MLP architecture, which is conditioned by concatenating a one-hot encoding vector to its inputs. The latter two datasets are modeled by a conditional DDPM with the UNet architecture, which is conditioned using FiLM transformations  within each residual block. Class-conditional samples are generated with classifier-free guidance . We also experiment with the open-source text-to-image model Stable Diffusion v1.4 , where the model is conditioned on CLIP  text embeddings using the cross-attention mechanism. Further experimental details can be found in Appendix B.

In addition to qualitative comparisons, we performed quantitative analyses using three types of metrics for the discrete classes:

Image Quality Metrics.First, we evaluate the image quality of the classes to remember using standard metrics such as the Frechet Inception Distance (FID), Precision, and Recall [31; 32]. Ideally, we would like SA to have minimal effects on the image quality of the classes to remember.

Probability of Class to Forget.Second, using an external classifier, we evaluate generated samples from the class to forget to ensure that the class has been successfully erased. The probability of a class to forget is defined as \(_{p(|,_{f})}[P_{}(=_{f}|)]\), where the expectation is over samples generated from our trained model, and \(P_{}(|)\) is a pretrained classifier. If we choose \(q(|_{f})\) to be an uninformative distribution, such as the uniform distribution, this should approach \(1/N_{classes}\)(=1/10 for the datasets studied here) as the classifier becomes completely uncertain which class it belongs to.

Classifier Entropy.This is the average entropy of the classifier's output distribution given \(_{f}\), defined as \(H(P_{}(|_{f}))=-_{p(|, _{f})}[_{i}P_{}(=_{i}|) P_ {}(=_{i}|)]\). When we choose \(q(|_{f})\) to be the uniform distribution, all class information in the generated \(_{f}\) should be erased. The entropy should therefore approach the theoretical maximum given by \(-_{i=1}^{10}=2.30\), as the classifier becomes maximally uncertain and assigns a probability of \(1/10\) to every outcome.

### MNIST, CIFAR10 and STL10 Results

In this experiment on MNIST, CIFAR10 and STL10, we attempt to forget the digit '0' in MNIST and the 'airplane' class in CIFAR10 and STL10. We choose \(q(|_{f})\) to be a uniform distribution over the pixel values. In brief, the results suggest that SA has successfully induced forgetting for the relevant class, with minor degradation in image diversity of the classes to remember. Qualitative samples are shown in Fig. 1, where it is clear that the classes to forget have been erased to noise, while the quality of the classes to remember remain visually indistinguishable from the original model. Additional samples are shown in Appendix D.1. In the following, we perform a quantitative comparison using our metrics shown in Table 1.

First, we evaluate the information content left in the generated \(_{f}\) by examining the rows in Table 1 that are highlighted in blue. On MNIST, there is a 96.7% probability of classifying the samples of the '0' class from the original model correctly, and correspondingly a low entropy in its distribution. However, after training with \(=100\), the probability drops to 5.8%, while the entropy closely approaches the theoretical maximum of 2.30, indicating that any information in the generated \(_{f}\) about the digit '0' has been erased. We see a similar result for the CIFAR10 and STL10 diffusion models, where the entropy increases significantly after training, although it does not approach the maximum value as closely as the VAE.

Next, we evaluate the image quality of the classes to remember on CIFAR10 and STL10. We compare with two baselines: the original models and a version trained only on the nine classes to remember. Surprisingly on CIFAR10, training with \(=10\) actually improves FID slightly, which a priori is unusual as the baselines should serve as natural upper-bounds on image quality. However, further examination shows that precision (fidelity) has improved at the expense of recall (diversity), which suggests a slight overfitting effect. On STL10, there is similarly a slight improvement in precision, but with a drop in recall, which overall resulted in a higher FID score. This can be attributed to the fact that we chose the number of GR samples to be relatively small at 5000 samples, as sampling for diffusion models can be expensive. We hypothesize that this can be alleviated by increasing the GR sample size, but we leave this to future investigation.

Ablations.We conduct ablations on the \(\) parameter and on the generative replay term in our objective function, using DDPM trained on CIFAR10. Hyperparameters other than the ones being ablated are kept fixed throughout runs. The results are highlighted in orange in Table 1. Starting with ablations on \(\), we see that when \(=1\), image fidelity as measured by FID and precision is significantly poorer than at larger values of \(\), showing that the FIM term is crucial in our training scheme. As \(\) is increased, there is a drastic improvement in fidelity, which comes at a slight cost to diversity as measured by recall, although the changes are relatively minor across the tested range of \(\). This suggests that the FIM primarily preserves fidelity in the generated images. When comparing classifier entropy, we see that increments beyond \(=10\) decreases entropy further from the upper-bound, which indicate some information leakage to the forgotten samples being generated. Moving to generative replay, we find that all metrics suffer significantly when the term is omitted. In summary, our ablation studies show that generative replay is crucial in our method, and intermediate values \(\) is sufficient for good performance.

### Case Study: Stable Diffusion

Forget Famous Persons.With the potential of large-scale generative models to be misused for impersonations and deepfakes, we apply SA to the forgetting of famous persons with SD v1.4. We leverage the fact that with language conditioning, we can choose \(q(|_{f})\) to be represented by images that are appropriate substitutes of the concept to forget. For instance, we attempt to forget the celebrities Brad Pitt and Angelina Jolie, thus we set \(_{f}\) = {"Brad Pitt"} and \(_{f}\)={"Angelina Jolie"}

    & & FID (\(\)) & Precision (\(\)) & Recall (\(\)) & \(H(P_{}(|_{f})\) & \(P_{}(=_{f}|_{f})\) \\   & Original & - & - & - & 0.103 & 0.967 \\  & \(=100\) & - & - & - & 2.19 & 0.0580 \\   & Original & 9.67 & 0.390 & 0.788 & 0.0293 & 0.979 \\  & 9 Classes & 9.46 & 0.399 & 0.783 & - & - \\   & \(=10\) & 9.08 & 0.412 & 0.767 & 1.47 & 0.156 \\   & \(=1\) & 19.3 & 0.286 & 0.770 & 0.977 & 0.700 \\  & \(=50\) & 8.41 & 0.428 & 0.760 & 1.17 & 0.142 \\  & \(=100\) & 8.33 & 0.429 & 0.758 & 1.07 & 0.235 \\  & No GR (\(=10\)) & 126 & 0.0296 & 0.268 & 0.893 & 0.737 \\   & Original & 14.5 & 0.356 & 0.796 & 0.0418 & 0.987 \\  & 9 Classes & 14.5 & 0.360 & 0.803 & - & - \\   & \(=10\) & 18.0 & 0.378 & 0.713 & 1.80 & 0.0189 \\  

Table 1: Quantitative results for forgetting on the MNIST, CIFAR10 and STL10 datasets. \(H(P_{}(|_{f}))\) and \(P_{}(=_{f}|_{f})\) indicate the entropy of the classifier’s distribution and the probability of the forgotten class respectively. The rows highlighted in blue correspond to the hyper-parameters chosen for the images visualized in Fig. 1. The rows highlighted in orange are ablation results for CIFAR10.

and represent \(q(|_{f})\) with images generated from SD v1.4 with the prompts "a middle aged man" and "a middle aged woman" respectively. In other words, we train the model to generate pictures of ordinary, unidentifiable persons when it is conditioned on text containing "Brad Pitt" or "Angelina Jolie". In this way, our model still generates semantically-relevant pictures of humans, as opposed to uniform noise if we had chosen the same \(q(|_{f})\) as the previous section.

To demonstrate the control and versatility of SA, we conduct a second set of experiments where we map the celebrities to clowns, by setting \(q(|_{f})\) to images of "male clown" or "female clown" generated by SD v1.41. For SD experiments, we only train the diffusion model operating in latent space, while freezing the encoder and decoder. Our qualitative results are shown in Fig. 3, where we see that the results generalize well to a variety of prompts, generating realistic images of regular people and clowns in various settings. Additional samples are shown in Appendix D.2.

We compare our results against the following baselines: 1) original SD v1.4, 2) SLD Medium  and 3) ESD-x , training only the cross-attention layers. We generate 20 images each of 50 random prompts containing "Brad Pitt" and "Angelina Jolie" and evaluate using the open-source GIPHY Celebrity Detector (GCD) . We calculate two metrics, the proportion of images generated with no faces detected and the average probability of the celebrity given that a face is detected, which we abbreviate as GCD Score (GCDS). Table 2 shows that SA generates the most images with faces, with significantly lower GCDS compared to SD v1.4. SLD and ESD have better GCDS, but they have a greater proportion of images without faces (particularly ESD). Looking at the qualitative samples in Figs. 4 and 5, ESD sometimes generates faceless and semantically unrelated images due to its uncontrollable training process. Also note that the faces generated by SLD tend to be distorted and low-quality, which we hypothesize is the reason behind its low GCDS. Visual inspection of the top-5 images in terms of GCDS in Fig. 4 shows that, despite the high scores, the images generated by SA

    &  &  \\   & Proportion of images &  & Proportion of images &  \\  & without faces (\(\)) & & without faces (\(\)) & \\  SD v1.4 (original) & 0.104 & 0.606 (0.424) & 0.117 & 0.738 (0.454) \\ SLD Medium & 0.141 & 0.00474 (0.0354) & 0.119 & 0.0329 (0.129) \\ ESD-x & 0.347 & 0.0201 (0.109) & 0.326 & 0.0335 (0.153) \\ SA (Ours) & 0.058 & 0.0752 (0.193) & 0.0440 & 0.0774 (0.213) \\   

Table 2: Quantitative results from the GIPHY Celebrity Detector. For SA, we use the variant with \(q(|_{f})\) set to “middle aged man” or “middle aged woman” for forgetting Brad Pitt and Angelina Jolie respectively. The GCD Score is the average probability of a face being classified as Brad Pitt or Angelina Jolie in the test set. Numbers in brackets are standard deviations. Note that the standard deviations are typically much larger than the mean, which indicates a highly skewed distribution, i.e., a majority of faces have very low probabilities, but a few have very large probabilities.

Figure 3: Qualitative results of SA applied to forgetting famous persons. Within each column, the leftmost image represents SD v1.4 samples, the middle image represents SA with \(q(|_{f})\) set to ”middle aged man/woman” and the rightmost image is SA with \(q(|_{f})\) set to ”male/female clown”. [...] is substituted with either ”Brad Pitt” or ”Angelina Jolie”.

would not be mistaken for Brad Pitt (with the possible exception of the middle image), and not more so than the other two methods. Similar observations can be made for the Angelina Jolie samples in Fig. 5. We also investigate the effects on celebrities other than the one being forgotten in Sec. E of the appendix. We observe that SA exhibits what we dub "concept leakage", where slight changes are observed in other celebrities with similar attributes. We view this as a double-edged sword, as it also means that SA can generalize to related concepts. If desired, this effect can be mitigated by tuning only the cross-attention layers of SD . We discuss this in greater detail in Sec. E.

Forget Nudity.We also attempt to tackle the problem of inappropriate concept generation by training SD v1.4 to forget the concept of nudity. Unlike the previous celebrity setting, nudity is a "global" concept that can be indirectly referenced through numerous text prompts, such as styles of artists that produce nude imagery. As such, we train only the unconditional (non-cross-attention)

Figure 4: Comparisons between SA with ESD and SLD in forgetting Brad Pitt. We use SA with \(q(|_{f})\) set to “middle aged man”. Images on the left are sample images with the prompts specified per column. Images on the right are the top-5 GCDS images from the generated test set, with their respective GCDS values displayed. Intuitively, these are the images with the 5 highest probabilities that the GCD network classifies as Brad Pitt.

Figure 5: Comparisons between our method with ESD and SLD in forgetting Angelina Jolie. We use the variant of SA with \(q(|_{f})\) set to “middle aged woman”. Images on the left are sample images with the prompts specified per column. Images on the right are the top-5 GCDS images from the generated test set, with their respective GCDS values displayed.

layers in SD v1.4, as proposed in . In this scenario, we represent \(q(|_{f})\) with images generated from SD v1.4 with the prompt "a person wearing clothes", which is a semantically-relevant antonym of the concept of nudity. We let our prompts to forget be \(_{f}\) =["nudity", "naked", "erotic", "sexual"] and sample them uniformly during training.

We evaluate on the I2P dataset , which is a collection of 4703 inappropriate prompts. Our results are in Table 3 of the appendix, where we compare against SD v1.4, SLD, ESD-u (train unconditional layers only) as well as SD v2.1, which is trained on a dataset filtered for nudity. The quantity of nudity content was detected using the NudeNet classifier (with a default detection threshold of 0.6, which results in some false positives). Our model generates significantly reduced nudity content compared to SD v1.4 and SD v2.1. SLD and ESD achieve better scores, potentially because they are model-specific and leverage inductive biases of Stable Diffusion, namely score-function composition. Qualitative samples between the three approaches are shown in Fig. 62. Similar to the celebrity experiments, we find that ESD tends to generate arbitrary images that are not semantically-relevant to the test prompt, due to its uncontrollable training process. On the other hand, SA generates semantically related images, but did not forget how to generate nude images to the same extent. We found that the I2P prompts associated with these images generally did not specify nudity terms explicitly, but involved specific artistic styles or figures that are associated with nudity. Additional evaluations shows SA to perform better on prompts with explicit nudity terms (Table 4 in appendix). Combining the positive traits of SA, such as controlled forgetting, with the efficacy of ESD's global erasure capabilities would be interesting future work.

## 5 Conclusion, Limitations, and Future Work

This paper contributes Selective Amnesia (SA), a continual learning approach to controlled forgetting of concepts in conditional variational models. Unlike prior methods, SA is a general formulation and can be applied to a variety of conditional generative models. We presented a unified training loss that combines the EWC and GR methods of continual learning, which when coupled to a surrogate distribution, enables targeted forgetting of a specific concept. Our approach allows the user to specify how the concept to forget can be remapped, which in the Stable Diffusion experiments, resulted in semantically relevant images but with the target concept erased.

Limitations and Broader Impacts.We believe SA is a step towards greater control over deep generative models, which when left unconstrained, may be misused to generate harmful and misleading content (e.g., deepfakes). There are several avenues for future work moving forward. First, the FIM calculation can be expensive, in particular for diffusion models as the ELBO requires a sum over \(T\) timesteps per sample. Future work can investigate more efficient and accurate ways of computing the FIM. Second, SA appears to be more proficient at removing "local" specific concepts, rather than "global" concepts (e.g., nudity); more work is needed on general methods that work well for both types of information. Third, SA requires manual selection of an appropriate surrogate distribution; a method to automate this process would be an interesting future direction. Finally, human assessment can also be explored to provide a more holistic evaluation of SA's forgetting capabilities. From a broader perspective, SA could potentially be used to alter concepts inappropriately or maliciously, such as erasing historical events. We believe that care should be taken by the community in ensuring that tools such as SA are used to improve generative models, and not propagate further harms.

Figure 6: Sample images with the prompt "a photo of a naked person" from the three approaches.