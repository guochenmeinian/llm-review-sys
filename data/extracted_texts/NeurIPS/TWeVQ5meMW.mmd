# Subject-driven Text-to-Image Generation via Preference-based Reinforcement Learning

Yanting Miao

Department of Computer Science

University of Waterloo, Vector Institute

y43miao@uwaterloo.ca

&William Loh

Department of Computer Science

University of Waterloo, Vector Institute

wmloh@uwaterloo.ca

&Suraj Kothawade

Google

skothawade@google.com

&Pascal Poupart

Department of Computer Science

University of Waterloo, Vector Institute

ppoupart@uwaterloo.ca

&Abdullah Rashwan

Google

arashwan@google.com

&Yeqing Li

Google

yeqing@google.com

This project was completed during work at Google

###### Abstract

Text-to-image generative models have recently attracted considerable interest, enabling the synthesis of high-quality images from textual prompts. However, these models often lack the capability to generate specific subjects from given reference images or to synthesize novel renditions under varying conditions. Methods like DreamBooth and Subject-driven Text-to-Image (SuTI) have made significant progress in this area. Yet, both approaches primarily focus on enhancing similarity to reference images and require expensive setups, often overlooking the need for efficient training and avoiding overfitting to the reference images. In this work, we present the \(\)-Harmonic reward function, which provides a reliable reward signal and enables early stopping for faster training and effective regularization. By combining the Bradley-Terry preference model, the \(\)-Harmonic reward function also provides preference labels for subject-driven generation tasks. We propose Reward Preference Optimization (RPO), which offers a simpler setup (requiring only \(3\%\) of the negative samples used by DreamBooth) and fewer gradient steps for fine-tuning. Unlike most existing methods, our approach does not require training a text encoder or optimizing text embeddings and achieves text-image alignment by fine-tuning only the U-Net component. Empirically, \(\)-Harmonic proves to be a reliable approach for model selection in subject-driven generation tasks. Based on preference labels and early stopping validation from the \(\)-Harmonic reward function, our algorithm achieves a state-of-the-art CLIP-I score of 0.833 and a CLIP-T score of 0.314 on DreamBench. Our Pytorch implementation is available at https://github.com/andrew-miao/RP0.

## 1 Introduction

In the evolving field of generative AI, text-to-image diffusion models  have demonstrated remarkable abilities in rendering scenes that are both imaginative and contextuallyappropriate. However, these models often struggle with tasks that require the portrayal of specific subjects within text prompts. For instance, if provided with a photo of your cat, current diffusion models are unable to generate an image of your cat situated in the castle of your childhood dreams. This challenge necessitates a deep understanding of subject identity. Consequently, _subject-driven_ text-to-image generation has attracted considerable interest within the community. Chen et al.  have noted that this task requires complex transformations of reference images. Additionally, Ruiz et al.  have highlighted that detailed and descriptive prompts about specific objects can lead to varied appearances in subjects. Thus, traditional image editing approaches and existing text-to-image models are ill-suited for subject-driven tasks.

Current subject-driven text-to-image generation methods are less expressive and expensive. Textual Inversion  performs poorly due to the limited expressiveness of frozen diffusion models. Imagic  is both time-consuming and resource-intensive during the fine-tuning phase. It requires text-embedding optimization for each prompt, fine-tuning of diffusion models, and interpolation between optimized and target prompts. The training process is complex and slow. These text-based methods require 30 to 70 minutes to fine-tune their models, which is not scalable for real applications. SuTI  proposes an in-context learning method for subject-driven tasks. However, SuTI demands half a million expert models for each different subject, making it prohibitively expensive. Although SuTI can perform in-context learning during inference, the setup of expert models remains costly. DreamBooth  provides a simpler method for handling subject-driven tasks. Nevertheless, DreamBooth requires approximately 1000 negative samples and 1000 gradient steps, and also needs fine-tuning of the text encoder to achieve state-of-the-art performance. Therefore, it is worthwhile to explore more efficient training methods: the setup should be as simple as possible. First, training should not include multiple optimization phases. Second, text-to-image alignment should fine-tune as few components as possible such as the UNet, but not the text encoder for each prompt. Third, faster evaluation and regularization should be enabled by early stopping based on model selection.

In this paper, we propose a \(\)-Harmonic reward function that enables early stopping and accelerates training. In addition, we incorporate the Bradley-Terry preference model to generate preference labels. We utilize preference-based reinforcement learning algorithms to finetune pre-trained diffusion models and to achieve text-to-image alignment without optimizing any text encoder or text embedding. The whole finetuning process including setup, training, validation, and model saving only takes 5 to 20 minutes on Cloud TPU V4. Our method, Reward Preference Optimization (RPO), only requires a

Figure 1: We illustrate the \(\)-Harmonic reward function applied to the subject-driven generation task. Leveraging preference labels produced by the \(\)-Harmonic reward function, alongside a few reference images, our preference-based algorithm efficiently generates unseen scenes that are both faithful to the reference images and the textual prompts.

few input reference images and the finetuned diffusion model can generate images that preserve the identity of a specific subject while aligning well with textual prompts (Figure 1).

To show the effectiveness of our \(\)-Harmonic reward function, we evaluate RPO on diverse subjects and text prompts on DreamBench  and we report the DINO and CLIP-I/CLIP-T of RPO's generated images on this benchmark and compare them with existing methods. Surprisingly, our method requires a simple setup (\(3\%\) of DreamBooth configuration) and with fewer gradient steps, but the experimental results outperform or match SOTA.

In summary, our contributions are as follows:

* We introduce the \(\)-Harmonic reward function, which permits early-stopping to alleviate overfitting in subject-driven generation tasks and to accelerate the finetuning process.
* By combining the \(\)-Harmonic reward function and a preference model, we present RPO, which only requires a cheap setup, but still can provide high quality results.
* We evaluate RPO and show the effectiveness of the \(\)-Harmonic function with diverse subjects and various prompts on DreamBench. We achieve results comparable to SOTA.

## 2 Related Works

Ruiz et al.  formulated a class of problems called _subject-driven generation_, which refers to preserving the appearance of a subject contextualized in different settings. DreamBooth  solves the issue of preserving the subject by binding it in textual space with a unique identifier for the subject in the reference images, and simultaneously generating diverse backgrounds by leveraging prior class-specific information previously learned. A related work that could possibly perform the same task is textual inversion . However, its original objective is to produce a modification of the subject or property marked by a unique token in the text. While it can be used to preserve the subject and change the background or setting, the performance is underwhelming compared to DreamBooth in various metrics .

The prevalent issue in DreamBooth and textual inversion is the long training time [23; 12] since gradient-based optimization has to be performed on their respective models for each subject. Subject-driven text-to-image generator (SuTI) by  aims to alleviate this issue by employing apprenticeship learning. By scraping millions of images online, many expert models are trained for different clusters of images centered around different subjects, which allows an apprentice to learn quickly from the experts . However, this is an incredibly resource intensive task with massive computational overhead during training.

In the field of natural language processing, direct preference optimization has found great success in large language models (LLM) . By bypassing reinforcement learning from human feedback and directly maximizing likelihoods using preference data, LLMs benefit from more stable training and reduced dependency on an external reward model. Subsequently, this inspired Diffusion-DPO by , which applies a similar technique to diffusion models. However, this relies on a preference labelled dataset, which can be expensive to collect or not publicly available for legal reasons.

Fortunately, there are reward models that can serve as functional substitutes such as CLIP  and ALIGN . ALIGN has a dual encoder architecture that was trained on a large dataset. The encoders can produce text and image embeddings, which allows us to obtain pairwise similarity scores by computing cosine similarity. There are also diffusion modelling techniques that can leverage reward models. An example is denoising diffusion policy optimization (DDOP) by Black et al.  that uses a policy gradient reinforcement learning method to encourage generations that lead to higher rewards.

## 3 Preliminary

In this section, we introduce the notation and some key concepts about text-to-image diffusion models and reinforcement learning.

Text-to-Image Diffusion Models.Diffusion models [13; 26; 28; 29; 27] are a family of latent variable models of the form \(p_{}(_{0})=_{}p_{}( _{0:T})d_{1:T}\), where the \(_{1},,_{T}\) are noised latent variables of the same dimensionality as the input data \(_{0} q(_{0})\). The diffusion or forward process is often a Markov chain that gradually adds Gaussian noise to the input data and each intermediate sample \(_{t}\) can be written as

\[_{t}=}_{0}+} _{t},t\{1,,T\},\] (1)

where \(_{t}\) refers to the variance schedule and \(_{t}(,)\). Given a conditioning tensor \(\) (often a text embedding), the core premise of text-to-image diffusion models is to use a neural network \(_{}(_{t},,t)\) that iteratively refines the current noised sample \(_{t}\) to obtain the previous step sample \(_{t-1}\), This network can be trained by optimizing a simple denoising objective function, which is the time coefficient weighted mean squared error, the derivation is shown in Appendix A.1:

\[_{_{0},,t,_{t}}[(t) \|_{}(_{t},,t)-_{t} \|_{2}^{2}],\] (2)

where \(t\) is uniformly sampled from \(\{1,,T\}\) and \((t)\) is a time dependent weight that can be simplified to 1 according to [13; 27; 22].

Reinforcement Learning and Diffusion DPOReinforcement Learning for diffusion models [3; 11; 30] aims to solve the following optimization problem:

\[_{_{0,T} p_{}(_{0,T}|)} [_{t=1}^{T}R(_{t},_{t-1},)- _{}(p_{}(_{t-1}_{t}, )\|p_{}(_{t-1}_{t},)) ],\] (3)

where \(\) is a hyperparameter controlling the KL-divergence between the finetuned model \(p_{}\) and the pre-trained base model \(p_{}\). In Equation (14) from Diffusion-DPO , the optimal \(p_{}\) can be approximated by minimizing the negative log-likelihood:

\[_{_{0}^{+},_{0}^{-},t,_{ t}^{+},_{t}^{-}}-\|_{ }(_{t}^{+},,t)-_{t}^{+}\|_{2}^{2} -\|_{}(_{t}^{+},,t)- ^{+}\|_{2}^{2}\] \[-(\|_{}(_{t}^{-},,t)- _{t}^{-}\|_{2}^{2}-\|_{}(_{t}^{ -},,t)-^{-}\|_{2}^{2}),\] (4)

where \(^{+}\) and \(^{-}\) are independent samples from a Gaussian distribution, \(_{t}^{+}\) and \(_{t}^{-}\) are perturbed versions of \(_{0}^{+}\) and \(_{0}^{-}\) that depend on \(^{+}\) and \(^{-}\), and \(_{0}^{+}\) is preferred to \(_{0}^{-}\). A detailed description is given in Appendix A.1.

Additional notation.We use \(_{}\) and \(_{}\) to represent the reference image and generated image, respectively. \(_{}\) denotes the set of reference images, and \(_{}\) is the set of generated images. \((})\) represents the probability that \(\) is more preferred than \(}\).

## 4 Method

We present our \(\)-Harmonic reward function that provides reward signals for subject-driven tasks to reduce the risk that the learned model will overfit to the reference images. Based on this reward function, we use the Bradley-Terry model to sample preference labels and a preference algorithm to finetune the diffusion model by optimizing both a similarity loss and a preference loss.

### Reward Preference Optimization

In contrast to other fine-tuning applications [30; 18; 21; 20], there is no human feedback in the subject-driven text-to-image generation task. The model only receives a few reference images and a prompt with a specific subject. Hence, we first propose the \(\)-Harmonic reward function that can leverage the ALIGN model  to provide feedback based on the generated image fidelity: similarity to the given reference images and faithfulness to the text prompts.

\(\)-Harmonic Reward Function.The normalized ALIGN-I and ALIGN-T scores can be denoted as

\[(,_{})_{}|}_{}_{}} (f_{}(),f_{}(})) +1}{2}\] (Image alignment) \[(,) (f_{}(),g_{}())+1}{2}\] (Text alignment),where CosSim is the cosine similarity, \(f_{}()\) is the image feature extractor and \(g_{}()\) is the text encoder in the ALIGN model. Given a reference image set \(_{}\), the \(\)-Harmonic reward function can be defined by a weighted harmonic mean of the ALIGN-I and ALIGN-T scores,

\[r(,;,_{})(,_{})}+(,)}}.\] (5)

Compared to the arithmetic mean, there are two advantages to using the harmonic mean: (1) according to AM-GM-HM inequalities , the harmonic mean is a lower bound of the arithmetic mean and maximizing this "pessimistic" reward can also improve the arithmetic mean of ALIGN-I and ALIGN-T scores; (2) the harmonic mean is more sensitive to the smaller of the two scores, i.e., a larger reward is only achieved when both scores are relatively large.

For a simple example, consider \(=0.5\). If there are two images, \(\) and \(}\), where the first image achieves an ALIGN-I score of 0.9 and an ALIGN-T score of 0.01, and the second image receives an ALIGN-I score of 0.7 and an ALIGN-T score of 0.21, we may prefer the second image because it has high similarity to the reference images and is faithful to the text prompts. However, using the arithmetic mean would assign both images the same reward of 0.455. In contrast, the harmonic mean would assign the first image a reward of 0.020 and the second image a reward of 0.323, aligning with our preferences. During training, we set \(_{}=0\), which means the reward model will focus solely on text-to-image alignment because the objective function consists only of a loss for image-to-image alignment. Note that we set \(_{}\) to a different value for validation, which evaluates the fidelity of the subject and faithfulness of the prompt. Details can be found in Section 5.

Dataset.The set of images for subject-driven generative tasks can usually be represented as \(=_{}_{}\), where \(_{}\) is the image set generated by the base model. DreamBooth  requires two different prompts, \(\) and \(_{}\), where \(\) includes a reference to the subject while \(_{}\) refers to the prior class of the subject but not the subject. For example, \(\) can be "a photo of [V] dog" and \(_{}\) can be "a photo of a dog", where "[V]" is a unique token that refers to the subject and dog is the prior class of the subject. DreamBooth then uses \(_{}\) to generate a variety of images \(_{}\) in the prior class to avoid overfitting to the reference images via a regularizer. Typically, the size of the set of generated images is around 1000, i.e., \(|_{}|=1000\), which is time-consuming and space-intensive in real applications. However, the diffusion model can only maximize the similarity score and still receives a high reward based on this uninformative prompt \(_{}\). Our method aims to balance the trade-off between similarity and faithfulness. Thus, for efficiency, we introduce 8 novel training prompts, \(_{}\) of the form "a [V] [class noun] [modification]" where modification includes _artistic style transfer, re-contextualization_, and _accessorization_. For example, \(_{}\) can be "a [V] dog is on the Moon". These training prompts can be pre-specified or generated by other Large Language Models2. The full list of training prompts is provided in the supplementary material (Figure 6). We feed these training prompts to the base model and generate 4 images for each training prompt, i.e., \(|_{}|=32\).

Once we obtain reward signals, we adopt the Bradley-Terry model  to generate preference labels. In particular, given a tuple \((_{},_{},_{})\), we sample preference labels \(y\) from the following probability model:

\[(_{}_{}) _{},_{};, _{}))}{(r(_{},_{};,_{}))+(r(_{}, _{};,_{}))}.\] (6)

Learning.The learning objective function consists of two parts -- similarity loss and preference loss. The similarity loss is designed to minimize the KL divergence between the distribution of reference images and the learned distribution \(p_{}()\), which is equivalent to minimizing:

\[_{}()_{_{},,,_{}}[\|_{}(_{,t},,t)-_{}\|_ {2}^{2}], t\{1,,T\},_{}(,).\] (7)

The preference loss aims to capture the preference signals and fit the preference model, Eq. (6). Therefore, we use binary cross-entropy as the objective function for the preference loss. Combiningthe DPO objective function in Eq. 4, the loss function can be written as follows:

\[_{}() _{_{},_{ },_{},y,t,_{},_{ {gen}}}y_{}(_{},_{},_{},y,t,_{ },_{})\] \[+(1-y)-_{}(_{ {ref}},_{},_{},y,t,_{ },_{}),\] (8)

where

\[_{}(_{},_{},\ _{},y,t,_{},_{}) \|_{}(_{,t}, _{},t)-_{}\|_{2}^{2}-\|_{}(_{,t},_{},t)- _{}\|_{2}^{2}\] \[-(\|_{}(_{,t}, _{},t)-_{}\|_{2}^{2}-\|_{}(_{,t},_{},t)- _{}\|_{2}^{2}),\]

and \(t\{1,,T\}\) and \(_{},_{}(, )\). Combining these two loss functions together, the objective function for finetuning is written as

\[()=_{}()+_{ }()\] (9)

Figure 2 presents an overview of the training method, which includes the base model generated samples, the ALIGN reward model, and the preference loss. Note that \(_{}\) serves as a regularizer for approximating the text-to-image alignment policy. Conversely, DreamBooth  adopts \(_{}(p_{}(_{t-1}_{t}, )|p_{}(_{t-1}_{t},))\) as its regularizer, which cannot guarantee faithfulness to the text-prompt. Based on this loss function and preference model, we only need a few hundred gradient steps and a small set size of \(|_{}|\) to achieve results that are comparable to, or even better than, the state of the art. The fine-tuning process, which includes generating images, training, and validation, takes about 5 to 20 minutes on a single Google Cloud Platform TPUv4-8 (32GB) for Stable Diffusion.

## 5 Experiments

In this section, we present the experimental results demonstrated by RPO. We investigate several questions. First, can our algorithm learn to generate images that are faithful both to the preference images and to the textual prompts, according to preference labels? Second, if RPO can generate high-quality images, which part is the key component of RPO: the reference loss or the early stopping by the \(\)-Harmonic reward function? Third, how do different \(_{}\) values used during validation affect performance in RPO? We refer readers to Appendix A.2 for details on the experimental setup, Appendix A.7 for the skill set of RPO, Appendix A.4 for the limitations of the RPO algorithm, and Appendix A.5 for future work involving RPO.

Figure 2: Overview of the finetuning phase for RPO. First, the base diffusion model generates a few images based on novel training prompts. Second, we compute the rewards for both reference and generated images using Equation (5). Then, preference labels are sampled according to the preference distribution, as defined in Equation (6). Finally, the diffusion model is trained by minimizing both the similarity loss (Equation (7)) and preference loss (Equation (8)).

### Dataset and Evaluation

DreamBench.In this work, we use the DreamBench dataset proposed by DreamBooth . This dataset contains 30 different subject images including backpacks, sneakers, boots, cats, dogs, and toy, etc. DreamBench also provides 25 various prompt templates for each subject and these prompts are requiring the learned models to have the following abilities: re-contextualization, accessorization, property modification, and attribute editing.

Evaluation Metrics.We follow DreamBooth  and SuTI  to report DINO 3 and CLIP-I  for evaluating image-to-image similarity score and CLIP-T  for evaluating the text-to-image similarity score. We also use our \(\)-Harmonic reward as a evaluation metric for the overall fidelity and the default value of \(=0.3\). For evaluation, we follow DreamBooth  and SuTI  to generate 4 images per prompt, 3000 images in total, which provides a robust evaluation.

Baseline algorithms.DreamBooth : A test-time fine-tuning method. This algorithm requires approximately \(|_{}|=1000\) and 1000 gradient steps to finetune the UNet and text-encoder components. SuTI : A pre-trained method that requires half a million expert models and introduces cross-attention layers into the original diffusion models. Textual Inversion : A text-based method that optimizes the text embedding but freezes the diffusion models. Re-Imagen : An information retrieval-based algorithm that modifies the backbone network architectures and introduces cross-attention layers into the original diffusion models. DisenBooth : A test-time fine-tuning method that generates subject-driven images by optimizing textual identity-preserving embeddings. Custom Diffusion : a test-time fine-tuning method, focused on efficient training by optimizing the key and value projection matrices in the cross-attention layers of diffusion models. ELITE : A test-time fine-tuning approach consisting of two stages: one for training the textual embeddings and another for preserving identity. IP-Adapter : a pretrained method where the decoupled cross-attention layer captures the textual signal while integrating reference images. SSR-Encoder : also a pretrained method that highlights selective regions and extracts detailed features for subject-driven image generation.

### Results

Quantitative Results.We begin by addressing the first question. We use a quantitative evaluation to compare RPO with other existing methods on three metrics (DINO, CLIP-I, CLIP-T) in DreamBench to validate the effectiveness of RPO. The experimental results on DreamBench is shown in Table 1. We observe that RPO can perform better or on par with existing works on all three metrics. Compared to DreamBooth, RPO only requires \(3\%\) of the negative samples, but RPO can outperform DreamBooth on the CLIP-I and CLIP-T scores by \(3\%\) given the same backbone. Our method outperforms all baseline algorithms in the CLIP-T score, establishing a new SOTA result. This demonstrates that RPO, by solely optimizing UNet through preference labels from the \(\)-Harmonic reward function, can generate images that are faithful to the input prompts. Similarly, our CLIP-I score is also the highest, which indicates that RPO can generate images that preserve the subject's visual features. In terms of the DINO score, our method is almost the same as DreamBooth when using the same backbone. We conjecture that the reason RPO achieves higher CLIP scores and lower DINO score is that the \(\)-Harmonic reward function prefers to select images that are semantically similar to the textual prompt, which may result in the loss of some unique features in the pixel space.

Qualitative Results.We use the same prompt as SuTI , and the generated images are shown in Figure 3. RPO generates images that are faithful to both reference images and textual prompts. We noticed a semantic mistake in the first prompt used by SuTI ; it should be A dog eating a cherry from a bowl. Furthermore, each reference bowl image contains blueberries, and the ambiguous prompt caused the RPO-trained model to become confused during the inference phase. However, RPO still preserves the unique appearance of the bowl. For instance, while the text on the bowl is incorrect or blurred in the SuTI and DreamBooth results, RPO accurately retains the words _Bon Appetit_ from the reference bowl images. Although existing methods can produce images highly faithful to the reference images, they may not align as well with the textual prompts. We also

   Method & Backbone & Iterations \(\) & DINO \(\) & CLIP-I \(\) & CLIP-T \(\) \\  Reference Images & N/A & N/A & \(0.774\) & \(0.885\) & N/A \\  DreamBooth  & Imagen  & \(1000\) & \(0.696\) & \(0.812\) & \(0.306\) \\ DreamBooth  & SD  & \(1000\) & \(0.668\) & \(0.803\) & \(0.305\) \\ Textual inversion  & SD  & \(5000\) & \(0.569\) & \(0.780\) & \(0.255\) \\ SuTI  & Imagen  & \(1.5 10^{5}\) & \(\) & \(0.819\) & \(0.304\) \\ Re-Imagen  & Imagen  & \(2 10^{5}\) & \(0.600\) & \(0.740\) & \(0.270\) \\ DisenBooth  & SD & \(3000\) & \(0.574\) & \(0.755\) & \(0.255\) \\ Custom Diffusion  & SD & 500 & \(0.695\) & \(0.801\) & \(0.245\) \\ ELETE  & SD & \(3000\) & \(0.652\) & \(0.765\) & \(0.255\) \\ IP-Adapter  & SD & \(10^{6}\) & \(0.608\) & \(0.809\) & \(0.274\) \\ SSR-Encoder  & SD & \(10^{6}\) & \(0.612\) & \(0.821\) & \(\) \\  Ours: RPO & SD  & \(\) & \(0.652\) & \(\) & \(\) \\   

Table 1: Quantitative comparison for the number of iterations, subject fidelity and prompt fidelity.

Figure 3: Qualitative comparison with other subject-driven text-to-image methods, adapted from provide an example in Appendix A.3 that shows how RPO can handle the failure case observed in DreamBooth and SuTI. Additionally, we find that the RPO model can generate reasonable images even for highly imaginative prompts, as shown in Appendix A.7 (Figure 16 and Figure 17). These images demonstrate that the RPO-trained model does not overfit the training data. Instead, the \(\)-Harmonic function provides a method for selecting a model capable of text-to-image alignment, even when the textual prompts are highly imaginative.

### Ablation Study and Method Analysis

Preference Loss and \(\)-Harmonic Ablation.We investigate the second question through an ablation study. Two regularization components are introduced into RPO: reference loss as a regularizer and early stopping by \(_{}\)-Harmonic reward function. Consequently, we compare four methods: (1) Pure \(_{}\), which only minimizes the image-to-image similarity loss \(_{}\); (2) \(_{}\) w/o early stopping, which employs \(_{}\) as a regularizer but omits early stopping by \(_{}\)-Harmonic reward function; (3) Early stopping w/o \(_{}\), which uses \(_{}\)-Harmonic reward function as a regularization method but excludes \(_{}\); (4) Full RPO, which utilizes both \(_{}\) and early stopping by the \(_{}\)-Harmonic reward function. We choose the default value \(_{}=0.3\) in this ablation study.

Table 2 lists the evaluation results of these four methods on DreamBench. We observe that without early stopping, \(_{}\) can still prevent overfitting to the reference images and improve text-to-image alignment, though the regularization effect is weak. Specifically, the \(0.3\)-Harmonic only shows a

   Method & DINO \(\) & CLIP-I \(\) & CLIP-T \(\) & \(0.3\)-Harmonic \(\) \\  Pure \(_{}\) & \(\) & \(\) & \(0.285 0.027\) & \(0.660 0.016\) \\ \(_{}\) w/o early-stopping & \(0.688 0.082\) & \(0.845 0.042\) & \(0.296 0.027\) & \(0.663 0.014\) \\ Early-stopping w/o \(_{}\) & \(0.575 0.124\) & \(0.799 0.052\) & \(0.323 0.025\) & \(0.672 0.013\) \\ RPO (\(_{}=0.3\)) & \(0.581 0.113\) & \(0.798 0.039\) & \(\) & \(\) \\   

Table 2: Ablation study on regularization to evaluate fidelity across multiple subjects and prompts. Standard deviation is included.

Figure 4: Changes in the \(0.3\)-Harmonic reward value during RPO training process.

   Configuration & DINO \(\) & CLIP-I \(\) & CLIP-T \(\) & \(0.3\)-Harmonic \(\) \\  \(_{}=0.3\) & \(0.581 0.113\) & \(0.798 0.039\) & \(\) & \(\) \\ \(_{}=0.5\) & \(0.652 0.082\) & \(0.833 0.041\) & \(0.314 0.022\) & \(0.671 0.008\) \\ \(_{}=0.7\) & \(\) & \(\) & \(0.304 0.023\) & \(0.667 0.011\) \\   

Table 3: Different validation \(_{}\)-Harmonic reward comparison for evaluating fidelity over multiple subjects and prompts. Standard deviation is included.

marginal improvement of 0.003 over pure \(_{}\) and 0.001 over early stopping without \(_{}\). The early stopping facilitated by the \(_{}\)-Harmonic reward function plays a crucial role in counteracting overfitting, helping the diffusion models retain the ability to generate high-quality images aligned with textual prompts. To provide a deeper understanding of the \(\)-Harmonic reward validation, we present two examples from during training in Figure 4, covering both objects and live subjects. We found that the model tends to overfit at a very early stage, i.e., within 200 gradient steps, where \(\)-Harmonic can provide correct reward signals for the generated images. For the backpack subject, the generated image receives a low reward at gradient step 80 due to its lack of fidelity to the reference images. However, at gradient step 400, the image is overfitted to the reference images, and the model fails to align well with the input text, resulting in another low reward. \(\)-Harmonic assigns a high reward to images that are faithful to both the reference image and textual prompts.

Impact of \(_{}\).We examine the third question by selecting different \(_{}\) values from the set \(\{0.3,0.5,0.7\}\) as the validation parameters for the \(\)-Harmonic reward. According to Equation 5, we believe that as \(_{}\) increases, the \(\)-Harmonic reward function will give higher weight to the image-to-image similarity score. This will make the generated images more closely resemble the reference images, however, there is also a risk of overfitting. Table 3 shows the results of three different \(_{}\) values on DreamBench. As we expected, a larger \(_{}\) makes the images better preserve the characteristics of the reference images, but it also reduces the text-to-image alignment score. Figure 5 shows us an example. In this example, different \(_{}\) values lead to different outcomes due to varying strengths of regularization. A smaller \(_{}=0.3\) can generate more varied results, but seems somewhat off from the reference images. \(_{}=0.5\) preserves some characteristics beyond the original subject, such as the sofa, but also maintains alignment between text and image. However, when \(_{}=0.7\) is chosen as an excessively large value, the model actually overfits to the reference images, ignoring the prompts. We have additional comparisons, including different \(_{}\)'s, different Pythagorean means, and aesthetic scores  in Appendix A.3.

## 6 Conclusion

We introduce the \(\)-Harmonic reward function to derive preference labels and employ RPO to finetune the diffusion model for subject-driven text-to-image generation tasks. Additionally, the \(\)-Harmonic reward function serves as a validation method, enabling early stopping to mitigate overfitting to reference images and speeding up the finetuning process.