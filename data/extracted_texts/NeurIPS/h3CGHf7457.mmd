# Multi-modal Queried Object Detection in the Wild

Yifan Xu\({}^{1,3}\)\({}^{}\), Mengdan Zhang\({}^{2}\)\({}^{1}\), Chaoyou Fu\({}^{2}\),

**Peixian Chen\({}^{2}\)**, **Xiaoshan Yang\({}^{1,3,4}\)**, **Ke Li\({}^{2}\)**, **Changsheng Xu\({}^{1,3,4}\)**

\({}^{1}\)MAIS, Institute of Automation, Chinese Academy of Sciences \({}^{2}\)Tencent Youtu Lab

\({}^{3}\)School of Artificial Intelligence, University of the Chinese Academy of Sciences

\({}^{4}\)Peng Cheng Laboratory

{yifan.xu, csxu}@nlpr.ia.ac.cn, davinazhang@tencent.com

Work done when interning at Tencent Youtu Lab. \({}^{}\)Equal contribution. \({}^{}\)Corresponding author.

###### Abstract

We introduce MQ-Det, an efficient architecture and pre-training strategy design to utilize both textual description with open-set generalization and visual exemplars with rich description granularity as category queries, namely, **M**ulti-modal **Q**ueried object **D**etection, for real-world detection with both open-vocabulary categories and various granularity. MQ-Det incorporates vision queries into existing well-established language-queried-only detectors. A plug-and-play gated class-scalable perceiver module upon the frozen detector is proposed to augment category text with class-wise visual information. To address the learning inertia problem brought by the frozen detector, a vision conditioned masked language prediction strategy is proposed. MQ-Det's simple yet effective architecture and training strategy design is compatible with most language-queried object detectors, thus yielding versatile applications. Experimental results demonstrate that multi-modal queries largely boost open-world detection. For instance, MQ-Det significantly improves the state-of-the-art open-set detector GLIP by +7.8% AP on the LVIS benchmark via multi-modal queries without any downstream finetuning, and averagely +6.3% AP on 13 few-shot downstream tasks, with merely additional 3% modulating time required by GLIP. Code is available at [https://github.com/YifanXu74/MQ-Det](https://github.com/YifanXu74/MQ-Det).

## 1 Introduction

As the thriving of large-scale vision-language pre-training models [25; 48; 24; 29; 27; 44; 20], object detection recently has ushered in a new fashionable paradigm, which locates the desired objects via a queried text. Benefited from the generalization of the pre-training models on large scale data [26; 13; 36; 43; 22; 18; 37; 31; 35], such a text queried paradigm makes steady progress on the long road to open-set object detection.

Compared with previous fixed category sets (usually represented by finite numbers), the foregoing text query has the merit of representing broad concepts, but also has an intrinsic limitation of insufficient description granularity [4; 9; 32]. For example, class homonyms (_e.g._, "bat" can be a piece of wood or a kind of animal) lead to ambiguous queries. Meanwhile, as exemplified in Fig. 1, for fine-grained fish species detection, it is usually struggled to use limited text to describe the fish with specific patterns. Empirically, one straightforward solution for the insufficient description granularity problem is to design additional textual description, but there are three distinct obstacles: 1) it is difficult to comprehensively describe visual details , and constructing textual description for a large amount of categories is a laborious work. 2) The longer queried text increases the understanding difficulty of the pre-training model and 3) brings more computational overhead. Experimentally, the state-of-the-art (SoTA) text queried detector GLIP  merely improves average precision (AP) from17.7% to 18.4% on the Aquarium dataset  (a fine-grained fish species detection dataset), even with extra manually designed textual description for some categories.

A picture paints a thousand word. Compared with the text, the image can provide richer clues about the target object. But at the same time, the human-generated text has higher information density and thereby brings stronger generalization capability [32; 8; 2]. In light of this, a natural idea springs up, _i.e._, combining the text and the image to constitute a multi-modal query, which has the advantages of both breadth of the former and rich granularity of the latter. Nevertheless, how to acquire such a multi-modal queried detection model still faces challenges: 1) directly finetuning with limited visual exemplars, typically in previous vision-queried few-shot detection methods [49; 39; 21; 16], leads to the risk of catastrophic forgetting. 2) Large foundation models have good generalization but require heavy training burden if re-organized and trained from scratch (e.g., over 30 million data storage and almost 480 V100 GPU days for GLIP [25; 48]).

This work fills in the blank of **M**ulti-modal **Q**ueried object **D**etection (**MQ-Det**), and introduces an efficient plug-in training architecture. The crux of the proposed MQ-Det is to fuse description-rich visual cues and the highly generalizable text representations, while only adding a small training cost on the basis of existing language queried fundamental detection models. We evaluate our models on a _finetuning-free_ setting, where users can detect their customized objects through textual descriptions, visual exemplars, or both without any finetuning. With only one epoch of modulating on the Objects365  dataset, accounting for a mere 3% of the total pre-training time for GLIP, our approach impressively improves the finetuning-free performance by +7.8% on the LVIS benchmark through providing the model with 5 visual exemplars along with textual category descriptions.

To be specific, as shown in Fig. 1, we first interleave a **Gated Class-scalable Perceiver (GCP)** that bridges class-wise visual cues and textual cues in each high-level stage of the text encoder of the detector. This module consists of class-wise cross attention layers to augment textual semantic embeddings with visual details, and a conditional gating layer that varies based on the quality of visual cues from the vision queries. The output of this module is incorporated to each high-level stage of the text encoder in a residual-like manner, which conducts textual perturbation in initial general embedding space . Additionally, this simple design has no class-specific parameters and can be extended to classes of various granularity. Second, we design a **vision conditioned masked language prediction strategy** to ensure sufficient visual intervention in the modulating stage. We observed a learning inertia problem during modulating, namely, when incorporating visual cues in a gated residual-like manner, the learning process tends to be stuck around the initial optimum point of the frozen pre-training detector and cannot introduce much visual knowledge. Thus, we randomly mask the text tokens and let corresponding vision queries independently make object prediction. Note that we freeze the initial detection foundation model and only train the GCP modules in the modulating stage, which is quite efficient.

**Contributions**. In summary, the contributions of this paper are as follows: **(i)** As far as we know, we are the pioneer to introduce the multi-modal query that has the characteristics of both breadth and rich granularity, paving a new path to the object detection in the wild. **(ii)** We propose a plug-and-play GCP module that dynamically fuses informative visual cues and highly generalizable text cues from the multi-modal query, and employ a vision conditioned masked language prediction strategy to permit sufficient multi-modal fusion upon frozen detection models. **(iii)** Our proposed MQ-Det demonstrates powerful transferability on the finetuning-free and few-shot scenarios, while requiring much less training time than previous SoTA fundamental detectors. Specifically, MQ-Det outperforms GLIP by +7.8% AP in the finetuning-free detection on the challenging LVIS  and averagely +6.3% AP on 13 downstream few-shot detection tasks , modulated with merely 3% of the training time required by GLIP.

## 2 Methodology

This section describes MQ-Det: an efficient modulation architecture with elaborately designed multi-modal fusion strategy to let vision-language (VL) detection foundation models accept text interleaved with images as input queries, thus leveraging the generalization capability of semantic-rich language while enhancing detection discrimination with visual details. We first briefly review the language-queried baseline GLIP  in Sec. 2.1, then introduce the plug-and-play architecture that can support class-scalable vision queries as input in Sec. 2.2, and finally shed light on the multi-modal fusion strategy in Sec. 2.3.

### Preliminaries

**Language-queried detection model**. Following CLIP , the paralleled formulation has been a mainstream architecture of existing VL detection foundation models [48; 29; 42; 44; 28; 27]. Take our baseline GLIP  as an example, as shown in the left part of Fig. 1, it consists of paralleled text encoder \(_{T}()\) and image encoder \(_{I}()\) for feature extraction, and reformulates detection as a grounding task, by grounding/aligning each visual region to class content in a text prompt. Since the detection models like GLIP recognize objects via class content only in the language queries, we denote them as language-queried detection models.

Specifically, the model takes image-text pairs \(\{(,)\}\) as inputs, where the text prompt \(=``t_{1},,t_{|C|}"\) contains \(|C|\) categories to be detected, _e.g._, "person, bicycle,..., toothbrush". Then, the model extracts image and text features (\(I,T\)) via paralleled encoders. Every linear classification layer in traditional vision models are replaced by a vision-language matching dot-product layer, calculating the region-word similarity logits \(S_{cls}\) in the detection head \(H()\). Formally,

\[I=_{I}(), T=_{T}(), R=H(I), S_{ cls}=R T^{}, \]

where \(I\) is the extracted image features, \(R^{N d}\) denotes the object/region/box features of the input image, and \(T^{|C| d}\) is the contextual word/token features from the text encoder in which each token represents one category. Additionally, a box regressor is applied to \(R\) for localization that is similar to traditional detectors. The model is finally trained with a phrase grounding loss calculated via the logits \(S_{cls}\) and a traditional localization loss. We refer to GLIP  for more details.

The language-queried model GLIP is trained on massive image-text pairs, and thus scales up visual concepts beyond traditional detection vocabularies, and finally achieves strong open-set capacity. However, the inference process via a simple language-queried prompt limits its performance especially for non-descriptive, ambiguous or fine-grained class names.

### Architecture design

Considering the limited description granularity of text stated in Sec. 1, we propose to integrate vision queries into the pre-trained language-queried detection models to enrich semantic cues. The architecture of such multi-modal queried detection models is expected to follow three principles: (1) **Class-scalable**: the introduced vision queries are open to arbitrary categories rather than fitting a

Figure 1: **Method overview**. The plug-and-play **G**ated **Class-scalable **P**erceivers (GCPs) are inserted to text encoder layers, trained with our masked language prediction strategy. Class-wise attention mask is applied to make the text query of each category only attends to corresponding vision queries.

closed set of categories. (2) **Semantic-complementary**: the vision queries are sufficiently interleaved with coarse text queries to provide fine-grained visual details that support to distinguish various granularity of categories. (3) **Generalization-retainable**: the introduction of visual queries into foundation models accumulates rich details while still retaining the generalization capability.

We compare three possible architecture designs for enriching queries as illustrated in Fig. 2, and show the superiority of our approach on the above-mentioned principles. First, soft prompt based designs  (e.g., CoOp , Fig. 2 (a)) can adapt text queries to describe diverse characteristics of categories beyond intensively-tuned manual prompts. However, most previous works only finetune task-specific prompts, which limits their generalization. Second, existing two-branch few-shot detection architectures  (e.g., FCT , Fig. 2 (b)) support vision queries and detect novel objects using very few training exemplars. But their designs are incompatible with the language-queried pre-training foundation models due to deficiency of language inputs and model complexity (_e.g._, early and deep fusion in ). We propose a simple yet effective plug-in architecture in Fig. 2 (c), which interleaves class-wise visual cues with textual cues, to let highly semantic language-queried models "perceive" visual details.

Inspired by , we augment the semantics of each category query by conditioning the text on corresponding visual representations produced by our proposed **Gated Class-scalable Perceiver (GCP)** illustrated in Fig. 1. Specifically, we insert GCP modules between the frozen pre-trained text encoder blocks in a residual manner and only train these modules from scratch. In a GCP module, each category token from the text encoder block is independently cross-attended to corresponding vision queries to acquire rich details (Principles 1,2). Then, the interleaved category tokens are gated by the GCP module so that the frozen text encoder is kept intact at initialization for improved stability and generalization performance (Principle 3).

Formally, given an image-text pair \((,)\) with vision queries \(=\{_{i}|_{i}=\{v_{i}^{(j)}\}_{j=1}^{k}\}_{i=1} ^{|C|}\) extracted from \(k\) exemplars of each category, the GCP module augments each text query features in \(T=_{T}()=\{t_{i}\}_{i=1}^{|C|}\) in a residual-like way, namely,

\[}_{i}=(_{i},I),_{i}=(t_{i},}_{i}),_{i}=t_{i}+(gate(_{i})) _{i}, i=1,2,,|C|, \]

where \((,)\) denotes a multi-head cross-attention layer with the former input as queries and the later as keys and values. \(=tanh()\) is a normalization function. Concretely, the vision query features \(_{i}=_{I}(_{i})\) are first augmented via the target image feature \(I=_{I}()\) to gather content-aware information . The augmented vision queries \(}_{i}\) are correlated with corresponding text token \(t_{i}\) to enrich it from multiple views. The conditional gating value \((gate(_{i}))\) is dynamically adjusted according to the quality of visual cues from the exemplars of each category, which is evaluated by a three-layer perceptron (MLP) that reduces the feature dimension gradually to a layer-specific learnable scalar. The conditional gating value multiplies the enriched feature \(_{i}\) before adding it to the initial text token \(t_{i}\) from the residual connection. Since the gating value is initialized to 0, the module output matches that of the pre-trained text encoder at initial training, improving training stability and final performance. It is worth noting that this independent class-wise cross-attention scheme has no class-specific parameters and can be scaled to any classes of various description granularity, as verified in Sec. 3.2. In addition, it importantly allows the model to seamlessly generalize to any number of vision queries in inference, regardless of the number used during training.

### Modulated pre-training

Equipped with the plug-in architecture design described in Sec. 2.2, we propose an efficient pre-training strategy that permits sufficient multi-modal fusion and rapid learning upon frozen detection foundation models using only a small subset of pre-training data. We denote this process as modulated pre-training in comparison to the massive pre-training of foundation models.

**Vision query extraction**. We provide vision queries for \(|C|\) categories from a bank \(D\) built from pre-training data:

\[D=\{_{i}|\,|_{i}|=K\}_{i=1}^{|C|}, v_{i}^{(j)}=RoIPool ((), b), j=1,2,,K, \]

where each \(_{i}^{K d}\) contains \(K\) vision queries modeled by the image encoder \(_{I}\). Specifically, given a query instance of the \(i\)-th category with box \(b^{4}\) in an image \(\), an RoI pooler  is employed to extract the corresponding region feature \(v_{i}^{(j)}^{d}\). The parameter \(=1.5^{2}\) enlarges the region area for contextual clues. During modulated pre-training, we randomly select \(k K\) vision queries from \(D\) for each category at each forward process, simulating the downstream scenarios where only low-shot vision queries are provided. In practical implementation, we set \(K=5000\) and \(k=5\). After modulating, our model can generalize to arbitrary categories and vision queries during downstream inference.

**Training upon frozen pre-trained language-queried detectors**. Given the inherently general semantic representations in the pre-trained detector, we argue that the textual features only require minor modulation from the visual details rather than significant alteration. This viewpoint has been previously discussed in , and we have observed in Tab. 3 (c) that the full-model training on a limited number of categories faces the risk of catastrophic forgetting. Consequently, for efficiency purpose, we freeze the entire pre-trained detector and only train the newly-added gated class-scalable perceiver modules, significantly accelerating the training process.

**Vision conditioned masked language prediction**. Since the fundamental VL models highly rely on text description, the learning process of the residual-like gated architecture will rapidly converge around the initial optimum point with frozen text features, causing failure of vision queries. That is, models still learn how to align region with text features not visual cues. We represent such an issue as a learning inertia problem. As shown in the first row of Tab. 3 (a), the AP merely improves +0.9% over GLIP-T  on LVIS. To address this issue, we propose a simple yet effective masking strategy:

\[=\{t_{1},t_{2},,[],,t_{|C|}\}. \]

Namely, given an image-text pair \((,)\), a ground-truth text token in \(\) corresponding to instances occurring in \(\) is randomly masked by a [MASK] token with probability 40%. The [MASK] token is forced to extract visual cues from vision queries in the GCP modules to provide accurate predictions, thus bridging the model's dependence upon vision queries. As shown in Tab. 1, our language masking strategy ensures sufficient visual intervention in the modulated pre-training stage and significantly boosts the performance, _e.g._, +4.4% AP over GLIP-T on finetuning-free LVIS.

## 3 Experiments

### Setup

#### 3.1.1 Datasets and benchmarks

**Objects365 dataset** is a large-scale, high-quality dataset for object detection. We use this dataset to conduct the modulated pre-training of our MQ-Det models. It contains 0.66 million images of 365 object categories, with 30 million bounding boxes, which are more diverse and fine-grained than those in other popular datasets such as COCO  and Pascal VOC . Objects365 is a widely-used pre-training dataset in previous foundamental detection models .

**LVIS benchmark** is a challenging dataset for long-tail objects. It contains 1,203 categories for evaluation, with many rare categories that scarcely exist in the pre-training datasets. Therefore, we

Figure 2: Architecture designs for query enriching, with only classification branches presented for clarity. (a) Prompt-based designs  only tune upon language instruction while (b) two-branch few-shot detection architectures  employ heavily coupled parallel encoders and merely take vision inputs. (c) MQ-Det elegantly combines both language and vision queries in an efficient way.

use LVIS in the downstream task to evaluate the finetuning-free transferability. We report on MiniVal containing 5,000 images introduced in MDETR  as well as the full validation set v1.0.

**ODinW benchmark** (Object Detection in the Wild) is a more challenging benchmark for evaluating model performance under real-world scenarios. For example, Aquarium requires locating fine-grained fish species, and Pothole concerns detecting holes on the road. It collects more than 35 datasets for evaluation. There are two commonly used versions of the ODinW benchmark, _i.e._, ODinW-13 and ODinW-35, where ODinW-13 is a subset of ODinW-35 and contains much less noisy data. The results on both benchmarks are reported for comprehensive comparison. We demonstrate that the fine-grained visual details in MQ-Det facilitate transfer to such diverse and challenging tasks.

#### 3.1.2 Implementation details

We conduct extensive experiments on three settings: an open-set setting on finetuning-free LVIS  and ODinW  benchmarks, a few-shot setting and a close-set full-shot setting on ODinW. To demonstrate the plug-and-play versatility of our approach, we apply MQ-Det on two typical SoTA language-queried object detectors, GLIP  and GroundingDINO , and obtain our multi-modal queried models **MQ-GLIP** and **MQ-GroundingDINO**, respectively. We incorporate our GCP modules into the last 6 layers of the frozen text encoder. We conduct modulated pre-training of our models on the Objects365 dataset  for only one epoch using 8 NVIDIA V100 GPUs. The finetuning process under few/full-shot settings completely follows the baseline method GLIP, where vision queries are extracted from the few/full-shot training set. We also evaluate our method in a _finetuning-free_ setting, namely, users can detect their customized objects through textual descriptions, visual exemplars, or both without any fine-tuning. During finetuning-free evaluation, we extract 5 instances as vision queries for each category from the downstream training set without any finetuning.

### MQ-Det helps low-shot object detection

#### 3.2.1 Multi-modal queried detection without finetuning

We evaluate the model's ability to recognize rare and diverse objects on both LVIS and ODinW in a _finetuning-free_ setting, where users can detect their customized objects through textual descriptions, visual exemplars, or both without any fine-tuning. Each category is provided with both language and vision queries. Tab. 1 shows the results on LVIS. Overall, MQ-Det demonstrates **strong finetuning-free transferability** with impressive **efficiency on both data usage and training time**. MQ-GLIP-L surpasses the current SoTA by a large margin with simply 5 visual exemplars provided, reaching \(34.7\)% AP on Val v1.0 (+7.8% over GLIP-L), which verifies the superiority of multi-modal queries over single-modal queries. Meanwhile, MQ-Det demonstrates good training efficiency, _e.g._, MQ-GLIP-T only requires additional 2% training time and 12% data usage when modulated upon GLIP-T.

Additionally, we find three contributing factors: **(i)** Strong generalization comes from combination of textual breadth and visual granularity. In Tab. 1, MQ-GLIP-T-Img replaces all text queries with [MASK] tokens and solely utilizes vision queries to predict objects, achieving 17.6% AP.

   Model & Backbone & Pre-Train Data &  Data \\ Size \\  &  Training Time \\ (V100 days) \\  &  \#Vision \\ Query \\  &  MiniVal (\%) \\  &  \% Val v1.0 (\%) \\  \\  MDETR † & RN101 & GoldG,RetC & 0.9M & 400 & 0 & 24.3 & 20.9 & 24.9 & 34.3 & 25.4 & 22.7 & 25.0 \\ Mask R-CNN ‡ & RN101 & & - & - & - & 0 & 33.6 & 33.4 & 30.9 & - & - & - \\ Superpixel-RFS ‡ & RN30 & - & - & - & 0 & - & - & - & - & 25.4 & 12.3 & 24.3 & 32.4 \\  GLIP-T (B)  & Swin-T & 0.0365 & 0.66M & 300 & 0 & 17.8 & 13.5 & 12.8 & 22.2 & 11.3 & 4.2 & 16.8 \\ GLIP-T  & Swin-T & 0.0365,GoldG,CCM & 5.3M & 480 & 0 & 20.8 & 20.8 & 21.4 & 31.0 & 17.2 & 10.1 & 25.5 \\ GLIP-T  & Swin-T & 0.0365,GoldG,CCM & 5.5M & - & 0 & 29.0 & - & - & - & - & - \\ GontinoDINO-T  & Swin-T & 0.0365,GoldG,capAtt & 5.5M & - & 0 & 25.7 & 15.2 & 21.9 & 30.9 & - & - & - \\ GLIP-L  & Swin-L & 
 FourDDs,GoldG,CapAtt \\  & 27.5M & 600 & 0 & 37.3 & 28.2 & 34.3 & 41.5 & 26.9 & 17.1 & 23.3 & 35.4 \\ GroundingDINO-L  & Swin-L & 0.0365,GoldG,CapAtt & 27.5M & 600 & - & 0 & 33.9 & 22.0 & 30.7 & 38.8 & - & - \\  MQ-GLIP-T-Img & Swin-T & 0.0365 & 0.66M & 10 & 5 & 17.6 & 12.0 & 14.5 & 21.2 & 12.4 & 8.9 & 9.2 & 18.3 \\ MQ-GLIP-T-Tat & Swin-T & 0.0365† & 0.66M & 10 & 0 & 26.0 & 20.8 & 21.4 & 31.0 & 17.2 & 10.1 & 12.5 & 25.5 \\  MQ-GroundingDINO-T & Swin-T & 0.0365† & 0.66M & 10 & 5 & 30.2 & 21.7 & 26.2 & 25.2 & 22.1 & 12.9 & 17.4 & 31.4 \\ MQ-GLIP-T & Swin-T & 0.0365† & 0.66M & 10 & 5 & 30.4 & 21.0 & 27.5 & 34.6 & 22.6 & 15.4 & 18.4 & 30.4 \\ MQ-GLIP-L & Swin-L & 0.0365† & 0.66M & 22 & 5 & **43.4** & **34.5** & **41.2** & **46.9** & **34.7** & **26.9** & **32.0** & **41.3** \\   

Table 1: Finetuning-free detection on the LVIS benchmark. * denotes supervised approaches. The training time is tested on one V100 GPU. We present the number of vision queries during evaluation.

This confirms that our low-cost modulated pre-training strategy introduces sufficient visual cues. Augmenting generalizable language with visual details, MQ-GLIP-T outperforms the text-only model MQ-GLIP-T-Txt by a large margin. **(ii)** Efficiency on data usage and training time benefits from frozen pre-trained detection models. Our approach preserves the generalizable knowledge in the frozen pre-trained models and thus only needs little additional cost for further modulation. This is also verified in Tab. 3 (c). **(iii)** Our simple plug-and-play architecture design enables wide-range applications, since most language-queried detectors share the similar parallel architecture design as GLIP and GroundingDINO, _e.g._, MDETR , OWL-ViT , and DetCLIP  in Tab. 2.

    &  &  &  & Data &  \(\)-35 \\ Size \\  } &  \(\)-13 \\ \(_{}\) \\  } &  \(\)-13 \\ \(_{}\) \\  } \\   & & & & & & & & & \\   \\  MDETR  & ✓ & ✓ & ✗ & ENB5  & GoldG,RetC & 0.9M & 10.7 & 25.1 \\ OWL-ViT  & ✓ & ✓ & ViT L1/4(CLIP) & 0.635, ViT & 0.8M & 18.8 & 40.9 \\ GLIP-T  & ✓ & ✗ & Swin-T & O365,GoldG,Cap4M & 5.5M & 18.7 & 41.9 \\ GLIP-L  & ✓ & ✗ & Swin-L & FourO,D6,GoldG,Cap24M & 27.5M & 22.6 & 51.0 \\ OmDet  & ✓ & ✗ & ConvNeXt-B & COCO,O365,LVIS,PhaseCut & 1.8M & 16.0 & 43.6 \\ GLIPv2-T  & ✓ & ✗ & Swin-T & O365,GoldG,Cap4M & 5.5M & 22.3 & 50.7 \\ DetCLIP  & ✓ & ✗ & Swin-T & O365,GoldG,YFCC1M & 2.4M & - & 43.3 \\ GroundingDINO-T  & ✓ & ✗ & Swin-T & O365,GoldG,Cap4M & 5.5M & 21.7 & 49.8 \\  MQ-GroundingDNO-T & ✓ & ✓ & Swin-T & O365\({}^{}\) & 0.66M & 22.5 & 50.9 \\ MQ-GLIP-T & ✓ & ✓ & Swin-T & O365\({}^{}\) & 0.66M & 20.8 & 45.6 \\ MQ-GLIP-L & ✓ & ✓ & Swin-L & O365\({}^{}\) & 0.66M & **23.9** & **54.1** \\   \\  DyHead-T  & ✗ & ✗ & Swin-T & O365 & 0.66M & 37.5 & 43.1 \\ GLIP-T  & ✓ & ✗ & Swin-T & O365,GoldG,Cap4M & 5.5M & 38.9 & 50.7 \\ DINO-Swin-T  & ✗ & ✗ & Swin-T & O365 & 0.66M & 41.2 & 49.0 \\ OmDet  & ✓ & ✗ & ConvNeXt-B & COCO,O365,LVIS,PhaseCut & 1.8M & 42.4 & 48.5 \\  MQ-GLIP-T & ✓ & ✓ & Swin-T & O365\({}^{}\) & 0.66M & **43.0** & **57.0** \\   \\  GLIP-T  & ✓ & ✗ & Swin-T & O365,GoldG,Cap4M & 5.5M & 62.6 & 61.9 \\ DyHead-T  & ✗ & ✗ & Swin-T & O365 & 0.66M & 63.2 & 58.7 \\ DINO-Swin-T  & ✗ & ✗ & Swin-T & O365 & 0.66M & 66.7 & - \\ OnDet  & ✓ & ✗ & ConvNeXt-B & COCO,O365,LVIS,PhaseCut & 1.8M & 67.1 & 65.3 \\ DINO-Swin-L  & ✗ & ✗ & Swin-L & O365 & 0.66M & 68.8 & 67.3 \\  MQ-GLIP-T & ✓ & ✓ & Swin-T & O365\({}^{}\) & 0.66M & 64.8 & 62.5 \\   

Table 2: Results on the ODinW benchmark. The few-shot performance is evaluated by 3-shot .

    Ablated \\ Setting \\  } &  Ablated \\ Details \\  } &  MQ-GLIP-T \\ Original Value \\  } &  \(\) \\ Value \\  } &  \(\) MiniVal (\%) \\  } &  \(\)-13 (\%) \\ \(\) \\  } &  \(\)-13 (\%) \\ \(_{}\) \\  } &  \(\)-13 (\%) \\ \(_{}\) \\  } \\   \\   \\   \\   &  &  & 40\% & 0\% & 26.9 & 20.1 & 23.4 & 31.2 & 42.8 & 46.8 \\  & & & 40\% & 80\% & 28.9 & 20.4 & 24.2 & 34.6 & 45.0 & 48.3 \\   &  & Enable & ✓ & ✗ & 27.1 & 19.4 & 24.4 & 31.1 & 44.1 & 47.1 \\   & & & Architecture & MLP & Scaler & 29.3 & 18.3 & 26.3 & 34.0 & 43.4 & 46.0 \\   & & & MLP & Linear & 30.0 & 19.7 & 26.9 & 34.5 & 44.4 & 47.3 \\   & & & Input & \(\) & cat(\(,t\)) & 29.2 & 17.8 & 26.0 & 34.2 & 44.8 & 48.2 \\   &  & Full Model & ✓ & ✗ & 24.4 & 28.6 & 21.1 & 17.2 & 38.6 & 45.4 \\   & & & Text Encoder & ✓ & ✗ & 26.6 & 17.3 & 23.0 & 31.5 & 42.7 & 46.6 \\   

Table 3: Ablation results. We evaluate finetuning-free performance on LVIS and ODinW. Both average and median values on 13 datasets of ODinW are reported. Each row in this ablation study table should be compared to the baseline MQ-GLIP-T reported at the top of the table.

#### 3.2.2 MQ-Det as a strong few-shot learner

The experiments on ODinW with respect to finetuning-free, few-shot and full-shot settings are listed in Tab. 2, where MQ-Det shows robust performance on this challenging benchmark. We observe that most large-scale fundamental detection models only support language as category queries. With additional vision queries, MQ-GLIP-T averagely improves GLIP-T by +3.7% AP and +6.3% AP in the finetuning-free and few-shot settings respectively on ODinW-13, thus exhibiting the superiority of the rich description granularity of vision queries in our approach. Meanwhile, MQ-GLIP-T sets a new record on the ODinW-35 few-shot setting with 43.0% AP. Fig. 3 demonstrates few-shot performance with respect to different amounts of training data. We also notice that MQ-GLIP-T makes limited gains over GLIP-T with sufficient training data in the full-shot setting (_e.g._, +0.6% AP on ODinW-13). The reason is that the learning procedure, with sufficient training data, degenerates to a traditional close-set detection problem, where the model just represents categories as index regardless of textual or visual descriptions.

### Ablation studies

**Importance of the masked language prediction strategy.** We propose our masked language prediction strategy to address the learning inertia problem described in Sec. 2.3. Tab. 3 (a) verifies the effect of our pre-training strategy. We observe that our approach holds a great performance degradation (-3.5% AP) on LVIS with the mask rate of 0. MQ-GLIP-T with the mask rate of 0 only improves GLIP-T by +0.9% AP on LVIS. This indicates that our multi-modal queried MQ-GLIP may lose its effect and degenerate to language-queried GLIP without the masked language prediction strategy. Meanwhile, a too large mask rate (_e.g._, 80%) may lead to overfitting on the vision queries and losing attention to semantic-rich language queries, thus impairing performance.

**The role of vision queries**. MQ-Det provides users a flexible way to construct customized vision queries. We use one vision query for each category of LVIS in MQ-GLIP-T, and investigate the effect of different query qualities as shown in Fig. 4. The vision queries are selected based on the following criteria: **(i)** Positive queries (in red) contain complete information about the target objects, which significantly improve detection. **(ii)** Hard positive queries (in brown) only provide partial object information and sometimes include additional noise, such as the example of the boy and car. Despite the additional noise, MQ-Det can filter it out and still help improve the detection accuracy to some extent, identifying at least one object correctly. **(iii)** Negative queries (in green) contain too much noise and do not benefit detection, while **(iv)** "no vision queries" (in blue) is used as a baseline. Finally, we also use **(v)** mixed queries (in purple) that combine the three types of vision queries. The results show the robustness of MQ-Det, namely, our approach can select relevant information from miscellaneous vision queries to boost detection.

**Architecture design**. We design a plug-and-play GCP module so that visual details can be smoothly interleaved with highly-semantic textual cues. Particularly, in GCP, the conditional gating mechanism on each interleaved category token in Eqn. (2) ensures smooth model initialization from the frozen model and stable modulating according to different quality of visual cues. As shown in the first two

Figure 4: Finetuning-free detection with different vision queries. The language queries of 1,203 LVIS categories are always provided. Similar boxes are located while vision queries with better quality provide more accurate predictions.

Figure 3: Average precision (%) on ODinW-13, from finetuning-free to full-shot data. MQ-Det clearly improves GLIP on data efficiency.

rows of Tab. 3 (b), roughly adding visual cues to each text token, or equally scaling them using the same learnable gating scalar for all vision queries, can result in sub-optimal performance (-3.3% AP and -1.1% AP on LVIS, respectively). When using a simple gating design consisting of one linear layer over different vision queries, we obtain a slight performance drop compared to the non-linear MLP design, indicating content-aware gating mechanism is necessary and more parametric design helps analyze the quality of vision queries. In order to encourage the gate to balance the vision query \(\) and the text query \(t\) in Eqn. (2), we combine the two query types as a joint input (the last row of Tab. 3 (b)). However, we discover that this task rises the learning difficulty for a simple gate design.

**Freezing pre-trained detectors.** Tab. 3 (c) shows the necessity of modulating on the frozen models to introduce vision queries into the language-queried detectors. Either modulating the whole pre-trained detector or only modulating the text encoder with the limited detection data inevitably lose the generalization capability of the initial language-queried model, leading to a significant performance decrease (-6.0% AP and -3.8% AP on LVIS respectively). Moreover, training without freezing the detector requires almost twice the time consumption, not to mention the additional memory cost.

## 4 Related work

**Language-queried object detection.** Recently, vision-and-language approaches have gained popularity in visual recognition, where language is used to query visual objects and enable open-set ability. CLIP  and ALIGN  are pioneers in cross-modal image-level contrastive learning , where they train on millions of image-text pairs to perform open-vocabulary image classification. Inspired by these works, object detection has pursued cross-modal region-level contrastive learning in two main pipelines. One pipeline is open-vocabulary object detection (OVD) [46; 12; 54; 51; 30; 45], where the detection data is rigorously split into base classes for training and novel classes for evaluation. For instance, Zareian  propose OVR-CNN that first pre-trains a visual encoder on image-caption pairs to learn rich vocabulary and then finetunes on detection data with only base classes. Detic  learns object localization and fine-grained vision-language matching simultaneously using max-size proposals to assign image-level labels. However, this pipeline utilizes limited detection data and only generalizes to specific datasets, _e.g._, from COCO  base classes to COCO novel classes. In contrast, our approach follows the second pipeline of large-scale fundamental detection models [42; 25; 48; 20; 50], which aims to generalize to arbitrary categories with extensive region-level image-text data. MDETR  trains an end-to-end DETR  model via phrase-region matching on existing multi-modal datasets, providing a strong baseline for few-shot object detection. GLIP [25; 48] formulates object detection as a grounding problem and leverages additional grounding data for aligned semantics at phrase and region levels, achieving even stronger performance on fully-supervised detection benchmarks without finetuning. GroundingDINO  applies GLIP's grounded training strategy to a stronger detection backbone DINO , achieving state-of-the-art open-set detection performance. Since these models all use a text encoder like BERT  to model the language queries, our approach can seamlessly integrate vision queries with these language-queried models to enable rich visual granularity.

**Vision-queried methods** commonly exist in current two-branch based few-shot object detection pipelines [16; 11; 14; 15; 21]. These methods are based on a siamese network to process the target images and vision queries in parallel, and calculate the similarity between image regions (usually proposals) and few-shot examples for detection. Kang  propose a feature reweighting module to aggregate the target image features via few-shot vision queries. Meta Faster R-CNN  performs feature alignment between the two inputs and focus on foreground regions using attention. FCT  proposes an early fusion transformer architecture to fuse target images and vision queries. However, the two-branch pipeline in few-shot detection lacks instruction of language, and is far from practical implementation, _e.g._, no more than 10% AP in 3-shot COCO  for current SoTA methods [16; 15], not to mention more challenging LVIS  and ODinW  benchmarks. In addition, only a very few open-vocabulary object detection approaches support both language and vision queries. OV-DETR  proposes to utilize CLIP  embeddings from both image and text encoders as the object queries of the DETR  decoder. OWL-ViT  uses detection data to finetune upon CLIP. Thanks to the parallel structure of CLIP, OWL-ViT can conduct both region-text and region-image similarity. However, these two methods do not support language and vision queries as joint inputs, and show low performance with vision queries. Our MQ-Det considers the complementary nature of vision and language queries, thus achieving both open-set generalization and fine-grained recognition.

## 5 Conclusion

MQ-Det is an efficient pre-training architecture and strategy design to let vision-language (VL) detection foundation models accept text interleaved with images as the input queries. After modulated pre-training, MQ-Det shows promising gains on the finetuning-free and few-shot settings on well-established benchmarks and 35 downstream tasks. Our results suggest that utilizing the complementary nature of language and vision is an important step towards open-world object detection.

## 6 Acknowledgements

This work was supported by Key-Area Research and Development Program of Guangdong Province (2021B010140002), National Natural Science Foundation of China (No. 62036012, 62322212, 62072455, 61721004), and by Tencent Rhino-Bird Research Elite Program.