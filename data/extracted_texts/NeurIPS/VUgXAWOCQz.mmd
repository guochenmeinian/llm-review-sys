# Randomized algorithms and PAC bounds for inverse reinforcement learning in continuous spaces

Angeliki Kamoutsi

EPFL, Switzerland

angeliki.kamoutsi@epfl.ch

&Peter Schmitt-Forster

University of Konstanz, Germany

peter.schmitt-foerster@uni-konstanz.de Tobias Sutter

University of Konstanz, Germany

tob.sutter@uni-konstanz.de

&Volkan Cevher

EPFL, Switzerland

volkan.cevher@epfl.ch

&John Lygeros

ETH Zurich, Switzerland

jlygeros@ethz.ch

###### Abstract

This work studies discrete-time discounted Markov decision processes with continuous state and action spaces and addresses the inverse problem of inferring a cost function from observed optimal behavior. We first consider the case in which we have access to the entire expert policy and characterize the set of solutions to the inverse problem by using occupation measures, linear duality, and complementary slackness conditions. To avoid trivial solutions and ill-posedness, we introduce a natural linear normalization constraint. This results in an infinite-dimensional linear feasibility problem, prompting a thorough analysis of its properties. Next, we use linear function approximators and adopt a randomized approach, namely the scenario approach and related probabilistic feasibility guarantees, to derive \(\)-optimal solutions for the inverse problem. We further discuss the sample complexity for a desired approximation accuracy. Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples.

## 1 Introduction

In the standard reinforcement learning (RL) setting [1; 2; 3; 4], a cost signal is given to instruct agents on completing a desired task. However, oftentimes, it is either too challenging to optimize a given cost (e.g., due to sparsity), or it is prohibitively hard to manually engineer a cost function that induces complex and multi-faceted optimal behaviors. At the same time, in many real-world scenarios, encoding preferences using expert demonstrations is easy and provides an intuitive and human-centric interface for behavioral specification [5; 6; 7]. Considering the inverse reinforcement (IRL) problem involves deducing a cost function from observed optimal behavior. IRL is actively researched with applications in engineering, operations research, and biology [8; 9; 10]. There are two main motivations behind inverse decision-making. The first one concerns situations where the cost function is of interest by itself, e.g., for scientific inquiry, modeling of human and animal behavior [11; 12] or modeling of other cooperative or adversarial agents . The second one concerns the task of imitation or apprenticeship learning  by first recovering the expert's cost function and then using it to reproduce and synthesize the optimal behavior. For instance, in engineering, IRL can be used to explain and imitate the observed expert behavior, e.g., in the highway driving task [14; 15], parking lot navigation , and urban navigation . Other examples can be found in humanoid robotics and understanding of human locomotion . Despite extensive research efforts, our understanding of IRL still has significant limitations. One major gap lies in the absence of algorithms designed for continuous state and action spaces, which are crucial for numerous promising applications likeautonomous vehicles and robotics that operate in continuous environments. Most existing state-of-the-art IRL algorithms for the continuous setting often adopt a policy-matching approach instead of directly solving the IRL problem . However, this approach tends to provide a less robust representation of agent preferences , since the recovered policy is highly dependent on the environment dynamics. State-of-the-art IRL algorithms are empirically successful but lack formal guarantees. Theoretical assurances are crucial for practical implementation, especially in safety-critical systems with potential fatal consequences.

Contributions.This work deals with discrete-time Markov decision processes (MDPs) on continuous state and action spaces under the total expected discounted cost optimality criterion and studies the inverse problem of inferring a cost function from observed optimal behavior. Under the assumption that the control model is Lipschitz continuous, we propose an optimization-based framework to infer the cost function of an MDP given a generative model and traces of an optimal policy. Our approach is based on the linear programming (LP) approach to continuous MDPs , complementary slackness optimality conditions, related developments in randomized convex optimization  and uniform finite sample bounds from statistical learning theory .

To this aim, we first consider the case in which we have access to the entire optimal policy \(_{}\) and starting from the LP formulation of the MDP, we characterize the set of solutions to the inverse problem by using occupation measures, linear duality and complementary slackness conditions. This results in an infinite-dimensional linear feasibility problem. Although from a theoretical point of view our approach succeeds in characterizing inverse optimality in its full generality, in practice the following important challenges need to be addressed. First, the inverse problem is ill-conditioned and ill-posed, since each task is consistent with many cost functions. Thus a main challenge is coming up with a meaningful one. To this end, we enforce an additional natural linear normalization constraint in order to avoid trivial solutions and ill-posedness. Another challenge is the infinite-dimensionality of the LP formulation, which makes it computationally intractable. To alleviate this difficulty, we propose an approximation scheme that involves a restriction of the decision variables from an infinite-dimensional function space to a finite dimensional subspace (tightening), followed by the approximation of the infinitely-uncountably-many constraints by a finite subset (relaxation). In particular, we use linear function approximators and adopt a randomized approach, namely the scenario approach , and related probabilistic feasibility guarantees , to derive \(\)-optimal solutions for the inverse problem, as well as explicit sample complexity bounds for a desired approximation accuracy. Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples.

Related literature.Our principal aim is to address problems with uncountably infinitely many states and actions. Existing IRL algorithms treat the unknown cost function as a linear combination  or nonlinear function  of features. In particular, there are three broad categories of formulations. In _feature expectation matching_ one attempts to match the feature expectation of a policy to the expert, while in _maximum margin planning_ the goal is to learn mappings from features to cost functions so that the demonstrated policy is better than any other policy by a margin defined by a loss-function. Moreover, a _probabilistic approach_ is to interpret the cost function as parametrization of a policy class such that the true cost function maximizes the likelihood of observing the demonstrations . Most existing IRL methods that recover a cost function, are either designed exclusively for MDPs with finite state and action spaces, or rely on an oracle access to an RL solver which is used repeatedly in the inner loop of an iterative procedure. An exception are the works , that perform well in the experimental settings considered, without providing theoretical guarantees. Relying on oracle access to an RL solver is a significant computational burden for applying these methods to MDPs with continuous state and action spaces since solving a continuous MDP is a challenging and computationally expensive problem on its own. As a result, IRL over uncountable spaces remains largely unexplored. In this work we aim to contribute to this line of research and propose a method that avoids repeatedly solving the forward problem and simultaneously provides probabilistic performance guarantees on the quality of the recovered solution.

Linear duality and complementarity were first proposed in  for solving finite-dimensional inverse LPs. The idea was then extended to inverse conic optimization problems in  by using KKT optimality conditions. The fundamental difference between these works and the present paper is that they deal with finite-dimensional convex optimization programs where the agent has complete knowledge of the optimal behavior as a finite-dimensional vector. In our setting we have the additional difficulty of the infinite-dimensional and data-driven nature of the problem. In  the authors use occupation measures and complementarity in linear programming to formulate the inverse deterministic continuous-time optimal control problem. Under the assumption of polynomial dynamics and semi-algebraic state and input constraints, they propose an approximation scheme based on sum-of-squares semidefinite programming. Contrary to , we consider the problem of inverse discrete-time stochastic optimal control. In such a stochastic environment, assuming polynomial dynamics clearly is restrictive, excluding any setting with Gaussian noise, e.g., the LQG problem. Our approach is not limited to the case of polynomial dynamics and semi-algebraic constraints but is able to tackle the general case, while also providing performance guarantees as in .

Our work is closely related to the recent theoretical works on IRL [49; 50; 51; 52; 53; 54; 55; 56]. However, these papers consider either tabular MDPs [49; 50; 52; 53; 54; 55] or MDPs with continuous states and finite action spaces [51; 56]. In contrast, our contribution delves into the theoretical analysis of IRL in the intricate landscape of continuous state and action spaces. Notably, our framework, when applied to finite tabular MDPs and a stationary Markov expert policy \(_{}\), simplifies to the inverse feasibility set considered in [52; 53; 54] (see also Appx A.2). The methodology put forth in those studies, extends the LP formulation previously explored in [12; 49; 50; 51], which primarily dealt with deterministic expert policies of the form \(_{} a_{1}\). In our work, by using occupancy measures instead of policies and employing Lagrangian duality, we are able to characterize inverse feasibility for general continuous MDPs regardless of the complexity of the expert policy. Moreover, our framework empowers us to leverage offline expert demonstrations to compute an approximate feasibility set and recover a cost through a sample-based convex program. This flexibility surpasses previous theoretical IRL settings, where either \(_{}\) is assumed to be fully known [49; 51; 52] or active querying of \(_{}\) is possible for each state [53; 54]. Finally, our assumption of a Lipschitz MDP model is milder and more general than the infinite matrix representation considered in , thus accommodating a broader range of MDP models. Overall, we establish a link between our methodology and the existing body of literature on LP formulations for IRL, while also accounting for continuous states and action spaces and more general expert policies. Finally, we would like to highlight a key distinction between our work and recent theoretical IRL papers [52; 53; 54; 55]. Unlike these recent works, our study goes beyond the examination of the properties of the inverse feasibility set and its estimated variant. Our contribution extends to tackling the reward ambiguity problem, a well-known limitation of the IRL paradigm, and provides theoretical results in this direction. Additionally, our work introduces function approximation techniques that come with robust theoretical guarantees. Finally, we study how constraint sampling in infinite-dimensional LPs can be exploited to derive a single nearly optimal solution with probabilistic performance guarantees.

Basic definitions and notations.Let \((X,)\) be a _Borel space_, i.e., \(X\) is a Borel subset of a complete and separable \(\)-metric space, and let \((X)\) be its Borel \(\)-algebra. We denote by \((X)\) the Banach space of finite signed Borel measures on \(X\) equipped with the total variation norm and by \((X)\) the convex set of Borel probability measures. Let \(_{x}(X)\) be the Dirac measure centered at \(x X\). Measurability is always understood in the sense of Borel measurability. An open ball in \((X,)\) with radius \(r\) and center \(x_{0}\) is denoted by \(B_{r}(x_{0})=\{x X:(x,x_{0})<r\}\). Given a measurable function \(u:X\), its sup-norm is given by \(\|u\|_{}_{x X}|u(x)|\). Moreover, we define the Lipschitz semi-norm by \(|u|_{}_{x x^{}}\{)\|}{(x,x^{})}\}\) and the Lipschitz norm by \(\|u\|_{}\|u\|_{}+|u|_{}\). Let \((X)\) be the Banach space of real-valued bounded Lipschitz continuous functions on \(X\) together with the Lipschitz norm \(\|\|_{}\). Then, \(((X),(X))\) forms a _dual pair_ of vector spaces with duality brackets \(,u_{}u(x)\), for all \((X)\), \(u(X)\). Moreover, if \((X)_{+}\) is the convex cone of finite nonnegative Borel measures on \(X\), then its dual convex cone is the set \((X)_{+}\) of nonnegative bounded and Lipschitz continuous functions on \(X\). Under the additional assumption that \(X\) is compact, the _Wasserstein norm_\(\|\|_{}\) on \((X)\) is dual to the Lipschitz norm, i.e., \(\|\|_{}_{\|u\|_{} 1},u\). If \(X,Y\) are Borel spaces, a _stochastic kernel_ on \(X\) given \(Y\) is a function \(P(|):(X) Y\) such that \(P(|y)(X)\), for each fixed \(y Y\), and \(P(B|)\) is a measurable real-valued function on \(Y\), for each fixed \(B(X)\).

## 2 Markov decision processes and linear programming formulation

**Continuous Markov decision process.**  Consider a _Markov decision process_ (MDP) given by a tuple \(_{c}(,,,, _{0},c)\), where \(\) is a Borel space called the _state space_, \(\) is a Borel space called the _action space_, \(\) is a stochastic kernel on \(\) given \(\) called the _transition law_, \((0,1)\) is the _discount factor_, \(_{0}()\) is the _initial probability distribution_, and \(c:\) is the _cost function_. The model \(_{c}\) represents a controlled discrete-time stochastic system with initial state \(x_{0}_{0}()\). At time step \(t\), if the system is in state \(x_{t}=x\), and the action \(a_{t}=a\) is taken, then a corresponding cost \(c(x,a)\) is incurred, and the system moves to the next state \(x_{t+1}(|x,a)\). Once transition into the new state has occurred, a new action is chosen, and the process is repeated. A _stationary Markov policy_\(\) is a stochastic kernel on \(\) given \(\) and \((|x)()\) denotes the probability distribution of the action \(a_{t}\) taken at time \(t\), while being in state \(x\). We denote the space of stationary Markov policies by \(_{0}\). Given a policy \(\), we denote by \(^{}_{_{0}}\) the induced probability measure1 on the canonical sample space \(()^{}\), i.e., \(^{}_{_{0}}[]=[,x_{0}_{0}]\) is the probability of an event when following \(\) starting from \(x_{0}_{0}\). The expectation operator with respect to the trajectories generated by \(\) when \(x_{0}_{0}\), is denoted by \(^{}_{_{0}}\). If \(_{0}=_{x}\) for some \(x\), then we will write for brevity \(^{}_{x}\) and \(^{}_{x}\).

The optimal control problem we are interested in is 2

\[V^{}_{c}(_{0})_{_{0}}V^{}_{c}(_{0}),\] ( \[_{}\] )

where \(V^{}_{c}(_{0})^{}_{_{0}}[_{t=0}^{} ^{t}c(x_{t},a_{t})]\). A policy \(^{}\) is called _\(\)-discounted \(_{0}\)-optimal_ if \(V^{^{}}_{c}(_{0})=V^{}_{c}(_{0})\), and the _optimal value function_\(V^{}_{c}:\) is given by \(V^{}_{c}(x) V^{}_{c}(_{x})\). We impose the following assumptions on the MDP model which hold throughout the article. These are the usual continuity-compactness conditions , together with the Lipschitz continuity of the elements of the MDP; see, e.g., . We recall that the transition law \(\) acts on bounded measurable functions \(u:\) from the left as \(u(x,a)_{}u(y)(y|x,a)\), for all \((x,a)\).

**Assumption 2.1** (Lipschitz control model).:
* \(\) _and_ \(\) _are compact subsets of Euclidean spaces;_
* _the transition law_ \(\) _is_ weakly continuous, _meaning that_ \(u\) _is continuous on_ \(\)_, for every continuous function_ \(u:\)_. Moreover,_ \(\) _is Lipschitz continuous, i.e., there exists a constant_ \(L_{}>0\) _such that for all_ \((x,a),(y,b)\) _and all_ \(u()\)_, it holds that_ \(|u(x,a)-u(y,b)| L_{}|u|_{}(\|x-y\|_{2}+\|a-b\|_{2});\)__
* _the cost function_ \(c\) _is in_ \(()\) _with Lipschitz constant_ \(L_{c}>0\)_. That is, for all_ \((x,a),(y,b)\)_,_ \(|c(x,a)-c(y,b)| L_{c}(\|x-y\|_{2}+\|a-b\|_{2})\)_;_

Note that Assumption2.1 (A2) is fulfilled when the transition law \(\) has a density function \(f(y,x,a)\) that is Lipschitz continuous in \(y\) uniformly in \((x,a)\). This encompasses various probability distributions, such as the uniform, Gaussian, exponential, Beta, Gamma, and Laplace distributions, among others. Additionally, it applies to the infinite matrix representation considered in. Consequently, Assumption 2.1 accommodates a broad range of MDP models and allows for the consideration of smooth and continuous dynamics that reflect the characteristics of several real-world applications, such as robotics, or autonomous driving. Importantly, Assumption 2.1 ensures that the value function \(V^{}_{c}\) is in \(()\) and is uniquely characterized by the _Bellman optimality equation_\(V^{}_{c}(x)=_{a}\{c(x,a)+_{}V^{ }_{c}(y)(dy|x,a)\}\), for all \(x\)[60, Thm. 3.1] and .

Occupancy measures.For every policy \(\), we define the _occupancy measure_\(^{}_{_{0}}()_{+}\) by \(^{}_{_{0}}(E)_{t=0}^{}^{t}^{}_{ _{0}}[(x_{t},a_{t}) E]\), \(E()\). The occupancy measure can be interpreted as the discounted visitation frequency of the set \(E\) when acting according to policy \(\). The set of occupancy measures is characterized in terms of linear constraint satisfaction [62, Theorem 6.3.7]. To this end consider the convex set of measures, \(\{()_{+}:\ T_{}=_{0}\}\), where \(T_{}:()()\) is a linear and weakly continuous operator given by

\[(T_{})(B)(B)-_{ }(B|x,a)(\,(x,a)),\]for all \(B()\). Then, \(=\{_{_{0}}^{}:\ _{0}\}\). Moreover, \(_{_{0}}^{},c=V_{c}^{}(_{0})\), for every \(\).

**The linear programming approach.** A direct consequence is that (MDP\({}_{}\)) can be stated equivalently as an infinite-dimensional LP over measures

\[_{c}(_{0})_{( )_{+}}\{,c:\ T_{}=_{0}\}.\] (P \[{}_{}\] )

In particular the infimum in (P\({}_{}\)) is attained and \(^{}\) is optimal for (MDP\({}_{}\)) if and only if \(_{_{0}}^{^{}}\) is optimal for the primal LP (P\({}_{}\)). The dual LP of (P\({}_{}\)) is given by

\[_{c}^{}(_{0})_{u( )}\{_{0},u:\ c-T_{}^{}u 0\ \ \},\] (D \[{}_{}\] )

where the adjoint linear operator \(T_{}^{}:()( )\) of \(T_{}\) is given by

\[(T_{}^{}u)(x,a) u(x)-_{}u(y)(y|x,a).\]

Under Assumption 2.1, \(T_{}^{}\) is well-defined and the dual LP (D\({}_{}\)) is solvable, i.e., the supremum is attained, and strong duality holds. That is, \(_{c}(_{0})=_{c}^{}(_{0})=V_{c}^{}(_{0})\). In particular, the value function \(V_{c}^{}\) is an optimal solution for the dual LP (D\({}_{}\)). More details on the LP formulations for MDPs can be found in Appendix A.1.

## 3 Inverse reinforcement learning and characterization of solutions

We first define the _inverse reinforcement learning_ (IRL) problem and the _inverse feasibility set_.

**Definition 3.1** (IRL ).: _An IRL problem is a pair \((,_{})\), where \((,,,_{0},)\) is an MDP without cost function and \(_{}\) is an observed expert policy. We say that \(c()\) is inverse feasible for \(\), if \(_{}\) is a \(\)-discount \(_{0}\)-optimal policy for (MDP\({}_{}\)) with cost \(c\). The set of all \(c()\) that are inverse feasible is called the inverse feasibility set and is denoted by \((_{})\)._

Next, we use the primal-dual LP approach to MDPs and complementary slackness to characterize \((_{})\). To this end, we first define the _\(\)-inverse feasibility set \(^{}(_{})\)_.

**Definition 3.2**.: _Let \( 0\). We say that a cost function \(c\) is \(\)-inverse feasible for \(=(,_{})\) and denote \(c^{}(_{})\) if and only if, \(c()\) and there exists \(u()\) such that_

\[\{_{_{0}}^{_{}},c-T_{}^{ }u&,\\ c-T_{}^{}u&-,\ \  ..\] (1)

We are now ready to characterize the solutions to IRL, following arguments from .

**Theorem 3.1** (Inverse feasibility set characterization).: _Let \(_{}\). Under Assumption 2.1 on the Markov decision model \(_{{}_{C}}\) the following assertions are equivalent_

1. \(c^{0}(_{})\)_;_
2. \(c_{>0}^{}(_{})\)_;_3. \(_{}\) _is_ \(\)_-discount_ \(_{0}\)_-optimal for_ (_MDP_\({}_{}\)_) _with cost function_ \(c\)_._

_As a consequence, \((_{})=^{0}(_{})=_{> 0}^{}(_{})\). Moreover, \((_{})\) is a convex cone and \(\|\|_{}\)-closed in \(()\)._

As a result, a cost function is inverse feasible for \(=(,_{})\) if and only if it is \(\)-inverse feasible for all \(>0\). The characterization of the inverse feasibility set is due to linear duality and complementary slackness conditions. In particular, the constraint that holds pointwise in (19) is due to dual feasibility while the constraint that holds in expectation is due to strong duality, The details are provided in the proof of Theorem 3.1 in Appendix B.1.

Notably, when \(\) and \(\) are finite, and the expert policy \(_{}\) is stationary Markov, our formulation aligns with the finite-dimensional inverse feasibility set introduced in . Furthermore, when the expert is deterministic of the form \(_{}(x) a_{1}\), for all \(x\), then we recover the linear programs discussed in  (see Appendix A.2).

Using occupancy measures instead of policies, we can assess inverse feasibility for continuous MDPs, regardless of expert policy complexity. This approach allows us to utilize offline expert demonstrations for computing an approximate feasibility set and deriving costs via a sample-based convex program This flexibility surpasses previous theoretical settings, where either \(_{}\) is assumed to be fully known and deterministic  or active querying of \(_{}\) is possible for each state .

**Proposition 3.1** (\(\)-inverse feasibility set characterization).: _Under Assumption 2.1, for any \(>0\), it holds that a cost function \(\) is in \(^{}(_{})\) if and only if \(_{}\) is \(\)-optimal for (MDP\({}_{}\)) with cost \(\)._

As \( 0\), the next proposition indicates a close approximation to the inverse problem solution.

**Proposition 3.2**.: _Let \((_{n})_{n}\) be a sequence such that \(_{n}_{n}=0\) and let \(c_{n}^{_{n}}(_{})\). Then, every accumulation point \(c\) of the sequence \((c_{n})_{n}\) is inverse feasible, i.e., \(c(_{})\)._

Finally we show that the \(\)-inverse feasibility set \(^{}(_{})\) satisfies the \(\)-optimality criterion considered in ; see for example [53, Def. 2].

**Proposition 3.3**.: _Let \(>0\). It holds that \(_{c(_{})}V_{c}^{}(_{0})-V_{c}^{_{}}(_{0})\), for all \(^{}(_{})\), where \(\) is an optimal policy for the recovered cost \(\)._

This condition ensures that when \(\) is small we avoid an unnecessarily large _approximate_ feasibility set since there is a possible true cost in \((_{})\) with a small error for every possible recovered cost function in \(^{}(_{})\). 3

## 4 Towards recovering a nearly optimal cost function

Although we characterized the inverse and \(\)-inverse feasibility sets in Theorem 3.1 and Proposition 3.1 respectively, it is not clear yet how to compute them, as (19) is an infinite-dimensional feasibility LP. In practice, the following challenges need to be addressed:

1. The inverse problem is ill-conditioned and ill-posed since each task is consistent with many cost functions, and thus a central challenge is to end up with a meaningful one. To avoid trivial solutions, in Section 4.1 we motivate the addition of a linear normalization constraint.
2. Another challenge appears because the LP formulation is infinite-dimensional, hence computationally intractable. In Section 4.2 we address this problem by proposing a tractable approximation method with probabilistic performance bounds.
3. In practice, complete knowledge of \(_{}\) and \(\) is often unavailable. In Section 4.3, we tackle this challenge by assuming that we have access to a finite set of traces of the expert policy and a _generative-model oracle_. We use empirical counterparts of \(_{}\) and \(\) and provide error bounds to quantify our approach's accuracy with sample data.

The main building blocks of our methodology are depicted in Figure 2.

### Normalization constraint

A well-known limitation of IRL is that it suffers from the _ambiguity_ issue, i.e., the problem admits infinitely many solutions. For example, any constant cost function, including the zero cost, is inverse feasible. In addition, as it is apparent from the characterization of \((_{})=^{0}(_{})\) in Theorem 3.1, for any \(u()\) and \(c(_{})\), the cost \(c+T_{}^{*}u\) is inverse feasible. This phenomenon, also known as _reward shaping_ refers to the modification or design of a reward function to provide additional guidance or incentives to an agent during learning. In addition, since \((_{})\) is a convex cone and closed for the sup-norm (Theorem 3.1) the set of solutions to IRL is closed to convex combinations and uniform limits.

All these examples illustrate that the inverse feasibility set \((_{})\) contains some solutions that arise from mathematical artifacts. To mitigate this difficulty and avoid trivial solutions, inspired by , we enforce the additional natural normalization constraint \(_{}(c-T_{}^{*}u)(x,a)\,(x,a)=1\). The following proposition justifies this choice.

**Proposition 4.1**.: _If Definition 3.2 of \((_{})=^{0}(_{})\) includes the normalization constraint \(_{}(c-T_{}^{*}u)xa=1\), then all constant cost functions are excluded from the inverse feasibility set._

Alternatively, it is possible to employ additional heuristics to narrow down the set of possible solutions and incorporate prior knowledge, e.g., by restricting the class where the true cost belongs, constraining the dependence between costs and value functions, and enforcing conic constraints or shape conditions.

It is worth mentioning that the normalization constraint in our formulation primarily aims to mitigate the ill-posedness, or ambiguity issue, intrinsic to the IRL problem, rather than to resolve issues of identifiability. In particular, we state and prove that the normalization constraint rules out a wide class of trivial solutions, i.e., all constant functions and inverse solutions of the form \(c=T_{}^{*}u\), an outcome devoid of physical meaning and a mathematical artifact. While the identifiability problem and the ill-posedness problem are related in IRL, they are not the same. Identifiability deals with the uniqueness of the true cost function and cannot be fully resolved unless, for example, one has access to multiple expert policies or environments for comparison [64; 65]. On the other hand, ill-posedness is a broader concept from mathematical and statistical problems and refers to situations where a problem does not satisfy the conditions for being well-posed, e.g., in our case due to infinitely many solutions. Note that, unlike recent theoretical IRL works [52; 53; 54] which avoid discussing this issue altogether, we attempt to address the ambiguity problem and provide theoretical results in this direction, hoping to lay the foundations for overcoming current limitations.

### The case of known dynamics and expert policy

We first consider the case where the expert policy \(_{}\), the induced occupation measure \(_{_{0}}^{_{}}\) and the transition law \(\) are known. We leverage developments in randomized convex optimization, leading to an approximation scheme with a priori performance guarantees. As a first step, we introduce a semi-infinite convex formulation that enforces the normalization constraint, involves a restriction of the decision variables from an infinite-dimensional function space to a finite-dimensional subspace, and considers an additional norm constraint that effectively acts as a regularizer. The resulting

Figure 2: Main building blocks of our methodologyregularized semi-infinite _inner approximation_, which we call _inverse program_ is given by

\[\{_{,,}&\\ &_{_{0}}^{_{}},c-T_{}^{*}u ,\\ &c(x,a)-T_{}^{*}u(x,a)-,\ \,(x,a) ,\\ &_{}(c-T_{}^{*}u(x),a)\,(x,a )=1,\\ &c_{n_{c}},u_{n_{u}}, 0,.\] (IP)

where \(_{n_{c}}\{_{j=1}^{n_{c}}_{j}c_{j}:\ =\{_{i}\}_{i=1}^{n_{c}}^{n_{c}} \,,\|\|_{1}\}\) with \(\{c_{i}\}_{i=1}^{n_{c}}()\) being linearly independent basis functions with Lipschitz constant \(L_{c}>0\), \(_{n_{u}}\{_{i=1}^{n_{u}}_{i}u_{i}:\ =\{_{i}\}_{i=1}^{n_{u}} ^{n_{u}}\), \(\|\|_{1}\}\), with \(\{u_{i}\}_{i=1}^{n_{u}}()\) being linearly independent basis functions with Lipschitz constant \(L_{u}>0\), and \(>0\) is an appropriately chosen regularization parameter.

Note that (IP) is derived by relaxing the constraints in the inverse feasibility set \((_{})\) and paying a penalty when violated. In particular, let \((,,)\) be an optimal solution of the semi-infinite program (IP). Then, \(_{i=1}^{n_{c}}_{c}i^{ }(_{})\), from where it becomes apparent that the smaller the value of \(\), the better the quality of the extracted cost function \(\) (as by Proposition 3.1). One would intuitively expect that \(\) depends on the choice of basis functions for the cost (resp. value) function as well as on the parameters \(n_{c}\) (resp. \(n_{u}\)) and \(\). This dependency is highlighted by the following proposition.

**Proposition 4.2** (Basis function dependency).: _Let \(_{}\) be an optimal policy for the optimal control problem \(_{^{*}}\) with cost \(c^{*}\) and let \(u^{}\) be the corresponding optimal value function. Under Assumption 2.1, if \(u_{1} 1\) and \(>}\), then \(_{}\) with_

\[_{}:=\!(+_{ ,}(2+)\{1,L_{},d\})(_{c^{} _{n_{c}}}\|c^{}-c\|_{}+_{u_{n_{u} }}\|u^{}-u\|_{}),\] (2)

_where \(d=()\) is the Lebesgue measure of \(\), \(_{,}+K_{u,})}{ (1-)^{2}\{1,d\}+-1}\), with constants \(K_{c,}_{i=1,,n_{c}}\|c_{i}\|_{}\) and \(K_{u,}_{j=1,,n_{u}}\|u_{i}\|_{}\)._

Proposition 4.2 sheds light on how the choice of basis functions and the regularization parameter \(\) influence the approximation error. Essentially, the approximation error term \(_{}\) measures the expressiveness of the linear function approximators. Prior knowledge about the properties of the true cost allows us to choose appropriate basis functions to make the projection residuals in the theorem sufficiently small. For example, if the true cost function is known to be smooth, Fourier or polynomial basis functions can be used. In general, if we choose linearly dense bases in \(()\) and \(()\), then the projection residuals and so \(\) tend to \(0\) as \(n_{c}\) and \(n_{u}\) and the regularization parameter \(\) tend to infinity. In particular note that when \(c^{}_{n_{c}}\) and \(u^{}_{n_{u}}\), then the corresponding projection residuals are \(0\), and thus \(=0\) as expected. In a practical setting, observing a large value of \(\) is an indicator that more basis functions are needed.

Finally, note that the regularizer helps to bound the dual optimizer using a dual norm, thus offering an explicit approximation error for the proposed solution (see Appendix B.6).

Computationally tractable approximations to the semi-infinite convex program can be obtained through the _scenario approach_ in which randomization over the set of constraints is considered. In particular, we treat the parameter \((x,a)\) as an uncertainty parameter living in the space \(\). Let \(\) be a Borel probability measure on \((,())\), where \(\) is equipped with the norm \(\|(x,a)\|=\|x\|_{2}+\|a\|_{2}\). We assume that \(\) has the following structure.

**Assumption 4.1** (Sampling distribution).: _There exists \(g:_{+}\) strictly increasing, such that \((B_{r}(x,a))>g(r)\), for all \((x,a)\) and \(r>0\)._

Assumption 4.1 is a sufficient structural assumption concerning the sample distribution \(\) ensuring that the gap between the robust program (IP) and its sampled counterpart (\(_{}\)) can be controlled. It implicitly restricts the state and action spaces to be bounded.

Let \(\{(x^{()},a^{()})\}_{=1}^{N}\) be independent and identically distributed (i.i.d.) samples drawn from \(\) according to \(\). We are interested in the following random finite-dimensional convex program:

\[\{_{,,}&\\ &_{_{0}}^{_{}},c-T_{}^{*}u ,\\ &c(x^{()},a^{()})-T_{}^{*}u(x^{()},a^{()})-, \ \,=1, N,\\ &_{}(c(x,a)-T_{}^{*}u(x,a))\,(x,a )=1,\\ &c_{n_{c}},u_{n_{u}}, 0..\] (SIPNotice that (\(}\)) naturally represents a randomized program as it depends on the random multi-sample \(\{(x^{(i)},a^{(i)})\}_{i=1}^{N}\). We assume the following measurability assumption holds for our analysis.

**Assumption 4.2**.: _The (\(}\)) optimizer generates a Borel measurable mapping that associates each multi-sample \(\{(x^{()},a^{()})\}_{=1}^{N}\) to a uniquely defined optimizer \((_{N},_{N},_{N})\)._

To ensure uniqueness when multiple solutions exist, use a _tie-break rule_, such as selecting the solution with the minimum \(\|\|_{2}\) norm. It has been shown [30, Proposition 3.10] that applying such a tie-break-rule also ensures the measurability in Assumption 4.2.

The appealing feature of (\(}\)) is that it is a convex finite-dimensional program and so it can be solved at low computational cost for small enough \(N\). We study how many samples are needed for a _good solution_ by examining the _generalization properties_ of the optimizer \((_{N},_{N},_{N})\) and the extracted cost function \(N=_{i=1}^{n_{c}}N_{i}c_{i}\). For each \(n\), \((0,1)\) and \((0,1)\), we define

\[(n,,)=\!N:_{i=0}^{n} {N}{i}^{i}(1-)^{N-i}}.\]

The sample size above asymptotically scales as \(\{1/,\ (1/),\ n\}\), see . The following theorem determines the sample complexity of (\(}\)). In particular, for a given reliability parameter \((0,1)\) and confidence level \((0,1)\), we establish how many samples are needed to guarantee with confidence at least \(1-\) that \(_{N}^{e_{}+}(_{})\).

**Theorem 4.1** (Scenario program guarantees).: _Let \(,(0,1)\). Under Assumptions 2.1, 4.1 and 4.2, if \(u_{1} 1\) and \(>()}\), then by sampling \(N(n_{c}+n_{u}+1,g(}),)\) constraints, where \(L_{}}L_{c}+}(L_{u}L_{ }+L_{u})\), with probability at least \(1-\), \(_{N}^{e_{}+}(_{})\)._

**Remark 4.1** (Curse of dimensionality).: As shown in , the function \(g(r)\) is of order \(r^{()}\). As a result, the number of samples grows exponentially as \(^{-()}\). A similar exponential dependence to the dimension of the state space has been established in . Considering the current performance of general LP solvers, this approach is attractive for small to medium-sized problems. As noted in , dealing with the general \(d\)-dimensional case without exponential scaling in \(d\) is challenging. Therefore, understanding the selection of a suitable distribution for future sample drawing is crucial.

**Remark 4.2** (\(l_{1}\)-regularization).: To cut down on required samples \(N\), a common method is using \(l_{1}\)-regularization to reduce the effective dimension of the optimization variable. This concept is formalized in the current setting . Moreover, in the spirit of the compressed sensing literature , \(l_{1}\)-regularization will promote sparse solutions and hence lead to "simple" cost functions. In the context of optimal control \(l_{1}\)-regularization term is studied as the so-called "maximum hands-off control" paradigm [69; 70]. In our case, the utilization of the \(l_{1}\)-norm offers two primary advantages. Firstly, the \(l_{1}\)-norm promotes sparsity in solutions, thereby potentially reducing computational demands. Secondly, this specific type of regularization preserves the linearity of the program.

### Sample-based inverse reinforcement learning

In this section, we explore the realistic scenario where we lack access to the expert policy \(_{}\) and the transition law \(\). The learner only receives a finite set of truncated expert sample trajectories and cannot interact or query the expert for additional data during training. Despite the unknown MDP model, we assume access to a _generative-model oracle_. It provides the next state \(x^{}\) given a state-action pair \((x,a)\) sampled from \((|x,a)\). This is known as the simulator-defined MDP [71; 72].

**Sampling process.** Let \(=\{_{i}=(x_{0}^{i},a_{0}^{i},,x_{H}^{i},a_{H}^{i})\}_{i=1}^{m}\) be i.i.d., truncated sample trajectories according to \(_{}\). For any \(c()\), we consider the sample average approximation of the expectation \(_{_{0}}^{_{}},c\) given by, \(}^{_{}},c}() {1}{m}_{t=0}^{H}_{j=1}^{m}^{t}c(x_{t}^{j},a_{t}^{j})\). Similarly, if \(=\{x_{0}^{k}\}_{k=1}^{n}\) are i.i.d., samples according to \(_{0}\), for any \(u()\), we define the corresponding sample average estimation of the expectation \(_{0},u\) by \(,u}()_{k=1}^{n}u(x_{0 }^{k})\). Moreover, let \(=\{(x^{(l)},a^{(l)})\}_{l=1}^{N}\) be i.i.d. samples drawn from \(\) according to \(()\). We also use the following notation \(_{}^{*}u(x^{(l)},a^{(l)},y^{(l)}) u(x^{(l)})-_{i=1}^{k}u(y_{i}^{(l)}),\{y_{i}^{(l)}\}_{i=1}^{k}}{}(|x^{(l)},a^{(l)})\)We are interested in the finite-sample analysis of the following random convex program:

\[_{,,}&\\ &_{c}}^{n_{b}},c}()- ,u}(),\\ &c(x^{(l)},a^{(l)})-_{*}^{*}u(x^{(l)},a^{(l)},y^{(l)})- ,\ \,l=1, N,\\ &_{[n_{u}]},_{[n_{c}]},\\ &c_{n_{c}},u_{n_{u}}, 0,\]

where the function classes \(_{n_{c}},_{n_{u}}\) are defined as in the previous Section. We make the following measurability assumption which is analogous to Assumption 4.2.

**Assumption 4.3**.: _The (\(_{,,,}\)) optimizer generates a Borel measurable mapping that associates each multi-sample \((y,,,)\) to a uniquely defined optimizer \((_{,m,n,k},_{N,m,n,k},_{ N,m,n,k})\)._

**Theorem 4.2**.: _Under Assumptions 2.1, 4.1 and 4.3, if \(u_{1} 1\) and \(>^{2}^{2}n_{u}(}{2})} {^{2}}\), \(m^{2}^{2}n_{c}()}{(1- )^{2}^{2}}\), expert samples with horizon \(H=()\), and \(k^{2}(N/}{ ^{2}})}{^{2}}\) calls to the generative model per constraint, with probability at least \(1-\), \(_{N,m,n,k}^{_{}+}( _{})\). The constants \(K_{c,}\) and \(K_{u,}\) are given as \(K_{c,}_{i=1,,n_{c}}\|c_{i}\|_{}\) and \(K_{u,}_{j=1,,n_{u}}\|u_{i}\|_{}\)._

Theorem 4.2 provides explicit sample complexity bounds for achieving a desired approximation accuracy with our proposed randomized algorithm. For continous MDPs when we use \(n_{u}\) basis functions for the value function and \(n_{c}\) basis functions for the cost function we need \(m=((}{2})}{(1-)^{2 }^{2}})\) expert samples and \(K=((N}{^{2}}) }{^{2}})\) calls to the generative model per constraint and \(N=(^{dim(X A)})\) sampled constraints and solve the resulted sampled finite LP with \(n_{u}+n_{c}\) variables and \(N\) constraints in polynomial time to learn a cost that is \(+_{}\)-inverse feasible with probability \(1-\).

The corresponding sample complexities include the expert sample complexity \(m\), the number of calls to the generator per constraint \(k\), and the number of sampled constraints \(N\). The first two complexities scale gracefully with respect to the problem parameters, whereas the number of sampled constraints scales exponentially with the dimension of the state and action spaces (see also Remark 4.1). This makes the algorithm particularly suitable for low-dimensional problems of practical interest, e.g., pendulum swing-up control, vehicle cruise control, and quadcopter stabilization. Note that a similar exponential dependence to the dimension of the state space has been established in Dexter et al. , a theoretical study addressing IRL in continuous state but discrete action spaces.

A promising research direction is to enhance the sample complexity bounds through the utilization of the underlying problem structure . In addition, it becomes imperative to gain an understanding regarding the selection of a suitable distribution for drawing samples in the future. Intuitively, it is reasonable to anticipate that certain regions within the state-action space carry more "informative" characteristics than others. One conjecture is that sampling constraints based on the expert occupancy measure could yield a more scalable bound . However, a comprehensive mathematical treatment of these inquiries will be addressed in future research endeavors.

**Redunction to tabular MDPs.** For tabular MDPs, offline access to expert for tabular MDPs, and a generative model we require \(m=()}{(1-)^{2} ^{2}})\) expert samples, and \(K|X||A|=(|A|(})}{ ^{2}})\) calls to the generative model, and solve the resulted sampled finite LP with \(|X|+|X|\) variables and \(|X||A|+2\) constraints in polynomial time, to learn a cost function that is \(\)-inverse feasible with probability \(1-\).

Note that as we argued in detail above our setting is different from the one in [52-54] since we have offline access to the expert and address a different question, i.e. learning a single reward with formal guarantees in continuous MDPs. In this case, there is no need to solve the resulting linear program.

**Numerical Experiment.** In Appendix C, we illustrate our method with a simple truncated Linear Quadratic Regulator (LQR) example to provide better intuition about the method and the proposed sample complexity bounds.