# Rank-N-Contrast: Learning Continuous Representations for Regression

Kaiwen Zha\({}^{1,}\) Peng Cao\({}^{1,}\)1 Jeany Son\({}^{2}\) Yuzhe Yang\({}^{1}\) Dina Katabi\({}^{1}\)

\({}^{1}\)MIT CSAIL \({}^{2}\)GIST

###### Abstract

Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a _regression-aware representation_. Consequently, the learned representations exhibit fragmentation and fail to capture the _continuous_ nature of sample orders, inducing suboptimal results across a wide range of regression tasks. To fill the gap, we propose Rank-N-Contrast (Rnc), a framework that learns continuous representations for regression by _contrasting_ samples against each other based on their _rankings_ in the target space. We demonstrate, theoretically and empirically, that Rnc guarantees the desired order of learned representations in accordance with the target orders, enjoying not only better performance but also significantly improved robustness, efficiency, and generalization. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify that Rnc achieves state-of-the-art performance, highlighting its intriguing properties including better data efficiency, robustness to spurious targets and data corruptions, and generalization to distribution shifts. Code is available at: https://github.com/Kaiwenzha/Rank-N-Contrast.

## 1 Introduction

Regression problems are pervasive and fundamental in the real world, spanning various tasks and domains including estimating age from human appearance , predicting health scores via human physiological signals , and detecting gaze directions using webcam images . Due to the

Figure 1: **Learned representations of different methods on a real-world temperature regression task** (details in Sec. 5). Existing general regression learning (\(L_{1}\)) or representation learning (SupCon) schemes fail to recognize the underlying continuous information in data. In contrast, Rnc learns _continuous_ representations that capture the intrinsic sample orders w.r.t. the regression targets.

continuity in regression targets, the most widely adopted approach for training regression models is to directly predict the target value and employ a distance-based loss function, such as \(L_{1}\) or \(L_{2}\) distance, between the prediction and the ground-truth target [51; 52; 38]. Methods that tackle regression tasks using classification models trained with cross-entropy loss have also been studied [36; 33; 40].

However, previous methods focus on imposing constraints on the _final_ predictions in an end-to-end fashion, but do not explicitly emphasize the _representations_ learned by the model. Unfortunately, these representations are often fragmented and incapable of capturing the _continuous_ relationships that underlie regression tasks. Fig. 1(a) highlights the representations learned by the \(L_{1}\) loss on SkyFinder , a regression dataset for predicting weather temperature from webcam outdoor images captured at different locations (details in Sec. 5). Rather than exhibiting the continuous ground-truth temperatures, the learned representations are grouped by _different webcams_ in a fragmented manner. Such unordered and fragmented representation is suboptimal for the regression task and can even hamper performance by including irrelevant information, such as the capturing webcam.

Furthermore, despite the great success of representation learning schemes on solving _discrete_ classification or segmentation tasks (e.g., contrastive learning [4; 20] and supervised contrastive learning (SupCon) ), less attention has been paid to designing algorithms that capture the intrinsic _continuity_ in data for regression. Interestingly, we highlight that existing representation learning methods inevitably overlook the continuous nature in data: Fig. 1(b) shows the representation learned by SupCon on the SkyFinder dataset, where it again fails to capture the underlying continuous order between the samples, resulting in a suboptimal representation for regression tasks.

To fill the gap, we present Rank-n-Contrast (RnC), a novel framework for generic regression learning. RnC first learns a _regression-aware_ representation that orders the distances in the embedding space based on the target values, and then leverages it to predict the continuous targets. To achieve this, we propose the Rank-n-Contrast loss (\(_{}\)), which _ranks_ the samples in a batch according to their labels and then _contrasts_ them against each other based on their relative rankings. Theoretically, we prove that optimizing \(_{}\) results in features that are ordered according to the continuous labels, leading to improved performance in downstream regression tasks. As confirmed in Fig. 1(c), RnC learns continuous representations that capture the intrinsic ordered relationships between samples. Notably, our framework is orthogonal to existing regression methods, allowing for the use of any regression method to map the learned representation to the final prediction values.

To support practical evaluations, we benchmark RnC against state-of-the-art (SOTA) regression and representation learning schemes on five real-world regression datasets that span computer vision, human-computer interaction, and healthcare. Rigorous experiments verify the superior performance, robustness, and efficiency of RnC on learning continuous targets. Our contributions are as follows:

* We identify the limitation of current regression and representation learning methods for continuous targets, and uncover intrinsic properties of learning regression-aware representations.
* We design RnC, a simple & effective method that learns continuous representations for regression.
* We conduct extensive experiments on five diverse regression datasets in _vision_, _human-computer interaction_, and _healthcare_, verifying the superior performance of RnC against SOTA schemes.
* Further analyses reveal intriguing properties of RnC on its data efficiency, robustness to spurious targets & data corruptions, and better generalization to unseen targets.

## 2 Related Work

**Regression Learning.** Deep learning has achieved great success in addressing regression tasks [36; 51; 52; 38; 11]. In regression learning, the final predictions of the model are typically trained in an end-to-end manner to be close to the targets. Standard regression losses include the \(L_{1}\) loss, the mean squared error (MSE) loss, and the Huber loss . Past work has also proposed several variants. One branch of work [36; 13; 14; 35] divides the regression range into small bins, converting the problem into a classification task. Another line of work [12; 2; 40] casts regression as an ordinal classification problem  using ordered thresholds and employing multiple binary classifiers. Recently, a line of work proposes to regularize the embedding space for regression, ranging from modeling feature space uncertainty , encouraging higher-entropy feature spaces , to regularizing features for imbalanced regression [44; 17]. In contrast to existing works, we provide a regression-aware representation learning approach that emphasizes the continuity in the features space w.r.t. the targets, which enjoys better performance while being compatible to prior regression schemes. In addition,C-Mixup  adapts the original mixup  by adjusting the sampling probability of the mixed pairs according to the target similarities. It is also worth noting that our method is orthogonal and complementary to data augmentation algorithms for regression learning, such as C-Mixup .

**Representation Learning.** Representation learning is crucial in machine learning, often studied in the context of classification. Recently, contrastive learning has emerged as a popular technique for self-supervised representation learning [4; 20; 5; 8; 3]. The supervised version of contrastive learning, SupCon , has been shown to outperform the conventional cross-entropy loss on multiple discrete classification tasks, including image recognition , noisy labels , long-tailed classification [23; 27], and out-of-domain detection . A few recent papers propose to adapt SupCon to tackle ordered labels in specific downstream applications, including gaze estimation , medical imaging [9; 10], and neural behavior analysis . Different from prior works that directly adapt SupCon, we incorporate the intrinsic property of label continuity for designing representation learning scheme tailored for regression, which offers a simple and principled approach for generic regression tasks.

## 3 Our Approach: Rank-N-Contrast (RNC)

**Problem Setup.** Given a regression task, we aim to train a neural network model composed of a feature encoder \(f():X^{d_{c}}\) and a predictor \(g():^{d_{c}}^{d_{t}}\) to predict the target \(^{d_{t}}\) based on the input data \( X\).

For a positive integer \(I\), let \([I]\) denote the set \(\{1,2,,I\}\). Given a randomly sampled batch of \(N\) input and label pairs \(\{(_{n},_{n})\}_{n[N]}\), we apply standard data augmentations to obtain a two-view batch \(\{(}_{},}_{})\}_{[2N]}\), where \(}_{2n}=t(_{n})\) and \(}_{2n-1}=t^{}(_{n})\), with \(t\) and \(t^{}\) being independently sampled augmentation operations, and \(}_{2n}=}_{2n-1}=_{n}\), \( n[N]\). The augmented batch is then fed into the encoder \(f\) to obtain the feature embedding for each augmented input data, i.e., \(_{l}=f(}_{l})^{d_{c}}, l[2N]\). The representation learning phase is then performed over the feature embeddings. To harness the acquired representation for regression, we freeze the encoder \(f()\) and train the predictor \(g()\) on top of it using a regression loss (e.g., \(L_{1}\) loss).

In this context, a natural question arises: _How to design a regression-aware representation learning scheme tailored for continuous and ordered samples?_

**The Rank-N-Contrast Loss.** In order to align distances in the embedding space ordered by distances in their labels, we propose Rank-N-Contrast loss (\(_{}\)), which first _**ranks**_ the samples according to their target distances, and then _**contrasts**_ them against each other based on their relative rankings.

Following [16; 45], given an anchor \(_{i}\), we model the likelihood of any other \(_{j}\) to increase exponentially with respect to their similarity in the representation space . Inspired by the listwise ranking methods [43; 6], we introduce \(_{i,j}:=\{_{k} k i,d(}_{i}, }_{k}) d(}_{i},}_{j})\}\) to denote the set of samples that are of higher **ranks** than \(_{j}\) in terms of label distance w.r.t. \(_{i}\), where \(d(,)\) is the distance measure between two labels (e.g., \(L_{1}\) distance). Then the normalized likelihood of \(_{j}\) given \(_{i}\) and \(_{i,j}\) can be written as

\[(_{j}|_{i},_{i,j})=_{i },_{j})/)}{_{_{k}_{i,j}}((_{i },_{k})/)},\] (1)

Figure 2: **Illustration of \(_{}\) in the context of positive and negative pairs.****(a)** An example batch of input data and their labels. **(b)** Two example positive pairs and corresponding negative pair(s) when the anchor is the 20-year-old man (shown in gray shading). When the anchor forms a positive pair with a 30-year-old man, their label distance is 10, hence the corresponding negative samples are the 0-year-old baby and the 70-year-old man, whose label distances to the anchor are larger than 10. When the 0-year-old baby creates a positive pair with the anchor, only the 70-year-old man has a larger label distance to the anchor, thus serving as a negative sample.

where \((,)\) is the similarity measure between two feature embeddings (e.g., negative \(L_{2}\) norm) and \(\) denotes the temperature parameter. Note that the denominator is a sum over the set of samples that possess _higher ranks_ than \(_{j}\); Maximizing \((_{j}|_{i},_{i,j})\) effectively increases the probability that \(_{j}\) outperforms the other samples in the set and emerges at the top rank within \(_{i,j}\).

As a result, we define the per-sample RnC loss as the average negative log-likelihood over all other samples in a given batch:

\[l_{}^{(i)}=_{j=1,\ j i}^{2N}-_{i},_{j})/)}{_{_{k}_{i,j}}( (_{i},_{k})/)}.\] (2)

Intuitively, for an anchor sample \(i\), _any_ other sample \(j\) in the batch is **contrasted** with it, enforcing the feature similarity between \(i\) and \(j\) to be larger than that of \(i\) and any other sample \(k\) in the batch, if the label distance between \(i\) and \(k\) is larger than that of \(i\) and \(j\). Minimizing \(l_{}^{(i)}\) will align the orders of feature embeddings with their corresponding orders in the label space w.r.t. anchor \(i\).

\(_{}\) is then enumerating over all \(2N\) samples as anchors to enforce the entire feature embeddings ordered according to their orders in the label space:

\[_{}=_{i=1}^{2N}l_{}^{(i)}= _{i=1}^{2N}_{j=1,\ j i}^{2N}-_{i},_{j})/)}{_{_{k}_{i,j}} ((_{i},_{k})/)}.\] (3)

**Interpretation.** To exploit the inherent continuity underlying the labels, \(_{}\) ranks samples in a batch with respect to their label distances to the anchor. When contrasting the anchor to the sample in the batch that is _closest_ in the label space, it enforces their similarity to be larger than all other samples in the batch. Similarly, when contrasting the anchor to the _second closest_ sample in the batch, it enforces their similarity to be larger than only those samples that have a rank of three or higher in terms of distance to the anchor. This process is repeated for higher-rank samples (i.e., the third closest, fourth closest, etc.) and for all anchors in a batch.

**Feature Ordinality.** We further examine the impact of RnC on the ordinality of learned features. Fig. 3 visualizes the feature similarity matrices obtained from 2,000 randomly sampled data points in a real-world temperature regression task  for models trained using the vanilla \(L_{1}\) loss and RnC. For clarity, the data points are sorted based on their ground-truth labels, with the expectation that the matrix values decrease progressively from the diagonal to the periphery. Notably, our method, RnC, exhibits a more discernible pattern compared to the \(L_{1}\) loss. Furthermore, we calculate two quantitative metrics, the Spearman's rank correlation coefficient  and the Kendall rank correlation coefficient , between the label similarities and the feature similarities for both methods. The results in Table 1 confirm that the feature similarities learned by our method have a significantly higher correlation with label similarities than those by the \(L_{1}\) loss.

**Connections to Contrastive Learning.** The loss can also be explained in the context of positive and negative pairs in contrastive learning. Contrastive learning and SupCon are designed for classification tasks, where positive pairs consist of samples that belong to the same class or the same input image, while all other samples are considered as negatives. In regression, however, there are no distinct classes but rather continuous labels [44; 46]. Thus in \(_{}\), any two samples can be considered as a positive or negative pair depending on the context. For a given anchor sample \(i\), _any_ other sample \(j\) in the same batch can be used to construct a positive pair with the corresponding negative samples set to all samples in the batch whose labels differ from \(i\)'s label by more than the label of \(j\). Fig. 2(a) shows an example batch, and Fig. 2(b) shows two positive pairs and their corresponding negative pair(s).

## 4 Theoretical Analysis

In this section, we theoretically prove that optimizing \(_{}\) results in an ordered feature embedding that corresponds to the ordering of the labels. All proofs are in Appendix A.

**Notations.** Let \(s_{i,j}:=(_{i},_{j})/\) and \(d_{i,j}:=d(}_{i},}_{j})\), \( i,j[2N]\). Let \(D_{i,1}<D_{i,2}<<D_{i,M_{i}}\) be the sorted label distances starting from the \(i\)-th sample (i.e., \((\{d_{i,j}|j[2N]\{i\}\})\)),

    & **Spearman’s \(^{i}\)** & **Kendall’s \(^{}\)** \\  \(L_{1}\) & 0.822 & 0.664 \\ \(\) & **0.971** & **0.870** \\   

Table 1: **Correlation between feature & label similarities.**

Figure 3: **Feature similarity matrices sorted by labels.**

\( i[2N]\). Let \(n_{i,m}:=[\{j\,|\,d_{i,j}=D_{i,m},\ j[2N]\{i\}\}]\) be the number of samples whose distance from the \(i\)-th sample equals \(D_{i,m}\), \( i[2N],m[M_{i}]\).

First, to formalize the concept of ordered feature embedding according to the order in the label space, we introduce a property termed as _\(\)-ordered_ for a set of feature embeddings \(\{_{l}\}_{l[2N]}\).

**Definition 1** (\(\)-ordered feature embeddings).: For any \(0<<1\), the feature embeddings \(\{_{l}\}_{l[2N]}\) are \(\)-ordered if \( i[2N],j,k[2N]\{i\}\),

\[\{ s_{i,j}>s_{i,k}+& \ d_{i,j}<d_{i,k}\\ |s_{i,j}-s_{i,k}|<&\ d_{i,j}=d_{i,k}\\ s_{i,j}<s_{i,k}-&\ d_{i,j}>d_{i,k} ..\]

Definition 1 implies that a set of feature embeddings that is \(\)-ordered satisfies the following properties:

For any \(j\) and \(k\) such that \(d_{i,j}=d_{i,k}\), the difference between \(s_{i,j}\) and \(s_{i,k}\) is no more than \(\); and

For any \(j\) and \(k\) such that \(d_{i,j}<d_{i,k}\), the value of \(s_{i,j}\) exceeds that of \(s_{i,k}\) by at least \(\). Note that \(>\), which indicates that the feature similarity gap between samples with different label distances to the anchor is always larger than that between samples with equal label distances to the anchor.

Next, we demonstrate that the batch of feature embeddings will be \(\)-ordered as the optimization of \(_{}\) approaches its lower bound. In order to prove this, it is necessary to derive a tight lower bound for \(_{}\). Let \(L^{}:=_{i=1}^{2N}_{m=1}^{M_{i}}n_{i,m} n_{ i,m}\), we have:

**Theorem 1** (Lower bound of \(_{}\)).: \(L^{}\) _is a lower bound of \(_{}\), i.e., \(_{}>L^{}\)._

**Theorem 2** (Lower bound tightness).: _For any \(>0\), there exists a set of feature embeddings such that \(_{}<L^{}+\)._

The above theorems verify the lower bound of \(_{}\) as well as its tightness. Given that \(_{}\) can approach its lower bound \(L^{}\) arbitrarily, we demonstrate that the feature embeddings will be \(\)-ordered when \(_{}\) is sufficiently close to \(L^{}\) for any \(0<<1\).

**Theorem 3** (Main theorem).: _For any \(0<<1\), there exist \(>0,\) such that if \(_{}<L^{}+\), then the feature embeddings are \(\)-ordered._

**From Batch to Entire Feature Space.** Now we have examined the property of a batch of feature embeddings optimized using \(_{}\). However, what will be the final outcome for the _entire_ feature space when \(_{}\) is optimized? In fact, if any batch of feature embeddings is optimized to achieve a low enough loss such that it is \(\)-ordered, the entire feature embedding will also be \(\)-ordered. This is because any triplet \((i,j,k)\) in the entire feature embeddings is certain to appear in some batch, thus their feature embeddings \((v_{i},v_{j},v_{k})\) will satisfy the condition in Definition 1. Nevertheless, to achieve \(\)-ordered features for the entire feature embeddings, do we need to optimize all batches to achieve a sufficiently low loss? The answer is no. Optimizing every batch is not only unnecessary, but also practically infeasible. In fact, one should consider the training process as a cohesive whole, which is effectively optimizing the _expectation_ of the loss over all possible random batches. Then, the Markov's inequality  guarantees that when the expectation of the loss is optimized to be sufficiently low, the loss on _any_ batch will be low enough with a high probability.

**Connections to Final Performance.** Suppose we have a \(\)-ordered feature embedding, how can it help to boost the _final performance_ of a regression task? In Appendix B, we present an analysis based on Rademacher Complexity  to prove that a \(\)-ordered feature embedding results in a better generalization bound. To put it intuitively, fitting an ordered feature embedding reduces the complexity of the regressor, which enables better generalization ability from training to testing, and ultimately leads to the final performance gain. Relatedly, we note that the enhanced generalization ability is further empirically verified in Sec. 5. Specifically, if not constrained, the learned feature embeddings could capture spurious or easy-to-learn features that are not generalizable to the real continuous targets. Moreoever, such property also leads to better robustness to data corruptions, better resilience to reduced training data, and better generalization to unseen targets.

[MISSING_PAGE_FAIL:6]

distribution over them; OR  and CORN  design multiple ordered thresholds for each label dimension and learn a binary classifier for each threshold. Further details of these baselines are included in Appendix E.1. In our comparison, we first train the encoder with the proposed \(_{}\). We then freeze the encoder and train a predictor on top of it using each of the baseline methods. The original baseline without the RNC representation is then compared to that with RnC. For instance, we denote the end-to-end training of the encoder and predictor using \(L_{1}\) loss as \(L_{1}\), while RNC(\(L_{1}\)) denotes training the encoder with \(_{}\) and subsequently training the predictor using \(L_{1}\) loss.

Table 2 summarizes the evaluation results on AgeDB, TUAB, MPIIFace6aze and SkyFinder. Green numbers highlight the performance gains by using RNC representation. The standard deviations of the best results on each dataset are reported in Appendix G.2. As the table indicates, RnC consistently achieves the best performance on both metrics across all datasets. Moreover, incorporating RnC to learn the representation consistently reduces the prediction error of all baselines by **5.8%**, **9.3%**, **11.7%**, and **7.0%** on average on AgeDB, TUAB, MPIIFace6aze, and SkyFinder, respectively.

**Comparison to state-of-the-art representation learning methods.** We further compare RnC with state-of-the-art representation learning methods, SimCLR , DINO , and SupCon, under both _linear probing_ and _fine-tuning_ schemes to evaluate their learned representations for regression tasks. Full details are in Appendix E.2. Note that SimCLR and DINO do not use label information while SupCon uses label information for training the encoder. The predictor is trained with \(L_{1}\) loss. Table 3 demonstrates that RnC outperforms all other methods across all datasets. Note that some of the representation learning schemes even underperform the vanilla \(L_{1}\) method, which further verifies that the performance gain of RnC stems from our proposed loss rather than the pre-training scheme.

**Comparison to state-of-the-art regression learning methods.** We also compare RnC with state-of-the-art regression learning schemes, including methods that perform regularization in the feature space [44; 17; 50] and methods that adopt dataset-specific techniques [1; 7]. We provide the details in Appendix E.3. \(L_{1}\) loss is employed as the default regression loss for all methods if not specified. The results are presented in Table 3, with a dash (\(-\)) indicating that the method is not applicable to the dataset. We observe that RnC achieves state-of-the-art performance across all datasets.

### Analysis

**Robustness to Data Corruptions.** Deep neural networks are widely acknowledged to be vulnerable to out-of-distribution data and various forms of corruption, such as noise, blur, and color distortions . To analyze the robustness of RnC, we generate corruptions on the AgeDB test set using the corruption process from the ImageNet-C benchmark , incorporating 19 distinct types of corruption at varying severity levels. We compare RnC(\(L_{1}\)) with \(L_{1}\) by training both models on the original AgeDB training set, but directly testing them on the corrupted test set across all severity levels. The

   Method & AgeDB & TUAB & MPIIFace6aze & SkyFinder \\    \\  SimCLR  & 9.59 & 11.01 & 9.43 & 4.70 \\ DINO  & 10.26 & 11.62 & 11.92 & 5.63 \\ SupCon  & 8.13 & 8.47 & 9.27 & 3.97 \\   \\  SimCLR  & 6.57 & 7.57 & 5.50 & 2.93 \\ DINO  & 6.61 & 7.58 & 5.80 & 2.98 \\ SupCon  & 6.55 & 7.41 & 5.54 & 2.95 \\   \\  \(L_{1}\) & 6.63 & 7.46 & 5.97 & 2.95 \\ LDS+FDS  & 6.45 & \(-\) & \(-\) & \(-\) \\ L2CS-NFT  & \(-\) & \(-\) & 5.45 & \(-\) \\ LDE  & \(-\) & \(-\) & \(-\) & 2.92 \\ RankSim  & 6.51 & 7.33 & 5.70 & 2.94 \\ Ordinal Entropy  & 6.47 & 7.28 & \(-\) & 2.94 \\  
**RNC(\(L_{1}\))** & **6.14** & **6.97** & **5.27** & **2.86** \\  Gains & **+0.31** & **+0.31** & **+0.18** & **+0.06** \\   

Table 3: **Comparisons to state-of-the-art representation & regression learning methods.** MAE\({}^{1}\) is used as the metric for AgeDB, TUAB, and SkyFinder, and Angular Error\({}^{+}\) is used for MPIIFace6aze. \(L_{1}\) loss is employed as the default regression loss if not specified. RnC surpasses state-of-the-art methods on all datasets.

reported results are averaged over all types of corruptions. Fig. 5 illustrates the results, where the representation learned by RNC is more robust to unforeseen data corruptions, with consistently less performance degradation when corruption severity increases.

**Resilience to Reduced Training Data.** The availability of massive training datasets has played an important role in the success of modern deep learning. However, in many real-world scenarios, the cost and time involved in labeling large training sets make it infeasible to do so. As a result, there is a need to enhance model resilience to limited training data. To investigate this, we subsample IMDB-WIKI to generate training sets of varying sizes and compare the performance of RNC(\(L_{1}\)) and \(L_{1}\). As Fig. 5 confirms, RNC is more robust to reduced training data and displays less performance degradation as the number of training samples decreases.

**Transfer Learning.** We evaluate whether the learned representations by RNC are transferable across datasets. To do so, we first pre-train the feature encoder on a large dataset, and subsequently utilize either _linear probing_ (fixed encoder) or _fine-tuning_ to learn a predictor on a small dataset (which shares the same prediction task). We investigate two scenarios: Transferring from AgeDB which contains \(\)12K samples to a subsampled IMDB-WIKI of 2K samples, and Transferring from another subsampled IMDB-WIKI of 32K samples to AgeDB. As shown in Table 4, RNC(\(L_{1}\)) exhibits consistent and superior performance compared to \(L_{1}\) in both linear probing and fine-tuning settings for both of the aforementioned scenarios.

**Robustness to Spurious Targets.** We show that RNC is able to deal with spurious targets that arise in data , while existing regression learning methods often fail to learn generalizable features. Specifically, the SkyFinder dataset naturally has a spurious target: different _webcams_ (with distinct locations). Therefore, we show in Fig. 6 the UMAP  visualization of the learned features obtained from both \(L_{1}\) and RNC, using data from 10 webcams in SkyFinder. The first column of the figure is colored by the ground-truth target (temperature), while the second column is colored by the 10 different webcams. Our results demonstrate that the representation learned by \(L_{1}\) is clustered by webcam, indicating that it is prone to capturing easy-to-learn features. In contrast, RNC learns the underlying continuous temperature information even in the presence of strong spurious targets, confirming its ability to learn robust representations that generalize.

**Generalization to Unseen Targets.** In practical regression tasks, it is frequently encountered that some targets are unseen during training. To investigate **zero-shot** generalization to unseen targets, following , we curate two subsets of IMDB-WIKI that contain unseen targets during training, while keeping the test set uniformly distributed across the target range. Table 5 shows the label distributions, where regions of unseen targets are marked with pink shading and those of seen targets are marked with blue shading. The first training set has a bi-modal Gaussian distribution, while the second one exhibits a tri-modal Gaussian distribution over the target space. The results confirm that RnC(\(L_{1}\)) outperforms \(L_{1}\) by a larger margin on the unseen targets without sacrificing the performance on the seen targets.

### Ablation Studies

**Ablation on Number of Positives.** Recall that in \(_{}\), all samples in the batch will be treated as the positive for each anchor. Here, we conduct an ablation study on only considering the first \(K\) closest samples to the anchor as positive. Table 5(a) shows the results on AgeDB for different \(K\), where \(K=511\) represents the scenario where all samples are considered as positive (default batch size \(N=256\)). The experiments reveal that larger values of \(K\) lead to better performance, which aligns with the intuition behind the design of \(_{}\): each contrastive term ensures that a group of orders related to the positive sample is maintained. Specifically, it guarantees that all samples that have a larger label distance from the anchor than the positive sample are farther from the anchor than the positive sample in the feature space. Only when all samples are treated as positive and their corresponding groups of orders are preserved can the order in the feature space be fully guaranteed.

**Ablation on Similarity Measure.** Table 5(b) shows the performance of using different feature similarity measures \((,)\). Compared to cosine similarity, both the negative \(L_{1}\) norm and \(L_{2}\) norm produce significantly better results. This improvement can be attributed to the fact that cosine similarity only captures the directions of feature vectors, while the negative \(L_{1}\) or \(L_{2}\) norm takes both the direction and magnitude of the vectors into account. This finding highlights the potential differences between representation learning for classification and regression tasks, where the standard practice for classification is to use cosine similarity [4; 25], while our findings suggest the superiority of \(L_{1}\) and \(L_{2}\) norm for regression-based representation learning.

**Ablation on Training Scheme.** There are typically three schemes to train the encoder: _Linear probing_, which first trains the feature encoder using the representation learning loss, then fixes the encoder and trains a linear regressor on top of it using a regression loss; _Fine-tuning_. which first trains the feature encoder using the representation learning loss, then fine-tunes the entire model using a regression loss; and _Regularization_, which trains the entire model while jointly optimizing the representation learning & the regression loss. Table 5(c) shows the results for the three schemes using \(_{}\) as the representation learning loss and \(L_{1}\) as the regression loss. All three schemes can improve performance over using \(L_{1}\) loss alone. Further, unlike classification problems where fine-tuning often delivers the best performance, freezing the feature encoder yields the best outcome in regression. This is because, in the case of regression, back-propagating the \(L_{1}\) loss to the representation can disrupt the order in the embedding space learned by \(_{}\), leading to inferior performance.

Table 6: **Ablation studies for RNC.** All experiments are performed on the AgeDB dataset and \(L_{1}\) is used as the default loss for training the predictor. Default settings used in the main experiments are marked in gray\(\).

Table 5: **Zero-shot generalization to unseen targets.** We create two IMDB-WIKI training sets with missing targets, and keep test sets uniformly distributed across the target range. MAE\({}^{}\) is used as the metric.

### Further Discussions

**Does RnC rely on data augmentation for superior performance (Appendix G.3)?** Table 10 confirms that RnC surpasses the baseline, _with_ or _without_ data augmentation. In fact, unlike typical (unsupervised) contrastive learning techniques that rely on data augmentation for distinguishing the augmented views, data augmentation is _not essential_ for RnC. This is because RnC contrasts samples according to label distance rather than the identity. The role of data augmentation for RnC is similar to its role in typical regression learning, which is to enhance model generalization.

**Does the benefit of RnC come from the two-stage training scheme (Appendix G.4)?** We confirm in Table 11 that training a predictor on top of the representation learned by the competing regression baselines does not improve performance. In fact, it can even be _detrimental_ to performance. This finding validates that the benefit of RnC is due to the proposed \(_{}\) and not the training scheme. The generic regression losses are not designed for representation learning and are computed based on the final model predictions, which fails to guarantee the final learned representation.

**Computational cost of RnC (Appendix G.5)?** We verify in Table 12 that the training time of RnC is comparable to standard contrastive learning methods (e.g., SupCon), indicating that RnC offers similar training efficiency, while achieving notably better performance for regression tasks.

## 6 Broader Impacts and Limitations

**Broader Impacts.** We introduce a novel framework designed to enhance the performance of generic deep regression learning. We believe this will significantly benefit regression tasks across various real-world applications. Nonetheless, several potential risks warrant discussion. First, when the framework is employed to regress sensitive personal attributes such as intellectual capabilities, health status, or financial standing from human data, there's a danger it might reinforce or even introduce new forms of bias. Utilizing the method in these contexts could inadvertently justify discrimination or the negative targeting of specific groups. Second, in contrast to handcrafted features which are grounded in physical interpretations, the feature representations our framework learns can be opaque. This makes it difficult to understand and rationalize the model's predictions, particularly when trying to determine if any biases exist. Third, when our method is trained on datasets that do not have a balanced representation of minority groups, there's no assurance of its performance on these groups being reliable. It is essential to recognize that these ethical concerns apply to deep regression models at large, not solely our method. However, the continuous nature of our representation which facilitates interpolation and extrapolation might inadvertently make it more tempting to justify such unethical applications. Anyone seeking to implement or use our proposed method should be mindful of these concerns. Both our specific method and deep regression models, in general, should be used cautiously to avoid situations where their deployment might contribute to unethical outcomes or interpretations.

**Limitations.** Our proposed method presents some limitations. Firstly, the technique cannot discern spurious or incidental correlations between the input and the target within the dataset. As outlined in the Broader Impact section, this could result in incorrect conclusions potentially promoting discrimination or unjust treatment when utilized to deduce personal attributes. Future research should delve deeper into the ethical dimensions of this issue and explore strategies to ensure ethical regression learning. A second limitation is that our evaluation primarily focuses on general regression accuracy metrics (e.g., MAE) without considering potential disparities when evaluating specific subgroups (e.g., minority groups). Given that a regression model's performance can vary across demographic segments, subgroup analysis is an avenue that warrants exploration in subsequent studies. Lastly, our approach learns continuous representations by contrasting samples against one another based on their ranking in the target space, necessitating label information. To adapt it for representation learning with unlabeled data, our framework will need some modifications, which we reserve for future work.

## 7 Conclusion

We present Rank-n-Contrast (RnC), a framework that learns continuous representations for regression by ranking samples according to their labels and then contrasting them against each other based on their relative rankings. Extensive experiments on different datasets over various real-world tasks verify the superior performance of RnC, highlighting its intriguing properties such as better data efficiency, robustness to corruptions and spurious targets, and generalization to unseen targets.

[MISSING_PAGE_FAIL:11]

*  Yu Gong, Greg Mori, and Frederick Tung. Ranksim: Ranking similarity regularization for deep imbalanced regression. In _Proceedings of the 39th International Conference on Machine Learning_, pages 7634-7649. PMLR, 2022.
*  Geoffrey Grimmett and David Stirzaker. _Probability and random processes_. Oxford university press, 2020.
*  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
*  Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9729-9738, 2020.
*  Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _Proceedings of the International Conference on Learning Representations_, 2019.
*  Peter J Huber. Robust estimation of a location parameter. In _Breakthroughs in statistics_, pages 492-518. Springer, 1992.
*  Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representation learning. In _International Conference on Learning Representations_, 2020.
*  Maurice G Kendall. A new measure of rank correlation. _Biometrika_, 30(1/2):81-93, 1938.
*  Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _Advances in Neural Information Processing Systems_, 33:18661-18673, 2020.
*  Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu. Selective-supervised contrastive learning with noisy labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 316-325, 2022.
*  Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe Yang, Rogerio S Feris, Piotr Indyk, and Dina Katabi. Targeted supervised contrastive learning for long-tailed recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6918-6928, 2022.
*  Wanhua Li, Xiaoke Huang, Jiwen Lu, Jianjiang Feng, and Jie Zhou. Learning probabilistic ordinal embeddings for uncertainty-aware regression. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13896-13905, 2021.
*  Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
*  Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. _arXiv preprint arXiv:1802.03426_, 2018.
*  Radu P Mihail, Scott Workman, Zach Bessinger, and Nathan Jacobs. Sky segmentation in the wild: An empirical study. In _2016 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 1-6. IEEE, 2016.
*  Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop_, volume 2, page 5, 2017.
*  Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. Ordinal regression with multiple output cnn for age estimation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4920-4928, 2016.
*  Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. _Frontiers in neuroscience_, 10:196, 2016.
*  Hongyu Pan, Hu Han, Shiguang Shan, and Xilin Chen. Mean-variance loss for deep age estimation from a face. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5285-5294, 2018.

*  Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 10-15, 2015.
*  Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for joint behavioral and neural analysis. _arXiv preprint arXiv:2204.00673_, 2022.
*  Fabian Schrumpf, Patrick Frenzel, Christoph Aust, Georg Osterhoff, and Mirco Fuchs. Assessment of non-invasive blood pressure prediction from ppg and rppg signals using deep learning. _Sensors_, 21(18):6022, 2021.
*  Shai Shalev-Shwartz and Shai Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
*  Xintong Shi, Wenzhi Cao, and Sebastian Raschka. Deep neural networks for rank-consistent ordinal regression based on conditional probabilities, 2021.
*  Charles Spearman. The proof and measurement of association between two things. _The American journal of psychology_, 100(3/4):441-471, 1987.
*  Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wenrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Contrastive regression for domain adaptation on gaze estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19376-19385, 2022.
*  Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank: theory and algorithm. In _Proceedings of the 25th international conference on Machine learning_, pages 1192-1199, 2008.
*  Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. Delving into deep imbalanced regression. In _International Conference on Machine Learning_, pages 11842-11851. PMLR, 2021.
*  Yuzhe Yang, Hao Wang, and Dina Katabi. On multi-domain long-tailed recognition, imbalanced domain generalization and beyond. In _European Conference on Computer Vision (ECCV)_, 2022.
*  Yuzhe Yang, Xin Liu, Jiang Wu, Silviu Borac, Dina Katabi, Ming-Zher Poh, and Daniel McDuff. Simper: Simple self-supervised learning of periodic targets. In _International Conference on Learning Representations_, 2023.
*  Huaxiu Yao, Yiping Wang, Linjun Zhang, James Y Zou, and Chelsea Finn. C-mixup: Improving generalization in regression. _Advances in Neural Information Processing Systems_, 35:3361-3376, 2022.
*  Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, Yanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu. Modeling discriminative representations for out-of-domain detection with supervised contrastive learning. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pages 870-878, 2021.
*  Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
*  Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, and Angela Yao. Improving deep regression with ordinal entropy. _arXiv preprint arXiv:2301.08915_, 2023.
*  Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. Mpiigaze: Real-world dataset and deep appearance-based gaze estimation. _IEEE transactions on pattern analysis and machine intelligence_, 41(1):162-175, 2017.
*  Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. It's written all over your face: Full-face appearance-based gaze estimation. In _Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on_, pages 2299-2308. IEEE, 2017.

Proofs

### Proof of Theorem 1

Recall that in Eqn. (3) we defined \(_{}:=_{i=1}^{2N} _{j=1,\ j i}^{2N}-)}{_{_{k} S_{i, j}}(s_{i,k})}\), where \(_{i,j}:=\{_{k} k i,d_{i,k} d_{i,j}\}\). We rewrite it as

\[_{}= -_{i=1}^{2N}_{j[2N] \{i\}})}{_{k[2N]\{i\},\ d_{i,k} d_{i,j}}(s_{i,k})}\] (4) \[= -_{i=1}^{2N}_{m=1}^{M_{i }}_{j[2N]\{i\},\ d_{i,j}=D_{i,m}})}{_{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})}\] \[= -_{i=1}^{2N}_{m=1}^{M_{i }}_{j[2N]\{i\},\ d_{i,j}=D_{i,m}})}{_{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})}\] \[+_{i=1}^{2N}_{m=1}^{M_{i }}_{j[2N]\{i\},\ d_{i,j}=D_{i,m}}(1+,\ d_{i,k}>D_{i,m}}(s_{i,k}-s_{i,j})}{ _{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k}-s_{i,j})})\] \[> -_{i=1}^{2N}_{m=1}^{M_{i }}_{j[2N]\{i\},\ d_{i,j}=D_{i,m}})}{_{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})}.\]

\( i[2N],m[M_{i}]\), from Jensen's Inequality we have

\[-_{j[2N]\{i\},\ d_{i,j}=D_{i,m}} )}{_{k[2N]\{i\},\ d_{i,k}=D_{i,m}} (s_{i,k})}\] (5) \[ -n_{i,m}(}_{j[2N] \{i\},\ d_{i,j}=D_{i,m}})}{_{k[2N] \{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})})=n_{i,m} n_{i,m}.\]

Thus, by plugging Eqn. (5) into Eqn. (4), we have

\[_{}>_{i=1}^{2N}_{ m=1}^{M_{i}}n_{i,m} n_{i,m}=L^{}.\] (6)

### Proof of Theorem 2

We will show \(>0\), there is a set of feature embeddings where

\[s_{i,j}>s_{i,k}+d_{i,j}<d_{i,k}\\ s_{i,j}=s_{i,k}d_{i,j}=d_{i,k}\]

and \(:=]}n_{i,m}},  i[2N],j,k[2N]\{i\}\), such that \(_{}<L^{}+\).

For such a set of feature embeddings, \( i[2N],m[M_{i}],j\{j[2N]\{i\} d_{i,j}=D_{i,m}\}\),

\[-)}{_{k[2N]\{i\},\ d_{i,k}=D_{i,m }}(s_{i,k})}= n_{i,m}\] (7)since \(s_{i,k}=s_{i,j}\) for all \(k\) such that \(d_{i,k}=D_{i,m}=d_{i,j}\), and

\[&(1+,\ d_{i,k}>D_{i,m}}(s_{i,k}-s_{i,j})}{_{k[2N]\{i \},\ d_{i,k}=D_{i,m}}(s_{i,k}-s_{i,j})})\\ <&(1+}) <}\] (8)

since \(s_{i,k}-s_{i,j}<-\) for all \(k\) such that \(d_{i,k}>D_{i,m}=d_{i,j}\) and \(s_{i,k}-s_{i,j}=0\) for all \(k\) such that \(d_{i,k}=D_{i,m}=d_{i,j}\).

From Eqn. (4) we have

\[_{}=&-_{i=1}^{2N}_{m=1}^{M_{i}}_{j[2N] \{i\},\ d_{i,j}=D_{i,m}})}{_{k[2N ]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})}\\ &+_{i=1}^{2N}_{m=1}^{M_{i }}_{j[2N]\{i\},\ d_{i,j}=D_{i,m}}(1+,\ d_{i,k}>D_{i,m}}(s_{i,k}-s_{i,j})}{ _{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k}-s_{i,j})} ),\] (9)

By plugging Eqn. (7) and Eqn. (8) into Eqn. (9) we have

\[_{}<_{i=1}^{2N}_{ m=1}^{M_{i}}n_{i,m} n_{i,m}+=L^{}+\] (10)

### Proof of Theorem 3

We will show \( 0<<1\), there is a

\[=(_{i[2N],m[M_{i}]}(1+ (+)}),2-)>0,\]

such that when \(_{}<L^{}+\), the feature embeddings are \(\)-ordered.

We first show that \(|s_{i,j}-s_{i,k}|<\) if \(d_{i,j}=d_{i,k}\), \( i[2N],\ j,k[2N]\{i\}\) when \(_{}<L^{}+\).

From Eqn. (4) we have

\[_{}>-_{i=1}^{2N}_ {m=1}^{M_{i}}_{j[2N]\{i\},\ d_{i,j}=D_{i,m}})}{_{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{ i,k})}.\] (11)

Let \(p_{i,m}:=,\ d_{i,j}=D_{i,m}}{}s_{i,j}\), \(q_{i,m}:=,\ d_{i,j}=D_{i,m}}{}s_{i,j}\), \(_{i,m}:=s_{i,p_{i,m}}\), \(_{i,m}:=s_{i,q_{i,m}}-s_{i,p_{i,m}}\), \( i[2N],m[M_{i}]\), by splitting out the maximum term and the minimum term we have

\[_{}>&-_{i=1}^{2N}_{m=1}^{M_{i}})}{_{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k} )}\\ &++_{i,m})}{_{k[2N] \{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})}+,q_{i,m}\},d_{i,j}=D_{i,m}}s_{i,j} )}{(_{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{ i,k}))^{n_{i,m}-2}}}.\] (12)

Let \(_{i,m}:=-2}_{j[2N]\{i,p_{i,m},q_{ i,m}\},d_{i,j}=D_{i,m}}(s_{i,j}-_{i,m})\), we have

\[-)}{_{k[2N] \{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})}=(1+(_{i,m})+(n_{i,m }-2)_{i,m})\] (13)and

\[-+_{i,m})}{_{k[2N]\{i\},\,d_{ i,k}=D_{i,m}}(s_{i,k})}=(1+(_{i,m})+(n_{i,m}-2)_{i,m})-_{i,m}.\] (14)

Then, from Jensen's inequality, we know

\[(_{j[2N]\{i,p_{i,m},q_{i,m}\},d_{ i,j}=D_{i,m}}s_{i,j})(-2}_{j[2N] \{i,p_{i,m},q_{i,m}\},d_{i,j}=D_{i,m}}(s_{i,j}))^{n_{i,m}- 2},\] (15)

thus

\[-,q_{i,m}\},d_{ i,j}=D_{i,m}}s_{i,j})}{(_{i[2N]\{i,p_{i,m},q_{i,m} \},d_{i,j}=D_{i,m}}(s_{i,k}))^{n_{i,m}-2}}(n_{i,m}-2) (1+(_{i,m})+(n_{i,m}-2)_{i,m})-(n_{i,m}-2)(_{i,m})\] (16)

By plugging Eqn. (13), Eqn. (14) and Eqn. (16) into Eqn. (12), we have

\[_{}>_{i=1}^{2N}_{m =1}^{M_{i}}(n_{i,m}(1+(_{i,m})+(n_{i,m}-2)_{i,m})-_ {i,m}-(n_{i,m}-2)(_{i,m})).\] (17)

Let \(h():=n_{i,m}(1+(_{i,m})+(n_{i,m}-2))-_{i,m}-(n_{i,m}-2)()\). From derivative analysis we know \(h()\) decreases monotonically when \([1,)}{2}]\) and increases monotonically when \([)}{2},(_{i,m})]\), thus

\[h() h()}{2})=n_{i,m}  n_{i,m}+2)}{2}-_{i,m}.\] (18)

By plugging Eqn. (18) into Eqn. (17), we have

\[_{} >_{i=1}^{2N}_{m=1}^{M_{i}}(n_{i,m } n_{i,m}+2)}{2}-_{i,m})\] (19) \[=L^{}+_{i=1}^{2N}_{m=1}^{M_{i}} (2)}{2}-_{i,m}).\]

Then, since \(_{i,m} 0\), we have \(2)}{2}-_{i,m} 0\). Thus, \( i[2N],\ m[M_{i}]\),

\[_{}>L^{}+(2 )}{2}-_{i,m}).\] (20)

If \(_{}<L^{}+ L^{}+ (2-)\), then

\[2)}{2}-_{i,m}<2-.\] (21)

Since \(y(x)=2-x\) increases monotonically when \(x>0\), we have \(_{i,m}<\). Hence, \( i[2N],\ j,k[2N]\{i\}\), if \(d_{i,j}=d_{i,k}=D_{i,m},|s_{i,j}-s_{i,k}|_{i,m}<\).

Next, we show \(s_{i,j}>s_{i,k}+\) if \(d_{i,j}<d_{i,k}\) when \(_{}<L^{}+\).

From Eqn. (4) we have

\[_{}= -_{i=1}^{2N}_{m=1}^{M_{i}}_{j[2N ]\{i\},\ d_{i,j}=D_{i,m}})}{_{k[2N ]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k})}\] (22) \[+_{i=1}^{2N}_{m=1}^{M_{i}}_{j[2N ]\{i\},\ d_{i,j}=D_{i,m}}(1+,\ d_{i,k}>D_{i,m}}(s_{i,k}-s_{i,j})}{_{k[2N] \{i\},\ d_{i,k}=D_{i,m}}(s_{i,k}-s_{i,j})}),\]and combining it with Eqn. (5) we have

\[_{}& L^{}+ _{i=1}^{2N}_{m=1}^{M_{t}}_{j[2N]\{i \},\ d_{i,j}=D_{i,m}}(1+,\ d_{i,k}>D_{i,m}}(s_{i,k}-s_{i,j})}{ _{k[2N]\{i\},\ d_{i,k}=D_{i,m}}(s_{i,k}-s_{i,j})} )\\ &>L^{}+(1+-s_{ i,j})}{_{l[2N]\{i\},\ d_{i,l}=d_{i,j}}(s_{i,l}-s_{i,j})}), \] (23)

\( i[2N],\ j[2N]\{i\},k\{k[2N]\{i\} d _{i,j}<d_{i,k}\}\).

When \(_{}<L^{}+\), we already have \(|s_{i,l}-s_{i,j}|<\), \( d_{i,l}=d_{i,j}\), which derives \(s_{i,l}-s_{i,j}<\) and thus \((s_{i,l}-s_{i,j})<()\). By putting this into Eqn. (22), we have \( i[2N],\ j[2N]\{i\},k\{k[2N]\{i\} d _{i,j}<d_{i,k}\}\),

\[_{}>L^{}+(1+-s_{i,j})}{n_{i,r_{i,j}}()}),\] (24)

where \(r_{i,j}[M_{i}]\) is the index such that \(D_{i,r_{i,j}}=d_{i,j}\).

Further, given \(_{}<L^{}+<L^{}+ (1+}(+)})\), we have

\[(1+-s_{i,j})}{n_{i,r_{i,j}}()})< (1+}(+)})\] (25)

which derives \(s_{i,j}>s_{i,k}+\), \( i[2N],\ j[2N]\{i\},k\{k[2N]\{i\} d _{i,j}<d_{i,k}\}\).

Finally, \( i[2N]\), \(j,k[2N]\{i\}\), \(s_{i,j}<s_{i,k}-\) if \(d_{i,j}>d_{i,k}\) directly follows from \(s_{i,j}>s_{i,k}+\) if \(d_{i,j}<d_{i,k}\).

## Appendix B Additional Theoretical Analysis

In this section, we present an analysis based on Rademacher Complexity  to substantiate that \(\)-ordered feature embedding results in a better generalization bound.

A regression learning task can be formulated as finding a hypothesis \(h\) to predict the target \(y\) from the input \(x\). Suppose there are \(m\) data points in the training set \(=\{(x_{k},y_{k})\}_{k=1}^{m}\). Let \(_{1}\) be the class of all possible functions from the input space to the target space.

If a \(\)-ordered feature embedding is guaranteed with an encoder \(f\) mapping \(x_{k}\) to \(v_{k}\), the set of candidate hypotheses can be reduced to all "\(\)-monotonic" functions \(h(x)=g(f(x))\), which satisfy \(d(g(v_{i}),g(v_{j}))<d(g(v_{i}),g(v_{k}))\) for \(s_{i,j}>s_{i,k}+\), \(d(g(v_{i}),g(v_{j}))=d(g(v_{i}),g(v_{k}))\) for \(|s_{i,j}-s_{i,k}|<\), and \(d(g(v_{i}),g(v_{j}))>d(g(v_{i}),g(v_{k}))\) for \(s_{i,j}<s_{i,k}-\) for any \(i,j\) and \(k\). We denote the class of all "\(\)-monotonic" functions by \(_{2}\). Note that both \(_{1}\) and \(_{2}\) contain the optimal hypothesis \(h^{*}\), which satisfies \( x,y,h^{*}(x)=y\).

Further, for a hypothesis set \(_{i}\), let \(_{i}=\{(l((x_{1},y_{1});h),...,l((x_{m},y_{m});h)):h_{i}\}\) be the loss set for each hypothesis in \(_{i}\) with respect to the training set \(\), where \(l\) is the loss function. Let \(c_{i}\) be the upper bound of \(|l((x,y);h))|\) for all \(x,y\) and \(h_{i}\). We introduce the Rademacher Complexity  of \(_{i}\), denoted as \(R(_{i})\). Then, the generalization bound based on Rademacher Complexity says that with a high probability (at least \(1-\)), the gap between the empirical risk (i.e., training error) and the expected risk (i.e., test error) is upper bounded by \(2R(_{i})+4c_{i}}\).

Since \(_{2}_{1}\), we have \(_{2}_{1}\) and \(c_{2} c_{1}\), and from the monotonicity of Rademacher Complexity we have \(R(_{2}) R(_{1})\). Hence, with a \(\)-ordered feature embedding, the upper bound on the gap between the training error and the test error will be reduced, which leads to better regression performance.

## Appendix C Dataset Details

Five real-world datasets are used in the experiments:

* AgeDB [32; 44] is a dataset for predicting age from face images. It contains 16,488 in-the-wild images of celebrities and the corresponding age labels. The age range is between 0 and 101. It is split into a 12,208-image training set, a 2140-image validation set, and a 2140-image test set.
* TUAB [34; 11] is a dataset for brain-age estimation from EEG resting-state signals. The dataset comes from EEG exams at the Temple University Hospital in Philadelphia. Following Engemann et al. , we use only the non-pathological subjects, so that we may consider their chronological age as their brain-age label. The dataset includes 1,385 21-channel EEG signals sampled at 200Hz from individuals whose age ranges from 0 to 95. It is split into a 1,246-subject training set and a 139-subject test set.
* MPIIFaceGaze [51; 52] is a dataset for gaze direction estimation from face images. It contains 213,659 face images collected from 15 participants during their natural everyday laptop use. We subsample and split it into a 33,000-image training set, a 6,000-image validation set, and a 6,000-image test set with no overlapping participants. The gaze direction is described as a 2-dimensional vector with the pitch angle in the first dimension and the yaw angle in the second dimension. The range of the pitch angle is -40\({}^{}\) to 10\({}^{}\) and the range of the yaw angle is -45\({}^{}\) to 45\({}^{}\).
* SkyFinder [31; 7] is a dataset for temperature prediction from outdoor webcam images. It contains 35,417 images captured by 44 cameras around 11am on each day under a wide range of weather and illumination conditions. The temperature range is \(-20\,^{}\) to \(49\,^{}\). It is split into a 28,373-image training set, a 3,522-image validation set, and a 3,522-image test set.
* IMDB-WIKI [36; 44] is a large dataset for predicting age from face images, which contains 523,051 celebrity images and the corresponding age labels. The age range is between 0 and 186 (some images are mislabeled). We use this dataset to test our method's resilience to reduced training data, performance on transfer learning, and ability to generalize to unseen targets. We subsample the dataset to create a variable size training set, and keep the validation set and test set unchanged with 11,022 images in each.

 p{113.8pt} p{113.8pt}}  
**Dataset** & **Original** & **Augmented** \\   AgeDB & & \\  TUAB & & \\  MPIIFaceGaze & & \\  SkyFinder & & \\   

Table 7: **Visualizations of original and augmented data samples on all datasets.**Details of Data Augmentation

Table 7 shows examples of original and augmented data samples on each dataset. The data augmentations used on each dataset are listed below:

* For AgeDB and SkyFinder, random crop and resize (with random horizontal flip), color distortions are used as data augmentation;
* For TUAB, random crop is used as data augmentation;
* For MPIIFaceGaze, random crop and resize (without random horizontal flip), color distortions are used as data augmentation.

## Appendix E Details of Competing Methods

### End-to-End Regression Methods

We compared with seven end-to-end regression methods:

* \(L_{1}\), MSE and Huber have the model directly predict the target value and train the model with an error-based loss function, where \(L_{1}\) uses the mean absolute error, MSE uses the mean squared error and Huber uses an MSE term when the error is below a threshold and an \(L_{1}\) term otherwise.
* DEX  and DLDL-v2  divide the regression range of each label dimension into several bins and learn the probability distribution over the bins. DEX  optimizes a cross-entropy loss between the predicted distribution and the one-hot ground-truth labels, while DLDL-v2  jointly optimizes a KL loss between the predicted distribution and a normal distribution centered at the ground-truth value, as well as an \(L_{1}\) loss between the expectation of the predicted distribution and the ground-truth value. During inference, they output the expectation of the predicted distribution for each label dimension.
* OR  and CORN  design multiple ordered thresholds for each label dimension, and learn a binary classifier for each threshold. OR  optimizes a binary cross-entropy loss for each binary classifier to learn whether the target value is larger than each threshold, while CORN  learns whether the target value is larger than each threshold conditioning on it is larger than the previous threshold. During inference, they aggregate all binary classification results to produce the final results.

For the classification-based baselines, the regression range is divided into small bins, and each bin is considered as a class. Details for each dataset are as follows:

* For AgeDB, the age range is 0 \(\) 101 and the bin size is set to 1;
* For TUAB, the brain-age range is 0 \(\) 95 and the bin size is set to 1;
* For MPIIFaceGaze, the target range is -40 \(\) 10 (\({}^{}\)) for the pitch angle and -45 \(\) 45 (\({}^{}\)) for the yaw angle, and the bin size is set to 0.5 for the pitch angle and is set to 1 for the yaw angle;
* For SkyFinder, the temperature range is -20 \(\) 49 (\({}^{}\)C) and the bin size is set to 1.

### State-of-the-art Representation Learning Methods

We compared with three state-of-the-art representation learning methods:

* SimCLR  is a contrastive learning method that learns representations by aligning positive pairs and repulsing negative pairs. Positive pairs are defined as different augmented views from the same data input, while negative pairs are defined as augmented views from different data inputs.
* DINO  is a self-supervised representation learning method using self-distillation. It passes two different augmented views from the same data input to both the student and the teacher networks and maximizes the similarity between the output features of the student network and those of the teacher network. The gradients are propagated only through the student network and the teacher parameters are updated with an exponential moving average of the student parameters.

* SupCon extends SimCLR  to the fully-supervised setting, where positive pairs are defined as augmented data samples from the same class and negative pairs are defined as augmented data samples from different classes. To adapt SupCon to the regression task, we follow the standard routine of classification-based regression methods to divide the regression range into small bins and regard each bin as a class.

### State-of-the-art Regression Learning Methods

We also compared with five state-of-the-art regression learning methods:

* LDS+FDS is the state-of-the-art method on the AgeDB dataset. It addresses the data imbalance issue by performing distribution smoothing for both labels and features.
* L2CS-Net is the state-of-the-art method on the MPIIFaceGaze dataset. It regresses each gaze angle separately and applies both a cross-entropy loss and an MSE loss on the predictions.
* LDE is the state-of-the-art method on the SkyFinder dataset. It converts temperature estimation to a classification task, and the class label is encoded by a Gaussian distribution centered at the ground-truth label.
* RankSim is a state-of-the-art regression method that proposes a regularization loss to match the sorted list of a given sample's neighbors in the feature space with the sorted list of its neighbors in the label space.
* Ordinal Entropy is a state-of-the-art regression method that proposes a regularization loss to encourage higher-entropy feature spaces while maintaining ordinal relationships.

## Appendix F Details of Experiment Settings

All experiments are trained using 8 NVIDIA TITAN RTX GPUs. We use the SGD optimizer and cosine learning rate annealing  for training. The batch size is set to 256. For one-stage methods and encoder training of two-stage methods, we select the best learning rates and weight decays for each dataset by grid search, with a grid of learning rates from \(\{0.01,0.05,0.1,0.2,0.5,1.0\}\) and weight decays from \(\{10^{-6},10^{-5},10^{-4},10^{-3}\}\). For the predictor training of two-stage methods, we adopt the same search setting as above except for adding no weight decay to the search choices of weight decays. For temperature parameter \(\), we search from \(\{0.1,0.2,0.5,1.0,2.0,5.0\}\) and select the best, which is \(2.0\). We train all one-stage methods and the encoder of two-stage methods for 400 epochs, and the linear regressor of two-stage methods for 100 epochs.

## Appendix G Additional Experiments and Analyses

### Impact of Model Architectures

In the main paper, we use ResNet-18 as the default encoder backbone for three visual datasets (AgeDB, MPIIFaceGaze and SkyFinder). In this section, we study the impact of backbone architectures on the experiment results. As Table 8 reports, the results of using ResNet-50 as the encoder backbone are consistent with the results using ResNet-18 in Table 2, indicating that our method is compatible with different model architectures.

    &  &  & SkyFinder \\  Metrics & MAE\({}^{}\) & R\({}^{2}\) & Angular\({}^{}\) & R\({}^{2}\) & MAE\({}^{}\) & R\({}^{2}\) \\   L1 & 6.49 & 0.830 & 5.74 & 0.748 & 2.88 & 0.863 \\ RNC(L1) & **6.10** & **0.851** & **5.16** & **0.819** & **2.78** & **0.877** \\  & **(+0.39)** & **(+0.021)** & **(+0.58)** & **(+0.071)** & **(+0.10)** & **(+0.014)** \\   

Table 8: **Evaluation results using ResNet-50 as the encoder backbone for visual datasets**. The results are consistent with the results using ResNet-18 as the encoder backbone.

### Standard Deviations of Results

In this section, we study the standard deviations of the best results on each dataset with 5 different random seeds. Table 9 shows their average prediction errors and standard deviations. These results are aligned with the results we reported in the main paper.

### Impact of Data Augmentation

In this section, we study the impact of data augmentation on Rnc. The following ablations are considered:

* Rnc(\(L_{1}\)) _(without augmentation)_: Data augmentation is removed and \(_{}\) is computed over \(N\) feature embeddings.
* Rnc(\(L_{1}\))_(one-view augmentation)_: Data augmentation is only performed once for each data point and \(_{}\) is computed over \(N\) feature embeddings.
* Rnc(\(L_{1}\))_(two-view augmentation)_: Data augmentation is performed twice to create a two-view batch and \(_{}\) is computed over \(2N\) feature embeddings.
* \(L_{1}\)_(without augmentation)_: Data augmentation is removed and \(L_{1}\) is computed over \(N\) samples.
* \(L_{1}\)_(with augmentation)_: Data augmentation is performed for each datapoint and \(L_{1}\) is computed over \(N\) samples.

Table 10 reports the results on each dataset, which demonstrate that removing data augmentation will result in a decrease in performance for both \(L_{1}\) and Rnc(\(L_{1}\)), and our method outperforms the baseline with or without data augmentation.

Here we further discuss the different roles played by data augmentation in (unsupervised) contrastive learning methods, end-to-end regression learning methods, and our method respectively:

* For (unsupervised) contrastive learning methods, data augmentation is essential for creating the pretext task, which is to distinguish whether the augmented views belong to the same identity or not. Therefore, removing the data augmentation will result in a complete collapse of the model.
* For end-to-end regression learning methods, data augmentation helps the model generalize better and become more robust to unseen variations. However, without augmentation, the model can still perform reasonably well. The improvement brought by the augmentation is often correlated to how much the data augmentation can compensate for the gap between training data and testing data.
* For Rnc, data augmentation is also not necessary since our loss contrasts samples according to label distance rather than the identity. Thus, creating augmented views is not crucial to our method. The role of data augmentation for our method is similar to its role in regression learning methods, namely, improving the generalization ability. The experiment results also confirm this: removing data augmentation will lead to a similar drop in performance for both Rnc(\(L_{1}\)) and \(L_{1}\).

  
**Method** & **Augmentation** & **AgeDB** & **TUAB** & **MPIIFaceGaze** & **SkyFinder** \\   \(L_{1}\) & \(\) & 9.53 & 10.24 & 6.93 & 3.14 \\ RNC(\(L_{1}\)) & \(\) & **8.96** & **9.88** & **6.31** & **2.97** \\  \(L_{1}\) & ✓ & 6.63 & 7.46 & 5.97 & 2.95 \\ RNC(\(L_{1}\)) & One-view & 6.40 & 7.29 & 5.46 & 2.89 \\ RNC(\(L_{1}\)) & Two-view & **6.14** & **6.97** & **5.27** & **2.86** \\   

Table 10: **Impact of data augmentation. Data augmentation is not essential for Rnc’s superior performance.**

   AgeDB: Rnc(\(L_{1}\)) & TUAB: Rnc(DLDL-v2) & MPIFaceGaze: Rnc(OR) & SkyFinder: Rnc(DLDL-v2) \\ 
6.19 \(\) 0.08 & 7.00 \(\) 0.10 & 5.24 \(\) 0.13 & 2.87 \(\) 0.04 \\   

Table 9: **Average prediction errors and standard deviations of the best results on each dataset.**

### Two-Stage Training Scheme for End-to-End Regression Methods

RnC employs a two-stage training scheme, where the encoder is trained with \(_{}\) in the first stage and a predictor is trained using a regression loss on top of the encoder in the second stage. In Table 11, we also train a predictor using \(L_{1}\) loss on top of the encoder learned by the competing end-to-end regression methods and compare with the performance of \((L_{1})\) on the AgeDB dataset. The results show that the two-stage training scheme does not increase the performance of end-to-end regression methods. In fact, it can even be _detrimental_ to performance. This is because these methods are not designed for representation learning, and their loss functions are calculated w.r.t. the final model predictions. As a result, there is no guarantee for the representation learned by these methods. This finding validates that the benefit of RnC is due to the proposed \(_{}\) and not the training scheme.

### Training Efficiency

We compute the average wall-clock running time (in seconds) per training epoch on 8 NVIDIA TITAN RTX GPUs for RnC and compare it with SupCon on all four datasets, as shown in Table 12. Results indicate that the training efficiency of RnC is comparable to SupCon.

   Method & End-to-End & Two-Stage \\  \(L_{1}\) & 6.63 & 6.68 \\ MSE & 6.57 & 6.57 \\ Huber & 6.54 & 6.63 \\ DEX  & 7.29 & 7.42 \\ DLDL-v2  & 6.60 & 7.28 \\ OR  & 6.40 & 6.72 \\ CORN  & 6.72 & 6.94 \\ RnC(\(L_{1}\)) & – & **6.14** \\   

Table 11: **Two-stage training results for end-to-end regression methods on AgeDB. The two-stage training scheme does not increase or even harms the performance of end-to-end regression methods.**

   Method & AgeDB & TUAB & MPIIFaceGaze & SkyFinder \\   SupCon & 23.1 & 25.3 & 69.1 & 55.6 \\ RnC & 26.2 & 27.3 & 75.4 & 61.8 \\  Ratio & 1.13 & 1.08 & 1.09 & 1.11 \\   

Table 12: **Average wall-clock running time (in seconds) per training epoch for RnC and SupCon on all datasets. The training efficiency of RnC is comparable to SupCon.**