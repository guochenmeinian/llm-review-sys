# Mix Data or Merge Models?

Optimizing for Diverse Multi-Task Learning

 Aakanksha1, Arash Ahmadian1, 2, Seraphina Goldfarb-Tarrant2,

Beyza Ermis1, Marzieh Fadaee1, Sara Hooker1

1Cohere For AI 2Cohere

Correspondence: {aakanksha, marzieh, sarahooker}@cohere.com

###### Abstract

Large Language Models (LLMs) have been adopted and deployed worldwide for a broad variety of applications. However, ensuring their safe use remains a significant challenge. Preference training and safety measures often overfit to harms prevalent in Western-centric datasets, and safety protocols frequently fail to extend to multilingual settings. We explore model merging in a diverse multi-task setting, combining safety and general-purpose tasks within a multilingual context. Each language introduces unique and varied learning challenges across tasks. We find that objective-based merging is more effective than mixing data, with improvements of up to 8% and 10% in general performance and safety respectively. We also find that language-based merging is highly effective -- by merging monolingually fine-tuned models, we achieve a 4% increase in general performance and 7% reduction in harm across all languages on top of the data mixtures method using the same available data. Overall, our comprehensive study of merging approaches provides a useful framework for building strong and safe multilingual models.

## 1 Introduction

Large language models demonstrate strong multitask capabilities, effectively addressing a wide range of tasks across diverse domains . _"Safety"_ in a model can be viewed as another "task-solving" ability that a model can learn. It is well established that equipping a model with any kind of capabilities with the standard paradigm of training requires copious amounts of data. Multi-tasking abilities typically arise from fine-tuning models on mixed datasets, which combine data from various sources and across many tasks . However, determining the optimal strategy for mixing datasets in multi-task training is often complex and resource-intensive, as it must ensure that all tasks benefit from the shared training process -- especially in the context of safety, where the general performance of models often gets cannibalized in exchange for safety .

More recently, an emerging approach for enabling multi-tasking has focused on training distinct models for specific tasks, followed by a weight-merging process governed by a pre-defined algorithm . This method has shown great promise in building models with new capabilities without incurring additional costs and challenges that accompany training from scratch. However, a key question remains - _how does it compare to traditional data mixing and weighting approaches?_ In this paper, we explore whether model merging can effectively balance safety and overall performance and how it compares to data mixing techniques, particularly for multilingual alignment.

We evaluate these trade-offs under severe multi-task constraints - optimizing for general and safe performance in a _multilingual setting_. The inherent difficulties of handling multiple languages, each with its unique linguistic structures, cultural nuances, and potential biases, present a formidable task in establishing alignment for these models . Mitigating harm acrossmultiple languages is critical given the wide adoption of large models across the world. However, a common issue in safety and alignment work is the narrow focus on addressing safety primarily for English. And so, the challenges are compounded in this scenario by the trivial amount of safety data available across different languages . However, it is precisely because of these severe constraints that this presents an interesting setting to thoroughly evaluate the benefits of merging.

We conduct an exhaustive study to compare traditional approaches for balancing multi-objective training by curating and varying an expansive set of training data mixtures with approaches that merge model checkpoints trained on different subsets of data. Our large-scale evaluation is across six languages from five different language families and encompasses both finetuning and preference training across four different merging techniques. Through our comprehensive experimental setup, we summarize the key findings and contributions of our work as follows:

**1. Merging outperforms mixing**. We find that model merging is more effective than weighting data mixtures for achieving a good balance between safety and generalizability in language models. The top-performing methods for individual objectives were TIES, which reduced harm by 10.4%, and Linear merging, which improved general performance by 8.6% beyond the data mixing approach. The best approach for balancing both objectives was SLERP, which consistently achieved optimal trade-offs across different training strategies, with 3.1% further reductions in harm and 7.0% gains in general performance over the data mixing approach.

**2. Merging is effective at extending multilingual coverage**. We show that merging models across languages is an effective way to manage the dual challenge of safety and multilinguality. Instead of merging across objectives (safety-finetuned model and general-finetuned model), we experiment with merging across languages. Our findings indicate that when each model is trained on a mixture of safety and general data in a single language and then merged, it achieves improvements across both harm reduction and general performance. Specifically, it yields enhancements of up to 3.8% in general benchmarks and a reduction of up to 6.6% in harmful generations compared to a multilingually finetuned model.

**3. Not all merging algorithms are created equal.** We find that not all merging algorithms provide similar performance gains that balance safety and general performance. Some methods consistently result in net positive gains across both axes simultaneously, while others display clear trade-offs between maintaining safe behaviors as well as general-purpose abilities. The highest reduction of harmful generations is achieved by merging DPO checkpoints using the TIES approach, however, this resulted in a decrease of 7.4% in general performance. We see a similar pattern with linear merging as well. Merging models using DARE-TIES and SLERP are more effective at balancing the dual

Figure 1: **Overview of our _Mix_ versus _Merge_ framework: We analyze the differences in merging models on trained with specialized multilingual datasets, particularly in the context of safety, in contrast to those trained directly on mixtures of these datasets. We follow the LLM-as-a-judge approach for evaluating the performance of these models along two axes – general and safety.**

objectives, with SLERP delivering the most significant improvements in both general performance and harm reduction (7% and 3.1% respectively).

## 2 _Mix_ versus _Merge_ Setup

In this section, we detail our experimental setup, which involves training models with various data mixtures targeting different objectives to establish the _"Mix"_, followed by merging some of these trained checkpoints into a single model to obtain the _"Merge"_. This setup serves as the foundation for our comprehensive comparison of merging methods' effectiveness in balancing safety and general performance in a multilingual setting. Our experiments cover both supervised fine-tuning (SFT) and offline preference alignment, specifically employing Direct Preference Optimization (DPO) .

### Merging Approaches

We conduct extensive experiments with diverse data mixtures to create a pool of model candidates. From this pool, we merge the best-performing checkpoints using four different algorithms to produce the final merged models.

**1) Linear Merge:** Linear merging involves simple linear weighted averaging of model parameters, weighted by specified coefficients. This method is widely used in convex optimization and deep learning [24; 38; 43]. This process is formulated as:

\[_{}=_{i=1}^{N}_{i}_{i}\] (1)

where \(_{i}\) represents the weight assigned to the parameters of each model, with the constraint that \(_{i=1}^{N}_{i}=1\). We conduct ablations by varying the values of \(_{i}\) to investigate different weighting ratios for the base models.

**2) Spherical Linear Interpolation (SLERP):** This technique is used to smoothly blend two models by interpolating their weights along the shortest path on a high-dimensional sphere [42; 11]. SLERP preserves each model's unique characteristics and geometric properties, even in complex spaces. The process involves normalizing the vectors to ensure equal length, calculating the angle \(\) between them, and performing the interpolation as follows:

\[_{}(t)=_{1}+ _{2}\] (2)

SLERP typically merges only two models at a time. Here, \(t\) determines the interpolation weight, with \(t=0\) using only _Model 1_ and \(t=1\) using only _Model 2_. This method improves upon standard weight averaging by preserving the models' geometric integrity.

**3) TIES-Merging:** This method efficiently combines multiple models by addressing parameter interference and sign conflicts, which occur when models suggest opposing adjustments to the same parameter due to task-specific fine-tuning . The process begins by trimming parameters to retain only those with significant magnitude changes. It then resolves sign conflicts by creating a consensus sign vector:

\[s=(_{i=1}^{N}(_{i}))\] (3)

Finally, it merges the parameters by averaging those that align with the consensus sign:

\[_{}=s_{i=1}^{N}|_{i}|\] (4)

TIES-Merging ensures that only parameters contributing to the agreed-upon direction are included in the final model, enhancing performance.

**4) DARE-TIES:** This technique  builds upon TIES by applying dropout to the delta parameters before merging them using the TIES method. It reduces interference from redundant parameters and helps maintain the model's overall performance.

We apply gradient weighting to all merging methods except for Linear Merge. With weighting, we define a blend ratio to specify the merge between the model parameters. Gradient weighting dictates how that ratio changes across the specified values and uses linear interpolation to further establish a smoother gradient of blend ratios for merging the tensors of the models. For example, if the blend ratio between _Model 1_ and _Model 2_ is defined as [0, 0.5, 1], this implies that the merge begins with 100% of _Model 2's_ parameters, gradually transitioning to a 50-50 blend between the two and concluding with only _Model 1's_ parameters at the end. For all methods, we conduct an exhaustive search over the set \(\{0,0.3,0.5,0.7,1\}\) to determine the optimal parameter contributions. Our experiments utilize the mergekit library from Arcee .

### Training Data

**Safety dataset.** We use the human-annotated prompts from the multilingual Aya Red-teaming dataset  as seeds to synthetically generate pairs of adversarial prompts and contextually safe completions following previous works .

**General purpose dataset.** Following previous works , we use a sampled set of 10,000 English prompts from the _Ultrafeedback Binarized_ dataset translated into our target languages. This dataset will be referred to as the _"general-purpose"_ dataset for the remainder of the paper.

**Training data Mix.** We study models trained on different mixtures of data - **0% Safety Mix, 15% Safety Mix** and **100% Safety Mix**. The varying ratio of safety data simulates different objectives - for example, training with 100% safety data allows us to model an upper bound of expected harm mitigation and to obtain a model optimized for safety. In contrast, the 15% Safety mix consists of a combination of safety and general-purpose data in a 1:5 ratio - this represents a more real-world scenario typical of deployment settings. Unless specified otherwise, we use the 15% Safety mix as the baseline for our experimentation. The other mixes follow similar relationships between their naming and ratios.

### Key Ablations

In order to study the relative merits of merging for different objectives across a wide set of languages, we conduct extensive ablations. We detail some of the most critical experiment variants below:

**Objective-based merging.** To evaluate the relative merits of merging on balancing dual-objectives, we merge models that have been separately optimized for general-purpose abilities and safety. This builds upon our multilingual 0% and 100% Safety Mixes (see Section 2.2) to balance the trade-offs between safety and general performance.

**Language-based merging.** Multilinguality remains one of the most challenging tasks in language modeling. We aim to determine whether language-specific models can be used off-the-shelf to incorporate language capabilities and explore how merging models based exclusively on different languages affects their downstream performance.

Figure 2: _Mixing versus merging_: Safety and general performance of a _15% Safety Mix_ model (§2.2) against SLERP merging, which emerges as the best method for balancing trade-offs, for both SFT and DPO based checkpoints. Lower is better for (a) and higher is better for (b). Both metrics are measured with respect to the Aya 23 base model.

[MISSING_PAGE_FAIL:5]

gains as high as 7.5% in general performance when combining models with DARE-TIES, closely followed by SLERP with 7% gains.

**Impact on _safety_ performance.** Table 1 illustrates that almost all merging methods perform superior to the 15% Safety Mix baseline, with the exception of Linear lagging behind by around 6%, implying that model merging proves beneficial for instilling safety in language models. TIES establishes substantial improvements in harm reduction by around 10% over the 15% Safety Mix.

**Balancing _general_ and _safety_.** We evaluate the model with the best trade-off by considering the average percentage change of both objectives relative to the 15% Safety Mix model. Amongst the four methods evaluated, SLERP proved to be the most effective in balancing the two-fold objective of safety and general performance (see Table 1). Figure 2 shows the outcome of SLERP merging for both SFT and DPO checkpoints against the 15% Safety Mix baseline. The model trained on the 15% Safety Mix demonstrates strong performance on general tasks, achieving win rates of 67.4% for SFT and 71% for DPO. However, we see even greater improvements when merging checkpoints, with win-rates rising to 72.6% and 78%, respectively. We observe similar patterns in safety performance -- the 15% Safety Mix model reduces harm by 56.6% for SFT and 54.7% for DPO. However, by merging checkpoints instead of mixing data, we achieve further reductions, reaching 58.2% for SFT and 57.8% for DPO. Overall, this supports the claim that merging models explicitly trained for different objectives outperforms building data mixtures aimed at the same goals. This is particularly compelling as a technique given previous studies have shown that optimizing for safety in a language model can negatively impact their general-purpose abilities .

Figure 4: Monolingual model merging: We compare mixing vs merging with SFT checkpoints optimized for languages. The “[All]” bars represent model variants with all 6 languages – _English, Hindi, French, Spanish, Arabic_ and _Russian_. “[EN,FR,SP]” represents the pool of _English, French_ and _Spanish_ “monolingual” models. Both metrics are measured with respect to the Aya 23 base model. Lower is better for the left and higher is better for the right.

Figure 3: Comparison between different merging methods across safety and general performance with **DPO checkpoints**. Both metrics are measured with respect to the Aya 23 base model. Lower is better for the left and higher is better for the right. The red dashed line represents the model trained on a mixture of safety and general data (_15% Safety Mix_).

### DPO merge is better than SFT merge

Given the versatility of merging, which can be applied to any grouping of checkpoints -- we compare merging gains when applied to both SFT and DPO (see Table 1). Our experiments show larger consistent improvements when merging DPO checkpoints, with average gains of 2.8% and 2.2% over the base model across the four merging methods assessed for general performance and safety respectively. While merging SFT checkpoints also resulted in significant general performance gains, averaging around 6%, it led to an average increase of 4.6% in harmful generations relative to the 15% Safety Mix model. The best-performing merging approach varies based on the objective (general vs. safety) and underlying training strategy (DPO vs. SFT).

### Uneven gains across languages

In this section, we evaluate how merging methods impact different languages. A detailed examination of Figure 3 reveals that although overall improvements are consistent, the optimal trade-offs for different languages depend on the underlying training regime (DPO vs. SFT) of the model checkpoints used for merging.

**Highest beneficiaries.** For DPO, we find that _Russian_ shows the most successful safety performance with a reduction of 15% over the 15% Safety Mix model with TIES merging. Spanish exhibits the most impressive improvements with around 6% with SLERP over the 15% Safety Mix baseline in general performance. For SFT, _Hindi_ displays the largest reduction in harm (12.14%) with SLERP over the 15% Safety Mix model. However, _Spanish_ continues to reap the most benefits from merging with an improvement of 10% gains in general performance with both Linear and TIES.

**Lowest beneficiaries.** Contrary to the above, when merging DPO-based checkpoints, we surprisingly find _English_ to benefit the least from merging across both axes of performance. We observe an overall decline of 24.87% in safety and 14.5% in general metrics compared to the 15% Safety Mix model with Linear and TIES merging respectively. For SFT checkpoints in the merging pool, we find that _Spanish_ shows the lowest safety performance with TIES with an increase in harmful generations of around 16% while _Hindi_ has the least gains in general performance with DARE-TIES with a decline of about 4% in comparison to the 15% Safety Mix.

It is worth noting that while merging leads to performance degradation in some languages compared to data mixing, it still delivers strong results, maintaining an absolute win-rate above 50% for all languages relative to the base model.

### Merging monolingual models

Given the challenges posed by multilinguality and the linguistic and cultural variability introduced by each language, especially in the backdrop of safety, we aim to study the impact of merging models exclusively grounded in different languages on their downstream performance. For this set of experiments, we fine-tune our base model, Aya 23 8B, on language-specific data maintaining the 15% Safety Mix (SS2.2) and use the resulting checkpoints for merging models across languages. For instance, to obtain a French-only model optimized for both safety and general performance, we fine-tune the model with only French samples, maintaining a 15% mix of safety in the training data. Extending this process for all languages yields 6 separately fine-tuned models on monolingual data.

Additionally, to understand the impact of scaling the number of languages during merging, we combine these models in gradation of two sets: one with 3 languages and another with 6. The 3-language set includes _English, French, Spanish_ chosen for their closer familial ties, and is referred to as the _"[EN,FR,SP]"_ selection. The 6-language set comprises all our target languages -- _English, French, Spanish, Hindi, Arabic_ and _Russian_ -- and is termed _"[All]"_ for conciseness henceforth.

We focus on TIES for this set of experiments because its permutation-invariant nature helps us eliminate additional confounders and isolate the impact of language-based merging on overall performance. We use the same baseline as in previous experiments: a fine-tuned version of Aya 23 on a multilingual 15% Safety Mix. Figure 4 presents the results. We find that when compared to the base model, we successfully increase general performance and reduce harm generations across all variants. Merging 6 monolingual models (_"[All]"_) consistently outperforms the corresponding _"mix"_ baseline, with safety metrics showing harm reductions as high as 6.6% and absolute improvements of 3.8% in general performance. However, we also observe some evidence of cross-lingual interference;merging 3 models (_"[EN,FR,SP]"_) yields better performance on both tasks compared to merging 6 models with differences of approximately 2% in safety and 6% in general performance. These results highlight model merging as an effective method for integrating a diverse set of languages without sacrificing performance on key metrics. However, the choice of languages and the number of models significantly influence the performance gains.

## 4 Related Work

**Model Merging.** Recent research has demonstrated success in developing innovative strategies to harness the collective power of multiple LLMs by suggesting methods for combining their unique strengths. This approach offers an efficient solution and has been widely explored for fine-tuned models sharing the same pre-trained base model, thereby sharing a part of their optimization trajectories . Initial efforts focused on merging models with simple weighted averaging of the parameters  and showed dramatic performance gains for the resultant merged model. More recently, many works have investigated non-linear methods of merging models  while aiming to improve general downstream performance. However, some recent works have focused on ensuring the safety of LLMs when merging, having demonstrated that misalignment transfers trivially from the base to the combined model in this process . Other works "realign" language models by fusing an initial aligned model with many task vectors based on the suitably identified safety subspace . Model merging has also been extended to a multilingual setting - for developing task-solving LLMs for low-resource languages without the availability of SFT data in the target languages . Our work distinguishes itself from prior approaches due to the complexity of the contrasting targets it seeks to satisfy -- balancing safety and general-purpose objectives across a wide set of languages. To the best of our knowledge, no prior work has investigated the alignment of LLMs via model merging in a multilingual context while optimizing for a two-fold objective.

**Multilingual Safety.** With the increased pervasiveness of LLMs in recent times, the landscape of language model research has evolved with a heightened emphasis on safeguarding user experiences, thereby placing an increased focus on mitigating potential risks across diverse linguistic contexts. Several works  have investigated challenges around multilingual jailbreaks, and introduced novel frameworks and datasets for building robust mitigation strategies. Previous work has examined multilingual toxicity mitigation with a detailed comparison between SFT and retrieval-augmented-based methods . It has been shown that LLMs tend to generate more harmful and irrelevant responses in low-resource languages when prompted maliciously . Techniques such as safety context distillation  which harness synthetic data to institute safety guardrails into a model, have shown significant promise towards reducing the harmfulness in model generations. Overall, for a more standardized analysis of safety in multilingual settings, several benchmarks  have been introduced and established in recent times. While methods such as SFT and DPO  have been studied extensively for aligning language models, some recent works have also pivoted towards weight interpolation for the same objective and have demonstrated the effectiveness of adding a safety vector to compromised fine-tuned models for successful realignment . We direct our efforts towards the development of aligned language models by merging a diverse range of languages.

## 5 Conclusion

In this work, we demonstrated the effectiveness of model merging as a potential solution towards building highly-performant aligned language models across a wide range of languages. Through our comprehensive experimentation, we concluded that models obtained as a result of merging exhibit superior performance on the dual axes of safety and general metrics. However, our experiments also revealed that there is variability in the trade-offs established by different merging algorithms, especially in a multilingual context. Additionally, we also demonstrated the success of combining models to extend language coverage while maintaining performance on the relevant metrics.