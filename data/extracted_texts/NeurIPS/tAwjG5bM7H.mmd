# A Bounded Ability Estimation for Computerized Adaptive Testing

Yan Zhuang\({}^{1,2}\), Qi Liu\({}^{1,2}\), GuanHao Zhao\({}^{1,2}\), Zhenya Huang\({}^{1,2}\),

**Weizhe Huang\({}^{1,2}\)**, **Zachary A. Pardos\({}^{3}\)**, **Enhong Chen\({}^{1,2}\)**, **Jinze Wu\({}^{2,4}\)**, **Xin Li\({}^{2,4}\)**

1: Anhui Province Key Laboratory of Big Data Analysis and Application,

University of Science and Technology of China

2: State Key Laboratory of Cognitive Intelligence

3: University of California, Berkeley

4: iFLYTEK Co., Ltd

{zykb,ghzhao0223,hw2871982879,hxwjz}@mail.ustc.edu.cn,

{qiliiuql,huangzhy,cheneh,leexin}@ustc.edu.cn, pardos@berkeley.edu

Corresponding Author.

###### Abstract

Computerized adaptive testing (CAT), as a tool that can efficiently measure student's ability, has been widely used in various standardized tests (e.g., GMAT and GRE). The adaptivity of CAT refers to the selection of the most informative questions for each student, reducing test length. Existing CAT methods do not explicitly target ability estimation accuracy since there is no student's true ability as ground truth; therefore, these methods cannot be guaranteed to make the estimate converge to the true with such limited responses. In this paper, we analyze the statistical properties of estimation and find a theoretical approximation of the true ability: the ability estimated by full responses to question bank. Based on this, a Bounded Ability Estimation framework for CAT (BECAT) is proposed in a data-summary manner, which selects a question subset that closely matches the gradient of the full responses. Thus, we develop an expected gradient difference approximation to design a simple greedy selection algorithm, and show the rigorous theoretical and error upper-bound guarantees of its ability estimate. Experiments on both real-world and synthetic datasets, show that it can reach the same estimation accuracy using 15% less questions on average, significantly reducing test length.

## 1 Introduction

As the landscape of education is changing rapidly, especially after COVID-19, many schools and institutions move from in-class to online platforms, providing individualized education, such as educational measurement and recommendation. They are looking to "right-size" the learning experience of students according to their ability level . To this end, Computerized Adaptive Testing (CAT)  becomes an indispensable tool to efficiently measure student's ability in the areas of standardized testing, computer tutoring, and online courses, through automatically selecting best-suited questions for individual students. Compared with the time-consuming and burdensome paper-and-pencil tests, CAT has been proven to require fewer questions to reach the same measurement accuracy .

A typical CAT system is shown in Figure 1: At test step \(t\), the Cognitive Diagnosis Model, e.g., Item Response Theory (IRT), as the user model based on psychology, first uses student's previous \(t\) item answer responses to estimate his/her current ability \(^{t}\). IRT family has been used for ability estimation in several state assessments, such as OECD/PISA Project . Next, the selection algorithm selects the next item from the entire question bank according to some criteria . Most of them areinformativeness metrics such as selecting the question with difficulty closest to his/her current ability estimate \(^{t}\), i.e., the student's probability of answering it correctly is closest to 50% . Obviously, the selection algorithm is the core component to realize CAT's adaptivity and seeks to answer the following question about _accuracy and efficiency_: Can we estimate student's true ability by asking him/her as few questions as possible, with negligible estimation error?

From the perspective of machine learning, CAT can be viewed as a parameter estimation problem with the least cost: it is essentially to select the fewest data samples (questions to be answered) sequentially from the whole unlabeled data (question bank), so that after obtaining their labels (correct/wrong responses), model's hidden parameters (student true ability \(_{0}\)) can be accurately estimated. Unfortunately, the exact true ability of student is unknown even to the students themselves, thus it is impossible to find such ground truth in datasets to design/train selection algorithms. As a result, most selection algorithms _are not designed explicitly with the goal of accurate and efficient estimation_. Existing approaches either select representative/diverse items solely from question feature space  (deviating from the goal of ability estimation), or require additional training overhead (e.g., Reinforcement Learning-based methods [10; 11; 12; 13]). Although these implicit methods achieve good results in experiments, the theoretical guarantee on approximating student's true ability is also critical for reliable CAT systems especially in standardized tests.

Obviously, the biggest challenge of designing reliable explicit methods is: student's true ability \(_{0}\) is unknown. Therefore, in this work, we propose a general (upper-)Bounded Ability Estimation CAT framework (BECAT), which explicitly targets the accuracy and efficiency of ability estimation. Due to the unknown \(_{0}\), we first find its theoretical approximation \(^{*}\) as the alternative: the ability estimated by his/her full responses on the entire question bank. Hence, our key idea is to select questions such that the estimate can best approximate the ability estimated by full responses. Specifically, we propose an expected gradient difference approximation method based on recent data efficiency/summary technique [14; 15; 16], and design a practical greedy selection algorithm in submodular function, which essentially finds representative items to approximate the gradient of full responses. We further provide the theoretical analysis about its upper-bound of ability estimation error.

To validate BECAT's effectiveness, we conduct experiments on three real-world datasets from different educational platforms. Empirical results show that this simple greedy selection achieves state-of-the-art performance compared with other implicit methods. The main contributions are:

* To better estimate the unknown \(_{0}\), we find its theoretical approximation as the new target for designing an explicit selection algorithm. Based on this, we formally redefine and transform CAT into an adaptive subset selection problem in data summary manner for the first time.
* An effective expected gradient-based selection algorithm is proposed to select appropriate items, which exactly minimizes the estimation error term, therefore admitting theoretical guarantees on ability estimation in CAT systems.
* We show the generality of BECAT -- it can be applied to any gradient-based method, including IRT and neural network methods. We observe that BECAT outperforms existing CAT methods at reducing test length, requiring 10%-20% less questions to reach the same estimation accuracy.

## 2 Problem Definitions of CAT

For accurate and efficient assessment, CAT needs to sequentially select best-fitting questions for each student from the question bank \(Q\); then uses the corresponding responses for ability estimation.

Figure 1: An illustration of the CAT system: At test step \(t[1,...,T]\), the selection algorithm uses the current ability estimate \(^{t}\) to select the next question \(q_{t+1}\) from the question bank. When the test stops, the \(^{T}\) (i.e., the final estimate of his/her true ability \(_{0}\)) will be output.

When the test stops, the final estimate is output as the result/score of this test. The goal of CAT is to accurately estimate examinee's true ability \(_{0}\), while minimizing the number of questions asked .

### Preliminaries

Specifically, at test step \(t[1,2,...,T]\), given the student's previous \(t\) responses \(S_{t}=\{(q_{1},y_{1}),...,(q_{t},y_{t})\}\), where \(\{q_{i}\}_{i=1}^{t} Q\) are selected sequentially by the selection algorithm and \(y\) is the binary outcomes of correct or incorrect; student's current ability can be estimated by minimizing the empirical risk (e.g., binary cross-entropy) from the whole ability space \(\):

\[^{t}=*{arg\,min}_{}_{i S_{t}}l_{i}( )=*{arg\,min}_{}_{i S_{t}}- p_{ }(q_{i},y_{i}),\] (1)

where \(p_{}(q_{i},y_{i})\) represents the probability of the response \((q_{i},y_{i})\) towards a student with \(\), and the specific form of \(p_{}\) is determined by IRT. Since the size of \(S_{t}\) is small, Standard Gradient Descent [18; 19] is sufficient to minimize Eq.(1), and requires the computations of \(_{i S_{t}} l_{i}()\) -- sum of the gradients over the previous \(t\) response data. It takes repeated steps in the opposite direction of the gradient, thus leading to a minimum of the empirical risk in Eq.(1).

Next, the selection algorithm selects the next question \(q_{t+1}\) from bank \(Q\) according to various criteria [7; 8; 12; 13]. The above process will be repeated for \(T\) times2, i.e., \(|S|=T\) (\(T 20\) in most tests ), ensuring the final step estimate \(^{T}\) close to the true \(_{0}\), i.e.,

**Definition 1** (Traditional Definition of CAT).: At each step \(t\), it will select the most suitable/informative question, according to student's current ability \(^{t}\). When the test ends \((t=T)\), the final ability estimate \(^{T}=*{arg\,min}_{}_{i S}l_{i}()\) can approximate the true ability:

\[_{|S|=T}\|^{T}-_{0}\|.\] (2)

Unfortunately, directly solving the above optimization problem is infeasible. Because the ground truth ability \(_{0}\) cannot be obtained and, even students themselves cannot know the exact value. As a result, traditional informativeness-based methods [7; 8] use asymptotic statistical properties of Maximum Likelihood Estimation to reduce estimation uncertainty, e.g., selecting the one whose difficulty is closest to student's current ability \(^{t}\); but they are all IRT-specific, i.e., they can not be applied into recent neural networks methods. Although recent active learning-based  and reinforcement learning-based [12; 13] methods achieve good experimental results, _there is no evidence that they can theoretically guarantee that estimate can efficiently approach \(_{0}\)_, which is unacceptable for CAT systems applied in standardized tests. Testing reliability requires not only satisfactory experimental results, but also good theoretical guarantees .

### New Definition of CAT

Given that there is no such ground truth \(_{0}\) in the dataset, thus, for designing explicit selection algorithms, we find its approximation as the new target.

**Proposition 1**.: _The student's one ability estimate \(^{*}\), estimated by his/her full responses to the entire question bank \(Q\), is an approximation of his/her true ability \(_{0}\), that is,_

\[^{*}_{0}\] (3)

Proof.: When we use consistent estimation approaches, such as Maximum Likelihood Estimation (cross-entropy loss) in Eq.(1), we have \(_{t}p(|^{t}-_{0}| )=0\), where \(t\) is the number of responses (steps) for ability estimation. The size of CAT's question bank is finite (i.e., \(t[0,|Q|]\)) and \(^{*}=_{t|Q|}^{t}\), thus \(^{*}_{t}^{t}_{0}\), i.e., \(^{*}\) can be regarded as an approximation of \(_{0}\). 

Since this proposition exploits estimator's asymptotic property and may make the approximation not perfect. For example, both the bank size \(|Q|\) and various perturbations in student's response can impact this proposition. Thus, we also conduct simulation experiments to further verify it: we randomly sample 100 \(_{0}\) from \(\) as groundtruth, using the smallest EXAM dataset (\(|Q|=1650\)) in Section 4; then use IRT with these \(_{0}\) to simulate the response behavior (correct/wrong) of 100 students. Figure 2(a) shows that when the bank size exceeds 300 (\(|Q|/5\)), the estimated \(^{*}_{0}\) (blue). Even if some extreme perturbations (e.g., guess and slip factors ) are added, their MSE will not exceed 0.1. Therefore, it is reasonable to replace \(_{0}\) with \(^{*}\) as the ground truth in optimization.

In this way, the selection algorithm can aim to approach \(^{*}\) instead of the unknown \(_{0}\). We can design explicit selection algorithms: _Select a subset of questions \(S\) from the bank \(Q\), so that the student's ability is estimated only on the subset \(S\) while still (approximately) converging to the optimal solution \(^{*}\)_ (i.e., the estimate that would be obtained if optimizing on the full responses to \(Q\)). As mentioned in Preliminaries, the ability estimation usually adopts cross-entropy loss with gradient computations, and denote the full gradient of the loss w.r.t. ability parameters by \(_{i Q} l_{i}()\) -- sum of the gradients over full responses. Thus,

**Definition 2** (New Definition of CAT).: It will adaptively find a subset \(S\) of size \(T\) and the corresponding weight \(\{\}_{j}\) that approximates the full gradient: minimizing their difference for all the possible ability values of the optimization parameter \(\) :

\[_{|S|=T}\|^{T}-^{*}\|_{|S|=T}_{ }\|_{j S}_{j} l_{j}()-_{i Q} l_{i} ()\|.\] (4)

Since we know nothing about the range of student's ability when optimizing, we consider the worst-case approximation error (\(_{}\)) instead of a particular \(\). After finding such \(S\) and the associated weights \(\{\}_{j}\), _the gradient updates on \(S\) will be similar to the gradient on \(Q\) regardless of the value of \(\), thus making the estimate close to the target \(^{*}\)_. In this way, CAT can be regarded as a subset selection optimization problem in a data-efficiency manner. Also, we find that it is consistent with recent Coreset techniques , which approximate the gradients of the full data, so that the model is trained only on the subset while still (approximately) converging to the optimal solution (i.e., the model parameters that would be obtained if training on the full data).

However, compared with the traditional Coreset problem, the biggest technical challenge is: the gradients on bank (\(_{i Q} l_{i}()\)) cannot be calculated without labels. Only the few questions that have been answered in previous steps (i.e., \(S_{t}\)) have the corresponding labels. Thus, to simplify the problem, we _assume for the moment that student's full responses are available_. In Section 3.1, we will further propose an expected approximate method to address this.

Figure 2: (a) Simulation experiments about Proposition 1 using MSE: \([\|^{*}-_{0}\|^{2}]\). In addition to the normal situation (blue), we also show the MSE under different perturbations, for example: Slip5% means that the label has a 5% probability of changing from 1 to 0; Guess25% means that the label changes from 0 to 1 with 25%. (b) The illustration of the optimization problem: selecting subset \(S\) to _cover_ the whole response data on \(Q\). The rectangles represent a student’s full responses to the bank \(Q\), and \(w(i,j)\) measures the similarity of response pair \((i,j)\).

The BECAT framework

In this section, to solve the above optimization problem Eq.(4), we design a simple greedy algorithm in submodular functions. More importantly, we provide an upper bound on the expected error of the ability estimate when using our method.

Optimization.The above subset selection problem is NP-hard, thus, we transform it based on the recent Coreset method. It proves that the subset \(S\) that minimizes the error of estimating the full gradient is upper-bounded by a submodular facility location function that has been used in various summarization applications [22; 23]. Thus

\[_{|S|=T}_{}\|_{j S}_{j} l _{j}()-_{i Q} l_{i}()\| _{|S|=T}_{}_{i Q}_{j  S}\| l_{i}()- l_{j}()\|\] \[_{|S|=T}_{i Q}_{j S}w(i,j),\] (5)

where \(w(i,j) d-_{}\| l_{i}()- l_{j}( )\|\) is the gradient similarity between response pair \(i=(q_{i},y_{i})\) and \(j=(q_{j},y_{j})\) for this student. The associated weight of the response \(j\), \(_{j}=_{i Q}[j=_{s S}w(i,s)]\), is the number of responses in \(Q\) that are most similar to \(j S\).

Given a subset \(S\), \(_{i Q}_{j S}w(i,j)\) in Eq.(5) _quantifies the coverage of the whole response data on \(Q\)_, by summing the similarities \(w\) between every \(i Q\) and its closest item \(j S\). The semantics of this optimization problem is shown in Figure 2(b). The larger the value of \(w(i,j)\), the smaller their gradient difference in ability estimation for all the possible ability \(\), which means these two responses \(i\) and \(j\) have _similar importance/influence on the student's ability estimation_. Thus, the transformed problem in Eq.(5) is equivalent to selecting the most representative responses to form the subset \(S\), which shares the same idea (i.e., selecting "representative" items) with previous selection algorithms [12; 9], active learning methods [24; 25] and unsupervised learning [26; 27].

Define a monotone non-decreasing submodular function -- the facility location function \(F:2^{Q}\): \(F(S)=_{i Q}_{j S}w(i,j)\). The submodular optimization provides a near-optimal solution with a \((1-1/e)\)-approximation bound , with simple greedy algorithm for selecting the \(t\)-th question:

\[q_{t}=_{(q,y) Q S_{t-1}}((q,y)|S_{t-1}).\] (6)

where \(((q,y)|S_{t-1})=F(\{(q,y)\} S_{t-1})-F(S_{t-1})\) and \(S_{t-1}\) is the set of previous \(t-1\) responses of this student in CAT.

### Expected Gradient Difference Approximation

However, the above selection algorithm is _impractical_ in CAT. Because we cannot get student's full responses to bank \(Q\), as a result, the gradient difference \(\| l_{i}()- l_{j}()\|\) in \(w(i,j)\) can not be calculated without related answer correctness labels. Actually, at step \(t\), only the responses of previous \(t-1\) steps (i.e., \(S_{t-1}\)) can be obtained. Therefore, we propose an expected gradient difference approximation method to replace the original to measure their similarity, then the new similarity function \((i,j)\) is:

\[(i,j) d-_{}_{y p_{  t}}[\| l_{i}()- l_{j}()\|],\] (7)

where the normed gradient difference is calculated as an expectation \(_{y p_{ t}}\) over the possible labelings, since student's response labels \(y\) to the candidate questions are unknown in the selection step. Moreover, for more accurate approximation and to make full use of the available previous \(t-1\) responses, in Eq.(7), the expectation is determined by the current estimate \(^{t}\). This method can be regarded as a gradient difference approximation based on "soft pseudo-labels". Thus, the selection of the next question \(q_{t}\) no longer requires the student's real answer correctness labels:

\[q_{t}=_{q Q S_{t-1}}(q|S_{t-1}).\] (8)

where \((q|S_{t-1})=(\{q\} S_{t-1})-(S_{t-1})\), and \((S)=_{i Q}_{j S}\ (i,j)\). Also, we uncover some important conclusions about this simple expected approximation:

**Lemma 1**.: _When we replace the original gradient difference in \(w(i,j)\) with \((i,j)\), the corresponding designed selection algorithm using submodular function \(\) is actually approximately solving the following optimization problem:_

\[_{|S|=T}_{}_{y}[\|_{j S}_{j}  l_{j}()-_{i Q} l_{i}()\|]\] (9)

Based on the conclusion in Lemma 1, we can assume that, after optimization, the preconditioned expected gradient can be approximated by an error of at most \(\): \(_{y}[\|_{i S}_{j} l_{j}()-_{i Q } l_{i}()\|]\). Then we find the theoretical guarantees for ability estimation when applying gradient-based estimation method to the subset \(S\) found by it:

**Theorem 1** (Expected estimation error bound).: _Assume that the loss function for ability estimation is \(\)-strongly convex (e.g., IRT). Let \(S\) be a weighted subset obtained by the proposed method. Then with learning rate \(\), ability estimation in gradient descent applied to the subsets has the following expected estimation error bound:_

\[[\|^{t+1}-^{*}\|^{2}]^{2}+2_{f}D H_{p}(^{t},^{* })}{^{2}}\] (10) \[where H_{p}(^{t},^{*})=_{(q,y) p _{^{t}}}[}(q,y)}]\] (11)

_where \(^{*}\) is the optimal estimate using full responses, \(\) is an upper bound on the norm of the gradients, and \(D=_{}\|-^{*}\|\)._

All the proofs can be found in Appendix. The above theorem shows that despite not being able to obtain student's full response, _such simple expected gradient difference approximation can make the estimate error upper bounded at each step_. This theorem is attainable for the case where the loss is strongly convex, such as the cross-entropy loss of the classic L2-regularized IRT . We will further verify the performance of other cases (e.g., neural network-based methods) in experiments.

Theorem 1 also suggests that, to minimize the expected error bound, the CAT systems should try to minimize \(H_{p}(^{t},^{*})\) that can be regarded as a type of statistical distance: measuring how probability distribution \(p_{^{t}}\) is different from \(p_{^{*}}\). Moreover, we find that with the help of the consistency estimation (i.e., binary cross-entropy) at each step, \(H_{p}(^{t},^{*})\) can reach its theoretical minimum:

**Theorem 2**.: _Assume that \(^{t}\), estimated by the cross-entropy loss in Eq.(1), can minimize the empirical risk i.e., \(_{i S_{t}}l_{i}(^{t})=0\). Then \(H_{p}(,^{*})\) can take its minimum when \(=^{t}\), that is_

\[H_{p}(^{t},^{*}) H_{p}(,^{*}),\] (12)

Therefore, the ability estimation methods commonly used in CAT can actually help minimize this upper bound. The proofs and related experiments can be found in the Appendix Cand E.4.

Complexity Analysis of BECAT.Algorithm 1 presents the pseudo-code of our BECAT framework. A naive implementation of our selection algorithm in Eq.(8) has the complexity of \(O(|Q|^{2}||)\), because at each step we have to: (1) find the question from the bank (\(O(|Q|)\)) that (2) maximizesthe marginal gain \((q|S_{t-1})=(\{q\} S_{t-1})-(S_{t-1})\) with complexity \(O(|Q|||)\). To make BECAT faster and more scalable from the above two aspects, we adopt two speed-up tricks: lazy evaluations [30; 31] and multifaceted estimation  (See Appendix D for implementation details). Also, we compare the time (second) spent on question selection by different methods in Appendix E.

## 4 Experiments

Evaluation Method.The goal of CAT is to estimate the student's ability accurately with the fewest steps. Therefore, there are usually two tasks to verify the performance of different CAT methods following prior works [9; 12]: **(1) Student Score Prediction**: To evaluate the ability estimate output by CAT, the estimate can be used for predicting the student's binary response (correct/wrong) on the questions he/she has answered in the held-out response data. Thus, Prediction Accuracy (ACC) and AUC are used for evaluations ; **(2) Simulation of Ability Estimation**: This is CAT's traditional evaluation methods. Since the ground truth of student ability \(_{0}\) is not available, we artificially generate the \(_{0}\) and further simulate student-question interaction process. Thus, we can use Mean Square Error (MSE) metric. See Appendix E for the details of these two evaluation methods.

Datasets.We conduct experiments on three educational benchmark datasets, namely ASSIST, NIPS-EDU, and EXAM. ASSIST  is collected from an online educational system ASSISTments and consists of students' practice logs on mathematics. NIPS-EDU  refers to the large-scale dataset in NeurIPS 2020 Education Challenge, which is collected from students' answers to questions from Eedi (an educational platform). The EXAM dataset was supplied by iFLYTEK Co., Ltd., which collected the records of junior high school students on mathematical exams. The statistics of the datasets are shown in appendix. The code can be found in the github: https://github.com/bigdata-ustc/EduCAT.

Compared Approaches.To verify the generality of BECAT, in addition to the traditional **IRT**, we also compare the neural network-based model **NeuralCDM**: It can cover many IRT and cognitive diagnosis models, such as MIRT  and MF [38; 39]. For the selection algorithm, we mainly use the following SOTA algorithms as baselines: **Random**: The random selection strategy is a benchmark to quantify the improvement of other methods; **FSI** and **KLI** select the question with the maximum Fisher/Kullback-Leibler information, which measures the amount of information that a question carries about the unknown parameter \(\). They are specially designed for IRT. **MAAT** utilizes Active Learning  to measure the uncertainty caused by each candidate question. **BOBCAT** and **NCAT** recast CAT as a bilevel optimization and Reinforcement Learning problem respectively, and train a data-driven selection algorithm from student response data.

### Results and Discussion

In this section, we compare the performance on two classic CAT tasks introduced above to evaluate the effectiveness and efficiency of our proposed BECAT framework. Also, we conduct a qualitative investigation of the characteristics of the selected questions, and gain deeper insights on why BECAT leads to more accurate ability estimation.

Task1: Student Score Prediction.Following prior work , we also fix the max length \(T=20\) and calculate the ACC and AUC at step 5, 10 and 20 on three datasets for Student Score Prediction task and the results are shown in Table 3. We find that:

(1) The explicit BECAT framework achieves the best overall performances on the three datasets. It performs significantly better than all the other methods, where the relative performance improvements are as least 1.5% with respect to ACC@20 and 1.1% with respect to AUC@20 on average on ASSIST. This result indicates that BECAT can provide accurate ability estimates at the end of the exam. Also, it even surpasses the implicit selection algorithms based on deep learning, such as NCAT and BOBCAT. This phenomenon shows that _compared to focusing on modeling complex student-question interactions, targeting the accuracy of estimation indeed achieves amazing results_.

(2) BECAT's performance on large-scale datasets (e.g., NIPS-EDU) is better. From Table 3, on NIPS-EDU dataset (the bank size is 27613), BECAT can achieve 2.48% AUC gain (on average) above the famous FSI baseline. On the other two datasets ASSIST and EXAM, the average improvement is only 0.32%. This finding inspires us: BECAT is more adaptable to practical large-scale testing situations, and can retrieve the most suitable questions from the massive candidate questions. However, it cannot be ignored that the _BECAT cannot surpass all other methods at the beginning of the exam_. For example, on NIPS-EDU dataset, it is about 2.53% behind BOBCAT on ACC@5. This is because the student's response data available in the initial stage of exam is limited, and the data-driven methods (e.g., BOBCAT and NCAT) can be pre-trained on large-scale student response datasets to learn the interaction patterns, thus addressing this cold-start problem . Thus, adapting the proposed explicit algorithm to data-driven frameworks is a very promising future work.

Task 2: Simulation of Ability Estimation.The goal of a practical CAT system is to accurately estimate student's ability. We conduct the Simulation of Ability Estimation experiment on the EXAM dataset using the mean square error \([\|^{t}-_{0}\|^{2}]\) between the ability estimate \(^{t}\) and the true ability \(_{0}\) at each step. Figure 3(a) reports the results of different methods on IRT. As the number of questions selected increases, we find that the BECAT method can always achieve much lower estimation errors, especially in the middle stage. Some implicit methods that do not aim at estimation accuracy perform better in the initial stage (e.g., NCAT), but the final accuracy still lags behind BECAT framework. Also, compared with the widely used FSI, the proposed BECAT can reach the same estimation error using up to 20% less questions. On average, it can reach the same estimation accuracy using 15% less questions, which demonstrates its efficiency in ability estimation, i.e., reducing test length.

The Characteristics of the Selected Questions.To gain deeper insights on why BECAT leads to more accurate estimation, we take a close look at the characteristics of the selected questions. First, for IRT, we output the difficulty and discrimination parameters of the selected questions and draw a scatter chart in Figure 3(b). We find that it tends to choose those questions with high discrimination,

Table 1: The performance of different methods on Student Score Prediction with ACC and AUC metrics. “\(-\)” indicates the information/uncertainty-based selection algorithms (e.g., FSI) cannot be applied to the deep learning method. The boldfaced indicates the statistically significant improvements (p-value < 0.01) over the best baseline.

and their difficulty is scattered and roughly concentrated in the middle difficulty area, which may be caused by the fact that most of the students are of middle-ability . Second, for NeuralCDM, to gain a better insight into the knowledge concepts (e.g., Geometry in mathematics) covered by the selected questions, and the association between BECAT and other methods. Figure 3(c) shows the Jaccard similarity coefficient of questions' concepts. Questions selected by the same type of method have a high overlap in knowledge concepts, such as FSI and KLI, BOBCAT and NCAT. MAAT and FSI have the highest similarity scores with BECAT: 1) Although BECAT does not directly adopt the concept features in the selection, it has a high score to MAAT that directly targets knowledge concept coverage/diversity, thus making the measurement more comprehensive. 2) The high similarity (with FSI) proves that BECAT is not only general but also capable of selecting informative items.

## 5 Related Works

Computerized Adaptive TestingComputerized Adaptive Testing (CAT) technology has been widely used in many standardized tests, such as GMAT, and the multistage testing in GRE is also its special case of CAT . It is an iterative procedure, mainly including Item Response Theory and a question selection algorithm. The following reviews these two components separately:

_(1) Item Response Theory (IRT)._ It is built on psychometric theory and has become popular in educational assessment to provide more individualized feedback about a student's latent ability . It assumes that the examinee's ability is unchanged throughout a test, thus the ability can be estimated using his/her previous response on questions in gradient-based optimization . The classic form is the two-parameter logistic (2PL): \(p\)(the response to question \(j\) is correct) \(=\)\(sigmoid(a_{j}(-b_{j}))\), where \(a_{j},b_{j}\) represent each question's discrimination and difficulty respectively that are pre-calibrated before testing , and \(\) is student's ability to be estimated. Recently, many studies  combine cognitive diagnosis and utilize neural networks to model such student-question interaction (e.g., NeuralCDM ).

_(2) Selection Algorithms._ The selection algorithm is the core component to realize CAT's adaptivity - accurately estimating student's ability with the fewest test steps. Traditional algorithms are based on some uncertainty or information metrics, e.g., the famous Fisher Information (FSI). Based on it, many methods  have been proposed to introduce additional information in selection. Since they are not general and not applicable to recent neural network methods, MAAT  uses active learning to select diverse and representative items in question's feature space. Recently, BOBCAT  and NCAT  regard CAT as a Reinforcement Learning (RL) problem and train selection algorithms directly from large-scale student response data. Due to the unknown of the \(_{0}\), their goal is to minimize the student performance prediction loss of the estimate on the held-out responses data, which is also implicit and prone to biases in training data. In this paper, BECAT is general and explicitly targets the accuracy and efficiency of ability estimation. Compared with previous implicit methods, we find that it exhibits superior performance both theoretically and experimentally. However, various biases, such as those introduced in test item design and respondent pool selection, can affect the validity of estimating a student's true ability . While our approach seeks to improve

Figure 3: (a) The error of ability estimation on EXAM dataset. (b) The characteristics (i.e., discrimination and difficulty) of the questions selected for 10 students in IRT, where grey dots represent all the questions in the bank, and the “\(\)” represent the ones selected by BECAT. (c) The Jaccard similarity coefficient of the selected questions.

the efficiency with which student ability is estimated, it does not diminish the need for test designers to mitigate sources of bias introduced outside of the model fitting process.

Data Efficiency.Another closely related literature is data efficiency (or data summary) . To alleviate various costs (computational  or labeling costs ), data efficiency is used to carefully select or generate some samples from dataset on par with the full data. Its specific implementation methods include Coreset , Active Learning , Data Distillation , etc. For example, recent Coreset approaches  try to find a subset that closely approximates the full gradient, i.e., the sum of the gradients of the whole training samples. In this paper, Coreset helps us transform our optimization problem in Section 2.2, but the gradient calculation requires labels, which is obviously not applicable to the CAT scenario (student's response labels cannot be obtained before the question selection). Therefore, we improve it and design an _expected gradient difference approximation_ method and provide good upper-bound guarantees to the optimal solution, which is one of the main contributions of this paper.

## 6 Conclusion

This paper focuses on the explicit approach for accurate and efficient estimation of student's true ability \(_{0}\). Given that the ground truth \(_{0}\) is unavailable, we find its theoretical approximation: the ability estimated by the full responses to the question bank, and use it as the optimization goal to design a Bounded Ability Estimation CAT framework (BECAT). For practical use in CAT scenario, we propose a simple but effective expected gradient difference approximation in the greedy selection algorithm. We further analyze its theoretical properties and prove the error upper-bound of the ability estimation on questions found by BECAT. Through extensive experiments on three real-world education datasets, we demonstrate that BECAT can achieve the best estimation accuracy and outperform existing CAT methods at reducing test length.