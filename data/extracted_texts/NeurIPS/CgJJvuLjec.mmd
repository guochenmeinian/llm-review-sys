# PAPR: Proximity Attention Point Rendering

Yanshu Zhang, Shichong Peng, Alireza Moazeni, Ke Li

APEX Lab

School of Computing Science

Simon Fraser University

{yanshu_zhang,shichong_peng,seyed_aliireza_moazenipourasil,keli}@sfu.ca

Denotes equal contribution

###### Abstract

Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning. Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture. To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer. Our scene representation uses a point cloud where each point is characterized by its spatial position, influence score, and view-independent feature vector. The renderer selects the relevant points for each ray and produces accurate colours using their associated features. PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry. Notably, our method captures fine texture details while using only a parsimonious set of points. We also demonstrate four practical applications of our method: zero-shot geometry editing, object manipulation, texture transfer, and exposure control. More results and code are available on our project website.

## 1 Introduction

Learning 3D representations is crucial for computer vision and graphics applications. Recent neural rendering methods  leverage deep neural networks within the rendering pipelines to capture complex geometry and texture details from multi-view RGB images. These methods have many practical applications, such as generating high-quality 3D models  and enabling interactive virtual reality experiences . However, balancing representation capacity and computational complexity remains a challenge. Large-scale representations offer better quality but demand more resources. In contrast, parsimonious representations strike a balance between efficiency and quality, enabling efficient learning, processing, and manipulation of 3D data.

3D representations can be categorized into volumetric and surface representations. Recent advances in volumetric representations  have shown impressive results in rendering quality. However, the cubic growth in encoded information as the scene radius increases makes volumetric representations computationally expensive to process. For example, to render a volumetric representation, all the information along a ray needs to be aggregated, which requires evaluating many samples along each ray. In contrast, surface representations are more parsimonious and efficient since the encoded information grows quadratically. So surface representations can be rendered with much fewer samples.

Efficient surface representations include meshes and surface point clouds. Meshes are difficult to learn from scratch, since many constraints (e.g., no self-intersection) need to be enforced. Additionally, the topology of the initial mesh is fixed, making it impossible to change during training . In contrast,point clouds offer more flexibility in modelling shapes with varying topologies. Hence, in this work, we focus on using point clouds to represent scene surfaces.

Learning a scene representation from scratch requires a differentiable renderer that can pass large gradients to the representation. Designing a differentiable renderer for point clouds is non-trivial - each point is infinitesimal and rarely intersects with rays, it is unclear what the output colour at such rays should be. Previous methods often rely on splat-based rasterization techniques, where points are turned into disks or spheres [45; 15; 35; 53; 11; 56]. These methods use a radial basis function (RBF) kernel to calculate the contribution of each point to each pixel. However, determining the optimal radius for the splats or the RBF kernel is challenging , and these methods suffer from the issue of vanishing gradient when the ray is far away from the points, limiting their ability to learn the ground truth geometry that drastically differs from the initial geometry. Additionally, the radius of each splat should be small for accurate texture modelling, and this would require a point cloud with a large number of points, which are difficult to process.

To address these limitations, we introduce a novel method called Proximity Attention Point Rendering (PAPR). PAPR consists of a point-based scene representation and a differentiable renderer. The scene representation is constructed using a point cloud, where each point is defined by its spatial position, an influence score indicating its influence on the rendering, and a view-independent feature vector that captures the local scene content. Our renderer learns to directly select points for each ray and combines them to generate the correct colour using their associated features. Remarkably, PAPR can learn accurate geometry and texture details using a parsimonious set of points, even when the initial point cloud substantially differs from the target geometry, as shown in Figure 1. We show that the proposed ray-dependent point embedding design is crucial for effective learning of point cloud positions from scratch. Furthermore, our experiments on both synthetic and real-world datasets demonstrate that PAPR outperforms prior point-based methods in terms of image quality when using a parsimonious set of points. We also showcase four practical applications of PAPR: zero-shot geometry editing with part rotations and deformations, object duplication and deletion, texture transfer and exposure control. In summary, our contributions are as follows:

1. We propose PAPR, a novel point-based scene representation and rendering method that can learn point-based surface geometry from scratch.
2. We demonstrate PAPR's ability to capture correct scene geometry and accurate texture details using a parsimonious set of points, leading to an efficient and effective representation.
3. We explore and demonstrate four practical applications with PAPR: zero-shot geometry editing, object manipulation, texture transfer and exposure control.

## 2 Related Work

Our work focuses on learning and rendering 3D representation, and it is most related to differentiable rendering, neural rendering and point-based rendering. While there are many relevant works in these fields, we only discuss the most pertinent ones in this context and refer readers to recent surveys [9; 41; 47] for a more comprehensive overview.

Figure 1: Our proposed method, PAPR, jointly learns a point-based scene representation and a differentiable renderer from scratch using only RGB supervision. PAPR effectively learns the point positions (a) that represent the correct surface geometry, as demonstrated by the depth map (b). Notably, PAPR achieves this even when starting from an initialization that substantially deviates from the target geometry. Furthermore, PAPR learns a view-independent feature vector for each point, capturing the local scene content. The clustering of point feature vectors is shown in (c). The renderer selects and combines these feature vectors to generate high-quality colour rendering (d).

Differentiable Rendering For 3D Representation LearningDifferentiable rendering plays a crucial role in learning 3D representations from data as it allows gradients to be backpropagated from the rendered output to the representation. Early mesh learning approach, OpenDR , approximate gradients using localized Taylor expansion and differentiable filters, but do not fully leverage loss function gradients. Neural Mesh Renderer  proposes non-local approximated gradients that utilize gradients backpropagated from a loss function. Some methods [33; 18; 27] modify the forward rasterization step to make the pipeline differentiable by softening object boundaries or making the contribution of triangles to each pixel probabilistic. Several point cloud learning methods, such as differentiable surface splatting  and differentiable Poisson solvers , explore surface reconstruction techniques but they do not focus on rendering fine textures. In contrast, our method jointly learns detailed colour appearance and geometry representations from scratch.

Neural Scene RepresentationIn recent years, there has been a growing interest in utilizing neural networks to generate realistic novel views of scenes. Some approaches utilize explicit representations like meshes [42; 49], multi-plane images [5; 20; 21; 36; 55], and point clouds [1; 30; 15]. Another approach represents the scene as a differentiable density field, as demonstrated by Neural Radiance Field (NeRF) . However, NeRF suffers from slow sampling speed due to volumetric ray marching. Subsequent works have focused on improving training and rendering speed through space discretization [39; 6; 8; 4; 31; 51; 46] and storage optimizations [23; 38; 50; 37; 17]. These methods still rely on volumetric representations, which have a cubic growth in encoded information. In contrast, our proposed method leverages surface representations, which are more efficient and parsimonious.

Point-based Representation Rendering and LearningRendering point clouds using ray tracing is challenging because they do not inherently define a surface, leading to an ill-defined intersection between rays and the point cloud. Early approaches used splatting techniques with disks, ellipsoids, or surfels [57; 32; 2; 28]. Recent methods [1; 30; 24; 14] incorporate neural networks in the rendering pipeline but assume ground truth point clouds from Structure-from-Motion (SfM), Multi-View Stereo (MVS) or LiDAR scanning without optimizing point positions. To learn the point positions, splat-based rasterization methods [45; 15; 35; 53; 11; 56] employ radial basis function kernels to compute point contributions to pixels but struggle with finding optimal splat or kernel radius  and suffer from gradient vanishing when points are far from target pixels. Consequently, these methods require a large number of points to accurately model scene details and are limited to learn small point cloud deformation. In contrast, our method can capture scene details using a parsimonious set of points and perform substantial deformation in the initial point cloud to match the correct geometry.

Another approach  predicts explicit intersection points between rays and optimal meshes reconstructed from point clouds, but it requires auxiliary data with ground truth geometry for learning and RGBD images for point cloud initialization. Our method overcomes these limitations. Xu et al.  represent radiance fields with points and use volumetric ray marching, our approach utilizes more efficient and parsimonious surface representations and generates output colour for each ray with only a single forward pass through the network.

## 3 Proximity Attention Point Rendering

Figure 2 provides an overview of our proposed end-to-end learnable point-based rendering method. The input to our model includes a set of RGB images along with their corresponding camera intrinsics and extrinsics. Our method jointly learns a point-based scene representation, where each point comprises a spatial position, an influence score, and a view-independent feature vector, along with a differentiable renderer. This is achieved by minimizing the reconstruction loss between the rendered image \(}\) and the ground truth image \(_{gt}\) according to some distance metric \(d(,)\), w.r.t. the scene representation and the parameters of the differentiable renderer. Unlike previous point cloud learning methods [48; 45; 15; 35; 53; 11; 3; 56] which rely on multi-view stereo (MVS), Structure-from-Motion (SfM), depth measurements or object masks for initializing the point cloud positions, our method can learn the point positions from scratch, even when the initialization significantly differs from the target geometry as demonstrated in Figure 4. In the following sections, we provide a more detailed description of each component of our proposed method.

### Point-based Scene Representation

Our scene representation consists of a set of \(N\) points \(P=\{(_{i},_{i},_{i})\}_{i=1}^{N}\) where each point \(i\) is characterized by its spatial position \(_{i}^{3}\), an individual view-independent feature vector \(_{i}^{h}\) that encodes local appearance and geometry and an influence score \(_{i}\) that represents the influence of the point on the rendering.

Notably, we make the feature vector for each point to be independently learnable, thereby allowing the content encoded in the features to be independent of the distance between neighbouring points. As a result, after training, points can be moved to different positions without needing to make a corresponding change in the features. This design also makes it possible to increase the fidelity of the representation by adding more points, thereby capturing more fine-grained details. Additionally, we specifically choose the feature vectors to be view-independent, ensuring that the encoded information in the representation remains invariant to view changes.

### Differentiable Point Rendering with Relative Distances

To learn the scene representation from images from different views, we minimize the reconstruction loss between the rendered output from those views and the ground truth image w.r.t. the scene representation. Typically, we do so with gradient-based optimization methods, which need to compute

Figure 3: Comparison of point contributions in splat-based rasterization and the proposed method. Three points are projected onto the image plane, with the ground truth image shown in the background. The bar charts illustrate the contribution of each point towards a specific pixel, represented by the red star. (a) If the points are splats with hard boundaries, none of them intersect with the given pixel, resulting in zero contributions. Consequently, the gradient of the loss at the pixel w.r.t. each point is non-existent, hindering the reduction of loss at that pixel. (b) For splats with soft boundaries, although the point contributions are non-zero, they are extremely small, leading to vanishing gradients. (c) Our proposed method normalizes the contribution of all points to the given pixel. This ensures that there are always points with substantial contributions, enabling learning point positions from scratch.

Figure 2: An overview of the proposed pipeline for rendering a point-based scene representation, where each point is defined by its spatial position, an influence score, and a view-independent feature vector. (a) Given a ray, ray-dependent features are created for each point. These features are used to generate the key, value, and query inputs for a proximity attention model. (b) The attention model selects the points based on their keys and the query, and combines their values to form an aggregated feature. (c) The aggregated feature is gathered for all pixels to create a feature map. This feature map is then passed through a feature renderer, implemented using a UNet architecture, to produce the output image. The entire pipeline is jointly trained in an end-to-end manner using only supervision from the ground truth image.

the gradient of the loss w.r.t. the scene representation. For this to be possible, the rendered output must be differentiable w.r.t. the representation - in other words, the renderer must be differentiable.

In classical rendering pipelines, point clouds are often rendered with splat-based rasterization. However, when attempting to learn point clouds from scratch using this method, several challenges arise for the following reasons. A splat only contributes to the pixels it intersects with, so when a splat does not intersect with a specific pixel, it does not contribute to the rendered output at that pixel. As a result, the gradient of the loss at that pixel w.r.t. the position of the splat is zero. Now, consider a pixel in the ground truth image that no splat intersects with. Even though the loss at that pixel might be high, the gradient w.r.t. the position of every splat is zero, so the loss at that pixel cannot be minimized, as illustrated in Figure 2(a).

To address this issue, prior methods have employed a radial basis function (RBF) kernel to soften splat boundaries. This approach makes the contribution of each splat to each pixel non-zero. However, even with the soft boundaries, the contribution of a splat to a pixel remains small if the pixel is located far away from the centre of the splat. Consequently, for a pixel far away from the centres of all splats, the gradient of the loss at the pixel w.r.t. each splat remains too minuscule to exert a significant influence on its position, as shown in Figure 2(b) - in other words, the gradient vanishes. As a result, even though the loss at the pixel might be high, it will take a long time before it is minimized.

We observe that with both hard- and soft-boundary splats, the contribution of each point to a given pixel only depends on its own _absolute distance_ to the pixel, without regard to the distances of other points. This gives rise to the vanishing gradient problem because the absolute distances for all points can be high. To address this issue, we make the contributions per point towards a pixel depend on _relative_ distances rather than _absolute_ distances. Instead of relying solely on the distance to the pixel from a single point to compute its contribution, we normalize the contributions across all points so that the total contribution from all points is always 1. This ensures that there are always points with significant contributions to each pixel, even if the pixel is far away from all points, as shown in Figure 2(c).

This allows points to be moved far from their initial positions, so the initialization can differ substantially from the ground truth scene geometry. This also means that we can move sufficiently many points from elsewhere to represent the scene geometry faithfully. On the other hand, the learnt representation uses no more points than necessary to represent each local part of the scene geometry faithfully, because if there were extraneous points, they could be moved to better represent other parts. The end result is a parsimonious scene representation that uses just enough points to faithfully represent each part of the scene geometry. So, fewer points tend to be allocated to smooth parts of the surface and the interior of opaque surfaces, because having more points there would not improve the rendering quality significantly. Moreover, since the points can be far away from a pixel without having their contributions diminished, we can preserve the surface continuity even if we apply a non-volume preserving transformation (e.g., stretching) to the point cloud post-hoc.

### Implementation of Relative Distances with Proximity Attention

We design an attention mechanism to compute contributions from relative distances, which we dub _proximity attention_. Unlike typical attention mechanisms, the queries correspond to rays and the keys correspond to points. To encode the relationship between points and viewing directions, we design an embedding of the points that account for their relative positions to rays. This embedding is then fed in as input to the proximity attention mechanism. We describe the details of each below.

Ray-dependent Point EmbeddingGiven a camera pose \(\) specified by its extrinsics and intrinsics, and a sensor resolution \(H W\), we utilize a pinhole camera model to cast a ray \(_{j}\) from the camera centre to each pixel. Each ray \(_{j}\) is characterized by its origin \(_{j}^{3}\) and unit-length viewing direction \(_{j}^{3}\), both defined in the world coordinate system.

To obtain the ray-dependent point embedding of point \(_{i}\) with respect to ray \(_{j}\), we start by finding the projection \(^{}_{i}\) of the point onto the ray:

\[^{}_{i}=_{j}+_{i}-_{j}, _{j}_{j}\] (1)

where '\(,\)' denotes the inner product. Next, we compute the displacement vectors, \(_{i,j}\) and \(_{i,j}\):

\[_{i,j}=^{}_{i}-_{j},\ _{i,j}= _{i}-^{}_{i},\] (2)Here, \(_{i,j}\) captures the depth of the point \(_{i}\) with respect to the camera centre \(_{j}\) along the ray, while \(_{i,j}\) represents the perpendicular displacement from the ray to the point \(_{i}\). The final ray-dependent point embedding contains these displacement vectors, \(_{i,j}\) and \(_{i,j}\), as well as the point position \(_{i}\).

Proximity AttentionFor a given ray \(_{j}\), we start by filtering the top \(K\) nearest points \(_{i}\) based on the magnitude of their perpendicular displacement vector \(||_{i,j}||\). This allows us to identify the points that are most relevant for modelling the local contents of the ray.

We adopt the ray-dependent point embedding method described earlier for the \(K\) closest points, which serves as the input to construct the key in the attention mechanism. For the value branch input, we utilize the \(K\) associated point feature vectors \(_{i}\), along with their displacement vectors \(_{i,j}\) and \(_{i,j}\). For the input to the query branch, we use the ray direction \(_{j}\). Additionally, we apply positional encoding [22; 40]\((p)=[(2^{l} p),(2^{l} p) ]_{\!=\!0}^{L=6}\) to all components except for the feature vector. These inputs pass through three separate embedding networks \(f_{_{K}}\), \(f_{_{V}}\) and \(f_{_{Q}}\), which are implemented as multi-layer perceptrons (MLPs). The resulting outputs form the final key, value, and query for the attention mechanism:

\[_{i,j} =f_{_{K}}([(_{i,j}), (_{i,j}),(_{i})])\] (3) \[_{i,j} =f_{_{V}}([(_{i,j}), (_{i,j}),_{i}])\] (4) \[_{j} =f_{_{Q}}((_{j}))\] (5)

To compute the aggregated value for the query, we calculate the weighted sum of the values as \(_{j}=_{i=1}^{K}w_{i,j}_{i,j}\), where each weighting term is calculated by applying the softmax function to the raw attention scores \(a_{i,j}\). The raw attention score \(a_{i,j}\) is obtained by applying a ReLU function to the dot product between the query \(_{j}\) and the keys \(_{i,j}\) scaled by their dimensionality \(\):

\[w_{i,j}=(a_{i,j})}{_{m=1}^{K}(a_{m,j})},\ a_{i,j}=(0,_{j},_{i,j}}{})\] (6)

By aggregating the features \(_{j}\) for all rays \(_{j}\) corresponding to each pixel on the image plane with resolution \(H W\), we construct the feature map \(F_{}^{H W d_{}}\) for a given camera pose \(\).

Point Feature RendererTo obtain the final rendered colour output image \(}\) for a given camera pose \(\), we input the aggregated feature map to a convolutional feature renderer, \(}=f_{_{}}(F_{})\). Our renderer is based on a modified version of the U-Net architecture , featuring two downsampling layers and two upsampling layers. Notably, we remove the BatchNorm layers in this network. For more details regarding the model architecture, please refer to the appendix.

### Progressive Refinement of Scene Representation

Point PruningTo eliminate potential outliers during the point position optimization process, we introduce a point pruning strategy that utilizes a learnable influence score \(_{i}\) associated with each point. We calculate the probability \(w_{b,j}\) of a ray \(_{j}\) hitting the background by comparing a fixed background token \(b\) to the product of each point's influence score \(_{i}\) and its raw attention score \(a_{i,j}\) (defined in Eqn. 6). The probability is given by:

\[w_{b,j}=(b)}{(b)+_{m=1}^{K}(a_{m,j} _{m})}\] (7)

By collecting the background probabilities \(w_{b,j}\) for all \(H W\) rays cast from camera pose \(\), we obtain a background probability mask \(P_{}^{H W 1}\). The rendered output image \(}\) can then be computed using this background probability mask as follows:

\[}=(1.-P_{}) f_{_{}}(F_{ })+P_{}_{b}\] (8)

Here, \(_{b}^{H W 3}\) represents the given background colour. All influence scores are initialized to zero. Starting from iteration \(10,000\), we prune points with \(_{i}<0\) every 500 iterations. Additional details and comparisons to the model without pruning are available in the appendix.

Point GrowingAs mentioned in Sec. 3.1, our scene representation allows for increased modelling capacity by adding more points. To achieve this, we propose a point growing strategy that incrementally adds points to the sparser regions of the point cloud every \(500\) iterations until the desired total number of points is reached. More information on how the sparse regions are identified can be found in the appendix. Fig. 1a illustrates the effectiveness of our point growing strategy in progressively refining the point cloud, resulting in a more detailed representation of the scene.

### Training Details

Our training objective is minimizing the distance metric \(d(,)\) from the rendered image \(}\) to the corresponding ground truth image \(_{gt}\). We define the distance metric as a weighted combination of mean squared error (MSE) and the LPIPS metric . The loss function is formulated as:

\[=d(},_{gt})=(}, _{gt})+(},_{gt})\] (9)

We set the weight \(\) to \(0.01\) for all experiments. During training, we jointly optimize all model parameters, including \(_{i}\), \(_{i}\), \(_{i}\), \(_{K}\), \(_{V}\), \(_{Q}\) and \(_{}\). We train our model using Adam optimizer  on a single NVIDIA A100 GPU.

### Learning Point Positions From Scratch

We demonstrate the effectiveness of our method in learning point positions from scratch using the Lego scene in the NeRF Synthetic Dataset . We compare our proposed method to recent point-based methods, namely DPBRF , Point-NeRF , NPLF  and Gaussian Splatting . It is worth noting that NPLF does not inherently support point position learning, so we made modifications to their official code to enable point position optimization.

In our comparison, all methods start with the same set of \(1,000\) points initialized on a sphere. To ensure a fair evaluation of point cloud position learnability, we disable point pruning and growing techniques for all methods. As shown in Figure 4, our method successfully deforms the initial point cloud to correctly represent the target geometry. In contrast, the baselines either fail to recover the geometry, produce noisy results, or lack structural details in the learnt geometry.

Figure 4: Comparison of our method and prior point-based methods on learning point cloud positions from scratch. We modify NPLF ’s official implementation to support point position updates. All methods start with the same initial point cloud of \(1,000\) points on a sphere. Point pruning and growing strategies are disabled for a fair comparison of geometry learnability. An RGB image of the scene from the same view point as the point cloud visualizations is provided as a reference. Point-NeRF  and NPLF  fail to reconstruct a reasonable point cloud. DPBRF  captures a rough shape but introduces significant noise and struggles to capture the rear end of the Lego bulldozer effectively. Gaussian Splatting  captures the Lego silhouette but lacks structural details, such as the bulldozer’s track (the black belt around the wheels). In contrast, our method successfully learns a point cloud that accurately represents the object’s surface and captures detailed scene structures.

Experiments

To validate our contribution of learning point-based scene representation and rendering pipeline directly from multi-view images, we compare our method to recent point-based neural rendering methods, namely NPLF , DPBRF , SNP , Point-NeRF  and Gaussian Splatting . As a secondary comparison, we demonstrate the potential broader impact of our method by comparing it to the widely used implicit volumetric representation method NeRF .

To assess the performance of scene modelling with a parsimonious representation, we conduct evaluations using a total of \(30,000\) points for each method. For all point-based baselines, we use their original point cloud initialization methods, which are based on Multi-View Stereo (MVS), Structure-from-Motion (SfM) or visual hull. Conversely, for our approach, we adopt a random point cloud initialization strategy within a predefined volume, introducing an additional level of complexity and challenge to our method. We set the parameter \(K=20\) for selecting the top nearest points, and the point feature vector dimension \(h=64\). We evaluate all methods in both synthetic and real-world scenarios. For the synthetic setting, we choose the NeRF Synthetic dataset , while for the real-world setting, we use the Tanks & Temples  subset, following the same data pre-processing steps as in . We evaluate rendered image quality using PSNR, SSIM and LPIPS  metrics.

### Quantitative Results

Table 1 shows the average image quality metric scores. PAPR consistently outperforms the baselines across all metrics in both synthetic and real-world settings, without relying on specific initialization. These results demonstrate PAPR's ability to render highly realistic images that accurately capture details using an efficient and parsimonious representation.

### Qualitative Results

Figure 5 shows a qualitative comparison between our method, PAPR, and the baselines on the NeRF Synthetic dataset. PAPR produces images with sharper details compared to the baselines. Notably, our method captures fine texture details on the bulldozer's body, the reflections on the drums and the crash, the mesh on the mic, and the reflection on the material ball. In contrast, the baselines either fail to capture the texture, exhibit blurriness in the generated output, or introduce high-frequency noise. These results validate the effectiveness of PAPR in capturing fine details and generating realistic renderings compared to the baselines. For additional qualitative results, please refer to the appendix.

### Ablation Study

Effect of Number of PointsWe analyze the impact of the number of points on the rendered image quality in our method. The evaluation is performed using the LPIPS metric on the NeRF synthetic dataset. As shown in Figure 6, our method achieves better image quality by using a higher number of points. Additionally, we compare the performance of our model to that of DPBRF and Point-NeRF. The results show that our model maintains higher rendering quality even with a reduced number of

    &  \\   & _PSNR \(\)_ & _SSIM \(\)_ & _LPIPS\({}_{vgg}\)_\(\)_ & _PSNR \(\)_ & _SSIM \(\)_ & _LPIPS\({}_{vgg}\)_\(\)_ & Initialization \\  _NeRF _ & \(31.00\) & \(0.947\) & \(0.081\) & \(27.94\) & \(0.904\) & \(0.168\) & \(-\) \\ _NPLF _ & \(18.36\) & \(0.780\) & \(0.213\) & \(21.19\) & \(0.761\) & \(0.240\) & MVS \\ _DPBRF _ & \(25.61\) & \(0.884\) & \(0.138\) & \(17.25\) & \(0.634\) & \(0.301\) & Visual Hull \\ _SNP _ & \(26.00\) & \(0.914\) & \(0.110\) & \(26.39\) & \(0.894\) & \(0.160\) & MVS \\ _Point-NeRF _ & \(25.93\) & \(0.923\) & \(0.129\) & \(24.75\) & \(0.890\) & \(0.184\) & MVS \\ _Gaussian Splatting _ & \(27.76\) & \(0.929\) & \(0.084\) & \(26.81\) & \(0.907\) & \(0.140\) & SfM \\ _PAPR (Ours)_ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & Random \\   

Table 1: Comparison of image quality metrics (PSNR, SSIM and LPIPS ) on the NeRF Synthetic dataset  and Tanks & Temples subset . Higher PSNR and SSIM scores are better, while lower LPIPS scores are better. All point-based baselines use a total number of \(30,000\) points and are initialized using the original techniques proposed by their respective authors. Despite being initialized from scratch, our method outperforms all baselines on all metrics for both datasets with the same total number of points.

points, as few as 1,000 points, compared to the baselines. These findings highlight the effectiveness of our method in representing the scene with high quality while using a parsimonious set of points.

Effect of Ray-dependent Point Embedding DesignWe analyze our ray-dependent point embedding design by gradually removing components, including the point position \(p_{i}\) and the displacement vector \(t_{i,j}\), which are described in Sec. 3.3. Figure 7 shows both the qualitative and quantitative results of this analysis. Removing \(p_{i}\) in key features introduces increased noise in the learnt point cloud, indicating its importance. Likewise, removing \(t_{i,j}\) in both key and value features results in a failure to learn the correct geometry. These findings validate the effectiveness of our point embedding design and highlight the necessity of both the point position and displacement vector for achieving accurate and high-quality results.

### Practical Applications

Zero-shot Geometry EditingWe showcase the zero-shot geometry editing capability of our method by manipulating the positions of the point representations _only_. In Figure 7(a), we present three cases that deform the object geometry: (1) rigid bending of the ficus branch, (2) rotation of the statue's head, and (3) stretching the tip of the microphone to perform a non-volume preserving transformation. The results demonstrate that our model effectively preserves surface continuity after geometry editing, without introducing holes or degrading the rendering quality. Additional results and comparisons with point-based baselines can be found in the appendix.

Object ManipulationIn Figure 7(b), we present object duplication and removal scenarios. Specifically, we add an additional hot dog to the plate and perform removal operations on some of the balls in the materials scene while duplicating others. These examples demonstrate the versatility of our method in manipulating and editing the objects in the scene.

Texture TransferWe showcase the capability of our method to manipulate the scene texture in Figure 7(c). In this example, we demonstrate texture transfer by transferring the associated feature vectors from points corresponding to the mustard to some of the points corresponding to the ketchup. The model successfully transfers the texture of the mustard onto the ketchup, resulting in a realistic and coherent texture transformation. This demonstrates the ability of our method to manipulate and modify the scene texture in a controlled manner.

Exposure ControlWe showcase the ability of our method to adjust the exposure of the rendered image. To accomplish this, we introduce an additional random latent co

Figure 5: Qualitative comparison of novel view synthesis on the NeRF Synthetic dataset . All point-based methods use a total number of \(30,000\) points. NPLF  fails to capture the texture detail, NeRF , SNP  and Gaussian Splitting  exhibit blurriness in the rendered images, while DPBRF  and Point-NeRF  display high-frequency artifacts. In contrast, PAPR achieves high-quality rendering without introducing high-frequency noise, demonstrating its ability to capture fine texture details.

renderer \(f_{_{}}\) and finetune the model using the cIMLE technique . Figure 7(d) shows different exposures in the rendered image by varying the latent code input. For more detailed information on this application, please refer to the appendix. This demonstrates our method's flexibility in controlling exposure and achieving desired visual effects in the rendered images.

## 5 Discussion and Conclusion

LimitationOur pruning strategy currently assumes the background image to be given, making it suitable for scenarios with a near-constant background colour. However, this assumption may not hold in more diverse and complex backgrounds. To address this limitation, we plan to explore learning a separate model dedicated to handling background variations in future work.

Societal ImpactExpanding our method's capacity with more points and deeper networks can improve rendering quality. However, it's important to consider the environmental impact of increased computational resources, potentially leading to higher greenhouse gas emissions.

ConclusionIn this paper, we present a novel point-based scene representation and rendering method. Our approach overcomes challenges in learning point cloud from scratch and effectively captures correct scene geometry and accurate texture details with a parsimonious set of points. Furthermore, we demonstrate the practical applications of our method through four compelling use cases.

AcknowledgementsThis research was enabled in part by support provided by NSERC, the BC DRI Group and the Digital Research Alliance of Canada.

Figure 8: Applications of PAPR: (a) Zero-shot Geometry editing - deforming objects (bending branch, rotating head, stretching mic), (b) Object manipulation - duplicating and removing objects, (c) Texture transfer - transferring texture from mustard to ketchup, (d) Exposure control - changing from dark to bright.

Figure 6: Ablation study on the effect of the number of points on rendered image quality. The results show that increasing the number of points improves the performance of our method. Moreover, our approach consistently outperforms the baselines when using an equal number of points, while also exhibiting greater robustness to a reduction in the number of points.

Figure 7: Ablation study on the design for the ray-dependent point embedding. We incrementally remove key components in the point embedding, starting with the point position \(p_{i}\) and then the displacement vector \(t_{i,j}\). The results show a significant degradation in the learnt geometry and rendering quality at each step of removal. This validates the importance of each component in the design of our point embedding.