# set parameters:

SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks

 Rainer Engelken

Zuckerman Mind Brain Behavior Institute, Columbia University, New York, USA

Max Planck Institute for Dynamics and Self-Organization, Gottingen, Germany

Gottingen Campus Institute for Dynamics of Biological Networks, Gottingen, Germany

re2365@columbia.edu

###### Abstract

Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we introduce _SparseProp_, a novel event-based algorithm for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both the forward and backward pass operations from O(N) to O(log(N)) per network spike, thereby enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By leveraging the sparsity of the network, _SparseProp_ eliminates the need to iterate through all neurons at each spike, employing efficient state updates instead. We demonstrate the efficacy of _SparseProp_ across several classical integrate-and-fire neuron models, including a simulation of a sparse SNN with one million LIF neurons. This results in a speed-up exceeding four orders of magnitude relative to previous event-based implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.

## 1 Introduction

The cortex processes information via streams of action potentials - commonly referred to as spikes - that propagate within and between layers of recurrent neural networks. Spiking neural networks (SNNs) provide a more biologically plausible description of neuronal activity, capturing in contrast to rate networks membrane potentials and temporal sequences of spikes. Moreover, SNNs promise solutions to the high energy consumption and CO\({}_{2}\) emissions of deep networks [1; 2], as well as the ability to transmit information through precise spike timing [3; 4; 5].

Unlike commonly used iterative ODE solvers that necessitate time discretization, event-based simulations resolve neural network dynamics precisely between spike events. This approach ensures machine-level precision in simulations, thereby alleviating the necessity for verification of result robustness with respect to time step size \( t\) and mitigating numerical issues, for instance in the vicinity of synchronous regimes . However, a major drawback of event-based simulations is the computational cost of iterating through all neurons at every network spike time.

To tackle this challenge, we introduce _SparseProp_, an novel event-based algorithm designed for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both the forward and backward pass from \(O(N)\) to \(O((N))\) per network spike. This efficiency enables numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By exploiting network sparsity and utilizing a change of variable to represent neuron states as times to the next spikes on a binary heap, _SparseProp_ avoids iterating through all neurons at every spike and employs efficient state updates.

We demonstrate the utility of _SparseProp_ by applying it to three popular spiking neuron models. While the current implementation is for univariate neuron models, we discuss a potential extension to more detailed multivariate models later.

Our contributions include:

* Introducing _SparseProp_, a novel and efficient algorithm for numerically exact event-based simulations of recurrent SNNs (section 2 and appendix A for minimal example code).
* Conducting a numerical and analytical scaling analysis that compares the computational cost of our proposed algorithm to conventional event-based spiking network simulations (section 3, Fig 3 and table 1).
* Providing concrete implementations of the algorithm for recurrent networks of leaky integrate-and-fire neurons and quadratic integrate-and-fire neurons (section 4 and here)
* Extending the algorithm to neuron models that lack an analytical solution for the next spike time using Chebyshev polynomials (section 5 and appendix B).
* Extending the algorithm to heterogeneous spiking networks (section 6).

In summary, our proposed _SparseProp_ algorithm offers a promising approach for simulating and training SNNs with both machine precision and practical applicability. It enables the simulation and training of large-scale SNNs with significantly reduced computational costs, paving the way for the advancement of brain-inspired models.

## 2 Algorithm for Efficient Event-Based Simulations in Recurrent Networks

We consider the dynamics of a spiking recurrent neural network of \(N\) neurons that is described by a system of coupled differential equations [7; 8; 9; 10; 11; 12; 13; 14]:

\[_{}V_{i}}{t}=F(V_{i})+I_{i}^{}+_{j,s}J_{ij}\:h(t-t_{j}^{(s)}).\] (1)

Here, the rate of change of the membrane potential \(V_{i}\) depends on its internal dynamics \(F(V_{i})\), an external input \(I_{i}^{}\) and the recurrent input \(_{j,s}J_{ij}\:h(t-t_{j}^{(s)})\). \(_{}\) is the membrane time constant.

Thus, when presynaptic neuron \(j\) spikes at time \(t_{j}^{(s)}\) the \(i^{}\) postsynaptic neuron is driven by by some temporal kernel \(h()\) and coupling strength \(J_{ij}\).

For event-based simulations, the dynamics of Eq 1 are evolved from network spike to network spike, usually based on an analytical solution of the membrane potential \(V_{i}(t)\) as a function of time \(t\), instead of using iterative ODE solvers like the Runge-Kutta methods. A key aspect of this method for spiking networks is that the spike times are not confined to the time steps of a numerical solver, allowing them to be obtained with machine precision.To simulate the entire network dynamics this way involves four simple steps:

1. Find the next spiking neuron \(j^{*}\) and its spike time \(t_{j^{*}}\).
2. Evolve the state \(V_{i}\) of every neuron to the next network spike time \(t_{j^{*}}\).
3. Update the neurons \(i\) postsynaptic to \(j^{*}\) using the weight \(J_{ij}\).
4. Reset the state of the spiking neuron \(V_{j^{*}}=V_{}\).

Such event-based simulation schemes are usually used for neuron models that have an analytical solution of the individual neurons' dynamics \(V_{i}(t)\)[10; 14; 15; 16; 17; 18]. The precision of the solution of event-based simulation is then limited only by the machine precision of the computer. In contrast, solving spiking network dynamics with an iterative ODE solver requires asserting that results are not dependent on the integration step size \( t\) of the integration scheme. The total accumulated error usually scale with \((( t)^{p})\), where \(p\) is the order of the ODE solver. However, both iteratingthrough all neurons to find the next spiking neuron (step 1) and evolving all neurons to the next spike time (step 2) conventionally require \((N)\) calculations per network spike. In the following, we suggest an algorithm that for sparse networks only requires \(((N))\) calculations per network spike by using a change of reference frame and an efficient data structure. In the next paragraphs, we will describe these two features in more detail.

### Change of Reference Frame

For clarity of presentation, we describe our algorithm first for a network of neurons that can be mapped to pulse-coupled phase oscillators like the leaky integrate-and-fire neuron and the quadratic integrate-and-fire neuron (Fig 1 A, C), and discuss later a more general implementation. For pulse-coupled oscillators, the network dynamics can be written as [10; 11; 14; 15; 16; 17; 18; 19]:

\[f_{i}(t_{s})=g_{i}(t_{s})+(t_{s +1}-t_{s})&\ i(j^{*})\\ _{i}(t_{s})+(t_{s+1}-t_{s})&\] (2)

where \(\) is the constant phase velocity and \(g()\) is the phase transition curve (PTC) evaluated for the neurons that are postsynaptic to the spiking neuron \(j^{*}\) just before the next network spike. In this phase representation, in the absence of recurrent input pulses, each phase evolves linearly in time with constant phase velocity \(\) from reset phase \(^{}\) to threshold phase \(^{}\) (Fig 1 C). Thus, finding the next spiking neuron in the network \(j^{*}\) amounts for homogeneous networks where all neurons have identical phase velocity to taking the maximum over all phases \(_{j^{*}}(t_{s})=(_{i}(t_{s}))\). The next network spike time \(t_{s+1}\) is given by \(t_{j^{*}}=-_{j^{*}}}{}\). \(g()\) quantifies the phase change of a neuron upon receiving an input spike as a function of its phase (Fig 1 B). In this conventional form of the event-based algorithm [10; 10; 11; 14; 15; 16; 17; 18; 19; 20; 21], all \(N\) phases must be linearly evolved to the state immediately before the next network spike time \(t_{s+1}\): \(_{i}(t_{s+1})=_{i}(t_{s})+(t_{s+1}-t_{s})\).

This operation, with a computational cost of \((N)\), can be avoided in our proposed algorithm. Instead, we introduce a modification where just the threshold and reset phase are changed by a global phase offset \(_{}\):

\[_{}^{(s+1)}=_{}^{(s)}+(t_{s+1}-t_{s}),\] (3)

Figure 1: **Change of reference frame allows efficient evolution of network state.****A** We map all membrane potentials \(V_{i}(t)\) to phases \(_{i}(t)\) that evolve linearly between spikes with phase velocity \(\)[10; 11; 15]. **B** At spike times \(t_{s}\), the phase transition curve (PTC) \(g((t_{s})\) has to be evaluated that tells neuron \(i\) how its phase \(_{i}\) changes when it receives an input spike. **C** In the phase representation, phases \(_{i}(t)\) evolve linearly in time between spikes, but the amount of change when the neuron receives an input spike depends on its phase. In this example at input spike time \(t_{1}\), the phase change is much larger than at input spike time \(t_{2}\), despite having same size in the voltage representation. **D** In conventional event-based simulations, the phase of all \(N\) neurons is shifted at every network spike time by \(_{i}= t\), where \( t=(t_{s+1}-t_{s})\) is the time to the next network spike. Instead, in _SparseProp_ just threshold and reset are changed at every spike.

starting with \(_{}=0\) at \(t=0\). Thus \(_{s+1}^{}=^{}-_{}^{(s+1)}\). Similarly, \(_{s+1}^{}=^{}-_{}^{(s+1)}\). See Fig 1D for an illustration. In inhibition-dominated networks, the phases \((t_{s})\) will thus become increasingly negative. For long simulations, all phases and the global phase should therefore be reset if the global phase exceeds a certain threshold \(_{}^{}\), to avoid numerical errors resulting from subtractive cancellation due to floating-point arithmetic . We discuss this in more detail in appendix C. In summary, this change of variables reduces the computational cost of propagating the network state from \((N)\) to \((1)\). Note, that we assumed here identical phase velocity \(\) for all neurons, but we will relax this assumption later in section 6.

### Efficient Data Structure

We next address the computational bottleneck encountered in very large sparse networks when finding the next spiking neuron, which has a numerical time complexity of \((N)\). To overcome this limitation, we introduce an efficient implementation using a priority queue. The priority queue is a data structure that facilitates returning elements with the highest priority and supports insertion and priority updates . We implement the priority queue as a heap-ordered binary tree, where each node's value is greater than its two children's values. This structure enables returning elements with the highest priority in constant time, with a time complexity of \((1)\), and inserting elements or updating priorities in logarithmic time, with a time complexity of \(((N))\). Thus, only \((K\,(N))\) operations have to be performed per network spike, as for \(K\) postsynaptic neurons and for one spiking neuron, a new phase needs to be updated in the priority queue.

Figure 2: **A suitable data structure allows efficient search for next spiking neuron for numerically exact simulation of large spiking networks.****A** By using a priority queue, the next spiking neuron can be found without iteration through all neurons of the network at every spike time. One implementation of a priority queue is a binary heap, as the example shown. In a max-heap, each child node has a value less or equal to its parent node. If an element is deleted, removed, or added, the heap property is restored by swapping parents and children systematically. Finding the node highest priority has a numerical complexity of \((1)\) and changing the value of any element has an amortized time complexity of \(((N))\). **B** Array implementation of binary heap shown in **A**. As per spike all postsynaptic neurons and the spiking neuron have to be updated, this requires on average \(K+1\) operation, thus an amortized time complexity \((K(N))\).

where \((_{i}(t_{s}))\) retrieves the highest-priority element without extracting it from the heap. In a min-heap--utilized for the phase representation here --the function fetches the smallest element, representing the next spike time. Conversely, in a max-heap--employed for heterogeneous networks--it obtains the largest element, signifying the maximum phase. The operation generally executes in constant time, \(O(1)\), given that the target element resides at the heap's root. Nonetheless, this function leaves the heap structure unaltered.

An example implementation of _SparseProp_ in Julia  is available here. There are two core innovations in our suggested algorithm: First, we perform a change of variables into a co-moving reference frame, which avoids iterating through all neurons in step 2 (Fig 1). Second, for finding the next spiking neuron in the network, we put the network state in a priority heap (Fig 2). A binary heap has efficient operations for returning the elements with the lowest (highest) key and insertion of new elements or updating of keys . We will next analyze the computational cost of _SparseProp_ empirically and theoretically.

## 3 Computational Cost Scaling

Overall, the total amortized cost per network spike of _SparseProp_ scales with \((K(N))\). For sparse networks, where \(K\) is fixed and only \(N\) is growing, the computational complexity of _SparseProp_ per network spike thus only scales with \(((N))\).

As the number of network spikes for a given simulation time grows linearly with network size, the overall computational cost for a given simulation time of _SparseProp_ scales \((K\,N\,(N))\), which for sparse networks is far more efficient compared to a conventional implementation (see Fig. 3). Specifically, the linearithmic scaling with network size \(N\) of our approach, grants remarkable efficiency gains compared to the quadratic scaling of conventional event-based simulations, particularly for sparse networks. For example, simulating a network of \(10^{6}\) neurons and \(K=100\) synapses for 100s takes less then one CPU hour with _SparseProp_, but would take more then 3 CPU years with the conventional algorithm1. We provide a detailed comparison of the computational cost in table 1.

## 4 Example Implementation for Integrate-And-Fire Neuron Models

### Pseudophase Representation of Leaky Integrate-And-Fire Neurons

For a network of pulse-coupled leaky integrate-and-fire neurons, Eq. 1 reads in dimensionless notation

\[_{}V_{i}}{t}=-V_{i}+I^{}+ _{}_{j,s}J_{ij}\,(t-t_{j}^{(s)})\] (4)If a membrane potential \(V_{i}\) reaches threshold \(V_{}\), it is reset to \(V_{}\). Without loss of generality, we set \(V_{}=0\) and \(V_{}=-1\). Between two network spikes, the solution is given by:

\[V_{i}(t_{s+1})=I^{}-(I^{}-V_{i}(t_{s}))( --t_{s}}{_{}})\] (5)

In this pseudophase representation (slightly different from ), the phases \(_{i}(-,\,0]\) describe the neuron states relative to the unperturbed interspike interval.

   & conventional algorithm & _SparseProp_ \\   Find next spiking neuron & \(_{i}(_{}-_{i}/)\) & peek(\(_{i}\)) \\  Evolve neurons & \(_{i}\)\(+=\,dt\) & \(+=\) \\  Update postsynaptic neurons & \(K\) operations & \(K\) operations \(+\)\(K\) key updates \(((N))\) \\  Reset spiking neuron & one array operation & \(((N))\) \\  Memory cost & \((N)\) & \((N)\) \\  
**total amortized costs per network spike** & \((N+K)\) & \((K\,(N))\) \\  
**total amortized costs for fixed simulation time** & \((N^{2})\) & \((K\,N\,(N))\) \\  

Table 1: **Comparison of computational cost for event-based simulation and training of recurrent spiking networks for different algorithms \(N\) denotes number of neurons, \(K\) is the average number of synapses per neuron. For large sparse networks (\(K N\)) and a fixed number of synapses per neuron \(K\), the dominant term grows quadratic for the conventional algorithm  and linearithmic (\(((N))\)) for the _SparseProp_ algorithm.**

Figure 3: **Benchmark of SparseProp vs. conventional algorithm. The computational cost per network spike scales linear with network size \(N\) in a conventional event-based implementation. In the large sparse network limit, the computational bottleneck is to find the next spiking neuron in the network and to propagate all neurons phases to the next network spike. Both operations can be implemented efficiently. First, instead of shifting all neurons phases, the threshold and reset values can be shifted. Second, by using a binary heap as a data structure for the phases, finding the next phase and keeping the heap-ordering has computational complexity of \((K\,(N))\). A: CPU time per network spike as a function of network size \(N\) for an inhibitory network of leaky integrate-and-fire neurons with fixed number of synapses per neuron \(K\). B: CPU time per simulation time shows a quadratic scaling for conventional algorithm, but only linearithmic scaling for the novel _SparseProp_ algorithm, Benchmark was performed on an Intel® Xeon® CPU E5-4620 v2 @ 2.60 GHz and 512 GB RAM. Parameters: mean firing rate \(=1\,\), \(J_{0}=1\), \(_{}=10\,\), \(K=100\).**

To obtain the unperturbed interspike interval \(T^{}\), we have to solve Eq. 5 between reset and threshold in the absence of synaptic input.

\[T^{} = -_{}(}-I^{}}{V_{ }-I^{}})\] (6) \[= _{}(1+}}).\] (7)

Its inverse is the phase velocity \(=1/T^{}\). The phase \(_{i}\) is thus given by \(_{i}=-(}}{I^{}-V_{i}}).\) The reverse transformation is \(V_{i}=I^{}(1-[-}{_{ }}]).\) Therefore, the phase transition curve is

\[g_{i^{*}}(t_{s+1}^{-})=-\,((-_{i^ {*}}(t_{s+1}^{-})/)+c),\] (8)

where \(c\) is the effective coupling strength \(c=}}\) and \(J\) is the synaptic coupling strength taken here to be identical for connected neurons. Usually, as discussed later in the appendix E in the section on balanced networks, \(J\) is scaled with \(J=}{}\), where \(K\) is the number of synapses per neuron. An example implementation of a LIF network with _SparseProp_ in Julia  is available here.

### Phase Representation of Quadratic Integrate-And-Fire Neurons

The quadratic integrate-and-fire (QIF) neuron has, in contrast to the LIF neuron, a dynamic spike generation mechanism and still has an analytical solution \(V_{i}(t)\) between network spikes.

For a network of QIF neurons, Eq. 1 reads in dimensionless voltage representation:

\[_{}V_{i}}{t}=V_{i}^{2}+I^{} +_{}_{j,s}J_{ij}\,(t-t_{j}^{(s)})\] (9)

The quadratic integrate-and-fire model can be mapped via a change of variables \(V=(/2)\) to the theta model with a phase variable \((-,]\)[29; 30; 31]. The dynamical equation between incoming spikes is the topological normal form for the saddle-node on a limit cycle bifurcation (SNIC) and allows a closed-form solution of the next network spike thanks to the exact derivation of the phase response curve . Therefore, the quadratic integrate-and-fire neuron is commonly used to analyze networks of spiking neurons [10; 33; 34; 35; 36]. When \(I_{i}^{}>0\)\(\)\(i\), the right-hand side of the dynamics is strictly positive and all neurons would spike periodically in the absence of incoming postsynaptic potentials. In this case, we can choose another particularly tractable phase representation, called phi-representation with \(V_{i}(t)=}}(_{i}(t)/2)\), where the neuron has a constant phase velocity . This transformation directly yields the phase transition curve

\[g(_{i})=2(}{2}+c),\] (10)

where \(c\) is the effective coupling strength \(c=}}}\). The phase velocity is given by \(=2}}\). We considered homogeneous networks here, where all neurons have identical external input and identical membrane time constant, and will later consider the more general case of heterogeneous networks in section 6. An example implementation of a QIF network with _SparseProp_ in Julia  is available here.

## 5 Event-Based Simulation via Chebyshev Polynomials

_SparseProp_ can also be used for univariate neuron models where no analytical solution between spike times is known, e.g., the exponential integrate-and-fire model (EIF)[37; 38]. In this case, the phase transition curve can be calculated numerically before the network simulation. During run time, the phase transition curve and its derivative can be interpolated from the precomputed lookup tables or using Chebyshev polynomials (see appendix B). An example implementation of an EIF network with _SparseProp_ in Julia  is available here. To test the numerical accuracy of the solution based on a lookup table or Chebyshev polynomials, we compared the spike times of two identical networks of LIF neurons with identical initial condition that were simulated either with the analytical phase transition curve or using the numerically interpolated solution. We used LIF networks for this comparison, because pulse-coupled inhibitory LIF networks are non-chaotic [11; 21; 39; 40; 41]. Thus, numerical errors originating at the level of machine precision are not exponentially amplified unlike in the chaotic pulse-coupled QIF network dynamics . We show in Fig. 4 that the difference in spike time between the two networks remain close to machine precision and the temporal order of the network either is not altered when replacing the analytical phase transition curve by the pulse-coupled LIF network.

## 6 Time-based SparseProp for Heterogeneous Networks

For heterogeneous networks, where there is heterogeneity of external input current \(I_{i}^{}\), or membrane time constant \(_{i}\), or threshold voltages \(V_{}^{i}\), every neuron has a different phase velocity \(_{i}\) and a single global phase shift as described in section 2.1 does not suffice.

In that case, we suggest yet another representation. Instead of having a priority queue of phases, instead, we suggest a priority queue of the next unperturbed spike time for all neurons, denoted by \((t_{0})\), as this can take heterogeneity into account. In this case, the algorithm for network simulation still consists of four steps: First finding the next spiking neuron, then updating the time followed by the update of the postsynaptic neurons by the incoming spikes, finally, resetting the spiking neuron to the reset value. This algorithm also has a numerical complexity of \((K.(N))\) per network spike. We provide pseudocode for _SparseProp_ in the time-based representation below:

```
1:Initialize heterogeneous phase velocity \(_{i}\) based on the neuron model
2:Initialize unperturbed interspike interval \(T_{i}\)
3:Initialize time to next spike for all neurons \((t_{0})\)
4:Initialize global time shift \(_{t}=0\)
5:Heapify \((t_{0})\)
6:Perform warm-up of network state \((t_{0})\)
7:for\(s=1 t\)do
8: Get index and spike time of next spiking neuron: \(j,n_{j}=((t_{s}))\)
9: Calculate time increment: \(t=n_{j}(t_{s})-_{t}\)
10: Update global time shift: \(_{t}t\)
11: Update postsynaptic neurons \(i^{*}\): \(n_{i^{*}}^{+}(t_{s})=updaten_{i^{*}}^{-}(t_{s}),_{t}\)
12: Reset spiking neuron: \(n_{j}(t_{s+1})=_{t}+T_{i}\)
13:if\(_{t}>_{t}^{}\)then
14:\(_{t}\)
15:\(_{t}=0\)
16:endif
17:endfor ```

**Algorithm 2**_SparseProp_: Event-Based Simulation of Heterogeneous Sparse Spiking Network

Again, peek\(((t_{s}))\) retrieves the highest-priority element without extracting it from the heap, which in this case is the minimum over all neurons' next unperturbed spike time. An example implementation of a heterogeneous QIF network with _SparseProp_ in this time-based representation is available here.

Figure 4: Left: Error of individual spike times for SparseProp are close to machine precision. Right: Error in spike index. Note that despite the small errors in the spike times that are close to machine precision, for non-chaotic network dynamics the spike index is still correct.

Note that this can also easily be extended to mixed networks of excitatory and inhibitory neurons (or to \(k\) population networks with different external input ).

The update of the postsynaptic neurons in line 11 is analogous to the evaluation of the phase transition curve in the homogeneous _SparseProp_, but takes into account the different phase velocities. For example, in the case of a QIF neuron, it takes the form

\[updaten_{i^{*}}^{-}(t_{s}),_{t}=-(n_{ i}-_{t})_{i}}{_{i}}+_{t}\] (11)

Here, the phase transition curve, denoted as \(g(_{i})\), is the same as previously described in Eq. 10: \(g(_{i})=2(}{2}+c)\) with effective coupling strength \(c\). For the QIF neuron, the period \(T_{i}\) is given by \(T_{i}=2/_{i}\), with \(=2}}\).

## 7 Efficient Event-Based Training of Spiking Neural Networks

_SparseProp_ can also be used for efficient event-based training of spiking neural networks. While surrogate gradients seem to facilitate training spiking networks [43; 44; 45; 46], it was recently claimed that EventProp, which uses exact analytical gradients based on spike times, can also yield competitive performance . To obtain precise gradients for event-based simulations, this algorithm makes use of the adjoint method from optimization theory [48; 49]. In this approach, in the forward pass the membrane potential dynamics (Eq. 1) is solved in an event-based simulation and spike times are stored. In the backward pass, an adjoint system of variables is integrated backward in time, and errors in spike times are back-propagated through the size of the jumps in these adjoint variables at the stored spike times. For event-based simulations of neuron models where the next spike time is analytically known or can be obtained like in section 5, the entries adjoint integration is being taken care of by differential programming . Besides the previously mentioned advantages of event-based simulation regarding precision, this approach is not only sparse in time but also sparse with respect to the synapses, as only synaptic events are used.

We suggest here to further improve the efficiency of event-based training using _SparseProp_. As the backward pass has the identical amortized time complexity of \(((N))\) per network spike as the forward pass, we expect a significant boost of training speed for large sparse networks [47; 50; 51]. Regrettably, the implementation of EventProp presented in  was not publicly available during the time of submission. In contrast, the authors of  made their re-implementation of EventProp publicly available, but they did not use an event-based implementation but forward Euler to integrate the network dynamics. It will be an important next step to also benchmark the performance of _SparseProp_ training. As the gradients are identical, we expect similar results to [47; 50; 51]

## 8 Limitations

In this work, we focus exclusively on recurrent networks of pulse-coupled univariate neuron models such as the quadratic integrate-and-fire neuron, leaky integrate-and-fire neuron, and the exponential integrate-and-fire neuron thereby excluding the exploration of multivariate neuron models. Extending our approach to encompass multivariate models, such as the leaky- or quadratic integrate-and-fire neurons with slow synapses [19; 28; 51; 52], remains a promising direction for future research.

A more fundamental limitation of our study is the apparent incompatibility of event-based simulations with surrogate gradient techniques [46; 53]. While one could introduce 'ghost spikes' in the event-based simulation to emulate surrogate gradients when neurons hit a lower threshold. However, it remains unclear how to preserve the favorable computational scaling of _SparseProp_ in this case.

Furthermore, _SparseProp_ requires an analytical solution of \(V_{i}(t)\) given the initial state and the neuronal dynamics \(_{i}(t)\) is necessary. This limitation excludes conductance-based models [54; 55; 56], for example. Additionally, time-varying input can only be introduced in the form of deterministic currents or point processes, such as Poisson input. Incorporating stochastic input without an analytical solution of \(V_{i}(t)\) would require further development.

Moreover, the reduction in computational cost per network spike from \((N)\) to \(((N))\) can only be achieved in sparse networks, where the number of synapses per neuron \(K\) is much smaller than the total number of neurons \(N\). For dense networks where the number of synapses scales proportionally to the number of neurons , a priority queue implemented by a binary heap is disadvantageous compared to a conventional array in the large network limit, as every network spike involves changing the priority of \((N)\) neurons thus \(((N+1)\,(N))\) flops per network spike which corresponds to \(((N+1) N\,(N))\) flops for a fixed simulation time. A batch-update of all postsynaptic neurons might be faster for very large networks  but this is beyond the scope of this work. It would involve \((K+(K)\,(N))\) flops corresponding to \((N+(N)\,(N))\) in dense networks. In the case of purely excitatory sparse spiking networks, a Fibonacci heap might be a more efficient implementation in the large network limit , as the 'decrease key' operation takes constant time \((1)\) compared to \(((N))\) in the case of a binary heap. Note that for practical purposes, the asymptotic scaling of the computational complexity of Fibonacci heaps has an unfavorably large prefactor . Therefore, other heap structure implementations might be faster . While large mammalian brains are sparse \(N K\), for models of small brains or local circuits with dense connectivity the conventional event-based simulations might be faster.

Additionally, _SparseProp_ cannot currently model biophysically detailed neuron features, including dendrites, threshold adaptation, slow synaptic timescales, and short-term plasticity. Incorporating these features would be another valuable avenue for future computational neuroscience research.

Lastly, we did not yet perform a comprehensive systematic benchmark of training with challenging tasks. Such benchmarks would offer further insights into the training performance of our proposed framework.

## 9 Discussion

We introduce _SparseProp_, an efficient and numerically exact algorithm for simulating and training large sparse spiking neural networks in event-based simulations. By exploiting network sparsity, we achieve a significant reduction in computational cost per network spike from \((N)\) to \(((N))\). This speedup is achieved by optimizing two critical steps in the event-based simulation: finding the next spiking neuron and evolving the network state to the next network spike time. First, we employ a binary heap data structure for finding the next spiking neuron and updating the membrane potential of postsynaptic neurons. Second, we utilize a change of variables into a co-moving reference frame, avoiding the need to iterate through all neurons at every spike.

Our results demonstrate the scalability and utility of _SparseProp_ in numerical experiments. We demonstrate that _SparseProp_ speeds up the simulation and training of a sparse SNN with one million neurons by over four orders of magnitude compared to previous implementations. In contrast to the conventional event-based algorithm, which would require three CPU years to simulate 100 seconds of network dynamics, _SparseProp_ achieves the same simulation in just one CPU hour.

This advancement enables the simulation and training of more biologically plausible, large-scale neural network models relevant to theoretical neuroscience. Furthermore, it might pave the way for the exploration of large-scale event-based spiking neural networks in other machine learning domains, such as natural language processing , embedded automotive applications , robotics , and _in silico_ pretraining of neuromorphic hardware .

The impact of _SparseProp_ extends to the recent surge of training for spiking neural networks in theoretical neuroscience , machine learning , and neuromorphic computing . This algorithm is expected to substantially accelerate the training speed in recent works focused on event-based spiking networks .

Future extensions of _SparseProp_ to multilayer and feedforward networks would be a promising avenue. Integration of _SparseProp_ with recent attempts in biologically more plausible backpropagation and real-time recurrent learning  could be used in future research in theoretical neuroscience. To improve training performance, future studies should consider incorporating additional slow time scales, such as slowly decaying synaptic currents , and slow threshold adaptation , which appear to enhance temporal-credit assignment in recurrent neural networks for rudimentary tasks . Additionally, incorporating gradient shaping and insights from advances in event-based spiking network training  could yield further gains in performance.