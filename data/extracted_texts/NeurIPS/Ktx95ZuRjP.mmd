# Learning to Handle Complex Constraints

for Vehicle Routing Problems

 Jieyi Bi\({}^{1}\), Yining Ma\({}^{1,}\), Jianan Zhou\({}^{1}\),

Wen Song\({}^{2}\), Zhiguang Cao\({}^{3}\), Yaoxin Wu\({}^{4}\), Jie Zhang\({}^{1}\)

\({}^{1}\)Nanyang Technological University

\({}^{2}\)Shandong University

\({}^{3}\)Singapore Management University

\({}^{4}\)Eindhoven University of Technology

jieyi001@e.ntu.edu.sg, yiningma@u.nus.edu,

jianan004@e.ntu.edu.sg, wensong@email.sdu.edu.cn,

zgcao@smu.edu.sg, y.wu2@tue.nl, zhangj@ntu.edu.sg

Yining Ma is the corresponding author.

###### Abstract

Vehicle Routing Problems (VRPs) can model many real-world scenarios and often involve complex constraints. While recent neural methods excel in constructing solutions based on feasibility masking, they struggle with handling complex constraints, especially when obtaining the masking itself is NP-hard. In this paper, we propose a novel Proactive Infeasibility Prevention (PIP) framework to advance the capabilities of neural methods towards more complex VRPs. Our PIP integrates the Lagrangian multiplier as a basis to enhance constraint awareness and introduces preventative infeasibility masking to proactively steer the solution construction process. Moreover, we present PIP-D, which employs an auxiliary decoder and two adaptive strategies to learn and predict these tailored masks, potentially enhancing performance while significantly reducing computational costs during training. To verify our PIP designs, we conduct extensive experiments on the highly challenging Traveling Salesman Problem with Time Window (TSPTW), and TSP with Draft Limit (TSPDL) variants under different constraint hardness levels. Notably, our PIP is generic to boost many neural methods, and exhibits both a significant reduction in infeasible rate and a substantial improvement in solution quality.

## 1 Introduction

Vehicle routing problems (VRPs) are NP-hard combinatorial optimization problems with complex constraints that model real-world scenarios, such as logistics  and supply chains . For decades, traditional solvers relied on hand-crafted rules for VRP optimization and constraint handling. Recently, the learning-to-optimize community  has successfully trained deep neural networks to automatically construct VRP solutions in an end-to-end manner . These data-driven neural methods offer greater efficiency and high parallelism for batch optimization, making them favorable alternatives.

In general, neural methods construct VRP solutions by autoregressively sampling a node from its predicted distribution while masking out nodes that would violate constraints to ensure the solution's feasibility. Despite successes (e.g., on TSP and CVRP), this masking mechanism assumes that 1) the feasibility of the entire solution can be properly decomposed into the feasibility of each node selectionstep, and 2) ground truth masks are easily obtainable for each step. However, such assumptions may fail in VRPs (e.g., TSPTW) with complex interdependent constraints among decision variables (i.e., nodes). As will be discussed in Section 4.1, this creates a masking dilemma - considering only the local feasibility of node selections does not guarantee the overall feasibility of the constructed solutions, while computing global feasibility masks that account for future impacts transforms masking itself into another intractable NP-hard problem.

These observations highlight significant gaps in applying recent neural methods to practical VRPs, necessitating research on new constraint-handling frameworks. In the literature, few studies have focused on novel ways of handling feasibility in neural constructive solvers. Although preliminary methods have attempted to mitigate it by relaxing constraints into soft ones [7; 8] or supplementing networks with more feasibility-related features , the former is prone to failure when applied to more complex scenarios, while the latter requires problem-specific features and a large supervised learning dataset, limiting its adaptability to broader VRPs. Consequently, neural methods still show limited flexibility, poor feasibility rates and large optimality gaps in solving those complex VRPs.

In this paper, we propose a novel **P**roactive **I**nfeasibility **P**revention (**PIP**) framework to extend the capabilities of neural constructive methods for VRPs with complex interdependent constraints. Our PIP first integrates the Lagrangian multiplier method into the reinforcement learning framework of neural methods, promoting initial constraint awareness and search guidance. To further address the limitations of the Lagrangian multiplier method on complex constraints, we then introduce preventative infeasibility masking to proactively steer the search to (near-)feasible regions during solution construction. By doing so, PIP significantly enhances feasibility rates and reduces optimality gaps. Moreover, to reduce the costs of obtaining preventative infeasibility information during training, we present **PIP-D**, which employs an auxiliary decoder to learn and predict masking information. Our PIP-D also incorporates two adaptive strategies: one to balance infeasible and feasible masking information for different problem hardness, and another to periodically update the model so as to balance training efficiency with prediction accuracy. These advancements enable PIP-D to achieve comparable or even better performance than PIP, particularly on larger and more constrained VRP instances, while significantly reducing computational complexity.

Our contributions are as follows: 1) _Conceptually_, we represent an early work to address and advance the handling of complex interdependent constraints in VRPs, where the original masking loses effectiveness due to the aforementioned dilemma, thereby extending the applications of neural methods to more practical scenarios. 2) _Methodologically_, we propose novel PIP and PIP-D approaches that can boost the capabilities of most constructive neural methods. Specifically, we leverage the Lagrangian multiplier method and introduce preventative infeasibility masking, which is further learned by an auxiliary decoder network with two adaptive strategies, to proactively and efficiently steer the search during solution construction. 3) _Experimentally_, we conduct extensive validation to demonstrate the effectiveness and versatility of PIP across various backbone _models_ (i.e., AM , POMO , and GFACS ) and complex VRP _variants_ (i.e., TSPTW and TSPDL). Notably, PIP achieves both a significant (up to 93.52%) reduction in infeasible rate and a substantial improvement in solution quality on synthetic and benchmark datasets with different constraint hardness levels.

## 2 Related work

**Neural solvers for VRPs.** Existing literature on learning to optimize VRPs features two primary paradigms: constructive solvers and iterative solvers. _Constructive solvers_ learn policies to construct solutions from scratch in an end-to-end manner. Early works introduce Pointer Network to approximate the optimal solution to TSP [11; 12] and CVRP  in an autoregressive (AR) way. Among all AR solvers, the attention-based model (AM)  represents a milestone in solving a series of VRPs. Later, the policy optimization with multiple optima (POMO)  further improves upon AM by considering the symmetry property of VRP solutions. Numerous recent studies have then aimed to further enhance their performance [14; 15; 16; 17; 18; 19; 20; 21; 22; 23] and versatility [24; 25; 26; 27]. Besides the AR methods, several works construct a heatmap, which indicates the probability distribution of each edge being part of the optimal solution, to solve VRPs in a non-autoregressive (NAR) manner [10; 28; 29; 30; 31; 32; 33; 34]. Despite the superior performance on large-scale instances, we note that a recent work  questions the effectiveness of heatmap generative methods due to the misalignment of training and testing objectives. Differently, _iterative solvers_ learn policies to iteratively refine an initial solution. The policies are often trained in contexts of classic heuristics or meta-heuristics for obtaining more efficient and effective search components [36; 37; 38; 39; 40; 41; 42; 43; 44; 45]. Generally, constructive solvers can efficiently achieve desirable performance levels, whereas iterative solvers hold the potential to search for near-optimal solutions with a prolonged time budget. Additionally, there are also several works studying the scalability [46; 47; 48; 49; 50; 51; 52], generalization [53; 54; 55; 56; 57; 58], and robustness [59; 60] of neural VRP solvers, and leveraging large language models (LLMs) to optimize VRPs [61; 62; 63].

**Constraint handling for VRPs.** Most neural methods for VRPs manage constraints using a feasibility masking mechanism that eliminates actions leading to infeasible solutions during construction or iteration search [4; 28; 32; 41]. However, such a mechanism assumes the availability of accurate masks and often lacks constraint awareness learning during training, which is not always practical or desirable. For example, Zhao et al.  highlighted the benefits of learning to modulate agent behaviours in the 3D Bin Packing Problem, and Ma et al.  showed that temporary constraint violations could enhance neural iterative solvers. Despite their successes, these approaches are inherently unsuitable for assisting constructive solvers to address the VRPs with complex interdependent constraints studied in this paper. While Tang et al.  and Zhang et al.  proposed methods to transform hard constraints into soft ones via relaxation techniques and problem redefinition, respectively, they may only be able to yield near-feasible solutions with large infeasible rates for VRPs with complex constraints. More recently, Chen et al.  developed a multi-step look-ahead (MUSLA) method specifically tailored for TSPTW, incorporating problem-specific features and a large supervised learning dataset. In contrast, this paper proposes a more flexible and generic PIP framework based on novel ideas of preventative infeasibility masking, learnable decoders, and adaptive strategies to advance a broader range of neural methods without needing labelled training data.

## 3 Preliminaries

In this paper, we mainly consider two VRP variants with complex interdependent constraints (i.e., TSPTW and TSPDL), and neural solvers (i.e., AM , POMO  and GFACS ).

**Problem definitions and notations.** A VRP instance can be defined over a complete graph \(=\{,\}\), where \(=\{v_{0},v_{1},,v_{n}\}\) denotes the node set, and \(=\{e(v_{i},v_{j})|v_{i},v_{j},i j\}\) denotes the directed edge set among all nodes. The objective is to minimize the total cost (e.g. Euclidean length) of the solution tour. To form a feasible solution, each node in \(\) should be visited exactly once while respecting problem-specific constraints. We consider two types of VRP constraints that are practical in industry: 1) _Time window constraint:_ The arrival time at node \(v_{i}\), denoted as \(t_{i}\), must fall within a customer-specific time window \([l_{i},u_{i}]\). If the vehicle arrives early (i.e., \(t_{i}<l_{i}\)), it must wait until \(l_{i}\); 2) _Draft limit constraint:_ Each node \(v_{i}\) represents a port with a non-negative demand \(_{i}\) and a maximum draft \(d_{i}\). We denote the current cumulative load of the freighter at port \(v_{i}\) as \(_{i}\) in a given solution, which should not exceed the corresponding maximum draft \(d_{i}\) of the port.

**Constructive solvers for VRPs.** Popular neural constructive solvers [4; 5] typically parameterize the policy using an encoder-decoder model with parameter \(\), trained with reinforcement learning (RL). Given a VRP instance \(=\{,\}\), the features of each node \(v_{i}\) are represented as \(f_{i}^{v}=\{x_{i},y_{i},c_{i}\}\), where \(x_{i},y_{i}\) are node coordinates, \(c_{i}\) represents constraint-related features (e.g., \(c_{i}=\{l_{i},u_{i}\}\) for time windows in TSPTW and \(c_{i}=\{_{i},d_{i}\}\) for demand and draft limits in TSPDL). The encoder transforms node features into high-dimensional representation embeddings \(h_{i}\), which, combined with the context of the partial tour, represent the current state. The decoder takes them as inputs and outputs probabilities for candidate nodes (actions). The reward \((|)\) is the negative tour length. The policy \(_{}\) is typically trained using REINFORCE  as follows:

\[_{}(|)= _{i=1}^{K}((_{i}|)-b() )_{}(_{i}|),\] (1)

where \(K\) denotes the number of sampled solutions \(_{i}\) for a given training instance \(\), and \(b()\) is a baseline function to reduce the variance. Specifically, the baseline is the reward (negative tour length) of the solution derived greedily in AM or the average reward of sampled solutions \(_{i=1}^{K}(_{i}|)\) in POMO. Notably, POMO stipulates the starting node of each solution for diversification, which, however, may hinder solution feasibility in our studied complex constrained problems. Based on our preliminary experiments, POMO with and without diverse starting nodes achieve around 50.70% and 1.75% infeasible rates on the easy TSPTW-50 datasets, respectively. Therefore, we remove the starting node stipulation in POMO and instead sample \(n\) solutions to calculate the baseline.

## 4 Methodology

We now discuss the limitations of existing masking mechanisms in solving VRPs with complex interdependent constraints, followed by a detailed introduction of our PIP and PIP-D frameworks.

### Dilemma of feasibility masking

The core of feasibility masking in neural constructive solvers is to filter out invalid actions that violate constraints, based on the assumption that the global feasibility can be decomposed into the feasibility of each node selection step, and that ground truth masks are obtainable for each step. Without loss of generality, we illustrate the dilemma of feasibility masking using a TSPTW example. In TSPTW, nodes are masked out if they have been visited or cannot be visited before their time window closes. However, the feasibility of selecting a node at a particular step impacts the current time, thereby affecting all future selections due to the interdependence of time window constraints. Thus, considering only local feasibility does not guarantee overall feasibility and may lead to irreversible infeasibility. For instance, in a 4-node TSPTW instance with time windows \(\{,,,\}\) as illustrated in the left panel of Figure 1, there is a feasible solution \(=(v_{0} v_{1} v_{2} v_{3})\). Yet, with the partial solution \(v_{0} v_{1}\), both \(v_{2}\) and \(v_{3}\) appear locally feasible. If the solver selects \(v_{3}\), the tour becomes infeasible irreversibly. A potential remedy is to compute global feasibility masks that consider all future possibilities, as illustrated in the right panel of Figure 1. However, this makes masking itself an NP-hard problem, which creates a dilemma between ensuring solution feasibility and managing computational complexity. Note that this dilemma is less critical in CVRPTW, which involves multiple vehicles and routes, providing more flexibility. If one route becomes infeasible, another vehicle departing at time \(0\) can cover the missed nodes, reducing the impact of constraint interdependencies. However, this issue is severe in TSPTW and other variants like TSPDL.

### Guided policy search by PIP

We first formulate the solution construction process of VRP as a Constrained MDP (CMDP) defined by the tuple \((,,,,)\), where \(\) is the state space, \(\) is the action space that travels from node \(v_{i}\) to node \(v_{j}\), \(:\) is the reward function, \(:\) is the constraint violation cost (penalty) function, and \(:\) is the transition probability function. At each time step, the neural solver outputs the probability of all candidate nodes, and selects one to construct a complete solution \(\). The objective of CMDP is to learn a policy \(_{}:()\) that maximizes the summation of the state-wise reward subject to certain constraints,

\[_{}(_{}) =_{_{}}[_{e(v_{i},v_{j}) }(e(v_{i},v_{j}))],\] (2) s.t. \[_{}_{F},\ _{F}=\{|_{ _{m}}()_{m},\  m[1,M]\},\]

where \(\) is the expected return of the policy, \(_{F}\) denotes the set of all feasible policies, \(_{m}\) represents the boundary of the inequality constraints \(_{m}\), and \(M\) is the number of constraints. Specifically, a

Figure 1: A TSPTW instance to illustrate the malfunction of existing masking mechanism (left three panels) and NP-hardness of obtaining precise infeasible masks (right panel). The orange bar represents the time window [\(l_{i}\), \(u_{i}\)] for node \(v_{i}\). For the partial solution \(v_{0} v_{1}\), both \(v_{2}\) and \(v_{3}\) are locally feasible. However, selecting \(v_{3}\) results in the irreversible infeasibility of \(v_{2}\) afterwards.

feasible policy \(\) is one whose expected value of constraint violation w.r.t \(_{m}\), denoted as \(_{_{m}}()\), does not exceed \(_{m}\). Note that \(_{m}\) is set to 0 throughout this paper since we consider the hard constraints that do not tolerate any violation. Moreover, we set the reward function \(\) to the negative value of the Euclidean distance between two nodes, i.e., \((e(v_{i},v_{j}))=-||v_{i}-v_{j}||_{2}\).

By applying feasibility masking, the search is confined to only feasible regions, allowing neural methods to focus solely on the objective function in Eq. (2) without explicitly considering constraint awareness or constraint violations. However, these methods lose effectiveness when such masks are unavailable, leading to inefficient searches in large infeasible regions. To address this, we propose PIP, combining a Lagrangian multiplier for constraint awareness and preventative infeasibility masking to confine the search space to near-feasible regions for complex constrained problems.

**Lagrangian-assisted constraint awareness.** We design a Lagrangian multiplier based method to incorporate constraints \(\) into the reward function \(\). Based on the Lagrangian Multiplier Theorem, the CMDP formulation in Eq. (2) is transformed into the following MDP formulation for VRPs:

\[_{ 0}_{}(,)=_{  0}_{}-_{_{}}[_{e(v_{i},v_{j}) }||v_{i}-v_{j}||_{2}+_{m=1}^{M}_{m}_{C_{m}}( )+_{}],\] (3)

where \(\) is the Lagrangian function, and \(_{m}\) is a non-negative Lagrangian multiplier. Generally, the constraint violation term is calculated as the total violation value of all constraints. In TSPTW, \(_{}()=_{i=0}^{n}(t_{i}-u_{i},0)\), and in TSPDL, \(_{}()=_{i=0}^{n}(_{i}-d_{i},0)\). Additionally, we introduce the number of infeasible nodes in the solution \(\), termed as \(_{}\), as an extra term in the Lagrangian function for better constraint awareness, which is empirically found to be effective to reduce the infeasibility rate. While Lagrangian relaxation has been explored in neural iterative methods for soft objectives , our approach introduces a distinct constraint violation cost function tailored for neural constructive methods and considers fixing the Lagrangian multiplier \(\) (the dual variable) and optimizing the primal variable \(\), significantly reducing computational overheads.

**Preventative infeasibility (PI) masking.** As depicted in Figure 2(b), the customized Lagrangian multiplier guides the neural policy towards a potentially feasible and high-quality space using Eq. (3). However, for more complex cases shown in Figure 2(c), neural solvers may still struggle to navigate the large search space. To further improve training efficiency and solution feasibility, we introduce _preventative infeasibility (PI) masking_ to proactively avoid selecting infeasible nodes during the solution construction process. As shown in the left panel of Figure 3, if selecting a candidate node (i.e., orange node) results in any remaining candidates (i.e., green node) becoming potentially unvisitable in the next step due to constraint violations, it is marked as infeasible (i.e., red node) since selecting it would cause irreversible future infeasibility (see Appendix A.3 for a detailed example). Note that we employ a simple yet effective one-step PI masking in this paper to balance computational costs without iterating over all future possibilities (which is NP-hard). Together with the customized Lagrangian multiplier, our PIP proactively reduces the search space to a near-feasible domain \(_{}\), as shown in Figures 2(d)-(e). Notably, such PIP design is generic and can be applied to enhance most neural constructive solvers for VRPs with complex interdependent constraints.

Figure 2: Illustration of policy optimization trajectories on VRP with varying difficulty levels - (a)(b)(d) easy and (c)(e) hard, and different constraint handling schemes - (a) feasibility masking, (b)(c) Lagrangian multiplier, and (d)(e) our PIP. The orange-filled circle denotes the feasible policy space \(_{F}\), while the dotted frame represents the actual search space of the neural policies \(_{}\).

[MISSING_PAGE_FAIL:6]

## 5 Experiments

In this paper, we propose a Proactive Infeasibility Prevention (PIP) framework and its enhanced version, PIP-D, to address the limitations of existing masking mechanisms for handling complex constraints. Notably, our PIP and PIP-D are generic and can be applied to boost various problem variants and neural methods. To evaluate the effectiveness of our method, we apply our PIP frameworks to two representative AR constructive methods, AM  and POMO , and the latest NAR constructive GFACS . For the benchmark problem, we consider two representative complex VRP variants with strong interdependent constraints that challenge existing neural methods (i.e., TSPTW and TSPDL, each at varying levels of hardness) with small problem scale \(n=50,100\) for AM  and POMO  and large scale \(n=500\) for GFACS . All the experiments are conducted on servers with NVIDIA GeForce RTX 3090 GPUs and Intel(R) Xeon(R) Gold 6326 CPU at 2.90GHz. Our implementation in PyTorch are publicly available at https://github.com/jieyibi/PIP-constraint.

**Implementation details.** We generate instances at different hardness levels following prior works. For TSPTW [7; 9; 30; 67], we generate three types of instances: Easy, Medium and Hard, by adjusting the width and overlap of the time window. For TSPDL [68; 69; 70], we consider two levels of hardness: Medium and Hard. More details of such instance generation are provided in Appendix A. To ensure a

   } &  &  \\   &  &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  \\    & LKH3 & 0.00\% & 0.00\% & 7.31 & 0.00\% & 4.6h & 0.00\% & 0.00\% & 10.21 & 0.00\% & 8.5h \\  & ORTools & 0.00\% & 0.00\% & 7.34 & 0.96\% & 7h & 0.00\% & 0.00\% & 10.41 & 1.97\% & 14h \\  & Greedy-L & 100.00\% & 100.00\% & / & / & 13.8s & 100.00\% & 100.00\% & / & / & 1.3m \\  & Greedy-C & 0.00\% & 0.00\% & 26.08 & 257.27\% & 4.5s & 0.00\% & 0.00\% & 52.14 & 411.13\% & 12s \\  & JAMPR \# & / & 0.00\% & / & 249.03\% & 1.2m & / & 100.00\% & / & / & 1.6m \\  & OSLA \# & / & 11.80\% & / & 8.15\% & 15.6s & / & / & / & / & / \\  & MUSLA \# & / & 8.20\% & / & 7.32\% & 1.3m & / & 18.60\% & / & 14.6\% & 9.8m \\  & MUSLA adapt \# & / & 0.10\% & / & 5.63\% & 7.7m & / & 0.60\% & / & 12.01\% & 1.1h \\   & AM & 100.00\% & 100.00\% & / & / & 100.00\% & 100.00\% & / & / & 21m \\  & AM* & 3.46\% & 0.22\% & 8.02 & 8.92\% & 5.2m & 7.87\% & 1.49\% & 11.84 & 16.07\% & 21m \\  & AM*+PIP & 0.55\% & 0.00\% & 7.87 & 7.67\% & 10.7m & 0.45\% & 0.00\% & 11.42 & 11.86\% & 1h \\  & AM*+PIP-D & 0.51\% & 0.00\% & 7.91 & 8.19\% & 11m & 0.25\% & 0.00\% & 11.53 & 13.02\% & 1h \\   & POMO & 100.00\% & 100.00\% & / & / & 13s & 100.00\% & 100.00\% & / & / & 21s \\  & POMO* & 1.75\% & 0.00\% & 7.54 & 3.08\% & 13s & 2.11\% & 0.00\% & 10.83 & 6.07\% & 21s \\  & POMO* + PIP & 0.32\% & 0.00\% & 7.50 & 2.65\% & 15s & 0.15\% & 0.00\% & 10.57 & 3.53\% & 48s \\  & POMO* + PIP-D & 0.28\% & 0.00\% & 7.49 & 2.51\% & 15s & 0.06\% & 0.00\% & 10.66 & 4.39\% & 48s \\    & LKH3 & 0.00\% & 0.00\% & 13.02 & 0.00\% & 7h & 0.00\% & 0.00\% & 18.74 & 0.00\% & 10.8h \\  & ORTools & 15.77\% & 15.77\% & 13.02 & 0.30\% & 5.9h & 0.52\% & 0.52\% & 19.34 & 3.23\% & 13.8h \\  & Greedy-L & 100.00\% & 100.00\% & / & / & 15s & 100.00\% & 100.00\% & / & / & 1m \\  & Greedy-C & 47.52\% & 47.52\% & 25.33 & 96.43\% & 4.2s & 20.34\% & 20.34\% & 51.62 & 176.07\% & 11.4s \\   & AM & 100.00\% & 100.00\% & / & / & 5m & 100.00\% & 100.00\% & / & / & 21m \\  & AM* & 24.84\% & 0.27\% & 13.81 & 6.11\% & 5m & 50.19\% & 0.09\% & 21.42 & 14.34\% & 21m \\  & AM*+PIP & 7.62\% & 0.35\% & 16.38 & 5.06\% & 11m & 12.73\% & 0.04\% & 20.57 & 9.82\% & 1h \\  & AM*+PIP-D & 11.96\% & 0.33\% & 13.65 & 4.87\% & 11m & 8.80\% & 0.02\% & 20.80 & 11.03\% & 1h \\   & POMO & 100.00\% & 100.00\% & / & 13s & 100.00\% & 100.00\% & / & / & 21s \\  & POMO* & 14.92\% & 3.77\% & 13.68 & 5.23\% & 13s & 18.77\% & 0.12\% & 20.78 & 10.93\% & 21s \\  & POMO* + PIP & 4.53\% & 0.90\% & 13.40 & 2.91\% & 15s & 3.88\% & 0.19\% & 19.61 & 4.65\% & 48s \\  & POMO* + PIP-D & 3.83\% & 0.65\% & 13.45 & 3.32\% & 15s & 3.34\% & 0.03\% & 19.79 & 5.64\% & 48s \\    & LKH3 & 0.12\% & 0.12\% & 0.12\% & 25.61 & 0.00\% & 7h & 0.07\% & 0.

comprehensive comparison, we also train and evaluate the models learned solely using our designed Lagrangian multiplier method. Meanwhile, we mark the models that use the Lagrangian multiplier with an \(*\) for clarity. For our proposed approaches, our PIP models build on the Lagrangian multiplier by further incorporating one-step preventative infeasibility masking, while the enhanced version, PIP-D, is trained with a periodically and adaptively updated PIP decoder as previously described. Hyper-parameters for training follow the original settings of the backbone models except for the ones related to the added PIP decoder. Detailed hyper-parameters and additional results are available in Appendix C and D. During inference, we adhere to the settings of the original backbone models. For the AM series models, we sample 1280 solutions per instance; for the POMO series models, we use a greedy strategy with 8\(\) augmentation; and for the GFACS series models, we employ 100 ants to generate solutions for each instance over 10 pheromone iterations.

**Baselines.** We compare our proposed PIP framework with two types of baselines: 1) heuristic methods, including _LKH3_, a strong solver designed for multiple VRP variants; _OR-Tools_, a more flexible solver allowing different combinations of multiple diverse constraints; and _Greedy Heuristics_ that selects locally optimal candidates at each step, where _Greedy-L_ picks the nearest candidate and _Greedy-C_ chooses based on complex constraints: in TSPTW, the soonest time window ends relative to the current time; and in TSPDL, the minimal draft limit; 2) Neural methods, including the original _AM_, _POMO_ and _GFACS_, as well as _JAMPR_, adapted by  to solve TSPTW from VRPTW; and _MUSLA_, a prior work on TSPTW trained in supervised manner, where _OSLA_ is its one-step version and _MUSLA adapt_ adopts an adaptive inference strategy. More details on the compared baselines are presented in Appendix C.

**Evaluation metrics.** In this paper, we report the following metrics to evaluate the performance of our proposed PIP framework: 1) the ratio of infeasible solutions (Infeasible%), which includes the solution-level (Sol.) infeasible rate that considers all generated solutions during inference and the instance-level (Inst.) infeasible rate that considers the comprehensive results of \(N_{s}\) solutions generated by the sampling (\(N_{s}=1,280\) in AM series models) or augmentation (\(N_{s}=8\) in POMO series models). If at least one feasible solution is found among these \(N_{s}\) solutions, the instance is considered to have feasible solutions; 2) average optimality gap (Gap) w.r.t the strong baseline LKH  for the best feasible solutions within \(N_{s}\) solutions; 3) average tour length (Obj.) of the feasible best solutions within \(N_{s}\) solutions; and 4) inference time, where we report the total time taken to solve 10,000 (\(n=50\) and 100) or 128 (\(n=500\)) instances, with batch parallelism enabled on a single GPU. For baselines run in CPU, we exhibit the results in parallel on 16 CPU cores.

### Model performance on complex constrained problems

The performance comparison on TSPTW and TSPDL at various levels of problem hardness is presented in Table 1 and Table 2, respectively. Notably, the original backbone models AM and POMO could not solve the problem even at the easiest level. By incorporating the Lagrangian multiplier (indicated by *), the models begin to generate some feasible solutions. However, this advantage diminishes under more complex constraints. For example, the instance-level infeasibility rates for

   } &  &  \\  &  &  &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  \\    & LKH3 & 0.00\% & 0.00\% & 10.87 & 0.00\% & 5.1h & 0.00\% & 0.00\% & 16.39 & 0.00\% & 14h \\  & GRTools & 100.00\% & 100.00\% & _/_ & _/_ & 10.9s & 100.00\% & 100.00\% & _/_ & _/_ & 56.9s \\  & Greedy-L & 100.00\% & 100.00\% & _/_ & _/_ & 2.4m & 100.00\% & 100.00\% & _/_ & _/_ & 9.5m \\  & Greedy-C & 0.00\% & 0.00\% & 26.09 & 144.24\% & 9.1s & 0.00\% & 0.00\% & 52.16 & 222.71\% & 27s \\   & POMO* & 17.72\% & 12.52\% & 10.98 & 3.80\% & 6.9s & 49.39\% & 32.19\% & 17.11 & 9.15\% & 18s \\  & POMO* + PIP & 2.21\% & 0.43\% & 11.22 & 3.41\% & 8.5s & 2.88\% & 0.38\% & 17.71 & 8.08\% & 31s \\  & POMO* + PIP-D & 2.64\% & 0.37\% & 11.26 & 3.78\% & 8.4s & 2.14\% & 0.23\% & 17.84 & 8.86\% & 31s \\    & LKH3 & 0.00\% & 0.00\% & 13.30 & 0.00\% & 6.8h & 0.00\% & 0.00\% & 20.70 & 0.00\% & 1.2d \\  & ORTools & 100.00\% & 100.00\% & _/_ & _/_ & 10.6s & 100.00\% & 100.00\% & _/_ & _/_ & 56.8s \\   & Greedy-L & 100.00\% & 100.00\% & _/_ & _/_ & 2.4m & 100.00\% & 100.00\% & _/_ & _/_ & 9.4m \\   & Greedy-C & 0.00\% & 0.00\% & 26.07 & 99.73\% & 10.9s & 0.00\% & 0.00\% & 52.17 & 156.37\% & 25s \\   & POMO* & 37.01\% & 29.25\% & 13.03 & 4.11\% & 6.8s & 99.98\% & 99.85\% & 20.95 & 15.87\% & 18s \\   & POMO* + PIP & 4.53\% & 2.10\% & 13.66 & 3.13\% & 8.5s & 28.55\% & 20.66\% & 22.30 & 12.67\% & 31s \\   & POMO* + PIP-D & 3.89\% & 0.82\% & 13.80 & 3.95\% & 8.5s & 12.84\% & 7.91\% & 22.84 & 12.32\% & 31s \\   

Table 2: Experiments on TSPDL instances with two different hardness.

POMO and AM on Hard TSPTW-100 reach 100% in Table 1, which is dramatically reduced to 6.28% with the addition of PIP-D, while also improving solution quality. Compared to traditional heuristics like ORTools, Greedy-L, and Greedy-C, our PIP-D consistently outperforms these methods and shows favourable results against JAMPR and MUSLA, especially in large-scale problems. Furthermore, compared to PIP, our PIP-D delivers competitive or even better objective values and optimality gaps while significantly enhancing training efficiency (e.g., 1.5 times faster for \(n\) = 50 and 5.8 times faster for \(n\) = 100, w.r.t POMO* + PIP). Notably, the superiority of PIP-D is more significant on the more constrained hardness levels and larger problem sizes. For TSPDL, we observe similar patterns, where our PIP and PIP-D models consistently outperform other baselines in terms of both infeasibility reduction and solution quality. These results validate that our PIP approach significantly reduces infeasible rates and substantially improves solution quality compared to existing neural methods.

### Model performance on large-scale problems

We further evaluate the capability of solving large-scale problems by implementing our PIP framework on GFACS . As displayed in Table 3, equipping GFACS with our PIP significantly reduces the infeasible rate, for both the solution level and instance level and simultaneously enhances solution quality. Notably, GFACS* + PIP-D almost guarantees to obtain all feasible solutions. Different from AM and POMO, GFACS is a NAR constructive solver, which showcases the generality of our framework.

### Further Experiments

**Ablation on each PIP and PIP-D design.** We now provide in-depth discussions on the effectiveness of the three proposed designs: the Lagrangian multiplier (*), the PI masking (PIP) and the learnable decoder (PIP-D). As shown in Tables 1 and 2, in _Easy_ datasets, the solution-level infeasible rate for POMO* is 2.11%, improving to 0.06% with PIP-D. This shows that, for less complex constraints, the Lagrangian multiplier alone effectively guides the policy to feasible regions, hedging the impact of PIP and PIP-D, which aligns with Figure 2(b) and (d) where \(_{F}\) is relatively large compared to \(\). However, in more complex scenarios, where \(_{F}\) is much smaller relative to \(\) (as in Figure 2(c)), the neural policy struggles even with the Lagrangian multiplier. In such cases, our PIP and PIP-D become crucial, significantly confining the search space as depicted in Figure 2(e). In _Medium_ datasets, the infeasible rate drops from 18.7% in POMO* to 3.34% in POMO* + PIP-D; in _Hard_ datasets, it drops dramatically from 100% in POMO* to 6.48% in POMO* + PIP-D. These results verify that our PIP framework achieves significant improvement, especially as problem complexity increases.

**Ablation on the terms in Lagrangian function.** In Figure 4, we exhibit the results with and without the \(_{}\) in Eq.(3), which validates its efficacy of enhancing the constraint awareness.

**Ablation on weighted balancing strategy.** Recall that the ratio of infeasible to feasible samples in PIP labels varies with the inherent constraint hardness. Our preliminary experiments suggest that such a ratio can reach up to 20:1 in the case of Hard datasets, causing significant label imbalance. This imbalance may significantly impact the performance of POMO* + PIP-D on several datasets, especially the harder ones, leading to 0% prediction accuracy on the minority class and causing a 100% infeasible rate for the backbone solver without a weighted balancing strategy. This indicates that the hardness-adaptive label balance strategy is essential. Moreover, for the accuracy of PIP-D, please refer to Appendix D.4.

**Ablation on periodical update strategy.** In Figure 5, we evaluate PIP-D models with fewer updates. Results show that more updates improve performance, despite a slight increase in training time.

**Ablation on different step numbers.** Instead of iterating over all future possibilities, we use one-step PI masking to approximate NP-hard feasibility mask and reduce computational cost. To provide a

    &  &  &  \\  & \(\) & & & \\   LKH3 & 0.00\% & 0.00\% & 0.00\% & 26m \\ Greedy-L & 100.00\% & 100.00\% & / & 3.2m \\ Greedy-C & 100.00\% & 100.00\% & / & 4.1s \\  GFACS* & 58.20\% & 57.81\% & 21.32\% & 6.4m \\ GFACS* + PIP & 4.72\% & 1.56\% & 15.04\% & 6.5m \\ GFACS* + PIP-D & 0.03\% & 0.00\% & 11.95\% & 6.5m \\   

Table 3: Results on Medium TSPTW-500.

Figure 4: Effects of \(_{}\)

Figure 5: Effects of Less Update.

comprehensive picture of the computational trade-offs, we further conduct experiments on PIP and PIP-D with different step numbers. In Table 4 and 5, we gather the results (solution feasibility and quality) and the inference time for different PIP steps. Results suggest that zero-step PIP saves time but suffers from unacceptable performance; the two-step PIP improves performance slightly but is computationally expensive. Hence, one-step PIP balances these trade-offs effectively.

**Comparison with LKH3 under different inference time budget.** To provide a more comprehensive comparison with LKH3, we provide additional results of LKH3 with identical time limits as the proposed approach across varying instance difficulty levels (Easy, Medium, Hard) and scales (\(n=50,100\) nodes). The time limits are configured in two ways: matching the _per instance inference time_ without parallelization and matching the _total inference time_ with parallelization on a GPU. As shown in Table 6, POMO*+PIP(-D) outperforms LKH3 on Hard datasets, while maintaining competitive results on Easy and Medium datasets. While comparing per-instance time might seem fair for CPU-based LKH3, ignoring parallelization could disadvantage GPU-based solvers (thus not fair for GPU-based solvers). To further explore this, we conduct another experiment but with a similar total inference time limit across both methods, which is a common practice in most existing NCO papers. Results, in Table 7, show that our POMO*+PIP(-D) performs consistently better than LKH3 across most of the hardness. Moreover, to leverage the strengths of both approaches and further reveal the practical usage of our method, we explore a hybrid method that combines our PIP(-D) framework with LKH3. The results, as shown in Appendix D.3, reveal that LKH3's search efficiency can be significantly enhanced when initialized with solutions from our PIP(-D) framework.

## 6 Conclusions

In this paper, we study an unsolved challenge in neural VRP solvers and correspondingly propose a novel Proactive Infeasibility Prevention (PIP) framework to advance their capabilities towards addressing VRPs with complex constraints. Technically, we introduce a Lagrangian multiplier method and preventative infeasibility masking to proactively guide the solution construction process. By further incorporating an auxiliary decoder, our PIP framework enhances training efficiency while exhibiting superior performance on more complex datasets. While our PIP is generic and has shown great ability to boost both AR and NAR constructive methods, one potential limitation is that it may not improve performance on all backbone solvers and all VRP variants. Future directions include: 1) exploring other strategies to reduce computational complexity, such as employing a trainable heatmap to confine the candidate space of PI masking calculation, 2) applying PIP to more neural methods at larger scales, 3) extending PIP to neural iterative solvers, 4) applying PIP to more VRP variants with complex constraints, including those hard-constrained VRPs whose feasibility masking is not NP-hard but with large optimality gaps, 5) exploring the applications of PIP in other domains, such as job shop scheduling, where operations need to be completed in a specific order and infeasibility can be proactively prevented using PIP, and 6) developing theoretical justifications for PIP.

    &  &  \\  & **Intel. Time** & **Oooo** & **Intel. Inf./oo** & **Intel. Inf./o** \\   & LKH3 (data) & 4.0 & 7.1 & 00.093 & 5.8 & 10.2 & 0.096 \\  & LKH3 & 25.0 & 8.9 & 0.92 & 9.5 & 10.7 & 0.096 \\  & POMO*+PIP & 0.36 & 7.8 & 0.908 & 0.93 & 10.87 & 0.096 \\  & POMO*+PIP+D & 0.36 & 7.8 & 0.908 & 0.93 & 10.66 & 0.096 \\  & LKH3 (data) & 4.0 & 13.0 & 0.908 & 11.0 & 18.3 & 0.096 \\  & LKH3 (data) & 0.53 & 13.0 & 0.908 & 0.93 & 10.0 & 0.096 \\  & POMO*+PIP+D & 0.36 & 13.0 & 0.908 & 0.93 & 10.94 & 0.196 \\  & POMO*+PIP+D & 0.33 & 13.5 & 0.45 & 0.93 & 10.95 & 0.095 \\   & LKH3 (data) & 4.0 & 25.61 & 0.129 & 5.20 & 51.24 & 0.096 \\  & LKH3 (data) & 0.53 & 25.4 & 80.06 & 0.49 & 49.84 & 97.289 \\  & POMO*+PIP+IP & 0.36 & 25.6 & 2.69 & 0.94 & 51.26 & 12.779 \\  & POMO*+PIP+D & 0.36 & 25.69 & 3.079 & 0.94 & 51.79 & 6.446 \\   

Table 6: Results on LKH3 with the similar instance inference time limit as POMO*+PIP(-D).

    & **PIP Step** & **Sol. Inf./oo** & **Intel. Inf./o** \(\) & **Gap.** & **Time** \\  POMO*+PIP & 0 & 100.00\% & 100.00\% & / & 21s \\ POMO*+PIP & 1 & 31.49\% & 16.27\% & 0.374\% & 48s \\ POMO*+PIP & 2 & 26.87\% & 12.88\% & 0.376\% & 35a \\  POMO*+PIP-D & 0 & 79.73\% & 63.29\% & 0.31\% & 21s \\ POMO*+PIP-D & 1 & 13.18\% & 6.48\% & 0.31\% & 48s \\ POMO*+PIP-D & 2 & 11.65\% & 5.63\% & 0.31\% & 35m \\   

Table 5: Results of PIP steps on Hard TSPTW-100.

    &  &  \\  & **Intel. Time** & **Ooo** & **Intel. Inf./o** \(\) & **Total Time** & **Ooo** & **Intel. Inf./o** \\   & LKH3 (data) & 4.0 & 7.1 & 00.093 & 5.8 & 10.21 & 0.096 \\  & LKH3 (data) & 25.0 & 8.91 & 0.925\% & 5.9 & 10.0 & 11.00\% \\  & POMO*+PIP & 21 & 25.76 & 0.098\% & 48s & 10.57 & 0.096 \\  & POMO*+PIP+D & 23 & 7.24 & 0.098\% & 48s & 10.66 & 0.096 \\  & LKH3 (data) & 33 & 10.12 & 0.098\% & 10.0 & 18.3 & 0.096 \\  & LKH3 (data) & 25.0 & 10.02 & 0.915\% & 6.6 & 1.006 \\  & POMO*+PIP+D & 23 & 11.49 & 0.908\% & 48s & 10.61 & 0.196 \\  & POMO*+PIP+D & 23 & 11.49 & 0.905\% & 48s & 10.79 & 0.096 \\   & LKH3 (data) & 7.1 & 29.63 & 0.12\% & 1.44 & 51.24 & 0.096 \\  & LKH3 (data) & 25.0 & 10.000\% & 54s & 7.2 & 100.00\% \\  & POMO*+PIP+D & 21 & 25.66 & 2.67\% & 48s & 51.26 & 2.72\% \\  & POMO*+PIP+D & 23 & 25.69 & 3.07\% & 48s & 51.39 & 6.43\% \\   

Table 7: Results on LKH3 with the similar total inference time limit as POMO*+PIP(-D).