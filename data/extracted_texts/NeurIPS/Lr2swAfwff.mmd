# Bridging Reinforcement Learning Theory

and Practice with the Effective Horizon

 Cassidy Laidlaw Stuart Russell Anca Dragan

University of California, Berkeley

{cassidy_laidlaw,russell,anca}@cs.berkeley.edu

###### Abstract

Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity bounds by introducing a new dataset, Bridge. It consists of 155 deterministic MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We choose to focus on deterministic environments because they share many interesting properties of stochastic environments, but are easier to analyze. Using Bridge, we find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the _random_ policy also have the highest Q-values under the _optimal_ policy (i.e. when it is optimal to be greedy on the random policy's Q function), deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the _effective horizon_, which roughly corresponds to how many steps of lookahead search would be needed in that MDP in order to identify the next optimal action, when leaf nodes are evaluated with random rollouts. Using Bridge, we show that the effective horizon-based bounds are more closely reflective of the empirical performance of PPO and DQN than prior sample complexity bounds across four metrics. We also find that, unlike existing bounds, the effective horizon can predict the effects of using reward shaping or a pre-trained exploration policy. Our code and data are available at https://github.com/cassidylaidlaw/effective-horizon.

## 1 Introduction

Deep reinforcement learning (RL) has produced impressive results in robotics , strategic games , and control . However, the same deep RL algorithms that achieve superhuman performance in some environments completely fail to learn in others. Sometimes, using techniques like reward shaping or pre-training help RL, and in other cases they don't. Our goal is to provide a theoretical understanding of why this is--a theoretical analysis that is _predictive_ of practical RL performance.

Unfortunately, there is a large gap between the current theory and practice of RL. Despite RL theorists often focusing on algorithms using strategic exploration (e.g., UCB exploration bonuses; Azar et al. , Jin et al. ), the most commonly-used deep RL algorithms, which explore randomly, resist such analysis. In fact, theory suggests that RL with random exploration is exponentially hard in the worst case , but this is not predictive of practical performance. Some theoretical research has explored instance-dependent bounds, identifying properties of MDPs when random exploration should perform better than this worst case [7; 8]. However, it is not clear whether these properties correlate with when RL algorithms work vs. fail--and our results will reveal that they tend not to.

If the current theory literature cannot explain the empirical performance of deep RL, what can? Ideally, a theory of RL should provably show why deep RL succeeds while using random exploration. It should also be able to predict which environments are harder or easier to solve empirically. Finally, it should be able to explain when and why tools like reward shaping or initializing with a pre-trained policy help make RL perform better.

We present a new theoretical complexity measure for MDPs called the _effective horizon_ that satisfies all of the above criteria. Intuitively, the effective horizon measures approximately how far ahead an algorithm must exhaustively plan in an environment before evaluating leaf nodes with random rollouts.

In order to assess previous bounds and eventually arrive at such a property, we start by creating a new dataset, Bridge, of deterministic MDPs from common deep RL benchmarks. A major difficulty with evaluating instance-dependent bounds is that they can't be calculated without tabular representations, so prior work work has typically relied on small toy environments for justification. To get a more realistic picture, we choose 155 MDPs across different benchmarks and compute their tabular representations--some with over 100 million states which must be exhaustively explored and stored. This is a massive engineering challenge, but it enables connecting theoretical and empirical results at an unprecedented scale. We focus on deterministic MDPs in Bridge and in this paper because they are simpler to analyze but still have many of the interesting properties of stochastic MDPs, like reward sparsity and credit assignment challenges. Many deep RL benchmarks are (nearly) deterministic, so we believe our analysis is highly relevent to practical RL.

Our journey to the effective horizon began with identifying a surprising property that holds in many of the environments in Bridge: one can learn to act _optimally_ by acting _randomly_. More specifically, actions with the highest Q-values under the uniformly _random_ policy _also_ have the highest Q-values under the _optimal_ policy. The random policy is about as far as one can get from the optimal policy, so this property may seem unlikely to hold. However, about two-thirds of the environments in Bridge satisfy the property. This proportion rises to four-fifths among environments that PPO , a popular deep RL algorithm, can solve efficiently (Table 1). Conversely, when this property does not hold, PPO is more likely to fail than succeed--and when it does succeed, so does simply applying a few steps of lookahead on the Q-function of the random policy (Figure 6). We found it remarkable that, at least in the environments in Bridge, modern algorithms seem to boil down to not much more than acting greedily on the random policy Q-values.

The property that it is optimal to act greedily with respect to the random policy's Q-function has important implications for RL theory and practice. Practically, it suggests that very simple algorithms designed to estimate the random policy's Q-function could efficiently find an optimal policy. We introduce such an algorithm, Greedy Over Random Policy (GORP), which also works in the case where one may need to apply a few steps of value iteration to the random policy's Q-function before acting greedily. Empirically, GORP finds an optimal policy in fewer timesteps than DQN (another deep RL algorithm) in more than half the environments in Bridge. Theoretically, it is simple to analyze GORP, which consists almost entirely of estimating the random policy's Q-function via a sample average over i.i.d. random rollouts. Since GORP works well empirically and can be easily understood theoretically, we thoroughly analyze it in the hopes of finding sample complexity bounds that can explain the performance of deep RL.

Our analysis of Greedy Over Random Policy leads to a single metric, the effective horizon, that measures the complexity of model-free RL in an MDP. As shown in Figure 1, GORP is an adaptation of a Monte Carlo planning algorithm to the reinforcement learning setup (where the transitions are unknown): it mimics exhaustively planning ahead \(k\) steps and then sampling \(m\) random rollouts from

Figure 1: We introduce the effective horizon, a property of MDPs that controls how difficult RL is. Our analysis is motivated by Greedy Over Random Policy (GORP), a simple Monte Carlo planning algorithm (left) that exhaustively explores action sequences of length \(k\) and then uses \(m\) random rollouts to evaluate each leaf node. The effective horizon combines both \(k\) and \(m\) into a single measure. We prove sample complexity bounds based on the effective horizon that correlate closely with the real performance of PPO, a deep RL algorithm, on our Bridge dataset of 155 deterministic MDPs (right).

each leaf node. The effective horizon \(H\) combines the depth \(k\) and number of rollouts \(m\). We call it the _effective_ horizon because worst-case sample complexity bounds for random exploration are exponential in the horizon \(T\), while we prove sample complexity bounds exponential only in the effective horizon \(H\). For most Bridge environments, \(H T\), explaining the efficiency of RL in these MDPs.

In the environments in Bridge, we find that the effective horizon-based sample complexity bounds satisfy all our desiderata above for a theory of RL. They are more predictive of the empirical sample complexities of PPO and DQN than several other bounds from the literature across four metrics, including measures of correlation, tightness, and accuracy (Table 2). Furthermore, the effective horizon can predict the effects of both reward shaping (Table 3a) and initializing using a pre-trained policy learned from human data or transferred from a similar environment (Table 3b). In contrast, none of the existing bounds we compare to depend on both the reward function and initial policy; thus, they are unable to explain why reward shaping, human data, and transfer learning can help RL. Although our results focus on deterministic MDPs, we plan to extend our work to stochastic environments in the future and already have some promising results in that direction.

## 2 Preliminaries

We begin by presenting the reinforcement learning (RL) setting we consider. An RL algorithm acts in a deterministic, tabular, episodic Markov decision process (MDP) with finite horizon. The MDP comprises a set of states \(\), a set of actions \(\), a horizon \(T\) and optional discount factor \(\), a start state \(s_{1}\), transition function \(f:\), and a reward function \(R:\). Throughout the paper we use \(=1\) but all our theory applies equally when \(<1\).

An RL agent interacts with the MDP for a number of episodes, starting at a fixed start state \(s_{1}\). At each step \(t[T]\) of an episode (using the notation \([n]=\{1,,n\}\)), the agent observes the state \(s_{t}\), picks an action \(a_{t}\), receives reward \(R(s_{t},a_{t})\), and transitions to the next state \(s_{t+1}=f(s_{t},a_{t})\). A policy \(\) is a set of functions \(_{1},,_{t}:()\), which defines for each state and timestep a distribution \(_{t}(a s)\) over actions. If a policy is deterministic at some state, then with slight abuse of notation we denote \(a=_{t}(s)\) to be the action taken by \(_{t}\) in state \(s\).

We denote a policy's Q-function \(Q_{t}^{}:\) and value function \(V_{t}^{}:\) for each \(t[T]\). In this paper, we also use a Q-function which is generalized to _sequences_ of actions. We use the shorthand \(a_{t:t+k}\) to denote the sequence \(a_{t},,a_{t+k}\), and define the action-sequence Q-function as

\[Q_{t}^{}(s_{t},a_{t:t+k})=_{}[_{t^{}=t}^{T} ^{t^{}-t}\,R(s_{t^{}},a_{t^{}}) s_{t},a_{t:t+k} ].\]

The objective of an RL algorithm is to find an optimal policy \(^{*}\), which maximizes \(J()=V_{1}^{}(s_{1})\), the expected discounted sum of rewards over an episode, also known as the return of the policy \(\).

Generally, an RL algorithm can be run for any number of timesteps \(n\) (i.e., counting one episode as \(T\) timesteps), returning a policy \(^{n}\). We define the _sample complexity_\(N\) of an RL algorithm as the minimum number of timesteps needed such that the algorithm has at least a 50-50 chance of returning an optimal policy:

\[N=\{n(J(^{n})=J^{*}) 1/ 2\}.\]

Here, the probability is with respect to any randomness in the algorithm itself. One can estimate the sample complexity \(N\) empirically by running an algorithm several times, calculating the number of samples \(n\) needed to reach the optimal policy during each run, and then taking the median.

The following simple theorem gives upper and lower bounds for the worst-case sample complexity in a deterministic MDP, depending on \(A\) and \(T\).

**Theorem 2.1**.: _There is an RL algorithm which can solve any deterministic MDP with sample complexity \(N T A^{T}/2\). Conversely, for any RL algorithm and any values of \(T\) and \(A\), there must be some deterministic MDP for which its sample complexity \(N T( A^{T}/2-1)\)._

All proofs are deferred to Appendix A. In this case, the idea of the proof is quite simple, and will later be useful to motivate our idea of the effective horizon: in an MDP where exactly one sequence of actions leads to a reward, an RL algorithm may have to try almost every sequence of actions to find the optimal policy; there are \(A^{T}\) such sequences. As we develop sample complexity bounds based on the effective horizon in Section 5, we can compare them to the worst-case bounds in Theorem 2.1.

**Why deterministic MDPs?** We focus on deterministic (as opposed to stochastic) MDPs in this study for several reasons. First, analyzing deterministic MDPs avoids the need to consider generalization within RL algorithms. In common stochastic MDPs, one often needs neural-network based policies, whereas in a deterministic MDP one can simply learn a sequence of actions. Since neural network generalization is not well understood even in supervised learning, analyzing generalization in RL is an especially difficult task. Second, deterministic MDPs still display many of the interesting properties of stochastic MDPs. For instance, deterministic MDPs have worst case exponential sample complexity when using naive exploration; environments with dense rewards are easier to solve empirically than those with sparse rewards; credit assignment can be challenging; and there is a wide range of how tractable environments are for deep RL, even for environments with similar horizons, state spaces, and action spaces.

Finally, many common RL benchmark environments are deterministic or nearly-deterministic. For instance, the ALE Atari games used to evaluate DQN , Rainbow , and MuZero  are all deterministic after the first state, which is selected randomly from one of only 30 start states. The widely used DeepMind Control Suite  is based on the MuJoCo simulator , which is also deterministic given the initial state (some of the environments do use a randomized start state). MiniGrid environments , which are commonly used for evaluating exploration , environment design , and language understanding , are also deterministic after the initial state. Thus, our investigation of deterministic environments is highly relevant to common deep RL practice.

## 3 Related Work

Before delving into our contributions, we briefly summarize existing work in theoretical RL and prior sample complexity bounds. Our novel bounds are contrasted with existing ones in Sections 5.1 and 6; for a detailed comparison with full definitions and proofs, please see Appendix D.

Recent RL theoretical results largely focus on strategic exploration using techniques like UCB exploration bonuses [18; 4; 19; 5; 20; 21; 22]. Such bounds suggest RL is tractable for smaller or low-dimensional state spaces. In deterministic MDPs, the UCB-based R-max algorithm [23; 18] has sample complexity bounded by \(SAT\).

Some prior work has focused on sample complexity bounds for random exploration. Liu and Brunskill  give bounds based on the covering length \(L\) of an MDP, which is the number of episodes needed to visit all state-action pairs at least once with probability at least \(1/2\) while taking actions at random. This yields a sample complexity bound of \(TL\) for deterministic MDPs. Other work suggests that it may not be necessary to consider rewards all the way to the end of the episode to select an optimal action [24; 25; 8]. One can define a "effective planning window" of \(W\) timesteps ahead that must be considered, resulting in a sample complexity bound of \(T^{2}A^{W}\) for deterministic MDPs. Finally, Dann et al.  define a "myopic exploration gap" that controls the sample complexity of using \(\)-greedy exploration, a form of naive exploration. However, in Appendix D.4, we demonstrate why their bounds are impractical and often vacuous.

There have been a few prior attempts to bridge the RL theory-practice gap. bsuite , MDP playground , and SEGAR  are collections of environments that are designed to empirically evaluate deep RL algorithms across various axes of environment difficulty. However, they do not provide theoretical explanations for why environments with varying properties are actually easier or harder. Furthermore, their environments are artificially constructed to have understandable properties. In contrast, we aim to find the mathematical reasons that deep RL succeeds or fails in "in-the-wild" environments like Atari and Procgen. Conservva and Rauber  calculate two regret bounds and compare the bounds to the empirical performance of RL algorithms. However, they consider tabular RL algorithms in simple artificial environments with less than a thousand states, while we experiment with deep RL algorithms on real benchmark environments with tens of millions of states.

Our GORP algorithm and the effective horizon are inspired by rollout and Monte Carlo planning algorithms, which have a long history [30; 31; 32; 33; 24; 34; 35; 36]. These algorithms were used effectively in Backgammon , Go , and real-time strategy games  before the start of deep RL. GORP and related Monte Carlo rollout algorithms are sometimes referred to as "one-step" or "multi-step" lookahead. Bertsekas [39; 40] suggests that one-step lookahead, possibly after a few steps of value iteration, often leads to fast convergence to optimal policies because it is equivalent to a step of Newton's method for finding a fixed-point of the Bellman equation . Our analysis suggests the success of deep RL is due to similar properties. However, we go beyond previous work by introducing GORP, which approximates a step of policy iteration in model-free RL--a setting where on-line planning approaches are not applicable. Furthermore, unlike previous work in Monte-Carlo planning, we combine our theoretical contributions with extensive empirical analysis to verify that our assumptions hold in common environments.

## 4 The Bridge Dataset

In order to assess how well existing bounds predict practical performance, and gain insight about novel properties of MDPs that could be predictive, we constructed Bridge (Bridging the RL Interdisciplinary Divide with Grounded Environments), a dataset of 155 popular deep RL benchmark environments with full tabular representations. One might assume that the ability to calculate the instance-dependent bounds we just presented in Section 3 already exists; however, it turns out that for many real environments even the number of states \(S\) is unknown! This is because a significant engineering effort is required to analyze large-scale environments and calculate their properties.

In Bridge, we tackle this problem by computing tabular representations for all the environments using a program that exhaustively enumerates all states, calculating the reward and transition functions at every state-action pair. We do this for 67 Atari games from the Arcade Learning Environment , 55 levels from the Progen Benchmark , and 33 gridworlds from MiniGrid  (Figure 5). The MDPs have state space sizes \(S\) ranging across 7 orders of magnitude from tens to tens of millions, 3 to 18 discrete actions, and horizons \(T\) ranging from 10 to 200, which are limited in some cases to avoid the state space becoming too large. See Appendix E for the full details of the Bridge dataset.

A surprisingly common property To motivate the effective horizon, which is introduced in the next section, we describe a property that we find holds in many of the MDPs in Bridge. Consider the random policy \(^{}\), which assigns equal probability to every action in every state, i.e., \(^{}_{t}(a s)=1/A\). We can use dynamic programming on a tabular MDP to calculate the random policy's Q-function \(Q^{=^{}}\). We denote by \((Q^{^{}})\) the set of policies which act greedily with respect to this Q-function; that is,

\[(Q^{^{}})= s,t_{t}(s )_{a}Q^{^{}}_{t}(s,a)}.\]

Perhaps surprisingly, we find that all the policies in \((Q^{^{}})\) are _optimal_ in about two-thirds of the MDPs in Bridge. This proportion is even higher when considering only the environments where PPO empirically succeeds in finding an optimal policy (Table 1). Thus, it seems that this property may be the key to what makes many of these environments tractable for deep RL.

## 5 The Effective Horizon

We now theoretically analyze why RL should be tractable in environments where, as we observe in Bridge, it is optimal to act greedily with respect to the random policy's Q-function. This leads to a more general measure of an environment's complexity for model-free RL: the effective horizon.

Our analysis centers around a simple algorithm, GORP (Greedy Over Random Policy), shown in Algorithm 1. GORP constructs an optimal policy iteratively; each iteration \(i\) aims to calculate an optimal policy \(_{i}\) for timestep \(t=i\). In the case where we set \(k=1\) and \(^{}=^{}\), GORP can solve environments which have the property we observe in Bridge. It does this at each iteration \(i\) by simulating \(m\) random rollouts for each action from the state reached at timestep \(t=i\). Then, it

   Acting greedily &  \\ with respect to &  \\ \(Q^{^{}}\) is optimal? &  & No \\  Yes & **80 MDPs** & 24 MDPs \\ No & 15 MDPs & **36 MDPs** \\   

Table 1: The distribution of the MDPs in our Bridge dataset according to two criteria: first, whether PPO empirically converges to an optimal policy in 5 million timesteps, and second, whether acting greedily with respect to the Q-function of the random policy is optimal. We find that a surprising number of environments satisfy the latter property, especially when only considering those where PPO succeeds.

averages the \(m\) rollouts' returns to obtain a Monte Carlo estimate of \(Q^{^{}}\) for each action. Finally, it greedily picks the action with the highest estimated Q-value.

Besides taking advantage of the surprising property we found in Bridge, GORP has other properties which help us bridge the theory-practice gap in RL. It explores randomly, like common deep RL algorithms, meaning that it can give us insight into why random exploration works much better than the worst-case given in Theorem 2.1. Also, unlike other RL algorithms, it has cleanly separated _exploration_ and _learning_ stages, making it much easier to analyze than algorithms in which exploration and learning are entangled.

Furthermore, GORP is extensible beyond environments satisfying the property we found in Bridge. First, it can solve MDPs where one may have to apply a few steps of value iteration to the random policy's Q-function before acting randomly. Second, it can use an "exploration policy" \(^{}\) different from the random policy \(^{}\). These two generalizations are captured in the following definition. In the definition, we use the notation that a step of Q-value iteration transforms a Q-function \(Q\) to \(Q^{}=(Q)\), where

\[Q^{}_{t}(s,a)=R_{t}(s,a)+_{a^{}}Q_{t+1}(f(s,a),a^{}).\]

**Definition 5.1** (\(k\)-QVI-solvable).: _Given an exploration policy \(^{}\) (\(^{}=^{}\) unless otherwise noted), let \(Q^{1}=Q^{^{}}\) and \(Q^{i+1}=(Q^{i})\) for \(i=1,,T-1\). We say an MDP is \(k\)-QVI-solvable for some \(k[T]\) if every policy in \((Q^{k})\) is optimal._

We will see that running GORP with \(k>1\) will allow it to find an optimal policy in MDPs that are \(k\)-QVI-solvable. Although the sample complexity of GORP scales with \(A^{k}\), we find that nearly all of the environments in Bridge are \(k\)-QVI-solvable for very small values of \(k\) (Figure 6).

We now use GORP to define the effective horizon of an MDP. Note that the total number of timesteps sampled by GORP with parameters \(k\) and \(m\) is \(T^{2}A^{k}m=T^{2}A^{k+_{A}m}\). Thus, analogously to how the horizon \(T\) appears in the exponent of the worst-case sample complexity bound \(O(TA^{T})\), we define the _effective_ horizon as the exponent of \(A\) in the sample complexity of GORP:

**Definition 5.2** (Effective horizon).: _Given \(k[T]\), let \(H_{k}=k+_{A}m_{k}\), where \(m_{k}\) is the minimum value of \(m\) needed for Algorithm 1 with parameter \(k\) to return the optimal policy with probability at least \(1/2\), or \(\) if no value of \(m\) suffices. The effective horizon is \(H=_{k}H_{k}\)._

By definition, the sample complexity of GORP can be given using the effective horizon:

**Lemma 5.3**.: _The sample complexity of GORP with optimal choices of \(k\) and \(m\) is \(T^{2}A^{H}\)._

As we noted in the introduction, when \(H T\), as we find is often true in practice, this is far better than the worst-case bound given in Theorem 2.1 which scales with \(A^{T}\).

Definition 5.2 does not give a method for actually calculating the effective horizon. It turns out we can bound the effective horizon using a generalized gap notion like those found throughout the RL theory literature. We denote by \(_{t}^{k}\) the gap of the Q-function \(Q^{k}\) from Definition 5.1, where

\[_{t}^{k}(s)=_{a}Q_{t}^{k}(s,a)-_{a^{} _{a}Q_{t}^{k}(s,a)}Q_{t}^{k}(s,a^{}).\]

The following theorem gives bounds on the effective horizon in terms of this gap.

**Theorem 5.4**.: _Suppose that an MDP is \(k\)-QVI-solvable and that all rewards are nonnegative, i.e. \(R(s,a) 0\) for all \(s,a\). Let \(^{k}\) denote the gap of the Q-function \(Q^{k}\) as defined in Definition 5.1.__Then_

\[H_{k} k+_{t[T],s^{}_{i},}_{A} (^{k}(s,a)V_{t}^{*}(s)}{_{t}^{k}(s)^{2}})+_{A}6 (2TA^{k}),\]

_where \(^{}_{i}\) is the set of states visited by some optimal policy at timestep \(i\) and \(V_{t}^{*}(s)=_{}V_{t}^{}(s)\) is the optimal value function._

A full proof of Theorem 5.4 is given in Appendix A. Intuitively, the smaller the gap \(_{t}^{k}(s)\), the more precisely we must estimate the Q-values in GORP in order to pick an optimal action.

The GORP algorithm is very amenable to theoretical analysis because it reduces the problem of finding an optimal policy to the problem of estimating several \(k\)-step Q-values, each of which is a simple mean of i.i.d. random variables. There are endless tail bounds that can be applied to analysis of GORP; we use some of these to obtain even tighter bounds on the effective horizon in Appendix C.

Why should the effective horizon, which is defined in terms of our GORP algorithm, also explain the performance of deep RL algorithms like PPO and DQN which are very different from GORP? In Appendix B, we present two algorithms, PG-GORP and FQI-GORP, which are more similar to PPO and DQN but whose sample complexities can still be bounded with the effective horizon. We also give additional bounds on the effective horizon and lower bounds on sample complexity.

### Examples of the effective horizon

To gain some intuition for the bound in Theorem 5.4, consider the examples in Figure 2. MDP (a) has extremely sparse rewards, with a reward of 1 only given for a single optimal action sequence. However, note that this MDP is still 1-QVI-solvable by Definition 5.1. The maximum of the bound in Theorem 5.4 is at \(t=1\) with the optimal action, where \(Q_{1}^{1}(s,a)=1/A^{T-1}\), \(V_{t}^{*}(s)=1\), and \(_{1}^{1}(s)=1/A^{T-1}\). Ignoring logarithmic factors and constants gives \(H 1+_{A}A^{T-1}=T\). That is, in the case of MDP (a), the effective horizon is no better than the horizon.

Next, consider MDP (b), which has dense rewards of 1 for every optimal action. Again, this MDP is 1-QVI-solvable. The maximum in the bound is obtained at \(t=1\) and the optimal action with \(Q_{1}^{1}(s,a) 2\), \(V^{*}(s)=T\), and the gap \(_{1}^{1}(s,a) 1\). Again ignoring logarithmic factors gives in this case \(H 1+_{A}T=(1)\). In this case, the effective horizon is much shorter than the horizon, and barely depends on it! This again reflects our intuition that in this case, finding the optimal policy via RL should be much easier.

MDP (c) is similar to MDP (b) except that all rewards are delayed to the end of the episode. In this case, the \(Q\) function is the same as in MDP (b) so the effective horizon remains \((1)\). This may seem counterintuitive since one needs to consider rewards \(T\) timesteps ahead to act optimally. However, the way GORP uses random rollouts to evaluate leaf nodes means that it can implicitly consider rewards quite far in the future even without needing to exhaustively plan that many timesteps ahead.

Finally, consider MDP (d), the first 50 timesteps of the Atari game Freeway, which is included in the Bridge dataset. This MDP is also 1-QVI-solvable and the maximum in the bound is obtained in the state shown in Figure 1(d) at timestep \(t=10\). Plugging in the Q values shown in the figure gives \(H 10.2\), which is far lower than the horizon \(T=50\). The low effective horizon reflects how this MDP is much easier than the worst case in practice. Both PPO and DQN are able to solve it with a sample complexity of less than 1.5 million timesteps, while the worst case bound would suggest a

Figure 2: Examples of calculating the effective horizon \(H\) using Theorem 5.4; see Section 5.1 for the details.

sample complexity greater than \(50 3^{50}/2 10^{25}\) timesteps!

**Comparison to other bounds** Intuitively, why might the effective horizon give better sample complexity bounds than previous works presented in Section 3? The MDP in Figure 1(b) presents a problem for the covering length and UCB-based bounds, both of which are \((A^{T})\). The exponential sample complexity arises because these bounds depend on visiting every state in the MDP during training. In contrast, GORP doesn't need to visit every state to find an optimal policy. The effective horizon of \((1)\) for MDP (b) reflects this, showing that our effective horizon-based bounds can actually be much smaller than the state space size, which is on the order of \(A^{T}\) for MDP (b).

The effective planning window (EPW) does manage to capture the same intuition as the effective horizon in the MDP in Figure 1(b): in this case, \(W=1\). However, the analysis based on the EPW is unsatisfactory because it entirely ignores rewards beyond the planning window. Thus, in MDP (c) the EPW \(W=T\), making EPW-based bounds no better than the worst case. In contrast, the effective horizon-based bound remains the same between MDPs (b) and (c), showing that it can account for the ability of RL algorithms to use rewards beyond the window where exhaustive planning is possible.

## 6 Experiments

We now show that sample complexity bounds based on the effective horizon predict the empirical performance of deep RL algorithms far better than other bounds in the literature. For each MDP in the Bridge dataset, we run deep RL algorithms to determine their empirical sample complexity. We also use the tabular representations of the MDPs to calculate the effective horizon and other sample complexity bounds for comparison.

**Deep RL algorithms** We run both PPO and DQN for five million timesteps for each MDP in Bridge, and record the empirical sample complexity (see Appendix F for hyperparameters and experiment details). PPO converges to the optimal policy in 95 of the 155 MDPs, and DQN does in 117 of 155. At least one of the two finds the optimal policy in 119 MDPs.

**Sample complexity bounds** We also compute sample complexity bounds for each MDP in Bridge. These include the worst-case bound of \(TA^{T}\) from Theorem 2.1, the effective-horizon-based bound of \(T^{2}A^{H}\) from Lemma 5.3, as well as three other bounds from the literature, introduced in Section 3 and proved in Appendix D: the UCB-based bound \(SAT\), the covering-length-based bound \(TL\), and the effective planning window (EPW)-based bound of \(T^{2}A^{W}\).

**Evaluation metrics** To determine which sample complexity bounds best reflect the empirical performance of PPO and DQN, we compute a few summary metrics for each bound. First, we measure the _Spearman (rank) correlation_ between the sample complexity bounds and the empirical sample complexity over environments where the algorithm converged to the optimal policy. The correlation (higher is better) is a useful for measuring how well the bounds can rank the relative difficulty of RL in different MDPs.

Second, we compute the _median ratio_ between the sample complexity bound and the empirical sample complexity for environments where the algorithm converged. The ratio between the bound \(N_{}\) and empirical value \(N_{}\) is calculated as \(\{N_{}/N_{},N_{}/N_{}\}\). For instance, a median ratio of 10 indicates that half the sample complexity bounds were within a factor of 10 of the empirical sample complexity. Lower values indicate a better bound; this metric is useful for determining whether the sample complexity bounds are vacuous or tight.

Finally, we consider the binary classification task of predicting whether the algorithm will converge at all within five million steps using the sample complexity bounds. That is, we consider simply thresholding each sample complexity bound and predicting that only environments with bounds below the threshold will converge. We compute the _area under the ROC curve (AUROC)_ for this prediction task as well as the _accuracy_ with the optimal threshold. Higher AUROC and accuracy both indicate a better bound.

**Results** The results of our experiments are shown in Table 2. The effective horizon-based bounds have higher correlation with the empirical sample complexity than the other bounds for both PPO and DQN. While the EPW-based bounds are also reasonably correlated, they are significantly off in absolute terms: the typical bound based on the EPW is 3-4 orders of magnitude off, while the effective horizon yields bounds that are typically within 1-2 orders of magnitude. The UCB-basedbounds are somewhat closer to the empirical sample complexity, but are not well correlated; this makes sense since the UCB bounds depend on strategic exploration, while PPO and DQN use random exploration. Finally, the effective horizon bounds are able to more accurately predict whether PPO or DQN will find an optimal policy, as evidenced by the AUROC and accuracy metrics.

As an additional baseline, we also calculate the four evaluation metrics when using the empirical sample complexity of PPO to predict the empirical sample complexity of DQN, or vice-versa, and when using the empirical sample complexity of GORP to predict PPO or DQN's performance (bottom two rows of Table 2). While these are not provable bounds, they provide another point of comparison for each metric. The effective horizon-based bounds correlate about as well with PPO and DQN's sample complexities as they do with each other's. The empirical performance of GORP is typically even closer to that of PPO and DQN than the effective horizon-based bounds.

**Reward shaping, human data, and transfer learning** In order for RL theory to be useful practically, it should help practitioners make decisions about which techniques to apply in order to improve their algorithms' performance. We show how the effective horizon can be used to explain the effect of three such techniques: reward shaping, using human data, and transfer learning.

Potential-based reward shaping  is a classic technique which can speed up the convergence of RL algorithms. It is generally used to transform a sparse-reward MDP like the one in Figure 1(a) to a dense-reward MDP like in Figure 1(b) without changing the optimal policy. If the effective horizon is a good measure of the difficulty of RL in an environment, then it should be able to predict whether (and by how much) the sample complexity of practical algorithms changes when reward shaping is applied. We develop 77 reward-shaped versions of the original 33 Minigrid environments and run PPO and DQN. Results in Table 2(a) show the effective horizon accurately captures the change in sample complexity from using reward shaping. We use similar metrics to those in Table 2: the correlation between the predicted and empirical ratio of the shaped sample complexity to the unshaped sample complexity, and the median ratio between the predicted change and the actual change.

Out of the five bounds we consider, three--worst case, covering length, and UCB--don't even depend on the reward function. The EPW does depend on the reward function and captures some of the effect of reward shaping for DQN, but does worse at predicting the effect on PPO. In comparison, the effective horizon does well for both algorithms, showing that it can accurately capture how reward shaping affects RL performance.

    &  &  \\  & Correl. & Median ratio & AUROC & Acc. & Correl. & Median ratio & AUROC & Acc. \\  Worst-case (\(T[A^{T}/2]\)) & 0.24 & \(7.2 10^{10}\) & 0.57 & 0.63 & 0.15 & \(5.5 10^{10}\) & 0.67 & 0.76 \\ Covering length (\(TL\)) & 0.35 & \(6.3 10^{6}\) & 0.78 & 0.72 & 0.27 & \(3.9 10^{6}\) & 0.86 & 0.85 \\ EPW (\(T^{2}A^{W}\)) & 0.69 & \(1.1 10^{5}\) & 0.78 & 0.75 & 0.58 & \(8.0 10^{4}\) & 0.88 & 0.85 \\ UCB (\(SAT\)) & 0.26 & **20** & 0.68 & 0.67 & 0.31 & **31** & 0.67 & 0.77 \\ Effective horizon (\(T^{2}A^{H}\)) & **0.81** & 31 & **0.92** & **0.86** & **0.74** & 67 & **0.92** & **0.86** \\  _Other deep RL algorithm_ & 0.77 & 2.3 & 0.84 & 0.85 & 0.77 & 2.3 & 0.86 & 0.99 \\ _GORP empirical_ & 0.79 & 7.3 & 0.77 & 0.82 & 0.65 & 11 & 0.80 & 0.94 \\   

Table 2: Effective horizon-based sample complexity bounds are the most predictive of the real performance of PPO and DQN according to the four metrics we describe in Section 6. The effective horizon bounds are about as good at predicting the sample complexity of PPO and DQN as one algorithm’s sample complexity is at predicting the other’s.

    &  &  \\  & Correl. & Ratio & Correl. & Ratio \\  EPW & 0.20 & 2.1 & **0.70** & 12 \\ Effective horizon & **0.48** & 2.4 & 0.35 & 12 \\ Other bounds & 0.00 & **1.3** & 0.00 & **1.9** \\   

Table 3: The effective horizon explains the effects of reward shaping and initializing with a pretrained policy by accurately predicting their effects on the empirical sample complexity of PPO and DQN. Correlation and median ratio are measured between the predicted change in sample complexity and the empirical change. See Section 6 for further discussion.

Another tool used to speed up RL is initializing with a pre-trained policy, which is used practically to make RL work on otherwise intractable tasks. Can the effective horizon also predict whether initializing RL with a pre-trained policy will help? We initialize PPO with pre-trained policies for 82 of the MDPs in Bridge, then calculate new sample complexity bounds based on using the pre-trained policies as an exploration policy \(^{}\). Table (b)b shows that the effective horizon accurately predicts the change in sample complexity due to using a pre-trained policy. Again, three bounds--worst case, EPW, and UCB--do not depend on the exploration policy at all, while the covering length gives wildly inaccurate predictions for its effect. In contrast, the effective horizon is accurate at predicting the changes in sample complexities when using pre-trained policies.

**Long-horizon environments** We also perform experiments on full-length Atari games to evaluate the predictive power of the effective horizon in longer-horizon environments. It is intractable to construct tabular representations of these environments and thus we cannot compute instance-dependent sample complexity bounds. However, it is still possible to compare the empirical performance of PPO, DQN, and GORP. If the performance of GORP is close to that of PPO and DQN, then this suggests that the effective horizon, which is defined in terms of GORP, can explain RL performance in these environments as well. Figure 3 compares the learning curves of PPO, DQN, and GORP in deterministic versions of the seven Atari games from Mnih et al.  (see Appendix F.1 for details). GORP performs better than both PPO and DQN in two games and better than at least one deep RL algorithm in an additional three games. This provides evidence that the effective horizon is also predictive of RL performance in long-horizon environments.

## 7 Discussion

Overall, our results suggest the effective horizon is a key measure of the difficulty of solving an MDP via reinforcement learning. The intuition behind the effective horizon presented in Section 5 and the empirical evidence in Section 6 both support its importance for better understanding RL.

**Limitations** While we have presented a thorough theoretical and empirical justification of the effective horizon, there are still some limitations to our analysis. First, we focus on deterministic MDPs with discrete action spaces, leaving the extension to stochastic environments and those with continuous action spaces an open question. Furthermore, the effective horizon is not easily calculable without full access to the MDP's tabular representation. Despite this, it serves as a useful perspective for understanding RL's effectiveness and potential improvement areas. An additional limitation is that the effective horizon cannot capture the performance effects of generalization--the ability to use actions that work well at some states for other similar states. For an example where the effective horizon fails to predict generalization, see Appendix G.1. However, the effective horizon is still quite predictive of deep RL performance even without modeling generalization.

**Implications and future work** We hope that this paper helps to bring the theoretical and empirical RL communities closer together in pursuit of shared understanding. Theorists can extend our analysis of the effective horizon to new algorithms or explore related properties, using our Bridge dataset to ground their investigations by testing assumptions in real environments. Empirical RL researchers can use the effective horizon as a foundation for new algorithms. For instance, Brandfonbrener et al.  present an offline RL algorithm similar to GORP with \(k=1\); our theoretical understanding of GORP might provide insights for improving it or developing related algorithms.

Figure 3: Learning curves for PPO, DQN, and GORP on full-horizon Atari games. We use 5 random seeds for all algorithms. The solid line shows the median return throughout training while the shaded region shows the range of returns over random seeds.