# ###### Abstract

###### Abstract

High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies due to the reward model sharing weights with the target model, amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and a judge model collaborate. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model's responses, mitigating biases in the process. We also introduce a feedback mechanism to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment across four applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 14.50% in four visuo-motor control tasks.

## 1 Introduction

Foundation models, including large language models (LLMs) and large vision-language models (LVLMs), have greatly enhanced AI model's ability to understand text, interpret images, and follow human instructions. Despite their impressive performance across many tasks, they still face reliability issues such as hallucinations, stemming from misalignment with human instructions (Thakur et al., 2024; Ouyang et al., 2022) or different modality information (Zhou et al., 2024; Wang et al., 2024; Yu et al., 2024). To address these misalignment issues, recent studies have employed preference learning techniques--such as reinforcement learning from human feedback (RLHF) (Yu et al., 2024; Sun et al., 2023) and direct preference optimization (DPO) (Deng et al., 2024; Rafailov et al., 2024), to align the outputs of foundation models with human preferences in LLMs or to harmonize multimodal knowledge in LVLMs.

The success of preference fine-tuning techniques hinges on the availability of high-quality, large-scale preference datasets. Researchers currently employ two main methods for constructing these datasets. The first involves human annotation, which yields high-quality data but is often limited in scale due to its labor-intensive nature (Yu et al., 2024; Ji et al., 2024). The second method uses external AI models to generate preference data Li et al. (2023); Zhou et al. (2024); however, this approach may fail to capture the inherent preferences of the target model being fine-tuned, rendering the generated data less useful. Recently, the self-rewarding (Zhou et al., 2024; Yuan et al., 2024) approach samples the target model's own outputs as responses and uses the model itself to reward these responses, constructing preference pairs. While promising, this method depends on the performance of the target model when serving as its own reward model. Inaccurate rewarding can bias the generated preference pairs, seriously compromising data quality. Therefore, improving the process of synthetic preference data synthesis is crucial for effective preference fine-tuning, given the scarcity of high-quality preference data and the challenges associated with annotation.

In this paper, as illustrated in Figure 1, we propose Anyprefer, a self-evolving synthetic preference data synthesis framework designed to automatically curate high-quality preference datasets. Anyprefer models the preference data synthesis process as a two-player cooperative Markov game between the _Target Model_ and the _Judge Model_ parameterized by the input prompts to maximize the feedback from _Reward Model_. In general, the goal for the _target model_ is to generate high-quality pairwise preference data and the goal for the _judge model_ is to provide robust and consistent ranking for the generated response. Anyprefer can accommodate various downstream applications, such as natural language generation, natural vision-language understanding, medical image analysis, and visuo-motor control. Specifically, Anyprefer generates preference data following the process of (1) response sampling, (2) response rewarding, (3) data quality evaluation, and (4) prompt optimization. First, in the model sampling stage, the _target model_ generates a set of candidate responses based on the input prompts. Next, the _judge model_ leverages external tools to gather relevant knowledge for rewarding these responses. Once ranked, the responses are used to construct preference data, which is then fed into a reward model to evaluate whether the preference data meets general quality criteria. Finally, with the feedback from the _reward model_, we refine the policy of the target model and the policy for the judge model by improving the prompt for these two models. Throughout this process, the target model and judge model act as cooperative players, working together to enhance preference data quality.

**Why Introducing Tools in Judge Model?** The inclusion of external tools is essential for ensuring annotation accuracy. Anyprefer strategically selects tools based on the input data to extract valuable information, mitigating bias during response rewarding. Additionally, the feedback mechanism introduced in the policy stage not only dynamically adjusts input prompts but also shares feedback with these tools, further enhancing their performance in supporting the judge model.

In summary, the primary contribution of this paper is Anyprefer, the first automatic framework for preference data synthesis. Experimental results across four key applications--natural language generation, vision-language understanding, medical image analysis, and visuo-motor control--spanning 21 datasets or tasks, demonstrate the effectiveness and advantages of Anyprefer in generating high-quality preference data and facilitating effective preference fine-tuning. In these four applications, Anyprefer achieves improvements of 18.55%, 3.66%, 30.05%, and 14.50%, respectively. Additionally, our experiments demonstrate the effectiveness of the tool-augmented judgment and feedback mechanism. Furthermore, we have compiled the synthesized data into a new preference dataset, Anyprefer-V1, comprising 58K high-quality preference pairs. The detailed information is presented in Appendix Table 14, compared to previous synthesized preference data, Anyprefer-V1 includes a broader range of application scenarios and data types. This will benefit the open-source community and further advance AI alignment research.

Figure 1: The figure illustrates the Anyprefer framework. First, Anyprefer selects the necessary tools based on the input prompt to obtain supplementary information, which is then integrated into a knowledge base. Next, the target model generates several responses for the input data. The judge model then ranks these responses using the constructed knowledge base. Subsequently, Anyprefer combines the best and worst-ranked responses into a preference pair. The reward model will then evaluate the quality of this preference pair, and all unqualified pairs will go through the optimization stage to refine its quality by using the proposed feedback mechanism.

## 2 Anyprefer

To address the challenges of synthesizing high-quality preference data, we propose an automatic framework called Anyprefer, which models the preference data synthesis process as a two-player cooperative Markov game. As illustrated in Figure 1, the target model and the judge model serve as two collaborative players working together to perform preference data synthesis. The target model first generates response candidates based on the input prompt, while the judge model integrates information from various tools to accurately reward and rank the responses. The ranked candidates are then evaluated by a reward model to ensure they meet general data quality criteria. Feedback from the reward model is used to optimize both the input prompts and the tools employed, enhancing the quality of low-quality preference data pairs. Ultimately, qualified preference pairs are used as preference data for preference fine-tuning. In the following sections, we will first detail the problem formulation and then discuss how to generate the preference data.

### Problem Formulation

In this section, we discuss the formulation of the proposed Anyprefer framework. To begin with, we denote the input data prompt as \(\) (e.g., a natural image) and the set of knowledge tools \(\{_{i}\}_{i=1}^{M}\). Each knowledge tool \(_{i}\) (e.g., Grounded SAM (Ren et al., 2024)) takes the data \(\) as the input and output a sequence \(_{i}=_{i}()\) extracting the information from \(\) using model \(_{i}\) as a delegate.

We model the preference data synthesis as a two-player cooperative Markov Game (MG). In particular, the first player is the target model \(_{t}\) which takes the data \(\) as input and generate a set of candidates \(\{_{c}\}_{c=1}^{C}\). The second player is the judge model \(_{j}\), it takes the candidate set \(\{_{c}\}_{c=1}^{C}\) and the knowledge base model \(\{_{i}\}_{i=1}^{M}\) as an input, then outputs the preference pair \(\{_{+},_{-}\}\). From the model selection perspective, judge model \(_{j}\) actively aggregates the information from \(_{i}\) and rank the \(\{_{c}\}\) output by \(_{t}\). Since both \(_{t}\) and \(_{j}\) are language-based models, the input prompt \(_{t}\) and \(_{j}\) can be used to serve as their parameters, respectively. The goal of this MG is to generate a set of preference pair \(\{_{+},_{-}\}\) so that the collected preference data can improve the preference fine-tuning of the target model \(_{t}\). Generally, it is costly and time-consuming to directly evaluate the preference fine-tuning performance in every step, we instead use a reward model \((_{+},_{-})\) to provide a surrogate reward by evaluating whether the target model benefits from the preference data \(\{_{+},_{-}\}\). Therefore the goal of this framework can be formulated as

\[*{arg\,max}_{_{t},_{j}}_{(_{+},_{-})}(_{+},_{-}) _{t}(|_{t}),_{j}(|_{j}),, \{_{i}\}_{i},\] (1)

where the expectation is taken over \((_{+},_{-})_{j}(|\{_{c}\}_{c};\{ _{i}\}_{i};_{j})\) and \(_{c}_{t}(|;_{t})\). According to equation 1, in the preference data generation process, it is feasible to optimize prompt \(_{t}\) and \(_{j}\) using policy optimization with prompt-based gradient ascent (Pryzant et al., 2023).

### Response Sampling and Rewarding

To synthesize preference data using Anyprefer, the first stage is sampling several candidate responses. Specifically, for a given input prompt \(\), we sample \(C\) unique response candidates \(\{_{c}\}_{c=1}^{C}\) from the target model \(_{t}(|_{t})\), where \(_{t}\) is initialized with the input prompt \(\). In our experimental setup, \(C\) is universally set to 5, balancing diversity of samples with sampling costs.

After sampling the candidate responses, the next step is to use the judge model to accurately reward and rank these responses \(\{_{c}\}_{c=1}^{C}\). To reduce potential bias from relying solely on the target model for evaluation (Yuan et al., 2024; Guo et al., 2024), we introduce a tool-augmented rewarding strategy for a more comprehensive evaluation. These knowledge tools gather relevant information from various perspectives to assist the judge model \(_{j}\) in providing accurate rewards. Based on the input prompt and candidate response, along with its own parameters (policy), i.e., the system prompt \(_{j}\), the judge model strategically aggregates information captured by external tools for evaluation. Specifically, the tools extract relevant information \(_{i}=_{i}()\) from the input prompt \(\). The judge model \(_{j}\) then leverages this extracted knowledge \(_{i}\) to provide an overall score \(_{j}(|_{c};\{_{i}\}_{i};_{j})\) for each candidate response \(_{c}\). Finally, the candidates are ranked, and the top-scoring response is selected as the preferred response \(_{+}\), while the lowest-scoring is selected as the disperferred response \(_{-}\), forming the preference pair \(\{_{+},_{-}\}\). The initial system prompt \(_{j}\) used in the judgemodel are detailed in Appendix E. And note that this prompt as part of the policy parameters can be constantly updated through the formulated two-player MG framework.

### Data Quality Evaluation

Ideally, after identifying the preference pair \(\{_{+},_{-}\}\), we can directly use it to fine-tune the target model, collecting performance feedback to enhance the prompts \(_{j}\) and \(_{t}\) of both the judge model and target model. This, in turn, improves the data synthesis process. However, the fine-tuning process can be costly and time-consuming, which prevents the immediate feedback for updating the judge model and the target model, setting barriers for effectively optimizing the policy. To address this issue, we instead adapt LLM-as-a-Judge strategy (Zheng et al., 2023) to a LLM-based reward model \(\) to judge the data quality. Here, the used LLM-as-a-Judge prompt can be found in the Appendix E. This reward model can evaluate the quality of the generated preference pair \(\{_{+},_{-}\}\) and return a reward \((_{+},_{-})\) that reflects the quality, and diversity of every preference pair. Generated preference pairs with high-quality rewards will be directly collected into the final preference dataset, while the others will be re-generated via the cooperation between the target model and judge model, using an updated policy guided by the reward \((_{+},_{-})\).

### Learning from the Feedback

To effectively refine and improve the filtered low-quality preference data, we can use the obtained reward \((_{+},_{-})\) as the feedback to optimize the policy of the target model and judge model as illustrated in equation 1. Specifically, for updating the policy of the target model \(_{t}\), the input prompt \(_{t}\) can be optimized to increase the probability of sampling more high-quality and diverse responses from the target model \(_{t}\). For updating the policy of the judge model \(_{j}\), the used system prompt \(_{j}\) will be also optimized, which will finally affect the aggregation of the tools information. Motivated by Przyant et al. (2023) and Yuksekgounul et al. (2024), the above policy optimization process can be formulated as follows:

\[_{t}_{t}+_{_{t}} (_{+},_{-}),_{j} _{j}+_{_{j}}(_{+},_{-}),\] (2)

where \(\) is the prompt adjustment step. The above policy gradient method aims at iteratively refining the input prompt (parameters) \(_{t}\) and \(_{j}\) of the target model \(_{t}\) and judge model \(_{j}\), respectively. By iteratively updating these parameters, the updated players \(\{_{t},_{j}\}\) are expected to better cooperate on generating preference pairs that meet criteria of the reward model and increase the reward. Finally, the proposed policy optimization are expected to effectively enhance the quality of the generated preference data. The overall algorithm flow is provided in 1 in the Appendix.

## 3 Experiment

In this section, empirically demonstrate how the preference data constructed by Anyprefer effectively enhances the performance of various foundation models across four downstream applications. We address the following key questions: (1) Does the preference data generated by Anyprefer improve model performance across diverse applications and benchmarks? (2) Can Anyprefer boost the capabilities of different foundation models through iterative preference learning? (3) Is there a positive correlation between the surrogate reward provided by the reward model and the performance of preference fine-tuning on the target model (i.e., the actual reward)? (4) What is the quality of the preference data automatically synthesized by Anyprefer?

### Applications and Experimental Setups

This section provides an overview of the downstream applications along with their corresponding experimental settings, deployment details, evaluation benchmarks, and baselines. The downstream applications include natural language generation, vision-language understanding, medical image analysis, and visuo-motor control, which are detailed below:

**Natural Language Generation.** The first application is using large language models for natural language generation. In our experiments, we utilize LLaMA2-7B-chat (Touvron et al., 2023) as the target model. We use GPT-4o as the judge model, which will utilize two tools: DuckDuckGo for web search1 and FsfairX-LLaMA3-RM-v0.1 (Xiong et al., 2024) for response quality assessment. The GPT-4o is also adopted as the reward model to provide the immediate feedback for the generated preference pair. For baseline methods, we include original LLaMA2 model and self-rewarding approach Yuan et al. (2024) for comparison. For evaluation, we use three natural language benchmarks: GSM8K (Cobbe et al., 2021), ARC-easy/challenge (Clark et al., 2018), and AlpacaEval (Li et al., 2023d), covering commonsense question answering, math reasoning and alignment domains. Further implementation details are provided in Appendix C.1.

**Natural Vision-Language Understanding.** The second downstream application is using large Vision-Language Models (LVLMs) for natural vision-language understanding. In this application, we use LLaVA-1.5 7B as the target model. For tool selection, we leverage several state-of-the-art vision models as external knowledge sources, including the visual detection model Florence-2-large (Xiao et al., 2023), the short captioning model BLIP-2 (Li et al., 2023b), and the detection and segmentation model Grounded SAM (Ren et al., 2024). Additionally, we employ a powerful central multimodal model, GPT-4o, to integrate and interpret all the information for judgment and reward assessment. For baselines, we compare original LLaVA-1.5 7B model and LLaVA-1.5 7B with the self-rewarding approach. For evaluation, we follow the setup from Zhou et al. (2024a) and validate Anyprefer on three types of benchmarks: comprehensive benchmarks, general QA benchmarks, and hallucination benchmarks. For specific configurations, please refer to Appendix C.2.

**Medical Image Analysis.** Furthermore, we also evaluate Anyprefer in medical image analysis (MIA). Here, we use LLVA-Med v1.5 (Li et al., 2023a) as the target model, which is a variant of LLaVA fine-tuned specifically for medical image understanding. For the tools and reward model selection, we use several powerful medical models in specific tasks (e.g., detection, captioning) as external knowledge source, including MiniGPT-Med (Alkhaldi et al., 2024), MedVInT (Zhang et al., 2023), CheXagent (Chen et al., 2024a) and a powerful central multimodal model (i.e., GPT-4o) for understanding and integrating all the information into judgment and rewarding. It is worthwhile to noting that the current Med-LVLMs are unable to generate high-quality data as preferred responses (Xia et al., 2024). Therefore, unlike natural language generation and vision-language understanding applications, we utilize the target model solely to synthesize dispreferred responses (Chen et al., 2024b), while the ground truth serves as the preferred responses. For evaluation, we conduct experiments on two tasks using three datasets: VQA-RAD (Lau et al., 2018) and SLAKE (Liu et al., 2021) for the medical VQA task, and IU-Xray (Demner-Fushman et al., 2016) for the report generation task. Implementation details are provided in Appendix C.3.

**Visuo-Motor Control.** The final application in Anyprefer is using vision-language-action model for visuo-motor control (VMC). In this case, we employ OpenVLA (Kim et al., 2024) as the target model. To implement Anyprefer, we use the image segmentation model Grounded SAM 2 (Ren et al., 2024) as a tool to segment the objects involved in the tasks and obtain their pixel coordinates. We then employ GPT-4o as a judge model to generate trajectory cost functions based on the pixel coordinate information and task prompts, including path cost, grasp cost, and collision cost. Following a feedback mechanism, the feedback generated by the scoring model is fed back to the judge model to produce prompts better suited for the current task, improving object segmentation and trajectory generation through multiple iterations. For baselines, we include several mainstream robotic

Figure 2: We evaluated Anyprefer using benchmarks from four applications. The target model represents the original model before preference fine-tuning. For medical image analysis, “B” for BLEU, “R” for ROUGE-L, “M” for METEOR, “C” for closed, and “O” for open tasks. In medical iamge analysis, “RAD”: VQA-RAD, “IU”: IU-Xray.

models, including RT-1 (Brohan et al., 2022), Octo-small (Team et al., 2024), Octo-base (Team et al., 2024), and OpenVLA-SFT (OpenVLA fine-tuned on the Simpler-Env (Li et al., 2024) dataset through SFT). We evaluate our model and the baseline models on four WidowX Robots tasks within the Simpler-Env (Li et al., 2024): "placing the carrot on a plate", "putting the spoon on a towel", "stacking the green cube on top of the yellow cube", and "placing the eggplant into a basket". We compare the generated trajectories with the ground truth trajectories, evaluating the accuracy of task completion by the generated trajectories. See detailed implementations in Appendix C.4.

### Main Results

In Figure 2, we compare Anyprefer with two key baselines: the original target model and self-rewarding. Detailed results, along with values from additional baselines tailored to each specific application, are provided in Table 2 to 13 in Appendix. Overall, Anyprefer demonstrates significant improvements across various applications, including natural language generation, vision-language understanding, medical image analysis, and visuomotor control. Specifically, in natural language generation, Anyprefer achieves up to a 10.92% increase in accuracy on the GSM8K and ARC datasets compared to baselines. On vision-language understanding benchmarks, Anyprefer outperforms both the original LLaVA-1.5 and the self-rewarding approach, notably achieving a 6.8% improvement on the VisWiz dataset. For medical image analysis, Anyprefer delivers the best performance, with an average improvement of 31.05% in medical VQA and report generation tasks. In visuomotor control, we observed success rate increases of up to 14.5% across various tasks.

Additionally, the self-rewarding approach also surpasses the original target model, further demonstrating the effectiveness of synthesized preference data. By integrating tool information and feedback-guided policy optimization, Anyprefer significantly enhances the model's ability to generate more accurate and high-quality responses, making the constructed preference data more precise and effective. Moreover, in specialized domains like medical image analysis and visuomotor control, where data scarcity often leads to unstable performance in target models, the inclusion of additional tools and feedback mechanisms helps overcome the knowledge limitations of the original models, resulting in substantial performance gains.

### Ablation Study

We conduct ablation studies to evaluate the effectiveness of incorporating tools for response judgment and the feedback mechanism for policy optimization. The results in Table 1 demonstrate that introducing additional tools significantly improves overall model performance compared to the original model that only use GPT-4o as the judge model. This outcome aligns with our expectations, as the external tools enhance the comprehensiveness of the judge model in rewarding and ranking candidate responses, while also reducing bias in the ranking process to some extent. Moreover, incorporating the feedback mechanism to optimize the policy--both the prompts for the target model and the judge model--further boosts performance, with an average improvement of 21.51% across all applications. For more specific results, please refer to Tables 3, 7, 10 and 13 in the Appendix. These findings indicate that the feedback mechanism elevates the quality of preference data, thereby strengthening the target model.

### Can Anyprefer Support Model Self-Improvement?

In this section, we validate if Anyprefer can continuously improve model performance across four applications through iterative updates. At each iteration, the Anyprefer framework generate the preference data, and then use the data to fine-tune the target model. As shown in Figure 3, we report the performance of Anyprefer in natural language generation, vision-language understanding, medical image analysis, and visuomotor control. Through multiple iterative updates, Anyprefer exhibits significant performance improvements in all tasks. For instance, in natural language generation, the model demonstrates a notable score increase on the GSM8K dataset

   T & TF & **LLM** & **LVLM** & **Med-LVLM** & **VLA** \\   & & 56.88 & 67.90 & 23.35 & 28.0 \\ ✓ & & 59.88 & 68.82 & 25.24 & 30.5 \\ ✓ & ✓ & **61.03** & **69.61** & **30.60** & **40.5** \\   

Table 1: Ablation study on the impact of tools and feedback. The table presents the average scores for each benchmark. “T” represents tool-augmented judgment, and “F” represents feedback mechanism.

compared to the baseline. Similarly, in vision-language understanding and medical image analysis, the model demonstrates significant progress, achieving improvements of 3.66% and 31.02%, respectively. In the visuo-motor control task, Anyprefer shows the most significant improvement in success rate, with a 14.5% increase compared to the base model. These results indicate that Anyprefer exhibits strong self-improvement capabilities across all four applications, improving the quality of preference data with each iteration, leading to better overall model performance.

### Analysis of Judge Model

In this section, we use natural vision-language understanding as an example to analyze the scoring accuracy of the judge model with and without tools (T) and feedback mechanism (F). We manually selected 200 examples, consisting of 100 samples generated using tool-captured knowledge and feedback mechanisms, and 100 samples generated without them. A human evaluation was conducted following the criteria outlined in Appendix E. The results, as shown in Figure 4, demonstrate that the introduction of tools and feedback mechanisms significantly improves the accuracy of the judge model: with tools and feedback mechanisms, the judge model's accuracy reaches 89.6%, whereas without them, it is only 67.2%, showing an absolute improvement of approximately 22.4%. This suggests that tools and feedback mechanisms can greatly enhance the judge model's evaluation accuracy, resulting in better ranking of responses generated by the target model.

### Analysis of Reward Model

Furthermore, we conducted experiments to evaluate whether the surrogate reward scores provided by the reward model in Anyprefer are highly correlated with the actual reward scores, i.e., the preference fine-tuning performance of the target model. We compared the correlation between the target model's performance over three preference fine-tuning iterations in Anyprefer and the surrogate reward scores corresponding to the preference data pairs generated by the target model during those iterations. As shown in Figure 5, the preference data produced by Anyprefer consistently improves the target model's performance across all four applications over three iterations. Moreover, as the iterations progress, the average surrogate reward score generated by our reward model increases in parallel with the target model's performance. This indicates a strong correlation between the surrogate reward scores and the direct evaluation results of preference tuning, demonstrating the effectiveness of our reward model in providing reliable surrogate rewards.

### Analysis of Synthesized Dataset Diversity and Quality

In this section, we evaluate the preference data Anyprefer-V1 synthesized by Anyprefer, comparing it against existing synthesized preference datasets to verify its diversity and qualtiy. Diversity is analyzed using methods from (Zhao et al., 2024), while data quality are evaluated through manual annotations and GPT-4 scoring, which are detailed as follow:

**Data Diversity.** For diversity, we categorize the datasets in Table 14 into two groups: natural language datasets and multimodal datasets. We select two representative datasets from each group and randomly sample 2,000 instances from each. Specifically, HH-RLHF and Orca are chosen for the natural language group, while LLaVA-RLHF and VLFeedback are selected for the multimodal group. The text data from both groups are mapped using the text encoder from CLIP-ViT-Base, and the image data in the multimodal group are mapped using the target model's image encoder. We apply t-SNE (Van der Maaten and Hinton, 2008) to project these embeddings into a two-dimensional space, as shown in Figure 6. The results show that Anyprefer-V1 nearly covers the full range of other datasets, both for text-only and multimodal data. Moreover, it occupies regions of the embedding space that are not covered by other datasets, highlighting its greater diversity.

**Data Quality.** For quality assessment, we randomly sampled 800 examples for manual evaluation, focusing primarily on two aspects: the difficulty of the data and the satisfaction level with the data. Specific scoring criteria and guidelines are provided in Appendix E.3. The results, shown in Figure 7, demonstrate that the difficulty of the preference data constructed by our framework mostly falls within the moderate range, with a reasonable distribution that avoids being too difficult or too simple. Moreover, the human evaluation results indicate that annotators are generally satisfied with the data generated by Anyprefer, which suggests that the preference data constructed by Anyprefer is of high quality. Furthermore, we randomly selected 200 examples from the VLFeedback, Orca, and our constructed Anyprefer-V1 datasets, and used GPT-4o to score them on a scale of 1 to 10, with a higher score indicating higher data quality. The results are represented as bar charts in part (b) of Figure 7. From the results we can see that it is clear that the data constructed by our framework received relatively higher scores, aligning with the manual validation results. This further demonstrates the high quality of the data generated by Anyprefer.

## 4 Conclusion

This paper introduces the Anyprefer framework, an automatic system for synthesizing high-quality preference data across diverse applications. By establishing a cooperative Markov game that synchronizes the target model with the judge model and incorporating external tools and feedback mechanisms, Anyprefer enhances both the quality and diversity of generated preference data, Anyprefer-V1, resulting in improved target model performance. Experimental results show that Anyprefer significantly boosts performance in applications such as natural language generation, vision-language understanding, medical image analysis, and visuo-motor control. Moreover, the experiments demonstrate the effectiveness of Anyprefer in enabling model self-improvement, as well as the value of tool-augmented response judgment and feedback mechanisms.

Figure 6: Comparison of Anyprefer-V1 and other representative datasets in t-SNE mapping.

Figure 7: Data quality evaluation. (a) shows the results of manual evaluation from two aspects, and (b) represents the results of GPT-4o scoring.