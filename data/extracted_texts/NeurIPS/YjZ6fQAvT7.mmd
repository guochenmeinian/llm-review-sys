# TreeVI: Reparameterizable Tree-structured Variational Inference for Instance-level Correlation Capturing

TreeVI: Reparameterizable Tree-structured Variational Inference for Instance-level Correlation Capturing

Junxi Xiao\({}^{1}\)&Qinliang Su\({}^{1,2}\)

\({}^{1}\)School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China

\({}^{2}\)Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China

xiaojx7@mail2.sysu.edu.cn suqliang@mail.sysu.edu.cn

Corresponding author.

###### Abstract

Mean-field variational inference (VI) is computationally scalable, but its highly-demanding independence requirement hinders it from being applied to wider scenarios. Although many VI methods that take correlation into account have been proposed, these methods generally are not scalable enough to capture the correlation among data instances, which often arises in applications involving graphs or explicit constraints among instances. In this paper, we developed the Tree-structured Variational Inference (TreeVI)2, which uses a tree to capture the correlation among latent variables in the posterior. We show that samples from the tree-structured posterior can be reparameterized efficiently and parallelly, making its training cost just 2 or 3 times that of VI under the mean-field assumption. To capture correlation with more complicated structure, the TreeVI is further extended to the multiple-tree case. Furthermore, we show that the underlying tree structure can be automatically learned from training data. With experiments on synthetic datasets, constrained clustering, user matching and link prediction, we demonstrate the efficacy of TreeVI in capturing instance-level correlation in posteriors and enhancing the performance of downstream applications.

## 1 Introduction

Variational inference is a probabilistic method that is widely used for approximating the exact posterior in latent-variable models \(p(,)=p(|)p()\), with \(\) and \(\) being the observed data and latent variable, respectively. When not considering the existence of any relation among instances, we can write the model into a factorized form as \(p(,)=_{i}p(_{i}|_{i})p(_{i})\), where \(_{i}\) and \(_{i}\) denote the \(i\)-th data instance and latent variable. Thanks to the factorized form, we can reasonably use the mean-field posterior \(q(|)=_{i}q(_{i}|_{i})\) for model inference and training. However, there are also many circumstances, under which complex relations among instances exist. For instances, in graph-structured data, VGAE  uses two instances' latent representations to model the probability of existing an edge \(e_{ij}\) between them via \(p(e_{ij}|_{i},_{j})\), which, obviously, makes all instances coupled together. Also, in the constrained clustering, DC-GMM  designs a prior \(p(;)\) to specify the assignment constraints and then uses it to construct the prior distribution \(p(,)=p(;)_{i}p(_{i}|c_ {i})\), which will also cause all \(_{i}\) to be correlated with each other. Obviously, under these circumstances, due to the existence of correlation among instances, it is unreasonable to assume a factorized form for the posterior anymore.

There has been many work on incorporating correlation structures into posterior approximation in variational inference recently [23; 31; 43], and they are mainly focused on modeling correlation structure among different dimensions within each latent variable. However, the amount of latent dimensions is usually no more than a few hundred, at a completely different level from the size of dataset which might be as large as millions. Therefore, these variational inference methods modeling correlations among dimensions cannot be leveraged to capture instance-level correlation especially for large datasets. To investigate correlations among different instances, Tang et al.  recently used a tree structure over latent variables to take instance-level correlations into consideration, but only capable of capturing correlations between immediately connected nodes.

In this work we propose a novel posterior approximation for variational inference, the Tree-structured Variational Inference (TreeVI), that models the latent correlation structure with a tree-structure-induced distribution and enables modeling high-order correlations between non-adjacent latent variables. Our method provides a matrix-form reparametreization for tree-structured correlated latents based on ancestral sampling, that is scalable to large datasets with computational complexity comparable to mean-field variational inference methods. To better capture structural correlation, we also generalize TreeVI to multiple trees, and propose a continuous optimization algorithm to stochastically learn a theoretically correlation-rich tree or mixture-of-trees structure.

## 2 Tree-structured Variational Inference

Consider a latent-variable model \(p_{}(,)=p_{}(|) p()\), where \(=[_{1}^{},_{2}^{},,_{N}^ {}]\) and \(=[_{1}^{},_{2}^{},,_{N} ^{}]\); and \(_{i}\) and \(_{i}^{D}\) denote the \(i\)-th data instance and its corresponding latent variable, respectively. Given the latent-variable model \(p_{}(,)\), variational inference aims to find a distribution that is closet to the true posterior \(p_{}(|)\) from a distribution family \(\), which is achieved by maximizing the evidence lower bound (ELBO)

\[(,)=_{ q_{}( |)}[ p_{}(,)- q _{}(|)],\] (1)

where \(q_{}(|)\) denotes the approximate posterior parameterized by \(\). In most of current works, the latent-variable model is assumed to take a factorized form as \(p_{}(,)=_{i=1}^{N}p_{}(_{i}|_{i})p(_{i})\) due to the observed independence among data instances. With the factorized form for the model, it can be seen that the true posterior \(p_{}(|)\) also takes a factorized form, hence it is reasonable to choose a fully factorized form \(q_{}(|)=_{i=1}^{N}q_{}(_ {i}|_{i})\) for the approximate posterior. However, for applications like constrained clustering with generative model and generative modeling on graph data, we often need to impose some constraints among the latent variables \(\{_{i}\}_{i=1}^{N}\) by using a correlated prior distribution \(p()\) with \(p()_{i=1}^{N}p(_{i})\), or use several latent variables \(_{j}\) to be responsible for generating a data instance \(_{i}\) (_e.g._, \(_{i}\) representing an edge), which will result in \(p_{}(|)_{i=1}^{N}p_{}( _{i}|_{i})\). Obviously, under these scenarios, latent variables \(\{_{i}\}_{i=1}^{N}\) from the true posterior \(p_{}(|)\) are not independent anymore. Thus, if a factorized posterior \(q_{}(|)=_{i=1}^{N}q_{}(_ {i}|_{i})\) is still employed to approximate the true posterior, the model will lose the ability to capture the correlations among data instances. To alleviate the deviation caused by mean-field approximation, we propose to approximate the correlation structure with a tree or a mixture-of-tree structure as shown in Fig. 1, respectively.

### TreeVI: Variational Inference under a Single Tree

To capture the correlations among latent variables \(\{_{i}\}_{i=1}^{N}\) in the posterior distribution, a multivariate normal distribution \(q(|)=(;_{}, _{})\) can be used, where \(_{}=[_{1}^{},,_{N}^{}]^{ }^{ND}\) is the mean and \(_{}=(_{})\,(_{})\) is the \(ND ND\) covariance matrix; \(_{}=[_{1}^{},,_{N}^{} ]^{}^{ND}\) is the standard deviation; and \(\) means the correlation matrix, with its \((i,j)\)-th block taking the form

\[[]_{ij}=(_{ij});\] (2)

\(_{ij}(-1,1)^{D}\) for \(i j\) denotes the correlation strength between latent variable \(_{i}\) and \(_{j}\) and \(_{ii}_{D}\) for \(i\{1,2,,N\}\). Here, the diagonal structure assumed for the \((i,j)\)-th block \([]_{ij}\) implies that we are only interested in capturing the correlation among different variables, without considering the correlation of different dimensions within one variable. Actually, due to the relatively low dimensions of a latent variable \(_{i}\), the correlation within a variable can be captured by combining with existing VI methods, but we here omit the modeling of correlation among dimensions for simplicity. In the following context, the diagonal operators are omitted for conciseness.

A crucial part of the variational inference is to determine appropriate values for the correlation parameters \(^{0}=\{_{ij}:i j\}\). For a general multivariate normal distribution, all correlation parameters in \(^{0}\) are learnable, which admits a highly expressive correlation structure, but also resulting in high computational complexity. The widely-used mean-field approximation reduces the cost by assuming an independence structure within any pair of latent variables, _i.e._, \(_{ij}=_{D}\) for any \(i j\), completely ignoring the correlation among latent variables. In order to achieve a balance between expressiveness and complexity, we propose to approximate the fully-connected correlation structure with a tree structure, introducing our TreeVI, namely tree-structured variational inference.

The idea of tree-structured variational inference is to impose a tree correlation structure \(=(,)\) among latent variables, as illustrated in the quin-variate example in Fig. 0(b). In our TreeVI, given a tree structure \(=(,)\), we only specify the correlation parameters \(_{ij}\) for latent variables \((_{i},_{j})\) adjacent on the tree, and for any non-adjacent latent variables \((_{i},_{j})\), the correlation between then are directly computed as the multiplication of correlation parameters along \(_{i j}\), that is

\[}_{ij}=_{(s,t)_{i j}} _{st},(i,j),\] (3)

where \(_{i j}\) denotes the path connecting the latent variable \(_{i}\) to \(_{j}\) on the tree \(\), and the vector multiplication is assigned as Hadamard product. Therefore, only the correlation strengths w.r.t. the edges of the tree \(\) are learnable, which can be denoted as \(^{}=\{_{ij}:(i,j) \}^{0}\). The parameterization leads to a correlation matrix \([^{()}]_{ij}=}_{ij}\) that is totally determined by parameters in \(^{}\), with \(}_{ii}=_{D}\) for \(i\). For example, for the quin-variate tree structure in Fig. 0(b), the correlation parameter between \(_{1}\) and \(_{5}\) is fixed as \(}_{12}_{23} _{35}\).

With the tree-structured correlation defined above, it can be easily shown that the distribution \(q_{}^{}(|)\) forms a Markov random field defined by the tree \(\), and its joint probability can be expressed as

\[q_{}^{}(|)=_{i }q_{}(_{i}|_{i})_{(i,j) }}(_{i},_{j}| _{i},_{j})}{q_{}(_{i}|_{i})q_{}(_{j}|_{j})},\] (4)

where the marginal distribution \(q_{}(_{i}|_{i})=(_{ i};_{i},(_{i}^{2}))\), and pairwise marginal distribution for adjacent variables \((i,j)\) is \(q(_{i},_{j}|_{i},_{j})=( _{i},_{j};_{ij},_{ij})\); and the mean \(_{ij}=[_{i}^{},_{j}^{} ]^{}\) and covariance matrix

\[_{ij}=_{i} _{i}&_{ij}_{i} _{j}\\ _{ij}_{i}_{ j}&_{j}_{j};\] (5)

\(_{ij}\) is the correlation parameter between two adjacent variables. Furthermore, we can also see that the distribution \(q_{}^{}(|)\) can be represented by an acyclic Bayesian network of the form

\[q_{}^{}(|)=q_{}(_{1})_{(i,j)}q_{}( _{j}|_{i}),\] (6)

Figure 1: Comparison between the fully-connected correlation structure and approximation by single and multiple tree-structured variational inference

where the conditioning on data \(_{i}\) is omitted for conciseness; \(_{1}\) is assumed to be the root node; the conditional distribution of latent variables with respect to an edge \((i,j)\) is

\[q_{}(_{j}|_{i})=(_{j};_{j}+_{ij}_{j} _{i}^{-1}(_{i}-_{i}), _{j}_{D}-_{ij}^{2}} ).\] (7)

With the conditional distribution \(q_{}(_{j}|_{i})\), joint samples \((_{1},_{2},,_{N})\) can be drawn from \(q_{}^{}(|)\) with ancestral sampling. By sampling \(_{i}\) one by one with ancestral sampling, we can show that the joint sample \((_{1},_{2},,_{N})\) can be equivalently represented by a set of \(N\) independent Gaussian noises \(\{_{i}\}_{i=1}^{N}\) with \(_{i}(_{D},_{D})\), as stated in the theorem below.

**Theorem 1**.: _Suppose that \(N\) latent variables \(=[_{1},,_{N}]^{}\) follow a tree-structured posterior distribution \(q_{}^{}(|)\) with the tree structure \(=(,)\), the joint sample \((_{1},_{2},,_{N}) q_{}^{}(|)\) can be expressed as_

\[_{j}=_{j}+}_{1j} _{1}_{j}+_{i_{1  j},i 1}}_{ij}_{D}- _{(i),i}^{2}}_{i} _{j},\;\;j,\] (8)

_where \(_{i}(_{D},_{D})\) is a Gaussian random noise and \((i)\) denotes the parent node of \(_{i}\) with respect to \(i\)._

For the proof and a concrete example, we refer to Appendix B. With the reparameterization of sample \(_{j}\) in Eq. (8), the joint sample \(=[_{1}^{},,_{N}^{}]^{}\) can be re-parameterized in a matrix-form as

\[^{()}=_{}+_{}^{()}=[_{1}^{},,_{N}^{ }]^{},\] (9)

where \(_{i}(_{D},_{D})\) for \(i\); and \(_{}^{()}\) is a \(ND ND\) matrix, with its \((i,j)\)-th block \([_{}^{()}]_{ij}=(_{ij})\), and

\[_{ij}=}_{1j} _{j},&i=1,j,\\ }_{ij}_{D}-_{ (i),i}^{2}}_{j},&i 1,i_{1  j},j,\\ _{D},&.\] (10)

Actually, if the variables are indexed according to their positions on the tree \(\) from left to right and then top to bottom, it can be easily shown that \(_{}^{()}\) is a lower-triangular block matrix. For example, the matrix \(_{}^{()}\) corresponding to the quin-variate example of Fig. 0(b) can be written as follows

\[_{}^{()}=(_{ })_{D}&\\ }_{12}&_{D}-_{12 }^{2}}&\\ }_{13}&}_{23}_{D}-_{12}^{2}}&_{D}-_{23}^{2}}&\\ }_{14}&}_{24}_{D}-_{12}^{2}}&_{D}&_{ D}-_{24}^{2}}&\\ }_{15}&}_{25}_{D}-_{12}^{2}}&}_{35} _{D}-_{23}^{2}}&_{D}&_{D}-_{35}^{2}}.\] (11)

where \(()\) has been omitted for conciseness.

By parameterizing the mean \(_{i}\), standard deviation \(_{i}\) and the correlation parameters \(_{ij}\) for \((i,j)\) with a neural network \(f_{}(,)\) as

\[_{ij}=f_{}(_{i},_{j}), (i,j),\] (12)

according to Eq. (9), samples drawn from \(q_{}^{}(|)\) can be reparameterized in the form of neural network parameters \(\) and random Gaussian noise \(\), which can facilitate the training of variational inference significantly. Importantly, except the mean and standard deviation, we only need to re-parameterize the parameters \(^{}=\{_{ij}:(i,j)\}\), while computing the other non-zero elements in \(_{}^{()}\) with \(^{}\). In this way, we only need to re-parameterize \(||\) parameters, and thus only need to run \(||\) times of neural network \(f_{}(,)\), instead of \((N^{2})\) times in the vanilla method. For a tree, the number of edges \(|| N-1\), thus we only need to additionally run \((N)\) times of neural networks \(f_{}(,)\), in addition to the runs required by the mean-field method. For a mean-field variational inference that assumes independence among instances, for each epoch, it also needs to run times of neural network. Thus, the complexity of our proposed method is roughly only 2 times of the mean-field method. As seen in the experiments on constrained clustering, the consumed time of our TreeVI is only 2 to 3 times that of mean-field variational inference methods.

It should be pointed that if we simply apply Cholesky decomposition to the correlation matrix \(\) to produce \(=^{}\) and then directly re-parameterize the elements \(_{ij}\) in the lower-triangular matrix \(\) by a neural network \(f_{}(,)\), that is, \(_{ij}=f_{}(_{i},_{j})\), then we have to re-parameterize as many as \(N(N+1)/2\) elements, i.e., all elements from the lower-triangular positions of \(\) need to be re-parameterized, where \(N\) is the number of instances in training dataset. That means we need to run the neural network \(f_{}(,)\) by \((N^{2})\) times for every epoch, which is computationally unacceptable, especially considering that \(N\) could be as large as millions in practice. And our main contribution lies at finding a way to reduce the required times of running the neural network \(f_{}(,)\) from \((N^{2})\) to \((N)\) by restricting the correlation matrix \(\) to a special form \(^{()}\) constructed from a tree \(=(,)\), which is actually a dense matrix with its \((i,j)\)-th element \([^{()}]_{ij}=}_{ij}\) defined as Eq. (3) for \((i,j)\). Under the restricted correlation matrix \(^{()}\), the lower-triangular matrix \(_{}^{()}\) possesses a very elegant form, as shown in Eq. (10). The elegance lies at that although \(_{}^{()}\) still has \((N^{2})\) non-zero elements, all of these non-zero elements can be explicitly computed from the \(||\) parameters \(^{}=\{_{ij}:(i,j)\}\).

With the tree-structured posterior, the data log-likelihood has the evidence lower bound \( p()_{ q_{}^{}( |)}[ p_{}(,)]+[q_{}^{}(|)]\), where the first term can be estimated by reparameterization \(^{()} q_{}^{}(|)\), and the entropy \(\) of the tree-structured posterior distribution \(q_{}^{}(|)=_{i}q_{ }(_{i}|_{i})_{(i,j)}}(_{i},_{j}|_{i},_{j})}{q_{ }(_{i}|_{i})q_{}(_{j}|_{j})}\) can be decomposed as entropy terms with respect to singleton posterior \(q_{}(_{i}|_{i})\) and pairwise posterior \(q_{}(_{i},_{j}|_{i},_{j})\) which can be both directly computed (for detailed expressions we refer to Appendix). Therefore, the optimization of variational inference with tree-structured posterior approximation can be simplified as maximizing the following sampling-based evidence lower bound of our proposed TreeVI

\[^{}(,,)= p_{ }(,^{()})+[q_{}^{ }(|)],\] (13)

where \(^{()}\) denotes the tree-structured reparameterization for latent variables with respect to the tree structure \(\). For detailed computation for the evidence lower bound, we refer to Appendix C.1.

### Extension to Multiple Trees

The expressiveness of a single tree-structured posterior is still restrictive. To alleviate this issue, we propose to approximate the true posterior with a mixture-of-trees posterior distribution

\[q_{}^{}(|)=_{m=1}^{M}_{m}q_{}^{_{m}}(|),\] (14)

where we use a weighted mixture of tree structures \(=\{_{1},,_{M}\}\), as shown in Fig. 0(c), to approximate the underlying correlation structure, and the posterior \(q_{}^{_{m}}(|)\) with respect to the \(m\)-th tree component is expressed in the form of Eq. (4). Each tree component \(_{m}=(,_{m})\) corresponds to a set of latent indices \(=\{1,,N\}\), a set of pairwise connections \(_{m}\), a set of correlation parameters \(^{_{m}}=\{_{ij}^{m}:(i,j)_{m}\}\) to be learned, and a weight controlled by a tree coefficient \(_{m}\), where \(m=1,,M\).

With the mixture-of-trees posterior, the data log-likelihood has the evidence lower bound \( p()_{m=1}^{M}_{m}_{ q_{}^{_{m}}(|)}[ p_{}(,)]+[q_{}^{}(|)]\), where the first term can be estimated by tree-structured reparameterization \(^{(_{m})} q_{}^{_{m}}(| )\) within each tree component \(_{m}\) for \(m=1,,M\), but the entropy term has no explicit expression. Since the entropy \([p]\) is concave in the probability distribution \(p\), the entropy of the mixture-of-trees posterior distribution is lower bounded by the weighted sum of entropies with respect to each tree component \(_{m=1}^{M}_{m}[q_{}^{_{m}}(| )]\), which can be similarly computed by the singular and pairwise marginal posterior distributions with respect to each tree component \(_{m}\), \(m=1,,M\). Therefore, the optimization of variational inference with mixture-of-trees posterior approximation can be simplified as maximizing the following sampling-based evidence lower bound of our proposed MTreeVI

\[^{}(,,)=_{m=1}^{M}_{m} [ p_{}(,^{(_{m})})+[q_{}^{_{m}}(|)]],\] (15)

where \(^{(_{m})}\) denotes the tree-structured reparameterization for latent variables with respect to the tree structure \(_{m}\). For detailed computation for the evidence lower bound, we refer to Appendix C.2.

### Learning the Tree Structure from Data

Our proposed tree-structured and mixture-of-trees structured posterior approximation make variational inference with pairwise latent correlations feasible, but a specific tree structure over latent variables need to be determined in advance. In this section, our goal is to develop an efficient algorithm for learning correlation-rich tree structures to approximate the underlying posterior correlation structure.

To learn a meaningful tree structure from the latent embeddings, we adopt a symmetric binary matrix \(\{0,1\}^{N N}\) with entries 0 along the diagonal, where each element \(a_{ij}\{0,1\}\) represents an edge between latents \(_{i}\) and \(_{j}\), \(i j\{1,,N\}\) (here we assume \(D=1\) for convenience).

**Proposition 1**.: _Suppose that the symmetric adjacency matrix \(\{0,1\}^{N N}\), then the undirected structure induced by adjacency matrix \(\) is acyclic if and only if_

\[h()=(^{2})=0\] (16)

_where \(()\) and \(()\) represent the trace and exponential of a matrix respectively, and \(\) is the Hadamard product._

For the proof we refer to the Appendix D.1. Inherited from the idea of Zheng et al. , we use a similar indicator function \(h()\) in Eq. (16) to check the acyclicity of the structure induced by the symmetric binary matrix \(\). If and only if \(h()=0\), the adjacency matrix \(\) determines a unique acyclic undirected graph, which can be leveraged to build a tree structure \(()=(,)\) with nodes \(\{1,,N\}\) and edges \(=\{(i,j):a_{ij} 0,\,i j\}\). Under the constraints Eq. (16), we seek to establish the following continuous optimization problem

\[_{,,}(,,, ),h()=0,\] (17)

where \((,,,)=-^{( )}(,,)\) is the negative of the evidence lower bound given by Eq. (13), and the correlation parameters \(^{()}=\{_{ij}:a_{ij} 0,\,i  j\}\) of the learned tree is determined by the binary adjacency matrix \(\) with \(_{ij}=f_{}(_{i},_{j})\) calculated by the neural network. A similar optimization can be implemented to stochastically learn a meaningful mixture of trees by using a set of symmetric adjacency matrices \(=\{_{1},,_{M}\}\) to represent multiple tree components, and further maximizing the evidence lower bound Eq. (15) under acyclic constraints Eq. (16) of each matrix in \(\). The constrained optimization problem above can be further converted into unconstrained subproblems with Lagrangian multiplies and efficiently solved by numerical algorithms or stochastic gradient methods. For implementation details, we refer to Appendix D.3.

To initialize the tree structure before constrained optimization, the easiest way is to randomly build from the fully connected graph over data instances by using depth-first search (DFS) algorithm. To enrich the initialized tree structure with neighboring correlation information, the uniform sampling process in the DFS algorithm can be further modified to generate a meaningful neighborhood for each instance, by assigning the probability of sampling a neighbor of each instance according to their similarity. For detailed implementation of the tree initialization, we refer to the Appendix D.2.

## 3 Related Work

Variational inference is broadly used for approximating intractable posterior in latent variable models, but notorious for its restricted variational families, especially mean-field variational family which is still widely used by modern generative models [4; 37]. So far there has been a wide variety of variational inference methods that attempt to improve on traditional mean-field variational inference by modeling dependence structures within latent posterior distribution, such as constructing the variational distribution with a normalizing flow [30; 6; 39], which deterministically transforms a simple probability distribution over latent variable to a complex one through a sequence of invertible and differentiable functions with tractable Jacobians. Besides deterministic transformations, the form of structured variational inference can be diverse, such as constructing with implicit models [12; 35; 26], modeling dependencies between local and global parameters [11; 38], constructing with a mixture of variational distributions [16; 27; 20], determining pairwise dependencies between univariate marginals with copula functions [34; 13; 33], and designing variational distribution with hierarchies [38; 1; 25]. However, these work are mostly focused on distributional assumption over latent dimensions, that fail to be directly extended to capture instance-level correlation structure.

Recently, there has been some work on incorporating instance-level correlation structure in variational inference [24; 36; 29]. Manduchi et al.  designs a prior information matrix to express must-link and cannot-link constraints between data in an explicit way, and integrates the instance-level prior information into the framework of variational inference by conditioning on the prior clustering preferences. But the instance-level correlation is only considered in the generative process regardless of the correlation structure induced from latent posterior by still adopting an amortized mean-field variational distribution. The work of Tang et al.  is the most related to ours, which takes instance-level correlation structure into consideration when learning latent representations, but restricts both the prior and posterior distributions to be identically tree-structured for tractable optimization. However, this assumption is unrealistic in most variational inference scenarios, and high-order correlations are unable to be modeled within the tree structure.

## 4 Experiments

**Tasks & Datasets.** We evaluate our methods with four different tasks: synthetic evidence lower bound test, constrained clustering, user matching and link prediction, on synthetic dataset, standard datasets (MNIST, Fashion MNIST, Reuters and STL-10), public movie rating and product rating datasets, respectively. We refer to Appendix E.1 for more dataset details.

**Baselines & Implementation Details.** For baselines on the user matching and link prediction tasks, we include the standard variational auto-encoders  and a recent modification to VAE  taking instance-level correlation structure into consideration. And we also compare our method to the state-of-the-art method learning latent embeddings with graph convolutional networks, GraphSAGE . With regard to the constrained clustering task, we take a variety of constrained clustering methods, e.g., the traditional pairwise constrained K-means (PCKmeans, ), deterministic deep constrained clustering method based on DEC (SDEC, ) and constrained IDEC (C-IDEC, ), as state-of-the-art constrained clustering methods for comparison. We also include the generative models, the unsupervised VaDE , the graph augmented VaDE (DGG, ), and the weakly-supervised DC-GMM . For comparison, we experiment on our methods TreeVI and MTreeVI, where the number of tree components for the mixture-of-trees posterior is fixed as \(M=3\). Aadditional information related to experimental implementation details are in Appendix E.2.

### Synthetic VAE

We design a synthetic dataset with a graph-structured latent variable model. The dataset contains \(N=6000\) data points \(_{1},,_{N}^{D}\) with \(D=4\), each independently generated from the conditional distribution \(p(|)=(;,^{2}_{4})\) given latent embeddings \(_{1},,_{N}^{D}\) where \(^{2}\) is a fixed value and set to 0.5. The latent embeddings \(_{1},,_{N}\) are drawn from a zero-mean normal distribution \(p()=(;_{i},_{})\), and the graph-structured correlation is incorporated into the latent covariance matrix \(_{}=_{4}+\) via an affinity matrix \(^{4 4}\) assigned with a loopy graph structure

\[=0&1&0&0.3\\ 1&0&1&0.3\\ 0&1&0&0.4\\ 0.3&0.3&0.4&0\] (18)

  
**Methods** & **Lower Bound** \\  Mean-field & -11.1535 \\ TreeVI (\(_{1}\)) & -10.8998 \\ TreeVI (\(_{2}\)) & -10.6137 \\ MTreeVI & -10.3586 \\  \( p()\) & -10.3417 \\   

Table 1: Estimated lower bounds (ELBO) of VAE with posterior distributions approximation by mean-field distributions, tree structures \(_{1}\) and \(_{2}\), as well as their mixture model MTreeVI(\(_{1},_{2}\)), compared to ground truth log-likelihood \( p()\).

where \(\) is leveraged to control the overall correlation strength and set to 0.5. Two tree structures over latent dimensions \(_{1}=(,_{1})\) and \(_{2}=(,_{2})\) are designed with vertex set \(=\{1,2,3,4\}\) and edge sets \(_{1}=\{(1,2),(1,3),(2,4)\}\) and \(_{2}=\{(1,2),(1,4),(2,3)\}\), respectively. The posterior correlation structure is modeled by \(_{1}\), \(_{2}\) and their mixture model MTreeVI(\(_{1},_{2}\)), with the estimated evidence lower bounds and ground truth log-likelihood shown in Table 1. It can be seen that tree-structured posteriors can learn more correlation information than traditional mean-field approximations. Moreover, different choices of the tree structures influence the amount of correlation information, and mixture of tree components can obtain more correlations than each individual.

### Constrained Clustering

Constrained clustering tasks differ from the classic clustering scenario with access to instance-level constraints, consisting of _must-links_ if two samples are believed to belong to the same cluster, and _cannot-links_, otherwise. Based on the variational deep embedding (VaDE) framework , constrained clustering can be formulated as a probabilistic clustering problem with joint probability \(p_{}(,,)=p_{}( |)p(|)p()\), where the sample \(_{i}\) is generated from a normal distribution conditioned on \(_{i}\), \(_{i}\) is sampled from \(p(_{i}|c_{i})=(_{i};_{c_{i}},(_{c_{i}}^{2}))\), and the cluster assignments \(=\{c_{i}\}_{i=1}^{N}\) are sampled from a categorical distribution. Following previous work , we incorporate the clustering preference through a conditional probability \(p(|)\) with a pairwise prior information matrix \(\)

\[p(|):=_{c_{i}}h_{i}(,)}{_{}_{j}_{c_{j}}h_{j}(,)}=)}_{i}_{c_{i}}h_{i}(,)\] (19)

where \(=\{_{k}\}_{k=1}^{K}\) are the weights associated to each cluster, \(()\) is the normalization factor and \(h_{i}(,)=_{j i}(_{ij}_{c_{i} c_{j}})\) is a weighting function. The pairwise prior information matrix \(\) is defined as a symmetric matrix containing the pairwise constraints: \(_{ij}>0\) if there is a must-link constraint between \(_{i}\) and \(_{j}\), \(_{ij}<0\) if there is a cannot-link constraint between \(_{i}\) and \(_{j}\), and \(_{ij}=0\) otherwise. The values \(|_{ij}|[0,)\) reflect the degree of certainty in the constraints, and are set to \(10^{4}\) for all datasets. And 6000 pairwise constraints are used for experiments on both our methods and other constrained clustering baselines.

The variational posterior distribution is defined as \(q_{}(,|)=q_{}(| )q(|)\), where the probability of cluster assignments is factorized as \(q(|)=_{i}q(c_{i}|_{i})\) which can be easily computed by Bayes theorem. In the work of DC-GMM , the posterior distribution \(q_{}(|)\) for latent variables is assumed to be mean-field which fails to capture the posterior correlation structure, while in our methods TreeVI and MTreeVI, the latent posterior is approximated by tree-structured and mixture-of-trees distribution, respectively. In Table 2 we report the averaged clustering performances across 10 turns of both our proposed methods against baseline methods. Accuracy (ACC), Normalized Mutual Information (NMI), and Adjusted Rand Index (ARI) are used as evaluation metrics. It can be

   Dataset & Metric & VaDE\({}^{}\) & SDEC\({}^{}\) & C-IDEC\({}^{}\) & DGG & DC-GMM & TreeVI & MTreeVI \\  MNIST & ACC & 89.0 \(\)5.0 & 86.2 \(\)0.1 & 96.3 \(\)0.2 & 95.8 \(\)0.1 & 96.5 \(\)0.2 & **97.4 \(\)0.3** & **97.5 \(\)0.4** \\  & NMI & 82.8 \(\)3.0 & 84.2 \(\)0.1 & 91.8 \(\)1.0 & 91.2 \(\)0.2 & 91.4 \(\)0.3 & **93.1 \(\)0.6** & **93.1 \(\)0.6** \\  & ARI & 80.9 \(\)5.0 & 80.1 \(\)0.1 & 92.1 \(\)0.4 & 91.4 \(\)0.3 & 92.5 \(\)0.5 & **93.7 \(\)0.7** & **94.0 \(\)0.5** \\  fMNIST & ACC & 55.1 \(\)2.2 & 54.0 \(\)0.2 & 68.1 \(\)3.0 & 79.9 \(\)0.4 & 80.5 \(\)0.8 & **81.4 \(\)0.6** & **82.1 \(\)0.7** \\  & NMI & 57.9 \(\)2.7 & 57.3 \(\)0.1 & 66.7 \(\)2.0 & 70.1 \(\)0.3 & 72.0 \(\)0.4 & **73.9 \(\)0.6** & **74.1 \(\)0.6** \\  & ARI & 41.6 \(\)3.1 & 40.2 \(\)0.1 & 52.3 \(\)3.0 & 64.9 \(\)0.3 & 66.4 \(\)0.5 & **67.9 \(\)0.9** & **68.1 \(\)0.6** \\  Reuters & ACC & 76.0 \(\)0.7 & 82.1 \(\)0.1 & 94.7 \(\)0.6 & 93.5 \(\)0.6 & 95.4 \(\)0.2 & **95.9 \(\)0.6** & **96.1 \(\)0.7** \\  & NMI & 50.1 \(\)1.3 & 62.3 \(\)0.1 & 81.4 \(\)0.7 & 81.2 \(\)0.8 & 82.7 \(\)0.7 & **83.4 \(\)0.5** & **83.9 \(\)0.5** \\  & ARI & 58.0 \(\)1.4 & 66.7 \(\)0.1 & 87.7 \(\)0.9 & 87.8 \(\)0.5 & 89.0 \(\)0.6 & **90.2 \(\)0.4** & **90.5 \(\)0.4** \\  STL-10 & ACC & 77.3 \(\)0.5 & 79.2 \(\)0.1 & 81.6 \(\)3.8 & 89.9 \(\)0.3 & 89.5 \(\)0.5 & **90.4 \(\)0.9** & **90.7 \(\)0.9** \\  & NMI & 70.6 \(\)0.4 & 78.6 \(\)0.1 & 77.3 \(\)1.7 & 80.9 \(\)0.5 & 80.2 \(\)0.7 & **81.3 \(\)0.8** & **81.6 \(\)0.7** \\  & ARI & 62.7 \(\)0.4 & 71.0 \(\)0.1 & 71.8 \(\)3.4 & 79.0 \(\)0.4 & 78.4 \(\)0.9 & **79.5 \(\)0.7** & **79.7 \(\)0.9** \\   

Table 2: Clustering performances (%) of our proposed methods TreeVI and MTreeVI compared with baselines. Means and standard deviations are computed across 10 runs with different random initializations. \(\) Results taken from DC-GMM easily observed that our models reach the state-of-the-art clustering performance in all metrics and datasets, benefiting from the correlation structures captured by our designed correlated posteriors. Moreover, due to high efficiency of tree-structured reparameterization, our methods have computational complexity comparable to DC-GMM that adopts the fully factorized posterior distribution. For example, the training of DC-GMM on MNIST dataset each epoch takes 2 to 3 seconds on GeForce RTX 3090, while our TreeVI and MTreeVI take around 5 and 9 seconds, respectively. Further, we plot the tree learned over the constrained clustering experiment on MNIST dataset, as shown in Figure 2. From the figure, we can see that instances from the same category are connected more tightly than those from different categories in the learned tree. This demonstrates that our method has the abilities to capture the underlying inherent correlations among different instances.

### User Matching

We evaluate our methods against the baselines on a public movie rating dataset **MovieLens 20M**. The ratings of each user \(u_{i}\) are binarized as a bag-of-word vector \(_{u_{i}}\) (\(i=1,2,,N\)), and only ratings for movies that have been rated over 1000 times are preserved for simplicity. In our experiments, the watch history of each user \(u_{i}\) is randomly split into two halves, leading to two synthetic users \(u_{i}^{A}\) and \(u_{i}^{B}\) with the most similar movie preference, and a correlation graph \(=(,)\) with nodes \(=\{u_{i}^{A},u_{i}^{B}:i=1,,N\}\) and edges \(=\{(u_{i}^{A},u_{i}^{B}):i=1,,N\}\).

Suppose that the joint distribution of rating data \(\) and the corresponding latent embeddings \(\) is modeled by \(p_{}(,)=p_{}( |)p()\), and the variational posterior distribution is modeled by \(q_{}(|)\). In the work of CVAE , both the prior distribution \(p()\) and posterior distribution \(q_{}(|)\) are designed as weighted sums of tree-structured distributions with respect to each maximal acyclic subgraph of the correlation graph \(\). While in our work, the latent representations are drawn from a Gaussian distribution of the form \(p()=(;_{2N};(_{2N}+ )_{D})\), where the hyper-parameter \((0,1)\) is used to control the overall correlation strength, and the affinity matrix \(=[a_{ij}]_{i,j}\) is derived from the correlation graph with \(a_{ij}=1\) if \((i,j)\) and \(a_{ij}=0\) otherwise. And our latent posterior distribution \(q_{}(|)\) is modeled by tree-structured and mixture-of-trees distributions, respectively.

And our goal is to identify the dual user \(u_{i}^{B}\) given a synthetic user \(u_{i}^{A}\) from a held-out set in terms of the latent embedding distance. The user data are implemented a random train/test split with a 90/10 ratio, and the synthetic user pairs from the training set are used to train all the methods. To evaluate the user matching accuracies, a fixed number \(N^{}=1000\) of synthetic user pairs are selected from the test set and for each synthetic user \(u_{i}^{A}\) (or \(u_{i}^{B}\)), we summarize the ranking of its dual user \(u_{i}^{B}\) (or \(u_{i}^{A}\)) among all other \(2N^{}-1\) synthetic user candidates in terms of latent embedding distances.

Figure 2: T-SNE visualization of MNIST samples in the embedded space and the learnt tree structure of our proposed TreeVI. 100 samples are randomly selected to plot their instance-level tree structure (colored in **black**).

In Table 3 we show the average Reciprocal Ranking (RR) for all the methods, which demonstrates superiority of our model over both implementations for the baseline method CVAE.

### Link Prediction

We perform link prediction task on a constructed undirected correlation graph \(=(,)\) within the public product rating dataset **Epinions**. The rating data are binarized into bag-of-words feature vectors \(_{u_{i}}\) for each user \(u_{i}\), and only products that have been rated at least 100 times are kept and users who have rated these products at least once are considered. To construct the undirected graph \(\) from the single-directional "trust" statements between users \(u_{i},u_{j}\) provided by the dataset, we only build an edge \((u_{i},u_{j})\) if both \(u_{i}\) trusts \(u_{j}\) and \(u_{j}\) trusts \(u_{i}\). The experiment setting of our TreeVI and MTreeVI are similar to the user matching task, by defining a correlation graph incorporated prior distribution and approximating latent posterior distribution with tree-structured and mixture-of-trees distributions, respectively.

The product rating dataset is split for each user \(u_{i}\) into training and test sets, with \((1,}(u_{i}))\) edges held out for test edge set \(_{}\). The remaining edges for the training edge set \(_{}\) are used to train all methods on the product rating data. To evaluate the link prediction accuracies, we calculate for each user \(u_{i}\) the Normalized Cumulative Reciprocal Rank \(_{i}\) of the ratings of \(u_{i}\)'s test edges among all possible connections except for the training edges, in terms of latent embedding distance metrics. Formally, the NCRR value is the \(\)-normalization of the Cumulative Reciprocal Rank (CRR) formulated as \(_{i}=_{(u_{i},u_{j})_{}}|\{k:(u_{ i},u_{k})_{},\,d_{ik} d_{ij}\}|^{-1}\), where \(d_{ij}\) represents the latent embedding distance between users \(u_{i}\) and \(u_{j}\) for \(1 i j N\). Larger \(_{i}\) indicates better ability to predict held-out test links with respect to each user \(u_{i}\), and the averaged results for all methods are reported in Table 4.

## 5 Conclusion

In this work, we present a novel variational inference method called TreeVI, that approximates the intractable latent posterior distribution with a tree-structured distribution. This induces a Bayesian network whose ancestral sampling gives a matrix-form reparameterization for the correlated latents and enables efficient optimization. To enrich the correlation structure, TreeVI is further extended to MTreeVI with a mixture of trees to better approximate the underlying posterior. Furthermore, the tree and mixture-of-trees structures are allowed stochastically learned from data by solving a constrained optimization problem under our proposed acyclicity constraints. With correlated posteriors, we show that our methods can capture more correlation information and achieve superior performances in real-world tasks.

Limitations & Future WorkThe proposed method requires a tree structure to approximate the graph-structured posterior correlation structure. This limitation is mitigated by adopting a weighted mixture of trees and stochastically learning a correlation-rich tree or mixture-of-trees structure with our proposed constrained optimization. For future work, we will further investigate on correlation structures with higher expressivity for approximating the latent posterior.

AcknowledgmentThis work is supported by the National Natural Science Foundation of China (No. 62276280, U1811264), Guangzhou Science and Technology Planning Project (No. 2024A04J9967).

  
**Methods** & **Test NCRR** \\  VAE & \(0.0052 0.0007\) \\ GraphSAGE & \(0.0115 0.0025\) \\ CVAE\({}_{}\) & \(0.0160 0.0004\) \\ CVAE\({}_{}\) & \(0.0171 0.0009\) \\ TreeVI (Ours) & \(\) \\ MTreeVI (Ours) & \(\) \\   

Table 4: Link prediction test Normalized CRR

  
**Methods** & **Test RR** \\  VAE & \(0.3498 0.0167\) \\ CVAE\({}_{}\) & \(0.6608 0.0066\) \\ CVAE\({}_{}\) & \(0.7129 0.0096\) \\ TreeVI (Ours) & \(\) \\ MTreeVI (Ours) & \(\) \\   

Table 3: Synthetic user matching test RR