# Deep Learning Through A Telescoping Lens:

A Simple Model Provides Empirical Insights On

Grokking, Gradient Boosting & Beyond

 Alan Jeffares

University of Cambridge

aj659@cam.ac.uk &Alicia Curth

University of Cambridge

amc253@cam.ac.uk &Mihaela van der Schaar

University of Cambridge

mv472@cam.ac.uk

Equal contribution

###### Abstract

Deep learning sometimes _appears to_ work in unexpected ways. In pursuit of a deeper understanding of its surprising behaviors, we investigate the utility of a simple yet accurate model of a trained neural network consisting of a sequence of first-order approximations _telescoping_ out into a single empirically operational tool for practical analysis. Across three case studies, we illustrate how it can be applied to derive new empirical insights on a diverse range of prominent phenomena in the literature - including double descent, grokking, linear mode connectivity, and the challenges of applying deep learning on tabular data - highlighting that this model allows us to construct and extract metrics that help predict and understand the a priori unexpected performance of neural networks. We also demonstrate that this model presents a pedagogical formalism allowing us to isolate components of the training process even in complex contemporary settings, providing a lens to reason about the effects of design choices such as architecture & optimization strategy, and reveals surprising parallels between neural network learning and gradient boosting.

## 1 Introduction

Deep learning _works_, but it sometimes works in mysterious ways. Despite the remarkable recent success of deep learning in applications ranging from image recognition  to text generation , there remain many contexts in which it performs in apparently unpredictable ways: neural networks sometimes exhibit surprisingly non-monotonic generalization performance , continue to be outperformed by gradient boosted trees on tabular tasks despite successes elsewhere , and sometimes behave surprisingly similarly to linear models . The pursuit of a deeper understanding of deep learning and its phenomena has since motivated many subfields, and progress on fundamental questions has been distributed across many distinct yet complementary perspectives that range from purely theoretical to predominantly empirical research.

**Outlook.** In this work, we take a hybrid approach and investigate how we can _apply_ ideas primarily used in theoretical research to investigate the behavior of a simple yet accurate model of a neural network _empirically_. Building upon previous work that studies linear approximations to learning in neural networks through tangent kernels (e.g. , see Sec. 2), we consider a model that uses first-order approximations for the functional updates made during training. However, unlike most previous work, we define this model incrementally by simply _telescoping out_ approximations to individual updates made during training (Sec. 3) such that it more closely approximates the true behavior of a fully trained neural network in practical settings. This provides us with a pedagogical lens through which we can view modern optimization strategies and other design choices (Sec. 5), and a mechanism with which we can conduct empirical investigations into several prominent deep learning phenomena that showcase how neural networks sometimes generalize _seemingly_ unpredictably.

Across three case studies in Sec. 4, we then show that this model allows us to _construct and extract metrics that help predict and understand_ the a priori unexpected performance of neural networks. First, in Sec. 4.1, we demonstrate that it allows us to extend 's recent model complexity metric to neural networks, and use this to investigate surprising generalization curves - discovering that the non-monotonic behaviors observed in both deep double descent  and grokking  are associated with _quantifiable_ divergence of train- and test-time model complexity. Second, in Sec. 4.2, we show that it reveals perhaps surprising parallels between gradient boosting  and neural network learning, which we then use to investigate the known performance differences between neural networks and gradient boosted trees on tabular data in the presence of dataset irregularities . Third, in Sec. 4.3, we use it to investigate the connections between gradient stabilization and the success of weight averaging (i.e. linear mode connectivity ).

## 2 Background

**Notation and preliminaries.** Let \(f_{}:^{d} ^{k}\) denote a neural network parameterized by (stacked) model weights \(^{p}\). Assume we observe a training sample of \(n\) input-output pairs \(\{_{i},y_{i}\}_{i=1}^{n}\), i.i.d. realizations of the tuple \((X,Y)\) sampled from some distribution \(P\), and wish to learn good model parameters \(\) for predicting outputs from this data by minimizing an empirical prediction loss \(_{i=1}^{n}(f_{}(_{i}),y_{i})\), where \(:^{k}^{k}\) denotes some differentiable loss function. Throughout, we let \(k=1\) for ease of exposition, but unless otherwise indicated our discussion generally extends to \(k>1\). We focus on the case where \(\) is optimized by initializing the model with some \(_{0}\) and then iteratively updating the parameters through stochastic gradient descent (SGD) with learning rates \(_{t}\) for \(T\) steps, where at each \(t[T]=\{1,,T\}\) we subsample batches \(B_{t}[n]=\{1,,n\}\) of the training indices, leading to parameter updates \(_{t}_{t}-_{t-1}\) as:

\[_{t}=_{t-1}+_{t}=_{t-1}- }{|B_{t}|}_{i B_{t}}_{}f_{ _{t-1}}(_{i})g^{}_{it}=_{t-1}-_{t}_{t }^{}_{t}\] (1)

where \(g^{}_{it}=_{t-1}}(_{i}),y_{i})} { f_{_{t-1}}(_{i})}\) is the gradient of the loss w.r.t. the model prediction for the \(i^{th}\) training example, which we will sometimes collect in the vector \(^{}_{t}=[g^{}_{1t},,g^{}_{nt}]^{}\), and the \(p n\) matrix \(_{t}=[\}}{|B_{t}|}_{}f_{_{t-1}}(_{1}),,\}}{|B_{t}|}_{ }f_{_{t-1}}(_{n})]\) has as columns the gradients of the model prediction with respect to its parameters for examples in the training batch (and \(\) otherwise). Beyond vanilla SGD, modern deep learning practice usually relies on a number of modifications to the update described above, such as momentum and weight decay; we discuss these in Sec. 5.

**Related work: Linearized neural networks and tangent kernels.** A growing body of recent work has explored the use of _linearized_ neural networks (linear in their parameters) as a tool for theoretical  and empirical  study. In this paper, we similarly make extensive use of the following observation (as in e.g. ): we can linearize the difference \( f_{t}() f_{_{t}}()-f_{_{t-1}}()\) between two parameter updates as

\[ f_{t}()=_{}f_{_{t-1}}( )^{}_{t}+(||_{t}||^{2}) _{}f_{_{t-1}}()^{}_{t}_{t}()\] (2)

where the quality of the approximation \(_{t}()\) is good whenever the parameter updates \(_{t}\) from a single batch are sufficiently small (or when the Hessian product \(||_{t}^{}_{}^{2}_{ _{t-1}}()_{t}||\) vanishes). If Eq. (2) holds exactly (e.g. for infinitesimal \(_{t}\)), then running SGD in the network's parameter space to obtain \(_{t}\) corresponds to executing steepest descent on the function output \(f_{}()\) itself using the _neural tangent kernel_\(K^{}_{t}(,_{i})\) at time-step \(t\), i.e. results in functional updates

\[_{t}()-_{t}_{i[n]}K^{ }_{t}(,_{i})g^{}_{it}K^{}_{t}(,_{i}) \{i B_{t}\}}{|B_{t}|}_{}f_{_{t- 1}}()^{}_{}f_{_{t-1}}(_{ i}).\] (3)

_Lazy learning_ occurs as the model gradients remain approximately constant during training, i.e. \(_{}f_{_{t}}()_{}f_{ _{0}}()\), \( t[T]\). For learned parameters \(_{T}\), this implies that the approximation \(f^{lin}_{_{T}}()=f_{_{0}}()+_{ }f_{_{0}}()^{}(_{T}-_{0})\) holds - which is a _linear function of the model parameters_, and thus corresponds to a linear regression in which features are given by the model gradients \(_{}f_{_{0}}()\) instead of the inputs \(\) directly - whose training dynamics can be more easily understood theoretically. For sufficiently wide neural networks the \(_{}f_{_{t}}()\), and thus the tangent kernel, have been theoretically shown to be constant throughout training in some settings , but in practice they generally vary during training, as shown theoretically in  and empirically in . A growing theoretical literature  investigates constant tangent kernel assumptions to study convergence and generalization of neural networks (e.g.

. This present work relates more closely to _empirical_ studies making use of tangent kernels and linear approximations, such as , OJMDF21] who highlight differences between lazy learning and real networks, and  who empirically investigate the relationship between loss landscapes and the evolution of \(K_{t}^{}(,_{i})\).

## 3 A Telescoping Model of Deep Learning

In this work, we explore whether we can exploit the approximation in Eq. (2) beyond the laziness assumption to gain new insight into neural network learning. Instead of applying the approximation across the entire training trajectory at once as in \(f_{_{T}}^{lin}()\), we consider using it _incrementally_ at each batch update during training to approximate _what has been learned at this step_. This still provides us with a greatly simplified and transparent model of a neural network, and results in a much more reasonable approximation of the true network. Specifically, we explore whether - instead of studying the final model \(f_{_{T}}()\) as a whole - we can gain insight by _telescoping out_ the functional updates made throughout training, i.e. exploiting that we can always equivalently express \(f_{_{T}}()\) as:

\[f_{_{T}}()=f_{_{0}}()+_{t=1}^{T}[f_{_{t}}()-f_{_{t-1}}()]=f_{_{0}}()+_{t=1 }^{T} f_{t}()\] (4)

This representation of a trained neural network in terms of its learning trajectory rather than its final parameters is interesting because we are able to better reason about the impact of the training procedure on the intermediate updates \( f_{t}()\) than the final function \(f_{_{T}}()\) itself. In particular, we investigate whether empirically monitoring behaviors of the sum in Eq. (4) while making use of the approximation in Eq. (2) will enable us to gain practical insights into learning in neural networks, while incorporating a variety of modern design choices into the training process. That is, we explore the use of the following _telescoping model_\(_{_{T}}()\) as an approximation of a trained neural network:

\[_{_{T}}():=f_{_{0}}( )+_{t=1}^{T}}f_{ _{t-1}}()^{}_{t}}_ {}=f_{_{0}}()-_{t=1 }^{T}K_{t}^{T}(,_{i})g_{it}^{ }}_{}\] (5)

where \(K_{t}^{T}(,_{i})\) is determined by the neural tangent kernel as \(_{t}K_{t}^{}(,_{i})\) in the case of standard SGD (in which case (ii) can also be interpreted as a discrete-time approximation of 's _path kernel_), but can take other forms for different choices of learning algorithm as we explore in Sec. 5.

**Practical considerations.** Before proceeding, it is important to emphasize that the telescoping approximation described in Eq. (5) is intended as _a tool for (empirical) analysis of learning in neural networks_ and is _not_ being proposed as an alternative approach to training neural networks. Obtaining \(_{_{T}}()\) requires computing \(_{}f_{_{t-1}}()\) for each training _and_ testing example at each training step \(t[T]\), leading to increased computation over standard training. Additionally, these computational costs are likely prohibitive for extremely large networks and datasets without further adjustments; for this purpose, further approximations such as  could be explored. Nonetheless, computing \(_{_{T}}()\) - or relevant parts of it - is still feasible in many pertinent settings as later illustrated in Sec. 4.

Figure 1: **Illustration of the telescoping model of a trained neural network. Unlike the more standard framing of a neural network in terms of an iteratively learned set of parameters, the telescoping model takes a functional perspective on training a neural network in which an arbitrary test exampleâ€™s initially random prediction, \(f_{_{0}}()\), is additively updated by a linearized adjustment \(_{t}()\) at each step \(t\) as in Eq. (5).**

**How good is this approximation?** In Fig. 2, we examine the quality of \(_{_{i}}()\) for a 3-layer fully-connected ReLU network of width 200, trained to discriminate 3-vs-5 from 1000 MNIST examples using the squared loss with SGD or AdamW . In red, we plot its mean average approximation error (\(_{_{test}}|f_{_{i}}( )-_{_{i}}()|\)) and observe that for small learning rates \(\) the difference remains negligible. In \(\) we plot the same quantity for \(f_{_{i}}^{in}()\) (i.e. the first-order expansion around \(_{0}\)) for reference and find that iteratively telescoping out the updates instead improves the approximation _by orders of magnitude_ - which is also reflected in their prediction performance (see Appendix D.1). Unsurprisingly, \(\) controls approximation quality as it determines \(||_{t}||\). Further, \(\)_interacts_ with the optimizer choice - e.g. Adam(W)  naturally makes larger updates due to rescaling (see Sec. 5) and therefore requires smaller \(\) to ensure approximation quality than SGD.

## 4 A Closer Look at Deep Learning Phenomena Through a Telescoping Lens

Next, we turn to _applying_ the telescoping model. Below, we present three case studies revisiting existing experiments that provided evidence for a range of unexpected behaviors of neural networks. These case studies have in common that they highlight cases in which neural networks appear to generalize somewhat _unpredictably_, which is also why each phenomenon has received considerable attention in recent years. For each, we then show that the telescoping model allows us to construct and extract metrics that can help predict and understand the unexpected performance of the networks. In particular, we investigate (i) surprising generalization curves (Sec. 4.1), (ii) performance differences between gradient boosting and neural networks on some tabular tasks (Sec. 4.2), and (iii) the success of weight averaging (Sec. 4.3). We include an extended literature review in Appendix A, a detailed discussion of all experimental setups in Appendix C, and additional results in Appendix D.

### Case study 1: Exploring surprising generalization curves and benign overfitting

Classical statistical wisdom provides clear intuitions about overfitting: models that can fit the training data too well - because they have too many parameters and/or because they were trained for too long - are expected to generalize poorly (e.g. [13, Ch. 7]). Modern phenomena like double descent , however, highlighted that pure capacity measures (capturing what _could_ be learned instead of what _is_ actually learned) would not be sufficient to understand the complexity-generalization relationship in deep learning . Raw parameter counts, for example, cannot be enough to understand the complexity of what has been learned by a neural network during training because, even when using _the same architecture_, what is learned could be wildly different across various implementation choices within the optimization process - and even at different points during the training process of the same model, as prominently exemplified by the grokking phenomenon . Here, with the goal of finding clues that may help predict phenomena like double descent and grokking, we explore whether the telescoping model allows us to gain insight into the relative complexity of what is learned.

A complexity measure that avoids the shortcomings listed above - because it allows to consider a _specific trained_ model - was recently used by  in their study of _non-deep_ double descent. As their measure \(p^{0}_{}}\) builds on the literature on smoothers , it requires to express learned predictions as a linear combination of the training labels, i.e. as \(f()=}()=_{i[n]}^{i }()y_{i}\). Then,  define the _effective parameters_\(p^{0}_{}}\) used by the model when issuing predictions for some set of inputs \(\{^{0}_{j}\}_{j_{0}}\) with indices collected in \(_{0}\) (here, \(_{0}\) is either \(_{train}=\{1,,n\}\) or \(_{test}=\{n+1,,n+m\}\)) as \(p^{0}_{}} p(_{0},}())= _{0}|}_{j_{0}}||}(^{0}_{j})||^{2}\). Intuitively, the larger \(p^{0}_{}}\), the less smoothing across the training labels is performed, which implies higher model complexity.

Due to the black-box nature of trained neural networks, however, it is not obvious how to link learned predictions to the labels observed during training. Here, we demonstrate how the telescoping model allows us to do precisely that - enabling us to make use of \(p^{0}_{}}\) as a proxy for complexity. We consider the special case of a single output (\(k=1\)) and training with squared loss \((f(),y)=(y-f())^{2}\)

Figure 2: **Approximation error** of the telescoping \((_{_{i}}(),)\) and the linear model \((f^{lin}_{_{i}}(),)\).

and note that we can now exploit that the SGD weight update simplifies to

\[_{t}=_{t}_{t}(-_{ _{t-1}})=[y_{1},,y_{n}]^{}_{_{t}}=[f_{_{t}}(_{1}), ,f_{_{t}}(_{n})]^{}.\] (6)

Assuming the telescoping approximation holds exactly, this implies functional updates

\[_{t}()=_{t}_{}f_{_{ t-1}}()^{}_{t}(-}_{_{t-1}})\] (7)

which use a linear combination of the training labels. Note further that after the _first_ SGD update

\[_{_{1}}()=f_{_{0}}()+ _{1}()=_{}f_{ _{0}}()^{}_{1}}_{_{1 }()}+_{0}}( )-_{1}_{}f_{_{0}}()^{}_{1}_{0}}}_{c^{0}_{_{1}}()}\] (8)

which means that the first telescoping predictions \(_{_{t}}()\) are indeed simply linear combinations of the training labels (and the predictions at initialization)! As detailed in Appendix B.1, this also implies that recursively substituting Eq. (7) into Eq. (5) further allows us to write _any_ prediction \(_{_{t}}()\) as a linear combination of the training labels and \(f_{_{0}}()\), i.e. \(_{_{t}}()=_{_{t}}()+c^{0}_{_{t}}()\) where the \(1 n\) vector \(_{_{t}}()\) is a function of the kernels \(\{K_{^{}}^{t}(,)\}_{^{} t}\), and the scalar \(c^{0}_{_{t}}()\) is a function of the \(\{K_{^{}}^{t}(,)\}_{^{} t}\) and \(f_{_{0}}()\). We derive precise expressions for \(_{_{t}}()\) and \(c^{0}_{_{t}}()\) for different optimizers in Appendix B.1 - enabling us to use \(_{_{t}}()\) to compute \(p^{0}_{}\) as a proxy for complexity below.

**Double descent: Model complexity vs model size.** While training error always monotonically decreases as model size (measured by parameter count) increases,  made a surprising observation regarding test error in their seminal paper on _double descent_: they found that test error initially improves with additional parameters and then worsens when the model is increasingly able to overfit to the training data (as is expected) but can _improve again_ as model size is increased further past the so-called interpolation threshold where perfect training performance is achieved. This would appear to contradict the classical U-shaped relationship between model complexity and test error [13, Ch. 7]. Here, we investigate whether tracking \(p^{0}_{}\) on train and test data separately will allow us to gain new insight into the phenomenon in neural networks.

In Fig. 3, we replicate the binary classification example of double descent in neural networks of , training single-hidden-layer ReLU networks of increasing width to distinguish cats and dogs on CIFAR-10 (we present additional results using MNIST in Appendix D.2). First, we indeed observe the characteristic behavior of error curves as described in  (top panel). Measuring learned complexity using \(p^{0}_{}\), we then find that while \(p^{train}_{}\) monotonically increases as model size is increasing, the effective parameters used on the test data \(p^{test}_{}\) implied by the trained neural network _decrease_ as model size is increased past the interpolation threshold (bottom panel). Thus, paralleling the findings made in  for linear regression and tree-based methods, we find that distinguishing between train- and test-time complexity of a neural network using \(p^{0}_{}\) provides new quantitative evidence that bigger networks are _not_ necessarily learning more complex prediction functions for unseen test examples, which resolves the ostensible tension between deep double descent and the classical U-curve. Importantly, note that \(p^{test}_{}\) can be computed without access to test-time labels, which means that the observed difference between \(p^{train}_{}\) and \(p^{test}_{}\) allows to quantify whether there is _benign overfitting_ in a neural network.

**Grokking: Model complexity throughout training.** The grokking phenomenon  then showcased that improvements in test performance _during a single training run_ can occur long after perfect training performance has been achieved (contradicting early stopping practice!). While  attribute this to weight decay causing \(||_{t}||\) to shrink late in training - which they demonstrate on an MNIST example using unusually large \(_{0}\) -  highlight that grokking can also occur as the weight norm \(||_{t}||\)_grows_ later in training - which they demonstrate on a polynomial regression task. In Fig. 4 we replicate2 both experiments while tracking \(p^{0}_{}\) to investigate whether

Figure 3: **Double descent** in MSE (top) and effective parameters \(p^{0}_{}\) (bottom) on CIFAR-10.

this provides new insight into this apparent disagreement. Then, we observe that the continued improvement in test error, past the point of perfect training performance, is associated with divergence of \(p_{}^{train}\) and \(p_{}^{test}\) in _both_ experiments (analogous to the double descent experiment in Fig. 3), suggesting that grokking may reflect _transition into_ a measurably benign overfitting regime during training. In Appendix D.2, we additionally investigate mechanisms known to induce grokking, and show that later onset of generalization indeed coincides with later divergence of \(p_{}^{train}\) and \(p_{}^{test}\).

**Inductive biases & learned complexity.** We observed that the large \(_{0}\) in 's MNIST example of grokking result in very large initial predictions \(|\!|f_{_{0}}()|\!\!1\). Because _no_ sigmoid is applied, the model needs to learn that all \(y_{i}\!\!\) by reducing the magnitude of predictions substantially - large \(_{0}\) thus constitute a very poor inductive bias for this task. One may expect that the better an inductive bias is, the less complex the component of the final prediction that is learned from data. To test whether this intuition is quantifiable, we repeat the MNIST experiment with standard initialization scale, with and without sigmoid activation \(()\), in column (3) of Fig. 4 (training results shown in Appendix D.2 for readability). We indeed find that both not only speed up learning significantly (a generalizing solution is found in \(10^{2}\) instead of \(10^{5}\) steps), but also substantially reduce effective parameters used, where the stronger inductive bias - using \(()\) - indeed leads to the least learned complexity.

_Takeaway Case Study 1._ The telescoping model enables us to use \(p_{}^{0}\) as a proxy for learned complexity, whose relative behavior on train and test data can quantify benign overfitting in neural networks.

### Case study 2: Understanding differences between gradient boosting and neural networks

Despite their overwhelming successes on image and language data, neural networks are - perhaps surprisingly - still widely considered to be outperformed by _gradient boosted trees_ (GBTs) on _tabular data_, an important modality in many data science applications. Exploring this apparent Achilles heel of neural networks has therefore been the goal of multiple extensive benchmarking studies . Here, we concentrate on a specific empirical finding of : their results suggest that GBTs may particularly outperform deep learning on heterogeneous data with greater irregularity in input features, a characteristic often present in tabular data. Below, we first show that the telescoping model offers a useful lens to compare and contrast two methods, and then use this insight to provide and test a new explanation of why GBTs can perform better in the presence of dataset irregularities.

**Identifying (dis)similarities between learning in GBTs and neural networks.** We begin by introducing gradient boosting  closely following [13, Ch. 10.10]. Gradient boosting (GB) also aims to learn a predictor \(^{GB}:^{k}\) minimizing expected prediction loss \(\). While deep learning solves this problem by iteratively updating a randomly initialized set of _parameters_ that transform inputs to predictions, the GB formulation iteratively updates _predictions_ directly without requiring any iterative learning of parameters - thus operating in function space rather than parameter space. Specifically, GB, with learning rate \(\) and initialized at predictor \(h_{0}()\), consists of a sequence \(^{GB}_{T}()=h_{0}()+_{t=1}^{T}_{t }()\) where each \(_{t}()\) improves upon the existing predictions \(^{GB}_{t-1}()\). The solution to the loss minimization problem can be achieved by executing steepest descent in function space _directly_, where each update \(_{t}\) simply outputs the negative training gradients of the loss function with respect to the previous model, i.e. \(_{t}(_{i})=-g^{}_{it}\) where \(g^{}_{it}=^{GB}_{t-1}(_{i}),\,y_ {i})}}{{^{GB}_{t-1}(_{i})}}\).

Figure 4: **Grokking in mean squared error on a polynomial regression task (1, replicated from ) and in misclassification error on MNIST using a network with large initialization (2, replicated from ) (top), against effective parameters (bottom). Column (3) shows test results on MNIST with standard initialization (with and without sigmoid activation) where time to generalization is quick and grokking does not occur.**

However, this process is only defined at the training points \(\{_{i},y_{i}\}_{i[n]}\). To obtain an estimate of the loss gradient for an arbitrary test point \(\), each iterative update instead fits a weak learner \(_{t}()\) to the current input-gradient pairs \(\{_{i},-g_{i[n]}^{}\}_{i[n]}\) which can then also be evaluated new, unseen inputs. While this process could in principle be implemented using any base learner, the term _gradient boosting_ today appears to exclusively refer to the approach outlined above implemented using shallow trees as \(_{t}()\). Focusing on trees which issue predictions by averaging the training outputs in each leaf, we can make use of the fact that these are sometimes interpreted as adaptive nearest neighbor estimators or kernel smoothers , allowing us to express the learned predictor as:

\[^{GB}()=h_{0}()-_{t=1}^{T}_{i[n]} \{l_{h_{t}}()=l_{h_{t}}(_{i})\}}{n_{l( )}}g_{it}^{}=h_{0}()-_{t=1}^{T}_{i[n] }K_{_{t}}(,_{i})g_{it}^{}\] (9)

where \(l_{_{t}}()\) denotes the leaf example \(\) falls into, \(n_{l()}=_{i[n]}\{l_{h_{t}}()=l_{h_{t}}( _{i})\}\) is the number of training examples in said leaf and \(K_{_{t}}(,_{i})=}{{n_{leaf}()}}\{l_{_{t}}()=l_{_{t}}(_{i})\}\) is thus the kernel learned by the \(t^{th}\) tree \(_{t}()\). Comparing Eq. (9) to the kernel representation of the telescoping model of neural network learning in Eq. (5), we make a perhaps surprising observation: the telescoping model of a neural network and GBTs have _identical_ structure and differ only in their used kernel! Below, we explore whether this new insight allows to understand some of their performance differences.

**Why can GBTs outperform deep learning in the presence of dataset irregularities?** Comparing Eq. (5) and Eq. (9) thus suggests that at least some of the performance differences between neural networks and GBTs are likely to be rooted in the differences between the behavior of the neural network tangent kernels \(K_{t}^{}(,_{i})\) and GBT's tree kernels \(K_{_{t}}(,_{i})\). One difference is obvious and purely architectural: it is possible that either kernel encodes a better inductive bias to fit the underlying outcome-generating process of a dataset at hand. Another difference is more subtle and relates to the behavior of the learned model on new inputs \(\): the tree kernels are likely to behave _much_ more predictable at test-time than the neural network tangent kernels. To see this, note that for the tree kernels we have that \(\) and \( i[n]\), \(0 K_{_{t}}(,_{i}) 1\) and \(_{i[n]}K_{_{t}}(,_{i})=1\); importantly, this is true regardless of whether \(=_{i}\) for some \(i\) or not. For the tangent kernels on the other hand, \(K_{t}^{}(,_{i})\) is in general unbounded and could behave _very_ differently for \(\) not observed during training. This leads us to hypothesize that this difference may be able to explain 's observation that GBTs perform better whenever features are heavy-tailed: if a test point \(\) is very different from training points, the kernels implied by the neural network \(_{t}^{}()[K_{t}^{}(,_{1}),,K_{t}^{}(,_{n})]^{}\) may behave very differently than at train-time while the tree kernels \(_{_{t}}()[K_{_{t}}(, _{1}),,K_{_{t}}(,_{n})]^{}\) will be less affected. For instance, \(}||_{_{t}}()||_{2} 1\) for all \(\) while \(||_{t}^{}()||_{2}\) is generally unbounded.

We empirically test this hypothesis on standard tabular benchmark datasets proposed in . We wish to examine the performance of the models and the behavior of the kernels as inputs become increasingly irregular, evaluating if GBT's kernels indeed display more consistent behavior compared to the network's tangent kernels. As a simple notion for input irregularity, we apply principal component analysis to the inputs to obtain a lower dimensional representation of the data and sort the observations according to their distance from the centroid. For a fixed trained model, we then evaluate on test sets consisting of increasing proportions \(p\) of the most irregular inputs (those in the top 10% furthest from the centroid). We compare the GBTs to neural networks by examining (i) the most extreme values their kernel weights take at test-time relative to the training data (measured as \(_{i=1}^{T}_{i T_{test}}||_{t}(x_{i})||_{2}} {_{i=1}^{T}_{i T_{train}}||_{t}(_{i})|| _{2}}\)) and (ii) how their relative mean squared error (measured as \(^{p}-MSE_{BBT}^{p}}{MSE_{NN}^{p}-MSE_{BBT}^{p}}\)) changes as the proportion \(p\) of irregular examples increases. In Fig. 5 using houses and in Appendix D.3

Figure 5: **Neural Networks vs GBTs: Relative performance (top) and behavior of kernels (bottom) with increasing test data irregularity using the houses dataset.**

using additional datasets, we first observe that GBTs outperform the neural network already in the absence of irregular examples; this highlights that there may indeed be differences in the suitability of the kernels in fitting the outcome-generating processes. Consistent with our expectations, we then find that, as the test data becomes more irregular, the performance of the neural network decays faster than that of the GBTs. Importantly, this is well tracked by their kernels, where the unbounded nature of the network's tangent kernel indeed results in it changing its behavior on new, challenging examples.

_Takeaway Case Study 2._ Eq. (5) provides a new lens for comparing neural networks to GBTs, and highlights that unboundedness in \(_{t}^{}()\) can predict performance differences due to dataset irregularities.

### Case study 3: Towards understanding the success of weight averaging

The final interesting phenomenon we investigate is that it is sometimes possible to simply average the weights \(_{1}\) and \(_{2}\) obtained from two stochastic training runs of the same model, resulting in a weight-averaged model that performs no worse than the individual models  - which has important applications in areas such as federated learning. This phenomenon is known as linear mode connectivity (LMC) and is surprising as, a priori, it is not obvious that simply _averaging the weights_ of independent neural networks (instead of their predictions, as in a deep ensemble ), which are highly nonlinear functions of their parameters, would not greatly worsen performance. While recent work has demonstrated empirically that it is sometimes possible to weight-average an even broader class of models after permuting weights , we focus here on understanding when LMC can be achieved for two models trained from the same initialization \(_{0}\).

In particular, we are interested in 's observation that LMC can emerge during training: the weights of two models \(_{jT}^{t^{}},j\{1,2\}\), which are initialized identically and follow identical optimization routine up until checkpoint \(t^{}\) but receive different batch orderings and data augmentations after \(t^{}\), can be averaged to give an equally performant model as long as \(t^{}\) exceeds a so-called _stability point_\(t^{*}\), which was empirically discovered to occur early in training in . Interestingly, [10, Sec. 5] implicitly hint at an explanation for this phenomenon in their empirical study of tangent kernels and loss landscapes, where they found an association between the disappearance of loss barriers between solutions during training and the rate of change in \(K_{t}^{}(,)\). We further explore potential implications of this observation through the lens of the telescoping model below.

**Why a transition into a constant-gradient regime would imply LMC.** Using the weight-averaging representation of the telescoping model, it becomes easy to see that not only would stabilization of the tangent kernel be _associated_ with lower linear loss barriers, but the transition into a lazy regime during training - i.e. reaching a point \(t^{*}\) after which the model gradients no longer change - can be _sufficient_ to imply LMC during training as observed in  under a mild assumption on the performance of the two networks' _ensemble_. To see this, let \(L(f):=_{X,Y P}[(f(X),Y)]\) denote the expected loss of \(f\) and recall that if \(sup_{}L(f_{_{1T}^{t^{}}+(1-)_{2T}^{t^{}}})-[ L(_{_{1T}^{t^{}}}) +(1-)L(f_{_{2T}^{t^{}}})] 0\) then LMC is said to hold. If we assume that ensembles \(^{}():= f_{_{1T}^{t^{}}}( )+(1-)f_{_{2T}^{t^{}}}()\) perform no worse than the individual models (i.e. \(L(^{}) L(f_{_{1T}^{t^{}}})+(1-)L( f_{_{2T}^{t^{}}})\ \), as is usually the case in practice ), then one case in which LMC is guaranteed is if the predictions of weight-averaged model and ensemble are identical. In Appendix B.2, we show that if there exists some \(t^{*}[0,T)\) after which the model gradients \(_{}f_{_{jt}^{t^{*}}}()\) no longer change (i.e. for all \(t^{} t^{*}\) the learned updates \(_{jt}^{t^{}}\) lie in a convex set \(_{j}^{stable}\) in which \(_{}f_{_{jt^{*}}^{t^{}}}() _{}f_{_{*}}()\)), then indeed

\[^{}() f_{_{1T}^{t^{}}+(1 -)_{2T}^{t^{}}}() f_{_{t^{ }}}()+_{}f_{_{*}}()^{ }_{t=t^{}+1}^{T}(_{1t}^{t^{}}+(1- )_{2t}^{t^{}}).\] (10)

That is, transitioning into a regime with constant model gradients during training can imply LMC because the ensemble and weight-averaged model become near-identical. This also has as an immediate corollary that models with the same \(_{0}\) which train fully within this regime (e.g. those discussed in ) will have \(t^{*}=0\). Note that, when using nonlinear (final) output activation \(()\) the post-activation model gradients will generally _not_ become constant during training (as we discuss in Sec. 5 for the sigmoid and as was shown theoretically in  for general nonlinearities). If, however, the _pre-activation model gradients_ become constant during training and the _pre-activation ensemble_ - which averages the two model's pre-activation outputs _before_ applying \(()\) - performs no worse than the individual models (as is also usually the case in practice ), then the above also immediately implies LMC for such models.

This suggests a candidate explanation for why LMC emerged at specific points in . To test this, we replicate their CIFAR-10 experiment using a ResNet-20 in Fig. 6. In addition to plotting the maximal decrease in accuracy when comparing \(f_{_{t^{}}^{}+(1-)_{2T}^{}}( )\) to the weighted average of the accuracies of the original models as  to measure LMC in (1), we also plot the squared change in (pre-softmax) gradients \((_{}_{t^{}+39}()-_{}_{t^{}}())^{2}\) over the next epoch (390 batches) after checkpoint \(t^{}\), averaged over the test set and the parameters in each layer in (2). We find that the disappearance of the loss barrier indeed coincides with the time in training when the model gradients become _more stable_ across all layers. Most saliently, the appearance of LMC appears to correlate with the stabilization of the gradients of the linear output layer. However, we also continue to observe some changes in other model gradients, which indicates that these models do not train fully linearly.

**Pre-training and weight averaging.** Because weight averaging methods have become increasingly popular when using _pre-trained_ instead of randomly initialized models , we are interested in testing whether pre-training may improve mode connectability through stabilizing the model gradients. To test this, we replicate the above experiment with the same architecture pre-trained on the SVHN dataset (in green in Fig. 6(1)). Mimicking findings of , we first find the loss barrier to be substantially lower after pre-training. In Fig. 6(3), we then observe that the gradients in the hidden and final layers indeed change less and stabilize earlier in training than in the randomly initialized model - yet the gradients of the BatchNorm parameters change _more_. Overall, the findings in this section thus highlight that while there may be a connection between gradient stabilization and LMC, it cannot fully explain it - suggesting that further investigation into the phenomenon using this lens, particularly into the role of BatchNorm layers, may be fruitful.

_Takeaway Case Study 3._ Reasoning through the learning process by telescoping out functional updates suggests that averaging model parameters trained from the same checkpoint can be effective if their models' gradients remain stable, however, this cannot fully explain LMC in the setting we consider.

## 5 The Effect of Design Choices on Linearized Functional Updates

The literature on the neural tangent kernel primarily considers plain SGD, while modern deep learning practice typically relies on a range of important modifications to the training process (see e.g. [14, Ch. 6]) - this includes many of the experiments demonstrating surprising deep learning phenomena we examined in Sec. 4. To enable us to use modern optimizers above, we derived their implied linearized functional updates through the weight-averaging representation \(_{t}()=_{}_{_{t-1}}( )^{}_{t}\), which in turn allows us to define \(K_{t}^{T}(,)\) in Eq. (5) for these modifications using straightforward algebra. As a by-product, we found that this provides us with an interesting and pedagogical formalism to reason about the relative effect of different design choices in neural network training, and elaborate on selected learnings below.

\(\)**Momentum** with scalar hyperparameter \(_{1}\) smoothes weight updates by employing an exponentially weighted average over the previous parameter gradients as \(_{t}=-_{t}}{1-_{1}^{}}_ {k=1}^{t}_{1}^{t-k}_{k}_{k}^{}\) instead of using the current gradients alone. This implies linearized functional updates

\[_{t}()=-_{t}}{1-_{1}^{ }}_{i[n]}(K_{t}^{}(,_{i})g_{it}^{ }+_{k=1}^{t-1}_{1}^{t-k}K_{t,k}^{}(,_{i})g_{ik}^{})\] (11)

where \(K_{t,k}^{}(,_{i})\{i  E_{k}\}}{|B_{k}|}_{}_{t-1}()^{} _{}_{b_{k-1}}(_{i})\) denotes the cross-temporal tangent kernel. Thus, the functional updates also utilize _previous_ loss gradients, where their weight is determined

Figure 6: **Linear mode connectivity and gradient changes by \(t^{}\). (1) Decrease in accuracy when using averaged weights \(_{1T}^{}+(1-)_{2T}^{}\) for randomly initialized (orange) and pre-trained ResNet-20 (green). (2) & (3) Changes in model gradients by layer for a randomly initialized (2) and pretrained (3) model.**

using an inner product of the model gradient features from different time steps. If \(_{}f_{_{t}}()\) is constant throughout training and we use full-batch GD, then the contribution of each training example \(i\) to \(_{t}()\) reduces to \(-_{t}K_{0}^{}(,_{i})}{1- _{l}}[_{k=1}^{d}_{i}^{t-k}g_{ik}^{}]\), an exponentially weighted moving average over its past loss gradients - making the effect of momentum on functional updates analogous to its effect on updates in parameter space. However, if \(_{}f_{_{t}}()\) changes over time, it is e.g. possible that \(K_{k,t}^{}(,_{i})\) has opposite sign from \(K_{t}^{}(,_{i})\) in which case momentum reduces instead of amplifies the effect of a previous \(g_{it}^{}\). This is more obvious when re-writing Eq. (11) to collect all terms containing a specific \(g_{it}^{}\), leading to \(K_{t}^{T}(,_{i})=_{k=t}^{T}_{k}}{1-_{1}^{}}_{1}^{k-t}K_{k,t}(,_{i})\) for Eq. (5).

\(\) **Weight decay** with scalar hyperparameter \(\) uses \(_{t}=-_{t}(_{t}_{t}^{}+ _{t-1})\). For constant learning rate \(\) this gives \(_{t}=_{0}-_{k=1}^{t}(_{t} _{k}^{}+_{k-1})=(1-)^{t}_{0}- _{k=1}^{t}(1-)^{t-k}_{k}_{k}^{}\). This then implies linearized functional updates

\[_{t}()=-_{i[ n]}(K_{t}(,_{i})g_{it}^{}-_{k=1}^{t-1}(1- )^{t-1-k}K_{t,k}(,_{i})g_{ik}^{})\\ -(1-)^{t-1}_{}_{_{t-1}}()^{}_{0}\] (12)

For full-batch GD and constant tangent kernels, \(- K_{0}^{}(,_{i})[g_{it}- _{k=1}^{t-1}(1-)^{t-1-k}g_{ik}]\) is the contribution of each training example to the functional updates, which effectively decays the previous contributions of this example. Further, comparing the signs in Eq. (12) to Eq. (11) highlights that momentum can _offset_ the effect of weight decay on the learned updates in function space (in which case weight decay mainly acts through the term decaying the initial weights \(_{0}\)).

\(\) **Adaptive & parameter-dependent learning rates** are another important modification in practice which enable the use of different step-sizes across parameters by dividing \(_{t}\)_elementwise_ by a \(p 1\) scaling vector \(_{t}\). Most prominently, this is used to adaptively normalize the magnitude of updates (e.g. Adam  uses \(_{t}=}{1-_{2}^{}}_{k=1}^{d}_ {2}^{-k}[_{k}_{k}^{}]^{2}+}\)). When combined with plain SGD, this results in kernel \(K_{t}^{}(,_{i})=\}}{ |B_{t}|}_{}f_{_{t-1}}()^{ }(_{t}})_{}f_{_{t-1}} (_{i})\). This expression highlights that \(_{t}\) admits an elegant interpretation as _re-scaling the relative influence of features_ on the tangent kernel, similar to structured kernels in non-parametric regression [14, Ch. 6.4.1].

\(\) **Architecture design choices** also impact the form of the kernel. One important practical example is whether \(f_{}()\) applies a non-linear activation function to the output \(g_{}()\) of its final layer. Consider the choice of using the sigmoid \((z)=}\) for a binary classification problem and recall \((z)=(z)(1-(z))(0,}{{4}}]\), which is largest where \((z)=}{{2}}\) and smallest when \((z) 0 1\). If \(K_{t}^{,g}(,_{i}) \{i B_{t}\}}{|B_{t}|}_{}g_{_{t-1}}()^{}_{}g_{_{t-1}}( _{i})\) denotes the tangent kernel of the model without activation, it is easy to see that the tangent kernel of the model \((g_{_{t}}())\) is

\[K_{t}^{,}(,_{i})=(g_{_{t}} ())(1-(g_{_{t}}()))(g_{ _{t}}(_{i}))(1-(g_{_{t}}(_{i})))K_{t}^{ ,g}(,_{i})\] (13)

indicating that \(K_{t}^{,}(,_{i})\) will give relatively higher weight in functional updates to training examples \(i\) for which the model is uncertain (\((g(_{i}))}{{2}})\)) and lower weight to examples where the model is certain (\((g_{_{t}}(_{i})) 0 1\)) - _regardless_ of whether \((g_{_{t}}(_{i}))\) is the correct label. Conversely, Eq. (13) also implies that when comparing the functional updates of \((g_{}())\) to those of \(g_{}()\) across inputs \(\), updates with \(()\) will be relatively larger for \(\) where the model is uncertain (\((g_{_{t}}())}{{2}})\)). Finally, Eq. (13) also highlights that the (post-activation) tangent kernel of a model with sigmoid activation will generally not be constant in \(t\) unless the model predictions \((g_{_{t}}())\) do not change.

## 6 Conclusion

This work investigated the utility of a telescoping model for neural network learning, consisting of a sequence of linear approximations, as a tool for understanding several recent deep learning phenomena. By revisiting existing empirical observations, we demonstrated how this perspective provides a lens through which certain surprising behaviors of deep learning can become more intelligible. In each case study, we intentionally restricted ourselves to specific, noteworthy empirical examples which we proceeded to re-examine in greater depth. We believe that there are therefore many interesting opportunities for future research to expand on these initial findings by building upon the ideas we present to investigate such phenomena in more generality, both empirically and theoretically.

#### Acknowledgements

We would like to thank James Bayliss, who first suggested to us to look into explicitly unravelling SGD updates to write trained neural networks as approximate smoothers to study deep double descent after a seminar on our paper  on non-deep double descent. This suggestion ultimately inspired many investigations far beyond the original double descent context. We are also grateful to anonymous reviewers for helpful comments and suggestions. AC and AJ gratefully acknowledge funding from AstraZeneca and the Cystic Fibrosis Trust, respectively. This work was supported by a G-Research grant, and Azure sponsorship credits granted by Microsoft's AI for Good Research Lab.