# Imitation Learning from Vague Feedback

Xin-Qiang Cai\({}^{1}\), Yu-Jie Zhang\({}^{1}\), Chao-Kai Chiang\({}^{1}\), Masashi Sugiyama\({}^{2,}\)\({}^{1}\)

\({}^{1}\) The University of Tokyo, Tokyo, Japan

\({}^{2}\) RIKEN AIP, Tokyo, Japan

###### Abstract

Imitation learning from human feedback studies how to train well-performed imitation agents with an annotator's relative comparison of two demonstrations (one demonstration is better/worse than the other), which is usually easier to collect than the perfect expert data required by traditional imitation learning. However, in many real-world applications, it is still expensive or even impossible to provide a clear pairwise comparison between two demonstrations with similar quality. This motivates us to study the problem of imitation learning with vague feedback, where the data annotator can only distinguish the paired demonstrations correctly when their quality differs significantly, i.e., one from the expert and another from the non-expert. By modeling the underlying demonstration pool as a mixture of expert and non-expert data, we show that the expert policy distribution can be recovered when the proportion \(\) of expert data is known. We also propose a mixture proportion estimation method for the unknown \(\) case. Then, we integrate the recovered expert policy distribution with generative adversarial imitation learning to form an end-to-end algorithm1. Experiments show that our methods outperform standard and preference-based imitation learning methods on various tasks.

## 1 Introduction

Imitation learning (IL) is a popular approach for training agents to perform tasks by learning from expert demonstrations [1; 2; 3]. It has been applied successfully to a variety of tasks, including robot control , autonomous driving , and game playing . However, traditional IL methods struggle when presented with both expert and non-expert demonstrations, as the agents may learn incorrect behaviors from the non-expert demonstrations . This problem, which researchers refer to as "Imitation Learning from Imperfect Demonstration" (ILfID), arises when the demonstrations used to train the model may contain some non-expert data [6; 7; 8; 9].

One popular solution to ILfID is resorting to an oracle to provide specific information, such as explicit labels (confidence from an expert) of each demonstration, as in Figure 0(a). However, such a specific oracle is quite expensive. A more recent framework, Reinforcement Learning from Human Feedback (RLHF) [10; 11], incorporates human feedback into the learning process, including a key part of the pipeline used for training ChatGPT . There exist two widely studied paradigms of feedback: full ranking and global binary comparison, as in Figures 0(b) and 0(c). Methods that use full-ranked demonstrations assumed that the feedback between every pair of trajectories is available, and further employed preference-based methods to solve the problem [13; 14]. Other studies have investigated situations where demonstrations can be divided into two global binary datasets, allowing the learner to filter out non-expert data from these global comparisons [7; 8]. However, data processed by any feedback of Figures 0(a), 0(b), and 0(c) require the guarantee that clear ranking information or at least one set consisting of pure expert demonstrations exists, so that off-the-shall IL methods can be applied immediately. The availability of expert demonstrations raises the question of what if, in practice, the feedback is not clear enough to provide purely expert demonstrations but mixed with non-expert demonstrations? The question poses a challenge that previous methods fail to address since none ofthe previous IL algorithms handles the issue of the _mixture_ of expert and non-expert demonstrations without other information. Their conventional strategies are to deal with the _vagueness_ in the data processing stage (relying on human feedback) not during the learning stage (by the learning agent).

Therefore, we consider a kind of weaker feedback than those previously formulated in the literature, which unintentionally mixes expert and non-expert demonstrations due to vague feedback. Specifically, the human annotator demonstrates in a pairwise way and only distinguishes the expert demonstration from the non-expert one. However, the annotator cannot tell the origin when both are from an expert or a non-expert. As depicted in Figure 0(d), this annotation process results in two datasets: \(^{+}\), containing demonstrations the annotator believes to be most likely from experts, and \(^{-}\), containing non-expert-like demonstrations. If one only places the distinguishable demonstrations into the pools and discards the indistinguishable ones, it results in Figure 0(c). Since we aim to investigate the potential of IL under the presence of non-expert demonstrations, we distribute the indistinguishable demonstrations randomly, one to \(^{+}\) and the other to \(^{-}\). This step embodies the _vagueness_ we wish to study in this paper. Note that either dataset contains non-expert demonstrations, and thus, a direct application of the existing IL method might be inappropriate due to the danger of learning from undesirable demonstrations.

_Vague feedback_ commonly exists in many scenarios, especially regarding RLHF . In a crowd-sourcing example of routes navigation, as shown in Figure 0(e), most of the time crowd-workers may lack domain expertise. Meanwhile, it is quite difficult to distinguish every pair of demonstrations (when facing two trajectories from the same source, such as (\(\),\(\)) or (\(\),\(\))). Therefore, we cannot obtain high-quality crowd-sourcing labels when the data are annotated  as in Figures 0(a), 0(b), and 0(c). On the other hand, it is natural for workers to provide _vague_ comparisons as in Figure 0(d).

In this work, we formulate the problem of _Vaguely Pairwise Imitation Learning (VPIL)_, in which the human annotator can only distinguish the paired demonstrations correctly when their quality differs significantly. In Section 4, we analyze two situations within this learning problem: V PIL with known expert ratio \(\) and unknown \(\). For VPIL with known \(\), we provide rigorous mathematical analysis and show that the expert policy distribution can be empirically estimated with the datasets \(^{+}\) and \(^{-}\); for the more challenging situation of VPIL with unknown \(\), we propose a reduction of the problem of estimating \(\) to a mixture proportion estimation problem  and develop an iterative update procedure to estimate \(\). In Section 5, we integrate our algorithm with an off-the-shelf IL method to solve VPIL problems. In Section 6, we evaluate our methods with state-of-the-art ILfID methods on a variety of tasks of MuJoCo  with different \(\) ratios and find that our methods obtained the best performance.

## 2 Related Work

In imitation learning scenarios, a model is trained to mimic the actions of expert demonstrations. One of the main challenges of imitation learning is that the gathered demonstrations can be imprecise, making it difficult for the learner to accurately replicate the underlying policy of the demonstrations. To overcome this issue, various works have utilized an annotator, a source of supplementary supervi

Figure 1: (a)â€“(d): The comparisons among the imitation learning with the explicit label, full ranking, global comparison, and our setting. \(\) denotes the expert data while \(\) denotes the non-expert one. (e) An example of labeling route navigation. In the example, Path 1 and Path 2 share the same distance. A similar situation lies in Path 3 and Path 4. The data collectors cannot provide explicit, ranking, or pairwise information as in (a), (b), and (c), while they can only provide vague comparisons that \(^{+}\) can be more expert-like than \(^{-}\).

sion that can aid the learner in better understanding the expert's intent. For example,  used explicit action signals of the annotator; , , and  used ranking/preference information from the annotator to learn the policy;  utilized confidence information from the annotator to re-weight the unlabeled demonstrations and further learned the policy. However, as illustrated in Section 1, in some cases, the annotator may not be able to provide explicit information for the imperfect demonstrations, especially when the demonstrations come from different sources.

Alternative strategies for IL by imposing prior knowledge instead of an annotator have also been put forward, such as state density estimation for importance weighting , state-action extrapolating , and adding noise to recover ranking . But this research line relies on certain assumptions about the state/state-action spaces, which may not always hold in many real-world applications. In this work, we focus on using an annotator with vague information, which is also low-cost.

Our methods drew inspiration from the risk rewriting technique in weakly supervised learning literature , which aims to establish an unbiased risk estimator for evaluating the model's quality only with weak supervision. Although the technique has been successfully applied to various specific weakly supervised learning problems [24; 25; 26; 27], these methods typically require knowledge of a parameter called the mixture proportion, i.e., the expert ratio in our problem , the estimation of which can be challenging in the imitation learning problem (see Section 6.3 for more details). To overcome this issue, we introduce an iterative data selection procedure to better estimate the expert ratio, leading to superior empirical performance. It is worth noting that  also used the risk rewriting technique to identify the expert demonstrations, but their algorithm does not require estimation of the expert ratio as the confidence score is given. However, such information is unavailable in VPIL, making the problem more challenging.

## 3 Preliminaries and Problem Setting

In this section, we first introduce the IL process. Then we formulate the VPIL problem.

### Preliminaries

Markov Decision Process.In policy learning problems, a Markov Decision Process (MDP) can be represented by a tuple \(,,,,r,T\), where \(\) is the state space; \(\) is the action space; \(:\) is the transition probability distribution; \(\) is a discount factor in the range \((0,1]\); \(r:\) is the reward function; and \(T\) is the horizon. The goal is to learn a policy \(\) that maximizes the expected returns \([_{t=0}^{}^{t}r(s_{t},a_{t})]\), where \(\) denotes the expectation.

In IL, the learner does not have access to the reward function \(r\). Instead, the learner is given \(m\) expert demonstrations \(_{,1},_{,2},,_{,m}\), where \(_{,i},i\{1,,m\}\) is the \(i\)-th trajectory (a series of state-action pairs) drawn independently following the demonstrator's policy \(_{}\). The goal of the learner is to learn a policy \(_{}\) to mimic \(_{}\).

Occupancy Measure.Since in IL we do not have reward functions, we need to measure the policy performance in the state-action space, i.e., occupancy measure \(_{}\). The occupancy measure \(_{}:\) was introduced to characterize the distribution of state-action pairs generated by a given policy \(\), which was defined with the discount factor: \(_{}(s,a)=(a|s)_{t=1}^{}^{t}[s_{t}=s|]\), where \([s_{t}=s|\ ]\) is the probability of reaching state \(s\) at time \(t\) following policy \(\).

Moreover, we know that there is a one-to-one correspondence between the occupancy measure and the policy. Specifically, we have the following theorem.

Figure 2: The description of the data collection process.

**Theorem 1** (Theorem 2 of ).: _Suppose \(\) is the occupancy measure for \(_{}(a\,|\,s) A}(s,a^{ })}\). Then, \(_{}\) is the only policy whose occupancy measure is \(\)._

We can also define a normalized version of the occupancy measure by \(p_{}(s,a)(s,a)}{_{s^{} S,a^{} A }p_{}(s^{},a^{})}=(1-)_{}(s,a)\). If we have \(m\) expert demonstrations \(_{,1},_{,2},,_{,m}\), the occupancy measure \(p_{_{}}\) can be empirically estimated by the trajectories as \(_{_{}}(s,a)=_{i=1}^{m} _{t=0}^{}^{t}(s_{t}^{(i)},a_{t}^{(i )})=(s,a)\), where \([]\) is an indicator function that returns 1 if \(\) is true and returns 0 otherwise. Finding out the underlying \(p_{_{}}\) is the key for solving IL problems .

### Vaguely Pairwise Imitation Learning

In this work, we focus on the ILfID problem with pairwise information, where the learner aims to learn a good policy with a pairwise dataset \((^{+},^{-})\), generated from a mixture demonstration pool performed by an expert policy \(_{}\) and a set of non-expert policies \(\{_{}^{(k)}\}_{k=1}^{K}\). In this work, we consider the mixed occupancy measure of the non-expert policy set as \(p_{_{}}\). The proportion of the expert data within the pool is denoted as \((0,1]\). The data collection process is shown in Figure 2. To clearly reveal the effect of mixed demonstrations and explain the proposed framework, we choose to assume that the data collector is not an attacker and will make mistakes, so there will be no noise during the collection process. In Section 6.4, we discuss how our method can easily be extended to handle the presence of human error.

Also, in this work, we do not consider that the data collector could be an attacker or make mistakes, so there will be no noise during the collection process.

Collection of Pairwise Datasets.The data collector would first sample a pair of trajectories \((_{i},_{j})\) independently from the mixture pool. If \((_{i},_{j})\) are from different sources, i.e., one is from the expert, and another is not, then the collector will take the expert one \(_{i}\) into \(^{+}\) and the non-expert one \(_{j}\) into \(^{-}\). Otherwise, the collector randomly puts them into \(^{+}\) and \(^{-}\). Under such a data generation process, the expertise probabilities of this pair are as follows:

\[\{[_{i} p_{_{}},_{j}  p_{_{}}]=^{2},\\ [_{i} p_{_{}},_{j} p_{_{}} ]=(1-)^{2},\\ [_{i} p_{_{}},_{j} p_{_{}} ]=2(1-)..\] (1)

In such a case, \(^{+}\) is always "not worse" than \(^{-}\) as it contains more expert data.

## 4 Learning frameworks for Solving VPIL Problems

In this section, we analyze the core challenge in VPIL problems with the pairwise datasets \(^{+}\) and \(^{-}\), i.e., recovering the occupancy measure of the expert policy \(p_{_{}}\). To this end, we propose two learning frameworks for VPIL with known \(\) and unknown \(\), respectively.

### VPIL with Known \(\)

Most of the IL methods based on the occupancy measure need to assume that the demonstrations are sampled only from the expert policy, so that they can directly estimate \(p_{_{}}\) and match the learner's distribution with \(p_{_{}}\). However, the occupancy measure of the expert policy \(p_{_{}}\) is inaccessible in the VPIL problem, since both \(^{+}\) and \(^{-}\) contain non-expert data while the label of each data is unavailable. Below, we attempt to approximate \(p_{_{}}\) with \(\{^{+},^{-}\}\).

Let \(n_{+}=|^{+}|\); \(_{i}^{+}(s,a)=(1-)_{t=0}^{}^{t} [(s_{t,i},a_{t,i})=(s,a)]\) for \(i=1,,n_{+}\) be the empirical occupancy measure of a trajectory \(_{i}^{+}\), where \((s_{t,i},a_{t,i})_{i}\) is a state action pair at time \(t\). Let \(_{_{+}}(s,a)=_{i=1}^{n_{+}}_{i}^{+}(s,a)/n_{+}\). Define \(_{_{-}}(s,a)\) similarly. We have the following theorem.

**Theorem 2**.: _Assume the pairwise datasets \((^{+},^{-})\) are generated following the procedure in Section 3.2. Let \(p_{_{+}}(s,a)=_{^{+}}[_{_{+}}(s,a)]\) and \(p_{_{-}}(s,a)=_{^{-}}[_{_{-}}(s,a)]\) be the expected occupancy measures of \(^{+}\) and \(^{-}\), where the randomness is taken over the draws of \(^{+}\) and \(^{-}\). Then, we have_

\[\{p_{_{+}}(s,a)=\ \ (2-^{2})p_{_{ }}(s,a)+(1-)^{2}p_{_{}}(s,a),\\ p_{_{-}}(s,a)=\ \ ^{2}p_{_{}}(s,a)+(1-^{2})p_{_{ }}(s,a),.\] (2)\[p_{_{}}(s,a)=&p_{_{+}}(s,a)- p_{_{-}}(s,a),\\ p_{_{NE}}(s,a)=&-p_{_{+}}(s,a)+ {2-2}p_{_{-}}(s,a).\] (3)

A proof can be found in Appendix A. The Theorem provides a feasible way to recover the unknown occupancy measure of the expert policy from contaminated data pools \(^{+}\) and \(^{-}\). Thus, we can use an off-the-shelf IL method to learn the policy.

Once we have recovered \(p_{_{}}\) by Eq. (10), we can use an off-the-shelf IL method to learn the policy.

### VPiL with Unknown \(\)

When facing a more challenging problem in which the expert ratio \(\) is unknown, a straightforward way is to estimate the ratio first and then reconstruct the expert policy by the approach developed for known \(\) cases. Here, we will first introduce how to estimate the expert ratio by reducing it into a mixture proportion estimation (MPE) problem  and then identify that a direct application of the estimated ratio is not accurate enough to reconstruct the expert policy. To this end, we further propose an iterative sample selection procedure to exploit the estimated expert ratio, which finally leads to a better approximation of the expert policy.

Estimation of the Expert Ratio \(\).In this paragraph, we show that the estimation of the expert ratio \(\) can be reduced to two MPE problems . Let \(\) and \(\) be two probability distributions, and

\[=+(1-)\] (4)

be a mixture of them with a certain proportion \((0,1)\). The MPE problem studies how to estimate mixture proportion \(\) with the empirical observations sampled from \(\) and \(\) (not from \(\)). Over the decades, various algorithms were proposed with sound theoretical and empirical studies .

To see how the estimation of the expert ratio \(\) is related to the MPE problem, we rewrite (2) as

\[p_{_{+}}(s,a)=_{1}p_{_{-}}(s,a)+(1-_{1})p_{_{}} \ \ \ \ \ p_{_{-}}(s,a)=_{2}p_{_{+}}(s,a)+(1-_{2})p_{_{NE}},\] (5)

where \(_{1}=\) and \(_{2}=\). By taking \(p_{_{+}}\) as \(\) and \(p_{_{-}}\) as \(\), the first line of (5) shares the same formulation as the MPE problem (4). Since \(p_{_{+}}\) and \(p_{_{-}}\) are empirically accessible via \(^{+}\) and \(^{+}\), we can estimate \(_{1}\) by taking \(^{+}\) and \(^{-}\) as the input of any MPE solver and obtain the expert ratio by \(=}{1+_{1}}\). The same argument also holds for the second line of (5), where we can take \(p_{_{-}}\) as \(\) and \(p_{_{+}}\) as \(\) to estimate \(\). Then, the expert ratio can also be obtained by \(=}{1+_{2}}\).

One might worry about the identifiability of the expert ratio \(\) by estimating it with MPE techniques . We can show that the true parameter is identifiable if the distribution \(p_{_{+}}\) and distribution \(p_{_{-}}\) are mutually irreducible  as follows:

**Proposition 1**.: _Suppose the distributions \(p_{_{}}\) and \(p_{_{NE}}\) are mutually irreducible such that there exists no decomposition of the form \(p_{_{}}=(1-)Q+ p_{_{NE}}\) and \(p_{_{NE}}=(1-^{})Q^{}+^{}p_{_{E}}\) for any probability distributions \(Q,Q^{}\) and scalars \(,^{}(0,1]\). Then, the true mixture proportions \(_{1}\) and \(_{2}\) are unique and can be identified by_

\[\{_{1}=\{|p_{_{+}}= p_{_{-}}+(1 -)K,K\},\\ _{2}=\{^{}|p_{_{-}}=^{}p_{_{+}}+(1-^{ })K^{},K^{}\},.\] (6)

_where \(\) is the set containing all possible probability distributions. Thus, \(\) is identifiable by_

\[=(1-_{1})/(1+_{1})\ \ \ \ \ \ \ =2_{2}/(1+_{2}).\] (7)

Proposition 1 demonstrates that the true mixture proportion \(_{1}\) (resp. \(_{2}\)) can be identified by finding the maximum proportion of \(p_{_{-}}\) contained in \(p_{_{+}}\) (resp. \(p_{_{+}}\) contained in \(p_{_{-}}\)). This idea can be empirically implemented via existing MPE methods . We note that the expert ratio is identifiable by either estimating \(_{1}\) or \(_{2}\) when we have infinite samples. In the finite sample case, the two estimators coming from different distribution components could lead to different estimation biases. Since the MPE solutions tend to have a positive estimation bias on the true value \(_{1}\) and \(_{2}\) as shown by [28, Theorem 12] and [31, Corollary 1], the estimation \(=(1-_{1})/(1+_{1})\) tends to yield an underestimated \(\) while that with \(_{2}\) will lead to an overestimation. Besides, when the number of expert data is quite small, the underlying true parameter \(_{2}\) would also have a small value,and its estimation would be highly unstable. Thus, we choose to estimate \(\) with \(_{1}\) in our algorithm. The empirical studies in Section 6.3 also supported our choice.

The relationship (2) between the expert and non-expert data shares a similar formulation to that of the unlabeled-unlabeled (UU) classification , which is a specific kind of weakly supervised learning problem studying how to train a classifier from two unlabeled datasets and requires the knowledge of mixture proportions. However, how to estimate these mixture proportions is still an open problem in the weakly supervised learning literature. Although our reduction is developed for estimating the expert ratio for IL problems, it can be applied to a UU learning setting for independent interest.

Reconstruct Expert Policy by Data Selection.To handle the MPE task for high dimensional data, a practice is to first train a classifier with probability output and then conduct the MPE on the probability outputs of samples . However, the quality of the trained classifier turns out to depend on the estimation accuracy of \(\). On the other hand, we found that it is possible to filter out the undesired component of the pairwise datasets (\(_{_{}}\) in \(^{-}\) and \(_{_{}}\) in \(^{+}\)) and keep the desired data to enhance estimating \(\). Therefore, also inspired by  of the distribution shift problem and weakly supervised learning, we propose an iteration-based learning framework. In each iteration, we throw away the non-expert data with higher confidence in \(^{+}\) and the expert data with higher confidence in \(^{-}\) after estimating \(\), and train a classifier with the datasets after selection. The detailed learning process can be found in Algorithm 1.

```
0: Pairwise Demonstrations \(^{+}\), \(^{-}\); Expert ratio \(\) (Unknown for COMPILER-E); Environment env.
0: The learner policy \(_{}\).
1: Initialize the learner policy \(_{}\) and the discriminator \(D_{}\).
2:if\(\) is unknown then\(\) COMPILER-E
3: Obtain the estimated expert ratio \((^{+},^{-})\).
4:else\(\) COMPILER
5:\(\) input(\(\)).
6:endif
7:for each training steps do
8: Sample a batch of learner's data \((s,a) p_{_{}}\) by the interactions between \(_{}\) and env.
9: Sample a batch of demonstrations data \((s,a)_{_{+}}\) and \((s,a)_{_{-}}\) from \(^{+}\) and \(^{-}\).
10: Update \(D_{}\) by maximizing (8).
11: Update \(_{}\) with \((s,a) p_{_{}}\) and the reward \(- D_{}(s,a)\) using off-the-shelf RL algorithm.
12:endfor ```

**Algorithm 1**ExpertRatioEstimation

After we obtained the estimated \(\), we can recover \(p_{_{}}\) as by Eq. (10).

COMPParative Imitation LEarning with Risk rewriting (COMPILER)

We have illustrated how to estimate the occupancy measure of the expert policy \(p_{_{}}\) from \((^{+},^{-})\) under known and unknown \(\) respectively. Here we describe how to adopt these learning frameworks into end-to-end algorithms for learning an policy. We name our algorithm for VPIL with known \(\)**COMParative Imitation LEarning with Risk-rewriting (COMPILER)**, and with unknown \(\)**COMParative Imitation LEarning with Risk-rewriting by Estimation (COMPILER-E)**.

Current state-of-the-art adversarial IL methods [37; 1] aim to learn a policy by matching the occupancy measure between the learner and the expert policies. We fuse our methods with one of the representative adversarial IL methods, Generative Adversarial Imitation Learning (GAIL) , whose optimization problem is given as follows:

\[_{}_{w}_{(s,a) p_{_{ }}} D_{w}(s,a)+_{(s,a) p_{_{}}}(1-D_{w} (s,a)),\]

where \(D_{w}:\) parameterized by \(w\) is a discriminator. \(p_{_{}}\) parameterized by \(\) is the occupancy measure of a policy. In our setting, the expert demonstration of \(p_{_{}}\) is unavailable.

COMPILER.As suggested by Theorem 2, we can recover \(p_{_{}}\) with the occupancy measures of \(^{+}\) and \(^{-}\). Then, we can train the policy \(p_{_{}}\) by mincing \(p_{_{}}=p_{_{+}}-p_{_{-}}\) in Eq. (10). Specifically, plugging (2) into (8) and approximating \(p_{_{+}}\) and \(p_{_{-}}\) with their empirical versions, we can obtain the desirable target. However, such rewriting can introduce a negative term and lead to overfitting [38; 39]. To avoid such an undesired phenomenon, instead of training the policy \(_{}\) to make \(p_{_{}}\) match \(_{_{+}}-_{ _{-}}\), we twist the objective function of GAIL (8) to minimize the discrepancy between \(p_{_{}}+_{ _{-}}\) and \(_{_{+}}\), which gives the following optimization problem without the negative term:

\[_{}_{w}2_{( s,a) p_{_{}}} D_{w}(s,a) +(1-)_{(s,a)_{_{-}}}(D_{w} (s,a))\] \[+(1+)_{(s,a)_{_{+}}}(1-D_{ w}(s,a)).\] (8)

Let \((_{},D_{w})\) be the objective function in (8) and \(V(_{},D_{w})\) be its expectation established on \(p_{_{+}}\) and \(p_{_{-}}\). We have the following theorem for the estimation error of the discriminator trained by (8).

**Theorem 3**.: _Let \(\) be a parameter space for training the discriminator and \(D_{}=\{D_{w} w\}\) be the hypothesis space. Assume the functions \(| D_{w}(s,a)| B\) and \(|(1-D_{w}(s,a))| B\) are upper-bounded for any state-action pair \((s,a)\) and \(w\). Further assume both the functions \( D_{w}(s,a)\) and \((1-D_{w}(s,a))\) are \(L\)-Lipschitz continuous in the state-action space. For a fixed policy \(_{}\), let \(^{}=\{_{i}^{}\}_{i=1}^{n_{}}\) be trajectories generated from \(_{}\). Then, for any \((0,1)\), with probability at least \(1-\), we have_

\[V(_{},D_{w}^{*})-V(_{},_{w})  4L(1+)_{n_{+}}(D_{})+4L(1-) _{n_{-}}(D_{}))\] \[+8L_{n_{}}(D_{})+C()( }}+}}+}} ),\]

_where \(_{w}=*{arg\,max}_{w}(_{ },D_{w})\) and \(D_{w}^{*}=*{arg\,max}_{w}V(_{},D_{w})\). The constants \(n_{+}\) and \(n_{-}\) are the number of trajectories in \(^{+}\) and \(^{-}\). We define \(C()=4B\). The empirical Radamacher complexities  on datasets \(^{+}\), \(^{-}\), and \(^{}\) are denoted by \(_{n_{+}}\), \(_{n_{-}}\), and \(_{n_{}}\)._

A proof is given in Appendix A. The theorem shows that the discriminator trained by (8) converges to the one optimized with the true distribution \(p_{_{+}}\) and \(p_{_{-}}\) at each step of the training.

COMPILER-E.For the VPIL problem with unknown \(\), first we estimate \(\) by Algorithm 1, then we learn the policy by (8) with \(\).

We integrate the two algorithmic processes COMPILER and COMPILER-E into Algorithm 2.

## 6 Experiments

In this section, we conducted extensive experiments under the setting of VPIL. Through the experiments, we want to investigate the following questions: (1) Can COMPILER and COMPILER-E solve VPIL problems under various expert ratios in demonstrations? (2) Is COMPILER-E still valid when using different MPE estimators to obtain \(\)? (3) How is the \(\) estimation with \(_{1}\) and \(_{2}\) as in (7)?

### Setup

To investigate the answer to the first question, we chose 20 different VPIL tasks with four MuJoCo benchmark environments to evaluate the performance of the contenders and our approaches under five different \(\) levels. The detailed setups are reported as follows.

Environments and Demonstrations.We set _HalfCheetah, Hopper, Swimmer_, and _Walker2d_ in MuJoCo as the basic environments. For each experiment, the demonstration pool contains 100 trajectories with different expert ratios \(=\{0.1,0.2,0.3,0.4,0.5\}\), in which \(=0.1\) is the most difficult situation of the VPIL problem. We also trained an expert-level RL agent and two non-expert RL agents as demonstrators by DDPG algorithm . The details of the environment and the demonstrations can be found in Table 1.

Dataset generation process.We start with the demonstration pool \(\{_{i}\}_{i=1}^{N}\), which contains \( N\) trajectories sampled from the optimal policy \(_{}\) and \((1-)N\) trajectories from the non-optimal policy \(_{}\). For notation simplicity, we denote by \(_{+}\) the part containing expert demonstrations only and by \(_{-}\) the part for the non-expert demonstrations. Then, we can generate the dataset as follows,

\[^{+}=&(_{+},(2- ^{2})N)(_{-},(1-)^{2}N), \\ ^{-}=&(_{+},^{2}N) (_{-},(1-^{2})N),\]

where \((,N)\) is the function that samples \(N\) trajectories from the dataset \(\).

Contenders.Since \(^{+}\) contains more expert demonstrations, here we use Behavior Cloning , GAIL , and AIRL  with \(^{+}\) only as basic baselines. Also, we provide GAIL with expert-level demonstrations (GAIL_expert) as the skyline of all methods. Besides, we set T-REX , a state-of-the-art preference-based IL method, by taking that every data in \(^{+}\) is more expert-like than that in \(^{-}\) as a form of preference to train its reward function. We also set the variants of T-REX algorithms, D-REX  and SSRR , that directly generate the ranking information through the imperfect datasets. CAIL proposed a confidence-based method to imitate from mixed demonstrations . In the experiment, we provide 5% trajectory labels to meet its requirement as suggested in their paper. Each method ran 4e7 steps. 5 trials were conducted with different seeds for each task.

Meanwhile, to investigate the answer to the second question, we conducted Best Bin Estimation (BBE)  and Kernel Mean (KM)  algorithm to estimate \(\) in algorithm 1 of COMPILER-E, as COMPILER-E (BBE) and COMPILER-E (KM) respectively. We note that COMPILER-E cannot obtain the true \(\) during the whole experiment.

Implementation of \(\) estimation.We implement a four-layer fully connected neural network as the binary classifier \(f_{}\) in Algorithm 1. The neurons in each layer are 1000, 1000, 100, and 50 respectively. The activation function of each layer is ReLU, and the output value will go through the sigmoid function to obtain the score for \(^{+}\) and \(^{-}\) as \(S^{+}\) and \(S^{-}\) respectively. The optimizer for \(f_{}\) is stochastic gradient descent. We totally train \(f_{}\) for 1000 epochs with bath size 1024. The initial learning rate is 5e-4, with an exponential decay rate of 0.99 at each step.

Implementation of policy training.We choose Proximal Policy Optimization (PPO)  as the basic RL algorithm, and set all hyper-parameters, update frequency, and network architectures of the policy part the same as . Besides, the hyper-parameters of the discriminator for all methods were the same: The discriminator was updated using Adam with a decayed learning rate of \(3 10^{-4}\); the batch size was 256. The ratio of update frequency between the learner and discriminator was 3: 1. For T-REX algorithm , we use the codes with default hyperparameters and model architecture of their official implementation.

### Empirical Results

The final results are gathered in Table 2. We can see that since AIRL and GAIL are standard IL algorithms, they cannot adaptively filter out non-expert data, so the underlying non-expert data reduces the performance of them. T-REX did not obtain promising results on VPIL tasks, which

    & Dimension of \(S\) & Dimension of \(\) & Non-Expert 1 & Non-Expert 2 & Expert & \(\) \\  Hopper & 11 & 3 & 1142.16\(\)159.28 & 1817.83\(\)819.69 & 3606.11\(\)43.95 & (0.1, 0.2, 0.3, 0.4, 0.5) \\ Swimmer & 8 & 2 & 46.28\(\)18.7 & 62.64\(\)16.82 & 100.63\(\)469 & (0.1, 0.2, 0.3, 0.4, 0.5) \\ Walker2d & 17 & 6 & 410.98\(\)36.59 & 766.67\(\)398.85 & 3204.37\(\)848.55 & (0.1, 0.2, 0.3, 0.4, 0.5) \\ HalfCheetah & 17 & 6 & 532.50\(\)58.66 & 864.18\(\)335.90 & 1599.06\(\)41.97 & (0.1, 0.2, 0.3, 0.4, 0.5) \\   

Table 1: The detailed information of demonstrations in the empirical studies.

[MISSING_PAGE_FAIL:9]

bigger as true \(\) decreased. On the other hand, although the underestimated method also overthrew the non-expert data in \(^{+}\), the proportion of these data was relatively high. So even if more were thrown away at the beginning, it will not affect the accuracy of the estimation. This is the reason why using \(\) to estimate \(\) is relatively stable and accurate.

### Case Study: Assessing the Robustness of the COMPILER Algorithm

To evaluate the robustness of the COMPILER algorithm, a comprehensive set of experiments were conducted. These were carried out in four distinct environments, implementing tasks with \(=0.5\). Each experiment was repeated five times to ensure the reliability of the results. To emulate real-world conditions, different levels of noise were introduced to the parameter \(\), resulting in the generation of corresponding noisy datasets. This noise was denoted as \(\), thereby resulting in \(=+\).

Figure 4 presents the findings from these experiments. Remarkably, the COMPILER algorithm demonstrated consistent performance, achieving at least 96% of the standard performance even in the presence of various noise levels. This robustness, as exhibited in the results, underlines the capacity of COMPILER to effectively handle and perform under noisy conditions, specifically when the provided \(\) is subject to disturbances. This emphasizes not only the resilience of the algorithm but also its potential for deployment in real-world situations where data is seldom perfect.

## 7 Conclusion

In this work, we formulated the problem of _Vaguely Pairwise Imitation Learning (VPIL)_, in which mixed expert and non-expert demonstrations are present, and the data collector only provides vague pairwise information of demonstrations. To solve this problem, we proposed two learning paradigms, with risk rewriting and mixture proportion estimations (MPE), to recover the expert distribution with the known expert ratio \(\) and unknown one respectively. Afterward, we showed that these paradigms can be integrated with off-the-shelf IL methods, such as GAIL, to form the algorithm COMParative Imitation LEearning with Risk rewriting (COMPILER) and that by Estimation (COMPILER-E) to solve the VPIL problem with known and unknown \(\) respectively. The experimental results showed that our methods outperformed standard and preference-based IL methods on a variety of tasks. In the future, we hope to use our algorithms to address more challenging problems, such as VPIL with multiple and noisy annotators.

Figure 4: The performance of COMPILER under different noise levels.

Figure 3: (a, b) The comparisons of overestimated (\(\)) and underestimated (\(\)) methods on various tasks with BBE and KM estimators. (c,d) The distribution map of thrown data ratio using estimated \(\) and true \(\). The thrown data ratio difference on \(^{+}\) is \((1-)^{2}-(1-)^{2}\), while that on \(^{-}\) is \(^{2}-^{2}\). The range marked by the red box is considered in our experiments, in which the expert ratio is smaller than (or equal to) the non-expert one.