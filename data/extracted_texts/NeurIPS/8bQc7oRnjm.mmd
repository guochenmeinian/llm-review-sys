# Provably Efficient Offline Reinforcement Learning

in Regular Decision Processes

 Roberto Cipollone

Sapienza University of Rome

cipollone@diag.uniroma1.it &Anders Jonsson

Universitat Pompeu Fabra

anders.jonsson@upf.edu &Alessandro Ronca

University of Oxford

alessandro.ronca@cs.ox.ac.uk &Mohammad Sadegh Talebi

University of Copenhagen

m.shahi@di.ku.dk

###### Abstract

This paper deals with offline (or batch) Reinforcement Learning (RL) in episodic Regular Decision Processes (RDPs). RDPs are the subclass of Non-Markov Decision Processes where the dependency on the history of past events can be captured by a finite-state automaton. We consider a setting where the automaton that underlies the RDP is unknown, and a learner strives to learn a near-optimal policy using pre-collected data, in the form of non-Markov sequences of observations, without further exploration. We present RegORL, an algorithm that suitably combines automata learning techniques and state-of-the-art algorithms for offline RL in MDPs. RegORL has a modular design allowing one to use any off-the-shelf offline RL algorithm in MDPs. We report a non-asymptotic high-probability sample complexity bound for RegORL to yield an \(\)-optimal policy, which makes appear a notion of concentrability relevant for RDPs. Furthermore, we present a sample complexity lower bound for offline RL in RDPs. To our best knowledge, this is the first work presenting a provably efficient algorithm for offline learning in RDPs.

## 1 Introduction

Most reinforcement learning (RL) algorithms hinge on the Markovian assumption, i.e. that the underlying system transitions and rewards are Markovian in some natural notion of (observable) state, and hence, the distribution of future observations depends only on the current state-action of the system. This fundamental assumption allows one to model decision making using the powerful framework of Markov Decision Processes (MDPs) . However, there are many application scenarios where rewards are issued according to temporal conditions over histories (or trajectories), and others where the environment itself evolves in a history-dependent manner. As a result, Markovian approaches may prove unsuitable for modeling such situations. These scenarios can be appropriately modeled as _Non-Markovian Decision Processes (NMDPs)_.

NMDPs describe environments where the distribution on the next observation and reward is a function of the history. In these environments, behaving optimally may also require to take histories into account. For example, a robot may receive a reward for delivering an item only if the item was previously requested, and a self-driving car is more likely to skid and lose control if it previously trained. Also, consider a mobile robot that has to track an object which may disappear from its field of view. The object is likely to be found again in the same place where it was seen last time. This requires the agent to remember, hence to act according to information in its interaction history. In general, an NMDP can show an arbitrary dependency on the history or trace, preventing efficient learning. Consequently, recent research has focused on tractable sub-classes of NMDPs. In RegularDecision Processes (RDPs) , the next observation and reward distributions depend on regular properties of the history, which can be captured by a deterministic finite-state automaton. This determines the existence of a finite state space where states are determined by histories, and where the Markov property is regained.

In this paper, we investigate offline RL in episodic RDPs, where the goal is to find a near-optimal policy using a pre-collected dataset, with minimal possible size, generated by a fixed behavior policy (and without further exploration). Offline RL in MDPs has received extensive attention recently, and provably sample efficient algorithms have been proposed for various settings. Despite the extensive and rich literature on MDPs, comparatively little work exists on offline RL in NMDPs. The scarcity of results may likely be attributed to the difficult nature of the problem rather than the lack of interest.

Partially-Observable Markov Decision Processes (POMDPs)  are also NMDPs, and RDPs can be seen as the subclass of POMDPs that enjoy the property of having hidden states determined by the history of observations. This is a key property that allows one to take advantage of a set of planning and learning techniques that do not apply to arbitrary POMDPs. Planning in POMDPs is computationally intractable , and two common approaches to solve (and learn) them rely on maintaining either a belief state or a finite history of observations. Maintaining and updating a belief state is worst-case exponential in the size of the original observation space, while the latter approach yields a space whose size is exponential in the history length. State-of-the-art work on offline RL in POMDPs considers restricted classes of POMDPs such as undercomplete POMDPs (e.g., [6; 7]), which cannot be used to model all RDP instances. General POMDPs are only considered under assumptions such as the possibility of reaching every belief state in a few steps  or ergodicity . While existing offline RL algorithms for solving POMDPs cannot guarantee provable learning in a generic RDP, the structural properties of RDPs indicate that they can be solved more efficiently using techniques that are carefully tailored to their structure. Exploiting the structure in RDPs is thus key in designing provably sample-efficient learning algorithms.

### Summary of Contributions

We formalize offline RL in RDPs (Section 2), and establish a first, to the best of our knowledge, sample complexity lower bound thereof (Section 5). We introduce an algorithm, called RegORL, that learns \(\)-optimal policies for any RDP, in the episodic setting. At the core of RegORL, there is a component called AdaCT-H, which is a variant of AdaCT, carefully tailored to episodic RDPs. AdaCT-H learns a minimal automaton that underlies the unknown RDP without prior knowledge. The output automaton is further used to derive a Markov abstraction of data to be used by any off-the-shelf algorithm for offline RL in episodic MDPs. We present a sample-complexity bound for AdaCT-H to return a minimal underlying automaton with high probability. This bound substantially improves the existing bound for the original AdaCT, and can be of independent interest. In view of the modular design of RegORL, the total sample complexity is controlled by twice that of AdaCT-H (Theorem 6) and that for the incorporated off-the-shelf algorithm. We also present another variant of AdaCT-H, called AdaCT-H-A. In contrast to AdaCT-H that learns a complete RDP, AdaCT-H-A only reconstructs a subset of states that are likely under the behavior policy, in relation to an input accuracy parameter. As such, AdaCT-H-A renders more aligned with the practice of RL than AdaCT-H. Furthermore, we provide a first lower-bound for offline RL in RDPs that involves relevant parameters for the problem, such as the RDP single-policy concentrability, which extends an analogous notion for MDPs from the literature. Finally, if contrasted to both online learning in RDPs and automata learning, our results suggest possible improvements in sample complexity results for both areas.

### Related Work

Offline RL in MDPs.There is a rich and growing literature on offline RL, and provably sample efficient algorithms have been proposed for various settings of MDPs; see, e.g., [11; 12; 13; 14; 15; 16; 17; 18; 19; 20]. For example, in the case of episodic MDPs, it is established that the optimal sample size in offline RL depends on the size of state-space, episode length, as well as some notion of concentrability, reflecting the distribution mismatch between the behavior and optimal policies. A closely related problem is off-policy learning; see, e.g., [21; 22; 23] and the recent survey .

Online RL in RDPs.Several algorithms for _online_ RL in RDPs exist [25; 26; 27] but complexity bounds are only given in  for the infinite-horizon discounted setting. The sample complexity bounds in  are not immediately comparable to ours, due to the different setting. Importantly, the algorithm in  uses the uniform policy for learning, and it therefore might be adapted to our setting only under the assumption that the behaviour policy is uniform. Even in this case, our bounds show an improved dependency on several key quantities. Furthermore, we provide a sample complexity lower bound, whereas their results are limited to showing that a dependency on the quantities occurring in their upper bounds is necessary.

The online RL algorithms in [28; 29; 30; 31] have been developed for formalisms that are closely related to RDPs, and such algorithms can be applied to RDPs. However, these algorithms are not proven to be sample efficient.

POMDPs.Every RDP can be seen as a POMDP whose hidden dynamics evolves according to its finite-state automaton. However, RL in POMDPs is a largely open problem. Even for a known POMDP, computing a near-optimal policy is PSpace-complete . For unknown dynamics, which is the setting considered here, favourable bounds have been obtained for the class of undercomplete POMDPs [6; 7], which does not include all RDPs, or alternatively, under other assumptions such as few-step reachability  or ergodicity . This relationship between RDPs and POMDPs can be also seen from the notion of state. In fact, the automaton state of an RDP is an instance of information state, as defined in , and of belief, as in classic POMDP literature .

PSRs.Predictive State Representations (PSRs) [34; 35; 36; 37] are general descriptions of dynamical systems that capture POMDPs and hence RDPs. There exist polynomial PAC bounds for online RL in PSRs . Nonetheless, these bounds are looser than the one we show here, since they must necessarily consider a wider class of models. Moreover, although a minimum core set for PSRs is similar to a minimal RDP, the bounds feature a number of quantities that are specific to PSRs (e.g., regularity parameter) and do not immediately apply to RDPs.

Other Non-Markovian Settings._Feature MDPs_ and _state representations_ both share the idea of having a map from histories to a state space. This is analogous to the map determined by the transition function of the automaton underlying an RDP. Algorithmic solutions for feature MDPs are based on suffix trees, and they cannot yield optimal performance in our setting [29; 30]. The automaton of an RDP can be seen as providing one kind of state representation [39; 40; 41; 42]. The existing bounds for state representations show a linear dependency on the number of candidate representations, which is exponential in the number of states in our case. A similar dependency is also observed in . RL with _non-Markovian rewards_ is considered in [44; 45; 46; 47; 48; 49]. The idea of a map from histories to states is also found in . Non-Markovianity is also introduced by logical specifications that the agent is required to satisfy [50; 51; 52; 53; 54]; however, it is resolved a priori from the known specification. The convergence properties of Q-learning over a (known) underlying state space such as the one of an RDP are studied in .

Learning PDFA.Our algorithms for learning an RDP borrow and improve over techniques for learning Probabilistic-Deterministic Finite Automata (PDFA) [56; 57; 58; 10; 59; 60]. Our algorithm builds upon the state-of-the-art algorithm AdaCT , and we derive bounds that are a substantial improvement over the ones that would be obtained from a straightforward application of any existing PDFA-learning algorithm to the offline RL setting.

We provide additional literature review in Appendix A.

## 2 Preliminaries and Problem Formulation

Notations.Given a set \(\), \(()\) denotes the set of probability distributions over \(\). For a function \(f:()\), \(f(x,y)\) is the probability of \(y\) given \(x\). Further, we write \(y f(x)\) to abbreviate \(y f(x,)\). For \(y\), we use \(_{y}()\) to denote the Kronecker delta defined as \(_{y}(y)=1\) and \(_{y}(y^{})=0\) for each \(y^{}\) such that \(y^{} y\). Given an event \(E\), \((E)\) denotes the indicator function of \(E\), which equals \(1\) if \(E\) is true, and \(0\) otherwise, e.g. \(_{y}(y^{})=(y=y^{})\). For any integer \(Z\!\!0\), we let \([Z]:=\{0,,Z\}\). Given a set \(\), for \(k\), \(^{k}\) represents the set of sequences of length \(k\) whose elements are from \(\). Also, \(^{*}=_{k=0}^{}^{k}\). The notation \(}()\) hides poly-logarithmic terms.

### Episodic Regular Decision Processes

We first introduce generic episodic decision processes. An episodic decision process is a tuple \(=,,,,,H\), where \(\) is a finite set of observations, \(\) is a finite set of actions, \(\) is a finite set of rewards and \(H 1\) is a finite horizon. As is common in automata theory, we use sequences \(a_{m}r_{m}o_{m} a_{n}r_{n}o_{n}\) to denote traces of actions, rewards and observations, and concatenation \(=\{aro:a,r,o\}\) to denote sets of sequences. Let \(_{t}=()^{t+1}\) be the set of traces of length \(t+1\), and let \(e_{m:n}_{n-m}\) denote a trace from time \(m\) to time \(n\), included. A _trajectory_\(e_{0:T}\) is the full trace generated until time \(T\). We assume that a trajectory \(e_{0:T}\) can be partitioned into _episodes_\(e_{:+H}_{H}\) of length \(H+1\), and that the dynamics at time \(T=k(H+1)+t,t[H]\), are _conditionally independent_ of the previous episodes and all rewards, i.e. the dynamics only depend on \(a_{k(H+1)}o_{k(H+1)} a_{T}o_{T}\). For \(t[H]\), let \(_{t}=()^{t+1}\) denote the relevant part of the trajectory for decision making, and let \(=_{t=0}^{H}_{t}\). We refer to elements in \(\) as _histories_, even though they are not complete trajectories. In each episode \(e_{0:H}\), \(a_{0}=a_{}\) is a dummy action used to initialize the distribution on \(_{0}\). The transition function \(:()\) and the reward function \(:()\) only depend on the history of the current episode. Given \(\), a generic policy is a function \(:()^{*}()\) that maps trajectories to distributions over actions. The value function \(V^{}:[H]\) of a policy \(\) is a mapping that assigns real values to histories. For \(h\), it is defined as \(V^{}(H,h) 0\) and

\[V^{}(t,h)[._{i=t+1}^{H}r_{i}|h, .],\ \  t<H, h_{t}..\] (1)

For brevity, we write \(V^{}_{t}(h) V^{}(t,h)\). The optimal value function \(V^{*}\) is defined as \(V^{*}_{t}(h)_{}V^{}_{t}(h), t[H], h _{t}\), where \(\) is taken over all policies \(:()^{*}()\). Any policy achieving \(V^{*}\) is called optimal, which we denote by \(^{*}\); namely \(V^{^{*}}=V^{*}\). Solving \(\) amounts to finding \(^{*}\). In what follows we consider simpler policies of the form \(:()\) mapping histories to distributions over actions. Let \(_{}\) denote the set of such policies. It can be shown that \(_{}\) always contains an optimal policy, i.e. \(V^{*}_{t}(h)_{_{}}V^{}_{t}(h), t [H], h_{t}\). An episodic MDP is an episodic decision process whose dynamics at each timestep \(t\) only depends on the last observation and action .

Episodic RDPs.An episodic Regular Decision Process (RDP)  is an episodic decision process \(=,,,,,H\) described by a _finite transducer_ (Moore machine) \(,,,,,q_{0}\), where \(\) is a finite set of states, \(=\,\) is a finite input alphabet composed of actions and observations, \(\) is a finite output alphabet, \(:\) is a transition function, \(:\) is an output function, and \(q_{0}\) is a fixed initial state . The output space \(=_{}_{}\) consists of a finite set of functions that compute the conditional probabilities of observations and rewards, meaning \(_{}()\) and \(_{}()\). For simplicity, we use two output functions, \(_{}:()\) and \(_{}:()\), to denote the individual conditional probabilities. Let \(^{-1}\) denote the inverse of \(\), i.e. \(^{-1}(q)\,\) is the subset of state-symbol pairs that map to \(q\). In this context, an input symbol is an element of \(\). We use \(A,R,O,Q\) to denote the cardinality of \(,,,\), respectively, and assume \(A 2\).

An RDP \(\) implicitly represents a function \(:\) from histories in \(\) to states in \(\), recursively defined as \((h_{0})(q_{0},a_{0}o_{0})\) and \((h_{t})((h_{t-1}),a_{t}o_{t})\). The dynamics and of \(\) are defined as \((h,a,o)=_{}((h),a,o)\) and \((h,a,r)=_{}((h),a,r)\), \( h, aro\). Episodic RDPs are acyclic, i.e. the states can be partitioned as \(=_{0}_{H+1}\), where each \(_{t+1}\) is the set of states generated by histories in \(_{t}\) for each \(t[H]\). An RDP is minimal if its Moore machine is minimal. Since there is nothing to predict at time \(H+1\), a minimal RDP contains a single state \(q_{H+1}\) in \(_{H+1}\). To ensure that an acyclic RDP \(\) is minimal, we introduce a designated termination observation \(o_{}\) in \(\) and define \((q_{H+1},a_{o})=q_{H+1}\) and \(_{}(q_{H+1},a)=_{o_{}}\) for each \(a\). Hence, \(q_{H+1}\) is absorbing and the states in \(\) implicitly count how many steps are left until we observe \(o_{}\). Without \(o_{}\), a Moore machine could potentially represent all episodes using fewer than \(H+2\) states.

Since the conditional probabilities of observations and rewards are fully determined by the current state-action pair \((q,a)\), an RDP \(\) adheres to the Markov property over its states, but _not over the observations_. Given a state \(q_{t}\) and an action \(a_{t}\), the probability of the next transition is

\[(r_{t},o_{t},q_{t+1} q_{t},a_{t},)=_{}(q_ {t},a_{t},r_{t})\,_{}(q_{t},a_{t},o_{t})\,(q_{t+1}= (q_{t},a_{t}o_{t})).\]Evidently, in the special case where an RDP is Markovian in both observations and rewards, it reduces to an episodic MDP. More precisely, any episodic MDP with actions \(\), states \(\) and horizon \(H\) can be represented by some episodic RDP with states \([H+1]\) and inputs \(\).

An important class of policies for RDPs are the regular policies. Given an RDP \(\), a policy \(:()\) is called _regular_ if \((h_{1})=(h_{2})\) whenever \((h_{1})=(h_{2})\), for all \(h_{1},h_{2}\). Let \(_{}\) denote the set of regular policies for \(\). Regular policies exhibit powerful properties. First, under a regular policy, suffixes have the same probability of being generated for histories that map to the same RDP state. Second, there exists at least one optimal policy that is regular.

**Proposition 1**.: _Consider an RDP \(\), a regular policy \(_{}\) and two histories \(h_{1}\) and \(h_{2}\) in \(_{t}\), \(t[H]\), such that \((h_{1})=(h_{2})\). For each suffix \(e_{t+1:H}_{H-t-1}\), the probability of generating \(e_{t+1:H}\) is the same for \(h_{1}\) and \(h_{2}\), i.e. \((e_{t+1:H} h_{1},,)=(e_{t+1:H} h_{2}, ,)\)._

**Proposition 2**.: _Each RDP \(\) has at least one optimal policy \(^{*}_{}\)._

Due to Proposition 2, when solving an RDP \(\), we can restrict our search to the set of regular policies \(_{}\). A regular policy can be compactly defined as \(:()\), where \((q_{0})=_{a_{}}\) always selects the dummy action \(a_{}\), and its value function as \(V^{}:[H]\).

Next, we define occupancy measures for RDPs. Given a regular policy \(:()\) and \(t[H]\), let \(d_{t}^{}(_{t}\,)\) be the induced probability distribution over states in \(_{t}\) and input symbols in \(\,\), recursively defined as \(d_{0}^{}(q_{0},a_{0}o_{0}):=_{}(q_{0},a_{0},o_{0})\) and

\[d_{t}^{}(q_{t},a_{t}o_{t})_{(q,ao)^{-1}(q_{t})}d_{t-1} ^{}(q,ao)\,(q_{t},a_{t})\,_{}(q_{t},a_{t},o_{t}) t >0.\]

We also overload the notation by writing \(d_{t}^{}(q_{t},a_{t})=_{o}d^{}(q_{t},a_{t}o)\). Of particular interest is the occupancy distribution \(d_{t}^{*} d_{t}^{^{*}}\), associated with an optimal policy \(^{*}\).

**Example 1** (The cookie domain ).: The _cookie domain_ (Figure 1a) has three rooms connected by a hallway. The agent (purple triangle) can move in the four cardinal directions. When pressing a button in the orange room, a cookie randomly appears in either the green or the blue room. The agent receives a reward of \(+1\) for eating the cookie and it may then press the button again. This domain is partially observable since the agent can only see what is in the room that it currently occupies (Figure 1b). The cookie domain can be modelled as an episodic RDP with states \(=[H+1]\), with \(=\{u_{1},u_{2},u_{3},u_{4}\}\). The value of \(\) is \(u_{1}\) when the button has _not_ been pressed yet (or not pressed since the last cookie was eaten). The value is \(u_{2}\) when the button has been pressed, but the agent has not visited the blue or green room yet. In this state, the environment has a \(50\%\) probability of generating the observation of a cookie when the agent enters either room for the first time. If the agent visits the green room and finds no cookie, the value becomes \(u_{3}\), meaning that the cookie is in the blue room. The meaning of \(u_{4}\) is dual to that of \(u_{3}\).

### Offline RL in RDPs

We are now ready to formally present the offline RL problem in episodic RDPs. Assume that we have access to a batch dataset \(\) collected through interacting with an unknown (but fixed) episodic RDP \(\) using a regular _behavior_ policy \(^{}\). We assume that \(\) comprises \(N\) episodes, where the \(k\)-th episode is of the form \(e_{0:H}^{k}=a_{0}^{k}r_{0}^{k}_{0}^{k} a_{H}^{k}r_{H}^{k}_{H}^ {k}\), where \(q_{0}^{k}=q_{0}\) and where, for each \(t[H]\),

\[a_{t}^{k}^{}(q_{t}^{k}), r_{t}^{k}_{ }(q_{t}^{k},a_{t}^{k}), o_{t}^{k}_{}(q_{t}^{k},a_{t}^ {k}), q_{t+1}^{k}=(q_{t}^{k},a_{t}^{k}o_{t}^{k}).\]

Figure 1: The _cookie_ domain: The agent can only see what is in the current room .

The goal is to compute a near-optimal policy \(\) using the dataset \(\) (and without further exploration). More precisely, for a pre-specified accuracy \((0,H]\), we aim to find an \(\)-optimal policy \(\), using the smallest dataset \(\) possible. A policy \(\) is \(\)-optimal iff \(_{h_{0}}[V_{0}^{*}(h_{0})-V_{0}^{}(h_{0})]\), where \(h_{0}=a_{}o_{0}\), for some random \(o_{0}\).

By virtue of Proposition 2, one may expect that it is sufficient to search for regular \(\)-optimal policies, which is indeed the case. In order to learn an \(\)-optimal policy from \(\), some assumption is necessary regarding the policy \(^{}\) that was used to collect the episodes. Let \(d_{t}^{}:=d_{t}^{^{}}\) be the occupancy distribution of \(^{}\). The following assumption requires that the behavior policy assigns a positive probability to all actions, which ensures that \(^{}\) explores the entire RDP.

**Assumption 1**.: \(_{t[H],q_{},a}d_{t}^{}(q,a)>0\)_._

Assumption 1 is only needed by Theorem 6, which reconstructs the full unknown RDP. Theorem 8, instead, relies on a weaker assumption that can be expressed with the coefficient introduced in Definition 1.

The second assumption we require concerns the richness of \(^{}\) and its capability to allow us to distinguish the various RDP states. This is perfectly captured by notions of _distiguishability_ arising in automata theory, such as in Balle et al. . We apply these concepts in our context, where such discrete distributions are generated from an RDP and a policy. Consider a minimal RDP \(\) with states \(=_{t[H+1]}_{t}\). Given some policy \(\), at each timestep \(t[H]\), every RDP state \(q_{t}\) defines a unique probability distribution over the episode suffixes \(_{H-t}=()^{H-t+1}\). Then, the states in each \(_{t}\) can be compared through the probability distributions they induce over \(_{H-t}\). Consider any \(L=\{L_{}\}_{=0}^{H}\), where each \(L_{}\) is a metric over \((_{})\). We define the _\(L\)-distinguishability_ of \(\) and \(\) as the maximum \(_{0}\) such that, for any \(t[H]\) and any two distinct \(q,q^{}_{t}\), the probability distributions over suffix traces \(_{t:H}_{}\) from the two states satisfy

\[L_{H-t}((e_{t:H} q_{t}=q,),(e_{t:H} q_{t}=q^{ },))_{0}\,.\]

We will often omit the remaining length of the episode \(=H-t\) from \(L_{}\) and simply write \(L\). We consider the \(L_{}^{p}\)-distinguishability, instantiating the definition above with the metric \(L_{}^{p}(p_{1},p_{2})=_{u[],e_{u}} p_{1}( e)-p_{2}(e)\), where \(p_{i}(e)\) represents the probability of the trace prefix \(e_{u}\), followed by any trace \(e^{}_{-u-1}\). The \(L_{1}^{p}\)-distinguishability is defined analogously using \(L_{1}^{p}(p_{1},p_{2})=_{u[],e_{u}} p_{1}(e )-p_{2}(e)\). We can now require a positive distinguishability with our second assumption.

**Assumption 2**.: The behavior policy \(^{}\) has \(L_{}^{p}\)-distinguishability of at least \(_{0}>0\).

Finally, in order to capture the mismatch in occupancy measure between the optimal policy and the behavior policy, we introduce a key quantity called _single-policy RDP concentrability coefficient_, which extends the single-policy concentrability coefficient in MDPs to RDPs:

**Definition 1**.: The _single-policy RDP concentrability coefficient_ of an RDP \(\) with episode horizon \(H\) and with respect to a policy \(^{}\) is defined as:

\[C_{}^{}=_{t[H],q_{t},ao }^{}(q,ao)}{d_{t}^{b}(q,ao)}\,.\] (2)

The concentrability coefficient in Definition 1 resembles the notions of concentrability in MDPs (e.g., [14; 15]). It should be stressed, however, that those in MDPs are defined in terms of observation-action pairs \((o,a)\), whereas \(C_{}^{}\) is defined in terms of _hidden_ RDP states and actions-observations, \((q,ao)\). It is worth remarking that \(C_{}^{}\) could be equivalently defined in terms of state-action pairs \((q,a)\). Finally, in the special case where the RDP is Markovian - in which case it coincides with an episodic MDP - we have \([H+1]\) and \(C_{}^{}\) coincides with the standard single-policy concentrability coefficient for MDPs in . This fact is shown in the proof of Corollary 17.

## 3 RegORL: Learning an Episodic RDP

In this section we present an algorithm for learning the transition function of an RDP \(\) from a dataset \(\) of episodes generated by a regular behavior policy \(^{}\). To simplify the presentation, we treat \(\) as a multiset of traces in \(_{H}\). The learning agent only has access to the non-Markovian tracesin \(\), and needs prior knowledge of \(\), \(\) and \(\), but no prior knowledge of \(^{}\) and \(\). Our algorithm is an adaptation of AdaCT  to episodic RDPs, and we thus refer to the algorithm as AdaCT-H.

The intuition behind AdaCT-H is that due to Proposition 1, two histories \(h_{1}\) and \(h_{2}\) should map to the same RDP state if they induce the same probability distribution on suffixes. AdaCT-H starts by adding an initial RDP state \(q_{0}\) to \(_{0}\), whose suffixes are the full traces in \(\) (line 1). The algorithm then iteratively constructs the state sets \(_{1},,_{H+1}\). In each iteration \(t[H]\), AdaCT-H creates a set of candidate states \(_{e,t+1}\) by extending all states in \(_{t}\) with symbols in \(\,\) (line 3). We use \(qao\) to simultaneously refer to a candidate state and its state-symbol prefix \((q,ao)\). We associate each candidate state \(qao\) with a multiset of suffixes \((qao)\), i.e. traces in \(_{H-t-1}\), obtained by selecting all suffixes in \((q)\) that start with action \(a\) and observation \(o\) (line 4).

Next, AdaCT-H finds the candidate state whose suffix multiset has maximum cardinality, and promotes this candidate to \(_{t+1}\) by defining the transition function \(\) accordingly (lines 5-7). The algorithm then iterates over each remaining candidate state \(qao_{e,t+1}\), comparing the distribution on suffixes in \((qao)\) to those of states in \(_{t+1}\) (line 9). If the suffix distribution is different from that of each state in \(_{t+1}\), \(qao\) is promoted to \(_{t+1}\) (line 10), else \(qao\) is merged with a state \(q^{}_{t+1}\) that has a similar suffix distribution (line 11). Finally, AdaCT-H returns the set of RDP states \(\) and the associated transition function \(\). The function TestDistinct compares two multisets \(_{1}\) and \(_{2}\) of traces in \(_{H-t-1}\) using the metric \(L^{}_{}\). For \(i\{1,2\}\) and each trace \(e_{H-t-1}\), let \(_{i}(e)=_{x_{i}}(x=e)/|_{i}|\) be the empirical estimate of \(p_{i}\), as the proportion of elements in \(_{i}\) equal to \(e\). TestDistinct compares \(L^{}_{}(_{1},_{2}) L^{} _{}(_{1},_{2})\) to a confidence threshold.

Markov transformation.We are now ready to connect the RDP learning phase with the MDP learning phase. RDPs do not respect the Markov property over their observations and rewards, if automaton states remain hidden. However, we can use the reconstructed transition function \(\) returned by AdaCT-H, extended over histories as \(:\), to recover the Markov property. In what follows we formalize the notion of Markov transformation and the properties that its outputs satisfy.

**Definition 2**.: Let \(e_{0:H}_{H}\) be an episode collected from an RDP \(\) and a policy \(^{}\) that is regular in \(\). The _Markov transformation_ of \(e_{H}\) with respect to \(\) is the episode constructed as \(a_{0}r_{0}q_{1} a_{H}r_{H}q_{H+1}\), where \(q_{t+1}=(h_{t})\) and \(h_{t}=a_{0}o_{0} a_{t}o_{t}\), \(t[H]\). The Markov transformation of a dataset \(\) is the Markov transformation of all the episodes it contains.

A Markov transformation discards all observations from \(\) and replaces them with RDP states output by \(\). The dataset so constructed can be seen as generated from an MDP, which we define next.

**Definition 3**.: The episodic MDP _associated to_ an episodic RDP \(\) is \(_{}=,,,T,_{ },H\), where \(T(q,a,q^{})=_{o}(q^{}=(q,ao))\, _{}(q,a,o)\) for each \((q,a,q^{})\).

The associated MDP in Definition 3 is the decision process that corresponds to the Markov transformation of Definition 2, i.e. any episode produced with the Markov transformation can be equivalently seen as being generated from the associated MDP, in the sense of the following proposition.

**Proposition 3**.: _Let \(e_{0:H}\) be an episode sampled from an episodic MDP \(\) under a regular policy \(_{}\), with \((h,a)=_{r}((h),a)\). If \(e^{}_{H}\) is the Markov transformation of \(e_{H}\) with respect to \(\), then \((e^{}_{H},)=(e^{}_{H} _{},_{r})\,,\) where \(_{}\) is the MDP associated to \(\)._

Rewards are not affected by the Markov transformation, only observations, implying the following.

**Proposition 4**.: _Let \(_{}\) be a regular policy in \(\) such that \((h,a)=_{r}((h),a)\). Then \([V^{}_{0,}]=[V^{_{r}}_{0,_{ }}]\), where \(V^{}_{0,}\) and \(V^{_{r}}_{0,_{}}\) are the values in the respective decision process, and \([V^{}_{0,}]=[V^{}_{0,_{ }}]\), where expectations are with respect to randomness in \(o_{0}\)._

**Corollary 5**.: _Given \((0,H]\), if \(_{r}:()\) is an \(\)-optimal policy of \(_{}\), the MDP associated to some RDP \(\), then, \((h,a)=_{r}((h),a)\) is \(\)-optimal in \(\)._

In summary, from Proposition 3, if \(_{}\) is the Markov transformation of a dataset \(\) with respect to an RDP \(\), then, \(_{}\) can be seen as being generated from the associated MDP \(_{}\). Hence, any offline RL algorithm for MDPs can be used for learning in \(_{}\). Moreover, according to Corollary 5, any solution for \(_{}\) can be translated via \(\) into a policy for the original RDP, with the same guarantees.

Complete algorithm.The complete procedure is illustrated in Algorithm 1. Initially, the input dataset \(\) is separated in two halves. The first portion is used for learning the transition function of the unknown RDP with AdaCT-H (Section 3). If an upper bound \(\) on \(||\) is available, it can optionally be provided to compute a more appropriate failure parameter for AdaCT-H. If not available, we adopt the upper bound of \(2(AO)^{H}\) states, which is valid for any instance, due to histories having finite length. As we will see in Theorem 6, this would only contribute linearly in \(H\) to the required dataset size. The output function computed by AdaCT-H is then used to compute a Markov transformation of the second phase, as specified in Definition 2. The resulting dataset, now Markovian, can be passed to a generic offline RL algorithm, which we represent with the function OfflineRL\((,,)\). In Appendix D, we instantiate it for a specific state-of-the-art offline RL algorithm.

``` Input: Dataset \(\), accuracy \((0,H]\), failure probability \(0<<1\), (optionally) upper bound \(\) on \(||\) Output: Policy \(:()\)
1\(_{1},_{2}\) separate \(\) into two datasets of the same size
2\(,(_{1},/(AAO))\), where \(=2(AO)^{H}\) if not provided
3\(_{2}^{}_{2}\) with respect to \(\) as in Definition 2
4\(_{m}(_{2}^{}, ,/2)\) return\(:h_{m}((h))\) ```

**Algorithm 1**Full procedure (RegORL)

## 4 Theoretical Guarantees

We now turn to theoretical performance guarantees of RegORL. Our main performance result is a sample complexity bound in Theorem 7, ensuring that, for any accuracy \((0,H]\), RegORL finds an \(\)-optimal policy. We also report a sample complexity bound for AdaCT-H in Theorem 6, and an alternative bound in Theorem 8. In comparison, the sample complexity bound for AdaCT  is

\[}(A^{2}O^{2}H^{5}(1/)}{ ^{2}}\{^{2}},O^{2}A^{2}}{ ^{4}}\}).\]

We achieve a tighter bound by using Bernstein's inequalities and exploiting the finiteness of histories.

**Theorem 6**.: _Consider a dataset \(\) of episodes sampled from an RDP \(\) and a regular policy \(^{}_{}\). With probability \(1-\), the output of AdaCT-H\((,/(2QAO))\) is the transition function of the minimal RDP equivalent to \(\), provided that \(|| N_{}\), where_

\[N_{}^{}\,_{0}} }(}{d_{}^{ }\,_{0}}),\]\(d^{}_{}:=\{d^{}_{t}(q,a) t[H],q_{t},aO,d^{}_{t}(q,a)>0\}\) is the minimal occupancy distribution, and \(_{0}\) is the \(L^{}_{}\)-distinguishability._

The proof appears in Appendix C.2. Theorem 6 tells us that the sample complexity of AdaCT-H, to return a minimal RDP, is inversely proportional to \(_{0}\), the \(L^{}_{}\)-distinguishability of \(\) and \(^{}\), and the minimal occupancy \(d^{}_{}\). Note that \(d^{}_{} 1/(QOA)\). The bound also depends on \(Q\), the number of RDP states, implicitly through \(d^{}_{}\) and explicitly via a logarithmic term. In the absence of prior knowledge of \(Q\), one may use in the argument of Algorithm 1 the worst-case upper bound \(=2(AO)^{H}\). The sample complexity would then have an additional linear term in \(H\), since \(\) is only used in the logarithmic term to set the appropriate value of \(\). However, this will not impact the value of the \(d^{}_{}\) term.

Theorem 6 is a sample complexity guarantee for the first phase of the algorithm, which learns \(\), the structure of the minimal RDP that is equivalent to the underlying RDP. If \(\) is the desired failure probability of the complete algorithm, RegORL executes AdaCT-H so that its success probability is at least \(1-/2\). This means that with the same probability, \(^{}_{2}\) is an MDP dataset with the properties listed in Section 3. As a consequence, provided that OfflineRL is some generic \((,/2)\)-PAC offline RL algorithm for MDPs, the output of RegORL is an \(\)-optimal policy with probability \(1-\).

**Theorem 7**.: _Consider a dataset \(\) of episodes sampled from an RDP \(\) and a regular policy \(^{}_{}\). For any \((0,H]\) and \(0<<1\), if OfflineRL is an \((,/2)\)-PAC offline algorithm for MDPs with sample complexity \(N_{}\), then, the output of RegORL\((,,)\) is an \(\)-optimal policy in \(\), with probability at least \(1-\), provided that \(|| 2\{N_{/2},N_{}\}\)._

As we can see, the sample complexity requirement separates for the two phases. While \(N_{/2}\) is due to the RDP learning component, defined in Theorem 6, the quantity \(N_{}\) completely depends on the offline RL algorithm for MDPs that is adopted. Among other terms, the performance guarantees of offline algorithms can often be characterized through the single-policy concentrability for MDPs \(C^{*}\). However, since states become observations in the associated MDP, due to the properties of Proposition 3, \(C^{*}\) coincides with \(C^{*}_{}\), the RDP single-policy concentrability of Definition 1.

In Appendix D, we demonstrate a specific instantiation of RegORL with an off-the-shelf offline RL algorithm from the literature by Li et al. . This yields the following requirement for \(N_{}\):

\[N_{}QC^{*}_{}} }{}}{^{2}},\]

for a constant \(c>0\).

To eliminate the dependence that Theorem 6 has on \(d^{}_{}\), we develop a variant of AdaCT-H which does not learn a complete RDP. Rather, it only reconstructs a subset of states that are likely under the behavior policy. The algorithm, which we call AdaCT-H-A (with 'A' standing for "approximation"), is defined in Appendix C.3. Theorem 8 is an upper bound on the sample complexity of AdaCT-H-A that takes the accuracy \(\) as input and returns the transition function of an \(/2\)-approximate RDP \(^{}\), whose optimal policy is \(/2\)-optimal for the original RDP \(\). By performing a Markov transformation for \(^{}\) and using an \((/2,/2)\)-PAC offline algorithm for MDPs, we can compute an \(\)-optimal policy for \(\). The total sample complexity can be combined in the same way as in Theorem 7.

**Theorem 8**.: _Consider a dataset \(\) of episodes sampled from an RDP \(\) and a regular policy \(^{}_{}\). With probability \(1-\), the output of AdaCT-H-A, called with \(\), \(/(2QOA)\) and \((0,H]\) in input, is the transition function of an \(/2\)-approximate RDP \(^{}\), provided that \(|| N^{}_{}\), where_

\[N^{}_{}_{^{}},(16 QAO/)}{\,_{0}}}( QAOC^{*}_{^{}}}{\,_{0}}).\]

This theorem does not rely on Assumption 1, because a finite \(C^{*}_{}\) suffices.

## 5 Sample Complexity Lower Bound

The main result of this section is Theorem 9, a sample complexity lower bound for offline RL in RDPs. It shows that the dataset size required by any RL algorithm scales with the relevant parameters.

**Theorem 9**.: _For any \((C_{}^{*},H,,_{0})\) satisfying \(C_{}^{*} 2\), \(H 2\) and \( H_{0}/64\), there exists an RDP with horizon \(H\), \(L_{1}^{}\)-distinguishability \(_{0}\) and a regular behavior policy \(^{}\) with RDP single-policy concentrability \(C_{}^{*}\), such that if \(\) has been generated using \(^{}\) and \(\), and_

\[||(}+}^{*}H^{2 }}{^{2}})\] (3)

_then, for any algorithm \(:\) returning non-Markov deterministic policies, the probability that \(\) is not \(\)-optimal is at least \(1/4\)._

The proof relies on worst-case RDP instances that carefully combine two-armed bandits with noisy parity functions. This last component allows to capture the difficulty of learning in presence of temporal dependencies. Figure 2 shows an RDP in this class. At the beginning of each episode, the observation causes a transition towards either the bandit component (bottom branch) or the noisy parity function (top branches). Acting optimally in the two parity branches requires to predict the output of a parity function, which depends on some unknown binary code (of length \(3\), in the example). The first term in Theorem 9 is due to this component, because the code scales linearly with \(H\), or \(Q\), while the amount of information revealed about the code is controlled by \(_{0}\). The second term is caused by the required optimality in the bandit.

Differently from this lower bound, the parameter \(_{0}\), appearing in the upper bounds of Theorems 6 and 8, is a \(L_{}^{}\)-distinguishability. However, the two are related, since \(L_{1}^{}(q,q^{}) L_{}^{}(q,q^{})\). Intuitively, the \(L_{1}^{}\)-distinguishability accounts for all the information that is available as differences in episode probabilities. The \(L_{}^{}\)-distinguishability, on the other hand, quantifies the maximum the difference in probability associated to specific suffixes. This is the information used by the algorithm and the one appearing in the two upper bounds.

## 6 Conclusion

In this paper we propose an algorithm for Offline RL in episodic Regular Decision Processes. Our algorithm exploits automata learning techniques to reduce the problem of RL in RDPs, in which observations and rewards are non-Markovian, into standard offline RL for MDPs. We provide the first high-probability sample complexity guarantees for this setting, as well as a lower bound that shows how its complexity relates to the parameters that characterize the decision process and the behavior policy. We identify the RDP single-policy concentrability as an analogous quantity to the one used for MDPs in the literature. Our sample complexity upper bound depends on the \(L_{}^{}\)-distinguishability of the behavior policy. As a future work, we plan to investigate if any milder notion of distinguishability also suffices. This is motivated by our lower bound which only involves the \(L_{1}^{}\)-distinguishability over the same policy. Finally, our results have strong implications for online learning in RDPs, which is a relevant setting to be explored.

Figure 2: One episodic RDP instance \(_{101,1}(L,H,,)\), associated to the parity function \(f_{101}\), with code \(101\), and the optimal arm \(a_{1}^{}\). The length is \(L=|101|=3\), the horizon \(H=5\), the noise parameter \(\) and the bandit bonus parameter is \(\). The transition function only depends on the observations, not the actions. The output distributions are: \(u=\{0,1\}\), \(u_{+}=\{+,-\}\), \(v_{}(+)=(1+)/2\), \(v_{}(-)=(1-)/2\). The star denotes any symbol. If the label of a state \(q\) is \(a.d\), then the observation function is \(_{}(q,a)=d\). Refer to Appendix E for details.