# Exploiting Descriptive Completeness Prior

for Cross Modal Hashing with Incomplete Labels

 Haoyang Luo

School of Computer Science and Technology

Harbin Institute of Technology, Shenzhen

Shenzhen, China

luohaoyang.lalutte@gmail.com

&Zheng Zhang

Harbin Institute of Technology, Shenzhen

Peng Cheng Laboratory

Shenzhen, China

darrenzz219@gmail.com

Corresponding author

Corresponding author

###### Abstract

In this paper, we tackle the challenge of generating high-quality hash codes for cross-modal retrieval in the presence of incomplete labels, which creates uncertainty in distinguishing between positive and negative pairs. Vision-language models such as CLIP offer a potential solution by providing generic knowledge for missing label recovery, yet their zero-shot performance remains insufficient. To address this, we propose a novel Prompt Contrastive Recovery approach, **PCRIL**, which progressively identifies promising positive classes from unknown label sets and recursively searches for other relevant labels. Identifying unknowns is nontrivial due to the fixed and long-tailed patterns of positive label sets in training data, which hampers the discovery of new label combinations. Therefore, we consider each subset of positive labels and construct three types of negative prompts through deletion, addition, and replacement for prompt learning. The augmented supervision guides the model to measure the completeness of label sets, thus facilitating the subsequent greedy tree search for label completion. We also address extreme cases of significant unknown labels and lack of negative pairwise supervision by deriving two augmentation strategies: seeking unknown-complementary samples for mixup and random flipping for negative labels. Extensive experiments reveal the vulnerability of current methods and demonstrate the effectiveness of PCRIL, achieving an average 12% mAP improvement to the current SOTA across all datasets. Our code is available at github.com/E-Galois/PCRIL.

## 1 Introduction

Cross-modal hashing (CMH)  addresses the highly demanding cross-modal similarity search in both web search systems and academic domains . By efficiently transforming multi-modal data (images, text, _etc._) into a collection of compact binary codes, CMH maintains high-dimensional cross-modal semantic similarity while significantly minimizing computational and storage costs. Deep cross-modal hashing has achieved remarkable development by leveraging dual-stream networks or semantic encoding branches based on semantic labels . However,due to limited labour resources, fully supervised annotation becomes impractical for large-scale datasets [27; 33]. A realistic compromise would be partial annotation, in which only a subset of classes are explicitly labeled, while others are marked as _unknown_. Although facilitating adaptation to large datasets, this labeling scheme provides significantly reduced semantic supervision. Therefore, learning with partial annotation has become a significant challenge in multi-label learning tasks.

Furthermore, CMH with partial labels inevitably encounters disrupted similarity learning due to pairwise uncertainty [27; 22]. Jointly missing classes cause incomplete labels to not only obscure class knowledge but also eliminate pairwise similarity by introducing _unknown pairs_. Among these, negative pairs constitute the majority and become particularly scarce with the increasing frequency of unknown labels. As shown in Fig. 1, negative pairs are completely removed even with 35% unknown labels in Flickr dataset. Therefore, such pairwise uncertainty can severely impair similarity-preserving hashing. However, existing CMHs desperately rely on clear positive and negative pairs to maximize supervision. It remains crucial yet unresolved how to accurately perceive potential categories and restore similarity learning in the task of CMH to prevent semantically uninformative binary codes.

To address the issues of incompleteness, some multi-label image recognition methods have investigated a feasible solution, _i.e._, leveraging prior knowledge in large vision-language models [19; 18; 15; 24]. Pre-trained with a contrastive loss on massive image-text pairs, CLIP  has shown its capability to align the global representation of visual and textual modalities with loaded prior knowledge (modal correspondence, similarity structures, partial semantics, _etc._). Although effective in many downstream tasks, they are hardly applicable for pairwise similarity recovery in CMH. Original CLIP's ability is empirically ineffective for label recovery via class-sample similarity scores. As revealed in Sec. 4.4, the original CLIP prompt yields an unsatisfactory 68% recovery precision even on the easier Flickr 30% unknown case, introducing substantial noisy classes and even degrading the final performance. Therefore, CLIP label recovery for deep CMH remains under-explored.

In this paper, we seek to overcome the aforementioned deficiencies and propose Prompt Contrastive Recovery for cross-modal hashing with Incomplete labels (PCRIL) by considering the CLIP prior knowledge of _descriptive completeness_, which is the ability to measure the completeness of a text caption for its described image. By selecting anchor class sets, we develop a simple learnable prompt to encode selected anchor class combinations into CLIP text embeddings. Multiple negative variants are constructed via editing operations on the anchor set. A prompt contrastive recovery paradigm then imposes separation gaps between these label subsets. By learning scores conditioned on their sample modalities, instance-aware class perception is enabled. Thereafter, potential labels are recovered via a tree search on the learned scores. To enhance hard incomplete samples, different instances' features and unknown labels are complementarily mixed. We also introduce an adaptive negative masking strategy to deal with negative pair scarcity at high missing rates. The main contributions of our work are as follows:

\(\) We propose a PCRIL framework, which jointly performs semantic recovery and pairwise uncertainty elimination for efficient cross-modal hashing with incomplete labels. To the best of our knowledge, there is no prior study on CLIP-based label recovery strategies in cross-modal hashing.

Figure 1: Incomplete labels can severely damage cross-modal similarity learning by reducing paired samples. For MIRFlickr-25k annotations (left), 35% unknown classes can completely exclude all negative pairs. For MS COCO (right), even 20% labels under-annotated can fundamentally remove the negative relationships.

\(\) A novel recovery architecture is proposed to recover the neglected semantic labels and pairwise similarities. Particularly, a contrastive learning objective between the anchor set and its negative variants learns instance-conditioned matching scores. A tree search process then leverages the learned scores to detect potential classes. Meanwhile, a complementary semantic augmentation and an adaptive negative masking strategy jointly enhance the similarity learning in extreme cases. Thus, PCRIL can fully restore potential labels and pairwise similarity.

\(\) Extensive experiments verify that our PCRIL can consistently outperform state-of-the-art CMH methods across a range of incompleteness levels and different benchmarks. Comprehensive analyses further validate our effective recoverability for incomplete labels.

## 2 Related Work

### Cross-modal Hashing

Cross-modal hashing (CMH) aims to encode different high-dimensional modalities into a common space of compact binary codes where modal similarity is preserved for fast and accurate retrieval. Early machine learning methods, including those by [32; 21; 26; 3], learn common codes from encoded features. Though simple, non-deep methods are restricted by two-step training paradigm and limited in discriminative similarity learning. Jiang _et al._ proposed deep cross-modal hashing (DCMH), introducing a pairwise similarity matrix into deep similarity preservation for the first time. Li _et al._ constructed a label net to project labels into a common space with modal binary codes. To close modality gaps, Gu _et al._ adopted cross-modal feature attentions with an adversarial learning scheme for semantic discriminability and modal consistency. Zhang _et al._ proposes to preserve multi-level knowledge in CMH with a variational information bottleneck. As efficient feature learners, pretrained vision-language models have been leveraged in recent works. Tu _et al._ introduced transformer-based CLIP image encoders with selective hash optimization. Liu _et al._ further investigated multi-granularity cross-modal alignment based on vision-language transformers. These methods rely heavily on the guidance of complete semantic labels for similarity learning, while they fail to handle the practical incompleteness problem of label annotations in large-scale datasets.

### Learning with Incomplete Labels

Learning with incomplete labels involves only partially known classes, which are a compromise resulting from inaccessible exhaustive supervision . Existing studies regarding incomplete labels mainly focus on the image recognition task. Some works seek to recover learnability upon partial labels with modified loss functions. Bucak _et al._ proposed to learn a modified ranking loss to alleviate the effects of false negatives while assuming all unknown labels to be negative. Durand _et al._ proposed to learn only known labels with the prior knowledge of the label ratios. Cole _et al._ proposed to solve the single positive label issue by imposing training constraints on label statistics. Another type of work investigates lost labels directly. Veit _et al._ resorted to learning from human re-annotations to recover original labels. Chu _et al._ proposed a variational generative model to explore data correlations with partial labels. Kim _et al._ proposed to distinguish exceedingly large losses of false negatives and reverse them during training. By adopting vision-language models with rich prior knowledge, Sun _et al._ proposed a prompt-tuning method with separately learned positive and negative prompt parameters. Ding _et al._ further considered constructing a graph-based label structure in vision-language models to enhance image recognition. In cross-modal retrieval, the problem is however unexplored. Some methods  focus on the problem of entirely missing labels. Ni _et al._ and Liu _et al._ proposed shallow CMH methods that re-predict the labels with consistency constraints between instances and labels. However, without a fine-grained measurement of sample-label consistency, their ability for class perception is limited to only distinguish salient ones.

## 3 Proposed Approach

### Problem Definition

We focus on image-text hashing, which is prioritized and fundamental. CMH with incomplete labels learns hash functions on a training set \(=\{(_{i},_{i},_{i})\}_{i=1}^{N}\), where \(_{i}\) and \(_{i}\) are the image and text modalities of the \(i\)-th sample, respectively, and \(_{i}\) is its label vector, which is the \(i\)-th column in the label matrix \(\{1,0,u\}^{C N}\). Here, \(C\) is the number of classes, \(N\) is the number of instances, and \(u\) denotes an unknown value. Hence, a sample class could be positive (1), negative (0), or unknown (\(u\)). A similarity matrix \(\) is derived from \(\), where \(s_{ij}=1\)_iff_\(_{i}\) and \(_{j}\) share at least one positive label. CMH aims to learn hash functions \(H^{t}\) and \(H^{v}\) to encode text and image samples into binary codes \(_{i}^{t}=(H^{t}(_{i}))\{0,1\}^{d}\) and \(_{i}^{v}=(H^{v}(_{i}))\{0,1\}^{d}\), preserving cross-modal similarity in the Hamming space. Incomplete labels can inject uncertainty in both \(\) and \(\). To tackle this, we propose our PCRIL framework in Fig. 2. Herein, a novel learnable CLIP prompt for label sets is designed to recover potential positive labels in Sec. 3.2. Additionally, an unknown-complementary sample augmentation and a negative masking strategy are developed in Sec. 3.3 to deal with hard samples and negative pair scarcity, respectively.

### Prompt Contrastive Recovery

Prompt for Positive Anchors.Prompt finetuning is a promising solution for lightweight model adaptation. The typical prompting method for CLIP decorates the class tokens, either with a predefined prefix  or a vector of learnable tokens [36; 29]. A typical handcrafted prompt template for a single unknown class can be "A photo of a CLS," where CLS is the class name. However, its fixed tokens and single-class scope hinder it from capturing inter-class complexity for label-wise recovery. In pursuit of separating labels of varied informativeness, we propose to encode label sets into CLIP embeddings.

Given a sample \((_{i},_{i},_{i})\) with incomplete labels, the known classes of this instance can be extracted as the positive label subset \(K_{p}^{i}=\{c l_{i}^{c}=1\}\) and the negative one \(K_{n}^{i}=\{c l_{i}^{c}=0\}\). However, directly maximizing agreement between \(K_{p}^{i}\) and modal representations is insufficient to capture its level of completeness. We quantified and ranked the various combinations of labels in Fig. 3. As illustrated, certain dominant combinations can overshadow less frequent cases in the current CMH scheme, obscuring their impact on model fitting. This long-tail distribution of \(K_{p}^{i}\) can induce bias and disable balanced learning. Furthermore, the number of label sets available is constrained by the samples in the training dataset, which further restricts the learned correspondence. Therefore, we propose to measure semantic completeness through a contrastive learning paradigm instead, in which we consider an anchor set \(K_{a}^{i} K_{p}^{i}\) as

Figure 3: The sorted frequency histogram of unique positive _label sets_ in MIRFlickr-25k samples at 70% known labels. This long-tail distribution induces bias for learning because many rare label combinations in the dataset are associated with limited samples.

Figure 2: Our proposed PCRIL consists of two major stages: prompt contrastive recovery and augmented pairwise similarity learning. The prompt contrastive recovery stage effectively perceives incompleteness via label prompts to learn contrastive matching scores with modal samples, recovering informative semantics. The similarity augmentations further eliminate unknown labels through a complementary blending of samples and recover the scarce negative pairs using an adaptive negative masking strategy.

positive instance and encode it into CLIP embeddings. For instance, a typical handcrafted template can be "A photo of some seagulls flying above the beach." for \(K_{a}^{i}=\) {seagull, beach} and \(K_{p}^{i}\) = {seagull, beach, sky}. It is observed that class names are often surrounded by class-related prefixes and suffixes. Therefore, we further construct a learnable prompt template. The proposed prompting operation is formulated as:

\[(K_{a}^{i}) =(_{head},(\{^{c}\}_{c K_{a}^{i}}),_{ tail})\] \[^{c} =(_{1}^{c},_{2}^{c},...,_{m}^{c},^ {c},_{1}^{c},_{2}^{c},...,_{n}^{c}),\] (1)

where \(^{c}\) is the specialized learnable prompt for the \(c\)-th class, \(_{head}^{m_{a} d_{f}}\) and \(_{tail}^{n_{a} d_{f}}\) are the learnable class-agnostic prefix and suffix, and \(()\) represents a random permutation operation. A specific example of the constructed prompt is given in Suppl. Sec. A.1.

**Negative Subsets and Contrastive Learning.** For the selected anchor label set, dropping any positive class object would degrade its alignment with modalities. The same is true for adding negative classes. Therefore, we aim to detect potential positive labels among the unknown ones by constructing negative subsets relative to the anchor set. Three types of negative subsets are constructed by 1) deleting a class: \(K_{d}^{i,s}=K_{a}^{i}-\{s\}\), 2) joining a negative class in \(K_{n}^{i}\): \(K_{j}^{i,t}=K_{a}^{i}\{t\}\), or 3) replacing a class by a negative one in \(K_{n}^{i}\): \(K_{r}^{i,s,t}=K_{a}^{i}-\{s\}\{t\}\), where \(s K_{a}^{i}\) and \(t K_{n}^{i}\). Note that generating a negative set by replacement is equivalent to successively performing deletion and joining. This variant is introduced to improve model robustness.

To learn a completeness measurement, we define a simple matching score that is compatible with the CLIP prior:

\[^{i}(K)=E_{t}(P(K))^{}_{i}/,\] (2)

where \(K\) is a label set, \(E_{t}()\) and \(\) are the CLIP textual encoder and its temperature parameter, \(_{i}\) represents the average of modal CLIP features \(E_{v}(_{i})\) and \(E_{t}(_{i})\), which is analyzed in Sec. 4.4. The contrastive loss between a positive anchor set and a negative variant is formulated as

\[^{i}(K_{a},K_{*})=max(^{i}(K_{*})-^{i}(K_{a})+m,0),\] (3)

where \(K_{*}\) is a negative variant of anchor set \(K_{a}\) and \(m\) is a margin parameter that separates sets of different completeness levels. The overall contrastive loss is derived as

\[^{ctr}=_{i=1}^{N}_{K_{a}^{i} K_{p}^{i}}(_{s K _{a}^{i}}^{i}(K_{a}^{i},K_{d}^{i,s})+_{t K_{n}^{i}}^{i}(K_{a}^{i},K_{j}^{i,t})+_{s,t}^{i}(K_{a}^{i},K_{r}^{i,s,t})),\] (4)

Notably, the only parameter introduced for contrastive learning is the learnable prompts.

**Potential Label Tree Search (PLTS).** Through contrastive learning between label sets, knowledge in the scores is supposed to generalize as a hierarchical measurement to distinguish potential positive classes in the unknown set \(K_{u}^{i}\), _i.e._, an anchor (positive) set can be a negative set for another larger anchor set. Therefore, a simple greedy search is designed to recover potential positive labels. The label search starts with the entire positive set \(K_{p}^{i}\) as a node with score \(^{i}(K_{p}^{i})\). At each step, we search for an unknown class that maximizes the class set score, and merge it into the positive set. Specifically, we denote the positive and unknown label sets before the \(\)-th iteration as \(K_{p}^{i}()\) and \(K_{u}^{i}()\), where \(K_{p}^{i}(1)=K_{p}^{i}\) and \(K_{u}^{i}(1)=K_{u}^{i}\). At the \(\)-th iteration, we try adding each \(c_{u} K_{u}^{i}()\) into \(K_{p}^{i}()\) and find the one that maximizes the score. This process is formulated as

\[c_{u}^{*}=*{arg\,max}_{c_{u} K_{u}^{i}()}^{i}(K_{p}^ {i}()\{c_{u}\})\] (5)

We then transfer the optimal \(c_{u}^{*}\) into the positive label set. \(K_{p}^{i}()\) and \(K_{u}^{i}()\) are updated as \(K_{p}^{i}(+1)=K_{p}^{i}()\{c_{u}^{*}\}\) and \(K_{u}^{i}(+1)=K_{u}^{i}()-\{c_{u}^{*}\}\). This process continues with the

Figure 4: An example of the potential label tree search process. Discs represent label sets.

updated sets at iteration \(+1\) to further discover potential labels. The termination condition is designed as \(^{i}(K_{p}^{i}(^{*})\{c_{u}^{*}\})<^{i}(K_{p}^{i}(^{*}))+ \), which ensures monotonic increase in the scores. Suppose it ends at \(^{*}\)-th iteration, the search stops with a final score of \(=^{i}(Q)\), where \(Q=K_{p}^{i}(^{*})\) is the final recovered positive set. For a specific example, Fig. 4 demonstrates a typical PLTS process. An optional choice is to further calculate a pseudo label for each of the remaining unknown classes \(c_{u}\) by \(l_{i}^{c_{u}}=H(^{i}(Q\{c_{u}\})-)\), where \(H(x)=(0,(1,+))\) is the hard sigmoid function with a linear window of \(m\).

### Augmentation Strategies for Handling Extreme Cases

Although PLTS can recover substantial positive labels, two issues can remain unresolved in highly incomplete cases. After recovery, a large portion of unknown labels can still exist. Meanwhile, negative pairs can become extremely scarce at high unknown ratios. To enhance the model for these cases, we further impose the following two augmentation strategies.

**Complementary Semantic Augmentation.** As PLTS only recovers positive labels, the unknown values cannot be fully determined, leaving uncertainty in similarity learning. Therefore, we propose to eliminate the residual uncertainty by mixing up complementary samples. However, the symmetrical mixup  augmentation can be ineffective because the complementary relationship is asymmetric, _i.e._, cases that sample \(_{i}\)'s positive set contains an uncertain class of sample \(_{i}\) but _not_ vice versa. Hence, we formulate an asymmetric matrix \(^{N N}\) to express sample matching scores as \(_{i,j}=[_{i}]^{}[_{i}]}{ [_{i}]^{}[_{i}]}\), where \([]\) sets unknown elements to \(1\) and others to \(0\), and \([]\) (\([]\)) sets only positive (negative) entries to \(1\), respectively. \(_{i,j}\) measures the proportion of sample \(i\)'s unknown categories that are positive for sample \(j\). This asymmetrical score evaluates the volume of semantic certainty that sample \(j\) can transfer into the semantic background of sample \(i\). Based on the scores, an asymmetric mix-up is introduced for a sample feature \(_{i}\) and its complementary counterpart \(_{i}^{*}=_{j}\) with label \(_{i}^{*}=_{j}\), where \(j\) is selected randomly from the top-\(K\) indices on the \(i\)-th row of \(\). The complementary mix-up is represented as

\[\{}_{i}=^{v}_{i}+(1-^{v })_{i}^{*}\\ }_{i}=^{v}[_{i}]+(1-^{v}) [_{i}^{*}].,\] (6)

where \(^{v}\) (\(^{t}\)) is a learnable coefficient for the image (text) modality, determining the asymmetry in the complementary mix-up. The formula for the text modality is similar. Through this augmentation, pairwise relationship \(s_{ij}\) is adjusted as deterministic values by \(s_{ij}=1-_{c=1}^{C}(1-_{i}^{c}_{j}^{c})\).

**Adaptive Negative Masking.** As illustrated in Fig. 1, negative pairs can disappear drastically at high unknown ratios. The existence of negative pairs is especially vulnerable yet significant in learning robust cross-modal relationships. Naive solutions such as AN (Assume Negative)  set all unknown values to \(0\), introducing excessive noisy pairs that can mix up with the true negative pairs and inhibit the model fitting. Therefore, we propose adaptive negative masking to restore a correct number of negative pairs. Through semantic augmentation, a real-valued similarity matrix \(^{D}(\{u\})^{D D}\) is constructed between unknown pairs with batch size \(D\). By denoting \(=[^{D}]\) and the ratio of \(S_{ij}^{D}=0\) to \(S_{ij}^{D}>0\) as \(r=|[^{D}]|/|[^{D}]|\), the similarity matrix is adjusted adaptively as

\[^{D*}=\{^{D}&r t\\ (1-)^{D}+(^{D})&r<t .,\] (7)

where \(\) denotes the Hadamard product, \(\{0,1\}^{D D}\) is a random mask that sets unknown values to \(0\) for a number that resets \(r=t\), and \(t\) is a threshold close to \(0\) that prevents noisy similarity structure while slightly enabling AN when there are few negatives. To learn hash codes with these augmentation strategies, the overall hash optimization is defined in Appendix Sec. A.2

## 4 Experiments

### Experiment Settings

We evaluate our method on the MIRFlickr-25k (Flickr) , MS COCO (COCO) , and NUS-WIDE (NUS)  datasets. We use the frozen ViT-B-32 CLIP as backbones for all methods and set the main hash bit as 32. Details regarding datasets, implementation, evaluation settings, and results for other bit configurations are presented in Appendix Sec. B and Sec. C.

### CMH with Incomplete Labels

To validate the model's effectiveness on CMH with incomplete labels, we illustrate mAP comparison results in Table 1. Our model consistently achieves superior results across various known ratios on all benchmarks. We clarify that there is no prior deep CMH method specifically designed for incomplete labels. Therefore, we have to use regular CMH baselines by setting different missing ratios but keeping other settings equivalent. Compared to shallow methods, our method achieves higher results,

    &  &  &  & Mean \\   & & i\(\) t & t\(\) i & Mean & i\(\) t & t\(\) i & Mean & i\(\) t & t\(\) i & Mean & \\   & DGCH  & 69.8 & 65.9 & 67.8 & 75.7 & 70.2 & 72.9 & 77.5 & 72.1 & 74.8 & 71.9 \\  & SDCHM  & 64.3 & 67.2 & 65.8 & 66.0 & 73.9 & 70.0 & 69.5 & 76.0 & 72.8 & 69.5 \\  & SCRATCH  & 75.8 & 68.7 & 72.2 & 82.1 & 74.6 & 78.3 & 85.0 & 77.8 & 81.4 & 77.3 \\  & WVlslab  & - & - & - & - & - & 62.5 & 62.6 & 62.6 & - \\  & DCHH  & 63.0 & 65.2 & 64.1 & 67.4 & 70.2 & 68.8 & 71.3 & 74.5 & 72.9 & 68.6 \\  & SSAH  & 58.8 & 67.6 & 63.2 & 69.2 & 73.3 & 71.3 & 75.3 & 77.4 & 76.4 & 70.3 \\  & AGH  & 59.8 & 63.4 & 61.6 & 78.4 & 76.6 & 77.5 & 84.1 & 79.2 & 81.6 & 73.6 \\  & DCHM  & 64.1 & 64.0 & 64.0 & 78.3 & 75.6 & 76.9 & 81.0 & 80.0 & 80.5 & 73.8 \\  & DCHM  & **78.5(2)** & **77.54(6)** & **77.0(4)** & **85.4(3)** & **79.4(2)** & **82.4(4)** & 87.5(2)** & **82.2(2)** & **83.4(3)** & **83.9(3)** & **81.4(4)** \\   & DGCH  & 65.1 & 66.1 & 65.6 & 65.2 & 66.9 & 66.0 & 67.1 & 68.2 & 67.6 & 66.4 \\  & SDCHM  & 55.7 & 59.9 & 57.8 & 58.9 & 61.2 & 60.0 & 59.3 & 62.2 & 60.7 & 59.5 \\  & SCRATCH  & 35.5 & 64.1 & 49.8 & 28.9 & 67.4 & 48.2 & 32.6 & 68.9 & 50.7 & 49.6 \\  & DCHM  & 29.5 & 31.3 & 30.4 & 32.4 & 33.4 & 32.9 & 36.3 & 35.5 & 35.9 & 33.1 \\  & SSAH  & 35.9 & 45.3 & 40.6 & 38.4 & 57.1 & 47.8 & 46.7 & 64.0 & 55.3 & 47.9 \\  & AGH  & 46.7 & 49.7 & 48.2 & 58.8 & 49.9 & 54.4 & 66.7 & 67.2 & 66.9 & 56.5 \\  & DCHM  & 35.7 & 35.0 & 35.4 & 57.6 & 55.9 & 56.7 & 67.3 & 67.4 & 67.4 & 53.1 \\  & CRRLN  & **67.2(1)** & **76.1(4)** & **68.7(2)** & **68.9(3)** & **77.0(4)** & **69.7(3)** & **77.3(4)** & **72.3(4)** & **74.1(4)** & **73.6(3)** & **69.9(3)** \\   & DGCH  & 60.9 & 61.1 & 61.0 & 63.0 & 63.4 & 63.2 & 64.2 & 64.9 & 64.5 & 62.9 \\  & SDCHM  & 53.7 & 55.5 & 54.6 & 57.3 & 56.9 & 57.1 & 58.5 & 58.7 & 58.6 & 56.8 \\  & SCRATCH  & 33.5 & 59.1 & 46.3 & 34.6 & 60.9 & 47.8 & 32.6 & 63.4 & 48.0 & 47.4 \\  & DCMM  & 49.2 & 47.0 & 48.1 & 52.3 & 53.1 & 52.7 & 52.9 & 53.1 & 53.0 & 51.3 \\  & SSAH  & 32.0 & 40.4 & 36.2 & 31.1 & 50.5 & 40.8 & 36.7 & 55.6 & 46.1 & 41.0 \\  & AGH  & 54.2 & 56.1 & 55.1 & 58.5 & 58.6 & 61.2 & 62.4 & 61.8 & 58.5 \\  & DCHMT  & 44.8 & 44.3 & 44.5 & 52.1 & 49.5 & 50.8 & 62.0 & 61.5 & 61.8 & 52.4 \\  & PCRRLN  & **62.8(1)** & **63.5(2)** & **63.2(2)** & **64.0(1)** & **64.7(2)** & **64.4(1)** & **67.8(3)** & **68.3(1)** & **68.3(1)** & **68.3(1)** \\   

Table 1: The mAP comparisons on Flickr, NUS, and COCO datasets with state-of-the-art CMH methods by different known ratios (30%, 50%, and 70%). We report performance rises in **red** compared to the second-best results. *: cited results with their original experiment settings. Our proposed PCRIL significantly outperforms both deep and non-deep CMH methods, verifying the ability to recover efficient similarity learning.

    &  &  & COCO \\   & 30\% known & 50\% known & 70\% known & 30\% known & 50\% known & 50\% known & 70\% known \\  B w/ LN  & 57.5 & 73.4 & 82.8 & 62.4 & 63.3 & 67.5 & 49.6 & 50.4 & 45.9 \\ B w/ AN  & 68.9 & 76.6 & 81.5 & 51.1 & 53.8 & 66.2 & 45.8 & 54.3 &especially on Flickr, with a \(4.4\)% improvement on average, validating the representation ability of our method. In comparison with deep CMH methods, our model enjoys greatly improved results on all three datasets, with a mean \(8.96\)% mAP enhancement. Some deep CMH methods perform worse with incomplete labels especially on the NUS dataset, indicating their collapsed similarity learning. In comparison with DCHMT, which also adopts CLIPs as backbones, the proposed PCRIL obtains an average of \(12.13\)% mAP improvement on all datasets. These results demonstrate that our model can consistently obtain superior results across unknown ratios and benchmarks, validating the effectiveness of our PCRIL in label recovery and similarity calibration for CMH.

### Ablation Study

To verify the validity of each module, we conduct an ablation study of the proposed method by comparing it with 4 variants shown in Table 2 on the Flickr, NUS, and COCO datasets, and 4 module variants exclusively on the conventional AN setting. In contrast with the two conventional baselines (IU and AN), our ANM-enhanced baseline surpasses them by \(6.66\)% and \(6.09\)% mAP, respectively. This verifies the effectiveness of the balanced pairing strategy. By adding PCR, the model's performance is significantly improved by an average of \(3.52\)%, validating its ability to detect incompleteness and recover labels. Although initial recovery by ANM and PCR is substantial, by adding CSA, the model further gains consistent improvements of \(0.60\)% on average across all datasets, verifying its ability of pairwise similarity augmentation. The two modules together make a joint improvement of \(4.12\)% on average, while the entire PCRIL rescues more than \(10\)% of mAP from unknown labels. In ablation studies within the AN setting (Table 3), our proposed components consistently deliver stable improvements, highlighting PCRIL's ability to overcome limitations inherent in traditional training schemes. These results have comprehensively verified the effectiveness of PCRIL for CMH with incomplete labels.

### Model Analysis

**Prompt Construction.** We analyze our proposed label prompt learning by comparing it with two variants: 1) **phrasal**: a handcrafted prompt in CLIP's original template of "A photo of \(^{c_{1}}\), \(^{c_{2}}\),..., and \(^{c_{n}}\)." is constructed to directly perform recovery search, and 2) **conventional**: averaging single-class prompt embeddings to acquire the label prompt embeddings. The recovery results are shown in Table 4. Compared with handcrafted prompts, our learnable prompts achieve a \(6.1\)% mAP enhancement and improve the restoration precision by \(22.33\)%, verifying the effectiveness of

    &  &  &  \\   & Learnable & Multi-label & 30\% known & 50\% known & 70\% known & Mean & 30\% known & 50\% known & 70\% known & Mean \\   & & ✓ & 75.0 & 76.9 & 74.0 & 75.3 & 66.5 & 68.2 & 68.3 & 67.3 \\ Conventional & ✓ & & 76.3 & 81.8 & 82.8 & 80.3 & 86.0 & 89.6 & 87.0 & 87.5 \\ Ours & ✓ & ✓ & **77.0** & **82.4** & **84.9** & **81.4** & **87.4** & **89.6** & **92.0** & **89.7** \\   

Table 4: Prompt construction variants compared on Flickr dataset. The mAP and precisions of recovered positive labels (Precision) are reported. Our PCRIL can successfully marry multi-label information with CLIP prior knowledge (compared to Conventional) and yield learned prompts for instance-label matching (compared to Phrasal).

    &  &  &  \\   & & 30\% known & 50\% known & 70\% known & Mean & 30\% known & 50\% known & 70\% known & Mean \\   & By image & **78.2** & 79.2 & **85.3** & 80.9 & 86.2 & 86.6 & 88.1 & 87.0 \\  & By text & 74.3 & 76.6 & 84.4 & 78.4 & 70.1 & 80.2 & 77.2 & 75.8 \\  & One-step all & 64.6 & 77.4 & 82.9 & 75.0 & 21.0 & 38.4 & 54.6 & 38.0 \\   & Ours & 77.0 & **82.4** & 84.9 & **81.4** & **87.4** & **89.6** & **92.0** & **89.7** \\   & By image & 51.3 & 65.4 & 68.5 & 61.7 & 78.4 & 76.0 & 74.9 & 76.4 \\   & By text & 50.8 & 65.1 & 69.3 & 61.7 & 69.4 & 69.1 & 68.9 & 69.1 \\   & One-step all & 48.4 & 64.9 & 67.5 & 60.2 & 12.1 & 23.7 & 27.1 & 21.0 \\   & Ours & **68.7** & **69.7** & **71.4** & **69.9** & **79.5** & **78.1** & **80.3** & **79.3** \\   

Table 5: Prompt search variants compared on Flickr and NUS datasets. Compared to single-modal recovery, our proposed PLTS can perform instance-level matching to produce more precise results. The one-step all variant validates the effectiveness of our recursive label recovery in PLTS.

learned prompts. Compared with conventional prompts, our method achieves consistently improved performances, further validating the effectiveness of the structural prior utilized in our method.

**Prompt Search for Recovery.** We validate our prompt recovery with three variants: 1) image: label recovery search with the image modality only, 2) text: label recovery search with the text modality only, and 3) one-step all: directly recovering pseudo-labels on the initial positive label set \(K^{i}_{p}(1)\) without searching, which means \(Q=K^{i}_{p}(1)\). The results are shown in Table 5. Compared with image and text recoveries respectively, our recovery with both modalities displays about \(3\)% and more than \(10\)% improvements. This reveals our model's ability to evaluate the joint completeness in label prompts. Performing "one-step all" recovery, it hardly recalls true positives but still achieves acceptable mAPs, validating the latent hierarchical structure of the matching scores. It can be inferred that the prompt search works by peeking at scores for confident classes that aid completeness the most. Meanwhile, we plot how the number and precision of recovered positives change through the beginning epochs _w.r.t._ modalities in Fig. 6 (a-b). The precision rises with the growth of epochs while recalling more ground-truth labels, validating that the prompt gradually separates more complete and more incomplete semantic descriptors to make true positives emerge. Recovery with a single modality can introduce a drop in precision, where the text modality has especially low results of \(77.16\)% mainly due to its nature of partial reference to labels. However, recovery with both modalities surprisingly boosts the precision to achieve a more effective \(89.78\)%.

**Matching Scores Visualization.** To further analyze the property of descriptive completeness, we illustrate some case images' matching scores during the PLTS process in Fig. 5. In the results, the recovery of a potential positive class is accompanied by an increase in the set score, indicating that the recovered label set is more consistent with the instance. For example, the score increases from the original \(25.38\) to its best \(31.76\) by successively recalling sky, clouds, and sunset for the bottom-left instance. We can observe that class rankings can vary at each step, indicating that the PLTS can generate more precise measurements of multi-label complexity. Meanwhile, though not recovered as positives, the remaining scores (in orange discs) of true positives can still surpass their anchors from the last iteration, validating their effectiveness as pseudo-labels.

Figure 5: The recovered classes and scores of 3 case images _w.r.t._ search iterations. For brevity, we only show top-3 results at all steps. The recursive recovery of potential classes results in successive increases in the set scores.

Figure 6: Recovery of labels and deterministic pairs. The left 2 subgraphs: the (a) recall and (b) precision of recovered positive classes _w.r.t_ initial epochs of prompt tuning. The right 2 subgraphs: pairwise similarity recovery by (c) complementary semantic augmentation and (d) prompt contrastive recovery respectively, on the Flickr and NUS datasets. ‘p.’, ‘n.’, and ‘u.’ stands for positive, negative, and unknown, respectively. Dashed lines are corresponding results w/o applying our modules.

**Enhancement of Pairwise Similarity.** As shown in Fig. 1, CMH can suffer from severe pairwise uncertainty. By adopting our proposed prompt contrastive recovery and complementary semantic augmentation, respectively, the similarity structure is efficiently recovered in Fig. 6 (c-d). The complementary semantic augmentation greatly alleviates the pairwise scarcity. For situations at approximately \(50\)% unknown where negative pairs are completely lost, a substantial \(20\)% proportion of negative pairs are reconstructed, ensuring its ability to rescue similarity learning at high unknown ratios. Meanwhile, the prompt contrastive recovery significantly improves the distribution of both positive and negative pairs via its pseudo-labeling, reducing an average of \(80\)% uncertain pairs, verifying its strong separability for completeness of labels. Generally, these two modules ensure the method can learn CMH on sufficient data pairs with clear relationships.

## 5 Conclusion

In this paper, we identified the problem of incomplete labels and the consequent collapse of similarity learning in CMH. To overcome these challenges, we introduced Prompt Contrastive Recovery for CMH with Incomplete Labels (PCRIL), a novel framework that jointly recovers semantic classes and pairwise similarity. This is the first CMH method to enable prompt learning with incomplete labels. Specifically, by constructing contrastive learning with a hierarchical label prompting method, prompt contrastive recovery learns the completeness of label descriptors and detects lost labels. Moreover, complementary semantic augmentation eliminates the sparsity of semantic pairs via a complementary feature blending strategy to restore similarity. An adaptive negative masking strategy is adopted to further balance the pairwise hashing. Extensive experiments on widely used benchmarks validated that PCRIL can significantly outperform state-of-the-art CMH methods with different partial levels.

**Limitations.** This study highlights the effectiveness of the vision-language model prior in perceiving label completeness, specifically for cross-modal retrieval. However, we note that it does have limitations. The prompt construction currently relies on the pretrained CLIP model with a limited number of text tokens, which hinders its ability for richly labeled samples. However, annotation length exceeding the CLIP capacity is extremely rare in practice. Meanwhile, the learning of PCR relies on sufficient multi-labeled samples. Single positive cases  may infrequently exist in real applications. We suspect that allowing unknown labels in the anchor sets could reactivate the proposed paradigm.