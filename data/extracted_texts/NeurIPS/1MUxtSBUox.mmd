# Budgeting Counterfactual for Offline RL

Yao Liu\({}^{1}\), Pratik Chaudhari\({}^{1,2}\), Rasool Fakoor\({}^{1}\)

\({}^{1}\)Amazon Web Services, \({}^{2}\)University of Pennsylvania

{yaoliuai,prtic,fakoor}@amazon.com

###### Abstract

The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvement from taking out-of-distribution actions and the risk of making errors due to extrapolation. Theoretically, we justify our method by the constrained optimality of the fixed point solution to our \(Q\) updating rules. Empirically, we show that the overall performance of our method is better than the state-of-the-art offline RL methods on tasks in the widely-used D4RL benchmarks.

## 1 Introduction

One of the primary hurdles in reinforcement learning (RL), or online RL, is its reliance on interacting with an environment in order to learn . This can be a significant barrier to applying RL to real-world problems, where it may not be feasible or safe to interact with the environment directly . In contrast, batch or offline RL  provides a more suitable framework for effectively learning policies by leveraging previously collected data to learn a policy. Notably, offline RL relies on a fixed but limited dataset comprising previously collected data from an unknown policy(s) and lacks the ability to continue interacting with the environment to gather additional samples.

These limitations in offline RL give rise to various challenges, with one notable issue being the occurrence of extrapolation problems resulting from the scarcity of training data . In order to overcome this challenge, previous approaches to offline RL primarily focus on constraining the gap between the behavioral policy and the learned policy , or limiting the disparity between the state-action values of logged actions in the data and extrapolated actions . Alternatively, other approaches utilize a learned model that avoids making predictions outside the distribution of the dataset . All these offline RL methods involve a sequence of counterfactual reasoning problems within the realm of potential actions, explicitly or implicitly. The question of "what if" we were to choose a different course of action than the behavior policy often arises, leading to extrapolation errors. Hence the difficulty of the problem increases as we plan for a longer horizon and involves more counterfactual decisions. For offline RL algorithms, it is difficult to find a balance between the potential for improvement from taking out-of-distribution actions and the risk of making errors due to extrapolation.

A critical aspect of offline RL which is relatively unexplored pertains to the varying importance of different actions at each step in determining the final outcome. Not all actions are created equal, and some have more influence on the outcome than others. Thus, it is not always essential to consider alternative actions or counterfactuals at every step than the one dictated by the behavior policy. Rather, the emphasis should be placed on the actions that yield the most impact, warranting careful decision-making at those steps. This limits the number of counterfactual decisions we make and assigns them to the most needed states/steps, to increase the return on the "extrapolation investment".

Unlike existing approaches that employ explicit or implicit regularization on either the policy, value functions, or both, we introduce a novel and effective algorithm, called _Budgeting Counterfactual for Offline RL_ (BCOL), that explicitly constrains the level of extrapolation during training. Specifically, our approach leverages dynamic programming to determine when and where extrapolation should occur, with a budget on the deviation from the behavior policy. This enables us to strike a balance between the potential for improvement achieved by exploring out-of-distribution actions and the inherent risks associated with extrapolation errors. Conceptually, such a method is different from regularized offline RL by allowing non-uniform constraints while keeping strict bounds on counterfactual decisions.

**Our contribution.** We propose a novel algorithm, BCOL, making only a few but important counterfactual decisions in offline RL. This idea extends the space of current offline RL methods that extensively focus on regularized methods. We demonstrate that the fixed point resulting from our \(Q\)-value updating rule corresponds to the optimal \(Q\)-value function, under the constraints that the policy deviates from the behavior policy within a budget. We conduct thorough evaluations on a diverse array of tasks from the widely-utilized D4RL benchmarks . In terms of overall performance, our approach exhibited a favorable comparison to existing state-of-the-art methods.

## 2 Problem Settings

We study the RL problem in the context of the infinite horizon, discounted Markov Decision Process (MDP) . An MDP is defined as a tuple \(=<,,r,P_{0},P,>\) where \(\) is a state space, \(\) is an action space, and both of these spaces can be infinite or continuous in nature. \(r:\) is the reward function. \(P_{0}\) is a distribution over \(\) that the initial states are drawn from. \(P:()\) maps a state-action pair to a distribution over state space. \(\) is the discount factor over future reward. The goal in MDP is to maximize the expectation of discounted future reward \(v^{}=_{t=0}^{}^{t}r_{t} a_{t}\). An important function in MDP is the state-action value function, known as the \(Q\) function, which represents the expected future reward, given an initial state or state-action pair. \(Q^{}(s,a):=[_{t=0}^{}^{t}r_{t} s_{0}=s,a _{0}=a]\).

The primary emphasis of this paper is on model-free learning algorithms within the context of offline RL. The goal is to learn a target policy \(\) that maximizes the expected future reward using a fixed dataset \(\). The dataset \(\) consists of transitions \((s,a,r,s^{},a^{})\) where \(s\) can be drawn from any fixed distribution, \(a(|s)\), \(r=r(s,a)\), \(s^{} P(|s,a),a^{}(|s^{})\). Here \(\) denotes an unknown behavior policy that was utilized in the past to collect dataset \(\).

In offline RL, the task of learning an optimal policy poses a greater challenge compared to online RL. This is primarily due to the fact that the target policy can be different from the behavior policy, leading to the introduction of out-of-distribution actions. This challenge is further amplified by the use of function approximation and the problem horizon as the extrapolation errors from fitted \(Q\) function tend to accumulate over time-steps [8; 12]. In particular, it will results in the \(Q(s,a)\) values increasing dramatically for out-of-distribution state and action pairs. Consequently, this results in the learning of a policy that is likely to perform poorly and risky at the deployment time. Many previous works on offline RL aim to address these issues by some form of regularization, which might hurt the flexibility of making certain key decisions at each step (which we will return to later). However, we take a different approach in this paper where we emphasize on cautiousness of making counterfactual decisions and, hence propose a method that only deviates from behavior policy for a few steps.

## 3 Method

As mentioned earlier, prior studies in offline RL mainly address the issue of extrapolation through two types of regularization terms: policy regularization or value regularization. Policy regularization typically involves utilizing a divergence metric between current candidate policy and behavior policy while value regularization is commonly defined as the positive difference between \(Q(s,)\)1 and \(Q(s,a)\), where \(a(|s)\). In these works, the regularization terms were _uniformly_ applied to all samples in the dataset, enforcing a flat penalization on the distance between \((|s)\) and \((|s)\), or flat penalization of overestimating \(Q(s,)\) for all \(s\). The utilization of a uniform regularization approach can present a problem. When strong regularization is enforced, the divergence bound from behavior policy over all states are small, hence the policy may not be able to improve significantly. In such cases, the resulting policy can only be as good as the behavior policy itself. Conversely, due to the fact that trajectory distribution mismatch increase exponentially with respect to the problem horizon , if the bound is not small enough, it causes significant issues such as large extrapolation errors that lead to a risky and poor-performing policy. Therefore, applying regularization uniformly without considering the specific characteristics of each state can pose challenges and limitations in offline RL.

This work emphasizes the importance of making _counterfactual decisions_, i.e., decisions that are different from the decisions that would have been made by the behavior policy. Building upon the notion that not all decision steps carry equal importance, we introduce a novel offline RL algorithm that incorporates only a limited number of counterfactual decisions. In other words, the algorithm follows the behavior policy \(\) most of the time, but it only makes counterfactual decisions (i.e. performing offline RL) in certain states. Thus intuitively it maximizes the potential improvement from counterfactual decisions while keeping the extrapolation error under control2. We refer to this number by the _budget_ of counterfactual, denoted as \(B\). The grid world MDP in Figure 1 shows a simple example of why using a budget of counterfactuals is an effective constraint. Here, the goal is to find the shortest path from the start to the goal state without passing through the fail state, given a dataset of previously recorded trajectories from different and unknown behavior policies. There are two types of trajectories in the dataset, marked as yellow and blue in Figure 1. While yellow trajectories are the most common, the shortest path requires stitching together these two types of trajectories and taking a counterfactual decision (shown by dashed green line). A budget of counterfactuals ensures that the agent only takes the most beneficial counterfactual decision and prevents it from taking unnecessary counterfactual actions.

In order to spend the budget of counterfactual strategically, a dynamic programming algorithm is utilized to plan and asses the potential policy improvement resulting from actions taken outside of the data distribution. Thus, the choice of policy on each step needs to balance between the immediate gain from taking greedy actions in the current step and the benefit from future counterfactual decisions. Naturally, the policy also depends on how many counterfactual decisions it can take before it exceeds the upper bound \(B\). We define the budget of counterfactual decisions at a time step \(t\) as follows:

\[b_{t+1}=b_{t}-\{(|s_{t},b_{t})(|s_{t})\}, b _{0}=B,\] (1)

where \(\) is an indicator function. The policies studied in this paper take the current budget \(b_{t}\) as an input, which \(b_{t}\) is the initial budget \(B\) subtracted by the number of counterfactual steps taken before time step \(t\) as shown in (1). Given that, the goal of our method is to solve the following constrained policy optimization:

\[_{}[_{t=0}^{}^{t}r_{t} s_{0} P_ {0},]\,s.t.\,b_{t} 0,\; t 0,\;\{(s_{t},b_{t},a_{t })\}_{t=0}^{}_{}()\] (2)

\(_{}()\) is the support set of trajectory distribution introduced by the MDP and policy \(\). Without additional statements, later we only consider state-action-budget trajectories in \(_{}()\) and drop this

Figure 1: Grid world example

requirement for the ease of notation. Note that \(b_{t}\) in (2) is a function of the initial budget \(b_{0}=B\), policies, and states before step \(t\). To provide more clarity, we expand the constraint in (2) as follows:

\[_{t=0}^{}\{(|s_{t},b_{t})(|s_{t})\} B\] (3)

In order to develop our offline RL algorithm that maximizes (2), we introduce a new Bellman operator that plans on backup future values as well as the number of counterfactual decisions.

**Definition 1** (Counterfactual-Budgeting Bellman Operator).: \[_{}Q(s,b,a):=r(s,a)+*{ }_{s^{}}[V_{Q}(s^{},b)]\] (4) \[V_{Q}(s^{},b):=\{ \{_{a^{}}Q(s^{},b-1,a^{}),\,_{a^{} }Q(s^{},b,a^{})\}&b>0\\ _{a^{}}Q(s^{},b,a^{})&b=0.\]

The Bellman operator \(_{}\) updates the \(Q\) values by taking the maximum value between two terms. The first term refers to the case where a counterfactual decision is made by maximizing \(Q\) values over all possible actions in the next state. This leads to a decrease in the counterfactual budget. The second term involves following the behavior policy in the next state (similar to SARSA ) while keeping the counterfactual budget intact. By selecting the optimal backup from the two cases, the \(Q\) value for a given budget \(b\) strikes a balance between the benefits of counterfactual decisions (i.e. \(_{a^{}}Q(s^{},b-1,a^{})\)) in the next step and in further future. Since \(b B\), there are at most \(B\) backup steps taking the maximum \(_{a^{}}Q(s^{},b-1,a^{})\) in the backup path. This intuitively upper bounds the amount of extrapolation that the \(Q\) function can take. We can recover the standard Bellman operator on state-action Q functions by considering \(b\) in \(Q(s,b,a)\) and the rule \(-1=\). Since \(b=b-1=\), the first \(\) operator in \(V_{Q}\) always prefers the former term and gives us the standard Bellman operator.

It is straightforward to show the counterfactual-budgeting Bellman operator is a \(\)-contraction. Based on that, we prove the fixed point is the optimal \(Q\) function constrained by a counterfactual budget.

**Theorem 2**.: There exists a unique fixed point of \(_{}\), and it is1

\[Q^{}(s,b,a):=_{}*{}[_{t=0}^{ }^{t}r_{t} s_{0}=s,a_{0}=a,b_{1}=b,],\ s.t.\ b_{t}  0,\  t 1\] (5)

This theorem (proved in the appendix) indicates that the fixed point iteration of \(_{}\) will lead to the optimal value function with the upper bound of counterfactual decisions. Thus it motivates us to minimize a temporal difference (TD) error of the counterfactual-budgeting Bellman operator, towards the goal of only improving behavior policy over a limited number of but important decision steps.

### Algorithm

In this section, we derive a practical offline algorithm for our approach when the counterfactual-budgeting Bellman operator (4) is estimated via function approximation. First, we need to replace the expectations in (4) by one-sample estimation to evaluate the Bellman backup with the fixed dataset \(\). Next, considering that the action space is continuous, we approximate the \(_{a}\) operator in (4) by maximizing over actions sampled from a policy network that is trained to maximize the current \(Q\) function. Note that these design choices are commonly employed in previous offline RL algorithms [8; 25; 26; 12]. As a result, we obtain a sampled-based counterfactual-budgeting Bellman operator denoted as \(}_{}\).

**Definition 3** (Approximate Counterfactual-Budgeting Bellman Operator).: \[(s,a),\,}_{}Q_{}(s,b,a):=r(s,a)+ \{\{_{(a_{h})_{h=1}^{k}}Q_{ }(s^{},b-1,),\,Q_{}(s^{},b,a^{})\}&b>0 \\ Q_{}(s^{},b,a^{})&b=0.\]where \((s^{},a^{})\) sampled from \(\) and \(\{a_{k}\}_{k=1}^{m}\) are sampled from \(\). Although this operator requires additional input \(b\) to the \(Q\) functions compared with the standard Bellman operator, it does not add any extra data requirement. We can augment the \((s,a,s^{},a^{})\) tuple from an offline dataset with all \(0 b B\) where \(B\) is the maximum number of counterfactual decisions that we consider.

To learn the \(Q\) function, we minimize the least square TD error introduced by \(}_{}\), using dataset \(\) consisting of \((s,a,s^{},a^{})\) sampled from behavior policy \(\). Subsequently, we update the policy by making it more likely to choose actions with higher Q-values. The objective functions for the \(Q\) function and the policy \(\) are formally defined as follows, with \(\) representing the parameters of the \(Q\) function and \(\) representing the parameters of the policy networks:

\[_{Q}(,;,B,) :=\;_{b=0}^{B}}_{(s,a,s^{},a^{ })}[(Q_{}(s,b,a)-}_ {}Q_{}(s,b,a))^{2}]\] (6) \[_{}(;,B,) :=\;-_{b=0}^{B}}_{s,a _{}(|s,b)}Q_{}(s,b,a)\] (7)

where \(\) denotes delayed target network. Algorithm 1 describes the training process of our method1. The counterfactual-budgeting Bellman operator has an interesting property that \(Q\) is monotonically increased with \(b\), for any \(Q\) such that \(Q=}_{}Q\).

\[Q(s,b,a) Q(s,b^{},a), s,a,b>b^{}.\] (8)

The proof is straightforward since \(Q\) with a larger budget maximizes the value over more action sequences. This property will be followed asymptotically. However, with a limited number of iterations, and function approximations, the gap is not always positive. Intuitively, enforcing the monotonic gap might reduce the search space and potentially accelerate the convergence. It is important to note that this modification does not alter the fixed point solution since \(Q^{}\) always has a monotonic gap. Such a regularizer does not enforce any behavior/pessimistic constraint to the policy. It only constrains the Q-values for the same action with different budgets, not the Q-values for different actions. This regularizer will help the dynamics programming over counterfactual actions finding a self-consistent solution.

To implement it, we introduce the following penalty term and add it to the \(_{Q}\). This results in the revised form of \(_{Q}\) as follows:

\[_{Q}(,;,B,)+_{b=0}^{B-1} }_{s,a_{}(|s,b)}[( \{Q_{}(s,b,a)-Q_{}(s,b+1,a),0\})^{2}]\] (9)

This penalty term minimizes the gap for the actions sampled from \(_{}(|s,b)\). As \(_{}(|s,b)\) maximizes \(Q_{}(s,b,)\), which is supposed to be smaller than \(Q_{}(s,b+1,)\), it can be viewed as a more efficient sampler of actions, compared to uniformly sampling, in order to ensure the constraints in Equation 8.

Putting these together, we have Algorithm 1: Budgeting Counterfactual for Offline reinforcement Learning (BCOL). It uses an offline actor-critic style algorithm to learn the \(Q\) function and a policy maximizing the learned Q values. However, as the problem is to find the best policy under the constraints, greedy policy \(_{}\) itself is not enough for inference. The action selection method during inference is implicitly included in the definition of \(_{}\). At test time, the policy needs to look ahead based on the current budget \(b_{t}\) and the \(Q\) values of taking counterfactual actions v.s. taking the behavior policy. Based on that decision, we updated the new budget as well. We define this step as an operator below.

\[(,;s,b,Q):=\{((|s),b)&}_{(s,b)}Q(s,b-1,)}_{(s)}Q(s,b,)\,\,b=0\\ ((|s,b),b-1)&.\]

The complete inference time procedure is described in Algorithm 2. It also takes the learned policy and \(Q\) function parameters as well as an approximate behavior policy \(\). In case of unknown behavior policy, \(\) can be learned from behavior cloning or any other imitation learning algorithm. Algorithm 2 starts with an initial counterfactual budget \(B\), takes action each time according to the condition in Select, and update the budget \(b_{t}\) if the action is not drawn from \(\).

### Comparison to regularized and one-step offline RL

One of the most used methods in offline RL methods is adding policy or value regularization and constraints terms on top of a vanilla off-policy RL algorithm [5; 8; 12; 22; 23; 25; 26; 50; 51], referred to as regularized methods in this paper. Our method can be viewed as an alternative to using regularized losses. Instead, we enforce a non-Markov budget constraint on the policy class, and we argue this provides several unique advantages. First, compared with the coefficients of regularization terms, the budget parameter has a more clear physical explanation in most applications. Thus it is easier to tune the hyper-parameters and explain the decisions from the policy in practice. Second, the constraints on budget will never be violated in the test, it provides an additional level of safety. While regularization terms penalize the divergence during training, they cannot provide a guarantee on the divergence or distance between test policy and behavior policy.

Another type of offline RL that is related to our core idea is one-step RL [3; 49]. We propose and leverage the concept of limiting the number of counterfactual/off-policy steps. However, the "step" here refers to the decision steps, and in one-step RL it refers to the (training) iteration step. Although one-step RL only applies only one training step, the resulting policy can still be far away from the behavior policy in many different states, and give different decisions during the test. From another perspective, our method, although with a limited number (or even one) of counterfactual decision steps, still applies dynamic programming to find the best allocation of counterfactual steps.

### Important implementation details

Algorithm 1 provided a general actor-critic framework to optimize policy value constrained by counterfactual decisions. To implement a practical offline deep RL algorithm, the first design choice is how we model the policy and choose the policy gradient estimator to \(_{}_{}\). Algorithm 1 is compatible with any off-policy policy gradient methods. We implement Algorithm 1 using SAC-style  policy gradient for stochastic policy and TD3-style  policy gradient for deterministic policy, to provide a generic algorithm for both stochastic and deterministic policy models. As TD3 uses deterministic policy, the value of \(m\) is 1 in \(}_{}\) and the actor loss becomes \(Q_{}(s,b,_{}(s,b))\). In both cases, we implement target updates following SAC/TD3. We defer more details to the appendix.

Both SAC and TD3 are originally proposed for online, off-policy RL. To better fit into the offline setting, previous offline RL methods based on these algorithms equip them with several key adaptations or implementation tricks. To eliminate orthogonal factors and focus on the budgeting idea in algorithm comparison, we follow these adaptations and describe them below. For SAC, prior work use twin Q functions , and a linear combination of the two \(Q\) values . Prior work [8; 24] also sample \(m\) actions from the actor and take the maximum \(Q\) values in backup, instead of a standard actor-critic backup, which is a trick firstly introduced in [12; 15]. The entropy term in SAC is dropped in  as this term is mostly for online exploration. Previous TD3-based work  normalizes the state features and Q losses in TD3. All these adaptations are commonly used in state-of-the-art offline RL and are mostly considered as minor implementation details rather than major algorithmic designs. We refer to SAC and TD3 with all these adaptations as offline SAC and offline TD3 in this paper and refer to the two families of algorithms SAC-style and TD3-style.

One-step RL and IQL  for offline RL is built on top of estimating the behavior value \(Q^{}\) by SARSA. Unlike SAC or TD3, it is not applicable to apply the counterfactual budgeting idea on top of SARSA, since it is a native on-policy method and does not consider counterfactual decisions. Thus we focus on the implementation of BCOL with SAC and TD3.

Looping over all budget values in the loss is not efficient in practice. We implement the \(Q\) function and policy with \(B\) output heads and tensorize the loop over \(b\). One can further reduce the computation cost by sampling budget values instead of updating all \(B\) heads over every sample.

## 4 Experiments

We evaluate our BCOL algorithm against prior offline RL methods on the OpenAI gym MuJoCo tasks and AntMaze tasks in the D4RL benchmark . We compare the SAC-style and TD3-style implementation of BCOL with state-of-the-art offline RL algorithms, studying the effectiveness of budgeting counterfactual in offline RL. Our experiment results also reveal that behavior cloning with only one strategic counterfactual decision still work surprisingly well on MuJoCo tasks. Finally, we study the value of our dynamic programming methods on where to spend the counterfactual budget.

**Baselines.** We compared BCOL with regularized methods (policy and/or value regularization) based on SAC and TD3: CQL , CDC , TD3+BC . Besides regularized methods, we also compare with one-step RL method  and IQL  as other state-of-the-art model-free offline RL methods, RAMBO  and ARMOR  as state-of-the-art in model-based offline RL, and behavior cloning, behavior cloning on the top \(10\%\) data, and decision transformer as imitation learning baselines. This covers a set of strong offline RL baselines. We are interested in both studying the effectiveness of counterfactual budget ideas in contrast to the closest regularized methods and state-of-the-art offline RL methods. We present a set of representative and the most performant offline RL baselines in this section, and defers the comparison against more offline RL baselines results to the Appendix due to the limit of space.

**Benchmark.** We report the results of BCOL together with baselines on 9 OpenAI gym MuJoCo tasks and 6 AntMaze tasks in D4RL. The gym MuJoCo tasks consist of the v2 version of medium, medium-reply, and medium-expert datasets in halfcheetah, walker2d, and hopper. MuJoCo tasks generally prefer imitation-type algorithm, as it contains a large portion of near-optimal trajectories. The 6 AntMaze tasks are considered harder tasks for offline RL as they contain very few or no near-optimal trajectories, and many previous methods lack performance reports on these tasks. We exclude the random datasets in MuJoCo tasks as they are not less discriminating and none of the offline RL algorithms learns a meaningful policy there. We exclude expert datasets in MuJoCo tasks since they can be solved simply by behavior cloning and mismatches the consideration in most offline RL algorithm designs.

**How we report the results.** Due to the inconsistency in the version of D4RL in the literature, it is necessary to clarify the source of baseline results here. We retrieve the results of baseline methods from the original papers if applicable (IQL, CQL, one-step RL, DT, RAMBO, ARMOR). We report the Rev. KL Reg variant of one-step RL as it shows the best performance on MuJoCo tasks . For AntMaze tasks we report one-step RL and decision transformer results from the IQL paper as they

  Task Name & BC & 10\%0 & DT & RAMBO & ARMOR & IQL & Onsager & TD3 & BCOOL & CQL & CDC & BCOOL \\  & & & & & & & & & & +BC & (TD3) & & (SNAC) \\  halfcheetah-m & 42.6 & 42.5 & 42.6 & **77.6** & 54.2 & 47.4 & 55.6 & 48.4 & 45.0 & 46.1 & 62.5 & 50.1 \\ hopper-m & 52.9 & 56.9 & 67.6 & 92.8 & **101.4** & 66.3 & 83.3 & 59.4 & 58.6 & 64.9 & 83.2 \\ waller2d-m & 53.3 & 75.0 & 74.0 & 86.9 & **90.7** & 78.3 & 85.6 & 54.5 & 76.7 & 74.5 & 70.7 & **84.1** \\ halfcheetah-m & 36.6 & 40.6 & 36.6 & **68.9** & 50.5 & 44.4 & 2.5 & 44.4 & 40.9 & 45.4 & 52.3 & 46.2 \\ hopper-m & 18.1 & 75.9 & 82.7 & 96.6 & 97.1 & 94.7 & 71.0 & 50.1 & 83.4 & 92.3 & 87.4 & **90.8** \\ waller2d-m & 20.0 & 62.5 & 66.6 & 85.0 & **85.6** & 73.9 & 71.6 & 80.2 & 49.7 & 83.7 & **87.8** & 86.0 \\ halfcheetah-m & 55.2 & 92.9 & 86.8 & **93.7** & 93.5 & 86.7 & 93.5 & 51.5 & 88.7 & 83.3 & 66.3 & **86.9** \\ hopper-m & 52.5 & 110.9 & 107.6 & 83.3 & **103.4** & 91.5 & 102.1 & 100.5 & 109.8 & 109.2 & 83.2 & 99.0 \\ waller2d-m & 107.5 & 109.0 & 108.1 & 68.3 & **112.2** & 109.6 & **110.9** & 110.0 & 108.5 & 109.9 & 103.9 & **110.9** \\  mojoco total & 466.7 & 66.2 & 67.6 & 75.1 & **78.6** & **92.4** & 71.6 & 66.9 & 26.5 & 65.6 & 71.3 & 69.0 & **74.6** \\  antimac-u & 54.6 & 62.8 & 59.2 & 25.0 & - & 87.5 & 64.3 & **90.3** & 93.4 & 94.0 & 93.6 & 90.3 \\ antimac-u & 45.6 & 50.2 & 53.0 & 16.4 & - & 62.2 & 60.7 & 21.7 & 88.0 & 47.3 & 57.3 & **92.0** \\ antimac-m & 0.0 & 5.4 & 0.0 & 0.0 & - & **71.2** & 0.3 & 1.7 & 12.3 & 62.4 & 59.5 & 70.9 \\ antimac-m-d & 0.0 & 9.8 & 0.0 & 0.0 & - & 70.0 & 0.0 & 0.3 & 14.0 & **74.3** & 64.6 & 72.3 \\ antimac-p & 0.0 & 0.0 & 0.0 & 23.2 & - & **39.6** & 0.0 & 0.0 & 0.0 & 34.2 & 33.0 & 35.6 \\ antimac-l & 0.0 & 6.0 & 0.0 & 0.0 & 2.4 & - & **47.5** & 0.0 & 0.3 & 0.0 & 40.7 & 25.3 & 37.6 \\  antimac total & 100.2 & 134.2 & 112.2 & 67.0 & - & 378.0 & 125.3 & 171.3 & 13.8Tare not reported in the original paper. We report the scores of behavior cloning and \(10\%\) behavior cloning. We report the scores of behavior cloning from the IQL paper. We report the scores of CQL from an updated version of CQL paper1 for D4RL v2 environments. We report the score of TD3+BC and CDC from our implementation since the original results are for D4RL v0 environments. The v2 results are generally better than the original v0 scores in the original papers and are consistent with TD3+BC's v2 scores from the IQL paper.

Table 1 shows the results of baseline algorithms as well as two variants of BCOL (TD3-style and SAC-style) on 15 D4RL tasks. For the scores from our experiment (BCOL, CDC, TD3+BC), we report the test episodic reward averaged over 300 test episodes as 10 test episodes per evaluation, the last 10 evaluations, and 3 runs with different random seeds. All algorithms run with 1M steps following the literature. We defer details like learning curves and standard deviations in the appendix.

**Main results.** As Table 1 shows, BCOL-SAC outperforms prior offline RL methods for both MuJoCo and AntMaze total scores. BCOL never falls behind the best methods by a large margin in any task. This indicates the effectiveness and robustness of the counterfactual budget in different scenarios. Especially for the total score of AntMaze, the harder tasks, BCOL-SAC outperforms most prior approaches by a large margin except IQL. We also find for both TD3-style and SAC-style algorithms, BCOL outperform the regularized method based on the same architecture, ablating orthogonal factors other than the key idea about the counterfactual budget.

**Hyper-parameters:**\(B\)**and \(\). Our algorithm only adds two hyper-parameters on top of SAC/TD3: budget \(B\) and \(\). We searched the value of \(B\) in \(\{1,10,50\}\) and the value of \(\) in \(\{0,1,10,100\}\). We select one set of hyper-parameters for MuJoCo (SAC-style: \(B=10,=10\), TD3-style: \(B=50,=10\)) and one set for AntMaze (SAC-style and TD3-style: \(B=50,=0\)) based on the overall performance. This provides a fair comparison with baseline methods as all of them select their hyper-parameters either as this, or per task.2 Figure 2 shows how the total scores changes with different values of \(B\) and \(\). Full results with scores on each task are in the appendix.

Figure 2 also shows another surprising finding from our experiments on the MuJoCo tasks. With the budget being only one \(B=1\), BCOL (SAC) shows comparable performance to some recent offline RL work including CDC and IQL. With 10 steps out of 1000 steps, it is able to outperform most prior methods. This highlights how strategically adding a few important actions on top of behavior policy can provide strong performance in this benchmark. It also indicates that the MuJoCo tasks in D4RL, even the more mixed datasets, considerably prefer the imitation-type approach.

**Ablation on how we realize the counterfactual budgeting constraints.** The idea of budgeting counterfactual decisions naturally requires both the planning on budgeting in Algorithm 1 during the training and the planning on budgeting in Algorithm 2 during the testing. Ablation on the roles of budgeting itself and two planning parts is nonetheless helpful for us to understand how BCOL works. In Figure 3, we report the results of three ablation methods.

The first column shows the performance without budgeting constraints. This leads to offline TD3 or SAC algorithm with all implementation adaptations used by BCOL and others . As we expected, vanilla offline RL without budgeting does not work well. The second column shows the performance with budgeting but without any planning on budgeting during either the training or the testing. This method realizes the budgeting by randomly assigning \(B\) decisions to the policy

Figure 2: Total normalized score with different values of \(B\) and \(\) in BCOL. The left two plots show MuJoCo average scores and the right two plots show AntMaze average scores.

learned by offline SAC/TD3, and the rest to the estimated behavior policy. Randomly stitching the counterfactual policy and behavior policy fails to fully realize the benefit of budgeting, since both training and inference are not budget-aware. The experiment in the third column studies if it is sufficient to be budget-aware during training. This ablation randomly selects \(B\) actions from \(\) trained by BCOL and the rest from the estimated behavior policy. The setting is closest to BCOL, but the lack of planning on how to spend the budget during the testing still hurts the performance. The results on SAC-style and TD3-style implementations are consistent. This ablation shows that the effectiveness of BCOL relies on the collaboration of all three parts of our core idea: budgeting, planning on budgeting during the training, and planning on budgeting during the testing.

## 5 Related Work

Most recently proposed offline reinforcement methods rely on some mechanism to force the learned counterfactual decision policy to stay close to the data support. One approach to this end is regularizing the policy by its divergence with data, either by parameterization [12; 15], constraints and projection [29; 32; 44], divergence regularization [10; 11; 25], or implicit regularization by weighted regression [10; 37; 39; 40; 50]. The other similar idea, which is often applied together, is regularizing the value estimate of policy in an actor-critic or Q learning architecture [5; 7; 8; 22; 26; 28; 33; 34; 51]. Similar regularization can also be realized by the ensemble of \(Q\) functions [1; 2; 14].

Some recent batch RL methods focus on more adaptive policy, similarly to this paper, but conditioning on history  or confidence level . A similar idea to us of making a few key decisions in one trajectory was studied [18; 46], but they focus on stitching the logged trajectories in data.

In contrast to batch RL, imitation learning often does not involve dynamic programming on extrapolated actions. It has been used as a regularization in online RL [31; 38] and offline RL [11; 37]. Recently imitation learning methods using transformers and conditional training also achieves good performance on offline RL benchmarks [4; 56]. One-step RL  can also be viewed as an extension

Figure 3: Percent difference of the performance on different budgeting methods compared with the full BCOL Algorithm (hc = HalfCheetah, hop = Hopper, w = Walker2d, am=AntMaze). The top row shows SAC-based experiments and the bottom row shows TD3-based experiments. TD3 plots do not include AntMaze-large tasks since the performances of BCOL are zero. No budgeting stands for offline SAC/TD3 without the budgeting constraints (equivalent to \(B\)). Budgeting without planning stands for randomly selecting \(B\) steps to follow from \(\) and the rest from \(\) during the test, where \(\) is learned by offline SAC/TD3. Budgeting without test-time planning stands for randomly selecting \(B\) steps (uniformly within the max horizon) to follow from \(\) and the rest from \(\) during the test, where \(\) is learned by Algorithm 1. In all settings, \(B\) is the same value as selected by BCOL.

of imitation learning with one-step policy extraction . Although utilizing the behavior policy as well, Our key idea is different from these as we train a policy with awareness of behavior policy, rather than regularized by behavior cloning or imitation learning loss.

## 6 Discussions

The form of budget considered in this work is counting the different decision distributions. A natural alternative is divergences between \(\) and \(\). This provides a soft count of counterfactual decisions. However, it requires good calibration of both distributions and is more suitable to the discrete-action settings. We leave this as future work. On the theory side, a natural question about the value of budgeting counterfactuals is how the benefit of less counterfactual decisions is reflected in the theoretical properties of offline RL. We leave the investigation on this for future theoretical work.

In conclusion, this paper studies offline reinforcement learning which aims to learn a good counterfactual decision policy from a fixed dataset. We propose a novel idea of budgeting the number of counterfactual decisions and solving the allocation problem by dynamic programming. We provide strong empirical performance over offline RL benchmarks and optimality of the fixed point solution as theoretical justification.