# Clustering then Propagation: Select Better Anchors

for Knowledge Graph Embedding

 Ke Liang\({}^{1}\) Yue Liu\({}^{1}\) Hao Li\({}^{1}\) Lingyuan Meng\({}^{1}\) Suyuan Liu\({}^{1}\)

Sivei Wang\({}^{2}\) Sihang Zhou\({}^{1}\) Xinwang Liu\({}^{1}\)

\({}^{1}\)National University of Defense Technology, Changsha, China

\({}^{2}\)Academy of Military Sciences, Beijing, China

Corresponding author

###### Abstract

Traditional knowledge graph embedding (KGE) models map entities and relations to unique embedding vectors in a shallow lookup manner. As the scale of data becomes larger, this manner will raise unaffordable computational costs. Anchorbased strategies have been treated as effective ways to alleviate such efficiency problems by propagation on representative entities instead of the whole graph. However, most existing anchor-based KGE models select the anchors in a primitive manner, which limits their performance. To this end, we propose a novel anchor-based strategy for KGE, i.e., a relational clustering-based anchor selection strategy (RecPiece), where two characteristics are leveraged, i.e., (1) representative ability of the cluster centroids and (2) descriptive ability of relation types in KGs. Specifically, we first perform clustering over features of factual triplets instead of entities, where cluster number is naturally set as number of relation types since each fact can be characterized by its relation in KGs. Then, representative triplets are selected around the clustering centroids and further mapped into corresponding anchor entities. Extensive experiments on six datasets show that RecPiece achieves higher performances but comparable or even fewer parameters compared to previous anchor-based KGE models, indicating that our model can select better anchors in a more scalable way.

## 1 Introduction

Knowledge graphs (KGs) , such as Freebase , Wikidata , consist of a large number of relational facts, such as Freebase , Wikidata , YAGO  and NELL , consist of a large number of relational facts, which are generally in the format of triplets, _i.e., (head entity, relation, tail entity)_. Each triplet in KGs reveals a specific connection between entities. To leverage such informative knowledge to enhance the capacity of models in different fields  and applications , multiple knowledge graph embedding (KGE) models  have been proposed these years.

However, traditional KGE models usually encode the entities, relations, and factual triplets in KGs shallowly. Assuming the dimension as \(d\), traditional KGE models, such as RotatE , will map these elements in KGs into subspace \(^{N d}\), where \(N\) is the number of the target objects. Such a shallow lookup manner in these traditional KGE models results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs . Thus, as the scale of data becomes larger and larger, top-level GPU or CPU clusters with more memory space are required for these traditional KGE models. For example, about 78M \(\) 200\(d\) entity feature matrix and 58.1 GB GPU RAM  are needed for the best-performing model on PyTorch-BigGraph dataset .

To address such efficiency problems, there are three typical strategies integrated with KGE models, including quantification [37; 53; 71], knowledge distillation [22; 72; 55; 85], and anchor-based sampling [20; 70; 30; 78]. However, the first two types of strategies aim to realize more lightweight models for deployment by compressing the normal-size trained model. In other words, the standard KGE models still need to be trained on large datasets in advance. Compared to them, the anchor-based sampling strategy, raised by , will be more efficient in both training and deployment phrases, since it actually reduces the size of the entity set for propagation, _i.e.,_ from an intact set of all of the entities to an anchor set of some representative entities. Note that propagation means the feature aggregation procedure. Compared to quantification and knowledge distillation, anchor-based sampling can be easily composed with different KGE baselines to optimize the efficiency of these models.

While, most of the existing anchor-based KGE models [20; 70; 30; 78] select the anchors in a primitive manner, _e.g.,_ random selection, and manual selection of different centrality measurement strategies , etc. Thus, the anchor quality cannot be well guaranteed, which limits the performance of the models. In particular, the weights for each strategy on different datasets are usually determined according to grid searching, which is resource-consuming. In addition, considering efficiency will generally bring performance loss compared to the selected shallow KGE backbones. Therefore, reducing performance loss while ensuring good efficiency is also a problem that our work expects to solve. More related works are discussed in Appendix A.2 due to the space limitation.

Our work takes an attempt to design a more reasonable and accurate anchor selection strategy for better-quality knowledge embedding. During the investigation, two important **characteristics** come to our sights and are used in our RecPiece, including: **(1)** representative ability of the cluster centroids and **(2)** descriptive ability of relation types in KGs. Specifically, cluster centroids are proven as the most representative samples within corresponding clusters in various works [67; 52; 26; 46]. Meanwhile, clustering will not introduce too many procedures via unsupervised learning techniques, which is proper to be adopted as the core mechanism for anchor selection in an efficient KGE model. Furthermore, typical clustering algorithms require two inputs, _i.e.,_ clustering features and cluster number. Both of them should be determined according to the characteristics of KGs, thus leading the clustering-based mechanism more suitable for the data type of KGs. As known to all, KGs focus more on relationships between entities compared to other graph types, so each factual triplet in KGs reveals relational knowledge. In addition, triplets can be easily categorized into different clusters according to relation types in KGs. For example, _(Mike, father of, Tom)_ and _(John, father of, James)_ can both be characterized into same type, _i.e.,_ facts to reveal the _"father of"_ relationship. The characteristic shows the descriptive ability of relation types in KGs. Inspired by it, we select features of factual triplets instead of entities as the clustering features, and the number of relation types is set as the cluster number. Note both of the information can be easily obtained as the attributes in any KGs.

To this end, we propose RecPiece, a novel anchor-based KGE model with a relational clustering-based anchor selection strategy. Specifically, we perform clustering over the features of the relational facts instead of entities, where the cluster number is naturally set to the number of relation types since each fact can be characterized by its relation in KGs. Then, the representative triplets are selected around the clustering centroids, which are further mapped into corresponding anchor entities. Extensive experiments are conducted on both link prediction and entity classification among RecPiece, shallow KGE models, and typical anchor-based KGE baseline, _i.e.,_ NodePiece, to demonstrate the promising capacity of our RecPiece from six aspects, _i.e.,_ superiority, effectiveness, scalability, efficiency, transferability, and sensitivity. In summary, the contributions are shown from three aspects below:

* **Problem.** We analyze the limitations of previous anchor-based KGE models, and point out two useful characteristics: (1) representative ability of the cluster centroids and (2) descriptive ability of relation types in KGs, which guide RecPiece to address the limitations.

Figure 1: The problem of shallow knowledge graph embedding method.

* **Method.** We design a novel anchor-based KGE model with a relational clustering-based anchor selection strategy. In particular, we perform clustering on the features of factual triplet into \(||\) (the number of relation types) clusters, which can be easily determined as the attributes of any given KGs, thus leading to a more scalable and explainable efficient KGE model.
* **Experiment.** Extensive experiments show that RecPiece can endow shallow KGE models to have better efficiency but without significant performance losses compared to other anchor-based KGE models on various downstream tasks, indicating that our model can select better anchors in a more scalable way. In particular, RecPiece is not only 84x and 1.2x lightweight than shallow KGE baseline, _i.e.,_ AutoSF, and anchor-based KGE baseline, _i.e.,_ NodePiece, but also makes 9.5% and 4.9% ranking performance improvements on MRR.

## 2 Related Work

This section summarizes the recent related works from three aspects: _i.e.,_ traditional knowledge graph embedding (KGE) model, parameter-efficient model, and anchor-based strategy. Due to the space limitation, please refer to Appendix A.2 for details.

## 3 Method

The methodology of our RecPiece is illustrated in this section. More concretely, we first formulate the task and present the overall framework of RecPiece. Then, we further introduce the modules and procedures within RecPiece in detail, especially for the relational clustering-based anchor selection procedure. At last, we provide a comprehensive discussion on the excellent attributes of RecPiece, which is enlightening for understanding our model. The framework of RecPiece is shown in Fig. 2.

### Preliminary

Task FormulationThe knowledge graph is the directed relational graph, denoted as \(KG=(,,)\), where \(\), \(\) and \(\) represent the set of entities (_i.e.,_ nodes), relations (_i.e.,_ edge types) and fact triplets (_i.e.,_ edges), respectively. Similar to typical anchor-based baseline , RecPiece are more like a plug-and-play auxiliary module, which can be easily applied to any KGE model to reduce the space complexity of the adopted KGE backbone. Moreover, RecPiece is evaluated on different downstream tasks, _i.e.,_ link prediction and entity classification. Note that the focus of this work is not only on those ranking and classification metrics but also the efficiency. In other words, the main goal of our RecPiece is to achieve better performances on different tasks with fewer or comparable parameters compared to the previous anchor-based KGE models.

Knowledge Graph CharacteristicKnowledge graphs (KGs) [35; 25] store the relational facts intuitively. Compared to other graph data, KG focuses more on the relationships between entities, and the knowledge is stored in the factual triplets. Moreover, considering adopting the clustering algorithm for anchor selection, the cluster number can be easily fetched according to the number of relation types. Thus, we selected the features of triplets as the clustering features in this paper.

### Overview Framework

Our RecPiece is a novel anchor-based KGE model with a relational clustering-based anchor selection strategy, which contains five procedures as shown in Fig. 2, including (a) feature preparation, (b) clustering over features of factual triplets, (c.1) candidate triplet selection, (c.2) triplet-entity mapping and (d) feature propagation. As anchors in RecPiece are selected based on the clustering-based mechanism, we need to generate the clustering features and determine the cluster number in advance. Thus, encoder \(p()\) is adopted for triplet feature preparation for clustering in (a). Then, the generated features are clustered into \(||\) clusters via \(g()\) during (b). Later on, we construct the anchor set via two procedures, (c.1) and (c.2). Finally, feature propagation happens on the anchors constrained with different task losses in (d). More details are described as follows, and notations refer to Tab. 1.

  Notation & Explanation \\  \(\),\(\),\(\) & set of entity, relation and triplet \\ \(e\), \(r\), \(t\) & element of entity, relation, fact \\ \(_{i}\) & fact set for relation \(r_{i}\) \\ \(p()\) & pretrained triplet encoder \\ \(g()\) & clustering algorithm \\ \(_{a}()\) & candidate triplet selection mechanism \\ \(_{b}()\) & triplet-entity mapping mechanisms \\ \(f()\) & KGE models for feature propagation \\ \([]\) & quantity number \\ H, h & feature matrix and vector \\ C & cluster centroid set \\ \(e_{i}\) & \(\)\(\)\(\) cluster centroid \\ \(()\) & distance function \\ \(_{i}\) & distance set \\ \(^{*}\), \(_{i}\) & candidate triplet set and subset for cluster i \\ \(\) & candidate anchor set \\ \(\) & triplet distribution based on relations \\ \(k\) & number of anchors (hyper-parameter) \\  

Table 1: Notation summary.

### Feature Preparation

Feature preparation aims to generate the features for clustering. During the procedure, the encoder \(p()\) takes the triplet set \(\) as input and outputs the corresponding feature matrix \(_{}^{|| d}\), where the feature vector \(_{t_{i}}\) of triplet \(t_{i}\) can be easily fetched from the corresponding row as \(_{}[i,:]\).

\[_{}=p()\] (1)

Specifically, \(p()\) contains the following steps: we first generate the embeddings of entities and relations with selected knowledge graph encoders. Then, we traverse all factual triplets in \(\) and get the feature vector \(_{t_{n}}\) for \(n^{th}\) triplet \(t_{n}=(e_{h},r,e_{t})\) by summing up the normalized embeddings \(}\) of entities (\(}_{}}\), \(}_{}}\)) and relation (\(}_{}\)). Finally, the triplet feature matrix \(_{t}\) is generated by concatenating all triplet embeddings together.

\[_{}=_{t_{n}}_{t_{n}}\] (2)

### Relational Clustering-based Anchor Selection

As the core of RecPiece, a novel relational clustering-based anchor selection strategy is designed based on characteristics of both clustering centroids and knowledge graphs. Specifically, the selection strategy can be separated into two parts, _i.e.,_ cluster centroid generation and anchor set construction.

#### 3.4.1 Cluster Centroid Generation

We first perform clustering over the priorly generated triplet feature matrix, _i.e.,_\(_{t}\) into \(||\) clusters, where cluster number is set as the number of the relation types. The adopted clustering algorithm \(g()\) can output the embedding vector set of cluster centroids \(=\{_{c_{1}},_{c_{2}},,_{c_{| |}}\}\).

\[=g(_{t},||)\] (3)

Figure 2: The illustration of our model, which is an anchor-based KGE model, termed RecPiece, by introducing a more explainable and scalable relational clustering-based anchor selection strategy. Note that different factual triplets are coloured in different colours according to relation types, _i.e.,_ blue for \(r_{1}\), orange for \(r_{2}\), and green for \(r_{3}\). \(p()\) and \(f()\) are two encoders for feature preparation and propagation, respectively. Besides, \(g()\) is the adopted clustering algorithm with the cluster number set as the number of relation type \(||\), and anchor set construction contains two steps \(_{a}()\) and \(_{b}()\). Note that the detailed description of the above modules is illustrated in Section 3 and the notations are summarized in Tab. 1.

#### 3.4.2 Anchor Set Construction

The cluster centroids are usually not the specific samples in datasets. Thus, we find representative samples as anchors around them. To achieve the goal, two procedures are designed for anchor set construction, including candidate triplet selection \(_{a}()\) and triplet-entity mapping \(_{b}()\). Concretely, the former procedure aims to select representative triplets for each relation type, while the latter procedure picks up the representative entities based on the triplets to constitute the final anchor set.

Candidate Triplet Selection.The factual triplets with the top \(m_{i}\) closest distance to the cluster centroid \(c_{i}\) are selected as the candidate triplets. Specifically, _cosine(\(\))_ function is used as the distance function \(()\) to measure the distance, which can also be substituted into other functions, such as _euclidean(\(\))_. Then, we can get the distance set \(\) for each cluster:

\[_{i}&=_{t_{n} }(c_{i},t_{n})\\ &=_{t_{n}}_{c _{i}},_{t}[n,:]}{\|_{c_{i}}\|_{2} \|_{t}[n,:]\|_{2}},\] (4)

where \(t_{n}\) denote the \(n^{th}\) factual triplet, and \(_{c_{i}}\) represents the feature vector for centroid \(c_{i}\).

\[^{*}&=_{i[1,| |]}_{i}\\ &=_{i[1,||]}}{m_{i}}\ _{i}\,\] (5)

where the candidate triplet set \(^{*}\) is composed of sets \(\) for different relation types. Triplets, whose embeddings are the top \(m_{i}\) closest to \(_{c_{i}}\), are selected as the candidate triplets via \(\)-\(m_{i}\). Note that \(m_{i}\) is not a hyperparameter, and it can be calculated when given the total anchor number \(k\):

\[m_{i}=k_{i}|}{||},\] (6)

where \(r_{i}\) represents the \(i^{th}\) relation type and \(|_{i}|/||\) is the frequency distribution \(\) of triplets with relation \(r_{i}\), which can be easily obtained based on the attribute of the KGs. Taking the KG in Fig. 2 as an example, the \([|_{1}|,|_{2}|,|_{3}|]=\) and \(r_{3}\), and if \(k=6\), \([m_{1},m_{2},m_{3}]=\).

Triplet-Entity Mapping.The entity anchor set \(\) is constructed from the selected triplet set \(^{*}\) by randomly picking either the head or tail entity as the anchor entity corresponding to each triplet. The \(|||^{*}|\), since we will remove those identical entities.

\[=_{b}(^{*})\] (7)

### Feature Propagation

The hashing and encoding procedures in  are leveraged for feature propagation. Specifically, the features of each entity \(e_{i}\) are first hashed into a _hash(\(e_{i}\))_ using two types of anchor information, _i.e.,_ discrete distances and relational contexts. Then, we leverage MLP as the encoder \(f()\) to bootstrap the feature embeddings of each entity based on the vectorized hashing features. Besides, according to the types of the downstream tasks, _i.e.,_ entity classification and link prediction, various loss functions are adopted for training and optimization. In conclusion, our RecPiece can be integrated with different combinations of KGE backbones and loss functions toward different tasks and scenarios.

### Attributes of RecPiece

In this section, we further discuss some attributes of the proposed RecPiece from various aspects shown below. (1) Random and manual anchor selection in previous anchor-based models is highly dependent on the human experience. Compared to them, ours is more reasonable and learnable according to the representative ability of the cluster centroids. (2) Our RecPiece is developed based on the characteristics of KGs. Specifically, we perform clustering on features of triplets instead of entities since the knowledge units in KG are stored in triplets, which can also be easily characterized based on the relation type. (3) The hyper-parameter, _i.e.,_ cluster number, for clustering algorithms can be determined according to the attributes in KGs in RecPiece as the number of relation types. Thus, our anchor selection only contains one hyper-parameter, _i.e.,_ anchor number, which is inevitable and

[MISSING_PAGE_FAIL:6]

The \(p()\) for feature preparation is selected as pretrained NodePiece  in the first few epochs. Besides, k-means [41; 44] is selected as \(g()\) for clustering, and the cluster number is set as "#Rel." in Tab. 9 for different datasets. For a fair comparison, we set anchor numbers \(k\) for each dataset as the same as , and 2-layer-MLP is adopted as \(f()\) feature propagation. In addition, we replace the default \(p()\), \(g()\) and \(()\) to pretrained GraIL , BitetchingK-means  and euclidean(\(\)) for robustness analysis. Moreover, as for different tasks, RecPiece is integrated with three KGE backbones, _i.e.,_ RotatE , ComPGCN , and AutoSF  to compare with thirteen KGE models, including (1) link prediction: TransE , DisMult , ComplEX , PairRE , RotatE , TripleRE , AutoSF , LRE + PairRE , NodePiece + RotatE , and NodePiece + AutoSF ; (2) entity classification: MLP, ComPGCN , and NodePiece + ComPGCN . More details are present in Appendix.

Evaluation Metrics.For link prediction, both MRR  and Hits@k  are used as the ranking metrics. Besides, ROC-AUC, PRC-AUC, AP, and Hard Accuracy are the evaluation metrics  for entity classification. To quantify the efficiency, we report the parameter number #P (M), memory cost (GB), running time (hours), and _Effi._. Note that _Effi._ is calculated by MRR/#P.

### Main Performance (RQ1)

The performance comparison is carried out between our RecPiece and the existing anchor-based KGE baseline, _i.e.,_ NodePiece, on two typical downstream tasks, _i.e.,_ link prediction and entity classification. It aims to answer **Q1**.

Results ReportTab. 2 shows that RecPiece can achieve better performance on link prediction, _i.e.,_ average 6.5% fewer on parameter number and 3.5% and 2.5% improvements on MRR and Hits@10. In particular, the improvements are apparent on FB15k-237 and CoDEx-L. Even though the performance improvements on WN18RR are the smallest, it is still comparable. According to Tab. 3, RecPiece can also achieve promising results on entity classification with about 14.7% fewer in parameter number and 3.3% and 4.4% boosts on PRC-AUC and Hard ACC, respectively.

DiscussionBased on the above results, we can easily get the answer to **Q1** that our RecPiece can achieve better performances on both link prediction and entity classification with comparable or even fewer parameters compared to the previous anchor-based strategy, NodePiece . It further indicates less performance loss will be caused by RecPiece in a more parameter-efficient manner. Although our RecPiece still raises the performance loss compared to the shallow KGE baselines, _i.e.,_ RotatE and COMPGCN, it is an inevitable trade-off for considering efficiency (over 90% reduction on parameters) via anchor-based strategy, which also occurs on other anchor-based KGE models. Moreover, we notice that different performance improvements are made by our RecPiece in different datasets. It may suggest that our RecPiece can achieve better performance on those denser datasets for link prediction, thus leading to fewer improvements on sparser WN18RR compared to denser FB15k-237.

   Model & MRR & Hits@10 \\  RotatE & 0.338 & 0.533 \\ NodePiece + RotatE & 0.254 & 0.420 \\  RecPiece (KG-self) + RotatE & 0.265 & 0.431 \\ RecPiece (PLM) + RotatE & 0.262 & 0.425 \\   

Table 6: Ablation study for whether pretrained based on language models. “KG-self” and “PLM” represent that the pretrained features are generated on structure information and extra-textual information, respectively.

    & Entity Prediction & Relation Prediction \\  MRR & Hits@10 & MRR & Hits@10 \\  Random & 0.249 & 0.417 & 0.878 & 0.971 \\ NDC & 0.250 & 0.418 & 0.877 & 0.970 \\ PPR & 0.251 & 0.419 & 0.878 & 0.971 \\ NodePiece & 0.254 & 0.420 & 0.881 & 0.970 \\  RecPiece & 0.265 & 0.431 & 0.884 & 0.975 \\   

Table 5: Ablation study for different anchor selection strategies.“EP” and “RP” represent entity prediction and relation prediction. “NDC” and “PPR” are short for Node Degree Centrality and Personalized PageRank.

### Ablation Studies (RQ2)

In this section, different ablation studies are presented to prove the effectiveness of RecPiece. Concretely, we first discuss the effectiveness of our anchor-selection strategy. Then, we further analyze the effectiveness of the important components in each step of RecPiece shown in Section 3.2 and Fig. 2.

#### 4.3.1 Anchor Selection Strategies

The anchors of better quality will definitely contribute to better performances. To prove that our relational clustering-based anchor selection strategy can effectively select better anchors, we compare it with four other strategies, _i.e.,_ Random Selection, Node Degree Centrality, Personalized PageRank, and NodePiece, on FB15k-237 for link prediction, which contains two different settings, _i.e.,_ missing entity prediction and missing relation prediction. Tab. 5 shows that our anchor selection strategy outperforms other strategies in both two settings. In particular, our strategy makes the 4.3% and 2.6% performance boost on MRR and Hits@10 metrics compared to the NodePiece-based model.

#### 4.3.2 Feature Preparation

In this section, we discuss the impact of different pretrain triplet encoders in the first step of RecPiece, _i.e.,_ feature preparation.

We first adopt different structural information encoders \(p()\) in KGs for pretraining. Fig. 3 (a) shows that both GraIL  and default RNP  can both achieve promising performance, but RNP is better than the GraIL model. Besides, we also attempt to leverage the typical pretrained language model (PLM), _i.e.,_ BERT , to prepare the pretrained features over the real textual meaning of different entities and relations. The results are shown in Tab. VII. It indicates that relying on structural information for pretraining on KG link prediction is more promising than extra-textual meaning. It is reasonable that link prediction is indeed a task more related to network structures. However, the results also show the potential capacity of our RecPiece when leveraging the extra information.

In our model, the pretrained procedure is only used for feature preparation, which can be replaced as you want. No matter which feature preparation it is, the key idea of the paper will not be affected. Nevertheless, our RecPiece can all make improvements when leveraging different pretrained encoders.

#### 4.3.3 Clustering over Features

As for the second step of RecPiece, we discuss and analyze the clustering features and the clustering methods.

Clustering FeaturesWe also conduct experiments to verify that it is better to perform clustering on relational triplet features. Table VI shows that although there are still performance boosts when leveraging entity features, there are more apparent improvements in performances with triplet features. Thus, the results prove our idea, _i.e.,_ relational triplet features are more representative than entity features in KGs. Note that we try different cluster numbers for experiments on entity clustering and select the best results of them (with cluster number 10).

Clustering MethodsWe also try different clustering methods for anchor selection, _i.e.,_ KMeans and BisectingKmeans. Fig. 3 (b) shows that both clustering methods can lead to promising performances. It further indicates that our framework is effective with different clustering method choices, which demonstrates the generalizability of our model.

#### 4.3.4 Anchor Selection

As for the anchor selection step, we further analyze the impact of different distance functions to select the anchors that are closer to the clustering centroids. The experiments are carried out on two

Figure 3: Ablation study of different components in RecPiece. (a), (b) and (c) show the impact of the different pretrained triplet encoder \(p()\), clustering algorithm \(g()\), and distance function \(()\) for link prediction task on FB15k-237types of distance function, _i.e.,_ Cosine function and Euclidean function. Fig. 3 (c) demonstrates that RecPiece is robust to different distance functions, where \(()\) is better than \(()\). But no matter which strategies they are, the performances are higher than the performance without distance function. It further indicates the effectiveness and generalizability of our framework.

#### 4.3.5 Discussion

We comprehensively present ablation studies from different aspects. Tracking all of the results of the experiments demonstrates the effectiveness of RecPiece, which is composed of the answer to **Q2**. Since the ablation study on feature propagation is to evaluate the effectiveness of different combinations of backbone KGE models, which is similar to transferability analysis, the detailed discussion of this part is shown in Section 4.6.

### Efficiency Analysis (RQ3)

This section presents and discusses the efficiency of RecPiece from three aspects, parameter efficiency, memory efficiency, and time efficiency. As shown in Tab. 2 and Tab. 3, RecPiece can incredible save about 10x, 8x, 26x, 30x, and 7x parameters on five benchmark datasets, including FB15k-237, WN18RR, CoDEx-L, YAGO 3-10 and WD50K compared to shallow KGE baselines, _i.e.,_ RotatE and COMPGCN. Compared to NodePiece, the results above prove that RecPiece can endow the KGE model to be more parameter-efficiency. Specifically, performance boosts are made on both link prediction and entity classification with comparable or even fewer parameters. It further indicates less performance loss will be caused by RecPiece in a more efficient manner. Although we focus more on parameter efficiency, Fig. 4 shows the recorded real GPU cost and running time are comparable and slightly less. Concretely, it has better efficiency on real memory cost and running time, _i.e.,_ on average 114 MB and 0.89 hours reduction compared to NodePiece. It further indicates that RecPiece also has a good effect on memory efficiency and time efficiency.

In conclusion, the efficiency of our RecPiece is promising (**RQ3**). We also admit that the pre-train paradigm and anchor-based strategies will cost some resources, both in time and memory, which will be optimized in the future. Note that such redundancy is commonly seen in other methods with similar techniques. Considering the performance gains, such a limited cost is also acceptable, which should not influence the effectiveness of RecPiece.

### Scalability Analysis (RQ4)

We compare RecPiece with nine state-of-the-art KGE models on OGB WikiKG 2  to measure its scalability to larger KGs. According to Table 8, we observe that our RecPiece + AutoSF can outperform other KGE models. Specifically, the RecPiece + AutoSF model has only 5.9 M parameters, about 84x smaller than the most efficient shallow models, _i.e.,_ AutoSF. Meanwhile, it is also about 1.2x lighter than the anchor-based KGE baseline, _i.e.,_ NodePiece + AutoSF , with the same quantity of anchors. Meanwhile, our RecPiece + AutoSF can even achieve better ranking performances compared to the KGE model with best performance, _i.e.,_ about 2.2% MRR improvement compared to PairRE + LRE . Moreover, the running time for NodePiece + AutoSF and RecPiece + AutoSF is recorded

  Model & \#Params & MRR \\  TransE (500d) & 1250M & 0.426 \(\)0.003 \\ DisMult (500d) & 1250M & 0.373 \(\)0.005 \\ RotatE (250d) & 1250M & 0.433 \(\)0.002 \\ Complex (250d) & 1250M & 0.503 \(\)0.003 \\ PairRE (200d) & 500M & 0.521 \(\)0.003 \\ PairRE+LRE (700d) & 505M & 0.584 \(\) n/a \\ TripleRE & 501M & 0.579 \(\)0.002 \\ AutoSF & 500M & 0.546 \(\)0.005 \\ NodePiece + AutoSF & 6.9M & 0.570 \(\)0.003 \\  RecPiece + AutoSF & 5.9M & 0.598 \(\)0.003 \\ Improvement & 14.5\% & 2.2\% \\  

Table 8: Link prediction results on OGB WikiKG 2. The best results are marked in Bold.

Figure 4: Memory cost and running time comparison.

as 5.33 and 5.25 hours, which is also comparable. We believe that the above results prove the better scalability of RecPiece with promising parameter reduction and better ranking performance.

### Transferability Analysis (RQ5)

We answer **Q5** in this section. Based on the aforementioned experimental results cover two different downstream tasks, _i.e.,_ entity classification and link prediction, and three different types of shallow KGE backbones, _i.e.,_ RotatE, COMPGCN, and AutoSF, we can reorganize and analyze the results from another view. First of all, all of the experimental results are promising. In particular, there are 5.2% MRR performance improvements made by RecPiece on YAGO 3-10 for link prediction when integrated with RotatE. Secondly, the improvements occur in all the situations when the backbone models are integrated with our RecPiece. Therefore, as the conclusion, it is demonstrated that our RecPiece is proven to be easily extended to different tasks and different KGE models as a plug-and-play auxiliary mechanism, which shows great transferability of our RecPiece.

### Sensitivity Analysis (RQ6)

We measure the sensitivity of RecPiece from two aspects to answer **Q6**, including the analysis on (1) pre-trained epochs for the feature preparation model \(p()\), (2) anchor number \(k\). The experiment results are shown in Fig. 5. In general, our RecPiece is insensitive to the hyperparameters, which demonstrates that our RecPiece can achieve stable performances. More specifically, we can get the following two observations according to the aforementioned two aspects.

1. Fig. 5 (a) and (b) reveals that there usually exists a pretrained epoch threshold \(pe^{*}\) for \(p()\), which indicates that the pretrained features are effective enough to be clustered for anchor selection after \(pe^{*}\), _e.g.,_ around 800 epochs (in 4000 epochs) for entity classification on WD15K (5% labeled) and 150 epochs (in 400 epochs) for link prediction on FB15k-237.
2. Fig. 5 (b) shows that more anchors will benefit the performance as same as NodePiece. Traditional KGE models propagate the features based on the whole graph, which is equivalent to the anchor set composed of all entities. A larger set of anchors is closer to the complete entity set so that less information will be abandoned, thus leading to better performance.

## 5 Conclusion

In this paper, we propose a novel anchor-based KGE model with a relational clustering-based anchor selection strategy, RecPiece, where two characteristics are leveraged, _i.e.,_ (1) representative ability of the cluster centroids and (2) descriptive ability of relation types in KGs. Specifically, we perform clustering over the features of triplets instead of entities into \(||\) (number of relation types) clusters. Then, representative samples are selected around cluster centroids, which are further mapped into corresponding anchor entities. Extensive experiments show that RecPiece can endow shallow KGE models to have fewer parameters without significant performance loss compared to other models, on various tasks, indicating that our model selects better anchors in a more scalable way. In the future, we plan to optimize this preparation procedure via a self-adaptive mechanism along with feature propagation for better practicability and adaptivity in the future.

Figure 5: Sensitive analysis in different settings. (a) and (b) reveals the influence of the models on feature preparation in different epochs (_i.e.,_ |P.E.| represents the pretrained epoch number), where (a) is for entity classification on WD15K (5% labeled), and (b) is for link prediction on FB15k-237. (c) indicates the influence of the total anchor number \(k\). (c) is based on link prediction on FB15k-237.