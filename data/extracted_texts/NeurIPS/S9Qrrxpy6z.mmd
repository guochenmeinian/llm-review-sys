# CiteME: Can Language Models

Accurately Cite Scientific Claims?

Ori Press*

Andreas Hochlehnert*

Ameya Prabhu

Vishaal Udandarao

Ofir Press

Matthias Bethge

###### Abstract

Thousands of new scientific papers are published each month. Such information overload complicates researcher efforts to stay current with the state-of-the-art as well as to verify and correctly attribute claims. We pose the following research question: Given a text excerpt referencing a paper, could an LM act as a research assistant to correctly identify the referenced paper? We advance efforts to answer this question by building a benchmark that evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME, consists of text excerpts from recent machine learning papers, each referencing a single other paper. CiteME use reveals a large gap between frontier LMs and human performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%. We close this gap by introducing CiteAgent, an autonomous system built on the GPT-4o LM that can also search and read papers, which achieves an accuracy of 35.3% on CiteME. Overall, CiteME serves as a challenging testbed for open-ended claim attribution, driving the research community towards a future where any claim made by an LM can be automatically verified and discarded if found to be incorrect.

## 1 Introduction

Scientific discoveries are advancing at an ever-growing rate, with tens of thousands of new papers added just to arXiv every month . This rapid progress has led to information overload within communities, making it nearly impossible for scientists to read all relevant papers. However, it

Figure 1: **Example of a CiteME instance. The input (left) is an excerpt from a published paper with an anonymized citation; the target answer (right) is the title of the cited paper.**remains a critical scholarship responsibility to check new claims and attribute credit to prior work accurately. Language models (LMs) have shown impressive abilities as assistants across tasks , which leads us to explore the following task in this paper: _Can language models act as research assistants to help scientists deal with information overload?_

We make progress towards answering this question by evaluating the abilities of LMs in citation attribution . Given a text excerpt referencing a scientific claim, _citation attribution_ is the task in which a system is asked to fetch the title of a referenced paper, as illustrated in Figure 1.

Current benchmarks are collected automatically, which leads to the dominance of ambiguous or unattributable text excerpts that make overly broad claims or are not used as evidence for any specific claim, as shown in Table 1. Furthermore, these benchmarks typically frame citation attribution as a retrieval task from a small set of pre-selected papers where only paper titles and abstracts can be viewed, not the full paper's content important for citation attribution .

To address these issues, we introduce _CiteME_ (Citation for Model Evaluation), the first _manually curated_ citation attribution benchmark with text excerpts that unambiguously reference a single paper. CiteMe's use of only unambiguous text excerpts eliminates the subjectivity that characterizes other benchmarks.

To evaluate CiteMe, we conduct benchmark tests that focus on _open-ended_ citation attribution. Human evaluators confirm the lack of ambiguity, achieving 69.7% accuracy while taking just 38.2 seconds on average to find the referenced papers. The current state-of-the-art system, SPECTER2 , experiences 0% accuracy on CiteME, highlighting the real-world difficulties of LM-based citation attribution. Similarly, current frontier LMs achieve performance of 4.2-18.5%, substantially beneath human performance. We conclude that current LMs cannot reliably link scientific claims to their sources.

To bridge this gap, we introduce CiteAgent, an autonomous system built on top of the GPT-4o  LM and the Semantic Scholar search engine . CiteAgent can search for and read papers repeatedly until it finds the referenced paper, mirroring how scientists perform this scholarship task to find targeted papers. CiteAgent correctly finds the right paper 35.3% of the time when evaluated on CiteME.

In summary, our main contributions are:

* CiteME, a challenging and human-curated benchmark of recent machine learning publications that evaluates the abilities of LMs to correctly attribute scientific claims. CiteME is both natural and challenging, even for SoTA LMs.
* CiteAgent, an LM-based agent that uses the Internet to attribute scientific claims. Our agent uses an existing LM without requiring additional training. It also uses a search engine, which makes it applicable to real-world settings and differentiates it from systems that can search only within a predetermined corpus of papers.

Future work that improves the accuracy of CiteME may lead to systems that can verify _all_ claims an LM makes, not just those in the ML research domain. This could reduce the hallucination rate  and increase factuality  of LM-generated text.

    & Reasonable [\%] & Ambiguous [\%] & Unattributable [\%] & Trivial [\%] \\  FullTextPeerRead  & 24 & 26 & 34 & 16 \\ ACL-200  & 26 & 42 & 18 & 14 \\ RefSeer  & 24 & 28 & 32 & 16 \\ arXiv  & 10 & 50 & 30 & 10 \\  Average & 21 & 36.5 & 28.5 & 14 \\   

Table 1: Percentage of reasonable, ambiguous, unattributable, and trivial excerpts across 4 citation datasets, as labeled by human experts. For a detailed breakdown of every analyzed sample, see Appendix A.

The CiteME Benchmark

We now present the CiteME benchmark, which we differentiate from other citation prediction benchmarks that are automatically curated, _i.e.,_ curated without human supervision or feedback in selecting text excerpts [32; 31; 9; 40; 72; 44; 42; 33]. For comparison, we study the quality of excerpts across four popular citation prediction benchmarks (FullTextPeerRead, , ACL-200 [9; 58], RefSeer [40; 58], and arXiv ). Specifically, we sample 50 excerpts from each dataset and categorize them using the following criteria:

**(1) Attributable vs Unattributable.** The cited paper should provide _evidence for the statement in the text excerpt_, i.e., be an attribution as opposed to a statement that does not clearly refer to supporting evidence. Excerpts that do not follow this criterion are termed _unattributable_, as in the example:

_For all of our experiments, we use the hyperparameters from [CITATION]._

**(2) Unambiguous vs Ambiguous.** The cited text excerpt should not be overly broad. The ground truth cited papers should clearly be the _only possible reference_ for the claim in the text excerpt. Excerpts that do not follow this criterion are termed _ambiguous_, as in the example:

**[CITATION1, CITATION2] explored paper recommendation using deep networks.**

**(3) Non-Trivial vs Trivial.** The text excerpt should not include author names or title acronyms, which simply tests LM memorization and retrieval. Excerpts that do not follow this criterion are termed _trivial_, as in the example:

_SciBERT [CITATION] is a BERT-model pretrained on scientific texts._

**(4) Reasonable vs Unreasonable.** The text excerpt should be attributable, unambiguous and non-trivial. We term excerpts that do not follow this criterion _unreasonable_, but we categorize them according to the underlying issue (e.g., unattributable, ambiguous, or trivial). An example of a reasonable excerpt is:

_We use the ICLR 2018-2022 database assembled by [CITATION], which includes 10,297 papers._

In Table 1 (left), we demonstrate that most samples from all four datasets lack sufficient information for humans to identify the cited paper and are often labeled as ambiguous or unattributable. Additionally, an average of 17.5% of the samples are tagged as trivial because they include the title of the paper or its authors directly in the excerpt. Excerpts also frequently have formatting errors, making some nearly unreadable (see examples in Appendix A). Past work also notes similar artifacts [33; 42; 58], further supporting our claims. This analysis leads us to contend that performance on existing citation benchmarks might not reflect real-world performance of LM research assistants.

In response to these deficiencies, we created CiteME, a new benchmark with human expert curation for unambiguous citation references. CiteME contains carefully selected text excerpts, each containing a single, clear citation to ensure easy and accurate evaluation.

**Curation.** A team of 4 machine learning graduate students, henceforth referred to as "experts", were responsible for collecting text excerpts. The experts were instructed to find samples that (1) referenced a single paper and (2) provided sufficient context to find the cited paper with scant background knowledge. Each sample was checked for reasonableness; only those deemed reasonable by two or more experts were retained. Some excerpts were slightly modified to make them reasonable.

**Filtering Out the Easy Instances.** To ensure that CiteMe is a challenging and robust dataset, we remove all dataset instances that GPT-4o can correctly answer. Filtering datasets by removing the samples that a strong model can correctly answer was previously done in Bamboo  and the Graduate-Level Google-Proof Q&A Benchmark . In our filtering process, GPT-4o was used with no Internet access or any other external tools. Therefore, it could answer only correctly specified papers that it memorized from its training process. We ran each sample through GPT-4o five times to cover its different outcomes. In the end, we filtered out 124 samples, leaving 130 samples in total.

**Human Evaluation.** To ensure that our benchmark instances are not unsolvable, we evaluate human performance on them. Using a random subset of 100 samples, we asked a group of 20 experts, who were not part of benchmark construction, to perform the task of finding the referenced papers given only the excerpt, with each expert given 5 random samples from CiteME and a maximum of two minutes to solve each instance (similar to ). We observe that the experts found the correct citation 69.7% of the time, spending an average of only 38.2 seconds to do so. Note that this accuracy number does not represent the maximum-possible human performance since our annotators were limited to two minutes per question for budget reasons. Human accuracy may rise even higher given more time per instance. To check the experts' consistency, five more experts were asked to solve the same instances previously answered by the original experts. In 71% of the cases, both experts agreed on the answer, and at least one expert got to the right answer in 93% of cases.

**Are 130 questions sufficient to evaluate LMs?** Though traditional machine learning benchmarks usually contain thousands or even millions of test samples, recent work [17; 71; 74; 86] shows that LM benchmarks can include only 100-200 samples and remain insightful. HumanEval , for example, which consists of 164 programming problems, is among the most influential LM datasets today, appearing in virtually every SoTA LM paper recently published [66; 1; 81; 19]. Similarly, Bamboogle  contains 125 questions, DrawBench  contains 200 instances, and Plot2Code  contains 132 questions. This is in line with [70; 69], who show that benchmarks with many samples can be reduced to around 100 samples without sacrificing their utility. In addition, smaller benchmarks are advantageous because they are both cheaper to evaluate and impose a less significant environmental impact .

## 3 CiteAgent

We now describe CiteAgent, an LM-based system that we built to mimic researcher performance of open-ended citation attribution. A researcher seeking the correct attribution for a claim might use a search engine, read several papers, refine the search query, and repeat until successful. To allow CiteAgent to perform these actions, we built it to use Semantic Scholar to search for and read papers. Unless specified otherwise, we refer to CiteAgent with the GPT-4o backbone simply as CiteAgent throughout this paper.

Given a text excerpt, we prompt CiteAgent to perform one of a fixed set of custom commands and provide the output that the given command generated. CiteAgent then gives its rationale before performing another action, following [90; 88]. Figure 3 shows this process. We now describe the starting prompt and custom agent commands.

**Prompt.** Our prompt includes the task description, descriptions of available commands, and a demonstration _trajectory_, i.e., the series of actions that the system executes while solving an instance [90; 88]. The trajectory includes searching, reading a paper, and searching again (see Figure 4). We model our prompt on the SWE-Agent prompt .

**Agent Commands.** CiteAgent can respond to three custom commands (see Table 2). It always begins by executing the search command (sorting by relevance or citation count), which searches Semantic Scholar for a query and returns top results in a sorted order. After searching, CiteAgent can either search again, read one of the listed papers, or select a paper. It can perform up to 15 actions for every sample. Once a select action is taken, the session ends, and the selected paper is recorded.

Figure 2: _(Left)_ The top 10 most frequent labels of papers in CiteME, as identified by GPT-4. Overly broad tags like ”Machine Learning” or ”Deep Networks” were excluded (see Appendix D for details). _(Right)_ Most excerpts in CiteME are from recent papers.

**Search.** CiteAgent initiates a search command by querying Semantic Scholar . We chose the Selenium API  over the Semantic Scholar API due to the former's significantly better re-ranked queries and its ability to provide a uniform interface for both our model and human trajectory annotators.

Selenium also lets us access features such as sorting search results by relevance and citation count, which our human trajectory annotators found particularly valuable.

To ensure correctness, we filter out search results published after the excerpt's source paper, and the source paper itself. We then give CiteAgent the top 10 search results, which include paper id, title, abstract, and citation count.

**Read.** Read command execution causes CiteAgent to retrieve the open-access PDF corresponding to the selected paper from Semantic Scholar. Using the PyPDF2 library , our system extracts the text from the PDF, excluding visual figures. It then presents the text to CiteAgent, which generates a thought and a new command. If an open-access PDF link is unavailable, CiteAgent returns a message to that effect. We note that due to the limited context length of 8K tokens in the LLaMA-3 LM, we excluded the read action when using that model.

**Select.** Select command execution causes CiteAgent to choose a paper to attribute to the input text excerpt, which ends the run. If the number of actions reaches 14, CiteAgent is prompted to make a selection, forcefully concluding the run. This design choice ensures that all runs complete within a finite time and budget.

## 4 Experiment Setup

Below, we provide detailed implementation information for the baseline models and the various CiteAgent configurations we used for our evaluations.

**SPECTER Models.** We present the results of SPECTER  and SPECTER2  on CiteME as our baselines. SPECTER  encodes robust document-level representations for scientific texts,

   Command & Description \\  search(query, sort) & Searches for a query; sorts results by relevance or by citation count; returns a list of papers, where each item consists of the paper ID, title, number of citations, and abstract. \\ read(ID) & Returns the full text of a paper, including title, author list, abstract, and the paper itself. \\ select(ID) & Selects a paper from the search results as the answer. \\   

Table 2: Commands available to the model using our system.

Figure 3: The demonstration trajectory we gave CiteAgent in the prompt.

achieving high performance on citation prediction tasks without the need for fine-tuning. We use the Semantic Scholar SPECTER API1 to embed the input text excerpts and the Semantic Scholar Datasets API2 to embed all papers on Semantic Scholar, using these embeddings as our retrieval set.

SPECTER2 models  introduce task-specific representations, each tailored to different tasks. For our experiments, we use the base customization of SPECTER2 from Hugging Face3 to embed text excerpts and the Semantic Scholar Datasets API to similarly embed all papers on Semantic Scholar, forming our retrieval set. We apply an exact kNN  match to identify the closest embedding, computing the cosine similarity between the embeddings of text excerpt and all available papers (title and abstract). Using exact kNN matches ensures no approximations/errors are introduced while matching queries. We embed the query text excerpts at title only and both title and abstract, but that did not change the performance of the SPECTER models.

**CiteAgent.** We run the CiteAgent system with three SoTA LMs as backbones: GPT-4o , Claude 3 Opus , and LLaMa-3-70B . We additionally ablate over three classes of commands (Table 2):

1. **Search and Read.** The model can perform both search and read commands.
2. **Search Only.** The model is not allowed to read papers but can perform searches.
3. **No Commands.** The model operates with no access to the interface for actions like searching and reading.

Each class of actions is evaluated with and without demonstrations trajectories in the prompt, resulting in six configurations per LM. With three LMs, two action classes, and the option to include or exclude demonstrations, we present a total of 12 CiteAgent ablations. We exclude LLaMa with both Search and Read because its context length is limited to 8k tokens. For all experiments, we use a temperature of 0.95, following Yang et al. , and provide our detailed prompts in Appendix E.

## 5 Results

We present the evaluation results of the CiteME benchmark in Table 3. Our best model, CiteAgent (GPT-4o, search and read commands, and a demonstration in the prompt) achieves 35.3% accuracy, while the previous state-of-the-art models, SPECTER2 and SPECTER, achieve 0%. Human performance on the same task is 69.7% accuracy, with less than a minute of search time, indicating that a significant 34.4% gap remains.

**Performance across Language Models.** Comparing the performance of LMs across columns in Table 4, GPT-4o demonstrates the highest accuracy when it has access to both read and search

    & & &  \\   & & GPT-4o & LLaMA-3-70B & Claude 3 Opus & SPECTER2 & SPECTER \\   & **No Commands** & w/o Demo & 0 & 4.2 & 15.1 & 0 & 0 \\  & & w/ Demo & 7.6 & 5.9 & 18.5 & – & – \\   & **Search Only** & w/o Demo & 26.1 & **21.0** & 26.1 & – & – \\  & & w/ Demo & 29.4 & 2.5 & 27.7 & – & – \\   & **Search and Read** & w/o Demo & 22.7 & N/A & **27.7** & – & – \\  & & w/ Demo & **35.3** & N/A & 26.1 & – & – \\   

Table 4: Accuracy (in %) of LMs and retrieval methods on CiteME. We test how the available commands and prompt demonstrations affect CiteME performance. LLaMA’s context window is too small and therefore incompatible with the read command.

    & GPT-4o & LLaMA-3-70B & Claude 3 Opus & SPECTER2 & SPECTER1 \\  Accuracy [\%] & **35.3** & 21.0 & 27.7 & 0 & 0 \\   

Table 3: Performance of LMs (using our system) and retrieval methods on CiteME, summarized.

commands, outperforming other LMs by a wide margin. This finding aligns with previous research , which shows that GPT-4 powered agents excel in solving software issues. Notably, GPT-4o achieves high performance across settings even though CiteME consists exclusively of samples that GPT-4o cannot predict correctly without commands; its 0% performance without commands and demonstration trajectory is by design. However, LMs outperforming the SPECTER models purely by autoregressive generation provides evidence that LMs act as implicit knowledge bases with sufficient capacity .

**Performance across Demonstrations.** Comparing the performance between w/o Demo and w/ Demo rows in Table 4, we observe that LLaMA and Claude surprisingly perform worse when provided with a demonstration trajectory in the prompt. This may be due to the increased prompt length, which complicates the detection of important information . LLaMA-3-70b incurs a performance drop to 2.5% due to combined history extending beyond its context length, resulting in errors. However, GPT-4o effectively utilizes demonstrations, which improves its accuracy.

**Performance across Commands.** GPT-4o is the only LM whose accuracy improves with access to more commands, allowing it to read full papers. CiteAgent with GPT-4o creatively uses its commands across test samples, demonstrating command behaviors not shown in the demonstration trajectory (see Figure 4). It frequently refines its searches based on previous results and occasionally reads multiple papers before making a selection. In contrast, Claude 3 Opus is less effective in utilizing additional commands, likely due to difficulties in detecting important information .

### Error Analysis

To better identify CiteAgent's shortcomings, we analyze 50 randomly chosen CiteME samples from the best performing CiteAgent (using the GPT-4o backbone, with demonstrations, Search and Read commands) failed to solve correctly. We classify each error into three types based on CiteAgent's searches, its predicted paper and the justification provided:

**Error Type 1: Misunderstands the Excerpt.** This category accounts for 50% of the errors. It occurs when CiteAgent focuses on irrelevant parts of the excerpt or omits critical details. For example, in the following excerpt:

_The pioneering work of Reed et al.  approached text-guided image generation by training a conditional GAN [CITATION], conditioned by text embeddings obtained from a pretrained encoder._

CiteAgent searches for "Reed text-guided image generation conditional GAN" instead of "conditional GAN". It mistakes "Reed" as relevant to the current citation although it pertains to the previous one.

Figure 4: Five CiteAgent trajectories on five different samples. CiteAgent often exhibits behavior not shown in the demonstration given in the prompt, for example: searching by citation count and then by relevance, and searching multiple times in a row. Gray dotted box: prompt demonstration; green dotted boxes: CiteAgent succeeds; red dotted boxes: CiteAgent fails.

**Error Type 2: Understands the Excerpt but Stops Prematively.** In 32% of cases, CiteAgent searches for the correct term, but it stops at a roughly matching paper instead of the exact match. For example, in the following excerpt:

_Using Gaussian noise and blur, [CITATION] demonstrate the superior robustness of human vision to convolutional networks, even after networks are fine-tuned on Gaussian noise or blur._

CiteAgent found a paper comparing human and machine robustness but missed that it did not cover fine-tuned networks. Notably, this paper referenced the correct target paper, meaning CiteAgent could have found the right answer with just one more step if it had properly understood the paper it was reading. Moreover, in 12.5% of such cases, the correct paper appeared in the search results but was not chosen by CiteAgent.

**Error Type 3: Finds the Correct Citation but Stops Prematively.** The last 18% of errors occur when CiteAgent reads an abstract or paper and finds the correct citation; however, instead of doing another search, it selects the paper that cites the correct citation and stops searching. For example, in the following excerpt:

_[CITATION] investigates transformers' theoretical expressiveness, showing that transformers cannot robustly model noncounter-free regular languages even when allowing infinite precision._

CiteAgent finds a paper discussing the target paper and reports it, but it stops at the citing paper instead of searching for the correct target paper. For instance, it reports: _".. specifically mentioning Hahn's work on transformers' classification decisions becoming ineffective over longer input strings. This fits well with the description in the excerpt.."_ but it selects the citing paper instead of finding Hahn's work, which is the correct target paper.

**Technical Errors.** Aside from comprehension errors that stem from a lack of understanding an excerpt, 5.8% of runs encountered technical issues. Occasionally, the LM formats responses incorrectly, making them unparseable by the system. Additionally, the Semantic Scholar API has inconsistencies, such as not providing open access PDF links when available or linking to non-existent web pages. Further details on these technical errors are provided in Appendix F.

### Analyzing the Succesful Runs

Manually examining the instances that were correctly predicted by GPT-4o and Claude 3 Opus (Figure 5) provides insights into how the LMs use commands they were given. First, we confirm the results presented in Table 4: GPT-4o frequently reads papers before it correctly predicts a citation. Second, when both LMs correctly predict a paper, they usually take just 5 steps or fewer to do so. This could stem from LMs loss of important details when given a long context window .

CiteAgent's trajectories on CiteME enable us to analyze the shortcomings of GPT-4o and other SoTA LMs. These range from understanding fine details in text (Type 1 and Type 2 Errors), to not completely understanding the task (Type 3 Errors), to being unable to use commands (Technical

Figure 5: CiteAgent trajectories on samples that were correctly predicted reveals differences in model behavior. GPT-4o reads more frequently than Claude 3 Opus and can correctly predict papers even after performing many actions.

Errors). Correcting these errors could improve the utility of LMs on CiteME and for other related tasks.

### Benchmarking Reasoning Capability Improvements with Latest Models

We compare the latest LLMs on the CiteME benchmark (Table 5) and find that Claude 3.5 Sonnet outperforms the previous best, Claude 3 Opus. This improvement stems from better generalization, as Sonnet achieves 9.2% without internet access, compared to Opus' 18.5%. Similarly, LLaMa-3.1-70B shows significant gains of 8% over LLaMa-3.0-70B, highlighting enhanced reasoning capabilities. However, GPT-01, while performing well on CiteME, appears to have memorized 38.7% of the dataset, making its 61.3% benchmark performance less clear in terms of true improvement compared to GPT-4o.

## 6 Related Work

Recent work has made substantial progress in developing methods and datasets to assist researchers in paper writing and literature review [8; 12; 87] or act as tutors . Early work [48; 56] showed that researchers automatically retrieved topics and papers considered highly relevant to their work. Other studies included methods that assist researchers in finding new ideas , understanding certain topics , provide expert answers backed up by evidence  or clarifying a paper's related work by supplementing it with more information and focus [15; 67].

Closer to our line of research, prior studies developed methods for substantiating specific claims using evidence from published papers [75; 83; 85; 84; 91; 24; 39; 45]. Retrieval-augmented LMs [49; 11; 30] are also popularly used to ground claims with real-world evidence (see  for a survey). Chen et al.  built a web-based retrieval-augmented pipeline for fact verification; this contrasts with methods that use a static dataset for claim retrieval and verification [36; 5]. Concurrent to this work, Ajith et al.  build a retrieval benchmark consisting of questions about discoveries shown in specific machine learning papers.

Paper discovery is a crucial component of systems that automate scientific research as shown in [10; 47; 54; 61; 78]. CiteME plays an important role in developing better tools for paper discovery, and provides a way to effectively measure their efficiency. Currently, these systems are tested as a whole, without isolating the tools responsible for scientific discovery. CiteME allows us to evaluate components within them independently - and we discover that current LM Agents are not yet ready for automated paper discovery, leading to serious gaps in end-to-end automated research pipelines.

In addition, most existing LM benchmarks are saturated, with most LMs scoring 80-95% on them [43; 38; 20]. There is a need in the AI community to show what properties LMs currently lack, to show LM developers what aspects they should work on. On CiteME, the best LMs get less than 40%, clearly indicating to developers an important task that they could improve LMs on, while also providing an indicator they can use to track progress.

**Context-aware Recommendation.** Relevant to our research focus, [57; 64; 37] take as input documents or parts thereof and recommend papers that are likely to be cited, often referred to as _context-aware citation recommendation_[51; 26; 89; 28; 42; 65; 33]. The text inputs we use in CiteME resemble those used in [42; 65; 80], which contain a few sentences with a masked out citation. However, CiteME differs because it uses excerpts containing only one unambiguous citation, making

    & & &  \\   & & & Claude-3.5-Sonnet & LLaMa-3.1-70B & o1-mini & o1-preview \\   & **No Commands** & w/o Demo & 8.4 & 3.4 & 16.0 & 38.7 \\  & & w/ Demo & 9.2 & 8.4 & 10.9 & – \\   & **Search Only** & w/o Demo & 36.1 & **29.4** & 25.2 & – \\  & & w/ Demo & **43.7** & **29.4** & 32.8 & – \\   & **Search and Read** & w/o Demo & 37.0 & 22.7 & 26.9 & – \\   & & w/ Demo & 40.3 & 27.7 & **34.5** & 61.3 \\   

Table 5: Accuracy (in %) of newly released LM models on CiteME.

the context sufficient to identify the cited paper. Furthermore, our work explores agents with access to real-time paper information through tools like Semantic Scholar. This is crucial for real-time use since thousands of new papers are indexed by arXiv monthly (e.g., 8,895 papers in March 2024 under the cs category) . Most previous approaches would be impractical due to the need for retraining with every new paper issuance.

**Citation Attribution Datasets.** A variety of datasets contain text excerpts from scientific papers and corresponding citations . There are many crucial distinctions between the aforementioned datasets and CiteME, with the main one being that CiteME is composed of manually selected excerpts that clearly reference a paper. To our best knowledge, _CiteME is the only dataset that reports human accuracy on the benchmark._

Additionally, the excerpts in CiteME are mostly taken from papers published in the last few years (see Figure 2), whereas other datasets contain older papers. For example, the arXiv dataset  includes papers from 1991-2020, and FullTextPaperRead  contains papers from 2007-2017. This currency is particularly relevant in rapidly evolving fields like machine learning. The key distinction between the dataset and methods we present compared to previous works is their _real life applicability_. Our agent is based on SoTA LMs, needs no extra training, and can use a search engine, all of which make it easily applicable to real-world settings.

## 7 Conclusion

This work introduces a citation attribution benchmark containing manually curated text excerpts that unambiguously refer to a single other paper. We posit that methods that succeed on CiteME are likely to be highly useful in assisting researchers with real-world ML-specific attribution tasks but also generally useful in finding sources for generic claims. Further, our CiteAgent autonomous system can search the Internet for and read papers, which we show to significantly enhance the abilities of LMs on CiteME. We anticipate that this work will lead to LMs that are more accurate research assistants in the vital scholarship tasks of attribution.

## Author Contributions

The project was initiated by Andreas Hochlehnert and Ori Press, with feedback from Ameya Prabhu, Ofir Press, and Matthias Bethge. The dataset was created by Ori Press and Ameya Prabhu, with help from Vishaal Udandarao and Ofir Press. Experiments were carried out by Andreas Hochlehnert, with help from Ameya Prabhu. All authors contributed to the final manuscript.