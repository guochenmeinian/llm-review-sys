# Parametric model reduction of mean-field and stochastic systems via higher-order action matching

Jules Berman

Courant Institute of

Mathematical Sciences

New York University

New York, NY 10012

jmb1174@nyu.edu

&Tobias Blickhan

Courant Institute of

Mathematical Sciences

New York University

New York, NY 10012

tobias.blickhan@nyu.edu

Equal contribution

Benjamin Peherstorfer

Courant Institute of Mathematical Sciences

New York University

New York, NY 10012

pehersto@cims.nyu.edu

###### Abstract

The aim of this work is to learn models of population dynamics of physical systems that feature stochastic and mean-field effects and that depend on physics parameters. The learned models can act as surrogates of classical numerical models to efficiently predict the system behavior over the physics parameters. Building on the Benamou-Brenier formula from optimal transport and action matching, we use a variational problem to infer parameter- and time-dependent gradient fields that represent approximations of the population dynamics. The inferred gradient fields can then be used to rapidly generate sample trajectories that mimic the dynamics of the physical system on a population level over varying physics parameters. We show that combining Monte Carlo sampling with higher-order quadrature rules is critical for accurately estimating the training objective from sample data and for stabilizing the training process. We demonstrate on Vlasov-Poisson instabilities as well as on high-dimensional particle and chaotic systems that our approach accurately predicts population dynamics over a wide range of parameters and outperforms state-of-the-art diffusion-based and flow-based modeling that simply condition on time and physics parameters.

## 1 Introduction

Predicting the behavior of time-dependent processes \(X_{t,}\) over time \(t\) and across varying physics parameters \(\) is a key challenge in computational science and engineering . The dynamics of \(X_{t,}\) typically are described by systems of (stochastic) differential equations, which are derived from physics models and can be computationally expensive to simulate . Thus, it is desirable to learn reduced or surrogate models that can be rapidly evaluated to predict the system behavior across varying physics parameters .

Reduced modeling via learning population dynamicsGiven a data set of samples, i.e., realizations of the random variable \(X_{t,}\) on a suitable domain \(^{d}\),

\[\{X_{t_{j},_{k}}^{i}\,|i=1,,N_{x}, j=1,,N_{t}, k=1, ,N_{}\},\] (1)

we aim to learn a dynamical-system reduced model to rapidly predict samples that approximately follow the same law \(_{t,}\) as \(X_{t,}\) over time \(t\) and varying physics parameter \(\). We refer to the evolution of \(_{t,}\) in time as population dynamics. Learning the population dynamics instead of learning the dynamics of the individual trajectories \(t X_{t,}^{i}\) for all \(i=1,,N_{x}\) and \(\) can be beneficial: There are cases where \(_{t,}\) does not change in time, yet every sample trajectory \(t X_{t,}^{i}\) follows complicated dynamics. For example, consider incompressible fluid dynamics with constant density. Samples corresponding to particles that comprise the fluid can have complicated trajectories, whereas on a distribution level, the density of the fluid is constant and so are the population dynamics. Furthermore, learning population dynamics seamlessly treats deterministic and stochastic systems because the stochastic models that we consider can be expressed as deterministic Fokker-Planck equations on the population level.

Our approach: Learning parametric minimal energy vector fields that represent population dynamicsBuilding on standard literature on optimal transport theory  as well as the so-called action-matching loss introduced in , we pose a variational problem to learn gradient fields \( s_{t,}\) so that the continuity equation corresponding to the vector field given by \( s_{t,}\) approximates the population dynamics \(_{t,}\) of the samples (1). In the spirit of reduced modeling , we seek a vector field \(s_{t,}\) that generalizes to different values of the physics parameters \(\). We therefore optimize for \(s_{t,}\) that minimizes the average objective of a variational problem over all parameters \(\), where \(\) describes the distribution of parameters on the domain \(^{p}\). We parametrize \(s_{t,}\) with a neural network with weight modulation  so that it can be evaluated quickly over \(t\) and \(\).

_Rapid sample generation in inference phase_ Predictions at inference time at new physics parameters \(\) are made by sampling based on the vector field \( s_{t,}\), which means that our approach represents \(_{t,}\) through the application of \( s_{t,}\) on an initial condition. Importantly, time \(t\) in the inference step corresponds to the time of the physics problem so that in one inference step a whole sample trajectory is obtained, rather than a sample at one specific time point as in regular conditioning-based methods (see literature review). Thus, we can rapidly generate samples that follow the law \(_{t,}\) in the inference phase.

_Stabilizing training with higher-order quadrature_ An important part of our contribution is stabilizing the training procedure by accurately estimating the objective of the variational problems from few data samples. In particular, instead of uniformly sampling over the data (1), we introduce an empirical loss (8) that utilizes higher-order quadrature  in the time direction so that the learned \( s_{t,}\) accurately captures the dynamics over time \(t\). Consequently, we refer to our approach as higher-order action matching (HOAM). Our numerical experiments show that the higher-order quadrature in the empirical loss is key for learning gradient fields \( s_{t,}\) that accurately capture the evolution in time \(t\) and that generalize across physics parameters \(\).

Literature reviewWe review relevant literature; see Figure 1 for an overview.

_Non-intrusive and data-driven surrogate modeling_ There is a range of surrogate and latent modeling methods that aim to learn or reduce the sample dynamics of the realizations rather than the population dynamics, such as dynamic mode decomposition, Koopman-based methods, and others  as well as neural network-based methods such as neural ordinary differential equations . There also are methods for stochastic systems . However, all of these methods ignore physics parameter dependencies and/or aim to learn the sample dynamics, whereas we focus on parametric population dynamics.

_Population dynamics and trajectory inference_ Learning population dynamics has been considered extensively in computational biology in the context of gene expression, where the focus is on learning from independent samples at selected time points rather than from sample trajectories ; however, many of these approaches  are simulation-based and thus require integrating dynamics during the training or parameterizing the density additionally to the vector field. These works also are not concerned with generalizing over a range of physics parameters in many cases.

_Diffusion- and flow-based modeling_ There is a large body of work on diffusion-based  and flow-based modeling ; see  for a detailed review. These approaches are not taking into account time \(t\) because they learn paths between a reference and a target distribution only. There are works that condition on time \(t\) and a parameter \(\) such as , but this requires then generating a path for each time step at inference time, which is computationally expensive. Furthermore, the conditioning on time \(t\) means that the target distribution \(_{t,}\) at each time \(t\) and \(\) is different, and thus a separate hyper-parameter tuning can be required, which is impractical over many time steps and physics parameters as in our physics problems; see our numerical experiments. The works  compute transport-based solutions but parametrize different quantities than our approach, require actively sampling data, and ignore physics parameters \(\). We note that there also is work on forecasting with diffusion- and flow-based modeling , which is a different task than our task of predicting across varying physics parameters.

_Optimal transport_ Besides the machine learning literature, variational approaches for inferring vector fields are extensively used in optimal transport theory . Of particular importance to us is the formulation by Benamou and Brenier . The Bennamou-Brenier formula describes a joint optimization problem over vector fields and paths in probability space and the action matching loss  is the restriction of this optimization problem to the case of a fixed path and the vector field parametrized by a neural network, which are core building blocks for us that we show can be used together with a parameter dependency.

ContributionsWe summarize our contributions:

**(a)** Developing a loss to learn population dynamics that remain valid across varying physics parameters by building on optimal transport literature  and action matching .

**(b)** Introducing higher-order quadrature schemes for the loss to efficiently couple the gradient fields over time. This leads to lower variance estimators of the loss that critically stabilize training.

**(c)** Demonstrating on a range of physics problems from Vlasov-Poisson instabilities to high-dimensional chaotic systems that our approach leads to (i) accurate predictions of population dynamics and (ii) orders of magnitude speedups in inference/prediction over

Figure 1: Parametric model reduction with our HOAM seeks to learn vector fields that represent population dynamics \(_{t}\) over time \(t\). In contrast, parametric model reduction with score-based diffusion denoising and flow-based modeling requires conditioning on time \(t\), which leads to separate, costly inference steps for each time step of a sample trajectory.

classical methods that numerically solve the underlying partial differential equations as well as standard diffusion- and flow-based models that condition on physical time.

We provide an implementation of our method at https://github.com/julesberman/HOAM.

## 2 Method

### Parameter-dependent population dynamics

Continuity equationLet us consider data (1) corresponding to the probability measure \(_{t,}\), which is absolutely continuous for \(t\) and \(\). We use the same notation for the measure and its density. The density \(\) of \(\) is also assumed to be absolutely continuous on \(\). We consider population dynamics of \(X_{t,}_{t,}\) that can be described by the continuity equation

\[_{t}_{t,}=-(_{t,}v_{t,}), t\,,\,,\] (2)

with the initial condition \(_{t=0,}=:_{0,}\) and vector field \(v_{t,}\). Notice that in our case the continuity equation (2) depends on the physics parameter \(\). There can be many vector fields \(v_{t,}\) that lead to the same population dynamics (2). For example, if \(v_{t,}\) is a vector field that describes the dynamics of \(_{t,}\) via (2), then another vector field is given by \(v^{}_{t,}=v_{t,}+w/_{t,}\) with any other \(w\) that satisfies \( w=0\) as long as \(_{t,}\) is positive.

Uniqueness via gradient fields and the corresponding elliptic problemsBecause we aim to learn a vector field from sample data (1) that describes the population dynamics (2) of the corresponding law \(_{t,}\), it is helpful to remove this non-uniqueness. One way to do so is to restrict the vector field to \(v_{t,}= s_{t,}\) so that it is a gradient field [4, p. 45]. Plugging \(v_{t,}= s_{t,}\) into (2), together with the assumptions \(_{t,}>0\) and \(_{}_{t}_{t,}x=0\), leads to parametric elliptic problems in \(s_{t,}\)

\[-(_{t,} s_{t,})=_{t}_{t, }\,,\] (3)

with coefficient function \(_{t,}\), right-hand side (source term) \(_{t}_{t,}\), and homogeneous Neumann boundary conditions \(_{t,} s_{t,}=0\) on \(\) with normal vector \(\) for all \(t\) and \(\). The weak forms of the elliptic problems (3) lead to energy minimization problems that can be used to learn the gradient field \(s_{t,}\) via optimization:

\[_{s H^{1}(_{t,},)}E_{t,}(s):=_{s H^{1}( _{t,},)}_{}| s|^{2}_{t, }x-_{}_{t}_{t,}sx\] (4)

for each \(t\) and \(\). The space \(H^{1}(_{t,},)\) contains functions \(s\) with \(_{}| s|^{2}_{t,}x<\), which is the energy (semi-)norm corresponding to the \(_{t,}\)-weighted inner product [29, Sec. 2.3.2].

Optimal transportStandard elliptic theory guarantees unique solutions up to constants of (4) in the Sobolev space \(H^{1}()\) under strong assumptions on \(_{t,}\) such as uniform boundedness by a positive constant for all \(t\) and \(\); see [29, Proposition 2.2] and [11, Section 3.2]. The theory of optimal transport allows treating the much more general case when \(_{t,}\) is not uniformly bounded away from zero and possibly atomic; we refer to  and [74, Section 5.3.1] for details. Among all vector fields \(v_{t,}\) that are compatible to \(_{t,}\) in the sense of (2), gradient fields \( s_{t,}\) have the smallest associated kinetic energy \(_{}|v|^{2}_{t,}x\), which is the objective considered in . In the language of optimal transport and in particular the formalism of , vector fields with minimal kinetic energy describe tangent vectors to the curve \(t_{t,}\). The metric is the inner product of \(L^{2}(_{t,},,^{d})\). This is the weak Riemannian structure of \(()\) equipped with the Kantorovich-Rubinstein metric and described in detail in [5, Chapter 8]. We give a short description in Appendix E.

Energy functional with entropy termInstead of the energy (4), we can also use other choices of the energy to select gradient fields, as long as energy functions are convex to maintain uniqueness. We consider an energy that is based on a different notion of discrepancy on \(()\), the entropic optimal transport or Schrodinger bridge problem ,

\[E^{}_{t,}(s)=_{}|(s-}{2}_{t,})|^{2}_{t,}x-_{} _{t}_{t,}sx\,,\] (5)which depends on \( 0\). The energy \(E^{}_{t,}\) is of particular interest for two reasons: One, the Euler-Lagrange equation of (5) in strong form is the Fokker-Planck equation for \(s^{}_{t,}\): \(_{t}_{t,}=-(_{t,} s^{}_{t,}) +}{2}_{t,}\), again with homogeneous Neumann boundary conditions for all \(t\) and \(\); see Appendix C. This means we can efficiently generate samples after learning \(s^{}_{t,}\) via corresponding stochastic differential equations (SDEs). Two, it can be interpreted as regularizing the field \(s_{t,}\), which we discuss in Appendix C.

### Loss for learning vector fields over time \(t\) and physics parameter \(\)

Variational formulation over \(t\) and \(\)So far we just carried along time \(t\) and physics parameter \(\) but did not address them in the variational problems, i.e., we had separate variational problems (4) for all \(t\) and \(\). We now propose to consider the average energy over \(t\) and \(\) to infer a map \(s: H^{1}(_{t,},),(t,) s_ {t,}\), which is called a solution map in reduced modeling ,

\[_{s: H^{1}(_{t,},)}E^{ }(s):=_{s}_{}_{0}^{1}E^{}_{t,}(s_{t,})\,t\,().\] (6)

Notice that time \(t\) and physics parameter \(\) have two different effects on the gradient field \( s_{t,}\): Time \(t\) couples the elliptic problems (i.e., (3) for \(=0\)) via the time derivative \(_{t}_{t,}\); see Appendix D. In contrast, the elliptic problems are uncoupled over \(\) and can be considered separately. This means that to compute the solution to an elliptic problem for one value of \(\), one does not need to consider any other \(^{}\). This will allow us to sample the physics parameters over \(\) independently from each other when estimating the corresponding loss, whereas we will use higher-order quadrature to obtain an accurate approximation of the time integral to ensure the coupling between the time points is reflected in \(s_{t,}\); see Section 2.3.

Loss for learning gradient fields from samples over \(t\) and \(\)The energy \(E_{t,}\) defined in (4) as well as the energy \(E^{}_{t,}\) defined in (5) leads to a loss that can be estimated from samples (1). The quantity \(_{t}_{t,}\) appears in (4) and (5), which is typically unavailable when we have access to data samples (1) only. Integration by parts of the term involving \(_{t}_{t,}\) eliminates it, see also Appendix D. We arrive at

\[E^{}(s)\!=\!_{}[_{0}^{1}_{ }(| s_{t,}|^{2}\!+\!_{t}s_{t,} \!+\!}{2} s_{t,})_{t,}x t\!-\!_{}s_{t,}_{t,}x_{t =0}^{t=1}]()\,.\] (7)

Note that this loss is comprised only of expectation values with respect to \(_{t,}\) and is therefore well-defined also for empirical distributions. The choice \(>0\) assumes that the Fisher information of \(_{t,}\) is finite.

**Remark 1**.: _Loss functions of the form as (7) but without the parameter dependence have been used in  and [47, Theorem 2.1]. In fact, the case with \(=0\) appears already in [8, Equation 35] and [64, Section 3]. We build on these results but work with population dynamics that depend on physics parameters, which leads to the loss shown in (7)._

### Parameterizing the vector field, estimating the loss from data, sampling

Parametrizing \(s_{t,}\) with weight modulationsWe parametrize the vector field \(s_{t,}\) via a neural network with continuous versions of low-rank adaptation (CoLoRA) layers, which have been successfully used for parametric model reduction of deterministic time-dependent dynamical systems ; see also . The layers have the form \((x)=Wx+(t,)ABx+b\), where \(W\) is a weight matrix, \(A,B\) are low-rank matrices, \(b\) is a bias vector, and \((t,)\) is a scalar weight modulation; see Appendix B. Only the weight modulations \((t,)\) depend on time \(t\) and physics parameter \(\). We use a hyper-network \(h:\) that depends on the weight vector \(^{q}\) to map \(t\) and \(\) to the modulation weights \((t,)=h(t,;)\). The weights \(W,A,B,b\), which are independent of \(t\) and \(\), over all layers are collected into the weight vector \(^{q^{{}^{}}}\). Typically \(q q^{}\). Using the hyper-network encourages continuity of \(s_{t,}\) in time \(t\), which is key for many physics problems .

Combining higher-order quadrature and Monte Carlo sampling for estimating the loss from sample dataEstimating the loss (7) from data can be challenging because the three nested integrals (expectations) over the samples \(X^{i}_{t,}\), time \(t\), and physics parameter \(\) can have different properties and correspondingly need different numerical treatment. Our numerical results show that it is critical to accurately estimate the loss to avoid instabilities in the training; see Section 3 and Figure 2.

We propose a combination of higher-order numerical quadrature and Monte Carlo sampling to estimate the loss (7). In particular, we propose to use a higher-order quadrature rule for the time \(t\) integral. Because it is a one-dimensional integral, standard higher-order quadrature rules from numerical analysis are applicable . The time integral needs to be estimated with particular high accuracy to ensure the coupling between the time points as well as the coupling to the boundary terms to match the path from \(_{0,}\) at time \(t=0\) to \(_{1,}\) at time \(t=1\). Our numerical results will show that estimating the time integral to high accuracy is essential for stabilizing the training. In contrast to the one-dimensional integral over time, the integrals over \(\) and the parameter domain \(\) can be high dimensional and thus we estimate them via Monte Carlo estimation.

We consider two high-order quadrature rules, composite Simpson's quadrature and Gauss-Legendre quadrature ; see Appendix A. We refer to our method as HOAM-S and HOAM-G when using either quadrature, respectively. Importantly, these quadrature rules require samples on specifically spaced time points, equidistant in the case of Simpson's and at the Gauss-Legendre nodes in the case of Gauss quadrature. If the data set (1) does not contain samples at these time points then we interpolate the data to the appropriate times. We note that for Simpson's quadrature, interpolation is typically unnecessary as data simulated with numerical methods often come at equispaced points in time.

We denote a Monte Carlo estimate of an expectation value obtained from a mini-batch as \(}^{n}_{x}[f]:=_{i=1}^{n}f(X^{i})\) where \(X^{1},X^{2},,X^{n}\). Then, the empirical loss with mini-batching of sizes \(n_{x},n_{}\) and \(n_{t}\) quadrature points in time is given by

\[^{}(s)\!=\!}^{n_{}}_{}\! _{n=1}^{n_{t}}w_{n}\,}^{n_{x}}_{x_{t_{n,}}}\, | s_{t_{n},}|^{2}\!+\!_{t}s_{t_{n},}\! +\!}{2} s_{t_{n},}\!-\!}^{n_ {x}}_{x_{t,}}\,[s_{t,}]_{t=0}^{t=1}\] (8)

where \(w_{n}\) are numerical quadrature weights and \(t_{n}\) are the corresponding nodes; see Appendix A for the Simpson's quadrature and Gauss-Legendre weights and nodes.

Rapid predictions (inference) with learned reduced modelsMaking predictions in the inference step means drawing samples that follow the law represented by the learned gradient field \( s_{t,}\), which approximates the law \(_{t,}\) of \(X_{t,}\). Because we train with the loss (7), we integrate the SDE \(_{t,}= s_{t,}(_{t,})t+ W_{t}\), where \(W_{t}\) are Wiener processes and \(\) is the same \(\) that is used in the training loss (7); see Appendix C. As initial condition, we use samples from \(_{0,}\) at time \(t=0\). Of course other sampling schemes can be used .

Notice that the time \(t\) in the SDE used for generating samples is the same time as of the physics problem and thus of the sample trajectory. This means that the costs of the inference step of our HOAM for generating a trajectory of length \(K\) scales as \((K)\). In contrast, introducing a conditioning on time and physics parameter in, e.g., noise-conditioned score matching (NCSM)  and conditional flow matching (CFM) or stochastic interpolants  requires inferring a separate sampling path for each \(t\) and \(\) pair of interest. In particular, the inference costs of CFM scale as \((K)\), where \(\) is the number of steps taken in the differential equation for generating one sample at one time point. For NCSM with annealed Langevin sampling, the inference costs scale as \((K)\), where \(\) is the number of annealing steps. Contrasting this to the scaling of \((K)\) of our HOAM approach shows that HOAM is well suited for fast predictions over \(t\) and \(\) as required in parametric model reduction.

## 3 Numerical experiments

ExamplesWe consider the following parametric dynamical systems; details in Appendix B. _1. Harmonic oscillator_: A collection of particles evolves in four-dimensional phase-space

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

dynamics to learn from such a system are the population dynamics \(_{t,}\) rather than the sample dynamics; see Appendix B.2. We observe the particles computed with a particle-in-cell method and learn the gradient field \( s_{t,}\) with the proposed HOAM approach. For a test physics parameter \(\) that controls the wave number, we then generate samples with \( s_{t,}\) and plot a histogram in Figure 3 for the bump-on-tail (top) and two-stream (middle top) instability. Our approach approximates well the histogram obtained with the classical particle-in-cell method. Figure 5 (right) shows that HOAM is the only method which provides speedup over the classical particle-in-cell (full) model, as NCSM and CFM lead to 1-2 orders of magnitude longer inference times than HOAM and the full models.

For the strong Landau damping problem in six dimensions (three spatial and three velocity), our HOAM approach achieves about 2 orders of magnitude speedup. This is because the runtime of the full model based on the traditional particle-in-cell method to compute the mean-field dynamics scales poorly with the dimension. In this example, the runtime of the full model increases by almost two orders of magnitude. In contrast, the runtime of our HOAM reduced model increases only from 6 to 8 seconds. This importantly shows that the computational costs of the inference step of reduced models built with HOAM avoid exponential scaling with the dimension in this example.

We now compute the electric energy as a quantity of interest from the generated samples over time \(t\) for the test physics parameters, which we plot in Figure 4 and its relative error averaged over time (e.e.) in Table 1 (see (25)). Our HOAM approximates the electric energy well at later times, whereas NCSM and CFM lead to poorer approximations at later times \(t\). This is relevant because this non-linear regime is where numerical solvers become important; the initial (linear) growth regime can be approximated well by analytical perturbation theory. Also for the six-dimensional strong Landau damping problem, our HOAM approach provides accurate predictions of the electric energy with orders of magnitude speedups; see Table 1 and Figure 3 as well as Figure 7 in the appendix.

Speedups in inference step (predictions)Recall two limitations of introducing a time and physics parameter dependence in NCSM/CFM via conditioning (see page 3 and Section 2.3): (i) For each \(t\) and \(\), a separate sampling path has to be computed, which leads to orders of magnitude higher inference runtimes than in HOAM; see Table 1, Section 2.3. (ii) For each \(t\) and \(\) pair, the target distribution \(_{t,}\) is different, which can require \(t\)- and \(\)-specific tuning of hyper-parameters of the inference step, which is impractical and thus can lead to a deterioration of accuracy compared to our HOAM approach; see Figure 3-4.

Predicting statistics of chaotic and particle dynamics in high dimensionsWe now consider the nine-dimensional dynamical system introduced in , which leads to chaotic behavior. We show in Figure 3 (bottom) the sample histogram corresponding to a test physics parameter that represents the Rayleigh number. At time \(t=3.7\) and projecting onto

Figure 4: Electric energy of bump-on-tail (top) and two-stream (bottom) instability. HOAM with Simpson’s and Gauss quadrature accurately predicts the energy growth in the transient regime and oscillations at later times. The ground truth is displayed in blue.

dimension three and nine, the histograms show that the proposed HOAM accurately matches the low probability region that connects the two high probability regions, whereas AM fails to converge. Consider now the example of the particles in an aharmonic trap, which leads to 100-dimensional samples \(X^{i}_{t,}\). For a test physics parameter, Figure 5 shows that HOAM accurately predicts the mean particle positions even for this high dimensional system.

## 4 Conclusions, limitations, and future work

For parametric model reduction, learning population dynamics via minimal-energy vector fields over time \(t\) and physics parameter \(\) with our variational approach helps reduce inference runtime compared to standard diffusion- and flow-based modeling that condition on \(t\) and \(\) and therefore have to solve a separate inference problem for each time step and physics parameter at test time. Because we learn the dynamics over time \(t\), it is critical to accurately capture the coupling over the time steps, for which we propose to use higher-order quadrature schemes when estimating time integrals in the training loss. The higher-order quadrature of the time integrals considerably improves training stability. Our approach achieves comparable errors as state-of-the-art methods while at the same time reducing inference runtime by 1-2 orders of magnitude. Additionally, HOAM provides speedups of up to 2 orders of magnitude to classical numerical full models.

_Limitations_: First, if there are only very few samples in time, even numerical quadrature cannot provide an accurate enough estimation of the loss, which could be a limitation in computational biology . Second, we currently seek a vector field that minimizes the kinetic energy or a variant thereof. Investigating other notions of energy that might lead to vector fields with other desired properties in certain problems remains a challenge.

We do not expect that this work has negative societal impacts.

  example: &  &  &  &  \\  metric: & e.e. & r.t. [s] & e.e. & r.t. [s] & e.e. & r.t. [s] & sinkhorn & r.t. [s] \\  CFM  & 1.44 & 139 & 5.52 & 141 & 0.629 & 161 & 0.259 & 36 \\ NCSM  & 0.245 & 1142 & 0.626 & 1133 & 4.06 & 4531 & 0.869 & 1109 \\ AM  & 0.275 & 6 & 0.892 & 6 & NaN & - & 80.1 & 7 \\ HOAM-S (ours) & **0.078** & 6 & **0.427** & 6 & 0.641 & 7 & **0.214** & 7 \\ HOAM-G (ours) & 0.208 & 6 & 0.429 & 6 & **0.447** & 7 & 0.217 & 7 \\  

Table 1: HOAM with Simpson’s and Gauss quadrature outperforms state-of-the-art methods w.r.t. inference runtime (r.t.) with comparable errors when applied to various physics problems for parametric model reduction. Metrics: e.e. is the relative error in electric energy, see (25); for the Sinkhorn divergence, see Appendix B.5.

Figure 5: **Left**: HOAM accurately predicts the time evolution of the mean position of a 100-dimensional particle system in an aharmonic moving trap (dim 1 vs dim 100). **Right**: HOAM reduced models provide about 2 orders of magnitude speedup over traditional numerical (full) models for the 6 dimensional strong Landau problem. HOAM is also 1–2 orders of magnitude faster than CFM and NCSM, which provide no speedup over the full models in our problems.