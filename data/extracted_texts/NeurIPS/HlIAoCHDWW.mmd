# Learning in the Presence of Low-dimensional Structure:

A Spiked Random Matrix Perspective

 Jimmy Ba\({}^{1,2,3}\), Murat A. Erdogdu\({}^{1,2}\), Taiji Suzuki\({}^{4,5}\), Zhichao Wang\({}^{6}\), Denny Wu\({}^{7,8}\)

\({}^{1}\)University of Toronto, \({}^{2}\)Vector Institute, \({}^{3}\)xAI, \({}^{4}\)University of Tokyo, \({}^{5}\)RIKEN AIP,

\({}^{6}\)University of California San Diego, \({}^{7}\)New York University, \({}^{8}\)Flatiron Institute

{jba,erdogdu}@cs.toronto.edu, taiji@mist.i.u-tokyo.ac.jp,

zhw036@ucsd.edu, dennywu@nyu.edu

###### Abstract

We consider the problem of learning a single-index target function \(f_{*}:^{d}\) under the spiked covariance data:

\[f_{*}()=_{*}},,\ \ (0,_{d}+^{ }),\ \  d^{}[0,1),\]

where the link function \(_{*}:\) is a degree-\(p\) polynomial with information exponent \(k\) (defined as the lowest degree in the Hermite expansion of \(_{*}\)), and it depends on the projection of input \(\) onto the spike (signal) direction \(^{d}\). In the proportional asymptotic limit where the number of training examples \(n\) and the dimensionality \(d\) jointly diverge: \(n,d,n/d(0,)\), we ask the following question: how large should the spike magnitude \(\) be, in order for \((i)\) kernel methods, \((ii)\) neural networks optimized by gradient descent, to learn \(f_{*}\)? We show that for kernel ridge regression, \( 1-\) is both sufficient and necessary. Whereas for two-layer neural networks trained with gradient descent, \(>1-\) suffices. Our results demonstrate that both kernel methods and neural networks benefit from low-dimensional structures in the data. Further, since \(k p\) by definition, neural networks can adapt to such structures more effectively.

## 1 Introduction

Learning under spiked covariance.Real-world data is often high-dimensional but it also exhibits certain low-dimensional structures. Indeed, the number of input features is exceedingly large in machine learning, yet most relevant information is concentrated in a low-dimensional subspace . Such structures have important consequences on learning performance. In random matrix theory, high dimensionality is reflected by the _proportional asymptotic limit_ where the numbers of training examples and input features diverge to infinity at the same rate, whereas the low-dimensional structure is often described by a _spiked random matrix model_ in which a low-rank signal ("spike") is hidden in a large-dimensional noise ("bulk"). In this work, we restrict ourselves to the simple setting where the signal is rank-1, and consider the following data-generating process:

\[(0,_{d}+^{}), f_{*}( )=_{*}}, , \]

where the goal is to estimate the target function (teacher) \(f_{*}\), which is a _single-index model_ depending on the signal direction \(^{d}\), and \(_{*}:\) is an unknown _link function_. The signal \(\) is also embedded in the spiked input data, and \(>0\) controls the spike magnitude; we scale \( d^{}\) for \([0,1)\), where larger \(\) indicates a more prominent low-dimensional structure, and hence the learning problem becomes easier. We aim to characterize the efficiency of learning \(f_{*}\) using kernel methods and neural networks (NNs) in relation to the strength of the low-dimensional signal \(\).

Prior results: isotropic data.When \(=0\), the input features are isotropic and do not reveal any information about the target \(f_{*}\). In this setting, the performance of kernel methods and two-layer NNs have been extensively studied in the proportional asymptotic limit.
* **Kernel methods.** For isotropic Gaussian or spherical input data, the performance of random features (RF) regression and kernel ridge regression (KRR) has been precisely characterized in the proportional limit . However, in this regime, RF models and KRR suffers from the "curse of dimensionality"  and cannot achieve vanishing generalization error unless \(f_{*}\) is _linear_. More generally, for KRR to learn a degree-\(p\) polynomial target, a sample complexity of \(n=(d^{p})\) is required .
* **Neural networks + Gradient descent.** While NNs can efficiently _approximate_ a single-index model (with polynomial link \(_{*}\)), the _optimization_ complexity of gradient-based learning on isotropic data is known to be governed by the _information exponent_, defined as the index of the first non-zero Hermite coefficient of \(_{*}\). For NNs trained with gradient descent (GD) to learn a target with information exponent \(k\), recent works showed that a sample complexity of \(n=(d^{k})\) suffices1. Hence, in the proportional regime (\(n d\)), learnability has only been established for \(k=1\). Hence, in the high-dimensional regime, KRR can only learn linear functions on the input, and NNs learn targets with information exponent \(k=1\), which cannot cover many important problems such as phase retrieval . Following the intuition that low-dimensional structure in the input data can make the learning problem easier, we ask the following question.

_If the input data contains low-dimensional structure given by the spiked covariance model (1.1), can kernel methods and neural networks learn a larger class of \(f_{*}\) in the proportional regime?_

### Our Contributions

We answer this question in the affirmative by showing that in the proportional asymptotic limit when the spike magnitude \(\) reaches a certain threshold, both KRR and GD-trained two-layer NN can learn the target function \(f_{*}\), where the link function \(_{*}\) is a degree-\(p\) polynomial with information exponent \(k\) (see Definition 1). Our findings are summarized below and illustrated in Figure 1.

* For KRR with the inner-product kernel, we give a sharp analysis of the prediction risk and show that to learn a link function \(_{*}\) with degree \(p\) in the proportional regime, a spike magnitude of \(=d^{1-}\) is _both necessary and sufficient_.
* For a two-layer NN with ReLU activation, we upper bound the prediction risk when representation learning is performed via one gradient descent step on the first-layer parameters (analogous to ) and show that a spike magnitude of \(=d^{1-}\) is _sufficient_ to learn \(_{*}\) with information exponent \(k\).

Our analysis reveals that both KRR and NNs can benefit from the low-dimensional structure (spike) in the input data, and they can learn a wider class of target functions in the proportional regime compared to the isotropic setting (i.e., \(p>1\) for KRR, and \(k>1\) for NNs). Furthermore, since for a given link function \(k p\) by definition, we know that NNs require a smaller spike magnitude \(\) to learn the same target compared to KRR (see Figure 1). In other words, a GD-trained NN can utilize such a low-dimensional structure more effectively than KRR.

Figure 1: Spike magnitude \( d^{}\) for KRR and GD-trained NN to learn the target \(f_{*}\) with degree \(p\) and information exponent \(k\) in the proportional regime. When \(k p\), KRR requires a stronger low-dimensional structure (i.e., larger spike) to learn \(f_{*}\).

Related works.Related to our problem setting,  studied the sample complexity of KRR and NNs in a spiked covariance model, and showed that both models indeed benefit from a stronger low-dimensional structure (i.e., larger spike). However, their analysis assumed the number of spikes to be diverging and cannot cover the proportional regime we consider. More importantly, the NN analysis is limited to approximation, and gradient-based optimization guarantee is not given.

In , the authors presented a non-rigorous analysis on classifying XOR mixtures using an RF model and a two-layer NN trained with online SGD and showed that the NN can learn the task under smaller spike magnitude. Such a task is parallel to the information exponent \(k=2\) setting (see ; our results rigorously demonstrate that similar separation exists for a different class of target functions with potentially higher degree and information exponent.

## 2 Preliminaries: Problem Setting and Assumptions

Notations.\(\|\|\) denotes the \(_{2}\) norm for vectors and the \(_{2}_{2}\) operator norm for matrices, and \(\|\|_{F}\) is the Frobenius norm. \(_{d}()\) and \(o_{d}()\) stand for the standard big-O and little-o notations, where the subscript highlights the asymptotic variable; we write \(}()\) when (poly-)logarithmic factors are ignored. \(_{d,}()\) (resp. \(o_{d,}()\)) represents big-O (resp. little-o) in probability as \(d\). \((),(),()\) are defined analogously. Denote \(\) as the standard Gaussian distribution in \(\). Given \(f:^{d}\), we denote its \(L^{p}\)-norm with respect to the data distribution as \(\|f\|_{L^{p}}\).

### Basic Assumptions

We consider Gaussian input data with a spiked covariance:

\[(0,_{d}+^{}),  d^{}[0,1), \]

where the signal direction \(^{d}\) satisfies \(\|\|=1\), and the spike magnitude \(\) is allowed to grow with the input dimensionality \(d\) as specified by the exponent \(\): as \(\) gets larger, the spike magnitude increases and hence the data exhibits stronger low-dimensional structure. Following the terminology in , we may define the _effective dimensionality_ of the input data as \(d_{} d^{1-}\), which captures the intuition that a larger spike renders the features more low-dimensional. As we will see, the sample complexity of KRR and two-layer NN is decided by this effective dimensionality.

Teacher model.We consider a _student-teaching setting_, where the labels \(y\) are generated from a teacher model (target function) \(f_{*}:^{d}\). In the spiked covariance model (2.1), it is known that input directions with large variations are often good predictors of the labels -- indeed, this is the main reason that principal component analysis is used in practice . We therefore consider the following single-index target function where the index features align with the spike direction \(\):

\[y=f_{*}()=_{*}},, \]

where the link function \(_{*} L^{2}()\) is centered such that \([_{*}(z)]=0\) for \(z(0,1)\) (this can be achieved by subtracting the mean from the training labels as in ), and the normalization factor \((1+)^{-1/2}\) ensures that the \(L^{2}\) norm of \(f_{*}\) remains constant. We remark that given the prior knowledge of low-dimensional structure, we may efficiently learn \(f_{*}\) by first performing PCA on the input features; our goal, however, is not to construct an optimal learning algorithm, but to understand the behavior of KRR and two-layer NN without such data preprocessing, similar to .

We also assume \(_{*}\) is a degree-\(p\) polynomial with information exponent (IE) \(k\) defined below.

**Definition 1** (Information exponent).: _Let \(\{h_{j}\}_{j=0}^{}\) denote the normalized Hermite polynomials. The information exponent of \(f L^{2}()\), which we denote by \(k\), is the index of the first non-zero Hermite coefficient of \(f\), that is, given the Hermite expansion \(f=_{j=0}^{}_{j}h_{j}\), \(k:=\{j>0:_{j} 0\}\)._

It is clear that by definition, we always have \(k p\), and equality is only achieved when \(_{*}\) is a "pure" degree-\(k\) Hermite polynomial. Intuitively, IE measures the magnitude of information contained in the gradient, and larger \(k\) implies increased gradient descent complexity . Such definition is also related to the recently introduced _leap complexity_.

**Remark**.: _In (2.2) we do not corrupt the training labels \(y\) with i.i.d. label noise, meaning that, in light of the bias-variance decomposition, we analyze the bias term describing the extent \(f_{*}\) is learned by the student model. If label noise is introduced, the additional variance term (describing the "overfitting" due to noise) can be handled by standard concentration argument._

### Learning Objective and Training Procedure

Given training examples \(\{(_{i},y_{i})\}_{i=1}^{n}\), where \(_{i}}{}(0,_{d}+ ^{})\) and \(y_{i}=f_{*}(_{i})\), we consider two student models obtained via the following training procedure in the proportional regime:

_Proportional asymptotic limit:_\(n,d, n/d(0,)\). (2.3)

Student model I: Kernel ridge regression.We focus on the inner-product kernel: \(k(,)=g(,)}{d})\) for some \(g L^{2}()\) (similar argument can also apply to rotationally invariant kernels as in ). Denoting the associated reproducing kernel Hilbert space (RKHS) with \(\), the kernel ridge regression estimator is given by

\[_{}=*{argmin}_{f} {n}_{i=1}^{n}(y_{i}-f(_{i}))^{2}+\|f\|_{}^{2}} \;\;_{}()=k(,)^{}(+ )^{-1}, \]

where \(>0\) is the ridge parameter, and \(^{n n}\) is the kernel Gram matrix with entries \(_{i,j}=k(_{i},_{j})\). Spectral properties of the kernel matrix and the prediction risk of KRR have been extensively studied in various high-dimensional settings . Since the kernel function is fixed before seeing the data, we intuitively expect that KRR cannot effectively adapt to a low-dimensional structure in the learning problem.

Student model II: Two-layer neural network.We aim to learn the following width-\(N\) network:

\[f_{}()=}_{i=1}^{N}a_{i}(,_{i}+b_{i})=,N^{-1/2}(^{} +), \]

where \(:\) is a nonlinear activation, and the trainable parameters \(^{d N},,^{N}\) are initialized from standard Gaussian distribution. We require the network width to be \(N=(d^{})\) for some small \(>0\). Such a choice entails that it is sufficient to have \((d^{1+})\) total training parameters, which is almost proportional to the sample size \(n d\); in contrast, for KRR we need to store the kernel Gram matrix with \(n^{2}\) entries, which is less computationally efficient at test time.

It is known that NNs can adapt to the learning problem via _representation learning_, in which the trained features encode relevant information of the target function. To realize this advantage, we update the parameters in (2.5) via the two-stage procedure introduced in , where we first learn the representation by taking one gradient descent step on the first-layer parameters \(\), and then estimate the second-layer parameters \(\) separately (which is a convex problem). This procedure is summarized as follows (see Algorithm 1 in Appendix for more details).

1. _Feature learning for the 1st layer._ We optimize the _representation_ of the NN via gradient descent, where the training objective is the empirical squared loss \((f)=_{i=1}^{n}(f(_{i})-y_{i})^{2}\). Specifically, denote the \(i\)-th column of the initialized weight matrix \(_{0}\) as \(_{i}^{(0)}^{d}\), and the initialized NN as \(f_{}^{0}\), we take one GD step with learning rate \(\) as follows, \[_{i}^{(1)}_{i}^{(0)}-_{_{i} ^{(0)}}f_{}^{(0)}.\] Due to the anisotropic input data, the pre-activation \(,_{i}^{(1)}\) may blow up if the trained parameter \(_{i}^{(1)}\) aligns with the signal direction \(\). To circumvent this issue, we normalize each neuron to have (roughly) unit pre-activation, i.e., for \(i[N]\), we perform the weight normalization: At high level, this resembles the often-used normalization layers in deep learning .
2. _Ridge regression for the 2nd layer._ After obtaining the updated first-layer parameters \(_{1}^{d N}\), we optimize the second-layer \(\) by solving the ridge regression objective - this can be done by using GD to solve an \(_{2}\)-penalized least squares. To circumvent the dependence between the training data \(\) and the learned \(_{1}\), we follow  and estimate the regression coefficients \(}\) using _a new set of training data_\(\{}_{i},_{i}\}_{i=1}^{n}\) with the same sample size \(n\). Denoting the feature matrix on the fresh training set \(\{},}\}\) as \(:=}(}_{1}+)^{n N}\), the ridge regression estimator can be obtained by: \(}=*{argmin}_{}\|} -\|^{2}+\|\|^{2}}\).

Learning in the proportional limit.Given a target function \(f_{*}\) and a learned (student) model \(\), we evaluate the model performance using the prediction risk:

\[():=_{}[(()-f_{*}())^{2}]= \|-f_{*}\|_{L^{2}}^{2},\]

where the expectation is taken over the (anisotropic) Gaussian data distribution. To benchmark the model performance in the proportional limit (2.3), we introduce the following notion of learnability.

**Definition 2** (Learnability in the proportional regime).: _We say an algorithm learns the target function \(f_{*}\) in the proportional regime if, for any (small) constant \(>0\), there exists some constant \(_{*}>0\) such that when \(n/d_{*}\), the algorithm constructs \(\) using \(n\) i.i.d. training examples, and achieves \(()=\|-f_{*}\|_{L^{2}}^{2}\) with probability \(1\) when \(n,d\) proportionally._

Based on this definition, in order for a student model \(\) (e.g., KRR or two-layer NN) to learn the teacher \(f_{*}\) in the proportional regime, it needs to achieve arbitrarily small prediction risk with high probability, and with _linear sample complexity_ (i.e., using \(n d\) training samples).

## 3 Kernel Ridge Regression

For an inner-product kernel, the performance of KRR depends on the behavior of the pairwise inner-product of the training examples \(_{i},_{j}\). Intuitively speaking, since the inner-products concentrate in high dimensions, the kernel matrix can be approximated in operator norm via low-degree Taylor expansion, and thus KRR can only learn \(f_{*}\) that is low-degree (e.g., see ). Introducing a low-dimensional structure to the data is one way to counteract such near-orthogonality in high dimensions. Hence we expect that KRR can achieve better performance when the spike magnitude \(\) becomes larger - this intuition will be made rigorous in the following subsections.

### Sharp Analysis of the Prediction Risk

In this section, we study the prediction risk of KRR defined in (2.4) with a positive semidefinite kernel given by \(k(_{i},_{j})=g(_{i},_{j}/d)\), i.e., \(g^{(k)}(0) 0\) for all \(k\) (see [24, Section 13.1]). Recall that the strength of the low-dimensional component is determined by the spike magnitude \( d^{}\); in the following, we characterize how the exponent \([0,1)\) affects the performance of KRR.

We denote by \(P_{}:L^{2} L^{2}\) the orthogonal projector onto the subspace of polynomials of degree at most \(\) for any \(\). Similarly, we denote by \(P_{>}=-P_{}\) the projector to the orthogonal complement where Id is the identity operator. To illustrate this notion, consider \(=1\), then we have

\[f_{*}()=P_{ 1}f_{*}()+P_{>1}f_{*}()=a^{*}+^ {*},+P_{>1}f_{*}(),\]

where \(a^{*},^{*}=*{argmin}_{a,b}_{}[(f_{*}()-a-,)^{2}]\). Note that any polynomial \(f_{*}\) with degree at most \(\) satisfies \(P_{>}f_{*}=0\). Denoting the KRR model as \(_{}\), we have the following sharp analysis of the asymptotic prediction risk.

**Theorem 3**.: _Given any fixed \(\). Suppose that \(g^{(k)}(0)>0\) for all \(k\), and the spike magnitude scales as \( d^{}\) for \((1-,1-)\). Then as \(n,d\), \(n/d(0,)\), the prediction risk of the KRR estimator \(_{}\) with \(=_{d}(1)\) satisfies the following with probability 1,_

\[(_{})-\|P_{>}f_{*}\|_{L^{2}}^{2}=o_{d}(1).\]

As a corollary of Theorem 3, based on Definition 2, we can obtain a sharp threshold of \(\) for learning a degree-\(p\) polynomial link function in the proportional regime (Definition 2).

**Corollary 4**.: _Assume \(_{*}\) is a degree-\(p\) polynomial with \(p 1\) defined in (2.2). Under the same assumptions of Theorem 3, if \(>1-\), then the KRR estimator with \(=_{d}(1)\) learns \(f_{*}\) in the proportional regime, i.e., \((_{})=o_{d}(1)\) with probability 1 when \(n,d\) proportionally._

We make the following remarks on Theorem 3 and Corollary 4.

* At a high level, the above theorem aligns with the conclusion of  which assumes spherical data with a diverging number of spikes. Recall that the _effective dimensionality_ of our anisotropic Gaussian data is \(d_{} d^{1-}\), and Theorem 3 implies that to extract the signal direction \(\) and learn a degree-\(p\) teacher model, a sample size of \(n=(d_{}^{p})\) is required; note that for \(>0\), this implies an improvement over the isotropic setting where the sample complexity is \(n=(d^{p})\)* While we only state the result for inner-product KRR (similar to ), we expect similar findings for general rotationally invariant kernels: \(k(_{i},_{j})=g(_{i},_{j},\|_{i}\|,\| _{j}\|)\), since the norm term has negligible contribution to the learning of a single-index target (2.2). In fact,  established a lower bound for rotationally invariant KRR, which, when applied to our setting, gives a necessary sample complexity of \(n d_{}^{(p)}\). Although this lower bound may not be sharp, it illustrates that the required spike magnitude needs to scale with target degree \(p\).

### Intuition behind the Analysis

We briefly summarize the two main ingredients for the proof of Theorem 3. As a preliminary step, we can rotate the signal \(\) to the \(_{1}\) direction due to the rotational invariance of Gaussian distribution. After such transformation, we have \(([_{i}]_{1},[_{i}]_{2:d})}}{ {}}(0,1+)(0,_{d-1})\) with \(=(d^{})\).

Polynomial approximation of the kernel matrix.Firstly, we approximate the inner-product KRR with some degree-\(L\) polynomial kernel in the form of

\[k(_{i},_{j})=_{k=1}^{L}c_{k}d^{-k}_{i},_{j} ^{k}.\]

Such approximation can be used to establish a learning lower bound for KRR. For instance in the proportional limit,  showed that for well-conditioned data, the inner-product kernel can be approximated by the linear kernel, which implies that KRR only learns linear \(_{*}\). For general deterministic datasets, similar approximation has been studied in  where the "orthogonality" of input data determines the polynomial approximation error. In our setting, the inner product \(_{i}^{}_{j}=_{d}(d^{-1})\) depends on the spike magnitude \( d^{}\). Consequently, we approximate our inner-product kernel by some polynomial kernel whose degree depends on \(\).

Orthogonal polynomial expansion.While the approximation in the preceding step simplifies the kernel, such analysis is generally not sharp. To refine the result, we expend the polynomial kernel in the Hermite bases (analogous to the analysis of spherical data in ). After rotation, the teacher model (2.2) can be written as \(f_{*}()=_{*}(z)\), for \(z(0,1)\) and \([]_{1}=z\) independent with other entries \([]_{i}\) for \(2 i d\). In light of the kernel trick [14, Section 3.7], we write \(k(_{i},_{j})=(_{i}),(_{j})\) where the feature vector \(()\) is composed of Hermite polynomials of \(\) up to some degree. Importantly, we only need to extract the Hermite components with respect to the first entry \([]_{1}\) to learn a function \(_{*}(z)\).

## 4 Two-layer Neural Network

During the feature learning phase of NN training, we intuitively expect the parameters to align with the (rank-1) signal direction which allows NN to overcome the "curse of dimensionality". Indeed, in this section, we show that the first gradient step on the first-layer weights \(\) enables the model to "zoom in" to the low-dimensional structure, given that the spike magnitude \(\) is sufficiently large.

### Upper Bound on the Prediction Risk

We consider the following standard Gaussian initialization for the two-layer NN (2.5):

\[[_{0}]_{ij}}}{{}}(0, 1/d),[_{0}]_{i}}}{{}} (0,1/N),[_{0}]_{i}}}{ {}}(0,1). \]

Denote the NN optimized by the two-stage procedure outlined in Section 2.2 as \(_{ NN}\). In the sequel, we restrict ourselves to \((z)=(z)=\{z,0\}\). We expect similar characterization to hold when the student activation \(\) has non-zero Hermite coefficients up to degree-\(p\) (see ).

The following theorem gives a sufficient condition on the spike magnitude \( d^{}\) in order for \(_{ NN}\) to learn the target function \(f_{*}\) with linear sample complexity (in terms of dimension dependence).

**Theorem 5**.: _Consider the NN training procedure in Section 2.2 with Gaussian initialization defined in (4.1), \(N=(d^{})\), \(=(N^{1/2+})\), and appropriately chosen \(_{2}\) regularization \(d^{-1} d^{-}\) for small \(>0\). Then, for \(=\), we have_

\[(_{ NN})=o_{d}(1)>1-1/k,\]

_as \(n,d,n/d\). That is, the two-layer ReLU NN can learn the target function with the information exponent \(k\) in the proportional regime._The above theorem predicts that a spike magnitude of \(=(d^{1-})\) is sufficient for a GD-trained two-layer NN to learn \(f_{*}\) in the \(n d\) regime. Similar to prior works , the information exponent \(k\) also determines the complexity of the learning problem in our setting: larger \(k\) implies a more "difficult" task for GD, hence we need a larger \(\) (i.e., stronger low-dimensional signal) to achieve linear sample complexity. Moreover, in light of the _effective dimensionality_ defined in , Theorem 5 implies a sample size of \(n=(d^{k}_{})\) suffices to learn a single-index target with information exponent \(k\); this contrasts the \(n=(d^{}_{})\) complexity of KRR given in Theorem 3. Such discrepancy will be highlighted and empirically validated in Section 5.

### Intuition behind the Analysis

Theorem 5 is established in two steps: first, we prove that under certain conditions, gradient descent can align the first-layer parameters \(\) with the signal direction \(\); then we show that after achieving such an alignment, \(f_{*}\) can be efficiently learned by optimizing the second-layer \(\).

Spiked covariance amplifies gradient signal.Due to our "mean-field" initialization (i.e., small second-layer coefficients), the initial NN has small output \(f^{(0)}_{}() 0\). Therefore the gradient of the squared loss is dominated by the correlation between the student and teacher model. Concretely, consider the population gradient for one parameter vector \(^{d}\) at initialization (for simplicity we omit the bias unit here):

\[_{}_{}f^{(0)}_{}()-_{}[^{}(, )f_{*}()]\] \[}{{=}}-_{} f^{}_{*}()^{}(,) (1+)^{-1/2}+f_{*}()^{}(,)\] \[}{{=}}- _{}[^{}_{*}(,)^{ }(,)]\,+\,,\]

where \((i)\) is due to multivariate Stein's lemma, and \((ii)\) follows from the definition of \(\). Utilizing the Hermite expansions of the student and teacher nonlinearities,

\[(z)=_{i=0}^{}_{i}h_{i}(z),_{*}(z)=_{i=0} ^{}_{i}^{*}h_{i}(z),\]

we have the following decomposition on the strength of the correlation term,

\[_{}[^{}_{*}(,)^ {}(,)]_{i=0}^{}(i+1)^{2} _{i+1}_{i+1}^{*}, ^{i}.\]

This calculation illustrates that the (expected) first gradient step indeed contains the direction of the signal \(\). More importantly, the magnitude of the signal term is affected by two factors: \((i)\) it is amplified by a larger spike \(\), and \((ii)\) it vanishes when the information exponent \(k\) is large, since \(,=_{}(d^{-1/2})\) at initialization. The gradient magnitude relates to the sample complexity because in order to establish learnability in the proportional regime, we need to achieve nontrivial gradient concentration using \(n d\) samples. Consequently, the sufficient condition in Theorem 5 requires a larger \(\) when the information exponent \(k\) is large, and vice versa.

**Remark**.: _One caveat in the above derivation is that the Hermite coefficients of the student nonlinearity \(\{_{i}\}_{i=1}^{}\) need to be non-zero up to degree-\(k\), which is not satisfied by ReLU or most commonly-used activation functions. We solve this issue by taking into account the bias units \(b_{i}\) at initialization which "diversity" the nonlinearity, similar to  (see Lemma 15 for details)._

Nonparametric learning with random biases.After the signal direction is identified via gradient descent, we need to learn the unknown link function \(_{*}\). We utilize random biases as a resource for univariate approximation - similar argument has appeared in many prior works . Recall that in the representation learning phase, the bias units \(b_{i}\) are not optimized. Therefore, if the weight vectors \(\) align with the signal direction \(\), then learning the second-layer coefficients \(\) via ridge regression can be reduced to a univariate kernel regression problem; specifically, given \(_{i},_{j}^{d}\), the univariate kernel is given by \(k(z_{i},z_{j})=_{a,b}[(a z_{i}+b)(a z_{j}+b)]\), where \(z_{i}=_{i},\), \(z_{j}=_{j},\). For \(=\), it is known that such kernel can efficiently learn a polynomial link function \(_{*}\) (e.g., see ).

Experiments: Comparing KRR and NN

In this section, we empirically validate the theoretical results presented in previous sections. We construct target functions \(f_{*}\) with a link of varying degree \(p\) and information exponent \(k\) and experimentally compute the prediction risk of KRR and that of a two-layer NN.

\(k=p\): NN & KRR are comparable.We first consider the setting where \(k=p\), which indicates that the link function is a pure degree-\(p\) Hermite polynomial: \(_{*}(z)=h_{p}(z)\). Recall that \( d^{}\); Theorem 3 implies that KRR learns the target function in the proportional regime when the spike magnitude satisfies \(>1-\), which matches the sufficient condition for NN given in Theorem 5.

In the experiments of Figure 2, we compute the prediction risk of KRR and that of two-layer NN optimized via the two-stage procedure outlined in Section 2, where the link function \(_{*}\) is a pure degree-2 and degree-3 Hermite polynomial. We observe that both KRR and NN can learn \(f_{*}\) with linear sample complexity when the spike magnitude \(\) exceeds the same threshold predicted by our theory; intuitively speaking, this means that for the \(k=p\) setting, NN and KRR utilize the low-dimensional structure with the same efficiency. Also, note that while Theorem 5 only provides an upper bound on the risk, the experiments demonstrate that the predicted scaling of \(\) is sharp.

\(k<p\): NN outperforms KRR.Next, we consider a setting where the link function \(_{*}\) is a mixture of low- and high-degree components: \(_{*}(z)=_{j}_{j}h_{j}(z)\), with \(=k,=p\). In this case, Theorem 3 predicts that KRR requires a spike magnitude of \(>1-\) to learn \(f_{*}\) with linear sample complexity, whereas Theorem 5 entails that a smaller spike magnitude \(>1-\) is sufficient for the two-layer NN.

Figure 2: Prediction risk of KRR (a)&(c) and GD-trained two-layer NN (b)&(d) to learn \(f_{*}\), where the link function \(_{*}(z)=h_{2}(z)\) in (a)&(b), and \(_{*}(z)=h_{3}(z)\) in (c)&(d). We set \(=5\). For KRR we use the Gaussian RBF kernel and \(=10^{-2}\). For two-layer NN, we set \(=\). The experiments are averaged over 50 runs. Darker color corresponds to a smaller prediction risk, and the red lines are predicted by the sufficient conditions of learnability given by Theorem 3 and 5.

Figure 3 shows that the predictions from Theorem 3 and 5 once again accurately align with the experimental results. Observe that when \(k<p\), GD-trained NN indeed outperforms KRR, in the sense that it requires a less prominent low-dimensional structure (i.e., smaller spike magnitude \(\)) to attain linear sample complexity. This illustrates the benefit of NN under structured data due to the presence of representation (feature) learning. Roughly speaking, for target function with \(k<p\), the sample complexity of representation learning (i.e., finding the signal direction \(\) via gradient-based learning) is low compared to directly estimating \(f_{*}\) on the original input features; therefore GD-trained NNs achieve better performance than KRR.

## 6 Conclusion and Future Directions

We investigated the performances of KRR and GD-trained NNs in the proportional asymptotic limit. We analyzed the prediction risk of these models in learning a nonlinear single-index model (2.2), where the index feature in the teacher model only relies on the spike direction \(\) hidden in the anisotropic Gaussian input data (2.1). The strength \(\) (signal-to-noise ratio) of this spike determines the level of low dimensionality, which affects the learnability of both NNs and KRR. Our results clearly demonstrate that both NNs and KRR benefit from this additional structure in the covariance; yet, NNs adapt to the low-dimensional patterns more efficiently.

It is worth noting that our theoretical analysis focuses on the "well-specified" scenario where the input spike perfectly aligns with the index features of \(f_{*}\). In a companion work , we investigate a more general setting where the spiked covariance data only provides partial information of \(f_{*}\) (i.e. the "misaligned" scenario), and show that feature learning (via gradient flow) also benefits from such structured data. Finally, extending our results to multi-index teacher models  and considering more general training dynamics (e.g., beyond the first gradient step) applied to multiple-layer NNs are interesting directions left for future studies.

Figure 3: Prediction risk of KRR (a)&(c) and GD-trained two-layer NN (b)&(d) to learn \(f_{*}\), where the link function \(_{*}(z)=h_{1}(z)+h_{3}(z)\) in (a)&(b), and \(_{*}(z)=h_{2}(z)+h_{3}(z)\) in (c)&(d). We use the same hyperparameters as in Figure 2. The red lines are predicted by the sufficient conditions in Theorem 3 and 5.