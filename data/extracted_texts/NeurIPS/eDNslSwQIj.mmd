# Neural Assets: 3D-Aware Multi-Object

Scene Synthesis with Image Diffusion Models

 Ziyi Wu\({}^{3,4,}\), Yulia Rubanova\({}^{1}\), Rishabh Kabra\({}^{1,5}\), Drew A. Hudson\({}^{1}\),

**Igor Gilitschenski\({}^{3,4}\), Yusuf Aytar\({}^{1}\), Sjoerd van Steenkiste\({}^{2}\),**

**Kelsey R. Allen\({}^{1}\), Thomas Kipf\({}^{1}\)**

\({}^{1}\)Google DeepMind \({}^{2}\)Google Research \({}^{3}\)University of Toronto \({}^{4}\)Vector Institute \({}^{5}\)UCL

Work done while interning at Google.

Contact: tkipf@google.com. Project page: neural-assets.github.io

###### Abstract

We address the problem of multi-object 3D pose control in image diffusion models. Instead of conditioning on a sequence of text tokens, we propose to use a set of per-object representations, _Neural Assets_, to control the 3D pose of individual objects in a scene. Neural Assets are obtained by pooling visual representations of objects from a reference image, such as a frame in a video, and are trained to reconstruct the respective objects in a different image, e.g., a later frame in the video. Importantly, we encode object visuals from the reference image while conditioning on object poses from the target frame. This enables learning disentangled appearance and pose features. Combining visual and 3D pose representations in a sequence-of-tokens format allows us to keep the text-to-image architecture of existing models, with Neural Assets in place of text tokens. By fine-tuning a pre-trained text-to-image diffusion model with this information, our approach enables fine-grained 3D pose and placement control of individual objects in a scene. We further demonstrate that Neural Assets can be transferred and recomposed across different scenes. Our model achieves state-of-the-art multi-object editing results on both synthetic 3D scene datasets, as well as two real-world video datasets (Objectron, Waymo Open). Additional details and video results are available at our project page.

## 1 Introduction

From animation movies to video games, the field of computer graphics has long relied on a traditional workflow for creating and manipulating visual content. This approach involves the creation of 3D assets, which are then placed in a scene and animated to achieve the desired visual effects. With the recent advance of deep generative models , a new paradigm is emerging. Diffusion models have achieved promising results in content creation  by training on web-scale text-image data . Users can now expect realistic image generation, depicting almost everything describable in text. However, text alone is often insufficient for precise control over the output image.

To address this challenge, an emerging body of work has investigated alternative ways to control the image generation process. One line of work studies different forms of conditioning inputs, such as depth maps, surface normals, and semantic layouts . Another direction is personalized image generation , which aims to synthesize a new image while preserving particular aspects of a reference image (e.g., placing an object of interest on a desired background). However, these approaches are still fundamentally limited in their 3D understanding of objects. As a result, they cannot achieve intuitive object control in the 3D space, e.g., rotation. While some recent works introduce 3D geometry to the generation process , they cannot handle multi-object real-world scenes as it is hard to obtain scalable training data (paired images and 3D annotations).

We address these limitations by taking inspiration from cognitive science to propose a scalable solution to 3D-aware multi-object control. When humans move through the world, their motor systems keep track of their movements through an efference copy and proprioceptive feedback [4; 96]. This allows the human perceptual system to track objects accurately across time even when the object's relative pose to the observer changes . We use this observation to propose the use of _videos_ of multiple objects as a scalable source of training data for 3D multi-object control. Specifically, for any two frames sampled from a video, the naturally occurring changes in the 3D pose (e.g., 3D bounding boxes) of objects can be treated as training labels for multi-object editing.

With this source of training data, we propose Neural Assets - per object latent representations with consistent 3D appearance but variable 3D pose. Neural Assets are trained by extracting their visual appearances from one frame in a video and reconstructing their appearances in a different frame in the video conditioned on the corresponding 3D bounding boxes. This supports learning consistent 3D appearance disentangled from 3D pose. We can then tokenize any number of Neural Assets and feed this sequence to a fine-tuned conditional image generator for precise, multi-object, 3D control.

**Our main contributions** are threefold: **(i)** A Neural Asset formulation that represent objects with disentangled appearance and pose features. By training on paired video frames, it enables fine-grained 3D control of individual objects. **(ii)** Our framework is applicable to both synthetic and real-world scenes, achieving state-of-the-art results on 3D-aware object editing. **(iii)** We extend Neural Assets to further support compositional scene generation, such as swapping the background of two scenes and transferring objects across scenes. We show the versatile control ability of our model in Fig. 1.

## 2 Related Work

**2D spatial control in diffusion models (DMs).** With the rapid growth of diffusion-based visual generation [22; 44; 70; 79; 82; 94], there have been many works aiming to inject spatial control to pre-trained DMs via 2D bounding boxes or segmentation masks. One line of research achieves this by manipulating text prompts [11; 32; 51], intermediate attention maps [12; 16; 17; 27; 41; 52; 110] or noisy latents [25; 64; 69; 90] in the diffusion process, without the need to change model weights. Closer to ours are methods that fine-tune pre-trained DMs to support additional spatial conditioning inputs [5; 29; 33; 45; 112; 114]. GILGEN  introduces new attention layers to condition on bounding boxes. InstanceDiffusion  further supports object masks, points, and scribbles with a unified feature fusion block. To incorporate dense control signals such as depth maps and surface normals, ControlNet  adds zero-initialized convolution layers around the original network blocks. Recently, Boximator  demonstrates that such 2D control can be extended to video models with a similar technique. Several existing works [3; 107] leverage natural motion observed in video and similar to our work propose to train on paired video frames to achieve pixel-level control. In our work, we build upon pre-trained DMs and leverage 3D bounding boxes as spatial conditioning, which enables 3D-aware control such as object rotation and occlusion handling.

Figure 1: **3D-aware editing with our Neural Asset representations**. Given a source image and object 3D bounding boxes, we can translate, rotate, and rescale the object. In addition, we support compositional generation by transferring objects or backgrounds across images.

**3D-aware image generation.** Earlier works leverage differentiable rendering to learn 3D Generative Adversarial Networks (GANs)  from monocular images, with explicit 3D representations such as radiance fields [14; 15; 37; 71; 86; 111] and meshes [18; 19; 31; 73; 74]. Inspired by the great success of DMs in image generation, several works try to lift the 2D knowledge to 3D [49; 60; 62; 66; 75; 89; 98; 105]. The pioneering work 3DiM  and follow-up work Zero-1-to-3  directly train diffusion models on multi-view renderings of 3D assets. However, this line of research only considers single objects without background, which cannot handle in-the-wild data with complex backgrounds. Closest to ours are methods that process multi-object real-world scenes [2; 72; 84; 115]. OBJect-3DIT  studies language-guided 3D-aware object editing by training on paired synthetic data, limiting its performance on real-world images . LooseControl  converts 3D bounding boxes to depth maps to guide the object pose. Yet, it cannot be directly applied to edit existing images. In contrast, our Neural Asset representation captures both object appearance and 3D pose. It can be easily trained on real-world videos to achieve multi-object 3D edits.

From a methodology perspective, there have been prior works learning disentangled appearance and pose representations for 3D-aware multi-object image editing [71; 100; 111]. However, they are all based on the GAN framework  and do not learn generalizable object representations via an encoder. In contrast, we build upon a large-scale pre-trained image diffusion model  and powerful feature extractors , enabling editing of complex real-world scenes.

**Personalized image generation.** Since the seminal works DreamBooth  and Textual Inversion  which perform personalized generation via test-time fine-tuning, huge efforts have been made to achieve this in a zero-shot manner [47; 58; 101; 106; 109]. Most of them are only able to synthesize one subject, and cannot control the spatial location of the generated instance. A notable exception is Subject-Diffusion , which leverages frozen CLIP embeddings for object appearance and 2D bounding boxes for object position. Still, it cannot explicitly control the 3D pose of objects.

**Object-centric representation learning.** Our Neural Asset representation is also related to recent object-centric slot representations [48; 63; 91; 92; 108] that decompose scenes into a set of object entities. Object slots provide a useful interface for editing such as object attributes , motions , 3D poses , and global camera poses . Nevertheless, these models show significantly degraded results on real-world data. Neural Assets also consist of disentangled appearance and pose features of objects. Different from existing slot-based models, we fine-tune self-supervised visual encoders and connect them with large-scale pre-trained DMs, which scales up to complex real-world data.

## 3 Method: Neural Assets

Inspired by 3D assets in computer graphics software, we propose Neural Assets as learnable object-centric representations. A Neural Asset comprises an appearance and an object pose representation, which is trained to reconstruct the object via conditioning a diffusion model (Sec. 3.2). Trained on paired images, our method learns disentangled representations, enabling 3D-aware object editing and compositional generation at inference time (Sec. 3.3). Our framework is summarized in Fig. 2.

### Background: 3D Assets in Computer Graphics

3D object models, or _3D assets_, are basic components of any 3D scene in computer graphics software, such as Blender . A typical workflow includes selecting \(N\) 3D assets \(\{_{1},...,_{N}\}\) from an asset library and placing them into a scene. Formally, one can define a 3D asset as a tuple \(_{i}(_{i},_{i})\), where \(_{i}\) is a set of descriptors defining the asset's appearance (e.g., canonical 3D shape and surface textures) and \(_{i}\) describes its pose (e.g., rigid transformation and scaling from its canonical pose).

### Neural Assets

Inspired by 3D assets in computer graphics, our goal is to enable such capabilities (i.e., 3D control and compositional generation) in recent generative models. To achieve this, we define a _Neural Asset_ as a tuple \(a_{i}(A_{i},P_{i})\), where \(A_{i}^{(K D)}\) is a flattened sequence of \(K\)\(D\)-dimensional vectors describing the appearance of an asset, and \(P_{i}^{D^{}}\) is a \(D^{}\)-dimensional embedding of the asset's pose in a scene. In other words, a Neural Asset is fully described by learnable embedding vectors, factorized into appearance and pose. This factorization enables independent control over appearance and pose of an asset, similar to how 3D object models can be controlled in traditional computergraphics software. Importantly, besides the 3D pose of assets, our approach does not require any explicit mapping of objects into 3D, such as depth maps or the NeRF representation .

#### 3.2.1 Asset Encoding

In the following, we describe how both the appearance \(A_{i}\) and the pose \(P_{i}\) of a Neural Asset \(a_{i}\) are obtained from visual observations (such as an image or a frame in a video). Importantly, the appearance and pose representations are not necessarily encoded from the same observation, i.e., they can be encoded from two separate frames sampled from a video. We find this strategy critical to learn disentangled and controllable representations, which we will discuss in detail in Sec. 3.3.

**Appearance encoding.** At a high level, we wish to obtain a set of \(N\) Neural Asset appearance tokens \(A_{i}\) from a visual observation \(x_{}\), where \(x_{}\) can be an image or a frame in a video. While one could approach this problem in a fully-unsupervised fashion, using a method such as Slot Attention  to decompose an image into a set of object representations, we choose to use readily-available annotations to allow fine-grained specification of objects of interest. In particular, we assume that a 2D bounding box \(b_{i}\) is provided for each Neural Asset \(a_{i}\), specifying which object should be extracted from \(x_{}\). Therefore, we obtain the appearance representation \(A_{i}\) as follows:

\[A_{i}=((H_{i},b_{i}))\,, H_ {i}=(x_{})\,,\] (1)

where \(H_{i}\) is the output feature map of a visual encoder \(\). \(\) extracts a fixed size feature map using the provided bounding box \(b_{i}\) which is flattened to form the appearance token \(A_{i}\). This factorization allows us to extract \(N\) object appearances from an image with just one encoder forward pass. In contrast, previous methods  crop each object out to extract features separately, and thus requires \(N\) encoder passes. This becomes unaffordable if we jointly fine-tune the visual encoder, which is key to learning generalizable features as we will show in the ablation study.

**Pose encoding.** The pose token \(P_{i}\) of a Neural Asset \(a_{i}\) is the primary interface for controlling the presence and 3D pose of an object in the rendered scene. In this work, we assume that the object pose is provided in terms of a 3D bounding box, which fully specifies its location, orientation, and size in the scene. Formally, we take four corners spanning the 3D bounding box1 and project them to the image plane to get \(\{c_{i}^{j}=(h_{i}^{j},w_{i}^{j},d_{i}^{j})\}_{j=1}^{4}\), with the projected 2D coordinate \((h_{i}^{j},w_{i}^{j})\), and the 3D depth \(d_{i,j}\). We obtain the pose representation \(P_{i}\) for a Neural Asset as follows:

\[P_{i}=(C_{i})\,, C_{i}=[c_{i}^{1}, c_{i}^{2},c_{i}^{3},c_{i}^{4}]\,,\] (2)

where we first concatenate the four corners \(c_{i}^{j}\) to form \(C_{i}^{12}\), and then project it to \(P_{i}^{D^{}}\) via an MLP. We tried the Fourier coordinate encoding in prior works  but did not find it helpful.

There are alternative ways to represent 3D bounding boxes (e.g., concatenation of center, size, and rotation commonly used in 3D object detection ), which we compare in Appendix B.4. In this

Figure 2: **Neural Assets framework**. (a) We train our model on pairs of video frames, which contain objects under different poses. We encode appearance tokens from a source image with \(\), and pose tokens from the objects’ 3D bounding boxes in a target image. They are combined to form our Neural Asset representations. (b) An image diffusion model is conditioned on Neural Assets and a separate background token to reconstruct the target image as the training signal. (c) During inference, we can manipulate the Neural Assets to control the objects in the generated image: rotate the object’s pose (blue) or replace an object by a different one from another image (pink).

work, we assume the availability of training data with 3D annotations - obtaining high-quality 3D object boxes for videos at scale is still an open research problem, but may soon be within reach given recent progress in monocular 3D detection , depth estimation , and pose tracking .

**Serialization of multiple Neural Assets.** We encode a set of \(N\) Neural Assets into a sequence of tokens that can be appended to or used in place of text embeddings for conditioning a generative model. In particular, we first concatenate the appearance token \(A_{i}\) and the pose token \(P_{i}\) channel-wise, and then linearly project it to obtain a Neural Asset representation \(a_{i}\) as follows:

\[a_{i}=(_{i})\,,_{i}=[A_{ i},P_{i}]^{K D+D^{}}\,.\] (3)

Channel-wise concatenation uniquely binds one pose token with one appearance representation in the presence of multiple Neural Assets. An alternative solution is to learn such association with positional encoding. Yet, it breaks the permutation-invariance of the generator against the order of input objects and leads to poor results in our preliminary experiments. Finally, we simply concatenate multiple Neural Assets along the token axis to arrive at our token sequence, which can be used as a drop-in replacement for a sequence of text tokens in a text-to-image generation model.

**Background modeling.** Similar to prior works , we found it helpful to encode the scene background separately, which enables independent control thereof (e.g., swapping out the scene, or controlling global properties such as lighting). We choose the following heuristic strategy to encode the background: to avoid leakage of foreground object information, we mask all pixels within asset bounding boxes \(b_{i}\). We then pass this masked image through the image encoder \(\) (shared weights with the foreground asset encoder) and apply a global \(\), i.e., using the entire image as region of interest, to obtain a background appearance token \(A_{}^{(K D)}\). Similar to a Neural Asset, we also attach a pose token \(P_{}\) to \(A_{}\). This can either be a timestep embedding of the video frame (relative to the source frame) or a relative camera pose embedding, if available. In the serialized representations, the background token is treated the same as Neural Assets, i.e., we concatenate \(A_{}\) and \(P_{}\) channel-wise and linearly project it. Finally, the foreground assets \(a_{i}\) and the background token are concatenated along the token dimension and used to condition the generator.

#### 3.2.2 Generative Decoder

To generate images from Neural Assets, we make minimal assumptions about the architecture or training setup of the generative image model to ensure compatibility with future large-scale pre-trained image generators. In particular, we assume that the generative image model accepts a sequence of tokens as conditioning signal: for most base models this would be a sequence of tokens derived from text prompts, which we can easily replace with a sequence of Neural Asset tokens.

As a representative for this class of models, we adopt Stable Diffusion v2.1  for the generative decoder. See Appendix C for details on this model. Starting from the pre-trained text-to-image checkpoint, we fine-tune the entire model end-to-end to accept Neural Assets tokens instead of text tokens as conditioning signal. The training and inference setup is explained in the following section.

### Learning and Inference

**Learning from frame pairs.** As outlined in the introduction, we require a scalable data source of object-level "edits" in 3D space to effectively learn multi-object 3D control capabilities. Video data offers a natural solution to this problem: as the camera and the content of the scene moves or changes over time, objects are observed from various view points and thus in various poses and lighting conditions . We exploit this signal by randomly sampling pairs of frames from video clips, where we take one frame as the "source" image \(x_{}\) and the other frame as the "target" image \(x_{}\).

As described earlier, we obtain the appearance token \(A_{i}\) of Neural Assets from the _source_ frame \(x_{}\) by extracting object features using 2D box annotations. Next, we obtain the pose token \(P_{i}\) for each extracted asset from the _target_ frame \(x_{}\), for which we need to identify the correspondences between objects in both frames. In practice, such correspondences can be obtained, for example, by applying an object tracking model on the underlying video. Finally, with the associated appearance and pose representations, we condition the image generator on them and train it to reconstruct the target frame \(x_{}\), i.e., using the denoising loss of Stable Diffusion v2.1 in our case. Such a paired frame training strategy forces the model to learn an appearance token that is invariant to object pose and leverage the pose token to synthesize the new object, avoiding the trivial solution of simple pixel-copying.

**Test-time controllability.** The learned disentangled representations naturally enable multi-object scene-level editing as we will show in Sec. 4.3. Since we encode 3D bounding boxes to pose tokens \(P_{i}\), we can move, rotate, and rescale objects by changing the box coordinates. We can also compose Neural Assets \(a_{i}\) across scenes to generate new scenes. In addition, our background modeling design supports swapping the environment map of the scene. Importantly, as we will see in the experiments, our image generator learns to naturally blend the objects into their new environment at new positions, with realistic lighting effects such as rendering and adapting shadows correctly.

## 4 Experiments

In this section, we conduct extensive experiments to answer the following questions: **(i)** Can Neural Assets enable accurate 3D object editing? **(ii)** What practical applications does our method support on real-world scenes? **(iii)** What is the impact of each design choice in our framework?

### Experimental Setup

**Datasets.** We select four datasets with object or camera motion, which span different levels of complexity. _OBJect_ is introduced in 3DIT , which is one of our baselines. It contains 400k synthetic scenes rendered by Blender  with a static camera. Up to four Objaverse  assets are placed on a textured ground and only one object is randomly moved on the ground. For a fair comparison with 3DIT, we use 2D bounding boxes plus rotation angles as object poses, and follow them to base our model on Stable Diffusion v1.5 . _MOVi-E_ consists of Blender simulated videos with up to 23 objects. It is more challenging than OObject as its linear camera motion and there can be multiple objects moving simultaneously. _Objectron_ is a big step up in complexity as it captures real-world objects with complex backgrounds. 15k object-centric videos covering objects from nine categories are recorded with 360\({}^{}\) camera movement. _Waymo Open_ is a real-world self-driving dataset captured by car mounted cameras. We follow prior work  to use only the front view and filter out cars that are too small. See Appendix A.1 for more details on datasets.

**Baselines.** We compare to methods that can perform 3D-aware editing on existing images and have released their code. _3DIT_ fine-tunes Zero-1-to-3  on the OBJECT dataset to support translation and rotation of objects. However, it cannot render big viewpoint changes as it does not encode camera poses. Following , we create another baseline (dubbed _Chained_) by using SAM  to segment the object of interest, removing it using Stable Diffusion inpainting model , running Zero-1-to-3

Figure 4: **Multi-object editing results on MOVi-E, Objectron, and Waymo Open (denoted as Waymo in the figures). We compute metrics inside the edited objects’ bounding boxes.**

Figure 3: **Single-object editing results on OBJect unseen object subset. We evaluate on the _Translation_, _Rotation_, and _Removal_ tasks. We follow 3DIT  to compute metrics inside the edited object’s bounding box. Our results are averaged over 3 random seeds.**to rotate and scale the object, and stitching it to the target position. Since none of these baselines can control multiple objects simultaneously, we apply them to edit all objects sequentially.

**Evaluation settings.** We report common metrics to measure the quality of the edited image - PSNR, SSIM , LPIPS , and FID . Following prior works , we also compute object-level metrics on cropped out image patches of edited objects. To evaluate the fidelity of edited objects, we take the DINO  feature similarity metric proposed in . On video datasets, we randomly sample source and target images in each testing video and fix them across runs for consistent results.

**Implementation Details.** For all experiments, we resize images to \(256 256\). DINO self-supervised pre-trained ViT-B/8  is adopted as the visual encoder \(\), and jointly fine-tuned with the generator. All our models are trained using the Adam optimizer  with a batch size of \(1536\) on 256 TPUv4 chips. For inference, we generate images by running the DDIM  sampler for 50 steps. For more training and inference details, please refer to Appendix A.4.

### Main Results

**Single-object editing.** We first compare the ability to control the 3D pose of a single object on the OBJect dataset. Fig. 3 presents the results on the unseen object subset. We do not show FID here as it mainly measures the visual quality of generated examples, which does not reflect the editing accuracy.

Figure 5: **Qualitative comparison on MOVi-E, Objectron, and Waymo Open**. All models generate a new image given a source image and the 3D bounding box of target objects. Our method performs the best in object identity preservation, editing accuracy, and background modeling.

For results on the seen object subset and FID, please refer to Appendix B.1, where we observe similar trends. Compared to baselines, our model does not condition on text (e.g., the category name of the object to edit) as in 3DIT and is not trained on curated multi-view images of 3D assets as in Zero-1-to-3. Still, we achieve state-of-the-art performance on all three tasks. This is because our Neural Assets representation learns disentangled appearance and pose features, which is able to preserve object identity while changing its placement smoothly. Also, the fine-tuned DINO encoder generalizes better to unseen objects compared to the frozen CLIP visual encoder used by baselines.

**Multi-object editing.** Fig. 4 shows the results on MOVi-E, Objectron, and Waymo Open, where multiple objects are manipulated in each sample. Similar to the single-object case, we compute metrics inside the object bounding boxes, and leave the image-level results to Appendix B.1. Our model outperforms baselines by a sizeable margin across datasets. Fig. 5 presents the qualitative results. When there are multiple objects of the same class in the scene (e.g., boxes in the MOVi-E example and cars on Waymo Open), 3DIT is unable to edit the correct instance. In addition, it generalizes poorly to real-world scenes. Thanks to the object cropping step, Chained baseline can identify the correct object of interest. However, the edited object is simply pasted to the target location, leading to unrealistic appearance due to missing lighting effects such as shadows. In contrast, our model is able to control all objects precisely, preserve their fidelity, and blend them into the background naturally. Since we encode the camera pose, we can also model global viewpoint change as shown in the third row. See Appendix B.1 for additional qualitative results.

### Controllable Scene Generation

In this section, we show versatile control of scene objects on Waymo Open. For results on Objectron, please refer to Appendix B.3. As shown in Fig. 6, we can translate and rotate cars in driving scenes. The model understands the 3D world as objects zoom in and out when moving, and show consistent

Figure 6: **Object translation and rotation** by manipulating 3D bounding boxes on Waymo Open. See our project page for videos and additional object rescaling results.

Figure 7: **Compositional generation results** on Waymo Open. By composing Neural Assets, we can remove and segment objects, as well as transfer and recompose objects between scenes.

novel views when rotating. Fig. 7 presents our ability of compositional generation, where objects are removed, segmented out, and transferred across scenes. Notice how the model handles occlusion and inpaints the scene properly. Finally, Fig. 8 demonstrates background swapping between scenes. The generator is able to harmonize objects with the new environment. For example, the car lights are turned on and rendered with specular highlight when using a background image from a night scene.

### Ablation Study

We study the effect of each component in the model. All ablations are run on Objectron since it is a real-world dataset with complex background, and has higher object diversity than Waymo Open.

**Visual encoder.** Previous image-conditioned diffusion models [49; 61; 62] usually use the frozen image encoder of CLIP  to extract visual features. Instead, as shown in Fig. 8(a), we found that both MAE  and DINO  pre-trained ViTs give better results. This is because CLIP's image encoder only captures high-level semantics of images, which suffices in single-object tasks, but fails in our multi-object setting. In contrast, MAE and DINO pre-training enable the model to extract more fine-grained features. Besides, DINO outperforms MAE as its features contain richer 3D information, which aligns with recent research . Finally, jointly fine-tuning the image encoder learns more generalizable appearance tokens in Neural Assets, leading to the best performance.

**Background modeling.** We compare our full model with two variants: **(i)** not conditioning on any background tokens (dubbed _No-BG_), and **(ii)** conditioning on background appearance tokens but not using relative camera pose as pose tokens (dubbed _No-Pose_). As shown in Fig. 8(b), our background modeling strategy performs the best in image-level metrics as backgrounds usually occupy a large part of real-world images. Interestingly, our method also achieves significantly better object-level metrics. This is because given background appearance and pose, the model does not need to infer them from object tokens, leading to more disentangled Neural Assets representations.

Figure 8: **Transfer backgrounds between scenes by replacing the background token on Waymo Open. The objects can adapt to new environments, e.g., the car lights are turned on at night.**

Figure 9: **Comparison of (a) visual encoders, (b) background modeling, and (c) training strategies on Objectron. Bold entry denotes our full model. See text for each variant. We report PSNR and LPIPS computed within object bounding boxes, and leave other metrics to Appendix B.2.**

**Training strategy.** As described in Sec. 3.3, we train on videos and extract appearance and pose tokens from different frames. We compare such design with training on a single frame in Fig. 9. Our paired frame training strategy clearly outperforms single frame training. Since the appearance token is extracted by a ViT with positional encoding, it already contains object position information, which acts as a shortcut for image reconstruction. Therefore, the model ignores the input object pose token, resulting in poor controllability. One way to alleviate this is removing the positional encoding in the image encoder (dubbed _NO-PE_), which still underperforms paired frame training. This is because to reconstruct objects with visual features extracted from a different frame, the model is forced to infer their underlying 3D structure instead of simply copying pixels. In addition, the generator needs to render realistic lighting effects such as shadows under the new scene configuration.

## 5 Conclusion

In this paper, we present Neural Assets, vector-based representations of objects and scene elements with disentangled appearance and pose features. By connecting with pre-trained image generators, we enable controllable 3D scene generation. Our method is capable of controlling multiple objects in the 3D space as well as transferring assets across scenes, both on synthetic and real-world datasets. We view our work as an important step towards general-purpose neural-based simulators.

**Limitations and future works.** One main failure case of our model is symmetry ambiguity. As can be seen from the rotation results in Fig. 10 (a), the handle of the cup gets flipped when it rotates by 180 degree. Another failure case that only happens on Objectron is the entanglement of global camera motion and local object movement (Fig. 10 (b)). This is because Objectron videos only contain camera motion while objects always stay static. Both issues will likely be resolved if we train our model on larger-scale datasets with more diverse object and camera motion.

An ideal Neural Asset should enable control over all potential configurations of an object such as deformation (e.g., a walking cat), rigid articulation (e.g., opening of a scissor), and structural decomposition (e.g., tomatoes being cut). In this work, we first tackle the foremost important aspect, i.e., controlling 3D rigid object pose and background composition which applies to almost all the objects. Hence our current method does not allow for controlling structural changes. However, it can be adapted when suitable datasets are developed that capture other changes in objects.

Another limitation is that our approach is currently limited to existing datasets that have 3D bounding box annotations. Yet, with recent advances in vision foundation models [9; 55; 113], we may soon have scalable 3D annotation pipelines similar to their 2D counterparts. One notable example is OmniNOCS , which works on both Waymo and Objectron (datasets we used in this work), and _diverse, in-the-wild Internet images_ for a wide range of object classes. It can be used to create larger open domain datasets to learn Neural Assets. We see this as an interesting future direction.