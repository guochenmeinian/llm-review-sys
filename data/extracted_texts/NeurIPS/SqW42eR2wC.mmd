# Offline Inverse Constrained Reinforcement Learning

for Safe-Critical Decision Making in Healthcare

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Reinforcement Learning (RL) applied in healthcare can lead to unsafe medical decisions and treatment, such as excessive dosages or abrupt changes, often due to agents overlooking common-sense constraints. Consequently, Constrained Reinforcement Learning (CRL) is a natural choice for safe decisions. However, specifying the exact cost function is inherently difficult in healthcare. Recent Inverse Constrained Reinforcement Learning (ICRL) is a promising approach that infers constraints from expert demonstrations. ICRL algorithms model Markovian decisions in an interactive environment. These settings do not align with the practical requirement of a decision-making system in healthcare, where decisions rely on historical treatment recorded in an offline dataset. To tackle these issues, we propose the Constraint Transformer (CT). Specifically, 1) utilize causal attention mechanism to incorporate historical decisions and observations into the constraint modeling and employ a non-Markovian layer for weighted constraints to capture critical states, 2) generative world model to perform exploratory data augmentation, thereby enabling offline RL methods to generate unsafe decision sequences. In multiple medical scenarios, empirical results demonstrate that CT can capture unsafe states and achieve strategies that approximate lower mortality rates, reducing the occurrence probability of unsafe behaviors.

## 1 Introduction

In recent years, the doctor-to-patient ratio imbalance has drawn attention, with the U.S. having only 223.1 physicians per 100,000 people . AI-assisted therapy emerges as a promising solution, offering timely diagnosis, personalized care, and reducing dependence on experienced physicians. Therefore, the development of an effective AI healthcare assistant is crucial.

Reinforcement learning (RL) offers a promising approach to develop AI assistants by addressing sequential decision-making tasks. However, this method can still lead to unsafe behaviors, such as administering excessive drug dosages, inappropriate adjustments of medical parameters, or abrupt changes in medication dosages. These behaviors, such as **"too high"** or **"sudden change"** can significantly endanger patients, potentially resulting in acute hypotension, hypertension, arrhythmias, and organ damage, with fatal consequences . For example, in sepsis treatment, patients receiving vasopressors (vaso) at dosages exceeding \(1 g/(kg min)\) have a mortality rate of \(90\%\). Moreover, the **"sudden change"** in vaso can rapidly affect blood vessels, causing acute fluctuations in blood pressure and posing life-threatening risks to patients . Our experiments demonstrate that the work

 Drug dosage (\( g/(kg min)\)) & Physician & DDPG \\  \(>0.75\\ >0.9\) & \(2.27\%\) & \(7.44\%\)\(\) \\  \(>0.75\\ >0.9\) & \(1.71\%\) & \(7.40\%\)\(\) \\  \(>0.75\\ >0.9\) & \(2.45\%\) & \(21.00\%\)\(\) \\  \(>0.9\) & \(1.88\%\) & \(20.62\%\)\(\) \\  

Table 1: The proportion of unsafe behaviors occurrences in vaso suggested by physician and DDPG. The typical range for vaso is \(0.1 0.2 g/(kg min)\), with doses exceeding \(0.5\) considered high . A cutoff value of \(0.75\) is identified as a critical threshold associated with increased mortality .

 applying the Deep Deterministic Policy Gradient (DDPG) algorithm in sepsis indeed exhibits **"too high"** and **"sudden change"**1 unsafe behaviors in vaso recommendations, as shown in Table 1.

This paper aims to achieve safe healthcare policy learning to mitigate unsafe behaviors. The most common method for learning safe policies is Constrained Reinforcement Learning (CRL) [10; 11], with the key to its success lying in the constraints representation. However, in healthcare, we can only design the cost function based on prior knowledge, which limits its application due to a lack of personalization, universality, and reliance on prior knowledge. For more details about issues, please refer to Appendix A. Therefore, Inverse Constrained Reinforcement Learning (ICRL)  emerges as a promising approach, as it can infer the constraints adhered to by experts from their demonstrations. However, directly applying ICRL in healthcare presents several challenges:

**1) The Markov decision is not compatible with medical decisions.** ICRL algorithms model Markov decisions, where the next state depends only on the current state and not on the history [13; 14]. However, in healthcare, the historical states of patients are crucial for medical decision-making , as demonstrated in the experiments shown in Figure 1. Therefore, ICRL algorithms based on Markov assumption can not capture patient history, and ignore individual patient differences, thereby limiting effectiveness.

**2) Interactive environment is not available for healthcare or medical decisions.** ICRL algorithms [12; 16] follow an online learning paradigm, allowing agents to explore and learn from interactive environments. However, unrestricted exploration in healthcare often entails unsafe behaviors that could breach constraints and result in substantial losses. Therefore, it is necessary to infer constraints using only offline datasets.

In this paper, we introduce offline Constraint Transformer (CT), a novel ICRL framework that incorporates patients' historical information into constraint modeling and learns from offline data to infer constraints in healthcare. Specifically,

1) Inspired by the recent success of transformers in sequence modeling [17; 18; 19], we incorporate historical decisions and observations into constraint modeling using a causal attention mechanism. To capture key events in trajectories, we introduce a non-Markovian transformer to generate constraints and importance weights, and then define constraints using weighted sums. CT takes trajectories as input, allowing for the observation of patients' historical information and evaluation of key states.

2) To learn from an offline dataset, we introduce a model-based offline RL method that simultaneously learns a policy model and a generative world model via auto-regressive imitation of the actions and observations in medical decisions. The policy model employs a stochastic policy with entropy regularization to prevent it from overfitting and improve its robustness. Utilizing expert datasets, the generative world model uses an auto-regressive exploration generation paradigm to effectively discover a set of violating trajectories. Then, CT can infer constraints in healthcare through these unsafe trajectories and expert trajectories.

In the medical scenarios of sepsis and mechanical ventilation, we conduct experimental evaluations of offline CT. Experimental evaluations demonstrate that offline CT can capture patients' unsafe states and assign higher penalties, thereby providing more interpretable constraints compared to previous works [9; 20; 21]. Compared to unconstrained and custom constraints, CT achieves strategies that closely approximate lower mortality rates with a higher probability (improving by \(8.85\%\) compared to DDPG). To investigate the avoidance of unsafe behaviors with offline CT, we evaluate the probabilities of "too high" and "sudden changes" occurring in the sepsis. The experimental results show that CRL with CT can reduce the probability of unsafe behaviors to zero.

## 2 Related Works

**Reinforcement Learning in Healthcare.** RL has made great progress in the realm of healthcare, such as sepsis treatment [9; 20; 21; 22], mechanical ventilation [23; 24; 25], sedation  and anesthesia

Figure 1: The distribution of vaso for patients with the same state. The physician makes different decisions due to reerencing historical information, while the agent based on Markov decision-making can only make the same decision.

. However, these works mentioned above have not addressed potential safety issues such as sudden changes or too high doses of medication. Therefore, the development of policies that are both safe and applicable across various healthcare domains is crucial.

**Inverse Constrained Reinforcement Learning.** Previous works inferred constraint functions by determining the feasibility of actions under current states. In discrete state-action spaces, Chou _et al._ and Park _et al._ learned constraint sets to differentiate constrained state-action pairs. Scobee & Sastry  proposed inferring constraint sets based on the principle of maximum entropy, while some studies  extended this approach to stochastic environments using maximum causal entropy . In continuous domains, Malik _et al._, Gaurav _et al._, and Qiao _et al._ used neural networks to approximate constraints. Some works  applied Bayesian Monte Carlo and variational inference to infer the posterior distribution of constraints in high-dimensional state spaces. Xu _et al._ modeled uncertainty perception constraints for arbitrary and epistemic uncertainties. However, these methods can only be applied online and lack historical dependency.

**Transformers for Reinforcement Learning.** Transformer has produced exciting progress on RL sequential decision problems . These works no longer explicitly learn Q-functions or policy gradients, but focus on action sequence prediction models driven by target rewards. Chen _et al._ and Janner _et al._ perform auto-regressive modeling of trajectories to achieve policy learning in an offline environment. Furthermore, Zheng _et al._ unify offline pretraining and online fine-tuning within the Transformer framework. Liu _et al._ and Kim _et al._ integrate the transformer architecture into constraint learning and preference learning. The transformer architecture, with its sequence modeling capability and independence from the Markov assumption, can capture temporal dependencies in medical decision-making. Thus, it is well-suited for trajectory learning and personalized learning in medical settings.

## 3 Problem Formulation

We model the medical environment with a Constrained Markov Decision Process (CMDP) \(^{c}\), which can be defined by a tuple \((,,,,,,, _{0})\). Similar to studies , we extract data within \(72\) hours of patient admission, with each \(4\)-hour interval constituting a window or time step. The state indicators of the patient at each time step are denoted as \(s\). The administered drug doses or instrument parameters of interest are considered as actions \(a\), while reward function \(\) is used to describe the quality of the patient's condition and provided by experts based on prior work . At each time step \(t\), an agent performs an action \(a_{t}\) at a patient's state \(s_{t}\). This process generates the reward \(r_{t}(s_{t},a_{t})\), the cost \(c_{t}\) and the next state \(s_{t+1}( s_{t},a_{t})\), where \(\) defines the transition probabilities. \(\) denotes the discount factor. \(_{+}\) denotes the bound of cumulative costs. \(_{0}\) defines the initial state distribution. The goal of the CRL policy \(\) is to maximize the reward return while limiting the cost in a threshold \(\):

\[_{}_{,_{0}}[_{t=1}^{T}^{t}r_{t}], _{,_{0}}[_{t=1}^{T}^{t}c_{t}].\] (1)

where \(T\) is the length of the trajectory \(\). CRL commonly assumes that constraint signals are directly observable. However, in healthcare, such signals are not easily obtainable. Therefore, Our objective is to infer reasonable constraints for CRL to achieve safe policy learning in healthcare.

**Safe-Critical Decision Making with Constraint Inference in Healthcare.** Our general goal is for our policy to approximate the optimal policy, which refers to the strategy under which the patient's mortality rate is minimized (achieving a zero mortality rate is often difficult since there are patients who can not recover, regardless of all potential future treatment sequences ). Decision-making with constraints can formulate safer strategies by discovering and avoiding unsafe states, thereby approaching the optimal policy.

However, most offline RL algorithms rely on online evaluation, where the agent is evaluated in an interactive environment, whereas in medical scenarios, only offline evaluation can be utilized. In previous works , they qualitatively analyzed by comparing the differences (DIFF) between the drug dosage recommended by our policy \(\) and the dosage administered by clinical physicians \(\), and its relationship with mortality rates, through graphical analysis. In the graph depicting the relationship between the DIFF and mortality rate, at the point when DIFF is zero, the lower the mortality rate of patients, the better the performance of the policy . To provide a more accurate quantitative evaluation, we introduce the concept of the probability of approaching the optimal policy, defined as \(\):

\[=}{N}\] (2)We randomly collect \(2N\) patients (with an equal number of known survivors and non-survivors under doctor's policy \(\)) from the offline dataset. We then calculate the DIFF and sort it in ascending order. The optimality of the policy can be evaluated through the following two points: 1) The higher the survival probability (i.e., \(\)) of the top \(N\) patients, the lower the mortality rate can be achieved by executing \(\); 2) The smaller the DIFF among the surviving patients in the top \(N\), the greater the probability that \(\) is optimal.

## 4 Method

To infer constraints and achieve safe decision-making in healthcare, we introduce the Offline Constraint Transformer (Figure 2), a novel ICRL framework.

**Inverse Constrained Reinforcement Learning.** ICRL aims to recover the cost function \(^{*}\) by leveraging a set of trajectories \(_{e}=\{_{e}^{(i)}\}_{i}^{N}\) sampled from an expert policy \(_{e}\), where \(N\) denotes the number of the trajectories. ICRL is commonly based on the Maximum Entropy framework , and the likelihood function is articulated as :

\[p(_{e})=^{}})^{N }}_{i=1}^{N}[R(^{(i)})]^{^{ }}(^{(i)})\] (3)

Here, \(Z_{}=( r())^{}()d\) is the normalizing term. The indicator \(^{^{}}(^{(i)})\) signifies the extent to which the trajectory \(^{(i)}\) satisfies the constraints. It can be approximated using a neural network \(_{}(^{(i)})\) parameterized with \(\), defined as \(_{}(^{(i)})=_{t=0}^{T}_{}(s_{t}^{i},a_{t}^{i})\). Consequently, the cost function can be formulated as \(C_{}=1-_{}\). Substituting the neural network for the indicator, we can update \(\) through the gradient of the log-likelihood function:

\[_{}()=_{^{(i)}_ {e}}[_{}[_{}(^{(i)})]]-_ {_{}^{}}}[_{} [_{}(^{(i)})]]\] (4)

where \(^{_{}}\) denotes the MDP obtained after augmenting \(\) with the cost function \(C_{}\), using the executing policy \(_{}^{}}\). And \(\) are sampled from the policy. In practice, ICRL can be conceptualized as a bi-level optimization task . We can 1) update this policy based on Equation 1, and 2) employ Equation 4 for constraint learning. Intuitively, the objective of Equation 4 is to distinguish between trajectories generated by expert policies and imitation policies that may violate the constraints.

Specifically, task 1) involves updating the policy using advanced CRL methods. Significant progress has been made in some works such as BCQ-Lagrangian (BCQ-Lag), COpiDICE , VOCE , and CDT . Meanwhile, task 2) focuses on learning the constraint function, as shown in Figure 2. Our research primarily improves the latter process due to two main challenges facing ICRL in healthcare: **Challenge 1)** pertains to the limitations of the Markov property, and **Challenge 2)** involves the issue of inferring constraints only from offline datasets. To address these challenges, we propose the offline CT as our solution.

Figure 2: The overview of the safe healthcare policy learning with offline CT.

Offline Constraint Transformer.To address the first challenge, we delve into the inherent issues of applying the Markov property to healthcare and draw inspiration from the successes of Transformer in decision-making, redefining the representation of the constraints. To realize the offline training, we consider the essence of ICRL updates, proposing a model-based RL to generate unsafe behaviors used to train CT. We outline three parts: establishing the constraint representation model (Section 4.1), creating an offline RL for violating data (Section 4.2), and learning safe policies (Section 4.3).

### Constraint Transformer

ICRL methods relying on the Markov property overlook patients' historical information, focusing only on the current state. However, both current and historical states, along with vital sign changes are crucial for a human doctor's decision-making process . To emulate the observational approach of humans, we draw inspiration from the Decision Transformer (DT)  to incorporate historical information into constraints for a more comprehensive observation and judgment. We propose a constraint modeling approach based on a causal attention mechanism, as shown in Figure 3. The structure comprises a causal Transformer for sequential modeling and a non-Markovian layer for weighted constraints learning.

Sequential Modeling for Constraints Inference.For a trajectory segment of length \(T\), \(2T\) input embeddings are generated, with each position containing state \(s\) and action \(a\) embeddings. Additionally, these embeddings undergo linear and normalization layers before being fed into the causal Transformer, which produces output embeddings \(\{d_{t}\}_{t=1}^{T}\) determined by preceding input embeddings from \((s_{1},a_{1},...,s_{T},a_{T})\). Here, \(d_{t}\) depends only on the previous \(t\) states and actions.

Modeling Non-Markovian for Weighted Constraints Learning.Although \(d_{t}\) represents the cost function \(c_{t}\) derived from observations over long trajectories, it doesn't pinpoint which previous key actions or states led to its increase. In healthcare, identifying key actions or states is vital for analyzing risky behaviors and status, and enhancing model interpretability. To address this, we draw inspiration from the design of the preference attention layer in  and introduce an additional attention layer. This layer is employed to define the cost weight for non-Markovians. It takes the output embeddings from the causality transformer as input and generates the corresponding cost and importance weights. The output of the attention layer is computed by weighting the values through the normalized dot product between the query and other keys:

\[_{t=1}^{T}(\{ q_{t},k_{t^{}}\}_ {t^{}=1}^{T})_{t} c_{t}=_{t=1}^{T}w_{t} c_{t}\] (5)

Here, the key \(k_{t}^{m}\), query \(q_{t}^{m}\), and value \(c_{t}^{m}\) are derived from the \(t\)-th input \(d_{t}\) through linear transformations, where \(m\) denotes the embedding dimension. Furthermore, for each time step \(t\), since \(d_{t}\) depends only on the previous state-action pairs \(\{(s_{i},a_{i})\}_{i=1}^{t}\) and serves as the input embedding for the attention layer, \(c_{t}\) is also associated solely with the preceding \(t\) time steps. The representation of the cost function as a weighted sum is defined as \(C()=_{t=1}^{T}w_{t} c_{t}\). Then, we can also determine the constraint function values for each preceding subsequence. Introducing the newly defined cost function, we redefine Equation 4 for CT as:

\[_{}()=_{_{v}}[_{}[C_{}()]]-_{ _{v}}[_{}[\,C_{}()]]\] (6)

where \(\) is the parameter of CT, \(_{e}\) and \(_{v}\) represent the expert data and the violating data. This formulation implies that the constraint should be minimized on the expert policy and maximized on the violating policy. We construct an expert and a violating dataset to evaluate Equation 6 in offline. The expert data can be acquired from existing medical datasets or hospitals. Regarding the violating dataset, we introduce a generative model to establish it, as detailed in Section 4.2.

### Model-based Offline RL

Figure 3: The structure of the Constraint Transformer.

To train CT offline, we introduce a model-based offline RL method (Figure 4) to generate violating data that refers to unsafe behavioral data and can be represented as \(_{v}=(s_{1},a_{1},r_{1},s_{2},...)_{v}\). The model simultaneously learns a policy model and a generative world model via auto-regressive imitation of the actions and observations in healthcare. The model processes a trajectory, \(_{e}_{e}\), as a sequence of tokens encompassing the return-to-go, states, and actions, defined as \((_{1},s_{1},a_{1},...,_{T},s_{T},a_{T})\). Notably, the return-to-go \(_{t}\) at timestep \(t\) is the sum of future rewards, calculated as \(_{t}=_{t^{}=t}^{T}_{t^{}}\). At each timestep \(t\), it employs the tokens from the preceding \(K\) timesteps as its input, where \(K\) represents the context length. Thus, the input tokens for it at timestep \(t\) are denoted as \(h_{t}=\{_{-K:t},s_{-K:t},a_{-K:t-1}\}\), where \(_{-K:t}=\{_{K},...,_{t}\}\), \(s_{-K:t}=\{s_{K},...,s_{t}\}\) and \(a_{-K:t-1}=\{a_{K},...,a_{t-1}\}\).

**Policy Model.** The input tokens are encoded through a linear layer for each modality. Subsequently, the encoded tokens pass through a casual transformer to predict future action tokens. We use a stochastic policy  to achieve policy learning. Additionally, we utilize a Shannon entropy regularizer \([_{}( h)]\) to prevent policy overfitting and enhance robustness. The optimization objective is to minimize the negative log-likelihood loss while maximizing the entropy with weight \(\):

\[_{}_{h_{t}_{e}}[-_{ }( h_{t})-[_{}(  h_{t})]]\] (7)

where the policy \(_{}( h_{t})=(_{} (h_{t}),_{}(h_{t}))\) adopts the stochastic Gaussian policy representation and \(\) is the parameter.

**Generative World Model.** To predict states and rewards, we use \(x_{t}=\{h_{t} a_{t}\}\) as input encoded by linear layers. The encoded tokens pass through the casual transformer to predict hidden tokens. Then we utilize two linear layers to fit the rewards and states. The optimization objective for the two linear layers \(\) with the parameters \(\) and \(\) can be defined as:

\[_{,}_{s_{t},r_{t-1} x_{t}_{e} }[(s_{t}-_{}(x_{t}))^{2}+(r_{t-1}-_{}(x_{t}))^{2}]\] (8)

**Generating Violating Data.** In RL, excessively high rewards, surpassing those provided by domain experts, may incentivize agents to violate the constraints in order to maximize the total reward . Therefore, we set a high initial target reward \(_{1}\) to obtain violation data. We feed \(_{1}\) and initial state \(s_{1}^{(i)}\) into the model-based offline RL to generate \(_{v}^{(i)}\) in an auto-regressive manner, as depicted in model-based offline RL of Figure 2, where \(\), \(\) and \(\) are predicted by the model. The target reward \(\) decreases incrementally and can be represented as \(_{t+1}=_{t}-_{t}\). Considering the average error in trajectory prediction, we generate trajectories with the length \(K=10\), as detailed in Appendix B.3. Repeating \(N\) initial states, we can get violating data \(_{v}=\{_{v}^{(i)}\}_{i=1}^{N}\).

Note that certain other generative models, such as Variational Auto-Encoder (VAE) , Generative Adversarial Networks (GAN) [46; 47], and Denoising Diffusion Probabilistic Models (DDPM) [48; 49], may be better at generating data. We introduce the model-based offline RL primarily because it has been shown to generate violating data with exploration  and possess the ability to process time-series features efficiently.

### Safe-Critical Decision Making with Constraints.

To train offline CT, we gather the medical expert dataset \(_{e}\) from the environment. Then, we employ gradient descent to train the model-based offline RL, guided by Equation 7 and Equation 8, continuing until the model converges. Using this RL model, we automatically generate violating data denoted as \(_{v}\). Subsequently, CT is optimized based on Equation 6 to get the cost function \(C\), leveraging samples from both \(_{e}\) and \(_{v}\). To learn a safe policy, we train the policy \(\) using \(C\) until it converges based on Equation 1. The detailed training procedure is presented in Algorithm 1.

## 5 Experiment

In this section, we first provide a brief overview of the task, as well as data extraction and preprocessing. Subsequently, in Section 5.1, we demonstrate that CT can describe constraints in healthcare and capture critical patient states. We emphasize its applicability to various CRL methods and its ability to approach the optimal policy for reducing mortality rates in Section 5.2. Finally, Section 5.3 discusses the realization of the objective of safe medical policies.

Figure 4: The structure of the model-based offline RL.

**Tasks.** We primarily use the sepsis task that is commonly used in previous works [9; 20; 42; 22], and supplement some experiments on the mechanical ventilator task [23; 50]. The detailed definition of the two tasks mentioned above can be found in Appendix B.1 and B.2.

**Data Extraction and Pre-processing.** Our medical dataset is derived from the Medical Information Mart for Intensive Care III (MIMIC-III) database . For each patient, we gather relevant physiological parameters, including demographics, lab values, vital signs, and intake/output events. Data is grouped into 4-hour windows, with each window representing a time step. In cases of multiple data points within a step, we record either the average or the sum. We eliminate variables with significant missing values and use the \(k\)-nearest neighbors method to fill in the rest. Notably, the training dataset consists of data from surviving patients, while the validation set includes survivors and non-survivors.

**Model-based Offline RL Evaluation.** To ensure the rigor of the experiments, we evaluate the validity of the model-based offline RL, as detailed in Appendix B.3.

### Can Offline CT Learn Effective Constraints?

In this section, we primarily assess the efficacy of the cost function learned by offline CT in sepsis, focusing particularly on its capability to evaluate patient mortality rates and capture critical events. First, we employ the cost function to compute cost values for the validation dataset. Subsequently, we statistically analyze the relationship between these cost values and mortality rates. As shown in Figure 5, there is an increase in patient mortality rates with rising cost values. It's noteworthy that such increases in mortality rates are often attributed to suboptimal medical decisions. Therefore, these experimental findings affirm that the cost values effectively reflect the quality of medical decision-making. To observe the impact of the attention layer (non-Markovian layer), we conduct experiments by removing the attention layer from CT. The results reveal that the penalty values do not correlate proportionally with mortality rates. This indicates that the attention layer plays a crucial role in assessing constraints.

To assess the capability of the cost function to capture key events, we analyze the relationship between physiological indicators and cost values. We focus on four key indicators in sepsis treatment: Sequential Organ Failure Assessment (SOFA) score , lactate levels , Mean Arterial Pressure

Figure 5: The relationship between cost and mortality.

Figure 6: The relationship between physiological indicators and cost values. As SOFA and lactate levels become increasingly unsafe, the cost increases. Mean BP and HR at lower values within the safe range incur a lower cost, but as they move into unsafe ranges, the cost increases, penalizing previous state-action pairs. The cost can differentiate between relatively safe and unsafe regions.

(MeanBP) , and Heart Rate (HR) . The SOFA score and lactate levels are critical indicators for assessing sepsis severity, with higher values indicating greater patient risk. MeanBP and HR are essential physiological metrics, typically ranging from \(70\) to \(100\) mmHg and \(60\) to \(100\) beats, respectively. Deviations from these ranges can signify patient risk. As depicted in Figure 6, the cost values effectively distinguish between high-risk and safe conditions, reflecting changes in patient status. Additional details on other parameters' relationship with cost are in Appendix B.4.

### Can Offline CT Improve the Performance of CRL?

**Baselines.** We adopt the DDPG method as the baseline in sepsis research , and the Double Deep Q-Learning (DDQN) and Conservative Q-Learning (CQL) methods as baselines in ventilator research . Since there are no other offline inverse reinforcement learning works available for reference, we have included two additional settings: no cost and custom cost. In the case of no cost, the cost is set to zero, while the design of custom constraints is outlined in Appendix A. These settings help evaluate whether CT can infer effective constraints.

**Metrics.** To assess effectiveness, we use \(\) to indicate the probability that the policy is optimal and analyze the relationship between DIFF and mortality rate through a graph. Recently, Kondrup _et al._ use the Fitted Q Evaluation (FQE)  to evaluate the policy in healthcare. However, the value estimates of FQE depend solely on the dataset \(\) and the actions chosen by the policy \(\) used to train FQE. This reliance can lead to inaccurate estimates when evaluating unseen state-action pairs. Therefore, we do not adopt this method as an evaluation metric.

**Results.** We combine our method CT with common CRL algorithms (e.g., VOCE, COpiDICE, BCQ-Lag, and CDT), and compare them with both no-cost and custom cost settings. Each CRL model is trained using no cost, custom cost, and CT separately, with other parameters set the same during training. For evaluation metrics, we use IV difference (IV DIFF), vaso difference (VASO DIFF), and combined [IV, VASO] difference (ACTION DIFF) as the metrics to be ranked. We measure the mean and variance of \(\%\) in \(10\) sets of random seeds, and the results are shown in Table 2. From the results, we can conclude: (1) In different CRL methods, CT consistently makes the strategy closer to the one with lower mortality rates, with a probability \(8.85\%\) higher than DDPG. (2) We find that CDT+CT achieves better results on all three metrics. CDT is also a transformer-based method, which indicates that transformer-based architecture indeed exhibits more outstanding performance in healthcare.

Figure 7 illustrates the relationship between IV and VASO DIFF with mortality rates under the DDPG and CDT+CT methods in sepsis. In VASO DIFF, when the gap is zero, the mortality rate under CDT+CT is lower than that under DDPG, indicating that following the former strategy could lead to a lower mortality rate. Similarly, in IV DIFF, the same trend is observed. Notably, for the IV strategy, the lowest mortality rate for DDPG does not occur at the point where the difference is zero, indicating a significant estimation bias.

In addition, corresponding experiments are conducted on the mechanical ventilator, as shown in Figure 8. Compared to previous methods DDQN and CQL, under the CDT+CT approach, a noticeable trend is observed where the proportion of mortality rates increases with increasing differences. When

   \(\)\% & COST & IV DIFF \(\) & VASO DIFF \(\) & ACTION DIFF \(\) \\  DDPG & - & 50.95\(\)1.34 & 51.45\(\)0.75 & 51.15\(\)1.15 \\   & No cost & 47.45\(\)0.52 & 46.35\(\)1.82 & 51.00\(\)0.86 \\  & Custom cost & 46.45\(\)0.46 & 52.00\(\)0.98 & 49.40\(\)1.04 \\  & CT & **53.33\(\)0.94** & **59.04\(\)1.13** & **56.15\(\)1.08** \\   & No cost & 48.30\(\)0.91 & 60.10\(\)0.66 & 51.25\(\)0.70 \\  & Custom cost & **53.05\(\)1.35** & 55.20\(\)0.24 & 53.90\(\)1.04 \\  & CT & 51.95\(\)0.41 & **60.85\(\)1.08** & **54.60\(\)0.60** \\   & No cost & 47.50\(\)1.32 & 51.05\(\)0.61 & 49.35\(\)1.08 \\  & Custom cost & 51.51\(\)1.06 & **56.23\(\)1.14** & 53.69\(\)1.62 \\  & CT & **52.45\(\)1.01** & 53.54\(\)1.20 & **54.93\(\)0.86** \\   & No cost & 56.50\(\)0.81 & 62.45\(\)1.20 & 58.90\(\)1.34 \\  & Custom cost & 54.70\(\)1.12 & 59.85\(\)1.51 & 57.80\(\)1.00 \\   & CT & **57.15\(\)1.67** & **65.20\(\)1.22** & **60.00\(\)1.49** \\   & Without CT & 56.50\(\)0.81 & 62.45\(\)1.20 & 58.90\(\)1.34 \\   & No attention layer & 55.25\(\)1.46 & 64.00\(\)1.54 & 57.90\(\)0.78 \\   & - & 55.49\(\)2.55 & 56.60\(\)1.33 & 57.00\(\)2.06 \\   

Table 2: Performance of sepsis strategies under various offline CRL models and different constraints.

Figure 7: The relationship between DIFF and the mortality rate in sepsis. The x-axis represents the DIFF. The y-axis indicates the mortality rate of patients at a given DIFF. The solid line represents the mean, while the shaded area indicates the Standard Error of the Mean (SEM).

there is a significant difference in DIFF, the results may be unreliable, possibly due to the limited data distribution in the tail.

### Can CRL with Offline CT Learn Safe Policies?

We have confirmed the existence of two unsafe strategy issues, namely "too high" and "sudden change" in the treatment of sepsis, particularly in vaso in Section 1. To validate whether the CRL+CT approach could address these concerns, we employ the same statistical methods to evaluate our methodology, shown in Table 3. To elucidate the efficacy of CT, we compare it with CDT+No-cost and CDT+Custom-cost approaches. We find that only the custom cost and CT methods successfully mitigated the risks associated with "too high" and "sudden change" behaviors. However, the custom cost approach opts to avoid administering drugs to mitigate these risks. Without these drugs, the patient's condition may not be alleviated, potentially leading to patient mortality. The CDT+CT approach can give a more appropriate drug dosage.

**Ablation Study.** To investigate the impact of each component on the model's performance, we conducted experiments by sequentially removing each component from the CDT+CT model. The results are presented in the lower half of Table 2. Both CT and its non-Markovian layer (attention layer) are indispensable and crucial components; removing either one results in a decrease in performance. Additionally, we observed that even a pure generative model outperforms DDPG in terms of performance. This is primarily because it inherently operates as a sequence-based reinforcement learning model, possessing exploration and consideration for long-term history. Therefore, this further underscores the effectiveness of sequence-based approaches in healthcare applications.

## 6 Conclusion

In this paper, we propose offline CT, a novel ICRL algorithm designed to address safety issues in healthcare. This method utilizes a causal attention mechanism to observe patients' historical information, similar to the approach taken by actual doctors and employs non-Markovian importance weights to effectively capture critical states. To achieve offline learning, we introduce a model-based offline RL for exploratory data augmentation to discover unsafe decisions and train CT. Experiments in sepsis and mechanical ventilation demonstrate that our method avoids risky behaviors while achieving strategies that closely approximate the lowest mortality rates.

**Limitations.** There are also several limitations of offline CT: (1) Lack of rigorous theoretical analysis: We did not precisely define the types of constraint sets, thereby conducting rigorous theoretical analysis on constraint sets remains challenging; (2) Need for more computational resources: Due to the Transformer architecture, more computational resources are required; (3) Fewer evaluation metrics: There is a lack of more medical-specific evaluation metrics in the experimental evaluation section; (4) Unrealistic assumptions of expert demonstrations: we assume that expert demonstrations are optimal in both constraint satisfaction and reward maximization. However, in reality, this assumption may not always hold. Therefore, researching a more effective approach to address the aforementioned issues holds promise for the field of secure medical reinforcement learning.

   Drug dosage &  \\ (\( g/kg min\)) & Physician & DDPG & No cost & Custom cost & CT \\  vaso \(>0.75\) & \(2.27\%\) & \(7.44\%\) & \(0.13\%\) & 0\% \(\) & 0\% \(\) \\ vaso \(>0.9\) & \(1.71\%\) & \(7.40\%\) & \(0.09\%\) & (max \(0.00\)) & (max \(0.11\)) \\   \(\) vaso \(>0.75\) & \(2.45\%\) & \(21.00\%\) & \(0.64\%\) & 0\% \(\) \\ \(\) vaso \(>0.9\) & \(1.88\%\) & \(20.62\%\) & \(0.48\%\) & (max \(=0.00\)) & (max \(=0.10\)) \\   

Table 3: The proportion of “too high” and “sudden change” occurrences in drug dosage recommended by RL methods.

Figure 8: The relationship between the DIFF of actions and mortality in mechanical ventilator. The actions mainly consist of Positive End Expiratory Pressure (PEEP) and Fraction of Inspired Oxygen (FiO2), which are crucial parameters in ventilator settings.