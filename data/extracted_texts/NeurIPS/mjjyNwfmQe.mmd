# Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?

Guiomar Pescador-Barrios

Imperial College London

&Sarah Filippi

Imperial College London

&Mark van der Wilk

University of Oxford

###### Abstract

Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing without improved performance. This leads to the question "How big is big enough?" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties. For our method, a single hyperparameter setting works well across diverse datasets, showing that it requires less tuning compared to others.

## 1 Introduction

Continual learning aims to train models when the data arrives in a stream of batches, without storing data after it has been processed, and while obtaining predictive performance that is as high as possible at each point in time . Selecting the size of the model is challenging in this setting, since typical non-continual training procedures do this by trial-and-error (cross-validation) using repeated training runs, which is not possible under our requirement of not storing any data. Selecting model size is crucial, since if the model is too small, predictive performance will suffer. One solution could be to simply make all continual learning models so large, that they will always have enough capacity, regardless of what dataset and what amount of data they will be given. However, this "worst-case" strategy is wasteful of computational resources.

A more elegant solution would be to grow the size of the model adaptively as data arrives, according to the needs of the problem (see Figure 1 for an illustration). For example, if data were only ever gathered from the same region, there would be diminishing novelty in every new batch, leading to a possible halt in growth, with growth resuming once data arrives from new regions. In this paper, we investigate a principle for determining how to select the size of a model so that it is sufficient to obtain near-optimal performance, while otherwise wasting a minimal amount of computation. In other words, we seek to answer the question of "how big is big enough?" for setting the size of models throughout continual learning.

We investigate this question for Gaussian processes where excellent continual learning methods exist but assume a fixed model capacity that is large enough. We introduce a criterion for determining the necessary number of inducing variables as new data arrives. Our method achieves near-optimal performance with fewer computational resources than other continual methods. With only one hyperparameter to balance cost and accuracy, a single value works effectively across datasets, enabling all modelling decisions to be made upfront. For related work, see App. B.

## 2 Background

### Sparse Variational Gaussian Processes

We consider the typical regression setting, with training data consisting of \(N\) input/output pairs \(\{_{n},y_{n}\}_{n=1}^{N},_{n}^{D},y_{n} \). We model \(y_{n}\) by passing \(_{n}\) through a function followed by additive Gaussian noise \(y_{n}=f(_{n})+_{n},_{n}(0,^{2})\), and take a Gaussian process prior on \(f(0,k_{}(,))\) with zero mean, and a kernel \(k\) with hyperparameters \(\). While the posterior (for prediction) and marginal likelihood (for finding \(\)) can be computed in closed form , they have a computational cost of \(O(N^{3})\) that is too high, and require all training data (or statistics greater in size) to be stored, both of which are prohibitive for continual learning. Variational inference can provide an approximation at a lower \(O(NM^{2})\) computational and \(O(NM)\) memory costs by selecting an approximation from a set of tractable posteriors

\[q(f()) = p(f()|,)q() \] (1) \[=(f();_{}_{}^{-1},k(,)-_{} _{}^{-1}(_{}-)_ {}^{-1}_{})\;,\] (2)

with \([_{}]_{ij}=k(_{i},_{j})\), \([_{}]_{i}=[_{}^{T}]_{i}=k( ,_{i}),=\{_{m}\}_{m=1}^{M}\), and \(q()=(;,)\). The variational parameters \(,,\) and hyperparameters \(\) are selected by maximising the Evidence Lower Bound (ELBO). This simultaneously minimises KL gap \([q(f)\,||\,p(f|,)]\) between the approximate and true GP posteriors [26; 25], and maximises an approximation to the marginal likelihood of the hyperparameters:

\[_{}=_{i=1}^{N}_{q(f(_{i}))}[  p(y_{i}|f(_{i}),)]-[q()\,||\,p (|)]\;.\] (3)

The variational approximation has the desirable properties  of **1)** providing a measure of discrepancy between the finite capacity approximation, and the true infinite capacity model, **2)** resulting in arbitrarily accurate approximations if enough capacity is added , and **3)** retaining the uncertainty quantification over the infinite number of basis functions. In this work, we will particularly rely on being able to measure the quality of the approximation to help determine how large \(M\) should be.

### Sparse Gaussian Processes are Equivalent to Single-Layer Neural Networks

For inner product kernels \(k(,)=()\) like the arc-cosine kernel , the mean is equivalent to a single-layer neural network with \(\) as the input weights, and \(_{}^{-1}\) as the output weights. This

Figure 1: Three continual learning scenarios with different capacity requirements. **Top**: Three consecutive batches for 1) a growing input space 2) i.i.d. samples from a uniform distribution, and 3) narrow-range samples with occasional outliers. **Bottom**: Number of inducing points selected using the VIPS algorithm at each batch. We observe: 1) a linear increase, 2) after initial training, we see a halt in growth, and 3) low model size until it encounters outliers.

construction also arises from other combinations of kernels and inter-domain inducing variables [9; 40], and has also shown equivalences between deep Gaussian processes and deep neural networks . As a consequence, our method for determining the number of inducing variables needed in a sparse GP, equivalently finds the number of neurons needed in a single-layer neural network.

### Online Sparse Gaussian Processes

We use the extension of the sparse variational GP approximation to the continual learning case developed by Bui et al. . We update our posterior and hyperparameter approximations after each batch of new data \(\{_{n},_{n}\}\). While we do not have access to data from older batches \(\{_{o},_{o}\}\), the parameters specifying the approximate posterior \(q_{o}(f)=p(f_{}|,_{o})q_{o}()\) are passed on. This approximate posterior is constructed as in eq. (1) but with \(=f(_{o})\) and the old hyperparameters \(_{o}\). Given the "old" \(q_{o}(f)\), online sparse GPs construct a "new" approximation \(q_{n}(f)=p(f_{}|,_{n})q_{n}()\), where \(=f(_{n})\) and \(_{n}\) is the new hyperparameter, of the posterior distribution for all observed data \(p(f|_{o},_{n},_{n})\). This is done by maximising the following the training objective:

\[}:= q_{n}(f)[|_{n})q_ {o}()p(_{n}|f)}{q_{n}()p(|_{o}) }]f,\] (4)

which we refer to as the "online ELBO". We provide technical details of this quantity in App. B.1, where we modify the typical derivation to **1**) clarify how the online ELBO provides an estimate to the full-batch ELBO, and **2**) clarify when this approximation is accurate.

To achieve a fully black-box solution, we must specify how to choose the hyperparameters \(_{n}\), the number of inducing variables \(M_{}\), and the inducing inputs \(_{n}\). We select \(_{n}\) by maximising \(}\) using L-BFGS and determine \(_{n}\) using the "greedy variance" criterion [12; 13; 3]. This leaves only the number of inducing variables \(M_{}\) to be chosen.

## 3 Automatically Adapting Approximation Capacity

We propose a method for adjusting the capacity of the approximation \(M_{}\) to maintain accuracy. We keep inducing points from old batches fixed, and select new inducing points from each incoming batch, with their locations set using the "greedy variance" criterion [3; 12; 13]. While optimising all inducing points leads to a strictly better approximation, we avoid this for simplicity. The question remains: To achieve a certain level of accuracy, "how big is big enough?" To answer this, we consider the online ELBO as a function of the capacity \(}(M_{})\), and propose a threshold after which to stop adding new inducing variables.

### Online Log Marginal Likelihood (LML) Upper Bound

The problem of selecting enough inducing variables remains open in the full-batch setting. One possible strategy is to derive an upper bound on the marginal likelihood (\(\)) and stop adding inducing variables the difference \(-\) (which upper bounds \([q(f)|[p(f|)]\)) falls below a tolerance \(\). Similarly, we consider the maximum possible value of our lower bound, which in the online setting is obtained by retaining previous inducing inputs and adding each new datapoint to the inducing set:

\[^{*}:=}(N_{n}+M_{})= (};\ ,_{}}}+_{}})+_{}_{ }}}=[_{}}}&_{}}}\\ _{}}}&_{} }}].\] (5)

Using properties of positive semi-definite matrices, we derive an upper bound \(}(M)\) to eq. (5):

\[^{*}-+M_{})}{2}(2)- |_{}}}+_{}}|- }^{T}(_{}}}+t+_{}})^{-1}}+_{ }}(M),\]

where \(t=(_{}}}-_{}}})\) and \(_{}}}=_{} }}_{}}}^{-1} _{}}}\) and \(M\) is the number of inducing points used to calculate the bound (which can be unequal to \(M_{}\)).

### Approximation Quality Guarantees

Adding inducing points will eventually increase \(}\) until it reaches \(^{*}\)[1; 25; 3]. If we add inducing points until \(}(M)-}(M_{})\) we can guarantee the following:

**Guarantee**.: _Let \(M\) be a fixed integer and \(M_{}\) be the number of selected inducing points such that \(}(M)-}(M_{})\). Assuming that \(_{n}=_{o}\), we have two equivalent bounds:_

\[[q_{n}(f) p(f|_{o},_{n}, _{o})] +\] (6) \[[q_{n}(f) q_{n}^{*}(f)] \] (7)

_where \(= q_{n}(f)^{*}(f)}{p(f|_{n},_{o})} f\) and \(q_{n}^{*}(f)=^{-1}q_{o}(f)p(_{n} f)\) represents the variational distribution associated with the optimal lower bound \(^{*}=}(N_{n}+M_{})\), with \(\) denoting the marginal likelihood that normalises \(q_{n}^{*}(f)\)._

Proof.: We cease the addition of points when \(}(M_{})}(M)-\). Given that \(}(M)^{*}\), and assuming \(_{n}=_{o}\), the rest follows from algebraic manipulation of eq. (9). See App. C for the complete proof. 

The first bound shows that if \(\) is near zero, the KL to the true posterior is bounded by \(\). While \(\) depends on the true posterior and therefore cannot be computed, if the posterior in the previous iteration was exact, \(\) would be equal to zero. The second bound shows that we are guaranteed to have our actual approximation \(q_{n}(f)\) be within \(\) nats of the best approximation that we can develop, given the limitations of the approximations made in previous iterations.

### Selecting a Threshold

In this final step of our online learning method, we must specify a heuristic for selecting \(\) that does not require knowing any data in advance, while also working in a uniform way across datasets with different properties. A constant value for \(\) does not work well, since the scale of the LML depends strongly on properties such as dataset size, and observation noise. This means that a tolerance of 1 nat  may be appropriate for a small dataset, but not for a large one.

As a principle for selecting the threshold, we take loose inspiration from compression and MDL , which takes the view of the ELBO being proportional to negative the code length that the model requires to encode the dataset. Intuitively, our desire is to select an \(\) such that our method captures a high proportion (e.g. 95%) of all the information in each batch, so that we can compress to within a small fraction of the optimal variational code. To address the issue of undefined quantisation tolerance, we use an independent random noise code as our baseline and choose \(\) to be within a small fraction of the optimal variational code relative to the random noise code. We want to be able to capture a high proportion of the additional information provided by our model relative to the noise model, i.e. we want our threshold to be:

\[=(^{*}-_{})\,, _{}=_{n=1}^{N_{n}}(y_{n};, ^{2})\]

where \(\) and \(^{2}\) are the average and variance of the observations for up to the current task and \(\) is a user-defined hyperparameter. We validate that this approach leads to values of \(\) giving consistent behaviour across a wide range of datasets, which allows it to be set in advance without needing much prior knowledge of the dataset characteristics.

Calculating this threshold is intractable for large batch sizes \(N_{n}\). However, if we change our stopping criterion to the more stringent upper bound

\[=(}(M)-_{})\] (8)

and increase \(M\) for calculating \(}\) as \(M_{}\) is increased for calculating \(}\), we obtain the same guarantees as before but at a lower computational cost. However, this strategy is only worthwhile for very large batch sizes \(N_{n}\), due to the importance of constant factors in the computational cost. In the common continual learning settings we investigate \(N_{n}\) is small enough to allow computing \(^{*}\).

The algorithm for our inducing point selection method can be found in App. D. We name our approach Vegas Inducing Point Selection (VIPS), drawing an analogy to Las Vegas Algorithms. These methods guarantee the accuracy of the output, however, their computational time fluctuates for every run .

## 4 Experiments

We evaluate the performance of our adaptive inducing point selection, VIPS, in a range of streaming scenarios where we assume the total number of observations is unknown. In all cases, the variational distribution and kernel hyperparameters are optimised using the online lower bound (Eq. (13)).

Continual learning scenarios pose unique challenges: memory allocation cannot be pre-determined due to unknown input space coverage, and cross-validation for hyperparameter tuning is infeasible as it requires storing all data. Thus, an effective method must **1)** have an adaptive memory that can grow with the demands of the data, **2)** work with hyperparameters that can be set before training. Our experiments aim to illustrate these points. Details and additional experiments are provided in App. F.

### Model size and data distribution

Figure 1 shows VIPS's ability to adapt across datasets with different characteristics, each divided into ten batches, illustrating how input distribution drives model growth as more data is seen. In the first dataset, each batch introduces new data, causing the model size to grow linearly. The second dataset remains within a fixed interval, leading to reduced novelty in batches and a converging model size. The third dataset combines narrow-range samples with occasional outliers, resulting in low model size with occasional growth when novelty appears (details in App. F.1).

### Continual learning of UCI datasets

We compare VIPS to two other inducing point selection methods: Conditional Variance (CV) and OIPS  (details in App. E). We use six datasets from the UCI repository , simulating a continual learning scenario by sorting the data along the first dimension and dividing it into batches. For each method, we assess multiple hyperparameter settings and identify the one that minimises model size while achieving RMSE within 10% of a full-batch GP across all datasets, considered equivalent to near-exact performance. Table 1 shows the number of inducing points used for that particular hyperparameter value. The method CV often leads to larger model sizes (excessive for "bike"). For the noiseless "naval" dataset, CV uses fewer inducing points but obtains poor uncertainty estimates (details in App. F.3.2) and OIPS fails to meet the accuracy constraint within its tested hyperparameter range. Meanwhile, VIPS consistently meets accuracy requirements and uses fewer inducing points in the majority of datasets, suggesting it requires less hyperparameter tuning (details in App. F.3).

## 5 Discussion

In this work, we propose a method to dynamically adjust the number of inducing variables in streaming GP regression, providing a capacity control criterion with approximation guarantees. Our method achieves a performance close to full-batch approaches while minimising model size. It relies on a single hyperparameter to balance accuracy and complexity, and we demonstrate that a single setting performs well across diverse datasets. This reduces the need for extensive hyperparameter tuning and eliminates the requirement to pre-define model size, thereby addressing a significant bottleneck in traditional methods. While our current focus is on GPs, we aim to extend this method to larger neural architectures.

   UCI Dataset & Dimension (N, D) & Conditional Variance (CV) & OIPS  & VIPS (Ours) \\  Concrete & 1030, 8 & 461(59) & 409(87) & **385(84)** \\ Skillcraft & 3338, 19 & 599(30) & 332(82) & **141(4)** \\ Kin8nm & 8192, 8 & 6194(13) & 6539(9) & **2953(72)** \\ Naval & 11934, 14 & **35(3)** & ✗ & 127(5) \\ Elevators & 16599, 18 & 2501(100) & 643(135) & **332(8)** \\ Bike & 17379, 17 & Max. 7000 & 5131(65) & **1037(24)** \\   

Table 1: Mean (std) over different training/test splits of the number of inducing points for the last batch for operating point (see 4.2). The cross (✗) denotes unmet accuracy constraint, while “Max.” indicates that maximum capacity was reached.