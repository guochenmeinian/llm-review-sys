# Transformers learn through gradual rank increase

Enric Boix-Adsera\({}^{*}\)\({}^{1,2}\) Etai Littwin\({}^{*}\)\({}^{1}\)

Emmanuel Abbe\({}^{1,3}\) Samy Bengio\({}^{1}\) Joshua Susskind\({}^{1}\)

\({}^{1}\)Apple \({}^{2}\)MIT \({}^{3}\)EPFL

eboix@mit.edu,emmanuel.abbe@epfl.ch

{elittwin,bengio,jsusskind}@apple.com

###### Abstract

We identify incremental learning dynamics in transformers, where the difference between trained and initial weights progressively increases in rank. We rigorously prove this occurs under the simplifying assumptions of diagonal weight matrices and small initialization. Our experiments support the theory and also show that phenomenon can occur in practice without the simplifying assumptions.

## 1 Introduction

The transformer architecture achieves state of the art performance in various domains, yet we still lack a solid theoretical understanding of its training dynamics . Nevertheless, the theoretical toolbox has matured over the last years and there are promising new approaches. One important line of work examines the role that initialization scale plays on the trajectory taken by gradient descent . When the weights are initialized small, it has been shown for simple networks that an _incremental learning_ behaviour occurs, where functions of increasing complexity are learned in stages. This regime is known to be richer than the large-initialization regime1, but the incremental learning dynamics are difficult to analyze, and are so far understood only for extremely simple architectures. Can we apply this analysis to transformers? Namely:

_Are there incremental learning dynamics when training a transformer architecture?_

An obstacle is that past work on incremental learning has mainly studied linear networks , with one paper studying nonlinear 2-layer fully-connected networks . In contrast, transformers have nonlinear attention heads that do not fall under previous analyses: given \(^{n d}\), an attention head computes

\[(;_{K},_{Q},_{V},_{O})=(_{K}_{Q}^{}^{})_{V}_{O}^{}\] (1)

where \(_{K},_{Q},_{V},_{O}^{d d^{}}\) are trainable matrices, and the softmax is applied row-wise. A transformer is even more complex, since it is formed by stacking alternating layers of attention heads and feedforward networks, along with residual connections.

Main findingOur main finding is that transformers exhibit incremental learning dynamics, where _the difference between the trained and initial weights incrementally increases in rank_. Our results have a theoretical component and an experimental component.

Theoretical contributionsFor our theory, we study a simplification of the transformer architecture, where the attention head weights are diagonal matrices: i.e., in each attention head we have \(_{K}=(_{K})\), where \(_{K}^{d}\) are trainable weights, and similarly for \(_{Q},_{V}\) and \(_{O}\). We rigorously establish the training dynamics of this architecture under gradient flow when the initialization is small. We prove that dynamics occur in discrete stages: (1) during most of each stage, the loss plateaus because the weights remain close to a saddle point, and (2) at the end, the saddle point is quickly escaped and the rank of the weights increases by at most one.

This theoretical result on transformers follows from a general theorem characterizing the learning dynamics of networks \(f_{}\) that depend on the product of parameters \(,^{p}\) as

\[f_{}(;,)=h(;)\,,\] (2)

where \(\) is the input, \(\) denotes the elementwise product, and \(h\) is a smooth function.

**Theorem 1.1** (Informal statement of incremental learning dynamics).: _Let \(f_{}\) be a network of the form (2), and suppose that the weights are initialized very small: i.e., the entries of \(,\) are initialized on the order \(()\) for some small \(>0\). Then the dynamics of gradient flow training effectively proceeds in discrete stages, each one lasting time \(((1/))\). In each stage, the number of nonnegligible entries of \(\) increases by at most one._

A transformer with diagonal weight matrices falls under this result when we only train the attention head weights. For example, if the transformer has one attention head, then we can take \(=[_{K},_{V}]^{2d}\) and \(=[_{Q},_{Q}]^{2d}\) to be concatenations of the diagonal entries of the weights of the head; see Example 3.2 for more details and the extension to transformers with many heads. Then, using Theorem 1.1, we see that in each stage either \(_{K}_{Q}^{}=(_{K})(_{Q})\) or \(_{V}_{O}^{}=(_{V})(_{O})\) increases in effective rank by at most one.2

Experimental contributionsIn our experiments, we first validate our theoretical results, which require the simplifying assumptions of small initialization and diagonal weight matrices.

Then, we conduct experiments on vision and language transformers in settings closer to practice, without any of the assumptions required by our theoretical analysis. Perhaps surprisingly, we again observe incremental learning dynamics, even though the assumptions of the theory are not met. The difference between trained and initial weights has low rank, and the rank of this difference grows gradually during training; see Figure 1. The incremental nature of the dynamics is easier to see for ImageNet, since for CIFAR-10 the rank of the weight difference does not grow as much.

### Related work

Relation to LoRAWe note an intriguing connection to the LoRA algorithm, where a pretrained base model is cheaply fine-tuned by training a low-rank perturbation of the weights . The method is surprisingly powerful, and recently LoRA has been fundamental to allowing the open-source community to inexpensively fine-tune language models . On the other hand, in our work we observe that the trained weights are a low-rank perturbation of the initial weights due to the training dynamics, without having to apply an explicit rank constraint as in LoRA. This raises an exciting open question for future work: _can we explain and improve algorithms like LoRA by better understanding and quantifying the incremental dynamics of large transformers?_

Figure 1: For an attention head in ViT trained on (a) CIFAR-10, and (b) ImageNet, we plot the normalized spectra of \(_{K}_{Q}^{}\) at initialization (in red), and of the learned perturbations to \(_{K}_{Q}^{}\) at different iterations (in green).

Low-rank bias in nonlinear networksFor 2-layer networks, it is known that low-rank bias in the weights emerges if the target function depends on a low-dimensional subspace of the input . The results of  are especially relevant, since they show that the rank of the weights increases in a sequential manner, determined by the "leap complexity" of the target function, which is reminiscent of our empirical observations on transformers. See also  for more investigations of low-rank bias in 2-layer networks under different assumptions. For transformers,  report that empirically the trained weights (using default initialization) are not low-rank. This is consistent with our claim that the difference between initial and trained weights is low-rank, since the initial weights might not be low-rank.

Incremental learning dynamicsSeveral works prove incremental learning behaviour in deep _linear_ networks when the initialization is small.  has shown that gradient descent dynamics on a 2-layer linear network with \(L_{2}\) loss effectively solve a reduced-rank regression problem with gradually increasing rank.  prove a dynamical depth separation result, allowing for milder assumptions on initialization scale.  show implicit bias towards low rank in deep matrix and tensor factorization.  show deep matrix factorization dynamics with small initialization are equivalent to a greedy low-rank learning (GLRL) algorithm. And  independently provides a similar description of the dynamics, but without requiring balanced initialization. Finally,  overcome a technical hurdle from previous analyses by proving incremental learning for the entire training trajectory, rather than just the first stage. In contrast to our result, these prior works apply only to _linear_ networks with certain convex losses, whereas our result applies to _nonlinear_ networks. In order to make our extension to nonlinear networks possible, we must make stronger assumptions on the training trajectory, which we verify hold empirically. As far as we are aware, one other work on incremental learning handles nonlinear networks:  proves that a 2-layer network learns with a two-stage incremental dynamic; but that result needs the stylized assumption that all data points are orthogonal.

### Paper organization

Sections 2, 3, and 4 contain theoretical preliminaries, definitions of the models to which our theory applies, and our main theoretical result on incremental dynamics. Section 5 provides experiments which verify and extend the theory. Section 6 discusses limitations and future directions.

## 2 Preliminaries

We consider training a network \(f_{}(;)\) parametrized by a vector of weights \(\), to minimize a loss

\[()=_{,}[(,f_{}( ;))]\,,\]

where the expectation is over samples \((,)^{d_{x}}^{d_{y}}\) from a training data distribution, and \(:^{d_{y}}^{d_{out}}\). Consider a solution \((t)\) to the gradient flow3

\[(0)=_{0},\ \ }{dt}=-_{}()\] (3)

where \(>0\) is a parameter governing the initialization scale, that we will take small. For our theory, we henceforth require the following mild regularity assumption on the loss and data.

**Assumption 2.1** (Regularity of data distribution and loss).: The function \((,)\) is continuously twice-differentiable in the arguments \([,]^{d_{y}+d_{out}}\). There exists \(C>0\) such that almost surely the data is bounded by \(\|\|,\|\| C\).

The assumption on \(\) is satisfied in typical cases such as the square and the cross-entropy losses. The data boundedness is often satisfied in practice (e.g., if the data is normalized).

We also use the notation \(():=\{i:a_{i} 0\}\) to denote the support of a vector \(\).

Neural networks with diagonal weights

Our theory analyzes the training dynamics of networks that depend on products of diagonal weight matrices. We use \(\) to denote elementwise vector product.

**Definition 3.1**.: A network \(f_{}\) is smooth with diagonal weights \(=(,)^{2p}\) if it is of the form

\[f_{}(;)=h(;)\]

where \(h:^{d_{x}}^{p}^{d_{out}}\) is continuously twice-differentiable in its arguments in \(^{d_{x}+p}\).

The assumption on \(h\) precludes the use of the ReLU function since it is not continuously-differentiable. Otherwise the assumption is fairly mild since any \(h\) can be used to express an architecture of any depth as long as the nonlinearities are twice-differentiable, which includes for example GeLUs (as used in ViT). We describe how to express a transformer with diagonal weights.

**Example 3.2** (Transformer with diagonal weights).: A transformer with \(L\) layers and \(H\) attention heads on each layer is defined inductively by \(_{0}=^{n d}\) and

* (Attention layer) \(}_{}=_{-1}+_{i=1}^{H}( _{-1};_{K}^{,i},_{Q}^{,i},_{V}^{,i}, _{O}^{,i})\)
* (Feedforward layer) \(_{}=}_{}+(}_{}_{A}^{ })(_{B}^{})^{}\),

where \(_{K}^{,i},_{Q}^{,i},_{V}^{,i},_{O}^{,i }^{d d^{}}\) are attention parameters, and \(_{A}^{},_{B}^{}^{d d^{}}\) are the feedforward parameters, and \(\) is a continuously twice-differentiable activation. Suppose that the attention parameters are diagonal matrices: i.e., \(_{K}^{,i}=(_{K}^{,i})^{d  d}\), and similarly for the \(_{Q}^{,i},_{V}^{,i},_{O}^{,i}\) matrices. Then by the definition of the attention layer (1), the final output of the transformer \(_{L}\) only depends on the attention parameters through the elementwise products \(_{K}^{,i}_{Q}^{,i}\) and \(_{V}^{,i}_{O}^{,i}\). In other words, we can write

\[_{L}=h(;)\,,\]

for vectors \(=[_{K}^{,i},_{V}^{,i}]_{(,i)[L][H]} ^{2dHL}\) and \(=[_{Q}^{,i},_{O}^{,i}]_{(,i)[L][H]} ^{2dHL}\), and some smooth model \(h\). Thus, if only the attention layers are trained, the diagonal transformer fits under Definition 3.1.

## 4 Incremental learning in networks with diagonal weights

We prove that if the initialization scale \(\) is small, then learning proceeds in incremental stages, where in each stage the effective sparsity of the weights increases by at most one. These stages are implicitly defined by Algorithm 1 below, which constructs a sequence of times \(0=T_{0}<T_{1}<<T_{k}<\) and weight vectors \(^{0},^{1},,^{k},^{2p}\) that define the stages. We prove the following:

**Theorem 4.1** (Incremental dynamics at small initialization).: _Let \(f_{}\) be a model with diagonal weights as in Definition 3.1. For any stage \(k\) and time \(t(T_{k},T_{k+1})\) the following holds under Assumptions 2.1, 4.3, 4.4 and 4.5. There is \(_{0}(t)>0\) such that for all \(<_{0}\), there exists a unique solution \(:[0,t(1/)]^{p}\) to the gradient flow (3) and_

\[_{ 0}(t(1/))^{k}\,\]

_and at each stage the sparsity increases by at most one: \((^{k+1})(^{k})\{i_{k}\}\).4_

Application: transformer with diagonal weightsBefore giving the intuition for this theorem and stating the assumptions formally, let us discuss its application to the diagonal transformer model from Example 3.2. As a corollary of Theorem 4.1, the gradient flow on a diagonal transformer with small initialization will learn in stages, where in each stage there will be at most one head \(i[H]\) in one layer \([L]\) such that either the rank of \(_{K}^{,i}(_{Q}^{,i})^{}=(_{K}^ {,i})(_{Q}^{,i})\) or the rank of \(_{V}^{,i}(_{O}^{,i})^{}=(_{V }^{,i})(_{O}^{,i})\) increases by at most one. In Figure 2, we illustrate these dynamics in the toy case of a single attention head trained in a student-teacher setup.

### Intuition for incremental learning dynamics

We develop an informal intuition for Theorem 4.1 and fill out the definition of Algorithm 1. A model \(f_{}\) with diagonal weights \(=(,)\) as in Definition 3.1 evolves under the gradient flow (3) as

\[}{dt} =(),}{dt}= ()\] (4) \[() =-}_{,}[D(,h( ;))^{}Dh(;)^{}]\,.\]

Here \(D(,)^{1 d_{out}}\) is the derivative of \(\) in the second argument and \(Dh(,)^{d_{out} p}\) is the derivative of \(h\) in the second argument. The first key observation is a conservation law that simplifies the dynamics. It can be viewed as the balancedness property for networks with linear activations , specialized to the case of diagonal layers.

**Lemma 4.2** (Conservation law).: _For any \(i[p]\) and any time \(t\), we have_

\[u_{i}^{2}(t)-v_{i}^{2}(t)=u_{i}^{2}(0)-v_{i}^{2}(0)\,.\] (5)

Proof.: This follows from \((u_{i}^{2}-v_{i}^{2})=u_{i}v_{i}g_{i}()-u_{i}v_{i}g_{i}( )=0\)

Figure 2: (a) Loss versus rescaled time in the toy task of learning an attention head with diagonal weights, for various initialization scales \(\). The loss curves converge as \( 0\) to a curve with stagewise loss plateaus and sharp decreases, as predicted by the theory; some stagewise learning behavior is already clear with \(=0.01\). (b) Each line shows the evolution of one of the entries of \((_{Q})(_{K})\) and \((_{V})(_{O})\) over rescaled time, demonstrating that the rank of these matrices increases incrementally; see Appendix A for experimental details and further results.

The conservation law reduces the degrees of freedom and means that we need only keep track of \(p\) parameters in total. Specifically, if we define \(w_{i}(t):=u_{i}(t)+v_{i}(t)\), then the vector \((t)=(t)+(t)\) evolves by

\[}{dt}=()\,.\] (6)

Using the conservation law (5), we can keep track of the weights in terms of the initialization and \((t)\):

\[(t)=((t)+^{ 2}(0)-^{  2}(0)}{(t)}),((t)-^{ 2}(0)-^{  2}(0)}{(t)})\] (7)

Therefore it suffices to analyze the dynamics of \((t)\).

#### 4.1.1 Stage 1 of dynamics

Stage 1A of dynamics: loss plateau for time \(((1/))\)At initialization, \((0)\) because the weights are initialized on the order of \(\) which is small. This motivates the approximation \(((t))()\), under which the dynamics solve to:

\[(t)(0) e^{()t}.\] (8)

Of course, this approximation is valid only while the weights are still close to the small initialization. The approximation breaks once one of the entries of \((t)\) reaches constant size. By combining (7) and (8), this happens at time \(t T_{1}(1/)\) for

\[T_{1}=_{i[p]}1/|g_{i}()|\,.\]

Until this time, the network remains close to its initialization, and so we observe a loss plateau.

Stage 1B of dynamics: nonlinear dynamics for time \(O(1)\)Subsequently, the loss decreases nonlinearly during a \(O(1)\) time-scale, which is vanishingly short relative to the time-scale of the loss plateau. To prove this, we make the non-degeneracy assumption that there is a unique coordinate \(i_{0}\) such that \(1/|g_{i_{0}}()|=T_{1}\). Under this assumption, in stage 1A all weights except for those at coordinate \(i_{0}\) remain vanishingly small, on the order of \(o_{}(1)\). Concretely, for any small \(>0\), there is a time \(_{1}() T_{1}(1/)\) and sign \(s\{+1,-1\}\) such that5

\[u_{i_{0}}(_{1}),v_{i_{0}}(_{1})  s|u_{i}(_{1})|,|v_{i}( _{1})|=o_{}(1)i i_{0}.\]

Because all coordinates except for \(i_{0}\) have vanishingly small \(o_{}(1)\) weights after stage 1A, we may perform the following approximation of the dynamics. Zero out the weights at coordinates except for \(i_{0}\), and consider the training dynamics starting at \(}=(_{i_{0}},s_{i_{0}})\). After \(O(1)\) time, we should expect these dynamics to approach a stationary point. Although the evolution is nonlinear, all entries remain zero except for the \(i_{0}\) entries, so the stationary point is also sparse. Mathematically, there is a time \(_{1}=_{1}+O(1) T_{1}(1/)\) such that

\[(_{1})(a_{i_{0}},s_{i_{0}}):=^ {1}\,,\]

for some \(a_{>0}\), where \(^{1}\) is a stationary point of the loss.6 Despite the nonlinearity of the dynamics, the approximation can be proved using Gronwall's inequality since \(_{1}-_{1}=O(1)\) is a constant time-scale.

To summarize, we have argued that the network approximately reaches stationary point that is 1-sparse, where only the weights at coordinate \(i_{0}\) are nonzero.

#### 4.1.2 Later stages

We inductively extend the argument to any number of stages \(k\), where each stage has a \(((1/))\)-time plateau, and then a \(O(1)\)-time nonlinear evolution, with the sparsity of the weights increasing by at most one. The argument to analyze multiple stages is analogous, but we must also keep track of the magnitude of the weights on the logarithmic scale, since these determine how much longer. Inductively on \(k\), suppose that there is some \(T_{k},^{k}^{p}\) and \(^{k}^{2p}\) and a time \(_{k} T_{k}(1/)\) such that

\[_{}((_{k}))^{k}(_{k})^{k},\]

where \(^{k}\) is a stationary point of the loss. Our inductive step shows that there is \(T_{k+1}\) such that during times \(t(_{k},T_{k+1}(1/)-(1))\) the weights remain close to the stationary point from the previous stage, i.e., \((t)^{k}\). And at a time \(_{k+1} T_{k+1}(1/)\) we have

\[_{}((_{k+1}))^{k+1}(_{k+1})^{k+1},\]

where \(^{k+1}\) and \(^{k+1}\) are defined below (summarized in Algorithm 1). Most notably, \(^{k+1}\) is a stationary point of the loss whose support grows by at most one compared to \(^{k}\).

Stage \((k+1)\)A, loss plateau for time \(((1/))\)At the beginning of stage \(k+1\), the weights are close to the stationary point \(^{k}\), and so, similarly to stage 1A, linear dynamics are valid.

\[(t)(_{k}) e^{(^{k})(t-_{k})}\,.\] (9)

Using the conservation law (7), we derive a "time until active" for each coordinate \(i[p]\), which corresponds to the time for the weight at that coordinate to grow from \(o_{}(1)\) to \((1)\) magnitude:

\[_{k}(i)=(b_{i}^{k}-1+(g_{i}(^ {k})))/g_{i}(^{k}),&g_{i}(^{k}) 0\\ ,&g_{i}(^{k})=0\] (10)

The linear dynamics approximation (9) breaks down at a time \(t T_{k+1}(1/)\), where

\[T_{k+1}=T_{k}+_{k}(i_{k}), i_{k}=_{i[p]}_{k}(i)\,,\] (11)

which corresponds to the first time at the weights at a coordinate grow from \(o_{}(1)\) to \((1)\) magnitude. And at times \(t T_{k+1}(1/)\), on the logarithmic scale \(\) is given by

\[_{}((t))^{k+1}:=^{k}-(^{ k})_{k}(i_{k})\,,\] (12)

Stage \((k+1)\)B of dynamics: nonlinear dynamics for time \(O(1)\)Subsequently, the weights evolve nonlinearly during \(O(1)\) time. In a similar way to the analysis of Stage 1B, we show that at a time \(_{k+1}=_{k+1}+O(1) T_{k+1}(1/)\), we have

\[(_{k+1})^{k+1}:=_{ 0} _{t}^{k}(t,)\,,\] (13)

where the dynamics \(^{k}(t,)^{2p}\) are initialized at \(^{k}(0,)=^{k}+(_{i_{k}}, (g_{i}(^{k}))_{i_{k}})\) and evolve according to the gradient flow \(^{k}(t,)}{dt}=-_{}(^{k})\). This concludes the inductive step.

### Assumptions for incremental dynamics

To make this intuition rigorous, we formalize below the assumptions required for Theorem 4.1. In Figure 3 and Appendix A, we provide experiments validating these assumptions on the toy model.

The first assumption is that the dynamics are non-degenerate, in the sense that two coordinates do not have weights that grow from \(o_{}(1)\) to \((1)\) size at the same rescaled time. We also place a technical condition to handle the corner case when a coordinate leaves the support of the current stage's stationary point.

**Assumption 4.3** (Nondegeneracy of dynamics in part (A)).: The initialization satisfies \(|u_{i}(0)||v_{i}(0)|\) for all \(i\). For stage \(k\), either \(T_{k+1}=\) or there is a unique minimizer \(i_{k}\) to \(_{i}_{k}(i_{k})\) in (11). Finally, for all \(i(^{k-1})(^{k})\) we have \(g_{i}(^{k}) 0\).

Next, we require that very small perturbations of the coordinates outside of \((^{k})\) do not change the dynamics. For this, it suffices that \(^{k}\) be a strict local minimum.

**Assumption 4.4** (Stationary points are strict local minima).: For stage \(k\), there exist \(_{k}>0\) and \(c_{k}>0\) such that for \(} B(^{k},)\) supported on \((^{k})\), we have

\[(},^{k}}) c_{k}\|^{ k}-}\|^{2}\]

Finally, we require a robust version of the assumption that the limit (13) exists, asking for convergence to a neighborhood of \(^{k+1}\) even when the initialization is slightly noisy.

**Assumption 4.5** (Noise-robustness of dynamics in part (B)).: For any stage \(k\) with \(T_{k+1}<\) and any \(>0\), there are \(>0\) and \(:_{>0}\) such that the following holds. For any \(} B(^{k},)_{ 0}^{p}\) supported on \((})(^{k}) \{i_{k}\}\), there exists a unique solution \(:[0,)^{p}\) of the gradient flow \(}{dt}=-_{}()\) initialized at \((0)=(},^{k+1}})\), and at times \(t(_{i_{k}})\),

\[\|(t)-^{k+1}\|<\,.\]

## 5 Experimental results

We run experiments that go beyond the toy diagonal attention head model (see Figures 2 and 3) to test the extent to which low-rank incremental learning occurs in popular models used in practice. We conduct experiments with vision transformers (ViT)  trained on the CIFAR-10/100 and ImageNet datasets, and with the GPT-2 language transformer  trained on the Wikitext-103 dataset. Full experiments are deferred to Appendix B.

Gradual rank increase in vision and language modelsWe train practical transformer architectures on vision and language tasks using Adam and the cross-entropy loss. We train all layers (including the feedforward layers). To capture the low-rank bias with a non-vanishing initialization scale, we study the spectrum of the difference \(_{K}_{Q}^{}\) and \(_{V}_{O}^{}\) between the weights post-training and their initial values. Specifically, in Figure 4, we plot the stable rank of the differences \(_{K}_{Q}^{}\) and \(_{V}_{O}^{}\). The weight perturbation learned during the training process gradually increases in stable rank during training, and is ultimately low-rank when compared to the initial spectrum. Finally, for CIFAR-10, we plot the spectrum of \(_{K}_{Q}^{}\) against that of its initialized state in Figure 5 for different self-attention heads, illustrating that the weight perturbation learned during the training process is extremely low-rank when compared to the initial spectrum. In Appendix B, we also study optimization with SGD, which shows similar gradual rank increase behavior.

Effect of initialization scaleWe probe the effect of initialization scale on gradual rank increase dynamics for a ViT trained on CIFAR-10. We use a ViT of depth 6, with 8 self-attention heads per layer (with layer normalization). We use an embedding and MLP dimension of \(d_{}=512\), and a head dimension of \(d_{h}=128\) (i.e \(_{K},_{Q},_{V},_{O}^{d_{} d _{h}}\)). We train the transformer

Figure 3: Validation of assumptions on the toy model of learning a single attention head. (a) Assumption 4.4: weights perturbed at a random time during training (solid lines) tend back to the near-stationary point (dashed lines). (b) Assumption 4.5: weights perturbed at the beginning of a stage (solid lines) have same nonlinear evolution as without perturbation (dashed lines). Details of these experiments and further validations are provided in Appendix A.

using Adam with the cross-entropy loss. We train all layers (including the feedforward layers) while varying the initialization scale of all layers by multiplying their initial values by a scale factor (we fix the scale of the initial token mapper). Figure 6 shows the evolution of the principal components of \(_{K}_{Q}^{}\) and \(_{V}_{O}^{}\) for a randomly-chosen self-attention head and layer throughout training, exhibiting incremental learning dynamics and a low-rank bias. Note that incremental learning and low-rank bias are increasingly evident with smaller initialization scales, as further demonstrated in Figure 7.

## 6 Discussion

We have identified incremental learning dynamics in transformers, proved them rigorously in a simplified setting, and shown them experimentally in networks trained with practical hyperparameters.

LimitationsThere are clear limitations to our theory: the diagonal weights and small initialization assumptions. More subtly, the theory does not apply to losses with exponential-like tails because the weights may not converge to a finite value and so Assumption 4.4 is not met (this could possibly be addressed by adding regularization). Also, the architecture must be smooth, which precludes ReLUs - but allows for smoothed ReLUs such as the GeLUs used in ViT . Finally, the theory is for training with gradient flow, while other optimizers such as Adam are used in practice instead . Nevertheless, our experiments on ViTs indicate that the incremental learning dynamics occur even when training with Adam.

Figure 4: Stable rank of \(_{K}_{Q}^{}\) (blue) and \(_{V}_{O}^{}\) (orange) on an arbitrary chosen layer throughout training for four different pairs of networks and tasks. The stable rank of a matrix \(\) is defined as \(\|\|_{F}^{2}/\|\|_{2}^{2}\), and gives a smooth approximation of the rank. Mean and standard deviation (shaded area) are computed across all heads in each attention layer. Full details and results are in Appendix B.

Figure 5: Spectrum of the weight perturbation \(_{K}_{Q}^{}\) vs. initialization in a vision transformer trained on CIFAR-10, using Adam and default initialization scale, in random self-attention heads in different layers. The learned perturbation exhibits extreme low-rank bias post-training even in default initialization scales. Analogous plots for CIFAR-100 and ImageNet are in Appendix B.

Future directionsAn interesting avenue of future research is to develop a theoretical understanding of the implicit bias in function space of transformers whose weights are a low-rank perturbation of randomly initialized weights. Another promising direction is to examine the connection between our results on incremental dynamics and the LoRA method , with the goal of explaining and improving on this algorithm; see also the discussion in Section 1.1. Along this vein, a concurrent work  independently observes gradual rank increase dynamics during transformer training and this inspires a low-rank training algorithm that obtains runtime and memory improvements over regular training. The results of  are complementary to ours, since they study the feedforward layers of the transformer, and their theory applies to _linear_ networks in the standard initialization scale; in contrast, we study the attention layers, and our theory applies to _nonlinear_ networks with small initialization scale.