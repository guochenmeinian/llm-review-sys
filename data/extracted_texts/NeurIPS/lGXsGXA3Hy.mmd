# MHP-DDP: Multivariate Hawkes Process based on Dependent Dirichlet Process

Alex Ziyu Jiang

University of Washington

jiang14@uw.edu &Abel Rodriguez

University of Washington

abelrod@uw.edu

###### Abstract

Multivariate Hawkes Processes (MHPs) model complex temporal dynamics among event sequences on multiple dimensions. Typically, strong parametric assumptions are made about the excitation functions of MHP, motivating the need for modeling flexible excitation patterns. Further, different excitation functions across dimensions often have strong similarities. Motivated by reasons above, we propose _MHP based on dependent Dirichlet process_ (MHP-DDP), a hierarchical nonparametric Bayesian modeling approach for MHP. MHP-DDP flexibly estimates the excitation function via a mixture of scaled Beta distributions, and borrows strengths across dimensions by modeling such mixing distribution as a mixture of a shared Dirichlet process (DP) and a group-specific idiosyncratic DP. We develop two algorithms using Markov chain Monte Carlo (MCMC) and the stochastic variational inference (SVI) algorithm. We also conduct simulations to compare MHP-DDP to benchmark methods where total or no information is borrowed. We show that MHP-DDP outperforms the benchmark methods in terms of lower estimation error for both algorithms, with SVI being computationally efficient than MCMC.

## 1 Introduction

Point processes (e.g., see Cox and Isham, 1980) are a widely used to model temporal event sequence data, i.e., data corresponding to the times of occurrence of a series of countable events. In particular, multivariate Hawkes Processes (MHPs, e.g., see Hawkes, 1971 and Liniger, 2009) have been widely used to model sequences of events that demonstrate _self-_ and _mutually-exciting_ behaviors, i.e., patterns in which the likelihood of events increase after the occurrence of others. Hawkes Processes have been widely applied in a wide range of fields, including seismology (Ogata, 1988), finance (Bacry et al., 2015), electronic health records (Choi et al., 2015; Sun et al., 2024) and social media analysis (Rizoiu et al., 2017). MHPs can be characterized through their conditional intensity functions, which describe the instantaneous rate of arrivals of new events. The excitation function is an important module of the conditional intensity function, as it controls how past events cause the conditional intensity function to change and decay. Excitation functions are often modeled parametrically using exponential functions that assume monotonic excitation decay (Hawkes, 1971). However, the inter-event times in many real-world examples tend to have complex patterns, motivating the need for flexible excitation functions (e.g., see Markwick, 2020 and Rodriguez et al., 2017).

In this paper, we introduce a Bayesian nonparametric model for MHPs that builds on the ideas of Donnet et al. (2020) and Markwick (2020) but addresses various practical questions that have so far remained open. One challenge associated with the estimation of MHPs is that the number of parameters that need to be estimated grows quadratically with the number of dimensions. Hence, with adataset of moderate size, modeling each dimension of the process independently can lead to inefficiencies. Motivated by the observation that often excitation functions looks similar across different dimensions, we propose a hierarchical modeling approach based on mixtures of non-parametric mixtures. More specifically, we adapt the approach introduced in Muller et al. (2004), which models each of the excitation functions as a mixture of an idiosyncratic component and a common component shared by all excitation functions, and study some of the properties of such formulation.

A second challenge in implementing MHP models is computation. As is the case more generally, Markov chain Monte Carlo (MCMC) algorithms are the most common approach to computation for Bayesian models for Hawkes processes (e.g., see Rasmussen, 2013). However, MCMC algorithms for Hawkes process models are often to be too slow even for moderate sample sizes because, except for special cases such as the exponential excitation function, their complexity is quadratic in both the number of observations and the number of dimensions. This challenge is amplified in the case of nonparametric models. To address it, we expand on previous work on the use of stochastic gradient methods for MHPs, and develop a scalable stochastic variational inference (SVI) algorithm that can be used to fit our model. An important part of this development involves a carefully comparison of the performace of SVI and MCMC methods, both in terms of accuracy and speed.

In summary we make two key contributions in this paper: (1) we propose a novel and flexible model for linear MHPs in which the various excitation functions are assigned a joint nonparametric prior that allows us to efficiently borrow information, (2) we develop MCMC and SVI algorithms for estimation and prediction in the context of this nonparametric model, and thoroughly evaluate their relative performance.

## 2 Multivariate Hawkes Processes

Let \(N^{(1)}(t),,N^{(K)}(t)\) be a collection of \(K\) point processes defined on the positive real line \(^{+}\), where \(N^{(k)}(t)\) represents the number of events on dimension \(k\) to occur on the interval \([0,t]\). We denote a generic set of observations from this process by \(=\{(t_{i},d_{i}):i=1,,n\}\), where \(t_{i}^{+}\) represents the timestamp at which the \(i\)-th event occurs and \(d_{i}\{1,,K\}\) represents the dimension in which the event occurs. Then, \(\) follows a multivariate Hawkes process (Hawkes, 1971; Liniger, 2009) if the conditional intensity function on dimension \(k\) has the following form:

\[_{k}(t)_{h 0}[N^{(k)}(t+h)-N^{(k)}(t) _{t}]}{h}=_{k}+_{k=1}^{K}_{t_{i}<t,d_{i}=k} _{,k}_{,k}(t-t_{i}),\]

where we \(_{k}>0\) is the background intensity for dimension \(k\), \(_{,k}>0\) be the parameter that controls the strength by which past events from dimension \(\) influence the occurrence of new events on dimension \(k\), and \(_{,k}():^{+}^{+}\) be the (normalized) excitation function that controls how such influence decays over time. Note that we require that \(_{0}^{}_{,k}(s)s=1\), which ensures that \(_{,k}\) are identifiable. An alternative construction of the MHP is as a multivariate branching process in which the first generation of events in dimension \(k\) (often called 'immigrants" in the literature) arise from a homogeneous Poisson process with rate \(_{k}\), and the points in subsequent generations are generated from non-homogenous Poisson processes with rates given by the \(_{,k}\)s and the interarrival times are controlled by the \(_{,k}\)s. In the sequel, we use the binary matrix \(\) where

\[B_{j,j}=1&\\ 0&,B_{i,j}=1&\\ 0&\]

to encode the latent branching structure associated with a realization of an MHP. The augmented likelihood for the data is then given by

\[(,, )=_{k=1}^{K}_{=1}^{K}[|O_{k,} |(_{k,})-_{i<j\\ d_{i}=k,d_{j}=}B_{i,j}_{k,}(t_{j}-t_{i}) ]\\ +_{=1}^{K}|I_{}|_{}-_{ =1}^{K}_{}T-_{k=1}^{K}_{=1}^{K}_{k,}_{i:d _{i}=k}_{k,}(T-t_{i}),\] (1)where \(|I_{}|=_{d_{i}=}I(B_{ii}=1)\) and \(|O_{k,}|=_{d_{i}=k,d_{j}=,i<j}I(B_{ij}=1)\) denote the number of immigrants for dimension \(k\) and the number of offspring on dimension \(k\) who arise from points on dimension \(\).

## 3 Nonparametric Bayesian modeling of excitation functions for multivariate Hawkes processes

First, we note that for all \(k,=1,,K\), \(_{k,}\) can be interpreted as a probability distribution function on \(^{+}\), with corresponding cumulative distribution function \(_{k,}\). Denote \(\) as the space of all density functions on \(\), MHP-DDP defines a prior on \(_{k,}\) over \(\) as a mixture of scaled Beta distributions, with respect to a random measure \(_{k,}()\):

\[_{k,}(t)=_{^{2}}f_{}(t a,b,T_{0} )d_{k,}(a,b),\] (2)

where the kernel density function

\[f_{}( a,b,T_{0}):= (})^{a-1}(1-})^{b-1}T_{0}^{- 1},\;\;a>0,b>0,0<t<T_{0}\]

is a scaled Beta density function with support on \((0,T_{0})\), indexed by shape parameters \(a\) and \(b\). The Beta mixture allows the excitation function to have a flexible form, including multi-modal structures and heavy tails.

We let \(_{k,}()\) be the mixing distribution for the two shape parameters that corresponds to \(_{k,}\), which is a distribution on \(^{2}\). Denote \((^{2})\) as the space of all probability distributions on \(^{2}\), we define a prior on \(_{k,}\) over \((^{2})\) using the following hierarchical model, following Muller et al. (2004):

\[_{k,}() =_{0}()+(1-)_{k,} (),\;\;k,=1,,K,\] \[_{0},_{k,} (_{},(c_{a},d_{a} )(c_{b},d_{b}))\;\;k,=1,,K,\] (3) \[ (1,1).\]

Under the hierarchical prior, \(_{k,}\) can be expressed as a mixture of two random measures: the common component \(H_{0}\) that is shared across all dimension pairs and the idiosyncratic component \(H_{k,}\) that characterizes the dimension-pair specific behaviors of \(_{k,}\). We let \(\) be the weighting factor that controls how much prior information on the characteristics of the distributions is borrowed across all dimension pairs, and assume it follows a uniform prior between \(0\) and \(1\). Finally, we let \(H_{0}\) and all \(H_{k,},k,=1,,K\) be independent random measures drawn from the Dirichlet process prior with concentration parameter \(_{}\) and base measure \((c_{a},d_{a})(c_{b},d_{b})\), where \(c_{a},d_{a},c_{b},d_{b}\) are fixed hyperparameters that correspond to the two shape parameters of the Beta distribution. We note that there are two extreme cases of special interest: If \(=1\), all dimension pairs share the same triggering kernel, and when \(=0\), \(_{k,},k,=1,,K\) are independently drawn from the same DP prior and share information only through the common hyperparameters in the base measure. Finally, we showed that the prior on the dependent Dirichlet mixture of scaled Beta kernels in (2) and (3) satisfies the Kulback-Leibler property model. The theorem and its proof is outlined in Appendix A.

For implementation, we consider a finite number truncation approximation on the number of mixtures (Ishwaran and James, 2002) for the DP mixture considered in (2). Further, we assign Gamma priors to the Hawkes process parameters:

\[_{} a_{},b_{} (a_{},b_{} ),\] \[_{k,} e_{k,},f_{k,} (e_{k,},f_{k,} ),k,=1,,K.\]

For the computational implementation of the model, we developed a Markov chain Monte Carlo (MCMC) method and a stochastic gradient variational inference (SVI) algorithm, which updates the variational parameters based on minibatches of the dataset.

## 4 Key Results

The data is generated from a multivariate Hawkes process with \(K=2\) dimensions and the following parameter settings: \(=0.6&0.15\\ 0.3&0.6,=0.05\\ 0.1,T=15000\). For the triggering kernels, we consider a scenario where the Beta mixture component is generated from a mixture of two Beta kernels:

\[_{k,}(t)=_{}(-t)+(1- _{})(-^{j,k}t),\]

where \(^{11}&^{12}\\ ^{21}&^{22}=2&0.8\\ 0.8&2\). Additionally, \(_{}=\{0,0.5,1\}\) represents the true degree of information borrowing across dimensions. We fit the model using two methods: a Metropolis-within-Gibbs exact full-batch sampler based on Markov chain Monte Carlo (MCMC) and a stochastic gradient variational inference (SVI) algorithm based on minibatches. We also compare our method with random \(\) ('RANDOM'), to two benchmark versions of both MCMC and SVI where there is no information borrowing ('IDIO') or the triggering kernels are identical ('COMMON'). Additionally, we also consider a frequentist benchmark method based on piecewise basis kernels using the EM algorithm (EM-BK, see Zhou et al., 2013). We evaluate both the point and uncertainty estimation accuracy for the triggering kernels, using a set of performance metrics. We use the root mean integrated sqaured error (RMISE) as a metric for point estimation accuracy:

\[()=}_{k=1}^{K}_{=1}^{K} ^{+}(_{k,}^{}(x)-_{k,}(x))^{2}\,x}\]

Figure 1 shows the results and their specific scenarios. It can be shown that our methods have the lowest estimation error for both methods under most scenarios (except for MCMC when \(=1\), where the difference is very small). For the computation costs, SVI took only less than hour to converge while the MCMC algorithm takes over a day, further suggesting that the usage of scalable stochastic variational algorithm methods dramatically increases computationally efficiency.

## 5 Conclusion

In this work, we developed a novel Multivariate Hawkes processes model for complex temporal event data. Especially, we flexibly modeled the decaying patterns of the triggering kernels using a Dependent Dirichlet process mixture of Beta distributions. Our model shows favorable results in both point and uncertainty prediction methods compared to benchmark models, and could serve as a basis for forecasting and decision making. We developed MCMC and SVI methods for computation, with SVI being computationally efficient and scalable to large datasets.

   &  &  \\  & RANDOM & IDIO & COMMON & RANDOM & IDIO & COMMON \\   & **0.060** & 0.063 & 0.254 & **0.159** & 0.170 & 0.289 \\  & (0.01) & (0.009) & (0.001) & (0.029) & (0.018) & (0.01) \\  & **0.055** & 0.061 & 0.130 & **0.152** & 0.160 & 0.162 \\  & (0.013) & (0.007) & (0.002) & (0.017) & (0.022) & (0.038) \\  & 0.027 & 0.061 & **0.023** & **0.087** & 0.105 & 0.097 \\  & (0.004) & (0.009) & (0.004) & (0.060) & (0.012) & (0.064) \\  

Table 1: RMISE as point estimation metric for all methods under true information-borrowing ratios. The values in the grid cells are averaged over 10 independently datasets, and the standard deviation is shown in the brackets. The numbers in bold refers to the best-performing method among ‘RANDOM’, ‘IDIO’ and ‘COMMON’ for both MCMC and SVI methods.

Kullback-Leibler property of the DDP mixture prior

We first prove the Kullback-Leibler property for a generic Dirichlet process mixture of Beta kernels model in Theorem 1, and then extend to our model setting in Corollary A.1.

**Theorem 1**.: _Let \(_{0}\) be the set of all continuous densities on \(\). If \(f_{0}(x)_{0}\), \(\) is the prior induced by the Beta likelihood mixture kernel and DP\((_{},(c_{a},d_{a}) (c_{b},d_{b}))\) on \(_{0}\), Then \(f_{0}()\), i.e. there exists \(>0\) such that \((\{g:KL(f_{0},g)\})<\)._

Proof.: We show the KL property of \(f_{0}\) by proving Conditions A1-A3 from Theorem 1 in Wu and Ghosal (2008) holds. Since we don't have \(\) in our case, Condition A2 is automatically satisfied. The remaining proof is similar to Theorem 11 in (Wu and Ghosal, 2008), except that the mixing distribution is drawn from a Dirichlet process, and that the two location parameters are also being mixed. For \( f_{0}_{0}\) and \(>0\), there exists a finite Beta mixture function \(f_{P_{}}\) with \(H\) mixtures where

\[f_{P_{}}(x)=_{k=1}^{H}()}{ _{k=1}^{H}f_{0}()}f_{}(x k,H-k)=  f_{}(x a,b)dP_{},\] (4)

and

\[P_{}=_{k=1}^{H}_{k}_{a,b}(a_{k}^{0},b_{k}^{0}), _{k}^{0}=()}{_{k=1}^{H}f_ {0}()}, a_{k}^{0}=k, b_{k}^{0}=H-k,\] (5)

such that

\[_{0}^{1}f_{0}(x)(x)}{f_{P_{}}(x)}dx<.\]

Thus Condition A1 holds. We then show Condition A3 also holds. First, we define \(_{}^{H-1},_{ab}^{h} ^{2},h=1,,H\) as sets such that

\[_{}=\{(_{1},,_{H}):_{h}> _{h}^{0}e^{-},\ \ _{h=1}^{H}_{h}=1\},\]

\[_{ab}^{h}=\{(a_{h},b_{h}):a_{h}<a_{h}^{0},b_{h}<b_{h}^{0},.\]

\[.(a_{h}^{0}-a_{h})( x_{M}+(a_{h}+b_{h})-(a_{h}))+ (b_{h}^{0}-b_{h})((1-x_{M})+(b_{h}+a_{h})-(b_{h}) )<\},\] (6)

where

\[x_{M}=\{d,1-d,^{0}-a_{h}}{a_{h}^{0}-a_{h}+b_{h}^{0}-b_{h}} \}.\]

Finally, we let \(_{ab}=_{h=1}^{H}_{ab}^{h}\), \(:=(_{1},,_{H})^{H-1}, :=(a_{1},,a_{H})^{H},:=(b_{1},,b_{H})^{H}\) and let \(\) be the set of finite mixture distributions induced by \(_{}\) and \(_{ab}\):

\[:=\{P P=_{k=1}^{H}_{k}_{a,b} (a_{k},b_{k}),_{},(a_{h},b _{h})_{ab}^{h}\}.\]

We note that for the chosen \(_{0},_{0},_{0}\), for any \(_{}\), we have

\[^{H}_{h}^{0}f_{}(x a_{h}^{0},b_{h}^{0})}{ _{j=1}^{H}_{h}f_{}(x a_{h}^{0},b_{h}^{0})}<e^{}, x(d,1-d).\] (7)

We then note that

\[(a_{h}^{0}-a_{h}) x+(b_{h}^{0}-b_{h})(1-x)(a_{h}^{0}-a_{h}) x_ {M}+(b_{h}^{0}-b_{h})(1-x_{M}),\ \  x(d,1-d).\] (8)Thus for any \((a_{h},b_{h})_{ab}^{h}\), consider the Cauchy remainder form of the first-order expansion for

\[l(x,a_{h}^{0},b_{h}^{0}) := f_{}(x;a_{h}^{0},b_{h}^{0})(a_{h},b_{h}),\] (9) \[l(x;a_{h}^{0},b_{h}^{0}) =l(x;a_{h},b_{h})+ l(x;a_{h},b_{h})^{T}a_{h} ^{0}-a_{h}\\ b_{h}^{0}-b_{h}+}{2}a_{h}^{0}-a_{h }&b_{h}^{0}-b_{h}^{2}l(x;a_{h}^{*},b_{h}^{*}) a_{h}^{0}-a_{h}\\ b_{h}^{0}-b_{h}\] \[<l(x;a_{h},b_{h})\] \[+(a_{h}^{0}-a_{h})( x+(a_{h}+b_{h})-(a_{h}) )+(b_{h}^{0}-b_{h})((1-x)+(b_{h}+a_{h})-(b_{h}))\] \[<l(x;a_{h},b_{h})+(a_{h}^{0}-a_{h}) x_{M}+(b_{h}^{0}-b_{h}) (1-x_{M})\] \[<l(x;a_{h},b_{h})+,\ \  x(d,1-d).\]

Note that the chain of inequalities are based on equations (6) and (8), and the fact that

\[^{2}l(x;a,b) 0,a>0,b>0.\]

Finally, note that (9) is equivalent to

\[}\ (x a_{h}^{0},b_{h}^{0})}{f_{}\ (x a_{h},b_{h})}<e^{}, x(d,1-d),\] (10)

and if for \(h=1,,H\), \((a_{h},b_{h})_{ab}^{h}\), for any \((_{1},,_{H})_{}\), we have

\[^{H}_{h}f_{}\ (x a_{h}^{0},b_{h}^{0} )}{_{j=1}^{H}_{h}f_{}\ (x a_{h},b_{h})}<e^{ }, x(d,1-d).\]

Combining equations (7) and (10), we have, for any \(_{},(a_{h},b_{h})_{ab}^{h},h=1,,H\), we have

\[^{H}_{h}^{0}f_{}\ (x a_{h}^{0},b_{h}^{0} )}{_{j=1}^{H}_{h}f_{}\ (x a_{h},b_{h})}=^{H}_{h}^{0}f_{}\ (x a_{h}^{0},b_{h}^{0})}{_{j=1}^{H}_{h}f_{}\ (x a_{h}^{0},b_{h}^{0})}^{H}_{h}f_{}\ (x a_{h}^{0},b_{h})}{_{j=1}^{H}_{h}f_{}\ (x a_{h},b_{h})}=e^{},\]

Thus

\[_{d}^{1-d}f_{0}(x)}(x)}{f_{P}(x)}dx<_{d}^{1 -d}f_{0}(x)dx<.\] (11)

We then consider the scenario where \(x(0,d][1-d,1)\). We want to show that the likelihood ratio \(}(x a_{h}^{0},b_{h}^{0})}{f_{} (x a_{h},b_{h})}\) has a uniform finite upper bound over all \(x(0,d][1-d,1)\) and \((a_{h},b_{h})_{ab}^{h},h=1,,H\), i.e.

\[_{x(0,d][1-d,1)}}\ (x a_{h}^{0},b_{h}^{0} )}{f_{}\ (x a_{h},b_{h})} =(a_{h},b_{h})}{(a_{h}^{0},b_{h}^{0})} _{x(0,d][1-d,1)}x^{a_{h}^{0}-a_{h}}(1-x)^{b_{h}^{0}-b_{h}}\] \[=(a_{h},b_{h})}{(a_{h}^{0},b_{h}^{0})}d ^{a_{h}+b_{h}^{0}-a_{h}-b_{h}}(a_{h},b_{h})}{(a_{h}^ {0},b_{h}^{0})},\]

which follows from the fact that \(a_{h}<a_{h}^{0},b_{h}<b_{h}^{0}\). Thus we have

\[_{,_{ab} }_{(0,d][1-d,1) }^{H}_{h}^{0}f_{}\ (x a_{h}^{0},b_{h}^{0} )}{_{j=1}^{H}_{h}f_{}\ (x a_{h},b_{h})}  e^{-}_{, _{ab}}(a_{h},b_{h})}{(a_{h}^{0},b_ {h}^{0})}\] \[_{,_{ab}}(a_{h},b_{h})}{(a_{h}^{0},b_{h}^{0})}:=M<+.\]

Thus we have

\[_{0}^{d}f_{0}(x)}(x)}{f_{P}(x)}dx+_{1-d}^{1}f_ {0}(x)}(x)}{f_{P}(x)}dx<M(F_{0}(d)+1-F_{0}(1-d) ).\]

Thus, we can choose \(d\) small enough such that \(M(F_{0}(d)+1-F_{0}(1-d))<\), such that

\[_{0}^{1}f_{0}(x)}(x)}{f_{P}(x)}dx =_{0}^{d}f_{0}(x)}(x)}{f_{P}(x)}dx+ _{1-d}^{1}f_{0}(x)}(x)}{f_{P}(x)}dx+_{d}^{1-d}

**Corollary A.1**.: _Consider \(K\) continuous densities on \(\), i.e. let \(f_{0}^{1},,f_{0}^{K}_{0}\). Consider the following joint prior \(^{*}:_{h=1}^{H}_{0}\) for \((f^{1},,f^{K})\) such that_

\[ f^{k}&= g_{0}+(1-)g^{k},k= 1,,K,\\ g_{k}(x)&= f_{}(x a,b)dP(a,b),k= 0,1,,K,\\ P&(_{},(c_{a},d_{a})(c_{b},d_{b})),\\ &(1,1),\] (12)

_we have,_

\[^{*}(\{f^{1},,f^{K}:(f_{0}^{k},f^{k})<,k= 1,,K\})>0.\]

Proof.: Let \(f_{0}^{0}(x):=_{k=1}^{K}f_{0}^{k}(x)\). For \(>0\), we let

\[_{0}=\{g^{0}_{0}:(f_{0}^{0},g^{0})<\},^{k}=\{g^{k}_{0}:( f_{0}^{k},g^{k})<\}.\]

Note that from Theorem 1.1 we have \(_{g^{k}}(g^{k}^{k})>0,k=0,1,,K\). Let \(M:=_{1 j,k M}_{g^{0}_{0}}(f_{ij}^{0},g _{0})<+\), we have, based on the convexity of KL divergence:

\[(f_{0}^{k}, g^{0}+(1-)g^{k})(f_{0 }^{k},g^{0})+(1-)(f_{0}^{k},g^{k})< M+(1-)<,\]

for all \(<\), \(g^{k}^{k},k=0,1,,K\). Thus

\[&^{*}(\{f^{1},,f^{K}: (f_{0}^{k},f^{k})<,k=1,,K\})\\ =&^{*}(\{g^{0},g^{1},,g^{K}, :(f_{0}^{k}, g^{0}+(1-)g^{k})< ,k=1,,K\})\\ >&^{*}(\{g^{0},g^{1},,g^{K}, :g^{k}^{k},k=0,1,,K,<\})\\ &=_{0 k K}_{g^{k}}(g^{k}^{k})_{ }(<)>0.\]