# Pre-training Differentially Private Models with

Limited Public Data

 Zhiqi Bu

Amazon

&Xinwei Zhang\({}^{*}\)

University of Southern California

&Sheng Zha

Amazon

&Mingyi Hong

University of Minnesota

George Karypis

Amazon

###### Abstract

The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when DP is applied during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process. In this work, we provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement, through the lens of Hessian matrix for large neural networks. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, using only 10% of public data and 90% of private data, our strategy can achieve DP accuracy of 41.5% on ImageNet-21k (with \(=8\)), as well as non-DP accuracy of 55.7% and 60.0% on downstream tasks Places365 and iNaturalist-2021, respectively, on par with state-of-the-art standard pre-training and substantially outperforming existing DP pre-trained models. Our DP pre-trained models are released in fastDP library (https://github.com/awslabs/fast-differential-privacy/releases/tag/v2.1).

## 1 Introduction

Large pre-trained models have been the backbone of computer vision and natural language processing. Finetuning or zero/few-shots learning (including in-context learning) based on these models can achieve superior performance. In particular, differentially private (DP) fine-tuning, such as full-parameter training, LoRA, Adapter, BiTiT, and linear probing , has shown to be almost as accurate as the standard non-DP fine-tuning on GPT , ViT , and ResNet  models, while protecting the privacy of fine-tuning data. To be more specific, DP language and vision models are highly effective in defending against canary insertion attacks  and membership inference attacks ; DP fine-tuned GPT2 also reduces the personally identifiable information leakage by \(5 10\) times compared with its non-DP counterpart .

These DP fine-tuned models all follow a two-stage training procedure, in which the first stage trains a model on large public datasets (e.g. ImageNet) from scratch without privacy protection, and the second stage fine-tunes on relatively small private datasets (e.g. CIFAR10). However, a growing concern has been raised against the pre-training on the vast amount of web-collected data . The pre-trained models could memorize and re-generate the sensitive information in thetraining data, which include copyright content in books and codes, images of faces and nudity, and other personally identifiable information such as address and phone numbers, even if the data have been pre-processed by some content filters. While it is common to use close-source or proprietary datasets (e.g. JFT [75; 24; 58]) instead and not release the models trained on these datasets, this approach renders the models not reproducible and may still violate the data privacy implicitly.

Consequently, because of the uncertainty in seemingly safe-to-use public datasets, it is important to apply DP to the pre-training phase, which is computationally feasible, since DP optimization can be as efficient as the standard non-DP training [13; 38]. However, DP optimization without any public data suffers from slow convergence, and suboptimal performance: even on CIFAR10, the non-DP accuracy drops from \(>95\%\) to \(<70\%\) at \(=8\); on GPT2, the non-DP BLEU score degrades from 65.73 (quality often better than human) to 15.457 (hard to get the gist) at \(=3\). In short, pre-training and fine-tuning differ significantly in the amount of data and computation resources used, as well as in optimization setups. We summarize the difference in Table 1.

### Contributions

Our main contributions are listed below. We emphasize that we have taken the privacy accounting and the convergence (i.e. the dependence of loss on \((B,T,)\) and \(B\)) jointly into consideration.

**(1)** We provide a new perspective of analyzing the loss improvement in DP training with different optimizers. Specifically, in Section 2, we propose a Hessian-based framework to analyze the per-iteration improvement on the generalization loss and the effects of per-sample gradient clipping, noising, and hyperparameter choices (e.g., batch size, learning rate).

**(2)** Based on the framework, we can analyze the effect that DP mechanisms, especially the DP noise but less so the DP clipping, have on pre-training and fine-tuning stages. This leads to some theoretical justifications about why pre-training is more vulnerable to DP noise; see Figure 1 for some empirical comparisons, and Section 3 for some in-depth discussion.

**(3)** Our analysis suggests that the deceleration due to DP mechanisms can be mitigated by using a certain amount of public data. We then propose a DP continual pre-training strategy that continues the pre-training privately from a non-DP initialization. Our strategy is easily implementable, automatic and effective, obtaining accuracy on upstream and downstream tasks that is on par with its non-DP variants, while significantly outperforming state-of-the-art DP pretraining algorithms (see Figure 2).

### Related works

This work is closely related to other works in DP convergence analysis, DP fine-tuning of large models, the continual training and the Hessian-based analysis. We refer to Appendix C for an extended discussion.

   & pre-training & fine-tuning \\  Dataset size & large & small \\  Training iterations & large & small \\ (amount of compute) & large & small \\  Trainable model parameters & 100\% & \(0.1 100\)\% \\  Major cause of & & \\ performance degradation & DP noising & DP clipping \\  

Table 1: Comparing DP pre-training and DP fine-tuning.

Figure 1: Comparison among the convergence of standard SGD, clipped SGD without noise, noisy SGD without clipping, and DP-SGD in different tasks and training stages.

### Notions & settings

We denote the mini-batch size as \(B\) with index set \(\), dataset size as \(n\), number of training iterations as \(T\), iteration index as \(t T\), learning rate as \(\), model parameters as \(^{d}\), and training samples as \(x\). We use \(\{,,\}\) for \(\{\}\).

We study DP optimization under a fixed computation budget (with a fixed number of samples), a fixed privacy budget (with a fixed DP guarantee), and limited availability of public data.

### Computation budget

We consider a fixed computation budget, so that the training ends when a fixed number of samples \(S:=BT\) have been processed. As \(B\) increases, the number of iterations \(T\) decreases linearly, while the per-iteration training time increases almost linearly. This is because foundation models are generally trained with large batch size, which requires the use of distributed learning and gradient accumulation2. For example, Vision Transformers (ViT, [27; 74] is trained with \(B=4\)k, GPT-3  and LLaMA  with \(B 2\)k, DP-ResNet/ViT with \(B=4\)k in  and \(B=1\)M in , DP-RoBERTa with \(B=2\)k in , and DP-GPT2 with \(B=1\)k in .

As a consequence, the batch size \(B\) has nearly zero influence on the total training time, leaving its effect only on the convergence speed.

### Privacy budget

**Definition 1.1** ([29; 26]).: A randomized algorithm \(\) is \((,)\)-DP if, for any two neighboring datasets \(,^{}\) that differ by one sample and for any event \(\),

\[[()]^{ }[(^{}) ]+,<1/n.\]

We consider a fixed privacy budget \((,)\), to be realized through the DP optimizers in (2). Essentially, a noise \((0,)\) is injected to the gradient at each iteration, whereas the noise magnitude \((B)\) is determined by privacy accountants such as RDP (default) , PRV  and GDP [26; 8] (see Figure 3).

Understanding DP training through the lens of Hessian

In this section, we derive the impact of different components in DP training, including the per-sample clipping, the noising, and hyper-parameter choices. To do so, we characterize the per-iteration per-sample loss improvement through the lens of Hessian.

We aim to optimize the generalization loss (equivalently the expected risk) for any differentiable loss function, e.g. the cross entropy, mean squared error, or adversarial loss :

\[_{^{d}}L()=_{x}[L( ,x)].\] (1)

We denote the per-sample gradient \(_{i}():=,x_{i})}{ }^{d}\), the oracle gradient \(():=_{x}[L(,x)]}{ }^{d}\), and the oracle Hessian matrix \(():=_{x}[L(,x)]}{ ^{2}}^{d d}\). In Assumption 2.1, samples follow identically and independently (i.i.d.) from some data distribution, with no restriction on the covariance structure \(()\).

**Assumption 2.1**.: Per-sample gradients \(_{i}()\) are i.i.d with

\[[_{i}()]=(), (_{i}())=().\]

Consider the general DP optimizers, such as SGD and Adam , which update the parameters with the privatized gradient,

\[=}C_{i}_{i}+ (0,_{d})}{B}} _{i}+(0,_{d})}{B}.\] (2)

Here \(C_{i}:=C(\|_{i}\|;R)\) is the per-sample clipping factor that restricts the sensitivity of \(_{i}C_{i}_{i}\) to some constant \(R\), i.e. \(\|C_{i}_{i}\| R\). We set \(\|C_{i}_{i}\| 1\) (thus omitting \(R\) throughout the paper) following the re-parameterized gradient clipping  and the automatic (AUTO) clipping , whose \(C_{i}\)'s are listed below:

\[C_{i,}=\{_{i}\|}, \}C_{i,}=_{i}\|}.\] (3)

Figure 4 illustrates the values of \(C_{i}\) under different clipping functions. Note that in (2), we employ a crucial approximation \(c[C_{i}]\) so as to unify the formula of DP and non-DP SGD in Remark 2.2. This approximation only holds when the directions of vectors \(_{i}C_{i}_{i}\) and \(_{i}_{i}\) are very close, i.e., there is little _per-sample clipping bias_. Such approximation is empirically validated in Figure 1, where we observe from the 'SGD' and 'SGD+clipping' curves that the convergence (without noising) is not much influenced by the bias of per-sample gradient clipping.

**Remark 2.2**.: Setting \(c=1\) and \(=0\), the gradient (2) reduces to the standard mini-batch gradient. Hence, the difference between SGD and DP-SGD is characterized by \((,c)\).

### Per-iteration improvement of DP-SGD

Next, we characterize and analyze the per-iteration improvement of DP-SGD through the lens of Hessian, under different choices of hyperparameters and clipping functions: \(_{t+1}=_{t}-_{t}\). The extension of the analysis to more general optimizers (e.g., DP-Adam) is given in Section 3.4. We are interested in minimizing the second-order Taylor approximation of \(L(-)\), which is sufficiently accurate since parameter updates are often quite small . The loss improvement in one iteration can be approximated as:

\[L()-L(-)^{} -}{2}^{}.\]

Taking the expectation of the right-hand side, we obtain the expected per-iteration loss improvement (derived in Appendix A.1):

\[ L:=^{}[]-}{2}((())+[]^{}[]).\] (4)

By applying Assumption 2.1 and (2), we have \([]=c,()=c^{2}/B+^{2}/B^{2}.\) Substitute to (4), we obtain a quadratic function of \(\):

\[ L_{}(,B):= c^{}-}{2}(c^{2}^{}+ ()}{B}+()}{B^{2}}).\] (5)We denote the batch size used for SGD and DP-SGD as \(B_{}\) and \(B_{}\), respectively. Then by optimizing the learning rate \(\)3, the per-sample and per-iteration improvement simplifies to

\[_{} L_{}/B_{}= L_{}^{}( B_{}):=|^{4}}{B_{}^{} +()+^{2} ()/(B_{} c^{2})}.\] (6)

Given that the total number of processed samples is fixed at \(S=BT\), (6) can be used as a metric to evaluate the _data efficiency_ of DP and non-DP training with different \(T,B\) and \((,)\).

### Per-iteration improvement of vanilla SGD

We can analyze the loss improvement of standard SGD as a sub-case of DP-SGD by substituting \(c=1,=0\) into (5), according to Remark 2.2:

\[ L_{}:=^{}-}{2} (()}{B_{}}+^{}), L_{}^{}(B):= |^{4}}{B_{}^{}+()}\] (7)

We visualize (6), (7), and their individual terms in Figure 5.

**Implication 2.3** (Better DP mechanism helps).: From (6), it is clear that smaller \(\) and larger \(c\) (hence larger \(C_{i}\)) can help DP training. To reduce \(\), we refer to Section 3.1 for a discussion of methods. For the clipping, under the same sensitivity bound, AUTO clipping  gives the largest \(C_{i}\) among all clipping functions (see Figure 4), and therefore is more preferred to use in practice.

**Implication 2.4** (Batch size should be large, but not too large).: As visualized by the red solid curves in Figure 5, there exists an optimal batch size (marked in red dashed vertical lines)

\[B_{}^{}:=_{B} L_{}^{}(B) ()}{c^{2}^{} }}.\]

Compared to previous DP literature, which encourages the batch size to be as a large as possible, our derivation of \(B_{}^{}\) indicates a sweet pot: while we also support the use of large batch size, we highlight the data inefficiency if \(B_{}\) is too large, a case that is often overlooked.

## 3 Impact of per-sample clipping and noising

In this section, we examine the effects of per-sample gradient clipping and noising on the DP training, leveraging the per-iteration per-sample loss improvement derived in Section 2. Specifically, we define and analyze a **"decelerator"** term that characterizes the slowdown by DP optimizers.

Comparing DP-SGD to SGD, we can attribute the slow convergence of DP optimization to the term \(^{2}()/(Bc^{2})\) in (6), which is not present in the standard training (7). We refer to such a term as

\[:}()}{Bc^{2}} ()|_{i}|^{2}}{B},\] (8)

which couples the effects of per-sample gradient clipping and noise through the trace of Hessian. We note that \(()\) in (8) characterizes the curvature (i.e. sharpness or flatness) of the loss landscape, which strongly correlates with the downstream performance [51; 45; 96; 31].

Next, we discuss how the decelerator impacts the non-DP training, DP pre-training and fine-tuning.

Figure 5: Illustration of different terms in (6) and (7). Left sub-plots depict the denominators in (6) and (7). Right sub-plots depict the whole terms and optimal batch sizes.

### No noise, (almost) no deceleration

When \(=0\) (i.e., no DP noise), the decelerator vanishes and hence (6) reduces to (7), even if the per-sample gradient clipping is used. We empirically verified this in Figure 1 (see blue and black curves), where we see that the difference in convergence with or without clipping is negligible.

Given that DP noise is critical to the convergence speed, we highlight some techniques to reduce \(\) under the same budget of \((,)\): the advances in privacy accounting theory can justify smaller noise; algorithms such as LoRA, low-pass and Kalman filters [93; 92] can reduce the effective noise.

### DP pre-training can be vulnerable to noise

When \( 0\), the decelerator is non-zero. Therefore, DP training is slowed down by the noise; in Figure 1, SGD with noise (yellow and red curves) has worse performance than SGD without noise (black and blue curves). Furthermore, in pre-training, the decelerator is relatively large in the denominator of (6), i.e., \(B^{}+() ()}{B^{2}}\) when \(B B_{}^{}\) (see left sub-plot of Figure 5(a)), and therefore the slowdown can be significant.

Note that the deceleration issue cannot be resolved by increasing \(B\). Although increasing \(B\) improves the relative speed of DP convergence in comparison to non-DP, i.e., the decelerator decreases, it hurts the absolute speed since \(B^{}\) increases, and thus the loss improvement (6) also worsens (see right sub-plot of Figure 5(a)). Therefore, to design an efficient DP pre-training strategy, we must keep \(B\) moderate and reduce the decelerator simultaneously.

### DP fine-tuning is robust to noise

Empirical evidence has shown that DP fine-tuning is comparable to (though slightly worse than) the standard non-DP fine-tuning [89; 49; 24; 58; 10; 14; 12], despite that \( 0\). Such a phenomenon implies that comparing public and DP finetuning, we have \( L_{}^{}(B) L_{}^{}(B)\). That is, the decelerator becomes small after the public pre-training. This is conceptually illustrated in Figure 5(b), where the DP curve is close to the non-DP curve at moderate \(B\) during fine-tuning, but not so during pre-training.

To understand the stark contrast between DP fine-tuning and DP pre-training, we plug in the optimal \(B_{}^{}=()}{^{2} }}\) from Implication 2.4 to \( L_{}^{}(B)\). Then, we have the optimal improvement of DP-SGD as

\[ L_{}^{}(B_{}^{})=|^{4}}{2^{}^{2} ()/c^{2}}+()}.\]

**Implication 3.1**.: Notice that by choosing \(B_{}=2B_{}^{}\) in (7), we have that _DP-SGD with the optimal batch size is as fast as the standard SGD with twice the batch size_,

\[ L_{}^{}(2B_{}^{})= L_{}^ {}(B_{}^{}).\] (9)

Moreover, if \(B_{}^{}\) is moderate, then DP-SGD can converge at similar speed to a fast converging SGD that uses a moderate batch size.

**Remark 3.2**.: From (7), we observe that non-DP training is data-efficient only if \(B_{}^{}()\). Otherwise, we can decrease \(B_{}\) to effectively improve \( L_{}^{}\). Therefore, DP training is data-efficient only if \(B_{}^{}B_{}( )}{2^{}}\), which holds in fine-tuning but not in early stage of pre-training4. We illustrate the magnitude of the three terms in (6) for pre-training and fine-tuning stages in Figure 6.

In the fine-tuning phase of Figure 6, \(()\) quickly decreases and so does the decelerator. Hence a moderate \(B_{} 100\) can allow fast convergence. However, in the pre-training phase, \(()\) increases to a large value within 5 epochs and remains for a long time (say epoch 5 to 40) before it decreases again. Consequently, DP convergence is initially fast but only for a short period and overall DP optimization is much slower than non-DP optimization, as shown in Figure 1(a)(c).

### Extension to general optimizers

The analysis in the previous two sections can be easily extended to arbitrary optimizers as well as techniques such as weight decay, gradient clipping, and parameter-efficient fine-tuning (PEFT). Let \(p\) be an optimizer's post-processor of gradient and consider \(_{t+1}=_{t}- p(_{t})\).

For examples, following the notation in Pytorch library , we can write Adam and SGD with momentum (\(\)) and weight decay (\(\)),

\[ p(;,)= +(1-_{1})}{1-_{1}^{2}}}{ +(1-_{2})^{2}}{1-_{2}^{2}}}+10^{-8}}(,) p(;,)=+ +.\]

Similarly, we can write any PEFT for any optimizer, e.g. PEFT (SGD): \(p(;)=\) where \(\{0,1\}\) is an element-wise mask that makes a parameter non-trainable or frozen when \(_{i}=0\).

Specially, we consider optimizers such that \(p()\) is scale-invariant (i.e. \(p(c)=p()\)), such as SignSGD/Adam  or normalized SGD/LAMB [61; 55; 87]. Applying Assumption 2.1 and the optimal learning rate, we derive the expected per-sample per-iteration improvement in Implication 3.3, leaving the details Appendix A.5.

**Implication 3.3**.: Suppose the post-processor \(p()\) is scale-invariant. Denote \(=p()\), \(^{}=p^{}()\), the per-sample per-iteration improvement \(_{} L()/B\) simplifies to

\[^{}|^{2}}{B^{} +(^{}^ {})+^{2}(^{} ^{})/(Bc^{2})}\]

Interestingly, similar to the decelerator of DP-SGD (8), the decelerator \(^{2}(^{}^{})/(Bc ^{2})\) of these DP optimizers also couples the per-sample gradient clipping, the noise and the Hessian, rendering the theoretical implications from DP-SGD extendable to general DP optimizers.

## 4 Continual pre-training with DP

### Necessity of public data in DP pre-training

In this section, we propose the DP continual pre-training strategy and demonstrate that the deceleration by DP can be effectively mitigated by using a certain amount of public data. We consider the mixed data training that uses both public data (with subscript \({}_{0}\) for related hyperparameters) and private data (with subscript \({}_{1}\)). Then SGD becomes

\[_{,t}:=}{B_{0}}_{j_{0}} _{j,t}+)}{B_{1}}(_{i_{1}}C_{i,t }_{i,t}+(0,_{d})).\]

Here \(_{t}\) controls the ratio of privatized and non-privatized gradients, taking different forms by public training (OnlyPublic, \(_{t}=1\)), private training (OnlyPrivate, \(_{t}=0\)), DPMD , a tunable constant [32; 52], and Sample [30; 44].

Using the mixed gradient \(_{}\) and following (4), we can show that expected loss improvement is a bivariate quadratic function of \((,)\) (see Appendix A.3). After minimizing with respect to both variables, we obtain:

\[^{}=(() B_{1}/B_{0}}{()+^{2} ()/(B_{1}c^{2})}+1)^{-1}\] (10)

  Ours & DPMD & Sample & OnlyPublic & OnlyPrivate \\  \((t<sT)\) & 1-cos \(\) & \(}}{n_{}+n_{}}\) & 1 & 0 \\  

Table 2: Summary of \(_{t}\) by mixed data training methods.

Figure 6: Evolution of terms in (6) and (7) that explains the deceleration of DP optimization, during pre-training (left two) and fine-tuning (right two) ViT-Base on CIFAR100.

**Remark 4.1**.: We see that the optimal choice in (10) gives \(^{} 0\), which indicates that \( L^{}_{}< L^{}_{}\), i.e. the public data helps. On the other hand, the fact that optimal \(^{} 1\) also indicates that \( L^{}_{}< L^{}_{}\), i.e. the private data helps.

### DP continual pre-training strategy overview

Motivated by Section 3 and Remark 4.1, we propose a two-phase DP continual pre-training strategy in Appendix D: public pre-training for \(sT\) steps followed by private continual pre-training for \((1-s)T\) steps, which is equivalent to setting \(_{t}=(t<sT)\) and the constant \(0<s<1\) controls the portion of steps of public training.

**Remark 4.2**.: Although (10) suggests that the optimal \(_{t}(0,1)\), we only set binary \(_{t}\{0,1\}\) so as to not implement two data loaders and two back-propagation mechanisms simultaneously (one for DP and one for standard). Note the methods in Table 2 are difficult to scale and not yet openly implemented on distributed systems due to memory and synchronization issues.

We highlight that DP continual pre-training is as distinct from DP fine-tuning as their non-DP counter-parts, despite both methods extending the learning from a pre-trained phase: the continual pre-training learns common knowledge without adapting to a specific task and serves a richer foundation for many downstream tasks (see more discussion in Appendix C). In fact, we show in Appendix A.7 that the loss improvement of DP continual pre-training can be almost as fast as non-DP pre-training on the full data (FullyPublic), thus closing the utility gap.

The switching (i.e. selecting \(s\)) can be automatically determined by public statistics without using any DP budget. For example, we can use early stopping based on the loss or accuracy, switching after the metrics stop improving (see Figure 7). Alternatively, we can monitor \(B^{}_{}\) and switch when it drops to a moderate value so that the DP training converges fast by (9).

## 5 DP vision foundation models on ImageNet

We leverage the DP continual pre-training strategy discussed in Section 4 to train vision transformers  on ImageNet datasets. We use ImageNet-1k (1.3M images, 1k classes; ) for public pre-training, then ImageNet-11k (formally known as ImageNet-21k-P5, 11M images, 11k classes; ) for private pre-training. Notice that ImageNet-11k/21k is significantly harder to learn, with SOTA accuracy \( 47\%\)[70; 74] as compared to \(>85\%\) for ImageNet-1k. We apply data augmentation including random flipping, contract shift, rotation, and resizing to \(224 224\).

We evaluate our DP ViT on upstream and downstream tasks (including few-shot), achieving high accuracy under low computation budget (compared to existing DP pre-trained models in Table 4). In short, we show that unlocking more data (90% of full ImageNet), which cannot be used by DINO due to privacy concern, is significantly beneficial.

Figure 7: Pre-training GPT2-small on CodeParrot with different pre-training strategies (\(=8\) if Mixed or FullyPrivate).

   & public data & private data & privacy \\  OnlyPublic & SGD & not used & Yes \\ OnlyPrivate & not used & DP-SGD & Yes \\ FullyPublic & SGD & SGD & No \\ FullyPrivate & DP-SGD & DP-SGD & Yes \\ Mixed (our \(_{}\)) & SGD & DP-SGD & Yes \\  

Table 3: Optimization by different training strategies.

### Training strategy

For the public pre-training, we follow the self-supervised learning by  (self-distillation with no labels, or DINO), whereas the private continual pre-training is a supervised learning following 6. We employ AdamW optimizer with batch size \(B=4096\) and learning rate \(=0.0002\) set by the line search. Our training strategy is similar to a concurrent work , with critical differences highlighted in Appendix C.

When automatically switching from public to private pre-training, the classification head (the last layer) is re-initialized because the number of classes is different in the two pre-training phases: we switch from 1k classes to 11k classes. This switching is triggered by early stopping. In the continual pre-training phase, we train with DP linear probing for 10 epochs then with DP full-parameter training for 20 epochs, with a total \(\)100k training steps. This strategy achieves an upstream accuracy 41.5% on ImageNet-11k by our ViT-Base under \(=8\), and 39.8% under \(=2\).

### Algorithm implementation

We employ fastDP library to apply the DP-AdamW with automatic per-sample clipping function  and layer-wise clipping style . Specifically, the DP optimization is under the multi-GPU distributed system, using DP-ZeRO  and mixed-precision training, so as to enjoy the same training speed and memory efficiency as the non-DP training. We calibrate the DP noise using the improved Renyi accountant.

### Downstream performance

Our DP pre-training learns highly transferable representations, demonstrated through the strong performance on a list of downstream datasets. We summarize in Table 4 a number of models from previous literature for comparison. We mark DP pre-trained models in green and leave **non-DP models** in black. The most informative baselines are DINO and MIIIL, since our DP models continue the pre-training from DINO, following a strategy similar to MIIIL.

In Table 5, we compare different pre-training strategies (all non-DP except ours) leveraging the same dataset and same model architecture - ViT-Base (86M param). Our evaluation shows that DP pre-trained models achieve high downstream accuracy under standard and non-DP fine-tuning: 98.4% on CIFAR10, 90.2% on CIFAR100, 86.5% on Food101 and 96.8% on SVHN. Our DP continual pre-training clearly improves upon DINO, with \(+0.3 2.0\%\) on accuracy, and is comparable to the non-DP pre-trained MIIIL that uses twice the data size (1.4B v.s. our 0.7B).

In Table 6, when the downstream tasks (non-DP) are more challenging with only a few data samples to learn from, our DP model substantially outperforms previous DP pre-trained models across all settings, for example, by \(+19 38\%\) on CIFAR100 when compared to ViP and TAN. We attribute the success to the high quality of pre-training data, i.e. ImageNet-1k/11k, in contrast to Shaders (by comparing DINO to (Syn)ViP) and LAION (by comparing the improvement from DINO to ours and from (Syn)ViP to ViP).

   & reference & model & pre-training & continual training & non-privacy & images \(\) \\  TAN &  & ImageNet-1k & — & — & 1.2B \\ (Syn)ViP &  & ViT-Base & Shaders21k\({}^{}\) & & Shaders21k & 1.3B \\ ViP &  & ViT-Base & Shaders21k\({}^{}\) & LAION400M\({}^{}\) & Shaders21k & 1.9B \\ DINO &  & ViT-Base & ImageNet-1k\({}^{}\) & — & ImageNet-1k & 0.3B \\ Ours & This work & ViT-Base & ImageNet-1k\({}^{}\) & ImageNet-11k & ImageNet-1k & 0.7B \\ MIIIL &  & ViT-Base & ImageNet-1k & ImageNet-11k & ImageNet-11k & 1.4B \\ Original &  & ViT-Base & ImageNet-21k & ImageNet-1k & ImageNet-21k & 1.2B \\ AugReg &  & ViT-Base & ImageNet-1k & ImageNet-21k & ImageNet-21k & 4.3B \\ NFnet-JFT &  & ResNet-50 & JFT300M & — & JFT300M & 4.0B \\  

Table 4: Pre-training strategies of models. Standard non-DP training is marked in black; DP training is in green. \(\) indicates self-supervised without using the labels. “Images \(\)” is the total number of images used (dataset size\(\)epochs). “Non-privacy” means no DP guarantee on a subset of training data due to the non-DP pre-training phase.

In Table 77, we extend the evaluation of full fine-tuning and linear probing to SOTA non-DP baselines and to million-image scale Our DP model achieves 55.7% (+9.6% over ViP) on Places365 with 1.8M images and 60.0% (+21.9% over ViP) on iNat2021 with 2.7M images. The current non-DP SOTA is 57-60% on Places365 [24; 27] and about 64% on iNat2021 [80; 60] after pre-training on \(2.7 4\)B images. This showcases the effectiveness of DP pre-training as our models only leverage 0.7B images.

### Privacy protection

We employ a white-box membership inference attack (MIA) to evaluate the data protection by our DP pre-training: 1) for each image in ImageNet-11k, we compute its output logits and loss, which serves as the feature of the MIA dataset; 2) we randomly select \(50\%\) of the testing images and the same number of training images (\(522,496\) samples) as the MIA test set, and the rest data as MIA train set; 3) we label the training images as class "1" and testing images as class "0". This creates the MIA dataset with 11k features and binary labels.

We fit a logistic regression with the MIA training set to classify whether an image belongs to the training set of ImageNet-11k (class "1") or not. We report the results on the MIA testing set in Table 8, showing the effectiveness of DP protection when \( 8\).

## 6 Discussion

In this paper, we conduct an insightful and unified convergence analysis on DP optimization. Specifically, we identify the decelerator (8) of DP training as a result of the per-sample gradient clipping, the noise and the Hessian, which can be significantly mitigated by a small amount (\(<10\%\)) of public training. Consequently, we propose DP continual pre-training that is almost as accurate and implementable as the fully public pre-training.

   & Aircraft & Aircraft & CIFAR100 & CIFAR100 & fine-tune \\  & (10-shot) & (20-shot) & (10-shot) & (30-shot) & epochs \\  \(_{}\) & 22.84 & 37.93 & 27.78 & 42.35 & 200 \\ (SynViP) & 21.79 & 46.85 & 38.96 & 55.84 & 200 \\ \(_{}\) & 31.62 & 53.05 & 40.95 & 57.52 & 200 \\ \(\) & 32.04 & 45.61 & 47.31 & 66.92 & 100 \\ \(_{}\) & 36.42 & 48.27 & 64.74 & 74.62 & 100 \\ \(_{}\) & **42.57** & **57.15** & **65.26** & **76.38** & 100 \\   
   & ImageNet-1k & Places365 & 1Nat2021 & fine-tune \\  \# images & 1M & 1.8M & 2.7M & epochs \\  \(_{}\) & 49.0 & 40.5 & 31.7 & 90 /90 /90 \\ (SynViP) & 49.8 & 43.2 & 32.4 & 90 /90 /90 \\ \(_{}\) & 55.7 & 46.1 & 38.1 & 90 /90 /90 \\ \(\) & **76.4** & 52.1 & 43.5 & 8 /5 /10 \\ \(_{}\) & **76.2** & **52.5** & **46.5** & 8 /5 /10 \\ \(_{}\) & 22.9 & **53.0** & **49.1** & 8 /5 /10 \\ \(_{}\) & **85.6** & **55.6** & **57.2** & 8 /5 /10 \\ \(_{}\) & **78.5** & **55.7** & **60.0** & **8 /5 /10** \\  \(\)-IFT & 74.1 & 54.5 & — & 10 /26 / — \\  

Table 7: Linear-probing accuracy (non-DP) of pre-trained models, except “full” indicating full-parameter.

  Pre-training &  &  &  &  \\  Pre-training & non-DP & \(=8\) & \(=2\) & non-DP & \(=8\) & \(=2\) & non-DP & \(=8\) & \(=2\) & non-DP & \(=8\) & \(=2\) \\  DINO (0.3B) & 98.1 & 97.2 & 97.0 & 88.2 & 84.7 & 82.7 & 77.2 & 73.5 & 96.2 & 91.7 & 90.3 \\  \(_{}\) (0.7B) & 97.8 & 96.6 & 96.1 & 88.8 & 83.1 & 81.1 & 84.8 & 75.5 & 72.5 & 96.3 & 91.3 & 90.1 \\  \(_{}\) (0.7B) & 98.4 & 97.2 & 96.9 & 90.2 & 85.0 & 82.8 & 86.5 & 78.4 & 75.3 & 96.8 & 92.5 & 91.3 \\  MIL (1.4B) & 98.8 & 98.5 & 98.2 & 91.4 & 90.9 & 89.2 & 87.2 & 84.5 & 83.0 & 96.8 & 93.3 & 92.0 \\  ViT.base (1.2B) & 98.9 & 98.3 & 98.1 & 92.6 & 89.9 & 88.2 & 89.4 & 85.5 & 83.1 & 96.9 & 93.5 & 92.5 \\  AugReg (4.3B) & 98.9 & 98.8 & 98.5 & 93.1 & 91.2 & 90.4 & 90.2 & 87.6 & 85.7 & 96.9 & 93.8 & 92.5 \\  

Table 5: Standard/DP fine-tuning accuracy with the same architecture (ViT-Base) and pre-training dataset (ImageNet-21k) up to subsampling and preprocessing. Number of processed images by each model is indicated in the parenthesis.

   & Accuracy & Precision & Recall & F1 & AUC \\  \(_{}\) & **50.1\%** & **50.1\%** & **47.5\%** & **0.49** & **0.50** \\ \(_{}\) & \(51.3\%\) & \(51.6\%\) & \(64.4\%\) & \(0.57\) & \(0.51\) \\ \(_{}\) & \(53.8\%\) & \(54.1\%\) & \(71.2\%\) & \(0.62\) & \(0.54\) \\ \(\) & \(54.0\%\) & \(58.5\%\) & \(77.3\%\) & \(0.67\) & \(0.62\) \\  

Table 8: Membership inference attack results. Values closer to \(0.5\) indicate better privacy protection.