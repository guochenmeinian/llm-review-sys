# ReF-LDM: A Latent Diffusion Model for

Reference-based Face Image Restoration

Chi-Wei Hsiao\({}^{1}\) Yu-Lun Liu\({}^{2}\) Cheng-Kun Yang\({}^{1}\) Sheng-Po Kuo\({}^{1}\)

Yucheun Kevin Jou\({}^{1}\) Chia-Ping Chen\({}^{1}\)

\({}^{1}\)MediaTek \({}^{2}\)National Yang Ming Chiao Tung University

###### Abstract

While recent works on blind face image restoration have successfully produced impressive high-quality (HQ) images with abundant details from low-quality (LQ) input images, the generated content may not accurately reflect the real appearance of a person. To address this problem, incorporating well-shot personal images as additional reference inputs could be a promising strategy. Inspired by the recent success of the Latent Diffusion Model (LDM), we propose ReF-LDM--an adaptation of LDM designed to generate HQ face images conditioned on one LQ image and multiple HQ reference images. Our model integrates an effective and efficient mechanism, CacheKV, to leverage the reference images during the generation process. Additionally, we design a timestep-scaled identity loss, enabling our LDM-based model to focus on learning the discriminating features of human faces. Lastly, we construct FFHQ-Ref, a dataset consisting of 20,405 high-quality (HQ) face images with corresponding reference images, which can serve as both training and evaluation data for reference-based face restoration models.

## 1 Introduction

Recent works  have achieved impressive results in generating a realistic high-quality (HQ) face image from an input low-quality (LQ) image. However, the important features of a person's face may be corrupted in the LQ image, and thus the reconstructed image may look like a different person. To tackle this problem, besides the LQ image, additional HQ images of this person can be used as reference input. Moreover, allowing multiple reference images may lead to better quality because they offer more comprehensive appearance of this person in different conditions, e.g., different poses, expressions, or lighting.

A previous work  has explored using multiple reference images for face restoration. Their method, however, depends on a face landmark model to detect facial components (i.e., eyes, nose, and mouse), which may become unreliable when the input LQ image is severely degraded. Besides, latent diffusion model (LDM)  has also been used in different image generating tasks with different input conditions, such as low-resolution images, semantic maps, or sketch images .

Inspired by the recent success of LDM, we propose **ReF-LDM** for reference-based face image restoration. Unlike previous conditional LDM methods where their input conditions are usually spatially aligned with the target image, the reference images are not aligned with the target HQ image in our case. Therefore, we design a **CacheKV** mechanism, which effectively and efficiently integrates the reference images, albeit with different poses and expressions. Furthermore, we introduce a timestep-scaled identity loss to drive the reconstructed image to look like the same person of the LQ and reference images. Lastly, we also construct a new large-scale dataset of face images with corresponding reference images, which can serve as both training and evaluation datasets for future reference-based face restoration research.

With the above components, our ReF-LDM outperforms recent state-of-the-art methods with a significant improvement in face identity similarity. Extensive ablation studies for the proposed CacheKV mechanism and timestep-scaled identity loss are also conducted and reported. The main contributions of this work can be summarized as:

* We propose ReF-LDM, which features an effective and efficient CacheKV mechanism, for restoring an LQ face image using multiple reference images.
* We introduce a timestep-scaled identity loss, which considers the characteristics of diffusion models and helps ReF-LDM better learn the discriminating features of human identities.
* We construct FFHQ-Ref, a dataset comprising 20,406 high-quality face images and their corresponding reference images, to facilitate the advance of reference-based face image restoration.

## 2 Related work

Face restoration without personal reference imagesNumerous studies have been proposed for blind face image restoration [28; 2; 5; 32; 20; 13; 26]. Recent works such as VQFR  and CodeFormer  have achieved promising results by exploiting VQGAN, while DAEFR  further employs a dual-branch encoder to mitigate the domain gap between LQ and HQ images. Inspired by the success of diffusion models, several works [23; 31; 17; 27; 25] have adopted diffusion models for face image restoration. However, as these methods do not leverage reference images, the restored images may differ from the authentic facial appearance of a person, especially when an input image is severely degraded.

Face restoration with personal reference imagesSeveral methods [14; 15; 16; 19] have attempted to utilize additional reference images to enhance personal fidelity in face restoration. GFRNet  warps a single reference image to match the face pose of the LQ image, while ASFNet  selects the reference image with the closest matching facial landmarks to serve as the network input. Closer to the setting of this work, DMDNet  also utilizes multiple reference images. It detects facial

Figure 1: **Reference-based face image restoration. Given an input low-quality face image (a), a Latent Diffusion Model (LDM) can reconstruct a high-quality image (b); however, it may not be faithful to the individualâ€™s facial identity. To address this problem, we propose ReF-LDM, which restores a high-quality image with faithful details (c) by utilizing additional reference images (d).**

landmarks on the LQ image and the reference images to extract features of facial components, and then integrates these features into the model by querying the corresponding components. However, their method relies on landmark detection, which may not be robust on severely degraded LQ images. In contrast, our ReF-LDM implicitly learns the correspondences between the features of the LQ image and the reference images, without the need for landmark detection. From a different perspective, MyStyle  adopts a per-person optimization setting, leveraging hundreds of images of an individual to define a personalized subspace within the latent space of a StyleGAN . In comparison, our approach offers greater flexibility, capable of utilizing one to several reference images without the need for personalized model optimization for each individual.

Latent diffusion models with image conditionsPrevious work demonstrates that LDM can generate an image from a low-resolution image by simple channel-axis concatenation . However, reference images in our task are not spatially aligned with the target HQ image, thus requiring a more sophisticated integration mechanism. MasaCtrl  achieves text-to-image synthesis with a single reference image by replacing the original keys and values tokens with those from the reference image. However, their solution requires passing the reference image through the denoising network for multiple timesteps, which increases computation and limits its feasibility for extending to multiple reference images. In contrast, we propose an efficient CacheKV mechanism that leverages multiple reference images by eliminating the redundant network passes.

## 3 The proposed ReF-LDM model

In this section, we present the proposed ReF-LDM model. We introduce the network architecture in Sec. 3.1, where a CacheKV mechanism is designed to leverage reference images. We illustrate how to train our model with the timestep-scaled identity loss in Sec. 3.2.

Figure 2: **The proposed ReF-LDM pipeline. Our model accepts a low-quality image and multiple high-quality reference images as input and generates a high-quality image. The blue top panel alone represents a typical LDM  denoising process. For an LQ image \(_{}\), we concatenate its latent \(_{}\) with \(_{t}\) along the channel axis to serve as the input for the denoising U-net. For the reference images \(\{_{}\}\), we design a CacheKV mechanism, depicted in the red panel, to extract and cache their key and value tokens using the same denoising U-net for just one time. These cached KV tokens can then be utilized repeatedly in each of the \(T\) timesteps of the main denoising process. During training, we adopt the classic LDM loss (\(_{}\)) and introduce a timestep-scaled identity loss (\(_{}\)).**

### Model architecture of ReF-LDM

The proposed ReF-LDM accepts an input LQ image and multiple reference images to generate a target HQ image. Its model architecture is based on the latent diffusion model , with additional designs to incorporate the input LQ image and the reference images.

#### 3.1.1 Preliminaries on Latent Diffusion Model

To generate an image, an image diffusion model  starts from a noisy image \(_{T}^{H W 3}\), initialized with a Gaussian distribution, and gradually denoises it to a clean image \(_{0}\) with a denoising network over \(T\) timesteps. A latent diffusion model  operates similarly, but the diffusion process takes place in a more compact latent space of a pre-trained and frozen autoencoder (encoder \(\) and decoder \(\)). That is, it begins with a random latent \(_{T}^{H_{z} W_{z} C_{z}}\) and progressively denoises it to a clean latent \(_{0}\). A clean image is then generated by passing the clean latent through the decoder during the inference phase, i.e., \(_{0}=(_{0})\); conversely, a ground truth clean latent is obtained by encoding a clean image with the encoder during the training phase, i.e., \(_{0}^{*}=(_{0}^{*})\). A typical choice for the denoising network is a U-net with self-attention layers at multiple scales.

#### 3.1.2 CacheKV: a mechanism for incorporating reference images

As illustrated in Fig. 2, our ReF-LDM leverages an input LQ image \(_{}\) and multiple reference images \(\{_{}\}\) to generate a target HQ image \(_{}\). For an LQ image, we simply concatenate its latent encoded by the frozen encoder, \(_{}=(_{})\), with the diffusion denoising latent \(_{t}\) along the channel axis to serve as the input of the denoising U-net. For reference images, we design a **CacheKV** mechanism. Essentially, we extract and cache the features of reference images using the same denoising U-net just once; these cached features can then be used repeatedly at each of the \(T\) timesteps in the main denoising process. Specifically, we pass the encoded latent of each reference image, \(_{}=(_{})\), through the U-net to extract their keys and values (KVs) at each self-attention layer and store them in a CacheKV. Subsequently, during the main diffusion process, within each self-attention layer of the U-net, we concatenate the reference KVs (from the corresponding self-attention layer) with the main KVs along the token axis. This mechanism enables the U-net to incorporate the additional KVs from the reference images into the main denoising process. When extracting KVs from the reference images, we use a timestep embedding of \(t=0\) and pad \(_{}\) with a zero tensor to accommodate the additional channels introduced for the LQ image.

To summarize, for inference, we first run the U-net once to extract CacheKV from the reference images; subsequently, we proceed through the main denoising process for \(T\) timesteps, during which the U-net integrates \(_{}\) and reference CacheKV. For training, in each iteration, we first run the U-net to extract CacheKV, and then we run the U-net again to estimate the target latent from a sampled noisy latent \(_{t}\), incorporating the conditions \(_{}\) and reference CacheKV.

#### 3.1.3 Comparing CacheKV with other designs

There are other intuitive designs for integrating the reference latents \(\{_{}\}\) into the diffusion denoising process. However, they are either ineffective or computationally inefficient compared to the proposed CacheKV. The quantitative evaluation and computational analysis is reported in Sec. 5.2.1. We depict these designs in Fig. 3 and provide an intuitive explanation as follows:

* **Channel-concatenation**: Concatenating the condition with \(_{t}\) along the channel axis works well for LQ images (and for other 2D conditions such as semantic maps ); however, it is not effective for reference images. A critical difference between these conditions is that--while the LQ image is spatially aligned with the target HQ image, the reference images are not. Therefore, it is challenging for the model to leverage reference images using simple channel-concatenation.
* **Cross-attention**: Cross-attention layers have been proven useful for text conditions in text-to-image models . In our ablation experiment, we insert a cross-attention layer after each self-attention layer and use the reference latents \(\{_{}\}\) to produce keys and values. While cross-attention appears to have the potential to address the spatial misalignment problem, it still fails to effectively utilize the reference images. The difference between our CacheKV and the cross-attention setting is that CacheKV provides the reference images in a more aligned feature space for the main denoising process to leverage. Specifically, the CacheKV is extracted using the same U-net and the corresponding self-attention layer as in the main denoising process. Incontrast, the cross-attention setting processes the reference images only with the frozen encoder, resulting in features that are less aligned with those in the U-net of the denoising process.
* **Spatial-concatenation**: Concatenating \(\{_{}\}\) with \(_{t}\) along the spatial dimension to serve as the input for U-net also effectively leverages the reference images. Conceptually, spatial-concatenation treats reference images in a very similar way to our CacheKV. In both mechanisms, \(\{_{}\}\) are processed through the denoising U-net, allowing the reference KVs to be accessed by the queries (Qs) of the main diffusion latent \(_{t}\). However, spatial-concatenation requires significantly more computational resources compared to our CacheKV. It passes \(\{_{}\}\) with \(_{t}\) to the U-net at each of the \(T\) denoising timesteps, whereas CacheKV only passes \(\{_{}\}\) through the U-net once. Moreover, spatial-concatenation also requires significantly more GPU memory, as the spatial size of the input for the U-net increases with the number of reference images. As for a self-attention layer in the U-net, both mechanisms increase memory usage; CacheKV introduces additional reference KVs, while spatial-concatenation introduces reference QKVs.

### Timestep-scaled identity loss

#### 3.2.1 Timestep-scaled identity loss

As this work aims for face image restoration, we employ the identity loss to enhance face similarity, which is adopted in many face-related tasks [9; 21; 28]. The identity loss minimizes the distance within the embedding space of a face recognition model, thereby capturing the discriminating features of human faces more effectively than the plain RGB pixel space. In our experiments, we use the ArcFace model  with cosine distance between the 1D embedding vectors as the identity loss.

However, naively adding identity loss to the training of Ref-LDM significantly worsens the image quality. One possible explanation might be that, the one-step model prediction \(_{0|t}=(_{0}|_{t})\) at a very noisy timestep (e.g., \(t=T\)) is very different from a natural face image and thus out of the distribution that the ArcFace model is trained on; therefore, the identity loss provides ineffective supervision for diffusion models at large timesteps.

Figure 3: Different mechanisms for incorporating reference images into the main denoising process.

Based on this assumption, we propose a timestep-scaled identity loss, where a timestep-dependent scaling factor is introduced to scale down the identity loss when a larger timestep is sampled in a training step. Specifically, the timestep-scaled identity loss is defined as:

\[_{}=}}_{}=}}1-) R(^{ })}{\|R()\|\|R(^{})\|},\] (1)

where \(R\) is a face recognition model, and \(}}\) follows the definition in a typical diffusion process [8; 22] in which a noisy latent \(_{t}\) is sampled given a clean latent \(_{0}^{}\) as:

\[q(_{t}|_{0}^{})=(}} _{0}^{},(1-}))\] (2)

#### 3.2.2 Training ReF-LDM with timestep-scaled identity loss

We train our ReF-LDM with the classic LDM loss and the proposed timestep-scaled identity loss:

\[_{total}=_{}+_{} \,_{}\] (3)

Recall that the denoising U-net estimates the target latent in the latent space of the frozen autoencoder, and a typical \(_{}\) is computed as the L1 distance between the estimated latent and the target latent. To compute the identity loss with the face recognition model, which accepts an image as input, we decode the estimated latent into the image space using the frozen decoder, i.e, \(_{0}=(_{0})\). The experiments in Sec. 5.2.2 show that timestep-scaled identity loss can improve face similarity without degrading image quality, unlike the naive usage of identity loss.

## 4 FFHQ-Ref dataset

Recent works for non-reference-based face restoration commonly train their models with FFHQ dataset , which comprises 70,000 high-quality face images of wide appearance variety with appropriate licenses crawled from Flickr. These images are not provided with reference labels originally; however, we find that a good portion of the images are of the same identities. Thus, we construct a reference-based dataset--FFHQ-Ref--based on the FFHQ dataset, with careful consideration described as follows.

### Finding reference images of the same identity

To determine whether two images belong to the same identity, we utilize the face recognition model ArcFace . Specifically, we first extract the 1D ArcFace embeddings for all images. Then, for each image, we compute the cosine distances between its embedding and the embeddings of all other images. A distance less than a threshold \(r=0.4\) indicates that the images are valid references belonging to the same person. Following this procedure, we identify 20,405 images with corresponding reference images.

### Splitting data according to identity

To enable the FFHQ-Ref dataset to serve as both training and evaluation datasets for reference-based face restoration models, we divide the images into train, validation, and test splits. However, random data splitting may result in the train and test splits containing images of the same individual, which is not ideal for a fair evaluation. To ensure that all images of a single identity are assigned to only one data split, we group the images based on their identities. Specifically, we consider identity grouping as a graph problem, where each image acts as a vertex and any pair of images with a distance less than \(r\) are connected by edges. We then apply the connected component algorithm from graph theory, where each connected component represents a group of images belonging to the same person. Finally, we identified 6,523 identities and divided them into three splits: a train split with 18,816 images of 6,073 identities, a validation split with 732 images of 300 identities, and a test split with 857 images of 150 identities. We report more statistics in Appendix C.

### Constructing evaluation dataset with practical considerations

Practical ConsiderationsFor a fair and meaningful evaluation, the input reference images should not be excessively similar to the target image; hence, we set a minimum cosine distance threshold of 0.1 for the test set. Additionally, we manually check the images in the test split to verify that all reference images indeed correspond to the same identity. Furthermore, in the context of reference-based face restoration applications, it is preferable to select input reference images that capture a more comprehensive representation of a person's appearance, such as varying face poses or expressions. Although a target image in the test split of our FFHQ-Ref may have two to nine reference images, different reference-based methods may have their own constraints on the maximum number of input reference images. To emulate a more representative set of reference images, we sort all available reference images of a target image using farthest point sampling on the ArcFace distance.

Degradation synthesis for input LQ imagesFor synthesizing input LQ images from ground truth HQ images, we follow the degradation model used in previous works [28; 5; 32]:

\[_{}=\{[(_{}*k_{})_{r}+n_ {}]_{_{q}}\}_{r},\] (4)

where an HQ image is blurred with a Gaussian kernel \(k_{}\), downsampled by \(r\) scale, added with a Gaussian noise \(n_{}\), compressed with JPEG quality level \(q\), and upscaled to the original size.

We construct two evaluation datasets with different degradation levels:

* FFHQ-Ref-Moderate: \(\), \(r\), \(\), and \(q\) are sampled from \(\), \(\), \(\), and \(\).
* FFHQ-Ref-Severe: \(\), \(r\), \(\), and \(q\) are sampled from \(\), \(\), \(\), and \(\).

### Comparison between FFHQ-Ref and existing datasets

Table 1 summarizes the differences between our proposed FFHQ-Ref and existing datasets. While the CelebRef-HQ dataset  has been constructed to train and evaluate reference-based face restoration models, our FFHQ-Ref dataset contains twice as many images and six times the number of identities compared to CelebRef-HQ. Moreover, built upon FFHQ , FFHQ-Ref provides superior image quality over CelebRef-HQ, as indicated by the lower NIQE score (3.68 vs. 3.97). Some ground-truth images in CelebRef-HQ are affected by watermarks and mirror padding artifacts, as shown in Appendix B.

## 5 Experiments

In this section, we describe the experimental setup in Sec. 5.1, discuss ablation studies in Sec. 5.2, and provide the comparison between our ReF-LDM and the state-of-the-art methods in Sec. 5.3

### Experimental setup

#### 5.1.1 Implementation details

To exploit more ground truth images without available reference images, we use 68,411 images in the FFHQ dataset to train a VQGAN  as the frozen autoencoder and an LDM with only LQ condition. We then finetune our ReF-LDM from the LQ-conditioned LDM with the 18,816 images in our FFHQ-Ref dataset. All models are trained excluding the test split images to ensure fair evaluation on our FFHQ-Ref benchmark. In our experiments, we adopt a 512x512 image resolution, fix the number of reference images to five, and set loss scale \(_{}\) to 0.1. During training, we synthesize input LQ images with \(\), \(r\), \(\), and \(q\) sampled from \(\), \(\), \(\), and \(\), respectively. For inference, we use 100 DDIM  steps and a classifier-free-guidance  with a scale of 1.5 towards reference images. We provides more implementation details in the Appendix G.

   Dataset & With reference & Licensed & Quality & Images & Identities \\  FFHQ  & & âœ“ & âœ“ & 70,000 & - \\  CelebRef-HQ  & âœ“ & & & 10,555 & 1,005 \\ FFHQ-Ref & âœ“ & âœ“ & âœ“ & **20,405** & **6,523** \\   

Table 1: Comparison between the proposed FFHQ-Ref and existing datasets.

#### 5.1.2 Evaluation datasets and metrics

For evaluation datasets, we use the test split of our FFHQ-Ref with two different degradation levels: severe and moderate. In addition, previous non-reference-based methods commonly use CelebA-Test  for evaluation, which comprises 3,000 LQ and HQ image pairs sampled from the CelebA-HQ dataset . Therefore, we follow the same procedures described in Sec. 4 to construct a subset of 2,533 images with available reference images, termed CelebA-Test-Ref.

For evaluation metrics, we adopt the identity similarity (IDS) [5; 32], which is the cosine similarity calculated using the face recognition model ArcFace . We also use the widely used perceptual metrics LPIPS . As face pixels are more of concern in the task of face restoration, we also measure the face-region LPIPS (fLPIPS), which is the LPIPS calculated using only the pixels in face regions. For assessing no-reference image quality, we adopt NIQE . Furthermore, we measure the FID , using 70,000 images from the FFHQ dataset as the target distribution.

### Ablation studies

We provide the ablation studies of the proposed CacheKV, timestep-scaled identity loss, and the number of input reference images. In each ablation experiment, we fine-tune the model for 50,000 steps from the same LDM pre-trained without reference images. We compare the difference settings with the FFHQ-Ref-Severe dataset.

#### 5.2.1 CacheKV and other mechanisms

The CacheKV is proposed for integrating the input reference images into the diffusion denoising process. We compare it with other mechanisms illustrated in Sec. 3.1.3. According to Table 2, channel-concatenation and cross-attention fail to leverage reference images to improve the identity similarity (IDS). In contrast, both spatial-concatenation and our CacheKV significantly enhance IDS. Moreover, our CacheKV is more computationally efficient than spatial-concatenation, requiring only 20% of the inference time and 39% of the GPU memory.

   Mechanism & IDS\(\) & NIQE\(\) & LPIPS\(\) & Inference time\(\) & Memory\(\) \\  Channel-concatenation & 0.23 & 4.49 & 0.46 & 4.17 & 1.77 \\ Cross-attention & 0.23 & 4.56 & 0.46 & 14.54 & 2.80 \\ Spatial-concatenation & 0.69 & 4.84 & 0.43 & 58.36 & 7.44 \\ CacheKV & 0.65 & 4.38 & 0.43 & 12.15 & 2.87 \\   

Table 2: Comparison between CacheKV and other mechanisms for input reference images (run with five reference images on a single GTX 1080).

   Loss & IDS\(\) & NIQE\(\) \\  \(_{}\) & 0.52 & 4.56 \\ \(_{}+_{}\) & 0.69 & 6.56 \\ \(_{}+_{}\) & 0.65 & 4.38 \\   

Table 4: Design choices for ID loss scaling.

Figure 4: Visual ablation results for the timestep-scaled identity loss.

[MISSING_PAGE_FAIL:9]

DMDNet , fails to restore the severely degraded images because it depends on unreliable facial landmark detection, reflected by higher fLPIPS. In contrast, our ReF-LDM consistently outperforms DMDNet on identity similarity and other metrics, owing to the proposed CacheKV mechanism and timestep-scaled identity loss, which effectively leverage the input reference images without the need for landmark detection. We also note that our method exhibits slightly inferior results in LPIPS metric. This is due to the difference in the background pixels, we provide further details in the Appendix D. It is also worth mentioning that the competing methods benefit from data leakage on the FFHQ-Ref benchmarks, as their models are trained with the entire FFHQ dataset or with a different train split than the identity-based one in the proposed FFHQ-Ref.

#### 5.3.2 Qualitative comparison

In Fig. 5, we present a qualitative comparison between our ReF-LDM, the pre-trained LDM without reference images, CodeFomer (a SOTA non-reference-based method), and DMDNet (a SOTA reference-based method). Given the severely degraded image, DMDNet generates distorted face images based on incorrectly detected landmarks. While CodeFormer yields realistic face images, it does not preserve the facial identity well. In contrast, our ReF-LDM produces results that are both realistic and faithful to the individual's facial identity.

## 6 Limitations

When the face region is occluded by other objects, our model may generate artifacts. For certain face poses (e.g, side face), the reconstructed eyes may appear unnatural. These problems are also commonly observed in other methods and might be caused due to the lack of such training images. However, there are some examples showing that these problems can be alleviated if our model is provided reference images with similar face poses to the target image. Visual examples of these limitations are provided in Appendix F.

## 7 Conclusion

In summary, we propose ReF-LDM, which incorporates the CacheKV mechanism and the timestep-scaled identity loss, to effectively utilize multiple reference images for face restoration. Additionally, we construct the FFHQ-Ref dataset, which surpasses the existing dataset in both quantity and quality, to facilitate the research in reference-based face restoration. Evaluation results demonstrate that ReF-LDM achieves superior performance in face identity similarity over state-of-the-art methods.

Figure 5: Qualitative comparison. From left to right: input LQ, ground truth, other methods, and our ReF-LDM. From top to bottom: FFHQ-Ref-Severe, FFHQ-Ref-Moderate, and CelebA-Test-Ref.

#### Acknowledgments

The authors wish to express their gratitude to Professor Wei-Chen Chiu for his valuable suggestion to exclude the reference images that are too similar to the target images when constructing the proposed FFHQ-Ref dataset.