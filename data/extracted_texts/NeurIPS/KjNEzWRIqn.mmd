# Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale

Tianyue Ou Frank F. Xu Aman Madaan Jirui Liu Robert Lo Abishek Sridhar Sudipta Sengupta Dan Roth Graham Neubig Shuyan Zhou Carnegie Mellon University Amazon AWS AI  xAI

{tianyueo, fangzhex, gneubig, shuyanzh}@cs.cmu.edu

###### Abstract

LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (_e.g.,_ arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, _direct_ demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may _indirectly_ assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use 100\(k\) such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only 3% the cost of human demonstrations (at $0.031 each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.1

## 1 Introduction

AI agents that operate within a digital environment (_e.g.,_ a browser in a computer) intelligently to accomplish complex tasks (_e.g.,_ _"Create my July online shopping expense report."_) have the potential to improve the productivity across a broad swath of tasks performed by humans every day . However, agents still lack the ability to complete tasks with a high degree of reliability, partly due to a paucity of training data for such agentic tasks. Typically, supervised finetuning is a standard way to adapt large language models (LLMs) to tasks such as text generation or classification, large-scale demonstration collections for digital agents are not readily available.

For AI agents, demonstrations typically involve specifying a sequence of actions and observations that results in successful task completion, as shown on the right of Figure 1. Existing works that automatically collect demonstrations (1) set up environments for an agent to interact with, (2) run a baseline agent within this environment, and (3) employ scoring functions to remove low quality demonstrations  or perform relabeling . All three of these requirements limit applicability to a variety of practical applications. Setting up an environment that is representative of the actualenvironments in which we would like agents to act is a difficult task, and existing environments for digital agents are generally limited in scope to a few websites or digital apps [58; 7; 45]. Even within these constrained settings, strong LLMs such as GPT-4 struggle on tackling tasks , making collecting successful demonstrations with LLMs inefficient. In addition, human annotation is costly and its scope can still be limited [17; 6]. For example, gathering a demonstration for canceling a PayPal order requires an actual PayPal account with a legitimate subscription history.

In this work, we propose Synatra, a data generation approach that synthesizes high-quality trajectories for complex digital tasks at scale. This approach is based on the intuition that there is a rich trove of existing knowledge that encodes _indirect_ supervision about how to perform digital tasks (SS2.2). One example of a piece of indirect knowledge is a tutorial that details the sequential breakdown of a complex web task, such as "how to cancel a recurring payment on Paypal" for human readers (Figure 1 upper left) [53; 57]. While this tutorial provides some procedural guidance, such as "Enter the keyword," it does not specify the executable actions or the tangible observations associated with them. The key idea of Synatra is that, given this indirect knowledge, we can leverage an LLM to _re-purpose_ it into more usable form that directly demonstrates the exact action to take given a concrete observation (SS3). In doing so, we leverage LLMs' ability to paraphrase and code, as well as its general knowledge of how tasks are performed (Figure 1). The main benefit of this approach is that it can scale with the availability of indirect knowledge created for humans, rather than rely on direct human annotations or LLM trajectories.

We carefully study the sources of indirect knowledge, the design of demonstration formats, and the mechanisms for iterative refinement in order to synthesize high-quality demonstrations using an LLM (SS3, SS4). We generate demonstrations of \(100k\) tasks from 21 domains, and finetune a 7b Codellama-instruct model with this synthetic data. The resulting agent, Synatra-CodeLlama, surpasses existing open-source models of similar size, excluding those finetuned with human annotations, on three web-based task benchmarks: Mind2Web, MiniWoB++, and WebArena. Moreover, it consistently outperforms models that are ten times larger and have been finetuned with interactive data (SS6.1). Our findings also indicate that our model is on-par or a more accurate option in browser copilot settings, in comparison to GPT-3.5. Importantly, while each synthetic example incurs only approximately 3% of the cost of a human-annotated demonstration, we demonstrate that synthetic data with good domain coverage can be _more_ effective than an identical quantity of limited-domain human demonstrations in real-world web-based tasks in WebArena (SS6.2).

## 2 Problem Formulation

### Controlling Digital Agents through Natural Language

An agent interacts with a computer environment \(=,,,\) with state space \(\), action space \(\), observation space \(\) and the environment dynamics \(:\). While this framework can be applied to different types of task, in this work, we consider a _web browser_ as the unified entry point to access different web applications. We follow WebArena  to define the observation space as the contents on screen, represented as in text-based accessibility tree format. We use the same universal action space as in WebArena. This action space resembles the keyboard and mouse operations of a

Figure 1: Our approach aims to synthesize direct demonstrations (right of the arrow) that specify the immediate next actions based on previous actions and current observations. The sources comprise only indirect knowledge (left of the arrow), such as tutorials designed for human consumption and randomly sampled observations that lack associated tasks and actions.

computer (_e.g.,_ click, type), and is applicable to _arbitrary_ web applications. Appendix A lists all valid actions. The environment dynamics (_e.g.,_ the effect of clicking a button) and the states (_e.g.,_ the database status) are decided by the implementation of each web application.

Given the natural language intent \(\), at each time step \(t\), an agent issues an action \(a_{t}\) based on \(s_{t}\). The environment state is updated to \(s_{t+1}\) with new observation \(o_{t+1}\). This process ends when the agent predicts the stop action. We follow existing works [6; 47; 58; 54] to represent \(s_{t}\) with current observation and all previous actions in the form of \((,a_{1},...,a_{t-1},o_{t})\). A benchmark (_e.g.,_ WebArena) supplies a scoring function \(r(,s_{n})\) that examines the final state and returns \(1\) if the desired goal state is satisfied, and \(0\) otherwise.

### Definition of Direct Demonstrations and Indirect Knowledge

We consider the expected action \(a_{t}\) given \(s_{t}\) as a form of direct demonstration, _i.e.,_\((s_{t},a_{t})\). This allows an agent to directly learn how to predict the next action under a given state. On the other hand, indirect knowledge is broadly defined as resources that can benefit the task execution, but is not in the format of state and expected action tuple. We mainly focus on three types of indirect knowledge:

1. **Procedural knowledge** details the sequence of steps \( a^{}_{1},a^{}_{2},....,a^{}_{n}\) required to complete a specific task \(\). Unlike an action \(a_{t}\) in trajectories, the steps in procedural knowledge are ungrounded, they lack a direct association with any particular observation and are not tied to specific action spaces. For instance, the tutorial in Figure 1 instructs the user to _"login with your credentials"_ without providing the concrete PayPal login page and the input fields to type.
2. **Environment knowledge**\(\) that describe the effects of applying different actions in some hypothetical states. Example knowledge includes verbal descriptions such as _"... after clicking the cancel button, you will see a pop up window..."_.
3. **Ungrounded observations**\(o\) that are not associated with particular tasks or trajectories. In the context of web-based tasks, an observation could be any random web page with different content and status (_e.g.,_ a product page with a query in the search field).

## 3 Scalable Demonstration Synthesis for Digital Agents

In this section, we first introduce our design choices on the canonical formalization of trajectories, which account for the _structural_ nature of procedures. Then, we delve into the sources for acquiring indirect knowledge, and the mechanisms for re-purposing this knowledge into direct supervision.

### Trajectories as Programs

Existing works demonstrate that representing task-related procedures as programs is beneficial for several reasons. This includes benefits from the structural nature of programs compared to free-form text [56; 23], and the flexibility of using tools [4; 10; 41]. Inspired by these observations, we represent a Python function that interleaves natural language planning articulated in comments and actions as API calls, as shown on the right. The pink background represents the prompt, while the blue background corresponds to the model's response format.

The planning process includes both task-level planning, which breaks the task into multiple sub-tasks, and action-level planning, which explains the low-level goals of each executable action. The model's generation consists of chain-of-thought (CoT, [43; 47]) reasoning that analyzes the objective, previous actions, and current observations. Since CoT reasoning includes detailed information about the current step that may not be relevant for future steps, we further design the model response to include an action summary. This summary serves as a description of the predicted action that is added to the prompt for future action predictions. A concrete example can be found in Appendix B.2 We study the empirical effect of program formalization in SS6.3.

### Synthesizing from Text Procedural Knowledge with Generative Environment

The Internet offers fairly extensive procedural knowledge that describes _how_ to perform high-level tasks by breaking down the task into detailed lower-level steps, such as how-tos and tutorials.

SourceWe use wikiHow3 as our main source for these tutorials due to its comprehensive coverage of diverse tasks as well as its consistent format. Each article consists of a high-level task description and step-by-step instructions. We performed a filtering step and only kept the articles that involve navigation through the graphical user interface (GUI) of a computer or a mobile phone. We prompted GPT-3.5-turbo with six examples mixing the target articles (_e.g.,_ How to redeem an Amazon gift card online4) and non-target articles (_e.g.,_ How to make a pizza) to perform the classification of all wikiHow articles. The prompt is shown in Appendix C. As a result, we obtained 25\(k\) articles that can be used to perform data synthesis.

Synthesis ApproachWe want to bridge two gaps to re-purpose \( a^{}_{1},a^{}_{2},...,a^{}_{n}\) for task \(i\) into \( a_{1},...,a_{t-1},o_{t}\). When re-purposing a sequence of actions \( a^{}_{1},a^{}_{2},...,a^{}_{n}\) for task \(i\) into a new sequence \( a_{1},...,at-1,o_{t}\), many challenges arise. First, the action descriptions provided in tutorials are not constrained to specific action spaces. Instead, they are presented as free-form natural language (NL) expressions, which can lead to ambiguity. For instance, various verbs such as "enter," "input," and others may all correspond to the same underlying action, type. Second, NL descriptions are often abstract, omitting concrete actions. For example, the process of "logging in" involves a series of actions, including typing in a username and password, but these specific actions may not be explicitly mentioned. Finally, the steps outlined in tutorials are ungrounded, meaning they are not directly associated with observable states or outcomes. Tutorials typically employ generic descriptions to accommodate various instances of conceptually similar tasks. For example, as illustrated in Figure 1, the tutorial merely instructs to "enter the keyword" without addressing any specific scenario.

Based on these findings, we propose an _iterative_ approach that first uses an LLM to rewrite an article into a hypothetical trajectory in the format shown in SS3.1, then we leverage a generative model to synthesize the intermediate observation between two consecutive actions. First, in the rewriting step, we ask the assistant LM to perform: (1) propose a hypothetical concrete scenario relevant to the task (2) perform basic parsing such as translating "enter the keyword [...]" into type ("search bar", "Amazon Prime"); (3) categorize actions into groups that reflect the sub-task structures outlined by coding blocks. These tasks mainly demand a LLMs's creativity, language processing ability, and event understanding respectively. An example of rewriting a how-to article into a trajectory in program format is showed in Appendix E. The prompt for the rewriting step is in Appendix D.

Because the previous procedures still only result in a sequence of ungrounded actions, we next leverage the assistant LM to generate the observations between randomly sampled consecutive actions. We use the consecutive actions of type("search bar", "Amazon Prime") and click("Amazon Inc", id=156) in Figure 1 as an example. There are mainly two requirements for generated observations. First, the observation must reflect the _outcomes_ of past actions. In the example, this corresponds to a page with a user logged in, and a search input field filled with "Amazon Prime". Second, the observation _encodes_ the necessary elements to perform the next action. In the example, this corresponds to a payment history list with a payment to Amazon. We prompt the assistant LM with the action sequence to generate a HTML snippet that fulfills the above requirements. Since the next action requires the concrete element to interact with, we ask the model to insert a tag of id="next-action-target-element" in the corresponding HTML node to indicate the grounding.5 This step mainly requires a model's coding capabilities, particularly in web development. However, we find that it is not necessary for the LLM to generate HTML with high fidelity and complexity, which is a open research question . The full prompt is in Appendix D and an example for this step is in Figure 5.

Note that in addition to wikiHow, there are other resources that share similar a similar procedural flavor, such as the captions of YouTube how-to videos . Our transformation mechanism is generally applicable to such resources, but we leave concrete examination of them as future work.

### Synthesizing from Random Observations

While the procedure in the previous section results in _real procedures_, the LLM-based method generates _simplified observations_. To compensate for this, we also perform data synthesis with _real observations_, and use synthesis to generate _simplified procedures_. We show that these two sources can compensate each other by examining the generated data in SS4 and comparing the actual web-based task performance in SS6.3.

SourceWe utilize ClueWeb  as our data source, which comprises HTML snapshots of more than 10 billion web pages. Our initial analysis indicates that a random sampling approach would likely lead to a homogeneous distribution dominated by less interactive pages, such as news articles. In contrast, more complex web-based tasks typically require interactions with various web elements to advance the tasks. To diversify the sampled web pages, we employed a temperature sampling approach to select pages based on their content categories. In general, web pages from higher frequency top-level domains in ClueWeb are typically more interactive, such as Amazon and Reddit, while domains with lower frequency are less interactive, such as news article. We use a temperature sampling with \(T=0.6\) so that the sample probability of choosing a page in domain \(i\), \(P^{}_{i}=p_{i}^{}/_{k}p_{k}^{}\), where \(p_{i}\) is the original probability of choosing a page in domain \(i\), and \(k\) is all the available domains. In doing so, we up-sample more interactive sites while maintaining diversity on more rare sites. More details are listed in Appendix J.

Synthesis ApproachWe treat each sampled web page as an intermediate observation at time step \(t\), aiming to synthesize the specific task \(\), the previous actions \(a_{1},...,a_{t-1}\) and the subsequent action \(a_{t}\) consisting of an action, a corresponding target element in the observation, and a natural language summary of the action. We first convert a web page into its corresponding accessibility tree at the beginning of each node, and sample a segment to present to the assistant LM. We follow the WebArea convention of assigning a unique ID to each node in the tree to ease the challenge of referencing the nodes. To increase the diversity of the tasks, we first instruct the model to brainstorm \(k\) task categories relevant to the web domain. Then the model randomly selects three of these categories and develops them into concrete scenarios with past actions leading up to the current observation and the next action to take. The prompt is in Appendix F and an example generation is in Appendix G.

### Data Filtering

To ensure the quality of the training set, we apply a two-part filtering pipeline. In the first part, we ensure that data samples are both complete and coherent. For a sample to pass this filter, it must include all required components in the correct format. These components include: (1) an action that falls within the defined action space, (2) a valid and meaningful action target element in the corresponding web page, (3) NL texts that are well formed, e.g. without the use of "..." in the texts as an abbreviation by the generative model, (4) overall comprehensive generation without placeholders from the prompt (_e.g., <brief step description>_).

To further eliminate accurately formatted but unresponsive actions, we apply a second filtering step using next state prediction. Here, we use the LLM to predict the next state, \(o_{t+1}\), based on the current state \(o_{t}\) and action \(a_{t}\) in our synthesized data. If the model predicts that \(o_{t+1}=o_{t}\), the action is deemed to have no impact, and we filter out the action accordingly.

## 4 Data Statistics

We assess the distribution of history lengths, task objectives, and observations in the form of accessibility trees. The first statistic reflects general task complexity, as longer trajectories typically indicate As shown on the right, the majority of our synthetic data consists of trajectories with a history length of fewer than eight steps, regardless of the source. Fewer trajectories have longer histories, with the longest exceeding 20 steps. Examples with longer histories represent more complex scenarios such as _"Create a new YouTube ad campaign for the Summer Collection with a focus on the 25-34 age group interested in sports, fitness, fashion, and style"_. In addition, we analyze the distribution of generated intents by projecting their high-dimensional vectors using the embedding model all-mpnet-base-v2. We visualize these embeddings with t-SNE . As shown on the left of Figure 2, intents synthesized from random observations exhibit broader coverage compared to those from tutorials. This may be because humans often prefer to write tutorials for critical domains, while randomly sampled observations offer a more objective reflection of the diverse internet. Although our data synthesis process is entirely independent of Mind2Web, the generated intents show substantial coverage of Mind2Web tasks, which were created by human annotators from top-use websites. Finally, we apply the same embedding and visualization approach for accessibility trees and display the results on the right of Figure 2. The generated observations from tutorials overlap significantly with real web pages, both from random observations and those in Mind2Web.

CostThe average end-to-end cost per sample is $0.031, out of which $0.028 is used to generate the actual sample, and $0.003 is used by our prediction-based filtering pipeline. These values represent the final average cost to generate one valid sample, including the additional cost for samples that were generated but later filtered out, which we distributes across all non-filtered samples.

## 5 Experimental Setup

Agent trainingWe finetune CodeLlama-7b with \(99,920\) web-navigation instruction examples, sourced from WikiHow articles and ClueWeb web pages 3. Training details can be found in Appendix K.

Evaluation tasksWe select three evaluation tasks in the domain of web navigation. (1) the Mind2Web test set . It consists of three categories, namely, cross-task, cross-website, and cross-domain--ranked by their degree of deviation from the training data. Since our model is not trained

Figure 2: Left: t-SNE of task intent embedding. Right: t-SNE plots of accessibility tree embeddings more complex tasks. The latter two measure the domain diversity of our synthetic data. We also present these statistics for the Mind2Web dataset as an example of human-collected trajectories.

on the Mind2Web training set, all categories are treated as held-out sets. Therefore, we report an overall step accuracy as the average across all examples. In addition, we simplify the two-stage setup used in Mind2Web, where the model first selects the top 50 candidate elements from the HTML before predicting the action on a selected element. Instead, we input in-viewport accessibility trees that include the ground-truth element. This approach is more challenging because it introduces more irrelevant elements, but it streamlines the prediction process by eliminating the need for an additional element selection model. (2) MiniWoB++ . It is an interactive benchmark with simplified web pages and low-level tasks (_e.g.,_ "Enter bul to the box"). On average, completing a task in MiniWoB++ requires fewer steps than that of WebArena. We tested on a text-only subset of tasks used in . To get the final score, we take the average score of five runs. (3) WebArena. In WebArena, agents are required to navigate in real-world web applications to accomplish high-levels web-based tasks (_e.g.,_ "How much I spent last month on grocery"). Both MiniWoB++ and WebArena offer outcome-based evaluations, using programmatic checkers to validate key features after execution and assess whether the task has been successfully completed.

BaselinesWe compare the performance of our model with 12 baseline models, including API-based models such as GPT-4, open-source instructed models like CodeLlama, and models finetuned with interactive data. For instance, Agent-LM  is finetuned on supervised data for tasks such as web navigation and interactive coding. Since most baseline models are not optimized for code generation, we follow Zhou et al.  and prompt the models to generate natural language responses instead of programs. The same prompt, providing general web navigation instructions without dataset-specific explanations, is used across three datasets. We show the prompt in Appendix H. Notably, we adopt the original prompts of AgentLM and CodeActAgent when evaluating them on MiniWoB++ to be consistent with their original evaluations. We further remove the task-specific in-context examples to ensure a fair comparison with other models in zero-shot settings.

## 6 Results

### Main Results

Table 1 presents the performance of various models across three web-based task benchmarks. Overall, Synatra-CodeLlama achieves the best performance among models of comparable size. Notably,

  
**Model** & **Mind2Web** & **MiniWoB++** & **WebArena** \\   & Single step & Short & Long \\  & Reference-based & Execution-based & Execution-based \\   \\  GPT-3.5 & 12.79 & 39.57 & 6.16 \\ GPT-4 & 29.09 & 53.04 & 14.41 \\   \\  CodeLlama-instruct-7b & 6.62 & 23.04 & 0.00 \\ Llama3-chat-8b & 11.50 & 31.74 & 3.32 \\ Llama3-chat-70b & 22.27 & 48.70 & 7.02 \\   \\  FireAct-7b  & - & - & 0.25\({}^{*}\) \\ AgentLM-7b  & 2.99 & 15.65 & 0.86 \\ CodeActAgent-7b  & 3.13 & 9.78 & 2.34 \\ AutoWebGLM-7b   & - & - & 2.50\({}^{*}\) \\ AgentPlan-7b  & 3.80 & 20.87 & 0.62 \\ Lemur-chat-70b  & 14.28 & 21.30 & 2.95 \\ AgentLM-70b  & 10.61 & 36.52 & 3.07 \\  Synatra-CodeLlama-7b & 15.85 & 38.20 & 6.28 \\   

Table 1: Performance of various models in three web-based task benchmarks. We measure step accuracy (%) for Mind2Web, and task success rate (%) for MiniWoB++ and WebArena. The numbers of FireAct-7b is taken from ; AutoWebGLM-7b(S1) represents the model trained with only synthetic data in . All other numbers are produced by our work.

Synatra-CodeLlama significantly outperforms its base model CodeLlama-instruct-7b. While CodeLlama-instruct-7b fails to complete any tasks in WebArena, Synatra-CodeLlama successfully executes 6.28% of them. Furthermore, Synatra-CodeLlama elevates the performance of CodeLlama-instruct-7b from 6.62% to 15.85% in Mind2Web (a 139.42% relative improvement) and from 23.04% to 38.20% in MiniWoB++. More encouragingly, Synatra-CodeLlama demonstrates superior performance on Mind2Web and WebArena compared to GPT-3.5. It also outperforms Lemur-chat-70b, which is finetuned with interactive data and is ten times larger, across all three benchmarks. The results suggest that our data synthesis approach is effective in helping the model predict the next action (as in Mind2Web) and performing simple tasks with a few steps (as in MiniWoB++). The synthesized data can also guide the model towards executing real-world complex tasks more accurately.

Synatra-CodeLlama surpassed the performance of all open-source model finetuned with interactive data. Among these models, AgentLM, CodeActAgent and AgentFlan include demonstrations to perform web-based tasks in their instruction finetuning dataset. However, we find that these models may not serve as capable agents to perform web-based tasks due to the special design choice encoded in the finetuned models. For instance, AgentLM and CodeActAgent use Regex expression to match interactive element on a web page and require carefully selected in-context examples to showcase which are the proper Regex expression for different examples. However, Regex expressions only work for simple web pages with a few elements as in MiniWoB++, while it is prohibitive to do pattern matching in complex web pages as in Mind2Web and WebArena. As a result, when we experiment with the more generic action space which is suitable for all three benchmarks without in-context examples, we see these models have a significant performance degradation. On the other hand, Synatra-CodeLlama targets at the generic web-based tasks and does not encode any dataset artifacts during training. Even though all three benchmarks are completely held-out during data generation, Synatra-CodeLlama achieves consistent superior performance on all benchmarks.

### Analysis

What does the model learn from the synthetic data?Our analysis suggests that the baseline acquires essential knowledge at multiple levels from synthetic data. The error rates for different error types are shown in Table 2. At the low level, synthetic data helps the model _emit valid actions_. For example, while approximately 95% of actions predicted by the base model CodeLlama-instruct-7b involve interacting with non-existent elements on the web page, Synatra reduces this error to just 0.7%. This low error rate is closer to that of larger models such as Llama3-chat-70b and GPT-4, which are better at following complex instructions with multiple requirements. Additionally, the synthetic data improves the model's ability to _understand web pages_ more accurately. To assess this, we measure the ratio of invalid actions, including both invalid click and type, where the predicted action attempts to interact with an element that is not clickable (_e.g.,_ a piece of text) or not typable (_e.g.,_ a button). We observe that models in the 7b-8b range have high error rates in interpreting basic web elements. For instance, the strong 8b model Llama3-chat-8b mistakenly click a non-clickable element over 15% of the time, while Synatra reduces this error to under 6% approaching the performance of GPT-4. Finally, at task completion level, Synatra shows a stronger ability to track progress accurately. Specifically, when filling out forms on web pages, Synatra-CodeLlama is 74% less likely than GPT-4-turbo to repeatedly input the same words into the same field, a common error in models. This indicates that our synthetic data enables the model to more precisely recognize completed actions and identify the remaining steps needed to achieve the goal. More qualitative study can be found in Appendix I.

Can Synatra be as effective as human annotations?We compare models trained with 9\(k\) human demonstrations (human only) from Mind2Web  and 9\(k\) demonstrations generated by Synatra. The

  
**Error Type** & GPT-4-turbo & Llama3-chat-70b & Llama3-chat-8b & CodeLlama-instruct-7b & Ours \\  Nonexistent element & 0.56 & **0.37** & 2.40 & 94.70 & 0.70 \\ Invalid actions & **1.75** & 2.17 & 9.74 & 58.88 & 4.91 \\  Click non-clickable & 5.34 & **3.40** & 15.20 & 100.00 & 5.77 \\ Type non-typable & **1.01** & 1.34 & 11.97 & 32.62 & 4.34 \\  Repeated type & 26.85 & 44.72 & 32.14 & 46.32 & **6.79** \\   

Table 2: Error rates (%) on five fine-grained metrics.

results are shown in Figure 3. Simply training CodeLlama with the Mind2Web human annotations provides modest improvement of 3.96% on MiniWoB++, while failing entirely on WebArena.

In contrast, Synatra led to a substantial improvement of 17.81% on MiniWoB++ and 4.56% on WebArena. The limited performance of the human annotations can be partially attributed to their restricted task coverage: notably, Mind2Web lacks information seeking tasks that require a string answer. Furthermore, the human trajectories did not specify conditions for terminating task execution. Despite adding such trajectories from Synatra (human + synthesis), the model still under-performed.

We hypothesize the diversity of tasks plays a role in this discrepancy, since many tasks cannot be covered without pre-set environment (_e.g.,_ return an order) (Figure 2). These findings underscore the efficacy of Synatra, which also exempts from the complexities of developing recording tools and managing communication with annotators. However, it is important to recognize that the quality of human demonstrations is presumably higher, but they require meticulous design and control during the data collection process.

### Ablations

In this section, we perform an ablation to validate the design choices of our data synthesis approach.

Model performance improves with more synthetic dataTo assess the impact of scaling our synthetic data, we trained three CodeLlama 7B models with 18\(k\), 50\(k\), and 100\(k\) samples respectively, while keeping all other parameters constant. We then evaluated the models on WebArena. As shown on the right, the success rates on WebArena steadily increase as the synthetic training data scales from 18\(k\) to 100\(k\) samples. This highlights the potential of scaling synthetic data with our approach.

Representing trajectories as programs is beneficialTo verify if the programs format is helpful, we convert 30\(k\) trajectories to the NL format similar to setting in WebArena  and compare its performance with model trained with the exact data, but in our program format. The results are shown in Figure 3(a). We can see that performance drops on both MiniWoB++ and WebArena when using the NL format. We hypothesize that program, in the form of API calls, is potentially a more natural format to represent sequence of actions. Our observation also echo the observations of using program representation for non-programming tasks [23; 29; 41], while our experiments further contributes insights towards finetuning setups for interactive tasks.

Different sources of indirect knowledge complement each otherOur indirect knowledge comes from two sources: tutorials and randomly sampled web pages. In the former source, the procedural knowledge from tutorials are written and verified by human. However, there is no guarantee on the authenticity of the generated observations of web pages. In contrast, the observations from the latter sources are completely real, while there is no guarantee of the trajectory accuracy. We hypothesize that the two sources can compensate each other. To test this hypothesis, we trained three models: one using 9\(k\) synthetic data mixed from both sources, and two others each using 9\(k\) data exclusively from one of the sources and the results are shown at Figure 3(b). We observe a noticeable performance degradation when models are trained with data from only one source. This indicates that utilizing multiple sources yields a more comprehensive dataset by integrating the precise procedural knowledge from tutorials with the realistic observations of web snapshot data.

Figure 3: the comparison between the models trained with trajectories generated by our approach and the data collected from human.

**Models have difficulty learning from indirect sources alone** To access if turning knowledge into trajectories is helpful, we tested a retrieval-augmented approach that directly uses the collection of tutorials as the knowledge base. We first projected text tutorials to embeddings with all-mpnet-base-v2 . Then a retriever retrieves the most relevant three tutorials measured by cosine similarity. These tutorials were included as additional context in the prompt. We tested this approach with LLama3.1-instruct-8B. As shown in Figure 3(c), feeding in tutorials directly improves model performance on WebArena marginally, and finetuning on our data shows a substantial advantage. This comparison indicates that models have difficulty making use of indirect knowledge to solve agent tasks, and trajectories is a preferable format for agent learning.

## 7 Related Work

Learning from Indirect SupervisionDue to the costly nature of human supervision, many digital agent learning works explore learning from existing yet indirect knowledge sources [8; 35; 12; 55], reinforcement learning optimization that learn from environment feedback [37; 21; 32; 39]. More recently, LLM self-improvement during inference time [34; 49; 28; 44; 14; 52] has been applied on creating more complex digital agents in the wild. Our work fills the gap of training with synthetic data generated from existing resources, _i.e.,_ indirect supervision, on complex web navigation tasks.

Prompting Approaches for AI AgentsExisting methods include performing reasoning about the current states before proceeding to next actions [43; 47; 22], search and planning [48; 13], and self-verification [18; 34; 24]. Our focus on instruction tuning data generation without human supervision can hopefully enable synthetic data generation recipes for various kinds of instruction prompts for agent tasks.

Data Generation for Interactive Agents and Instruction FinetuningExisting works design ways of generating training data that adapt LLMs to agent-specific tasks [11; 19], while our work aim to generate more realistic data without human intervention. Considering more broadly over all instruction finetuning: [59; 50; 38; 16] generate instructions of a specific task given a few examples. [42; 15] generates task-agnostic large-scale instruction tuning data without given examples. Li et al.  adopts an iterative instruction data generation and model finetuning approach, While our work generates instruction tuning datasets from readily available, indirect, unstructured knowledge resources.

## 8 Conclusion

We propose a data synthesis approach Synatra. Empirically we showcase that finetuning with data generated from our approach can improve existing general-purpose LLMs to perform better on digital agent tasks. Since Synatra can synthesize trajectories given a single, static piece of indirect knowledge, we argue that when equipped with a capable LLM, a regular tutorial designed for human consumption and a random observation, can also be re-purposed a trajectory. We show that even considering the relative high cost of calling state-of-the-art LLMs such as GPT-4, synthesizing is more cost-effective than collecting human demonstrations of similar quantity for model training.

Figure 4: ablation on trajectory formats, sources of knowledge, and ways to use knowledge.