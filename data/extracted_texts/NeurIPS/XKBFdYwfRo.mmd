# Solving Linear Inverse Problems Provably via

Posterior Sampling with Latent Diffusion Models

 Litu Rout  Negin Raoof  Giannis Daras

Constantine Caramanis  Alexandros G. Dimakis  Sanjay Shakkottai

The University of Texas at Austin

Email:{litu.rout,neginmr,giannisdaras,constantine,sanjay.shakkottai}utexas.edu, dimakis@austin.utexas.edu

###### Abstract

We present the first framework to solve linear inverse problems leveraging pre-trained _latent_ diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to _pixel-space_ diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.

## 1 Introduction

We study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand , and unsupervised methods that use the prior learned by a generative model to guide the restoration process ; see also the survey of Ongie et al.  and references therein.

The second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models _sample_ from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions  and regression to the mean .

Diffusion models have emerged as a powerful new approach to generative modeling . This family of generative models works by first corrupting the data distribution \(p_{0}(_{0})\) using an Ito Stochastic Differential Equation (SDE), \(=(,t)t+g(t)\), and then by learning the score-function, \(_{_{t}} p_{t}(_{t})\), at all levels \(t\), using Denoising Score Matching (DSM) . The seminal result of Anderson  shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another Ito SDE. The SDE that corrupts the data is often termed as Forward SDE and its reverse as Reverse SDE . The latter depends on the score-function \(_{_{t}} p_{t}(_{t})\) that we learn through DSM. In , the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.

The success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al.  showed that given measurements \(=_{0}+_{y}\), we canprovably sample from the distribution \(p_{0}(_{0}|)\) by running a modified Reverse SDE that depends on the unconditional score \(_{_{i}} p_{t}(_{t})\) and the term \(_{_{i}} p(|_{t})\). The latter term captures how much the current iterate explains the measurements and it is intractable even for linear inverse problems without assumptions on the distribution \(p_{0}(x_{0})\)[11; 14]. To deal with the intractability of the problem, a series of approximation algorithms have been developed [22; 11; 2; 13; 26; 10; 6; 46; 12; 27] for solving (linear and non-linear) inverse problems with diffusion models. These algorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively solve problems such as inpainting, deblurring, super-resolution among others.

Recently, diffusion models have been generalized to learn to invert non-Markovian and non-linear corruption processes [16; 15; 3]. One instance of this generalization is the family of Latent Diffusion Models (LDMs) . LDMs project the data into some latent space, \(_{0}=(_{0})\), perform the

Figure 1: Overall pipeline of our proposed framework from left to right. Given an image (**left**) and a user defined mask (**center**), our algorithm inpaints the masked region (**right**). The known part of the images are unaltered (see Appendix C for web demo and image sources).

diffusion in the latent space and use a decoder, \((_{0})\), to move back to the pixel space. LDMs power state-of-the-art foundation models such as Stable Diffusion  and have enabled a wide-range of applications across many data modalities including images , video , audio  and medical domain distributions (e.g., for MRI and proteins) [38; 51]. Unfortunately, none of the existing algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to use a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform finetuning for each task of interest.

In this paper, we present the first framework to solve general inverse problems with pre-trained _latent_ diffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the diffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing the power of available foundation models, we are able to outperform previous approaches without finetuning across a wide range of problems (see Figure 1 and 2).

**Our contributions are as follows:**

1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve linear inverse problem when the degradation operator is known.
2. We theoretically analyze our algorithm and show provable sample recovery in a linear model setting with two-step diffusion processes.
3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models, outperforming previous approaches for inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.2 
## 2 Background and Method

**Notation:** Bold lower-case \(\), bold upper-case \(\), and normal lower case \(x\) denote a vector, a matrix, and a scalar variable, respectively. We denote by \(\) element-wise multiplication. \(()\) represents a diagonal matrix with entries \(\). We use \((.)\) for the encoder and \((.)\) for the decoder. \( p\) is a pushforward measure of \(p\), i.e., for every \( p\), the sample \(()\) is a sample from \( p\). We use arrows in Section 3 to distinguish random variables of the forward (\(\)) and the reverse process (\(\)).

The standard diffusion modeling framework involves training a network, \(_{}(_{t},t)\), to learn the score-function, \(_{_{t}} p_{t}(_{t})\), at all levels \(t\), of a stochastic process described by an Ito SDE:

\[=(,t)t+g(t),\] (1)

where \(\) is the standard Wiener process. To generate samples from the trained model, one can run the (unconditional) Reverse SDE, where the score-function is approximated by the trained neural network. Given measurements \(=x_{0}+_{y}\), one can sample from the distribution \(p_{0}(_{0}|)\) by running the conditional Reverse SDE given by:

\[=(,t)-g^{2}(t)(_{_{t}}  p_{t}(_{t})+_{_{t}} p(|_{t})) \,t+g(t).\] (2)

As mentioned, \(_{_{t}} p(|_{t})\) is intractable for general inverse problems. One of the most effective approximation methods is the DPS algorithm proposed by Chung et al. . DPS assumes that:

\[p(|_{t}) p(|}_{0}[ _{0}|_{t}])=(;=[ _{0}|_{t}],=_{y}^{2}I).\] (3)

Essentially, DPS substitutes the unknown clean image \(_{0}\) with its conditional expectation given the noisy input, \([_{0}|_{t}]\). Under this approximation, the term \(p(|_{t})\) becomes tractable.

The theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze DPS in a linear model setting where the data distribution lives in a low-dimensional subspace, and show that DPS actually samples from \(p(_{0}|)\) (Section A.1). Then, we provide an _algorithm_ (Section 2.1) and its _analysis_ to sample from \(p(_{0}|)\) using latent diffusion models (Section 3.2). Importantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while avoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS. Using experiments (Section 4), we show that our algorithm allows us to use powerful foundation models and solve linear inverse problems, outperforming previous unsupervised approaches without the need for finetuning.

### Method

In Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model \(_{}(_{t},t)\) to predict the score \(_{_{t}} p_{t}(_{t})\), of a diffusion process:

\[=(,t)t+g(t),\] (4)

where \(_{0}=(_{0})\) for some encoder function \(():^{d}^{k}\). During sampling, we start with \(_{T}\), we run the Reverse Diffusion Process and then we obtain a clean image by passing \(_{0} p_{0}(_{0}|_{T})\) through a decoder \(:^{k}^{d}\).

Although Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate \(p(|_{t})\) with:

\[p(|_{t}) p(|_{0}=([ _{0}|_{t}])),\] (5)

i.e., to approximate the unknown clean image \(_{0}\) with the decoded version of the conditional expectation of the clean latent \(_{0}\) given the noisy latent \(_{t}\). However, as we show experimentally in Section 4, this idea does not work. The failure of the "vanilla" extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents \(_{0}\) that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling \(_{t}\) towards any of these latents \(_{0}\), potentially in different directions. On the other hand, the score-function is pulling \(_{t}\) towards a specific \(_{0}\) that corresponds to the best denoised version of \(_{t}\).

To address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \( p(|_{t})\) with:

\[_{_{t}} p(|_{t})=_{t}}  p(|}_{0}=([_{0}|_ {t}]))}_{}+_{t}_{t}} ||[_{0}|_{t}]-(( [_{0}|_{t}]))||^{2}}_{_{0}$}}.\] (6)

We refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \(_{0}\) as long as \((_{0})\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.

\[_{_{t}} p(|_{t}) =_{t}} p(|_{0}= ([_{0}|_{t}]))}_{}\] \[+_{t}}||[_{0}|_{t}]-(^{T}+(-^{T} )([_{0}|_{t}]))||^{2}}_ {_{0}$}}.\] (7)

The gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation (7) as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms.

**Remark 2.1**.: Consider the optimization problem of projecting onto the measurements:

\[_{_{0}} \|}_{0}-_{0}\|_{2}^{2}\] subject to \[_{0}=,\]

In the linear setting, the optimal solution is given by \(x_{0}^{*}=^{T}(^{T})^{-1}+(} _{0}-^{T}(^{T})^{-1}(} _{0}))\). Now further suppose that the measurement rows are orthogonal, i.e. \(^{T}=_{l}\). This condition holds for some natural linear inverse problems like inpainting. Suppose that we want to update the latent vector \(_{t}\) such that \([_{0}|_{t}]=(x_{0}^{*})\); this ensures that the gradientsresulting from the two terms in (7) both point to the same optima in the latent space. Equivalently, we want to solve the following minimization problem: \(_{_{t}}\|[_{0}|_{t}]-(x_{0}^{*}) \|_{2}^{2}\). Substituting \((x_{0}^{*})=(^{T}+(}_{0}- ^{T}}_{0}))=(^{T}+(-^{T})}_{0})\), and \(}_{0}=([_{0}|_{t}])\), we can thus interpret the gluing objective in (7) as a one step of gradient descent of this loss \(\|[_{0}|_{t}]-(x_{0}^{*})\|_{2}^{2}\) with respect to \(z_{t}\). Note that, if there was no latent space, our gluing would be equivalent to a projection on the measurements, but now because of the encoder and decoder, it is not.

## 3 Theoretical Results

As discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by Ito SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \(}_{0}^{} p(}_{0}^{})\) and gradually adds noise, i.e., \(}_{t+1}^{}=}}_{t}+}\) where \(_{t}\) and \(_{t}_{t-1}\) for \(t=0,,T-1\). The reverse process is initialized with \(}_{T}(,_{d})\) and generates \(}_{t-1}=_{}(}_{t},t)+ }\). In the last step, \(_{}(}_{1},1)\) is displayed without the noise.

In this section, we consider the diffusion discretized to two steps (\((}_{0}^{},}_{1}^{})\)), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of **Algorithm 1** and **Algorithm 2** in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory. In Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps (\((}_{0}^{},}_{1}^{}, ,}_{T}^{})\), where \(T=1000\)), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of **Algorithm 2** in Appendix C.1. All proofs are given in Appendix B.

### Problem Setup

The goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to , where the images (\(}_{0}^{}^{d}\)) reside in a linear subspace of the form \(}_{0}^{}=}_{0}^{ },^{d l},}_{0}^{ }^{l}\), and \(_{y}=0\). Here, \(\) is a tall thin matrix with \(rank()=l d\) that lifts any latent vector \(}_{0}^{}(,_{l})\) to the image space with ambient dimension \(d\). Given the measurements \(=}_{0}^{}+_{y}\), \(^{l d},^{l}\), the goal is to sample from \(p_{0}(}_{0}^{}|)\) using a pre-trained latent diffusion model. In the inpainting task, the measurement operator \(\) is such that \(^{T}\) is a diagonal matrix \(()\), where \(\) is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix B for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice , we consider a setting where the latent vector of the VAE is \(k\)-dimensional and the latent distribution is a standard Gaussian \((,_{k})\). Our analysis shows that the proposed **Algorithm 2** provably solves inverse problems under the following assumptions.

**Assumption 3.1**.: The columns of the data generating model \(\) are orthonormal, i.e., \(^{T}=_{l}\).

**Assumption 3.2**.: The measurement operator \(\) satisfies \(()^{T}()\).

These assumptions have previously appeared, e.g., . While **Assumption 3.1** is mild and can be relaxed at the expense of (standard) mathematical complications, **Assumption 3.2** indicates that \(()^{T}()\) is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of \(l\) coordinates exactly determines the remaining \((d-l)\) coordinates of \(_{0}}\). The underlying assumption is that there _exists_ a solution and it is _unique_. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works , we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: _Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?_

### Posterior Sampling using Latent Diffusion Model

In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in **Algorithm 2**. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., \(k=l\), and normalize \(_{i}=1\) (as this is immaterial in the linear setting). In **Proposition 3.3**, we provide analytical solutions for the encoder and the decoder of the VAE.

**Proposition 3.3** (Variational Autoencoder).: _Suppose **Assumption 3.1** holds. For an encoder \(:^{d}^{k}\) and a decoder \(:^{k}^{d}\), denote by \((,)\) the training objective of VAE:_

\[_{,}(,)_ {_{0}} p}[\|(( _{0}};);)-_{0}}\| _{2}^{2}]+ KL( p,(,_{k})),\]

_then the combination of \((_{0}};)=^{T}_{0}}\) and \((_{0}};)=_{0}}\) is a minimizer of \((,)\)._

Using the encoder \((_{0}};)=^{T}_{0}}\), we can use the analytical solution \(^{*}\) of the LDM obtained in **Theorem A.1**. To verify that \(^{*}\) recovers the true subspace \(p(_{0}})\), we compose the decoder \((_{0}};)=_{0}}\) with the generator of the LDM, i.e., \(_{0}}=(^{*}_{1}})=(_{k}_{1}} )=_{1}}\). Since \(_{1}}(,_{k})\) and \(\) is the data generating model, this shows that \(_{0}}\) is a sample from \(p(_{0}})\). Thus we have the following.

**Theorem 3.4** (Generative Modeling using Diffusion in Latent Space).: _Suppose **Assumption 3.1** holds. Let the optimal solution of the latent diffusion model be_

\[^{*}=_{}_{_{0} },}}[\|_{1}( {_{1}}(_{0}},}), _{0}})-_{}(_{1 }}(_{0}},}) )\|^{2}].\]

_For a fixed variance \(>0\), if \(_{}(_{1}}(_{0}},})) _{1}}(_{0}},})\), then the closed-form solution is \(^{*}=_{k}\), which after normalization by \(}\) and composition with the decoder \((_{0}};)= _{0}}\) recovers the true subspace of \(p(_{0}})\)._

With this optimal \(^{*}\), we can now prove exact sample recovery using GML-DPS (6).

**Theorem 3.5** (Posterior Sampling using Goodness Modified Latent DPS).: _Let \(\) and 3.2 hold. Let \(_{j}, j=1,,r\), denote the singular values of \(()^{T}()\), and let_

\[^{*}=_{}_{_{0}}, }}[\|_{1}(_{1}}(_{0}},}), _{0}})-_{}(_{1 }}(_{0}},})) \|^{2}].\]

_Given a partially known image \(_{0}} p(_{0}})\), any fixed variance \((0,1)\), then with the (unique) step size \(_{i}^{j}=1/2_{j},j=1,2,,r\), the GML-DPS Algorithm (6) samples from the true posterior \(p(_{0}}|y)\) and exactly recovers the groundtruth sample, i.e., \(_{0}}=_{0}}\)._

**Theorem 3.5** shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size \(\) to be chosen _coordinate-wise_ in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern forLDMs due to one-to-many mappings of the decoder. Note that the _goodness objective_ (Section 2.1) cannot help in this scenario because it assigns uniform probability to many of these latents \(_{1}}\) for which \(_{_{1}}}}_{0}( _{1}})-((_{0}}(_{1}})))^{2}=0\). These challenges motivate the _gluing objective_ in **Theorem 3.6**. This is crucial for two reasons. First, we show that it helps recover the true sample even when the step size \(\) is chosen arbitrarily. Second, it assigns all the probability mass to the desired (unique) solution in the pixel space.

**Theorem 3.6** (Posterior Sampling using Diffusion in Latent Space).: _Let **Assumptions 3.1 and 3.2 hold. Let \(_{j}, j=1,,r\) denote the singular values of \(()^{T}()\) and let_

\[^{*}=_{}_{_{0}}, }}[\|_{1}(_{1}}(_{0}},}),_{0}})-_{}(_{1}}( _{0}},}))\|^{2} ].\]

_Given a partially known image \(_{0}} p(_{0}})\), any fixed variance \((0,1)\), and any positive step sizes \(_{i}^{j},j=1,2,,r\), the PSLD Algorithm 2 samples from the true posterior \(p(_{0}}|y)\) and exactly recovers the groundtruth sample, i.e., \(}_{0}=_{0}}\)._

The important distinction between **Theorem 3.5** and **Theorem 3.6** is that the former requires the _exact_ step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar \(\)), and gluing updates, we have

\[_{0}}=^{*}_{1}}- _{_{1}}}\|( {_{0}}(_{1}}))-\|_{2}^{2}-_{ _{1}}}\|_{0}}(_{1}})-(^{T}_{0}}+( _{d}-^{T})(_{0}}( _{1}})))\|_{2}^{2}.\]

When \(\) is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution \(_{0}}\). When the reverse SDE generates the exact same groundtruth sample, i.e., \((_{1}}(_{0}}))= _{0}}\), then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of \(\).

We draw the following key insights from our **Theorem 3.6**: **Curse of ambient dimension:** In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the \(d\)-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension. **Large-scale foundation model:** We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems. **Robustness to measurement step:** The gluing objective makes our algorithm robust to the choice of step size \(\). Furthermore, it allows the same (scalar) step size across all the coordinates of \(_{0}}\).

## 4 Experimental Evaluation

We experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset  (downscaled to \(256 256^{3}\), denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset  (resized to \(256 256\), denoted by ImageNet 256). To make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS . It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset [44; 45]; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ \(256\) dataset . The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ \(256\) images, which excludes the first 1K images used as validation set.

**Inverse Problems.** We experiment with the following task-specific measurement operators from the baseline DPS : (i) Box inpainting uses a mask of size 128x128 at the center. (ii) Random inpainting chooses a drop probability uniformly at random between \((0.2,0.8)\) and applies this drop

   Method & PSLD (Ours) & DPS  \\  \(2\) & **0.185** & 0.220 \\ \(3\) & **0.220** & 0.247 \\ \(4\) & **0.233** & 0.291 \\   

Table 1: Quantitative super-resolution (using measurement operator from ) results on FFHQ \(256\) validation samples [25; 11]. We use PSLD with Stable Diffusion. Table shows LPIPS (\(\)).

probability to all the pixels. (iii) Super-resolution downsamples images at \(4\) scale. (iv) Gaussian blur convolves images with a Gaussian blur kernel. (v) Motion blur convolves images with a motion blur kernel. We also experiment with these additional operators from RePaint : (vi) Super-resolution downsamples images at \(2\), \(3\), and \(4\) scale. (vii) Denoising has Gaussian noise with \(=0.05\). (viii) Destriping has vertical and horizontal stripes in the input images.

**Evaluation.** We compare the performance of our PSLD algorithm with the state-of-the-art DPS algorithm  on random inpainting, box inpainting, denoising, Gaussian deblur, motion deblur, arbitrary masking, and super-resolution tasks. We show that PSLD outperforms DPS, both in-distribution and out-of-distribution datasets, using the Stable Diffusion v-1.5 model pre-trained on the LAION dataset. We also test PSLD with LDM-VQ-4 trained on FFHQ \(256\), to compare with DPS trained on the same data distribution. Note that the LDM-v4 is a latent-based model released prior to Stable Diffusion. Therefore, it does not match the performance of Stable Diffusion in solving inverse problems. However, it shows the general applicability of our framework to leverage an LDM in posterior sampling. Since Stable Diffusion v-1.5 is trained with an image resolution of \(512 512\), we apply the forward operator after upsampling inputs to \(512 512\), run posterior sampling at \(512 512\), and then downsample images to the original \(256 256\) resolution for a fair comparison with DPS. We observed a similar performance while applying the masking operator at \(256 256\) and upscaling to \(512 512\) before running PSLD. More implementation details are provided in Appendix C.1.

**Metrics.** We use the commonly used Learned Perceptual Image Patch Similarity (LPIPS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM), and Frechet Inception Distance4 (FID) metrics for quantitative evaluation.

**Results.** Figure 2 shows the inpainting results on out-of-distribution samples. This experiment was performed on commercial platforms that use (to the best of our knowledge) Stable diffusion and additional proprietary models. This evaluation was performed on models deployed in May 2023 and may change as commercial providers improve their platforms.

The qualitative advantage of PSLD is clearly demonstrated in Figures 2, 3, 4, 15 and 16. In Figure 5, we compare PSLD and DPS in random inpainting task for varying percentage of dropped pixels. Quantitatively, PSLD outperforms DPS in commonly used metrics: LPIPS, PSNR, and SSIM.

In our PSLD algorithm, we use Stable Diffusion v1.5 model and (zero-shot) test it on inverse problems. Table 6 compares the quantitative results of PSLD with related works on random inpainting, box inpainting, super-resolution, and Gaussian deblur tasks. PSLD significantly outperforms previous approaches on the relatively easier random inpainting task, and it is better or comparable on harder tasks. Table 4 draws a comparison between PSLD and the strongest baseline (among the compared methods) on out-of-distribution images. Table 1 shows the super-resolution results using nearest-neighbor kernels from  on FFHQ 256 validation dataset. Observe that PSLD outperforms state-of-the-art methods across diverse tasks and standard evaluation metrics.

In Table 3, we compare PSLD (using LDM-VQ-4) and DPS on random and box inpainting tasks with the same operating resolution (\(256 256\)) and training distributions (FFHQ 256). Although the LDM model exceeds DPS performance in box inpainting, it is comparable in random inpainting. As expected, using a more powerful pre-trained model such as Stable Diffusion is beneficial in

    &  &  &  &  \\  Method & FID (\(\)) & LPIPS (\(\)) & FID (\(\)) & LPIPS (\(\)) & FID (\(\)) & LPIPS (\(\)) & FID (\(\)) & LPIPS (\(\)) \\  PSLD (Ours) & **21.34** & **0.096** & 43.11 & **0.167** & **34.28** & **0.201** & **41.53** & **0.221** \\  DPS  & 33.48 & 0.212 & **35.14** & 0.216 & 39.35 & 0.214 & 44.05 & 0.257 \\ DDRM  & 69.71 & 0.587 & 42.93 & 0.204 & 62.15 & 0.294 & 74.92 & 0.332 \\ MCG  & 29.26 & 0.286 & 40.11 & 0.309 & 87.64 & 0.520 & 101.2 & 0.340 \\ PnP-ADMM  & 123.6 & 0.692 & 151.9 & 0.406 & 66.52 & 0.353 & 90.42 & 0.441 \\ Score-SDE  & 76.54 & 0.612 & 60.06 & 0.331 & 96.72 & 0.563 & 109.0 & 0.403 \\ ADMM-TV & 181.5 & 0.463 & 68.94 & 0.322 & 110.6 & 0.428 & 186.7 & 0.507 \\   

Table 2: Quantitative inpainting results on FFHQ \(256\) validation set . We use Stable Diffusion v-1.5 and the measurement operators as in DPS . As shown, our PSLD model outperforms DPS since it is able to leverage the power of the Stable Diffusion foundation model.

reconstruction-see Table 6. This highlights the significance of our PSLD algorithm that has the provision to incorporate a powerful foundation model with no extra training costs for solving inverse problems. Importantly, PSLD uses latent-based diffusion, and thus it avoids the curse of ambient dimension (**Theorem 3.6**), while still achieving comparable results to the state-of-the-art method DPS  that has been trained on the same dataset. Additional experimental evaluation is provided in Appendix C.

## 5 Conclusion

In this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm - Posterior

    &  &  \\  Method & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) \\  PSLD (Ours) & **34.02** & **0.951** & **0.083** & **33.71** & **0.943** & **0.096** \\ DPS  & 31.41 & 0.884 & 0.171 & 29.49 & 0.844 & 0.212 \\   

Table 4: Quantitative results of random inpainting and denoising on FFHQ \(256\)[25; 11] using Stable Diffusion v-1.5. Note that DPS is trained on FFHQ \(256\). The results show that our method PSLD generalizes well to out-of-distribution samples even without finetuning.

    &  &  \\  Method & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) \\  PSLD (Ours) & **30.31** & **0.851** & 0.221 & **24.22** & **0.819** & **0.158** \\ DPS  & 29.49 & 0.844 & **0.212** & 23.39 & 0.798 & 0.214 \\   

Table 3: Quantitative inpainting results on FFHQ \(256\) validation set [25; 11]. We use the _latent diffusion_ (LDM-VQ-4) trained on FFHQ \(256\). Note that in this experiment PSLD and DPS use diffusion models trained on the same dataset. As shown, PSLD with LDM-VQ-4 as diffusion model outperforms DPS in box inpainting and has comparable performance in random inpainting.

Figure 2: Inpainting results in general domain images from the web (see Appendix C for image sources). Our model compared to state-of-art commercial inpainting services that leverage the same foundation model (Stable Diffusion v-1.5).

Sampling with Latent Diffusion (PSLD) - that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.

**Limitations.** Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like  to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.

Figure 4: Inpainting (random and box) results on out-of-distribution samples, \(256 256\) (see Appendix C for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.

Figure 5: Comparing DPS and PSLD performance in random inpainting on FFHQ 256 , as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.

Figure 3: **Left panel:** Random Inpainting on images from FFHQ 256  using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row. **Right panel:** Block (\(128 128\)) inpainting, using the LDM-VQ-4 model trained on FFHQ \(256\). Notice the glasses in the top row and eyes in the bottom row.