# Stability-penalty-adaptive

follow-the-regularized-leader:

Sparsity, game-dependency, and best-of-both-worlds

 Taira Tsuchiya

The University of Tokyo

tsuchiya@mist.i.u-tokyo.ac.jp

&Shinji Ito

NEC Corporation / RIKEN

i-shinji@nec.com

&Junya Honda

Kyoto University / RIKEN

honda@i.kyoto-u.ac.jp

This work was done when the author was with Kyoto University and RIKEN.

###### Abstract

Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-regularized-leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called _stability-penalty-adaptive (SPA) learning rate_ for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: _sparsity_, _game-dependency_, and _best-of-both-worlds_ (BOBW). Despite the fact that sparsity appears frequently in real problems, existing sparse multi-armed bandit algorithms with \(k\)-arms assume that the sparsity level \(s k\) is known in advance, which is often not the case in real-world scenarios. To address this issue, we first establish \(s\)-agnostic algorithms with regret bounds of \(()\) in the adversarial regime for \(T\) rounds, which matches the existing lower bound up to a logarithmic factor. Meanwhile, BOBW algorithms aim to achieve a near-optimal regret in both the stochastic and adversarial regimes. Leveraging the SPA learning rate and the technique for \(s\)-agnostic algorithms combined with a new analysis to bound the variation in FTRL output in response to changes in a regularizer, we establish the first BOBW algorithm with a sparsity-dependent bound. Additionally, we explore partial monitoring and demonstrate that the proposed SPA learning rate framework allows us to achieve a game-dependent bound and the BOBW simultaneously.

## 1 Introduction

This study considers the multi-armed bandits (MAB) and partial monitoring (PM). In the MAB problem, the learner selects one of \(k\) arms, and the adversary simultaneously determines the loss of each arm, \(_{t}=(_{t1},,_{tk})^{}\) in \(^{k}\) or \([-1,1]^{k}\). After that, the learner observes only the loss for the chosen arm. The learner's goal is to minimize the regret, which is the difference between the learner's total loss and the total loss of an optimal arm fixed in hindsight. PM is a generalization of MAB, and the learner observes feedback symbols instead of the losses.

One of the most promising frameworks for MABs and PM is follow-the-regularized-leader (FTRL) [5; 12], which determines the arm selection probability at each round by minimizing the sum of the cumulative (estimated) losses so far plus a convex regularizer. Note that the well-known Exp3 algorithm developed in  is equivalent to FTRL with the (negative) Shannon entropy regularizer. FTRL is also known to perform well for the classic expert problem  and reinforcement learning . Furthermore, when the problem is "benign", it is known that FTRL can exploit the underlying structure to adaptively improve its performance. Typical examples of such adaptive improvements are (i) _data-dependent bounds_ and (ii) _best-of-both-worlds_ (BOBW).

Data-dependent bounds have been investigated to enhance the adaptivity of algorithms to a given structure of losses in the _adversarial regime_, where feedback (_e.g.,_ losses in MABs) is decided in an arbitrary manner. There are various examples of data-dependent bounds, and this study considers _sparsity-dependent bounds_ and _game-dependent bounds_.

A sparsity-dependent bound is an important example of data-dependent bounds, as sparsity frequently appears in real-world problems. For example, in online advertisement allocation, it is often the case that only a fraction of the ads is clicked. Although there are some studies for sparse MABs [10; 27; 51], all of them assume that (an upper bound of) sparsity level \(s\|_{t}\|_{0}=|\{i[k]_{ti} 0\}|\) is known beforehand, which in many practical scenarios does not hold.

The concept of a game-dependent bound was recently introduced by Lattimore and Szepesvari  to derive a regret upper bound that depends on the game the learner is facing. As the authors suggest, one of the motivations for the game-dependent bound is that previous PM algorithms are "quite conservative and not practical for normal problems". For example, whereas the Bernoulli MAB is expressed as a PM, algorithms for PM do not always achieve the minimax regret of MAB . The game-dependent bound enables the learner to automatically adapt to the essential difficulty of the game the algorithm is actually facing.

The BOBW algorithm aims to achieve near-optimal regret bounds in stochastic and adversarial regimes, where the feedback is stochastically generated in the stochastic regime. Since we often do not know the underlying regime, it is desirable for an algorithm to _simultaneously_ obtain a near-optimal performance both for the stochastic and adversarial regimes. For multi-armed bandits, Bubeck and Slivkins  developed the first BOBW algorithm, and Zimmert and Seldin  proposed the well-known Tsallis-INF algorithm, which achieves the optimal regret for both regimes. The Tsallis-INF algorithm also achieves favorable regret guarantees in the _adversarial regime with a self-bounding constraint_, which interpolates between the stochastic and adversarial regimes.

To realize the aforementioned adaptivity in FTRL, the _adaptive learning rate_ (a.k.a. time-varying learning rate) is one of the most representative approaches. This approach adjusts the learning rate based on previous observations. In the literature, adaptive learning rates have been designed to depend on _stability_ or _penalty_, which are components of a regret upper bound of FTRL. The stability term increases if the variation of FTRL outputs in the adjacent rounds is large, and stability-dependent learning rates have been used in a considerable number of algorithms available in the literature, _e.g.,_[32; 37; 38] and references therein. In contrast, the penalty term comes from the strength of the regularization, and recently penalty-dependent learning rates were considered to achieve BOBW guarantees [22; 46]. However, existing stability-dependent (resp. penalty-dependent) learning rates are designed with the worst-case penalty (resp. stability), which could potentially limit the adaptivity and performance of FTRL. (There are numerous studies related to this paper and we include additional related work in Appendix C.)

### Contribution of this study

In this paper, in order to further broaden the applicability of FTRL, we establish a generic framework for designing an adaptive learning rate that depends on both the stability and penalty components simultaneously, which we call a _stability-penalty-adaptive (SPA) learning rate_ (Definition 2). This enables us to bound the regret approximately by \(^{T}z_{t}h_{t+1}}\) for stability component \((z_{t})_{t}\) and a penalty component \((h_{t})_{t}\), which we call a _SPA regret bound_ (Theorem 1). With appropriate selections of \(z_{t}\) and \(h_{t}\), this result yields the three important adaptive bounds mentioned earlier, namely sparsity, game-dependency, and BOBW. In particular, our contributions are as follows (see also Tables 1 and 2):* (Section 5.1) We initially provide new algorithms for sparse MABs as preliminaries for establishing a BOBW algorithm with a sparsity-dependent bound. In Section 5.1.1, we propose a novel estimator of the sparsity level, which is linked to a stability component and induces \(L_{2}=_{t=1}^{T}_{t}^{2} sT\). We demonstrate that a learning rate using this estimator with the Shannon entropy regularizer and \(((kT)^{-2/3})\) uniform exploration immediately results in an \(O( k})\) regret bound for \(_{t}^{k}\). In Section 5.1.2, we investigate possibly negative losses \(_{t}[-1,1]^{k}\). We employ the time-invariant log-barrier proposed in  to control the stability term. This allows us to achieve an \(O( k})\) regret bound for losses in \([-1,1]^{k}\) even _without_ the \(((kT)^{-2/3})\) uniform exploration. This is a key component for developing the BOBW guarantee that we discuss next. Note that Section 5.1 serves as preliminary findings for the subsequent section.
* (Section 5.2) We establish a BOBW algorithm with a sparsity-dependent bound. In order to achieve this goal, we make another major technical development: we analyze the variation in the FTRL output when the regularizer changes (Lemma 7), which holds thanks to the time-invariant log-barrier and may be of independent interest. This analysis is necessary since we use a time-varying learning rate to obtain a BOBW guarantee, whereas Bubeck et al.  uses a constant learning rate. This technical development successfully allows us to achieve the goal (Theorem 4) in combination with the SPA learning rate developed in Section 4 and a technique for exploiting sparsity in Section 5.1.2.
* (Section 6) We show that the SPA learning rate established in Section 4 can also be used to achieve a game-dependent bound and a BOBW guarantee simultaneously, which further highlights the usefulness of the SPA learning rate.

## 2 Setup

This section introduces the preliminaries of this study. Sections 2.1 and 2.2 formulate the MAB and PM problems, respectively, and Section 2.3 defines regimes considered in this paper.

   Reference & \(s\)-agnostic? & Range of \(_{ti}\) & Regime & Regret bound \\  Kwon and Perchet  & â€“ & \(\) & Adv. & \(()\) \\  Kwon and Perchet  & No & \(\) & Adv. & \(2\) \\
**Ours (Sec. 5.1.1, Cor. 2)** & Yes & \(\) & Adv. & \(2 k}+O((kT k)^{1/3})\) \\  Bubeck et al.  & No & \([-1,1]\) & Adv. & \(10 k}+20k T\) \\
**Ours (Sec. 5.1.2, Cor. 3)** & Yes & \([-1,1]\) & Adv. & \(4 k}+2k T\) \\
**Ours (Sec. 5.2, Thm. 4)** & Yes & \([-1,1]\) & Adv. & \(4 k T}+O(k T)\) \\  & & & Stoc. & \(O(s(T)(kT)/_{})\) \\   

Table 1: Regret upper bounds with sparsity-dependent bounds in multi-armed bandits. \(T\) is the time horizon. \(s k\) is the level of sparsity in losses. Let \(L_{2}=_{t=1}^{T}_{t}^{2}\), and \(_{t}_{0} s\) implies \(L_{2}=_{t=1}^{T}_{t}^{2} sT\) since \(_{t}_{} 1\). \(_{}\) is the minimum suboptimality gap. Adv. and Stoc. are the abbreviations of the adversarial and stochastic regime, respectively.

   Reference & Game-dependent? & BOBW? & Order of regret bound \\  Many existing studies on PM & No & No & â€“ \\ Lattimore and Szepesvari  & Yes & No & \(^{T}V_{t} k}\) \\ Tsuchiya et al.  & No (only game-class-dependent) & Yes & \(_{t=1}^{T}H(q_{t+1})}\) \\
**Ours (Sec. 6, Cor. 5)** & Yes & Yes & \(^{T}V_{t}^{}H(q_{t+1}) T}\) \\   

Table 2: Regret bounds for non-degenerate local PM games. \(V_{t}\), \(V_{t}^{}\), and \(^{}\) are game-dependent quantities satisfying \(V_{t} V_{t}^{}\) (see Section 6 and Appendix B for definitions). \(H(q_{t})\) is the Shannon entropy for FTRL output \(q_{t}\).

NotationLet \(\|x\|\), \(\|x\|_{1}\), and \(\|x\|_{}\) be the Euclidian, \(_{1}\)-, and \(_{}\)-norms for a vector \(x\), respectively. Let \(\|x\|_{0}\) be the number of non-zero elements for a vector \(x\). Let \(_{k}=\{p^{k}:\|p\|_{1}=1\}\) be the \((k-1)\)-dimensional probability simplex. A vector \(e_{i}\{0,1\}^{k}\) is the \(i\)-th standard basis of \(^{k}\), and \(\) is the all-one vector. Let \(D_{}\) be the _Bregman divergence_ induced by differentiable convex function \(\), _i.e.,_\(D_{}(p,q)=(p)-(q)-(q),p-q\). Table 3 in Appendix A summarizes the notation used in this paper.

### Multi-armed bandits

In MAB with \(k\)-arms, at each round \(t[T]\{1,2,,T\}\), the environment determines the loss vector \(_{t}=(_{t1},_{t2},,_{tk})^{}\) in \(^{k}\) or \([-1,1]^{k}\), and the learner simultaneously chooses an arm \(A_{t}[k]\) without knowing \(_{t}\). After that, the learner observes only the loss \(_{tA_{t}}\) for the chosen arm. The performance of the learner is evaluated by the regret \(_{T}\), which is the difference between the cumulative loss of the learner and of the single optimal arm, that is, \(a^{*}=_{a[k]}_{t=1}^{T}_{ta}\) and \(_{T}=_{t=1}^{T}(_{tA_{t}}-_{ta^{*}}) ,\) where the expectation is taken with respect to the internal randomness of the algorithm and the randomness of the loss vectors \((_{t})_{t=1}^{T}\).

### Partial monitoring

FormulationA PM game \(=(,)\) with \(k\)-actions and \(d\)-outcomes is defined by a pair of a loss matrix \(^{k d}\) and feedback matrix \(^{k d}\), where \(\) is a set of feedback symbols. The game is played in a sequential manner by a learner and an opponent across \(T\) rounds. The learner begins the game with knowledge of \(\) and \(\). For every round \(t[T]\), the opponent selects an outcome \(x_{t}[d]\), and the learner simultaneously chooses an action \(A_{t}[k]\). Then the learner suffers an unobserved loss \(_{A_{t}x_{t}}\) and receives only a feedback symbol \(_{t}=_{A_{t}x_{t}}\), where \(_{ax}\) is the \((a,x)\)-th element of \(\). The learner's performance in the game is evaluated by the regret \(_{T}\) as in the MAB case: \(a^{*}=_{a[k]}_{t=1}^{T}_{ax_{t}} \) and \(_{T}=_{t=1}^{T}(_{A_{t}x_{t}}- _{a^{*}x_{t}})=_{t=1}^{T}_{ A_{t}}-_{a^{*}},e_{x_{t}},\) where \(_{a}^{d}\) is the \(a\)-th row of \(\).

Several concepts in PMLet \(m||\) be the maximum number of distinct symbols in a single row of \(^{k d}\). Different actions \(a\) and \(b\) are duplicate if \(_{a}=_{b}\). We can decompose possible distributions of \(d\) outcomes in \(_{d}\) based on the loss matrix. For every action \(a[k]\), cell \(_{a}=\{u_{d}:_{b[k]}(_{a}-_{b})^{} u 0\}\) is the set of probability vectors in \(_{d}\) for which action \(a\) is optimal. Each cell is a closed convex polytope.

Define \((_{a})\) as the dimension of the affine hull of \(_{a}\). Action \(a\) is said to be dominated if \(_{a}=\). For non-dominated actions, action \(a\) is said to be Pareto optimal if \((_{a})=d-1\), and degenerate if \((_{a})<d-1\). Let \(\) be the set of Pareto optimal actions. Two Pareto optimal actions \(a,b\) are called neighbors if \((_{a}_{b})=d-2\), which is used to define the difficulty of PM games. A PM game is said to be non-degenerate if it has no degenerate actions. We assume that PM game \(\) is non-degenerate and contains no duplicate actions.

The difficulty of PM games is characterized by the following observability conditions. Neighbouring actions \(a\) and \(b\) are locally observable if there exists \(w_{ab}:[k]\) such that \(w_{ab}(c,)=0\) for \(c\{a,b\}\) and \(_{c=1}^{k}w_{ab}(c,_{cx})=_{ax}-_{bx}\) for all \(x[d]\). A PM game is locally observable if all neighboring actions are locally observable, and this study considers locally observable games.

Loss difference estimationLet \(\) be the set of all functions from \([k]\) to \(^{d}\). For any locally observable games, there exists \(G\) such that for any \(b,c\), \(_{a=1}^{k}(G(a,_{ax})_{b}-G(a,_{ax})_{c})=_{bx}- _{cx}\) for all \(x[d]\). For example, we can take \(G=G_{0}\) defined by \(G_{0}(a,)_{b}=_{e_{}(b)}w_{e}(a,)\) for \(a\), where \(\) is a tree over \(\) induced by neighborhood relations and \(_{}(b)\) is the set of edges from \(b\) to an arbitrarily chosen root \(c\) on \(\). See Appendix C and [31, Chapter 37] for a more detailed explanation and background of PM.

### Considered regimes

We consider three regimes on the assumptions for losses in MABs and outcomes in PM. In the _stochastic regime_, a sequence of loss vector \((_{t})\) in MAB and that of outcome vector \((x_{t})\) in PM follow an unknown distribution \(^{*}\) in an i.i.d. manner. Define the minimum suboptimality gap in \(_{}=_{a a^{*}}_{a}\) for \(_{a}=_{_{t}^{*}}(_{ta}-_{ta^{*}}) \) in MAB and \(_{a}=_{x_{t}^{*}}(_{a}-_{a^{*}})^{}e _{x_{t}}\) in PM. Note that the definitions of \(\) in MAB and PM are different.

In contrast, the _adversarial regime_ does not assume any stochastic structure for the losses or outcomes, and they can be chosen in an arbitrary manner. In this regime, the environment can choose \(_{t}\) for MAB and \(x_{t}\) for PM depending on the past history until the \((t-1)\)-th round, \((A_{s})_{s=1}^{t-1}\).

We also consider, the _adversarial regime with a self-bounding constraint_, an intermediate regime between the stochastic and adversarial regimes.

**Definition 1**.: _Let \(^{k}\) and \(C 0\). The environment is in an adversarial regime with a \((,C,T)\) self-bounding constraint if it holds for any algorithm that \(_{T}_{t=1}^{T}_{A_{t}}-C\)._

One can see that the stochastic and adversarial regimes are indeed instances of this regime, and that well-known _stochastic regimes with adversarial corruptions_ are also in this regime (see  and  for definitions in MAB and PM, respectively).

We assume that there exists a unique optimal arm (or action) \(a^{*}\), which was employed by many studies aiming at developing BOBW algorithms [18; 34; 49; 53].

## 3 Preliminaries

This section provides preliminaries for developing and analyzing algorithms. We first introduce FTRL, upon which we develop our algorithms, and then describe the self-bounding technique, which is a common technique for proving a BOBW guarantee.

Follow-the-regularized-leaderIn the FTRL framework, an arm selection probability \(p_{t}_{k}\) at round \(t\) is given by

\[q_{t}=*{arg\,min}_{q_{k}}_{s=1}^{t -1}_{s},\,q+_{t}(q) p_{t}= _{t}(q_{t})\,,\] (1)

where \(_{t}^{k}\) is an estimator of loss \(_{t}\) at round \(t\), \(_{t}_{k}\) is a strongly-convex regularizer, and \(_{t}_{k}_{k}\) is a map from the output of FTRL \(q_{t}\) to an arm selection probability vector \(p_{t}\).

In the analysis of FTRL, it is common to evaluate \(_{t=1}^{T}_{t},p_{t}-p=_{t=1}^{T} _{t},q_{t}-p+_{t=1}^{T} _{t},p_{t}-q_{t}\) for some \(p_{k}\). It is known (see _e.g.,_[31; Exercise 28.12]) that quantity \(_{t=1}^{T}_{t},q_{t}-p\) is bounded from above by

\[_{t=1}^{T}((q_{t+1})-_{t+1}(q_{t+1})}_{})+_{T+1}(p)-_{1}(q_{1})+_{t=1}^{T}(-q_{t+1}, _{t}-D_{_{t}}(q_{t+1},q_{t})}_{})\,.\] (2)

We refer to the terms in (2) as a _penalty_ and _stability_ terms, and to the quantity \(_{t},p_{t}-q_{t}\) as a _transformation_ term. Note that, though this study focuses on examples in which \(_{T+1}(p)\) is not dominant, this term may be dominant dependent on the choice of regularizers.

Self-bounding techniqueA self-bounding technique is a common method for proving a BOBW guarantee [18; 49; 53]. In the self-bounding technique, we first derive regret upper and lower bounds in terms of a variable dependent on the arm selection probabilities \((p_{t})_{t}\) or the FTRL outputs \((q_{t})_{t}\), and then derive a regret bound by combining the upper and lower bounds. We use a version proposed in . We consider \(Q(i)\), \((i)\), \(P(i)\), and \((i)\) for \(i[k]\) defined by \(Q(i)=_{t=1}^{T}(1-q_{ti}),\)\((i)=[Q(i)],\)\(P(i)=_{t=1}^{T}(1-p_{ti}),\) and \((i)=[P(i)].\) Note that \((i),(i)[0,T]\) for any \(i[k]\). In terms of \((i)\) or \((i)\), we can obtain the lower bound of the regret for the adversarial regime with a self-bounding constraint as follows:

**Lemma 1** ([46, Lemma 4]).: _In the adversarial regime with a self-bounding constraint (Definition 1), if there exists \(c^{}(0,1]\) such that \(p_{ti} c^{}\,q_{ti}\) for all \(t[T]\) and \(i[k]\), then \(_{T}_{}(a^{*})-C c^{}\,_{} (a^{*})-C\,.\)_

It is known that the sums of the entropy \(H()\) of \((p_{t})_{t}\) is bounded by \(P(i)\) as follows:

**Lemma 2** ([22, Lemma 4]).: _Let \((q_{t})_{t=1}^{T}\) be any sequence of probability vectors and define \(Q(i)=_{t=1}^{T}(1-q_{ti})\). Then for any \(i[k]\), \(_{t=1}^{T}H(q_{t}) Q(i)(kT/Q(i))\)._

Based on Lemmas 1 and 2, it suffices to show \(_{T}^{T}H(q_{t})\, (T)}\) to prove a BOBW guarantee in MAB. This is because, for the adversarial regime, using \(H(q_{t}) k\) immediately implies a \(()\) bound, and for the stochastic regime, using Lemmas 1 and 2 roughly bounds the regret as \(_{T}=2_{T}-_{T})\, (T)}-_{}(a^{*})(T)/ _{}\).

## 4 Stability-penalty-adaptive (SPA) learning rate and regret bound

This section proposes a new adaptive learning rate, which yields a regret upper bound dependent on both the stability component \(z_{t}\) and penalty component \(h_{t}\) for various choices of \(z_{t}\) and \(h_{t}\). When we use a learning rate \(_{t}\), the analysis of FTRL boils down to the evaluation of

\[}_{T}^{}=_{t=1}^{T}(}-})h_{t+1}+_{t=1}^{T}_{t}z_{t} >0\,.\] (3)

In particular, when we use the Exp3 algorithm, \(h_{t}\) is the Shannon entropy of the FTRL output at round \(t\). This can be confirmed by checking the existing studies (_e.g.,_) or the proofs in Appendices F, G, H.2, and I. To favorably bound \(}_{T}^{}\), we develop a new learning rate framework, which we call the jointly stability- and penalty-adaptive learning rate, or the _stability-penalty-adaptive (SPA) learning rate_ for short:

**Definition 2** (Stability-penalty-adaptive learning rate).: _Let \(((h_{t},z_{t},_{t}))_{t=1}^{T}\) be non-negative reals such that \(h_{1} h_{t}\) for all \(t[T]\), \((_{t}h_{1}+_{s=1}^{t}z_{s}h_{s+1})_{t=1}^{T}\) is non-decreasing, and \(_{t}h_{1} z_{t}h_{t+1}\) for all \(t[T]\). Let \(c_{1},c_{2}>0\). Then, a sequence of \((_{t})_{t=1}^{T}\) is a SPA learning rate if it has a form of_

\[_{t}=}\,,_{1}>0\,,_{ t+1}=_{t}+z_{t}}{+_{t}h_{1}+_{s=1}^{t-1}z_{s}h _{s+1}}}\,.\] (4)

**Remark**.: To the best of our knowledge, this is the first learning rate that depends on both the stability and penalty components. Note that when we set the penalties to their worst-case value, that is, \(h_{t}=h_{1}\) for all \(t[T]\) (recalling \(h_{t} h_{1}\)), the SPA learning rate in (4) becomes equivalent to the standard type of the learning rate, which depends only on the stability and has the form of \(_{t}=1/_{t}}{}}_{1}+_{s= 1}^{t-1}z_{s}}\). On the other hand, when we set the stabilities to be their worst-case value, that is, \(z_{t[T]}z_{t}\), the SPA learning rate in (4) corresponds to the learning rate dependent only on the penalty in .

Using learning rate \((_{t})\) in (4), we can bound \(}_{T}^{}\) as follows.

**Theorem 1** (Stability-penalty-adaptive regret bound).: _Let \((_{t})_{t=1}^{T}\) be a SPA learning rate in Definition 2. Then \(}_{T}^{}\) in (3) is bounded as follows:_

**(I)** _If \(((h_{t},z_{t},_{t}))_{t=1}^{T}\) in \((_{t})\) satisfies \(+_{t}h_{1}}}{c_{1}}(_{1}+_{t})+ z_{t}\) for all \(t[T]\) for some \(>0\) (stability condition (S1)), then_

\[}_{T}^{} 2(c_{1}+} (1+_{u=1}^{T}}{}))+ _{T}h_{1}+_{t=1}^{T}z_{t}h_{t+1}}\,.\] (5)

**(II)** _If \(h_{t}=h_{1}\) for all \(t[T]\), \(c_{2}=0\), and \(((h_{t},z_{t},_{t}))_{t=1}^{T}\) in \((_{t})\) satisfies \(_{t}}{}}^{t}z_{s}}\) for some \(a>0\) (stability condition (S2)), then \(}_{T}^{} 2(c_{1}+} )_{t=1}^{T}z_{t}}\)._The proof of Theorem 1 can be found in Appendix D. In Part (I) of Theorem 1, we can see that \(}_{T}^{}\) is bounded by \(^{T}z_{t}h_{t+1}}\), which will enable us to obtain BOBW and data-dependent bounds simultaneously. Note that \(}_{T}^{}\), which is the component of the regret, often becomes dominant in particular when we use the Shannon entropy regularizer. Thus, checking the stability condition (S1) and applying Theorem 1 to bound \(}_{T}^{}\) almost complete the regret analysis. In Section 6, we will see that applying Theorem 1 immediately provides a BOBW bound with a game-dependent bound for PM. In contrast, when deriving BOBW with a sparsity-dependent bound for MAB in Section 5, we will develop additional techniques and conduct further analysis, for example, to satisfy the stability condition (S1), making use of the time-invariant log-barrier regularizer.

## 5 Sparsity-dependent bounds in multi-armed bandits

This section establishes several sparsity-dependent bounds. We use the FTRL framework in (1) with the inverse weighted estimator \(_{t}^{k}\) given by \(_{t_{i}}=_{t_{i}}[A_{t}=i]/p_{t_{i}}\). This estimator is common in the literature and is useful for its unbiasedness, _i.e._, \(_{A_{t} p_{t}}[_{t}\,|\,p_{t}]=_{t}\). We first propose algorithms that achieve sparsity-dependent bounds using substibility-dependent learning rates in Section 5.1 as preliminaries for the subsequent section. Following that, in Section 5.2, we establish a BOBW algorithm with a sparsity-dependent bound based on the SPA learning rate. More specific steps are summarized as follows.

* Section 5.1.1 discusses the case of \(_{t}^{k}\) and shows that appropriately choosing \(z_{t}\) in the SPA learning rate (4) with the Shannon entropy regularizer and \(((kT)^{-2/3})\) uniform exploration achieves a \(O( k})\) regret for \(_{t}^{k}\) without knowing \(L_{2}\).
* Section 5.1.2 considers the case of \(_{t}[-1,1]^{k}\), which is known to be more challenging than \(_{t}^{k}\). We show that the time-invariant log-barrier enables us to choose a "tighter" \(z_{t}\) in (4), which removes the uniform exploration used in Section 5.1.1. This not only results in the bound of \(O( k})\) for \(_{t}[-1,1]^{k}\) but also becomes one of the key properties to achieve BOBW.
* Section 5.2 presents a BOBW algorithm with a sparsity-dependent bound using the technique developed in Section 5.1 and Theorem 1. While Theorem 1 itself is a strong tool leading directly to the result for PM (Section 6), its application does not lead to the desired bounds. In particular, in this setting the \(^{T}z_{t}h_{t+1}}\) term derived through Theorem 1 does not immediately imply a BOBW guarantee with a sparsity-dependent bound. To solve this problem, we develop a novel technique to analyze _the variation in FTRL outputs \(q_{t}\) in response to the change in a regularizer (Lemma 7)_, and prove a BOBW bound with a sparsity-dependent bound of \(O( k T})\).

### Parameter-agnostic sparsity-dependent bounds

This section establishes \(s\)-agnostic algorithms to achieve sparsity-dependent bounds for the adversarial regime, which are preliminaries for Section 5.2.

5.1.1 \(L_{2}\)-agnostic algorithm with \(O( k})\) bound for \(_{t}^{k}\)

Here, we use \(p_{t}=_{t}(q_{t})\) for \(_{t}(q)=(1-)q+\) and \(=( k)^{1/3}}{T^{2/3}}\) and assume \([0,1/2]\) (this holds when \(T\)), which implies \(2p_{t} q_{t}\). We use the Shannon entropy regularizer \(_{t}(p)=-}H(p)=}^{}(p)= }_{i=1}^{k}p_{i} p_{i}\) with learning rate \(_{t}=1/_{t}\) and

\[_{1}=}{}}}\,,_{t+1 }=_{t}+_{t}}{+_{s =1}^{t-1}_{s}}}_{t} }^{2}}{p_{tA_{t}}}\,,\] (6)

which corresponds to the learning rate in Definition 2 with \(h_{t} H(q_{1})= k\), \(z_{t}_{t}\), \(_{t} k/\), and \(c_{2} 0\). The uniform exploration is used to satisfy stability condition (S2) in Theorem 1, the amount of which is determined by balancing the regret coming from the uniform exploration and stability condition (S2). Theorem 1 immediately gives the following bound.

**Corollary 2**.: _When \(T\), the above algorithm with \(c_{1}=1/\) achieves \(_{T} 2 k}+(2+1)(kT k)^{1/3}\) without knowing \(L_{2}\). In particular, when \(T 7k^{2}/s^{3}\), \(_{T}(4+1)\)._

The proof is given in Appendix F. The most striking feature of the algorithm is its \(L_{2}\) (or \(s\))-agnostic property. This is essentially made possible by the learning rate using the data-dependent quantity \(_{t}\) in (6), which satisfies \(^{T}_{t}} ^{T}[_{t}]}=^{T}_{i=1}^{k}_{ti}^{2}}= }.\) The leading constant of the bound is better than the existing bounds, as shown in Table 1, despite its agnostic property. Note that while the first-order bound in  implies the above sparsity-dependent bound when \(_{t}^{k}\), this does not hold when \(_{t}[-1,1]\), which we investigate in the following (see Appendix K for details). We will see in Section 5.1.2 that this assumption can be totally removed by adding a time-invariant log-barrier regularization.

1.2 \(L_{2}\)-agnostic algorithm with \(O( k}+k T)\) bound for \(_{t}[-1,1]^{k}\)

Here, we consider the case of \(_{t}[-1,1]^{k}\). It is worth noting that the negative loss cannot be handled by simply shifting the loss since it removes the sparsity from the losses \((_{t})\); see [10; 27] and Appendix K for further details. We directly use the output \(q_{t}\) as \(p_{t}\), that is, \(p_{t}=q_{t}.\) We use the hybrid regularizer consisting of the negative Shannon entropy and the log-barrier function, \(_{t}(p)=}^{}(p)+2^{}(p),\) where \(^{}(p)=_{i=1}^{k}(1/x_{i})\). We use learning rate \(_{t}=1/_{t}\) and

\[_{1}=^{2}}{8h_{1}}\,,_{t+1}=_{t}+ _{t}}{+_{s=1}^{t-1}_{s}}} _{t}_{t}\{1,}}{2_{t}} \}\,,\] (7)

where \(_{t}\) is defined in (6). Learning rate (7) corresponds to that in Definition 2 with \(h_{t} H(q_{1})= k\), \(z_{t}_{t}\), \(_{t}_{t}\), and \(c_{2} 0\). We then have the following bound:

**Corollary 3**.: _If we run the above algorithm with \(c_{1}=\), \(_{T} 4 k}+2k T+k+1/4,\) which implies that \(_{T} 4+2k T+k+1/4.\)_

The proof is given in Appendix G. Corollary 3 removes the assumption of \(T 7k^{2}/s^{3}\) in Corollary 2, and it also improves the leading constant of the regret in . Note that one can prove a bound of the same order, but with a worse leading constant, by setting \(_{1} 15k\) and combining the analysis similar to that in Section 5.1.1 and the stability bound in . We successfully remove the assumption of \(T 7k^{2}/s^{3}\) thanks to the following lemma, which serves as one of the key elements in achieving a BOBW guarantee with a sparsity-dependent bound (The proof is given in Appendix G.):

**Lemma 3** (Stability bound for negative losses).: _Let \(_{t}[-1,1]^{k}\) and \(_{ti}=_{ti}[A_{t}=i]/p_{ti}\) be the inverse weighted estimator. Assume that \(q_{t} p_{t}\) for some \( 1\). Then the stability term of FTRL with the hybrid regularizer \(_{t}=}^{}+2\,^{}\) is bounded as_

\[ q_{t}-q_{t+1},_{t}-D_{_{t}}(q_{t+1},q_{t}) _{t}}^{2}}{p_{tA_{t}}}\{1,}}{2_{t}}\}=_{t}_{t}\,.\]

**Remark.** We can observe from Lemma 3 that the stability term is bounded in terms of \(_{t}\), and the most important observation is that this \(_{t}\) is bounded by the inverse of the learning rate \(1/(2_{t})=_{t}/2\), _i.e.,_\(_{t}_{t}/2\). This enables us to guarantee the stability condition (S2) in Theorem 1 without needing to mix the \(((kT)^{-2/3})\) uniform exploration used in Section 5.1.1. Moreover, this will be a key property to prove a BOBW with a sparsity-dependent bound in the next section.

As a minor contribution, by directly bounding the stability component, the RHS of Lemma 3 has a smaller leading constant than the bound obtained by using the bound in .

### Best-of-both-worlds guarantee with sparsity-dependent bound

Finally, we are ready to establish a BOBW algorithm with a sparsity-dependent bound and derive its regret bound. We use \(p_{t}=_{t}(q_{t})=(1-)q_{t}+\) with \(=\) (_i.e.,_\((1/T)\) uniform exploration)and assume \([0,1/2]\), which implies \(2p_{t} q_{t}\) and \(_{t} T\). We use the hybrid regularizer \(_{t}=}^{}+4^{}\) with learning rate \(_{t}=1/_{t}\) and

\[_{1}=15k\,,\ \ _{t+1}=_{t}+_{t}}{^{2} +_{t}h_{t+1}+_{s=1}^{t-1}_{s}h_{s+1}}}\ \ \ \ h_{t}=}H(p_{t})\,,\] (8)

where \(_{t}\) is defined in (7). Note that this learning depends on both the stability and penalty components. This corresponds to the SPA learning rate in Definition 2 with \(z_{t}_{t}\), \(_{t}_{t}h_{t+1}/h_{1}\), and \(c_{2} 81c_{1}^{2}\). One can see that stability assumption (S1) in Part (I) of Theorem 1 are satisfied thanks to \(_{t}_{t}\) (see Appendix H for the proof). We then have the following bounds.

**Theorem 4** (BOBW with sparsity-dependent bound).: _Suppose that \(T 2k\). Then the above algorithm with \(c_{1}=)}}\) (Algorithm 2 in Appendix H) achieves_

\[_{T} 4(k)(1+T)}+O(k)\]

_in the adversarial regime,_

\[_{T}=O}+}}\]

_in the adversarial regime with a \((,C,T)\) self-bounding constraint, and \(_{T}=O([_{i=1}^{k}_{ti}^{2}](T)(kT)/ _{})\) in the stochastic regime._

The proof is given in Appendix H. This is the first BOBW bound with the sparsity-dependent (\(L_{2}\)-dependent) bound. Note that the bounds in the stochastic regime and the adversarial regime with a self-bounding constraint can also exploit the propoerty of the underlying losses. The bound in the stochastic regime is suboptimal in two respects: its dependence on \(_{}\) and \(( T)^{2}\). Concurrently, two separate studies improve each suboptimality ( for \(_{}\) and  for \(( T)^{2}\)), but it is highly uncertain if we can prove a BOBW with _a sparsity-dependent bound_ based on their approach, and it is an important future work to investigate this problem.

Key elements of the proofIn the following, we describe some key elements of the proof of Theorem 4. We need to solve one remaining technical issue. Using Part (I) of Theorem 1, we can show that the regret is roughly bounded by \(^{T}_{t}h_{t+1}}^{T}[_{t}h_{t+1}]}\). However, this quantity cannot be straightforwardly bounded since \(h_{t+1}\) depends on \(_{t}\).

To address this issue, we analyze the behavior of arm selection probabilities when the regularizer changes. In particular, we first prove in Lemma 7 that \(h_{t+1} h_{t}+k(_{t+1}/_{t}-1)h_{t+1}\). This lemma can be proven by a novel analysis evaluating the changes of the FTRL outputs when the learning rate varies (given in Appendices H.1 and H.2) which is not considered and required when we use a time-invariant learning rate (_e.g._, ). Using the last inequality, we have \(^{T}[_{t}h_{t+1}]}^{T} [_{t}h_{t}]}+k_{t=1}^{T}[_{t}(_{t+1}/ _{t}-1)h_{t+1}]^{T}[_{t}h_{t} ]}+k_{t=1}^{T}^{T}_{t}h_{t+1}} ^{T}[_{t}h_{t}]}+k_{t=1}^{T} ^{T}_{t}h_{t+1}}^{T} [_{t}h_{t}]}+k,\) which holds thanks again to \(_{t}_{t}\) and based on the fact that \(x\) for \(a,b,x>0\) implies \(x+a\) (here we ignore some logarithmic factors). This combined with the self-bounding technique leads to a BOBW guarantee in the stochastic regime.

ImplementationOne may wonder how to compute \(_{t+1}\) satisfying (8) since \(h_{t+1}=h_{t+1}(_{t+1})\) depends on \(_{t+1}\). In fact, this can be computed by defining \(F_{t}:[_{t},_{t}+T]\) as \(F_{t}()=-_{t}+c_{1}_{t}/^{2}+_{t}h_{t +1}()+_{s=1}^{t-1}_{s}h_{s+1}}\) and setting \(_{t+1}\) to be a root of \(F_{t}()=0\). Such \(\) can be computed using the bisection method because \(F_{t}\) is continuous (proved in Proposition 2 in Appendix H.3). The detailed discussion can be found in Appendix H.3.

## 6 Best-of-both-worlds with game-dependent bound for partial monitoring

This section discusses the result of a BOBW guarantee with a game-dependent bound for PM. The desired bound is obtained by direct application of the SPA learning rate and Theorem 1, which highlights the usefulness of the SPA learning rate. Due to the space constraints, the background and algorithm are given in Appendix B.

We rely on the Exploration by Optimization (EbO) by Lattimore and Szepesvari . Define

\[(p,G;q_{t},_{t})=_{x[d]}[)^{} e_{x}}{_{t}}+_{q_{t}}(G;x)}{_{t}}+ ^{2}}_{a=1}^{k}p_{a}_{q_{t}}(G(a, _{ax})}{p_{a}})\,],\] (9)

where \(_{t}\) is a learning rate, \(_{q}(z)= q,(-z)+z-1\) and \(_{q}(G;x)= q,\,e_{x}-_{a=1}^{k}G(a,_{ax })+_{c}(_{a=1}^{k}G(a,_{ax})_{c}-_{cx})\) is the bias of the estimator. Note that the first and third terms in (9) correspond to the stability and transformation terms. Then in EbO we choose \((p_{t},G_{t})_{k}\) by

\[(p_{t},G_{t})=*{arg\,min}_{p_{k}^{}(q_{t}), \,G}(p,G;q_{t},_{t})_{k}^{}(q)=\{p_{k} p q/(2k)\}\,\] (10)

where \(_{k}^{}(q)\) is the feasible set proposed in . Let \(_{q_{t}}^{}(_{t})\) be the optimal value of the optimization problem and \(V_{t}^{}=\{0,_{q_{t}}^{}(_{t})\}\) be its truncation.

**Remark.** It is worth noting that \(V_{t}^{}=\{0,_{q_{t}}^{}(_{t})\}\) captures game difficulty the learner is facing as discussed in . As discussed above, \((p,G)\) in (9), the objective function that determines \(_{q_{t}}^{}(_{t})\), correspond to the components of the regret upper bound of FTRL. Hence, the smaller \((p,G)\) can become by optimizing \(p\) and \(G\), the smaller the regret can become, and thus \(_{q_{t}}^{}(_{t})\) captures the game difficulty.

Now we are ready to state our result.

**Corollary 5**.: _For any non-degenerate PM games, there exists an algorithm based on a SPA learning rate achieving \(_{T}=O^{T}V_{t}^{} (k)(1+T)}+mk^{2}\) in the adversarial regime, and \(_{T}=O(m^{2}k^{4}(T)(kT)/_{}+k^{4} (T)(kT)/_{}}+mk^{2})\) in the adversarial regime with a \((,C,T)\) self-bounding constraint._

An extended result and the proof are given in Appendices B and I, respectively. Recall that \(V_{t}^{}=\{0,_{q_{t}}^{}(_{t})\}\) in the bound reflects the difficulty of the game the learner is facing, rather than the worst-case difficulty of the class of the game. The bounds in both regimes are optimal up to logarithmic factors, and further detailed comparisons are given in Table 2 and Appendix B.

## 7 Conclusion and future work

In this paper, we established the stability-penalty-adaptive (SPA) learning rate (Definition 2), which provides the regret upper bound that jointly depends on the stability and penalty components of FTRL (Theorem 1). This learning rate combined with the technique and analysis for bounding stability terms allows us to achieve BOBW and data-dependent bounds (sparsity- and game-dependent bounds) simultaneously in MAB and PM.

There are some remaining questions. First of all, it would be important future direction to apply the SPA learning rate to other online decision-making problems or regularizers. For example, it is as to investigate online learning with feedback graphs , in which the Shannon entropy regularizer (or the Tsallis entropy with the exponent larger than \(1-1/ k\)) is necessary to achieve nearly optimal regret bounds. Another interesting example is to employ the Tsallis entropy as a dominant regularizer in the SPA learning rate, for instance, to improve logarithmic dependences on the regret bounds, while in this paper we only focused on the Shannon entropy. Second, it is an open question whether we can achieve the bound of \(O()\) in the sparsity-dependent bound without knowing the sparsity level \(s\). Finally, while we only considered PM games with local observability, investigating if the game-dependent bound with the BOBW guarantee is possible for PM with global observability is important future work.