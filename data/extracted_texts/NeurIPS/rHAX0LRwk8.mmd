# Adversarial Counterfactual Environment Model Learning

Xiong-Hui Chen\({}^{1,2,4,}\), Yang Yu\({}^{1,2,4,*,}\), Zheng-Mao Zhu\({}^{1,2}\), Zhihua Yu\({}^{1,2,3}\), Zhenjun Chen\({}^{1,2,3}\),

**Chenghe Wang\({}^{1,2}\), Yinan Wu\({}^{3}\), Hongqiu Wu\({}^{1,2}\), Rong-Jun Qin\({}^{1,2,4}\),Ruijin Ding\({}^{5}\), Fansheng Huang\({}^{3}\)**

\({}^{1}\) National Key Laboratory for Novel Software Technology, Nanjing University

\({}^{2}\) School of Artificial Intelligence, Nanjing University, \({}^{3}\) Meituan, \({}^{4}\) Polixir.ai, \({}^{5}\) Tsinghua University

{chenxh, yuy, zhuzm}@lamda.nju.edu.cn, {yuzh,chenzj}@smail.nju.edu.cn wangch@lamda.nju.edu.cn, wuyinan02@meituan.com, {wuhq,qinrj}@lamda.nju.edu.cn drj17@mails.tsinghua.edu.cn, huangfangsheng@meituan.com

###### Abstract

An accurate environment dynamics model is crucial for various downstream tasks in sequential decision-making, such as counterfactual prediction, off-policy evaluation, and offline reinforcement learning. Currently, these models were learned through empirical risk minimization (ERM) by step-wise fitting of historical transition data. This way was previously believed unreliable over long-horizon rollouts because of the compounding errors, which can lead to uncontrollable inaccuracies in predictions. In this paper, we find that the challenge extends beyond just long-term prediction errors: we reveal that even when planning with one step, learned dynamics models can also perform poorly due to the selection bias of behavior policies during data collection. This issue will significantly mislead the policy optimization process even in identifying single-step optimal actions, further leading to a greater risk in sequential decision-making scenarios. To tackle this problem, we introduce a novel model-learning objective called adversarial weighted empirical risk minimization (AWRM). AWRM incorporates an adversarial policy that exploits the model to generate a data distribution that weakens the model's prediction accuracy, and subsequently, the model is learned under this adversarial data distribution. We implement a practical algorithm, GALILEO, for AWRM and evaluate it on two synthetic tasks, three continuous-control tasks, and _a real-world application_. The experiments demonstrate that GALILEO can accurately predict counterfactual actions and improve various downstream tasks, including offline policy evaluation and improvement, as well as online decision-making.

## 1 Introduction

A good environment dynamics model for action-effect prediction is essential for many downstream tasks. For example, humans or agents can leverage this model to conduct simulations to understand future outcomes, evaluate other policies' performance, and discover better policies. With environment models, costly real-world trial-and-error processes can be avoided. These tasks are vital research problems in counterfactual predictions , off-policy evaluation (OPE) , and offline reinforcement learning (Offline RL) . In these problems, the core role of the models is to answer queries on counterfactual data unbiasedly, that is, given states, correctly answer _what might happen_ if we were to carry out actions _unseen_ in the training data. However, addressing counterfactual queries differentiates environment model learning from standard supervised learning (SL), which directly fits the offline dataset for empirical risk minimization (ERM). In essence,the problem involves training the model on one dataset and testing it on another with a shifted distribution, specifically, the dataset generated by counterfactual queries. This challenge surpasses the SL's capability as it violates the independent and identically distributed (_i.i.d._) assumption .

In this paper, we concentrate on faithful dynamics model learning in sequential decision-making settings like RL. Specifically, we first highlight a distinct situation of distribution shift that can easily lead to _catastrophic failures in the model's predictions_: In many real-world applications, offline data is often gathered using a single policy with exhibiting selection bias, meaning that, for each state, actions are chosen unfairly. As illustrated in Fig. 1(a), to maintain the ball's trajectory along a target line, a behavior policy applies a smaller force when the ball's location is nearer to the target line. When a dataset is collected with such selection bias, the association between the states (location) and actions (force) will make SL hard to identify the correct causal relationship of the states and actions to the next states respectively (see Fig. 1(c)). Then when we query the model with counterfactual data, the predictions might be catastrophic failures. _Finally, offline policy optimization based on this SL model, even for just seeking one-step optimal actions, will select also a totally opposite direction of policy improvement, making the offline policy learning system fail._ The selection bias can be regarded as an instance of the distributional-shift problem in offline model-based RL, which has also received great attention . However, previous methods employing _naive supervised learning_ for environment model learning tend to overlook this issue during the learning process,

Figure 1: An example of selection bias and predictions under counterfactual queries. Suppose a ball locates in a 2D plane whose position is \(s_{t}=(x_{t},y_{t})\) at time \(t\). The ball will move to \(s_{t+1}=(x_{t+1},y_{t+1})\) according to \(x_{t+1}=x_{t}+1\) and \(y_{t+1}(y_{t}+a_{t},2)\). Here, \(a_{t}\) is chosen by a control policy \(a_{t}_{}(a|s_{t})=((-y_{t})/15,0.05)\) parameterized by \(\), which tries to keep the ball near the line \(y=\). In Fig. 1(a), the behavior policy \(\) is \(_{62.5}\). Fig. 1(b) shows the collected training data and the learned models’ prediction of the next position of \(y\). Besides, the dataset superficially presents the relation that the corresponding next \(y\) will be smaller with a larger action. However, the truth is not because the larger \(a_{t}\) causes a smaller \(y_{t+1}\), but the policy selects a small \(a_{t}\) when \(y_{t}\) is close to the target line. **Mistakenly exploiting the “association” will lead to local optima with serious factual errors**, e.g., believed that \(y_{t+1}_{}^{-1}(y_{t}|a)+a_{t}-14a_{t}\), where \(_{}^{-1}\) is the inverse function of \(_{}\). When we estimate the response curves by fixing \(y_{t}\) and reassigning action \(a_{t}\) with other actions \(a_{t}+ a\), where \( a[-1,1]\) is a variation of action value, we found that the model of SL indeed exploit the association and give opposite responses, while in AWRM and its practical implementation GALILEO, the predictions are closer to the ground truths (\(y_{t+1} y_{t}+a_{t}\)). The result is in Fig. 1(c), where the darker a region is, the more samples are fallen in. AWRM injects data collected by adversarial policies for model learning to eliminate the unidentifiability between \(y_{t+1}_{}^{-1}(y_{t}|a)+a_{t}\) and \(y_{t+1} y_{t}+a_{t}\) in offline data.

Figure 2: An illustration of the prediction error in counterfactual datasets. The error of SL is small only in training data (\(=62.5\)) but becomes much larger in the dataset “far away from” the training data. AWRM-oracle selects the oracle worst counterfactual dataset for training for each iteration (pseudocode is in Alg. 2) which reaches small MSE in all datasets and gives correct response curves (Fig. 1(c)). GALILEO approximates the optimal adversarial counterfactual data distribution based on the training data and model. Although the MSE of GALILEO is a bit larger than SL in the training data, in the counterfactual datasets, the MSE is on the same scale as AWRM-oracle.

addressing it instead by limiting policy exploration and learning in high-risk regions. So far, how to learn a faithful environment model that can alleviate the problem directly has rarely been discussed.

In this work, we focus on faithful environment model learning techniques. The work is first inspired by weighted empirical risk minimization (WERM), which is a typical solution to solve the selection bias problem in causal inference for individual treatment effects (ITEs) estimation in many scenarios like patients' treatment selection [26; 1; 41]. ITEs measure the effects of treatments on individuals by administering treatments uniformly and evaluating the differences in outcomes. To estimate ITEs from offline datasets with selection bias, they estimate an inverse propensity score (IPS) to reweight the training data, approximating the data distribution under a uniform policy, and train the model under this reweighted distribution. Compared with ITEs estimation, the extra challenge of faithful model learning in sequential decision-making settings include: (1) the model needs to answer queries on numerous different policies, resulting in various and unknown target data distributions for reweighting, and (2) the IPS should account for the cumulative effects of behavior policies on state distribution rather than solely focusing on bias of actions. To address these issues, we propose an objective called adversarial weighted empirical risk minimization (AWRM). For each iteration, AWRM employs adversarial policies to construct an adversarial counterfactual dataset that maximizes the model's prediction error, and drive the model to reduce the prediction risks under the adversarial counterfactual data distribution. However, obtaining the adversarial counterfactual data distribution is infeasible in the offline setting. Therefore, we derive an approximation of the counterfactual data distribution queried by the optimal adversarial policy and provide a tractable solution to learn a model from the approximated data distribution. As a result, we propose a practical approach named **G**enerative **A**dversarial off**L**he counterfactual**L **E**nvironment **m**O**del learning (GALILEO) for AWRM. Fig. 2 illustrates the difference in the prediction errors learned by these algorithms.

Experiments are conducted in two synthetic tasks, three continuous-control tasks, and _a real-world application_. We first verify that GALILEO can make accurate predictions on counterfactual data queried by other policies compared with baselines. We then demonstrate that the model learned by GALILEO is helpful to several downstream tasks including offline policy evaluation and improvement, and online decision-making in a large-scale production environment.

## 2 Preliminaries

We first introduce weighted empirical risk minimization (WERM) through inverse propensity scoring (IPS), which is commonly used in individualized treatment effects (ITEs) estimation . It can be regarded as a scenario of single-step model learning. We define \(M^{*}(y|x,a)\) as the one-step environment, where \(x\) denotes the state vector containing pre-treatment covariates (such as age and weight), \(a\) denotes the treatment variable which is the action intervening with the state \(x\), and \(y\) is the feedback of the environment. When the offline dataset is collected with a behavior policy \((a|x)\) which has selection bias, a classical solution to handle the above problem is WERM through IPS \(\)[48; 3; 29]:

**Definition 2.1**.: The learning objective of WERM through IPS is formulated as

\[_{M}_{x,a,y p^{}_{M^{*}}}[(x,a)( M(y|x,a),y)],\] (1)

where \((x,a):=\), \(\) and \(\) denote the policies in testing and training domains, and the joint probability \(p^{}_{M^{*}}(x,a,y):=_{0}(x)(a|x)M^{*}(y|x,a)\) in which \(_{0}(x)\) is the distribution of state. \(\) is the model space. \(\) is a loss function.

The \(\) is also known as importance sampling (IS) weight, which corrects the sampling bias by aligning the training data distribution with the testing data. By selecting different \(\) to approximate \(\) to learn the model \(M\), current environment model learning algorithms employing reweighting are fallen into the framework. For standard ITEs estimation, \(=}\) (i.e., \(\) is a uniform policy) for balancing treatments, where \(\) is an approximated behavior policy \(\). Note that it is a reasonable weight in ITEs estimation: ITEs are defined to evaluate the differences of effect on each state under a uniform policy.

In sequential decision-making setting, decision-making processes in a sequential environment are often formulated into Markov Decision Process (MDP) . MDP depicts an agent interacting with the environment through actions. In the first step, states are sampled from an initial state distribution \(x_{0}_{0}(x)\). Then at each time-step \(t\{0,1,2,...\}\), the agent takes an action \(a_{t}\) through a policy \((a_{t}|x_{t})\) based on the state \(x_{t}\), then the agent receives a reward \(r_{t}\) from a reward function \(r(x_{t},a_{t})\) and transits to the next state \(x_{t+1}\) given by a transition function \(M^{*}(x_{t+1}|x_{t},a_{t})\) built in the environment. \(\), \(\), and \(\) denote the policy, state, and action spaces.

## 3 Related Work

We give related adversarial algorithms for model learning in the following and leave other related work in Appx. F. In particular, ITEs in Rubin causal model  and causal effect estimation in structural causal model  attracted widespread attention in recent years [56; 55; 58; 7]. GANTIE  uses a generator to fill counterfactual outcomes for each data pair and a discriminator to judge the source (treatment group or control group) of the filled data pair. The generator is trained to minimize the output of the discriminator.  propose SCIGAN to extend GANITE to continuous ITEs estimation via a hierarchical discriminator architecture. In real-world applications, environment model learning based on Generative Adversarial Imitation Learning (GAIL) has also been adopted for sequential decision-making problems . GAIL is first proposed for policy imitation , which uses the imitated policy to generate trajectories by interacting with the environment. The policy is learned with the trajectories through RL which maximizes the cumulative rewards given by the discriminator. [47; 11] use GAIL for environment model learning by regarding the environment model as the generator and the behavior policy as the "environment" in standard GAIL.  further inject the technique into a unified objective for model-based RL, which joints model and policy optimization. Our study reveals the connection between adversarial model learning and the WERM through IPS, where previous adversarial model learning methods can be regarded as partial implementations of GALILEO, explaining the effectiveness of the former.

## 4 Adversarial Counterfactual Environment Model Learning

In this section, we first propose a new offline model-learning objective for sequential decision-making setting in Sec. 4.1; In Sec. 4.2, we give a surrogate objective to the proposed objective; Finally, we give a practical solution in Sec. 4.3.

### Problem Formulation

In scenarios like offline policy evaluation and improvement, it is crucial for the environment model to have generalization ability in counterfactual queries, as we need to query accurate feedback from \(M\) for numerous different policies. Referring to the formulation of WERM through IPS in Def. 2.1, these requirements necessitate minimizing counterfactual-query risks for \(M\) under multiple unknown policies, rather than focusing on _a specific target policy_\(\). More specifically, the question is: If \(\) is unknown and can be varied, how can we generally reduce the risks in counterfactual queries? In this article, we call the model learning problem in this setting "counterfactual environment model learning" and propose a new objective to address the issue. To be compatible with multi-step environment model learning, we first define a generalized WERM through IPS based on Def. 2.1:

**Definition 4.1**.: In an MDP, given a transition function \(M^{*}\) that satisfies \(M^{*}(x^{}|x,a)>0, x, a,  x^{}\) and \(\) satisfies \((a|x)>0, a, x\), the learning objective of generalized WERM through IPS is:

\[_{M}_{x,a,x^{}_{M^{*}}^{}}[ (x,a,x^{})_{M}(x,a,x^{})],\] (2)

where \((x,a,x^{})=}^{}(x,a,x^{})}{_{M^{*} }^{}(x,a,x^{})}\), \(_{M^{*}}^{}\), and \(_{M^{*}}^{}\). the training and testing data distributions collected by policy \(\) and \(\) respectively. We define \(_{M}(x,a,x^{}):=(M(x^{}|x,a),x^{})\) for brevity.

In an MDP, for any given policy \(\), we have \(_{M^{*}}^{}(x,a,x^{})=_{M^{*}}^{}(x)(a|x)M^{*}(x^{ }|x,a)\) where \(_{M^{*}}^{}(x)\) denotes the occupancy measure of \(x\) for policy \(\). This measure can be defined as \((1-)_{x_{0}_{0}}[_{t=0}^{}^{t}(x_{t}=x|x_{0},M^{*})]\)[51; 25] where \(^{}[x_{t}=x|x_{0},M^{*}]\) is the discounted state visitation probability that the policy \(\) visits \(x\) at time-step \(t\) by executing in the environment \(M^{*}\) and starting at the state \(x_{0}\). Here \(\) is the discount factor. We also define \(_{M^{*}}^{}(x,a):=_{M^{*}}^{}(x)(a|x)\) for simplicity.

With this definition, \(\) can be rewritten: \(=}^{}(x)(a|x)M^{*}(x^{}|x,a)}{_{M^{* }}^{}(x)(a|x)M^{*}(x^{}|x,a)}=}^{}(x,a)}{ _{M^{*}}^{}(x,a)}\). In single-step environments, for any policy \(\), \(_{M^{*}}^{}(x)=_{0}(x)\). Consequently, we obtain \(=(x)(a|x)}{_{0}(x)(a|x)}=\), and the objective degrade to Eq. (1). Therefore, Def. 2.1 is a special case of this generalized form.

**Remark 4.2**.: \(\) is referred to as density ratio and is commonly used to correct the weighting of rewards in off-policy datasets to estimate the value of a specific target policy in off-policy evaluation [35; 60]. Recent studies in offline RL also provide similar evidence through upper bound analysis, suggesting that offline model learning should be corrected to specific target policies' distribution using \(\). We derive the objective from the perspective of selection bias correlation, further demonstrate the necessity and effects of this term.

In contrast to previous studies, in this article, we would like to propose an objective for faithful model learning which can generally reduce the risks in counterfactual queries in scenarios where \(\)_is unknown and can be varied_. To address the problem, we introduce adversarial policies that can _iteratively_ induce the worst prediction performance of the current model and propose to optimize WERM under the adversarial policies. In particular, we propose **A**dversarial **W**eighted empirical **R**isk **M**nimization (AWRM) based on Def. 4.1 to handle this problem.

**Definition 4.3**.: Given the MDP transition function \(M^{*}\), the learning objective of adversarial weighted empirical risk minimization through IPS is formulated as

\[_{M}_{}L(_{M^{*}}^{},M)=_{M }_{}_{x,a,x^{}_{M^{*}}^{ }},[(x,a|_{M^{*}}^{})_{M}(x,a,x^{})],\] (3)

where \((x,a|_{M^{*}}^{})=}^{}(x,a)}{_{M^{*} }^{}(x,a)}\), and the re-weighting term \((x,a|_{M^{*}}^{})\) is conditioned on the distribution \(_{M^{*}}^{}\) of the adversarial policy \(\). In the following, we will ignore \(_{M^{*}}^{}\) and use \((x,a)\) for brevity.

In a nutshell, Eq. (3) minimizes the maximum model loss under all counterfactual data distributions \(_{M^{*}}^{},\) to guarantee the generalization ability for counterfactual queried by policies in \(\).

### Surrogate AWRM through Optimal Adversarial Data Distribution Approximation

The main challenge of solving AWRM is constructing the data distribution \(_{M^{*}}^{^{*}}\) of the best-response policy \(^{*}\) in \(M^{*}\) since obtaining additional data from \(M^{*}\) can be expensive in real-world applications. In this paper, instead of deriving the optimal \(^{*}\), our solution is to offline estimate the optimal adversarial distribution \(_{M^{*}}^{^{*}}(x,a,x^{})\) with respect to \(M\), enabling the construction of a surrogate objective to optimize \(M\) without directly querying the real environment \(M^{*}\).

In the following, we select \(_{M}\) as the negative log-likelihood loss for our full derivation, instantiating \(L(_{M^{*}}^{},M)\) in Eq. (3) as: \(_{x,a_{M^{*}}^{}}[(x,a|_{M^{*}}^{}) _{M^{*}}(- M(x^{}|x,a))]\), where \(_{M^{*}}[]\) denotes \(_{x^{} M^{*}(x^{}|x,a)}[]\). Ideally, for any given \(M\), it is obvious that the optimal \(\) is the one that makes \(_{M^{*}}^{}(x,a)\) assign all densities to the point with the largest negative log-likelihood. However, this maximization process is impractical, particularly in continuous spaces. To provide a tractable yet relaxed solution, we introduce an \(L_{2}\) regularizer to the original objective in Eq. (3).

\[_{M}_{}(_{M^{*}}^{},M)=_{ M}_{}L(_{M^{*}}^{},M)-\| _{M^{*}}^{}(,)\|_{2}^{2},\] (4)

where \(\) denotes the regularization coefficient of \(_{M^{*}}^{}\) and \(\|_{M^{*}}^{}(,)\|_{2}^{2}=_{,} (_{M^{*}}^{}(x,a))^{2}ax\).

Now we present the final results and the intuitions behind them, while providing a full derivation in Appx.A. Suppossing we have \(_{M^{*}}^{^{*}}\) representing the approximated data distribution of the approximated best-response policy \(^{*}\) under model \(M_{}\) parameterized by \(\), we can find the optimal \(^{*}\) of \(_{}_{}(_{M^{*}}^{},M_{})\) (Eq. (4)) through iterative optimization of the objective \(_{t+1}=_{}(_{M^{*}}^{^{*}},M_{ })\). To this end, we approximate \(_{M^{*}}^{^{*}}\) via the last-iteration model \(M_{_{t}}\) and derive an upper bound objective for \(_{}(_{M^{*}}^{^{*}},M_{})\):

\[_{t+1}=_{}_{_{M^{*}}^{}}[(x,a)} M_{}(x^{}|x,a)}}^{_{M_{_{t}}}}(x,a,x^{})}{_{M^{*}}^{ _{M^{*}}}(x,a,x^{})})}_{}-}}^{_{M_{_{t}}}}(x,a)}{_{M^{*}}^{_{M^{*}}}(x,a)} )}_{}+}(x,a)}_{ })}_{W(x,a,x^{})}],\] (5)

where \(_{_{M^{*}}^{}}[]\) denotes \(_{x,a,x^{}_{M^{*}}^{}}[]\), \(f\) is a convex and lower semi-continuous (l.s.c.) function satisfying \(f^{}(x) 0, x\), which is also called \(f\) function in \(f\)-divergence , \(_{0}(x,a)=_{M_{_{t}}}_{M^{*}}^{}(x,a)\), and \(H_{M^{*}}(x,a)\) denotes the entropy of \(M^{*}(|x,a)\).

**Remark 4.4** (Intuition of \(W\)).: After derivation, we found that the optimal adversarial data distribution can be approximated by \(_{M^{*}}^{j^{*}}(x,a)=_{}_{M^{*}}^{}(x,a,x^{ })W(x,a,x^{})x^{}\) (see Appx. A), leading to the upper-bound objective Eq. (5), which is a WERM dynamically weighting by \(W\). Intuitively, \(W\) assigns more learning propensities for data points with (1) larger discrepancy between \(_{M_{t}}^{}\) (generated by model) and \(_{M^{*}}^{}\) (real-data distribution), or (2) larger stochasticity of the real model \(M^{*}\). The latter is contributed by the entropy \(H_{M^{*}}\), while the former is contributed by the first two terms combined. In particular, through the definition of \(f\)-divergence, we known that the discrepancy of two distribution \(P\) and \(Q\) can be measured by \(_{}Q(x)f(P(x)/Q(x))x\), thus the terms \(f(_{M_{t}}^{}(x,a,x^{})/_{M^{*}}^{}(x,a,x^{}))\) can be interpreted as the discrepancy measure unit between \(_{M_{t}}^{}(x,a,x^{})\) and \(_{M^{*}}^{}(x,a,x^{})\), while \(f(_{M_{t}}^{}(x,a)/_{M^{*}}^{}(x,a))\) serves as a baseline on \(x\) and \(a\) measured by \(f\) to balance the discrepancy contributed by \(x\) and \(a\), making \(M\) focus on errors on \(x^{}\).

In summary, by adjusting the weights \(W\), the learning process will iteratively exploit subtle errors of the current model in any data point, _regardless of how many proportions it contributes in the original data distribution, to eliminate potential unidentifiability on counterfactual data caused by selection bias._

### Tractable Solution

In Eq. (5), the terms \(f(_{M_{t}}^{}(x,a,x^{})/_{M^{*}}^{}(x,a,x^{}) )-f(_{M_{t}}^{}(x,a)/_{M^{*}}^{}(x,a))\) are still intractable. Thanks to previous successful practices in GAN  and GAIL , we achieve the objective via a generator-discriminator-paradigm objective through similar derivation. We show the results as follows and leave the complete derivation in Appx. A.4. In particular, by introducing two discriminators \(D_{_{0}^{*}}(x,a,x^{})\) and \(D_{_{0}^{*}}(x,a)\), we can optimize the surrogate objective Eq. (5) via:

\[_{t+1}= _{}\ (_{_{M_{t}}^{}}[A_{ _{0}^{*}_{1}^{*}}(x,a,x^{}) M_{}(x^{ }|x,a)]+_{_{M^{*}}^{}}[(H_{M^{*}}(x,a)-A_{_ {0}^{*}_{1}^{*}}(x,a,x^{})) M_{}(x^{}|x,a) ])\] \[s.t. _{0}^{*}=_{_{0}}(_{_{M^{* }}^{}}[ D_{_{0}}(x,a,x^{})]+_{_{M _{t}}^{}}[(1-D_{_{0}}(x,a,x^{}))])\] \[_{1}^{*}=_{_{1}}\ (_{_{M^{*}}^{}} [ D_{_{1}}(x,a)]+_{_{M_{t}}^{}} [(1-D_{_{1}}(x,a))]),\] (6)

where \(_{_{M^{*}}^{}}[]\) is a simplification of \(_{x,a,x^{}_{M^{*}}^{}}[]\), \(A_{_{0}^{*},_{1}^{*}}(x,a,x^{})= D_{_{0}^{*}}(x,a,x^{})- D_{_{1}^{*}}(x,a)\), and \(_{0}\) and \(_{1}\) are the parameters of \(D_{_{0}}\) and \(D_{_{1}}\) respectively. We learn a policy \(\) via imitation learning based on the offline dataset \(_{}\)[40; 25]. Note that in the process, we ignore the term \(_{0}(x,a)\) for simplifying the objective. The discussion on the impacts of removing \(_{0}(x,a)\) is left in App. B.

The overall optimization pipeline is illustration in Fig. 3. In Eq. (6), the reweighting term \(W\) from Eq. (5) is split into two terms in the RHS of the equation: the first term is a GAIL-style objective , treating \(M_{}\) as the policy generator, \(\) as the environment, and \(A\) as the advantage function, while the second term is WERM through \(H_{M^{*}}-A_{_{0}^{*},_{1}^{*}}\). The first term resembles the previous adversarial model learning objectives [46; 47; 11]. These two terms have intuitive explanations: _the first term_ assigns learning weights on data generated by the model \(M_{_{t}}\). If the predictions of the model appear realistic, mainly assessed by \(D_{_{0}^{*}}\), the propensity weights would be increased, encouraging the model to generate more such kind of data; Conversely, _the second term_ assigns weights on real data generated by \(M^{*}\). If the model's predictions seem unrealistic (mainly assessed by \(-D_{_{0}^{*}}\)) or stochastic (evaluated by \(H_{M^{*}}\)), the propensity weights will be increased, encouraging the model to pay more attention to these real data points when improving the likelihoods.

Figure 3: Illustration of the GALILEO workflow.

```
0:\(_{}\): offline dataset sampled from \(_{M^{*}}^{}\) where \(\) is the behavior policy; \(N\): total iterations; Process:
1: Approximate a behavior policy \(\) via behavior cloning through offline dataset \(_{}\)
2: Initialize an environment model \(M_{_{1}}\)
3:for\(t=1:N\)do
4: Use \(\) to generate a dataset \(_{}\) with the model \(M_{_{t}}\)
5: Update the discriminators \(D_{_{0}}\) and \(D_{_{1}}\) through the second and third equations in Eq. (6) where \(_{M_{_{t}}}^{}\) is estimated by \(_{}\) and \(_{M^{*}}^{}\) is estimated by \(_{}\)
6: Generative adversarial training for \(M_{_{t}}\) by regarding \(A_{_{0}^{*},_{1}^{*}}\) as the advantage function and computing the gradient to \(M_{_{t}}\), named \(g_{}\), with a standard policy gradient method like TRPO  or PPO  based on \(_{}\).
7: Regard \(H_{M^{*}}-A_{_{0}^{*},_{1}^{*}}\) as the reweighting term for WERM and compute the gradient to \(M_{_{t}}\) based on \(_{}\). Record it as \(g_{}\).
8: Update the model \(_{t+1}_{t}+g_{}+g_{}\).
9:endfor ```

**Algorithm 1** Pseudocode for GALILEO

## 5 Experiments

In this section, we first conduct experiments in two synthetic environments to quantify the performance of GALILEO on counterfactual queries 3. Then we deploy GALILEO in two complex environments: MuJoCo in Gym  and a real-world food-delivery platform to test the performance of GALILEO in difficult tasks. The results are in Sec. 5.2. Finally, to further verify the abiliy GALILEO, in Sec. 5.3, we apply models learned by GALILEO to several downstream tasks including off-policy evaluation, offline policy improvement, and online decision-making in production environment. The algorithms compared are: (1) **SL**: using standard empirical risk minimization for model learning; (2) **IPW**: a standard implementation of WERM based IPS; (3) **SCIGAN**: an adversarial algorithms for model learning used for causal effect estimation, which can be roughly regarded as a partial implementation of GALILEO (Refer to Appx. E.2). We give a detailed description in Appx. G.2.

### Environment Settings

Synthetic EnvironmentsPrevious experiments on counterfactual environment model learning are based on single-step semi-synthetic data simulation . As GALILEO is compatible with _single-step_ environment model learning, we first benchmark GALILEO in the same task named TCGA as previous studies do . Based on the three synthetic response functions, we construct 9 tasks by choosing different parameters of selection bias on \(\) which is constructed with beta distribution, and

Figure 4: Illustration of the performance in GNFC and TCGA. The grey bar denotes the standard error (\( 0.3\) for brevity) of \(3\) random seeds.

design a coefficient \(c\) to control the selection bias. We name the tasks with the format of "t?_bias?". For example, t1_bias2 is the task with the _first_ response functions and \(c=2\). The details of TCGA is in Appx. G.1.2. Besides, for _sequential_ environment model learning under selection bias, we construct a new synthetic environment, general negative feedback control (GNFC), which can represent a classic type of task with policies having selection bias, where Fig. 1(a) is also an instance of GNFC. We construct 9 tasks on GNFC by adding behavior policies \(\) with different scales of uniform noise \(U(-e,e)\) with probabilities \(p\). Similarly, we name them with the format "e?_p?".

Continuous-control EnvironmentsWe select 3 MuJoCo environments from D4RL  to construct our model learning tasks. We compare it with a standard transition model learning algorithm used in the previous offline model-based RL algorithms [59; 30], which is a standard supervised learning. We name the method OFF-SL. Besides, we also implement IPW and SCIGAN as the baselines. In D4RL benchmark, only the "medium" tasks is collected with a fixed policy, i.e., the behavior policy is with 1/3 performance to the expert policy), which is most matching to our proposed problem. So we train models in datasets HalfCheetah-medium, Walker2d-medium, and Hopper-medium.

A Real-world Large-scale Food-delivery PlatformWe finally deploy GALILEO in a real-world large-scale food-delivery platform. We focus on a Budget Allocation task to the Time period (BAT) in the platform (see Appx. G.1.3 for details). The goal of the BAT task is to handle the imbalance problem between the demanded orders from customers and the supply of delivery clerks in different time periods by allocating reasonable allowances to those time periods. The challenge of the environment model learning in BAT tasks is similar to the challenge in Fig. 1: the behavior policy is a human-expert policy, which tends to increase the budget of allowance in the time periods with a lower supply of delivery clerks, otherwise tends to decrease the budget (We gives a real-data instance in Appx. G.1.3).

### Prediction Accuracy on Shifted Data Distributions

Test in Synthetic EnvironmentsFor all of the tasks, we select mean-integrated-square error \(=[_{}(M^{*}(x^{}|x,a)- M(x^{}|x,a))^{2}a]\) as the metric, which is a metric to measure the accuracy in counterfactual queries by considering the prediction errors in the whole action space. The results are summarized in Fig. 4 and the detailed results can be found in Appx. H. The results show that the property of the behavior policy (i.e., \(e\) and \(p\)) dominates the generalization ability of the baseline algorithms. When \(e=0.05\), almost all of the baselines fail and give a completely opposite response curve, while GALILEO gives the correct response. (see Fig. 5). IPW still performs well when \(0.2 e 1.0\) but fails when \(e=0.05,p<=0.2\). We also found that SCIGAN can reach a better performance than other baselines when \(e=0.05,p<=0.2\), but the results in other tasks are unstable. GALILEO is the only algorithm that is robust to the selection bias and outputs correct response curves in all of the tasks. Based on the experiment, we also indicate that the commonly used overlap assumption is unreasonable to a certain extent especially in real-world applications since it is impractical to inject noises into the whole action space. The problem of overlap assumption being violated, e.g., \(e<1\) in our setting, should be taken into consideration otherwise the algorithm will be hard to use in practice if it is sensitive to the noise range. On the other hand, we found the phenomenon in TCGA experiment is similar to the one in GNFC, which demonstrates the compatibility of GALILEO to single-step environments.

We also found that the results of IPW are unstable in TCGA experiment. It might be because the behavior policy is modeled with beta distribution while the propensity score \(\) is modeled with Gaussian distribution. Since IPW directly reweight loss with \(}\), the results are sensitive to the error of \(\).

Finally, we plot the averaged response curves which are constructed by equidistantly sampling action from the action space and averaging the feedback of the states in the dataset as the averaged response. One result is in Fig. 5 (all curves can be seen in Appx. H). For those tasks where baselines fail in reconstructing response curves, GALILEO not only reaches a better MISE score but reconstructs almost exact responses, while the baselines might give completely opposite responses.

Test in MuJoCo BenchmarksWe test the prediction error of the learned model in corresponding unseen "expert" and "medium-replay" datasets. Fig. 6 illustrates the results in halfcheetah. We can see that all algorithms perform well in the training datasets. OFF-SL can even reach a bit lower error. However, when we verify the models through "expert" and "medium-replay" datasets, which

Figure 5: Illustration of the averaged response curves in task e0.05_p0.2.

are collected by other policies, the performance of GALILEO is more stable and better than all other algorithms. As the training continues, the baseline algorithms even gets worse and worse. The phenomenon are similar among three datasets, and we leave the full results in Appx. H.5.

Test in a Real-world DatasetWe first learn a model to predict the supply of delivery clerks (measured by fulfilled order amount) on given allowances. Although the SL model can efficiently fit the offline data, the tendency of the response curve is easily to be incorrect. As can be seen in Fig. 7(a), with a larger budget of allowance, the prediction of the supply is decreased in SL, which obviously goes against our prior knowledge. This is because, in the offline dataset, the corresponding supply will be smaller when the allowance is larger. It is conceivable that if we learn a policy through the model of SL, the optimal solution is canceling all of the allowances, which is obviously incorrect in practice. On the other hand, the tendency of GALILEO's response is correct. In Appx. H.7, we plot all the results in 6 cities. We further collect some randomized controlled trials data, and the Area Under the Uplift Curve (AUUC)  curve in Fig. 7(b) verify that GALILEO gives a reasonable sort order on the supply prediction while the standard SL technique fails to achieve this task.

### Apply GALILEO to Downstream Tasks

Off-policy Evaluation (OPE)We first verify the ability of the models in MuJoCo environments by adopting them into off-policy evaluation tasks. We use 10 unseen policies constructed by DOPE benchmark  to conduct our experiments. We select three common-used metrics: value gap, regret@1, and rank correlation and averaged the results among three tasks in Tab. 1. The baselines and the corre

   Algorithm & Norm. value gap & Rank corr. & Regret@1 \\  
**GALILEO** & \(\) & \(\) & \(\) \\  Best DICE & \(0.48 0.19\) & \(0.15 0.04\) & \(0.42 0.28\) \\ VPM & \(0.71 0.04\) & \(0.29 0.15\) & \(0.17 0.11\) \\ FQE (L2) & \(0.54 0.09\) & -0.19 0.10\) & \(0.34 0.03\) \\ IS & \(0.67 0.01\) & -0.40 0.15\) & \(0.36 0.27\) \\ Doubly Rubost & \(0.57 0.07\) & -0.14 0.17\) & \(0.33 0.06\) \\   

Table 1: Results of OPE on DOPE benchmark. We list the **averaged** performances on three tasks. The detailed results are in Appx. H.6. \(\) is the standard deviation among the tasks. We bold the best scores for each metric.

Figure 6: Illustration of learning curves of the halfcheetah tasks (full results are in Appx. H.5). The figures with titles ending in “(train)” means the dataset is used for training while the titles ending in “(test)” means the dataset is **just used for testing**. The X-axis records the steps of the environment model update, and the Y-axis is the prediction errors in the corresponding steps evaluated by the datasets. The solid curves are the mean reward and the shadow is the standard error of three seeds.

Figure 7: Parts of the results in BAT tasks. Fig. 7(a) demonstrate the averaged response curves of the SL and GALILEO model in City A. It is expected to be monotonically increasing through our prior knowledge. In Fig. 7(b) show the AUCC curves, where the model with larger areas above the “random” line makes better predictions in randomized-controlled-trials data .

sponding results we used are the same as the one proposed by . As seen in Tab. 1, compared with all the baselines, OPE by GALILEO always reach the better performance with _a large margin (at least 23%, 193% and 47% respectively)_, which verifies that GALILEO can eliminate the effect of selection bias and give correct evaluations on unseen policies.

Offline Policy ImprovementWe then verify the generalization ability of the models in MuJoCo environments by adopting them into offline model-based RL. To strictly verify the ability of the models, we abandon all tricks to suppress policy exploration and learning in risky regions as current offline model-based RL algorithms  do, and we just use the standard SAC algorithm  to fully exploit the models to search an optimal policy. Unfortunately, we found that the compounding error will still be inevitably large in the 1,000-step rollout, which is the standard horizon in MuJoCo tasks, leading all models to fail to derive a reasonable policy. To better verify the effects of models on policy improvement, we learn and evaluate the policies with three smaller horizons: \(H\{10,20,40\}\). The results are listed in Tab. 2. We first averaged the normalized return (refer to "avg. norm.") under each task, and we can see that the policy obtained by GALILEO is significantly higher than other models (the improvements are 24% to 161%). But in HalfCheetah, IPW works slightly better. However, compared with MAX-RETURN, it can be found that all methods fail to derive reasonable policies because their policies' performances are far away from the optimal policy. By further checking the trajectories, we found that all the learned policies just keep the cheetah standing in the same place or even going backward. This phenomenon is also similar to the results in MOPO . In MOPO's experiment in the medium datasets, the truncated-rollout horizon used in Walker and Hopper for policy training is set to 5, while HalfCheetah has to be set to _the minimal value: 1_. These phenomena indicate that HalfCheetah may still have unknown problems, resulting in the generalization bottleneck of the models.

Online Decision-making in a Production EnvironmentFinally, we search for the optimal policy via model-predict control (MPC) using cross-entropy planner  based on the learned model and deploy the policy in a real-world platform. The results of A/B test in City A is shown in Fig. 7(c). It can be seen that after the day of the A/B test, the treatment group (deploying our policy) significant improve the five-minute order-taken rate than the baseline policy (the same as the behavior policy). In summary, _the policy improves the supply from 0.14 to 1.63 percentage points to the behavior policies in the 6 cities_. The details of these results are in Appx. H.7.

## 6 Discussion and Future Work

In this work, we propose AWRM which handles the generalization challenges of the counterfactual environment model learning. By theoretical modeling, we give a tractable solution to handle AWRM and propose GALILEO. GALILEO is verified in synthetic environments, complex robot control tasks, and a real-world platform, and shows great generalization ability on counterfactual queries.

Giving correct answers to counterfactual queries is important for policy learning. We hope the work can inspire researchers to develop more powerful tools for counterfactual environment model learning. The current limitation lies in: There are several simplifications in the theoretical modeling process (further discussion is in Appx. B), which can be modeled more elaborately. Besides, experiments on MuJoCo indicate that these tasks are still challenging to give correct predictions on counterfactual data. These should also be further investigated in future work.

   Task &  &  &  &  \\  Horizon & H=10 & H=20 & H=40 & H=10 & H=20 & H=40 & H=10 & H=20 & H=40 & / \\  GALILEO & **13.0\(\)0.1** & **33.2\(\)0.1** & **53.5\(\)1.2** & **11.7\(\)0.2** & **20.9\(\)0.3** & **61.2\(\)3.4** & 0.7\(\)0.2 & -11.1\(\)0.2 & -14.2\(\)1.4 & **51.1** \\ OFF-SL & 4.8 \(\) 0.5 & 3.0 \(\) 0.2 & 4.6 \(\) 0.2 & 10.7 \(\) 0.2 & 20.1 \(\) 0.3 & 37.5\(\) 6.7 & 0.4 \(\) 0.5 & -1.1 \(\) 0.6 & -13.2 \(\) 0.3 & 21.1 \\ IPW & 5.9 \(\) 0.7 & 4.1 \(\) 0.5 & 5.9 \(\) 0.2 & 4.7 \(\) 1.1 & 2.8 \(\) 3.9 & 14.5 \(\) 1.4 & **1.6\(\)0.2** & **0.5\(\)0.8** & **-13.3\(\)0.9** & 19.7 \\ SCIGAN & 12.7 \(\) 0.1 & 29.2 \(\) 0.6 & 46.2 \(\) 5.2 & 8.4 \(\) 0.5 & 9.1 \(\) 1.7 & 1.0 \(\) 5.8 & 1.2 \(\) 0.3 & -0.3 \(\) 1.0 & -11.4 \(\) 0.3 & 41.8 \\  MAX-RETURN & 13.2 \(\) 0.0 & 33.3 \(\) 0.2 & 71.0 \(\) 0.5 & 14.9 \(\) 1.3 & 60.7 \(\) 1.1 & 221.1 \(\) 8.9 & 2.6 \(\) 0.1 & 13.3 \(\) 1.1 & 49.1 \(\) 2.3 & 100.0 \\   

Table 2: Results of policy performance directly optimized through SAC  using the learned dynamics models and deployed in MuJoCo environments. MAX-RETURN is the policy performance of SAC in the MuJoCo environments, and “avg. norm.” is the averaged normalized return of the policies in the 9 tasks, where the returns are normalized to lie between 0 and 100, where a score of 0 corresponds to the worst policy, and 100 corresponds to MAX-RETURN.