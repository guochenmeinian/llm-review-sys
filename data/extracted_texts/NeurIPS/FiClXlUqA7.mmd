# A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm

Haizhou Shi

Department of Computer Science

Rutgers University

Piscataway, NJ 08854

haizhou.shi@rutgers.edu

&Hao Wang

Department of Computer Science

Rutgers University

Piscataway, NJ 08854

hw488@cs.rutgers.edu

###### Abstract

Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL unifies various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different _fixed_ coefficients; based on insights from this unification, our UDIL allows _adaptive_ coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. Code will be available at https://github.com/Wang-ML-Lab/unified-continual-learning.

## 1 Introduction

Despite recent success of large-scale machine learning models , continually learning from evolving environments remains a longstanding challenge. Unlike the conventional machine learning paradigms where learning is performed on a static dataset, _domain incremental learning, i.e., continual learning with evolving domains_, hopes to accommodate the model to the dynamically changing data distributions, while retaining the knowledge learned from previous domains . Naive methods, such as continually finetuning the model on new-coming domains, will suffer a substantial performance drop on the previous domains; this is referred to as "catastrophic forgetting" . In general, domain incremental learning algorithms aim to minimize the total risk of _all_ domains, i.e.,

\[^{*}()=_{t}()+_{1:t-1}()= _{(x,y)_{t}}[(y,h_{}(x)]+_{i=1}^{t-1} _{(x,y)_{i}}[(y,h_{}(x)],\] (1)

where \(_{t}\) calculates model \(h_{}\)'s expected prediction error \(\) over the current domain's data distribution \(_{t}\). \(_{1:t-1}\) is the total error evaluated on the past \(t-1\) domains' data distributions, i.e., \(\{_{i}\}_{i=1}^{t-1}\).

The main challenge of domain incremental learning comes from the practical _memory constraint_ that no (or only very limited) access to the past domains' data is allowed . Under such a constraint, it is difficult, if not impossible, to accurately estimate and optimize the past error \(_{1:t-1}\). Therefore the main focus of recent domain incremental learning methods has been to develop effective surrogate learning objectives for \(_{1:t-1}\). Among these methods , replay-based methods, which replay a small set of old exemplars during training , has consistently shown promise and is therefore commonly used in practice.

One typical example is ER , which stores a set of exemplars \(\) and uses a replay loss \(_{}\) as the surrogate of \(_{1:t-1}\). In addition, a fixed, predetermined coefficient \(\) is used to balance current domain learning and past sample replay. Specifically,

\[}()=_{t}()+_{}()=_{t}()+_{(x^{ },y^{})}[(y^{},h_{}(x^{})].\] (2)

While such methods are popular in practice, there is still a gap between the surrogate loss (\(_{}\)) and the true objective (\(_{1:t-1}\)), rendering them lacking in theoretical support and therefore calling into question their reliability. Besides, different methods use different schemes of setting \(\), and it is unclear how they are related and when practitioners should choose one method over another.

To address these challenges, we develop a unified generalization error bound and theoretically show that different existing methods are actually minimizing the same error bound with different _fixed_ coefficients (more details in Table 1 later). Based on such insights, we then develop an algorithm that allows _adaptive_ coefficients during training, thereby always achieving the tightest bound and improving the performance. Our contributions are as follows:

* We propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory to unify various existing methods.
* Our theoretical analysis shows that different existing methods are equivalent to minimizing the same error bound with different _fixed_ coefficients. Based on insights from this unification, our UDIL allows _adaptive_ coefficients during training, thereby always achieving the tightest bound and improving the performance.
* Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets.

## 2 Related Work

**Continual Learning.** Prior work on continual learning can be roughly categorized into three scenarios : (i) task-incremental learning, where task indices are available during both training and testing , (ii) class-incremental learning, where new classes are incrementally included for the classifier , and (iii) domain-incremental learning, where the data distribution's incremental shift is explicitly modeled . Regardless of scenarios, the main challenge of continual learning is to alleviate catastrophic forgetting with only limited access to the previous data; therefore methods in one scenario can often be easily adapted for another. Many methods have been proposed to tackle this challenge, including functional and parameter regularization , constraining the optimization process , developing incrementally updated components , designing modularized model architectures , improving representation learning with additional inductive biases , and Bayesian approaches . Among them, replaying a small set of old exemplars, i.e., memory, during training has shown great promise as it is easy to deploy, _applicable in all three scenarios_, and, most importantly, achieves impressive performance . Therefore in this paper, we focus on _domain incremental learning_ with _memory_, aiming to provide a principled theoretical framework to unify these existing methods.

**Domain Adaptation and Domain Incremental Learning.** Loosely related to our work are domain adaptation (DA) methods, which adapt a model trained on _labeled_ source domains to _unlabeled_ target domains . Much prior work on DA focuses on matching the distribution of the source and target domains by directly matching the statistical attributions  or adversarial training . Compared to DA's popularity, domain incremental learning (DIL) has received limited attention in the past. However, it is now gaining significant traction in the research community . These studies predominantly focus on the practical applications of DIL, such as semantic segmentation , object detection for autonomous driving , and learning continually in an open-world setting . Inspired by the theoretical foundation of adversarial DA , we propose, to the best of our knowledge, **the first unified upper bound for DIL**. Most related to our work are previous DA methods that flexibly align different domains according to their associated given or inferred domain index [94; 101], domain graph , and domain taxonomy . The main difference between DA and DIL is that the former focuses on improving the accuracy of the _target domains_, while the latter focuses on the total error of _all domains_, with additional measures taken to alleviate forgetting on the previous domains. More importantly, DA methods typically require access to target domain data to match the distributions, and therefore are not directly applicable to DIL.

## 3 Theory: Unifying Domain Incremental Learning

In this section, we formalize the problem of domain incremental learning, provide the generalization bound of naively applying empirical risk minimization (ERM) on the memory bank, derive two error bounds (i.e., intra-domain and cross-domain error bounds) more suited for domain incremental learning, and then unify these three bounds to provide our final adaptive error bound. We then develop an algorithm inspired by this bound in Sec. 4. **All proofs of lemmas, theorems, and corollaries can be found in Appendix A.**

Problem Setting and Notation.We consider the problem of domain incremental learning with \(T\) domains arriving one by one. Each domain \(i\) contains \(N_{i}\) data points \(_{i}=\{(_{j}^{(i)},y_{j}^{(i)})\}_{j=1}^{N_{i}}\), where \((_{j}^{(i)},y_{j}^{(i)})\) is sampled from domain \(i\)'s data distribution \(_{i}\). Assume that when domain \(t[T]\{1,2,,T\}\) arrives at time \(t\), one has access to (1) the current domain \(t\)'s data \(_{t}\), (2) a memory bank \(=\{M_{i}\}_{i=1}^{t-1}\), where \(M_{i}=\{(}_{j}^{(i)},_{j}^{(i)})\}_{j=1}^{_{i}}\) is a small subset (\(_{i} N_{i}\)) randomly sampled from \(_{i}\), and (3) the history model \(H_{t-1}\) after training on the previous \(t-1\) domains. For convenience we use shorthand notation \(_{i}\{_{j}^{(i)}\}_{j=1}^{N_{i}}\) and \(}_{i}\{}_{j}^{(i)}\}_{j=1}^{ _{i}}\). The goal is to learn the optimal model (hypothesis) \(h^{*}\) that minimizes the prediction error over all \(t\) domains after each domain \(t\) arrives. Formally,

\[h^{*}=_{h}_{i=1}^{t}_{_{i}}(h), _{_{i}}(h)_{_{i}}[h() f_{i}()],\] (3)

where for domain \(i\), we assume the labels \(y=\{0,1\}\) are produced by an unknown deterministic function \(y=f_{i}()\) and \(_{_{i}}(h)\) denotes the expected error of domain \(i\).

### Naive Generalization Bound Based on ERM

**Definition 3.1** (Domain-Specific Empirical Risks).: _When domain \(t\) arrives, model \(h\)'s empirical risk \(_{_{i}}(h)\) for each domain \(i\) (where \(i t\)) is computed on the available data at time \(t\), i.e.,_

\[_{_{i}}(h)=}_{ _{i}}_{h() f_{i}()}&i=t,\\ }_{}_{i}}_{h( {x}) f_{i}()}&i<t.\] (4)

Note that at time \(t\), only a small subset of data from previous domains (\(i<t\)) is available in the memory bank (\(_{i} N_{i}\)). Therefore empirical risks for previous domains (\(_{_{i}}(h)\) with \(i<t\)) can deviate a lot from the true risk \(_{_{i}}(h)\) (defined in Eqn. 3); this is reflected in Lemma 3.1 below.

**Lemma 3.1** (**ERM-Based Generalization Bound)**.: _Let \(\) be a hypothesis space of VC dimension \(d\). When domain \(t\) arrives, there are \(N_{t}\) data points from domain \(t\) and \(_{i}\) data points from each previous domain \(i<t\) in the memory bank. With probability at least \(1-\), we have:_

\[_{i=1}^{t}_{_{i}}(h)_{i=1}^{t}_{_{i}}(h)+}+_{i=1}^{t-1}})(8d()+8( ))}.\] (5)

Lemma 3.1 shows that naively using ERM to learn \(h\) is equivalent to minimizing a loose generalization bound in Eqn. 33. Since \(_{i} N_{i}\), there is a large constant \(_{i=1}^{t-1}}\) compared to \(}\), making the second term of Eqn. 33 much larger and leading to a looser bound.

### Intra-Domain and Cross-Domain Model-Based Bounds

In domain incremental learning, one has access to the history model \(H_{t-1}\) besides the memory bank \(\{M_{i}\}_{i=1}^{t-1}\); this offers an opportunity to derive tighter error bounds, potentially leading to better algorithms. In this section, we will derive two such bounds, an intra-domain error bound (Lemma 3.2) and a cross-domain error bound (Lemma 3.3), and then integrate them two with the ERM-based bound in Eqn. 33 to arrive at our final adaptive bound (Theorem 3.4).

**Lemma 3.2** (**Intra-Domain Model-Based Bound**).: _Let \(h\) be an arbitrary function in the hypothesis space \(\), and \(H_{t-1}\) be the model trained after domain \(t-1\). The domain-specific error \(_{_{i}}(h)\) on the previous domain \(i\) has an upper bound:_

\[_{_{i}}(h)_{_{i}}(h,H_{t-1})+ _{_{i}}(H_{t-1}),\] (6)

_where \(_{_{i}}(h,H_{t-1})_{_{i}}[h() H_{t-1}()]\)._

Lemma 3.2 shows that the current model \(h\)'s error on domain \(i\) is bounded by the discrepancy between \(h\) and the history model \(H_{t-1}\) plus the error of \(H_{t-1}\) on domain \(i\).

One potential issue with the bound Eqn. 34 is that only a limited number of data is available for each previous domain \(i\) in the memory bank, making empirical estimation of \(_{_{i}}(h,H_{t-1})+_{_{i}}(H_{t-1})\) challenging. Lemma 3.3 therefore provides an alternative bound.

**Lemma 3.3** (**Cross-Domain Model-Based Bound**).: _Let \(h\) be an arbitrary function in the hypothesis space \(\), and \(H_{t-1}\) be the function trained after domain \(t-1\). The domain-specific error \(_{_{i}}(h)\) evaluated on the previous domain \(i\) then has an upper bound:_

\[_{_{i}}(h)_{_{t}}(h,H_{t-1})+ {1}{2}d_{}(_{i},_{t})+ _{_{i}}(H_{t-1}),\] (7)

_where \(d_{}(,)=2_{h}|_{}[h(x)=1]-_{ }[h(x)=1]|\) denotes the \(\)-divergence between distribution \(\) and \(\), and \(_{_{t}}(h,H_{t-1})_{_{t}}[h() H_{t-1}()]\)._

Lemma 3.3 shows that if the divergence between domain \(i\) and domain \(t\), i.e., \(d_{}(_{i},_{t})\), is small enough, one can use \(H_{t-1}\)'s predictions evaluated on the current domain \(_{t}\) as a surrogate loss to prevent catastrophic forgetting. Compared to the error bound Eqn. 34 which is hindered by limited data from previous domains, Eqn. 35 relies on the current domain \(t\) which contains abundant data and therefore enjoys much lower generalization error. Our lemma also justifies LwF-like cross-domain distillation loss \(_{_{t}}(h,H_{t-1})\) which are widely adopted [52; 100; 23].

### A Unified and Adaptive Generalization Error Bound

Our Lemma 3.1, Lemma 3.2, and Lemma 3.3 provide three different ways to bound the true risk \(_{i=1}^{t}_{_{i}}(h)\); each has its own advantages and disadvantages. Lemma 3.1 overly relies on the limited number of data points from previous domains \(i<t\) in the memory bank to compute the empirical risk; Lemma 3.2 leverages the history model \(H_{t-1}\) for knowledge distillation, but is still hindered by the limited number of data points in the memory bank; Lemma 3.3 improves over Lemma 3.2 by leveraging the abundant data \(_{t}\) in the current domain \(t\), but only works well if the divergence between domain \(i\) and domain \(t\), i.e., \(d_{}(_{i},_{t})\), is small. Therefore, we propose to integrate these three bounds using coefficients \(\{_{i},_{i},_{i}\}_{i=1}^{t-1}\) (with \(_{i}+_{i}+_{i}=1\)) in the theorem below.

**Theorem 3.4** (**Unified Generalization Bound for All Domains**).: _Let \(\) be a hypothesis space of VC dimension \(d\). Let \(N=N_{t}+_{i}^{t-1}_{i}\) denoting the total number of data points available to the training of current domain \(t\), where \(N_{t}\) and \(_{i}\) denote the numbers of data points collected at domain \(t\) and data points from the previous domain \(i\) in the memory bank, respectively. With probability atleast \(1-\), we have:_

\[_{i=1}^{t}_{_{i}}(h) \{_{i=1}^{t-1}[_{i}_{ _{i}}(h)+_{i}_{_{i}}(h,H_{t-1}) ]\}+\{_{_{i}}(h)+(_{i=1}^{t- 1}_{i})_{_{i}}(h,H_{t-1})\}\] \[+_{i=1}^{t-1}_{i}d_{}(_{i},_{t})+_{i=1}^{t-1}(_{i}+_{i}) _{_{i}}(H_{t-1})\] \[+^{t-1}_{i})^{2}}{N_{t}}+ _{i=1}^{t-1}+_{i})^{2}}{N_{i}})(8d ()+8())}\] \[ g(h,H_{t-1},),\] (8)

_where \(_{_{i}}(h,H_{t-1})=}_{ }_{i}}_{h() H_{t-1}()}\), \(_{_{i}}(h,H_{t-1})=}_{ _{i}}_{h() H_{t-1}()}\), and \(\{_{i},_{i},_{i}\}_{i=1}^{t-1}\)._

Theorem 3.4 offers the opportunity of adaptively adjusting the coefficients (\(_{i}\), \(_{i}\), and \(_{i}\)) according to the data (current domain data \(_{t}\) and the memory bank \(=\{M_{i}\}_{i=1}^{t-1}\)) and history model (\(H_{t-1}\)) at hand, thereby achieving the tightest bound. For example, when the \(\) divergence between domain \(i\) and domain \(t\), i.e., \(d_{}(_{i},_{t})\), is small, minimizing this unified bound (Eqn. 3.4) leads to a large coefficient \(_{i}\) and therefore naturally puts on more focus on cross-domain bound in Eqn. 3.4 which leverages the current domain \(t\)'s data to estimate the true risk.

**UDIL as a Unified Framework.** Interestingly, Eqn. 3.4 unifies various domain incremental learning methods. Table 1 shows that different methods are equivalent to fixing the coefficients \(\{_{i},_{i},_{i}\}_{i=1}^{t-1}\) to different values (see Appendix B for a detailed discussion). For example, assuming default configurations, LwF  corresponds to Eqn. 3.4 with _fixed_ coefficients \(\{_{i}=_{i}=0,_{i}=1\}\); ER  corresponds to Eqn. 3.4 with _fixed_ coefficients \(\{_{i}=_{i}=0,_{i}=1\}\), and DER++  corresponds to Eqn. 3.4 with _fixed_ coefficients \(\{_{i}=_{i}=0.5,_{i}=0\}\), under certain achievable conditions. Inspired by this unification, our UDIL adaptively adjusts these coefficients to search for the tightest bound in the range \(\) when each domain arrives during domain incremental learning, thereby improving performance. Corollary 3.4.1 below shows that such _adaptive_ bound is always tighter, or at least as tight as, any bounds with _fixed_ coefficients.

**Corollary 3.4.1**.: _For any bound \(g(h,H_{t-1},_{})\) (defined in Eqn. 3.4) with fixed coefficients \(_{}\), e.g., \(_{}=_{}=\{_{i}=_{i}=0,_{i}=1 \}_{i=1}^{t-1}\) for ER , we have_

\[_{i=1}^{t}_{_{i}}(h)_{ }g(h,H_{t-1},) g(h,H_{t-1},_{}), h,H_{t-1}.\] (9)

Corollary 3.4.1 shows that the unified bound Eqn. 3.4 with _adaptive_ coefficients is always preferable to other bounds with _fixed_ coefficients. We therefore use it to develop a better domain incremental learning algorithm in Sec. 4 below.

    & UDIL (Ours) & LwF  & ER  & DER++  & iCaRL  & CLS-ER  & EMS-ER  & BiC  \\   \(_{i}\) & \(\) & \(0\) & \(0\) & \(0.5\) & \(1\) & \(}{{(1+)}}\) & \(}}{{(1+^{})}}\) & \(}{{(2t-1)}}\) \\ \(_{i}\) & \(\) & \(1\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) & \((t-1)/(2t-1)\) \\ \(_{i}\) & \(\) & \(0\) & \(1\) & \(0.5\) & \(0\) & \(}{{(1+)}}\) & \(}{{(1+)}}\) & \(}{{(2t-1)}}\) \\   

Table 1: **UDIL as a unified framework** for domain incremental learning with memory. Three methods (LwF , ER , and DER++ ) are by default compatible with DIL setting. For the remaining four CIL methods (iCaRL , CLS-ER , EMS-ER , and BiC ), we adapt their original training objective to DIL settings before the analysis. For CLS-ER  and EMS-ER , \(\) and \(^{}\) are the intensity coefficients of the logits distillation. For BiC , \(t\) is the current number of the incremental domain. The conditions under which the unification of each method is achieved are provided in detail in Appendix B.

Method: Adaptively Minimizing the Tightest Bound in UDIL

Although Theorem 3.4 provides a unified perspective for domain incremental learning, it does not immediately translate to a practical objective function to train a model. It is also unclear what coefficients \(\) for Eqn. 8 would be the best choice. In fact, a _static_ and _fixed_ setting will not suffice, as different problems may involve different sequences of domains with dynamic changes; therefore ideally \(\) should be _dynamic_ (e.g., \(_{i}_{i+1}\)) and _adaptive_ (i.e., learnable from data). In this section, we start by mapping the unified bound in Eqn. 8 to concrete loss terms, discuss how the coefficients \(\) are learned, and then provide a final objective function to learn the optimal model.

### From Theory to Practice: Translating the Bound in Eqn. 8 to Differentiable Loss Terms

**(1) ERM Terms.** We use the cross-entropy classification loss in Definition 4.1 below to optimize domain \(t\)'s ERM term \(}_{_{t}}(h)\) and memory replay ERM terms \(\{_{i}_{_{t}}(h)\}_{i=1}^{t-1}\) in Eqn. 8.

**Definition 4.1** (**Classification Loss)**.: _Let \(h:^{n}^{K-1}\) be a function that maps the input \(^{n}\) to the space of \(K\)-class probability simplex, i.e., \(^{K-1}\{^{K}:z_{i} 0,_{i}z_{i}=1\}\); let \(\) be a collection of samples drawn from an arbitrary data distribution and \(f:^{n}[K]\) be the function that maps the input to the true label. The classification loss is defined as the average cross-entropy between the true label \(f()\) and the predicted probability \(h()\), i.e.,_

\[_{}(h)|} _{}[-_{j=1}^{K}_{f()= j}([h()]_{j})].\] (10)

Following Definition 4.1, we replace \(_{_{t}}(h)\) and \(_{_{t}}(h)\) in Eqn. 8 with \(_{_{t}}(h)\) and \(_{_{t}}(h)\).

**(2) Intra- and Cross-Domain Terms.** We use the distillation loss below to optimize intra-domain (\(\{}_{_{t}}(h,H_{t-1})\}_{i=1}^{t-1}\)) and cross-domain (\(_{_{t}}(h,H_{t-1})\)) model-based error terms in Eqn. 8.

**Definition 4.2** (**Distillation Loss)**.: _Let \(h,H_{t-1}:^{n}^{K-1}\) both be functions that map the input \(^{n}\) to the space of \(K\)-class probability simplex as defined in Definition 4.1; let \(\) be a collection of samples drawn from an arbitrary data distribution. The distillation loss is defined as the average cross-entropy between the target probability \(H_{t-1}()\) and the predicted probability \(h()\), i.e.,_

\[_{}(h,H_{t-1})|} _{}[-_{j=1}^{K}[H_{t-1}( )]_{j}([h()]_{j})].\] (11)

Accordingly, we replace \(_{_{t}}(h,H_{t-1})\) with \(_{_{t}}(h,H_{t-1})\) and \(_{_{t}}(h,H_{t-1})\) with \(_{_{t}}(h,H_{t-1})\).

**(3) Constant Term.** The error term \(_{i=1}^{t-1}(_{i}+_{i})_{_{t}}(H_{t-1})\) in Eqn. 8 is a constant and contains no trainable parameters (since \(H_{t-1}\) is a fixed history model); therefore it does not need a loss term.

**(4) Divergence Term.** In Eqn. 8, \(_{i=1}^{t-1}_{i}d_{}(_{i}, _{t})\) measures the weighted average of the dissimilarity between domain \(i\)'s and domain \(t\)'s data distributions. Inspired by existing adversarial domain adaptation methods , we can further tighten this divergence term by considering the _embedding distributions_ instead of _data distributions_ using an learnable encoder. Specifically, given an encoder \(e:^{n}^{m}\) and a family of domain discriminators (classifiers) \(_{d}\), we have the empirical estimate of the divergence term as follows:

\[_{i=1}^{t-1}_{i}_{}(e(_{i}),e(_{t}))=2_{i=1}^{t-1}_{i}-2_{d_{d}} _{i=1}^{t-1}_{i}[_{i}|}_{ _{i}}_{_{i}() 0}+_{d}|} _{_{t}}_{_{i}()<0}],\]

where \(_{i}\) (and \(_{t}\)) is a set of samples drawn from domain \(_{i}\) (and \(_{t}\)), \(d:^{m}^{t-1}\) is a domain classifier, and \(_{i}()=[d(e())]_{i}-[d(e())]_{t}\) is the difference between the probability of \(\) belonging to domain \(i\) and domain \(t\). Replacing the indicator function with the differentiable cross-entropy loss, \(_{i=1}^{t-1}_{i}_{}(e(_{t}),e(_{t}))\) above then becomes

\[2_{i=1}^{t-1}_{i}-2_{d_{d}}_{i=1}^{t-1}_{i} [}_{_{i}}[-([d(e ())]_{i})]+}_{_{t}}[-([d(e())]_{t})]].\] (12)

### Putting Everything Together: UDIL Training Algorithm

**Objective Function.** With these differentiable loss terms above, we can derive an algorithm that learns the optimal model by minimizing the tightest bound in Eqn. 8. As mentioned above, to achieve a tighter \(d_{}\), we decompose the hypothesis as \(h=p e\), where \(e:^{n}^{m}\) and \(p:^{m}^{K-1}\) are the encoder and predictor, respectively. To find and to minimize the tightest bound in Theorem 3.4, we treat \(=\{_{i},_{i},_{i}\}_{i=1}^{t-1}\) as learnable parameters and seek to optimize the following objective (we denote as \(=(x)\) the 'copy-weights-and-stop-gradients' operation):

\[_{\{,h=poe\}}_{d} V_{l}(h,)+V_{0:1}(, )-_{d}V_{d}(d,e,)\] (13) s.t. \[_{i}+_{i}+_{i}=1,  i\{1,2,,t-1\}\] \[_{i},_{i},_{i} 0,  i\{1,2,,t-1\}\]

**Details of \(V_{l}\), \(V_{01}\), and \(V_{d}\).**\(V_{l}\) is the loss for **learning the model \(h\)**, where the terms \(.()\) are differentiable cross-entropy losses as defined in Eqn. 10 and Eqn. 11:

\[V_{l}(h,)=_{i=1}^{t-1}_{i} _{_{i}}(h)+_{i}_{_{i} }(h,H_{t-1})+_{_{i}}(h)+(_{i=1} ^{t-1}})_{_{i}}(h,H_{t-1}).\] (14)

\(V_{01}\) is the loss for **finding the optimal coefficient set \(\)**. Its loss terms use Definition 3.1 and Eqn. 12 to estimate ERM terms and \(\)-divergence, respectively:

\[V_{01}(,) =_{i=1}^{t-1}_{i}_{ _{i}}()+_{i}_{_{i}}(,H_{t-1})+(_{i=1}^{t-1}_{i}) _{_{i}}(,H_{t-1})\] \[+_{i=1}^{t-1}_{i}_{ }((_{i}), (_{t}))+_{i=1}^{t-1}(_ {i}+_{i})_{_{i}}(H_{t-1})\] \[+C^{t-1}_{i} )^{2}}{N_{t}}+_{i=1}^{t-1}+_{i})^{2}}{N_ {i}}}.\] (15)

In Eqn. 15, \(.()\) uses _discrete 0-1 loss_, which is different from Eqn. 14, and a hyper-parameter \(C=\) is introduced to model the combined influence of \(\)'s VC-dimension and \(\).

\(V_{d}\) follows Eqn. 12 to **minimize the divergence between different domains' embedding distributions** (i.e., aligning domains) by the minimax game between \(e\) and \(d\) with the value function:

\[V_{d}(d,e,)=(_{i=1}^{t-1}})}_{_{t}} [-([d(e())]_{t})]+_{i=1}^{t-1} {}}{N_{i}}_{_{i}} [-([d(e())]_{i})].\] (16)Here in Eqn. 16, if an optimal \(d^{*}\) and a fixed \(\) is given, maximizing \(V_{d}(d^{*},e,)\) with respect to the encoder \(e\) is equivalent to minimizing the weighted sum of the divergence \(_{i=1}^{t-1}_{i}d_{}(e(_{i}),e (_{t}))\). This result indicates that the divergence between two domains' _embedding distributions_ can be actually minimized. Intuitively this minimax game learns an encoder \(e\) that aligns the embedding distributions of different domains so that their domain IDs can not be predicted (distinguished) by a powerful discriminator given an embedding \(e()\). Algorithm 1 below outlines how UDIL minimizes the tightest bound. Please refer to Appendix C for more implementation details, including a model diagram in Fig. 2.

## 5 Experiments

In this section, we compare UDIL with existing methods on both synthetic and real-world datasets.

### Baselines and Implementation Details

We compare UDIL with the state-of-the-art continual learning methods that are either specifically designed for domain incremental learning or can be easily adapted to the domain incremental learning setting. For fair comparison, we do not consider methods that leverage large-scale pre-training or prompt-tuning [99; 88; 53; 88]. Exemplar-free baselines include online Elastic Weight Consolidation (**oEWC**) , Synaptic Intelligence (**SI**) , and Learning without Forgetting (**LwF**) . Memory-based domain incremental learning baselines include Gradient Episodic Memory (**GEM**) , Averaged Gradient Episodic Memory (**A-GEM**) , Experience Replay (**ER**) , Dark Experience Replay (**DER++**) , and two recent methods, Complementary Learning System based Experience Replay (**CLS-ER**)  and Error Senesitivity Modulation based Experience Replay (**ESM-ER**)  (see Appendix C.5 for more detailed introduction to the baseline methods above). In addition, we implement the fine-tuning (**Fine-tune**)  and joint-training (**Joint**) as the performance lower bound and upper bound (Oracle).

We train all models using three different random seeds and report the mean and standard deviation. All methods are implemented with PyTorch , based on the mammoth code base [7; 8], and run on a single NVIDIA RTX A5000 GPU. For fair comparison, within the same dataset, all methods adopt the same neural network architecture, and the memory sampling strategy is set to random

Figure 1: Results on _HD-Balls_. In (a-b), data is colored according to labels; in (c-h), data is colored according to domain ID. All data is plotted after PCA . **(a)** Simplified _HD-Balls_ dataset with 3 domains in the 3D space (for visualization purposes only). **(b-c)** Embeddings of _HD-Balls_’s raw data colored by labels and domain ID. **(d-h)** Accuracy and embeddings learned by Joint (oracle), UDIL, and three best baselines (more in Appendix C.5). Joint, as the _oracle_, naturally aligns different domains, and UDIL outperforms all baselines in terms of embedding alignment and accuracy.

balanced sampling (see Appendix C.2 and Appendix C.6 for more implementation details on training). We evaluate all methods with standard continual learning metrics including 'average accuracy', 'forgetting', and 'forward transfer' (see Appendix C.4 for detailed definitions).

### Toy Dataset: High-Dimensional Balls

To gain insight into UDIL, we start with a toy dataset, high dimensional balls on a sphere (referred to as _HD-Balls_ below), for domain incremental learning. _HD-Balls_ includes 20 domains, each containing 2,000 data points sampled from a Gaussian distribution \((,0.2^{2})\). The mean \(\) is randomly sampled from a 100-dimensional unit sphere, i.e., \(\{^{100}:\|\|_{2}=1\}\); the covariance matrix \(\) is fixed. In _HD-Balls_, each domain represents a binary classification task, where the decision boundary is the hyperplane that passes the center \(\) and is tangent to the unit sphere. Fig. 1(a-c) shows some visualization on _HD-Balls_.

Column 3 and 4 of Table 2 compare the performance of our UDIL with different baselines. We can see that UDIL achieves the highest final average accuracy and the lowest forgetting. Fig. 1(d-h) shows the embedding distributions (i.e., \(e()\)) for different methods. We can see better embedding alignment across domains generally leads to better performance. Specifically, Joint, as the oracle, naturally aligns different domains' embedding distributions and achieves an accuracy upper bound of \(91.083\%\). Similarly, our UDIL can adaptively adjust the coefficients of different loss terms, including Eqn. 12, successfully align different domains, and thereby outperform all baselines.

### Permutation MNIST

We further evaluate our method on the Permutation MNIST (_P-MNIST_) dataset . _P-MNIST_ includes 20 sequential domains, with each domain constructed by applying a fixed random permutation to the pixels in the images. Column 5 and 6 of Table 2 show the results of different methods. Our UDIL achieves the second best (\(92.666\%\)) final average accuracy, which is only \(0.284\%\) lower than the best baseline DER++. We believe this is because (i) there is not much space for improvement as the gap between joint-training (oracle) and most methods are small; (ii) under the permutation, different domains' data distributions are too distinct from each other, lacking the meaningful relations among the domains, and therefore weakens the effect of embedding alignment in our method. Nevertheless, UDIL still achieves best performance in terms of forgetting (\(2.853\%\)). This is mainly because our unified UDIL framework (i) is directly derived from the total loss of _all_ domains, and (ii) uses adaptive coefficients to achieve a more balanced trade-off between learning the current domain and avoiding forgetting previous domains.

    &  &  &  &  \\  & & Avg. Acc (\(\)) & Forgetting (\(\)) & Avg. Acc (\(\)) & Forgetting (\(\)) & Avg. Acc (\(\)) & Forgetting (\(\)) \\   Fine-tune & - & 52.319\(\)0.024 & 43.520\(\)0.079 & 70.102\(\)2.945 & 27.522\(\)3.042 & 47.803\(\)1.703 & 52.281\(\)1.797 \\  oEWC  & - & 54.131\(\)0.193 & 39.743\(\)1.388 & 78.476\(\)1.223 & 18.068\(\)1.321 & 48.203\(\)0.837 & 51.181\(\)0.867 \\ SI  & - & 52.303\(\)0.037 & 43.175\(\)0.041 & 79.045\(\)1.337 & 17.409\(\)1.446 & 48.251\(\)1.381 & 51.053\(\)1.507 \\ LwF  & - & 51.523\(\)0.065 & 25.155\(\)0.264 & 73.545\(\)2.646 & 24.556\(\)2.789 & 54.709\(\)0.515 & 45.473\(\)9.565 \\  GEM  & & 69.747\(\)0.065 & 13.591\(\)0.779 & 89.097\(\)0.149 & 6.975\(\)0.167 & 76.619\(\)0.581 & 21.289\(\)0.579 \\ A-GEM  & & 62.777\(\)0.295 & 12.878\(\)1.588 & 87.560\(\)0.087 & 8.577\(\)0.053 & 59.654\(\)0.122 & 39.196\(\)0.171 \\ ER  & 82.255\(\)1.552 & 9.524\(\)1.655 & 88.339\(\)0.044 & 7.180\(\)0.029 & 76.794\(\)0.066 & 20.696\(\)0.744 \\ DER++  & 400 & 79.332\(\)1.347 & 13.762\(\)1.514 & **92.950\(\)0.364** & 3.378\(\)0.245 & 84.258\(\)0.544 & 13.692\(\)0.560 \\ CLS-ER  & & 85.844\(\)0.165 & 5.297\(\)0.281 & 91.598\(\)0.117 & 3.795\(\)0.144 & 81.771\(\)0.354 & 15.455\(\)0.356 \\ ESM-ER  & & 71.995\(\)0.333 & 13.245\(\)3.397 & 89.829\(\)0.698 & 6.888\(\)0.738 & 82.192\(\)1.64 & 16.195\(\)0.150 \\ UDIL (Ours) & & **86.872\(\)0.195** & **3.428\(\)0.359** & 92.666\(\)0.108 & **2.853\(\)0.107** & **86.635\(\)0.886** & **8.506\(\)1.181** \\  Joint (Oracle) & \(\) & 91.083\(\)0.332 & - & 96.368\(\)0.042 & - & 97.150\(\)0.036 & - \\   

Table 2: **Performances (%) on _HD-Balls_, _P-MNIST_, and _R-MNIST_. We use two metrics, Average Accuracy and Forgetting, to evaluate the methods’ effectiveness. “\(\)” and “\(\)” mean higher and lower numbers are better, respectively. We use **boldface** and underlining to denote the best and the second-best performance, respectively. We use “-\(\) to denote “not appliable”.

### Rotating MNIST

We also evaluate our method on the Rotating MNIST dataset (_R-MNIST_) containing 20 sequential domains. Different from _P-MNIST_ where shift from domain \(t\) to domain \(t+1\) is abrupt, _R-MNIST_'s domain shift is gradual. Specifically, domain \(t\)'s images are rotated by an angle randomly sampled from the range \([9^{}(t-1),9^{} t)\). Column 7 and 8 of Table 2 show that our UDIL achieves the highest average accuracy (\(86.635\%\)) and the lowest forgetting (\(8.506\%\)) simultaneously, significantly improving on the best baseline DER++ (average accuracy of \(84.258\%\) and forgetting of \(13.692\%\)). Interestingly, such improvement is achieved when our UDIL's \(_{i}\) is high, which further verifies that UDIL indeed leverages the similarities shared across different domains so that the generalization error is reduced.

### Sequential CORe50

CORe50 [55; 56] is a real-world continual object recognition dataset that contains 50 domestic objects collected from 11 domains (120,000 images in total). Prior work has used CORe50 for settings such as domain generalization (e.g., train a model on only 8 domains and test it on 3 domains), which is different from our domain-incremental learning setting. To focus the evaluation on alleviating catastrophic forgetting, we retain \(20\%\) of the data as the test set and continually train the model on these 11 domains; we therefore call this dataset variant _Seq-CORe50_. Table 3 shows that our UDIL outperforms all baselines in every aspect on _Seq-CORe50_. Besides the average accuracy over all domains, we also report average accuracy over different domain intervals (e.g., \(}_{1:3}\) denotes average accuracy from domain 1 to domain 3) to show how different model's performance drops over time. The results show that our UDIL consistently achieves the highest average accuracy until the end. It is also worth noting that UDIL also achieves the best performance on another two metrics, i.e., forgetting and forward transfer.

## 6 Conclusion

In this paper, we propose a principled framework, UDIL, for domain incremental learning with memory to unify various existing methods. Our theoretical analysis shows that different existing methods are equivalent to minimizing the same error bound with different _fixed_ coefficients. With this unification, our UDIL allows _adaptive_ coefficients during training, thereby always achieving the tightest bound and improving the performance. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. One limitation of this work is the implicit _i.i.d._ exemplar assumption, which may not hold if memory is selected using specific strategies. Addressing this limitation can lead to a more powerful unified framework and algorithms, which would be interesting future work.

  
**Method** & **Buffer** & \(_{1:3}\) & \(_{1:4.6}\) & \(_{7:9}\) & \(_{10:11}\) & & **Overall** \\   & & & Avg. Acc (\(\)) & & Avg. Acc (\(\)) & Forgetting (\(\)) & Fwd. Transfer (\(\)) \\   Fine-tune & - & 73.707\(\)13.144 & 34.551\(\)1.254 & 29.406\(\)2.59 & 28.689\(\)1.344 & 31.832\(\)1.044 & 73.296\(\)1.399 & 15.153\(\)0.258 \\  oEWC  & - & 74.567\(\)21.360 & 35.915\(\)0.200 & 30.174\(\)2.135 & 28.291\(\)5.252 & 30.813\(\)1.154 & 74.563\(\)0.037 & 15.041\(\)0.240 \\ SI  & - & 74.661\(\)11.62 & 34.354\(\)1.001 & 30.127\(\)2.971 & 28.839\(\)3.631 & 32.469\(\)1.135 & 73.144\(\)1.880 & 14.837\(\)1.005 \\ LwF  & - & 80.332\(\)10.104 & 28.357\(\)1.140 & 31.366\(\)0.287 & 28.711\(\)2.988 & 72.990\(\)1.390 & 15.356\(\)0.270 \\  GEM  & - & 98.826\(\)0.861 & 38.961\(\)1.178 & 39.258\(\)2.614 & 38.569\(\)0.682 & 37.701\(\)0.273 & 22.272\(\)1.554 & 19.030\(\)0.936 \\ A-GEM  & 80.348\(\)9.941 & 41.723\(\)3.943 & 43.213\(\)3.154 & 39.181\(\)1.899 & 43.181\(\)2.053 & 37.375\(\)3.500 & 19.033\(\)0.972 \\ ER  & 90.838\(\)2.177 & 79.343\(\)2.698 & 68.151\(\)2.026 & 65.034\(\)1.571 & 66.605\(\)0.244 & 32.750\(\)0.455 & 21.735\(\)0.802 \\ DER++  & 500 & 92.444\(\)1.204 & 88.652\(\)1.084 & 90.911\(\)2.07 & 78.038\(\)0.091 & 78.629\(\)0.721 & 21.910\(\)1.082 & 22.488\(\)1.002 \\ CLS-ER  & 89.834\(\)1.323 & 78.909\(\)1.724 & 70.591\(\)0.322 & * & * & * & * & * \\ ESM-ER  & 84.905\(\)4.671 & 51.905\(\)3.257 & 53.815\(\)1.770 & 50.178\(\)2.574 & 52.751\(\)1.296 & 25.444\(\)0.590 & 21.435\(\)1.018 \\ UDIL (Ours) & **98.152\(\)1.469** & **89.814\(\)1.201** & **83.052\(\)1.815** & **81.547\(\)1.209** & **82.103\(\)0.279** & **19.589\(\)4.803** & **31.215\(\)0.831** \\  Joint (Oracle) & \(\) & - & - & - & - & 99.137\(\)0.000 & - & - \\   

Table 3: **Performances (%) evaluated on _Seq-CORe50_. We use three metrics, Average Accuracy, Forgetting, and Forward Transfer, to evaluate the methods’ effectiveness. “\(\)” and “\(\)” mean higher and lower numbers are better, respectively. We use boldface and underlining to denote the best and the second-best performance, respectively. We use “-” to denote ”not applicable” and “\(\)” to denote out-of-memory (_OOM_) error when running the experiments.**