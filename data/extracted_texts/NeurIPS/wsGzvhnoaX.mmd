# Quantum Algorithms for Non-smooth Non-convex Optimization

Chengchang Liu

1 The Chinese University of Hong Kong

2 University of Cincinnati

7liuchengchang@gmail.com

jianhaohe9@cuhk.edu.hk

Chaowen Guan

2 University of Cincinnati

guance@ucmail.uc.edu

jianhaohe9@cuhk.edu.hk

Jianhao He

1 The Chinese University of Hong Kong

2 University of Cincinnati

7liuchengchang@gmail.com

jianhaohe9@cuhk.edu.hk

cslui@cse.cuhk.edu.hk

John C.S. Lui

1 The Chinese University of Hong Kong

2 University of Cincinnati

7liuchengchang@gmail.com

jianhaohe9@cuhk.edu.hk

###### Abstract

This paper considers the problem of finding the \((,)\)-Goldstein stationary point of the Lipschitz continuous objective, which is a rich function class to cover a large number of important applications. We construct a novel zeroth-order quantum estimator for the gradient of the smoothed surrogate. Based on such estimator, we propose a novel quantum algorithm that achieves a query complexity of \(}d^{3/2}^{-1}^{-3}\) on the stochastic function value oracle, where \(d\) is the dimension of the problem. We also improve the query complexity to \(}(d^{3/2}^{-1}^{-7/3})\) by introducing a variance reduction variant. Our findings demonstrate the clear advantages of using quantum techniques for non-convex non-smooth optimization, as they outperform the optimal classical methods in dependence on \(\) by a factor of \(^{-2/3}\).

## 1 Introduction

In this paper, we study the following problem

\[_{^{d}}f()_ {}[F(;)]},\] (1)

where the stochastic component \(F(;)\) is \(L\)-Lipschitz continuous but possibly _non-convex_ and _non-smooth_. This problem has received increasing attention recently because it is general enough to cover many important applications, including deep neural networks , reinforcement learning , and statistical learning .

Due to the absence of both smoothness and convexity in the objective function, neither the gradient nor the sub-differentials are valid anymore to measure the convergence behavior. The Clarke subdifferential is a natural extension for describing the first-order information of the Lipschitz continuous function , however, it is intractable for finding the near-approximate stationary point in terms of the Clarke subdifferential as suggested by the hard instances . Zhang et al.  introduce the notion of \((,)\)-Goldstein stationary point (cf. Section 2.2), which weakens the traditional stationary point by considering the convex hull of the Clarke subdifferentials. Following this, we focus on the problem of finding the \((,)\)-Goldstein stationary points of the objective.

There are many optimization methods for finding the \((,)\)-Goldstein stationary points via classical stochastic oracles . Zhang et al.  proposed stochastic interpolated normalized gradient descent method (SINGD) with the first non-asymptotic result, which has the stochastic first-order complexity of \((^{-1}^{-4})\). Later, Tian et al.  developed the perturbedSINGD method which queries the gradient at the differentiable point and established the same complexity. Cutkosky et al.  improved the stochastic first-order oracle complexities to \((^{-1}^{-3})\) by using the "online to non-convex conversation", assuming \(f()\) is differentiable. This improvement aligns with the theoretical lower bound .

Zeroth-order methods, which only query the function value oracle, are more practical for the Lipschitz continuous objective. This is because computing first-order oracles can be extremely challenging [29; 52] or even inaccessible for numerous real-world applications [16; 27; 43]. Lin et al.  proposed a gradient-free method to find the \((,)\)-Goldstein stationary point within \((d^{3/2}^{-1}^{-4})\) query complexity to the stochastic function value via a connection between the randomized smoothing  and the Goldstein stationary point. This complexity was further improved to \((d^{3/2}^{-1}^{-3})\) and \((d^{-1}^{-3})\) by Chen et al. , Kornowski and Shamir  respectively. However, all these methods using the classical oracles to find the Goldstein stationary point face a bottleneck of \(^{-1}^{-3}\) due to the lower bound reported by .

Recently, we have witnessed the power of quantum optimization methods by accessing the quantum counterparts of classical oracles for _non-convex_ optimization [7; 23; 37; 48; 61; 64], _convex_ optimization [4; 5; 48; 55; 64], and semi-definite programming [1; 2; 53; 54]. However, most of these results focus on deterministic methods and the case where the objective function is _smooth_. Garg et al.  and Zhang and Li  showed the negative results for _non-smooth convex_ and _smooth non-convex_ optimization that quantum algorithms have no improved rates over classical ones when the dimension is large. Sidford and Zhang  proposed stochastic quantum methods which show the advantage of using quantum stochastic first-order oracles for _smooth_ objectives when the dimension is relatively small. To the best of our knowledge, there is no work showing the quantum speedups for minimizing _non-smooth non-convex_ objectives, which is the most general and fundamental function class. Based on this, it is a natural question to ask:

_Can we go beyond the complexity of \((^{-1}^{-3})\) to find the \((,)\)-Goldstein stationary point for non-smooth non-convex stochastic optimization by involving quantum oracles?_

We give an affirmative answer to the above question by proposing novel quantum zeroth-order methods and showing their explicit query complexities. We summarize our contributions as follows.

* We construct efficient quantum gradient estimators for the smoothed surrogate of the objectives with \((1)\)-queries of the function value oracles, which allows us to construct efficient quantum

  
**Methods** & **Oracle** & **Query Complexity** & **Reference** \\    &  & \((d^{3/2}^{-1}^{-4})\) & Lin et al.  \\   &  & \((d^{3/2}^{-1}^{-3})\) & Chen et al.  \\  OptimalZO &  & \((d^{-1}^{-3})\) & Kornowski and Shamir  \\   &  & \(}(d^{3/2}^{-1}^{-3})\) & Theorem 4.1 \\   &  & \(}(d^{3/2}^{-1}^{-7/3})\) & Theorem 4.3 \\  

Table 1: We summarize the complexities of classical and quantum zeroth-order methods for finding the \((,)\)-Goldstein point of a _non-smooth non-convex_ objective, where \(d\) is the dimension of the problem.

  
**Methods** & **Oracle** & **Query Complexity** & **Reference** \\    &  & \((^{-3})\) & Fang et al. , Li et al.  \\   &  & \(}(d^{1/2}^{-5/2})\) & Sidford and Zhang  \\   &  & \(}(d^{1/2}^{-7/3})\) & Theorem G.1 \\  

Table 2: We summarize the complexities of classical and quantum first-order methods for finding the \(\)-stationary point of a _smooth non-convex_ objective, where \(d\) is the dimension of the problem.

zeroth-order methods. Moreover, we provide explicit constructions of quantum superposition over required distributions. We present these results in Section 3 and Appendix A.
* We propose the quantum gradient-free method (QGFM) and the fast quantum gradient-free method (QGFM+) for _non-smooth non-convex_ optimization. We achieve the query complexities of \((d^{3/2}^{-1}^{-3})\) for QGFM and \((d^{3/2}^{-1}^{-7/3})\) for QGFM+ in finding the \((,)\)-Goldstein stationary point using quantum stochastic function value oracle. The query complexity of QGFM+ surpasses the optimal result achieved by classical methods by a factor of \(^{-2/3}\). We compare our methods with the classical zeroth-order methods in Table 1 and present the results in Section 4.
* We generalize the algorithm framework of QGFM+ for _smooth non-convex_ optimization (i.e. the gradient of the objective function is Lipschitz continuous). We propose the fast quantum gradient method (QGM+), which takes the advantage of QGFM+ to choose the variance level adaptively. QGM+ enjoys an improved complexity of \(}(d^{1/2}^{-7/3})\) queries of the quantum stochastic gradient oracle, which outperforms the existing state-of-the-art method (Q-SPIDER ) by a factor of \(^{-1/6}\). We compare our method with the classical and quantum first-order methods in Table 2. A discussion of this is presented in Remark 4.5, and the formal results are stated in Appendix G.

## 2 Preliminaries

We introduce preliminaries for quantum computing model and non-smooth non-convex optimization in this section.

### Preliminaries for Quantum Computing Model

Here we formally review the basics and some concepts from quantum computing with which we work. For more details, see Nielsen and Chuang .

**Quantum Basics.** A quantum state can be seen as a vector \(=(x_{1},x_{2},,x_{m})^{}\) in the Hilbert space \(^{m}\) such that \(_{i}|x_{i}|^{2}=1\). We follow the Dirac bra/ket notation on quantum states, i.e., we denote the quantum state for \(\) by \(}\) and denote \(^{}\) by \(}\), where \(\) means the Hermitian conjugation.

Given a state \(=_{i=1}^{m}c_{i}|i\), we call \(c_{i}\) the amplitude of the state \(\). Given two quantum states \(}^{m}\) and \(}^{m}\), we denote their inner product by \(}}_{i}x_{i}^{}y_{i}\). Given \(}^{m}\) and \(}^{n}\), we denote their tensor product by \(}}(x_{1}y_{1},,x_{m}y_{n })^{}^{m n}\). If we measure state \(=_{i=1}^{m}c_{i}|i\) on a computational basis, we will obtain \(i\) with probability \(|c_{i}|^{2}\) and the state will collapse into \(\) after measurement for all \(i\). A quantum algorithm works by applying a sequence of unitary operators to a initial quantum state.

**Quantum Query Complexity.** Corresponding to the classical query model, quantum query complexity considers the number of queries to a black box of a particular function which needs to be invoked to solve a problem. In many cases, the black box corresponds to the process that has the highest overhead, and therefore reducing the number of queries to it will effectively reduce the computational complexity of the entire algorithm. For example, if a classical oracle \(_{f}\) for a function \(f\) is a black box that, when queried with a point \(\), outputs the function value \(_{f()}=f()\), then the corresponding quantum oracle \(_{f}\) is a unitary transformation that maps a quantum state \(}\) to the state \(})}\). Moreover, given the superposition input \(_{,q}_{,q}}\), applying the quantum oracle once will, by linearity, output the quantum state \(_{,q}_{,q}})}\).

### Preliminaries for Non-convex Non-smooth Optimization

We introduce the necessary background for non-convex non-smooth optimization, with the following mild assumption that the objective function is Lipschitz continuous.

**Assumption 1**.: _We assume the stochastic component \(F(;)\) of the objective \(f()\) satisfies that \(|F(;)-F(;)| L\|-\|\) for every \(,^{d}\). In addition, we assume \(f:^{d}\) is lower bounded and denote \(f^{*}_{^{d}}f()\)._The Rademencher theorem indicates that \(f()\) is differentiable almost everywhere under Assumption 1, which allows us to define its Clarke subdifferential as follows .

**Definition 2.1** (Clarke sub-differential).: _The Clarke sub-differential of a Lipschitz function at point \(\) is defined by \( f()\{:= _{_{k}} f(_{k})\}\)._

We then introduce the Goldstein subdifferential  and the \((,)\)-Goldstein stationary point .

**Definition 2.2** (Goldstein sub-differential).: _The Goldstein subdifferential of a Lipschitz function at point \(\) is defined by \(_{}f()\{_{ _{}()} f()\}\)._

**Definition 2.3** (\((,)\)-Goldstein stationary point).: _We call \(\) the \((,)\)-Goldstein stationary point of a given Lipschitz function if it satisfies \((0,_{}f())\), where \(_{}f()\) is the Goldstein subdifferential._

Next, we define the smoothed surrogate of \(f()\) as follows.

**Definition 2.4** (\(\)-smoothed surrogate).: _The \(\)-smoothed surrogate of \(f\) is defined by_

\[f_{}()_{}[f (+)],\] (2)

_where \(\) is the uniform distribution on a unit ball._

Although \(f()\) is non-smooth, its smoothed surrogate \(f_{}()\) enjoys some good properties as presented in the following proposition .

**Proposition 2.1**.: _If \(f()\) satisfies Assumption 1, its smoothed surrogate \(f_{}()\) satisfies that:_

* \(|f_{}()-f()| L\) _and_ \(|f_{}()-f_{}()| L\|-\|\)_._
* \( f_{}()\) _is_ \(cL^{-1}\)_-Lipschitz for some constant_ \(c>0\)_, i.e._ \(\| f_{}()- f_{}()\| cL\|-\|\)_._
* \( f_{}()_{}f()\)_, where_ \(_{}f()\) _is the Goldstein subdifferential._

_Remark 2.2_.: Proposition 2.1 implies that the task of finding the \((,)\)-Goldstein stationary point of \(f()\) is equivalent to finding the \(\)-stationary point of a smoothed function \(f_{}()\), i.e. finding some point \(\) such that \(| f_{}()|\).

## 3 Zeroth-order Based Stochastic Quantum Estimator

In this section, we present a novel quantum estimator for the gradient of the smoothed surrogate \(f_{}()\) by using the quantum stochastic function value oracle, which is essential for designing our quantum algorithms for non-convex non-smooth optimization.

### Quantum Estimators via Quantum Stochastic Function Value Oracle

In this section, we construct quantum estimators for the gradient of the smoothed surrogate by \((1)\) -queries of the quantum stochastic function value oracle.

We start with the definition of the stochastic function value oracle. Classically, a stochastic function value evaluation is defined as \(F(,)\) for a function \(f:^{d}\) with \(\) such that \(_{}[F(,)]=f()\). In this work, we assume access to a _quantum_ stochastic function value oracle \(_{F}\) for \(f()\), which is defined as follows.

**Definition 3.1** (Quantum stochastic function value oracle).: _For \(f:^{d}\), the quantum stochastic function value oracle, denoted by \(_{F}\), works as: \(_{F}:|)|)|b)|) |)|b+F(,)), where \(F(,)\) is sampled from a distribution \(p_{}()\) such that \(_{}[F(;)]=F()\)._

It is common to construct the following stochastic gradient estimator for \( f_{}()\):

\[_{}(;,) (F(+;)-F(-;) ),\] (3)

where \(^{d}\) is uniformly distributed on a unit sphere. The following proposition shows that \(_{}(;,)\) is a good estimator of \( f_{}()\).

**Proposition 3.1** ([6, Proposition 3 and 4]).: _Under Assumption 1, i.e. the random variable \(\) satisfies that_

\[|F(;)-F(;)| L\|-\|_{}[F(;)]=f(),\] (4)

_hold for all \(,^{d}\), then \(_{}(;,)\) defined in eq. (3) satisfies that \(_{,}[_{}(;,)]=  f_{}()\), \(_{,}[\|_{}(;,)-  f_{}()\|^{2}] c dL^{2}\), and \(_{,}[\|_{}(;,)- _{}(;,)]^{2}L^{2}}{ ^{2}}|-\|^{2}\), where \(c=16\)._

Next, to exploit the power of quantum algorithms, we generalize eq. (3) to its quantum counterpart. Based on eq. (3) and Proposition 3.1, \(_{}(;,)\) can be interpreted as a random variable. In the quantum setting, accessing a random variable typically involves querying a _quantum sampling oracle_, which returns a quantum superposition over the associated distribution.

**Definition 3.2** (Quantum sampling oracle).: _For a random variable \(X\) with sample space \(\), its quantum sampling oracle \(_{X}\) is defined as \(_{X}:|0_{}]}| |_{},\) where \(|_{}\) is an arbitrary quantum state for every \(\)._

The content in the second quantum register can also be viewed as possible quantum garbage appearing during the implementation of the oracle. Observe that if we directly measure the output of \(_{X}\), it will collapse to a classical sampling access to \(X\) that returns a random sample \(\) with respect to probability \([X=]\). Note that the output of \(_{X}\) can also be represented as integral to continuous random variables, as used in .

Hence, based on our observation that \(_{}(;,)\) can be viewed as a random variable, our target oracle \(_{_{}}\)-quantum stochastic gradient oracle-is essentially a quantum sampling oracle. Given this, we formally define the quantum \(\)-estimated stochastic gradient oracle as follows.

**Definition 3.3** (Quantum \(\)-estimated stochastic gradient oracle).: _For \(f_{}():^{d}\), its quantum \(\)-estimated stochastic gradient oracle is defined as_

\[_{^{}}:|| ||_{, },]}|_{}(;,)|_{,},\]

_where the random variable \(\) is uniformly distributed in a unit sphere and \(\) satisfies eq. (4)._

Proposition 3.1 implies \(_{}()\) can serve as an estimator of \( f_{}\), and can be calculated with access to a quantum \(\)-estimated stochastic gradient oracle as defined above. The following theorem shows that such an oracle can be built with only \((1)\) access to the quantum stochastic function value oracle.

**Lemma 3.2**.: _Given access to a quantum sampling oracle \(_{,}\) to the joint distribution on \((,)\), one can construct a quantum \(\)-estimated stochastic gradient oracle (as defined in Definition 3.3) with two queries to the quantum stochastic function value oracle \(_{F}\)._

_Remark 3.3_.: In Lemma 3.2, we assume a black-box access to quantum sampling oracle \(_{,}\) following Sidford and Zhang . We present the explicit construction of this oracle in Appendix A.

Similarly, we can also constructed the estimator of \( f_{}()- f_{}()\) by the following oracle:

\[_{^{}}:|| ||||_{,},]}|_{}(;,)-_{ }(;,)|_{,},\]

with only \((1)\)-queries of stochastic quantum function value oracle.

**Corollary 3.4**.: _Under the same conditions as in Lemma 3.2, one can construct \(_{_{}}\) with four queries to the quantum stochastic function value oracle \(_{F}\)._

### Mini-batch Quantum Estimators via Quantum Mean Estimation

We constructed the quantum oracles \(_{_{}}\) and \(_{_{}}\) with \((1)\)-queries of quantum function value oracles in Section 3.1. These oracles produce outputs in the form of random variables. Specifically,\(_{_{}}\) provides an output with expectation \( f_{}()\) with the input \(\), and \(_{_{}}\) provides an output with expectation \( f_{}()- f_{}()\) for \(_{_{}}\) with the inputs \(\) and \(\).

The variance of the outputs can be reduced by constructing the mini-batch estimator. Inspired by the recent advance on quantum mean estimation [11; 12; 48] which improve the classical mini-batch estimator for multi-dimensional random variables, we construct improved estimators for \( f_{}()\) and \( f_{}()- f_{}()\). We formally present the results in the following theorem.

**Theorem 3.5**.: _Under Assumption 1, and given access to a quantum sampling oracle \(_{,}\) to the joint distribution on \((,)\), it holds that:_

1. _there exists an algorithm that can construct an unbiased quantum estimator_ \(}\) _of_ \( f_{}()\) _such that_ \(}- f_{}() ^{2}_{1}^{2}\) _within_ \(}(dL_{1}^{-1})\) _queries of_ \(_{F}\) _in expectation._
2. _there exists an algorithm that can construct an unbiased quantum estimator_ \(\) _of_ \( f_{}()- f_{}()\) _such that_ \(-( f_{}()-  f_{}())^{2}_{2}^{2}\) _within_ \(}(d^{3/2}L\|-\|_{2}^{-1} ^{-1})\) _queries of_ \(_{F}\) _in expectation._

_Remark 3.6_.: Compared to the classical mini-batch estimator for \( f_{}()\), which requires \((dL^{2}_{1}^{-2})\) queries of \(_{F}\) to achieve the level of variance \(_{1}^{2}\) ([6, Corollary 2.1]), our mini-batch quantum estimator for \( f_{}()\) in Theorem 3.5 reduces a factor of \(L_{1}^{-1}\) without increasing the dimension dependence.

## 4 Quantum Algorithms for Finding the Goldstein Stationary Point

In this section, we develop novel quantum algorithms for finding the \((,)\)-Goldstein stationary point of a non-smooth non-convex objective \(f()\). Instead of finding the stationary point directly, we consider finding the \(\)-stationary point of its smoothed surrogate \(f_{}()\), which is equivalent to the original problem according to Remark 2.2. The classical zeroth-order methods based on such equivalence require to access the gradient estimator to \( f_{}()\) by stochastic function values [6; 32; 35; 36]. Different from the classical methods, we can take the advantage of the quantum estimators, which can be constructed by accessing quantum stochastic function value oracles due to our novel results in Section 3.

We first propose an algorithm which uses the quantum gradient estimator to replace \( f_{}()\) to do the gradient descent step at each iteration. We present the quantum gradient-free method (QGFM) in Algorithm 1. Given a desired variance level \(_{t}^{2}\), line 2 of Algorithm 1 can be constructed explicitly and efficiently by the quantum stochastic function value oracles \(_{F}\) according to Theorem 3.5. The following theorem gives the upper bound on the total \(_{F}\) that Algorithm 1 require to access for finding the \((,)\)-Goldstein stationary point.

**Theorem 4.1**.: _Under Assumption 1, by setting the parameter in Algorithm 1 as \(=/(2d^{1/2}L)\) and \(_{t}^{2}^{2}/2,\) then the total queries of stochastic quantum function value oracle \(_{F}\) for finding the \((,)\)-Goldstein stationary point of \(f()\) can be bounded by \(}(d^{3/2}(}{^{3}}+ }{^{3}})),\) where \(=f(_{0})-f^{*}\)._

_Remark 4.2_.: QGFM(Algorithm 1) speedups the gradient-free method (GFM)  for finding \((,)\)-stationary point by a factor of \(L^{-1}\).

In particular, Algorithm 1 utilized a simple gradient descent step to achieve \((^{-1}^{-3})\), which is optimal for classical zeroth-order and first-order methods in terms of \(\) and \(\). It is worth mentioning that the classical methods that achieve this lower bound typically involve multiple loops  or rely on additional online optimization algorithms [13; 32].

To further enhance the query complexity in Theorem 4.1, we propose the fast quantum gradient-free method (QGFM+) by incorporating variance reduction techniques, as outlined in Algorithm 2.

QGFM+ can be seen as a quantum-accelerated version of GFM+ . Unlike GFM+, which required double loops, QGFM+ simplifies the implementation by using a single loop based on the PAGE framework . Moreover, we replace all classical estimators with quantum estimators in lines 6 and 8 of Algorithm 2. These quantum estimators can be constructed efficiently using stochastic quantum function value oracles with a desired variance level, as demonstrated in Theorem 3.5. We present the total number of queries of \(_{F}\) for QGFM+ in the following theorem. We present the total queries of \(_{F}\) for QGFM+ in the following theorem.

**Theorem 4.3**.: _Under Assumption 1, by setting the parameters in Algorithm 2 as follows_

\[=/2d^{1/2}L,\ \ \ p_{t}^{2/3}/L^{2/3},\ \ \ _{1,t}^{2}=^{2}/2,\ \ \ and\ \ \ _{2,t}^{2}=^{2/3}L^{4/3}d\|_{t}-_{t-1} \|^{2}/^{2},\]

_then the total queries of stochastic quantum function value oracle \(_{F}\) for finding the \((,)\)-Goldstein stationary point of \(f()\) can be bounded by \(}(d^{3/2}(}{^{7/3}}+}{^{7/3}})),\) where \(=f(_{0})-f^{*}\)._

_Remark 4.4_.: QGFM+ (Algorithm 2) speedups the GFM+  for finding \((,)\)-stationary point by a factor of \(L^{-2/3}\).

We can see that QGFM+ achieves the query complexity of \(}(d^{3/2}^{-7/3}^{-1})\), which cannot be achieved by any of the classical methods. Furthermore, we observe the applicability of our framework to _smooth non-convex_ optimization.

_Remark 4.5_.: QGFM+ is different from the quantum speedups algorithm (Q-SPIDER) for _non-convex smooth_ stochastic optimization : QGFM+ adjusts the variance level of \(_{t}\) according to the difference between the current iteration point and the previous one, while Q-SPIDER fixes the variance levels. Using the adaptive variance level and the QGFM + framework, we can further accelerate the Q-SPIDER for _smooth non-convex_ optimization. In Appendix G, we propose the fast quantum gradient method (QGM +) with the query complexity of \(}(^{-7/3})\), which improves the one of \(}(^{-5/2})\) obtained in Sidford and Zhang .

```
1: Construct \(_{0}\) as an unbiased estimator of \( f_{}(_{0})\) with variance at most \(_{1,0}^{2}\).
2:for\(t=0,1 T\)do
3:\(_{t+1}=_{t}-_{t}\)
4: Flip a coin \(_{t}\{0,1\}\) where \(P(_{t}=1)=p_{t}\)
5:If\(_{t}=1\)then
6: Construct \(_{t+1}\) as an unbiased quantum estimator of \( f_{}(_{t+1})\) with variance at most \(_{1,t+1}^{2}\) using \(_{F}\) according to Theorem 3.5.
7:else
8: Construct \(_{t+1}\) as an unbiased quantum estimator of \( f_{}(_{t+1})\) - \( f_{}(_{t})\) with variance at most \(_{2,t+1}^{2}\) using \(_{F}\) according to Theorem 3.5.
9:\(_{t+1}=_{t}+_{t+1}\).
10:endfor ```

**Algorithm 2** Fast Quantum Gradient-Free Method (QGFM+)

## 5 Conclusion and Future Work

In this paper, we have presented quantum algorithms for finding the \((,)\)-Goldstein stationary point for a non-smooth non-convex objective. Our query complexities demonstrate a clearly quantum speedup over the classical methods. In future work, it would be intriguing to explore the framework without ideal distributions which is caused by the limitation of classical or quantum resources. It is also interesting to find the quantum speedups for deterministic methods [14; 28; 51] or the NS-NC objective with constraints . We are also interested in seeing if similar strategies can be applied to quantum online optimization with zeroth-order feedback [25; 26; 33; 56; 58]. The query complexity of the proposed methods still have heavy dependency on the dimension; it is also possible to reduce the dimension dependency based on other quantum techniques and design efficient first-order quantum methods.