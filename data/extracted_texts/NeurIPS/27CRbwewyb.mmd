# Noether Embedding:

Efficient Learning of Temporal Regularities

 Chi Gao\({}^{}\) Zidong Zhou\({}^{}\) Luping Shi\({}^{*}\)

Center for Brain-Inspired Computing Research,

Optical Memory National Engineering Research Center,

Tsinghua University - China Electronics Technology HIK Group

Co. Joint Research Center for Brain-Inspired Computing,

IDG / McGovern Institute for Brain Research at Tsinghua University,

Department of Precision Instrument,

Tsinghua University, Beijing 100084, China.

{gaoc20, zzd21}@mails.tsinghua.edu.cn

lpshi@mail.tsinghua.edu.cn

Equal contribution.Corresponding author.The code is publicly available at: https://github.com/KevinGao7/Noether-Embedding.

###### Abstract

Learning to detect and encode temporal regularities (TRs) in events is a prerequisite for human-like intelligence. These regularities should be formed from limited event samples and stored as easily retrievable representations. Existing event embeddings, however, cannot effectively decode TR validity with well-trained vectors, let alone satisfy the efficiency requirements. We develop Noether Embedding (NE) as the first efficient TR learner with event embeddings. Specifically, NE possesses the intrinsic time-translation symmetries of TRs indicated as conserved local energies in the embedding space. This structural bias reduces the calculation of each TR validity to embedding each event sample, enabling NE to achieve data-efficient TR formation insensitive to sample size and time-efficient TR retrieval in constant time complexity. To comprehensively evaluate the TR learning capability of embedding models, we define complementary tasks of TR detection and TR query, formulate their evaluation metrics, and assess embeddings on classic ICEWS14, ICEWS18, and GDELT datasets. Our experiments demonstrate that NE consistently achieves about double the F1 scores for detecting valid TRs compared to classic embeddings, and it provides over ten times higher confidence scores for querying TR intervals. Additionally, we showcase NE's potential applications in social event prediction, personal decision-making, and memory-constrained scenarios.

## 1 Introduction

Recall the last time you went to a restaurant but waited for half an hour after ordering dishes. You probably knew something was wrong and may have called the waitperson for help. This behavior is guided by the temporal regularity (TR) of 'order dishes -(about 10 minutes)-> have meals' stored in your brain as schemas (Ghosh & Gilboa, 2014). Such TRs play a significant role in enabling humans to exhibit flexible out-of-distribution and systematic generalization abilities (Goyal & Bengio, 2022), and are directly learned from experience through a statistical accumulation of common event structures (Pudhiyidath et al., 2020), as shown in Figure 1. Since there exist enormous potential TRs due to a large number of event types and time intervals, detecting valid TRs from allpotential ones is therefore necessary, serving as a prerequisite capability for humans to form more complex event schemas in the brain to support downstream cognitive functions (Schapiro et al., 2017; McClelland et al., 1995). To attain human-level abilities in event scenarios, it is crucial to possess two fundamental properties when learning TRs. Firstly, TRs should be formed from even a limited number of experiences, which humans achieve since childhood (Pudhiyjidath et al., 2020). Secondly, TRs should be stored as representations that can be instantly retrieved given appropriate cues, which is a central feature of human memory (Chaudhuri and Fiete, 2016).

It remains an open problem to jointly achieve the data-efficient formation and time-efficient retrieval of 1-1 TRs with event embeddings, although embedding models have achieved outstanding performance in various tasks such as event completion and event prediction (Cai et al., 2022; Zhao, 2021). Classic event embeddings can only encode patterns such as inversion and composition and decode the fitted event occurrences for better performance in completion tasks (Xu et al., 2020; Wang et al., 2020; Messner et al., 2022). However, we aim to encode TRs by directly training the embeddings of each event sample and decode TRs by calculating well-trained embeddings without first decoding the fitted event occurrences. Our primary challenge in achieving such a counterintuitive function is to design the inductive bias that automatically integrates the event statistics of each potential TR over time.

Symmetry governs regularities in nature (Tanaka and Kunin, 2021), long before Noether proved the equivalence between symmetries and conservation laws in a physical system (Noether, 1918). Inspired by Noether's theorem, we develop Noether Embedding (NE) with the intrinsic time-translation symmetries of TRs indicated as conserved local energies in the embedding space. Calculating the event statistics of each potential TR is therefore converted to reducing the training loss of all event embeddings. This allows the direct revelation of TR validity by decoding the corresponding local energies through calculating well-trained embeddings after training convergence.

Contributions are twofold. Firstly, we develop NE which for the first time jointly achieves the data-efficient formation and time-efficient retrieval of TRs solely by embedding event samples. Secondly, we define complementary tasks of TR detection and TR query, formulate their evaluation metrics, and adopt classic datasets for evaluations, aiming at complete evaluations of embeddings' TR learning capabilities. Both our tasks and method generalize to arbitrary forms of structured events.

## 2 Problem Formalization

### Definitions

Temporal Regularity (TR)An event item \(q\) can generally be represented by the basic symbolic form of \((ev,t)\), where \(ev\) is the event type, and \(t\) is the discrete occurrence time. Building on the interpretations from cognitive science literature (Ghosh and Gilboa, 2014; Pudhiyidath et al., 2020), we formally define TRs as temporal associations that remain invariant to time shifts:

\[(ev_{b},t)(ev_{h},t+) t_{a}\] (1)

\(ev_{b} ev_{h},t,t+\) respectively refer to the body and head event type and their occurrence time. \(_{a}\) refers to the complete collection of the absolute time points in the whole event set, and \(\) denotes the relative time. Note that \(=0\) indicates the synchrony of body and head event occurrences.

Figure 1: Illustration of TR learning. TRs indicate temporal associations invariant to time shifts, and are learned from event items through statistical accumulation.

For example, a TR could be: Whenever someone orders dishes in a restaurant, he or she will have meals in around ten minutes, where \(=10\).

Metrics for TR ValiditySince real-world data often contain noise, we introduce an adaptive \(=[(1-),(1+)]\) to replace \(\) when evaluating statistically learned temporal regularities from events. For an evaluated TR abbreviated as \(tr:(ev_{b},ev_{h},,)\), if an event \(q:(ev,t)\) satisfies that \(ev=ev_{b}\), we denote it as \(b(q;tr)\); if \(ev=ev_{h}\), we denote it as \(h(q;tr)\). We define the support of a TR as the number of event pairs respectively satisfying the body and head:

\[sp(tr)=n(b(q;tr) h(q^{};tr)(t^{}-t)(, ))\] (2)

\(\) denotes 'and', \(\) denotes 'in', and \(q:(ev,t),q^{}:(ev^{},t^{})\) refer to arbitrary two different events in the event set. Note that when calculating \(sp(tr)\), we can only count one event once and in one pair to avoid overcounting when events occur in consecutive periods.

We respectively define the standard confidence, head coverage, and general confidence of a TR as:

\[sc(tr)=, hc(tr)=, gc( tr)=+\] (3)

\(n(b(tr)),n(h(tr))\) respectively represent the number of events \(q:(ev,t)\) satisfying \(ev=ev_{b},ev=ev_{h}\) in the event set. Here we borrow the metrics \(sc,hc,gc\) generally used in the rule mining field (Galarraga et al., 2015) to ensure fair and reasonable evaluations. We modify them by introducing an adaptive \(\) with \(\) to evaluate TR validity. Intuitively, standard confidence \(sc\) can be viewed as the probability that the head event will occur within time \(t+\) once a body event occurs at time \(t\), whose statistical sufficiency is supported by \(sp\). \(hc\) and \(gc\) can be interpreted similarly.

Above some \(sp\), the higher the \(gc\), the more valid a TR is. For a potential TR \(tr:(ev_{b},ev_{h},,)\) with fixed event types \(ev_{b},ev_{h}\) and ratio \(\), its general confidence can be written as a function of \(\): \(gc()\).

### Tasks

For a fixed \(\) in \(\), a potential TR can be an arbitrary \((ev_{i},ev_{j},)\), where \(i,j,_{r}\) (\(\) is the set of event types, \(_{r}\) is the set of relative time points). Therefore, we define the two complementary tasks below to comprehensively evaluate the TR learning capabilities of event embeddings.

TR DetectionFor a query \((ev_{b},ev_{h})\), its ground truth confidence \(gc_{g}=_{_{r}}gc()\). Queries whose \(gc_{g}\) are considered to imply valid TRs that reveal good regularities, while queries whose \(gc_{g}<\) are considered to imply invalid TRs, where \(\) is a fixed threshold.

The task of TR detection is to identify valid TRs from all tested ones. The model is expected to determine whether a query \((ev_{b},ev_{h})\) implies a valid TR. The F1 score of all judgments is reported.

TR QueryOnly queries that imply valid TRs are considered for testing. The ground truth relative time \(_{g}\) is set as what maximizes \(gc()\) in computing \(gc_{g}\). The model outputs \(^{}\).

The task of TR query is to output the correct \(^{}=_{g}\) for valid TRs. For each tested query \((ev_{b},ev_{h})\), a ratio \(r^{}=)}{gc_{g}}\) is computed. The averaged ratio \(r\) of all queries is reported.

## 3 Noether Embedding

#### 3.1.1 Noether's Theorem

Noether's theoremIn 1915, mathematician Emmy Noether proved one of the most fundamental theorems in theoretical physics: every differentiable symmetry of the action of a physical system with conservative forces has a corresponding conservation law. Specifically, time-translation symmetry corresponds to energy conservation.

TRs indicate time-translation symmetriesAn event pair \((ev_{b},t)(ev_{h},t+)\) can be regarded as a mapping of the body and the head event type over \(t\) with a parameter \(\). Therefore, ideal TRs indicate the invariance of such mappings under the transformation of time translation since the mapping holds \( t_{a}\) for TRs.

Construct embeddings with conserved local energiesDenote \((t;ev)\) as the embedding of each event sample, and \(g((t;ev_{b}),(t+;ev_{h}))\) as the local energy of a corresponding body-and-head event pair of a potential TR. If \(g\) is innately conserved, meaning that \(g=g(;ev_{b},ev_{h})\) is invariant to \(t\), it indicates time-translation symmetry. We can then use the value of \(g\) to approximate TR validity after training each event embedding \((t;ev)\). A more strict correspondence between NE variables and those in a physical system is shown in Appendix A.1.1.

Noether-Inspired structural biasesAccordingly, the enabling factors of NE can be summarized as follows: (i) the event embedding \(\) should be constructed to make each local energy \(g\) remain invariant to \(t\); (ii) the training loss should be designed to make the value of \(g\) approximate TR validity; (iii) the local energy \(g\) should be used as the decoding function. We thus construct NE as below.

### NE's Framework and Formulas

FrameworkAs illustrated in Figure 2, NE uses a distributed storage of complex vectors to learn TRs. At the encoding stage, the event items are converted to NE representations through embedding each event sample, where TR validity is automatically calculated and stored in the embedding space. At the decoding stage, given each query \((ev_{b},ev_{h})\), potential TRs are detected and queried by directly calculating their relevant embeddings to derive the corresponding decoding results \(g()\).

#### The Encoding Stage

\[(t;ev)=(ev)(t)\] (4)

In the event embedding above, \(\) denotes the Hadmard (or element-wise) product. \((ev),(t)^{d}\) are complex vectors where \((ev)=(ev)}{||(ev)||}\), \((ev)^{d}\), \((t)=e^{it}\), \(^{d}\), and \(d\) is the dimension of vectors. Each event type \(ev\) corresponds to an independently trainable vector of \((ev)\), while \(\) is a global time vector for \((t)\) of all event embeddings. Note that the \(d\)\(\)s in \(\) are fixed to a certain distribution. The event embedding \((t;ev)\) can thus be depicted as a rotation of event-type vectors \((ev)\) by time \((t)\) in the d-dimensional complex space.

The score function and loss function of each event sample are defined as follows:

\[f(t;ev)=_{i=1}^{d}Real((t;ev))_{i}\] (5)

\[L(;C_{p},C_{n})=(}f()-C_{p})^{2}+( }f(^{})-C_{n})^{2}\] (6)

We denote \(\) as a positive event sample and \(^{}\)s as its generated negative samples whose number is \(N\). For a positive sample \(:(ev,t)\), its negative samples \(^{}\)s are the whole set of \(\{(ev,t^{} t)\}\)

Figure 2: Illustration of NE. Solid lines and purple graphs in the middle jointly represent the data flow of NE. The red graphs below and the blue ones above demonstrate cases of a TR with significant temporal regularities and a TR with no. It is shown that each decoding result reveals an integrated temporal association of its relevant event pairs separated in time.

where \(t^{}_{a}\). \(C_{p},C_{n}\) are two different constants for positive and negative samples, respectively. \(C_{p},C_{n}[-1,1]\) because \(||(ev)||=1\), and we generally set \(C_{p}=1,C_{n}=0\).

Until the training converges, events and TRs form a distributed storage, which includes the global time vector of \(\) and trainable event type vectors \((ev)\). At the decoding stage, no training but only the inference of vectors is conducted, as described below.

The Decoding StageThe decoding function for a query \((ev_{b},ev_{h})\) is:

\[g()=||_{b}-_{h}()||^{2}\] (7)

\(_{b},_{h}\) are the event type vectors respectively of \(ev_{b},ev_{h}\). \(\) is traversed through set \(_{r}\) of the relative time points such as \(_{r}:\{-_{max},...,0,...,_{max}\}\) to plot the decoding results. \(_{_{r}}g()\) is computed, which is compared with a global threshold \(g_{th}\) to decide whether a potential TR is valid or not (for TR detection). For a valid TR, the \(^{}\) which minimizes \(g(),_{r}\) is selected as the model output of the relative time (for TR query).

Since \((t+)=(t)()\), \((t)(t)}=, t_{a}\), the decoding function exactly indicates the conserved local energy: \(g()=||_{b}-_{h}()||^{2}=||_{b}(t)-_{h}(t+)||^{2}=||_{b}(t)-_{h}(t+)||^{2},  t_{a}\). This indicates that \(g=g(;ev_{b},ev_{h})\) is invariant to \(t\), and the conserved energy \(g\) of arbitrary two event samples is of a quadratic form in the embedding space.

### Why NE is Efficient

Briefly speaking, the Fourier-like representations enable NE's large-capacity storage for TR validity and event occurrences, serving as a prerequisite for NE to learn TR effectively. The Noether-inspired structural biases further leads to NE's efficient TR learning capabilities. Here we only illustrate the effect of the structural biases. The reasons why NE learns TR effectively is explained in Appendix A.2.

Data-efficiency by Encoding Translation SymmetriesThe invariance of \(g()\) to \(t\) means that the value of \(g()\) is determined by a competitive effect between sample pairs across time. Considering event sample pairs \((ev_{b},t),(ev_{h},t+)\) with varying \(t\) in the embedding space, if a sample pair are both positive or both negative samples, they will decrease \(g()\) since their score functions are mapped to the same constant. Otherwise, they will increase \(g()\). Since \(g()\) is invariant to \(t\), \(g()\) is trained to balance these two forces. Therefore, the value of \(g()\) after training convergence is generally determined by the ratio of sample pairs with increasing or decreasing forces. \(g()\) is thus insensitive to the number of sample pairs that are both positive. This results in a data-efficient TR formation in NE, even with limited event occurrences from which to learn a TR.

Time-efficiency by Decoding Conserved EnergiesBy calculating \(g()=||_{b}-_{h}()||^{2},_{r}\), we enable efficient TR querying for each query \((ev_{b},ev_{h})\). This process has a constant time complexity since \(_{r}\) is an arbitrary user-selected set of relative time points, and the vector dimension \(d\) can be effectively handled using GPUs. Importantly, the querying time is independent of the number of events in the entire event set and the relevant event occurrences supporting the queried TR.

## 4 Experiment

It is important to highlight that in our evaluation, we initially compare NE and classic embeddings in terms of learning effectiveness (Section 4.2), without considering the efficiency requirements. As classic embeddings are shown to be ineffective in learning TRs, we then focus on demonstrating the learning efficiency of NE in Section 4.3.

### Experimental Setting

DatasetA temporal knowledge graph (TKG) comprises \((s,p,o,t)\) quadruples (Leblay and Chekol, 2018), where \(s,p,o,t\) represent the subject, predicate, object, and time. TKG is widely used in a variety of fields to represent global political events (Trivedi et al., 2017), financial incidents (Yang et al., 2019), user-item behaviors (Xiao et al., 2020), etc. Notably, ICEWS (Boschee et al., 2015) and GDELT (Leetaru and Schrodt, 2013) are two popular data sources for TKG research (Cai et al., 2022).

In our experiments, we use ICEWS14 and ICEWS18, the same as in (Han et al., 2020). They contain global political events in 2014 and 2018, and we denote them as D14 and D18, respectively. We also use the GDELT released by (Jin et al., 2019), which contains global social events from 2018/1/1 to 2018/1/31. In the experiments, we denote each \((s,p,o)\) triple as a specific event type \(ev\). It is worth mentioning that alternative settings, such as representing each predicate \(p\) as an event type \(ev\), are also applicable to our model.

Model ImplementationFor NE, \(d=400,C_{p}=1,C_{n}=0\) and the global time vector \(\) is set as \(_{k}=)^{}-1}{T_{a}},k=0,1,...,d-1\), where \(T_{a}\) is the number of absolute time points, and \(_{max}\) is a tunable hyperparameter set as 600. The training details are in Appendix B.1. We compare NE with six classic and vastly different TKG embeddings, DE-SimplE (Goel et al., 2020), TeRo (Xu et al., 2020b), ATiSE (Xu et al., 2020a), TNTComplex (Lacroix et al., 2020), BoxTE (Messner et al., 2022), and TASTER (Wang et al., 2023) with their original parameter settings and \(d=400\) the same as NE. We set the queried time set \(_{r}=\{1-T_{a},...,0,...,T_{a}-1\}\) at the decoding stage. In this way, only one query needs to be decoded between each \((ev_{i},ev_{j})\) and \((ev_{j},ev_{i})\), \(i j\).

Adaptations. All baselines can only output their score function \(f^{}(t)\) to decode event occurrences but cannot directly decode TR validity by \(g()\) as NE does. For comparison, we add an interface \(g^{}()=_{a}}f^{}_{b}(t)f^{ }_{b}(t+)}{_{t_{a}}f^{}_{b}(t)_{t _{a}}f^{}_{b}(t)}\) to these models that indirectly compute TR validity from the decoded event occurrences. We also evaluate NE with \(g^{}()\) to show the validity of \(g^{}()\) itself.

Evaluation DetailsWe select \((ev_{b},ev_{h})\)s for tests whose event occurrences of \(ev_{b}\) and \(ev_{h}\) are both \( 2\). Otherwise, their supports (\(sp\) in Definition 2) will be too small for evaluating TR validity. We set \(=0.1\) in \(\)s for strict evaluations and take the upper integer \(=[-,+]\). Note that in the extreme situation where body and head event occurrences both \(=2\), stochastic noises are still quite unlikely to interfere with the evaluation of TR validity since \(=0.1\) is strict. Only forward or reverse queries (\((ev_{b},ev_{h})\)s whose \(s_{b}=s_{h},o_{b}=o_{h}\) or \(s_{b}=o_{h},o_{b}=s_{h}\) for \((s,p,o,t)\) quadruples) are tested for better interpretability without sacrificing generality. We set \(=0.8\) to distinguish between valid and invalid TRs. The fact that TRs whose \(gc_{g} 0.8\) are of a tiny percentage of all tested TRs adds to the rationality of such metrics. Ablations where \(=0.7,0.9\) are in Appendix B.3.3.

In comparative studies with baselines 4.2, we report the highest F1 in TR detection by tuning the global threshold \(g_{th}\) (defined in Section 3.2) after embedding the whole event set to achieve full evaluations. We also remove TRs whose \(_{g}=0\) for TR query because they account for most valid TRs but can hardly reflect the query difficulty. In NE's demonstration studies 4.3 4.4, we first use D14 to derive the global threshold \(g_{th}\) with the highest F1 in TR detection and then apply the same \(g_{th}\) for evaluating NE's performance in D18. This setting better demonstrates NE's practicality.

### Comparisons of Learning Effectiveness

PerformancesTable 10 shows that NE with \(g()\) has an overwhelming advantage over all baselines with \(g^{}()\), both in detecting valid TRs and querying the relative time on all evaluated datasets. The

    &  &  \\  & D14 & D18 & GDELT & D14 & D18 & GDELT \\  TNTComplEx & 0.26 & 0.18 & 0.08 & 0.08 & 0.08 & 0.01 \\ DE-SimplE & 0.22 & 0.20 & - & 0.09 & 0.09 & - \\ TASTER & 0.18 & 0.15 & 0.08 & 0.09 & 0.09 & 0.00 \\ TeRo & 0.43 & 0.64 & 0.16 & 0.08 & 0.08 & 0.01 \\ BoxTE & 0.40 & 0.40 & 0.18 & 0.08 & 0.08 & 0.01 \\ ATISE & 0.40 & 0.44 & 0.18 & 0.08 & 0.08 & 0.01 \\  NE with \(g^{}()\) & 0.78 & 0.79 & 0.48 & 0.85 & 0.83 & 0.83 \\ NE with \(g()\) & **0.82** & **0.83** & **0.51** & **0.87** & **0.86** & **0.85** \\   

Table 1: Statistical results on ICEWS14, ICEWS18, and GDELTexcellent performance of NE with \(g^{}()\) indicates that \(g^{}()\) itself is valid and thus guarantees fair comparisons. It is worth noting that NE is intrinsically different from all existing baselines because only NE can directly decode TR validity by \(g()\). In contrast, baselines can only decode event occurrences \(f^{}(t)\) from which to indirectly calculate TR validity (such as by \(g^{}()\)). Detailed results of precision and recall rates with error bars are reported in Appendix B.2.

DiscussionFigure 3(a) shows NE's decoding distribution \(g_{m}=_{_{r}}g()\) by each query's ground truth \(gc_{g}=_{_{r}}gc()\). It can be observed that the decoded conserved local energy accurately reveals the TR validity, which enables NE to successfully distinguish between valid and invalid TRs, as demonstrated in the case shown in Figure 3(b). Table 10 shows that baselines with \(g^{}()\) still perform poorly. This is mainly because their \(f(t)\)s do not fill well. Specifically, Figure 3(c) illustrates that TNTComplEx has much noise in its \(f^{}(t)\) compared to NE. The reason is that baseline models are generally designed to achieve good performance in the completion task and, therefore, over-apply the generalization capabilities of distributed representations, which hinders the fit of event occurrences.

### NE's Superior Learning Capabilities

Data EfficiencyIn Figure 4(a) and 4(b), we group TRs by their number \(n\) of relevant events. It is shown that NE accurately detects valid TRs and reports correct \(\)s with only two event pairs as positive samples. This performance is comparable to humans, able to form temporally associative memories with minimal experience (Hudson et al., 1992; Bauer and Mandler, 1989; Schapiro et al., 2017). Note that the maximum group in the test has \(n>400\), while we only show groups with \(n 40\) for display considerations.

Time EfficiencyAs explained in 3.3, NE's specific decoding function \(g()\) enables NE to retrieve TRs in a constant time complexity by vector computations. Calculating \(g^{}()\) of classic embeddings, however, requires an additional time complexity relevant to \(T_{a}\) (the number of absolute time points).

Storage EfficiencyIn addition to the data-efficient and time-efficient properties, NE is, in fact, also a storage-efficient memory for TRs and event occurrences. Here is a detailed analysis:

Figure 4: Grouped performances of NE

Figure 3: Illustrations of NE or baselines

The storage of NE vectors, denoted as \(S(NE)\), can be calculated as follows: \(S(NE)=S(ev-vector)+S(time-vector)=2*N*d*64bit+2*d*64bit\). In our experiments, we used torch.LongTensor and N represents the number of event-type vectors. On the other hand, the storage of exact counting, denoted as \(S(CT)\), can be calculated as follows: \(S(CT)=S(TR)+S(event)=N^{2}*T_{a}*log_{2}(n/N)bit+N*(n/N)*log_{2}(T_{a})bit\). Here, \(n\) represents the number of all event occurrences. We reserved the storage accuracy of TR validity to effectively distinguish different values, resulting in approximately \(log_{2}(n/N)bit\) for each TR validity \((ev_{b},ev_{h},)\).

For the ICEWS14 and ICEWS18 datasets, where \(d=400,T_{a}=365\), and \(n=90730,468558,N=50295,266631\), we calculated the compression ratio \(\) of NE as 421 and 2336, respectively. This remarkable capability of NE can be attributed to the fact that it separately stores the information of TR validities \((ev_{b},ev_{h},)\) using event-type vectors and a global time vector. By representing the common information of related TRs efficiently in memory, NE achieves a compression ratio that is approximately linear to the number of event types.

FlexibilityIn Figure 4(c), we group valid TRs by their golden \(_{g}\)s. NE is shown to be flexible for learning TRs with \(\)s varying broadly, comparable to humans with stable memory codes for various time intervals in the hippocampus (Mankin et al., 2012).

### NE's Wide Potential Use

Potential Use in Social Event PredictionIn D18, NE successfully reports 21010 valid TRs with an F1 score of 0.83. The encoding stage takes around 1 hour, while decoding only takes less than 1 minute. Cases are presented below and in Figure 5, with additional cases available in Appendix B.4.

(1) Case 1. Citizen (India) will _Reject_ to Narendra Modi (events in day 23, 122, and 168) in around 87 days whenever Narendra Modi _Appeal for diplomatic cooperation (such as policy support)_ to Citizen (India) (events in day 102, 200, and 264).

(2) Case 2. Russia will _Meet at a 'third' location_ with Ukraine (events in day 31 and 138) in around 136 days whenever Ukraine _Use conventional military force_ to Russia (events in day 161 and 288).

Since the TRs mined can generalize across time, the results above imply NE's potential use in both reliable and interpretable event predictions urgently needed in the big data era (Zhao, 2021).

Figure 5: Social event prediction Figure 6: Personal decision making

Potential Use in Personal Decision MakingConsider an intelligent machine that has visited a restaurant four times, with the occurrence time of each event episode used as input for NE, as shown in Figure 6(a). After training all events, the decoded TR validity \(_{_{r}}g()\) is transformed linearly and demonstrated in Figure 6(b). Despite the recurrent TRs on the slash that can be set aside, valid TRs such as 'order dishes -(about 10 minutes)-> have meals' are well distinguished from invalid ones such as 'order dishes -(about 5 minutes) -> look at the floor'.

Combining NE with front-end methods that take unstructured videos as input and output event items in the form of \((ev,t)\), and with back-end methods that use the decoded valid TRs to guide decision-making, NE has the potential to aid intelligent machines in surviving in changing environments by generalizing from little experience, just as human beings (Goyal & Bengio, 2022; Xue et al., 2018).

Potential Use in Memory-constrained ScenariosAs discussed in 4.3, NE approximately reduces the required storage space from \(M^{2}\) to \(M\) in our experimental settings, where \(M\) is the number of event types. Therefore, NE holds significant potential for applications in memory-constrained scenarios like the edge. This is important when \(M\) is large, which is usual in the big-data era.

### Ablation Studies

Here we demonstrate ablation studies of loss constants \(C_{p},C_{n}\) and time vector \(\), while those for dimension \(d\) and event type vector \(\) are shown in Appendix B.3.

Loss ConstantsTable 2 shows that NE performs optimally when \(C_{p}=1\) and \(C_{n}=0\). In fact, as \(C_{p}\) approaches 1, the \(g()\) of perfect TRs (\(gc()=1,=1\)) is enforced to converge to its minimum \(0\). This global constant for all potential TRs in the embedding space allows \(g()\) to reveal TR validity better. In terms of \(C_{n}\), setting it to \(0\) results in negative samples occupying the largest embedding space. Since negative samples comprise most of all trained event samples, this setting improves the fit of negative samples and optimizes NE's performance.

Maximal Frequency CoefficientTable 3 shows that NE performs optimally with different values of \(_{max}\), respectively, in the TR detection and query task.

    &  &  &  \\  \(\{_{k}\}\) & linear & exponential & linear & exponential & linear & exponential \\  TR Query (r) & 0.81 & 0.82 & 0.75 & 0.82 & 0.24 & 0.85 \\   

Table 4: NE on the three datasets with increasing events, and with different distributions of \(\{_{k}\}\)

   \(C_{p}\)\(C_{n}\) &  &  \\
0.4 & 0.2 & 0 & -0.2 & -0.4 & 0.4 & 0.2 & 0 & -0.2 & -0.4 \\ 
1 & 0.78 & 0.80 & 0.82 & 0.81 & 0.80 & 0.86 & 0.87 & 0.87 & 0.87 \\
0.8 & 0.64 & 0.67 & 0.79 & 0.80 & 0.80 & 0.85 & 0.86 & 0.87 & 0.86 \\
0.6 & 0.29 & 0.40 & 0.68 & 0.79 & 0.79 & 0.44 & 0.82 & 0.86 & 0.86 & 0.85 \\
0.4 & 0.18 & 0.31 & 0.49 & 0.76 & 0.79 & 0.12 & 0.35 & 0.84 & 0.84 & 0.81 \\
0.2 & 0.53 & 0.19 & 0.27 & 0.71 & 0.78 & 0.27 & 0.05 & 0.28 & 0.79 & 0.71 \\   

Table 2: NE on ICEWS14 in different \(C_{p}\) and \(C_{n}\) settings

   \(_{max}\) & 1 & 5 & 10 & 50 & 100 & 200 & 400 & 600 & 800 \\  TR Query (r) & 0.15 & 0.93 & 0.92 & 0.85 & 0.85 & 0.85 & 0.85 & 0.85 & 0.85 \\ TR Detection (F1) & 0.22 & 0.45 & 0.55 & 0.56 & 0.54 & 0.53 & 0.53 & 0.51 & 0.53 \\   

Table 3: NE on GDELT with different \(_{max}\)sFrequency DistributionTable 4 shows that the larger the dataset, the more exponential distribution (\(_{k}=)^{}-1}{T_{a}},k=0,1,...,d-1\)) surpasses linear distribution (\(_{k}=}{d T_{a}},k=0,1,...,d-1\)) with the same parameters of \(d=400,_{max}=600\). This suggests that real-world event occurrences depend more on low-frequency terms than high-frequency ones.

## 5 Related Work

Event Schema InductionIn the natural language processing (NLP) field, a significant research focus is on inducing event schemas from text (Huang et al., 2016; Li et al., 2021), including from language models (Dror et al., 2022), to support downstream applications such as search, question-answering, and recommendation (Guan et al., 2022). These NLP methods aim to organize known event regularities already given as priors for the extracting algorithm (such as extracting 'earthquake -> tsunami' from the sentence 'An earthquake causes a tsunami.') and focus on the schemas for use. In contrast, our tasks are designed to learn event regularities directly from experience without supervision. Specifically, the only prior models know is whether an event occurs, and models are required to detect valid TRs from all potential ones and report the correct relative time of valid TRs.

Temporal Rule MiningVarious temporal rules are mined from event sets to reveal regularities in industry, security, healthcare, etc (Segura-Delgado et al., 2020; Chen et al., 2007; Yoo and Shekhar, 2008; Namaki et al., 2017). Although the search methods used discover event regularities directly from events without supervision, both the mined rules and source events are generally stored as symbolic representations in list form. In contrast, by applying event embeddings, NE is a distributed and approximate memory for both TRs and event items. NE strikes a balance between storage efficiency and storage accuracy compared to exact counting, as detailedly discussed in 4.3.

Embedding Models of Structured DataWithin all embedding models of static and temporal knowledge graphs (Chen et al., 2020; Cai et al., 2022; Wang et al., 2020; Messner et al., 2022), three are most related to NE. RotatE (Sun et al., 2019) represents each entity and relationship as a complex vector to model relation patterns on knowledge graphs, and TeRo (Xu et al., 2020) represents time as a rotation of entities to model time relation patterns on temporal knowledge graphs. While both RotatE and TeRo introduce complex vectors for better completion performance, NE first explores using complex vectors for TR detection in events. In particular, the specific use of complex vectors in RotatE encodes inverse relations and in TeRo encodes asymmetric and reflexive relations. NE, instead, apply rotating complex unit vectors to encode time-translation symmetries of all potential TRs. IterE (Zhang et al., 2019) construct a decodable embedding model to discover rules for better knowledge graph completion performance. While we take functional inspiration from IterE that embedding models can jointly encode data and decode regularities, we focus on event data and define the new problems of TR detection and TR query. Specifically, while IterE focuses on discrete variables, NE focuses on the continuous variable of time that involves Fourier-like transformations.

To summarize, TR detection and TR query focus on achieving human-like schema learning capabilities rather than pursuing better support for NLP applications like the event schema induction task. Meanwhile, NE leverages the advantages of distributed representations over symbolic ones of search methods in temporal rule mining and is distinct from existing embedding models of structured data.

## 6 Conclusion

We have developed NE which for the first time enables data-efficient TR formation and time-efficient TR retrieval simply through embedding event samples. We have formally defined the tasks of TR detection and TR query to comprehensively evaluate the TR learning capabilities of embedding models. We have demonstrated NE's potential use in social event prediction, personal decision-making, and memory-constrained scenarios. We hope that we have facilitated the development of human-like event intelligence.

One limitation of NE is that when the vector dimension \(d\) is set much lower than the number of absolute time points \(T_{a}\), significant performance degradation of NE will occur as observed in the GDELT experiment. Future research is needed to improve this weakness. The privacy issues potentially brought about by TR detection and the causality of TRs should also be handled properly.