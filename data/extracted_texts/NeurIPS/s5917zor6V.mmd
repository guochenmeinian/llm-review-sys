# On the Curses of Future and History in

Future-dependent Value Functions for OPE

 Yuheng Zhang

University of Illinois Urbana-Champaign

yuhengz2@illinois.edu

&Nan Jiang

University of Illinois Urbana-Champaign

nanjiang@illinois.edu

###### Abstract

We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed _future-dependent value functions_ as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the _latent_ state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such as _outcome coverage_ and _belief coverage_, which enable polynomial bounds on the aforementioned quantities. As a side product, our analyses also lead to the discovery of new algorithms with complementary properties.

## 1 Introduction and Related Works

Off-policy evaluation (OPE) is the problem of estimating the return of a new _evaluation policy_\(_{e}\) based on historical data, which is typically collected using a different policy \(_{b}\) (the _behavior policy_). OPE plays a central role in the pipeline of offline reinforcement learning (RL), but is also notoriously difficult. Among the major approaches, importance sampling (IS) and its variants Precup et al. (2000), Jiang and Li (2016) provide unbiased and/or asymptotically correct estimation using the _cumulative importance weights_: given a trajectory of observations and actions \(o_{1},a_{1},o_{2},a_{2},,o_{H},a_{H},\) the cumulative importance weight is \(_{h=1}^{H}(a_{h})|o_{h}}{_{b}(a_{h}|o_{h})}\), whose variance grows _exponentially_ with the horizon, unless \(_{b}\) and \(_{e}\) are very close in their action distributions. In the language of offline RL theory (Chen and Jiang, 2019; Xie and Jiang, 2021; Yin and Wang, 2021), the boundedness of the cumulative importance weights is the coverage assumption required by IS, a very stringent one.

Alternatively, algorithms such as Fitted-Q Evaluation (FQE; Ernst et al., 2005; Munos and Szepesvari, 2008; Le et al., 2019) and Marginalized Importance Sampling (MIS; Liu et al., 2018; Xie et al., 2019; Nachum et al., 2019; Uehara et al., 2020) enjoy more favorable coverage assumptions, at the cost of function-approximation biases. Instead of requiring bounded cumulative importance weights, FQE and MIS only require that of the _state-density ratios_, which can be substantially smaller (Chen and Jiang, 2019; Xie and Jiang, 2021). That is, when the environment satisfies the Markov assumption (namely \(o_{h}\) is a _state_), the guarantees of FQE and MIS only depend on the range of \(d_{h}^{_{e}}(o_{h})/d_{h}^{_{b}}(o_{h})\), where \(d_{h}^{_{e}}\) and \(d_{h}^{_{b}}\) is the marginal distribution of \(o_{h}\) under \(_{e}\) and \(_{b}\), respectively.

In this paper, we study the non-Markov setting, which is ubiquitous in real-world applications. Such environments are typically modeled as Partially Observable Markov Decision Processes (POMDPs)Kaelbling et al. (1998). Despite the more general formulation, one can reduce a POMDP to an MDP, making algorithms for MDPs applicable: we can simply define an equivalent MDP, with its state being the _history_ of the original POMDP, \((o_{1},a_{1},,o_{h})\). Unfortunately, a close inspection reveals the problem: the state-density ratio after conversion is

\[^{_{e}}(o_{1},a_{1},,o_{h})}{d_{h}^{_{h}}(o_{1},a_{1}, ,o_{h})}=_{h^{}=1}^{h-1}(a_{h^{}}|o_{h^{ }})}{_{b}(a_{h^{}}|o_{h^{}})},\]

which is exactly the cumulative importance weights in IS and thus also an exponential object!

To address this issue, Uehara et al. (2022) recently proposed a promising framework called _future-dependent value functions_ (or FDVF for short). Notably, their coverage assumption is the boundedness of density ratios between \(_{e}\) and \(_{b}\) over the _latent state_ for memoryless policies. This is as if we were dealing directly with the latent MDP underlying the POMDP, a perhaps best possible scenario. Nevertheless, _have we achieved exponential-free OPE in POMDPs?_

The answer to this question turns out to be nontrivial. In addition to the latent-state coverage parameter, the guarantee in Uehara et al. (2022) also depends on other quantities that are less interpretable. Among them, the boundedness of FDVF itself--a concept central to this framework--is unclear, and we show that a natural construction yields an upper bound that still scales with the cumulative importance weights, thus possibly erasing the superiority of the framework over IS or MDP-reduction.

In this work, we address these caveats by proposing novel coverage assumptions tailored to the structure of POMDPs, under which fully polynomial estimation guarantees can be established. More concretely, our contributions are:

1. For FDVFs, we show that a novel coverage concept called _outcome coverage_ is sufficient for guaranteeing its boundedness (Section 4). Notably, outcome coverage concerns the overlap between \(_{b}\) and \(_{e}\)_from the current time step onward_, whereas all MDP coverage assumptions concern that _before_ the current step.
2. With another novel concept called _belief coverage_ (Section 5.1), we establish fully polynomial estimation guarantee for the algorithm in Uehara et al. (2022). The discovery of belief coverage also leads to a novel algorithm (Section 5.3) that is analogous to MIS for MDPs.
3. Despite the similarity to linear MDP coverage (Duan et al., 2020) due to the linear-algebraic structure, these POMDP coverage conditions also have their own unique properties due to the \(L_{1}\) normalization of belief and outcome vectors. We present improved analyses that leverage such properties and avoid explicit dependence on the size of the latent state space (Section 4.2).

## 2 Preliminaries

POMDP Setup.We consider a finite-horizon POMDP \( H,=_{h=1}^{H}_{h},, =_{h=1}^{H}_{h},R,,,d_{1}\), where \(H\) is the horizon, \(_{h}\) is the latent state space at step \(h\) with \(|_{h}|=S\), \(\) is the action space with \(||=A\), \(_{h}\) is the observation space at step \(h\) with \(|_{h}|=O\), \(R:\) is the reward function, \(:()\) is the emission dynamics with \((|s_{h})\) supported on \(_{h}\) for \(s_{h}_{h}\), \(:()\) is the dynamics (\((|s_{h},a_{h})\) is supported on \(_{h+1}\)), and \(d_{1}(_{1})\) is the initial latent state distribution. For mathematical convenience we assume all the spaces are finite and discrete, but the cardinality of \(_{h}\), \(O\), **can be arbitrarily large**. A trajectory (or episode) is sampled as \(s_{1} d_{1}\), then \(o_{h}(|s_{h})\), \(r_{h}=R(o_{h},a_{h})\), \(s_{h+1}(|s_{h},a_{h})\) for \(1 h H\), with \(a_{1:H}\) decided by the decision-making agent, and the episode terminates after \(a_{H}\). \(s_{1:H}\) are latent and not observable to the agent.

History-future Split.Given an episode \(o_{1},a_{1},,o_{H},a_{H}\) and a time step \(h\) of interest, it will be convenient to rewrite the episode as

\[(_{h}\,,,\,a_{h},\,f_{h+1}}^{f_{h}}).\]

Here \(_{h}=(o_{1},a_{1}, o_{h-1},a_{h-1})_{h}:=_{h^{ }=1}^{h-1}(_{h})\) denotes the historical observation-action sequence (or simply _history_) prior to step \(h\), and \(f_{h+1}=(o_{h+1},a_{h+1},,o_{H},a_{H})_{h+1}:=_{h^{ }=h+1}^{H}(_{h^{}})\) denotes the _future_ after step \(h\). This format will be convenient for reasoning about the system dynamics at step \(h\). We use \(=_{h=1}^{H}_{h}\) to denote the entire history domain and use \(=_{h=1}^{H}_{h}\) to denote the entire future domain. This way we can use stationary notation for functions over \(,,,\), where the time step can be identified from the function input (e.g., \(R(o_{h},a_{h})\)), and functions with time-step subscripts refer to their restriction to the \(h\)-th step input space, often treated as a vector (e.g., \(R_{h}^{_{h}}\)).

Memoryless and History-dependent Policies.A policy \(:_{h=1}^{H}(_{h}_{h})()\) specifies the action probability conditioned on the past observation-action sequence.1 In the main text, we will restrict ourselves to _memory-less_ (or reactive) policies that only depends on the current observation \(o_{h}\); extension to general policies is similar to Uehara et al. (2022) and discussed in Appendix B.6. For any \(\), we use \(_{}\) and \(_{}\) for the probabilities and expectations under episodes generated by \(\), and define \(J()\) as the expected cumulative return: \(J():=_{}[_{h=1}^{H}R(o_{h},a_{h})]\). For memoryless \(\), we also define \(V_{}^{}(s_{h})\) as the latent state value function at \(s_{h}\): \(V_{}^{}(s_{h}):=_{}[_{h^{}=h}^{H}R( o_{h^{}},a_{h^{}}) s_{h}][0,H]\). \(d^{}(s_{h})\) denotes the marginal distribution of \(s_{h}\) under \(\).

Off-policy Evaluation.In OPE, the goal is to estimate \(J(_{e})\) using \(n\) data trajectories \(=\{(o_{1}^{(i)},a_{1}^{(i)},r_{1}^{(i)},,o_{H}^{(i)},a_{H}^{( i)},r_{H}^{(i)}):i[n]\}\) collected using \(_{b}\). We write \(_{}[]\) to denote empirical approximation of expectation using \(\). Define the one-step action probability ratio \((o_{h},a_{h}):=(a_{h}|o_{h})}{_{b}(a_{h}|o_{h})}\). We make the following assumption throughout:

**Assumption 1** (Action coverage).: We assume \(_{b}(a_{h}|o_{h})\) is known and \(_{h,o_{h},a_{h}}(o_{h},a_{h}) C_{}\).

This is a standard assumption in the OPE literature, and is needed by IS and value-based estimators that model state value functions (Jiang and Li, 2016; Liu et al., 2018).

Belief and Outcome Matrices.We now introduce two matrices of central importance to our discussions. Given history \(_{h}\), we define \((_{h})^{S}\) as its belief state vector where \(_{i}(_{h})=(s_{h}=i|_{h})\). Then the **belief matrix**\(M_{,h}^{S_{h}}\) is one where the column indexed by \(_{h}_{h}\) is \((_{h})\). Similarly, for future \(f_{h}\), we define \((f_{h})^{S}\) as its outcome vector where \([(f_{h})]_{i}=_{_{b}}(f_{h}|s_{h}=i)\). The **outcome matrix**\(M_{,h}^{S_{h}}\) is one where the column indexed by \(f_{h}\) is \((f_{h})\). Unlike the belief matrix, the outcome matrix \(M_{,h}\) is dependent on the behavior policy \(_{b}\), which is omitted in the notation.

For mathematical conveniences, we make the following assumptions throughout merely for simplifying presentations; they allow us to invert certain covariance matrices and avoid \(0/0\) situations, which can be easily handled with extra care when the assumptions do not hold.

**Assumption 2** (Invertibility).: \( h[H]\), (1) \((M_{,h})=(M_{,h})=S\).

(2) \( f_{h},\ _{_{h}}(f_{h})>0\); \( o_{h},a_{h}\), \(R(o_{h},a_{h})>0\).

Other Notation.Given a vector \(\), \(\|\|_{}:=^{}}\), where \(\) is a positive semi-definite (PSD) matrix. When \(=(d)\) for a stochastic vector \(d\), this is the \(d\)-weighted 2-norm of \(\), which we also write as \(\|\|_{2,d}\). For a positive integer \(\), we use \([m]\) to denote the set \(\{1,2,,m\}\). For a matrix \(M\), we use \((M)_{ij}\) to denote the \(ij\) entry of \(M\).

## 3 Future-dependent Value Functions

In this section, we provide a recap of FDVFs and translate the main result of Uehara et al. (2022) into the finite-horizon setting, which is mathematically cleaner and more natural in many aspects; see Appendix B.2 for further discussion.

To illustrate the main idea behind FDVFs, recall that the tool that avoids the exponential weights in MDPs is to model the _value functions_. While we would like to apply the same idea to POMDPs,history-dependent value functions lead to unfavorable coverage conditions (see Section 1). The only other known notion of value functions we are left with is that over the latent state space, \(V_{}^{_{e}}(s_{h})\), which unfortunately is not accessible to the learner since it operates on unobservable latent states.

The central idea is to find _observable proxies_ of \(V_{}^{_{e}}(s_{h})\), which takes _future_ as inputs:

**Definition 3** (Future-dependent value functions ).: A future-dependent value function \(V_{}:\), where \(:=_{h}_{h}\), is any function that satisfies the following: \( s_{h}\), \(_{_{b}}[V_{}(f_{h}) s_{h}]=V_{}^{_{e}}(s_{h})\). Equivalently, in matrix form, we have \( h\),

\[M_{,h} V_{,h}=V_{,h}^{_{e}}.\] (1)

Recall our convention, that \(V_{,h}^{|_{h}|}\) is \(V_{}\) restricted to \(_{h}\), and \(V_{,h}^{_{e}}\) is defined similarly.

A FDVF \(V_{}\) is a property of \(_{e}\), but also depends on \(_{b}\). As we will see later in Section 4, the boundedness of \(V_{}\) will depend on certain notion of coverage of \(_{b}\) over \(_{e}\). As another important property, the FDVF \(V_{}\) is generally not unique even if we fix \(_{e}\) and \(_{b}\), as Eq.(1) is generally an underdetermined linear system (\(S|_{h}|\)) and can yield many solutions. As we see below, it suffices to model _any_ one of the solutions. Thus, from now on, when we talk about the boundedness of \(V_{}\), we always consider the \(V_{}\) with the smallest range among all solutions.

Finite Sample Learning.Like in MDPs, to learn an approximate FDVF from data, we will minimize some form of estimated Bellman residuals (or errors). For that we need to first introduce the Bellman residual operators for FDVFs:

**Definition 4** (Bellman residual operators).: \( V:\), the Bellman residual on state \(s_{h}\) is:2

\[(^{}V)(_{h}):=_{a_{h }_{e}\\ a_{h+1:H}_{b}}[r_{h}+V(f_{h+1})_{h}]-_ {_{b}}[V(f_{h})_{h}]=(_{h}),_{h}^ {}V.\] (2)

The following lemma shows that, ideally, we would want to find \(V\) with small \(^{}V\):

**Lemma 1**.: _For any \(_{e}\), \(_{b}\), and \(V:\), \(J(_{e})-_{_{b}}[V(f_{1})]=_{h=1}^{H}_{_{e}} [(^{}V)(s_{h})].\)_

See proof in Appendix C.1. As the lemma shows, any \(V\) with small \(^{}V\) (such as \(V_{}\), since \(^{}V_{} 0\)) can be used to estimate \(J(_{e})\) via \(_{_{b}}[V(f_{1})]\). But again, \(^{}\) operates on \(\) which is unobserved, and we turn to its proxy \(^{}\), which are linear measures of \(^{}\) (Eq.2). More concretely, Uehara et al. (2022a) proposed to estimate \(_{_{b}}[(^{}V)^{2}]\) using an additional helper class \(:\) to handle the double-sampling issue :

\[=*{argmin}_{V}_{}_{h=1}^ {H}_{h}(V,),\] (3)

where \(_{h}(V,)=_{}[\{(a_{h},o_{h})(r_{h}+V(f_{ h+1}))-V(f_{h})\}(_{h})-0.5^{2}(_{h})]\). Under the following assumptions, the estimator enjoys a finite-sample guarantee:

**Assumption 5** (Realizability).: Let \(()\) be a finite function class. Assume \(V_{}\) for some \(V_{}\) satisfying Definition 3.

**Assumption 6** (Bellman completeness).: Let \(()\) be a finite function class. Assume \(^{}V,\; V\).

**Theorem 2**.: _Under Assumptions 5 and 6, w.p. \( 1-\),_

\[|J(_{e})-_{}[(f_{1})] cH\{C_{ }+1,C_{}\}()_{}[ d^{_{e}},d^{_{b}}]|||}{ }}{n}},\]

_where \(c\) is an absolute constant,3 and \(C_{}:=_{V}\|V\|_{}\), \(C_{}:=_{}\|\|_{}\),_

\[():=_{h}_{V} _{_{b}}[(^{}V)(s_{h})^{2}]}{ _{_{b}}[(^{}V)(_{h})^{2} ]}},_{}[d^{_{e}},d^{_{b}}]:=_{h} _{V}_{_{e}}[(^{ }V)(s_{h})^{2}]}{_{_{b}}[( ^{}V)(s_{h})^{2}]}}.\]

[MISSING_PAGE_FAIL:5]

In this example, \(C_{}\) measures how the distribution of \(f_{h}\) under \(_{b}\) deviates multiplicatively from a uniform distribution over \(_{h}\), and an even moderately stochastic \(_{b}\) and emission process \(\) will lead to small \(C_{}\), which implies an exponentially large \(1/_{}(M_{,h})\).

### Minimum Weighted 2-Norm Solution and \(L_{2}\) Outcome Coverage

Pseudo-inverse finds the minimum \(L_{2}\) norm solution. However, given that we are searching for solutions in \(^{_{h}}\) which has an exponential dimensionality, the standard \(L_{2}\) norm--which treats all coordinates equally--is not a particularly informative metric. Instead, we propose to minimize the _weighted_\(L_{2}\) norm with a particular weighting scheme, which has also been used in HMMs [Mahajan et al., 2023] and enjoys benign properties.

We first define the diagonal weight matrix \(Z_{h}:=(_{}^{}M_{,h})\), where \(_{}^{}=[1,,1]^{}\) is the all-one vector. Then, the solution that minimizes \(\|\|_{Z_{h}}\) is:

\[V_{,h}=Z_{h}^{-1}M_{,h}^{}_{,h}^{- 1}V_{,h}^{_{b}},\;\;_{,h}:=M_{,h}Z_{h}^{-1}M_{ ,h}^{}.\] (4)

\(_{,h}^{S S}\) plays an important role in this construction. Recall that its counterpart in the pseudo-inverse solution, namely \(M_{,h}M_{,h}^{}\), has scaling issues (Example 1), that even its _largest_ eigenvalue can decay exponentially with \(H-h+1\). In contrast, \(_{,h}\) is very well-behaved in its magnitude, as shown below. Furthermore, while \(M_{,h}^{}\) on the left is now multiplied by \(Z_{h}^{-1}\) which can be exponentially large, \(Z_{h}^{-1}M_{,h}^{}\) together is still well-behaved; see proof in Appendix D.2.

**Proposition 3** (Properties of Eq.(4)).: _I. \(_{,h}\) is doubly-stochastic: that is, each row/column of \(_{,h}\) is non-negative and sums up to \(1\). As a consequence, \(_{}(_{,h})=1\)._

_2. Rows of \(Z_{h}^{-1}M_{,h}^{}\), i.e., \(\{(f_{h})^{}/Z(f_{h}):f_{h}_{h}\}\), are stochastic vectors, i.e., they are non-negative and the row sum is \(1\)._

Therefore, it is promising to make the assumption that \(_{}(_{,h})\) is bounded away from zero, which immediately leads to the boundedness of \(V_{}\) given that of \(Z_{h}^{-1}M_{,h}^{}\) (\(\)) and \(V_{}^{_{c}}\) (\([0,H]\)). However, we need to rule out the possibility that \(_{,h}\) is always near-singular, and find natural examples that admit large \(_{}(_{,h})\), as given below.

**Example 2**.: _Suppose \(f_{h}\) always reveals \(s_{h}\), in the sense that for any \(j_{h}\), \(_{_{b}}(f_{h}=j s_{h}=i)\) is only non-zero for a single \(i[S]\), and zero for all other latent states. Then, \(_{,h}=\), the identity matrix. Furthermore, \(V_{}\) from Eq.(4) satisfies \(\|V_{}\|_{} H\). See Appendix D.3 for details._

The example shows an ideal case where \(_{}(_{,h})=_{}(_{,h})=1\), when the future fully determines \(s_{h}\). This can happen when the last observation \(o_{H}\) reveals the identity of an earlier latent state \(s_{h}\). Note that in this case, \(M_{,h}M_{,h}^{}\) can still have poor scaling if the actions and observations between step \(h\) and \(H\) are sufficiently stochastic, which shows how the weighted 2-norm solution and analysis improve over the pseudo-inverse one. More generally, \(_{,h}\) is the confusion matrix of making posterior predictions of \(s_{h}\) from \(f_{h}\) based on a uniform prior over \(_{h}\) (see Appendix B.8 for how to incorporate different priors) with \((f_{h})^{}/Z(f_{h})\) being the posterior, and \(_{}(_{,h})\) serves as a measure of how the distribution of future \(f_{h}\) helps reveal the latent state \(s_{h}\).

We now break down the boundedness of Eq.(4) into more interpretable assumptions.

**Assumption 7** (\(L_{2}\) outcome coverage).: Assume for all \(h\), \(\|V_{,h}^{_{}}\|_{_{,h}^{-1}}^{2} C_{ ,V}\).

**Assumption 8** (\(_{,h}\) regularity).: Assume for any \(f_{h}\): \(\|(f_{h})/Z(f_{h})\|_{_{,h}^{-1}}^{2} C_{ ,U}\).

**Proposition 4** (Boundedness of FDVF).: _Under Assumptions 7 and 8, \(V_{}\) in Eq.4 satisfies \(\|V_{}\|_{},2}}:=,V }C_{,U}}\). Furthermore, when only Assumption 7 holds, \(\|V_{,h}\|_{Z_{h}},V}}\), \( h\)._

See proof in Appendix D.4. Assumption 7 requires that the weighted covariance matrix \(_{,h}\) covers the direction of \(V_{,h}^{_{c}}\) well. As a sanity check, it is always bounded in the on-policy case:

**Example 3**.: _When \(_{b}=_{e}\), \(\|V_{,h}^{_{c}}\|_{_{,h}^{-1}} H\)._

Notably, mathematically similar coverage assumptions are also found in the linear MDP literature. For example, with state-action feature \(_{h}\) for time step \(h\), a very tight coverage parameter for linear MDPs is \(\|_{_{e}}[_{h}]\|^{2}_{_{_{b}}[_{h},^{ }_{}]^{-1}}\)(Zanette et al., 2021). Despite the mathematically similarity, there are important high-level differences between these notions of coverage:

1. As mentioned earlier, MDP coverage is concerned with the dynamics **before** step \(h\), whereas our outcome coverage concerns that **after**\(h\). Relatedly, MDP coverage depends on the initial distribution (which our outcome coverage does not depend on), and our coverage depends on the reward function through \(V_{}\) (which MDP coverage does not explicitly depend on). In Section 5, we will discuss our other coverage assumption (belief coverage), which is more similar to the MDP coverage in that they are both concerned with the past.
2. The linear MDP coverage assumption is a refinement of state-density ratio using the knowledge of the function class (Chen and Jiang, 2019; Song et al., 2022). In comparison, the linear structure of our outcome-coverage assumption comes directly from the internal structure of POMDPs.

### Addressing \(S\) dependence via \(L_{1}/l_{}\) Holder and \(L_{}\) Outcome Coverage

Example 3 shows that even in the on-policy case, \(C_{,V}\) may depend on \(S\) which makes the assumption only meaningful for finite and small \(\). In fact, we showed earlier that \(V_{}=R^{+}\) is a natural and obvious \(L_{}\)-bounded solution for \(_{b}=_{e}\), but this is not recovered by the construction in Eq. (4). We also need an additional regularity Assumption 8.

As it turns out, these undesired properties arise because \(L_{2}\) Holder--which is natural for linear MDP settings mentioned above--fails to leverage the \(L_{1}\) normalization of \((f_{h})^{}/Z(f_{h})\) (Proposition 4, Claim 2) and is loose for POMDPs; see Appendix B.5 for further details. A better choice is \(L_{1}/L_{}\) Holder, motivating the \(L_{}\) coverage assumption below, which requires a slightly different construction of \(V_{}\). These definitions may seem mysterious or even counterintuitive; it will be easier to explain the intuitions when we get to their counterparts for belief coverage in Section 5.2.

Construction of \(V_{}\)Define \(Z^{R}(f_{h}):=Z(f_{h})/R^{+}(f_{h})\), and we use \(Z^{R}\) to replace \(Z\) in Eq.(4):

\[V_{,h}=(Z^{R}_{h})^{-1}M^{}_{,h}(^{R}_{ ,h})^{-1}V^{_{e}}_{,h},\ \ \ ^{R}_{,h}:=M_{,h}(Z^{R}_{h})^{-1}M^{}_{ ,h}.\] (5)

**Assumption 9** (\(L_{}\) outcome coverage).: Assume for all \(h\), \(\|(^{R}_{,h})^{-1}V^{_{e}}_{}\|_{} C_ {,}\).

**Lemma 5**.: _Under Assumption 9, \(\|V_{}\|_{} HC_{,}\). See proof in Appendix E.7._

In Appendix B.5 we show that the construction shares similar properties to Eq.(4) in the scenario of Example 2. On the other hand, it has better scaling properties w.r.t. \(S\) and does not additionally require a regularity assumption like Assumption 8. In the on-policy case, Eq.(5) _exactly_ recovers \(V_{}=R^{+}\), a property that Eq.(4) does not enjoy; see Appendix E.8 for details.

**Example 4**.: _When \(_{e}=_{b}\), \((^{R}_{,h})^{-1}V^{_{e}}_{}=\), thus Assumption 9 holds with \(C_{,}=1\) (c.f. \(C_{,V} H\) in Example 3). Furthermore, the construction in Eq.(5) is exactly \(V_{}=R^{+}\)._

## 5 Effective History Weights and A New Algorithm

We now turn to **Q2** in Section 3, which asks for a quantitative understanding of the IV\(()\) term. Note that IV\(()\) and \(_{}[d^{_{e}},d^{_{b}}]\), taken together, are to address the conversion between:

\[\ _{_{b}}[(^{ }V)(_{h})^{2}]}\ \ \ \ \ |_{_{e}}[(^{}V)(s_{h})]|.\] (6)

The two terms differ both in the policy (\(_{b}_{e}\)) and the operator (\(^{}^{}\)). While we could directly define a parameter by taking the worst-case (over \(V\)) ratio between the two expressions,5 the real question is to provide more intuitive understanding of when it can be bounded.

Towards this goal, Uehara et al. (2022) split the above ratio into two terms, IV\(()\) and \(_{}[d^{_{e}},d^{_{b}}]\), which take care of the \(^{}^{}\) conversion (under \(_{b}\)) and \(_{b}_{e}\) conversion (under \(^{}\)), respectively. While this leads to an intuitive upper bound of the \(_{b}_{e}\) conversion parameter in terms of latent state coverage, the nature of the \(^{}^{}\) conversion, IV\(()\), remains mysterious.

In this section, we take a different approach by _directly_ providing an intuitive upper bound on both conversions altogether, under a novel _belief coverage_ assumption. In Appendix E.4 we will also revisit the split into \(()\) and \(_{}[d^{_{e}},d^{_{b}}]\): in the absence of strong structures from \(\), the boundedness of \(()\) turns out to require an even stronger version of belief coverage, rendering the split unnecessary. Furthermore, our approach also leads to a novel algorithm for estimating \(J(_{e})\) that replaces Bellman-completeness (Assumption 6) with a weight-realizability assumption, similar to MIS estimators for MDPs (Liu et al., 2018; Uehara et al., 2020).

### Effective History Weights

The key idea in this section is the notion of _effective history weights_, that perform the \(_{b}_{e}\) and \(^{}^{}\) conversions jointly.

**Definition 10** (Effective history weights).: An effective history weight function \(w^{}:\) is any function that satisfies: \( V\), \(h[H]\),

\[_{_{b}}[w^{}(_{h})(^{}V)(_{h} )]=_{_{e}}[(^{}V)(s_{h})].\] (7)

We first see that well-bounded \(w^{}\) immediately leads to a good conversion ratio for Eq.(6):

\[|_{_{e}}[(^{}V)(s_{h})] |=|_{_{b}}[w^{}(_{h})(^{}V)(_{h})]|_{_{b}}[w^{}(_{h})^{2}] _{_{b}}[(^{}V)()^{2}]},\]

where the inequality follows from Cauchy-Schwartz for r.v.'s. Hence, all we need is \(\|w^{}\|_{2,d^{_{b}}_{h}}:=_{_{b}}[w^{ }(_{h})^{2}]}\), the \(_{b}\)-weighted 2-norm of \(w^{}\), to be bounded, and we focus on this quantity next. Similar to Section 4, there may be multiple \(w^{}\) that satisfies the definition, and we only need to show the boundeness of _any_ solution. Also similarly, the most obvious solution is \(w^{}(_{h})=_{_{b}}(_{h})}{_{_{b }}(_{h})}=_{h^{}=1}^{h-1}(a_{h^{}}|a_{h^{ }})}{_{b}(a_{h^{}}|a_{h^{}})}\), noting that \(_{_{e}}[(^{}V)(s_{h})]=_{_{e}}[(^{}V)(_{h})]\) and importance weighting on \(_{h}\) changes \(_{_{b}}\) to \(_{_{e}}\). However, the use of cumulative importance weights is undesirable given its exponential nature, causing the history-version of "curse of horizon" Liu et al. (2018).

Construction by Belief Matching.We now show a better construction that is bounded under a natural _belief coverage_ assumption. Note that Def 10 can be written as:

\[_{_{b}}[w^{}(_{h})(_{h}),^{}_{h}V]=_{_{e}}[( _{h}),^{}_{h}V],\]

where \(^{}_{h}V^{S}\) is the Bellman residual vector for \(V\) on \(_{h}\). As a sufficient condition (which is also necessary when \(\) lacks strong structures, i.e., \(\{^{}_{h}V:V\}\) spans the entire \(^{S}\)), we can find \(w^{}\) that satisfies: \(_{_{b}}[w^{}(_{h})(_{h})]=_{_ {e}}[(_{h})]=:^{_{e}}_{h}\). This is related to the mean matching problem in the distribution shift literature (Gretton et al., 2009; Yu and Szepesvari, 2012), and a standard solution is (Bruns-Smith et al., 2023):

\[w^{}(_{h})=(_{h})^{}_{,h}^{-1} ^{_{e}}_{h},\] (8)

where \(_{,h}:=_{_{h}}d^{_{b}}(_{h})(_{h })(_{h})^{}\), and \(^{_{e}}_{h}\) coincides with \([d^{_{e}}(s_{h})]_{s_{h}}\).

The weighted 2-norm of this solution is immediately bounded under the following assumption.

**Assumption 11** (\(L_{2}\) belief coverage).: Assume \( h\), \(\|^{_{e}}_{h}\|_{_{,h}^{-1}}^{2} C _{,2}\).

**Lemma 6**.: _Under Assumption 11, \(w^{}\) in Eq. (8) satisfies \( h\), \(\|w^{}\|_{2,d^{_{b}}_{h}}^{2} C_{,2}\). See Appendix E.1._

Assumption 11 requires that the covariance matrix of belief states under \(_{b}\) covers \(^{_{e}}\), the average belief state under \(_{e}\). As before, we also check the on-policy case (see Appendix E.2):

**Example 5**.: _In the on-policy case (\(_{b}=_{e}\)), \(C_{,2} 1\)._

Algorithm GuaranteeNow we have all the pieces to present a fully polynomial version of Theorem 2 under the proposed coverage assumptions. One subtlety is that the guarantee depends on \(C_{}:=_{V}\|V\|_{}\), which is closely related to \(\|V_{}\|_{}\) since we require \(V_{}\), but they are not equal since \(\) can include other functions with higher range. To highlight the dependence of \(\|V_{}\|_{}\) on the proposed coverage assumptions, we follow Xie and Jiang (2020) to assume that the range of the function classes is not much larger than that of the function it needs to capture. A similar assumption applies for bounding \(C_{}\). The proof of the theorem is deferred to Appendix E.10.

**Theorem 7**.: _Consider the same setting as Theorem 2, and let Assumptions 11 and 9 hold. For some absolute constant \(c\), further assume \(C_{V} c\|V_{}\|_{}\) for \(V_{}\) in Eq.(5) and \(C_{} c(\|V_{}\|_{}+1)\). W.p. \( 1-\), \(|J(_{e})-_{}[(f_{1})]| cH^{2}(C_{ ,}+1),2}C_{}(|V|||/ )}{n}}\)._

In addition to \(H\) and complexities of \(||\) and \(||\), the bound _only_ depends on the intuitive coverage parameters: \(C_{}\) (action coverage), \(C_{,2}\) (\(L_{2}\) belief coverage), and \(C_{,}\) (\(L_{}\) outcome coverage).

### \(L_{}\) Belief Coverage

Assumption 11 enables bounded second moment of \(w^{}\) (Lemma 6) but does not control \(\|w^{}\|_{}\), which we show will be useful for the new algorithm in Section 5.3. Here we present the \(L_{}\) version of belief coverage that controls \(\|w^{}\|_{}\), which also helps understand \(L_{}\) outcome coverage given the symmetry between history and future. As alluded to in Section 4.2 and Appendix B.5, \(L_{2}\) Holder is inappropriate for controlling the infinity-norm since it does not leverage the \(L_{1}\) normalization of vectors in POMDPs. Instead, we propose the following decomposition based on \(L_{1}/L_{}\) Holder: \(|w^{}(_{h})|\|(_{h})\|_{1}\|_{,h}^ {-1}_{h}^{_{e}}\|_{}=\|_{,h}^{-1}_{h}^{_{e}}\|_{}\). This way, we can immediately bound \(\|w^{}\|_{}\) with the following assumption:

**Assumption 12** (\(L_{}\) belief coverage).: Assume \( h\), \(\|_{,h}^{-1}_{h}^{_{e}}\|_{} C_{ ,}\). Then \(\|w^{}\|_{} C_{,}\). \(_{,h}^{-1}_{h}^{_{e}}\) is the inverse of _second_ moment (covariance) multiplying the _first_ moment (expectation), raising the concern that the quantity may be poorly scaled: for example, given bounded (but otherwise arbitrary) random vector \(X\), \([XX^{}]^{-1}[X]\) can go to infinity if we rescale \(X\) by a small constant. First note that such a pathology cannot happen here because the random vectors (\((_{h})\)) are \(L_{1}\)-normalized and cannot be arbitrarily rescaled. Below we use a few examples to show that \(_{,h}^{-1}_{h}^{_{e}}\) is a very well-behaved quantity, and naturally generalize familiar concepts such as _concentrability coefficient_ from MDPs (Munos, 2007; Chen and Jiang, 2019; Uehara et al., 2022).

We start by checking the on-policy case. Perhaps surprisingly, \(_{,h}^{-1}_{h}^{_{e}}\) has an _exact_ solution:

**Example 6**.: _When \(_{e}=_{b}\), \(_{,h}^{-1}_{h}^{_{e}}=\), the all-one vector; see Appendix E.5 for the calculation. Consequently, Assumption 12 is satisfied with \(C_{,}=1\)._

The next scenario considers when \((_{h})\) is always one-hot, i.e., histories reveal the latent state (this is analogous to Example 2. \(L_{}\) coverage reduces to the familiar _concentrability coefficient_, the infinity-norm of density ratio as a standard coverage parameter:

**Example 7**.: _When \((_{h})\) is always one-hot, \(\|_{,h}^{-1}_{h}^{_{e}}\|_{}=_{s_{h}} d^{_{e}}(s_{h})/d^{_{h}}(s_{h})\)._

We can also calculate \(\|_{h}^{_{e}}\|_{_{,h}^{-1}}^{2}\) from Assumption 11, which equals \(_{_{b}}[(d^{_{e}}(s_{h})/d^{_{h}}(s_{h}))^{2}]\) in this "1-hot belief" scenario. Xie and Jiang (2020) show that this is tighter than \(_{s_{h}}d^{_{e}}(s_{h})/d^{_{h}}(s_{h})\), and this relation extends elegantly to general belief vectors in our setting:

**Lemma 8**.: \(\|_{h}^{_{e}}\|_{_{,h}^{-1}}^{2}\|_{ ,h}^{-1}_{h}^{_{e}}\|_{}\)_. See proof in Appendix E.6._

### New Algorithm

The discovery of effective history weights and its boundedness also lead to a new algorithm analogous to MIS methods for MDPs (Uehara et al., 2020). The idea is that since Lemma 1 tells us to minimize \(_{_{e}}[(^{S}V)(s_{h})]\), which equals \(_{_{b}}[w^{}(_{h})(^{}V)(_{h})]\) from Def 10, we can then use another function class \(()\) to model \(w^{}\), and approximately solve the following:

\[*{argmin}_{V}_{w}_{h=1}^{H}| _{_{b}}[w(_{h})(^{}V)(_{h})]|,\] (9)

which minimizes an upper bound of \(_{_{b}}[w^{}(_{h})(^{}V)(_{h})]\) as long as \(w^{}()\). Since there is no square inside the expectation, there is no double-sampling issue and we thus do not need the \(\) class and its Bellman-completeness assumption. The \(h\)-th term of the loss can be estimated straightforwardly as

\[|_{}[w(_{h})((o_{h},a_{h})(r_{h}+V(f_{ h+1}))-V(f_{h}))]|.\] (10)We now provide the sample-complexity analysis of the algorithm, using a more general analysis that allows for approximation errors in \(\) and \(\).

**Assumption 13** (Approximate realizablity).: Assume

\[_{V}_{w}|_{h=1}^{H} _{_{b}}[w_{h}(_{h})(^{}V)( _{h})]|_{},\] \[_{w()}_{V} |_{h=1}^{H}_{_{b}}[(w_{h}^{}(_{h})-w_{h}( _{h}))(^{}V)(_{h})]| _{}.\]

Instead of measuring how \(\) and \(\) capture our specific constructions of \(V_{}\) and \(w^{}\), the above approximation errors automatically allow all possible solutions by measuring the violation of the equations that define \(V_{}\) and \(w^{}\).

We present the sample complexity bound of our algorithm as follows. Similar to Theorem 7, we assume that \(C_{}:=_{V}\|V\|_{}\) and \(C_{}:=_{h}_{w}\|w\|_{}\) are not much larger than the corresponding norms of \(V_{}\) and \(w^{}\), respectively. See the proof in Appendix E.11.

**Theorem 9**.: _Let \(\) be the result of approximating Eq.9 with empirical estimation in Eq.(10). Assume that \(C_{} c\|V_{}\|_{}\) for \(V_{}\) in Eq.(5), and \(C_{} c\|w^{}\|_{}\) for \(w^{}\) in Eq.(8). Under Assumptions 12, 9, and 13, w.p. \( 1-\), \(|J(_{e})-_{}[(f_{1})]| _{}+_{}+cH^{2}C_{,}(C_ {,}+1)}|}{ }}{n}}\)._

As a remark, there is also a way to leverage the tighter \(L_{2}\) belief coverage, despite that it does not guarantee bounded \(\|w^{}\|_{}\) and only \(\|w^{}\|_{2,d_{h}^{_{b}}}^{2}\). In particular, if all functions in \(\) have bounded \(\|\|_{2,d_{h}^{_{b}}}^{2}\), the estimator in Eq.(10) will have bounded 2nd moment on the data distribution. In this case, using Median-of-Means estimators (Lerasle, 2019; Chen, 2020) instead of plain averages for Eq.(10) will only pay for the 2nd moment and not the range.

## 6 Conclusion and Future Work

The main text considers memoryless policies. Similar to Uehara et al. (2022), we can extend to policies that depend on recent observations and actions (or _memory_). In fact, we provide a more general result in Appendix B.6 that handles recurrent policies that are _finite state machines_, which allows the policy to depend on long histories. However, the coverage coefficient will be diluted quickly when the memory contains rich information, which we call the _curse of memory_. We suspect that structural policies are needed to avoid the curse of memory and leave this to future work.