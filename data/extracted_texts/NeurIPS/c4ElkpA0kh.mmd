# Efficient \(\Phi\)-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games

**Efficient \(\)-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games**

**Brian Hu Zhang**

Carnegie Mellon University

bhzhang@cs.cmu.edu

**Ioannis Anagnostides**

Carnegie Mellon University

ianagnos@cs.cmu.edu

**Gabriele Farina**

MIT

gfarina@mit.edu

**Tuomas Sandholm**

Carnegie Mellon University

Strategic Machine, Inc.

Strategy Robot, Inc.

Optimized Markets, Inc.

sandholm@cs.cmu.edu

###### Abstract

Recent breakthrough results by Dagan, Daskalakis, Fishelson and Golowich  and Peng and Rubinstein  established an efficient algorithm attaining at most \(\)_swap regret_ over extensive-form strategy spaces of dimension \(N\) in \(N^{O(1/)}\) rounds. On the other extreme, Farina and Pipis  developed an efficient algorithm for minimizing the weaker notion of _linear-swap_ regret in \((N)/^{2}\) rounds. In this paper, we develop efficient parameterized algorithms for regimes between these two extremes. We introduce the set of \(k\)_-mediator deviations_, which generalize the _untimed communication deviations_ recently introduced by Zhang, Farina and Sandholm  to the case of having multiple mediators, and we develop algorithms for minimizing the regret with respect to this set of deviations in \(N^{O(k)}/^{2}\) rounds. Moreover, by relating \(k\)-mediator deviations to low-degree polynomials, we show that regret minimization against degree-\(k\) polynomial swap deviations is achievable in \(N^{O(kd)^{3}}/^{2}\) rounds, where \(d\) is the depth of the game, assuming a constant branching factor. For a fixed degree \(k\), this is polynomial for Bayesian games and quasipolynomial more broadly when \(d=N\)--the usual balancedness assumption on the game tree. The first key ingredient in our approach is a relaxation of the usual notion of a fixed point required in the framework of Gordon, Greenwald and Marks . Namely, for a given deviation \(\), we show that it suffices to compute what we refer to as a _fixed point in expectation_; that is, a distribution \(\) such that \(_{}[()-] 0\). Unlike the problem of computing an actual (approximate) fixed point \(()\), which we show is \(\)-hard, there is a simple and efficient algorithm for finding a solution that satisfies our relaxed notion. As a byproduct, we provide, to our knowledge, the fastest algorithm for computing \(\)-correlated equilibria in normal-form games in the medium-precision regime, obviating the need to solve a linear system in every round. Our second main contribution is a characterization of the set of low-degree deviations, made possible through a connection to low-depth decisions trees from Boolean analysis.

Introduction

_Correlated equilibrium (CE)_, introduced in a groundbreaking work by Aumann (1974), has emerged as one of the most influential solution concepts in game theory. Often contrasted with _Nash equilibrium_(Nash, 1950), it is regarded by many as more natural; in the words attributed to another Nobel laureate, Roger Myerson, "if there is intelligent life on other planets, in a majority of them, they would have discovered correlated equilibrium before Nash equilibrium." Correlated equilibria also enjoy more favorable computational properties: unlike Nash equilibria, they can be expressed as solutions to a linear program, thereby enabling their computation in polynomial time, at least in _normal-form_ games (Papadimitriou and Roughgarden, 2008; Jiang and Leyton-Brown, 2011). Further, a correlated equilibrium arises through repeated play from natural _no-regret_ learning dynamics (Hart and Mas-Colell, 2000; Foster and Vohra, 1997).

However, many real-world strategic interactions feature sequential moves and imperfect information. In such scenarios, the so-called _extensive form_ constitutes the canonical game representation (Kuhn, 1953; Shoham and Leyton-Brown, 2009): a normal-form description of the game would be prohibitively large. It is startling to realize that 50 years after Aumann's original work, the complexity of computing correlated equilibria in extensive-form games--sometimes referred to as _normal-form correlated equilibria (NFCE)_ to disambiguate from other pertinent but weaker solution concepts--remains an outstanding open problem (von Stengel and Forges, 2008; Papadimitriou and Roughgarden, 2008).

The long-standing absence of efficient algorithms for computing an NFCE shifted the focus to natural relaxations thereof, which can be understood through the notion of \(\)_-regret_(Greenwald and Hall, 2003; Stoltz and Lugosi, 2007; Rakhlin et al., 2011). In particular, \(\) represents a set of strategy deviations; the richer the set of deviations, the stronger the induced solution concept. When \(\) contains all possible transformations, one recovers the notion of NFCE--corresponding to _swap regret_. At the other end of the spectrum, _coarse correlated equilibria_ correspond to \(\) consisting solely of constant transformations (aka. _external regret_). Perhaps the most notable relaxation is the _extensive-form correlated equilibrium (EFCE)_(von Stengel and Forges, 2008), which can be computed exactly in time polynomial in the representation of the game tree (Huang and von Stengel, 2008). Considerable interest in the literature has recently been on _learning dynamics_ that minimize \(\)-regret (_e.g._, Morrill et al. (2021, 2021); Bai et al. (2022); Bernasconi et al. (2023); Noarov et al. (2023); Dudik and Gordon (2009); Gordon et al. (2008); Fujii (2023); Dann et al. (2023); Mansour et al. (2022)). A key reference point in this line of work is the recent construction of Farina and Pipis (2023), an efficient algorithm minimizing _linear swap regret_--that is, the notion of \(\)-regret where \(\) contains all _linear_ deviations. Such algorithms lead to an \(\)-equilibrium in time polynomial in the game's description and \(1/\)--aka. a fully polynomial-time approximation scheme (\(\)).

Yet, virtually nothing was known beyond those special cases until recent breakthrough results by Dagan et al. (2024) and Peng and Rubinstein (2024), who introduced a new approach for reducing swap regret to external regret; unlike earlier reductions (Gordon et al., 2008; Blum and Mansour, 2007; Stoltz and Lugosi, 2005), their algorithm can be implemented efficiently even in certain settings with an exponential number of pure strategies. For extensive-form games, their reduction implies a polynomial-time approximation scheme (\(\)) for computing an \(\)-correlated equilibrium; their algorithm has complexity \(N^{(1/)}\) for games of size \(N\), which is polynomial only when \(\) is an absolute constant. Unfortunately, it was thereafter shown that in the usual regime of interest, where instead \((1/N)\), an exponential number of rounds is inevitable even against an oblivious adversary (Daskalakis et al., 2024). In light of that lower bound, our focus here is on developing algorithms attaining a better complexity bound of \((N,1/)\)--the typical guarantee one hopes for within the no-regret framework--by considering a more structured but rich class of deviations \(\).

## 2 Preliminaries

Before we proceed by giving an overview of our results and technical contributions, we first introduce some basic background on tree-form decisions problems and \(\)-regret minimization.

### Tree-form decision problems

A _tree-form decision problem_ describes a sequential interaction between a _player_ and a (possibly adversarial) _environment_. There is a tree of _nodes_. The root is denoted \(\). We will use \(s\) to denote a generic node, and \(p_{s}\) (where \(s\)) to denote the parent of \(s\). Leaves are called _terminal nodes_; a generic terminal node is denoted \(z\). Internal nodes can be one of three types: _decision points_, where the player plays an action, _observation points_, where the environment picks the next decision point. A generic decision point will be denoted \(j\), and the set of actions at \(j\) will be denoted \(_{j}\). The child node reached by following action \(a_{j}\) is denoted \(ja\). We will use \(N\) to denote the number of terminal nodes. We will also assume without loss of generality that all decision points have branching factor at least 2, and that decision and observation points alternate. Thus, the total number of nodes in the tree is also \(O(N)\). The _depth_ of a decision problem is the largest number of decision points in any root-to-terminal-node path. An example of a tree-form decision problem is depicted below in Figure 1.

A _pure strategy_ consists of an assignment of one action \(a_{j}_{j}\) to each decision point \(j\). The _tree-form representation_ of the pure strategy is the vector \(\{0,1\}^{N}\) where \([z]=1\) if and only if the player plays all the actions on the \( z\) path. Although \(\) is a vector indexed only by terminal nodes, we also overload notation to write \([s]=1\) if and only if the player plays all actions on the \( s\) path (In other words, \([s]=1\) if there exists some \(z s\) with \([z]=1\)). Multiple pure strategies can have the same tree-form representation, but in this paper we will only concern ourselves with strategies in tree-form representation, and thus for our purposes such strategies will be treated as identical. We will use \(\{0,1\}^{N}\) to denote the set of tree-form strategies, and sometimes (when context is clear) we will also use \(\) to denote the tree-form decision problem itself. For a point in the convex hull of \(\), \(\), we also use the symbol \(\). For _mixed_ strategies, we instead use \(()\). When it is relevant, we assume that utilities are rational numbers representable with \((N)\) bits.

### Regret minimization

In the framework of online learning, a learner interacts with an adversary over a sequence of rounds. In each round, the learner selects a strategy, whereupon the adversary constructs a utility function. Throughout this paper, we operate in the _full feedback_ setting, wherein the learner gets to observe the entire utility function produced by the adversary after each round. We allow the adversary to be _strongly adaptive_, so that the (linear) utility function at the \(t\)th round \(u^{(t)}:^{(t)},\) can depend on the strategy of the learner at that round; this is a standard assumption (_cf._ the notion of _leaky forecasts_ in the context of calibration ) that will be used for our lower bound (Theorem 3.3). We assume that utilities belong to \(:=\{:|,| 1, \}\). It will be convenient to use \(_{}:=_{} ,\) for the induced norm.

We measure the performance of an online learning algorithm as follows. Suppose that \(()^{}\) is a set of deviations. If the learner outputs in each round a _mixed strategy_\(^{(t)}()\)

Figure 1: An example of a tree-form decision problem. Decision points are black squares with white text labels; observataion points are white squares. Edges are labeled with action names, which are numbers. Pure strategies in this decision problem are identified with vectors \(=(x_{1},x_{2},x_{3},x_{4},x_{5})\{0,1\}^{5}\) satisfying \(1-x_{1}=x_{2}+x_{3}=x_{4}+x_{5}\).

its (time-average) \(\)_-regret_(Greenwald and Hall, 2003; Stoltz and Lugosi, 2007) is defined as

\[}_{}^{T}_{}_{t= 1}^{T}^{(t)},*{}_{^{(t)}^ {(t)}}[(^{(t)})-^{(t)}]. \]

In the special case where \(\) contains only _constant transformations_, one recovers the notion of _external regret_. On the other extreme, _swap regret_ corresponds to \(\) containing all functions \(\).

It is sometimes assumed that the learner instead selects in each round a strategy \(^{(t)}\). To translate (1) in that case, we introduce the _extended mapping_ of a deviation \(:\) as \(^{}*{}_{^{}( )}[(^{})]\), where \(:()\) is a function that is _consistent_ in the sense that \(*{}_{^{}()}[^{ }]=\). A canonical example of such a function \(\) is the _behavioral strategy map_\(:()\), which returns the unique (ignoring actions at decision points reached with probability zero) mixed strategy whose actions at different decision points are independent and whose expectation is \(\). We give another example of a consistent map later in Appendix C.2. Accordingly, we let \(^{}\) denote all extended mappings. In this context, \(^{}\)-regret is defined as

\[}_{^{}}^{T}_{^{ }^{}}_{t=1}^{T}^{(t)},^{}( ^{(t)})-^{(t)}.\]

We are interested in algorithms whose regret is bounded by \(\) after \(T=(N,1/)\) rounds. We refer to such algorithms as _fully polynomial no-regret learners_.

_Remark 2.1_.: We clarify that all the algorithms we consider in this paper are _deterministic_, even when we allow mixed strategies. The fact that (1) contains an expectation over \(^{(t)}^{(t)}\) is simply how \(\)-regret is defined; at no point does the algorithm actually sample from \(^{(t)}\). Using deterministic algorithms is in line with most of the prior work in the full feedback setting.

## 3 Overview of our results

In this section, we present an overview of our results on parameterized algorithms for minimizing \(\)-regret in extensive-form games. We shall first describe our results for the special case of Bayesian games with two actions per player, and we then treat general extensive-form games.

### Bayesian games

For now, we assume that each player's strategy space is a hypercube \(\{0,1\}^{N}\). Hypercubes are linear transformations of tree-form decision problems; in particular, for Bayesian games in which each player has exactly two actions, the strategy space of every player is, up to linear transformations, a hypercube. Since our results are particularly clean for the hypercube case, we start with that.

First, we introduce the set of _depth-\(k\) decision tree deviations_\(^{k}_{},\) which can be described as follows. For each of \(k\) rounds, the deviator first elects a decision point and receives a recommendation, whereupon the deviator gets to decide which action to follow in that decision point. More formally, the set of deviations \(^{k}_{}\) is defined as follows:

1. The deviator observes an index \(j_{0}[N]\).
2. For \(i=1,,k\): the deviator selects an index \(j_{i}[N]\), and observes \([j_{i}]\).
3. The deviator selects \(a_{0}\{0,1\}\).

We call attention to the order of operations. In particular, each query \(j\) is allowed to depend on previously observed \([j]\)s. We can assume (WLOG) that the deviator always chooses \(k\) distinct indices \(j\). Now, the set of deviations \(:\{0,1\}^{N}^{N}\) that can be expressed in the above manner is precisely the set of functions representable as (randomized) depth-\(k\) decision trees on \(N\) variables. To connect \(^{k}_{}\) with the concepts referred to earlier, we clarify that \(k=1\) corresponds to linear-swap deviations, while \(k=N\) captures all possible swap deviations. Our first result is a parameterized online algorithm minimizing regret with respect to deviations in \(^{k}_{}\). (All our results are in the full feedback model under a strongly adaptive adversary.)

**Theorem 3.1**.: _There is an online algorithm incurring (average) \(^{k}_{}\)-regret at most \(\) in \(N^{O(k)}/^{2}\) rounds with a per-round running time of \(N^{O(k)}/\)._Next, we consider the set \(^{k}_{}\) consisting of all _degree-\(k\)_ polynomials \(:\{0,1\}^{N}\{0,1\}^{N}\). Our result for this class of deviations mirrors the one for \(^{k}_{}\), but with a worse dependence on \(k\).

**Theorem 3.2**.: _There is an online algorithm incurring \(^{k}_{}\)-regret at most \(\) in \(N^{O(k^{3})}/^{2}\) rounds with a per-round running time of \(N^{O(k^{3})}/\)._

We find those results surprising; we originally surmised that even for quadratic polynomials (\(k=2\)) the underlying online problem would be hard in the regime where \((1/N)\). We will elaborate on our technical approach for establishing those results in Section 4 coming up.

Hardness in behavioral strategiesA salient aspect of the previous results, which was intentionally blurred above, is that the learner is allowed to output a _mixed strategy_--a probability distribution over \(\{0,1\}^{N}\). In stark contrast, and perhaps surprisingly, when the learner is constrained to output _behavioral_ strategies, that is to say, points in \(^{N}\), we show that the problem immediately becomes \(\)-hard even for degree \(k=2\) (Theorem 3.3)--thereby being intractable under standard complexity assumptions. We are not aware of any such hardness results pertaining to a natural online learning problem, necessitating the use of mixed strategies.

The key connection behind our lower bound is an observation by Hazan and Kale (2007), which reveals that any \(^{}\)-regret minimizer is inadvertedly able to compute approximate fixed points of any deviation in \(^{}\) (Proposition B.1). Computing fixed points is in general a well-known (presumably) intractable problem, being \(\)-hard. In our context, the set \(^{}\) does not contain arbitrary (Lipschitz continuous) functions \(^{N}^{N}\), but instead contains multilinear functions from \(^{N}\) to \(^{N}\). To establish \(\)-hardness for our problem, we start with a _generalized circuit_ (Definition I.3), and we show that all gates can be approximately simulated using exclusively gates involving multilinear operations (Proposition I.7); we defer the formal argument to Appendix I.1. As a result, we arrive at the following hardness result.

**Theorem 3.3**.: _If a regret minimizer \(\) outputs strategies in \(^{N}\), it is \(\)-hard to guarantee \(}_{^{}}/\), even with respect to low-degree deviations and an absolute constant \(>0\)._

### Extensive-form games

We next expand our scope to arbitrary extensive-form games. We will assume here that the branching factor \(b\) of the game is 2--any game can be transformed as such by incurring a \( b\) factor overhead in the depth \(d\) of the game tree. Generalizing \(^{k}_{}\) described above, we introduce the set of \(k\)_-mediator deviations_\(^{k}_{}\). Informally, the player here has access to \(k\) distinct mediators, which the player can query at any time; a formal definition is given in Section 4. Once again, the case \(k=1\) corresponds to linear-swap deviations. Further, if \(\) denotes the set of pure strategies, we let \(^{k}_{}\) denote the set of all degree-\(k\) deviations \(\). We establish similar parameterized results in extensive-form games, but which may now also depend on the depth of the game tree \(d\).

**Theorem 3.4**.: _There is an online algorithm incurring at most an \(\)\(^{k}_{}\) regret in \(N^{O(kd)^{3}}/^{2}\) rounds with a per-round running time of \(N^{O(kd)^{3}}/\). For \(^{k}_{}\) both bounds instead scale as \(N^{O(k)}\)._

We recall that \(N\) here denotes the dimension of the strategy space. We further clarify that parameter \(k\) appearing in \(^{k}_{}\) is different than the \(k\) in \(^{k}_{}\): the former refers to the degree of a polynomial, while the latter is the number of mediators. As all \(k\)-mediator deviations are degree-\(k\) polynomials (but not vice versa), it is to be expected that the bound in the theorem above concerning the former is worse. For a fixed degree \(k\) and assuming that the game tree is _balanced_, in the sense that \(d=\)\(N\), Theorem 3.4 guarantees a quasipolynomial complexity with respect to \(^{k}_{}\), even when \(\) is itself inversely quasipolynomial. The complexity we obtain for \(^{k}_{}\) is more favorable, being polynomial for any extensive-form game.1 Finally, in light of the connection between no-regret learning and convergence to correlated equilibria, our results imply parameterized tractability of the equilibrium concepts induced by \(^{k}_{}\) or \(^{k}_{}\) (see Appendix F.1 for a formal treatment).

Technical contributions

From a technical standpoint, our starting point is the familiar template of Gordon et al. (2008) for minimizing \(\)-regret, which consists of two key components. Accordingly, we split our technical overview into two parts.

### Circumventing fixed points

The first key ingredient one requires in the framework of Gordon et al. (2008) is an algorithm for computing an approximate _fixed point_ of any function within the set of deviations. In particular, if \(\) is the set of pure strategies and \(\) is the convex hull of \(\), we now work with functions \(^{}^{}: {conv}\), so that fixed points exist by virtue of Brouwer's theorem.2 As we discussed earlier, this fixed point computation is--at least in some sense--inherent: Hazan and Kale (2007) observed that minimizing \(^{}\)-regret is computationally equivalent to computing approximate fixed points of transformations in \(^{}\). Specifically, an efficient algorithm minimizing \(^{}\)-regret--with respect to any sequence of utilities--can be used to compute an approximate fixed point of any transformation in \(^{}\) (Proposition B.1 in Appendix B). Given that functions in \(^{}\) are generally nonlinear, this brings us to \(\)-hard territory (Theorem 3.3), seemingly contradicting the recent positive results of Dagan et al. (2024) and Peng and Rubinstein (2024).

As we have alluded to, it turns out that there is a delicate precondition on the reduction of Hazan and Kale (2007) that makes all the difference: computing approximate fixed points is only necessary if the learner outputs points on \(\). In stark contrast, a crucial observation that drives our approach is that a learner who selects a probability distribution over \(\) does _not_ have to compute (approximate) fixed points of functions in \(\). Instead, we show that it is enough to determine what we refer to as an approximate fixed point _in expectation_. More precisely, for a deviation \(:\) with an efficient representation, it is enough to compute a distribution \(()\) such that \(_{}\,()_{}\,\). It is quite easy to compute an approximate fixed point in expectation: take any \(_{1}\), and consider the sequence \(_{1},,_{L}\) such that \(_{+1}_{^{}_{}(_{ })}\,(^{}_{})\) for all \(\), where \(:()\) is a mapping such that \(_{^{}()}[^{}]=\).3 Then, for \(_{[L]}[(_{})]\), we have

\[*{}_{}[()-]= _{=1}^{L}*{}_{^{}_{}( _{})}[(^{}_{})-^{}_{}]=*{}_{^{}_{L}(_{L})}[ (^{}_{L})-_{1}]=O().\]

This procedure can replace the fixed point oracle required by the template of Gordon et al. (2008), which is prohibitive when \(\) contains nonlinear functions, as we formalize in Appendix C.

Application to faster computation of correlated equilibriaIn fact, even in normal-form games where considering linear deviations suffices, computing a fixed point is relatively expensive, amounting to solving a linear system, dominating the per-iteration complexity. Leveraging instead our new reduction, we obtain the fastest algorithm for computing an approximate correlated equilibrium in the moderate-precision regime (Corollary 4.1). In particular, let us focus for simplicity on \(n\)-player normal-form games with a succinct representation. Here, each player \(i[n]\) selects as strategy a probability distribution \(_{i}(_{i})\), where we recall that \(_{i}\) is a finite set of available actions. The expected utility of player \(i\) is given by \(u_{i}(_{1},,_{n})_{a_{1}_{1},,a_{n }_{n}}[u_{i}(a_{1},,a_{n})]\), where \(u_{i}:_{1}_{n}[-1,1]\). We assume that there is an expectation oracle that computes the vector

\[(u_{i}(a_{i},_{-i}))_{i[n],a_{i}_{i}} \]

in time bounded by \((n,A)\), where \(A:=_{i}|_{i}|\); it is known that \((n,A)(n,A)\) for most interesting classes of succinct classes of games (Papadimitriou and Roughgarden, 2008). Using our framework, we arrive at the following result.

**Corollary 4.1**.: _For any \(n\)-player game in normal form, there is an algorithm that computes an \(\)-correlated equilibrium and runs in time_

\[O(}((n,A)+n}{ })).\]

Assuming that the oracle call to (2) (\((n,A)\)) does not dominate the per-iteration running time--which is indeed the case in, for example, polymatrix games--Corollary 4.1 gives (to our knowledge) the fastest algorithm for computing \(\)-correlated equilibria in the moderate-precision regime \(1/A^{-1} 1/ A\), where \( 2.37\) is the exponent of matrix multiplication [Williams et al., 2024]; without fast matrix multiplication, which is widely impractical, the lower bound instead reads \( 1/\). We provide a comparison with previous algorithms in Table 1 and defer the details to Appendix I.3. Finally, we stress that similar improvements can be obtained beyond normal-form games using our template; indeed, virtually all prior \(\)-regret minimizers rely on some fixed point operation.

Before moving on, it is worth stressing that the discrepancy that has arisen between operating over \(()\) versus \(\) is quite singular when it comes to regret minimization in extensive-form games and beyond. Kuhn's theorem [Kuhn, 1953] is often invoked to argue about their equivalence, but in our setting it is the nonlinear nature of deviations in \(\) that invalidates that equivalence.4 To tie up the loose ends, we adapt the reduction of Hazan and Kale  to show that minimizing \(\)-regret over \(()\) necessitates computing approximate fixed points in expectation (Proposition C.3), and we observe that the reductions of Dagan et al.  and Peng and Rubinstein  are indeed compatible with computing approximate fixed points in expectation; the latter observation is made precise in Appendix F.3.

### Regret minimization over the set of deviations \(\)

The second ingredient prescribed by Gordon et al.  is an algorithm minimizing _external regret_ but with respect to the _set of deviations_\(\). The crux in this second step lies in the fact that, even in normal-form games, \(\) contains at least an exponential number of deviations, so black-box reductions are of little use here. Instead, the problem boils down to appropriately leveraging the combinatorial structure of \(\), as we explain below.

We will first describe our approach when \(=\{0,1\}^{N}\), and we then proceed with the more technical generalization to extensive-form games. The key observation here is that regret minimization over \(^{k}_{}\) can be viewed as a tree-form decision problem of size \(N^{O(k)}\). Terminal nodes in this decision problem are identified by the original index \(j_{0}[N]\), the queries \(j_{1},,j_{k}[N]\), their replies \(a_{1},,a_{k}\{0,1\}\), and finally the action \(a_{0}\{0,1\}\) that is played. Each tree-form strategy \(\) in this decision problem defines a function \(_{}:\), which is computed by following the

 Reference & Time complexity \\  Ours (Theorem C.7) & \(}((n,A)+n}{})\) \\ \([\)Anagnostides et al., 2022, Daskalakis et al., 2021] & \(((n,A)+nA^{})\) \\ \([\)Dagan et al., 2024, Peng and Rubinstein, 2024] & \(nA^{1/}(nA)\) \\ \([\)Papadimitriou and Roughgarden, 2008\()\((nA)^{c}(n,A)\) for \(c 1\) \\ \([\)Huang and Pan, 2023\(]\) & \(}{^{2}}(nA^{})\) \\  

Table 1: Time complexity for computing \(\)-correlated equilibria in \(n\)-player normal-form games with \(A\) actions per player. The second column suppresses absolute constants and polylogarithmic factors. For simplicity, issues related to bit complexity have been ignored (that is, we work in the RealRAM model of computation).

strategy \(\) through the decision problem. Formally, we have

\[_{}()[j_{0}]=_{j_{1},a_{1},,j_{k},a_{k}}[j_{0},j_{1 },a_{1},,j_{k},a_{k},1]_{i=1}^{k}[j_{i},a_{i}]\]

where \([j_{i},a_{i}]=[j_{i}]\) if \(a_{i}=1\), and \(1-[j_{i}]\) if \(a_{i}=0\). Hence \(_{}\) is a degree-\(k\) polynomial in \(\).

Now, since \(_{}()[i]\) is linear, it follows that \(,_{}()\) is also linear for any given \(^{n}\). Therefore, a regret minimizer on \(_{}^{k}\) can be constructed starting from any regret minimizer for tree-form decision problems; for example, _counterfactual regret minimization_(Zinkevich et al., 2007), or any of its modern variants. This enables us to rely on usual techniques for dealing with such problems, eventually leading to a complexity bound of \(N^{O(k)}\), as we formalize in Appendix D.

For the set of low-degree polynomials \(_{}^{k}\), we leverage a result from Boolean analysis relating (randomized) low-depth decision trees with low-degree polynomials, stated below.

**Theorem 4.2** (Midrijanis, 2004).: _Every degree-\(k\) polynomial \(f:\{0,1\}^{N}\{0,1\}\) can be written as a decision tree of depth at most \(2k^{3}\)._

In particular, this implies that \(_{}^{k}_{}^{2k^{3}}\). Consequently, low-degree polynomials can be reduced to low-depth decision trees, albeit with an overhead in the exponent.

Turning to general extensive-form games, we follow a similar blueprint, although there are now additional technical challenges. In particular, in what follows, to describe the set of deviations it will be convenient to introduce a new formalism related to tree-form decision problems.

**Definition 4.3**.: The _dual_\(}\) of \(\) is the decision problem identical to \(\), except that the decision points and observation points have been swapped.

**Definition 4.4**.: The _interleaving_\(\) is the tree-form decision problem defined as follows. There is a state \(=(s_{1},s_{2})_{1}_{2}\). The root state is the tuple \((,)\). The decision problem is defined by the player being able to interact with _both_ decision problems, in the following manner. At each state \(=(s_{1},s_{2})\):

* If \(s_{1}\) and \(s_{2}\) are both terminal then so is \(\). Otherwise:
* If either of the \(s_{i}\)s is an observation point, then so is \(\). The children are the states \((s^{}_{i},s_{-i})\) where \(s^{}_{i}\) is a child of \(s_{i}\). (If both \(s_{i}\)s are observation points, both children \(s^{}_{1},s^{}_{2}\) are selected simultaneously. This can only happen at the root.)
* Otherwise, \(\) is a decision point. The player selects an index \(i\{1,2\}\) at which to act, and a child \(s^{}_{i}\) to transition to. The next state is \((s^{}_{i},s_{-i})\).

In \(\), the same state \((s_{1},s_{2})\) can be reachable through possibly exponentially many paths, because the learner may choose to interleave actions in \(\) with actions in \(\) in any order. Thus, each state \((s_{1},s_{2})\) corresponds to actually exponentially many histories in \(\). In the discussion below, we will therefore carefully distinguish between _histories_ and _states_. In light of the above exponential gap between histories and states, it seems wasteful to represent \(\) as a tree. Indeed, Zhang et al. (2023) recently studied _DAG_-form decision problems, and showed that regret minimization on them is possible so long as the DAG obeys some natural properties.

Using the language we have now introduced, we can define the set of _\(k\)-mediator deviations_\(_{}^{k}\) as the set of reduced strategies in the decision problem \(}^{ k}\). That is, the player has access to not one but \(k\) mediators, all holding strategy \(\), which the player can query at any time. This is a significant advantage over having just one mediator since the player can send different queries to each of the \(k\) mediators (who must all reply according to \(\)), and therefore can learn more about the strategy \(\) than it could have otherwise. We will call the responses given by the mediator _action recommendations_. For a graphical illustration of such deviations, we refer to Figure 2 (in Appendix E).

Reduced strategies \((}^{ k})\), once again, induce functions \(_{}:\,\) given by

\[_{}()[z]=_{z_{1},,z_{k}}[z,z_{1},,z_{k}] _{i=1}^{k}[z_{i}],\]and in particular we have that \(_{}\) is a degree-\(k\) polynomial. We define \(_{}^{k}\) as the set of such deviations. For that set, we show that there is a reduction to a particular type of DAG-form decision problem of size \(N^{O(k)}\). As we explained, that formulation is more suitable than tree-form decision problems when the number of possible histories far exceeds the number of states, which is precisely the case when the player is gradually querying multiple mediators as the game progresses.

Finally, we establish a reduction from low-degree polynomials to having few mediators; namely, we show that \(_{}^{k}_{}^{O(kd)^{3}}\), where we recall that \(d\) is the depth of the game tree. Our basic strategy is to again leverage the connection between low-depth decision trees and low-degree polynomials we described earlier (Theorem 4.2). To do so, we need to cast our problem in terms of functions \(\{0,1\}^{N}\{0,1\}^{N}\) instead of \(\). To that end, we first show how to _extend_ a degree-\(k\) function \(f:\{0,1\}\) to a degree-\(kd\) function \(:\{0,1\}^{N}\{0,1\}\); that is, \(\) coincides with \(f\) on all points in \(\{0,1\}^{N}\) (Lemma E.7). This step is where the overhead factor \(d\) comes from. The final technical piece is to show that if each component of \(:\) can be expressed using \(K\) mediators, the same holds for \(\); the naive argument here incurs another factor of \(d\), but we show that this is in fact not necessary. The details of the above argument are deferred to Appendix E.

## 5 Further related research

A key reference point is the result of Blum and Mansour (2007), and a generalization due to Gordon et al. (2008), which reduces minimizing swap regret to minimizing external regret. Specifically, for the probability simplex \(()\), it maintains a separate external-regret minimizer, one for each action \(a\). Both the per-iteration complexity and the number of iterations required is generally polynomial in \(A:=||\). Therefore, in settings where \(A\) is exponentially large in the natural parameters of the problem (such as extensive-form games) it does not appear that the reduction of Blum and Mansour (2007) is of much use. It is tempting to instead rely on the reduction of Stoltz and Lugosi (2005) for minimizing _internal_ regret, a weaker notion than swap regret, which is nonetheless sufficient for (asymptotic) convergence to correlated equilibria. However, one should be careful when relying on internal regret in settings where \(A\) is exponentially large; as we point out in Remark A.1, internal regret can be smaller than swap regret by up to a factor of \(A\), so it is only meaningful when \( 1/A\), a regime which is generally out of reach for regret minimization techniques when \(A\) is exponentially large.

This gap motivated the new reduction by Dagan et al. (2024) and Peng and Rubinstein (2024), which we discussed earlier. Beyond extensive-form games, those reductions apply whenever it is possible to minimize external regret efficiently. The complexity of computing correlated equilibria beyond the regime where the precision parameter \(\) is an absolute constant remains a major open problem, generally conjectured to be hard (von Stengel and Forges, 2008); the recent online lower bound in the adversarial setting (Daskalakis et al., 2024) provides further evidence in support of that conjecture.

As a result, most prior work has focused on more permissive equilibrium concepts, understood through the framework of \(\)-regret (Morrill et al., 2021, 2021, 2022, 2023, 2023, 2023, 2024, 2022, 2024, 2024)). In terms of the most recent developments, Farina and Pipis (2023) established efficient learning dynamics minimizing what is referred to as linear swap regret (_cf._Dann et al. (2023), Fujii (2023) for related results in Bayesian games). The solution concept that arises from linear swap regret was later endowed with a natural mediator-based interpretation by Zhang et al. (2024), which can be viewed as a natural precursor to this work. Convergence to correlated equilibria has also attracted attention in the context of Markov (aka. stochastic) games (_e.g._, (Cai et al., 2024, Jin et al., 2021, Erez et al., 2023, Liu and Zhang, 2023), and references therein).

Moreover, as we explained earlier, our approach also gives rise to a faster algorithm for computing approximate correlated equilibria in a certain regime. As we discuss further in Appendix I.3, improving the per-iteration complexity of Blum and Mansour (2007) has received interest in prior work (Ito, 2020, Greenwald et al., 2006, Yang and Mohri, 2017) (see also (Huang and Pan, 2023, Huang et al., 2023)). The main bottleneck lies in the (approximate) computation of a stationary distribution of a Markov chain, which can be phrased as a linear system. It is worth noting that solving linear systems faster than matrix multiplication even for a crude approximation is precluded, at least subject to fine-grained complexity assumptions (Bafna and Vyas, 2021); we are not aware whether such hardness results are also known for computing the stationary distribution of a Markov chain.

Finally, although we have so far mostly directed our attention to the game-theoretic implication of minimizing swap (or indeed \(\)) regret, namely the celebrated connection with correlated equilibria in repeated games, the notion of swap regret is a fundamental solution concept in its own right more broadly in online learning and learning theory. Compared to the more common notion of external regret, swap regret gives rise to a more appealing notion of hindsight rationality; as such, it is often adopted as a behavioral assumption to model learning agents (_e.g._, (Deng et al., 2019)). It is also fundamentally tied to the notion of _calibration_(Hu and Wu, 2024), and recently inspired work by Gopalan et al. (2023) in the context of multi-group fairness.

## 6 Conclusions and future research

We provided a new family of parameterized algorithms for minimizing \(\)-regret in extensive-form games. Our results capture perhaps the most natural class of functions interpolating between linear-swap and swap deviations, namely degree-\(k\) deviations. Along the way, we refined the usual template for minimizing \(\)-regret--taught in many courses on algorithmic game theory and online learning--which revolves around (approximate) fixed points (Gordon et al., 2008; Blum and Mansour, 2007; Stoltz and Lugosi, 2005). Instead, we showed that it suffices to rely on a relaxation that we refer to as an approximate fixed point in expectation, which--unlike actual fixed points--can always be computed efficiently. Our refinement of the usual template for minimizing \(\)-regret is of independent interest beyond extensive-form games. For example, it can speed up the computation of approximate correlated equilibria even in normal-form games, as it obviates the need to solve a linear system in every round. As in the recent works by Dagan et al. (2024) and Peng and Rubinstein (2024), a crucial feature of our approach is to allow the learner to select a distribution over pure strategies, for otherwise we showed that regret minimization immediately becomes PPAD-hard (under a strongly adaptive adversary).

There are many interesting avenues for future research. First, the complexity of our algorithm pertaining to degree-\(k\) deviations depends exponentially on the depth of the game tree. We suspect that such a dependency could be superfluous. To show this, it would be enough to refine Lemma E.7 by coming up with an extension whose degree does not depend on the depth of the game tree. It would also be interesting to devise parameterized algorithms for \(k\)-mediator deviations that recover as a special case the PTAS of Peng and Rubinstein (2024) and Dagan et al. (2024), so as to smoothly interpolate between existing results for linear-swap regret (Farina and Pipis, 2023) and the aforementioned results for swap regret; is \(k=(1/)\) enough to capture swap regret?

Finally, perhaps the most important question is to understand the computational complexity of computing \(\)-equilibria in extensive-form games. In particular, our results raise the interesting question of whether there is an algorithm (in the centralized model) for computing in polynomial time an _exact_ correlated equilibrium induced by low-degree deviations. Extending the paradigm of Papadimitriou and Roughgarden (2008) in that setting presents several challenges, not least because computing fixed points--which are crucial for implementing the separation oracle (Papadimitriou and Roughgarden, 2008)--is now computationally hard. Relatedly, we suspect that there is an inherent connection between fixed points and correlated equilibria, in the spirit of the equivalence between \(\)-regret minimization and fixed points established by Hazan and Kale (2007).