# Ambrosia: A Benchmark for Parsing Ambiguous Questions into Database Queries

Irina Saparina & Mirella Lapata

Institute for Language, Cognition and Computation

School of Informatics, University of Edinburgh

10 Crichton Street, Edinburgh EH8 9AB

i.saparina@sms.ed.ac.uk &mlap@inf.ed.ac.uk

###### Abstract

Practical semantic parsers are expected to understand user utterances and map them to executable programs, even when these are ambiguous. We introduce a new benchmark, **AMBROSIA**, which we hope will inform and inspire the development of text-to-SQL parsers capable of recognizing and interpreting ambiguous requests. Our dataset contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries. In each case, the ambiguity persists even when the database context is provided. This is achieved through a novel approach that involves controlled generation of databases from scratch. We benchmark various LLMs on **AMBROSIA**, revealing that even the most advanced models struggle to identify and interpret ambiguity in questions.

## 1 Introduction

Semantic parsing translates natural language utterances to logical forms or executable programs in some machine-readable language (e.g., SQL). It has emerged as an important component in many real-world applications (Ozcan et al., 2020; Liang, 2016; Wang et al., 2023; Dukes, 2014) as it allows users to seek information and control computer systems naturally and flexibly in natural language. Practical semantic parsers are expected to understand user utterances and map them to executable forms, even when these are ambiguous (see Figure 1 where a user request allows multiple interpretations, each corresponding to a different logical form). Ambiguity is a pervasive challenge in natural language applications (Min et al., 2020; Liu et al., 2023; Yuan et al., 2023), and semantic parsing is no exception. Wang et al. (2023) show that more than half of failure cases for a text-to-SQL semantic parser are due to ambiguity which can occur in different forms and at different levels.

Although the problem of mapping natural language utterances to formal representations has been studied extensively, the issue of ambiguity has received less attention. Stengel-Eskin et al. (2024) evaluate the ability of large language models to parse ambiguous sentences to first-order logic, focusing on five well-known linguistic ambiguities. In the context of text-to-SQL parsing, other work (Wang et al., 2023; Bhaskar et al., 2023) introduces vagueness into the questions of popular benchmarks like Spider (Yu et al., 2018) by modifying their databases, e.g., through synonyms. Although targeting a real-world application, database augmentation is limited to a single type of ambiguity1 and often operates in an artificial setting. For example, consider the database shown in Figure 0(b). We could add a "Scriptwriters" table with the same content as the existing "Screenwriters" one. Our new database would allow vague questions, but would not be very realistic or well-designed.

In this paper we introduce **AMBROSIA**, a novel benchmark which we hope will both inform and inspire the development of parsers capable of recognizing and interpreting ambiguous queries. **AMBROSIA** covers 16 distinct domains, it contains 846 multi-table databases, ambiguous questions, their unambiguous interpretations provided by humans, and complex SQL queries (4,242 in total). It includes three types of ambiguity, i.e., scope ambiguity, attachment ambiguity, and vagueness, showcasing a diverse range of SQL queries. Figure 1 shows examples of ambiguous questions in blue blocks and their unambiguous interpretations in green, both of which can be phrased as requests. Aiming to mimic real-world semantic parsing scenarios with realistic and diverse databases, we create them automatically in three steps: (a) we specify a domain of interest (e.g., Banking); (b) we generate key concepts and relations such that they obey constraints imposed by a chosen ambiguity type (e.g., for scope ambiguity, the database must have a many-to-many relationship with a common element; see Figure 0(a) where multiple gyms offer the same class, namely yoga); and (c) we generate SQL statements to construct tables with the desired structure. We use a large language model for generation (OpenChat; Wang et al. 2024) and view database creation as a semantic parsing problem. Since we can automatically filter predicted SQL statements based on execution results, minimal manual effort is required to validate the generated databases and their content.2

We benchmark multiple advanced large language models of different sizes on **AMBROSIA**, including the most recent Llama 3 (Dubey et al., 2024) and GPT-4o. Our experiments reveal that models struggle to recognize ambiguity and provide all possible SQL queries for all interpretations. They often capture only one interpretation and are biased towards a specific type of ambiguity. The best model, Llama 3-70B, achieves only 31% recall on ambiguous questions compared to 66% on unambiguous ones. **AMBROSIA** offers a diverse range of questions from various domains, each introducing distinct types of ambiguity along with their interpretations. This diversity offers invaluable insights into the challenges of real-world semantic parsing.

## 2 Related Work

The ambiguity inherent in natural language has been studied through the lens of various tasks, including question-answering (Min et al., 2020), natural language inference (NLI; Liu et al. 2023), and coreference resolution (Yuan et al., 2023), where models have been broadly found lacking in their ability to resolve ambiguities. The bulk of previous work has focused on question answering, with emphasis on asking clarification questions to understand user intent (Rahmani et al., 2023), open-domain question answering where a query can plausibly have multiple valid answers (Min et al.,

Figure 1: Types of ambiguous questions (highlighted in blue), their interpretations (highlighted in green), and corresponding SQL queries. Database elements that could lead to ambiguity are highlighted in orange.

2020), disambiguating database search results in the context of task-oriented dialogue systems (Qian et al., 2022; Kim et al., 2023), and leveraging relevance feedback to rerank the answers returned from a QA engine based on knowledge graphs (Liu et al., 2023).

Within the broader area of semantic parsing, some work (Li et al., 2023; Mu et al., 2024) has concentrated on clarifying vague questions for code generation. There is also interest in creating datasets with ambiguous utterances and corresponding logical form representations. Rasmussen and Schuler (2020) collect a dataset of \(\)-calculus translations that includes examples of scope ambiguity, Arthur et al. (2015) explore different types of ambiguities that arise in the task of mapping search queries into SCFGs, and Stengel-Eskin et al. (2024) create a benchmark for mapping ambiguous sentences into first-order logic. We also collect ambiguous utterances, however, our benchmark is designed for parsing questions into SQL database queries. Unlike Arthur et al. (2015) and Stengel-Eskin et al. (2024) who create synthetic examples from templates, we ask human annotators to write natural questions for a real-world application.

Related work in text-to-SQL parsing has primarily focused on vague questions. Wang et al. (2023) detect questions containing ambiguous tokens that could map to multiple columns. Their dataset builds on WikiSQL (Zhong et al., 2017) and Squall (Shi et al., 2020) which are limited to _single-table_ databases. Bhaskar et al. (2023) modify Spider (Yu et al., 2018) with ChatGPT to create databases that exclusively support vague questions. Despite relying on Spider, their approach often yields unrealistic databases, e.g., they introduce ambiguity in table names by copying and renaming existing tables, which leads to information being duplicated. Our dataset not only supports vagueness but also includes scope and attachment ambiguities. Moreover, **AMBROSIA** provides _multi-table_ databases that mirror realistic applications.

Huang et al. (2023) explore ambiguity in the KaggleDBQA dataset (Lee et al., 2021) focusing on vagueness, underspecified output formats, and unknown data structures. In contrast, our work assumes the database context is fully specified and focuses on different types of linguistic ambiguity. Other work Floratou et al. (2024); Pourreza and Rafiei (2023) analyzes vague questions and highlights issues in existing text-to-SQL benchmarks, where ambiguous questions are often linked to only one SQL query, leading to execution accuracy failures. **AMBROSIA** addresses this limitation by providing multiple SQL interpretations for ambiguous questions. Veltri et al. (2023) automatically generate declarative sentences containing facts that may lead to contradictions due to vague tables. Our approach, however, centers on human-written and verified questions that users might ask in real-world scenarios, rather than fact-checking.

Overall, our dataset introduces various types of ambiguity in questions, including scope and attachment ambiguities, which are often overlooked. Additionally, **AMBROSIA** features human-written interpretations and diverse SQL queries within a single ambiguity type and multi-table databases.

## 3 The Ambrosia Dataset Creation

### Formal Definition of Ambiguity

Before discussing how our dataset was created, we formally define ambiguity in text-to-SQL parsing, adapting the definition presented in Floratou et al. (2024):

**Definition 1**.: Two SQL queries are **non-equivalent** if they produce different execution results, notwithstanding variations in layout or format.

**Definition 2**.: Let \(Q=\{q_{1},,q_{N}\}\) denote the universe of non-equivalent SQL queries that can be formulated given a database \(D\), with known database schema and values. Let \(s\) denote a natural language question and \(f:s P(Q)\) a function that operates in the context of database \(D\) and deterministically maps \(s\) to \(P(Q)\), the power set of \(Q\). Question \(s\) is **ambiguous** if \(f(s)\) has a cardinality of at least two.

This definition excludes ambiguities emanating from data management issues (e.g., relating to formatting, coverage, or the handling of NULL values), and assumes that the database schema and values are known. We also do not consider underspecification of the output format (e.g., whether the result should contain only specific columns or if auxiliary columns are acceptable). Instead, our focus is on ambiguity as a **linguistic phenomenon**, arising from the way a question is formulated, and leading to multiple interpretations and corresponding SQL queries. This ambiguity persists because the database context does not uniquely resolve the interpretations a question invites.

### Design Considerations

Executable Logical FormsIn designing our benchmark, we concentrated on text-to-SQL semantic parsing for several reasons. It represents a real-world use case where ambiguity arises naturally, e.g., in questions posed by users and the structure of databases. Unlike some other logical forms, SQL queries can be easily executed and inspected for correctness. The task is familiar to LLMs, they have demonstrated strong performance on standard benchmarks like Spider (Yu et al., 2018) and BIRD-Bench (Li et al., 2023b); it is reasonable to expect them to be able to parse SQL queries even in zero-shot settings since they likely have learned SQL syntax during training. This allows us to focus on ambiguity per se, rather than the model's ability to generate well-formed SQL.

Databases that Support AmbiguityAnother important consideration is ambiguity _in the context of a database_; it is not enough to just have ambiguous questions, they must also retain their ambiguity in relation to the database context. For instance, the question "What activities does each gym offer?" in Figure 0(a) is ambiguous precisely because there are fitness classes in the database common to multiple gyms (see the "Gyms_Classes" table). Most databases in academic text-to-SQL benchmarks (e.g., Spider) do not support ambiguous questions. As discussed earlier, modifying these databases, e.g., by adding tables or columns with synonymous names, makes them unrealistic with duplicate information and does not cover different types of ambiguity. In Section 3.3 we describe a controllable, multi-step approach that uses LLMs to generate databases supporting question ambiguity.

Different Ambiguity TypesFinally, we wish to include different types of ambiguity (see Figure 1).

_Scope ambiguity_ arises when it is unclear which elements a quantifier, such as "each", "every", or "all", refers to. There are two possible interpretations for the ambiguous question in Figure 0(a): in the collective interpretation, the quantifier is interpreted widely (i.e., "each gym" refers to all gyms in the database) and in the distributive interpretation the quantifier is interpreted narrowly (i.e., "each gym" is considered separately).

_Attachment ambiguity_ occurs when it is unclear how a modifier or phrase is attached to the rest of the sentence. There are two possible interpretations for the question in Figure 0(b): in the high attachment reading, the prepositional phrase "on a work-for-hire" is attached to the verb "show" (i.e., both screenwriters and editors are on work-for-hire contracts), whereas in the low attachment reading it is attached to "editors" (i.e., only editors have work-for-hire contracts, and screenwriters are on any contract). Within this category, we also consider attachment ambiguities for relative clauses (e.g., "writers and editors who have work-for-hire contracts") and adjectives (e.g., "work-for-hire editors and screenwriters") as their underlying database structure and SQL queries are similar to prepositional phrases.

_Vagueness_ occurs when context creates uncertainty about which set of entities is being referred to. Similarly to ambiguous questions, there can be several interpretations. In the example in Figure 0(c), the question has three interpretations depending on whether the answer refers to a general entity (e.g., the bank) or a more specific subtype (e.g., the branch), or both.

Scope and attachment ambiguities are well-known examples of structural ambiguity (Resnik, 1993; Kearns, 2000; Carnie, 2013; Kiss and Pafel, 2017) that arise when a sentence has more than one syntactic parse. However, the research community has only recently started exploring them in the context of LLMs (Liu et al., 2023a; Kamath et al., 2024a; Stengel-Eskin et al., 2024). We classify vagueness separately, as vague questions typically have a single syntactic parse, but, due to semantic imprecision, can refer to different database entities. We recognize that we do not exhaustively cover all cases of ambiguity in questions. For instance, we do not address lexical ambiguity (e.g., "Mississippi" as a river vs. state) which is less common in our context. We hope follow-on work will augment our dataset with additional types of ambiguity.

Figure 2: Annotation process for scope ambiguity in the “Health” domain.

We create **AMBROSIA** with these considerations in mind, following three steps: we select a domain and generate a database that supports one of the above ambiguity types; next, human annotators write ambiguous questions along with their unambiguous interpretations; finally, we automatically generate SQL queries based on templates for scope and attachment ambiguity, and ask annotators to write SQL queries for vague questions. Figure 2 shows a sketch of the annotation process for scope ambiguity.

### Database Generation

In this section we describe the database generation process. We break this task into the following subtasks: selecting domains, generating key concepts and relations for each type of ambiguity, and generating CREATE TABLE and INSERT INTO SQL statements to build the database.

Domains, Concepts, and RelationsAfter analyzing existing semantic parsing datasets and publicly available databases, we compiled a list of real-world domains for database use. We refined this list to 16 domains (e.g., Entertainment, Banking, Hospitality; see the full list in Appendix B) as we found that some were either too narrow or too broad for LLM-based database generation.

To allow different interpretations in the context of a database, the latter must comply with constraints specific to a particular type of ambiguity. In the scope ambiguity example in Figure 0(a), the database has information pertaining to different gyms and classes: a gym can offer multiple classes and a class can be offered by multiple gyms but one class, i.e., yoga, is common among them. Due to this structure (a many-to-many relationship with a common element), the question "What activities does each gym offer?" has two different interpretations (collective vs. distributive reading).

Similar to the conceptual data modeling stage used in database modeling, we first identify key concepts and relations that later become sources of ambiguity in questions and databases. For each type of ambiguity, we define a template that captures the general structure of the data and the potential for ambiguity. Using this template and ten in-context learning examples, we generate different structures (key concepts and relationships) within a given domain. Figure 3 shows the prompt, templates, in-context examples, and predictions which we obtain with a large language model (see Appendix C for the full prompts). Specifically, we employ OpenChat (Wang et al., 2024), one of the strongest open-sourced 7B LLMs with high performance in both code and text generation.

We manually inspect and filter LLM predictions with hallucinations or expressions otherwise unsuitable as database elements (e.g., proper nouns are unlikely to serve as table or column names). We found the vague category to be the most difficult to generate, requiring the most filtering. Examples

Figure 3: The prompt, templates, in-context examples (only one out of ten is shown for brevity, see Appendix C for the full versions), and predictions of key concepts and relations for each ambiguity type. Generated key concepts and relations later become sources of ambiguity in questions and databases (shown at the bottom for illustrative purposes).

include entities unsuitable for further use as table or column names (e.g., "CGI" and "Practical Effects" can be valid answers for "What special effects were used in a movie?" but do not suit table or column names in a realistic database) or mutually exclusive (e.g., "Major Studio" and "India Producer" generally do not produce the same movie but are both types of production). Out of 1,883 predictions generated across all three categories, 881 (47%) were retained.

Database Generation via SQL statementsBased on the domain, key concepts, and relations from the previous step, we generate and execute CREATE TABLE statements to define database tables and their structure, as well as INSERT INTO statements to add values to tables. For this, we use the OpenChat model in a zero-shot setting. We determine possible database configurations for each type of ambiguity, since the concepts and relations generated in the previous step can be mapped to database elements in different ways. For instance, "Screenwriters" and "Editors" are two concepts predicted for the attachment example in Figure 2(c), and can be used as table names (shown in the example) or column names within a single table. In total, we consider four configurations for databases that support attachment ambiguity, two for databases that support vagueness, and one for scope ambiguity, each of these corresponds to a separate instruction to the LLM (see details in Appendix D).

We first generate CREATE TABLE statements (4-6 tables per database on average), and then proceed to generate INSERT INTO statements (3-5 rows per table on average). At each step, we do not accept predictions that are non-executable or different from the selected configuration. We manually validate the generated databases and filter those we consider unnatural. For example, in Figure 2(c), a database with "Contract Type" as a table would not be realistic, even though possible to generate. In contrast, "Projects", an alternative common property for screenwriters and editors, could be mapped to a column or table name. The total number of databases we obtain after filtering is 846.

### Question and SQL Annotation

Ambiguous questions and their interpretations were written by human annotators. SQL queries for scope and attachment ambiguity were generated automatically, while those for vague questions were written by annotators. Annotators were recruited via the Prolific crowdsourcing platform based on various screening tasks; they were all native English speakers with prior knowledge of SQL or experience in database management. We manually reviewed all submissions and ensured all annotators followed our instructions. Details on crowdsourcing are provided in Appendix E.

Scope and Attachment AmbiguityTo create examples of scope and attachment ambiguity, we first automatically generate SQL queries using pre-defined templates for each database configuration (one for scope and four for attachment ambiguity). SQL queries are executed to ensure they yield non-empty, distinct results (and thus correspond to an ambiguous intent). We then automatically generate templates for questions (e.g., "What Name of Classes do we have for every Gyms?" in Figure 0(a)). Prolific annotators (20 in total) are shown a database as context and asked to write ambiguous questions and their interpretations in natural language based on these templates. The annotators substantially edited and paraphrased the original templates. The mean edit distance between our templates and their annotations is 9.2 for scope ambiguity and 12.3 for attachment ambiguity (higher values indicates greater deviation from the template, and edit distance is 0 when there is no difference).

In total, we obtained 501 questions with scope ambiguity, 362 questions with attachment ambiguity and two interpretations in natural language per question. Each ambiguous question corresponds to two golden SQL queries, and each interpretation is itself an unambiguous question and corresponds to one of these SQL queries.

VaguenessA major difference between vague questions and those with scope and attachment ambiguities is that the former can map to very different SQL queries. To promote the diversity of SQL queries in our dataset, we ask experts (10 in total) to write SQL queries from scratch for this category rather than relying on predefined templates. To make the task easier, we simplify the databases by dropping and renaming tables or columns. For example, the "Bank" and "Branch" tables shown in Figure 2(d) are merged into a "Banking Institution" table (so that annotators inspect one general entity instead of two specific ones). Annotators write SQL queries and corresponding questions for the simpler tables (e.g., "Banking Institution") which we restore to the original databases once the annotation is complete. A question is ambiguous as an entity or its reference can be interpreted in different ways (e.g., a banking institution might be a bank, a branch, or both). From the SQLquery written by the annotators, we create SQL queries corresponding to different interpretations by replacing table or column names referring to the general entity with the specific original names. Again, we execute the queries to validate that they produce non-empty, distinct results.

Finally, a different batch of annotators (the same ones who wrote scope and attachment questions and interpretations) were shown vague questions and corresponding key concepts (e.g., a vague question "Who issued CD Special?" and key concepts "Bank" and "Branch" shown in Figure 2(d)) and asked to write interpretations in natural language. In total, we obtained 414 vague questions and 1,239 interpretations (two or three per question). Each question has two or three golden SQL queries, and each interpretation is itself an unambiguous question and corresponds to one of these SQL queries.

### Dataset Analysis

Table 1 shows dataset statistics for AMBROSIA compared to two other text-to-SQL datasets with ambiguous questions (Wang et al., 2023; Bhaskar et al., 2023). A unique aspect of AMBROSIA is that it includes three different types of ambiguity and provides interpretations in natural language for ambiguous questions. The number of tables per database in AMBROSIA is comparable to AMBQT, which is based on Spider. Incidentally, our database generation approach could be used to augment existing text-to-SQL benchmarks, e.g., to assess robustness or out-of-domain generalization.

## 4 Experiments

Below we present an experimental framework for evaluating model performance on AMBROSIA and offer insights into model capabilities and failure modes. Implementation details are provided in Appendix F, and additional experimental results can be found in Appendix G.

ModelsWe benchmark various large language models (LLMs) on AMBROSIA in light of their growing use and good performance on text-to-SQL tasks (Yu et al., 2018; Li et al., 2023). Our experiments include LLMs of different sizes: **OpenChat-7B**(Wang et al., 2024), the model we used for database generation; the **instruction-tuned Llama3-8B** and **Llama3-70B** models from the Llama family (Dubey et al., 2024); **instruction-tuned CodeLlama-70B**(Roziere et al., 2023) which is trained specifically for code generation tasks; and **GPT-3.5 Turbo** and **GPT-40** models from OpenAI.

PromptingAMBROSIA offers various options for exploring ambiguity in semantic parsing. In general, we expect a performant model to be able to recognize ambiguity in the context of a database and output as many SQL interpretations as applicable. Thus, our experiments follow two scenarios: (1) the model is given instructions that acknowledge the potential of ambiguity in the questions and specify that the output should include SQL queries for each possible interpretation; and (2) we provide standard text-to-SQL instructions but consider top-5 predictions from a beam of size 5 as does previous work (Bhaskar et al., 2023; Stengel-Eskin et al., 2024). We refer to the first method as **Prompt** and the second as **Beam**. In both scenarios, models have access to database context as we display the CREATE TABLE and INSERT INTO statements which fully describe the database schema and content. Databases in AMBROSIA do not have many rows, and as such fit within the context limits of the LLMs we use. We acknowledge that in real-world applications database content can be very large, requiring specific methods to extract related database entities. However, we leave this to future work. For Prompt, we conduct experiments with temperature equal to 0.5 and 5 random seeds and average the results for all models except for the OpenAI ones (due to cost constraints). For Beam, experimental results are deterministic since temperature is fixed to 0. We reserve 10% of the dataset for few-shot learning.

  &  &  &  &  &  \\  & \# **ambig** & \# **unambig** & \# **ambig** & \# **unambig** & \# **ambig** & \# **unambig** & **\# **DBB** & \# **Tab/DB** \\  NoisySP & 0 & 0 & 0 & 0 & 8,673 & 0 & 8,086 & 1.0 \\ AMBQT & 0 & 0 & 0 & 0 & 23,295 & 0 & 200 & 5.1 \\  AMBROSIA & 501 & 1,002 & 362 & 724 & 414 & 1,239 & 846 & 5.0 \\  

Table 1: Comparison between AMBROSIA and other text-to-SQL datasets with ambiguous questions, NoisySP (Wang et al., 2023) and AMBQT (Bhaskar et al., 2023). # ambig and # unambig refer to the number of ambiguous questions and their unambiguous interpretations.

Evaluation MetricsA common approach to evaluating text-to-SQL semantic parsing is to compare whether the predicted SQL retrieves the same answer from the database as the gold logical form, typically by measuring execution accuracy. This method accommodates different formulations of the same SQL query and we employ it as well, but in our case, the output for ambiguous questions can be mapped to _several_ correct SQL queries. We report recall and precision, but our primary focus is _recall_ on _ambiguous_ questions, as it captures the extent to which a model predicts different SQL queries that correspond to all possible interpretations. Following recent work (Bhaskar et al., 2023; Stengel-Eskin et al., 2024), we also measure whether _all_ SQL queries are generated for ambiguous questions, i.e., whether recall is 100%. We call this metric AllFound.

### Zero-Shot Results

Table 2 summarizes the performance of zero-shot models on AMBROSIA. We report results using micro-averaging. The standard deviation of the Prompt method is within 0.3%-1.2% for precision and recall for all models, except for CodeLlama, which varies from 4% to 5%. The standard deviation for AllFound is below 0.3%. For fairness, we do not report precision for the Beam method as it consistently outputs the top-5 predictions, although there are only 1-3 gold SQL queries.

As can be seen, all models demonstrate substantially higher recall on unambiguous questions compared to ambiguous ones, with differences ranging from 21% for OpenChat-7B to 36% for GPT-4o and 38% for Llama3-70B. As indicated by the AllFound metric, models generally fail to capture the ambiguity in the question as they rarely predict SQL queries for different interpretations. They often predict a correct SQL query for one interpretation only, which is why precision on ambiguous questions is higher than recall. Conversely, models sometimes predict more than one SQL query for unambiguous questions, which explains why they have lower precision than recall. Precision is 1.0 for both ambiguous and unambiguous questions when a model produces a _single_ correct interpretation. However, when a model produces multiple predictions and only one is correct precision can drop significantly (e.g., to 0.1). This effect explains why ambiguous and unambiguous questions obtain somewhat similar precision.

Note that recall for unambiguous questions is the same as execution accuracy in standard text-to-SQL with one gold SQL query. Table 2 shows that the best recall is only 65.5% (achieved by Llama3-70B), which suggests AMBROSIA has challenging examples even in a standard text-to-SQL setting.

Overall, Llama3-70B (Prompt) captures ambiguity best, with the highest recall on ambiguous questions at nearly 31% and the highest AllFound value of 1.9% which is admittedly still very low. Llama3-70B performs better with the Prompt method, while CodeLlama-70B appears to perform better when considering top-k predictions (Beam), however, it never predicts both interpretations in this case. CodeLlama-70B is very unstable with the Prompt method, and shows performance comparable to Llama3-8B. This instability indicates that parsing ambiguous questions is significantly different from other code generation tasks encountered during training, requiring different skills (e.g., explicit instructions or documentation). GPT-4o performs best in terms of precision, however, even for this model, precision is lower than recall on unambiguous questions which means that it predicts SQL queries when it should not. **In general, all models fail to provide multiple SQL

  &  &  &  &  \\  & & **ambig** & **unambig** & **ambig** & **unambig** & **ambig** \\   & Prompt & 15.5 & 36.8 & 24.7 & 28.2 & 0.2 \\  & Beam & 14.7 & 37.9 & — & — & 1.1 \\  & Prompt & 18.0 & 45.4 & 30.2 & 37.9 & 0.1 \\  & Beam & 19.9 & 48.6 & — & — & 1.7 \\  & Prompt & 17.9 & 44.1 & 34.3 & 40.9 & 0.1 \\  & Beam & 25.4 & 56.2 & — & — & 0.1 \\  & Prompt & **30.7** & 64.5 & 42.7 & 49.4 & **1.9** \\  & Beam & 28.0 & **65.5** & — & — & 1.4 \\  GPT-3.5 Turbo & Prompt & 26.7 & 61.6 & 40.2 & 52.1 & 0.5 \\ GPT-4o & Prompt & 27.1 & 63.4 & **51.1** & **59.6** & 0.4 \\  

Table 2: Recall, precision and AllFound metrics for different zero-shot LMs on AMBROSIA. Llama3-70B (Prompt) captures ambiguity best, with the highest recall on ambiguous questions. All models are unable to parse multiple interpretations when these exist (see AllFound metric).

queries when several interpretations are possible due to ambiguity but can mistakenly offer more than one SQL query for unambiguous questions.**

### Analysis

We perform a more detailed analysis on the zero-shot instruction-tuned Llama 3-70B model, which has demonstrated the highest performance on our dataset. Table 3 presents recall, precision, and AllFound metrics for different categories of ambiguous and unambiguous questions. We observe that attachment ambiguity is most challenging. This category involves complex SQL queries (often requiring the UNION operator) and has the largest diversity in database configurations and corresponding gold SQL queries, which explains poor performance even on unambiguous questions. In contrast, we obtain best results for scope ambiguity, achieving nearly 92% recall (i.e., execution accuracy) on unambiguous questions, and 42% on ambiguous ones. We consider only one database configuration for scope ambiguity, which might be more familiar to LLMs due to the widespread use of many-to-many relationships. Interestingly, the model captures vagueness better with the Prompt method as evidenced by superior recall and AllFound results.

Table 4 shows the distribution of model predictions by interpretation type. We consider ambiguous examples that have at least one correctly predicted SQL query. For vague questions, we focus on those with three interpretations and define two types of predictions: those with only one component (interpretations 1 and 2 in Figure 0(c); Component) and those with all components (interpretation 3 in Figure 0(c); Full). There is a clear bias towards one interpretation type. The distributive interpretation is preferred for scope ambiguity (in 80% of cases) corroborating the findings of Kamath et al. (2024b). High attachment is chosen in more than 97% of cases for attachment ambiguity, and interpretations involving individual components are also overwhelmingly preferred in the case of vagueness. Overall, the Prompt method leads to more biased predictions compared to Beam.

### Few-Shot Results

Figure 4 shows the performance of Llama3-70B (Prompt) in a few-shot learning setting. We select in-context examples randomly, each including ambiguous questions, their unambiguous interpretations, and corresponding SQL queries. We observe largest improvements in recall and precision over the zero-shot method with one to three examples. Recall on ambiguous questions improves by 4% but remains substantially worse compared to unambiguous questions. Increasing the number of examples helps, but improvements are not statistically significant given the 2-7% standard deviation.

Table 5 presents results with GPT-4o in one-shot setting (with 3 seeds). As can be seen, LLaMa3-70B and GPT-4o perform similarly: one-shot improves recall and AllFound in ambiguous questions for both models. The only difference is a slight (not statistically significant) decrease for LLaMa3-70B in precision, which could be due to the model generating more SQL queries, leading to more incorrect predictions and thus lower precision.

  &  &  &  \\ Method & **Collective** & **Distributive** & **High** & **Low** & **Component** & **Full** \\  Prompt & 16.5 & 83.5 & 98.2 & 1.8 & 83.5 & 16.5 \\ Beam & 18.6 & 81.4 & 97.3 & 2.7 & 74.3 & 25.7 \\  

Table 4: Distribution of model predictions (zero-shot Llama3-70B) by interpretation type. Across ambiguities, there is a clear bias towards one interpretation type.

  &  &  &  \\ Prompt & **ambig** & ** unambig** & **ambig** & **ambig** & **ambig** \\   Scope & 41.5 & 90.4 & 52.7 & 66.4 & 2.9 \\ Attachment & 12.7 & 24.0 & 13.7 & 13.4 & 0.3 \\ Vague & 35.6 & 69.4 & 56.7 & 56.4 & 4.6 \\   
  &  &  \\ Beam & **ambig** & **ambig** & **ambig** \\   Scope & 41.6 & 91.8 & 1.1 \\ Attachment & 10.3 & 22.2 & 0.0 \\ Vague & 25.8 & 69.1 & 0.3 \\  

Table 3: Breakdown of model performance (zero-shot Llama3-70B) by ambiguity type shows that attachment ambiguity is most challenging.

Further analysis indicates that providing more examples for one type of ambiguity improves performance for that type but may negatively impact others. We thus benchmark Llama3-70B with a prompt that includes _all_ three ambiguities and their definitions from Section 3.2, unlike random sampling. The two prompt formats yield similar results, however, we observe a smaller standard deviation (2-4%) when prompting with all three ambiguities. Since it is unrealistic to have examples for all possible ambiguities, we consider these results an upper bound.

## 5 Limitations

Despite our best efforts to create a high-quality dataset, we cannot guarantee that **AMBROSIA** is error-free. Recall that we rely on annotators to provide ambiguous questions and their interpretations, both of which may have flaws. Hence, some interpretations may be unclear, failing to disambiguate the question, or unnatural and overly explicit with direct mentions to database entities. Our databases generally have simple and clear names, whereas in reality, they might be incomplete, have abbreviations, and so on. Since our experiments show that LLMs struggle to detect ambiguity and provide interpretations, we believe the current databases are well-suited for our task. However, future work might include augmentations to render them more realistic. When conducting experiments, we display the full database content, which is neither scalable nor safe for real-world applications. Consequently, our results can be seen as an upper bound on semantic parsing performance with ambiguous questions. Although our work broadens the scope of linguistic ambiguity in the text-to-SQL task, we acknowledge our dataset does not exhaustively cover all cases of ambiguity. We hope follow-on work will explore these further.

## 6 Conclusion

In this paper, we present **AMBROSIA**, a novel dataset for parsing ambiguous questions into SQL database queries across multiple domains. We populate **AMBROSIA** with multi-table realistic databases that support ambiguity, having developed an automatic pipeline for controlled database generation using key concepts and relations. **AMBROSIA** covers three types of ambiguity and contains ambiguous questions along with their interpretations in natural language. Our experiments demonstrate that even the most advanced LLMs struggle to capture ambiguity and provide accurate SQL queries for different interpretations of ambiguous questions, leaving ample room for improvement. We further hope **AMBROSIA** will spur future research on generalization (e.g., across domains and ambiguity types). Databases with fixed structures can be also modified to explore other interesting scenarios, including cases where the database context helps clarify originally ambiguous questions.

Figure 4: Recall, precision, and AllFound metrics for zero-shot and few-shot Llama3-70B. In-context examples are selected randomly. We obtain best results with 1-3 examples.

**Model** & **ICL Examples** &  &  &  \\  & & **ambig** & **unambig** & **ambig** & **unambig** & **ambig** \\   & 0-shot & 30.7 & 64.5 & 42.7 & 49.4 & 1.9 \\  & 1-shot & 32.4 & **67.3** & **51.7** & 57.1 & 3.7 \\  & 3 ambiguities & **35.0** & 66.6 & 48.3 & 50.7 & 3.0 \\   & 0-shot & 27.1 & 63.4 & 51.1 & **59.6** & 0.4 \\  & 1-shot & 31.3 & 63.8 & 49.8 & 53.1 & **4.5** \\ 

Table 5: Recall, precision, and AllFound metrics for Llama3-70B and GPT-4o in zero- and one-shot settings. We also include results for Llama3-70B simultaneously prompted with all three ambiguities and their definitions. Differences between models and settings are negligible.

Acknowledgments

We thank the anonymous reviewers for their constructive feedback and Tom Hosking for his insightful comments. We gratefully acknowledge the support of the UK Engineering and Physical Sciences Research Council (grant EP/W002876/1).