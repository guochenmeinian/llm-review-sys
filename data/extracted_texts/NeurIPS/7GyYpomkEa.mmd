# AbDiffuser: Full-Atom Generation of in vitro Functioning Antibodies

Karolis Martinkus\({}^{1}\), Jan Ludwiczak\({}^{1}\), Kyunghyun Cho\({}^{1,4}\), Wei-Ching Liang\({}^{2}\), Julien Lafrance-Vanasse\({}^{2}\), Isidro Hotzel\({}^{2}\), Arvind Rajpal\({}^{2}\), Yan Wu\({}^{2}\), Richard Bonneau\({}^{1}\), Vladimir Gligorijevic\({}^{1}\), Andreas Loukas\({}^{1}\)

\({}^{1}\)Prescient Design, Genentech, Roche \({}^{2}\)Antibody Engineering, Genentech, Roche \({}^{4}\)NYU

###### Abstract

We introduce _AbDiffuser_, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude, enabling backbone and side chain generation. We validate AbDiffuser _in silico_ and _in vitro_. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of the selected designs were tight binders.

## 1 Introduction

We focus on the generation of immunoglobulin proteins, also known as antibodies, that help the immune system recognize and neutralize pathogens. Due to their potency and versatility, antibodies constitute one of the most popular drug modalities, with 10 out of 37 newly FDA-approved drugs in 2022 being immunoglobulins . The ability to generate new antibodies with pre-defined biochemical properties _in silico_ carries the promise of speeding up the drug design process.

Several works have attempted to generate antibodies by learning to form new sequences that resemble those found in nature [21; 19; 82]. An issue with sequence-based approaches is that it is hard to determine the properties that render a protein a functional molecule (and an antibody a potent drug) without inspecting a 3D model of the functional state such as an interface or active site. So far, almost all of the first design methods that have enabled novel protein design used carefully curated structure-function information to score the designs [45; 48; 30]. The determinant role of structure on function has motivated numerous works to co-design sequence and structure [4; 24; 89; 34; 43; 3; 51] or to first design the protein backbone and then fill in the amino acid identities [90; 32; 15; 87; 50; 92; 47].

An emerging paradigm for protein generation is that of equivariant diffusion [90; 89; 51]. Protein diffusion models combine ideas from equivariant and physics-informed protein structure representations  with advances in denoising diffusion probabilistic models  to gradually transform noise to a partial structure. The generated structure is then refined by adding side chains and optimizing atom positions to form a full-atom 3D model of a protein.

A pertinent challenge when generating protein structure is satisfying the appropriate equivariance and physics constraints while also balancing modeling complexity and fidelity. Most current protein models rely on equivariant transformers [37; 73] or graph neural networks [11; 43] to satisfy SE(3)equivariance and typically represent parts of the protein geometry in angle space. The latter recipe can be set up to build a physics-informed model that respects (many) desired constraints but comes with increased complexity. As a consequence, these models are expensive to train and are often applied to a partial protein, ignoring the side chain placement [43; 51] or even deferring the determination of amino acid identity to a later stage [90; 32]. Alternatively, some works focus on the regeneration of complementarity-determining regions (CDRs) of antibodies that are of particular importance to binding [34; 43; 33; 53; 78; 44], which also helps to reduce the complexity of the problem.

Our work is motivated by the observation that key large protein families, here we focus on the antibody protein family, typically have strong properties, such as an ability to be mapped to a reliable sequence ordinate via sequence alignment. Our main contribution is an equivariant diffusion model called _AbDiffuser_ that is designed to exploit these properties. We show that incorporating family-specific priors into the diffusion process significantly improves generation efficiency and quality.

AbDiffuser relies on a new _universal_ SE(3) equivariant neural network that we call the Aligned Protein Mixer (APMixer). In contrast to existing equivariant architectures used for antibodies and proteins, APMixer models residue-to-residue relations implicitly and is particularly effective in handling sequence length changes. Additionally, its significantly smaller memory footprint makes it possible to generate full antibody structures, including framework, CDRs, and side chains. Our approach to residue representation is made possible by a projection method guaranteeing that bond and angle constraints are respected while operating in coordinate and not angle space, a better match to diffusion with Gaussian noise. We also benefit from the effects of overparameterization by scaling the network size to hundreds of millions of parameters on a single GPU; an order of magnitude improvement over the corresponding (E)GNN architectures. Having a powerful model that generates the full antibody structure is shown to be beneficial for the quality of the designed proteins.

We evaluate AbDiffuser on the generation of antibodies from paired Observable Antibody Space (pOAS) , modeling of HER2 binders and antigen-conditioned CDR redesign. Numerical experiments demonstrate that the proposed representation, architecture, and priors enable AbDiffuser to better model the sequence and structural properties of natural antibodies. We also submitted a subset of our proposed HER2 binders for experimental validation in a laboratory. Of the 16 samples submitted, 100% were expressed as actual antibodies and 37. 5% bound to the target with an average pKD of 8.7. Of these, the subset containing raw/filtered samples achieved a binding rate of 22.2%/57.1% and an average pKD of 8.53/8.78, with our tightest binder slightly improving upon the affinity of the cancer drug Trastuzumab. These results provide the first experimental evidence that a generative model trained on mutagenesis data can reliably (with high probability) create new antibody binders of high affinity, even without post-selection by learned or physics-based binding predictors.

Due to space limitations, we refer the reader to the appendix for a detailed discussion of related work, for method details including proofs, as well as for additional numerical results.

## 2 Denoising Diffusion for Protein Generation

This section describes how we utilize denoising diffusion to generate antibody sequence and structure. The ideas presented are influenced by previous work on denoising diffusion [26; 5; 28]. The main contributions that distinguish our work are presented in Sections 3 and 4.

We adopt the denoising diffusion framework in which, given a data point \(X_{0}\), the forward diffusion process gradually adds noise to form corrupted samples \(X_{t}\). These samples form a trajectory \((X_{0},X_{1},...,X_{t},...,X_{T})\) of increasingly noisy data, interpolating from the data distribution \(X_{0} p(X_{0})\) to that of an easy-to-sample prior \(X_{T} p(X_{T})\), such as a Gaussian. The process is constructed to be Markovian, so that \(q(X_{2},...,X_{T}|X_{0})=q(X_{1}|X_{0})\,_{t=2}^{T}q(X_{t}|X_{t-1})\).

To generate new data, a neural network learns to approximate the true denoising process \(_{0}=(X_{t},t)\), which can be achieved by minimizing the variational upper bound of the negative log-likelihood [84; 26; 5]. In our diffusion process, we factorize the posterior probability distribution over the atom positions and residue types:

\[q(X_{t}|X_{t-1})=q(X_{t}^{}|X_{t-1}^{})\,q(X_{t}^{}|X_{t-1}^{}),\]

where a Gaussian and a categorical distribution govern the noise applied to atom positions and residue types, respectively. We cover these in detail in Sections B.2 and B.3. The reverse diffusion process is joint: the model jointly considers atom positions and residue types \((^{}_{0},^{}_{0})=(X^{}_{t},X^{ }_{t},t)\). Throughout the paper, we use \(X^{}^{n 3}\) as a matrix of antibody atom positions and \(X^{}^{r 21}\) as a matrix of one-hot encodings of residue types (20 amino acids and a gap).

## 3 Aligned Protein Mixer

We present Aligned Protein Mixer (APMixer), a novel neural network for processing proteins from a family of aligned proteins. In particular, we focus on antibodies, although the method can in principle be applied to any sufficiently large protein family by using a family-specific global alignment [81; 57]. We first explain how we ensure SE(3) equivariance and how we account for variable sequence lengths in Sections 3.1 and 3.2. The model is detailed in Section 3.3 and our approach to incorporate physics-based constraints on bond angles and lengths is described in Section 3.4.

### SE(3) Equivariance by Frame Averaging

Any neural network \(\) whose input \(X\) has dimension \(n 3\) can be made equivariant or invariant to group transformations by averaging the model outputs over a carefully selected subset of group elements called frames \((X)\). For example, this has been used successfully to make equivariant self-attention models for the prediction of protein-ligand binding energy .

We achieve the desired equivariance to rotations and translations (the SE(3) group) as follows:

\[X^{}=(X^{})|}_{(R,t)(X^{})}\!\!\!\!\!\!\!(X^{}R-t,X^{})R^{T}+t, \]

where \(t=^{T}X^{}\) is the centroid of our points and the four canonical rotation matrices \(R\) forming the four frames \((X^{})(3)\) needed to achieve equivariance can be determined based on Principle Component Analysis. More specifically, we obtain three unit-length eigenvectors \(v_{1},v_{2},v_{3}\) corresponding to the eigenvalues \(_{1},_{2},_{3}\) from the eigendecomposition of the covariance matrix \(C=(X^{}-t)^{T}(X^{}-t)^{ 3 3}\) and define the four frames as

\[(X^{})=([ v_{1}, v_{2}, v_ {1} v_{2}],t),\{-1,1\}}.\]

To respect the fixed chirality of proteins observed in humans, we desire equivariance w.r.t. SE(3) and not E(3) which also includes reflections. As such, when constructing frames the third axis sign is not varied but its direction is determined by the right-hand rule (cross product of the first two axes).

SE(3) invariance can be similarly achieved:

\[X^{}=(X^{})|}_{(R,t)(X^{})}(X^{}R-t,X^{}) \]

We make use of equation 2 when denoising residue types \(X^{}\) as they are invariant to the rotation of the antibody, and equation 1 for the prediction of atom positions \(X^{}\).

### Handling Length Changes by Multiple Sequence Alignment

The version of APMixer investigated in this work is built on top of the AHo antibody residue numbering scheme proposed by Honegger and Pluckthun . This numbering scheme was constructed in a data-driven fashion using information from known antibody structures. For each residue in an antibody chain, it assigns an integer position in \(\) based on the structural role of the residue (e.g. being in a particular CDR loop or in the framework region between two particular CDR loops). Essentially all known antibodies fit into this representation .

As we are modeling paired antibody sequences (the heavy and light chain), the full representation is a \(2 149\) element sequence, where \(149\) heavy chain elements are followed by \(149\) light chain elements. We represent AHo gaps physically as 'ghost' residue placeholders; their position is determined in data pre-processing by linearly interpolating between the corresponding atoms of the nearest existing residues (trivial, due to the use of AHo numbering).

Our experiments confirm that the proposed representation consistently improves generation quality. The reason is two-fold: a) Each antibody chain is now represented as a fixed-length sequence with \(149\) positions that are either empty gaps or are filled with an appropriate amino acid. This fixed-length representation encompasses antibodies with diverse loop lengths via alignment and thus allows our generative model to internally choose how many residues the generated protein will have. In contrast, many of the non-sequence-based protein, small molecule, and graph generative models [49; 54; 28; 90; 32] require the number of elements in the object to be specified beforehand. b) The sequence position directly implies the structural role that the amino acid needs to perform, which makes it easier for the model to pick up structure-specific rules.

### The APMixer Architecture

Our use of a fixed length representation for the immunoglobulin variable domain fold family (Section 3.2) also allows us to forgo traditional architecture choices in favor of a more efficient architecture inspired by the MLP-Mixer . We build the model architecture out of blocks, where each block consists of two MLPs, that are applied consecutively on the columns and the rows

\[X_{,j}=X_{,j}+W_{2}(W_{1}(X_{ ,j}))j[c]\\ X_{i,}=X_{i,}+W_{4}(W_{3}(X_{i,}))i[r],\]

of the input matrix \(X^{r c}\), with \(\) being an activation function, and using the notation \([k]=(1,,k)\). We define the model input as a fixed-size matrix \(X\) combining \(X^{}\) and \(X^{}\) with one sequence element per row (\(2 149\) rows in total). In each row, we encode the residue type and all of the atom positions for that residue (e.g., the \(C,C_{},N,C_{}\) backbone atoms). These input matrix rows are embedded in higher-dimensional vectors using an MLP. Specifically, using this representation, our input matrix \(X\) has \(r=2 149\) rows and \(c=21+4 3\) columns.

To achieve equivariance to atom translations and rotations, we can either apply frame averaging (Section 3.1) on the whole model or on each AbDiffuser block individually. We chose the second option as in our preliminary experiments this improved performance. Frame averaging can be applied on high-dimensional embeddings simply by splitting them into three-dimensional sub-vectors and using each to compute the SE(3) frames . To account for that residue types are invariant to Euclidean transformations, while the atom positions are equivariant, we split each block's input and output vectors in half, with one half being treated equivariantly and the other invariantly.

**Model complexity and SE(3)-universality.** APMixer models pairwise interactions implicitly by operating on rows and columns of the input interchangeably. Thus, its memory complexity grows _linearly_ with the number of residues. This contrasts with the usual quadratic complexity of traditional structure-based models and allows us to do more with a fixed run-time, parameter, and/or memory budget. In Appendix G we prove that the model on top of which APMixer is built is SE(3)-universal, meaning that it can approximate any SE(3)-equivariant function.

We also remark that, in principle, other models, such as a 1D CNN or a transformer, could be used in place of the MLPs in APMixer. With such sequence-length-independent models, we would no longer require a multiple sequence alignment of the given protein family, though this would possibly come at the expense of universality and efficiency.

### Physics-informed Residue Representation by Projection

Atoms within a protein adhere to strong constraints. In principle, a neural network trained with enough data can learn to respect these constraints. However, over a fixed data budget, it can be advantageous to construct a model in a manner that guarantees that its outputs never violate the constraints. Previous work commonly represents proteins in terms of rotation and translation of rigid residue frames and uses idealized residue representations to recover atom positions [37; 73]. Although there is a long tradition of modeling backbone and side chain degrees of freedom in angle space [67; 14], operating in angle space adds modeling complexity and makes diffusion potentially inaccurate [94; 8; 90]. We take a different route and devise a way to respect bond constraints while operating in a global coordinate frame, which works seamlessly with standard Gaussian diffusion for atom positions.

Specifically, inspired by interior-point methods  that alternate between optimization and projection steps, we design a new non-parametric projection layer that is applied to both model inputs and outputs. The model and the noising process are allowed to move the atoms freely, and the projection layer then corrects their positions such that the constraints are respected.

**Backbone residue projection.** We use a reference residue backbone (\(C,C_{},N,C_{}\)) with idealized bond lengths and angles . As the backbone atoms are rigid, we rely on the Kabsch algorithm  to identify the optimal roto-translation between the projection layer's input and the rigid reference residue's atom positions. We apply the transformation to the reference residue and output the resulting atom positions. We also ensure that the distance between the corresponding \(C\) and \(O\) atoms in the output is \(1.231\)Awhile staying as close as possible to the input \(O\) position. The reference residue is also used to represent the AHo ghost residue atoms. Idealizing a backbone in this way usually results in a negligible RMSE to the original structure of \( 5 10^{-3}\)A.

**Side chain projection.** We employ a similar idea to constrain the placement of side chains. In contrast to structure prediction [37; 46], we cannot use one idealized side chain per amino acid, since the sequence is unknown during generation. Our solution is to convert all side chains to a generic representation with enough degrees of freedom to account for the exact placement of all atoms. The amino acid side chains have up to 4 bonds that they can rotate around by dihedral angles. These degrees of freedom can be captured by constructing a side-chain template that consists of 4 pseudo-carbon atoms, for which the dihedral angles are rotated in the same way as for the original side chain. If the original side chain has fewer degrees of freedom, we simply set the corresponding dihedral angles to \(180^{}\) such that the atoms forming the dihedral angle lie on a plane. The projection layer then only has to ensure that the bond lengths between the pseudo-atoms are respected. We set the bond length to 1.54A because carbon atoms are the most common atoms that form the dihedral angles of real amino acids. This representation can be seen in Figure 1. To recover the full-atom structure, we extract the dihedral angles from the side chain template; the angles are then applied to idealized amino acid-specific templates.

## 4 Informative Diffusion Priors

It has been hypothesized that diffusion processes for generation should be tailored to the particular task to increase performance [88; 5]. In Appendix E.1, we present a theoretical argument confirming that choosing a good prior reduces the complexity of the learnable model needed to achieve good-quality denoising. Our bound stated in Theorem E.1 reveals that one cannot learn a simple generative model that fits the data well unless the prior has a small Wasserstein distance to the data distribution.

Armed with this knowledge, we next introduce two types of priors that are incorporated into our diffusion model: a) position-specific residue frequencies that describe sequence conservation of the immunoglobulin fold, and b) conditional dependencies between atom positions.

### AHo-specific Residue Frequencies

A separate benefit of having a fixed-length sequence representation (see Section 3.2) is that we can use the residue frequency at each position as a diffusion prior. To do this, we estimate marginal position-specific categorical distributions \(Q^{1},...,Q^{2*149}\) over residue types from the training data and use these to define the noise in the discrete residue-type diffusion (see Appendix B.3). Since AHo aligns residues based on their structural role within the protein, the estimated prior distributions exhibit significant variability across positions and have low entropy in preserved regions of the immunoglobulin fold. In this way, the noise \(q(X_{t}^{}|X_{0}^{})=(X_{0}^{}(_{ t}I+(1-_{t})Q^{i}))\) at every step of the forward process depends on the residue position \(i\) in the fixed-length sequence representation. Gap frequencies are also captured in these noise matrices.

Figure 1: The proposed internal generic side chain representation. The dihedral-defining atoms (orange) from the full-atom representation (top) are used to construct a generic four-atom representation (bottom). If the side chain has fewer than four angles, additional atoms (gray) are placed in the generic side chain to correspond to a \(180^{}\) angle. The full atom representation is recovered by applying matching rotations to an appropriate side chain template.

### Encoding Conditional Atom Dependencies

It is a consequence of their chain-like structure that neighboring residue atom positions of proteins are strongly correlated. Theorem E.1 suggests that encoding these correlations within the diffusion process eases the denoising complexity and can free the model to focus on more challenging aspects.

To this end, we capture the conditional independence relations between atom positions by a Gaussian Markov Random Field (GMRF) . The latter corresponds to a Gaussian \((0,)\) over atom positions whose precision matrix \(^{-1}=L+aI\) is equal to the shifted graph Laplacian \(L=(A)-A\) associated with the adjacency matrix \(A\). The GMRF operates under the assumption that node features (e.g., 3D positions) for which there is no edge in the adjacency \(A\) are conditionally independent. Some concurrent works [32; 36] also considered conditional atom dependencies by hand-crafting correlation matrices that capture chain relations between residues. We take a step further by proposing to automatically learn the sparse conditional dependence relations from the training data. Our approach entails estimating a sparse adjacency matrix \(A\) that captures the data variance under the GMRF model. The details are described in Appendix F.

## 5 Experiments

We evaluate AbDiffuser's ability to design antibodies. After describing the experimental setup and metrics, Section 5.1 presents _in silico_ tests illustrating the effect of our proposed changes on the generation quality. Section 5.2 details _in vitro_ results showcasing the ability of our method to design new expressing antibodies that bind to a known antigen. Further analyses can be found in the Appendices K and L, whereas experiments on CDR redesign in SAbDab are in Appendix N.

**Baselines.** We compare APMixer with: a) a sequence transformer based on BERT [12; 74] whose output is folded to recover the structure; b) an E(n) Equivariant Graph Neural Network (EGNN)  which is a popular choice for tasks such as 3D molecule generation , antibody CDR loop inpainting , and antibody generation ; and c) a FA-GNN , corresponding to a standard GNN with SE(3) equivariance attained by frame averaging. We also evaluate the proposed informative antibody-specific priors using all of these architectures. To ensure the comparison is performed fairly and under similar settings, we always use the projection layer and our diffusion implementation only varying the denoising model architecture. In the OAS task, we also compare our diffusion-based approach with d) the IgLM  antibody language model. To ensure that it generates paired OAS-like sequences, we condition the generation on the subsequences of the first and last few amino acids taken from pOAS sequences (something that our approach does not need). It is also important to note that the comparison is not fair, since the publicly available IgLM model was trained on 558M sequences that also include the whole paired OAS (105k sequences) and the test set we use. So, in many ways, IgLM's performance represents the best results we could ever hope to achieve with a sequence-only approach. We also compare with e) dyMEAN , which is the only other antibody generative model previously shown to be able to jointly generate the full strucutre and sequence. In the binder generation task, we compare with f) RefineGNN , g) MEAN  and h) DiffAb , three state-of-the-art geometric deep learning methods for CDR redesign.

**Metrics.** The quality of generated sequences is measured in terms of their _naturalness_ (inverse perplexity of the AntiBERTy  model), _closeness_ to the closest antibody in the training set in terms of edit distance, and _stability_ estimated by IgFold. We also verify that the generated antibody sequences satisfy the appropriate biophysical properties using four additional structure-based metrics : CDR region hydrophobicity (_CDR PSH_), patches of positive (_CDR PPC_), and negative charge (_CDR PNC_), and symmetry of electrostatic charges of heavy and light chains (_SFV CSP_). The metrics applied to generated structures focus primarily on the estimated free energy \( G\) using Rosetta  and _RMSD_ for backbone heavy atom positions as compared to IgFold  predictions. More details can be found in Appendix J.

As we want to estimate how well the entire data distribution is captured, in all cases except RMSD, we report the Wasserstein distance between the scores of the sequences in the test split and the scores of the generated sequences. As a reference, we also report the baseline metrics achieved by the sequences and structures in the validation set. A generative model that approaches or matches these values is effectively as good at modeling the distribution of the specific metric in consideration as i.i.d. sampling. The test set and the generated set always have 1000 examples.

Further details on training and implementation can be found in Appendix M.

### Paired OAS Generation

We focus on matching the distribution of 105k paired sequences from the Observed Antibody Space database  folded with IgFold and optimized with Rosetta .

Table 1 summarizes the results. Generated samples by AbDiffuser improve upon baselines on nearly all fronts, even compared to the IgLM language model which was trained on magnitudes more data (especially when concerning structure-related metrics). dyMEAN GNN-based model struggles in this distribution modeling task and is the only model tested that does not achieve the perfect uniqueness of the generated samples (58. 2% unique). The experiment also corroborates our analysis (Theorem E.1) on the benefit of informative priors to diffusion: using a position-specific residue type frequency (AHo) and encoding conditional atom dependencies through a learned covariance matrix (Cov.) helps to improve the ability of most models to capture the pOAS distribution. Interestingly, including the learned covariance can sometimes noticeably improve the quality of the generated sequences (FA-GNN), but its strong benefit to structure quality is only felt when the model is powerful enough to model the structure well (APMixer). Inclusion of AHo numbering and position-specific frequencies improves all models. We perform a similar ablation for APMixer by setting the prior distribution to uniform (uniform prior) and observe a similar performance drop.

To interpret the fidelity of the generated structures, we recall that IgFold uses an ensemble of 4 models followed by Rosetta optimization and that, on average, individual IgFold models (before ensembling) achieve an RMSD of \(0.4239\) on the test set. Therefore, in this regard, the structures created by AbDiffuser are nearly indistinguishable from the test set structures (RMSD of 0.4962). A more detailed analysis of per-region RMSD can be found in Appendix K. We further observe that when no projection layer is used and instead one uses standard diffusion to predict the noise added to the atom positions , the training becomes less stable and the model can fail to learn to generate good structures. Encouragingly, forcing AbDiffuser to model side chain positions alongside the backbone tends to improve the similarity of the generated sequences (Naturalness, Closeness, Stability). This is likely due to the strong coupling between feasible side chain conformations and residue types. The generated side chains get an average Rosetta packing score of 0.624, whereas folded and Rosetta-optimized structures have a mean packing score of 0.666. Recalling that a packing score above 0.6 is widely considered good [77; 66; 3], we deduce that AbDiffuser is able to generate physically plausible side chain conformations. When available, we also use the side chain positions

   Model & \(W_{1}()\)\(\) & \(W_{1}()\)\(\) & \(W_{1}()\)\(\) & \(W_{1}()\)\(\) & \(W_{1}()\)\(\) & \(W_{1}()\)\(\) & \(W_{1}()\)\(\) & \(W_{1}()\) & RMSD \(\) \\  _Validation Set Baseline_ & _0.0150_ & _0.0043_ & _0.0102_ & _0.8301_ & _0.0441_ & _0.0176_ & _0.4889_ & _1.0814_ & — \\  Transformer & 0.5308 & 0.4410 & 1.2384 & 25.8265 & 23.0324 & 2.27925 & — & — \\ Transformer (AHo) & 0.4456 & 0.3474 & 0.5514 & 6.4409 & 1.641 & 0.0993 & 2.3472 & — & — \\  IgM\({}^{*}\) & **0.1103** & **0.0484** & **0.0577** & 11.0675 & **0.0413** & 0.0671 & 1.9274 & — & — \\  d\(y\)MEAN  & **0.1319** & 0.1600 & **0.0423** & 3.9145 & 0.1566 & 0.2929 & 2.3711 & 60.1153 & 3.8157 \\  BGNN & 0.3988 & 0.2655 & 0.3547 & 2.1115 & 0.1486 & 1.085 & 1.9881 & 1586.016 & 9.8231 \\ EGNN (AHo) & 0.3329 & 0.2229 & 0.2904 & 8.1620 & 0.1263 & 0.1075 & 0.7978 & 11472.062 & 10.0628 \\ EGNN (AHo \& Cov.) & 0.3482 & 0.2374 & 0.2443 & 2.5632 & 0.1190 & **0.0462** & 1.2184 & 1015.8926 & 9.4814 \\  FA-GNN & 0.4414 & 0.2822 & 0.4302 & 2.5330 & 0.1696 & 0.1164 & 1.7886 & 22.7988 & 0.8617 \\ FA-GNN (AHo) & 0.3407 & 0.2263 & 0.2344 & 2.3272 & 0.1411 & 0.1306 & 1.6046 & **0.8796** & 0.8321 \\ FA-GNN (AHo \& Cov.) & 0.2785 & 0.1669 & 0.0815 & 5.4440 & 0.0493 & **0.0212** & 0.7768 & **15.3670** & 0.8814 \\  AbDiffuser (uniform prior) & 0.2837 & 0.1419 & 0.2188 & 3.1364 & 0.0727 & 0.1691 & 1.3874 & 38.8417 & 0.8398 \\ AbDiffuser (no projection) & 0.2378 & 0.1529 & 0.0694 & 2.3530 & 0.0637 & 0.0973 & **0.7376** & 6313.2495 & 11.431 \\ AbDiffuser (no Cov.) & 0.2309 & 0.1107 & 0.1235 & **1.2392** & 0.0864 & 0.0511 & **0.6453** & 17.7322 & **0.6302** \\  AbDiffuser & 0.1979 & **0.0921** & 0.0662 & **2.3197** & **0.0314** & **0.0285** & **0.0662** & **13.3051** & **0.5230** \\ AbDiffuser (side chains) & **0.0916** & **0.0520** & **0.0186** & 6.3166 & **0.0299** & 0.0754 & 0.8676 & 16.6117 & **0.4962** \\   

Table 1: Antibody generation based on Paired OAS . AHo denotes models that use aHo numbering and position-specific residue frequencies. Cov denotes models that use the learned covariance. IgLM is denoted by \(*\) since it was trained on significantly more data (including the test set) and was given part of the sequence to bootstrap generation. The top three results in each column are highlighted as **First**, **Second**, **Third**.

   Model & Parameters \(\) & Memory (training) \(\) & Memory (generation) \(\) & Generation time \(\) \\  Transformer & 84M & 14GB & 15GB & 3.2 min \\ EGNN & 39.3M & 78GB & 16GB & 22.6 min \\ FA-GNN & 94.4M & 75GB & 38GB & 9.5 min \\  AbDiffuser & 169M & 12GB & 3GB & 2.3 min \\   

Table 2: Number of parameters, model memory consumption during training with a batch size of 4 and memory consumption with the time taken to generate a batch of 10 examples for paired OAS.

predicted by the model for the \( G\) energy estimation. Even though this procedure is expected to generate results slightly different when compared to backbone-only models (in the latter case missing side chain atoms are first repacked based on the rotamer libraries before the energy minimization step), we still observe a high overlap between the resultant energies. This further highlights the quality of side-chain prediction.

It should be noted that \( G\) energy computation and some additional structure-based metrics  (i.e., _CDR PSH_, _CDR PPC_, _CDR PNC_, _SFV CSP_) are inherently susceptible to even minor changes in the geometry of the modeled structures. Thus, in line with the discussions by Raybould et al. , the overal trends of these metrics can be used to assess the generated samples as similar or dissimilar to the reference distributions, but one should not fall into the trap of overly focusing on the specific values attained. From this perspective, most structure-based models do sufficiently well on these metrics, perhaps with the exception of EGNN \( G\).

In Table 2 we show that APMixer is able to use an order of magnitude more parameters with a smaller memory footprint during training and offers more efficient sample generation, compared to the baseline architectures, on Nvidia A100 80GB GPU.

### Generating HER2 Binders

Antibody generative models can be used to explore a subset of the general antibody space, such as the binders of the target antigen. The purpose of modeling and expanding a set of binders is twofold: a) it allows us to rigorously validate our generative models in a setup more tractable than _denovo_ design; b) from a practical standpoint, it sets the ground for the optimization of properties that render binding antibodies drugs, such as developability, immunogenicity, and expression, allowing efficient exploration of the binder space [21; 63]. Note that sufficiently large libraries consisting of antibodies of variable binding affinity are usually discovered during the very early stages of the drug design process by means of high-throughput experiments [64; 76; 55]. Thus, the data used here can be sufficiently easier to obtain than the crystal structures usually assumed in CDR redesign experiments.

We use the Trastuzumab CDR H3 mutant dataset by Mason et al.  which was constructed by mutating 10 positions in the CDR H3 of the cancer drug Trastuzumab. The mutants were then evaluated using a high-throughput noisy assay to determine if they bind the HER2 antigen. After discarding all duplicate sequences and sequences that appear in both the binder set and the non-binder set, we are left with 9k binders and 25k non-binders. The generative models were trained only on the binder set. Separately, we train a classifier based on the APMixer architecture to distinguish binders from non-binders. The classifier achieves \(87.8\%\) accuracy on a randomly selected test set of 3000 antibodies, implying that the predicted binding probability is a somewhat informative metric for how tightly the binder distribution is approximated by the models.

The computational results summarized in Table 3 evidence that AbDiffuser closely approximates the binder distribution. Although most models achieve results close to those achieved by the validation set1, the sequence-based baseline struggles to capture the features that convey binding; evidenced by the low predicted binding probability. GNN-based baselines struggle to predict the correct structure

   Model & \(( G)}\) & \(( G_{0})}\) & \(( G_{0})}\) & \(( G)}\) & \(( G)}\) & \(( G)}\) & \(( G)}\) & RMSD1 & \(( G)}\) & \(( G)}\) & Units \\  Validation Set Baseline & 0.071 & 0.0093 & 0.0067 & 8.1187 & 0.0196 & 0.0174 & 0.0289 & 0.2059 & 0.2079 & — & 0.6876 & 1009 \\  BELAN  & 0.0707 & **0.0099** & 0.0067 & 8.1418 & 0.0184 & 0.0231 & 0.0148 & 0.5981 & 0.7729 & 0.7787 & 38.9 \\ DIAE  & 0.0704 & 0.0014 & 0.0048 & 0.5481 & 0.0007 & 0.0067 & 0.3647 & **6.2419** & 0.4151 & 0.8876 & 99.7 \\ ReflexCNN  & 0.011 & **0.0004** & **0.0048** & 0.5482 & **0.0452** & **0.0453** & **0.0046** & **0.1280** & — & — & 0.1212 & 1009 \\  Transformer (All60) & 0.0014 & 0.0031 & 0.0007 & 1.3681 & 0.0171 & 0.0140 & 0.3657 & — & — & 0.3627 & 1009 \\ ECAN (All60 \& A-Conv) & 0.013 & 0.0000 & 0.0012 & 1.2411 & 0.0123 & 0.024 & 0.2441 & 1097,5280 & 0.2180 & 0.3626 & 1009 \\ ECAN-(All60 \& A-Conv) & 0.018 & 0.0000 & 0.0003 & **0.4158** & 0.0007 & 0.0056 & 0.3844 & 76.782 & 3.3100 & 0.4565 & 1009 \\  AbDiffuser (sd-class) & 0.0103 & **0.0018** & **0.0028** & **0.0048** & 0.00205 & 0.00113 & 0.1588 & **6.4010** & **0.8022** & 0.3761 & 1009 \\  AbDiffuser (sd-class) & **0.0100** & **0.0005** & 0.0002 & 1.2509 & **0.00115** & **0.0029** & **0.0035** & 20.0464 & 0.0466 & 0.0688 & 1009 \\  AbDiffuser (\(r=0.75\)) & **0.0005** & 0.0011 & **0.0054** & **0.00148** & 0.0148 & 0.0129 & 0.1785 & **6.2408** & **0.3707** & 0.6352 & **1009** \\  AbDiffuser (\(s_{r}=0.75\)) & **0.0005** & **0.0004** & 0.01026 & 1.8310 & 0.0126 & **0.0017** & **0.0017** & 12.8973 & 0.9862 & **0.7705** & 1009 \\  AbDiffuser (\(r=0.01\)) & **0.0005** & 0.0004 & 0.0026 & 2.9044 & 0.0206 & 0.0003 & 0.0238 & 15.2200 & **0.0340** & **0.0151** & 99.78-9 \\ AbDiffuser (s.c. \(r=0.1\)) & 0.0015 & 0.00024 & 0.0159 & 1.5043 & 0.0210 & 0.0126 & 0.5173 & 114.4841 & 0.6705 & **0.8448** & 91.4\% \\   

Table 3: Generating Trastuzumab mutants based on the dataset by Mason et al. . The top three results in each column are highlighted as **First**, **Second**, and **Third**. Multiple approaches can generate sequences similar to the test set, but generating predicted binders (large \(p_{}\)) is considerably harder.

(RMSD above 3) producing structures with unreasonably high Amber energy. As a result, the binding probability of their designs is lower than that of the more powerful structural models.

Next, we look at the two baselines that only redesign CDR H3 instead of generating the whole antibody. RefineGNN  manages to closely capture the sequence and biophysical characteristics and generates sufficiently good binders. DiffAb , which is another diffusion-based model, achieves the best binding probability out of the CDR redesign baselines. MEAN  generated few unique sequences (38.9% uniqueness) and failed some of the distribution-based metrics (Naturalness, CDR PSH). dyMEAN  collapsed to always generating a single sample, thus we did not report its results. The overfitting behavior of MEAN and dyMEAN can be attributed to the reconstruction-based training objective and the fact that they were designed for a slightly different task of CDR redesign with different antigens, instead of the single antigen we have here.

Due to the use of iterative refinement and reconstruction-based training objectives, RefineGNN and MEAN focus on the generation of the most likely examples. Focusing on high-likelihood modes is especially beneficial here, as the experiment that was used to create the dataset is noisy. Diffusion models can also be adjusted to focus on prevalent modes by reducing the temperature \(\) of the denoising process. In the Gaussian case, we specified the temperature as the scaling factor of the noise added during the reverse process (Appendix B.2), whereas in the discrete case we specified it as the temperature of the model output softmax function (Appendix B.3). We observe that a slight reduction of the temperature helps to improve the general distribution fit across almost all metrics. Reducing the temperature further boosts the binding probability, but, as expected, can result in a slight loss of diversity. Using a higher temperature slightly increases the Stability Wasserstein distance while improving Stability. The phenomenon occurs because the model is no longer concerned with fitting low-likelihood modes of the real distribution that contain structures of poor stability.

In contrast to MEAN, RefineGNN and DiffAb, which only redesign the CDR H3 of the test-set structures, AbDiffuser generates full antibodies and still achieves better sequence similarity. MEAN  also produced structures of noticeably worse RMSD, which can be explained as follows: a) as we see in Appendix K, MEAN does not predict CDR H3 positions as well; b) changing CDR H3 can impact the most-likely conformation of the overall antibody, something that the CDR inpainting models cannot account for. We do not report the RMSD and \( G\) for RefineGNN as it does not place the generated CDR H3 loop in the reference frame of the original antibody.

The AbDiffuser model that also generates side chains generally achieved better sequence similarity (Naturalness, Closeness) and better binding probability than the backbone-only model, but a worse similarity of sequence stability. Furthermore, while the side chain model achieved a worse overall structure quality (\( G\), RMSD), as we see in Appendix K it predicted CDR H3 positions more precisely, which is the main desideratum in this experiment.

### In Vitro Validation

We further validate our designs through an _in vitro_ experiment. As shown in Figure 2, all submitted designs were expressed and purified successfully (average concentration of 1.25 mg/ml) and an average of 37.5% of the designs were confirmed binders with \([8.32,9.50]\) (higher is better) whose average was 8.70. The binding rate was improved (from 22. 2% to 57. 1%) when considering designs that were additionally filtered so that they were better than the bottom 25th quantile in every metric (naturalness, RMSD, etc.) and a classifier trained to distinguish binders from non-binders predicted that they bound with high confidence. This increase in binding for filtered samples suggests that our selected metrics indeed correlate with desirable _in vitro_ characteristics. Our best binder belonged to the latter set and its pKD of 9.50 was slightly above Trastuzumab (not in the training set) while differing in 4 positions of the CDR H3 loop. Further examination of the top binders is performed in Figure 3 and Appendix L.

In contrast to the only other comparable study for ML-based Trastuzumab CDR H3 mutant design by Shanehsazzadeh et al. , our best binder had \(=-_{10}(3.17^{-10})=9.50\), while the best binder found by Shanehsazzadeh et al. had \(=9.03\). Two important differences between the two studies are that: we trained on binders and tested 16 samples in the wet lab, while Shanehsazzadeh et al. used a simpler sequence infiling model trained on a large set of generic antibodies to generate 440k candidate sequences that were filtered in the wet-lab using high-throughput screening to identify 4k binders, of which 421 were selected to be tested using precise SPR measurements. The fact that our approach required 26x fewer precise SPR wet-lab experiments to find a better binder hints toward a major improvement in efficiency. This highlights the importance of new and more efficient generative models for antibodies and more efficient, and thus powerful architectures such as APMixer.

The work of Shanehsazzadeh et al.  also shows that high-throughput wet-lab experiments can be used to build sufficiently large datasets of target-specific binders without starting from a known binder. This provides a straightforward way to train AbDiffuser on new targets.

After good binders are determined through high-throughput and SPR experiments, at the end of the drug discovery process, an experimental 3D structure of the bound complex is often produced. In recent works, redesigning CDRs in such co-crystal structures has been a popular task [53; 43; 34]. To that end, Appendix N investigates how AbDiffuser can be adopted for it and shows that it offers much better sequence recovery rates than the current state-of-the-art diffusion model for the CDR redesign .

## 6 Conclusions

We propose a denoising model that deviates from conventional practice in deep learning for protein design. APMixer enjoys the benefits of the best protein models (SE(3) equivariance, universality), while being significantly more memory efficient. We also show how to tailor the diffusion process to antibody generation by incorporating priors that enable the handling of variable-length sequences and allow the model to navigate strong bond constraints while operating in an extrinsic coordinate space that supports Gaussian noise. In our future work, we want to apply our method to protein families beyond antibodies, as organized in CATH  and Pfam . Promising examples include the TIM-barrel fold and enzyme families, such as SAM-dependent methyltransferases, long-chain alcohol oxidases, and amine dehydrogenases .

Figure 3: HER2 structure and the 3D structures of generated binders highlighted in different colors. AbDiffuser has learned to redesign part of the binding interface while maintaining affinity. Our tightest binder achieved a pKD of 9.5 which, accounting for experimental variability, is akin to Trastuzumab whose average measured pKD was 9.21.

Figure 2: _In vitro_ validation of AbDiffuser designs in terms of their ability to express (left), binding affinity (center), and binding rate (right). The ‘raw’ column corresponds to randomly selected generated antibodies, whereas ‘filtered’ designs were additionally filtered by _in silico_ screening.