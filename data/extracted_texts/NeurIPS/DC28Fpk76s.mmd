# Intervention and Conditioning in Causal Bayesian Networks

Sainyam Galhotra

Computer Science Dept.

Cornell University

sg@cs.cornell.edu &Joseph Y. Halpern

Computer Science Dept.

Cornell University

halpern@cs.cornell.edu

###### Abstract

Causal models are crucial for understanding complex systems and identifying causal relationships among variables. Even though causal models are extremely popular, conditional probability calculation of formulas involving interventions pose significant challenges. In case of Causal Bayesian Networks (CBNs), Pearl assumes autonomy of mechanisms that determine interventions to calculate a range of probabilities. We show that by making simple yet often realistic independence assumptions, it is possible to uniquely estimate the probability of an interventional formula (including the well-studied notions of probability of sufficiency and necessity). We discuss when these assumptions are appropriate. Importantly, in many cases of interest, when the assumptions are appropriate, these probability estimates can be evaluated using observational data, which carries immense significance in scenarios where conducting experiments is impractical or unfeasible.

## 1 Introduction

Causal models play a pivotal role in elucidating the causal relationships among variables. These models facilitate a principled approach to understanding how various factors interact and influence each other in complex systems. For instance, in epidemiology, causal models often help us understand the relationship between lifestyle choices and health outcomes (Greenland, Pearl, and Robins 1999); and in economics, they help to analyze the impact of policy changes on market dynamics (Hicks 1979). These examples underscore the versatility and utility of causal models for providing a formal representation of system variables.

Interventions and conditioning are the most fundamental procedures in the application of causal models, useful to examine and analyze causal mechanisms. For example, interventions help explain the outcome of complex ML systems (Galhotra, Pradhan, and Salimi 2021); and in AI-driven healthcare diagnostics, it is crucial to discern the effect of a particular intervention (like a change in treatment protocol) on patient outcomes (Greenland 1999).

Despite their utility, calculating the probabilities related to interventions and conditioning in tandem presents significant challenges. Indeed, it is not even clear what the semantics of queries involving counterfactuals is. Work in the AI literature has focused on two types of models: _functional_ causal models1 and _causal Bayesian networks_ (Pearl 2000). Both are typically described using directed acyclic graphs, where each node is associated with a variable. In a causal model, with each variable \(Y\) associated with a non-root node, there is a deterministic (structural) equation, that gives the value of \(Y\) as a function of the values of its parents; there is also a probability on the values of root nodes. In a CBN, like in a Bayesian network, each variable \(Y\) is associated with a _conditional probability_table (cpt)_, that for each setting of the parents of \(Y\), gives the probability of \(Y\) conditional on that setting. In a functional causal model, it is actually straightforward to determine the conditional probability of formulas involving interventions. In a CBN, this is far from true. Indeed, recent work of Beckers (2023) has shown that an approach given by Pearl (2000) to calculate these probabilities in a CBN is incorrect. 2 Pearl also calculates probabilities in a CBN by implicitly reducing the CBN to a family of functional causal models (see, e.g., (Pearl 2000, Theorem 9.2.10)), but he does not give an explicit reduction, nor does he give a formal definition of the probability of a formula in a CBN. Here, we do both. Using this approach leads to formulas having a range of probabilities in a CBN, whereas in a functional causal model, their probability is unique.

But we take an additional significant step. Pearl assumes that mechanisms that determine how interventions work (which are given by the cpts in the case of CBNs and the structural equations in the case of causal models) are _autonomous_: as Pearl puts it, "external changes affecting one equation do not imply changes to the others" (Pearl 2000, p. 28). We model this autonomy formally by taking the equations to be independent of each other, in an appropriate space. As shown recently by Richardson, Peters, and Halpern (2024), taking the equations that characterize different variables to be independent is a necessary and sufficient condition for reproducing all the (conditional) independencies in the underlying Bayesian network, as determined by _d-separation_(Pearl 1988). Thus, this independence seems like a natural and critical assumption to get CBNs and causal models to work as we would expect.

Here we assume that, not only are the equations that define different variables independent, but also the equations that give the values of a variable for different settings of its parents. We never need to consider the values of a variable for different settings of its parents in a standard Bayesian network, but this is necessary to determine the probability of a formula involving interventions, such as \(X=0 Y=0[X 1](Y=1)\) (\(X\) and \(Y\) have value 0, but if \(X\) is set to 1, \(Y\) gets value 1). Taking these latter equations to be independent is not always appropriate;3 For example, there may be a latent exogenous variable that affects the value of \(Y\) for different settings of \(Y\)'s parents. But if the parents of \(Y\) (including exogenous variables) are all observable, and screen \(Y\) off from the effects of all other variables, then the independence assumption seems appropriate.

Making these independence assumptions has significant benefits. For one thing, it allows us to uniquely identify the probability of queries in a CBN; rather than getting a range of values, we get a unique value. Moreover, for many formulas of interest (including the _probability of necessity_ and _probability of sufficiency_(Pearl 2000), we can compute the probability by considering only conditional probabilities involving only a subset of endogenous and exogenous variables, which do not involve interventions. This means that these probabilities can be estimated from observational data, without requiring involving controlled experiments. This can have huge implications in settings where such experimental data is not available but the exogenous variables can be observed.

The rest of this paper is organized as follows. Section 2 reviews the formalism of causal models. Section 3 gives semantics to formulas in Causal Bayesian Networks (CBNs) and Section 4 shows that any CBN can be converted to a compatible causal model that satisfies the independence assumptions that we are interested in. We show how counterfactual probabilities of necessity and sufficiency can be simplified and calculated in the appendix.

## 2 Causal Models and CBNs

In a _(functional) causal model_ (also called a _structural equations model_), the world is assumed to be described in terms of variables and their values. Some variables may have a causal influence on others. This influence is modeled by a set of _structural equations_. It is conceptually useful to split the variables into two sets: the _exogenous_ variables, whose values are determined by factors outside the model, and the _endogenous_ variables, whose values are ultimately determined by the exogenous variables. In some settings, exogenous variables can be observed; but they can never be intervened upon, as (by assumption) their values are determined by factors outside the model. Note that exogenous variables may involve latent factors that are not observable, and may even be unknown. For example, in an agricultural setting, we could have endogenous variables that describe crop produce, amount of fertilizers used, water consumption, and so on, and exogenous variables that describe weather conditions (which cannot be modified, but can be observed) and some latent factors, like the activity level of pollinators (which cannot be observed or measured). The structural equations describe how the values of endogenous variables are determined (e.g., how the water consumption depends on the weather conditions and the amount of fertilizer used).

Formally, a _causal model_\(M\) is a pair \((,),\) where \(\) is a _signature_, which explicitly lists the endogenous and exogenous variables and characterizes their possible values, and \(\) defines a set of _modifiable structural equations_, relating the values of the variables. A signature \(\) is a tuple \((,,)\), where \(\) is a set of exogenous variables, \(\) is a set of endogenous variables, and \(\) associates with every variable \(Y\) a nonempty set \((Y)\) of possible values for \(Y\) (that is, the set of values over which \(Y\)_ranges_). For simplicity, we assume that \(\) is finite, as is \((Y)\) for every endogenous variable \(Y\). \(\) associates with each endogenous variable \(X\) a function denoted \(F_{X}\) such that \(F_{X}:(_{U}(U))(_{Y- \{X\}}(Y))(X).\) This mathematical notation just makes precise the fact that \(F_{X}\) determines the value of \(X\), given the values of all the other variables in \(\).

The structural equations define what happens in the presence of external interventions. Setting the value of some variable \(X\) to \(x\) in a causal model \(M=(,)\) results in a new causal model, denoted \(M_{X x}\), which is identical to \(M\), except that the equation for \(X\) in \(\) is replaced by \(X=x\).

Following most of the literature, we restrict attention here to what are called _recursive_ (or _acyclic_) models. In such models, there is a total ordering \(\) of the endogenous variables such that if \(X Y\), then \(X\) is not causally influenced by \(Y\), that is, \(F_{X}(,y,)=F_{X}(,y^{},)\) for all \(y,y^{}(Y)\). If \(X Y\), then the value of \(X\) may affect the value of \(Y\), but the value of \(Y\) cannot affect the value of \(X\). It should be clear that if \(M\) is an acyclic causal model, then given a _context_, that is, a setting \(\) for the exogenous variables in \(\), there is a unique solution for all the equations. We simply solve for the variables in the order given by \(\).

A recursive causal model can be described by a dag (directed acyclic graph) whose nodes are labeled by variables, and there is an edge from \(X\) to \(Y\) if \(X Y\). We can assume without loss of generality that the equation for \(Y\) involves only the parents of \(Y\) in the dag. The roots of the dag are labeled by exogenous variables or endogenous variables with no parents; all the remaining nodes are labeled by endogenous variables.4

A _probabilistic_ (functional) causal model is a pair \((M,)\) consisting of a causal model \(M\) and a probability \(\) on the contexts of \(M\). In the rest of this paper, when we refer to a "causal model", we mean a probabilistic functional causal model, unless we explicitly say otherwise.

A _causal Bayesian network (CBN)_ is a tuple \(M=(,)\) described by a signature \(\), just like a causal model, and a collection \(\) of _conditional probability tables (cpts)_, one for each (endogenous and exogenous) variable.5 For this paper, we focus on recursive CBNs that can be characterized by a dag, where there is a bijection between the nodes and the (exogenous and endogenous) variables. The cpt for a variable \(X\) quantifies the effects of the parents of \(X\) on \(X\). For example, if the parents of \(X\) are \(Y\) and \(Z\) and all variables are binary, then the cpt for \(X\) would have entries for all \(j,k\{0,1\}^{2}\), where the entry for \((j,k)\) describes\(\{Pr(X=0 Y=j,Z=k)\). (There is no need to havean explicit entry for \(P(X=1 Y=j Z=k)\), since this is just \(1-P(X=0 Y=j Z=k)\).) The cpt for a root of the dag is just an unconditional probability, since a root has no parents.

Just as for causal models, we can also perform interventions in a CBN: intervening to set the value of some variable \(X\) to \(x\) in a CBN \(M\) results in a new CBN, denoted \(M_{X x}\), which is identical to \(M\), except that now \(X\) has no parents; the cpt for \(X\) just gives \(X\) value \(x\) with probability 1.

Note that we typically use the letter \(M\) to refer to both non-probabilistic causal models and CBNs, while we use \(\) to refer to the probability on contexts in a probabilistic causal model. We use \(P\) to refer to the probability in a cpt. It is also worth noting that a causal model can be viewed as a CBN; the equation \(Y=F()\) can be identified with the entry \(P(Y=F())=)=1\) in a cpt.

## 3 Giving semantics to formulas in CBNs

### The problem

Consider the following (standard) language for reasoning about causality: Given a signature \(=(,,)\), a _primitive event_ is a formula of the form \(X=x\), for \(X\) and \(x(X)\). A _causal formula (over \(\))_ is one of the form \([Y_{1} y_{1},,Y_{k} y_{k}]\), where \(\) is a Boolean combination of primitive events, \(Y_{1},,Y_{k}\) are distinct variables in \(\), and \(y_{i}(Y_{i})\). Such a formula is abbreviated as \([]\). The special case where \(k=0\) is abbreviated as \(\). Intuitively, \([Y_{1} y_{1},,Y_{k} y_{k}]\) says that \(\) would hold if \(Y_{i}\) were set to \(y_{i}\), for \(i=1,,k\). \(()\) is the language consisting of Boolean combinations of causal formulas. We typically take the signature \(\) to be fixed, and just write \(\). It will be convenient to consider a slightly richer language, that we denote \(^{+}()\). It extends \(()\) by allowing primitive events \(U=u\), where \(U\), and also allowing interventions on exogenous variables.6

A pair \((M,)\) consisting of a (non-probabilistic) causal model \(M\) and a context \(\) is called a _(causal) setting_. A formula \(^{+}\) is either true or false in a setting. We write \((M,)\) if the causal formula \(\) is true in the setting \((M,)\). The \(\) relation is defined inductively. \((M,) X=x\) if the variable \(X\) has value \(x\) in the unique (since we are dealing with acyclic models) solution to the equations in \(M\) in context \(\) (that is, the unique vector of values for the endogenous variables that simultaneously satisfies all equations in \(M\) with the variables in \(\) set to \(\)). The truth of conjunctions and negations is defined in the standard way. Finally, \((M,)[]\) if \((M_{},_{})\), where \((M_{}\) is identical to \(M\) except that the equation for each endogenous variable \(Y\) is replaced by \(Y=y^{*}\), where \(y^{*}(Y)\) is the value in \(\) corresponding to \(Y\), and \(_{}\) is identical to \(\), except that for each exogenous variable \(U\), the component of \(\) corresponding to \(U\) is replaced by \(u^{*}\), where \(u^{*}(U)\) is the value in \(\) corresponding to \(U\). (We remark that in a CBN, intervening to set an exogenous variable \(U\) to \(u^{*}\) is just like any other intervention; we change the cpt for \(U\) so that \(u^{*}\) gets probability 1.)

In a probabilistic causal model \((M,)\), we can assign a probability to formulas in \(\) by taking the probability of a formula \(\) in \(M\), denoted \(()\), to be \((\{:(M,)=\})\). Thus, the probability of \(\) in \(M\) is simply the probability of the set of contexts in which \(\) is true; we can view each formula as corresponding to an event.

When we move to CBNs, things are not so straightforward. First, while we still have a probability on contexts, each context determines a probability on _states_, assignments of values to variables. A state clearly determines a truth value for formulas that do not involve interventions; call such formulas _simple formulas_. Thus, we can compute the truth of a simple formula \(\) in a context, and then using the probability of contexts, determine the probability of \(\) in a CBN \(M\). But what about a causal formula such as \(=[]\)? Given a context \(\), we can determine the model \(M^{}=M_{}\). In \((M^{},)\), \(\) is an event whose probability we can compute, as discussed above. We can (and will) take this probability to be the probability of the formula \(\) in \((M,)\). But note that \(\) does not correspond to an event in \(M\), although we assign it a probability.

It gets harder to evaluate probability if we add another conjunct \(^{}\) and consider the formula \(^{}\). While we can use the procedure above to compute the probability of \(\) and \(^{}\) individually in \((M,)\), what is the probability of the conjunction? Because such formulas do not correspond to events in \(M\), this is not obvious. We give one approach for defining the probability of a formula in a CBN by making one key assumption, which can be viewed as a generalization of Pearl's assumption. Pearl assumes that mechanisms that determine how interventions work (which are the cpts in the case of CBNs and the structural equations in the case of causal models) are _autonomous_; he takes that to mean "it is conceivable to change one such relationship without changing the others" (Pearl 2000, p. 22). We go further and assume, roughly speaking, that they are (probabilistically) independent. In a causal model, the mechanism for a given variable (specifically, the outcome after the intervention) is an event, so we can talk about mechanisms being independent. While it is not an event in a CBN, we nevertheless use the assumption that mechanisms are independent to guide how we determine the probability of formulas in \(\) in a CBN.

### Independence of cpts and complete combinations of conditional events

To describe our approach, we must first make clear what we mean by mechanisms (cpts) being independent. This has two components: the outcomes of cpts for different variables are independent, and for the cpt for a single variable Y, the outcomes for different settings of the parents of \(Y\) are independent. Indeed, all these outcomes are mutually independent. We believe that these independence assumptions are quite reasonable and, capture the spirit of Bayesian networks. In fact, Richardson, Peters, and Halpern (2024) show that the assumption that cpts involving different variables are independent is equivalent to the (conditional) independence assumptions made in Bayesian networks (see Section 3.4 for further discussion).

In more detail, suppose that we have a variable \(Y_{1}\) in a CBN \(M\) with parents \(X_{1},X_{m}\). We want to consider events of the form \(Y_{1}=y_{1}(X_{1}=x_{1},,X_{m}=x_{m})\), which we read "\(Y_{1}=y_{1}\) given that \(X_{1}=x_{1}\),..., and \(X_{m}=x_{m}\)". Such events have a probability, given by the cpts for \(Y_{1}\). We call such an event a _conditional event for CBN_\(M\). (Explicitly mentioning the CBN \(M\) is necessary, since on the right-hand side of the conditional with left-hand side \(Y\), we have all the parents of \(Y\); what the parents are depends on \(M\)). Roughly speaking, we identify such a conditional event with the formula \([X_{1} j_{1},,X_{m} j_{m}](Y_{1}=1)\). This identification already hints at why we care about conditional events (and their independence). Suppose for simplicity that \(m=1\). To determine the probability of a formula such as \(X_{1}=0 Y_{1}=0[X_{1} 1](Y_{1}=1)\) we need to apply both the entry in the cpt for \(Y_{1}=0 X_{1}=0\) and the entry for \(Y_{1}=1 X=1\). They each give a probability; the probability of the formula \(X_{1}=0 Y_{1}=0[X_{1} 1](Y_{1}=1)\) is the probability that the conditional events \(Y_{1}=0 X_{1}=0\) and \(Y_{1}=1 X=1\) hold simultaneously. Our independence assumption implies that this probability is the product of the probability that each of them holds individually (which is given by the cpt for \(Y_{1}\)).

This is an instance of independence within a cpt; we want the conditional events in a cpt for a variable \(Y\) for different settings of the parents of \(Y\) to be independent. (Of course, conditional events for the same setting of the parents, such as \(Y_{1}=0 X_{1}=1\) and \(Y_{1}=1 X_{1}=1\), are not independent.) Independence for cpts of different variables is most easily explained by example: Suppose that \(Y_{2}\) has parents \(X_{1}\) and \(X_{3}\). Then we want the events \(Y_{1}=0 X_{1}=0\) and \(Y_{2}=1(X_{1}=0,X_{3}=1)\) to be independent. This independence assumption will be needed to compute the probability of formulas such as \([X_{1} 0](Y_{1}=0)[X_{1}=0,X_{3}=1](Y_{2}=1)\). As we said, we in fact want to view all the relevant conditional events as _mutually_ independent.7

Although we use the term "conditional event", these are not events in a CBN. On the other hand, in a causal model, there are corresponding notions that really do correspond to events. For example, the conditional event \(Y_{1}=0 X_{1}=1\) corresponds to the set of contexts where the formula \([X_{1} 1](Y_{1}=1)\) is true. Starting with a CBN \(M\), we will be interested in causal models for which the probability \(P(Y_{1}=0 X_{1}=1)\), as given by the cpt for \(Y_{1}\) in \(M\), is equal to the probability of the corresponding event in the causal model.

Going back to CBNs, define a _complete combination of conditional events (ccee) for \(M\)_ to be a conjunction consisting of the choice of one conditional event for \(M\) for each endogenous variable \(X\) and each setting of the parents of \(X\). A _fixed-context_ ccee (fccce) involves fewer conjuncts; we have only conditional events where for all the exogenous parents \(U\) of a variable \(X\), the value of \(U\) is the same as its value in the conjunct determining the value of \(U\) (the examples should make clear what this means).

**Example 3.1**.: _Consider the CBN \(M^{*}\) with the following dag:, where all variables are binary, and the cpts give the following probabilities: \(P(U=0)=a\), \(P(X=0 U=0)=b\), \(P(X=0 U=1)=c\), \(P(Y=0 X=0)=d\), and \(P(Y=0 X=1)=e\). Then a cccc consists of 5 conjuncts:_

* _one of_ \(U=0\) _and_ \(U=1\)_;_
* _one of_ \(X=0 U=0\) _and_ \(X=1 U=0\)_;_
* _one of_ \(X=0 U=1\) _and_ \(X=1 U=1\)_;_
* _one of_ \(Y=0 X=0\) _and_ \(Y=1 X=0\)_; and_
* _one of_ \(Y=0 X=1\) _and_ \(Y=1 X=1\)_._

_An fccc consist of only 4 conjuncts; it has only one of the second and third conjuncts of a cccc. In particular, if \(U=0\) is a conjunct of the fccce, then we have neither \(X=0 U=1\) nor \(X=1 U=1\) as a conjunct; similarly, if \(U=1\) is a conjunct, then we have neither \(X=0 U=0\) nor \(X=1 U=0\) as a conjunct. (This is what we meant above by saying that each exogenous parent \(U\) of \(X\) must have the same value as in conjunct that determines \(U\)'s value.)_

It is not hard to show that, in this case, there are 32 ccces and 16 fccces. Moreover, (in this example and in general) each fccce is equivalent to a disjunction of ccces. The number of ccces and fccces can be as high as doubly exponential (in the number of variables), each one involving exponentially many choices. For example, if a variable \(Y\) has \(n\) parents, each of them binary, there are \(2^{n}\) possible settings of the parents of \(Y\), and we must choose one value of \(Y\) for each of these \(2^{n}\) settings, already giving us \(2^{2^{n}}\) choices. It is easy to see that there is also a double-exponential upper bound.

If we think of a conditional event of the form \(Z=1 X=0,Y=0\) as saying "if \(X\) were (set to) 0 and \(Y\) were (set to) 0, then \(Z\) would be 1", then given a ccc and a formula \(\) and context \(\), we can determine if \(\) is true or false. We formalize this shortly. We can then take the probability of \(\) to be the sum of the probabilities of the ccces that make \(\) true. The probability of a cccc is determined by the corresponding entry of the cpt. Thus, if we further assume independence, we can determine the probability of each cccc, and hence the probability of any formula \(\). We now give some informal examples of how this works, and then formalize the procedure in Section 3.3.

**Example 3.2**.: _In the CBN \(M^{*}\) described in Example 3.1, there are two fccces where \(=X=0 Y=0[X 1](Y=1)\) is true: (a) \(U=0(X=0 U=0)(Y=0 X=0)(Y=1 X=1)\); and (b) \(U=1(X=0 U=1)(Y=0 X=0)(Y=1 X=1)\). Each of these two fccces is the disjunction of two ccces, which extend the fccce by adding a fifth conjunct. For example, for the first fccce, we can add either the conjunct \(X=0 U=1\) or the conjunct \(X=1 U=1\). The total probability of these two fccces is \(abd(1-e)+(1-a)cd(1-e)\); this is the probability of \(\) in \(M^{*}\)._

We give one more example of this calculation.

**Example 3.3**.: _Consider the model CBN \(M^{}\), which differs from \(M^{*}\) in that now \(U\) is also a parent of \(Y\); the dag is shown below. \(M^{*}\) and \(M^{}\) have the same cpts for \(U\) and \(X\); the cpt of \(Y\) in \(M^{}\) is \(P(Y=0 U=0,X=0)=f_{1}\), \(P(Y=0 U=0,X=1)=f_{2}\), \(P(Y=0 U=1,X=0)=f_{3}\), \(P(Y=0 U=1,X=1)=f_{4}\)._

_Now there are 128 ccces, but only 16 fccces; the formula \(=X=0 Y=0[X 1](Y=1)\) is true in only two of these fccces: (a) \(U=0(X=0 U=0)(Y=0(U=0,X=0))(Y=1(U=0,X=1))\); and (b) \(U=1)X=0 U=1)(Y=0(U=1,X=0))(Y=1(U=1,X=1))\). It is easy to check that \(_{M^{}}()=abf_{1}(1-f_{2})+(1-a)cf_{3}(1-f_{4})\). The calculation of the probability of \(\) is essentially the same in \(M^{*}\) and \(M^{}\)._We denote by \(_{M}()\) the probability of a formula \(\) in a CBN or causal model \(M\). (We provide a formal definition of \(_{M}()\) for a CBN \(M\) at the end of Section 3.)

### Giving semantics to formulas in CBNs

We already hinted in Examples 3.2 and 3.3 how we give semantics to formulas in CBNs. We now formalize this.

The first step is to show that a cccc (resp., fccce) determines the truth of a formula in \(^{+}()\) (resp., \(()\)) in a causal model. To make this precise, we need a few definitions. We take the _type_ of a CBN \(M=(,)\), where \(=(,,)\) to consist of its signature \(\) and, for each endogenous variable, a list of its parents (which is essentially given by the dag associated with \(M\), without the cpts). A causal model \(M^{}=(^{},^{})\) has the same type as \(M\) if \(^{}=(^{},,^{})\), where \(^{}\) is arbitrary, \(^{}|_{}=\), and \(^{}\) is such that each endogenous variable \(X\) depends on the same variables in \(\) according to \(^{}\) as it does according to the type of \(M\) (but may also depend on any subset of \(^{}\)).

**Definition 3.4**.: _For the conditional event \(Y=y(X_{1}=x_{1},,X_{m}=x_{m})\), let the corresponding formula be \([X_{1} x_{1},,X_{m} x_{m}](Y=y)\). (Note that the corresponding formula may be in \(^{+}-\), since some of the \(X_{i}\)s may be exogenous.) Let \(_{}^{+}()\), the formula corresponding to the cccc \(\), be the conjunction of the formulas corresponding to the conditional events in \(\). We can similarly define the formula corresponding to an fccce._

**Example 3.5**.: _In the model \(M^{}\) of Example 3.3, if \(\) is the fccce \(U=0(X=0 U=0)(Y=0(U=0,X=0))(Y=1(U=0,X=1))\), then \(_{}\) is \(U=0[U 0]X=0[U 0,X 0](Y=0)[U  0,X 1](Y=1)\)._

Say that a formula \(\) is _valid with respect to a CBN \(M\)_ if \((M^{},)\) for all causal settings \((M^{},)\), where \(M^{}\) is a causal model with the same type as \(M\). The following theorem makes precise the sense in which a cccc determines whether or not an arbitrary formula is true.

**Theorem 3.6**.: _Given a CBN \(M=(,)\) and a cccc (resp., fccce) \(\), then for all formulas \(^{+}()\) (resp., \(()\)) either \(_{}\) is valid with respect to \(M\) or \(_{}-\) is valid with respect to \(M\)._

**Proof:** We show that if two causal models \(M_{1}\) and \(M_{2}\) have the same type as \(M\) and \(_{1}\) and \(_{2}\) are contexts such that \((M_{1},_{1})_{}\) and \((M_{2},_{2})_{}\), then for all formulas \(^{+}()\) (resp., \(()\)), we have that

\[(M_{1},_{1})(M_{2},_{2}).\] (1)

The claimed result follows immediately. The details of the proof can be found in the appendix.

Based on this result, we can take the probability of a formula \(^{+}()\) in a CBN \(M\) to be the probability of the cccces that imply it. To make this precise, given a CBN \(M\), say that a probabilistic causal model \((M^{},)\) is _compatible_ with \(M\) if \(M^{}\) has the same type as \(M\), and the probability \(\) is such that all the cpts in \(M\) get the right probability in \(M^{}\). More precisely, for each endogenous variable \(Y\) in \(M\), if \(X_{1},,X_{k}\) are the parents of \(Y\) in \(M\), then for each entry \(P(Y=y X_{1}=x_{1},,X_{k}=x_{k})=a\) in the cpt for \(Y\), \(\) is such that the corresponding formula \([X_{1} x_{1},,X_{k} x_{k}](Y=y)\) gets probability \(a\). \((M^{},)\) is _i-compatible_ with \(M\) (the \(i\) stands for _independence_) if it is compatible with \(M\) and, in addition, \(\) is such that the events described by the formulas corresponding to entries for cpts for different variable (i.e. the set of contexts in \(M\) that make these formulas true) are independent, as are the events described by the formulas corresponding to different entries for the cpt for a given variable. Thus, for example, if \((x^{}_{1},,x^{}_{k})(x_{1},,x_{k})\), then we want the events described by \([X_{1} x_{1},,X_{k} x_{k}](Y=y)\) and \([X_{1} x^{}_{1},,X_{k} x^{}_{k}](Y=y)\) to be independent (these are different entries of the cpt for \(Y\)); and if \(Y^{} Y\) and has parents \(X^{}_{1},,X^{}_{m}\) in \(M\), then we want the events described by \([X_{1} x_{1},,X_{k} x_{k}](Y=y)\) and \([X^{}_{1} x^{}_{1},,X_{m} x^{}_{m}](Y^{ }=y^{})\) to be independent (these are entries of cpts for different variables).

**Theorem 3.7**.: _Given a CBN \(M\) and a formula \(^{+}()\), the probability of \(\) is the same in all causal models \(M^{}\) i-compatible with \(M\)._

**Proof:** It follows from Theorem 3.6 that the probability of \(\) is the sum of the probabilities of the formulas \(_{}\) for the ccces \(\) such that \(_{}\) is valid. It is immediate that these formulas have the same probability in all causal models i-compatible with \(M\).

Formally, we take \(_{M}()\), the probability of \(\) in the CBN \(M\), to be \(_{M^{}}()\) for a causal model \(M^{}\) i-compatible with \(M\). By Theorem 3.7, it does not matter which causal model \(M^{}\) i-compatible with \(M\) we consider. Note for future reference that if we consider only causal models compatible with \(M\), dropping the independence assumption, we would get a range of probabilities.

### Discussion

Four points are worth making: First, note that this way of assigning probabilities in a CBN \(M\) always results in the probability of a formula \(^{+}\) being a sum of products of entries in the cpt. Thus, we can in principle compute the probabilities of (conditional) events involving interventions from observations of statistical frequencies (at least, as long as all settings of the parents of a variable in the relevant entries of the cpt have positive probability).

Second, the number of ccces may make the computation of the probability of a formula in a CBN seem unacceptably high. As the examples above shows, in practice, it is not so bad. For example, we typically do not actually have to deal with ccces. For one thing, it follows from Theorem 3.6 that to compute the probability of \(\), it suffices to consider fccces. Moreover, when computing \(_{M}()\) where \(\) involves an intervention of the form \(X x\), we can ignore the entries in the cpts involving \(X\), and for variables for which \(X\) is a parent, we consider only entries in the cpts where \(X=x\). We can also take advantage of the structure of the formula whose probability we are interested in computing to further simplify the computation, although the details are beyond the scope of this paper.

Third, as mentioned above, a formula involving interventions does not correspond in an obvious way to an event in a CBN, but it does correspond to an event in a (functional) causal model. The key point is that in a causal model, a context not only determines a state; it determines a state for every intervention. We can view a formula involving interventions as an event in a space whose elements are functions from interventions to worlds. Since a context can be viewed this way, we can view a formula involving interventions as an event in such a space. This makes conditioning on arbitrary formulas in \(^{+}\) (with positive probability) in causal models well defined. By way of contrast, in a CBN, we can view a context as a function from interventions to distributions over worlds. Finally, it is worth asking how reasonable is the assumption that cpts are independent, that is, considering i-compatible causal models rather than just compatible causal models, which is what seems to have been done elsewhere in the literature (see, e.g., (Balke and Pearl 1994; Tian and Pearl 2000)).

As we said, Richardson, Peters, and Halpern (2024) show that the assumption that cpts involving different variables are independent is equivalent to the (conditional) independence assumptions made in Bayesian networks. More precisely, given a CBN \(M\), let \(M^{}\) be the non-probabilistic causal model constructed above. They show that if the probability \(^{}\) makes interventions on different variables independent (i.e., if \(^{}(,f_{1},,f_{m})=()_{Y_{1}}(f_{1}) _{Y_{m}}(f_{m})\), as in our construction), then all the conditional independencies implied by d-separation hold in \((M,^{})\) (see (Pearl 1988) for the formal definition of d-separation and further discussion). Conversely, if all the dependencies implied by d-separation hold in \((M,^{})\), then \(^{}\) must make interventions on different variables independent.

This result says nothing about making interventions for different settings of the parents of a single variable independent. This is relevant only if we are interested in computing the probability of formulas such as \(X=0 Y=0[X 1](Y=1)\), for which we need to consider (simultaneously) the cpt for \(Y\) when \(X=0\) and when \(X=1\). As discussed earlier, independence is reasonable in this case if we can observe all the parents of a variable \(Y\), and thus screen off \(Y\) from the effects of all other variables (and other settings of the parents). We cannot always assume this, but in many realistic circumstances, we can. We give two general classes of examples where we can:

1. When debugging systems (including ml pipelines, database engines, or any general software) and network failures, users have access to all parameters related to the code and the execution environment (Fariha, Nath, and Meliou 2020; Kobayashi, Otomo, and Fukuda 2019; Galhotra, Fariha, Lourenco, Freire, Meliou, and Srivastava 2022). With this information, a causal graph over different blocks of code, its parameters, and other environment variables like information about background processes can be constructed. This means we can address queries like "Given that component A is faulty, with what probability would repairing component B solve the problem" using our techniques.
2. Manufacturing pipelines across various industries, such as semiconductor fabrication, pharmaceuticals, automobile assembly, and battery production, typically consist of a series of interconnected stages. Each of these stages is equipped with several sensors designed to monitor and measure critical environmental conditions that directly impact the production process. These sensors collect data about variables such as temperature, humidity, pressure, and other factors that can influence the quality, efficiency, and consistency of the final product. For instance, in semiconductor fabrication, precise control of environmental conditions like temperature and humidity is crucial to ensuring the integrity of the microchips produced (Wu 2008). Similarly, in pharmaceutical manufacturing, sensors monitor parameters like pH levels and chemical concentrations to maintain the efficacy of the drugs being produced. Thus, we can answer queries like "what is the probability that a temperature increase of 3 degrees Celsius would result in a poor quality product, given that the humidity is high?".

Other potential applications of our framework include (a) modeling player performance in sports by considering factors like injury, skill, and sports facilities, (b) urban planning scenarios to analyze the impact of zoning laws, interest rates, and other factors on house prices, and (c) modeling agriculture yield by considering variables like soil quality and weather conditions.

## 4 Converting a CBN to a (Probabilistic) Causal Model

Our semantics for formulas in CBNs reduced to considering their semantics in i-compatible causal models. It would be useful to show explicitly that such i-compatible causal models exist and how to construct them. That is the goal of this section. Balke and Pearl (1994) sketched how this could be done. We largely follow and formalize their construction.

Starting with a CBN \(M\), we want to construct an i-compatible probabilistic causal model \((M^{},^{})\), where \(M^{}\) has the same type as \(M\). To do this, for each endogenous variable \(Y\) in \(M\) with parents \(X_{1},,X_{n}\), we add a new exogenous variable \(U_{Y}\); \((U_{Y})\) consists of all functions from \((X_{1})(X_{n})\) to \((Y)\). Balke and Pearl (1994) call such an exogenous variable a _response function_. (Response functions, in turn, are closely related to the _potential response variables_ introduced by Rubin (1974).) We take \(U_{Y}\) to be a parent of \(Y\) (in addition to \(X_{1},,X_{n}\)). We replace the cpt for \(Y\) be the following equation for \(Y\); \(F_{Y}(x_{1},,x_{n},f)=f(x_{1},,x_{n})\), where \(f\) is the value of \(U_{Y}\). Since \(f\) is a function from \((X_{1})(X_{n})\) to \((Y)\), this indeed gives a value of \(Y\), as desired. Let \(Y_{1},,Y_{m}\) be the endogenous variables in \(M\). We define the probability \(^{}\) on \((U)(U_{Y_{1}})(U_{Y_ {m}})\) by taking \(^{}(,f_{1}, f_{m})=()_{i=1,,m} _{Y_{i}}(f_{i})\), where \(_{Y_{i}}\) reproduces the probability of the cpt for \(Y_{i}\). Specifically, for an endogenous variable \(Y\) with parents \(X_{1},,X_{n}\), \(_{Y}(f)=_{(X_{1})(X_{ n})}(Y=f(x_{1},,x_{n}) X_{1}=x_{1},,X_{n}=x_{n})\). This makes interventions for different settings of \(X_{1},,X_{n}\) independent, which is essentially what we assumed in the previous section when defining the probability of formulas in \(\) in \(M_{0}\), in addition to making interventions on different variables independent and independent of the context in \(M\). In any case, it is easy to see that this gives a well-defined probability on \((U)(U_{Y_{1}})(U_{Y_{m}})\), the contexts in \(M^{}\). Moreover, \(M^{}\) is clearly a causal model with the same type as \(M\) that is i-compatible with \(M\).

We can easily modify this construction to get a family of causal models compatible with \(M\), by loosening the requirements on \(^{}\). While we do want the marginal of \(^{}\) on \(\) to agree with the marginal of \(\) on \(\), and we want it to reproduce the probability of the cpt for each variable \(Y_{i}\) (as defined above), there are no further independence requirements. If we do that, we get the bounds computed by Balke and Pearl (1994). The following example illustrates the impact of dropping the independence assumptions.

**Example 4.1**.: _Consider the CBN \(M^{*}\) from Example 3.1 again. Using the notation from that example, suppose that \(a=1\) and \(b=d=1/2\). Independence guarantees that the set of ccces that includes \(U=0\), \(X=0 U=0\), and \(Y=0 X=0\) has probability \(abd=1/4\). But now consider a causal model \((M^{**},^{**})\) compatible with \(M^{*}\) where the contexts are the same as in our construction, but the probability \(^{**}\) does not build in the independence assumptions of our construction. Recall that contexts in \(M^{**}\) have the form \((u,f_{X},f_{Y})\). Since we want \((M^{**},^{**})\) to be compatible with \(M^{*}\), we must have \(^{**}(\{(u,f_{X},f_{U},f_{Y}):u=0\})=1\), \(^{**}(\{(u,f_{X},f_{Y}):f_{X}(0)=0\})=1/2\), and \(^{**}(\{(u,f_{X},f_{Y}):f_{Y}(0)=0\})=1/2\), so that \(^{**}\) agrees with the three cpts. But this still leaves a lot of flexibility. For example, we might have \(^{**}(\{(u,f_{X},f_{Y}):f_{X}(0)=f_{Y}(0)=0\}=^{**}(\{(u,f_{X},f_{Y}):f_{X} (1)=f_{Y}(1)=1\}=1/2\) (so that \(^{**}(\{(u,f_{X},f_{Y}):f_{X}(0)=0,f_{Y}(1)=1\})=^{**}(\{(u,f_{X},f_{Y}):f _{X}(0)=1,f_{Y}(1)=0\})=0\)). As shown in Example 3.2, \(_{M^{*}}(X=0 Y=0[X 1](Y=1))=1/4\). However, it is easy to check that \(_{M^{**}}(X=0 Y=0[X 1](Y=1))=1/2\). (Tian and Pearl (2000) give bounds on the range of probabilities for this formula, which is called the probability of necessity; see also Section B and (Pearl 2000, Section 9.2).)_

## 5 Computing counterfactual probabilities

In this section, we analyze _counterfactual probabilities_, introduced by Balke and Pearl (1994). Counterfactual probabilities have been widely used in several domains, including psychology (Hoerl, McCormack, and Beck 2011), epidemiology (Greenland and Robins 1999), and political science (Grynaviski 2013), to explain the effects on the outcome. More recently, they have proved useful in machine learning to explain the output of ML models (Beckers 2022).

Two types of counterfactual formulas that have proved particularly useful are the _probability of necessity_ and the _probability of sufficiency_; we focus on them in this section. As discussed by Pearl (2000), counterfactual analysis is particularly useful when it comes to understanding the impact of a decision on the outcome. For example, we might be interested in the probability that an outcome \(O\) would not have been favorable if \(A\) were not true. This captures the extent to which \(A\) is a _necessary_ cause of \(O\). Similarly, we might be interested in whether \(A\) is _sufficient_ for \(O\): that is if \(A\) were true, would \(O\) necessarily be true? We now review the formal definitions of these notions; see (Pearl 2000) for more discussion.

**Definition 5.1**.: _Let \(X\) and \(Y\) be binary variables in a causal model or CBN \(M\)._

1. Probability of necessity of \(X\) for \(Y\)_:_ \(_{M}^{X,Y}=_{M}([X 0](Y=0)|X=1 Y=1)\)_._
2. Probability of sufficiency of \(X\) for \(Y\)_:_ \(_{M}^{X,Y}=_{M}([X 1](Y=1) X=0 Y=0)\)_._
3. Probability of necessity and sufficiency of \(X\) for \(Y\)_:_ \(_{M}^{X,Y}=_{M}([X 1](Y=1)[X 0](Y=0))\)_._

Pearl (2000) gives examples showing that neither the probability of necessity nor the probability of sufficiency in a CBN can be identified; we can just determine a range for these probabilities. But with our independence assumptions, they can be identified, justifying our notation. Moreover, these probabilities can be computed using only conditional probabilities of (singly) exponentially many simple formulas (not involving interventions). Since these formulas do not involve interventions, they can be estimated from observational data, without requiring involving controlled experiments. Thus, our results and assumptions have significant practical implications.

Let \(Pa^{X}(Y)\) consist of all the parents of \(Y\) other than \(X\). For a set \(\) of variables, let \(_{}\) consist of all possible settings of the variables in \(\).

**Theorem 5.2**.: _If \(M\) is a CBN where \(Y\) is a child of \(X\), then_

1. \(_{M}^{X,Y}=_{c^{j}_{Pa^{X}(Y)}_{Pa^{X}(Y)}}\ \ \ \ \ _{M}(Pa^{X}(Y)=c^{j}_{Pa^{X}(Y)} Y=1 X=1)\)__ \[_{M}(Y=0 X=0 Pa^{X}(Y)=c^{j}_{Pa^{X}(Y)});\]
2. \(_{M}^{X,Y}=_{c^{j}_{Pa^{X}(Y)}_{Pa^{X}(Y)}}\ \ \ \ \ _{M}(Pa^{X}(Y)=c^{j}_{Pa^{X}(Y)} Y=0 X=0)\)__ \[_{M}(Y=1 X=1 Pa^{X}(Y)=c^{j}_{Pa^{X}(Y)});\]
3. \(_{M}^{X,Y}=_{M}^{X,Y}_{M}(X=0 Y=0)+ _{M}^{X,Y}_{M}(X=1 Y=1)\)_._

We defer the proof of the theorem to Section B.1 in the appendix, where further extensions are also provided.

Acknowledgments:Halpern's work was supported in part by AFOSR grant FA23862114029, MURI grant W911NF-19-1-0217, ARO grant W911NF-22-1-0061, and NSF grant FMitF-2319186. Galhotra's work is supported by a grant from Infosys.