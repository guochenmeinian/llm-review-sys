# Differentially Private Approximate Near Neighbor Counting in High Dimensions

Alexandr Andoni

Columbia University

andoni@cs.columbia.edu

&Piotr Indyk

MIT

indyk@mit.edu

&Sepideh Mahabadi

Microsoft Research

smahabadi@microsoft.com

&Shyam Narayanan

MIT

shyamsn@mit.edu

###### Abstract

Range counting (e.g., counting the number of data points falling into a given query ball) under differential privacy has been studied extensively. However, the current algorithms for this problem are subject to the following dichotomy. One class of algorithms suffers from an additive error that is a fixed polynomial in the number of points. Another class of algorithms allows for polylogarithmic additive error, but the error grows exponentially in the dimension. To achieve the latter, the problem is relaxed to allow a "fuzzy" definition of the range boundary, e.g., a count of the points in a ball of radius \(r\) might also include points in a ball of radius \(cr\) for some \(c>1\). In this paper we present an efficient algorithm that offers a sweet spot between these two classes. The algorithm has an additive error that is an arbitrary small power of the data set size, depending on how fuzzy the range boundary is, as well as a small (\(1+o(1)\)) multiplicative error. Crucially, the amount of noise added has no dependence on the dimension. Our algorithm introduces a variant of Locality-Sensitive Hashing, utilizing it in a novel manner.

## 1 Introduction

Differential Privacy (DP)  is a widely used tool for preserving the privacy of sensitive personal information. It allows a data structure to provide approximate answers to queries about the data it holds, while ensuring that the removal or addition of a single database entry does not significantly affect the outcome of any analysis. The latter guarantee is accomplished by adding some amount of noise to the answers, so that the data cannot be "reverse-engineered" from the answers to the queries. See Definition 2.1 for the formal setup. The notion has been deployed in many important scenarios in industry  as well as the U.S. census .

One of the key data analysis problems studied under differential privacy is _range counting_. Here, the goal is to construct a data structure that answers queries about the total number of data items in a database satisfying a given property. Formally, given a multiset \(X\) of \(n\) elements from a universe \(\), the goal of _range counting_ queries is to report the number of points within a given range \(Q\) from a prespecified class of query ranges \(\). For example, this could correspond to retrieving the number of users in a particular geographic location, or a number of patients with given symptoms.

Differentially private range counting has been studied extensively, for many classes of queries. For example, the case of \(=\), i.e., when the queries ask for the number of occurrences of a given element in the database, this correspond to the well-known _histogram problem_.

space \(^{d}\) (or its discretization), and \(\) consists of natural geometric objects, like rectangles, half-planes or balls. The seminal work of  demonstrated a strong relationship between the amount of additive noise necessary to guarantee the privacy, and the _discrepancy_ of the space of queries \(\). Unfortunately, except for a few classes of queries such as axis-parallel rectangles, most natural geometric objects have discrepancy that is polynomial in the number of data points \(n\). This means that, in order to guarantee privacy, it is necessary to distort the counts by an additive term \((n^{})\), for a _fixed_ constant \(>0\) that depends on \(\).

To alleviate this issue,  introduced differentially private data structures for _approximate_ range queries. For simplicity, we recap their notion for the special case of ball queries that we consider in the rest of the paper. We assume that \(\) is a metric space equipped with a distance function \(\). For a radius parameter \(r\), we define the query family \(=\{B(q,r) q\}\), where \(B(q,r)=\{p(p,q) r\}\) is the ball of radius \(r\) around \(q\). In other words, in the _exact_ problem, the goal is to count the number of data points that are "near" the given query point. In turn, the _approximate_ problem is parameterized by an approximation factor \(c 1\), and the algorithm is allowed to report any number between \(|X B(q,r)|\) and \(|X B(q,cr)|\). Figure 1 illustrates this notion. We will refer to the approximate version as the \((c,r)\)-near neighbor counting problem.

The main result of  shows that, if one allows approximate queries as defined above, then one can construct a data structure that guarantees privacy while adding much less noise. Specifically, if \(=[u]^{d}\) (i.e., the universe is the discrete \(d\)-dimensional space) for some fixed \(d=O(1)\) and \((p,q)=\|p-q\|_{s}\) for some \(_{s}\) norm, then their algorithm achieves privacy while adding noise of magnitude roughly \((O(1/))^{d}(u+n)\), where \(0<<1\) is a constant that depends on \(c>1\). This guarantees that the noise amount is only polylogarithmic, as long as the dimension \(d\) is constant. However, the exponential dependence of the noise bound on the dimension precludes the applicability of this algorithm to the common case of high-dimensional data sets.

Our contributionOur main result is an efficient data structure that can approximate the number of near neighbors to any query point over Euclidean (\(_{2}\)) space, with differential privacy (see Preliminaries for the formal of definition of privacy). Specifically, for any fixed radius \(r\), and any series of query points \(q\), the data structure can privately answer \((c,r)\)-near neighbor counting queries, where we are allowed a small additive error (\(n^{}\) for a small constant \(\)) and multiplicative error \(1 o(1)\)). Let \(B_{p}(q,r)\) denote the \(_{p}\) ball of radius \(r\) around \(q\).

**Theorem 1.1**.: _Fix a radius \(r>0\) and constant \(c 1\), and privacy parameters \(1>,>0\). For any integer \(n\) and dimension \(d\), there exists a differentially private algorithm that, on a dataset \(X^{d}\) of at most \(n\) points, creates a data structure that can answer \(m\) (non-adaptive) queries and that satisfies the following properties._

1. _(Privacy) The full output of the algorithm is_ \((,)\)_-differentially private._
2. _(Accuracy) With high probability, for every non-adaptive query_ \(q\)_, the output_ \(ans(q)\) _is between_ \[(1-o(1))|X B_{2}(q,r)|-n^{+o(1)}\] _and_ \[(1+o(1))|X B_{2}(q,cr)|+n^{+o(1)},\]

Figure 1: An illustration of approximate near neighbor query, with parameters \(c\) and \(r\).

_where_ \(=}{(c^{2}+1)^{2}}=O(})\)_._
3. _(Runtime) The preprocessing time is_ \(O(n^{1+o(1)} d m)\)_, and the expected runtime for answering each query is_ \(O(n^{+o(1)} d m)\)_._

The key two features of the above result are as follows. First, the amount of additive noise is controlled by the amount of allowed approximation: the exponent \(\) tends to \(0\) as \(c\) grows large. This makes it possible to trade the "proximity" error \(c\) and the "privacy" error \(n^{}\) to achieve the best results. Second, the magnitude of added noise does not depend on the dimension; in particular, it avoids the exponential dependence of .

Finally, we remark that our algorithm can be extended to alternative/more general settings:

1. \(_{1}\) **metric:** If we considered the balls \(B_{1}(q,r)\) rather than \(B_{2}(q,r)\), we can still obtain a similar result, at the cost of having \(}=O()\). This is based on the fact that the \(_{1}\) metric embeds near-isometrically into the squared \(_{2}\) metric.
2. **Pure differential privacy:** We can modify our algorithm to handle pure-DP queries (i.e., when \(=0\)), while still having good accuracy. In this case, however, the algorithm, while fast in expectation, has some small probability of being very slow.
3. **Large \(\):** Our results also hold even when \(1 n^{o(1)}\).
4. **Adaptive queries:** Our method also extends to adaptive queries, at the expense of the runtime and additive error multiplying by a factor of \((d R/r)\), where \(R\) is some promised bound on the radius of the dataset.
5. \(k\)**-NN queries:** Finally, we note that using similar approach to that of , one can get a data structure for finding an approximation to the distance of the query to its \(k\)-nearest neighbor. However, as opposed to , our data structure works only for a fixed value of \(r\), and thus one needs to build separate data structures for various choices of \(r\) leading to worse privacy guarantees which depend on the aspect ratio.

We discuss extensions 1-4 more formally in Appendix C.

A natural question is whether the tradeoff between approximation ratio \(c\) and additive error \(n^{}\) is necessary, or whether one can obtain \(c=O(1)\) and \(\) to be an arbitrarily small constant. While we are unable to prove such a lower bound in the Euclidean setting, we show that under the closely related \(_{}\) norm for approximation ratio \(c=3-o(1)\), one must have an additive error of \(n^{(1)}\).

**Theorem 1.2**.: _For sufficiently large \(n\), there exists \(d=C n\) for a large constant \(C\), fixed constants \(1>,,>0\), and \(n\) query points \(Q=\{q_{1},,q_{n}\}\), with the following property. For any (arbitrarily small) \(>0\) and any differentially private algorithm \(\) that acts on a dataset \(X^{d}\) of at most \(n\) points, and outputs \(\{ans(q)\}_{q Q}\), must have_

\[ q Q,|B_{}(q,0.5)|-n^{} ans(q) |B_{}(q,1.5-)|+n^{}<.\]

Theorem 1.2 provides some evidence that a tradeoff may be necessary even in the \(_{2}\) case: proving such a result is an interesting open problem. We defer the proof of Theorem 1.2 to Appendix B.

Overview of the techniques.Our high-level approach is similar to that of the low-dimensional approximate range query data structure of , and consists of the following steps. First, we design a partition of \(^{d}\), and count the number of points that fall into each cell in the partition. Then we add Laplace noise to each cell to ensure the privacy of the counts. Finally, for each query \(q\), we approximate the ball \(B(q,r)\) using the cells in the partition, and report the sum of the (noisy) counts of the approximating cells.

However, applying this approach as described requires using an exponential (in \(d\)) number of partition cells to approximate a query ball \(B(q,r)\), leading to the exponential error bound. To overcome this issue, we resort to _randomized_ space partitions obtained using Locality-Sensitive Hashing (LSH) techniques . These partitions have the property that, for any two points \(p\) and \(q\), the probability that both of them belong in the same partition cell depends on the distance \(\|p-q\|_{2}\). As shown in  in the context of (non-private) approximate near neighbor search, enumerating a bounded number (\(n^{}\)) of cells in such partition makes it possible to report each data point from the range \(B(q,r)\) with probability at least \(1/( n)\). To amplify the probability of correctness, the process is repeated \(O( n)\) times, using a different random partition in each repetition.

Unfortunately, the aforementioned approach does not appear to yield an efficient algorithm for _approximately counting_ the number of points in \(B(q,r)\). The main issue is that, as indicated above, when enumerating the \(n^{}\) cells corresponding to a query's ball, we are guaranteed to include only a \(1/( n)\) fraction of points, leading to a large approximation factor. The standard way to deal with this -- using multiple partitions -- yields inconsistent counts which are difficult to aggregate. A number of follow-up works showed improved bounds on exponent \(\), but all of them similarly have a guarantee of identifying only a \( 1/( n)\) fraction of points . To overcome these obstacles, we show how we can adapt the algorithm from , which yield an approximate near neighbor data structure with the best known tradeoff between space and time. However, their randomized algorithm does not yield partitions (technically, they construct so-called _locality-sensitive filters_, which are not partitions).

Our main technical contribution is two-fold. First, we show it is possible to "force" the algorithm of  to construct proper partitions. The quality of those partitions is comparable to the original structures, at the price of a more involved analysis. We then use those partitions to compute the counts and approximate the query ball as outlined at the beginning of this section. Second, we show that it is also possible to modify the algorithm so that the probability of including each qualifying point into the count is at least \(1-o(1)\). This is of separate interest, as this yields the most efficient algorithm for approximate nearest neighbor search with space \(O(nd)\), improving over .

### Related work

The problem of range counting when the query class \(\) is arbitrary, can be solved with an optimal error bound of \((1/,|||}{})\). In the setting of approximate near-neighbor queries, the size of \(\) and \(\) can be thought of as at most exponential in the dimension \(d\), by using a standard \(\)-net technique. Hence, their error is in fact smaller than ours when \(}{(^{2}+1)^{2}}\), or equivalently, when \(c 1+\). However, whenever an approximation of \(c>1+\) is acceptable, our error is smaller than . Moreover, our approach is computationally efficient, whereas their method has runtime linear in the size of \(\), which is very inefficient.

When the query class is restricted to points in \(\), i.e., histogram queries, then the best achievable error is \(O(\{||,(1/)\})\). When the points are in 1-dimensional Euclidean space taking values in \([u]\), and \(\) is a set of intervals,  got an \((,0)\)-DP algorithm with error \(O(^{1.5}u)\) and a lower bound of \(()\). When resorting to \((,)\)-DP,  showed an algorithm with error \(2^{(1+o(1))^{*}u}(1/)/\) and a lower bound of \((^{*}u(1/)/)\) for \(e^{- n/^{*}n} 1/n^{2}\). The problem for the axis-parallel rectangles have been further studied  where polylogarithmic error bounds were obtained.

More generally,  showed an equivalence between the error and the discrepancy of the range space \(\). Given that the discrepancy of many natural range spaces such as half-spaces, simplices, and spheres is \(n^{(d)}\) where \((d)[1/4,1/2]\) is a constant depending on \(d\), it rules out such a polylogarithmic error for these range spaces. For non-convex ranges, the discrepancy already becomes \(\) reaching the threshold for arbitrary query ranges.

There are a number of papers based on space decompositions that provide a DP data structure for range counting that perform relatively well in practice  but perform poorly on high-discrepancy point sets.

Finally, we note that several prior works used LSH for differentially private kernel estimation . However, those works do not seem to be applicable to ball range queries, which correspond to uniform kernels.

Preliminaries

Range Counting:In the range counting problems, given a point set \(X\), where \(\) is a universe, the goal is to construct a differentially private data structure that for any query \(Q\) from a certain query family \(\), one can compute \(|Q X|\).

Here we focus on Near Neighbor counting queries in high dimensions. In particular \(\) is a metric space equipped with a distance function \(\). Given a prespecified radius parameter \(r\), the query family \(=\{B(q,r) q\}\), where \(B(q,r)=\{p(p,q) r\}\) is the ball of radius \(r\) around \(q\). In other words, in this problem the goal is to count the number of _neighbors_ of a given query point.

Similar to the work of , we consider the approximate variant of the problem where the points within distance \(r\) and \(cr\) of the query can be either counted or discarded. In particular for an approximation factor \(c 1\), any number between \(|X B(q,r)|\) and \(|X B(q,cr)|\) is valid.

Differential Privacy (DP):For two datasets \(X\) and \(X^{}\), we use \(X X^{}\) to mean that they are neighboring datasets, i.e., one of them can be obtained from the other by an addition of a single element (in our problem a point).

**Definition 2.1**.: _(Differential Privacy ). For \(>0, 0\), a randomized algorithm \(\) is \((,)\)-differentially private (\((,)\)-DP) if for any two datasets \(X X^{}\), and any possible outcome of the algorithm \(S\)Range(\(\)), \([(X) S] e^{} [(X^{}) S]+\). When \(=0\), the algorithm is said to have pure differential privacy._

The sensitivity of a function \(f\) is defined to be \(_{f}=_{X X^{}}|f(X)-f(X^{})|\).

We use \(()\) to denote the _Laplace_ distribution with parameter \(\) with PDF \([Z=z]=e^{-|z|/}\), which has mean \(0\) and variance \(^{2}\). We also use \((,,)\) to denote the _Truncated Laplace_ distribution with PDF proportional to \(e^{-|z|/}\) on the region \([-B,B]\), where \(B=(1+-1}{2 })\).

**Lemma 2.2** ((Truncated) Laplace Mechanism ).: _Given a numeric function \(f\) that takes a dataset \(X\) as the input, and has sensitivity \(\), the mechanism that outputs \(f(X)+Z\) where \(Z(/)\) is \((,0)\)-DP. In addition, if \(,,\,f(X)+Z\), where \(Z(,,)\), is \((,)\)-DP. Moreover, the Truncated Laplace mechanism is always accurate up to error \(B\)._

## 3 Upper Bound

In this section, we describe the algorithm and perform the majority of the analysis for Theorem 1.1. We defer certain parts to Appendix A, and finish the proof there.

### Setup

The algorithm we develop is inspired by the data-independent approximate nearest neighbor algorithm in . We will use similar notation to their paper as well.

Define \(S^{d-1}\) to be the \((d-1)\)-dimensional unit sphere in \(^{d}\). For any parameter \(r(0,2),\) let \((r)=1-}{2}\) and \((r)=}\) be the cosine and the sine, respectively, of the angle between two points in \(S^{d-1}\) of distance \(r\). For any parameter \(>0\), define

\[F():=}_{g(0,I)}[ g,u],\]

where \(u S^{d-1}\) is an arbitrary point on the unit sphere. Note that \(F()\) is independent of \(u\) by the spherical symmetry of Gaussians, and in fact equals the probability that a univariate standard Normal exceeds \(\). Next, for any parameters \(r(0,2)\) and \(_{q},_{u}\), we define

\[G(r,_{q},_{u}):=}_{g(0,I)}[ g,q_{q} g,p_{u}],\]

where \(p,q S^{d-1}\) are points that have distance exactly \(r\) between them. As with \(F,\)\(G\) has no dependence on the specific points \(p,q\) but only on the distance \(r=\|p-q\|_{2}\) between them. Note also that \(G(r,_{q},_{u})\) is non-increasing in \(r\), even if \(_{q}\) or \(_{u}\) is negative.

Next, we note the following well-known bounds for \(F\).

**Proposition 3.1**.: _For \( 1\),_

\[() e^{-^{2}/2} F() e^{-^ {2}/2}.\]

_In addition, as \(\), \(F() o(1) e^{-^{2}/2}\)._

### Algorithm for data on the sphere

We now present an algorithm that obtains the desired privacy and accuracy guarantees, assuming both the data points and query points are on a unit sphere and the radius \(r\) is not too small. The initial algorithm description will not necessarily run in polynomial time, but we will later describe a simple modification to make this algorithm efficient. Finally, in Appendix A.2 we will modify the algorithm to work for general \(r\) and in Euclidean space, rather than just on the unit sphere. This follows from known embedding methods , which is why we defer it to the Appendix.

Data Structure:We describe the data structure in words here, provide pseudocode in Algorithm 1. Fix \(K\) and \(T\) to be positive integers, to be set later. We generate a data structure, which is a \(T\)-ary tree of depth \(K\). We index the levels as \(0,1,,K\), where level \(0\) is the root and level \(K\) consists of the \(T^{K}\) leaves. For each node \(v\) except the root, we generate an independent \(d\)-dimensional standard Gaussian \(g_{v}(0,I)\). In addition, we will define two parameters \(_{q}\) and \(_{u}\), to be set later.

```
1:Input: data \(X\) of size at most \(n\), privacy parameters \(,\), parameters \(T,K,_{u}\).
2:Output: data structure \(=(,\{g_{v}\}_{v},\{c_{v}\}_{v})\).
3:Create \(\): \(T\)-ary tree of depth \(K\), with root \(v_{}\).
4:for each node \(v\)do
5:\(g_{v}(0,I_{d})\). {Random \(d\)-dimensional Gaussian}
6:\(c_{v}\). {Counts number of points assigned to each node}
7:for point \(p X\)do
8:\(v v_{}\).
9:for\(=0\) to \(K-1\)do
10:if\((v)<\)then
11:BREAK {Break out of "for \(=0\) to \(K-1\)" loop (\(p\) failed to map to a child)}
12:for\(i=1\) to \(T\)do
13:\(v_{i} i\)th child of \(v\)
14:if\( g_{v_{i}},p_{u}\)then
15:\(c_{v_{i}} c_{v_{i}}+1\)
16:\(v v_{i}\)
17:BREAK {Break out of "for \(i=1\) to \(T\)" loop}
18:for each leaf node \(v\)do
19:\(c_{v} c_{v}+(1,,)\).
20:if\(c_{v}(1+-1}{})\)then
21:\(c_{v} 0\) ```

**Algorithm 1** Data Structure

The tree nodes at level \(\) will partition the unit sphere \(S^{d-1}\) into \(T^{}\) regions, plus an extra "remainder" region of points that are not mapped into any of the \(T^{}\) regions. At level \(0\), all points in \(S^{d-1}\) are sent to the single region indexed by the root. For any node \(v\) in the tree of depth \(0 K-1\) and with children \(v_{1},,v_{K}\), the region \(P_{v}\) is partitioned into \(P_{v_{1}},,P_{v_{T}}\)1 as follows. A point \(p P_{v}\) is sent to \(P_{v_{i}}\) if \(i T\) is the smallest index such that \( g_{v_{i}},p_{u}\). Note that some points in \(P_{v}\) may not be sent to any of \(P_{v_{1}},,P_{v_{T}}\). For each leaf node \(v\), we store an approximate count \(c_{v}=|X P_{v}|+(1,,)\), where we added Truncated Laplace noise. If the noised count \(c_{v}\) is too small (at most \((1+-1}{})\)), we replace the count \(c_{v}\) with \(0\).

Answering a query:We describe the algorithm in words here, provide pseudocode in Algorithm 2. Given a query \(q\), we "send" \(q\) to every leaf node \(v=v_{K}\) such that the path \(v_{1},v_{2},,v_{K}\) from the root \(v_{}\) satisfies \( g_{v_{i}},q_{q}\) for all \(1 i K\). Hence, each query \(q\) will correspond to a subset \(V_{q}\) of leaf nodes. The response to the query \(q\) will be

\[_{v V_{q}}c_{v}.\]

To improve the accuracy of the algorithm, we repeat this procedure \(O( m)\) times if there are \(m\) queries, each initialized to \((^{},^{})\)-DP for \(^{}=/O( m)\) and \(^{}=/O( m)\). The overall algorithm will still be \((,)\)-DP, by the well-known Basic Composition theorem [14, Corollary 3.15]. For each query, we output the median response of the individual data structure responses. In the analysis, we will ignore this amplification procedure, as by a Chernoff bound it suffices to show that each individual query is answered accurately with at least \(2/3\) probability.

```
1:Input: data structure \(=(,\{g_{v}\}_{v},\{c_{v}\}_{v})\), parameter \(_{q}\), query point \(q\), node \(v\).
2:if\( g_{v},q_{u}\)then
3:if\(v\) is a leaf node then
4:Return\(c_{v}\)
5:else
6:\(ans=0\)
7:for\(i=1\) to \(T\)do
8:\(v_{i} i\)th child of \(v\)
9:\(ans ans+(,_{q},q,v_{i})\)
10:Return\(ans\) ```

**Algorithm 2**Answer\((,_{q},q,v)\): Answering a query

### Analysis

Privacy:The analysis of privacy is quite simple. First, note that the responses to all of the queries do not depend on the data directly, but only depend on the counts \(\{c_{v}\}\). If we let \(_{v}(X):=|X P_{v}|\) and \(_{v}(X):=|X P_{v}|+(1,,)\), then note that \(c_{v}(X)\) is entirely dependent on \(_{v}(X)\). Hence, it suffices to show the following.

**Lemma 3.2** (Privacy).: _The set \(\{_{v}(X)\}_{v}\) is \((,)\)-DP with respect to the data \(X\). Hence, the entire algorithm is also \((,)\)-DP, even if we have an arbitrary number of queries._

Proof.: Consider two adjacent datasets \(X,X^{}\), i.e., where we either removed or added a single data point from \(X\) to obtain \(X^{}\). Note that the construction partitioning of the dataset into leaves is not dependent on the data, but merely on random vectors \(g_{v}\) over all nodes \(v\). Therefore, if we condition on the partitioning of the entire space, the values \(_{v}(X)=|X P_{v}|\) and \(_{v}(X^{})=|X^{} P_{v}|\) are the same for all but at most one leaf \(v\), which changes by at most \(1\). The sensitivity of each such \(_{v}(X)\) is at most \(1\), which means that we have \((,)\)-privacy loss from \(_{v}(X)\). Because this happens for a single choice of \(v\), we have \((,)\)-privacy loss in total. 

Accuracy:We will only consider the accuracy with respect to a fixed query \(q\) and a single copy of the data structure, and show accuracy holds in expectation. For a set of \(m\) queries, since we repeat this data structure \(O( m)\) times and use the median estimate, a Chernoff bound implies all queries will be answered accurately with at least \(99\%\) probability.

We prove the following lemma to bound accuracy.

**Lemma 3.3** (Accuracy, assuming parameters are set properly).: _Suppose the following hold, for some fixed choice of parameters \(c,,K,T,_{q},_{u}\):_

1. \((T F(_{q}))^{K} n^{+o(1)}\)_. (Additive error due to Laplace noise)._
2. \(e^{-TF(_{u})}=o(1/K)\)_, and_ \(,_{u})}{F(_{u})}=1-o(1/K)\)_. (Multiplicative error due to not including points within_ \(r\) _of_ \(q\)_)._3. \((,_{u})}{F(_{u})})^{K} n^{-1++o(1)}\)_. (Additive error due to including points not within_ \(cr\) _of_ \(q\)_)._

_Then, if \(ans\) is the response when querying a fixed \(q\) on a dataset \(X\), with probability at least \(2/3\), \(ans\) is between_

\[(1-o(1))|X B_{2}(q,r)|-O( n^{+o(1)})\]

_and_

\[(1+o(1))|X B_{2}(q,cr)|+O( n^{+o(1)}).\]

_In other words, we solve the \((c,r)\)-near neighbor counting problem with probability at least \(2/3\), up to a multiplicative \(1 o(1)\) factor and an additive \(O( n^{+o(1)})\) factor._

In Subsection 3.4, we will show how to set \(T,K,_{q},_{u}\) so that the three conditions above hold.

Proof.: Our error comes from two sources. The first is the Truncated Laplace noise that we add, which we add in \(|V_{q}|\) locations to compute the answer to query \(q\). This adds an additive noise of at most \(O(|V_{q}|)\) in expectation, as \(|c_{v}-|X P_{v}|| O()\) with probability \(1\).

To bound \(|V_{q}|\), note that it simply equals the sum of the probabilities that \(q P_{v}\) for each leaf \(v\), which is \(T^{K}\) times the probability that \(q P_{v}\) for a fixed \(v\). This probability is just \(F(_{q})^{K}\) since each Gaussian along the path to \(v\) is independent. Hence, the first source error has magnitude at most \(O((T F(_{q}))^ {K}) O( n^{ +o(1)})\) in expectation.

Hence, if we let \(\) be the response we would have received if we did not add Laplace noise (i.e., if we used \(_{v}\) instead of \(c_{v}\)), then \([|ans-|] O( n^{+o(1)})\), so \(|ans-| O(  n^{+o(1)})\) with probability at least \(0.9\) by Markov's inequality.

The second source of error is that \(\) counts the number of points in \(X\) that are in \(P_{v}\) for some \(v V_{q},\) whereas we actually want to count the number of points in \(X\) that are within distance \(r\) of \(q\) (tolerating the inclusion of points up to distance \(cr\) of \(q\)). Hence, we need to make sure of two things, corresponding to not underestimating or overestimating, respectively:

1. Most points in \(X\) within distance \(r\) of \(q\) are mapped to some \(P_{v}\) where \(v V_{q}\).
2. Few points in \(X\) that are not within distance \(cr\) of \(q\) are mapped to some \(P_{v}\) where \(v V_{q}\).

This way, we will show that with high probability,

\[(1-o(1))|X B_{2}(q,r)||X B_{2}(q,cr)|+O( n^{+o(1)}),\]

which is sufficient.

For any point \(p\) of distance \(s\) from \(q\), we compute the probability that \(p P_{v}\) for some \(v V_{q}\). Let's start by assuming \(K=1\), i.e., we have a depth-1 tree with leaves labeled \(1,2,,T\). In this case there are two possibilities for failure: either \(p\) is not mapped to any \(P_{i}\) for \(1 i T\), or \(p\) is mapped to some \(P_{i}\) but \(q_{i}\) is not sent there. Suppose \(p\) is mapped to \(P_{1}\): this means \( g_{1},p_{u}\). Conditioned on this event, the probability that \( g_{1},q_{q}\) is precisely \([ g_{1},q_{q}| g_{1},p_ {u}]=[ g_{1},g_{1}_{q}, g_{1},p _{u}]}{[ g_{1},p_{u}]}=,_{u})}{F(_{u})}\). Suppose \(p\) is mapped to \(P_{i}\) for \(i 2\). Then, the probability that \( g_{i},q_{q}\) is \([ g_{i},q_{q}| g_{i},p_ {u}, g_{j},p<_{u} j<i]\). But even if \(p,q\) are fixed, then the values of \( g_{j},p\) for \(j<i\) are independent of \((g_{i},p, g_{i},q)\). Hence, we may remove the conditioning on \( g_{j},p<_{u} j<i\), to again say that the conditional probability is \(,_{u})}{F(_{u})}\). Hence, the probability that \(p P_{v}\) for \(v V_{q}\), conditioned on \(p\) being in some \(P_{v}\), is \(,_{u})}{F(_{u})}\). The probability that \(s P_{v}\) for any \(v\) is \((1-F(_{u}))^{T}\), so the overall probability that \(p P_{v}\) for \(v V_{q}\), for \(K=1\), is

\[(1-(1-F(_{u}))^{T}),_{u})}{F(_{u})}.\]

Note that \(1-e^{-TF(_{u})} 1-(1-F(_{u}))^{T} 1\).

For general depths, note that the Gaussians selected at each level are independent of the previous levels. Hence, the success probability simply raises to the power of \(K\), or equals

\[(1-(1-F(_{u}))^{T})^{K}(, _{u})}{F(_{u})})^{K}.\]

Note that \(1(1-e^{-TF(_{u})})^{K} 1-K e^{-TF(_{u})}\). Hence, the probability that a point \(p\) of distance \(s\) from \(q\) is mapped to some \(P_{v}\) where \(v V_{q}\) is between \((1-K e^{-TF(_{u})})(,_{u}) }{F(_{u})})^{K}\) and \((,_{u})}{F(_{u})})^{K}\).

Because \(G(s,_{q},_{u})\) is an increasing function in \(s\), the probability at any point of distance \(s r\) from \(q\) is not included in the count \(\) is at least \((1-K e^{-TF(_{u})})(,_{u} )}{F(_{u})})^{K}=1-o(1)\), as we are assuming that \(e^{-T F(_{u})}=o(1/K)\) and \(,_{u})}{F(_{u})}=1-o(1/K)\). Thus, by Markov's inequality, with probability at least \(0.9\), \((1-o(1))|X B_{2}(q,r)|\). Next, the probability that any point of distance \(s cr\) from \(q\) is included in the count \(\) is at most \((,_{u})}{F(_{u})})^{K},\) which we assume is at most \(n^{-1++o(1)}\). Because there are at most \(n\) points, by Markov's inequality, \(|X B_{2}(q,cr)|+n^{+o(1)}\) with probability at least \(0.9\). 

Runtime:While we have written the algorithm to create a \(T^{K}\)-sized tree, we can speed up the implementation by only including the necessary parts of the data structure.

We can bound the runtime as follows. Naively, for preprocessing, it takes space and time \(O(T^{K} d)\) to construct the tree and generate and store the Gaussians. Next, for each point \(p\), if \(p P_{v}\), it takes up to \(O(T d)\) time to determine which child of \(v\) is that \(p\) will be sent to. Since \(p\) is sent to only one node in each partition, this takes total time \(O(K T d)\) per point, which means \(O(n K T d)\) total time to partition the points in \(X\). To improve the preprocessing time, we do not generate the full tree: rather, we only generate a node of the tree (with a corresponding Gaussian \(g_{v}\)) if we need to check whether some data point \(p X\) is sent to the node, which means we can improve the preprocessing time to \(O(n K T d)\), without the extra term of \(O(T^{K} d)\). Note that if some leaf node \(v\) was not created in our modified implementation, then no point would have been sent to some partition piece \(P_{v}\), which means \(|X P_{v}|+(1,,)(1+-1}{})\). So, we would have set \(c_{v}\) to be 0 anyway, which means that this modification does not affect any of the responses to queries.

To answer a query \(q\), if \(q P_{v}\) (where \(v\) is possibly not a leaf node) it takes \(O(T d)\) time to determine all of the children of \(v\) that \(q\) will be sent to. In expectation, \(q\) is sent to \((TF(_{q}))^{}\) nodes at level \(\), which means at level \(+1\) we need to check up to \((TF(_{q}))^{} T\) nodes in expectation. (If a node \(w\) is not created in our implementation, we do not have to check it, since we know \(c_{v}=0\) for any leaf \(v\) that is a descendant of \(w\).) Therefore, the total time it takes to determine all leaf nodes \(q\) is sent to, in expectation, is \(O(d_{=0}^{K-1}(TF(_{q}))^{} T)=O((TF(_ {q}))^{K} K T d)\). Finally, we can add \(c_{v}\) over all leaf nodes \(v\) that \(q\) is sent to, which takes an additional \(O((TF(_{q}))^{K})\) time in expectation.

Hence, we have the following lemma.

**Lemma 3.4** (Runtime).: _The total preprocessing time of the data structure is \(O(n K T d)\), and the expected time needed to answer each query is \(O((TF(_{q}))^{K} K T d)\)._

### Parameter Settings and Finishing the Accuracy/Runtime Analysis

In this section, we set parameters properly so that Lemmas 3.3 and 3.4 match the goals of Theorem 1.1. We recall that \(c>1\) is a fixed constant representing the approximation ratio. We assume that \(d=( n)^{O(1)}\) and \(r=(})\). For fixed \(c\) and \(r\) as set above, we set the parameters as follows:

* \(K:=\).
* \(_{u}:=},\) for some constant \(=(c)\), that we will set later.

* \(_{q}:=(r)_{u}-2(r)\). Recall that \((r)=1-}{2}\) and \((r)=}\).
* \(T:=10 K/F(_{u})\).

First, we must show that our parameter choices imply that the algorithm is accurate. We prove the following lemma, for which the proof is deferred to Appendix A.

**Lemma 3.5** (Accuracy, completed).: _For an appropriate choice of \((c)=c}{c^{2}+1}\), all three conditions in Lemma 3.3 hold, with \(=}{(c^{2}+1)^{2}}=O(})\). Hence, Lemma 3.3 holds without the conditions, for \(=}{(c^{2}+1)^{2}}\)._

Finally, we show that the runtime is good, under our parameter choices.

**Lemma 3.6** (Runtime, completed).: _For the parameters we have defined, assuming \(r=(})\), the total preprocessing time is \(n^{1+o(1)} d\) and the expected time to answer each query is \(n^{+o(1)} d\), for \(=}{(c^{2}+1)^{2}}\)._

Proof.: The preprocessing time is \(O(n K T d)\). We know that \(T=)},\) and that \()}(_{u}) e^{_{u}^{2}/2}( n)^ {O(1)} e^{ n^{2}/(r^{2} K)}\). By our settings of \(r=(( n)^{-1/8})\) and \(K=\), and since \(\) is a constant, this equals \(n^{(1/( n)^{1/4})}=n^{o(1)}\). Hence, \(1/F(_{u})=n^{o(1)}\), and since \(K=O()\), the total preprocessing time is \(n^{1+o(1)} d\).

The time to answer each query is \(O((TF(_{q}))^{K} K T d)\). By Lemma 3.5, we know that \((TF(_{q}))^{K}=n^{+o(1)}\). Moreover, we know that \(K=( n)^{O(1)}\), and we already saw that \(T=10 K/F(_{u})=n^{o(1)}\). Hence, the time to answer each query is \(n^{+o(1)} d\). 

Lemmas 3.2, 3.5, and 3.6 will end up being sufficient to prove Theorem 1.1 if we assume the data lies on a \(d=( n)^{O(1)}\)-dimensional unit sphere, and if \(r=(( n)^{1/8})\). To finish the analysis, we must show that this assumption can be made without loss of generality, which will follow from an embedding argument of . We defer this argument, and the proof of the remainder of Theorem 1.1, to Appendix A.2.

_Remark_.: We note that the data structure can be implemented so that extra space is only \(O(nd)\). Instead of generating and storing the Gaussians \(g_{v}\) independently, we note that in each level \(\), each group of \(T\) siblings can reuse exactly the same random set of \(T\) Gaussians. The analysis goes through mutatis mutandis as independence between different branches is not needed.

Acknowledgements:PI was supported by the NSF TRIPODS program (award DMS-2022448), Simons Investigator Award, and GIST-MIT Research Collaboration grant.

SN is supported by a NSF Graduate Fellowship (Grant No. 1745302) and a Google PhD Fellowship. AA supported in part by NSF (CCF2008733) and ONR (N00014-22-1-2713).