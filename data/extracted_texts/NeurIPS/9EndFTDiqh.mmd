# Should We Learn

Most Likely Functions or Parameters?

Shikai Qiu

Equal contribution.

Tim G. J. Rudner

Equal contribution.

Sanyam Kapoor1

Andrew Gordon Wilson

New York University

###### Abstract

Standard regularized training procedures correspond to maximizing a posterior distribution over parameters, known as maximum a posteriori (MAP) estimation. However, model parameters are of interest only insomuch as they combine with the functional form of a model to provide a function that can make good predictions. Moreover, the most likely parameters under the parameter posterior do not generally correspond to the most likely function induced by the parameter posterior. In fact, we can re-parametrize a model such that any setting of parameters can maximize the parameter posterior. As an alternative, we investigate the benefits and drawbacks of directly estimating the most likely function implied by the model and the data. We show that this procedure leads to pathological solutions when using neural networks and prove conditions under which the procedure is well-behaved, as well as a scalable approximation. Under these conditions, we find that function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting.

## 1 Introduction

Machine learning has matured to the point where we often take key design decisions for granted. One of the most fundamental such decisions is the loss function we use to train our models. Minimizing standard regularized loss functions, including cross-entropy for classification and mean-squared or mean-absolute error for regression, with \(_{1}\) or \(_{2}\) regularization, exactly corresponds to maximizing a posterior distribution over model parameters . This standard procedure is known in probabilistic modeling as _maximum a posteriori_ (map) parameter estimation. However, parameters have no meaning independent of the functional form of the models they parameterize. In particular, our models \(f_{}(x)\) are functions given parameters \(\), which map inputs \(x\) (e.g., images, spatial locations, etc.) to targets (e.g., softmax probabilities, regression outputs, etc.). We are typically only directly interested in the function and its properties, such as smoothness, which we use to make predictions.

Alarmingly, the function corresponding to the most likely parameters under the parameter posterior does not generally correspond to the most likely function under the function posterior. For example, in Figure 0(a), we visualize the posterior over a mixture coefficient \(_{R}\) in a Gaussian mixture regression model in both parameter and function space, using the same Gaussian prior for the mixture coefficients (corresponding to \(_{2}\) regularization in parameter space). We see that each distribution is maximized by a different parameter \(_{R}\), leading to very different learned functions in Figure 0(b). Moreover, we can re-parametrize the functional form of any model such that any arbitrary setting of parameters maximizes the parameter posterior (we provide further discussion and an example in Appendix A).

_Should we then be learning the most likely functions or parameters?_ As we will see, the nuanced pros and cons of each approach are fascinating and often unexpected.

On one hand, we might expect the answer to be a clear-cut "We should learn most likely functions!". In Section 3, we present a well-defined function-space map objective through a generalization of the change of variables formula for probability densities. In addition to functions being the direct quantity of interest and the function-space map objective being invariant to the parametrization of our model, we show that optimization of the function-space map objective indeed results in more probable functions than standard parameter-space map, and these functions often correspond to _flat minima_[10; 12] that are more robust to overfitting.

On the other hand, function-space map is not without its own pathologies and practical limitations. We show in Section 4 that the Jacobian term in the function-space map objective admits trivial solutions with infinite posterior density and can require orders of magnitude more computation and memory than parameter-space map, making it difficult to scale to modern neural networks. We also show that function-space map will not necessarily be closer than parameter-space map to the posterior expectation over functions, which in Bayesian inference forms the mean of the posterior predictive distribution and has desirable generalization properties [2; 19; 29]. To help address the computational limitations, we provide in Section 4 a scalable approximation to the function-space map objective applicable to large neural networks using Laplacian regularization, which we refer to as l-map. We show empirically that parameter-space map is often able to perform on par with l-map in accuracy, although l-map tends to improve calibration.

The aim of this paper is to improve our understanding of what it means to learn most likely functions instead of parameters. Our analysis, which includes theoretical insights as well as experiments with both carefully designed tractable models and neural networks, paints a complex picture and unearths distinct benefits and drawbacks of learning most likely functions instead of parameters. We conclude with a discussion of practical considerations around function-space map estimation, its relationship to flat minima and generalization, and the practical relevance of parameterization invariance.

Our code is available at https://github.com/activatedgeek/function-space-map.

## 2 Preliminaries

We consider supervised learning problems with a training dataset \(=(x_{},y_{})=\{x_{}^{(i)},y_{ }^{(i)}\}_{i=1}^{N}\) of inputs \(x\) and targets \(y\) with input space \(^{D}\) and output space \(^{K}\). For example, the inputs \(x\) could correspond to times, spatial locations, tabular data, images, and the targets \(y\) to regression values, class labels, etc.

Figure 1: **Illustration of the Difference Between Most Likely Functions and Parameters. The function that is most probable (denoted as fs-map) under the posterior distribution over functions can substantially diverge from the function represented by the most probable parameters (denoted as PS-MAP) under the posterior distribution over parameters. We illustrate this fact with a regression model with two parameters, \(_{L}\) and \(_{R}\), both with a prior \((0,1.2^{2})\). This model is used to learn a mixture of two Gaussians with fixed mean and variance, where the mixture coefficients are given by \((_{L})\) and \((_{R})\). Both the most probable solution and the posterior density show significant differences when analyzed in function-space versus parameter-space. Since \(_{L}\) is well-determined, we only plot the posterior as a function of \(_{R}\). We normalize the area under \(p(f_{}|)\) to 1.**

To create a model of the data \(\), one typically starts by specifying a function \(f_{}:\) parametrized by a vector \(^{P}\) which maps inputs to outputs. \(f_{}\) could be a neural network, a polynomial model, a Fourier series, and so on. Learning typically amounts to estimating model parameters \(\). To this end, we can relate the function \(f_{}\) to the targets through an _observation model_, \(p(y|x,f_{})\). For example, in regression, we could assume the outputs \(y=f_{}(x)+\), where \((0,^{2})\) is additive Gaussian noise with variance \(^{2}\). Equivalently, \(p(y|x,)=(y|f_{}(x),^{2})\). Alternatively, in classification, we could specify \(p(y|x,)=((f_{}(x)))\). We then use this observation model to form a _likelihood_ over the whole dataset \(p(y_{}|x_{},)\). In each of these example observation models, the likelihood factorizes, as the data points are conditionally independent given model parameters \(\), \(p(y_{}|x_{},)=_{i=1}^{N}p(y_{}^{( i)}|x_{}^{(i)},)\). We can further express a belief over values of parameters through a prior \(p()\), such as \(p()=(|,)\). Finally, using Bayes rule, the log of the posterior up to a constant \(c\) independent of \(\) is,

\[},x_{})}^{}=}|x_{},)}^{}+^{}+c.\] (1)

Notably, standard loss functions are negative log posteriors, such that maximizing this parameter posterior, which we refer to as _parameter-space_ maximum a posteriori (PS-map) estimation, corresponds to minimizing standard loss functions . For example, if the observation model is regression with Gaussian noise or Laplace noise, the negative log likelihood is proportional to mean-square or mean-absolute error functions, respectively. If the observation model is a categorical distribution with a softmax link function, then the log likelihood is negative cross-entropy. If we use a zero-mean Gaussian prior, we recover standard \(_{2}\) regularization, also known as weight-decay. If we use a Laplace prior, we recover \(_{1}\) regularization, also known as LASSO .

Once we maximize the posterior to find

\[^{}=_{}p(|y_{},x_{ }),\] (2)

we can condition on these parameters to form our function \(f_{^{}}\) to make predictions. However, as we saw in Figure 1, \(f_{^{}}\) is not in general the function that maximizes the posterior over functions, \(p(f_{}|y_{},x_{})\). In other words, if

\[^{}=_{}p(f_{}|y_{},x_ {})\] (3)

then generally \(f_{^{}} f_{^{}}\). Naively, one can write the log posterior over \(f_{}\) up to the same constant \(c\) as above as

\[|y_{},x_{})}^{}=}|x_{},f_{})}^{}+)}^{}+c,\] (4)

where \(p(y_{}|x_{},f_{})=p(y_{}|x_{},)\), but just written in terms of the function \(f_{}\). The prior \(p(f_{})\) however is a different function from \(p()\), because we incur an additional Jacobian factor in this change of variables, making the posteriors also different.

We must take care in interpreting the quantity \(p(f_{})\) since probability densities in infinite-dimensional vector spaces are generally ill-defined. While prior work  avoids this problem by considering a prior only over functions evaluated at a finite number of evaluation points, we provide a more general objective that enables the use of infinitely many evaluation points to construct a more informative prior and makes the relevant design choices of function-space map estimation more interpretable.

## 3 Understanding Function-Space Maximum A Posteriori Estimation

Function-space map estimation seeks to answer a fundamentally different question than parameter-space map estimation, namely, what is the most likely function under the posterior distribution over functions implied by the posterior distribution over parameters, rather than the most likely parameters under the parameter posterior.

To better understand the benefits and shortfalls of function-space map estimation, we derive a function-space map objective that generalizes the objective considered by prior work and analyze its properties both theoretically and empirically.

### The Finite Evaluation Point Objective

Starting from Equation (4), Wolpert  proposed to instead find the map estimate for \(f_{}()\), the function evaluated at a finite set of points \(=\{x_{1},...,x_{M}\}\), where \(M<\) can be chosen to be arbitrarily large so as to capture the behavior of the function to arbitrary resolution. This choice then yields the fs-map optimization objective

\[_{}(;)=_{i=1}^{N} p( y_{}^{(i)}|x_{}^{(i)},f_{}())+ p(f_{ }()),\] (5)

where \(p(f_{}())\) is a well-defined but not in general analytically tractable probability density function. (See Appendix B.1 for further discussion.) Let \(P\) be the number of parameters \(\), and \(K\) the number of function outputs. Assuming the set of evaluation points is sufficiently large so that \(MK P\), using a generalization of the change of variable formula that only assumes injectivity rather than bijectivity, Wolpert  showed that the prior density over \(f()\) is given by

\[p(f_{}())=p()^{-1/2}((;)),\] (6)

where \((;)\) is a \(P\)-by-\(P\) matrix defined by

\[_{}(;) J_{}()^{ }J_{}()\] (7)

and \(J_{}() f_{}()/\) is the \(MK\)-by-\(P\) Jacobian of \(f_{}()\), viewed as an \(MK\)-dimensional vector, with respect to the parameters \(\). Substituting Equation (6) into Equation (5) the function-space map objective as a function of the parameters \(\) can be expressed as

\[_{}(;)=_{i=1}^{N} p( y_{}^{(i)}|x_{}^{(i)},f_{})+ p()- ((;)),\] (8)

where we are allowed to condition directly on \(f_{}\) instead of \(f_{}()\) because by assumption \(\) is large enough to uniquely determine the function. In addition to replacing the function, our true quantity of interest, with its evaluations at a finite set of points \(\), this objective makes it unclear how we should select \(\) and how that choice can be interpreted or justified in a principled way.

### Deriving a More General Objective and its Interpretation

We now derive a more general class of function-space map objectives that allow us to use effectively infinitely many evaluation points and meaningfully interpret the choice of those points.

In general, when performing a change of variables from \(v^{n}\) to \(u^{m}\) via an injective map \(u=(v)\), their probability densities relate as \(p(u)(u)=p(v)(v)\), where \(\) and \(\) define respective volume measures. Suppose we let \((u)=^{m}}u\), the volume induced by an \(M M\) metric tensor \(g\), and \((v)=^{n}v\), the Lebesgue measure, then we can write \((u)=(v))(v)}\), where the \(N N\) metric \((v)=J(v)^{}g(u)J(v)\) is known as the pullback of \(g\) via \(\) and \(J\) is the Jacobian of \(\). As a result, we have \(p(u)=p(v)^{-1/2}(J(v)^{}g(u)J(v))\). Applying this argument and identifying \(u\) with \(f_{}()\) and \(v\) with \(\), Wolpert  thereby establishes \(p(f_{}())=p()^{-1/2}(J_{}()^{}J_{ }())\).

However, an important implicit assumption in the last step is that the metric in function space is Euclidean. That is, \(g=I\) and the squared distance between \(f_{}()\) and \(f_{}()+f_{}()\) is \(s^{2}=_{i=1}^{M}f_{}(x_{i})^{2}\), rather than the general case \(s^{2}=_{i=1}^{M}_{j=1}^{M}g_{ij}f_{}(x_{i}) f_{}(x_{j})\). To account for a generic metric \(g\), we therefore replace \(J_{}()^{}J_{}()\) with \(J_{}()^{}gJ_{}()\). For simplicity, we assume the function output is univariate (\(K=1\)) and only consider \(g\) that is constant i.e., independent of \(f_{}()\) and diagonal, with \(g_{ii}=g(x_{i})\) for some function \(g:^{+}\). For a discussion of the general case, see Appendix B.2. To better interpret the choice of \(g\), we rewrite

\[J_{}()^{}gJ_{}()=_{j=1}^{M}_{X}(x_{j})J_{}(x_{j})^{}J_{}(x_{j}),\] (9)

where we suggestively defined the alias \(_{X}(x) g(x)\) and \(J_{}(x_{j})\) is the \(K\)-by-\(P\)-dimensional Jacobian evaluated at the point \(x_{j}\). Generalizing from the finite and discrete set \(\) to the possibly infinite entire domain \(^{D}\) and further dividing by an unimportant normalization constant \(Z\), we obtain

\[(;p_{X})_{}_{X}(x)J_ {}(x)^{}J_{}(x)\,x=_{p_{X}}J_{ }(X)^{}J_{}(X),\] (10)where \(p_{X}=_{X}/Z\) is a normalized probability density function, with normalization \(Z\)--which is independent of \(\)--only appearing as an additive constant in \((;p_{X})\). We include a further discussion about this limit in Appendix B.4.

Under this limit, we can identify \( p()-(;p_{X})\) with \( p(f_{})\) up to a constant, and thereby express \( p(f_{}|),\) the function-space map objective in Equation (4) as

\[(;p_{X})=_{i=1}^{N} p(y_{}^{(i)} |x_{}^{(i)},f_{})+ p()-(;p_{X}),\] (11)

where the choice of \(p_{X}\) corresponds to a choice of the metric \(g\) in function space. Equation (11) (and approximations thereof) will be the primary object of interest in the remainder of this paper.

Evaluation Distribution as Function-Space Geometry.From this discussion and Equations (10) and (11), it is now clear that the role of the metric tensor \(g\) is to impose a distribution \(p_{X}\) over the evaluation points. Or conversely, a given distribution over evaluation points implies a metric tensor, and as such, specifies the geometry of the function space. This correspondence is intuitive: points \(x\) with higher values of \(g(x)\) contribute more to defining the geometry in function space and therefore should be assigned higher weights under the evaluation distribution when maximizing function space posterior density. Suppose, for example, \(f_{}\) is an image classifier for which we only care about its outputs on set of natural images \(\) when comparing it to another image classifier. The metric \(g()\) and therefore the evaluation distribution \(p_{X}\) only needs support in \(\), and the fs-map objective is defined only in terms of the Jacobians evaluated at natural images \(x\).

Finite Evaluation Point Objective as a Special Case.Consequently, the finite evaluation point objective in Equation (8) can be arrived at by specifying the evaluation distribution \(p_{X}(x)\) to be \(_{X}(x)=_{x^{}}(x-x^{})\), where \(\) is the Dirac delta function and \(=\{x_{1},...,x_{M}\}\) with \(M<\), as before. It is easy to see that \(_{}(;)(; _{X})\). Therefore, the objective proposed by Wolpert  is a special case of the more general class of objectives.

### Investigating the Properties of Function-Space map Estimation

To illustrate the properties of fs-map, we consider the class of models \(f_{}(x)=_{i=1}^{P}(_{i})_{i}(x)\) with domain \(=[-1,1]\), where \(\{_{i}\}_{i=1}^{P}\) is a fixed set of basis functions and \(\) is a non-linear function to introduce a difference between function-space and parameter-space map. The advantage of working with this class of models is that \((;p_{X})\) has a simple closed form, such that

\[_{ij}(;p_{X})=_{p_{X}}_{_{i}} f_{}(X)_{_{j}}f_{}(X)=^{}( _{i})^{}(_{j})_{ij},\] (12)

where \(\) is a constant matrix with elements \(_{ij}=_{p_{X}}[_{i}(X)_{j}(X)]\). Therefore, \(\) can be precomputed once and reused throughout training. In this experiment, we use the set of Fourier features \(\{(k_{i}),(k_{i})\}_{i=1}^{100}\) where \(k_{i}=i\) and set \(=\). We generate training data by sampling \(x_{}(-1,1)\), \(_{i}(0,^{2})\) with \(=10\), evaluating \(f_{}(x_{})\), and adding Gaussian noise with standard deviation \(^{*}=0.1\) to each observation. We use 1,000 test points sampled from \((-1,1)\). To train the model, we set the prior \(p()\) and the likelihood to correspond to the data-generating process. For fs-map, we specify \(p_{X}=(-1,1)\), the ground-truth distribution of test inputs, which conveniently results in \(=I/2\).

FS-map Finds More Probable Functions.Figure 1(a) shows the improvement in \( p(f_{}|)\) when using fs-map over ps-map. As expected, fs-map consistently finds functions with much higher posterior probability \(p(f_{}|)\).

Figure 2: **fs-map exhibits desirable properties.** On a non-linear regression problem, fs-map empirically (a) learns more probable functions, (b) finds flatter minima, (c) improves generalization, and (d) is less prone to overfitting. The plot shows means and standard deviations computed from 3 random seeds.

FS-map Prefers Flat Minima.In Figure 1(b), we compare the curvature of the minima found by fs-map and ps-map, as measured by the average eigenvalue of the Hessian of the mean squared error loss, showing that fs-map consistently finds flatter minima. As the objective of fs-map favors small singular values for the Jacobian \(J_{}\), when multiple functions can fit the data, the fs-map objective will generically prefer functions that are less sensitive to perturbations of the parameters, leading to flatter minima. To make this connection more precise, consider the Hessian \(^{2}()\). If the model fits the training data well, we can apply the Gauss-Newton approximation: \(^{2}_{}()|}\,J_{ }(x_{})^{}J_{}(x_{})\), which is identical to \((;p_{X})\) if \(p_{X}\) is chosen to be the empirical distribution of the training inputs. More generally, a distribution \(p_{X}\) with high density over likely inputs will assign high density to the training inputs, and hence minimizing \((;p_{X})\) will similarly reduces the magnitude of \(J_{}(x_{})\). Therefore, the fs-map objective explicitly encourages finding flatter minima, which have been found to correlate with generalization [10; 17; 18], robustness to data perturbations and noisy activations [3; 12] for neural networks.

fs-map can Achieve Better Generalization.Figure 1(c) shows that fs-map achieves lower test RMSE across a wide range of sample sizes. It's worth noting that this synthetic example satisfies two important criteria such that fs-map is likely to improve generalization over ps-map. First, the condition outlined in Section 4 for a well-behaved fs-map objective is met, namely that the set of partial derivatives \(\{_{_{i}}f_{}^{j}()\}_{i=1}^{P}\) are linearly independent. Specifically, the partial derivatives are given by \(\{(_{i})(k_{i}),(_{i})(k_{ i})\}_{i=1}^{P}\). Since \(\) is non-zero everywhere, the linear independence follows from the linear independence of the Fourier basis. As a result, the function space prior has no singularities and fs-map is thus able to learn from data. The second important criterion is the well-specification of the probabilistic model, which we have defined to precisely match the true data-generating process. Therefore, fs-map seeks the most likely function according to its _true_ probability, without incorrect modeling assumptions.

fs-map is Less Prone to Overfitting.As shown in Figure 1(d), fs-map tends to have a higher train RMSE than ps-map as a result of the additional log determinant regularization. While ps-map achieves near-zero train RMSE with a small number of samples by overfitting to the noise, fs-map's train RMSE is consistently around \(^{*}=0.1,\) the true standard deviation of the observation noise.

Performance Improves with Number of Evaluation Points.We compare fs-map estimation with \(p_{X}=(-1,1)\) and with a finite number of equidistant evaluation points in \([-1,1]\), where the former corresponds to the latter with infinitely many points. In Figure 2(a), we show that the test RMSE of the fs-map estimate (evaluated at 400 training samples) decreases monotonically until the number of evaluation points reaches 200. The more evaluation points, the more the finite-point fs-map objective approximates its infinite limit and the better it captures the behavior of the function. Indeed, 200 points is the minimum sampling rate required such that the Nyquist frequency reaches the maximum frequency in the Fourier basis, explaining the saturation of the performance.

Choice of Evaluation Distribution is Important.In Figure 2(b), we compare test RMSE for 400 training samples for the default choice of \(p_{X}=(-1,1)\), \(p_{X}=(-0.1,0.1)\)--a distribution that does not reflect inputs at test time--and ps-map (\(p_{X}=/\)) for reference. The result shows that specifying the evaluation distribution to correctly reflect the distribution of inputs at test time is required for fs-map to achieve optimal performance in this case.

Figure 3: **Effect of important hyperparameters in fs-map.** fs-map improves with the number of evaluation points and requires carefully specifying the evaluation distribution and the probabilistic model.

## 4 Limitations and Practical Considerations

We now discuss the limitations of function-space map estimation and propose partial remedies, including a scalable approximation that improves its applicability to modern deep learning.

### fs-map Does not Necessarily Generalize Better than ps-map

_There is no guarantee that the most likely function is the one that generalizes the best, especially since our prior can be arbitrary. For example, fs-map can under-fit the data if the log determinant introduces excessive regularization, which can happen if our probabilistic model is misspecified by overestimating the observation noise. Returning to the setup in Section 3.3, in Figure 4, we find that fs-map (with 400 training samples) is sensitive to the observation noise scale \(\) in the likelihood. As \(\) deviates from the true noise scale \(^{*}\), the test RMSE of fs-map can change dramatically. At \(/^{*}=1/3\), the likelihood dominates in both the fs-map and ps-map objectives, resulting in similar test RMSEs. At \(/^{*}=10\), the likelihood is overpowered by the log determinant regularization in fs-map and causes the model to under-fit the data and achieve a high test error. By contrast, the performance of the ps-map estimate is relatively stable. Finally, even if our prior exactly describes the data-generating process, the function that best generalizes won't necessarily be the most likely under the posterior._

### Pathological Solutions

In order for function-space map estimation to be useful in practice, the prior "density" \(p(f_{})=p()^{-}((;p_{X}))\) must not be infinite for any allowed values of \(\), since if it were, those values would constitute global optima of the objective function independent of the actual data. To ensure that there are no such pathological solutions, we require the matrix \((;p_{X})\) to be non-singular for any allowed \(\). We present two results that help determine if this requirement is met.

**Theorem 1**.: _Assuming \(p_{X}\) and \(J_{}=_{}f_{}\) are continuous, \((;p_{X})\) is non-singular if and only if the partial derivatives \(\{_{_{i}}f_{}()\}_{i=1}^{P}\) are linearly independent functions over the support of \(p_{X}\)._

Proof.: See Appendix B.5 

**Theorem 2**.: _If there exists a non-trivial symmetry \(S^{P P}\) with \(S I\) such that \(f_{}=f_{S}\) for all \(\), then \((^{*};p_{X})\) is singular for all fixed points \(^{*}\) where \(S^{*}=^{*}\)._

Proof.: See Appendix B.6 

Theorem 1 is analogous to \(A^{}A\) having the same null space as \(A\) for any matrix \(A\). It suggests that to avoid pathological optima the effect of small changes to each parameter must be linearly independent. Theorem 2 builds on this observation to show that if the model exhibits symmetries in its parameterization, fs-map will necessarily have pathological optima. Since most neural networks at least possess permutation symmetries of the hidden units, we show that these pathological fs-map solutions are almost universally present, generalizing the specific cases observed by Wolpert .

A Simple Remedy to Remove Singularities.Instead of finding a point approximation of the function space posterior, we can perform variational inference under a variational family \(\{q(|)\}_{}\), where \(q(|)\) is localized around \(\) with a small and constant _function-space_ entropy \(h\). Ignoring constants and \((h)\) terms, the variational lower bound is given by

\[_{}(;p_{X})=_{i=1}^{N} p(y^{(i)} _{}|x^{(i)}_{},f_{})+ p()- _{q(^{}|)}[(^{ };p_{X})].\] (13)

Similar to how convolving an image with a localized Gaussian filter removes high-frequency components, the expectation \(_{q(^{}|)}\) removes potential singularities in \((^{};p_{X})\). This effect can be approximated by simply adding a small diagonal jitter \(\) to \((;p_{X}):\)

\[}(;p_{X})_{i=1}^{N} p(y^{(i)}_{ }|x^{(i)}_{},f_{})+ p()- ((;p_{X})+ I).\] (14)

Alternatively, we know \(_{q(^{}|)}[(^{ };p_{X})]\) must be finite because the variational lower bound cannot exceed the log marginal likelihood. \(}(;p_{X})\) can be used as a minimal modification of fs-map that eliminates pathological optima. We provide full derivation for these results in Appendix B.7.

Figure 4: fs-map can underperform ps-map with a misspecified probabilistic model.

### Does the Function-Space map Better Approximate the Bayesian Model Average?

The Bayesian model average (BMA), \(f_{}\), of the function \(f_{}\) is given by the posterior mean, that is:

\[f_{}()\,\,_{p(|)}[f_{}( )]=_{p(f_{}|)}[f_{}()].\]

This expectation is the same when computed in parameter or function space, and has theoretically desirable generalization properties [19; 29]. Both ps-map and fs-map provide point approximations of \(f_{}\). However, fs-map seeks the mode of the distribution \(p(f_{}|)\), the mean of which we aim to compute. By contrast ps-map finds the mode of a distinct distribution, \(p(|)\), which can markedly diverge from \(p(f_{}|)\), depending on the parameterization. Consequently, it is reasonable to anticipate that the fs-map objective generally encourages finding a superior estimate of the BMA.

Consider the large data regime where the posterior in both parameter and function space follows a Gaussian distribution, in line with the Bernstein-von Mises theorem . In Gaussian distributions, the mode and mean coincide, and therefore \(f_{}=f_{}\). However, even in this setting, where \(^{}=_{p(|)}[]\), generally \(f_{^{}} f_{}\), because the expectation does not distribute across a function \(f\) that is non-linear in its parameters \(\): \(f_{_{p(|)}[]}()_{p( |)}[f_{}()]\).

However, we find that whether fs-map better approximates the BMA depends strongly on the problem setting. Returning to the setup in Figure 1, where we have a Gaussian mixture regression model, we compare the BMA function with functions learned by fs-map and ps-map under the prior \(p()=(0,^{2})\) with several settings of \(\). We observe in Figure 5 that fs-map only approximates the BMA function better than ps-map at larger \(\) values. To understand this behavior, recall the height \(h_{R}\) for the right Gaussian bump is given by \((_{R})\), which has a \((0,^{2})\) prior. As we increase \(\), more prior mass is assigned to \(h_{R}\) with near-zero value and therefore to functions with small values within \(x\). While the lognormal distribution also has a heavy tail at large \(h_{R}\), the likelihood constrains the posterior \(p(h_{R}|)\) to only place high mass for small \(h_{R}\). These two effects combine to make the posterior increasingly concentrated in function space around functions described by \(h_{L} 1\) and \(h_{R} 0\), implying that the mode in function space should better approximate its mean. In contrast, as we decrease \(,\) both the prior and posterior become more concentrated in parameter space since the parameter prior \(p()=(0,^{2})\) approaches the delta function at zero, suggesting that ps-map should be a good approximation to the BMA function. By varying \(\), we can interpolate between how well ps-map and fs-map approximate the BMA.

### Scalable Approximation for Large Neural Networks

In practice, the expectation \((;p_{X})=_{p_{X}}[J_{}(X)^{}J_{}(X)]\) is almost never analytically tractable due to the integral over \(X\). We show in Appendix B.9 that a simple Monte Carlo estimate for \((;p_{X})\) with \(S\) samples of \(X\) can yield decent accuracy. For large neural networks, this estimator is still prohibitively expensive: each sample will require \(K\) backward passes, taking a total of \((SKP)\) time. In addition, computing the resulting \(SK\)-by\(P\) determinant takes time \((SKP^{2})\) (assuming \(P SK\)). However, we can remedy the challenge of scalability by further leaning into the variational objective described in Equation (13) and consider the regime where \(_{i}\) for all eigenvalues \(_{i}\) of \((;p_{X}))\). To first order in \(_{i}_{i}/\), we have \(((;p_{X}))+ I)=_{}d( ,+))_{=0}\) where \(d(,^{})\,\,_{p_{X}}[ f_{} (X)-f_{^{}}(X)^{2}]\) and \(=_{i=1}^{P}_{_{i}}^{2}\) denotes the Laplacian operator. Exploiting the identity \(_{}d(,+))_{=0}=}_{(0,^{2}I)}[d(,+)]+ ^{2}\) and choosing \(\) small enough, we obtain an accurate Monte Carlo estimator for the Laplacian

Figure 5: fs-map does not necessarily approximate the BMA better than ps-map. A Gaussian mixture regression model, where the right Gaussian has weight \((_{R})\) with prior \(p(_{R})=(0,^{2})\). As we increase \(\), we interpolate between ps-map and fs-map approximating the BMA.

using only forward passes. The resulting objective which we refer to as Laplacian Regularized map (l-map) is given by

\[_{}(;p_{X})_{i=1}^{N} p( y_{}^{(i)}|x_{}^{(i)},f_{})+ p()-}_{(0,^{2}I)}[d(, +)].\] (15)

We use a single sample of \(\) and \(S\) samples of evaluation points for estimating \(d(,+)\) at each step, reducing the overhead from \(SKP^{2}\) to only \((SP)\). A more detailed exposition is available in Appendix B.10.

### Experimental Evaluation of l-map

We now evaluate l-map applied to neural networks on various commonly used datasets. We provide full experimental details and extended results in Appendix C. Unlike the synthetic task in Section 3.3, in applying neural networks to these more complex datasets we often cannot specify a prior that accurately models the true data-generating process, beyond choosing a plausible architecture whose high-level inductive biases align with the task (e.g. CNNs for images) and a simple prior \(p()\) favoring smooth functions (e.g. an isotropic Gaussian with small variances). Therefore, we have less reason to expect l-map should outperform ps-map in these settings.

UCI Regression.In Table 1, we report normalized test RMSE on UCI datasets , using an MLP with 3 hidden layers and 256 units. l-map achieves lower error than ps-map on 7 out 8 datasets. Since the inputs are normalized and low-dimensional, we use \(p_{X}=(0,I)\) for l-map.

Image Classification.In Table 2, we compare l-map and ps-map on image classification using a ResNet-18 . l-map achieves comparable or slightly better accuracies and is often better calibrated. We further test the effectiveness of l-map with transfer learning with a larger ResNet-50 trained on ImageNet. In Table 3, we show l-map also achieves small improvements in accuracy and calibration in transfer learning.

Loss Landscape.In Figure 6 (Left), we show that l-map indeed finds flatter minima. Further, we plot the Laplacian estimate in Figure 6 (Right) as the training progresses. We see that the Laplacian is much lower for l-map, showing its effectiveness at constraining the eigenvalues of \((;p_{X})\).

Distribution of Evaluation Points.In Table 2, we study the impact of the choice of distribution of evaluation points. Alongside our main choice of the evaluation set (KMNIST for FashionMNIST and CIFAR-100 for CIFAR-10), we use two additional distributions - the training set itself and a white noise \((0,I)\) distribution of the same dimensions as the training inputs. For both tasks, we find that using an external evaluation set beyond the empirical training distribution is often beneficial.

Number of Evaluation Point Samples.In Figure 7(a), we compare different Monte Carlo sample sizes \(S\) for estimating the Laplacian. Overall, l-map is not sensitive to this choice in terms of accuracy. However, calibration error  sometimes monotonically decreases with \(S\).

   Method & Boston & Concrete & Energy & Naval & Power & Protein & Winered & Winwhite \\  ps-map & \(\) & \(.272.016\) & \(.042.003\) & \(.032.005\) & \(.219.006\) & \(.584.005\) & \(.851.029\) & \(.758.013\) \\ l-map & \(.352.040\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Normalized test RMSE (\(\)) on UCI datasets. We report mean and standard errors over six trials.

    &  &  \\  & Acc.\(\) & Sel. Pred.\(\) & NLL\(\) & ECE\(\) & Acc.\(\) & Sel. Pred.\(\) & NLL\(\) & ECE\(\) \\  ps-map & \(93.9\%.1\) & \(98.6\%.1\) & \(.26.01\) & \(4.0\%.1\) & \(95.4\%.1\) & \(99.4\%.0\) & \(.18.00\) & \(2.5\%.1\) \\  l-map \(p_{X}\)=\((0,I)\) & \(94.0\%.0\) & \(99.2\%.0\) & \(.25.01\) & \(4.0\%.2\) & \(95.3\%.1\) & \(99.4\%.0\) & \(.20.00\) & \(3.0\%.1\) \\ \(p_{X}\)=Train & \(93.8\%.1\) & \(99.2\%.1\) & \(.27.01\) & \(4.3\%.2\) & \(95.6\%.1\) & \(99.5\%.0\) & \(.18.01\) & \(2.6\%.0\) \\ \(p_{X}\)=\(^{}\) & \(94.1\%.1\) & \(99.2\%.1\) & \(.26.01\) & \(4.1\%.1\) & \(95.5\%.1\) & \(99.5\%.0\) & \(.16.01\) & \(1.4\%.1\) \\   

Table 2: We report the accuracy (acc.), negative log-likelihood (nll), expected calibration error  (ece), and area under selective prediction accuracy curve  (Sel. Pred.) for FashionMNIST  (\(^{}\) = KMNIST ), and CIFAR-10  (\(^{}\) = CIFAR-100). A ResNet-18  is used. Std. errors are reported over five trials.

Robustness to Label Noise.In Figure 7(b), we find l-map is slightly more robust to label noise than ps-map on CIFAR-10.

Main Takeaways.l-map shows qualitatively similar properties as fs-map such as favoring flat minima and often provides better calibration. However, it achieves comparable or only slightly better accuracy on more complex image classification tasks. In line with our expectations, these results suggest that accounting for the precise difference between fs-map and ps-map is less useful without a sufficiently well-motivated prior.

## 5 Discussion

While we typically train our models through ps-map, we show that fs-map has many appealing properties in addition to re-parametrization invariance. We empirically verify these properties, including the potential for better robustness to noise and improved calibration. But we also reveal a more nuanced and unexpected set of pros and cons for each approach. For example, while it is natural to assume that fs-map more closely approximates a Bayesian model average, we clearly demonstrate how there can be a significant discrepancy. Moreover, while ps-map is not invariant to re-parametrization, which can be seen as a fundamental pathology, we show fs-map has its own failure modes such as pathological optima, as well as practical challenges around scalability. In general, our results suggest the benefits of fs-map will be greatest when the prior is sufficiently well-motivated.

Our findings engage with and contribute to active discussions across the literature. For example, several works have argued conceptually--and found empirically--that solutions in _flatter_ regions of the loss landscape correspond to better generalization [8; 10; 11; 12; 25]. On the other hand, Dinh et al.  argue that the ways we measure flatness, for example, through Hessian eigenvalues, are not parameterization invariant, making it possible to construct striking failure modes. Similarly, ps-map estimation is not parameterization invariant. However, our analysis and comparison to fs-map estimation raise the question to what extent lack of parametrization invariance is actually a significant practical shortcoming--after all, we are not reparametrizing our models on the fly, and we have evolved our models and training techniques conditioned on standard parameterizations.

Figure 6: **Empirical evidence that l-map finds flatter minima.****(Left)** For various step sizes in 20 randomly sampled directions starting at the minima, we compute the training loss averaged over all directions to summarize the local loss landscape. We use filter normalization for landscape visualization . l-map visibly finds flatter minima. **(Right)** We plot the Laplacian estimate throughout training, showing l-map is indeed effective at constraining the eigenvalues of \((;p_{X})\).

Figure 7: **(Left)** Calibration error can be reduced by increasing the number of Monte Carlo samples for the Laplacian estimator in Equation (15). **(Right)** l-map is slightly more robust to training with label noise. For each level of noise, we replace a fraction of labels with uniformly random labels.

   Method & Acc. \(\) & Sel. Pred. \(\) & NLL \(\) & ECE \(\) \\  ps-map & \(96.3\% 0.1\) & \(99.5\% 0.1\) & \(0.18 0.01\) & \(2.6\% 0.2\) \\ l-map & \(96.4\% 0.1\) & \(99.5\% 0.1\) & \(0.15 0.01\) & \(2.1\% 0.1\) \\   

Table 3: Transfer learning from ImageNet with ResNet-50 on CIFAR-10 can lead to slightly better accuracy and improved calibration.

**Acknowledgements.** We thank Marc Finzi, Pavel Izmailov, and Micah Goldblum for helpful discussions. This work is supported by NSF CAREER IIS-2145492, NSF I-DISRE 193471, NSF IIS-1910266, BigHat Biosciences, Capital One, and an Amazon Research Award.