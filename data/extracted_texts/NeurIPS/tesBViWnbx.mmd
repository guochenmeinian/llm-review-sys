# Stable Diffusion is Unstable

Chengbin Du, Yanxi Li, Zhongwei Qiu, Chang Xu

School of Computer Science, Faculty of Engineering

University of Sydney, Australia

chdu5632@uni.sydney.edu.au, yali0722@uni.sydney.edu.au

zhongwei.qiu@sydney.edu.au, c.xu@sydney.edu.au

###### Abstract

Recently, text-to-image models have been thriving. Despite their powerful generative capacity, our research has uncovered a lack of robustness in this generation process. Specifically, the introduction of small perturbations to the text prompts can result in the blending of primary subjects with other categories or their complete disappearance in the generated images. In this paper, we propose **Auto-attack on Text-to-image Models (ATM)**, a gradient-based approach, to effectively and efficiently generate such perturbations. By learning a Gumbel Softmax distribution, we can make the discrete process of word replacement or extension continuous, thus ensuring the differentiability of the perturbation generation. Once the distribution is learned, ATM can sample multiple attack samples simultaneously. These attack samples can prevent the generative model from generating the desired subjects without tampering with the category keywords in the prompt. ATM has achieved a 91.1% success rate in short-text attacks and an 81.2% success rate in long-text attacks. Further empirical analysis revealed three attack patterns based on: 1) variability in generation speed, 2) similarity of coarse-grained characteristics, and 3) polysemy of words. The code is available at https://github.com/duchengbin8/Stable_Diffusion_is_Unstable

## 1 Introduction

In recent years, the field of text-to-image generation has witnessed remarkable advancements, paving the way for groundbreaking applications in computer vision and creative arts. Notably, many significant developments have captured the attention of researchers and enthusiasts, such as Stable Diffusion , DALL-E2  and Midjourney . These developments push text-to-image synthesis boundaries, fostering artistic expression and driving computer vision research.

Despite the remarkable progress in text-to-image models, it is important to acknowledge their current limitations. One significant challenge lies in the instability and inconsistency of the generated outputs. In some cases, it can take multiple attempts to obtain the desired image that accurately represents the given textual input. An additional obstacle revealed by recent researches  is that the quality of generated images can be influenced by specific characteristics inherent to text prompts. Tang et al.  proposes DAAM, which performs a text-image attribution analysis on conditional text-to-image model and produces pixel-level attribution maps. Their research focuses on the phenomenon of feature entanglement and uncovers that the presence of cohyponyms may degrade the quality of generated images and that descriptive adjectives can attend too broadly across the image. Attend-and-Excite  investigates the presence of catastrophic neglect in the Stable diffusion model, where the generative model fails to include one or more of the subjects specified in the input prompt. Additionally, they discover instances where the model fails to accurately associate attributes such as colors with their respective subjects. Although those works have some progress, there is still work to be done to enhance the stability and reliability of text-to-image models, ensuring consistent and satisfactory results for a wide range of text prompts.

One prominent constraint observed in those works related to the stability of text-to-image models lies in their dependence on manually crafted prompts for the purpose of vulnerability identification. This approach presents several challenges. Firstly, it becomes difficult to quantify the success andfailure cases accurately, as the evaluation largely depends on subjective judgments and qualitative assessments. Additionally, the manual design of prompts can only uncover a limited number of potential failure cases, leaving many unexplored scenarios. Without a substantial number of cases, it becomes challenging to identify the underlying reasons for failures and effectively address them. To overcome these limitations, there is a growing demand for a learnable method that can automatically identify failure cases, enabling a more comprehensive and data-driven approach to improve text-to-image models. By leveraging such an approach, researchers can gain valuable insights into the shortcomings of current methods and develop more robust and reliable systems for generating images from textual descriptions.

In this paper, we propose **Auto-attack on Text-to-image Models (ATM)**, to automatically and efficiently generate attack prompts with high similarity to given clean prompts (Fig. 1). We use Stable Diffusion [28; 32] as our target model. With the open-source implementation and model parameters, we can generate attack prompts with a white-box attack strategy. Remarkably, those attack prompts can transfer to other generative models, enabling black-box attacks. Two methods to modify a text prompt are considered, including **replacing** an existing word or **extending** with new ones. By incorporating a Gumbel Softmax distribution into the word embedding, the discrete modifications can be transformed into continuous ones, thereby ensuring differentiability. To ensure the similarity between clean and the attack prompts, a binary mask that selectively preserves the noun representing the desired object is applied. Moreover, two constraints are imposed: a **fluency constraint** that ensures the attack prompt is fluent and easy to read, and a **similarity constraint** that regulates the extent of semantic changes.

After the distribution is learned, ATM can sample multiple attack prompts at once. The attack prompts can prevent the diffusion model from generating desired subjects without modifying the nouns of desired subjects and maintain a high degree of similarity with the original prompt. We have achieved a 91.1% success rate in short-text attacks and a 81.2% success rate in long-text attacks. Moreover, drawing upon extensive experiments and empirical analyses employing ATM, we are able to disclose the existence of three distinct attack patterns, each of which corresponds to a vulnerability in the generative model: 1) the variability in generation speed; 2) the similarity of coarse-grained characteristics; 3) the polysemy of words. In the following, we will commence with an analysis of the discovered attack patterns in Section 4, followed by a detailed exposition of our attack method in Section 5.

In addition, ATM can automatically and efficiently generate plenty of successful attack prompts, which serves as a valuable tool for investigating vulnerabilities in text-to-image generation pipelines. This method enables the identification of a wider range of attack patterns, facilitating a comprehensive examination of the underlying causes. It will inspire the research community and garner increased attention toward exploring the vulnerabilities present in contemporary text-to-image models and will foster further research concerning both attack and defensive mechanisms, ultimately leading to enhanced security within the industry.

Figure 1: The overall pipeline of our attack method. The selection of words is relaxed to be differentiable by using a Gumbel Softmax with temperature \(\). When \( 0\), the Gumbel Softmax outputs exhibit proximity to one-hot vectors while retaining differentiability. After an image is generated, a CLIP  classifier and a margin loss are employed to optimize \(\) aiming to generate images that cannot be correctly classified by CLIP.

## 2 Related Work

### Diffusion Model.

Recently, the diffusion probabilistic model  and its variants [12; 21; 31; 28; 29; 4; 5; 15] have achieved great success in content generation [31; 13; 29], including image generation [12; 31], conditional image generation , video generation [13; 36], 3D scenes synthesis  and so on. Specifically, DDPM  adds noises to images and learns to recover images from noises step by step. Then, DDIM  improves the generation speed of the diffusion model by skipping steps inference. Then, the conditional latent diffusion model  formulates the image generation in latent space guided by multiple conditions, such as texts, images, and semantic maps, further improving the inference speed and boarding the application of the diffusion model. Stable diffusion , a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, and its enhanced versions [37; 14; 20], have been widely used in current AI-generated content products, such as Stability-AI , Midjourney , DALL-E2 , and Runaway . However, these methods and products cannot always generate satisfactory results from the given prompt. Therefore, in this work, we aim to analyze the robustness of stable diffusion in the generation process.

### Vulnerilities in Text-to-image Models.

With the open-source of Stable Diffusion , text-to-image generation achieves great process and shows the unparalleled ability on generating diverse and creative images with the guidance of a text prompt. However, there are some vulnerabilities have been discovered in existing works [8; 2; 33]. Typically, StructureDiffusion  discovers that some attributes in the prompt are not assigned correctly in the generated images, thus they employ consistency trees or scene graphs to enhance the embedding learning of the prompt. In addition, Attend-and-Excite  also introduces that the Stable Diffusion model fails to generate one or more of the subjects from the input prompt and fails to correctly bind attributes to their corresponding subjects. These pieces of evidence demonstrate the vulnerabilities of the current Stable Diffusion model. However, to the best of our knowledge, no work has systematically analyzed the vulnerabilities of the Stable Diffusion model, which is the goal of this work.

## 3 Preliminary

The architecture of the Stable Diffusion  comprises of an encoder \(:\) and a decoder \(:\), where \(}=(())\). Additionally, a conditional denoising network \(_{}\) and a condition encoder \(_{}\) are employed. In the text-to-image task, the condition encoder is a text encoder that maps text prompts into a latent space. The text prompt is typically a sequence of word tokens \(=\{_{1},,_{K}\}\), where \(K\) is the sequence length. During the image generation process, a random latent representation \(_{T}\) is draw from a distribution such as a Gaussian distribution. Then, the reverse diffusion process is used to gradually recover a noise-free latent representation \(\). Specifically, a conditional denoising network \(_{}(_{t},t,_{}())\) is trained to gradually denoise \(_{t}\) at each time step \(t=T,,1\) to gradually reduce the noise level of \(_{t}\), where the condition \(\) is mapped in to a latent space using \(_{}()\) maps and the cross-attention between the condition and features is incorporated in \(_{}\) to introduce the condition. Finally, the denoised latent representation \(\) is decoded by a decoder \(\) to produce the final output \(}\). The aim of our study is to introduce slight perturbations to the text prompts, thereby inducing the intended object to be blended with other objects or entirely omitted from the generated images. For the sake of simplification, in the forthcoming sections, we shall represent the image generation process using the GM as \(}=(_{T}|)\).

## 4 Vulnerabilities of Stable Diffusion Model

By applying the attack method proposed in this paper, a variety of attack text prompts can be generated and analyzed. In this section, the identified attack patterns are discussed. Details of the attack method are introduced in Section 5. We have discovered three distinct patterns of impairment in Stable Diffusion model: 1) Variability in Generation Speed, where the model struggles to reconcile the differences in generation speed among various categories effectively. 2) Similarity of Coarse-grained Characteristics, which arises from the feature entanglement of global or partial coarse-grained characteristics, which possess a high degree of entanglement. 3) Polysemy of Words, which involves the addition of semantically complementary words to the original prompt, resulting in the creation of images that contain brand-new content and are not related to the original category.

### Variability in Generation Speed

**Observation 1**.: _When a given text prompt contains a noun \(A\) representing the object to be generated, the introduction of another noun \(B\) into the prompt, either through addition or replacement, leads to the disappearance of noun \(A\) from the generated image, replaced instead by noun \(B\)._

In Observation 1, we identify a phenomenon when replacing or adding a noun in the description in a text prompt, the new noun will lead to the completely disappear of the desired subject. As shown in Fig. 2, if the noun "peak" is replaced by "peafowl", the desired subject "mountain" disappears and the new subject "peafowl" is generated. To further investigate this phenomenon, we use two short prompts \(_{1}\) and \(_{2}\) containing "mountain" and "peafowl", respectively, to exclude the influence of other words in the long prompts. To eliminate the additional impact of all possible extraneous factors, such as contextual relationships, they are embedded separately and then concatenated together: \((_{}(_{1}),_{}(_{2}))\). The result shows that almost no element of the mountain is visible in the generated image (Fig. 2).

Further analysis reveals a nontrivial difference in the generation speeds of the two subjects. To define the generation speed, a metric to measure the distance from a generated image \(}_{t}\) at a given time step \(t=T-1,,0\) to the output image \(}_{0}\) is desired (note, \(}_{T}\) is the initial noise). We use the structural similarity (SSIM)  as the distance metrics: \(s(t):=(}_{t},}_{0})\). Therefore, the generation speed can be formally defined as the derivative of the SSIM regarding the time step: \(v(t):=ds(t)/dt(s(t)-s(t+1))/ t\), where \( t=1\). Thereby, we propose our Pattern 1.

**Pattern 1** (Variability in Generation Speed).: _Comparing the generation speeds (\(v_{1}\) and \(v_{2}\)) of two subjects ( \(S_{1}\) and \(S_{2}\)), it can be observed that the outline of the object in the generated image will be taken by \(S_{1}\) if \(v_{1}>v_{2}\). And it can be inferred that \(S_{2}\) will not be visible in the resulting image._

We further generates \(1,000\) images of various classes with the same initial noise and visualize their generation speed in Fig. 4 as a violin plot. The SSIM distance from the generated images at each

Figure 4: A violin plot illustrating the generation speeds of \(1,000\) images of various classes. The horizontal axis represents the number of steps taken, ranging from 49 to 0, while the vertical axis displays the SSIM scores. The width of each violin represents the number of samples that attained a specific range of SSIM scores at a given step.

step to the final image is calculated. The horizontal axis represents \(49 0\) steps, while the vertical axis represents the SSIM scores. Each violin represents the distribution of the SSIM scores of the \(1,000\) images in a step, with the width corresponds to the frequency of images reaches the score. In the early stages of generation, the median of the distribution is positioned closer to the minimum value, indicating that a majority of classes exhibit slow generation speeds. However, the presence of a high maximum value suggests the existence of classes that generate relatively quickly. In the middle stages of generation, the median of the distribution gradually increases, positioning itself between the maximum and minimum values. In the later stages of generation, the median of the distribution is positioned closer to the maximum value, indicating that a majority of classes are nearing completion. However, the persistence of a low minimum value suggests the presence of classes that still exhibit slow generation speeds. This analysis highlights the variation in generation speeds across different classes throughout the entire generation process. This phenomenon can be interpreted as a characteristic of the generation process, where different classes exhibit varying speeds throughout the stages. It is possible that certain classes have inherent complexities or dependencies that cause them to generate more slowly. Conversely, other classes may have simpler structures or fewer dependencies, leading to faster generation.

### Similarity of Coarse-grained Characteristics

**Observation 2**.: _When a text prompt contains a noun \(A\) representing the object to be generated, the introduction of another noun \(B\), which describes an object with similar coarse-grained characteristics to the object represented by noun \(A\), into the prompt, either through addition or replacement, results in the generation of an image that contains an object combining elements of both nouns \(A\) and \(B\)._

Figure 5: Images generated by the template "A photo of \(A\) and \(B\)".

Figure 6: The first row illustrates the generation process with the prompt "a photo of a silver salmon". The second row, based on the forty-second step of the first row, shows the generation process with the prompt "a photo of a feather". The third row, also building upon the forty-second step of the first row, presents the generation procedure when the prompt is "a photo of a magician". The fourth row depicts the generation process in the presence of feature entanglement. The fifth row demonstrates the generation process for two distinct categories without feature entanglement.

The second case that we observed in our attacks is when two nouns in the text prompt share similar coarse-grained characteristics, the generated image will contain a subject that is a combination of these two nouns. As illustrated in Fig. 6, when given the text "a silver salmon and a feather," the GMs generate an image of a feather with the outline of a salmon. This happens because these two nouns (_i.e._, salmon and feather) exhibit a certain degree of similarity in their coarse-grained attributes. In contrast, there is no feature entanglement between "salmon" and "magician" because their coarse-grained features are vastly different from each other.

To verify this assumption, we first obtain the generated latent variable for the prompt "a photo of a silver salmon" at the early sampling step (_e.g._, 8 steps). Using this latent variable, we replace the prompt with "a photo of a feather" and continue generating images. The results confirm that feathers can continue to be generated based on the coarse-grained properties of silver salmon, and the final generated graph has high similarity with the generated graph of the prompt "a photo of a silver salmon and a feather". However, replacing "silver salmon" with "magician" does not seem to generate any object similar to "magician". This observation indicates that there is no coarse-grained feature entanglement between these two subjects. We summarize this observation in Pattern 2.

**Pattern 2** (Similarity of Coarse-grained Characteristics).: _Let \(X_{t}^{A}\) and \(X_{t}^{B}\) denote the latent variables generated by the GM for word tokens \(A\) and \(B\), respectively. Suppose \(t\) is small and let \(d\) represent the metric that measures the outline similarity between two images. If the text prompt contains both \(A\) and \(B\), and \(d(X_{t}^{A},X_{t}^{B})\) falls below the threshold \(\), then feature entanglement occurs in the generated image._

Based on the observed Pattern 2, The types of feature entanglement can be further divided into, direct entanglement and indirect entanglement. As shown in Fig. 5, direct entanglement represents the direct entanglement triggered by two categories of coarse-grained attributes that have global or local similarities. Indirect entanglement is shown in Fig. 4(d), where the additional attribute trunk brought by the howler monkey has a high similarity with the coarse-grained attribute of the snake, thus triggering the entanglement phenomenon.

### Polysemy of Words

**Observation 3**.: _When a text prompt contains a noun \(A\) representing the object to be generated, if the semantic scope of noun \(A\) encompasses multiple distinct objects, the generated image contains one of the objects described by noun \(A\). If there exists another word \(B\) that, when combined with noun \(A\), directs its semantics to a particular object, the introduction of word \(B\) into the prompt, either through addition or replacement, leads to the generation of an image that specifically contains that particular object._

The third scenario we observed in the attack is that the content of the generated image is not directly related to either the desired image or the added word. However, this is again different from a category of disappearance, where the desired target has a clear individual in the image. As illustrated in Figs. 6(a), 6(b) and 6(c), when the cleaning prompt "a photo of a bat" was modified to "a photo of a bat and a ball", the bat disappeared completely from the generated image, but the DDAM heat map showed that the word "bat" is highly associated with the stick-like object in the newly generated image.

Figure 8: a) t-SNE Visualization of 100 images each of "bat", "baseball bat", "bat and ball" and text “a photo of a bat." b) The boxplot of cosine similarities between the text embedding of “a photo of a bat” and 100 of image embeddings each of "bat", "baseball bat", and "bat and ball".

Figure 7: a) ”A photo of a bat”; b) ”A photo of a bat and a ball;” c) Heat map of the word ”bat” in generated image; d) “A photo of a warthog”; e) “A photo of a warthog and a traitor”; f) Heat map of the word ”warthog” in generated image.

**Pattern 3** (Polysemy of Words).: _When interpreting polysemous words, language models must rely on contextual cues to distinguish meanings. However, in some cases, the available contextual information may be insufficient or confuse the model by modifying specific words, resulting in a model-generated image that deviates from the actual intention of the user._

To further investigate the phenomenon of word polysemy in the Stable diffusion model, we used the prompts "a photo of a bat", "a photo of a baseball bat" and "a photo of a bat and a ball" to generate 100 images each using the stable diffusion model, and transformed these images into embedding form by CLIP image encoder, and transformed "a photo of a bat" into embedding form by CLIP text encoder, then visualized these 301 embeddings by t-SNE. As illustrated in Fig. 7(a), Considering the entire set of bat images, bats (the animal) are closer to the text "a photo of a bat" than baseball bats are, as depicted in the t-SNE visualization. However, the distribution of the two categories also has relatively close proximity, indicating their underlying similarities. The category of "bat and ball" shows a more expansive distribution, almost enveloping the other two. This suggests that by modifying the original text from "a photo of a bat" to "a photo of a bat and a ball", the distribution of the clean text can be pulled towards another meaning in the polysemous nature of the word "bat". From the perspective of text-to-image model, this kind of modification can stimulate the polysemous property of the word, thereby achieving an attack effect.

In addition to this explicit polysemy, our algorithm further demonstrates its aptitude for detecting subtler instances of polysemous words. As depicted in Figure 7, the transformative capacity of our algorithm is evident when an image of a warthog (Fig. 6(d)) transfigures into an image of a military chariot (Fig. 6(e)) with the incorporation of the term "traitor".

## 5 Auto-attack on Text-to-image Model

We aim to design an automatic attack method that targets the recent popular text-to-image models. The objective of our method is to identify attack prompts \(^{}\) based on a clean prompt \(\), which leads to a vision model \(h:\) failing to predict the desired class \(y\), i.e. \(*{argmax}_{i}h(})_{i} y\):

\[^{}=*{argmax}_{^{} B_{d}(,) }(y,h((^{})))\] (1)

where \(()\) is a text-to-image model, \(B(,)=\{^{}:d(,^{})\}\), \(d(,)\) is a distance measure regularizing the similarity between \(\) and \(^{}\), and \(\) is a maximum distance. To enable auto-attack, a differentiable method that can be optimized using gradient descent is desired. We introduce a Gumbel-Softmax sampler to enable differentiable modifications on text prompts during the word embedding phase. To minimize the distance \(d(,^{})\), we introduce two constraints, including a fluency constraint and a similarity constraint.

In our experimental setup, the open-source Stable Diffusion model is employed as the targeted generative model \(()\). By generating white-box attack prompts for Stable Diffusion, we can subsequently transfer these prompts to other generative models to execute black-box attacks. To facilitate the classification task, we utilize a CLIP classifier as the vision model \(h()\), benefiting from its exceptional zero-shot classification accuracy. To establish the desired classes, we employ the 1,000 classes derived from ImageNet-1K. In the case of generating short prompts, a fixed template of "A photo of [CLASS_NAME]" is utilized to generate the prompts. Conversely, for the generation of long prompts, we employ ChatGPT 4  as a prompt generation model. Subsequently, human experts verify the correctness of the prompts and check that the prompts indeed contain the noun associated with the desired class.

### Differentiable Text Prompt Modification

A text prompt is typically a sequence of words \(=\{_{1},,_{K}\}\). Owing to the discrete nature of text prompts \(\), perturbations can be incorporated either by replacing an existing word \(_{k}\) where \(1 k K\) or augmenting with new ones \(\{_{K+i}|1 i K^{}\}\). However, the non-differentiable nature of this procedure makes it unsuitable for optimization utilizing gradient-based techniques. Therefore, there is a need for a mechanism that guarantees the differentiability of the word selection process. In this regard, we integrate a Gumbel Softmax sampler \((;)\) into the word embedding phase. The Gumbel Softmax function has the ability to approximate a one-hot distribution as the temperature \( 0\). Additionally, the Gumbel distribution has the ability to introduce further randomness, thereby enhancing the exploitability during the initial stages of perturbation search.

**Differentiable Sampling.** We employ a trainable matrix \(^{K V}\) to learn the word selection distribution, where \(K\) is the length of the text prompt and \(V\) is the vocabulary size. In the scenario of augmenting with a new word, the sequence length \(K\) can be extended to \(K+K^{}\) to facilitate the addition of new words. The Gumbel Softmax can be represented as follows:

\[(_{k};):=_{k,i}+g _{k,i})/)}{_{j}((_{k,j}+g_{k,j})/)},\] (2)

where \(g_{k,i}(0,1)\) are i.i.d. samples from a Gumbel distribution. The word embedding stage employs a matrix \(E^{V D}\), where \(V\) is the vocabulary size and \(D\) is for the embedding dimensionality. The discrete process of word embedding is to select \(E_{i}\) from \(E\) based on the index \(1 i V\) of a word in the vocabulary. To make this process differentiable, the dot product between \((_{k})\) and \(E\) can be calculate:

\[^{}_{k}=(_{k};)=(_{k};) E E_{i},i=*{argmax}_{i}_{k,i},\] (3)

where \(^{}_{k}\) is the new word selected according to \(_{k}\). As \( 0\), Eq. 3 can effectively emulate an \(*{argmax}\) selection procedure.

Additionally, in order to ensure similarity, it is desired to preserve the noun representing the desired object in the new prompt. This is achieved by utilizing a binary mask \(M\{0,1\}^{K}\), where the position corresponding to the desired noun is set to \(0\) while other positions are set to \(1\). By computing \(^{}(1-M)+M^{}\), the desired noun can be retained in the prompt while other words can be modified.

**Attack Objective.** To generate images \(}\) that cannot be correctly classified by the classifier, a margin loss  can be used as the loss function \((,)\) in Eq. 1:

\[_{}(},y;h)=(0,h(})_{y}-_{i y}h(})_{i}+),\] (4)

where \(\) is a margin. Eq. 4 reduces the classifier's confidence on the true class \(y\) and improve its confidence on the class with the largest confidence, excluding \(y\) until a margin \(\) is reached.

### Constraints on Fluency and Similarity

Given that we search for perturbations in a \(^{K V}\) space to attack the text prompt, the attack prompts may be too diverse if the added perturbations are not properly constrained, making it easily detectable. Eq. 1 includes a distance constraint such that \(d(,^{})\), which ensures that the added perturbations are subtle and hard to notice. The measurement of distance between two pieces of text can be approached through various methods. We introduce two constraints to reduce this distance, namely a **fluency** constraint and a **semantic similarity** constraint. The fluency constraint ensures that the generated sentence is smooth and readable, while the semantic similarity semantic constraint regularize the semantic changes introduced by the perturbations, making the attack prompt \(^{}\) closely resemble the clean prompt \(\).

**Fluency constraint.** The fluency constraint can be achieved visa a Casual Language Model (CLM) \(\) with log-probability outputs . The next token distribution we learn is compared with the next token distribution predicted by \(\). Given a sequence of perturbed text \(^{}\), we use \(\) to predict the a token \(^{}_{i}\) based on \(\{^{}_{1},^{}_{i-1}\}\). Therefore, we can have a log-likelihood of the possible next word:

\[ p_{}(^{}_{i}|^{}_{1},,^{ }_{i-1})=(^{}_{1},^{}_{i-1}).\] (5)

The next token distribution we learn can be easily obtained by \((_{i};)\). Subsequently, a cross-entropy loss function can be employed to optimize the learned distribution:

\[_{}()=-_{i=1}^{K}_{j=1}^{D}(_{i};)_{j}((_{1};),( _{i-1};))_{j}.\] (6)

Eq. 6 serves as a regularizer to encourage the next word selection distribution to resemble the prediction of the CLM \(\), thereby ensuring fluency.

**Semantic similarity constraint.** Rather than simply considering a word similarity, we concern more about semantic similarity. One prominent metric used to evaluate semantic similarity is the BERTScore . The calculation of BERTScore requires contextualized word embeddings. The aforementioned CLM \(\) is used again to extract the embeddings \(=^{()}()\), where \(^{()}\) denotes the embedding network used in \(\). The BERTScore between the clean prompt \(\) and the attack prompt \(^{}\) can be calcualted by

\[S_{}(,^{})=_{i=1}^{N}w_{i}_ {j=1,,M}_{i}^{}_{j}^{},\] (7)

where \(w_{i}:=(_{i})/_{i=1}^{N}(_{i})\) is the normalized inverse document frequency (\(\)), \(N=K\), and \(M\) is either \(K\) or \(K+K^{}\) depending on whether existing words are being replaced or new words are being added. To improve the similarity, we use \(1-S_{}(,^{})\) as the our loss term.

**The constrained objective.** Considering that the addition of constraints may limit the diversity of perturbation search, we introduce two hyper-parameters, \(\) and \(\), to control the strength of the constraints. Then, the overall objective function can be written as:

\[_{} _{_{T}(0,1),^{}= (;)}[_{}((_{T}| ^{}),y;h)]\] (8) \[ _{} _{}()+(1-S_{ }(,^{})).\] (9)

### Generation of Attack Prompts

The overall procedure of ATM is as described in Algorithm 1. It consists of two stages: a search stage, where the Gumbel Softmax distribution is learned, and an attack stage, where we generate attack prompts using the learned distribution. In the search stage, we use gradient descent to optimize the parameters \(\) for each clean prompt \(\) over a period of \(T\) iterations. Once \(\) is learned, we proceed to the attack stage. In this stage, we sample \(N\) attack prompts from each learned \(\). An attack prompt \(^{}\) is considered successful if the image \(}^{}\) generated from it cannot be correctly classified by the visual classifier \(h\).

```
0: The maximum number of iterations \(T\). The maximum number of attack candidates \(N\). The clean prompt \(\). The desired class \(y\). A binary mask \(M\). A learning rate \(\).
0: A set of attack prompts \(\).
1: Initialize \(\)
2:for\(t=1 T\)do\(\) The search stage
3: Sample an attack prompt \(^{}=\{(_{k};)|1 k K\}\)
4: Apply the mask by \(^{}(1-M)+M^{}\)
5: Generate an image \(}^{}=(_{T}|^{})\)
6: Get classification results \(y^{}=h(}^{})\)
7: Conduct a gradient descent step \(-_{}( )\)
8:endfor
9: Initialize \(=\)
10:for\(n=1 N\)do\(\) The attack stage
11: Sample an attack prompt \(^{}=\{(_{k};)|1 k K\}\)
12: Apply the mask by \(^{}(1-M)+M^{}\)
13: Generate an image \(}^{}=(_{T}|^{})\)
14:if\(*{argmax}h(}^{}) y\)then\(\) If attack success
15: Save the success attack prompt \(\{^{}\}\)
16:endif
17:endfor ```

**Algorithm 1** Auto-attack on Text-to-image Models (ATM)

To corroborate the effectiveness of our proposed algorithm, we implemented a structured evaluation scheme based on the model template: "A photo of [CLASS_NAME]". An extensive dataset comprising one thousand abbreviated textual descriptions, corresponding to all classes within ImageNet , was meticulously curated. Additionally, comprehensive long-form textual descriptions for thesesame classes were generated utilizing the capabilities of ChatGPT4 . Upon rigorous testing, our algorithm demonstrated noteworthy performance, recording attack success rates of 91.1% and 81.2% on the short-text and long-text datasets, respectively. Moreover, the calculated cosine similarity between the embeddings of the original and attack prompts yielded values of 0.7227 and 0.8364, respectively. Due to the length of this paper, detailed experiment results, including comparisons to baselines and ablation studies, are reported in the supplementary material.

To further investigate whether our generated attack prompts can be transferred to different text-to-image models, we randomly select several attack prompts to attack DALL-E2 and Midjourney, respectively. The experimental results prove that our attack prompts can also be used for black-box attacks. More results of black-box attacks are reported in the supplementary material.

## 6 Conclusion

The realm of text-to-image generation has observed a remarkable evolution over recent years, while concurrently exposing several vulnerabilities that require further exploration. Despite the many advancements, there are key limitations, specifically concerning the stability and reliability of generative models, which remain to be addressed. This paper has introduced Auto-attack on Text-to-image Models (ATM), a novel approach that generates a plethora of successful attack prompts, providing an efficient tool for probing vulnerabilities in text-to-image models. ATM not only identifies a broader array of attack patterns but also facilitates a comprehensive examination of the root causes. We believe that our proposed method will inspire the research community to shift their focus toward the vulnerabilities of present-day text-to-image models, stimulating further exploration of both attack and defensive strategies. This process will be pivotal in advancing the security mechanisms within the industry and contributing to the development of more robust and reliable systems for generating images from textual descriptions.

## 7 Acknowledgments

This work was supported in part by the Australian Research Council under Projects DP210101859 and FT230100549.