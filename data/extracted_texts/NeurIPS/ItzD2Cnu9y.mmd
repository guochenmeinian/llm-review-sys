# Randomized Sparse Matrix Compression for

Large-Scale Constrained Optimization in Cancer Radiotherapy

Shima Adeli\({}^{1}\) &Mojtaba Tefagh\({}^{1,2}\)1 &Gourav Jhanwar\({}^{3}\) &Masoud Zarepisheh\({}^{3}\)

\({}^{1}\)Sharif University of Technology

\({}^{2}\)University of Edinburgh

\({}^{3}\)Memorial Sloan Kettering Cancer Center

###### Abstract

Radiation therapy, treating over half of all cancer patients, involves using specialized machines to direct high-energy beams at tumors, aiming to damage cancer cells while minimizing harm to nearby healthy tissues. Customizing the shape and intensity of radiation beams for each patient leads to solving large-scale constrained optimization problems that need to be solved within tight clinical time-frame. At the core of these challenges is a large matrix that is commonly sparsified for computational efficiency by neglecting small elements. Such a crude approximation can degrade the quality of treatment, potentially causing unnecessary radiation exposure to healthy tissues--this may lead to significant radiation-induced side effects--or delivering inadequate radiation to the tumor, which is crucial for effective tumor treatment. In this work, we demonstrate, for the first time, that randomized sketch tools can effectively sparsify this matrix without sacrificing treatment quality. We also develop a novel randomized sketch method with desirable theoretical guarantees that outperforms existing techniques in practical application. Beyond developing a novel randomized sketch method, this work emphasizes the potential of harnessing scientific computing tools, crucial in today's big data analysis, to tackle computationally intensive challenges in healthcare. The application of these tools could have a profound impact on the lives of numerous cancer patients. Code and sample data available at https://github.com/PortPy-Project/CompressRTP

## 1 Introduction

In 2020, an estimated 18.1 million new cancer cases and about 9.9 million cancer-related deaths were reported globally . Radiation therapy (RT) is integral to cancer treatment, utilized in approximately half of all cases, either alone or in combination with other treatments like surgery or chemotherapy . RT involves using specialized machines to direct high-energy radiation beams at tumors, with the primary goal of destroying cancer cells while minimizing damage to healthy tissues. This process requires precise optimization of machine parameters, such as beam shapes and angles, tailored to each patient's unique anatomy. It involves solving large-scale, constrained, non-linear optimization problems swiftly within clinical time constraints [22; 19]. The urgency of this task is heightened in modern online adaptive radiotherapy techniques, where rapid solution is essential since patients remain immobilized on the treatment couch during preparation . Delays not only compromise patient comfort but can also affect treatment outcomes, as any rapid anatomical changes (e.g., bladder filling in prostate cancer) can render treatment plans based on initial anatomy sub-optimal. Thus, quickly solving these optimization problems is crucial.

We briefly describe the mathematical modeling of radiotherapy treatment (see  for more details). The patient's body is discretized into small three-dimensional voxels (indexed by \(i=1,,m\)), and each radiation beam is discretized into small two-dimensional beamlets (indexed by \(j=1,,n\)). The radiation dose delivered to each voxel \(i\) from each beamlet \(j\) with unit intensity is precalculated and represented by \(a_{ij}\), forming a matrix \(A\) -- commonly referred to as the _dose influence matrix_. This matrix is typically large, containing about 100,000 to 500,000 rows corresponding to the patient's voxels, and 1,000 to 20,000 columns representing the beamlets of the radiotherapy machine . Our objective is to optimize the intensities of the beamlets, denoted by \(x\), in order to achieve a desired radiation dose, \(Ax\), that is delivered to the patient's body. For the tumor voxels, we aim to achieve a radiation dose that approximates the dose prescribed by a physician while for healthy voxels we aim to minimize the radiation dose as much as possible. The optimization problem can be described in the following general form:

\[&f_{0}(Ax)+f_{1}(x)\\ &g(Ax) 0\\ &h(x) 0,\] (1)

where, \(f_{0}(Ax)\) measures the quality and 'goodness' of the radiation dose \(Ax\), \(f_{1}(x)\) assesses the quality of the beamlet intensities. The functions \(g\) and \(h\) represent constraints on the dose received by the voxels and the intensities of the beamlets, respectively. Various formulations for these functions have been suggested in existing research ; however, the following quadratic optimization problem has arguably been the most commonly used formulation :

\[&_{s}_{i I_{s}}(w_{+} ^{s}(A_{i}^{s}x-d^{s},0)^{2}+w_{-}^{s}(d^{s}-A_{i}^{s}x,0)^{2})+||Px|| _{2}^{2}\\ &A_{i}^{s}x d_{Max}^{s}, s,i I_{s}\\ &Mean(A^{s}x) d_{Mean}^{s}, s\\ &x 0,\] (2)

where, \(\) represents the set of structures (i.e., organs, tumors), including tumor and healthy structures, and \(I_{s}\) represents the set of voxels belonging to structure \(s\). The first term in the objective function is a two-sided quadratic function penalizing the radiation overdose/underdose with the penalty weight \(w_{+}^{s}/w_{-}^{s}\), where the radiation overdose/underdose is defined as a delivered radiation dose, \(A^{s}x\), exceeding/below the prescribed dose, \(d^{s}\), for each structure \(s\). For tumor-affiliated structures, the prescribed dose is given by a radiation oncologist. For healthy structures, the prescribed dose is zero, \(d^{s}=0\), and there is no underdose penalty, \(w_{-}^{s}=0\). The second term in the objective function, \(f_{1}(x)=||Px||_{2}^{2}\), aims to penalize variations in intensities across neighboring beamlets to promote smoothness in beamlet intensities for enhanced radiation delivery (each row of matrix \(P\) has a value of 1 and -1 for two neighboring beamlets). The first/second set of constraints impose maximum/mean dose constraints to satisfy the maximum/mean dose limits defined by a clinical protocol for each structure \(s\), and the last constraint is a physical non-negativity constraint on the beamlet intensities. The overdose/underdose penalty weights \(w_{+}^{s}/w_{-}^{s}\) need to be adjusted for each patient and various techniques have been developed to automate this process (see  and references therein).

We will use the second formulation (Eq. 2) for our experiments in this study. However, regardless of the specific functions chosen in Problem 1 and the technique used to adjust the problem hyperparameters, the size and structure of matrix \(A\) are crucial in determining the computational intensity of the problem. Considering that these large-scale, non-linear, constrained optimization problems are often solved using the interior-point method with cubic computational complexity, the computational time can increase significantly with the size and density of non-zero elements in matrix \(A\). Therefore, matrix sketching could be a compelling choice to improve the computational complexity of these problems. There has been a body of research employing matrix sketching, often using a transformation matrix \(S\), resulting in \(SA\) with a reduced number of rows, to improve the computational efficiency of the least-squares optimization problems commonly arising in machine learning applications . However, these sketching techniques cannot be used where matrix \(A\) is also involved in the constraints, as is the case in radiotherapy applications. Reducing the number of rows would prevent direct access to \(Ax\), which is crucial for evaluating \(g(Ax) 0\) in Problem 1. Thus, we explore the potential of using matrix sparsification, a specific form of matrix sketching, to substitute the dose influence matrix \(A\) with a sparse matrix \(S\) to improve the computational efficiency of our radiotherapy constrained optimization problems. In fact, a very simple form of matrix sparsification is currently being used in practice, where all small elements of the matrix below a predefined threshold, typically less than \(0.01(A)\), are discarded . This method, which we will refer to as the "naive" approach, is clearly not the most optimal solution and may adversely affect the quality of the treatment. The primary concern with this approach is that the radiation dose calculated and optimized in Problem 1 using the modified matrix \(A\) may not accurately reflect the actual dose received by the patient. This discrepancy arises from the inherent inaccuracies introduced by the truncation of matrix \(A\), which could potentially lead to sub-optimal treatment outcomes.

In this study, we demonstrate that using matrix sparsification techniques, primarily developed by the machine learning community, we can enjoy the computational efficiency of working with sparse matrices and still being able to solve constrained optimization problems within the clinical timeframe, without significantly compromising the integrity of the original problem that could potentially degrade the treatment quality. Matrix sparsification techniques carefully sample and scale the elements of the original dense matrix \(A\) to create a sparse sketch matrix \(S\) that minimizes \(||A-S||_{2}\). Prior research predominantly utilized matrix sparsification for applications such as low-rank approximation and principal component analysis (PCA) [1; 2]. This study uniquely demonstrates the utility of matrix sparsification for efficiently addressing large-scale, constrained, nonlinear optimization challenges within constrained timeframes. We demonstrate that applying a randomized sketch to the influence matrix \(A\) in radiotherapy optimization significantly outperforms the current naive sparsification approach. To the best of our knowledge, this is the first application of matrix sparsification with a publicly available benchmark dataset, encouraging further research in this direction. Furthermore, we have developed a novel randomized sketching algorithm that exhibits superior performance compared to existing techniques and is supported by theoretical guarantees for its efficacy. As formally stated in Theorem 3.6, Lemma 3.7, and Theorem 3.9, the proposed algorithm ensures a minimal impact on the constraints, objective function and the optimal points of the original optimization Problem 2.

## 2 Related Work

In matrix sparsification, we typically aim for an unbiased approximation of a matrix \(A\) by another matrix \(S\) such that \(\|A-S\|_{2}\) for a given \(>0\), while minimizing the number of nonzero entries in \(S\)[1; 2; 3; 7; 8; 15]. The \(_{2}\) norm of the difference between \(A\) and \(S\) serves as a measure of this error, a choice that has been justified in the literature  for various applications. The sparsification techniques proposed in the literature fall into two categories. The first involves randomly scaling each matrix entry independently. Specifically, for an entry \(a_{ij}\) it is scaled to \(a_{ij}/p_{ij}\) with probability \(p_{ij}\), or is set to zero otherwise [1; 3; 8; 15]. This process increases the magnitude of certain matrix entries and zeros out others, resulting in a sparse matrix. Importantly, the resulting matrix serves as an unbiased estimator of the original matrix, with its entries acting as independent random variables. The probability typically follows the formula \(p_{ij}=cf(a_{ij})\), where \(c>0\) is a universal constant. This method, first introduced by Achlioptas and McSherry , involves scaling each entry \(a_{ij}\) to \(a_{ij}/p_{ij}\) with probability \(p_{ij}=ca_{ij}^{2}\); otherwise, the entry is set to zero. This technique, referred to as \(_{2}\) scaling, laid a foundational basis in the domain of matrix sparsification. However, Achlioptas and McSherry 's approach faced limitations in theoretical guarantees and required a significant number of non-zero entries to achieve a satisfactory \(_{2}\) norm bound. Additionally, scaling small entries could disproportionately inflate values in the resulting matrix. Building upon this, Arora et al.  introduced a variation focused on deterministically retaining the largest entries in the matrix while randomly scaling the smaller ones. During the scaling phase, each matrix entry is scaled to \(a_{ij}/p\) with a probability \(p=c|a_{ij}|\), or set to zero otherwise. This method is fast in practice as it requires only a single pass over all the non-zero entries and Arora et al.  demonstrated that their method outperforms Achlioptas and McSherry  approach. However, this approach substitutes all small entries, which are not reduced to zero, with a constant (\(c\) or \(-c\)). This could significantly affect its performance, especially when the matrix undergoes extensive sparsification.

The second category of sparsification techniques, introduced later, involves independently sampling from the entries of \(A\) using a probability distribution \(p\). Each sample generates a matrix filled with zeroes, except for the sampled entry. Subsequently, \(s\) samples are collected, and their average forms a sparse approximation. Contrary to the first approach, the entries of the resulting matrix \(S\) are not independent. Instead, \(S\) is formed by summing independent random matrices. To ensure that the resulting matrix is an unbiased estimator of \(A\), the sampled matrix's entry should be \(a_{ij}/p_{ij}\), where \(p_{ij}\) is derived from the probability distribution \(p\). In this context, the choice of probability distribution \(p\) is critical. Drineas and Zouzias  introduced a technique where \(p_{ij}\) is proportional to \(a_{ij}^{2}\), termed \(_{2}\) sampling, and used Bernstein inequality  to calculate the number of samples needed to achieve a desired accuracy. Later, Achlioptas et al.  introduced a near-optimal probability distribution under specific conditions. Additionally, Braverman et al.  developed a near-optimal method specifically designed for numerically sparse matrices (this method belongs to the first category). While the theoretical bounds of this method closely align with those of Achlioptas et al. , the approach by Braverman et al.  is applicable to all matrices.

The primary issue with the second category of techniques is the increase in the required number of non-zero elements in the sparse matrix, which grows in proportion to the inverse square of the error, \(1/^{2}\), unlike the \(1/\) growth rate seen in the first category. This is especially problematic given that the interior point methods typically used to solve Problem 1 have cubic computational complexity, which results in a cubic increase in processing time with the size of the input data. However, the second category may be preferable in applications when data is only accessible in a streaming manner. The first category necessitates access to the complete dataset. In this study, we assume that the matrix \(A\) is precomputed--a time-intensive process that often takes as long as solving the optimization problems themselves. Theoretically, these matrices could be computed on a row-by-row basis , allowing the sparsification algorithm to operate concurrently, provided it can handle streaming data. Our proposed hybrid approach can, in principle, be used in a streaming manner .

Notations.Let \(a_{ij}\) denote the element in the \(i\)-the row and \(j\)-th column of matrix \(A\), and let \((A)\) denote the number of non-zero entries in \(A\). For matrix \(A\), we consider the entry-wise \(_{1}\) norm, defined as \(\|A\|_{1}=_{i=1}^{m}_{j=1}^{n}|a_{ij}|\), and the spectral norm defined as \(\|A\|_{2}=_{\|x\|_{2}=1}\|Ax\|_{2}\). Additionally, we introduce \(A_{(i)}^{m n}\), a matrix with all zero entries except for the \(i\)-th row, which remains identical to the \(i\)-th row of \(A\). Finally, for a vector \(a\), we define its numerical sparsity as \((a)=\{k 0:\|a\|_{1}\|a \|_{2}\}\) and let \((A)\) denote the maximum numerical sparsity of its rows and columns.

## 3 Algorithm Description

In this study, we introduce an algorithm that matches the theoretical requirements of the first category for the number of non-zero elements (i.e., the growth of the non-zero elements proportional to \(1/\)), while outperforming existing techniques in practical experiments. Our findings reveal that this approach markedly enhances the accuracy of sparse sketches for large matrices that appear in the context of cancer radiotherapy optimization. This method can be seen as a hybrid, combining advantages from both categories. We deterministically retain the largest entries, akin to the strategy suggested by Arora et al. . This method is particularly relevant for our application, as the matrices appearing in radiotherapy exhibit a distribution closely resembling an exponential curve. Consequently, the number of elements exceeding any given threshold remains significantly small relative to the total matrix size. This phenomenon occurs because radiation delivered from each individual beamlet (corresponding to a matrix column) directly deposits radiation to a limited number of voxels (corresponding to matrix rows), resulting in a few large matrix entries. However, radiation also scatters, delivering smaller doses to additional voxels throughout the body, leading to small values across all voxels. In contrast to the approach by Arora et al. , which involves substituting all minor non-zeroed-out entries with \( c\), our strategy seeks to counterbalance the effects of sparsification on a row-by-row basis by redistributing the sum of all minor elements within that row. This method is designed to preserve the integrity of each row, taking into consideration the diverse distribution patterns across the matrix's rows. Specifically, in our application, rows corresponding to tumor voxels often exhibit higher values due to their position in the direct path ("cross-fire") of radiation, receiving more substantial doses. Applying a uniform value for all substituted entities, in this context, results in a more pronounced approximation for tumor voxels, potentially compromising the accuracy of the final dose delivered to the tumor, as our results will demonstrate. After isolating the larger elements, we apply \(_{1}\) sampling to the remaining entries in each row independently. This process generates a random matrix, essentially the aggregate of several random matrices, which more closely aligns with the practices of the second category. However, unlike conventional sampling methods (e.g., ), our technique automatically identifies the optimal number of samples for each row, consistently less than the total number of columns, eliminating the necessity for tuning sampling parameters.

Our method is nearly as fast as that proposed by Arora et al.  but provides superior accuracy, particularly beneficial when high levels of sparsification needed. This is crucial in our application, where approximately 96-98% sparsification is required to solve the large-scale non-linear constraint optimization problems within a feasible clinical timeframe. Our algorithm, described in _Algorithm 1_, is named the Randomized Minor-value Rectification (RMR), as it preserves the larger values while rectifying a random selection of smaller values to offset those that have been zeroed out.

``` input\(A^{m n}\): The matrix to be sparsified, \(\): The threshold for sparsification output\(S\): The sparsified matrix \(S=A\) for each row \(i\) in \(\{1,2,,m\}\)do \(T_{i}=\{j 0<a_{ij}|\}\)  set \(s_{ij}=0\) for all \(j T_{i}\) \(_{i}=_{j T_{i}}|a_{ij}|\) \(k_{i}=_{i}/\) for\(t=1,2,,k_{i}\)do  randomly select \(j T_{i}\) (with probability proportional to \(|a_{ij}|\)) and update \(s_{ij} s_{ij}+_{i}/k_{i}(a_{ij})\) endfor endfor ```

**Algorithm 1** Randomized Minor-value Rectification (RMR)

**Lemma 3.1**.: _For any matrix \(A^{m n}\) the number of samples taken by the RMR algorithm is bounded above by \((A)\)._

The \((A)\) upper bound in the above lemma, which also applies to the algorithm proposed by Arora et al. , may not seem interesting at first; however, the number of samples could be much larger than \((A)\) for other algorithms, as each entry of the matrix could be sampled multiple times. The following theorems provide the theoretical guarantees of the RMR algorithm (proof in Appendix A).

In the remainder of this section, we assume that the sparse matrix \(S\) is obtained by applying Algorithm 1 to \(A\). Instead of solving the original optimization problem 2 directly, we substitute the dense matrix \(A\) with the sparse matrix \(S\), thereby formulating an approximated optimization problem that we refer to as the _surrogate problem_. Let \(x_{A}\) and \(x_{S}\) denote arbitrary optimal points for the original and surrogate optimization problems, respectively.

**Theorem 3.2** (Absolute \(_{2}\)-norm error).: _Given a matrix \(A^{m n}\), it can be shown that:_

1. _The number of nonzero entries of_ \(S\) _is less than_ \(m+\|A\|_{1}/\)_._
2. _For any given_ \(,>0\)_, by setting_ \[= }}-1})}{3(m,n-1)},\] _it follows that_ \(\{\|A-S\|_{2}\}\)_._

**Corollary 3.3** (Absolute \(_{2}\)-norm error).: _In Algorithm 1, by setting_

\[=}\]

_the resultant matrix \(S\) will contain no more than \(m+4\|A\|_{1}/\) nonzero entries, and we have \(\{\|A-S\|_{2}\} 1/(m+n)\)._

Theorems 3.6 and 3.9 establish theoretical guarantees and provide bounds on the discrepancies between the original and surrogate optimization problems with respect to their constraints and objective functions, respectively. In essence, applying the RMR algorithm to sparsify the matrix and subsequently solving the surrogate problem yields a near-optimal solution to the original problem.

**Lemma 3.4** (Row-wise error).: _For every vector \(x^{n}\), the probability that the absolute value of the \(i\)-th entry of the vector \((A-S)x\) exceeds \(c\|x\|_{2}\) is less than \(2(-c^{2}/(4+2c/3))\), i.e., \(\{|((A-S)x)_{i}| c\|x\|_{2} \} 2(-c^{2}/(4+2c/3))\)._Note that the upper bound obtained from Lemma (3.4) is significantly tighter than the bound derived directly from Corollary (3.3). To understand this, consider the following inequality:

\[|((A-S)x)_{i}|\|(A-S)x\|_{2}\|A-S\|_{2}\|x\|_{2} 4\|x\|_{2}\]

with probability at least \(1-1/(m+n)\), as stated in (3.3). However, if we set \(c=5(m+n)\), we achieve the same failure probability, but the upper bound on the error becomes \(5(m+n)\|x\|_{2}\), which is of a better order.

**Lemma 3.5** (Feasibility gap).: _Let \(C^{k m}\) be a normalized matrix where the \(l_{1}\)-norm of each row is less than or equal to one, and assume that for an arbitrary \(x^{n}\), there exists \(u^{k}\) for which we have \(CSx u\). Then, with a probability of 0.95, the maximum violation of any constraint in \(CAx u\) is bounded above by \((19+5 m)\|x\|_{2}\). Conversely, if \(CAx u\), then the maximum violation of any constraint in \(CSx u\) is bounded above by \((19+5 m)\|x\|_{2}\) with a probability of 0.95._

The feasible region of the optimization problem (2) satisfies the assumptions of Lemma 3.5, leading to the following theorem.

**Theorem 3.6** (Feasibility gap for Problem 2).: _An optimal point of the original problem, \(x_{A}\), violates each constraint of the surrogate problem by no more than \((19+5 m)\|x\|_{2}\) with a probability of at least 0.95. Conversely, an optimal point of the surrogate problem, \(x_{S}\), violates each constraint of the original problem by no more than \((19+5 m)\|x\|_{2}\) with the same probability._

**Lemma 3.7** (Objective function discrepancy).: _The absolute discrepancy between the objective functions of the original and surrogate problems does not exceed \(e(f_{0}(Ax)+m(1+e)_{s}(w_{+}^{s}+w_{-}^{s}))\) with a probability of at least 0.95, where \(e=(19+5 m)\|x\|_{2}\)._

**Lemma 3.8**.: _Suppose that for an \(x^{n}\) satisfying the convex constraints \(h(x) 0\), the maximum violation of any convex constraint \(g_{i} 0\) in the optimization problem (1) is bounded above by \(e 0\), i.e., \(g_{i}(Ax) e\). Additionally, assume there exists an interior point of the feasible set \(^{n}\) that is strictly feasible by a margin of at least \(s>0\) for each constraint \(g_{i} 0\), i.e., \(g_{i}(A)-s\). Then, there exists a feasible point \(^{n}\) such that:_

\[\|x-\|(\|x\|+\|\|)\]

**Theorem 3.9** (Sub-optimality gap for Problem 2).: _An optimal point of the surrogate problem, \(x_{S}\), is a near-optimal solution to the original problem with a probability of at least 0.95, and the sub-optimality gap of \(O(e)\), where_

\[e=(19+5 m)\,\,(\|x_{A}\|_{2},\|x_{S}\|_{2}).\]

_In other words, we have:_

\[[f_{0}(Ax_{S})+f_{1}(x_{S})]-[f_{0}(Ax_{A})+f_{1}(x_{A})]=O(e).\]

_Remark 3.10_.: Note that the proof of Theorem (3.9) can be generalized to any matrix approximation scheme with a bounded error norm for the general convex optimization problem (1), given the key assumptions that the objective function is Lipschitz continuous and that there exists a strictly feasible point with a lower bounded slackness for all approximated constraints. These assumptions are crucial. If the objective function is not Lipschitz continuous, the error in matrix approximation can significantly alter the value of the objective function, thereby drastically affecting the optimal point. Additionally, if the assumption of Lemma (3.8) is not satisfied and no interior point exists for the approximated constraints, the feasible set could be reduced to a single point in extreme cases. Even with an arbitrarily small error in approximating the constraints, the feasible set might vanish, rendering the approximated problem infeasible.

Now we conduct a comparative analysis of the performance of the RMR algorithm against the algorithms proposed by Arora et al. , Drineas and Zouzias , Achlioptas et al. , and Braverman et al.  denoted as "AHK06", "DZ11", "AKL13", and "BKKS21" respectively. Table 1 presents the sparsity of the resulting matrix \(S\) for each considered method, adhering to the constraint \(\|A-S\|_{2}\). The computational time required to solve the constrained optimization Problem 2 is strongly correlated with the sparsity of the matrix, as also verified by our computational experiments (see Figure 1). Therefore, we use sparsity as a proxy for computational efficiency. Although the valuesin Table 1 are not directly comparable, for DZ11, AKL13, and BKKS21 the number of non-zero elements scales inversely with the square of the error rate (i.e., \(}\)), while for RMR and AHK06, it is determined by the inverse of the error rate (i.e., \(\)). As also confirmed by our computational experiments (Figure 1), this makes RMR and AHK06 algorithms particularly beneficial at lower error rates, which is needed for our application. Furthermore, due to the dependence of the BKKS21 on the numerical sparsity of the matrix, and considering that the matrices we tested exhibit low numerical sparsity, this algorithm also performs well across various metrics. In terms of the runtime and computational complexity of the algorithms themselves, it mainly depends on the number of required samplings. For DZ11 and AKL13, the number of required samplings also depends on the error rate, while for AHK06, BKKS21, and RMR, the dependency is solely on \((A)\). This also gives AHK06, BKKS21, and RMR an edge in runtime, particularly for small error rates, as confirmed by our experiments in Figure 1.

## 4 Experiments

Our objective is to illustrate that employing a randomized sparse sketch of the large _dose influence matrix_ in radiotherapy markedly surpasses the existing "naive" method used in practice, which simply sparsifies the matrix by neglecting all minor elements. Additionally, we demonstrate the superior performance of our RMR algorithm over the randomized sketch techniques introduced by Arora et al.  ("AHK06"), Drineas and Zouzias  ("DZ11"), Achlioptas et al.  ("AKL13") and Braverman et al.  ("BKKS21").

Dataset.Our analysis utilized real-world data recently made publicly available through the open-source package PortPy . We conducted experiments on data from 10 randomly selected lung patients, with detailed information provided in the Appendix (Table 2). The dose influence matrices in PortPy were derived from an FDA-approved commercial treatment planning system, Varian Eclipse(tm), using its Application Programming Interface (API). Further detailed information about the data can be found on PortPy's GitHub page. Furthermore, we conducted experiments using data from five prostate patients, which are not publicly available. The results of these experiments are included in the Appendix.

Experiment Settings.Due to the inherent randomness of all the algorithms, except the naive one, each experiment was repeated 5 times to assess and report variations and the algorithms' robustness. Each algorithm has a hyper-parameter threshold that determines the sparsity of the output matrix. To ensure a fair comparison, we ran each algorithm with various threshold values and compared the results based on different levels of sparsity in the output matrix (e.g., runtime of algorithm 1 vs. algorithm 2 for sparsity levels x, y). The experiments were conducted on a dual CPU system (Intel(R) Xeon(R) 6248 2.5GHz) running Windows 10 with 128 GB of RAM. For each patient, the optimization problem 2 was solved using the penalty weights, \(w_{+}^{s}/w_{-}^{s}\), recommended by the PortPy package , along with the maximum and mean dose constraints presented in Table 3 in Appendix. All optimization problems were modeled and solved using CVXPY  and MOSEK . To report the optimality gap, we needed to solve the original optimization problems using the original matrix \(A\). However, we encountered limited memory errors on the PC. Consequently, we solved the original optimization problems on a powerful high-performance computing (HPC) system with approximately

   Method & Number of non-zero elements & Failure Probability \\  AHK06 & \(O(\|A\|_{1}/)\) & \((-(m+n))\) \\  DZ11 & \(28(m+n)((m+n))\|A\|_{F}^{2}/^{2}\) & \(1/(m+n)\) \\  AKL13 & \(O((n)_{i=1}^{m}\|A_{(i)}\|_{1}^{2}/^{2}+\|A\|_{1}/)\) & \(1/10\) \\  BKKS21 & Not specified \\  RMR & \(m+4\|A\|_{1}/\) & \(1/(m+n)\) \\   

Table 1: Comparison of Theoretical Guarantees Across Various Algorithms.

[MISSING_PAGE_FAIL:8]

of the RMR algorithm should not be seen as a significant limitation. The primary computational challenge lies in solving the optimization problem 2, which often needs to be solved multiple times for hyper-parameter tuning. For example, to achieve a relative optimality gap of 35% for patient 1, the RMR algorithm necessitates a sparse matrix with 98.90% relative sparsification (approximately 2,850,000 non-zero elements), in contrast to the 95.05% sparsification (about 12,750,000 non-zero elements) required by AHK06. Consequently, solving the optimization problem 2 with RMR's sparse sketch is considerably faster, taking 57 seconds, compared to 200 seconds for the sparse sketch produced by AHK06. The bottom-right plot illustrates the strong correlation between the number of non-zero elements in the matrix and the computational time of the constrained optimization problem, indicating that relative sparsification serves as an excellent proxy for the computational time of the optimization problems.

Figure 2 offers a high-level comparison across all patients at a fixed relative sparsity level of 98%, which can also be interpreted as a fixed computational time for the constrained optimization problem. Confirming the results of Figure 1 for more patients, this figure demonstrates the superior performance of AHK06 in terms of the \(_{2}\) norm error and algorithm runtime, while highlighting the significant advantage of the RMR algorithm in reducing optimality and feasibility gaps.

Figure 1: _(One patient, various sparsification levels)_ The performance of different algorithms in terms of: relative \(_{2}\)-norm error, relative optimality gap, feasibility gap, relative dose discrepancy, algorithm runtime, and optimization runtime, for a lung patient.

Figure 2: _(Ten patients, one sparsification level)_ The performance of different algorithms in terms of: relative \(_{2}\)-norm error, relative optimality gap, feasibility gap, and algorithm runtime, for ten lung patients.

Figure 3 presents a qualitative comparison between treatment plans generated using the naive approach (left) and RMR (middle), based on the radiation dose map commonly used by clinicians. The dose map visually represents the distribution of radiation, with color-coding overlaid on a medical image. Ideally, the high-dose regions (shown in red) should conform closely to the tumor's shape, with minimal radiation spillover into surrounding healthy tissues. While clinical expertise is required for a detailed interpretation, it is clear that using the RMR sparse matrix, as opposed to the naive sparse matrix, results in reduced radiation exposure to the right lung (visible on the left side of the figure).

### Limitations and Broader Impacts

We acknowledge that this study primarily focused on leveraging advanced matrix sparsification techniques to accurately and efficiently solve the computationally intensive optimization problems in cancer radiotherapy. While we have presented dose maps and DVH figures to illustrate potential improvements in treatment plan quality, our analysis remains limited. Future studies are necessary to comprehensively evaluate the clinical benefits of our techniques. Additionally, this study was reviewed by machine learning experts who focused on the technical aspects rather than the clinical implications of the study. Another limitation of this study is that the theoretical bounds for feasibility and optimality gaps are provided for the quadratic optimization problem 2, not the general problem 1. It can be proven that, by assuming Lipschitz continuity for the functions \(f_{0}\) and \(g\), a small \(||A-S||_{2}\) ensures small feasibility and optimality gaps. However, the bounds will depend on the Lipschitz constants of \(f_{0}\) and \(g\).

Although our work primarily focused on Intensity Modulated Radiation Therapy (IMRT) with photon radiation, a widely used treatment modality, our approach could have a much broader impact in the field of radiotherapy. This is because all treatment modalities eventually boil down to solving constrained optimization problems with large and dense matrices. As technology advances and new digital machines with greater flexibility, such as those allowing couch movement, become available, the resultant optimization problems are becoming larger, making efficient approximation techniques increasingly essential to solve these problems within clinical timeframes [9; 11].

## 5 Conclusion

We introduced a novel and high-impact application of matrix sparsification alongside an innovative algorithm that combines desirable theoretical guarantees with superior experimental performance. Our algorithm creates a highly sparse, randomized sketch of the original dose influence matrix used in radiotherapy optimization, preserving essential information. This enables solving the large-scale radiotherapy optimization problems within clinically viable timeframes, ensuring minimal discrepancy between the optimized radiation dose and the actual dose received by patients. It effectively tackles the "garbage-in-garbage-out" problem prevalent in current radiotherapy optimization caused by reliance on inaccurately sparsified matrices from the current "naive" approaches. This method holds significant promise for enhancing the quality of radiotherapy treatments.

Figure 3: Radiation dose maps: Naive (left), RMR (middle) and dose labels (right)