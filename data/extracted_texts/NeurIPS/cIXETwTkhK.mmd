# Training Binary Neural Networks

via Gaussian Variational Inference

and Low-Rank Semidefinite Programming

 Lorenzo Orecchia

University of Chicago

&Jiawei Hu

Georgia Institute of Technology

&Xue He

Northeastern University of China

&Zhe Wang

I\({}^{2}\)R, A*STAR

&Xulei Yang

I\({}^{2}\)R, A*STAR

&Min Wu\({}^{*}\)

I\({}^{2}\)R, A*STAR

&Xue Geng

I\({}^{2}\)R, A*STAR

Corresponding authors

###### Abstract

Improving the training of Binarized Neural Networks (BNNs) is a longstanding challenge whose outcome can significantly affect our ability to deploy deep learning ubiquitously. Current methods heavily rely on latent weights and the heuristic _straight-through estimator_ (STE), which enable the application of SGD-based optimizers to the combinatorial training problem, but remain theoretically poorly understood. In this paper, we propose an optimization framework for BNN training based on Gaussian variational inference. Our approach yields a non-convex linear programming formulation that _theoretically motivates the use of latent weights, STE and weight clipping_. More importantly, it allows us to _go beyond latent weights_ to formulate and solve low-rank semidefinite programming (SDP) relaxations that explicitly _model and learn pairwise correlations between weights during training_, resulting in improved accuracy. Our empirical evaluation on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet datasets shows our method consistently outperforms all state-of-the-art algorithms for training BNNs.

## 1 Introduction

The advent of deep learning has revolutionized the field of machine learning and enabled stunning technological advances in numerous application areas, including computer vision , speech recognition  and natural language processing . Despite these achievements, the broader application of deep learning is impeded by high computational demands, requiring the advanced hardware and energy consumption typically reserved for supercomputers , both for training and inference. This barrier is particularly formidable when deep learning is deployed on resource-constrained devices, like smartphones and IoT devices, where limitations in memory, processing power and energy are critical .

To address these issues, Muller and Indiveri  and Courbariaux et al.  noted that neural networks could provide the same level of performance while restricting the precision of the representation of parameters to a small number of bits, effectively quantizing the space of weights. To reap the full benefits of quantization, Courbariaux et al.  and Kim and Smaragdis  independently introduced _binarized neural networks_ (BNNs), which use 1-bit representations for each weight, directly leading to a 32-fold reduction in model size compared to single-precision weights. When activation is further binarized , the multiplication and addition operations can be replaced by much faster and cheaper XNOR and popcount operations, resulting insignificant cost reduction in both memory and computation. Despite their compelling computational advantages, the problem of constructing and training high-performing BNNs is still open, as current approaches still yield severe accuracy loss compared to their high-precision counterparts (Rastegari et al., 2016; Liu et al., 2018).

BNN trainingWhile substantial effort has been aimed at constructing larger and more effective BNN architectures (Umuroglu et al., 2017; Tang et al., 2017; Liu et al., 2020; Martinez et al., 2020; Shen et al., 2020), training BNNs has remained a significant challenge, as the binary constraints yield an intrinsically combinatorial optimization problem, which is a poor fit for traditional continuous optimizers like SGD and Adam. For a domain set \(\) and a label set \(\), let \(f:^{n}\) represent a neural network with an \(n\)-dimensional weight vector. Denote by \(y_{}\) the true label of instance \(\) and by \(L\) the smooth loss function. The BNN training problem can then be formulated as the following stochastic optimization problem over the hypercube of binary weights \(}\):

\[_{}\{ 1\}^{n}}_{}[L(f(,} ),y_{})],\] (1)

where \(_{}\) denotes the expectation over \(\) uniformly distributed over \(\). This computational problem adds to the challenge of non-convexity, the additional obstacle of a combinatorial feasible set, making continuous gradient queries potentially uninformative. Moreover, as the training must be carried out over a BNN architecture, we face the _additional restriction that the gradient \(_{}}L(f(,}),y_{})\) of the loss can only be evaluated at binary weights \(}\{ 1\}^{n}\)_. This further limits our capability to explore the loss landscape.

Latent WeightsCurrently, most BNN-training procedures make use of latent real weights \(^{n}\)(Courbariaux et al., 2015, 2016) to maintain the state of an iterative training algorithm and guide the optimization process. Latent weights are rounded to binary weights \(}=()\) via a potentially stochastic function \(:^{n}\{ 1\}^{n}\) to compute forward and backward passes over the BNN architecture. The most common choice of **round** function is simply the deterministic **sign** function applied to each latent weight. Unfortunately, any non-trivial **round** function is discontinuous and cannot be differentiable over \(^{n}\), so that it is not possible to evaluate the true gradient of the loss function with respect to the latent weights \(\). The main practical solution to this problem has been to _simply ignore the **round** function in the back-propagation of the gradient_, yielding the _straight-through estimator_ (STE) (Bengio et al., 2013; Le et al., 2022):

\[_{}L(f(,()),y_{})_{ }}L(f(,}),y_{})|_{}=()}\] (STE)

While there is no theoretical assurance that the STE is a valid proxy for the gradient (Yin et al., 2019), the STE and its variants (Le et al., 2022; Wu et al., 2023) have proved remarkably effective in practice, particularly in combination with _weight clipping_(Alizadeh et al., 2018; Merolla et al., 2016), by which latent weights are constrained to a fixed range around the origin.

Theoretical Interpretations of Latent-Weights MethodsLatent weights have usually been interpreted as fractional approximations of the true binary weights (Anderson and Berg, 2017). The influential work of Helwegen et al. (2019) instead proposes to view latent weights as a measurement of the algorithm's confidence in the binary weight taking on a certain sign. With this intuition, Ajanthan et al. (2019) and Meng et al. (2020) have suggested a more formal interpretation of latent weights based on the mean-field approximation from variational inference (Wainwright et al., 2008). In this setup, which we review in Section 3, the latent weights are the mean parameters of an exponential family of probability distributions over binary weights. However, none of these works are able to fully justify the use of the STE and weight clipping in latent-weights methods.

Our contributionsIn this paper, we provide a general optimization framework for BNN training based on Gaussian variational inference, a refinement of the mean-field approximation used by Ajanthan et al. (2019) and Meng et al. (2020). _Our framework allows us to generalize the notion of latent weights as mean parameters in order to introduce new variables modeling the covariances of the weights_. As a result, our optimization formulation is able to capture more intricate dependencies among weights and exploit them to learn better solutions during the training phase. We believe the mere statement of our formulation in Section 3.1 to be a significant contribution of our work, which will hopefully lead to further study of Gaussian variational methods for neural network training.

In Section 3.2, we show that the general case of our framework, i.e., the problem of learning an optimal Gaussian distribution over weights in \(^{n}\), can be cast as a non-convex semi-definite program (SDP) over the weights mean vector \(^{n}\) and covariance matrix \(^{n n}\). Because of the large number of weights \(n\) in typical applications, we do not attempt to maintain full-rank covariance matrices, but only consider low-rank approximate SDP solutions, following the approach championed by Burer and Monteiro (2005) for linear SDP programs. _The resulting algorithm (Algorithm 1) is the main contribution of this paper._ We name our method the Variational Inference Semidefinite Programming Algorithm (VISPA).

In Section 3.3, we demonstrate that a simpler instantiation of our framework yields a non-convex program, whose solution by gradient descent _naturally recovers the use of the STE and weight clipping_ in latent-weights methods. Finally, in Section 4, we present a thorough experimental evaluation of VISPA against a large number of BNN training procedures in the literature over four standard benchmark datasets. We find that VISPA _almost always improves the state-of-the-art accuracy of BNN training_, in some cases dramatically. For instance, Top-1 accuracy with AlexNet on ImageNet with fully binarized weights and activations increases by more than \(3\%\) compared to the state-of-the-art method (see Table 3). Through an ablation study, we also show that the SDP component of our algorithm is crucial in realizing the observed empirical advantage. Code for our algorithm and experimental evaluation can be found at https://github.com/snownus/bnn_vi.

Limitations and Open ProblemsWe focus our first presentation of VISPA on vision-based applications because of the availability of well-studied binarized architectures and well-established baselines, which isolate the performance of our method more closely. Indeed, in the case of transformers, there is not yet agreement on the best binarized architecture, due to the difficulty of binarizing activations in softmax layers. Only recently, researchers have made progress in bypassing this obstacle He et al. (2023). This also contributes to the scarcity of baselines and the absence of a standard benchmark. In Section 5, we discuss other limitations of the current work and opportunities for future extensions.

## 2 Related Work

Courbariaux et al. (2015) introduced the use of the STE for training BNNs. Since then, researchers have put forward many variants to this idea, such as adaptive versions of the STE (Le et al., 2022; Wu et al., 2023; Qin et al., 2023), and a number of extensions, e.g., to non-binary quantization (Huh et al., 2023; Liu et al., 2024) and sparsity-driven network designs (Vanderschueren and De Vleeschouwer, 2023). All of these variations are in principle applicable within our optimization framework.

A different line of work investigates alternatives to the STE, with two approaches standing out. Modifications to the gradient estimator include using piecewise polynomial functions (BiRealnet (Liu et al., 2018)) and dynamic gradient estimators (IR-Net (Qin et al., 2020), RBNN (Lin et al., 2020)). The other approach designs separate frameworks for discrete back-propagation (PCNN (Gu et al., 2019), BiPer (Vargas et al., 2024), ReCU (Xu et al., 2021), ReActNet (Liu et al., 2020)). Among this latter class, the aforementioned work of Helwegen et al. (2019), followed by several variants (Suarez-Ramirez et al., 2021; Shan et al., 2023), proposes a novel Binary Optimizer (Bop) that maintains a binary solution and accumulates gradients to determine when to flip a bit. The only BNN-training method based on variational inference, BayesBiNN by Meng et al. (2020), effectively combines the STE and Bop in a principled way. The work of Ajanthan et al. (2019), which is also based on variational inference, only deals with network quantization and does not perform training over a BNN architecture. Of particular relevance to this paper is the LNS algorithm of Han et al. (2020), who also notice that simply binarizing each latent weight independently does not fully explore the relationship between neurons and may not lead to the optimal solution. They propose to train a custom binarization function via supervision noise learning, but do not explicitly model correlations between weights via new variables. Our experimental evaluation compares our algorithm with all the methods just described, showing the superior accuracy of our technique in practice.

Since the seminal work of Goemans and Williamson (1995), semidefinite programming (Vandenberghe and Boyd, 1996) has become a fundamental tool in the design of approximation algorithms for combinatorial optimization problems. Recently, its application to network quantization has been studied by Bartan and Pilanci (2021). They construct a tight SDP relaxation for training a two-layer quantized neural network. Crucially, their method does not run the training on the quantized architecture, i.e., network gradients are evaluated at non-quantized weight settings, and are limited to small-scale, shallow networks. However, we believe their idea provides valuable theoretical evidence in favor of the deployment of SDP techniques at a larger scale, as is done in our work.

## 3 Variational Inference Approach to BNN Training

We start by quickly reviewing the variational inference approach to BNN training before introducing our novel contribution in Section 3.1. A common approach to the construction of approximation algorithms for intractable combinatorial optimization tasks (Vazirani, 2010; Barak and Steurer, 2024) is to consider relaxed, regularized formulations over a subset \(\) of the simplex \(_{n}=\{:\{ 1\}^{n}_{ 0},_{}\{ 1 \}^{n}}_{}}=1\}\) of probability distributions over the hypercube \(\{ 1\}^{n}\):

\[_{}=_{}_{ },}[L(f(,}),y_{})]- H(),\] (2)

where \(}\) indicates that the random variable \(}\) is distributed according to the distribution \(\) and \(H\) denotes the entropy function. When the regularization parameter \( 0\) is strictly positive, the regularization term \(- H\) is known to encourage generalization. When \(\) equals the set of all probability distributions over \(\{ 1\}^{n}\), this relaxation renders the loss term linear in the distribution \(\), but requires an exponential-size representation, hence maintaining the computational hardness of the problem. However, this formulation allows us to reason more directly about stochastic approaches, such as Monte Carlo Markov Chain (Gamerman and Lopes, 2006) and variational inference methods (Wainwright et al., 2008). In particular, the latter approach suggests restricting the feasible space of Problem 2 to a computationally tractable class of distributions \(\), such as an exponential family, in order to obtain a more compact parametrization of a space of probabilities. We can then attempt to find an approximately optimal solution to the resulting non-convex optimization problem via gradient descent over the distribution parameters.

Previous works by Ajanthan et al. (2019) and Meng et al. (2020) provide a more rigorous justification for the STE step by deploying this variational inference blueprint in the form of the well-known mean-field approximation (Friedli and Velenik, 2017; Sayama, 2015). Specifically, they restrict \(\) to a product of \(\{ 1\}\)-Bernoulli distributions, one for each coordinate, where \(_{i}\) equals the probability that \(}_{i}\) equals \(1\). The distribution of the variable \(}_{i}\) can then be re-parametrized in terms of its mean \(_{i}\) as, for all \(i\),

\[}_{i}(_{i}),_{i}=_{i}}{2},_{i}[-1,1].\] ( \[_{}\] )

Notice that Problem 2 with \(=_{}\) is still a relaxation to the original Problem 1, as the extreme values of \(\) yield deterministic weight choices. At this point, Ajanthan et al. (2019) and Meng et al. (2020) then argue that the vector of mean parameter \(\) constitutes the right choice of latent weights for the BNN training problem. In this way, the fixed range \([-1,+1]^{n}\) of \(\) explains the use of weight clipping. However, the main challenge with this approach is the estimation of the gradient \(_{}_{(),}[L(f(, ),y_{})]\) of the expected loss with respect to the mean parameters \(\). In their case, one can only establish the following general form (Williams, 1992):

\[_{}_{(),}[L(f(, ),y_{})]=_{(),}[L(f(,),y_{})_{}()].\]

This expression leads to estimating the gradient via sampling from \(\) and evaluating the loss function, but fails to take advantage of the differentiability of \(\) and fails to reproduce the STE. Indeed, this setback forces Meng et al. (2020) to use a smooth proxy to the \(\) function to recover an approximation of the STE. The work of Ajanthan et al. (2019) only performs binary compression and does not rely on the STE.

### BNN Training via Gaussian Variational Inference

In this section, we describe our novel application of Gaussian variational inference, which has long been recognized as the most practical refinement of the mean-field approach (Giordano et al., 2015), to BNN training. Specifically, we consider optimizing Problem 2 over the class \(_{}\) of correlated multivariate Gaussian distributions over \(^{n}\), including degenerate Gaussian distributions with rank-deficient covariance matrices:

\[(,), [-1,1]^{n}, 0,\] ( \[_{}\] ) \[ i[n],_{ii}+_{i}^{2}=1.\]The joint constraints on mean and covariance ensure that the second moments \([_{i}^{2}]\) equal \(1\), matching those of a distribution over \(\{ 1\}^{n}\). The resulting non-convex semidefinite program is also a valid relaxation of Problem 1, as setting \(=0\) yields \(\{ 1\}^{n}\).

At first, the relaxation of the sample space of \(_{}\) from \(\{ 1\}\) to \(^{n}\) may seem problematic, as sampling now fails to yield the desired binary weights. However, the use of multivariate Gaussians as tractable proxies for the discrete probability distributions in \(_{}\) has a long history in approximation algorithms, particularly in the context of semidefinite programming relaxations (Goemans and Williamson, 1995; Alon and Naor, 2006). Indeed, the celebrated Grothendieck's inequality (Grothendieck, 1953) shows that \(_{}\) can be effectively relaxed to \(_{}\), with only a multiplicative constant loss, when optimizing the expectation of a quadratic polynomial. Unfortunately, the corresponding rounding procedure from a sample \(,_{}\) to a binary vector \(}\) is fairly complex, as it requires taking large tensor powers of the entries of the covariance of \(\)(Alon and Naor, 2006). We opt instead for the more straightforward _hyperplane rounding_(Goemans and Williamson, 1995), which takes the simple form \(}=()\), recovering the standard sign-based rounding. In this case, the approximation guarantee only holds for quadratic polynomials with non-negative coefficients. This still provides sufficient theoretical motivation for our method and enables the higher performance of our algorithms, as practical results in Section 4 demonstrate.

### Solving the SDP over Low-Rank Covariances

In this subsection, we describe VISPA, our algorithm for solving the SDP formulation of Problem 2 over \(=_{}\). Inspired by the previous discussion on hyperplane rounding, we let \(\) go to \(0\), so that no entropy regularization is performed, but the SDP formulation captures more closely the original Problem 2. It is crucial to notice that we cannot hope to maintain a general covariance matrix \(^{n n}, 0,\) as this requires storing \((n^{2})\) matrix entries, which is infeasible for the large number of weights (\(n>>10^{6}\)) in practical BNN architectures. Fortunately, this is a typical issue with large-scale SDPs (Yurtsever et al., 2021), which can be tackled by restricting our attention to low-rank covariance solutions, as first suggested by Burer and Monteiro (2005). Following this setup, for a rank parameter \(K,\) our algorithm maintains a vector of means \(^{n}\) and a square root \(^{n K}\) of the covariance \(=^{T}\), which is now of rank at most \(K\). We call \(\) a _weight_deviation matrix_, as it describes the typical deviation from the mean \(\) along the subspace identified by the image of \(\). Pseudocode for the resulting algorithm \(\) is given in Algorithm 1. Here, we immediately see an advantage of the parametrization by the square root \(\) of \(\) rather than by \(\) itself: it facilitates sampling from the underlying Gaussian distribution, as a sample \(\) can be easily taken by rescaling a standard \(K\)-dimensional Gaussian \(\) (line 4 of Algorithm 1).

In contrast with classical applications of semidefinite programming, the objective function for the SDP of interest is nonlinear and non-convex, so that we must rely on gradient descent to solve the program to local optimality. The following theorem, proved in the Appendix, shows that the gradients of the expected loss with respect to the parameters \(\) and \(\) take on a particularly simple form, which is easy to estimate stochastically. This result, which heavily relies on the Gaussianity of the weights \(\), implicitly solves the challenge encountered by Meng et al. (2020) in previous work.

**Theorem 1**.: _For a random variable \((,^{T}),\) with \(^{n}\) and \(^{n K}\), we have:_

\[_{}_{,}[L(f(,),y _{})] =_{,}[_{}L(f(,),y_{ })];\] \[_{}_{,}[L(f(,),y_{ })] =_{,}[_{}[L(f(,),y_{ })^{T}]|_{=+}],\]

_where \((0,_{K})\)._

The approximation provided by hyperplane rounding justifies replacing \(\) with \(()\) in the right-hand side of the gradient expressions in the theorem. The mini-batch stochastic gradient descent step with momentum then takes the form of lines 6-10 in Algorithm 1, with lines 7-8 regulating the momentum. The moment-matching constraints are enforced by the projection steps of lines 11-13. Finally, at the inference stage, the output mean vector \(\) and deviation matrix \(\) are used to sample binary weights via \(\) rounding. In Section 4, we carry out a comprehensive evaluation of the accuracy of Algorithm 1 against state-of-the-art methods for BNN training.

Running Time of \(\)As in Algorithm 1, we let \(n\) be the number of weight parameters and \(K\) the embedding dimension. Additionally, denote by \(M\) the batch size. The running time of one iteration of \(\) is \(O(Mn+nK)\), where the first term stems from propagating each example through the neural network and the second term comes from updating the weight mean vector \(\) and the weight deviation matrix \(\).. In comparison, other state-of-the-art BNN training approaches typically just require \(O(Mn)\) time. In most cases, we have that \(K<<M\), so that the increased cost due to the \(nK\) term is a negligible fraction of the total running time, as most time is spent performing forward- and back-propagation through the neural network.

Memory consumption of \(\)The main potential limitation of \(\) on resource-constrained devices is the increase in memory usage due to having to maintain the covariance variable \(\). Indeed, \(\) requires \(n(K+1)\) memory for storing relevant variables, compared to just \(n\) for other methods. The total significance of this increase depends on the batch size and the size of the examples. For instance, in the case of ResNet18, there are roughly \(n=9 10^{6}\) weights while, for batch size \(100\), the total size of a data batch is \(15 10^{6}\). Hence, standard methods yield a total memory usage of \(24 10^{6}\). In contrast, choosing \(K=1\) in \(\) will lead to a usage of \(33 10^{6}\), a \(37.5\%\) increase. This increase will typically get smaller as we consider models trained on larger images. In practice, we often observe even smaller increases, as our estimate does not include memory usage due to back-propagation, which can be very large, e.g., in the case of residual connections. A possible mitigation strategy is to store our variable at a lower precision. Given that this variable is only accessed via multiplication with Gaussian noise, we believe that this will not change the behavior of our algorithm.

### Diagonal Covariances and New Interpretation of Latent-Weights Methods

Now, we consider the simple case in which the covariance matrix \(\) is a multiple \(^{2}\) of the identity, so that the underlying weights \(\) are independent Gaussian random variables with mean \(\). We denote the associated class of distributions by \(_{ indep}\):

\[(,^{2}),[-1,1]^{n}\] ( \[_{ indep}\] )By applying Theorem 1 in conjunction with hyperplane rounding, the mini-batch stochastic gradient descent step for Problem 2 with \(=_{}\) takes the following form:

\[ t,^{(t)}=_{m=1}^{M} L(f(_{m},(^{(t)})),y_{_{m}}),^{(t+1)}=(^{(t)}-^{(t)}),\] (3)

for a choice of step length \(>0\), sample mini-batch \((_{1},y_{_{1}}),,(_{M},y_{_{M}})\) and a sample \(^{(t)}\) from \(_{}\) with mean parameter \(^{(t)}\). The \(\) function restricts the argument to the range \([-1,+1]\). Surprisingly, this derivation recovers both the use of the STE and weight clipping, as well as that of the \(\) function for rounding. In Section 4.1, we compare the deterministic version of this algorithm (\(=0\)) with Algorithm 1 to show the accuracy gain due to the use of the deviation matrix \(\).

## 4 Experiments

**Datasets** We evaluate the performance of various methods on four datasets: CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), Tiny-ImageNet (Le and Yang, 2015) and ImageNet (Deng et al., 2009). CIFAR-10 consists of 50k training samples and 10k testing images with 10 classes, while CIFAR-100 consists of 50k training samples and 10k testing images with 100 non-overlapping classes. Tiny-ImageNet is a subset of ImageNet with 100k images and 200 classes. ImageNet contains 1.28 million training samples and 50k testing images for 1000 classes.

**Implementation Details** We implement VISPA using PyTorch (Paszke et al., 2019) and run on a single NVIDIA A100 with 40GB GPU memory per GPU card. On CIFAR-10, CIFAR-100 and Tiny-ImageNet, models with only binarized weights (denoted as 1W32A) are trained for 500 epochs following (Le et al., 2022), using a batch size of 256, an initial learning rate of 0.1, and a weight decay of \(5e-4\), with covariance rank \(K\) set to 8. For models with both binarized weights and activations (denoted as 1W1A), the training epochs is 600 following (Xu et al., 2021b), the initial learning rate is set to 0.5, and the weight decay is reduced to \(1e-5\), with \(K\) set to 4. We run 5 runs to report the mean and standard deviation. All experiments are conducted on a single GPU card.

On ImageNet, we train AlexNet (Krizhevsky et al., 2012) for 100 epochs, and ResNet18 (He et al., 2016) for 200 epochs following (Xu et al., 2021b), with a batch size of 1024, and standard pre-processing with random flips and resize in (He et al., 2016). Models with 1W32A are trained using an initial learning rate of 0.1, and a weight decay of \(5e-5\), with \(K\) set at 4. For models with 1W1A, the initial learning rate is set at 0.5, and the weight decay is reduced to \(1e-5\), with \(K\) set at 2. All experiments are conducted on four GPU cards.

All runs utilize a cosine annealing learning rate schedule with a 5-epoch warm-up to optimize training. To accelerate convergence, we employ the momentum technique (Sutskever et al., 2013), setting the momentum coefficient to \(=0.9\). To capture the correlation among weights, at the inference stage, we perform 40 samples and average the results to obtain the final prediction. The mean weights \(\) and the deviation matrix \(\) are initialized using a Xavier normal distribution (Glorot and Bengio, 2010) with a mean of 0 and a standard deviation as \(s*}}\), where fan_in is the number of input units, fan_out is the number of output units and \(s\) is the scaling factor. We empirically set \(s=10\) for \(\) and \(s=1\) for \(\). For a detailed study of the impact of the initialization of \(\), please refer to the Appendix A.2.

Comparison on CIFAR-10, CIFAR-100 and Tiny-ImageNetWe evaluate the performance of the 1W1A and 1W32A settings on CIFAR-10, CIFAR-100 and Tiny-ImageNet. For the 1W1A setting, we compare various methods, including ReSTE (Wu et al., 2023) and DIR-Net (Qin et al., 2023), using the commonly employed VGG-Small (Zhang et al., 2018) and ResNet18 (He et al., 2016) architectures in (Qin et al., 2020; Wu et al., 2023; Xu et al., 2021), Lin et al., 2022). Table 1 shows the result. Our method achieves the highest accuracy with 92.7% on VGG-Small and matches the top performance of 92.8% on ResNet18. While our approach only shows marginal improvements over recent methods like ReSTE, the results demonstrates its efficiency in optimizing binarized neural networks. For the 1W32A setting, we compare various approaches, including AdaSTE (Le et al., 2022) and BayesBiNN (Meng et al., 2020), using the commonly employed VGG16 (Simonyan and Zisserman, 2014) and ResNet18 (He et al., 2016) architectures in (Le et al., 2022; Ajanthan et al., 2019). Table 2 presents the result. Our proposed method achieves the highest accuracy across all datasets and architectures, outperforming existing state-of-the-art techniques. Specifically, on more complex datasets, our method shows significant improvements. For CIFAR-100, our method reaches \(72.09\%\) on VGG16, outperforming the best baseline, AdaSTE, by 2.81%. For Tiny-ImageNet, our approach achieves \(58.98\%\) on ResNet18, which is 4.06% higher than AdaSTE. These results highlight the effectiveness of our method in handling more complex datasets.

Comparison on ImageNetTo evaluate the performance of our proposed binarized neural network method, we compare a list of SOTA methods including ReBNN (Xu et al., 2023), ReSTE (Wu et al., 2023), DIR-Net (Qin et al., 2023) and BiPer (Vargas et al., 2024) on the ImageNet dataset using AlexNet and ResNet18 architectures. For AlexNet, we used the standard architecture without binarizing the first and last layers, adapting it for 1W1A and 1W32A configurations. For ResNet18, we employed the BiRealNet architecture for 1W1A, as described in (Xu et al., 2021b; Qin et al., 2020). For 1W32A, we used the original ResNet18 architecture, following common practices in

  
**Methods** & VGG-Small & ResNet18 \\  IR-Net Qin et al. (2020) & 90.4 & 91.5 \\ SD-BNN Xue et al. (2022) & 90.8 & 92.5 \\ RBNN Lin et al. (2020) & 91.3 & 92.2 \\ ReCU Xu et al. (2021b) & 92.2 & \(\) \\ LCR-BNN Shang et al. (2022a) & – & 91.8 \\ FDA-BNN Xu et al. (2021a) & 92.5 & – \\ RBNN + CMM Shang et al. (2022b) & 92.2 & \(\) \\ SiMaN Lin et al. (2022) & 92.5 & 92.5 \\ ReSTE Wu et al. (2023) & 92.6 & 92.6 \\ DIR-Net Qin et al. (2023) & \(91.1 0.1\) & \(92.8 0.1\) \\  VISPA (Ours) & \(\) & \(\) \\   

Table 1: Performance comparison by testing accuracy (%) on CIFAR-10 using VGG-Small and ResNet18 architectures with binarized activations and weights.

  
**Methods** &  &  &  \\   & VGG16 & ResNet18 & VGG16 & ResNet18 & ResNet18 \\  BinaryConnect (1) Coutbariaux et al. (2015a) & 89.04 & 91.64 & 59.13 & 72.14 & 49.65 \\ ProxQuant (1) Bai et al. (2018) & 90.11 & 92.32 & 55.10 & 68.35 & 49.97 \\ MDS-softmax (1) Ajanthan et al. (2021) & 91.30 & 93.28 & 63.97 & 72.18 & 51.81 \\ MDS-softmax (1) Ajanthan et al. (2021) & 91.53 & 93.18 & 61.69 & 72.18 & 52.32 \\ PMF (1) Ajanthan et al. (2021) & 91.40 & 93.24 & 64.71 & 71.56 & 51.52 \\ BayesBiNN (2) Meng et al. (2020) & \(90.68 0.07\) & \(92.28 0.09\) & \(65.92 0.18\) & \(70.33 0.25\) & 54.22 \\ AdaSTE(2) Le et al. (2022) & \(92.37 0.09\) & \(94.11 0.08\) & \(69.28 0.17\) & \(75.03 0.35\) & 54.92 \\  VISPA (Ours) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Performance comparison by testing accuracy (%) of various approaches on CIFAR-10, CIFAR-100, and Tiny-ImageNet across VGG16 and ResNet18 architectures with binarized weights only. (\(\)) indicates that results are obtained from the numbers reported by Ajanthan et al. (2021). (*) indicates that results are obtained from the numbers reported by Le et al. (2022).

  
**Methods** &  &  \\   & & Top1 (\%) & Top5 (\%) \\  BinaryNet Hubara et al. (2016) & 1/1 & 41.2 & 65.6 \\ XNOR-Net Rastegari et al. (2016) & 1/1 & 44.2 & 69.2 \\ Bop Helwegen et al. (2019) & 1/1 & 45.9 & 70.0 \\ Bop2ndOrder Suarez-Ramirez et al. (2021) & 1/1 & 46.9 & 70.9 \\ LNS Han et al. (2020) & 1/1 & 44.4 & - \\ FDA-BNN Xu et al. (2021a) & 1/1 & 46.2 & 69.7 \\ Quantization networks Yang et al. (2019) & 1/1 & 47.9 & 72.5 \\ BNN-DL Ding et al. (2019) & 1/1 & 47.8 & 71.5 \\ VISPA (Ours) & 1/1 & **51.1** & **75.0** \\  BinaryConnect Courbariaux et al. (2015a) & 1/32 & 35.4 & 61.0 \\ DoReFa Zhu et al. (2016) & 1/32 & 53.9 & 76.3 \\ XNOR-Net Rastegari et al. (2016) & 1/32 & 56.8 & 79.4 \\ ADMM Leng et al. (2018) & 1/32 & 57.0 & 79.7 \\ Quantization networks Yang et al. (2019) & 1/32 & 58.8 & **81.7** \\ VISPA (Ours) & 1/32 & **59.4** & 81.1 \\   

Table 3: Performance comparison by testing accuracy of various methods on ImageNet dataset at AlexNet. W/A denotes the bit-width of weights and activations.

related literature (Qin et al., 2020; Shang et al., 2022). Additionally, for ResNet18, we kept the first, last, and down-sampling layers in full precision.

Table 3 and Table 4 present a performance comparison of various binarized neural network methods on the ImageNet dataset using AlexNet and ResNet18 architectures separately. For AlexNet, our method achieves state-of-the-art performance with a Top-1 accuracy of 51.1%, surpassing previous best results from Quantization Networks (Yang et al., 2019) by 3.2% in Top-1 accuracy. Similarly, in the 1W32A configuration, our method outperforms all others with a Top-1 accuracy of 59.4% and a Top-5 accuracy of 81.1%, demonstrating a significant improvement over the next best method. Quantization Networks (Yang et al., 2019).

For ResNet18, our method again sets new benchmarks with a Top-1 accuracy of 62.1%, which is 0.5% higher in Top-1 accuracy compared to the best previous method, ReBNN (Xu et al., 2023). In the 1W32A configuration, our method achieves the highest Top-1 accuracy of 68.2% and a Top-5 accuracy of 87.8%, indicating its robustness and superior performance. These results highlight once again the effectiveness of our approach in improving the accuracy of BNNs across different architectures and configurations on complex datasets like ImageNet.

### Ablation Studies

Impact of Deviation Matrix \(\)To investigate the significance of maintaining the correlation between weights, we compare VISPA with the simpler, correlation-free algorithm of Equation 3 with \(=0\) on CIFAR-10, CIFAR-100, and Tiny-ImageNet with the 1W32A setting. Table 5 provides the experimental results. It can be seen that VISPA consistently performs better across all configurations,

    &  &  \\   & & Top1 (\%) & Top5 (\%) \\  Bop Helwegen et al. (2019) & 1/1 & 54.2 & 77.2 \\ Bi-RealNet Liu et al. (2018) & 1/1 & 56.4 & 79.5 \\ IR-Net Qin et al. (2020) & 1/1 & 58.1 & 80.0 \\ BONN Gu et al. (2019) & 1/1 & 59.3 & 81.6 \\ LCR-BNN Shang et al. (2022) & 1/1 & 59.6 & 81.6 \\ SiBNN Wang et al. (2020) & 1/1 & 59.7 & 81.8 \\ SiMax Lin et al. (2022) & 1/1 & 60.1 & 82.3 \\ md-tann-s Ajanthan et al. (2021) & 1/1 & 60.3 & 82.3 \\ EqualBits Li et al. (2022) & 1/1 & 60.4 & 82.9 \\ DIR-Net Qin et al. (2023) & 1/1 & 60.4 & 81.9 \\ ReSTE Wu et al. (2023) & 1/1 & 60.9 & 82.6 \\ RecU Xu et al. (2021b) & 1/1 & 61.0 & 82.6 \\ BiPer Vargas et al. (2024) & 1/1 & 61.4 & 83.1 \\ ReBNN Xu et al. (2023) & 1/1 & 61.6 & **83.4** \\ VISPA (Ours) & 1/1 & **62.1** & **83.4** \\  XNOR-Net Rastegari et al. (2016) & 1/32 & 60.8 & 83.0 \\ HWGQ Cai et al. (2017) & 1/32 & 61.3 & 83.2 \\ ADMM Leng et al. (2018) & 1/32 & 64.8 & 86.2 \\ IR-Net Qin et al. (2020) & 1/32 & 66.5 & 86.8 \\ Quantization networks Yang et al. (2019) & 1/32 & 66.5 & 87.3 \\ LCR-BNN Shang et al. (2022) & 1/32 & 66.9 & 86.4 \\ ReSTE Wu et al. (2023) & 1/32 & 67.4 & 87.2 \\ DIR-Net Qin et al. (2023) & 1/32 & 67.5 & **87.9** \\ VISPA (Ours) & 1/32 & **68.2** & 87.8 \\   

Table 4: Performance comparison by testing accuracy of optimizers on ImageNet dataset across ResNet18 architectures. W/A denotes the bit-width of weights and activations.

    &  &  &  \\   & VGG16 & ResNet18 & VGG16 & ResNet18 & ResNet18 \\  VISPA wo Z (Ours) & \(92.85 0.09\) & \(95.03 0.08\) & \(70.12 0.10\) & \(76.23 0.36\) & \(56.73 0.33\) \\  VISPA (Ours) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 5: Performance comparison by testing accuracy (%) with and without \(\) across CIFAR-10, CIFAR-100, and Tiny-ImageNet on VGG16 and ResNet18.

especially on the more complex dataset Tiny-ImageNet, where the accuracy increases from 56.73 % to 58.89% for ResNet18. This suggests that the deviation matrix \(\) might be particularly useful in scenarios with a higher number of classes and potentially more complex patterns.

Impact of Covariance Rank \(K\)We investigate the impact of the covariance rank \(K\) on the accuracy of various datasets and models. Experiments are conducted on CIFAR-10 and CIFAR-100 datasets using configurations of 1W32A and 1W1A across architectures including VGG16, VGG-Small, and ResNet18, by setting different values of \(K\) and performing 5 runs. Figure 1 presents the results. The data reveals that ResNet18 and VGG16 generally benefit from increasing \(K\) values, with ResNet18 + CIFAR-10 (1W1A) and VGG16 + CIFAR-100 (1W32A) peaking at \(K=10\). The accuracy of ResNet18 on CIFAR-10 (1W1A) increases with higher \(K\) values, from \(92.53 0.11\%\) at \(K=1\) to \(93.01 0.07\%\) at \(K=10\), indicating improved performance and stability. Conversely, VGG-Small on CIFAR-10 (1W1A) experiences a slight decline in accuracy with higher \(K\). ResNet18 on CIFAR-100 (1W32A) shows the most variability, with accuracy peaking at \(77.05 0.41\%\) for \(K=8\), suggesting an unpredictable impact of \(K\). These results highlight that higher \(K\) values generally improve accuracy and stability for most models, particularly for ResNet18 and VGG16, but the benefits may vary depending on the specific model and dataset configuration. We have two hypotheses for this lack of conclusive evidence: i) higher values of \(K\) yield a larger feasible space and require a longer time to converge; ii) higher values of yield a more complex distribution from which it is more expensive to sample good weights at the inference stage.

## 5 Limitations and Open Problems

Our contributions naturally open a number of directions for further research. On the theoretical side, it would be interesting to efficiently implement the rounding suggested by Grothendieck's inequality. Similarly, we believe that the gradient descent approach of Burer-Monteiro can be formally analyzed as a gradient flow over the Bures-Wasserstein (BW) manifold of Gaussian distributions, which has recently been applied successfully in the context of variational inference [Lambert et al., 2022], to show that our method, with a proper choice of step size, converges to a stationary point of Problem 2 in the BW geometry.

On the experimental side, at the inference stage, our method currently draws and rounds 40 samples from the computed Gaussian distribution and averages the results. It is an active area of focus to further reduce the impact of this procedure on inference time. In preliminary results, we find that the process may be sped by using a smaller number of correlated samples via Gaussian quadrature. In this case, \(2K+1\) samples would suffice, which is as small as 3 for the \(K=1\) version of our method. Here we note that the prime runtime concern is to reduce the cost of training, which we achieve by binarizing both weights and activations while outperforming competitors.

Finally, we believe that the application of our method to the binarization of transformer architectures [He et al., 2023, Zhang et al., 2024] and to the general quantization settings, where the weights can take on more than two values, could have substantial practical consequences.

## 6 Acknowledgments

This research is supported by the Agency for Science, Technology, and Research (A*STAR) under its MTC Programmatic Fund M23L7b0021. And this work is partially supported by the National Natural Science Foundation of China under Grant (Nos. 62272093, 62137001).

Figure 1: Impact of \(K\) on model accuracy. The table shows the mean of testing accuracy and standard deviation for different \(K\) across models and datasets. Darker colors indicate higher accuracy.