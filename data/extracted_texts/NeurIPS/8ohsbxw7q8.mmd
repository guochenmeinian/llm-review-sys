# Graph Diffusion Policy Optimization

Yijing Liu\({}^{1}\), Chao Du\({}^{*}\)\({}^{}\)\({}^{2}\), Tianyu Pang\({}^{2}\), Chongxuan Li\({}^{3}\), Min Lin\({}^{2}\), Wei Chen\({}^{}\)\({}^{1}\)

\({}^{1}\)State Key Lab of CAD&CG, Zhejiang University

\({}^{2}\)Sea AI Lab, Singapore

\({}^{3}\)Renmin University of China

{liuyj86,chenvis}@zju.edu.cn;

{duchao,tianyupang,linmin}@sea.com; Chongxuanli@ruc.edu.cn

Equal contribution. Work done during Yijing Liu's internship at Sea AI Lab.Correspondence to Wei Chen and Chao Du.

###### Abstract

Recent research has made significant progress in optimizing diffusion models for downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph presents challenges, resulting in suboptimal performance. This paper introduces _graph diffusion policy optimization_ (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an _eager policy gradient_ tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.

## 1 Introduction

Graph generation, a key facet of graph learning, has applications in a variety of domains, including drug and material design , code completion , social network analysis , and neural architecture search . Numerous studies have shown significant progress in graph generation with deep generative models . one of The most notable advances in the field is the introduction of graph diffusion probabilistic models (DPMs) . These methods can learn the underlying distribution from graph data samples and produce high-quality novel graph structures.

In many use cases of graph generation, the primary focus is on achieving specific objectives, such as high drug efficacy  or creating novel graphs with special discrete properties . These objectives are often expressed as specific reward signals, such as binding affinity  and synthetic accessibility , rather than a set of training graph samples. Therefore, a more pertinent goal in such scenarios is to train graph generative models to meet these predefined objectives directly, rather than learning to match a distribution over training data .

A major challenge in this context is that most signals are non-differentiable w.r.t. graph representations, making it difficult to apply many optimization algorithms. To address this, methods based on property predictors  learn parametric models to predict the reward signals, providing gradient guidance for graph generation. However, since reward signals can be highly complex (e.g., results from physical simulations), these predictors often struggle to provide accurate guidance . An alternative direction is to learn graph generative models as policies through reinforcement learning (RL) , which enables the integration of exact reward signals into the optimization. However, existing work primarily explores earlier graph generative models and has yet to leverage the superior performance of graph DPMs . On the other hand, several pioneer works have seen significantprogress in optimizing continuous-variable (e.g., images) DPMs for downstream objectives [6; 16]. The central idea is to formulate the sampling process as a policy, with the objective serving as a reward, and then learn the model using policy gradient methods. However, when these approaches are directly extended to (discrete-variable) graph DPMs, we empirically observe a substantial failure, which we will illustrate and discuss in Sec. 4.

To close this gap, we present _graph diffusion policy optimization_ (GDPO), a policy gradient method designed to optimize graph DPMs for arbitrary reward signals. Using an RL formulation similar to that introduced by Black et al.  and Fan et al.  for continuous-variable DPMs, we first adapt the discrete diffusion process of graph DPMs to a Markov decision process (MDP) and formulate the learning problem as policy optimization. Then, to address the observed empirical failure, we introduce a slight modification to the standard policy gradient method REINFORCE , dubbed the _eager policy gradient_ and specifically tailored for graph DPMs. Experimental evaluation shows that GDPO proves effective across various scenarios and achieves high sample efficiency. Remarkably, our method achieves a \(\) to \(\) average reduction in generation-test distance and a \(1.03\%\) to \(\) improvement in the rate of generating effective drugs, while only querying a small number of samples (1/25 of the training samples).

## 2 Related Works

**Graph Generative Models.** Early work in graph generation employs nonparametric random graph models [15; 26]. To learn complex distributions from graph-structured data, recent research has shifted towards leveraging deep generative models. This includes approaches based on auto-regressive generative models [69; 39], variational autoencoders (VAEs) [34; 41; 23], generative adversarial networks (GANs) [62; 9; 43], and normalizing flows [53; 40; 42].

Recently, diffusion probabilistic models (DPMs) [25; 56] have significantly advanced graph generation . Models like EDP-GNN  GDSS  and DruM  construct graph DPMs using continuous diffusion processes . While effective, the use of continuous representations and Gaussian noise can hurt the sparsity of generated graphs. DiGress  employs categorical distributions as the Markov transitions in discrete diffusion , performing well on complex graph generation tasks. While these works focus on learning graph DPMs from a given dataset, our primary focus in this paper is on learning from arbitrary reward signals.

**Controllable Generation for Graphs.** Recent progress in controllable generation has also enabled graph generation to achieve specific objectives or properties. Previous work leverages mature conditional generation techniques from GANs and VAEs [66; 52; 36; 28; 14]. This paradigm has been extended with the introduction of guidance-based conditional generation in DPMs . DiGress  and GDSS  provide solutions that sample desired graphs with guidance from additional property predictors. MOOD  improves these methods by incorporating out-of-distribution control. However, as predicting the properties (e.g., drug efficacy) can be extremely difficult [33; 44], the predictors often struggle to provide accurate guidance. Our work directly performs property optimization on graph DPMs, thus bypassing this challenge.

**Graph Generation using RL.** RL techniques find wide application in graph generation to meet downstream objectives. REINVENT  and GCPN  are representative works, which define graph environments and optimize policy networks with policy gradient methods . For data-free generation modelling, MoIDQN  replaces the data-related environment with a human-defined graph environmental and utilizes Q-Learning  for policy optimi zation. To generate more realistic molecules, DGAPN  and FREED  investigate the fragment-based chemical environment, which reduce the search space significantly. Despite the great successes, existing methods exhibit high time complexity and limited policy model capabilities. Our work, based on graph DPMs with enhanced policy optimization, achieves new state-of-the-art performance.

**Aligning DPMs.** Several works focus on optimizing generative models to align with human preferences [45; 3]. DPOK  and DDPO  are representative works that align text-to-image DPMs with black-box reward signals. They formulate the denoising process of DPMs as an MDP and optimize the model using policy gradient methods. For differentiable rewards, such as human preference models , AlignProp  and DRaFT  propose effective approaches to optimize DPMs with direct backpropagation, providing a more accurate gradient estimation than DDPO and DPOK. However,these works are conducted on images. To the best of our knowledge, our work is the first effective method for aligning graph DPMs, filling a notable gap in the literature.

## 3 Preliminaries

In this section, we briefly introduce the background of graph DPMs and policy gradient methods.

Following Vignac et al. , we consider graphs with categorical node and edge attributes, allowing representation of diverse structured data like molecules. Let \(\) and \(\) be the space of categories for nodes and edges, respectively, with cardinalities \(a=||\) and \(b=||\). For a graph with \(n\) nodes, we denote the attribute of node \(i\) by a one-hot encoding vector \(^{(i)}^{a}\). Similarly, the attribute of the edge1 from node \(i\) to node \(j\) is represented as \(^{(ij)}^{b}\). By grouping these one-hot vectors, the graph can then be represented as a tuple \((,)\), where \(^{n a}\) and \(^{n n b}\).

### Graph Diffusion Probabilistic Models

Graph diffusion probabilistic models (DPMs)  involve a forward diffusion process \(q(_{1:T}|_{0})=_{t=1}^{T}q(_{t}|_{t-1})\), which gradually corrupts a data distribution \(q(_{0})\) into a simple noise distribution \(q(_{T})\) over a specified number of diffusion steps, denoted as \(T\). The transition distribution \(q(_{t}|_{t-1})\) can be factorized into a product of categorical distributions for individual nodes and edges, i.e., \(q(_{t}^{(i)}|_{t-1}^{(i)})\) and \(q(_{t}^{(ij)}|_{t-1}^{(ij)})\). For simplicity, superscripts are omitted when no ambiguity is caused in the following. The transition distribution for each node is defined as \(q(_{t}|_{t-1})=(_{t};_{t-1}_{t})\), where the transition matrix is chosen as \(_{t}_{t}+(1-_{t})(_{t}_{t}^{ })/a\), with \(_{t}\) transitioning from \(1\) to \(0\) as \(t\) increases . It then follows that \(q(_{t}|_{0})=(_{t};_{0}}_{t})\) and \(q(_{t-1}|_{t},_{0})=(_{t-1};_{t }_{t}^{}_{0}_{t-1}}{_{0}_{t}_{t}^ {}})\), where \(}_{t}_{1}_{2}_{t}\) and \(\) denotes element-wise product. The design choice of \(_{t}\) ensures that \(q(_{T}|_{0})(_{T};_{a}/a)\), i.e., a uniform distribution over \(\). The transition distribution for edges is defined similarly, and we omit it for brevity.

Given the forward diffusion process, a parametric reverse denoising process \(p_{}(_{0:T})=p(_{T})_{t=1}^{T}p_{}(_{t-1}| _{t})\) is then learned to recover the data distribution from \(p(_{T}) q(_{T})\) (an approximate uniform distribution). The reverse transition \(p_{}(_{t-1}|_{t})\) is a product of categorical distributions over nodes and edges, denoted as \(p_{}(_{t-1}|_{t})\) and \(p_{}(_{t-1}|_{t})\). Notably, in line with the \(_{0}\)-parameterization used in continuous DPMs [25; 32], \(p_{}(_{t-1}|_{t})\) is modeled as:

\[p_{}(_{t-1}|_{t})_{}_{0} }q(_{t-1}|_{t},}_{0})p_{}( }_{0}|_{t}),\] (1)

where \(p_{}(}_{0}|_{t})\) is a neural network predicting the posterior probability of \(_{0}\) given a noisy graph \(_{t}\). For edges, each definition is analogous and thus omitted.

The model is learned with a graph dataset \(\) by maximizing the following objective :

\[_{}()=_{_{0},t}_{ (_{t}|_{0})}[ p_{}(_{0}|_{t})],\] (2)

where \(_{0}\) and \(t\) follow uniform distributions over \(\) and \([1,T]\), respectively. After learning, graph samples can then be generated by first sampling \(_{T}\) from \(p(_{T})\) and subsequently sampling \(_{t}\) from \(p_{}(_{t-1}|_{t})\), resulting in a generation trajectory \((_{T},_{T-1},,_{0})\).

### Markov Decision Process and Policy Gradient

Markov decision processes (MDPs) are commonly used to model sequential decision-making problems . An MDP is formally defined by a quintuple \((,,P,r,_{0})\), where \(\) is the state space containing all possible environment states, \(\) is the action space comprising all available potential actions, \(P\) is the transition function determining the probabilities of state transitions, \(r\) is the reward signal, and \(_{0}\) gives the distribution of the initial state.

In the context of an MDP, an agent engages with the environment across multiple steps. At each step \(t\), the agent observes a state \(_{t}\) and selects an action \(_{t}\) based on its policy distribution \(_{}(_{t}|_{t})\). Subsequently, the agent receives a reward \(r(_{t},_{t})\) and transitions to a new state \(_{t+1}\) following the transition function \(P(_{t+1}|_{t},_{t})\). As the agent interacts in the MDP (starting from an initial state \(_{0}_{0}\)), it generates a trajectory (i.e., a sequence of states and actions) denoted as \(=(_{0},_{0},_{1},_{1},,_{T},_{T})\). The cumulative reward over a trajectory \(\) is given by \(R()=_{t=0}^{T}r(_{t},_{t})\). In most scenarios, the objective is to maximize the following expectation:

\[_{}()=_{ p(|_{ })}[R()].\] (3)

Policy gradient methods aim to estimate \(_{}_{}()\) and thus solve the problem by gradient descent. An important result is the policy gradient theorem , which estimates \(_{}_{}()\) as follows:

\[_{}_{}()=_{ p( |_{})}_{t=0}^{T}_{}_{} (_{t}|_{t})R().\] (4)

The REINFORCE algorithm  provides a simple method for estimating the above policy gradient using Monte-Carlo simulation, which will be adopted and discussed in the following section.

## 4 Method

In this section, we study the problem of learning graph DPMs from arbitrary reward signals. We first present an MDP formulation of the problem and conduct an analysis on the failure of a direct application of REINFORCE. Based on the analysis, we introduce a substitute termed _eager policy gradient_, which forms the core of our method _Graph Diffusion Policy Optimization_ (GDPO).

### A Markov Decision Process Formulation

A graph DPM defines a sample distribution \(p_{}(_{0})\) through its reverse denoising process \(p_{}(_{0:T})\). Given a reward signal \(r()\) for \(_{0}\), we aim to maximize the expected reward (ER) over \(p_{}(_{0})\):

\[_{}()=_{_{0} p_{}(_ {0})}[r(_{0})].\] (5)

However, directly optimizing \(_{}()\) is challenging since the likelihood \(p_{}(_{0})\) is unavailable  and \(r()\) is black-box, hindering the use of typical RL algorithms . Following Fan et al. , we

Figure 1: Overview of GDPO. (1) In each optimization step, GDPO samples multiple generation trajectories from the current Graph DPM and queries the reward function with different \(_{0}\). (2) For each trajectory, GDPO accumulates the gradient \(_{} p_{}(_{0}|_{t})\) of each \((_{0},_{t})\) pair and assigns a weight to the aggregated gradient based on the corresponding reward signal. Finally, GDPO estimates the _eager policy gradient_ by averaging the aggregated gradient from all trajectories.

formulate the denoising process as a \(T\)-step MDP and obtain an equivalent objective. Using notations in Sec. 3, we define the MDP of graph DPMs as follows:

\[_{t}(_{T-t},T-t),_{t} _{T-t-1},_{}(_{t}|_{t}) p_{}( _{T-t-1}|_{T-t}),\] (6) \[P(_{t+1}|_{t},_{t})(_{_ {T-t-1}},_{T-t-1}), r(_{t},_{t}) r(_{0}) t=T,\ r(_{t},_{t}) 0t<T,\]

where the initial state \(_{0}\) corresponds to the initial noisy graph \(_{T}\) and the policy corresponds to the reverse transition distribution. As a result, the graph generation trajectory \((_{T},_{T-1},,_{0})\) can be considered as a state-action trajectory \(\) produced by an agent acting in the MDP. It then follows that \(p(|_{})=p_{}(_{0:T})\).2 Moreover, we have \(R()=_{t=0}^{T}r(_{t},_{t})=r(_{0})\). Therefore, the expected cumulative reward of the agent \(_{}()=_{p(|_{})}[R( {})]=_{p_{}(_{0:T})}[r(_{0})]\) is equivalent to \(_{}()\), and thus \(_{}()\) can also be optimized with the policy gradient \(_{}_{}()\):

\[_{}_{}()=_{}[r( _{0})}_{} p_{}( _{t-1}|_{t})],\] (7)

where the generation trajectory \(\) follows the parametric reverse process \(p_{}(_{0:T})\).

### Learning Graph DPMs with Policy Gradient

The policy gradient \(_{}_{}()\) in Eq. (7) is generally intractable and an efficient estimation is necessary. In a related setting centered on continuous-variable DPMs for image generation, DDPO  estimates the policy gradient \(_{}_{}()\) with REINFORCE and achieves great results. This motivates us to also try REINFORCE on graph DPMs, i.e., to approximate Eq. (7) with a Monte Carlo estimation:

\[_{}_{}_{k=1}^{K} {|_{k}|}_{t_{k}}r(_{0}^{(k)})_{ } p_{}(_{t-1}^{(k)}|_{t}^{(k)}),\] (8)

where \(\{_{0:T}^{(k)}\}_{k=1}^{K}\) are \(K\) trajectories sampled from \(p_{}(_{0:T})\) and \(\{_{k}\!\![\![1,T]\!]\}_{k=1}^{K}\) are uniformly random subsets of timesteps (which avoid summing over all timesteps and accelerate the estimation).

However, we empirically observe that it rarely converges on graph DPMs. To investigate this, we design a toy experiment, where the reward signal is whether \(_{0}\) is connected. The graph DPMs are randomly initialized and optimized using Eq. (8). We refer to this setting as DDPO. Fig. 2 depicts the learning curves, where the horizontal axis represents the number of queries to the reward signal and the vertical axis represents the average reward. The results demonstrate that DDPO fails to converge to a high reward signal area when generating graphs with more than 4 nodes. Furthermore, as the

Figure 2: Toy experiment comparing DDPO and GDPO. We generate connected graphs with increasing number of nodes. Node categories are disregarded, and the edge categories are binary, indicating whether two nodes are linked. The graph DPM is initialized randomly as a one-layer graph transformer from DiGress . The diffusion step \(T\) is set to \(50\), and the reward signal \(r(_{0})\) is defined as \(1\) if \(_{0}\) is connected and \(0\) otherwise. We use \(256\) trajectories for gradient estimation in each update. The learning curve illustrates the diminishing performance of DDPO as the number of nodes increases, while GDPO consistently performs well.

number of nodes increases, the fluctuation of the learning curves grows significantly. This implies that DDPO is essentially unable to optimize properly on randomly initialized models. We conjecture that the failure is due to the vast space constituted by discrete graph trajectories and the well-known high variance issue of REINFORCE . A straightforward method to reduce variance is to sample more trajectories. However, this is typically expensive in DPMs, as each trajectory requires multiple rounds of model inference. Moreover, evaluating the reward signals of additional trajectories also incurs high computational costs, such as drug simulation .

This prompts us to delve deeper at a micro level. Since the policy gradient estimation in Eq. (8) is a weighted summation of gradients, we first inspect each summand term \(_{} p_{}(_{t-1}|_{t})\). With the parameterization Eq. (1) described in Sec. 3.1, it has the following form:

\[_{} p_{}(_{t-1}|_{t})=( _{t-1}|_{t})}_{}_{0}}_{t -1}|_{t},}_{0})}_{}p_{}(}_{0}|_{t})}_{},\] (9)

where we can view the "weight" term as a weight assigned to the gradient \(_{}p_{}(}_{0}|_{t})\), and thus \(_{} p_{}(_{t-1}|_{t})\) as a weighted sum of such gradients, with \(}_{0}\) taken over all possible graphs. Intuitively, the gradient \(_{}p_{}(}_{0}|_{t})\) promotes the probability of predicting \(}_{0}\) from \(_{t}\). Note, however, that the weight \(q(_{t-1}|_{t},}_{0})\) is completely independent of \(r(}_{0})\) and could assign large weight for \(}_{0}\) that has low reward. Since the weighted sum in Eq. (9) can be dominated by gradient terms with large \(q(_{t-1}|_{t},}_{0})\), given a particular sampled trajectory, it is fairly possible that \(_{} p_{}(_{t-1}|_{t})\) increases the probabilities of predicting undesired \(}_{0}\) with low rewards from \(_{t}\). This explains why Eq. (8) tends to produce fluctuating and unreliable policy gradient estimates when the number of Monte Carlo samples (i.e., \(K\) and \(|_{k}|\)) is limited. To further analyze why DDPO does not yield satisfactory results, we present additional findings in Appendix A.5. Besides, we discuss the impact of importance sampling techniques in the same section.

### Graph Diffusion Policy Optimization

To address the above issues, we suggest a slight modification to Eq. (8) and obtain a new policy gradient denoted as \(()\):

\[()_{k=1}^{K}_{k}|} _{t_{k}}r(_{0}^{(k)})_{} p_{}( _{0}^{(k)}|_{t}^{(k)}),\] (10)

which we refer to as the _eager policy gradient_. Intuitively, although the number of possible graph trajectories is tremendous, if we partition them into different equivalence classes according to \(_{0}\), where trajectories with the same \(_{0}\) are considered equivalent, then the number of these equivalence classes will be much smaller than the number of graph trajectories. The optimization over these equivalence classes will be much easier than optimizing in the entire trajectory space.

Technically, by replacing the summand gradient term \(_{} p_{}(_{t-1}|_{t})\) with \(_{} p_{}(_{0}|_{t})\) in Eq. (8), we skip the weighted sum in Eq. (9) and directly promotes the probability of predicting \(_{0}\) which has higher reward from \(_{t}\) at all timestep \(t\). As a result, our estimation does not focus on how \(_{t}\) changes to \(_{t-1}\) within the trajectory; instead, it aims to force the model's generated results to be close to the desired \(_{0}\), which can be seen as optimizing in equivalence classes. While being a biased estimator of the policy gradient \(_{}_{}()\), the eager policy gradient consistently leads to more stable learning and better performance than DDPO, as demonstrated in Fig. 2. We present the resulting method in Fig. 1 and Algorithm 1, naming it _Graph Diffusion Policy Optimization_ (GDPO).

## 5 Reward Functions for Graph Generation

In this work, we study both general graph and molecule reward signals that are crucial in real-world tasks. Below, we elaborate on how we formulate diverse reward signals as numerical functions.

### Reward Functions for General Graph Generation

**Validity.** For graph generation, a common objective is to generate a specific type of graph. For instance, one might be interested in graphs that can be drawn without edges crossing each other .

For such objectives, the reward function \(r_{}()\) is then formulated as binary, with \(r_{}(_{0}) 1\) indicating that the generated graph \(_{0}\) conforms to the specified type; otherwise, \(r_{}(_{0}) 0\).

**Similarity.** In certain scenarios, the objective is to generate graphs that resemble a known set of graphs \(\) at the distribution level, based on a pre-defined distance metric \(d(,)\) between sets of graphs. As an example, the \((,)\) measures the maximum mean discrepancy (MMD)  between the degree distributions of a set \(\) of generated graphs and the given graphs \(\). Since our method requires a reward for each single generated graph \(_{0}\), we simply adopt \((\{(_{0}),\})\) as the signal. As the magnitude of reward is critical for policy gradients , we define \(r_{}(_{0})(-(\{_{0}\}, )^{2}/^{2})\), where the \(\) controls the reward distribution, ensuring that the reward lies within the range of 0 to 1. The other two similar distance metrics are \((,)\) and \((,)\), which respectively measure the distances between two sets of graphs in terms of the distribution of clustering coefficients  and the distribution of substructures . Based on the two metrics, we define two reward signals analogous to \(r_{}\), namely, \(r_{}\) and \(r_{}\).

### Reward Functions for Molecular Graph Generation

**Novelty.** A primary objective of molecular graph generation is to discover novel drugs with desired therapeutic potentials. Due to drug patent restrictions, the novelty of generated molecules has paramount importance. The Tanimoto similarity , denoted as \(J(,)\), measures the chemical similarity between two molecules, defined by the Jaccard index of molecule fingerprint bits. Specifically, \(J\), and \(J(_{0},_{0}^{})=1\) indicates that two molecules \(_{0}\) and \(_{0}^{}\) have identical fingerprints. Following Xie et al. , we define the novelty of a generated graph \(_{0}\) as \((_{0}) 1-_{_{0}^{}}J(_ {0},_{0}^{})\), i.e., the similarity gap between \(_{0}\) and its nearest neighbor in the training dataset \(\), and further define \(r_{}(_{0})(_{0})\).

**Drug-Likeness.** Regarding the efficacy of molecular graph generation in drug design, a critical indicator is the binding affinity between the generated drug candidate and a target protein. The docking score , denoted as \(()\), estimates the binding energy (in kcal/mol) between the ligand and the target protein through physical simulations in 3D space. Following Lee et al. , we clip the docking score in the range \([-20,0]\) and define the reward function as \(r_{}(_{0})-(_{0})/20\).

Another metric is the quantitative estimate of drug-likeness \(()\), which measures the chemical properties to gauge the likelihood of a molecule being a successful drug . As it takes values in the range \(\), we adopt \(r_{}(_{0})[(_{0})>0.5]\).

**Synthetic Accessibility**. The synthetic accessibility \(()\) evaluates the inherent difficulty in synthesizing a chemical compound, with values in the range from \(1\) to \(10\). We follow Lee et al.  and use a normalized version as the reward function: \(r_{}(_{0})(10-(_{0}))/9\).

## 6 Experiments

In this section, we first examine the performance of GDPO on both general graph generation tasks and molecular graph generation tasks. Then, we conduct several ablation studies to investigate the effectiveness of GDPO's design. Our code can be found in the supplementary material.

    &  &  \\   & _Deg_\(\) & _Clus_\(\) & _Orb_\(\) & _VU.N (\%)_ & _Deg_\(\) & _Clus_\(\) & _Orb_\(\) & _VU.N (\%)_ \\  GraphRNN & 24.51 \(\) 3.22 & 9.03 \(\) 0.78 & 2508.30 \(\) 30.81 & 0 & 6.92 \(\) 1.13 & 1.72 \(\) 0.05 & 3.15 \(\) 0.23 & 4.92 \(\) 0.35 \\ SPECTRE & 2.55 \(\) 0.34 & 2.52 \(\) 0.26 & 2.42 \(\) 0.37 & 25.46 \(\) 1.33 & 1.92 \(\) 1.21 & 1.64 \(\) 0.06 & 1.67 \(\) 0.14 & 53.76 \(\) 3.62 \\ GDSS & 10.81 \(\) 0.86 & 12.99 \(\) 0.22 & 38.71 \(\) 0.83 & 0.78 \(\) 0.72 & 15.53 \(\) 1.30 & 3.50 \(\) 0.81 & 15.98 \(\) 2.30 & 0 \\ MOOD & 5.73 \(\) 0.82 & 11.87 \(\) 0.34 & 30.62 \(\) 0.67 & 1.21 \(\) 0.83 & 1.287 \(\) 1.20 & 3.66 \(\) 0.37 & 2.81 \(\) 0.35 & 0 \\ DiGress & 1.43 \(\) 0.90 & 1.22 \(\) 0.32 & 1.72 \(\) 0.44 & 70.02 \(\) 2.17 & 1.63 \(\) 1.51 & 1.50 \(\) 0.04 & 1.70 \(\) 0.16 & 60.94 \(\) 4.98 \\ DDPO & 109.59\(\) 36.69 & 31.47 \(\) 4.96 & 504.19 \(\) 17.61 & 2.34 \(\) 1.10 & 250.06 \(\) 7.44 & 2.93 \(\) 0.32 & 6.65 \(\) 0.45 & 31.25 \(\) 5.22 \\ GDPO (ours) & **0.03**\(\) 0.04 & **0.62**\(\) 0.11 & **0.02**\(\) 0.01 & **73.83**\(\) 2.49 & **0.15**\(\) 0.13 & **1.50**\(\) 0.01 & **1.12**\(\) 0.14 & **80.08**\(\) 2.07 \\   

Table 1: General graph generation on SBM and Planar datasets.

### General Graph Generation

**Datasets and Baselines.** Following DiGress , we evaluate GDPO on two benchmark datasets: SBM (200 nodes) and Planar (64 nodes), each consisting of 200 graphs. We compare GDPO with GraphRNN , SPECTRE , GDSS , MOOD  and DiGress. The first two models are based on RNN and GAN, respectively. The remaining methods are graph DPMs, and MOOD employs an additional property predictor. We also test DDPO , i.e., graph DPMs optimized with Eq. (8).

**Implementation.** We set \(T=1000\), \(||=200\), and \(N=100\). The number of trajectory samples \(K\) is \(64\) for SBM and \(256\) for Planar. We use a DiGress model with \(10\) layers. More implementation details can be found in Appendix A.1.

**Metrics and Reward Functions.** We consider four metrics: _Deg\((,_{test})\)_, _Clus\((,_{test})\)_, _Orb\((,_{test})\)_, and the _V.U.N_ metrics. _V.U.N_ measures the proportion of generated graphs that are valid, unique, and novel. The reward function is defined as follows:

\[r_{}=0.1(r_{}+r_{}+r_{})+0.7 r_{},\] (11)

where we do not explicitly incorporate uniqueness and novelty. All rewards are calculated on the training dataset if a reference graph set is required. All evaluation metrics are calculated on the test dataset. More details about baselines, reward signals, and metrics are in Appendix A.3.

**Results.** Table 1 summarizes GDPO's superior performance in general graph generation, showing notable improvements in _Deg_ and _V.U.N_ across both SBM and Planar datasets. On the Planar dataset, GDPO significantly reduces distribution distance, achieving an \(\) average decrease in metrics of _Deg_, _Clus_, and _Orb_ compared to DiGress (the best baseline method). For the SBM dataset, GDPO has a \(\) average improvement. The low distributional distances to the test dataset suggests that GDPO accurately captures the data distribution with well-designed rewards. Moreover, we observe that our method outperforms DDPO by a large margin, primarily because the graphs in Planar and SBM contain too many nodes, which aligns with the observation in Fig. 2.

### Molecule Property Optimization

**Datasets and Baselines.** Molecule property optimization aims to generate molecules with desired properties. We evaluate our method on two large molecule datasets: ZINC250k  and MOSES . The ZINC250k dataset comprises 249,456 molecules, each containing 9 types of atoms, with a maximum node count of 38; the MOSES dataset consists of 1,584,663 molecules, with 8 types of atoms and a maximum node count of 30. We compare GDPO with several leading methods:

    &  &  \\   & & _parp1_ & _fa7_ & _Sht1b_ & _braf_ & _jak2_ \\   & _Hit Ratio_ & 0 & 0 & 1.455 \(\) 1.173 & 0 & 0 \\  & _DS (top \(5\%\))_ & -8.102\(\) 0.105 & -6.688\(\)0.186 & -8.544\(\) 0.505 & -8.713\(\) 0.155 & -8.073\(\)0.093 \\   & _Hit Ratio_ & 0.480 \(\) 0.344 & 0.213 \(\) 0.081 & 2.453 \(\) 0.561 & 0.127 \(\) 0.088 & 0.613 \(\) 0.167 \\  & _DS (top \(5\%\))_ & -8.702 \(\) 0.523 & -7.205 \(\) 0.264 & -8.770 \(\) 0.316 & -8.392 \(\) 0.400 & -8.165 \(\) 0.277 \\   & _Hit Ratio_ & 4.627 \(\) 0.727 & 1.332 \(\) 0.113 & 16.767 \(\) 0.897 & 2.940 \(\) 0.359 & 5.800 \(\) 0.295 \\  & _DS (top \(5\%\))_ & -10.579 \(\) 0.104 & -8.378 \(\) 0.044 & -10.714 \(\) 0.183 & -10.561 \(\) 0.080 & -9.735 \(\) 0.022 \\   & _Hit Ratio_ & 7.017 \(\) 0.428 & 0.733 \(\) 0.141 & 18.673 \(\) 0.423 & 5.240 \(\) 0.285 & 9.200 \(\) 0.524 \\  & _DS (top \(5\%\))_ & -10.865 \(\) 0.113 & -8.160 \(\) 0.071 & -11.145 \(\) 0.042 & -11.063 \(\) 0.034 & -10.147 \(\) 0.060 \\   & _Hit Ratio_ & 0.366 \(\) 0.146 & 0.182 \(\) 0.232 & 4.236 \(\) 0.887 & 0.122 \(\) 0.141 & 0.861 \(\) 0.332 \\  & _DS (top \(5\%\))_ & -9.219 \(\) 0.078 & -7.736 \(\) 0.156 & -9.280 \(\) 0.198 & -9.052 \(\) 0.044 & -8.706 \(\) 0.222 \\   & _Hit Ratio_ & 1.172\(\)0.672 & 0.321\(\)0.370 & 2.821\(\) 1.140 & 0.152\(\)0.303 & 0.311\(\)0.621 \\  & _DS (top \(5\%\))_ & -9.463\(\) 0.524 & -7.318\(\)0.213 & -8.971\(\) 0.395 & -8.825\(\) 0.459 & -8.360\(\)0.217 \\   & _Hit Ratio_ & 0.419 \(\) 0.280 & 0.342 \(\) 0.685 & 5.488 \(\) 1.989 & 0.445 \(\) 0.297 & 1.717 \(\) 0.684 \\  & _DS (top \(5\%\))_ & -9.247 \(\) 0.242 & -7.739 \(\) 0.244 & -9.488 \(\) 0.287 & -9.470 \(\) 0.373 & -8.990 \(\) 0.221 \\   GDPO (ours) \\  } & _Hit Ratio_ & **9.814 \(\) 1.352** & **3.449 \(\)** 0.188 & **34.359 \(\)** 2.734 & **9.039 \(\)** 1.473 & **13.405 \(\)** 1.515 \\  & _DS (top \(5\%\))_ & **-10.938 \(\)** 0.042 & **8.691 \(\)** 0.074 & **-**11.304 \(\)** 0.093 & **-**11.197 \(\)** 0.132 & **-10.183 \(\)** 0.124 \\   

Table 2: Molecule property optimization results on ZINC250k.

GCPN , REINVENT , FREED  and MOOD . GCPN, REINVENT and FREED are RL methods that search in the chemical environment. MOOD, based on graph DPMs, employs a property predictor for guided sampling. Similar to general graph generation, we also compare our method with DiGress and DDPO. Besides, we show the performance of DiGress with property predictors, termed as DiGress-guidance.

**Implementation.** We set \(T=500\), \(||=100\), \(N=100\), and \(K=256\) for both datasets. We use the same model structure with DiGress. See more details in Appendix A.1.

**Metrics and Reward Functions.** Following MOOD, we consider two metrics essential for real-world novel drug discovery: **Novel hit ratio (%)** and **Novel top \(5\%\) docking score**, denoted as _Hit Ratio_ and _DS (top \(5\%\))_, respectively. Using the notations from Sec. 5.2, the _Hit Ratio_ is the proportion of unique generated molecules that satisfy: _DS \(<\)_ median _DS_ of the known effective molecules, _NOV \(>\)_0.6, _QED \(>\)_0.5, and _SA \(<\)_5. The _DS (top \(5\%\))_ is the average _DS_ of the top \(5\%\) molecules (ranked by _DS_) that satisfy: _NOV \(>\)_0.6, _QED \(>\)_0.5, and _SA \(<\)_5. Since calculating _DS_ requires specifying a target protein, we set five different protein targets to fully test GDPO: _parp1_, _fa7_, _5th1b_, _braf_, and _jak2_. The reward function for molecule property optimization is defined as follows:

\[r_{}=0.1(r_{_{QED}}}+r_{_{SA}}})+0.3  r_{_{NOV}}}+0.5 r_{_{DS}}}.\] (12)

We do not directly use _Hit Ratio_ and _DS (top \(5\%\))_ as rewards in consideration of method generality. The reward weights are determined through several rounds of search, and we find that assigning a high weight to \(r_{_{NOV}}}\) leads to training instability, which is discussed in Sec. 6.3. More details about the experiment settings are discussed in Appendix A.4.

**Results.** In Table 2, GDPO shows significant improvement on ZINC250k, especially in the _Hit Ratio_. A higher _Hit Ratio_ means the model is more likely to generate valuable new drugs, and GDPO averagely improves the _Hit Ratio_ by \(5.72\%\) in comparison with other SOTA methods. For _DS (top \(5\%\))_, GDPO also has a \(1.48\%\) improvement on average. Discovering new drugs on MOSES is much more challenging than on ZINC250k due to its vast training dataset. In Table 3, GDPO also shows promising results on MOSES. Despite a less favorable _Hit Ratio_ on _5th1b_, GDPO achieves an average improvement of \(12.94\%\) on the other four target proteins. For _DS (top \(5\%\))_, GDPO records an average improvement of \(5.54\%\) compared to MOOD, showing a big improvement in drug efficacy.

### Generalizability, Sample Efficiency, and A Failure Case

To validate whether GDPO correctly optimizes the model, we test the performance of GDPO on metrics not used in the reward signal. In Table 4, we evaluate the performance on Spectral MMD , where the GDPO is optimized by Eq. (11). The results demonstrate that GDPO does

    &  \\   & _DiGress_ & _DDPO_ & _GDPO (ours)_ \\  PLANAR & 1.0353\(\) 0.4474 & 20.1431\(\) 3.5810 & **0.8047\(\)** 0.2030 \\  SBM & 1.2024\(\) 0.2874 & 13.2773\(\) 1.4233 & **1.0861\(\)** 0.2551 \\   

Table 4: Generalizability of GDPO on Spectral MMD.

    &  &  \\   & & _parp1_ & _fa7_ & _5ht1b_ & _braf_ & _jak2_ \\   & _Hit Ratio_ & 0.532 \(\) 0.614 & 0 & 4.255 \(\) 0.869 & 0.263 \(\) 0.532 & 0.798 \(\) 0.532 \\  & _DS (top \(5\%\))_ & -9.313 \(\) 0.357 & -7.825 \(\) 0.167 & -9.506 \(\) 0.236 & -9.306 \(\) 0.327 & -8.594 \(\) 0.240 \\   & _Hit Ratio_ & 5.402 \(\) 0.042 & 0.365 \(\) 0.200 & 26.143 \(\) 1.647 & 3.932 \(\) 1.290 & 11.301 \(\) 1.154 \\  & _DS (top \(5\%\))_ & -9.814 \(\) 1.352 & -7.974 \(\) 0.029 & 10.734 \(\) 0.049 & -10.722 \(\) 0.135 & -10.158 \(\) 0.185 \\   & _Hit Ratio_ & 0.231 \(\) 0.463 & 0.113 \(\) 0.131 & 3.852 \(\) 5.013 & 0 & 0.228 \(\) 0.457 \\  & _DS (top \(5\%\))_ & -9.223 \(\) 0.083 & -6.644 \(\) 0.533 & -8.640 \(\) 0.907 & 8.522 \(\) 1.017 & -7.424 \(\) 0.994 \\   & _Hit Ratio_ & 3.037 \(\) 2.107 & 0.504 \(\) 0.667 & 7.855 \(\) 1.745 & 0 & 3.943 \(\) 2.204 \\  & _DS (top \(5\%\))_ & -9.727 \(\) 0.529 & -8.025 \(\) 0.253 & -9.631 \(\) 0.123 & -9.407 \(\) 0.125 & -9.404 \(\) 0.319 \\   & _Hit Ratio_ & **24.711 \(\) 1.775** & **1.393 \(\) 0.982** & 17.646 \(\) 2.484 & **19.968 \(\) 2.309** & **26.688 \(\) 2.401 \\  & _DS (top \(5\%\))_ & **-11.002 \(\) 0.056** & **-8.468 \(\) 0.058** & **-10.990 \(\) 0.334** & **-11.337 \(\) 0.137** & **-10.290 \(\) 0.069** \\   

Table 3: Molecule property optimization results on MOSES.

not show overfitting; instead, it finds a more powerful model. The results presented in Appendix A.5 further support that GDPO can attain high sample novelty and diversity.

We then investigate two crucial factors for GDPO: 1) the number of trajectories; 2) the selection of the reward signals. We test our method on ZINC250k and set the target proteins as \(5ht1b\). In Fig. 3 (a), the results indicate that GDPO exhibits good sampling efficiency, as it achieves a significant improvement in average reward by querying only 10k molecule reward signals, which is much less than the number of molecules contained in ZINC250k. Moreover, the sample efficiency can be further improved by reducing the number of trajectories, but this may lead to training instability. To achieve consistent results, we use 256 trajectories. In Fig. 3 (b), we illustrate a failure case of GDPO when assigning a high weight to \(r_{}\). Generating novel samples is challenging. MOOD  addresses this challenge by controlling noise in the sampling process, whereas we achieve it by novelty optimization. However, assigning a large weight to \(r_{}\) can lead the model to rapidly degenerate. One potential solution is to gradually increase the weight and conduct multi-stage optimization.

## 7 Conclusion

We introduce GDPO, a novel policy gradient method for learning graph DPMs that effectively addresses the problem of graph generation under given objectives. Evaluation results on both general and molecular graphs indicate that GDPO is compatible with complex multi-objective optimization and achieves state-of-the-art performance on a series of representative graph generation tasks. We discuss some limitations of our work in Appendix A.2. Our future work will investigate the theoretical gap between GDPO and DDPO in order to obtain effective unbiased estimators.