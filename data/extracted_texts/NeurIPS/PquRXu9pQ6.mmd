# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Despite the remarkable flexibility and performance of C-MCR, its broader applications are hindered by a critical limitation: C-MCR mainly focuses on learning a new space for the two non-overlapping modalities, while the modality alignments in powerful original pre-trained spaces are forgotten. As a result, C-MCR faces challenges in conducting continuous connection operations and fully utilizing all the knowledge in unified representation spaces. Therefore, it is difficult for C-MCR to build a unified embedding space, especially with more than three modalities.

This paper introduces **E**xtending **M**ulti-modal **C**ontrastive **R**epresentations (Ex-MCR), a novel training-efficient and paired-data-free unified representation learning method with excellent modality extensibility. Ex-MCR better preserves the alignment within the original pre-trained space and enhances the overall learning pipeline to align different spaces more robustly. By inheriting and reorganizing existing knowledge of the representation space, Ex-MCR achieves low training costs and data requirements. Furthermore, when used in conjunction with large-scale pre-training methods, Ex-MCR can complementarily enhance the unified representation space. Specifically, the two important designs of Ex-MCR are discussed in detail below:

1. We extend one space (called leaf space) into another fixed space (called base space) rather than connecting two pre-trained spaces to a new space. Such a simple yet effective approach maximizes the preservation of modality alignment within base space, demonstrating great potential for augmenting existing unified space and integrating more pre-trained spaces.

2. We enhance the whole learning pipeline to promote stronger alignment across different spaces. Specifically: From a training data perspective, since another modality cannot fully represent semantic information in one modality, we treat different modalities as queries to retrieve pseudo-data pairs (so-called different mode-centric data) and combine them to form a comprehensive view of multimodal semantic alignment. From the architecture perspective, we propose a decoupled projector, which reduces interference among different optimization objectives. From the learning objective perspective, we employ a dense contrastive loss on pseudo-pairs between all possible modalities pairs, further enhancing the stability of learned alignments.

Utilizing Ex-MCR, we can flexibly align multiple leaf spaces onto the same base space without any paired data and with extremely low training costs. To evaluate the effectiveness of our Ex-MCR, we try to extend pre-trained 3D-image and audio-text spaces onto image-text space via the overlapping image and text modality, which derive unified audio-image-text-3D representations. Without using any paired data, Ex-MCR attains state-of-the-art performance results across various zero-shot tasks, including audio-visual, 3D-image, audio-text, visual-text retrieval, and 3D object classification. More importantly, semantic alignment is also observed between extended modalities (e.g., audio-3D), which highlights the potential of Ex-MCR in modality extensibility.

Our contributions can be summarized as three-fold:

(1) We propose **E**xtending **M**ulti-modal **C**ontrastive **R**epresentations (Ex-MCR), a novel training-efficient and paired-data-free representation learning method for more than three modalities. Moreover, Ex-MCR is orthogonal and complementary to previous data-driven methods, combining both can bring an enhanced space.

(2) We comprehensively augment the entire space alignment learning pipeline from the perspectives of training data, architecture, and learning objectives. These novel designs offer valuable insights about effectively integrating knowledge within existing spaces.

(3) Leveraging pre-trained models like CLIP, CLAP, and ULIP, we extend audio and 3D to image-text space and obtain high-quality unified audio-image-text-3D representations. These representations exhibit advanced performance on a series of tasks.

## 2 Related Works

### Multi-Modal Contrastive Representations

Multi-modal Contrastive Representations (MCR) learning aims to acquire semantically aligned cross-modal representations by pretraining the model on large-scale paired data. These aligned representations play a pivotal role in downstream comprehension and generation tasks. Inspired by the success of CLIP , many works try to learn contrative representations for two modalities [20; 21; 22; 23; 24]. CLIP  and ALIGN  learn shared image-text representations from million-levelimage-text pairs. CLAP [26; 27] learns the audio-text representation, and CAV-MAE  focus on acquiring shared audio-visual feature space. C-MCR  focuses on learning new representation space by connecting the pre-trained spaces through overlapping modality.

Apart from aligning two modalities, shared representations for more than three modalities attract increasing attention. AudioCLIP  and WAV2CLIP  train an audio encoder aligned with CLIP using audio-text-image triplets data. ULIP [3; 4] and openshape  construct 3D-image-text triplets data through rendering 3D mesh into 2D images and captioning images for textual description, thereby learning a corresponding 3D encoder for image-text MCR space. Furthermore, Imagebind  exclusively utilizes data pairs between various modalities and images to expand CLIP with multiple modal alignment encoders.

However, these methods heavily rely on large-scale, high-quality paired data collected from the internet or generated automatically and exceptionally high computational resources. Due to the lack of high-quality paired data for more modal combinations, such as audio-visual and text-3D, the extensibility of representation learning is notably constrained. Furthermore, the exceedingly high computational costs also diminish the flexibility of MCR learning.

### Audio-Visual-Text and 3D-Visual-Text Learning

Audio-visual-text and 3D-visual-text learning have significant applications in multi-modal recognition [30; 31; 32; 33], localization [34; 35; 36; 37; 38; 39; 40; 417], question-answer [11; 10; 42; 43; 44], and generation [45; 46; 47; 48; 49; 50]. They also play important roles in robot-related tasks such as human-machine interaction and synthetical information obtaining in complex environments [51; 52].

Previous unified spaces, such as AudioCLIP  and ULIP [3; 4], mainly focus on automatically collecting or generating more paired data, but they are limited by the relatively low quality of the training datasets. Imagebind  employed individual vision-aligned data instead of triplets but pre-training the encoders from scratch results in high computational costs. FreeBind  and OmniBind  achieve strong modality alignment by integrating representation spaces that simultaneously contain multiple instances of the same modality. These two methods mainly focus on enhancing modality alignment within existing spaces, whereas Ex-MCR is a training paradigm designed to construct new modality alignments. Our approach uses paired-free data and minimal computational resources, yet it still achieves superior performance in audio-image-text and 3D-image-text retrieval. More importantly, Ex-MCR is orthogonal to existing data-driven solutions, allowing it to be flexibly used in parallel with the large-scale pre-training unified space for even stronger performance.

## 3 Extending Multi-modal Contrastive Representations

### Extending Rather Than Connecting

Given two pre-trained MCR spaces on modalities \((,)\) and \((,)\), C-MCR  employs two projectors to map them into a new shared space, where the alignment of different spaces can be learned from overlapping modality \(\). Since each pre-trained space intrinsically contains the alignment of \((,)\) and \((,)\), the alignment learned from overlapping modality theoretically can be transferred to the non-overlapping modalities.

Specifically, for aligning different spaces, the embeddings of \(\) are aligned in the new space, and pseudo \((,)\) pairs retrieved by the same data of \(\) are also aligned for a more comprehensive inter-space alignment. Moreover, the embeddings of different modalities within the same space are realigned to close the modality gap , which significantly enhances the transferability of learned inter-space alignment. C-MCR shows remarkable flexibility and versatility since connecting two existing spaces only requires two learnable projectors and unpaired unimodal data.

However, as C-MCR is designed to learn a new latent space for the two non-overlapping modalities (\(\), \(\)) and projects them onto this space, a significant amount of information from their original spaces is lost in the projection process.. As a result, it faces challenges in concurrently establishing connections among three or more spaces. Therefore, C-MCR is not suitable for learning a unified representation space for more than three modalities.

To learn unified multi-modal representations in a training-efficient and paired-data-free manner, we propose to extend one space into another space rather than connect two spaces to a new space. Considering the two spaces on modalities \((,)\) and \((,)\), Ex-MCR chooses one as the base space \((,)\), and the other as the leaf space \((,)\). In the "Extending" scheme, the base space is frozen, and we only train one projector to map leaf space to base space via the overlapping modalities \(\). Specifically, we employ the native pairs of \(\) and pseudo pairs generated by \(\) to align leaf space to base space. Simultaneously, we close the modality gap between \((,)\) modalities of leaf space, thereby facilitating more transferable alignments.

In contrast to C-MCR, Ex-MCR can conveniently expand more spaces and learn unified representation for three or more modalities. Benefiting from efficient training and no need for paired data, we can flexibly align multiple leaf spaces to the same base space. In addition to explicitly establishing alignment among modalities of leaf space and base space, semantic alignment also emerges between extended modalities. Ex-MCR employs base space as a bridge for achieving semantic alignment among modalities in multiple leaf spaces.

### Enhancing Alignment Learning Pipeline

Before delving into the details of our learning pipeline, we first clarify the necessary symbols and notations. We align the ULIP (3D-image) and CLAP (audio-text) onto CLIP (image-text). As shown in Fig. 1 (a), the unimodal data of audios \(A\), texts \(T\), images \(V\), and 3D point clouds \(P\) are input to their corresponding encoders, and the set of the extracted feature is denoted as \(^{A}\), \(^{A}\), \(^{I}\), \(^{I}\), \(^{U}\) and \(^{U}\), where superscripts \({}^{A}\), \({}^{I}\), \({}^{U}\) indicate representation space of CLAP, CLIP, ULIP, respectively. The \(^{A}=\{_{1}^{A},_{2}^{A},_{n_{a}}^ {A}\}\) where \(n_{a}\) is the number of all audio data and \(_{i}^{A}\) represents the CLAP feature of \(i\)-th audio. Similarly, there are \(_{i}^{A}\), \(_{i}^{I}\), \(_{i}^{I}\), \(_{i}^{U}\), \(_{i}^{U}\) in \(^{A}\), \(^{I}\), \(^{I}\), \(^{U}\), \(^{U}\) respectively.

In Ex-MCR, freezing base space allows us to maintain the original alignment of base space but also implies that the modality gap within base space is preserved. Consequently, it becomes necessary to map the leaf space to more suitable positions within the base space. To this end, we enhance the entire alignment learning pipeline from perspectives of data, architecture, and learning objectives.

#### 3.2.1 Various Modality-centric Data

C-MCR only uses data of overlapping modality to retrieve semantically similar embeddings of other modalities and treats these generated embeddings as pseudo pairs (we call single modality-centric data). However, it is difficult to fully represent one modality with another, and retrieved embeddings by one modality often ignore some semantics of other modalities. For example, images about

Figure 1: **The pipeline of Ex-MCR. (a) We extend leaf spaces to base space via the overlapping modalities. The base space is frozen and the leaf spaces are aligned to the base space via projectors. (b) When extending the audio-text space to the text-image space, we iteratively use texts, audio, and images as queries to retrieve and aggregate the corresponding semantically consistent embeddings. The pseudo embedding pairs generated from different modality data are shuffled together to build the final various modality-centric data pool.**

"mushrooms" tend to be absent when retrieving embeddings by audio, and audio of "wind noise" may be ignored in embeddings aggregated by images. Therefore, aggregating embeddings from only a single modality struggles to capture the entire representation space of different modalities.

To tackle the above problem, we propose various modality-centric data strategy. By ensembling semantic consistent embeddings aggregated by multiple modalities, the final embeddings can reflect the representation space of different modalities in different MCRs more comprehensively. As depicted in Fig.1 (b), all modalities in two spaces are iteratively employed as queries to aggregate corresponding semantic consistent embeddings. Take aligning audio-text space to text-image space as an example, the consistent embeddings based on overlapping modality (e.g., text) are aggregated as follows:

\[}_{i}^{A}=_{i}^{A};& }_{i}^{A}=((}_{i}^{A} ^{A})/_{1})(^{A})^{T}\\ }_{i}^{I}=_{i}^{I};& }_{i}^{I}=((}_{i}^{I} ^{I})/_{1})(^{I})^{T}\] (1)

where the \(_{1}\) is the temperature parameter of softmax, and the softmax is over all the samples in used datasets. The tilde symbols mean the features are processed to be semantically consistent. The \(}_{i}^{A}\) and \(}_{i}^{I}\) are derived from the same text data, and their semantics are natively consistent. Benefiting from the modality semantic alignment within each pre-trained space, the generated \(}_{i}^{A}\) and \(}_{i}^{I}\) are also semantically relevant to the \(}_{i}^{A}\) and \(}_{i}^{I}\).

To capture the representation space of non-overlapping modality more comprehensively, we further aggregate semantic consistent embeddings via data of non-overlapping modality (e.g., audio and image). The process of generating embeddings based on audio can be expressed as:

\[}_{i}^{A}=_{i}^{A};& }_{i}^{I}=((}_{i}^{I} ^{I})/_{1})(^{I})^{T}\\ }_{i}^{A}=((}_ {i}^{A}^{A})/_{1})(^{A})^{T};\,}_{i}^{I}=((}_{i}^{A}^{A})/_{1})(^{I})^{T}\] (2)

Since the embeddings of \(^{A}\) and \(^{I}\) of overlapping modality are one-to-one matched, the similarity weights between \(}_{i}^{A}\) and \(^{A}\) can be naturally transferred to \(^{I}\).

Based on the aforementioned formulas, when extending audio-text to text-image, we iteratively employ texts, audios, and images as queries to aggregate corresponding semantic consistent embeddings. During training, semantic consistent embeddings from different sources are shuffled together and the final data pool of various modality-centric data can be represented as \(\{}_{i}^{A},}_{i}^{I},}_{i}^ {A},}_{i}^{I}\}_{i=0}^{n}\).

#### 3.2.2 Decoupled Projector

The main network structure of Ex-MCR is a projector, and it serves two purposes: 1) Learning the intra-space alignment to close the modality gaps within leaf space and prompt more stable alignment between spaces. 2) Learning the inter-space alignment for extending leaf space to base space. Considering these two different purposes, we propose a decoupled projector to alleviate the potential conflict between distinct optimization objectives and explore a more reasonable mapping layer design for these two purposes. As shown in Fig.1, the projector is decoupled into a linear layer \(f_{l}()\) for intra-space alignment and a multi-layer perceptron layer \(f_{m}()\) for inter-space alignment. For extending CLAP to CLIP, we first use \(f_{l}\) to align \(}_{i}^{A}\) to \(}_{i}^{A}\), the loss function is defined as:

\[L_{intra}=_{i=1}^{B} f_{l}(} _{i}^{A})-}_{i}^{A})_{2}\] (3)

With the intra-space alignment loss, \(f_{l}()\) learns the mapping between audio subspace and text subspace within the CLAP, thereby effectively closing the modality gap. Since the subspaces of different modalities within pre-trained spaces are very similar, linear mapping is enough to bridge the modality gap. Moreover, our experiments even found that activation layers hurt bridging the modality gap.

After bridging the modality gap, the shared \(f_{m}()\) are employed to map both audio and text embeddings of CLAP space to the CLIP space, which can be expressed as:

\[}_{i}^{A}=f_{m}(f_{l}(}_{i}^{A}));\,\,\,}_{i}^{A}=f_{m}(}_{i}^{A})\] (4)

#### 3.2.3 Dense Alignment Objective

Since the modality gap within the base space is still preserved, a more robust learning objective is needed to map leaf space to the appropriate position in the base space. To this end, we propose to learn the alignment densely among the quadruple semantic consistent embedding pairs described in Sec.3.2.1. When extending CLAP to CLIP, the dense inter-space alignment losses are defined as:

\[ L_{avc}=(}^{A}, {}^{I});& L_{tvc}=(}^{A},}^{I})\\ L_{ate}=(}^{A}, }^{I});& L_{ttc}=(}^{A},}^{I})\] (5)

where the \((,)\) is the standard contrastive loss function, which is defined as:

\[(,)=-_{i=1}^{B}[ _{i}_{i})/_{2})}{_{j=1}^{B} ((_{i}_{j})/_{2})}+_{i}_{i})/_{2})}{_{j=1}^{B}((_{i} _{j})/_{2})}]\] (6)

where the \(_{2}\) is the temperature parameter. The overall loss is defined as a weighted combination of the intra-space and inter-space losses:

\[L= L_{intra}+(L_{avc}+L_{atc}+L_{tvc}+L_{ttc})\] (7)

where \(\) is the hyper-parameter to balance the two terms.

Various modality-centric data 3.2.1, decoupled projector 3.2.2, and dense alignment loss 3.2.3 are also symmetrically employed to extend the 3D-image space to image-text space via images. As a result, we obtain a unified 3D-image-text-audio representation. Considering audio, text, image, and 3D point cloud inputs, we use CLAP's audio encoder, CLIP's text and image encoder, and UILP's 3D encoder to extract corresponding features \(_{i}^{A}\), \(_{i}^{I}\), \(_{i}^{I}\), \(_{i}^{U}\). The \(_{i}^{I}\), \(_{i}^{I}\), \(f_{m}^{A}(f_{i}^{A}(_{i}^{A}))\), \(f_{m}^{U}(f_{i}^{U}(_{i}^{U}))\) are the final audio-text-image-3D unified representation learned by Ex-MCR, where the \(f_{m}^{A}(),f_{i}^{A}();f_{m}^{U}(),f_{l}^{U}()\) are the learned projectors of CLAP and ULIP respectively.

    &  &  &  \\  &  &  &  &  &  \\   & R@1 & R@5 & R@1 & R@5 & R@1 & R@5 & R@1 & R@5 & R@1 & R@5 \\ CLAP & - & - & - & - & - & - & - & - & - & - & - & - \\ CLIP & 1.37 & 4.91 & 0.61 & 2.65 & 1.25 & 3.94 & 3.53 & 11.30 & 17.51 & 37.50 \\ AudioCLIP & 0.82 & 3.41 & 0.95 & 4.23 & 2.51 & 1.02 & 0.88 & 4.22 & 40.24 & 64.78 \\ ImageBind & 7.68 & 20.78 & **18.00** & **40.11** & 14.82 & 35.67 & 9.24 & 24.74 & 57.28 & **79.54** \\ C-MCR\({}_{CLIP-CLAP}\) & 1.39 & 5.97 & 1.25 & 4.49 & 1.94 & 7.69 & 15.76 & 41.37 & 16.67 & 37.04 \\  Ex-MCR-base & 1.57 & 5.95 & 1.40 & 4.94 & 2.13 & 8.12 & 19.07 & 47.05 & 40.24 & 64.78 \\ Ex-MCR-huge & 1.80 & 6.16 & 1.89 & 7.36 & 3.26 & 11.77 & **26.95** & **59.60** & **57.28** & **79.54** \\ Ex-MCR-huge + ImageBind & **7.92** & **21.26** & 17.11 & 38.95 & **15.49** & **37.55** & 18.34 & 47.44 & **57.28** & **79.54** \\   

Table 1: Results of audio-image-text experiments. The best results are **bolded**.

    &  &  &  \\  &  &  &  \\   & Acc@1 & Acc@3 & Acc@5 & R@1 & R@5 & R@1 & R@5 \\ CLIP & - & - & - & - & 40.24 & 64.78 \\ ULIP & 60.40 & 79.00 & 84.40 & 1.45 & 4.51 & 28.69 & 53.14 \\ ULIP v2 & **73.06** & 86.39 & 91.50 & **6.00** & **15.63** & 28.69 & 53.14 \\ C-MCR\({}_{CLIP-ULIP}\) & 64.90 & 87.00 & 92.80 & 1.36 & 4.80 & 24.53 & 48.25 \\  Ex-MCR-base & 66.53 & **87.88** & **93.60** & 2.54 & 8.25 & **40.24** & **64.78** \\   

Table 2: Results of 3D-image-text experiments.

Experiment

### Experimental Setting

DatasetsFor a fair comparison, we use the same unimodal datasets to C-MCR  for training, totaling 2.31M texts, 1.3M images, 1.8M audio, and 0.8M 3D point clouds. More details about training datasets are provided in the Appendix.

Implementation DetailsFor Ex-MCR-base, We employ pre-trained frozen CLIP ViT-B/32 , CLAP , and ULIPv2 (PointBERT version)  models. We also extend CLAP's audio encoder to OpenCLIP ViT-H  to build the Audio-Image-Text space Ex-MCR-huge in parallel with ImageBind . The temperature \(_{1}\) in Eq.12 for embedding aggregation is set to 0.01 following , while the \(_{2}\) in Eq.6 is set to 0.05. The hyper-parameter \(\) in Eq.7 is set to 0.1. Following , we also add Gaussian noise with a variance of 0.004 to the semantic consistent embeddings described in Sec.3.2.1. The linear projector \(f_{l}()\) is a simple linear layer, and the MLP projector \(f_{m}()\) is a 2-layer MLP. We train our model with a batch size of 4096 for 36 epochs. We employ the AdamW optimizer with an initial learning rate of 1e-3 and a cosine learning rate decay strategy.

### Audio-Image-Text Results

Downstream TasksWe employ zero-shot audio-image, audio-text, and image-text retrieval tasks to evaluate the audio-image-text representations of Ex-MCR. For audio-image retrieval, we conduct evaluations on Flickr-SoundNet , VGGSSS , and AVE  datasets. Due to their small dataset sizes, we utilize all their available data, comprising 5,000, 5,000, and 4,097 samples. For audio-text retrieval, we utilize the validation set from the AudioCaps  dataset, which includes 964 audio samples, and for each audio, there are 5 corresponding captions for retrieval. Regarding image-text retrieval, we employ the validation set of COCO  dataset, consisting of 5,000 images and 25,014 text captions. We calculate the cosine similarity between modalities in representation space and use Top-1 and Top-5 metrics for performance comparison.

Performance ComparisonIn the upper part of Fig.1, we compare Ex-MCR-base to WAV2CLIP, AudioCLIP, and C-MCR. Notably, even without using audio-image paired data, Ex-MCR-base achieves significantly better performance over WAV2CLIP and AudioCLIP, which illustrates that Ex-MCR is a more effective representation learning method when high-quality data pairs are limited. Furthermore, compared to C-MCR, Ex-MCR not only achieves better audio-image alignment but also inherits more audio-text alignment from CLAP, fully retaining CLIP's image-text modal alignment, suggesting that Ex-MCR is generally superior to C-MCR in both establishing new Spaces and maintaining original ones. We then compare the performance of Ex-MCR-huge and data-driven alignment-building methods in the bottom half of Fig.1. Inheriting the audio-text alignment of CLAP, Ex-MCR-huge achieved better results on audio-text retrieval tasks, while ImageBind, trained directly with Audio-Image pairing data, has better audio-image performance. We were pleasantly surprised to find that using Ex-MCR and data-driven methods in parallel, with very little additional cost, can complement each other to achieve a state-of-the-art unified audio-image-text representation.

### 3D-Image-Text Results

Downstream TasksTo evaluate the performance of 3D-image-text space learned by extending ULIP to CLIP, we conduct a zero-shot 3D object classification task to assess the alignment between 3D and text. We also perform zero-shot 3D-image and image-text retrieval tasks to evaluate the alignment between 3D and image, as well as image and text. The zero-shot 3D object classification task is carried on the ModelNet40  validation set, and we use the same prompt strategy as . Regarding the zero-shot 3D-image retrieval task, we use the Objaverse-LVIS dataset , which includes 46,054 3D objects. Additionally, we continued to use the COCO dataset's validation set for zero-shot image-text retrieval.

It is worth noting that ULIP aligns a 3D encoder to a vision-language model called SLIP  (not CLIP) through 3D-image-text data. Ex-MCR only uses the aligned 3D-image representation of ULIP to extend it to a different vision-language model (i.e., CLIP) via the paired-data-free way. So we are not reproducing or refining the alignment of ULIP, but building a new alignment from scratch between the 3D representation of ULIP and CLIP.

[MISSING_PAGE_EMPTY:8]

**Various modality-centric data** As described in Sec.3.2.1, we employ various modality-centric data to train our projectors. For investigating the effect of different modality-centric data, we ablate each modality-centric data, and the results are reported in Tab.3. The A, I, and T represent pseudo data derived from audio, image, and text respectively. Each kind of data is beneficial for audio-visual and audio-image alignment, and using all kinds of data simultaneously brings the best performance. In addition, we find that pseudo-pairs from audios are critical to the performance of audio-text retrieval, demonstrating the importance of various modality-centric data, and proving that previous single modality-centric data really can not fully reflect the audio representation space.

**Dense alignment objective** To analyze the impact of different alignment objectives, we train the model with each alignment objective. From the results reported in Tab.4, we find that directly aligning the pseudo audio-image or audio-text embedding pairs leads to sub-optimal audio alignment, whereas aligning spaces by overlapping text modality brings better alignment than learning alignment directly from pseudo pairs. This observation further suggests that overlapping modalities play a key pivotal role in aligning different spaces.

**Structure of \(f_{l}()\)** Tab.5 demonstrates the impact of different structures of \(f_{l}()\). The results prove our hypothesis: the representation structures between different modalities within one MCR space are similar, and a simple linear layer is enough to bridge the modality gap. Moreover, the activation layer of the MLP introduces non-linearity, which may disrupt the spatial structure of representations.

**Structure of \(f_{m}()\)** The ablation studies of \(f_{m}()\) are summarized in Tab.6. When aligning different MCR spaces, the nonlinear MLP structure with stronger expressivity is better than the simple linear layer. Besides, good results are achieved no matter how many layers of MLP, which demonstrates the robustness of our method. According to more detailed experiments in Tab.11, empirically, MLP with 2 or 3 layers achieves a good balance between expressivity and learning difficulty.

Figure 3: Visualization of 3D to Audio retrieval.

Figure 2: Visualization of Audio to 3D retrieval.

**Training hyperparameters \(_{2}\) and \(\)**: The results of ablation experiments show that the performance is insensitive to the \(_{2}\) in Eq6 and \(\) in Eq7. So the picked \(_{2}\) is 0.05 which is commonly used and the picked \(\) is only to equal the absolute value of different loss terms. For detailed experimental results, please refer to the table in Appendix C.

## 5 Conclusion

This paper proposes **E**xtending **M**ulti-modal **C**ontrastive **R**epresentations (Ex-MCR), a novel training-efficient and paired-data-free unified constrastive representation learning method for more than three modalities. Ex-MCR effectively integrates the knowledge in pre-trained spaces through overlapping modalities between these spaces. By extending ULIP and CLAP onto CLIP via the overlapping image and text modality, respectively, we derive unified and high-quality audio-image-text-3D representations. Additionally, Ex-MCR provides a new view to build unified representations. Even without using paired data, Ex-MCR still achieves competitive performance, and when combined with data-driven approaches, it complementarily enhances unified representation spaces, leading to state-of-the-art results across various tasks.