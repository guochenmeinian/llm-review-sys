# Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models

Jinlin Lai, Justin Domke, Daniel Sheldon

Manning College of Information and Computer Sciences

University of Massachusetts Amherst

{jinlinlai,domke,sheldon}@cs.umass.edu

###### Abstract

Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and often requires advanced sampling techniques like Markov chain Monte Carlo (MCMC). A common approach is to write the model in a probabilistic programming language and then sample via Hamiltonian Monte Carlo (HMC). However, there are many ways a user can transform a model that make inference more or less efficient. In particular, marginalizing some variables can greatly improve inference but is difficult for users to do manually. We develop an algorithm to easily marginalize random effects in LMMs. A naive approach introduces cubic time operations within an inference algorithm like HMC, but we reduce the running time to linear using fast linear algebra techniques. We show that marginalization is always beneficial when applicable and highlight improvements in various models, especially ones from cognitive sciences1.

## 1 Introduction

Bayesian hierarchical models account for complicated relationships in data by introducing hierarchical structures . Among hierarchical models, linear mixed effects models (LMMs) are widely used in various scientific disciplines, including ecology , medicine , psychology , neuroscience  and cognitive science . Solving LMMs involves inferring latent variables, such as fixed and random effects, based on the observed data. Fixed effects are shared by all observations, while random effects vary across different groups within the data. LMMs are often implemented using probabilistic programming languages (PPLs), which isolate inference from modeling: users write a program representing the model and the PPL automatically executes a suitable inference algorithm. Variants of Hamiltonian Monte Carlo (HMC)  are dominant in many PPLs today and are widely used for LMMs. For example, BRMS  is an influential R package that allows users to write regression-style formulas that are automatically translated to Stan programs  representing an LMM, and then Stan's HMC implementation is called to generate posterior samples.

We develop techniques that allow users to easily transform their models to analytically marginalize random effect variables from LMMs to improve the efficiency of HMC. Marginalization has several benefits. First, there are often pathologies in LMMs that hinder efficient HMC sampling. A notable one is the "funnel" shape created by correlation between variance parameters and parameters for fixed or random effects . Marginalization  and other program transformations  have been shown to be useful in addressing such pathologies. Second, marginalization reduces the number \(H\) of latent variables for HMC. The complexity of HMC is about \((H^{5/4})\), so it is desirable to run HMC on a subset of variables if marginalization can be done efficiently. Our methods enable marginalization of random effects in LMMs with a linear Gaussian structure, which includes models with normal and log-normal likelihoods as well as other likelihoods for continuous data based ontransforming a normal distribution. Note that our methods are not limited to HMC, and could be applied to many inference algorithms.

There are several challenges to efficient marginalization. The automatic marginalization algorithm of  can be applied to LMMs but is limited to scalar random variables, so it requires users to construct the LMM as a graphical model with separate variables for each effect and observation. Another alternative is to model the relationships between effects and observations with a design matrix and marginalize effects using properties of multivariate normal distributions. We call this the "vectorized approach" since it can leverage vectorization to accelerate computations. Unfortunately, vectorized marginalization leads to a dense covariance matrix over the observations and thus cubic time for evaluating the log-density within HMC, when the log-density of the original could be evaluated in linear time. Our main technical contribution is to accelerate vectorized marginalization for LMMs using fast linear algebra: we show that marginalization for a single random effect can be achieved with linear time complexity and can significantly accelerate HMC compared to both the original model and non-vectorized marginalization.

We implement vectorized marginalization for LMMs in NumPyro [5; 54] via simple classes users can use to express their models. We evaluate our approach on a variety of real LMMs from past scientific investigations, including nine models and datasets from cognitive sciences, and find that marginalization is always beneficial. Our findings suggest that practitioners should marginalize group-level effects whenever applicable in Bayesian hierarchical inference.

## 2 Background

To motivate our problem, we present an example model. In , a set of experiments were run to examine the relationship between human pupil and attention load. A total of \(N=2228\) measurements of pupil sizes from \(M=20\) subjects were taken under different attention load levels. Specifically, in the \(i\)th measurement, the pupil size \(y_{i}^{+}\) of subject \(g_{i}\{1,2,...,k\}\) under attention load \(c_{i}\{0,1,2,3,4,5\}\) was recorded. Pupil size can be assumed to have linear relationship \(y_{i}_{0}+_{1}c_{i}\) with respect to the attention load \(c_{i}\), where both the slope \(_{1}\) and intercept \(_{0}\) split into fixed and random effects:

\[y_{i}=+u_{g_{i},1}+c_{i}(+u_{g_{i},2})+,\; (0,^{2}),\]

where \(,\) are variables for fixed effects and \(u_{.,}\) are variables for subject-specific random effects. Bayesian hierarchical modeling assigns priors to each unknown variable:

\[(1000,500^{2}),\;(0,100),\; ^{+}(0,1000),\;^{+}(, {diag}(1000^{2},1000^{2})),\]

\[_{u}(2,1),\;[u_{j,1},u_{j,2}]( ,_{u}_{u}^{T}),\;j=1,2,...,k.\]

A half-normal distribution (\(^{+}\)) and an LKJ distribution (LKJCholesky)  are used as a prior on the covariance matrix. Inference for the unknown parameters determining the relationship between pupil size and attention load can be performed by writing a probabilistic program and running HMC. For example, in NumPyro, the regression model for all measurements may be implemented as below.

The code above uses advanced indexing and vectorization techniques in numpy, where u,g,c,y are all vectors or matrices. We further observe that, conditioned on \(,,,,_{u}\), the distribution of all \(_{j}\) and all \(y_{i}\) form a multivariate normal distribution. Theoretically it is possible to analytically integrate \(\) out from the model to improve inference efficiency. But it is not straightforward for users to transform the probabilistic program to do so, and, as we will see, if done in the most obvious way, may not make the model more efficient for HMC.

To be more clear about how marginalization can be implemented, we rearrange the model into a canonical form that focuses on the random effects. All observations are collected into the vector \(=[y_{1},...,y_{N}]^{T}\) and random effects into the vector \(=[u_{1,1},u_{1,2},...,u_{k,1},u_{k,2}]^{T}\). Then, we can write

\[(,_{} ),\;(+,_{}),\]

Figure 1: A tree-structured model conditioned on \(\).

where \(\), \(_{}\), \(\), \(\), \(_{}\) are functions of \(,,,,_{u},g_{i},c_{i}\). Note that \(y_{i}\) only depends on the entry \(_{g_{i}}\) of \(\). The corresponding graphical model has a tree structure, as demonstrated in Figure 1. This tree structure has several benefits: first, matrix multiplications like \(\) and \(^{T}\) can be done efficiently; second, we will see that it leads to a block-diagonal structure that facilitates efficient inversion in a key matrix that appears later.

For more general LMMs with more than one class of random effects we generalize the canonical form as

\[ p(),_{i}| (_{i}(),_{_{i}}( {})), i=1,2,...,L\] \[|,_{1},_{2},..._{L} (_{i=1}^{L}_{i}() _{i}+(),_{}() ),\] (1)

where \(p()\) is the distribution for global variables (including fixed effects), \(p(_{i}|)\) is the distribution for random effects and \(p(|,_{1},...,_{L})\) is the distribution for observations. Notationally this generalization further adds an index to each random effect to specify its class. A user might specify the model directly in this canonical form, or in another syntax (e.g., the formula syntax of BRMS) that is compiled to this form. Each pair \((_{i},_{i})\) specifies a class of random effects for a particular classification of the observations (e.g., by subject, age, gender, etc.). Each classification contains multiple groups and different classifications are distinct from one another. Each observation belongs to one group for each classification. The vector \(_{i}=[_{i,1}^{T},_{i,2}^{T},...,_{i,k_ {i}}^{T}]^{T}\) contains random effects for the \(i\)th classification (e.g., subject, age, or gender), consisting of \(k_{i}\) groups (e.g., one subject, age, or gender), with \(_{i,j}\) containing the random effects (e.g., slope and intercept) for the \(j\)th group. We denote the number of observations as \(()=N\), and the number of random effects per group as \((_{i,j})=d\). Any covariates--such as \(c_{i}\) in the pupil size example--are considered constants and not represented in the notation. In LMMs, the number \(d\) is related to the number of covariates and is usually small. The total number of random effects for \(_{i}\) is denoted as \((_{i})=M_{i}=k_{i}d\). The matrix \(_{i}\) therefore has size \(N M_{i}\), and encodes the group structure for \(_{i}\) by mapping random effects (together with covariates) to observations. Each row of \(_{i}\) encodes the assignment of an observation to one group, so it has at most \(d\) nonzero elements. Therefore, the complexity of computing \(_{i}_{i}\) is \((Nd)\), as \(\) has at most \(Nd\) nonzero elements. Henceforth, we omit the dependence on \(\) for \(\), \(_{}\), \(\), \(\), \(_{}\) for simplicity.

Marginalizing \(_{i}\)It is possible to analytically marginalize variables in this model: since the mean of \(\) is linear in each \(_{i}\) and all of these variables are normally distributed, the joint distribution of \((,_{1},,_{L})\) is also multivariate normal. We will focus for most of the paper on marginalizing the random effects \(_{i}\) for a single \(i\) in order to leverage the tree structure mentioned earlier, but return in Section 4 to the idea of marginalizing many effects. Locally, \(_{i}\) and \(\) form the conditional distribution \(p(_{i},|,_{-i})=p(_{i}| {})p(|,_{-i},_{i})\). Marginalized MCMC verifies this conditional distribution as \(p(_{i},|,_{-i})=p(|,_{-i})p(_{i}|,,_{- i})\), which reverses the dependence between \(_{i}\) and \(\). During sampling, \(_{i}\) is marginalized from the HMC procedure by using \(p(|,_{-i})\) as the likelihood function and \(p(,_{-i})\) as the distribution of latent variables. After HMC sampling, \(_{i}\) is recovered through ancestral sampling from \(p(_{i}|,,_{-i})\) given posterior samples of \((,_{-i})\). The reversal requires analytical forms of \(p(|,_{-i})\) and \(p(_{i}|,,_{-i})\), which can be obtained via standard marginalization and conditioning operations on multivariate normal distributions [e.g., 6]

\[|,_{-i} (_{j i}_{j}_{j}+ _{i}_{i}+,_{i}_{_{i} }_{i}^{T}+_{}),\] \[_{i}|,,_{-i} (_{i}+(-_{j  i}_{j}_{j}-_{i}_{i}- ),(-_{i})_{i}),\] (2)

where \(=_{_{i}}_{i}^{T}(_{i}_{_{i}}_{i}^{T}+_{})^{-1}\). Marginalization introduces the benefit of sampling in a lower dimensional space, but the cost depends on the complexity of evaluating the log-density functions of these two distributions in order to run HMC.

### Challenges of multivariate marginalization

In practice, the original model usually has structure that makes evaluating its density very efficient, which is lost by naive marginalization. For example, the observations in \(\) are usually conditionally independent, making \(_{}\) diagonal; also, \(_{i}\) is usually block diagonal with blocks of size \(d d\). So evaluating the density \(p(_{i},|,_{-i})=p(_{i}| )p(|,_{1:L})\) requires \((k_{i}d^{3}+NLd)=(M_{i}d^{2}+NLd)\) time with the main operations being (1) inverting and computing the determinant of \(_{}\) and \(_{}\); (2) computing the mean parameter of \(\). When \(_{i}\) is diagonal, the complexity goes down to \((M_{i}d+NLd)\). However, it is more expensive to evaluate the density of the reversed model in Equation (2). Computing \(p(|,_{-i})\) and \(p(_{i}|,,_{-i})\) requires the inverting and computing the determinant of the \(N N\) matrix \(_{i}_{i}_{i}^{T}+ _{}\), which we denote by \(\) for simplicity. For the log likelihood, we need to compute \( p(|,_{-i})=-)}-^{T}^{-1}+C\), where \(=-_{j i}_{j}_{j}-_ {i}_{i}-\). \(\) is not diagonal and without using additional structure will trigger \((N^{3})\) operations within each step of the leapfrog integrator within HMC. For the recovery distribution \(p(_{i}|,,_{-i})\), \(\) will be inverted when calculating \(\). Also, a Cholesky decomposition for the covariance \((-_{i})_{i}\) should be computed for sampling, which takes \((M_{i}^{3})\) time. These cubic time operations are prohibitively expensive for large datasets. We summarize the complexities of different approaches in Table 1. In Section 3, we discuss how to marginalize one group of random effects with lemmas from linear algebra. In Section 4, we discuss how to marginalize all random effects with additional assumptions.

## 3 Marginalization with fast linear algebra

We now show how to speed up calculations with the marginalized model using fast linear algebra methods. In particular, we use the matrix inversion lemma and matrix determinant lemma together with special structure in the relevant matrices. In this section, we sometimes omit the subscript \(i\) such as for \(_{i}\) and \(_{i}\) for simplicity. The steps in log density evaluation and recovery are summarized in Algorithm 1, and in Algorithm 2 in the appendix, with comments about their implementation and cost. We mainly use sparsity and tree-structure in \(\) to make operations faster. As an overview, computing \(\) takes \((NLd)\) time for \(L\) sparse matrix multiplications of time \((Nd)\) each. Also, evaluating \(\) and \(^{T}\) both take \((Nd)\) for any \(^{M}\) and any \(^{N}\). With tree-structure, we will see that \(^{T}_{}^{-1}\) is block-diagonal and can be computed efficiently.

### Matrix inversion and determinant lemmas in marginalization

The two main bottlenecks when evaluating \( p(|,_{-i})\) are computing \(()\) and \(^{T}^{-1}\). With the matrix determinant lemma , we have that

\[()=(_{}^{T}+ _{})=(_{}^{-1}+^{T}_{}^{-1})(_{ })(_{}).\] (3)

By the matrix inversion lemma or the Woodbury formula  we have that

\[^{-1}=(_{}^{T}+ _{})^{-1}=_{}^{-1}-_{}^{-1}(_{}^{-1}+^{T}_{}^{-1})^{-1}^{T}_{}^{-1}.\]

Therefore,

\[^{T}^{-1}=^{T}_{}^{-1}-^{T}_{}^{-1}( _{}^{-1}+^{T}_{}^{- 1})^{-1}^{T}_{}^{-1}.\] (4)

   Submodel & Approach & Initialization & \(\) & Recovery \\  _{i},|,_{-i})\)} & No marginalization & - & \((Nd^{2}+NLd)\) & - \\  & Naive marginalization & - & \((Nd^{3}+N^{3})\) & \((M^{3}+N^{3})\) \\  & Marginalize with lemmas & - & \((Nd^{2}+NLd+Nd^{2})\) & \((Nd^{2}+NLd+Nd^{2})\) \\  ,|)\)} & No marginalization & - & \((Nd^{2}+NLd)\) & - \\  & Naive marginalization & - & \((D^{3}+N^{2})\) & \((D^{2}+N^{3})\) \\   & Marginalize with assumptions & \((D^{3}+NL^{2}d^{2})\) & \((D^{2}+NLd)\) & \((D^{2}+NLd)\) \\   

Table 1: Time complexities of different HMC approaches for the submodel involved in marginalization. Initialization is done once before the HMC loop. The log density is computed within each step of the leapfrog integrator. Recovery is performed for each sample from HMC. \(N\) is the number of observations, \(M\) is the dimension for one class of random effects, \(D\) is the dimension for all classes of random effects, \(L\) is the number of classes, \(d\) is the dimension for an effect of a group in a class.

By using the facts that \(}\) is block-diagonal, \(}\) is diagonal, and \(\) has \(Nd\) nonzero elements, the quantities \((})\), \((})\), \(^{T}^{-1}}\), and \(^{T}^{-1}}\) can each be calculated in \((Md^{2}+Nd)\) time.

Equations (3) and (4) contain the expressions \(^{-1}\) or \(()\) for the \(M M\) matrix \(:=^{-1}}+^{T}^{-1}} \), which both require \((M^{3})\) time when done naively. The following theorem shows that these quantities can be computed in \(((M+N)d^{2})\) for LMMs.

**Theorem 1**.: _If \(}\) is diagonal, \(}\) is block-diagonal with blocks of size \(d d\), then \(=^{-1}}+^{T}^{-1}} \) is also block-diagonal with \(d d\) blocks and computing \(^{T}^{-1}}\) takes \((Nd^{2})\)._

Proof.: The proof uses the tree-structure in \(\). For details, see Appendix B.1. 

Therefore, it is \(((M+N)d^{2})\) to compute \(()\) and \(^{-1}\). Combined with other parts in the formulas, the overall complexity is \((Md^{2}+NLd+Nd^{2})\). In LMMs, \(d\) is usually small, so the complexity with marginalization can be viewed as the same as the complexity without marginalization.

### Speeding up the recovery step

Different from evaluating \( p(|,_{-i})\), ancestral sampling from \(p(_{i}|,,_{-i})\) is only performed once for each posterior sample. When sampling from \(p(_{i}|,,_{-i})\), computing \(\) directly is also costly. With the matrix inversion lemma, we have

\[ =}^{T}( }^{T}+})^{-1}\] \[=}^{T}^{-1}}- }^{T}^{-1}}(^{-1}}+^{T}^{-1}})^{-1} ^{T}^{-1}}.\] (5)

With this expression, the mean variable \(+\), then is evaluated in \(((M+N)d^{2})\), by computing \(^{-1}}+^{T}^{-1}}\) in the same way as Line 2 of Algorithm 1. For the covariance variable \((-)}\), we have from substituting Equation 5 that

\[(-)} =}-}^{T}^{-1}}}+}^{T }^{-1}}(^{-1}}+^{T} ^{-1}})^{-1}^{T}^{-1} }}\] \[=}(^{-1}}-^{T} ^{-1}}+^{T}^{-1}} (^{-1}}+^{T}^{-1}} )^{-1}^{T}^{-1}})}.\]

Note that \(^{-1}}\), \(^{T}^{-1}}\), and \(=^{-1}}+^{T}^{-1}} \) are all block diagonal, so the result of adding and multiplying them is also block diagonal. For a block diagonal matrix with \(k\) blocks of size \(d d\), the time complexity for a Cholesky decomposition is \((kd^{3})=(Md^{2})\). Combined with the complexity of computing \(\), the recovery step takes \((Md^{2}+NLd+Nd^{2})\) time.

## 4 Marginalizing multiple effects with additional assumptions

We have shown that it is efficient to marginalize one class of random effects. With additional practical assumptions, it is possible to marginalize all classes of random effects for efficient HMC inference. Instead of separating different classes of random effects, LMMs can also be written as \((,}),(+,})\), where \(=[_{1},...,_{L}]\) and \(=[_{1}^{T},...,_{L}^{T}]^{T}\). We define that \(D=_{i=1}^{L}M_{i}\). The matrix inversion and determinant lemmas can still be applied to marginalize \(\) out, but the combined matrix \(\) does not have the special structure of \(_{i}\) we exploited in Section 3. More specifically, the computation of \(()\) and the evaluation of \(^{-1}\) for \(=^{-1}}+^{T}^{-1}} \) both become non-trivial. We introduce additional assumptions to show that they can be solved faster in some special cases. For the general case, see the discussion section.

The assumption we make is that \(}=}\) and \(}=}\), where \(},}\) are scalars that either belong to \(\) or are fixed non-random parameters. This means that all effects share the same variance and all observations share the same noise scale. These assumptions are not as restrictive as it may appear. If the underlying distribution is \(_{i}(_{i},_{i}^{2})\) where \(_{i}\) is a fixed parameter, it is possible to reparameterize this distribution as \(_{i}^{}(,)\), \(_{i}^{}=_{i}_{i}\), \(^{}=+_{i}\), and use \(_{i}^{},_{i}^{},^{}\) in place of \(_{i},_{i},\). Then \(}\) becomes a scaled identity matrix. Also, in many models, the noise scale for different observations is the same, making \(}\) a scaled identity matrix as well.

In practice, if the assumptions are satisfied, marginalization can be done in \((D^{2}+Nd)\) time with \((D^{3}+NL^{2}d^{2})\) preprocessing. Details are provided in Appendix B.3.

## 5 Related Work

While many works aim to improve HMC directly [71; 30; 58; 73], a number of other works focus on model transformation. Non-centered parameterization  is a widely used trick among MCMC users to alleviate slow sampling in difficult posterior distributions. However, there is no general way to know whether a non-centered parameterization will be beneficial . Variationally inferred parameterization  proposes to learn a model parameterization from a specified family that will lead to effective sampling. In Parno and Marzouk  and Hoffman et al. , preconditioners for HMC are learned to transform the model to be approximately isotropic Gaussians. Marginalization differs from reparameterization in that it reduces the problem dimension as well as potentially alleviating difficult characteristics such as funnels, so it has two mechanisms to improve MCMC efficiency. The Laplace approximation (LA) is one way to approximately marginalize variables in MCMC [59; 40; 65], but it may be difficult to quantify the error or recover the marginalized variables.

Marginalization, or Rao-Blackwellization, has been an important topic in Bayesian inference and probabilistic programming. In Gibbs sampling, marginalization is usually called collapsing . Collapsed Gibbs sampling has been developed for latent Dirichlet allocation  and LMMs . We explore marginalization in the context of HMC, which induces different considerations. Methods with HMC do not have to make the conditional distributions of the marginalized model tractable. Marginalization is also related to symbolic inference in probabilistic programming. Hakaru  and PSI [21; 22] are systems for performing exact Bayesian inference by symbolically marginalizing all latent variables. To marginalize discrete variables, Gorinova et al.  propose an information flow type system. Another line of related work is delayed sampling [43; 3], which automates marginalization of variables within Rao-Blackwellized particle filters . Lai et al.  developed an automatic system for marginalizing variables in HMC, but is limited to scalar variables so cannot leverage vectorization and forces users to write models with univariate distributions.

Linear algebra tricks have been widely utilized in various machine learning algorithms, such as ridge regression , Gaussian processes  and Kalman filters . Recently, frameworks [62; 20; 57] have been proposed to ease the implementation of fast linear algebras in machine learning algorithms. Marginalization in Bayesian models may be an interesting application of those frameworks.

Fast and scalable inference for LMMs has been studied in the context of maximum likelihood estimation , variational EM , Gibbs sampling  and numerical integration . We are the first to consider speeding up the inference of LMMs with HMC. There is also a recent trend in integrating random effects into deep neural networks for correlated data  or personalization [66; 64; 74] with parameters estimated by maximum likelihood.

## 6 Experiments

We conduct experiments on LMMs from various disciplines using the default no-U-turn sampler (NUTS)  from NumPyro [5; 54], which has an adaptive step size with dual averaging, adaptive and diagonal mass matrix, target acceptance probability of 0.8, and maximum tree depth of 10. For the ETH instructor evaluation model, we set the maximum tree depth to 12 to overcome difficulties performing inference without marginalization in preliminary experiments. For all models, we use weakly informative priors unless specified. In general, our conclusion is insensitive to the choice of hyperparameters and priors. For all experiments, we collect 10,000 warm up samples for tuning, and 100,000 samples for evaluation, and evaluate performance via effective sample size (ESS) and running time.

[MISSING_PAGE_FAIL:7]

**Grouse ticks** The dataset  contains observations \(\) of the the number of ticks on the heads of red grouse chicks in the field. Each observation \(y_{k}\) comes from brood \(b_{k}\) in location \(l_{k}\) during year \(e_{k}\) at altitude \(a_{k}\), where year and altitude give fixed effects, and there are random effects \(_{1}\) and \(_{2}\) corresponding to brood and location. There are \(N=403\) observations, \(M_{1}=118\) broods and \(M_{2}=63\) locations. We define the hierarchical model as follows:

\[:y_{k}(u_{1,b_{k}}+u_{2,l_{k}}+ _{e}e_{k}+_{a}a_{k},_{t}^{2})\] \[:_{1}(0,1),\ _{1}(5),\ _{2}(0,1),\ _{2}(5),\] \[_{e}(0,1),\ _{a}(0,1),\ u_{1,i} (_{1},_{1}^{2}),\ u_{2,j}(_{2}, _{2}^{2}),_{t}(5),\]

where \(i=1,...,M_{1}\), \(j=1,...,M_{2}\), \(k=1,...,N\) and each \(y_{k}\) is observed. The correlation between \(\) and \(\) creates the funnel shape that makes vanilla HMC inefficient. Nevertheless, it is possible to apply either marginalization or reparameterization to each random effect. In Figure 3, we plot the distributions of samples for variable pairs \((_{1},u_{1,1})\) and \((_{2},u_{2,1})\) with different combinations of marginalization and reparameterization. There is a difficult correlation between \(_{2}\) and \(_{2}\). After applying marginalization or reparameterization to \(_{2}\), HMC manages to explore the funnel region (at low values of \(_{1}\)). However, we find that only samplers that marginalize \(_{2}\) report zero divergent transitions after warm-up. Such behavior is consistent with different random seeds. See Table 6 in the Appendix. Also, the distribution of divergent samples is related to specific parameters when reparameterizing \(_{2}\), implying that reparameterization introduces pathologies that create challenges for HMC inference. In addition, we find that reparameterization does not improve the running time of HMC, while marginalizing \(_{2}\) speeds up sampling by about 20%.

### Benefits from vectorization

In theory, marginalization with LMMs can be done by constructing a graphical model for scalar random variables and performing automatic marginalization as in . But it is more efficient to marginalize in a vectorized way. We demonstrate the benefits from vectorization in Table 3. Both marginalization strategies are performed on two hierarchical linear regression models, the electric company model  and the pulmonary fibrosis model . We find that vectorized marginalization is much more efficient for sampling from the two models.

   Model & \(T_{c}\) of  & \(T_{r}\) of  & \(T_{c}\) of ours & \(T_{r}\) of ours \\  Electric company & 552 (4) & 1249 (95) & 7 (0) & 252 (23) \\ Pulmonary fibrosis & 727 (11) & 2208 (80) & 10 (1) & 178 (3) \\   

Table 3: Compilation time \(T_{c}\) and running time \(T_{r}\) in seconds for marginalized MCMC , with or without vectorization. Mean and std across 5 independ runs are reported.

Figure 3: Distribution of 10,000 samples for variable pairs \((_{1},u_{1,1})\) and \((_{2},u_{2,61})\) on the grousetrics model with different methods. We use M1 to represent marginalizing \(_{1}\), M2 to represent marginalizing \(_{2}\), R1 to represent reparameterizing \(_{1}\), R2 to represent reparameterizing \(_{2}\). The number of divergences for each case are reported, with locations shown as red dots. We choose \(u_{2,61}\) to demonstrate the distribution of divergences when reparameterizing \(_{2}\).

### Applications in cognitive sciences

Hierarchical Bayesian inference with LMMs has wide applications in cognitive science . We highlight the effectiveness of marginalization with 9 datasets from cognitive science (Table 4). They cover various settings, with one or two random effects, normal or log-normal likelihoods, on CPU or GPU. Experiments that are slow on CPU are performed on GPU. Each dataset corresponds to an LMM where both the intercept and the coefficient include random effects. Details of all the models can be found in Appendix D. Results are summarized in Figure 4. Marginalization usually improves the sampling speed of HMC and consistently improves efficiency measured by ESS per iteration.

## 7 Discussion

There are several promising directions for future work.

### Marginalization vs Rao-Blackwellization

Marginalization is related to Rao-Blackwellization. This paper focuses on marginalization, which improves the speed of obtaining samples from the remaining variables by improving mixing times, reducing the cost per iteration, or both. Combining marginalization with Rao-Blackwellization is an interesting avenue for future work. More formally, if one is interested in some expectation \(_{(,) p(, |)}[f(,)]\) in an LMM, there is a Monte Carlo estimator

\[E_{1}=_{i=1}^{N}f(^{i},^{i}),\]

where \((^{i},^{i}) p(, |)\) and \(N\) is the sample size. Marginalization is a trick to improve the efficiency of the posterior sampling, so that we can achieve the same estimation variance with smaller \(N\) or less runtime. At the same time, we also have access to a conditional distribution that is useful for Rao-Blackwellization. If the effects variable \(\) can be marginalized we have both an approximate

    &  & dutch & eg & eng & gg0 & mandarin & mandarin & pup & strogot \\  \(N\) & 2855 & 372 & 26176 & 768 & 672 & 547 & 595 & 2228 & 3058 \\ \(L\) & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 1 & 1 \\ \(M_{1}\) & 40 & 24 & 334 & 48 & 42 & 37 & 40 & 20 & 50 \\ \(M_{2}\) & 48 & 16 & 80 & 16 & 16 & 15 & 15 & - \\ Likelihood & LogNormal & Normal & Normal & Normal & LogNormal & LogNormal & Normal & LogNormal \\ Device & GPU & GPU & GPU & CPU & CPU & CPU & GPU & GPU \\   

Table 4: Specifications of the datasets from cognitive sciences. Details of each model are provided in Appendix D. GPU models run on an NVIDIA RTX 2080ti GPU. CPU models run on one Intel Xeon Gold 6148 processor.

Figure 4: Experimental results for the 9 cognitive science datasets with and without marginalization. Each experiment is performed 5 times with different random seeds. Marginalization usually improves sampling speed measured by iterations per second (iter/s) and sample efficiency measured by ESS per iteration (ESS/iter).

posterior for \(p(|)\) and an analytical conditional distribution \(p(|,)\). With Rao-Blackwellization we have that \(_{(,) p(,|)}[ f(,)]=_{ p(|)}[ _{ p(|,)}[f(,)]]\). In such case, another Monte Carlo estimator can be constructed:

\[E_{2}=_{i=1}^{N}_{ p(|,)}[f(^{i},)],\]

where \(^{i} p(|)\). For some functions, such as those that are polynomial in \(\), the inner expectation can be computed exactly using properties of Gaussians. In other cases, the inner expectation can be estimated cheaply via Monte Carlo using exact samples from \(p(|_{i},)\).

### Marginalizing multiple effects in general models

In Section 4, we proposed to marginalize multiple classes of random effects by assuming a scaled identity covariance matrix. To marginalize multiple effects in general models, a possibility is to compute \(^{T}^{-1}\) and estimate \(()\) and the corresponding gradients with conjugate gradient (CG) solvers [14; 20]. However, this approach uses stochastic estimators for the determinant and gradients, which introduce bias into the HMC dynamics. These biases can be corrected through pseudo-marginalization , but it is unclear how significantly the extra stochasticity will affect the sampling. Another possible way to marginalize multiple effects for LMMs is to introduced the balanced levels assumption . We leave these ideas for future exploration.

### Beyond normal likelihoods

In this work, we only consider normal or log-normal likelihoods, but our method can be easily generalized to other deterministic transformation of normal likelihood. This implies that marginalization can benefit regression with most continuous predictors given proper link functions. Another potential future direction is to marginalize classification models with probit regressions . Marginalization will turn probit models into multivariate probit models as \(_{}^{T}+_{}\) is a dense covariance matrix, which may require a simulation-based method  or variational Bayes . It will be interesting to see how ideas from multivariate probit regression could be fit into an HMC pipeline. In a broader context, marginalization is related to data augmentation techniques that "create" conjugacy for non-normal likelihoods or non-normal effects. Those techniques were developed for Gibbs sampling, e.g. [18; 55], but may also be useful for HMC.

### Integration with probabilistic programming

We have developed a tool to speed up the HMC inference for LMMs. In our implementation, the marginalized likelihood \(p(|,_{-i})\) is defined as a special type of parametric distribution available to the user, and the recovery distribution \(p(_{i}|,_{-i},)\) is a function called after sampling. In our experiments, marginalization never hurt sampling efficiency measured by ESS/s, and usually helped. Thus, it would be desirable to always marginalize one group of random effects when the model is an LMM. Future work could aim to automatically apply such transformations to user-specified LMMs. There are two possible high-level approaches. The first is to perform marginalization starting with a model described using a high-level abstraction such as an R formula. Then, when compiling the high-level model description into a concrete model (e.g., a probabilistic program), we can marginalize one or more of the effects using our methods. The second is to perform marginalization starting with a user-written probabilistic program representing an LMM. In this case, some compilation or program tracing technique will be needed to convert the user's program to a model representation suitable for manipulation. For example, Lai et al.  used program tracing to construct a graphical model representation that could be programmatically analyzed and transformed. To apply this methodology to LMMs, a special parser would also be needed to match the models to LMMs.