# MomentDiff: Generative Video Moment Retrieval

from Random to Real

 Pandeng Li\({}^{1}\), Chen-Wei Xie\({}^{2}\), Hongtao Xie\({}^{1}\), Liming Zhao\({}^{2}\),

**Lei Zhang\({}^{1}\), Yun Zheng\({}^{2}\), Deli Zhao\({}^{2}\), Yongdong Zhang\({}^{1}\)**

\({}^{1}\) University of Science and Technology of China, Hefei, China

\({}^{2}\) Alibaba Group

lpd@mail.ustc.edu.cn, {htxie, leizh23, zhyd73}@ustc.edu.cn

{eniac.xcw, lingchen.zlm, zhengyun.zy}@alibaba-inc.com

zhaodeli@gmail.com

Interns at Alibaba GroupCorresponding author

###### Abstract

Video moment retrieval pursues an efficient and generalized solution to identify the specific temporal segments within an untrimmed video that correspond to a given language description. To achieve this goal, we provide a generative diffusion-based framework called MomentDiff, which simulates a typical human retrieval process from random browsing to gradual localization. Specifically, we first diffuse the real span to random noise, and learn to denoise the random noise to the original span with the guidance of similarity between text and video. This allows the model to learn a mapping from arbitrary random locations to real moments, enabling the ability to locate segments from random initialization. Once trained, MomentDiff could sample random temporal segments as initial guesses and iteratively refine them to generate an accurate temporal boundary. Different from discriminative works (_e.g.,_ based on learnable proposals or queries), MomentDiff with random initialized spans could resist the temporal location biases from datasets. To evaluate the influence of the temporal location biases, we propose two "anti-bias" datasets with location distribution shifts, named Charades-STA-Len and Charades-STA-Mom. The experimental results demonstrate that our efficient framework consistently outperforms state-of-the-art methods on three public benchmarks, and exhibits better generalization and robustness on the proposed anti-bias datasets. The code, model, and anti-bias evaluation datasets are available at https://github.com/IMCCretrieval/MomentDiff.

## 1 Introduction

Video understanding  is a crucial problem in machine learning , which covers various video analysis tasks, such as video classification and action detection. But both tasks above are limited to predicting predefined action categories. A more natural and elaborate video understanding process is the ability for machines to match human language descriptions to specific activity segments in a complex video. Hence, a series of studies  are conducted on Video Moment Retrieval (VMR), with the aim of identifying the moment boundaries ( _i.e.,_ the start and end time) within a given video that best semantically correspond to the text query.

As shown in Fig. 1(a), early works address the VMR task by designing predefined dense video proposals ( _i.e.,_ sliding windows , anchors  and 2D map  ). Then, the prediction segment is determined based on the maximum similarity score between dense proposals and thequery text. However, these methods have a large redundancy of proposals and the numbers of positive and negative proposals are unbalanced, which limits the learning efficiency . To deal with this problem, a series of VMR works  have recently emerged, mainly discussing how to reduce the number of proposals and improve the quality of proposals. Among them, a promising scheme (Fig. 1(b)) is to use sparse and learnable proposals  or queries  (_i.e.,_ soft proposals) to model the statistics of the entire dataset and adaptively predict video segments. However, these proposal-learnable methods rely on a few specific proposals or queries to fit the location distribution of ground truth moments. For example, these proposals or queries may tend to focus on video segments where locations in the dataset occur more often (_i.e.,_ yellow highlights in Fig. 1(b)). Thus, these methods potentially disregard significant events that transpire in out-of-sample situations. Recent studies  indicate that VMR models  may exploit the location biases present in dataset annotations , while downplaying multimodal interaction content. This leads to the limited generalization of the model, especially in real-world scenarios with location distribution shifts.

To tackle the above issues, we propose a generative perspective for the VMR task. As shown in Fig. 1 (c) and (d), given an untrimmed video and the corresponding text query, we first introduce several random spans as the initial prediction, then employ a diffusion-based denoiser to iteratively refine the random spans by conditioning on similarity relations between the text query and video frames. A heuristic explanation of our method is that, it can be viewed as a way for humans to quickly retrieve moments of interest in a video. Specifically, given an unseen video, instead of watching the entire video from beginning to end (which is too slow), humans may first glance through random contents to identify a rough location, and finally iteratively focus on key semantic moments and generate temporal coordinates. In this way, we do not rely on distribution-specific proposals or queries (as mentioned in the above discriminative approaches) and exhibit more generalization and robustness (Tab. 5 and Fig. 4) when the ground truth location distributions of training and test sets are different.

To implement our idea, we introduce a generative diffusion-based framework, named MomentDiff. Firstly, MomentDiff extracts feature embeddings for both the input text query and video frames. Subsequently, these text and video embeddings are fed into a similarity-aware condition generator. This generator modulates the video embeddings with text embeddings to produce text-video fusion embeddings. The fusion embeddings contain rich semantic information about the similarity relations between the text query and each video frame, so we can use them as a guide to help us generate predictions. Finally, we develop a Video Moment Denoiser (VMD) that enhances noise perception and enables efficient generation with only a small number of random spans and flexible embedding learning. Specifically, VMD directly maps randomly initialized spans into the multimodal space,

Figure 1: (a) Proposal-predefined methods. Yellow highlights in the timeline represent frequently occurring moments in the dataset. (b) Proposal-learnable methods. (c) Our generative method. (d) We model the VMR task as a process of gradually generating real temporal span from random noise.

taking them as input together with noise intensities. Then, VMD iteratively refines spans according to the similarity relations of fusion embeddings, thereby generating true spans from random to real.

Our main contributions are summarized as follows. 1) To the best of our knowledge, we are the first to tackle video moment retrieval from a generative perspective, which does not rely on predefined or learnable proposals and mitigates temporal location biases from datasets. 2) We propose a new framework, MomentDiff, which utilizes diffusion models to iteratively denoise random spans to the correct results. 3) We propose two "anti-bias" datasets with location distribution shifts to evaluate the influence of location biases, named Charades-STA-Len and Charades-STA-Mom. Extensive experiments demonstrate that MomentDiff is more efficient and transferable than state-of-the-art methods on three public datasets and two anti-bias datasets.

## 2 Related Work

**Video Moment Retrieval.** Video moment retrieval [32; 31; 41; 42; 33; 43; 44; 45; 46] is a newly researched subject that emphasizes retrieving correlated moments in a video, given a natural language query. Pioneering works are proposal-based approaches, which employ a "proposal-rank" two-stage pipeline. Early methods [23; 25; 26; 27; 32] usually use handcrafted predefined proposals to retrieve moments. For example, CTRL  and MCN  aim to generate video proposals by using sliding windows of different scales. TGN  emphasizes temporal information and develops multi-scale candidate solutions through predefined anchors. 2DTAN  designs a 2D temporal map to enumerate proposals. However, these dense proposals introduce redundant computation with a large number of negative samples [28; 36]. Therefore, two types of methods are proposed: 1) Proposal-free methods [17; 31; 47] do not use any proposals and are developed to directly regress start and end boundary values or probabilities based on ground-truth segments. These methods are usually much faster than proposal-based methods. 2) Proposal-learnable methods that use proposal prediction networks [34; 35; 36] or learnable queries [37; 38] to model dataset statistics and adaptively predict video segments. QSPN  and APGN  adaptively obtain discriminative proposals without handcrafted design. LPNet  uses learnable proposals to alleviate the redundant calculations in dense proposals. MomentDETR  can predict multiple segments using learnable queries. Since proposal-learnable methods adopt a two-stage prediction [34; 35; 36] or implicit iterative  design, the performance is often better than that of proposal-free methods. However, proposal-learnable methods explicitly fit the location distribution of target moments. Thus, models are likely to be inclined to learn location bias in datasets [39; 40], resulting in limited generalization. We make no assumptions about the location and instead use random inputs to alleviate this problem.

**Diffusion models.** Diffusion Models [48; 49; 50; 51] are inspired by stochastic diffusion processes in non-equilibrium thermodynamics. The model first defines a Markov chain of diffusion steps to slowly add random noise to the data, and then learns the reverse diffusion process to construct the desired data samples from the noise. The diffusion-based generation has achieved disruptive achievements in tasks such as vision generation [52; 53; 54; 55; 56; 57; 58] and text generation . Motivated by their great success in generative tasks , diffusion models have been used in image perception tasks such as object detection  and image segmentation . However, diffusion models are less explored for video-text perception tasks. This paper models similarity-aware multimodal information as coarse-grained cues, which can guide the diffusion model to generate the correct moment boundary from random noise in a gradual manner. Unlike DiffusionDet , we avoid a large number of Region of Interest (ROI) features and do not require additional post-processing techniques. To our knowledge, this is the first study to adapt the diffusion model for video moment retrieval.

## 3 Method

In this section, we first define the problem in Sec. 3.1, introduce our framework in Sec. 3.2, and describe the inference process in Sec. 3.3.

### Problem Definition

Suppose an untrimmed video \(=\{_{i}\}_{i=1}^{N_{v}}\) is associated with a natural text description \(=\{_{i}\}_{i=1}^{N_{t}}\), where \(N_{v}\) and \(N_{t}\) represent the frame number and word number, respectively. Under this notation definition, Video Moment Retrieval (VMR) aims to learn a model \(\) to effectively predict the moment \(}_{0}=(}_{0},}_{0})\) that is most relevant to the given text description: \(}_{0}=(,)\), where \(}_{0}\) and \(}_{0}\) represent the center time and duration length of the temporal moments, _i.e.,_ predicted spans.

### The MomentDiff Framework

Fig. 2 sheds light on the generation modeling architecture of our proposed MomentDiff. Concretely, we first extract frame-level and word-level features by utilizing pre-trained video and text backbone networks. Afterward, we employ a similarity-aware condition generator to interact text and visual features into fusion embeddings. Finally, combined with the fusion embeddings, the video moment denoiser can progressively produce accurate temporal targets from random noise.

#### 3.2.1 Visual and Textual Representations.

Before performing multimodal interaction, we should convert the raw data into a continuous feature space. To demonstrate the generality of our model, we use three distinct visual extractors [32; 44] to obtain video features \(\): 1) 2D visual encoder, the VGG model . 2) 3D visual encoder, the C3D model . 3) Cross-modal pre-train encoder, the CLIP visual model . However, due to the absence of temporal information in CLIP global features, we additionally employ the SlowFast model  to extract features, which concatenate CLIP features. Besides, to take full advantage of the video information , we try to incorporate audio features, which are extracted using a pre-trained PANN model . To obtain text features, we try two feature extractors: the Glove model  and the CLIP textual model to extract 300-d and 512-d text features \(\), respectively.

#### 3.2.2 Similarity-aware Condition Generator

Unlike generation tasks  that focus on the veracity and diversity of results, the key to the VMR task is to fully understand the video and sentence information and to mine the similarities between text queries and video segments. To this end, we need to provide multimodal information to cue the denoising network to learn the implicit relationships in the multimodal space.

A natural idea is to interact and aggregate information between video and text sequences with a multilayer Transformer . Specifically, we first use two multilayer perceptron (MLP) networks to map feature sequences into the common multimodal space: \(^{N_{v} D}\) and \(^{N_{t} D}\), where \(D\) is the embedding dimension. Then, we employ two cross-attention layers to perform interactions between multiple modalities, where video embeddings \(\) are projected as the query \(_{v}\), text embeddings \(\) are projected as key \(_{t}\) and value \(_{t}\): \(}=(_{v}_{t}^{T})_{t}+_{v},\) where \(}^{N_{v} D}\). To help the model better understand the video sequence relations, we feed \(}\) into a 2-layer self-attention network, and the final similarity-aware fusion embedding is \(=(_{}_{}^{T}) _{}+_{}\), where \(_{},_{},_{}\) is the matrix obtained from \(}\) after three different projections respectively.

In the span generation process, even for the same video, the correct video segments corresponding to different text queries are very different. Since the fusion embedding \(\) serves as the input condition of the denoiser, the quality of \(\) directly affects the denoising process. To learn similarity relations for \(\) in the multimodal space, we design the similarity loss \(_{sim}\), which contains the pointwise cross entropy loss and the pairwise margin loss:

\[_{sim}=-}_{i=1}_{i}*log(_{i})+(1- {y}_{i})*log(1-_{i})+}_{j=1}(0,+_{ n_{j}}-_{p_{j}}),\] (1)

Figure 2: Our MomentDiff framework, which includes a Similarity-aware Condition Generator (SCG) and a Video Moment Denoiser (VMD). The diffusion process is conducted progressively in VMD.

where \(^{N_{v}}\) is the similarity score, which is obtained by predicting the fusion embedding \(\) through the MLP network. \(^{N_{v}}\) is the similarity label, where \(_{i}=1\) if the \(i\)-th frame is within the ground truth temporal moment and \(_{i}=0\) otherwise. \(_{p_{j}}\) and \(_{n_{j}}\) are the randomly sampled positive and negative frames. \(N_{s}\) is the number of samples and the margin \(=0.2\). Although \(_{sim}\) may only help the fusion embedding retain some coarse-grained similarity semantics, this still provides indispensable multimodal information for the denoiser.

#### 3.2.3 Video Moment Denoiser

Recent works  have revealed that previous models  may rely on the presence of location bias in annotations to achieve seemingly good predictions. To alleviate this problem, instead of improving distribution-specific proposals or queries, we use random location spans to iteratively obtain real spans from a generative perspective. In this section, we first introduce the principle of the forward and reverse processes in diffusion models. Then, we build the diffusion generation process in the video moment denoiser with model distribution \(p_{}(_{0})\) to learn the data distribution \(q(_{0})\).

**Forward process.** During training, we first construct a forward process that corrupts real segment spans \(_{0} q(_{0})\) to noisy data \(_{m}\), where \(m\) is the noisy intensity. Specifically, the Gaussian noise process of any two consecutive intensities  can be defined as: \(q(_{m}_{m-1})=(_{m};}_{m-1},_{m}),\) where \(\) is the variance schedule. In this way, \(_{m}\) can be constructed by \(_{0}\): \(q(_{1:m}_{0})=_{i=1}^{m}q(_{i} _{i-1}).\) Benefiting from the reparameterization technique, the final forward process is simplified to:

\[_{m}=_{m}}_{0}+_{m}}_{m},\] (2)

where the noise \(_{m}(0,)\) and \(_{m}=_{i=1}^{m}(1-_{i})\).

**Reverse process.** The denoising process is learning to remove noise asymptotically from \(_{m}\) to \(_{0}\), and its traditional single-step process can be defined as:

\[p_{}(_{m-1}_{m})=(_{m- 1};_{}(_{m},m),_{m}^{2})\] (3)

where \(_{m}^{2}\)is associated with \(_{m}\) and \(_{}(_{m},m)\) is the predicted mean. In this paper, we train the Video Moment Denoiser (VMD) to reverse this process. The difference is that we predict spans from the VMD network \(f_{}(_{m},m,)\) instead of \(_{}(_{m},m)\).

**Denoiser network.** As shown in Fig. 3, the VMD network mainly consists of 2-layer cross-attention Transformer layers. Next, we walk through how VMD works step by step. For clarity, the input span and output prediction presented below are a single vector.

\(\)**Span normalization.** Unlike generation tasks, our ground-truth temporal span \(_{0}\) is defined by two parameters \(c_{0}\) and \(_{0}\) that have been normalized to \(\), where \(_{0}\) and \(_{0}\) are the center and length of the span \(_{0}\). Therefore, in the above forward process, we need to extend its scale to \([-,]\) to stay close to the Gaussian distribution . After the noise addition is completed, we need to clamp \(_{m}\) to \([-,]\) and then transform the range to \(\): \(_{m}=(_{m}/+1)/2\), where \(=2\).

\(\)**Span embedding.** To model the data distribution in multimodal space, we directly project the discrete span to the embedding space through the Fully Connected (FC) layer: \(_{m}^{}=(_{m})^{D}\). Compared to constructing ROI features in DiffusionDet , linear projection is very flexible and decoupled from conditional information (_i.e.,_ fusion embeddings), avoiding more redundancy.

\(\)**Intensity-aware attention.** The denoiser needs to understand the added noise intensity \(m\) during denoising, so we design the intensity-aware attention to perceive the intensity magnitude explicitly. In Fig. 3, we use sinusoidal mapping for the noise intensity \(m\) to obtain \(_{m}^{D}\) in the multimodal space and add it to the span embedding. We project \(_{m}^{}+_{m}\) as query embedding and the positional embedding \(_{m}^{D}\) is obtained by sinusoidal mapping of \(_{m}\). We can obtain the input query: \(_{m}=((_{m}^{}+_{m}),_{ m})\). Similarly, The input key is \(_{f}=((),_{f})\) and the input value is \(_{f}=()\), where \(()\) is the projection function and \(_{f}^{D}\) is the standard position embedding in Transformer . Thus, the intensity-aware attention is:

\[_{m}=(_{m}_{f}^{})_{f} +_{m}.\] (4)

Figure 3: Video moment denoiser. For simplicity, we only draw the intensity-aware attention structure that is different from the general Transformer.

**Denoising training.** Finally, the generated transformer output is transformed into predicted spans \(}_{m-1}=(}_{m-1},}_{m-1})\) and confidence scores \(}_{m-1}\), which are implemented through a simple FC layer, respectively. Following , the network prediction should be as close to ground truth \(_{0}\) as possible. In addition, inspired by [37; 72], we define the denoising loss as:

\[_{}(_{0},f_{}(_{m},m,) )=_{}\|_{0}-}_{m-1}\|+ _{}\,_{}\,(_{0},}_{m-1} )+_{}_{ce}(}_{m-1}),\] (5)

where \(_{}\), \(_{}\) and \(_{}\) are hyperparameters, \(_{}\) is a generalized IoU loss , \(_{}\) is a cross-entropy loss. Note that the above procedure is a simplification of training. Considering that there may be more than one ground truth span in the dataset , we set the number of input and output spans to \(N_{r}\). For the input, apart from the ground truth, the extra spans are padded with random noise. For the output, we calculate the matching cost of each predicted span and ground truth according to \(_{vmr}\) (_i.e.,_ the Hungarian match ), and find the span with the smallest cost to calculate the loss. In \(_{}\), we set the confidence label to 1 for the best predicted span and 0 for the remaining spans.

### Inference

After training, MomentDiff can be applied to generate temporal moments for video-text pairs including unseen pairs during training. Specifically, we randomly sample noise \(}_{m}\) from a Gaussian distribution \((0,)\), the model can remove noise according to the update rule of diffusion models :

\[}_{m-1}= _{m-1}}f_{}(}_{m},m,)+ _{m-1}-_{m}^{2}}}_{m}-_{m}}f_{}(}_{m},m,)}{_{m}}} +_{m}_{m}.\] (6)

As shown in Fig 1(d), we iterate this process continuously to obtain \(}_{0}\) from coarse to fine. Note that in the last step we directly use \(f_{}(}_{1},1,)\) as \(}_{0}\). In \(}_{0}\), we choose the span with the highest confidence score in \(}_{0}\) as the final prediction. To reduce inference overhead, we do not employ any post-processing techniques, such as box renewal in DiffusionDet  and self-condition .

## 4 Experiments

### Datasets, Metrics and Implementation Details

**Datasets.** We evaluate the efficacy of our model by conducting experiments on three representative datasets: Charades-STA , QVHighlights  and TACoS . The reason is that the above three datasets exhibit diversity. Charades-STA comprises intricate daily human activities. QVHighlights contains a broad spectrum of themes, ranging from everyday activities and travel in lifestyle vlogs to social and political events in news videos. TACoS mainly presents long-form videos featuring culinary activities. The training and testing divisions are consistent with existing methods [28; 38].

**Metrics.** To make fair comparisons, we adopt the same evaluation metrics as those used in previous works [38; 37; 23; 75; 29; 19; 27], namely R1@n, MAP@n, and MAP\({}_{avg}\). Specifically, R1@n is defined as the percentage of testing queries that have at least one correct retrieved moment (with an intersection over union (IoU) greater than n) within the top-1 results. Similarly, MAP@n is defined as the mean average precision with an IoU greater than n, while MAP\({}_{avg}\) is determined as the average MAP@n across multiple IoU thresholds [0.5: 0.05: 0.95].

**Implementation details.** For a fair comparison [44; 20; 30], we freeze the video encoder and text encoder and use only the extracted features. For VGG , C3D  or SlowFast+CLIP (SF+C) [66; 65], we extract video features every 1/6s, 1s or 2s. So the frame number \(N_{v}\) is related to the length of the video, while the max text length \(N_{t}\) is set to 32. Note that since the videos in TACoS are long, we uniformly sample the video frame features and set the max frame number \(N_{v}\) to 100 in TACoS. We set the hidden size \(D=256\) in all Transformer layers. In SCG, we also use a variant of the pairwise margin loss called InfoNCE loss . The number of random spans \(N_{r}\) is set to 10 for QVHighlights, 5 for Charades-STA and TACoS. We use the cosine schedule for \(\). For all datasets, we optimize MomentDiff for 100 epochs on one NVIDIA Tesla A100 GPU, employ Adam optimizer  with 1e-4 weight decay and fix the batch size as 32. The learning rate is set to 1e-4. By default, the loss hyperparameters \(_{}=10\), \(_{}=1\) and \(_{}=4\). The weight values for \(_{sim}\) and \(_{vmr}\) are 4 and 1. To speed up the sampling process during inference, we follow DDIM  and iterate 50 times.

### Performance Comparisons

**Comparison with state-of-the-art methods.** To prove the effectiveness of MomentDiff, we compare the retrieval performance with 17 discriminative VMR methods. Tab. 1, Tab. 2, and Tab. 3 show the R1@n, MAP@n, and MAP\({}_{avg}\) results on Charades-STA, QVHighlights and TACoS. Compared with SOTA methods [28; 38; 75; 44; 30; 37], MomentDiff achieves significant improvements on Charades-STA regardless of whether 2D features (VGG), multimodal features (VGG+A), 3D features (C3D), or multimodal pre-trained features (SF+C) are used. This proves that MomentDiff is a universal generative VMR method. In the other two datasets (QVHighlights and TACoS), we still have highly competitive results. Specifically, compared to MomentDETR , MomentDiff obtains 2.35%, 3.86%, and 13.13% average gains in R1@0.5 on three datasets. It is worth noting that TACoS contains long videos of cooking events where different events are only slightly different in terms of cookware, food and other items. The learnable queries in MomentDETR may not cope well with such fine-grained dynamic changes. We attribute the great advantage of MomentDiff over these methods to fully exploiting similarity-aware condition information and progressive refinement denoising.

**Transfer experiments.** To explore the location bias problem, we first organize the Out of Distribution (OOD) experiment following , which repartitions Charades-STA  and ActivityNet-Captions  datasets according to moment annotation density values . In Tab. 4, we exceed

    &  \\   & R1@0.5 & R1@0.7 & MAP@0.5 & MAP@0.75 & MAP\({}_{avg}\) \\  MCN  & 11.41 & 2.72 & 24.94 & 8.22 & 10.67 \\ CAL  & 25.49 & 11.54 & 23.40 & 7.65 & 9.89 \\ XML  & 41.83 & 30.35 & 44.63 & 31.73 & 32.14 \\ XML+  & 46.69 & 33.46 & 47.89 & 34.67 & 34.90 \\ MDE*  & 53.56 & 34.09 & 53.97 & 28.65 & 29.39 \\ MomentDiff & **57.42** & **39.66** & **54.02** & **35.73** & **35.95** \\ UMT*  & 56.26 & 40.31 & 52.77 & 36.82 & 35.79 \\ MomentDiff*  & **58.21** & **41.48** & **54.57** & **37.21** & **36.84** \\   

Table 2: Performance comparisons (%) on QVHighlights Table 3: Performance comparisons (%) with SF+C video features and CLIP text features. ”+” de-on TACoS. We adopt C3D features to notes that we re-implement the method with only segment encode videos. MDE is the abbreviation moment labels. ”+” stands for using audio data. MDE is the of MomentDETR .

    &  \\   & R1@0.5 & R1@0.7 & MAP@0.5 & MAP@0.75 & MAP\({}_{avg}\) \\  MCN  & 11.41 & 2.72 & 24.94 & 8.22 & 10.67 \\ CAL  & 25.49 & 11.54 & 23.40 & 7.65 & 9.89 \\ XML  & 41.83 & 30.35 & 44.63 & 31.73 & 32.14 \\ XML+  & 46.69 & 33.46 & 47.89 & 34.67 & 34.90 \\ MDE*  & 53.56 & 34.09 & 53.97 & 28.65 & 29.39 \\ MomentDiff & **57.42** & **39.66** & **54.02** & **35.73** & **35.95** \\ UMT*  & 56.26 & 40.31 & 52.77 & 36.82 & 35.79 \\ MomentDiff*  & **58.21** & **41.48** & **54.57** & **37.21** & **36.84** \\   

Table 1: Performance comparisons (%) on the Charades-STA dataset. ”\(\)” denotes that we re-implement the method under the same training scheme. ”A” stands for using audio data.

[MISSING_PAGE_FAIL:8]

**Span embedding type.** Regarding the way discrete spans are mapped to the embedding space, we compare the ROI strategy  with our linear projection (FC) in Tab. 6(a). For the ROI strategy, we slice the fusion embeddings \(\) corresponding to random spans, followed by mean pooling on the sliced features. Tab. 6(a) shows that ROI does not work well. This may be due to two points: 1) ROI is a hard projection strategy, while the importance of each video frame is quite different. FC is similar to soft ROI, and its process can be trained end-to-end. 2) FC is decoupled from \(\), which allows the model to focus on modeling the diffusion process and avoid over-dependence on \(\).

**Scale \(\).**\(\) is the signal-to-noise ratio  of the diffusion process, and its effect is shown in Tab. 6(b). We find that the effect of larger \(\) drops significantly, which may be due to the lack of more hard samples for denoising training when the proportion of noise is small, resulting in poor generalization.

**Video Moment Denoiser (VMD) and noise intensity \(m\).** In Tab. 6(c), we first remove the denoiser and the diffusion process (w/o VMD). After training with the same losses, we find that predicting with only fusion embeddings \(\) leads to a drastic drop in results, which reveals the effectiveness of denoising training. Then we remove the noise intensity \(m\) (w/o \(m\)), and the result is reduced by 5.16% on R1@0.5. This shows that explicitly aggregating noise intensity with random spans improves noise modeling. Combined with VMD and \(m\), the diffusion mechanism can fully understand the data distribution and generate the real span from coarse to fine.

**Loss designs.** In Tab. 6(d), we show the impact of loss functions. In \(_{sim}\), we use pointwise and pairwise constraints to guide token-wise interactions between multimodal features, while ensuring reliable conditions for subsequent denoising. In \(_{curr}\), the model can learn to accurately localize exact segments. Adequate multimodal interaction and denoising training procedures are complementary.

**Span number.** In Tab. 6(e), we only need 5 random spans to achieve good results. Unlike object detection , the number of correct video segments corresponding to text query is small. Therefore, a large number of random inputs may make the model difficult to train and deteriorate the performance.

Table 6: Ablation study (%) on the Charades-STA dataset with SF+C video features and CLIP text features. We report R1@0.5, R1@0.7 and MAP\({}_{avg}\). Default settings are marked in blue.

Figure 5: Visualization of the diffusion process on Charades-STA (Left) and QVHighlights (Right). For clarity, we show 3 random spans (\(_{50}\)) with Gaussian initialization, and progressively get the top-1 result (\(_{0}\)) according to the confidence score. Green box: right segment. MDE: MomentDETR .

**Model performance vs. speed.** In Tab. 6(f), we explore the effects of different diffusion steps. When step=2, good results and fast inference speed have been achieved. Subsequent iterations can improve the results of high IoU (_i.e.,_ R1@0.7), which shows the natural advantages of diffusion models.

### Qualitative Results

We show two examples of the diffusion process in Fig. 5. We can find that the retrieved moments by MomentDiff are closer to the ground truth than those by MomentDETR. The diffusion process can gradually reveal the similarity between text and video frames, thus achieving better results. Besides, the final predictions corresponding to spans with multiple random initial locations are close to the ground truth. This shows that our model achieves a mapping from arbitrary locations to real segments.

## 5 Limitation and Conclusion

**Limitation.** Compared to existing methods [27; 37], the diffusion process requires multiple rounds of iterations, which may affect the inference speed. As shown in Tab. 6(f), we reduce the number of iterations, with only a small sacrifice in performance. In practical usage, we suggest choosing a reasonable step number for a better trade-off between performance and speed.

**Conclusion.** This paper proposes a novel generative video moment retrieval framework, MomentDiff, which simulates a typical human retrieval style via diffusion models. Benefiting from the denoising diffusion process from random noise to temporal span, we achieve the refinement of prediction results and alleviate the location bias problem existing in discriminative methods. MomentDiff demonstrates efficiency and generalization on multiple diverse and anti-bias datasets. We aim to stimulate further research on video moment retrieval by addressing the inadequacies in the framework design, and firmly believe that this work provides fundamental insights into the multimodal domain.

## 6 Acknowledgement

This work is supported by the National Key Research and Development Program of China (2022YFB3104700), the National Nature Science Foundation of China (62121002, 62022076, 62232006).