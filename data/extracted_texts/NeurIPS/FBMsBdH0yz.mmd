# Masked Hard-Attention Transformers Recognize

Exactly the Star-Free Languages

 Andy Yang

University of Notre Dame

&David Chiang

University of Notre Dame

&Dana Angluin

Yale University

###### Abstract

The expressive power of transformers over inputs of unbounded size can be studied through their ability to recognize classes of formal languages. In this paper, we establish exact characterizations of transformers with hard attention (in which all attention is focused on exactly one position) and attention masking (in which each position only attends to positions on one side). With strict masking (each position cannot attend to itself) and without position embeddings, these transformers are expressively equivalent to linear temporal logic (LTL), which defines exactly the star-free languages. A key technique is the use of Boolean RASP as a convenient intermediate language between transformers and LTL. We then take numerous results known for LTL and apply them to transformers, showing how position embeddings, strict masking, and depth all increase expressive power.

## 1 Introduction

Significant progress has been made in the last few years on characterizing the expressivity of transformers  in terms of well-understood classes of formal languages . Results have been obtained for a wide range of variants of transformers, and nearly all take the form of either upper bounds (transformers recognize only languages in class \(C\)) or lower bounds (transformers recognize all languages in class \(C\)). In this paper, we establish _exact_ characterizations of transformers with hard attention (in which all attention is focused on exactly one position) and attention masking (in which each position \(i\) only attends to positions on one side of \(i\)).

With strict masking (in which each position cannot attend to itself) and without position embeddings, these transformers recognize exactly the class of _star-free_ regular languages. The left side of Figure 1 summarizes our results relating masked hard-attention transformers and linear temporal logic (**LTL**), which defines exactly the star-free regular languages.

A key technique in these proofs is the use of **B-RASP**, which, like RASP , is a small programming language that compiles into transformers. **B-RASP** is restricted to Boolean values and compiles to masked hard-attention transformers. Additionally, a masked hard-attention transformer can be decompiled back to a **B-RASP** program. We use **B-RASP** as an intermediate language between transformers and **LTL**.

The equivalence of masked hard-attention transformers with **LTL** (and other equivalent characterizations, like counter-free automata and first-order logic) enables us to take numerous results known for **LTL** and apply them to transformers, as shown on the right side of Figure 1:

* Strict future-masked rightmost-hard attention is sufficient; adding past-masked, non-masked, and/or leftmost-hard attention does not increase expressivity (Section 5.1).
* Strict masking is important (Section 5.2); without it, masked hard-attention transformers are less expressive, recognizing only the _sutter-invariant_ star-free languages.

* Adding position embeddings increases the class of recognized languages to other well-studied classes (Section 5.3); for example:
* With rational sinusoidal position embeddings, masked hard-attention transformers recognize exactly the regular languages in \(^{0}\).
* With arbitrary finite-image position embeddings, they are equivalent to \([]\) (linear temporal logic with arbitrary monadic predicates).
* Adding more layers always increases expressive power (Section 5.4).

## 2 Background

### Preliminaries

Let \(\) be a finite alphabet, and let \(w=w_{1} w_{n}\) be an input string of length \(n\), where each \(w_{i}\). Throughout, we assume that \(w\) is not empty. We write \(^{+}\) for the set of all non-empty strings over \(\). (We disallow empty strings because several formalisms used here require a designated position where an accept/reject decision appears. Adding a \(\) or \(\) token not in \(\) for this purpose would make it possible to handle the empty string.) We write \([n]\) for the set \(\{1,,n\}\).

The _star-free regular languages_ are the closure of \(\), \(\{\}\), and \(\{\}\) for each \(\), under the operations of union, concatenation, and complementation. For example:

* \(^{*}\) is star-free because \(^{*}=^{}\).
* \((ab)^{*}\) is star-free because \((ab)^{*}=(b^{*}^{*}a^{*}aa^{*}^{*}bb ^{*})^{}\).
* \((aa)^{*}\) is regular but not star-free.

This class of languages has several other characterizations, including counter-free automata (Appendix B.5), first-order logic with order (McNaughton and Papert, 1971), and linear temporal logic (Kamp, 1968), which is what we will focus on in this paper.

### Transformer variants

The original transformer (Vaswani et al., 2017), designed for machine translation, had both an encoder and a decoder. In practice, both encoder-only models like BERT (Devlin et al., 2019) and decoder-only models like GPT (Brown et al., 2020) are common. Like much previous work on transformer expressivity (e.g. Hahn, 2020), we study an encoder-only setup, where the input is a string and the output is a binary classification; but our results could easily be adapted to a decoder-only setting where the input is a prefix and the output is the next symbol.

Figure 1: Overview of results in this paper. One-way arrows denote strict inclusion; two-way arrows denote equivalence. PE = position embedding.

The transformers studied here use _unique hard attention_ (or simply _hard attention_), in which an attention head focuses all attention on the position with the highest score, with ties broken to the left or right. Although this is different from the soft attention in actual transformers, theoretical studies unavoidably involve models of the real objects of study, and we are using unique-hard attention as a stepping-stone towards understanding real transformers. However, unique-hard attention may be more appropriate than it appears:

* Real transformers are often observed to focus attention on a very small number of positions (Merrill et al., 2021). On Dyck languages, they have been found to learn effectively unique-hard attention in their second layer (Ebrahimi et al., 2020, Figure 1).
* There exist soft-attention transformers that compute parity (Chiang and Cholak, 2022), but in practice, transformers cannot learn parity (Bhattamishra et al., 2020). Unique-hard attention transformers also cannot compute parity (Hahn, 2020), so they are in some sense more realistic.
* Hard attention has occasionally been used in practice in previous research on interpretability (Kinley, 2020) and efficiency (Gupta et al., 2021; Xu et al., 2021).

In this paper, we use _future masking_, in which every position may only attend to positions to its left. This kind of masking is common in decoder-only models and has been studied in encoder-only models as well (Bhattamishra et al., 2020). We also consider _past masking_(Yao et al., 2021).

### Previous work

Perez et al. (2021) show that average-hard attention transformer encoder-decoders, where the decoder runs for a polynomial number of steps before accepting or rejecting a string, recognize all of **P** (that is, all languages decidable by a deterministic Turing machine in polynomial time). Merrill and Sabharwal (2024) prove another version of this result, and further observe that all such transformers are in **P**. This result is the only other exact characterization of any transformer variant that we are aware of.

Hao et al. (2022) show that (non-masked) hard-attention transformer encoders with arbitrary position embeddings have an upper bound of \(^{0}\) (that is, languages defined by circuit families with polynomial size, unbounded fan-in, and bounded depth), and Barcelo et al. (2024) show that they have a lower bound of \([]\), which is linear temporal logic with all possible monadic numerical predicates. They leave open the question of whether these transformers are equivalent to \([]\)--a question which, with suitable adjustments, we answer here in the affirmative.

## 3 Boolean RASP

RASP (Weiss et al., 2021) is a programming language intended to help programmers "think like transformers." It has the same basic operations as transformers, but it is easier to compose these operations in RASP than to write transformers by hand. Variants of RASP have been used fruitfully to study transformers' length-generalization capabilities (Zhou et al., 2024) and expressive power (Strobl et al., 2024; Yang and Chiang, 2024). In this section, we define a version of RASP restricted to Boolean values, which we call Boolean RASP or **B-RASP**. As we will see, it can be compiled into masked hard-attention transformers, and masked hard-attention transformers can be decompiled back into **B-RASP**. We use it as an intermediate language between transformers and \(\), and find it more convenient to work with than either of them.

### Definition

The input to a **B-RASP** program is a string \(w=w_{1} w_{n}^{+}\). There is one type of data, a _Boolean vector_, which is a vector of Boolean values indexed by \(i[n]\). The _initial_ Boolean vectors are \(Q_{}\) for each \(\), where \(Q_{}(i)=1\) iff \(w_{i}=\).

A **B-RASP** program is a sequence of operations that compute new Boolean vectors. Although they may have descriptive names, and names may be reused, here, to streamline definitions and proofs, we assume that all the Boolean vectors are numbered consecutively. That is, \(P_{1},,P_{||}\) are the initial Boolean vectors \(Q_{}\) for \(\), and the Boolean vectors computed by the program are numberedstarting from \(P_{||+1}\) without repetition. After the first \(t\) vectors, vector \(P_{t+1}\) is computed using one of the following operations.

_Position-wise operations._\(P_{t+1}(i)\) can be be computed by \(P_{t+1}(i):=R(i)\), where \(R(i)\) is a Boolean combination of zero or more of \(\{P_{1}(i),,P_{t}(i)\}\).

_Attention operations._\(P_{t+1}(i)\) can be computed by either of

\[P_{t+1}(i) :=_{j}M(i,j),S(i,j)\;V(i,j)\,:D(i)\] \[P_{t+1}(i) :=_{j}M(i,j),S(i,j)\;V(i,j)\,:D(i)\]

where:

* \(M(i,j)\), the _mask predicate_, is one of \(M(i,j)=1\) (no masking), \(M(i,j)=(j<i)\) (strict future masking), or \(M(i,j)=(j>i)\) (strict past masking).
* \(S(i,j)\), the _score predicate_, is a Boolean combination of zero or more atomic formulas from \(\{P_{1}(i),,P_{t}(i)\}\{P_{1}(j),,P_{t}(j)\}\).
* \(V(i,j)\), the _value predicate_, has the same form as the score predicate.
* \(D(i)\), the _default predicate_, is a Boolean combination of zero or more atomic formulas from \(\{P_{1}(i),,P_{t}(i)\}\).

For each \(i[n]\), let \(j_{i}\) be the minimum (if the operator is \(\)) or maximum (if \(\)) value of \(j[n]\) such that \(M(i,j)=1\) and \(S(i,j)=1\). If \(j_{i}\) exists, then \(P_{t+1}(i)=V(i,j_{i})\). If \(j_{i}\) does not exist, then \(P_{t+1}(i)=D(i)\).

If \(P\) is a Boolean vector computed by program \(\), we write \(w P(i)\) just in case \(P(i)=1\) when \(\) is run on input string \(w\). To make a **B-RASP** program \(\) recognize a language, one Boolean vector \(Y\) is designated the output vector, and position \(n\) is designated the output position. Then, the input string \(w\) is accepted iff \(w Y(n)\). To make a **B-RASP** program compute a length-preserving sequence-to-sequence function from \(^{+}\) to \(^{+}\), we designate a collection of output Boolean vectors \(Y_{}\) indexed by the symbols \(\), and consider the output at position \(i\) to be \(\) iff \(Y_{}(i)\) is true.

### Example: Dyck-1 of depth 2

As an example, we consider the Dyck language with 1 pair of parentheses, limited to depth 2, or \(L_{1,2}\) for short. It is recognized by the DFA in Figure 1(a), where \(\) and \(r\) are left and right brackets. We show how to define this language in **B-RASP**, with a construction very similar to that of Yao et al. (2021).

Consider the input string \( r r r\), which should be accepted. The basic idea is to identify brackets that are immediately matched (\( r^{} r r r\), then look at the remaining brackets (\( r r r r r\)) to make sure they are matched. We describe the **B-RASP** program for this problem below; the resulting Boolean vectors are shown in Figure 1(b).

Figure 2: Examples related to \(L_{1,2}\) (Dyck-1 of depth 2). The left bracket is \(\) and the right bracket is \(r\).

We first construct Boolean vectors \(P_{}(i)\) and \(S_{r}(i)\) that indicate whether the predecessor (respectively, successor) symbol of \(i\) is \(\) (respectively, \(r\)). This is done with attention operations:

\[P_{}(i) :=_{j}[j<i,1]\;\;Q_{}(j):0\] \[S_{r}(i) :=_{j}[j>i,1]\;\;Q_{r}(j):0.\]

Vector \(P_{}(i)\) makes position \(i\) attend to the position immediately to its left, and its value predicate \(Q_{}(j)\) tests whether that position has an \(\). Vector \(S_{r}\) is similar.

The Boolean vector \(I(i)\) indicates whether position \(i\) is in a consecutive pair \( r\), that is, whether it is _immediately matched_:

\[I(i):=(Q_{}(i) S_{r}(i))(Q_{r}(i) P_{}(i)).\]

The Boolean vectors \(B_{}(i)\) and \(A_{r}(i)\) test if the symbol before (respectively, after) \(i\) that is not immediately matched is \(\) (respectively, \(r\)). Then \(C\) checks each position \(i\) to see if it is immediately matched, or it has \(\) and the following not-immediately-matched symbol is \(r\), or it has \(r\) and the preceding not-immediately-matched symbol is \(\):

\[B_{}(i) :=_{j}[j<i, I(j)]\;\;Q_{}(j):0\] \[A_{r}(i) :=_{j}[j>i, I(j)]\;\;Q_{r}(j):0\] \[C(i) :=I(i)(Q_{}(i) A_{r}(i))(Q_{r}(i) B_{} (i)).\]

Finally, the output Boolean vector \(Y\) tests if \(C(i)\) is true everywhere:

\[Y(i):=_{j}[1, C(j)]\;\;0:1.\]

Boolean vectors for deciding non-membership of \( rr rr\) in \(L_{1,2}\) are shown in Figure 1(c). It is straightforward to generalize this technique to recognize Dyck-\(k\) of depth \(D\) in **B-RASP**.1 For another example program for an associative recall task, please see Appendix A. A **B-RASP** simulator that allows one to write and run additional examples can be found at https://b-rasp.github.io/.

### Normal forms

In **B-RASP**, the value predicate \(V(i,j)\) depends on both \(i\) (the query position) and \(j\) (the key/value position), but in actual transformers, it depends on \(j\) only. The dependence on \(i\) is sometimes convenient, but it does not change expressivity (see Appendix B.1).

The score predicate \(S(i,j)\) depends on both \(i\) and \(j\) in both **B-RASP** and actual transformers. Perhaps surprisingly, in **B-RASP**, it too can be made to depend only on \(j\) without reducing expressivity, but as a tradeoff the program may become exponentially larger in size (see Appendix B.2).

### Equivalence with linear temporal logic

We prove that **B-RASP** recognizes exactly the star-free languages, by proving that **B-RASP** is equivalent to linear temporal logic. Appendix B.5 gives another proof of the star-free-to-**B-RASP** direction via counter-free automata.

In linear temporal logic or **LTL**(Kamp, 1968), every formula implicitly depends on a single "time" (or position). The atomic formulas are \(Q_{}\) for every \(\), and we have the usual connectives \(\), \(\), and \(\), as well as operators **since** and **until**.2 For any input string \(w=w_{1} w_{n}\) and position \(i[n]\)we define \(w,i\) as follows:

\[w,i Q_{}&w_{i}=\\ w,i_{1}_{2}&w,i_{1}w,i_{2}\\ w,i_{1}_{2}&w,i_{1}w,i _{2}\\ w,i_{1}&w,i_{1}\\ w,i_{1}_{2}&j<iw,j _{2},\\ &kj<k<iw,k_{1}\\ w,i_{1}_{2}&j>iw,j _{2},\\ &ki<k<jw,k_{1}.\]

To use a formula \(\) of \(\) to define a language over \(\), for an input string \(w^{+}\) of length \(n\) we designate the last position as the output position, so that \(w()\) if and only if \(w,n\).

For example, let \(=\{a,b,\#\}\) and consider the following formulas:

\[_{1}=Q_{\#}\\ _{2}=Q_{\#}(Q_{b}Q_{\#})\\ _{3}=Q_{\#}(Q_{b}(Q_{\#}(Q_{a}Q_{\#})))\\ _{4}=Q_{\#}(Q_{b}(Q_{\#}(Q_{a}(Q_{\#}(01))))).\]

The formula \(_{1}\) defines the language \(^{*}\#\), which contains all and only strings with a \(\#\) in the last position. The formula \(_{2}\) defines the language \(^{*}\#b^{*}\#\), and \(_{3}\) defines the language \(^{*}\#a^{*}\#b^{*}\#\). Finally, \(_{4}\) defines the language \(\#a^{*}\#b^{*}\#\), because \((01)\) is only true at the first position.

**Theorem 1**.: _For any formula of \(\) that defines a language \(L^{+}\), there is a \(\)-\(\) program that recognizes \(L\)._

Proof.: See Appendix B.3. This is shown via direct construction. 

**Theorem 2**.: _For any \(\)-\(\) program that recognizes a language \(L^{+}\), there is a formula of \(\) that defines \(L\)._

Proof.: See Appendix B.4. We use the unary normal forms (Section 3.3) to facilitate this proof. 

## 4 Masked Hard-Attention Transformers

### Definition

A _masked hard-attention transformer layer with width_\(d>0\) is a length-preserving function

\[\ (^{d})^{+}(^{d})^{+} \\ (x_{1},,x_{n})(y_{1},,y_{n})\\ (c_{1},,c_{n})=att(x_{1},,x_{n})+(x_{1},,x_{n})\\ y_{i}=(c_{i})+c_{i} 56.905512pti=1,,n.\] (1)

The self-attention layer \(att\) is specified by

* A score function, which is a bilinear function \(f_{S}^{d}^{d}\).
* A mask, which is \(M(i,j)=1\) (no masking), \(M(i,j)=(j<i)\) (strict future masking), or \(M(i,j)=(i<j)\) (strict past masking).
* A tie-breaking function \(C\) to select one element of a finite non-empty set \(I_{+}\), which is either \(C(I)= I\) (choose leftmost position) or \(C(I)= I\) (choose rightmost position).
* A value function, which is a linear transformation \(f_{V}^{d}^{d}\).

The layer works as follows, for each \(i[n]\). Let

\[U_{i}=\{j[n]\ |\ M(i,j)=1\}&\\ B_{i}=\{j U_{i}\ |\ ( j^{} U_{i})(f_{S}(x_{i},x_{j^{}})  f_{S}(x_{i},x_{j}))\}&\]If \(U_{i}\), let \(j_{i}=C(B_{i})\) and output \(c_{i}=f_{}(x_{j_{i}})\); but if \(U_{i}=\), output \(c_{i}=\).

The function _ffn_ is a feed-forward neural network with 2 layers and ReLU activations in between.

Then a _masked hard-attention transformer_ is a length-preserving function

\[^{}(^{d})^{+}\] \[=_{k}_{1} \]

where \(^{}(^{d})^{+}\) is a position-wise function (a word embedding), and each \(_{}\) is a masked hard-attention transformer layer.

We write \([(w)]_{i}\) for the final activation value at position \(i[n]\) when \(\) is run on input \(w\). To use \(\) as a language recognizer, we add an output layer, which linearly projects \([(w)]_{n}\) to a scalar. If the result is nonnegative, we accept \(w\); otherwise, we reject. The exact criterion does not matter much, as the transformers we construct only output \(+\) or \(-\), and could easily be changed to another convention. The language recognized by \(\) (with the output layer) is the set of strings it accepts.

Our definition above differs from the standard definition (Vaswani et al., 2017) in a few ways besides unique-hard attention, which was discussed above in Section 2.2. Ours lacks layer normalization and position embeddings, but we add them in Sections 4.3 and 5.3, respectively. We only use single-head attention; multi-head attention can be simulated by summing the outputs of multiple single-head attentions, or it can be added to the definition, as in Appendix D.3.1. Our attention masking is strict, but we consider non-strict masking in Section 5.2.

### Equivalence with B-RASP

**Theorem 3**.: _For any \(\)-**RASP** program that recognizes a language \(L^{}\), there is a masked hard-attention transformer (with output layer) that recognizes \(L\)._

Proof.: See Appendix C.1. Attention layers simulate attention operations, and FFNs simulate position-wise operations. 

**Theorem 4**.: _For any masked hard-attention transformer (with output layer) that recognizes a language \(L^{}\), there is a \(\)-**RASP** program that recognizes \(L\)._

Proof.: See Appendix C.2. To convert a masked hard-attention transformer to \(\)-**RASP**, we first show that all of the intermediate values computed by the transformer are drawn from a finite set and therefore can be represented using \(O(1)\) bits.3 

### Layer normalization

Standard transformers (Vaswani et al., 2017) include layer normalization (Ba et al., 2016), but our definition above does not. Since layer normalization is a position-wise function, the proof of Lemma 24 is unaffected. But the construction of Lemma 21 does need to be modified to circumvent layer normalization (cf. Chiang et al., 2023, Proposition 22). Previously, we used 1 to represent true and 0 to represent false; now, we use a pair of activations to represent a truth value, \((1,0)\) for true and \((0,1)\) for false. This ensures that every vector has mean and variance independent of the input \(w\), so we can set the parameters of each layer normalization so that it has no effect. (In the proof of Theorem 3, we use a flag to indicate whether there are any unmasked positions or not. This flag already uses the encoding described above, and does not need to be modified.)

## 5 Further Results

In this final section, we leverage results from temporal logic and the equivalences established above to obtain numerous new results for masked hard-attention transformers (and \(\)-**RASP**).

### Asymmetric attention

Our definitions of both **B-RASP** and masked hard-attention transformers include both leftmost-hard and rightmost-hard attention, and both future and past masking. But we can use the fact that, in **LTL**, if the output is read out only at the last position, it suffices to have only **since** and not **until**(Gabbay et al., 1980) to obtain the following result.

**Theorem 5**.: _Both **B-RASP** and transformers with only future-masked rightmost-hard attention recognize exactly the star-free languages._

Proof.: Any star-free language can be defined in **LTL** using only **since**(Gabbay et al., 1980), and restricting Theorem 1 to translate from **LTL** with only **since** into **B-RASP** will only use future-masked \(\). Therefore, **B-RASP** with only future-masked \(\) can define any star-free language. Similarly, the translation (Theorem 3) from **B-RASP** with only future-masked \(\) to masked hard-attention transformers only uses future-masked rightmost-hard attention. Therefore, transformers with only future-masked rightmost-hard attention can define any star-free language. 

Note that this applies only in a setting where we accept or reject strings by looking at the output at the last position. It does not apply to other settings, like transduction (Strobl et al., 2024).

### Non-strict masking

Our definitions of both **B-RASP** and masked hard-attention transformers use strict masking, in which a position cannot attend to itself. Standard transformers, however, use non-strict masking. We can modify the definitions to use _non-strict_ masking, that is, \(i j\) or \(j i\).

Non-strictness is known to reduce expressivity in **LTL**(Peled and Wilke, 1997), so it reduces expressivity in **B-RASP** and masked hard-attention transformers as well. Intuitively, non-strict masked operations are unable to distinguish between consecutive positions that have the same symbol. More formally, a language over \(\) is called _stutter-invariant4_ iff for all \(u,v^{*}\) and \(\), \(u v L\) iff \(u v L\). An example of a language that is stutter-invariant star-free is \((a^{+}b^{+})^{*}\) (where \(^{+}\) means "one or more occurrences of \(^{*}\)"); a language that is star-free but not stutter-invariant is \((ab)^{*}\).

**Theorem 6**.: _Both **B-RASP** and masked hard-attention transformers with only non-strict masking recognize exactly the stutter-invariant star-free languages._

Proof.: Peled and Wilke (1997) prove that **LTL** with non-strict **since\({}^{}\)** and **until\({}^{}\)** recognizes exactly the stutter-invariant star-free languages. The proofs of Theorems 1 and 2 may be adapted to use non-strict temporal operators and non-strict masking. Thus, non-strict **B-RASP** and non-strict **LTL** are equivalent. Similarly, using \(j i\) or \(j i\) as \(M(i,j)\) in the proofs of Theorems 3 and 4, we can show that non-strict masked hard-attention transformers are equivalent to non-strict **B-RASP**. 

In Section 3.2, we showed how to define \(L_{1,2}\), Dyck-1 of depth 2. Bhattamisra et al. (2020, SS7.1) find experimentally that \(L_{1,2}\) is not learnable by transformers, and they argue that it is not even expressible by transformers (with soft attention, non-strict masking, and no position embeddings). The reason is that while reading the prefix of \(\)'s at the start of the string, the soft-attention layer computes the same value vector at every position and cannot count the number of occurrences of \(\). However, with the addition of a BOS symbol, soft attention can measure what fraction of symbols are \(\), overcoming this limitation as observed empirically by Ebrahimi et al. (2020). The similarities between how strict masking in the hard attention setting and the addition of BOS in soft attention both enable positions to be distinguished are notable for future investigation.

### Position embeddings

Our definition of a transformer does not, so far, include position embeddings; all information about ordering comes from attention masking. A position embedding is a family of functions \(=(_{n})_{n 0}\) where \(_{n}(i)\) is a scalar or vector representation of position \(i\) in a string of length \(n\). Then the input layer _emb_ becomes the sum of a word embedding and a position embedding.

We say that \(\) has _finite image_ if \(_{n 0}_{n}\) is finite. In general, our results extend to transformers with any position embedding that has finite image. The class of languages recognized may grow, and we give a recipe for characterizing the new class of languages.

We can add numerical predicates to **LTL** and initial Boolean vectors to **B-RASP** as follows. Let \(=(_{n})_{n 0}\) be a family of functions \(_{n}[n]\{0,1\}\). Then there is an additional predicate symbol \(\) such that for any string \(w\) with length \(n\),

\[w(i)_{n}(i)=1 \] \[w,i_{n}(i)=1 .\]

For example, if \(_{n}(i)\) is true iff \(n\) is odd and \(i= n/2\), then we can define the language \(\{\#a^{m}\#b^{m}\# m 0\}\) in \([]\) as:

\[=Q_{\#}(Q_{t}( Q_{\#}(Q_{a} (Q_{\#}(01))))).\]

A similar definition could be written in **B-RASP\([]\)**.

**Theorem 7**.: _Let \(=(_{n})_{n 0}\) be a position embedding with finite image. There exists a collection of predicates \(_{}\) such that the following classes of languages are the same:_

* _languages recognized by masked hard-attention transformers with position embedding_ \(\)__
* _languages defined by_ \([_{}]\)__
* _languages defined by_ \([_{}]\)_._

Proof.: See Appendix D.1. 

We discuss two important special cases below.

Sinusoidal position embeddingsThe original transformer (Vaswani et al., 2017) used position embeddings with coordinates of the form \((2 fi)\) or \((2 fi)\). If the \(f\)'s are rational (though in the original definition they were not), then the position embeddings form a finite set, so Lemma 22 still holds. For any even \(d\), let us define a _rational sinusoidal positional embedding_ with \(d\) dimensions to be a position embedding \(=(_{n})_{n 0}\) where

\[_{n}(i)= 2 f_{1}i& 2 f_{1}i&& 2 f _{d/2}i& 2 f_{d/2}^{} f_{1},,f_{d/2}.\]

**Corollary 8**.: _Masked hard-attention transformers with rational sinusoidal position embeddings recognize exactly the regular languages in \(^{0}\) (that is, regular languages definable by a family of Boolean circuits with polynomial size and constant depth)._

Proof.: This uses the fact that the regular languages in \(^{0}\) are exactly the languages definable in first-order logic with modular predicates (Barrington et al., 1992). See Appendix D.2 for details. 

An example of a language that belongs to this class but is not star-free is \((aa)^{*}\). The classic example of a language that is regular but not in \(^{0}\) is \(=\{w\{a,b\}^{*} wb\}\)(Furst et al., 1984).

Arbitrary position embeddingsFinally, we may consider arbitrary position embeddings, subject to the condition of finite image. The corresponding collection of predicates is the set of all possible monadic predicates, which we call Mon.5

**Corollary 9**.: _Masked hard-attention transformers that have position embeddings with finite image recognize exactly the languages definable in \([]\)._

Barcelo et al. (2024) show that any language definable in \([]\) can be recognized by a hard-attention transformer without attention masking and with some position embedding (with infinite image), but left the other direction as an open question. Here, by making use of attention masking and restricting position embeddings to those with finite image, we have obtained an exact characterization.

The addition of attention masking appears to be important. With finite image position embeddings but without attention masking, there must be two positions \(i\) and \(j\) with the same position embedding (by the pigeonhole principle), so an unmasked attention transformer would not be able to distinguish one string with \(a\) and \(b\) at positions \(i\) and \(j\) and another string with \(a\) and \(b\) at positions \(j\) and \(i\). So no masked hard-attention transformer with finite image position embeddings and unmasked attention can recognize the language \(\#a^{*}\#b^{*}\#\), but we showed already how to define this language even in **LTL**.

### Depth hierarchy

Finally, we establish that (unlike with feed-forward networks), we can always increase the expressive power of masked hard-attention transformers by adding more self-attention layers. We consider masked hard-attention transformers with only future masking (as is typical in practice) and with only rightmost-hard attention. Other masking and tie-breaking schemes are treated in Appendix D.3. We also add multi-head attention (as is typical in practice).

First, we define depth for all models in this paper. The _layer depth_ of a masked hard-attention transformer is the number of attention layers. The _temporal depth_ of an **LTL** formula is as follows:

\[(Q_{}) =0()=()\] \[() =()=((), ())\] \[() =(\ )=((), ())+1\]

The _attention depth_ of a **B-RASP** expression is defined as follows:

\[(Q_{}(i)) =0( P(i))=(P(i))\] \[(P_{1}(i) P_{2}(i)) =(P_{1}(i) P_{2}(i))=(( P_{1}(i)),(P_{2}(i))).\]

We then extend this definition to **B-RASP** operations. If \(P(i):=(i)\) (a position-wise operation),

\[(P(i))=((i)).\]

If \(P(i):=_{j}[M(i,j),S(i,j)]\)\(V(i,j):D(i)\) or \(P(i):=_{j}[M(i,j),S(i,j)]\)\(V(i,j):D(i)\),

\[(P(i))=((S(i,j)),(V(i,j )),(D(i)))+1.\]

Finally, the attention depth of a program is the maximum of the attention depths of all of its operations.

Let \(( F)_{k}\) (respectively, **B-RASP\(( F)_{k}\)**) be the languages recognizable by multi-head transformers of depth \(k\) (respectively, **B-RASP** programs of depth \(k\)) using only future-masked rightmost-hard attention. Let \(()_{k}\) be the languages definable by \(\) formulas of depth \(k\) without **until**.

**Theorem 10**.: _For every \(k 0\), there is a language \(L_{k+1}\) such that no multi-head masked hard-attention transformer of depth \(k\) recognizes \(L_{k}\), but a transformer of depth \((k+1)\) does recognize \(L_{k+1}\)._

Proof.: The constructions in the proofs of Theorems 1 and 2 preserve depth, so \(( F)_{k}=()_{k}\). Moreover, by Theorem 4 (shallower version in Appendix C.2), and by Theorem 27 (a depth-preserving version of Theorem 3 found in Appendix D.3), \(( F)_{k}=( F )_{k}\). Finally, Etessami and Wilke (2000) prove that \(()_{k}()_{k+1}\). Namely, the classes are separated by \(L_{k+1}=_{k+1}\), which is the language over \(=\{a,b,c\}\) of strings which, after deleting \(c\)'s, contain \(a^{k+1}\) as a substring. This gives the following picture:

\[()_{k}&& ()_{k+1}&\\ ( F)_{k}&( F )_{k+1}\\ &( F)_{k+1}\\ \]

Therefore, \(( F)_{k}( F )_{k+1}\). 

## 6 Limitations

This work focuses exclusively on masked hard-attention transformers. We discussed the rationale for hard attention in Section 2.2. These results do not apply to softmax-attention transformers, although they demonstrate what kinds of results one might hope to obtain for softmax-attention transformers. Nor do they apply to transformers with unmasked attention.

Finally, our restriction of position embeddings to have finite image, and in particular our restriction of sinusoidal position embeddings to have angles that are rational multiples of \(\), does not exactly match the standard definition.