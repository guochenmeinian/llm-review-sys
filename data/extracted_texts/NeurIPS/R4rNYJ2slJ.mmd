# OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction

Hongbo Zhao\({}^{1,3}\)1 ue Fan\({}^{1,3}\)2  Yuntao Chen\({}^{2}\)3  Haochen Wang\({}^{1,3}\)1  Yuran Yang\({}^{4,5}\)1

**Xiaojuan Jin\({}^{1}\)  Yixin Zhang\({}^{4}\)  Gaofeng Meng\({}^{1,2,3}\)  Zhaoxiang Zhang\({}^{1,2,3}\)2 31**

https://opensatmap.github.io

###### Abstract

In this paper, we propose OpenSatMap, a fine-grained, high-resolution satellite dataset for large-scale map construction. Map construction is one of the foundations of the transportation industry, such as navigation and autonomous driving. Extracting road structures from satellite images is an efficient way to construct large-scale maps. However, existing satellite datasets provide only coarse semantic-level labels with a relatively low resolution (up to level 19), impeding the advancement of this field. In contrast, the proposed OpenSatMap (1) has fine-grained instance-level annotations; (2) consists of high-resolution images (level 20); (3) is currently the largest one of its kind; (4) collects data with high diversity. Moreover, OpenSatMap covers and aligns with the popular nuScenes dataset and Argoverse 2 dataset to potentially advance autonomous driving technologies. By publishing and maintaining the dataset, we provide a high-quality benchmark for satellite-based map construction and downstream tasks like autonomous driving.

Figure 1: **Demonstrations of OpenSatMap dataset**. It contains high-resolution satellite images with fine-grained annotations, covering diverse geographic locations and popular driving datasets [8; 11].

Introduction

Large-scale, up-to-date, and fine-grained map construction is fundamental to many tasks such as traffic control and autonomous driving. Compared with on-road mapping, constructing maps from satellite images is a highly efficient way for this purpose because of the large geographic coverage.

Although it is promising, parsing fine-grained road structures from satellite images for map construction is a highly challenging task for the following reasons. _(i)_ The resolution of satellite images is usually not high enough for fine-grained lane line detection. For example, the common level-19 satellite images have a resolution of 30 cm per pixel, which can barely make out a lane line that is 20 cm wide. _(ii)_ The lane lines in modern cities possess highly complex topological structures and function-based semantics. It not only poses challenges for designing models with enough capacities but also makes it difficult to build fine-grained datasets.

Existing benchmarks [6; 14; 18; 30; 22] have made attempts to handle the challenging tasks. However, they each have their limitations, preventing them from effectively supporting large-scale and fine-grained map construction. Specifically, existing benchmarks have the following limitations.

* **Coarse annotations**. All of these benchmarks support only coarse semantic-level lane segmentation, which is far from enough to parse fine-grained lane markings with various functionalities and highly complicated topologies.
* **Low resolution**. The images in existing benchmarks often have relatively low resolutions. The highest resolution they have is 0.3 m per pixel, which is inadequate for accurate perception of lane lines.
* **Small scale**. Existing benchmarks have relatively small scales. The largest of them have less than 10k 1024 \(\) 1024 images.
* **Unalignment with autonomous driving benchmarks**. Existing benchmarks solely focus on specific regions, limiting their further applications in autonomous driving, which also heavily relies on map construction.

Table 1 shows the basic information of them. Given the limitations of existing benchmarks and the challenges of this task, we construct **OpenSatMap**, a new dataset for map construction from satellite images, with the following unique features.

* **Fine-grained instance-level annotations**. Unlike the coarse semantic level annotations in previous datasets, we carefully label fine-grained instance-level lane lines according to fine-grained line attributes.
* **Higher resolution.** We collect level-20 satellite images with a resolution of 0.15 m per pixel, which is higher than the resolutions of all previous benchmarks. Considering the data accessibility and diversity, we additionally provide level-19 images with a resolution of 0.3 m per pixel.
* **Larger scale**. We collect and carefully annotate **38k** 1024 \(\) 1024 images and around **445k** instances, which is around **5x larger** than the largest one of the previous datasets in terms of the number of images.
* **Higher diversity.** We collect data from 60 cities and 19 countries around the world. The collected data is highly diverse and covers various road types, geographic environments, special structures, and traffic regulations.
* **Alignment with autonomous driving benchmarks.** In order to advance the map construction in autonomous driving, OpenSatMap covers the regions of nuScenes  and Argoverse 2  datasets, which are the most popular autonomous driving datasets. In addition, we manually align the GPS locations of our data with them for practical use.

Given these characteristics, we believe OpenSatMap can serve as a foundation for various applications, including city-scale map construction, lane detection, and autonomous driving.

## 2 Related Work

**Satellite Imagery Datasets.** Satellite imagery datasets can be used for several computer vision tasks such as instance segmentation [24; 46; 15], object detection [20; 50; 19; 33], semantic segmentation [49; 10; 42; 37], scene classification [43; 31], and change detection [17; 40; 45; 12]_etc._. These datasets include optical and hyper-spectral images with varying resolutions, covering a wide range of application fields such as urban [41; 10], agricultural [24; 15], transportation [37; 7; 6] and marine [3; 25] areas on a global scale. They provide a wealth of labeling information, such as building outlines, road networks, vegetation types, water distribution, _etc._, which greatly promotes the development of deep learning in the field of remote sensing images. In this paper, we are interested in the application of satellite images on road extraction and we will discuss the datasets for road construction using remote sensing imagery later.

**Satellite Imagery Datasets for Road Extraction.** Early datasets mainly focus on semantic labeling for the satellite images and contribute significantly to semantic-level road extraction. Massachusetts  contains 1171 images of size 1500 \(\) 1500 with a resolution of 1 m/pixel. DeepGlobe  collects images from Thailand, Indonesia, and India and scrapes labels from QGIS . It contains 8570 images with image size 1024 \(\) 1024. Roadtracer  uses OpenStreetMap  to label 300 images from big cities with 0.6 m/pixel. SpaceNet  contains 2780 images collected from Las Vegas, Paris, Shanghai, and Khartoum. There are some datasets manually labeled. Ottawa  collects several typical urban areas with 0.30 m/pixel spatial resolution and annotates the road surface, road edges and road centerlines. CHN6-CUG  collects 6 cities in China with 0.5 m/pixel. and CasNet  is composed of 224 images with a spatial resolution of 1.2 m per pixel.

However, existing satellite road extraction datasets tend to be relatively small [37; 14; 29; 53], or labeled by OpenStreetMap [47; 39; 6] and QGIS [18; 52] platforms, which are limited by the meta information in the database. Besides, _semantic_ labeling results in a wealth of under-utilized information. With the booming development of high-resolution remote sensing imagery, academics urgently need a higher resolution, more richly labeled dataset. Our work fills this gap by providing a high quality, higher resolution, and bigger dataset with _instance-level_ vectorized annotations. The readers may refer to Table 1 for a detailed comparison against previous datasets.

## 3 OpenSatMap Dataset

In this section, we introduce our pipeline to collect (Sec. 3.1), annotate (Sec. 3.2) high-resolution satellite images and demonstrate the statistics of OpenSatMap in Sec. 3.3.

### Data Collection

We collect images from Google Maps  using public Maps Static API, which is publicly available and has been processed by Google to remove sensitive information. Considering the limitations mentioned in Sec. 1, we collect the data OpenSatMap with the following guidelines and factors.

**High resolution.** To fulfill the goal of higher resolution. We collect level-20 satellite images, which have a resolution of 0.15 cm/pixel. In this way, our dataset has a higher resolution than all the existing datasets as Table 1 shows. Google Maps does not have real level-20 images in some regions. Therefore, during the collection, we carefully verify the resolution for each image to avoid the

   Dataset & \# of Images\({}^{*}\) & Resolution & GT Source & Labeling Level & Region \\  Massachusetts  & 2513 & 1.00 _m/pixel_ & OSM & Semantic & America \\ CasNet  & 77 & 1.20 _m/pixel_ & Manually & Semantic & - \\ DeepGlobe  & 8570 & 0.50 _m/pixel_ & QGIS & Semantic & 3 Counties \\ SpaceNet  & 4481 & 0.31 _m/pixel_ & OSM & Semantic & 4 Counties \\ Roadtracer  & 4800 & 0.60 _m/pixel_ & OSM & Semantic & 6 Counties \\ Ottawa  & 235 & 0.30 _m/pixel_ & Manually & Semantic & Canada \\ CHN6-CUG  & 4511 & 0.50 _m/pixel_ & Manually & Semantic & China \\  OpenSatMap (Ours) & 7224 & 0.30 _m/pixel_ & Manually & Instance, Vectorized & 60 Cities, 19 Countries \\   

Table 1: **Comparison against previous datasets.** OpenSatMap is the largest road extraction dataset with the highest resolution and the most detailed annotations. \({}^{*}\) denotes that we _standardize the size of images to 1024 \(\) 1024_ and calculate the number of images in all datasets.

"pseudo level-20 images" which are actually resized from level-19 images. Considering the practical requirements of users, we also additionally collect level-19 images. To be precise, **OpenSatMap19** stands for the level-19 part, and **OpenSatMap20** stands for the level-20 part.

Geographic DiversityThe roads in different geographic locations exhibit significant differences in the surrounding natural environment, construction regulations, spatial distribution, road conditions, and appearance. To ensure such diversities, we sample the geographic locations considering _(i)_ famous and typical cities around the world; _(ii)_ terrain with different appearances such as plains, mountains, and deserts; _(iii)_ regions with different road densities such as urban and suburban. In this way, we ensure the overall diversity of our dataset by controlling the geographic diversity. In the following, we take more factors into account to further improve the data quality and diversity.

Special Road StructuresUnlike the common straight roads, some roads have special and complicated structures such as flyovers, roundabouts, and winding roads. The lane instances of these regions demonstrate different shapes and distributions from the straight lanes. To ensure the models trained on our data could well handle these structures, we manually search and collect some regions containing them. Figure 1 shows examples of special road structures.

Traffic FeatureHere we use the term traffic feature to represent left-hand/right-hand traffic regulations and vehicle density. The left-hand/right-hand regulations determine the direction of lanes. The vehicle density has an impact on the difficulty of lane detection since vehicles may occlude the lanes. During data collection, we deliberately consider the regions with different driving regulations and balance the samples with different vehicle densities.

Alignment with Driving BenchmarkTo facilitate the map construction task in the field of autonomous driving, we make the collecting regions fully cover the driving region in famous driving benchmark nuScenes  and Argoverse 2 . Since Argoverse 2 only provides the relative pose without GPS, we manually recognize a couple of landmarks of each city in Argoverse 2 and find the GPS coordinates of these landmarks. Then the GPS coordinates of all samples in Argoverse 2 can be derived by the relative poses. Figure 2 showcases the alignment.

### Annotation

After the data collection, we employ experienced annotators to carefully annotate the data at a fine-grained instance level. The annotations are performed by a professional remote sensing imagery labeling team of approximately 50 annotators for labeling and 7 for quality checking. The total cost of labeling is roughly $72,000, _i.e._, $19 per image. Note that, for labeling, the image size of OpenSatMap20 we collect is 4096 \(\) 4096 and 2048 \(\) 2048 for OpenSatMap19.

Figure 2: **Alignment with driving benchmark**.

Line representation.To represent the lane lines with highly variable lengths and curvatures, we adopt vectorized polylines as the representation. Each line contains a wealth of category and attribute information. Meanwhile, the order of the points in the polylines defines the line direction.

Line categories.We first categorize all lines into three categories: **curb**, **lane line**, and **virtual line**. A curb is the boundary of a road. Lane lines are those visible lines forming the lanes. A virtual line means that there is no lane line or curb here, but logically there should be a boundary to form a full lane. For example, a fork in the road breaks a curb into two segments, then we use a virtual line to connect the two segments. Figure 2(a) shows examples of these three categories.

Where should it be labeled?Generally speaking, we label all three categories above. However, there is usually occlusion in the satellite images. If a line is partially occluded by buildings, trees, and vehicles but its complete shape can be inferred with high confidence, we annotate it. Fully occluded lines are not labeled. A special case is that overpasses usually occlude the bottom roads, where the occluded part is not labeled. We show these cases in the supplementary materials.

Fine-grained line attributes.Lines are assigned with a couple of attributes based on their appearance and functionalities. Specifically, there are 8 attributes as follows:

* The colors of lines.
* The line type such as solid line, thick solid line, dashed line, and short dashed line.
* The number of lines such as single lines and double lines.
* Lines with special functionalities such as bus lanes, guide lines, lane-borrowing areas.
* Whether it is bi-directional.
* Whether it is the outermost boundary of the pavement.
* The level of occlusion.
* The level of clearness.

Figure 2(b) shows some examples of different attributes.

Instances definition.After defining the line attributes, we further define the instance using the following guidelines. _(i)_ Different instances have different attributes. For example, if a solid line becomes a dashed line, we cut the line into two instances. Figure 3(a) shows an example of such attribute change. _(ii)_ When lines fork or merge (_e.g._, "Y"-shape point), we break the lines into multiple instances as Figure 3(b) shows.

Figure 4: **Definition of instances (_zoom in_ for best viewing). In (a), a change of line type results in two instances sharing the same point. In (b), a line should be divided into three instances when it is forked or merged.

Figure 3: **Examples of categories and attributes**.

Image-level tags.Furthermore, we provide OpenSatMap20 with 4 additional image-level tags to describe general information, including image clearness, overall vehicle density, urban/suburban/rural, and the existence of special road structures. We present the details in the supplementary materials.

### Statistics of OpenSatMap

In this section, we summarize the statistics of the established OpenSatMap. There are 1806 \(2048 2048\) images in OpenSatMap19 and 1981 \(4096 4096\) images in OpenSatMap20. We randomly divided them into training set, validation set and testing set in the ratio of 6:2:2. Figure 5 shows the number of instances in each image in OpenSatMap19 and OpenSatMap20. The number of instances of most images is under 300. Figure 6 illustrates the distributions of attributes. Most attributes have a non-uniform distribution, which reflects the real condition of the roads. Figure 7 shows the tag distribution in OpenSatMap20.

## 4 Instance-level Line Detection

### Formulations and Baseline Method

Given an input image \(^{H W 3}\), where \(H W\) indicates the input resolution, we aim to detect each line instance in the input image. For each instance, we use polylines as the _vectorized_ representation and pixel-wise instance-level masks as the _rasterized_ representation. A polyline is defined as a set of points \(^{N 2}\), where \(N\) means the number of sampled points. \([:,0]\) and \([:,1]\) represent the \(x\) and \(y\) coordinates, respectively. One can rasterize polylines to pixel-level masks by simply connecting adjacent points with lines with a pre-defined line width.

The instance-level line detection task needs to convert an RGB image \(\) into a set of polylines \(\{_{i}\}_{i=1}^{M}\), where \(M\) is the number of instances. We develop a simple baseline without whistle and bells illustrated in Figure 8. We decompose this task into 3 steps: (1) semantic segmentation, (2) instance detection, and (3) instance vectorization. (2) and (3) are post-processing techniques. Detailed formulations for each step are provided as follows and implementation details are provided in the _Supplementary materials_.

Figure 5: Number of instances in each image in OpenSatMap19 (left) and OpenSatMap20 (right).

Figure 6: Distribution of line attributes in OpenSatMap20.

**Semantic segmentation \(f:^{H W 3}\{0,1,,C-1\}^{H W}\)** aims to classify each pixel into a unique semantic category, where \(C\) is the number of categories. We denote \(=f()\) as the segmentation result. We adopt SegNeXt  as the segmentation network since it is quite efficient for high-resolution images.

**Instance detection \(g:\{0,1\}^{H W}\{0,1,,M\}^{H W}\)** converts the binary segmentation mask of each category into a pixel-level mask, where each instance shares the same value within this mask. Technically, we first extract the binary mask \(_{c}\{0,1\}^{H W}\) of each category \(c\) and then apply the watershed algorithm  to obtain instance-level masks.

**Instance vectorization \(h:\{0,1\}^{H W}^{N 2}\)** aims to build a _vectorized_ representation of each instance, which contains (1) instance denoising, and (2) point sampling. Instance denoising follows a simple sample-then-reconstruct pipeline. Specifically, for each instance \(i\) that belongs to category \(c\), we first extract its binary mask \(_{c}^{i}\{0,1\}^{H W}\). We subsequently sample \(N\) points and obtain their coordinates \(_{c}^{i}^{N 2}\). Then, we simply connect adjacent points with lines and remove instances with fewer than 100 pixels, obtaining the denoised instance map \(}_{c}^{i}=h(_{c}^{i})\) for each instance \(i\) that belongs to category \(c\). Note that \(}_{c}^{i}\) is the final _rasterized_ representation of a line instance. As for point sampling, we sample \(N\) points from \(}_{c}^{i}\), resulting in \(}_{c}^{i}=(}_{c}^{i})\) to build the _vectorized_ representation.

### Evaluation

**Semantic-level evaluation.** We adopt the mean intersection over union (mIoU)  over different categories as the metric to evaluate the performance at the semantic level.

**Instance-level evaluation.** We adopt the average precision (AP) as the metric to evaluate the performance at the instance level. Under instance segmentation settings, a common practice is to use the Mask AP (AP\({}^{}\)) by leveraging a mask IoU threshold to identify whether an instance is a true positive sample or not . However, line markings typically exhibit _narrow_ features and segmentation outputs frequently lack precision in edge definition. Consequently, relying exclusively on Mask IoU thresholds may be excessively strict. To this end, we incorporate Chamfer distance proposed by  as an alternative. Chamfer distance between two sets of points \(^{M 2}\) and \(^{K 2}\) is defined as:

\[D_{}(,)=_{i=1}^{M}_{j=1,2,,K}d(p_{i},q_{j}),\] (1)

where \(d(,)\) is the Euclidean distance between two points. Then, we get Chamfer AP (AP\({}^{}_{D}\)) by choosing the distance threshold \(D\).

Figure 8: Illustration of our baseline method.

Figure 7: **Image-level tag distribution in OpenSatMap20.** CC = complete clear, PC = partially clear, Sp = sparse, M = moderate, D = dense, S = suburban, R = rural, U = urban, B = bridge, N = None, RA = roundabout, OP = overpass.

### Main results

Empirical results in Table 2 indicate that _instance-level_ line detection is a much more challenging task compared with semantic-level segmentation, since values of AP\({}^{ C}\) and AP\({}^{ M}\) are much lower than that of mIoU.

Fundamentally, this issue arises because instances are associated with fine-grained attributes while semantic segmentation only involves several categories. Consequently, identifying true positive instance predictions becomes a notably difficult task.

**Qualitative results.** We provide qualitative results in Figure 9. Our method manages to detect line instances. Although the visual results of our model are almost satisfactorily presented, they are accompanied by disappointingly low quantitative scores of AP\({}^{ C}\) and AP\({}^{ M}\). Usually, our method fails to detect accurate line instances, when the segmentation performs poorly, such as failing to predict separate masks when attributes change, and adhered segmentation masks of two separate lines. This disparity indicates that this benchmark is of substantial difficulty, necessitating deeper exploration of an effective end-to-end method.

## 5 Potential Application in Autonomous Driving

To facilitate the map construction task in autonomous driving, we make OpenSatMap deliberately cover the regions of nuScenes  and Argoverse 2  datasets. Recently, a line of work has proved the effectiveness and success of using additional map information to boost the performance of HDMap construction for autonomous driving. MapEX  uses historically stored map information to optimize the construction of current local high-precision maps. NMP  uses a neural map prior to boost the performance of VectorMapNet .

Some more closed work uses satellite image information to enhance map construction. SatorHDMap  leverages satellite images as an additional input modality for MapTR . P-MapNet  extracts weakly aligned SDMap from OpenStreetMap , and encode it as an additional feature for MapTR, achieving better performance. Our OpenSatMap has the potential to advance this field. We take SatorHDMap as an example. We use intersection-over-union (IoU) as the metric following

   Dataset & AP\({}^{ C}_{0.9}\) & AP\({}^{ C}_{1.5}\) & AP\({}^{ C}_{3.0}\) & AP\({}^{ C}_{4.5}\) & AP\({}^{ M}_{50.95}\) & AP\({}^{ M}_{50}\) & AP\({}^{ M}_{75}\) & mIoU \\  OpenSatMap19 & 16.04\(\)0.35 & 22.68\(\)0.35 & 26.88\(\)0.52 & 29.18\(\)0.22 & 3.66\(\)0.15 & 10.66\(\)0.44 & 1.45\(\)0.12 & 28.71\(\)0.38 \\ OpenSatMap20 & 20.30\(\)0.21 & 25.93\(\)0.35 & 29.50\(\)0.40 & 31.38\(\)0.43 & 6.98\(\)0.21 & 16.05\(\)0.32 & 5.26\(\)0.13 & 33.69\(\)0.45 \\   

Table 2: **Evaluation on OpenSatMap validation set.** The line width is set to 6 pixels in OpenSatMap20 and 3 pixels in OpenSatMap19 by default. AP\({}^{ M}\) means that the mask IoU is used when determining true positives, while AP\({}^{ C}_{0}\) means Chamfer AP with a threshold of \(D\) meters. AP\({}_{x}\) denotes that the threshold is set to \(x\). AP\({}_{x:y}\) indicates the _averaged_ values, varying the threshold from \(x\) to \(y\).

Figure 9: **Qualitative results** on OpenSatMap19 (the first two rows) and OpenSatMap20 (the last two rows) test split. For each scene, we put **(a)** the input image, **(b)** the instance prediction, **(c)** the _denoised_ instance prediction, and **(d)** the annotation, **from left to right**, respectively.

semantic map learning . Let \(_{1},_{2}^{H W D}\) be dense predictions of shapes, \(H\) and \(W\) are the height and width of the grid, \(D\) is the number of categories. IoU can be expressed as follows:

\[(_{1},_{2})=_ {1}_{2}|}{|_{1}_{2}|},\] (2)

where \(||\) is the size of the set. Table 3 shows the results using SatforHDMap  with OpenSatMap. The use of corresponding satellite imagery significantly enhanced the performance of online map construction, demonstrating the potential superiority of this approach.

## 6 Availability and Maintenance

OpenSatMap is collected from Google Maps, which is publicly available. According to the regulations of Google, the use of this dataset is restricted to **non-commercial research**. Our project page is https://opensatmap.github.io, which contains the full dataset with annotations and code repository. The repository contains the full-stack tools to use this dataset, including code for collecting images from Google Maps, documents, baseline models, and evaluation tools. We encourage the community to further explore more tracks and effective methods using OpenSatMap. This dataset will be maintained over the long term and continually iterated upon.

## 7 Limitations and Potential Social Impact

OpenSatMap has the following limitations. _(i)_ OpenSatMap is collected from Google Maps, which is not updated in real time, and certain areas may be outdated and not reflect the current conditions. Besides, the timing of captured remote sensing images may not be aligned with nuScenes and Argoverse dataset. Although we noticed this potential misalignment on road structures and asked the annotators to check the consistency of road structures between our data and the data in the driving datasets during annotating process, there are still very few inconsistencies. _(ii)_ High-resolution (level-20) images in certain areas are not available or may be covered by the cloud, limiting the diversity of collection locations to some extent. _(iii)_ Although the annotators are quite professional and we conduct strict sanity checks, the subjectivity of annotators may inevitably lead to slightly inconsistent annotation results since this project employs around 60 annotators. In addition, the inconsistent annotation can also stem from the different training and expertise levels of the annotators. A potential social impact is that it might cause a safety risk for driving if the model is directly trained on our dataset, which contains outdated data as discussed above.

## 8 Conclusion

In summary, we introduce OpenSatMap, a large-scale, high-resolution, geographically diverse satellite dataset with detailed annotations and alignment with driving benchmarks. To validate the utility of OpenSatMap, we construct the instance-level line detection track and provide a baseline for it. Moreover, we use OpenSatMap to improve the performance of online map construction, which shows the great potential of autonomous driving. We believe that our carefully annotated high-quality OpenSatMap can serve as the foundation for various applications, including lane detection, city-scale map construction, and autonomous driving.