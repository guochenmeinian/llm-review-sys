# Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling

Shuaipeng Li\({}^{*,1,}\), Penghao Zhao\({}^{*,1,2}\), Hailin Zhang\({}^{*,1,2}\), Xingwu Sun\({}^{*,1,3}\), Hao Wu\({}^{1}\),

**Dian Jiao\({}^{1}\)**, **Weiyan Wang\({}^{1}\)**, **Chengjun Liu\({}^{1}\)**, **Zheng Fang\({}^{1}\)**, **Jinbao Xue\({}^{1}\)**, **Yangyu Tao\({}^{1}\)**,

**Bin Cui\({}^{2,4}\), Di Wang\({}^{1,}\)**

\({}^{1}\) Tencent Hunyuan

\({}^{2}\) School of Computer Science & Key Lab of High Confidence

Software Technologies (MOE), Peking University

\({}^{3}\) University of Macau

\({}^{4}\) Institute of Computational Social Science, Peking University (Qingdao)

Equal contribution. Corresponding author.

###### Abstract

In current deep learning tasks, Adam-style optimizers--such as Adam, Adagrad, RMSprop, Adafactor, and Lion--have been widely used as alternatives to SGD-style optimizers. These optimizers typically update model parameters using the sign of gradients, resulting in more stable convergence curves. The learning rate and the batch size are the most critical hyperparameters for optimizers, which require careful tuning to enable effective convergence. Previous research has shown that the optimal learning rate increases linearly (or follows similar rules) with batch size for SGD-style optimizers. However, this conclusion is not applicable to Adam-style optimizers. In this paper, we elucidate the connection between optimal learning rates and batch sizes for Adam-style optimizers through both theoretical analysis and extensive experiments. First, we raise the scaling law between batch sizes and optimal learning rates in the "sign of gradient" case, in which we prove that the optimal learning rate first rises and then falls as the batch size increases. Moreover, the peak value of the surge will gradually move toward the larger batch size as training progresses. Second, we conduct experiments on various CV and NLP tasks and verify the correctness of the scaling law.

## 1 Introduction

Deep learning techniques, initiated by Stochastic Gradient Descent (SGD) learning on large datasets, have significantly revolutionized various real-world applications . Over the past decade, numerous optimizers, such as momentum , Adagrad , ADADELTA , RMSprop , Adam , Adafactor , and Lion , have been introduced to stabilize the iterative learning process and expedite convergence. Among them, the Adam optimizer is the most widely used across various domains including Computer Vision (CV) , Natural Language Processing (NLP)  and many others . It retains the first and second moment information of parameters to facilitate adaptive learning step size. Unlike SGD-style optimizers that use the raw gradient to determine the learning direction and step size, Adam and its variants (Adagrad, RMSprop, Lion, etc.) employ the sign of gradient for this purpose, thereby ensuring greater robustness .

Beyond specific hyper-parameters in optimizer configurations, the batch size and the learning rate are the most critical hyperparameters influencing convergence. As the scale of training datasets in variousworkloads (e.g. CV [19; 20], NLP [21; 14], and others) continues to grow, there is an increasing demand for large batch size training across multiple data parallel workers. However, large batch training presents great challenges for robust training and meticulous tuning. The learning rate, which determines the actual step size in each learning iteration, is highly dependent on the batch size used. Prior research has explored methods to determine an optimal learning rate according to the batch size in scenarios utilizing SGD optimizers, including square root scaling , linear scaling [19; 23], and others . Among these, the empirical model focusing on large batch training  yields convincing results in both theoretical and empirical contexts, proposing the following rule to depict the relationship between the optimal learning rate and the batch size:

\[_{opt}(B)=}{1+_{noise}}{B}}\] (1)

For Adam-style optimizers, though existing works also provide some approximation [24; 23], they fail to capture the true scaling law of optimal learning rates with batch sizes. For illustrative purposes, Figure 1 presents curves that simulate the optimal learning rates for the Adam optimizer. We find that, in scenarios involving small batch sizes, the optimal learning rate initially increases and then decreases, resembling a surge in the sea, as depicted by the dashed orange line. For large batch sizes, we identify a value to which the optimal learning rate converges. The solid orange line represents a schematic based on our findings for both small and large batch sizes, showing that the learning rate tends to rise initially, then decrease, and subsequently gradually ascend to asymptotically approach a stable value. For clarity in visualization, we have omitted the final asymptotic portion of the curve.

In this paper, we aim to elucidate and formalize the connection between optimal learning rates and batch sizes for Adam-style optimizers. By following the notation from the empirical model  and conducting a more in-depth theoretical analysis, we discover that the relationship between the optimal learning rate and the batch size in the above parameter update formular satisfies:

\[_{opt}(B)=}{(_{noise}}{B}}+_{noise}}})},\] (2)

which differs from SGD, especially when the batch size is not too large. Here the meaning of \(_{noise}\) is consistent with papers of scaling laws [24; 25], representing the trade-off point between training speed and data efficiency. When the batch size is equal to \(_{noise}\), the optimal learning rate reaches a local maximum in accordance with Eq 2. Furthermore, we provide additional proof that when the batch size becomes significantly large, the optimal learning rate gradually converges to a non-zero value. We also prove that the previous conclusions about training speed and data efficiency are still valid for Adam-style optimizers, and the variable \(_{noise}\) gradually increases as the training progresses. It is worth noting that when \(B_{noise}\), for SGD, the scaling law of optimal learning rates with

Figure 1: The relationship between the optimal learning rate and the batch size is different between Adam and SGD. The orange line represents the tendency of the optimal learning rate to converge to a non-zero value when the batch size is large enough.

batch sizes transitions into linear scaling, consistent with previous conclusions [19; 23]:

\[_{opt}(B)}{_{noise}}B;\] (3)

while for Adam, the relationship transitions into square root scaling, aligning with previous approximations [23; 24]:

\[_{opt}(B)}{_{noise}}} .\] (4)

In addition to theoretical analysis, our extensive empirical study on various CV and NLP workloads further validate the conclusions. The true optimal learning rate, across different Adam configurations, exhibits a clear downward trend after reaching its peak value as the batch size increases. This behavior contradicts previous research, but demonstrates the correctness and generalizability of our theory. The experiments also reveal a gradual increase in the variable \(_{noise}\), corresponding to the peak optimal learning rate, as training progresses.

## 2 Theorems

### Batch Size and Optimal Learning Rate

In this section, we theoretically derive the optimal learning rate for a given batch size. Initially, we introduce an approximation of Adam-style optimizers. In alignment with the insights elucidated in , a thorough examination of the Adam optimizer and its variants reveals that their primary distinction from SGD resides in the utilization of the gradient's sign for updates during each iteration, as opposed to the gradient itself:

\[_{i+1}=_{i}- sign(G_{est}),\] (5)

where \(_{t}\) is the parameter at time \(t\), \(G_{est}\) is the gradient estimated via mini-batch, and \(\) is the learning rate. As the batch size increases, the expected value of the update amount tends to saturate. For example, assuming that the mean value of the gradient is positive, when the accumulated gradient of the mini-batch is positive, increasing the batch size will have no contribution to the signed update amount. This is significantly different from the behavior of SGD where the larger the batch size, the more accurate the gradient estimate. In Appendix A, we provide a detailed discussion on this approximation for the Adam optimizer. Next, we derive the optimal learning rate that maximizes the loss improvement. And then we establish a lemma that addresses the optimal learning rate given an estimated mini-batch gradient:

**Lemma 1**.: _Suppose that we are updating the parameter \(\) using the mini-batch gradient \(V\), with the true gradient being \(G\) and the true Hessian being \(H\). Then the optimal learning rate that maximizes the decrease in loss is:_

\[_{opt} argmax_{}\ [ L]= [V]}{tr[H cov(V)]+[V]^{T}H[V]},\] (6)

_and the corresponding loss improvement \( L\) is:_

\[ L_{opt}=[V]}{2}_{opt}.\] (7)

The proof is in Appendix B. Although our conclusion is based on an approximation, we adopt the equal sign here to simplify the analysis, following the notation used in previous work .

Now let's consider the case where \(V=sign(G_{est})\), and assuming that the estimated gradient \(G_{est}\) follows a Gaussian distribution. The Gaussian distribution assumption is motivated by the following: if the mini batch size is sufficiently large, we can invoke the Central Limit Theorem (CLT) and approximate the distribution as Gaussian - a common assumption in previous research [27; 28; 29; 30]. Furthermore, our experimental results confirm that the gradient distributions closely approximate Gaussian distributions, as illustrated in Figure 8 of Appendix H. We have the following theorem:

**Theorem 2**.: _Suppose the gradient of parameter \(i\) for each sample follows a Gaussian distribution with mean \(_{i}\) and variance \(_{i}^{2}\), the expected loss improvement is:_

\[ L_{opt}=_{j}_{i}_{ j}_{i}_{j}}{_{i}(1-_{i}^{2})H_{i,i}+_{i}_{j} _{i}_{j}H_{i,j}},\] (8)_and the corresponding optimal learning rate is_

\[_{opt}=_{i}_{i}}{_{i}(1-_{i}^{ 2})H_{i,i}+_{i}_{j}_{i}_{j}H_{i,j}},\] (9)

_where \(_{i}\) is a function (derived from the Gauss error function) with respect to the batch size \(B\):_

\[_{i}(B)=}_{0}^{}}{_{i}}}e^{-t^{2}}dt}{_{i}}}{+(}{_{i}})^{2}}}.\] (10)

We prove the above theorem in Appendix C.

An important observation from the proof is that, not only is the covariance matrix of \(sign(G_{est})\) related to \(B\), but its expected value also depends on \(B\). This implies that in Eq 6 the numerator is the first-order form of the function about \(B\), and the denominator is the second-order form of the function about \(B\):

\[(B)=+}=}.\] (11)

Therefore, the conclusion in the case of Adam cannot be derived by simply following the form mentioned in :

\[(B)}{(1+}{B})^{}}.\] (12)

Then we aim to derive the specific expression for the optimal learning rate with respect to the batch size through the following theorems.

**Theorem 3**.: _When \(B^{2}}{2_{i}^{2}}\), the optimal learning rate is a function with respect to batch size \(B\):_

\[_{opt}(B)(_{ noise}}{B}}+_{noise}}})}_{noise}}{2}_{i}^{2}}{_{i}}}}{_{i}H_ {i,i}}_{noise}}{2}_{i}^{ 2}}{_{i}}}}{_{i}H_{i,i}},\] (13)

_where \(_{noise}\) is a variable unrelated to batch size \(B\):_

\[_{noise}=H_{i,i}}{2_{i}_{j} _{j}}{_{i}_{j}}&i j\\ 0&i=jH_{i,j}}.\] (14)

_Defining \(B_{peak}\) as the batch size at which the optimal learning rate reaches a peak value, it is obvious that:_

\[B_{peak}=_{noise}.\] (15)

_The peak value is:_

\[_{max}=_{noise}}{2}_{i}^{2}}{_{i}}}}{_{i}H_{i,i}}.\] (16)

We prove the theorem in Appendix D. From the theorem we can finally get Eq 2, which implies that there is an interval where the batch size becomes larger and the optimal learning rate needs to be reduced. Considering that \(^{2}}{2_{i}^{2}}\) is much larger than normal batch sizes in research and industry (as shown in Figure 2), this theorem can cover most of the scenarios. To make the conclusion more comprehensive, we also derive the following theorem:

**Theorem 4**.: _When \(B^{2}}{2_{i}^{2}}\), the optimal learning rate becomes:_

\[_{opt}=|_{i}|}{_{i}_{j}sign(_{i})sign(_ {j})H_{i,j}}.\] (17)We prove the theorem in Appendix E.

Therefore, when B increases infinitely, the optimal learning rate will eventually converge to a non-zero value. If we make an (unrealistic) assumption that \(}{_{i}} sign(_{i})\), we will find that the lower bound of \(_{max}\) in Theorem 3 will become the one in Theorem 4, which means that the local peak value of the optimal learning rate is larger than the final convergence value. However, considering that the variance of the gradient in the later stages of training is very small, which makes the above conclusion \(}{_{i}} sign(_{i})\) difficult to establish, so the stable value in the later stages of training is more likely to exceed the local maximum. We provide a reference curve in Figure 1.

### Data/Time Efficiency Trade-off

Following the empirical model for large-batch training , we also review the trade-off between data and time efficiency during batch size selection. We have the following theorem:

**Theorem 5**.: _When \(B^{2}}{2_{i}^{2}}\), the derived loss improvement with respect to the batch size is_

\[ L_{opt}(B)=}{1+_{aug _{i}}}{B}},\] (18)

_where \( L_{max}\) is defined as_

\[ L_{max}=_{j}^{2}_{j}^{2}}{_{i} _{j}}}{2_{i}_{j}_{j}}{_{i} _{j}}&i j\\ 0&i=jH_{i,j}},\] (19)

We prove the theorem in Appendix F. This result aligns with the conclusion drawn in the SGD situation , indicating that many related conclusions also remain valid.

It has been concluded in previous work  that, when using the SGD optimizer with the same form as Eq 18, the relationship between training speed (number of steps \(S\)) and data efficiency (number of samples \(E\)) is given by:

\[(}-1)(}-1)=1.\] (20)

Here \(S_{(min)}\) represents training speed, the actual (minimum) possible number of steps taken to reach a specified model performance; and \(E_{(min)}\) represents data efficiency, the actual (minimum) possible number of training examples processed to reach that same level of performance. For more details, please refer to the Eq 2.11 and the Appendix D in . Additionally, as referenced in the Eq 2.12 in  and Eq 1.4 in , \(_{noise}\) is the balance point between training speed and data efficiency:

\[_{noise}_{crit}=}{S_{min}} }{L^{}}}.\] (21)

Since in Adam optimizer we arrive at the same Eq 18 as in SGD optimizer, the above equations 20 and 21 still hold. In Adam scenarios, \(B_{peak}=_{noise}\) is not only the local maximum of the optimal learning rate, but also the balance point between training speed and data efficiency. Moreover, as training progresses and the loss decreases, according to Eq 21, \(B_{peak}\) will gradually becomes larger.

### Summary

In this section, we have drawn several conclusions from our theoretical analysis, which are summarized as follows:

1. As the batch size increases, the optimal learning rate demonstrates a decreasing trend within a specified range (Eq 2).
2. The batch size that corresponds to the local maximum optimal learning rate is consistent with the balance point of training speed and data efficiency (Eq 21). As the training progresses and the loss decreases, \(B_{peak}\) will gradually becomes larger.

Experiments

In this section, we carry out a series of experiments to corroborate the theoretical scaling law we proposed in Section 2 and detail the experimental workloads and configurations in Section 3.1. The process for deriving the estimated variables from our theory is elucidated in Section 3.2. We also showcase and dissect the applicability of our scaling law across a variety of workloads in Section 3.3.

### Experimental Setup

**Workloads.** In our empirical study, we incorporate 4 open-source workloads that are extensively utilized: (1) training a 5-layer CNN model on the Fashion-MNIST , which is a typical CV test case to start with. It consists of 60000 28x28 grayscale images in 10 classes; (2) training a ResNet-18 model  on the Tiny-ImageNet dataset , which contains 100000 images of 200 classes (500 for each class) downsized to 64x64 colored images. In each epoch we train the model with random 10k samples to reduce the overall complexity; (3) training a dense Transformer model  (simplified DistilGPT2 ) on the ELI5-Category dataset , which is a smaller but newer and categorized version of the original ELI5 dataset . It contains 10.5k complex, diverse questions that require explanatory multi-sentence answers. (4) training a fine-grained Mixture-of-Experts (MoE) model, similar in structure to Mistral-MoE  but with shared experts , on the RedPajama-v2 dataset , which contains 30 trillion filtered and deduplicated tokens (100+ trillions raw) from 84 CommonCrawl dumps covering 5 languages, along with 40+ pre-computed data quality annotations. These workloads are popular in both academia and industry, covering typical deep learning tasks in the domains of CV and NLP.

**Batch sizes and learning rates.** To showcase the optimal learning rate for each batch size configuration, we leverage a grid-search-style experiments set. Each point in the grid search corresponds to a certain round with the same configuration but a different random number seed. The start point, stop point, and the interval of different workloads are listed in Table 1. In NLP tasks, the term "batch size" refers to the number of tokens in a batch, as practiced in related works .

**Hyper-parameters.** Since we derive the theorems on Adam-style optimizers, we conduct experiments using the Adam optimizer. We experiment on both the "sign of gradient" configuration (\(_{1}=0\), \(_{2}=0\)) and the default hyper-parameters (\(_{1}=0.9\), \(_{2}=0.999\)), as shown in Table 1.

**Hardware environment.** We execute each round of experiments utilizing an NVIDIA A100 card. The training time of each round for the datasets are approximately 1 hour for Fashion-MNIST, 1.5 hours for TinyImageNet, 2 hours for ELI5-Category and 11 hours for RedPajama-v2. Given our primary focus on the convergence process, the specific hardware environment does not matter in our experiments. Our theoretical and empirical findings can be generalized to other hardware settings. Additionally, some system optimizations [40; 41; 42; 43] are also beneficial to enhancing training efficiency.

### Variable Estimation

We try to estimate the value of \(_{noise}\) and the expectation of \(_{max}\) through curve fitting. After using Eq 21 to simplify Eq 20 (see Appendix G for details), we can record the actual possible number of steps taken \(S\) and the actual possible number of training examples processed \(E\) to reach a specified level of performance corresponding to the optimal learning rate of each batch size in the grid search results, and then perform linear fitting to obtain the estimated value of \(_{noise}\):

\[=-_{noise}+}\] (22)

    &  &  &  &  \\  & \(_{1}\) & \(_{2}\) & **Start** & **Stop** & **Step** & **Start** & **Stop** & **Step** \\  CNN & 0.9 & 0.999 & 1e-4 & 1e-3 & 1e-4 & 1 & 12 & 1 & 100 \\ CNN & 0.9 & 0.999 & 1e-4 & 1e-3 & 1e-4 & 64 & 1164 & 100 & 100 \\ DistilGPT2 & 0.9 & 0.999 & 1e-5 & 1.09e-3 & 1.2e-4 & 4 & 114 & 10 & 30 \\ DistilGPT2 & 0.0 & 0.0 & 1e-5 & 5.5e-4 & 6e-5 & 4 & 114 & 10 & 30 \\ ResNet18 & 0.0 & 0.0 & 1e-4 & 7.876e-4 & 7.65e-5 & 16 & 376 & 33 & 100 \\ MoE & 0.9 & 0.999 & 2e-6 & 6e-5 & 2e-6 & 192k & 12M & 1.2M & 10 \\   

Table 1: Grid search configurations.

Subsequently, we use the optimal learning rate and batch size of the grid search results to estimate the max optimal learning rate of Adam-style \([_{max}]_{Adam}\):

\[[_{max}]_{Adam}=[}{2}( _{noise}}{B}}+_{noise}}} )]\] (23)

and SGD-style \([_{max}]_{SGD}\):

\[[_{max}]_{SGD}=[_{opt}(1+ {_{noise}}{B})^{}]\] (24)

Previous research  represents the SGD optimizer and the Adam optimizer using Eq 24 with \(=1\) and \(=0.5\), respectively. We include these fitted curves as comparisons in the following section.

While we use grid search to estimate the value of \(_{noise}\), in practice we can efficiently approximate it using the scaling law from previous studies , where \(_{noise} B_{crit}=}{L^{1/}}\). With this approximation, we only need a simple search for a pair of (batch size, optimal learning rate) to determine the final hyperparameter \(_{max}\). Therefore, the costly grid-search can be avoided.

### Results

Following Section 3.2, we first estimate the variables and fit the curves using observations, then conduct grid-search-style experiments for learning rates and batch sizes.

Figure 2, 3, 4, 5 illustrate the experimental results of CNN-FashionMNIST, ResNet18-TinyImageNet, DistilGPT2-EL15Category and MoE-RedPajama-v2, respectively. Each figure is divided into two parts: the left subfigure illustrates the grid-search results for batch sizes and learning rates, and these data points are utilized to fit the curve of Eq 22 in the right subfigure. In order to estimate the variables, we train models from scratch using different learning rates and batch sizes, then record the number of steps \(S\) and examples \(E\) in each experiment to achieve an equivalent training loss. Using the recorded \(S\) and \(E\), we fit the curve in the right subfigure and obtain the estimated \(_{noise}\). In the left subfigure, upon achieving the desired training loss, all experiments continue to train the same number of steps. Any subsequent decrease in training loss is represented through different colors, as indicated in the color bar. For each batch size, we highlight the optimal learning rate that results in the most significant reduction in training loss. We also plot the batch size \(_{noise}\) that corresponds to the peak optimal learning rate, the fitted SGD curves with \(=0.5\) and \(=1\) as derived from previous research , and the fitted Adam curve as derived from our theorems.

For the CNN-FashionMNIST workload, we train exactly 10 more step after achieving the desired training loss. As shown in Figure 2(a), the batch size bound \(^{2}}{2_{i}^{2}}\) for Theorem 3 is around 800 in this task. Given the simplicity of the CNN-FashionMNIST workload, commonly-used batch sizes are usually smaller than the batch size bound. We plot the situations corresponding to Theorem 3 and Theorem 4 in Figure 2(b) and 2(c), respectively. In both cases, the trend predicted by our theory is consistent with the actual optimal learning rate performance, showing a declining range at small batch sizes and a saturation range at large batch sizes.

For the ResNet18-TinyImageNet workload, we train 50 more steps after achieving the desired training loss. We plot the figures for Theorem 3 at different achieved training losses, which represent the progress of training, as shown in Figure 3. The observed optimal learning rates primarily exhibit a downward trend after the batch size exceeds the estimated \(_{noise}\). Although the SGD curve with \(=0.5\), which is claimed by  to represent the Adam optimizer, serves as a good approximation in certain cases, it fails to capture the peak optimal learning rate as our Adam curve does. Comparing the red dashed lines in different figures, we can see that the estimated \(_{noise}\) gradually increases as the training progresses (i.e. training loss decreases), which corroborates the second conclusion in Section 2.3.

For the DistilGPT2-Eli5Category workload, we train 50 more steps after achieving the desired training loss. As shown in Figure 4, we test on two distinct Adam configurations for Theorem 3: the first with \(_{1}=0.0\), \(_{2}=0.0\), and the second with \(_{1}=0.9\), \(_{2}=0.999\). In both configurations, promising learning rates that lead to a substantial decrease in loss are consistent with our Adam curve. It is worth noting that another curve, SGD with \(=0.5\), also provides a suitable approximation in this scenario. To more clearly demonstrate the accuracy of our theoretical predictions, we present detailed results from a finer-grained grid search in Figure 6 of Appendix H. These experiments demonstrate that our theorems can be generalized to different optimizer configurations, validating the analysis in Appendix A.

For the sparse MoE model using the RedPajama-v2 dataset, we train 300 more steps after achieving the desired training loss. Figure 5 demonstrates that our predictions on optimal learning rate are both accurate and appropriate.

In addition to the above workloads, we also conduct an analysis of experimental results from third parties, confirming that our conclusions remain valid. Detailed results are presented in Figure 7 of Appendix H.

## 4 Discussion

We have carried out empirical studies on representative workloads using the Adam optimizer. Our investigation into the scaling laws of learning rates relative to batch sizes has provided deeper insights into the training dynamics of deep learning models. This understanding can help fine-tune hyperparameters, enhance convergence speeds, and circumvent exhaustive grid searches. By leveraging prior knowledge that the optimal learning rate decreases after reaching a peak, researchers and engineers can more effectively adjust the learning rate to achieve efficient training outcomes.

In real-world applications, there are numerous different learning workloads . Other factors, beyond the scope of this paper, may influence the learning process - the specific optimizer used, weight decay, gradient clipping, etc. While we assert that our theorem can be applied to numerous practical scenarios, it may not fully encompass all situations involving intricate training configurations.

As one of our conclusions points out, the variable \(_{noise}\) will gradually increases as the training progresses. It is natural to implement adaptive learning rates (and batch sizes) if possible, to speed up the training process. As mentioned in , using adaptive batch sizes and warmed-up learning rates brings considerable benefits. Fully exploring the potential of batch size and learning rate scheduling requires meticulous design, which we leave as future work.

Figure 2: Batch size versus optimal learning rate within the context of CNN trained on FashionMNIST.

Figure 4: The relationship between batch sizes and optimal learning rates within the context of DistilGPT2 trained on Eli5Category.

Figure 3: The relationship between batch sizes and optimal learning rates within the context of ResNet-18 trained on TinyImageNet. The red dashed line accurately predicts the peak value, and as the training loss decreases, the peak value gradually shifts to the right.

Our theory is based on the quadratic approximation of the loss function. Experimental results demonstrate that conclusions drawn from this second-order expansion effectively predict the surge phenomenon observed in most mainstream scenarios. However, recent studies [48; 49; 50] have proposed that quadratic approximations do not accurately capture the loss in scenarios involving large learning rates. We acknowledge the potential benefits of exploring higher-order approximations and consider this a promising direction for future research.

## 5 Related Work

Aiming to accelerate convergence, our work analyzes the scaling law of optimal learning rates with respect to batch sizes for Adam-style optimizers. Numerous related studies have been proposed to enhance the convergence of deep learning tasks by investigating optimal learning rates, developing new optimizers, and analyzing gradient noise.

Earlier works have proposed various scaling laws to tune learning rates for SGD-style optimizers, including square root scaling , linear scaling [19; 23], and others . They also obtained a scaling law for Adam-style optimizers [24; 23] through approximation, revealing a square root-like relationship where the optimal learning rate monotonically increases with the batch size. However, as illustrated in Section 1 and 2, their analysis holds only for small batch sizes, whereas the true scaling law exhibits greater complexity, with the optimal learning rate reaching a peak value at a balanced batch size.

There are many meticulously designed optimizers for various tasks and scenarios. First-order optimizers dominate nowadays deep learning models, including adaptive methods [3; 4; 5; 6; 7], sign-based methods [18; 8], layer-wise methods (for large-batch training) [51; 52]. Second-order optimizers [53; 54; 55], though with stronger theoretical guarantees, are not efficient for large-scale models due to quadratic complexity with respect to the number of parameters. Despite the emergence of new optimizers, empirical evidence confirms that Adam has remained the most widely used and effective optimizer over the past decade.

Our analysis is inspired by the empirical model of large-batch training , which predicts the useful batch size using the gradient noise scale. Gradient noise can help with learning rate determination , batch size selection [57; 58], and gaining deeper insights into the convergence process [59; 60; 61; 62].

## 6 Conclusion

In this paper, we established a scaling law between optimal learning rates and batch sizes for Adam-style optimizers. We theoretically proved that the optimal learning rate initially increases and then decreases as the batch size grows, and that the peak value of the surge represents a trade-off point between training speed and data efficiency. Through extensive experiments, we validated our theory on diverse deep learning models and datasets.

Figure 5: Grid search results for the MoE [37; 38] structure model.