# MindMerger: Efficiently Boosting LLM Reasoning in non-English Languages

Zixian Huang\({}^{1}\), Wenhao Zhu\({}^{1}\), Gong Cheng\({}^{1}\), Lei Li\({}^{2}\), Fei Yuan\({}^{3}\)

\({}^{1}\)State Key Laboratory for Novel Software Technology, Nanjing University

\({}^{2}\) Carnegie Mellon University

\({}^{3}\)Shanghai Artificial Intelligence Laboratory

{zixianhuang, zhuwh}@smail.nju.edu.cn, gcheng@nju.edu.cn

leili@cs.cmu.edu, yuanfei@pjlab.org.cn

Corresponding author.

###### Abstract

Reasoning capabilities are crucial for Large Language Models (LLMs), yet a notable gap exists between English and non-English languages. To bridge this disparity, some works fine-tune LLMs to relearn reasoning capabilities in non-English languages, while others replace non-English inputs with an external model's outputs such as English translation text to circumvent the challenge of LLM understanding non-English. Unfortunately, these methods often underutilize the built-in skilled reasoning and useful language understanding capabilities of LLMs. In order to better utilize the minds of reasoning and language understanding in LLMs, we propose a new method, namely MindMerger, which merges LLMs with the external language understanding capabilities from multilingual models to boost the multilingual reasoning performance. Furthermore, a two-step training scheme is introduced to first train to embeded the external capabilities into LLMs and then train the collaborative utilization of the external capabilities and the built-in capabilities in LLMs. Experiments on three multilingual reasoning datasets and a language understanding dataset demonstrate that MindMerger consistently outperforms all baselines, especially in low-resource languages. Without updating the parameters of LLMs, the average accuracy improved by 6.7% and 8.0% across all languages and low-resource languages on the MGSM dataset, respectively 2.

## 1 Introduction

One of the primary focuses of Artificial Intelligence research currently revolves around improving its reasoning capabilities , which is derived from the need to enable Large Language Models (LLMs)  to think rationally and perform functions like humans . Substantial progress has been made in English reasoning , but the performance in non-English, especially low-resource languages, still lags behind  due to the scarce of multilingual training data .

Existing work tries to use external models to compensate for the deficiencies of LLM in multilingual reasoning. Some works use the relearning-based strategy, which uses translation models to generate multilingual training data for fine-tuning LLMs to relearn reasoning in each language . Some other works use the replacement-based strategy, which use translation models to translate queries from non-English to English text for replacing the non-English input ofLLM [Shi et al., 2023]. Both strategies try to use the translation model to help LLMs master new capabilities, but the insufficient translation quality constrains the performance of these methods.

Moreover, certain capabilities, such as reasoning and language understanding, are built-in in LLMs and should be utilized without the need to develop them from scratch. For the reasoning capabilities, as illustrated in Figure 1, regardless of the language in which the same mathematical question is posed, the correct reasoning process remains consistent. It shows that the reasoning capabilities is universal rather than language-specific [Brannon, 2005]. For language understanding capabilities, while the Chinese question in the first example of Figure 1 may cause a reasoning error by failing in understanding the English-origin term "dozen or", the LLM successfully differentiated between "the week" and "the weekdays" in the Chinese question, contrasting with the failure of the English question in the second example. It shows that although the proficiency may not match that of English, expressions in non-English languages remain valuable to LLMs.

Considering that the built-in reasoning and language understanding capabilities of LLMs need to be better utilized, in this paper, we propose a new method **MindMerger**, which preserves the minds of reasoning and language understanding capabilities in LLMs, and merges the external language understanding capabilities from pre-trained multilingual models to boost the multilingual reasoning effects. To address the challenge of insufficient generation quality of external models, MindMerger feeds the LLM the undecoded query representation from the multilingual model rather than text. Additionally, it uses an augmentation strategy that combines the encoded query representation with the input of LLM to utilize both external and built-in language understanding capabilities.

Given the inconsistency in the representation space, understanding the query representation encoded by multilingual model is not trivial for LLMs. To address this, we propose a two-stage training scheme including the **mapping stage** and the **augmentation stage**. In the mapping stage, we train MindMerger to embed the language capabilities of the multilingual model into the LLM by using accessible general bilingual pairs such as translation data. In the augmentation stage, MindMerger is further trained to collaboratively utilize the built-in capabilities of LLM and the embedded external language capabilities by using query translation task data generated from translation model. Throughout the two stages, only a small number of parameters that connect two models are trained, while both the multilingual model and the LLM are frozen to prevent their built-in capabilities from forgetting.

Extensive experiments are conducted on three multilingual reasoning datasets and a language understanding dataset. Taking on the MGSM dataset [Shi et al., 2023] as an example, MindMerger outperforms all baselines and achieves a lead of at least 6.7% on the average accuracy across all languages, and its performance in low-resource languages is even more significant leading by at least 8.0% (SS 4.3). Compared with the replacement-based method that translates non-English text into English as the LLM input, MindMerger can lead by at least 6.6% in average accuracy based on the same translation model (SS 5.1). Benefiting from the accessible general bilingual pairs used in the mapping stage, the average accuracy across low-resource languages increased by 14.9% (SS 5.2).

Our contributions can be summarized as follows:

* We propose a new method MindMerger to boost the multilingual reasoning of LLMs, which preserves the built-in reasoning and language understanding capabilities of LLMs while augmenting them with the external language understanding capabilities from multilingual models.
* We propose a two-stage training scheme to help MindMerger sequentially learn to embed external capabilities and collaboratively utilize internal and external capabilities.

Figure 1: Examples of multilingual mathematical reasoning from the MGSM dataset. LLM can generate correct and incorrect answers when asked in different languages.

* MindMerger achieves the best results on four dataset about multilingual reasoning or understanding datasets, notably improving all languages, including low-resource languages, with an average accuracy increase of at least 6.7% and 8.0% on the MGSM datasets.

## 2 Related Work

**Multilingual Reasoning.** There have been some attempts to improve LLM's performance on multilingual reasoning. Several works design crated prompts to assist LLMs in reasoning (Huang et al., 2023; Qin et al., 2023), but their effectiveness diminishes when used with open-source LLMs like Llama (Touvron et al., 2023) due to limited capacity for multi-step instruction-following (Huang et al., 2023). Supervised fine-tuning LLMs is another effective way, where some works use translation models to translate query-response (Chen et al., 2023), or query-only (Zhu et al., 2024) to build multilingual task datasets, enabling LLMs to relearn reasoning or language understanding. Some other works utilize external models to generate English text (Shi et al., 2023) to replace the non-English input of LLMs, aiming to utilize the reasoning capabilities of LLMs directly. However, the above methods are limited by the generation quality of the translation model, and the built-in reasoning or language understanding capabilities of LLM are neglected. In contrast, MindMerger avoid the loss introduced by autoregressive decoding and employs an augmentation-based strategy that utilizes the built-in capabilities of LLMs to enhance multilingual reasoning performance.

**Model Merging.** Model Merging is a popular topic in recent LLM research. Generally, it aims to combine an external module to strengthen the capabilities that LLM lacks, such as multi-modal vision capabilities (Liu et al., 2023; Li et al., 2023; Zhu et al., 2023). Some works (Sun et al., 2021; Bansal et al., 2024) find that interpolating models with multilingual capabilities using cross-attention can improve the performance of multilingual tasks, but research on merging multilingual models and LLMs with English reasoning capabilities is still scarce. Recently, Yoon et al. (2024) merge the encoder of multilingual model and the LLM to improve the multilingual reasoning performance. However, the input of LLMs is completely replaced by the output of a multilingual encoder, making its built-in multilingual capabilities underutilized. In addition, only using English task data for training limits the performance of model merging. Instead of replacing the input of LLMs, we collaboratively utilize the features of the multilingual model and use the available general bilingual pair to train to obtain multilingual reasoning capabilities.

## 3 Approach

Given an LLM with skilled reasoning capabilities and useful language understanding capabilities, our target is to maintain the built-in capabilities and compensate for its shortcomings in non-English language understanding capabilities with an external multilingual model. To this end, as shown in Figure 2, we propose MindMerger that uses the output of the multilingual model as an augmentation complementing to the original input (SS 3.1). We further design a two-stage training scheme to learn the collaborative utilizing both the external and built-in capabilities (SS 3.2).

Figure 2: Overview of the model structure and training scheme of MindMerger, which consists of an LLM (blue) and a external model (yellow) and is trained by a two-stage scheme.

### Model Structure

Given a query \(q\) with \(l\) tokens, we first utilize the multilingual model to understand and encode it into a representation \(\) that is more general and reduces the challenge of multilingual understanding:

\[=(q)\,,\] (1)

where \(()\) is a pre-trained multilingual model, typically using its encoder part, and \(^{l d_{1}}\) is the hidden state output of the multilingual model with a dimension of \(d_{1}\).

The representation \(\) resides in the multilingual model space, which is separate from the LLM space and cannot be used directly. Therefore, we introduce a mapping layer:

\[}=()\,,\] (2)

where \(}^{l d_{2}}\) is the projection of \(\) on the space of LLM. Unless otherwise stated, the implementation of \(()\) is a two-layer multi-layer perceptron (MLP).

The acquisition of \(\) utilizes the language understanding capabilities of the external multilingual model. In order to take advantage of the built-in capabilities of the LLM, we prompt it directly:

\[=(q)\,,\] (3)

where \(^{l d_{2}}\) is the representation of query \(q\) in the space of LLM and \(()\) is the embedding layer of LLM.

Then, we concatenate the query representation from the multilingual model and the LLM for the collaborative utilizing of LLM's capabilities:

\[(},)=[;};;],\] (4)

where \(^{d_{2}}\) is the representation of the start token of LLM and \(^{d_{2}}\) is a trainable variable that denotes the boundary of \(}\). Finally, \((},)\) is fed to LLM to generate the response.

### Two-Stage Training

The training scheme of MindMerger is divided into two stages: mapping stage and augmentation stage. The former helps LLM learn to use the capabilities of multilingual model, and the latter helps LLM collaboratively utilize its own and the capabilities from multilingual model. Examples of training data for each stage are shown in Appendix D.

**Mapping Stage.** Given the representation spaces between multilingual model and LLM are distant different, it is not trivial for LLM to understand and utilize the external capabilities provided by multilingual model. In order to better learn to utilize external capabilities, we force MindMerger to focus on input from the multilingual model using the replacement-based strategy during this stage. Specifically, we simplify the input of Equation (4) as follows:

\[}=[;}; ].\] (5)

Since general bilingual pairs such as translation data is in vast availability, we use it from various languages to English to train MindMerger in this stage. The loss function is outlined as follows:

\[-*{arg\,min}_{}(Y|},,,)\,,\] (6)

where \(Y\) is the text of training target, \(\) and \(\) are the parameters of the multilingual model and LLM respectively, which are frozen during the training to prevent forgetting, and \(\) contains the trainable parameters of the mapping layer in Equation (2) and the boundary token \(\).

**Augmentation Stage.** Although MindMerger have learned to utilize external capabilities in the mapping stage, the replacement-based strategy may cause LLM to neglect the use of its own built-in capabilities. To help MindMerger further learn to merge capabilities from external and built-in LLM, in this stage we use the augmentation-based strategy as described in the Equation (4). The loss function is calculated as follows:

\[-*{arg\,min}_{}(Y|( },),,,)\,,\] (7)

where \(\) is initiated from the checkpoint of \(\) trained at the mapping stage, and the parameters of the multilingual model and LLM represented as \(\) and \(\), respectively, are kept constant during training. In this stage, the query translation task data generated from public translation models is used as the training data to adapt MindMerger to downstream task.

Experiments

### Compared Methods

**Our Methods.** Two variants of MindMerger were implemented. (1) The implementation described in SS 3.1 is denoted as **MindMerger-Soft**. (2) **MindMerger-Hard** augments the prompts of LLM with the translated query given by the translation model (Appendix D for the details of prompts).

**Baselines.** We compared our methods with three categories of baselines. (1) One basic method MonoReason Yu et al. (2023); Zhu et al. (2024) which is fine-tuned on the English task dataset. (2) Three relearning-based methods that use task data with query translation, including the full-parameter fine-tuning model **MultiReason-Full**Zhu et al. (2024), parameter-efficient fine-tuning model **MultiReason-Lora**Hu et al. (2022), and the state-of-the-art method **QAlign**Zhu et al. (2024) which first learns language understanding by training LLM on query translation data and then further fine-tunes LLM as MonoReason. (3) Two replacement-based methods that introduce external models, including **Translate-En**Shi et al. (2023) and **LangBridge**Yoon et al. (2024). Translate-En is a hard replacement-based which translates the query into English to replace the prompt of LLM. LangBridge is a soft replacement-based method which replaces the input of LLMs with the hidden states output by mT5-xl Xue et al. (2021). The prompts of each baselines are presented in Appendix D.

**Details.** Unless otherwise stated, we used the encoder part of mT5-xl Xue et al. (2021) as the multilingual model in our methods, used the NLLB200-3.3B as the translation model for baselines, and used the Llama 2-7B as the LLM for all methods. The influence of different multilingual models including encoder-only, decoder-only and encoder-decoder architectures will be analyzed in SS 5.1. Both MindMerger and all the baselines, except QAlign, are based on the same MonoReason model. Additionally, MonoReason and QAlign are trained based on the same LLM. Specifically, we used the publicly available checkpoint given by Yu et al. (2023) as MonoReason model for mathematical reasoning task. For all models, we set learning rate=2e-5, batch size=128, max length=512, and epoch=3 and used 8 NVIDIA A100 GPUs for training.

### Datasets

**Evaluation Datasets.** We experimented MindMerger with the latest multilingual mathematical reasoning **MGSM**Shi et al. (2023) and **MSVAMP**Chen et al. (2023), where MSVAMP serves as an out-of-domain test set. In order to evaluate the diverse multilingual reasoning capabilities of the models, a challenging multilingual dataset **X-CSQA**Lin et al. (2021) in commonsense reasoning task and a language understanding dataset **XNLI**Conneau et al. (2018) in natural language inference (NLI) task were used. The statistics of these datasets are presented in Appendix D.

**Training Datasets.** Three categories of training data were used in our methods and baselines. (1) **General bilingual pairs**. We used the translation data from the multilingual language to English and randomly sampled 100K of data for each language (except English) from the Lego-MT (Yuan et al., 2023b) dataset, which is a large-scale translation dataset that contains all the languages that involved in our experiments. (2) **English task data**. We used MetaMathQA (Yu et al., 2023) and MultiNLI (Williams et al., 2018) datasets for mathematical reasoning and NLI task, respectively. Similar to Huang et al. (2022), we unified the training set of X-CSQA, OpenBookQA (Mihaylov et al., 2018), ARC (Bhakhtavasalam et al., 2021) and QASC (Khot et al., 2020) to train commonsense reasoning task more fully. (3) **Query translation task data**. We used the translated results given by Chen et al. (2023) and the official dev set of XNLI for mathematical reasoning and the NLI task, respectively, and translated the X-CSQA training set based on M2M100-1.2B (Fan et al., 2021).

### Experimental Results

**MindMerger improves LLM performance on all datasets, especially benefiting low-resource languages.** MindMerger-Soft has an average accuracy better than all other baselines at least 6.7%, 3.2% on the MGSM and MSVAMP datasets in Table 1, demonstrating its remarkable multilingual reasoning capabilities. For the commonsense reasoning task in Table 2, MindMerger-Soft also significantly leads all baselines by at least 4.1%. In Table 3, MindMerger-Soft demonstrates advantages in language understanding, which significantly outperformed all the baselines (with p-value < 0.01). MindMerger-Hard achieves the best results except MindMerger-Soft on three out of four datasets, demonstrating the advantages of augmentation-based methods over other categories of methods.

[MISSING_PAGE_FAIL:6]

and the encoder part of encoder-decoder models M2M100 (Fan et al., 2021), NLLB-200 (Costa-jussa et al., 2022) and mT5 (Xue et al., 2021). The experimental results in Table 4 show that the encoder part of the encoder-decoder model and the encoder-only model are more suitable interpolate into MindMerger-Soft than the decoder-only model, which can achieve better performance than mGPT with a smaller number of parameters. M2M100 is the most cost-effective model, reaching or even exceeding the performance of XLM-RoBERTa-large and mT5-large while using only half of the parameters of XLM-RoBERTa-large and mT5-large.

**Our augmentation-based strategy outperforms the translate-then-replace strategy.** Comparing Translate-En and MindMerger-Hard which only differ in input strategy, augmentation-based MindMerger-Hard, based on the same translation model, consistently exceeds Translate-En by 5.0%, 5.9%, 4.7%, and 4.1% in the average accuracy. MindMerger-Soft further expands its lead with an increment of the average accuracy by at least 6.6% based on the same translation model.

**MindMerger-Soft is a better utilization of existing multilingual model.** As shown in Table 4, the performance of MindMerger-Soft consistently exceeds MindMerger-Hard under the same multilingual model with increases of average accuracy of 5.5% and 3.3% on two versions of M2M100, and 3.6% and 2.5% on two versions of NLLB-200. Although only the encoder part of the multilingual model is used, MindMerger-Soft merges LLM with a dense representation rather than decoded text based on sparse bag-of-words, enhancing the effectiveness of utilizing multilingual model.

**A more powerful multilingual model can better enhance the multilingual capabilities.** We compared the different sizes of each encoder-decoder models and consistently observed that the larger version outperformed the smaller one in Table 4. With the help of the larger model size, the average accuracy is improved to 1.1%, 1.0%, and 3.7% on M2M100, NLLB-200, and mT5, respectively. The improvement in low-resource languages is even more obvious with an average increase in accuracy of 1.1%, 1.0% and 3.7% in M2M100, NLLB-200, and mT5, respectively. This underscores the greater imperative to enhance language understanding in low-resource languages.

  
**M5SM** & **\# Param** & **Bn** & **Th** & **Sw** & **Ja** & **Zh** & **De** & **Fr** & **Ru** & **Es** & **En** & **Lrl.** & **Hrl.** & **Avg.** \\  Translate-En & & & & & & & & & & & & & & & \\  M2M100-418M & 484 M & 30.0 & **38.0** & 38.8 & 31.6 & **50.8** & 52.0 & 50.0 & 42.4 & 54.0 & **65.5** & 35.6 & 49.5 & 44.7 \\ M2M100-12.8 & 1,239 M & 42.4 & 34.0 & **49.6** & 40.8 & 42.8 & 55.2 & 50.4 & **49.2** & 46.8 & **65.5** & 42.0 & 50.1 & 47.1 \\ NLLB-200-13.8 & 1,371 M & 46.0 & 32.0 & 40.4 & 47.2 & 45.6 & 52.2 & 51.2 & 46.0 & 55.2 & **65.5** & 39.5 & 52.3 & 47.9 \\ NLLB-200-3.3B & 3,345 M & **48.4** & 37.6 & 37.6 & **49.2** & 46.8 & **60.4** & **56.4** & 47.6 & **59.6** & **65.5** & 41.2 & **55.1** & **50.6** \\  MindMerger-Hard & & & & & & & & & & & & & & & \\  M2M100-418M & 484 M & 39.6 & 28.0 & 36.4 & 49.2 & 48.8 & 60.0 & **56.4** & 55.6 & 58.4 & 64.8 & 34.7 & 56.2 & 49.7 \\ M2M100-12.8 & 1,239 M & 40.0 & **36.0** & 47.6 & **52.4** & 50.8 & 58.0 & **56.4** & **60.8** & 61.2 & 66.8 & 41.2 & 58.1 & 53.0 \\ NLLB-200-13.8 & 1,371 M & 44.0 & 30.0 & 42.8 & 48.0 & 53.6 & **61.6** & **56.4** & 56.4 & **63.6** & 70.8 & 38.9 & 58.4 & 52.6 \\ NLLB-200-3.3B & 3,345 M & **46.0** & **36.0** & **48.4** & **52.4** & **54.4** & 60.4 & 56.0 & 60.4 & 62.0 & **71.2** & **43.5** & **59.5** & **54.7** \\   MindMerger-Soft & & & & & & & & & & & & & & & & \\  mGPT & 1,418 M & 19.6 & 20.4 & 15.6 & 42.8 & 48.0 & 59.2 & 59.6 & 54.0 & **61.2** & 64.0 & 18.5 & 55.5 & 44.4 \\ mBERT & 178 M & 30.8 & 37.6 & 46.8 & 50.0 & 48.8 & 55.6 & 52.4 & 59.6 & 60.8 & 66.4 & 38.4 & 56.2 & 50.9 \\ XLM-RoBERTa-large & 560 M & 44.0 & 52.4 & 50.4 & 52.4 & 54.0 & 60.8 & 58.4 & 56.8 & 56.8 & 66.4 & 48.9 & 57.9 & 55.2 \\ M2M100-148M & 282 M & 49.2 & **52.8** & 46.0 & 48.8 & 52.4 & 59.6 & 58.0 & 59.2 & 60.8 & 65.6 & 49.3 & 57.8 & 55.2 \\ M2M100-12.8 & 635 M & 49.6 & 52.4 & 53.2 & 52.8 & **54.4** & 60.0 & 56.4 & 60.0 & 58.0 & 66.0 & 51.7 & 58.2 & 56.3 \\ NLLB-200-1.3B & 766 M & 45.6 & 47.6 & **57.6** & **54.4** & 52.4 & 57.2 & 57.2 & **60.8** & 60.8 & 66.8 & 50.3 & 58.5 & 56.2 \\ NLLB-200-3.3B & 1,733 M & **52.4** & 51.6 & 53.6 & 52.8 & 53.2 & 60.4 & **60.0** & 60.4 & 60.4 & **67.6** & 52.5 & 59.3 & 57.2 \\ mT5-large & 564 M & 40.4 & 47.2 & 53.6 & 47.6 & 51.6 & 59.2 & 55.5 & 57.6 & 56.8 & 66.4 & 47.1 & 56.3 & 53.6 \\ mT5-xl & 1,670 M & 50.4 & **52.8** & 57.2 & **54.4** & 53.6 & **61.2** & 57.6 & **60.8** & 58.4 & 66.8 & **53.5** & 59.0 & **57.3** \\   

Table 4: Merging with different multilingual models on the MGSM dataset. # Param represents the number of parameters of the used external model. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. (2023), we regard Bn, Th, and Sw as low-resource languages, and regard the remaining languages as high-resource languages.

  
**NLL** & **Sw** & **Lr** & **Hl** & **Th** & **Ar** & **Pr** & **Bn** & **Yh** & **Zh** & **Ro** & **Bp** & **Pr** & **Es** & **Es** & **Avg.** \\  MonoResen (Yu et al., 2023, Zhu et al., 2024) & 45.9 & 49.2 & 55.7 & 55.4 & 60.9 & 61.9 & 63.7 & 73.7 & 74.7 & 77.6 & 78.0 & 86.2 & 82.2 & 82.8 & **90.0** & 68.7 \\ MathReson-Lou et al. (2022) & 45.9 & 49.3 & 36.4 & 55.7 & 60.9 & 61.9 & 64.7 & 73.7 & 74.7 & 76.7 & 78.6 & 80.6 & 82.2 & 82.8 & **90.0** & 68.9 \\ MathReson-Lou et al. (2022) & 45.9 & 49.3 & 36.4 & 55.7 & 61.7 & 60.5 & 68.6 & 67.3 & 79.1 & 79.7 & 78.2 & 78.3 & 82

### Ablation Studies

**Mapping Stage.** In Figure 2(a), we ablated the mapping stage to observe its necessity. A significant drop in the performance of low-resource languages can be observed when ablating the mapping stage, which shows that the accessible general bilingual pairs are beneficial to help MindMerger-Soft understand the low-resource language information that exists in the multilingual model representation space. More detailed are reported in Appendix B.1.

**Augmentation Stage.** In Figure 2(b), we ablated the augmentation stage to observe its necessity. It can be observed that after ablating the augmentation stage, even without using any task-related data, MindMerger-Soft can still outperform MonoReason on low-resource languages, which demonstrates the generalization of MindMerger-Soft that benefits from using accessible general bilingual pairs in the mapping stage. Furthermore, when the augmentation stage is added, MindMerger-Soft exhibits a significant improvement, demonstrating its effectiveness in learning the utilization of both external and built-in capabilities. More detailed are reported in Appendix B.2.

**Replacement vs. Augmentation.** MindMerger uses the representation \(\) outputted by multilingual model to augment the LLM's original representation \(\) rather than replace it to better utilize the built-in capabilities of LLMs. To verify the advantages of the augmentation-based strategy, we removed \(\) to make MindMerger-Soft a replacement-based method. As shown in Figure 2(c). although with exactly the same training data and process, the performance of the replacement-based MindMerger-Soft drops significantly, indicating that it is valuable to use the built-in capabilities of LLM to understand the original input. More detailed are reported in Appendix B.3.

  
**MetaMath-Llama-13B** & **Bn** & **Th** & **Sw** & **Ja** & **Zh** & **De** & **Fr** & **Ru** & **Es** & **En** & **Lrl.** & **Hrl.** & **Avg.** \\  MonoReason  & 12.0 & 8.8 & 6.4 & 48.0 & 56.0 & 64.0 & 63.6 & 62.0 & 67.2 & **70.8** & 9.1 & 61.7 & 45.9 \\ MultiReson-Lon  & 44.0 & 49.2 & 40.8 & 58.0 & 61.2 & 64.0 & 64.4 & 64.8 & 67.6 & 68.4 & 44.7 & 64.1 & 58.2 \\ MultiReson-Full  & 44.8 & 51.6 & 50.8 & 58.0 & 61.6 & 64.8 & 59.2 & 60.8 & 67.6 & 66.4 & 49.1 & 62.6 & 58.6 \\ OAlign  & 38.4 & 49.6 & 46.0 & 52.4 & 59.2 & 62.0 & 62.4 & 64.4 & 67.2 & 69.2 & 44.7 & 62.4 & 57.1 \\ LangBridge  & 39.2 & 42.8 & 42.0 & 33.6 & 42.0 & 55.2 & 54.8 & 58.8 & 60.8 & 52.5 & 41.3 & 52.9 & 49.4 \\ Translate  & 34.8 & 54.0 & 44.4 & 44.4 & 58.0 & 53.6 & 54.0 & 58.6 & 64.8 & 65.6 & 62.4 & **70.8** & 44.4 & 55.5 & 52.2 \\  MindMerger-Hard & 48.0 & 38.4 & 53.6 & 51.6 & 52.8 & **66.8** & 61.6 & 60.8 & 68.4 & 67.6 & 46.7 & 61.4 & 57.0 \\ MindMerger-Soft & **55.2** & **59.6** & **56.4** & **60.0** & 64.5 & 62.3 & **63.6** & **68.0** & **69.6** & 68.8 & **57.1** & **65.1** & **62.7** \\  
**MetaMath-Mistral-7B** & **Bn** & **Th** & **Sw** & **Ja** & **Zh** & **De** & **Fr** & **Ru** & **Es** & **En** & **Lrl.** & **Hrl.** & **Avg.** \\  MonoReason  & 38.4 & 34.8 & 16.8 & 50.8 & 57.2 & 70.4 & 70.8 & 67.2 & 71.2 & 78.0 & 30.0 & 66.5 & 55.6 \\ MultiReson-Lon  & 46.8 & 51.2 & 39.6 & 54.4 & 62.4 & **72.0** & 66.0 & 68.4 & 70.0 & 45.9 & 67.0 & 60.7 \\ MultiReson-Full  & 18.4 & 26.4 & 26.8 & 30.8 & 28.8 & 32.4 & 34.8 & 32.0 & 38.0 & 39.6 & 23.9 & 33.8 & 30.8 \\ OAlign  & 45.6 & 51.2 & 55.2 & 49.4 & 57.2 & 59.2 & 59.8 & 62.0 & 63.6 & 65.8 & 50.7 & 59.3 & 56.7 \\ LangBridge  & 50.0 & **60.0** & 47.2 & 58.4 & 65.6 & 68.4 & 68.8 & 68.4 & 65.6 & 65.2 & 54.6 & 68.1 \\ Translate-En  & 54.6 & 58.7 & 47.7 & 57.2 & 63.1 & 50.4 & 56.7 & 64.9 & 58.6 & 69.7 & 53.7 & 60.1 & 58.2 \\  MindMerger-Hard & 52.4 & 48.4 & **57.6** & **62.0** & 60.6 & 66.4 & 66.8 & **69.6** & **71.6** & 76.4 & 52.8 & 67.6 & 63.2 \\ MindMerger-Soft & **57.6** & 59.6 & 53.2 & 57.2 & **68.8** & 69.2 & **69.6** & 68.4 & **71.6** & **79.2** & **56.8** & **69.1** & **65.4** \\   

Table 5: Results on the MGSM dataset based on MetaMath-Llama-13B and MetaMath-Mistral-7B. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. , we regard Bn, Th, and Sw as low-resource languages, and regard the remaining languages as high-resource languages.

Figure 3: Ablation experiments of MindMerger-Soft on the MGSM dataset. Lrl., Hrl., and Avg. represent the average accuracy across low-resource languages, high-resource languages, and all languages, respectively. Referring to Shi et al. , we regard Bn, Th, and Sw as low-resource languages, and regard the remaining languages as high-resource languages.

### Merging with Different LLMs

MindMerger can be flexibly integrated with different LLMs. To verify this, we conducted experiments on a different type of LLM, MetaMath-Mistral-7B (Jiang et al., 2023; Yu et al., 2023), and a larger size, MetaMath-Llama-13B (Touvron et al., 2023; Yu et al., 2023). The experimental results are shown in Table 5, MindMerger-Soft has achieved superior performance across various baselines. The average accuracy of MindMerger-Soft on the MetaMath-Llama-13B and MetaMath-Mistral-7B versions is at least 4.1% and 4.7% higher than all baselines, respectively, demonstrating the potential of extending MindMerger to a wider range of LLMs.

### Representation Space Changes

For each language, we selected the same 100 texts from the Flores-101 (Fan et al., 2021) dataset, used the mean pooling operation to obtain the representation vectors of the LLM embedding and the hidden states output by and the mapping layer, and visualized it based on T-SNE (Van der Maaten and Hinton, 2008). As shown in Figure 4, the representation spaces of non-English on the LLM embedding are independent and away from English, especially in low-resource languages, which leads to understanding challenges for non-English and the inability to use built-in reasoning capabilities. By contrast, as shown in Figure 3(b), the representations of all languages outputted by the mapping layer almost overlap with English, which reduces the difficulty for LLM to understand non-English languages and enables various languages to utilize the built-in reasoning capabilities.

### Supplementary Experiments

We experimented with several supplementary experiments, including the influence of training dataset size used in augmentation stage (Appendix A.1), the selection of mapping layers structure (Appendix A.2), the usage of encoder-decoder model in MindMerger-Soft (Appendix A.3), the quantitative analysis on representation space changes (Appendix A.4), and the translation performance of MindMerger-Soft after mapping stage (Appendix A.5).

## 6 Conclusion

This paper explores a way to more fully utilize the built-in capabilities of LLMs to improve multilingual reasoning effects. We proposed MindMerger to merge the expert multilingual in the multilingual model with the skilled reasoning and not very proficient but useful multilingual capabilities in the LLM. Through more fully utilizing the potential of LLM and more effective fusion of multilingual models, the performance of MindMerger exceeds all baselines on three reasoning datasets and a language understanding dataset. In the future, we will explore the possibility of MindMerger empowering more professional skills besides reasoning, such as code generation.

Figure 4: T-SNE visualization in the spaces of the LLM embeddings and mapping layer outputs.