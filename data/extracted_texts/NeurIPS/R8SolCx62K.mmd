# Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering

Dongxiao He

College of Intelligence and Computing, Tianjin University, Tianjin, China

Lianze Shan

College of Intelligence and Computing, Tianjin University, Tianjin, China

Jitao Zhao

College of Intelligence and Computing, Tianjin University, Tianjin, China

Hongrui Zhang

Department of Computer Science, University of Illinois at Chicago, Chicago, IL, United States

Zhen Wang

School of Cybersecurity, Northwestern Polytechnical University, Xi'an, China

Weixiong Zhang

Department of Computing, Department of Health Technology and Informatics, The Hong Kong Polytechnic University, Kowloon, Hong Kong

###### Abstract

Graph Contrastive Learning (GCL) has emerged as a powerful approach for generating graph representations without the need for manual annotation. Most advanced GCL methods fall into three main frameworks: node discrimination, group discrimination, and bootstrapping schemes, all of which achieve comparable performance. However, the underlying mechanisms and factors that contribute to their effectiveness are not yet fully understood. In this paper, we revisit these frameworks and reveal a common mechanism--representation scattering--that significantly enhances their performance. Our discovery highlights an essential feature of GCL and unifies these seemingly disparate methods under the concept of representation scattering. To leverage this insight, we introduce Scattering Graph Representation Learning (SGRL), a novel framework that incorporates a new representation scattering mechanism designed to enhance representation diversity through a center-away strategy. Additionally, consider the interconnected nature of graphs, we develop a topology-based constraint mechanism that integrates graph structural properties with representation scattering to prevent excessive scattering. We extensively evaluate SGRL across various downstream tasks on benchmark datasets, demonstrating its efficacy and superiority over existing GCL methods. Our findings underscore the significance of representation scattering in GCL and provide a structured framework for harnessing this mechanism to advance graph representation learning. The code of SGRL is at https://github.com/hedongxiao-tju/SGRL.

## 1 Introduction

Graph Neural Networks (GNNs) have shown impressive performance across various fields, including social networks [1; 2], bioinformatics [3; 4], and fraud detection . However, training GNNs typically requires large datasets with manually annotated labels, which can be both costly and labor-intensive . This limitation restricts their broader application. To address this challenge, Graph Contrastive Learning (GCL) has attracted significant attention [7; 8; 9; 10], focusing on creating proxy tasks from the data itself to enable self-supervised training of GNN encoders . Currently, most GCL research [6; 12; 13] is concentrated on enhancing one of the three main frameworks: InfoNCE-based (i.e., node discrimination) , DGI-like (i.e., group discrimination) , and BGRL-like (i.e., bootstrapping schemes) .

The three mainstream graph contrastive learning frameworks differ significantly, particularly in their approach to node-level tasks. InfoNCE-based methods prioritize node-level discrimination [8; 9; 12], treating an anchor node as a reference while considering all other nodes as negative samples to enhance the distinctiveness of each node's representation. In contrast, DGI-like methods adopt a group discrimination paradigm [6; 7; 15], viewing all nodes as positive samples from the same distribution and differentiating them from a noise distribution. BGRL-like methods [10; 13; 16] employ a bootstrapping scheme that eliminates the need for negative samples during training, focusing instead on aligning positive samples. Despite these differences, all three frameworks demonstrate comparable performance, leading us to conjecture **that they may share a common mechanism.** This idea is partially inspired by previous research in visual contrastive learning, which identified uniformity as a key factor in many contrastive learning methods . However, uniformity, which focuses solely on the discriminative properties of node instances, cannot explain the shared underlying factors across these GCL frameworks, as further discussed in Appendix A.

We carried out a detailed analysis of the three GCL frameworks and discovered that representation scattering is a crucial common factor in their success (in Section 3). In Section 3, we provide a formal definition of representation scattering and examine how each framework achieves it. For the DGI framework, we analyze the distributions of original and noise data, proving that after GNN message passing [18; 19], the noise data distribution aligns with the mean distribution of the original data. This indicates that DGI's objective can be interpreted as distinguishing between the local semantics of nodes within the original graph and its mean, which correlates with representation scattering (Section 3.1). In the InfoNCE framework, existing work has demonstrated that the mechanism of negative sampling facilitates a process of achieving uniformity . We extend this further by theoretically proving that the InfoNCE loss serves as an upper bound for representation scattering loss (Section 3.2). Regarding the BGRL framework, we explore the role of Batch Normalization  and its connection to representation scattering, showing that it acts as a specific instance of this concept. Notably, removing Batch Normalization significantly degrades BGRL's performance (Figure 2). Our findings confirm that representation scattering is a key mechanism present across all three mainstream GCL frameworks.

However, the existing GCL frameworks have not fully leveraged the latent mechanisms inherent in their designs and neglected the interconnected nature of graphs when implementing representation scattering. This oversight results in inefficiencies and reduced robustness. Firstly, generating augmented views or computing similarities between node pairs for representation scattering [7; 8; 9] incurs significant computational and memory overhead. Secondly, manually defined negative samples may result in the generation of numerous false negative samples, introducing noise  that can hinder model training. Lastly, those frameworks that rely solely on positive samples  implement representation scattering indirectly through Batch Normalization, which lacks explicit guidance for scattering and can lead to suboptimal performance. Addressing these challenges is crucial for enhancing the effectiveness of GCL methods.

To fully and effectively utilize representation scattering, we propose **S**cattering **G**raph **R**epresentation **L**earning (SGRL). Our approach introduces a Representation Scattering Mechanism (RSM) that embeds node representations into a designated hypersphere, positioning them away from the mean center. This method offers a direct algorithm for representation scattering compared to the existing GCL frameworks, eliminating biases introduced by manually defined negative samples. Additionally, we introduce a Topology-based Constraint Mechanism (TCM) that considers the interconnected nature of graphs. TCM aligns representations derived from structural information with scattered representations, thereby preserving topology information while facilitating scattering. Through these innovations, we aim to enhance the efficiency and robustness of GCL methods. We made the following contributions in this paper:

* We discovered a common representation scattering mechanism in GCLs and showed that the three mainstream GCL frameworks implicitly utilize this mechanism, and importantly can be unified under the concept to representation scattering.
* We showed that the existing methods do not fully exploit the inherent mechanism of representation scattering. We introduced the novel SGRL framework to integrate RSM and TCM, producing an adaptive scattering approach for model training.
* We experimented with various downstream tasks on benchmark datasets and demonstrated the effectiveness and efficiency of SGRL.

## 2 Preliminary

**Graph Data.** We define a graph as \(=(,)\), where \(=\{v_{1},v_{2},,v_{N}\}\) denotes the set of nodes, and \(\) represents the set of edges. The node feature matrix is denoted by \(^{N D}\), where \(N\) is the number of nodes and \(D\) is the feature dimension. In addition, the adjacency matrix is indicated by \(^{N N}\), formulated such that \(_{ij}=1\) when an edge \((v_{i},v_{j})\) exists within the set \(\), or \(_{ij}=0\), otherwise. The degree matrix is denoted as \(=(d_{1},d_{2},,d_{N})\), where each element \(d_{i}=_{j}_{ij}\). The degree-normalized adjacency matrix with self-loops is represented as \(_{}=^{-1/2}(+)^{- 1/2}\).

**Graph Contrastive Learning.** Given a graph's attributes \(\) and adjacency matrix \(\), the objective of GCL is to train an encoder \(f()\) in a self-supervised fashion. The learned encoder \(f()\) can generate representations \(=f(,),^{N K}\), which are both topologies decoupled and dense. These representations can be applied to many downstream tasks.

## 3 Representation Scattering in GCL

We now examine the shared elements that contribute to the effectiveness of popular Graph Contrastive Learning (GCL) frameworks. Upon revisiting three widely used baseline GCL frameworks, we find that they all inherently utilize the mechanism of representation scattering, which plays a crucial role in their success. Here, we formally define Representation Scattering:

**Definition 1**.: _(Representation Scattering) In a \(d\)-dimensional embedding space \(^{d}\) comprising \(n\) vectors organized into a matrix \(^{n d}\), consider a subspace \(^{k}\) (\(1 k d\)) of \(^{d}\) and a scatter center \(\). Representation scattering is a process satisfying two constraints, (i) Center-Away Constraint: Node representations are encouraged to be distant from the scattered center \(\), and (ii) Uniformity Constraint: Node representations are uniformly distributed over the subspace \(^{k}\)._

According to Definition 1, achieving representation scattering requires identifying a scattered center \(\) within the subspace \(^{k}\), and simultaneously satisfying the Center-Away and Uniformity Constraints. We will study its relationship with the popular GCL frameworks in the following section.

### DGI-like methods

DGI-like methods generate negative samples through random permutation of nodes. They employ a mutual information discriminator \(()\), which maximizes the mutual information between nodes and their source graphs to train the model [6; 7; 15]. Here, we show that the objective function of DGI is a special case of representation scattering. To facilitate the proof, we present the following assumption:

**Assumption 1**.: _(a) The normalized propagation matrix \(}\) is defined as \(}=^{-1}}\), where \(}=+\) (b) DGI generates the corrupted graph by randomly shuffling the entities in the feature matrix \(\), while keeping the adjacency matrix \(}\) unchanged. (c) The original data are class-balanced, i.e., for any classes \(k\) and \(j\), \((k)=(j)\)._

The following results do not strictly require Assumption 1 to be satisfied. Assumption 1 represents a common scenario and serves to simplify the proof. A discussion on the validity of Assumption 1, and proofs of the subsequent results without Assumption 1, can be found in Appendix C.

**Theorem 1**.: _At the node level, minimizing the DGI loss is equivalent to maximizing the Jensen-Shannon (JS) divergence between the local semantic distribution in the original graph and its average distribution._

Proof.: Let \(p_{}\) denote the distribution of all nodes in the original graph, characterized by a mean \(\) and variance \(^{2}\). To analyze the local distribution of node \(v_{i}\) after its embedding aggregation by GNN encoders, we define \(p_{i}\) as the distribution of node \(v_{i}\) and its first-order neighbors with mean \(_{i}\) and variance \(_{i}^{2}\). We make use of a conclusion in  and introduce the following lemma:

**Lemma 2**.: _Minimizing the DGI loss, denoted as \(_{}\), equals to maximizing the Jensen-Shannon (JS) divergence between the distribution of the original graph \(\) and the corrupted graph \(}\), i.e., \((_{})(( }))\)._We discuss Lemma 2 in Appendix D.1. Lemma 2 establishes the relationship between the DGI loss and the distributions of the original and corrupted graphs. To investigate the distribution of representations in original and corrupted graphs, we focus on the case of a single-layer GNN  and have the following formulation:

\[=(,)=} ^{},_{i}=_{j_{i}}_{ij} _{j}^{}=_{j_{i}}}_{j}^{},\] (1)

where \(^{}=(),^{D K}\) with \(\) being an activation function like ReLU  for ease of understanding, \(_{i}\) denotes the set of first-order neighbors of node \(v_{i}\), inclusive of \(v_{i}\) itself. In Eq. 1, \( j_{i},_{j} p_{i}(_{i},_{i}^{2})\). For node \(v_{i}\), subsequent to GNN message passing, we compute the mean and variance of the aggregated representation \(_{i}\) as follows:

\[[_{i}]&= [_{j(i)}_{ij}_{j}^{ }]=_{j(i)}_{ij}[_{j}] ^{}=_{j(i)}_{ij}_{i} ^{},\\ (_{i})&=(_{j (i)}_{ij}_{j}^{})= _{j(i)}_{ij}(_{j})^ { 2}=_{i}^{2}_{j(i)}_{ij}^{2}^{ 2}.\] (2)

Given that \(}\) is a normalized propagation matrix with \(_{j(i)}_{ij}=1\) for the aggregated representation \(_{i}\) in the original graph, the mean of representation \(_{i}\)'s distribution equals \(_{i}^{}\). We then analyze the distribution of the aggregated representation \(}_{i}\) in the corrupted graph. Based on Assumption 1, the adjacency matrix \(\) is unaltered and the feature vector \(}_{i}\) is selected randomly from the feature matrix \(\) of the original graph, following the distribution \(p_{}(,^{2})\). According to Eq.2, the mean of the distribution of \(}_{i}\) is \(^{}\), and the variance is \(^{2}_{j(i)}_{ij}^{2}^{ 2}\). Consequently, we have \(_{i} p_{i}^{}(_{i}^{},_{i}^{ 2}^{ 2})\) and \(}_{i} p_{}^{}(^{}, ^{2}^{ 2})\), which directly prove that maximizing the JS divergence between the original and corrupted graph distributions at the node level maximizes, in effect, the JS divergence between the local semantic distribution of node \(v_{i}\) and the mean distribution of the nodes in the original graph. 

**Corollary 3**.: _Taking the mean of the original graph as the center \(\), and the original representation space as a subspace \(^{k}\), the objective of DGI can be described as follows: within the subspace \(^{k}\), DGI increases the distance between the nodes of the original graph and its center \(\), achieving the objective of representation scattering._

Corollary 3 reveals, for the first time, that the primary goal of DGI-like methods is to position node representations away from a central point to encourage a uniform distribution of the nodes. To illustrate these theoretical insights, we performed a visualization experiment. As shown in Figure 1 (a) and (b), both a randomly initialized GNN and a trained single-layer GNN demonstrated that their representations are distanced from the mean of the original graph nodes. Additionally, Figure 1 (b) and (c) provide clearer evidence that minimizing the \(_{}\) is equivalent to maximizing the Jensen-Shannon (JS) divergence between the positive and negative samples, i.e., maximizing the JS divergence between the local semantic distribution in the original graph and its mean distribution. However, the row-wise shuffling mechanism may introduce potential disturbances. In a graph with \(n\) nodes, each node \(v\) retains its local semantic distribution with a probability of \(1/k\) during the shuffling process, where \(k\) denotes the number of distinct node types, assuming class balance. Despite this, the non-discriminatory nature of the perturbation means that these unchanged nodes could still be mistakenly classified as negative samples. Consequently, genuine node representations may be incorrectly labeled as negatives, leading to bias in the learning process. The overlapping nodes depicted in Figure 1 intuitively support this perspective.

Figure 1: t-SNE embedding of DGI on Co.CS dataset. The blue points represent the embeddings of the perturbed negative samples, and the red points denote that of positive nodes. As can be seen in Figures (a) and (b) of model random initialization and the layer of the trained encoder, the DGI-like methods essentially maximize the JS divergence between node embedding and embedding mean.

### InfoNCE-based methods

We now show that the mechanism of negative sampling in the InfoNCE-based methods is equivalent to representation scattering.

**Theorem 4**.: _Let \(}=1/n_{i=1}^{n}_{i}\), and \(()\) be the cosine similarity function (here, \(_{i}\) represents the encoded representation of node \(v_{i}\)). For node \(v_{i}\), the lower bound of the InfoNCE loss \(_{}(_{i})\) exists: \(_{}(_{i})(_{i}, })+(2n)\)._

A detailed proof is given in Appendix D.3. Theorem 4 indicates that when minimizing \(_{}(_{i})\), the similarity between node \(v_{i}\) and mean node \(\), i.e., \((_{i},})\) is also minimized. Taking the mean of all nodes as the scattered center \(\), and the hypersphere as the subspace \(^{k}\), the mechanism of negative sampling is equivalent to representation scattering. However, the InfoNCE loss function is inefficient for representation scattering as it needs to compute and reduce the similarity of each negative pair. Moreover, it indiscriminately treats all negative samples, ignoring the distinctions among them, which leads to inappropriate scattering of negative samples and potential bias from false negatives. Consequently, many recent methods have incurred additional computational overhead by manually and intuitively defining positive and negative samples [12; 23].

### BGRL-like methods

All BGRL-like methods incorporate a key component: Batch Normalization (BN). For a feature vector \(_{i}\) and its corresponding batch statistics, mean \(\) and variance \(^{2}\), the BN is applied as follows: \((_{i})=(_{i}-)/+ }+\), where \(\) and \(\) are learnable parameters that scale and shift the normalized value, and \(\) is a small constant added for numerical stability.

**Theorem 5**.: _The process of data normalization by batch normalization can be seen as a special case of representation scattering._

A detailed proof can be found in Appendix D.2. To empirically assess the impact of batch normalization on BGRL, we conducted experiments comparing both BGRL and BGRL w/o BN across four benchmark datasets. The results, shown in Figure 2, reveal a significant drop in accuracy for BGRL across all datasets without batch normalization, highlighting its critical role in the bootstrapping framework. While BGRL incorporates representation scattering through batch normalization, its training process lacks explicit guidance or a dedicated mechanism to efficiently manage this scattering. The absence of direct supervision during the representation scattering phase can lead to an unoptimized distribution of representation within the embedding space, resulting in suboptimal performance.

In summary, our analysis of the three mainstream GCL frameworks theoretically proved that they all inherently utilize representation scattering but also fail to fully utilize this effective mechanism. This analytic result has motivated us to design a more effective representation scattering method tailored for learning on graphs.

## 4 Methodology

The proceeding sections have revealed the importance of representation scattering in GCLs. Based on the findings, we have designed a novel method, namely Scattering Graph Representation Learning, short-handed as SGRL (Figure 3). We introduce the components of SGRL and provide discussion in the following sub-sections.

### Representation Scattering Mechanism (RSM)

To address the shortcomings in the application of representation scattering within the three mainstream graph contrastive learning frameworks, we design RSM to explicitly guide the target encoder in learning scattered representations. Following Definition 1, we introduce a subspace \(^{k}\) and a scattered

Figure 2: The impact of Batch Normalization in BGRL.

center \(\) to effectively perform representation scattering. For the subspace \(^{k}\), a transformation function \(()\) is introduced to transform representations from the original space \(^{d}\) into \(^{k}\). Specifically, we apply \(_{2}\) normalization to each row vector \(_{i}\) in the matrix \(_{}\):

\[}_{i}=_{^{d}^{k}}( _{i})=_{i}}{(\|_{i}\|_{2}, )},^{k}=\{}_{i}:\| }_{i}\|_{2}=1\},\] (3)

where \(_{i}\) is representation for node \(v_{i}\), generated by the target encoder, \(\|}_{i}\|_{2}=(_{j=1}^{k}}_{ij}^{2})^{ }\), and \(\) is a small value to avoid division by zero. As defined by Eq. 3, the representations of all nodes are distributed on a hypersphere \(^{k}\). This mapping prevents arbitrary scattering of representations in the space, avoiding instability and optimization difficulties during training.

Next, we define the scattered center \(\) and introduce a representation scattering loss function \(_{}\) in \(^{k}\) to push node representations away from the center \(\), as formulated as follows:

\[_{}=-_{i=1}^{n}\|}_{i}-\|_{2}^{2},= _{i=1}^{n}}_{i}.\] (4)

Through Eq. 4, SGRL achieves uniformity of representations globally across the entire dataset, without emphasizing local uniformity. Specifically, RSM enables representations of different semantics to be globally scattered across the hypersphere while accommodating representations of the same semantics to aggregate locally.

**Discussion of RSM.** We now provide a theoretical analysis demonstrating that the proposed representation scattering mechanism outperforms the three graph contrastive learning frameworks. RSM achieves representation scattering more effectively by encouraging distances between node representations and the scattered center, eliminating the dependence on manually designed negative samples. Most traditional methods [7; 8; 9; 15] rely on negative samples to indirectly promote representation scattering, which is inefficient and introduces biases. DGI-like methods, as discussed in Section 3.1, aim to maximize the Jensen-Shannon (JS) divergence between the distribution of the original graph and its mean distribution. Based on this, generating additional noise graph is inefficient. Moreover, some negative samples generated through random shuffling may align with the distribution of positive samples, resulting in false negative samples that hinder model training. InfoNCE-based methods consider all nodes as negative samples, except the matching ones in two augmented views. While pushing nodes away from each other ensures the discriminability of each node, it also results in significant computational overhead. Moreover, due to the potential conflict between the encoder's message-passing mechanism and the InfoNCE loss function, many negative samples can not effectively distance themselves from each other . Therefore, by employing a center-away strategy,

Figure 3: The overview of SGRL. Consider a graph \(\) processed using two distinct encoders (online encoder and target encoder): \(f_{}()\) with parameters \(\) and \(f_{}()\) with \(\), aimed at generating node representations \(_{}\) and \(_{}\), respectively. For \(_{}\), the mean representation of all nodes is calculated to serve as the scattered center \(\). The parameters of \(f_{}()\) are updated via RSM that encourages node representations to diverge from \(\). \(_{}\) is processed through TCM to incorporate topology information, resulting in \(_{}^{}\). Subsequently, \(_{}^{}\) is embedded through a predictor \(q_{}\) to predict \(_{}\), and the parameters in \(f_{}()\) is updated through back-propagation while stopping the gradient of \(f_{}()\). Both channels are trained simultaneously. At the end of each epoch, we employ an Exponential Moving Average (EMA) to update parameters \(\). Finally, the representations generated by \(f_{}()\) are employed across various downstream tasks.

RSM effectively reduces the additional computational overhead and mitigates the biases caused by manually designed negative samples.

### Topology-based Constraint Mechanism(TCM)

After obtaining the scattered representations \(_{}=f_{}(,)\) through the target encoder, it is necessary to consider the differences in the degree of scattering of different node representations.

Consider the interconnected nature of graphs, the representations of two nodes connected topologically should be closer in space \(^{k}\). Specifically, \( v_{i},v_{j} V\), \(_{i},_{j}^{k}\), given a threshold \(d\): \(\|_{i}-_{j}\|_{2}^{2}<d\), if \((i,j)\) and \(\|_{i}-_{j}\|_{2}^{2}>d\), if \((i,j)\). Intuitively, to this end, a simple way is to replace the individually scattered representations with the aggregated representations \(_{}^{}\) from first-order neighbors: \(_{}^{}=_{}\). However, attempting to consider the topology information and achieve representation scattering through the same encoder may lead to conflicts.

To address this issue, we propose a Topology-based Constraint Mechanism (TCM). Specifically, we separate the process of constraint from the process of scattering by letting the online encoder generate topologically aggregated representations instead of the target. The online encoder enhances its representations by summing the original representations \(_{}\) with the topologically aggregated representations of its k-order neighbors \(}^{}_{}\), which can be described as:

\[_{}^{}=}^{} _{}+_{},\] (5)

where k represents the order of neighbors and \(}=+\) is the adjacency matrix with self-loops. By separating scattering and constraints, SGRL can effectively achieve a balance between representation scattering and topology aggregation adaptively, rather than setting the scattering distance empirically. Next, the topology representations \(_{}^{}\) are fed into a predictor \(q_{}()\) to generate the predicted representations \(_{}=q_{}(_{}^{})\). Our objective is to align the predicted topology representations \(_{}\) closely to the scattered representations \(_{}\), enhancing the model's effectiveness in capturing the essential semantic details of the graph. Based on this, the alignment loss \(_{}\) is defined as follows:

\[_{}=-_{i=1}^{N}_{( ,i)}^{}_{(,i)}}{\|_{( ,i)}\|\|_{(,i)}\|},\] (6)

where \(_{}\) and \(_{}\) represent the predicted and scattered representations, respectively. During this process, the online encoder's parameters \(\) are updated and the target encoder's parameters \(\) stop gradient propagation. Compared to directly aligning constrained and scattered representations, this predictive objective can act as a buffer, allowing the online encoder to adaptively learn scattered representations and topology information. Furthermore, to make the target encoder consider topological semantic information into the process of representation scattering, instead of solely focusing on scattering, we employ an Exponential Moving Average at the end of each training epoch:

\[+(1-),\] (7)

where \(\) is a target decay rate and \(\). Eq. 7 effectively mitigates the adversarial interactions between RSM and TCM, while also facilitating the integration of topology information into the representation scattering process. Moreover, RSM enhances the discriminability of representations, while TCM incorporates topology information into the representations. The interaction between these two mechanisms effectively mimics the role of data augmentation, i.e., train encoders to learn the invariance of data to perturbations. Consequently, SGRL obviates the need for explicitly designing data augmentation strategies, which leads to additional computational overhead and heavy reliance on the choice of augmentation techniques.

## 5 Experiment

We evaluated SGRL on the five of the most widely used benchmark datasets, including Amazon-Photo (Photo) and Amazon-Computers (Computers) , WikiCS , Coauthor-CS (Co.CS) and Coauthor-Physics (Co.Physics) . Detailed information of these datasets is in Appendix B.2. We compared SGRL with four types of methods: (1) Three mainstream baselines: GRACE ,DGI , and BGRL . (2)Six recently advanced algorithms: GCA , ProGCL , AFGRL , iGCL , GBT, MVGRL . (3)Two classic graph representation learning methods: Node2vec  and Deepwalk . (4)The semi-supervised training baseline GCN . We utilized the representations generated by the online encoder for downstream tasks. For node classification, we followed the evaluate scheme from . We trained a simple linear model using only the representations from a logistic regression loss with an \(_{2}\) regularization and no backpropagation of any gradient to the graph encoder network. Specifically, we trained the downstream classifier using \(10\%\) of the data and tested the classifier on the remaining \(90\%\). We ran SGRL 20 times and report here the average and standard deviation of the F1-score. For node clustering, we adopted the evaluation method from . The testing was conducted on the learned representations at each epoch, and the best performance is reported below. More details of the experiments can be found in Appendix B.

### Performance Analysis

**Overall evaluation.** Table 1 presents the averages and standard deviations of the F1-scores for all methods on the node classification task. Some statistics for the existing methods are reported from either their original papers or . As shown in the table, the proposed SGRL exhibits superior performance across all five datasets tested, achieving the highest accuracy. Compared to the three baselines, BGRL, GRACE, and DGI, our new method outperforms them by 1.23%, 2.14%, and 3.26%, respectively, showing SGRL's effectiveness in more effectively exploiting representation scattering. Moreover, we observe that SGRL outperforms methods like GCA, AFGRL and iGCL, which focus on improving upon data augmentation.

This demonstrates the validity of employing a graph contrastive framework based on representation scattering and topology aggregation, instead of using data augmentation. SGRL also outperforms the advanced negative sampling method, ProGCL. We attribute this to SGRL's representation scattering, which does not explicitly define negative samples, achieving better performance than manually defined negative samples. When evaluated on node clustering tasks using the scheme in , it can be observed that SGRL achieves the best or second-best accuracy on most datasets. Although SGRL attempts to make node representations scattered, which seems unsuitable for clustering tasks, it still manages to deliver competitive performance. This demonstrates that TCM can preserve the topology information of the original graph in the process of representation scattering, ensuring that while

  
**Method** & **Available Data** & **WikiCS** & **Computers** & **Photo** & **Co.CS** & **Co.Physics** \\  Raw Features & \(X\) & \(71.98 0.00\) & \(73.81 0.00\) & \(78.53 0.00\) & \(90.37 0.00\) & \(93.58 0.00\) \\ Node2vec & \(A\) & \(71.79 0.05\) & \(84.39 0.08\) & \(89.67 0.12\) & \(85.08 0.03\) & \(91.19 0.04\) \\ DeepWalk & \(A\) & \(74.35 0.06\) & \(85.68 0.06\) & \(89.44 0.11\) & \(84.61 0.22\) & \(91.77 0.15\) \\  GRACE & \(X,A\) & \(77.97 0.63\) & \(86.50 0.33\) & \(92.46 0.18\) & \(92.17 0.04\) & OOM \\ DGI & \(X,A\) & \(75.35 0.14\) & \(83.95 0.47\) & \(91.61 0.22\) & \(92.15 0.63\) & \(94.51 0.52\) \\ BGRL & \(X,A\) & \(76.86 0.74\) & \(89.69 0.37\) & \(93.07 0.38\) & \(92.59 0.14\) & \(95.48 0.08\) \\  GBT & \(X,A\) & \(76.65 0.62\) & \(88.14 0.33\) & \(92.63 0.44\) & \(92.95 0.17\) & \(95.07 0.17\) \\ MVGRL & \(X,A\) & \(77.52 0.08\) & \(87.52 0.11\) & \(91.74 0.07\) & \(92.11 0.12\) & \(95.33 0.03\) \\ GCA & \(X,A\) & \(77.94 0.67\) & \(87.32 0.50\) & \(92.39 0.33\) & \(92.84 0.15\) & OOM \\ ProGCL & \(X,A\) & \(78.45 0.04\) & \(89.55 0.16\) & \(93.64 0.13\) & \(93.67 0.12\) & OOM \\ AFGRL & \(X,A\) & \(77.62 0.49\) & \(89.88 0.33\) & \(93.22 0.28\) & \(93.27 0.17\) & \(95.69 0.10\) \\ iGCL & \(X,A\) & \(78.83 0.08\) & \(89.41 0.06\) & \(93.02 0.06\) & \(93.52 0.04\) & \(94.77 0.20\) \\
**SGRL(Ours)** & \(X,A\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  Supervised GCN & \(X,A,Y\) & \(77.19 0.12\) & \(86.51 0.54\) & \(92.42 0.22\) & \(93.03 0.31\) & \(95.65 0.16\) \\   

Table 1: Performance on node classification. OOM signifies out-of-memory on 24GB RTX 3090. \(X,A,Y\) denote the node attributes, adjacency matrix, and labels in the datasets. Optimal results are shown in bold.

performing representation scattering, it maintains the local structure and global semantic consistency of the nodes.

**Visualization.** To more intuitively show the advantages of the representations learned by SGRL, we employ t-SNE  for visualizing the learned representations from the Co-CS dataset. Each point represents a node, with the color indicating the node's label. As shown in Figure 4, SGRL shows clearer inter-class boundaries compared to the other three methods, indicating that RSM achieves superior scattering. Moreover, we observe better intra-class clustering in SGRL. This highlights that SGRL, which does not require manually defined negative samples, prevents the inappropriate distancing of intra-class nodes. It achieves both global scattering and local semantic aggregation, demonstrating the effectiveness of adaptive scattering methods based on topology constraints.

### Model Analysis

**Hyperparamter Analysis.** In this subsection, we investigate the sensitivity of the hyperparameter k in SGRL, as shown in Eq. 5. The parameter k represents the order of neighbors aggregated, which directly influences the strength of the topology constraints. In our experiments, we adjust k in the range \(0,1,2,,7\) to evaluate the impact of different constraint strengths on SGRL. The results are shown in in Figure 5: when \(=0\), i.e., "SGRL w/o TCM", the lack of topology constraints leads to a decrease in model performance. As k increases, the constraint ability of TCM gradually strengthens, and the model performance exhibits a unimodal shape with respect to changes in k. This highlights that weak topology cannot preserve adequate topological information for adaptive representation scattering, whereas strong topology may excessively restrict the scattering of representations, which aligns with the perspectives proposed in Sections 3 and 4.

**Ablation Studies.** To verify the effectiveness of each component of SGRL, we conducted ablation studies on five datasets. As shown in Table 3, "SGRL w/o TCM and TCM" demonstrates significant performance improvements on four datasets with the addition of RSM alone, i.e., "SGRL w/o TCM", confirming its effectiveness. We observe that there is a slight performance decrease on the Computers dataset, which we attribute to over-scattering resulting from the absence of constraints during representation scattering. Additionally, a comparison between SGRL and "SGRL w/o TCM" reveals that SGRL achieves improved accuracy across all five datasets, particularly on the Computers dataset. This indicates that the TCM effectively constrain scattering, achieving an adaptive representation

   Variant & WikiCS & Amazon-Computers & Amazon-Photo & Co-CS & Co-Physics \\  SGRL w/o RSM and TCM & 76.86 \(\) 0.74 & 89.69 \(\) 0.37 & 93.07 \(\) 0.38 & 92.59 \(\) 0.14 & 95.48 \(\) 0.08 \\ SGRL w/o TCM & 78.55 \(\) 0.08 & 89.54 \(\) 0.10 & 93.58 \(\) 0.05 & 94.08 \(\) 0.03 & 96.19 \(\) 0.04 \\ SGRL w/o EMA & 79.36 \(\) 0.08 & 90.03 \(\) 0.07 & 93.92 \(\) 0.02 & 93.89 \(\) 0.07 & 96.16 \(\) 0.07 \\  SGRL (Ours) & **79.40 \(\) 0.13** & **90.23 \(\) 0.03** & **93.95 \(\) 0.03** & **94.15 \(\) 0.04** & **96.23 \(\) 0.01** \\   

Table 3: Ablation study on node classification. Optimal results are shown in bold.

Figure 4: t-SNE embeddings of nodes in CS dataset.

Figure 5: Hyperparameter Analysis on k

scattering, verifying our view in Section 4.2. Finally, we performed an ablation study on EMA. Given the potential adversarial relationship between RSM and TCM, we employed EMA to balance this effect. The experimental results in Table 3 highlight the necessity of incorporating EMA to mitigate the adversarial interaction between RSM and TCM.

## 6 Conclusion

In this paper, we made two significant contributions to Graph Contrastive Learning (GCL), an actively researched subject with numerous applications across diverse domains. First, through a comprehensive analysis of the three popular GCL frameworks, we discovered a common latent mechanism - representation scattering - that underlies these distinct contrastive methods. This discovery highlights an essential feature of GCL and unifies these seemingly disparate frameworks under the concept of representation scattering. Despite their popularity, the existing methods have not fully leveraged this mechanism, leaving the potential of representation scattering largely untaped. However, applying representation scattering directly to GCL poses technical challenges. Our second contribution addressed this issue by introducing the Representation Scattering Mechanism (RSM) and the Topology-based Constraint Mechanism (TCM), which we integrate into a novel GCL approach named Scattering Graph Representation Learning (SGRL). In SGRL, we effectively balance the adversarial relationship between RSM and TCM using an Exponential Moving Average (EMA) strategy. Extensive experimental results on benchmark datasets validate the effectiveness of our proposed method. Future work will further explore the broader implications of representation scattering beyond GCL, discussed in Appendices F and G.