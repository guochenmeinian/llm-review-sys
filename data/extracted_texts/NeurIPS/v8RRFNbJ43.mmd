# Measuring _Deja vu_ Memorization Efficiently

Narine Kokhlikyan

FAIR at Meta

&Bargav Jayaraman

FAIR at Meta

&Florian Bordes

FAIR at Meta

&Chuan Guo

FAIR at Meta

&Kamalika Chaudhuri

FAIR at Meta

###### Abstract

Recent research has shown that representation learning models may accidentally memorize their training data. For example, the _deja vu_ method shows that for certain representation learning models and training images, it is sometimes possible to correctly predict the foreground label given only the representation of the background - better than through dataset-level correlations. However, their measurement method requires training two models - one to estimate dataset-level correlations and the other to estimate memorization. This multiple model setup becomes infeasible for large open-source models. In this work, we propose alternative simple methods to estimate dataset-level correlations, and show that these can be used to approximate an off-the-shelf model's memorization ability without any retraining. This enables, for the first time, the measurement of memorization in pre-trained open-source image representation and vision-language models. Our results show that different ways of measuring memorization yield very similar aggregate results. We also find that open-source models typically have lower aggregate memorization than similar models trained on a subset of the data. The code is available both for vision and vision language models.

## 1 Introduction

Representation learning has emerged as one of the major tasks in computer vision. The goal in representation learning is to learn a model that produces semantically meaningful representations, where images or image-text pairs that are close in meaning occur close together in representation space. These learned representations can then be used in numerous downstream applications such as semantic segmentation , image generation  and multi-modal LLMs . A natural question that arises is whether these representation learning models memorize their training data and to what extent. Excessive memorization may call the generalization abilities of the models into question. Thus, there is a need to develop a way to measure if and to what extent memorization is taking place.

Since learned representations are usually abstract and hard to interpret, memorization measurement for representation learning models requires careful design. Currently, a standard way of doing this is the _deja vu_ method, which designs a causal task of predicting parts of the training sample given another disjoint part, and uses performance on this task to determine if the model memorizes. For example, Meehan et al.  designed the task of predicting the foreground object given the background crop of a training image, as shown in Figure 1 (orange block). Achieving a high performance on this task indicates two possibilities: _(i)_ if the model has memorized the association of the background crop with the foreground object for a specific training sample, or _(ii)_ if the model has learned the dataset-level correlation between the background crop and a given foreground object. To rule out the second possibility, Meehan et al.  opted for a two-model approach, training two separate models on disjoint parts of the training set and using the gap in performance betweenthe two models as indication of memorization. This approach has been extended by Jayaraman et al. (2024) to measure memorization of vision-language models such as CLIP (Radford et al., 2021).

Despite the success of the _dejja vu_ method in defining and measuring memorization, scaling this approach to state-of-the-art representation learning models is challenging. First of all, the two-model approach requires the model trainer to split the training set into disjoint halves, which severely constrains the valuable training data. Moreover, even if data is abundant, training the second model on internet-scale datasets is computationally expensive. Due to these limitations, the _dejja vu_ test cannot be used to measure memorization of pre-trained models out-of-the-box.

In this work, we provide simple alternative ways of quantifying dataset-level correlations and show that they suffice for the purpose of measuring _dejja vu_ memorization. Specifically, for image representation learning, we propose two alternative ways to derive reference models to predict the foreground label from a background crop: training an image classification network directly, and using a Naive Bayes classifier on top of a pre-trained object detection model. We then leverage these reference models to define a _one-model dejja vu_ test--a memorization test for representation learning models that only requires training a simpler reference model _once per dataset_. Figure 1 (green block) gives an illustration of our proposed one-model _deja vu_ test. We also propose a variant of the method for vision-language models by leveraging a pre-trained text embedding model.

We validate our proposed methods by comparing them to the two-model test on ImageNet-trained image representation learning models, as well as CLIP models trained on a privately licensed image-caption pair dataset. We find that the one-model test can successfully identify memorized examples and obtain similar population-level memorization scores as the two-model test. We then apply the one-model _dejja vu_ test on pre-trained open-source models and provide for the first time a principled memorization measurement on these models. Our results reveal that open-source models have significantly lower memorization rates than similar models trained on a smaller subset of data. We conclude that our one-model test can be a practical tool for evaluating memorization rates in representation learning models.

Contributions.To summarize, our main contributions are as follows:

1. We develop simple and efficient methods to quantify dataset-level correlations for both image-only and vision-language representation learning models. Our methods enable _dejja vu_ memorization tests without training two models on disjoints splits of the training set.
2. We validate our proposed methods by comparing them to the two-model test, and analyze the strengths and weaknesses of both tests.
3. We evaluate the one-model _dejja vu_ test on open-source image-only and vision-language representation models. Our test reveals that open-source models do memorize specific training samples, but overall to a lesser degree than the same model trained on smaller subsets of data.

Figure 1: Illustration of our one-model _dejja vu_ test for image representation learning. The task is to predict the foreground object given a background crop. The original _dejja vu_ test (Meehan et al., 2023) trains two models SSL\({}_{A}\) and SSL\({}_{B}\) on disjoint splits of the training set, and uses SSL\({}_{B}\) to quantify the degree of dataset-level correlation between the foreground and background crop. Our one-model test replaces SSL\({}_{B}\) with a classifier that directly predicts the foreground given background crop, and we show that both ResNet50 network and Naive Bayes classifier work well for this purpose.

Related Work

A body of literature has been built around how to detect and measure memorization of large foundation models.

The first line of work is on _extraction attacks_(Carlini et al., 2019, 2021), where the goal is to extract snippets of training data from a model. These attacks typically tend to work well when models are trained on data that is duplicated many times (Kandpal et al., 2022), and are successful on a very small fraction of training data. Consequently, it is challenging to use them to develop a consistent metric that can be used to compare different models in terms of their memorization capacity.

The second line is on _membership inference_(Shokri et al., 2017), which involve a statistic, such as, a loss function or score, where low values suggest membership in the training set. State-of-the-art membership inference attacks Carlini et al. (2022), Watson et al. (2021) also involve training multiple "shadow models" that are used to calibrate the values of the statistics. Membership inference tests have close connections to overfitting Yeom et al. (2018) in that the statistic is chosen to be one that is overfitted during training. The challenges of membership inference tests is that low values of the statistic only _suggest_ membership, and do not necessarily provide concrete sample-level evidence. Additionally, they sometimes do not work well on large models (Duan et al., 2024). Finally, we note that for representation learning methods such as DINO (Caron et al., 2021) that use self-distillation, loss minimization is not usually the training objective - which might lead to failure of membership inference attacks based on loss statistics (Liu et al., 2021; He and Zhang, 2021). In contrast, our measurement method is more concrete, agnostic to the method of training, and does not require training multiple similar models.

A third line of work is on _attribute inference_(Fredrikson et al., 2014), where we are given a model, and some attributes of a training data point, and the goal is to use the model to infer the rest.Jayaraman and Evans (2022) recently show that most attribute inference tests apply equally well to training and test data, and hence may not be very relevant in measuring privacy. In contrast, _deja vu_ memorization specifically looks at sample-level attribute inference in training data points _beyond what could be achieved through dataset level correlations_, which justifies its relevance.

Our work also has connections to prior work on measuring memorization in classification models and influence functions (Koh and Liang, 2017). Feldman (2020) proposes a stability-based definition of memorization, where a classifier memorizes the label of \((x,y)\) if \(f_{S}(x)=y\), where \(f_{S}\) is trained on a training set \(S\), and \(f_{S(x,y)}(x) y\) where \(f_{S(x,y)}\) is trained on \(S\{(x,y)\}\). Unfortunately this can be highly computationally demanding, as measuring memorization for a single example requires training a full model.

Stability-based memorization is also very related influence functions (Koh and Liang, 2017), which approximate the impact of a single training example on a test prediction. Specifically, if a training point has high influence on its own prediction, then it is likely memorized. However, calculating influences, while easier than re-training a model, is also compute-heavy for large models, and involve many approximations. In contrast, our approach has lower computational cost.

## 3 Measuring Dataset-level Correlations

As explained before, the main challenge with prior work is that we need a second model trained on similar data to determine if the task could be done by dataset-level correlations. Our main contribution is to introduce alternative approaches for inferring dataset-level correlations, and empirically demonstrate that these approaches suffice for the purpose of measuring memorization.

### Formal Definition

Formally, we define memorization as follows. We have a training dataset \(D=\{z_{1},,z_{n}\}\) drawn i.i.d from an underlying data distribution \(\); this is used to train a representation learning model \(f\). Suppose that a data point \(z\) drawn from \(\) can be written as: \(z=(v,t)\) where \(v\) and \(t\) represent disjoint but possibly correlated information. For example, \(v\) could be the background of an image, and \(t\) the label of the foreground object in it. Similarly, suppose that we can write the data distribution \(\) as the product of the marginal \((v)\) over \(v\) and the conditional distribution \((t|v)\).

Loosely speaking, _deja vu_ memorization happens when we can use \(f\) to infer \(t\) from \(v\) for points \(z_{i}\) in the training set \(D\) better than what we could do from knowing \((t|v)\). Formally, for a discrete label \(t\), we can rigorously define memorization as follows.

**Definition 1** (_Deja vu_ Memorization).: _Let \(z=(v,t)\) be a training data point. \(z\) is said to be memorized if there exists a predictor \(h\) such that \(h(f,v)=t\) while \(*{argmax}_{t^{}}(t^{}|v) t\)._

Observe that the second part of the definition precludes inference through dataset-level correlations. Figure 1 shows a concrete example. Suppose we have an image \(z=(v,t)\) of a patch of water \(v\) (background) with a black swan \(t\) in the foreground. Suppose also that based on the representation \(f(v)\), we can predict there is a black swan in the image. Is this image memorized by \(f\)? It is possible, but it might also be possible that all patches of water in the training dataset are adjacent to black swans, _i.e._\(*{argmax}_{t^{}}(t^{}|v)=t\). Therefore to determine if the image is being memorized, we need to rule out this possibility.

Related to, but different from us, Feldman (2020) provides a stability-based definition of memorization. We adapt their original definition to our setting as follows.

**Definition 2** (Stability-based Memorization).: _Let \(z_{i}=(v_{i},t_{i})\) be a training data point, and let \(f_{D}\) denote a model trained on the dataset \(D\). \(z_{i}\) is said to be stably memorized if there exists a predictor \(h\) such that \(h(f_{D},v_{i})=t_{i}\) and \(h(f_{D z_{i}},v_{i}) t_{i}\)._

In other words, if we exclude \(z_{i}\) from the training set, then we cannot use the model \(f\) to predict \(t_{i}\) correctly. Observe that this definition is very closely related to the notion of stability in learning theory (Bousquet and Elisseeff, 2002).

These two definitions are related, but subtly different - there can be examples that are _deja vu_ memorized, but not stably memorized and vice-versa. It can be easily shown that the rate of _deja vu_ memorization is upper-bounded by the generalization error of \(h\); on the other hand, the rate of stability-based memorization is by definition the leave-one-out error (Bousquet and Elisseeff, 2002) of \(h\). Classical learning theory (Bousquet and Elisseeff, 2002) predicts that the leave-one-out error of a classifier is close to its generalization error. Therefore, the rates of these two notions are close for well-generalized classifiers that adapt themselves to the data such as neural networks.

Feldman (2020) provides a method to measure stability-based memorization for a sub-sample of the training data that involves training a large number of auxiliary models; in contrast, our notion has the advantage that it can be measured in a much more computationally efficient manner.

### Image Representation Learning Models

For image representation learning, _deja vu_ memorization measures the accuracy of inferring the foreground object given a background crop. Let \(\) be a function that, when given any image \(x\), produces a background crop \((x)\). Then for a sample \(z_{i}=(x_{i},y_{i}) D\) where \(x_{i}\) is an image and \(y_{i}\) is the label of the foreground object, we have \(v_{i}=(x_{i})\) and \(t_{i}=y_{i}\) in the notation of Definition 1. Observe that since we are looking at unsupervised representation learning, the label \(y_{i}\) was not used to train the model \(f\). Define

\[_{f}(v,t)=((h f)(v)=t)\{0,1\},\] (1)

where \(h\) is a predictor that takes the representation of \(f(v)\) and outputs a foreground object label. Observe that \(_{f}(v,t)\) is a \(0/1\) value which is \(1\) when the foreground prediction is correct. Since \(v_{i}\) does not contain the foreground object, for a training sample \(z_{i}\) that is not memorized, one expects \(_{f}(v_{i},t_{i})=0\), except by sheer chance. However, dataset-level correlations may in fact allow accurate prediction of the foreground object from a background crop, _e.g._ if the foreground object is a basketball and the background is a basketball court. To isolate this effect, Meehan et al. (2023) proposed to split the training set \(D\) into disjoint sets \(A\) and \(B\), and train two models \(f_{A}\) and \(f_{B}\) on the two datasets. Then, for \(z_{i} A\), if \(_{f_{A}}(v_{i},t_{i})=1\) but \(_{f_{B}}(v_{i},t_{i})=0\), one can then infer that \(t_{i}\) cannot be predicted from \(v_{i}\) from correlation alone, and thus \(f_{A}\) has likely memorized \(z_{i}\).

In this analysis, \(_{f_{B}}\) determines if the foreground object can be predicted from \((x_{i})\). To enable _deja vu_ memorization measurement with a single model, we propose to replace \(_{f_{B}}\) with the prediction of a reference model that directly classifies the foreground object given the background crop. We propose two ways to do this: training an image classification network end-to-end, and using naive Bayes classifier on top of an object detector.

Image classification network.Our first approach is straightforward: we train an image classifier to predict \(t_{i}\) directly given \(v_{i}=(x_{i})\). For ImageNet, we train a ResNet50 model over a split \(D^{}\) of the training set \(D\) and evaluate _dej\(\)u_ memorization on \(D D^{}\). This ensures the reference model itself is not memorizing, but rather predicting the correlation between the background crop and the foreground object.

Naive Bayes classifier.If the training set \(D^{}\) for the image classifier is large, the above approach can be just as expensive as training the model \(f_{B}\). Our second approach alleviates this by fitting a simpler model, a naive Bayes classifier, on top of objects detected in \((x)\). In detail, let \(\) be an object detection model with vocabulary set \(\); that is, for a given image \(x\), \((x)\{0,1\}^{||}\) is a binary vector such that \((x)_{k}=1\) if and only if object \(o_{k}\) exists in image \(x\) for each \(o_{k}\) in the vocabulary set \(\). We then derive the empirical probability estimates over a split \(D^{}\) of the training set:

\[P(o_{k})=|}_{z_{i} D^{}}(v_{i} )_{k}, P(o_{k} t_{i}=t)= D^{}:t_{i}=t\}|} _{z_{i} D^{}:t_{i}=t\}(v_{i})_{k}.\]

For a sample \(z_{i} D D^{}\), the naive Bayes classifier predicts the probability \(P(t_{i}=t v_{i})\) for each foreground object \(t\) given the background \(v_{i}\) as:

\[P(t_{i}=t v_{i})=P(t_{i}=tv_{i})=P(t) _{k:(v_{i})_{k}>0} t_{i}=t)}{P(o_{k})},\]

where the last equality uses the independence assumption for naive Bayes. In practice, because the object detection result can be noisy, we truncate the list of detected objects to the top-\(K\) according to detection score.

Results.We now investigate how effective the two approaches are at measuring dataset-level correlations in comparison with a second model (Meehan et al., 2023). Specifically, we compare the ResNet classifier and two versions of the Naive Bayes Classifier that uses the top-5 and top-20 crop annotations, as well as three SSL models--VICReg Bardes et al. (2022), DINO Caron et al. (2021) and Barlow Twins Zbontar et al. (2021) --and look at how much these classifiers agree on the predicted correlations.

Figure 2: Left: Population-level correlation accuracy scores across different models. The accuracies for two model tests are based on KNNs computed on top of VICReg, Barlow Twins and DINO representations. ResNet50 and Naive Bayes classifier are used for one model tests. The results show that ResNet50 and NB Top-2 are similar to both VICReg and Barlow Twins, and the aggregate accuracy is low.

Figure 2(a) shows that the overall accuracy across these classifiers are largely comparable. We then zoom into top-5 most correlated classes in Figure 2(b), where we show the number of correctly predicted correlations for the top-5 most correlated classes. Across the three methods that we compared, namely KNN, ResNet and Naive Bayes, the top-5 most correlated classes are identical. However, at a sample level, there is in fact a large divergence in prediction across different methods. Figure 2(a) shows the fraction of samples where the correlation prediction agreed for the different reference models. Here, we see that the agreement is quite low, only about \(40\%\). This suggests that the methods have different inductive biases from the SSL-based classifiers when measuring dataset-level correlations and thus can overestimate memorization when used in the one-model test. Appendix B.0.1 looks deeper into the intersection of common memorized examples across multiple reference models. It shows that ResNet classifier agrees with the intersection of three two model tests for approximately 86% and Naive Bayes for 78% of top-1 correlated examples. Figure 2(b) showcases different scenarios when the reference models agree and disagree. It unveils the strengths and the weaknesses of the one and two model tests and suggests that these methods can be used conjointly.

### Vision Language Models

For vision-language models (VLMs), the training dataset \(D\) consists of image-text pairs \(z=(z_{},z_{})\). The model \(f\) learns to simultaneously embed \(z_{}\) and \(z_{}\) into low-dimensional representations, with the training objective of aligning the representations \(f(z_{})\) and \(f(z_{})\). Following the setup of Jayaraman et al. (2024), we consider \(v=z_{}\) and \(t=(z_{})\{0,1\}^{|V|}\), where \((z_{})\) is the set of detected objects in a vocabulary \(\). _Deja vu_ memorization occurs when one can leverage \(f\) to infer objects in \(z_{}\) using \(z_{}\)_significantly beyond dataset-level correlation_. Specifically, consider a predictor \(h\) that operates on \(f(v)\) and outputs a binary vector of predicted objects. We can define the precision and recall metrics for the predictor:

\[_{f}(v,t)= ,_{f}(v,t)=.\] (2)

One might expect \(_{f}(v,t)=_{f}(v,t)=0\) when \(f\) does not memorize. However, dataset-level correlations may in fact enable the prediction of objects in \(z_{}\) from \(z_{}\), _e.g._ if \(z_{}=}$ contains objects such as apples, oranges, carrots, _etc._ To design one-model _deja vu_ memorization tests, we would like to capture this type

Figure 3: Left: Pairwise sample-level agreement in measuring dataset-level correlations and Right: Examples demonstrating when one model tests (Resnet and Naive Bayes classifiers) succeed and two model tests (KNN) fail and vice versa. One model tests learn the correlations between foreground and background better since it is enforced by the classifier training, however, they are less accurate when the relationships between foreground and background are ambiguous. One model tests, in contrast, are better at disambiguating the foreground and background relationships. They, however, sometimes tend to predict what’s on the background and not what foreground it is associated with.

of dataset-level correlation with a reference model. This is especially hard for VLMs since these models are typically trained on internet-scale datasets consisting of billions of diverse samples under a long-tailed distribution. Training this reference model from scratch on a subset of \(D\) requires a similar effort as training the VLM itself, which defeats the purpose of a one-model test.

Using pre-trained text embedding models as reference models.To tackle this challenge, we leverage a pre-trained text embedding model \(g\) that transforms text into vector representations, with the requirement that \( g(z_{}),g(z^{}_{})\) is high when \(z_{}\) and \(z^{}_{}\) are semantically similar (and vice versa). We can then utilize \(g\) to define a reference model similar to the two-model setup of Jayaraman et al. (2024). Given a public set \(D_{}\) of image-text pairs and a training sample \(z\), the reference model first performs inner product search in the embedding space of \(g\) to find the \(K\) most similar captions in \(D_{}\), \((z^{}_{1})_{},,(z^{}_{K})_{}\). Then, we predict \(o_{k} z_{}\) if and only if \(o_{k}(z^{}_{j})_{}\) for some \(j\{1,,K\}\); see Figure 13 in Appendix C for an example.

Result.We investigate how well the LLM (\(g\)) captures the dataset-level correlations for predicting ground-truth objects (\(f_{B}\)) of Jayaraman et al. (2024), We plot heatmaps for pairwise sample-level agreement similar to the vision model case above. However, since this setting has multiple objects per image, we calculate the Jaccard similarity between the correct object predictions per sample for the two models \(g\) and \(f_{B}\), and report the averaged value across all the training samples.

Figure 4 shows the pairwise sample-level agreement between the two models for predicting various top-\(k\) object labels and for different number of NNs. As shown, even when predicting all objects, the two models agree only on 84% objects on average. This agreement decreases as we limit the number of top-\(k\) object predictions or alternatively limit the number of NNs. We see a similar trend that reference models do not always agree on the predictions. We show some examples of what the two models, VLM \(f_{B}\) and LLM \(g\), predict for a given caption in Figure 14 in the appendix.

## 4 Measuring _Deja vu_ Memorization using One Model Test

In this section, we investigate how effective new methods for measuring dataset-level correlations are when we use them for measuring memorization. Specifically, we look at two main questions: **1.** How close are the results of the single-model deja-vu test to the two-model test? **2.** What is the fraction of memorization in open-source (OSS) pre-trained representation learning models? These questions are addressed in the context of both image representation learning models and vision language models.

### Image Representation Learning

**Dataset.** We conduct all our image representation learning experiments on ImageNet Deng et al. (2009) dataset.

We use 300k (300 per class) examples to train the reference models to learn dataset-level correlations. We measure memorization accuracy on an additional disjoint set of 300k images. For the two model tests, these images are included in the training set of the target models, but not the reference models. Finally, we use another additional distinct 500k images to predict the nearest foreground object given the representation of a background crop through KNN.

**Models.** Two model tests are conducted analogous to Meehan et al. (2023). One model tests rely on a classifier that is trained once to predict dataset-level correlations for SSL models. The dataset used

Figure 4: Pairwise sample-level agreement (using Jaccard similarity for predicting correct objects) between the reference VLM \(f_{B}\) in previous two-model test and the GTE language model \(g\). The heatmap shows that the agreement fraction for one model and two model tests are comparable.

to train this classifier overlaps with the training dataset of open-source models but is disjoint from the subset of the examples for which we measure memorization.

We compare two kinds of classifiers to detect dataset-level correlations. The first is a ResNet50 trained on the background crops to predict the foreground object. We used LARS optimizer and 0.1 weight decay for L2 regularization to avoid overfitting. The second classifier is a Naive Bayes classifier. It uses background crop annotations as features. We automatically annotate background crops using Grounded-SAM (Liu et al., 2023; Ren et al., 2024). Annotations represent textual tags associated with probability scores. We use these probability scores to pick top 1, 2 and 5 features to compute final Naive Bayes probability scores. The reference models are trained on a single machine with 8 Nvidia v100 GPUs, 32GB per GPU using 128 batch size. All other experiments are performed on the same machine.

**Metrics.** Following Meehan et al. (2023), we report the _dejja vu_ score and the _dejja vu_ score at \(p\%\). The _deja vu_ score for a model \(f\) is the difference between two accuracy values: the first is the accuracy of predicting the foreground label \(y\) from the representation \(f(v)\) of the background crop \(v\) based on KNN. The second is the accuracy of predicting \(y\) from a reference model. The _deja vu_ score at \(p\%\) is the difference between the same two accuracies, but now calculated only on the top \(p\%\) of the most confident examples.

#### 4.1.1 How close is the _deja vu_ memorization of one-model and the two-model tests?

Section 3.2 discusses how close one and two model tests are in terms of dataset-level correlation accuracy. In this section we compare _deja vu_ memorization scores for one and two model tests. Figure 6 shows that KNN classifier (two model test) and ResNet classifier (one model test) identify similar amount of _deja vu_ memorization for VICReg (Bardes et al., 2022) and Barlow Twins models. _Dejja vu_ memorization is substantially lower in DINO (Caron et al., 2021). Similar findings are reported in Meehan et al. (2023) as well. In addition, we observe that _dejja vu_ score decreases as we increase the number of features (crop annotations) in Naive Bayes. This is due to the increasing accuracy of dataset-level correlation as we increase the number of crop annotations.

#### 4.1.2 Do pre-trained representation learning models in the wild exhibit _deja vu_ memorization?

In this section we present _deja vu_ memorization for pre-trained OSS representation learning models on population-level using one model tests. Two model tests aren't applicable in this scenario since pre-trained models are trained on the entire ImageNet dataset and the validation dataset is relatively small to be considered for training a second representation learning model.

Hence, Figure 6 compares only one model tests. A comparison of one model tests between Figure 6 and Figure 6 shows that pre-trained models memorize less compared to the same models trained on a smaller subset of the training data. We hypothesis that this is due to the lower generalization error of the pre-trained models as a result of having a larger training set. We provide additional examples of common dataset-level correlations and memorized images in appendix subsection B.1

#### 4.1.3 Sample-level memorization

Figure 7 visualizes the distribution of memorization confidence scores for pre-trained VICReg OSS model with ResNet as correlation detector. The memorization confidence for the \(i\)-th example is computed based on the following formula:

\[MemConf(x_{i})=Entropy(Correlation\:Classifier)-Entropy_{SSL}(KNN)\] (3)

\(Entropy_{SSL}(KNN)\) is computed according to [Meehan et al., 2023]'s Section 4 description and \(Entropy(Correlation\:Classifier)\) is correlation classifier's entropy over the softmax values.

### Vision Language Models

Experiment setup.We train CLIP models using the OpenCLIP Ilharco et al.  framework on the Shutterstock dataset (a private licensed dataset consisting of 239M image-caption pairs). See subsection C.1 for details on dataset preparation and training. We quantify dataset-level memorization using the population precision gap (PPG) and population recall gap (PRG) metrics of Jayaraman et al. . These metrics capture the population-level gap between the fraction of memorized objects and fraction of objects inferred through correlation; see subsection C.1 for details.

Figure 8: Data set level memorization of various VLMs. We use top-10 public set NNs to predict the top-\(k\) objects and report PPG and PRG as done in Jayaraman et al. .

Figure 7: A histogram of sample-based memorization confidence for VICReg OOB model. Given a background patch, VICReg predicts the correct class (green). ResNet (correlation classifier) predicts the incorrect (red) class.

#### 4.2.1 How close is the _deja vu_ memorization of one-model and the two-model tests?

As explained in subsection 3.3, in our one-model test we use a GTE language model \(g\) as a reference model to quantify data set level memorization of a target VLM \(f\). We compare this test to the previous work of Jayaraman et al. (2024), which trains a reference VLM from scratch on a separate hold-out set. Figure 7(a) compares the two tests in terms of the PPG and PRG metrics for predicting top-\(k\) object labels in training images with 10 nearest neighbors from the Shutterstock public set. While the previous two-model test achieves 0.06 PPG and PRG values for predicting top-10 objects, our approach obtains 0.07 PPG and 0.06 PRG values for the same setting. Our test thus slightly overestimates the memorization as in the vision case above. We also compare the dataset-level metrics for the two tests for different settings where we vary both the number of nearest neighbors used in the test and also the number of top-\(k\) objects predicted in Table 2 and Table 3 respectively in Appendix C.

#### 4.2.2 Do pre-trained vision-language models in the wild exhibit _deja vu_ memorization?

We perform our one-model test against an out-of-the-box ResNet-50 CLIP model pre-trained on the YFCC15M data set from OpenCLIP. Figure 7(b) shows the PPG and PRG values for predicting different top-\(k\) objects. These results are comparable to our one-model test results in Figure 7(a) where we evaluate our CLIP model trained on 40M Shutterstock data. More specifically, for predicting top-10 objects with 10 nearest neighbors from public set, our Shutterstock model achieves 0.07 PPG and 0.06 PRG, whereas the OSS YFCC15M pre-trained model achieves 0.07 PPG and PRG values. Additional results can be found in Table 4 and Table 5 in the appendix. We include the most memorized examples for our Shutterstock models in Figure 15 in Appendix C.

#### 4.2.3 Sample-level memorization

Figure 9 shows samples with higher degree of memorization. The samples are sorted from high to low memorization such that the top-L samples have higher precision and recall gaps for recovering objects using target and reference models. We find the gap between the objects recovered from target and reference models for each training record, and estimate the precision and recall gaps. A positive gap indicates that the target model memorizes the training sample and the magnitude of the gap indicates the degree of memorization.

## 5 Discussions and Conclusion

This paper proposes a principled method for measuring memorization in vision and vision-language encoder models that does not rely on training similar shadow models. This enables, for the first time, direct measurement of memorization in open-source representation learning and vision-language models. One consequence of these new measurements is that now we can find out how much different OSS models memorize. In particular, we find that VicReg and Barlow Twins memorize more than DINO. Additionally, all standard OSS models memorize less than their versions trained on subsets of the data.

Finally, our method of measurement involves approximations to theoretical quantities, and as such, has some limitations when these approximations do not hold. One such limitation is that our alternative dataset-level correlation estimation might be a poor approximation to the Bayes optimal, or might itself memorize its own training set, thus skewing the results. However, given that these are much simpler classifiers, their own rate of memorization is expected to be lower. Another limitation is that the additional annotations that we use for our measurements may be lower quality, which might also lead to biased results. A closer analysis of the impact of these factors is an avenue for future work.

Figure 9: Sample-level memorization in VLM trained on 40M Shutterstock images, quantified in terms of precision and recall gap between target VLM and off-the-shelf GTE LM.