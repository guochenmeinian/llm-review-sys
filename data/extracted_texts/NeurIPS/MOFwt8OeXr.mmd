# Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization

Haoran Li

Institute of Automation,

Chinese Academy of Sciences

University of Chinese Academy of Sciences

lihaoran20150@ia.ac.cn

&Zhennan Jiang

Institute of Automation,

Chinese Academy of Sciences

University of Chinese Academy of Sciences

jiangzhennan2024@ia.ac.cn

&Yuhui Chen

Institute of Automation,

Chinese Academy of Sciences

University of Chinese Academy of Sciences

chenyuhui2022@ia.ac.cn

&Dongbin Zhao

Institute of Automation,

Chinese Academy of Sciences

University of Chinese Academy of Sciences

dongbin.zhao@ia.ac.cn

Corresponding author.

###### Abstract

With high-dimensional state spaces, visual reinforcement learning (RL) faces significant challenges in exploitation and exploration, resulting in low sample efficiency and training stability. As a time-efficient diffusion model, although consistency models have been validated in online state-based RL, it is still an open question whether it can be extended to visual RL. In this paper, we investigate the impact of non-stationary distribution and the actor-critic framework on consistency policy in online RL, and find that consistency policy was unstable during the training, especially in visual RL with the high-dimensional state space. To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21 tasks across DeepMind control suite and Meta-world. To the best of our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL. Our project page is hosted at https://jzndd.github.io/CP3ER-Page/.

## 1 Introduction

RL has achieved remarkable results in many fields, such as video games , Go , Chess [3; 4] and robotics [5; 6; 7; 8]. Since it is hard to parameterize the complex policy distribution over high-dimensional state and continuous action spaces, the performance and stability of visual RL are still unsatisfactory. As the most common policy distribution, Gaussian distribution is easy to sample, but its unimodal nature limits the expressiveness to represent complex behaviors . While complex distributions have the rich, expressive power to improve the exploration ability , the difficulty of sampling makes it hard to apply to online RL directly. Parameterizing the complex policy distribution to balance ease of sampling and expressiveness is a bottleneck to improving the efficiency of visual RL.

As an emerging generation model, the diffusion model  stands out in fields such as image generation [12; 13; 14] and video generation [15; 16] with its ability to model complex distributions and ease of sampling characteristics. These properties have also been explored for learning a complexpolicy . For example, diffusion models are used to imitat e the diverse expert policies [18; 19] or trajectories [20; 21; 22; 23] in datasets. In addition, due to their excellent expressive and data generation abilities, diffusion models are often employed to address policy constraints [24; 25; 26] and data scarcity [21; 27; 28] in offline RL. Most of these applications are limited to offline learning due to the demand for pre-collected datasets to train diffusion models.

Applying diffusion models in online RL will face different problems than offline RL. Firstly, unlike pre-collected data in offline RL, the data distribution in online RL is non-stationary , and it is currently unclear whether this change will impact training diffusion models. Secondly, since the optimal policy distribution is unknown, samples from this distribution are inaccessible, resulting in the ill-posed traditional score matching problem . In addition, the time-inefficiency of diffusion models  will become more prominent with a large number of online interactions, leading to unacceptable time costs for online learning. As an efficient diffusion model, the consistency model  directly establishes a mapping from noise to denoised data, which is employed for online RL and achieves time efficiency and better performance [33; 34]. These methods simply replace the Gaussian model in the actor-critic framework with the consistency model and train consistency policy with the Q-loss. Although they achieve competitive performance in state-based RL tasks, this training method is incompatible with traditional score matching for diffusion models. Therefore, the question is whether this training framework is suitable for consistency model-based policy training, especially for visual RL tasks with high-dimensional state spaces.

In this paper, we investigate the impact of non-stationary dataset and the actor-critic framework on consistency policy. By analyzing the dormant ratio  of the policy network, we find that the non-stationary of training data is not the main factor affecting the instability of consistency policy, while the Q-loss in the actor-critic framework leads to a sharp increase in the dormant ratio of the policy network, resulting in the loss of complex expression ability, which is particularly significant in visual RL tasks. To address the above issues, we suggest sample-based entropy regularization to stabilize the policy training and propose the prioritized proximal experience regularization, which uses weighted sampling to construct an appropriate proxy policy for policy regularization and achieves sample-efficiency consistency policy. Overall, our contributions are as follows:

* We investigate the impact of non-stationary distribution and actor-critic framework on consistency policy in online RL, and find that the Q-loss of the actor-critic framework can impair the expressive ability of the consistency model, leading to unstable policy training. This phenomenon is particularly significant in visual RL tasks.
* We suggest sample-based entropy regularization and propose a consistency policy with prioritized proximal experience regularization (CP3ER) which significantly enhances the stability of policy training with the Q-loss under the actor-critic framework.
* Our proposed method performs new SOTA in 21 visual control tasks, including DeepMind control suite and Meta-world tasks. To our knowledge, our proposed CP3ER is the first method to apply diffusion/consistency models to visual RL.

## 2 Related Work

### Diffusion Model in Reinforcement Learning

Due to its high-quality sample generation ability and training stability, diffusion models  have been widely applied in fields such as image generation, video generation, and natural language processing and have also been promoted in RL. Since the diffusion model can represent complex distribution in datasets, it is commonly used in offline RL to model behavior policies [18; 25; 35] or expected policies [36; 37; 34; 38] to meet the requirements of diversity policy constraints and achieve a balance between constraint and exploitation. The diffusion model can also model trajectory distribution [20; 39; 40], achieving specified trajectory generation under different guidance. In addition, diffusion models are also employed to generate data to augment limited training data [27; 28].

Although diffusion models have been widely applied in offline learning, using diffusion models in online learning remains a challenging problem.  proposes the concept of action gradient, which uses a value function to estimate the gradient of actions and updates the actions in the replay buffer. The diffusion model-based policy is trained based on the updated actions.  employs a diffusion model as the world model to generate complete rollouts at once instead of auto-regressive generation.

 introduces the Q-score matching (QSM), which iteratively matches the parameterized score of a policy with the action gradient of its Q-function. Considering the low inference efficiency and long training time of diffusion models in RL training,  and  use consistency models instead of diffusion models and implement policy training under the actor-critic framework, achieving excellent performance in continuous control tasks.

### Visual Reinforcement Learning

Compared to state-based RL, visual RL is faced with high-dimensional state space and continuous action space and is sensitive to training parameters and random seeds, which leads to unstable training and sample inefficiency. Image data augmentation [43; 44; 45] is a common technique to alleviate the above problems. In addition, auxilliary losses are usually combined to improve the efficiency of state representation learning from the image, such as contrastive learning loss , state representation loss [47; 48], action and state representation loss , and self-supervised loss . Recent works have focused on enhancing the stability of visual RL from a micro perspective of neural networks. For example,  proposes the visual dead trial phenomenon and introduces an adaptive regularization method for convolutional features.  proposes the concept of dormant neuron phenomenon to explain the behavior of the policy network during RL training.  controls the dormant ratio of the policy network during training so that it achieves the SOTA performance on multiple tasks.

## 3 Preliminary

### Reinforcement Learning

Online RL solves sequential decision problems, typically modeled through Markov Decision Processes (MDP). MDP is represented by 6 tuples \((,,,,_{0},)\). Here, \(\) is the state space, \(\) is the action space, \(\) and \(\) represent the reward function and state transition function of the environment, respectively. \(_{0}\) is the initial distribution of the state, and \(\) is the discount factor. In visual RL, it is difficult for agents to directly obtain the state \(s_{t}\) from the image \(o_{t}\), where \(\) is observation space. Therefore, an image encoder \(f()\) is usually required, and the state is estimated from the image through this encoder. The goal of the agent is to learn an optimal policy \(^{*}\) and the corresponding encoder \(f^{*}\) to maximize the expected cumulative reward \(_{(f())}[_{t=0}^{}^{t}r_{t}]\) under that policy.

### Consistency Policy

The consistency model  is a new diffusion model proposed to address the time inefficiency caused by hundreds of reverse diffusion steps in diffusion models. It replaces the iterative denoising process with learned score functions in traditional diffusion models by constructing a mapping between noise and denoised samples, and directly maps any point on the probability flow ordinary differential equation (ODE) trajectory to the original data in the reverse diffusion process. Thus, it only requires a small number of steps or even one step to achieve the generation from noise to denoised data. Consistency policy [33; 34] is a new policy representation under the actor-critic framework, which replaces traditional Gaussian models with the consistency model and updates the policy by maximizing the state-action value. Consistency policy is defined as

\[_{}(a_{t}|s_{t}) c_{skip}(_{k})a_{t}^{_{k}}+c_{out }(_{k})F_{}(a_{t}^{_{k}},_{k}|s_{t})\] (1)

where \(\{_{k}|k[N]\}\) a sub-sequence of time points on the time period \([,K]\) with \(_{1}=\) and \(_{N}=K\). \(a_{t}^{_{k}}\) is the noised action and \(a_{t}^{_{k}}=a_{t}+_{k}z\) where \(z(0,I)\) is Gaussian noise. \(F_{}\) is a trainable network that takes the state \(s_{t}\) as a condition and outputs an action of the same dimension as the input \(a_{t}^{k}\). \(c_{skip}()\) and \(c_{out}()\) are differentiable functions such that \(c_{skip}()=1\) and \(c_{out}()=0\) to ensure consistency policy is differentiable at \(_{k}=\). \(\) is a real number close to 0. To train this policy,  directly applies the above policy to the actor-critic framework and updates the policy using the following the Q-loss, which is named Consistency-AC.

\[_{a}()=-_{s_{t},a_{t}_{ }}[Q_{}(s_{t},a_{t})]\] (2)

where \(\) is the replay buffer and \(Q_{}\) is the state-action value function. Compared to diffusion-based policies, consistency policy have significant advantages in inference speed and performance in online learning tasks .

### Dormant Ratio of Neural Networks

The expressive ability is crucial for training the policy with RL.  proposes the concept of dormant ratio \(_{r}\), which quantifies the expression ability of a neural network by calculating the proportion of dormant neurons in the neural networks.

\[_{r}=H_{}^{l}}{_{l}N^{l}}\] (3)

where \(N^{l}\) represents the number of neurons in the \(l\)-th layer. \(H_{}^{l}\) is the number of neurons in the \(l\)-th layer whose score \(s_{i}^{l}\) is less than \(\). The score of each neuron is calculated as follows:

\[s_{i}^{l}=_{x}|h_{i}^{l}(x)|}{} _{k l}_{x}|h_{k}^{l}(x)|}\] (4)

Here \(h_{i}^{l}()\) is the activation function of the \(i\)-th neuron in the \(l\)-th layer. \(\) is the distribution of the input \(x\). In the following sections of this paper, we use the dormant ratio to evaluate the expression ability of consistency policy during the training.

As introduced in , the dormant ratio of a neural network indicates the proportion of inactive neurons and is typically used to measure the activity of the network. A higher dormant ratio implies fewer active neurons in the network, implying the network's capacity and expressiveness are damaged. In RL, the episode return is closely related to the dormant ratio of the policy network. A higher dormant ratio results in more lazy action outputs, inactive agent behavior, and lower episode returns; conversely, when policy performance is good, the policy network is usually more active, and the dormant ratio is typically lower. This phenomenon has been reported in .

## 4 Is Consistency-AC Applicable to Visual RL?

**Does the non-stationary distribution in online RL affect the training of consistency models?** Unlike offline RL, online RL does not have pre-collected datasets. The data distribution for training the policy is constantly changing with policy improvement. So, whether this non-stationarity distribution affects the training of consistency models is a question that needs to be explored. In order to investigate the impact of non-stationarity of data for consistency model training, we record the dormant ratio of the policy network during consistency model training under two different settings: online training and offline training. We employ two tasks (MuJoCo Halfcheetah and MuJoCo Walker2d) and conduct 4 random seeds for each setting. In order to eliminate the impact of Q-loss, we follow the behavior clone setting and train the consistency model with consistency loss  using data from offline datasets or online replay buffers. The distribution of the data in the replay buffer varies with policy improvement. The results are shown in Figure 1. Although there is a difference in the dormant ratios between online and offline learning settings in the Halfcheetah task, the overall trend is the same. We speculate that this difference is caused by the diversity of the samples included in the dataset. For the Walker2d task, the dormant ratios are nearly the same under two different settings. Therefore, we can infer that the non-stationary distribution of online RL does not significantly affect the consistency model training.

**Is the actor-critic framework suitable for training consistency policy?** The actor-critic framework is a highly effective policy training framework for online RL, in which the policy network achieves policy improvement by maximizing the value function. Some works  directly apply consistency models to this framework. Although they achieve good results in RL tasks with low dimensional state spaces, whether the actor-critic training framework is compatible with consistency model training remains a question that needs further investigation. To evaluate the impact of the actor-critic framework on the training of consistency models, we compare the dormant ratios of policy

Figure 1: The dormant ratios of the policy under the online and offline training.

networks under the consistency loss and Q-loss settings under the actor-critic framework. The results are shown in (a) and (b) of Figure 2, the solid line shows the dormant ratio of the policy while the dashed line shows the performance of the policy. When training the policy with the consistency loss, the dormant ratios of the network show a trend of first decreasing and then increasing. This means that the policy learns the distribution from the data and then overfits the distribution. When training the policy with the Q-loss, the dormant ratio of the policy network will rapidly increase and maintain a high value, which means that the policy network will quickly fall into local optima, making the policy no longer change. In addition, we can also see that when using the Q-loss, the variance of the dormant ratios is relatively large under different random seeds. When the dormant ratio is low, the policy network can iterate properly to learn good policy. Therefore, we can determine that the Q-loss under the actor-critic framework will destabilize the consistency policy training.

**Will high-dimensional state space exacerbate the degradation phenomenon of consistency policy?** Compared to RL with low dimensional state space, training stability in visual RL is still a challenge. In order to investigate whether the degradation phenomenon of consistency policy will become more significant under visual RL tasks, we compare the dormant ratios of the policy networks with the state as input and image as input on 2 tasks (Walker-walk and Cheetah-run in DeepMind control suite) under the setting of online learning. During the training process, only the Q-loss was used. To maintain consistency in the settings, we only count the dormant ratio of the multilayer perceptron (MLP) of the policy network in the image-based settings. The results are shown in (c) and (d) of Figure 2. Similar to using the state as input, in visual RL with the image as input, most of the neurons in the MLP of the policy network go dormant. Unlike the high variance of the former, the dormant ratios of consistency policy network in visual RL maintain a low variance and a high value. This indicates that there have been almost no successful trials under different random seeds. Therefore, we can infer that visual RL will exacerbate the instability of consistency policy training caused by the Q-loss under the actor-critic framework.

## 5 Consistency Policy with Prioritized Proximal Experience Regularization

**Consistency Policy with Entropy Regularization.** To solve the problem of consistency policy quickly falling into local optima under the influence of the Q-loss, we introduce policy regularization to stabilize policy improvement. Here, we employ entropy regularization to constrain policy behavior. The objective of RL is:

\[J()=_{s_{t},a_{t}_{}}[_{t=0}^ {}^{t}r_{t}(s_{t},a_{t})]-_{s_{t},a_{ t}_{}}[_{}(a_{t}|s_{t})]\] (5)

where \(_{}\) is the proxy distribution required for policy regularization. Entropy regularization is a commonly method for stabilizing policy training in RL. When \(_{}\) is a uniform distribution, the above objective is equal to maximum entropy RL, which maximizes the entropy of the policy while optimizing the return. The prerequisite for this method is to obtain the closed form of the policy distribution to calculate its entropy. However, for diffusion models or consistency models, obtaining the closed form of the policy distribution is very difficult. Thanks to the development of generative models, we can use score matching instead of solving analytic entropy in entropy regularization RL, thus achieving sample-based policy regularization. Therefore, the training loss of consistency policy with the entropy regularization is:

\[_{a}^{r}()=-_{s_{t},a_{t}_{ }}[Q_{}(s_{t},a_{t})]+_{c}()\] (6)

Figure 2: The dormant ratios of the policy networks with different losses and observations.

where \(_{c}\) is consistency loss defined by following:

\[_{c}()=_{k(1,N-1),s_{t}, a_{t}_{},z(0,I)}[(_{k})d(_{}(s_{t},a _{t}^{_{k+1}},_{k+1}),_{}(s_{t},a_{t}^{_{k}},_{ k}))\] (7)

Here \(()\) is a step-dependent weight function, \(d(,)\) is a distance metric. Since there is no need to obtain the closed form of the proxy distribution, only the data under that distribution needs to be obtained, making the selection of proxy distribution flexible. The remaining question is how to construct a suitable proxy distribution \(_{}\).

**Prioritized Proximal Experience Regularization.** When the proxy distribution is uniform, this method approximates the maximum entropy consistency policy (MaxEnt CP). It should be noted that the difference between the proxy distribution and the optimal policy distribution can lead to difficulty in optimizing the above objectives. When the proxy distribution is far from the optimal policy or the proxy distribution is complex, the above optimization objectives require more samples to converge to better results. To better balance training stability and sample efficiency, we propose the prioritized proximal experience regularization (PPER). Specifically, when sampling data from the replay buffer, we design sampling weight \(\) for each data instead of sampling the data uniformly.

\[=|}{ t})}}\] (8)

where \(\) is the hyperparameter and \( t\) is the interval between the sample generation step and the current step. \(||\) is the capacity of the replay buffer. The curves with different \(\) are shown in Figure 3 (b). In the above settings, data closer to the current step will be sampled with a higher probability, while data farther away will have a lower probability. We refer to the above sampling method as a prioritized proximal experience (PPE).

For policy evaluation, we suggest a distributional value function instead of a deterministic value function to ensure the stability and accuracy of the value estimation. Precisely, we follow  and use a mixture of Gaussian (MoG) to model the distribution of the state-action value. When MoG is employed for value distribution, the following loss is used to update Q-function.

\[_{q}()=-_{(s_{t},s_{t+1}),a_{t} _{}}[_{i=1}^{M} Z_{}^{(s_{t},a_{t})}(r_{t}(s_{ t},a_{t})+ z_{i}^{})],z_{i}^{} Z_{}^{(s_{t+1},a_{t+1})}\\ a_{t+1}_{}(s_{t+1})\] (9)

where \(Z_{}^{(s_{t},a_{t})}\) is the estimated value distribution. According to the equation (9), we need to sample \(M\) target Q-values \(z_{i}^{}\) and update the value distribution. Different from , we sample only one next action \(a_{t+1}\) instead of multiple actions to reduce the time cost and find that this simplification can achieve good experiment results. Considering the simplicity and efficiency of DrQ-v2 , our proposed consistency policy with prioritized proximal experience regularization (CP3ER) is built based on DrQ-v2. The framework is shown as Figure 3 (a). We sample the data from the replay buffer, and augment the image with the techniques in DrQ-v2. Thanks to the natural randomness of the action from consistency policy, our method no longer requires additional exploration strategies.

Figure 3: (a) The framework of CP3ER, where PPE is the abbreviation of prioritized proximal experience. (b) The sampling weights \(\) with different \(\).

In addition, we only used a single Q-network to estimate the mean and variance of the mixture of Gaussian instead of double Q-network. We consider prioritized proximal experience regularization when updating consistency policy and used equation (9) when training the Q-network, which differs from DrQ-v2. The complete algorithm is included in the appendix B.1.

## 6 Experiments

In this section, we evaluate the proposed method from the following aspects: 1) Does CP3ER have performance advantages compared to current SOTA methods? 2) Can policy regularization improve the behavior of the policy? 3) What is the impact of different modules on the performance?

### Visual Continuous Control Tasks

**Environment Setup.** We evaluate the methods on 21 visual control tasks from DeepMind control suite  and Meta-world . We split these tasks into three domains, including 8 medium-level tasks in the DeepMind control suite, 7 hard-level tasks in the DeepMind control suite, and 6 tasks in the Meta-world. The details of each domain are included in the appendix C.

**Baselines.** We compare current advanced model-free visual RL methods, including DrQ-v2 , ALIX  and TACO . The more detailed results are shown in the appendix C.

#### 6.1.1 Does CP3ER have performance advantages compared to current SOTA methods?

**Medium-level tasks in DeepMind control suite.** We evaluate CP3ER on 8 medium-level tasks  in DeepMind control suite. The results are shown in Figure 4. From the left part of Figure 4, it can be seen that compared to the current SOTA methods, our proposed CP3ER has achieved better sample efficiency. It should be noted that TACO uses auxiliary losses of action and state representation during training to improve sample efficiency. Moreover, our proposed CP3ER uses no additional losses or exploration strategies. On the right part of Figure 4, we compare the mean, Interquartile Mean (IQM), median, and optimal gap of these methods. CP3ER has significant advantages in all metrics and has more minor variance. This means that CP3ER has better training stability.

**Hard-level tasks in DeepMind control suite.** We also evaluate CP3ER on 7 challenging tasks [53; 57] in the DeepMind control suite. It should be noted that all tasks here only train 5M frames, rather than the commonly used 30M frames in other work[53; 57]. This means it is a very hard challenge. From the results on the left part of Figure 5, it can be seen that most methods have yet to learn effective policy within 5M frames. Our proposed CP3ER surpasses the performance of all methods without relying on any additional loss or exploration strategies. Moreover, it has significant advantages on all metrics including mean, IQM, median, and optimal gap.

**Meta-world.** We also evaluated the methods on 6 complex tasks in the Meta-world. The results are shown in Figure 6. We record the success rates of the tasks, and all results are based on the success rates. Compared to other methods, CP3ER can quickly learn effective manipulation policy, and the success rate can reach nearly 100% in all tasks within 2M steps. By comparing the mean, IQM, median, and optimal gap of the success rates, CP3ER has a significant performance advantage with minimal variance.

Figure 4: Results on medium-level tasks in DeepMind control suite with 5 random seeds.

### Ablation Study

#### 6.2.1 Can policy regularization improve the behavior of the policy during training?

**Action distribution analysis with toy example.** In order to further explore the impact of policy regularization on the training, we borrow the 1D continuous bandit problem  to analyze the policy behavior. The green curve in Figure 7 (a) shows the reward function. Within a narrow range of actions, the agent receives higher rewards, while within a broader range, the agent can only receive suboptimal rewards. Therefore, the policy needs strong exploration ability to achieve the highest return. We compare Gaussian policy with entropy regularization (MaxEnt GP), Consistency-AC and consistency policy with entropy regularization (MaxEnt CP), and record the action distribution during the training. As shown in Figure 7, Consistency-AC quickly converges to the local optimal value with the Q-loss. Policy regularization ensures the diversity of action distribution during consistency policy training, preventing the policy from falling into local optima too early. Moreover, consistency policy has robust exploration compared to the Gaussian policy and achieves higher returns.

Figure 5: Results on hard-level tasks in DeepMind control suite with 5 random seeds.

Figure 6: Results on Meta-world tasks with 5 random seeds.

Figure 7: Results on the toy example. Left part is action distributions during training, while right is returns of different policies.

**Dormant ratio analysis.** We have shown that the Q-loss can rapidly increase the dormant ratio of the consistency policy network, leading to a loss of policy diversity. In order to analyze whether entropy regularization can alleviate the phenomenon, we record the dormant ratios of the policy networks during the training in 3 tasks. The results are shown in Figure 8. Compared to the rapid increase in the dormant rate in Consistency-AC, CP3ER has a lower dormant ratio, which means that entropy regularization can effectively reduce the dormant ratios of consistency policy.

#### 6.2.2 What is the impact of different modules on the performance?

We conduct ablation studies in 2 tasks to evaluate the contribution of each module in the proposed method. In addition, to analyze the impact of proxy distribution for policy regularization on performance, we also compare several candidates, including uniform distribution or behavioral policy in the replay buffer. The results are shown in Figure 9. It is noticeable that policy regularization is crucial for consistency policy. Without policy regularization, consistency policy (CP3ER w/o PPER) makes it difficult to learn meaningful behavior in the tasks. The proxy distribution also has an impact on the performance. Using uniform distribution to regularize policies can make the policy (CP3ER w. MaxEnt) improvement difficult, resulting in low sample efficiency. Compared to using behavior distribution in the replay buffer (CP3ER w. URB), the policy (CP3ER) obtained through prioritized proximal experience sampling has a closer distribution to the current policy, making policy optimization easier and resulting in higher sample efficiency. In addition, we find that the performance of CP3ER is significantly better than the baseline (DrQ-v2), indicating that the feasible usage of consistency policy can help solve visual RL tasks.

## 7 Conclusion

In this paper, we analyze the problems faced by extending consistency policy to visual RL under the actor-critic framework and discover the phenomenon of the collapse of consistency policy during training under the actor-critic framework by analyzing the dormant ratio of the neural networks. To address this issue, we propose a consistency policy with prioritized proximal experience regularization (CP3ER) that effectively alleviates the training collapse problem of consistency policy. The method is evaluated on 21 visual control tasks and shows significantly better sample efficiency and performance

Figure 8: Dormant ratios of the policy networks on different tasks with 5 random seeds.

Figure 9: Results of ablation study on 2 visual control tasks with 4 random seeds.

than the current SOTA methods. It is worth mentioning that, to the best of our knowledge, our proposed CP3ER is the first method to apply diffusion/consistency models to visual RL tasks.

Our experimental results show that the consistency policy benefits from its expressive ability and ease of sampling, effectively balancing exploration and exploitation in RL with high-dimensional state space and continuous action space. It achieves significant performance advantages without any auxiliary loss and additional exploration strategies. We believe that consistency policy will play an essential role in visual RL. There are still some issues worth exploring in future work. Firstly, auxiliary losses for representation in current visual RL have the potential to improve the performance of consistency policy. Secondly, the diversity of behavior in consistency policy is crucial for RL exploration. This paper only discusses the stability of policy training and does not analyze the diversity of behavior during training, which will help improve the exploration performance of policies. In addition, consistency policy under the on-policy framework is also a direction worth exploring.

## 8 Acknowledgments

This work is supported by the National Natural Science Foundation of China (NSFC) under Grants No. 62103409, No. 62136008, No. 62293545 and in part by the International Partnership Program of the Chinese Academy of Sciences under Grant 104GJHEZ2022013GC.