# Learning Successor Features the Simple Way

Raymond Chua

Correspondence to: ray.chua@gmail.com

Arna Ghosh

Dept of Neurology & Neurosurgery, and Montreal Neurological Institute of McGill University.

Christos Kaplanis

Co-senior Authorship. CIFAR Learning in Machines and Brains.

Blake A. Richards

School of Computer Science, McGill University & Mila

Doina Precup

Google Deepmind

Code: https://github.com/raymondchua/simple_successor_features

###### Abstract

In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporal-difference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Minworld) mazes and Mujoco, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required1.

## 1 Introduction

Deep reinforcement learning (RL)  is important to modern artificial intelligence (AI), but standard approaches to deep RL can struggle when deployed for continual learning . When either the reward function or the transition dynamics of the environment changes, standard deep RL techniques, such as deep Q-learning, will either struggle to adapt to the changes or they will exhibit catastrophic forgetting . Given that the real-world is often non-stationary, better techniques for deep RL in continual learning are a major goal in AI research .

One potential solution that researchers have explored is the use of Successor Features (SFs). Successor Features, the function approximation variant of Successor Representations (SRs) , decompose the value function into a separate reward function and transition dynamics representation . In doing so, they make it easier to adapt to changes in the environment, because the network can learn either the reward function or the transition dynamics separately . Furthermore, there are theoretical guarantees that SFs can improve generalization in multi-tasksettings (Barreto et al., 2017). SFs are therefore a promising candidate for deep RL in non-stationary settings.

However, learning SFs is non-trivial. The most straightforward solution, which is to use a temporal-difference (TD) error on subsequent observations (Barreto et al., 2018), can lead to representational collapse, where the artificial neural network maps all inputs to the same point in a high-dimensional representation space. This phenomenon is commonly observed in various deep learning pipelines that end up learning similar or identical latent representations for very different inputs (Aghajanyan et al., 2020). In RL, representation collapse can lead to different states or state-action pairs being mapped to similar representations, leading to suboptimal policy decisions or inaccurate estimation of values.

To solve this problem, a variety of solutions have been proposed. One solution is to use an additional reconstruction loss (Kulkarni et al., 2016; Zhang et al., 2017; Machado et al., 2020) to force the network to maintain information about the inputs in its representations. Another solution is to use extensive pretraining coupled with additional loss terms to encourage high-entropy representations (Hansen et al., 2019; Liu and Abbeel, 2021). More recently, an alternative solution using loss terms to promote orthogonal representations has been put forward (Mahadevan and Maggioni, 2007; Machado et al., 2017). Finally, an unconventional approach integrates Q-learning and reward prediction losses with the SF-TD loss, enhancing the learning process by providing additional supergravity signals that improve the robustness and effectiveness of the successor features (Janz et al., 2019). This method allows the network to simultaneously learn the basis features, successor features, and task encoding vector, with the hope that the learned variables will satisfy their respective constraints.

Though these solutions prevent representational collapse, they can impair learning, introduce additional training phases, or add expensive covariance calculations to the loss function (Touati et al., 2022). Ideally, there would be a way to learn deep SFs directly during task engagement with a simple, easy to calculate loss function.

Here, we introduce a simple technique for learning SFs directly during task engagement. We designed a neural network architecture specifically to achieve this training objective. Our approach leverages the mathematical definition of SFs and constructs a loss function with two terms: one that learns the value function with a TD-error, and another that enforces representations that make the rewards linearly predictable. By mathematical definition, this loss is minimized when the system has learned a set of SFs. We show that training with this loss during task engagement, facilitated by our neural network architecture, leads to the learning of deep SFs as well as, or better than, other approaches. It does so with no pretraining required and very minimal computational overhead. As well, we show that our technique improves continual reinforcement learning in dynamic environments, in both 2D grid worlds and 3D mazes. Altogether, our simple deep SF learning approach is an effective way to achieve the benefits of deep SFs without any of the drawbacks.

## 2 Related work

Our work builds on an extensive literature on decomposing the value function dating back to the 1990s (Dayan, 1993). More recent work on learning deep SFs falls broadly into three categories

Figure 1: (**a**) Results from a single task within a 2D two-room environment, illustrating the suboptimal performance of the canonical Successor Features (SF) learning rule (Eq. 4) due to representation collapse. (**b**) In the canonical SF approach, the average cosine similarity between pairs of SFs converges towards a value of 1, demonstrating representation collapse occurs. (**c**) The canonical SF learning rule does not develop distinct clusters in its representations, as evidenced by lower silhouette scores and higher Davies-Bouldin scores, which again indicates representation collapse. A mathematical proof can be found in section 3.4.

of solutions. The first are solutions that use a reconstruction term in the loss function in order to avoid representational collapse (Kulkarni et al., 2016; Zhang et al., 2017; Machado et al., 2020). This general approach is effective at avoiding collapse, but it can lead to impaired performance on the actual RL task, as we show below. The next set of solutions rely on hand-crafted features (Lehnert et al., 2017; Barreto et al., 2018; Borsa et al., 2018; Madarsaz and Behrens, 2019; Machado et al., 2021; Emukpere et al., 2021; Nemecek and Parr, 2021; Brantley et al., 2021; McLeod et al., 2022; Alegre et al., 2022; Reinke and Alameda-Pineda, 2021) or hand-crafted task knowledge (Hansen et al., 2019; Filos et al., 2021; Liu and Abbeel, 2021; Carvalho et al., 2023a). In these cases, the networks can learn and generalize well, but hand-crafted solutions cannot scale-up to real-world applications. Another category of solutions uses pretraining of the features in the deep neural network before any engagement with the actual RL task (Fujimoto et al., 2021; Abdolshah et al., 2021; Touati et al., 2022; Carvalho et al., 2023b). Such solutions are not as applicable for continual RL because they introduce the need to engage in new pretraining when the environment changes, which assumes some form of oracle knowledge of the environment. Finally, there are solutions that rely on additional loss terms to encourage orthogonal representations, since SRs are built off of purely orthogonal tabular inputs (Touati et al., 2022; Farebrother et al., 2023). These techniques can improve SF learning, but they require computationally expensive calculations of orthogonality in the basis features.

Among these prior approaches, the work most closely related to ours is the application of multiple losses to jointly learn the SFs, a task-encoding vector, and Q-values (Ma et al., 2020). However, there are several key differences: (1) Our approach does not require the agent to be provided with a goal--it is learned through interaction with the environment; (2) We provide direct evidence that our method works with pixel inputs; (3) We demonstrate that our approach eliminates the need for an SF loss; and (4) By removing the SF loss, we reduce the number of hyperparameters required, thereby simplifying the model.

In our results below, we compare our method to these classes of solutions described, namely reconstruction solutions (Machado et al., 2020), pretraining solutions (Liu and Abbeel, 2021), and orthogonality solutions (Touati et al., 2022).

## 3 Preliminaries

### Reinforcement Learning

The RL setting is formalized as a Markov Decision Process defined by a tuple \((S,A,p,r,)\), where \(\) is the set of states, \(\) is the set of actions, \(r:S\) is the reward function, \(p:\) is the transition probability function and \([0,1)\) is the discount factor which is being to used to balance the importance of immediate and future rewards (Sutton and Barto, 2018).

At each time step \(t\), the agent observes state \(S_{t}\) and takes an action \(A_{t}\) sampled from a policy \(:\), resulting in to a transition of next state \(S_{t+1}\) with probability \(p(S_{t+1} S_{t},A_{t})\) and the reward \(R_{t+1}\).

### Successor Features

SFs are defined via a decomposition of the state-action value function (i.e. the expected return), \(Q\), into the reward function and a representation of expected features occupancy for each state \(S_{t}\) and action \(A_{t}\) of time step \(t\):

\[Q(S_{t},A_{t},)=(S_{t},A_{t},)^{}\] (1)

where \(^{n}\) are the SFs that capture expected feature occupancy and \(^{n}\) is a vector of the task encoding, which can be considered a representation of the reward function (Borsa et al., 2018).

Canonically, the SFs for a state-action pair \((s,a)\) under a policy \(\) are defined as:

\[^{}(s,a)^{}[_{i=t}^{}^{i-t} _{i+1} S_{t}=s,A_{t}=a]\] (2)

where \(^{n}\) is a set of basis features, and \(\) is the policy (Barreto et al., 2017).

However, as shown by Borsa et al. (2018), we can treat the task encoding vector \(\) as a way to encode policy \(\). This results in _Universal SFs_, \((s,a,)\), on which we base our work.

The task encoding vector \(\) can also be related directly to the rewards themselves via the underlying basis features (\(\)):

\[R_{t+1}=(S_{t+1})^{}\] (3)

### Canonical Approach to Learning Successor Features and its Limitations

The canonical approach for learning the basis features \(\) and successor features \(\) for each state \(S_{t}\) and action \(A_{t}\) of time step \(t\), with respect to policy \(\), are achieved by optimizing the following SF Temporal-Difference loss:

\[L_{,}=\|(S_{t+1})+(S_{t+1},a,))- (S_{t},A_{t},)\|^{2}\] (4)

where action \(a(S_{t+1})\). The basis features \(\) are typically defined as the normalized output of an encoder, which the SFs \(\) learn from concurrently (see Figure 2 for an example).

However, when the basis features, \(\), must be learned from high-dimensional, complex observations such as pixels, optimizing Eq. 4 may result in the basis features, \(\), converging to a constant vector. This outcome occurs because it can minimize the loss, as noted by Machado et al. (2020), which we will also prove mathematically below.

### Proof by Contradiction: Representation Collapse in Successor Features

Consider the basis features function \(()\) and the Successor Features \(()\), omitting the inputs for clarity. The canonical SF-TD loss (Eq. 4) is defined as:

\[L_{,}=\|()+()-() \|^{2}\] (5)

Using _proof by contradiction_, we aim to show that when both \(()\) and \(()\) are constants across all states \(S\), specifically when \(()=c_{1}\) and \(()=c_{2}\) with \(c_{1}=(1-)c_{2}\), the system satisfies the zero-loss conditions, leading to representation collapse.

We start with the assumption that if \(()=c_{1},()=c_{2}\), then \(L_{,} 0\)\( c_{1},c_{2}\).

Substituting \(()=c_{1}\) and \(()=c_{2}\) into the loss function:

\[L_{,}=\|c_{1}+ c_{2}-c_{2}\|^{2}\] (6)

It is trivial to observe that if \(c_{1}=(1-)c_{2}\), the expression for \(L_{,}\) is as follows:

\[L_{,} =\|(1-)c_{2}+ c_{2}-c_{2}\|^{2}\] \[=\|0\|^{2}\] \[=0\] (7)

This contradicts our assumption that \(L_{,} 0\) for a particular relationship between \(c_{1}\) and \(c_{2}\). 

Thus, we have shown that there exist constants \(c_{1},c_{2}\) such that when \(()=c_{1}\) and \(()=c_{2}\) with \(c_{1}=(1-)c_{2}\), the system **does** satisfy the zero-loss conditions, resulting in degenerate solutions for \(L_{,}\), i.e. causing representation collapse. In this collapsed state, \(()\) loses its ability to distinguish between different states effectively, causing the model to lose critical discriminative information and thus impairing its generalization capabilities.

Additionally, we also show empirically in Figure 1(a-c) of the presence of representation collapse when learning using Eq. 4. In this work, our method aims to mitigate these issues with a novel, simple approach for learning SFs directly from pixels.

Proposed Method

The key insight from the proof above (section 3.4) is that preventing representation collapse requires avoiding the scenario where the basis features \(\) become a constant vector for all states \(S\), which would minimize the loss without contributing to meaningful learning. Below, we will describe the steps taken in our approach to mitigate these issues causing representation collapse.

We note that when the representations \(\) form a set of SFs, Eq. (1) is satisfied for some \(\) that also satisfies Eq. (3). Therefore, the approach we take to learn SFs is simply to ensure that over the course of the learning \(\) and \(\) come to satisfy both of these equations, which can be achieved by using the following loss functions:

\[L_{w}=\|R_{t+1}-(S_{t+1})^{}\| ^{2}\] (8)

\[L_{}=\|-(S_{t},A_{t},)^{} \|^{2}\] (9)

where \((S_{t+1})\) is treated as a constant in Eq. 8 using a stop-gradient operator, and \(\) is the bootstrapped target:

\[=R_{t+1}+_{a^{}}(S_{t+1},a^{},)^{} \] (10)

Here, \(\) is only altered by \(L_{w}\), whereas SF \(\) and the basis features \(\) are learned via \(L_{}\).

Specifically, our proposed approach can _overcome representation collapse by treating the basis features \(\) as the L2 normalized output from the encoder of the SF \(\) network_ (Figure 2), because unlike in Eq. 4, Eq. 8 and Eq. 9 are not minimized by setting \(\) to a constant value, given that \(\) and \(R_{t+1}\) are _not constants for all states_\(S\). Hence, there is nothing encouraging the network to converge to a constant vector, naturally avoiding representational collapse.

When the basis features \(\) are needed to learn the task encoding vector \(w\) through the reward prediction loss (Eq. 8), we apply a stop-gradient operator to treat the basis features \(\) as fixed. As we will demonstrate in section \(7\) "Analysis of Efficiency and Efficacy", this inclusion of a stop-gradient operator is crucial. Without it, learning both the basis features \(\) and the task encoding vector \(w\) concurrently can lead to learning instability.

Next, we will clarify how our approach relates to learning SFs, as they are defined mathematically. Given the straightforward nature of our approach, we refer to the SFs learned as _"Simple SFs."_

### Bridging Simple SFs and Universal Successor Features

In Proposition 1 (Appendix C), we show that our approach ultimately produces true SFs, equivalent to the SFs learned using Eq. 4. Proposition 1 does this by proving that minimizing our losses (Eq. 8 & Eq.9) also minimizes the canonical SF loss used in Universal Successor Features (Eq. 4). Furthermore, Proposition 1 supports the proof above (Section 3.4) that our approach minimizes these losses in a manner such that setting the basis features \(\) to a constant is not a solution. Once again, if \(=c_{2}\) and \(=c_{1}=(1-)c_{2}\) then Eq. 8 & Eq. 9 are not minimized, due to the fact that \(\) and \(R_{t+1}\) in Eq. 10 are not constants for all states \(S\).

Figure 2: Our proposed model for learning SFs. Starting from the top, the representations of state \(S_{t}\) are learned using the shared encoder, resulting in \(h_{t}\). The basis features \((S_{t+1})\) are the normalized output of the encoder using state \(S_{t+1}\). The task-encoding vector \(\) is learned through the reward prediction loss (Eq. 8). Concatenated with \(w\), the basis features and successor features are learned through computing the Q-values with \(\) and minimizing the _Q-SF-TD_ loss function (Eq. 9). A schematic for continuous actions and previous approaches can be found in Appendix G and H respectively.

## 5 Learning Successor Features the Simple Way

The architecture for our network is shown in Figure 2, which is broadly inspired by Liu and Abbeel (2021). Pixel-level observations, \(S_{t}\), are fed into a convolutional encoder that outputs a latent representation \(h(S_{t})\), which is used both to construct the basis features and the SFs. To construct the basis features, \((S_{t})\), we simply normalize the latent representations \(h\) (via \(L2\) normalization, following Machado et al. (2020)). To calculate the representations \((S_{t},A_{t},)\), the latent representation is combined with the task encoding vector, \(\), and fed into a multilayer perceptron that generates one set of representations for each possible action, \(A_{t}\). These representations are then combined with the task encoding via a dot product operation to estimate the \(Q\)-value function, \(Q(S_{t},A_{t},)=(S_{t},A_{t},)^{}\). The policy is then simply an \(\)-greedy policy based on the \(Q\)-value function.

To learn the basis features (\(\)) and representations (\(\)), we minimize the losses in Eq. 8 and Eq. 9 using minibatch samples of experience tuples \((S_{t},A_{t},R_{t+1},S_{t+1},)\), collected while interacting with the environment and sampled from a replay buffer which is similar to Mnih et al. (2015). Critically, only the task-encoding vector \(\) is learned by optimizing Eq. 8, so a stop gradient operator is applied to the basis features \((S_{t})\) (see Figure 2). The successor features, \(\), in the bootstrap target, \(\), actually come from a target network, \(\), which is updated periodically by using the actual network, a common approach in deep RL (Mnih et al., 2015). The successor features \(\), and all of the upstream network parameters \(\), are learned by minimizing Eq. 9. The full algorithm used for training our network is given in Algorithm 1 in Appendix B.

## 6 Experimental results

The environments used in our studies are \(10 10\) 2D grid worlds, 3D Four Rooms environments (Figure 9 in Appendix D) and Mujoco. All studies were conducted exclusively using pixel observations, as the primary motivation for this paper is to address representation collapse when learning with pixel observations.

The grid worlds offer both egocentric (partially) and allocentric (fully observable) scenarios while the 3D Four Rooms environments provide exclusively egocentric observations. The rationale behind selecting these environments was threefold: first, to evaluate the agent's learning capabilities across varying levels of environmental visibility, second, to examine its ability to interpret spatial relation

Figure 3: Continual Reinforcement Learning Evaluation with pixel observations in 2D Minigrid and 3D Four Rooms environment. **Replay buffer resets at each task transitions** to simulate drastic distribution shifts: Agents face two sequential tasks (Task 1 & Task 2), each repeated twice (Exposure 1 & Exposure 2). **(a-c):** The total cumulative returns accumulated during training. Overall, our agent, Simple SF (orange), shows notable superiority and exhibited better transfer in later tasks over both DQN (blue) and agents with added constraints. Importantly, constraints like reconstruction and orthogonality on basis features can impede learning. The plots for moving average episode returns are available in Appendix J.6 for additional insights.

ships and distances, and third, to provide a set of tasks where the transition dynamics are easy to quantify for constructing SRs that can serve as a comparison to evaluate the SFs with.

For a more complex setting, we considered the Mujoco environment because it allows for direct manipulation of the reward function and domains switching, such as moving from half-cheetah to walker, given that they both have the same action dimensions.

To evaluate the generalization capabilities of the learned SFs, our studies focus on continual learning setting. In the 2D grid worlds and 3D Four Rooms environment, agents are exposed to two cycles of two distinct tasks. These tasks involves changes in reward locations (as shown in Figure 8(b) & Figure 8(d)) and/or changes in environment dynamics (as shown in Figure 8(a) & Figure 8(e)). Additionally, we explored two different scenarios to better simulate real-world conditions. The first scenario involves resetting the replay buffer at each task transition, which emulates drastic distribution shifts typically encountered in real-world applications. The second scenario maintains the replay buffer across task transitions, allowing us to assess the agent's learning continuity in a more stable data setting.

In the Mujoco environment, agents are exposed to one cycle of two distinct task as in this setting, we primarily wish to evaluate how well the agents can adapt to new tasks and mitigating interference.

In all experiments, we make comparisons with several baselines, namely, a Double Deep Q-Network (DQN) agent  and agents learning SFs (\(\)) with constraints on their basis features (\(\)), including reconstruction loss , orthogonal loss , and unlearnable random features . Additionally, we compare with an agent that learns SFs through a non-task-engaged pre-training regime . The mathematical definitions of the constraints can be found in Appendix F. To ensure the robustness of our results, all experiments are conducted across 5 different random seeds.

### 2D Grid world

The 2D Gridworld environments were developed based on 2D Minigrid . We created two different layouts of the 2D Gridworld environment, namely Center-Wall (Figure 8(a)) and Inverted-LWalls (Figure 8(b)). In order to align the setting more closely with the canonical Gridworld environment as described by Sutton and Barto , we altered the reward function such that it returns a reward of +1 when the agent successfully reaches the goal state and 0 otherwise. For the 2D Gridworld environments, the agents were trained for one million steps per task.

Figure 2(a) presents the cumulative returns for the Center-Wall environment with egocentric observations, while Figure 2(b) shows the results for allocentric observations.

The results show that our agent learns as well as, if not better than, the baseline models. Furthermore, when analysing the cumulative total returns during training, our model, SF Simple, exhibited better transfer that the baseline models. Particularly, SFs that are learned with constraints on the basis features clearly struggle to learn effectively, either due to the additional computational overhead or because representations that fulfill those constraints do not lead to effective policy learning.

### 3D Four Rooms environment

We developed the 3D Four Rooms environments (Figure 8(d)) using Miniworld . In this environment, the state and action spaces are continuous. In the first task, the agent receives a reward of +1 when it reaches the green box and a reward of -1 when it reaches the yellow box and this alternates for the second task. The agents were trained for five million steps per task. Similarly, the results in Figure 2(c) show that our agent is able to learn effectively using egocentric pixel observations in a 3D environment.

### Mujoco

In order to demonstrates our model's capabilities with continuous actions, we consider the Mujoco environment. We followed the established protocol in Yarats et al.  for effective learning with pixels observations in this environment. We started in the half-cheetah domain, rewarding agents for running forward in Task 1. For Task 2, we introduced scenarios with running backwards, running faster, and switching to the walker domain. The results are presented in Figure 4.

Across all scenarios, our model not only maintained high performance but consistently outperformed all baselines in both Task 1 and Task 2, highlighting its superior adaptability and effectiveness in complex environments. This contrasted sharply with other SF-related baseline models, which struggled to adapt under these conditions.

## 7 Analysis of Efficacy and Efficiency

### Comparison to Successor Representations

Can our SF-learning technique, like traditional SRs, effectively capture the transition dynamics of the environment [Stachenfeld et al., 2017]? To investigate, we first sought a quantitative measure to compare SFs to SRs. To do this, we trained a simple non-linear decoder to assess which model's SFs can be most effectively decode into SRs. We conducted this evaluation using both allocentric and egocentric observations within the center-wall environment. The results, depicted in Figure 5, shows that our model demonstrate consistently high accuracy (lower errors) across both settings. This contrasts sharply with SFs developed using reconstruction constraints or random basis features, which, while effective in egocentric settings, perform poorly in allocentric settings where feature sparsity is greater.

We next utilized 2D visualizations with geospatial color mapping to differentiate environmental locations, aiming to see if similar SFs that are proximate in neural space are proximate in physical space. Using UMAP [McInnes et al., 2018] for dimension reduction, our results (Figure 6) suggest that our simple approach captures environmental statistics comparably to other models, but with less overhead for calculating the loss. Moreover, our technique consistently forms organized spatial clusters across partially, fully, and egocentric observational settings.

Additionally, we performed a correlation analysis in 2D Gridworld environments, comparing each spatial position and head direction against analytically computed SRs [Dayan, 1993], further confirm

Figure 4: Continual Reinforcement Learning results using pixel observations in _Mujoco_ environment across 5 random seeds. **Replay buffer resets at each task transitions** to simulate drastic distribution shifts. we started with the half-cheetah domain in Task 1 where agents were rewarded for running forward. We then introduced three different scenarios in Task 2: **(a)** agents were rewarded for running backwards, **(b)** running faster, and, in the most drastic change, **(c)** switching from the half-cheetah to the walker domain with a forward running task. To ensure comparability across these diverse scenarios, we normalized the returns, considering that each task has different maximum attainable returns per episode. We did not evaluate APS (Pre-train) here because it struggles in the Continual RL setting, even in simpler environments such as the 2D Minigrid and 3D Minisworld.

Figure 5: Decoding performance comparison of models’ SFs into SRs using a non-linear decoder in the Center-Wall environment. Ground truth SRs are generated analytically using Eq. 21, described in Appendix N. Lower Mean Squared Error values on the y-axis indicate better performance.

ing the robustness and adaptability of our model's SFs in various observational contexts (Table 6 and Table 7 in Appendix N).

### Is Stop Gradient critical for learning?

Previous methods that concurrently learn the basis features, \(\), and the task-encoding vector \(w\), often face challenges with learning efficiency and stability, particularly in environments characterized by sparse rewards. This issue is illustrated in Figure 10 in Appendix D of Ma et al. (2020), where optimizing the reward prediction loss (Eq. 8) can inadvertently drive the basis features towards zero (\(\)), causing significant representational collapse. Representational collapse not only reduces the discriminative capabilities of \(\) but also undermines the agent's ability to differentiate between distinct states, thus severely impacting the overall learning process.

As depicted in Figure 2, our solution involves the strategic use of a stop gradient operator applied to the basis features \(\). This operator prevents the gradient from the reward prediction loss from updating basis features \(\), effectively decoupling the learning of \(\) from \(w\), thus ensuring that it retains its critical discriminative statistics, allowing for effective learning as demonstrated in Figure 7.

### Robustness to Stochasticity within the environment

How robust are the SFs learned using our approach as the environment dynamics become noisier? To explore this question and verify the robustness of our technique, we also created a slippery variant of the Four Rooms environment (Figure 8(e)). Specifically, in the top right and bottom left rooms, the agent experiences a "slippery" dynamic: chosen actions have a certain probability of being replaced with random, alternative actions. This design mimics the effects of a low-friction or slippery surface, creating a scenario where the agent's intended movements might lead to unpredictable outcomes.

Figure 6: 2D visualization of Successor Features in **(a)** the fully-observable Center-Wall environment and **(b)** the 3D Four Rooms environment. Each row represents different models’ visualizations post-training, starting with geospatial color mapping of the layout in the first column, followed by comparisons of SF-based models. Clustering indicates the capture of environmental statistics. Despite this, well-clustered SF models, especially those with orthogonality constraints, may not always translate to effective policy learning, as seen in Figure 3. In allocentric scenarios, SFs with reconstruction constraints struggle with minimal pixel variations, unlike in the distinct pixel changes in the Four Rooms environment. For more visualizations, see Appendix M.

The results in Figure 8a-d demonstrate that our approach is robust to increasing levels of stochasticity. Notably, when the stochasticity is high (slippery probability \(>=0.3\)), all other SF methods fail to learn effectively in the second task, whereas our approach continues to perform well.

### Efficiency analysis

How do alternative SF learning methods with extra loss functions, like orthogonality constraints, stack up against our approach in terms of efficiency? We analyzed the number of steps each method takes to learn an effective policy, using a performance threshold defined by the shortest expected episode length from the last 10 episodes. A shorter episode length indicates better performance, as it signifies quicker goal achievement. We noted the timestep when each model first met or exceeded this threshold. Our results, shown in Figure 8e, demonstrate that our method outperforms all baselines in learning efficiency. Additionally, our method leverages simpler compute blocks and loss functions, enhancing computational speed and reducing training duration, as shown by faster frame processing rates (Figure 8f) and shorter overall training times (Figure 8g). Therefore, our approach is more efficient than the baseline methods for learning SFs.

## 8 Discussion

In this work, we developed a method for learning SFs from pixel-level observations without pre-training or complex auxiliary losses. By applying the mathematical principles of SFs, our system effectively learns during task engagement using standard losses based on typical training returns and rewards. This simplicity and efficiency are key advantages of our approach.

Our experiments demonstrate that our method learns SFs effectively under various conditions and surpasses baseline models in continual RL scenarios. It effectively captures environmental transition dynamics and correlates well with analytically computed Successor Representations (SRs), offering a streamlined, efficient strategy for integrating SFs into RL models. Future work could build on this to create more sophisticated models that leverage SFs for enhanced flexibility in RL.

Figure 8: Efficiency analysis using 3D Slippery Four Rooms environment. **(a-d)**: Robustness analysis to increasing levels of stochasticity. **(e)** Bar plot showing efficiency in learning, measured as steps to achieve a policy that produces a reasonable level of performance, with low values indicating higher efficiency. **(f)** Bar plot showing frames per second achieved by the agent during gradient computation, back-propagation, and interaction with the environment. These metrics provide insights into the computational efficiency and the real-time interaction capabilities of the agent across different tasks or conditions. **(g)** Bar plot showing the total training duration for completing two exposures of two tasks, demonstrating overall time efficiency. Collectively, these plots reveal that our agent not only learns tasks effectively but also excels in computational efficiency.

Limitations and Broader Impact

The algorithms we developed were evaluated predominantly in simulated environments, which may not capture the diverse complexity of real-world scenarios. A key assumption in our approach is that pixel observations are of good quality. This assumption is critical as poor image quality could substantially degrade the performance and applicability of our algorithms.

The use of Successor Features in learning algorithms, as demonstrated in this work, offers significant advantages, particularly in mitigating catastrophic interference. This capability is crucial for the development of machine learning systems that require continuous learning, such as in dynamic environments. For instance, autonomous vehicles operating in ever-changing conditions can retain learned knowledge while adapting to new information, enhancing their safety and reliability.

However, the enhanced capabilities of these systems also raise concerns. The ability of machine learning models to continuously adapt and learn can lead to challenges in predictability and control, potentially making outcomes less transparent. As systems become more autonomous and capable of adapting over time, there's a risk that errors or biases in the learning process could propagate more extensively before detection, especially if oversight does not keep pace with the rate of learning.

## 10 Acknowledgements

Raymond Chua was supported by the DeepMind Graduate Award and UNIQUE Excellence Scholarship (PhD). We extend our gratitude to the FRQNT Strategic Clusters Program (2020-RS4-265502 - Centre UNIQUE - Quebec Neuro-AI Research Center).

Arna Ghosh was supported by the Vanier Canada Graduate scholarship and Healthy Brains, Healthy Lives Doctoral Fellowship.

Blake A. Richards was also supported by NSERC (Discovery Grant RGPIN-2020- 05105, RGPIN-2018-04821; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022), Healthy Brains, Healthy Lives (New Investigator Award: 2b-NIISU-8), and CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship).

This research was further enabled by computational resources provided by Calcul Quebec 2 and the Digital Research Alliance of Canada 3, along with the computational resources support from NVIDIA Corporation.

We are grateful to Gheorghe Comanici, Pranshu Malviya, Xing Han Lu, Isabeau Premont-Schwarz and the anonymous reviewers whose insightful comments and suggestions significantly enhanced the quality of this manuscript. Additionally, our discussions with members of the LiNC lab 4, Mila 5, and early collaborators from Microsoft Research (MSR) have been invaluable in shaping this research. Special thanks to Ida Momennejad, Geoff Gordon and Mehdi Fatemi from MSR for their substantial insights and contributions during the initial phases of this work.