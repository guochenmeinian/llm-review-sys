# Multi-scale Consistency for Robust 3D Registration via Hierarchical Sinkhorn Tree

Chengwei Ren\({}^{1,2}\) Yifan Feng\({}^{3}\) Weixiang Zhang\({}^{2}\) Xiao-Ping Zhang\({}^{1,2}\)1 Yue Gao\({}^{3}\)1

{\({}^{1}\)Shenzhenen Ubiquitous Data Enabling Key Lab, \({}^{2}\)Shenzhenen International Graduate School,

\({}^{3}\)BNRist, THUIBCS, School of Software}, Tsinghua University

rcw22@mails.tsinghua.edu.cn, evanfeng97@gmail.com, zhang-wx22@mails.tsinghua.edu.cn

xpzhang@ieee.org, gaoyue@tsinghua.edu.cn

###### Abstract

We study the problem of retrieving accurate correspondence through multi-scale consistency (MSC) for robust point cloud registration. Existing works in a coarse-to-fine manner either suffer from severe noisy correspondences caused by unreliable coarse matching or struggle to form outlier-free coarse-level correspondence sets. To tackle this, we present **H**ierarchical **S**inkhorn **T**ree (HST), a pruned tree structure designed to hierarchically measure the local consistency of each coarse correspondence across multiple feature scales, thereby filtering out the local dissimilar ones. In this way, we convert the modeling of MSC for each correspondence into a BFS traversal with pruning of a K-ary tree rooted at the superpoint, with its K nearest neighbors in the feature pyramid serving as child nodes. To achieve efficient pruning and accurate vicinity characterization, we further propose a novel overlap-aware Sinkhorn Distance, which retains only the most likely overlapping points for local measurement and next level exploration. The modeling process essentially involves traversing a pair of HSTs synchronously and aggregating the consistency measures of corresponding tree nodes. Extensive experiments demonstrate HST consistently outperforms the state-of-the-art methods on both indoor and outdoor benchmarks.

## 1 Introduction

Point cloud registration is a crucial task in 3D computer vision that involves aligning a pair of partially overlapped point clouds to create a unified scene representation. The most common approaches  follow the two stage technical roadmap, i.e., matching and transformation. They first form a set of high confident correspondences through repeatable feature descriptors, and then a robust estimator is utilized to calculate the rigid transformation. Although extensively studied over the past decades, the task remains challenging due to limited overlap, severe noise, etc.

Recent advances  have made substantial progress in learning-based methods. The key idea is to train a shared network to extract point-wise features and establish reliable correspondences based on them. Inspired by the progress in image registration counterparts , a coarse-to-fine strategy  is leveraged to avoid keypoint detection and unrepeatable correspondence searching. It has demonstrated superior performance over the state-of-the-art methods. They establish sparse superpoint correspondences on the downsampled input point clouds (coarse level) and then refine them to point level to yield dense point

Figure 1: Illustration of the proposed HST for measuring MSC. It extracts local patches at multi-scales in the feature pyramid and then calculates their similarity layer by layer.

correspondences (fine level). Therefore, the accuracy of coarse correspondences directly impacts that of fine correspondences [12; 14]. Intuitively, A pair of incorrect coarse matches may introduce potential outlier fine correspondences, thereby influencing the final transformation estimation.

Prior works try to alleviate this problem from two perspectives. The first [13; 14; 15] attempts to nip it in the bud during coarse matching. They adapt the Transformer  to unordered point cloud representation to mitigate matching ambiguity. A series of embeddings are proposed to facilitate learning discriminative geometric consistency in both explicit and implicit manners for more accurate matching. The second [17; 18; 19; 20; 21; 22] ignores the inaccuracy in coarse stage and directly filters out the outlier correspondences at fine stage, known as outlier removal methods. Such methods distinguish between inlier and outlier by developing well-designed compatibilities [23; 24; 21; 22] to characterize the affinity relationship between correspondences in geometric or feature space. Though the above techniques have achieved surprising performances, both of them still suffer from noise and low overlap. Specifically, the first only considers coarse-level feature interactions, lacking exploration of finer-scale local consistency. Erroneous correspondences are more likely to occur, especially in low-overlapping scenarios. The second suffers from inaccurate outlier filtering when confronted with a set of high outlier rates correspondences caused by severe noise. Consequently, the outliers, which arise primarily from inaccurate matching in the coarse stage, would inevitably be involved in the transformation estimation step, ultimately resulting in failed registration.

To tackle the above problems, we propose the Hierarchical Sinkhorn Tree with overlap-aware Sinkhorn distance to model the multi-scale consistency (MSC) of correspondences for more accurate coarse-level matching. Multi-scale consistency analysis, due to its innate capability to analyze patterns at different levels of detail, has been variously applied to matching- [25; 26; 27] and retrieval- [28; 29] related vision tasks. However, in point cloud registration, where the irregular nature of data hinders the promotion of corresponding image MSC analysis methods, leveraging them for more reliable correspondence retrieval remains unexplored. Given that MSC characterizes the local consistency of matches across multiple feature scales, which effectively addresses the challenge of inaccurate matches at the coarse stage, we aim to tackle them both. The key idea is to hierarchically evaluate the vicinity geometric similarity of each correspondence at multiple scales using Sinkhorn distance  and filter out highly dissimilar coarse matches. Specifically, we employ Local Exploration to

Figure 2: Overview of the proposed method. The overall architecture (**Top**) is designed in a coarse-to-fine manner, which extracts coarse correspondences and then refines them to point level. Our proposed method contains the following parts: **1.** Local exploration (**Left**) extracts local patch pairs for each putative match in multiple scales from the feature pyramid (Sec. 3.2.1). **2.** Patch Overlap Prediction module (**Bottom Middle**) is then adopted to predict the overlap points between patch pairs (Sec. 3.2.2). **3.** Overlap-aware Sinkhorn Distance (**Bottom Right**) measures the patch similarity by focusing on potential overlap points (Sec. 3.2.3). Finally, we repeat the above operations across the layers to construct the Hierarchical Sinkhorn Tree (**Left**) in a BFS way. The process of modeling MSC essentially involves traversing a pair of HSTs and then aggregating the consistency measures of the corresponding tree nodes (Sec. 3.2.4).

extract local patches at each correspondence's next decoder layer. Subsequently, the overlap-aware Sinkhorn distance (overlap SD) measures the patch differences (see Fig. 3) by focusing the Sinkhorn operation on potential overlap points within the patch, disregarding the non-overlapping ones. This significantly enhances the robustness to noise points while achieving computational savings. Following this, a subset of the most likely overlapping points is retained for further local exploration and overlap SD measurement. This repetitive process refines the multi-scale local consistency of each correspondence as the scale becomes finer. Finally, we aggregate the consistency measures across all scales and utilize probabilistic search to select the most reliable coarse correspondences robustly. An overview of our method can be found in Fig. 1.

To sum up, our main contributions are three-fold:

* To the best of our knowledge, we are the first to introduce multi-scale consistency into point cloud registration task to mitigate the effects of low overlap and high noise.
* We propose a method for modeling multi-scale consistency called HST, which characterizes the similarity of potential overlapping points in the vicinity areas layer by layer and aggregates them into multi-scale consistency.
* We introduce an overlap-aware Sinkhorn Distance to focus optimal transport processes only on potential overlapping points, significantly enhancing the robustness of consistency calculations while reducing solution complexity.

Extensive experiments on both indoor and outdoor benchmarks demonstrate our scene-agnostic superiority. HST significantly outperforms the state-of-the-art methods on Registration Recall on the challenging 3DLoMatch benchmark.

## 2 Related Work

Correspondence-based point cloud registration.Early correspondence-based registration methods follow the detect-and-transform pipelines. A series of local geometric descriptors [1; 31; 32; 33] are proposed to detect repeatable salient points of point cloud pairs. Then point-level correspondences [5; 4; 6] are established to recover the transformation between point clouds through a robust estimator [20; 21; 22]. Recently, detector-free methods [12; 13; 15] to avoid unrepeatable keypoint detection are proposed. They introduce a coarse-to-fine manner derived from 2D image matching [8; 9; 10; 11] to shrink the correspondence searching space. It first extracts full and reliable matches on coarse-resolution features and then refines them on the corresponding local patch from finer resolution. It's becoming prevalent due to its excellent matching accuracy and computational efficiency. Our method follows the technical roadmap of detector-free methods and focuses on improving the correspondence reliability of the coarse phase.

Coarse-to-fine matching.Recent works [8; 9; 10; 11] advance 2D image matching by leveraging a coarse-to-fine fashion to avoid unrepeatable keypoint detection while increasing matching reliability, known as detector-free methods. DualRC-Net  extracts coarse feature maps to form complete correlations and generate pixel-level matches with the help of fine features. Patch2pix  refines coarse patch matches by regressing fine pixel matches from local regions. Loftr  and CasMTR  introduce an attention mechanism  to search for accurate low-resolution correspondences and then establish dense matching based on informative high-resolution local patches.

Multi-scale consistency.The core idea of multi-scale consistency is to evaluate the similarity of matches across multiple feature scales. The criterion under this for evaluating an inlier match is that each spatial resolution of feature pyramids should maintain a certain degree of similarity. BiseNet  utilizes an auxiliary loss to supervise the consistency between multi-scale features and ground-truth to enhance the feature representation. MSCAN  leverages a multi-scale attention module to capture features of different scales and aggregates them into a global descriptor for robust image retrieval. ACMM  conducts the multi-scale geometric consistency to refine depth maps at each scale, reaching satisfying performance.

Figure 3: A toy example illustrating the Sinkhorn Distance (SD) between local patches. The patch pair from inlier correspondence (green) maintains a lower SD metric.

## 3 Method

### Problem Statement

Given two point clouds \(=\{_{i}^{3} i=1..N\}\), and \(=\{_{j}^{3} i=1..M\}\), the goal is to recover a rigid transformation \((,)\) that aligns the two point clouds with rotation \( SO(3)\) and translation \(^{3}\). We utilize the KPConv-FPN  as the backbone and leverage geometric self- and cross-attention  to estimate \(C\) pairs of coarse correspondences for registration in a coarse-to-fine manner [12; 13]. Our goal is to remove the outlier correspondences from the above estimated correspondences for further accurate and robust registration. The key idea is to hierarchically model the vicinity similarity at multiple scales between each correspondence.

### Hierarchical Sinkhorn Tree

We first propose _Multi-scale Consistency_ (MSC) for coarse level correspondences, which assumes that each correspondence contains similar local features at different down-/up-sampling scales. I.e., given a pair of points \(\) and \(\) with their local patch features at \(L\) scales to be \(\{}^{(1)},}^{(2)},...,}^{(L)}\}\) and \(\{}^{(1)},}^{(2)},...,}^{(L)}\}\), then \(\) and \(\) is an putative inlier correspondence only they satisfy:

\[_{l=1}^{L}(}^{(l)},}^{(l )})<_{d},\;l[1,L],\] (1)

where \(()\) is the pre-defined metric for measuring the patch difference and \(_{d}\) is the threshold.

We further propose the _Hierarchical Sinkhorn Tree_ with _Overlap-aware Sinkhorn Distance_ to model the MSC of correspondences. It first applies Local Exploration (Sec. 3.2.1) to extract local patch at next finer scale from the feature pyramid. Then the Overlap-aware Sinkhorn Distance (Sec. 3.2.3) picks the overlap points from patches using Patch Overlap Prediction (Sec. 3.2.2) module and performs Sinkhorn Distance with overlap-aware marginal initialization on them. These two steps are performed layer-wisely in a hierarchical tree way called the Hierarchical Sinkhorn Tree (Sec. 3.2.4).

#### 3.2.1 Local Exploration

KPConv-FPN  varies point density by altering the size of grid cells at each layer to form feature pyramids. Moreover, the features from FPN decoder provide sufficient information to analyze the vicinity similarity between correspondences due to the skip connections. Consequently, we perform nearest-neighbor exploration  on the match's neighboring points from subsequent decoder layer.

Given point cloud \(^{(l)}\) from the \(l\)-th layer of decoder, and its next layer point cloud \(^{(l+1)}\), where \(0 l L-1\) and \(L\) is the number of decoder. We have \(k\)-nearest neighbor (\(k\)-NN) search to obtain the local patch \(_{i}^{(l+1)}\) from the next layer for point \(_{i}^{(l)}\):

\[_{i}^{(l+1)}=_{_{j}^{(l+1)} ^{(l+1)}}(-||_{i}^{(l)},_{j}^{(l+1)}||_{2})\] (2)

#### 3.2.2 Overlap Points Prediction

After the above local exploration, each correspondence obtains a pair of local patch features from the first decoder layer. We then utilize them to predict overlap points between each patch pair for subsequent Overlap-aware Sinkhorn Distance Computation. We extract overlap information from local patches using a simple yet effective **Patch Overlap Prediction (POP)** module. It takes input as the coarse-grained features containing global overlap information from coarse matching module and the fine-grained features from the current decoder layer, aiming to integrate features across granularity and predict fine-grained overlap between patches. It consists of two components: a borderless EdgeConv for aggregating local features, and a cross-attention [6; 36] for interacting cross-patch information.

Though EdgeConv  effectively captures local information by constructing point-wise \(k\)-NN graphs within the patch, it restricts the search within the patch. It pushes the vicinity towards the interior of the patch when encountering boundary points, affecting the smoothness of local descriptions. Therefore, we have lifted this restriction, allowing the \(k\)-NN search to include points outside the patch, and we refer to it as borderless EdgeConv. Furthermore, we strengthen each pointfeature as \(}=_{}([^{d}}\;;^{ t}}])\) for integrating overlap features into patches, Here, \(^{d}}\) is the feature from the current decoder layer and \(^{t}}\) is the nearest upsampled feature from the last transformer block. \(_{}\) is a nonlinear layer activated by Softplus and each decoder layer's point features are mapped to the same dimension, allowing for the sharing parameters of subsequent modules across scales. Then the edge feature \(_{ij}\) between point pair \(_{i}\) and \(_{j}\) with features \(^{i}}\) and \(^{j}}\) in a patch can be calculated as \(_{ij}^{(k)}=_{}([^{i}}^{(k- 1)},^{j}}^{(k-1)}-^{i}}^{(k-1)}])\). Here, \(_{}\) denotes a nonlinear layer activated by LeakyReLU, \(k\) denotes the \(k\)-th layer of EdgeConv. Finally, the point-wise feature is calculated as \(^{i}}=_{}([^{i}}^{(0)}, ^{i}}^{(1)},^{i}}^{(2)}])\), and \(^{i}}^{(k)}\) is max-pooled point feature from \(k\)-th layer.

Feature-based cross-attention is then used to achieve information interaction between two local patches corresponding to a pair of node correspondences. Given the features \(}\) and \(}\) after EdgeConv of two patches \(\) and \(\), the output feature of \(}\) after cross-attention is:

\[^{i}}=_{j=1}^{||}a_{i,j}(^{j}W^{V}}),^{i}}},^{j}} },\] (3)

where \(a_{i,j}=((^{i}}^{Q})( ^{j}W^{K}})^{}/})\) is the attention score between \(}\) and \(}\).

The co-contextual feature of \(}\) can be calculated in the same way. Now the output features \(}\) and \(}\) contain sufficient information to achieve overlap prediction. We utilize the 0-1 scaled cosine similarity between points of two patches as the overlap score:

\[=(}^{}}}{\|}\|\|}\|}+1)/2.\] (4)

#### 3.2.3 Overlap-ware Sinkhorn Distance

Overlap Points FilteringConducting a direct search for optimal transport across _all_ points within patches may precipitate erroneous matches, consequently inducing inaccuracies in Sinkhorn distance computation. To mitigate this issue, we advocate limiting the computation of the Sinkhorn distance to the most probable overlapping points. The rationale is that the selectively filtered overlapping points exhibit a higher likelihood of successful matching, thereby mitigating the susceptibility to distance measurement errors. Therefore, we only retain points with high overlap scores for further Sinkhorn iteration to improve the robustness of optimal transport solver.

Specifically, we adopt a dynamic \(k\)[38; 39] strategy to select the points with top overlap score adaptively. Intuitively, the number of potential overlap points should vary across the patches due to factors like geometry, overlap area, etc. Patch with higher overlap scores should retain more overlapping points. Therefore, we select the top-\(q\) predicted scores and sum them up to represent the roughly estimated overlap points number, i.e., \(k=\{_{i=1}^{q}_{q},1\}\). Then points with top-\(k\) overlap scores are retained as overlap points. Rows and columns devoid of overlap points are discarded, and only the remaining entries are involved in the subsequent Sinkhorn operation. This strategy effectively enhances the robustness while reducing computational overhead.

Overlap-aware Marginal PriorThe initialization of the Sinkhorn algorithm typically sets the marginal distributions, \(\) and \(\), as uniform distributions. This initialization strategy presupposes equal importance among points to be allocated. However, it is suboptimal and potentially unreasonable in most cases. The importance of individual points ideally varies according to factors such as their positions, features, etc. With the availability of estimated overlap scores between two patches, it is prudent to incorporate this as prior during initialization. With the help of POP for estimating the overlap score, a more informed initialization of the Sinkhorn algorithm can be achieved.

Specifically, with the overlap score denoted as \(\), we discard the rows and columns where without top-\(k\) scores to yield the filtered overlap score \(}\). Then we apply row- and column-normalization to \(}\) to adjust the marginal vectors, weighting them according to the overlap scores:

\[_{ov}=_{j=1}^{|_{c}^{}|}_{ij}^{ }/_{i,j=1}^{|^{}|}_{ij}^{} \:,\:\:_{ov}=_{i=1}^{|_{c}^{}|}_{ij}^ {}/_{i,j=1}^{|^{}|}_{ij}^{}.\] (5)

Figure 4: Overlap Points Filtering with Dynamic Top-\(k\) Illustration

Overlap-aware Sinkhorn DistanceGiven a pair of patches \(\) and \(\) with their overlap score \(\), the filtered overlap points can be computed as \(_{}=\{_{i}_{(i,j)} ()\}\) and \(_{}\) is computed in the same way. Similar to , we then calculate the cost matrix \(=-_{}(_{ })^{}/}\) based on their features. Following the dustbin setting as in [40; 41] to handle unmatched pairs, we augment the cost matrix \(^{|_{}||_{}|}\) to \(}^{(|_{}|+1)(| _{}|+1)}\) by appending a new row and column with a learnable parameter \(z\). To incorporate overlap-aware marginal prior, we append the sum of one marginal to another to serve as the dustbin. Specifically, the unnormalized \(_{ov}\) can be updated as \(_{ov}[_{ov}\ ;_{j=1}^{|_{}|} _{ov}]\), and similarly for \(_{ov}\). Then normalization is applied as in Eq. (5) to ensure that the sum equals 1. The formulation of the Overlap-aware Sinkhorn Distance is as follows:

\[_{}^{*}=_{T U(_{ov},_{ov})}_ {i=1}^{|_{}|+1}_{j=1}^{|_{}|+1} _{ij} T_{ij}\] (6) \[\ T_{|_{}|+1}=_{ov}, \ T^{}_{|_{}|+1}=_{ov},\]

where \(T_{ij}\) the \(i,j\)-th element from the assignment matrix \(T R^{(|_{ov}|+1)(|_{ov}|+1)}\) of optimal transport problem. The above problem can be solved efficiently via the Sinkhorn Algorithm [42; 30] on GPUs. Moreover, it is differentiable , enabling the back-propagation of the supervision signal from the transport result to the Patch Overlap Prediction module.

#### 3.2.4 Hierarchical Sinkhorn Tree for Multi-scale Consistency Modeling

Though the above proposed overlap SD effectively models the similarity between patches, it solely explores the neighborhood features at a single scale while still neglecting multi-scale information. To gather features at various scales for characterizing informative MSC, we propose the Hierarchical Sinkhorn Tree to traverse the feature hierarchy by repeating the above exploring and measuring steps.

Specifically, for modeling single-scale consistency, we conduct local exploration of correspondence at the next level of the feature pyramid to form local patches and then measure their similarity using overlap SD. Notably, besides the measurement computation, overlap SD also matches potential overlapping points across patches. Moreover, due to the presence of dustbin strategy, these points are further filtered. If the assignment results \(T\) generated by Sinkhorn algorithm can help select more accurate overlap points for finer-grained local exploration, it enables a more precise characterization of MSC. Therefore, we retain the most likely overlapping matches via mutual top-\(k\) selection (dropping dustbin). These matches continue exploring the feature at the next level with overlap SD. Repeating these two steps achieves the complete modeling of MSC across all scales. The whole modeling process essentially involves a Breath First Search (BFS) traversal with pruning of a K-ary tree rooted at the superpoint, with its \(k\)-NN in the feature pyramid serving as child nodes. HST is the subtrees pruned by overlap SD. Modeling MSC is essentially synchronously traversing a pair of HSTs and aggregating the overlap SD of corresponding tree nodes. We take the mean of overlap SD for each layer and then perform a weighted sum across scales to obtain the final MSC. Following [6; 14], a more robust probabilistic selection strategy is then adopted to form output correspondence set. The probability for all putative matches is \(p=(1/(-))\), where \(\) is the 0-1 normalized MSCs and \(\) is the temperature parameter that controls the soft assignment.

### Loss Functions

The total loss \(L\) is the sum of each layer's loss. And each layer-wise loss \(^{(l)}\) is composed of the overlap loss \(^{(l)}_{o}\), the overlap-aware matching loss \(^{(l)}_{om}\), and the Overlap-aware circle loss \(^{(l)}_{oc}\), i.e., \(^{(l)}=^{(l)}_{o}+^{(l)}_{om}+^{( l)}_{oc}\).

Overlap lossTo supervise the overlap prediction, we minimize the cross entropy loss between the predicted overlap score \(\) and the ground truth \(}\) with radius \(_{o}\). Here \(\) is the Ivversion bracket.

\[_{o}=|}_{i,j=1}^{||}}_{ij}(_{ij})+(1-}_{ ij})(1-_{ij}),\ \ }_{ij}=\ \|(_{i})-_{j}\| _{2}<_{o}\ \] (7)Overlap matching lossTo supervise the optimal transport result of Overlap-aware Sinkhorn Distance, we minimize the negative log-likelihood loss as in  on the assignment matrix \(T\). Given the set \(\) representing the overlap point pairs within the overlap radius \(_{ov}\) as the ground truth matching, \(\) and \(\) denoting the unmatched points, \(_{om}\) is defined as:

\[_{om}=-_{(x,y)} T_{x,y}-_{x}  T_{x,m+1}-_{y} T_{n+1,y}.\] (8)

Overlap-aware circle lossInspired by , we extend the coarse matching loss  to the feature hierarchy to further supervise the overlap prediction at each layer, i.e., \(_{oc}=(_{oc}^{}+_{oc}^{})/2\), and

\[_{oc}^{}=|}_{i=1}^{| |}[1+_{_{j}^{}_{p}^{ }}e^{_{i}^{2}_{p}^{i,j}(d_{i}^{j}-_{p})} _{_{k}^{}_{n}^{}}e^{_{n} ^{i,k}(_{n}-d_{i}^{k})}].\] (9)

We provide details on the individual terms and further particulars in the supplementary material.

## 4 Experiments

We evaluate our proposed Hierarchical Sinkhorn Tree on both indoor 3DMatch , 3DLoMatch  dataset, and outdoor KITTI odometry  dataset. More details about the datasets, evaluation metrics, and implementation are provided in the supplementary material.

### Indoor 3DMatch & 3DLoMatch

Metrics.We adopt 5 metrics to evaluate our method. Our main metric is the Registration Recall (RR), the fraction of correctly aligned point cloud pairs. Following the settings in [6; 13], the

    \\   &  &  &  \\ \# Samples & 5000 & 2500 & 1000 & 500 & 250 & 5000 & 2500 & 1000 & 500 & 250 & 5000 & 2500 & 1000 & 500 & 250 \\  FCGF  & 85.1 & 84.7 & 83.3 & 81.6 & 71.4 & 97.4 & 97.3 & 97.0 & 96.7 & 96.6 & 56.8 & 54.1 & 48.7 & 42.5 & 34.1 \\ D3Feat  & 81.6 & 84.5 & 83.4 & 82.4 & 77.9 & 95.6 & 95.4 & 94.5 & 94.1 & 93.1 & 39.0 & 38.8 & 40.4 & 41.5 & 41.8 \\ SpinNet  & 88.6 & 86.6 & 85.5 & 83.5 & 70.2 & 97.6 & 97.2 & 96.8 & 95.5 & 94.3 & 47.5 & 44.7 & 39.4 & 33.9 & 27.6 \\ Predator  & 89.0 & 89.9 & 90.6 & 88.5 & 86.6 & 96.6 & 96.6 & 96.5 & 96.3 & 96.5 & 58.0 & 58.4 & 57.1 & 54.1 & 49.3 \\ CoFiNet  & 89.3 & 88.9 & 88.4 & 87.4 & 87.0 & 98.1 & 98.3 & 98.1 & 98.2 & 98.3 & 49.8 & 51.2 & 51.9 & 52.2 & 52.2 \\ YOHO  & 90.8 & 90.3 & 89.1 & 88.6 & 84.5 & 98.2 & 97.6 & 97.5 & 97.7 & 96.0 & 64.4 & 60.7 & 55.7 & 46.4 & 41.2 \\ GeoTR  & 92.0 & 91.8 & 91.8 & 91.4 & 91.2 & 97.9 & 97.9 & 97.9 & 97.9 & 97.6 & 71.9 & 75.2 & 76.0 & 82.2 & **85.1** \\ RIGA  & 89.3 & 88.4 & 89.1 & 89.0 & 87.7 & 97.9 & 97.8 & 97.7 & 97.7 & 97.6 & 68.4 & 69.7 & 70.6 & 70.9 & 71.0 \\ REGTR  & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - \\ OIF  & 92.4 & 91.9 & 91.8 & **92.1** & 91.2 & 98.1 & 98.1 & 97.9 & 98.4 & 98.4 & 62.3 & 65.2 & 66.8 & 67.1 & 67.5 \\ RoITr  & 91.9 & 91.7 & 91.8 & 91.4 & 91.0 & 98.0 & 98.0 & 97.9 & 98.0 & 97.9 & **82.6** & **82.8** & **83.0** & 83.0 & 83.0 \\ HST (**Ours**) & **93.5** & **92.9** & **92.6** & **92.1** & **92.1** & **98.8** & **98.8** & **98.5** & **98.6** & **98.5** & 75.9 & 80.5 & 80.3 & **83.3** & 83.6 \\   \\   &  &  &  \\ \# Samples & 5000 & 2500 & 1000 & 500 & 2500 & 5000 & 2500 & 1000 & 500 & 2500 & 5000 & 2500 & 1000 & 500 & 250 \\  FCGF  & 40.1 & 41.7 & 38.2 & 35.4 & 26.8 & 76.6 & 75.4 & 74.2 & 71.7 & 67.3 & 21.4 & 20.0 & 17.2 & 14.8 & 11.6 \\ D3Feat  & 37.2 & 42.7 & 46.9 & 43.8 & 39.1 & 67.3 & 66.7 & 67.0 & 66.7 & 66.5 & 13.2 & 13.1 & 14.0 & 14.6 & 15.0 \\ SpinNet  & 59.8 & 54.9 & 48.3 & 39.8 & 26.8 & 75.3 & 74.9 & 72.5 & 70.0 & 63.6 & 20.5 & 19.0 & 16.3 & 13.8 & 11.1 \\ Predator  & 59.8 & 61.2 & 62.4 & 60.8 & 58.1 & 78.6 & 77.4 & 76.3 & 75.7 & 75.3 & 26.7 & 28.1 & 28.3 & 27.5 & 25.8 \\ CoFiNet  & 67.5 & 66.2 & 64.2 & 63.1 & 61.0 & 83.1 & 83.5 & 83.3 & 83.1 & 82.6 & 24.4 & 25.9 & 26.7 & 26.8 & 26.9 \\ YOHO  & 65.2 & 65.5 & 63.2 & 56.5 & 48.0 & 79.4 & 78.1 & 76.3 & 73.8 & 69.1 & 25.9 & 23.3 & 22.6 & 18.2 & 15.0 \\ GeoTR  & 75.0 & 74.8 & 74.2 & 74.1 & 73.5 & 88.3 & 86.6 & 88.8 & 88.6 & 83.3 & 43.5 & 45.3 & 46.2 & 52.9 & 57.7 \\ RIGA  & 65.1 & 64.7 & 64.5 & 64.1 & 64.8 & 85.1 & 85.0 & 85.1 & 84.3 & 85.1 & 32.1 & 33.4 & 34.3 & 34.5 & 34.6 \\ REGTR registration is considered correct if the root mean square error (RMSE) is under 0.2m. We also report the feature matching recall (FMR) and inlier ratio (IR). IR is defined as the fraction of point pairs' distance less than 0.1m under correct transformation and FMR is the fraction of point pairs' IR larger than 5%. Relative Rotation Error (RRE) and Relative Translation Error (RTE) are the metrics to measure the difference between the predicted transformation and the ground-truth transformation.

Comparisons to the state-of-the-art methods.We compare our proposed HST to recent state-of-the-art methods including: FCGF , D3Feat , SpinNet , Predator , CoFiNet , YOHO , GeoTransformer (GeoTR) , RIGA , REGTR , OIF-PCR (OIF)  and RoITr , see in Tab. 1. We first report the RR, FMR, and IR results using the RANSAC  estimator under different numbers of sampled correspondences. Following , we run RANSAC for 50k iterations to estimate the final transformation. Following , we only report individual results of REGTR  not distinguishing the number of sampling points because it estimates the final pose based on all the superpoints instead of sampling. HST outperforms all the previous descriptor-based and end-to-end methods on both 3DMatch and 3DLoMatch. It surpasses the closest competitor by 1.1 pp and 1.7 pp respectively on RR, reflecting the superiority in actual high and low overlap alignment. When the correspondence number varies, HST consistently maintains a significant lead over the others on the boards. For FMR and IR, HST still shows outstanding performance. It reaches the best or second best in most data points, indicating its stability when confronting limited correspondences.

We further compare the registration results replacing RANSAC with other estimators including weighted SVD , local-to-global registration (LGR) , and Iterative LGR  in Tab. 2. In addition to RR, we further introduce RRE (\({}^{}\)), RTE (\(m\)), and time (\(s\)) to evaluate the estimated error and latency of the methods. First, when replacing with weighted SVD, all methods suffer severe performance degradation and some even fail to align while HST still achieved the best performance across all the metrics. When using LGR, HST consistently outperforms all the state-of-the-art methods by a large margin. It improves the previous best () by 1.7% on 3DMatch  and 3.6% on 3DLoMatch  while with the smallest RRE and RTE. The most recent advances  refine the registration in an iterative update way. We adapt HST to iterative registration built upon PEAL  with 3d overlap prior and utilize LGR multiple times to gradually refine the final transformation. The results demonstrate that HST achieving competitive performance although we have not optimized it for the multi-step pipeline, which once again proves the effectiveness of our method. The above results demonstrate that our method performs well even without RANSAC to filter out outliers in

   Model & Estimator &  &  & Time(\(s\)) \\  & & RR & RRE & RTE & RR & RRE & RTE & \\  Predator  & RANSAC-50k & 89.0 & 2.02 & 0.064 & 62.5 & 3.04 & 0.093 & 3.915 \\ CoFiNet  & RANSAC-50k & 89.3 & 2.44 & 0.067 & 67.5 & 5.44 & 0.155 & **1.746** \\ GeoTrans  & RANSAC-50k & 92.0 & 1.87 & 0.065 & 75.0 & 2.94 & 0.090 & 1.992 \\ OIF-PCR  & RANSAC-50k & 92.4 & 1.86 & 0.064 & 76.1 & 3.04 & 0.092 & - \\ RoReg  & RANSAC-50k & 92.9 & 1.84 & **0.063** & 70.3 & 3.09 & 0.093 & - \\ RoITr  & RANSAC-50k & 92.4 & 1.87 & **0.063** & 75.7 & 2.84 & 0.091 & 2.270 \\ HST (**Ours**) & RANSAC-50k & **93.5** & **1.83** & **0.063** & **77.8** & **2.80** & **0.088** & 2.160 \\  Predator  & weighted SVD & 50.0 & 3.89 & 0.122 & 6.4 & 10 & 1 & **0.073** \\ CoFiNet  & weighted SVD & 64.6 & 2.92 & 0.087 & 21.6 & 6.34 & 0.154 & 0.264 \\ GeoTrans  & weighted SVD & 86.5 & 2.13 & 0.070 & 59.9 & 3.88 & **0.105** & 0.245 \\ HST (**Ours**) & weighted SVD & **88.1** & **2.10** & **0.068** & **62.1** & **3.78** & **0.105** & 0.279 \\  CoFiNet  & LGR & 87.6 & 2.23 & 0.071 & 64.8 & 3.49 & 0.124 & 0.279 \\ GeoTrans  & LGR & 91.5 & 1.91 & 0.068 & 74.0 & 2.95 & 0.090 & **0.260** \\ Roltr  & LGR & 91.5 & 1.80 & 0.065 & 73.7 & 2.88 & 0.090 & 0.311 \\ HST (**Ours**) & LGR & **93.2** & **1.70** & **0.059** & **77.3** & **2.71** & **0.084** & 0.296 \\  PEAL-3d  & Iterative LGR & 94.1 & 1.75 & 0.061 & 78.8 & 2.80 & 0.087 & - \\ HST (**Ours**) & Iterative LGR & **94.4** & **1.72** & **0.061** & **79.9** & **2.72** & **0.084** & - \\   

Table 2: Registration results w/ and w/o RANSAC on 3DMatch and 3DLoMatch. The number of samples for RANSAC-50k, weighted SVD, and LGR are 5000, 250, and all respectively. The units for metrics are RR (%), RRE (\({}^{}\)), RTE (\(m\)), and time (\(s\)).

fine-level correspondences. It indicates that HST is capable of forming reliable and robust coarse matching sets, which effectively assist subsequent fine matching to achieve accurate registration.

Comparisons under extreme low overlap.We design a set of experiments targeting overlap ratios of less than 10% and compare the performance with GeoTransformer  to test the robustness of HST when facing extremely low overlap. However, the currently available preprocessed datasets, 3DMatch  and 3DLoMatch  ( > 30% and 10 - 30%, respectively), do not include samples with such extremely low overlap (< 10%). Therefore, we access the 3DMatch raw dataset  and collect a new set of point cloud pairs with overlap ratio under 10%, which we refer to as 3DExtremeLoMatch (3DExMatch) dataset, containing a total of 1,343 samples. We first test the model pre-trained on 3DMatch directly on 3DExMatch, and the results can be found in the left part of Tab. 3.

We then randomly divide the 3DExMatch into training, validation, and test sets with proportions of 60% (805 samples), 10% (134 samples), and 30% (404 samples), respectively. We fine-tune the pretrained GeoTransformer  and HST both for 3 epochs on the training set, and then evaluate them on the test set. The results can be found in right part of Tab. 3. Both results clearly demonstrate that HST maintains strong performance even under low overlap conditions, confirming the effectiveness of our method.

Comparisons to the outlier-rejection methods.We further compare HST with other state-of-the-art outlier-rejection methods to gain more insights into handling outliers. For fairness, we replace HST directly with GC-RANSAC , MAC , and FastMAC . Results on both 3DMatch and 3DLoMatch can be found in the following Tab. 4. All three methods, along with HST, showed performance improvements compared to the vanilla GeoTransformer, with HST demonstrating the most significant enhancement. However, on 3DLoMatch, the improvements from these three methods were less pronounced, and some even showed a potential negative impact. In contrast, HST maintained better robustness, highlighting its superior effectiveness over previous outlier rejection methods in scenarios with low overlap and high noise.

    &  &  \\  & RR(\%) & RRE(\({}^{}\)) & RTE(cm) & RR(\%) & RRE(\({}^{}\)) & RTE(cm) \\  vanilla  & 91.5 & 1.91 & 0.068 & 74.0 & 2.95 & 0.090 \\ GC-RANSAC  & 92.1 & 1.78 & 0.068 & 73.4 & 2.96 & 0.088 \\ MAC  & 92.2 & 1.99 & 0.067 & 74.4 & 2.85 & 0.086 \\ FastMAC  & 91.9 & 1.73 & 0.062 & 74.2 & 2.86 & 0.087 \\  HST (**Ours**) & **93.2** & **1.70** & **0.059** & **77.3** & **2.71** & **0.084** \\   

Table 4: Registration results compare with state-of-the-art outlier-rejection methods.

    &  &  &  &  \\  & & & RR(\%) & RRE(\({}^{}\)) & RTE(cm) & RR(\%) & RRE(\({}^{}\)) & RTE(cm) \\  GeoTrans & LGR & - & 29.8 & 3.83 & **0.110** & 53.8 & **3.73** & 0.107 \\ HST (**Ours**) & LGR & - & **32.5** & **3.78** & 0.111 & **59.0** & 3.79 & **0.106** \\  GeoTrans & RANSAC & 250 & 28.8 & 4.56 & 0.123 & 47.0 & **4.39** & 0.121 \\ HST (**Ours**) & RANSAC & 250 & **31.5** & **4.42** & **0.122** & **49.8** & 4.43 & **0.119** \\  GeoTrans & RANSAC & 500 & 30.5 & 4.25 & 0.121 & 49.9 & 4.55 & **0.113** \\ HST (**Ours**) & RANSAC & 500 & **32.5** & **4.16** & **0.118** & **52.6** & **4.03** & 0.117 \\  GeoTrans & RANSAC & 1000 & 31.7 & 4.23 & 0.118 & 52.4 & **4.03** & 0.117 \\ HST (**Ours**) & RANSAC & 1000 & **33.6** & **4.03** & **0.114** & **57.5** & 4.13 & **0.115** \\  GeoTrans & RANSAC & 2500 & 31.9 & 4.09 & 0.116 & 55.1 & **3.89** & 0.116 \\ HST (**Ours**) & RANSAC & 2500 & **34.7** & **3.81** & **0.111** & **59.0** & **3.89** & **0.112** \\  GeoTrans & RANSAC & 5000 & 31.0 & 4.04 & 0.117 & 56.1 & 4.07 & 0.116 \\ HST (**Ours**) & RANSAC & 5000 & **34.4** & **3.87** & **0.114** & **60.3** & **3.93** & **0.111** \\   

Table 3: Comparisons to the baseline w/o and w/ fine-tuning on 3DExMatch using different estimators.

### Outdoor KITTI Odometry

Metrics.Following [6; 13], 3 metrics are adopted to evaluate our methods: Relative Rotation Error (RRE), Relative Translation Error (RTE), and Registration Recall (RR).

Registration Results.We compare our method with recent state-of-the-art methods in Tab. 5, including FCGF , D3Feat , SpinNet , Predator , CoFiNet , GeoTR , OIF , and PEAL . Our method outperforms all the baselines for all metrics. It indicates that HST is effective in handling both indoor and outdoor scenes.

### Ablation Study

Importance of individual modules.Tab. 6 shows the results of the ablation studies of each component on 3DMatch and 3DLoMatch. We first replace our proposed overlap-aware Sinkhorn Distance with vanilla Sinkhorn Distance, i.e., removing overlap filtering and overlap-aware initialization. Results indicate that all the metrics decrease significantly, proving that HST benefits from our designed scheme. Then we ablate these two components and the POP module individually. Removing either will cause severe performance degradation, indicating each component is beneficial for patch difference measurement. Finally, we ablate the depth of HST, i.e., the number of scales used. The results show that performance degrades as the depth reduces, with optimal performance achieved when all scales are explored. This confirms that robust coarse matching relies on accurate MSC.

Robustness studyFig. 5 shows the results of adding zero-mean Gaussian noise with a standard deviation of 0.01 to 3DLoMatch  dataset, and gradually increasing the proportion of noise points to test the robustness of the model. It demonstrates that HST maintains the most stable performance compared to [13; 12], indicating its superior noise resistance.

## 5 Conclusion

In the paper, we present a simple but effective Hierarchical Sinkhorn Tree (HST) to model the multiscale geometric consistency for robust point cloud registration. We hierarchically explore the neighborhoods of each correspondence in their feature pyramids, and devise a novel overlap-aware Sinkhorn Distance to compute the vicinity similarity. Subsequently, the most likely overlapping points are retained to continue local exploration. The modeling process essentially involves a BFS traversal of a k-ary tree rooted at the coarse-level point. Pruning is performed during traversal using the overlap-aware Sinkhorn distance to obtain subtrees, which is the so-called HST. Extensive experiments show HST consistently outperforms the state-of-the-art methods on both indoor and outdoor benchmarks.

   Model &  &  & Time(s) \\  & IR(\%) & FMR(\%) & RR(\%) & IR(\%) & FMR(\%) & RR(\%) & \\ 
**Full (\# Depth=2, all scales)** & **75.9** & **98.8** & **93.5** & **41.7** & **88.8** & **77.8** & **2.160** \\  vanilla SD & 66.0 & 98.1 & 90.4 & 37.9 & 86.4 & 73.3 & - \\ w/o Overlap Filtering & 70.5 & 98.1 & 91.6 & 39.7 & 86.5 & 75.1 & - \\ w/o Overlap-aware Initial & 71.1 & 98.6 & 92.9 & 39.3 & 88.4 & 77.0 & - \\ w/o Patch Overlap Pred & 69.8 & 98.2 & 92.0 & 39.4 & 86.8 & 75.6 & - \\ HST \# Depth=1 & 74.3 & 98.2 & 92.8 & 40.5 & 88.1 & 76.8 & 2.075 \\ HST \# Depth=0 & 71.0 & 98.1 & 92.4 & 39.9 & 87.5 & 74.6 & 2.039 \\   

Table 6: Ablation study for each component. Tested with RANSAC # Samples=5000.

Figure 5: Details of robustness study.

   Method & RRE(\({}^{}\))\(\) & RTE(cm)\(\) & RR(\%)\(\) \\  FCGF  & 0.30 & 9.5 & 96.6 \\ D3Feat  & 0.30 & 7.2 & **99.8** \\ SpinNet  & 0.47 & 9.9 & 99.1 \\ Predator  & 0.28 & 6.8 & **99.8** \\ CoFiNet  & 0.41 & 8.2 & **99.8** \\ GeoTR  & 0.24 & 6.8 & **99.8** \\ OIF  & **0.23** & 6.5 & **99.8** \\ PEAL  & **0.23** & 6.8 & **99.8** \\ HST (**Ours**) & **0.23** & **6.3** & **99.8** \\   

Table 5: Registration results on KITTI odometry.