# Reconciling Competing Sampling Strategies of Network Embedding

Yuchen Yan, Baoyu Jing, Lihui Liu, Ruijie Wang, Jinning Li,

**Tarek Abdelzaher, Hanghang Tong**

University of Illinois at Urbana Champaign, IL, USA

{yucheny5, baoyuj2, lihuil2, ruijiew2, jinning4, zaher, htong}@illinois.edu

###### Abstract

Network embedding plays a significant role in a variety of applications. To capture the topology of the network, most of the existing network embedding algorithms follow a _sampling_ training procedure, which maximizes the similarity (e.g., embedding vectors' dot product) between positively sampled node pairs and minimizes the similarity between negatively sampled node pairs in the embedding space. Typically, close node pairs function as positive samples while distant node pairs are usually considered as negative samples. However, under different or even competing sampling strategies, some methods champion sampling distant node pairs as positive samples to encapsulate longer distance information in link prediction, whereas others advocate adding close nodes into the negative sample set to boost the performance of node recommendation. In this paper, we seek to understand the intrinsic relationships between these competing strategies. To this end, we identify two properties (_discrimination_ and _monotonicity_) that given any node pair proximity distribution, node embeddings should embrace. Moreover, we quantify the empirical error of the trained similarity score w.r.t. the sampling strategy, which leads to an important finding that the _discrimination_ property and the _monotonicity_ property for _all_ node pairs can not be satisfied simultaneously in real-world applications. Guided by such analysis, a simple yet novel model (Sensei) is proposed, which seamlessly fulfills the _discrimination_ property and the _partial monotonicity_ within the top-\(K\) ranking list. Extensive experiments show that Sensei outperforms the state-of-the-arts in plain network embedding.

## 1 Introduction

In the era of big data, network embedding [36; 14; 21; 7; 42] maps nodes in the network to low-dimensional vectors in the embedding space, which plays an important role in many tasks such as node recommendation [54; 61; 38; 8; 20; 19; 1; 2], networked time series imputation [44; 18; 48; 9], knowledge graph completion [49; 46; 47; 45; 28], and network alignment [52; 53; 60; 62; 16; 27; 59]. To distill the topology information of the network, most existing network embedding methods follow a _sampling_ training procedure. Given any central node to be considered, existing network embedding methods build a positively sampled node pair set and a negatively sampled node pair set. Then, they optimize the embeddings of nodes by maximizing/minimizing the similarity between the positively/negatively sampled node pairs in the embedding space. Explicitly or implicitly, these methods are based on an assumption that nodes close to the central node should be included in the positively sampled node pair set, whereas distant nodes are likely to be considered as negative samples. For example, DeepWalk  and metapath2vec  explicitly construct random walk containing positively sampled nodes and select negative nodes according to the degree distribution of the network. The message-passing mechanism in GraphSAGE  and graph auto-encoder (GAE)  is built on graph Laplacian regularization [21; 57], which makes connected node pairs to besimilar. In this case, the close/distant nodes become implicit positive/negative samples for the central node, which can be regarded as a general form of such sampling training procedure.

Numerous network embedding algorithms have been proposed, which focus on improving either the positive sampling strategy or the negative sampling strategy based on various intuitions. Consequentially, disparate or even competing sampling strategies emerge in various methods. To name a few, rather than solely favor nodes within two hops as positive samples in LINE , node2vec  allows a longer random walk length. For graph convolution network (GCN) , APPNP  utilizes personalized pagerank  to sample more distant nodes w.r.t the central node for feature aggregation. The aforementioned two algorithms accommodate farther nodes in the positively sampled node pair set. Meanwhile, SPNE , KBGAN  and RecNS  encourage choosing closer nodes as _difficult_ negative samples to promote the performance of node recommendation.

Thus, some fundamental questions arise: _what is the theoretic root cause behind such antithetical sampling strategies? what are possible and what are impossible for sampling of network embedding? how can we develop a practical embedding algorithm that simultaneously embraces these competing strategies?_

In this paper, we hammer at bringing the intrinsic relationships behind these competing sampling strategies in light. Concretely, we start from two fundamental tasks of graph learning: the _link prediction_ task and the _node recommendation_ task. To tackle the above two tasks synchronously, we identify two desirable properties, including the _discrimination_ property and the _monotonicity_ property, that network embedding vectors should satisfy given a node pair proximity distribution. The _discrimination_ property means that the node pair with high proximity should be distinguished from the node pair owning low proximity in the embedding space (_link prediction_). For the _monotonicity_ property, the ranking list of nodes recommended to the query node needs to be consistent with the proximity list in the descending order. Theoretically, we analyze the general form of network embedding algorithms' loss functions. We show that the negative sampling distribution should be negatively correlated with the node pair proximity distribution. Furthermore, we show that in the ideal case where the algorithm can sample an infinite number1 of positive nodes for the central node, both the _discrimination_ property and the _monotonicity_ property can be fulfilled (i.e., possibility result). However, due to the limited sample size of the real-world data, there exists an inevitable error between the ideal optimal similarity scores and the empirical optimal similarity scores in the embedding space. Regardless of the specific sampling strategy, the _discrimination_ property and the _monotonicity_ property for _all_ node pairs in the network can not be fulfilled at the same time (i.e., impossibility result).

Fortunately, in real-world applications, ranking all nodes in the network for the query node is often unnecessary. In many cases, only the top-\(K\) recommendation list and its internal order matters, which suggests that we can first achieve the _discrimination_ property to detect candidate nodes to be recommended. After that, we can attain the _monotonicity_ property within the top-\(K\) ranking list (_partial monotonicity_). Guided by this intuition and the theoretical results, we propose a simple yet novel model (Sensei). In detail, Sensei adopts a commonly used proximity measurement (personalized pagerank ) and decomposes the embedding process into two steps. The first step is to satisfy the _discrimination_ property. In addition to sampling nodes with large proximity as positive samples, Sensei also includes nodes with intermediate proximity in the positively sampled node pair set, which reduces the empirical error of the similarity scores for these nodes. Then, in the second step, Sensei pays attention to the _monotonicity_ property within the positively sampled node pairs from the first step. Nodes are ranked w.r.t. their proximities and importantly, some positively sampled nodes are turned into negative samples, which resembles the strategy of selecting _difficult_ negative samples in some existing methods . In this way, Sensei creatively integrates these two competing sampling strategies in one integral framework. The experiments demonstrate that Sensei greatly outperforms various baselines.

To summarize, our contributions are three folds:

* **Theoretical Analysis.** We reveal the underlying relationships between the competing sampling strategies of existing network embedding methods. Specially, we prove that any sampling strategy bears an inevitable error gap between the empirical and ideal optimal embedding similarity scores.
* **Simple yet Novel Model.** Based on the theoretical results, we propose a two-step model Sensei which creatively integrates competing sampling strategies in one network embedding framework. It satisfies the _discrimination_ property in the first step and obtains the _partial monotonicity_ property within the positively sampled node pair set in the second step.
* **Experimental Results.** Extensive experiments show that Sensei outperforms the state-of-the-arts in plain network embedding.

Problem setting.We primarily focus on plain network embedding, in which given the adjacency matrix of a plain network \(\), we aim to output the embedding matrix \(\).

## 2 Analysis

In this section, we uncover the intrinsic relationships of existing competing sampling strategies. First, we describe the sampling-based network embedding process mathematically. Then, we propose two desired properties for the learned embeddings: the _discrimination_ property and the _monotonicity_ property, which correspond to two common learning tasks: _link prediction_ and _node recommendation_. Theoretically, we analyze the general form of network embedding algorithms' loss functions. We show that the negative sampling distribution should be _negatively_ correlated with the given node pair proximity distribution. Guided by this critical insight, we give both possibility and impossibility results for network embedding. First (possibility result), we show that, in the ideal case where an infinite number of positive samples can be obtained, both the _discrimination_ property and the _monotonicity_ property can be satisfied with the optimal solution of the loss function. Second (impossibility result), for the empirical loss function with a limited number of positive samples, we prove that there always exists an error gap between the empirical optimal solution and the ideal optimal solution. Regardless of the specific sampling strategy, the _discrimination_ property and the _monotonicity_ property can not be simultaneously satisfied for _all_ node pairs in the network in the empirical situation.

Current sampling-based network embedding algorithms can be divided into two phases. The first phase is to construct a node pair proximity distribution \(p(u|v)\), which serves as the positive sampling distribution in the training process. For example, personalized pagerank  in APPNP  is an explicit node pair proximity distribution, while random walk in DeepWalk  and node2vec  implicitly constructs such node pair proximity distribution. The second phase is to design a negative sampling distribution \(p_{n}(u|v)\) and to utilize the network embedding loss function to train the embedding model. Similar to previous works [55; 56], we start from analyzing the second phase of the sampling-based network embedding process as follows:

**Definition 1**.: _Sampling-based Network Embedding Process in the Second Phase. Given a node proximity distribution \(p\), where \(p(u|v)\) refers to the proximity of node \(u\) w.r.t. the central node \(v\) and \(_{u}p(u|v)=1\), the algorithm designs a negative sampling distribution \(p_{n}\) and adopts a loss function \(J\) to obtain similarity scores \(s(u,v)\) of all node pairs \((u,v)\), which are calculated by the node embedding matrix \(\)._

The similarity score \(s(u,v)\) defined here is general, which can be implemented with various similarity measurements (e.g., the sigmoid function \(((u,:)^{}(v,:))\) or the cosine function \(cos((u,:),(v,:))\)) in various algorithms.

Then, to solve both link prediction and node recommendation tasks, we propose that the general similarity score \(s(u,v)\) calculated by node embeddings should possess two properties: the _discrimination_ property and the _monotonicity_ property, which are defined as the following:

**Definition 2**.: _Discrimination and Monotonicity._

_Discrimination Property: Given the central node \(v\), node pair \((u,v)\) with large \(p(u|v)\) should be clearly distinguished with node pair \((w,v)\) with small \(p(w|v)\) in the embedding space, i.e., \(=+\) when \( 0\)._

_Monotonicity Property: Given the central node \(v\) and two arbitrary nodes \(u\), \(w\) in the network, if \(p(u|v)>p(w|v)\), then \(s(u,v)>s(w,v)\)._The _discrimination_ property bears subtle difference from the _monotonicity_ property. Let us look at an illustrative example in the first line in Figure 1. For the central node \(v\), we assume that \(p(u_{1}|v)=0.45\), \(p(u_{2}|v)=0.4\) and \(p(w|v)=0.001\). _Case 1_: If \(s(u_{1},v)=0.7\), \(s(u_{2},v)=0.8\) and \(s(w,v)=0.01\), the final node embeddings satisfy the _discrimination_ property (\(,v)}=0.0143\) and \(,v)}=0.0125\), both approaching zero) but do not satisfy the _monotonicity_ property (\(p(u_{1}|v)>p(u_{2}|v)\) but \(s(u_{1},v)<s(u_{2},v)\)). _Case 2_: If \(s(u_{1},v)=0.51\), \(s(u_{2},v)=0.50\) and \(s(w,v)=0.49\), the final node embeddings fulfill the _monotonicity_ property (\(s(u_{1},v)>s(u_{2},v)>s(w,v)\)) but do not satisfy the _discrimination_ property (\(,v)}{s(w,v)}=1.04\) and \(,v)}{s(w,v)}=1.02\)). From the above example, we can see that the _discrimination_ property focuses on making node pairs with large proximity closer and pushing away node pairs with small proximity in the embedding space, which is desirable for tasks like binary classification for link prediction (i.e., predicting the existence of a link between two nodes). On the other hand, the _monotonicity_ property pays more attention to the rank of the similarity scores of node pairs, which is critical for tasks like ranking for node recommendation.

### Possibility Results in the Ideal Case

We conduct theoretic analysis about the loss function of network embedding. Previous works [24; 37; 55] have demonstrated that most existing network embedding algorithms can be regarded as implicit matrix factorization and there exists a general form of loss function for the given central node \(v\). If the sigmoid function is adopted as the similarity function. The loss function is:

\[J=-_{u p(u|v)}((u,:)^{}(v,:)) -k_{w p_{n}(w|v)}(1-((w,:)^{} (v,:)))\] (1)

where \(()\) is the sigmoid function, \((u,:),(v,:)\) and \((w,:)\) are the embeddings of \(u,v\) and \(w\) respectively, and \(k\) is the number of negative samples for each positive sample. If the sigmoid function is replaced by other similarity measurements, we can obtain a more general form of this loss function:

\[J=-_{u p(u|v)}(s(u,v))-k_{w p_{n}(w|v)}(1- s(w,v))\] (2)

where \(s(,)\) is a general form of similarity measurement.

In the ideal case where an infinite number of positive samples from \(p(u|v)\) are available, we have the following theorem about the optimal solution of \(s\):

**Theorem 1**.: _Optimal Solution of \(s\). The optimal solution of the similarity function \(s\) satisfies that for each node pair \((u,v)\),_

\[s(u,v)=(u|v)}\] (3)

Figure 1: _Discrimination_ and _monotonicity_ properties. We use star nodes to denote nodes with large \(p(u|v)\), round nodes to denote nodes with intermediate \(p(u|v)\) and rectangle nodes to denote nodes with small \(p(u|v)\). We assume that \(p(u_{1}|v)>p(u_{2}|v)>>p(u_{10}|v)\).

Proof.: \[J=-_{u p(u|v)} s(u,v)-k_{w p_{n}(u|v)}(1-s(w,v))\] (4)

Since \(s(u,v)\) is the parameter to be optimized, we can calculate \(_{s(u,v)}J\) as following:

\[_{s(u,v)}J=-p(u|v)-kp_{n}(u|v)\] (5)

Let the derivative to be \(0\), we can obtain \(s(u,v)=(u|v)}\), which is the optimal solution for \(J\). 

With the optimal solution of \(s(u,v)\), we give the following proposition about the design of \(p_{n}\):

**Proposition 1**.: _The Design of Negative Sampling. In the sampling-based network embedding process, the negative sampling distribution \(p_{n}\) should be **negatively** correlated with the given node pair proximity distribution \(p\) to fulfill the monotonicity property._

Proof.: Here we prove the above proposition in the ideal case by contradiction: In the ideal case, assume that the best embeddings and corresponding similarity scores to satisfy the _discrimination_ property and the _monotonicity_ property are obtained with \(p_{n}\), and \(p_{n}\) is not negatively correlated with \(p\). It is equivalent to the situation that for one central node \(v\), there must exist two nodes \(u\), \(w\) with \(p(u|v)>p(w|v)\) and \(p_{n}(u|v)>p_{n}(w|v)\). From Theorem 1 we know that \(s(u,v)=(u|v)}\) and \(s(w,v)=(w|v)}\). Now, we interchange \(p_{n}(u|v)\) with \(p_{n}(w|v)\) and we can get new similarity scores \(s^{}(u,v)\) and \(s^{}(w,v)\) for \((u,v)\) and \((w,v)\). \(s^{}(u,v)=(w|v)}>( u|v)}=s(u,v)\). In addition, \(s^{}(w,v)=(u|v)}\)\(<(u|v)}=s(w,v)\). So, we get two new similarity scores that can satisfy the _monotonicity_ property better, which is contradictory with the assumption that we have already achieved the best embeddings and corresponding similarity scores. 

Intuitively, this proposition makes sense because if one node is more likely to be sampled as a positive sample for the central node, it is less likely that this node also acts as a negative sample.

From Theorem 1, we can see that if \(p_{n}\) is negatively correlated with \(p\), when \(p(u|v)\) is large and \(p_{n}(u|v)\) is small, \(s(u,v) 1\); and when \(p(u|v)\) is small and \(p_{n}(u|v)\) is large, \(s(u,v) 0\), which satisfies the _discrimination_ property. For all nodes in the network with \(p(u_{1}|v)>p(u_{2}|v)>>p(u_{n}|v)\), if we set \(p_{n}(u_{1}|v)<p_{n}(u_{2}|v)<<p_{n}(u_{n}|v)\), the _monotonicity_ property can be fulfilled with \(s(u_{1},v)>s(u_{2},v)>>s(u_{n},v)\).

### Impossibility Results in the Empirical Case

However, in real-world applications, we can _not_ sample an infinite number of nodes from \(p(u|v)\). Empirically, existing algorithms set a fixed number \(T\) of positive samples for the central node and the general loss function in Eq. (2) turns into:

\[J_{e}=-_{i=1}^{T}(s(u_{i},v))-_{i=1}^{kT} (1-s(w_{i},v))\] (6)

where \(u_{i}\) is a positive sample from \(p(u|v)\) and \(w_{i}\) is a negative sample from \(p_{n}(w|v)\). We use \(S=[s(u_{1},v),s(u_{2},v),,s(u_{n},v)]\) to denote the final similarity scores to be optimized w.r.t. the central node \(v\). For the general loss function in the ideal case, the optimal solution of \(S\) is \(S^{*}\), where \(S^{*}=[|v)}{p(u_{1}|v)+kp_{n}(u_{1}|v)},,|v)}{ p(u_{n}|v)+kp_{n}(u_{n}|v)}]\). For the loss function \(J_{e}\) in the empirical case with \(T\) positive samples from \(p\), we represent the optimal solution as \(S_{e}\). For the mean squared error between \(S^{*}\) and \(S_{e}\), we have the following theorem:

**Theorem 2**.: _Mean Squared Error. For the mean squared error between the empirical optimal solution \(S_{e}\) and the ideal solution \(S^{*}\), we have_

\[[||(S_{e}-S^{*})_{u}||^{2}]=(u|v)}{p(u|v)} +(u|v)})^{2}}(+(u|v)}-1- ),\] (7)

_where \(\) is the expectation and \((S_{e}-S^{*})_{u}=s_{e}(u,v)-s^{*}(u,v)\) is the error between the optimal similarity scores obtained in the empirical and ideal cases respectively._The proof for Theorem 2 is attached in Appendix A.1. If the sigmoid function is adopted as the similarity function, the expectation of the similarity error in Theorem 2 degenerates to its special case:

\[[||_{e}(u,:)^{}_{e}(v,:)-^{*}(u,:)^{ }^{*}(v,:)||^{2}]=(+(u |v)}-1-),\] (8)

where \(_{e}(u,:)\) and \(^{*}(u,:)\) are the optimal embedding vectors of node \(u\) in \(J_{e}\) and \(J\). Eq. (8) was first discovered in MCNS . Based on Eq. (8), MCNS advocates that, in order to bound the dot product error for nodes with large \(p(u|v)\), \(p_{n}\) should be _positively_ correlated with \(p\). Unfortunately, this is not always the best choice due to the following reason. In MCNS, the authors set \(p_{n}(u|v) p(u|v)^{a}\), where \(0<a<1\). With this negative sampling distribution, the right part of Eq. (8) changes into \(((1+}{c})-1-)\), where \(c\) is a constant. If \(p(u|v)\) is large, this term can indeed be bounded. However, for distant nodes with small \(p(u|v)\) (i.e., \(p(u|v) 0\)), its positively correlated negative sampling probability \(p_{n}(u|v)\) is also very small and \(p_{n}(u|v) 0\), which leads to an extremely large error considering that the term \(+(u|v)}\), which may fail to fulfill the _discrimination_ property.

_Discussion_. Let us analyze the implications of Theorem 2. If \(p_{n}(u|v)\) is negatively correlated with \(p(u|v)\), it means that \((u|v)}{p(u|v)} 0\) for nodes with the largest \(p(u|v)\)s and \((u|v)}{p(u|v)}\) for nodes with the smallest \(p(u|v)\)s. Let us consider the case that \(p(u|v)\) is large and \((u|v)}{p(u|v)} 0\). \([||(S_{e}-S^{*})_{u}||^{2}]<)^{2}}( +(u|v)}-1-)<( (u|v)}{p(u|v)})^{2}(+(u|v)})\). It can be rewritten as \((((u|v)}{p(u|v)})^{2}+(u|v )}{p(u|v)})\). Since \(p(u|v)\) is among the largest proximity scores, \(\) is approximately bounded by a finite number \(\), where \(n\) is the number of nodes in the network. As a result, \((u|v)}{p(u|v)} 0\) and \(((u|v)}{p(u|v)})^{2} 0\). Therefore, for nodes with large \(p(u|v)\), the empirical embedding similarity score \(s_{e}(u,v) s^{*}(u,v)\). With a similar analysis on nodes with small \(p(u|v)\), we reach the same conclusion that \(s_{e}(u,v) s^{*}(u,v)\). Based on the above analysis, we can see that the empirical optimal solution can achieve the _discrimination_ property with a negatively correlated \(p_{n}\): for node pairs with large/small \(p(u|v)\), their similarity scores approach the ideal optimal solution. However, there might exist some nodes with \(p(u|v) kp_{n}(u|v)\), which are referred to as nodes with _intermediate_\(p(u|v)\). For these nodes, the error in Theorem 2 is larger than nodes with small/large \(p(u|v)\) because \((2+(u|v)}{p(u|v)}+(u|v)})^{2}\) achieves its minimum when \(p(u|v)=kp_{n}(u|v)\). This can also be explained intuitively as follows. Nodes with large \(p(u|v)\) and large \(p_{n}(u|v)\) (small \(p(u|v)\)) are always sampled as positive samples and negative samples, while nodes with intermediate \(p(u|v)\) and \(p_{n}(u|v)\) might be ignored and are less likely to be sampled as positive samples or negative samples. As such, the embedding similarity scores for these nodes bear more uncertainty and large error between the empirical optimal solution and the ideal optimal solution (Theorem 2).

With these findings, we can now unveil the underlying reason for the competing sampling strategies in existing methods. The first category of methods (e.g., [36; 14; 23]) champion sampling nodes with longer distance from the given central node as positive samples. The essence of this positive sampling strategy is to include more nodes with intermediate \(p(u|v)\) in the positively sampled node pair set. In this way, these methods realize the goal to minimize the error in Theorem 2 for these nodes with intermediate \(p(u|v)\). However, this design has cost. When choosing more nodes with intermediate \(p(u|v)\) as positive samples, it implicitly decreases the \(p(u|v)\) for nodes with large \(p(u|v)\) (since \(_{u}p(u|v)=1\)). As shown on the second line in Figure 1, it hurts the _monotonicity_ between these nodes (round nodes) and nodes with large \(p(u|v)\) (star nodes). On the contrary, for the second category of methods (e.g., [4; 55; 56]), they prefer to select nodes with intermediate \(p(u|v)\) as _difficult_ negative samples in the recommendation task, they successfully fulfill the _monotonicity_ property between nodes with large \(p(u|v)\) (star nodes) and nodes with intermediate \(p(u|v)\) (round nodes). Unfortunately, as our discussion about Eq. (8) has demonstrated, this _positively_ correlated negative sampling distribution could make the empirical error of the embedding similarity scores of nodes with small \(p(u|v)\) quite large. It does harm the _discrimination_ property, which is shown on the last line in Figure 1 (e.g., \(s(u_{8},v)\)).

To conclude, due to the limited number \(T\) of positive samples, the empirical loss tends to sample nodes with large \(p(u|v)\) or \(p_{n}(u|v)\) and there always exist some nodes that can not be sampled as positive/negative samples and thus are ignored. Therefore, the _discrimination_ property and the _monotonicity_ property for _all_ node pairs in the network can not be satisfied simultaneously in the empirical situation.

## 3 Model

In many real-world applications, given the query node, we primarily care about the top-\(K\) recommendation list. This suggests that we only need to satisfy the _discrimination_ property and the _partial monotonicity_ property within nodes that are likely to appear in the top-\(K\) recommendation list. Guided by this and the theoretical results in Section 2, we propose a simple yet novel model named Sensei.

**Key Idea.** The key idea of Sensei is to seamlessly integrate two competing sampling strategies into one model together. Concretely, it means that we can decompose the proposed Sensei into two steps. The first step is to satisfy the _discrimination_ property and the second step is to achieve the _monotonicity_ property within nodes that are likely to appear in the top-\(K\) recommendation list. As shown in Figure 2, in the first step, Sensei samples nodes with intermediate \(p(u|v)\), which is the strategy by the first category of methods mentioned in Section 2. Sensei constructs a _combined_ positive sample set, including nodes with large \(p(u|v)\) and nodes with intermediate \(p(u|v)\). During the training process, Sensei treats these nodes equivalently as positive samples and maximizes their similarity scores with the given central node. At the same time, it minimizes the similarity scores for nodes with small \(p(u|v)\) (large \(p_{n}(u|v)\)) to fulfill the _discrimination_ property. In the second step, Sensei focuses on the _difficult_ negative samples, which is the strategy by the second category of methods in Section 2. Sensei pays attention to the _monotonicity_ property within the _combined_ positive sample set in the first step. In detail, the previously sampled positive nodes have two roles: they act as the positive samples compared to nodes with smaller \(p(u|v)\) in the first step and meanwhile function as the _difficult_ negative samples compared to nodes with larger \(p(u|v)\) in the second step.

**Details.** For \(p(u|v)\), we run personalized pagerank , which is used throughout Sensei. In addition, we normalize node embeddings to have unit L2 norm and adopt the dot product (\((u,)^{}(v,)\)) as the similarity function \(s(u,v)\).

**Step 1: Fulfill the _Discrimination_ Property.** In this step, to satisfy the _discrimination_ property, we construct a _combined_ positive sample set \((v)\) for \(v\) in \(\), which includes nodes with large and intermediate \(p(u|v)\). Specifically, we rank the proximity \(p(u|v)\) for all nodes in \(\) to obtain a descending list \([p(u_{1}|v),p(u_{2}|v),,p(u_{n}|v)]\). Then, we set a threshold \(\). Nodes with \(p(u|v)>\) form the _combined_ positive sample set \((v)\) for the central node \(v\) and the remaining nodes are added into the set \((v)\), where negative samples will be randomly selected. So, the loss for node \(v\) is:

\[J(v)=-(v)|}(_{u(v)}(k(u, )^{}(v,)-_{m=1}^{m=k}(w_{m},)^{ }(v,)))\] (9)

where \(k\) is the negative sample number for each positive sample, \(u\) is the positively sampled node for \(v\) and \(w_{m}(v)\) is the negatively sampled node. By adding the loss for all \(v\)s in \(\), we obtain the

Figure 2: **The training process of Sensei.** Similar to Figure 1, we still utilize star nodes to denote nodes with large \(p(u|v)\), round nodes to denote nodes with intermediate \(p(u|v)\) and rectangle nodes to denote nodes with small \(p(u|v)\). In Step 1, Sensei includes round nodes (nodes with intermediate \(p(u|v)\)) as positive samples. In Step 2, the round nodes are used as hard negative samples for the star nodes.

overall loss in Step 1 as:

\[J_{P}=_{v}J(v)\] (10)

**Step 2: Fulfilil the _Partial Monotonicity_ Property.** In this step, we focus on the _partial monotonicity_ property within the _combined_ positive sample set in Step 1. For any two nodes \(u_{l}\) and \(u_{m}\) in the positive sample set \((v)\), if \(p(u_{l}|v)>p(u_{m}|v)\), we want to retain the _monotonicity_ between them with \((u_{l},:)^{}(v,:)>(u_{m},:)^{} (v,:)\). Node \(u_{m}\) changes from a positive sample for \(v\) in Step 1 to a negative sample for \(v\) compared with \(u_{l}\), which is consistent with the idea of sampling _difficult_ negative samples. The margin-based ranking loss is:

\[L(v)=_{p(u_{l}|v)>p(u_{m}|v)}(\{(u_{m},:)^{} (v,:)-(u_{l},:)^{}(v,:)+,0\})\] (11)

where \(u_{l},u_{m}(v)\), \(_{p(u_{l}|v)>p(u_{m}|v)}\) is the indicator function and \(\) is a positive margin. When \(p(u_{l}|v)>p(u_{m}|v)\), \(_{p(u_{l}|v)>p(u_{m}|v)}=1\), otherwise, \(_{p(u_{l}|v)>p(u_{m}|v)}=0\). Similar to Eq. (10) in Step 1, we have the loss \(L_{p}\) in Step 2 as:

\[L_{P}=_{v}L(v).\] (12)

In fact, Step 2 can be regarded as a fine-tuning process to fulfill the _partial monotonicity_ property. Therefore, this step usually has a small learning rate and a small positive margin \(\). The pseudocode of this algorithm can be found in Appendix A.2.

**Complexity Analysis.** We give a complexity analysis of Sensei here. For Sensei, the time complexity can be analyzed as the following: (1) Calculating personalized pagerank has the complexity \(O(_{}\|\|)\), where \(\|\|\) is the number of edges in the graph and \(_{}\) is the maximum iterations or hops for personalized pagerank; (2) the time complexity for sorting the proximity distribution \(p(u|v)\) and sampling positive/negative nodes for node \(v\) is \(O(n(n))\); (3) Calculating \(J(v)\) takes \(O(\|(v)\| k d)\), where \(\|(v)\|\) is the positive sample set, \(k\) is the number of negative samples and \(d\) is the dimension of node embedding. Obtaining \(J_{P}\) takes \(O(n)\) time; (4) Computing \(L(v)\) in Eq. (11) takes \(O(\|(v)\|^{2} d)\) and calculating \(L_{p}\) costs \(O(n)\). Most of these computations can be further parallelized.

## 4 Experiment

In this section, we evaluate the effectiveness of the proposed algorithm (Sensei) for solving link prediction and node recommendation simultaneously in plain networks.

### Experimental Setup

**Datasets.** We use 4 public real-world datasets to evaluate the proposed Sensei model: C.ele , Cora , Citeseer , NS .

**Baselines.** We compare the proposed Sensei with the following 5 plain network embedding methods: node2vec , VGAE , GAT , ARGVA  and RBGE .

**Metrics.** For link prediction and node recommendation in plain networks, we randomly split edges in every dataset into 70/10/20% for training, validation, and test.2 The same amount of additionally sampled non-existent edges are taken as negative edges for training, validation and test in the link prediction task.

**Settings.** For the link prediction task in plain networks, we use Area Under the ROC and Precision-Recall Curves (i.e., AUC-ROC and AUC-PR) as the metrics to evaluate the performance of different methods. For the node recommendation task in plain networks, we take Hit@\(K\) as the metric. Only when the model ranks the correct node as top-\(K\) in the recommendation list, it is counted as a hit for this query node in the test set. Therefore, the average hit number for all query nodes is Hit@\(K\). We set \(K=10\) for Sensei.

**Additional Contents.** In Appendix, we provide more contents related to experiments, including (1) dataset statistics (Appendix A.3); (2) implementation details (Appendix A.3); (3) the comparison with additional GCN-based methods: GraphSAGE , Cluster-gcn , APPNP , GPRNN  and H2GCN  (Appendix A.4).

### Effectiveness of Sensei

The results of link prediction on all 4 plain networks are presented in Table 1. Our proposed Sensei generally outperforms all baselines on all 4 datasets. In particular, Sensei achieves about 2% improvement in AUC-PR compared to the best competitor (e.g., VGAE on Citeseer and node2vec on C.ele). For the node recommendation task, as shown in Table 2, node2vec obtains the best Hit@10 on Cora and NS among all baselines, which indicates that including more positive samples (\(T\)) by setting a larger walk length (80) can indeed reduce the error in Theorem 2. Notice that the proposed Sensei still outperforms node2vec on all 4 datasets, which demonstrates the ability of Sensei to fulfill the _partial monotonicity property_.

### Ablation Study and Sensitivity Study

In this subsection, we conduct the ablation study to validate the importance of Step 2 of Sensei. As illustrated in Table 2, for the node recommendation task (Hit@10), Step 2 consistently improves the performance of Sensei. The results verify that Step 2 is indeed beneficial by fine-tuning node embeddings to satisfy the _partial monotonicity_ property. In addition, we carry out a sensitivity study on different \(K\)s for Hit@\(K\), which is presented in Figure 3. We observe that when \(K\) increases from 1 to 30, Hit@\(K\) gradually increases and Hit@30 is much higher than Hit@1, which is expected.

## 5 Related works

**Network Embeddings.** Network embedding maps nodes in the network to low dimensional vectors. It can be traced back to matrix low rank approximation  and spectral clustering . DeepWalk  and node2vec  rely on random walk to encode the topological information of homogeneous networks. For multi-layered networks or heterogeneous networks, metapath2vec  and Hin2vec  design metapaths to capture the connectivity between different layers or different types of nodes. Since graph convolutional network (GCN)  emerges, many GCN-based network embedding methods have been proposed. For example, variational graph auto-encoder (VGAE)  adopts the same message-passing mechanism as GCN to embed the homogeneous network. More recently, GCN has been integrated with random walk based heterogeneous network embedding methods, which leads to heterogeneous graph convolutional networks like MAGNN  and HAN . MANE  successfully employs network embedding on multi-layered networks and DMGC  accomplishes both network embedding and clustering by utilizing cross-layer links as regularization. In addition, many embedding algorithms for knowledge graph have been proposed such as translational distance model  and semantic matching model .

  Models & Cora & Citeseer & NS & C.ele \\   & AUC-ROC & AUC-PR & AUC-ROC & AUC-ROC & AUC-PR & AUC-ROC & AUC-PR \\  node2vec & 75.96\(\)1.18 & 82.73\(\)0.69 & 69.63\(\)0.97 & 77.38\(\)0.89 & 86.48\(\)0.76 & 91.36\(\)0.75 & **79.36\(\)1.14** & 74.27\(\)1.33 \\ VGAE  & 72.86\(\)0.53 & 81.07\(\)0.59 & 72.58\(\)0.74 & 78.00\(\)0.40 & 86.45\(\)0.95 & 89.99\(\)0.58 & 77.18\(\)1.39 & 73.50\(\)2.38 \\ GAT  & 76.48\(\)0.77 & **78.50\(\)**1.03 & 71.79\(\)1.40 & 76.64\(\)1.18 & 85.89\(\)0.94 & 88.48\(\)1.92 & 74.16\(\)4.33 & 71.21\(\)1.69 \\ ARGVA  & 79.93\(\)0.97 & 87.11\(\)1.44 & 79.66\(\)1.10 & 74.47\(\)0.59 & 88.51\(\)1.12 & 90.32\(\)1.36 & 77.04\(\)1.71 & 71.29\(\)2.42 \\ RREG  & 79.33\(\)0.95 & 83.11\(\)0.86 & 73.99\(\)1.17 & 77.48\(\)1.30 & 83.98\(\)2.11 & 87.87\(\)2.77 & 74.65\(\)4.54 & 72.07\(\)3.70 \\  Sensei & **81.27\(\)0.65** & **83.59\(\)1.11** & **76.28\(\)0.90** & **82.02\(\)0.79** & **91.42\(\)1.25** & **89.10\(\)0.77** & 77.13\(\)0.81 & **77.28\(\)1.53** \\  

Table 1: The AUC-ROC (\( std\)) and AUC-PR (\( std\)) of link prediction in plain networks (%).

  Models & Cora & Citeseer & NS & C.ele \\  node2vec & 19.78\(\)0.84 & 18.50\(\)1.30 & 59.88\(\)2.68 & 9.43\(\)2.41 \\ VGAE  & 8.19\(\)1.62 & 8.93\(\)0.91 & 37.62\(\)2.36 & 9.34\(\)1.32 \\ GAT  & 5.77\(\)0.64 & 5.99\(\)2.25 & 32.96\(\)5.39 & 9.71\(\)3.00 \\ ARGVA  & 5.65\(\)1.22 & 4.71\(\)0.56 & 36.78\(\)5.20 & 9.71\(\)2.39 \\ RBGE  & 17.19\(\)1.24 & 18.99\(\)1.17 & 52.48\(\)5.18 & 9.48\(\)4.86 \\  Sensei(w@5 step 2) & 19.82\(\)1.18 & 18.17\(\)1.31 & 61.01\(\)4.22 & 11.07\(\)2.18 \\ Sensei & **20.51\(\)1.13** & **19.03\(\)1.50** & **61.31\(\)3.46** & **11.52\(\)2.13** \\  

Table 2: Hit@10 (\( std\)) of node recommendation in plain networks (%).

**Sampling in Network Embedding.** Sampling is an important technique, which appears in recommendation  and text embedding  to speed up the training process. For network embedding algorithms, both the positive sampling strategy and the negative sampling strategy have been applied and improved. For matrix factorization , spectral clustering  and LINE , direct or two-hop neighboring nodes act as positive samples and negative nodes are sampled uniformly or according to the degree distribution. In DeepWalk , node2vec  and metapath2vec , the positive sampling strategy is modified based on truncated random walk starting from the central node. The positive and negative sampling are implemented implicitly with a message-passing mechanism in GCN-based methods, where nodes to be aggregated can be viewed as positive samples and the remaining nodes are implicit negative samples. For the GCN related works, efforts are mainly made to improve the positive sampling strategy. For example, APPNP  manipulates positive samples with personalized pagerank  to catch long distance information and PGNN  randomly fixes an anchor node set for aggregation in each training epoch. Recently, more attention has been paid to improve the negative sampling strategy. Many works suggest to sample closer nodes as _difficult_ negative samples. For instance, MCNS  proposes that the negative sampling distribution should be positively correlated with the positive sampling distribution. KBGAN  samples most similar entities to replace the groundtruth positive entity in knowledge graph embedding and RecNS  tends to sample negative nodes from the intermediate distance region.

## 6 Conclusion and Limitations

In this paper, we study the sampling strategies of network embedding. To uncover the underlying relationships of existing competing sampling strategies, we conduct theoretical analysis on the sampling-based network embedding process. In the analysis, we identify two desirable properties for the similarity scores of node embedding, including the _discrimination_ property and the _monotonicity_ property. Furthermore, we prove that there always exists an error gap between the empirical and ideal optimal embedding similarity scores. Guided by such analysis, we propose a simple yet novel model (Sensei), which creatively integrates the two competing sampling strategies to fulfill the the _discrimination_ property and the _partial monotonicity_ property. The effectiveness of Sensei is verified by extensive experiments.

This paper studies sampling strategies of network embedding, which has no negative ethical impacts on society. The limitations of our paper lie in that the theoretical analysis is conducted on proximity scores and the proposed Sensei model is designed for plain (non-attributed) networks rather than attributed networks. The key to generalize the proposed properties/Sensei model to attributed networks is a new definition of the node pair similarity, which we leave for future exploration.

## 7 Acknowledgement

This work is supported by NSF (1947135, 2134079, 2316233, 1939725, and 2324770), DARPA (HR001121C0165), NIFA (2020-67021-32799), DHS (17STQAC00001-07-00), ARO (W911NF2110088). The content of the information in this document does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.