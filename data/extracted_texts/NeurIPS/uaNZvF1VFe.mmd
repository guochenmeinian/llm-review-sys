# Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction

Wei Jiang\({}^{1}\), Sifan Yang\({}^{1,2}\), Wenhao Yang\({}^{1,2}\), Lijun Zhang\({}^{1,3,2,}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{2}\)School of Artificial Intelligence, Nanjing University, Nanjing, China

\({}^{3}\)Pazhou Laboratory (Huangpu), Guangzhou, China

{jiangw, yangsf, yangwh, zhanglj}@lamda.nju.edu.cn

Lijun Zhang\({}^{1,3,2,}\)

Lijun Zhang is the corresponding author.

###### Abstract

Sign stochastic gradient descent (signSGD) is a communication-efficient method that transmits only the sign of stochastic gradients for parameter updating. Existing literature has demonstrated that signSGD can achieve a convergence rate of \((d^{1/2}T^{-1/4})\), where \(d\) represents the dimension and \(T\) is the iteration number. In this paper, we improve this convergence rate to \((d^{1/2}T^{-1/3})\) by introducing the Sign-based Stochastic Variance Reduction (SSVR) method, which employs variance reduction estimators to track gradients and leverages their signs to update. For finite-sum problems, our method can be further enhanced to achieve a convergence rate of \((m^{1/4}d^{1/2}T^{-1/2})\), where \(m\) denotes the number of component functions. Furthermore, we investigate the heterogeneous majority vote in distributed settings and introduce two novel algorithms that attain improved convergence rates of \((d^{1/2}T^{-1/2}+dn^{-1/2})\) and \((d^{1/4}T^{-1/4})\) respectively, outperforming the previous results of \((dT^{-1/4}+dn^{-1/2})\) and \((d^{3/8}T^{-1/8})\), where \(n\) represents the number of nodes. Numerical experiments across different tasks validate the effectiveness of our proposed methods.

## 1 Introduction

This paper investigates the stochastic optimization problem

\[_{^{d}}f(),\] (1)

where \(f:^{d}\) is a smooth and non-convex function. We assume that only noisy estimations of the gradient \( f()\) can be accessed, represented as \( f(;)\), where \(\) is a random sample drawn from a stochastic oracle such that \([ f(;)]= f()\).

The most well-known method for problem (1) is stochastic gradient descent (SGD), which performs \(_{t+1}=_{t}- f(_{t};_{t})\) for each iteration, where \(_{t}\) is the sample used in the \(t\)-th iteration, and \(\) is the learning rate. It has been proved that the SGD method can obtain a convergence rate of \((T^{-1/4})\)[Ghadimi and Lan, 2013], where \(T\) is the iteration number. Recently, sign stochastic gradient descent (signSGD) method [Seide et al., 2014, Bernstein et al., 2018] has become popular in the machine learning community, which uses the sign of the stochastic gradient to update, i.e.,

\[_{t+1}=_{t}-( f(_{ t};_{t})).\]

This method can largely reduce the communication overhead in distributed environments, and prior research [Bernstein et al., 2018, 2019] has established that signSGD can achieve a convergence rateof \((d^{1/2}T^{-1/4})\) measured in terms of the \(l_{1}\)-norm. Since the \((T^{-1/4})\) rate is already optimal for SGD methods when measured in the \(l_{2}\)-norm (Arjevani et al., 2023), we can not further improve the dependence on \(T\) for signSGD method, considering that \(\|\|_{2}\|\|_{1}\) for any \(\).2 However, it is also known that variance reduction techniques can further enhance the convergence rate to \((T^{-1/3})\), under a slightly stronger assumption of average smoothness (Fang et al., 2018; Wang et al., 2019; Cutkosky and Orabona, 2019). This leads to a natural question: _Can the convergence of sign-based methods be further improved by employing variance reduction techniques along with the average smoothness assumption?_ We respond affirmatively by introducing the Sign-based Stochastic Variance Reduction (SSVR) method. By integrating variance reduction technique (Cutkosky and Orabona, 2019) with sign operations, we achieve an improved convergence rate of \((d^{1/2}T^{-1/3})\) measured in the \(l_{1}\)-norm, matching the optimal rates in terms of \(T\) for stochastic variance reduction methods (Fang et al., 2018; Li et al., 2021; Arjevani et al., 2023).

Furthermore, we investigate a special case of problem (1), in which the objective function exhibits a finite-sum structure:

\[_{^{d}}f()=_{i=1}^{m}f_{i }(),\] (2)

where each \(f_{i}()\) is smooth and non-convex. This problem has been extensively studied in stochastic optimization (Zhang et al., 2013; Defazio et al., 2014; Fang et al., 2018), but is less explored with sign-based methods. Previous literature proposes signSVRG (Chzhen and Schechtman, 2023) method to deal with the finite-sum problem, which achieves a convergence rate of \((m^{1/2}d^{1/2}T^{-1/2})\). However, its dependence on \(m\) is sub-optimal, failing to match the \((m^{1/4}T^{-1/2})\) lower bound (Fang et al., 2018; Li et al., 2021) for problem (2). To address this gap, we propose the SSVR-FS algorithm, which periodically computes the exact gradient (Zhang et al., 2013; Johnson and Zhang, 2013) and incorporates it into the variance reduction estimator. In this way, we can achieve an improved convergence rate of \((m^{1/4}d^{1/2}T^{-1/2})\) for finite-sum problems.

Finally, sign-based methods are especially favorable in distributed settings, where the parameter server aggregates gradient signs from each worker through majority vote (Bernstein et al., 2018), allowing 1-bit compression of communication in both directions. Existing literature (Bernstein et al., 2018, 2019) has proved that signSGD can obtain a convergence rate of \((d^{1/2}T^{-1/4})\) for majority vote in homogeneous settings, where the data across nodes is uniformly distributed or identical. For the more challenging heterogeneous setting, in which data distribution can vary significantly across nodes, existing methods can only achieve convergence rates of \((dT^{-1/4}+dn^{-1/2})\)(Sun et al., 2023) and \((d^{3/8}T^{-1/8})\)(Jin et al., 2023), where \(n\) denotes the number of nodes. Note that the first rate indicates that the gradient does not converge to zero as \(T\) approaches infinity, and the second one suffers from a high sample complexity. To address these limitations, we first introduce our basic SSVR-MV method, which employs variance reduction estimators to track gradients and replaces the sign operation in each worker as a stochastic unbiased sign operation. This practice ensures 1-bit compression and unbiased estimation at the same time, and the newly proposed method can obtain an improved convergence rate of \((d^{1/2}T^{-1/2}+dn^{-1/2})\). By further substituting the sign operation in the parameter server with another stochastic unbiased sign operation, our method can further achieve a convergence rate of \((d^{1/4}T^{-1/4})\), which converges to zero as \(T\) increases.

In summary, compared with existing methods, this paper makes the following contributions:

* For stochastic non-convex functions, we develop a sign-based variance reduction algorithm to achieve an improved convergence rate of \((d^{1/2}T^{-1/3})\), surpassing the \((d^{1/2}T^{-1/4})\) rate for signSGD methods.
* For non-convex finite-sum optimization, we further improve the our proposed method to obtain an enhanced convergence rate of \((m^{1/4}d^{1/2}T^{-1/2})\), which is better than the \((m^{1/2}d^{1/2}T^{-1/2})\) convergence rate for SignSVRG method.

* We also investigate sign-based variance reduction methods with heterogeneous majority vote in distributed settings. The proposed algorithms can obtain the convergence rates of \((d^{1/2}T^{-1/2}+dn^{-1/2})\) and \((d^{1/4}T^{-1/4})\), which outperform the previous results of \((dT^{-1/4}+dn^{-1/2})\) and \((d^{3/8}T^{-1/8})\), respectively.

We compare our results with existing methods in Table 1 and Table 2, and validate the effectiveness of our method via numerical experiments in Section 5.

## 2 Related work

This section provides an overview of the existing literature on signSGD methods and stochastic variance reduction techniques.

### SignSGD and its variants

The idea of only transmitting the sign information of the stochastic gradient traces back to the 1-bit SGD algorithm, introduced by Seide et al. (2014). Despite the biased nature of the sign operation, Bernstein et al. (2018) demonstrated that signSGD achieves a convergence rate of \((d^{1/2}T^{-1/4})\) by using large batch sizes in each iteration. Despite the theoretical assurance, Karimireddy et al. (2019) highlighted that signSGD may not converge to the optimal solutions for convex functions and could suffer from poor generalization without large batches. To address these issues, they proposed the

   Method & Setting & Measure & Convergence rate \\  signSGD (Bernstein et al., 2018) & stochastic & \(l_{1}\)-norm & \((}{N^{1/4}})\) \\ EF-signSGD (Karimireddy et al., 2019) & stochastic & \(l_{2}\)-norm & \((})\) \\ signSGD-SIM (Sun et al., 2023) & stochastic & \(l_{1}\)-norm & \((})\) \\ SignSVRG (Chzhen and Schechtman, 2023) & finite-sum & \(l_{1}\)-norm & \((m^{1/2}}{N^{1/2}})\) \\ SignRVR/SignRVM (Qin et al., 2023) & finite-sum & \(l_{1}\)-norm & \((m^{1/2}}{N^{1/2}})\) \\ 
**Theorem 1** & stochastic & \(l_{1}\)-norm & \((}{N^{1/3}})\) \\
**Theorem 2** & finite-sum & \(l_{1}\)-norm & \((m^{1/4}}{N^{1/2}})\) \\   

Table 1: Summary of results for sign-based algorithms. Here, stochastic indicates problem (1), finite-sum represents problem (2), \(N\) is the number of stochastic gradient calls, and \(m\) is the number of component functions. Note that some rates are measured under squared \(l_{1}\)- or \(l_{2}\)-norm, and we convert them to \(l_{1}\)- or \(l_{2}\)-norm for a fair comparison.

   Method & Heterogeneous & Measure & Convergence rate \\  signSGD (Bernstein et al., 2018) & ✗ & \(l_{1}\)-norm & \((}{T^{1/4}})\) \\ Signum (Bernstein et al., 2019) & ✗ & \(l_{1}\)-norm & \((}{T^{1/4}})\) \\ SSDM (Safaryan and Richtarik, 2021) & ✓ & \(l_{2}\)-norm & \((}{T^{1/4}})\) \\ Sto-signSGD (Jin et al., 2023) & ✓ & \(l_{2}\)-norm & \((}{T^{1/8}})\) \\ MV-sto-signSGD-SIM (Sun et al., 2023) & ✓ & \(l_{1}\)-norm & \((}+})\) \\ 
**Theorem 3** & ✓ & \(l_{1}\)-norm & \((}{T^{1/2}}+})\) \\
**Theorem 4** & ✓ & \(l_{2}\)-norm & \((}{T^{1/4}})\) \\   

Table 2: Summary of results for sign-based algorithms under the majority vote setting, where \(n\) is the number of workers. Some rates are measured under squared \(l_{1}\)- or \(l_{2}\)-norm, and we convert them to \(l_{1}\)- or \(l_{2}\)-norm for a fair comparison.

EF-signSGD method, which integrates error feedback into signSGD to correct errors introduced by the sign operation. Instead of requiring unbiased stochastic gradients in previous literature, Safaryan and Richtarik (2021) assumed that the signs of the stochastic gradient are the same as those of true gradient with a probability greater than \(1/2\). Under this assumption, they demonstrated that signSGD can obtain a similar convergence rate but does not require large batches anymore. Recently, Sun et al. (2023) proposed the signSGD-SIM method, which incorporates the momentum into the signSGD, achieving a convergence rate of \((dT^{-1/4})\) with constant batch sizes and an improved convergence of \((d^{3/2}T^{-2/7})\) with second-order smoothness.

To deal with the finite-sum problems, Chzhen and Schechtman (2023) developed SignSVRG algorithm, which combines SVRG (Johnson and Zhang, 2013) method with signSGD and achieves a convergence rate of \((d^{1/2}m^{1/2}T^{-1/2})\), where \(m\) is the number of component functions. More recently, Qin et al. (2023) further investigate signSGD with random reshuffling, achieving a convergence rate of \((m^{-1/2}T^{-1/2}(mT)+\|\|_{1})\), where \(\) is the variance bound of stochastic gradients. By leveraging variance-reduced gradients and momentum updates, they further propose the SignRVR and SignRVM methods, both achieving the convergence rate of \((d^{1/2}m^{1/2}T^{-1/2})\).

In distributed settings, sign-based methods with majority vote are also widely investigated. Bernstein et al. (2018, 2019) first indicated that signSGD and its momentum variant Signnum can enable 1-bit compression of worker-server communication, obtaining the \((d^{1/2}T^{-1/4})\) convergence rates in the homogeneous environment. For the more challenging heterogeneous settings, SSDM method (Safaryan and Richtarik, 2021) attains the same \((d^{1/2}T^{-1/4})\) convergence rate, but the information sent back to the server is not a sign information anymore. To remedy this issue, Sto-signSGD algorithm (Jin et al., 2023) is proposed, equipped with a convergence rate of \((d^{3/4}T^{-1/4})\) measured in squared \(l_{2}\)-norm. More recently, Sun et al. (2023) introduced the MV-signSGD-SIM algorithm and demonstrated a convergence rate of \((dT^{-1/4}+dn^{-1/2})\), which could be further enhanced to \((d^{3/2}T^{-2/7}+dn^{-1/2})\) under second-order smoothness conditions, where \(n\) denotes the number of nodes in the distributed system.

### Stochastic variance reduction methods

Stochastic variance reduction methods have gained significant attention in the optimization community in recent years. Among the pioneering approaches, the stochastic average gradient (SAG) method (Roux et al., 2012) and Stochastic Dual Coordinate Ascent (SDCA) algorithm (Shalev-Shwartz and Zhang, 2013) utilize a memory of previous gradients to ensure variance reduction, achieving linear convergence for strongly convex functions. To circumvent the need for storing gradients, the stochastic variance reduced gradient (SVRG) (Zhang et al., 2013; Johnson and Zhang, 2013) recalculates the full gradient periodically to enhance the accuracy of gradient estimators, maintaining linear convergence for strongly convex functions. Inspired by SAG and SVRG, Defazio et al. (2014) introduced the SAGA algorithm, which not only provides superior convergence rates but also supports proximal regularization. Subsequently, the stochastic recursive gradient algorithm (SARAH) (Nguyen et al., 2017) employs a simple recursive approach to update gradient estimators, ensuring better convergence for smooth convex functions.

For non-convex optimization, inspired by the SVRG algorithm, many methods (Reddi et al., 2016; Lei et al., 2017; Zhou et al., 2020) employ variance reduction to design their algorithms and provide the corresponding convergence guarantees. More recent well-known advancements include the SPIDER (Fang et al., 2018) and SpiderBoost (Wang et al., 2019) methods, which improved the \((T^{-1/4})\) convergence rate of traditional SGD to \((T^{-1/3})\) under the average smoothness assumption. The convergence rate can be further improved to \((m^{1/4}T^{-1/2})\) for problems with a finite-sum structure, where \(m\) represents the number of component functions. However, these methods typically require a huge batch size to ensure convergence. To avoid this limitation, the stochastic recursive momentum (STORM) method (Cutkosky and Orabona, 2019) introduces a momentum-based updating and an adaptive learning rate based on the stochastic gradients, achieving a convergence rate of \(}(T^{-1/3})\) without necessitating large batches. More recently, variance reduction techniques are widely employed in more complex problems to improve the existing convergence rates, such as compositional optimization (Wang et al., 2017, 2017, 2019; Jiang et al., 2022, 2023), multi-level optimization (Chen et al., 2021; Zhang and Xiao, 2021; Jiang et al., 2022, 2024), adaptive algorithms (Kavis et al., 2022; Jiang et al., 2024), and distributionally robust optimization (Yu et al., 2024).

## 3 The proposed methods

In this section, we present the proposed methods for the expectation case, i.e., problem (1), and the finite-sum structure, i.e., problem (2), respectively, along with corresponding theoretical guarantees.

### Sign-based stochastic variance reduction

In this subsection, we introduce our Sign-based Stochastic Variance Reduction (SSVR) method for problem (1). One crucial step in stochastic optimization is to track the gradient of the objective function. Here, we use a variance reduction gradient estimator \(_{t}\) to evaluate the overall gradient \( f(_{t})\). In the first iteration (\(t=1\)), the estimator is defined as \(_{1}=}_{k=1}^{B_{0}} f(_{1};_{1 }^{k})\), where \(B_{0}\) is the batch size used in the first iteration. For subsequent iterations (\(t 2\)), \(_{t}\) is updated in the style of STORM (Cutkosky and Orabona, 2019), i.e.,

\[_{t}=}_{k=1}^{B_{1}} f(_{t};_{t }^{k})+(1-)(_{t-1}-}_{k=1}^{B_{1}} f (_{t-1};_{t}^{k})),\]

where \(\) represents the momentum parameter and \(B_{1}\) is the batch size. This method ensures that the expectation of the estimation error \([\|_{t}- f(_{t})\|^{2}]\) would be reduced gradually. After obtaining the gradient estimator \(_{t}\), we update the decision variable using the sign of \(_{t}\):

\[_{t+1}=_{t}-(_{t} ).\]

The whole algorithm is outlined in Algorithm 1. Next, we introduce the following assumptions for our SSVR method, which are standard and commonly adopted in the analysis of variance reduction methods and stochastic non-convex optimization (Fang et al., 2018; Wang et al., 2019; Cutkosky and Orabona, 2019; Li et al., 2021).

**Assumption 1**: _(Average smoothness)_

\[_{}[\| f(;)- f(;)\|^ {2}] L^{2}\|-\|^{2}.\]

**Assumption 2**: _(Bounded variance)_

\[_{}[\| f(;)- f()\|^{2} ]^{2}.\]

With the above assumptions, we can obtain the theoretical guarantee for our method as stated below.

**Theorem 1**: _Under Assumptions 1 and 2, by setting \(=(})\), \(=(T^{2/3}})\), \(B_{0}=(T^{1/3})\), and \(B_{1}=(1)\), our SSVR method ensures:_

\[[\| f(_{})\|_{1}] (}{T^{1/3}}).\]

**Remark:** This convergence rate surpasses the \((d^{1/2}T^{-1/4})\) rate achieved by previous sign-based methods (Bernstein et al., 2018, 2019), and it also outperforms the \((d^{3/2}T^{-2/7})\) convergence rate under the second-order smoothness condition (Sun et al., 2023). Specifically, to ensure that \([\| f(_{})\|_{1}]\), our method requires a sample complexity of \((d^{3/2}^{-3})\), which is much better than the \((d^{2}^{-4})\) and \((d^{21/4}^{-7/2})\) complexities of previous approaches.

### Sign-based stochastic variance reduction for finite-sum structure

We now extend our SSVR method to deal with the finite-sum structure in problem (2). In this context, we introduce the following assumption for each component function, which is standard and widely adopted in existing literature (Fang et al., 2018; Wang et al., 2019; Li et al., 2021).

**Assumption 3**: _(Smoothness) For each \(i\{1,2,,m\}\), the gradient functions satisfy:_

\[\| f_{i}()- f_{i}()\| L\|- \|.\]

To handle the finite-sum problems, we retain the core structure of our SSVR method while incorporating elements from the SVRG (Zhang et al., 2013; Johnson and Zhang, 2013) approach. Specifically, we compute a full batch gradient at the first step and every \(I\) iteration, i.e.,

\[ f(_{})=_{i=1}^{m} f_{i}(_ {}).\]

For other iterations, we randomly select an index \(i_{t}\) from the set \(\{1,2,,m\}\) and construct a variance reduction gradient estimator \(_{t}\) as follows:

\[_{t}=}(_{t})+(1-)( _{t-1}- f_{i_{t}}(_{t-1}))}_{}- }(_{})- f(_{}))}_{}.\] (3)

The first two terms of \(_{t}\) align with the STORM estimator, and the last term measures the difference of past gradients between the selected component function \( f_{i_{t}}(_{})\) and the overall objective \( f(_{})\). Note that the STORM estimator employs the component gradient \( f_{i_{t}}(_{t})\) to track the overall gradient \( f(_{t})\), which leads to an estimation error due to the gap between the component function and the overall objective. This gap can be effectively mitigated by the error correction term we introduced in equation (3). With such a design, we can obtain a better gradient estimation of the overall gradient, and ensure that the estimation error \([\| f(_{t})-_{t}\|^{2}]\) can be reduced gradually. After computing \(_{t}\), we utilize its sign information to update the decision variable. The detailed procedure is outlined in Algorithm 2. Next, we present the theoretical convergence for this method.

```
1:Input: time step \(T\), initial point \(_{1}\)
2:for time step \(t=1\)to\(T\)do
3:if\(t I==0\)then
4: Set \(=t\) and compute \( f(_{})=_{i=1}^{m} f_{i}(_ {})\)
5:endif
6: Sample \(i_{t}\) randomly from \(\{1,2,,m\}\)
7: Compute gradient estimator \(_{t}\) according to equation (3)
8: Update the decision variable: \(_{t+1}=_{t}-(_{t})\)
9:endfor
10: Select \(\) uniformly at random from \(\{1,,T\}\)
11: Return \(_{}\) ```

**Algorithm 2** SSVR for Finite-Sum (SSVR-FS)

**Theorem 2**: _Under Assumption 3, by setting \(=()\), \(I=m\), and \(=(}T^{1/2}})\), our algorithm ensures:_

\[[\| F(_{})\|_{1}](d^{1/2}}{T^{1/2}}).\]

**Remark:** To ensure \([\| F(_{})\|_{1}]\), the sample complexity is \((m+}{^{2}})\), which improves over the \((})\) complexity of the previous SignSVRG method (Chzhen and Schechtman, 2023).

## 4 Sign-based stochastic variance reduction with majority vote

Sign-based methods are advantageous in distributed settings for their low communication overhead, as they can only transmit sign information between nodes via majority vote. This section explores sign-based stochastic methods with majority vote, a typical example of distributed learning extensivelystudied in previous sign-based algorithms (Bernstein et al., 2018, 2019; Safaryan and Richtarik, 2021; Sun et al., 2023). To begin with, we investigate the following distributed learning task:

\[_{^{d}}f()_{j=1}^ {n}f_{j}(), f_{j}()=_{^{j}_{j}}[f_{j}(;^{j})],\] (4)

where \(_{j}\) represents the data distribution for node \(j\), and \(f_{j}()\) is the corresponding loss function. Some previous studies (Bernstein et al., 2018, 2019) investigate the homogeneous setting, which assumes the data across each node is uniformly distributed or identical, ensuring that \([f_{i}()]=f()\). In contrast, this paper considers the more challenging heterogeneous setting (Jin et al., 2023; Sun et al., 2023), where data distributions can vary significantly across nodes.

For sign-based methods in distributed settings, each node \(j\) computes a gradient estimator \(_{t}^{j}\) and transmits its sign, i.e., \((_{t}^{j})\), to the parameter server. Note that the server can not directly send the aggregate information \(_{j=1}^{n}(_{t}^{j})\) back to each node, since it loses binary characteristic after summation. A natural solution is to apply another sign operation to update the decision variable as:

\[_{t+1}=_{t}-(_ {j=1}^{n}(_{t}^{j})).\]

This process is called majority vote (Bernstein et al., 2018), as each worker votes on the sign of the gradient, with the server tallying these votes and broadcasting the decision back to the nodes. However, the sign operation introduces bias in the estimation, and employing it twice can significantly amplify this bias, particularly in a heterogeneous environment. Previous analysis (Chen et al., 2020) indicates that signSGD fails to converge in the heterogeneous setting. To deal with this problem, we introduce an unbiased sign operation \(_{}()\), which is defined below.

**Definition 1**: _For any vector \(\) with \(\|\|_{} R\), define the function mapping \(_{}()\) as:_

\[[_{}()]_{k}=+1,&+]_{k}}{2R},\\ -1,&-]_{k}}{2R}.\] (5)

**Remark:** This operation provides an unbiased estimation of \(/R\), such that \([_{}()]=/R\). It is worth noting that the function mapping is valid when \(\|\|_{} R\), since the probability should always fall within \(\). For this purpose, we need to further assume that the gradient is bounded.

Utilizing this unbiased sign operation, we can update the decision variable as:

\[_{t+1}=_{t}-(_{j=1} ^{n}_{}(_{t}^{j})).\]

After applying \(_{}()\), the output is a sign information, which can be transported between nodes efficiently. The complete algorithm, named SSVR with majority vote (SSVR-MV), is described in Algorithm 3 (with _Option 1_). Note that in Step 4, we set \(_{1}^{j}= f(_{1};_{1}^{j})\) when \(t=1\). Next, we present the convergence guarantee for the proposed algorithm with the following assumption.

**Assumption 4**: _For each node \(j\), the stochastic gradient is bounded by \(G\) in the infinity norm, such that \(\| f_{j}(;)\|_{} G\)._

**Theorem 3**: _Under Assumptions 1, 2 and 4, by setting \(=\) and \(=(d^{1/2}})\), our SSVR-MV method (with Option 1) ensures:_

\[[\| f(_{})\|_{1}] (}{T^{1/2}}+}).\]

**Remark:** Our rate is better than the previous result of \((dT^{-1/4}+dn^{-1/2})\), and also outperforms the rate of \((d^{3/2}T^{-2/7}+dn^{-1/2})\) under the second-order smoothness (Sun et al., 2023).

Although the above convergence rate is superior to previous results, we have to note that the gradient does not converge to zero even as \(T\). To address this issue, we propose replacing another sign operation with the \(_{1}()\) mapping, as defined in equation (5) with \(R=1\). Additionally, in our prior analysis, we ensured that each \(_{t}^{j}\) is bounded by assuming the stochastic gradient is bounded and using a constant \(\). Here, we instead suppose that the true gradient is bounded, as detailed below.

**Assumption 4\({}^{}\)**: _For each node \(j\), the gradient is bounded such that \(\| f_{j}()\| G\)._

**Remark:** This assumption is weaker than the one used by Sun et al. (2023), which assumes all _stochastic_ gradients are bounded, i.e., \(\| f_{j}(;)\| G\).

To ensure each gradient estimator is bounded, we employ a projection operation \(}_{t}^{j}=_{G}[_{t}^{j}]\), where \(_{G}\) denotes the projection onto a ball of radius \(G\). This allows us to utilize an unbiased sign mapping \(_{G}(}_{t}^{j})\) before transmission to the parameter server. The revised algorithm is presented in Algorithm 3 (with _Option 2_), and the modifications lie in Steps 6 and 9. We now present the convergence guarantee for this modified approach below.

**Theorem 4**: _Under Assumptions 1, 2 and 4\({}^{}\), by setting \(=(})\) and \(=(T^{1/2}})\), our SSVR-MV method (with Option 2) ensures:_

\[[\| f(_{})\|]( }{T^{1/4}}).\]

**Remark:** This rate converges to zero as \(T\), and offers a significant improvement over the previous results of \((d^{3/8}T^{-1/8})\)(Jin et al., 2023). Our result is also better than the \((d^{1/2}T^{-1/4})\) convergence rate obtained by Safaryan and Richtarik (2021), whose algorithm requires transmitting \(_{j=1}^{n}(_{t}^{j})\) back to all nodes, which is actually not sign information anymore.

## 5 Experiments

In this section, we assess the performance of the proposed methods through numerical experiments. We first evaluate the SSVR and SSVR-FS algorithms within the centralized setting, and then assess the performance of SSVR-MV method in the distributed learning environment. All experiments are conducted on NVIDIA 3090 GPUs.

### Evaluation of SSVR and SSVR-FS methods in the centralized environment

To begin with, we conduct numerical experiments on multi-class image classification tasks to validate the effectiveness of our proposed methods. Concretely, we train a ResNet18 model (He et al., 2016) on the CIFAR-10 dataset (Krizhevsky, 2009). We compare the performance of our SSVR and SSVR-FS methods against signSGD (Bernstein et al., 2018), signSGD-SIM (Sun et al., 2023), and SignSVRG (Chzhen and Schechtman, 2023). For hyper-parameter tuning, we either follow the recommendations from the original papers or employ a grid search to determine the best settings. Specifically, the momentum parameter \(\) is searched from the set \(\{0.1,0.5,0.9,0.99\}\), and the learning rate is fine-tuned within the range of \(\{1e-5,1e-4,1e-3,1e-2,1e-1\}\).

**Results.** The training loss, gradient norm, and testing accuracy are presented in Figure 1, with curves averaged over five runs. We observe that all methods exhibit a rapid decrease in training losses, with our methods showing a more pronounced reduction in the gradient norm. In terms of testing accuracy, our SSVR algorithm outperforms other sign-based methods, and our SSVR-FS method achieves superior accuracy in the final epochs.

### Evaluation of SSVR-MV method in the distributed learning

Subsequently, we conduct experiments to evaluate the effectiveness of the SSVR-MV method in the distributed environment. Specifically, we train a ResNet50 model (He et al., 2016) on the CIFAR-100 dataset (Krizhevsky, 2009) with 4 and 8 nodes respectively. We compare the performance of our method against signSGD (with majority vote) (Bernstein et al., 2018), Signum (with majority vote) (Bernstein et al., 2019), SSDM (Safaryan and Richtarik, 2021), Sto-signSGD (Jin et al., 2023), and MV-signSGD-SIM (Sun et al., 2023). The hyper-parameter tuning follows the same methodology as in the centralized environment experiment.

**Results.** We plot the training loss and testing accuracy in Figure 2, with all curves averaged over five runs. The results indicate that the training loss of our SSVR-MV algorithm decreases rapidly, and our method obtains higher testing accuracy compared to other methods, both in experiments with 4 nodes and 8 nodes.

Figure 1: Results for CIFAR-10 dataset in the centralized environment.

Figure 2: Results for CIFAR-100 dataset in the distributed environment.

Conclusion

In this paper, we explore sign-based stochastic variance reduction (SSVR) methods, which use only the sign information of variance-reduced estimators to update decision variables. The proposed method achieves an improved convergence rate of \((d^{1/2}T^{-1/3})\), surpassing the \((d^{1/2}T^{-1/4})\) convergence rate of signSGD methods. When applied to finite-sum problems, this rate can be further enhanced to \((m^{1/4}d^{1/2}T^{-1/2})\), which is also better than the \((m^{1/2}d^{1/2}T^{-1/2})\) convergence rate of SignSVRG. Finally, we investigate the SSVR method in distributed settings and devise novel algorithms to attain convergence rates of \((d^{1/2}T^{-1/2}+dn^{-1/2})\) and \((d^{1/4}T^{-1/4})\), which improve upon the previous results of \((dT^{-1/4}+dn^{-1/2})\) and \((d^{3/8}T^{-1/8})\) respectively.