# Dimension-free Private Mean Estimation for Anisotropic Distributions

Yuval Dagan

School of Computer Science

Tel Aviv University

ydagan@tauex.tau.ac.il

&Michael I. Jordan

Department of EECS and Statistics

University of California Berkeley

michael_jordan@berkeley.edu

&Xuelin Yang

Department of Statistics

University of California Berkeley

xuellin@berkeley.edu

&Lydia Zakynthinou

Department of EECS

University of California Berkeley

lydiazak@berkeley.edu

&Nikita Zhivotovskiy

Department of Statistics

University of California Berkeley

zhivotovskiy@berkeley.edu

Authors ordered alphabetically.

###### Abstract

We present differentially private algorithms for high-dimensional mean estimation. Previous private estimators on distributions over \(^{d}\) suffer from a curse of dimensionality, as they require \((d^{1/2})\) samples to achieve non-trivial error, even in cases where \(O(1)\) samples suffice without privacy. This rate is unavoidable when the distribution is isotropic, namely, when the covariance is a multiple of the identity matrix. Yet, real-world data is often highly anisotropic, with signals concentrated on a small number of principal components. We develop estimators that are appropriate for such signals--our estimators are \((,)\)-differentially private and have sample complexity that is dimension-independent for anisotropic subgaussian distributions. Given \(n\) samples from a distribution with known covariance-proxy \(\) and unknown mean \(\), we present an estimator \(\) that achieves error, \(\|-\|_{2}\), as long as \(n()/^{2}+(^{1/2})/( )\). We show that this is the optimal sample complexity for this task up to logarithmic factors. Moreover, for the case of unknown covariance, we present an algorithm whose sample complexity has improved dependence on the dimension, from \(d^{1/2}\) to \(d^{1/4}\).

## 1 Introduction

Machine learning is increasingly deployed in real-world settings to learn about properties of populations, both large and small. When the data comes from human populations, it is essential that algorithm design allows inferring properties of populations without revealing potentially sensitive information about specific individuals in the population. That sensitive information can be revealed, inadvertently or adversarially, has been demonstrated in numerous ways, including via reconstruction attacks , as well as membership-inference attacks , often targeting sensitive genomic data . To mitigate the risk of privacy violations in general database theory, Dwork, McSherry, Nissim, and Smith  proposed the rigorous guarantee of _differential privacy_ (DP), which hasbeen widely adopted in industry [29; 11; 33; 62; 53; 4] and government [30; 1; 2]. Algorithms that are differentially private are guaranteed to not leak too much information about the individuals in a database.

In the machine learning setting, there is a tension between differential privacy and inferential and predictive accuracy. It is an ongoing challenge to capture that tension mathematically, in a way that is applicable to a wide variety of problems and is sufficiently quantitative so as to provide a guide for real users and real systems designers. A particularly salient theoretical challenge is to obtain results that capture dimension-dependence--given that machine learning data are often of high dimensionality and involve significant correlation among dimensions, and given that privacy is difficult to guarantee in high dimensions, particularly so when there are correlations. Indeed, differentially private inference suffers from a _curse of dimensionality_--the sample size \(n\) that is required to obtain a non-trivial DP learner is often polynomial in the dimension \(d\) of the data.

Significant progress has been made in addressing this challenge in recent years by focusing on a relatively simple inferential task, that of high-dimensional mean estimation. Formally, given a data set of \(n\) points, \(X=(X^{(1)},,X^{(n)})^{d n}\) drawn i.i.d. from a multivariate distribution \(\) with unknown mean \(^{d}\), the goal is to learn \(\).

Obtaining low-error private mean estimators in the high-dimensional regime is not always possible. For example, consider a Gaussian distribution \(=(,^{2}I_{d})\), where \(I_{d}\) is the \(d d\) identity matrix. Here, the sample complexity of any private estimator \(\) achieving error \(\|-\|_{2}\) is \(n=(d^{2}/^{2}+d/())\), where \(\) is the privacy parameter.1 The first term corresponds to the non-private sample complexity and the second term to the additional samples required due to privacy. Although both depend on \(d\), note that for non-trivial error \(=0.01\) and \(=0.1\), the non-private term is \(O(1)\), whereas the dimension-dependence persists in the cost of privacy which is \(O()\).

In spite of this lower bound, there is still hope for obtaining better dependence on the dimension in certain cases. This is due to the fact that the lower bound instance assumes that the covariance is isotropic: a multiple of the identity matrix. However, real-world data are far from being isotropic. Often, the signal is concentrated in a few directions, while it is significantly weaker in others, as can be revealed via Singular Value Decomposition (SVD). In these cases, there are several examples of non-private estimators for a variety of tasks which exploit the structure of the data to achieve lower sample complexity. Specifically for mean estimation of Gaussian distributions, as in our example above, only \(n=O(()/^{2})\) samples are required  (this number of samples is sufficient even for robust estimators under the strong contamination model ). This bound is instance-adaptive, as the trace of the covariance matrix \(()\) equals its upper bound, \(d\|\|_{2}\), in the isotropic case, but can be much smaller for anisotropic data. Exploiting the non-isotropic structure of the covariance matrix is also central to the covariance estimation problem with respect to the operator norm (namely, when the error between the true covariance matrix \(\) and its estimate \(\) is measured in terms of \(\|-\|_{2}\)) [43; 65]. A more recent focus is on overparametrized linear regression , where again the highly non-isotropic structure of the covariance matrix allows for inference under certain assumptions on the decay of eigenvalues of the covariance matrix. In all the mentioned results, non-private estimation is possible when \(n d\), including even infinite-dimensional Hilbert spaces.

Returning to private estimation, prior work has obtained optimal bounds for learning the mean of high-dimensional (sub)Gaussian distributions in the affine-invariant Mahalanobis distance [18; 39; 3; 46; 13; 47]. These imply an upper bound for learning the mean in Euclidean distance in the order of \(n=O(d\|\|_{2}/^{2}+d/()})\), which is optimal for isotropic, but loose for anisotropic cases. A folklore estimator, based on  achieves \(n=(()}/())\), while  are the first to focus on the anisotropic case and obtain improved bounds for diagonal covariance, achieving error \(n=((^{1/2})/()+/)\). Thus, all previous work requires that the sample complexity is at least \(()\), which excludes the high-dimensional scenario we are interested in. We are led to pose the following question.:

**Question 1**.: _Is it possible to obtain good private mean estimators with a sample size that grows slower with the dimension, or is even dimension-independent, when the covariance of the data is far from isotropic? What is the optimal sample complexity in the case of known and unknown covariance?_

### Our contributions

First, note that no improved bounds are possible for _pure_ DP, as follows directly from the so-called _packing_ technique  and specifically applying [18, Lemma 5.1]: any \(\)-DP algorithm which estimates the mean of a Gaussian distribution up to constant accuracy requires \(n=(d/)\) samples. This negative result motivates us to focus on \((,)\)-differential privacy.

In order to make progress, one would like to utilize the fact that when the covariance is far from being isotropic, the data is closer to being low-dimensional. Concretely, let \(\) be the covariance matrix of \(\) and \(_{1}^{2}_{d}^{2}\) its singular values. If the covariance is far from isotropic, there are only few directions with non-trivial variance. For illustration, if \(_{1}==_{k}=1\), whereas \(_{k+1}==_{d}=1/d\), then the distribution is, in some sense, close to being \(k\)-dimensional. Here, we would like our sample complexity to be of order \(k\) rather than \(\).

We start by presenting a result in the case where the covariance matrix is known. Here, the bound depends _only_ on \(_{i=1}^{d}_{i}=(^{1/2})\), a quantity allowing less contribution from small singular values:

**Theorem 1.1** (Upper bound, known covariance, informal).: _Set \(,(0,1)\), \(>0\). Let \(X(,)^{n}\) with known covariance. There exists an \((,)\)-differentially private algorithm which, with probability \(0.99\), returns an estimate \(\) such that \(\|-\|_{2}\), and has sample complexity_

\[n=(()}{^{2}}+(^{1/2})}{}+ ).\] (1)

The first term corresponds to the non-private sample complexity, whereas the remaining two terms are due to privacy. The result extends to subgaussian distributions. In the example illustrated above, this bound indeed yields a dimension-independent complexity of \(n=_{}(k/^{2}+k/())\).

We show that the sample complexity of Theorem 1.1 is nearly optimal. Indeed, the first summand is optimal due to [19, Theorem 4], while the last summand is optimal by a lower bound in the univariate case . We show the optimality of the intermediate summand in (1) up to polylogarithmic terms.

**Theorem 1.2** (Lower bound, informal).: _Any \((,)\)-DP algorithm which estimates the mean \([-1,1]^{d}\) of a Gaussian up to \(\) with probability \(0.99\) has sample complexity \(n=((^{1/2})}{ ^{2}(d)}).\)_

We now move to the case of unknown covariance. A first approach would be to learn the covariance approximately, namely, find a matrix \(A\) such that \(A CA\), for some \(C>1\), and then use \(A\) instead of \(\) in our known-covariance estimator. However, learning such a matrix \(A\) privately requires sample size \(n=(d^{3/2})\). Another approach would be to learn only the diagonal elements of the covariance  this would require \(n=O(/)\) samples. Below, we obtain a sample complexity whose dependence in the dimension is \(d^{1/4}\), together with a dependendence on the diagonal elements of the covariance matrix:

**Theorem 1.3** (Upper bound, unknown covariance, informal).: _Let parameters \(,(0,1)\). Let \(X(,)^{n}\) with unknown covariance \(\). There exists an \((,)\)-DP algorithm which, with probability \(0.99\), returns an estimate \(\) such that \(\|-\|_{2}\), and has sample complexity_

\[n=(()}{^{2}}+^{d}_{ii}^{1/2}}{}+^{d}_{ii}^{1/2}}(1/)}{ }).\] (2)

In general, \((^{1/2})_{i=1}^{d}_{ii}^{1/2}\), and if \(\) is diagonal, the two quantities coincide. Our theorem is in fact more adaptable to easier cases of covariance structure. As a special case, when the covariance is diagonal and the singular values exhibit an exponential decay, that is, \(_{i}=_{1}e^{-(i-1)}\), then \(n=(()}{^{2}}+(^{1/2})}{}+ (d)^{3/2}(1/)}{})\) samples suffice even under unknown covariance.

### Techniques

Known covariance.A folklore \((,)\)-DP algorithm, based on techniques for the univariate case developed by , is to filter outliers by privately estimating each individual coordinate of the mean, \(_{i}\), up to an additive error of \((_{ii}^{1/2})\) for all \(i\), clipping any sample point to within that range, and outputting the mean of the modified data set with added spherical Gaussian noise \((0,()I_{d}/(^{2}n^{2}))\).

A standard analysis of this procedure yields a sample complexity of \(n()}{^{2}}+}{ }+()}}{}\), where the dependence on \(\) is omitted for clarity. For constant \(\), the folklore estimator achieves _privacy for free_, that is, the error due to privacy is lower than the error of statistical estimation, when \(n d\).

An improvement to this simple analysis, proposed recently by Aumuller et al.  for matrices of diagonal covariance, suggests adding noise \((0,(^{1/2})^{1/2}/(^ {2}n^{2}))\) instead, which introduces more noise in the directions of larger variance. Slightly simplifying their result and additionally ignoring logarithmic factors in \(d\), and the range of \(\), their sample complexity is \(n()}{^{2}}+}{ }+(^{1/2})}{}\). This estimator achieves privacy for free as long as \(n\{\|\|_{1}^{2}/\|\|_{2}^{2},\}\), where \(^{2}\) denotes the vector of singular values of \(\). While this removes the dimension dependence in the third term compared to the naive sample complexity, the second term still requires \(()\) samples. This is due to the first step of the algorithm (inherited from ), which performs \(d\) independent estimation tasks. In both approaches, the pre-processing step is a form of coarse mean estimation which ensures that the data will not include outliers, and it is the source of sample-inefficiency.

Thus, in our work, we remove outliers, namely vectors too far away from the true mean in one of the coordinates, using only \(n=(1/)\) samples, thus completely removing the dependence on \(d\) in the final sample complexity bounds. (Indeed, our estimator achieves privacy for free for \(n\|\|_{1}^{2}/\|\|_{2}^{2}\).) Next, we generalize the approach of  to general covariance, rather than diagonal. Finally, we show that the sample complexity is nearly optimal. Specifically:

Our pre-processing is realized by using a polynomial-time filtering algorithm of Tsfadia et al. . Given a predicate computed for two data points, so-called \(\) returns a subset \(X^{}\) of the input, such that all pairs of the remaining, unfiltered data points satisfy the predicate. Its sample complexity is \((1/)\) for _any_ predicate, hence it has the potential to yield a dimension-independent bound. For our purposes, \(X^{}\) needs to satisfy some sensitivity properties. It follows from our analysis that the filtering should be such that for any two points \(X^{(j)},X^{()} X^{}\), \(\|^{-1/4}(X^{(j)}-X^{()})\|_{2}^{2}( {tr}(^{1/2}))\).

The lower bound for \((,)\)-DP is an application of the standard fingerprinting [15; 28; 39; 40] technique for isotropic Gaussians. A straightforward modification of the technique to anisotropic covariance \(\) gives a weaker bound than Theorem 1.2. Instead one needs to choose an appropriate set of almost-isotropic coordinates whose size scales with \((^{1/2})\), and apply the technique to that set.

Unknown covariance.Moving to the case of unknown covariance, for illustration, we focus on the simpler, yet fundamental, case where the covariance matrix is diagonal, so that \(=(_{1}^{2},,_{d}^{2})\). First, the folklore algorithm described in the known-covariance setting, which adds spherical Gaussian noise, does not require knowledge of the covariance but only of its trace. The trace can be privately learned with \(n=(1/)\) samples. Second, we note that with \(n=(/)\) samples, it is possible to learn each \(_{i}\) up to a multiplicative constant . This allows us to apply the algorithm with known covariance from Theorem 1.1. However, the first step in this approach still requires \(()\) samples.

Our approach is to combine these two methods. We privately learn the largest \(k^{2}n^{2}\) variances, and their indices. This is done using the sparse vector technique  and can be achieved with \(n\) samples. We use the _known-covariance_ algorithm to estimate the mean in these top \(k\) coordinates, with the same error bound as in the known-covariance setting. For the mean at the remaining coordinates, we use the algorithm that only requires knowledge of the trace of the covariance. The error of the latter estimate is \(_{}\|_{}\|_{2}/(n )\), where \(_{}\) is the vector containing the lowest \(d-k\) variances. The first observation is that \(\) contains at least \(k\) entries as large as \(\|_{}\|_{}\), hence, \(\|\|_{1} k\|_{}\|_{}\). Then, by Holder's inequality, \(\|_{}\|_{2}_{}\|_{ 1}\|_{}\|_{}}\). Substituting \(k\) yields \(_{}\|\|_{1}/(^{2}n^{2})\), which implies the desired sample complexity bound in Theorem 1.3.

### Related work

Differentially private Gaussian mean estimation.Smith  proposed estimators for asymptotically normal statistics with optimal convergence rates under a certain range of parameters. The optimal sample complexity for learning the mean of a Gaussian with known covariance in _Mahalanobis norm_ under \((,)\)-DP is \(n d/^{2}+d/()+(1/)/\) and has been established in a series of works , starting from  in the univariate setting. Given the covariance matrix \(\), the Mahalanobis distance between the estimate \(\) and the true mean \(\) is defined as: \(\|-\|_{}=\|^{-1/2}(-)\|_{2}\). When \(=I_{d}\), the Mahalanobis and Euclidean norms coincide. The Mahalanobis distance yields an affine-invariant accuracy guarantee, and \(\|-\|_{}\) immediately implies \(\|-\|_{2}}\). However, the power of the Mahalanobis guarantee is overshadowed by the fact that even for \(=\), a large sample size, namely \(n=()\), is required, which excludes the high-dimensional scenario we are interested in.2 Furthermore, confidence sets induced by guarantees in the Euclidean distance have the pleasant property of being more easily constructible.

Beyond global sensitivity.There are several lines of work within differential privacy which aim to satisfy some form of instance-adaptive accuracy guarantee, as we do. General purpose frameworks which aim to privately estimate a statistic of the data, with error which adapts to "good" data sets, include propose-test-release , smooth-sensitivity , and Lipschitz extensions . Our method follows the same high-level structure as propose-test-release. The latter has been combined with robust estimators to yield optimal private learners for several tasks . Even more generally,  give a black-box method which transforms robust estimators to private ones via the _inverse-sensitivity_ mechanism  (see  for a discussion on inverse-sensitivity). As there exist optimal robust estimators for the mean of anisotropic Gaussians , this would be a viable approach, but the volumetric analysis of the transformation involves terms which depend on the dimension. Tsfadia et al.  propose a filtering method which yields private aggregators whose error adapts to the _diameter_ of the input data set. It is their method that we utilize for our upper bounds. A series of works formalize instance-optimality for private estimation of empirical  or population  quantities. These are all generally well-suited to our setting but either do not adapt to high dimensions, or a direct application would require \(n()}/()\).

Nikolov and Tang  study instance-optimality specifically for Gaussian noise mechanisms, albeit for data that belong in a bounded convex set. Although this is not the case for Gaussian data, it is worth noting that our error rates match those of , which hold for arbitrary distributions over \(K\), when the bounded set is \(K=+^{1/2}^{d}(1)\). Privately learning \(K\) however would require more samples.

Privately learning nuisance parameters.Karwa and Vadhan  learn (a constant multiple of) the variance of a univariate Gaussian using \(n=((1/)/)\) samples. In high dimensions, privately learning the covariance matrix of a Gaussian in spectral norm requires \(n d^{3/2}\) samples , which is more than one needs to learn the mean under known covariance. Brown et al.  avoid the bottleneck of private covariance estimation, showing that the sample complexity of Gaussian mean estimation under known covariance with respect to Mahalanobis distance can in fact be matched, even when the covariance is unknown. Their tools also follow the propose-test-release approach and could be modified to fit our setting, but the privacy analysis would still require \(n d\). Singhal and Steinke  learn a subspace in which the majority of the data lie, which could be used as a pre-processing step, followed by projection. However, to recover the set of top \(k\) eigenvectors, they require that there exists a large gap between the two consecutive variances, that is, \(_{k}(d)_{k+1}\).

Comparison with .The paper by Aumuller et al.  is the closest work to ours, aiming to find sample-efficient mean estimators with respect to the Euclidean norm in the anisotropic case. Their work focuses on the less general case of diagonal, (almost) known covariance. The sample complexity of their estimator requires \(n\), whereas our estimator for the known covariance case is dimension-independent, and, as we prove, optimal. However, the focus in  is on estimators that satisfy the stricter privacy guarantee of \(\)-zCDP, which forces the need for dimension-dependent sample size. This is the key contrast with our dimension-free philosophy. As an interesting distinction,Aumuller et al.  provide accuracy guarantees with respect to the \(_{p}\) norm (the upper bounds) for slightly more general classes of so-called well-concentrated distributions, which include subgaussians. It would be interesting to establish optimal private mean estimation bounds with respect to general \(_{p}\) norms. In fact, the optimal non-private sample complexity of Gaussian mean estimation, with matching upper and lower bounds, with respect to general norms has been established only recently, and it depends on the Gaussian mean width of the set induced by the unit dual ball of the norm .

## 2 Preliminaries

We write \([n]=\{1,,n\}\), \(\) denotes the natural logarithm, and \(^{d}(c,r)\) denotes the \(d\)-dimensional Euclidean ball with radius \(r\) and center \(c\). We omit \(c\) if \(c=0\).

We introduce _differential privacy_ here. We say that \(X,X^{}\) are _neighboring data sets_ if either \( j[|X|]\) such that \(X^{}=X X^{(j)}\) or \( j[|X^{}|]\) such that \(X=X^{} X^{(j)}\).3 Differentially private algorithms have _indistinguishable_ output distributions on neighboring data sets.

**Definition 2.1** (\((,)\)-indistinguishability).: _Two distributions \(P,Q\) over domain \(\) are \((,)\)-indistinguishable, denoted by \(P_{,}Q\), if for any measurable subset \(W\),_

\[_{w P}[w W] e^{}_{w Q}[w W]+ _{w Q}[w W] e^{}_{w P}[w W]+.\]

**Definition 2.2** (Differential Privacy ).: _A randomized algorithm \(^{*}\) is \((,)\)-differentially private if for all neighboring data sets \(X,X^{}\) we have \((X)_{,}(X^{})\). We say that algorithm \(\) satisfies pure differential privacy if it satisfies the definition for \(=0\)._

Differential privacy satisfies several useful properties, such as post-processing and composition . For further details and guarantees of standard DP mechanisms, see Section A.

Our estimators will use the \(\) procedure of Tsfadia et al. , whose detailed definition is presented in Section 3. They provide a framework which allows us to extend an algorithm which is private _only for "easy" pairs of data sets_, to an algorithm that is private for any worst-case pair. "Easy" pairs of data sets are modelled with respect to a predicate \(f\) between two data points:

**Definition 2.3** (\(f\)-friendly, Def. 1.1 ).: _Let \(X\) be a data set over \(\) and let \(f:^{2}\{0,1\}\) be a predicate. We say \(X\) is \(f\)-friendly if for all \(x,y X\) there exists \(z\) such that \(f(x,z)=f(z,y)=1\)._

**Definition 2.4** (\(f\)-friendly DP, Def. 1.3 ).: _An algorithm \(\) is called \(f\)-friendly \((,)\)-DP if for any neighboring data sets \(X,X^{}\), such that \(X X^{}\) is \(f\)-friendly, \((X)_{,}(X^{})\)._

**Theorem 2.5** (Theorem 4.11 ).: _Let \(\) be an \(f\)-friendly \((,)\)-DP algorithm. Given data set \(X\), let \(=(X,f,=0)\) and \(C(X)=\{X^{(j)}\}_{\{j:\ v_{j}=1\}}\). Then \((X):=(C(X))\) is \((2(e^{}-1),2e^{+2(e^{}-1)})\)-DP._

We assume data are drawn from subgaussian distributions, which include Gaussians.

**Definition 2.6** (Subgaussian distributions).: _The random vector \(X\) with mean \(\) is subgaussian with a p.s.d. covariance matrix prox \(\) if for any \(\) and any \(v^{d}\), \(\,e^{(X-,v)} e^{^{2}v^{} v/2}\)._

**Lemma 2.7** (Norm of the subgaussian vector ).: _Let \(X=(X^{(1)},,X^{(n)})\) be drawn i.i.d. from the subgaussian distribution with mean \(\) and covariance-proxy \(\). With probability at least \(1-\), \(\|_{j=1}^{n}X^{(j)}-\|_{2}()/n}+ (1/)/n}\)._

## 3 Nearly-matching upper and lower bounds under known covariance

Algorithm 1 proceeds in two simple steps. The first step filters out outliers so that all remaining pairs of data points satisfy the re-scaled distance predicate \(_{M,}\) and, assuming enough data points remain, the second step releases their empirical mean along with appropriate Gaussian noise.

We retrieve the folklore result, by taking \(M=I_{d},()}\), which is known (otherwise, can be easily privately estimated as in Section 4). The filtering then guarantees that all pairs of pointsare within distance \(()}\), and adds spherical Gaussian noise with covariance \(()I_{d}/(^{2}n^{2})\). To retrieve the optimal bound, take \(M=\), which splits the privacy budget unevenly among coordinates. Then, \((^{1/2})}\) and the Gaussian noise has covariance \((^{1/2})^{1/2}/(^{2}n^{2})\), as in .

```
0: Data set \(X=(X^{(1)},,X^{(n)})^{T}^{n d}\). Privacy parameters: \(,>0\). Failure probability \(>0\). Symmetric invertible matrix \(M\). Parameter \(\).
1: Let \(_{M,}(x,y)=1\{\|M^{-1/4}(x-y)\|_{2}\}\).
2:\(=(X,_{M,}, =0)\).
3: Let \(C=\{X^{(j)}\}_{\{j:\,\,_{j}=1\}}\).
4: Compute \(_{C}=|C|-+z\) where \(z()\).
5:if\(|C|=0\) or \(_{C} 0\)then
6:return\(\).
7:return\(=_{x C}x+\) where \((0,}{^{2 }_{C}^{2}}M^{1/2})\).
8:procedure\((X,f,)\)\(\) Algorithm 4.3 from .
9:for\(j=1,,n\)do
10: Let \(z_{j}=_{k=1}^{n}f(X^{(j)},X^{(k)})-n/2\).
11: Sample \(v_{j}=(p_{j})\), where \(p_{j}=0,&z_{j} 0,\\ 1,&z_{j}(1/2-)n,\\ }{(1/2-)n},&.\)
12:return\(=(v_{1},,v_{n})\) ```

**Algorithm 1** Private Re-scaled Averaging: \(_{M,,,}(X)\)

**Theorem 3.1**.: _Let \((0,10),(0,1),>0,(0,1)\). Algorithm 1 is \((,)\)-differentially private. Let \(X\) be a data set of size \(n\), drawn from a subgaussian distribution with covariance-proxy \(\) and mean \(\). Given \(M=\), \((M^{-1/4} M^{-1/4})}+2 M^{-1/4}\|_{2}()}\), with probability at least \(1-\), Algorithm 1 returns \(\) such that \(\|-\|_{2}\), as long as_

\[n=()+\|\|_{2} }{^{2}}+(^{1/2})}}{}+}}{}+}{}\,\] (3)

_where \(\) hides constants and a log factor of the third term multiplied with itself._

The theorem holds more generally for any symmetric invertible \(M\), and \(\) satisfying the assumptions. We sketch the proof of Theorem 3.1 next. All remaining details are in Appendix B.

Proof sketch.: We start with the accuracy analysis. First we show that the original dataset \(X\) passes through \(\) (i.e., \(C=X\)) with high probability. It suffices to show that each pair \(j k[n]\), satisfies \(_{M,}(X^{(j)},X^{(k)})=1\) with probability \(1-/n^{2}\). Observe that for \(j k\), \(M^{-1/4}(X^{(j)}-X^{(k)})\) is subgaussian with mean 0 and covariance proxy \(2M^{-1/4} M^{-1/4}\). By Lemma 2.7 and our setting of \(\), indeed \(\|M^{-1/4}(X^{(j)}-X^{(k)})\|_{2}\) for each pair with probability \(1-/n^{2}\). We condition on \(C=X\). With high probability by the CDF of the Laplace distribution and since \(|C|=n=((1/)/)\), \(_{C}=(n)\). Thus, the algorithm does not abort, and returns estimate \(\). It remains to upper bound the total error of \(\). This is at most the error of the empirical mean plus the error due to noise \(\|\|_{2}\). By Lemma 2.7, with high probability, the former is \((()/n})\) and the latter \(((M^{1/2})}/( n))\). Substituting \(M=\), and the value for \(\), the total error becomes \((()/n}+(^{1/2})/(  n))\), which yields the stated sample complexity.

The privacy analysis follows the steps of [63, Claim 3.4]. By Theorem 2.5, it suffices to show that lines 4-7 of Algorithm 1, namely, \(\), are \(_{,}\)-friendly DP. Consider neighboring inputs \(X,X^{}\), differing in the \(n\)-th data point w.l.o.g., that is \(X^{}=X X^{(n)}\). Since \(||X|-|X^{}||=1\), by the guarantees of the Laplace mechanism, the r.v.s \(_{X},_{X^{}}\) in Line 4 are \((,0)\)-indistinguishable. Moreover, they both satisfy \(_{X},_{X}^{}<|X|\) with probability \(1-/2>1/2\). Conditioning on this event for the remainder of the sketch, we can fix \(_{X}=_{X}^{}=<|X|\), for some value \(\). If \( 0\), both runs abort. Otherwise,it suffices to show that Line 7 adds sufficient noise to maintain privacy. By post-processing, since \(M\) is not data-dependent, this is equivalent to ensuring that \((M^{-1/4}_{i=1}^{|X|}X^{(i)},v^{2}I_{d})_{ ,}(M^{-1/4}_{i=1}^{|X|-1}X^{(i)},v^{2}I_{d})\) where \(v=(2/)(/)\). This is true by the guarantees of the Gaussian mechanism applied to \(f(X)=M^{-1/4}_{i=1}^{|X|}X^{(i)}/|X|\), whose \(_{2}\)-sensitivity for \(_{,}\)-friendly \(X,X^{}\) can be upper bounded by \(2/|X| 2/\) (since \(0<<|X|\), by assumption). By composition, \(\) indeed satisfies \(_{M,}\)-friendly \((O(),O())\)-DP. 

We show that the sample complexity of Theorem 3.1 is optimal. We briefly explain our lower bound construction here. All remaining details are in Appendix C.

Proof Sketch of Theorem 1.2.: Let \(=(^{2})\). Assume w.l.o.g. that \(_{1}^{2}_{d}^{2}\). Partition the set of coordinates into buckets \(S_{k}=\{i[d]:_{i}_{1}(2^{-k},2^{-k+1}]\}\), \( k[(d)]\) and \(S_{(d)+1}=[d]_{k[(d)]}S_{k}\). We have that \(_{k[(d)+1]}_{i S_{k}}_{i}=\|\|_{1}\). Consider the bucket \(S\) which contributes the most to this sum and let \(_{S}\) be the maximum variance in this bucket. It must be that \(|S|\|_{1}}{((d)+1)_{S}}\). The lower bound of [39, Theorem 6.5] for isotropic Gaussians, implies that any \((,)\)-private mean estimator which returns, with constant probability, an estimate \(_{S}\) with error \(\) for the coordinates in \(S\) (note that they are all within a factor of \(2\)), requires \(n=(|S|_{S}/((d)))=(\|\|_{1} /(^{2}(d)))\) samples. As an estimator for the \(d\)-dimensional Gaussian mean restricted to \(S\), would give us such a \(_{S}\), the statement follows. 

## 4 Handling unknown covariance

In this section we consider the case of unknown covariance. First, recall that \((d^{3/2})\) samples are required to privately learn the covariance matrix in spectral norm , which is prohibitive. The lower bound instance is an almost-isotropic Gaussian, which means that anisotropic distributions may circumvent it. Still, the superlinear dependence on \(d\) implies that this approach will yield suboptimal sample complexity for mean estimation. Avoiding private covariance estimation, Brown et al.  propose a "covariance-aware" private mean estimator which returns the mean with Gaussian noise which scales with the empirical covariance matrix of the data set \(_{X}\), as \((0,_{M}^{2}_{X}/(^{2}n^{2}))\) for appropriate factor \(_{M}^{2}\). Since adding data-dependent noise can break privacy, a pre-processing step is required to ensure that no outliers exist in the data set with respect to the empirical covariance, roughly ensuring that \(\|_{X}^{-1/2}(X^{(k)}-X^{(j)})\|_{2}_{M}\), for all \(j k[n]\). In our case, to maintain the accuracy guarantee of the known-covariance case, the Gaussian noise should be \((0,^{2}_{X}^{1/2}/(^{2}n^{2}))\) and all data points should satisfy \(\|_{X}^{-1/4}(X^{(k)}-X^{(j)})\|_{2}\). Note that \(n()/\|\|_{2}\) samples suffice for the empirical covariance to be close to the true covariance \(\) in spectral norm , so applying the algorithm from  could maintain accuracy while still allowing a dimension-free sample complexity. Unfortunately, we still cannot use this approach because \(n d\) samples are required for the privacy analysis to go though, namely, for neighboring data sets \(X,X^{}\) it holds that \((0,_{X}^{1/2})_{,}(0, _{X^{}}^{1/2})\) for \( d/n\), which forces us to take \(n d/\) samples. The same is true for the follow-up works of  which give polynomial-time versions of this algorithm with slightly better statistical guarantees.

Luckily, our accuracy guarantee does not require the variance estimate in all directions to be accurate. For example, consider all directions with variance at most \(\|\|_{2}/d\). Adding spherical Gaussian noise to these directions maintains a dimension-free error, without requiring tighter estimates for their variance. Thus, on a high level, our approach for mean estimation in the unknown covariance case is to identify and estimate as many of the top variances as our sample size allows, which turns out to be \(k^{2}n^{2}\), while adding spherical Gaussian noise to the remaining ones.

We sketch the proof of the following theorem. All remaining details are in Appendix D.

**Theorem 4.1**.: _Let parameters \(,(0,1)\). Let \(X(,)^{n}\) with unknown covariance \(\). Algorithm 2 is \((,)\)-differentially private and, with probability \(1-\), returns an estimate \(\) such that \(\|-\|_{2}\), as long as_

\[n=(^{2}(d)+()}{^{2}}+ ^{d}_{ii}^{1/2}}}{ }+^{d}_{ii}^{1/2}}^{5/4}()(d)}{})\,\] (4)

_where the symbol \(\) hides multiplicative logarithmic factors in \(1/\)._

Next, we describe Algorithm 2 and introduce some of its subroutines along with their guarantees. All omitted proofs are in Appendix D. Our algorithm receives a data set \(X^{(1)},,X^{(n)}\), where each \(X^{(i)}\) is a \(d\)-dimensional vector distributed as \((,)\). The algorithm starts by splitting the dataset into \(m= n/(2)\) groups each of size \(2\), where \(=( d)\). Denote the elements of each group \(j\) by \(X^{(j,1)},,X^{(j,2)}\). Within each group \(j\), for each coordinate \(i\), we compute an estimate \(V_{i}^{(j)}\) for \(_{ii}\): \(V_{i}^{(j)}=_{r=1}^{}(X_{i}^{(j,2r-1)}-X_{i}^{(j,2r)}) ^{2}\). For convenience, we define what it means for the \(V_{i}^{(j)}\) variables to provide a good estimate of the set of \(\{_{ii}\}_{i[d]}\).

**Definition 4.2**.: _Given variances \(_{11},,_{dd}\) and given a set of estimates, \(V=\{V_{i}^{(j)}\}_{j[m],i[d]}\), we say that \(V\) is valid if \(|\{j i[d],\ _{ii}/2 V_{i}^{(j)} 2_{ii}\}|  4m/5\)._

Proof sketch of Theorem 4.1.: For ease of notation, we assume that \(=(^{2})\). We start from the accuracy analysis. Assume \(n\) satisfies the sample complexity bound of Eq. (4). By Chernoff bound, since \(=((d))\) and \(m=((1/))\), with probability \(1-\), \(V\) is valid. We use the estimates \(V_{i}^{(j)}\) as inputs to private procedures: \((V,k)\) (compute the \(k\)-th largest variance up to a multiplicative constant), \((V,I)\) (compute the sum \(_{i I}_{i}^{2}\) up to a multiplicative constant), and \((V,R,k)\) (identify the indices of the at-most-\(k\) largest variances \(_{i}^{2} R\)). The first two tasks can be implemented by the Stable Histogram algorithm  if \(m=((1/)/)\). \(\) can be implemented via the Sparse Vector technique , if \(m=((d/)/)\). Both are satisfied for the given \(m=n/2\). With these procedures, the algorithm privately learns the \(k\)-th largest variance \(R\), identifies the top \(k\) coordinates, \(I_{}\), and learns estimates \(\{_{i}\}_{I_{}}\) that are accurate up to a multiplicative constant.

Next, we estimate the mean \(\) in the coordinates \(I_{}\) separately from \(I_{}:=[d] I_{}\), denoted by \(_{}\) and \(_{}\), respectively. Denote vector \(_{}=(\{_{i}\}_{i I_{ }})\) and \(_{}=(\{_{i}\}_{i I_{ }})\). To estimate \(_{}\), Algorithm 1, given input vectors \(X^{(1)},,X^{(n)}\), restricted to coordinates \(I_{}\)diagonal matrix \(M\), with \(M_{ii}=_{i}^{2}\) (assume that the rows and columns of \(M\) are indexed by \(I_{}\)), and \(_{}\|_{1}}\), returns an estimate \(_{}\) with error \(\) (since the sample complexity of Eq. (4) is larger than the one required by Theorem 3.1).

To estimate \(_{}\), we do not know the variances, so we use the naive approach. We first call \(\) once again, to provide an estimate \(\) of \(\|_{}\|_{2}\) up to a multiplicative constant. Given \(\), we again call Algorithm 1, now for a \((d-k)\)-dimensional estimation problem. Given input vectors \(X^{(1)},,X^{(n)}\), restricted to coordinates \(I_{}\), matrix \(M=I_{d-k}\), and \(\), Algorithm 1 returns an estimate \(_{}\) with error \(\) as long as \(n=(\|_{}\|_{2}/( ))\). By Holder's inequality, \(\|_{}\|_{2}_{ }\|_{1}\|_{}\|_{}}\|_{1}\|_{}\|_{}}\). By the guarantees of \(\) and \(\), \(\|_{}\|_{}\) is smaller than the \(k\)-th largest variance of \(\) up to a multiplicative constant, which, in turn, must be smaller than \(\|\|_{1}/k\). Substituting this above, we obtain that it suffices for the stated sample complexity to additionally satisfy \(n=(\|\|_{1}/( ))\), which can be confirmed by substituting the definition for \(k\).

The privacy guarantee follows directly by composition of \(O(1)\)\((,)\)-DP mechanisms. 

_Remark 1_.: We note that the sample complexity of Algorithm 2 in fact depends on the decay of the diagonal elements of \(\), and can yield improved bounds for easier instances. In particular, the error of the algorithm due to privacy is in the order of \(\|_{I_{}}\|_{1}/( n)+}|}\|_{I_{}}\|_{2}/( n)\). Thus, if \(\) follows an exponential decay, i.e., the \(i\)-th largest variance is proportional to \(e^{-(i-1)}\), or all \(_{}\) variances are smaller than \(\|\|_{1}/d\), then it suffices to learn only the top \(k=(d)\) variances and the error almost matches that of the known-covariance case, up to additional logarithmic factors in \(d\), \(1/\). Moreover, identifying easier instances is possible by computing a private histogram over \((d)\) buckets of the form \((2^{-j},2^{-j+1}]\|\|_{}\), given \(n=((d)/)\) samples [16; 41].

Thus, we can determine special cases where the decay of \(\) allows us to achieve the optimal rate of Theorem 1.1 even with unknown diagonal covariance. But without further assumptions, our algorithm has sample complexity that depends on \(d^{1/4}\). The question of the optimal sample complexity for mean estimation in the case of unknown covariance, which captures anisotropic distributions, remains open.

## 5 Conclusion and future work

We present \((,)\)-differentially private mean estimators for subgaussian distributions with error \(\) as measured in Euclidean distance, with high probability, as long as the sample size is \(n=(()/^{2}+(^{1/2 })/())\). The sample complexity is thus dimension-independent when the covariance is highly anisotropic. We show that this is the optimal sample complexity for this task up to logarithmic factors. We also present an algorithm in the more challenging case of unknown covariance, whose sample complexity has improved dependence on the dimension, that is, \(d^{1/4}\).

In the known covariance case, the dependence on \((1/)\) could possibly be decoupled from the \((^{1/2})/()\) term. This is an artifact of the Gaussian noise added for privacy and can possibly be avoided using mean estimators based on the exponential mechanism, as in the spherical Gaussian case [13; 3; 35], but the volumetric arguments involved in their analysis incur factors dependent on \(d\), which seem hard to overcome.

A more interesting direction for future work is the case of unknown covariance. We can determine special cases where the decay of \(\) allows us to achieve the optimal rate of Theorem 1.1 with unknown diagonal covariance. What is the appropriate norm in which one needs to learn \(\) for the current known-covariance approach to be accurate, and how many samples are needed for this task privately? More generally, the optimal sample complexity of mean estimation in the unknown (even diagonal) covariance case for anisotropic distributions (possibly achieved by an algorithm which doesn't follow the same structure) is an open question.