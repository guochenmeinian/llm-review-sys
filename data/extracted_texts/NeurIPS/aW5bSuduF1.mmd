# Drift doesn't Matter: Dynamic Decomposition with Diffusion Reconstruction for Unstable Multivariate Time Series Anomaly Detection

Drift doesn't Matter: Dynamic Decomposition with Diffusion Reconstruction for Unstable Multivariate Time Series Anomaly Detection

 Chengsen Wang1

Zirui Zhuang1

Qi Qi2

Jingyu Wang2

Xingyu Wang

Haifeng Sun

Jianxin Liao

State Key Laboratory of Networking and Switching Technology,

Beijing University of Posts and Telecommunications, Beijing, China

{cswang, zhuangzirui, qiqi8266, wangjingyu}@bupt.edu.cn

{wangxingyu, hfsun, liaojx}@bupt.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Many unsupervised methods have recently been proposed for multivariate time series anomaly detection. However, existing works mainly focus on stable data yet often omit the drift generated from non-stationary environments, which may lead to numerous false alarms. We propose **D**ynamic **D**ecomposition with **D**iffusion **R**econstruction (D\({}^{3}\)R), a novel anomaly detection network for real-world unstable data to fill the gap. D\({}^{3}\)R tackles the drift via decomposition and reconstruction. In the decomposition procedure, we utilize data-time mix-attention to dynamically decompose long-period multivariate time series, overcoming the limitation of the local sliding window. The information bottleneck is critical yet difficult to determine in the reconstruction procedure. To avoid retraining once the bottleneck changes, we control it externally by noise diffusion and directly reconstruct the polluted data. The whole model can be trained end-to-end. Extensive experiments on various real-world datasets demonstrate that D\({}^{3}\)R significantly outperforms existing methods, with a 11% average relative improvement over the previous SOTA models. Code is available at [https://github.com/ForestsKing/D3R](https://github.com/ForestsKing/D3R).

## 1 Introduction

Due to the rarity of anomalies, unsupervised anomaly detection of multivariate time series is an essential area in data mining and industrial applications. Reconstruction-based models are commonly used in unsupervised anomaly detection. The reconstruction error is small for normal series while large for abnormal series. Based on this principle, most anomalies can be detected without the label. In the real world, the temporal patterns typically change over time as they are generated from non-stationary environments. For example, the growth in the popularity of a service would cause customer metrics (e.g., request count) to drift upwards over time. Ignoring these factors would cause a deterioration in the performance of the anomaly detector. Despite the enormous advancements of previous research, most focus on stable data. Figure 1 illustrates how the anomaly scores provided by existing approaches tend to increase in the red area when the data is unstable, leading to false alarms.

To handle the unstable data, we attempt to decompose it into a stable component and a trend component, focusing more on the stable part in the reconstruction procedure. However, the trend part cannot be ignored completely, as the drift may be anomalous. There are still two challenges in this context: **Challenge 1**: Limitation of the decomposition for long-period time series. Most classical decomposition algorithms [2; 17; 27] are static, making it hard to be applied in the real world, where the data is updated in real time. With deep learning, some dynamic algorithms have been proposed; however, they are mainly based on averaging  or Fourier Transform [28; 26] within a local sliding window. Fundamentally, they can not apply to the data whose period is larger than the size of the sliding window. **Challenge 2**: High training cost of adjusting the information bottleneck in the reconstruction procedure. The information bottleneck is critical for reconstruction-based models, and adjusting it is difficult. If it is too small, normal data will be poorly reconstructed; if it is too large, abnormal data will be successfully reconstructed. Previous methods most rely on an internal information bottleneck, such as the latent space size. As the information bottleneck is an attribute of the model itself, any change to the bottleneck necessitates retraining the model.

After analyzing numerous data series, we systematically organize the distribution shift into two categories. Vertical drift refers to statistical characteristics such as mean changing over time, while horizontal drift refers to values shifting left or right at similar times in various periods. In response to the above challenges, we propose **D**ynamic **D**ecomposition with **D**iffusion **R**econstruction (D\({}^{3}\)R) for long-period unstable multivariate time series anomaly detection. To solve Challenge 1, we utilize timestamps as external information to overcome the limitations of the local sliding window. Specifically, the data-time mix-attention and offset subtraction are utilized to solve vertical and horizontal drift, respectively. Moreover, we introduced a disturbance strategy in training to increase the robustness of the model. To solve Challenge 2, we propose a new method named noise diffusion to control the information bottleneck externally. The diffusion  provides a new view of the information bottleneck, treating the noise as a bottleneck and the unpolluted information as a condition. Because the bottleneck is no longer an attribute of the model itself, the different sizes can be set during revision without retraining the model.

The contribution of our paper is summarised as follows:

* A novel dynamic decomposition method for long-period multivariate time series is proposed. It effectively utilizes external information to overcome the limitations of the local sliding window.
* A new approach to control information bottleneck externally by noise diffusion is also proposed. It avoids the high training cost of adjusting the information bottleneck.
* Based on the findings for the unstable data when taking unsupervised anomaly detection, we propose a novel anomaly detection network called D\({}^{3}\)R. The D\({}^{3}\)R achieves the new SOTA results on various real-world datasets and significantly outperforms baselines on unstable datasets.

## 2 Related work

As a significant real-world problem, unsupervised anomaly detection of multivariate time series has received much attention. According to the criterion for anomaly detection, the paradigms roughly fall

Figure 1: Anomaly scores of existing methods on SMD and SWaT datasets. All moments are normal. Vertical drift occurs in the red area. The moment with a higher anomaly score is more likely to be identified as an anomaly.

into the categories of probability-based, linear transformation-based, proximity-based, outlier-based, and neural network-based approaches.

As for the probability-based approach, [12; 13] are modeled by a statistical probability function on a multivariate cumulative distribution, providing anomaly scores based on the probability of occurrence of the sample. In the linear transformation-based method, [20; 22] begin by mapping the multivariate data before determining the boundary from the mapping space. Proximity-based algorithms, such as [18; 5], seek to cluster data based on similarity and then calculate intra-cluster and inter-cluster distances. In the outlier-based approach, [14; 16] compare the outliers degree of the testing samples with the training samples to identify the anomaly.

Most of the approaches mentioned above do not consider the temporal continuity of series. In recent years, neural network-based methods have become increasingly significant. Current neural network-based approaches can be divided into prediction-based and reconstruction-based. Although prediction-based methods [21; 8] are effective in modeling for the next timestamp, they are susceptible to interference from historical information. Reconstruction-based approaches [32; 11] do a great job capturing the distribution across the whole series. However, they are sensitive to the size of the information bottleneck.

Furthermore, existing works mainly focus on stable data, and their performance may suffer significantly in the real world, where the drift is frequent.

## 3 Method

An input of multivariate time series anomaly detection is denoted by \(^{n k}\), where \(n\) is the length of timestamps, and \(k\) is the number of variables. The task is to produce an output vector \(^{n}\), where \(y_{i}\{0,1\}\) denotes whether the \(i^{th}\) timestamp is an anomaly.

### Overview

The overall architecture of D\({}^{3}\)R is shown in Figure 2. The dynamic decomposition module first models the data and timestamp features by the data encoder and time encoder. Next, it uses stacked decomposition blocks to extract the stable component. Finally, we get the trend component by offset subtraction. The diffusion reconstruction module utilizes noise diffusion to construct information bottleneck externally and then directly reconstructs the polluted data by the backbone network. Reconstruction error is the anomaly score. In order to model both temporal dependency and dimension dependency, the data encoder and the reconstruction backbone network both consist of stacked spatial

Figure 2: The architecture of D\({}^{3}\)R mainly consists of two modules: dynamic decomposition and diffusion reconstruction. The detailed architecture of the decomposition blocks is shown on the right panel. The detailed architecture of the reconstruction backbone network is shown on the left panel.

temporal transformer blocks. To increase the robustness of the model, we also propose a disturbance strategy during training.

### Data preprocessing

We perform timestamp hard embedding, labeled stable component construction, and disturbance strategy for the input. Timestamp hard embedding is applied to the training and testing sets, while labeled stable component construction and disturbance strategy are applied to the training set only.

Timestamp hard embeddingTo make better use of timestamps in anomaly detection, we hard-code the timestamps of \(^{n 1}\) into an embedding \(_{}^{n 5}\) like , with each dimension representing minute of the hour, hour of the day, day of the week, day of the month, and month of the year.

Labeled stable component constructionWe extract the trend \(^{n k}\) by moving average, then the labeled stable component \(=-,^{n k}\) is obtained. The construction of the labeled stable components prevents our model from being disturbed when the training data is unstable.

Disturbance strategyTo increase the robustness of the model, we add a vertical drift \(^{k}\) sampled from a \([-p,p]\) uniform distribution to each variable of the training data. The final input of dynamic decomposition module is \(_{}=+,_{}^{n k}\).

### Dynamic decomposition

The dynamic decomposition module consists of a data encoder, a time encoder, stacked decomposition blocks, and an offset subtraction. The data encoder is implemented based on the spatial-temporal transformer block, which captures the temporal and dimension dependency. The output of data encoder is \(_{}^{n d_{}}\), where \(d_{}\) is hidden state dimension in the model. The time encoder consists only of the temporal transformer block, which models the temporal correlation of timestamps to obtain \(_{}^{n d_{}}\). Data-time mix-attention constitutes the subject of stacked decomposition blocks, which are used to extract the stable component \(}^{n k}\). Finally, to solve the challenges of horizontal drift, we obtain the trend component \(}_{}^{n k}\) through offset subtraction.

Spatial-temporal transformer blockThe architecture of the spatial-temporal transformer block is shown in the solid line box on the left panel of Figure 2. Assuming the input of \(l^{th}\) layer is \(^{l}^{n d_{}}\) with \(n\) timestamps and \(d_{}\) dimensions. In the temporal transformer, we obtain temporal relationship \(^{l}_{}^{n d_{}}\) by directly applying multi-head self-attention (MSA) to \(n\) vectors of size \(d_{}\):

\[}^{l}_{}& =(^{l}+( ^{l},^{l},^{l}))\\ ^{l}_{}&= (}^{l}_{}+(}^{l}_{})) \]

where \(}^{l}_{}\) is intermediate variable. \(()\) denotes layer normalization as widely adopted in , \(()\) denotes a multi-layer feedforward network, \((,,)\) denotes the multi-head self-attention  layer where \(,,\) serve as queries, keys and values. In the spatial transformer, we obtain dimension relationship \(^{l}_{}^{n d_{}}\) by applying MSA to \(d_{}\) vectors of size \(n\):

\[}^{l}_{}& =((^{l})^{ }+((^{l})^{},(^{ l})^{},(^{l})^{}))\\ ^{l}_{}&= ((}^{l}_{})^{}+((}^{l}_{})^{} )) \]

where \(}^{l}_{}\) is intermediate variable. \(()^{}\) denotes the transposition of matrices. Finally, we obtain the input of \(l+1^{th}\) layer \(^{l+1}^{n d_{}}\) by:

\[}^{l+1}& =(^{l}_{} ^{l}_{})\\ ^{l+1}&=((}^{l+1})) \]where \(}^{l+1}^{n 2d_{}}\) is intermediate variable, and \(\) represents concatenation.

Data-time mix-attentionThe original self-attention \(_{}(,,)\) only models the data information and ignores the role of timestamps. We define the data-time mix-attention as:

\[_{}(_{},_{}, _{},_{},_{})= (_{}_{}^{ }+_{}_{}^{}}{}}})_{} \]

where \(()\) is conducted row by row like [25; 34; 29], \(_{},_{}^{n d_{}}\) are length-n data queries, data keys of \(d_{}\) dimension mapped from \(_{}\), respectively. \(_{},_{},_{}^{n d_{}}\) are time queries, time keys and time values of \(d_{}\) dimension mapped from \(_{}\), respectively. \(^{3}\) learns to autonomously merge data and time information by mapping them into attention space.

Decomposition blockThe structure of stacked decomposition blocks is shown on the right side of Figure 2. Assuming the input of \(l^{th}\) layer is \(_{}^{l}\) and \(_{}^{l}\), we can obtain the sub-stable components \(}^{l}^{n k}\) by:

\[}^{l}&=_{}(_{}^{l},_{}^{l}, _{}^{l},_{}^{l},_{}^{l})\\ }^{l}&=( }^{l}+_{}( }^{l},}^{l},}^{l}))\\ }^{l}&= (}^{l}+(}^{ l}))\\ }^{l}&=( }^{l}) \]

where \(}^{l},}^{l},}^{l} ^{n d_{}}\)are intermediate variables. \(()\) is a single linear layer. Then we can obtain the input of \(l+1^{th}\) layer \(_{}^{l+1}=_{}^{l}-}^{l}\) and \(_{}^{l+1}=_{}^{l}\). Finally, the sub-stable components \(}^{l}\) of all stacked decomposition blocks are summed to obtain the stable component \(}\) of the series.

Offset subtractionAlthough the same time slots in different periods usually have similar fluctuations, it is not strictly one-to-one correspondence. Here, we employ a simple method to handle the horizontal drift, considering the vector \(\) minus the vector \(\), with a maximum horizontal offset \(d\):

\[(_{i},)=(_{i}-_{i-d},,_{i}-_{i +d}) \]

where \(_{i}\) and \(_{i}\) are the \(i^{th}\) element in \(\) and \(\), respectively. \(()\) means taking the minimum value. Based on the offset subtraction, we obtain the predicted trend component as \(}_{}=(_{},})\).

### Diffusion reconstruction

The diffusion reconstruction module mainly consists of a noise diffusion and a reconstruction backbone network. The noise diffusion is used to construct an information bottleneck externally by polluting the input data with noise, and the backbone network is used to reconstruct the polluted data directly.

Noise diffusionGiven the original data \(x_{0}\), the polluted data \(x_{1},x_{2},,x_{T}\) are obtained by adding \(T\) step Gaussian noise. Each moment \(t\) in the noise addition process is only relevant to the \(t-1\) moment. The hyperparameter \(\) controls the ratio of added noise. We add trend retaining to the DDPM so that the model can concentrate on the more crucial stable components. The data \(x_{t}\) after \(t\) steps of noise pollution is:

\[x_{t}=_{t}}x_{0}+_{t}}_{t}+( 1-_{t}})b \]

where \(_{t}=_{i=1}^{t}_{i},_{i}=1-_{i}\), \(_{t}(0,1)\) is the noise, and \(b\) is the retained information. The proof is given in Appendix A. So we can obtain the noisy data:

\[_{}^{}=_{t}}_{}+ _{t}}}+(1-_{t}})}_{} \]

where\(_{}^{}^{n k},} (0,)\), and \(}^{n k}\).

Backbone networkAs shown in the left panel of Figure 2, the backbone network consists of stacked spatial-temporal transformer blocks. The extracted trend \(}_{}\), the noisy data \(_{}^{}\), and the timestamp \(_{}\) are encoded by the embedding layer first. To make the network more focused on the stable part without completely ignoring the trend part, we make the hidden vector:

\[=(_{}^{})-(}_{})+(_{}) \]

where\(^{n d_{}}\), \(()\) is the embedding network. Then, we model the temporal and dimension relationship by feeding \(\) into the stacked spatial-temporal Transformer blocks. After that, we sum the outputs of the stacked blocks with \(}_{}\) and send the result into the output layer. Finally, we directly obtain the reconstructed result \(}_{}\) of \(_{}\) instead of the predicted noise.

### Joint optimization

As described in the previous section, the dynamic decomposition and diffusion reconstruction are interconnected. The dynamic decomposition module learns the inherent features of long-period unstable multivariate time series, i.e., the stable component. We use the Mean Square Error (MSE) between \(\) and \(}\) as the loss of this module. The diffusion reconstruction module directly reconstructs the data polluted by noise diffusion. It is worth noting that the output of this module is the reconstruction of \(_{}\). Because the drift \(\) is added inside \(^{3}\), and the external who calculates the loss is only aware of \(\), we obtain the reconstruction of \(\) by \(}=}_{}-,} ^{n k}\). Similar to the dynamic decomposition module, the MSE of \(\) and \(}\) is used directly as the loss of this module.

During the training process, \(^{3}\) needs to be trained end-to-end, so the loss function is defined as the sum of the two optimization objectives:

\[Loss=_{i=1}^{n}_{j=1}^{k}((_{i,j}- {}_{i,j})^{2}+(_{i,j}-}_{i,j} )^{2}) \]

where \(_{i,j},}_{i,j},_{i,j},}_{i,j}\) represent the labeled stable component, the predicted stable component, the original input and the reconstructed output of the \(j^{th}\) variable at the \(i^{th}\) timestamp, respectively.

### Model inference

The online detection (inference) does not need labeled stable component construction and disturbance strategy in the data preprocessing stage. The anomaly score for the current timestamp consists only of the MSE of the original input \(\) and the reconstructed output. Then, we run the SPOT , a streaming algorithm based on extreme value theory, to label each timestamp.

## 4 Experiments

### Experimental settings

DatasetsWe evaluate \(^{3}\) extensively on three real-world datasets: PSM (Pooled Server Metrics) , SMD (Server Machine Dataset) , and SWaT (Secure Water Treatment) . For each dataset, we only preserve continuous variables. That is why we do not employ MSL (Mars Science Laboratory rover) and SMAP (Soil Moisture Active Passive satellite) , which are discrete except for the first dimension. The normal data is divided into training data (80%) and validation data (20%). Anomaly is only present in the testing data. More descriptions of the datasets are shown in Table 1. Further details about the datasets are available in Appendix B.1.

    & Training & Testing & Series & Attacks & Anomaly & Anomaly & Frequency & ADF \\  & Size & Size & Number & Number & Durations & Rate & Frequency & Test Statistic \\  PSM & 132481 & 87841 & 25 & 73 & 1\(\)8861 & 0.2776 & 1 minute & -9.2314 \\ SMD & 23688 & 23689 & 33 & 30 & 3\(\)3161 & 0.1565 & 1 minute & -4.0947 \\ SWaT & 6840 & 7500 & 25 & 33 & 3\(\)599 & 0.1263 & 1 minute & -2.9442 \\   

Table 1: Statistics of the datasets. A smaller ADF test statistic indicates a more stationary dataset.

BaselinesWe extensively compare our model with 15 baselines, including the probability-based methods: Sampling, COPOD , ECOD ; the linear transformation-based methods: OCSVM , PCA ; the proximity-based methods: kNN , CBLOF , HBOS ; the outlier-based methods: IForest , LODA ; the neural network-based methods: VAE , DeepSVDD , Lstm-AE , MTAD-GAT , TFAD , Anomaly Transformer . Additionally, we set up an adversary algorithm, which is implemented simply. A timestamp will be marked as abnormal if its sequential id is divisible by \(n\), else it will be marked as normal. In our experiment, \(n\) is set as 40. This adversary algorithm works as a timed detector and offers no information about the location of the anomaly. All baselines are based on our runs, using the identical hardware. We employ official or open-source implementations published in GitHub and follow the configurations recommended in their papers. Further details concerning the Baselines are available in Appendix B.2.

MetricsFor performance evaluation, we use precision, recall, and F1 score. The classical metrics are reasonable for tasks with sample granularity but are not applicable for continuous time series, where anomalies are frequently continuous. Most previous work [30; 33; 3; 21] employ the point adjustment method: If a point in a contiguous anomalous segment is detected correctly, all anomalies in the same segment are also considered to have been correctly detected. However, point adjustment is unreasonable, as  pointed out, and creates the illusion of progress. Assuming that ground truth and the predicted event are shown in Figure 3, although the real predicted event is just a timed detection, it still achieves an F1 score of 0.91 after point adjustment. The point adjustment may increase TP and decrease FN dramatically . We test all baselines on all datasets with point adjustment metrics. The detailed experiment results can be found in Appendix C. Experimental results show that the average F1 score of the adversary algorithm surprisingly outperforms all baselines, even though it can provide no anomaly location information. This phenomenon arises because point adjustment unfairly assigns higher weights to long anomaly events. Furthermore, this algorithm does not consider the adjacency of time series . To address the aforementioned challenges, we employ an F1 score based on affiliation . This score takes into account the average directed distance between predicted anomaly events and ground truth events to calculate precision, as well as the average directed distance between ground truth events and predicted anomaly events to determine recall.

    &  &  &  &  \\ Method & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & F1 \\  Sampling & 0.8439 & 0.5165 & 0.6408 & 0.7453 & 0.3144 & 0.4422 & 0.6062 & 0.8466 & 0.7065 & 0.5965 \\ COPOD & 0.7602 & 0.3175 & 0.4479 & 0.6676 & 0.1366 & 0.2268 & 0.9876 & 0.1180 & 0.2108 & 0.2951 \\ ECOD & 0.7460 & 0.3384 & 0.4656 & 0.7398 & 0.1615 & 0.2651 & 0.9761 & 0.1151 & 0.2059 & 0.3122 \\ OCSVM & 0.8761 & 0.4744 & 0.6155 & 0.0000 & 0.0000 & 0.0000 & 0.6196 & 0.7558 & 0.6810 & 0.4321 \\ PCA & 0.9220 & 0.3771 & 0.5353 & 0.8388 & 0.4019 & 0.5434 & 0.6338 & 0.7218 & 0.6761 & 0.5849 \\ kNN & 0.5317 & 1.0000 & 0.6943 & 0.6988 & 0.3365 & 0.4546 & 0.0000 & 0.0000 & 0.0000 & 0.3830 \\ CBLOF & 0.5990 & 0.9845 & 0.7449 & 0.8667 & 0.3352 & 0.4834 & 0.6308 & 0.7091 & 0.6677 & 0.6320 \\ HBOS & 1.0000 & 0.0654 & 0.1228 & 0.5628 & 0.8007 & 0.6610 & 0.5771 & 0.8049 & 0.6722 & 0.4853 \\ IForest & 1.0000 & 0.0335 & 0.0064 & 1.0000 & 0.0937 & 0.1713 & 0.6127 & 0.6280 & 0.6203 & 0.2855 \\ LODA & 0.9266 & 0.4017 & 0.5605 & 0.5902 & 0.6618 & 0.6240 & 0.6117 & 0.7014 & 0.6535 & 0.6126 \\ VAE & 0.6221 & 0.8772 & 0.7280 & 0.8209 & 0.4349 & 0.5686 & 0.6355 & 0.7218 & 0.6759 & 0.6575 \\ DeepSVDD & 0.7405 & 0.5064 & 0.6015 & 0.6498 & 0.6477 & 0.6488 & 0.5911 & 0.9353 & 0.7244 & 0.6582 \\ LSTM-AE & 0.7511 & 0.7586 & 0.7548 & 0.8496 & 0.4349 & 0.5753 & 0.6018 & 0.7219 & 0.6564 & 0.6622 \\ MTAD-GAT & 0.7990 & 0.6014 & 0.6863 & 0.8590 & 0.6769 & 0.7571 & 0.6590 & 0.7751 & 0.7123 & 0.7186 \\ TFAD & 0.7914 & 0.7163 & 0.7520 & 0.5632 & 0.9783 & 0.7149 & 0.6038 & 0.8196 & 0.6953 & 0.7207 \\ Anomaly Transformer & 0.5201 & 0.8504 & 0.6455 & 1.0000 & 0.0319 & 0.0619 & 0.5451 & 0.5994 & 0.5759 & 0.4278 \\ Adversary & 0.5351 & 0.8971 & 0.6703 & 0.5135 & 0.9663 & 0.6706 & 0.5410 & 0.7531 & 0.6297 & 0.6569 \\  Our & 0.6294 & 0.9619 & **0.7609** & 0.7715 & 0.9926 & **0.8682** & 0.7206 & 0.8529 & **0.7812** & **0.8034** \\   

Table 2: Results in the three real-world datasets. The higher values for all metrics represent the better performance, and the best F1 scores are highlighted in bold.

Figure 3: Example of point adjustment strategy. The red area represents the ground truth of the anomaly event. The precision/recall/f1 before adjustment (Real) is 0.20/0.05/0.08, while after adjustment (Adjusted) is 0.84/1.00/0.91.

Detailed implementation of D\({}^{3}\)R can be found in Appendix B.3. All experiments are repeated 5 times, and we report the average for each metric.

### Detection Results

On all three real-world datasets, as shown in Table 2, D\({}^{3}\)R outperforms the adversary algorithm and achieves the best F1 performance, confirming its effectiveness and superiority. Specifically, we achieve 0.61% (0.7548\(\)0.7609), 11.11% (0.7571\(\)0.8682), and 5.68% (0.7244\(\)0.7812) absolute improvement over the previous SOTA methods on PSM, SMD, and SWaT datasets, respectively.

The statistical and machine learning methods frequently perform poorly in generalization because they do not account for time series continuity. They may perform well on a partial dataset while performing poorly or even failing on another (e.g., OCSVM and CBLOF). In opposition, neural network-based methods usually perform more balanced over various datasets. Furthermore, much of the previous works were evaluated by point adjustment, leading to a false boom. The metrics based on affiliation provide a more objective and reasonable evaluation of various methods, although their scores will drop.

Noticeably, D\({}^{3}\)R significantly outperforms the other methods on the SMD and SWaT datasets, which are typically characterized by high nonstationarity, suggesting the limitation of previous work on unstable real-world data. In our model, the dynamic decomposition and diffusion reconstruction modules complement each other. The former dynamically eliminates the unstable interference from the original data. The latter models the series from more crucial components. These designs tackle the shortcomings of previous work and maintain excellent robustness in complex real-world data.

In addition to the affiliation evaluation metric, we compare D\({}^{3}\)R with other baselines in all datasets using the AUC score. The detailed experiment results can be found in Appendix D. For a more intuitive comparison, we visualize the anomaly scores of the D\({}^{3}\)R and partial baselines, as shown in Appendix E. The quantitative and qualitative experimental results all show that D\({}^{3}\)R outperforms other baselines. Moreover, to validate the practicality of D\({}^{3}\)R in the actual production environment, we compare run time across different neural network-based algorithms in the SMD and present a summary in Appendix F. Both the training time and inference time of our model are acceptable.

### Effectiveness verifications

Dynamic decomposition moduleWe propose a dynamic decomposition module to tackle Challenge 1. To more intuitively verify the performance of the dynamic decomposition module, we provide visualization of the decomposition results on highly unstable datasets (SMD and SWaT). Figure 4(a) shows that our model robustly extracts the fundamental stable components, although real-world multivariate data is long-period and complex.

Diffusion reconstruction moduleWe propose a diffusion reconstruction module to tackle Challenge 2. To verify the superiority of controlling information bottleneck externally, we replace the entire diffusion reconstruction module with the VAE module and perform sensitivity analyses for the VAE module and diffusion reconstruction module. The hyperparameter that significantly af

Figure 4: Results of effectiveness analyses. (a) is the visualization in the dynamic decomposition. (b) is the statistic results of sensitivity analyses in the diffusion reconstruction module.

fects the performance of the VAE module is the latent space size \(z\). The hyperparameters that may affect the performance of the diffusion reconstruction module are the hidden state dimension \(d_{}\), the noise ratio \(\), and the pollution step \(T\). We scale each hyperparameter by a factor of \(\{0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0\}\), the statistic results for the SMD are shown in Figure 4(b). As well as allowing adjusting the information bottleneck without retraining, the diffusion reconstruction module is significantly more effective (i.e., the F1 score is higher) and more robust (i.e., the F1 score is more concentrated) than the VAE module. During the hyperparameter changing, the F1 score span of the VAE module is 8.96% (0.7496\(\)0.8392), while the maximum span of the diffusion reconstruction module (\(\)) is 3.30% (0.8381\(\)0.8711).

### Ablation studies

In order to verify the effectiveness and necessity of our designs, we perform ablation studies on both dynamic decomposition and diffusion reconstruction modules of D\({}^{3}\)R, respectively.

Dynamic decomposition moduleIn this module, our main designs are spatial-temporal transformer, data-time mix-attention, offset subtraction, and disturbance strategy. Their ablation results are shown in Table 3. The w/o temporal, w/o spatial, w/o time, w/o offset, and w/o disturbance represent the variants of D\({}^{3}\)R removing temporal transformer, spatial transformer, time attention, data attention, offset subtraction, and disturbance strategy, respectively. The temporal transformer and spatial transformer bring an average absolute improvement of 5.41% (0.7493\(\)0.8034) and 3.61% (0.7673\(\)0.8034), respectively. Compared with the spatial transformer, the temporal transformer is more critical. The spatial transformer performs much worse when the datasets have smaller series number (PSM and SWaT). When the series number is large (SMD), the dimension dependency is more prosperous, and the spatial transformer performs better. Moreover, time attention and data attention in data-time mix-attention bring a great absolute promotion of 6.87% (0.7347\(\)0.8034) and 6.31% (0.7403\(\)0.8034). While data attention is better suited to short-period data (SWaT), time attention is better suited to long-period data (PSM and SMD). Offset subtraction can further improve our model with 3.27% (0.7690\(\)0.8034). Finally, the disturbance strategy brings a significant improvement of 7.85% (0.7249\(\)0.8034), especially for the SMD and SWaT datasets with high nonstationary characteristics.

Diffusion reconstruction moduleThis module uses noise diffusion to control the information bottleneck externally. To verify the superiority of noise diffusion, we replace the entire diffusion reconstruction module with the VAE module, and the ablation results are shown in Table 3 (w/o diffusion). As well as allowing adjustment of the information bottleneck without retraining, noise diffusion also provides a 3.57% (0.7677\(\)0.8034) absolute improvement. The following section will discuss more advantages of the diffusion reconstruction module. An additional innovation of this module is trend retaining, and the ablation results are shown in Table 3 (w/o retaining). After removing trend retraining, the performance of D\({}^{3}\)R decreases significantly, especially in the highly nonstationary SMD dataset (0.8682\(\)0.8006) and SWaT dataset (0.7812\(\)0.7109). Trend retaining enables the model to focus on the critical stable components, avoiding being distracted by irrelevant information.

### Hyperparameter analyses

The hyperparameter that significantly affects the performance of the VAE module is the latent space size \(z\). The hyperparameters that may affect the performance of D\({}^{3}\)R are the added drift

   dataset & D\({}^{3}\)R &  w/o \\ temporal \\  &  w/o \\ spatial \\  &  w/o \\ time \\  &  w/o \\ data \\  &  w/o \\ offset \\  &  w/o \\ disturbance \\  &  w/o \\ diffusion \\  & 
 w/o \\ trend \\  \\  PSM & **0.7609** & 0.7280 & 0.7571 & 0.7074 & 0.7374 & 0.7420 & 0.7508 & 0.7347 & 0.7592 \\ SMD & **0.8682** & 0.8169 & 0.8173 & 0.7403 & 0.8114 & 0.8446 & 0.6947 & 0.8392 & 0.8006 \\ SWaT & **0.7812** & 0.7031 & 0.7274 & 0.7563 & 0.6720 & 0.7205 & 0.7293 & 0.7293 & 0.7109 \\  Average & **0.8034** & 0.7493 & 0.7673 & 0.7347 & 0.7403 & 0.7690 & 0.7249 & 0.7677 & 0.7569 \\   

Table 3: Results of ablation studies. F1 scores are reported, with higher values meaning better performance. The best scores are highlighted in bold.

boundary \(p\), the maximum horizontal offset \(d\), the hidden state dimension \(d_{}\), the noise ratio \(\), and the pollution step \(T\). To analyze their influence on anomaly detection, we perform hyperparameter sensitivity analysis in the SMD. We scale each hyperparameter by a factor of \(\{0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0\}\), and the results are shown in Figure 5.

Anomaly detection methods based on reconstruction require constructing the information bottleneck to reconstruct normal data and inhibit anomaly effectively. While traditional autoencoders control the information bottleneck from inside through the latent space size, our approach controls the information bottleneck from outside through noise diffusion. As the experimental results show, with the bottleneck going from strict to loose (latent space size \(z\), noise ratio \(\), pollution step \(T\) going from small to large), the performance of the model goes from up to down. If the information bottleneck is too tight, the normal data cannot be reconstructed, and the precision is low. If the information bottleneck is too loose, the anomaly can be reconstructed well, and the recall is low. On the one hand, our approach to control the information bottleneck externally is more robust (smaller range of variation). On the other hand, as the information bottleneck is not an attribute of the model itself, we can try different bottleneck sizes when inference without retraining the model again.

Since it is no longer an information bottleneck, D\({}^{3}\)R is not sensitive to the hidden state dimension \(d_{}\). A larger hidden space tends to bring better results but also a greater computational cost. As for the added drift boundary \(p\) and maximum horizontal offset \(d\), the impact on our model becomes small once they exceed a certain threshold. Overall, D\({}^{3}\)R is robust to all hyperparameters we test.

## 5 Conclusion

This paper proposes **D**ynamic **D**ecomposition with **D**iffusion **R**econstruction (D\({}^{3}\)R) for long-period unstable multivariate time series anomaly detection to cover the overlook in previous work. We first decompose the long-period unstable multivariate time series dynamically and then directly reconstruct the data polluted by noise diffusion. Extensive experiments prove that D\({}^{3}\)R significantly outperforms existing works. The method we proposed to break the limitation of the local sliding window is also meaningful for other long-period multivariate time series analysis tasks, such as prediction and imputation. Meanwhile, the approach to controlling information bottleneck externally can also be used for anomaly detection of other modal data, such as picture, video, and log data.