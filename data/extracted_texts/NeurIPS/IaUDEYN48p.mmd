# Koopman-Assisted Reinforcement Learning

Preston Rozwood\({}^{*}\), Edward Mehrez\({}^{*}\), Ludger Paehler\({}^{}\), Wen Sun\({}^{}\), Steven L. Brunton\({}^{}\)

Subharmonic Technologies\({}^{}\), Cornell University\({}^{}\),

Technical University of Munich\({}^{}\), University of Washington\({}^{}\)

{pwr36, ejm322}@cornell.edu, ludger.paehler@tum.de,

ws455@cornell.edu, sbrunton@uw.edu

###### Abstract

The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning (RL) and control theory contexts due, in part, to their guaranteed convergence towards a system's optimal value function. However, this approach has severe limitations. This paper explores the connection between the data-driven Koopman operator and Bellman Markov Decision Processes, resulting in the development of two new RL algorithms to address these limitations. In particular, we focus on Koopman operator methods that reformulate a nonlinear system by lifting into new coordinates where the dynamics become linear, and where HJB-based methods are more tractable. These transformations enable the estimation, prediction, and control of strongly nonlinear dynamics. Viewing the Bellman equation as a controlled dynamical system, the Koopman operator is able to capture the expectation of the time evolution of the value function in the given systems via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with the control actions, we construct a new "Koopman tensor" that facilitates the estimation of the optimal value function. Then, a transformation of Bellman's framework in terms of the Koopman tensor enables us to reformulate two max-entropy RL algorithms: soft-value iteration and soft actor-critic (SAC). This highly flexible framework can be used for deterministic or stochastic systems as well as for discrete or continuous-time dynamics. Finally, we show that these algorithms attain state-of-the-art (SOTA) performance with respect to traditional neural network-based SAC and linear quadratic regulator (LQR) baselines on three controlled dynamical systems: the Lorenz system, fluid flow past a cylinder, and a double-well potential with non-isotropic stochastic forcing. It does this all while maintaining an interpretability that shows how inputs tend to affect outputs, what we call "input-output" interpretability.

## 1 Introduction

Interpretability is frequently lost in RL algorithms, especially those driven by large neural networks. In this paper, we re-examine the underlying Bellman equation through a dynamical systems lens and a novel application of a fundamental _transfer operator_ known as the **Koopman operator**. Nonlinear dynamics may be represented in terms of this infinite-dimensional linear Koopman operator, which acts on the space of all possible measurement functions of the system. Mathematically, given a function from the state space to the reals \(g:\), the Koopman operator \(\) for deterministic (time-homogenous) autonomous systems is defined as:

\[g(x):=g(F(x))=g(x^{}),\] (1)

where \(F\) is the single-step flow map or law of motion. More generally, in stochastic autonomous systems, it is defined as the conditional forecast operator:

\[g(x)=(g(X^{})|X=x).\] (2)In this paper, we recast the continuation term in the Bellman equation in terms of the Koopman operator:

\[V^{}(x)=_{u(|x)}\{r(x,u)+ _{x^{} p(|x,u)}[V(x^{})]\}\] (3a) \[ V_{}(x)=_{u(|x)}\{r(x,u)+ ^{u}V(x)\}.\] (3b)

It is important to note that since the Koopman operator is equivalent to the conditional forecast operator given state \(x\), it only relies on the current state. In addition, the Koopman operator's dependence on the action \(u\) has been made explicit via the \(^{u}\) notation.

The Koopman operator allows us to rewrite the equations of motion of a nonlinear system as a linear system of equations on an infinite-dimensional function space [24; 7; 23; 3; 4]. When considering a particular observable, like the value function in reinforcement learning, for many problems it is possible to isolate a finite set of basis, or dictionary, functions for which we can approximately express the dynamics of the value function with a finite-dimensional Koopman operator/matrix. Many such approaches have been explored in traditional applied dynamical systems such as dynamic mode decomposition (DMD)  for linear or approximately linear systems, and extended dynamic mode decomposition (EDMD) , as well as the closely related sparse identification of nonlinear dynamics (SINDy) , which can be thought of as generalizing the previous approaches to linear mappings rather than just linear operators of dynamics . This body of work was subsequently extended to stochastic dynamical systems [16; 17] and controlled dynamical systems [15; 28].

An important contribution of this work is the construction of the **Koopman tensor** in a multiplicatively separable dictionary space on states and controls, respectively. We then solve for this tensor via least squares for the prediction of future state dictionaries using (state, control, future state) samples collected from simulated environments. Slices of the resulting Koopman tensor are then weighted by the control dictionary elements to construct a finite-dimensional Koopman matrix in the lifted state observable space for any control value, including those outside of the training dataset.

Finally, as a proof of concept of _Koopman assisted RL algorithms_, we reformulate two max-entropy RL algorithms: **soft value iteration** and **soft actor-critic**[11; 12]. We refer to this approach broadly as **Koopman-Assisted Reinforcement Learning (KARL)** and to the two particular reformulated algorithms as **soft Koopman value iteration (SKVI)** and **soft actor Koopman critic (SAKC)**. We validate these algorithms in four environments using quadratic cost functions: linear system, the Lorenz system, fluid flow past a cylinder, and a double-well potential with non-isotropic stochastic forcing. We demonstrate in each system that our KARL methods achieve SOTA or near-SOTA compared to traditional neural network-based SAC and LQR baselines.

### Related Work

Use of the Koopman operator in RL is an emerging field of research. Previous works have used the Koopman operator for imitation learning and identifying symmetries in state dynamics [31; 33]. Koopman analysis has been extended to controlled systems [28; 15]; however, our main contribution

Figure 1: Koopman-assisted reinforcement learning in the example of the Soft Actor Koopman-Critic, a Koopman variant of the popular Soft Actor-Critic algorithm. The _Koopman Critic_ receives the state, and the reward as original, nonlinear dynamics, before lifting these dynamics onto the vector space, where they can be advanced in time with the _Koopman operator_ linearly. This critique is then fed back to the Actor which issues the action to be performed in the environment.

of recasting Markov Decision Processes (MDPs) using Koopman embedding frameworks has not been explored. Notably, we introduce a novel approach to parameterize the Koopman operator using a Koopman tensor on a lifted state-control space which is essential for handling controlled dynamical systems. Other attempts to incorporate Koopman into controlled systems include dynamic mode decompositon with control (DMDc)  for system identification and applications of LQR to Koopman-linearized dynamics [28; 15]. The Koopman operator has also been used for model predictive control (MPC) [6; 20; 21; 29], although without the same convergence guarantees as KARL. One recent work learns a Koopman autoencoder [22; 25; 27] for Q-learning . In contrast, our approach of rewriting the Bellman and HJB equations using control-dependent Koopman operators enables both interpretability and single-step estimates of the expectation of the value function.

## 2 Koopman-Assisted Reinforcement Learning (KARL)

### Technical Background

Here, we discuss relevant theory and algorithms. We review Koopman operator theory, discuss the reformulation of MDPs, and build intuition for the Koopman tensor. Finally we apply insights from our reformulation of MDPs to develop two new max entropy RL algorithms: **soft Koopman value iteration (SKVI)** and **soft actor Koopman critic (SAKC)**.

#### 2.1.1 Koopman Operator Theory

The Koopman operator describes the time evolution for any function of the state of autonomous and controlled systems. Formally, we consider real-valued vector measurement functions \(g M\), which are themselves elements of an infinite-dimensional Hilbert space and where \(M\) is a manifold that represents the state space. Typically the observables are assumed to belong to the function space \(L^{}()\) where \(M=^{d}\). Often, these functions \(g\) are called **observables** or **measurements** of the state \(x\). Formally, the **Koopman operator**\(:L^{}() L^{}()\) for deterministic (time-homogenous) autonomous systems is defined as:

\[g(x):=g(F(x))=g(x^{}),\] (4)

where \(F\) is the single-step flow map or law of motion. More generally in stochastic autonomous systems, it is defined as the conditional forecast operator:

\[g(x)=(g(X^{})|X_{t}=x).\] (5)

**Remark 2.1** (Basis Functions of the Koopman Operator): _The main insight from this operator theoretic view of dynamics is that a finite set of basis functions may be found to characterize observables of the state as long as the **Koopman operator has a finite point spectra rather than a continuum**. This perspective is crucial below as we lift our coordinate spaces to a finite set of basis functions, typically called **dictionary functions**, which we denote by \(\{_{i}\}\). See A.1 (brief technical exposition);  (broader treatment of Koopman operator in applied dynamical systems)._

### MDPs and Bellman's Equation

Below, we assume an infinite horizon Markov decision process (MDP) setting for the agent's objective. In discrete time, assuming that the agent follows policy \(\) which is a distribution over actions, the \(\)-value function takes the form:

\[V^{}(x)=[._{t=0}^{}-^{t}c(x_{t},u_{t}) |,x_{0}=x],\] (6)

where \(\) represents the discount rate and we express rewards in terms of negative costs \(r(x,u)=-c(x,u)\) as we will use a quadratic cost function as in the standard setting LQR below.

**Remark 2.2** (Finite-Horizon MDPs and the Time-Inhomogenous Koopman Operator): _The finite horizon MDP can also be transformed using the Koopman operator, however, in that case, the operator will not be time-homogeneous and will depend not only on action, but also on point in time. For an excellent in-depth discussion of discrete-time MDPs (both finite and infinite horizon) and their place in RL, see ._The agent's optimal value function is expressed as follows:

\[V^{*}(x)=_{}V^{}(x).\] (7)

We use the Bellman equation (or Hamilton-Jacobi-Bellman equation in continuous time) to recursively characterize the above optimal value function. The discrete Bellman optimality equation is:

\[V(x) =_{_{1}}_{u(|x)}[-c(x,u)+ _{x^{} p(|x,u)}[V(x^{})]]\] (8a) \[=_{}_{u_{}(|x)}[-c(x,u)+ ^{u}V(x)]\] (8b) \[=_{}_{u_{}(|x)}[-c(x,u)+  w^{T}K^{u}(x)],\] (8c)

where \(:^{d_{x}}\) is the dictionary vector function \(=(_{1},,_{d_{x}})\) that serves as a basis for the Koopman operator applied to the value function. Note that the Koopman operator applies element-wise when applied to vector functions.

### Koopman Tensor Formulation of Controlled Dynamics

We introduce the Koopman tensor as an extended Koopman operator that includes a third dimension specifically to capture control action observables. With it, we are able to construct a 2D Koopman operator from any action on a continuum by collapsing the action observable dimension.

```
0: State feature map \(:^{d_{x}}\), control feature map \(:^{d_{u}}\), and a sample \(\{(x_{i},u_{i})\}_{i=0}^{N}\)
1: Solve for \(M\) as in Equation 9
2: Convert \(M\) into \(K\) tensor using Fortran-style reshaping ```

**Algorithm 1** Koopman Tensor Estimation

Denote \(:^{d_{x}}\) as the feature mapping (each coordinate of \(\) is an observable function), and \(:^{d_{u}}\) as the feature mapping for control. Let us seek a finite-dimensional approximation of the Koopman operator \(^{u_{i}}\). Denote \(K^{d_{x} d_{x} d_{u}}\) as a 3D tensor as shown in Figure 2. For any \(u\), let us denote \(K^{u}^{d_{x} d_{x}}\) as follows: \(K^{u}[i,j]=_{z=1}^{d}K(i,j,z)(u)[z]\). Namely, \(K^{u}\) is the result of the tensor vector model along the third dimension of \(K\) and \(K^{u}\) serves as the finite-dimensional approximation of Koopman operator \(^{u}\). We learn \(K\) as follows:

\[_{K}_{i=1}^{N}\|K^{u_{i}}(x_{i})-(x^{}_{i})\| ^{2}.\]

Figure 2: Construction of action-dependent Koopman operators \(K^{u}\) from the Koopman tensor \(K\). Colors match along the \(k\) index (depth of tensor box). Each of the matrix slices is then weighted according to the \(\) dictionary elements to construct the control-dependent Koopman operator \(K^{u}\).

We can slightly rewrite the above objective so that it becomes the regular multi-variate linear regression problem. We can rearrange to write \(K\) as a 2-dimensional matrix in \(^{d_{x} d_{x} d_{u}}\). Denote \(M^{d_{x} d_{x} d_{u}}\), where \(M[i,:]^{d_{x} d_{u}}\) is the vector from stacking the columns of the 2D matrix \(K[i,:,:]\). A nice way to visualize this rearrangement can be seen in the definition of M in Figure 2. Denote \((u)(x)^{d_{x} d_{u}}\) as the Kronecker product. Thus we have:

\[K^{u}(x)=M((u)(x)).\]

Therefore, the optimization problem becomes a regular linear regression:

\[_{M}_{i=1}^{N}\|M((u)(x_{i}))-(x_{i }^{})\|^{2}.\] (9)

Once we compute \(M\), we can convert back to Koopman operator for any \(u\) by reshaping \(M\) back to the 3D tensor \(K\). Then the \(d_{x} d_{x}\) finite dimensional Koopman operator approximation is again \(K^{u}\) for any \(u\) as seen in the summation in Figure 2.

Note that the above formulation also works for discrete control set \(\). The benefit of doing this is that control share information and similarity via their feature \((u)\). This could give better sample efficiency than learning independent Koopman operators one for each discrete control.

### Max Entropy Koopman RL Algorithms

To demonstrate the effectiveness of how the Koopman operator can be used in RL we follow a popular strand of RL literature and add an **entropy penalty**\(\) to the cost function to encourage exploration of the environment.

#### 2.4.1 Koopman Value Iteration

In addition to the assumption about the finite dimensional representation of the Koopman operator, we will also assume that the optimal value function \(V^{}(x)\) can be written as a linear combination of basis functions. In other words, there exists a \(w^{}^{d}\), such that \(V^{}(x)=(w^{})^{}(x)\). Given a \(w^{d}\), we can express the (entropy regularized) Bellman error as follows. For any \(x\):

\[w^{}(x)-_{:()}[_{u(x)}[c(x,u)+(u|x)+w^{}K^{u}(x)] ].\]

Thanks to the entropy regularization, given a \(w\), we can express the optimal form of \(\) as follows:

\[(u|x)=(-(c(x,u)+w^{}K^{u}(x))/)/Z_ {x},\] (10)

where \(Z_{x}\) is the normalizing constant that is only dependent on \(x\) that makes \((|x)\) a proper probability distribution. Note that \(\) depends on \(w\).

Converting this into an iterative procedure to find the value function weights, \(w^{}\) in terms of the previous weights \(w\), the **average Bellman error (ABE)** over the dataset can be expressed as follows:

\[_{w^{}:\|w\|_{2} W}_{i=1}^{N}(w^{} (x)-_{(|x)}[_{u(x)}[c(x,u)+ (u|x)+w^{}K^{u}(x)]])^{2}.\] (11)

The above is a canonical ordinary least squares (OLS) problem and can thus be solved explicitly. We repeat this procedure until the ABE is small or until there is minimal improvement between update steps. Unfortunately, given finite data, it is possible for a suboptimal value function to satisfy the Bellman equation exactly . As a result, we need to be careful when using the Bellman error as a training objective for RL agents.

### Soft Actor Koopman-Critic (SAKC)

Here, we outline how we modify the the Soft-Actor-Critic (SAC) framework  to restrict the search space by incorporating information from the Koopman operator. Using the same loss functions and similar notation to that of the SAC paper , we first specify the soft value function loss:

\[J_{V}(w)=_{x}[(V_{w}(x)- _{u_{}}[Q_{}(x,u)-_{}(u|x) ])^{2}].\] (12)The additional specification that is imposed in the Koopman RL framework would be a restriction around the specifications of \(V_{w}(x)\) and \(Q_{}(x,u)\):

\[V_{w}(x)=w^{T}(x),\] (13)

where \(w\) is a vector of coefficients for the dictionary functions.

Next, we show how the loss function for the (Q) quality function changes:

\[J_{Q}()=_{(x,u)}[(Q_{ }(x,u)-(x,u))],\] (14)

where the target Q-function incorporates the Koopman operator and is defined as:

\[(x,u) =r(x,u)+_{x^{} p(|x,u)}[V_{ }(x^{})]\] \[=r(x,u)+^{T}K^{u}(x),\] (15)

where \(\) represents the infinite-dimensional Koopman for a fixed action \(u\) and \(K^{u}\) represents the Koopman operator's finite-dimensional form on the state-dictionary space.

Finally, the loss function for the policy does not change and is given by:

\[J_{}()=_{x}[D_{KL}(_{}(|x) \|(x,))}{Z_{}(x)})].\] (16)

After these adjustments, the general algorithm remains the same as in the SAC paper and is given by:

```
0: Initial parameter vectors \(w\), \(\), \(\), \(\)
1:for each iteration do
2:for each environment step do
3:\(u_{}(u|x)\) \(x^{} p(x^{}|x,u)\) \(\{(x,u,r(x,u),x^{}\}\)
4:endfor
5:for each gradient step do \(w w-_{V}_{}J_{V}(w)\) \(_{i}_{i}-_{Q}_{_{i}}J_{Q}( _{i})\) for \(i\{1,2\}\) \(-_{}_{}J_{}()\) \( w+(1-)\)
6:endfor
7:endfor ```

**Algorithm 3** Soft Actor Koopman-Critic

```
0: Initial parameter vectors \(w\), \(\), \(\), \(\)
1:for each iteration do
2:for each environment step do
3:\(u_{}(u|x)\) \(x^{} p(x^{}|x,u)\) \(\{(x,u,r(x,u),x^{}\}\)
4:endfor
5:for each gradient step do \(w w-_{V}_{}J_{V}(w)\) \(_{i}_{i}-_{Q}_{_{i}}J_{Q}( _{i})\) for \(i\{1,2\}\) \(-_{}_{}J_{}()\) \( w+(1-)\)
6:endfor
7:endfor ```

**Algorithm 4** Soft Actor Koopman-Critic

## 3 Experimental Evaluation

### Evaluation Design

We apply KARL in a linear system, fluid flow past a cylinder, Lorenz, and a double-well potential: figure 3 for an illustration of the dynamics for the four systems. For details such as formal definitions and variable values of these systems, please refer to A.2. For purposes of evaluating the proposed algorithms, we build upon CleanRL  from which we use the reference implementations of the corresponding RL algorithms,1 and upon which our implementation of Soft Actor Koopman-Critic is based.2 For the Koopman tensor, we use an order 2 monomial dictionary space for all systems, and allow the Koopman tensor access to 30,000 steps of data collected by a random agent for each environment. Each environment has different action ranges, which were selected with some minimal a priori understanding of the system, but are mostly random. To ensure that we train the various policies to minimize the same cost, we use the LQR cost function for each system, defined as:

\[c(x,u)=x^{T}Qx+u^{T}Ru.\] (17)

### Results

Evaluating the performance of the Soft Actor Koopman-Critic (SAKC) on the four environments against the classical control baseline, as well as the reference soft actor-critic implementations, we see that on the _Linear System_ the SAKC needs \( 5,000\) environment steps to properly calibrate its Koopman critic, and match the SOTA performance of LQR, Value Iteration, and the value-based SAC. The Q-function based SAC exhibits a slightly greater degree of instability and requires \( 8,000\) environment steps to match SOTA performance. On the _Fluid Flow_ this training dynamic is matched with SAKC requiring \(5,000-8,000\) environment steps to calibrate itself in this more difficult environment and reach SOTA. The required number of environment steps matches those required by the value-function based SAC. On the even more difficult chaotic _Lorenz System_ SAKC requires slightly more exploration with \(10,000\) environment steps, before hitting SOTA. Of note here is that SAKC exhibits more performance instability as compared to the better-performing SAC, and Value Iteration. On the _Double Well_, we see a fast calibration of SAKC to quickly match SOTA, while LQR is unable to match the performance of the reinforcement learning algorithms on this even more difficult environment. Zooming in on the performance of the different RL algorithms 5, we can see that after seemingly reaching SOTA, the episodic returns of all agents equally fluctuate and SAKC matches the performance of the other SAC methods.

When tasked with learning dynamics and converging to an optimal controller in non-linear systems, our new Koopman-assisted reinforcement learning approach significantly outperformed LQR in non-linear systems, as measured by relative cost. See Figure 4 and Table 1. KARL's success in these unique dynamical settings demonstrates its significant flexibility across varying environments: fluid flow shows its prowess in non-linear systems; Lorenz proves KARL's ability to control chaotic systems; and double well shows its ability to learn in stochastic environments.

## 4 Conclusion and Future Work

In summary, we present a novel approach to RL by integrating Koopman operator methods with existing maximum entropy RL algorithms to formulate Koopman-Assisted Reinforcement Learning.

Figure 3: Four benchmark problems investigated: (a) simple linear system; (b) Lorenz 1963 model; (c) incompressible fluid flow past a cylinder [at Reynolds number 100]; and (d) double-well potential with non-isotropic stochastic forcing.

By leveraging the Koopman operator, KARL overcomes limitations of traditional RL algorithms by making them more "input-output" interpretable, allowing for a deeper understanding of the learned policies and their underlying dynamics. The empirical results presented in this paper on a diverse set of dynamical systems-based environments including non-linear, chaotic, and stochastic systems show that KARL is able to match the SOTA performance of the reference Soft Actor-Critic algorithm. In addition, KARL outperforms the classical control baseline of LQR methods on the non-linear environments, showcasing its flexibility, and adaptability. KARL's SOTA performance in these various complex systems highlights its potential for real-world applications, making it a valuable addition to the repertoire of current RL techniques. The future of KARL lies in its continuous evolution and adaptation to more complex and realistic settings. Addressing these challenges and exploring these directions will allow the Koopman operator to aid in the development of robust, interpretable, and efficient future RL algorithms.

Prospects for further development and application of KARL are both numerous and promising. For example, integration of KARL with modern online learning techniques  could support real-time applications, especially in combination with techniques to improve the efficiency of the algorithm such as knowledge gradients . A comprehensive theoretical analysis of KARL algorithms, including convergence properties and sample complexity bounds, would provide valuable insights into their behavior, and would aid in providing more intuition and guarantees for safety-critical applications. Incorporating sparsification techniques such as SINDy  to facilitate the use of large dictionary spaces in unfamiliar complex systems to determine value function dynamics will help further interpret the main driving features (observables) of the optimal value function. Visualization techniques to better interpret the learned Koopman tensor and Koopman-dependent operators could facilitate broader adoption in domains where interpretability is a top priority, such as healthcare, economics, and autonomous driving systems.

Figure 4: Episodic returns of the evaluation environments for the compared algorithms: value iteration, a linear quadratic regulator (LQR), the Q-function based soft actor-critic (SAC (Q)), the V-function based soft actor-critic (SAC (V)), and the soft actor Koopman-critic (SAKC).