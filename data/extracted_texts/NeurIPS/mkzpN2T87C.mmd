# Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search

Qiujiang Jin

ECE, UT Austin

qiujiangjin0@gmail.com &Ruichen Jiang

ECE, UT Austin

rjiang@utexas.edu &Aryan Mokhtari

ECE, UT Austin

mokhtari@austin.utexas.edu

###### Abstract

In this paper, we present the first explicit and non-asymptotic global convergence rates of the BFGS method when implemented with an inexact line search scheme satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global linear convergence rate of \((1-)^{t}\) for \(\)-strongly convex functions with \(L\)-Lipschitz gradients, where \(=\) represents the condition number. Additionally, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate that depends solely on the line search parameters, independent of the condition number. We also establish a global superlinear convergence rate of \((()^{t})\). These global bounds are all valid for any starting point \(x_{0}\) and any symmetric positive definite initial Hessian approximation matrix \(B_{0}\), though the choice of \(B_{0}\) impacts the number of iterations needed to achieve these rates. By synthesizing these results, we outline the first global complexity characterization of BFGS with the Armijo-Wolfe line search. Additionally, we clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe conditions and characterize its overall complexity.

## 1 Introduction

In this paper, we focus on solving the following unconstrained convex minimization problem

\[_{x^{d}}f(x),\] (1)

where \(f:^{d}\) is strongly convex and twice differentiable. Quasi-Newton methods are among the most popular algorithms for solving this class of problems due to their simplicity and fast convergence. Like gradient descent-type methods, they require only gradient information for implementation, while they aim to mimic the behavior of Newton's method by using gradient information to approximate the curvature of the objective function. There are several variations of quasi-Newton methods, primarily distinguished by their update rules for the Hessian approximation matrices. The most well-known among these include the Davidon-Fletcher-Powell (DFP) method [1; 2], the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method [3; 4; 5; 6], the Symmetric Rank-One (SR1) method [7; 8], and the Broyden method . Apart from these classical methods, other variants have also been proposed in the literature, including randomized quasi-Newton methods [10; 11; 12; 13; 14], greedy quasi-Newton methods [13; 14; 15; 16], and those based on online learning techniques [17; 18]. In this paper, we mainly focus on the global analysis of the BFGS method, arguably the most successful quasi-Newton method in practice.

The classic analyses of BFGS, including [19; 20; 21; 22; 23; 24; 25; 26; 27; 28], primarily focused on demonstrating local asymptotic superlinear convergence without addressing an explicit global convergence rate when BFGS is deployed with a line-search scheme. While attempts have been made to establish global convergence for quasi-Newton methods using line search or trust-region techniques in previous studies [29; 30; 31; 32; 8; 33], these efforts provided only asymptotic convergence guarantees without explicit global convergence rates, thus not fully characterizing the global convergence rate of classical quasi-Newton methods.

In recent years, there have been efforts to characterize the explicit convergence rate of BFGS within a local neighborhood of the solution, establishing a superlinear convergence rate of the form \((})^{t}\); see, for example, [34; 35; 36; 37]. However, these results focus solely on local convergence analysis of BFGS under conditions where the stepsize is consistently set to one, the iterate remains close to the optimal solution, and the initial Hessian approximation matrix meets certain necessary conditions. Consequently, these analyses do not extend to providing a global convergence guarantee. For more details on this subject, we refer the reader to the discussion section in .

To the best of our knowledge, only few papers are closely related to our work and establish a global non-asymptotic guarantee for BFGS. In , it was shown that BFGS with exact line search achieves a global linear rate of \((1-}{L^{3}}(1+(B_{0}^{-1})}{Lt})^{-1}(1+ (B_{0})}{Lt})^{-1})^{t}\), where \(\) is the strong convexity parameter, \(L\) is the Lipschitz constant of the gradient, \(B_{0}\) is the initial Hessian approximation matrix, and \(()\) denotes the trace of a matrix. After \(t=O(d)\) iterations, this rate approaches \((1-}{L^{3}})^{t}\), which is significantly slower than the convergence rate of gradient descent. Additionally, a recent draft in  studied the global convergence of BFGS under an inexact line search. While this work establishes a local superlinear rate, it only shows a global linear rate of the form \((1-}{L^{2}})^{t}\). Hence, both these results fail to prove any global advantage for BFGS over gradient descent. In , the authors improved upon  by showing a better global linear convergence rate and a faster superlinear rate for BFGS with exact line search. Specifically, for an \(L\)-Lipschitz and \(\)-strongly convex function, BFGS initialized with \(B_{0}=LI\) achieves a global linear rate of \((1-}{L^{3/2}})^{t}\) for \(t 1\), while BFGS with \(B_{0}= I\) achieves the same rate after \(d\) iterations. With the additional assumption that the objective's Hessian is Lipschitz, an improved linear rate of \((1-)^{t}\) is achieved after \(()\) iterations when \(B_{0}=LI\) and after \((d+)\) when \(B_{0}= I\), matching the rate of gradient descent. A superlinear rate of \((^{1}\!/\!)^{t}\) was also shown when the number of iterations exceeds specific thresholds.

**Contributions.** In this paper, we analyze the BFGS method combined with the Armijo-Wolfe line search, the most commonly used line search criteria in practical BFGS applications; see, e.g., . For minimizing an \(L\)-smooth and \(\)-strongly convex function, we present a global convergence rate of \((1-)^{t}\). To the best of our knowledge, this is the first result demonstrating a global linear convergence rate for BFGS that matches the rate of gradient descent under these assumptions. Furthermore, we show that if the objective function's Hessian is Lipschitz continuous, BFGS with the Armijo-Wolfe line search converges at a linear rate determined solely by the line search parameters and not the problem's condition number, \(=}{{}}\), when the number of iterations is sufficiently large. Finally, we prove a global non-asymptotic superlinear convergence rate of \((h(d,,C_{0})/t)^{t}\), where \(h(d,,C_{0})\) depends on the condition number \(\), the dimension \(d\), and the weighted distance between the initial point \(x_{0}\) and the optimal solution \(x_{*}\), denoted by \(C_{0}\). We summarize our results in Table 1. By combining these convergence results, we establish the total iteration complexity of BFGS with the Armijo-Wolfe line search. We also specify the line search complexity by investigating a bisection algorithm for choosing the step size that satisfies the Armijo-Wolfe conditions. Our result is one of the first non-asymptotic analysis characterizing the global convergence complexity of the BFGS quasi-Newton method with an inexact line search.

**Notation.** We denote the \(_{2}\)-norm by \(\|\|\), the set of \(d d\) symmetric positive definite matrices by \(_{++}^{d}\), and use \(A B\) to mean \(B-A\) is symmetric positive semi-definite. The trace and determinant of a matrix \(A\) are represented as \((A)\) and \((A)\), respectively.

## 2 Preliminaries

In this section, we present the assumptions, notations, and intermediate results useful for the global convergence analysis. First, we state the following assumptions on the objective function \(f\).

**Assumption 2.1**.: _The function \(f\) is twice differentiable and strongly convex with parameter \(>0\)._

**Assumption 2.2**.: _The gradient of \(f\) is Lipschitz continuous with parameter \(L>0\)._

These assumptions are common in the convergence analysis of quasi-Newton methods. Under these, we show a global linear convergence rate of \(O((1-)^{t})\). To achieve a faster linear convergence rate that is independent of the problem condition number, and a global superlinear rate, we require an additional assumption that the objective function Hessian is Lipschitz continuous, as stated next.

**Assumption 2.3**.: _The Hessian of \(f\) is Lipschitz continuous with parameter \(M>0\), i.e., for \(x,y^{d}\), we have \(\|^{2}f(x)-^{2}f(y)\| M\|x-y\|\)._Note that the above regularity condition on the Hessian assumption is also common for establishing the superlinear convergence rate of quasi-Newton methods .

**BFGS Update.** Next, we state the general update rule of BFGS. If we denote \(x_{t}\) as the iterate at time \(t\), the vector \(g_{t}= f(x_{t})\) as the objective function gradient at \(x_{t}\), and \(B_{t}\) as the Hessian approximation matrix at step \(t\), then the update is given by

\[x_{t+1}=x_{t}+_{t}d_{t}, d_{t}=-B_{t}^{-1}g_{t},\] (2)

where \(_{t}>0\) is the step size and \(d_{t}\) is the descent direction. By defining the variable difference \(s_{t}:=x_{t+1}-x_{t}\) and the gradient difference \(y_{t}:= f(x_{t+1})- f(x_{t})\), we can present the Hessian approximation matrix update for BFGS as follows:

\[B_{t+1}=B_{t}-s_{t}s_{t}^{}B_{t}}{s_{t}^{}B_{t}s_{t}}+ {y_{t}y_{t}^{}}{s_{t}^{}y_{t}}.\] (3)

To avoid the costly operation of inverting the matrix \(B_{t}\), one can define the inverse Hessian approximation matrix as \(H_{t}:=B_{t}^{-1}\) and apply the Sherman-Morrison-Woodbury formula to obtain

\[H_{t+1}:=(I-y_{t}^{}}{y_{t}^{}s_{t}})H_{t}( I-s_{t}^{}}{s_{t}^{}y_{t}})+s_{t}^{}}{ y_{t}^{}s_{t}}.\] (4)

It is well-known that for a strongly convex objective function, the Hessian approximation matrices \(B_{t}\) remain symmetric and positive definite if the initial matrix \(B_{0}\) is symmetric positive definite . Therefore, all matrices \(B_{t}\) and \(H_{t}\) are symmetric positive definite throughout this paper.

As mentioned earlier, establishing a global convergence guarantee for BFGS requires pairing it with a line search scheme to select the stepsize \(_{t}\). This paper focuses on implementing BFGS with the Armijo-Wolfe line search, detailed in the following subsection.

**Armijo-Wolfe Line Search.** We consider a stepsize \(_{t}>0\) that satisfies the Armijo-Wolfe conditions

\[f(x_{t}+_{t}d_{t})  f(x_{t})+_{t} f(x_{t})^{}d_{t},\] (5) \[ f(x_{t}+_{t}d_{t})^{}d_{t}  f(x_{t})^{}d_{t},\] (6)

where \(\) and \(\) are the line search parameters, satisfying \(0<<<1\) and \(0<<\). The condition in (5) is the Armijo condition, ensuring that the step size \(_{t}\) provides a sufficient decrease in the objective function \(f\). The condition in (6) is the curvature condition, which guarantees that the slope \( f(x_{t}+_{t}d_{t})^{}d_{t}\) at \(_{t}\) is not strongly negative, indicating that further movement along \(d_{t}\) would significantly decrease the function value. These conditions provide upper and lower bounds on the admissible step size \(_{t}\). In some references, the Armijo-Wolfe line search conditions are known as the weak Wolfe conditions . The procedure for finding \(_{t}\) that satisfies these conditions is described in Section 7. Next lemma presents key properties of the Armijo-Wolfe conditions.

  Initial Matrix & Convergence Phase & Convergence Rate & Starting moment \\   \(B_{0}\) & Linear phase I & \((1-)^{t}\) & \((})\) \\  \(B_{0}\) & Linear phase II & \((1-})^{t}\) & \((})+C_{0}(})+C_{0}\) \\  \(B_{0}\) & Superlinear phase & \((})+C_{0}(})+C_{0}}{t})^{t}\) & \((})+C_{0}(})+C_{0}\) \\   \(LI\) & Linear phase I & \((1-})^{t}\) & \(1\) \\  \(LI\) & Linear phase II & \((1-})^{t}\) & \(d+C_{0}\) \\  \(LI\) & Superlinear phase & \((}{t})^{t}\) & \(d+C_{0}\) \\   \( I\) & Linear phase I & \((1-})^{t}\) & \(d\) \\  \( I\) & Linear phase II & \((1-})^{t}\) & \((1+C_{0})d+C_{0}\) \\  \( I\) & Superlinear phase & \(()d+C_{0}}{t})^{t}\) & \((1+C_{0})d+C_{0}\) \\  

Table 1: Summary of our results for (i) an arbitrary positive definite \(B_{0}\), (ii) \(B_{0}=LI\), and (iii) \(B_{0}= I\). Here, \((A):=(A)-d-(A)\), \(_{0}=B_{0}\) and \(_{0}=^{2}f(x_{*})^{-}B_{0}^{2}f(x_{*})^{- }\). The last column shows the number of iterations required to achieve the corresponding linear or superlinear convergence phase. For brevity, the absolute constants are dropped.

**Lemma 2.1**.: _Consider the BFGS method with Armijo-Wolfe inexact line search, where the step size satisfies the conditions in (5) and (6). Then, for any initial point \(x_{0}\) and any symmetric positive definite initial Hessian approximation matrix \(B_{0}\), the following results hold for all \(t 0\):_

\[)-f(x_{t+1})}{-g_{t}^{}s_{t}},^{ }s_{t}}{-g_{t}^{}s_{t}} 1-, f(x_{t+1})  f(x_{t}).\] (7)

**Remark 2.1**.: _While in this paper we only focus on the Armijo-Wolfe line search, our results are also valid for some other line search schemes that require stricter conditions. For instance, in the strong Wolfe line search, given \(0<<<1\) and \(0<<\), the required conditions for the step size are_

\[f(x_{t}+_{t}d_{t}) f(x_{t})+_{t} f(x_{t})^{}d_{t}, | f(x_{t}+_{t}d_{t})^{}d_{t}| f(x_{t})^{ }d_{t},\]

_Indeed, if \(_{t}\) satisfies the strong Wolfe conditions, it also satisfies the Armijo-Wolfe conditions._

_Another commonly employed line search scheme is Armijo-Goldstein, which imposes the conditions_

\[-c_{1}_{t} f(x_{t})^{}d_{t} f(x_{t})-f(x_{t}+_{t}d_{t}) -c_{2}_{t} f(x_{t})^{}d_{t},\]

_with \(0<c_{1} c_{2}<1\). The lower bound on \(f(x_{t})-f(x_{t}+_{t}d_{t})\) in the Armijo-Goldstein line search indicates that \(_{t}\) satisfies the sufficient decrease condition in (5) required for the Armijo-Wolfe conditions, with \(=c_{1}\). Moreover, given the convexity of \(f\), the upper bound on \(f(x_{t})-f(x_{t}+_{t}d_{t})\) in the Armijo-Goldstein line search suggests \(-_{t} f(x_{t}+_{t}d_{t})^{}d_{t} f(x_{t})-f(x_{t}+_ {t}d_{t})-c_{2}_{t} f(x_{t})^{}d_{t}\). Thus, \(_{t}\) also meets the curvature condition in (6) required in the Armijo-Wolfe conditions with \(=c_{2}\). Hence, all our results derived under the Armijo-Wolfe line search are also valid for both the strong Wolfe line search and the Armijo-Goldstein line search._

## 3 Convergence Analysis

In this section, we present our theoretical framework for analyzing the global linear convergence rates of BFGS with the Armijo-Wolfe line search scheme. To start, we introduce some necessary definitions and notations. We define the average Hessian matrices \(J_{t}\) and \(G_{t}\) as

\[J_{t}:=_{0}^{1}^{2}f(x_{t}+(x_{t+1}-x_{t}))d, G_{t}: =_{0}^{1}^{2}f(x_{t}+(x_{*}-x_{t}))d.\] (8)

Further, for measuring the suboptimality of the iterates we define the sequence \(C_{t}\) as

\[C_{t}:=}})-f(x_{*}))}, t  0,\] (9)

where \(M\) is the Lipschitz constant of the Hessian defined in Assumption 2.3 and \(\) is the strong convexity parameter introduced in Assumption 2.1.To analyze the dynamics of the Hessian approximation matrices \(\{B_{t}\}_{t=0}^{+}\), we use the function \((A)\)

\[(A):=(A)-d-(A),\] (10)

well-defined for any \(A_{++}^{d}\). It was introduced in  to capture the discrepancy between \(A\) and the identity matrix \(I\). Note that \((A) 0\) for any \(A_{++}^{d}\) and \((A)=0\) if and only if \(A=I\).

Before we start convergence analysis, given any weight matrix \(P_{++}^{d}\), we define the weighted versions of the vectors \(g_{t}\), \(s_{t}\), \(y_{t}\), \(d_{t}\) and the matrix \(B_{t}\), \(J_{t}\) as

\[_{t}=P^{-}g_{t},_{t}=P^{}s_{t}, _{t}=P^{-}y_{t},_{t}=P^{}d_{t}.\] (11)

\[_{t}=P^{-}B_{t}P^{-},_{t}=P^{- {1}{2}}J_{t}P^{-}.\] (12)

Note that these weighted matrices and vectors preserve many properties of their unweighted counterparts. For instance, two of these main properties are \(_{t}^{}_{t}=g_{t}^{}s_{t}\) and \(_{t}^{}_{t}=y_{t}^{}s_{t}\). Similarly, the update for the weighted version of Hessian approximation matrices closely mirrors the update of their unweighted counterparts, as noted in the following expression:

\[_{t+1}=_{t}-_{t}_{t}_{t}^{}_ {t}}{_{t}^{}_{t}_{t}}+_{t}_{t}^{ }}{_{t}^{}_{t}}, t 0.\] (13)

Finally, we define a crucial quantity, \(_{t}\), which measures the angle between the weighted descent direction and the negative of the weighted gradient direction, satisfying

\[(_{t})=_{t}^{}_{t}}{\|\|_{t}\|}.\] (14)

### Intermediate Results

In this section, we present our framework for analyzing the convergence of BFGS with an inexact line search. We first characterize the relationship between the function value decrease at each iteration and key quantities, including the angle \(_{t}\) defined in (14).

**Proposition 3.1**.: _Let \(\{x_{t}\}_{t 0}\) be the iterates generated by BFGS. Recall the definitions of weighted vectors in (11). Then, for any weight matrix \(P\) and for all \(t 1\), we have_

\[)-f(x_{*})}{f(x_{0})-f(x_{*})}1- _{i=0}^{t-1}_{i}_{i}_{i}(_{ i})}{_{i}}^{}^{t}.\] (15)

_where \(_{t}\), \(_{t}\), \(_{t}\) and \(_{t}\) are defined as_

\[_{t}:=)-f(x_{t+1})}{-_{t}^{}_{t}},_{t}:=_{t}\|^{2}}{f(x_{t})-f(x_{*})}, _{t}:=_{t}^{}_{t}}{\|_{t}\|^{2}},_ {t}=_{t}^{}_{t}}{-_{t}^{}_{t}}.\] (16)

This result shows the convergence rate of BFGS with Armijo-Wolfe line search depends on four products: \(_{i=0}^{t-1}_{i}\), \(_{i=0}^{t-1}_{i}\), \(_{i=0}^{t-1}_{i}\), and \(_{i=0}^{t-1}(_{i})}{_{i}}\). To establish an explicit rate, we need lower bounds on these products. Lemma 2.1 shows that the lower bounds for \(_{i=0}^{t-1}_{i}\) and \(_{i=0}^{t-1}_{i}\) depend on the next line search parameters \(\) and \(\). We will further prove that if the unit step size \(_{t}=1\) satisfies the Armijo-Wolfe conditions, better lower bounds can be obtained for these products. The lower bounds for \(_{i=0}^{t-1}_{i}\) and \(_{i=0}^{t-1}(_{i})}{_{i}}\) were established in previous work  as presented in Appendix D. Specifically, the bounds for \(_{i=0}^{t-1}_{i}\) depend on the choice of the weight matrix, which varies in different sections of the paper, requiring separate bounds for each case. However, the bound for \(_{i=0}^{t-1}(_{i})}{_{i}}\) does not require separate treatment. This is explicitly established in Proposition D.1, a classical result, as discussed in [41, Section 6.4]. We build all our linear and superlinear results by establishing different bounds on the terms in (15).

## 4 Global Linear Convergence Rates

Building on the tools introduced in Section 3, we establish explicit global linear convergence rates for BFGS with the Armijo-Wolfe line search, requiring only the strong convexity and gradient Lipschitz conditions from Assumptions 2.1 and 2.2. Our proof leverages the fundamental inequality in (15) from Proposition 3.1 and lower bounds on the terms that appear in the contraction factor. Here, we set the weight matrix \(P\) to \(P=LI\) and hence define the initial weighted matrix \(_{0}\) as \(_{0}=B_{0}\). The following theorem presents our first global linear convergence rate of BFGS for any \(B_{0}_{++}^{d}\).

**Theorem 4.1**.: _Suppose Assumptions 2.1 and 2.2 hold. Let \(\{x_{t}\}_{t 0}\) be the iterates generated by BFGS, where the step size satisfies the Armijo-Wolfe conditions in (5) and (6). For any initial point \(x_{0}^{d}\) and any initial Hessian approximation matrix \(B_{0}_{++}^{d}\), we have_

\[)-f(x_{*})}{f(x_{0})-f(x_{*})}1-e^{- _{0})}{t}}^{t},  t 1.\] (17)

**Remark 4.1**.: _In , the authors analyzed BFGS with exact line search and established a global linear rate of \((1-e^{-_{0})}{t}})})^{t}\). In comparison, our result in (17) achieves a faster linear rate by eliminating the \(\) factor in the denominator. This improvement arises from using the Armijo-Wolfe conditions. Specifically, under these conditions, we show \()-f(x_{t+1})}{-^{}_{t}}{-_{t }^{}_{t}}}\) as shown in Lemma 2.1, where \((0,1/2)\) is a line search parameter. In contrast, using exact line search, the authors in  proved that \()-f(x_{t+1})}{-}{s_{t}}_{t}} {+1}\), thus leading to the extra \(\) factor in their rate._

From Theorem 4.1, we observe that the linear convergence rate is determined by the quantity \((_{0})\) Thus, to simplify our bounds, we consider two different initializations: \(B_{0}=LI\) and \(B_{0}= I\).

**Corollary 4.2**.: _Suppose Assumptions 2.1 and 2.2 hold, \(\{x_{t}\}_{t 0}\) are generated by BFGS with step size satisfying the Armijo-Wolfe conditions in (5) and (6), and \(x_{0}^{d}\) is an arbitrary initial point._* _If the initial Hessian approximation matrix is set as_ \(B_{0}=LI\)_, then for any_ \(t 1\)__ \[)-f(x_{*})}{f(x_{0})-f(x_{*})}(1-)^{t}.\] (18)
* _If the initial Hessian approximation matrix is set as_ \(B_{0}= I\)_, then for any_ \(t 1\) _we have_ \()-f(x_{*})}{f(x_{0})-f(x_{*})}(1-e^{-})^{t}\)_. Moreover, for_ \(t d\)_, we have_ \[)-f(x_{*})}{f(x_{0})-f(x_{*})}(1-)^{t}.\] (19)

Corollary 4.2 shows that when initialized with \(B_{0}=LI\), BFGS achieves a linear rate of \(((1-)^{t})\) from the first iteration, matching the rate of gradient descent. It also indicates that initializing with \(B_{0}= I\) achieves a similar rate but after \(d\) iterations. While this suggests a preference for initializing with \(B_{0}=LI\), subsequent analysis reveals that with enough iterations, BFGS with either initialization can attain a faster linear rate independent of \(\). In some cases, starting with \(B_{0}= I\) may lead to fewer total iterations to achieve this faster rate. We will explore this trade-off later.

## 5 Condition Number Independent Linear Convergence Rates

In this section, we improve the previous results and establish a non-asymptotic, condition number-free global linear convergence rate for BFGS with the Armijo-Wolfe line search. This requires the additional assumption that the Hessian is Lipschitz continuous. Our analysis builds on the previous methodology but uses \(P=^{2}f(x_{*})\) instead of \(P=LI\) to prove the condition number-independent global linear rate. Thus, the weighted initial matrix \(_{0}\) is \(^{2}f(x_{*})^{-}B_{0}^{2}f(x_{*})^{-}\). Next, we present a general global convergence bound for any initial Hessian approximation \(B_{0}^{d}_{++}\).

**Proposition 5.1**.: _Suppose Assumptions 2.1, 2.2 and 2.3 hold. Let \(\{x_{t}\}_{t 0}\) be the iterates generated by BFGS with the step size satisfying the Armijo-Wolfe conditions in (5) and (6). Recall the definition of \(C_{t}\) in (9) and \(()\) in (10). For any initial point \(x_{0}^{d}\) and any initial Hessian approximation matrix \(B_{0}^{d}_{++}\), the following result holds:_

\[)-f(x_{*})}{f(x_{0})-f(x_{*})}(1-2(1-)e^{- _{0})+3_{i=0}^{t-1}C_{i}}{t}})^{t}, t  1.\]

Proposition 5.1 demonstrates that the convergence rate of BFGS with the Armijo-Wolfe line search is influenced by \((_{0})\) and the sum \(_{i=0}^{t-1}C_{i}\). The first term \((_{0})\) is a constant that depends on our choice of the initial Hessian approximation matrix \(B_{0}\). The second term \(_{i=0}^{t-1}C_{i}\) can also be upper bounded using the non-asymptotic global linear convergence rate provided in Theorem 4.1.

**Theorem 5.2**.: _Suppose Assumptions 2.1, 2.2 and 2.3 hold, and let \(\{x_{t}\}_{t 0}\) be the iterates generated by BFGS with the Armijo-Wolfe line search in (5) and (6). Then, for any initial point \(x_{0}^{d}\) and any initial Hessian approximation \(B_{0}^{d}_{++}\), if \(t(_{0})+3C_{0}(_{0})+C_ {0}\), we have_

\[)-f(x_{*})}{f(x_{0})-f(x_{*})}(1-)^{t}.\] (20)

This result shows that when the number of iterations meets \(t(_{0})+3C_{0}(_{0})+C_ {0}\), BFGS with Armijo-Wolfe conditions achieves a condition number-independent linear rate. The choice of \(B_{0}\) is critical as it influences the required iterations through \(_{0}=^{2}f(x_{*})^{-}B_{0}^{2}f(x_{*})^{- }\) and \(_{0}=B_{0}\). Different choices of \(B_{0}\) affect \((_{0})+3C_{0}(_{0})\) and thus the number of iterations needed for condition-free linear convergence. While optimizing \(B_{0}\) to minimize \((_{0})+3C_{0}(_{0})\) is possible, we focus on two practical initialization schemes: \(B_{0}=LI\) and \(B_{0}= I\).

**Corollary 5.3**.: _Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Let \(\{x_{t}\}_{t 0}\) be the iterates generated by the BFGS method, where the step size satisfies the Armijo-Wolfe conditions in (5) and (6), and \(x_{0}^{d}\) as an arbitrary initial point. Then, given the result in Theorem 5.2, we have_

* _If we set_ \(B_{0}=LI\)_, the rate in (_20_) holds for_ \(t d+C_{0}\)_,_
* _If we set_ \(B_{0}= I\)_, the rate in (_20_) holds for_ \(t(1+3C_{0})d+C_{0}\)_._

Based on Corollary 5.3, if \(C_{0}\), or equivalently \(f(x_{0})-f(x_{*})}{M^{2}}\), then BFGS with \(B_{0}= I\) requires less iterations to achieve the condition number-independent linear convergence rate.

## 6 Global Superlinear Convergence Rates

In this section, we present our global superlinear result. Consider the definition \(_{0}=^{2}f(x_{*})^{-}B_{0}^{2}f(x_{*})^{-}\) as well as the definition of \(_{t}\) which is given by

\[_{t}:=^{}d_{t}}{\|_{t}\|^{2}},_{t}: =^{2}f(x_{*})^{}d_{t}, t 0.\] (21)

To motivate, let us briefly discuss why we are only able to show a linear convergence rate instead of a superlinear rate in Theorem 5.2. By inspecting the proof, we observe that the bottleneck is due to the lower bounds on \(_{t}\) and \(_{t}\): we used \(_{t}\) and \(_{t} 1-\) from Lemma 2.1, which leads to the constant factor \((1-)\) in the final linear rate in Theorem 5.2. Thus, to show a superlinear convergence rate, we need to establish tighter lower bounds for \(_{t}\) and \(_{t}\). In the following lemma, we show that if the step size \(_{t}=1\), we are able to establish such tighter lower bounds.

**Lemma 6.1**.: _Recall \(_{t}=)-f(x_{t+1})}{-_{t}_{t}}\) and \(_{t}=_{t}^{}_{t}}{-_{t}^{}_{ t}}\) defined in (16). If the unit step size \(_{t}=1\) satisfies the Armijo-Wolfe conditions (5) and (6), then we have_

\[_{t} 1-}{2_{t}},_{t})_{t}}.\] (22)

In contrast to the constant lower bounds in Lemma 2.1, the lower bounds in (22) depend on \(C_{t}\) and \(_{t}\). Later, we show \(C_{t} 0\) and \(_{t} 1\). Hence, the lower bounds in (22) approach 1 as the number of iterations increases, enabling us to prove a superlinear rate. That said, the lower bounds in Lemma 6.1 hold only when \(_{t}=1\). To complete the picture, we need to quantify when and how often the unit step size is selected during BFGS execution. This is addressed in the next lemmas.

**Lemma 6.2**.: _Suppose Assumptions 2.1, 2.2, and 2.3 hold and define the constants_

\[_{1}\!:=\!\{,\,-1,}\,-1\},\;_{2}:=\{,}\},\;_{3}:=},\] (23)

_which satisfy \(0<_{1}<_{2}<1<_{3}\). If \(C_{t}_{1}\) and \(_{2}_{t}_{3}\), then \(_{t}=1\) satisfies the Armijo-Wolfe conditions (5) and (6)._

Lemma 6.2 shows that when \(C_{t}_{1}\) and \(_{t}\) falls within the interval \([_{2},_{3}]\), the step size \(_{t}=1\) is admissible and meets the Armijo-Wolfe conditions. Note that by the linear convergence result in Theorem 4.1, the first condition on \(C_{t}\) will be satisfied when \(t\) is sufficiently large. Additionally, using Proposition G.2 in the Appendix, we can show that the second condition on \(_{t}\) is violated only for a finite number of iterations. These observations are formally presented in the following lemma.

**Lemma 6.3**.: _Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates \(\{x_{t}\}_{t 0}\) are generated by the BFGS method with step size satisfying the Armijo-Wolfe conditions in (5) and (6). Recall \(C_{t}\) defined in (9), \(()\) defined in (10), \(\{_{i}\}_{i=1}^{3}\) defined in (23) and \(_{0}=B_{0}\). We have \(C_{t}_{1}\) when_

\[t t_{0}:=\{(_{0}), }{_{1}}\}.\] (24)

_Moreover, if we define \((x)=x-(1+x)\), the size of the set \(I=\{t:\;_{t}[_{2},_{3}]\}\) is at most_

\[|I|_{4}(_{0})+2C_{0}(_{0})+ }{(1-)},_{4}:=-1),(_{3}-1)\}}.\] (25)

Lemma 6.3 implies that conditions \(C_{t}_{1}\) and \(_{t}[_{2},_{3}]\) will be satisfied for all but a finite number of iterations. Thus, if the line search always starts by testing the unit step size (as shown in Section 7), we will choose \(_{t}=1\), and accordingly, the tighter lower bound in Lemma 6.1 will apply for all but a finite number of iterations. By applying these lower bounds along with (15) from Proposition 3.1, we can prove a global superlinear convergence rate, as presented next.

**Remark 6.1**.: _Lemmas 6.2 and 6.3 are inspired by the analysis in . Specifically, Lemma 5.10 of  characterized the conditions on \(C_{t}\) and \(_{t}\) under which \(=1\) satisfies the Armijo condition (5), and further bounded the number of iterations where these conditions are violated. However, our Lemma 6.2 addresses both the Armijo condition in (5) and the curvature condition in (6), and the arguments appear simpler. Additionally, our proof for the superlinear convergence rate differs from . Their approach analyzed the Dennis-More ratio and measured "local" superlinear convergence using the distance \(\| f(x_{*})^{}(x_{t}-x_{*})\|\). In contrast, our "global" result is based on the unified framework in Proposition 3.1 and uses the function value gap as a measure of convergence._

**Theorem 6.4**.: _Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates \(\{x_{t}\}_{t 0}\) are generated by BFGS with step size satisfying the Armijo-Wolfe conditions in (5) and (6). Recall the definition of \(C_{t}\) in (9), \(()\) in (10), \(_{0}:=B_{0}\), \(_{0}:=^{2}f(x_{*})^{-}B_{0}^{2}f(x_{*})^{-}\), and \(_{1},_{2},_{3},_{4}\) in (23) and (25). Then, for any \(x_{0}^{d}\) and any \(B_{0}^{d}_{++}\), the following global superlinear result holds:_

\[)-f(x_{*})}{f(x_{0})-f(x_{*})}(( _{0})+(_{6}+_{8}C_{0})(_{0})+(}{(1-)}}{_{1}}+}{(1 -)}C_{0})}{t})^{t},\] (26)

_where \(\{_{i}\}_{i=5}^{8}\) defined below are constants that only depend on line search parameters \(\) and \(\),_

\[_{5}\!:=\!},4_{3}\}}{2_{2} -1-_{1}},\;_{6}\!:=\!,\;_{7 }\!:=\!1\!+\!_{4}_{6}\!+_{5},\;_{8}\!:=\!1\!+\!2_ {7}\!+\!\!-\!_{1}\!-\!_{2}}{2_{2}\!-\!1 \!-\!_{1}}.\]

The above result shows a global superlinear convergence rate of the form \(((}{t})^{t})\), where \(C^{}\) depends on the condition number \(\), the initial weighted distance \(C_{0}\), and the initial Hessian approximation matrix \(B_{0}\). To simplify the expression, we report the above bound for \(B_{0}=LI\) and \(B_{0}= I\).

**Corollary 6.5**.: _Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates \(\{x_{t}\}_{t 0}\) are generated by the BFGS method with step size satisfying the Armijo-Wolfe conditions in (5) and (6), and \(x_{0}^{d}\) as an arbitrary initial point. Then, given the result in Theorem 6.4, the following results hold:_

* _If we set_ \(B_{0}=LI\)_, then we have_ \[)-f(x_{*})}{f(x_{0})-f(x_{*})}(d+( }{(1-)}}{_{1}}+}{(1-)}C_{0})}{t})^{t}.\] (27)
* _If we set_ \(B_{0}= I\)_, then we have_ \[)-f(x_{*})}{f(x_{0})-f(x_{*})}(+ _{7}+_{8}C_{0})d+(}{(1-)} }{_{1}}+}{(1-)}C_{0})}{t} )^{t}.\] (28)

This result shows that BFGS with \(B_{0}=LI\) achieves a global superlinear rate of \(((}{t})^{t})\), while BFGS with the initialization \(B_{0}= I\) converges at a global superlinear rate of \(((d+C_{0}}{t})^{t})\). Hence, the superlinear result for \(B_{0}= I\) outperforms the rate for \(B_{0}=LI\) when \(C_{0}\).

**Remark 6.2**.: _We chose \(B_{0}=LI\) and \(B_{0}= I\) as two specific cases since they lead to explicit upper bounds in terms of the dimension \(d\) and the condition number \(\) in various theorems, simplifying the interpretation of our results. In practice, however, we often set \(B_{0}=cI\), where \(c=y}{\|s\|^{2}}\), with \(s=x_{2}-x_{1}\), \(y= f(x_{2})- f(x_{1})\), and \(x_{1},x_{2}\) as two randomly selected vectors. This choice ensures \(c[,L]\), and in the following numerical experiments, the performance of \(B_{0}=cI\) is similar to that of \(B_{0}= I\). The complexity of BFGS with this initialization is reported in Appendix H._

## 7 Complexity Analysis

**Discussions on the iteration complexity.** Using the three established convergence results in Theorems 4.1, 5.2 and 6.4, we can characterize the total number of iterations required for the BFGS method with the Armijo-Wolfe line search to find a solution with function suboptimality less than \(\). However, as discussed above, the choice of the initial Hessian approximation \(B_{0}\) heavily influences the number of iterations required to observe these rates. To simplify our discussion, we focus on two specific initializations: \(B_{0}=LI\) and \(B_{0}= I\).

**The case of \(B_{0}=LI\):** The overall iteration complexity of BFGS with \(B_{0}=LI\) is given by

\[(\{,(d+C_{0})+ ,}{(++}})} \}).\]

**The case of \(B_{0}= I\):** The overall iteration complexity of BFGS with \(B_{0}= I\) is given by

\[\!(\{d+,C_{0}(d +)+,}{ (\!+\!\!+\!(d+)} })}\}).\]We remark that the comparison between these two complexity bounds depends on the relative values of \(\), \(d\), \(C_{0}\), and \(\), and neither is uniformly better than the other. It is worth noting that for BFGS with \(B_{0}=LI\), we achieve a complexity that is consistently superior to the \(()\) complexity of gradient descent. Moreover, in scenarios where \(C_{0}=(1)\) and \(d\), BFGS with \(B_{0}= I\) could result in an iteration complexity of \((+)\), which is much more favorable than that of gradient descent. The proof of these complexity bounds can be found in Appendix I.

**Discussions on the line search complexity.** We present the log bisection algorithm to choose the step size \(_{t}\) at iteration \(t\) satisfying the Armijo-Wolfe conditions (5) and (6) in Algorithm 1 in Appendix J. We define \(_{min}\) and \(_{max}\) as the lower and upper bounds of the "slicing window" containing the trial step size \(_{t}\), respectively. We start with the initial trial step size \(_{t}=1\) and keep enlarging or decreasing it depending on whether the Armijo condition (5) or the curvature condition (6) is satisfied. Then, we dynamically update \(_{min}\), \(_{max}\) and shrink the size of this "slicing window" \((_{min},_{max})\). We pick the trial step size \(\) as the geometric mean of \(_{min}\) and \(_{max}\), i.e., \(=(_{max}+_{max})/2\), which is the reason why we call this algorithm "log bisection". Note that in each loop of Algorithm 1, we query the function value and gradient at most once to check the Armijo-Wolfe conditions at Lines 2 and 9. The next theorem characterizes the average number of function value and gradient evaluations per iteration in Algorithm 1 after \(t\) iterations, denoted by \(_{t}\), which is equivalent to the average number of loops per iterations.

**Theorem 7.1**.: _Suppose Assumptions 2.1, 2.2 and 2.3 hold. Let \(\{x_{t}\}_{t 0}\) be generated by BFGS with step size satisfying the Armijo-Wolfe conditions in (5) and (6) and is chosen by Algorithm 1. If we define \(:=((_{0})+)C_{0}\), then for any initial point \(x_{0}^{d}\) and initial Hessian approximation \(B_{0}^{d}_{++}\), the average number of the function value and gradient evaluations per iteration in Algorithm 1 after \(t\) iterations satisfies_

\[_{t} 2+_{2}1+++2_{2}_{2}16(1- )+_{2}1+)+_{0})+12 }{t}.\]

The above result shows that when we run BFGS for \(N\) iterations, the total number of function and gradient evaluations is \(N+N(1+)+N(1+_{ 0})+}{N})\). Thus, the total line search complexity can always be bounded by \((N((_{0})+))=(N\{ d, , C_{0}\})\). Furthermore, notice that when \(N\) is sufficiently large such that we reach the superlinear convergence stage, i.e., \(N=((_{0})+)\), the total line search complexity becomes \((N)\), which means the average number of function and gradient evaluations per iteration is a constant \((1)\). We report the line search complexity results of different \(B_{0}=LI\) and \(B_{0}= I\) in Appendix K.4.

## 8 Numerical Experiments

We conduct numerical experiments on a cubic objective function defined as

\[f(x)=(_{i=1}^{d-1}g(v_{i}^{}x-v_{i+1}^{}x)-  v_{1}^{}x)+\|x\|^{2},\] (29)

and \(g:\) is defined as

\[g(w)=|w|^{3}&|w|,\\  w^{2}-^{2}|w|+^{3}&|w|>,\] (30)

where \(,,,\) are hyper-parameters and \(\{v_{i}\}_{i=1}^{n}\) are standard orthogonal unit vectors in \(^{d}\). We focus on this objective function because it is used in  to establish a tight lower bound for second-order methods. We compare the convergence paths of BFGS with an inexact line search step size \(_{t}\) that satisfies the Armijo-Wolfe conditions (5) and (6) for various initialization matrices \(B_{0}\): specifically, \(B_{0}=LI\), \(B_{0}= I\), \(B_{0}=I\), and \(B_{0}=cI\) where \(c\) is defined in Remark 6.2. It is easily verified that \(c[,L]\). We also compare the performance of BFGS methods to the gradient descent (GD) method with backtracking line search, using \(=0.1\) in condition (5) and \(=0.9\) in condition (6). Step size \(_{t}\) is chosen at each iteration via log bisection in Algorithm 1. Empirical results are compared across various dimensions \(d\) and condition numbers \(\), with the x-axis representing the number of iterations \(t\) and the y-axis showing the ratio \()-f(x_{*})}{f(x_{0})-f(x_{*})}\).

First, we observe that BFGS with \(B_{0}=LI\) initially converges faster than BFGS with \(B_{0}= I\) in most plots, aligning with our theoretical findings that the linear convergence rate of BFGS with \(B_{0}=LI\) surpasses that of \(B_{0}= I\) in Corollary 4.2. In Corollary 4.2, we show that BFGS with \(B_{0}=LI\) could achieve the linear rate of \((1-1/)\) from the first iteration while BFGS with \(B_{0}= I\) needs to run \(d\) to reach the same linear rate. Second, the transition to superlinear convergence for BFGS with \(B_{0}= I\) typically occurs around \(t d\), as predicted by our theoretical analysis. Although BFGS with \(B_{0}=LI\) initially converges faster, its transition to superlinear convergence consistently occurs later than for \(B_{0}= I\). Notably, for a fixed dimension \(d=600\), the transition to superlinear convergence for \(B_{0}=LI\) occurs increasingly later as the problem condition number rises, an effect not observed for \(B_{0}= I\). This phenomenon indicates that the superlinear rate for \(B_{0}=LI\) is more sensitive to the condition number \(\), which corroborates our results in Corollary 6.5. In Corollary 6.5, we present that BFGS with \(B_{0}=LI\) needs \(d\) steps to reach the superlinear convergence stage while this is improved to \(d\) for BFGS with \(B_{0}= I\). Moreover, the performance of BFGS with \(B_{0}=I\) and \(B_{0}=cI\) is similar to BFGS with \(B_{0}= I\). Notice that the initializations of \(B_{0}=I\) and \(B_{0}=cI\) are two commonly-used practical choices of the initial Hessian approximation matrix \(B_{0}\).

## 9 Conclusions, Limitations, and Future Directions

In this paper, we analyzed the global non-asymptotic convergence rates of BFGS with Armijo-Wolfe line search. We showed for an objective function that is \(\)-strongly convex with an \(L\)-Lipschitz gradient, BFGS achieves a global convergence rate of \((1-1/)^{t}\), where \(=L/\). Additionally, assuming the Hessian is \(M\)-Lipschitz, we showed BFGS achieves a linear convergence rate determined solely by the line search parameters, independent of the condition number. Under similar assumptions, we also established a global superlinear convergence rate. Given these bounds, we determined the overall iteration complexity of BFGS with the Armijo-Wolfe line search and specified this complexity for initial Hessian approximations \(B_{0}=LI\) and \(B_{0}= I\).

One limitation of this paper is that the analysis only applies to strongly convex functions. Developing an analysis for the general convex setting is still unsolved. Another drawback is that we focus solely on the BFGS method. Extending our theoretical results to the entire convex Broyden's class of quasi-Newton methods, including both BFGS and DFP, is a natural next step.

Figure 1: Convergence curves of BFGS with inexact line search of different \(B_{0}\) and gradeint descent with backtracking line search.