# The Evolution of Statistical Induction Heads:

In-Context Learning Markov Chains

 Ezra Edelman

University of Pennsylvania

ezrae@cis.upenn.edu

&Nikolaos Tsilivis

New York University

nt2231@nyu.edu

Benjamin L. Edelman

Harvard University

bedelman@g.harvard.edu

&Eran Malach

Harvard University

emalach@g.harvard.edu

&Surbhi Goel

University of Pennsylvania

surbhig@cis.upenn.edu

Equal Contribution

Work done while visiting Harvard University.

###### Abstract

Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form _statistical induction heads_ which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between the transformer's layers, and uncovering evidence that the presence of the simpler unigram solution may delay formation of the final bigram solution. We examine how learning is affected by varying the prior distribution over Markov chains, and consider the generalization of our in-context learning of Markov chains (ICL-MC) task to \(n\)-grams for \(n>2\).

## 1 Introduction

Large language models (LLMs) exhibit a remarkable ability to perform _in-context learning_ (ICL) from patterns in their input context . The ability of LLMs to adaptively learn from context is profoundly useful, yet the underlying mechanisms of this emergent capability are not fully understood.

In an effort to better understand ICL, some recent works propose to study ICL in controlled synthetic settings--in particular, training transformers on mathematically defined tasks which require learning from the input context. For example, a recent line of works studies the ability of transformers to perform ICL of standard supervised learning problems such as linear regression . Studying these well-understood synthetic learning tasks enables fine-grained control over the data distribution, allows for comparisons with established supervised learning algorithms, and facilitates the examination of the in-context "algorithm" implemented by the network. That said, these supervised settings are reflective specifically of _few-shot learning_, which is only a special case of the more general phenomenon of networks incorporating patterns from their context into theirpredictions. A few recent works  go beyond the case of cleanly separated in-context inputs and outputs, studying in-context learning on distributions based on discrete stochastic processes.

The goal of this work is to propose and analyze a simple synthetic setting for studying ICL. To achieve this, we consider \(n\)-gram models , one of the simplest and oldest methods for language modeling. An \(n\)-gram language model predicts the probability of a token based on the preceding \(n-1\) tokens, using fixed-size chunks (\(n\)-grams) of text data to capture linguistic patterns. Our work studies ICL of \(n\)-gram models, where the network needs to compute the conditional probability of the next token based on the statistics of the tokens observed in the input context, rather than on the statistics of the entire training data. We mainly focus on the simple case of \(n=2\); i.e., bigram models, which can be represented as Markov chains. We therefore consider ICL of Markov chains (ICL-MC): we train two layer attention-only transformers on sequences of tokens, where each sequence is produced by a different Markov chain, generated using a different transition matrix (see Figure 1 (left)).

By studying ICL-MC, we are able to replicate and study multiple phenomena that have been observed in ICL for LLMs, and identify new ones. We demonstrate our findings using a combination of empirical observations on transformers trained from scratch on ICL-MC and theoretical analysis of a simplified linear transformer. Our key findings are summarized below:

(1) Transformers learn statistical induction heads to optimally solve ICL-MC.Prior work studying ICL in transformers revealed the formation of _induction heads_, a circuit that looks for recent occurrence(s) of the current token, and boosts the probabilities of tokens which followed in the input context. We show that in order to solve ICL-MC, transformers learn _statistical_ induction heads that are able to compute the correct _conditional (posterior) probability_ of the next token given all previous occurrences of the prior token (see attention patterns in Figure 2). We show that these statistical induction heads lead to the transformer achieving performance approaching that of the Bayes-optimal predictor.

(2) Transformers learn predictors of increasing complexity and undergo a phase transition when increasing complexity.We observe that transformers display _phase transitions_ when learning Markov chains--learning appears to be separated into phases, with fast drops in loss between the phases. We are able to show that different phases correspond to learning models of increased complexity--unigrams, then bigrams (see Figure 1)--and characterize the transition between the phases. We also consider the \(n\)-gram generalization of our setting, where the next token is generated based on the previous \(n-1\) tokens, and observe a similar multi-stage learning process.

(3) Simplicity bias may slow down learning.We provide evidence that the model's inherent bias towards simpler solutions (in particular, in-context unigrams) causes learning of the optimal solution

Figure 1: (_left_) We train small transformers to perform in-context learning of Markov chains (ICL-MC). Each training sequence is generated by sampling a transition matrix from a prior distribution, and then sampling a sequence from this Markov chain. (_right_) Distance of a transformer’s output distribution to several well-defined strategies over the course of training on our in-context Markov chain task. The model passes through three stages: (1) predicting a uniform distribution (_blue_ region), (2) predicting based on in-context unigram statistics (_orange_ region), (3) predicting based on in-context bigram statistics (_green_ region). Shading is based on the minimum of the curves.

to be delayed. Changing the distribution of the in-context examples to remove the usefulness of in-context unigrams leads to faster convergence, even when evaluated on the original distribution.

(4) Alignment of layers is crucial.We show that the transition from a phase of learning the simple-but-inadequate solution to the complex-and-correct solution happens due to an alignment between the layers of the model: the learning signal for the first layer is tied to the extent to which the second layer approaches its correct weights.

### Related Work

**In-Context Learning.** In , the authors discuss how properties of the data distribution promote ICL. Xie et al.  suggest a Bayesian interpretation of ICL and studies how ICL emerges when the training distribution comes from a Hidden Markov Model (HMM). Abernethy et al.  study the ability of transformers to segment the context into pairs of examples and labels and provide learning guarantees when the labeling is of the form of a sparse function. The work of Bietti et al.  studies the dynamics of training transformers on a task that is reminiscent of our Markov chain setting but has additional complexities. Instead of drawing a fresh Markov chain for each sequence, in their task all sequences are sampled from the same Markov chain; after certain 'trigger' tokens, the following 'output' token is chosen deterministically within a sequence. Thus, successful prediction requires incorporating both global bigram statistics and in-context deterministic bigram copying, unlike in our setting where the patterns computed by _statistical_ induction heads are necessary and sufficient. As in our work, the authors identify multiple distinct stages of training and show how multiple top-down gradient steps lead to a solution.

**Induction Heads.** Elhage et al.  relates ICL with the formation of induction heads, sub-components of transformers that match previous occurrences of the current token, retrieving the token that succeeds the most recent occurrence. Reddy  studies the formation of induction heads and their role in ICL, showing empirically that a three layer network exhibits a sudden formation of induction heads towards solving some ICL problem of interest. Bietti et al.  study the effect of specific trigger tokens on the formation of induction heads.

**Phase Transitions.** It has been observed in different contexts that neural networks and language models display a sudden drop in loss during their training process. This phase transition is often related to emergence of new capabilities in the network. The work of Power et al.  observed the "grokking" phenomenon, where the test loss of neural networks sharply drops, long after the network overfits the training data. Chen et al.  shows another example of a phase transition in language model training, where the formation of specific attention mechanisms happen suddenly in training, causing the loss to quickly drop. Barak et al.  observe that neural networks trained on complex learning problems display a phase transition when converging to the correct solution. Several works [25; 27] attribute these phase transitions to rapid changes in the inductive bias of networks, while Merrill et al.  argue that the models are sparser after the phase change. Schaeffer et al.  warn that phenomena in deep learning that seem to be discontinuous can actually be understood to evolve continuously once seen through the right lens.

**Simplicity Bias.** Various works observed that neural networks have a "simplicity bias", which causes them to "prioritize" learning simple patterns first [5; 39]. The work of Kalimeris et al.  shows that SGD learns functions of increased complexity, first fitting a linear concept to the data before moving to more complex functions.  shows that the simplicity bias of neural networks can sometimes be harmful, causing them to ignore important features of the data. Chen et al.  demonstrate the effect of simplicity bias on language tasks that require understanding of syntactic structure. Abbe et al.  provide a theoretical framework for understanding how the simplicity of the target function can govern the convergence time of SGD, describing how simple partial solutions can speed up learning; in contrast, in our setting, the unigram solution appears likely to be a distractor which delays learning of the correct solution.

**Concurrent works.** In parallel to this work, there have been a number of papers devoted to the study of similar questions regarding in-context learning or Markov chains: Akyurek et al.  empirically compare the ability of different architectures to perform in-context learning of regular languages. Their experiments with synthetic languages motivate architectural changes which improve natural language modeling in large scale datasets. Hoogland et al.  observe similar stage-wise learning behaviors on transformers trained on language or synthetic linear regression tasks. Makkuva et al.

 study the loss landscape of transformers trained on sequences sampled from a single Markov Chain. Perhaps closest to our work, Nichani et al.  introduces a general family of in-context learning tasks with causal structure, a special case of which is in-context Markov chains. The authors prove that a simplified transformer architecture (similar to the one we introduce in Section 2.2) can learn to identify the causal relationships by training via gradient descent, and also characterize the ability of the trained models to adapt to out-of-distribution data. The focus of our work, instead, is on the different stages of training and how they relate to specific, well-defined, strategies.

## 2 Setup

In this section, we describe our learning problem and present the neural network architectures that we will use for learning.

ICL-MC Task.Our learning task consists of sequences generated from Markov Chains with random transition matrices. The goal is to in-context estimate the transition probabilities from sampled sequences, in order to predict the next state. Formally, each sample sequence is generated by a Markov Chain with state space \(S=\{1,,k\}\) and a transition matrix \(\) sampled from a prior distribution, with \(x_{1}\) drawn from some other prior distribution (potentially dependent on \(\)), and the rest of \(=(x_{1},,x_{T})\) drawn from the Markov Chain. We primarily focus on the case where each row of the matrix is sampled from the Dirichlet distribution with concentration parameter \(\), i.e. \(_{i,:}()\). We want to learn a predictor that, given context \(x_{1},,x_{T}\), predicts the next token, \(x_{T+1}\). Note that this is an inherently non-deterministic task, even provided full information about the transition matrix, and as such it can better capture certain properties of language than previous in-context learning modeling approaches, such as linear regression . We focus on the case of the _flat_ Dirichlet distribution, with \(=(1,,1)^{}\), that corresponds to uniform transition probabilities between states. We draw the initial state \(x_{1}\) from the stationary distribution \(\) of the chain (which exists almost surely). We primarily consider the case where the number of states \(k\) is 2 or 3.

In subsection 3.3, we consider the generalization of this setting to \(n\)-grams for \(n>2\). Instead of the distribution of \(x_{T}\) being determined by \(x_{T-1}\), we let it be determined by \(x_{T-n+1},,x_{T-1}\), according to a conditional distribution \(\) which is uniform over the possible states3.

### Potential Strategies for (Partially) Solving ICL-MC

We adopt the Bayesian interpretation of in-context learning , in which a prior distribution is provided by the training data, and, at test time, the model updates this prior given the in-context sequence. In this framework, we focus on two strategies for Bayesian inference: a (suboptimal) _unigram_ strategy which assumes tokens in each sequence are i.i.d. samples (and counts the frequency of the states in the sequence so far), and the _bigram_ strategy which correctly takes into account dependencies among adjacent tokens (and counts frequency of pairs of tokens).

1st strategy: Unigrams.Since we initialize the Markov chain at its stationary distribution (which exists a.s.), the optimal strategy across unigrams is just to count frequency of states and form a posterior belief about the stationary distribution. Unfortunately, the stationary distribution of this random Markov chain does not admit a simple analytical characterization when there is a finite number of states, but it can be estimated approximately. At the limit of \(k\), the stationary distribution converges to the uniform distribution .

2nd strategy: Bigrams.For any pair of states \(i\) and \(j\), let \(_{ij}\) be the probability of transitioning from \(i\) to \(j\). On each sample \(\), we can focus on the transitions from the \(i\)-th state, which follow a categorical distribution with probabilities equal to \((_{i1},,_{ik})\). If we observe the in-context empirical counts \(\{c_{ij}\}_{j=1}^{k}\) of the transitions, then \(_{ij}\) is given by: \((_{i1},,_{ik})|(k,c_{i1 }+_{1},,c_{ik}+_{k}).\), where \(_{1},,_{k}\) are the Dirichlet concentration parameters of the prior. Hence, each \(_{ij}\) has a (marginal) distribution that is actually a Beta distribution: \(_{ij}|(c_{ij}+_{j},_{j} _{j}+N_{i}-_{j}-c_{ij})\), where \(N_{i}\) is the total number of observed transitions from state \(i\). As such, our best (point) estimate for each state \(j\) is given by: \([_{ij}|]=+_{j}}{N+_{ i}_{i}}\). For the uniform Dirichlet, \(=(1,,1)^{}\), it is \([_{ij}|]=+1}{N_{i}+k}\).

### Architectures: Transformers and Simplifications

We are interested in investigating how transformers  can succeed in in-context learning this task. We focus on attention-only transformers with 2 layers with causal masking which is a popular architecture for language modeling. Given an input sequence \(\), the output of an \(n\)-layer attention-only transformer4 is:

\[TF(E)=P(Attn_{n}+I)(Attn_{1}+I) E,\] (1)

where \(E^{T d}\) is an embedding of \(^{d}\), \(P^{d k}\) is a linear projection to the output logits, and \(Attn()\) is masked self attention with relative position embeddings , which is parameterized by \(W_{Q},W_{K},W_{V}^{d d},v^{T d}\):

\[Attn(z)=((A))zW_{V}, A_{i,j}=W_{Q })(z_{j}W_{K}+v_{i-j+1})^{}}{}.\] (2)

In general, transformers often contain an MLP module, but for this task they are not necessary (see Appendix A and Figure 10 for additional experiments with transformers with MLPs). During training, we minimize the loss:

\[L()=*{}_{\\ ()^{k}}[_{i=1} ^{T}l(TF(;)_{i},x_{i+1})],\] (3)

where \(\) denotes the parameters of the model and \(l\) is the cross entropy loss. Notice that we provide supervision in all positions, as standard in language modeling.

We now show how a two-layer transformer of the above architecture can represent the optimal bigrams solution.

**Proposition 2.1** (Transformer Construction).: _A single-head two layer attention-only transformer can find the bigram statistics in the in-context learning Markov chain task._

Intuitively, the first layer of the transformer copies the previous token at each position, and in the second layer each token sums the embeddings of all the tokens whose output from the first layer matches itself. The full proof can be found in Appendix B.1.

Simplified Transformer Architecture.As we see from the construction, there are two main ingredients in the solution realized by the transformer; (1st layer) the ability to look one token back and (2nd layer) the ability to attend to itself. For this reason, we define a _minimal model_ that is

Figure 2: Attention patterns that correspond to the last token of the sequence for a transformer trained to perform ICL-MC. The intensity of each blue line signifies the strength of the corresponding attention value. As the model gets trained, we observe that the attention weights mimic the construction of Proposition 2.1. Specifically, at the end of training (_right_), each token in the first layer is attending to the previous token. In the second layer, the last token, a “2”, is attending to tokens that followed “2”s, allowing bigram statistics to be calculated. See also Figure 9 for full attention matrices during the course of training.

expressive enough to be able to represent such a solution, but also simple enough to be amenable to analysis. Let \(e_{x_{i}}\) denote the one-hot embedding that corresponds to the state at position \(i[T]\), and let \(E\) be the \(^{(T+1) k}\) one-hot embedding matrix. Then the model is parameterized by \(W^{k k}\) and \(v^{T+1}\) and defined as:

\[f(E)=(EW((M)E)^{})E, M=v_{0} &-&&-\\ v_{1}&v_{0}&&-\\ &&&\\ v_{T}&v_{T-1}&&v_{0}^{(T+1)(T+1)},\] (4)

where mask \(()\) is a causal mask, and \((M)_{i,j}=)}{_{t=0}^{T}(M_{T,j})}\). Notice that the role of \(W\) is to mimic the attention mechanism of the second layer and the role of \(v\) is that of the relative positional embeddings. This model can be seen as a simplified version of a two-layer linear attention-only transformer. See also Appendix B.2 for a discussion.

_Fact 2.2_.: Both the bigrams strategy and the unigrams strategy can be expressed by the minimal model with a simple choice of weights.

* **Bigrams:** Let \(v=(0 c 0 0)^{}\) and \(W=I_{k k}\), then \(f(E)_{T,s}=_{t=2}^{T}\{x_{t}=s\}\{x_{ t-1}=x_{T}\}+O(}{(c)})\).
* **Unigrams:** For \(v=(0 0)^{}\), \(W=11^{}\), we have \(f(E)_{T,s}=_{t=1}^{T}\{x_{t}=s\}\).

See Section B for the proofs.

Figure 3: A two layer transformer (_top_) and a minimal model (_bottom_) trained on our in-context Markov Chain task. A comparison of the two layer attention-only transformer and minimal model (4) (with \(v\) and \(W\) initialized to \(0\)). The graphs on the left are test loss measured by KL-Divergence from the underlying truth. The orange line shows the loss of the unigram strategy, and the green line shows the loss of the bigram strategy. The middle graph shows the effective positional encoding (for the transformer, these are for the first layer, and averaged over all tokens). The graph on the right shows the KL-divergence between the outputs of the models and three strategy. The lower the KL-divergence, the more similar the model is to that strategy.

## 3 Empirical Findings and Theoretical Validation

In this section, we present our empirical findings on how transformers succeed in in-context learning Markov Chains5, we demonstrate the different learning stages during training and the sudden transitions between them, and draw analytical and empirical insights from the minimal model.

### Transformers In-Context Learn Markov Chains Hierarchically

We focus on attention-only transformers with 2 layers with causal masking and relative positional encodings and train them with the Adam optimizer on ICL-MC (see Section A for experimental details). As can be seen in Figure 3, all the models converge near the Bayesian optimal solution, suggesting that they learn to implement the bigram strategy. Curiously, however, learning seems to be happening in stages; there is an initial rapid drop and the model quickly finds a better than random solution. Afterwards, there is a long period of only slight improvement before a second rapid drop brings the model close to the Bayes optimal loss. We observe that training a 1-layer transformer fails to undergo a phase transition or converge to the right solution, even if trained for 10x the amount of time - see Figure 7 in the Appendix.

Interestingly, as can be seen from the horizontal lines in Figure 3, the intermediate plateau corresponds to a phase when the model reaches the unigram baseline. We provide evidence that this is not a coincidence, and that after the initial drop in loss, the model's strategy is very similar to the unigram strategy, before eventually being overtaken by the bigram strategy. Some of the strongest such evidence is on the right in Figure 3, where we plot the KL divergence between model's prediction and the two different strategies. For both the strategies, their KL divergence from the model quickly goes down, with the unigram solution being significantly lower. Around the point of the second loss drop, the KL divergence between the model and the bigram solution decreases, while the other one increases, making it clear that the model transitions from the one solution to the other. This final drop is what has been associated to prior work with _induction heads_ formation ; special dedicated heads inside a transformer are suddenly being formed to facilitate in-context learning. Similar observation hold for Markov Chains with a larger number of states - see Figures 8 and 11.

Mechanistic evidence for solutions found by transformer.To confirm how the two layer attention-only transformer solves ICL-MC, we inspected the attention in each layer throughout training. Figure 2 shows the attention for a particular input during different parts of training. By the end of training, the attention patterns match that of our construction in Proposition 2.1, with the first layer attending to tokens one in the past, and the second layer attending to tokens that follow the same token as the current one.

Figure 4: (_left_) Unigrams slow down optimization: Comparison of two-layer attention only transformers trained on two distributions; one with a uniformly random doubly stochastic transition matrix and another with a mixture of the doubly stochastic and unigrams distribution (see Appendix A.1 for details). We see that in absence of unigrams “signal” the model minimizes the loss (evaluated on the full distribution) much faster. (_center, right_) Training of the minimal model on ICL-MC with \(k=2\) states: (_center_) The heatmap of the second layer (\(W\) matrix) that learns to be close to diagonal. (_right_) The values of the positional embeddings (1st layer) that display a curious even/odd pattern. This is before any softmax is applied to the positional embeddings.

Varying the data distribution - Unigrams slow down learning.There are several interesting phenomena in the learning scenario that we just described, but it is the second drop (and the preceding plateau) that warrants the most investigation. In particular, one can ask the question: _is the unigram solution helpful for the eventual convergence of the model, or is it perhaps just a by-product of the learning procedure?_ To answer these questions, we define distributions over Markov chains that are in between the distribution where unigrams is Bayes optimal, and the distribution where unigrams is as good as uniform - see Appendix A for more details. As we see in Figure 4, the transformers that are being trained on the distribution where there is no unigrams "signal" train much faster. And even more tellingly, giving additional "unigram samples" curiously slows down learning. See also Figure 12 in the Appendix that displays how the models perform on different parts of the distribution during training.

### Theoretical Insights from the Minimal Model

To abstract away some of the many complicated components from the transformer architecture, we focus our attention now to the minimal model of Section 2.2. We train minimal models of eq. (4), starting from a deterministic constant initialization, by minimizing the cross entropy loss with SGD. Full experimental details can be found in Appendix A. Figure 3 (bottom) displays the training curves for the minimal model. Similar to the transformer, learning occurs in two stages and the models eventually converge close to the optimal solution.

We now provide theoretical insights on how training progresses stage by stage and how this is achieved by the synergy between the two layers. As it turns out, there need to be at least two steps of gradient descent in order for both elements of the solution to be formed.

**Proposition 3.1**.: _Let the model be defined as in eq. (4) and initialized with \(W^{(0)}=,v^{(0)}=\). Suppose the transition matrix \(P^{k k}\) is sampled from one of the following two types of distribution:_

1. \(k=2\)_, and_ \(P\) _is sampled from the uniform distribution over the set of_ \(2 2\) _stochastic matrices._
2. _For any constant_ \(k\) _and_ \(0<<1\)_, with probability_ \(\)_, sample the matrix_ \(P\) _uniformly from a "bigram-only" distribution--the set of_ \(k k\) _doubly stochastic matrices; and with probability_ \(1-\) _use a "unigram-only distribution": draw a vector_ \(u\) _uniformly from the set_ \(\{u_{ 0}^{k}:\|u\|_{1}=1\}\) _and let_ \(P=u^{}\)_._

_Then after one step of population gradient descent with step size \(>0\),_

\[W^{(1)}=( T)I+( T)^{}+Ev^{(1)}=\]

_where \(\|E\| O( T)\). In other words, for large \(T\), the second layer weights are a mixture of the correct solution \(I\) and uniform attention \(^{}\)._

_Assuming in the first step \(_{1}=O(})\), then \(W^{(2)}\) has the same structure as \(W^{(1)}\) (up to scaling). Furthermore,_

\[v_{1}^{(2)}=(_{2} T),v_{n}^{(2)} cv_{1}^{(2)} \; n 1,c<1\]

_where \(_{2}\) is the step size for the second step._

_If \(_{2}=(T)\), then the output of the model will be a weighted sum of bigrams and unigrams strategy. Formally,_

\[f(E)_{T,s}=(_{2}T)_{i=1}^{T}[x_{i-1}=x_{t},x_{i}= s]+(_{2}T)_{i=1}^{T}[x_{i}=s]+O( T)\]

NoteIn the first distribution (uniformly random \(2 2\)) or the second distribution with \(k>6\), at the end of the two steps, the weight on bigrams is greater than that of the weight on unigrams strategy.

Proof Overview.: The idea of the proof is that a first step of gradient descent with a small learning rate can align the second layer, while a second step can learn to identify the correct relative positional embedding. The identity bias of \(W\) in the second layer ensures there is a strong signal in the gradient to look back one in the first layer. Without a bias in \(W\), the gradient for the positional encodings, \(v\), is zero.

We get additional intuition from the second distribution (mixture of unigrams and doubly stochastic): in the first step, effectively all of the gradient comes from the examples where the unigram strategy is optimal, while in the second step effectively all of the gradient comes from the examples where the bigram strategy is optimal.

_Remark 3.2_.: It is worth noting that, while this is a simplified setting, the analysis goes beyond NTK-based  analyses where the representations do not change much and it crucially involves more than one step which has been a standard tool in the analysis of feature learning .

We summarize the key theoretical implications:

Learning occurs in two phases.Both in the theoretical and experimental models, training has two phases that work at very different speeds. The first phase is fast in both cases; in the theoretical setting, even a single step with step size \(O()\) is sufficient for learning the second layer. In the second phase, a much larger step size of \((1)\) is needed in order to learn the positional encodings (in one step).

Second layer is learned first.It has been observed before in a similar bigram learning setting with a two-layer transformer that the model might be learning first the second layer . We also make similar observations in our experiments with the minimal model and the transformers (see Figure 2). For the minimal model, the gradient calculations, clearly suggest that starting from a default initialization, it is only the second layer that quickly "picks up" the right solution.

Even/odd pattern in positional encodings emerges.We notice in the experiments, that the positional embeddings of both the transformer and minimal model displayed an intriguing even/odd oscillating pattern - see Figure 3 (_top, center_) and Figure 4 (_right_). We believe that a careful analysis the gradient of \(v\) in the second step will recover this pattern, which is likely related to the moments of the eigenvalues of the transition matrix.

### Beyond Bigrams: \(n\)-gram Statistics

Finally, we investigate the performance of transformers on learning in-context \(n\)-grams for \(n>2\); in particular, 3-grams. We train attention-only transformers with three heads in each layer6 by minimizing the in-context cross entropy loss with the Adam optimizer. As can be seen in Figure 5 (left), the model eventually converges to the Bayes optimal solution. Interestingly, as in the case of Markov Chains, the model displays a "hierarchical learning" behavior characterized by long plateaus and sudden drops. In this setup, the different strategies correspond to unigrams, bigrams and trigrams, respectively. This is presented clearly on the right of Figure 5, where we plot the similarity of the model with the different strategies and it exhibits the same clear pattern as in the case of \(n=2\). We leave a more thorough investigation of \(n\)-grams for future work.

## 4 Conclusion and Discussion

In this work, we have introduced a simple learning problem which serves as a controlled setting for understanding in-context learning and the emergence of (statistical) induction heads. Through a combination of theoretical analysis and empirical investigation, we identify different stages during learning, which we were able to precisely characterize. These validate similar observations from training large-scale language models.

The main limitation of our work is that our analysis relies on a simplified transformer architecture and our learning task is synthetic. Yet, we see the simplicity of our modeling as a positive, since it allows to make rigorous predictions about the mechanisms behind in-context learning abilities of a transformer. On the theoretical front, it would be interesting to extend our analysis to handle higher number of symbols and more complex models for language generation (beyond Markov chains).

On the empirical front, it would be worthwhile to understand similar stage-wise learning with natural language data, and use insights from our minimal model to improve formation of induction heads. In particular, it would be great to understand if better data curriculum could remove the undesirable simplicity bias we observe from unigrams. Such simple but incomplete solutions may be commonplace in language modeling and other rich learning settings; for any such solution, one can ask to what extent its presence speeds up or slows down the formation of more complex circuits with higher accuracy.

Acknowledgements.EE thanks Alan Yan for helpful conversations. NT acknowledges support through the National Science Foundation under NSF Award 1922658. NT would like to thank Boaz Barak, Cengiz Pehlevan and the whole ML Foundations Group at Harvard for their hospitality during Fall 2023 when most of this work was done. BE acknowledges funding from the ONR under award N00014-22-1-2377 and the NSF under award IIS 2229881. SG acknowledges support through the Open AI SuperAlignment Fast Grants.