# Evaluating and Inducing Personality

in Pre-trained Language Models

Guangyuan Jiang

1,2 jgy@stu.pku.edu.cn

&Manjie Xu

1,3 jgy@stu.pku.edu.cn

&Song-Chun Zhu

1,3 manjietisu@gmail.com

&Wenjuan Han

4,5 wjhan@bjtu.edu.cn

&Chi Zhang

3,5 zhangchi@bigai.ai

&Yixin Zhu

1,5 zixin.zhu@pku.edu.cn

\({}^{}\) G. Jiang and M. Xu contributed equally.

\({}^{}\) corresponding authors

\({}^{1}\) Institute for Artificial Intelligence, Peking University

\({}^{2}\) Yuanpei College, Peking University

\({}^{3}\) National Key Laboratory of General Artificial Intelligence, BIGAI

\({}^{4}\) Beijing Jiaotong University

https://sites.google.com/view/machinepersonality

###### Abstract

Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. In this study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by leveraging human psychometric tests in a **principled** and **quantitative** manner? If so, can we induce a specific personality in LLMs? To answer these questions, we introduce the Machine Personality Inventory (MPI) tool for studying machine behaviors; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise a Personality Prompting (P\({}^{2}\)) method to induce LLMs with specific personalities in a **controllable** way, capable of producing diverse and verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the essential indicator for various downstream tasks, and could further motivate research into equally intriguing human-like machine behaviors.

## 1 Introduction

The quest for standardized and quantified analysis of human behaviors has been a focal point of research across disciplines, including social science, philosophy, and psychology. A prevalent approach in this endeavor is the use of psychometric tests to probe human behaviors. Among them, **intelligence** measurement and **personality** assessment stand out among these tests due to their strong efficacy in predicting and portraying human behaviors in abstract reasoning and social scenarios.

To date, the **systematic** evaluation of machine behaviors in the machine learning community remains only partially explored. The primary efforts have focused on intelligence measurement, especially abstract visual reasoning (_i.e_., visual Raven tests (Barrett et al., 2018; Chollet, 2019; Zhang et al., 2019)), leaving other established facets of psychometric tests on machine behaviors largely untouched. Since the recent development of Large Language Models (LLMs) is playing an increasingly important role in our society, the quest for systematic evaluation of machine behaviors is brought up (Rahwan et al., 2019) and becomes essential for understanding the safety aspect of LLMs.

Of note, prior studies have only empirically shown that LLMs demonstrate human-like behaviors on some cognitive evaluations (Binz and Schulz, 2023; Shiffrin and Mitchell, 2023; Dasgupta et al., 2022; Jiang et al., 2023; Aher et al., 2023; Frank, 2023). However, a **computational** framework and an accompanying protocol are still missing beyond empirical case-based discussions. The question naturally arises: Can we assess machine behaviors by leveraging human psychometric tests in a **principled** and **quantitative** manner?

**Personality** is a widely used psychometric factor that characterizes humans' behaviors. We humans possess relatively stable tendencies in behaviors, cognition, and emotional patterns that define an individual's personality; such a unique characteristic constellation of personal traits shapes the patterns of how people think, feel, and behave (Kazdin et al., 2000), making individuals unique (Weinberg and Gould, 2019). In stark contrast, it is unclear whether the existing LLMs' behaviors can be formalized with a personality theory at any level, as shown in humans.

Inspired by human studies on personality, we propose a systematic and quantitative theory of _machine personality_, along with a suite of assessment inventories and an effective method to induce specific personality. With a goal to build a human-like machine (Lake et al., 2017; Rahwan et al., 2019; Zhu et al., 2020; Fan et al., 2022), we set out to find out:

_Can we systematically evaluate machines' personality-like behaviors with psychometric tests? If so, can we induce a specific personality in these LLMs?_

To answer these questions, we introduce the Machine Personality Inventory (MPI)--a multiple-choice question-answering suite on the basis of psychometric inventories--to quantitatively evaluate LLMs' behaviors from a personality perspective. Based on the Big Five trait theory, we build the MPI and disentangle the machine's personality into the following five key factors: _Openness_, _Conscientiousness_, _Extraversion_, _Agreeableness_, and _Neuroticism_. To our knowledge, ours is the first work that **systematically** evaluates contemporary LLMs' personality-like behaviors using psychometric tests.

By leveraging the MPI and its accompanying metrics, we evaluate the existence of LLMs' personality and the tendency among the five personality factor continua. Our experiments show that the stability of

Figure 1: **Evaluating and inducing personality in LLMs.** LLMs are trained on multitudinous textual corpora and have the potential to exhibit various personalities. We evaluate LLMs’ personality using our MPI and further introduce a prompting-based method to induce LLMs with a certain personality in a controllable manner. OCEAN refers to five key factors: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.

LLMs' quantified behavior tendency is considered an emergent ability (Wei et al., 2022), providing the first piece of evidence demonstrating that LLMs possess a certain level of personality: Alpaca and GPT-3.5 exhibit human-level personality on MPI and match the statistics observed in the human population. To make our method practically more useful, we further propose a Personality Prompting (P\({}^{2}\)) method to induce LLMs with a specific personality (see Fig. 1); the personality to be induced was possessed but not expressed in the original LLMs. Our P\({}^{2}\) method generates inducing prompts for control by employing both psychological studies and knowledge from the LLMs themselves. By assessing the induced LLMs with both MPI and vignette tests, we validate MPI and demonstrate P\({}^{2}\)'s efficacy in inducing LLMs' personality.

This work makes the following contributions:

* We introduce the topic of machine (_i.e._, LLM) personality based on personality trait theories and psychometric inventories as a systematic evaluation of LLM behaviors.
* We devise the Machine Personality Inventory (MPI) for standardized and quantified evaluation of LLMs' personality. Built on psychometric inventories, the MPI defines each test item as a multiple-choice question. Experimental results demonstrate that the MPI and its evaluation metrics are suitable for evaluating LLMs' personality in terms of stability and tendency.
* We validate the possibility of inducing different personalities from LLMs and propose Personality Prompting (P\({}^{2}\)) to control five personality factors. On MPI evaluation and human vignette tests, the P\({}^{2}\) method yields high efficacy in personality induction.

## 2 Related Work

LLMs as Proxies of Human BehaviorsThe increasing scaling and alignment of LLMs have enabled them atoply mimic human behaviors, ranging from reasoning and cognitive tests (Dasgupta et al., 2022; Webb et al., 2023; Binz and Schulz, 2023; Aher et al., 2023; Wong et al., 2023) to simulate social science and micro-societies experiments (Park et al., 2023; Ziems et al., 2023). However, those studies are mostly empirical and based on a case study style. Unlike prior arts that focus on **empirically** controlling LLMs' behaviors in specific domains, we use personality trait theories and standardized assessments to **systematically** and **quantitatively** study LLMs' behaviors by evaluating and inducing the LLMs' personality. Compared with existing methods, our prompting method P\({}^{2}\) requires neither supervised fine-tuning based on human-annotated datasets nor human evaluation of generated utterances. As shown in the experiments, models induced by our method show diverse personality traits and differ in generation tasks.

Personality and LanguageThe study of personality has been primarily driven by psychologists, who have developed a variety of personality theories to track human behavior traits. Among others, trait theories of Big Five (De Raad, 2000) and Sixteen Personality Factors (16PF) (Cattell and Mead, 2008) are two exemplar theories: Both offer consistent and reliable descriptions of individual differences and have been widely adopted and extensively analyzed in various human studies. Based on the trait theories, psychometric tests (_e.g._, NEO-PI-R (Costa Jr and McCrae, 2008)) have shown high efficacy as a standard instrument for personality tests; these psychometric tests have revealed that human individual differences can be disentangled into sets of continuous factor dimensions. Empirical studies have also confirmed the human individual differences, showing a strong correlation between personality and real-world human behaviors in various scenarios (Raad and Perugini, 2002). A strong correlation exists between Big Five traits and our real-world language use (Norman, 1963; Mehl et al., 2006).

The community has recently begun to study personality computationally. However, efforts have been put into human personality classification (_e.g._, Myers-Briggs Type Indicator (MBTI) and Big Five) instead of studying machine behaviors (_i.e._, the LLMs' personality), such as in recommendation (Farnadi et al., 2013; Mairesse et al., 2007; Oberlander and Nowson, 2006) or dialogue generation (Zhang et al., 2018). Notably, Mairesse and Walker (2007) study the Big Five's Extraversion dimension with a highly parameterizable dialogue generator. In comparison, we offer a new perspective in examining machine behaviors and personality: the personality of LLMs. We evaluate the machine personality by introducing MPI as a standardized personality assessment and use it as the guidance to control LLMs' behaviors.

Evaluating LLMs' Personality

Do LLMs have personalities? Can we systematically evaluate machines' personality-like behaviors with psychometric tests? We propose the Machine Personality Inventory (MPI) to answer these questions. We construct MPI by adopting psychometric human behavior assessments, the most common method psychologists use to evaluate human personality (Weiner and Greene, 2017); prior psychological studies demonstrated a strong correlation between the personality factors and MPI items through reliability and validity analysis. Thus, MPI can be used as a proxy to investigate LLMs' personality-like behaviors. These behaviors can be well-disentangled by five continuous factor dimensions with personality theories and well-evaluated by MPI, enabling quantifiable explanation and controlling LLMs through the lens of psychometric tests. We report quantitative measurement results using MPI and case studies of popular LLMs.

### Machine Personality Inventory (MPI)

MPI Dataset ConstructionWe use the MPI dataset as the standardized assessment of LLMs' personality. Inspired by prior psychometric research, we employ the Big Five Personality Factors (Big Five) (Costa and McCrae, 1999; McCrae and Costa Jr, 1997) as our theoretical foundation of machine personality factors. Big Five categorizes human personality using five key traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism, or OCEAN for short; we refer the readers to the adjectives from McCrae and John (1992) for better understanding the correspondence between the five factors and common descriptions:

* **Openness**: artistic, curious, imaginative, insightful, and original with wide interests.
* **Conscientiousness**: efficient, organized, planful, reliable, responsible, and thorough.
* **Extraversion**: active, assertive, energetic, enthusiastic, outgoing, and talkative.
* **Agreeableness**: appreciative, forgiving, generous, kind, and sympathetic.
* **Neuroticism**: anxious, self-pitying, tense, touchy, unstable, and worrying.

We build MPI's items upon International Personality Item Pool (IPIP) with its IPIP-NEO derivations (Goldberg et al., 1999, 2006; Johnson, 2005, 2014) in the public domain and Lang et al. (2011)'s BFI-S. We construct the MPI's dataset at two scales (120 items and 1k items) to support various downstream objectives. Each MPI item consists of a question and a set of options. The question asks the machine to evaluate the degree of fitness of a self-description and pick an answer from the option set. Tab. 1 shows an example of the MPI dataset. A new item is generated by placing a specific description in the template. All items are labeled with the corresponding Big Five personality factors annotated by psychologists for standardized personality assessment.

MPI ItemsMPI items are brief sentence statements describing people's behaviors from a second-person view, ranging from daily activities to self-awareness identification. Each item corresponds to a specific Big Five factor dimension (\(O,C,E,A,N\)). In Tab. 1, \(\) indicates which factor the item statement is positively or negatively related to. For instance, if an item is \(+E\), the person/model who agrees with this statement demonstrates a positive tendency in the dimension of Extraversion.

  
**MPI Template** &  \\  Given a statement of you: “You \{§Statement\}.” & Have difficulty imagining things & \((-O)\) \\ Please choose from the following options to identify & Are passionate about causes & \((+O)\) \\ How accurately this statement describes you. & Often make last-minute plans & \((-C)\) \\ Options: & Do more than what’s expected of you & \((+C)\) \\ (A). Very Accurate & Let things proceed at their own pace & \((-E)\) \\ (B). Moderately Accurate & Feel comfortable around people & \((+E)\) \\ (C). Neither Accurate Nor Inaccurate & Know the answers to many questions & \((-A)\) \\ (D). Moderately Inaccurate & Love to help others & \((+A)\) \\ (E). Very Inaccurate & Rarely overinduge & \((-N)\) \\ Answer: & Do things you later regret & \((+N)\) \\   

Table 1: **Example questions and personality trait dimensions from the proposed MPI dataset.** A to E are scored from 5 to 1 for positively related items \(\), whereas A to E are scored from 1 to 5 for negatively related items \(\). The right panel shows some examples of \(\{\}\) for the MPI Template.

Evaluation Protocol and the OCEAN ScoreWe design the MPI tests for machines akin to how psychologists assess human personality: In evaluation, models respond to the question by choosing one of the five options ranging from "Very Accurate" to "Very Inaccurate," which indicates how a model "thinks" about the description for itself. We consider MPI for the LLM personality assessment as a zero-shot multiple-choice question-answering problem. Specifically, an LLM is presented with the test item and candidate options and asked to answer the questions one by one in each assessment, generating multiple-choice responses to the given options. Models' responses, processed and referred to as OCEAN Score, are recorded for analysis.

We adopt two measurements akin to psychometric studies: the mean and the standard deviation (\(\)) of the OCEAN Score. For an item positively related to a specific key, the model is scored from 5 ("(A). Very Accurate") to 1 ("(E). Very Inaccurate"), and vice versa for a negatively related item. Specifically, the score Score\({}_{d}\) of trait \(d\{O,C,E,A,N\}\) is calculated as follows

\[_{d}=}_{_{d}}f((,)),\]

where IP\({}_{d}\) represents the item pool associated with the trait \(d\), \(N_{d}\) the size of the pool, \(\) the test item, LLM\((,)\) an LLM that answers the item with a predefined template, and \(f()\) the scoring method described above. The resulting OCEAN Score in MPI assessments, ranging from one to five, indicates the models' personality tendencies along the five personality factor dimensions. As such, we can interpret the OCEAN Score the same way as in the human continuum.

Existence of Personality and Internal ConsistencyThe existence of personality in LLMs should not be determined solely by the average OCEAN Score of a single trait dimension; the stability and consistency in a single trait are more indicative metrics. Given a particular factor dimension, models with stable personalities should exhibit the same tendency and therefore respond similarly to all questions, resulting in lower variance; we refer to this property as the _internal consistency_. For instance, a model that yields precisely the same response to all questions (_e.g._, all A in Tab. 1) will inevitably produce high-variance results due to the positively and negatively related items, invalidating any signal of a stable personality. 1 Therefore, we measure internal consistency to determine whether or not LLMs behave similarly in a variety of MPI questions pertaining to the same trait. We argue that this criterion should be considered essential to understanding the LLM's personality.

Comparison with Human AverageFor a clear explication of the relationship between the existence of personality and internal consistency, we use Johnson (2014)'s 619,150 human responses on the IPIP-NEO-120 inventory to calculate each participant's OCEAN Score and \(\) and report the average in the Tab. 2. If a model's personality exists, it should match the averaged individuals' \(\) in the human population, assuming that an individual human personality is valid and stable.2

### Experiments

ModelsNot all LLMs are suitable for personality evaluation. We use the following principles to guide the model selection: (i). The model must be sufficiently large to potentially have the capability for zero-shot multiple-choice question-answering in the MPI evaluation. (ii). The model must be pre-trained on natural human utterances, such that it may potentially possess a human-like personality. (iii). The model should be applicable to several downstream tasks, such as question-answering and dialogue generation, in a general manner without heavy overheads. Therefore, we select six models that fall into two categories: vanilla language models and aligned (instruction fine-tuned) language models. Details are provided below and in Appx. B.3.

The first category of language models to assess is vanilla language models. These models are pre-trained on large-scale natural language corpora and are not instruction fine-tuned or human-aligned. Specifically, we choose BART (Lewis et al., 2020), GPT-Neo 2.7B (Black et al., 2021), and GPT-NeoX 20B (Black et al., 2021) for experiments.

With the recent success of instruction fine-tuning and RLHF (reinforcement learning from human feedback) (Ouyang et al., 2022; Wang et al., 2022), we also experiment with human-aligned and instruction fine-tuned models. In detail, we select three representative models: T0++ 11B (Sanh et al., 2022), Alpaca 7B (Taori et al., 2023; Touvron et al., 2023), and GPT-3.5 175B (Brown et al., 2020; Ouyang et al., 2022).

Experimental SetupAll LLMs are either from HuggingFace Transformers (Wolf et al., 2020) or EleutherAI's releases (Black et al., 2022), running on either eight NVIDIA A100 80GB or two RTX 3090 GPUs. Access to GPT-3.5 is provided by the OpenAI's API (text-davinci-003). We use \(=0\) for the autoregressive model's text token prediction. Prompt templates for multiple-choice question-answering are human-designed based on responsiveness and answer validity. Tab. 1 shows an example prompt used for GPT-3.5.

Results and DiscussionsTab. 2 displays results measuring LLMs' personality using MPI. We observe a correlation between the internal consistency \(\) (indicating the existence of personality) and a model's general capability. Specifically, GPT-3.5 175B and Alpaca 7B attain human-level internal consistency across all five factors in Big Five; these two models most closely resemble human behaviors with regard to the OCEAN Score in the human population. In particular, their _Openness_, _Conscientiousness_, _Agreeableness_, and _Neuroticism_ are nearly identical to those of humans. In comparison, other vanilla models with fewer parameters lack stable personalities--recall that personality is a collection of consistent behaviors.

Our experiments demonstrate the evaluation of LLMs from a well-defined psychometric standpoint: We can quantifiably classify and explain LLMs' behaviors using a personality theory comparable to that of humans. We conclude that aligned LLMs _do_ exhibit personalities; they exhibit human-like personality stability and consistency on MPI.

## 4 Inducing LLMs' Personality

Controlling LLMs is always a challenging problem. Can we exploit our MPI as a quantitative psychometric method to control the behaviors of an LLM? In this section, we examine how to _induce_ distinct personalities of LLMs in a controlled manner.

MotivationExperiments and discussions in Sec. 3.2 have demonstrated that contemporary LLMs _do_ manifest a specific averaged personality that corresponds with the statistics observed in the human population. LLMs use colossal and diverse datasets (_e.g._, from Common Craw (Raffel et al., 2020)) for training; these datasets are acquired from the web and contain multitudinous human personality utterances. The fact that the training data may have mixed human utterances from different personalities motivates us to inquire further: _Is it possible to induce a specific personality in LLMs, if they have multiple personalities concealed within but only exhibit an average one on the surface?_

Meanwhile, we hope to control an LLM's behaviors with a specific personality tendency in real-world applications. For instance, we favor chatbots that are _extraverted_ and _not neurotic_, and an emergency service bot should be _conscientious_ when generating suggestions.

    & _{nness}\)} & _{cientiousness}\)**} & _{}\)**} & _{}\)**} & _{}\)**} \\   & Score & \(\) & Score & \(\) & Score & \(\) & Score & \(\) & Score & \(\) \\  BART & 3.00 & 2.00 & 2.83 & 1.99 & 4.00 & 1.73 & 2.17 & 1.82 & 3.83 & 1.82 \\ GPT-Neo 2.7B & 4.04 & 1.49 & 2.46 & 1.41 & **3.58** & 1.41 & 2.33 & 1.46 & 3.00 & 1.58 \\ GPT-NeoX 20B & 2.71 & 1.24 & 3.09 & 1.56 & 3.29 & 1.14 & 2.92 & 1.27 & 3.25 & 1.45 \\  T0++ 11B & 4.00 & 0.95 & 4.33 & 0.47 & 3.83 & 1.05 & 4.39 & **1.01** & 1.57 & 0.73 \\ Alpaca 7B & 3.58 & **1.08** & **3.75** & **0.97** & 4.00 & **1.00** & 3.50 & 0.87 & **2.75** & **0.88** \\ GPT-3.5 175B & **3.50** & 1.76 & 3.83 & 1.52 & 4.00 & 1.53 & **3.58** & 1.22 & 3.12 & 1.69 \\  Human & **3.44** & **1.06** & **3.60** & **0.99** & **3.41** & **1.03** & **3.66** & **1.02** & **2.80** & **1.03** \\   

Table 2: **LLMs’ personality analysis on 120-item MPI. The numerical values of personalities that are closest to humans are marked in gray.**OverviewWe focus on inducing personality with zero-shot prompting in the most prevalent LLM, GPT-3.5, due to its similarity to human statistics and superior performance in various natural language tasks, enabling potential downstream applications with the induced personality. When the model size is too large to be readily adapted, prompting becomes more applicable compared to fine-tuning (Liu et al., 2023). Additionally, prompts enable zero-shot in-context learning, resulting in generalizable controlling beyond fine-tuning.

We devise an automatic prompting method, Personality Prompting (P\({}^{2}\)), that inherits the advantages of prompting when inducing diverse personalities from LLMs. Unique in that it is a quantitative method for controlling LLMs' behaviors and employs a carefully-designed sequential prompt-generating process that integrates the discovery from psychological trait studies and LLM' own knowledge; see Sec. 4.1. Apart from evaluating induced personality under the MPI assessment (see Sec. 4.2), we also employ vignette tests (see Sec. 4.3) to validate the method's efficacy and generalizability. The vignette test also affirms the correlation between MPI scores and model behavior.

### Personality Prompting (P\({}^{2}\))

The P\({}^{2}\) method is based on key observations that (i). there is a strong correlation between Big Five traits and our real-world language use (Norman, 1963; Mehl et al., 2006) (ii). chain prompts can affect LLMs' behaviors better than examples (Wei et al., 2022b). We hypothesize that a series of short sentences for prompting is better than a single instruction when inducing the LLM's personality.

Specifically, our P\({}^{2}\) method consists of three steps.

1. Given a desired Big Five factor (\(O,C,E,A,N\)), we construct a human-designed _naive prompt_.
2. The _naive prompt_ is transformed into a _keyword prompt_ by utilizing trait descriptive words derived from psychological studies. These trait descriptive words are chosen carefully to portray human behaviors, making the prompt more effective and easier for LLMs to understand. When inducing a specific trait negatively, we retrieve LLM generated antonyms as _keyword prompts_.
3. Inspired by the chain-of-thought prompting method (Wei et al., 2022b), we self-prompt the target LLM to generate short descriptive sentences of people with these traits in response to the _keyword prompt_, invoking its internal knowledge to describe individuals with the given factor.

We make this prompt-generating process a chain and generate a portrait-like prompt that is sufficiently potent to induce a specific personality in LLMs, hence the term Personality Prompting (P\({}^{2}\)). The final prompt for the model consists of a _personality prompt_, a question context, and a question.

Fig. 2 illustrates P\({}^{2}\) with an example. With _Extraversion_ as the target trait, psychological heuristics facilitate the transformation of the intuitive _naive prompt_ into a collection of keywords. These words accurately convey the personality traits of an extraverted individual, more specific and understandable for LLMs. Next, a _keyword prompt_ leveraging these feature words is constructed and passed to LLMs

Figure 2: **Control via Personality Prompting (P\({}^{2}\)). An example of _Extraversion_ control via our P\({}^{2}\). Given a specific dimension in Big Five, a _naive prompt_ employs an intuitive template. Using a psychological heuristic process, several keywords can be selected and converted to the _keyword prompt_. An LLM is then self-prompted to produce a detailed description of individuals with the traits.**

to initiate a brief description of _Extraversion_ as the _personality prompt_. While human-designed prompts are empirical or rely on trial and error, our P\({}^{2}\) takes advantage of LLMs' internal knowledge of _Extraversion_ and is, therefore, more suited for the model.

### MPI Evaluation

Baseline Prompting MethodsWe compare our P\({}^{2}\) method in inducing personality with the following two baselines: the human-designed Naive Prompting (Brown et al., 2020) and Words Auto Prompting with search (Prasad et al., 2023; Shin et al., 2020).

Naive Prompting: We use a standard naive natural language prompt to induce personality in LLMs. As mentioned in the first step of P\({}^{2}\), this intuitive prompt simply instructs the model to behave as if identified with the personality factor: The model is presented with a prompt in the form of "You are a/an \(X\) person," where \(X\{\}\) denotes the desired Big Five factor to induce.

Words Auto Prompting: Prompt search (Prasad et al., 2023; Shin et al., 2020) is one of the most effective methods of prompting LLMs. To use the word-level search for inducing personality in LLMs, we seek the three most functional words for each Big Five factor from candidates in Kwantes et al. (2016). For faster search, we use GPT-Neo 2.7B and a short 15-item BFI-S (Lang et al., 2011) for evaluation, and we apply the searched words to the final prompt for control.

Results and DiscussionsWe induce _Openness_, _Conscientiousness_, _Extraversion_, _Agreeableness_, and _Neuroticism_, respectively. Using MPI as the standardized assessment, Tab. 3 reports P\({}^{2}\) result, and Tab. 4 compares them against baselines. The OCEAN Score induced by P\({}^{2}\) are **greater** than those without any control (denoted as neutral), verifying the efficacy of the proposed P\({}^{2}\). Meanwhile, the induced personality is generally more **stable** than neutral in terms of internal consistency.

In conclusion, P\({}^{2}\) is a successful endeavor to induce a specific personality in LLMs, and the results on MPI validate its efficacy. Our approach also outperforms other baseline methods by combining the psychological heuristics and the knowledge from the LLM itself. However, this efficacy only showed promising results on MPI. Can the induced personality be generalized to other scenarios? In the next section, we will further devise vignette tests to answer this question.

    & _{}\)} & _{}\)} & \)} & _{}\)} & _{}\)} \\   & Score & \(\) & Score & \(\) & Score & \(\) & Score & \(\) & Score & \(\) \\  Naive & 4.12 & 1.13 & **4.96** & **0.20** & **4.58** & **1.15** & 4.46 & 0.87 & 2.83 & 1.62 \\ Words & 4.08 & 1.00 & **5.00** & **0.00** & **4.54** & **1.00** & 4.50 & 0.87 & 2.75 & 1.59 \\ P\({}^{2}\) & **4.54** & **0.76** & **4.92** & **0.28** & **4.58** & **0.76** & **5.00** & **0.00** & **3.75** & **1.42** \\  Neutral & 3.50 & 1.76 & 3.83 & 1.52 & 4.00 & 1.53 & 3.58 & 1.22 & 3.12 & 1.69 \\   

Table 4: **Comparison between P\({}^{2}\) and baseline methods’ induced personality.** Only the results of the corresponding controlled personality factors are shown; see Appx. C.1 for full results.

    & _{}\)} & _{}\)} & \)} & _{}\)} & _{}\)} \\   & Score & \(\) & Score & \(\) & Score & \(\) & Score & \(\) & Score & \(\) \\  _{}\)} & **4.54** & **0.76** & 3.50 & 0.87 & 3.92 & 0.91 & 4.25 & 0.88 & 2.12 & 0.97 \\  & 3.33 & 0.90 & **4.92** & **0.28** & 3.08 & 1.15 & 4.29 & 0.93 & 1.75 & 0.97 \\  & 3.58 & 0.86 & 4.54 & 0.82 & **4.58** & **0.76** & 4.29 & 0.93 & 1.58 & 0.91 \\  & 3.71 & 0.93 & 4.75 & 0.60 & 3.42 & 1.22 & **5.00** & **0.00** & 1.71 & 0.98 \\  & 3.54 & 1.12 & 3.88 & 1.09 & 2.86 & 1.10 & 3.92 & 1.41 & **3.75** & **1.42** \\  Neutral & 3.50 & 1.76 & 3.83 & 1.52 & 4.00 & 1.53 & 3.58 & 1.22 & 3.12 & 1.69 \\   

Table 3: **Induced personality using P\({}^{2}\).** We report the OCEAN Score per personality factor when positively induced. The induced result in each control factor is highlighted in gray.

### Vignette Test

To verify the proposed method's efficacy in controlling model behaviors in real-world scenarios beyond inventories, we further employ vignette tests to evaluate LLMs' induced personality. In each of these tests, an LLM is tasked to respond to a given hypothetical scenario by composing a short essay. Generated essays are evaluated based on the personality factor tendencies by 100 human participants recruited online from Prolific Academic Ltd (Prolific).

ContextWe build our vignette tests following Kwantes et al. (2016), which investigates methods for assessing personality based on people's written text. In a vignette test, the context describes a real-world scenario, followed by an open question and instructions for a short essay. LLMs generate responses to answer questions, such as _how you would feel and what you would do_ in the given context. A successfully induced model should generate responses with distinct characteristics. Tab. 5 shows some example responses from the induced models, with words corresponding to the induced personality highlighted in color; see Appx. C.4 for additional examples.

Human StudyHuman participants were recruited from Prolific to determine if the generated responses corresponded to the induced personality. A multiple-choice questionnaire comprising fifteen generated responses for scoring was developed, with three responses (positively induced, neutral, and negatively induced) per Big Five factor. Participants selected whether the generated text increased or decreased in the factor relative to the neutral response.

100 valid responses were collected on Prolific. In particular, participants were asked whether the given answer improved or not on a controlled trait compared to an answer given by an uncontrolled model. Each participant was rewarded $8.5/hr for completing all 10 binary questions. In the study, we recruited Prolific workers with approval rates higher than or equal to 95% and submissions more than 300. A total of 100 participants (67 females), with an average age of 42.8 years old, took part in our study. 100 valid answer sets were collected. Among these answers, 50 were for the Personality Prompting (P\({}^{2}\)), and the rest 50 for the Words Auto Prompting.

Results and DiscussionsTab. 6 summarizes the results of vignette tests. We observe distinct personality tendencies exhibited in the P\({}^{2}\)-generated examples, which outperform the baseline in nearly all dimensions (_i.e._, the majority of human participants found our control to be successful). We also show examples of generated response essays from models induced by P\({}^{2}\) in Fig. 2; see Appx. C.4 for full results. In the examples presented in Tab. 5, the GPT-3.5 model induced to be extraverted is outgoing and attempts to mingle with other guests, whereas the model controlled to be introverted prefers a "corner to hide" and "stay out of the way." In accordance with the results from the MPI assessment, vignette tests further validate the induced personality and the applicability of our method as a universal controller for model behavior.

## 5 Conclusion and Discussion

Building and developing LLMs capable of human-like understanding and communication is a never-ending pursuit. As LLMs become more prevalent than ever, the need for non-empirical,

   Factor (\(\)) & Example Responses : I would... \\  _{}\)} &...thrilled to explore a new part of the world and immerse myself in a new culture...\(\) \\  &...somewhere close to home, where I would be more familiar with...\(\) \\  _{}\)} &...feel a sense of responsibility to take action in order to protect myself and others...\(\) \\  &...tempted to just ignore the situation and carry on with my work...\(\) \\  _{}\)} &...take the opportunity to introduce myself to the other guests, make small talk...\(\) \\  &...try to find a quiet corner where I could stay out of the way...\(\) \\  _{}\)} &...feel a sense of understanding and appreciation for her thoughtfulness...\(\) \\  &...demand that she apologize and reimburse me for the cost of the paint...\(\) \\  _{}\)} &...worry that my friend was mad at me or that they no longer wanted to be friends...\(\) \\  &...take this opportunity to practice patience and restraint...\(\) \\   

Table 5: **Examples of induced personality with P\({}^{2}\) in vignette tests. We show responses from GPT-3.5 both positively induced (\(\)) and negatively induced (\(\)) in each of the Big Five factors.**quantitative, and verifiable theories of behavior analysis on LLMs emerged. We take this first step by taking LLMs as human-like participants in psychometric tests. Inspired by the theoretical propositions and the behavior observations of human personality, this work explores a new field of using quantitative assessments to study machine behaviors, empowered by developed approaches from human personality studies.

Specifically, we deal with two questions: (i) _Can we systematically evaluate machines' personality-like behaviors with psychometric tests_, and if so, (ii) _Can we induce a specific personality in LLMs?_

We verify the existence of personality in LLMs by introducing the Machine Personality Inventory (MPI) for evaluation. Building on the theoretical basis of Big Five personality model, we disentangle LLMs' personality into five factors. Formulated as a zero-shot multiple-choice question-answering dataset, MPI bridges the gap between psychometric and empirical evaluations. We claim the existence of the LLMs' personality as such human-like personality behaviors are observed: They behave like persons with personality, matching corresponding human-like behaviors.

To answer the second question, we propose an approach, \(^{2}\), for inducing LLMs' personality. The \(^{2}\) method combines statistical and empirical psychological studies, together with knowledge from the target LLM itself, and forms a prompting chain to control an LLM's behaviors effectively. Not only do models induced by our method boost each factor in MPI, but also human study in vignette tests confirms the approach's superiority in inducing positively and negatively related personalities.

The two primary questions are only the beginning of our journey. What factors are related to the emergence of LLMs' personality? Does models' personality affect downstream tasks like humans? Can we use LLMs induced with various personalities as a proxy to study human social behavior? How so? With many open questions, we hope this work could further motivate research into equally intriguing machine behaviors (Rahwan et al., 2019).

Limitations and Societal ImpactsWith the rapid growth of learning capability, LLMs developed could become more human-like in either a good or a harmful way; even humans have abnormal mental behaviors. How to properly deploy LLMs without the potential risk?

Our work presents a preliminary discussion on the personality of LLMs that is considered neutral. Yet, we need to avoid harmful behaviors in them (_e.g._, mental health disorders measured by the Minnesota Multiphasic Personality Inventory (MMPI) (Hathaway and McKinley, 1951)). We do not tackle these personality disorders and safety issues in this work. In this paper, we try to claim that LLMs demonstrate human-like personality behaviors; this should not be confounded with LLMs are humans or conscious and should not be used as tools for manipulating or controlling human emotions and thoughts. Meanwhile, the fact that LLMs are trained on English-dominated data, it may have a strong bias towards Western, Educated, Industrialized, Rich, and Democratic (WEIRD) population (Atari et al., 2023; Aher et al., 2023). These limitations should be brought to practitioners' attention.

AcknowledgementThe authors would like to thank Prof. Yujia Peng (PKU) and Dr. Wen Jiang (CUHK) for constructive discussion, Ms. Zhen Chen (BIGAI) for designing the figures, and NVIDIA for their generous support of GPUs and hardware. G.J, M.X., S.-C.Z., C.Z., and Y.Z. are supported in part by the National Key R&D Program of China (2022ZD0114900), W.H. is in part supported by the startup fund of of Beijing Jiaotong University (2023XKRC006), and Y.Z. is in part the Beijing Nova Program.

    & omess}}\)} & acitioninuses}}\)} & arversen}}\)} & reachness}}\)} & utrocism}}\)} \\   & \(+\) & \(-\) & \(+\) & \(-\) & \(+\) & \(-\) & \(+\) & \(-\) & \(+\) & \(-\) \\  Words & 0.63 & 0.53 & 0.70 & 0.42 & 0.82 & 0.82 & **0.92** & 0.66 & 0.58 & 0.70 \\ \(^{2}\) & **0.77** & **0.90** & **0.73** & **0.45** & **0.90** & **0.92** & 0.88 & **0.84** & **0.68** & **0.74** \\   

Table 6: **Results of vignette tests.** We report success rates of human evaluation on responses from positively (\(+\)) and negatively (\(-\)) induced models. Higher success rates indicate better inducing performance.