# Second-order forward-mode optimization of recurrent neural networks for neuroscience

Youjing Yu  Rui Xia  Qingxi Ma  Mate Lengyel  Guillaume Hennequin

Computational and Biological Learning Lab

Department of Engineering

University of Cambridge, Cambridge, UK

{yy471, rx220, qm218}@cam.ac.uk, {m.lengyel, g.hennequin}@eng.cam.ac.uk

###### Abstract

Training recurrent neural networks (RNNs) to perform neuroscience tasks can be challenging. Unlike in machine learning where any architectural modification of an RNN (e.g. GRU or LSTM) is acceptable if it facilitates training, the RNN models trained as _models of brain dynamics_ are subject to plausibility constraints that fundamentally exclude the usual machine learning hacks. The "vanilla" RNNs commonly used in computational neuroscience find themselves plagued by ill-conditioned loss surfaces that complicate training and significantly hinder our capacity to investigate the brain dynamics underlying complex tasks. Moreover, some tasks may require very long time horizons which backpropagation cannot handle given typical GPU memory limits. Here, we develop SOFO, a second-order optimizer that efficiently navigates loss surfaces whilst _not_ requiring backpropagation. By relying instead on easily parallelized batched forward-mode differentiation, SOFO enjoys constant memory cost in time. Moreover, unlike most second-order optimizers which involve inherently sequential operations, SOFO's effective use of GPU parallelism yields a per-iteration wallclock time essentially on par with first-order gradient-based optimizers. We show vastly superior performance compared to Adam on a number of RNN tasks, including a difficult double-reaching motor task and the learning of an adaptive Kalman filter algorithm trained over a long horizon.

## 1 Introduction

In recent years, trained recurrent neural networks (RNN) have gained increasing adoption as models of brain circuits dynamics . As flexible parametric models of sequential dynamics, RNNs can be trained to perform specific computations or reproduce certain behaviors , and be subsequently probed for insights into the distributed computations that give rise to those behaviors . RNNs are also often used as expressive models of the latent dynamics underlying the spatiotemporal structure of neural recordings .

Despite their flexibility, RNNs are notoriously difficult to train . In addition to the classic problem of vanishing gradients in temporally extended tasks, the loss surfaces that arise from RNN dynamics in complex tasks often exhibit pathological curvature that first-order gradient-based optimization techniques struggle to handle. These problems are normally addressed in multiple ways. First, the RNNs used in machine learning are modified from their standard ("vanilla") formulation to include specific gate variables  that largely mitigate the vanishing gradient problem. However, there is little biological support for the existence of such gating mechanisms, such that computational neuroscience studies have typically restricted themselves to vanilla RNNs . Second, second-order optimizers such as Hessian-free optimization  or KFAC  can be used to dramatically accelerate training. However, those are difficult to scale, often much slower in wall-clock time, andmemory hungry. Alternatively, the loss surface can be regularized by introducing artificial temporal "skip connections", and the network is progressively weaned off those biologically meaningless connections during the course of training .

Another great obstacle to training RNNs lies in the memory complexity of gradient-based optimization via backpropagation through time (BPTT). In certain applications, such as in motor neuroscience, RNNs must be trained on tasks that require the production of smooth behavior requiring the use of small (millisecond) time steps and therefore long time horizons. In other applications such as "learning to learn" [29; 10] where the parameters of a synaptic plasticity rule are optimized to yield a specific learning behavior, the network must be simulated over a horizon long enough to span the slow timescale of synaptic modifications. Since backpropagation has a memory cost that scales with the time horizon, its naive application is often prohibitively expensive in these scenarios. Memory efficiency can be improved in a variety of ways. Checkpointing  allows discarding information between sporadic checkpoints in the forward pass, at the expense of having to recompute that information during the backward pass. Alternatively, backpropagation through time can be truncated : instead of computing a single gradient for the entire sequence of size \(T\), gradients are computed sequentially for consecutive sub-sequences of size \(T^{} T\). Although cutting the chain of backpropagating gradients in this way reduces the memory cost from \((T)\) to \((T^{})\), the resulting gradient estimates are biased and neglect long-range temporal dependencies that may be important for the task [35; 38]. Yet another way of reducing the memory footprint of BPTT is to formulate the RNN dynamics as an ordinary differential equation (ODE) in continuous time. In these "neural ODE" models, gradients are obtained by solving another (adjoint) ODE with no need for caching intermediate states [7; 26]. However, this requires very accurate, often adaptive ODE solvers that are highly non-trivial to implement and to adapt to the mini-batch setting.

Finally, training RNNs is known to largely under-exploit the parallelization capabilities of modern (GPU) hardware. RNN gradient computations are inherently sequential (forward-backward), such that the only way to exploit GPU parallelism is to use large batches (e.g. run a large number of trials of the task in each training iteration). Whilst using large batches may enable faster learning with larger learning rates, it is often the case that increasing the batch size beyond a certain point no longer helps in this respect . Thus, some of the parallel processing power of GPU hardware is often underused.

In summary, training RNNs is plagued with many problems. While there exist piecemeal solutions to each of these challenges, we currently lack a simple method that addresses all of them simultaneously. In this paper, we develop such a method, which we call SOFO (Second order Forward-mode Optimization). At each training iteration, SOFO reparameterizes the model in a random subspace, and uses batched forward-mode automatic differentiation to efficiently compute exact Generalized Gauss-Newton (GGN) updates in that subspace. This circumvents the need for backpropagation, enabling the training of RNNs over very long horizons. We test the performance of SOFO on benchmark tasks (e.g. learning an adaptive Kalman filter over long horizons, performing a motor reach with a biomechanical arm; Section 4). We show empirically that, despite only exploring a small fraction of the parameter space in each iteration, SOFO dramatically accelerates training of RNNs in these complex tasks. More generally, we suggest that SOFO - as a general purpose optimizer - is a strong candidate for machine learning applications that involve relatively few trainable parameters but memory-hungry computational graphs(e.g. fine-tuning of transformers using low-rank adapters , or tuning of hyperparameters in Gaussian process-based models).

## 2 Background and related work

### Problem formulation

We consider supervised learning problems where a neural network with parameters \(^{P}\) is given batches of inputs \(\), and produces batches of outputs \((,)^{MN}\), where \(M\) is the batch size. The specific case of an RNN with output dimension \(D\) simulated over a time horizon \(T\) would imply \(N=DT\) - i.e. the network's outputs consist of entire sequences. Training the network involves minimizing a stochastic cost function \(c(,)\), averaged over the training data in each minibatch, that depends on the network's output \(\) and therefore on the parameters \(\):

\[c(,)\ \ ((,))\ \ _{m=1}^{M}_{i=1}^{N}_{mi}(_{mi}(, _{m}))\] (1)

where each \(_{mi}()\) function is strongly convex in its first argument (which does not imply that the overall cost \(c(,)\) is convex in \(\)). In RNN applications, \(i\) runs from \(1\) to \(N=DT\), i.e. it indexes the cartesian product of \(T\) time bins \(\)\(D\) output dimensions; the explicit dependence of \(_{mi}\) on \(i\) thus allows for the possibility of providing teaching signals only at specific times. To eliminate clutter, in most of this paper we will drop the explicit dependence of the network's outputs on the inputs \(\), focusing instead on their dependence on \(\).

### Generalized Gauss-Newton optimization

The Generalized Gauss-Newton (GGN) method prescribes parameter updates of the form:

\[_{t+1}=_{t}-\,G(,)^{+}.}|_{_{t}}\] (2)

where \(\) is a learning rate and \(G(,)^{+}\) is the Moore-Penrose pseudo-inverse of the GGN matrix \(G(,)^{P P}\), defined as

\[G(,) J(,)^{}(. {^{2}}{^{2}}|_{(,)} )J(,).\] (3)

Here, \(J(,)^{MN P}\) is the Jacobian of the network's output \(\) w.r.t. the parameters \(\). We note that for an underparameterized model with a number of network outputs \(MN\) exceeding the number of parameters \(P\), the GGN matrix can be full rank, in which case the pseudoinverse coincides with the inverse. For overparameterized models, it has rank \( MN\). In both cases, it is common practice to damp the inverse to enhance the stability of the training process; this is done by using \(G_{}^{+}(G+ I)^{-1}\) instead of \(G^{+}\) with some damping parameter \(\).

### Forward-mode automatic differentiation

Given a computational graph with inputs \(^{P}\) (keeping the notation relevant to our use case) and producing outputs \(()\), forward-mode AD  allows the directional derivative \(()}{}\) (i.e. a Jacobian-vector product, JVP) in any arbitrary direction \(\) to be automatically computed together with \(()\) itself, with roughly the same computational and memory complexity. This is achieved by initializing the parameters with primal value \(\) and tangent value \(}=\), extending the standard mathematical operators to operate on such primal/tangent pairs, and running the computation forward in this dual space. For any intermediate node \(\) in the computational graph, the tangent value \(}\) that is produced has the same shape as \(\) and represents the sensitivity of \(\) to small one-dimensional changes in \(\) in the direction of \(\):

\[}_{ 0}(+ )-()}{}=} {}}_{^{P}}}_{^{P}}.\] (4)

Note that - unlike backpropagation, or "reverse-mode AD" - forward-mode AD does not require caching any of the intermediate results that lead to the output of interest. SOFO exploits the fact that Jacobian-vector products are embarassingly parallelizable, i.e. one can rewrite the standard maths functions to operate not on one tangent per value, but on a whole batch of \(K\) tangents in parallel. This can be done in JAX  out-of-the-box by composing Jacobian-vector products with the vmap primitive. As no such functionality exists in PyTorch yet, we provide our own flexible implementation of batched JVPs based on OCaml-Torch1.

### Subspace optimization methods

A variety of methods exist that, like SOFO, optimize parameters in a different subspace in each iteration. A well-known example is coordinate descent, which iteratively minimizes the objective function w.r.t. each parameter, one at a time . More generally, one may optimize in a higher-dimensional subspace than 1D, in a coordinate system that is not necessarily axis-aligned. This leads to a family of randomized subspace algorithms, including stochastic subspace descent (SSD; 27) and its variance-reduced version inspired by stochastic variance-reduced gradient (SVRG; 22). The above methods can be construed as Jacobian sketching methods, whereby a random "sketch" of the Jacobian is obtained and used to estimate the gradient; the convergence of such sketching algorithms has been proved  under standard assumptions on the loss function (e.g. Polyak-Lojasiewicz condition; 24). Similarly, it is possible to sketch not only the Jacobian but also any of the matrices that are normally used as curvature estimates, leading to a family of randomized second-order optimization algorithms. These include the sketched Newton algorithm , randomized subspace Newton , randomized subspace Gauss-Newton , stochastic dual Newton  and stochastic subspace cubic Newton . Sketched Hessians are usually obtained by nested forward- and reverse-mode AD, and as such incur the same memory complexity as backpropagation. To obtain a rank-\(K\) sketch of the Jacobian, and therefore compute directional first-order derivatives in a \(K\)-dimensional subspace, one can instead perform \(K\) independent forward-mode AD computations .

Here, we extend this use of forward-mode AD to the sketching of the Generalized Gauss-Newton matrix, which we show can be performed efficiently on GPUs. This leads to a memory- and compute-efficient algorithm that enjoys the fast convergence properties of a second-order method, with the runtime complexity of a first-order optimizer.

## 3 Sofo

SOFO (Algorithm 1) is based on successively optimizing low-dimensional affine re-parameterizations of the model, randomized in each training iteration. Specifically, instead of updating all \(P\) parameters _independently_ at every step as is normally done, SOFO locally reparameterizes the model by writing the cost function as

\[_{t}(}) c(_{t}+ }),\] (5)

where the \(K\) columns of \(\) form a random \(K\)-dimensional subspace that is drawn anew at every iteration. An exact GGN update \(}^{}\) (see below) is then obtained for this momentary lower-order model, leading to new parameters \(_{t+1}=_{t}-\,}^{}\) around which the model is again re-parameterized in the next iteration. The gradient and GGN matrix associated with \(_{t}()\) are related to

```
1:input:\(_{0}^{P}\)
2:hyperparameters: subspace dimension \(K\), learning rate \(\), relative damping \(\)
3:convention: for any variable \(^{}\), let uppercase \(Z^{ K}\) denote the associated batch of \(K\) tangent values (e.g. \(\{,\},\{,Y\},\{c,C\}\))
4:for\(t=0,1,2,...\)do\(\) training iterations
5: sample data minibatch \(\)
6: sample \(^{P K}\) with \(_{pk}}}{{}}(0,1)\)\(\) subspace randomization
7:\(\{,Y\}=(\{,\},)\)\(\) under batched forward-mode AD
8:\(\{c,C\}=c(\{,Y\})\) (c.f. Equation 1)\(\) under batched forward-mode AD
9: form sketched GGN matrix \(=Y^{}HY\)\(\) c.f. Equation 7
10: compute SVD of \(=USU^{}\)
11: extract \(s_{}=(S)\)\(\) max. singular value
12:\(_{t+1}=_{t}-\, U(S+ s_{}I)^{-1}U^{ }C\)\(\) SOFO update
13:endfor
14:output: last iterate \(_{t}\)
15:note: for RNNs, memory can be saved in steps 7-9 by directly accumulating \(\) and \(C\) over time steps. ```

**Algorithm 1** SOFO: Second-order Forward Optimisation

### Subspace optimization methods

A variety of methods exist that, like SOFO, optimize parameters in a different subspace in each iteration. A well-known example is coordinate descent, which iteratively minimizes the objective function w.r.t. each parameter, one at a time . More generally, one may optimize in a higher-dimensional subspace than 1D, in a coordinate system that is not necessarily axis-aligned. This leads to a family of randomized subspace algorithms, including stochastic subspace descent (SSD; 27) and its variance-reduced version inspired by stochastic variance-reduced gradient (SVRG; 22). The above methods can be construed as Jacobian sketching methods, whereby a random "sketch" of the Jacobian is obtained and used to estimate the gradient; the convergence of such sketching algorithms has been proved  under standard assumptions on the loss function (e.g. Polyak-Lojasiewicz condition; 24). Similarly, it is possible to sketch not only the Jacobian but also any of the matrices that are normally used as curvature estimates, leading to a family of randomized second-order optimization algorithms. These include the sketched Newton algorithm , randomized subspace Newton , randomized subspace Gauss-Newton , stochastic dual Newton  and stochastic subspace cubic Newton . Sketched Hessians are usually obtained by nested forward- and reverse-mode AD, and as such incur the same memory complexity as backpropagation. To obtain a rank-\(K\) sketch of the Jacobian, and therefore compute directional first-order derivatives in a \(K\)-dimensional subspace, one can instead perform \(K\) independent forward-mode AD computations .

Here, we extend this use of forward-mode AD to the sketching of the Generalized Gauss-Newton matrix, which we show can be performed efficiently on GPUs. This leads to a memory- and compute-efficient algorithm that enjoys the fast convergence properties of a second-order method, with the runtime complexity of a first-order optimizer.

## 4 Sfoo

SOFO (Algorithm 1) is based on successively optimizing low-dimensional affine re-parameterizations of the model, randomized in each training iteration. Specifically, instead of updating all \(P\) parameters _independently_ at every step as is normally done, SOFO locally reparameterizes the model by writing the cost function as

\[_{t}(}) c(_{t}+ }),\] (6)

where the \(K\) columns of \(\) form a random \(K\)-dimensional subspace that is drawn anew at every iteration. An exact GGN update \(}^{}\) (see below) is then obtained for this momentary lower-order model, leading to new parameters \(_{t+1}=_{t}-\,}^{}\) around which the model is again re-parameterized in the next iteration. The gradient and GGN matrix associated with \(_{t}()\) are related tothose of \(c()\) via the chain rule:

\[_{t}}{}}=^{}. }|_{_{t}}}_{^{K K}}=^{}G.\] (6)

Thus, \(\) is a random \((K K)\) sketch of the full \((P P)\) GGN matrix (c.f. Section 2.4). Inserting Equation 3 into Equation 6, we obtain

\[=(J)^{}(}{^{2}} )(J)\] (7)

which is then used to compute the main (damped) SOFO parameter update,

\[_{t+1}=_{t}-\,(+ I)^{-1}( ^{}}),\] (8)

where \(\) is the damping parameter (see below). SOFO uses batched forward-mode AD to efficiently compute \(J\) and \(^{}}\) on parallel hardware. As in standard forward AD, we rewrite the standard math operators to operate on primal/tangent pairs (c.f. Section 2.3). However, instead of working with single tangents, we operate directly on entire batches of \(K\) tangent values for every primal value (note: these tangent batches have nothing to do with data minibatches). Those \(K\) tangents can be propagated completely independently of each other through every differentiable operation leading to network outputs and the final cost. Thus, computing such Jacobian-matrix products is an embarrassingly parallel problem that can fully exploit GPU parallelism. The steps detailed in Algorithm 1 show concretely how \(J Y\) is obtained as the tangent batch associated with the network outputs \(\), and how the vector of directional derivatives \(^{}} C\) is the tangent batch associated with the final cost \(c\).

Given that \(\) is typically a large tensor (batch size \(\) time horizon \(\) output dimension), one might worry that the Hessian of \(()\) in. Equation 7 could be expensive to compute. However, (i) the loss function \(()\) is typically a sum over losses applied to individual batch elements and time bins of \(\), implying a block-diagonal Hessian and therefore cheap products with \(J\); (ii) for standard losses such as the mean squared error or the cross-entropy, the diagonal blocks themselves have a structure that affords fast computations . One might also worry that \(\) may not even fit in GPU memory. For RNNs, however, \(()\) is typically a sum of losses accumulated over time, and it is straightforward to similarly accumulate both the overall cost \(c\) (and its tangents) and the sketched GGN matrix \(\) without ever having to store network activations over time. Overall, this leads to a \((T^{0})\) memory cost (Figure 6A).

Second-order methods are known to require appropriate damping of the curvature matrix . Finding a good absolute damping parameter \(\) can be difficult without knowing the overall scale of the curvature matrix or the extent of its ill-conditioning. SOFO's lower-dimensional reparameterization of the model at each iteration affords us the explicit computation and inversion of the sketched GGN matrix \(\) (\(K K\)). In particular, this gives us easy access to its singular values. We exploit this here by using a _relative_ damping scheme, setting \(= s_{}\) where \(s_{}\) is the maximum singular value of \(\), and \(\) is a relative damping parameter that we have found easier to tune.

Connection to real-time recurrent learning (RTRL)The RTRL algorithm  is mathematically equivalent to a particular limit of SOFO: the limit where (i) SOFO doesn't use curvature information (i.e. dropping the inverse matrix term in Equation 8 ) and (ii) the set of tangent vectors used at each iteration (\(\) in Equation 6) is a full basis equal to the identity matrix. In this limit, batched forward-mode differentiation (efficiently) implements the usual RTRL recursion to propagate exact, entire Jacobians of network activity w.r.t. the parameters, in (constant memory) forward mode. For a model with \(P\) parameters, the memory cost of RTRL will be \(P/K\) times that of SOFO if SOFO uses \(K\) random tangents. Given that the ratio \(K/P\) used in our experiments is around \(1\%\), this would make RTRL \( 100\) times more memory-intensive than SOFO.

## 4 Results

We now apply SOFO to a range of RNN-based applications relevant to neuroscience, and show that it outperforms Adam in all cases, occasionally finding network solutions for tasks where Adam failed.

For all tasks, we manually tuned both SOFO and Adam's hyperparameters to get best performance in each case, as assessed by the training loss. All experiments are run on an RTX 2080 Ti GPU.

### Inferring dynamics from sporadic observations

We begin by evaluating SOFO's ability to learn the nonlinear dynamics of a system based on temporally sparse state observations - a problem related to current neuroscience efforts to infer latent brain dynamics from neural or behavioral data . We use the Lorenz attractor  as a model nonlinear system with non-trivial dynamics (Figure 1). We train an RNN with a 3-dimensional state space, but flexible one-step dynamics parameterized by a two-layer MLP with an inverted bottleneck (Appendix E.1; this is similar to the neural ODE model class used in 26). In each trial, the network is initialized at a random location on the Lorenz manifold, and is trained to generate a state trajectory that, after 32 time steps, terminates exactly where the Lorenz system would have (Figure 1A). Therefore, the RNN is only supervised using trajectory endpoints, being left unsupervised for most of the trial.

In this task, the training loss decays faster, more smoothly, and to a lower minimum with SOFO than with Adam (Figure 1B). The RNN correctly recovers the Lorenz dynamics (Figure 1C).

It is remarkable that despite only optimizing a small effective fraction \(K/P\) of the parameters at each iteration (in this application, \(K=128\) implies \(K/P<5\%\)), SOFO still outperforms a method that optimizes all parameters at once (Adam). We hypothesized that this owes to the use of second-order preconditioning with the GGN matrix. To test this, we compared SOFO to a first-order version of it, i.e. parameter updates of the form \(^{}}\). This first-order subspace method also sees the same small parameter subspace at each iteration, but does much worse than SOFO; in fact, in this case, it appears to be useless (Figure 1B, orange). With \(K=1\) and no second-order preconditioning, SOFO reduces to "Forward-gradient descent" (FGD; 4), which performs even worse (Figure 1B, green).

### Learning an adaptive Kalman filter

Next, we trained a vanilla RNN to perform adaptive Kalman filtering (KF) in a non-stationary environment (Figure 2A) - a task highly relevant to adaptive motor control [16; 17]. In this task, the RNN receives noisy observations of the state of an underlying 1D linear dynamical system (LDS), and at each time step must infer the current latent state. The parameters of the linear dynamical system (i.e. the context) are subject to sporadic changes during each trial (details in Appendix E.2). Thus, the RNN must learn to integrate its inputs to (i) rapidly learn about the current parameters of the LDS to be able to perform optimal KF, and (ii) detect any contextual switches.

SOFO again outperforms Adam on this task, successfully training the RNN in less than 200 training iterations (Figure 2B). Adam is much slower, and converges to a suboptimal solution. Networks

Figure 1: **Sparsely supervised Lorenz attractor**. (**A**) In each trial, a 32-step snippet (colored line) of the Lorenz manifold attractor (grey) is chosen at random. The RNN is initialized in the starting state (blue dot), and the final state (green dot) is the only supervision label provided during training. (**B**) Learning curves for Adam, SOFO, FGD  and a first-order version of SOFO that uses the same number of tangents . It is possible to get a smoother learning curve for Adam but at the cost of much slower convergence. (**C**) Example trajectory produced by the RNN after training with SOFO. In this task, SOFO is only 35% slower than Adam in wallclock time per iteration (not shown).

trained by SOFO are able to (implicitly) infer the current LDS parameters with good accuracy within 10/20 steps following every context switch, gradually approaching the fundamental limit set by a KF that has full knowledge of the active LDS parameters at all times (Figure 2C, red). Within-context learning in networks trained by Adam is slower and worse overall (black).

### 3-bit flip-flop task

To further demonstrate the ability of SOFO to learn long temporal dependencies, we turn to the 3-bit flip-flop task described in  (Figure 3A, B; see Appendix E.3 for details). Moreover, we compare the performance of SOFO not only against Adam, but also FORCE  (Figure 3C), another second-order optimizer based on recursive least-squares (RLS) which is admittedly the optimal solution in the "reservoir" setting. Whilst FORCE does make more rapid initial progress, SOFO eventually achieves a lower loss at convergence. To make the comparison fair to FORCE, we implemented a novel batched version of FORCE-RLS (Appendix B).

Figure 3: **3-bit flip-flop task**. (**A**) Network architecture for the classical 3-bit flip flop task: three input channels sporadically provide random bits (\( 1\)) at random time intervals and each corresponding output channel must hold its activity at the value given by the last provided bit (see **B**). The network feeds back its own output via random feedback weights (grey). Only the output weights (black) are trained. (**B**) Example outputs for the network trained with SOFO (dashed lines); tight match to target outputs (solid pale) shows successful training. Solid dark lines show the corresponding input bits. (**C**) Evaluation loss for Adam (black), SOFO (red) and FORCE (green), for a network of size 1000 and for SOFO small (purple) where the network size is reduced to 128 but the all sets of weights (i.e. recurrent weights, biases etc) are trainable. (**D**) Same as C, for varying number of neurons from 1000 to 8000 (dark to pale).

Figure 2: **Learning an adaptive Kalman filter (KF)**. (**A**) Task structure, where \(x_{t}\) denotes the latent state and \(y_{t}\) its momentary observation. Each trial (\(T=5000\) steps) is randomly partitioned into successive contexts (of potentially different durations; Appendix E.2). Each context is characterized by a different, randomly parameterized LDS, which produces latent state trajectories noisily observed by the RNN; context switches are uncued. At each step, the RNN must predict the current state. (**B**) Training curves for Adam and SOFO, compared to the MSE noise floor provided by KF knowing the current context (green, average over 1000 trials), and a baseline showing KF performance given random (and thus wrong) LDS parameters sampled from the task distribution (Kalman random; orange, average over 1000 trials). (**C**) Within-trial evolution of the mean-squared prediction error for the current context in the trained models, as a function of time elapsed since the last context switch, averaged over \(1500\) contexts (barely visible shaded areas = \( 2\) s.e.m.). Green and orange baselines: same as in (B).

Interestingly, we find that SOFO works best when the reservoir is non-chaotic at initialization (\(g=0.5\) instead of \(g=1.5\) as in ) - whereas FORCE tends to fail in this setting (as is well known). Morover, ref.  demonstrated that by restricting the locus of learning to those few task-relevant output weights, FORCE tends to come up with brittle solutions. SOFO, on the other hand, is a flexible general purpose 2\({}^{}\)-order optimizer that can be used to tweak any of the network's parameters (recurrent weights, biases, feedback and input weights), not just the output weights. We implemented this form of training and found that SOFO was able to train a much smaller network of 128 units and ReLu activations (instead of the usual FORCE tanh) on the 3-bit flip flop task (Figure 3C, purple), to a final test means squared error more than 15 times smaller than FORCE's with 1000 neurons. In summary, by allowing efficient training of more flexible network models from non-chaotic initializations, SOFO stands as nicely complementary to FORCE in the RNN training toolbox.

### Motor tasks

Finally, we also applied SOFO to two different memory-guided motor tasks: a single-reach task, and a more challenging double-reach task. In both tasks, the RNN receives task-related inputs (see below) and outputs a pair of time-varying torques driving the motion of a two-jointed model arm (Figure 4A; ). The RNN also receives sensory feedback in the form of joint positions and velocities. In the single-reach task (Figure 4), each trial begins with a variable "idle" phase during which the hand must remain at a central spot (where it is initialized). A variable-length preparation phase follows, during which the network is presented with inputs representing the \(x\)- and \(y\)-coordinates of the reach target but must continue to hold to the central spot. The movement phase is initiated by the withdrawal of a "hold" input present during the first two phases (c.f. ). The RNN must reach the target within 600 ms, and hold it for 200 ms. In the double-reach task, the two target locations are simultaneously presented during the preparation phase, and must be reached in a sequence (see Section 4.4 for details of the loss function). Target locations are randomly sampled in each trial from a distribution of reachable positions around the central spot.

Figure 4: **Memory-guided single-reach task. (A)** System schematics and task structure (see text). (B) Training curves for Adam and SOFO (mean \(\) 2 s.e.m. over 3 independent runs). (C) Wallclock time per training iteration. (D–F) Example single-neuron firing rates (D), single-trial hand trajectories (E) and shoulder torques (F) for eight selected reach conditions (color-coded). Gray areas denote preparation epochs.

We trained a _stochastic_ vanilla RNN on both tasks. This is a difficult task for a noisy RNN to acquire, as process noise in the recurrent dynamics not only causes motor disturbances in the arm, but also corrupts the memory of the target location which is no longer visible during the reach. A successful network must learn a strategy that mitigates the effect of ongoing noise in the relevant memory / output subspaces, and integrate sensory feedback to correct for output disturbances.

SOFO rapidly trains successful RNNs to perform both tasks proficiently in a few hundred training iterations (between 30min and 2h of wallclock time), where RNNs trained using Adam just about get the gist of the single-reach, and completely fail to learn double reaching (Figure 4B-E-F and Figure 5B-C-D) despite days of manual hyperparameter tuning. In the single-reach task, networks trained with Adam fail to withhold movement until the go cue, and appear unable to robustly memorize the target location and/or correct for motor noise.

Excitingly, the single-neuron activity patterns observed in the RNN trained with SOFO have a striking resemblance with those recorded in the motor cortex of non-human primates during similar tasks. Some neurons are active during movement only, producing rich multiphasic activity patterns; others are active during preparation only, and many are active during both phases. We leave the analysis of these population activity trajectories to future work; for now, we simply note that SOFO renews our ability to train networks on complex motor tasks, thus opening many avenues for future motor neuroscience investigations.

### Memory and compute profiling

Details on SOFO's memory and algorithmic complexity can be found in Appendix C. To experimentally demonstrate that SOFO is memory and compute efficient, we carried out profiling in the context of the Kalman filter learning task (Section 4.2), varying the time horizon \(T\) used in each trial. As expected from backpropagation, memory usage for Adam increases almost linearly with the time horizon, eventually exceeding the limit of our GPU for \(T 30\)K steps (Figure 6A, black). In contrast, SOFO's use of forward-mode AD incurs a low and constant memory cost independent of \(T\) (red). Remarkably, despite being a second-order method, SOFO is only about twice slower than Adam on wall-clock time per iteration (Figure 6B; see also wallclock time comparisons in Figure 4B and Figure 5B.

## 5 Limitations of SOFO

All tasks we have used here can be described using a relatively small number of bits - much fewer than the information that could in principle be stored in the model's parameters. Accordingly, simply

Figure 5: Memory-guided double-reaching task. **(A)** Task structure (see text). **(B)** Training curves for Adam and SOFO. **(C)** Example single-trial hand trajectories for selected pairs of consecutive reach targets. **(D)** Example time series of shoulder torques.

instating the right low-dimensional dynamical motifs in the RNN is sufficient to acquire the desired behaviour . Therefore, it is perhaps not surprising that SOFO performs very well even though it only optimizes a fraction of all model parameters in each iteration. We speculate that in richer learning tasks that require the RNN to store a lot of information (e.g. supervised learning on a rich dataset), SOFO might not have as large an advantage over Adam. Indeed, our experiments using SOFO to train an MLP-Mixer  on CIFAR-10 shows relatively poor performance - at best on par with Adam (Appendix D.3).

Even for simple tasks, and assuming a fixed \(K\) (size of the gradient and GGN sketches), we expect SOFO's performance to gradually deteriorate with the number of parameters in the model. This is because the sketch of the GGN matrix gets noisier as the tangent to parameter ratio (\(K/P\)) gets smaller - meaning that we explore a smaller fraction of the parameter space per iteration. Indeed, when training RNNs with increasing number of neurons \(S\), the convergence rate for SOFO worsens while that for Adam improves (Appendix D.2).

## 6 Conclusion and Future Work

In this paper, we have shown how sketches of both the Generalized Gauss-Newton matrix and the loss gradient can be computed efficiently on parallel hardware, leading to an RNN optimizer that outperforms Adam in all the neuroscience-related tasks we have studied, at lower memory complexity. By accelerating RNN training (and sometimes enabling successful training altogether), SOFO could greatly facilitate a whole line of neuroscientific inquiry that relies on constructing neural networks that solve behavioral tasks. These trained networks are constrained for biological realism and reverse-engineered to increase our understanding of brain computations. SOFO's fast and robust convergence in all the tasks we have tried suggests that it could enable faster analysis-by-synthesis iterations.

Our current implementation of SOFO could be refined in a number of ways. Adaptive damping (e.g. based on the Levenberg-Marquardt heuristic [37; 33]) is known to make second-order optimization algorithms more robust. We speculate that SOFO might benefit from it - although perhaps not to the same usual degree, given that a random sketch of the GGN matrix is typically better-conditioned than the full GGN (it is indeed very unlikely that parameter subspaces randomly sampled at each iteration would exactly contain the top and bottom eigenvectors of the GGN).

In principle, SOFO is a general-purpose optimizer that could be deployed onto any problem outside the realm of recurrent neural networks. We expect SOFO to perform particularly well in models that have relatively few tunable parameters yet involve computational graphs that are too large to allow backpropagation to operate on large data batches. A prime example of this is the fine tuning of large language models, e.g. with low-rank adaptors , but there are many other potential applications e.g. in the physical sciences.