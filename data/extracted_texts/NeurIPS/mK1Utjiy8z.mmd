# RClicks: Realistic Click Simulation

for Benchmarking Interactive Segmentation

 Anton Antonov\({}^{1\)\(\)

Andrey Moskalenko\({}^{1,\,2\,}\)

Denis Shepelev\({}^{1}\)

Alexander Krapukhin\({}^{1}\)

Konstantin Soshin\({}^{1}\)

Anton Konushin\({}^{1,\,2}\)

Vlad Shakhuro\({}^{1,\,2\,}\)

\({}^{1}\)AIRI, Moscow, Russia

\({}^{2}\)Lomonosov Moscow State University

{lastname}@airi.net

\({}^{}\)Equal contribution

\({}^{}\)Project leader

Corresponding author

https://github.com/emb-ai/rclicks

###### Abstract

The emergence of Segment Anything (SAM) sparked research interest in the field of interactive segmentation, especially in the context of image editing tasks and speeding up data annotation. Unlike common semantic segmentation, interactive segmentation methods allow users to directly influence their output through prompts (e.g. clicks). However, click patterns in real-world interactive segmentation scenarios remain largely unexplored. Most methods rely on the assumption that users would click in the center of the largest erroneous area. Nevertheless, recent studies show that this is not always the case. Thus, methods may have poor performance in real-world deployment despite high metrics in a baseline benchmark. To accurately simulate real-user clicks, we conducted a large crowdsourcing study of click patterns in an interactive segmentation scenario and collected 475K real-user clicks. Drawing on ideas from saliency tasks, we develop a clickability model that enables sampling clicks, which closely resemble actual user inputs. Using our model and dataset, we propose RClicks benchmark for a comprehensive comparison of existing interactive segmentation methods on realistic clicks. Specifically, we evaluate not only the average quality of methods, but also the robustness w.r.t. click patterns. According to our benchmark, in real-world usage interactive segmentation models may perform worse than it has been reported in the baseline benchmark, and most of the methods are not robust. We believe that RClicks is a significant step towards creating interactive segmentation methods that provide the best user experience in real-world cases.

## 1 Introduction

The task of interactive segmentation involves providing additional hints or prompts to the method, allowing it to produce more precise annotations compared to conventional semantic segmentation. The most famous member of interactive segmentation methods is Segment Anything (SAM) [1; 2]. Nowadays, SAM-like methods are applied in various fields, including the thin object segmentation [3; 4], medical segmentation [5; 6; 7; 8; 9; 10], 3D segmentation [11; 12], tracking  and video .

Typically, interactions occur in several rounds, where in each round the user corrects the prediction errors of the previous one. Evaluation of such methods requires user inputs. However, collecting many real-user inputs for multiple rounds is impractical since such a dataset needs to be rebuilt for every method and every interaction round due to its iterative nature. Thus, researchers often resort to a simple strategy to simulate user inputs. According to this strategy, a single click for each interaction round is generated as follows: (1) select the largest error region in the previous interaction round, and (2) click in the furthest point from the boundaries of this region (center point). Hereinafter, we referto the above click sampling strategy as a _baseline strategy_, following . However, relying solely on this approach may result in overfitting and degraded performance in real-world usage scenarios.

Our goal is to create a highly realistic simulator of user clicks to enable a more accurate evaluation of interactive segmentation methods. We begin our research with a simple observation: when a user clicks, their gaze is focused on the area where they click. In turn, the task of predicting saliency is well-studied, with benchmark data collected using specialized devices such as eye trackers. Saliency prediction models generate spatial attention heatmaps, from which fixation points of viewers can be sampled and utilized. However, saliency models assume a free-viewing task, which differs from interactive segmentation, where the user should segment a specific area. In other words, clicks should be sampled from a spatial distribution, that is conditioned not only on an image but also on a target segmentation area.

Drawing from best practices in interactive segmentation and saliency prediction tasks, we collect a dataset of task-specific user clicks, and propose a model that facilitates sampling of the most realistic click positions in interactive segmentation. Overall, our main contributions are as follows:

* We curate a large multiple-round interaction dataset in the interactive segmentation task (see samples from the first round in Figure 1). To achieve this, we introduce a click collection methodology and conduct an ablation to address presentation bias, involving users on both PCs and mobile devices.
* We introduce a novel click sampling strategy based on a _clickability model_ that can sample more realistic clicks than the _baseline strategy_ and estimate click probabilities.
* a novel benchmark, that leverages the _clickability model_ to estimate the real-world performance of interactive methods. We conduct extensive comparisons and benchmark state-of-the-art methods using both the _baseline strategy_ clicks, and realistic clicks simulated by the _clickability model_. This comparison reveals that benchmarks employing the _baseline strategy_ may overestimate methods' real-world performance. Moreover, we conclude that current segmentation methods are unable to achieve both optimal performance and robustness simultaneously on all datasets.
* We utilize the collected first-round real-user clicks to evaluate the performance of segmentation methods. Furthermore, we propose a methodology to estimate the real-world segmentation difficulty for state-of-the-art methods for each instance in a dataset.

We believe that the proposed methodology enhances comprehension of real-users actions and will facilitate the development of interactive methods that are more applicable in real-world cases.

Figure 1: Examples of real and predicted users’ clicks of interactive segmentation task. The upper row depicts real-users clicks (green) for a given target object (white contour); the middle and bottom rows visualize, correspondingly, clicks and their distribution predicted by our clickability model. Purple points in the middle and bottom rows represent clicks generated by the _baseline strategy_. Mostly baseline click is close to a mode of users’ distribution (see (b) and (e)), however, in some cases it may be far from the mode (e.g. (a), (d)) or may not represent all modes of the distribution (e.g. (c), (e)).

Related Work

### User Input Types in Interactive Segmentation

Various types of user inputs have been explored in the literature. In [15; 20] an initial selection is obtained using bounding boxes, and then refined with strokes. In  object selection is done with strokes.  considers contours for selecting small objects, minor parts of an object, or a group of objects of the same type.  proposes to use trimap, scribblemap or clickmap as an input. Segment Anything, or SAM , processes multiple types of user prompts, including a point, a box, a mask, or a text.

Clicks-based approach selects objects of interest according to multiple user clicks (either positive or negative), and was first introduced in  and investigated in [19; 25; 26; 27; 1; 3; 28]. We focus solely on the click-based approach, since it is well-explored and has an established evaluation procedure in the field.

### Benchmarking Interactive Segmentation

GrabCut  is the first dataset proposed for interactive segmentation task. Then  adapted Berkeley  segmentation dataset to evaluate interactive segmentation methods, but it required manual testing. However, manual testing is a time-consuming and resource-intensive process. Interactive segmentation expects multiple rounds of interactions, when each interaction depends on previous ones, and it is infeasible to apply manual procedure for larger scales. For these reasons, in practice, benchmarks generate user interactions automatically based on previous interactions.

In  authors proposed an automatic clicks generation strategy for evaluation on PASCAL VOC 2012  and COCO  segmentation datasets. The subsequent work  used DAVIS  and SBD  datasets for interactive segmentation, applying the same _baseline strategy_.

Most of the existing click-based methods [19; 25; 26; 27; 1; 3; 28] use the _baseline strategy_. However, it has not been validated in real-world usage scenarios until recently.  introduced TETRIS benchmark and revealed that real users do not always click in the center of an area with the largest error, as assumed in the _baseline strategy_. Using the adversarial attacks, the paper demonstrated that methods have a tendency to overfit to the _baseline strategy_. Specifically, when the baseline clicks are used, the segmentation quality may be high, but even a slight change in the click position can result in a significant drop in quality. Therefore, the _baseline strategy_ may not accurately estimate the quality of the methods in real usage. We believe that to estimate the actual quality, each click should be generated in accordance with _human perception_.

### Saliency Prediction

The task of saliency prediction aims to model human perception by predicting probability maps [33; 34] of user engagement in a free-view observation for a given media content. Reference data for this task usually comes from a specialized device - an eye tracker - which records eye _fixations_[35; 36; 37]. Subsequently, fixations from multiple viewers are aggregated into a probability distribution through Gaussian at each fixation point, with sigma corresponding to the retinal angle of a human's field of view . Since scaling expensive eye tracker experiments is too complex, several researchers [38; 39] proposed to use mouse movements as a proxy for saliency when training saliency models. However, saliency fixations cannot be directly used in the interactive segmentation task because saliency observers engage in free-viewing, while in our task, the user's goal is to make a click to highlight a specific object or a part of it. Thus, for the interactive segmentation problem, real-user clicks should be collected.

## 3 Users' Clicks Dataset

We propose a novel dataset of real-users clicks for interactive segmentation. Our dataset is based on the existing image segmentation datasets. In total, we collected 475 544 user inputs for GrabCut , Berkeley , DAVIS , COCO-MVal , TETRIS . To gather users' clicks, we developed a specialized presentation tool. Specifically, in each task, we asked users to click on the target objects by displaying images and corresponding segmentation masks. We considered several display modesto instruct users what objects should be selected. As interface can cause bias in clicks distribution, we conducted a user study to select the option that best mimics natural user object selections.

This section is organized as follows. First, we present task display modes (3.1). Then we choose one that eliminates bias associated with user viewing mode behavior (3.2). Finally, we describe clicks collection in the first and the subsequent rounds of interactions (3.3).

### Collection Procedure

When collecting user clicks, we executed the following procedure (see Figure 2): (1) Show the entire image for 1.5 seconds. (2) Show segmentation target using one of the Display Modes. (3) The entire image is shown again for 1.5 seconds, during which clicking is not allowed. (4) The user makes a click. Steps (1) and (3) simulate the user behavior during real-world interactive segmentation, when individuals initially view the image and then interact with it by clicking. Step (2) visualizes the object that should be selected by the user.

We considered the following task displaying modes. (a) _Text Description_ mode shows the textual description of the target object for 2.5 seconds. (b) _Object CutOut_ visualizes the target instance in its original position on a gray background for 2 seconds. (c) During _Shifted CutOut_, the target instance is shown for 2 seconds on a gray background, then shifted to the top-left corner, which aims to motivate the assessor to independently locate the instance on the image as its position is shifted. (d) _Silhouette Mask_ shows a black-and-white mask of the target instance in the original position for 2 seconds. (e) _Highlighted Instance_ displays the original image with the background where the target instance is highlighted with a green border. Rationale for our choice of these displaying modes and time periods can be found in Appendix C.1.

### Selecting Unbiased Task Display Mode

_Text Description_ is considered to be unbiased because users do not see the target segmentation mask, as in real-world interactive segmentation. However, textual descriptions may be ambiguous for certain types of instances or areas (see Figure 3). In other display modes, the mask is presented, which could potentially distort the distribution of user clicks. To choose mask-based display mode with minimal bias, we compare all modes with _Text Description_ mode.

Figure 3: Examples of situations where instructing participants with text descriptions may be challenging or ambiguous: selection of a certain instance in the first round ((a)-(b)); and selecting or unselecting a certain error area in the subsequent round ((c)-(d)).

Figure 2: Illustration of the tested display modes to reduce presentation bias. The best result was obtained with the _Object CutOut_ mode, where an object is presented on a gray background without shifts.

Therefore, we conducted an ablation study on 100 randomly selected images from TETRIS dataset . We used images and segmentation masks from TETRIS, and additionally manually annotated textual descriptions. In this study, we compare the clicks obtained via considered display modes with clicks collected through _Text Description_.

Clicks gathering was done on Toloka AI 1 crowdsourcing platform. Each participant was given a batch of 10 unique images, in each image they were required to make one click. Each participant received on average 3 batches of images. Participants did not receive the same image more than once. For every display mode, different people were involved in labeling. A participant's clicks are considered to be valid, if at least 7 out of the 10 clicks in a batch were within the object mask. We also considered a click to be valid whether it was within the object mask or not farther than one click radius from the border. Otherwise, clicks were disregarded as invalid. To select an unbiased presentation strategy, for each image, we collected 25 clicks from participants using computers and 25 clicks from those using mobile devices. After filtering, in total we obtained 47 725 valid clicks.

To compare quantitatively display modes with unbiased _Text Description_ mode, we utilized the following sample-based metrics: (a) PL\({}_{1}\) - mean of all pairwise L\({}_{1}\)-distances between click coordinates, normalized by object width and height. (b) WD - Wasserstein distance  between click coordinates, normalized by object width and height. (c) KS - Kolmogorov-Smirnov test in 2D case . We conclude that clicks are not significantly different if a p-value is greater than 0.05. The indicator function is used as a metric, which equals 1 when clicks are not significantly different.

The average values w.r.t. images of the listed metrics for each display mode are presented in Table 1. The best results were obtained with the _Object CutOut_ method. Note that we did not use probability map based metrics used for evaluating saliency prediction, because the actual model of clicks distribution is unknown, and we did not want to limit it in the ablation stage. Here, we ablated display modes only for the first interaction rounds. We cannot examine our display modes in the subsequent rounds, as, in addition to the reference instance, the user needs to know which error should be corrected and where it is located, which is not possible to describe textually. Thus, we assume that the best display mode for the first round is also best for the subsequent ones.

In the following, we used _Object CutOut_ mode to collect clicks for the remaining datasets.

### Collected Interactions

We annotated each instance in all common interactive segmentation benchmark datasets - DAVIS , GrabCut , COCO-MVal , Berkeley , TETRIS  using PC and mobile clicks. Collected clicks were validated similarly to the ablation stage. When annotating subsequent interaction rounds, the user should click in the area of the segmentation error. To obtain error masks for the subsequent rounds, we applied state-of-the-art interactive segmentation methods - SAM , SimpleClick , and RITM  - to all images and all clicks corresponding to those images from the first round. Then, for each image, we selected the mask with the highest quality up to a threshold of 0.95 IoU. We motivate it by the fact that at such a high level of quality, the user is likely to stop annotating the instance as the errors would be minimal, and even the radius of the click may exceed the size of the erroneous area. In total, we collected 475 thousand valid clicks from users. The number of valid clicks annotated for each dataset and interaction round is presented in Table 2.

   Display mode & PL\({}_{1}\) \(\) & KS \(\) & WD \(\) \\  _Object CutOut_ & **0.242** & **0.58** & **0.042** \\ _Shifted CutOut_ & 0.246 & 0.56 & 0.046 \\ _Silhouette Mask_ & 0.246 & 0.41 & 0.048 \\ _Highlighted Instance_ & 0.258 & 0.37 & 0.051 \\   

Table 1: Comparison of display modes with _Text Description_.

   Dataset & First \# & Subseq. \# & Sum \# \\  GrabCut & 2 395 & 3 427 & 5 822 \\ Berkeley & 4 859 & 6 937 & 11 796 \\ DAVIS & 16 975 & 23 687 & 40 662 \\ COCO-MV & 38 097 & 53 926 & 92 023 \\ TETRIS & 123 023 & 202 218 & 325 241 \\  All & 185 349 & 290 195 & 475 544 \\   

Table 2: The number of collected clicks for each dataset in interaction rounds.

Click Simulation

In this section, we explore models for predicting user clicks. Firstly, the baselines are described (4.1). Secondly, we introduce _a clickability model_ used for click prediction (4.2) in our interactive segmentation benchmark. Thirdly, in (4.3) we describe the construction of training dataset for _clickability model_. Finally, we compare our _clickability model_ with baselines (4.4).

### Baseline Models

As baselines for comparison, we considered uniform, distance, and saliency distribution models. The uniform hypothesis postulates that the clickability of all pixels within the target area is equally distributed (see Figure 4(b)). When the area of interest is relatively small, the uniform assumption is reasonable. However, according to this assumption, the click probability of object boundaries and their centers is equal, which is not necessarily true. The distance transform [14; 19] addresses this issue by assigning greater weight to pixels in the center of the object than to those on the boundary (see Figure 4(c)). Nevertheless, this transform considers only the shape of the object, neglecting human perception. To account for human perception, saliency distribution can be used. This is a reasonable baseline, as users look at the target area of interaction when clicking. For the saliency baseline, we utilized a state-of-the-art model, TranSalNet . The example of constructed saliency distribution is presented in Figure 4(d), details of how such map is constructed can be found in the Appendix B.2. However, saliency models are trained for free-viewing task, and do not take into consideration the setup of our task.

### Clickability Prediction Model

Similar to the saliency prediction task, we formulate the task of simulating user clicks as a probabilistic problem. Given an image, a ground truth object mask, and a segmentation error mask (FP or FN), the model should predict at each pixel the probability of being clicked. We refer to this as a _clickability map_ (see details in Section 4.3). The proposed pipeline is shown in Figure 5. As a base architecture for our model, we adapted state-of-the-art SegNeXt segmentation network . We input the original image into the network and concatenate the ground truth mask with the error mask, feeding the resulting tensor as an additional input to the network, a technique inspired by the Conv1S . We use the Kullback-Leibler divergence (KLD) loss function between the predicted and ground truth distributions.

Figure 4: Examples of considered clickability models: (a) visualizes target object (white contour) and ground-truth clicks (green points); (b) – (d) depict uniform distribution (UD), distance transform (DT), and saliency map (SM) respectively; (e) – our predicted clickability map.

Figure 5: Proposed clickability prediction pipeline.

   Model & KS\(\) & PL\({}_{1}\)\(\) & WD\(\) & NSS\(\) & PDE\(\) \\  UD & 0.10 & 0.57 & 0.17 & 3.99 & 1.36E-05 \\ DT & 0.14 & 0.52 & 0.16 & 6.45 & 2.76E-05 \\ SM & 0.13 & 0.51 & 0.15 & 4.79 & 1.83E-05 \\ Ours & **0.55** & **0.40** & **0.08** & **9.11** & **4.69E-05** \\   

Table 3: Evaluation of various clickability models on real-user clicks of TETRIS validation part. Our approach outperforms existing clicking strategies in terms of the proximity of samples to real-user clicks.

### Clickability Maps Dataset

We introduce the concept of a _clickability map_ as a single-channel image, such that the value of each pixel corresponds to the probability that the user will click on it during the interaction round. We propose to use such maps to train _clickability models_.

Given an image, error mask, and user clicks, the _clickability map_ is constructed as follows: (1) initialize the map as an image of zero values; (2) at each pixel position that was clicked, add one; (3) smooth the map by a Gaussian with some sigma, where sigma is a hyperparameter; (4) multiply pixel values of the map by corresponding pixel values from a _soft error mask_, obtained by smoothing the original error mask by a Gaussian; (5) normalize pixels by the map sum. The proposed method is analogous to the construction of saliency maps from human eye fixations, except for step (4).

Unlike saliency, we need to somehow constrain the most likely click positions within the boundaries of the mask. Moreover, recall that during the collection of clicks, we considered clicks as valid if they were inside the mask or close to its border. For these reasons, in step (4) the _clickability map_ is conditioned by multiplying it on the _soft error mask_ - smoothing the error mask we consider the allowed radius of the border vicinity. We smooth the error mask by Gaussian blurring with a sigma equal to the radius of the click used in the user interface (i.e., 1% of image diagonal). The sigma in step (3) simulates the probability density of clicks inside the mask.

We constructed train and validation datasets as follows. We split images of TETRIS dataset into non-overlapping train and validation parts. Since we do not know the real click density, for model training and validation, several sets of _clickability maps_ were constructed with varying magnitudes of sigma from step (3). To choose the best sigma, we conducted an ablation study, which can be found in Appendix B.3. Note that we constructed _clickability maps_ using clicks from both smartphones and PCs. This was done to ensure that the model would predict clicks regardless of the device type.

### Models Evaluation

To choose the best _clickability model_, we evaluated considered models on the real-user clicks of TETRIS validation part. Here, in addition to sample-based metrics considered above, we calculated additional metrics, that were computed based on the ground-truth clicks positions and predicted clicks distribution: PDE - likelihood of ground-truth clicks, and NSS from saliency benchmarks . Evaluation results are presented in Table 3. Our clickability model shows the best performance. We address the question of model generalizability in the Appendix B.2.

## 5 Benchmarking Interactive Segmentation

In this section, we introduce **RClicks** benchmark that evaluates the interactive segmentation methods according to the proposed _clickability model_. Our evaluation protocol aims to estimate not only the average annotating time but also the spread w.r.t. _clicking groups_. Our model returns a probability density for an instance. For every possible click, we have \((x,y)\) coordinates and probability. We sort clicks according to their probabilities and split them into 10 intervals (called _clicking groups_) \(\{_{i}\}_{i=1}^{10}\) s.t. every interval has 10% of total probability mass. We interpret these _groups_ as different user clicking patterns, and evaluate methods for each group separately. Visualization of _clicking groups_ for an instance may be seen in Figure 6. Note, that even the probability mass of each group \(_{i}\) is equal, the average probability of clicks in each group increases with increase of \(i\).

We modify a common evaluation protocol  by replacing the _baseline sampling strategy_ with sampling from different _clicking groups_, obtained through the _clickability model_. Specifically, (1) interactive segmentation metrics (e.g. NoC) are calculated for every instance in a dataset and group \(_{i}\) by sampling click from \(_{i}\) (weighted by the _clickability model_) for every interaction round (in our experiments - 20 rounds); (2) then for each instance statistics (e.g. mean and standard deviation) of sampled metrics over _clicking groups_ are estimated; (3) finally, these statistics are averaged over all instances in the dataset.

Figure 6: Spatial distribution of clicking groups for the instance in Figure 4(a).

In interactive segmentation, a commonly used metric is NoC\({}_{20}\)@90, which estimates the annotation time (in clicks, not more than 20) to achieve 90% IoU using a particular method. We use this metric with our sampling strategy to estimate the following averaged statistics: (i) Mean and standard deviation, which are denoted as **Sample** NoC. (ii) Relative increase of Sample NoC compared to the _baseline strategy_ NoC. This statistic indicates how much extra annotation time an average user spends compared to the _baseline strategy_. We denote it as \(\)SB. (iii) Relative increase of annotation time using clicks from group G\({}_{1}\) over using clicks from group G\({}_{10}\). This metric is denoted as \(\)**GR**. This metric represents a difference in annotation speed between two _clicking groups_, that have a maximum difference of average clicking probabilities.

Figure 7 shows plots of the averaged IoU versus the number of clicks for various segmentation methods when sampling clicks according to the _baseline strategy_, G\({}_{1}\) and G\({}_{10}\) groups. Overall, clicks from G\({}_{10}\) outperform clicks from G\({}_{1}\) in terms of IoU-AuC, while the _baseline strategy_ mostly outperforms clicks from both groups. With a sufficient number of interactions, clicks from both G\({}_{10}\) and G\({}_{1}\) achieve high IoU. However, clicks from G\({}_{1}\) require more interactions.

Estimated statistics for NoC\({}_{20}\)@90 are presented in Table 4. Additional evaluation results for NoF\({}_{20}\)@90, IoU-AuC\({}_{20}\) are provided in Appendix D.

In addition to the evaluation results on simulated clicks, we provide evaluation results on the first round real clicks. The evaluation on the subsequent real clicks is infeasible, since interactive segmentation in a subsequent round depends on a model output from a previous round. However, there is no such problem in the first round, and actual performance metrics can be computed on the real clicks of the first round. Therefore, we employed the real clicks from the first round as follows: (1) computed real-world accuracy (see Appendix D); (2) compared accuracy of interactive segmentation methods using real and simulated clicks (see Appendix D); and (3) estimated real-world robustness of the methods for each instance in the dataset. We estimated the latter through IoU noise-to-signal ratio (NSR). The greater the value of NSR, the more difficult such an instance is for segmentation. Figure 8 plots a scatter of mean vs. standard deviation of IoU over real-users first clicks and segmentation methods from Table 4.

Figure 8: A scatter plot of the mean vs. standard deviation (STD) of IoU for the first real-users clicks. Each point represents the statistics for each instance, averaged across all considered segmentation methods and real clicks. An average NSR for each dataset is provided in brackets in the legend.

Figure 7: Mean IoU for varying number of clicks for _baseline strategy_, G\({}_{1}\) and G\({}_{10}\)_clicking groups_. IoU-AuC under these curves provided in brackets.

## 6 Discussion

A review of Table 4, Figures 7 and 8 leads to the following conclusions. First, according to \(\)SB, _baseline strategy_ underestimates the real-world annotation time from 5% up to 29%. This implies that the baseline benchmark may significantly underestimate the real-world annotation costs. Consequently, our benchmark may be employed for a more accurate estimation of annotation costs.

Then, according to \(\)GR, **annotation time of users from different _clicking groups_ varies from 3% up to 79%. The observed variations in \(\)GR indicate that segmentation methods are unstable w.r.t. click positions in the image.

    &  &  &  &  &  \\  & & & _{20}\)@90} & _{20}\)@90} & _{20}\)@90} \\  & & &  & \(\)SB & \(\)GR & Sample & \(\)SB & \(\)GR & Sample & \(\)SB & \(\)GR \\  & & & (\(\)std) & (\(\)\%) & (\(\)std) & (\(\)\%) & (\(\)std) & (\(\)\%) & (\(\)std) & (\(\)\%) & (\(\)\%) \\  GPCIS  & RN50 & C+L & 6.44±0.85 & 16.88 & 53.65 & 4.74±1.31 & 26.43 & 79.00 & 3.87±0.79 & 19.55 & 56.43 \\   & RN34 & C+L & 5.95±0.73 & 14.95 & 45.88 & 4.13±0.85 & 15.15 & 49.79 & 3.10±0.53 & 14.16 & 44.06 \\  & RN34 & SBD & 7.87±1.25 & 23.39 & 64.95 & 6.36±1.29 & 20.86 & 51.58 & 4.51±0.76 & 17.08 & 55.22 \\   & HR18 & C+L & 6.23±0.67 & 6.92 & 16.13 & 3.71±0.78 & 10.27 & 20.22 & 3.69±0.52 & 7.02 & 13.95 \\  & HR188-IT & C+L & 6.71±0.99 & 20.88 & 54.15 & 3.65±0.92 & 16.81 & 33.89 & 3.80±0.67 & 15.79 & 32.66 \\  & HR18-IT & C+L & 6.15±0.83 & 11.37 & 31.14 & 3.22±0.83 & 15.84 & 37.01 & 3.48±0.60 & 11.59 & 23.99 \\  & HR32-IT & C+L & 5.90±0.89 & 18.34 & 51.07 & 3.24±0.83 & 15.50 & 37.31 & 3.44±0.65 & 17.47 & 30.69 \\  & HR18-IT & SBD & 7.42±1.03 & 17.85 & 38.39 & 4.81±1.24 & 17.17 & 43.23 & 4.80±0.74 & 11.96 & 25.31 \\   & ViT-B & C+L & 4.97±0.40 & 8.60 & 15.14 & 2.93±0.58 & 9.44 & 19.75 & 2.62±0.37 & 6.99 & 12.94 \\  & ViT-B & SBD & 5.37±0.49 & 8.69 & 17.94 & 4.33±1.06 & 14.59 & 35.07 & 3.49±0.50 & 8.40 & 16.44 \\   & ViT-B & C+L & 5.32±0.54 & 9.05 & 26.33 & 3.07±0.70 & 11.72 & 23.60 & 2.73±0.41 & 8.86 & 16.64 \\  & ViT-L & C+L & 5.03±0.42 & 8.71 & 16.67 & 2.67±0.56 & 8.05 & 20.88 & _2.46±0.35_ & 7.11 & 10.01 \\  & ViT-H & C+L & 5.00±0.42 & 7.06 & 12.29 & **2.57±0.54** & 6.14 & 17.65 & 2.36±0.33 & 6.94 & 10.83 \\  & ViT-XT & SBD & 8.35±1.36 & 18.67 & 51.05 & 5.86±1.65 & 26.28 & 61.63 & 5.49±1.22 & 28.57 & 35.40 \\  & ViT-B & SBD & 5.77±0.58 & 8.44 & 25.72 & 4.52±1.07 & 17.58 & 36.18 & 3.75±0.54 & 11.42 & 19.50 \\  & ViT-L & SBD & 5.56±0.53 & 7.26 & 15.97 & 3.83±0.88 & 10.02 & 33.06 & 3.40±0.43 & 7.32 & 16.15 \\  & ViT-H & SBD & 5.49±0.55 & 7.67 & 23.69 & 3.74±0.86 & 10.46 & 31.97 & 3.32±0.43 & 7.37 & 16.88 \\  CFR-ICL  & ViT-H & C+L & _4.53±0.46_ & 9.32 & 18.47 & 2.70±_0.63_ & 9.58 & 24.13 & **2.12±0.34** & 8.76 & 14.33 \\  MobileSAM  & ViT-Tiny & SA-1B & 5.96±0.56 & 8.63 & 15.39 & 5.25±0.78 & 9.79 & 19.78 & 3.42±0.48 & 7.47 & 12.69 \\   & ViT-B & SA-1B & 5.30±0.53 & 8.26 & _11.27_ & 4.91±0.79 & 9.88 & 15.73 & 3.04±0.51 & 11.17 & 10.06 \\  & ViT-L & SA-1B & 5.21±0.41 & 8.82 & 11.59 & 4.81±0.63 & 8.89 & 14.97 & 2.60±0.40 & 8.11 & 7.08 \\  & ViT-H & SA-1B & 5.42±0.49 & 8.00 & 15.02 & 5.14±0.68 & 7.63 & 15.61 & 2.66±0.38 & **5.95** & 8.50 \\   & ViT-B & SA-1B & 5.32±0.50 & 7.45 & 13.03 & 5.39±0.89 & 12.48 & 16.68 & 3.35±0.67 & 16.41 & 10.74 \\  & ViT-L & SA-1B & 5.19±0.48 & 8.58 & 15.69 & 5.05±0.74 & 9.64 & 13.50 & 2.81±0.51 & 11.02 & 7.69 \\  & ViT-H & SA-1B & 5.16±0.44 & 8.15 & 18.36 & 4.97±0.68 & 7.71 & _12.36_ & 2.75±0.41 & _6.78_ & 7.95 \\   & Hiera-T & SA-V & 4.65±0.28 & **4.86** & **7.46** & 3.86±0.64 & 7.79 & 13.14 & 3.11±0.50 & 9.45 & 3.57 \\  & Hiera-B & SA-V & 4.67±0.33 & 8.49 & 15.86 & 3.75±0.61 & _7.44_ & 12.67 & 3.02±0.47 & 9.51 & _4.79_ \\   & Hiera-L & SA-V & _4.61±0.29_ & 9.51 & 13.28 & 3.84±0.62 & 9.12 & 12.35 & 2.83±0.41 & 7.46 & _4.10_ \\   & Hiera-H & SA-V & **4.39±0.23** & 7.55 & 10.03 & 3.42±0.51According to Sample and \(\)GR values, the best annotation time is achieved by SAM 2 Hiera-H (on DAVIS), CFR-ICL (on TETRIS) and SimpleClick ViT-H (on COCO-MVal). The two latter methods are less robust compared to SAM-like methods, which perform more consistently across _clicking groups_. However, SAM 2 Hiera-H backbone is less robust than Hiera-T backbone on DAVIS dataset. This indicates that **there is currently no segmentation method that is optimal in terms of both performance and robustness on all datasets**. Consequently, **developers should select a method in accordance with their requirements**.

Finally, points in the bottom-right part of Figure 8 correspond to instances with high NSR. These values may be utilized **to identify and analyze hard instances in the datasets**. Additionally, according to averaged NSR, we **identified the hardest dataset for annotation**, it is DAVIS with 24.15 NSR.

## 7 Conclusion

In this paper, we presented RClicks - a benchmark for interactive segmentation methods, that evaluates both real-world quality and robustness with respect to different clicking patterns. Using the developed unbiased presentation strategy, we collected the multi-round real-user click dataset. We developed the _clickability model_ that can be utilized to estimate click probabilities and sample realistic user clicks. By employing this model in our benchmark, we demonstrated that _baseline strategy_ may overestimate methods' performance in the real world. Furthermore, our analysis showed that there is currently no interactive segmentation method that is optimal in terms of both performance and robustness on all datasets. Additionally, we evaluated segmentation methods using real-user clicks of the first round and proposed a methodology to estimate the instance difficulty for state-of-the-art methods. We hope RClicks will facilitate the advancement of interactive segmentation methods that provide optimal user experiences in real-world scenarios.