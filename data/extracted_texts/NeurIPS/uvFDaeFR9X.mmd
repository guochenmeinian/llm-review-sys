# Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations

Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations

Artem Agafonov\({}^{1,\,2}\)

Pert Ostroukhov\({}^{1,\,2}\)

Roman Mozhaev\({}^{2}\)

Konstantin Yakovlev\({}^{2}\)

Eduard Gorbunov\({}^{1}\)

Martin Takac\({}^{1}\)

Alexander Gasnikov\({}^{3,2,4}\)

Dmitry Kamzolov\({}^{1}\)

Contact details: Artem Agafonov - agafonov.ad@phystech.edu; Petr Ostroukhov - postroukhov12@gmail.com; Roman Mozhaev - mozhaev.rm@phystech.edu; Konstantin Yakovlev - iakovlev.kd@phystech.edu; Eduard Gorbunov - eduard.gorbunov@bzuai.ac.ae; Martin Takac - takac.MT@gmail.com; Alexander Gasnikov - gasnikov@yandex.ru; Dmitry Kamzolov - kamzolov.opt@gmail.com.

###### Abstract

Variational inequalities represent a broad class of problems, including minimization and min-max problems, commonly found in machine learning. Existing second-order and high-order methods for variational inequalities require precise computation of derivatives, often resulting in prohibitively high iteration costs. In this work, we study the impact of Jacobian inaccuracy on second-order methods. For the smooth and monotone case, we establish a lower bound with explicit dependence on the level of Jacobian inaccuracy and propose an optimal algorithm for this key setting. When derivatives are exact, our method converges at the same rate as exact optimal second-order methods. To reduce the cost of solving the auxiliary problem, which arises in all high-order methods with global convergence, we introduce several Quasi-Newton approximations. Our method with Quasi-Newton updates achieves a global sublinear convergence rate. We extend our approach with a tensor generalization for inexact high-order derivatives and support the theory with experiments.

## 1 Introduction

In this paper, we primarily address the problem of solving the Minty Variational Inequality (MVI) . Given a continuous operator \(F:^{d}\), where \(^{d}\) is a closed bounded convex subset with a diameter \(D=_{x,y}\|x-y\|\), the objective is to find a point \(x^{*}\) such that

\[ F(x),x-x^{*} 0,x.\] (1)

The solution to (1) is referred to as a weak solution of the Variational Inequality (VI) . In contrast, the Stampacchia variational inequality problem  consists in finding a point \(x^{*}\) such that

\[ F(x^{*}),x-x^{*} 0,x.\] (2)

This solution is often called a strong solution to the variational inequality. When the operator \(F\) is both continuous and monotone, the weak and strong solutions are equivalent .

**Assumption 1.1**: _The operator \(F(x)\) is called monotone, if_

\[ F(x)-F(y),x-y 0,x,y.\] (3)

Another useful assumption is \(L_{1}\)-smoothness.

**Assumption 1.2**: _The operator \(F(x)\) is \(L_{1}\)-smooth, if it has Lipschitz-continuous first-order derivative_

\[\| F(x)- F(y)\|_{} L_{1}\|x-y\|,x,y.\]

First-order methods.Variational inequalities encompass a wide range of problems, including minimization , min-max problems, Nash equilibrium, differential equations, and others . The extensive research on VI methods dates back several decades, with a notable breakthrough in the 1970s--the development of the Extragradient method . Subsequently, it was demonstrated that this method achieves global convergence of \(O(^{-1})\), matching the convergence rates of other first-order2 methods such as optimistic gradient , forward-backward splitting , and dual extrapolation . These first-order methods collectively exhibit optimal convergence .

Second-order and high-order methods.To achieve further notable acceleration of methods for VIs, one can leverage information about higher-order derivatives. For instance, simply incorporating first-order derivatives (Jacobian) can significantly enhance the convergence speed of the method. Following recent advancements in second-order and high-order methods with global rates for minimization , several high-order methods for VIs have been proposed . However, all these methods involve a line-search procedure, resulting in \((^{-2/3})\) convergence for the case of first-order information (Jacobians). Recent works  propose methods with improved rates \(O(^{-2/3})\) and establish the lower bound \((^{-2/3})\), rendering these algorithms optimal.

Jacobian's approximation.In the last decade, VIs found new applications in machine learning. There are many problems that could not be reduced to minimization, including reinforcement learning , adversarial training , GANs , classical learning tasks in supervised learning , unsupervised learning , image denoising , robust optimization . Applying second-order methods, without even mentioning high-order ones, described in a previous paragraph to machine learning problems could be a challenging task. Although these methods may theoretically converge faster, computing exact Jacobians and the per-iteration costs can be expensive. Therefore, it seems natural to introduce inexact approximations of the first-order derivatives. In the context of minimization, several works with inexact Hessians were introduced for both convex  and nonconvex  problems. Regarding VIs, Quasi-Newton (QN) methods  can be highlighted, though they, unfortunately, achieve only local convergence in the strongly monotone case . These methods are relatively less advanced for VIs compared to their counterparts in the field of minimization, where they are considered classics in optimization due to their effectiveness and practicality . Modern research on QN approximations for minimization includes methods that exhibit global convergence . A recent work  introduces the Newton-MinMax method for convex-concave unconstrained min-max optimization problems, demonstrating an optimal rate under special assumptions on the accuracy of the Jacobian approximation. However, the field of VIs lacks globally convergent inexact second-order methods with an explicit dependence on the accuracy of the Jacobian. This raises several natural questions:

_What are the lower bounds for methods with inexact Jacobians?_

_Can we construct an optimal method with inexact first-order information?_

_What is the proper way to approximate the Jacobian to ensure global convergence and reduce the iteration complexity?_

In our work, we attempt to answer these questions in a systematic manner.

Optimality measure.Most of our results are stated for the monotone setting (Assumption 1.1). In this context, the optimality of a point \(\) is typically measured by a gap function \(():_{+}\), defined by

\[()=_{x}\  F(x),-x ,\] (4)

where \( 0\) is the accuracy of solution. The boundedness of \(\) and the existence of a strong solution ensure that the gap function is well-defined. If \(=0\), we get by (1) that \(\) is a weak solution of VI.

We explore the performance of the proposed algorithm in scenarios involving nonmonotone operators \(F\). In such cases, it is essential to assume that the operator satisfies the Minty condition to ensure that the problem is computationally manageable .

**Assumption 1.3**: _The operator \(F(x)\) satisfies Minty condition, if there exists a point \(x^{*}\) such that_

\[ F(x),x-x^{*} 0,x.\] (5)

The range of applications of nonmonotone VIs satisfying Minty conditions is quite extensive . We note, that this condition is weaker than monotonicity  and guarantees the existence of at least one strong solution since \(F\) is continuous and \(\) is closed and bounded . To measure the optimality of point \(\) we define the residue function \(():_{+}\)

\[()=_{x}\  F(),-x ,\] (6)

The boundedness of \(\) and the existence of a strong solution ensure that the residual function is well-defined. If \(=0\), by (2), we get that \(\) is a strong solution of VI.

Contributions.The main contribution of this paper lies in the development of a new second-order method robust to inexactness in the Jacobian, a common occurrence in machine learning. We demonstrate the algorithm's optimality in the monotone case by establishing a lower bound for this key setting. Expanding further:

1. We introduce a novel second-order algorithm, VIJI (Second-order Method for **V**ariational **I**nequalitues under **J**acoibian **I**nexactness), designed to handle \(\)-inexact3 Jacobian information. Specifically, in the context of smooth and monotone VIs, VIJI achieves a convergence rate of \(O(}{T}+D^{3}}{T^{3/2}})\) to find weak solution. For smooth nonmonotone VIs satisfying the Minty condition, we demonstrate a convergence rate of \(O(}{}+D^{3}}{T})\) to identify strong solution. Notably, when \(D}{}\), our method matches the convergence rates of optimal exact second-order methods . 2. We establish the optimal performance of our algorithm on monotone smooth operators by deriving a theoretical complexity lower bound of \((}{T}+D^{3}}{T^{3/2}})\) to find weak solution for the case of \(\)-inexact Jacobians.
3. Our algorithm involves solving a variational inequality subproblem. To tackle this challenge, we introduce an approximation condition, which makes the solution computationally feasible.
4. We introduce a new Quasi-Newton update for approximating the Jacobian, which significantly decreases the per-iteration cost of the algorithm while maintaining a global sublinear convergence rate. Numerical experiments demonstrate the practical benefits of our method.
5. We extend our algorithm for higher-order VIs with inexact high-order derivatives, resulting in \(O(_{i=1}^{p-1}D^{i+1}}{T^{(i+1)/2}}+D^ {p+1}}{T^{(p+1)/2}})\) rate for monotone and smooth VIs with \(_{i}\)-inexact \(i\)-th derivative to find weak solution. Moreover, we extend our proposed high-order method to nonmonotone VIs.
6. We propose a restarted version of VIJI for strongly monotone VIs, which exhibits a linear rate.

Comparison with Lin, Mertikopoulos, and Jordan .To the best of our knowledge, the work  is the most closely related to our research.The objective of  was to develop a method for convex-concave unconstrained min-max optimization under inexact Jacobian information with an optimal convergence rate \(O(^{-2/3})\) matching the lower bound \((^{-2/3})\). The authors successfully achieve this goal by proposing a second-order algorithm Inexact-Newton-MinMaxmethod based on the Perseus . To attain optimal convergence, they constrained the Jacobian inexactness with a function that decreases as the method converges and bounded the norm of Jacobian from above. While these assumptions might be suitable for randomized sampling in finite-sum and stochastic problems, they may not hold for many approximation strategies, such as Quasi-Newton algorithms. The aim of our work, however, is to study the impact of Jacobian inaccuracy on the convergence of second-order methods for VIs (a special case of which are min-max problems) and to identify the explicit dependence of the convergence rate on the inexactness. Compared to the work , the Jacobian inaccuracy directly affects the step sizes in our algorithm, allowing us to achieve a convergence rate of \(O(^{-2/3}+^{-1})\) for any given \(\). VIJI can be viewed as a generalization of Inexact-Newton-MinMax. With the same assumption on \(\) as in , our methods for min-max optimization are equivalent. The inexactness of the subproblem and the solution approach proposed in  remain valid for our method even with arbitrary large \(\). Further details about application of our method to min-max problems can be found in Appendix J.

## 2 Preliminaries

Notation.Let \(^{d}\) be a finite-dimensional vector space with scalar product \(,\). For vector \(x^{d}\) we denote Euclidean norm as \(\|x\|\). For \(X^{d_{1} d_{P}}\), we define

\[X[z^{1},,z^{p}]=_{1 i_{j} d_{j},1 j p}(X_{i_{1}, ,i_{p}})z_{i_{1}}^{1} z_{i_{p}}^{p},\]

and \(\|X\|_{}=_{\|z^{i}\|=1,1 j p}X[z^{1},,z^{p}]\). Fixing \(p 1\) and letting \(F:^{d}^{d}\) be a continuous and high-order differentiable operator, we define \(^{(p)}F(x)\) as the \(p^{}\)-order derivative at a point \(x^{d}\). To be more precise, letting \(z_{1},,z_{k}^{d}\), we have

\[^{(k)}F(x)[z^{1},,z^{k}]=_{1 i_{1},,i_{k} d} (}}{ x_{i_{2}} x_{i_{k}}}( x))z_{i_{1}}^{1} z_{i_{k}}^{k}.\]

Taylor approximation and oracle feedback.The starting point for our method is the first-order Taylor polynomial of the operator \(F\) at point \(v:_{v}(x)=F(v)+ F(v)[x-v]\). Since the computation of Jacobian \( F(v)\) could be a quite tiresome task, it seems natural to introduce an inexact approximation \(J(v)\). Based on it we introduce inexact Taylor approximation - one of the main building blocks of our algorithm

\[_{v}(x)=F(v)+J(v)[x-v], v^{d},\] (7)

where \(J(x)\) satisfies the following assumption.

**Assumption 2.1**: _For given \(v\)\(\)-inexact Jacobian satisfies:_

\[\| F(v)-J(v)\|.\] (8)

As it was shown, e.g. in , Assumption 1.2 allows to control the quality of approximation of operator \(F\) by its Taylor polynomial

\[\|F(x)-_{v}(x)\|}{2}\|x-v\|^{2}, x,v.\] (9)

The next lemma is counterpart of (9) for the case of inexact Jacobian.

**Lemma 2.2**: _Let Assumptions 1.2 and 2.1 hold. Then, for any \(x,v\)_

\[\|F(x)-_{v}(x)\|}{2}\|x-v\|^{2}+\|x-v\|.\]

## 3 VIJI algorithm

In this section, extending on recent optimal high-order method for MVIs Perseus , we present our proposed method, dubbed as VIJI and detailed in Algorithm 1.

The model of objective and subproblem's solution.We begin the description of the algorithm by introducing the inexact model of objective \(_{v}^{}(x)\)

\[_{v}^{}(x)=_{v}(x)+(x-v)+5L_{1}\|x-v\|(x-v),\] (10)

where \(>0\) is given constant. Here, we introduced the additional regularization term \((x-v)\). As we will demonstrate, this term is crucial for ensuring that the method's subproblem has a solution. For brevity, we use a regularization constant of \(5L\). Using larger coefficients would yield the same convergence rate. As other dual extrapolation-type methods, VJI includes the following subproblem

\[x_{k+1}_{v_{k+1}}^{}(x_{k+1}),x-x_{k+1}  0x.\] (11)

First of all, the strong solution of this VI exists because \(_{v_{k+1}}^{}(x)\) is continuous, and \(\) is a closed, bounded, and convex set. Next, we demonstrate that in a monotone setting VI (11) is also monotone.

**Lemma 3.1**: _Let Assumptions (1.1), (1.2), (2.1) hold. Then for any \(x,v_{k+1}\) VI (11) is monotone_

\[(_{v}(x)+_{v}(x)^{T}) 4 L_{1}\|x-v\|I_{d d}+5L_{1}}{\|x-v\|}+(-1) I _{d d}.\]

Following the work , one can find a strong solution to such VI using mirror-prox methods from , achieving the following approximate condition

\[_{x}_{v_{k+1}}(x_{k+1}),x_{k+1}-x }{2}\|x_{k+1}-v_{k+1}\|^{3}+\|x_{k+1}-v_{k+1}\|^{2}.\] (12)

This ensures that the subproblem is computationally solvable in the monotone setting. In specific cases, such as minimax optimization, other efficient subsolvers can be employed .

Adaptive dual stepsizes.Adaptive stepsizes in dual space \(_{k}\) are another core aspect of the algorithm. Due to inaccuracies in the Jacobian, applying the standard adaptive strategy, such as in the Perseus algorithm , can lead to excessively large steps, potentially slowing down the method. To address this issue, an additional term \(_{k}\) is incorporated into the adaptive strategy for selecting \(_{k}\). Thus, when \(\|x_{k}-v_{k}\|\) is small (indicating proximity to the optimum), \(_{k}\) has a greater influence on the choice of \(_{k}\), preventing the method from taking overly aggressive steps. Similar behavior can be observed in accelerated second-order methods for minimization with inexact Hessians from theoretical [3, Lemma 5], [4, Appendix B, Lemma E.3] and practical [4, Section 7] perspectives.

Convergence in monotone setting.Now, we are prepared to present the convergence theorem of Algorithm 1 in the monotone case.

**Theorem 3.2**: _Let Assumptions 1.1, 1.2, 2.1 hold. Then, after \(T 1\) iterations of Algorithm 1 with parameters \(_{k}=,\ =10,\ =0\), we get the following bound_

\[(_{T})=_{x}\, F(x),_{T} -x=O(D^{3}}{T^{3/2}}+}{T}).\] (13)

The upper bound (13) consists of two terms. The first one corresponds to exact convergence and matches the lower bound for second-order VI methods . The second term illustrates the impact of the Jacobian's inexactness on the convergence rate, aligning with the lower bound for first-order methods . We also notice that one can make the bound from (13) tighter for the so-called restricted gap-function  defined as \((y)=_{x} F(x),y-x\), where \(\) and the solution set \(^{*}\) of (2) satisfies \(^{*}\). In particular, following similar steps as in the proof of Theorem 3.2, one can derive \(O(^{3}}{T^{3/2}}+^{2}}{T})\) bound for \((_{T})\), where \(=_{x}\|x-x_{0}\|\).

In the next theorem, we show that the method can achieve the optimal convergence rate of second-order methods under additional assumption on \(\).

**Theorem 3.3**: _Let Assumptions 1.1, 1.2, 2.1 hold. Let \(\{x_{k},v_{k}\}\) be iterates generated by Algorithm 1 and_

\[\|( F(v_{k})-J(v_{k}))[x_{k}-v_{k}]\|_{k}\|x_{k}-v_{k}\|, _{k}}{2}\|x_{k}-v_{k}\|.\] (14)

_Then, after \(T 1\) iterations of Algorithm 1 with parameters \(_{k}=}{2}\|x_{k}-v_{k}\|,\ =10,\ =0\), we get the following bound_

\[(_{T})=_{x} F(x), _{T}-x O(D^{3}}{T^{3/2}}).\]

Note, that condition (14) is verifiable, indicating that the method can adjust to \(_{k}\) in cases of controllable inexactness. Specifically, at each iteration, we can solve subproblem (12). If the assumption regarding \(_{k}\) is not met, we can improve Jacobian approximation and repeat procedure. Moreover, similarly to the previous theorem, one can tighten the above bound for \((_{T})\) and get the dependence on \(=_{x}\|x-x_{0}\|\) instead of \(D\).

Convergence in nonmonotone setting.To begin with, in the nonmonotone case, the subproblem (11) may not exhibit monotonicity, and solving (12) becomes challenging . Yet, in certain specific scenarios, such as unconstrained minimization tasks, it remains feasible to find a solution by leveraging the cubic structure of the subproblem . The following theorem establishes the convergence of VIJI in the nonmonotone setting.

**Theorem 3.4**: _Let Assumptions 1.2, 1.3, 2.1 hold. Then after \(T 1\) iterations of Algorithm 1 with parameters \(_{k}=,=10,=2\) we get the following bound_

\[()=_{x} F(_{T}),_{T}-x=O(D^{3}}{T}+}{ }).\]

The convergence rate could be improved by additional assumption on inexact Jacobian.

**Theorem 3.5**: _Let Assumptions 1.2, 1.3, 2.1 hold. Let \(\{x_{k},v_{k}\}\) be iterates generated by Algorithm 1 that satisfy (14). Then after \(T 1\) iterations of Algorithm 1 with parameters \(_{k}=\|x_{k}-v_{k}\|,\ =10,=2\) we get the following bound_

\[()=_{x} F(_{T}),_{T}-x=O(D^{3}}{T}).\]

## 4 The lower bound

In this section, we establish a theoretical lower bound for the complexity of first-order algorithms using inexact Jacobians for monotone MVIs. The proof technique draws inspiration from works  and based on lower bounds from .

We start by describing the available information and the method's structure. The considered class of algorithms relies on data provided by a first-order \(\)-inexact oracle, denoted as \(:^{d}^{d d}\). Given a point \(\), the oracle returns

\[()=(F(),J()), holds.}\] (15)

The method is able to generate points \(\{x_{k}\}_{k 0}\) that satisfy the following condition

\[s(F(x_{0}),,F(x_{k})),= *{argmax}_{x}\{ s,x-x_{0}-\|x-x_{0}\|^{2}\},\\ x_{k+1}_{}(x_{k+1}-x_{k}),x-x_{k+1}  0x,\]

where \(_{}(h)=a_{1}F()+a_{2}J()[h]+b_{1}h+b_{2}\|h\|h\).

Next, we state the primary assumption concerning the method's ability to generate new points.

**Assumption 4.1**: _The method generates a recursive sequence of iterates \(\{x_{k}\}_{k 0}\) that satisfies the following condition: for all \(k 0\), we have that \(x_{k+1}\) satisfies that \(_{}(x_{k+1}-x_{k}),x-x_{k+1} 0\) for all \(x\), where_

\[=*{argmax}_{x}\{ s,x-x_{0}- \|x-x_{0}\|^{2}\}s(F(x_{0}),,F(x_{k})).\]

As highlighted in , Assumption 4.1 is suitably satisfied by various dual extrapolation methods. However, it might not be applicable to alternative methods for variational inequalities, such as extragradient methods and their variants. We leave the generalization of inexact lower bounds for these algorithms to future research, as even lower bounds for exact algorithms  do not address this case. Now, let us introduce the generalization of smoothness

**Assumption 4.2**: _The operator \(F(x)\) is \(i\)-th-order \(L_{i}\)-smooth (\(i 0\)), if it has Lipschitz-continious \(i\)-th-order derivative_

\[\|^{i}F(x)-^{i}F(y)\|_{} L_{i}\|x-y\|,x,y.\] (16)

Finally, we present the lower bound theorem for first-order methods with inexact Jacobians.

**Theorem 4.3**: _Let some first-order method \(\) satisfy Assumption 4.1 and have access only \(\)-inexact first-order oracle 15. Assume the method \(\) ensures for any \(L_{0}\)-zero-order smooth and \(L_{1}\)-first-order smooth monotone operator \(F\) the following convergence rate_

\[() O(1)\{}{_{1}(T)}; D^{3}}{_{2}(T)}\}.\] (17)

_Then for all \(T 1\) we have_

\[_{1}(T) T,_{2}(T) T^{3/2}.\] (18)

## 5 Quasi-Newton Approximation

In this section, inspired by Quasi-Newton (QN) methods for Hessian approximation, we propose QN approximations for Jacobians. Our goal is to create a simple scheme to approximate the first-order derivative and thereby reduce the complexity of the subproblem. We compute \(J_{x}\) using a QN update and use it as an inexact Jacobian in the model \(_{}^{}(x)\) (10).

\[J_{x}=J^{r}=J^{0}+_{i=0}^{r-1}c_{i}u_{i}v_{i}^{}=J^{0}+U^{}CV,\] (19)

where \(r\) is a rank of approximation, \(J^{0}^{d d} 0,u_{i}^{d},v_{i} ^{d}\) are known. \(U^{r d}\) and \(V^{r d}\) are matrices of stacked vectors \(U=[u_{0},,u_{r-1}]\) and \(V=[v_{0},,v_{r-1}]\) and \(C^{r r}\) is a diagonal matrix \(C=([c_{0},,c_{r-1}])\). If \(u_{i}=v_{i}\) the update becomes symmetric.

**L-Broyden** is a non-symmetric variant of QN approximation  of the following form

\[J^{i+1}=J^{i}+-J^{i}s_{i})s_{i}^{}}{s_{i}^{}s_{i}},  i=0,,m-1.\] (20)

In a view of (19), this update is obtained by setting \(u_{i}=y_{i}-J^{i}s_{i}\), \(v_{i}=s_{i}\), \(c_{i}=1/(s_{i}^{}s_{i})\), and the rank \(r=m\) is equal to memory size \(m\).

_Damped_ **L-Broyden** is another option for QN approximation with non-symmetric damped update

\[J^{i+1}=J^{i}+-J^{i}s_{i})s_{i}^{}}{s_{i}^{ }s_{i}}, i=0,,m-1.\] (21)

By choosing \(u_{i}=y_{i}-J^{i}s_{i}\), \(v_{i}=s_{i}\), \(c_{i}=1/((m+1)s_{i}^{}s_{i})\), \(r=m\), we derive this update from (19). We define the matrix \(J^{r}(J^{0},U,V,C)=J^{r}(J^{0},Y,S,C)\), where \(Y\) and \(S\) are formed by stacking the vectors \([y_{0},,y_{m-1}]\) and \([s_{0},,s_{m-1}]\). The matrix \(J^{m}(J^{0},Y,S)\) can be computed for any given pair (\(Y\), \(S\)). Next, we describe two strategies for the choice of \((s,y)\) pairs used in (20), (21).

**QN with operator history** is the well-known classic variant where operator differences are stored:

\[s_{i}=z_{i+1}-z_{i}, y_{i}=F(z_{i+1})-F(z_{i}).\]

This approach is computationally efficient as it does not require additional operator calculations.

**QN with JVP sampling** is based on fast computation of Jacobian-Vector Products (JVP):

\[y_{i}= F(x)s_{i},\]

where \(s_{i}\) are random vectors uniformly distributed on the unit sphere such that \(\|s_{i}\|=1\) and \(s_{0},,s_{m-1}\) are linearly independent. Note, for \(m d\), each \(s_{i}\) is linearly independent with high probability. This approach requires only \(m\) operator/JVP computations per step, which is considerably fewer than the \(d\) JPVs needed for a full Jacobian. Utilizing the current Jacobian information allows to improve the accuracy of the approximation.

In the following theorem, we demonstrate that these approximations satisfy Assumption 2.1 and condition (8) for both the QN with operator history and JVP sampling methods.

**Theorem 5.1**: _Let \(F(x)\) be \(L_{0}\)-zero-order smooth operator. For \(m\)-memory L-Broyden approximation of the Jacobian \(J_{x}=J^{m}\) defined iteratively by (20) with \(0 J^{0} L_{0}I\), we have \((m+2)L_{0}\). For \(m\)-memory Damped L-Broyden approximation \(J_{x}=J^{m}\) of the Jacobian defined iteratively by (21) with \(0 J^{0}}{m+1}I\), the condition \( 2L_{0}\) holds true._

With the primary toolkit for QN approximation in VIs established, we can now discuss efficient way of solving the subproblem (11), which takes the following form

\[y F(x)+(J_{x}+ I+5 L_{1}\|y-x\|I)(y-x),z-y 0z.\] (22)

Let us introduce a parameter \(=\|y-x\|\) for a segment search in \([0;D]\). To solve (22), we consider another problem

\[y_{} A_{}^{-1}F(x)+y_{}-x,z-y_{} 0z,\] (23)

where \(A_{}=J_{x}+(+5L_{1})I\). Problems (22) and (23) are equivalent when \(=\|y_{}-x\|\). The subproblem (23) can be reformulated as minimization problem

\[y_{}=*{argmin}_{y}\{ A_{}^{-1} F(x),y-x+\|y-x\|^{2}\},\] (24)

where (23) is an optimality condition for (24). The goal is to find \(y_{}\) such that \(()=|-\|y_{}-x\|\). As \(()\) is a continuous function of \(\), we can find this solution via bisection segment-search with \(_{2}\) iterations. This ray-search procedure is similar to the subproblem solution for the Cubic Regularized Newton subproblem.

For \(r\)-rank QN approximation \(J^{r}\) from (19), we can effectively compute \(A_{}^{-1}F(x)\), where \(A_{}=J^{r}+(+5L_{1})I=U^{}CV+J^{0}+(+5L_{1} )I=U^{}CV+B\) and \(B=J^{0}+(+5L_{1})I\) by using the Woodbury matrix identity.

\[A_{}^{-1}F(x)=(B+U^{}CV)^{-1}F(x)=B^{-1}F(x)-B^{-1}U^{ }(C^{-1}+VB^{-1}U^{})^{-1}VB^{-1}F(x).\]

For computational efficiency, it is better to choose \(J^{0}\) as a diagonal matrix, then inversion \(B^{-1}\) is computed by \(O(d)\) arithmetical operations, \(C^{-1}\) by \(O(r)\) operations. \(VB^{-1}U^{}\) requires \(O(r^{2}d)\) for classical multiplication and can be improved by fast matrix multiplication. \((C^{-1}+VB^{-1}U^{})^{-1}\) can be computed by \(O(r^{3})\), as an inverse of \(r\)-rank matrix. The rest of the operations are cheaper. Thus, the total number of arithmetic operations is \(O(r^{2}d)\) instead of \(O(d^{3})\) for Jacobian inversion. We need to perform this inversion logarithmic number of times. Therefore, the total computational cost with the segment-search procedure is \((r^{2}d+r^{3}_{2}())\).

## 6 Strongly monotone setting

**Assumption 6.1**: _The operator \(F:^{d}^{d}\) is called strongly monotone if there exists a constant \(>0\) such that_

\[ F(x)-F(y),x-y\|x-y\|^{2},x,y .\] (25)

To leverage the strong monotonicity of the objective function and achieve a linear convergence rate, we introduce the restarted version of Algorithm 1 dubbed as VIJI-Restarted. Restart techniques are widely utilized in optimization and typically preserve optimality. In other words, restarting an optimal method for convex functions or monotone problems effectively transforms it into an optimal method for strongly convex or strongly monotone problems. Within iteration of VIJI-Restarted listed as Algorithm 2, we execute VIII for a predefined number of iterations (26). Next, the output of this run is used as initial point for next run of VIJI with parameters reset, and this iterative process continues.

**Theorem 6.2**: _Let Assumptions 1.2, 2.1, 6.1 hold. Then the total number of iterations of Algorithm 2 to reach desired accuracy \(\|z_{s}-x^{*}\|\), where \(x^{*}\) is the solution of (2) is_

\[O((D}{})^{}+( +1)[]).\]

## 7 Tensor generalization

In this section, we generalize the results presented in Section 3 to the \(p\)-th order case. We consider a higher-order method with inexact high-order derivatives, satisfying the following assumption.

**Assumption 7.1**: _For all \(x,v,\ i 1\), \(i\)-th inexact derivative of \(F\), which we denote as \(G_{i}\), satisfies_

\[\|(^{i}F(v)-G_{i}(v))[x-v]^{i-1}\|_{i} \|x-v\|^{i-1}.\] (27)

Based on inexact \((p-1)\)-th-order Taylor approximation \(_{p,v}(x)=F(v)+_{i=1}^{p-1}^{i}G_{i}(v)[x-v]^{i}\), we introduce the inexact tensor model \(_{p,v}(x)\) of the objective

\[_{p,v}(x)=_{p,v}(x)+_{i=1}^{p-1}_{i}}{i!} \|x-v\|^{i-1}(x-v)+}{(p-1)!}\|x-v\|^{p-1}(x-v).\] (28)

The tensor generalization of Algorithm 1, referred to as VIHI (High-order Method for **V**ariational **I**nequalities under **H**igh-order derivatives **I**nexactness ) and detailed in Appendix G, involves the inexact solution of the subproblem, which satisfies the following condition:

\[_{x}_{v_{k+1}}(x_{k+1}),x_{k+1}-x }{p!}\|x_{k+1}-v_{k+1}\|^{p+1}+_{i=1}^{p-1} }{i!}\|x_{k+1}-v_{k+1}\|^{i+1}.\]

Another difference in VIHI compared to VIJI (Algorithm 1) is the adaptive strategy for \(_{k+1}\):

\[_{k}(}{p!}\|x_{k+1}-v_{k+1}\| ^{p+1}+_{i=1}^{p-1}}{i!}\|x_{k+1}-v_{k+1}\|^{i+1}) .\]

The other steps of Algorithm 1 remain unchanged for the higher-order method. Now, we are ready to present the convergence properties of VIHI.

**Theorem 7.2**: _Let Assumptions 1.1, 4.2 with \(i=p-1\), and 7.1 hold. Then, after \(T 1\) iterations of VIHI with parameters \(_{i}=5p,\ =0\), we get the following bound_

\[(_{T})=_{x}\  F(x),_{T}-x  O(D^{p+1}}{T^{}}+_{i=1}^{p-1} D^{i+1}}{T^{}}).\]

Finally, we extend our tensor generalization to nonmonotone case and obtain the following result.

**Theorem 7.3**: _Let Assumptions 1.3, 4.2 with \(i=p-1\), and 7.1 hold. Then after \(T 1\) iterations of VIHI with parameters \(=5p,\ =2\) we get the following bound_

\[():=_{x}\  F(_{t}),_{t}-x =O(D^{p+1}}{T^{}}+_{i=1}^{p-1}D^{i+1}}{T^{}}).\]Experiments

In this section, we present numerical experiments to demonstrate the efficiency of our proposed methods. We consider the cubic regularized bilinear min-max problem of the form:

\[_{x^{d}}_{y^{d}}f(x,y)=y^{}(Ax-b)+\|x\|^{3},\]

where \(>0\), \(b=[1,0,,0]^{d}\), and \(A^{d d}\), with all \(1\) on the main diagonal and all \(-1\) on the upper diagonal, the rest elements are \(0\). To reformulate it as variational inequality, we define \(F(x)=[_{x}f(x,y),-_{y}f(x,y)]\). This problem is inspired by the first-order lower bound function for variational inequalities and min-max problems and is commonly used to verify the convergence of high-order methods for VI .

We implement our second-order method for **V**ariational **I**nequalities with **Q**uasi-Newton **A**pproximation (VIQA) as a PyTorch optimizer. The code is available in the OPTAMI package4. VIQA Broyden refers to L-Broyden approximation (20) and VIQA Damped Broyden to (21), which are used as inexact Jacobians in VIJI (Algorithm 1). We compare them with the Extragradient method (EG) , first-order Perseus (Perseus1), and second-order Perseus with Jacobian (Perseus2).

In Figure 1, one can see that second-order information in Perseus2 significantly accelerates the convergence compared to Perseus1. However, it is expensive to compute Jacobian and solve second-order subproblems every iteration. VIQA with the proposed Damped Broyden approximation (21) is significantly faster than EG, Perseus1, and VIQA Broyden (20). It shows that this approximation improves the convergence of first-order Perseus1 and confirms the theoretical result that Damped Broyden is a more accurate approximation than classical Broyden from Theorem 5.1. The detailed parameters and setup are presented in Appendix I.

## 9 Conclusion

In this work, we introduced a second-order method specifically designed to handle Jacobian inexactness. We demonstrated its optimality in the monotone case by introducing a new lower bound and extended its applicability to tensor methods. However, similar to other high-order methods with global convergence properties, our algorithm involves a subproblem that necessitates an additional subroutine for its solution. To address this challenge, we proposed a computationally feasible criterion for solving the subproblem and implemented Quasi-Newton approximations for Jacobians, resulting in a significant reduction in per-iteration cost. Future investigations could explore incorporating inexactness within the operator itself and developing adaptive schemes to dynamically adjust for the level of inexactness encountered during the optimization process. Another open problem is a design of more accurate Quasi-Newton approximations specifically for Jacobians, focusing on non-symmetrical structure of Jacobian and specific inexactness criteria such as Assumption 2.1.

Figure 1: Comparison of different methods for \(d=50,=1e-3\).

#### Acknowledgments.

The work was supported by MIPT based Center of National Technology Initiatives in the field of Artificial Intelligence for the purposes of the "road map" of Artificial Intelligence development up to 2030 and supported by NTI Foundation (agreement No.70-2021-00207 dated 22.11.2021, identifier 0000008507521QYL0002).