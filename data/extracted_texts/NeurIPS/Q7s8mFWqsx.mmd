# Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training

Haoran He\({}^{1}\)  Chenjia Bai\({}^{2,4}\) Ling Pan\({}^{1}\)  Weinan Zhang\({}^{3}\)  Bin Zhao\({}^{4}\)  Xuelong Li\({}^{2,4}\)

\({}^{1}\)Hong Kong University of Science and Technology

\({}^{2}\)Institute of Artificial Intelligence (TeleAI), China Telecom

\({}^{3}\)Shanghai Jiao Tong University \({}^{4}\)Shanghai Artificial Intelligence Laboratory

Correspondence to: Chenjia Bai (baicj@chinatelecom.cn).

###### Abstract

Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. However, it remains a challenge due to the domain gap between humans and robots. Moreover, it is difficult to extract useful information representing the dynamic world from human videos, because of its noisy and multimodal data structure. In this paper, we introduce a novel framework to tackle these challenges, which leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning with a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior performance. Our project webpage is available at https://video-diff.github.io/.

## 1 Introduction

How do we derive a general-purpose robot agent that can complete a wide variety of tasks? We believe that recent advances in vision and language give us a clue, which delves into pre-training foundation models on extremely large and diverse datasets, followed by fine-tuning on specific domains. For instance, through pre-training on internet-scale datasets , large language models  and vision models  showcase impressive performance on various downstream tasks such as question answering, coding, and image generation. However, unlike general visual language tasks that can exploit copious amounts of data available on the Internet, embodied tasks necessitate high-quality egocentric data in robotics domains for precise control. Collecting such data can be expensive or time-consuming due to the reliance on robot interactions through teleoperation or kinematic solvers , and significant gaps in embodiments and dynamics persist when applying them to different robots.

In contrast to the limited availability of robot data, there is a wealth of human interaction videos capturing intricate tasks and varied interactions with the physical world . These videos inherentlyencapsulate rich semantic information regarding objects, environmental backgrounds, and hand-object interactions across diverse scenarios, making them potentially valuable for acquiring shareable knowledge relevant to embodied tasks.

Motivated by this, many works have emerged to learn various objectives pre-trained on human actionless videos, aiming to capture useful knowledge that can be beneficial for embodied tasks. These approaches involve learning pre-trained image representations [61; 57; 68; 91], trajectory representations [4; 85; 74], reward functions [13; 56] and world models [59; 89]. However, they are still limited to comprehending the dynamic rules of the world or reasoning based on long-term behavior rather than relying solely on step-by-step transitions. We summarize three main challenges that bottleneck their performance: (i) The domain gap between humans and robots which hinders knowledge transfer; (ii) Complex, diverse and noisy behavior patterns hidden in human videos which are difficult to learn; (iii) Large-scale data from different modalities (e.g., videos, actions, texts) which requires a scalable and high-expressive model architecture to process.

To address these challenges, we propose a Video-based Policy learning framework via Discrete Diffusion (VPDD). VPDD bridges the visual gap between the human and robot domains by representing these two data types as unified latent representations. Then, VPDD performs video prediction as a _pre-training stage_ with actionless videos, which acquires the commonsense knowledge shared between human and robot interactions, including dynamic rules and behavior patterns (e.g., pick, place, push) to understand and complete tasks. Then, VPDD performs policy learning via a _fine-tuning stage_ with action-labeled robot videos, where VPDD learns to predict actions with foresight from future video predictions. The pre-training stage learns extensive knowledge from human video prediction, and the fine-tuning stage concentrates on training parameters specifically associated with actions. To tackle the challenge of modeling the noisy and complex distribution of large-scale videos while enabling the multi-modal generation of both videos and actions, we leverage the generative capability and flexible architecture offered by discrete diffusion models [2; 29]. We provide an overview of our method in Fig. 1.

In summary, we highlight our contributions as follows. (i) We propose VPDD, a novel pretraining-finetuning paradigm for learning an actionable policy with limited robot data accessible. This paradigm demonstrates superior ability in transferring valuable knowledge from large-scale actionless human videos to downstream embodied tasks. (ii) We formulate both video prediction and action learning processes as unified discrete denoising problems, showing the supreme effectiveness in handling high-dimensional, multi-modal data. (iii) We conduct thorough experiments using human videos from Ego4D , as well as embodied datasets from Meta-World  and RLBench , showcasing its ability to predict dynamic-consistent future videos. Our actionable discrete diffusion policy also exhibits superior performance compared to previous state-of-the-art approaches [32; 61; 24; 75; 15], encompassing both seen and unseen scenes for multi-task robotic problems.

## 2 Preliminaries

### Multi-Task POMDP

In this work, we consider a generalist vision-based agent that is capable of addressing multi-task predicaments, where the landscape is characterized by the inherent challenge of acquiring different skills across tasks and partial observability when dealing with image inputs. Given a specific task \( p()\), we further approach the problem as a task-specified Partially Observable Markov Decision Process (POMDP), defined as \((^{},,,^{}, ^{},^{},)\). Here, \(\) is a shared observation space as we use image observations for all tasks. We also assume all tasks share the same action space with the same embodiment.

Figure 1: Overall framework of VPDD.

### Vector Quantized Model

In order to unify the feature space of both human videos and robot videos, we leverage the Vector Quantized Variational Auto Encoder (VQ-VAE)  to compress high-dimensional data points into information-rich discretized latent codes. Given a high-dimensional video segment \(^{T H W C}\), the encoder \(E\) first converts it to the temporal-spatial features \(=E()=\{z_{m,i,l}\}^{t h w d}\), where \(t h w\) represents the encoded sequence length and is much smaller than \(T H W\). Then we transfer the continuous features into discrete space by performing a nearest neighbors lookup in a codebook of embeddings \(=\{e_{j}\}_{j=1}^{J}^{J d}\) to obtain the tokens

\[z_{q}=(z_{m,i,l}):=_{J}\|z_{m,i,l}-e_{j}\|_{2}^{2},\] (1)

where the video tokens \(_{q}^{t h w d}\) can be faithfully reconstructed via a decoder, i.e., \(}=G(_{q})\). The encoder \(E\), decoder \(G\), and codebook \(\) can be trained end-to-end via the following loss function \(=\|-}\|_{1}+\|[E()]-_{q}\| _{2}^{2}+\|[_{q}]-E()\|_{2}^{2}\), where \(\) denotes stop gradient.

### Discrete Diffusion Model

The discrete diffusion model was first proposed to deal with discrete state space with transitions converging to a binomial distribution , and then extended to multinomial diffusion with more options for transition matrices [36; 2]. In this work, we utilize discrete diffusion with the absorbing state for sequence prediction of discrete tokens. Besides \(J\) tokens from a codebook, an additional [MASK] token is introduced. We denote \(_{k}\) as a one-hot vector identifying the token index. The forward process from \(_{k-1}\) to \(_{k}\) follows a Categorical distribution of \(_{k}_{k-1}\), as

\[q(_{k}|_{k-1})=(_{k};p=_{k}_{k-1})= _{k}^{T}_{k}_{k-1},\] (2)

where \([_{k}]_{m,n}=q(_{k}=m|_{k-1}=n)^{(J+1)(J +1)}\) is the Markov transition matrix from \(k-1\) to \(k\), which is formulated as:

\[_{k-1 k}=(_{k}+_{k}&_{k}& _{k}&&0\\ _{k}&_{k}+_{k}&_{k}&&0\\ _{k}&_{k}&_{k}+_{k}&&0\\ &&&&\\ _{k}&_{k}&_{k}&&),\] (3)

where \(_{k}\) is the probability of retaining the token, and each ordinary token has a probability of \(_{k}\) to be replaced by [MASK] token, leaving a chance of \(_{k}=(1-_{k}-_{k})/J\) to be diffused. Importantly, due to the property of the Markov chain, we can derive the probability of \(_{k}\) at arbitrary timestep directly from \(_{0}\) as

\[q(_{k}|_{0})=_{k}^{T}}_{k}_{0},\ }_{k}=_{k}_{1}.\] (4)

Besides, the posterior of this diffusion process is tractable as \(q(_{k-1}|_{k},_{0})=_{k}|_{k-1},_{ 0})q(_{k-1}|_{0})}{q(_{k}|_{0})}=_{k}^{T} _{k}_{k-1})(_{k-1}^{T}_{k-1}_{0})}{_{k}^{T} _{k}_{0}}\). In the reverse process, rather than explicitly predicting the posterior through a denoising neural network, the \(_{0}\)-parameterisation enhances stability and allows for fast inference (by skipping \( k\) steps per iteration). The reverse transition with reparameterisation is formulated as

\[p_{}(_{k-1}|_{k})=_{}_{0}}q(_{k-1}|_{k},}_{0})p_{}(}_{0}|_{k}),\] (5)

where the neural network predicts the logits of the target data \(q(_{0})\).

## 3 Methodology

We commence with the pre-training of our model through future video prediction, enabling the learning of a general dynamic pattern across diverse domains. Subsequently, we fine-tune the model using a limited dataset of robot data for policy learning, leveraging foresight from predicted videos. Our framework is illustrated in Figure 2.

### Data Preparing and Tokenizing

**Robot Data Collection.** We use the rule-based script policy to rollout 20 expert demonstrations for each task in Meta-World . We also run VPDD on 16 tasks from RLBench , a more challenging benchmark involving 6-Dof manipulation that necessitates multi-view images from a 3D scene to select actions. Following the multi-view manipulation frameworks , we utilize the script motion-planner to collect 10 demonstrations for each task. Each demonstration from robot-data is formulated as \(_{i}=\{v_{1},a_{1},,v_{t},a_{t},,v_{T},a_{T}\}\) with \(v_{t}=[o_{t-I+1},,o_{t-1},o_{t}]\), where we set \(I=4\) throughout the paper and \(a\) denotes the actions. In the context of Meta-World, \(o_{t}\) represents a single-view RGB image at timestep \(t\). For RLBench, \(o_{t}=\{o_{t}^{},o_{t}^{},o_{t}^{},o_{t}^ {}\}\) comprises 4 RGB multi-view images (i.e., front, left shoulder, right shoulder, and wrist). Consequently, in RLBench, \(v_{t}\) is formulated as \(v_{t}=\{v_{t}^{},v_{t}^{},v_{t}^{},v_{t}^ {}\}\).

**Human Data Collection.** As for human data collection, we obtain untrimmed videos from the open-sourced Ego4D dataset , which contains massive-scale human-object interactions of various durations ranging from 5 seconds to 7 hours. We filter out videos without human-object interaction and segment each video into short clips with 8-frame intervals . Thus each video is represented as \(_{i}=\{v_{1},v_{2},,v_{n}\}\), where \(v_{t}\) denotes a clip containing 4 frames. This approach yields a total of 996,177 clips of human videos, comprising approximately 4M frames. More details on data collection and processing are given in SSC.1.

**VQ-VAE Encoding.** To extract useful features from raw videos in both human and robot domains, a conventional approach is to directly encode them into an embedding space using pre-trained vision models like ViT. However, these models are usually specifically trained on image dataset , posing a significant challenge due to the domain gap with our interaction videos. Thus, we leverage VQ-VAE to compress the diverse and noisy videos into discrete latent codes, which provide a unified codebook for mixed videos and alleviate the domain gaps between human and robot videos. Formally, we adopt the VQ-VAE architecture introduced by VideoGPT  for encoding videos into a discrete latent space. The codebook comprises 2048 codes, each represented by 256-dimensional embeddings. The encoder architecture consists of a series of 3D convolutions that downsample by a factor of 4 over space-time (resulting in a \(64\) total reduction), followed by 6 attention residual blocks. Consequently,

Figure 2: **Overall pipeline of VPDD. A video-based VQ-VAE is leveraged to encode both human and robot videos into discrete latent codes. Subsequently, a unified discrete diffusion is firstly pre-trained on these video latent codes via a self-supervised objective, predicting future videos conditioning on language instructions and historical videos. The pre-trained video prediction model \(p_{_{1}}\) can capture temporal dynamics and task-specific representations. Lastly, we fine-tune our diffusion model on a limited number of robot data. In each diffusion step of the fine-tuning stage, we leverage \(p_{_{1}}\) to provide hidden representations \(z_{}_{V}^{}}\) to benefit downstream action learning with video foresight. This integration of video prediction and action learning is achieved through our unified discrete diffusion.**each video clip \(v_{t}\{_{i}\}\) is embedded into latent codes \(_{t}\). The architecture for the decoder is the reverse of the encoder, featuring attention residual blocks followed by an equivalent number of 3D transposed convolutions for upsampling over space-time. The VQ-VAE is pre-trained on large-scale videos and remains fixed in the subsequent processes, providing flexibility for various downstream utilization methods.

**Action Discretizing.** For subsequent pre-training and fine-tuning, we process the collected continuous actions via uniform action discretization [46; 11]. In the case of Meta-World, the action space is a 2-tuple consisting of the change in the 3D space of the end-effector followed by a normalized torque that the gripper fingers should apply. Here all the continuous dimensions are discretized into 48 bins uniformly. Thus, the robot action can be represented using ordinals of the discrete bins as a 4 integer number. For RLBench, an action consists of the gripper open state and 6-DoF pose including position and rotation. The position is discretized into 360 bins, and rotation is discretized into Euler angles as 1-degree bins for each of the 3 rotation axes . Gripper open state is a binary value.

### Video Prediction via Unified Discrete Diffusion

Extracting general patterns useful for downstream decision-making from large-scale in-the-wild human videos is challenging, primarily because of the absence of labeled actions and the complexity of the underlying structure of human interactions. Different from previous ways of learning a visual representation, we propose a novel objective to further unleash the representation and temporal modeling ability of diffusion models. Specifically, after obtaining discrete tokens from VQ-VAE encoding, we train a unified discrete diffusion model on the latent space via a self-supervised objective. This objective involves predicting future videos based on observed historical videos for both humans and robots, while masking action tokens. Benefiting from the proposed objective and the \(_{0}\)-parameterisation of discrete diffusion, the diffusion model is incentivized to capture both the high-level temporal dynamics and the low-level visual commonalities between historical and future videos at each diffusion step. Then the acquired knowledge can be leveraged to guide action denoising at each step.

**Unified Transition Matrix.** The presence of a transition matrix determines the nature of the discrete diffusion model . While the original discrete diffusion is limited to one modality, drawing inspiration from UniD3 , which enhances the transition matrix to encompass both images and text, we construct a unified transition matrix to capture global connections between the two modalities--videos and actions. The matrix \([_{k}]_{m,n}\) below illustrates the unified transition process:

\[_{k}=[_{k}+_{k}&_{k}& &_{k}&0&0&&0&0\\ _{k}&_{k}+_{k}&&_{k}&0&0&&0&0\\ &&&&&&&\\ _{k}&_{k}&&_{k}+_{k}&0&0&&0&0\\  0&0&&0&a_{k}+_{k}&_{k}^{*}&&_{k}^{*}&0\\ 0&0&&0&_{k}^{*}&_{k}+_{k}^{*}&&_{k}^{*}&0\\ &&&&&&&\\ 0&0&&0&_{k}^{*}&_{k}^{*}&&a_{k}+_{k}^{*}&0\\ _{k}&_{k}&&_{k}&_{k}&_{k}&_{k}& &_{k}&1],\]

where \(_{k}\) and \(_{k}^{*}\) are the probabilities of a token to be replaced by any other accessible tokens in different modalities. The dimension of \(_{k}\) is \((J+J^{*}+1)(J+J^{*}+1)\), where \(J\) and \(J^{*}\) are the number of tokens in different modalities, i.e., \(J\) is the size of codebook in VQ-VAE and \(J^{*}\) is the number of action classes in discretization. The sum of each column in this transition matrix is one to preserve probability mass. Mathematically, we have \(_{k}=(1-_{k}-_{k})/J\) and \(_{k}^{*}=(1-_{k}-_{k})/J^{*}\). All the mass of the stationary distribution falls on the [MASK] token, which satisfies the prerequisite for a discrete diffusion model transition matrix . The details of the diffusion process are provided in SSA.1.

**Unified Objective.** We cast both video prediction and action learning as a conditional generative problem, and the goal is to maximize \(_{_{}}} p_{}(_{0}() (),\). Here \(=[^{},^{}]\), where \(^{}=[_{t+h+1},,_{t+h+M}]\) represents future video segments with \(M\) clips, and \(^{}=[a_{t},,a_{t+H-1}]\) denotes action sequences of \(H\) steps. \(=_{t}\) serves as the condition containing historical video tokens. \(\) is the language instructions describing current tasks. In practice, we train two separate denoising networks, namely \(p_{_{1}}(^{}_{k-1}|_{k},,)\) and \(p_{_{2}}(^{}_{k-1}|_{k},}^{}_ {k},)\), to learn videos and actions, respectively. Here, \(z_{}^{}_{0}}\) represents the hidden representation of predicted future videos given by \(p_{_{1}}\) at each diffusion step, which is utilized to guide action learning. Formally, \(}^{}_{0}=((z_{}^{ }_{0}}))\).

As the actions are absent during the pre-training stage, we mask \(^{ a}\) and freeze \(p_{_{2}}\), as illustrated in Figure 2. The network is trained to minimize the following variational lower bound (VLB) [76; 29]:

\[_{ vlb}=_{0}+_{k=2}^{K}_{k-1 }+_{K},\] (6)

where

\[_{0}=-_{q(_{1}|_{0})}[ p_{_{1} }(_{0}^{ v}|_{1},,)+ p_{_{2}}(_{0}^ { a}|_{1},z_{}_{0}^{ v}},|],\] (7)

\[_{k-1}=_{q(_{k}|_{0})}[D_{ KL}(q(_{k -1}|_{k},_{0})[p_{_{1}}(_{k-1}^{ v}| _{k},,);p_{_{2}}(_{k-1}^{ a}|_{k},z_{}_{0}^{ v}},)])],\] (8)

\[_{K}=_{q(_{0})}[D_{ KL}(q(_{K}| _{0}) p(_{K}))].\] (9)

\(_{K}\) is a constant number that can be ignored in the training, as the prior distribution \(p(_{K})\) is fixed:

\[p(_{K})=[_{K},_{K},,_{K}^{*}, _{K}^{*},,_{K}].\] (10)

**Model Architecture.** As in Eq. (8), the neural network \(p_{_{1}}\) receives \(_{k}\), history \(\), language \(\) and the diffusion timestep \(k\) as inputs. These inputs are individually embedded into embeddings \(h\) of size \(d\) via separate MLPs \(f\), depicted as:

\[h_{l}=f_{l}(()),\;h_{Ti}=f_{Ti}(k),h_{_{k}}=f_{_{ k}}(_{k}),\;h_{y}=f_{y}(),\]

where language instructions \(\) is encoded with CLIP's language encoder . Afterwards, the embeddings are formulated as input tokens as \(h_{ tokens}=(h_{Ti}[h_{l},h_{Ti},h_{_{k}},h_{y}]+E^{  pos})\), where \(E^{ pos}\) is the positional embedding, and \(\) denotes layer normalization  for stabilizing training. The input sequence that represents a video can be extremely long so a standard Transformer with \((n^{2})\) complexity is hard to fit. We adopt Perceiver Transformer  to tackle this problem, as it has been widely utilized for modeling long sequences [75; 24]. Perceiver is a latent-space Transformer, where instead of attending to the entire input, it computes cross-attention between the input and a much smaller set of latent vectors (which are randomly initialized and trained). These latents are encoded with self-attention layers, and are cross-attended with the input to match the size for the final outputs. More details about the Perceiver Transformer are referred to SSA.2.

### Learning to Act via Few-Shot Fine-Tuning

During the fine-tuning stage, we leverage a limited dataset of robot data, including both videos and actions, for rapid adaptation. Both \(^{v}\) and \(^{a}\) attend to training the diffusion model. Given that \(p_{_{1}}\) has been trained sufficiently to capture fruitful information to predict future videos from history, we freeze \(p_{_{1}}\) and solely tune parameters of \(p_{_{2}}\) to minimize \(_{ vlb}\). As expressed in Eq. (8), the input of \(p_{_{2}}\) consists of \(_{k}\), language \(\), hidden representation \(z_{}_{0}^{ v}}\), and diffusion timestep \(k\). In this case, we are tasked with predicting a sequence of action tokens \(_{0}^{ a}\), considerably shorter than video-token sequence \(_{0}^{ v}\), so we employ GPT2  Transformer to process tokens embedded with MLPs. GPT2 has demonstrated an impressive ability to solve multi-task problems and model multimodal distributions. The model architecture of \(p_{_{2}}\) closely resembles that of MTDiff-p . More details of our method can be found in Appendix A.3.

## 4 Related Work

**Robot Learning from Human Videos.** Leveraging human videos [26; 23; 18; 27] for policy learning is promising to extract commonsense knowledge from human activities, which can be shared to embodied scenarios that suffer from scarce robot data [17; 22]. Since the human data is actionless and the domain gap between humans and robots exists, a main branch of research employs human video to learn shareable visual representations [12; 47] via time-contrastive , video-language alignment [61; 48], value function , and perceptual skills [40; 52]. Visual affordance like human-object interaction hotspots [54; 25] and the post-grasp trajectory  are also helpful for embodied agents in goal-conditioned imitation. Alternative methods involve extracting hand trajectories [4; 85] or keypoints  to transfer plans to robots. Different from the above methods, we eliminate the domain gaps by learning video tokens and representations for video prediction, which implicitly captures visual features, affordances, and long-term plans. Other works attempt to infer actions from videos via inverse kinematics [6; 50], whereas we learn action prediction through policy fine-tuning without external models.

**Pretraining for Generalized Policy Learning.** Early works of policy adaptation emerged in meta-RL [30; 99], while the pre-training and fine-tuning environments are assumed to be similar. Leveraging the transformer architecture, works perform pre-training in multi-task datasets by optimizing the multi-task policy [51; 70; 79; 80] or self-supervised objectives [78; 73; 59]. In tasks involving visual observations, methods adopt visual tokens for transformer-based multi-task policy learning [10; 11] and adaptation . Additionally, some studies pre-train a reward function through video-language correspondence [13; 56] or diffusion models [21; 38] for downstream RL training. Different from the previous Transformer and continuous diffusion frameworks, our work first integrates visual tokens with discrete diffusion to predict consistent videos and actions simultaneously. Concurrently, GR-1  utilizes human data to pre-train a GPT-style architecture for predicting future observations. In contrast, we perform video prediction instead of step-by-step image prediction using a unified discrete diffusion architecture.

**Diffusion Models for RL.** Diffusion models are a powerful family of generative models [72; 69] that can be categorized into continuous Gaussian diffusion models and discrete diffusion models that handle discrete visual tokens or symbols . Continuous diffusion models have found extensive applications as multi-modal policies [87; 64; 16], environmental dynamics , and planners to generate action [45; 90] or state sequences [1; 14], guided by desired properties. Several methods also extend the diffusion models for multi-task learning [32; 62; 19] with low-dimensional states, while we specifically address the more challenging image-based setting. UniPi  and its following work  are related to our method by performing video generation via continuous diffusion, while we adopt a more flexible architecture with discrete diffusion to seamlessly connect the pre-training and fine-tuning stages, without relying on a task-specific inverse-dynamic model for acting. Additionally, we train the model on video tokens that maintain temporal consistency, while UniPi relies on super-resolution to improve the time consistency of generated frames.

## 5 Experiments

### Benchmarks and Baselines

After the video pertaining in Ego4D , we use the following robotic benchmarks to evaluate our method.

**Meta-World.** The Meta-World benchmark  contains 50 distinct manipulation tasks that require a Sawyer robot to interact with various objects with different shapes, joints, and connectivity. The action is the 3D position movements of the robot's end effector and the gripper openness. We follow recent works [96; 77] to extend the original environment to a more challenging setting with random goals, and refer to it as MT50-rand. We train the policy with 20 demonstrations per task, and report the average success rates on 50 evaluation episodes per task.

**RLBench.** RLBench  is a more challenging 3D manipulation benchmark with diverse tasks concerning interactions with a wide range of objects. We select 16 tasks from RLBench to evaluate our method, where each task has several possible variations, such as the shapes, colors, sizes and positions of objects. The input observations are captured from four RGB cameras positioned at the front, left shoulder, right shoulder, and wrist. The action is an 8-dimensional vector including 3-dimensional transitions, 4-dimensional quaternion, and a binary value about gripper openness. We follow the convention by using macro steps , which are key turning points in the action trajectory where the gripper changes its state (open/close) or the joint velocities approach to zero. We train the policy with 10 demonstrations per task and report average success rates on 25 evaluation episodes per task.

**Baselines for Meta-World.** We compare the proposed method VPDD with the following baselines: (i) **R3M-Diffusion** is a discrete diffusion model sharing identical architecture with \(p_{_{2}}\), leveraging the R3M  ResNet50 encoder to encode images as input. R3M is also trained on Ego4D videos via a contrastive learning objective and stands as the state-of-the-art (SOTA) visual representation specifically designed for manipulation tasks; (ii) **VC-1-Diffusion** utilizes VC-1  encoder (ViT-L) to extract image representations, which is also trained on large-scale egocentric videos  and ImageNet  using Masked Auto-Encoding . (iii) **MTDiff-P** is the SOTA method for multi-task RL, which employs a continuous diffusion model with a Transformer architecture. Since it is designed to handle state-based input, we employ the R3M encoder to extract hidden embeddings from images, which are then fed into MTDiff-P; (iii) **Video-MTDT** is an extension of Decision Transformer (MT) , learning from multi-task data with video tokens as input; (v) **VPDD-w/o.

**human** excludes human videos during pre-training, remaining other parts unchanged. This baseline helps to ablate the effect of pre-train on large-scale human videos; (vi) **SODA** is a recently proposed diffusion-based representation method that employs an encoder to generate representations \(z\) from input images to support the denoising process. We pre-train the encoder by employing the video prediction objective on the same dataset and subsequently feed the learned \(z\) into \(p_{_{2}}\) during fine-tuning. See more details in SSB.

**Baselines for RLBench.** Learning policies for RLBench is more challenging as it requires understanding the 3D scene structure for predicting the 6D poses of end-effectors. The baselines used in Meta-World all fail in the benchmark since they are disadvantaged with single-view observations. In contrast, VPDD can predict multi-view images, implicitly recovering the 3D geometry in manipulation. Thus, we use the following SOTA imitation architectures designed for 3D manipulation: (i) **RVT** stands as the SOTA method, initially re-rendering visual observations into orthographic projections of cube views and subsequently predicting the next move based on these multi-view projections.; (ii) **PerAct** encodes RGB-D images into voxel grid patches for 3D representation and predicts the action using the Perceiver Transformer.

### Results Analysis

**Question 1.**_Does our pre-trained diffusion model (i.e., \(p_{_{1}}\)) capable of generating dynamic-consistent future videos?_

Although our primary contribution is not about video generation, it remains crucial to predict consistent future videos to aid in policy fine-tuning. The predicted raw videos are visually depicted in Fig. 4, with more video samples including real robots (trained by RoboSet data ) accessible at https://video-diff.github.io. These videos are reconstructed from predicted video tokens by using the decoder of VQ-VAE. After pre-training, the video-prediction model \(p_{_{1}}\) demonstrates the capability to generate dynamically consistent single-view videos for Meta-World and multi-view videos for RLBench. Furthermore, we computed the Frechet Video Distance (FVD) score [34; 86] averaged across frames on 32 generated video samples from the Meta-World dataset. From the results in Table 2, we observe that the FVD score of VPDD is considered acceptable and even lower than the hierarchical video synthesizing method UniPi  (score reported from its original paper). We attribute the

   Method &  & \)} & \)} & \)} & \)} & \)} & \)} \\  & \)sick} & \)sick} & \)sick} & \)sick} & \)sick} & \)sick} \\   PraAct(10-frame)  & 32 & 72 & 68 & 72 & 16 & 32 & 36 & 12 \\ RY(0-frame)  & 67.47 \(\) 1.89 & 76.0 \(\) 3.27 & 67.31 \(\) 4.50 & **96.0 \(\) 3.27** & **91.33 \(\) 6.60** & 27.67 \(\) 4.99 & **97.33 \(\) 1.89** & 5.31 \(\) 1.89 \\ YPDD(Ours) & **70.87\(\) 1.89** & 40.0 \(\) 3.27 & **73.33 \(\) 4.39** & 88.67 \(\) 3.77 & 26.80 \(\) 3.27 & **37.33 \(\) 8.22** & 66.67 \(\) 1.30 & **56.0 \(\) 5.05** \\   & \)} & \)} & \)} & \)} & \)} & \)} & \)} & \)} \\  & \)sick} & \)sick} & \)sick} & \)sick} & \)sick} & \)sick} & \)sick} & \)sick} \\  PraAct(10-frame)  & 20 & 16 & 30 & 35 & 56 & 4 & 0 & 0 \\ RY(0-frame)  & 29.87 \(\) 3.27 & 36.67 \(\) 4.99 & 53.3 \(\) 6.50 & 53.3 \(\) 1.69 & **65.33 \(\) 1.89** & 2.07 \(\) 1.89 & 2.07 \(\) 1.89 & 0.40 \(\) 1.89 \\  VPDD(Ours) & **61.32\(\) 6.80** & **70.67\(\) 1.89** & **60.0 \(\) 6.53** & **30.67 \(\) 4.99** & 56.67 \(\) 1.59 & **73.33 \(\) 1.89** & **56.0 \(\) 5.06** & **30.67 \(\) 1.89** \\   

Table 1: Success rates (mean and std %) across 3 random seeds of various multi-task agents trained with 10 demonstrations and evaluated on 25 episodes per task. VPDD (Ours) outperforms SOTA methods of RLBench, i.e., PerAct and RVT, with an average improvement of \(\). Note that VPDD only takes RGB images as input while both RVT and PerAct utilize additional depth images as inputs.

capability of generating high-quality videos to the well-trained video codebook and the proposed discrete diffusion model learned from large-scale human data.

**Question 2**.: _How does VPDD compare to other offline baselines for vision-based multi-task decision-making?_

To evaluate the learned policy after fine-tuning, we take the first action generated by \(p_{_{2}}\) to interact with the environment. The results on Meta-World and RLBench are referred to Fig. 4 and Table 1 respectively, yielding the following key findings: (i) VPDD outperforms other SOTA methods in success rate by a large margin. For Meta-World, VPDD performs the best across 50 tasks with random goals. For RLBench, VPDD even outperforms the SOTA imitation architectures based on voxel and multi-view representations that are carefully designed for 3D manipulation, which usually require point clouds or 3D world rendering for scene understanding. Notably, VPDD achieves a remarkable success rate on _put in cupboard_, _insert peg_, _stack cups_ and _place cups_, while both RVT and PerAct struggle on these challenging tasks. This verifies the efficacy of video pre-training for few-shot policy fine-tuning; (ii) According to Fig. 4, VPDD obtains a \(6.9\%\) relative improvement through pre-training on both human and robot videos compared with VPDD-w/o.-human. Furthermore, VPDD surpasses R3M and VC-1 with a notable \(16.4\%\) and \(26.5\%\) higher success rate, demonstrating the potential of our diffusion representation via video prediction; (iii) R3M-Diffusion outperforms MTDiff-p which employ R3M encoder with continuous diffusion architecture by \(26.0\%\), and Video-MTDT with Transformer architecture by \(60.6\%\). This highlights the superior capacity of discrete diffusion models compared to other model architectures.

**Question 3**.: _How does VPDD generalize to unseen scenes?_

As suggested in recent works , we evaluate the generalization ability of our model in the two most challenging settings, i.e., camera view and visual background. Specifically, we alter the camera position and table texture in the visual scene of Meta-World to assess the generalization ability of our method. According to Fig. 5, VPDD exhibits superior generalizability, attributed to the training of the diffusion representation on large-scale diverse human videos. Regarding the shift in camera position, VPDD outperforms the _VPDD-w/o.-human_ by \(\%\) and R3M by \(\%\). Moreover, we show that VPDD can generalize on different tasks with a competitive performance, demonstrating its potential to serve as a foundation model. Detailed results and settings can be found in SSC.3.

**Question 4**.: _Can VPDD maintain satisfactory performance when provided with fewer robotic demonstrations?_

Leveraging the large-scale video pre-training, VPDD can learn the policy using only a small number of demonstrations. To validate the sample efficiency of our method, we conduct an ablation study on the number of demonstrations used in the fine-tuning stage. The results, depicted in Fig. 6, reveal that the performance of VPDD exhibits linear growth after training on 5 or more demonstrations, indicating the potential for VPDD to achieve better performance with increased demonstration data. Moreover, VPDD maintains a comparable success rate even when only \(\) demonstration is used in the fine-tuning process.

**Question 5**.: _How does VPDD perform when trained on different amounts of human data?_

The superior generalizability of VPDD, which is validated in Fig. 5, stems from large-scale human-data pertaining, which effectively extracts commonsense

Figure 5: Average success rate across 3 seeds on shifted _button-press-v2_ and _handle-press-v2_ tasks.

Figure 6: Average success rate across 3 seeds on MT50-rand, where VPDD is trained on a different number of demonstrations.

Figure 7: Ablation on the number of human videos during the pre-training stage, where the red curve is evaluated using the same experimental setting as in Fig. 4, and the blue curve corresponds to the setting in Fig. 5.

knowledge and representations that can be shared between human and unseen robot scenarios. To further investigate the effects of human-data pre-training, we ablate the number of human videos used during pre-training and present the performance changes in Fig. 7. Our findings indicate that an increased number of human videos enhances success rates, particularly in improving generalizability by a large margin.

**Question 6**.: _Does VPDD outperforms other diffusion- based representation learning method?_

To verify the representation capability inherent in our unified discrete diffusion framework, we reproduce SODA , which serves as a strong diffusion representation method. As shown in Fig. 4, VPDD outperforms SODA in the context of policy fine-tuning. We hypothesize that VPDD provides coarse-to-fine representations (i.e., \(z_{}_{0}^{}}\)) throughout the action-denoising steps from \(K 1\), exactly encapsulating useful information the denoising network focuses on at each step . In contrast, SODA produces representation \(z\) that remains constant across all steps during action denoising.

## 6 Conclusion

We propose VPDD, a video-based policy learning framework via discrete diffusion. With a VQ-VAE tokenizer, we bridge the gap between human and robot videos by a discrete latent codebook. We leverage a unified discrete diffusion for pre-training on large-scale actionless mixture videos and subsequent fine-tuning the policy on a limited number of robot demonstrations. Experiments demonstrate that VPDD achieves superior performance on a variety of challenging manipulation tasks and showcases impressive generalization ability benefited from human video prediction. More Discussions of our work are given in SSD.