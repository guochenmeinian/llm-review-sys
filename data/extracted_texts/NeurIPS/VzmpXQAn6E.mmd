# Exposing Attention Glitches with

Flip-Flop Language Modeling

 Bingbin Liu\({}^{1}\)  Jordan T. Ash\({}^{2}\)  Surbhi Goel\({}^{3}\)  Akshay Krishnamurthy\({}^{2}\)  Cyril Zhang\({}^{2}\)

\({}^{1}\)Carnegie Mellon University  \({}^{2}\)Microsoft Research NYC  \({}^{3}\)University of Pennsylvania

bingbinl@cs.cmu.edu, surbhig@cis.upenn.edu,

{ash.jordan, akshaykr, cyrilzhang}@microsoft.com

###### Abstract

Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of _attention glitches_, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce _flip-flop language modeling_ (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that TransformerFFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs.

## 1 Introduction

Recent advancements in scale have yielded large language models (LLMs) with extraordinary proficiency in nuanced reasoning with factual knowledge. Despite these achievements, LLMs are known to produce incorrect outputs, often referred to colloquially as "hallucinations" or "distractions" (Ji et al., 2023). Generally, hallucinations refer to the phenomenon that a model's outputs are syntactically and grammatically accurate but factually incorrect. There are various types of hallucinations, and the focus of this work is the "closed-domain" variety (Saparov and He, 2022; OpenAI, 2023), where the model predictions contain factually incorrect or made-up information _according to a given context_, regardless of their correctness in the real world.

Perhaps surprisingly, such hallucinations can be observed even on simple algorithmic reasoning tasks. As a warmup, consider the queries shown in Figure 1 (and Appendix B.1), where we prompt LLMs to solve addition problems of various lengths. The responses simultaneously illustrate the following:

1. _Nontrivial algorithmic generalization:_ In cases where the models succeed, it is unlikely that these exact numerical sequences appeared in the training data. To correctly output the first digit of the answer, the LLM must resolve a long dependency chain which generally depends on every digit in the input. Somewhere within these networks' internal representations, implementations of addition algorithms have emerged.

2. _Sporadic errors ("hallucinations"):_ These internal algorithms can be brittle and unreliable, especially when processing long inferential chains. Their failures can be subtle and unpredictable.

In this work, we consider the task of processing the _flip-flop language_, a minimal unit of sequential computation which consists of memory operations on a single bit (see Definition 1) and underlies virtually all1 syntactic parsing and algorithmic reasoning capabilities (including implementing adders, and far more). A _flip-flop language modeling_ (FFLM) task is defined on sequences of write, read, and ignore instructions: write sets the memory state to a certain value which is later retrieved by read, while ignoring any contents in between. We find that when trained to complete flip-flop sequences, the Transformer architecture exhibits a long tail of reasoning errors (incorrect read retrievals), unlike previous-generation recurrent models such as the LSTM (Hochreiter and Schmidhuber, 1997). We coin the term _attention glitch_ for this phenomenon, and hypothesize that this captures a systematic failure mode of Transformer-based LLMs when internally manifesting long chains of algorithmic reasoning.

Our contributions are as follows:

* **FFLM: a minimalistic long-range dependency benchmark.** We propose _flip-flop language modeling_, a parametric family of synthetic benchmarks for autoregressive sequence modeling, designed to isolate and probe reasoning errors like those demonstrated in Figure 1. We view FFLM as a robust complement to the Long Range Arena (Tay et al., 2020) and some of the tests in BIG-Bench (Srivastava et al., 2022), and recommend measuring glitch rates as a **"stress test"** for architectural innovations in sequence modeling.2 * **Main empirical result: attention attends gittichly.** We find that while Transformer models can appear to learn flip-flop languages perfectly on held-out samples from the training distribution, they make a long tail of unpredictable reasoning errors (_attention glitches_), on both long-range and short-range dependencies. We evaluate various direct and indirect mitigations, including commonly-used regularization techniques and **attention-sharpening regularizers** -- a plug-and-play way to sparsify self-attention architectures. We find that attention sharpening reduces reasoning errors by an order of magnitude, but none of our attempts were successful in driving the number of errors to exactly 0. Meanwhile, recurrent models work perfectly.3 * **Preliminary mechanistic analyses.** We provide some theoretical and empirical explorations which account for some of the internal mechanisms for attention glitches, and why they are so difficult to eliminate completely.

### Related work

The challenge of learning long-range dependencies is a long-standing one in the statistical modeling of sequences (Samorodnitsky et al., 2007). The Transformer architecture (Vaswani et al., 2017), a paradigm-shifting sequence model, enables the scalable learning of a feedforward hierarchy of

Figure 1: Cherry-picked integer addition prompts, showing how state-of-the-art LLMs can generalize non-trivially on algorithmic sequences, but sporadic reasoning errors persist. The first digit of the correct answer depends on every input; thus, an autoregressive model must propagate a “carry” bit across these long-range dependencies in a single pass. This (and many other algorithmic reasoning capabilities) can be implemented by a Transformer model using internal _flip-flops_.

meaningful long-range dependencies. Yet, factual errors over long contexts persist in these models; this is the subject of many careful studies in deep NLP (Khandelwal et al., 2018; Tay et al., 2020; Guo et al., 2022; Ji et al., 2023).

The sporadic non-factual outputs of LLMs have been popularly called "hallucinations", especially when there is an expectation that producing a correct answer is disproportionately "easy". Popular approaches for improving robustness to such errors include _chain-of-thought generation_ (explicitly outputting intermediate reasoning steps) (Nye et al., 2021; Wei et al., 2022b) and enforcing self-consistency (Wang et al., 2022). In the emerging taxonomy of LLM pathologies (Saparov and He, 2022; Ji et al., 2023), the hallucinations studied in this work are of the _closed-domain_ variety, under a deterministic notion of factuality (namely, consistency with flip-flop transitions) which is unambiguously reflected by the training data. We provide further discussion on the connections to natural LLM hallucinations in Section 6 and Appendix A.4.

Long-range dependency and reasoning benchmarks.Many datasets and benchmarks have been designed to isolate qualitative issues in langauge modeling (Tay et al., 2020; Wu et al., 2021; Zhang et al., 2021, 2022; Saparov and He, 2022; Shi et al., 2023; van der Poel et al., 2023; Eldan and Li, 2023). Aside from being focused on the "smallest" and "purest" compositional unit of sequential reasoning (see Section 3.2), FFLM is distinguished by a few factors:

* "\(L_{}\)" **objective:** Unlike usual benchmarks, we consider any model with less than \(100\%\) accuracy as exhibiting a _reasoning error_. Aside from the motivation of completely eliminating hallucinations, we argue that this stringent notion of correctness is needed to avoid error amplification when flip-flops are embedded in more complex networks; see Appendix A.1 and Liu et al. (2023).
* **Parametric, procedurally generated, and generalizable:** Our empirical study precisely quantifies long-tail errors via a large number of replicates over the randomness of both model initialization and data generation. This methodology is easily adapted and rescaled (by adjusting \(T,\), and other difficulty knobs) to probe language models of any size.

We provide an expanded discussion of related literature in Appendix A.2.

## 2 Background and notation

Modern language models are powered by _sequence-to-sequence (seq2seq) neural networks_\(f_{}:^{T d}^{T d}\), which transduce sequences of vectors according to internal computations determined by the inputs as well as trainable parameters \(\). When equipped with mappings to and from symbolic tokens (an "embedding layer" \(E:[M]^{d}\) (here, \(M\) is the _vocabulary size_) and classification layer \(W:^{d}([M])\), shared across positions), \(W f E:[M]^{T}([M])^{T}\) can represent an autoregressive generative model of a joint distribution over tokens \(x_{1:T}[M]^{T}\), where the output at the \(t\)-th position gives the estimated next-symbol probabilities \(}[x_{t+1}= x_{1:t}]\). The overarching challenge of statistical language modeling is to fit complex distributions such as natural language; recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997; Wu et al., 2016) and self-attention-based (Vaswani et al., 2017) architectures have shown remarkable capabilities in fitting the seq2seq functions necessary for fluent linguistic parsing and reasoning.

Recurrent inductive biases, attention, and length generalization.To correctly process uniform (i.e. fixed-description-length) algorithmic computations on arbitrarily long sequences, it is natural to embed recurrences within a seq2seq network. Imitating the recurrent nature of the Turing machine, one can hope for RNNs to learn representations of the desired looped computations (Sutskever et al., 2013; Graves et al., 2014; Linzen et al., 2016). However, the key innovation in the Transformer architecture is a non-recurrent _self-attention_ module.4 Various works have noted that **Transformers and RNNs learn qualitatively different solutions**, discussing potential ways to reconcile these nuanced discrepancies (Dehghani et al., 2018; Abnar et al., 2020; Liu et al., 2023).

## 3 Flip-flop automata and the FFLM task

### Definitions

For any even number \(T 4\), we define a flip-flop string as a sequence of symbols \(\{,,,0,1\}^{T}\), which have the semantics of _instructions_ (write, read, ignore) and _data_ (one bit). A valid flip-flop string consists of alternating pairs of instructions and data (e.g. "w 0 i 1 i 0 r 0"), for which every symbol following a r instruction must be equal to the symbol following the most recent \(\); thus, "w 0 i 1 w 1 r 0" is not a legal flip-flop string. These sequences can be viewed as correct execution transcripts of a program which can (perhaps occasionally) write to a single bit of memory, and always correctly reads its contents. All sequences are required to begin with \(\) and end with \(\).

There are many possible choices of (probabilistic) _flip-flop languages_, which are distributions over valid flip-flop strings. We define a canonical family of them: let \((T,)\) be the distribution over length-\(T\) flip-flop strings, parameterized by \(=(p_{},p_{},p_{})(\{,,\})\), such that:

1. The first instruction \(x_{1}\) is always \(\), and the last instruction \(x_{T-1}\) is always \(\).
2. The other instructions are drawn i.i.d. according to \((p_{},p_{},p_{})\) with \(p_{}=1-p_{}-p_{}\).
3. The nondeterministic data symbols (paired with \(\) or \(\)) are drawn i.i.d. and uniformly.

We are interested in whether language models can learn a flip-flop language from samples, which we define as processing the read operations _perfectly_. Two variants of the autoregressive language modeling task can be defined on this distribution:

* **Generative ("noisy") mode:** Estimate the conditional next-token distribution \([x_{t+1}|x_{1:t}]\), for each \(t=1,,T-1\). In this mode, the sequences can be treated as drop-in replacements for natural text in GPT-style training. Generative FFLMs can be evaluated by checking their completions on prefix "prompts" (e.g. "... w 0 i 1 i 1 i [?]").
* **Deterministic ("clean") mode:** Predict only the continuations which are deterministic: correctly output \(x_{t+1}\) only for the prefixes \(x_{1:t}\) such that \(x_{t}=\). At the cost of a slight departure from vanilla language modeling, this setting isolates the long-range memory task. It is similar to the non-autoregressive flip-flop monoid simulation problem discussed in Liu et al. (2023), with limited supervision. 5 
These tasks naturally embed the capability of simulating the _flip-flop_, a machine which memorizes a single bit (see Figure 1(a),b for closely related variants).6 It is easy to see that recurrent networks and

Figure 2: Elementary objects and examples associated with flip-flop languages. (a) the 2-state flip-flop machine (elided transitions are self-loops). (2) A 4-state automaton which processes flip-flop languages (implying the existence of a small RNN). (c) Simple examples of sequential prediction tasks which require processing a flip-flop language.

2-layer Transformers (see Proposition 2) _can_ both represent FFLM parsers. The question of whether they _do_, especially from less-than-ideal data, turns out to be extremely subtle, and is the subject of the remainder of this paper.

### Why focus on the flip-flop?

The most immediate rationale for this synthetic benchmark is that flip-flop simulation (maintaining memory in a sequence) is a direct necessity in many reasoning settings (see Figure 1(c)). It is a special (depth-\(1\)) case of Dyck language processing (Chomsky and Schutzenberger, 1959; Yao et al., 2021; Zhao et al., 2023), which is necessary for parsing recursive grammars. It also captures certain structures in code or language tasks, such as tracking semantic changes (Miceli-Barone et al., 2023) or ignoring irrelevant contexts in general (Tafjord et al., 2020; Ho et al., 2020; Shi et al., 2023). The BIG-Bench suite of reasoning benchmarks (Srivastava et al., 2022) contains many tasks which require maintaining a discrete state over a sequence of transitions. Thus, more than a toy model, flip-flop languages are embedded verbatim within many sequence processing tasks. We offer some additional perspectives below.

Algebraic properties and expressive power.Flip-flops are the computational building blocks of memory. The _flip-flop monoid_\(\) (Definition 1), an algebraic encoding of a flip-flop's dynamics, is the smallest monoid whose operation is both _non-commutative_ and _non-invertible_. \(\) plays an essential role in the Krohn-Rhodes theory of automata and semigroups (Rhodes et al., 2010), whose central structure theorem (Krohn and Rhodes, 1965; Zeiger, 1967; Eilenberg, 1974) implies that a constant-depth cascade of parallel flip-flops simulates _all_ group-free finite-state automata. Thus, in a rigorous sense, the robust learning of flip-flops is not only a _necessary_ condition for reasoning, but a _sufficient_ condition for a wide class of algorithmic capabilities.

Intended functionality of attention.One can also appeal to the origin of attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017): attention was specifically designed to _attend_ to7 (i.e. selectively retrieve and copy) data over long-range dependencies. Indeed, it is easy to verify that a single attention head can perform the required lookup (see Proposition 2). It is thus logical to ask how well a purely attention-based architecture performs this elementary operation.

## 4 Attention glitches: a long tail of errors for Transformer FFLMs

In our main set of synthetic experiments, we train neural language models to generate strings from the flip-flop language \((T=512,=(0.1,0.1,0.8))\) (for short, \((p_{i}=0.8)\)), 8 and probe whether the networks robustly learn the language. Although every valid flip-flop string is supported in this distribution, some sequences are far rarer than others; we measure tail behavior via probes of extrapolation, defined here as out-of-distribution evaluations which amplify the probabilities of the rare sequences. To create these "challenging" sequences, we sample \(>3 10^{5}\) sequences from \((0.98)\) (containing unusually many "sparse" sequences with mostly ignore instructions), as well as \((0.1)\) (many "dense" sequences). Training and evaluating the read accuracies of Transformer models of various sizes, as well as a recurrent LSTM model, we find the following (see Figure 3):

1. [label=(R0)]
2. **Transformers exhibit a long, irregular tail of errors.** Such errors occur on both sparse and dense sequences. Further, a model's out-of-distribution test error varies significantly between random seeds (initializations as well as stochastic minibatches), and even between iterates within the same training run.
3. **LSTMs extrapolate perfectly.** In stark contrast, with 20 times fewer training samples and iterations, a 1-layer recurrent model achieves \(100\%\) accuracy, on \(100\) out of \(100\) runs.

Data release.For reproducibility, we publish this synthetic data at https://huggingface.co/datasets/synthesq/flipflop: 16M \((0.8)\) training sequences, 16K \((0.8)\) in-distributiontest sequences, 160K sparse o.o.d. sequences from \((0.98)\), and 4K \((0.1))\) dense o.o.d. sequences from \((0.98)\). Our data pipelines can be replicated _exactly_ by taking the appropriate prefix slices of these datasets.

As a counterpart to these findings, we observe similar anomalies in real LLMs, when prompted to complete natural textual embeddings (Figure 2, top right) of flip-flop tasks:

1. **10B-scale natural LMs can correctly process flip-flop languages, but not robustly.** Beyond a certain scale, natural language models can learn to process (natural embeddings of) flip-flop languages from in-context demonstrations. However, this emergent capability is not robust: there exist rare read errors, whose probabilities amplify as the sequence length \(T\) grows. We provide details for the few-shot evaluation protocol in Appendix B.2.1.

### Multiplicity of mechanisms for attention glitches

What failure mechanisms account for these reasoning errors, which occur for both short- and long-range dependencies? The model capability is not a concern as discussed earlier (see Proposition 2). In this section, we discuss how Transformer self-attention modules, when tasked with representing flip-flops, can exhibit multiple (perhaps mutually entangled) failure mechanisms. The accompanying propositions are proven in Appendices C.2 and C.3.

An insufficient explanation: implicit \(n\)-gram models.As a warmup, consider a language model \([x_{t+1}|x_{ t}]\) which only depends on the \(n\) most recent tokens in the context. Then, if \(n\), the bulk of \(\)'s predictions on \((p_{}=p)\) can be no more accurate than random guessing. This recovers one qualitative trend (degradation of accuracy with dependency length) observed in the experiments. However, this cannot fully explain our findings: it fails to account for the incorrect predictions on dense sequences. Furthermore, the Transformers' outputs on \((0.98)\) are _mostly_ correct; their accuracies on very long-range dependencies are nontrivial, despite not being perfect. There must therefore be subtler explanations for these errors.

Lipschitz limitations of soft attention.Moving to finer-grained failure mechanisms, a known (Hahn, 2020; Chiang and Cholak, 2022) drawback of soft attention is that its softmax operation can be "too soft"--for any weight matrices with fixed norms, the attention gets "diluted" across positions as the sequence length \(T\) increases, and can fail to perform an intended "hard selection" operation. We provide a formal statement and proof (Proposition 3) in Appendix C.2.

Figure 3: _Top:_ Training curves of recurrent (left) vs. Transformer (center) architectures on FFLM, with _best-so-far_ evaluation errors highlighted for clarity. **Transformers fail to extrapolate robustly** to the long tail of long-range dependencies, even on this extremely simple task of remembering one bit. The bolded box contains our chosen 6-layer 19M-parameter canonical baseline model. We find that the ability to complete flip-flop language prompts emerges in natural language models, but is not robust (right). _Bottom:_ examples from the sparser \((0.98)\) and denser \((0.1)\) distributions, causing distinct (_long-range_ and _short-range_) failure modes for the baseline Transformer model.

Difficulty of non-commutative tiebreaking.Can we simply robustify soft attention by replacing it with hard attention? We present a brief analysis which suggests that even hard attention can be brittle. In a stylized setting (one-layer models with linear position encodings), we show that self-attention can _confidently attend to the wrong index_, unless the weight matrices precisely satisfy an orthogonality condition (Proposition 4). This suggests the existence of _spurious local optima_, which we do not attempt to prove end-to-end; however, we provide supporting empirical evidence in the experiments in Appendix C.3.

## 5 Mitigations for attention glitches

In this section, we investigate various approaches towards eliminating the long tail of reasoning errors exhibited by Transformer FFLMs. We select the 19M-parameter model (which has \(L=6\) layers, \(d=512\) embedding dimensions, and \(H=8\) heads) from Section 4 as a canonical baseline, and conduct precise evaluations of various direct and indirect interventions.

### Effects of training data and scale

Ideal solution: improving data coverage.Prior work has made clear that data significantly impacts the performance (Schuhmann et al., 2022; Eldan and Li, 2023). Hence, we begin by examining what is perhaps the most obvious solution: removing the need for out-of-distribution extrapolation, by training directly on more diverse examples. Indeed, we verify that this works near-perfectly:

1. [label=(R0)]
2. **Training on rare sequences works best, by a wide margin.** By training on a uniform mixture of \(\) distributions with \(p_{}=\{0.9,0.98,0.1\}\), the baseline architecture reliably converges to solutions with significantly fewer errors on each of these 3 distributions (teal violins in Figure 4). In 6 out of 25 runs, we did not detect a single error.

This should not be surprising, in light of the realizability of flip-flops by self-attention (and, more generally, the existence of shortcuts functionally identical to RNNs (Liu et al., 2023)), and corroborates similar conclusions from (Zhang et al., 2021). We also find that weaker improvements emerge by straightforwardly increasing scale parameters in the model and training pipelines:

1. [label=(R0)]
2. **Resource scaling (in-distribution data, training steps, network size) helps.** However, the improvements are orders of magnitude smaller than those in (R4), and we observe tradeoffs between sparse- and dense-sequence extrapolation; see the blue violins in Figure 4.

Another class of direct solutions is to _externalize the chain of thought_ (CoT): train (or finetune, or prompt) the model to explicitly output the intermediate reasoning steps (Nye et al., 2021; Wei et al., 2022b). We do not investigate this strategy in this paper, and note that prior work has provided sufficient evidence to affirm its efficacy in inducing the robust learning of recurrences on long synthetic sequences (Anil et al., 2022; Zhou et al., 2022; Liu et al., 2023). Even when applying CoT in practice, we believe attention glitches may still occur, as flip-flops operations may be embedded within a single indivisible reasoning step. Thus, the focus of this work is to isolate and mitigate this intrinsic architectural issue. We provide additional references and discussion in Appendix A.2.

### Indirect algorithmic controls: a bag of regularization tricks

The interventions listed in Section 5.1 are all potentially practical, and may shed light on how closed-domain LLM hallucinations will diminish with data quality, scale, and improved inference strategies. However, it is not always _feasible_ to implement these fixes under resource constraints (especially data). We next investigate an orthogonal design space, of how to robustify the _internal_ memory mechanisms of neural sequence models. Note that the exceptionally strong extrapolative performance of the LSTM provides a "skyline", showing the possibility of far more robust architectures than the Transformer (in the flip-flop setting, with this restricted set of considerations).

Standard regularization heuristics.There is a large array of not-fully-understood algorithmic tricks for "smoothing" the behavior of LLMs. We test the extrapolative behavior of models trained with weight decay and dropout (at the attention, feedforward, and embedding layers), as well as a host of algorithmic choices known to modulate generalization (batch sizes, learning rates, optimizerhyperparameters, position embeddings, activation functions). Due to the extreme variability noted in (R1), we quantify effects on extrapolation by training and evaluating at least 25 replicates for each choice under consideration.

Attention sharpening: a non-standard regularization technique.Inspired by the "diluted hard attention" calculation in Section 4.1, and the fact that the attention heads of trained models do not attend sharply (see Figure 5), we train Transformer models with _attention-sharpening regularizers_:9 during training, for attention weights \(([T])\), adding differentiable loss terms which encourage sparsity (e.g. the mixture's entropy \(H()\), or negative \(p\)-norms \(-_{2}\), \(-_{}\)).

1. [label=(R0)]
2. **Many algorithmic choices influence extrapolative behaviors.** We find that some architectural variants and regularization tricks have orders-of-magnitude effects on the out-of-distribution performance of Transformers; see the purple, brown, red, and gold violins in Figure 4 (right). Our strongest improvements on sparse sequences are obtained by large (\(0.5\)) embedding dropout and attention-sharpening losses; on dense sequences, non-trainable position embeddings are the most helpful.
3. **Despite many partial mitigations, nothing eliminates attention glitches entirely.** The scatter plot in Figure 4 (left) gives an overview of our entire search over architectures and hyperparameters, showing (dense-sequence error, sparse-sequence error) pairs for _every_ model we trained. We found it extremely difficult to find a setting that reliably produces Transformer models with simultaneous improvements over the baseline on sparse and dense sequences. Recall that it is trivial to do so with an LSTM model.

### Preliminary mechanistic study of the trained networks

In this section, we move to a simpler setting to gain finer-grained understanding of how sparsity regularization affects the learned solutions. Specifically, we look at the task of _simulating the flip-flip automaton_ (Definition 1), whose inputs consist of \(\{_{0},_{1},\}\) as two types of write and 1 no-op. This task (elaborated in Appendix A.1) can be solved by a 1-layer Transformer with a single attention head which attends sparsely on the most recent write position. It also serves as a building block for more complex tasks (Liu et al., 2023), hence observations from this simple setup can potentially be useful in broader contexts.

Figure 4: A long tail of flip-flop errors for \(10,\!625\) Transformer models. _Left:_ Out-of-distribution evaluations for all models; some algorithmic choices help substantially (note the logarithmic axes), but **nothing we tried, aside from training on o.o.d. data, could fully eliminate attention glitches**. _Right:_ Effects of individual architectural and algorithmic choices on both types of extrapolation (sparse and dense sequences). Some configurations reduce attention glitch rates by orders of magnitude. Horizontal marks denote {min, 25%, median, 75%, max} test errors on \(>\!3 10^{5}\) predictions, over \(25\) replicates (\(500\) for the baseline model). Dots at the bottom indicate runs with 0 error.

Figure 5 shows examples of attention patterns on the flip-flop simulation task, subselected from 6-layer 8-head models trained with and without attention-sharpening regularization. It is evident that the attention patterns of the sparse model are less complex and easier to interpret compared to those of the un-regularized model. For example, we can identify one head in the sparse model that exactly coincide with the attention pattern10 that an "ideal" 1-layer 1-head model implements (Figure 4(c)).

(R8) **Attention-sharpening regularizers successfully promote hard attention, but errors persist.** As mentioned in (R7), attention-sharpening regularization cannot fully eliminate the sporadic errors, which are partially induced by the complexity and redundancy of attention patterns. Moreover, sharpened attention can induce additional failure modes, such as confidently attending to incorrect write positions. An example is demonstrated in Figure 4(d), where the attention focuses on an initial write, likely caused by the fact that earlier positions are overemphasized due to the use of causal attention masks. Another example occurs in length generalization, where the attention is correct at positions earlier in the sequence, but starts to confidently focus on wrong positions as it moves towards later positions (Proposition 4).

In a similar spirit to concurrent work on generating Dyck languages (Wen et al., 2023) (a more complex capability which also requires parallel simulation of memory registers), these glitchy solutions point to a concrete obstruction to mechanistic interpretability. Due to factors such as overparameterization, spurious solutions, and the opaqueness of optimization dynamics, **learned neural implementations of algorithms generally do not coincide with "ideal", "minimal", or "natural" theoretical constructions**. Details for these experiments and further discussion are provided in Appendix B.5.

## 6 Conclusion and future challenges

We have introduced _flip-flop language modeling_ (FFLM), a synthetic benchmark for probing the fine-grained extrapolative behavior of neural sequence models, based on a one-bit memory operation which forms a fundamental building block of algorithmic reasoning. Despite being able to realize this operation trivially, **Transformer models do not extrapolate robustly**: they exhibit a long tail of sporadic reasoning errors, which we call _attention glitches_. Through extensive controlled experiments, we find that many algorithmic mitigations can reduce the frequency of attention glitches, but **only recurrence and training on longer-tailed data work perfectly**. FFLM provides a concrete and minimalistic setting in which Transformers are far inferior to recurrent sequence models, with respect to multiple criteria (efficiency, stability, and extrapolation).

Figure 5: Causal attention patterns for flip-flop simulation (Definition 1); orange dots / blue diamonds mark the positions of write tokens \(_{0}\) / \(_{1}\). (a),(b) are subselected respectively from a regular (non-sparse) and a sparse multi-layer model (details in Appendix B.5). (c), (d) are from two 1-layer 1-head models. The attention pattern highlighted by the purple box in (b) coincides with the “ideal” attention pattern in (c). However, sparse models can be wrong, as shown in (d) (error marked in red).

What does this entail about hallucinations in natural LLMs?The motivating issue for this work is the phenomenon of "closed-domain hallucinations" in non-synthetic LLMs (e.g. the errors demonstrated in Figure 1). We hypothesize that attention glitches occur in the internal algorithmic representations of Transformer models of natural language, and that they account for (a non-negligible portion of) the reasoning errors encountered in practice. To our knowledge, this is the first attempt to attribute model hallucinations to a systematic architectural flaw in the Transformer. However, confirming or refuting this hypothesis is far outside the scope of this paper; the opaque indirections and lack of adequate controls on the training data present significant methodological challenges. Even precisely articulating this hypothesis leaves degrees of freedom which are difficult to resolve; see the discussion in Appendix A.4. We therefore leave these topics for future work.

Paths to hallucination-free Transformers?Our findings suggest that in the near term, there are many mutually-compatible approaches for reducing the frequency of attention glitches: data (particularly with high diversity), scale, and various forms of regularization. Yet, the strikingly outsized benefit of replacing the Transformer with an LSTM network suggests that _architectural_ innovations towards the same ends are well worth examining. Obtaining a practical best-of-both-worlds architecture is a grand open challenge, for which new recurrent designs (Katharopoulos et al., 2020; Dao et al., 2022; Peng et al., 2023; Anonymous, 2023) show great promise. Note that we do not make the claim that recurrent architectures are the only ones which can extrapolate robustly.11

Broader impacts and limitations.This work is inherently foundational, and focuses on precise measurements of generalization in an idealized setting; see Appendix A.4 for a discussion of the limitations this entails. By introducing methodologies to isolate, measure, and control the long tail of reasoning errors in neural sequence models, we hope that this work will contribute to the systematic and principled discovery of LLM pipelines with improved factual reliability. Such improvements may result in unintended downstream consequences, such as higher-fluency malicious content generation.