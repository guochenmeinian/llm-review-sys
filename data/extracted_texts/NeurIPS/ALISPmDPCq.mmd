# ConStat: Performance-Based Contamination Detection in Large Language Models

Jasper Dekoninck\({}^{1}\), Mark Niklas Muller\({}^{1,2}\), Martin Vechev\({}^{1}\)

Department of Computer Science\({}^{1}\)

ETH Zurich, Switzerland

{jasper.dekoninck,martin.vechev}@inf.ethz.ch

&LogicStar.ai\({}^{2}\)

mark@logicstar.ai

###### Abstract

Public benchmarks play an essential role in the evaluation of large language models. However, data contamination can lead to inflated performance, rendering them unreliable for model comparison. It is therefore crucial to detect contamination and estimate its impact on measured performance. Unfortunately, existing detection methods can be easily evaded and fail to quantify contamination. To overcome these limitations, we propose a novel definition of _contamination as artificially inflated and non-generalizing benchmark performance_ instead of the inclusion of benchmark samples in the training data. This perspective enables us to detect _any_ model with inflated performance, i.e., performance that does not generalize to rephrased samples, synthetic samples from the same distribution, or different benchmarks for the same task. Based on this insight, we develop ConStat, a statistical method that reliably detects and quantifies contamination by comparing performance between a primary and reference benchmark relative to a set of reference models. We demonstrate the effectiveness of ConStat in an extensive evaluation of diverse model architectures, benchmarks, and contamination scenarios and find high levels of contamination in multiple popular models including Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models.1

## 1 Introduction

As large language models (LLMs) become increasingly effective at a wide range of tasks, many companies and research institutions compete to develop better models . To facilitate this development, a variety of benchmarks have been proposed that allow a standardized in-depth comparison of model performance across diverse tasks .

Data ContaminationModern LLMs are trained on vast amounts of internet-sourced data, raising the risk of unintentionally including benchmark samples in the training set. Such _data contamination_ can lead to artificially inflated benchmark performance that does not accurately reflect a model's true ability to generalize to unseen tasks. However, model providers argue that the impact of this contamination on model performance is negligible  and the enormous size of current training sets almost guarantees contamination to some extent. This casts doubt on the relevance of this traditional definition of contamination in the context of LLMs.

This Work: A New Perspective on Data ContaminationWe propose a new perspective on contamination, defining it based on its effect on model performance rather than its cause. Specifically, we _define contamination as artificially inflated, non-generalizing performance_, i.e., we say a model is contaminated if and only if its performance relative to other models is significantly higher on the original benchmark than on a similar reference benchmark. This definition captures the essence of the contamination problem, i.e., performance measurements becoming unreliable for model comparisons.

Furthermore, it enables principled detection methods that are robust against evasion attacks by malicious providers as this would require generalizing performance improvements.

Traditional Contamination DetectionExisting contamination detection methods  aim to detect the inclusion of benchmark samples in the training data as a measure of contamination. However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade .

This Work: A Statistical Test for ContaminationIn contrast, we leverage our novel performance-based definition of data contamination to propose a statistical contamination test called ConStat, illustrated in Fig. 1. Given a target model (\(M_{1}\) or \(M_{2}\)) to check for contamination (first step in Fig. 1), we select a set of reference models for performance comparison and a reference benchmark \(D_{}\) that is similar to the original benchmark \(D\) (second step). This reference benchmark can be a rephrased version of the original benchmark, a synthetic benchmark generated from the same distribution, or a different benchmark measuring performance on the same task. We then evaluate the reference models on both benchmarks \(D\) and \(D_{}\) and fit the difficulty correction function \(H_{D_{}}\) describing the relation between performance on the reference and original benchmarks (blue curve). By evaluating \(H_{D_{}}\) at the target model's performance on the reference benchmark, we predict its expected performance on the original benchmark (third step). Finally, we compute the difference \(\) between this expected performance and the model's actual performance on the original benchmark. Using bootstrapping, we obtain an estimate of the contamination magnitude \(\) and a p-value that quantifies the likelihood of the observed performance difference under the null hypothesis that the target model is not contaminated (fourth step). In the illustrated case, model \(M_{1}\) achieves \(60\%\) on the reference benchmark, which translates to an expected performance of \(37\%\) on the original benchmark. However, the measured performance of \(72\%\) indicates a large contamination effect \(_{1}=35\%\) and thus strong contamination with a p-value of \(0.01\%\). In contrast, model \(M_{2}\) shows no signs of contamination.

EvaluationWe evaluate ConStat on a wide range of contamination scenarios and model architectures, demonstrating that it is significantly more effective at detecting contamination than any prior method. We then use ConStat to study a range of popular open and proprietary models and find high levels of contamination in Mistral-7b-v0.1 , Llama-3-70b , Llama-2-Instruct-70b , Yt-34b , and a range of top Open LLM Leaderboard  models.

Key ContributionsOur key contributions are:

* We propose a new performance-based definition of benchmark contamination (SS2).
* We introduce ConStat, a statistical test that detects and quantifies contamination in language models (SS3).
* We empirically demonstrate ConStat's effectiveness in an extensive evaluation across various contamination scenarios (SS4.2).
* We leverage ConStat to study a range of popular models and find contamination for Mistral, Llama, Yi, and the top-3 Open LLM Leaderboard models (SS4.3-SS4.5).

Figure 1: Overview of our method. We first select models to check for contamination, then select reference models and benchmarks, and finally compute ConStat to detect and quantify contamination.

Defining Contamination

Before formalizing our novel definition, we first informally contrast the traditional, information-flow-based perspective on contamination with our novel performance-based one.

Information-Flow PerspectiveIn traditional machine learning, contamination typically refers to any information flow between the benchmark used for performance measurement and model training. In the context of LLMs, this is usually restricted to the direct inclusion of test set samples (or their semantic equivalents) in the training dataset [37; 39; 51; 56].

However, this perspective suffers from several drawbacks. First, it does not fully capture the core issue of contamination, which is whether it renders test set performance an unreliable predictor of real-world performance. Second, in the era of zero-shot learning, we aim to measure performance on "unseen" tasks, yet we train on internet-scale data that likely contains samples of almost any task. This makes the threshold for contamination blurry. Third, limiting the definition to test sample inclusion neglects the possibility of model and hyperparameter selection based on benchmark performance as a source of contamination. Finally, even with this narrow definition, detecting contamination without access to the training data is challenging, which makes it easy to circumvent .

Performance PerspectiveTo overcome these limitations, we propose to _define contamination based on its outcome, rather than its cause_. Informally, we define contamination as artificially inflated performance on a benchmark that does not generalize to real-world performance on the corresponding task, regardless of how it was achieved. This definition aligns better with the practical implications of contamination and enables a more principled detection method that makes evasion difficult.

To detect contamination, we compare the performance of a model \(M\) on a benchmark \(D\) to its performance on a reference benchmark \(D_{}\), the choice of which we will discuss later. It is crucial to account for differences in difficulty between \(D\) and \(D_{}\). Otherwise, a slightly harder reference benchmark \(D_{}\) would falsely indicate inflated performance on \(D\). Thus, direct performance comparison is only valid if the distribution over sample difficulties is the same for both benchmarks, which is a very strong assumption that is rarely true. To address this, we compare performances relative to a set of reference models, allowing us to determine if a model's performance on \(D\) is significantly higher than expected, given its difficulty. In the next section, we make this definition more formal.

### Formal Definition of Performance-Based Contamination

Reference ModelsTo accurately compare performance between benchmarks, we use reference models to correct for benchmark difficulty differences. For this purpose, we consider the set of all reliable LLMs \(_{}\) from reputable sources to estimate the performance distribution of uncontaminated models. Although we cannot guarantee these models are uncontaminated, we can perform leave-one-out contamination detection to remove suspicious models from the reference set. Furthermore, including contaminated models in \(_{}\) will only make our test more conservative, making it _less_ likely for uncontaminated models to be detected as contaminated.

Contamination DetectionFor each benchmark \(D\), we define a scoring function \(S_{D}\) that assigns a score (e.g., accuracy) to every model from the space of all possible language models \(\). Applied to the reference models \(_{}\), it induces a cumulative distribution function \(F_{D}\) over the uncontaminated performance on this benchmark.

We now use the cumulative distributions \(F_{D}\) and \(F_{D_{}}\) to predict the performance of a model \(M\) on \(D\) given its performance on \(D_{}\). Specifically, we first map the performance on the reference data \(S_{D_{}}(M)\) to a percentile \(q=F_{D_{}}(S_{D_{}}(M))\) and then map this percentile to the corresponding performance on the original benchmark \(F_{D}^{-1}(q)\) using the percentile function \(F_{D}^{-1}\). To simplify notation, we define the hardness correction function \(H_{D_{}}\) as \(H_{D_{}}=F_{D}^{-1} F_{D_{}}\). This allows us to estimate the effect of contamination on the model's performance as \(S_{D}(M)-H_{D_{}}(S_{D_{}}(M))\) and gives our formal definition of contamination:

**Definition 1** (\(\)-Contamination).: _A model \(M\) is \(\)-contaminated on a benchmark \(D\) with respect to a reference benchmark \(D_{}\) if \(S_{D}(M)-H_{D_{}}(S_{D_{}}(M))>\)._

### Types of Contamination

Depending on the choice of reference benchmark \(D_{}\), we can measure different types of contamination, depending on how poorly the inflated performance generalizes.

_Syntax-Specific Contamination_ occurs when the model fails to generalize to semantically equivalent samples. That is, the model has memorized the exact samples in the benchmark, and its performance drops as soon as the wording changes. We therefore consider it to be the worst kind of contamination. To measure syntax-specific contamination we create our reference benchmark \(D_{}\) by rephrasing the samples in the original benchmark \(D\) to obtain a semantically equivalent benchmark.

_Sample-Specific Contamination_ occurs when the model fails to generalize to new samples from the benchmark distribution. That is, while the model generalizes to samples that are semantically equivalent to those in the original benchmark, it does not generalize to new samples from the same distribution. To accurately measure sample-specific contamination, we would preferably generate samples for \(D_{}\) following the same steps used to produce \(D\). As this is often infeasible in practice, we instead generate synthetic samples for \(D_{}\) by querying a strong LLM using few-shot prompting and varying the provided few-shot examples to increase diversity.

_Benchmark-Specific Contamination_ occurs when the model fails to generalize to different benchmarks that aim to measure performance on the same task. That is, the model generalizes to new samples from the original benchmark distribution but does not generalize to closely related benchmarks. To measure benchmark-specific contamination we create (or select) a different benchmark \(D_{}\) (e.g., MathQA) that aims to measure performance on the same task as \(D\) (e.g., GSM8k). We note that benchmark-specific contamination is by far the least severe type of contamination. Further, while strong sensitivity to the exact benchmark is undesirable, it is important to recognize that even small differences between benchmarks can impact model performance. Therefore, benchmark-specific contamination requires a more nuanced interpretation that takes into account these differences.

## 3 ConStat: A Statistical Test for Detecting Contamination

We now present ConStat, a novel method for detecting contamination as defined in SS2 by computing confidence bounds on the estimated contamination effect using a statistical test.

Reference ModelsTo approximate the underlying distribution of reference models \(_{}\), we select a diverse sample of \(m\) models \(}_{}=\{M_{,1},...,M_{,m}\} _{}\). We additionally include an inherently uncontaminated random-guessing model to extend the coverage of our reference set.

Null HypothesisTo rigorously test for contamination, we derive a null hypothesis based on our definition of contamination. The null hypothesis is the assumption that the model \(M\) is not contaminated, meaning its actual score on the original data is at most \(\) worse than the predicted one: \(S_{D}(M)-H_{D_{}}(S_{D_{}}(M))\) where \(_{ 0}\) can be chosen freely.

Estimating the Hardness Correction FunctionTo compute the hardness correction function \(H_{D_{}}\), we first estimate the CDFs \(F_{D}\) and \(F_{D_{}}\) as the empirical CDFs \(_{D}\) and \(_{D_{}}\), respectively. To this end, let \(i_{1},...,i_{n}\) be an index such that \(S_{D}(M_{,i_{k}}) S_{D}(M_{,i_{k+1}})\). We obtain the CDF \(_{D}\) as

\[_{D}(x)=0&\ x<S_{D}(M_{i_{1}})\\ k/n&\ S_{D}(M_{i_{k}}) x<S_{D}(M_{i_{k+1}})\\ 1&\ S_{D}(M_{i_{n}}) x. \]

Similarly, \(_{D_{}}\) can be obtained from an index \(j_{1},...,j_{n}\) such that \(S_{D_{}}(M_{,j_{k}}) S_{D_{}}(M_{ {ref},j_{k+1}})\). Using Eq. (1), we find that \(H_{D_{}}(S_{D_{}}(M_{j_{k}}))=S_{D}(M_{i_{k}})\). Applying the empirical CDFs directly to other points \(x\) would result in a step function estimate of \(H_{D_{}}\), leading to an overly rough approximation of the hardness correction function. Thus, we compute the approximate hardness function \(_{D_{}}\) by fitting the points \((S_{D_{}}(M_{j_{k}}),S_{D}(M_{i_{k}}))\) using a smoothing spline, minimizing the following loss function:\[_{k=1}^{n}(S_{D}(M_{i_{k}})-_{D_{}}(S_{D_{}}(M_{ j_{k}})))^{2}+_{0}^{1}^{}_{D_{}}(x)^{2 }\,dx \]

where \(\) is a smoothing parameter that is chosen using generalized cross-validation .

Significance EstimationWe determine the statistical significance for rejecting the null hypothesis via bootstrapping over both the reference models and the samples in the benchmark, using pivotal intervals  to correct for uncertainty in the bootstrapping process. By bootstrapping the models, we consider the effect of our reference model selection \(}_{}\). By bootstrapping the samples, we additionally include the error in our estimation of the scores themselves. Thus, given the estimate \(=S_{D}(M)-_{D_{}}(S_{D_{}}(M))\) and corresponding bootstrap estimates \(_{1},...,_{n}\), we compute the p-confidence lower bound for \(\) as \(_{1-p}=2-^{}_{1-p}\) where \(^{}_{q}\) is the \(q\)-quantile of \(_{1},...,_{n}\). From this, we obtain the p-value by inverting this lower bound with respect to \(q\). Thus, we reject the null hypothesis for a given \(\) with significance level \(p\) by computing the lowest \(p\) such that \(2-^{}_{1-p}\).

Threat ModelIn accordance with , we briefly outline the threat model assumed by ConStat. Since we only require the ability to measure the performance of the model on the benchmark, our method is a black-box benchmark-level detection method that is robust to semantic preserving operations. Furthermore, we make no additional assumptions on potential metadata contamination. However, we do rely on the existence of reference models which we can use to estimate the performance of uncontaminated models. Notably, however, we do not assume these reference models to have a similar performance or architecture as the model we wish to test.

## 4 Evaluation

In this section, we evaluate ConStat empirically. We first demonstrate ConStat's effectiveness, showing it outperforms prior methods in detecting and quantifying contamination across a range of intentionally contaminated models (SS4.2). Next, we investigate the contamination of our chosen reference models (SS4.3), popular model families (SS4.4), and top Open LLM Leaderboard models (SS4.5). Further, we conduct an ablation study in a simulated environment in App. B to validate several design choices of ConStat.

### Experimental Setup

Reference ModelsWe select 20 models from reputable providers, including Meta's Llama model families , Microsoft's Phi-2  and Phi-3 , Google's Gemma-1.1 , several Mistral models , Falcon-7b , and the fully open-source OLMo . A detailed overview of these reference models is available in App. C.

BenchmarksWe select a diverse set of four of the most popular LLM benchmarks to evaluate ConStat: GSM8k  is a benchmark for mathematical reasoning, ARC-Challenge  is a multiple-choice benchmark for science questions, MMLU  is a multiple-choice general purpose benchmark and Hellaswag  is a dataset for commonsense natural language inference. Due to computational constraints, we limit the number of samples in each benchmark to \(2000\).

Reference BenchmarksTo generate reference data for syntax-specific and sample-specific contamination we query GPT-4-Turbo  to rephrase samples from the original benchmark and generate new synthetic samples. We generate around 1000 synthetic samples per benchmark and refer to App. C for further details on the generation process. To detect benchmark-specific contamination, we select appropriate reference benchmarks that measure performance on the same task: for GSM8k, we use MathQA , for ARC-Challenge we use SCIQ , and for Hellaswag we use the Lambad-OpenAI benchmark . For MMLU, we did not select any reference benchmark and thus measured only syntax- and sample-specific contamination.

EvaluationFor evaluation, we use the LM Evaluation Harness  in a \(5\)-shot setting. We report estimated effects \(\) along with the p-value for the null hypothesis that the effect \(\) is less than \(0\).

### Validating Contamination Detection with ConStat in a Controlled Setting

In this section, we demonstrate the effectiveness of ConStat in detecting and quantifying contamination in a controlled setting and compare it to multiple baselines. For this purpose, we finetune both Llama-2-Instruct-7b and Phi-2 using a variety of hyperparameters and contamination scenarios on each benchmark separately. We vary the number of epochs, the learning rate, the portion of contaminated training samples, whether or not few-shot examples are used during fine-tuning, and whether the model is trained on the original benchmark samples or on rephrased data. For more details, we refer to App. C. We trained a total of \(70\) models, \(9\) of which finished training at a loss spike and were therefore excluded from further analysis. \(46\) of the remaining models were trained on the actual benchmark and should therefore exhibit both syntax- and sample-specific contamination. The rest were trained on rephrased benchmark data and should therefore only exhibit sample-specific contamination. To quantify the true sample-specific contamination effect, we only use half of each benchmark for contamination and measure the performance gap to the other half.

Detecting ContaminationWe first check whether ConStat can accurately detect the presence of contamination. We compare ConStat against several baselines  that aim to detect contamination based on the presence of benchmark samples in the training data. Most of these baselines  require a detection threshold to be chosen for each model and benchmark separately. This tuning process requires uncontaminated samples, making it impossible to apply these methods in practice. For comparison to ConStat, we tuned these thresholds on the uncontaminated half of the benchmark, which is the most ideal (but unrealistic) scenario. We extract a p-value for these baselines by bootstrapping the samples in the benchmarks and checking how often TPR@\(1\%\)FPR is bigger than \(1\%\). Models are considered contaminated for any method if \(p<0.05\). The only baseline applicable in a realistic setting is Shi  and we use their recommendation to consider a model contaminated if the score returned by their method is above \(0.85\).

Results in Table 1 show that ConStat significantly outperforms all other methods without needing prior knowledge of uncontaminated samples. In particular, we find that ConStat can detect \(89\%\) of syntax-specifically contaminated models, while the best baseline achieves only \(85\%\). The gap widens further for sample-specific contamination, where ConStat detects \(98\%\) of contaminated models, while the best baseline only detects \(71\%\). The only baseline that can be applied in a realistic setting, Shi , performs significantly worse than ConStat.

We thus conclude that ConStat is the only contamination detection method that can reliably detect contamination and significantly outperforms all baselines even if they are tuned optimally using oracle access to the uncontaminated samples.

Quantifying ContaminationTo evaluate ConStat's ability to estimate the sample-specific contamination effect, we compare its estimate to ground truth measurements on uncontaminated samples. As shown in Fig. 2, we observe excellent predictiveness at a coefficient of determination of \(r^{2}=0.94\). The only three models that show a significantly higher estimate than the true effect achieve a perfect score on the contaminated samples, capping the true effect and explaining the overestimation.

Detailed Analysis on GSM8kWe conduct an in-depth analysis of contaminated models finetuned on GSM8k, referring to App. A.2 for a detailed table with all p-values. We finetuned 18 models on this benchmark, one of which remained undetected under sample-specific contamination detection.

   Method & Syntax [\%] & Sample [\%] \\  Carlini et al.  & \(76.1^{*}\) & \(65.6^{*}\) \\ Mieshballah et al.  & \(76.1^{*}\) & \(68.9^{*}\) \\ Yeom et al.  & \(78.3^{*}\) & \(67.2^{*}\) \\ Shi et al.  & \(84.8^{*}\) & \(70.5^{*}\) \\ Shi  & \(21.7\) & \(16.4\) \\  ConStat & \(\) & \(\) \\   

Table 1: Percentage of syntax- and sample-specific contaminated models detected by several methods.

For the detected syntax-specifically contaminated models, we observe an average increase in \(\) with a factor of \(2.28\) when transitioning from syntax-specific to sample-specific contamination. This indicates that the models still generalize somewhat to semantically equivalent samples. Furthermore, the models that were not detected by the syntax-specific contamination detection are exactly those models that were trained on rephrased data or were trained for just one epoch. This indicates that these models can still generalize to semantically equivalent samples. Since these scenarios are also more likely to occur in practice, this shows that it is crucial to also consider sample-specific contamination when applying ConStat. Finally, the model that remained undetected by the sample-specific contamination detection was a Phi-2 model trained with a lower learning rate. For this model, the actual contamination effect is approximately \(5\%\), which is relatively small and thus indicates that ConStat is not missing any major contamination.

### Contamination of Reputable Reference Models

To determine if our set of reference models exhibit signs of contamination, we perform a leave-one-out analysis, where we evaluate the contamination of model \(M\) using \(}_{}\{M\}\) as reference models. To control for performing multiple p-value tests and reduce the chance of false positives, we apply the Benjamini-Hochberg  procedure per benchmark and contamination type to control the false discovery rate at \(5\%\). We report all significant results in Table 2 and we discuss them for each type of contamination below.

Syntax-Specific ContaminationAs expected, we do not find syntax-specific contamination in any reference model, i.e., none of the models fail to generalize to semantically equivalent samples.

Sample-Specific ContaminationWe find four instances of sample-specific contamination, all with very significant p-values of less than \(p=0.5\%\) and considerable estimated contamination effects between \(3\%\) and \(8\%\). Specifically, we find contamination of Llama-3-70b on ARC, of Mistral-7b-v0.1 and Llama-2-Instruct-70b on Hellaswag, and Mistral-7b-v0.1 on GSM8k. We note that the contamination of Llama-2-Instruct-70b on Hellaswag is noted by its model provider , but the other model providers do not provide any contamination report for their models.

We investigate these models further on the other benchmarks where the corrected p-value using the Benjamini-Hochberg procedure was not significant. We discuss these results below and refer to App. A for a full overview of their sample-specific contamination. We find that Mistral-7b-v0.1 achieves relatively low p-values on both remaining benchmarks (\(8\%\) for ARC, \(15\%\) for MMLU). Furthermore, we additionally evaluated Mistral-7b-v0.2 after obtaining these results and found similar results for this model (see Table 17 in App. E). Therefore, we exclude Mistral-7b-v0.1 from our set of reference models. While in particular Llama-3-70b also exhibits low p-values for other benchmarks, none fall below \(p 1\%\). It is thus highly likely that also Llama-3-70b and Llama-2-Instruct-70b are contaminated across several benchmarks, but we keep both as reference models to ensure that we do not obtain a higher false positive rate in our further analysis.

   Model & Benchmark & Type & Perf. [\%] & \(p\) [\%] & \(\) [\%] & \(_{0.95}\) [\%] \\  Llama-3-70b & ARC & S & \(69.03\) & \(0.03\) & \(6.61\) & \(3.21\) \\ Mistral-7b-v0.1 & GSM8k & S & \(39.04\) & \(0.15\) & \(8.25\) & \(4.48\) \\ Mistral-7b-v0.1 & Hellaswag & S & \(83.65\) & \(0.24\) & \(3.14\) & \(1.27\) \\ Llama-2-Instruct-70b & Hellaswag & S & \(85.55\) & \(0.41\) & \(3.37\) & \(1.29\) \\ Mistral-Instruct-7b-v0.2 & ARC & B & \(62.46\) & \(0.04\) & \(10.62\) & \(5.95\) \\ Mistral-Instruct-7b-v0.2 & Hellaswag & B & \(84.55\) & \(0.18\) & \(3.52\) & \(1.56\) \\ Phi-2 & GSM8k & B & \(58.91\) & \(<10^{-2}\) & \(36.42\) & \(26.46\) \\ Phi-3-Mini & GSM8k & B & \(76.65\) & \(0.29\) & \(16.30\) & \(6.33\) \\ OLMo-Instruct-7b & GSM8k & B & \(11.75\) & \(<10^{-2}\) & \(8.86\) & \(4.99\) \\   

Table 2: Contamination results for the reference models on syntax-specific, sample-specific, and benchmark-specific contamination. We only report tests for which the multiple testing corrected p-value is lower than \(5\%\) and include the non-corrected p-value, the estimated effect \(\), the \(95\%\) lower bound of the effect \(_{0.95}\) and the model performance on the benchmark. S stands for sample-specific and B for benchmark-specific contamination. All numbers are reported in percentages.

Benchmark-Specific ContaminationWhile we find several instances of benchmark-specific contamination in the reference models, several at very low p-values (\(p<0.01\%\)), this requires a more nuanced interpretation. For example, both Phi models exhibit very large effect sizes (\(>15\%\)) and small p-values (\(p<0.01\%\)) for contamination on GSM8k. We suspect that this is due to their reasoning-focused training process and small model size. While GSM8k allows free text answers, giving the model tokens to reason, MathQA is a multiple-choice benchmark that requires the model to answer with a single token indicating the chosen option and therefore gives no room for this reasoning ability to shine.

### Contamination of Popular Model Families

We now use ConStat to detect contamination in four popular model families, discussing results for Owen-1.5  and Yi  below, while deferring discussions of StableLM-2  and InternLM-2  to App. A.1.

Qwen-1.5We evaluate all chat models from the Qwen-1.5 model family, with sizes 1.8b, 4b, 7b, 14b, 72b, and 110b. The only case of sample-specific contamination is for the 4b model on GSM8k with \(p<10^{-4}\) and an estimated effect of \(5.4\%\). The larger models show significant benchmark-specific contamination on ARC and Hellaswag, with p-values smaller than \(1\%\) and estimated effects between \(8\%\) and \(14\%\).

Y1We evaluate both the 6b and 34b parameter base models of the Yi model-family. Only Yi-34b shows significant contamination, with sample-specific contamination at \(p<0.2\%\) and estimated effects of around \(6\%\) on both ARC and Hellaswag. We find additional sample-specific contamination on GSM8k of around \(4\%\) at a p-value of \(p=6\%\) and _syntax-specific_ contamination on Hellaswag at a p-value of \(p=5\%\). Thus, we conclude that this model shows significant contamination across multiple benchmarks.

### Contamination of Top Open LLM Leaderboard Models

We use ConStat to investigate contamination in the top three 7B models on the open LLM Leaderboard2, BarraHome/Mistroll-7b-v2.2, yam-peleg/Experiment26-7b, and MTSAIR/multi_verse_model and find that all three models exhibit significant benchmark-specific contamination. Specifically, all models show strong contamination with estimated effects of \(>10\)% for the benchmarks where the reference benchmark is not included in the Open LLM Leaderboard (GSM8k, Hellaswag, and ARC). Further, all models show significant sample-specific contamination on GSM8k with \(\) 9%. For more detailed results, we refer to App. A.

This inflated performance could be caused by a model selection bias, as the Open LLM Leaderboard features thousands of models. This issue is exacerbated by the recent trend of merging models  where hyperparameters are frequently selected based on their benchmark performance. We therefore urge the community to be more cautious when selecting models from the leaderboard.

## 5 Related Work

Contamination DetectionContamination detection methods can be broadly divided into two main categories. The first category  focuses on analyzing the training data directly to identify overlaps with the benchmarks used for model evaluation. However, training data is rarely shared, even for open-weight models, making it irrelevant for third-party contamination detection. The second category  relies solely on access to the model and its predictions, aiming to detect contamination through model queries. As noted by Dekoninck et al. , some of these methods require metadata (e.g., benchmark name, canonical ordering) to be leaked along with the benchmark samples in the training data . Methods that do not require metadata depend on perplexity-based metrics to measure the model's uncertainty on benchmark samples, but these can be easily circumvented by training on rephrased samples . It is important to note that none of these methods can estimate the influence of contamination and that they are outperformed by ConStat in terms of detection accuracy (see SS4.2).

An alternative approach is presented by Zhu et al. , who measure model performance on rephrased benchmarks instead of the original benchmarks to obtain more accurate estimates of model performance. However, their results vary significantly across benchmarks, they do not provide a statistical framework for contamination detection, and they only demonstrate that evaluating on rephrased samples _partially_ recovers the results of uncontaminated base models. Furthermore, they do not go beyond measuring performance on rephrased benchmarks and can therefore also be evaded by training on rephrased samples .

Reference BenchmarksRecent studies have introduced new benchmarks designed to evaluate performance on tasks similar to those in prior popular benchmarks and thus can be used to estimate the degree of contamination. GSM1k  was developed to closely replicate the efforts behind GSM8k and to compare model performances between these benchmarks. However, GSM1k lacks a statistical test, and the slight variations between GSM8k and GSM1k might partially explain the contamination levels observed in their analysis. Another recent benchmark, SWE-bench , focuses on evaluating performance on coding tasks. By comparing their results with those of Human-Eval , one can visually interpret potential contamination in Human-Eval. However, the absence of a statistical test hinders precise contamination detection. In both scenarios, ConStat can improve their findings, enabling accurate estimations of contamination in existing models.

## 6 Discussion

LimitationsOur method estimates the effect of contamination on performance relative to a set of reference models. Therefore, if these reference models are also contaminated, our method only measures the effect relative to this base level of contamination. However, our leave-one-out experiment, presented in SS4.3, helps identify and exclude contaminated models, partially mitigating this limitation. Furthermore, it is important to note that accurate relative performance measurements are sufficient for both model selection and to assess methodological improvements, which are the most important use cases of benchmarks.

Further, our work uses an LLM to generate synthetic samples, introducing potential distributional biases into the synthetic benchmark \(D_{}\). We briefly discuss these biases here. Firstly, synthetic benchmark may contain more mislabeled samples. However, since these samples equally affect all models, ConStat accounts for this in its difficulty correction. Secondly, synthetic samples generated by a model are likely easier for that model itself to solve. Therefore, contamination results for the model used to generate the samples would be unreliable for sample-specific contamination detection. However, these limitations are not inherent flaws of ConStat, and can be mitigated by using more sophisticated synthetic benchmark generation techniques.

ImpactModel evaluation is a crucial part of LLM development, with benchmarks playing a key role in evaluating model performance on tasks like code generation, question answering, and summarization. Contamination of these benchmarks can inflate performance estimates, potentially misleading researchers and practitioners. To address this, ConStat provides a statistical framework to estimate the impact of contamination on model performance. This enables more accurate evaluations and allows for the removal of suspicious models from leaderboards, ensuring a fairer evaluation of model capabilities. Furthermore, it is important to note that ConStat can be applied to any model, not just LLMs, as long as the model's performance can be measured on a benchmark.

## 7 Conclusion

We present ConStat, a statistical framework designed to detect contamination and estimate its effect on model performance. Unlike existing methods, ConStat is based on a novel, performance-based definition of contamination and compares performance with various reference benchmarks to obtain a detailed contamination analysis that distinguishes between syntax-, sample-, and benchmark-specific contamination. We investigate ConStat's effectiveness in an extensive control study and demonstrate that it not only outperforms existing methods but also, in contrast to them, does not require prior knowledge about uncontaminated samples. Finally, we use ConStat to investigate contamination in popular models and find, among others, very high levels of contamination in Mistral-7b-v0.1 and Y1-34b and high levels of contamination in Llama-3-70b and Llama-2-Instruct-70b.