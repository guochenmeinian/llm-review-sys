# Scalable Constrained Policy Optimization for

Safe Multi-agent Reinforcement Learning

 Lijun Zhang1, Lin Li1, Wei Wei1, Huizhong Song1, Yaodong Yang2, Jiye Liang1

1. Key Laboratory of Computational Intelligence and Chinese Information Processing of

Ministry of Education, School of Computer and Information Technology,

Shanxi University, Taiyuan, Shanxi, China.

2. Institute for AI, Peking University, Beijing, China.

Correspondence to <weiwei@sxu.edu.cn>.

###### Abstract

A challenging problem in seeking to bring multi-agent reinforcement learning (MARL) techniques into real-world applications, such as autonomous driving and drone swarms, is how to control multiple agents safely and cooperatively to accomplish tasks. Most existing safe MARL methods learn the centralized value function by introducing a global state to guide safety cooperation. However, the global coupling arising from safety constraints and the exponential growth of the state-action space size limit their applicability in instant communication or computing resource-constrained systems and larger multi-agent systems. In this paper, we develop a novel scalable and theoretically-justified multi-agent constrained policy optimization method. This method integrates the rigorous bounds of the trust region method and the bounds of the truncated advantage function to provide a new local policy optimization objective for each agent. Also, we prove that the safety constraints and the joint policy improvement can be met when each agent adopts a sequential update scheme to optimize a \(\)-hop policy. Furthermore, we propose a practical algorithm called Scalable MAPPO-Lagrangian (Scal-MAPPO-L). The proposed method's effectiveness is verified on a collection of benchmark tasks, and the results support our theory that decentralized training with local interactions can still improve reward performance and satisfy safe constraints.

## 1 Introduction

With the advanced and rapid developments of reinforcement learning technology, many researchers have gradually shifted their focus from virtual simulation to real-world cyber-physical applications . In this process, safety challenges are inevitable, especially in multi-agent safety-critical scenarios, e.g., autonomous vehicle navigation , power grids , and drone swarms , in which agents perform complex cooperative tasks while adhering to a variety of local and system-wide limitations or constraints. These constraints can be derived from domain-specific knowledge and are intended to prevent damage to people or other environmental elements, such as equipment and infrastructure, or to prevent the inability to accomplish specific tasks or objectives. Take multi-robot control as an example. Each running robot must not take certain actions or not visit certain states, which may imply unsafe for itself, its collaborators, or the infrastructure of its environment . These widespread potential dangers exacerbate the difficulty of safety decision-making when applying MARL. Consequently, it is necessary to research the safe decision-making problem in MARL to ensure that agents can work together safely and cooperatively to accomplish tasks.

There are two main approaches concerning safe MARL techniques in the existing literature. The first type is shielded-based reactive methods [7; 8], which combine environmental dynamics and safety specification constraints to predict whether the actions chosen by agents will violate cost constraints. Nevertheless, due to the reliance on precise modeling knowledge, these methods may lead to poor performance when the accurate state transition model is unavailable. The second type formulates the safe MARL problem as a constrained Markov game, which requires agents to solve a constrained optimization problem, i.e., maximize total reward while avoiding violating cost constraints. To mention a few, several safe MARL variants, such as CMIX  and MAPPO-L , have been proposed, which learn the centralized value function to overcome policy conflicts caused by the partially observable and non-stationarity nature of the environment faced by each agent. Unfortunately, the global coupling arising from agents' safety constraints and the exponential growth of the state-action space size make the usability of these algorithms in instant communication or computing resource-constrained systems and the scalability in larger multi-agent systems become a bottleneck, limiting their applicability.

A promising approach for avoiding these shortcomings, which has received attention in recent years, is to exploit networked application-specific structures. For example, Safe Dec-PG  employs a primal-dual framework to find the saddle point between maximizing decoupled rewards and minimizing costs under a consensus network. However, it is worth noting that this approach still assumes each agent can access the global state and requires that the actions of all neighboring agents on the network be available. Recent research  proposes a scalable safe MARL approach based on the spatial decay assumption of the environment dynamics, which updates the policies of agents by the truncated gradient estimators depending on the local states and actions of the \(\)-hop neighboring agents. However, due to the dependence on the actions and states of its neighbors, this method necessarily involves joint training in a local area, which is still plagued by non-stationary issues. Motivated by the urgent desire for scalable learning in practical applications and the fact that meeting both safety constraints and joint policy improvement is challenging for most methods, we investigate a novel scalable safe MARL with theoretical analysis, practical algorithm, and simulation verification.

Specifically, we focus on decentralized learning settings without global observability, where each agent can only access the local state information of itself and its neighbors. Our main contributions are summarized as follows.

* We develop a novel scalable multi-agent constrained policy optimization method that eliminates dependence on the global state and other agent actions during each agent's training. Furthermore, we parameterize each agent's policy and propose a practical algorithm called Scalable MAPPO-Lagrangian (Scal-MAPPO-L).
* We quantify the maximum information loss regarding the advantage truncation based on two assumptions about the transition dynamics and policies. Then, each agent's new local policy optimization objective is provided by integrating the rigorous bounds of the trust region method and the bounds of the truncated advantage function. In addition, we prove that the safety constraints and the joint policy improvement can be guaranteed when updating the local policy with a sequential update scheme.
* Experimentally, we provide the results on several safe MARL tasks to evaluate the effectiveness of our proposed method and the sensitivity for the parameter \(\). The results support our theory that decentralized training with local interactions can still improve reward performance and satisfy safe constraints.

## 2 Preliminaries

### Constrained Markov game

Consider a safe MARL problem subject to multiple constraints, where each agent are associated with an underlying undirected graph \(=(,)\). Here, \(=\{1,,n\}\) is the set of \(n\) agents and \(\) is the set of edges. The problem can be formulated as a constrained Markov game \(,,,P,_{0},, ,,\). \(=_{i}^{i}\) and \(=_{i}^{i}\) are the state and action spaces, which are the product of local spaces; global state \(=(^{1},,^{n})\) and joint action \(=(^{1},,^{n})\) for any \(\) and \(\). \(P:\) is the probabilistic transition dynamics function, which satisfies the Dobrushin condition  as follows:

\[W^{ij}=_{^{j},^{j},^{-j} }\|P^{i}(|^{j},}^{-j})-P^{i}( |^{ j},}^{-j})\|_{1},\] (1)

where \(^{j}=(^{j},^{j})\) and \(^{ j}=(^{ j},^{ j})\) represent two different state-action pairs of the agent \(j\) respectively, and \(}^{-j}\) represents the state-action pair of the agent other than \(j\). The value of \(W^{ij}\) reflects the extent to which the local transition probability of agent \(i\) is affected by the state and action of agent \(j\). \(_{0}\) is the initial state distribution, \([0,1)\) is the discount factor. \(:\) is the joint reward function, \(=\{C_{j}^{i}\}_{1 j m}^{i}\) is the sets of cost functions (every agent \(i\) has \(m^{i}\) cost functions) of the form \(C_{j}^{i}:^{i}^{i}\), and finally the set of corresponding cost values is given by \(=\{c_{j}^{i}\}_{1 j m^{i}}^{i}\).

At each timestep \(t\), every agent \(i\) is in a state \(s_{t}^{i}\), and takes an action \(_{t}^{i}\) according to its policy \(^{i}=(^{i}|s_{t}^{i})\). Together with other agents actions, it gives a joint action \(_{t}=(_{t}^{1},,_{t}^{n})\) and the joint policy \(=_{i=1}^{n}^{i}(^{i}|_{t}^{i})\). The agents receive the reward \((_{t},_{t})\), meanwhile each agent \(i\) pays the costs \(C_{j}^{i}(s_{t}^{i},_{t}^{i}),\ j=1,,m^{i}\), and all agents have a joint goal, i.e., maximizing the expected total reward of

\[J()_{_{0}_{0}, _{0:}}[_{t=0}^{}^{t}(_ {t},_{t})],\] (2)

meanwhile satisfying every agent \(i\)'s safety constraints, written as

\[J_{j}^{i}()_{_{0} _{0},_{0:}}[_{t=0}^{} ^{t}C_{j}^{i}(_{t},_{t}^{i})] c_{j}^{i}, \ j=1,,m^{i}.\] (3)

### Spatial correlation decay

Exponential decay property [13; 14], also known as spatial correlation decay, is a powerful property associated with local interactions, which says that the impact of agents on each other decays exponentially in their graph distance. More information about spatial correlation decay is presented in Appendix B.1. Here, inspired by , we make the following two assumptions for the spatial correlation of the transition dynamics and policies. We use the notation \(^{i}(|_{_{k}^{i}})\) for \(\)-hop policies, where \(_{_{k}^{i}}\) represents the state of agent \(i\)'s \(\)-hop neighbors. It may be replaced with \(_{}^{i}\) for simplicity when it is clear from context.

**Assumption 2.1**.: (Spatial Decay of Correlation for the Dynamics) Assume that there exist \(>0\) in (1), for any agents \(i,j\), such that

\[_{i}_{j}e^{ d(i,j)}W^{ij},\] (4)

where \(d(i,j)\) represents the distance between agent \(i\) and agent \(j\), and \([0,2/)\) is a constant.

**Assumption 2.2**.: (Spatial Decay of Correlation for the Policies) Assume that there exist \(, 0\) such that for any agent \(i\), \(_{_{k}^{i}}_{_{k}^{i}}, _{_{k}^{-i}},_{_{k}^{-i}}^{}_{_{k}^{-i}}\), one have

\[_{_{_{k}^{i}},_{_{k }^{-i}},_{_{k}^{-i}}^{}}|^{i}(| _{_{k}^{i}},_{_{k}^{-i}}^{} )-^{i}(|_{_{k}^{i}},_{ _{k}^{-i}}^{})| e^{-}.\] (5)

Assumption 2.2 reveals how much information is lost compared with access to the global state and allows us to consider a policy class with the necessary properties for the optimal policy under Assumption 2.1. More information is stated in Appendix B.2.

## 3 Scalable constrained policy optimization

This section develops a novel scalable and theoretically-justified multi-agent constrained policy optimization method and proposes a practical algorithm, i.e., Scal-MAPPO-L, by parameterizing each agent's policy. Specifically, we first quantify the maximum information loss regarding the advantage truncation based on the spatial correlation decay property of the transition dynamics and policies. Then, the rigorous bounds of the trust region method and the bounds of the truncated advantage function are integrated to provide a new local policy optimization objective for each agent. Further, we prove that the safety constraints and the joint policy improvement can be guaranteed when updating the local police with a sequential update scheme, in which the policy update only depends on its action and the state of its \(\)-hop neighbors for each agent.

### Truncated advantage function estimator

For a standard safe MARL, the state-action value function (the definition can be seen in Appendix C.1) and advantage function of agent \(i\) yield that

\[Q_{}^{i}(,^{i})=_{^{-i} ^{-i}}Q_{}^{i}(,^{-i},^{i}),\] (6)

\[A_{}^{i}(,^{j},^{i})=Q_{}^{j,i}( ,^{j},^{i})-Q_{}^{j}(,^{j}).\] (7)

where \(\) represents the global state, \(^{-i}\) represents the actions of all other agents, and \(Q_{}^{j,i}(,^{j},^{i})\) represents the state-action value function of agent \(i\) and agent \(j\). Then, updating agents' policies with a sequential update scheme , the multi-agent joint advantage function \(_{}(,)\) can be written as a sum of sequentially unfolding multi-agent advantages of individual agents, as stated by the following lemma.

**Lemma 3.1**.: _(Multi-agent advantage decomposition). For any action \(^{i}\), \(i\), and the state \(\), the following identity holds_

\[_{}(,)=_{i=1}^{n}A_{}^{i}( ,^{-i},^{i}).\] (8)

Similar result to Lemma 3.1 can be seen in , and the proof is reported in Appendix C.2. Specifically, based on the multi-agent advantage decomposition in Lemma 3.1, the "surrogate" return is given as follows.

**Definition 3.2**.: Let \(\) be a joint policy, \(^{1:i-1}\) be some other joint policy of agents \(1:i-1\), and \(^{i}\) be a policy of agent \(i\). Then, the surrogate return can be defined as

\[L_{}^{1:i}(^{1:i-1},^{i}) _{_{},^{1:i-1} ^{1:i-1},^{i}^{1}}[A_{}^{i}(,^{1:i-1},^{i})].\] (9)

Building on Lemma 3.1 and Definition 3.2, one can obtain

\[L_{}^{1:i}(^{1:i-1},^{i})=_{ _{},^{1:i}^{1:i}}[ _{h=1}^{i}A_{}^{h}(,^{1:h-1},^{ h})].\] (10)

Further, recalling Assumption 2.1 and Assumption 2.2, we can quantify the maximum information loss regarding the advantage function as stated by the following proposition.

**Proposition 3.3**.: _For any agent \(i\), let the parameters \((,)=(,e^{-})\). If Assumption 2.1 and Assumption 2.2 hold, for any \(_{_{n}^{i}}=(_{_{n}^{i}}, _{_{n}^{i}})_{_{n}^{i}} _{_{n}^{i}}\), the exponential decay property of the advantage function holds, i.e., we have_

\[_{_{_{n}^{i}},_{_{n}^{-i}}, _{_{n}^{-i}}^{}}|A^{i}(_{ _{n}^{i}},_{_{n}^{-i}})-A^{i}( _{_{n}^{i}},_{_{n}^{-i}}^{} )|^{}.\] (11)

Proposition 3.3 shows that when the transition dynamics and policies correlation satisfy the exponential correlation decay property, the advantage functions also have exponential decay dependence on the states and actions of the more distant agents. The proof of Proposition 3.3 is reported in Appendix C.3. In addition, based on this proposition, we can obtain the following corollary.

**Corollary 3.4**.: _For any agent \(i\), let the parameters \((^{},)=(}{1-}+,e^{-})\), \(M^{i}\) is a constant. If Proposition 3.3 holds, the exponential decay property of the surrogate return holds, i.e., we have_

\[|L_{}^{1:i}(^{1:i-1},^{i})-L_{_{n}^{ i}}^{i}(_{}^{i})|^{}^{ }.\] (12)The proofs of Corollary 3.4 is reported in Appendix C.4.

Corollary 3.4 shows that the approximation error of \(L^{i}_{^{i}_{}}(^{i}_{})\) decreases exponentially with \(\) when the truncated advantage functions are bounded. The main advantage of using the estimator \(L^{i}_{^{i}_{}}(^{i}_{})\) lies in that every agent \(i\) only needs to know the action and state of its \(\)-hop neighbors, which can significantly reduce the communication burden and expand its application scenarios.

### Scalable constrained policy optimization

With the Definition 3.2, we see that Lemma 3.1 allows for decomposing the joint surrogate return \(L_{}(})_{ _{},}}[A_{}( ,)]\) into a sum over surrogates of \(L^{1:i}_{}(}^{1:i-1},^{i})\). Then, combining the rigorous bounds of the trust region method  and the bounds of the truncated advantage function, we can obtain the following proposition.

**Proposition 3.5**.: _Let \(\) and \(}\) be joint policies. Let each agent \(i\) sequentially solves the following optimization problem:_

\[^{i}_{}=*{arg\,max}_{^{i}_{ }}(L^{i}_{^{i}_{}}(^{i}_{})- ^{}^{}-^{i}_{}D^{}_{}(^{i} _{}|^{i}_{})),\] (13)

_where \((^{},)=(}{1-}+,e^{-})\), \(^{i}_{}=_{^{i}_{}},a^{i}} |A^{i}_{^{i}_{}}(s_{^{i}_{}},a^{i}) |}{(1-)^{2}}\), and \(D^{}_{}(^{i}_{}|^{i}_{})= _{s^{i}_{^{i}_{}}}D_{}(^{i}(  s_{^{i}_{}}),^{i}( s_{^{i} _{}}))\), then the resulting joint policy \(}\) will improve the expected return, i.e.,_

\[J(})-J()_{i=1}^{N}(L^{ i}_{^{i}_{}}(^{i}_{})-^{}^{ }-^{i}_{}D^{}_{}(^{i}_{}|^{i}_{})).\] (14)

The proof of Proposition 3.5 is reported in Appendix C.5. Similarly, by generalizing the result about the surrogate return in Equation (12), we can derive how the expected costs change when the agents update their policies. Specifically, we provide the following corollary.

**Corollary 3.6**.: _Let \(\) and \(}\) be joint policies. For any agent \(i\) and its cost index \(j\{1,,m^{i}\}\), the following inequality holds_

\[J^{i}_{j}(}) J^{i}_{j}()+L^{i}_{j,^{i}_{}} (^{i}_{})+^{}^{}+^{i}_ {j,}_{h=1}^{i-1}D^{}_{}(^{h}_{},^{h}_{}),\] (15)

_where \(L^{i}_{j,^{i}_{}}(^{i}_{})=_{ _{^{i}_{}}_{^{i}_{}},a^{i} ^{i}_{}}[A^{i}_{j,^{i}_{}}(_{ ^{i}_{}},a^{i})]\), \(^{i}_{j,}=_{^{i}_{}},a^{i}} |A^{i}_{j,^{i}_{}}(_{^{i}_{}},a^{ i})|}{(1-)^{2}}\), \((^{},)=(}{1-}+,e^{-})\), and \(M_{j}\) is a constant._

The proofs of Corollary 3.6 is reported in Appendix C.6.

From (14), we can derive that the lower bound for the difference between the new joint policy \(}\) and the old joint policy \(\) in terms of expected return can be decomposed into a cumulative sum of local surrogate TRPO policy objectives. From (15), we can derive the upper bound for the new joint policy \(}\), which can be used to restrict agents only to choose safe actions. Therefore, we use the objective, i.e., maximize the lower bound for the reward performance and minimize the upper bound for the safety constraints with a proper update size, as a surrogate for each agent. Then, we can obtaine the following theorem.

**Theorem 3.7**.: _The joint policy \(\) has the monotonic improvement property, \(J(}) J()\), as well as it satisfies the safety constraints, \(J^{i}_{j}(}) c^{i}_{j}\), for any agent \(i\) and its cost index \(j\{1,,m^{i}\}\), when the policy is updated by following a sequential update scheme, that is, each agent sequentially solves the following optimization problem:_

\[^{i}_{}=*{arg\,max}_{^{i}_{ }^{i}_{}}(L^{i}_{^{i}_{}}(^{i}_{})-^{}^{}-^{i}_{}D^{}_{ }(^{i}_{}|^{i}_{})),\] (16) \[s.t.\{^{i}_{}^{i}_{} D^{ }_{}(^{i}_{},^{i}_{}) ^{i}_{},.\] \[.J^{i}_{j}(_{})+L^{i}_{j,^{i}_{ }}(^{i}_{})+^{}^{}+ ^{i}_{j,}D^{}_{}(^{i}_{},^{i}_{ }) c^{i}_{j}-^{i}_{j,}_{h=1}^{i-1}D^{}_{}(^{h}_{},^{h}_{})\},\]_where \(^{i}_{}=\{_{_{k} i-1}_{1 j m^{h}} _{}-L^{i}_{j,}(^{i}_{})- ^{}^{i}}{^{i}_{j,}},_{h i+1}_{1 j m ^{h}}_{}}{^{i}_{j,}}\},\)_

\(^{i}_{}=_{},^{i}_{ }}|A^{i}_{^{i}_{}}(^{i}_{^{i}_{}}, ^{i})|}}{(1-)^{2}},^{i}_{j,}=_{},^{i}_{}}|A^{i}_{^{i} _{}}(^{i}_{^{i}_{}},^{i})|}}{ (1-)^{2}},(^{},)=(_{ }}{1-}+,e ^{-}),(^{},)=(_{} }{1-}+,e^{-}),^{j}_{j}=c^{h}_{j}-J^{h}_{j}(^{h}_{})-^{h}_{j,}_{l=1}^{i-1}D^{}_{}(^{l}_{},^{l}_{}).\)_

The proof of Theorem 3.7 is reported in Appendix C.7. It assures that if one follows (16) to update policies, agents will not only explore safe policies independently; meanwhile, every new policy will be guaranteed to result in performance improvement. It is worth mentioning that these two properties hold only under the condition that the only policy update restriction, i.e., \(^{i}_{}^{i}_{}\), is satisfied; this is due to the KL-penalty term in every agent's objective, i.e., \(^{i}_{}D^{}_{}(^{i}_{},^{i}_{ })\), as well as the constraints on cost surrogates.

### Algorithm

In this section, we focus on how to practically implement policy updates in Theorem 3.7 for each agent. Specifically, we parameterize each local policy \(^{i}_{^{i}_{}}\) by a neural network with parameter \(^{i}_{}\). At each policy update, every agent \(i\) maximizes its surrogate return subject to surrogate cost constraints and a form of expected KL-divergence constraint \(_{}(^{i}_{},^{i}_{}) ^{i}_{}\), which avoids computing KL-divergence at every state. Then, we introduce a scalar variable \(^{i}\) for any agent \(i\) and convert the constrained optimization problem from (16) into a min-max optimization problem with Lagrangian multipliers by subsuming the cost constraints. As such, the new optimization problem for any agent \(i\) is as follows:

\[_{^{i}_{}}_{^{i}_{1:m} 0} [_{_{^{i}_{}}_{^{i}_{ ^{i}_{}},^{i}^{i}_{^{i}_{}}}}[A^ {i}_{^{i}_{^{i}_{}}}(_{^{i}_{}},^{i})].\] (17) \[-_{u=1}^{m^{i}}^{i}_{u}(_{_{ ^{i}_{}}_{^{i}_{^{i}_{}},^{i} ^{i}_{^{i}_{}}}}[A^{i}_{u,^{i}_{^{i}_{} }}(_{^{i}_{}},^{i})]+d^{i }_{u})],\] \[_{}(^{i}_{^{i}_{ }},^{i}_{^{i}_{}})^{i}_{}.\]

where \(^{i}_{1:m^{i}}\) is a scalar variable, \(^{i}_{}\) is a parameter of neural network, and \(d^{i}_{u}\) is the cost-constraining value for agent \(i\).

Further, denoting

\[A^{i,()}_{^{i}_{^{i}_{}}}(_{^{i} _{}},^{i})=A^{i}_{^{i}_{^{i}_{}}}( _{^{i}_{}},^{i})-_{u=1}^{m^{i}} ^{i}_{u}(A^{i}_{u,^{i}_{^{i}_{}}}(_{ ^{i}_{}},^{i})+d^{i}_{u}),\] (18)

then the optimization problem in (17) can be rewritten as

\[_{^{i}_{}}_{^{i}_{1:m^{i}} 0}[_{ _{^{i}_{}}_{^{i}_{^{i}_{}}, ^{i}^{i}_{^{i}_{}}}}[A^{i,()}_{^{ i}_{^{i}_{}}}(_{^{i}_{}}, ^{i})]],_{}(^{i}_{^{i}_{}},^{i}_{ ^{i}_{}})^{i}_{}.\] (19)

To alleviate the complications caused by computing the KL-divergence constraint, we simplify it by adopting the PPO-clip objective , i.e., replacing the KL-divergence constraint with the clip operator and updating the policy parameter with first-order methods. The final optimization problem takes the form

\[_{^{i}_{}}_{^{i}_{1:m^{i}} 0}_{ _{^{i}_{}}_{^{i}_{^{i}_{}}, ^{i}^{i}_{^{i}_{}}}}[(^{ i}_{^{i}_{}}}{^{i}_{^{i}_{}}}A^{i,()}_{^{ i}_{^{i}_{}}}(_{^{i}_{}}, ^{i}),(^{i}_{^{i}_{}}}{^{i}_{ ^{i}_{}}},1)A^{i,()}_{^{i}_{^{i}_{ }}}(_{^{i}_{}},^{i}) )],\] (20)

where the clip operator replaces the policy ratio with \(1+\), or \(1-\), depending on whether its value is below or above the threshold interval. As such, agent \(i\) can learn within its trust region by updating\(_{}^{i}\) to maximize Equation (20), which only depends on its action and the state of its \(\)-hop neighbors and can be computed analytically.

To summarize, we give a procedure for each agent \(i\), name Scalable MAPPO-Lagrangian (ScalMAPPO-L), and provide its pseudocode (Algorithm 1) in Appendix C.8. The algorithm has a simple idea that each agent independently optimizes the surrogate objective (20), which only depends on its action and the state of its \(\)-hop neighbors for each agent. In the actual execution, some approximations of the surrogate objective are employed, the same as the MAPPO-L . Most of these approximations are traditional practices in RL, yet they may make it impossible for the practical algorithm to rigorously maintain the theoretical guarantees in Theorem 3.7.

## 4 Experiments

In this section, we evaluate our method via several numerical experiments. Our experiments aim to answer the following questions: First, how does the cost and reward performance of Scal-MAPPO-L compare with existing methods on challenging multi-agent safe tasks? Second, how does the different \(\) affect the performance of Scal-MAPPO-L, and could the advantage truncation effectively alleviate computational load?

### Experimental setup

Safe MAMuJoCo  is an extension of MAMuJoCo , which preserves the agents, physics simulator, background environment, and reward function and comes with obstacles, like walls or pitfalls. To answer the first question, we compare our method against the other PPO family algorithms, i.e., IPPO , HAPPO , and MAPPO-L  and choose three games from Safe MAMuJoCo: Safe ManyAgent Ant task with 2 agents (2 \(\) 3), 3 agents (3 \(\) 2) and 6 agents (6 \(\) 1) to evaluate their performance. Concerning the second question, we choose three games with different tasks and agent numbers from Safe MAMuJoCo: Safe ManyAgent Ant task with 6 agents (\(6 1\)), Safe Ant task with 8 agents (\(8 1\)), and Safe Coupled HalfCheetah task with 12 agents (\(12 1\)). We train Scal-MAPPO-L with the same network architecture and hyperparameters as the original MAPPO-L implementation. All reported results are averaged over three or more random seeds, and the curves are smooth over time.

Figure 1: Performance comparisons in terms of cost and reward on three Safe ManyAgent Ant tasks. Each column subfigure represents a different task, and we plot the cost curves (the lower the better) in the upper row and the reward curves (the higher the better) in the bottom row for each task.

### Results

**Comparisons with baselines:** Figure 1 shows the cost and reward performance of Scal-MAPPO-L and other PPO family algorithms on three Safe ManyAgent Ant tasks, where each agent in Scal-MAPPO-L is set to access the state of about half of the agents by adjusting the value of \(\). Specifically, \(=1\) in Safe ManyAgent Ant (\(2 3\)), \(=2\) in Safe ManyAgent Ant (\(3 2\)), and \(=3\) in Safe ManyAgent Ant (\(6 1\)). From Figure 1, we can see that compared to IPPO and HAPPO, on all three tasks, both Scal-MAPPO-L and MAPPO-L have fewer constraint violations and good performance (in terms of reward), i.e., they keep their explorations within the feasible policy space and quickly learn to satisfy safety constraints, which show that the safe learning algorithm is effective. Moreover, it should be further pointed out that Scal-MAPPO-L only accesses half of the state information on all tasks; it exhibits almost identical performance and constraint violations with MAPPO-L (which accesses the global state). This means that the sensitivity of each agent to the states and actions perturbations of distant agents is minimal, and Scal-MAPPO-L is effective. More experimental results are in Appendix D.

**Performance with different \(\):** Figure 2 shows the performance of Scal-MAPPO-L in different environments with varying values of \(\), where MAPPO-L accesses the global state. We have noticed that the algorithm's performance is consistently the lowest, and the cost is nearly the highest when \(=1\). However, when the truncation with \(>=3\), i.e., each agent has access to the states of at least two neighbors, we can observe that the performance of Scal-MAPPO-L improves considerably and can approach or even outperform MAPPO-L in some environments, such as \(=6\) in the Safe Ant task (\(8 1\)). This may be due to the fact that the impact of far-away agents' states and actions on the agent's decision is almost negligible in many cases. However, for algorithms with global communication, such as MAPPO-L, the difficulty of extracting useful information from many messages may lead to lower performance. Overall, these results underscore the efficiency of Scal-MAPPO-L since it employs a smaller communication radius that can significantly reduce the computation.

## 5 Related work

### Safe RL

Safety is one of the bottlenecks preventing RL use in real-life applications, such as physical robotics , medical applications  and autonomous driving . It has become a research hotspot in recent years and a growing number of safe RL approaches, such as primal-dual methods , formal methods , Lyapunov methods , Gaussian processes methods , and safety-augmented

Figure 2: Performance comparisons in terms of cost and reward on Safe ManyAgent Ant task, Safe Ant task, and Safe Coupled HalfCheetah task. In each task, the performance of Scal-MAPPO-L with different \(\) and MAPPO-L are demonstrated.

methods , have been developed. However, when it comes to multi-agent systems, a great challenge is exacerbated by policy conflicts caused by multiple agents interacting within a shared environment and learning simultaneously. In other words, each agent has to not only satisfy its safety constraints but also consider the conflicts between its safety constraints and maximization reward as well as the safety constraints of others so that their joint behaviors have a safety guarantee. In order to address the above issue, CMIX  and MAPPO-L  have been proposed with the in-depth study of MARL. These algorithms follow the centralized training and decentralized execution (CTDE) framework [29; 30; 16], which learns the centralized value function by introducing the global state. Unfortunately, the global coupling arising from agents' safety constraints and the exponential growth of the state-action space size make the usability in communication or computing resource-constrained systems and the scalability of these algorithms in larger multi-agent systems become a bottleneck, limiting their applicability. Recent works [11; 12] have provided some theoretical results to avoid these shortcomings. However, most of these methods fail to ensure both safety guarantee and joint policy improvement under a decentralized learning framework under a decentralized learning framework, which motivates us to investigate a new scalable and theoretically-justified safe MARL method.

### Centralized training

In cooperative MARL settings, the training of agents can be broadly divided into two paradigms, namely centralized and decentralized . The centralized training paradigm describes agent policies updated based on mutual information, which can be further differentiated into the centralized and decentralized execution framework. Centralized training and centralized execution (CTCE) utilize the centralized evaluator and executor to learn the joint policy of all agents [32; 18]. The obvious flaw is that its applicability is limited because its implementation requires the premise that instantaneous and unconstrained information exchange between agents. Recently, centralized training and decentralized execution (CTDE) has become the most popular framework [30; 20; 16; 10], since the fact that it addresses the non-stationarity issue with the centralized value function, and removes the dependency on global state and actions during execution. Many experiment results demonstrate state-of-the-art performance on challenging tasks, such as unit micromanagement in StarCraft II . However, although this framework does not require agents to access the global state during execution, the reliance on the global state only during training still poses a significant barrier to real-world applications, especially in scenarios where communication and computational resources are constrained [34; 35].

### Decentralized training

In a decentralized learning paradigm, each agent learns independently and accesses local observations rather than the global state; the idea is direct, comprehensible, and easy to realize in practice [36; 34]. There are two mainline research approaches concerning decentralized learning in the existing literature. One line of research pursues fully decentralized learning, such as independent Q-learning (IQL) [37; 38] and independent actor-critic (IAC) [39; 20], which make agents directly execute the single-agent Q-learning or actor-critic algorithm individually. Another line of research allows agents to establish rational local communication networks, such as setting certain distance or neighbor graphs [40; 41], which is also known as networked MARL. Communication networks expand agents' perceptual capabilities and mitigate, to some extent, the decision conflicts or errors caused by partial observability. However, it is worth noting that each agent's decision violates the stationary condition of the Markov Decision Process (MDP) in both lines of research, even though they achieve good experimental results on a collection of benchmark tasks. It poses a significant challenge to the convergence analysis of algorithms in the short term. Recently, motivated by good experiment performance, some studies have tried to provide theoretical support for these phenomena. To mention a few, Qu et al.  introduced the spatial correlation decay property into the field of MARL and carried out a series of fundamental results [15; 43; 12], which broadened the research avenues of scalable MARL. However, all of these studies mainly focus on (natural) policy gradient methods with average rewards or general utilities and have not yet been combined with trust region methods, which rigorously enable RL agents to learn monotonically improving policies. Furthermore, only recent research  considers both safety and scalability for MARL. Our results build upon the scalable MARL family of works [42; 15; 43; 12] and PPO-based (TRPO-based) MARL family of works [16; 10].

Conclusion

Safety is a tremendous challenge for MARL when applied to real-world scenarios. In this paper, we quantize the approximation errors arising from policy implementation and advantage truncation and then derive a novel lower bound for joint policy improvement and an upper bound for the safety constraints for every agent. Furthermore, we propose a novel scalable and theoretically justified multi-agent constrained policy optimization method that follows a sequential update scheme to optimize \(\)-hop policies. Finally, we introduce a practical constrained policy optimization algorithm called Scal-MAPPO-L and experimentally validate the effectiveness of the proposed algorithm on a collection of benchmark tasks.