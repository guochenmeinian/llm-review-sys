# AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents

Chang Ma\({}^{, the inherent complexity in agent tasks characterized by multi-round interactions distinguishes them significantly from other language tasks. Due to this complexity, there is a pressing need to delve into the details and gain a deeper understanding of how models function during the process. Nonetheless, most current evaluations predominantly rely on the final success rate as their metric, which provides limited insights into these intricate processes (Liu et al., 2023; Wang et al., 2023; Yao et al., 2023; Liu et al., 2023; Mialon et al., 2023). This simplified evaluation is particularly inadequate in challenging environments where most models demonstrate nearly zero success rates, consequently blurring finer distinctions and obscuring underlying mechanisms (Liu et al., 2023).

To address these issues, we introduce AgentBoard, a benchmark designed for multi-turn LLM agents, complemented by an analytical evaluation board for detailed model assessment beyond final success rates. AgentBoard encompasses a diverse set of 9 unique tasks and 1013 exemplary environments, covering a range from embodied AI and game agents to web and tool agents. Each environment, whether newly created or adapted from pre-existing ones, is carefully crafted and authenticated by humans to ensure multi-round and partially observable characteristics in a unified manner. Notably, we have defined or manually annotated subgoals for each data sample, introducing a unified _progress rate_ metric to track the agents' detailed advancements. As we will demonstrate in SS4.2, this metric uncovers significant progress made by models that would otherwise appear trivial due to negligible differences in success rates.

Along with the benchmark, we develop the AgentBoard evaluation framework as an open-source toolkit that features an analytical web panel to examine various dimensions of agent abilities through interactive visualization. The toolkit offers a unified interface, providing users with easy access and effortless customization options. As shown in Figure 1, the AgentBoard toolkit currently supports analysis and visualization on fine-grained progress rates tracking, performance breakdown for hard and easy examples, detailed performance across various sub-skills, long-range interaction assessment, grounding accuracy, and trajectory. This detailed evaluation is crucial for acknowledging the progress of LLM agents and for guiding the development of more robust LLM agent models. The comparison between AgentBoard and previous works is shown in Table 1.

We evaluated a range of proprietary and open-weight LLM agents using AgentBoard, obtaining insights into the current landscape of LLMs as agents. Key findings include: (1) GPT-4, unsurprisingly, outperforms all other models by exhibiting extensive proficiency across distinct agentic abilities with Llama3 (Touvron et al., 2023) and DeepSeek LLM (DeepSeek-Al et al., 2024) taking the lead; (2) Strong LLM agents are characterized by their capability for _multi-turn_ interaction with the environment, an ability that is notably lacking in most open-weight models; (3) Emergent agentic abilities are strongly dependent on basic abilities like grounding, world modeling, and self-reflection. Current proprietary models typically demonstrate comprehensive agentic abilities, while open-weight LLMs show varying deficiencies. Through AgentBoard, we highlight the importance of analytic evaluation of LLM agents. The detailed evaluations provided by AgentBoard and its open-source toolkit are expected to significantly contribute to the further development of LLM agents.

Figure 1: The illustrative overview of AgentBoard. AgentBoard consists of a 9 diverse tasks. Agents interact in multi-rounds with partially-observable environments to achieve each subgoal. Furthermore, AgentBoard provides an open-source analytical evaluation framework, as shown in the figure.

## 2 AgentBoard - Overview

AgentBoard is a unified, open-source benchmark for evaluating LLM agents that adheres to five key principles: _task diversity, multi-round interaction, partially-observable environments, fine-grained metrics, and analytical evaluation_, as shown in Table 1. Our commitment to these principles manifests in three key areas:

* _Task Diversity and Uniformity_, where we carefully curate nine diverse environments across four scenarios, ensuring they require multi-round interactions, are fully text-based and primarily partially-observable. This contrasts with many existing LLM agent benchmarks, which are often solvable in a single round, derived from fully-observable tasks like MMLU, or focus on a specific type of task, as demonstrated in Table 1. Compared to AgentBench (Liu et al., 2023), in particular, our choice of tasks makes AgentBoard planning-heavier, where the results on AgentBoard well-correlates with the scores on traditional reasoning and coding benchmarks, while the AgentBench scores strongly correlated with scores on the knowledge test MMLU, as illustrated in Ruan et al. (2024). We elaborate on the choice of tasks and their adaptation in SS3.
* _Fine-grained Progress Rate_, where AgentBoard is the first to propose a fine-grained progress rate metric tracking the intermediate progress of different agents. This metric distinguishes our benchmark in tracking minimal improvement in LLM agent performances. Such a capability is crucial in current endeavors to develop stronger open-weight LLMs, providing detailed insights that are essential for incremental advancements in agent capabilities. We provide detailed introduction for this metric and its annotation process in SS2.2.
* _Comprehensive Analysis_, where AgentBoard is the first LLM Agent benchmark to expand metrics beyond mere success rate and scores to include detailed analyses. As illustrated in Figure 1, such a comprehensive evaluation includes (1) fine-grained progress rates tracking different agents, (2) grounding accuracy, (3) performance breakdown for hard and easy examples, (4) long-range interactions, (5) analyses of performance across various sub-skills, and (6) trajectory with friendly visualization. We elaborate these analyses in our experiments at SS4. Additionally, AgentBoard provides an web interface2 through Wandb dashboard that offers interactive visualizations of these analyses during evaluation. We perform a case study on the panel in SS5. 
### A Unified Multi-Round Reflex Agent

Preliminaries:An LLM agent receives textual world descriptions, chooses a text action, and gets feedback detailing state changes and any action errors. Interaction with these environments can be modeled as a special case of Partially Observable Markov Decision Processes (POMDPs) defined by tuple \( g,,,,\), with goal \(g\), state space \(\), valid actions space \(\), observation space (including environment feedback) \(\), transition function \(:\). An agent with policy \(\) makes prediction at time step \(t\) based on goal \(g\) and memory \(m_{t}=\{o_{j},a_{j},o_{j+1},a_{j+1}, o_{t}\},0 j<t\), which is a sequence of actions and observations. This trajectory of the agent \(=[s_{0},a_{0},s_{1},a_{1}, s_{t}]\) is formulated by policy and environmental state transitions, such as

\[p_{}()=p(s_{0})_{t=0}^{T}(a_{t}|g,s_{t},m_{t})(s_{t+ 1}|s_{t},a_{t})\] (1)

    &  &  **Multi-round** \\ **Interaction** \\  } &  **Partially-Observable** \\ **Environments** \\  } &  **Fine-grained** \\ **Progress Metrics** \\  } &  **Analytical** \\ **Evaluation** \\  } \\  AgentBench (Liu et al., 2023) & ✔ & ✗1 & ✔ & ✗ & ✗ \\ GAIA (Mialon et al., 2023) & ✔ & ✔ & ✔ & ✗ & ✗ \\ MINT (Wang et al., 2023) & ✔ & ✔ & ✗1 & ✗ & ✗ \\ API-Bank (Li et al., 2023) & ✔ & ✔ & ✔ & ✗ & ✗ \\ ToolEval (Qin et al., 2023) & ✔ & ✔ & ✔ & ✗ & ✗ \\ LLM-Eval (Lin and Chen, 2023) & ✔ & ✗ & ✗ & ✗ & ✗ \\ AgentBoard & ✔ & ✔ & ✔ & ✔ & ✔ \\   

Table 1: AgentBoard differs from other LLM benchmarks by providing a comprehensive framework that integrates all four guiding principles within its evaluation system. 1 Notably, AgentBench entails both single and multi-round tasks, with mainly the former differentiating open-sourced models. 5 The GAIA benchmark focuses solely on question answering tasks. 6MINT benchmark primarily includes fully-observable environments tasks derived from conventional evaluations such as HumanEval and GSM8K.

The Unified Framework:AggentBoard unifies all tasks around a general framework where the agent receives observations \(o_{t}\) and performs actions \(a_{t}\), causing deterministic state transitions \(:(s_{t},a_{t})_{t+1}\) based on real-world dynamics. A feedback function \(f\) is also defined in the environment to derive feedback from each interaction round \(o_{t}=f(s_{t},a_{t})\). This feedback includes: (1) list all valid actions when the agent uses help actions such as _check valid actions_; (2) execute valid action \(a_{t}\) and return a description of the changed state \(s_{t+1}\); (3) issue errors when the agent performs an action outside of the action space.

We aim to use a simplistic agent framework to showcase LLM basic agentic abilities. As shown in Figure 2, our agent makes decisions based on its memory of past perceptions, similar to how humans learn from experience and adapt. The implementation of the reflex agent assessed in this paper adopts an act-only prompting strategy in line with recent studies (Liu et al., 2023; Zhou et al., 2023; Xu et al., 2023b), detailed in the right part of Figure 2, while other prompting strategies can be easily incorporated into our open-source framework. Also, LLM agents tend to struggle with limited context lengths in long interactions, failing to retain full history. Following the _"sliding window"_ method from LangChain (Chase, 2022), we focus on recent, more impactful interactions (Puterman, 1990) within context constraints. This differs from previous practices that stop the agent when context limits are surpassed (Liu et al., 2023a; Wang et al., 2023b), allowing for extended, intricate interactions in our approach. We provide ablation results such as ReAct prompting (Yao et al., 2023) and other long-context processing techniques to justify our framework in Appendix F.

### Fine-grained Progress Rate

Recent studies highlight the predominant use of success rate as the main metric for agent evaluation, which fails to capture the nuances of partial task completion by language model agents (Liu et al., 2023a; Li et al., 2023). This approach does not differentiate between near-complete tasks and minimal task execution, treating both as equivalent failures. Alternative metrics like reward scores are available but lack standardization (Chevalier-Boisvert et al., 2019; Wang et al., 2022). To mitigate this issue, we introduce a _progress rate_ metric to accurately reflect LM agents' goal attainment at various stages.

  
**Task** & **Goal \& Trajectory** \\    & **Goal: put a clean egg in microwave.** \\   & Step 02 Action: open fridge 1 Observation: You open the fridge 1. The fridge 1 is open. In it, you see a apple 2, a egg 1, a lettuce 1, a pan 2, a plate 1, and a tomato 1. \\    & **Goal: Get out of the house. Then escape the city without getting caught via driving.** \\   & Step 2 Action: take a shower Observation: You step into the shower, turn on the water, and within a few moments you feel like a new man. But no time to dawdle - you hop back out again and dry off in record time \\    & **Goal: Display the list of issues in the kkroening/fmpeq-python repository that have labels related to questions** \\   & Step 05 Action: click  & Observation: Tab 0 (current): Issues - Karl Kroening / ffmpeep-python GitLab  \\   & RootWebArea [Issues - Karl Kroening / ffmpeep-python GitLab focused: True  link & Progress Rate 0.25 \(\) 0.50 \\    & **Goal: In “Sheet17”, calculate and complete the “Profit” of the products in the table based on the sales information of the products. And then, sort the table in descending order by “Profit”.** \\   & Step 07 Action: update_cell_by_formula with Action Input: \{“operator”: “PRODUCT”, “start_position”: “CS”, “end_, \(s_{t+1}\); (3) issue errors when the agent performs an action outside of the action space. \\   & “end_position”: “DS”, “result_position”: “ES” & Observation: [[ Product, 'Category’,... & Progress Rate 0.47 \(\) 0.49 \\   

Table 2: Examples of goals for the 4 task categories in AgentBoard, along with a sampled step of the trajectory and progress rate. The trajectory is generated by GPT-4. Some lengthy observations are omitted with “...” for brevity. The task name in the table uses an abbreviation, the full name can be found in §3.

Figure 2: (Left) A structural overview of the reflex agent, which iteratively interacts with the environment and makes next step predictions based on the goal and history. (Right) An example of a prompt for our reflex agent.

In each round of interaction, a progress rate, denoted as \(r_{t}\), is assigned to evaluate the agent's advancement towards the goal state \(g\). As the agent moves through the states \(}=[s_{0},,s_{t}]\), we assess its progress using a matching score \(f(,g)\) that quantifies the similarity between the current state and the goal state. The initial value of \(r_{t}\) is set to \(0\), indicating no progress. The progress rate \(r_{t}\) reflects the highest matching score achieved, reaching 1 when the task is completed. The progress rate is formulated as below:

\[r_{t}=r_{t}^{}=_{i,0 i t}f(s_{i},g),& { if $f(,g)$ is continuous}\\ r_{t}^{}=_{i,0 i t}(_{k=1}^{K}f( s_{i},g_{k})),&\] (2)

The function \(f(,g)\) measures state similarity in tasks, such as comparing table states in manipulation activities. It works well for tasks with direct state comparisons but is less effective for tasks with ambiguous intermediate states, where progress is hard to measure. We mitigate this by introducing a discrete matching score to assess how closely intermediate states align with defined subgoals. We begin by decomposing the overall goal \(g\) into a sequence of subgoals \(=[g_{1},,g_{K}]\), with each subgoal leading into the next. The authors manually label each subgoal, which is then checked and adjusted through a rigorous process described in SS3.2. Notably, we manually edit the problems for a simpler setup where each final goal aligns with a unique subgoal sequence, and this affects only 5% of the original problems (a detailed descriptions for our adaptations are in Appendix L.1). Note that while we maintain a unique subgoal sequence, this allows for a diverse set of trajectories, e.g. taking deotors when accomplishing the task. As an example, for task "clean an egg and put it in microwave", the necessary subgoals would be "open the fridge" \(\) "taking an egg from the fridge" \(\) "clean the egg with sinkbasin" \(\) "put the egg in the microwave". Each subgoal \(g_{i}\) is associated with a labeled state that indicates its completion. To evaluate the match between an agent state and a subgoal, we employ a regular-expression-based matching function denoted as \(f(,g_{i})\{0,1\}\) and the progress rate as \(r_{t}^{}\) in Equation 2.

We employ progress rate along with the commonly used success rate metric, which computes the proportion of tasks completed within \(T\) interactions.

## 3 AgentBoard - Task Composition

AgentBoard features four task scenarios: embodied, game, web, and tool. These tasks are selected for their diversity and relevance to everyday activities, offering broader scenario coverage than tool-using benchmarks like MINT (Wang et al., 2023b) and ToolEval (Qin et al., 2023b). We specifically select tasks that require multi-round interactions in partially-observable environments, creating a realistic and challenging setting for agents. Examples of goals and trajectories are displayed in Table 2, and task statistics are summarized in Table 14. Further details on the environments and annotation are provided in Appendix K and L.

### Environments

_Embodied -_ **AlfWorld (ALF)**(Shridhar et al., 2021)** are household tasks that require agents to explore surroundings and perform commonsense tasks like "put two soapbars in garbageecan". This task uses subgoal-based progress rates (Appendix L.2) and the original success rate of the environment as metrics.

_Embodied -_ **ScienceWorld (SW)**(Wang et al., 2022)** is a challenging interactive text environment testing scientific commonsense, e.g."measure the melting point of the orange juice". The current subgoals provided by SW do not accurately reflect a language model's performance due to their sparsity and uneven weighting, as further explained in Appendix L.3. To rectify this, we re-annotate the subgoals for calculating progress rate \(r_{t}^{}\). We also re-annotate instructions on tool usage and rooms to explore to ensure the uniqueness of subgoal sequence for task completion.

_Embodied -_ **BabyAI (BA)**(Chevalier-Boisvert et al., 2019)** is an interactive 20x20 grid environment where agents navigate and interact with objects within a limited sight range. The original setup uses image-based observations and tensor-based actions like "0: move left". We adapted it to include a textual action space and descriptive textual observations. Furthermore, we re-annotate subgoals for progress rates to fix subgoal sparsity problem in the original environment (Appendix L.4).

_Game -_ **Jericho (JC)**(Hausknecht et al., 2020)** is a collection of text-based game environments staged in fictional worlds. This task is unique as it requires strong world modeling ability : agents could only gain information about the magic world through exploration and interaction. The original games are too long (need 50-300 steps to finish for LLM agents with fixed context length. Therefore we rewrite the goal of each adventure to restrict the games to be finished within 15 subgoals.

_Game -_ **PDDL (PL)**(Vallati et al., 2015)** is a set of strategic games defined with Planning Domain Definition Language (PDDL). We selected 4 representative games, _Gripper, Barman, Blocksworld, Tyreworld_ to benchmarkLLM agents in diverse scenarios. We adapted the environment implementation (Silver and Chitnis, 2020) written in PDDL expressions to provide text-based observations for agents, enabling a natural language interface for LLM. We measure progress using a matching score, \(r_{t}^{}\), which assesses similarity between the current and goal states (Appendix L.6).

_Web - WebShop (WS)_(Yao et al., 2022)_ is a network-based simulation environment for e-commerce experiences. Based on the original implementation method (Yao et al., 2022; Shinn et al., 2023), we have improved the error feedback, including refining the observation for exceeding page limits and interacting with wrong objects. These enhancements contribute to the effective interaction of the LLM agent with the environment. We also measure the distance of the current state to the final goal as the progress rate and expand the product scoring rules from Yao et al. (2022) to derive the score (Appendix L.7).

_Web - WebArena (WA)_(Zhou et al., 2023)_ is a real web environment featuring various scenarios including business content and discussion forums. To obtain the progress rate, we revised the existing method for calculating the final score (Zhou et al., 2023) and continuously computed the progress rate at each step, fusing the URL matching score with the content matching score, as detailed in Appendix L.8.

_Tool - Tool-Query (TQ)_ consists of three sub-environments: Weather, Movie and Academia Environment. This tasks primarily involves querying information from respective databases by planning the use of diverse query tools. We manually curate diverse problems for each environment. We also annotate subgoals to compute the progress rate \(r_{t}^{}\) (Appendix L.9). While LLMs often provide direct answers in question answering, we only consider an answer correct if the model follows the appropriate trajectory to access the databases. To ensure this, we design questions that cannot be answered directly by state-of-the-art LLMs and provide in-context examples to guide the LLM in querying the databases effectively.

_Tool - Tool-Operation (TO)_ includes two sub-environments: Todo list management and Google Sheet Operations. These tasks involve using tools to access and modify information. The progress rate in the Todo Environment is measured using \(r_{t}^{}\), similar to Tool-Query Environments. In the Sheet Environment, progress is evaluated using \(r_{t}^{}\), which uses a matching score between the cells of the current and golden table (Appendix L.10).

### Annotation Verification and Metric Justification

After human annotation, we manually verified our labeled subgoals through multiple verification stages to ensure its quality, as detailed in Appendix J. More importantly, we conduct a user study to justify our proposed progress rate metric, asking human annotators to assess the progress of model trajectories and then evaluating its correlation with our automatic progress rate metric. Specifically, we gather 60 model trajectories for each of the 8 tasks from three strong LLMs--GPT-4, GPT-3.5-Turbo, and Deepseek-67b. Each trajectory is assessed by four authors of the paper. The individual human rater is asked to select progress score from \(\{0\%,25\%,50\%,75\%,100\%\}\) given trajectories and task descriptions, without seeing the automatic score. Mean of four scores is taken as the final human score for every trajectory. We show the Pearson correlation between human progress score and the progress rate in Figure 3, and report Fleiss'kappa \(\) to reflect inter-annotator agreement. Results show that progress rate highly correlates with human assessment on the progress where the Pearson correlation exceeds 0.95 on all tasks, and substantial agreement is reached among the annotators.

## 4 Experiments

We conduct a comprehensive evaluation of popular LLMs, including proprietary and open-weight models. Firstly, we report the success rate and progress rate of these agents. Then, we perform detailed analysis of the

Figure 3: Human verification of AgentBoard progress rate. The Pearson correlation coefficients \(\) compare human evaluations with the progress rates on 60 different trajectories per task. The Fleiss’ kappa \(\) reflects inter-annotator agreement. The trajectories are generated by three models: GPT-4, GPT-3.5-Turbo, and DeepSeek-67b.

performance of agents and measure the various abilities of LLM agents, as part of the AgentBoard evaluation automatically supported by our open-source toolkit.

### Evaluation Setup

We implement the agent as described in SS2.1. We use a one-shot in-context example in our prompt, in addition to task instructions. For the detailed prompt, please refer to Appendix N. We benchmark a series of strong proprietary and open-weight models. For open-weight models, we assess the corresponding chat version of them. Please refer to Appendix I for detailed setup.

### Main Results

Progress Rate is more informative and discriminative than success rate.The success rate and progress rate across various tasks and categories are presented in Table 3. Regarding the overall performance, the progress rate serves as a more effective differentiator between models. For example, Llama2-13b and Mistral-7b exhibit similarly negligible success rates (2.1% and 3.9%, respectively), but their progress rates differ significantly: 18.9% for Llama2-13b and 24.6% for Mistral-7b. This disparity suggests that Mistral-7b generally outperforms Llama2-13b. For models with substantial differences in success rates, such as Text-Daviinci-003 outperforming Llama2-70b by 11.7% in success rate, Text-Daviinci-003 leads the progress rate by 13.4% as well, which indicates the consistency in performance disparity between significantly different models. Investigating the agent performance on specific tasks, progress rate is often able to differentiate models that have similar success rates - for instance, on the Embodied AI and Game categories, the success rates of most of the open-weight models are similarly low, while they are able to make meaningfully different progresses. Also, the success rate can be influenced by specific characteristics of agents, for example, an agent like CodeLlama-34b often fails to generate the action "finish" when performing tool-using tasks, leading to a higher progress rate and lower success rate compared to CodeLlama-13b. In contrast, progress rate is less susceptible to these agent-specific features as it reflects the overall ability of the agent at each step.

Proprietary models outperform the open-weight ones.The performances of _general_ LLMs as agents overall follow the scaling law (Kaplan et al., 2020). Larger LLMs outperform their smaller counterparts in most tasks. For instance, the 70 billion parameter models, such as Llama3-70b, DeepSeek-67b, and Lemur-67b, demonstrate superior performance compared to the 7-13 billion parameter models like Mistral-7b and CodeLlama-13b. Notably proprietary models still outperform the best open-weight models: GPT-4 significantly surpasses other LLMs, achieving an average progress rate of 70.0%, followed by Claude and Gemini.

Strong coding skills help agent tasks.In the realm of open-weight LLMs, Code LLMs demonstrate a notable advantage compared to other open-weight models: For instance, CodeLlama-34b outperforms Llama2-70b by 6.2% in terms of progress rate, while the significantly smaller CodeLlama-13b surpasses Llama2-70b by 2%. Lemur-70b, which is continual pretrained on code, also significantly surpasses Llama2-70b. This suggests that incorporating a greater volume of code in training data may enhance performance in agent tasks. Additionally,

    &  &  &  &  &  \\  & **ALF** & **SW** & **BA** & & & **JC** & & **PL** & **WS** & **WA** & **TQ** & **TO** & **Avg.** \\  GPT-4 & **65.5/43.3** & **78.8/52.2** & **70.7/56.2** & **52.4/35.0** & **81.2/61.7** & **76.5/39.0** & **39.4**/**15.1** & **85.1/63.3** & **80.8/60.0** & **70.0/47.9** \\ Claude2 & 34.1/24.6 & 32.0/11.4 & 14.8/17.35 & 20.4 / 0.6 & 61.4/40.0 & 74.6/37.3 & 36.4 / 8.6 & 73.5/43.5 & 59.6/27.5 & 48.9/26.2 \\ Gemini1.5-Flash1 & 40.9/15.7 & 17.8 / 4.4 & 50.6/38.4 & 11.1 / 0.2 & 35.5 / 6.7 & 72.3/24.7 & 32.4/11.1 & 73.9/46.7 & 68.6/37.5 & 43.5/20.6 \\ Claude3-Haiku & 20.7/ 2.2 & 43.7/14.4 & 34.4/24.1 & 34.7/10.0 & 31.9/13.3 & 73.6/30.3 & 26.8/17.1 & 61.9/24.0 & 62.4/27.5 & 43.3/17.6 \\ Llama3-70b & 29.6/12.7 & 30.4 / 7.8 & 41.1/27.7 & 16.0 / 5.0 & 32.2/20.0 & 74.6/29.9 & 35.6/12.6 & 52.6/36.7 & 65.2/30.0 & 41.9/20.2 \\ GPT3-5-Turbo & 53.6/17.1 & 31.9/15.9 & 51.9/13.9 & 19.9 & 52.0 / 5.0 & 5.0 / 6.45.1 & 25.5 / 4.6 & 49.4/40.0 & 37.7 / 5.4 & 41.9/17.5 \\ ALMA7-70b & 53.4/42.5 & 15.4 / 1.1 & 31.7/28.6 & 16.2 & 50.3 / 26.4 & 16.7/ 73.6/32.7 & 34.5/11.3 & 66.5/38.3 & 26.5/75.5 & 40.2/20.4 \\ DeepSeek-67b & 34.5/20.9 & 36.1/10.0 & 31.7/22.3 & 13.7 / 0.2 & 22.6/ 6.7 & 72.7/ 31.9 & 23.9/ 5.7 & 71.4/40.0 & 40.5/17.5 & 38.5/17.2 \\ Text-Daviinci-003 & 18.9/ 0.8 & 29.9 / 8.7 & 15.7/14.3 & 28.6/10.0 & 32.1/17.1 & 77.2/39.5 & 16.2/ & 55.0/33.8 & 36.2/22.5 & 37.2/16.2 \\ GPT3-5-Turbo-168 & 25.2/ 4.5 & 2.2 / 0.0 & 45.1/33.9 & 16.0 & 12.6/ 0.2 & 33.3 / 78.8/27.9 & 23.7/ 6.1 & 5.91/31.7 & 39.6/15.0 & 34.2/13.6 \\ AgentLM-70b & 58.4/50.7 & 13.0 / 1.1 & 38.0/27.7 & 8.8 / 0.0 & 13.0/ 3.3 & 72.9/11.1 & 13.0/ 5.3 & 50.5/13.3 & 31.8/0.0 & 33.3/14.7 \\ Lemur-70b & 10.8/ 0.7 & 33.4/ 5.6 & 19.4 / 9.8 & 10.1/ 0.0 & 9.7/ 3.3 & 71.8/11.6 & 12.2/ 3.3 & 72.0/28.3 & 37.7/12.5 & 30.8/ 8.3 \\ CodeLlama3-34b & 11.3/ 3.0 & 3.5 / 0.9 & 19.3/13.4 & 15.5 / 0.5 & 18.5/ 3.3 & 71.7/23.5 & 21.2/ 4.1 & 60.0/13.3 & 48.8/ 7.5 & 30.0/ 7.6 \\ Llama3-8b & 14.1/ 0.7 & 38.8/10.0 & 36.7/22.3 & 10.4 / 0.0 & 20.1/ 3.3 & 68.7/17.5 & 8.3/ 16.4 & 44.2/ 0.2 & 28.7/ 0.0 & 30.0/ 6.2 \\ CodeLlama3-13b & 13.4/ 2.2 & 9.6 / 2.2 & 22.2/17.0 & 0.0 / 0.9 & 9.3 / 1.7 & 65.5/25.9 & 17.7/ 3.7 & 52.5/25.0 & 41.8/12.5 & 25.8/10.0 \\ Llama2-70b & 13.2/ 3.0 & 2.6 / 0.0 & 30.0/19.6 & 7.8 / 0.0 & 8.1/ 1.7 & 35.6/13.1 & 11.6/ & 43.3 & 48.0/ 3.0 & 30.6 & 36.0 & 23.8/ 4.5 \\ Mistral-7b & 9.8/ 0.0 & 15.8/ 2.2 & 20.1/14.3 & 11.0 / 0.4 & 47.0 & 68.2/13.9 & 13.2/ 1.3 & 51.0/ 3.3 & 27.2/ 0.4 & 26.4/ 3.9 \\ Vicuna-13b-16k & 11.0/ & 1.5 & 14.1 & 2.2 & 14.3 / 5.4 & 15.2 / 0.0 & 7.2/ 1.7 & 7.3/23.1 & 11.3/ 2.9 & 34.3/ 3.3 & 26.9/ 0 & 20.1/ 4.3 \\ Llama2-13b & 7.8/ 0.0 & 1.1 / 0.0 & 18.1 / 6.2 & 3.2 / 0.0 & 4.1/ 0.0 & 63.5/10.8 & 7.9/ 2.0 & 35.1/ 0.0 & 29.3/ 0 & 18.9/ 2.1 \\   

Table 3: Performance of different LLMs sorted by success rate. The “Avg.” represents the average calculated 

[MISSING_PAGE_FAIL:8]

**Performance breakdown for hard and easy examples.** For each task, we divide environments into "hard" or "easy" based on the number of subgoals/conditions to meet, as shown in Table 14. The outcomes are presented in Table 5. Unsurprisingly, all models show a significant performance drop on hard examples, consistent with Dziri et al. (2023)'s findings that even robust LLMs like GPT-4 struggle with task compositionality. The performance on hard examples, reflecting challenging multiple subgoals settings, could be more crucial than average metrics.

**Long-Range Interaction.** We analyze their progress rate over interaction steps (Figure 4). Models like GPT-4, Claude2 show consistent progress over 30 steps in Alfordl and PDDL tasks. However, in WebArena and Tool tasks, their performance peaks early and then stagnates. This may be due to that later stages for these tasks are challenging. Open-weight models, except Llama3-70b and Deepseek-67b, peak early and generally stop progressing after about 6 steps, likely struggling with the increased complexity of long-range interactions and extended context requirements.

**Sub-skill Analysis.** We aim to assess LLMs across several facets: _memory_ that measures incorporating long-range information in context, _planning_ that assesses decomposing complex goals into manageable sub-goals, _world modeling_ which tests knowledge necessary for task completion, _self-reflection_ that captures the ability to use environmental feedback, _grounding_ that focuses on competency in generating valid actions, and _spatial navigation_ that represents efficiency in moving to a target location. We develop a sub-skill scoring system based on Table 11 As depicted in Figure 5, GPT-4 surpasses all other LLMs across all sub-skills.

**Exploration Behavior.** Analysis on the exploration behavior are available in Appendix E.

## 5 Visualization Panel for LLM Agent Analysis: A Case Study

We use Weights&Bias for our visualization panel with task boards for individual task analysis (SS2 and SS4). As shown in Appendix Figure 7, for GPT-4, we have GPT-4 as _Current Run_, and 6 other models as baselines for comparison. We first look at the summary board, showing GPT-4 outperforms all baselines by a large margin in terms of overall metrics. Also, GPT-4 demonstrates high capability score on all 6 subskills. From the radar plot "summary/all results" we can see that GPT-4 performs the worst on Jericho and WebArena, and we can check their respective task board for more information. In the Jericho task board, it is evident that although the performance metrics of GPT-4 are relatively low, it still outperforms other baselines. However, the performance notably declines for challenging examples, as indicated in the "jericho/progress rate w.r.t difficulty" bar plot. To further investigate, we can examine the trajectory of several failed cases in the "jericho/predictions" table. For instance, in the "zenon" sub-task, the agent successfully unlocks the cell door but fails to distract the guards, resulting in an inability to escape. This failure can be attributed to the limited exploration ability of the agent, as it should have explored the available gadgets in the room to distract the guards.

## 6 Related Work

**LLM as Agent** Traditional Reinforcement Learning offers general decision-making solutions but struggles with sample efficiency and generalization (Pourchot and Sigaud, 2019). In contrast, the emergent reasoning and instruction-following abilities of LLMs (Wei et al., 2022) enable them to excel as agents (Yao et al., 2023; Richards, 2023; Wang et al., 2023a). The primary method for employing LLMs as agents involves prompting them with task instructions and environmental context to generate actionable responses (Richards, 2023; Xie et al., 2023). Specialized training can further enhance their agentic capabilities (Xu et al., 2023; Reed et al., 2022; Driess et al., 2023). We benchmark both general (OpenAI, 2023; Touvron et al., 2023; Chiang et al., 2023) and agent-specific LLMs (Xu et al., 2023c) to study their effectiveness as agents. Additionally, research explores various dimensions of agent abilities, including grounding goals to actions (Gu et al., 2022; Ahn et al., 2022), world modeling (LeCun, 2022), step-by-step planning (Song et al., 2023), and self-reflection (Madaan et al., 2023; Wang et al., 2023b). Evaluating these skills is essential to understand limitations of LLMs as agents.

**Evaluating LLM in Decision Making Problems** Several benchmarks and toolkits for LLM agents have been established, focusing on various tasks such as web-browsing, games, and tool use (Yao et al., 2022; Zhou et al., 2023; Shridhar et al., 2021; Qin et al., 2022; Wang et al., 2023; Ye et al., 2024; Kinniment et al., 2023). A few other benchmarks provide a proof-of-concept study on specific LLM features, with Wang et al. (2023b) focusing on model interaction ability, and Liu et al. (2023b) examining agent structures. Recent works by Liu et al. (2023a); Wu et al. (2023); Mialon et al. (2023) present a generalist challenge for LLM agents, please refer to Table 1 for a comparison. Note that recent progress in multimodal LLMs has spurred research into multimodal LLM agents (Zheng et al., 2024; Yang et al., 2023). Our study focuses exclusively on text-based environments to assess LLM agent abilities via textual reasoning and actions in-depth.

## 7 Conclusion

In this work, we introduce AgentBoard as a benchmark for evaluating generalist LLM agents. In addition to being a benchmark, AgentBoard offers an open-source, analytical evaluation framework that facilitates easy customization, unified metrics, and comprehensive analysis from diverse aspects, in addition to an interactive visualization web panel. Such analytical evaluation is equipped with an interactive visualization web panel, allowing users to efficiently explore the evaluation and gain a deeper understanding of the agents of interest. Overall, AgentBoard aims to facilitate detailed evaluation and understanding of LLM agents, driving further advancements in the field.

**Limitations:** Limitations of AgentBoard include reliance on human-annotated subgoals to calculate progress rate. Although using LLMs for annotation is considered, current models underperform on AgentBoard tasks and cannot accurately generate subgoals. Additionally, AgentBoard evaluates agents mainly in simulated environments to maintain standardization. However, real-world benchmarking is crucial for practical applications but presents challenges such as variable ground truth labels and security risks. We will address them in future work.