# Summary of Contributions

On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games

Awni Altabaa

Department of Statistics & Data Science

Yale University

awni.altabaa@yale.edu

&Zhuoran Yang

Department of Statistics & Data Science

Yale University

zhuoran.yang@yale.edu

###### Abstract

In sequential decision-making problems, the _information structure_ describes the causal dependencies between system variables, encompassing the dynamics of the environment and the agents' actions. Classical models of reinforcement learning (e.g., MDPs, POMDPs) assume a restricted and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure. In this paper, we formalize a novel reinforcement learning model which explicitly represents the information structure. We then use this model to carry out an information-structural analysis of the statistical complexity of general sequential decision-making problems, obtaining a characterization via a graph-theoretic quantity of the DAG representation of the information structure. We prove an upper bound on the sample complexity of learning a general sequential decision-making problem in terms of its information structure by exhibiting an algorithm achieving the upper bound. This recovers known tractability results and gives a novel perspective on reinforcement learning in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems.

## 1 Introduction

The _information structure_ of a sequential decision-making problem is a description of the causal dependencies between system variables. In particular, the information structure specifies the subset of past events that causally influence the present state. This includes information affecting system dynamics and information available to each agent when choosing an action. The control community has long recognized the importance of information structure, leading to the development of the celebrated Witsenhausen intrinsic model , and extensive study since the 1970s [e.g., 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].

In general, sequential decision-making problems with arbitrary causal dependencies between system variables can be computationally and statistically intractable in both control and learning settings . In response, early research has identified classes of tractable information structures. For example, Markov Decision Processes (MDP) and Markov teams/games assume an observable Markovian state, which serves as a sufficient statistic for the system's evolution. These structural assumptions enable practical planning and learning algorithms, for example based on dynamic programming . Although such restricted model classes can enable more fruitful analysis, they lack the expressiveness needed to naturally capture the causal structures of complex real-world sequential decision-making problems, where each event in the system may depend arbitrarily on past events.

The concept of information structure is also fundamental to studying the phenomenon of _partial observability_. In general, partial observability refers to situations where a system's evolution depends on a potentially large number of sequential events, but only a subset of these events is observable by the learning agent. However, commonly studied models capture only a restrictive form of this phenomenon. For example, in a Partially-Observable Markov Decision Process (POMDP)--the standard model of partial observability --it is assumed that a Markovian state exists and that each observation is a noisy measurement of the current state. This assumption is often unrealistic, as general systems may not have clearly defined "states", and observations may be generated by more complex dependencies. Information structure provides a more powerful framework for understanding partial observability, capturing a general notion in which system variables evolve with an arbitrary causal structure (not necessarily Markovian), and the set of observables is any subset of all system variables.

The highly-regular information structures of classical models make analysis more tractable, enabling favorable theoretical results [e.g., 18, 19, 20, 21, 22] and driving notable empirical success across a wide range of domains [e.g., 23, 24, 25, 26, 27, 28]. Despite this empirical success, a general theory addressing the role of information structure in the statistical aspects of reinforcement learning is missing. We argue for the perspective that information structure is an important component of analyzing and solving reinforcement learning problems. A rich and flexible representation of information structure is essential to faithfully capture real-world sequential decision-making problems, where the system evolves according to a complex and time-varying dependence on the past, and different agents have different information available to them at different points in time.

_In this work, we formulate a general model of sequential decision-making with an explicit representation of information structure, and study the role of information structure in the statistical complexity of reinforcement learning._ Explicitly modeling information structure allows us to identify a broader class of tractable decision-making problems and ultimately develop more tailored reinforcement learning approaches that leverage this structure._A model for representing information structure._ We propose _partially-observable sequential teams_ (POST) and _partially-observable sequential games_ (POSG) as general models that contain an explicit representation of information structure as part of the problem specification. This forms a unifying framework that enables an analysis of the role of information structure in RL and captures commonly studied RL models as special cases, including MDPs, Markov teams/games, POMDPs, and Dec-POMDPs/POMGs (Figure 1). The models introduced in this work draw inspiration from Witsenhausen's intrinsic model  and its variants from the control literature , with added elements to model partial observability in the context of reinforcement learning.

_Theoretical analysis of sequential decision-making through information structure._ We characterize the complexity of the observable dynamics of any sequential decision-making problem through a graph-theoretic analysis of the information structure. Moreover, we propose a generalization of predictive state representations--which may be of independent interest--and construct such a representation for POSTs/POSGs by exploiting information structure. This provides a robust and efficient parameterization amenable to learning.

_Learning theory & information-structural analysis of statistical complexity._ We prove an upper bound on the sample complexity of learning general sequential decision-making problems, expressed as a function of information structure. In particular, the dependence is on an interpretable quantity derived from a graphical representation that can be thought of as an effective "information-structural state", generalizing the typical notion of a Markovian state. We prove this result by exhibiting an algorithm achieving this upper bound, making use of the generalized predictive state representation constructed earlier which provides a robust representation amenable to learning. In doing so, we identify a larger class of statistically tractable reinforcement learning problems.

Related Work.We provide a detailed discussion of related work in Appendix B.

Figure 1: A depiction of the generality of our proposed models. POSTs and POSGs capture MDPs, POMDPs, Dec-POMDPs, and POMGs as special cases.

Background & Preliminaries

What follows is an abridged description of the relevant background and preliminaries. We refer the reader to Appendix C for a more detailed treatment, and to Appendix A for a summary of notations.

### Generic Sequential Decision Making Problems

Consider a controlled stochastic process \((X_{1},,X_{H})\), where \(X_{h}\) is a random variable corresponding to the variable at time \(h\). At each time \(h[H]\), the variable \(X_{h}\) may be either an 'observation' (i.e., observable system variable) or an 'action'. The dynamics of this stochastic process are described by a tuple \((H,\{_{h}\}_{h})_{h},,,)\), where \(H\) is the time horizon, \(_{h}\) is the variable space at time \(h\), \([H]\) is the index set of observations (i.e., \(X_{h}\) is an observation if \(h\)), \([H]\) is the index set of actions, and \(=\{_{h}\}_{h}\) is a set of probability kernels which describes the probability of any trajectory \(x_{1},,x_{H}\) given the actions, \([\{x_{s}:s\}\{x_{s}:s\}] =_{h}_{h}[x_{h} x_{1},,x_{h-1}]\). A choice of policy \(\) induces a probability distribution \(^{}\) on \(_{1}_{H}\). The objective of the agent(s) is to choose a policy which maximizes their expected reward \(V^{R}()=^{}[R(X_{1},,X_{H})]\).

Let \(_{h}=_{s 1:h}_{s}\) denote the space of histories at time \(h\) and \(_{h}=_{s h+1:H}_{s}\) denote the space futures at time \(h\). Similarly, let \(_{h}^{a}=(_{h})=_{s_{:h}} _{s}\) denote the observation component of histories and let \(_{h}^{a}=(_{h})=_{s_{:h}} _{s}\) denote the action component. The observation and action components of the futures, \(_{h}^{a}\) and \(_{h}^{a}\) respectively, are defined similarly. Here, \(()\) and \(()\) extract the observation and action components, respectively, of any trajectory.

We define the _system dynamics matrix_\(_{h}^{|_{h}||_{h}|}\) as the matrix giving the probability of each possible pair of history and future at time \(h\) given the execution of the actions, \([_{h}]_{_{n},_{h}}:=}[_{h}, _{h}][_{h}^{o},_{h}^{o}( _{h}^{a},_{h}^{o})]\), where \(_{h}\) is the history, \(_{h}\) is the future, and \(_{h}^{o},_{h}^{a},_{h}^{o},_{h}^{o}\) separate the history and future into observation components and action components.

The _rank_ of the dynamics of a sequential decision-making problem is a measure of its complexity. It is defined as the maximal rank of its dynamics matrices.

**Definition 1** (Rank of dynamics).: _The rank of the dynamics \(\{_{h}\}_{h[H]}\) is \(r=_{h[H]}(_{h})\)._

### Generalized Predictive State Representations

Predictive state representations (PSR) [31; 32] are a representation of dynamical systems and sequential decision-making problems based on predicting future observations given the past, without explicitly modeling a latent state. In this section, we propose and formalize a generalization of standard PSRs which allows for an arbitrary order of observations and actions. This generalization is necessary to capture POSTs/POSGs (introduced in Section 3), but may also be of independent interest.

The "PSR rank" of a sequential decision-making problem coincides with the rank of its dynamics (Definition 1). Denote \(r_{h}:=(_{h})\). At the heart of predictive state representations is the concept of "core test sets." A core test set at time \(h\) is a set of futures such that the vector of probabilities of those futures conditioned on the past encodes all the information that the past contains about the future.

**Definition 2** (Core test sets).: _A core test set at time \(h\) is a subset of \(d_{h} r_{h}\) futures, \(_{h}:=\{q_{h}^{1},,q_{h}^{d_{h}}\}_{h}\), such that the submatrix \(_{h}[_{h}]^{|_{h}| d_{h}}\) is full-rank._

The \(d_{h}\)-dimensional vector \(_{h}(_{h}):=(}[_{h},q_{h}^{1}],, }[_{h},q_{h}^{d_{h}}])\) serves as a sufficient statistic for computing the probability of any future conditioned on \(_{h}\). Intuitively, core test sets relate the rank of the dynamics to a representation based on predicting future outcomes. A PSR parameterization of the system dynamics represents the probability of any trajectory using the sufficient statistics \(_{h}\) via a set of operators that iteratively update the predictive representations at each step as new observations come in. We proceed to formally define our generalized predictive state representation.

**Definition 3** (Generalized Predictive State Representations).: _Consider a sequential decision-making problem \((X_{h})_{h[H]}\) where \(,\) partition \([H]\) into actions and observations, respectively. Then, a generalized predictive state representation for this sequential decision-making problem is a tuple \((\{_{h}\}_{0 h H-1},_{H},\{M_{h}\}_{1 h H-1}, _{0})\) satisfying_

\[}[x_{1},,x_{H}] =_{H}(x_{H})^{}M_{H-1}(x_{H-1}) M_{1}(x_{1})_{0}\] (1) \[_{h}(x_{1},,x_{h}) =M_{h}(x_{h}) M_{1}(x_{1})_{0},\, h,\] (2)_where \(M_{h}:_{h}^{d_{h} d_{h-1}}\), \(_{H}:_{H}^{d_{H-1}}\), and \(_{0}=()\)._

An important condition for the learnability of PSR models, which was used in prior work [including 33, 34, 35], is the so-called "well-conditioning assumption". We state the analogous assumption for our generalized PSR model below. For a core test set \(_{h}\), let \(_{h}^{A}=(_{h})\) be the set of core action sequences, let \(Q_{A}=_{h}|_{h}^{A}|\) be the number of core action sequences, and let \(d=_{h}d_{h}\).

**Assumption 1** (\(\)-well-conditioned generalized PSR).: _A generalized PSR model is said to be \(\)-well conditioned for \(>0\) if, for any \(h[H]\) and any \(\), it satisfies_

\[_{z^{d_{h}}\\ \|z\|_{1} 1}_{_{h}_{h}}(_{h}) |m_{h}(_{h})^{}z|,_{ z^{d_{h}}\\ \|z\|_{1} 1}_{x_{h}_{h}}\|M_{h}(x_{h})z \|_{1}(x_{h})_{h+1}^{A}|}{},\]

_where \(m_{h}(_{h})^{}=_{H}(x_{H})^{}M_{H-1}(x_{H-1}) M_{h+1}(x _{h+1})\) and \(\,_{h}^{a}\), \(_{_{h}^{a}}(_{h}^{o},_{h}^{a})=1\)._

To understand this condition, recall that \(m_{h}(_{h})^{}_{h}(_{h})=}[_{h},_{h}]\). Thus, we can interpret \(z\) as the error in estimating the prediction features \(_{h}\). This condition ensures that the error in estimating the probability of system trajectories remains controlled when the estimation error of \(_{h}\) is small.

Although we focus on finite spaces \(_{h}\) in this work, we briefly discuss possible extensions to continuous spaces. In finite settings, the core tests \(q_{h}\) are represented by future trajectories \(q=(x_{h+1},...,x_{H})_{h}\). In continuous spaces, the core tests become trajectories over subsets of the underlying continuous space: \(q=(_{h+1},,_{H})_{h}\), where each \(_{k}_{k}\) is a measureable subset. We point to  for more discussion on how to extend (standard) PSRs to continuous spaces.

## 3 Information Structure

The "information structure" of a sequential decision-making problem defines the causal dependencies between events in the system occurring at different points time, whether those events are observable by the learning agent or not. In this section, we will introduce a novel reinforcement learning model that explicitly represents information structure. We demonstrate that this enables a rich analysis of the system's dynamics and is crucial for characterizing the statistical complexity of general RL problems.

### Partially-Observable Sequential Teams

We propose a model of sequential decision-making problems that includes an explicit representation of information structure, which we call partially-observable sequential teams (POST). A POST is a controlled stochastic process consisting of a sequence of variables, where each variable is either a "system variable" or an "action variable". POSTs also model the _observability_ of each system variable with respect to the learning algorithm (i.e., which system variables are available to the learning algorithm). The information structure of a POST describes the causal dependence between these variables. The _information set_ of each system variable describes the subset of past variables that are coupled to it in the dynamics. The information set of an action variable describes the information available to the agent when choosing an action, hence defining the policy class they optimize over.

**Definition 4** (Post).: _A partially-observable sequential team is a controlled stochastic process that specifies the joint distribution of \(T\) variables \((X_{t})_{t[T]}\), together with a specification of the observability of each variable. Here each \(X_{t}\) is either a system variable or an action variable, and is either observable by the learning agent or not. A POST is specified by the following components._

1. _Variable Structures._ _The variables_ \(\{X_{t}\}_{t[T]}\) _are partitioned into two disjoint subsets -- system variables and action variables._ \([T]\) _indexes system variables and_ \([T]\) _indexes action variables, with_ \(=\)_,_ \(=[T]\)_._
2. _Information Structure._ _For_ \(t[T]\)_, the "information set"_ \(_{t}[t-1]\) _of the variable_ \(X_{t}\) _is the set of past variables that are coupled to_ \(X_{t}\) _in the dynamics. That is, the transition to_ \(X_{t}\) _depends on the value of_ \(I_{t}:=(X_{s}\,:\,s_{t})\)_. We call_ \(I_{t}\) _the "information variable" at time_ \(t\)_, and call_ \(_{t}=_{s_{t}}_{s}\) _the "information space"._
3. _System Kernels._ _For any_ \(t\)_,_ \(_{t}(_{t}\|_{t})\) _is kernel from_ \(_{t}\) _to_ \(_{t}\) _that specifies the conditional distribution of a system variable_ \(X_{t}\) _given the information variable_ \(I_{t}\)_._
4. _Decision Kernels._ _Each agent chooses a decision kernel_ \(_{t}:_{t}(_{t})\)_, specifying a (potentially randomized) policy for choosing an action at time_ \(t\)5. _Observability._ _We denote the observable system variables by_ \(\)_. We require that the information sets of the action variables are observable to the learning algorithm:_ \(_{t}(_{t})\)_. We define_ \(\)_, and let_ \(H||\) _be the time-horizon of the observable variables._
6. _Reward Function._ _At the end of an episode, the team receives a reward determined by the function_ \(R:_{s}_{s}\)_._

With the above components, any set of decision kernels (i.e., joint policy) \(=(_{t}:t)\) induces a unique probability measure over \(_{1}_{T}\), which is given by

\[^{}[x_{1},,x_{T}]=_{t} _{t}(x_{t}|\{x_{s}:s_{t}\})_{t }_{t}(x_{t}|\{x_{s}:s_{t}\}).\] (3)

We will be interested in modeling the _observable_ dynamics of the POST. We index the observable variables by their position among the observables \(h[H]\) rather than their position among all variables, as follows: \((X_{t})_{t}=(X_{t(1)},,X_{t(H)})\), where \(t:[H][T]\) maps the index over observables to the index over all variables. The distribution of the observables is obtained by marginalizing over the unobservable variables, \(^{}[x_{t(1)},,x_{t(H)}]=_{s^{}}_{x_{s}_{s}}^{}[X_{1}=x_{1},  X_{T}=x_{T}]\).

The value of a policy is given by its expected reward \(V()^{}[R(X_{t(1)},,X_{t(H)}) ]\), where \(^{}\) is the expectation associated with the probability measure \(^{}\). The objective of a POST is to learn a policy \(=(_{t})_{t}\) which maximizes the expected reward, \(_{}V()\). When the variable spaces \(_{t}\) are finite, this supremum is attained by a deterministic policy, \(_{t}_{t}_{t},t\).

**Representation of the information structure as a directed acyclic graph.** The information structure of a POST can be naturally represented as a labeled directed acyclic graph (DAG). Given the variable structure and information structure of a POST, \((,,\{_{t}\}_{t})\), its DAG representation is given by \((,,)\). The nodes of the graph are the set of variables, \(=[T]=\). The edges \(\) of the DAG are given by \(=\{(i,t):t[],i_{t}\}\). That is, there exists an edge from \(i\) to \(t\) if \(i\) is in the information set of \(t\). Finally, \(\) contains labels for each node as being a system variable (in \(\)) or an action variable (in \(\)). Further, the observability of system variables (in \(\)) is also labeled.

This DAG represents a directed _graphical model_ for the POST. In particular, the probability distribution on \(_{1}_{T}\) factors according to \(\),

\[[X_{1},,X_{T}]=_{t} [X_{t}(X_{t})],\] (4)

where \((X_{t})\) is the set of parents of \(X_{t}\) in \(\) (which are \(_{t}\)). This representation of the information structure as a DAG will be crucial for our analysis of the dynamics of sequential teams in Section 3.2.

**Partially observable sequential games.** We define partially-observable sequential games (POSGs) in a similar manner. The main difference is that, in the game setting, there exists an expanded reward structure with \(N\) different reward objectives \(R^{1},...,R^{N}\), with different agents pursuing different objectives. The action index set is partitioned into \(N\) subsets \(=^{1}^{N}\), where \(^{i}\) denotes the action index set associated with the agent(s) optimizing for objective \(R^{i}\). The extension to the game setting is treated in detail in Appendix F.

### Information Structure Determines the Rank of POSTs/POSGs

POSTs and POSGs form a highly general framework that captures any sequential decision-making problem that can be described by a controlled stochastic process, subsuming classical models such as MDPs, POMDPs, Dec-POMDPs, and POMGs. By introducing a model with an explicit representation of information structure, we gain the ability to perform a richer analysis of sequential decision-making problems. In particular, we will show that the rank of the dynamics, and ultimately the statistical complexity of reinforcement learning, can be characterized as a function of the information structure.

We begin by defining the central quantity in our analysis, which we call the "information-structural state", hinting at the role it will play. The information structural state is defined for each point in time as a subset of the past (whether observed or latent) which forms a sufficient statistic for the future.

**Definition 5** (Information-structural state).: _Let \(^{}\) be the DAG obtained from \(\) by removing all edges directed towards actions. That is, it consists of the edges \(^{}\{(x,a):x,a \}\). For each \(h[H]\), let \(_{h}^{}[t(h)]\) be the minimal set of past variables (observed or unobserved) which \(d\)-separates the past observations \((X_{t(1)},,X_{t(h)})\) from the future observations \((X_{t(h+1)},,X_{t(H)})\) in the DAG \(^{}\). Define \(_{h}^{}:=_{s_{h}^{}}_{s}\) as the joint space of those variables._The following theorem, whose proof is given in Appendix H, states that the rank of the observable system dynamics of POSTs and POSGs is bounded by the cardinality of \(_{h}^{}\).

**Theorem 1**.: _The rank of the observable system dynamics of a POST or POSG is bounded in terms of its information structure by \(r_{h|H|}_{h}^{}|\)._

This result shows that the complexity of the observable system dynamics is characterized by the information structure through \(_{h}^{}\). In particular, \(i_{h}^{}_{h}^{}\) describes a set of system variables, either observable or latent, which provide a sufficient statistic of the past at time \(h\) for predicting future observations--\(I_{h}^{}\) "separates" the past from the future. Hence, the quantity \(\|_{h}^{}\|\) admits an interpretation as the size an effective state space at time \(h\). This is a generalization of the standard notion of a latent state. For example, in the case of POMDPs or Dec-POMDPs, the information-structural state indeed coincides with the latent Markovian state (Figure 1(a)). We emphasize that \(_{h}^{}\) may contain observable variables as well as _unobservable_ system variables. In fact, unobservable system variables can introduce crucial structure that simplifies the observable system dynamics.

### Examples of Information Structures and their Rank

In this section, to provide some intuition, we present examples of information structures and apply Theorem 1 to obtain a bound the rank of their observable system dynamics. We see that classical models such as MDPs, POMDPs, and POMGs are special cases of the POST/POSG framework, and known results about their rank  are recovered by the generalized graph-theoretic analysis of their information structure. For notational convenience, we adopt a modified notation in this section where information sets \(\) are indexed by the symbol of the variable rather than its time-index. For example, in an MDP, we write \((s_{t})=\{s_{t-1},a_{t-1}\}\) for the information set of the state variable \(s_{t}\).

**Decentralized POMDPs and POMGs.** At each time \(t\), the system variables of a decentralized POMDP (or POMG) consists of a latent state \(s_{t}\), observations for each agent \(o_{t}^{1},,o_{t}^{N}\), and actions of each agent \(a_{t}^{1},,a_{t}^{N}\). The latent state transitions are Markovian and depend on the agents' joint action. The observations are sampled via a kernel conditioned on the latent state. Each agent can use their own history of observations to choose an action. Thus, the information structure is given by

\[(s_{t})=\{s_{t-1},a_{t-1}^{1},,a_{t-1}^{N}\},\, (o_{t}^{i})=\{s_{t}\},\,(a_{t}^{i})=\{o _{1:t-1}^{i},a_{1:t-1}^{i}\}.\]

Here, the observable variables are \(=\{o_{1:T}^{i},a_{1:T}^{i},i[N]\}\). By Theorem 1, we have \(^{}(o_{t}^{i})=^{}(a_{t}^{i})=\{s_{t }\},\, t,i\), as shown in Figure 1(a). Thus, the rank of a Dec-POMDP is bounded by \(||\), where \(\) is the state space. Note that in the case of models with a true latent state (e.g., POMDPs, Dec-POMDPs, and POMGs), the information-structural state coincides with the Markovian latent state.

**Point-to-Point Real-Time Communication with Feedback.** Consider the following model of real-time communication with feedback. Let \(x_{t}\) be a Markov source. At time \(t\), the encoder receives the source \(x_{t}\) and sends an encoded symbol \(z_{t}\). The symbol is sent through a memoryless noisy channel which outputs \(y_{t}\) to the receiver. The decoder produces the estimate \(_{t}\). The output of the noisy channel is also fed back to the encoder. The encoder and decoder have full memory of their observations and previous "actions". The observation variables are \(=\{x_{1:T},\,y_{1:T}\}\) and the "actions" are \(=\{z_{1:T},\,_{1:T}\}\). Hence, the information structure is given by the following,

\[(x_{t})=\{x_{t-1}\},\,(z_{t})=\{x_{1:t}, y_{1:t-1},z_{1:t-1}\},\,(y_{t})=\{z_{t}\},\, (_{t})=\{y_{1:t}\}.\]

By Theorem 1, we have that,

\[^{}(x_{t})=\{x_{t}\},\,^{}(z_{t })=\{x_{t}\},\,^{}(y_{t})=\{x_{t},z_{t} \},\,^{}(_{t})=\{x_{t}\}.\]

Hence, the rank is bounded by \(||||\). This is depicted in Figure 1(c).

**Limited-memory information structures.** Consider a sequential decision making problem with variables \(o_{t},a_{t},t[T]\) and an information structure with length-\(m\) "memory". That is, observations do not directly depend on variables more than \(m\) steps in the past. That is, the information structure is

\[(o_{t})=\{o_{t-m:t-1},a_{t-m:t-1}\},(a_{t})= \{o_{1:t},a_{1:t-1}\}.\]

The observables consist of all observations and actions, \(=\{o_{1:T},a_{1:T}\}\). By Theorem 1 we have that \(^{}(o_{t})=\{o_{t-m:t-1},a_{t-m:t-1}\}\), as shown in Figure 1(d). Hence, the rank of this sequential decision-making problem is bounded by \(||^{m}||^{m}\).

The examples above show that the complexity of the dynamics of a sequential decision-making problem depends directly on its information structure. An expanded version of this discussion is provided in Appendix D. Next, we use an information-structural analysis to construct a generalized PSR representation for a class of POSTs/POSGs, which we will ultimately use to prove an upper bound on the _statistical complexity_ of reinforcement learning as a function of information structure.

## 4 Constructing a PSR Parameterization for POSTs and POSGs

A key challenge in reinforcement learning is constructing representations which enable robustly and efficiently modeling probabilities of system trajectories (i.e., probabilities of the form \([]\)). In this section, we will construct a generalized predictive state representation for a class of POSTs/POSGs, ultimately enabling sample-efficient reinforcement learning.

### Core test sets for POSTs/POSGs

Recall that a core test set is a set of futures such that the probabilities of those futures given the past encode all the information that the past contains about the future. For systems with a simple and

Figure 2: DAG representation of various information structures. Solid edges indicate the edges in \(^{}\) and light edges indicate the information sets of action variables. Grey nodes represent unobservable variables, blue nodes represent past observable variables, green nodes represent future observable variables, and red nodes represent \(_{h}^{}\). To find \(_{h}^{}\), as per Theorem 1, we first remove the incoming edges into the action variables, then we find the minimal set among all past variables (both observable and unobservable) which \(d\)-separates the past observations from the future observations.

regular information structure, a core test set may be simple to obtain. For example, undercomplete POMDPs with a full-rank emission matrix admit the 1-step observation futures as core test sets.

For POSTs/POSGs with arbitrary information structures, obtaining a core test set is much more challenging without knowing the system dynamics. In this section, we identify a condition in terms of the information structure under which \(m\)-step futures are a core test set for POSTs/POSGs. Let us denote the candidate core test set of \(m\)-step futures at time \(h\) by \(_{h}^{m}:=_{s_{h+1:(h+m,\,H)}}_{s}\). We define the matrix \(_{h}\) as encoding the probability of observing each \(m\)-step future conditioned on the information-structural state \(i_{h}^{}_{h}^{}\):

\[_{h}[}q\ |\ i_{h}^{} ]_{q,i_{h}^{}}^{|_{h}^{m}|| _{h}^{}|},\ q_{h}^{m},\,i_{h}^{} _{h}^{}.\] (5)

The operational meaning of \(_{h}\) is depicted in Figure 3. Next, we formulate a condition in terms of information structure that we will show implies that the \(m\)-step futures are core test sets.

**Definition 6** (\(m\)-step \(^{}\)-weakly revealing).: _We say that a sequential team is \(m\)-step \(^{}\)-weakly revealing if for all \(h[H]\), \((_{h})=|_{h}^{}|\). Furthermore, we say that the sequential team is \(\)-robustly \(m\)-step \(^{}\)-weakly revealing if for all \(h[H-m+1]\), \(_{|_{h}^{}|}(_{h})\)._

The \(^{}\)-weakly revealing condition is essentially a statistical identifiability condition. If a POST/POSG is \(^{}\)-weakly revealing, then, for any two mixtures of the information-structural state, the distributions of the \(m\)-step futures are distinct. Formally, for any \(_{1},_{2}(_{h}^{})\) with \((_{1})(_{2})=\), we have \(_{h}_{1}_{h}_{2}\). That is, the future observations contain information that can distinguish between mixtures of the latent information-structural state. The \(\)-robust version of the \(^{}\)-weakly revealing condition requires that \(_{h}\) is not only full rank, but that its \(|_{h}^{}|\)-th singular value is bounded away from zero, so that \(\|G_{h}_{1}-G_{h}_{2}\|\|_{1}-_{2}\|\).

The condition holds whenever there exists a sequence of actions within the \(m\)-step futures such that executing these actions results in a sequence of observations that is informative about the information-structural state \(i_{h}^{}_{h}^{}\). In general, this condition will be harder to satisfy when \(_{h}^{}\) is large since it would require the \(m\)-step future observations to encode more information. In particular, \(_{h}\) cannot be full rank when \(|_{h}^{m}|<|_{h}^{}|\). As a heuristic, when we don't have prior knowledge about the dynamics (e.g., in the learning setting), we can choose \(m\) such that \(|_{h}^{m}||_{h}^{}|\). In general, it will be possible to find a smaller core test set when the \(d\)-separating set \(_{h}^{}\) is small. This happens when the system dynamics contain state-like variables that are low-dimensional.

The \(^{}\)-weakly-revealing condition is a generalization of the "weakly-revealing" condition for POMDPs introduced by Liu et al. . Liu et al.  proposed an algorithm for learning weakly-revealing POMGs. Our analysis here recovers weakly-revealing POMGs as a special case and enables learning a much more general class of problems. We note that such an identifiability condition is necessary, and is reflective of the fundamental difficulty of the partially-observable setting. For example, in the case of POMDPs, there exist hardness results that state that if an analogous condition does not hold, the statistical complexity can scale exponentially with the relevant quantities .

We now proceed to show that under the \(^{}\)-weakly-revealing condition, \(m\)-step futures are core test sets for POSTs/POSGs which can be used to construct a generalized PSR representation amenable

Figure 3: A depiction of the construction of a generalized generalized predictive state representation for POST/POSG models.

to learning. Recall that the vector of core test set probabilities for the history \(_{h}\) is given by the mappings \(_{h},_{h}:_{h}^{|_{h}^{m}|}\),

\[_{h}(_{h})=[[q^{o},_{h}^{o}(_ {h}^{a}),\,(q^{a})]]_{q_{h}^{m}}, _{h}(_{h})=[[q^{o}_{h}^{o};\, (_{h}^{a}),\,(q^{a})]]_{q_ {h}^{m}}.\]

Define the mapping \(m_{h}:_{h}^{|_{h}^{m}|}\) as,

\[m_{h}(_{h})(_{h}^{})^{}[}[_{h}\,|\,i_{h}^{}]]_{i_{h}^{}_{h}^{}}.\] (6)

The following lemma, whose proof is given in Appendix I, shows that the \(m\)-step futures \(_{h}^{m}\) are core test sets for any \(m\)-step \(^{}\)-weakly revealing POST/POSG. In particular, given any future \(_{h}_{h}\) and history \(_{h}_{h}\), the conditional probability \(}[_{h}_{h}]\) can be written as a linear combination of the core test probabilities \(_{h}(_{h})\), with weights given by \(m_{h}(_{h})\), depending only on the future \(_{h}\).

**Lemma 1** (Core test set for POSTs).: _Suppose that the POST is \(m\)-step \(^{}\)-weakly revealing. Then, \(_{h}^{m}\) is a core test set for all \(h[H]\). Furthermore, we have \(}[_{h},_{h}]= m_{h}( _{h}),_{h}(_{h})\) and \(}[_{h}_{h}]= m_{h}( _{h}),_{h}(_{h})\)._

### Generalized PSR parameterization of POST/POSG

Consider a POST/POSG which is \(m\)-step \(^{}\)-weakly revealing. Lemma 1 shows that the \(m\)-step futures \(_{h}^{m}\) are core test sets. In this section, we will explicitly construct a generalized PSR parameterization for this class of sequential decision-making problems. Moreover, we will show that this generalized PSR representation is well-conditioned when the weakly revealing condition is robust.

Let \(d_{h}|_{h}^{m}|\). The key observation that allows us to construct the generalized PSR representation is that the vector mappings \(m_{h}:_{h}^{d_{h}}\) and \(_{h}:_{h}^{d_{h}}\) can be used to derive a recursive form of the dynamics of the POST/POSG. We define the operator mapping \(M_{h}:_{t(h)}^{d_{h} d_{h-1}}\) by

\[[M_{h}(x_{t(h)})]_{q_{:}}=m_{h-1}(x_{t(h)},q)^{},\,q_{h}.\] (7)

That is, \(M_{h}(x_{t(h)})\) is the matrix whose rows are indexed by the core tests at the \(h\)-th observable step, where the row corresponding to each \(q_{h}\) is the weights returned by the mapping \(m_{h-1}\) when applied to the future consisting of \(x_{t(h)}\) followed by core test \(q\). The operator map \(M_{h}\) allows us to update the probabilities of the core test sets after receiving an additional observation \(x_{t(h)}\): \(_{h}(x_{t(1)},...,x_{t(h)})=M_{h}(x_{t(h)})_{h-1}(x_{t(1)},...,x_{t(h-1)})\). The following result, whose proof is given in Appendix I, states that the set of operators \(\{M_{h}\}_{h}\) forms a generalized PSR.

**Theorem 2**.: _Consider an \(m\)-step \(^{}\)-weakly revealing POST/POSG. Let \(\{M_{h}\}_{h[H-1]}\) be defined as above, and let \(_{0}=[}[q]]_{q_{0}^{ m}},\,_{H}(x_{t(H)})=_{x_{t(H)}}\). Then, \((\{_{h}^{m}\}_{h},_{H},\{M_{h}\}_{h[H-1]},_{0})\) forms a generalized predictive state representation. Moreover, if the weakly-revealing property is \(\)-robust, then this PSR is \(\)-well-conditioned with \(=/_{h}_{h}^{}^{1/2}\)._

Thus, we have constructed a robust parameterization of the sequential decision-making problem, making use of its information structure, that will enable us to design an efficient learning algorithm.

Characterizing the Statistical Complexity of General Reinforcement Problems via Information Structure

In this section, we establish an upper bound on the achievable sample complexity of general reinforcement learning problems in terms of their information structure. In plain language, this is a result that roughly says "any sequential decision-making problem with an information structure \(\) can be learned with a sample complexity at most \(f()\)". This identifies a class of sequential decision-making problems that are statistically tractable via conditions on the information structure, expanding the set of known-tractable problems while recovering existing tractability results as a special case.

We will prove this result by exhibiting an algorithm that achieves this upper bound. Our approach will be to use the generalized predictive state representation constructed for POSTs/POSGs in Section 4, which provides a robust representation amenable to learning. We will introduce an algorithm for learning generalized PSRs and prove a corresponding sample complexity result, which will in turn imply a bound on the sample complexity of learning POST/POSG models via the information-structural characterization of the rank of observable dynamics established in Section 3.2.

There exist several works in the literature which study learning in PSRs (e.g. [33; 34; 35; 38; 39]). Using the technical tools developed in this paper, most of these algorithms can be directly extended to our generalization of PSRs. With such an algorithm, Theorems 1 and 2 then imply a bound on the achievable sample complexity for learning general sequential decision-making problems in terms of their information structure. In this work, we will adapt the model-based UCB-type algorithm of Huang et al. , extending it to generalized PSRs to obtain a bound on the achievable sample complexity for POSTs/POSGs. We will defer the details of the algorithm to Appendix E and formally verify the proof in Appendix J. Here, we will focus on discussing the role of information structure in determining the statistical complexity of reinforcement learning.

The following result states that the size of the information-structural state, \(|_{h}^{}|\), characterizes an upper bound on the statistical complexity of learning a sequential decision-making problem.

**Theorem 3**.: _Suppose a sequential decision-making problem described by a POST is \(\)-robustly \(m\)-step \(^{}\)-weakly revealing. Let \(Q_{m}_{h}_{h}^{m}|\) be the size of the \(m\)-step observable trajectories, and let \(A=_{s}}_{s}|\) be the size of largest action space. Then, there exists an algorithm that can learn an \(\)-optimal policy with a sample complexity (omitting log factors) bounded by_

\[}(,_{h} |_{h}^{}|,Q_{m},A,H).\]

_Under the game setting, the same assumption implies the existence of a self-play algorithm that learns an \(\) (Nash or coarse-correlated) equilibrium with the same sample complexity._

This result identifies \(^{}\)-weakly revealing POSTs/POSGs as a class of statistically tractable sequential decision-making problems. The sample complexity is polynomial in the size of the information-structural state space \(_{h}_{h}^{}\), the size of the action space \(A\), and the time horizon \(H\). Further, it depends on \(Q_{m}\), the size of \(m\)-step observable trajectories, and the robustness parameter \(^{-1}\), which corresponds to the \(m\)-step \(^{}\)-weakly revealing identifiability condition. We note that the algorithm constructed to prove Theorem 3 only needs to know the parameters of the \(^{}\)-weakly revealing condition (i.e., \(m\) and \(\)), and does not need to know the full information structure.

This result shows that the size of the information-structural state is a fundamental measure of the statistical complexity of reinforcement learning. As a result, learning is tractable when \(_{h}_{h}^{}\) is of modest size, and the information structural state is strongly coupled to the observable system variables.

One notable special case of POSTs/POSGs is POMDPs. Learning in POMDPs has been studied extensively in the literature. Theorem 3 implies a \((S,O,A,H,^{-1})^{-2}\) bound on the sample complexity of learning in \(\)-weakly POMDPs and \((S,(OA)^{m},H,^{-1})^{-2}\) for learning \(m\)-step weakly revealing POMDPs. This recovers a similar sample complexity as was shown by more specialized analysis tailored to POMDPs (e.g., [40; 36]).

## 6 Conclusion

**Summary.** This paper examines the role of information structure in general reinforcement learning problems. We introduced novel models that explicitly represent information structure, and proved an upper bound on the sample complexity of general reinforcement learning problems in terms of information structure. The central quantity in this upper bound is derived from a graphical representation of the information structure, and admits an interpretation as an effective information-structural state, generalizing the typical notion of a Markovian state.

**Limitations.** The results of this paper are theoretical in nature, and the algorithm proposed to prove our main result is not computationally practical. Current SOTA algorithms for partially-observable algorithms are based on recurrent neural networks or other deep learning models, which create internal representations of latent states based on the history, akin to belief states. Our theory may offer insights into the design of improved architectures in such deep learning-based algorithms. For example, the information structure can be incorporated as an inductive bias of the neural network.