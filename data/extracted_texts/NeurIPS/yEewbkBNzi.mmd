# Convergence of Adam Under Relaxed Assumptions

Haochuan Li

MIT

haochuan@mit.edu &Alexander Rakhlin

MIT

rakhlin@mit.edu &Ali Jadbabaie

MIT

jadbabai@mit.edu

###### Abstract

In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to \(\)-stationary points with \((^{-4})\) gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory of Adam, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradient complexity of \((^{-3})\).

## 1 Introduction

In this paper, we study the non-convex unconstrained stochastic optimization problem

\[_{x}\{f(x)=_{}[f(x,)]\}.\] (1)

The Adaptive Moment Estimation (Adam) algorithm  has become one of the most popular optimizers for solving (1) when \(f\) is the loss for training deep neural networks. Owing to its efficiency and robustness to hyper-parameters, it is widely applied or even sometimes the default choice in many machine learning application domains such as natural language processing [44; 4; 13], generative adversarial networks [35; 21; 55], computer vision , and reinforcement learning [28; 33; 40]. It is also well known that Adam significantly outperforms stochastic gradient descent (SGD) for certain models like transformer [50; 24; 1].

Despite its success in practice, theoretical analyses of Adam are still limited. The original proof of convergence in  was later shown by  to contain gaps. The authors in  also showed that for a range of momentum parameters chosen _independently with the problem instance_, Adam does not necessarily converge even for convex objectives. However, in deep learning practice, the hyper-parameters are in fact _problem-dependent_ as they are usually tuned after given the problem and weight initialization. Recently, there have been many works proving the convergence of Adam for non-convex functions with various assumptions and problem-dependent hyper-parameter choices. However, these results leave significant room for improvement. For example, [12; 19] prove the convergence to stationary points assuming the gradients are bounded by a constant, either explicitly or implicitly. On the other hand, [51; 45] consider weak assumptions, but their convergence results are still limited. See Section 2 for more detailed discussions of related works.

To address the above-mentioned gap between theory and practice, we provide a new convergence analysis of Adam _without assuming bounded gradients_, or equivalently, Lipschitzness of the objective function. In addition, we also relax the standard global smoothness assumption, i.e., the Lipschitzness of the gradient function, as it is far from being satisfied in deep neural network training. Instead, weconsider a more general, relaxed, and non-uniform smoothness condition according to which the local smoothness (i.e., Hessian norm when it exists) around \(x\) is bounded by a sub-quadratic function of the gradient norm \(\| f(x)\|\) (see Definition 3.2 and Assumption 2 for the details). This generalizes the \((L_{0},L_{1})\) smoothness condition proposed by  based on language model experiments. Even though our assumptions are much weaker and more realistic, we can still obtain the same \((^{-4})\) gradient complexity for convergence to an \(\)-stationary point.

The key to our analysis is a new technique to obtain a high probability, constant upper bound on the gradients along the optimization trajectory of Adam, without assuming Lipschitzness of the objective function. In other words, it essentially turns the bounded gradient assumption into a result that can be directly proven. Bounded gradients imply bounded stepsize at each step, with which the analysis of Adam essentially reduces to the simpler analysis of AdaBound . Furthermore, once the gradient boundedness is achieved, the analysis under the generalized non-uniform smoothness assumption is not much harder than that under the standard smoothness condition. We will introduce the technique in more details in Section 5. We note that the idea of bounding gradient norm along the trajectory of the optimization algorithm can be use in other problems as well. For more details, we refer the reader to our concurrent work  in which we present a set of new techiniques and methods for bounding gradient norm for other optimization algorithms under a generalized smoothness condition.

Another contribution of this paper is to show that the gradient complexity of Adam can be further improved with variance reduction methods. To this end, we propose a variance-reduced version of Adam by modifying its momentum update rule, inspired by the idea of the STORM algorithm . Under additional generalized smoothness assumption of _the component function_\(f(,)\) for each \(\), we show that this provably accelerates the convergence with a gradient complexity of \((^{-3})\). This rate improves upon the existing result of  where the authors obtain an asymptotic convergence of their approach to variance reduction for Adam in the non-convex setting, under the bounded gradient assumption.

### Contributions

In light of the above background, we summarize our main contributions as follows.

* We develop a new analysis to show that Adam converges to stationary points under relaxed assumptions. In particular, we do not assume bounded gradients or Lipschitzness of the objective function. Furthermore, we also consider a generalized non-uniform smoothness condition where the local smoothness or Hessian norm is bounded by a _sub-quadratic_ function of the gradient norm. Under these more realistic assumptions, we obtain a _dimension free_ gradient complexity of \((^{-4})\) if the gradient noise is centered and bounded.
* We generalize our analysis to the setting where the gradient noise is centered and has sub-Gaussian norm, and show the convergence of Adam with a gradient complexity of \((^{-4}^{3.25}(1/))\).
* We propose a variance-reduced version of Adam (VRAdam) with provable convergence guarantees. In particular, we obtain the accelerated \((^{-3})\) gradient complexity.

## 2 Related work

In this section, we discuss the relevant literature related to convergence of Adam and the generalized smoothness condition, and defer additional related work on variants of Adam and variance reduction methods to Appendix A.

**Convergence of Adam.** Adam was first proposed by Kingma and Ba  with a theoretical convergence guarantee for convex functions. However, Reddi et al.  found a gap in the proof of this convergence analysis, and also constructed counter-examples for a range of hyper-parameters on which Adam does not converge. That being said, the counter-examples depend on the hyper-parameters of Adam, i.e., they are constructed after picking the hyper-parameters. Therefore, it does not rule out the possibility of obtaining convergence guarantees for problem-dependent hyper-parameters, as also pointed out by [42; 51].

Many recent works have developed convergence analyses of Adam with various assumptions and hyper-parameter choices. Zhou et al.  show Adam with certain hyper-parameters can work on the counter-examples of . De et al.  prove convergence for general non-convex functionsassuming gradients are bounded and the signs of stochastic gradients are the same along the trajectory. The analysis in  also relies on the bounded gradient assumption. Guo et al.  assume the adaptive stepsize is upper and lower bounded by two constants, which is not necessarily satisfied unless assuming bounded gradients or considering the AdaBound variant . [51; 45] consider very weak assumptions. However, they show either 1) "convergence" only to some neighborhood of stationary points with a constant radius, unless assuming the strong growth condition; or 2) convergence to stationary points but with a slower rate.

**Generalized smoothness condition.** Generalizing the standard smoothness condition in a variety of settings has been a focus of many recent papers. Recently,  proposed a generalized smoothness condition called \((L_{0},L_{1})\) smoothness, which assumes the local smoothness or Hessian norm is bounded by an affine function of the gradient norm. The assumption was well-validated by extensive experiments conducted on language models. Various analyses of different algorithms under this condition were later developed [48; 34; 52; 17; 38; 8]. One recent closely-related work is  which studies converges of Adam under the \((L_{0},L_{1})\) smoothness condition. However, their results are still limited, as we have mentioned above. In this paper, we consider an even more general smoothness condition where the local smoothness is bounded by a sub-quadratic function of the gradient norm, and prove the convergence of Adam under this condition. In our concurrent work , we further analyze various other algorithms in both convex and non-convex settings under similar generalized smoothness conditions following the same key idea of bounding gradients along the trajectory.

## 3 Preliminaries

**Notation.** Let \(\|\|\) denote the Euclidean norm of a vector or spectral norm of a matrix. For any given vector \(x\), we use \((x)_{i}\) to denote its \(i\)-th coordinate and \(x^{2}\), \(\), \(|x|\) to denote its coordinate-wise square, square root, and absolute value respectively. For any two vectors \(x\) and \(y\), we use \(x y\) and \(x/y\) to denote their coordinate-wise product and quotient respectively. We also write \(x y\) or \(x y\) to denote the coordinate-wise inequality between \(x\) and \(y\), which means \((x)_{i}(y)_{i}\) or \((x)_{i}(y)_{i}\) for each coordinate index \(i\). For two symmetric real matrices \(A\) and \(B\), we say \(A B\) or \(A B\) if \(B-A\) or \(A-B\) is positive semi-definite (PSD). Given two real numbers \(a,b\), we denote \(a b:=\{a,b\}\) for simplicity. Finally, we use \(()\), \(()\), and \(()\) for the standard big-O, big-Theta, and big-Theta, and big-Omega notation.

### Description of the Adam algorithm

```
1:Input:\(,_{},,,T,x_{}\)
2:Initialize\(m_{0}=v_{0}=0\) and \(x_{1}=x_{}\)
3:for\(t=1,,T\)do
4: Draw a new sample \(_{t}\) and perform the following updates
5:\(m_{t}=(1-)m_{t-1}+ f(x_{t},_{t})\)
6:\(v_{t}=(1-_{})v_{t-1}+_{}( f(x_{t},_{t}) )^{2}\)
7:\(_{t}=}{1-(1-)^{t}}\)
8:\(_{t}=)}})^{t}}\)
9:\(x_{t+1}=x_{t}-_{t}+}}_{t}\)
10:endfor ```

**Algorithm 1**Adam

The formal definition of Adam proposed in  is shown in Algorithm 1, where Lines 5-9 describe the update rule of iterates \(\{x_{t}\}_{1 t T}\). Lines 5-6 are the updates for the first and second order momentum, \(m_{t}\) and \(v_{t}\), respectively. In Lines 7-8, they are re-scaled to \(_{t}\) and \(_{t}\) in order to correct the initialization bias due to setting \(m_{0}=v_{0}=0\). Then the iterate is updated by \(x_{t+1}=x_{t}-h_{t}_{t}\) where \(h_{t}=/(_{t}}+)\) is the adaptive stepsize vector for some parameters \(\) and \(\).

### Assumptions

In what follows below, we will state our main assumptions for analysis of Adam.

#### 3.2.1 Function class

We start with a standard assumption in optimization on the objective function \(f\) whose domain lies in a Euclidean space with dimension \(d\).

**Assumption 1**.: _The objective function \(f\) is differentiable and closed within its open domain \((f)^{d}\) and is bounded from below, i.e., \(f^{*}:=_{x}f(x)>-\)._

_Remark 3.1_.: A function \(f\) is said to be closed if its sub-level set \(\{x(f) f(x) a\}\) is closed for each \(a\). In addition, a continuous function \(f\) over an open domain is closed if and only \(f(x)\) tends to infinity whenever \(x\) approaches to the boundary of \((f)\), which is an important condition to ensure the iterates of Adam with a small enough stepsize \(\) stay within the domain with high probability. Note that this condition is mild since any continuous function defined over the entire space \(^{d}\) is closed.

Besides Assumption 1, the only additional assumption we make regarding \(f\) is that its local smoothness is bounded by a sub-quadratic function of the gradient norm. More formally, we consider the following \((,L_{0},L_{})\) smoothness condition with \(0<2\).

**Definition 3.2**.: A differentiable real-valued function \(f\) is \((,L_{0},L_{})\) smooth for some constants \(,L_{0},L_{} 0\) if the following inequality holds _almost everywhere_ in \((f)\)

\[\|^{2}f(x)\| L_{0}+L_{}\| f(x)\|^{ }.\]

_Remark 3.3_.: When \(=1\), Definition 3.2 reduces to the \((L_{0},L_{1})\) smoothness condition in . When \(=0\) or \(L_{}=0\), it reduces to the standard smoothness condition.

**Assumption 2**.: _The objective function \(f\) is \((,L_{0},L_{})\) smooth with \(0<2\)._

The standard smooth function class is very restrictive as it only contains functions that are upper and lower bounded by quadratic functions. The \((L_{0},L_{1})\) smooth function class is more general since it also contains, e.g., univariate polynomials and exponential functions. Assumption 2 is even more general and contains univariate rational functions, double exponential functions, etc. See Appendix D.1 for the formal propositions and proofs. We also refer the reader to our concurrent work  for more detailed discussions of examples of \((,L_{0},L_{})\) smooth functions for different \(\)s.

It turns out that bounded Hessian norm at a point \(x\) implies local Lipschitzness of the gradient in the neighborhood around \(x\). In particular, we have the following lemma.

**Lemma 3.4**.: _Under Assumptions 1 and 2, for any \(a>0\) and two points \(x(f),y^{d}\) such that \(\|y-x\|+L_{}(\| f(x)\|+a)^{ }}\), we have \(y(f)\) and_

\[\| f(y)- f(x)\|(L_{0}+L_{}(\| f(x) \|+a)^{})\|y-x\|.\]

_Remark 3.5_.: Lemma 3.4 can be actually used as the definition of \((,L_{0},L_{})\) smooth functions in place of Assumption 2. Besides the local gradient Lipschitz condition, it also suggests that, as long as the update at each step is small enough, the iterates will not go outside of the domain.

For the special case of \(=1\), choosing \(a=\{\| f(x)\|,L_{0}/L_{1}\}\), one can verify that the required locality size in Lemma 3.4 satisfies \(+L_{1}(\| f(x)\|+a)}}\). In this case, Lemma 3.4 states that \(\|x-y\| 1/(3L_{1})\) implies \(\| f(y)- f(x)\| 2(L_{0}+L_{1}\| f(x) \|)\|y-x\|.\) Therefore, it reduces to the local gradient Lipschitz condition for \((L_{0},L_{1})\) smooth functions in [49; 48] up to numerical constant factors. For \( 1\), the proof is more involved because Gronwall's inequality used in [49; 48] no longer applies. Therefore we defer the detailed proof of Lemma 3.4 to Appendix D.2.

#### 3.2.2 Stochastic gradient

We consider one of the following two assumptions on the stochastic gradient \( f(x_{t},_{t})\) in our analysis of Adam.

**Assumption 3**.: _The gradient noise is centered and almost surely bounded. In particular, for some \( 0\) and all \(t 1\),_

\[_{t-1}[ f(x_{t},_{t})]= f(x_{t}),\| f (x_{t},_{t})- f(x_{t})\|,\ a.s.,\]

_where \(_{t-1}[\,\,]:=[\,\,|_{1},,_{t-1}]\) is the conditional expectation given \(_{1},,_{t-1}\)._

**Assumption 4**.: _The gradient noise is centered with sub-Gaussian norm. In particular, for some \(R 0\) and all \(t 1\),_

\[_{t-1}[ f(x_{t},_{t})]= f(x_{t}),_{t-1 }(\| f(x_{t},_{t})- f(x_{t})\| s) 2 e^{-}{2R^{2}}},\  s,\]

_where \(_{t-1}[\,\,]:=[\,\,|_{1},,_{t-1}]\) and \(_{t-1}[\,\,]:=[\,\,|_{1},,_{t-1}]\) are the conditional expectation and probability given \(_{1},,_{t-1}\)._Assumption 4 is strictly weaker than Assumption 3 since an almost surely bounded random variable clearly has sub-Gaussian norm, but it results in a slightly worse convergece rate up to poly-log factors (see Theorems 4.1 and 4.2). Both of them are stronger than the most standard bounded variance assumption \([\| f(x_{t},_{t})- f(x_{t})\|^{2}]^{2}\) for some \( 0\), although Assumption 3 is actually a common assumption in existing analyses under the \((L_{0},L_{1})\) smoothness condition (see e.g. [49; 48]). The extension to the bounded variance assumption is challenging and a very interesting future work as it is also the assumption considered in the lower bound . We suspect that such an extension would be straightforward if we consider a mini-batch version of Algorithm 1 with a batch size of \(S=(^{-2})\), since this results in a very small variance of \((^{2})\) and thus essentially reduces the analysis to the deterministic setting. However, for practical Adam with an \((1)\) batch size, the extension is challenging and we leave it as a future work.

## 4 Results

In the section, we provide our convergence results for Adam under Assumptions 1, 2, and 3 or 4. To keep the statements of the theorems concise, we first define several problem-dependent constants. First, we let \(_{1}:=f(x_{1})-f^{*}<\) be the initial sub-optimality gap. Next, given a large enough constant \(G>0\), we define

\[r:=\{G^{-1}},^{ }-L_{})^{1/}}\}, L:=3L_{0}+4L_{}G^{},\] (2)

where \(L\) can be viewed as the effective smoothness constant along the trajectory if one can show \(\| f(x_{t})\| G\) and \(\|x_{t+1}-x_{t}\| r\) at each step (see Section 5 for more detailed discussions). We will also use \(c_{1},c_{2}\) to denote some small enough numerical constants and \(C_{1},C_{2}\) to denote some large enough ones. The formal convergence results under Assumptions 1, 2, and 3 are presented in the following theorem, whose proof is deferred in Appendix E.

**Theorem 4.1**.: _Suppose Assumptions 1, 2, and 3 hold. Denote \(:=(1/)\) for any \(0<<1\). Let \(G\) be a constant satisfying \(G\{2,2,_{1}L_{0}},(C_{1}_{1}L _{})^{}\}\). Choose_

\[0_{ sq} 1,\{1, ^{2}}{^{2}G}\}, c_{2}\{ ,\ },}{L }\}.\]

_Let \(T=\{},\ _{1}G}{^{2}}\}\). Then with probability at least \(1-\), we have \(\| f(x_{t})\| G\) for every \(1 t T\), and \(_{t=1}^{T}\| f(x_{t})\|^{2}^{2}\)._

Note that \(G\), the upper bound of gradients along the trajectory, is a constant that depends on \(,,L_{0},L_{}\), and the initial sub-optimality gap \(_{1}\), but not on \(\). There is no requirement on the second order momentum parameter \(_{ sq}\), although many existing works like [12; 51; 45] need certain restrictions on it. We choose very small \(\) and \(\), both of which are \((^{2})\). Therefore, from the choice of \(T\), it is clear that we obtain a gradient complexity of \((^{-4})\), where we only consider the leading term. We are not clear whether the dependence on \(\) is optimal or not, as the \((^{-4})\) lower bound in  assumes the weaker bounded variance assumption than our Assumption 3. However, it matches the state-of-the-art complexity among existing analyses of Adam.

One limitation of the dependence of our complexity on \(\) is \((^{-2})\), which might be large since \(\) is usually small in practice, e.g., the default choice is \(=10^{-8}\) in the PyTorch implementation. There are some existing analyses on Adam [12; 51; 45] whose rates do not depend explicitly on \(\) or only depend on \((1/)\). However, all of them depend on \((d)\), whereas our rate is dimension free. The dimension \(d\) is also very large, especially when training transformers, for which Adam is widely used. We believe that independence on \(d\) is better than that on \(\), because \(d\) is fixed given the architecture of the neural network but \(\) is a hyper-parameter which we have the freedom to tune. In fact, based on our preliminary experimental results on CIFAR-10 shown in Figure 1, the performance of Adam is not very sensitive to the choice of \(\). Although the default choice of \(\) is \(10^{-8}\), increasing it up to \(0.01\) only makes minor differences.

As discussed in Section 3.2.2, we can generalize the bounded gradient noise condition in Assumption 3 to the weaker sub-Gaussian noise condition in Assumption 4. The following theorem formally shows the convergence result under Assumptions 1, 2, and 4, whose proof is deferred in Appendix E.6.

**Theorem 4.2**.: _Suppose Assumptions 1, 2, and 4 hold. Denote \(:=(2/)\) and \(:=R\) for any \(0<<1\). Let \(G\) be a constant satisfying \(G\{2,2,_{1}L_{0}},(C_{1}_{1}L_{ })^{}\}\). Choose_

\[0_{ sq} 1,\{1,^{ 2}}{^{2}G}\}, c_{2}\{,\ },}{L }\}.\]

_Let \(T=\{},\ _{1}G}{^{2}}\}\). Then with probability at least \(1-\), we have \(\| f(x_{t})\| G\) for every \(1 t T\), and \(_{t=1}^{T}\| f(x_{t})\|^{2}^{2}\)._

Note that the main difference of Theorem 4.2 from Theorem 4.1 is that \(\) is now \(()\) instead of a constant. With some standard calculations, one can show that the gradient complexity in Theorem 4.2 is bounded by \((^{-4}^{p}(1/))\), where \(p=\{3,\}<3.25\).

## 5 Analysis

### Bounding the gradients along the optimization trajectory

We want to bound the gradients along the optimization trajectory mainly for two reasons. First, as discussed in Section 2, many existing analyses of Adam rely on the assumption of bounded gradients, because unbounded gradient norm leads to unbounded second order momentum \(_{t}\) which implies very small stepsize, and slow convergence. On the other hand, once the gradients are bounded, it is straightforward to control \(_{t}\) as well as the stepsize, and therefore the analysis essentially reduces to the easier one for AdaBound. Second, informally speaking1, under Assumption 2, bounded gradients also imply bounded Hessians, which essentially reduces the \((,L_{0},L_{})\) smoothness to the standard smoothness. See Section 5.2 for more formal discussions.

In this paper, instead of imposing the strong assumption of globally bounded gradients, we develop a new analysis to show that with high probability, the gradients are always bounded along the trajectory of Adam until convergence. The essential idea can be informally illustrated by the following "circular" reasoning that we will make precise later. On the one hand, if \(\| f(x_{t})\| G\) for every \(t 1\), it is not hard to show the gradient converges to zero based on our discussions above. On the other hand, we know that a converging sequence must be upper bounded. Therefore there exists some \(G^{}\) such that \(\| f(x_{t})\| G^{}\) for every \(t 1\). In other words, the bounded gradient condition implies the convergence result and the convergence result also implies the boundedness condition, forming a circular argument.

This circular argument is of course flawed. However, we can break the circularity of reasoning and rigorously prove both the bounded gradient condition and the convergence result using a contradiction

Figure 1: Test errors of different models trained on CIFAR-10 using the Adam optimizer with \(=0.9,_{ sq}=0.999,=0.001\) and different \(\)s. From left to right: (a) a shallow CNN with 6 layers; (b) ResNet-Small with 20 layers; and (c) ResNet110 with 110 layers.

argument. Before introducing the contradiction argument, we first need to provide the following useful lemma, which is the reverse direction of a generalized Polyak-Lojasiewicz (PL) inequality, whose proof is deferred in Appendix D.3.

**Lemma 5.1**.: _Under Assumptions 1 and 2, we have \(\| f(x)\|^{2} 3(3L_{0}+4L_{}\,\| f(x)\|^{})(f(x)-f^{*})\)._

Define the function \((u):=}{3(3L_{0}+4L_{}u^{})}\) over \(u 0\). It is easy to verify that if \(<2\), \(\) is increasing and its range is \([0,)\). Therefore, \(\) is invertible and \(^{-1}\) is also increasing. Then, for any constant \(G>0\), denoting \(F=(G)\), Lemma 5.1 suggests that if \(f(x)-f^{*} F\), we have

\[\| f(x)\|^{-1}(f(x)-f^{*})^{-1}(F)=G.\]

In other words, if \(<2\), the gradient is bounded within any sub-level set, even though the sub-level set could be unbounded. Then, let \(\) be the first time the sub-optimality gap is strictly greater than \(F\), truncated at \(T+1\), or formally,

\[:=\{t f(x_{t})-f^{*}>F\}(T+1).\] (3)

Then at least when \(t<\), we have \(f(x_{t})-f^{*} F\) and thus \(\| f(x_{t})\| G\). Based on our discussions above, it is not hard to analyze the updates before time \(\), and one can contruct some Lyapunov function to obtain an upper bound on \(f(x_{})-f^{*}\). On the other hand, if \( T\), we immediately obtain a lower bound on \(f(x_{})\), that is \(f(x_{})-f^{*}>F\), by the definition of \(\) in (3). If the lower bound is greater than the upper bound, it leads to a contradiction, which shows \(=T+1\), i.e., the sub-optimality gap and the gradient norm are always bounded by \(F\) and \(G\) respectively before the algorithm terminates. We will illustrate the technique in more details in the simple deterministic setting in Section 5.3, but first, in Section 5.2, we introduce several prerequisite lemmas on the \((,L_{0},L_{})\) smoothness.

### Local smoothness

In Section 5.1, we informally mentioned that \((,L_{0},L_{})\) smoothness essentially reduces to the standard smoothness if the gradient is bounded. In this section, we will make the statement more precise. First, note that Lemma 3.4 implies the following useful corollary.

**Corollary 5.2**.: _Under Assumptions 1 and 2, for any \(G>0\) and two points \(x(f),y^{d}\) such that \(\| f(x)\| G\) and \(\|y-x\| r:=\{G^{-1}},^{- 1}L_{})^{1/}}\}\), denoting \(L:=3L_{0}+4L_{}G^{}\), we have \(y(f)\) and_

\[\| f(y)- f(x)\| L\,\|y-x\|\,, f(y) f(x)+  f(x),y-x+\,\|y-x\|^{2}\,.\]

The proof of Corollary 5.2 is deferred in Appendix D.4. Although the inequalities in Corollary 5.2 look very similar to the standard global smoothness condition with constant \(L\), it is still a local condition as it requires \(\|x-y\| r\). Fortunately, at least before \(\), such a requirement is easy to satisfy for small enough \(\), according to the following lemma whose proof is deferred in Appendix E.5.

**Lemma 5.3**.: _Under Assumption 3, if \(t<\) and choosing \(G\), we have \(\|x_{t+1}-x_{t}\| D\) where \(D:=2G/\)._

Then as long as \( r/D\), we have \(\|x_{t+1}-x_{t}\| r\) which satisfies the requirement in Corollary 5.2. Then we can apply the inequalities in it in the same way as the standard smoothness condition. In other words, most classical inequalities derived for standard smooth functions also apply to \((,L_{0},L_{})\) smooth functions.

### Warm-up: analysis in the deterministic setting

In this section, we consider the simpler deterministic setting where the stochastic gradient \( f(x_{t},_{t})\) in Algorithm 1 or (18) is replaced with the exact gradient \( f(x_{t})\). As discussed in Section 5.1, the key in our contradiction argument is to obtain both upper and lower bounds on \(f(x_{})-f^{*}\). In the following derivations, we focus on illustrating the main idea of our analysis technique and ignore minor proof details. In addition, all of them are under Assumptions 1, 2, and 3.

In order to obtain the upper bound, we need the following two lemmas. First, denoting \(_{t}:=_{t}- f(x_{t})\), we can obtain the following informal descent lemma for deterministic Adam.

**Lemma 5.4** (Descent lemma, informal).: _For any \(t<\), choosing \(G\) and a small enough \(\),_

\[f(x_{t+1})-f(x_{t})-\| f(x_{t})\|^{2 }+\|_{t}\|^{2},\] (4)

_where "\(\)" omits less important terms._

Compared with the standard descent lemma for gradient descent, there is an additional term of \(\|_{t}\|^{2}\) in Lemma 5.4. In the next lemma, we bound this term recursively.

**Lemma 5.5** (Informal).: _Choosing \(=( G^{+1/2})\), if \(t<\), we have_

\[\|_{t+1}\|^{2} (1-/4)\|_{t}\|^{2}+\| f(x_{t})\|^{2}.\] (5)

The proof sketches of the above two lemmas are deferred in Appendix B. Now we combine them to get the upper bound on \(f(x_{})-f^{*}\). Define the function \(_{t}:=f(x_{t})-f^{*}+\|_{t}\| ^{2}\). Note that for any \(t<\), (4)\(+\)(5) gives

\[_{t+1}-_{t}-\| f(x_{t})\|^{2}.\] (6)

The above inequality shows \(_{t}\) is non-increasing and thus a Lyapunov function. Therefore, we have

\[f(x_{})-f^{*}_{}_{1}=_{1},\]

where in the last inequality we use \(_{1}=f(x_{1})-f^{*}=_{1}\) since \(_{1}=_{1}- f(x_{1})=0\) in the deterministic setting.

As discussed in Section 5.1, if \( T\), we have \(F<f(x_{})-f^{*}_{1}\). Note that we are able to choose a large enough constant \(G\) so that \(F=}{3(3L_{0}+4L_{}G^{})}\) is greater than \(_{1}\), which leads to a contradiction and shows \(=T+1\). Therefore, (6) holds for all \(1 t T\). Taking a summation over \(1 t T\) and re-arranging terms, we get

\[_{t=1}^{T}\| f(x_{t})\|^{2}-_{T+1})}{ T}}{ T}^{2},\]

if choosing \(T}{^{2}}\), i.e., it shows convergence with a gradient complexity of \((^{-2})\) since both \(G\) and \(\) are constants independent of \(\) in the deterministic setting.

### Extension to the stochastic setting

In this part, we briefly introduce how to extend the analysis to the more challenging stochastic setting. It becomes harder to obtain an upper bound on \(f(x_{})-f^{*}\) because \(_{t}\) is no longer non-increasing due to the existence of noise. In addition, \(\) defined in (3) is now a random variable. Note that all the derivations, such as Lemmas 5.4 and 5.5, are conditioned on the random event \(t<\). Therefore, one can not simply take a total expectation of them to show \([_{t}]\) is non-increasing.

Fortunately, \(\) is in fact a stopping time with nice properties. If the noise is almost surely bounded as in Assumption 3, by a more careful analysis, we can obtain a high probability upper bound on \(f(x_{})-f^{*}\) using concentration inequalities. Then we can still obtain a contradiction and convergence under this high probability event. If the noise has sub-Gaussian norm as in Assumption 4, one can change the definition of \(\) to

\[:=\{t f(x_{t})-f^{*}>F\}\{t\| f(x_{t})-  f(x_{t},_{t})\|>\}(T+1)\]

for appropriately chosen \(F\) and \(\). Then at least when \(t<\), the noise is bounded by \(\). Hence we can get the same upper bound on \(f(x_{})-f^{*}\) as if Assumption 3 still holds. However, when \(t T\), the lower bound \(f(x_{})-f^{*}>F\) does not necessarily holds, which requires some more careful analyses. The details of the proofs are involved and we defer them in Appendix E.

## 6 Variance-reduced Adam

In this section, we propose a variance-reduced version of Adam (VRAdam). This new algorithm is depicted in Algorithm 2. Its main difference from the original Adam is that in the momentum updaterule (Line 6), an additional term of \((1-)( f(x_{t},_{t})- f(x_{t-1},_{t}))\) is added, inspired by the STORM algorithm . This term corrects the bias of \(m_{t}\) so that it is an unbiased estimate of \( f(x_{t})\) in the sense of total expectation, i.e., \([m_{t}]= f(x_{t})\). We will also show that it reduces the variance and accelerates the convergence.

Aside from the adaptive stepsize, one major difference between Algorithm 2 and STORM is that our hyper-parameters \(\) and \(\) are fixed constants whereas theirs are decreasing as a function of \(t\). Choosing constant hyper-parameters requires a more accurate estimate at the initialization. That is why we use a mega-batch \(_{1}\) to evaluate the gradient at the initial point to initialize \(m_{1}\) and \(v_{1}\) (Lines 2-3). In practice, one can also do a full-batch gradient evaluation at initialization. Note that there is no initialization bias for the momentum, so we do not re-scale \(m_{t}\) and only re-scale \(v_{t}\). We also want to point out that although the initial mega-batch gradient evaluation makes the algorithm a bit harder to implement, constant hyper-parameters are usually easier to tune and more common in training deep neural networks. It should be not hard to extend our analysis to time-decreasing \(\) and \(\) and we leave it as an interesting future work.

```
1:Input:\(,_{},,,T,S_{1},x_{}\)
2:Draw a batch of samples \(_{1}\) with size \(S_{1}\) and use them to evaluate the gradient \( f(x_{},_{1})\).
3:Initialize\(m_{1}= f(x_{},_{1})\), \(v_{1}=_{}m_{1}^{2}\), and \(x_{2}=x_{}-}{|m_{1}|+}\).
4:for\(t=2,,T\)do
5: Draw a new sample \(_{t}\) and perform the following updates:
6:\(m_{t}=(1-)m_{t-1}+ f(x_{t},_{t})+(1-)( f(x _{t},_{t})- f(x_{t-1},_{t}))\)
7:\(v_{t}=(1-_{})v_{t-1}+_{}( f(x_{t},_{t}))^ {2}\)
8:\(_{t}=}{1-(1-_{})^{t}}\)
9:\(x_{t+1}=x_{t}-+}} m_{t}\)
10:endfor ```

**Algorithm 2** Variance-Reduced Adam (VRAdam)

In addition to Assumption 1, we need to impose the following assumptions which can be viewed as stronger versions of Assumptions 2 and 3, respectively.

**Assumption 5**.: _The objective function \(f\) and the component function \(f(,)\) for each fixed \(\) are \((,L_{0},L_{})\) smooth with \(0<2\)._

**Assumption 6**.: _The random variables \(\{_{t}\}_{1 t T}\) are sampled i.i.d. from some distribution \(\) such that for any \(x(f)\),_

\[_{}[ f(x,)]= f(x),\| f(x,)- f(x)\|,\ a.s.\]

_Remark 6.1_.: Assumption 6 is stronger than Assumption 3. Assumption 3 applies only to the iterates generated by the algorithm, while Assumption 6 is a pointwise assumption over all \(x(f)\) and further assumes an i.i.d. nature of the random variables \(\{_{t}\}_{1 t T}\). Also note that, similar to Adam, it is straightforward to generalize the assumption to noise with sub-Gaussian norm as in Assumption 4.

### Analysis

In this part, we briefly discuss challenges in the analysis of VRAdam. The detailed analysis is deferred in Appendix F. Note that Corollary 5.2 requires bounded update \(\|x_{t+1}-x_{t}\| r\) at each step. For Adam, it is easy to satisfy for a small enough \(\) according to Lemma 5.3. However, for VRAdam, obtaining a good enough almost sure bound on the update is challenging even though the gradient noise is bounded. To bypass this difficulty, we directly impose a bound on \(\| f(x_{t})-m_{t}\|\) by changing the definition of the stopping time \(\), similar to how we deal with the sub-Gaussian noise condition for Adam. In particular, we define

\[:=\{t\| f(x_{t})\|>G\}\{t\| f(x_{t})-m_ {t}\|>G\}(T+1).\]

Then by definition, both \(\| f(x_{t})\|\) and \(\| f(x_{t})-m_{t}\|\) are bounded by \(G\) before time \(\), which directly implies bounded update \(\|x_{t+1}-x_{t}\|\). Of course, the new definition brings new challenges to lower bounding \(f(x_{})-f^{*}\), which requires more careful analyses specific to the VRAdam algorithm. Please see Appendix F for the details.

### Convergence guarantees for VRAdam

In the section, we provide our main results for convergence of VRAdam under Assumptions 1, 5, and 6. We consider the same definitions of problem-dependent constants \(_{1},r,L\) as those in Section 4 to make the statements of theorems concise. Let \(c\) be a small enough numerical constant and \(C\) be a large enough numerical constant. The formal convergence result is shown in the following theorem.

**Theorem 6.2**.: _Suppose Assumptions 1, 5, and 6 hold. For any \(0<<1\), let \(G>0\) be a constant satisfying \(G\{2,2,L_{0}/},(C_ {1}L_{}/)^{}\}.\) Choose \(0_{} 1\) and \(=a^{2}^{2}\), where \(a=40L^{-3/2}\). Choose_

\[ c\{,, }{_{1}L^{2}},}{ GL}\}, T=}{ ^{2}}, S_{1}T}.\]

_Then with probability at least \(1-\), we have \(\| f(x_{t})\| G\) for every \(1 t T\), and \(_{t=1}^{T}\| f(x_{t})\|^{2}^{2}\)._

Note that the choice of \(G\), the upper bound of gradients along the trajectory of VRAdam, is very similar to that in Theorem 4.1 for Adam. The only difference is that now it also depends on the failure probability \(\). Similar to Theorem 4.1, there is no requirement on \(_{}\) and we choose a very small \(=(^{2})\). However, the variance reduction technique allows us to take a larger stepsize \(=()\) (compared with \((^{2})\) for Adam) and obtain an accelerated gradient complexity of \((^{-3})\), where we only consider the leading term. We are not sure whether it is optimal as the \((^{-3})\) lower bound in  assumes the weaker bounded variance condition. However, our result significantly improves upon , which considers a variance-reduced version of Adam by combining Adam and SVRG  and only obtains asymptotic convergence in the non-convex setting. Similar to Adam, our gradient complexity for VRAdam is dimension free but its dependence on \(\) is \((^{-2})\). Another limitation is that, the dependence on the failure probability \(\) is polynomial, worse than the poly-log dependence in Theorem 4.1 for Adam.

## 7 Conclusion and future works

In this paper, we proved the convergence of Adam and its variance-reduced version under less restrictive assumptions compared to those in the existing literature. We considered a generalized non-uniform smoothness condition, according to which the Hessian norm is bounded by a sub-quadratic function of the gradient norm almost everywhere. Instead of assuming the Lipschitzness of the objective function as in existing analyses of Adam, we use a new contradiction argument to prove that gradients are bounded by a constant along the optimization trajectory. There are several interesting future directions that one could pursue following this work.

**Relaxation of the bounded noise assumption.** Our analysis relies on the assumption of bounded noise or noise with sub-Gaussian norm. However, the existing lower bounds in  consider the weaker bounded variance assumption. Hence, it is not clear whether the \((^{-4})\) complexity we obtain for Adam is tight in this setting. It will be interesting to see whether one can relax the assumption to the bounded variance setting. One may gain some insights from recent papers such as [16; 46] that analyze AdaGrad under weak noise conditions. An alternative way to show the tightness of the \((^{-4})\) complexity is to prove a lower bound under the bounded noise assumption.

**Potential applications of our technique.** Another interesting future direction is to see if the techniques developed in this work for bounding gradients (including those in the the concurrent work ) can be generalized to improve the convergence results for other optimization problems and algorithms. We believe it is possible so long as the function class is well behaved and the algorithm is efficient enough so that \(f(x_{})-f^{*}\) can be well bounded for some appropriately defined stopping time \(\).

**Understanding why Adam is better than SGD.** We want to note that our results can not explain why Adam is better than SGD for training transformers, because  shows that non-adaptive SGD converges with the same \((^{-4})\) gradient complexity under even weaker conditions. It would be interesting and impactful if one can find a reasonable setting (function class, gradient oracle, etc) under which Adam or other adaptive methods provably outperform SGD.