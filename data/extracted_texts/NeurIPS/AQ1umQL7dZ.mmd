# Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with MxDNA

Lifeng Qiao\({}^{1,2}\), Peng Ye\({}^{1,3}\), Yuchen Ren\({}^{1,4}\), Weiqiang Bai\({}^{1}\), Chaoqi Liang\({}^{1}\),

**Xinzhu Ma\({}^{1,3}\), Nanqing Dong\({}^{1}\), Wanli Ouyang\({}^{1,3}\)**

\({}^{1}\)Shanghai Artificial Intelligence Laboratory, \({}^{2}\)Shanghai Jiao Tong University

\({}^{3}\)The Chinese University of Hong Kong, \({}^{4}\)The University of Sydney

yepeng@pjlab.org.cn

Work done during an internship at Shanghai Artificial Intelligence Laboratory.Corresponding Author.

###### Abstract

Foundation models have made significant strides in understanding the genomic language of DNA sequences. However, previous models typically adopt the tokenization methods designed for natural language, which are unsuitable for DNA sequences due to their unique characteristics. In addition, the optimal approach to tokenize DNA remains largely under-explored, and may not be intuitively understood by humans even if discovered. To address these challenges, we introduce MxDNA, a novel framework where the model autonomously learns an effective DNA tokenization strategy through gradient decent. MxDNA employs a sparse Mixture of Convolution Experts coupled with a deformable convolution to model the tokenization process, with the discontinuous, overlapping, and ambiguous nature of meaningful genomic segments explicitly considered. On Nucleotide Transformer Benchmarks and Genomic Benchmarks, MxDNA demonstrates superior performance to existing methods with less pretraining data and time, highlighting its effectiveness. Finally, we show that MxDNA learns unique tokenization strategy distinct to those of previous methods and captures genomic functionalities at a token level during self-supervised pretraining. Our MxDNA aims to provide a new perspective on DNA tokenization, potentially offering broad applications in various domains and yielding profound insights. Code is available at https://github.com/qiaoqiaoLF/MxDNA.

## 1 Introduction

Foundation models in natural language processing (NLP) have achieved remarkable success, transforming how machines understand and generate human language . Inspired by this success, researchers are now exploring the application of foundation models to decode the complex "language" of genomic sequences, aiming to potentially revolutionize our understanding of genomics . Tokenization, a critical initial step in NLP models, leverages human knowledge of natural language structures such as grammar and punctuation to segment text into meaningful units. However, DNA sequences present a distinct challenge: they lack natural delimiters and their "grammar" is not readily understood by humans. These challenges make the tokenization process of DNA sequences not straightforward.

Various tokenization methods have been employed by existing foundation models to analyse DNA sequences . For example, single nucleotide tokenization  treats each nucleotide as an individual token, K-mer  segments the DNA blocks of k consecutive nucleotides, and Byte-PairEncoding (BPE)  iteratively merges the most frequent pairs of existing tokens. All of these methods are borrowed directly from NLP as depicted in Fig. 1, each with its own inherent limitations. Single nucleotide tokenization, while offering high resolution for input, leads to an extremely large number of tokens, significantly increasing the complexity of the model. K-mer comes in two forms: overlapping and non-overlapping. Overlapping K-mer, despite its attempt to capture more contextual information, does not offer substantial benefits over single nucleotide approaches and can suffer from information leakage [5; 9]. Non-overlapping K-mer greatly reduces tokenized sequence length but can disrupt a potentially meaningful unit by splitting it into separate K-mers. BPE, adopted from NLP, attempts to optimize vocabulary size but often results in suboptimal segmentation that may not correspond to meaningful units [10; 11].

Unlike natural languages, where linguistically meaningful units such as words and sentences are almost standardized and well understood, the optimal approach to tokenize DNA remains under-explored due to the complex and varied nature of genomics. In NLP, common tokenization strategies have been validated by human knowledge, but such understanding does not extend to DNA. Consequently, rather than relying manually crafted tokenization rules, it may be better to trust a neural network to learn and determine the most effective tokenization strategy for genomic sequences. Additionally, recent research suggests that biologically meaningful protein tokens can be discontinuous, overlapping, and may require mapping to several tokenization possibilities [8; 12; 13], properties that are likely applicable to DNA sequences due to the genetic central dogma . To handle these complexities, we can further equip our model with capabilities to manage discontinuities, overlaps, and the ambiguities of genomic sequences explicitly.

Building on the analysis above, we introduce MxDNA ("Mx" draws from **M**ixture of **F**xperts ), a novel framework designed to autonomously learn an effective DNA tokenization strategy solely through gradient decent. The core of the framework starts with a sparse Mixture of Convolution Experts that identifies and embeds basic units within DNA sequences. Unlike conventional Mixture of Experts models, which focus on scaling up the model while maintaining computational efficiency [15; 16; 17], the experts in MxDNA are uniquely designed to capture DNA basic units of varied lengths. Following this, a deformable convolution [18; 19] assembles these basic units into final tokens. Throughout the process, MxDNA is explicitly equipped to manage the inherent discontinuities, overlaps and ambiguities in genomic sequences, enabling it to handle complex biological characteristics it encounters. Furthermore, we incorporate a cross-attention mechanism to align the output resolution with the original input during pretraining on a masked language modeling task .

The proposed MxDNA demonstrates strong performance on both the Nucleotide Transformer Benchmarks  and the Genomic Benchmarks . Despite only being pretrained on human reference genome, it still outperforms or matches previous models [7; 4; 6; 5]--some pretrained on multi-species data and for much longer duration, achieving state-of-the-art average performance and the best on 15 of the 26 individual tasks. Finally, by visualizing the learnt tokenization process, we illustrate that MxDNA learns unique tokenization strategy distinct to those of previous methods

Figure 1: Evolution of tokenization and Ideal Properties. **Left:** The progression from basic tokenization methods to more sophisticated techniques, with the direct but unsuitable applications from natural language to genomic language. **Right:** the ideal tokenization properties for genomics—Meaningful, Discontinuous, Overlapping, and Ambiguous—outlined in , which our MxDNA aims to achieve.

and captures genomic functionalities at a token level during self-supervised pretraining, potentially offering novel biological insights. Our contributions can be summarized as follows:

* **Learnt Tokenization**: We highlight the unsuitability of current DNA tokenization methods directly borrowed from NLP. Based on the belief that humans may not know the best tokenization approach but a model could potentially discover it, we propose a novel approach where the model autonomously learns an effective tokenization strategy.
* **Architectural Design**: We introduce a sparse Mixture of Convolution Experts coupled with a deformable convolution to dynamically learn tokenization, specifically designed to manage the inherent discontinuities overlaps and ambiguities in genomic sequences. Additionally, we leverage cross attention to align input and output sequence length to enable self-supervised pretraining.
* **Empirical Results**: MxDNA demonstrates robust quantitative performance with less pretraining data compared to some existing models, achieving state-of-the-art average performance on both Nucleotide Transformer Benchmarks and Genomic Benchmarks. Furthermore, visual analysis of the tokenization behaviour and the token embedding space highlights the unique strategy and capability to capture genomic functionalities at a token level of MxDNA, potentially offering new biological insights.

## 2 Background and Related Work

### Tokenization Methods

Tokenization is a fundamental step in both natural language processing (NLP) and DNA sequence modelling, transforming complex texts or DNA sequences into manageable tokens. In NLP, whitespace tokenization uses spaces and punctuation as delimiters but faces out-of-vocabulary issues. Similarly, in both fields, character (or single nucleotide in DNA) tokenization provides high resolution but can lead to computational inefficiency [21; 22; 7; 23]. N-gram in NLP and K-mer in DNA analysis both use contiguous sequences of N (K) items from given inputs [24; 25; 6; 26; 4], but can disrupt meaningful units due to their fixed-length nature (non-overlapping) or lead to potential information redundancy or leakage (overlapping) [5; 9]. Byte-Pair Encoding (BPE) is employed across both domains to reduce vocabulary size by merging frequent pairs of existing tokens [27; 5]. However, it might not adequately encode more complex patterns and are unreliable for finding linguistically sound tokens [11; 10]. These rule-based methods show limitations in different aspects, and our study aim to develop a learning-based tokenization method without these limitations.

### DNA Foundation Models

Recent advancements in DNA modeling have leveraged foundation models to decode the complex language of genomes. **DNABERT** pioneers the use of a BERT-like pretrained model for genomic sequence analysis, enhancing the understanding of nucleotide relationships via attention mechanisms. **Nucleotide Transformer** offers a comprehensive analysis of foundation models pretrained on DNA sequences, with model sizes reaching up to 2.5 billion parameters and pretraining data drawn from the 1000G human genomes and 850 various species. **DNABERT2** introduces an enhanced genome foundation model, utilizing an efficient BPE tokenizer and techniques to address input length constraints, resulting in reduced time and memory consumption while improving performance. **HyenaDNA** introduces a genomic foundation model capable of handling context with 1 million tokens at single nucleotide resolution, enabling the first exploration of in-context learning in genomics. **DNAGPT** extends the traditional GPT model by integrating tasks such as binary classification of DNA sequence order and numerical regression for predicting guanine-cytosine content, alongside developing a comprehensive token language. **Caducens** designs an architecture that leverages the long-range Mamba  block to support bi-directionality and reverse complementarity equivariance, addressing specific challenges in genomic analysis. Following VQVAE , **VQDNA** employs a convolutional encoder alongside a vector-quantized codebook to model tokenization, sharing a similar motivation with us yet ultimately adopting distinct solutions. Each work offers unique insights and innovations to the filed. Our research specifically concentrate on the tokenization methods for DNA, hopefully providing our unique contributions to the filed.

## 3 Method

### Motivation

The concept of "correct" tokenization in genomic sequences analysis remains undefined due to the complex nature of genomics. Unlike natural languages, where linguistically meaningful units are well-understood, biological units in genomics are not limited to contiguous nucleotide or amino acid sequences. Instead, they often encompass discontinuous, overlapping, and ambiguous segments crucial for understanding biological functions . Current DNA modeling practices directly borrow tokenization methods from natural language processing (NLP), such as single nucleotide tokenization, K-mer and Byte-Pair Encoding (BPE). These fixed, predefined approaches, though useful, often fail to capture the unique properties of DNA sequences, which lack explicit delimiters and consist of biologically meaningful units that defy simple segmentation.

Recognizing these challenges, MxDNA was developed based on the belief that although an optimal tokenization schema for genomic sequences is yet to be discovered, we can explicitly equip our model with the desired tokenization properties--such as handling discontinuities, overlaps, and ambiguities, and allow it to learn and adapt its tokenization strategy all by itself.

### Learnt Tokenization Module

This section introduces our learned tokenization module, which is central to our approach. The module first identifies meaningful basic units within the input sequence, which are then assembled into tokens with discontinuous, overlapping, and ambiguous properties. Implementation details are in Appx. A.2.

Figure 2: Our proposed MxDNA. **(Top) Overall pipeline of the MxDNA model:** Black arrows indicate pretraining data flow, and red arrows indicate finetuning data flow. The learnt tokenization module tokenizes single nucleotide input into learnt tokens. **(Bottom) Illustration of the learnt tokenization module:** Meaningful basic units are recognized with a linearly scoring layer and non-maximum suppression, embedded through convolution experts (Sec. 3.2.1), and assembled into final tokens by a deformable convolution. (Sec. 3.2.2) This process ensures meaningful, discontinuous, overlapping, and ambiguous tokenization, addressing the unique properties of genomic data.

#### 3.2.1 Basic Units Recognition

Basic Units ScoringInitially, MxDNA identifies the basic units that serve as the building blocks for tokens. It estimates the probability of the existence of various sized basic units centred at each nucleotide position by a linear gating mechanism commonly used in Mixture of Experts models. Following this, one-dimensional non-maximum suppression is applied to eliminate redundant proposals and select the most significant basic units.

Specifically, given the input nucleotide sequence \(X^{l d}\), where \(l\) is the sequence length and \(d\) is the hidden dimension, \(X\) is first linearly scored to produce \(S^{l n}\), where \(n\) represents the number of experts. Training-time multiplicative jitter noise  is applied to introduce **ambiguity**, while ensuring deterministic inference. The jitter noise is applied by multiplying the scores with a random factor sampled uniformly between \([1-0.01,1+0.01]\), resulting in slight perturbations to the probability distribution used for tokenization.

Modified non-maximum suppression is then applied to \(S\), where \(S_{ij}\) indicates the presence probability of a basic unit of length \(L_{j}\) centered at position \(i\), and \(L^{n}\) is a predefined set of lengths. The results are tracked using an expert mask \(M^{l}\), where each \(M_{i}\) is a natural integer indicating the presence of a basic unit's center of length \(M_{i}\) at position \(i\).

Basic Units EmbeddingAfter identifying the basic units, the nucleotides within each unit are aggregated to form embeddings. Convolution kernels of corresponding sizes are applied to the center of each basic unit to capture local features. The initial scoring and gating into specific convolution experts is similar to the Mixture of Experts paradigm, with each expert being a convolutional unit focusing on a specific segment rather than a single nucleotide.

Specifically, a basic unit at position \(i\) of length \(L_{j}=M_{i}\) is processed by the convolution expert \(E_{j}\) with kernel size \(L_{j}\), and weighted by \((S_{i})_{j}\), aggregating the nucleotides within the unit. This transforms the original input \(X^{l d}\) into an array of basic units \(U^{l d}\), where \(k\) is the number of basic units:

\[U_{i}=E_{j}(X_{[i-}{2}+1: i+}{2}})(S_{i})_{j},L_{j}=M_{i}&,M_{i}>0\\ 0&,M_{i}=0\] (1)

Then, the unwanted entries \(\{i|M_{i}=0\}\) of \(U\) are removed to keep the basic units \(U^{k d}\) only, where \(k\) is the number of basic units.

#### 3.2.2 Basic Units Assembly

Distal Relation EstimationBuilding upon the identified basic units, the more complex genomic patterns that extend beyond simple segmentation are modelled by a one-dimensional deformable convolution. This technique uniquely accommodates the modeling of complex local geometric transformations, adaptively adjusting to the input sequence. The linkages between the distal basic units are modeled by the offsets and modulation factors of each basic unit.

Following , offsets \( P^{k f}\) and modulation factors \( M^{k f}\) are computed based on the basic units \(U\) to model the distal relationships among them. This strategy ensures the combination of basic units is **discontinuous**, and reuses units across tokens achieve the **overlapping** property.

Final Tokens EmbeddingUsing the computed offsets and modulation factors, deformable convolution is applied to embed basic units into final tokens. The embedding process for each position incorporates deformations of the convolution kernel specified by the offsets, with the results modulated by the modulation factors.

Specifically, a one-dimensional deformable convolution with kernel size \(f\) is applied to embed these basic units into the final learnt tokens \(T^{k d}\):

\[T_{i}=_{p\{-+1,..., \}}w_{p} U_{i+p+ p} m\] (2)For a fractional location \(p^{}=i+p+ p\), bilinear interpolation is implemented as:

\[U_{p^{}}=_{q\{1,,k\}}(0,1-|p^{}-q|) U _{q}\] (3)

### Overall Pipeline

The framework begins with single nucleotide input represented as \(X_{input}^{l d}\). This single nucleotide resolution input allows for fine-grained analysis of genomics from the beginning.

Initially, \(X_{input}\) is processed through several transformer encoder blocks designed to extract global relationships within the sequence, producing \(X^{l d}\). This sets the stage for effective tokenization. Following this, the learnt tokenization module transforms the nucleotide sequence \(X\) into a more manageable form \(T^{k d}\), improving the efficiency and focus of subsequent layers. The tokenized output \(T\) is then passed through another series of transformer encoder blocks to further refine the token representation to \(T_{output}^{k d}\), enhancing the model's ability to encode deeper genomic information.

For the mask language modeling pretraining stage, the enriched nucleotide level representation \(X\) serves as the query, with the refined tokenized output \(T_{output}\) acting as both the key and value. This setup maps the output resolution to single nucleotides, essential for reconstructing masked tokens. During the finetuning stage, the [CLS] token of \(T_{output}\) is used for classification by convention.

## 4 Experiments

In this section, we first introduce the implementation and pretraining settings of MxDNA. Then, we evaluate MxDNA against other foundation models on Genomic Benchmarks  and Nucleotide Transformer Benchmarks . Next, we present ablation studies on the effect of different tokenization methods and different components of MxDNA. Finally, we conduct a simple analysis on the tokenization behaviors of MxDNA. Experiment settings and results are detailed in Appx. A.4.

### Model Implementation & Pretraining

Our MxDNA is built on the architecture Nucleotide Transformer v2 100M model with 512 hidden units and 22 layers, totaling approximately 100M parameters. Specifically, the model's learnt tokenization module includes 10 convolution experts with kernel sizes ranging from 1 to 10, along with a deformable convolution block with a kernel size of three. We integrate this module by replacing the fifth transformer block, aiming to avoid introducing additional computations.

MxDNA is pretrained on the whole Human Reference Genome  on masked language modeling task  with 15% of the nucleotides randomly masked. An auxiliary balancing loss with a weight of 0.01 is used to prevent degradation towards a single expert, following . The model undergoes training for 500k steps for main performance comparisons and 100k steps for ablations.

### Downstream Evaluation

We primarily follow the evaluation settings of HyenaDNA , performing evaluation on Nucleotide Transformer Benchmarks and Genomic Benchmarks. To ensure fair comparison, we fully finetune all the BERT-like DNA foundation models including Nucleotide Transformer v2 , DNABERT , DNABERT2 , MxDNA under same hyperparameter settings. For HyenaDNA, we utilize the hyperparameters recommended by [7; 23]. All experiments are repeated with three random seeds, and we report the average performance with sample standard deviations. 3

#### 4.2.1 Genomic Benchmarks

First, we begin our evaluation on the Genomic Benchmarks , which consists of eight regulatory element classification tasks. For this benchmark, all BERT-like models are finetuned for 10 epochs with Top-1 accuracy reported for each dataset.

[MISSING_PAGE_FAIL:7]

As shown in Table 2, MxDNA achieves the best performance on 10 out of 18 datasets and ranks in the top-2 on 16 out of 18 datasets. On average, MxDNA shows an improvement of 1.48 points compared to the second-best model, DNABERT2. Notably, MxDNA significantly outperforms all other models in the histone markers tasks while maintaining competitive performance in regulatory annotation and splice site annotation tasks.

### Ablation Studies

Different Tokenization Methods:We compare various tokenization methods by pretraining models for 100k steps using the same backbone but different tokenization methods. The results in Table 3 show that our learnt tokenization significantly outperforms traditional methods such as non-overlapping K-mer, BPE, overlapping K-mer, and single nucleotide tokenization. Among the rule-based methods, single nucleotide tokenization performs best, possibly because it doesn't incorporate human biases and focuses solely on the raw data, though it may make it difficult to capture higher-level semantics. Conversely, non-overlapping K-mer might disrupt meaningful units, BPE might fail to segment DNA sequences meaningfully, and overlapping K-mer could suffer from information leakage.

Different Components: We assess the impact of individual components by incrementally integrating each into the baseline model and pretraining them for 100k steps. Starting with a baseline of single nucleotide tokenization, we sequentially add the Mixture of Convolution Experts, the deformable convolution and jitter noise, resulting in the proposed MxDNA. The results in Table 4 show substantial performance gains from the Mixture of Convolution Experts alone, demonstrating the effectiveness of the idea which allows the model to learn tokenization autonomously rather than depending on predefined tokenization. There are also noticeable performance improvements contributed by the deformable convolution and jitter noise, showing the effectiveness of explicitly equipping the model with capabilities to handle discontinuities, overlaps and ambiguities.

### Analysis

We conduct an analysis of the tokenization behaviors of MxDNA against previous methods on both a sample and dataset level, and present a output embedding analysis at a token level. Notably, MxDNA exhibits unique tokenization strategy distinct from prior methods and is able to inherently capture and differentiate genomic functionalities at a token level during self-supervised pretraining, potentially providing new biological insights. Visualization details are in Appx A.6.

Sample LevelWe first visualize the tokenization of a DNA sequence. For MxDNA, two individual forward passes with identical input and model yield slightly different results during training. It is

   Method & NT Benchmarks & Genomic Benchmarks \\  Single Nucleotide Baseline & 75.07 \(\) 0.26 & 88.56 \(\) 0.02 \\ + Mixture of Convolution Experts & 77.00 \(\) 0.05 & 88.72 \(\) 0.07 \\ + Deformable Convolution & 77.35 \(\) 0.12 & 88.86 \(\) 0.18 \\ + Jitter Noise (MxDNA) & **77.52**\(\) 0.18 & **88.89**\(\) 0.05 \\   

Table 4: Average results on Nucleotide Transformer Benchmarks and Genomic Benchmarks with components added successively. We highlight the best values in **bold** type, underline the second best.

   Method & NT Benchmarks & Genomic Benchmarks \\  Single Nucleotide (1-mer) & 75.07 \(\) 0.26 & 88.56 \(\) 0.02 \\ Overlapping k-mer (6-mer) & 74.35 \(\) 0.35 & 88.55 \(\) 0.07 \\ Non-overlapping k-mer (6-mer) & 67.65 \(\) 0.17 & 86.83 \(\) 0.06 \\ Byte-pair Encoding (4096 tokens) & 74.96 \(\) 0.16 & 87.30 \(\) 0.16 \\ MxDNA (Learnt Tokenization) & **77.52**\(\) 0.18 & **88.89**\(\) 0.05 \\   

Table 3: Average results on Nucleotide Transformer Benchmarks and Genomic Benchmarks with different tokenization methods. We highlight the best values in **bold** type, underline the second best.

worth mentioning that there are usually a small number of differences between the two results, and we display the region where the tokenization outcomes are different to show the ambiguous property for illustrative purposes. For previous rule-based methods, tokenization is static and performed only once. As depicted in Fig. 3, our learnt tokenization method tokenize the DNA sequence in a way distinctly different from previous rule-based method. Moreover, the discontinuous, overlapping and ambiguous tokenization results validate our design choices to manage these properties.

Dataset LevelTo gain more insights, we measure the distribution of token lengths across different downstream datasets for both MxDNA and BPE. For simplicity, we regard the basic units as tokens for MxDNA. BPE and MxDNA shows very distinct distribution of token lengths. As shown in Fig. 4, BPE tends to produce a bell-shaped distribution, inherently biased by its frequency-based merging rule. Conversely, MxDNA's distribution is closer to a uniform distribution with preferences for specific lengths, reflecting its adaptive, task-oriented segmentation capabilities. Moreover, the

Figure 4: Distribution of token lengths for BPE (top) and MxDNA (bottom) across different downstream datasets, illustrating the distinct strategy of MxDNA for handling DNA tokenization. For the sake of simplicity, we regard the basic units as tokens for MxDNA.

Figure 3: Tokenization results of MxDNA over two individual forward passes (left) compared to those of traditional rule-based methods (right). A block of the same colour refers to a single token.

variability in token distribution across datasets might suggest that DNA sequences of different functions might possess distinct patterns and meaningful units.

Token Embedding AnalysisNext, we use t-SNE to visualize the pretrained output tokens in sequences with different genomic functions of different foundation models. As is shown in Fig. 4, without any finetuning, the token embedding distributions of MxDNA are different across sequences with different functions: the tokens of Histone Marker, Promoter and Splice Site form unique clusters. While for all other foundation models, their tokens do not form clear clusters as MxDNA does. This shows MxDNA's superior capability to inherently capture and differentiate genomic functionalities at a token level, suggesting its robustness and specificity in representing biological sequences even before any supervised finetuning is applied.

## 5 Conclusion

SummaryWe present MxDNA, a framework developed to autonomously learn effective DNA tokenization strategies solely through gradient descent. MxDNA demonstrates strong performance against existing sota models and tokenization methods across 26 diverse genomic tasks in Nucleotide Transformer Benchmarks and Genomic Benchmarks with no additional cost. We also perform an analysis of the tokenization mechanism and the token embedding space of MxDNA, showing its distinct tokenization strategy against previous methods and unique capability to capture genomic functionalities at a token level.

Limitations & Future WorksWhile MxDNA demonstrates strong quantitative performance on various downstream tasks, direct biological validation of the model's tokenization decision remains limited. Furthermore, the evaluation on long range tasks is lacking due to quadratic cost of self-attention, although the learnt tokenization is expected to help reduce sequence length effectively and can be combined with sub-quadratic architectures . Future research will focus on refining MxDNA's design to learn a better and more interpretable tokenization strategy, and testing its applicability to broader genomic analyses especially on more long range tasks.

Figure 5: t-SNE visualization of the output embeddings at a token level across different functional sequences of different models, demonstrating MxDNA’s unique capability to inherently capture and differentiate genomic functionalities at a token level.