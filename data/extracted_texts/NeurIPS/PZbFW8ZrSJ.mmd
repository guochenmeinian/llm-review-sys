# TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases

Thibault Simonetto

University of Luxembourg

 Luxembourg

thibault.simonetto@uni.lu

&Salah Ghamizi

LIST / RIKEN AIP

 Luxembourg

 salah.ghamizi@gmail.com

&Maxime Cordy

University of Luxembourg

 Luxembourg

 maxime.cordy@uni.lu

###### Abstract

While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses. We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks. To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model. In addition to our open benchmark https://github.com/serval-uni-lu/tabularbench where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep-learning over 200 models across five critical scenarios in finance, healthcare, and security. We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations. We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs. Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms.

## 1 Introduction

Modern machine learning (ML) models have reached or surpassed human-level performance in numerous tasks, leading to their adoption in critical settings such as finance, security, and healthcare. However, concomitantly to their increasing deployment, researchers have uncovered significant vulnerabilities in generating valid adversarial examples (i.e., constraint-satisfying) where test or deployment data are manipulated to deceive the model. Most analyses of these performance drops have focused on the fields of Computer Vision and Large Language Models where extensive benchmarks for adversarial robustness are available (e.g., Croce et al. (2020) and Wang et al. (2023)).

Despite the widespread use of tabular data and the maturity of Deep Learning (DL) models for this field, the impact of evasion attacks on tabular data has not been thoroughly investigated. Although there are existing benchmarks for _in-distribution_ (ID) tabular classification (Borisov et al., 2021), and distribution shifts (Gardner et al., 2023), there is no available benchmark of adversarial robustness for deep tabular models, in particular in critical real-world settings. We summarize in Table 1 these related benchmarks.

The need for dedicated benchmarks for tabular model robustness is enhanced by the unique challenges that tabular machine learning raises compared to computer vision and NLP tasks.

One significant challenge is that tabular data exhibit _feature constraints_, which are complex relationships and interactions between features. Satisfying these feature constraints can be a non-convex or even nondifferentiable problem, making established evasion attack algorithms relying on gradient descent ineffective in generating valid adversarial examples (i.e., constraint-satisfying) (Ghamizi et al., 2020). Furthermore, attacks designed specifically for tabular data often disregard feature-type constraints (Ballet et al., 2019) or, at best, consider categorical features without accounting for feature relationships (Wang et al., 2020; Xu et al., 2023; Bao et al., 2023), and are evaluated on datasets that contain only such features. This limitation restricts their applicability to domains with heterogeneous feature types.

Moreover, tabular ML models often involve specific feature engineering, that is, "secret" and inaccessible to an attacker. For example, in credit scoring applications, the end user can alter a subset of model features, but the other features result from internal processing that adds domain knowledge before reaching the model (Ghamizi et al., 2020). This raises the need for new threat models that take into account these specificities. We summarize the unique specificities of tabular machine learning and the challenges they pose to an adversarial user in Figure 1.

Thus, the machine learning research community currently lacks not only (1) an empirical understanding of the impact of architecture and robustification mechanisms on tabular data model architectures, but also (2) a reliable and high-quality benchmark to enable such investigations. Such a benchmark for tabular adversarial attacks should feature deployable attacks and defenses that reflect as accurately as possible the robustness of models within a reasonable computational budget. A reliable benchmark should also consider recent advances in tabular deep learning architectures and data augmentation techniques, and tackle realistic attack scenarios and real-world use cases considering their domain constraints and realistic capabilities of an attacker.

To address both gaps, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness using _Constrained Adaptive Attack (CAA)_(Simonetto et al., 2024), a combination of gradient-based and search-based attacks that have recently been shown to be the most effective against tabular models. We take advantage of our new benchmark and uncover unique findings on deep tabular learning architectures and defenses. We focus our study on defenses based on adversarial training (AT), and draw the following insights:

**Test performance is misleading:** Given the same tasks, different architectures have similar ID performance but lead to very disparate robust performances. Even more, data augmentations that improve ID performance can hurt robust performance.

   Benchmark & Domain & Metric & Realistic evaluation \\  Tabsurvey (Borisov et al., 2021) & Tabular & ID performance & No \\ Tableshift (Gardner et al., 2023) & Tabular & OOD performance & No \\ ARES (Dong et al., 2020) & CV & Adversarial performance & No \\ Robustbench (Croce et al., 2020) & CV & Adversarial performance & Yes \\ DecodingTrust (Wang et al., 2023) & LLM & Trust (incl adversarial) & Yes \\ 
**OURS** & Tabular & Adversarial performance & Yes \\   

Table 1: Existing related benchmarks and their differences with ours

Figure 1: The main challenges for adversarial attacks in Tabular Machine Learning: When an adversary perturbs some features (red), it may not be aware of the new features that are computed internally and added (blue), or the relationships between features (green). If the monitoring system detects a constraint violation, the input is quarantined and a rejection (1) is returned.

**Importance of domain constraints:** Disregarding domain constraints overestimates robustness and leads to selection of sub-optimal architectures and defenses when considering the domain constraints.

**Data augmentation effectiveness is task-specific.** There is no data augmentation that is optimal for both ID and robust performance across all tasks. Some simpler augmentations (like Cutmix) can outperform complex generative approaches.

Contributions.To summarize, our work makes the following key contributions:

* **Leaderboard** (https://serval-uni-lu.github.io/tabularbench): a website with a leaderboard based on _more than 200_ evaluations to track the progress and the current state of the art in adversarial robustness of tabular deep learning models for each critical setting. The goal is to clearly identify the most successful ideas in tabular architectures and robust training mechanisms to accelerate progress in the field.
* **Dataset Zoo** : a collection of real and synthetic datasets generated with and without domain-constraint satisfaction, over five critical tabular machine learning use cases.
* **Model Zoo** : a collection of the most robust models that are easy to use for any downstream application. We pre-trained these models in particular on our five downstream tasks and we expect that this collection will promote the creation of more effective adversarial attacks by simplifying the evaluation process across a broad set of _over 200_ models.
* **Analysis**: based on our trained models, we analyze how architectures, AT, and data augmentation mechanisms affect the robust performance of tabular deep learning models and provide insights on the best strategies per use case.

Figure 2: Summary of our main experiments; Y-axis: Robust Accuracy, X-axis ID accuracyBackground

Tabular data are one of the most common forms of data (Shwartz-Ziv and Armon, 2021), especially in critical applications such as medical diagnosis (Ulmer et al., 2020; Somani et al., 2021) and financial applications (Ghamizi et al., 2020; Cartella et al., 2021).

Traditional ML such as random forests and XGBoost often outperform DL on tabular data, primarily due to their robustness in handling feature heterogeneity and interdependence (Borisov et al., 2022).

To bridge the gap, researchers have proposed various improvements, from regularization mechanisms (e.g., RLN (Shavitt and Segal, 2018)) to attention layers (TabNet (Arik and Pfister, 2021)). These innovations are catching up and even outperforming shallow models in some settings, demonstrating the competitiveness of DL for Tabular Data.

The maturity of DL for ID tasks opens new perspectives for studying its performance in advanced settings, such as out-of-distribution (OOD) performance and adversarial robustness. One major work on OOD research is the Tabeshift benchmark (Gardner et al., 2023), an exhaustive evaluation of the OOD performance of a variety of DNN classifiers. There is, however, to the best of our knowledge, no similar work on adversarial robustness, while the use cases when DL models are deployed for tabular data are among the most critical settings, and many are prone to malicious users.

Our work is the first exhaustive benchmark for the critical property of adversarial robustness of DL models. Our work is timely and leverages CAA (Simonetto et al., 2024), a novel attack previously demonstrated as the most effective and efficient tabular attack in the literature in multiple classification tasks under realistic constraints. CAA combines two attacks, CAPGD and MOEVA. CAPGD is an iterative gradient attack that maximizes the error and minimizes the features' constraint violations with regularization losses and projection mechanisms. MOEVA is a genetic algorithm attack that considers the three adversarial objectives: (1) classifier's error maximization, (2) perturbation minimization, and (3) constraint violations minimization, in its fitness function.

Although CAA was only evaluated against vanilla and simple madry AT, we have implemented advanced robustification mechanisms, inspired by proven techniques from top-performing research in the Robustbench computer vision benchmark Robustbench (Croce et al., 2020). Our work is the first implementation and evaluation of state-of-the-art defense mechanisms for tabular DL models.

## 3 TabularBench: Adversarial Robustness Benchmark for Tabular Data

In Appendix A.3 we report the detailed evaluation settings such as metrics, attack parameters, and hardware. We focus below on the datasets, classifiers, and synthetic data generators.

### Tasks

We curated datasets meeting the following criteria: (1) **open source:** the datasets must be publicly available with a clear definition of the features and preprocessing, (2) **from real-world applications:** datasets that do not contain simulated data, (3) **binary classification:** datasets that support a meaningful binary classification task, and (4) **with feature relationships**: datasets that contain feature relationships and constraints, or they can be inferred directly from the definitions of features.

After an extensive review of tabular datasets, only the following five datasets match our requirements.

The **CTU**(Chernikova and Oprea, 2022) includes legitimate and botnet traffic from CTU University. Its challenge lies in the extensive number of linear domain constraints, totaling 360. **LCLD**(George, 2018) is a credit-scoring containing accepted and rejected credit requests. It has \(28\) features and \(9\)_non-linear_ constraints. The most challenging dataset of our benchmark is the **Malware** dataset prepared by Dymrishi et al. (2023). The very large number of features (\(24222\)), most of which are involved in each constraint, make this dataset challenging to attack. **URL**(Hannousse and Yahiouche, 2021) is a dataset comprising both legitimate and phishing URLs. Featuring only 14 linear domain constraints and 63 features, it represents the simplest case in our benchmark. The **WiDS**(Lee et al., 2020) includes medical data on the survival of patients admitted to the ICU, with only 31 linear domain constraints.

Our datasets include varying complexity in terms of number of features and constraints and diverse class imbalance intensity. We summarize the datasets and their relevant properties in Table 2 and provide more details in Appendix A.1.

### Architectures

We consider five state-of-the-art deep tabular architectures from the survey by Borisov et al. (2021): **TabTransformer**Huang et al. (2020) and **TabNet**Arik and Pfister (2021), are based on transformer architectures. **RLN**Shavitt and Segal (2018) uses a regularization coefficient to minimize a counterfactual loss, **STG**Yamada et al. (2020) improves feature selection using stochastic gates, while **VIME**Yoon et al. (2020) depends on self-supervised learning. We provide in Appendix A.2 the details of the architectures and the training hyperparameters. These architectures are on par with XGBoost, the top shallow machine-learning model for our applications.

### Data Augmentation

Our benchmark considers synthetic data augmentation using five state-of-the-art tabular data generators. These generators were pre-trained to learn the distribution of the training data. Then, we augmented each of our datasets 100-fold (for example, for URL dataset, we generated \(1.143.000\) synthetic examples). Appendix A.4 details the generator architectures and the training hyperparameters.

**WGAN**Arjovsky et al. (2017) is a typical generator-discriminator GAN model using Wasserstein loss. We follow the implementation of Stoian et al. (2024) and apply a MinMax transformation for continuous features and one-hot encoding for categorical to adapt this architecture for tabular data.

**TableGAN**Park et al. (2018) is an improvement over standard GAN generators for tabular data. It adds a classifier (trained to learn the labels and feature relationships) to the generator-discriminator setup to improve semantic accuracy. TableGAN uses MinMax transformation for features.

**CTGAN**Xu et al. (2019) uses a conditional generator and training-by-sampling strategy in a generator-discriminator GAN architecture to model tabular data.

**TVAE**Xu et al. (2019) is an adaptation of the Variational AutoEncoder architecture for tabular data. It uses the same data transformations as CTGAN and training with ELBO loss.

**GOGBLE**Liu et al. (2023) is a graph-based model that learns relational and functional dependencies in data using graphs and a message passing DNN, generating variables based on their neighborhood.

**Cutmix**Yun et al. (2019) In computer vision, patches are cut and pasted among training images where the labels are also mixed proportionally. We adapted the approach to tabular ML and for each pair of rows of the same class, we randomly mix half of the features to generate a new sample.

For training, each batch of real examples is augmented with a same-size random synthetic batch (without replacement). However, the evaluation only runs on real examples. In AT, we generate adversarials from half of the real examples randomly selected and half of the synthetic examples.

### Attack

To build our robustness benchmark, we leverage the Constrained Adaptive Attack (CAA) Simonetto et al. (2024) as the attack algorithm. To the best of our knowledge, CAA is the most effective and efficient tabular attack in the literature in multiple classification tasks under realistic constraints that appear in real-world applications. These constraints can be of four types: (1) mutability (whether a feature can be modified), (2) range (the minimum and maximum values a feature can take), (3) types

   Dataset & Domain & Output to flip & Total size & \# Features & \# Ctrs & Inbalance \\  CTU & Botnet detection & Malicious connections & 198 128 & 756 & 360 & 99.3/0.7 \\ LCLD & Credit scoring & Reject loan request & 1 220 092 & 28 & 9 & 80/20 \\ Malware & Malware detection & Malicious software & 17 584 & 24 222 & 7 & 45.5/54.5 \\ URL & Phishing & Malicious URL & 11 430 & 63 & 14 & 50/50 \\ WIDS & ICU survival & Expected survival & 91 713 & 186 & 31 & 91.4/8.6 \\   

Table 2: Properties of the use cases of our benchmark.

(the type of the feature, e.g., categorical or numerical), and (4) relations (the dependencies between features, e.g., the sum of two features must be equal to a third feature).

CAA is a novel attack that combines two attacks, Constrained Adaptive Projected Gradient Descent (CAPGD) and Multi-Objective Evolutionary Adversarial (MOEVA) attack.

We denote by \(x^{d}\) an input example and by \(y\{1,,C\}\) its correct label. Let \(h:^{d}^{C}\) be a classifier and \(h_{c_{k}}(x)\) the classification score that \(h\) outputs for input \(x\) to be in class \(c_{k}\).

CAPGD (Simonetto et al., 2024) is an iterative attack that generates adversarial examples by computing the following perturbed example at each iteration:

\[z^{(k+1)} =P_{}x^{(k)}+^{(k)}^{}(x^{(k)})\] (1) \[x^{(k+1)} =R_{}P_{}x^{(k)}+(z^{( k+1)}-x^{(k)})+(1-)(x^{(k)}-x^{(k-1)})\]

where \(P_{}\) is the projection operator onto the set of maximum perturbation \(\) denoted \(\), \(R_{}\) is a repair operator for a subset of constraints \(\), \(^{(k)}\) is the step size, \(\) is the momentum parameter, and \(^{}\) abbreviates the objective function to be maximized defined as:

\[^{}(x)=(x,y,h,)=l(h(x),y)-_{_{i} }penalty(x,_{i}).\] (2)

where \(l\) is the loss function of the model, and \(penalty\) is the penalty function for each relation constraint \(_{i}\).

MOEVA (Simonetto et al., 2022) multi-objective evolutionary algorithm based on NSGA-III that generates adversarial examples by minimizing the following objectives:

\[minimise\;g_{1}(x)  h(x)\] (3) \[minimise\;g_{2}(x)  L_{p}(x-x^{0})\] (4) \[minimise\;g_{3}(x) _{_{i}}penalty(x,_{i})\] (5)

where \(x^{0}\) is the original input, \(L_{p}\) is the \(L_{p}\) norm, in our case \(L_{2}\).

CAA only applies MOEVA when CAPGD fails to find an adversarial example. The attack is successful if it finds an adversarial example that is misclassified by the classifier and satisfies all the constraints.

Although to the extent of our knowledge, CAA is the best attack, we acknowledge that better attacks may be developed in the future. We provide the code for CAA in our repository, and we encourage the community to develop new attacks and evaluate them on our benchmark. Additionally, CAA is extendable in its design. Inspired by Auto-Attack, CAA is the sequential application of multiple strong attacks and complementary attacks, from fastest to slowest to find the best adversarial example. The attacks are complementary in the sense that they generate adversarial examples from different examples in the input space. This design allows for the easy integration of new attacks into the CAA framework. We encourage the development of new effective attacks and the evaluation of their complementarity with CAA.

### TabularBench API

To encourage the wide adoption of TabularBench as the go-to place for Tabular Machine Learning evaluation, we designed its API to be modular, extensible, and standardized. We split its architecture into three independent components. More details of each component are provided in Appendix C.

**A dataset Zoo** For each dataset in this study, we have collected, cleaned, and pre-processed the existing raw dataset. We implemented a novel _Constraint Parser_ where the user can write the relations in a natural human-readable format to describe the relationships between features. The processed datasets are loaded with a _Dataset factory_, then the user gets their associated meta-data and pre-defined constraints. The datasets are automatically downloaded when not found.

ds = dataset_factory.get_dataset("lcld_v2_iid") metadata = ds.get_metadata(only_x=True) constraints = ds.get_constraints() ```

**A model Zoo** Our API supports five architectures, and for each, six data augmentation techniques (as well as no data augmentation) and two training schemes (standard training and adversarial training). Hence, 70 pre-trained models for each of our five datasets are accessible. Below, we fine-tune with CAA AT and CTGAN augmentation a pre-trained Tabtramsofter with Cutmix augmentation:

``` scaler=TabScaler(num_scaler="min_max",one_hot_encode=True) scaler.fit(x,metadata["type"]) model=TabTransformer("regression",metadata,scaler-scaler,pretrained="lCLD_TabTr_Cutmix") train_dataloader=CTGANDataLoader(dataset=ds,split="train",scaler=scaler,attack="caa") model.fit(train_dataloader) ```

**A standardized benchmark** To generate our leaderboard, we offer a one-line command that loads a pre-trained model from the zoo, and reports the clean and robust accuracy of the model following our benchmark's setting (taking into consideration constraint satisfaction and L2 minimization):

``` clean_acc,robust_acc=benchmark(dataset="LCLD',model="TabTr_Cutmix",distance="L2',constraints=True) ```

## 4 Empirical Findings

In the main paper, we provide multiple figures to visualize the main insights. We only report scenarios where data augmentation and adversarial training do not lead to performance collapse. We report in Appendix B all the results and investigate the collapsed scenarios.

### Without Data Augmentations

We report the ID and robust accuracies of our architectures prior to data increase in Table 3.

**All models on malware dataset are robust without data augmentation.** AT improves adversarial accuracy for all the cases, but AT alone is not sufficient to completely robustify the models on URL and WIDS datasets. All malware classification models are completely robust with and without adversarial training; hence, we will restrict the study of improved defenses with augmentation in the following sections to the remaining datasets.

### Impact of Data Augmentations

**With data augmentation alone, ID and robust performances are not aligned.** In Figure 2 we study the impact of data augmentation on ID and robust performance, both in standard and adversarial

   Dataset & Accuracy & TabTr. & RLN & VIME & STG & TabNet \\   & ID & \(95.3/95.3\) & \(97.8/97.3\) & \(95.1/95.1\) & \(95.3/95.1\) & \(96.0/0.2\) \\  & Robust & \(95.3/95.3\) & \(94.1/97.1\) & \(40.8/94.0\) & \(95.3/95.1\) & \(0.0/0.2\) \\   & ID & \(69.5/73.9\) & \(68.3/69.5\) & \(67.0/65.5\) & \(66.4/15.6\) & \(67.4/0.0\) \\  & Robust & \(7.9/70.3\) & \(0.0/63.0\) & \(2.4/10.4\) & \(53.6/12.1\) & \(0.4/0.0\) \\   & ID & \(95.0/95.0\) & \(95.0/96.0\) & \(95.0/92.0\) & \(93.0/93.0\) & \(99.0/99.0\) \\  & Robust & \(94.0/95.0\) & \(94.0/96.0\) & \(95.0/92.0\) & \(93.0/93.0\) & \(97.0/99.0\) \\   & ID & \(93.6/93.9\) & \(94.4/95.2\) & \(92.5/93.4\) & \(93.3/94.3\) & \(93.4/99.5\) \\  & Robust & \(8.9/56.7\) & \(10.8/56.2\) & \(49.5/69.8\) & \(58.0/90.0\) & \(11.0/91.8\) \\   & ID & \(75.5/77.3\) & \(77.5/78.0\) & \(72.3/72.1\) & \(77.7/62.6\) & \(79.8/98.4\) \\  & Robust & \(45.9/65.1\) & \(60.9/66.6\) & \(50.3/52.1\) & \(50.3/45.2\) & \(5.3/58.4\) \\   

Table 3: Clean and robust performances across all architectures in the form XX/YY. XX is the accuracy with standard training, and YY is the accuracy with adversarial training.

training. With standard training, ID performance is misleading in CTU and URL datasets. Although all models exhibit similar ID performance, some of the augmentations lead to robust models, while others decrease it. CTGAN data augmentation is the best data augmentation for ID performance in all use cases, both with standard and adversarial training.

### Impact of Adversarial Training

**With data augmentation and AT, ID and robust performances are correlated.** Although there is no trend of relationship between ID performance and robust performance in standard training, our study shows that robustness and ID performance are correlated after adversarial training. For example, the Pearson correlation between ID and robust performance increases from \(0.15\) to \(0.76\) for LCLD. All correlation values are in Appendix B.4.

Overall, all architectures can benefit from at least one data augmentation technique with adversarial training; however, standard training with data augmentation can outperform adversarial training without data augmentation (for e.g., on URL dataset using GOGGLE or CTGAN augmentations).

### Impact of Architecture

In Figure 3 we study the robustness of each architecture with different defense mechanisms. We report both the robustness against unconstrained attacks (attacks unaware of domain knowledge) and attacks optimized to preserve the feature relationships and constraints.

Figure 3: Robust performance while considering domain constraints (ADV+CTR: Y-axis) and without (ADV: X-axis) on all our use cases confirms the relevance of studying constrained-aware attacks.

**Evaluation with unconstrained attacks is misleading.** Under standard training (orange scatters in Fig. 3), there is no relation between robustness to unconstrained attacks and the robustness when domain constraints are enforced. There is, however, a linear relationship under adversarial training with data augmentation only for STG, Tabistransformer, and VIME architectures. These results show that nonconstrained attacks are not sufficient to reliably assess the robustness of deep tabular models. Detailed correlation values are in the Appendix B.4.

**No data augmentation consistently outperforms the baselines with AT.** Among the 20 scenarios in Fig. 3, the original models achieve better constrained robustness than augmented models with adversarial training only for 4 scenarios: TabNet architecture on URL, LCLD and WIDS, and STG architecture on URL datasets. No data-augmentation technique consistently outperforms the others across all architectures. Cutmix, the simplest data augmentation, is often the best (in 7/20 scenarios).

### Impact of Attack Budgets

We evaluated each robustified model against variants of the CAA attack, varying the \(L_{2}\) distance of the perturbation \(\) from \(0.5\) to \(\{0.25,1,5\}\), the gradient iterations from \(10\) to \(\{5,20,100\}\), and the search iterations from \(100\) to \(\{50,200,1000\}\). We report per architecture for each dataset the most robust model with AT and augmentation, and the robust model with AT only. We present in Fig. 4 the results for the URL dataset and refer to Appendix B.5 for the other use cases.

**AT+Augmentations models remain robust even under stronger attacks.** Our results show that the best defenses with AT+Augmentations (continuous lines) remain robust against increased gradient and search iteration budgets and remain more robust than AT alone (dashed lines) for VIME, RLN, and Tabtransformer architectures. Against an increase in perturbation size \(\), AT+Augmentations is more robust than AT alone for TabNet, TabTransformer, VIME, and RLN architectures. In particular, for \(=5\), the robust accuracy of TabNet architectures remains above 40% with AT+Augmentations while the robust accuracy with AT alone drops to 0%.

## 5 Limitations

While our benchmark is the first to tackle adversarial robustness in tabular deep learning models, it does not cover all the directions of the field and focuses on domain constraints and defense mechanisms. Some of the orthogonal work is not addressed:

**Generalization to other distances:** We restricted our study to the \(L_{2}\) distance to measure imperceptibility. Imperceptibility varies by domain, and several methods have been proposed to measure it (Ballet et al., 2019; Kireev et al., 2022; Dymishi et al., 2023). These methods have not been evaluated against human judgment or compared with one another, so there is no clear motivation to use one or another. In our research, we chose to use the well-established \(L_{2}\) norm (following Dymishi et al. (2023)). Our algorithms and benchmarks support other distances and definitions of imperceptibility. We provide in Appendix B.6 an introduction to how our benchmark generalizes to other distances.

**Generalization to non-binary classification:** We restricted our study to binary tabular classification as it is the only case where we identified public datasets with domain constraints. The attacks used in our benchmark natively support multi-class classification. Our live leaderboard welcomes new datasets and will be updated if relevant datasets are designed by the community.

Figure 4: Impact of attack budget on the robust accuracy for URL dataset.

**Generalization to other types of defenses:** We only considered defenses based on data augmentation with adversarial training. Adversarial training-based defenses are recognized as the only reliable defenses against evasion attack (Tramer et al., 2020; Carlini, 2023). All other defenses are proven ineffective when the attacker is aware of them and performs adaptive attacks.

**Generalization to other (adaptive) attacks:** We only considered the Constrained Adaptive Attack (CAA) as it is the most effective and efficient attack in the literature. We encourage the community to develop new attacks and evaluate them on our benchmark. We provide the code for CAA in our repository and encourage the community to develop new attacks and evaluate them on our benchmark. In the face of new attacks, we will update our benchmark to include them. A limitation of our current evaluation regarding attacks is the lack of adaptive attacks, that adapt their strategy based on the defense mechanism. We welcome the development of new adaptive attacks and their evaluation on our benchmark at https://github.com/serval-uni-lu/tabularbench/issues/new/choose.

## 6 Broader Impact

Our work proposes the first benchmark of robustness of constrained tabular deep learning against evasion attacks. We focus on designing new defense mechanisms, inspired by effective approaches in computer vision (by combining data augmentation and adversarial training). Hence, we expect that our research will significantly contribute to the enhancement of defenses and will lead to even more resilient models, which may balance the potential harms research on adversarial attacks can have.

## Conclusion

In this work, we introduce TabularBench, the first benchmark of adversarial robustness of tabular deep learning models against constrained evasion attacks. We leverage Constrained Adaptive Attack (CAA), the best constrained tabular attack, to benchmark state-of-the-art architectures and defenses.

We provide a Python API to access the datasets, along with implementations of multiple tabular deep learning architectures, and provide all our pre-trained robust models directly through the API.

We conducted an empirical study that constitutes the first large-scale study of tabular data model robustness against evasion attacks. Our study covers five real-world use cases, five architectures, and six data augmentation mechanisms totaling more than 200 models. Our study identifies the best augmentation mechanisms for IID performance (CTGAN) and robust performance (Cutmix), and provides actionable insights on the selection of architectures and robustification mechanisms.

We are confident that our benchmark will accelerate the research of adversarial defenses for tabular ML and welcome all contributions to improve and extend our benchmark with new realistic use cases (multiclass), models, and defenses.