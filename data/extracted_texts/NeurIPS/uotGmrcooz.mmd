# Optimal approximation

using complex-valued neural networks

 Paul Geuchen

MIDS,

KU Eichstatt-Ingolstadt,

Auf der Schanz 49,

85049 Ingolstadt, Germany

paul.geuchen@ku.de &Felix Voigtlaender

MIDS,

KU Eichstatt-Ingolstadt,

Auf der Schanz 49,

85049 Ingolstadt, Germany

felix.voigtlaender@ku.de

###### Abstract

Complex-valued neural networks (CVNNs) have recently shown promising empirical success, for instance for increasing the stability of recurrent neural networks and for improving the performance in tasks with complex-valued inputs, such as in MRI fingerprinting. While the overwhelming success of Deep Learning in the real-valued case is supported by a growing mathematical foundation, such a foundation is still largely lacking in the complex-valued case. We thus analyze the expressivity of CVNNs by studying their approximation properties. Our results yield the first quantitative approximation bounds for CVNNs that apply to a wide class of activation functions including the popular modReLU and complex cardioid activation functions. Precisely, our results apply to any activation function that is smooth but not polyharmonic on some non-empty open set; this is the natural generalization of the class of smooth and non-polynomial activation functions to the complex setting. Our main result shows that the error for the approximation of \(C^{k}\)-functions scales as \(m^{-k/(2n)}\) for \(m\) where \(m\) is the number of neurons, \(k\) the smoothness of the target function and \(n\) is the (complex) input dimension. Under a natural continuity assumption, we show that this rate is optimal; we further discuss the optimality when dropping this assumption. Moreover, we prove that the problem of approximating \(C^{k}\)-functions using continuous approximation methods unavoidably suffers from the curse of dimensionality.

## 1 Introduction

Deep Learning currently predominantly relies on real-valued neural networks, which have led to breakthroughs in fields like image classification or speech recognition . However, recent work has uncovered several application areas in which the use of complex-valued neural networks (CVNNs) leads to better results than the use of their real-valued counterparts. These application areas mainly include tasks where complex numbers inherently occur as inputs of a machine learning model such as Magnetic Resonance Imaging (MRI)  and Polarimetric Synthetic Aperture Radar (PolSAR) Imaging . Moreover, CVNNs have been used to improve the stability of recurrent neural networks  and have been successfully applied in various other fields . The mathematical theory of these complex-valued neural networks, however, is still in its infancy. There is therefore a great interest in studying CVNNs and in particular in uncovering the differences and commonalities between CVNNs and their real-valued counterparts. A prominent example highlighting the unexpected differences between both network classes is the _universal approximation theorem_ for neural networks, whose most general real-valued version was proven in 1993  (with a more restricted version appearing earlier ) and which was recently generalized to the case of CVNNs . The two theorems describe necessary and sufficient conditions onan activation function which guarantee that arbitrarily wide neural networks of a fixed depth can approximate any continuous function on any compact set arbitrarily well (with respect to the uniform norm). Already here it was shown that complex-valued networks behave significantly different from real-valued networks: While real-valued networks are universal if and only if the activation function is non-polynomial, complex-valued networks with a single hidden layer are universal if and only if the activation function is non-polyharmonic (see below for a definition). Furthermore, there exist continuous activation functions for which deep CVNNs are universal but shallow CVNNs are not, whereas the same cannot happen for real-valued neural networks. This example shows that it is highly relevant to study the properties of CVNNs and to examine which of the fundamental properties of real-valued networks extend to complex-valued networks.

Essentially the only existing _quantitative_ result regarding the approximation-theoretical properties of CVNNs is , which provides results for approximating \(C^{k}\)-functions by _deep_ CVNNs using the modReLU activation function. However, for real-valued NNs it is known that already _shallow_ NNs can approximate \(C^{k}\)-functions at the optimal rate. Precisely, Mhaskar showed in  that one can approximate \(C^{k}\)-functions on \([-1,1]^{n}\) with an error of order \(m^{-k/n}\) as \(m\), where \(m\) is the number of neurons in the hidden layer. Here he assumed that the activation function is smooth on an open interval and that at some point of this interval no derivative vanishes. This is equivalent to the activation function being smooth and non-polynomial on that interval, cf. [20, p. 53].

The present paper shows that a comparable result holds in the setting of complex-valued networks, by proving that one can approximate every function in \(C^{k}\) (\(_{n};\)) (where differentiability is understood in the sense of real variables) with an error of the order \(m^{-k/(2n)}\) (as \(m\)) using shallow complex-valued neural networks with \(m\) neurons in the hidden layer. Here we define the cube \(_{n}:=[-1,1]^{n}+i[-1,1]^{n}\). The result holds whenever the activation function \(:\) is smooth and non-polyharmonic on some non-empty open set. This is a very natural condition, since for polyharmonic activation functions there exist \(C^{k}\)-functions that cannot be approximated at all below some error threshold using shallow neural networks with this activation function .

Furthermore, the present paper studies in how far the approximation order of \(m^{-k/(2n)}\) is _optimal_, meaning that an order of \(m^{-(k/2n)-}\) (where \(>0\)) cannot be achieved. Here it turns out that the derived order of approximation is indeed optimal (even in the class of CVNNs with possibly more than one hidden layer) in the setting that the weight selection is _continuous_, meaning that the map that assigns to a function \(f C^{k}(_{n};)\) the weights of the approximating network is continuous with respect to some norm on \(C^{k}(_{n};)\). This continuity assumption is natural since typical learning algorithms such as (stochastic) gradient descent use samples \(f(x_{j})\) of the target function and then apply continuous operations to them to update the network weights.

We investigate this optimality result further by dropping the continuity assumption and constructing two special smooth and non-polyharmonic activation functions with the first one having the property that the order of approximation can indeed be strictly improved via a _discontinuous_ selection of

  & Condition on & Continuity of & Approximation \\  & activation function & weight selection & Error \\  Theorem 3.2 & smooth \& & \\  & non-polyharmonic & possible & \((m^{-k/(2n)})\) \\   Consequence of \\ Theorem 4.1 \\  & continuous & assumed & \((m^{-k/(2n)})\) \\  Theorem 4.2 & 
 very special \\ activation function \\  & not assumed & \((m^{-k/(2n-1)})\) \\  Theorem 4.3 & \((z)}}\) & not assumed & \((m^{-k/(2n)})\) \\ 

Table 1: Overview of the proven approximation bounds. \(k\) is the regularity of the approximated functions (which are assumed to be \(C^{k}\)), \(n\) the (complex) input dimension and \(m\) the number of neurons in the hidden layer of the network. The notation \(\) is similar to \(\), but ignoring log factors.

the related weights. For the second activation function we show that the order of \(m^{-k/(2n)}\)_cannot_ be improved, even if one allows a discontinuous weight selection. This in particular shows that in the given generality of arbitrary smooth, non-polyharmonic activation functions, the upper bound \((m^{-k/(2n)})\) cannot be improved, even for a possibly discontinuous choice of the weights. An overview of the approximation bounds proven in this paper can be found in Table 1.

Moreover, we analyze the _tractability_ (in terms of the input dimension \(n\)) of the considered problem of approximating \(C^{k}\)-functions using neural networks. Theorem 5.1 shows that one necessarily needs a number of parameters _exponential_ in \(n\) to obtain a non-trivial approximation error. To the best of our knowledge, Theorem 5.1 is the first result showing that the problem of approximating \(C^{k}\)-functions using continuous approximation methods is intractable (in terms of the input dimension \(n\)).

### Related Work

**Real-valued neural networks.** By now, the approximation properties of real-valued neural networks are quite well-studied (cf. [10; 28; 33; 40; 41; 46; 47] and the references therein). We here only discuss a few papers that are most closely related to the present work.

In , Mhaskar analyzes the rate of approximation of shallow real-valued neural networks for target functions of regularity \(C^{k}\). Our results can be seen as the generalization of  to the complex setting. Our proofs rely on several techniques from ; however, significant modifications are required to make the proofs work for general smooth non-polyharmonic functions.

One of the first papers to observe that neural networks with general (smooth) activation function can be surprisingly expressive is  where it was shown that a neural network of _constant size_ can be universal. One of the activation functions in Section 4 is based on a similar idea.

The importance of distinguishing between continuous and discontinuous weight selection (which in our setting is discussed in Section 4) was observed for ReLU-networks in .

The paper  shows that neural network approximation is _not_ continuous in the following sense: The best approximating neural network \((f)\) of a given size does not depend continuously on \(f C^{k}\). This result, however, is not in conflict with our results: We want to assign to any \(^{k}\)-function \(f\) a network \((f)\) that approximates \(f\) below the error threshold \(m^{-k/(2n)}\). The network \((f)\), however, does _not_ have to coincide with the best approximating network \((f)\).

**Complex-valued neural networks.** When it comes to general literature about mathematical properties of complex-valued neural networks, surprisingly little work can be found. The _Universal Approximation Theorem for Complex-Valued Neural Networks_ has already been mentioned above. In particular, it has been shown that shallow CVNNs are universal if and only if the activation function \(\) is not polyharmonic. Thus, the condition assumed in the present paper (that \(\) should be smooth, but not polyharmonic) is quite natural.

Regarding _quantitative_ approximation results for CVNNs, the only existing work of which we are aware is , analyzing the approximation capabilities of _deep_ CVNNs where the modReLU is used as activation function. Since the modReLU satisfies our condition regarding the activation function, the present work can be seen as an improvement to . Precisely, (i) we consider general activation functions, including but not limited to the modReLU, (ii) we improve the order of approximation by a log factor, and (iii) we show that this order of approximation can be achieved using shallow networks instead of the deep networks used in . We stress that our proof techniques differ significantly from the ones applied in : The arguments in  take their main ideas from  making heavy use of the specific definition of the modReLU. In contrast, since we consider quite general activation functions, we necessarily follow a much more general approach following the ideas from .

## 2 Preliminaries

**Shallow complex-valued neural networks.** In this paper we mainly consider so-called shallow complex-valued neural networks, meaning complex-valued neural networks with a single hidden layer. Precisely, we consider functions of the form

\[^{n} z_{j=1}^{m}_{j}(_{j}^{T} z +_{j}),\]with \(_{1},...,_{m}^{n}\), \(_{1},...,_{m},_{1},...,_{m}\) and an _activation function_\(:\). Here, \(m\) denotes the number of neurons of the network and we write \(v^{T}\) for the transpose of a vector \(v\).

To simplify the formulation of the results, we introduce the following notation: We write \(^{}_{n,m}\) for the set of _first layers_ of shallow complex-valued neural networks with activation function \(\), with \(n\) input neurons and \(m\) hidden neurons, meaning

\[^{}_{n,m}:=\{z((_{j}^{T} z+ _{j}))_{j=1}^{m}:\;_{j}^{n},\;_{j} \}\{F:^{n}^{m} \}.\]

Hence, each shallow CVNN can be written as \(^{T}\) with \(^{m}\) and \(^{}_{n,m}\); see Figure 1 for a graphical representation of a shallow CVNN.

**Approximation.** The paper aims to analyze the approximation of \(C^{k}\)-functions on the complex cube

\[_{n}:=[-1,1]^{n}+i[-1,1]^{n}\]

using shallow CVNNs. Here, we say that a function \(f:_{n}\) is in \(C^{k}(_{n};)\) if and only if \(f\) is \(k\) times continuously differentiable on \(_{n}\), where the derivative is to be understood in the sense of real variables, i.e., in the sense of interpreting \(f\) as a function \([-1,1]^{2n}^{2}\) and taking usual real derivatives. We further define the \(C^{k}\)_-norm_ of a function \(f C^{k}(_{n};)\) as

\[\|f\|_{C^{k}(_{n};)}:=_{ _{0}^{2}\\ || k}\|^{}f\|_{L^{}( _{n};)},\|g\|_{L^{}(_{n}; )}:=_{z_{n}}|g(z)|\] (2.1)

for any function \(g:_{n}\). Note that we write \(=\{1,2,3,...\}\) and \(_{0}=\{0\}\). Using the previously introduced notation, we thus seek to bound the worst-case approximation error, i.e.,

\[_{f C^{k}(_{n};)\\ \|f\|_{C^{k}(_{n};)} 1}_{ ^{}_{n,m}\\ ^{m}}\|f-^{T}\|_{L^{}( _{n};)}.\]

**Wirtinger calculus and polyharmonic functions.** For a function \(f:\) which is differentiable in the sense of real variables at a point \(z_{0}\) we define its _Wirtinger derivatives_ at \(z_{0}\) as

\[_{}f(z_{0}):=((z_{0})-i(z_{0})) _{}f(z_{0}):=((z_{0})+i(z_{0}) ).\]

Here, \(\) and \(\) denote the usual partial derivatives in the sense of real variables. We extend this definition to multivariate functions defined on open subsets of \(^{n}\) by considering _coordinatewise_ Wirtinger derivatives.

A function \(f:U\), where \(U\) is an open set, is called _smooth_ if it is differentiable arbitrarily many times (in the sense of real variables). We write \(f C^{}(U;)\) in that case. Moreover, \(f\) is called _polyharmonic_ (on \(U\)) if it is smooth and if there exists \(m_{0}\) satisfying

\[^{m}f 0U.\]

Figure 1: Graphical representation of a shallow neural network. Input and output neurons are depicted as dots, hidden neurons are depicted as circles. The term _first layer_ describes the transformation from the input to the hidden neurons, including the application of the activation function.

Here, \(:=}{ x^{2}}+}{ y^{2}}=4 _{}_{}\) denotes the usual Laplace-Operator.

The following Proposition 2.1 describes a property of non-polyharmonic functions which is crucial for proving the approximation results of this paper.

**Proposition 2.1**.: _Let \( U\) be an open set and let \( C^{}(U;)\) be non-polyharmonic. Then for every \(M_{0}\) there exists a point \(z_{M} U\) satisfying_

\[_{}^{m}_{}^{}(z_ {M}) 0m,_{0}m, M.\]

The proof of Proposition 2.1 is an application of the Baire category theorem; see Appendix B.2.

**Important complex activation functions.** We briefly discuss in how far two commonly used complex activation functions satisfy our assumptions: The _modReLU_ proposed in  and the _complex cardioid_ used in  for MRI fingerprinting where the performance could be significantly improved using complex-valued neural networks. The modReLU is defined as

\[_{,b}:,_{ ,}(z):=(|z|+b),&|z|+b 0,\\ 0,&\]

where \(b<0\) is a fixed parameter. The complex cardioid is defined as

\[:,(z):=( 1+( z))z.\]

Here, \( z=\) denotes the polar angle of a complex number \(z=re^{i}\), where we define \( 0:=0\); see Figure 2 for plots of the absolute value of the two functions.

Both functions are smooth and non-polyharmonic on a non-empty open subset of \(\), which is proven in Appendix A.2. Furthermore, they are both continuous on \(\). Therefore, our approximation bounds established in Theorems 3.1 and 3.2 in particular apply to those two functions.

## 3 Main results

In this section we state the main results of this paper and provide proof sketches for them. Detailed proofs of the two statements can be found in Appendices B.3 and C.3.

First we show in Theorem 3.1 that it is possible to approximate any complex polynomial in \(z\) and \(\) arbitrarily well using shallow CVNNs with the size of the networks only depending on the degree of the polynomial (not on the desired approximation accuracy). Using this result we can prove the main approximation bound, Theorem 3.2, by first approximating a given \(C^{k}\)-function using a polynomial in \(z\) and \(\) and then approximating this polynomial using Theorem 3.1.

For \(m,n\) let

\[_{m}^{n}:=\{^{n},\;z_{  m}\;_{ m}a_{,}z^{}^{}:\;a_{, }\}\]

Figure 2: Absolute value of the activation functions \(_{,-1}\) (left) and \(\) (right).

denote the space of all complex polynomials on \(^{n}\) in \(z\) and \(\) of componentwise degree at most \(m\). Here, we are summing over all multi-indices \(,_{0}^{n}\) with \(_{j},_{j} m\) for every \(j\{1,...,n\}\) and use the notation

\[z^{}:=_{j=1}^{n}z_{j}^{_{j}} ^{}:=_{j=1}^{n}}^{ _{j}}.\]

The space \(_{m}^{n}\) is finite-dimensional; hence, it makes sense to talk about bounded subsets of \(_{m}^{n}\) without specifying a norm.

**Theorem 3.1**.: _Let \(m,n\), \(>0\) and \(:\) be smooth and non-polyharmonic on an open set \( U\). Let \(^{}_{m}^{n}\) be bounded and set \(N:=(4m+1)^{2n}\). Then there exists a first layer \(_{n,N}^{}\) with the following property: For each polynomial \(p^{}\) there exists \(^{N}\), such that_

\[\|p-^{T}\|_{L^{}(_{n};)}.\]

Sketch of proof.: For any multi-indices \(,_{0}^{n}\) an inductive argument shows for every fixed \(z_{n}\) and \(b\) that

\[_{}^{}}_{ }^{}}[w(w^{T}z+b)] =z^{}^{}(_{ }^{||}_{}^{| }|})(w^{T}z+b).\]

Here, \(_{}^{}\) and \(_{}^{}}\) denote the multivariate Wirtinger derivatives with respect to \(w\) according to the multi-indices \(\) and \(\), respectively. Evaluating this at \(w=0\) and taking \(b\) such that none of the mixed Wirtinger derivatives of \(\) at \(b\) up to a sufficiently high order vanish (where such a \(b\) exists by Proposition 2.1) shows that we can rewrite

\[z^{}^{}=(_{} ^{}}_{}^{}}[w(w^{T}z+b)])_{w=0} ((_{}^{||}_{ }^{|}|})(b))^{-1}.\] (3.1)

The mixed Wirtinger derivatives can by definition be expressed as linear combinations of usual partial derivatives. Those partial derivatives can be approximated using a generalized version of _divided differences_: If \(g C^{k}((-r,r)^{s};)\) and \(_{0}^{s}\) with \(|| k\) we have

\[^{}g(0)(2h)^{-||}_{0 r }(-1)^{||-||}}{}g (h(2-)).\] (3.2)

See Appendix B.1 for a proof of this approximation. Note that when one takes \(g(w)=(w^{T}z+b)\), the right-hand side of (3.2) is a shallow neural network, as a function of \(z\).

Combining (3.1) and (3.2) yields the desired result; see Appendix B.3 for the details. 

It is crucial that the size of the networks considered in Theorem 3.1 is independent of the approximation accuracy \(\). Moreover, the first layer \(\) can be chosen independently of the particular polynomial \(p\). Only the weights \(\) connecting hidden layer and output neuron have to be adapted to \(p\).

The final approximation result is as follows. Its full proof can be found in Appendix C.3.

**Theorem 3.2**.: _Let \(n,k\). Then there exists a constant \(c=c(n,k)>0\) with the following property: For any activation function \(:\) that is smooth and non-polyharmonic on an open set \( U\) and for any \(m\) there exists a first layer \(_{n,m}^{}\) with the following property: For any \(f C^{k}(_{n};)\) there exist coefficients \(=(f)^{m}\), such that_

\[\|f-^{T}\|_{L^{}(_{n};)} c m ^{-k/(2n)}\|f\|_{C^{k}(_{n};)}.\]

_Furthermore, the map \(f(f)\) is a continuous linear operator with respect to the \(L^{}\)-norm._

Sketch of proof.: By splitting \(f\) into real and imaginary part we may assume that \(f\) is real-valued. Fourier-analytical results (recalled in Appendix C.1) imply that each \(C^{k}\)-function \(f\) can be well approximated by a linear combination of (multivariate) _Chebyshev polynomials_. Precisely, we have

\[\|f-P\|_{L^{}(_{n};)}} \|f\|_{C^{k}(_{n};)},\]where \(P\) is given via the formula

\[P(z)=_{0 2m-1}_{}^{m}(f) T_{ }(z), z_{n}.\]

Here, the functions \(T_{}\) are multivariate versions of Chebyshev polynomials and \(_{}^{m}(f)\) are continuous linear functionals in \(f\). The constant \(c>0\) only depends on \(n\) and \(k\). See Appendix C.1 for a rigorous proof of this approximation property. Approximating the polynomials \(T_{}\) by neural networks according to Theorem 3.1 yields the desired claim; see Appendix C.3 for the details. 

_Remark 3.3_.: Theorem 3.1 can not only be used to derive approximation rates for the approximation of \(C^{k}\)-functions but can be applied to any function class that is well approximable by algebraic polynomials. For example, it can be used to prove an order of approximation of \(^{-m^{1/(2n)}/17}\) for the approximation of functions \(f:_{n}\) that can be _holomorphically extended_ onto some polyellipse in \(^{2n}\). The parameter \(>1\) specifies the size of this polyellipse. We refer the interested reader to Appendix D for detailed definitions, statements and proofs for this fact.

## 4 Optimality of the derived approximation rate

In this section we discuss the optimality of the approximation rate proven in Theorem 3.2. We first deduce from general results by DeVore et al.  that the rate is optimal in the setting that the map which assigns to a function \(f C^{k}(_{n};)\) the weights of the approximating network is continuous, as is the case in Theorem 3.2. However, it might be possible to achieve a better degree of approximation if this map is _not_ required to be continuous, which is discussed in the second part of this section. Proofs for all the statements in this section are given in Appendices E.1, F.2 and F.3.

**Continuous weight selection.** We begin by considering the case of continuous weight selection. As mentioned in the introduction, this is a natural assumption, since in classical training algorithms such as (S)GD, continuous operations based on samples \(f(x_{j})\) are used to adjust the weights.

In [19, Theorem 4.2] a lower bound of \(m^{-k/s}\) is established for the rate of approximating functions \(f\) of Sobolev regularity \(W^{k,}\) in the following very general setting: The set of functions that is used for approximation can be parametrized using \(m\) (real) parameters and the map that assigns to \(f\) the parameters of the approximating function is continuous with respect to _some_ norm on \(W^{k,}([-1,1]^{s};)\). A detailed version of the proof of that statement (for \(C^{k}\) instead of \(W^{k,}\)) is contained in Appendix E.1. A careful transfer of this result to the complex-valued setting yields the following theorem (see also Appendix E.1).

**Theorem 4.1**.: _Let \(n,k\). Then there exists a constant \(c=c(n,k)>0\) with the following property: For any \(m\), any map \(:C^{k}(_{n};)^{m}\) that is continuous with respect to some norm on \(C^{k}(_{n};)\) and any map \(M:^{m} C(_{n};)\) we have_

\[_{f C^{k}(_{n};),\|f\|_{C^{k}(_{n};)}  1}\ \|f-M((f))\|_{L^{}(_{n};)} c m^{-k /(2n)}.\]

The interpretation of Theorem 4.1 is as follows: If one approximates \(C^{k}\)-functions on \(_{n}\) using a set of functions that can be parametrized using \(m\) (complex) parameters and one assumes that the weight assignment is continuous, one cannot achieve a better rate of approximation than \(m^{-k/(2n)}\). As a special case it can be deduced that the approximation rate is at most \(m^{-k/(2n)}\) when approximating \(C^{k}\)-functions using shallow CVNNs with \(m\) parameters under continuous weight assignment (see Corollary E.3). One can show that this even holds for _deep_ CVNNs. Hence, for continuous weight selection the rate proven in Theorem 3.2 is optimal, even in the class of (possibly) deep networks.

**Discontinuous weight selection.** When we drop the continuity assumption on the selection of the weights, the behavior of the optimal approximation rate is more subtle. Precisely, we show that there are activation functions for which the rate of approximation can be improved to \(m^{-k/(2n-1)}\). On the other hand, we also show that there are activation functions for which an improvement of the approximation rate (up to logarithmic factors) is not possible.

**Theorem 4.2**.: _There exists a function \( C^{}(;)\) which is non-polyharmonic with the following additional property: For every \(k\) and \(n\) there exists a constant \(c=c(n,k)>0\) such that _for any \(m\) and \(f C^{k}(_{n};)\) there is a first layer \(_{n,m}^{}\) and a vector \(^{m}\) such that_

\[\|f-^{T}\|_{L^{}(_{n};)}  c m^{-k/(2n-1)}\|f\|_{C^{k}(_{n}; )}.\]

_Sketch of proof._ The function \(\) is constructed in the following way: Take a countable dense subset \(\{u_{}\}_{}\) of \(C(_{1};)\), for instance the set of all polynomials in \(z\) and \(\) with coefficients in \(+i\). Define \(\) in a way such that

\[(z+3)=u_{}(z)\]

for every \(z_{1}\) and \(\). Furthermore, to ensure that \(\) is non-polyharmonic, let \((z)=e^{(z)}\) for every \(z_{1}\). The smoothness of \(\) can be accomplished by multiplying with a smooth _bump function_; see Lemma F.4 for details. The construction of \(\) is illustrated in Figure 3.

Let then \(f C^{k}(_{n};)\) be arbitrary. General results from the theory of _ridge functions_ show that there exist \(b_{1},...,b_{m}^{n}\) and \(g_{1},...,g_{m} C(_{1};)\) such that

\[\|f(z)-_{j=1}^{m}g_{j}(b_{j}^{T} z)\|_{L^{ }(_{n};)} c m^{-k/(2n-1)}\| f\|_{C^{k}(_{n};)};\]

see Proposition F.3 and Appendix F.1 for a detailed proof of this fact. Approximating the functions \(g_{j}\) by suitable functions \(u_{_{j}}\) and expressing those functions via \((+3_{j})\) yields the claim. 

The preceding theorem showed that there exists an activation function for which the rate in Theorem 3.2 can be strictly improved, if one allows a discontinuous weight selection. In contrast, the following theorem shows for a certain (quite natural) activation function that the rate \(m^{-k/(2n)}\)_cannot_ be improved (up to logarithmic factors), even if one allows a discontinuous weight assignment.

**Theorem 4.3**.: _Let \(n,k\) and_

\[:,(z):=(z)}}.\]

_Then \(\) is smooth but non-polyharmonic. Furthermore, there exists a constant \(c=c(n,k)>0\) with the following property: For any \(m_{ 2}\) there exists a function \(f C^{k}(_{n};)\) with \(\|f\|_{C^{k}(_{n};)} 1\), such that for every \(_{n,m}^{}\) and \(^{m}\) we have_

\[\|f-^{T}\|_{L^{}(_{n};) } c(m(m))^{-k/(2n)}.\]

Sketch of proof.: The idea of the proof is based on that of [46, Theorem 4] but instead of the bound for the VC dimension of ReLU networks used in , we will employ a bound for the VC dimension stated in [4, Theorem 8.11] using the real sigmoid function. For a detailed introduction to the concept of the VC dimension and related topics, see for instance [39, Chapter 6].

A technical reduction from the complex to the real case (see Appendix F.3) shows that it suffices to show the following: If \((0,)\) and \(m\) are such that for every \(f C^{k}([-1,1]^{n};)\) with \(\|f\|_{C^{k}([-1,1]^{n};)} 1\) there exists a real-valued shallow network \(\) with \((x)=}\) as activation function satisfying \(\|f-\|_{L^{}([-1,1]^{n};)}\), then necessarily

\[m c}{(1/)},\]

Figure 3: Illustration of the construction of the activation function in Theorem 4.2.

where the constant \(c\) only depends on \(n\) and \(k\).

To show that the latter claim holds, we assume that \(\) and \(m\) have the property stated above. Take \(N\) such that \(N^{-k}\) and consider the grid

\[:=\{-N,...,N\}^{n}[-1,1]^{n}.\]

For every \(\) we pick a number \(z_{}\{0,1\}\) arbitrarily and construct a map \(f C^{}([-1,1]^{n};)\) satisfying \(f()=z_{}\) for very \(\). Scaling of \(f\) to \(\) ensures \(\|\|_{C^{k}([-1,1]^{n};)} 1\), but then \(()=c_{0} z_{} N^{-k}\) where \(c_{0}=c_{0}(n,k)>0\). By assumption, we can infer the existence of a shallow real-valued neural network \(\) with \(\) as activation function and \(m\) hidden neurons satisfying \(\|-\|_{L^{}([-1,1]^{n};)}\). But this shows

\[()>N^{-k},&z_{}=1,\\ <N^{-k},&z_{}=0 \]

with a constant \(=(n,k)>0\). Since the \(z_{}\) are arbitrary, it follows that the set

\[H:=(>N^{-k})_{}:\; m}\]

_shatters_ the whole grid \(\). This yields \((H)||=(2N+1)^{n}\). On the other hand, the bound provided by [4, Theorem 8.11] for linear threshold networks yields \((H) m(N)\). Combining the two bounds and using \(N^{-k}\) yields the claim. 

## 5 Tractability of the considered problem in terms of the input dimension

In this section we discuss the _tractability_ (in terms of the input dimension \(n\)) of the considered problem, i.e., the dependence of the approximation error on \(n\). We show a novel result stating that, assuming a continuous weight selection, the problem of approximating \(C^{k}\)-functions is _intractable_, i.e., that the number of neurons that is required to achieve a non-trivial approximation error is necessarily exponential in \(n\). In the literature this is referred to as the _curse of dimensionality_. The proof of the theorem combines ideas from  and  and is contained in Appendix E.2.

**Theorem 5.1**.: _Let \(s\). With \(\|\|_{C^{k}([-1,1]^{s};)}\) defined similarly to (2.1), we write_

\[\|f\|_{C^{}([-1,1]^{s};)}:=_{k}\|f\|_{C^{k}([ -1,1]^{s};)}[0,]\]

_for any function \(f C^{}([-1,1]^{s};)\) and denote by \(C^{,*,s}\) the set of all \(f C^{}([-1,1]^{s};)\) for which this expression is finite. Let \(:C^{,*,s}^{2^{2}-1}\) be continuous with respect to some norm on \(C^{,*,s}\) and moreover, let \(M:^{2^{2}-1} C([-1,1]^{s};)\) be an arbitrary map. Then it holds_

\[_{f C^{},*,*\\ \|f\|_{C^{}([-1,1]^{s};)} 1}\|f-M( (f))\|_{L^{}([-1,1]^{s};)} 1.\]

Note that Theorem 5.1 is formulated for real-valued functions but can be transferred to the complex-valued setting (see Corollary E.6). We decided to include the real-valued statement because it is expected to be of greater interest in the community than the complex-valued analog. Moreover, we stress that Theorem 5.1 is not limited to the class of shallow neural networks but refers to any function class that is parametrizable using finitely many parameters (in particular, e.g., the class of neural networks with possibly more than one hidden layer).

We now examine in what way the constant \(c\) appearing in Theorem 3.2 suffers from the curse of dimensionality. To this end, it is convenient to rewrite the result from Theorem 3.2 as

\[_{\|f\|_{C^{k}(n_{i};)} 1}_{^{0}_{n,m},^{m}}\|f-^{T}\|_{L^{}(_{n};)}( m)^{-k/(2n)}\]

where the constant \(=(n,k)>0\) is independent of \(m\). Writing the result in that way, one sees immediately that, if one seeks to have a worst-case approximation error of less than \(>0\), it is sufficient to take \(m=}^{-(2n)/k}\) neurons in the hidden layer of the network. Corollary E.7 shows that it _necessarily_ holds \( 16 2^{-n}\) and therefore, the constant \(\) unavoidably suffers from the curse of dimensionality. An analysis of the constant (where we refer to Appendices C.1 to C.3 for the details) shows that in our case we can establish the bound \((n,k)(-C n^{2}) k^{-4n}\) with an _absolute_ constant \(C>0\). We remark that, since the constant suffers from the curse of dimensionality in any case, we have not put much effort into optimizing the constant; there is therefore probably ample room for improvement.

## 6 Limitations

To conduct a comprehensive evaluation of machine learning algorithms, one must analyze the questions of approximation, generalization, and optimization through training algorithms. The present paper, however, only focuses on the aspect of approximation. Analyzing if the proven approximation rate can be attained with learning algorithms such as (stochastic) gradient descent falls outside the scope of this paper. Furthermore, the examination of approximation rates under possibly discontinuous weight assignment is not yet fully resolved by our results. It is an open question which rate is optimally achievable in that case, depending on the choice of the activation function, and specifically in distinguishing between shallow and deep networks. We want to mention the two following points which indicate that this is a quite subtle question:

1. For deep NNs (with more than two hidden layers) with general smooth activation function, it is _not possible_ to derive any non-trivial lower bounds in the setting of unrestricted weight assignment, since there exists an activation function with the property that NNs of _constant size_ using this activation function can approximate any continuous function to arbitrary precision (see [31, Theorem 4]). Note that  considers real-valued NNs, but the results can be transferred to CVNNs with a suitable choice of the activation function.
2. In the real-valued case, fully general lower bounds for the approximation capabilities of shallow NNs have been derived by using results from  regarding the approximation properties of so-called ridge functions, i.e., functions of the form \(_{j=1}^{m}_{j} a_{j},x\) with \(a_{j}^{d}\) and each \(_{j}:\). It is an interesting problem to generalize these results to higher-dimensional ridge functions of the form \(_{j=1}^{m}_{j}(A_{j}x)\), where each \(_{j}:^{s}\) and \(A_{j}^{s d}\). This would imply lower bounds for shallow CVNNs. However, such a generalization seems to be highly non-trivial and is outside the scope of the present paper.

## 7 Conclusion

This paper analyzes error bounds for the approximation of complex-valued \(C^{k}\)-functions by means of complex-valued neural networks with smooth and non-polyharmonic activation functions. It is demonstrated that complex-valued neural networks with these activation functions achieve the _identical_ approximation rate as real-valued networks that employ smooth and non-polynomial activation functions. This is an important theoretical finding, since CVNNs are on the one hand more restrictive than real-valued neural networks (since the mappings between layers should be \(\)-linear and not just \(\)-linear), but on the other hand more versatile, since the activation function is a mapping from \(\) to \(\) (i.e., from \(^{2}^{2}\)) rather than from \(\) to \(\) as in the real case. Additionally, it is established that the proven approximation rate is optimal if one assumes a continuous weight selection. In summary, if one focuses on the approximation rate for \(C^{k}\)-functions, CVNNs have the same excellent approximation properties as real-valued networks.

The behavior of the approximation rate for unrestricted weight selection is more subtle. It is shown that a rate of \(m^{-k/(2n-1)}\) can be achieved for certain activation functions (Theorem 4.2) but in general, one cannot improve on the rate that is attainable for continuous weight selection (Theorem 4.3).

While the proven approximation _rate_ is optimal under the assumption of continuous weight selection, the involved constants suffer from the curse of dimensionality. Section 5, however, shows that this is inevitable in the given setting.

Such theoretical approximation results contribute to the mathematical understanding of Deep Learning. The remarkable approximation-theoretical properties of neural networks can be seen as one reason why neural networks provide outstanding results in many applications.

**Acknowledgments.** PG and FV acknowledge support by the German Science Foundation (DFG) in the context of the Emmy Noether junior research group VO 2594/1-1. FV acknowledges support by the Hightech Agenda Bavaria.