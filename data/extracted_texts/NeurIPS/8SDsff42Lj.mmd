# Continual Learning with Global Prototypes: Beyond the Scope of Task Supervision

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Continual learning aims to sequentially learn from different tasks without catastrophic forgetting. With no assumptions of task dependence, the knowledge learned from observed tasks may not align with that required for future tasks. This may result in models' disruptive updates for learning future tasks, causing abrupt changes to previously learned knowledge (e.g. representation drift ) which induces catastrophic forgetting. To reduce such disruptive updates, we connect knowledge for observed and unknown tasks by learning task data representations properly related to a set of global prototypes, which have general-purpose connections and are shared across all tasks. We derive global prototypes and the corresponding objective for NLP tasks. For those tasks, the correlated global prototypes can be obtained from a model pre-trained by masked language modeling. And the data representations that have proper relationships to global prototypes can be learned by specific adaptations of the pre-trained model. We investigate existing adaptation models and propose a neighbor attention model which combines different advantages of existing models for our objective. Experiments show that models learning data representations well related to global prototypes can induce significantly less catastrophic forgetting, without memorizing information from past tasks.

## 1 Introduction

In the continual learning paradigm, models progressively learn a sequence of tasks. This paradigm supports real-world applications which face continuous streams of data and tasks [35; 20]. In practice, models may be under storage constraints to use a fixed structure and under privacy considerations that restrict revisiting of previous tasks' data. These introduce the challenge of _catastrophic forgetting_, where models lose knowledge of previously learned tasks after learning new tasks.

Most prior works address catastrophic forgetting using models that integrate the knowledge of the past and present tasks, i.e. the observed tasks. For example, regularization-based models constrain the deviation of current parameters from the previous ones [27; 56; 2; 29]; replay-based models memorize samples from past tasks and rehearse when learning present tasks [35; 9; 46; 26]. However, since there are no assumptions on task dependence in continual learning, models learned from a set of observed tasks may not contain knowledge needed for unknown _future_ tasks [28; 16]. To learn such a future task, these models may have disruptive changes on previously learned knowledge (e.g. representation drift ), which still induces catastrophic forgetting. One way to reduce such disruptive updates is to make models consider potential knowledge connections to future tasks.

Our key idea is to build connections between observed and unknown tasks by connecting _task-specific_ data representations to a _general-purpose_ representation base that is shared across all tasks. In many domains, task-specific information about classes can be represented by specific combinations of general units. For example, consider the data instance '_A boy in a red hooded top is smiling. The_boy is upset._' from _'contradiction'_ class in an entailment classification task. The set {_smiling_, _upset_} conveys the task-specific information of _'contradiction'_ using the general (i.e. not task-specific) semantics of the token units _'smiling'_ and _'upset'_. Based on this, we construct a general-purpose representation base consisting a set of unit representations, which we call _global prototypes_. These global prototypes are pre-learned to reflect semantic connections between them. Then we learn data representations with appropriate task-specific connections to global prototypes. This allows knowledge learned from observed tasks to connect to that of future tasks via the interconnection of global prototypes, which is beyond the scope of task supervision from observed tasks. Our idea mimics mechanism in the brain, a biological continual learning system  which rewires existing neurons instead of creating new neurons to learn new tasks . Here, global prototypes mimic the neurons, and learning different connections between data representations and global prototypes mimic the rewiring process. A figure of the idea is shown in Figure 1.

We address two main challenges in realizing this idea: (1). constructing the representation base with correlated global prototypes; (2). learning data representations with task-specific connections to global prototypes. We investigate the above challenges for NLP tasks. For text, the non-contextual token representations are a natural choice for global prototypes, as any text information can be represented by sets of tokens from a fixed vocabulary. For the first challenge, we obtain the global prototypes from a pre-trained language model which learns semantic connections between tokens through self-supervised learning . For the second challenge, we learn data representations by lightly adapting a pre-trained model to obtain task-specific connections to the global prototypes (Section 3). We investigate existing adaptation models with learnable projections (_Adapters_), learnable embeddings (_Prompt Tuning_), and propose a neighbor attention module combining properties of these two (Section 4). Results show that catastrophic forgetting can be significantly mitigated with models that can learn representations well connected to global prototypes. In addition, our neighbor attention model combines the advantages of existing adaptation models, and achieves superior performance in both vanilla and replay settings.

In conclusion, our contributions in this paper are:

1. We propose to learn task-specific information over a general-purpose base with global prototypes to address general task connections in continual learning. Specifically, we derive the construction of the base and the corresponding objective for NLP tasks.
2. We investigate existing adaptation models and propose a new neighbor attention model to learn data representations that have proper relationships to global prototypes.
3. We conduct experiments on different adaptation models and continual learning frameworks. Results show our model can significantly reduce forgetting without replay.

Figure 1: Representations learned with or without global prototypes. The shaded regions cover data representations for each class. In (a), with knowledge only learned for observed supervised tasks, models may have disruptive updates that cause data representation drift when learning a new task. In (b), with reference to correlated global prototypes (dots) in each task learning, representations for different tasks (shaded regions) can properly connect to each other which reduces representation drift.

Related Work

**Continual Learning** Continual learning aims to sequentially learn new tasks while not forgetting previously learned tasks. Models for continual learning can be divided into three main categories: (1). regularization-based models which constrain the deviation of new parameters from the older ones [27; 56; 2; 29]; (2) replay-based models which reduce catastrophic forgetting by rehearsing on real or pseudo samples from previous tasks [35; 9] or generative models [46; 26]; (3). architecture-based models which learn evolving architectures for sequential tasks, with their capacities for each task carefully assigned [44; 53]. Most works above focus on knowledge based on observed tasks.

Some recent works show that knowledge only from observed task supervision is insufficient for continual learning. Knoblauch et al.  claim that optimal continual learning requires perfect memory and is NP-hard; Guo et al.  suggest preservingholistic information which may not benefit current task but help future tasks. With biased knowledge, models can have disruptive updating when learning a new task, causing problems like representation drift [7; 36; 24]. In this paper, we propose to consider knowledge beyond observed task supervision through a general-purpose base with pre-learned global prototypes. Unlike previous works [39; 4] which use a pre-defined classifier to help class separation , our global prototypes are pre-learned with general semantic connections and thus can build connections between tasks. Some works use self-supervised learning to learn more general representations for continual learning [36; 14; 15]. However, those representations do not necessarily connect to specific global prototypes, which is different from our objective.

Continual learning for NLP is an emerging area . Liu et al.  introduce a sentence encoder with matrix conceptors; MBPA++  uses an episodic memory with replay and local adaptation to mitigate catastrophic forgetting; LAMOL  learns to generate training samples for replay based on pre-trained knowledge; IDBR  disentangles hidden spaces to distinguish task-agnostic and task-specific information. Most of them require a memory of past task information, or converting data to a question-answering format along with text-to-text models [6; 42]. Our model does not have such restriction. There are also works  focusing on knowledge transfer in continual learning.

**Adaptation Models** In this work, we use adaptation models to learn representations connected to global prototypes. Prior works using pre-trained model with light adaptation for target tasks were originally aimed at parameter efficient tuning. Different methods include adding limited trainable parameters on the frozen transformer layer [21; 40; 18; 22]; or selectively updating existing parameters during training [41; 55]. Recent prompt tuning works [31; 30; 34] learn target tasks by trainable prompt embeddings for generalization purposes as well.

Most closely related work are adaptation models used for continual learning [51; 13; 43]. However, most use the models' parameter efficiency to construct progressive memory. Whether utilizing the pre-trained knowledge can help continual learning, why and how they help remain unexplored. Our approach is based on a fixed model _without_ progressive memory of parameters. We use the adaptation model for our desiderata, which also provides a metric to interpret whether the model can benefit continual learning. We believe our work can inspire further utilization of adaptation models for CL.

## 3 Learning over Global Prototypes

We consider the following continual learning setting: the model learns from a sequence of tasks, where each task consists of data \(_{}=\{(_{}^{(i)},y_{}^{(i)})_{i=1}^{n_{}}\}\). \(_{}\) is the input data and \(y_{}\) is the class label. A task identifier \(\) is provided at the training time. We consider two scenarios: _task-incremental_ and _class-incremental_ learning, where models are task-aware or task-agnostic at the inference time . Without replay, we use the same training objective for both task-incremental and class-incremental learning while evaluating them in different ways.

**Notation**\(C_{}\) represents a set of all classes for each task \(\), \(C=[C_{1},...C_{},...]\) represents all classes for all tasks. For NLP tasks, \(V\) represents the set of tokens with global prototypes in the representation base. \(^{i}\) is the \(i\)-th column of a matrix \(\). Our main model consists of two components: an encoder \(f_{}\) to generate representation \(f_{}(_{})\) for each data instance \(_{}\); and a classifier with matrix \(_{}^{d|C|}\) for class prediction, where \(d\) represents the dimension of data representations. At the inference time, the class label is predicted by \(*{arg\,max}_{i C_{}}}f_{}( _{})_{}^{i}\). For task-incremental inference we have \(C_{}}=C_{}\), while for class-incremental inference we have \(C_{}}=C_{1:}\).

### The Learning Objective

Classification LossFor a task \(\), the typical classification objective is to minimize the cross-entropy loss \(_{c}(_{};,)\) over the training data for the task, as shown below:

\[_{c}(_{};,)=-_{}^{y_{}} f_{}(_{}))}{_{c C_ {}}(_{}^{c} f_{}(_{}) )}.\] (1)

After learning task \(\), models have knowledge about data \(_{1:}\) and class vectors \(_{}^{c C_{1:}}\) from observed tasks \(1:\). However, the knowledge may not align with that required for the unknown future task \((+1)\). Specifically, after adjusting \(\) in task \((+1)\), the alignment between \(_{}^{y_{}}\) learned from task \(\) and \(f_{}(_{})\) with adjusted \(\) may shift and degrade. In other words, to learn a future task, models may have disruptive updates which make abrupt changes to previously learned knowledge (e.g. representation drift ), and induce forgetting.

Prototype LossTo mitigate models' disruptive updates, we consider potential connections between observed and unknown tasks. The connection is built by learning task-specific data representations connected to a general-purpose representation base, which is shared across all tasks. The base consists of global token prototypes (denoted \([v]\) for token \(v\)) which reflect semantic connections between them. In particular, we want the data representation \(f_{}(_{})\) to be connected to the task-relevant global prototypes. Given a reference probability distribution \(p(v|_{},_{})\) which indicates the strength of connection between data representation and \([v]\), we push the data representations towards the prototypes in proportion to their reference probability. Formally, we define the prototype loss as:

\[_{v}(_{};)=-_{v V}p(v|_{},y_{})[v] f_{}( _{}))}{_{v^{} V}([v^ {}] f_{}(_{}))}.\] (2)

In Eq.(2), the softmax is calculated over all global prototypes, i.e. \([v]\) for any \(v V\), regardless of task difference. Such calculation is task-agnostic, while the referenced probability \(p(v|_{},_{})\) gives task-specific guidance for representation learning. By doing this, Eq. (2) learns representations with task-specific connections to global prototypes. Since global prototypes are pre-learned to reflect semantic connections, representations learned by Eq. (2) can connect across tasks via connections of global prototypes. This can reduce abrupt representation change caused by disruptive updating.

The reference probability \(p(v|_{},_{})\) gives task-specific guidance for representation learning, where tokens with task-specific information of \(_{}\) should have high probabilities. Considering both task-specific and holistic information of the data [16; 36], we set \(p(v|_{},y_{})=1/r_{}\) when \(v\) is one of data's \(r_{}\) rationale tokens, i.e. tokens _in the data_ that are essential for class prediction , otherwise \(p(v|_{},y_{})=0\). Using multiple rationale tokens as task-specific guidance brings extra benefits to the expressiveness of data representations and global prototypes. First, different data representations from the same class have different guidance. Second, a small number of global prototypes can convey rich information when connecting representations to different sets of global prototypes.

Learning ObjectiveBased on the above analysis, our learning objective is to learn data representations that can correctly predict class labels (Eq. (1)); and properly connect to global prototypes (Eq. (2)). The optimal parameters \(^{}\), \(^{}\) for task \(\) should satisfy the desiderata below:

* **Task performance.**\(_{c}(_{};^{},^{})_{c}(_{};,)\) for any \(^{}\), \(^{}\) (3)
* **Global alignment.**\(_{v}(_{};^{}) a_{}\) (4)

where \(a_{}>0\) is a threshold value of the prototype loss. Task performance desiderata (Eq. (3)) can be satisfied by optimization on classification loss in Eq. (1). In the rest of this section, we discuss two questions that are necessary for our desiderata: **(1)**. How to get the semantically connected global prototype \([v]\) for Eq. (2)? **(2)**. How to get feasible models for the second desiderata in Eq. (4)?

### Pre-trained Models for Prototypes and Data Representations

To get correlated global prototypes and learn data representations with reference to them, we utilize a model pre-trained by masked language modeling (MLM). The MLM objective is to predict maskedtoken \(v_{m}\) from a masked input \(}\), with the following loss:

\[_{m}(};,)=-_{v V}p(v| })_{}^{v} f_{}( })}{_{v^{} V} _{}^{v^{}} f_{}(})},\] (5)

where \(f_{}\) denotes the encoder for MLM, \(_{}\) consists of the token vector \(_{}^{v}\) for each token \(v\). The probability \(p(v|})=1\) if \(v\) is the masked token \(v_{m}\), and \(0\) otherwise.

Pre-Trained Model for Global PrototypesThe MLM objective learns token vectors \(_{}\) that reflect semantic connections between tokens, which suits our requirement for global prototypes. Therefore, we can get the global prototype \([v]\) as the \(v\)-th token vector (\([v]=_{}^{v}\)) from a model pre-trained by MLM. Extending to cases when pre-trained models are unavailable, we can first train a model by self-supervised learning which learns global prototypes. Global prototypes are fixed once learned. We leave improving them during continual task learning for future study.

Adapting Pre-Trained Models for FeasibilityTo get feasible models for the desiderata in Eq.(4), we have two options: (a). learning with the prototype loss in Eq.(2); (b). designing a model which can satisfy the desiderata without direct supervision of probabilities \(p(v|_{},y_{})\). Option (a) needs rationale tokens to get \(p(v|_{},y_{})\), which requires expensive human annotations. In this work, we investigate models for option (b). Specifically, we investigate whether adapting a pre-trained model where we get global prototypes can satisfy our desiderata. Comparing Eq.(5) and Eq.(2), when having \([v]=_{}^{v}\), models for Eq.(5) learn representations that have task-agnostic connections to global prototypes, which is a variant of Eq.(2). When lightly adapting a pre-trained encoder \(f_{}\) to task encoder \(f_{}\), data representations are learned with reference to those task-agnostic connections. Therefore, the adapted representations may have better connections to global prototypes.

In general, our learning includes two stages: first training a model by self-supervised learning for global prototypes (can be skipped if starting from a pre-trained language model); then lightly adapting this model for target tasks while satisfying the desiderata in Eq. (4). We investigate different adaptation models and whether they satisfy our desiderata in the following sections.

## 4 Adaptation Models for Global Alignment

We investigate the potential of different adaptation models for our desiderata of global alignment in Eq.(4). In this section, we first introduce existing adaptation models (Section 4.1) and propose a new neighbor attention model for the desiderata (Section 4.2). A comparison of models is shown in Fig. 2.

### Existing Adaptation Models

For a transformer model, representations are calculated by the self-attention mechanism. Given input representations \(=[_{1},...,_{n}]\), each output representation \(_{i}\) after self-attention is:

\[_{i}=f_{}(_{i}), _{}(),_{}(),\] (6)

where MHA is the multi-head attention function (Appendix A), \(f\) is the feed-forward function, \(_{}\), \(_{}\), \(_{}\) are linear functions for query, key and value. Adaptation models utilize pre-trained parameters for self-attentions, while adding extra components to adapt the model for target tasks. According to He et al. , different adaptations can be viewed as combining different modification vectors \(_{}_{i}\) to pre-trained representation \(_{i}\). We investigate two types of modifications below.

Learnable ProjectionsModels like _Adapters_ insert adaptation modules between transformer layers. The module applies linear projections to the self-attention output \(_{i}\), with the non-linear

Figure 2: Layers of the transformer and different adaptation models. Shaded blocks are learnable.

activation between them. With a residual connection , the adapted output \(_{i}^{}\) is:

\[_{i}^{}_{i}+_{} _{i},_{}_{i}:=_{}_{i}.\] (7)

\(_{}^{d d}\) represents the linear projections. (We omit the non-linear activation for simplicity).

**Learnable Embeddings** Models like _Prompt Tuning_ add learnable embeddings in the input. Then self-attention is performed based on the input with prompts. The adapted output is :

\[_{i}^{}(1-(_{i})) _{i}+(_{i})_{}_{i},_{} _{i}:=_{}(_{i}),_ {}(_{}),_{}(_{}).\] (8)

\(_{}\) are learnable prompt embeddings in \(^{p d}\), \(p\) is the number of prompts. \((_{i})\) is a gate value computed from self-attention which decides the ratio of pre-trained and modified representations.

**Choices for Global Alignment** Both of the adaptations show effectiveness in single-task performance for our desiderata Eq. (3) [21; 31]. For global alignment in Eq. (4), _Prompt Tuning_ has a gate \((_{i})\) to mix pre-trained and modified representations. With a small gate value, this may generate representations close to pre-trained representations, and thus better connect to global prototypes. However, the gate \((_{i})\) in Eq. (8) is decided by self attention over inputs and prompts, thus can lean to modified representations \(_{}_{i}\). Also, the learned prompts \(_{}\) may convey information far away from the original data. These may degrade the models' capacity for global alignment. Because of this, we propose a model that has a controlled gate value and relies on neighbors of tokens instead of searching from random prompts for task adaptation. In addition, the training for prompt embeddings is not as easy as that for linear projections [30; 22], which may cause efficiency issues when adapting multiple tasks. We also introduce learnable projections in our model for fast adaptations.

### Transformer with Neighbor Attentions

We design a neighbor attention module added to the pre-trained model for task adaptations. The module has three properties: (1). utilizing learnable linear projections to learn modified representations; (2). acquiring neighbor representations for extra information; (3). using a controlled gate to mix pre-trained and modified representations. The adapted output of the neighbor attention module is:

\[_{i}^{}(1-)_{i}+ _{}_{i},\ _{}_{i}:=_{}(_{i }),_{}(_{i}||_{i}),_{}( _{i}||_{i}).\] (9)

where \(\) is the ratio of modified representations in the mix-up, \(||\) denotes the concatenation operation. \(_{}\), \(_{}\) are learnable linear functions for key and value. \(_{i}=[_{i1},...,_{ik}]\) are \(k\) neighbor representations of the input representation \(_{i}\).

Comparing Eq. (9) to Eq. (8), neighbor attention has learnable linear functions for key and value. Moreover, we manually control the gate by setting \(=0.1\) to push the module to focus more on the pre-trained representations. This is for our desiderata to have representations close to pre-trained ones which are trained over global prototypes. Finally, we introduce neighbor representations \(_{i}\) for information out of the inputs, which can improve the model's expressivity. Details are shown below.

**Neighbor Representations** Before the first neighbor attention layer, we find the initial neighbor representations \(_{i}\) for a hidden representation \(_{i}\). Neighbors of \(_{i}\) can be obtained by comparing the dot product between \(_{i}\) and token embeddings from the pre-trained embedding layer, then selecting \(k\) tokens which have top-\(K\) scores as neighbors. \(K\) decides the range of the neighborhood.

Then we transform neighbor embeddings to the space of \(_{i}\). We disentangle \(_{i}\)'s \(j\)-th neighbor representation \(_{ij}\) into two parts: one related to the hidden representation \(_{i}\); and the other related to neighbor information out of \(_{i}\). The latter can be obtained by deviating neighbor embedding \(_{ij}\) from \(_{i}\)'s token embedding \(_{i}\). Then the transformed neighbor representation is: \(_{ij}=(_{ij}-_{i})+_{i}\), where \(0<\), \(<1\) are scalars. In this paper, we set \(==0.2\).

After that, the neighbor representation \(_{i}\) is updated at each neighbor attention layer. For the \(j\)-th neighbor representation \(_{ij}\), the updated representation \(_{ij}^{}\) for the next layer is:

\[_{ij}^{}_{ij}+_{} _{ij},\ _{}_{ij}:=f(_{}( _{ij}),_{}(_{i}||_{i}),_ {}(_{i}||_{i})).\]

Adding neighbor attention on more layers will increase the model capacity, but also cause more risk of over-smoothing , i.e., neighbor tokens all have the same representations. In practice, we add neighbor attention to less than half of the transformer layers, and leave the last layer untouched for guidance. In continual learning, the optimal layer selections for different tasks may vary.

## 5 Experimental Settings

Single Task Evaluation for DesiderataWe first evaluate the models' capacities for our desiderata Eq. (3) and Eq. (4) on single tasks. We test classification accuracies for desiderata of task performance on tasks from the GLUE benchmark  and SNLI data . For the desiderata of global alignment, we predict top-20 tokens from the learned representation by the pre-trained decoder (global prototypes), and compute the ratio of rationle tokens in the top-20 predictions (i.e. Recall@20). We evaluate this on e-SNLI dataset , where data's rationale tokens  are highlighted by human annotators.

Continual Learning (CL) EvaluationWe evaluate four sequences of tasks: (1) **Yahoo 1**: a split of Yahoo dataset for news question-answer categorization  with 5 disjoint tasks containing 2 classes each; (2) **Yahoo 2**: a Yahoo sequence with the same split as (1) but with more data; (3) **DB**: a split of DBPedia data for Wikipedia article classification  with 7 disjoint tasks containing 2 classes each; (4) **News Series**: a sequence of tasks on news-related data, including AG_news (news classification, 4 classes), MRPC (paraphrase detection, 2 classes) , RTE (text entailment, 2 classes)  and SST (sentiment analysis, 2 classes) . For the above sequences except (2), we randomly sample 1245 samples per class, which is the least number of class samples in our datasets. For (2), we sample 10000 samples per class. We measure the average accuracy and forgetting (Appendix C) with standard deviations. For each sequence, we test five random orders of tasks.

We evaluate for both _task-incremental_ and _class-incremental_ learning. Task identifiers are available at inference time for task-incremental learning but not for class-incremental learning . For class-incremental learning, the original cross-entropy loss over all seen classes will cause significant forgetting [54; 1]. Since our work does not focus on the problem of cross-entropy, we apply the asymmetric strategy (**ACE**) : the current task's classification loss is calculated over in-task classes, while the replay loss is calculated over all seen classes in the memory (if applicable).

Models and CL FrameworksWe compare different adaptation models on BERT-base. Data representation is from a [MASK] token added to the beginning of input to match the pre-training format. Models for comparison are: (1) **NeiAtt**: our standard neighbor attention model. (2) **NeiReg**: our neighbor attention model with extra regularization for holistic information (Appendix B). (3) **Fine-tuning (FT)**: a model in which all parameters are learnable. (4) **Prompt Tuning (ProT)**: the model adding learnable embeddings only to data inputs. (5) **Prefix Tuning v2 (PT2)**: an adaptation model adding learnable embeddings to inputs of all attention layers. (6) **Adapter**: an adaptation model with learnable linear projections injected in each layer. (7) **BitFit**: an adaptation model tuning only bias terms in the pre-trained model. More settings are in the appendix.

We consider different frameworks (methods) for continual learning: (1) **Vanilla**: the vanilla online learning framework; (2) **MBPA**: an episodic memory framework retrieving stored samples to locally adapt the model at inference time . (3) **ER**: an episodic memory framework storing all seen examples and performs sparse (1%) experience replay; (4) **A-GEM**: an episodic memory framework constraining on gradients to prevent degrading performance of previous tasks ; (5) **Probing**: a framework which learns the encoder with Vanilla setting while tunes the classifier for each task using all task data. This is used to evaluate the discrimination of data representations; (6). **MTL**: a muti-task framework that jointly trains on all tasks (not continual learning). For class-incremental cases, we have the above replay-based methods combined with the **ACE** strategy. The baseline performance for each continual learning framework is that on FT model.

## 6 Experimental Results

Models for Desiderata in Eq.(3) and Eq.(4)Figure 3 shows models' capacities for our desiderata. We compare the classification accuracy for desiderata in Eq.(3) and Recall@20 of rationale tokens for desiderata in Eq.(4). The higher scores on both metrics, the better model capacities for our desiderata.

Overall, NeiAttn and PT2 consistently achieve a superior balance between classification and recall scores on different NLI tasks. However, Adapter and FT achieve high classification scores but do not generate representations well related to global prototypes (low recall scores). This supports our intuition that mixing pre-trained and modified representations with a gate can result representations better connected to global prototypes. With explicit regularization on holistic information, NeiRegperforms best in in-task (SNLI\(\)E-SNLI) rationale recalls, while losing its superiority in cross-task (GLUE\(\)E-SNLI) rationale recalls. This may suggest the explicit regularization may not generalize well across tasks. With prompts only in the input, ProT has insufficient capacity for task performance.

For desiderata Eq.(4), NeiAttn and PT2 perform much better than Adapter and FT. We set \(a_{}\) to make NeiAttn and PT2 satisfy Eq.(4) while Adapter and FT fail to, then we evaluate them for CL scenarios.

**Task-Incremental Learning** We test models' capacities for task-incremental learning under different CL frameworks. Results are shown in Table 1. Models are split into two categories according to our desiderata (Eq.(4)) experiment above: (NeiAttn, PT2) which satisfy it and (FT, Adapters) in opposite.

In the vanilla setting, both PT2 and NeiAttn significantly outperform other models with minor forgetting. Adapter on most CL frameworks performs worse than PT2 and NeiAttn, marginally better than FT. This supports our claim that models learning representations better connected to global prototypes perform better in continual learning. Combined with ER and A-GEM, NeiAttn can improve more than PT2 in most cases. FT has significant improvement with replay but can also suffer from overfitting to the replay buffer (ER for Yahoo 1). We also evaluate on a probing framework with only the classifier retrained over task data to evaluate whether the forgetting will cause representations to lose separation. PT2 and NeiAttn also preserve the most separation of representations in this case.

In general, (NeiAttn, PT2) consistently outperform (FT, Adapter) under different CL frameworks. This supports that our desiderata Eq. (4) helps improve models' continual learning ability. NeiAttn performs better with replay. The capacity of models also depends on different data distributions in

    &  &  &  &  &  \\   & & _Acc._ & _Fogret._ & _Acc._ & _Fogret._ & _Acc._ & _Fogret._ & _Acc._ & _Fogret._ \\   & Pretrained & 82.95\(\)5.64 & 7.34\(\)4.09 & 78.70\(\)4.09 & 7.71\(\)1.05 & 95.38\(\)3.34 & 40.83\(\)3.67 & 66.66\(\)4.77 & **5.35\(\)**5.05 \\  & FT (*) & 73.07\(\)1.05 & 18.67\(\)5.41 & 79.82\(\)3.27 & 13.27\(\)4.03 & 73.15\(\)3.34 & 24.90\(\)1.57 & 59.98\(\)3.12 & 21.13\(\)4.14 \\  & Adapter & 79.85\(\)1.05 & 11.86\(\)1.05 & 71.90\(\)2.05 & 20.92\(\)2.07 & 98.70\(\)1.00 & 1.19\(\)1.00 & 65.43\(\)1.75 & 15.53\(\)2.29 \\  & PT2 & 88.62\(\)1.00 & 2.04\(\)1.09 & **96.43\(\)**2.08 & **97.93\(\)**3.60 & **60.07\(\)**5.03 & 61.38\(\)3.08 & **61.39\(\)**2.00 \\  & NeiAttn & **88.96\(\)**2.00 & 19.20\(\)1.09 & 89.84\(\)3.24 & 24.60\(\)9.74 & 34.41\(\)2.54 & 71.95\(\)3.05 & 98.92\(\)2.29 \\   & FT (*) & 72.40\(\)5.02 & 19.34\(\)4.09 & 78.71\(\)3.29 & 14.38\(\)3.26 & 73.01\(\)4.59 & 25.04\(\)4.37 & 60.60\(\)5.00 & 20.52\(\)6.07 \\  & Adapter & 78.50\(\)1.23 & 13.13\(\)2.09 & 73.66\(\)2.09 & 19.15\(\)3.09 & 99.09\(\)1.00 & 80.10\(\)6.28 & 15.67\(\)1.11 \\  & PT2 & **90.69\(\)1.09** & **90.57\(\)1.09** & **13.70\(\)**1.03 & **93.59\(\)**9.06 & **-0.01\(\)**10.00 & **76.16\(\)**1.08 & **4.99\(\)**1.00 \\  & NeiAttn & **90.69\(\)1.09** & 16.27\(\)1.05 & 19.18\(\)1.09 & 19.00\(\)5.73 & 23.55\(\)3.23 & 23.55\(\)3.56 & 55.61\(\)1.12 \\   & FT (*) & 70.77\(\)7.02 & 20.92\(\)2.02 & 90.31\(\)2.02 & 2.67\(\)0.07 & 91.05\(\)3.24 & 8.75\(\)0.09 & 70.44\(\)3.95 & 10.93\(\)3.45 \\  & Adapter & 77.47\(\)3.09 & 14.13\(\)3.25 & 75.79\(\)1.08 & 97.92\(\)2.34 & 0.97\(\)3.24 & 68.11\(\)1.09 & 13.16\(\)1.09 \\  & PT2 & **88.91\(\)**6.20 & **27.65\(\)**9.10 & 92.08\(\)2.09 & **92.03\(\)**9.84** & **0.03\(\)**0.08 & 69.00\(\)1.00 & 15.85\(\)1.09 \\  & NeiAttn & 84.02\(\)1.09 & 7.87\(\)3.12 & **91.54\(\)**2.04 & **1.52\(\)**2.02 & 96.68\(\)1.18 & 0.20\(\)1.78 & **75.05\(\)**0.05 & **73.14\(\)**0.05 \\   & FT (*) & 87.56\(\)3.02 & 4.11\(\)4.00 & 89.98\(\)7.37 & 31.77\(\)0.08 & 84.45\(\)1.08 & 15.34\(\)0.12 & 75.06\(\)0.17 & 5.48\(\)0.01 \\  & Adapter & 80.86\(\)2.06 & 10.65\(\)2.07 & 77.47\(\)3.15 & 15.37\(\)0.92 & 92.52\(\)2.38 & 0.38\(\)3.74 & 73.80\(\)1.06 & 6.72\(\)1.14 \\  & PT2 & 90.40\(\)1.09 & 1.39\( sequence. On News Series, when with replay, FT can even outperform PT2. This may happen because News Series includes data from similar distributions related to the news. And models should have the capacity to deal with knowledge transfer besides catastrophic forgetting.

Results for Class-Incremental LearningFigure 4 shows models' performance on class-incremental learning. PT2 and NeiAttn perform well in the vanilla case, where the training is the same as that for task-incremental learning. This indicates that they can address connections between classes from different tasks even without supervision. On the other side, Adapter and FT perform much worse in this case. Then we evaluate three frameworks with replay: one is the full **ER-ACE** with experience replay at each step; one is the **ER-ACE** (sparse) with sparse experience replay; the other is the ACE strategy with only previous task's data stored in the replay (**AGEM-ACE**). We observe that performance on class-incremental learning heavily relies on the quality of replay. In most cases, FT, Adapter and NeiAttn can benefit more from the replay. We hypothesize that it is related to the fast adaptation ability related to linear projections.

Influence of Parameter-EfficiencyWith limited parameters, adaptation models have less risk of deviating fast from previously learned knowledge compared to FT, and thus may perform better in CL. However, different models' improvements come not just from having fewer trainable parameters. Table 2 shows the comparison of parameters in each model. NeiAttn has better performance in most cases compared to Adapter and Pre-trained models, which have fewer or no trainable parameters in the encoder. Even with more parameters, NeiAttn performs on par with PT2 with Vanilla and outperform PT2 with replay. NeiAttn also requires much less time to train (5 vs 20 epochs). These suggest the adaptation model structure will highly influence its performance on CL.

Visualization of RepresentationsIn Figure 5, we visualize NeiAttn and FT's data representations for class-incremental DB under Vanilla and ER-ACE frameworks. Even trained with in-task classes, Vanilla NeiAttn can well disperse data representations. Learning a model includes learning the encoder (representations) and classifier (class vectors). The learned class vectors may not well align with representations even with replay (left bottom). We hypothesize this may result from different training pages for the encoder and classifier. For FT, the encoder quickly learns representations close to single class centroids, which may degrade the function of the classifier. However, with connections to multiple different global prototypes, NeiAttn representations may not quickly move to one centroid. Therefore, it can better balance the training of the encoder and classifier (right bottom).

## 7 Conclusion

In this paper, we investigate models which consider potential connections between observed and unknown tasks to reduce disruptive updating in CL. Specifically, we learn task-specific data representations appropriately connected to a general-purpose representation base with global prototypes. For NLP tasks, the global prototypes can be obtained from a pre-trained language model. And the representation connected to global prototypes can be obtained by lightly adapting the pre-trained model. We investigate existing adaptation models and propose a neighbor attention model which combines advantages of existing models. Experimental results show that models learning representations appropriately connected to global prototypes have significantly less catastrophic forgetting in CL, even without using experience replay. Specifically, when neighbor attention is used, we suffer from less catastrophic forgetting than FT and Adapter, and surpass PT2 when experience replay is applied. We consider the main limitations of our work as: (1) requiring extra memory to compute neighbor attentions; (2) the optimal number of neighbor attention layers may vary for different tasks.

   Models & FT & Bitfit & Adapter & ProT & PT2 & NeiAttn \\ Parameters (\%) & 1 & 0.5 & 2.3 & 0.5 & 0.8 & 4.9 \\   

Table 2: The ratio of models’ learnable parameters compared to FT.

Figure 4: Results on class incremental learning. Dashed lines show scores of a pre-trained model in the vanilla setting.

Figure 5: T-SNE plot of FT, NeiAttn representations. Triangles are class vectors.