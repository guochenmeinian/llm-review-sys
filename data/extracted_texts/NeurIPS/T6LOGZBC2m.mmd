# OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators

Allen Nie\({}^{1}\) Yash Chandak\({}^{1}\) Christina J. Yuan\({}^{2}\)

**Anirudhan Badrinath\({}^{1}\) Yannis Flet-Berliac\({}^{3}\) Emma Brunskill\({}^{1}\)\({}^{*}\)**

\({}^{1}\)Computer Science, Stanford University

\({}^{2}\)Computer Science, University of Texas, Austin

\({}^{3}\)Cohere

###### Abstract

Offline policy evaluation (OPE) allows us to evaluate and estimate a new sequential decision-making policy's performance by leveraging historical interaction data collected from other policies. Evaluating a new policy online without a confident estimate of its performance can lead to costly, unsafe, or hazardous outcomes, especially in education and healthcare. Several OPE estimators have been proposed in the last decade, many of which have hyperparameters and require training. Unfortunately, choosing the best OPE algorithm for each task and domain is still unclear. In this paper, we propose a new algorithm that adaptively blends a set of OPE estimators given a dataset without relying on an explicit selection using a statistical procedure. We prove that our estimator is consistent and satisfies several desirable properties for policy evaluation. Additionally, we demonstrate that when compared to alternative approaches, our estimator can be used to select higher-performing policies in healthcare and robotics. Our work contributes to improving ease of use for a general-purpose, estimator-agnostic, off-policy evaluation framework for offline RL.

## 1 Introduction

Offline reinforcement learning (RL) involves learning better sequential decision policies from logged historical data, such as learning a personalized policy for math education software (Mandel et al., 2014; Ruan et al., 2024), providing treatment recommendations in the ICU (Komorowski et al., 2018; Luo et al., 2024) or learning new controllers for robotics(Kumar et al., 2020; Yu et al., 2020). Offline policy evaluation (OPE), in which the performance \(J(_{e})\) of a new evaluation policy \(_{e}\) is estimated given historical data, is a common subroutine in offline RL for policy selection, and can be particularly important when deciding whether to deploy a new decision policy that might be unsafe or costly. Offline policy evaluation methods estimate the performance of an evaluation policy \(_{e}\) given data collected by a behavior policy \(_{b}\). There are many existing OPE algorithms, including those that create importance sampling-based estimators (IS) (Precup, 2000), value-based estimators (FQE) (Le et al., 2019), model-based estimators (Paduraru, 2013; Liu et al., 2018; Dong et al., 2023), doubly robust estimators Jiang and Li (2016); Thomas and Brunskill (2016), and minimax-style estimators (Liu et al., 2018; Nachum et al., 2019; Yang et al., 2020).

This raises an important practical question: given a set of different OPE methods, each producing a particular value estimate for an evaluation policy, what value estimate should be returned? A simple approach is to avoid the problem and pick only one OPE algorithm or look at the direction of a set of OPE algorithms' scores as a coarse agreement measure. Voloshin et al. (2021) offered heuristics basedon high-level domain structure (e.g., horizon length, stochasticity, or partial observability), but this does not account for instance-specific information related to the offline dataset adn policies.

In this paper we seek to aggregate the results of a set of multiple off-policy RL estimators to produce a new estimand with low mean square error. This work is related to several streams of prior work: (1) multi-armed bandit and RL algorithms that combine two estimands to yield a more accurate estimate; (2) multi-armed bandit and RL algorithms that select a single estimand out of a set of estimands, and (3) stacked generalization / meta-learning / super learning methods in machine learning.

Research in (1) builds on doubly robust (DR) estimation in statistics to produce an estimand that combines important sampling and model-based methods (Jiang and Li, 2016; Gottesman et al., 2019; Farajtabar et al., 2018). Similarly, accounting for multiple steps, the MAGIC estimator blends between IS-based and value-based estimators within a trajectory (Thomas and Brunskill, 2016). The second line of work (2) does not combine scores but instead introduces an automatic estimator selection subroutine in the algorithm. However, such methods typically assume strong structural requirements on the input estimators. For example, Su et al. (2020); Tucker and Lee (2021) assume as input a nested set of OPE estimators, where the bias is known to strictly decrease across the set. Zhang and Jiang (2021) leveraged a set of Q-functions trained with fitted Q-evaluation (FQE) and cross-compare them in a tournament style until one Q-function emerged. None of these methods allow mix-and-match of different kinds of OPE estimators.

Our work is closest to a third line of more distant work, that of stacked generalization (Wolpert, 1992) / meta-learning and super learning across ensembles. There is a long history in statistics and supervised learning of combining multiple input classification or regression functions to produce a better meta-function. Perhaps surprizingly, there is little exploration of this idea to our knowledge in the context of RL or multi-armed bandits. The one exception we are aware of was for heterogeneous treatment effect estimation in a 2-action contextual bandit problem, where Nie and Wager (2021) utilized linear stacking to build a consensus treatment effect estimate using two input estimatands.

In this paper we introduce the meta-algorithm OPERA (**O**ffline **Policy** Evaluation with **Re**-weighted **A**ggregates of Multiple Estimators). Inspired by a linear weighted stack, OPERA combines multiple generic OPE estimates for RL in an ensemble to produce an aggregate estimate. Unlike in supervised learning where ground truth labels are available, in our setting a key choice is how to estimate the mean squared error of the resulting weighted ensemble. Under certain conditions, bootstrapping (Efron, 1992) can approximate finite sample bias and variance. We use bootstrapping to compute estimates of the mean squared error of different weightings of the underlying input estimators, which can then be optimized as a constrained convex problem. OPERA can be used with any input OPE estimands. We prove under mild conditions that OPERA produces an estimate that is consistent, and will be at least as accurate as any input estimand. We show on several common benchmark tasks that OPERA achieves more accurate offline policy evaluation than prior approaches, and we also provide a more detailed analysis of the accuracy of OPERA as a function of choices made for the meta-algorithm.

## 2 Related Work

Offline policy evaluationMost commonly used offline policy estimators can be divided into a few categories depending on the algorithm. An important family of estimators focuses on using importance sampling (IS) and weighted importance sampling (WIS) to reweigh the reward from the behavior policy (Precup, 2000). These estimators are known to produce an unbiased estimate but have a high variance when the dataset size is small. For a fully observed Markov Decision Process (MDP), a model-free estimator, such as fitted Q evaluation (FQE), is proposed by Le et al. (2019), and one can also learn a model given the data to produce a model-based (MB) estimate (Paduraru, 2007; Fu et al., 2021; Gao et al., 2023). When the behavior policy's probability distribution over action is unknown, a minimax style optimization estimator (DualDICE) can jointly estimate the distribution ratio and the policy performance (Nachum et al., 2019). For a partial observable MDP (POMDP), many of these methods have been extended to account for unobserved confounding, such as minimax style estimation (Shi et al., 2022), value-based estimation (Tennenholtz et al., 2020; Nair and Jiang, 2021), uses sensitivity analysis to bound policy value (Kallus and Zhou, 2020; Namkoong et al., 2020; Zhang and Bareinboim, 2021), or learns useful representation over latent space (Chang et al., 2022).

OPE with multiple estimatorsChoosing the right estimators has become an issue when there are many proposals even under the same task setup and assumptions. Voloshin et al. (2021) proposedan empirical guide on estimator selection. One line of work tries to combine multiple estimators to produce a better estimate by leveraging the strengths of the underlying estimators, for example, (weighted) doubly robust (DR) method (Jiang and Li, 2016). For contextual bandit, Wang et al. (2017) proposed a switch estimator that interpolates between DM and DR estimates with an explicitly set hyperparameter. For sequential problems, MAGIC blends a model-based estimator and guided importance sampling estimator to produce a single score (Thomas and Brunskill, 2016). Another line of work tackles the many-estimator problem by reformulating multiple estimators as one estimator. Yang et al. (2020) reformulated a set of minimax estimators as a single estimator with different hyperparameter configurations. Yuan et al. (2021) constructed a spectrum of estimators where the endpoints are an IS estimator and a minimax estimator and proposed a hyperparameter to control the new estimator. This line of approaches does not leverage multiple estimators or solve the OPE selection problem because they recast the OPE selection problem as a hyperparameter selection problem. The last line of work provides an automatic selection algorithm that chooses one estimator from many, relying on an ordering of estimators (Tucker and Lee, 2021) or being able to compare the output (such as Q-values) directly Zhang and Jiang (2021).

Bootstrapping for model selectionUsing bootstrap to estimate the mean-squared error for model selection was initially proposed by Hall (1990), for the application of kernel density estimation. The idea was subsequently used by others for density estimation (Delaigle and Gijbels, 2004), selecting sample fractions for tail index estimation (Danielsson et al., 2001), time-series forecasting (dos Santos and Franco, 2019) and other econometric applications (Marchetti et al., 2012). Similar ideas have been explored by Thomas et al. (2015) to construct a confidence interval for the estimator. We extend this idea to use bootstrapping to combine multiple OPE estimators to produce a single score.

## 3 Notation and Problem Setup

We define a stochastic Decision Process \(M=,A,T,r,\), where \(\) is a set of states; \(A\) is a set of actions; \(T\) is the transition dynamics; \(r\) is the reward function; and \((0,1)\) is the discount factor. Let \(D_{n}=\{_{i}\}_{i=1}^{n}=\{s_{i},a_{i},s^{}_{i},r_{i}\}_{i=1}^{n}\) be the trajectories sampled from \(\) on \(M\). We denote the true performance of a policy \(\) as its expected discounted return \(J()=_{_{}}[G()]\) where \(G()=_{t=0}^{}^{t}r_{t}\) and \(_{}\) is the distribution of \(\) under policy \(\). In an off-policy policy evaluation problem, we take a dataset \(D_{n}\), which can be collected by one or a group of policies which we refer to as the behavior policy \(_{b}\) on the decision process \(M\). An OPE estimator takes in a policy \(_{e}\) and a dataset \(D_{n}\) and returns an estimate of its performance, where we mark it as \(:\). We focus on estimating the performance of a single policy \(\). We define the true performance of the policy \(V^{}=J()\), and multiple OPE estimates of its performance as \(_{i}^{}(D_{n})=_{i}(,D_{n})\) for the \(i\)-th OPE's estimate.

## 4 Opera

In this section, we consider combining results from multiple estimators \(\{_{i}^{}\}_{i=1}^{k}\) to obtain a better estimate for \(V^{}\). Towards this goal, given \(\{_{i}^{}\}_{i=1}^{k}\), we propose estimating a set of weights \(_{i}^{*}\) such that \(^{}_{i=1}^{k}_{i}^{*}_{i}^{}\) has the lowest mean squared error (MSE) towards estimating \(V^{}\). Formally, let \(}^{k 1}\) be a vector whose elements correspond to values from different estimators, and let \(^{k 1}\) correspond to a vector where each element is the same and corresponds to \(V^{}\). Let \(^{*}^{k 1}\) be a vector with values of all \(_{i}^{*}\)'s and let \(^{k 1}\) be an estimate of \(^{*}\). For any estimator \(_{i}^{}\), the mean-squared error is denoted by,

\[(_{i}^{})_{D_{n}}_{i}^{}(D_{n})-V^{}^{2},\] (1)

where we make \(_{i}^{}\) explicitly depend on \(D_{n}\) to indicate that the expectation is over the random variables \(_{i}^{}\) which depend on the sampled data \(D_{n}\). With this formulation, estimating \(^{*}\) can be elicited as a solution to the following constrained optimization problem.

**Remark 1**.: _Let \(_{i=1}^{k}_{i}=1\), then_

\[^{*}*{arg\,min}_{^{k 1}} ^{}A A}-}-^{ }^{k k}.\] (2)Using the fact that \(_{i=1}^{k}_{i}=1\),

\[^{}=(_{i=1}^{k} _{i}_{i}^{}-V^{})^{2}=( _{i=1}^{k}_{i}_{i}^{}-V^{})^{2} .\] (3)

Now re-writing the equation above equation 3 in vector form,

\[^{}=}-^{}^{2}= ^{}}-}-^{}.\] (4)

Finally, simplifying equation 4 further

\[^{}=^{} }-}- ^{}=^{}A.\] (5)

Therefore, \(\) that minimizes \(^{}\) is equivalent to \(\) that minimizes \(^{}A\).

It is worth highlighting that the optimization problem in Remark 1 is convex in \(\) with linear constraint and thus can be solved by any off-the-shelf solvers (Diamond and Boyd, 2016).

**Estimating \(A\):** An advantage of Remark 1 is that it provides the objective for estimating \(^{*}\). Unfortunately, this objective depends on \(A\), and thus on \(V^{}\), which is not available. Further, observe that \(A\) can be decomposed as

\[A=}-}}-} ^{}+}- }-^{}.\] (6)

where the first term corresponds to co-variance between the estimators \(\{V_{i}^{}\}_{i=1}^{k}\) and the second term corresponds to the outer product between their biases. One potential approach for approximating \(A\) could be to ignore biases. While this could resolve the issue of not requiring access to \(\), ignoring bias can result in severe underestimation of \(A\), especially in finite-sample settings or when function approximation is used. Further, even if we ignore the biases, it is not immediate how to compute the covariance of various OPE estimators, e.g., FQE.

We propose overcoming these challenges by constructing \(^{k k}\), an estimate of \(A^{k k}\), using a statistical bootstrapping procedure (Efron and Tibshirani, 1994). Subsequently, we will use the \(\) as a plug-in replacement for \(A\) to search for the values of \(\) as discussed in Remark 1. There is a rich literature on using bootstrap to estimate bias (Efron, 1990; Efron and Tibshirani, 1994; Hong, 1999; Shi, 2012; Mikusheva, 2013) and variance (Chen, 2017; Gamero et al., 1998; Shao, 1990; Ghosh et al., 1984; Li and Maddada, 1999) of an estimator that can be leveraged to estimate the terms in equation 6. Instead of estimating the bias and variance individually, we directly use the bootstrap MSE estimate (Chen, 2017; Williams, 2010; Cao, 1993; Hall, 1990) to approximate \(A\).

For bootstrap estimation to work, two key challenges need to be resolved. Even if provided with \(V^{}\), the regular bootstrap is not guaranteed to yield an MSE estimate which is asymptotic to the true MSE if the distribution has heavy tails (Ghosh et al., 1984). Furthermore, \(V^{}\) is unknown in the first place. To address these challenges we follow the work by Hall (1990), where the first issue is resolved by using sub-sampling based bootstrap resamples of size \(n_{1}<n\), where \(n_{1}\) is of a smaller order than \(n\). Therefore, we draw data \(D_{n_{1}}^{*}=\{_{1}^{*},...,_{n_{1}}^{*}\}\) from \(D_{n}=\{_{1},...,_{n}\}\) with replacement. To resolve the second issue, we leverage the MSE estimate by Hall (1990), and approximate equation 1 using

\[}(_{i}^{})_{D_{n_{1}}^{*}} [_{i}^{}(D_{n_{1}}^{*})-_{i}^{}^{2} D_{n}].\] (7)

Building upon this direction, we propose using the following estimator \(\) for \(A\),

\[_{D_{n_{1}}^{*}}}(D_{n_{1}}^{*})-}}(D_ {n_{1}}^{*})-}^{}D_{n}, ^{k k}\] (8)

and we substitute \(\) for \(A\) in equation 2 to obtain the weights for combining estimates \(\{_{i}^{}\}_{i=1}^{k}\)

\[^{}_{i=1}^{k}_{i}_{i}^{} *{arg\,min}_{ ^{k 1}}^{}^{k  1}.\] (9)There are two key advantages of the proposed procedure: (1) Bias: it does not require access to the ground truth performance estimates \(^{*}\). In the supervised learning setting, a held-out/validation set can provide a way to infer approximation error, However, for the OPE setting there is no such held-out dataset that can be used to obtain reliable estimates of the ground truth performance. (2) Variance: Depending on the choice of the estimator (e.g., FQE), it might not be possible to have a closed-form estimate of the variance, especially when using rich function approximators. Using statistical bootstrapping, OPERA mitigates both these issues and thus is particularly suitable for off-policy evaluation.

Estimating \(^{*}\):We now consider how error in estimating the optimal weight coefficient \(^{*}\) affects the MSE of the resulting estimator \(^{}\). Without loss of generality, we consider \(|^{}_{i}| 1\), since we can trivially normalize each estimator's output by \(|V_{}|\). We now prove that under the mild assumption that the error in the estimated \(\) can be bounded as some function of the dataset size, that we can bound the mean squared error of the resulting value estimate:

**Theorem 1** (Finite Sample Analysis).: _Assume given \(n\) samples in dataset \(D\), and let \(_{c}:=_{D_{n}}^{}-V^{}^{2} \), there exists a \(>0\) such that_

\[ i,_{D_{n}}[\|_{i}-^{*}_{i}\|] n ^{-},\] (10)

\[(^{})}{n^{2}}+_{c}.\] (11)

The error of OPERA is divided into two terms. First note that \(_{c}\) is the approximation error: the difference between the true estimate of the policy performance \(V^{}\) and the best estimand OPERA can yield when using the optimal (unknown) \(^{*}\). If \(V^{}\) can be expressed as a linear combination of the input OPE estimands \(}\), then there is zero approximation error and \(_{c}=0\). The second term in the bound comes from the estimation error due to estimating \(^{*}\)- this arises from the bootstrapping process used for estimating \(A\) in equation 8. For this second term we compute an upper bound using a Cauchy-Schwartz inequality. This term decreases as the dataset size \(n\) increases. The resulting error depends on the rate at which the estimated \(\) converges to the true \(\) as a function of the dataset size. For example, if \(=0.5\), (a \(n^{-.5}\) rate), the MSE will converge at a \(n^{-1}\) rate in the first term, and if \(=0.25\) (a \(n^{-.25}\) rate) the MSE will converge at a \(n^{-0.5}\) rate in the first term. We provide the full proof in Appendix A.4.

We show a full practical implementation of OPERA in Algorithm 1, where we demonstrate how to efficiently construct \(\) and compute \(\).

### Properties of OPERA

For \(\) obtained from the bootstrap procedure in equation 8 to be an asymptotically accurate estimate of \(A\), (a) a consistent estimator of \(\) is required, and (b) the estimators \(}\) need to be smooth. We discuss these points in more detail in Appendix A.3. In the following, we theoretically establish the properties of OPERA on performance improvement and consistency. We also demonstrate how OPERA allows us to interpret each estimator's quality. Further, in Section 6, we empirically study the effectiveness of OPERA even when we do not have any consistent base estimators, or \(^{}_{i}\) is constructed using deep neural networks.

Performance ImprovementIt would be ideal that the combined estimator \(}^{}\) does not perform worse than any of the base estimators \(\{^{}_{i}\}_{i=1}^{n}\). As OPERA optimizes for the MSE, we can directly obtain the following desired result.

**Theorem 2** (Performance improvement).: _If \(=^{*}\), \( i\{1,...,k\},(}^{})(^{}_{i}).\)_

However, observe that due to bootstrap approximation, \(\) may not be equal to \(A\), and thus \(\) may not be equal to \(^{*}\). Nonetheless, as we will illustrate in Section 6, even in the non-idealized setting OPERA can often achieve MSE better than any of the base estimators \(\{V^{}_{i}\}_{i=1}^{n}\).

ConsistencySome prior works that deal with multiple OPE estimators assume that there is at least one known consistent estimator (Thomas and Brunskill, 2016). Under a similar assumption that \(^{}_{i}:^{}_{i}J(),\) OPERA can be made to fall back to the consistent estimator after a large \(n\), such that \(^{}\) is also consistent, i.e., \(}^{}J()\). Naturally, as \(^{}\) is a weighted combination of the base estimators \(\{_{i}^{}\}_{i=1}^{k}\), if _all_ the base estimators provide unreliable estimates, even in the limit of infinite data, then there is not much that can be achieved by weighted combinations of these unreliable estimators.

InterpretabilityWith a linear weighted formulation for \(^{}\), OPERA allows for the inspection of the assigned weights to which give further insights into the procedure. In Figure 1 we provide a synthetic example to illustrate the impact of bias and variance of the input estimators on the values of \(\).

Consider a case where there are two OPE estimators (\(_{1}^{}\) and \(_{2}^{}\)) and two corresponding weights \(_{1}\) and \(_{2}\)). Let the true unknown quantity be \(V^{}=0\). As we can see below, when both estimators have low bias, but one has higher variance, OPERA assigns a higher magnitude of \(_{i}\) for \(V_{i}^{}\) with a lower variance (Figure1, left). When both estimators have similar variance, and their biases have _opposite_ signs with similar magnitude, then \(_{2}_{1}\) (Figure1, middle left).

Interestingly, unlike related prior work (Thomas and Brunskill, 2016), our optimization procedure in equation 2 does not require \(_{i} 0\). Therefore the resulting estimator \(^{}\) may assign negative weights for some of the estimators.

This can be observed for the case when the _sign_ of the \((_{1}^{})\) and \((_{1}^{})\) are the same. In such a case, using a positive and a negative weight can help cancel out the biases of the base estimators, as observed in Figure1 (middle right). When one estimator has no bias and the other has no variance, \(\) values are inversely proportional to their contributions towards the MSE (Figure1, right).

## 5 Experiment

We now evaluate OPERA on a number of domains commonly used for offline policy evaluation. Experimental details, when omitted, are presented in the appendix.

### Task/Domains

**Contextual Bandit**. We validate the performance of OPERA on the synthetic bandit domain with a 10-dimensional feature space proposed in SLOPE (Su et al., 2020). This domain illustrates how OPERA compares to an estimator-selection algorithm (SLOPE) that assumes a special structure between the estimators. The true reward is a non-linear neural network function. The reward estimators are parametrized by kernels and the bandwidths are the main hyperparameters. As in their paper, we ran 180 configurations of this simulated environment with different parametrization of the environment. Each configuration is replicated 30 times.

**Sepsis**. This domain is based on a simulator that allows us to model learning treatment options for sepsis patients in ICU (Oberst and Sontag, 2019). There are 8 actions and a +1/-1/0 reward at the

Figure 1: Interpreting weights for different estimators. X-axis shows the value of \(_{1}^{}\) and Y-axis shows the value of \(_{2}^{}\).

episode end. We experiment with two settings: Sepsis-MDP and Sepsis-POMDP, where some crucial states have been masked. We evaluate 7 different policies: an optimal policy and 6 noised suboptimal policies, which we obtain by adding uniform noise to the optimal policy.

**Graph**. Voloshin et al. (2019) introduced a ToyGraph environment with a horizon length T and an absorbing state \(x_{}=2T\). Rewards can be deterministic or stochastic, with +1 for odd states, -1 for even states plus one based on the penultimate state.We evaluate the considered methods on a short horizon H=4, varying stochasticity of the reward and transitions, and MDP/POMDP settings.

**D4RL-Gym**. D4RL (Fu et al., 2020) is an offline RL standardized benchmark designed and commonly used to evaluate the progress of offline RL algorithms. We use 6 datasets (200k samples each) from three Gym environments: Hopper, HalfCheetah, and Walker2d. We use two datasets from each: the medium-replay dataset, which consists of samples from the experience replay buffer, and the medium dataset, which consists of samples collected by the medium-quality policy. We use conservative Q-learning (CQL) (Kumar et al., 2020), implicit Q-learning (IQL) (Kostrikov et al., 2021), and TD3 (Fujimoto et al., 2018). We train 6 policies from these three algorithms with 2 different hyperparameters for the neural network. We selected 2 FQE hyperparameters for each task and picked 2 checkpoints (one early, one late) to obtain 4 estimators to build the OPE ensemble.

### Baseline Ensemble OPE Methods

We compare to using single OPE estimators as well as two new baseline algorithms that combine OPE estimates together. **AvgOPE**: We can compute a simple average estimator that just outputs the average of all underlying OPE estimates. If an estimator in the ensemble outputs an arbitrarily bad value, this estimator has no implicit mechanism to ignore such an adversarial estimator. **BestOPE**: We select the OPE estimator that has the smallest estimated MSE. This estimator can be better than AvgOPE as it can ignore bad estimators. In addition, in different domains, we compare to other OPE strategies such as **BVFT** (Batch Value Function Tournament): making pairwise comparisons between different Q-function estimators with the BVFT-loss (Xie and Jiang, 2021; Zhang and Jiang, 2021). **SLOPE**: an estimator selection method that based on Lepski's method, assuming the estimators forming an order of decreasing variance and increasing bias (Yuan et al., 2021). **DR** (Doubly Robust): a semi-parametric estimator that combines the **IS** estimator and **FQE** estimator to have an unbiased

   Sepsis & N & OPERA & IS & WIS & FQE \\  MDP & 200 & **0.2205** & 0.2753 & 0.2998 & 0,2448 \\ MDP & 1000 & **0.1705** & 0.1720 & 0.2948 & 0.2995 \\  POMDP & 200 & **0.2750** & 0.2804 & 0.2850 & 0.3931 \\ POMDP & 1000 & **0.2749** & 0.2799 & 0.3092 & 0.4078 \\   

Table 1: We report the Mean-Squared Error (MSE) for the Sepsis domain. Each number is averaged across 20 trials. We underscore the estimator that has the lowest MSE in the ensemble.

Figure 2: Left: Results for contextual bandits. (a) MSE of estimators when the dataset size grows. (b) CDF of normalized MSE across 180 conditions by the worst MSE of that condition. Better methods lie in the top-left quadrant. Right: (c) For an MDP domain (Sepsis), we show that as dataset sizes increase, our bootstrap estimation of MSE approaches true MSE for each OPE estimator.

low variance estimator (Jiang and Li, 2016; Gottesman et al., 2019; Farajtabar et al., 2018). All of these methods place explicit constraints on the type of OPE estimator to include.

### Results

Contextual BanditWe report the result in Figure 2. Figure 1(a) shows that as the dataset size grows, the bootstrapping procedure employed by OPERA can quickly estimate the performance each estimator and compute a weighted score that is better than a single estimator. In the ultra-small data regime, OPERA is worse than single-estimator selection style algorithms, mainly because OPERA does not explicitly reject estimators. We can add an additional procedure to reject bad estimators and then combine the rest with OPERA, using a rejection algorithm by Lee et al. (2022).

SepsisWe report the results in Table 1. In this domain, OPERA is able to produce an estimate, on average, across many policies with different degrees of optimality, that matches and exceeds the best estimator in the ensemble. Even though in three out of four tasks, OPERA MSE is close to the MSE of the best estimator in the ensemble, in the MDP (N=200) setup, OPERA is able to get a significantly lower MSE than any of the estimators in the ensemble, suggesting a future direction of carefully choosing a set of weak estimators to put in the ensemble to obtain a strong estimator.

GraphWe report the graph domain result in Appendix A.9 and in Table 5. We find a similar result to the Sepsis domain. OPERA is able to outperform AvgOPE and BestOPE in different setups.

D4RLWe report the results in Table 2. We choose this domain because, in continuous control tasks, the horizon is often very long. Many OPE estimators that rely on short-horizon or discrete actions will not be able to extend to this domain. A popular OPE choice is FQE with function approximation, but it is difficult to determine hyperparameters like early stopping, network architecture, and learning rate. We can see that even though FQE used in D4RL is not a consistent estimator and does not satisfy OPERA's theoretical assumption, we are still able to combine the estimations to reach an aggregate estimate with lower MSE.

## 6 Discussion: Different MSE Estimation Strategies

### Estimating MSE with MAGIC

Part of our algorithm implicitly involves estimating the MSE of each OPE estimator. In our algorithm we do this using bootstrapping but other alternatives are possible. For example, prior work by (Thomas and Brunskill, 2016) provided a way to estimate the bias and variance of an OPE estimator are computed through per-trajectory OPE scores and used this as part of their MAGIC estimator. However, this method cannot estimate the MSE of self-normalizing estimators (such as WIS) or minimax-style estimators (such as any estimator in the DICE family (Yang et al., 2020)). We denote this estimator as \(}_{}(V^{})\) and now explore how our approach of using boostrapping compares to this method in an illustrative setting.

In particular, we consider estimating the MSE of the FQE and IS estimands on the Sepsis-POMDP and Sepsis-MDP domains. MAGIC estimates the bias of an OPE as the distance between the OPE

    &  &  \\  Env/Dataset & OPERA & BestOPE & AvgOPE & BVFT & DR & Dual-DICE & MB \\ 
**Hopper** & & & & & & & \\  medium-replay & **13.0** & 15.5 & 60.7 & 61.2 & 112.7 & 1565.2 & 298.7 \\ medium & **8.5** & 12.5 & 120.8 & 16.4 & 16.5 & 368.58 & 269.7 \\ 
**HalfCheetah** & & & & & & & \\  medium-replay & **46.0** & 65.0 & 218.6 & 140.2 & 119.5 & 567.9 & 750.9 \\ medium & **100.5** & 111.8 & 262.1 & 166.6 & 145.2 & 3450.0 & 589.9 \\ 
**Walker2d** & & & & & & & \\  medium-replay & **138.3** & 167.4 & 187.2 & 221.5 & 155.3 & 2124.3 & 316.8 \\ medium & **149.0** & 183.8 & 859.4 & 264.1 & 232.1 & 1756.4 & 1269.3 \\   

Table 2: Root Mean-Squared Error (RMSE) of different OPE algorithms across D4RL tasks.

value and the closest upper or lower bound of a weighted importance sampling (WIS) policy estimate. We use a percentile bootstrap to construct a 50% confidence interval CI around WIS.

Our bootstrap \(}(V^{})\) procedure is able to provide a consistently more accurate estimate of the true MSE of the FQE estimate compared to \(}_{}(V^{})\) and a comparable or better one for the IS estimate (see Table 3). We suspect that this is due to MAGIC's unique way of computing bias. Specifically, MAGIC computes bias by comparing two estimates (in this case, FQE and the upper/lower bounds on WIS) which may significantly misestimate the bias in some situations.

### Variants of OPERA with Different Strategies

We now explore two alternative strategies to estimate the MSE of each estimator. The first strategy is, instead of using the estimator's own score as the centering variable \(}\), we use a consistent and unbiased estimator's score as \(}\). We call this OPERA-IS. Another strategy is to use the idea from (Thomas and Brunskill, 2016)'s MAGIC algorithm, where the bias estimate of each estimator compares the estimand to the upper or lower confidence bound of a weighted importance sampling estimator, as above. We call this OPERA-MAGIC. These are two new variants of our OPERA algorithm that will may lead to learning different \(\) weights and producing different linearly stacked estimates. We use these two new methods, and compute the true MSE of the resulting stacked estimate, compared to OPERA and other baseline estimates. We use the Sepsis domains to illustrate the results and use as input IS, WIS and FQE OPE estimates.

The true MSE of the resulting estimates are presented in Table 4. While using an unbiased consistent estimator as the centering variable can help further improve OPERA's estimate, sometimes it also hurts the performance (MDP N=1000 setting). OPERA-MAGIC however almost always performs worse than the best estimator in the ensemble. This suggests that when combining OPE scores this bound on the bias, which will provide a distorted estimate of the estimator bias especially in low data regimes, can lead to learning less effective weightings of the input OPE estimands. OPERA remains a solid option across all settings presented in the table.

## 7 Conclusion

We propose a novel offline policy evaluation algorithm, OPERA, that leverages ideas from stack generalization to combine many OPE estimators to produce a single estimate that achieves a lower MSE. Though such stacked generalization / meta-learning has been frequently used to create better estimates from ensembles of input methods in supervised learning, to our knowledge this is the first time it has been explored in offline reinforcement learning. One challenge is that unlike in supervised learning, we do not have ground truth labels for offline policy learning. OPERA uses bootstrapping to estimate the MSE for each OPE estimator in order to find a set of weights to blend each OPE's estimate. We provide a finite sample analysis of OPERA's performance under mild assumptions, and demonstrate that OPERA provides notably more accurate offline policy evaluation

   Sepsis & N & OPERA & OPERA-IS & OPERA-MAGIC & IS & WIS & FQE \\  MDP & 200 & 0.2205 & **0.2181** & 0.2657 & 0.2753 & 0.2998 & 0.2448 \\ MDP & 1000 & **0.1705** & 0.1779 & 0.1848 & 0.1720 & 0.2948 & 0.2995 \\ POMDP & 200 & **0.2750** & 0.2768 & 0.2827 & 0.2804 & 0.2850 & 0.3931 \\ POMDP & 1000 & 0.2749 & **0.2720** & 0.2802 & 0.2799 & 0.3092 & 0.4078 \\   

Table 4: We report the Mean-Squared Error (MSE) for the Sepsis domain. We additionally present two variants of OPERA where we experimented with different MSE estimation strategies.

    &  &  \\  & \((V^{})\) & \(}_{}(V^{})\) & \(}(V^{})\) & \((V^{})\) & \(}_{}(V^{})\) & \(}(V^{})\) \\  IS & 0.0161 & 0.0281 & **0.0088** & 0.3445 & **0.0485** & 0.0056 \\ FQE & 0.0979 & 0.4953 & **0.0163** & 0.0077 & 0.0771 & **0.0011** \\   

Table 3: We compare two styles of MSE estimations and how well they can estimate the true MSE of each estimator. We report averaged results over 10 trials, with N=200.

estimates compared to prior methods in benchmark bandit tasks and offline RL tasks, including a Sepsis simulator and the D4RL settings. There are many interesting directions for future work, including using more complicated meta-aggregators.