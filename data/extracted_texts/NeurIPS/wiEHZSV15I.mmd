# Parsimony or Capability? Decomposition Delivers

Both in Long-term Time Series Forecasting

 Jinliang Deng\({}^{1,2}\)   Feiyang Ye\({}^{3}\)   Du Yin\({}^{4}\)   Xuan Song\({}^{6,2}\)1   Ivor Tsang\({}^{5}\)   Hui Xiong\({}^{7,8}\)1

\({}^{1}\) Hong Kong Generative AI Research and Development Center

\({}^{2}\) Research Institute of Trustworthy Autonomous Systems,

Southern University of Science and Technology (SUSTech),

\({}^{3}\) University of Technology Sydney,   \({}^{4}\) University of New South Wales,

\({}^{5}\) CFAR and IHPC, Agency for Science, Technology and Research, Singapore,

\({}^{6}\) School of Artificial Intelligence, Jilin University,

\({}^{7}\) AI Thrust, The Hong Kong University of Science and Technology (Guangzhou),

\({}^{8}\) CSE, The Hong Kong University of Science and Technology

{dengjinliang, xionghui}@ust.hk, feiyang.ye.uts@gmail.com

du.yin@unsw.edu.au, songxuan@jlu.edu.cn, Ivor_Tsang@cfar.a-star.edu.sg

###### Abstract

Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, characterized by extensive input sequences, as opposed to the shorter spans typical of traditional approaches. While longer sequences inherently offer richer information for enhanced predictive precision, prevailing studies often respond by escalating model complexity. These intricate models can inflate into millions of parameters, resulting in prohibitive parameter scales. Our study demonstrates, through both analytical and empirical evidence, that decomposition is key to containing excessive model inflation while achieving uniformly superior and robust results across various datasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of time series data, our proposed model outperforms existing benchmarks, using over 99% fewer parameters than the majority of competing methods. Through this work, we aim to unleash the power of a restricted set of parameters by capitalizing on domain characteristics--a timely reminder that in the realm of LTSF, bigger is not invariably better. The code is available at https://github.com/JLDeng/SSCNN.

## 1 Introduction

Time series forecasting is a cornerstone in the fields of data mining, machine learning, and statistics, with wide-ranging applications in finance, meteorology, city management, telecommunications, and beyond (Jiang et al., 2021; Han et al., 2023; Zhang et al., 2020; Wu et al., 2020, 2019; Cui et al., 2023; Zhang et al., 2017; Liang et al., 2018; Zhu et al., 2024; Fan et al., 2022, 2023). Traditional univariate time series models, such as Auto-Regressive Integrated Moving Average (ARIMA) and Exponential Smoothing, fail to capture the intricate complexities present in open, dynamic systems. Fortunately, the advent of deep learning has marked a significant shift in this domain. Recently, the utilization of the Transformer (Vaswani et al., 2017) model has revolutionized time series forecasting, setting new benchmarks in the accuracy of forecasting models due to its capability of depicting intricate pairwise dependencies and extracting multi-level representations from sequences.

Inspired by the extraordinary power exhibited by large language models (LLMs), expanding model scale has increasingly become the dominant direction in the pursuit of improvement for time seriesforecasting. Currently, the majority of advanced models require millions of parameters (Nie et al., 2023; Zhang and Yan, 2023; Wu et al., 2023). With the recent introduction of pre-trained large language models, such as LLaMa and GPT-2, the parameter scale has inflated to billions (Jin et al., 2024; Cao et al., 2024; Jia et al., 2024; Zhou et al., 2023; Gruver et al., 2024). Despite the significant increase in the number of parameters to levels comparable to foundational models for language and image, the efficacy of these models has seen only marginal improvements. In particular, these large models have shown up to only a 30% improvement in MSE and MAE, yet at the cost of 100 to 1000 times more parameters compared to a simple linear model across representative tasks (Nie et al., 2023; Cao et al., 2024; Jin et al., 2024). Additionally, we have observed a convergence in the abilities demonstrated by the models since the advent of PatchTST (Nie et al., 2023), with recent advancements achieving only incremental improvements.

These evidences indicate that a large model may not necessarily be a prerequisite for the future of time series forecasting, motivating us to explore the opposite trend--minimizing the number of parameters. Before introducing our design, we reflect on why existing methods struggle to maintain their optimal effectiveness with a reduced number of parameters. Existing methods predominantly adopt data patching over either the temporal or spatial dimensions (Zhou et al., 2021; Wu et al., 2021; Zhou et al., 2022; Nie et al., 2023; Liu et al., 2024; Zhang and Yan, 2023), which, in conjunction with the attention mechanism, allows them to capture complex temporal and spatial dependencies. However, a significant drawback of data patching is the elimination of temporal (or spatial) identities along with the destruction of temporal (or spatial) correlations, resulting in the potential loss of complex temporal (or spatial) information. To counteract this undesirable information loss, these methods establish a high-dimensional latent space to accommodate the encodings of the temporal and spatial identities in addition to the embeddings of the real-time observations. As a result, the dimensionality of the latent space typically scales with the number of identities to be encoded, inevitably leading to the exponential inflation of parameter scale. Moreover, the expansion of model size makes the models prone to overfitting, a common challenge in time series tasks where data is often limited.

To achieve a capable yet parsimonious model, it is fundamentally paramount to reinvent the paradigm to maintain and harness the spatial and temporal regularities, eliminating unnecessary and redundant parameters for encoding them into the latent space. Recent studies (Deng et al., 2021, 2024; Wang et al., 2024) showcase the potential of feature decomposition in attaining improved efficacy with limited parameters. While remarkable progress has been made, these methods struggle with long-term forecasting, especially for datasets exhibiting intricate temporal and spatial correlations, such as the Traffic and Electricity datasets (Deng et al., 2024). Moreover, the analytical aspect of decomposition along with its relation to patching is under-explored, hindering further advancements in this line of research.

In response to these limitations, we propose a Selective Structured Components-based Neural Network (SSCNN). For the first time, we address the analytical gap in feature decomposition, providing insights into its rationale for capability and parsimony compared to patching. In addition, SSCNN enhances plain feature decomposition with a selection mechanism, enabling the model to distinguish fine-grained dependencies across individual time steps, which is crucial for improving the accuracy of the decomposed structured components and, ultimately, the overall prediction accuracy. SSCNN has been benchmarked against state-of-the-art (SOTA) methods, demonstrating consistent improvements ranging from 2% to 10% in efficacy while using 99% fewer parameters than SOTA LTSF methods, including PatchTST (Nie et al., 2023) and iTransformer (Liu et al., 2024). Remarkably, it uses 87% fewer parameters than DLinear (Zeng et al., 2023) when tasked with extensive long-term forecasting. Our contributions can be summarized as follows:

1. We introduce SSCNN, a decomposition-based model innovatively enhanced with a selection mechanism. This model is specifically designed to adeptly capture complex regularities in data while maintaining a minimal parameter scale.
2. We conduct an in-depth comparison between decomposition and patching, examining both capability and parsimony.
3. We carry out comprehensive experiments to demonstrate SSCNN's superior performance across various dimensions. These extensive evaluations not only prove its effectiveness but also highlight its versatility in handling diverse time series forecasting scenarios.

Related Work

The field of time series forecasting or spatial-temporal prediction has traditionally leveraged multi-layer perceptrons (MLPs) (Zhang et al., 2017), recurrent neural networks (RNNs) (BAI et al., 2020; Zhao et al., 2019; Jiang et al., 2023; Jia et al., 2024b), graph convolution networks (GCNs) (Yu et al., 2018), and temporal convolution networks (TCNs) (Bai et al., 2018). The recent development of ST-Norm (Deng et al., 2021), STID (Shao et al., 2022a) and STAEformer (Liu et al., 2023) shows promise in enhancing model capabilities to distinguish spatial and temporal features more effectively. Motivated by the success of self-supervised learning and pre-training in natural language processing (NLP) and computer vision (CV), these two techniques are also gaining attention and application in this field (Guo et al., 2021; Shao et al., 2022b).

Over the last few years, the focus has shifted to long-term sequence forecasting (LTSF). The majority of studies have concentrated on adapting the Transformer (Vaswani et al., 2017), successful in NLP (Devlin et al., 2018) and CV (Khan et al., 2022), for LTSF tasks. Pioneering works like LogTrans (Li et al., 2019) addressed the computational challenges of long sequences through sparse attention mechanisms. Subsequent developments, such as Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), and Fedformer (Zhou et al., 2022), introduced innovative approaches to improve predictive accuracy with temporal feature characterization, autocorrelation-based series similarities, and frequency domain conversions, respectively. Other notable contributions include the Non-stationary Transformer (Liu et al., 2022) and Triformer (Cirstea et al., 2022). Furthermore, diverse normalization techniques have been developed to mitigate the distribution shift present in time series data (Liu et al., 2024b; Kim et al., 2021). Probabilistic forecasting (Kollovieh et al., 2024) and irregular time series forecasting (Chen et al., 2024; Ansari et al., 2023) are two growing subfields receiving increasing attention.

A significant shift in LTSF research occurred with DLinear (Zeng et al., 2023), an embarrassingly simple linear model. DLinear highlighted the limitation of Transformers in capturing the unique ordering information of time series data (Zeng et al., 2023). To overcome this limitation, recent methods like PatchTST (Nie et al., 2023), TimesNet (Wu et al., 2023), Crossformer (Zhang and Yan, 2023), and iTransformer (Liu et al., 2024a) blend the global dependency capabilities of Transformers with the local order modeling strengths of MLPs. However, the success of these methods is achieved at the cost of an enormous number of parameters.

A handful of emerging studies have sought to reduce parameter usage in pursuit of a parsimonious model (Lin et al., 2024; Xu et al., 2024; Wang et al., 2024; Deng et al., 2024). The techniques they employed to remove redundancies fall into three categories: downsampling (Lin et al., 2024), decomposition (Deng et al., 2024; Wang et al., 2024), and Fourier transform (Xu et al., 2024; Yi et al., 2024). Despite efficient parameter usage, these methods often compromise accuracy for specific datasets, especially those presenting complex yet predictable patterns, such as Traffic (Deng et al., 2024; Xu et al., 2024). _Our study, as far as we know, is the first to realize a parsimonious model without any sacrifice of capability._

## 3 Selective Structured Components-based Neural Network

In multivariate time series forecasting, given historical observations \(=\{_{1},,_{N}\}^{N T_ {}}\) with \(N\) variates and \(T_{}\) time steps, we predict the future \(T_{}\) time steps \(}^{N T_{}}\). The input data goes through the processing, visualized in Fig. 1, for predicting the unknown, future data. Over the course of prediction, a sequence of intermediate representations are yielded. Essentially, SSCNN is structured into two distinct branches: the top branch illustrates the inference process used to derive the components accompanied by the residuals, while the bottom branch depicts the extrapolation process, forecasting the potential evolution of these components. The components and residuals obtained are combined into a wide vector, which is then input into a polynomial regression layer to capture their complex interrelations. In the following sections, we detail the inference and extrapolation processes for these components, respectively.

### Temporal Component

We invent temporal attention-based normalization (T-AttnNorm) to decompose the temporal components, consisting of the long-term component, the seasonal component, and the short-term component,in a sequential manner. The inference for each of these three components is applied to the representation of each series individually along the temporal dimension, with a selection/attention map characterizing the dynamics of the component of interest. Upon inference, our model disentangles the derived structured component from the data, resulting in a residual term summarizing other remaining components. The initial representation of the time series, the derived structured component, and the residual term are denoted as \(^{*},^{*},^{*}^{N T_{}  d}\), respectively. The selection map is denoted as \(^{*}^{T_{} T_{}}\). T-AttnNorm is formulated as follows:

\[^{*}_{i},^{*}_{i}=(^{*}_{i};^ {*}),\] (1)

where \(^{*}_{i}=^{*}^{*}_{i},\ ^{*2}_{i}=^{*} ^{*2}_{i}-{^{*}_{i}}^{2}+,\ ^{*}_{i}=^{*}_{i}- ^{*}_{i}}{^{*}_{i}}\). To ensure the unbiasedness of \(^{*}_{i}\) and \(^{*}_{i}\), the sum of each row of \(^{*}\) is constrained to 1. The distinction between the three components is the realization of \(^{*}\). The residual term resulting from the current block is taken as the input for the subsequent block, e.g., \(^{}=^{}\).

Then, the component series and the residual series are respectively extrapolated to forward horizons by a mapping parameterized with \(^{*}^{T_{} T_{}}\):

\[^{*}_{i},}^{*}_{i}=(^{*}_{i}, ^{*}_{i};^{*}),\] (2)

where \(^{*}_{i}=^{*}^{*}_{i}\), \(}^{*}_{i}=^{*}^{*}_{i}\), with \(^{*}_{i},}^{*}_{i}^{T_{}  d}\), leading to \(}^{*}\), \(^{*}^{N T d}\). Similar to \(^{*}\), \(^{*}\) is also defined as a matrix with the sum of each row equal to 1. Next, we introduce how to realize \(^{*}\) and \(^{*}\) to extract and simulate the dynamics of the considered components, respectively.

Long-Term ComponentThe long-term component aims to characterize the trend patterns of the time series data. To acquire an estimation of the long-term component with less bias, we aggregate the samples collected across multiple seasons, eliminating the seasonal and short-term impacts that impose only local effects. The realizations of the inference and extrapolation matrices for the

Figure 1: An overview of the SSCNN. The grids are used to exemplify the selection maps \(^{*}\) and \(^{*}\) as defined in the main text, with \(T_{}\), \(T_{}\) and \(N\) instantiated as 4, 4 and 3, respectively.

long-term component are given by:

\[^{}(i,j)=}},\ ^{}(i,j)= }}.\] (3)

The attention mechanism is excluded from the long-term component as it does not improve forecasting accuracy in our datasets. The attention mechanism is beneficial when a component's distribution changes significantly over time, helping to reduce estimation bias. However, for our long-term component, the distribution remains stable throughout the input period. In such cases, the attention mechanism adds no value, making it redundant and unnecessary.

Seasonal ComponentThe seasonal component is created to model the seasonal fluctuation. Inference of the seasonal component operates under the assumption of a consistent cycle duration to streamline the detection of seasonal trends. We introduce \(c\) to indicate the length of a cycle, \(_{}\) to signify the maximal count of cycles encompassed by the input sequence, i.e., \(_{}c T_{}\), and \(_{}\) to denote the minimal number of cycles covering the output sequence, i.e., \(_{}c T_{}\). To simplify the notation, we assume that \(T_{}\) is a multiple of \(c\), i.e., \(T_{}=_{} c\).

To acquire unbiased and precise seasonal components, we introduce a parameter matrix \(^{}^{_{}_{}}\), allocating distinct weights to each pair of periods to represent the inter-cycle correlations. The matrix undergoes row-wise normalization via a softmax operation to satisfy the constraint on the sum of 1. The selection map for inferring the seasonal component is defined as:

\[^{}(i,j)=\{^ {}_{u,k})}{_{k=0}^{_{}-1}(^{} _{u,k})}&u=,v=,i-j 0 \\ 0&.,\] (4)

where \(\) denotes the floor function.

When dealing with extrapolation, we define \(}^{}^{_{}_{ }}\) as the parameter matrix capturing the correlations between each pair of cycles encompassed by the input and output sequences, respectively. The selection map for extrapolating the seasonal component is written as follows:

\[^{}(i,j)=\{^ {}_{u,k})}{_{k=0}^{_{}-1}(}^{ }_{u,k})}&u=,v=,i-j  0\\ 0&..\] (5)

Short-Term ComponentThe short-term component discerns irregularities and ephemeral phenomena unaccounted for by the seasonal and long-term components. In contrast to the long-term component, it necessitates only a limited window size, \(\), encapsulating recent observations with immediate relevance. Moreover, these observations exhibit varying degrees of correlation depending on the associated lag. Therefore, the inference of the short-term component involves a parameter vector \(^{}^{}\), and is expressed mathematically as:

\[^{}(i,j)=\{^ {}_{i})}{_{k=0}^{-1}(^{}_{i})}&(i-j >=0)(i-j<)\\ 0&..\] (6)

The extrapolation of the short-term component bifurcates based on the targeted horizon. Immediate horizons retain correlations with preceding estimations of the short-term component, prompting regression-based forecasting with a parameter vector \(}^{}^{}\). Conversely, as the horizon extends ahead, it accumulates compounded uncertainties, decreasing the predictability. Herein, we opt for zero-padding to eliminate unnecessary parameters. The entire extrapolation is formalized as follows:

\[^{}(i,j)=\{}^{}_{i,j})}{_{k=0}^{-1}(}^{ }_{i,k})}.&(i<)(j>T_{}--1)\\ 0&..\] (7)

### Spatial Component

The spatial component refers to the component that is temporally irregular, i.e., cannot be captured by the aforementioned three temporal components, but spatially regular, i.e., showing consistent behavior across a group of residual series. The inference of the spatial component, referred to as spatial attention-based normalization (S-AttnNorm), shares a similar formula representation as the temporal component, except that it is applied to each frame independently along the spatial dimension:

\[^{*}_{:,t},^{*}_{:,t}=(^{*}_{:,t};^{*}),\] (8)

where \(^{*}_{:,t}=^{*}^{*}_{:,t},~{}{^{*}_{:,t}}^{2}= ^{*}^{*}_{:,t}-{^{*}_{:,t}}^{2}+,~{} ^{*}_{:,t}=^{*}_{:,t}-{^{*}_{:,t}}}{^{*}_{:,t}}\). We acquire the similarities among the series in terms of the spatial component by applying correlation to the residuals post controlling the long-term, seasonal, and short-term components. Given that each series is represented by a \(T_{} d\) matrix in the hidden layer, we vectorize them before performing the measurement. This results in a similarity matrix \(^{}^{N N}\), with each entry \(^{}(i,j)\) representing the conditional correlation between the \(i^{}\) series and \(j^{}\) series:

\[^{}(i,j)=(^{}_{i})^{}(^{}_{j}))}{_{k=1}^{N} ((^{}_{i})^{} (^{}_{k}))}.\] (9)

Considering the erratic and unpredictable nature of the spatial component along time, we simply realize component extrapolation with zero-padding: \(=\) and \(}=\).

### Component Fusion

The Polynomial Regression layer is inspired by the work of Deng et al. (2024), where we extend the original module to include both additive and multiplicative relations. This extension allows us to model more complex interactions between the decomposed components.

\[_{i}=_{k=1}(_{k}(_{i}) _{k}(_{i}))+_{k}(_{i}),\] (10)

where

\[_{i}=[^{}_{i},^{}_{i},^{}_{i},^{}_{i},^{}_{i},^{}_ {i},^{}_{i},^{}_{i}].\]

The resulting \(_{i}\) is fed to the next layer as \(^{}_{i}\) for further manipulation.

## 4 Comparison Between Decomposition and Patching

Capability AnalysisWe first demonstrate that the representation space accommodating only a plain single-step observation is ill-structured due to the entanglement of diverse component signals. Then, we elucidate how patching and decomposition adjust the structure of the representation space, facilitating the capture of faithful and reliable relations among the samples. The detailed derivations for this section are provided in Appendix A.

For simplicity, we assume that time series \(x\) is driven by two distinct components \(a\) and \(b\), but the analysis can be trivially extended to cases with multiple components. Consider a triplet \((x_{t_{1}},x_{t_{2}},x_{t_{3}})\) collected at three time steps \(t_{1}\), \(t_{2}\), and \(t_{3}\), respectively, where \(x_{t_{i}}=a_{t_{i}}+b_{t_{i}}\) for \(i=1,2,3\), subject to \(a_{t_{1}}=a_{t_{2}}\), \(b_{t_{2}}=b_{t_{3}}\), and \(a_{t_{1}} a_{t_{3}}\). We expect that \(x_{t_{1}}\) should be closer to \(x_{t_{2}}\) than \(x_{t_{3}}\), given \(\|a_{t_{1}}-a_{t_{2}}\|<\|a_{t_{1}}-a_{t_{3}}\|\) along with \(\|b_{t_{1}}-b_{t_{2}}\|=\|b_{t_{1}}-b_{t_{3}}\|\). However, this relationship may not necessarily hold with step-wise observations, i.e., there exist specified triplets \((_{t_{1}},_{t_{2}},_{t_{3}})\) such that \(\|_{t_{1}}-_{t_{2}}\|>\|_{t_{1}}-_{t_{3}}\|\), due to the entanglement of \(a\) and \(b\).

Patching can rectify sample relations by creating orthogonality between distinct components, resulting in a well-structured representation space. By incrementally augmenting the time series representation with preceding observations, i.e., \([x_{t-p},,x_{t}]\), the \(a\)-component (\([a_{t-p},,a_{t}]\)) and the \(b\)-component (\([b_{t-p},,b_{t}]\)) become increasingly orthogonal, reducing negative interference caused by their interactions. It is worth noting that while patching is widely acknowledged for enriching the semantic information of step-wise data (Nie et al., 2023), no formal explanation from the perspective of disentanglement has been offered yet.

Distinct from patching, which implicitly attenuates the interactions among distinct components, decomposition can explicitly avert any interactions. This is done by first recovering and subsequently distributing different components into disjoint sets of dimensions, completely isolating them in independent subspaces. Specifically, this involves finding a decomposition mapping\((a_{t_{i}},b_{t_{i}})^{}\) such that \(\|D(x_{t_{1}})-D(x_{t_{2}})\|\|D(x_{t_{1}})-D(x_{t_{3}})\|\). The effectiveness of decomposition depends on how accurately the model can recover \(a\) and \(b\). Thus, it is paramount to ensure that no irrelevant information is included in estimating the component of interest, suggesting the necessity of using a selection mechanism. The above analysis is based on deterministic data, but similar results can be obtained if the data possesses stochasticity, which is the case for real-world applications. See detailed discussion in Appendix A.

Parsimony AnalysisTo justify that SSCNN is fundamentally more parameter-efficient than patching-based methods, we analyze how the number of required parameters scales with \(T_{}\), \(T_{}\), and \(d_{}/d_{}\), where \(d_{}\) and \(d_{}\) represent the dimensionalities of the representation spaces for patching-based and decomposition-based methods, respectively.

Patching-based methods establish intense connections from backward variables to forward variables via multiple layers of patch-wise fully-connected networks, typically necessitating \((d_{}T_{}T_{}+d_{}^{2})\) parameters (Nie et al., 2023). Empirically, the optimal performance of these models is achieved when \(d_{}\) is adjusted within the range of 64 and 256, resulting in an explosion in the total number of parameters, especially for large \(T_{}\) and \(T_{}\).

In the case of SSCNN, the portion scaling with \(T_{}\), \(T_{}\), and \(d_{}\) contains \((d_{}^{2}+}T_{}T_{}+ }T_{}^{2}+d_{}T_{})\) parameters. These come from three sources: the convolution operators in the polynomial regression module, the attention map responsible for characterizing seasonal patterns, and the predictor. In contrast to \(d_{}\), \(d_{}\) can be assigned a small number, e.g., 8, which is sufficient to prompt the model to exhibit its full potential, making \(d_{}^{2}\) orders of magnitude smaller than \(d_{}^{2}\). Additionally, SSCNN significantly reduces the number of connections scaling with \(T_{}\) and \(T_{}\) from \(d_{}T_{}T_{}\) to \(}T_{}T_{}+}T_{}^{2}+d _{}T_{}\), thanks to the capability of decomposition in eliminating redundant correlations across variables, as further analyzed in Appendix D. The presence of the scaling factor \(}\) makes SSCNN even more parameter-efficient than DLinear (Zeng et al., 2023) for large \(T_{}\) and \(T_{}\), when the sum of terms related to \(T_{}\) and \(T_{}\) dominates \((d_{}^{2})\). The actual results are presented in Sec. 5.2.

## 5 Evaluations

In this section, we conduct comprehensive experiments across standard datasets to substantiate from various perspectives that SSCNN achieves SOTA performance with a minimal parameter count.

### Experiment Setting

DatasetsWe evaluate the performance of our proposed SSCNN on seven popular datasets with diverse regularities, including Weather, Traffic, Electricity, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). Among these seven datasets, Traffic and Electricity consistently show more regular patterns over time, while the rest contain more volatile data. Detailed dataset descriptions and data processing are provided in Appendix B.

Evaluation MetricsIn line with established practices in LTSF (Nie et al., 2023; Wu et al., 2021), we evaluate the model performance using mean squared error (MSE) and mean absolute error (MAE).

Baseline ModelsWe compare SSCNN with the state-of-the-art models, including (1) Transformer-based methods: Autoformer (Wu et al., 2021), Crossformer (Zhang and Yan, 2023), PatchTST (Nie et al., 2023) and iTransformer Liu et al. (2024a), (2) Linear-based method: DLinear Zeng et al. (2023); (3) TCN-based method: TimesNet Wu et al. (2023); (4) Decomposition-based method: TimeMixer (Wang et al., 2024), SCNN (Deng et al., 2024). For implementing state-of-the-art models (SOTAs), we adhere to the default settings as provided in the Time-Series-Library 2.

Network SettingAll the experiments are implemented in PyTorch and conducted on a single NVIDIA 2080Ti 11GB GPU. For ECL and Traffic, SSCNN is configured with 4 layers, and the number of hidden channels \(d\) is set to 8. We set \(\) to a value from \(\{2,4,8,16\}\). The kernel size for the convolution used in polynomial regression is chosen as 2. The total number of parameters resulting from this configuration is around **25,000**. For the other five datasets showing unstable and volatile patterns, the spatial component is found to be useless due to the absence of spatial correlations, thus this component is disabled. Additionally, the number of layers is configured as 2, resulting in around **5,000** parameters. During the training phase, we employ L2 loss for model optimization. In addition, the batch size is set to 8, and an Adam optimizer is used with a learning rate of 0.0005.

### Comparative Analysis with Baselines

Forecasting AccuracyAs reported in Table 1, for Electricity and Traffic, SSCNN showcased remarkable proficiency by achieving the lowest MSE and MAE. Specifically, on the Electricity dataset, SSCNN outperformed iTransformer and PatchTST, two representative baselines, by 2% and 4%, respectively. On the Traffic dataset, SSCNN again surpassed them by 2% and 3%, respectively. By comparing SSCNN to SCNN, the benefit of the selection mechanism is evident, leading to approximate 8% improvement, highlighting the necessity of modeling fine-grained correlations for complex yet regular patterns. On the other five datasets, we find that the attention mechanism benefits little, resulting in comparable performance between SSCNN and SCNN. When it comes to the comparative analysis with three leading Transformer-based models, including iTransformer, PatchTST, and Crossformer, SSCNN exhibits notable improvements by 8.8%, 4.4%, and 8.3%, respectively, on average. This suggests the competitiveness of SSCNN in handling irregular dynamics.

Parameter ScaleParameter scale is represented by the number of parameters. This metric varies with the lookback window size and the forward window size for all models, as illustrated in Fig. 1(a) and Fig. 1(c), respectively. Apparently, SSCNN and DLinear emerge as the top two parameter-efficient models, requiring **99%** fewer parameters than other models. Remarkably, SSCNN proves to be even more parsimonious than DLinear when tasked with extensively long-term forecasting, showing a considerable reduction of **87%** in parameter scale at the forward window size of 336.

Computation ScaleComputation scale is measured using the number of floating point operations (FLOPs), as illustrated in Fig. 1(b) and Fig. 1(d), respectively. SSCNN falls short in computation scale compared to iTransformer. This is because, instead of performing compression using fully connected layers, it manages and processes the entire sequence at each layer. As a result, the computational

   & SSCNN & SCNN &  & TimeMixer &  &  &  &  &  \\  & (Ours) & (2024) & (2024) & (2024) & (2023) & (2023) & (2023) & (2023) & (2023) & (2023) & (2021) \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  ECL & **0.118** & **0.212** & 0.128 & 0.222 & 0.212 & 0.1216 & 0.123 & 0.125 & 0.129 & 0.163 & 0.267 & 0.142 & 0.141 & 0.236 & 0.191 & 0.308 \\ Traffic & **0.338** & **0.235** & 0.360 & 0.453 & 0.343 & 0.246 & 0.387 & 0.271 & 0.349 & 0.241 & 0.579 & 0.314 & 0.375 & 0.254 & 0.426 & 0.290 & 0.583 & 0.420 \\ ETH1 & **0.330** & **0.363** & 0.339 & 0.368 & 0.359 & 0.387 & 0.348 & 0.375 & 0.335 & 0.372 & 0.399 & 0.418 & 0.349 & 0.378 & 0.368 & 0.393 & 0.452 & 0.466 \\ ETH2 & **0.255** & **0.313** & 0.257 & 0.145 & 0.274 & 0.331 & 0.264 & 0.324 & 0.264 & 0.322 & 0.304 & 0.356 & 0.289 & 0.338 & 0.282 & 0.338 & 0.365 & 0.411 \\ ETH1 & **0.242** & **0.300** & 0.244 & 0.302 & 0.261 & 0.315 & 0.258 & 0.316 & 0.248 & 0.305 & 0.271 & 0.318 & 0.272 & 0.318 & 0.259 & 0.312 & 0.464 & 0.445 \\ ETH2 & **0.158** & 0.236 & **0.158** & **0.235** & 0.175 & 0.251 & 0.164 & 0.241 & 0.167 & 0.240 & 0.180 & 0.258 & 0.168 & 0.244 & 0.167 & 0.251 & 0.225 & 0.306 \\ Weather & **0.139** & **0.175** & 0.140 & 0.178 & 0.158 & 0.190 & 0.143 & 0.182 & 0.153 & 0.184 & 0.165 & 0.206 & 0.150 & 0.194 & 0.161 & 0.209 & 0.185 & 0.231 \\  

Table 1: Multivariate forecasting results with prediction lengths S \(\) {3, 24, 96, 192, 336}. Results are averaged from all prediction lengths. Full results are listed in Appendix C. The best result is highlighted in **bold**, and the second best is highlighted with underline.

Figure 2: Examination of parameter scale and computation scale against the forward window size and the backward window size on the ECL dataset.

cost scales with the sum of the backward window and forward window sizes, raising an issue to be addressed in future work. Despite the relatively high computational cost, SSCNN has the potential to be accelerated through parallel computing.

Sensitivity to Lookback Window SizeAs shown in Fig. 3, for ECL data, although SSCNN initially lags behind other state-of-the-art models at a window size of 96, it progressively outperforms them as more historical data is included. In contrast, iTransformer and Crossformer achieve advantageous performance with shorter input ranges but exhibit diminished gains from extended historical data. Furthermore, when handling the volatile fluctuations of ETTh2 data, SSCNN demonstrates indisputably enhanced capability with respect to the lookback window size compared to competing methods, some of which even show degraded performance due to the overfitting problem.

### Comparative Analysis of Model Configurations

Analysis of Hyper-parametersAs shown in Fig. 4, the efficacy of SSCNN is impacted by adjustments to the five considered hyperparameters, showing up to a 10% difference between the best and the worst records. Focusing on the performance change against cycle length, we find that the misalignment between the configured value and the authentic value of 24 results in a significant drop in effectiveness, verifying our presumption. Additionally, the short-term span also has a non-negligible influence on the outcome.

Ablation StudyWe create seven variants: The first six variants are used to assess the individual contributions of the four components as well as the two introduced attention maps, respectively. The last variant, represented as 'w FCN', is constructed by inserting an additional fully-connected network (FCN) between backward variables and forward variables to verify the redundancy of the FCN, which is prevalently adopted by previous works (Liu et al., 2024; Nie et al., 2023) to capture temporal correlations, under the framework of SSCNN. It is evident from Fig. 5 that all these components

Figure 4: Sensitivity analysis of hyper-parameters on the ECL and Traffic datasets.

Figure 5: Performance comparison with various component in ECL and Traffic dataset.

Figure 3: Impacts of backward window size.

are vital, with the seasonal component being the most prominently important. Interestingly, our approach shows that basing dependencies on the evaluation of inter-channel conditional correlations actually enhances outcomes, contradicting the evidence delivered by Han et al. (2023b); Nie et al. (2023), where a channel-dependent strategy was seen as detrimental to model performance. Furthermore, we note that 'w FCN' does not bring any benefit, proving the sufficiency of the proposed inference and extrapolation operations for characterizing useful dynamics.

## 6 Conclusions, Limitations and Broader Impacts

In this research, we developed SSCNN, a structured component-based neural network augmented with a selection mechanism, adept at capturing complex data regularities with a reduced parameter scale. Through extensive experimentation, SSCNN has demonstrated exceptional performance and versatility, effectively handling a variety of time series forecasting scenarios and proving its efficacy.

While the proposed SSCNN model achieves satisfactory results in reducing parameter usage, it falls short in terms of computational complexity, as shown in Fig. 2b and Fig. 2d. This drawback, however, has the potential to be mitigated in future work by identifying and eliminating redundant computations with down-sampling techniques. Moreover, our future research will explore the potential for automating the process of identifying the optimal neural architecture, using these fundamental modules and operations as building blocks. This approach promises to alleviate the laborious task of manually testing various combinations in search of the optimal architecture for each new dataset encountered.

Time series forecasting can significantly benefit various sectors, including meteorology, economics, traffic management, and healthcare. However, it also poses potential negative impacts. Inaccurate forecasts can lead to significant economic or operational disruptions. Additionally, the large amounts of data required for accurate forecasting may include personal or sensitive information, raising concerns about data privacy and potential misuse.

## 7 Acknowledgment

The research was supported by Theme-based Research Scheme (T45-205/21-N) from Hong Kong RGC, Generative AI Research and Development Centre from InnoHK, and the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No.VRLAB2024A**). This work was also partially supported by the grants of National Key Research and Development Project (2021YFB1714400) of China, Jilin Provincial International Cooperation Key Laboratory for Super Smart City and Zhujiang Project (2019QN01S744). This work was supported in part by the National Key R&D Program of China (Grant No.2023YFF0725001),in part by the National Natural Science Foundation of China (Grant No.92370204), in part by the guangdong Basic and Applied Basic Research Foundation (Grant No.2023B1515120057), and in part by Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality.