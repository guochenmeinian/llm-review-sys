# A Riemannian Exponential Augmented Lagrangian Method for Computing the Projection Robust Wasserstein Distance

A Riemannian Exponential Augmented Lagrangian Method for Computing the Projection Robust Wasserstein Distance

Bo Jiang

Ministry of Education Key Laboratory of NSLSCS

School of Mathematical Sciences, Nanjing Normal University

Nanjing 210023, China

jiangbo@njnu.edu.cn

Ya-Feng Liu

State Key Laboratory of Scientific and Engineering Computing

Institute of Computational Mathematics and Scientific/Engineering Computing

Academy of Mathematics and Systems Science, Chinese Academy of Sciences

Beijing 100190, China

yafliu@lsec.cc.ac.cn

Corresponding author.

###### Abstract

Projection robust Wasserstein (PRW) distance is recently proposed to efficiently mitigate the curse of dimensionality in the classical Wasserstein distance. In this paper, by equivalently reformulating the computation of the PRW distance as an optimization problem over the Cartesian product of the Stiefel manifold and the Euclidean space with additional nonlinear inequality constraints, we propose a Riemannian exponential augmented Lagrangian method (REALM) for solving this problem. Compared with the existing Riemannian exponential penalty-based approaches, REALM can potentially avoid too small penalty parameters and exhibit more stable numerical performance. To solve the subproblems in REALM efficiently, we design an inexact Riemannian Barzilai-Borwein method with Sinkhorn iteration (iRBBS), which selects the stepsizes adaptively rather than tuning the stepsizes in efforts as done in the existing methods. We show that iRBBS can return an \(\)-stationary point of the original PRW distance problem within \((^{-3})\) iterations, which matches the best known iteration complexity result. Extensive numerical results demonstrate that our proposed methods outperform the state-of-the-art solvers for computing the PRW distance.

## 1 Introduction

The optimal transport (OT) problem has found wide applications in machine learning, representation learning, data sciences, and image sciences; see [21; 5; 42; 1; 14] and the references therein for more details. However, its direct application in machine learning may encounter the issue of the curse of dimensionality since the sample complexity of approximating the Wasserstein distance can grow exponentially in dimension [22; 49]. To resolve this issue, by making an important extension to the sliced Wasserstein distance , Paty and Cuturi , Deshpande et al. , and Niles-Weed and Rigollet  proposed to project the distributions to a low-dimensional subspace that maximizes the Wasserstein distance between the projected distribution, which can reduce the sample complexity and overcome the issue of the curse of dimensionality [40; 16; 35].

In this paper, we focus on the discrete probability measure case. For \(\{x_{1},,x_{n}\}^{d}\) and \(\{y_{1},,y_{n}\}^{d}\), define \(M_{ij}=(x_{i}-y_{j})(x_{i}-y_{j})^{}\) for each \((i,j)[n][n]\) with \([n]:=\{1, n\}\). Let \(^{n}\) be the all-one vector and \(_{x}\) be the Dirac delta function at \(x\). Given \(r=(r_{1},,r_{n})^{}^{n}:=\{z^{n} ^{}z=1,z>0\}\) and \(c=(c_{1},,c_{n})^{}^{n}\), define two discrete probability measures \(_{n}=_{i=1}^{n}r_{i}_{x_{i}}\) and \(_{n}=_{i=1}^{n}c_{i}_{y_{i}}\). For \(k[d]\), the \(k\)-dimensional projection robust Wasserstein (PRW) distance between \(_{n}\) and \(_{n}\) is defined as 

\[_{k}^{2}(_{n},_{n})=_{U}_{(r,c )},C(U),\] (1)

where \(,\) is the standard inner product in \(^{n n}\), \(C(U)^{n n}\) with \([C(U)]_{ij}= M_{ij},UU^{}\), \(=\{U^{d k} U^{}U=I_{k}\}\) is known as the Stiefel manifold with \(I_{k}\) being the \(k\)-by-\(k\) identity matrix, and \((r,c)=\{^{n n}=r,^{} =c, 0\}\). Problem (1) is a nonconcave-convex max-min problem over the Stiefel manifold, which makes it very challenging to solve.

**Related works and motivations**. To compute the PRW distance, Paty and Cuturi  proposed two algorithms for solving the subspace robust Wasserstein distance, which is a convex relaxation of problem (1) (without the theoretical guarantee on the relaxation gap). An OT or entropy-regularized OT subproblem with dimension \(n\) and a full or top \(k\) eigendecomposition of a \(d\)-by-\(d\) matrix needs to be solved exactly at each iteration. Very recently, Lin et al.  proposed a Riemannian (adaptive) gradient ascent with Sinkhorn (R(A)GAS) algorithm for solving the following entropy-regularized problem with a small regularization parameter \(\):

\[_{U}\,p_{}(U),\] (2)

where \(p_{}(U)=_{(r,c)}\{,C(U)- H ()\}\), in which \(H()=-_{ij}_{ij}_{ij}\) is the entropy function. They showed that R(A)GAS can return an \(\)-stationary point of PRW problem (1) within \((^{-4})\) iterations if \(=()\) in (2). However, at each iteration, R(A)GAS needs to solve a regularized OT problem in relatively high precision, which results in a high computational cost. To reduce the complexity of R(A)GAS, Huang et al. [28; 29] proposed a Riemannian (adaptive) block coordinate descent (R(A)BCD) algorithm for solving an equivalent "min" formulation of (2) as

\[_{U,,^{n}}\,_{}( ,^{}),\] (3)

where \(_{}(,)\) is defined in (8) further ahead. By choosing \(=()\) in (3), they showed that the whole iteration complexity of R(A)BCD to attain an \(\)-stationary point of PRW problem (1) reduces to \((^{-3})\), which significantly improves the complexity of R(A)GAS.

However, there are two main issues of R(A)BCD and R(A)GAS. First, to compute a solution of problem (1) with relatively high quality, \(\) in problem (2) or (3) has to be chosen small, which makes the corresponding problem ill-conditioned and may cause numerical instability in solving it. Second, the performance of the above algorithms is sensitive to the stepsizes in updating \(U\). Hence, to achieve a better performance, one has to spend some efforts tuning the stepsizes carefully. Resolving these two main issues demands some novel approaches from both theoretical and computational points of view, and this is the motivation and focus of our paper.

**Contributions**. In this paper, by reformulating (1) as an optimization problem defined over the Cartesian product of the Stiefel manifold and the Euclidean space with additional inequality constraints (see problem (6) further ahead), we can resolve the above-mentioned two issues. Our main contributions are summarized as follows. See also Figure 1 for a summary of the related works and the main results of this paper.

(i) We propose a Riemannian exponential augmented Lagrangian method (REALM) to efficiently and faithfully compute the PRW distance, in which a series of subproblems with dynamically decreasing penalty parameters and adaptively updated multiplier matrices are solved approximately. In theory, we establish the global convergence of REALM in the sense that any limit point of the sequence generated by the algorithm is a stationary point of the original problem; see Theorem 2.7. Numerically, REALM always outperforms the Riemannian exponential penalty approach since it could avoid too small penalty parameters in many cases.

(ii) To efficiently solve the subproblem in REALM (i.e., problem (11) further ahead), we view it as a one-block optimization problem over the Stiefel manifold and propose a novel and practical algorithm, namely, the inexact Riemannian Barzilai-Borwein (BB) method with Sinkhorn iteration (iRBBS), wherein a flexible number of Sinkhorn iterations is performed to compute the inexact Riemannian gradient. Compared with R(A)BCD, our proposed iRBBS (applied to problem (11) with fixed \(^{k}\) and \(_{k}=O()\)) can not only return a stronger \(\)-stationary point of PRW problem (1) (compared with the definitions in [34; 28]; see Remark 2.2), within \((^{-3})\) iterations (see Theorem 3.2 with \(_{1}=_{2}=\)), but also has a better numerical performance, which mainly benefits from the adaptive Riemannian BB stepsize (based on the inexact Riemannian gradient information).

_Notations._ For \(x^{n}\), \((x)\) is an \(n n\) diagonal matrix with \(x\) being its main diagonal. For a matrix \(A\), denote \(A_{}=_{ij}A_{ij}\), \(A_{}=_{ij}A_{ij},\|A\|_{1}=_{ij}|A_{ij}|\), \(\|A\|_{}=_{ij}|A_{ij}|\), \(\|A\|_{}^{2}=_{ij}A_{ij}^{2}\), and \(\|A\|_{}=A_{}-A_{}\). Denote by \(()\) and \(()\) the element-wise logarithmic and exponential operators, respectively. We use \(_{+}^{n n}\) and \(_{+}^{n n}\) to denote the nonnegative and positive orthants of \(^{n n}\), respectively. Throughout this paper, we define \(C^{n n}\) with \(C_{ij}=\|x_{i}-y_{j}\|^{2}\) and \(V_{}=_{ij}_{ij}M_{ij}\).

The tangent space at \(U\) is \(_{U}=\{^{d k} U^{} +^{}U=0\}\). Let \(=\{(U,) U\) and \(_{U}\}\) be the tangent bundle of \(\). A smooth map \(::(U,) _{U}()\) is called a retraction if each curve \((t)=_{U}(t)\) satisfies \((0)=U\) and \(^{}(0)=\); see [9; Definition 3.47] or [2, Definition 4.1.1]. The Riemannian metric \(,_{U}\) endowed on the Stiefel manifold \(\) is taken as the standard metric \(,\) on \(^{d k}\). The Riemannian gradient of a smooth function \(f:^{d k}\) at \(U\) is defined as \(\,f(U)\), which satisfies \(\,f(U),_{U}= f(U), \) for all \(_{U}\), where \( f(U)\) denotes the Euclidean gradient of \(f\) at \(U\). If \(U^{} f(U)\) is symmetric, we have \(\,f(U)=_{_{U}}( f(U))=(I _{d}-UU^{}) f(U)\).

The rest of this paper is organized as follows. The proposed REALM is introduced in Section 2. A practical iRBBS for solving the subproblem in REALM is proposed in Section 3. Numerical results are presented in Section 4. Finally, we draw some concluding remarks in Section 5.

## 2 A Riemannian Exponential ALM for Computing the PRW Distance (1)

Given a fixed \(U\), consider the OT problem

\[_{^{n n}}\ ,C(U) =r,\ ^{}=c,\  0.\] (4)

By adding a redundant constraint \(\|\|_{1}=1\)[29; 36], we derive the dual of (4) as

\[_{^{n},^{n}}-(r^{}+c^ {}+y)()_{ij}+y 0, (i,j)[n][n],\] (5)

where \(=(,,U)\) and \(()^{n n}\) with \(()_{ij}=_{i}+_{j}+ M_{ij},UU^{}\) for each \((i,j)[n][n]\). Note that the matrix \(\) in (4) can also be understood as the Lagrange multiplier corresponding to the inequalities in (5). Therefore, the value \(_{k}^{2}(_{n},_{n})\) defined in (1) is equal to the opposite of the optimal objective value of the following optimization problem:

\[_{,y}\ r^{}+c^{ }+y()_{ij}+y 0, (i,j)[n][n],\] (6)

Figure 1: A summary of the related works and the main results of this paper.

where \(=^{n}^{n}\). Motivated by the first-order necessary condition of problem (6) (see Appendix A.1 for details), we define the \((_{1},_{2})\)-stationary point of problem (1) as follows.

**Definition 2.1**.: We call \((},)(r,c)\) an \((_{1},_{2})\)-stationary point of PRW problem (1), if \(\|_{_{_{}}}(-2V_{})\|_{}_{1}\) and \(,Z(})_{2}\), where \(Z(})^{n n}\) with \(Z(})_{ij}=()_{ij}-()_{}\). If \(_{1}=_{2}=0\), we call such \((},)\) a stationary point of PRW problem (1).

_Remark 2.2_.: Our Definition 2.1 is stronger than [28, Definition 3.1] and [34, Definition 2.7] in the sense that the \((_{1},_{2})\)-stationary point satisfying the conditions here also satisfies all conditions therein. See Appendix A.2 for more details.

Given \(^{n n}_{++}\) and \(>0\), define the function \(_{}(,)^{n n}\) with

\[[_{}(,)]_{ij}=_{ij}(-)_{ij}}{})\!,\] (7)

define

\[}_{}(,y,)=r^{}+c^{ }+y+_{ij}_{ij}(-)_{ij}+y}{})\]

and

\[_{}(,):=r^{}+c^{}+ (\|_{}(,)\|_{1}).\] (8)

One natural approach for solving problem (6) is the Riemannian exponential penalty approach (where the manifold constraints are kept in the subproblem), which aims to solve the penalty subproblem

\[_{,y}}_{} (,y,^{}).\] (9)

For any fixed \(x\), letting \(_{y}}_{}(,y,^{})=0\), we can obtain the optimal \(y\) as \(y=(\|_{}(,^{})\|_{1})\). By eliminating the variable \(y\) in (9), we thus obtain the subproblem (3) of the approach in Huang et al. .

It is known that the exponential augmented Lagrangian method (ALM) is usually more stable than the exponential penalty approach; see [18, Tables 3.1-3.3] for a detailed example. More specifically, the penalty parameter in the exponential ALM can be chosen as any positive number in the convex case  or can be bounded away from zero under some standard assumptions in the general nonlinear case , which is in sharp contrast to the exponential penalty approach. Based on the aforementioned knowledge, we thus extend the exponential ALM  to the manifold case to solve problem (6). Fix the current estimate of the Lagrange multiplier corresponding to the inequality constraints in (6) and the penalty parameter as \(^{k}\) and \(_{k}\), respectively. Then the subproblem at the \(k\)-th iteration is given as

\[_{,y}}_{_{k }}(,y,^{k}).\] (10)

Similar to the way for eliminating \(y\) in (9), we obtain an equivalent formulation of (10):

\[_{}_{_{k}}(,^{k}).\] (11)

Define the matrix \(_{_{k}}(,^{k})^{n n}\) with

\[[_{_{k}}(,^{k})]_{ij}=[_{_{k}}(,^{k })]_{ij}/\|_{_{k}}(,^{k})\|_{1}.\] (12)

By the chain rule, we have \(_{}_{_{k}}(,^{k})=r-_{_{k}}( ,^{k})\), \(_{}_{_{k}}(,^{k})=c-_{_{k}}( ,^{k})^{}\), \(_{U}_{_{k}}(,^{k})=-2V_{_{_{k}}( ,^{k})}U\), and \(_{U}_{_{k}}(,^{k})=_{ _{U}}(-2V_{_{_{k}}(,^{k})}U)\). Let \(^{1}_{_{k}}(,^{k})=\|_{U} _{_{k}}(,^{k})\|_{}\) and \(^{2}_{_{k}}(,^{k})=\|_{}_{ _{k}}(,^{k})\|_{1}+\|_{}_{_{k}}( ,^{k})\|_{1}\). The \((_{1},_{2})\)-stationary point of (11) and the connections of the approximate stationary points of problems (11) and (1) are given as follows.

**Definition 2.3**.: We say \(}\) an \((_{1},_{2})\)-stationary point of problem (11) (with fixed \(_{k}\) and \(^{k}\)) if \(^{1}_{_{k}}(},^{k})_{1}\) and \(^{2}_{_{k}}(},^{k})_{2}\).

**Theorem 2.4**.: _Suppose \(}=(,,)\) with \(\|_{_{k}}(},^{k})\|_{1}=1\) is an \((_{1},_{2})\)-stationary point of problem (11). Then, we have_

\[\|_{_{_{_{}}}}(-2V_{ })\|_{}_{1}+2\|C\|_{}_{2},\] (13a) \[,Z(})(2 n+\|\! ^{k}\|_{})_{k}+(\|\|_{}+\|\|_{}+\|C\|_{})_{2},\] (13b)

_where \(:=(_{_{k}}(},^{k}),(r,c))\) is a feasible matrix returned by running the rounding procedure "Round" given in [3, Algorithm 2] with input \(_{_{k}}(},^{k})\).__Remark 2.5_.: By Theorem 2.4, we can see that, given any \(>0\), for fixed \(^{k}\), by choosing \(_{k}=O()\) with \(_{1}=_{2}=\), an \(\)-stationary point with bounded \((,)\) (which can be found efficiently by iRBBS proposed in Section 3) of problem (11) can recover an \(\)-stationary point of PRW problem (1). This may also be of independent interest for the case with fixed \(U\). In such case, one can return a feasible \(\)-stationary point of the OT problem (4) (with fixed \(U\)) evaluated by the primal-dual gap other than the primal gap typically used in the literature, such as [3, Theorem 1].

Let \(^{0}\) be an initial point satisfying \(r^{}^{0}=c^{}^{0}\) and \(\|_{_{1}}(^{0},^{1})\|_{1}=1\) with \(^{1}=^{}\). We require that \(^{k}=(^{k},^{k},U^{k})\), the \((_{k,1},_{k,2})\)-stationary point of the subproblem (11) with \(k 1\), satisfies the following conditions:

\[r^{}^{k}=c^{}^{k},\;\|_{_{k} }(^{k},^{k})\|_{1}=1,\;_{_{k}}(^{k}, ^{k})\{_{_{k}}(^{k-1},^{k}),_{ _{k}}(^{0},^{k})\}.\] (14)

These conditions are important to establish the convergence of REALM (as shown in Appendix A.4). The following key observation to (11) shows that \(^{k}\) satisfying the first two conditions in (14) can be easily obtained. We omit the detailed proof for brevity since they can be verified easily by noticing that \(_{_{k}}(,^{k})=_{_{k}}(^ {s},^{k})\) where \(^{s}\) is defined in Proposition 2.6. The third condition in (14) can be easily satisfied by using any descent-like method starting from the better point of \(^{k-1}\) and \(^{0}\) to solve problem (11).

**Proposition 2.6**.: _For any \(=(,,U)\), consider \(^{s}=(+v_{1},+v_{2},U)\) with \(v_{1}=(c^{}-r^{}+_{k}(\|_{_{ k}}(,^{k})\|_{1}))/2\) and \(v_{2}=(r^{}-c^{}+_{k}(\|_{_{ k}}(,^{k})\|_{1}))/2\). Then we have that \(r^{}(+v_{1})=c^{}(+v_{2})\) and \(\|_{_{k}}(^{s},^{k})\|_{1}=1\) and also that \(_{_{k}}(,^{k})=_{_{k}}(^ {s},^{k})\) and \(_{}_{_{k}}(,^{k})=_{ }_{_{k}}(^{s},^{k})\)._

With such \(^{k}\) in hand, we compute the candidate of the next estimate \(^{k+1}\) as

\[^{k+1}=_{_{k}}(^{k},^{k}).\] (15)

Denote \(W^{k}^{n n}\) with \(W^{k}_{ij}=\{_{k}^{k+1}_{ij},(^{k})_{ij}\}\). The penalty parameter \(_{k+1}\) is updated according to the progress of the complementarity violation , denoted by \(\|W^{k}\|_{}\). If \(\|W^{k}\|_{}_{W}\|W^{k-1}\|_{}\) with \(_{W}(0,1)\), we keep \(_{k+1}=_{k}\) and update \(^{k+1}=^{k+1}\); otherwise we keep \(^{k+1}=^{k}\) and reduce \(_{k+1}\) via

\[_{k+1}=\{_{}_{k},_{k}/\|^{k}\|_{ }\},\] (16)

where \(_{}(0,1)\) is a constant and \(_{k} 0\) with \(_{k}>0\).

We summarize the above discussion as the complete algorithm in Algorithm 1, whose convergence is established as follows.

**Theorem 2.7**.: _Let \(\{(^{k},^{k})\}\) be the sequence generated by Algorithm 1 with \(_{1}=_{2}=_{c}=0\) and \((^{},^{})\) be a limit point of \(\{(^{k},^{k})\}\). Then, \((^{},^{})\) is a stationary point of PRW problem (1)._

_Remark 2.8_.: Our proposed REALM is a nontrivial extension of the exponential ALM from the Euclidean case  to the Riemannian case. The following two differences distinguish our proposed REALM from the existing exponential ALM (e.g., the one proposed in : (i) **Measure of complementarity.** The measure of complementarity used in our proposed REALM is motivated by the direct use of the complementarity condition adopted in the classical (quadratic) ALM, while that used in  is a variant of the measure for the exponential case. (ii) **Conditions on global convergence.** To guarantee the global convergence of the exponential ALMs, some (strong) constraint qualifications, the boundness of the iterates, and the feasibility of the limit point of the iterates generally need to be assumed; see Proposition 2.1 and Theorem 2.1 in  for the corresponding results. In contrast, for our considered PRW distance problem (6), we can prove the boundness of the iterates generated by REALM without making the assumption and establish the global convergence of REALM without explicitly dealing with the constraint qualification assumption. This advantage is mainly due to the _essential_ changes in the proposed REALM (compared with the existing exponential ALMs), i.e., specific conditions (14) and (16) (motivated by (13b)) on the solution of subproblems and the adopted measure of complementarity.

Moreover, it would be possible to extend the analysis in  to prove that the penalty parameter \(_{k}\) in Algorithm 1 is bounded away from zero if the Riemannian versions of the three conditions hold, including the linear independence constraint qualification, the strict complementarity condition, and the second-order sufficient condition. However, these three conditions might not be easy to check since we do not have prior knowledge of the solution.

_Remark 2.9_.: We cannot establish the iteration complexity of Algorithm 1 due to the following two main difficulties: (i) characterizing the connection between the two complementarity measures \(\|W^{k}\|_{}\) and \(e^{k}:=^{k},Z(^{k})\) with \(^{k}:=(_{_{k}}(^{k},^{k}),(r,c))\) at the approximate stationary point of the subproblem; (ii) establishing the relationship between \(_{k}_{ij}^{k+1}\) and \((^{k})_{ij}\). Thanks to Theorem 2.4, we can slightly modify Algorithm 1 to establish the iteration complexity. By modifying the "if" condition in Line 6 of Algorithm 1 as "\(\|W^{k}\|_{}_{W}\|W^{k-1}\|_{}\) and \(e^{k}_{W}e^{k-1}\)", and leveraging the connection between the approximate stationary points of the subproblem and the original problem as proven in (13), we know that the modified Algorithm 1 will terminate within at most \((\{_{1}^{-1},_{2}^{-1},T_{k}\})\) iterations, where \(T_{k}:=\{k\ |\ _{k}_{c}\}\).

_Remark 2.10_.: Problem (6) can also be solved by the Riemannian ALMs based on the quadratic penalty function . However, the subproblems therein have four blocks of the variable, i.e., (\(,,y,\,U\)), and some customized solvers are needed to solve them. Moreover, the connections between the stationary points of problem (1) and the subproblems therein remain unclear.

## 3 A Practical iRBBS for Solving Subproblem (11)

At first glance, (11) is a three-block optimization problem and can be solved by R(A)BCD proposed by . However, as stated therein, tuning the stepsizes for updating \(U\) is not easy for R(A)BCD. In sharp contrast, we understand (11) as optimization with only one variable \(U\) as follows:

\[_{U}\,q(U):=_{^{n}, ^{n}}_{_{k}}(,^{k})}.\] (17)

By [34, Lemma 3.1], we know that \(q()\) is differentiable over \(^{d k}\). Here, we give a new formulation of \(q(U)\), which can provide more insights into approximating \(q(U)\).

**Lemma 3.1**.: _Let \((_{U}^{*},_{U}^{*})_{^{ n},^{n}}(,^{k})\) and \(_{U}^{*}=(_{U}^{*},_{U}^{*},U)\). Then we have \(q(U)=_{U}_{_{k}}( _{U}^{*},^{k})=_{_{U}}(-2V_{_{ _{k}}(_{U}^{*},^{k})}U)\)._

Hence we could use the Riemannian gradient descent (RGD) method  to solve problem (11). Letting \(_{t}>0\) be some stepsize, the main iterations of RGD are given as

\[U^{t+1}=_{U^{t}}-_{t}q(U^{t}).\] (18)

However, RGD (18) needs to calculate \((_{U}^{*},_{U}^{*})\) exactly, which can be challenging (or might be unnecessary) to do. Motivated by the well-established inexact gradient type methods for optimization in the Euclidean space , we propose an inexact RGD framework. Let \(^{t}=(^{t},^{t},U^{t})\) with \((^{t},^{t})(_{U^{t}}^{*},_{U^{t}}^{*})\), wherein the inexactness level is determined by \(_{_{k}}^{2}(^{t},^{k})=\|_{}_{_{k}}(^{t},^{k})\|_{1}+\|_{}_{ _{k}}(^{t},^{k})\|_{1}_{t}\) for given \(_{t}\). By Lemma 3.1, we use

\[^{t}:=_{U}_{_{k}}(^{t},^{k} )=_{_{U}}-2V_{_{_{k}}( ^{t},^{k})}U^{t}\]

to approximate \(q(U^{t})\). Then, we perform an inexact RGD with \(q(U^{t})\) in (18) replaced by \(^{t}\). More specifically, given the inexactness parameter \(_{t+1} 0\) and the stepsize \(_{t} 0\), we update \(^{t+1}=(^{t+1},^{t+1},U^{t+1})\) with \(U^{t+1}\) and \((^{t+1},^{t+1})(_{U^{t+1}}^{*},_{U^{t+1}}^{*})\) satisfying

\[U^{t+1}=_{U^{t}}-_{t}^{t}\,, \] (19a) \[_{_{k}}^{2}(^{t+1},^{k})_{t +1}. \] (19b)To make iRGD (19) practical, the first main ingredient is how to compute \((^{t+1},^{t+1})\) such that (19b) holds. Given \(U^{t+1}\), \(^{(0)}=^{t}\), and \(^{(0)}=^{t}\), for \(=0,1,\), we adopt the block coordinate descent method to update

\[^{(+1)}=*{argmin}_{^{n}}_ {_{k}}(,^{()},U^{t+1},^{k}),^{(+1)}= *{argmin}_{^{n}}_{_{k}}(^ {(+1)},,U^{t+1},^{k}).\] (20)

Note that \(^{(+1)}\) and \(^{(+1)}\) admit the closed-form solutions as follows:

\[^{(+1)}=^{()}-_{k} r+_{k}(^{()} ),^{(+1)}=^{()}-_{k} c+_{k}(( ^{(+)})^{}),\] (21)

where \(^{()}:=_{_{k}}(^{()},^{()},U^{t+1},^{ k})\) and \(^{(+)}:=_{_{k}}(^{(+1)},^{()},U ^{t+1},^{k})\) (see (7) for its definition). Note that for fixed \(U^{t+1}\) and \(^{k}=^{}\), (21) reduces to the famous Sinkhorn iteration . Therefore, we still name (21) as the Sinkhorn iteration. It is easy to verify that \(\|^{(+)}\|_{1}=\|^{(+1)}\|_{1}=1\) (see also [28, Remark 3.1]). By (12), we have

\[^{(+)}:=_{_{k}}(^{(+1)},^{()},U^ {t+1},^{k})=^{(+)},\ ^{(+1)}:=_{_{k}}(^{(+1)},^{(+1)},U^{t+1}, ^{k})=^{(+1)}.\] (22)

From the update of \(^{(+1)}\), we have \((^{(+1)})^{}-c=0\). Therefore, to make condition (19b) hold, we stop the Sinkhorn iteration once

\[\|^{(+1)}-r\|_{1}_{t+1},\] (23)

and set \(^{t+1}=^{(+1)}\), \(^{t+1}=^{(+1)}\). Recalling the calculation of the gradient and the definition of \(^{2}_{_{k}}(^{t+1},^{k})\) after (12), we have

\[^{2}_{_{k}}(^{t+1},^{k})=\|^{(+1)} -r\|_{1}.\]

Next, we choose the stepsize \(_{t}\) in (19a). Since the accurate function and gradient information of \(q(U^{t})=_{_{k}}(^{*}_{U^{t}},^{k})\) is unavailable, we cannot expect to build the linesearch condition based on \(q(U^{t})\) and need to find some appropriate potential function instead. Considering that \(_{_{k}}(^{t},^{k})\) is an approximation of \(q(U^{t})\) and the approximation error is controlled by \(^{2}_{_{k}}(^{t},^{k})\), it is thus desirable that some combinations of \(_{_{k}}(^{t},^{k})\) and \(^{2}_{_{k}}(^{t},^{k})\) will be smaller than the corresponding values at the previous iteration. Given \([0,_{k}/2)\), we define the potential function as

\[E_{}(^{t})=_{_{k}}(^{t},^{k})+( ^{2}_{_{k}}(^{t},^{k}))^{2}\] (24)

and require the stepsize \(_{t}\) to satisfy the following nonmonotone line search condition:

\[E_{}(^{t+1}) E^{r}_{t}-_{1}_{t}\|^{t}\|^{2}_{ }-}{2}-(^{2}_{_{k}}( ^{t+1},^{k}))^{2},\] (25)

where \(E^{r}_{t+1}=( Q_{t}E^{r}_{t}+E_{}(^{t+1}))/Q_{t+1}\) and \(Q_{t+1}= Q_{t}+1\) with a constant \([0,1)\) and \(E^{r}_{0}=E_{}(^{0})\), \(Q_{0}=1\); see . Such \(_{t}\) can be found by adopting the simple backtracking line search technique starting from an initial guess of the stepsize \(^{(0)}_{t}\). Owing to the excellent performance of the BB method in Riemannian optimization , we choose the initial guess \(^{(0)}_{t}\) for \(t 1\) as a new Riemannian BB stepsize with safeguards:

\[^{(0)}_{t}=\{\{^{}_{t},_{}\},_{}\},\] (26)

where \(_{}>_{}>0\) are preselected stepsize safeguards and \(^{}_{1}=^{}_{1}\) and for \(t 2\), we set \(^{}_{t}=\{^{}_{t-1},^{}_{t}, \{^{}_{t},0\}\}\) if \(^{}_{}<_{t}^{}_{t}\) and set \(^{}_{t}=^{}_{t}\) otherwise. Here, \(^{}_{t}=\|U^{t}-U^{t-1}\|^{2}_{}/| U^{t}-U^{t-1}, ^{t}-^{t-1}|,^{}_{t}=| U^{t}-U^{t-1},^ {t}-^{t-1}|/\|^{t}-^{t-1}\|^{2}_{}\), and \(^{}_{t}\) is chosen according to [30, Eq. (2.15)]. In our numerical tests, we set the initial \(_{t}\) to be 0.05 and update \(_{t+1}=_{t}/1.02\) if \(^{}_{t}/^{}_{t}<_{t}\) and update \(_{t+1}=1.02_{t}\) otherwise.

We are ready to summarize the complete iRBDS in Algorithm 2. The overall complexity of Algorithm 2 to find an \((_{1},_{2})\)-stationary point of problem (1) is in the same order as that of R(A)BCD.

**Theorem 3.2**.: _By choosing \(_{1}=^{}_{1}/2\), \(_{2}=\{^{}_{1}/(4\|C\|_{}),^{}_{2}/(4 _{k}+6\|C\|_{})\}\) with \(=\|^{k}\|_{}+\{\| r\|_{},\| c \|_{}\}\) and \(_{k}=^{}_{2}/(4 n+2\|^{k}\|_{})\), Algorithm 2 can return an \((^{}_{1},^{}_{2})\)-stationary point of problem (1) in \((T_{^{}_{1},^{}_{2}})\) iterations with_

\[T_{^{}_{1},^{}_{2}}=(^{}_{1} )^{-2},(^{}_{2})^{-2}}(^{}_{2})^{-1}.\]

_If \(_{t} 2R^{t}/(_{k}(_{}-2+))\) with \(_{} 1\) and \(R^{t}=\|C(U^{t})\|_{}+_{k},\) the total number of Sinkhorn iterations is \((_{}T_{```
1Input: Choose \(_{}>_{}>0\), \(_{0}^{(0)}>0\), \(_{1},_{2} 0\), \(,_{1}(0,1)\), \([0,_{k}/2)\), \([0,1)\), and \((^{-1},^{-1},U^{0})\). Set \(^{(0)}=^{-1},^{(0)}=^{-1}\) and perform the Sinkhorn iteration (21) at \(U^{0}\) until (23) holds with \(_{0}=1\) for some \(\). Set \(^{0}=^{(+1)},^{0}=^{(+1)}\).
2for\(t=0,1,\)do
3 Compute \(^{t}=_{U}_{_{k}}(^{t},^{k})\);
4if\(\|^{t}\|_{}_{1}\)and \(_{_{k}}^{2}(^{t},^{k})_{2}\)thenreturn\(^{t}\);
5for\(s=0,1,\)do
6 Set \(U^{t+1}=_{U^{t}}(-_{t}^{t})\) with \(_{t}=_{t}^{(0)}^{s}\) and update \(_{t+1}\) (e.g., via (27) further ahead);
7 Set \(^{(0)}=^{t}\) and \(^{(0)}=^{t}\) and perform the Sinkhorn iteration (21) at \(U^{t+1}\) until (23) holds for some \(\); set \(^{t+1}=^{(+1)}\) and \(^{t+1}=^{(+1)}\).
8if(25) holdsthenbreak; ```

**Algorithm 2**A practical iRBBS for solving problem (11).

_Remark 3.3_.: The basic idea of proposing Algorithm 2 is sharply different from that of R(A)BCD developed in . Ours is based on the inexact RGD viewpoint, while the latter is based on the BCD approach. Such an inexact RGD viewpoint enables us to choose the stepsize adaptively via leveraging the efficient BB stepsize. Actually, tuning the best stepsize for the \(U\)-update in R(A)BCD is nontrivial. It is remarked in [28, Remark 4.1] that _"the adaptive algorithms RABCD and RAGAS are also sensitive to the step size, though they are usually faster than their non-adaptive versions RBCD and RGAS."_ Numerical results in Section 4.1 show the higher efficiency of our iRBBS over R(A)BCD.

_Remark 3.4_.: Although the inexact gradient type methods have been well explored in the Euclidean case [11; 26; 46; 17; 39; 7; 52], to our best knowledge, there are little results for the Riemannian case and on how to choose the stepsizes adaptively for general nonlinear objective functions. One exception is R(A)GAS proposed in , which can be understood as the inexact RGD method. However, it needs to compute the inexact Riemannian gradient with relatively high accuracy and essentially uses the constant-like stepsizes. In contrast, our iRBBS allows to compute the inexact Riemannian gradient with low accuracy and choose the stepsize adaptively.

_Remark 3.5_.: It might be better to use possible multiple Sinkhorn iterations rather than only one iteration as done in R(A)BCD in updating \(\) and \(\) from the computational point of view. The cost of updating \(^{(l+1)}\) and \(^{(+1)}\) via one Sinkhorn iteration (21) is \((n^{2})\). In contrast, the cost of updating \(U^{t+1}\) via performing a RGD step (19a) is \((ndk+n^{2}k+dk^{2})\), wherein the main cost is to compute \(V_{_{k}(^{t},^{k})}U^{t}\), which can be done by observing \(V_{}U=X()X^{}U+Y(^{})Y^{}U-X Y^{}U-Y^{ }X^{}U\). Considering that the cost of updating \(\) and \(\) is much less than that of updating \(U\), it is reasonable to update \(\) and \(\) multiple times and update \(U\) only once.

## 4 Experimental Results

In this section, we conduct numerical experiments on six Shakespeare operas to evaluate the performance of our proposed approaches; see Dataset C.1 for a more detailed description of the dataset. All methods are implemented in MATLAB. More numerical results can be found in Appendix D. The codes are available from https://github.com/bjiangopt/ReALM.

### Comparison with R(A)BCD on Solving Subproblem (11)

Subproblem (11) with \(^{k}=^{}\) and a relatively small \(_{k}=\) is used to compute the PRW distance in . We choose \(=0.1\) as done in . Since  has shown the superiority of R(A)BCD over R(A)GAS proposed in , we mainly compare our proposed iRBBS, namely, Algorithm 2, with R(A)BCD. For iRBBS and R(A)BCD, we use the same stopping conditions with \(_{1}=2\|C\|_{}_{2}\) (motivated by (13a)) and \(_{2}=10^{-6}\{\|r\|_{},\|c\|_{}\}\). To make the residual error more comparable, we choose

\[_{0}=1,_{t+1}=\{_{_{k }}^{1}(^{t},^{k})}{_{1}},1\}_{2}, t  0,\] (27)

[MISSING_PAGE_FAIL:9]

\(_{k}=_{}\). Subproblem (11) in REALM is solved by iRBBS. If \(\{\|r\|_{},\|c\|_{}\} 500_{k}\) or \(\|C(U^{t})-_{k}^{k}\|_{} 900_{k}\), we set \(=10\) and perform the Sinkhorn iteration (21); otherwise, we set \(=0.1\) and perform an equivalent formulation of Sinkhorn iteration (28).

The results over 20 runs on Dataset C.1 are reported in Table 2. In this table, the terms "nSk\({}_{}\)" and "nSk\({}_{}\)" mean the total numbers of Sinkhorn iterations (21) and (28), respectively, the pair "\(k_{1}\)/\(k\)" in the column "iter" means that the corresponding algorithm stops at the \(k\)-iteration and updates the multiplier matrix \(k_{1}\) times. To save space, we only report instances where one method can return the value "\(}_{k}^{2}\)" larger than 1.005 times of the smaller one of the two \(}_{k}^{2}\) values returned by the two methods. The better "\(}_{k}^{2}\)" is marked in **bold**. Besides, the average performance over all 15 instances is also kept in the "AVG" line.

From Table 2, we can observe that REALM-\((0.007,0.9)\) can not only return better solutions than REALM-\((0.0035,0)\) but also is about 5.2x faster. On average, REALM-\((0.007,0.9)\) updates the multiplier matrix 8 times in 15 total iterations, which shows that updating the multiplier matrix does help. The reasons why REALM with updating the multiplier matrix outperforms REALM without updating the multiplier matrix in terms of solution quality and speed are as follows. First, updating the multiplier matrix in REALM can keep the solution quality even using a larger \(_{k}\). Second, solving the subproblem with a larger \(_{k}\) is always easier, which enables that REALM-\((0.07,0.9)\) computes less \(_{U}\,_{_{k}}(^{t},^{k})\) and performs less Sinkhorn iterations (21) which involves computing the log-sum-exp function \(_{i}(x_{i}/_{k})=x_{}/_{k}+_{i}((x_{i}-x_{ })/_{k})\) for small \(_{k}\).

## 5 Concluding Remarks

In this paper, we considered the computation of the PRW distance. By reformulating this problem as an optimization problem over the Cartesian product of the Stiefel manifold and the Euclidean space with additional nonlinear inequality constraints, we proposed a method called REALM. The convergence of REALM was also established. To solve the subproblem in REALM efficiently, we developed a practical iRBBS method with convergence and iteration complexity guarantees, wherein the Riemannian BB stepsize (based on the inexact Riemannian gradient information) and Sinkhorn iterations are employed. The complexity of iRBBS to attain an \(\)-stationary point of the original PRW distance problem matches the best known iteration complexity result. Numerical results showed that, compared with the state-of-the-art methods, our proposed REALM and iRBBS methods have advantages in solution quality and speed.

Moreover, our proposed REALM and iRBBS can also be extended to solve some important minimax problems over the Riemannian manifolds arising from machine learning, such as the fair PCA problem  and the projection robust Wasserstein barycenters , etc.

Lastly, a limitation of our work is that we did not establish the positive lower bound of \(_{k}\) in REALM, despite the fact that REALM performs well in practice and can avoid too small \(_{k}\) in many cases. We shall investigate the conditions under which it is possible to establish a lower bound of \(_{k}\) in REALM. This can be achievable by extending the analysis and conditions in  to the Riemannian case.

    & }_{k}^{2}\)} &  & _{}\)/nSk\({}_{}\)} &  &  \\  data & a & b & a & b & a & b & a & b & a & b \\  H5/DC & 0.09270 & **0.10985** & 1081 & 675 & 23809/8267 & 97860 & 84.1 & 8.6 & 0.0/8.0 & 8.0/15.0 \\ H/MV & 0.06378 & **0.06424** & 1116 & 727 & 3864/13382 & 309970 & 175.7 & 14.9 & 0.0/8.0 & 8.0/15.0 \\ H/RJ & 0.21706 & **0.22607** & 547 & 788 & 3091/4062 & 1174/7470 & 63.3 & 15.6 & 0.0/8.0 & 8.0/15.0 \\ JC/MV & **0.06267** & 0.06255 & 1324 & 711 & 12875/5727 & 170730 & 53.3 & 5.9 & 0.0/8.0 & 8.0/15.0 \\ JC/O & 0.04221 & **0.04277** & 1650 & 1100 & 4997/26612 & 35031/4500 & 204.7 & 46.7 & 0.0/8.0 & 8.0/15.0 \\ MV/O & **0.04181** & 0.03661 & 890 & 822 & 6101/5289 & 31651/0 & 57.0 & 13.3 & 0.0/8.0 & 8.0/15.0 \\ AVG & 0.11338 & **0.11461** & 863 & 787 & 7824/7364 & 20261/300 & 78.0 & 15.0 & 0.0/8.0 & 8.0/15.0 \\   

Table 2: Average results of REALM for Dataset C.1, “a” and “b” stand for REALM-\((0.0035,0)\) and REALM-\((0.007,0.9)\), respectively.

AcknowledgmentsThe work of Bo Jiang was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 11971239, Grant 12371314, and in part by the Natural Science Foundation of the Higher Education Institutions of Jiangsu Province under Grant 21KJA110002. The work of Ya-Feng Liu was supported in part by NSFC under Grant 11991020, Grant 11991021, and Grant 12288201.