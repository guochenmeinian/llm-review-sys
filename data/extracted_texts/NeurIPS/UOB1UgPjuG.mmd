# Class-Distribution-Aware Pseudo-Labeling for Semi-Supervised Multi-Label Learning

Ming-Kun Xie\({}^{1,2}\), Jia-Hao Xiao\({}^{1,2}\), Hao-Zhe Liu\({}^{1,2}\), Gang Niu\({}^{3}\), Masashi Sugiyama\({}^{3,4}\), Sheng-Jun Huang\({}^{1,2}\)

\({}^{1}\)Nanjing University of Aeronautics and Astronautics

\({}^{2}\)MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, China

\({}^{3}\)RIKEN Center for Advanced Intelligence Project

\({}^{4}\)The University of Tokyo, Tokyo, Japan

{mkxie, jiahaoxiao, haozheliu, huangsj}@nuaa.edu.cn

gang.niu.ml@gmail.com sugi@k.u-tokyo.ac.jp

Correspondence to: Sheng-Jun Huang (huangsj@nuaa.edu.cn).

###### Abstract

Pseudo-labeling has emerged as a popular and effective approach for utilizing unlabeled data. However, in the context of semi-supervised multi-label learning (SSMLL), conventional pseudo-labeling methods encounter difficulties when dealing with instances associated with multiple labels and an unknown label count. These limitations often result in the introduction of false positive labels or the neglect of true positive ones. To overcome these challenges, this paper proposes a novel solution called Class-Aware Pseudo-Labeling (CAP) that performs pseudo-labeling in a class-aware manner. The proposed approach introduces a regularized learning framework incorporating class-aware thresholds, which effectively control the assignment of positive and negative pseudo-labels for each class. Notably, even with a small proportion of labeled examples, our observations demonstrate that the estimated class distribution serves as a reliable approximation. Motivated by this finding, we develop a class-distribution-aware thresholding strategy to ensure the alignment of pseudo-label distribution with the true distribution. The correctness of the estimated class distribution is theoretically verified, and a generalization error bound is provided for our proposed method. Extensive experiments on multiple benchmark datasets confirm the efficacy of CAP in addressing the challenges of SSMLL problems. The implementation is available at https://github.com/milkxie/SSMLL-CAP.

## 1 Introduction

In _single-label_ supervised learning, each instance is assumed to be associated with only one class label, while many realistic scenarios may be _multi-labeled_, where each instance consists of multiple semantics. For example, an image of a nature landscape often contains the objects of _sky_, _cloud_, and _mountain_. Multi-label learning (MLL) is a practical and effective paradigm for handling examples with multiple labels. It trains a classifier that can predict all the relevant labels for unseen instances based on the given training examples. A large number of recent works have witnessed the great progress that MLL has made in many practical applications , _e.g._, image annotation , protein subcellular localization , and visual attribute recognition .

Thanks to its powerful capacity, the deep neural network (DNN) has become a prevalent learning model for handling MLL examples . Unfortunately, it requires a large number of precisely labeledexamples to achieve favorable performance. This leads to a high cost of manual annotation, especially when the dataset is large and the labeling task must be carried out by an expert. Given that it is hard to train an effective DNN based on a small subset of training examples, it is rather important to exploit the information from unlabeled instances. The problem has been formalized as a learning framework called semi-supervised multi-label learning (SSMLL), which aims to train a classifier based on a small set of labeled MLL examples and a large set of unlabeled ones.

Compared to semi-supervised learning (SSL) that has made great progress [4; 38], SSMLL has received relatively less attention in the context of deep learning. Generally, there are still three main challenges towards the development of SSMLL. Firstly, since each instance is associated with multiple labels and the number is unknown, the commonly used pseudo-labeling strategy that selects the most probable label or the top-\(k\) probable labels cannot be applied to the SSMLL problems. It would face the dilemma of either introducing false positive labels or neglecting true positive ones. Secondly, due to the intrinsic class-imbalance property of MLL data, it is hard to achieve favorable performance by using a fixed threshold for each instance. Thirdly, recent studies have mainly focused on multi-label learning with missing labels (MLML) [11; 17; 2] scenarios, where each training instance is assumed to be assigned with a subset of true labels. Unfortunately, these methods often fail to achieve favorable performance, or cannot even be applied to the SSMLL scenarios, since most of them were designed under the assumption of MLML.

To solve these challenges, in this paper, we propose a novel Class-Aware Pseudo-labeling (CAP) method for handling the SSMLL problems. Unlike the existing methods, we perform pseudo-labeling in a class-aware manner to avoid estimating the number of true labels for each instance, which can be very hard in practice. Specifically, a regularized learning framework is proposed to determine the numbers of positive and negative pseudo-labels for each class based on the class-aware thresholds. Given that the true class distribution is unknown, we alternatively determine the thresholds based on the estimated class distribution of labeled data, which can be a tight approximation according to our observation. Our theoretical results show the correctness of estimated class distribution and provide a generalization error bound for CAP. Extensive experimental results on multiple benchmark datasets with a variety of comparing methods validate that the proposed method can achieve state-of-the-art performance.

## 2 Related Work

Thanks to the powerful learning capacity of DNNs, MLL has made great advances in the context of deep learning. Some methods designed architectures  or training strategies [22; 49] to exploit the label correlations. Some other methods designed sophisticated loss functions to improve the performance of MLL . The last group of methods designed specific architectures to capture the objects related to semantic labels. Global-average-pooling (GAP) based models  and attention-based models [22; 26] are two groups of representative methods.

There are relatively few works that study how to improve the performance of deep models in SSMLL scenarios. Instead of end-to-end training, the only deep SSMLL method  performed the two-stage training, which first used a DNN to extract features, and then used a linear model to perform classification.  proposed a deep sequential generative model to handle the noisy labels collected by crowdsourcing and unlabeled data simultaneously.  focused on the transductive and non-deep scenario, and thus cannot be applied to our setting. The method proposed by  utilized the graph neural network (GNN) to deal with SSMLL data with graph structures. In contrast, there are many works that trained linear models to solve the SSMLL problems [5; 15; 42; 52; 50; 40]. M3DN was proposed to deal with SSMLL data in multi-modal multi-instance learning scenarios by adopting optimal transport technique .

Pseudo-labeling has become a popular method in semi-supervised learning (SSL). The idea was firstly applied to semi-supervised training of deep neural networks . Subsequently, a great deal of works have been devoted to improving the quality of pseudo-labels either by adopting consistency regularization [4; 38], or by using distribution alignment [30; 3]. The contrastive learning technique has been applied to improve the performance of SSL . To improve the reliability of pseudo-labels, an uncertainty-aware pseudo-labeling method proposed in  selected reliable pseudo-labels based on the prediction uncertainty. Recent studies have also paid attention to dealing with the class-imbalance problem of pseudo-labeling in SSL scenarios [18; 45; 14]. Unlike FixMatch that selects unlabeled examples with a fixed threshold, Dash  selected unlabeled examples with a dynamic threshold, with the goal of achieving better pseudo-labeling performance. FlexMatch  was proposed to select unlabeled examples for every class according to the current learning status of the model. Several works have been explored the idea of selecting different thresholds for different classes to improve the performance of SLL [13; 14; 44].However, these methods are designed for the multi-class single-label scenario, and cannot be directly applied to the multi-label scenario.

In order to reduce the annotation cost, a cost-effective strategy is to assign a subset of true labels to each instance. For example,  designed a partial binary cross entropy (BCE) loss that re-weights the losses of known labels. As an extreme case of MLML, single positive multi-label learning (SPML) [8; 53; 41] assumes that only one of multiple true labels can be observed during the training stage. The pioneering work  trains DNNs by simply treating unobserved labels as negative ones and utilizes the regularization to alleviate the harmfulness of false negative labels.  propose asymmetric pseudo labeling technique to recover true labels.

## 3 The Method

In the SSMLL problem, let \(\) be a feature vector and \(\) be its corresponding label vector, where \(=^{d}\) is the feature space and \(=\{0,1\}^{q}\) is the label space with \(q\) possible class labels. Here, \(y_{k}=1\) indicates the \(k\)-th label is relevant to the instance, while \(y_{k}=0\), otherwise. Suppose that we are given a labeled dataset with \(n\) training examples \(_{l}=\{(_{i},_{i})\}_{i=1}^{n}\) and an unlabeled dataset with \(m\) training instances \(_{u}=\{_{j}\}_{j=1}^{m}\). Our goal is to train a DNN \(f(;)\) based on the labeled dataset \(_{l}\) and unlabeled dataset \(_{u}\), where \(\) is the parameter of the network. For notational simplicity, we omit the notation \(\) and let \(f()\) be the predicted probability distribution over classes and \(f_{k}()\) be the predicted probability of the \(k\)-th class for input \(\).

Typical multi-label learning methods usually train a DNN with the commonly used binary cross entropy (BCE) loss, which decomposes the original task into multiple binary classification problems. Unfortunately, BCE loss often suffers from positive-negative imbalance issue. To mitigate this problem, we adopt the asymmetric loss (ASL) , which is a variant of focal loss with different focusing parameters for positive and negative instances. In our experiment, we found it works better than BCE loss. Formally, given the predicted probabilities \(f()\) on instance \(\), the ASL loss is defined as

\[(f(),)=_{k=1}^{q}y_{k}_{1}(f_{k}())+(1-y_{k})_{0}(f_{k}()),\] (1)

Here, \(_{1}(f_{k})=-(1-f_{k})^{_{1}}(f_{k})\) and \(_{0}(f_{k})=-(f_{k})^{_{0}}(1-f_{k})\) represent the losses calculated on positive and negative labels, where \(_{1}\) and \(_{0}\) are positive and negative focusing parameters.

### Instance-Aware Pseudo-Labeling

The loss function may not be the best choice to solve the SSMLL problem, since besides the labeled training examples, there still exist a large number of unlabeled training examples. To exploit the information of unlabeled data, inspired by recent SSL works [4; 38], an intuitive strategy is assigning the unlabeled instances with pseudo-labels based on the model outputs. Formally, we define the unlabeled loss \(_{u}\) as

\[_{u}(f(),})=_{k=1}^{q}_{k} _{1}(f_{k}())+(1-_{k})_{0}(f_{k}()),\]

where \(}=[_{1},,_{j}]^{}\) represents the pseudo-label vector for instance \(\).

In the above formulation, the most significant element is how to obtain the pseudo-labels \(}\) that significantly affects the final performance of SSMLL. Most of existing pseudo-labeling methods are performed in an instance-aware manner by assigning pseudo-labels to each unlabeled instance based on its probability distribution. Below, we briefly review three instance-aware pseudo-labeling strategies that can be applied to the SSMLL problems. The most commonly used strategy adopted by the SSL method called FixMatch  is to select the most probable label as the ground-truth one:

\[_{k}=1&k=_{c[q]}f_{c}(),\\ 0&.\] (2)One advantage of the strategy is that it is likely to safely identify a true label for each unlabeled training instance. Unfortunately, it is obvious that the strategy would neglect multiple true labels. Generally, it transforms the unlabeled dataset into another learning scenario called single positive multi-label learning (SPML) , where only one of multiple positive labels is available for each instance. A straightforward strategy is to simply treat unobserved labels as negative ones. Although this strategy enables us to train a classifier based on SPML data, it would introduce a large number of false negative labels, leading to unfavorable performance.

The second choice is an improved version of the above strategy, which selects the top \(l\) probable labels as the true ones:

\[_{k}=1&f_{k}()^{l},\\ 0&,\] (3)

where \(^{l}\) is the \(l\)-th predicted probability in a descending order. The strategy conducts a competition among labels, and selects top \(l\) winners. The optimal solution is to set \(l\) as the true number of positive labels for each unlabeled instance. Unfortunately, since the true number is unknown in practice, as a compromise, we set \(l\) as the average number of positive labels per instance. Given that the true number does not always equal to the average number, it would be caught in a dilemma of either introducing false positive labels or neglecting true positive ones.

The last choice is to adopt an instance-aware threshold \(_{j}\) that separates positive and negative labels for each unlabeled instance.

\[_{jk}=1&f_{k}(_{j})_{j},\\ 0&.\] (4)

Compared to the above methods, this strategy achieves a strong flexibility that allows it to assign different numbers of positive labels to different instances. A potential limitation is that it is hard to find the optimal thresholds for different instances. In practice, a feasible solution is to adopt a global threshold, that is \( j[m],_{j}=\). Obviously, it is impossible to adopt a global threshold that is optimal for all instances, especially considering the class-imbalance property of MLL data. In general, a large threshold often leads to a small recall score of tail classes, which indicates that less positive labels would be identified. While a small threshold often results in a small precision score of head classes, which indicates besides positive labels, a great deal of negative ones would be treated as positive ones. The dilemma prevents the model from obtaining favorable performance.

### Class-Aware Pseudo-Labeling

As discussed above, in many real-world scenarios, it is really difficult to acquire the true number of positive labels for each instance. This leads the instance-aware pseudo-labeling methods to be caught in the dilemma of either mislabeling false positive labels or neglecting true positive labels, resulting in a noticeable decrease of the model performance.

To solve this issue, we propose a regularized learning framework to assign pseudo-labels in a class-aware manner. Formally, we reformulate the optimization problem of SSMLL as

\[_{},} _{i=1}^{n}_{k=1}^{q}y_{ik}_{1}(f_{k}(_{i}))+(1-y _{ik})_{0}(f_{k}(_{i}))\] (5) \[+_{j=1}^{m}_{k=1}^{q}_{jk}_{1}(f_{k}(_{ j}))+(1-_{jk})_{0}(f_{k}(_{j}))-_{j=1}^{m}_{k=1}^{q} _{k}_{jk}+_{k}(1-_{jk}),\] \[  j[m],}_{j}=[y_{j1},,y_{jq}]^{} \{0,1\}^{q},\] \[ k[q],_{k}>0,_{k}>0,\]

where \(_{k}\) and \(_{k}\) are class-aware regularized parameters to control how many positive and negative labels would be included into model training for class \(k\). Below, we primarily provide a solution of the optimization problem Eq.(5), and then discuss how to set parameters \(_{k}\) and \(_{k}\) to capture the true class distribution of unlabeled examples.

Alternative SearchIt is hard to directly solve the optimization problem Eq.(5), since there are two sets of variables. A feasible solution is to adopt the alternative convex search [1; 54] strategy that optimizes a group of variables by fixing the other group of variables.

Suppose that pseudo labels \(}\) are given, then the optimization problem Eq.(5) can be transformed into an ordinary loss by treating the pseudo labels as the true ones:

\[_{}_{i=1}^{n}(f(_{i}),_ {i})+_{j=1}^{m}_{u}(f(_{j}),}_{j}),\] (6)

which can be solved by applying the stochastic gradient decent (SGD) method.

With the parameters \(\) fixed, we reformulate the optimization problem with respect to \(}\) as

\[_{}}_{j=1}^{m}_{k=1}^{q}_{jk}_{1}(f_{k}(_{j}))+(1-_{jk})_{0}(f_{k}(_{j}))-_{j=1}^{m}_{k=1}^{ q}_{k}_{jk}+_{k}(1-_{jk}).\] (7)

Consider that \(_{k}\) is assume to be one or zero, we can obtain the following solution:

\[_{k}=1&f_{k}()(_{k}),\\ 0&f_{k}()(_{k}),\\ -1&,\] (8)

where \((_{k})=(-_{k})\) and \((_{k})=1-(-_{k})\) are two class-aware thresholds, and \(_{k}=-1\) means that the label \(_{k}\) would not be used for model training.

### Class-Distribution-Aware Thresholding

An important problem is how to set the thresholds \((_{k})\) and \((_{k})\), which determine the numbers of positive and negative pseudo-labels for every class \(k\). In order to capture the true class distribution, we propose the Class-distribution-Aware Thresholding (CAT) strategy to determine \((_{k})\) and \((_{k})\). Suppose that we are given \(_{j}, j[m]\), _i.e._, the true label vectors of unlabeled training instances. By solving the following equation, we can obtain \((_{k})\) and \((_{k})\) that capture the true class distribution of unlabeled data.

\[^{m}(f_{k}(_{j})(_{k}))}{m}= _{k}^{*},^{m}(f_{k}(_{j})( _{k}))}{m}=_{k}^{*},\]

where \(_{k}^{*}=^{m}(y_{j}=1)}{m}\) and \(_{k}^{*}=^{m}(y_{j}=0)}{m}\) are respectively the proportions of positive and negative labels in unlabeled data for class \(k\). Although during the training process, the true labels of unlabeled instances are inaccessible, our observation shows that the estimated class distribution, i.e.,

Figure 1: (a) An illustration of the comparison between instance-aware and class-aware pseudo-labeling methods. (b) The curves of the estimated and true class proportions on COCO and VOC. By using the CAT strategy, CAP can provide high-quality pseudo-labels by approximating the true class distribution. This can be validate by the results in (b), where the empirical and true class proportions of positive labels show high-level consistency.

the class proportions of positive and negative labels in labeled examples, can tightly approximate the true class distribution. As shown Figure 1 (b), we illustrate the proportions of positive labels in labeled examples and unlabeled examples for every class \(k\) on two benchmark datasets COCO and VOC. The proportions of labeled examples are respectively \(p=0.05\) and \(p=0.1\) for COCO and VOC. From the figures, it can be observed that even with a small proportion of labeled examples (\(p=0.05\)), it achieves a nearly complete overlap between the estimated and true curves, which validates that the estimated class distribution can be a tight approximation of the true one. This motivate us to alternatively utilize the estimated class distribution to solve the solutions for \((_{k})\) and \((_{k})\):

\[^{m}(f_{k}(_{j})(_{k}))}{m}= _{k},^{m}(f_{k}(_{j}) (_{k}))}{m}=_{k},\] (9)

where \(_{k}=^{n}(y_{ik}=1)}{n}\) and \(_{k}=^{n}(y_{ik}=0)}{n}\) are respectively the proportions of positive and negative labels in labeled data for class \(k\). Figure 1 provides an illustration of the comparison between three instance-aware pseudo-labeling methods and the CAP method. By utilizing CAT strategy, CAP is expected to assign pseudo-labels with the class distribution that approximates the true one.

In practice, to further improve the performance of CAP, one feasible solution is to discard a fraction of unreliable pseudo-labels with relatively low confidences, which may have a negative impact on the model training. Specifically, for any class \(k[q]\), we select top \(_{1}_{k}\) and \(_{0}_{k}\) proportion probable pseudo-labels, where \(_{1},_{0}\) are two parameters to control the reliable intervals of pseudo-labels. By substituting the two terms into the right sides of Eq.(9), we can obtain the thresholds correspondingly. In Section 5.4, we perform ablation experiments to study the influence of reliable intervals on the model performance.

## 4 Theoretical Analysis

In this section, we perform theoretical analyses for the proposed method. In general, the performance of pseudo-labeling depends mainly on two factors, i.e., the quality of the model predictions and the correctness of estimated class distribution. Our work focuses on the latter. Consider an extreme case, where the model predictions are perfect, i.e., the confidences of positive labels are always greater than that of negative labels. In such a case, we still need an appropriate threshold to precisely separate the positive and negative labels. This implies that we need to capture the true class distribution of unlabeled data in order to achieve desirable pseudo-labeling performance.

### Correctness of the Estimated Class Distribution

To study the correctness of estimated class distribution, we provide the following theorem, which gives an upper bound on the difference between the estimated class proportion \(_{k}\) and the true class proportion \(_{k}^{}\) (its proof is given in Appendix A). A similar result can be derived for \(_{k}\).

**Theorem 1**.: _Assume the estimated class proportion \(_{k}=_{i=1}^{n}(y_{ik}=1)\), and the true class proportion \(_{k}^{}=_{j=1}^{m}(y_{jk}=1)\) for any \(k[q]\), where \(n\) and \(m\) are the numbers of labeled and unlabeled examples that satisfy \(m>>n\). Then, with the probability larger than \(1-2n^{-1}-2m^{-1}\), we have, \( k[q],|_{k}-_{k}^{}|}{ }+}{}\)._

Theorem 1 tells us that the correctness of the estimated class distribution mainly depends on the number of labeled and unlabeled data. In general, the bound is dominated by the first term, since it always satisfies \(m>>n\). By neglecting the second term, we can see that \( k[q],_{k}^{}_{k}\) in the parametric rate \(_{p}(1/)\), where \(_{p}\) denotes the order in probability. Obviously, as the number of training examples increase, the estimated class distribution would quickly converge to the true one.

### Generalization Bound

Moreover, we study the generalization performance of CAP. Before providing the main results, we first define the true risk with respect to the classification model \(f(;)\):

\[R(f)=_{(,)}[(f(),)].\]Our goal is to learn a good classification model by minimizing the empirical risk \((f)=_{l}(f)+_{u}(f)\), where \(_{l}(f)\) and \(_{u}(f)\) are respectively the empirical risk of the labeled loss \(_{l}(f(),)\) and unlabeled loss \(_{u}(f(),)\):

\[_{l}(f)=_{i=1}^{n}(f(_{i} ),_{i}),_{u}(f)=_{j=1}^{m} _{u}(f(_{j}),_{j}).\]

Note that during the training, we cannot train a model directly by optimizing \(_{u}(f)\), since the labels of unlabeled data are inaccessible. Instead, we train the model with \(_{u}^{}(f)=_{j=1}^{m}_{u}(f(_ {j}),}_{j})\), where \(}_{j}\) represents the pseudo-label vector of the instance \(_{j}\).

Let \((f_{k}())=y_{k}_{1}(f_{k}())+(1-y_{k})_{0}(f_{k}())\) be the loss for the class \(k\), and \(L_{}\) be any (not necessarily the best) Lipschitz constant of \(\). Let \(_{N}()\) be the expected Rademacher complexity  of \(\) with \(N=m+n\) training points. Let \(=*{arg\,min}_{f}(f)\) be the empirical risk minimizer, where \(\) is a function class, and \(f^{}=*{arg\,min}_{f}R(f)\) be the true minimizer. We derive the following theorem, which provides a generalization error bound for the proposed method (its proof is given in Appendix B).

**Theorem 2**.: _Suppose that \(()\) is bounded by \(B\). For some \(>0\), if \(_{j=1}^{m}\|(f_{k}(x_{j})(_{k}))-(y_{jk }=1)|/m\) for any \(k[q]\), for any \(>0\), with probability at least \(1-\), we have_

\[R()-R(f^{}) 2qB+4qL_{}R_{N}()+2qB }{2N}}.\]

From Theorem 4, it can be observed that the generalization performance of \(\) mainly depends on two factors, _i.e._, the pseudo-labeling error \(\) and the number of training examples \(N\). Apparently, a smaller pseudo-labeling error \(\) often leads to better generalization performance. Thanks to its ability to capture the true class distribution, CAP can achieve a much smaller pseudo-labeling error \(\) than existing instance-aware pseudo-labeling methods, which is beneficial for obtaining better classification performance. This can be further validated by our empirical results in Section 5.3. The second factor is the number of training examples. As \(N\) and \( 0\), Theorem 4 shows that the empirical risk minimizer \(\) converges to the true risk minimizer \(f^{}\).

## 5 Experiments

In this section, we first perform experiments to validate the effectiveness of the proposed method; then, we perform ablation studies to analyze the mechanism behind CAP.

### Experimental Settings

DatasetsTo evaluate the performance of the propose method, we conduct experiments on three benchmark image datasets, including Pascal VOC-2012 (VOC for short) 2, MS-COCO-2014 (MS-COCO for short) 3, and NUS-WIDE (NUS for short) 4. The detailed information of these datasets can be found in the appendix. For each dataset, we randomly sample a proportion \(p\{0.05,0.1,0.15,0.2\}\) of examples with full labels while the others without any supervised information. Following the previous works [8; 53], we report the mean average precision (mAP) on the test set for each method.

Comparing methodsTo validate the effectiveness of the proposed method, we compare it with five groups of methods: 1) three instance-aware pseudo-labeling methods: **Top-1** (Eq.(2)), **Top-\(\)** (Eq.(3)), **IAT** (Eq.(4)); 2) two state-of-the-art MLML methods: **LL** (includes three variants **LL-R**, **LL-Ct**, and **LL-Cp**), **PLC**; 3) Two state-of-the-art SSL methods: **Adsh**, **FreeMatch**; 4) One state-of-the-art SSMLL method: **DRML**; 5) Two baseline methods, **BCE**, **ASL**. DRML is the only deep SSMLL method whose source code could be found on the Internet.

Furthermore, most MLML methods cannot be applied to the SSMLL scenario, since they assume that a subset of labels have been annotated for each training instance. The detailed information of these methods can be found in the appendix.

ImplementationWe employ ResNet-50  pre-trained on ImageNet  for training the classification model. We adopt RandAugment  and Cutout  for data augmentation. We employ AdamW  optimizer and one-cycle policy scheduler  to train the model with maximal learning rate of 0.0001. The number of warm-up epochs is set as 12 for all datasets. The batch size is set as 32, 64, and 64 for VOC, MS-COCO, and NUS. Furthermore, we perform exponential moving average (EMA) for the model parameter \(\) with a decay of 0.9997. For all methods, we use the ASL loss as the base loss function, since it shows superiority to BCE loss . We perform all experiments on GeForce RTX 3090 GPUs. The random seed is set to 1 for all experiments.

### Comparison Results

Table 1 and Table 2 report the comparison results between CAP and the comparing methods in terms of mAP on VOC, COCO, and NUS. From the tables, we can see that: 1) DRML obtains unfavorable performance, even worse than baselines BCE and ASL, since it performs two-stage training, which may destroy its representation learning. The original paper did not report the results on these three datasets. Therefore, it is rather important to design an effective SSMLL method in deep learning paradigm. 2) CAP outperforms three instance-aware pseudo-labeling methods, which demonstrates that by utilizing CAT strategy, CAP can precisely estimate the class distribution of unlabeled data and thus obtain desirable pseudo-labeling performance. 3) The performance of CAP is better than that of two state-of-the-art SSL methods. To achieve better performance, we have made several modifications for these two methods not limited to the following: a) use the ASL loss ; b) adopt stronger data augmentations; c) change the training scheme to make them more suitable for the multi-label scenario. 4) CAP achieves the best performance in all cases and significantly outperforms

    &  &  \\   & \(p=0.05\) & \(p=0.1\) & \(p=0.15\) & \(p=0.2\) & \(p=0.05\) & \(p=0.1\) & \(p=0.15\) & \(p=0.2\) \\  BCE & 67.95 & 75.35 & 78.19 & 79.38 & 58.90 & 63.75 & 65.91 & 67.33 \\ ASL & 71.46 & 78.00 & 79.69 & 80.77 & 59.12 & 63.82 & 66.10 & 67.51 \\  LL-R & 75.69 & 80.96 & 82.31 & 83.55 & 59.31 & 64.25 & 66.61 & 68.01 \\ LL-Ct & 75.77 & 81.04 & 82.31 & 83.50 & 59.33 & 64.23 & 66.69 & 68.11 \\ LL-Cp & 75.79 & 81.03 & 82.36 & 83.68 & 59.27 & 64.19 & 66.68 & 68.12 \\ PLC & 74.49 & 80.35 & 82.35 & 83.39 & 59.85 & 65.03 & 67.62 & 69.14 \\  Top-1 & 75.77 & 80.78 & 82.65 & 83.72 & 57.62 & 62.84 & 65.50 & 66.96 \\ Top-\(k\) & 75.07 & 80.20 & 81.99 & 83.16 & 58.25 & 63.52 & 66.11 & 67.49 \\ IAT & 73.24 & 80.27 & 82.39 & 83.55 & 60.34 & 65.54 & 67.88 & 69.25 \\  ADSH & 75.37 & 80.34 & 82.80 & 83.93 & 60.75 & 65.37 & 67.70 & 69.01 \\ FreeMatch & 75.11 & 80.66 & 82.63 & 83.60 & 59.94 & 64.46 & 66.79 & 68.04 \\  DRML & 61.77 & 71.01 & 72.98 & 74.49 & 53.60 & 57.06 & 58.53 & 59.24 \\  Ours & **76.16** & **82.16** & **83.48** & **84.41** & **62.43** & **67.36** & **69.11** & **70.41** \\   

Table 1: Comparison results on VOC and COCO in terms of mAP (%). The best performance is highlighted in bold.

   Method & ASL & LL-R & PLC & Top-1 & Top-\(k\) & IAT & ADSH & FreeMatch & DRML & Ours \\  \(p=0.05\) & 42.87 & 40.20 & 43.55 & 40.99 & 40.89 & 42.58 & 43.94 & 43.12 & 30.61 & **44.82** \\ \(p=0.10\) & 46.50 & 44.95 & 47.51 & 45.07 & 45.04 & 46.60 & 47.28 & 46.65 & 35.09 & **48.24** \\ \(p=0.15\) & 48.42 & 47.32 & 49.75 & 47.43 & 47.22 & 48.76 & 49.22 & 48.74 & 37.91 & **49.90** \\ \(p=0.20\) & 49.65 & 48.31 & 50.71 & 48.49 & 48.37 & 49.62 & 49.93 & 49.59 & 39.98 & **51.06** \\   

Table 2: Comparison results on NUS in terms of mAP (%). The best performance is highlighted in bold.

the comparing methods, especially when the number of labeled examples is small. These results convincingly validate the effectiveness of the proposed method.

### Study on the Performance of Pseudo-Labeling

In this section, we explain why CAP is better than the conventional instance-aware pseudo-labeling methods. Figure 2 illustrates the performance of different pseudo-labeling methods in terms of CF1 score on VOC and COCO. From the figures, we can see that CAP achieves the best performance in all cases. As discussed above, the pseudo-labeling performance mainly depends on two factors, _i.e._, the quality of model predictions and the correctness of estimated class proportions. CAP improves the pseudo-labeling performance by precisely estimating the class distribution. An interesting observation is that at the first epoch, when the model predictions are the same for four methods, our method significantly outperforms the comparing methods, since it is able to capture the true class proportions. These results validate that CAP can achieve better pseudo-labeling performance.

### Study on the Influence of Reliable Interval

As mentioned above, to improve the performance, instead of using all pseudo-labels, we can train the model with only reliable pseudo-labels within the reliable intervals that are controlled by \(_{1}\), \(_{0}\). Figure 3 illustrates the performance of CAP as \(_{1}\) and \(_{0}\) change in the ranges of \([0.8,0.85,0.9,0.95,1]\) and \([0.95,0.96,0.97,0.98,0.99,1]\). From the figures, it can be observed that discarding the unreliable positive pseudo-labels would improve the performance, but discarding the unreliable negative pseudo-labels would degrade the performance. One possible reason behinds the phenomenon is due to the significant positive-negative imbalance in MLL data, _i.e._, the number of negative labels is often much greater than that of positive labels. This leads the model to be sensitive to false positive labels, while be robust to false negative labels. In our main experiments (Table 1 and Table 2), we set

Figure 3: Performance of CAP on VOC and COCO in terms of mAP (%) with the increase of \(_{1}\) and \(_{0}\).

Figure 2: Pseudo-labeling performance in terms of CF1 score on VOC, COCO. Each row corresponds one dataset.

\(_{1}=1,_{0}=1\). In practice, we are expected to achieve better performance by tuning the parameter \(_{1}\).

## 6 Conclusion

The paper studies the problem of semi-supervised multi-label learning, which aims to train a multi-label classifier by leveraging the information of unlabeled data. Different from the conventional instance-aware pseudo-labeling methods, we propose to assign pseudo-labels to unlabeled instances in a class-aware manner, with the aim of capturing the true class distribution of unlabeled data. Towards this goal, we propose the CAT strategy to obtain an estimated class distribution, which has been proven to be a desirable estimation of the true class distribution based on our observations. Theoretically, we first perform an analysis on the correctness of estimated class distribution; then, we provide the generalization error bound for CAP and show its dependence to the pseudo-labeling performance. Extensive experimental results on multiple benchmark datasets validate that CAP can achieve state-of-the-art performance.

In general, the performance of pseudo-labeling depends mainly on two factors, i.e., the quality of the model predictions and the correctness of the estimated class distribution. This work focuses on the latter. In future, we plan to boost the performance of SSMLL by improving the quality of model predictions.