# Gaussian Mixture Solvers for Diffusion Models

Hanzhong Guo\({}^{*}\)\({}^{1,3}\), Cheng Lu\({}^{4}\), Fan Bao\({}^{4}\), Tianyu Pang\({}^{2}\), Shuicheng Yan\({}^{2}\),

**Chao Du\({}^{1}\)\({}^{2}\), Chongxuan Li\({}^{1,3}\)**

\({}^{1}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\)Sea AI Lab, Singapore

\({}^{3}\)Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

\({}^{4}\)Tsinghua University

{allanguo, tianyupang, yansc, duchao}@sea.com; lucheng.lc15@gmail.com; bf19@mails.tsinghua.edu.cn; chongxuanli@ruc.edu.cn

Work done during an internship at Sea AI Lab. \({}^{}\)Correspondence to Chao Du and Chongxuan Li.

###### Abstract

Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called _Gaussian Mixture Solvers (GMS)_ for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture transition kernel using generalized methods of moments in each step during sampling. Empirically, our solver outperforms numerous SDE-based solvers in terms of sample quality in image generation and stroke-based synthesis in various diffusion models, which validates the motivation and effectiveness of GMS. Our code is available at https://github.com/Guohanzhong/GMS.

## 1 Introduction

In recent years, deep generative models and especially (score-based) diffusion models [35; 14; 37; 18] have made remarkable progress in various domains, including image generation [15; 7], audio generation [22; 32], video generation , 3D object generation , multi-modal generation [42; 5; 12; 43], and several downstream tasks such as image translation [28; 45] and image restoration .

Sampling from diffusion models can be interpreted as solving the reverse-time diffusion stochastic differential equations (SDEs) or their corresponding probability flow ordinary differential equations (ODEs) . SDE-based and ODE-based solvers of diffusion models have very different properties and application scenarios In particular, SDE-based solvers usually perform better when given a sufficient number of discretization steps . Indeed, a recent empirical study  suggests that SDE-based solvers can potentially generate high-fidelity samples with realistic details and intricate semantic coherence from pre-trained large-scale text-to-image diffusion models . Besides, SDE-based solvers are preferable in many downstream tasks such as stroke-based synthesis , image translation , and image manipulation .

Despite their wide applications, SDE-based solvers face a significant trade-off between efficiency and effectiveness during sampling, since an insufficient number of steps lead to larger discretization errors. To this end, Bao et al.  estimate the optimal variance of the Gaussian transition kernel in the reverse process instead of using a handcraft variance to reduce the discretization errors. Additionally, Bao et al.  explore the optimal diagonal variance when dealing with an imperfect noise network and demonstrate state-of-the-art (SOTA) performance with a few steps among other SDE-based solvers. Notably, these solvers all assume that the transition kernel in the reverse process is Gaussian.

In this paper, we systematically examine the assumption of the Gaussian transition kernel and reveal that it can be easily violated under a limited number of discretization steps even in the case of simple mixture data. To this end, we propose a new type of SDE-based solver called _Gaussian Mixture Solver (GMS)_, which employs a more expressive Gaussian mixture transition kernel in the reverse process for better approximation given a limited number of steps (see visualization results on toy data in Fig. 1). In particular, we first learn a noise prediction network with multiple heads that estimate the high-order moments of the true reverse transition kernel respectively. For sampling, we fit a Gaussian mixture transition kernel in each step via the _generalized methods of the moments_ using the predicted high-order moments of the true reverse process.

To evaluate GMS, we compare it against a variety of baselines [14; 2; 17; 36] in terms of several widely used metrics (particularly sample quality measured by FID) in the tasks of generation and stroke-based synthesis on multiple datasets. Our results show that GMS outperforms state-of-the-art SDE-based solvers [14; 2; 17] in terms of sample quality with the limited number of discretization steps (e.g., \(<100\)). For instance, GMS improves the FID by 4.44 over the SOTA SDE-based solver  given 10 steps on CIFAR10. Furthermore, We evaluate GMS on a stroke-based synthesis task. The findings consistently reveal that GMS achieves higher levels of realism than all aforementioned SDE-based solvers as well as the widely adopted ODE-based solver DDIM  while maintaining comparable computation budgets and faithfulness scores (measured by \(L_{2}\) distance). Such empirical findings validate the motivation and effectiveness of GMS.

## 2 Background

In this section, we provide a brief overview of the (score-based) diffusion models, representative SDE-based solvers for diffusion models, and applications of such solvers.

### Diffusion models

Diffusion models gradually perturb data with a forward diffusion process and then learn to reverse such process to recover the data distribution. Formally, let \(x_{0}^{n}\) be a random variable with unknown data distribution \(q(x_{0})\). Diffusion models define the forward process \(\{x_{t}\}_{t}\) indexed by time \(t\), which perturbs the data by adding Gaussian noise to \(x_{0}\) with

\[q(x_{t}|x_{0})=(x_{t}|a(t)x_{0},^{2}(t)I).\] (1)

Figure 1: **Sampling on a mixture of Gaussian.** GMS (**ours**) and SN-DDPM excel in fitting the true distribution when the transition kernel is Gaussian, with a sufficient number of sampling steps (c). However, GMS outperforms SN-DDPM  when sampling steps are limited and the reverse transition kernel deviates from Gaussian (a-b).

In general, the function \(a(t)\) and \((t)\) are selected so that the logarithmic signal-to-noise ratio \((t)}{^{2}(t)}\) decreases monotonically with time \(t\), causing the data to diffuse towards random Gaussian noise . Furthermore, it has been demonstrated by Kingma et al.  that the following SDE shares an identical transition distribution \(q_{t|0}(x_{t}|x_{0})\) with Eq. (1):

\[x_{t}=f(t)x_{t}t+g(t), x_{0} q(x_ {0}),\] (2)

where \(^{n}\) is a standard Wiener process and

\[f(t)= a(t)}{t}, g^{2}(t)= ^{2}(t)}{t}-2^{2}(t) a(t)}{t}.\] (3)

Let \(q(x_{t})\) be the marginal distribution of the above SDE at time \(t\). Its reversal process can be described by a corresponding continuous SDE which recovers the data distribution :

\[x=[f(t)x_{t}-g^{2}(t)_{x_{t}} q(x_{t})] t+g(t), x_{1} q(x_{1}),\] (4)

where \(^{n}\) is a reverse-time standard Wiener process. The only unknown term in Eq. (4) is the score function \(_{x_{t}} q(x_{t})\). To estimate it, existing works [14; 37; 18] train a noise network \(_{}(x_{t},t)\) to obtain a scaled score function \((t)_{x_{t}} q(x_{t})\) using denoising score matching :

\[()=_{0}^{1}w(t)_{q(x_{0})}_{q( )}[\|_{}(x_{t},t)-\|_{2}^{2}]t,\] (5)

where \(w(t)\) is a weighting function, \(q()\) is standard Gaussian distribution and \(x_{t} q(x_{t}|x_{0})\) follows Eq. (1). The optimal solution of the optimization objective Eq. (5) is \(_{}(x_{t},t)=-(t)_{x_{t}} q(x_{t})\).

Hence, samples can be obtained by initiating the process with a standard Gaussian variable \(x_{1}\), then substituting \(_{x_{t}} q(x_{t})\) with \(-(x_{t},t)}{(t)}\) and discretizing reverse SDE Eq. (4) from \(t=1\) to \(t=0\) to generate \(x_{0}\).

### SDE-based solvers for diffusion models

The primary objective of SDE-based solvers lies in decreasing discretization error and therefore minimizing function evaluations required for convergence during the process of discretizing Eq. (4). Discretizing the reverse SDE in Eq. (4) is equivalent to sample from a Markov chain \(p(x_{0:1})=p(x_{1})_{t_{i-1},t_{i}_{t_{i}}}p(x_{t_{i-1}}|x_ {t_{i}})\) with its trajectory \(S_{t}=[0,t_{1},t_{2},...,t_{i},..,1]\). Song et al.  proves that the conventional ancestral sampling technique used in the DPMs  that models \(p(x_{t_{i-1}}|x_{t_{i}})\) as a Gaussian distribution, can be perceived as a first-order solver for the reverse SDE in Eq. (4). Bao et al.  finds that the optimal variance of \(p(x_{t_{i-1}}|x_{t_{i}})(x_{t_{i-1}}|_{t_{i-1}|t_{i}}, _{t_{i-1}|t_{i}}(x_{t_{i}}))\) is

\[^{*}_{t_{i-1}|t_{i}}(x_{t_{i}})=_{t_{i}}^{2}+_{t_{i}}^{2} (t_{i})}{(t_{i})}(1-_{q(x_{t_{i}})}[ \|_{q(x_{0}|x_{t_{i}})}[_{}(x_{t_{i}}, t_{i})]\|_{2}^{2}]),\] (6)

where, \(_{t_{i}}=)}-(t_{i-1})-_{t_{i} }^{2}})}{^{2}(t_{i})}}\), \(_{t_{i}}^{2}\) is the variance of \(q(x_{t_{i-1}}|x_{t_{i}},x_{0})\). AnalyticDPM  offers a significant reduction in discretization error during sampling and achieves faster convergence with fewer steps. Moreover, SN-DDPM  employs a Gaussian transition kernel with an optimal diagonal covariance instead of an isotropic covariance. This approach yields improved sample quality and likelihood compared to other SDE-based solvers within a limited number of steps.

### Applications of SDE-based solvers for stroke-based synthesis

The stroke-based image synthesis is a representative downstream task suitable for SDE-based solvers. It involves the user providing a full-resolution image \(x^{(g)}\) through the manipulation of RGB pixels, referred to as the guided image. The guided image \(x^{(g)}\) possibly contains three levels of guidance: a high-level guide consisting of coarse colored strokes, a mid-level guide comprising colored strokes on a real image, and a low-level guide containing image patches from a target image.

SDEdit  solves the task by first starting from the guided image \(x^{(g)}\) and adding Gaussian noise to disturb the guided images to \(x_{t}\) and \(q(x^{(g)}(t_{0})|x^{(g)})(x_{t_{0}}|a(t_{0})x^{(g)},^{2} (t)I)\) same with Eq. (1). Subsequently, it solves the corresponding reverse stochastic differential equation (SDE) up to \(t=0\) to generate the synthesized image \(x(0)\) discretizing Eq. (4). Apart from the discretization steps taken by the SDE-based solver, the key hyper-parameter for SDEdit is \(t_{0}\), the time step from which we begin the image synthesis procedure in the reverse SDE.

## 3 Gaussian mixture solvers for diffusion models

In this section, we first show through both theoretical and empirical evidence, that the true reverse transition kernel can significantly diverge from Gaussian distributions as assumed in previous SOTA SDE-based solvers [3; 2], indicating that the reverse transition can be improved further by employing more flexible distributions (see Sec. 3.1). This motivates us to propose a novel class of SDE-based solvers, dubbed _Gaussian Mixture Solvers_, which determines a Gaussian mixture distribution via the _generalized methods of the moments_ using higher-order moments information for the true reverse transition (see Sec. 3.2). The higher-order moments are estimated by a noise prediction network with multiple heads on training data, as detailed in Sec. 3.3.

### Suboptimality of Gaussian distributions for reverse transition kernels

As described in Sec. 2, existing state-of-the-art SDE-based solvers for DPMs [3; 2] approximate the reverse transition \(q(x_{t_{i-1}}|x_{t_{i}})\) using Gaussian distributions. Such approximations work well when the number of discretization steps in these solvers is large (e.g., \(1000\) steps). However, for smaller discretization steps (such as when faster sampling is required), the validity of this Gaussian assumption will be largely broken. We demonstrate this theoretically and empirically below.

First, observe that \(q(x_{s}|x_{t})\)1 can be expressed as \(q(x_{s}|x_{t})= q(x_{s}|x_{t},x_{0})q(x_{0}|x_{t})x_{0}\), which is non-Gaussian for a general data distribution \(q(x_{0})\). For instance, for a mixture of Gaussian or a mixture of Dirac \(q(x_{0})\), we can prove that the conditional distributions \(q(x_{s}|x_{t})\) in the reverse process are non-Gaussian, as characterized by Proposition 3.1, proven in Appendix A.1.

**Proposition 3.1** (Mixture Data Have non-Gaussian Reverse Kernel).: _Assume \(q(x_{0})\) is a mixture of Dirac or a mixture of Gaussian distribution and the forward process is defined in Eq. (1). The reverse transition kernel \(q(x_{s}|x_{t}),s<t\) is a Gaussian mixture instead of a Gaussian._

Empirically, we do not know the distributions of the real data (e.g., high-dimensional images) and cannot obtain an explicit form of \(q(x_{s}|x_{t})\). However, even in such cases, it is easy to validate that \(q(x_{s}|x_{t})\) are non-Gaussian. In particular, note that the third-order moment of one Gaussian distribution (\(M_{3}^{(G)}\)) can be represented by its first-order moment (\(M_{1}\)) and its second-order moment (\(M_{2}\)) 2, which motivates us to check whether the first three order moments of \(q(x_{s}|x_{t})\) satisfy the relationship induced by the Gaussian assumption. We perform an experiment on CIFAR-10 and estimate the first three orders of moments \(}\), \(}\), \(}\) for \(q(x_{s}|x_{t})\) from data by high-order noise networks (see details in Sec. D.2). As shown in Fig. 2, we plot the mean and median of \(l_{2}\)-norm

Figure 2: **Empirical evidence of suboptimality of Gaussian kernel on CIFAR10.** (a) and (b) plot the logarithm of the image-wise mean and median of \(L_{2}\)-norm between Gaussian-assumed third-order moments and estimated third-order moments. Clearly, as the number of sampling steps decreases, the disparity between the following two third-order moments increases, denoting that the true transition kernel diverges further from the Gaussian distribution. See Appendix D.2 for more details.

between the estimated third-order moment \(_{3}\) and third-order moment \(M_{3}^{(G)}\) calculated under the Gaussian assumption given the different number of steps at different time steps \(t\). In particular, when the number of steps is large (i.e., \(\#\)Step=\(1000\)), the difference between the two calculation methods is small. As the number of steps decreases, the \(l_{2}\)-norm increases, indicating that the true reverse distribution \(q(x_{s}|x_{t})\) is non-Gaussian. With the time step closer to \(t=0\), the \(l_{2}\)-norm increases too.

Both the theoretical and empirical results motivate us to weaken the Gaussian assumption in SDE-based solvers for better performance, especially when the step size is large.

### Sampling with Gaussian mixture transition kernel

There are extensive choices of \(p(x_{s}|x_{t})\) such that it is more powerful and potentially fits \(q(x_{s}|x_{t})\) better than a Gaussian. In this paper, we choose a simple mixture of the Gaussian model as follows:

\[p(x_{s}|x_{t})=_{i=1}^{M}w_{i}(x_{s}|_{i}(x_{t}),_{i}( x_{t})),_{i=1}^{M}w_{i}=1,\] (7)

where \(w_{i}\) is a scalar and \(_{i}(x_{t})\) and \(_{i}(x_{t})\) are vectors. The reasons for choosing a Gaussian mixture model are three-fold. First, a Gaussian mixture model is multi-modal (e.g., see Proposition 3.1), potentially leading to a better performance than Gaussian with few steps. Second, when the number of steps is large and the Gaussian is nearly optimal [35; 3], a designed Gaussian mixture model such as our proposed kernel in Eq. (9) can degenerate to a Gaussian when the mean of two components are same, making the performance unchanged. Third, a Gaussian mixture model is relatively easy to sample. For the sake of completeness, we also discuss other distribution families in Appendix C.

Traditionally, the EM algorithm is employed for estimating the Gaussian mixture . However, it is nontrivial to apply EM here because we need to learn the reverse transition kernel in Eq. (7) for all time step pairs \((s,t)\) by individual EM processes where we need to sample multiple \(x_{s}\)3 given a \(x_{t}\) to estimate the parameters in the Gaussian mixture indexed by \((s,t)\). This is time-consuming, especially in a high-dimensional space (e.g., natural images) and we present an efficient approach.

For improved flexibility in sampling and training, diffusion models introduce the parameterization of the noise network \((x_{t},t)\) or the data prediction network \(x_{0}(x_{t},t)\). With such a network, the moments under the \(q(x_{s}|x_{t})\) measure can be decomposed into moments under the \(q(x_{0}|x_{t})\) measure so that sampling any \(x_{t}\) to \(x_{s}\) requires only a network whose inputs are \(x_{t}\) and \(t\), such as the decomposition shown in Eq. (10). In previous studies, Gaussian transition kernel was utilized, allowing for the distribution to be directly determined after obtaining the estimated first-order moment and handcrafted second-order moment  or estimated first-order and second-order moment . In contrast, such a feature is not available for the Gaussian mixtures transition kernel in our paper.

Here we present the method to determine the Gaussian mixture given a set of moments and we will discuss how to obtain the moments by the parameterization of the noise network in Sec. 3.3. Assume the length of the estimated moments set is \(N\) and the number of parameters in the Gaussian mixture is \(d\). We adopt a popular and theoretically sound method called the generalized method of moments (GMM)  to learn the parameters by:

\[_{}Q(,M_{1},...,M_{N})=_{}[}_{i=1} ^{N_{c}}g(x_{i},)]^{T}W[}_{i=1}^{N_{c}}g(x_{i},)],\] (8)

where \(^{d 1}\) includes all parameters (e.g., the mean of each component) in the Gaussian mixture defined in Eq. (7). For instance, \(d=2M*D_{}+M-1\) in Eq. (7), where \(D_{}\) represents the number of dimensions of the data and \(\) contains \(M\) mean vectors with \(D_{}\) dimensions, \(M\) variance vectors of with \(D_{}\) dimensions (considering only the diagonal elements), and \(M-1\) weight coefficients which are scalar. The component of \(g(x_{i},)^{N 1}\) is defined by \(g_{n}(x_{i},)=M_{n}(x_{i})-M_{n}^{(GM)}()\), where \(M_{n}(x_{i})\) is the \(n\)-th order empirical moments stated in Sec. 3.3 and \(M_{n}^{(GM)}()\) is the \(n\)-th order moments of Gaussian mixture under \(\), and \(W\) is a weighted matrix, \(N_{c}\) is the number of samples.

Theoretically, the parameter \(_{}\) obtained by GMM in Eq. (8) consistently converges to the potential optimal parameters \(^{*}\) for Gaussian mixture models given the moments' condition because \(}(_{}-^{*}) (0,(_{GMM}))\), as stated in Theorem 3.1 in Hansen .

Hence, we can employ GMM to determine a Gaussian mixture transition kernel after estimating the moments of the transition kernel. To strike a balance between computational tractability and expressive power, we specifically focus on the first three-order moments in this work and define a Gaussian mixture transition kernel with two components shown in Eq. (9) whose vectors \(_{t}^{(1)}\), \(_{t}^{(2)}\), and \(_{t}^{2}\) are parameters to be optimized. This degree of simplification is acceptable in terms of its impact. Intuitively, such a selection has the potential to encompass exponential modes throughout the entire trajectory. Empirically, we consistently observe that utilizing a bimodal Gaussian mixture yields favorable outcomes across all experimental configurations.

\[p(x_{s}|x_{t})=(_{t}^{(1)},_{t}^{2})+(_{t}^{(2)},_{t}^{2}),\] (9)

meanwhile, under the simplification in our paper, the number of parameters \(d=3*D_{}\) in our Gaussian transition kernel is equal to the number of moments condition \(N=3*D_{}\). According to proposition 3.2, under the selection of arbitrary weighted weights, the asymptotic mean (asymptoticly consistent) and asymptotic variance of the estimator remain consistent, proof in Appendix A.2. Hence, any choice of weighted weights is optimal. To further streamline optimization, we set \(W=I\).

**Proposition 3.2** (Any weighted matrix is optimal for \(d=N\)).: _Assume the number of parameters \(d\) equals the number of moments condition \(N\), and \(_{b}^{}(x_{t},t)_{q(x_{0}|x_{t})}[ (^{n-1})]\) (where \(^{n}\) denotes \(n\)-fold outers product) which denotes \(n\)-th order noise network converging in probability. The asymptotic variance \((_{GMM})\) and the convergence speed of GMM remain the same no matter which weighted matrix is adopted in Eq. (8). Namely, any weighted matrix is optimal._

What's more, we provide a detailed discussion on the selection of parameters such as the choice of different parameters to optimize and the different choices of weight \(w_{i}\) in the Gaussian mixture in Appendix E.1. Combining with Sec. 4.1, our empirical findings illustrate the efficacy of the GMS across various benchmarks via using the Gaussian transition kernel in Eq. (9) fitted by the objective function shown in Eq. (31) in Appendix B.1 via the ADAN  as optimization method. Details regarding the parameters for this optimizer are provided in Appendix E.5.

### Estimating high-order moments for non-Gaussian reverse process

In Sec. 3.2, we have explored the methodology for determining a Gaussian mixture transition kernel given a set of moments \(M_{1},...,M_{n}\). In the subsequent section, we will present an approach for estimating these moments utilizing noise networks and elucidate the process of network learning.

Given the forward process described by Eq. (1), it can be inferred that both \(q(x_{t}|x_{s})\) and \(q(x_{s}|x_{t},x_{0})\) follow Gaussian distributions. Specifically, \(q(x_{t}|x_{s})(x_{t}|a_{t|s}x_{s},_{t|s}^{2})\), where \(a_{t|s}=\) and \(_{t|s}^{2}=^{2}(t)-a_{t|s}^{2}^{2}(s)\). Consequently, we can deduce that \(_{q(x_{s}|x_{t})}[x_{s}^{n}x_{s}]=_{q(x_{0}|x_{t})q (x_{s}|x_{t},x_{0})}[x_{s}^{n}x_{s}]\), where \(^{n}\) denotes \(n\)-fold outer product. Thus, we can first utilize the Gaussian property of \(q(x_{s}|x_{t},x_{0})\) and employ an explicit formula to calculate the moments under the measure of \(q(x_{s}|x_{t},x_{0})\). What's more, we only consider the diagonal elements of higher-order moments for computational efficiency, similar to Bao et al.  since estimating full higher-order moments results in escalated output dimensions (e.g., quadratic growth for covariance and cubic for the third-order moments) and thus requires substantial computational demands. The expression of diagonal elements of the third-order moment of the reverse transition kernel can be derived as:

\[_{3}=_{q(x_{s}|x_{t})}[(x_{s} x _{s} x_{s})]=_{q(x_{0}|x_{t})}_{q(x_{s}|x_{t},x_{0} )}[(x_{s} x_{s} x_{s})]=\] (10) \[_{s}^{2}}{_{t}^{2}})^{3} (x_{t} x_{t} x_{t})+3_{t}^{2} _{s}^{2}}{_{t}^{2}}x_{t}]}_{}\] \[+^{2}_{s}^{4}a_{s}^{2} _{t|s}^{2}}_{_{t}^{2}}((x_{t} x_{t}))+ _{t|s}}{_{t}^{2}}I]_{q(x_{0}|x_{t})}[x_{0}]}_{ {Linear term in }x_{0}}\] \[+3_{s}^{2}}{_{t}^{2}}(_{t|s}}{_{t}^{2}})^{2}x_{t}_{q(x_{0}|x_{t})}[ (x_{0} x_{0})]}_{x_{0}}+_{t|s}}{ _{t}^{2}})^{3}_{q(x_{0}|x_{t})}[(x_{0} x_{0 } x_{0})]}_{x_{0}},\]where \(\) is the outer product \(\) is the Hadamard product. Additionally, \(_{t}^{2}\) corresponds to the variance of \(q(x_{s}|x_{t},x_{0})\), and further elaboration on this matter can be found in Appendix B.

In order to compute the \(n\)-th order moment \(_{n}\), as exemplified by the third-order moment in Eq. (10), it is necessary to evaluate the expectations \(_{q(x_{0}|x_{t})}[x_{0}],,_{q(x_{0}|x_{t})}[(x_{0}^{n-1}x_{0})]\). Furthermore, by expressing \(x_{0}\) as \(x_{0}=}(x_{t}-(t))\), we can decompose \(_{n}\) into a combination of terms \(_{q(x_{0}|x_{t})}[],,_{q(x_{0}|x_{t})}[( ^{n-1})]\), which are learned by neural networks. The decomposition of third-order moments \(_{3}\), as outlined in Eq. (29), is provided to illustrate this concept. Therefore in training, we learn several neural networks \(\{_{}^{n}\}_{n=1}^{N}\) by training on the following objective functions:

\[_{\{_{}^{n}\}_{n=1}^{N}}_{t}_{q(x_{0})q()}\|^{n}-_{}^{n}(x_{t},t) \|_{2}^{2},\] (11)

where \((0,I)\), \(x_{t}=(t)x_{0}+(t)\), and \(^{n}\) denotes \(^{n-1}\). After training, we can use \(\{_{}^{n}\}_{n=1}^{N}\) to replace \(\{_{q(x_{0}|x_{t})}[^{n-1}]\}_{n=1}^{N}\) and estimate the moments \(\{_{n}\}_{n=1}^{N}\) of reverse transition kernel.

However, in the present scenario, it is necessary to infer the network a minimum of \(n\) times in order to make a single step of GMS. To mitigate the high cost of the aforementioned overhead in sampling, we adopt the two-stage learning approach proposed by Bao et al. . Specifically, in the first stage, we optimize the noise network \(_{}(x_{t},t)\) by minimizing the expression \(_{t}_{q(x_{0})q()}\|-_{ }(x_{t},t)\|_{2}^{2}\) or by utilizing a pre-trained noise network as proposed by . In the second stage, we utilize the optimized network as the backbone and keep its parameters \(\) fixed, while adding additional heads to generate the \(n\)-th order noise network \(_{,_{n}}^{n}(x_{t},t)\).

\[_{,_{n}}^{n}(x_{t},t)=(_{ }(x_{t},t),_{n}),\] (12)

where NN is the extra head, which is a small network such as convolution layers or small attention block, parameterized by \(_{n}\), details in Appendix E.3. We present the second stage learning procedure in Algorithm 1. Upon training all the heads, we can readily concatenate the outputs of different heads, as the backbone of the higher-order noise network is shared. By doing so, we obtain the assembled noise network \(f^{N}(x_{t},t)\). When estimating the \(k\)-th order moments, it suffices to extract only the first \(k\) components of the assembled noise network.

\[f^{N}(x_{t},t)=((x_{t},t),_{,_{2}}^{2}(x_{t},t),..,_{, _{k}}^{k}(x_{t},t)}_{_{k}},.., _{,_{N}}^{N}(x_{t},t)]),\] (13)To this end, we outline the GMS sampling process in Algorithm 2, where \(f_{}^{3}(x_{t},t)\) represents \(([_{}(x_{t},t),_{,_{2}}^{2}(x_{t},t )])\). In comparison to existing methods with the same network structure, we report the additional memory cost of the assembled noise network in Appendix E.6.

## 4 Experiment

In this section, we first illustrate that GMS exhibits superior sample quality compared to existing SDE-based solvers when using both linear and cosine noise schedules [14; 30]. Additionally, we evaluate various solvers in stroke-based image synthesis (i.e., SDEdit) and demonstrate that GMS surpasses other SDE-based solvers, as well as the widely used ODE-based solver DDIM .

### Sample quality on image data

In this section, we conduct a quantitative comparison of sample quality using the widely adopted FID score . Specifically, we evaluate multiple SDE-based solvers, including a comparison with DDPM  and Extended AnalyticDPM  (referred to as SN-DDPM) using the even trajectory.

As shown in Tab. 1, GMS demonstrates superior performance compared to DDPM and SN-DDPM under the same number of steps in CIFAR10 and ImageNet 64 \(\) 64. Specifically, GMS achieves a remarkable 4.44 improvement in FID given 10 steps on CIFAR10. Appendix E.7 illustrates in more detail the improvement of GMS when the number of sampling steps is limited. Meanwhile, we conduct GMS in ImageNet 256 \(\) 256 via adopting the U-ViT-Huge  backbone as the noise network in Appendix E.8. Furthermore, taking into account the additional time required by GMS, our method still exhibits improved performance, as detailed in Appendix E.9. For integrity, we provide a comparison with other SDE-based solvers based on continuous time diffusion such as Gotta Go Fast , EDM  and SEED  in Appendix E.10 and shows that GMS largely outperforms other SDE-based solvers when the number of steps is less than 100. In Appendix G, we provide generated samples from GMS.

### Stroke-based image synthesis based on SDEdit 

**Evaluation metrics.** We evaluate the editing results based on realism and faithfulness similar with Meng et al. . To quantify the realism of sample images, we use FID between the generated images and the target realistic image dataset. To quantify faithfulness, we report the \(L_{2}\) distance summed over all pixels between the stroke images and the edited output images.

SDEdit  applies noise to the stroke image \(x^{g}\) at time step \(t_{0}\) using \((x_{t_{0}}^{g}|(t)x^{g},^{2}(t)I)\) and discretize the reverse SDE in Eq. (4) for sampling. Fig. 9 demonstrates the significant impact of \(t_{0}\) on the realism of sampled images. As \(t_{0}\) increases, the similarity to real images decreases. we choose

    &  \\  \# timesteps \(K\) & 10 & 20 & 25 & 40 & 50 & 100 & 200 & 1000 \\  DDPM, \(_{t}\) & 43.14 & 25.28 & 21.63 & 15.24 & 15.21 & 10.94 & 8.23 & 5.11 \\ DDPM, \(_{t}\) & 233.41 & 168.22 & 125.05 & 82.31 & 66.28 & 31.36 & 12.96 & 3.04 \\ SN-DDPM & 21.87 & 8.32 & 6.91 & 4.99 & 4.58 & 3.74 & 3.34 & 3.71 \\ GMS (**ours**) & **17.43** & **7.18** & **5.96** & **4.52** & **4.16** & **3.26** & **3.01** & **2.76** \\    
    &  \\  \# timesteps \(K\) & 10 & 25 & 50 & 100 & 200 & 1000 & 25 & 50 & 100 & 200 & 400 & 4000 \\  DDPM, \(_{t}\) & 34.76 & 16.18 & 11.11 & 8.38 & 6.66 & 4.92 & 29.21 & 21.71 & 19.12 & 17.81 & 17.48 & 16.55 \\ DDPM, \(_{t}\) & 205.31 & 84.71 & 37.35 & 14.81 & 5.74 & **3.34** & 170.28 & 83.86 & 45.04 & 28.39 & 21.38 & 16.38 \\ SN-DDPM & 16.33 & 6.05 & 4.19 & 3.83 & 3.72 & 4.08 & 27.58 & 20.74 & 18.04 & 16.72 & 16.37 & 16.22 \\ GMS (**ours**) & **13.80** & **5.48** & **4.00** & **3.46** & **3.34** & 4.23 & **26.50** & **20.13** & **17.29** & **16.60** & **15.98** & **15.79** \\   

Table 1: **Comparison with competitive SDE-based solvers w.r.t. FID score \(\) on CIFAR10 and ImageNet 64\(\)64. our GMS outperforms existing SDE-based solvers in most cases. SN-DDPM denotes Extended AnalyticDPM from Bao et al. .**the range from \(t_{0}=0.3T\) to \(t_{0}=0.5T\) (\(T=1000\) in our experiments) for our further experiments since sampled images closely resemble the real images in this range.

Fig. 3(a) illustrates that when using the same \(t_{0}\) and the same number of steps, edited output images from GMS have lower faithfulness but with higher realism. This phenomenon is likely attributed to the Gaussian noise introduced by the SDE-based solver during each sampling step. This noise causes the sampling to deviate from the original image (resulting in low faithfulness) but enables the solver to transition from the stroke domain to the real image domain. Fig. 3(b) to Fig. 3(d) further demonstrates this phenomenon to a certain extent. The realism of the sampled images generated by the SDE-based solver escalates with an increase in the number of sampling steps. Conversely, the realism of the sampled images produced by the ODE-based solver diminishes due to the absence of noise, which prevents the ODE-based solver from transitioning from the stroke domain to the real image domain. Additionally, in the SDEdit task, GMS exhibits superior performance compared to SN-DDPM  in terms of sample computation cost. Fig. 4 shows the samples using DDIM and GMS when \(t_{0}=400\) and the number of steps is \(40\).

## 5 Related work

**Faster solvers.** In addition to SDE-based solvers, there are works dedicated to improving the efficiency of ODE-based solvers [25; 23; 8]. Some approaches use explicit reverse transition kernels, such as those based on generative adversarial networks proposed by Xiao et al.  and Wang et al. . Gao et al.  employ an energy function to model the reverse transition kernel. Zhang and Chen  use a flow model for the transition kernel.

**Non-Gaussian diffusion.** Apart from diffusion, some literature suggests using non-Gaussian forward processes, which consequently involve non-Gaussian reverse processes. Bansal et al.  introduce a generalized noise operator that incorporates noise. Nachmani et al.  incorporate Gaussian mixture or Gamma noise into the forward process. While these works replace both the forward and reverse processes with non-Gaussian distributions, our approach aims to identify a suitable combination of non-Gaussian distributions to model the reverse process.

## 6 Conclusion

This paper presents a novel Gaussian mixture solver (GMS) for diffusion models. GMS relaxes the Gaussian reverse kernel assumption to reduce discretization errors and improves the sample quality under the same sampling steps. Experimental results show that GMS outperforms existing SDE-based solvers, achieving a remarkable 4.44 improvement in FID compared to the state-of-the-art SDE-based solver proposed by Bao et al.  given 10 steps. Furthermore, due to the presence of noise, SDE-based solvers prove more suitable for stroke-based synthesis tasks and GMS still outperforms state-of-the-art SDE-based solvers.

Figure 3: **Result among different solvers in SDEdit. \(t_{0}\) denotes the time step of the start of reverse. (a): The points on each line represent the same \(t_{0}\) and the number of sampling steps. We select \(t_{0}=\), number of steps \(=\). (b): When \(t_{0}=300\), the effect of DDIM diminishes more prominently with the increase in the number of steps.**

**Limitations and broader impacts.** While GMS enhances sample quality and potentially accelerates inference speed compared to existing SDE-based solvers, employing GMS still fall short of real-time applicability. Like other generative models, diffusion models can generate problematic fake content, and the use of GMS may amplify these undesirable effects.