# Structure of universal formulas

Dmitry Yarotsky

Skoltech

d.yarotsky@skoltech.ru

###### Abstract

By universal formulas we understand parameterized analytic expressions that have a fixed complexity, but nevertheless can approximate any continuous function on a compact set. There exist various examples of such formulas, including some in the form of neural networks. In this paper we analyze the essential structural elements of these highly expressive models. We introduce a hierarchy of expressiveness classes connecting the global approximability property to the weaker property of infinite VC dimension, and prove a series of classification results for several increasingly complex functional families. In particular, we introduce a general family of polynomially-exponentially-algebraic functions that, as we prove, is subject to polynomial constraints. As a consequence, we show that fixed-size neural networks with not more than one layer of neurons having transcendental activations (e.g., sine or standard sigmoid) cannot in general approximate functions on arbitrary finite sets. On the other hand, we give examples of functional families, including two-hidden-layer neural networks, that approximate functions on arbitrary finite sets, but fail to do that on the whole domain of definition.

## 1 Introduction

By _universal formulas_ we broadly (informally) understand parameterized explicit analytic expressions that have a fixed complexity (as expressions) but nevertheless can approximate any continuous function on a compact set. An example of such formula, given by Boshernitzan , is

\[y(x)=_{0}^{x+a}-(bt)}(e^{t})dt+c,\] (1)

where \(d>0,a,b\) and \(c\) are parameters.  proves that, by varying the parameters, functions (1) can uniformly approximate any continuous function on any compact interval in \(\).

Expression (1) involves integration. Laczkovich and Ruzsa  prove existence of universal formulas representable as elementary functions without integration (moreover, their formulas can uniformly approximate continuous functions on the whole \(\) under a mild growth assumption).

One can further restrict the types of operations and show that universal formulas can be realized by fixed-size classical neural networks with suitable activation functions . By a classical neural network we mean a computational model consisting of units ("hidden neurons"), each computing a map \(z_{1},,z_{n}(_{k=1}^{n}w_{k}z_{k}+h),\) where \(z_{1},,z_{n}\) (signals) are outputs of some previous neurons, and \(w_{k}\) and \(h\) are weights (parameters) of the neuron. In addition to hidden neurons, one distinguishes "input neurons" that just represent the scalar components of the network input, and "output neuron(s)" that represent the scalar component(s) of the output. The output neurons work as hidden neurons but without activations, i.e. as \(z_{1},,z_{n}_{k=1}^{n}w_{k}z_{k}+h.\) As defined, neural networks are fairly restrictive in terms of used operations: for example, they contain multiplications of signals \(z\) by constants (weights) \(w\), but not multiplications between signals, \(z_{1}z_{2}\). However, this and some other extra operations can be implemented with any accuracy using suitable combinations of neurons. The universal network in  uses a fixed architecture graph, activations \(\) and \(\)and can approximate any \(f C()\) (see Figure 1). Another fixed-size universal network had been constructed earlier in , but the activation was not an elementary function in that work.

The Kolmogorov(-Arnold) Superposition Theorem  shows that any function \(f C(^{d})\) can be expressed in terms of a fixed number (only depending on \(d\)) of summations and univariate continuous functions. This implies that multivariate universal formulas can be easily constructed from univariate ones, by plugging the latter in the Kolmogorov ansatz. Accordingly, the question of multivariate universal formulas is not significantly more interesting or complicated than its counterpart for univariate formulas, and in this paper restrict our attention to the latter. We also mention in passing another interesting property of universal formulas: they naturally give rise to universal polynomial differential equations \(P(x,f^{},,f^{(n)})=0\), with a polynomial \(P\) and a dense set of solutions in \(C([a,b])\), see . (In fact, one can find such a polynomial for any parameterized family of elementary functions; the density of solutions then follows from the universality of the formula.) See  for a connection to analog computers.

A foundational theorem on neural networks is the universal approximation theorem (UAT). It states that, for a broad class of activation functions, a neural network with a single layer of hidden neurons can approximate any continuous function on a compact set if we increase the number of neurons . This result is very different from, and should not be confused with the above _fixed-size_ universal networks of  or . In UAT, universality is achieved by increasing the number of parameters, and accordingly UAT imposes much weaker requirements on the activation functions (e.g., it holds for any continuous non-polynomial activation, see ).

On the other hand, the fixed-size universal network in Figure 1 is fairly simple. One of the two activation functions used there, \(\), is actually used as an activation in practical multi-layer networks (e.g., the popular SIREN network of ). This raises the following natural question motivating the present paper: _can practically used neural-network-type models be universal as fixed-size formulas (i.e., without increasing the number of neurons)_?

There is an important caveat here: the universality of universal formulas is, of course, an idealization that requires their parameters to have unbounded precision or magnitude. If parameters are finitely defined so as to be representable on real computer, one can give a simple entropy bound constraining feasible approximation accuracy. Specifically, suppose that a formula contains \(p\) parameters, each somehow represented using \(B\) bits (e.g., \(B=32\) or 64 for standard floats). Suppose we are approximating a function \(f\) at \(N\) points \(x_{1},,x_{N}\) with absolute accuracy \(\). Suppose finally that \(|f|\) is bounded by \(M\) and \(f(x_{k}),k=1,,N\), can be chosen independently subject to this constraint. Then

\[N_{2}(M/) pB.\] (2)

This shows that in a practical application with parameters implemented as standard floats, universal formulas such as (1) can typically produce only crude approximations. In the sequel, we will ignore these finite-precision aspects.

Figure 1: **Left:** A (slightly simplified) fixed-size universal neural network approximating any \(f C()\) from . Most hidden neurons are standard neurons \(z_{1},,z_{n}(_{k=1}^{n}w_{k}z_{k}+h)\) with one of the activation functions \(\) (identity, \(\) or \(\)). Also, there are several multiplication neurons. The parameters of the model are all the weights \(w_{k},h\) in the standard neurons. **Right:** A multiplication neuron can be replaced by a group of standard neurons while preserving universality.

Our contribution.Our goal in this paper is to understand which structural properties are critical for universal formulas. This question does not seem to be simple. As a general strategy of tackling it, we analyze a sequence of progressively more complex parametric functional families with respect to several relevant classes of model expressiveness. Our specific contributions are as follows.

1. We introduce a hierarchy \(_{0}_{1}_{2}\) of classes of model expressiveness (Section 2). Here \(_{0}\) represents the families of infinite VC-dimension, \(_{1}\) the families achieving approximation on arbitrary finite sets, and \(_{2}\) the families achieving uniform approximation on the whole segment \(\). Universal formulas correspond to the class \(_{2}\). The class \(_{0}\) is important because elementary functions belonging to it must involve, in some form, the function \(\) acting on an unbounded domain (Section 3). We argue that the class \(_{1}\) is also important because, as we show, some functional families cannot belong to it due to algebraic (polynomial) constraints.
2. As a first result on algebraic constraints, we consider the family of fixed-size linear combinations of sine waves with unbounded weights (equivalently, fixed-size single-hidden-layer neural networks with activation \(\)) and prove that it belongs to the class \(_{0}_{1}\). Moreover, we give a complete description of the limit points of this family (Section 4).
3. As a generalization of the previous result, we introduce a general family of _polynomially-exponentially-algebraic_ expressions of bounded complexity, and prove that it is also subject to polynomial constraints and lies outside \(_{1}\) (Section 5).
4. We give an application of the previous result to branching expressions and neural networks with conventional activations (Section 6). Specifically, we prove that if the network has a bounded complexity and possibly multiple layers, but only one layer with transcendental activation functions (such as \( x\), Gaussian or standard sigmoid), then the network is subject to polynomial constraints and lies outside \(_{1}\). Moreover, polynomial constraints hold even if the activation functions are only _piecewise_ analytic, as is common in practice.
5. The situation is drastically different for formulas involving compositions of non-polynomial analytic functions and \(\): such formulas generally belong to \(_{1}\) (Section 7). It seems hard to separate the classes \(_{1}_{2}\) and \(_{2}\). We give a couple of examples of families for which we can prove that they belong to \(_{1}_{2}\). One of these families can be described as that of fixed-complexity two-hidden-layer networks with \(\) activation that have a single neuron in the second hidden layer. We conjecture that general sine networks of fixed complexity also belong to \(_{1}_{2}\).

Most proofs are deferred to the appendix (the respective sections are indicated in the theorems).

## 2 The hierarchy of expressiveness classes

Throughout the paper, we consider various families \(H\) of functions \(g:\). We define for them expressiveness classes \(_{0},_{1},_{2}\):

\[H_{0}}\]

Vapnik-Chervonenkis dimension \[(H)=\]

\[H_{1}}\]

For any _finite_ subset \(\{x_{1},,x_{N}\}\), any \(y_{1},,y_{N}\) and any \(>0\) there is \(g H\) such that \(|g(x_{k})-y_{k}|<\) for all \(k=1,,N\).

\[H_{2}}\]

For any \(f C()\) and \(>0\), there is \(g H\) such that \(\|g-f\|_{}=_{x}|g(x)-f(x)|<\).

Here, \((H_{f})=\) means that for any \(N\) we can find a size-\(N\) subset \(X\) shattered by \(H\) thresholded at \(0\), i.e. such that for any \(S X\) there is \(g H\) for which \(g(x)>0\) if \(x S\) and \(g(x) 0\) if \(x X S\).

Clearly,

\[_{0}_{1}_{2}.\] (3)

The classes \(_{1}\) and \(_{2}\) reflect approximability on arbitrary finite sets and on the whole interval \(\), respectively. Universal formulas as discussed in the Introduction correspond to the class \(_{2}\). One can say that \(_{1}\) corresponds to universality in a weaker sense, which is particularly relevant if we try to learn the parameters using a finite training set \(\{(x_{k},y_{k})\}\).

We will consider several families \(H\) that we will distinguish by superscripts denoting or enumerating their types (e.g. \(H^{f}\) or \(H^{(1)},H^{(2)},\)) and by subscripts reflecting complexity within the type (e.g., \(H^{(2)}_{N}\) will denote linear combinations of \(N\) sine waves). Most of these families admit natural interpretations in terms of conventional neural networks - we will mention them along the way.

While the remainder of this work focuses on the mathematical analysis of the relation between various functional families and the classes \(_{k}\), let us briefly clarify the role of these classes from the general perspective of approximation and learning.

1. Most conventional learning models (e.g., neural networks) can be said to be defined using (piecewise) elementary functions. If the model is assumed to have a bounded complexity (e.g., a bounded number of neurons) and its weights and signals are bounded, this model falls outside even the broadest class \(_{0}\) (see Section 3 below). This does not apply, however, to models containing the function \(\) acting on an unbounded domain.
2. For a fixed-complexity model, being outside the classes \(_{k}\) and thus not having the full approximation power is not a practically negative property: this just means that the complexity of the model needs to increase to achieve a better fit.
3. The situation when a fixed-complexity model belongs to the class \(_{2}\) (i.e., is a universal formula) is mathematically interesting but somewhat exotic from the perspective of conventional computation, since the respective model parameters need to contain a potentially unbounded amount of information.
4. The class \(_{1}_{2}\) seems especially dangerous from the generalization point of view: given a hypothesis space \(H\) from this class, we can fit a model arbitrarily well to a generic continuous ground truth on any finite training set, but can hardly ever uniformly approximate this ground truth on the whole domain. This cannot be said about the class \(_{2}\). In Section 7 we give examples of functional families (neural networks) provably belonging to this dangerous class \(_{1}_{2}\).

## 3 Pfaffian functions

There is an important broad class of parameterized functions \(f(x,)\) for which one can guarantee that the family \(H^{f}\) obtained by fixing various \(\) has a finite VC-dimension. This class is most naturally described in terms of _Pfaffian_ functions introduced by . Pfaffian functions can be defined using function sequences in which, at each step, the next function satisfies polynomial first-order differential equations involving itself and previous functions (see [9; 5; 30] for details). In particular, Pfaffian functions include all elementary functions when the latter are defined on suitable domains.

Importantly, in contrast to functions like \(,\) and polynomials, which are Pfaffian on their whole natural domain of definition, the function \(\) is not Pfaffian on the whole set \(\), though it is Pfaffian on any bounded interval \((a,b)\) (moreover, the representation of \(\) as a Pfaffian function depends on \((a,b)\) and becomes more complex as \(b-a\) increases). For this reason, elementary functions are Pfaffian only when they involve \(\) (or a closely related function, like \(\)) restricted to bounded intervals.

The fundamental theorem of  gives an explicit finite upper bound on the number of zeros of Pfaffian functions, which implies, in particular, that the respective parametric families have finite VC-dimension. We state this result specialized to elementary functions and in the form suitable for our purposes; see [9; 5; 30; 29] for details.

**Theorem 1**.: _Suppose that a formula \(y=f(x,)\) is constructed from the variables \(x\) and \(w_{1}, w_{d}\) using real numbers, standard arithmetic operations \((+,-,,/)\), elementary functions \(\), \(\), \(\), \(\), and compositions. Suppose that there is a nonempty subset \(W^{d}\) such that for any \(=(w_{1},,w_{d}) W\) the function \(f(,):)\) is well-defined, i.e., \(\) and \(\) are applied on the intervals \((0,)\) and \((-1,1)\), respectively, and there is no division by 0. Moreover, suppose that there is a bounded interval \((a,b)\) to which the arguments of \(\) always belong for all \( W\). Then the family \(H^{f}=\{f(,)\}_{ W}\) has a finite VC-dimension._This theorem shows that an elementary function has chances to be universal only if it includes \(\) (or some related function) acting on an unbounded domain. Therefore, we will focus on these latter functions in the sequel.

It is crucial for Theorem 1 that all the computations in the formula are real-valued and not complex-valued (the proof eventually boils down to Rolle's theorem, which does not hold for \(\)-valued functions). In particular, we cannot bypass the domain restriction of \(\) by simply writing \( x=(e^{ix}-e^{-ix})/2i\). In contrast, many results in the next sections will be essentially algebraic and will hold for complex-valued operations.

We remark that Sontag  considers a class of formulas closely related to Pfaffian functions and proves that shattering \(k\)-point sets in general position requires at least \((k-1)/2\) parameters in such formulas.

## 4 Linear combinations of sines

A simple example of a family with an infinite VC-dimension is \(H^{(1)}=\{c( x)\}_{c,}\). This is usually proved (see e.g. , Section 3.6) by giving a particular example of an arbitrarily large finite set shattered by \(H^{(1)}\). For our purposes, however, it is instructive to describe a general collection of finite sets on which \(H^{(1)}\) has universal approximability. We say that numbers \(x_{1},x_{2},\) are _rationally independent_ if they are linearly independent over the field \(\) of rational numbers.

**Theorem 2**.: _Let \(X=\{x_{1},,x_{N}\}\) be a finite subset of \(\) such that the values \(x_{1},,x_{N}\) are rationally independent. Then the family \(H^{(1)}\) can approximate any \(\)-valued function on \(X\)._

Proof.: Consider the torus \(^{N}=(/2)^{N}\); its points can be identified with the points of the cube \([0,2)^{N}\). Let \(:[0,1)\) be the standard floor function. By a classical Kronecker's theorem , the set \(\{(2( x_{1}- x_{1}),,2( x_{N}-  x_{N}))\}_{}\) is a dense subset of \(^{N}\) if and only if the numbers \(x_{1},,x_{N}\) are rationally independent. Using the \(2\)-periodicity of \(\), this implies immediately that if \(x_{1},,x_{N}\) are rationally independent, then \(\{(( x_{1}),,( x_{N}))\}_{}\) is dense in \((-1,1)^{N}\) and hence \(\{(c( x_{1}),,c( x_{N}))\}_{c,}\) is dense in \(^{N}\). 

This result shows that the family \(H^{(1)}_{0}\). It is also clear that \(H^{(1)}_{1}\) (e.g., since \(g(0)=0\) for any \(g H^{(1)}\)). In fact, the sufficient condition of rational independence in the theorem is close to also being necessary. In particular, a simple kind of sets not shattered by \(H^{(1)}\) are all nontrivial arithmetic progressions \(x_{k}=u+vk,v 0,\) of length 5. Indeed, it follows from the identity \((u+(k-1)v)+(u+(k+1)v)=2(u+kv)(v)\) that the values of functions from \(H^{(1)}\) cannot, for example, have the sign configuration \(++++-+\) on such progressions.

Consider now the more general family consisting of all linear combinations of a fixed number of sine waves with arbitrary frequencies and phases:

\[H^{(2)}_{N}=_{n=1}^{N}c_{n}(_{n}x+h_{n})}_{c_{n},_{n},h_{n},n=1,,N}.\] (4)

One can view this family as representing single-hidden-layer neural networks with the \(\) activation function (\(_{n}\) and \(h_{n}\) are the weights and biases in the hidden layer, and \(c_{n}\) are the weights in the output layer).

One can again easily show that \(H^{(2)}_{N}_{0}_{1}\), by observing that the values of all functions \(g H^{(2)}_{N}\) are constrained by common nontrivial polynomial equations:

**Theorem 3**.:
1. _For any_ \(g H^{(2)}_{N}\)_,_ \(x_{0}\) _and_ \(_{n},_{n},n=0,,2N,\)__ \[ A=0,A=g(x_{0}+_{k}+_{m})_{k,m=0}^{2N}.\] (5)
2. \( A\) _is a nonzero polynomial in the variables_ \(g(x)\)_, where_ \(x X=\{x_{0}+_{k}+_{m}|k,m=0,,2N\},\) _if and only if all_ \(_{k}\) _are different and all_ \(_{m}\) _are different._Proof.: 1. The matrix \(A\) is degenerate because each of its \(2N+1\) rows is a linear combination of \(2N\) vectors \((e^{_{n}_{1}},,e^{_{n}_{N}})\) with \(n=1,,N\) and \(s= i\).

2. If some \(_{k}\) or \(_{m}\) are repeated, then the matrix \(A\) contains repeated columns or rows and so the polynomial \( A\) identically vanishes. Conversely, suppose that all \(_{k}\) are different and all \(_{m}\) are different. Without loss, we can assume \(_{0}<<_{2N}\) and \(_{0}<<_{2N}\). We can then expand \( A=g(x_{0}+_{2N}+_{2N}) A^{}+,\) where \( A^{}\) is the top left \(2N 2N\) minor and the terms \(\) do not contain the variable \(g(x_{0}+_{2N}+_{2N})\). Repeating the expansion for \(A^{}\), etc., we see that the polynomial \( A\) contains the monomial \(_{k=0}^{2N}g(x_{0}+_{k}+_{k})\) with coefficient 1, i.e. is nonzero. 

This result shows that there are (infinitely many) nontrivial polynomial constraints on the values of the functions \(g H_{N}^{(2)}\). Importantly, these constraints are preserved under pointwise limits, so, in particular, \(H_{N}^{(2)}_{0}_{1}\). Taking \(N=1\) and \(_{k}=_{k}=vk,k=0,1,2,\) we obtain a nontrivial constraint for the values of \(g\) on any nontrivial length-5 arithmetic progression \(x_{k}=u+vk\), in agreement with an earlier observation.

In fact, thanks to the semi-linear structure of the family \(H_{N}^{(2)}\), we can go further and give an explicit description of its limit points. It turns out that the limiting functions can only differ from the original combinations of sine waves by resonant factors \(x^{m}\):

**Theorem 4** (A).: _Let \(N\) be fixed. Then a function \(f_{*}\) is a limit point of \(H_{N}^{(2)}\) in the sense of uniform convergence on the segment \(\) if and only if_

\[f_{*}(x)=_{m=0}^{2M_{0}-1}c_{0m}x^{m}+_{k=1}^{K}_{m=0}^{M_{k}-1}c_ {km}x^{m}(_{k}x+h_{km}),\] (6)

_where \(M_{0} 0\), \(M_{k} 1\) for \(k=1,,K\), and \(_{k=0}^{K}M_{k}=N\)._

Resonant factors appear by taking suitable combinations of sine waves at close frequencies, e.g. \(x( x)=_{ x 0}()^{-1}(((+ )x)-( x))\). We show explicitly how to construct resonances of arbitrary degrees subject to the constraint \(_{k=0}^{K}M_{k}=N\). Note that the frequency \(=0\) corresponding to the first sum in expansion (6) is special as it is associated with a potential double-degree resonance.

The more difficult part of the proof is the "only if" part, i.e. that any limiting function has the form (6). Our basic idea is to show that a limiting function is subject to a linear differential equation describing waves with a given set of frequencies; the general resonant form (6) would then appear from the usual solution of such an equation with nontrivial multiplicities of the frequencies. This strategy, however, is not so easy to implement rigorously, since in our setting the frequencies are unbounded. We give two alternative proofs overcoming this difficulty. The first, _clustering-based_ proof, groups the frequencies found in a converging sequence \(f_{n} H_{N}^{(2)}\) into spatially separated "frequency clusters", and then shows that unbounded clusters can be removed without changing the limit. The second, _discretization-based_ proof, starts with describing the discretized functions of \(H_{N}^{(2)}\) by suitable linear finite-difference equations which also hold for the limiting function \(f_{*}\). Then, we consider these discretizations at different length scales and show that they can only be consistent with each other and with the continuity of the uniform limit \(f_{*}\) if representation (6) holds.

## 5 Polynomially-exponentially-algebraic expressions

Theorem 3 shows that the values of functions \(H_{N}^{(2)}\) are subject to nontrivial polynomial constraints preserved under pointwise limits. We extend this argument to a much larger family of functions, involving complex algebraic operations. This generaliztion will allow us, in particular, to prove a related result for multi-layer neural networks.

If \(U^{N}\) is an open domain, we say that a holomorphic function \(Q:U\) is an _algebraic function_ if there exist \(N\)-variable polynomials \(_{0},,_{q}\), not all zero, such that

\[_{k=0}^{q}_{k}(z_{1},,z_{N})Q^{k}(z_{1},,z_{N})=0, =(z_{1},,z_{N}) U.\] (7)The value \(q\) is called the _degree_ of the algebraic function \(Q\). Algebraic functions include, in particular, all polynomials and (on their holomorphy domains) rational functions and powers \(z^{a}\) with rational \(a\). Arithmetic operations and compositions applied to algebraic function produce again algebraic functions.

We define the family \(H^{(3)}_{N,q,r,d}\) as consisting of all functions \(g:\) of the form

\[g(x)=Q(x,e^{P_{1}(x)},,e^{P_{N}(x)}),\] (8)

where \(P_{1},,P_{N}\) are complex-valued polynomials of degree \( d\), and \(Q\) is an algebraic function of degree \( q\) such that all polynomials \(_{k}\) appearing in its definition have degree \( r\). Here, the functions \(Q\) are assumed to be defined on some domains \(U^{N+1}\) that contain all the values \((x,e^{P_{1}(x)},,e^{P_{N}(x)}),x\). Our main result is the following extension of Theorem 3.

**Theorem 5** (B).: _Fix \(N,q,r,d\). Let \(m=(q+1){N+r+1 N+1}+((1,d)+1)(N+1)\). Then there exists a nonzero polynomial \(R\) of \(m+1\) variables such that for any \(g H^{(3)}_{N,q,r,d}\) and any \(a,b\)_

\[R(g(a),g(a+h),g(a+2h),,g(b))=0, h=.\] (9)

As a corollary, since the polynomial \(R\) is the same for all \(g H^{(3)}_{N,q,r,d}\), constraint (9) is preserved by pointwise limits, so \(H^{(3)}_{N,q,r,d}_{1}\).

Our proof of Theorem 5 is inspired by the theory of algebraic-transcendent functions  that addresses representability of functions by algebraic differential equations. Our context is rather different: we are interested in the pointwise convergence of functions (or convergence in the uniform norm), so conditions involving derivatives are not applicable in our setting. We observe, however, that some elements of this theory can be extended from the usual to discretized derivatives. This is why, in particular, the conditions in Eq. (9) are formulated for a regular grid of points \(a,a+h,,b\).

Since the polynomials \(P_{n}\) in the definition of \(H^{(3)}_{N,q,r,d}\) are allowed to be complex-valued, the families \(H^{(3)}_{N,q,r,d}\) subsume the sine wave families \(H^{(2)}_{N}\) considered in the previous section:

\[H^{(2)}_{N} H^{(3)}_{2N,1,1,1}.\] (10)

## 6 Branching expressions and neural networks

We describe now an application of Theorem 5 to neural networks. We start by noting that many standard activation functions can be classified as piecewise polynomial or (piecewise) algebraic-exponential1:

**Piecewise polynomial:** Binary step function \((x)=[x 0]\), ReLU \((x)=(0,x),\) leaky ReLU \((x)=(ax,x),\) squared ReLU \((x)=^{2}(0,x)\).

**(Piecewise) algebraic-exponential:**\( x\), \( x\), standard sigmoid \((x)=(1+e^{-x})^{-1},\) ELU

\[(x)=a(e^{x}-1),&x<0\\ x,&x 0(x)=x/(1+e^{-x})\] , Gaussian \((x)=e^{-x^{2}}\).

"Piecewise" here means that \(\) is in general divided into several subsets (in fact, up to two subsets in the above examples) on which the activation is defined by different analytic formulas. We will refer to this as "branching". On each branch, activations of the first type are polynomials, and those of the second type can be represented in the form (8) with a polynomial or rational \(Q\).

Define the family \(H^{(4)}_{N}\) of functions \(g:\) as realization, with various weight assignment, of the following class of feedforward neural networks. Suppose that the underlying directed acyclic graph of the network contains at most \(N\) neurons. Suppose that each hidden neuron is equipped with one of the piecewise polynomial or algebraic-exponential activation functions mentioned above (possibly different at different neurons). Suppose finally that any directed path from the input to the output neuron contains _at most one_ (piecewise) algebraic-exponential neuron (for example, this is the case if the network has a single algebraic-exponential layer, while the other layers are piecewise polynomial). Then we have 

**Theorem 6** (C).: _Given \(N\), there exists \(m\) and a nontrivial polynomial \(R\) of \(m+1\) variables such that for any \(g H_{N}^{(4)}\) and any \(a,b\)_

\[R(g(a),g(a+h),g(a+2h),,g(b))=0, h=.\] (11)

The main obstacle in deriving this theorem from Theorem 5 is the branching in the activations that destroys the global analytic structure of functions \(g H_{N}^{(4)}\). We overcome this obstacle using the well-known Van der Waerden's theorem on arithmetic progressions.

**Theorem 7** (; see, e.g.,  for a short proof).: _For any integers \(s,p 1\) there exists an integer \(N_{vdW}(s,p) 1\) such that every coloring \(C:\{1,...,N_{vdW}\}\{1,...,p\}\) of \(\{1,...,N_{vdW}\}\) into \(p\) colors contains at least one monochromatic arithmetic progression of length \(s\) (i.e. a progression in \(\{1,...,N_{vdW}\}\) of cardinality \(s\) on which \(C\) is constant)._

## 7 Compositions of sines and non-polynomial functions

The polynomially-exponentially-algebraic structure (8) is actually close to being necessary for the polynomial constraints of Theorem 5, as we show by the following example. Let \(\) be a real analytic function on the interval \([-1,1]\). Let \(f(x)=c((bx)+h)\) with parameters \(c,b,h\) and \(b[-1,1]\). Denote the respective family by \(H^{}\).

**Theorem 8** (D).: \(H^{}_{1}\) _if and only if \(\) is not a polynomial._

This shows that the polynomial constraints of Theorem 5 are destroyed if we relax even slightly the requirement of polynomial arguments of the exponentials in Eq. (8). They are also destroyed if we replace the algebraic function \(Q\) by the sine function.

While \(H^{}_{1}\) for non-polynomial \(\), we can show that \(H^{}\) is never in \(_{2}\), by directly finding all limit points of \(H^{}\) under uniform limits. Let us call the limit points of \(H^{}\) not belonging to \(H^{}\)_nourrivial_.

**Theorem 9** (E).: _For a non-constant \(\), the only nontrivial limit points of the family \(H^{}\) are \(a_{0}+a_{r}x^{r},a(bx)+c,\) and \(c(a_{0}+a_{r}x^{r})\), where \(r=(m>0:}{dx^{m}}(0) 0)\) and \(a_{0},a_{r},a,b,c\). In particular, \(H^{}_{2}\)._

The families \(H^{}\) with non-polynomial \(\) thus provide examples of elements of the class \(_{1}_{2}\) of finitely- but not globally-universal formulas. Consider now a more complex family \(H_{N}^{(5)}\) consisting of the functions

\[f(x)=c(g(x))+h, g H_{N}^{(2)},\] (12)

where \(H_{N}^{(2)}\) consists of linear combinations of \(N\) sine waves as defined earlier in Eq. (4). One can interpret the family \(H_{N}^{(5)}\) as a two-hidden-layer neural network with the \(\) activation function, \(N\) neurons in the first hidden layer, and a single neuron in the second hidden layer. Using the same approach of examining the limit points and invoking additional topological arguments, we prove that the family \(H_{N}^{(5)}\) belongs to the same class as \(H^{}\):

**Theorem 10** (F).: _For any \(N 1\), \(H_{N}^{(5)}_{1}_{2}\)._

It appears that the class \(_{1}_{2}\) is quite generic - we conjecture that an analog of Theorem 10 holds for any multi-layer neural network with the \(\) activation and a bounded number of neurons. However, proving such a general result by explicitly describing the limit points as in Theorems 9, 10 is difficult. We are not aware of simple conditions allowing to separate \(_{1}_{2}\) from \(_{2}\).

In this context, let us discuss the structure of universal formulas mentioned in the Introduction. Boshernitzan's formula (1) includes the factor \(-(bt)}\) which is of the form (8) and hence in \(_{1}\), and the transcendental composition \((e^{t})\). This agrees with our results showing that compositions of sines and non-polynomial functions are essential for universal formulas. However, Boshernitzan's formula also includes integration, which effectively serves to produce an approximation to a piecewise constant function. The neural network in Figure 1 and the formulas of  combine sines with arcsines, which serves to approximate periodic piecewise linear or piecewise constant functions. It seems that approximate piecewise constant functions are an essential element of existing universalformulas, and so one can conjecture that arcsine or a related inverse trigonometric function is a necessary component of an elementary universal formula without integration.

In the fairly different context of recursive decidability, there is a remarkably similar sharp difference between polynomial-sine and more complex compositions . Let \(_{1}\) denote the ring generated by the integers and the expressions \(x, x^{n}\) and \((x x^{n})(n=1,2,)\). Let also \(_{2}\) denote the ring generated by the integers and the expressions \( x^{n}\) and \( x^{n}(n=1,2,)\). Then the predicate that there exists a real \(x\) such that \(f(x)>0\) is undecidable for \(f_{1}\), but decidable for \(f_{2}\).

## 8 Discussion

The expressiveness hierarchy.To analyze the necessary features of the structure of universal formulas, we have examined the chain of expressiveness classes \(_{0}_{1}_{2}\) corresponding, respectively, to the families having the infinite VC-dimension, general approximability on all finite sets, and general global uniform approximability. Membership in the class \(_{0}\) can be conveniently studied using the theory of Pfaffian functions, while, as we have shown, membership in the class \(_{1}\) can be conveniently studied using algebraic methods. In particular, our results suggest that globally universal formulas need to involve suitable compositions of non-polynomial and sine operations.

Transcendental structure of universal formulas.Our main result in Section 5 shows that combining a single layer of exponential and sine operations with polynomials and algebraic functions has a fairly weak expressiveness, in the sense that such formulas of bounded complexity are constrained by polynomial relations and do not even have general approximability on finite sets. In this family, the requirement on the operations preceding the exponential/sine layer is much stronger (polynomial) than the requirement on the subsequent operations (algebraic). As Theorem 8 shows, any non-polynomial operation preceding the sine operation and combined with linear operations restores the general finite approximability.

General methods.All results presented in this paper that rule out global universality are essentially obtained by one of the three general methods: Khovansky's analytic theory of Pfaffian functions (Theorem 1), finding algebraic constraints (Theorems 3, 5, 6), and direct computations of limit functions (Theorems 4, 9, 10). It appears that all these methods are insufficient to conveniently separate the finite approximability class \(_{1}_{2}\) from the global approximability class \(_{2}\). Only the last of the three methods seems useful for that, but it seems hard to find the full set of limit points for more complex families \(H\) similarly to how we did that for the linear combinations of sines (Theorems 4) and the families \(H^{},H^{(5)}_{N}\) (Theorems 9, 10). It would be interesting to have a general method suitable for separating \(_{1}_{2}\) from \(_{2}\).

Limit points.Theorems 4 and 9 show that the nontrivial limit points of the families \(H^{(2)}_{N}\) and \(H^{}\) are some sort of resonances, reflecting some degeneracies of the model. The set of nontrivial limit points is small and explicit in these two cases. However, in a universal formula this set is the whole \(C()\). It would be interesting to see how general is this dichotomy, e.g. whether the set of nontrivial limit points of some formula can be a small but not explicitly describable subset of \(C()\).

Neural networks.In Section 6 we described a general family of neural networks lying outside the class \(_{1}\) and so non-universal. These networks can have a single layer of transcendental neurons (with activations such as \(,,e^{-x^{2}}\)) as well as some layers with piecewise-polynomial activations. We have shown that though the resulting functions are only piecewise analytic, they still admit polynomial constraints. It remains, however, an open question whether networks with multiple transcendental layers involving the sine activation can be universal. Theorem 8 shows that such networks belong to \(_{1}\), while Theorem 10 shows that the two-hidden-layer sine network of bounded complexity and with a single neuron in the second hidden layer belongs to \(_{1}_{2}\). We conjecture that, without \(\) or other special functions that help create periodic piecewise-linear dependencies, general \(\) networks of fixed complexity belong to \(_{1}_{2}\).