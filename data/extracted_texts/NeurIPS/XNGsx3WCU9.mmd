# Visual Data Diagnosis and Debiasing

with Concept Graphs

Rwidthi Chakraborty\({}^{1,2}\)1 Yinong (Oliver) Wang\({}^{1}\) Jialu Gao\({}^{1}\) Runkai Zheng\({}^{1}\)

**Cheng Zhang\({}^{1,3}\) Fernando De la Torre\({}^{1}\)**

\({}^{1}\)Carnegie Mellon University \({}^{2}\)UiT The Arctic University of Norway \({}^{3}\)Texas A&M University

###### Abstract

The widespread success of deep learning models today is owed to the curation of extensive datasets significant in size and complexity. However, such models frequently pick up inherent biases in the data during the training process, leading to unreliable predictions. Diagnosing and debiasing datasets is thus a necessity to ensure reliable model performance. In this paper, we present ConBias, a novel framework for diagnosing and mitigating **Con**cept co-occurrence **Bias**es in visual datasets. ConBias represents visual datasets as knowledge graphs of concepts, enabling meticulous analysis of spurious concept co-occurrences to uncover concept imbalances across the whole dataset. Moreover, we show that by employing a novel clique-based concept balancing strategy, we can mitigate these imbalances, leading to enhanced performance on downstream tasks. Extensive experiments show that data augmentation based on a balanced concept distribution augmented by ConBias improves generalization performance across multiple datasets compared to state-of-the-art methods.2

## 1 Introduction

Over the last decade we have witnessed an unparalleled growth in the capabilities of deep learning models across a wide range of tasks, such as image classification , object detection , semantic segmentation , and so on. More recently, with the introduction of large multi-modal models, these capabilities have improved further . However, such models, while demonstrating impressive performance on a wide range of tasks, have been shown to be biased in their predictions . These biases come in various forms, based in texture , shape , object co-occurrence , and so on. In addition to exploring model biases, dataset diagnosis, or evaluating biases directly within the dataset, is particularly crucial as large datasets available today are beyond the scope of human evaluation, owing to their size and complexity. For example, ImageNet , a widely used dataset in deep learning literature, is known to have thousands of erroneous labels and a lack of diversity in its class hierarchy . Other popular datasets such as MS-COCO  and CelebA , have problematic social biases with respect to gendered captions and prejudicial attributes of people from different races. As a result, frameworks that effectively diagnose and debias these datasets are sought.

While multiple works exist in the categorization and exploration of biases in visual data , an end-to-end pipeline incorporating both diagnosis and debiasing has received relatively scant attention. ALIA  is the closest and most recent work exploring such a data-augmentation-based approach to debiasing, but it has two shortcomings - first, it does not diagnose the dataset which it aims to debias. Without such a diagnosis, it is challenging to identify the biases to be mitigated in the first place.

Second, the method relies on a large language model (ChatGPT-4 ) to generate diverse, unbiased, in-domain descriptions. This approach is potentially confounding since there is no reliable way to ensure that the biases of the large language model itself do not affect the quality of such domain descriptions. In this work, we address both these shortcomings.

We present ConBias, our framework for diagnosis and debiasing of visual data. Our key contribution is in representing a visual dataset as a knowledge graph of concepts. Analyzing this graph for imbalanced class-concept combinations leads to a principled diagnosis of biases present in the dataset. Once identified, we generate images to address under-represented class-concept combinations, promoting a more uniform concept distribution across classes. By using a concept graph, we circumvent the reliance on a large language model to generate debiased data. Figure 1 illustrates the core idea of our approach in contrast with existing methods. We target object co-occurrence bias, a human-interpretable issue known to confound downstream tasks [34; 10]. Object co-occurrence bias refers to any spurious correlation between a label and an object causally unrelated to the label. Representing the dataset as a knowledge graph of object co-occurrences provides a structured and controllable method to diagnose and mitigate these spurious correlations.

Our framework proceeds in three steps: (1) _Concept Graph Construction:_ We construct a knowledge graph of concepts from the dataset. These concepts are assumed to come from dataset ground truth such as captions or segmentation masks. (2) _Concept Diagnosis:_ This stage then analyzes the knowledge graph for concept imbalances, revealing potential biases in the original dataset. (3) _Concept Debiasing:_ We sample imbalanced concept combinations from the knowledge graph using graph cliques, each representing a class-concept combination identified as imbalanced. Finally, we generate images containing under-represented concept combinations to supplement the dataset. The image generation protocol is generic and uses an off-the-shelf inpainting process with a text-to-image generative model. This principled approach ensures that the concept distribution in our augmented data is uniform and less biased. Our experiments validate this approach, showing that data augmentation based on a balanced concept distribution improves generalization performance across multiple datasets compared to existing baselines. In summary, our contributions include:

* We propose a new concept graph-based framework to diagnose biases in visual datasets, which represents a principled approach to diagnosing datasets for biases, and to mitigating them.
* Based on our graph construction and diagnosis, we propose a novel clique-based concept balancing strategy to address detected biases.
* We demonstrate that balanced concept generation in data augmentation enhances classifier generalization across multiple datasets, over baselines.

## 2 Related Work

**Bias discovery in deep learning models.** The identification of biases in trained deep learning models has a rich history, with early works exploring the texture and shape-bias tradeoff in ImageNet

Figure 1: The conventional data diagnosis and augmentation pipeline begins with an original (biased) dataset. Existing methods address these biases via object frequency calibration , metadata analysis , or traditional augmentation techniques [60; 5]. In contrast, our framework models visual data as a knowledge graph of concepts, with orange nodes representing classes and blue nodes representing concepts, facilitating a systematic diagnosis of class-concept imbalances for debiasing object co-occurrences in vision datasets.

pretrained ResNets [14; 21; 32; 39]. More recently, the field of worst group robustness has emerged, aiming to generalize classifier performance across multiple groups in the data that correspond to known spurious correlations [49; 45; 24; 42]. Debiasing and concept discovery in the feature space of the learned classifier is also common [1; 54; 59]. Testing model performance sensitivity to the presence of particular attributes has also been explored [53; 36]. With the recent rise in popularity of large language models, efforts have been made to identify learned biases using off-the-shelf captioning models , adaptive testing , and language guidance [19; 37]. Traditional data augmentation approaches such as CutMix , and RandAug , are used as baselines as well. Our work intervenes on the dataset directly, instead of operating in the model feature space or testing model sensitivity. This allows for a more intuitive and principled approach to bias discovery.

**Data diagnosis.** Our work is placed in the context of data diagnosis, i.e. identifying biases directly from the data without using the model as a proxy. One of the early influential works expounding the importance of datasets in deep learning research was a systematic review of the popular datasets in computer vision . A modern appraisal categorizing more diverse types of biases in visual datasets exists in . Additionally, works investigating possible issues with dataset labels have also received interest [33; 58]. Data diagnosis tools such as REVISE  compute object statistics (including co-occurrence) to generate high-level insights of the data. However, REVISE is not an end-to-end framework that at once diagnoses and debiases data. It is rather an exploratory tool for an overview of common concepts in the dataset. A more recent method, ALIA, uses a language model to populate diverse descriptions of the given dataset, consequently generating images from such descriptions. A more critical look on dataset bias lies in the field of fairness, particularly with regards to societal bias [12; 16]. Finally, benchmark datasets for data diagnosis have also been proposed [29; 28].

**Object co-occurrence bias in visual recognition.** Objects are biased in the company they keep. This adage is well known in the computer vision literature, as outlined in [34; 10]. Modern efforts to mitigate object co-occurrence bias involve feature decorrelation , object aware contrastive representations , causal interventions , and fusing object and contextual information via attention . The common theme in tackling contextual and co-occurrence bias lies entirely in using better models (feature representations) rather than intervening in the dataset directly. We place our debiasing method along the data augmentation direction, allowing for better controllability and interpretability of the debiasing stage, rather than relying on semantic features learned by a classifier, which may be difficult for humans to interpret.

## 3 Approach

Figure 2 illustrates the overall pipeline of our method. In this section, we begin with the problem statement in Section 3.1, and move to the three major stages in our method definition. Section 3.2 describes the procedure of concept graph construction. Section 3.3 illustrates the details of concept diagnosis. Finally, Section 3.4 presents our method for concept debiasing.

### Problem Statement

We are given a dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{N}\), a set of images and their corresponding labels. We also assume access to a concept set \(C=\{c_{1},c_{2},,c_{k}\}\) that describes unique objects present in the data. An example concept set looks like the following: {alley, crosswalk, downtown,..., gas station}, i.e. a list of unique objects present in each image in addition to the class label. Finally, we are given a classifier \(f_{}(X)\) parameterized by network parameters \(\). The central hypothesis of this work is that the class labels exhibit co-occurring bias with the concept set \(C\), affecting downstream task performance. In this light, we wish to generate an augmented dataset \(D_{}\) that is debiased with respect to the concepts and their corresponding class labels. Thus, given the new dataset \(D^{}=D D_{}\), we wish to retrain \(f_{}(X)\) in the standard classification setup:

\[^{*}=_{f}_{(x,y) D^{}}[(y,f_{ }(x))],\] (1)

where \((y,f_{}(x))\) is the cross entropy loss between the class label and classifier prediction. Our framework consists of three stages: _Concept Graph Construction_, _Concept Diagnosis_, and _Concept Debiasing_. Next, we provide details on each step.

### Concept Graph Construction

We construct a concept graph \(G=(V,E,W)\) from the data, where \(|V|\) is the node set of the graph, \(|E|\) is the edge set, and \(|W|\) is the set of weights for each edge in the graph. We first construct the node set \(V\) as a union of the label set \(Y\) and concept set \(C\):

\[V=Y C.\]

Next, we construct the edge set \(E\):

\[E=\{(i,j)\,D_{k}ijD_{k}\}.\]

Finally, we construct the weight set \(W\) by computing the weights \(w_{ij}\) for each edge \((i,j)\) in \(G\):

\[w_{ij}=_{n=1}^{N}(i D_{n}j D_{n}),\]

where \(\) is the indicator function that returns 1 if both \(i\) and \(j\) are present in the \(n\)-th image in \(D\), and 0 otherwise, and \(N\) is the total number of images in the dataset.

The concept graph \(G\) encapsulates co-occurrence counts between nodes, thus providing an alternative representation of the (originally visual) data. As we show in the next section, this representation helps uncover novel imbalances (bias) contained in the dataset.

### Concept Graph Diagnosis

In the previous section, we define how to build the concept graph. Here, we present how to leverage the concept graph for discovering co-occurrence biases. We present a principled approach to discovering concept-combinations across classes that co-occur in an imbalanced fashion.

Definition (Class Clique Sets)For each class \(Y_{i} Y\), we construct a set of \(k\)-cliques using the concept graph \(G\). The set of all possible \(k\)-cliques for class \(Y_{i}\) is denoted as \(_{i}^{k}\):

\[_{i}^{k}=\{\{c_{j_{1}},c_{j_{2}},,c_{j_{k}}\} c_{j_{1}}, c_{j_{2}},,c_{j_{k}} Cj_{1}<j_{2}<<j_{k}\},\]

where \(j_{1},j_{2},\) are the indices of concepts in \(C\). Then, \(_{i}\) for class \(Y_{i}\) can be successfully constructed for \(k=1,2,,K\), where \(K\) is the size of the largest clique in \(G\) containing \(Y_{i}\). We construct class clique sets for every class in the dataset. An illustration of concept cliques in the Waterbirds dataset that help in bias diagnosis is provided in Figure 3.

Figure 2: **Overview of our framework ConBias. (a) Given a dataset and its concept metadata which contains the objects present in each image, (b) we build the concept graph using object co-occurrences. The line thickness indicates the co-occurrence frequencies of particular concepts with their respective classes. (c) Next, the clique-based sampling strategy generates under-represented class-concept combinations, which yield (d) the dataset diagnosis result. (e) Finally, with biases discovered, we generate images of classes containing under-represented concept combinations in the dataset with a standard text-to-image generative model.**Definition (Common Class Clique Sets)Given \(_{i}\) for each class, we then compute the cliques common to all classes. These are the cliques of interest, whose imbalances we want to investigate:

\[=_{i}_{i},\]

where \(\) encapsulates all common cliques enumerated across the dataset for all classes. Refer to Figure 2 for a broad illustration of the \(k\)-clique set construction from the concept graph \(G\).

Definition (Imbalanced Common Cliques)Given the set of common cliques across all classes \(\), we compute the imbalanced class-concept combinations, i.e. the imbalanced clique set \(I\):

\[I_{[]_{m=1}^{M}}=\{(|F_{_{y_{i}}^{m}}-F_{_{y_{ j}}^{m}}|,(F_{_{y_{i}}^{m}},F_{_{y_{j}}^{m}}))\},  i,j,\]

where \(F_{_{y_{i}}^{m}}\) and \(F_{_{y_{j}}^{m}}\) indicates the co-occurrence frequency of concepts in clique \(m\) with respect to class \(y_{i}\) and \(y_{j}\) respectively, and the \(\) operator identifies the underrepresented class for the particular concept clique. Thus, each element in \(I\) is a number representing the imbalance of each common clique across all classes. For the special case where the size of clique \(m\) is 1, this equates to simply looking up the value \(w_{ij}\) in \(G\). For the case where the size of \(m>2\), it is straightforward to compute the co-occurrence of class \(y_{i}\) with respect to concepts in \(m\):

\[w_{ij k}=_{n=1}^{N}(i D_{n}j D_{n} k D_{n}),\]

for each image \(D_{n}\) in the data. The set \(I\) holds rich information about the data. In addition to holding the imbalanced counts of concept combinations across all classes, the set \(I\) also holds which is the _underrepresented_ class with respect to a particular concept clique.

Intuitively, concept combinations that are common across all the classes, but do _not_ co-occur uniformly across the classes are likely biased concept combinations. We provide an example from the _Waterbirds_ dataset in Figure 4. The training set in _Waterbirds_ is intentionally biased to the background: 95% of landbirds appear with land backgrounds, and 95% of waterbirds appear with water backgrounds. First, we find common cliques of varying sizes across the classes (Landbird, Waterbird). One example of a common clique of size \(3\) is (Landbird, Beach, Ocean) and (Waterbird, Beach, Ocean). We compute the co-occurrence of (Landbird, Beach, Ocean) from the extracted metadata, and the imbalance is clear. Since waterbirds are far more prone to appear on water, there are significantly more images of waterbirds

Figure 4: **Examples of concept imbalances in the Waterbirds dataset. We show the frequencies of concepts cliques as discovered in the dataset. We see imbalances across not only single concepts (e.g., Ocean, Grass) but also concept combinations (e.g., (Beach, Ocean), (Tree, Forest)). These are the biases we aim to mitigate for the downstream task.**

Figure 3: Examples of concept clique sets for Landbird class in Waterbirds dataset uncovered by our diagnosis. Concepts such as Tree, Forest, Man, Woman, Bamboo are overwhelmingly associated with this class, indicating strong co-occurrence bias. All these concepts are causally unrelated to the bird type.

containing concepts \(\) and \(\) than landbirds, which are more prone to be in land-based environments. If we look at the co-occurence of \(\) with a land-based concept such as \(\), we see the opposite imbalance. There are significantly more images of landbirds containing trees over waterbirds. Similarly, for the water-based concept of \(\), we see a strong imbalance towards the \(\) class. In our debiasing stage, we should therefore generate more images of waterbirds with tree-based backgrounds, and landbirds with beach/water-based backgrounds. Using the clique-based approach, we have successfully uncovered the known background bias in the Waterbirds dataset. This approach is generalizable to multiple classes. All we need are common cliques, and the computation of concept co-occurrences across the dataset. In this way, our concept graph approach uncovers interesting concept combinations across the _whole_ dataset that appear in an imbalanced and spurious fashion. More examples of such imbalances are provided in the supplementary material.

### Concept Graph Debiasing

We have, to this point, constructed a knowledge graph of the visual data, and diagnosed it for concept-based co-occurrence biases. Once the imbalanced clique set \(I\) is identified in \(\), we debias the data by generating images containing under-represented concepts across classes.

Recall that \(I=\{f_{i},Y_{i}\}\) inherently holds the underrepresented class \(Y_{i}\) and the frequency \(f_{i}\) by which the original dataset needs to be adjusted with new images of class \(Y_{i}\) with respect to concept clique \(i\). Following the example in the previous section, we notice that the concepts (\(\), \(\)) are significantly over-represented in the \(\) class than \(\). Similarly, the concept \(\) is significantly over-represented with the \(\) class than the \(\) class. As a result, we sample \(f_{i}\) instances of these under-represented cliques with respect to their classes, and prompt a text-to-image generative model for more images of the \(\) class with the concept \(\). Similarly, we would prompt the model to generate images of \(\) with the concept \(\), \(\). We generate images for all class based imbalances following this upsampling protocol.Typical prompts for our image-inpainting model would look like: An image of a ocean and a beach, An image of a tree, An image of a forest, etc. We use an inpainting-based method to make sure that the original object is not modified in the image, and that the new concepts are only injected into the non-object space in the image. See the supplementary material for the generated images and the prompts.

Using this upsampling protocol, we generate a set of images that leads to our augmented, debiased dataset \(D_{}\). The original training data \(D\) can now be augmented using this data, and the classifier \(f_{}(X)\) can be retrained on the dataset \(D D_{}\). In the next section, we conduct experiments on three datasets to demonstrate our method's significant improvements of baselines.

**Note on concept set annotations.** We assume the availability of reliable ground truth concept sets. Such annotations already exist for the datasets we investigate - Waterbirds, UrbanCars, and COCO-GB. We agree that unreliable ground truth concept sets would hinder generalization abilities, but this assumption is not dissimilar to the assumption of reliable ground truth labels in classification tasks. Moreover, the reliance on ground truth concept sets, sometimes referred to as concept banks, have also been considered in prior work . Ground truth concept sets serve as auxiliary knowledge bases and provide human level interpretability to the task at hand.

**Note on computational complexity.** In general, given \(K\) classes and \(C\) concepts, the graph clique enumeration is expected to grow in \(O(exp(K+C))\). However, in practice, we find that constraining clique sizes \( 4\) leads to interpretable bias combinations, with no significant effect of the exponential runtime.

## 4 Experiments

We validate our method on vision datatset diagnosis and debiasing across various scenarios. We begin by introducing the experimental setup including the datasets, baselines, tasks, and implementation details in Section 4.1. Section 4.2 presents the main results of our proposed framework, \(\), compared with state-of-the-art methods. Finally, Section 4.3 details ablation studies and analyses.

### Setup

**Datasets.** We use three datasets in our work: Waterbirds , UrbanCars , and COCO-GB , that are commonly used in the bias mitigation domain. We tackle background bias in the Waterbirds dataset, background and co-occuring object bias in the UrbanCars dataset, and finally gender bias in COCO-GB. All the tasks are binary classification tasks. More details on the training splits and class labels are provided in the supplementary material. For Waterbirds, there are 66 nodes and 865 edges in the concept graph. For UrbanCars, the graph contains 19 nodes and 106 edges. For COCO-GB, there are 81 nodes and 2326 edges in the graph.

**Baseline methods.** Our baselines are include a vanilla Resnet-50 model pre-trained on ImageNet, two typical data augmentation based debiasing methods: (1) CutMix, a technique where we cut and paste patches between different training images to generate diverse discriminative features, and (2) RandAug, which creates random transformations on the training data during the learning phase. Finally, we compare against the recently proposed and state-of-the-art ALIA, which uses a large language model to generate diverse, in-distribution prompts for a text-to-image generative model.

**Evaluation protocols.** We compute the mean test accuracy over the class-balanced test data and the out-of-distribution (OOD) test data, similar to . The class-balanced data contains an even distribution of classes and their respective spurious correlations, while the OOD data contains counterfactual concepts. For example, in Waterbirds dataset, for the class-balanced test data 50% images of Landbirds have Land backgrounds, while 50% images of Waterbirds have Water backgrounds. The OOD test set contains Landbirds on Water, and Waterbirds on Land. More details on the test sets are presented in the supplementary material.

**Implementation details.** We use existing implementations to train our models. Our Base model is a Resnet-50 pretrained on ImageNet . We generate the same number of images per data-augmentation protocol to ensure a fair comparison. For comparison with ALIA on Waterbirds, we directly use their generated dataset available here. For the other datasets, we used the existing ALIA implementation to generate the augmented data. Following previous work, we use validation loss based checkpointing to choose the best model, the Adam optimizer with a learning rate of \(10^{-3}\), a weight decay of \(10^{-5}\), and a cosine learning schedule over \(100\) epochs. To generate images, we use Stable Diffusion  with a CLIP -based filtering mechanism to ensure reliable image generation. Finally, we inpaint the object onto the generated image using ground truth masks (available for all datasets). All code was written in PyTorch .

### Main Results

In Table 1 we present the main results, averaged over three training runs. First, we note that for Waterbirds and UrbanCars, we observe significant improvements in both the Class-Balanced and OOD test sets over the typical augmentation methods such as CutMix and RandAug. Second, we note the significant improvement in performance over the most recent state-of-the-art augmentation method, ALIA. Third, for COCO-GB, while we notice slightly smaller difference in the CB and OOD accuracies between our method and the baselines, our hypothesis is that this happens because of limited number of samples used for augmentation. ALIA uses a confidence based filtering mechanism to remove generated samples. This leads to a small final number of 260 samples to be added for the

    &  &  &  \\   & CB \(\) & OOD \(\) & CB \(\) & OOD \(\) & CB\(\) & OOD \(\) \\  Baseline  & 67.1 & 44.9 & 73.5 & 40.5 & 58.5 & 51.9 \\ + RandAug  & 73.7 & 60.2 & 76.3 & 46.1 & 55.8 & 50.2 \\ + CutMix  & 67.9 & 45.6 & 74.4 & 39.3 & 57.4 & 51.2 \\ + ALIA  & 69.6 & 48.2 & 74.0 & 42.5 & 58.7 & **52.4** \\ + ConBias (Ours) & **77.9** & **69.3** & **78.3** & **52.9** & **58.8** & 51.4 \\   

Table 1: **State-of-the-art comparison on different datasets. Results are averaged over three training runs. CB: class balanced split. OOD: out-of-distribution split. Binary class classification accuracy is used as the metric. Our method outperforms previous approaches across multiple datasets.**retaining part. In the ablation section, we show this hypothesis to be true, and further demonstrate that on adding more images for the retraining step, we progressively increase the performance gap between our method and the baselines. These three observations taken together validate the usefulness of our approach. The next section provides additional insights on the usefulness of our method and the effect of ablating its components.

### Ablations and Analyses

We further analyze our method along five axes: (1) The usefulness of the graph structure, (2) Robustness of our method to other evaluation metrics, (3) The impact on CB and OOD performance by increasing the number of added samples for the retraining step, (4) The usefulness of discovered concepts by our method on the trained classifier, and (5) The impact of the generative component in our work compared to ALIA, since the latter uses InstructPix2Pix  while we use a Stable Diffusion based inpainting protocol.

**Effect of the graph structure.** Recalling the definition of Class Clique Sets, in principle one could only use cliques sizes of 1, i.e., the direct neighbors of each class node. This would be equivalent to computing the frequency of co-occurrence over a single hop neighborhood of the class node in the graph. In this ablation we show that one should use larger cliques, i.e. leverage the graph structure, instead of a simple direct neighborhood based frequency calculation. We trained three separate models on three different types of \(D_{}\): Ours (BG), trained on images containing only background shortcuts, Ours (CoObj), images containing only the co-occurrence shortcuts, and Ours (Both), images containing both shortcuts, but _not simultaneously_.

Table 2 shows the results. First, our approach of leveraging the graph structure provides improvement over simply using the frequency of a 1-hop neighborhood. Second, we note that _all_ the methods outperform the baseline and ALIA, which shows that incorporating frequency based co-occurrences is in a broader sense much more useful than relying on diverse prompts generated by ChatGPT-4, which is the approach taken by ALIA.

**Robustness to evaluation metrics.** The CB and OOD test accuracies test for generalization capabilities, but more direct evaluators of shortcut learning exist in the literature. In , for instance, the authors propose (i) The _ID Accuracy_ - which is the accuracy when the test set contains common background and co-occurring objects, (ii) The _BG-GAP_ - which is the drop in _ID accuracy_ when the test set contains common co-occurring objects, but uncommon background objects, (iii) The

   Model & BG-GAP \(\) & CoObj-GAP \(\) & BG+CoObj GAP \(\) \\  Base & -11.2 & -21.5 & -54.8 \\ Base (BG) & **-5.0** & -19.4 & -38.0 \\ Base (CoObj) & -6.3 & -19.2 & -47.3 \\ Base (Both) & -5.6 & -23.2 & -47.6 \\ Base (ConBias) & -6.0 & **-18.4** & **-41.4** \\   

Table 4: **Robustness of our method to evaluation metrics** In addition to CB and OOD performance, we also report metrics evaluating multiple shortcut mitigation. Results on UrbanCars (Average of three training runs).

  
**Model** & **CB \(\)** & **OOD \(\)** \\  Base & 73.4 & 40.4 \\ Base + ALIA & 74.0 & 42.5 \\  Base (BG) & 78.5 & 51.9 \\ Base (CoObj) & 77.0 & 47.3 \\ Base (Both) & 78.1 & 51.3 \\ Base (ConBias) & **79.4** & **53.2** \\   

Table 2: **Benefit of the graph structure in ConBias.** Leveraging the graph structure is beneficial as opposed to simply computing single concept-class frequency counts on UrbanCars.

  
**Models** & **CB \(\)** & **OOD \(\)** \\  Base & 67.1 & 44.9 \\ Base + ALIA & 69.6 & 48.2 \\  Base + ConBias (IP2P) & 72.9 & 60.5 \\ Base + ConBias & **77.9** & **69.3** \\   

Table 3: **Performance for the IP2P variant** of ConBias with respect to base, ALIA, and our original model on Waterbirds. Our method significantly improves over ALIA even when using IP2P, although the best results are still achieved when using the stable diffusion based inpainting protocol.

_CoObj-GAP_, which is the drop in _ID accuracy_ when the test set contains uncommon co-occurring objects, but common background objects, and finally (iv) The _BG + CoObj GAP_, which is the case when both background and co-occurring objects are uncommon in the test set. A multiple shortcut mitigation method should _minimize_ the _BG + CoObj GAP_ metric, and also make sure it does not exacerbate any shortcut that the base model relies on. In Table 4, we present results of Base, Base (BG), Base (CoObj), Base(Both), and Base (ConBias) on these metrics for UrbanCars. We are able to post the lowest drops among all baselines on the _CoObj-GAP_ and _BG + CoObj GAP_ metrics, suggesting mitigation of multiple shortcut reliance. This places our method in a more realistic context, as it is infeasible to assume that real world data will only have a single type of bias in them.

**Scaling the number of images in \(D_{}\).** In Table 1, we commented on the fact that our method provides marginal improvement over the baselines in the COCO-GB dataset. Our hypothesis was that this was due to the low number of images in the augmented dataset. In Figure 5, we demonstrate the impact of adding more images to \(D_{}\) for retraining. Clearly, our method benefits from this protocol, leading to significant differences over ALIA as we keep increasing the number of images. Note that, infinite enrichment is not recommended and has been found to be detrimental to classifier performance, as progressive addition of synthetic images will likely lead to addition of out-of-distribution examples in the training data. This explains why, after an inflexion point, the accuracy suffers from adding more images. Similar observations have been made in  and .

**Discovered concept imbalances and feature attributions.** To verify that the model indeed debiases the imbalanced concepts that our method discovers, we present GradCAM  attributions of the model predictions after retraining. In Figure 6, we show results on all datasets. While other methods frequently focus on the spurious feature, ConBias helps the model focus only on the relevant, object level features of the data.

Figure 5: **Performance on COCO-GB.** We show the accuracies on (a) Class-Balanced (CB) and (b) Out-of-Distribution (OOD) splits. We observe that increasing number of images in \(D_{}\) improves performance up to a certain point (1000 images).

Figure 6: **Visualization of the heatmaps for different methods.** Top row: Waterbirds. Middle row: UrbanCars. Bottom row: COCO-GB. Our method enforces the base model to focus on only the relevant features in the data, and removing reliance on shortcut features, i.e. the background for Waterbirds, the background and co-occurring object for UrbanCars, and the gender for COCO-GB.

The impact of the generative model.ALIA uses an InstructPix2Pix (IP2P) based generation procedure, while we use stable diffusion with a mask-inpainting procedure to make sure the objects remain consistent in the image. To ablate the effect of the generation, we present results of our method with IP2P as the generative model instead, on Waterbirds dataset, in Table 3. First, we note that even with IP2P as the generative component, we are able to outperform ALIA, which suggests that it is actually the superior quality of our concept discovery method that leads to the improved results. Second, our inpainting based method outperforms our IP2P based method, which we argue is due to the objects being preserved in the generated image, as opposed to traditional image editing methods, where the object may transform arbitrarily, hurting the quality of augmented data.

## 5 Conclusion, Limitations, and Future Works

While ConBias is the first end-to-end pipeline to both diagnose and debias visual datasets, there are some limitations: First, that the enumeration of cliques grows exponentially with the size of the graph. For larger real world graphs, there could be more efficient strategies to find the concept combinations. Second, in this work we focus on biases emanating out of object co-occurrences. A variety of other biases exist in vision datasets, and future work would look to address the same. We add an extended section on broader impact of our work in the supplementary material. In summary, datasets in the real world are biased, and the exponential increase in dataset sizes over the past decade amplifies the challenge of investigating model and dataset biases. While both dataset and model diagnosis are exciting areas of research, an end-to-end diagnosis and debiasing pipeline such as ConBias offers a principled approach to diagnosing and debiasing visual datasets, in turn improving downstream classification performance. Our state-of-the art results open up numerous interesting possibilities for future work - incorporating more novel graph structures, and diagnosis under the regime where concept sets may be wholly or partially unavailable, remain interesting directions to pursue.