# Collaborative Learning via Prediction Consensus

Dongyang Fan\({}^{1}\)  Celestine Mendler-Dunner\({}^{2,3}\)  Martin Jaggi\({}^{1}\)

\({}^{1}\)EPFL, Switzerland

\({}^{2}\)Max Planck Institute for Intelligent Systems, Tubingen, Germany

\({}^{3}\)ELLIS Institute Tubingen, Germany

{dongyang.fan, martin.jaggi}@epfl.ch

celestine@tue.ellis.eu

###### Abstract

We consider a collaborative learning setting where the goal of each agent is to improve their own model by leveraging the expertise of collaborators, in addition to their own training data. To facilitate the exchange of expertise among agents, we propose a distillation-based method leveraging shared unlabeled auxiliary data, which is pseudo-labeled by the collective. Central to our method is a trust weighting scheme that serves to adaptively weigh the influence of each collaborator on the pseudo-labels until a consensus on how to label the auxiliary data is reached. We demonstrate empirically that our collaboration scheme is able to significantly boost individual models' performance in the target domain from which the auxiliary data is sampled. At the same time, it can provably mitigate the negative impact of bad models on the collective. By design, our method adeptly accommodates heterogeneity in model architectures and substantially reduces communication overhead compared to typical collaborative learning methods.

## 1 Introduction

This work considers a decentralized learning setting where each agent has access to a labeled dataset and a local model. The agents may differ in the data distribution they have access to as well as the quality of their local models. In addition, we assume a shared unlabeled dataset \(^{*}\) sampled from a target distribution \(\) is available to all agents. The central question is _how can agents effectively exchange information to benefit from each other's local expertise in order to improve their predictive performance on the target domain \(\)?_

Towards this goal, our work takes inspiration from social science on how a panel of human experts collaborate on a task. Humans typically engage in discourse to exchange information, they share their opinions, and based on how much they trust their peers, each individual will then adjust their subjective belief towards the opinion of peers. When repeated, this process gives rise to a dynamic process of consensus finding, as formalized by DeGroot . Central to the consensus mechanism of DeGroot is the concept of _trust_. It determines how much individual agents influence each other's opinion, and thus the influence of each agent on the final consensus.

Our proposed algorithm mimics this consensus-finding mechanism in the context of collaborative learning, inspired by . In particular, our consensus procedure is aimed at how to label the shared dataset \(^{*}\). Therefore, we carefully design a strategy by which each agent determines the trust towards others, given its local information, to optimally leverage each agent's expertise to collectively pseudo-label \(^{*}\). This mechanism of knowledge distillation is then combined with techniques from self-training  in order to transfer the shared knowledge from the pseudo-labels into the local models in an iterative fashion.

Crucial to our approach is that instead of aiming for a consensus on model parameters, such as typically done in federated learning, we leverage the abundance of unlabeled data to enforce consensus in _prediction space_ over \(^{*}\). A key benefit of information exchange in prediction space is a reduction in communication complexity and the ability to control privacy leakage, but also the ability to seamlessly cope with both data heterogeneity and model heterogeneity.

**Problem setup.** We consider a collaborative learning setup with \(N\) agents. Each agent \(i[N]\) holds local training data, sampled from a local data distribution \(_{i}\). We denote the local training data as \((},})\), where the matrix \(}\) composes of \(n_{i}\) local datapoints and \(}\) denotes the vector of corresponding labels. The number \(n_{i}\) needs not to be the same across agents. In addition, we assume agents have access to a shared unlabeled dataset \(^{*}\) sampled from a target distribution \(\). We use \(n^{*}\) to denote the number of datapoints in \(^{*}\). The ultimate success measure we consider is each agent's prediction performance on the target distribution \(\). We work under the assumption that sharing of raw labeled data is not desirable due to privacy concerns, data ownership, or storage constraints and we wish to keep communication at a minimum to avoid overheads. Our setting recovers the goal of both decentralized and federated learning, with or without personalization , when \(\) is defined as the prediction task on the mixture of all local data distributions. In addition, we allow agents to differ in their model architectures. We do not require a coordination server but assume agents have all-to-all communication available.

**Contributions.** The key contributions of our work can be summarized in the following aspects:

1. We propose a novel collaborative learning algorithm based on prediction-consensus, which effectively addresses statistical and model heterogeneity in the learning process, and provably mitigates the negative impact of low-quality participating agents.
2. Our algorithm is able to significantly reduce communication overhead and privacy-sensitive information sharing in comparison to other collaborative learning methods while achieving superior empirical performance.
3. We show theoretically that consensus can be reached via our algorithm and justify the conditions for good consensus to be achieved.

## 2 Related work

In _federated learning_ a central server coordinates local updates toward learning a global model. Local nodes upload gradients or model parameters, instead of data itself, to maintain a certain level of privacy. McMahan et al.  describe the classic FedAvg algorithm. Follow-up works mainly focus on addressing challenges from non-i.i.d. local data [6; 7; 8] and robustness towards Byzantine attacks. Apart from communicating gradients or model parameters, several works discuss alternatives to allow for heterogeneous model architectures. These methods are based on variants of model distillation [9; 10], reaching an agreement in the representations space  or output space . Similar to our work, both assume access to a shared unlabeled dataset. However, we go beyond naive averaging to determine agreement to account for heterogeneity in model or data quality.

In contrast to federated learning, the _fully decentralized learning_ setting does not assume the existence of a central server. Instead, decentralized schemes such as gossip averaging are used to aggregate local information across agents [13; 14]. Despite the lack of a global state, such methods can provably converge to the desired global solution, leading to a gradual consensus among individual models . In this context, Dandi et al. , Le Bars et al.  optimizes the communication topology to adapt to data heterogeneity but do not offer any collaborator selection mechanisms. Bellet et al.  allows personalized models on each agent, but assumes prior information about task-relatedness, as opposed to learned selection. _Gossip algorithms_ typically assume a fixed gossip mixing matrix given by e.g. physical connections of nodes [17; 18; 19]. These approaches fail to consider data-dependent communication as with task similarities and node qualities. Several recent works have addressed this issue by proposing alternative methods that consider these factors. Notably, Li et al.  directly optimizes the mixing weights by minimizing the local validation loss per node, which requires labeled validation sets. Sui et al.  uses the E-step of EM algorithm to estimate the importance of other agents to one specific agent \(i\), by evaluating the accuracy of other agents' models on the local data of agent \(i\). This way of trust computation does not allow the algorithm to be applied to target distributions that differ from the local distribution, differentiating it from our work. Moreover, for both Li et al. , Sui et al.  the aggregation is performed in the gradient space, therefore not allowing heterogeneous models.

Our work relates to _semi-supervised learning_ as it involves partially unlabeled data. Most relevant are self-training methods  that first train a model using labeled data, then use the trained model to give pseudo-labels to unlabeled data. The pseudo-labels can further be fed back to the training loop to attain a better model. Wei et al.  shows that under expansion and separation assumptions, self-training with input consistency regularization can achieve high accuracy with respect to ground-truth labels. When more than one learner is involved, co-training  appears as an extension to self-training, benefiting from the knowledge of learners from independent views in labeling a set of unlabeled data. Diao et al.  incorporates SSL into federated learning. In a setting where agents are with unlabeled data and the center server is with labeled data, experimental studies demonstrate that the performance of a labeled server is significantly improved with unlabeled clients. Farina  presented a collective learning framework for distributed SSL, where they combine predictions on a shared dataset via weights evaluated from local models' performances on local validation datasets. While their algorithm bears similarities to ours, it is important to note that it is exclusively tailored to scenarios in which the target domain matches the global distribution. In a similar spirit, we want to leverage unlabeled data in a fully decentralized setting.

Finally, Mendler-Dunner et al.  have previously formalized collective prediction as a dynamic _consensus finding procedure_. They demonstrated that such an approach can lead to significant gains over naive model averaging. We extend their approach from test-time prediction to collaborative model training.

## 3 Method description

Our proposed method is designed to take advantage of shared unlabeled data in the context of collaborative learning through knowledge distillation. Therefore, it emulates human opinion dynamics to collectively pseudo-label the shared auxiliary data. These labels are then incorporated in the local model update steps towards collectively improving the performance on the data distribution from which the shared data is sampled.

### Collective pseudo-labeling

To describe the pseudo labeling step, let us use \(f_{_{i}}\) to denote the local model of agent \(i[N]\) parameterized by \(_{i}\). We write \(}_{i}=f_{_{i}}(^{*})\) to denote the predictions of agent \(i\) on the auxiliary data \(^{*}\). Agents share these predictions with their peers. Naturally, the individual models may differ in these predictions and it is a priori unclear which model is most accurate, as ground truth labels of the auxiliary data are not available. To combine the predictions into pseudo-labels for \(^{*}\), each agent locally decides how to weigh other agents' predictions by estimating their respective expertise on the target task. We refer to these weights as trust scores and we use \(w_{ij}\) to denote the trust of agent \(i\) towards the predictions of agent \(j\). It's worth noting that the trust between agents is not necessarily mutual, i.e., can be asymmetrical: agent \(i\) can trust agent \(j\) without agent \(j\) necessarily trusting agent \(i\) back. We use \(\) to denote the matrix of trust scores. Given the trust scores, agent \(i\) uses the following pseudo labels for the auxiliary data:

\[_{i}=_{j}w_{ij}}_{j}.\] (1)

Trust scores are determined locally by each agent based on query access to other agents' predictions and they are refined iteratively throughout training as models are being updated. The adaptive weight computation will be detailed in Section 4.

### Collaborative learning from pseudo labels

In the second step, the proxy labels for the auxiliary data are used to augment local model training. Therefore, in each step, the local optimization problem is augmented by a disagreement loss, and the new objective is given by

\[(f_{_{i}}(_{i}),_{i})+( f_{_{i}}(^{*}),_{i})\] (2)where \(f_{_{i}}()\) denotes the vector of agent \(i\)'s predictions on the dataset \(\), \(\) is the local training loss and \(()\) is a disagreement measure. We choose \(l_{2}\) distance for the disagreement measure in the regression case and cross-entropy for the classification case. \(>0\) is a trade-off hyperparameter that weighs the local loss and the cost of disagreement. This objective adheres to a conventional semi-supervised learning approach, however, we generate pseudo-labels in a trust-based collective manner.

To iteratively refine the local models in the spirit of self-training, the pseudo labeling step and the local training step are performed in an alternating fashion as described in Algorithm 1. Starting from pre-trained models \(_{i}^{(0)}\), in each round \(t\{1,..,T\}\) model predictions on the auxiliary data are shared and then each agent aggregates them into a set of pseudo labels to augment local data and perform an update step.

Our algorithm is motivated conceptually by co-training  where it was demonstrated that unlabeled data can be used to augment labeled data to boost model performance. Moreover, learning from collective pseudo-labels offers several additional benefits. Firstly, aggregation is performed in the prediction space, eliminating the need for all agents to have identical model architectures. Secondly, the communication cost of transmitting predictions is significantly lower than that of sharing model weights, and the same pseudo-labels \(_{i}\) can be reused for multiple local epochs to further reduce the communication burden.

``` Input: For each agent \(i[N]\) we are given a local model \(_{i}^{(0)}\), a labeled local dataset \((_{i},_{i})\), and unlabeled shared data \(^{*}\). for\(t=1,...,T\)do  Each node \(i[N]\) broadcasts their soft labels \(}_{i}^{(t-1)}=f_{_{i}^{(t-1)}}(^{*})\) to all other nodes in parallel for each agent \(i\)do  Calculate pairwise trust score \(w_{ij}^{(t)}(j[N])\), based on the received soft decisions using methods provided in Section 4  Get pseudo-labels on \(^{*}\) from collaborators: \(_{i}^{(t)}=_{j}w_{ij}^{(t)}}_{j}^{(t-1)}\)  Do local training with collaborative disagreement loss \[_{i}^{(t)}*{arg\,min}_{}\ (f_{}(_{i}),_{i})+(f_{}(^{ *}),_{i}^{(t)})\] (3) endfor ```

**Algorithm 1** Pseudo code of our proposed algorithm

### Convergence analysis

We study under what conditions Algorithm 1 will reach a consensus among agents on how to label the auxiliary data. For the analysis, we focus on the over-parameterized regime 1 and we make the following assumption on the local data distributions:

**Assumption 1**.: _There is no concept shift between the local data distributions and the target domain \(\) from which the shared data is sampled, i.e., \(_{i}(Y|X\!=\!x)=(Y|X\!=\!x)\) for all \(i[N]\)._

Together with over-parameterization the assumption implies that the minimizer of the objective specified in (2) can always reach zero loss. Further, this allows us to model the update of agents' predictions on \(^{*}\) as a Markov process where the state transition matrix corresponds to the trust matrix \(^{(t)}\). Therefore, it is convenient to write the update of the predictions on \(^{*}\) performed by the algorithm in matrix form, as \(^{(t)}=[}_{1}^{(t)},..,}_{N}^{(t)}]\). Adopting this notation we have for \(t 1\)

\[^{(t)}=^{(t)}^{(t-1)}=^{(t)}^{(t-1)}..^{(1)}^{(0)}\;.\] (4)

The following result provides sufficient conditions under which consensus will be reached by our algorithm.

**Theorem 1** (Consensus on predictions).: _Assume all agents' models are over-parameterized and the data distributions satisfy Assumption 1. Then, for \(t\) Algorithm 1 converges to a consensus among the local models on the predictions on \(^{*}\), that is,_

\[_{i}^{(t)}=_{}^{(t)} i j,\] (5)

_as long as \(^{(t)}\) is row-stochastic and positive for any \(t 0\)._

The proof is given in Appendix B and the main insight is that as long as \(\) is row-stochastic and positive (that is \(_{j}w_{ij}=1\) for any row \(i\), and \(w_{ij}>0\)), the product of any \(^{(t)}\)'s is stochastic, irreducible, and aperiodic, and this leads to the differences between rows in \(^{(t)}\) vanishing in time. Together with no concept shift, over-parameterization is important to guarantee that models can fit the consensus predictions on the unlabeled data while at the same time minimizing local losses. In contrast, in the under-parameterized setting, consensus and local loss minimization cannot necessarily be achieved at the same time.

### Information sharing in prediction space

A key feature of our method is that agents do not share model parameters, but they communicate by exchanging prediction queries. If Algorithm 1 achieves a consensus this means that agents arrive at solutions where they agree on predictions on \(^{*}\), but this does not imply that they have learned the same model, or that they agree on predictions outside \(^{*}\). To illustrate this, we provide a simple example where local data are generated using cubic regression with additive i.i.d. noise in the output, as shown in Fig. 1. In this example, uniform weights are the ideal solution. We apply this optimal uniform trust, as the main purpose here is to illustrate the difference between information sharing in prediction space and parameter space. Each agent fits a polynomial regression of degree 4, which leads to _over-parameterization_ of the model to fit the data. Full details of our example are given in Appendix A.1. We refer to the work of  for a similar setting with under-parameterized models. Here we note the most interesting observations in the over-parameterized regime.

First, we observe that for \(T 20\) the three agents reach a consensus on the predictions of \(^{*}\). However, the model parameters are not the same across the agents, as depicted in the rightmost panel. Further, considering the properties of the algorithm across rounds and the predictions in different regions of the input space in more detail, the following desirable behaviors are observed:

* In the region where agent \(i\) has more data, it fits the local data more accurately and it moves pseudo-labels closer to its own predictions.
* In the region where agent \(i\) has no or little data, agent \(i\) only updates its model parameters to fit the pseudo-labels.
* When local loss minimization and prediction consensus can be achieved at the same time, agents can arrive at models with a perfect agreement in the target prediction space.

## 4 Design of trust weights

In Section 3.3 we have shown that our algorithm is guaranteed to reach consensus on \(^{*}\) under weak assumptions on the trust matrix \(^{(t)}\). In this section, we discuss how to design \(^{(t)}\) to encourage that the achieved consensus leads to a high-quality labeling of \(^{*}\).

Figure 1: Local data distributions are shown in (a), and the initial fit on local data is shown in (b). (c) and (d) are predictions on \(^{*}\) after 5 rounds and 20 rounds of our algorithm update respectively. (e) is the comparison of model fits in a larger range ((d) is zoom-in of the rectangular area of (e))Therefore, we focus on multi-class classification. We let \(f_{_{i}}()\) denote the class probabilities obtained using model \(_{i}\) for a datapoint \(\). We choose the cross-entropy measure \((,)\) to define the agreement loss function \((,)\) in (3). If \(=[_{1},..,_{n}]^{}\), then \(f_{}()=[f_{}(_{1}),..,f_{}(_{n})]^{}^{n C}\), where \(n\) is the number of samples in \(\) and \(C\) is the number of classes.

### Trust evaluation through self-confidence

The quality of the local models could differ due to various factors, such as the amount of labeled data available during training, due to the expressivity of the local model, the training algorithm, or due to the relevance of the local data for the target task of labeling \(\). Thus, a desirable property of the consensus solution is that malicious agents, or agents with low-quality models contribute less to the pseudo-labeling than agents with better models.

Hadjicostis and Dominguez-Garcia  differentiate between malicious and non-malicious agents and they discuss the concept of trustworthy consensus, where only non-malicious agents contribute to the consensus. In contrast to prior work, we do not aim for trustworthy agents to contribute equally. Instead, we specifically want consensus to come from potentially _unequal_ contribution of all agents, weighted according to their relevance. We allow for the trust matrix to be asymmetric. All agents determine trust from information given locally to the respective agent, which differs across agents. Central to any such strategy is that the capabilities of models on \(\) can be estimated appropriately. In the following, we discuss a strategy of how to determine trust from local data and prediction queries to other models.

As no label information on \(^{*}\) is available to evaluate trust, it is natural to use agents' own predictions on \(^{*}\) as a local reference point. Then, each agent distributes their trust towards other agents based on the alignment of their predictions. We use weighted pairwise cosine similarity as a measure of alignment which motivates the following trust weight calculation:

\[w_{ij}^{(t)}=^{(t)}}{_{j}_{ij}^{(t)}}\ \ \ \ \ \ _{ij}^{(t)}=}_{^{*}}_{i}^{(t)}( {x})_{i}^{(t-1)}}(),f_{_{j}^ {(t-1)}}()}{\|f_{_{i}^{(t-1)}}()\|_{2}\|f _{_{j}^{(t-1)}}()\|_{2}}\,.\] (6)

The inclusion of the weighting factor \(_{i}^{(t)}()\) and how to choose it will be discussed in Section 4.2.

**Self-confident trust.** Naturally, pairwise cosine similarity leads to a trust matrix that has diagonal entries being the highest value among each row. We call this property _self-confident_, as each agent trusts itself the most. We now demonstrate that this property is not particularly restrictive. Even if constraining trust matrix to be self-confident, it is still possible to design such a matrix that facilitates any consensus. The proof is given in Appendix D.

**Proposition 2**.: _For any given consensus distribution \(\), it is always possible to find a trust matrix \(\) that leads to it, which is both row stochastic and self-confident._

A second nice property our trust calculation has is that for an appropriate choice of \(_{i}^{(t)}\) the proposed calculation of trust scores in (6) leads to scores that become more evenly distributed over time as agents gradually reach consensus.

**Claim 3**.: _Given Assumption 1 holds and that all agents are over-parameterized. Assume \(_{i}^{(t)}\) is chosen such that the trust matrix \(^{(t)}\) is row-stochastic and positive for all \(t 0\). Then, for the trust calculation in (6), we have \(^{(t)}\) loses self-confidence over time and finally converges to a uniform matrix:_

\[tr(^{(t)}) tr(^{(t-1)})^{(t)} ^{}\] (7)

The proof is provided in Appendix C. This claim characterizes the behavior of our dynamic trust scheme: while initially all agents distribute trust towards helpful collaborators and try to achieve a consensus on \(^{*}\). Once consensus is reached, we will have \(}_{i}^{(t)}=}_{j}^{(t)}\) for any \(i,j\), and \(^{(t)}\) will become a matrix with uniform weights. This means no individual agent has increasingly high weight or the ability to manipulate the labeling.

Now that we know that a self-confident matrix can lead to the desired consensus, what other properties should our trust matrix have?

### Robustness to low-quality nodes

If agents possess low-quality local data, we aim to minimize their influence on the labeling of the auxiliary data throughout the algorithm. Proposition 4 gives a sufficient condition for such a desired consensus: if there exists only one node with low-quality data, as long as it receives the lowest trust from other regular nodes, and the sum of trust it receives is the smallest compared to the others, it will remain to have lowest importance in the consensus.

**Proposition 4**.: _Given Assumption 1, the trust matrix is row stochastic and positive, and all agents hold over-parameterized models. Let \(b\) be the only node with low-quality data and \(\) be the timestep that consensus is reached. If the following desirable properties hold for \(t<\):_

1. \(b\) _receives the lowest trust from others than itself, i.e.,_ \(w_{jb}^{(t)}=_{i}w_{ji}^{(t)}\) _for_ \(j b\)_._
2. \(b\)_-th column has the lowest column sum:_ \(_{j}w_{jb}^{(t)}<_{i b}_{j}w_{ji}^{(t)}\)_._

_Then node \(b\) will have the lowest importance in the consensus._

The proof is given in Appendix E, where we also provide desired properties in the presence of multiple nodes with low-quality data, under some extra assumptions. Proposition 4 emphasizes the desired trust weights during training _before_ consensus is reached.

When nodes with weak model architectures (such as under-parameterized models) are involved, achieving consensus is not assured. If such a consensus solution does exist, it will be constrained by the underfitting of weak nodes. Consequently, this solution would not serve as a stationary solution concerning the local training loss of a strong node. Nevertheless, we conjecture that these desired properties can still enhance training by mitigating the impact of the weak nodes.

### Confidence weighting

In the following paragraph, we discuss the choice of the weights \(_{i}^{(t)}\) in (6). Specifically, we incorporate confidence weighting into the pairwise cosine similarity calculation to emulate the construction of a transition matrix based on a known consensus distribution.

Let us start by outlining an idealized trust calculation that effectively down-weighs agents with low quality data. We first construct an intermediate transition matrix \(\) from pairwise cosine similarities of the agents' predictions on \(^{*}\) (with row normalization). For the low-quality node \(b\), we will have \(_{jb}\) being the lowest value in the \(j\)-th row, for any \(j b\). According to Proposition 4, in order to have low importance of low-quality workers in the consensus, we need to set the overall trust that \(b\) receives to be the lowest among all the nodes. To achieve this, we need to assign the trust of regular workers towards the low-quality workers to a very small value, as it is difficult to alter self-confidence. If the consensus importance weight is known, one can easily calculate the corresponding trust matrix

\[w_{jb}=_{jb}(1,}{_{jb}} ),\]

which is a classical result from Metropolis chains  (also see Appendix D). We will have \(w_{jb}<_{jb}\) for \(j b\), as \(\) should be sufficiently small.

**Practical scheme.** Since the consensus importance weight is unknown, we cannot attain the ideal trust matrix. Therefore, we propose an alternative weighting scheme that achieves similar effects: we up-weight the similarity in the region where agent \(j\) has more confidence, i.e., where agent \(j\)'s class probability assignments have lower entropy. By doing this, we encourage that the trust weights become more concentrated on themselves and helpful workers, and less concentrated on low-quality workers. We incorporate this into the trust weight calculation (6) by choosing

\[_{i}^{(t)}()=(f_{_{i}^{(t-1)}}( {x}))}\]

where \(\) denotes the entropy. We offer further intuition as well as justification of this weighting scheme in Appendix F. Moreover, we empirically demonstrate how our choice of trust matrix leads to a low column sum for bad nodes in Section 5.2.

## 5 Experiments

We start with a synthetic example to visualize the decision boundary achieved by our algorithm and then demonstrate its performance on real data in a heterogeneous collaborative learning setting.

### Decision boundary visualization

Four classes are generated via multivariate Gaussian following \(P^{c}(_{c},)\), where \(_{0}=(-2,2)^{}\), \(_{1}=(2,2)^{}\), \(_{2}=(-2,-2)^{}\), \(_{3}=(2,-2)^{}\). \(=_{2 2}\). Four clients have local data sampled from a mixture of \(P^{c}\)'s. For clients 0-3, we randomly flip 10% of the labels, and for client 3, we flip all labels. The unlabeled data \(^{*}\) are sampled equally from \(P^{c}\)'s. The data distribution is shown in Fig. 3(a). The base model used in each node is a multi-layer perceptron of 3 layers with 5, 10, and 4 neurons respectively. We now compare Algorithm 1 with dynamic trust weight to with naive trust weight. When a client with low-quality data is involved, i.e. client 3 in the toy example, our trust update scheme gives a better decision boundary to good agents after collaboration, as blind trust towards low-quality clients will impair the effectiveness of pseudo labeling.

### Deep learning experiments

**Datasets and model architectures.** We consider a more challenging setting, where local data distributions are non-i.i.d. Two different statistical heterogeneities are considered: (1) _Synthetic heterogeneity_. We utilize the classic Cifar10 and Cifar100 datasets  and create 10 clients from each dataset. To distribute classes among clients, we use a Dirichlet distribution2 with \(=1\). Unless specified otherwise, we employ ResNet20  without pretraining. (2) _Real-world data heterogeneity_. A real-world dermoscopic lesion image dataset from the ISIC 2019 challenge [31; 32; 33] is included here. The same client splits are used as in , based on the imaging acquisition system employed in six different hospitals. The dataset includes eight classes of lesions to classify, with the class distribution among the clients displayed in Fig. 2(b). Following , we choose pretrained EfficientNet  as the base model, and use _balanced accuracy_ as the evaluation metric. For every dataset, we construct \(^{*}\) from equally contributed samples by every agent.

**Comparison against baseline methods.** We compare our methods with several baseline methods, including FedAvg , FedProx , SCAFFOLD  (SCA), FedDyn , local training without collaboration (LT), and training with naive trust (Naive). Note with naive trust we are realizing soft majority voting, which represents the baseline method proposed from . We adhere to the samearchitecture setting, where the standard federated learning algorithms can be applied. To initiate the process, we allow each client to perform local training for 5 global rounds, with the objective of obtaining a sufficiently refined model that can be used for trust evaluation. From the 6th training round, the clients start collaboration. Over a total of 50 global rounds, each consisting of 5 local epochs, we report the averaged accuracy results from three repeated experiments in Table 1. The evaluation metric is calculated on the dataset \(^{*}\). \(\) is fixed as 0.5 in all experiments.

When all nodes share the same data quality and degree of statistical heterogeneity (denoted by "regular" in the table), our methods align closely with consensus through naive averaging, which is optimal in this case. When all nodes share the same degree of statistical heterogeneity but differ in data quality, exemplified by randomly selecting two nodes (indexed as 2 and 9) for a complete flip of local training labels, our dynamic trust update shows better overall performances3, proving the effectiveness of our approach in limiting the detrimental influences from nodes with low-quality data. We further plotted out the learned trust matrix in the dynamic update mode during one of the middle training rounds in the left plot of Fig. 4. Clearly, our algorithm is able to give low trust weights to the nodes with low-quality data, and the 2nd and 9th columns have the lowest column sum.

**Adaptability to varying model architectures.** We allocate a more expressive model architecture to the first half of the nodes and a less expressive one to the other half. The former comprises ResNet20 and EfficientNet, which were the models of choice in the previous experiments. For the latter, we employ a linear model (i.e., one-layer fully connected neural network) with a flattened image tensor as input and the output is of size equivalent to the number of classes. It is worth noting that if agents with strong and weak model architectures (as in cases of under-parameterization) coexist, consensus might not occur, as suggested by our empirical findings illustrated in Fig. 5. Nevertheless, our trust-based collaborator selection mechanism consistently outperforms local training and simple averaging. The trust weight matrix learned during Cifar100 training is depicted in the right plot of Fig. 4, revealing the presence of asymmetric trust. Specifically, the last 5 nodes exhibit a higher level of trust towards the first 5 nodes, while the opposite is not true. The trust allocation is desired in identifying the helpers. We further refer to Appendix A.2 for more empirical evidence from a toy polynomial regression example on the presence of strong and weak architectures.

**Reduced communication costs.** Gradient aggregation-based methods incur a significant communication burden proportional to the number of model parameters (\((N||)\)), which is particularly heavy given the over-parameterized nature of modern deep learning.

    & & FedAvg & FedProx & SCA & FedDyn & LT & Naive & Ours-S & Ours-D \\   & Cifar10 & 0.542 & 0.517 & 0.578 & 0.578 & 0.475 & **0.618** & 0.604 & 0.612 \\  & Cifar100 & 0.261 & 0.240 & 0.317 & 0.310 & 0.178 & 0.311 & **0.319** & 0.308 \\  & Fed-ISIC & 0.279 & 0.261 & 0.213 & 0.243 & 0.248 & 0.290 & **0.302** & 0.291 \\  Low- & Cifar10 & 0.541 & 0.530 & 0.570 & 0.575 & 0.470 & 0.596 & 0.605 & **0.608** \\ Quality & Cifar100 & 0.254 & 0.240 & 0.289 & **0.308** & 0.171 & 0.285 & 0.300 & 0.306 \\ Data & Fed-ISIC & 0.229 & 0.242 & 0.221 & 0.243 & 0.217 & 0.247 & 0.249 & **0.269** \\   

Table 1: Our methods compare to baseline methods. **Blue** denotes the algorithm with top 1 accuracy and green denotes the method with 2nd best accuracy. “Ours - S” denotes the static version where the trust score is kept constant after first-time calculation (after 5 rounds of local training) and “Ours - D" denotes the dynamic version where the trust score is updated per global round.

Figure 5: Target accuracy comparison with 2 different model architectures with error bars (hatch pattern denotes fully connected NN is used). From top to bottom: Cifar10, Cifar100, FedISIC

In contrast to existing approaches, our proposed method significantly reduces the communication burden by enabling each node to transmit only their predictions on the shared dataset. This results in communication overhead \((N^{2} n^{} C)\). It is clear that this value does not scale up with more complex models, and is much smaller than the model size. Moreover, our methods maintain their high performance even when the number of local epochs increases. On the other hand, FedAvg loses its effectiveness with less frequent synchronization, i.e. more local epochs between global aggregation rounds, as shown in the left panel of Fig. 6.

## 6 Conclusions and extensions

In the context of decentralized learning, we leverage the collective knowledge of individual nodes to improve the accuracy of predictions with respect to a target distribution. Our proposed trust update scheme, based on self-confidence, ensures robustness against nodes with low-quality data. By achieving consensus in the prediction space, our method effectively handles diverse model architectures within local clients, while maintaining a low communication overhead, thereby exhibiting important practical potential. Despite coming from a different perspective, our trust-based collaborative pseudo-labeling method may provide some inspiration in the semi-supervised learning community. Notably, our algorithm is intrinsically compatible with personalization, in terms of allowing some concept shift across clients. We leave this for future work.

**Robustness.** We have designed our algorithm with the assumption that all agents communicate _honestly_, meaning that no Byzantine workers _intentionally_ provide incorrect information. Nevertheless, our method exhibits some resilience against a common Byzantine attack, known as the label flip (referred to as "low-quality workers" in our paper). For instance, even with 2 out of 10 workers having 100% flipped labels, our algorithm maintains good performance. If there are malicious workers deliberately providing incorrect information, the nodes may refuse to reach a consensus, instead of reaching a detrimental bad consensus, assuming a reasonable \(\) is chosen. Consider the scenario in which a detrimental consensus is reached with malicious nodes involved; in this case, the consensus loss and local loss for regular nodes will not decrease in the same direction, making the consensus solution non-stationary. Notably, the "personal" component of our loss function adds an element of robustness against malicious nodes.

**Privacy Concerns.** While previous works show that training data can be reconstructed from model parameters  or gradients , our algorithm requires less privacy-sensitive information sharing, which is predictions on a shared dataset. While we are aware that model predictions can still leak private information on training data due to memorization , there is a trade-off between the gain from collaboration and the amount of information that users are willing to share. As the number of outer rounds increases, we observe a notable improvement in accuracy within the context of \(^{*}\). However, this enhanced accuracy comes at the cost of disclosing more information, a relationship that is depicted in Fig. 6. An interesting extension would be to apply differential privacy to further guarantee privacy.

**Acknowledgements.** DF would like to thank Anastasia Koloskova, Felix Kuchelmeister, Matteo Pagliardini, and Nikita Doikov for helpful discussions during the project and El Mahdi Chayti for proofreading. DF acknowledges funding from EDIC fellowship from the Department of Computer Science at EPFL. CM acknowledges support from the Tubingen AI Center. This project was supported by SNSF grant 200020_200342.

Figure 6: Algorithm performances on Cifar100 for different algorithm configurations. (left) effect of varying number of local epochs on final performance; (right) algorithm performance as a function of the number of training rounds for 5 local epochs each