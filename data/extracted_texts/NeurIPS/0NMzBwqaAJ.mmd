# Not All Tokens Are What You Need for Pretraining

Zhenghao Lin\({}^{}^{}\)  Zhibin Gou\({}^{}\)  Yeyun Gong\({}^{}\)\({}^{}\)  Xiao Liu\({}^{}\)  Yelong Shen\({}^{}\)

Ruochen Xu\({}^{}\)  Chen Lin\({}^{}\)  Yujiu Yang\({}^{}\)  Jian Jiao\({}^{}\)  Nan Duan\({}^{}\)  Weizhu Chen\({}^{}\)

\({}^{}\)Xiamen University \({}^{}\)Tsinghua University \({}^{}\)Shanghai AI Laboratory

\({}^{}\)Microsoft

https://aka.ms/rho

Equal contribution. See author contributions for details. Work done during their internships at Microsoft Research Asia. : zhenghaolin@stu.xmu.edu.cn; zebgou@gmail.com Correspondence authors.

###### Abstract

Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that _"Not all tokens in a corpus are equally important for language model training"_. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively -- matching DeepSeeKath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.

Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. Rho-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.

## 1 Introduction

Scaling up model parameters and dataset size has consistently elevated the next-token prediction accuracy in large language models, yielding significant advancements in artificial intelligence (Kaplan et al., 2020; Brown et al., 2020; OpenAI, 2023; Team et al., 2023). However, training on all available data is not always optimal or feasible. As a result, the practice of data filtering has become crucial, using various heuristics and classifiers (Brown et al., 2020; Wenzek et al., 2019) to select training documents. These techniques significantly improve data quality and boost model performance.

However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training, as illustrated in Figure 2 (Upper). Removing such tokens might alter the text's meaning, while overly strict filtering could exclude useful data (Welbl et al., 2021; Muennighoff et al., 2024) and lead to biases (Dodge et al., 2021; Longpre et al., 2023). Furthermore, research indicates that the distribution of web data does not inherently align with the ideal distribution for downstream applications (Tay et al., 2022; Wettig et al., 2023). For example, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict. Applying the same loss to all tokens can lead to inefficient computation on non-essential tokens, potentially restricting LLMs from achieving more advanced levels of intelligence.

To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In SS2.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are "easy tokens" that are already learned, and some are "hard tokens" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates.

Based on these analyses, we introduce Rho-1 models trained with a novel Selective Language Modeling (SLM) objective 3. As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss (SS2.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications (SS2.2).

We show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores

Figure 2: **Upper:** Even an extensively filtered pretraining corpus contains token-level noise. **Left:** Previous Causal Language Modeling (CLM) trains on all tokens. **Right:** Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.

on benchmarks for models trained with the selected tokens. SS3.2 shows the effectiveness of SLM on math continual pretraining: both 1B and 7B Rho-1 outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in Figure 1. Remarkably, Rho-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath. Upon fine-tuning, Rho-1-1B and 7B achieve 40.6% and 51.8% on MATH, respectively. Notably, Rho-1-1B is the first 1B LM to exceed 40% accuracy, nearing the early GPT-4's CoT performance of 42.5%. SS3.3 confirms the efficacy of SLM in general continual pretraining: Training Tinyllama-1B on 80B tokens with SLM improves 6.8% on average across 15 benchmarks, with gains over 10% in code and math tasks. In SS3.4, we demonstrate that in settings without high-quality reference data, we can use SLM for self-referencing, leading to an average improvement of up to 3.3% in downstream tasks.

## 2 Selective Language Modeling

### Not All Tokens Are Equal: Training Dynamics of Token Loss

Our investigation begins with a critical look at how individual tokens' losses evolve during standard pre-training. We continue pre-training Tinyllama-1B with 15B tokens from OpenWebMath, saving checkpoints after every 1B tokens. We then evaluate token-level loss at these intervals using the validation set of approximately 320,000 tokens. Figure 3(a) reveals a striking pattern: tokens fall into four categories based on their loss trajectory--persistent high loss (H\(\)H), increasing loss (L\(\)H), decreasing loss (H\(\)L), and consistent low loss (L\(\)L). For further details on these categories, see SSD.1. Our analysis uncovers that a mere 26% of tokens show a notable loss reduction (H\(\)L), while the majority (51%) remain in the L\(\)L category, indicating they have already been learned. Interestingly, 11% of the tokens are persistently challenging (H\(\)H), likely due to high aleatoric uncertainty (Hulermeier and Waegeman, 2021). Additionally, 12% of tokens experience an unexpected loss increase (L\(\)H) during training.

Our second observation is that a significant number of token losses exhibit persistent fluctuations, and resist convergence. The loss of many L\(\)L and H\(\)H tokens, as depicted in Figure 3 (b) and (c), show high variance during training. In SSD.2, we visualize and analyze the content of these tokens and find that many of them are noisy, which is consistent with our hypothesis.

Consequently, we learn that the loss associated with each token during training does not decrease smoothly like the overall loss; instead, there is a complex training dynamic among different tokens. If we can select the appropriate tokens for the model to focus on during training, we may be able to stabilize the trajectory of the model's training and enhance its data efficiency.

### Selective Language Modeling

OverviewInspired by the practice of reference model in document-level filtering, we propose a simple pipeline of token-level data selection, termed "Selective Language Modeling (SLM)". Our method comprises three steps, as depicted in Figure 4. We begin by training a reference model on a curated, high-quality dataset. This model then assesses the loss of each token within the pretraining

Figure 3: **The loss of four categories of tokens during pretraining.** (a) shows the loss of H\(\)H, L\(\)H, H\(\)L, and L\(\)L tokens during pretraining. (b) and (c) show three cases of fluctuating tokensâ€™ loss in L\(\)L and H\(\)H during pretraining, respectively.

corpus. In the final phase, we train the language model selectively, focusing on tokens with high excess loss between the training and reference model. The intuition is that tokens with high excess loss are more learnable and better aligned with the desired distribution, naturally excluding tokens that are either irrelevant or of low quality. Below, we provide a detailed description of each step.

Reference ModelingWe begin by curating a high-quality dataset that reflects the desired data distribution. We train a reference model (RM) using standard cross-entropy loss on the curated data. The resulting RM is then used to assess the token loss within a larger pretraining corpus. We compute the reference loss (\(_{}\)) of a token \(x_{i}\) based on the probability that the RM assigns to this token. The calculation is formalized as follows:

\[_{}(x_{i})=- P(x_{i}|x_{<i})\] (1)

By evaluating \(_{}\) for each token, we establish the reference loss for selective pretraining, allowing us to focus on the most influential tokens in language modeling.

Selective PretrainingNote that Causal Language Modeling (CLM) employs the cross-entropy loss:

\[_{}()=-_{i=1}^{N} P(x_{i}|x_{<i};)\] (2)

Here, \(_{}()\) represents the loss function parameterized by model \(\). \(N\) is the length of the sequence, \(x_{i}\) is the \(i\)-th token in the sequence, and \(x_{<i}\) represents all tokens before the \(i\)-th token. In contrast, Selective Language Modeling (SLM) trains the language model with a focus on tokens that exhibit a high excess loss when compared to the reference model. The excess loss (\(_{}\)) for a token \(x_{i}\) is defined as the difference between the current training model loss (\(_{}\)) and the reference loss:

\[_{}(x_{i})=_{}(x_{i})-_{}(x_{i})\] (3)

We introduce a token selection ratio \(k\%\), which determines the proportion of tokens to be included based on their excess loss. The cross-entropy loss for the selected tokens is computed as follows:

\[_{}()=-_{i=1}^{N}I_{k\%}(x_{i} ) P(x_{i}|x_{<i};)\] (4)

Here, \(N*k\%\) defines the number of tokens that fall within the top \(k\%\) of excess loss. The indicator function \(I_{k\%}(x_{i})\) is defined as:

\[I_{k\%}(x_{i})=1&$ ranks in the top $k\%$ by $S(x_{i})$}\\ 0&\] (5)

Figure 4: **The pipeline of Selective Language Modeling (SLM).** SLM optimizes language model performance by concentrating on valuable, clean tokens during pre-training. It involves three steps: (Step 1) Initially, train a reference model on high-quality data. (Step 2) Then, score each tokenâ€™s loss in a corpus using the reference model. (Step 3) Finally, selectively train the language model on tokens that have higher scores.

By default, we use \(_{}\) as the score function \(S\). This ensures that the loss is applied only to the tokens that are deemed most beneficial for the language model to learn from. In practice, token selection can be implemented by ranking the tokens in a batch according to their excess loss and using only the top \(k\%\) of tokens for training. This process eliminates the loss for undesired tokens without incurring additional costs during pretraining, making our approach both efficient and easily integrated.

## 3 Experiments

We continually pretrained models in both mathematical and general domain and designed ablation and analysis experiments to understand the effectiveness of SLM.

### Experimental Setup

Reference Model TrainingTo train our mathematical reference model, we gathered a dataset of 0.5B high-quality, math-related tokens. This dataset is a blend of synthetic data from GPT (Yu et al., 2024; Huang et al., 2024) and manually curated data (Yue et al., 2024; Ni et al., 2024). For the general reference model, we compiled a corpus of 1.9B tokens from open-source datasets, such as Tulu-v2 (Ivison et al., 2023) and OpenHermes-2.5 (Teknium, 2023). We trained the reference models for 3 epochs. The maximum learning rate was set at 5e-5 for 1B models and 1e-5 for 7B models, applying a cosine decay schedule. We set the maximum sequence lengths to 2048 for 1B models and 4096 for 7B models, packing multiple samples into these lengths for model input. In all main experiments, we initialized the continual pretraining model and the reference model with the _same_ base model.

   & |\)**Data**} &  &  &  &  &  &  &  &  &  &  & ^{}\)**} &  \\  & & & & & & & & & & & & & & & \\   \\  Tinyllama & 1.1B & - & - & - & 2.9 & 3.2 & 11.0 & 18.1 & 20.4 & 12.5 & 14.6 & 16.1 & 21.9 & 13.4 \\ Phi-1.5 & 1.3B & - & - & - & 32.4 & 4.2 & 43.4 & 53.1 & 66.2 & 24.4 & 14.3 & 21.8 & 18.8 & 31.0 \\ Qwen1.5 & 1.8B & - & - & - & 36.1 & 6.8 & 48.5 & 63.6 & 79.0 & 29.2 & 25.1 & 31.3 & 40.6 & 40.0 \\ Gemma & 2.0B & - & - & - & 18.8 & 11.4 & 38.0 & 56.6 & 72.5 & 36.9 & 26.8 & 34.4 & 50.0 & 38.4 \\ DeepSeeLM & 1.3B & OWM & 14B & 15OB & 11.5 & 8.9 & - & - & - & - & 29.6 & 31.3 & - \\ DeepSeeLM & 1.3B & - & 12OB & 15OB & 23.8 & 13.6 & - & - & - & - & - & 33.1 & 56.3 & - \\   \\  Tinyllama-CT & 1.1B OWM & 14B & 15B & 6.4 & 2.4 & 21.7 & 36.7 & 47.7 & 17.9 & 13.9 & 23.0 & 25.0 & 21.6 \\ Rho-1-Math & 1.1B OWM & 14B & 9B & 29.8 & 14.0 & 49.2 & 61.4 & 79.8 & 25.8 & 30.4 & 24.7 & 28.1 & 38.1 \\ \(\) & & -40\% & +23.4 & +11.6 & +27.5 & +24.7 & +32.1 & +7.9 & +16.5 & +1.7 & 43.1 & **+16.5** \\  Rho-1-Math & 1.1B OWM & 14B & 3OB & 36.2 & 15.6 & 52.1 & 67.0 & 83.9 & 29.0 & 32.5 & 23.3 & 28.1 & 40.9 \\   \\  LLAMA-2 & 7B & - & - & 14.0 & 3.6 & 39.5 & 51.7 & 63.5 & 30.9 & 12.4 & 32.7 & 34.4 & 31.4 \\ Mistral & 7B & - & - & 41.2 & 11.6 & 64.7 & 68.5 & 87.5 & 52.9 & 33.0 & 49.5 & 59.4 & 52.0 \\ Minerva & 8B & - & 39B & 164B & 16.2 & 14.1 & - & - & - & - & - & 35.6 & - \\ Minerva & 62B & - & 39B & 109B & 52.4 & 27.6 & - & - & - & - & 53.9 & - & - \\ Minerva & 540B & - & 39B & 26B & 58.8 & 33.6 & - & - & - & - & 63.9 & - & - \\ LLemma & 7B PPile & 55B & 200B & 38.8 & 17.2 & 56.1 & 69.1 & 82.4 & 48.7 & 41.0 & 45.4 & 59.4 & 50.9 \\ LLemma & 34B PPile & 55B & 5OB & 54.2 & 23.0 & 67.9 & 75.7 & 90.1 & 57.0 & 49.8 & 54.7 & 68.8 & 60.1 \\ Intern-Math & 7B & - & 31B & 125B & 41.8 & 14.4 & 61.6 & 66.8 & 83.7 & 50.0 & 57.3 & 24.8 & 37.5 & 48.7 \\ Intern-Math & 2OB & - & 31B & 125B & 65.4 & 30.0 & 75.7 & 79.3 & 94.0 & 50.9 & 38.5 & 53.1 & 71.9 & 62.1 \\ DeepSeeMath & 7B & - & 120B & 500B & 64.1 & 34.2 & 74.0 & 83.9 & 92.4 & 63.4 & 62.4 & 56.4 & 84.4 & 68.4 \\   \\  Mistral-CT & 7B OWM & 14B & 15B & 42.9 & 22.2 & 68.6 & 71.0 & 86.1 & 45.1 & 47.7 & 52.6 & 65.6 & 55.8 \\ Rho-1-Math & 7B OWM & 14B & 10.5B & 66.9 & 31.0 & 77.8 & 79.0 & 93.9 & 49.9 & 58.7 & 54.6 & 84.4 & 66.2 \\ \(\) & & -30\% & +24.0 & +8.8 & +9.2 & +8.0 & +7.8 & +4.8 & +11.0 & +2.0 & +18.8 & +10.4 \\  

Table 1: **Few-shot CoT reasoning results of math pretraining. All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. \({}^{*}\)Only unique math-related tokens are calculated. For Rho-1, we calculate only the selected tokens that are used for training. \({}^{}\)We use OpenAIâ€™s MATH subset (Lightman et al., 2023) for evaluation, since some original test samples have been used in public training sets such as PRM800k. \({}^{}\)The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.**

[MISSING_PAGE_FAIL:6]

16.5% on 1B models and 10.4% on 7B models. Furthermore, after training for multiple epochs on OpenWebMath, we find that Rho-1 could further increase the average few-shot accuracy to 40.9%. Compared to DeepSeeMath-7B, which pretrained on 500 billion math-related tokens, Rho-1-7B pretrained on only 15 billion tokens (selecting 10.5 billion tokens) achieved comparable results, demonstrating the efficiency of our approach.

Tool-Integrated Reasoning ResultsWe fine-tune Rho-1 and baseline models on 69k ToRA corpus , consisting of 16k GPT-4-generated trajectories in a tool-integrated reasoning format, and 53k answer-augmented samples using LLaMA. As presented in Table 2, Rho-1-1B and Rho-1-7B achieved a state-of-the-art 40.6% and 51.8% on MATH dataset, respectively. On some unseen tasks (_e.g.,_ TabMWP and GSM-Hard), Rho-1 also demonstrates a certain degree of generalizability, with an average few-shot accuracy improvement of 6.2% on the Rho-1-Math-1B and 2.7% on Rho-1-Math-7B.

### General Pre-training Results

We confirm the efficacy of the SLM in general pretraining by continual training Tinyllama-1.1B on 80 billion tokens. The results depicted in Figure 5 indicate that although Tinyllama has already undergone extensive training on the majority of these tokens, the application of SLM yields an average enhancement of 6.8% across 15 benchmarks compared to direct continual pretraining. The improvements were especially pronounced in code and math tasks, exceeding 10%.

### Self-Reference Results

In this section, we demonstrate that SLM can enhance the effectiveness of model pre-training using only pre-training corpora, without the need for additional high-quality data. Specifically, we initially trained the reference model on the OpenWebMath (OWM) corpus, a subset of Proof-Pile-2 (PPile). We evaluated OWM and PPile using the trained reference model and selected tokens for training. In this scenario, we assume the absence of downstream task-related data, a common situation in real-world applications. We hypothesize that the key factor is not scoring the desired distribution but filtering out noisy tokens. Therefore, we employed two different scoring functions based on the reference model loss, \(_{}\), and the information entropy of the next token, \(_{}\), which measures the uncertainty of the next token. Details are provided in Appendix H.

Figure 5: **General pretraining results.** We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is retained with CLM, while Rho-1 is trained with our proposed SLM.

[MISSING_PAGE_FAIL:8]

by the SLM method are closely related to mathematics, effectively training the model on the parts of the original corpus that are pertinent to mathematical content.

Furthermore, we investigated the differences in token filtering across various checkpoints during the training process and tested the perplexity of these tokens on different checkpoints. As illustrated in Figure 9, we found that the tokens selected by later checkpoints tend to have higher perplexity towards the later stages of training and lower perplexity in the earlier stages. This may suggest that the model first optimizes tokens with a larger learnable space, thereby increasing learning efficiency. Moreover, we noticed a sample-wise "double descent" [Nakkiran et al., 2021] on the loss of selected tokens, where the select token's perplexity initially increases before decreases. This might be an effect of selecting tokens based on excess loss, targeting those most in need at each checkpoint.

Effect of Token Select RatioWe investigate the impact of token selecting ratios of the SLM. Generally, the selecting ratio is defined by heuristic rules, similar to the approach previously employed in the training of Masked Language Models (MLMs) [Devlin et al., 2019, Liu et al., 2019]. As shown in Figure 9, the selected tokens is suitable for accounting for about 60% of the original tokens.

## 4 Conclusion

In this paper, we propose using Selective Language Modeling(SLM) to train Rho-1, which select more suitable tokens for current pretraining stage. We conducted the detailed analysis of the loss of tokens during the pretraining process and found that not all tokens are equal during pretraining. Ourexperiments and analysis in the fields of mathematics and general have demonstrated the effectiveness of the SLM method, emphasizing the importance of token level in the LLM pretraining process. In the future, how to improve pretraining of LLMs from the perspective of token level worthy of in-depth research.