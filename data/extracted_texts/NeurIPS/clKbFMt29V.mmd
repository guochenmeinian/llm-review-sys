# Self-Supervised Visual Acoustic Matching

Arjun Somayazulu\({}^{1}\)  Changan Chen\({}^{1}\)  Kristen Grauman\({}^{1,2}\)

\({}^{1}\)UT Austin \({}^{2}\)FAIR, Meta

###### Abstract

Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both the source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio--without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments. Project page: [https://vision.cs.utexas.edu/projects/ss_vam](https://vision.cs.utexas.edu/projects/ss_vam)

## 1 Introduction

The acoustic properties of the audio we hear are strongly influenced by the geometry of the room and the materials that make up its surfaces--large, empty rooms with hard surfaces like concrete and glass lead to longer reverberation times, while smaller, more cluttered rooms with soft materials like curtains and carpets will absorb sound waves quickly and produce audio that sounds dull and anechoic. Human perception exploits this acoustic-visual correspondence, and we rely on perceiving natural-sounding audio that is consistent with our environment as we navigate daily life.

Likewise, this phenomenon is important in virtual environments, such as in AR/VR applications. When one hears audio that is acoustically consistent with the virtual environment they are seeing, their brain can better integrate audio and visual information, leading to a more immersive experience. Conversely, when one hears audio that does not match the expected acoustics of the virtual environment, the perceptual mismatch can be jarring. The problem of audio-visual acoustic correspondence extends well beyond AR/VR applications. Film and media production involve recording audio in diverse spaces, which can be expensive and logistically challenging. Similarly, interior designers and architects face the problem of previewing how a space will sound before it is built.

Today's approaches for modeling acoustic-visual coherence typically assume physical access to the target space , which can be impractical or impossible in some cases. In particular, in _acoustic matching_, the audio captured in one environment is re-synthesized to sound as if it were recorded in another target environment, by matching the statistics (e.g. reverberation time) of audio samples recorded in that target environment . _Visual acoustic matching_ (VAM) instead takes an image of the target space as input, learning to transform the audio to match the likely acoustics in the depicted visual scene  (see Figure 1(a)). In both cases, however, learned models ideally have access to _paired_ training data, where each training audio clip is recorded in two different environments. This permits a straightforward supervised learning strategy, since a model can learn to transform one clip (source) to the other (target). See Figure 1(b), left. Unfortunately, this approachputs heavy demands on data collection: large-scale collection of paired data from a variety of diverse environments is typically impractical.

In-the-wild web videos provide us with a large, readily available corpus of diverse acoustic environments and human speakers. However, this data is _unpaired_--we only observe the sound recorded in the target space, without a second recording in a different environment for reference. Prior work attempts to turn unpaired data into paired data by using an off-the-shelf dereverberator model  to produce pseudo-anechoic source recordings from in-the-wild reverberant audio, which are then passed to a visual acoustic matching (VAM) model with the true reverberant audio as the target . While that approach is inspiring, it has a fundamental limitation. The automatic dereverberation process is necessarily imperfect, which means that _residual acoustic information indicative of the target environment's acoustics can remain in the (pseudo) source example_. In turn, the acoustic matching model trained to produce the target audio can learn to use those residual acoustic cues in the audio--instead of the target space's visual features. When evaluated at test-time on arbitrary source audio and unseen images, the residual acoustic clues exploited during training are no longer available, leading to poor acoustically matched audio outputs.

We propose a _self-supervised visual acoustic matching_ approach that accommodates training with unpaired data from in-the-wild videos (See Figure 1(b), right). Our key insight is a training objective that explicitly removes residual acoustics in the audio, forcing reliance on the visual target. In particular, our approach jointly trains an audio-visual _debiaser_--which is trained to adversarially minimize residual acoustic information in the dereverberated audio--alongside a reverberator that performs visual acoustic matching. To this end, we develop an _acoustic residue_ metric that quantifies the level of residual acoustic information in a waveform, based on the difference in performance between a) an acoustic matching model that conditions on the target image and b) a model that does not condition on any image. Intuitively, training on audio with low residual acoustic information frees the model from relying on that (unrealistic) information during training, allowing it to instead leverage the necessary visual cues.

We use a time-domain conditional GAN as our debiaser, and continually update the integrated reverberator as the distribution of generated audio shifts during training. Unlike prior work, our approach allows training directly on unpaired videos and speech.2

Our proposed LeMARA model outperforms existing approaches  on challenging in-the-wild audio and environments from multiple datasets. Further, to benchmark this task, we introduce a high audio-visual correspondence subset of the AVSpeech  video dataset. Though we focus on the task of visual-acoustic matching, our insight potentially has broader implications for other self-supervised multi-modal learning tasks in which one wants to control the impact of a dominating modality.

## 2 Related work

Room acoustics and spatial soundAudio-visual acoustic matching has limited prior work, though there is growing interest from vision and audio communities. Image2Reverb  learns to map an input image to its corresponding Room Impulse Response (RIR)--the transfer function that

Figure 1: **Self-supervised visual acoustic matching. (a) Given source audio and target image1, the goal is to re-synthesize the audio to reflect the acoustics of the target environment. b) Two possible data settings: the _paired audio_ setting (left) observes both the source audio and the audio in the target environment, allowing for supervised training, while the _unpaired audio_ setting (right), observes only the audio in the target environment. Our self-supervised strategy handles the unpaired setting.**

characterizes acoustics at a particular listener/speaker location--which can then be convolved with an arbitrary source waveform. In audio-only approaches,  generate RIRs from reverberant audio clips, and  uses a WaveNet model to warp source audio into a target space conditioned on a learned embedding of the target audio. Generalizing RIR inference to full 3D environments, Few-Shot RIR  and Neural Acoustic Fields  sample impulse responses at multiple places in an environment in order to synthesize the RIR at novel locations with a transformer. In related tasks, Novel View Acoustic Synthesis  directly synthesizes a source audio at a new camera pose in the room, while other methods binauralize monaural source audio . Unlike our model, which can train from arbitrary images, existing methods require knowledge of the ground truth RIRs  or paired source-target audio . Most relevant to our approach, AViTAR  uses a cross-modal transformer to re-synthesize the audio; it relies on an off-the-shelf audio-visual dereverberator trained in simulation  to produce a pseudo-clean source signal, and suffers from the acoustic residue issue discussed above. Our results illustrate how our model overcomes this shortcoming.

Speech synthesis and enhancementRecent work for speech re-synthesis treats acoustic properties as a "style" which can be disentangled from underlying speech and used to perform acoustic-style matching of audio . However, they require either paired data or speaker embeddings to be learned. Web videos (our target domain) can consist of entirely unique speakers, making it difficult to learn a robust speaker embedding. While environment-aware text-to-speech  is applicable even in settings where each speaker in the dataset is unique, the model requires access to paired speech for supervised training. Supervised methods for speech enhancement and dereverberation assume access to paired training data, i.e., anechoic "clean" reference waveforms alongside the altered waveforms . Unsupervised speech enhancement approaches  such as MetricGAN-U  optimize generic perceptual speech quality scores, such as PESQ  (which requires paired samples) and speech-to-reverberation modulation energy (SRMR)  (which does not). While we share the objective of relaxing supervision requirements, our overall goal is distinct: rather than optimize a generic quality metric, we aim to retarget the input sound to match a specific (pictured) environment. To achieve that goal, we introduce a novel debiasing objective applicable in a conditional GAN framework.

## 3 Approach

We present **L**earning to **M**atch **A**coustics by **R**emoving **A**coustics, **LeMARA**, to achieve self-supervised visual acoustic matching (VAM). Let \(A\) denote an audio waveform and let \(V\) denote an image frame. During training, we are given \(N\) unlabeled examples of _target_ audio and scenes \(\{(A_{t},V_{t})\}_{t=1}^{N}\), taken from frames and accompanying audio in YouTube videos (cf. Sec. 4 for dataset details). While the data is multi-modal, it is unpaired: it has both audio and visual features, but it lacks a paired sample of the audio in some other source domain (see Fig. 1). With this data, we wish to learn a function \(f(A_{s},V_{t}):,\) that takes _source_ audio \(A_{s}\) (which may or may not be reverberant) and a _target_ image \(V_{t}\), and produces the audio re-synthesized to sound like it was recorded in the target scene.

To self-supervise \(f\) using unpaired training data, we would like a model that (1) dereverberates \(A_{t}\) to strip it of its room acoustics, yielding pseudo-source audio \(_{t}^{(s)}\) and then (2) reverberates \(_{t}^{(s)}\) by "adding" in the room acoustics of image \(V_{t}\) to regenerate the target audio: \(f(_{t}^{(s)},V_{t}) A_{t}\). A naive joint training of two such modules--a dereverberator and reverberator--would collapse to a trivial solution of \(\) doing nothing. A better solution would pre-train the dereverberator with a well-trained audio-only model, yet as we show in experiments, this leaves residuals of the target environment in the audio \(_{t}^{(s)}\) that handicap the reverberator at inference time, when such signals will not exist.

The key insight of our approach is to make the dereverberation stage a _de-biasing_ stage that is explicitly trained to remove room acoustics information not available in the target image \(V_{t}\). This forces reliance on the visual target and helps our model \(f\) generalize to unseen audio and scenes.

Our model has two main components that are trained jointly: (1) a de-biaser model \(G\) responsible for removing room acoustics from the target audio and (2) a visually-guided reverberator \(R\) that injects acoustics from the target environment into the output waveform. We first introduce the de-biaser architecture (Sec. 3.1), followed by our novel acoustic residue metric that is optimized during de-biaser training (Sec. 3.2), and our training strategy that allows joint fine-tuning of the reverberator and de-biaser (Sec. 3.3). Finally, we present our approach for training self-supervised VAM (Sec. 3.4). Figure 3 overviews our model.

### De-biasing Conditional Generative Adversarial Network

The role of our de-biaser is to dereverberate audio samples in a way that minimizes any residual room acoustics information. To that end, we devise an adversarial de-biased based on MetricGANs [12; 14]. MetricGANs are similar to conventional generative adversarial networks (GAN) --with a generator that aims to enhance a speech waveform--except the discriminator's job is to mimic the behavior of some target _quality function_. Our de-biased module extends the basic MetricGAN, augmenting it with novel acoustic residue metric (Sec. 3.2) and training procedure (Sec. 3.3) that accounts for the evolving distribution of the de-biased audio.

Our GAN consists of a generator \(G\), a discriminator \(D\), and an audio quality metric \(\). \(D\) is trained to approximate the quality metric \(\), and \(G\) is trained to maximize the metric score on generated data, using \(D\) as a learned surrogate of the metric function \(\). \(G\) is a conditional generator: given an input waveform, it produces a modified waveform which optimizes the quality metric score.

Let \(\{A_{t},V_{t}\}\) be a dataset of target audio-image pairs, and let \((A_{t},V_{t})\) be our quality metric \(\) (defined below) that produces a scalar measure of the residual room acoustic information in audio. As in the conventional GAN framework, we alternate between discriminator and generator updates. During an epoch of discriminator training, \(D\) trains to approximate the metric function \(\)'s scores on both raw audio \(A_{t}\) and generated audio \(G(A_{t})\). The discriminator loss function is:

\[_{D}=\|D(A_{t})-(A_{t},V_{t})\|_{2}+\|D(G(A_{t}))- (G(A_{t}),V_{t})\|_{2}+\|D(A_{hist})-s_{hist}\|_{2}, \]

where the first and second terms incentivize \(D\) to produce score estimates that approximate the metric function when evaluated on raw input audio (\(A_{t}\)) and generated audio \(G(A_{t})\), respectively. Following , the third term trains the discriminator on samples from a historical replay buffer \(\{(A_{hist},s_{hist})\}\), where \(A_{hist}=G_{prev}(A_{t})\) is a generated sample from a previous epoch, and \(s_{hist}=_{prev}(G_{prev}(A_{t}),V_{t})\) is its associated metric score. Training on these historical samples helps improve stability and mitigates issues with catastrophic forgetting in the discriminator.

The generator is trained with an adversarial loss, using the discriminator \(D\) learned from the previous epoch of discriminator training (which depends only on \(A_{t}\)) as a surrogate of the true metric \(\) (which depends on both \(A_{t}\) and \(V_{t}\)). The generator loss is

\[_{G}=\|D(G(A_{t}))-1\|_{2}. \]

Our metric is normalized to produce scores between 0 and 1 (1 being optimal), so this loss forces \(G\) to generate audio that maximizes the estimated metric score. Next, we introduce our metric \(\).

### Acoustic Residue Metric

Rather than train the de-biased GAN to optimize a generic speech quality metric [12; 14], we wish to quantify the amount of _residual room acoustics information_ in an audio sample. Hence, we define a metric \(\) that allows the downstream reverberator model \(R\) itself to quantify the level of residual acoustic information in the waveform. Specifically, the metric consists of two models trained to perform reverberation on dereverberated speech. Importantly, one model \(R_{v}\) has been trained to perform VAM (using the target image as conditioner), while the other, \(R_{b}\), has been trained to perform blind acoustic matching (without the target image as conditioner). We next define the reverberator modules, and then return to their role in \(\).

Inspired by recent work in time-domain signal modeling [45; 36; 5], we use a WaveNet-like architecture for the reverberators consisting of multiple stacked blocks of 1D convolutional layers, with an optional gated fusion network to inject visual information for \(R_{v}\). Similar to , the reverberators use a sinusoidal activation function followed by two separate 1D conv layers that produce residual and skip connections, the latter being mean pooled and fed to a decoder to produce reverberated audio. We choose this model because it is parameter-efficient, consisting entirely of 1D convolutions, and because it allows time-domain conditional waveform generation. See Sec. 5 for training details.

We use these models to compute the acoustic residue metric. Given input audio \(A\) and image \(V\), our metric is defined as:

\[(A,V)=((R_{b}(A))-(A_{t} )|-|(R_{v}(A,V))-(A_{t})|}{(0.1,(A_{t}))}), \]where \(A_{t}\) denotes the known target audio and \(\) is a scalar-output function characterizing the general reverberant properties of its input audio, which we define below.

Eqn. 3 quantifies the level of acoustic residue--that is, how much greater the blind reverberator's error is compared to the visually-guided reverberator's error. If de-biasing of \(A\) has gone well, this value will be high. When evaluated on audio that contains a high level of residual acoustic information, the visual features will not provide additional useful information, resulting in similar performance by both visual and blind reverberation models. In other words, if the two errors are similar, visual is not adding much, and there is room acoustic information lingering in the audio \(A\). This pushes the \(\) score to be smaller (poor quality under the metric). On the other hand, when a waveform contains very little residual acoustic information, the visual features will help the visual model \(R_{v}\) produce a more accurate acoustically matched waveform model with lower error compared to the blind model \(R_{b}\). This will result in a higher metric score.

Figure 2 demonstrates the effect of de-biasing. Reverberant audio (left) has been imperfectly dereverberated (middle), leaving residual reverberation trails which contain information about the original acoustic environment. De-biased audio (right) removes these residual artifacts. The waveform plots (right) show de-biased audio (green) significantly attenuates the long sound decay present in both reverberant (blue) and dereverberated (orange) audio. This forces the reverberator to learn acoustics from the target image.

For the function \(\) in Eqn. 3, we leverage a classic content-invariant metric for characterizing room acoustics: the Reverberation Time at 60 or "RT60". RT60 is the amount of time after a sound source ceases that it takes for the sound pressure level to reduce by 60 dB--which depends directly on the geometry and materials of the space. For example, no matter the initial direct sound, a big cathedral will yield a high RT60, and a cozy living room will yield a low RT60. While RT60 can be quantified by sensing when one has access to the physical environment, we use a learned estimator for RT60 to allow generalization (see Sec. 5 for details). In Eqn. 3, the normalization by the RT60 of the target audio improves stability of discriminator training. We use the clipping function \(max(0.1,)\) here to prevent samples with extremely low RT60 from destabilizing training.

Training with this acoustic residue metric allows the downstream reverberator models \(R_{v},R_{b}\) themselves to improve the performance of the de-biaser model \(G\). Unlike SRMR, DNSMOS , or any existing off-the-shelf metric that quantifies general speech quality, our metric directly addresses the problem of _residual_ acoustic information in audio. Although \(G\) may learn a function similar to that of dereverberation, we use the term de-biaser to describe the generator to highlight the novel training objective it is trained against, which distinguishes it from a conventional dereverberation model.

### Joint Training of the De-biaser and Reverberators

At initialization, \(R_{v}\) and \(R_{b}\) are trained on a certain distribution of speech. When training the de-biasing GAN with the acoustic residue metric, generated speech can eventually fall out of the distribution on which these reverberators were trained, causing \(\) to produce unreliable metric scores that destabilize training. To address this, we introduce a strategy to update \(R_{v}\) and \(R_{b}\) during training of the de-biasing GAN. Updating these models ensures that \(\) consistently produces reliable acoustic residue scores as the distribution of generated speech shifts over the course of GAN training.

At the start of GAN training, we initialize the _target networks_\(R_{v}^{t}\) and \(R_{b}^{t}\) as copies of \(R_{v}\) and \(R_{b}\) respectively. During discriminator training, each batch of \(\{(G(A_{i}),V_{i})\}\) samples are passed to \(R_{v}\) and \(R_{b}\) to compute metric scores under their current frozen state. This batch is also passed to the

Figure 2: **De-biased audio. The de-biaser removes residual acoustic traces in audio, and attenuates long sound decay faster than in dereverberated speech.**

target networks, which compute the losses

\[_{} =||R_{v}^{t}(G(A),V)-A||_{2} \] \[_{} =||R_{b}^{t}(G(A))-A||_{2} \]

and perform an update step. Every \(E\) epochs, the target networks' weights are copied over into the metric networks. This update strategy allows \(R_{v}\) and \(R_{b}\) to be jointly fine-tuned with the de-biaser model \(G\). Figure 3 overviews the model components and data flow.

### Training and Inference

Training proceeds in three steps. (1) First we pretrain the de-biaser \(G\). This entails pretraining a MetricGAN-U  with the Speech-to-Energy-Modulation-Ratio (SRMR) metric  on speech pre-processed with our off-the-shelf dereverberator (see Sec. 5). By refining the dereverberated output with the MetricGAN-U, we improve its quality and intelligibility without introducing additional supervision requirements. (2) Second, we pretrain the reverberators that perform (visual) acoustic matching. Specifically, we train \(R_{v}\) and \(R_{b}\) with \(_{visual}\) and \(_{blind}\), respectively, using the dereverberated and SRMR-optimized outputs from the MetricGAN in step (1). (3) Finally, we jointly fine-tune both the de-biaser \(G\) and reverberators, using the acoustic residue metric (Eqn. 3) for the GAN metric \(\), the generator and discriminator losses given in Eqns. 2, and 1 together with our alternating training scheme defined in Sec. 3.3. To improve stability in training, since the discriminator \(D\) starts with a good approximation of the SRMR metric, we continue training in step 3 using a weighted combination of SRMR and our residue metric: \((A)+(1-)(A,V)\). Although the acoustic residue is approximately differentiable, the implementation of SRMR we use is not, motivating our use of MetricGAN over an end-to-end alternative. This approach also allows for extensibility to other non-differentiable speech quality scores such as PESQ.

At test time, we use LeMARA for visual-acoustic matching as follows: given a source audio \(A_{s}^{(q)}\) and target environment image \(V_{t}^{(q)}\), we apply the trained de-biaser \(G\) followed by the visual reverberator \(R_{v}\):

\[f(A_{s}^{(q)},V_{t}^{(q)})=R_{v}(G(A_{s}^{(q)}),V_{t}^{(q)}). \]

Altogether, our approach adds the room acoustics depicted in \(V_{t}^{(q)}\) to the input audio. In the case that the source audio \(A_{s}^{(q)}\) is known to be anechoic (e.g., a user is using LeMARA to re-synthesize stock anechoic sounds for a new scene), then we simply bypass the de-biaser \(G\) and directly apply \(R_{v}\).

Figure 3: **LeMARA overview. a) Training procedure**. Reverberant audio is first processed with an off-the-shelf dereverberator. It is then input to a de-biaser model which strips acoustics from the audio. The clean audio is passed to the reverberator along with the target image for acoustic matching. **b) De-biaser architecture**. \(G\) is trained to adversarially maximize the score of the discriminator \(D\), which learns a surrogate of the acoustic residue metric \(\) (Sec. 3.1 and 3.2). **c) Acoustic residue metric**. Both \(R_{v}\) and \(R_{b}\) are continually trained on generated data to ensure that they provide accurate metric scores as the distribution of generated data evolves during training (Sec. 3.3). **At test time**, we use the trained de-biaser \(G\) and the visual reverberator \(R_{v}\) to perform VAM.

## 4 Datasets

We use two datasets: SoundSpaces-Speech  and AVSpeech . See Figure 4. For all results, we test only on audio and environments that are not observed during training.

SoundSpaces-SpeechSoundSpaces-Speech  is an audio-visual dataset created using the SoundSpaces audio simulation platform  together with 82 3D home scans from Matterport3D  and anechoic speech samples from LibriSpeech . SoundSpaces offers state-of-the-art generation of Room Impulse Responses (RIRs) computed at regular source-listener locations throughout the environment, accounting for all major real-world acoustic phenomena (reverberation, early specular/diffuse reflections, etc.) See  for details.

By convolving an anechoic sample with the appropriate RIR, it allows realistic simulation of audio reverberation at arbitrary locations in the pre-scanned real-world Matterport environments. A 3D humanoid is rendered at the source location in the Matterport home. SoundSpaces-Speech consists of anechoic speech samples from LibriSpeech paired with their acoustically-correct reverberated waveform (rendered using SoundSpaces) in any of 82 unique environments, together with an RGB-D image at the listener's position. To train on SoundSpaces-Speech in a self-supervised setting, we discard the source audio and only use the target audio (simulated reverberant audio). We use train/val/test splits of 28,853/280/1,489 samples.

AVSpeech-RoomsAVSpeech  is a large-scale video dataset consisting of 3-10 second clips from YouTube videos, most of which feature a single speaker and little background noise. Not all of the clips have naturalistic audio-visual correpondence, due to video editing tricks, microphone locations, virtual backgrounds, etc. Hence, we create a subset of AVSpeech, called AVSpeech-Rooms, that preserves samples with useful information about room geometry and material types (see Supp. for details). A randomly selected frame from the video clip is used as the target image. Our final set consists of 72,615/1,911/1,911 train/val/test samples. See Figure 4 (bottom).

## 5 Experiments

Implementation DetailsWe use the procedure outlined in Sec. 3.4 to train on SoundSpaces-Speech. For training on AVSpeech-Rooms, we proceed directly to step (2), pre-training \(R_{v}\) and \(R_{b}\) on audio that has been de-biased using the fine-tuned SoundSpaces-Speech de-biaser model (instead of an SRMR-optimized model trained on AVSpeech-Rooms). We then proceed to step (3) as in SoundSpaces-Speech. We refer to this setup as "shortcut training" to highlight our use of the SoundSpaces-Speech trained de-biaser to bypass step (1) when training on AVSpeech-Rooms. While AVSpeech can be trained with the full procedure outlined in Sec. 3.4 (see ablations in Supp.), shortcut training allows us the advantage of the strong prior for de-biasing learned by the fine-tuned SoundSpaces de-biaser.

We train LeMARA using the combined acoustic residue metric with \(=0.7\). We train a WaveNet-based dereverbator  ("off-the-shelf") on paired SoundSpaces-Speech audio which is "reversed" (reverberant input audio, anechoic target audio). We use this dereverberator for both LeMARA and the ViGAS  baseline. Prior VAM work  used an audio-visual dereverberator  trained on both

Figure 4: **Datasets. SoundSpaces-Speech** (top row) renders panoramic views of people in indoor environments. **AVSpeech-Rooms** (bottom) contains a wide variety of naturalistic indoor environments with diverse acoustic properties.

simulated and real-world data to pre-process reverberant audio. For fair evaluation, we train their model with their same original audio-visual dereverberator.3

We adapt our code for the reverberator models and ViGAS from .4. We train ViGAS with the same hyperparameters and loss as our reverberators during pre-training. Our de-biaser is adapted from the speechbrain MetricGAN-U implementation . Our RT60 estimator is trained on pairs of reverberant samples from a SoTA audio simulator  paired with the ground truth RT60 for the RIR used to produce the reverberant sample. We use a Resnet18  model to encode our visual features on RGB images. The last feature map before the final pooling layer is flattened and used as the visual feature conditioner. See Supp. for training details and architecture for these models. We plan to release our code to facilitate further research.

MetricsWe rely on three metrics to evaluate quality of VAM models. **STFT Error** and **logSTFT Error** compute the MSE loss and log MSE loss, respectively, between the magnitude spectrograms of predicted and target speech. Because we do not have access to the ground truth RIR, we also use an RIR reverberation metric that can be reliably estimated from audio, **RT60 Error (RTE)**, which measures the MSE between RT60 estimates of predicted and target speech. The first two apply only when we have ground truth re-synthesized audio (in simulation), while the third is content-invariant and captures room acoustics signatures for any target.

Seen and unseen evaluationWe report results on two different settings: _unseen environments_, where both the source audio \(A_{s}\) and target image \(V_{t}\) come from the test set; and _seen environments_, where an audio sample \(A_{s}\) from the test set is paired with a target image \(V_{t}\) from the train set (seen by the model). The unseen environment setting is important for evaluating generalization to novel scenes (e.g. matching an audio sample to an image from the Web), while the seen environment setting is useful for cases in which we already have video recordings, such as in film dubbing.

BaselinesAs baselines, we compare to state-of-the-art models for audio-visual re-targeting: (1) **AVITAR**, the only prior method that addresses the visual-acoustic matching task. It consists of a Transformer for audio-visual fusion, followed by a generator that synthesizes the reverberant waveform from the audio-visual latent feature. As discussed above, for self-supervised training, the authors use a pre-trained audio-visual dereverberation model  to create pseudo-source audio, which is passed as input to the model. (2) **ViGAS**, a model designed for novel-view acoustic synthesis, conditioned on a camera pose. We adopt its Acoustic Synthesis module, a WaveNet model based on , for our task. To apply it for VAM, we replace the camera pose with the flattened feature from the ResNet. (3) **Non-visual LeMARA**. We evaluate LeMARA with the blind reverberator \(R_{b}\) fine-tuned during training. (4) **Input audio**. We copy the dereverberated audio to the output. AViTAR and ViGAS are trained with the data augmentation strategy introduced in  (see Supp. for details).

Results on SoundSpaces-SpeechTables 1 and 2 report results on SoundSpaces-Speech (left three columns) for the unseen and seen settings, respectively. LeMARA outperforms the baselines on most metrics. Non-visual LeMARA performs significantly worse, indicating LeMARA effectively utilizes visual features during acoustic matching. This demonstrates the success of our acoustic de-biasing, which forces stronger learning from the visual stream. Notably, our model significantly outperforms

   Train &  &  \\ Test &  &  &  \\ Model & RTE (s) & STFT & logSTFT & RTE (s) & STFT & logSTFT & RTE (s) \\  Input audio & 0.320 & 1.427 & 1.274 & 0.310 & **1.327** & 2.107 & 0.358 \\ AViTAR & 0.080 & 2.471 & 1.629 & 0.136 & 2.894 & 1.290 & 0.239 \\ ViGAS & 0.108 & 4.373 & **1.232** & 0.109 & 7.007 & 0.662 & 0.254 \\  LeMARA(no vis) & 0.152 & 5.612 & 1.884 & 0.137 & 6.256 & 0.705 & 0.223 \\ LeMARA (ours) & **0.079** & **0.690** & 1.530 & **0.071** & 6.298 & **0.571** & **0.210** \\   

Table 1: VAM results on two datasets (unseen environments). RTE standard errors are less than 0.005, and STFT standard errors are 0.031 and 0.64 for the two datasets, respectively.

ViGAS--which shares the same architecture as our reverberator --indicating that our performance improvement over AViTAR can be attributed to our novel training objective, and not simply due to a shift in architecture from Transformer to WaveNet.

Results on AVSpeech-RoomsTables 1 and 2 show results on AVSpeech-Rooms (right four columns) for both unseen and seen settings, respectively. We further divide the unseen evaluation into two scenarios: (1) Where the source audio and target visual come from the same (unobserved) AVSpeech-Rooms sample and (2) where the source audio comes from LibriSpeech , a dataset of anechoic source samples of people reading passages in English (rightmost column). In both cases the target visual environment is a frame from an unseen AVSpeech video.

LeMARA outperforms the baselines in both settings on RTE. We outperform ViGAS in the LibriSpeech scenario despite sharing the same reverberator architecture, highlighting the efficacy of our novel training objective. Figure 5 (right) shows the distribution of RT60 values for audio reverberated by our model (blue), the baselines, and the target RT60 distribution (purple). Our model best matches the target RT60 distribution. LeMARA also outperforms baselines across samples with a diverse range of real-world reverberation intensities. We stratify the test set by ground truth RT60 and evaluate performance on samples within each reverberation bin (Figure 5, left). LeMARA outperforms the baselines across all reverberation levels, demonstrating its robustness to variation in real-world data.

Although our model performs poorly on STFT error, we significantly outperform existing approaches on logSTFT error, which better reflects perceptual quality given the logarithmic nature of human hearing. Furthermore, the naive baseline achieves the lowest STFT error, indicating that dereverberated audio itself strongly resembles reverberant audio even prior to acoustic matching. Models that use this dereverberator without further de-biasing will display artificially low STFT error when evaluated in-dataset (AVSpeech-Rooms \(\) AVSpeech-Rooms). Thus, it is important to balance the in-dataset evaluation with the LibriSpeech generalization case (far right in Table 1, where we soundly outperform Input audio) to gain a complete picture of model performance.

Furthermore, AVSpeech-Rooms contains a variety of non-speech sounds (e.g. air conditioning, clicking/tapping noises from object interactions) which make reverberation even more challenging. These signals may be perceptually weak, but they appear on spectrograms and contribute to the larger

  &  &  \\ Model & RTE (s) & STFT & logSTFT & RTE (s) & STFT & logSTFT \\  AViTAR & 0.062 & 3.186 & 1.506 & 0.131 & **2.852** & 1.273 \\ ViGAS & 0.076 & 6.386 & **0.962** & 0.111 & 6.946 & 0.657 \\ LeMARA (ours) & **0.060** & **0.667** & 1.417 & **0.067** & 6.198 & **0.570** \\ 

Table 2: Results on seen environments. We evaluate VAM using audio samples from the test set paired with target images from the train set.

Figure 5: **Normalized RTE across reverberation levels (left). We evaluate RTE normalized by target RT60 on AVSpeech-Rooms samples with varying levels of reverberation. LeMARA consistently achieves lowest error at all reverberation levels. RT60 distribution (right). The distribution of RT60 values for audio generated by LeMARA best matches the target distribution.**

STFT error we observe on AVSpeech-Rooms. In contrast, SoundSpaces-Speech contains no artifacts or background noise that could contribute to spectrogram errors.

An ablation study of our model analyzing the impact of varying the target metric during training shows the clear advantage of our residue metric compared to SRMR alone (see Supp.). In particular, training with an SRMR objective alone yields an RTE of 0.2308 on LibriSpeech, whereas training with a pure acoustic residue metric yields an RTE of 0.2156; our combined metric yields an RTE of 0.2123. This reinforces the efficacy of our metric as a training objective: de-biasing is distinct from generic enhancement.

Figure 6 shows examples of our model's generated sounds for different target images, compared to the output of AViTAR  (the best performing baseline). We display the RT60 of the source audio, the visual scene's ground truth RT60, and the RT60 of the two method's generated audio. In the majority of cases, our model produces audio that more closely matches the true acoustics of the visual scene. Our model learns to add little reverberation in enclosed environments with soft surfaces (top left), and to add more reverberation in open acoustic environments with hard surfaces (bottom right). We also highlight a failure mode in which our model does not inject proper acoustics (bottom left), likely due to the irregular shape of the room. See our project webpage to listen to examples.

Human perception studyWe augment these quantitative results with a human subject study, in order to gauge whether listeners perceive our results as successfully retargeting the audio to the pictured environment. We design a survey using 23 images \(V_{t}^{(q)}\) selected from AVSpeech-Rooms that show room geometry and materials clearly, and are representative of a diverse variety of acoustic environments. We couple those with 23 anechoic source samples \(A_{s}^{(q)}\) from LibriSpeech . For each sample, we generate the acoustically matched audio with both LeMARA and AViTAR --the best performing baseline. We anonymize and shuffle the generated audio, and ask 10 subjects with normal hearing to identify which room matches best with the audio among three given rooms, one of which is the true room \(V_{t}^{(q)}\). Users correctly identified the target room with 46.1% accuracy on speech generated by LeMARA versus 34.7% accuracy with speech generated by AViTAR. This shows our model achieves higher quality acoustic matching according to human perception. That said, the subjects' accuracy rates are fairly low in the absolute sense, which suggests both the difficulty of the problem and subtlety of the perception task. Our results have pushed the state of the art, both objectively and subjectively, but there remain challenges to fully mature visual acoustic matching.

## 6 Conclusion

We introduced a self-supervised approach to visual acoustic matching. Built on a novel idea for disentangling room acoustics from audio with a GAN-debiaser, our model improves the state of the art on two datasets. Our acoustic residue metric and adversarial training has potential to generalize to other multi-modal learning settings where there is risk of unintentionally silencing a paired modality during training. For example, our framework could be explored for binauralization of mono sounds using video or audio-visual source separation. In future work, we plan to explore generalizations to spatial dynamics that would account for movement of a speaker throughout 3D space.

Figure 6: **Acoustically matched audio.** LeMARA produces audio with more accurate acoustics than AViTAR  across diverse acoustic and visual scenes. Shown here for LibriSpeech setting.

Acknowledgments:Thanks to Ami Baid for help in data collection. UT Austin is supported in part by the IFML NSF AI Institute. K.G. is paid as a research scientist at Meta.