# TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios

Yuheng Shi

Tianjin University

yuheng@tju.edu.cn

&Zehao Huang

TuSimple

zehaohuang18@gmail.com

Yan Yan

TuSimple

yanyan.paper@outlook.com

&Naiyan Wang

TuSimple

winsty@gmail.com

&Xiaojie Guo

Tianjin University

xj.max.guo@gmail.com

###### Abstract

Time-to-Contact (TTC) estimation is a critical task for assessing collision risk and is widely used in various driver assistance and autonomous driving systems. The past few decades have witnessed development of related theories and algorithms. The prevalent learning-based methods call for a large-scale TTC dataset in real-world scenarios. In this work, we present a large-scale object oriented TTC dataset in the driving scene for promoting the TTC estimation by a monocular camera. To collect valuable samples and make data with different TTC values relatively balanced, we go through thousands of hours of driving data and select over 200K sequences with a preset data distribution. To augment the quantity of small TTC cases, we also generate clips using the latest Neural rendering methods. Additionally, we provide several simple yet effective TTC estimation baselines and evaluate them extensively on the proposed dataset to demonstrate their effectiveness. The proposed dataset and code are publicly available at dataset link and code link.

## 1 Introduction

In recent years, there has been a growing trend towards equipping vehicles with Advanced Driver Assistance System (ADAS), which consists of several subsystems such as Adaptive Cruise Control (ACC), Automated Emergency Braking (AEB) and Forward Collision Warning (FCW). ADAS aims to detect potential hazards as quickly as possible and alert the driver, or take corrective action to improve driving safety. AEB and FCW are critical features of ADAS that protect drivers and passengers and prevent traffic accidents. They both rely on the estimation of Time-to-Contact (TTC) which is defined as the time for an object to collide with the observer's plane. Although TTC can be predicted using data from various sensors, such as LiDAR, radar or camera. Vision-based methods are particularly attractive due to their low-cost and have been a popular choice among ADAS designers and manufacturers. Even in high-level (L3+) autonomous driving system, direct TTC estimation could also serve as an redundant observation when other distance measuring sensors fail.

Prior to the widespread adoption of deep learning, numerous vision-based theories and algorithms [25; 29; 10; 6; 4] for estimating TTC had been proposed. These algorithms are not data-driven, and usually rely on hand-crafted cues. Recently, several deep learning based TTC estimation algorithms have emerged [1; 45] and demonstrate promising results in driving scenarios. The emergence of deeplearning has brought more powerful tools to computer vision and also brought higher demands for large-scale datasets. However, due to the lack of large-scale TTC datasets that capture real-world driving scenarios, these methods have to pre-train their model on synthetic flow datasets [27; 12; 22].

In this paper, we primarily address the challenge of TTC estimation in highway scenarios. Compared to urban scenarios, high speed driving in highway exhibits longer braking distances, thereby necessitating a broader range of perception capabilities. In order to facilitate the development of vision-based TTC estimation algorithms, we propose a large-scale monocular TTC dataset in this paper, using a class 8 heavy truck as the data collection platform. From raw data collected in urban and highway scenarios, we identify over 200K sequences covering a depth range of 400 meters. Each sequence contains six consecutive frames captured at a rate of 10Hz, with 2D, 3D bounding box and TTC ground-truth provided for a single object in each frame. Additionally, to address the limited availability of samples in rare scenarios, such as sudden braking (e.g. small TTC cases), we utilize Neural Radiance Fields (NeRF)  to generate additional data. These artificially generated data can be seamlessly integrated into our dataset, thereby increasing the quantity and diversity of data available for training. Fig.1 illustrates two typical examples from our dataset: vehicles are gradually approaching in real scenes and NeRF scenes, respectively. Besides the proposed dataset, we focus on object level TTC estimation rather than pixel level TTC estimation in . Specifically, we provide a sequence of images for a certain object and ask for estimating the TTC value of it in the last frame. 2D Bounding boxes are available as optional inputs. Then we provide simple yet effective baselines based on the relationship between the scale ratio of objects in adjacent frames and TTC. We reformulate the problem as choosing the scale with the highest similarity in adjacent frames. Inspired by recent studies [1; 2], we further transform scale estimation from a regression problem into a set of binary classification tasks. A series of quantitative experiments are conducted to demonstrate the effectiveness and feasibility of our proposed techniques. Our main contribution can be summarized as follows:

* We propose a large-scale monocular TTC dataset for driving scenarios and will make it publicly available along with relevant toolkits to facilitate the development of TTC estimation algorithms for driving scenes.
* We propose two simple yet effective TTC estimation algorithms and extensively test them on the dataset to validate their effectiveness, which could serve as baseline methods for future study.

## 2 Related work

**Task and Methods.** In the scheme of monocular TTC estimation, TTC describes the time that an object will cross the camera plane under concurrent relative velocity. Denote the depth of an object in the camera coordinate as \(y\), the time for the object under the current velocity to cross the camera plane could be calculated by:

\[=-y/=-y/,\] (1)

Figure 1: Example sequences and annotations from our dataset. The \(\) denotes the TTC ground-truth while the subscript denotes the frame index. We could observe that, the scale of the object increases as the TTC decreases.

where \(\) is the relative velocity between the object and the camera. Though estimating either velocity or depth is an ill-posed problem, TTC can be estimated from images directly because it only depends on the ratio of them. Researchers have proposed various approaches to accomplish TTC estimation.

A viable approach is to utilize hand-crafted features such as closed contours, optical flow, brightness, or intensity from images . Mobileye  adopted geometric information of the vehicles in image to estimate TTC by establishing the relationship between TTC and the width of vehicle. In addition to geometric-based methods, several studies have been proposed to address the task of TTC estimation using photometric-based features, without relying on geometric features or high-level processing. For instance,  adopted accumulated sums of suitable products of image brightness derivatives from time varying images to determine the TTC value. Furthermore,  elucidates the relationship between TTC and the changes in intensity in images. However, these hand-crafted features need carefully tuned parameters or strong priors, such as constant brightness  or static scene , which restricts their practical applicability.

Besides to hand-crafted methods, deep-learning approaches can also be employed for TTC estimation. One alternative approach is to use scene flow estimation methods  that predict both depth and velocity simultaneously, enabling the generation of pixel-level TTC estimation maps. However, these methods depend on accurate optical flow information, which can result in significant computational overhead. Recently,  proposed Binary TTC that bypasses optical flow computation and directly computes TTC via estimating scale ratio between consecutive images. While these learning-based methods may produce more promising results, they require a larger amount of data with annotated scene flow ground-truth. Due to the expensive labeling cost, scene flow datasets are mostly obtained through synthesis, leading to a domain gap for real-world applications.

In addition to the aforementioned literature, single object tracking (SOT)  can also infer the scale ratio between the template and the tracked object, serving as an alternative approach to estimating TTC. However, these methods often rely on large downsampling rates, which may lead to the bounding box estimation not accurate enough to meet the requirements of TTC estimation. The baseline method which utilized SOT models in our experiment validates the similar effect.

**Datasets.** Contrary to previous TTC estimation studies, our proposed dataset facilitates the expansion of hand-craft features from solely relying on RGB images to utilizing the features extracted by neural networks. Moreover, we propose a method to address the problem of estimating the TTC by classifying the scale ratio. And we extend the implementation on RGB images to feature maps extracted using deep learning models, thereby significantly enhancing the accuracy of TTC estimation.

For TTC estimation, several datasets collected in real scenes are available. For example,  proposed a multi-person tracking dataset with stereo depth details, and  presented a large-scale dataset for TTC estimation in indoor scenes. However, these datasets mainly focus on low speed scenarios and are not suitable for direct application to TTC estimation in driving scenarios. In addition to datasets specifically designed for TTC estimation, some datasets have been synthesized for scene flow tasks and can provide detailed depth information, which are suitable for TTC estimation. For example, KITTI scene flow  proposed an outdoor scene flow dataset containing 400 dynamic scenes collected from KITTI . These scenes are annotated using 3D CAD models for vehicles in motion and manually mask non-rigid moving objects. The Driving  dataset proposed a synthetic stereo video dataset rendering in realistic style. However, these datasets

    & KITTI & NuScenes & Waymo & Ours \\  Scenes & U & U & U & **U+H** \\ Frequency (\(hz\)) & 10 & 2 & 10 & 10 \\ Range (\(m\)) &  & [-80,80] & [-75,75] & **[-160,400]** \\
2D-to-3D & ✔ & ✔ & ✔ & ✔ \\ Avg Speed (\(m/s\)) & - & 5.1 & 6.9 & 19.1 \\ Ann. Frames & 15K & 40K & 200K & **1M** \\ Boxes & 200K & 1.4M & **12M** & 1M \\   

Table 1: Comparison with several autonomous vehicle (AV) datasets. ”2D-to-3D” indicates the presence of tightly bounded 2D box annotations with corresponding 3D bounding boxes, which is crucial for TTC estimation.U and H in scenes indicate Urban and Highway respectively. And we report the average velocity of the recording platform in the training set for comparison.

are constrained by synthetic and limited scenes, which result in domain gaps with real scenes. Except to the scene flow datasets, some RGBD datasets [33; 34; 39], equipped with comprehensive depth annotations, can be utilized to train depth estimation models, which in turn can be used for TTC estimation. However, the majority depth annotations in these datasets are typically confined to a relatively small range (less than 50 meters) and the number of scenes available is limited. In addition to the aforementioned datasets, several large-scale datasets proposed for autonomous driving, such as [5; 7; 26; 38], offering comprehensive annotations like 3D LiDAR bounding boxes that can be used to generate TTC ground-truth. Nevertheless, these datasets were not specifically tailored for TTC estimation and presented issues like unbalanced TTC distribution. Furthermore, these datasets were mainly collected from urban areas, lacking data for highway scenarios where the estimation of TTC is particularly important. Please refer to Table 1 for a comparison of various datasets. Compared to the aforementioned datasets, our proposed dataset holds the distinct advantage of being large-scale and recorded in real scenarios, encompassing both urban and highway scenes.

## 3 Discussion

A common question related to TTC is that is TTC only applicable for low level assistant driving system? Why do we need TTC when we have distance and velocity measurement in advanced assistant or autonomous driving system? We argue that there are two main reasons for the essence of TTC: First, among all the three physical properties of one object (distance, velocity and time to contact), TTC is the only direct observation from monocular image. Monocular depth estimations are mostly from fully data-driven aspect, which highly relies on the recognition of the objects and scenes, which suffers from out of domain issue seriously . For velocity, the situation is similar to that of depth estimation. However, TTC estimation does not require the recognition of semantic of objects (can even be achieved by pixel photometric loss), thus has better generalization in corner cases. Second, for a high level autonomous driving system, redundancy design is indispensable. Even though we have distance and velocity observation, TTC can also be fused into these observations in subsequent perception fusion modules, which serves as another independent safety guarantee.

## 4 TTC Dataset

### Data Collection

The dataset consists of two parts, including a majority of data from real scenes and a minority of data from NeRF virtual rendering. After obtaining raw sensor data, we consider a sequence that contains a number of consecutive frames of a vehicle as a sample. To gather valuable data, we discard truncated objects within the camera plane and filter out 2D boxes smaller than 15 \(\) 15 pixels.

**Real Scenes.** For real-world scenarios, raw data was collected by our commercial trucks. We capture image data using three frontal and two backwards cameras with same \(1024 576\) resolution. We adopt 2D object detection and tracking algorithms on the images to obtain 2D bounding boxes and corresponding track ID for other vehicles. And the LiDAR and Radar data are adopted to generate accurate depth and velocity of them. The sampling rate is 10Hz. Detailed sensor specification can be found in the appendix. After applying these rules to filter the raw data, we observe that the resulting data distribution is highly imbalanced across different TTC ranges. For example, vehicles with small TTC are extremely rare in driving scenes, especially for vehicles lie in the same lanes as ego vehicle. However these are the cases which FCW and AEB should focus on. In such conditions, data collection without rebalancing may result in lack of these valuable scenarios. To overcome this challenge, we pre-define a data distribution and sample the data accordingly. The sampling weights for the TTC intervals, specifically (0,3], (3,6], (6,10], (10,15], and (15,20], is set to 14%. For the TTC range of [-20,0), the sampling weight is designated at 30%.

**NeRF Scenes.** Despite thorough data collection efforts, we discover samples with small TTC values within the same lane are still extremely scarce, which is a crucial scenario in automated driving and ADAS. To supplement the absence of data in particular conditions, we adopt an internal undisclosed project which is developed based on Instant-NGP  to render novel scenes. Briefly speaking, the background models and object models are firstly extracted and trained separately. And then we can form a new scene and render them together. Given a specific object, we pre-define some scripts in which the TTC of the object is distributed between 0 and 6. The preset script can be found in our appendix. After obtaining NeRF rendered images, we organize them with the same format as real scenes data, which serves as an optional component within the training set.

### Annotation

In each sequence, we provide the TTC ground-truth for every frame as the annotation. In the following, we will describe how we generate the TTC annotation. Given a frame, we first run 2D detection on the image and 3D detection on LiDAR. The long range LiDAR detection algorithm which reliably covers [-160, 400] meter range. Then, we could obtain its corresponding 2D detection box by projecting the 3D box to the image plane then picking the 2D detection box which has highest IoU between the projected box. In the vehicle coordinate system, one corner of the 3D bounding box could be denoted as \(x_{j},y_{j},z_{j}\) where \(j\{1,2,...,8\}\) is the corner index. Here, we take the y-coordinate of the corner point which is the closest to ego as the depth of this object:

\[j^{*}=*{arg\,min}_{j\{1,2,...,8\}}(^{i}-0)^{2}+(y_ {j}^{i}-0)^{2}+(z_{j}^{i}-0)^{2}}),y^{i}=y_{j^{*}}^{i},\] (2)

where \(y^{i}\) is the depth of the vehicle in \(i\)-th frame. Here, we assume the relative velocity between the vehicle and ego is constant in a short time interval (e.g. \(q\) frames) to acquire a stable estimation of the velocity. Given the depth of the vehicle in the past \(q\) frames, we fit the depth to obtain the relative velocity \(v^{i}\) by RANSAC  algorithm. We set the value of \(q\) to 10 by default. It is worth noting that the velocity is acquired prior to sequence splitting, thereby \(q\) may be greater than the sequence length. After obtaining the depth and relative velocity of current frame, the TTC ground-truth could be obtained by \(^{i}=y^{i}/v^{i}\).

Since the camera planes are almost parallel to the XZ plane in the vehicle coordinate and the origins of these coordinate systems are highly aligned, we regard the TTC value calculated in the vehicle coordinate system as the TTC ground-truth for camera planes. The annotation process is illustrated in Fig 2. One may challenge that using past \(10\) frames to fit the velocity may result in latency in velocity estimation especially for sudden break. To remedy this issue, we first generate TTC ground-truth with different \(q\) (e.g. 3, 5 and 10) and consider the vehicle with varied TTC values as an accelerating or decelerating object. For object with varied TTC values, we select the one closest to the TTC computed by the velocity of radar sensor as the ground truth. Note that we do not directly utilize radar sensors for all objects since they could not cover too distant objects. **The data annotations are then manually checked by our annotation team to ensure the quality**

### Dataset Statistics

We construct the dataset following the pre-defined rules and annotation pipeline, resulting in 206K sequences comprising over 1M frames from real scenes, as well as 1K sequences from 6.0K NeRF rendered images. We split the sequences from the real scenes based on their recorded date to training, validation and test sets, yielding 149.1K, 28.8K, and 28.6K sequences respectively. For better understanding the data distribution, we plot the histogram of the TTC ground-truth and depth in the training set in Fig. 3. The distribution in validation and test set is similar. Note that the far away samples are rare because we set a minimum 2D bounding box size. More detailed information about the dataset statistics is provided in our appendix.

### Task Definition

As the tracklet of a vehicle is collected in the format of fixed length sequences, we formulate the TTC estimation task in sequence level. For a sequence of a specific vehicle, we provide six consecutive frames and their corresponding 2D bounding boxes as input. The last frame in the sequence is considered as the target frame while the rest of the frames serve as references. With all frames and boxes available in the sequence, the objective is to predict the TTC value of the object in the target frame or equivalently relative scale ratio between the target box and the referenced one.

## 5 Metrics & Method

In this section, we first review the relationship between TTC estimation and scale ratio briefly. Then, we explicate the evaluation metrics for our TTC estimation task. Subsequently, we introduce our approach, which comprises two variants: the pixel MSE approach and its deep learning counterpart.

### Estimate TTC via Scale Ratio

As shown in Sec. 4, TTC could be obtained by depth and its rate of change. However, estimating the depth and relative velocity of an object directly with only a monocular camera is very challenging. To address this issue, researchers proposed to estimate the TTC of frontal-parallel, planar non-deformable objects according to the change of object scales. As described in , we can obtain the image size of a frontal-parallel, non-deformable object of size \(S\) at distance \(y\) as:

\[s=fS/y,\] (3)

where \(f\) is the focal length of the camera. For an object without rotation, TTC can be formulated as a function of object size in image space by combining Eq. (1) and (3):

\[=-t_{0}}{1-)}{s(t_{1})}}=-t_{0}}{1- },\] (4)

where \(s(t_{0})\) and \(s(t_{1})\) are the sizes of image of an object in frame \(t_{0}\) and \(t_{1}\) correspondingly. Thus, the estimation of TTC can be simplified as a scale ratio estimation problem which can be done with only observations in image space. With the development of deep learning, modern object detectors or tackers could produce relatively accurate 2D bounding boxes for vehicles. Given the detection or tracking bounding box in consecutive frames of a vehicle, one intuitive idea is that we can use the ratio of the box or mask area to accomplish the scale ratio estimation. However, are these bounding boxes accurate enough for the TTC estimation task and does there exist more accurate scale ratio estimation algorithms under such conditions? We try to answer these questions via experimental validation on the proposed dataset.

### Evaluation Metrics

Before introducing the detailed design, we need to design the metrics for evaluation. Here, we adopt Motion-in-Depth (MiD) error and Relative TTC Error (RTE) as performance indicators, which could be denoted as:

\[ =||()-()||_{1} 10^{4},\] (5) RTE \[=||}{}||_{1} 100\%,\]

where \(\) and \(\) mean the ground-truth and predicted scale ratios while the \(\) denotes predicted TTC value. The scale ratio ground-truth \(\) is obtained by Eq. (4). MiD is utilized in previous works [45; 1] to describe the TTC error indirectly from the perspective of scale ratio. Due to the instability of TTC at larger values, **we prioritize the MiD as the primary metric**. With the purpose of revealing more information from the evaluation metrics, we partition several TTC intervals, namely crucial(c) / small(s) / large(l) / negative(n)1, which correspond to TTC values of 0\(\)3, 3\(\)6, 6\(\)20, and -20\(\)0, respectively. We mark them on the indices of the RTE and MiD. The threshold for crucial cases is determined by rounding the typical TTC threshold of 2.7 seconds used in FCW systems [36; 48]. This division allows a more detailed analysis of the performance for the TTC estimation algorithms in different TTC intervals, providing a better understanding of their limitations and strengths. During the prediction, TTC values that exceed the predefined range will be truncated to the boundary value.

### Our Design

**Formulation.** We denote the 2D bounding box as \(=[x,y,w,h]\) where \(x,y\) represent the center coordinate and \(w,h\) denote the width and height. Given a reference frame \(_{0}\) and a target frame \(_{1}\), \(_{0}=[x_{0},y_{0},w_{0},h_{0}]\) and \(_{1}=[x_{1},y_{1},w_{1},h_{1}]\) are bounding boxes of a specific object in these two frames. The core idea of our methods is to estimate the relative scale ratio of this vehicle between the reference frame and the target frame. A straightforward approach is simply adopting the scale ratio between \(_{0}\) and \(_{1}\) as the result. However, this strategy is largely influenced by the precision of detection algorithms. In our methods, we estimate the scale ratio change in these two frames in pixel space or feature space, yielding two kinds of implementation: Pixel MSE and Deep Scale. For the reference frame, we enumerate \(n\) different scale ratios to obtain a series of scaled boxes \(=[_{0}^{_{1}},_{0}^{_{2}},..., _{0}^{_{i}},...,_{0}^{_{n}}]\) where \(_{0}^{_{i}}=[x_{0},y_{0},_{i}w_{1},_{i}h_{1}]\). Then, we crop \(_{0}\) via \(\) and resize them to a target size of \(W,H\), which could be denoted as \((_{0},_{0}^{_{i}})\) for the \(i\)-th scale ratio, where \((,)\) denotes the crop \(\) on \(\) and resize it. Similarly, we could get \((_{1},_{1})\) for the target frame. Finally, we use an operator to measure the similarity between \((_{0},_{0}^{_{i}})\) and

Figure 4: Framework of our proposed methods. After aligning the size between contents in \(_{1}\) and \(\), an operator \(\) is applied to measure their similarity. For simplicity, we only illustrate three scale rates of \(\). Some operations such as center shift are omitted for brevity. The green dashed box and the orange dashed box represent Pixel MSE and Deep Scale, respectively.

\((_{1},_{1})\), yielding \(n\) similarity scores. With five frames free to reference, taking different frames as the reference will produce different scale ratios. To address this issue, we convert scale ratio to corresponding TTC value via Eq. (4) and convert it to the scale ratio under the setting of 10Hz when computing MiD. We list the relationship between different scale ratios of same \(\) in the appendix.

Pixel MSE.We can measure the similarity between \((_{0},_{0}^{_{i}})\) and \((_{1},_{1})\) in image space with Mean Squared Error (MSE). The weighted sum of the top \(k\) similar scale ratios is adopted as the final estimation and the weight is normalized by the reciprocal of MSE. The top of Fig. 4 illustrates the pipeline of Pixel MSE.

Deep Scale.For the deep version, we first input two images into a backbone network for feature extraction. Afterwards, the grid sampling operation is used to align the features of different box sizes into one fixed size. Then, we calculate the similarity of each position in the two feature maps via cosine similarity, yielding a similarity map \(_{i}\). Afterwards, the similarity score of scale \(_{i}\) is obtained by adopting a Global Average Pooling (GAP) operation to \(_{i}\). Then we concatenate the similarity scores of different scale ratios and use a Fully-Connected (FC) layer to obtain the final prediction. During training, binary cross-entropy (BCE) loss is used and we adopt the strategy proposed in [24; 47] to convert the scale ratio ground-truth to a size \(n\) vector as soft label. Similar to Pixel MSE, we apply a top \(k\) weighted sum operation to get the final results and the weight is defined as the sigmoid of the FC output. For fast inference, we adopt a convolutional layer followed by a stage of modified CSPNet  used in  as our backbone. After obtaining backbone features, we fed them into one transposed convolutional layer and two convolutional layers before similarity measurement. To capture subtle details, all the stride in the network is set to 1 except the first and the transposed convoultional layer which are set to 2 yielding a feature map with the same size of input. The framework is illustrated at the bottom of Fig. 4.

Center Shift.The boxes predicted by the object detection model may be inaccurate, which will bring misalignment between the centers of reference and target boxes. To address this problem, we introduce a center shift operation. Specifically, we enumerate an offset of \([-c,c]\) along height and width direction, respectively, which yields a total of \((2c+1)(2c+1)\) candidates. After obtaining \((2c+1)(2c+1)\) similarity scores for a single scale, we adopt the highest score as the final score for both Pixel MSE and Deep Scale. Our experiments show that this operation will bring significant improvement in terms of MiD and RTE with little time cost.

## 6 Experimental Validation

### Implementation Details

Pixel MSE.For Pixel MSE, we validate its performance on validation, and test set. The target size \(W,H\) after interpolation is set to the size of \(_{1}\). The scale ratio is set to the range of \([0.65,1.5]\) to cover samples with different scale ratios in the training set. The number of scale bins \(n\) is 125, the top \(k\) for the weighted sum and the \(c\) for center shift are 3. Besides, the detection boxes are manually expanded with a maximum ratio of 1.1 (if the expanded boxes do not exceed the image boundary) to reduce the influence of inaccurate detection results.

Deep Scale.In terms of Deep Scale, we train it for 36 epochs on the train/train\(+\)val set dataset for evaluation on val/test respectively with a batch size of 16 using SGD . The image is resized to \(1024 576\) for both training and test phases. We adopt random color on HSV space as data augmentation during training. The weight decay and SGD momentum parameters are set to 0.0005 and 0.9, respectively. We start from a learning rate of \(10^{-4}\) BatchSize and adopt cosine learning rate schedule. The target size is set to \(50 50\) for grid sample as larger size does not bring more benefits. The input channel for the backbone is set to 12, while the channel for the three followed convolutional layers is set to 24. The kernel size of the convolutional layers is 7 while the transposed one is 3. For the scale range and box expansion, we keep them the same as Pixel MSE. Besides, the number of scale bins \(n\), the top \(k\) for the weighted sum and the \(c\) for center shift are set to 20, 4 and 1 respectively. We test the latency of all models with FP16 and batch size of 1 on a 3090 GPU.

Besides the aforementioned methods, we further propose two baselines termed as Detection and SOT in Table 4. For Detection, the scale ratio is obtained by simply computing the ratio between the area of the target box and the reference box. As for SOT, given a target box, we adopt a state-of-the-art (SOTA) SOT tracker  to obtain a reference box and then estimate the scale ratio as the same as in Detection. Additionally, we include results from an internal monocular depth algorithm and a LiDAR detection + tracking algorithm for a comprehensive evaluation. Details of these algorithms are provided in our appendix. Although there are other methods available, such as , the models released by the authors achieve poor results in our dataset due to the domain gap, and no training codes are provided.

### Main Results

The detailed comparison between various methods is presented in Tab. 4. Due to page constraints, we report only their performance on the MiD metric and overall RTE. As observed, the RTE predicted by box detection or tracking methods is approximately 50%, which is inadequate for practical applications. In contrast, our Pixel MSE method demonstrates significantly lower MiD errors, indicating more accurate scale ratio estimations and consequently, lower RTEs. Among learning-based methods, our Deep Scale significantly outperforms the depth estimation method, although it remains inferior to the LiDAR detection + tracking algorithm. Nevertheless, it achieves the best performance among methods that utilize images. **More detailed comparison, ablations and visualizations could be found in the appendix.**

## 7 Limitations

Our work is a large-scale benchmark for Time-To-Contact estimation, but there are still some limitations that need to be addressed in future works. In regard to our dataset, the collected data primarily focuses on trucks and cars in highway and urban scenarios, lacking more diverse categories that are commonly found in autonomous driving datasets, such as cyclists and pedestrians. Besides, we have to acknowledge that due to the inherent differences in distribution between our dataset and real-world scenarios, there may be potential challenges when directly applying the model trained on our dataset to real-world contexts.

Furthermore, the baseline methods proposed in this study operate under the assumption that objects exhibit frontal-parallel characteristics and are non-deformable. However, it is essential to acknowledge that real-world conditions are considerably more intricate, and these methods may yield suboptimal performance when the underlying assumption is not met. Additionally, as we mentioned earlier, our methods are sensitive to the alignment between the box center of the target and reference frame, which poses another limitation.

## 8 Conclusion

In this work, we built a large-scale TTC dataset and provided a simple yet effective TTC estimation algorithm as baselines for the community. Our dataset is characterized by its focus on objects in driving scenes, which contains both urban and highway scenarios and covers a wider range of depth. We hope that our proposed dataset could facilitate the development of TTC estimation algorithms.

   Methods & MiD & MiD\({}_{c}\) & MiD\({}_{s}\) & MiD\({}_{t}\) & MiD\({}_{n}\) & RTE \\  Detection & 213.9 & 675.4 & 305.1 & 112.4 & 115.3 & 58.1 \\ SOT  & 200.8 & 641.1 & 261.1 & 77.4 & 158.8 & 57.1 \\ Pixel MSE & 41.0 & 57.4 & 36.5 & 32.5 & 48.4 & 29.9 \\ Depth & 62.3 & 111.9 & 74.6 & 36.1 & 68.4 & 47.3 \\ LiDAR & 6.4 & 12.5 & 6.0 & 5.3 & 3.3 & - \\ Deep Scale & 14.4 & 27.1 & 16.4 & 10.9 & 13.5 & 12.1 \\ Deep Scale\({}^{}\) & 14.3 & 26.5 & 15.2 & 10.8 & 13.5 & 12.0 \\   

Table 2: Main results of different methods on the validation set. The \(\) means the result is obtained under padding NeRF data. The \(\%\) after RTE is omitted for brevity.