# Chronicling Germany: An Annotated Historical Newspaper Dataset

Christian Schultze

High-Performance Computing

and Analytics (HPCA-Lab)

Universitat Bonn

&Niklas Kerkfeld

HPCA-Lab, Universitat Bonn

&Kara Kuebart

Institut fur Geschichtswissenschaft

Universitat Bonn

&Princilia Weber

Institut fur Geschichtswissenschaft

Universitat Bonn

&Moritz Wolter

HPCA-Lab, Universitat Bonn

&Felix Selgert

Institut fur Geschichtswissenschaft

Universitat Bonn

equal supervision

###### Abstract

The correct detection of article layout in historical newspaper pages remains challenging but is important for Natural Language Processing (NLP) and machine learning applications in the field of digital history. Digital newspaper portals typically provide Optical Character Recognition (OCR) text, albeit of varying quality. Unfortunately, layout information is often missing, limiting this rich source's scope. Our dataset is designed to address this issue for historic German-language newspapers. The Chronicling Germany dataset contains 581 annotated historical newspaper pages from the time period between 1852 and 1924. Historic domain experts have spent more than 1,500 hours annotating the dataset. The paper presents a processing pipeline and establishes baseline results on in- and out-of-domain test data using this pipeline. Both our dataset and the corresponding baseline code are freely available online. This work creates a starting point for future research in the field of digital history and historic German language newspaper processing. Furthermore, it provides the opportunity to study a low-resource task in computer vision.

## 1 Introduction

Newspapers are essential sources of information, not just for modern readers, but particularly in the past when other communication channels like the internet or radio were not yet available. Even more importantly, newspapers allow researchers to study social groups' opinions and cultural values both now and then. This paper presents the _Chronicling Germany_-dataset, consisting of 581 annotated high-resolution scanned newspaper pages from the period between 1852 and 1924.

With the emergence of digital newspaper portals, using historical newspapers has become easier in recent years2. These portals provide text via OCR but lack reliable layout information, whichis essential for digital history applications, many of which would require newspaper articles to be treated as individual documents. Our dataset will help reduce the character error rate and considerably improve the detection of individual elements of a newspaper page, like articles or single advertisements. The former is important to prevent algorithms from connecting unrelated text regions and preserve the order in which text regions should be read. To this end, the text layout is systematically annotated using nine classes.

From a computer science view, a collection of successful approaches allows us to process modern documents (Blecher et al., 2023; Davis et al., 2022). For historic documents, large-scale data sets exist (Dell et al., 2024) but are mostly focused on English language material set in Antiqua-like typefaces. For continental European languages, existing datasets are much smaller (Abadie et al., 2022; Kodym and Hradis, 2021; Clausner et al., 2015; Nikolaidou et al., 2022).

Until more annotated data becomes available, the processing of historical continental European newspaper pages is, therefore, a low-resource task, highlighting the need for more data. While low-resource tasks are well-established in natural language processing (Adams et al., 2017; Fadaee et al., 2017; Hedderich et al., 2021; Zoph et al., 2016), low resource settings remain under-explored in computer vision (Zhang et al., 2024). Historical German newspapers are interesting in this context due to their dense layout (see also Supplementary Figure 6) as well as the Fraktur font. Fraktur differs significantly from the Antiqua typefaces that dominate modern Western texts. To the modern eye, Fraktur letters appear dense. Furthermore, in addition to the font, our dataset features the archaic 'long s' or 'f', which is no longer used today. The'sz' or 'B' is specific to the German language and also appears in the data. Historically, it emerged when the common combination 'fz' merged into a single letter 'B', unlike the 'long s' it still appears in contemporary texts. The aforementioned differences limit our ability to transfer existing solutions that were designed for modern documents or English-language historical newspapers. This motivates the collection of additional data.

The German newspaper processing task is also highly relevant to scholars of history. Especially in the 19th century, local communities, interest groups, and political parties created their own newspapers. The _Deutsche Zeitungsportal3_ counts 698 German newspapers in 1780, this number rose to over 14,000 in 1860 and peaked at 50,848 papers in 1916 (see Figure 1). Plenty of digitized pages are available, which will allow researchers to systematically search for cultural values and historical change. Unfortunately, untrained modern human readers struggle with font differences, limiting the usefulness of unprocessed data to researchers lacking this specific skill. Thus, creating a pipeline capable of accurately processing this vast amount of data to a format readable to both a machine and a researcher without specific language and typeface skills is an important step in making these resources accessible.

Figure 1: Left: Number of available digitized newspapers per year at www.deutsche-digitale-bibliothek.de/newspaper over time. Data from January 2024. Right: Front page of the _KÃ¶lnische Zeitung_ from the \(1^{st}\) of January 1924.

Additionally, the layout of German historical newspapers is often complex, consisting of several columns, multiple horizontal sections and up to 500 elements to annotate per page. To create this dataset, eleven student assistants with a background in history have spent a total of 1,500 hours annotating the layout of our 581 pages. These include approx. 1,900 individually annotated advertisements, that consist of approx. 5,700 polygon regions. We also provide ground truth text annotations, which are not as costly since we start from network-generated OCR-output and correct errors. Overall, our dataset includes approx. 26,000 layout polygon regions as well as approx. 330,000 text lines.

Our dataset features sections and elements that are especially challenging for OCR and baseline models. For example, advertisement pages mix large and small font sizes and include drop capitals, where the initial letter of an advertisement spans over multiple rows but is read as part of the first row. Both features are a challenge for the baseline detection task. Other challenges are fractions in stock exchange news and abbreviations in lists of casualties.4

In summary, this paper makes the following contributions: (1) We introduce the _Chronicling Germany_-dataset consisting of 581 manually annotated high-resolution pages. (2) We establish a baseline recognition pipeline for the layout detection, text-line recognition, and OCR-tasks. (3) We verify generalization properties using 24 historic newspaper pages from the earlier 1785 - 1866 period. We observe good generalization performance.

The dataset and code for our pipeline are freely available online.5

https://gitlab.uni-bonn.de/digital-history/Chronicling-Germany-Dataset

## 2 Related Work

Very early text recognition systems worked with separately designed systems for line detection, baseline fitting, word detection, and word recognition (Smith, 2007). With the increased adoption of deep learning methodology in the field, neural networks took over many of these tasks until only text line detection and text detection remained as separate tasks (Zhang et al., 2016). The process culminated recently. Kim et al. (2022) propose to train transformer networks directly on images and annotated text without any intermediate steps. Their networks combine a swin transformer (Liu et al., 2021) with Bart decoder (Lewis et al., 2019). During an initial pre-training, their encoder is trained on two million synthetics and eleven million scanned documents. The decoder initially starts from weights pre-trained on multilingual text data. Full system training relies on 800 Latin alphabet receipts, 1,500 Chinese train ticket images, 20 thousand business cards, and 40 thousand Korean receipts. Using a similar system Blecher et al. (2023) trains an OCR engine to recover latex code from scans or PDFs of academic documents. Surprisingly, their network generalizes to old mathematical literature. However, Arxiv papers do not resemble historical newspapers. Therefore, transferring these results to the German historic newspaper domain remains challenging.

### Historical Newspaper Processing

Unfortunately, from a digital history perspective, many modern systems focus on recent data and suffer from poor performance in a historical setting. The current situation has led to a large body of OCR error correction work (Carlson et al., 2023), highlighting the need for specialized data sets and software. Liebl and Burghard (2020), for example, combine existing open-source components for this task.

Related datasets include the Europeana corpus (Clausner et al., 2015). The dataset contains _528_ annotated pages from European sources. More recently Dell et al. (2024), published perhaps the largest historical newspaper dataset to date. Their dataset also includes layout annotations. Our work complements these existing datasets by additionally providing compatible annotations for German historical newspapers that differ significantly from other Western European and American newspapers. Furthermore, we annotate advertisements in detail, which significantly add to the complexity of the OCR-task (Dell et al., 2024). Advertisements are particularly interesting to scholars of economic history who are interested in labor markets, for example.

### Common processing pipeline elements

**Layout Segmentation** is a longstanding task in document processing. For example, dhSegment Oliveira et al. (2018) propose a UNet structure based on the popular ResNet50 architecture (He et al., 2016). As described by Ronneberger et al. (2015), the network features a contracting and an expanding part. The contracting subnetwork uses ResNet50 as an encoder, and an additional expansive subnetwork produces segmentation maps at the resolution of the original input.Transformer-based solutions trained on modern documents are available for similar tasks (Davis et al., 2022). However, Convolutional Neural Networks (CNNs) are cheaper to run (Dell et al., 2024) and require less training data, making them a budget-friendly solution.

**Baseline-detection** or text-line detection, means finding the straight line that connects the base points from each letter. Early work employed quadratic splines for this task (Smith, 2007). Modern solutions often employ architectures devised for segmentation or object detection tasks. Kodym and Hradis (2021) for example choose a U-Net, while Dell et al. (2024) work with YOLOv8.

**Optical Character Recognition (OCR)** is an important tool in digital history. Liebl and Burghard (2020), successfully work with a topological feature extraction step followed by a classifier as described by Smith (2007) for the digitization of the _Berliner Borsen Zeitung_. Following Breuel (2007), Kiessling (2022) uses a Recurrent Neural Network (RNN) based system. Dell et al. (2024) apply the contrastive learning approach presented by Carlson et al. (2023). Using a vision encoder, characters are projected into a metric space. The system works because patches containing the same character will cluster together.

## 3 The Chronicling Germany Dataset

Our Dataset contains pages from the _Kohnische Zeitung_, mostly from 1866, specifically from the period of the austro-prussian war. Of these 416 pages, 15 pages contain only advertisements with approx. 1,900 individual advertisement blocks. We also include 141 pages from 1924, as well as 24 special editions from 1852-1888. Polygons placed by our expert human annotators capture the layout for each page. All annotations are stored in PAGE-XML files. The Polygons capture different text-region types. Subclasses can exist within these. Each region type has a unique XML tag: TextRegion, SeparatorRegion, TableRegion and GraphicRegion. Graphic regions are always assigned the class image. Within text regions, we include the following classes: paragraph, header, heading, caption, inverted_text. Within table regions, the only possible subclass is table. To facilitate

Figure 2: A _Chronicling Germany_ page with its corresponding annotation side by side.

correct reading order detection, we introduce the separator subclass separator_vertical, and separator_horizontal. Vertical separators highlight different columns of a page. Horizontal separators split the page into sections and are relevant for the reading order if they span over multiple columns. Otherwise, they are found at the beginning of a new article or between caption or header elements. The header category covers the newspaper's name, which appears at the top of the front pages. To the left and right of the newspaper name, historical newspapers often have smaller blocks with additional information, such as the name of the editor-in-chief, the publication date, or the subscription price. These polygons are annotated as captions. Polygons that cover paragraphs, headlines, and tables are annotated, respectively. The result of this annotation process is shown in Figure 2. Overall, the dataset includes 26,255 polygon regions.

We primarily use a combination of the classes described above to annotate the historic advertisements. We have decided not to introduce new classes to avoid confounding the model's training. This applies, in particular, to the separator classes. Therefore, we use the classes separator_vertical and separator_horizontal for the annotation of separator regions around individual advertisements. Advertisements tend to use text blocks with bigger fonts. To be consistent with our annotations, we mark these as heading. For the same reason, the normal-sized text is annotated as paragraph. Additionally, we include the classes inverted_text and graphic elements as image. These are present, especially in the advertisement pages, as well as the 1924 pages. Table 1 illustrates this numerically. The two classes inverted_text and image are only present in a subset of the data, which explains its low share of pixels overall.

Regions of each page have a reading order number assigned to them. These numbers are assigned automatically and not corrected manually. Reading order is not the main scope of this dataset. Automatic assignment leads to satisfactory results for most pages. For advertisement pages, however, it does not. Yet, advertisements don't need a meaningful reading order, as they are comprised of elements that are independent of each other.

In addition to the layout data, we include transcribed text divided into text lines. In our dataset, each text line is comprised of a polygon, which contains all characters, as well as a baseline and the corresponding text. Baselines and text transcriptions are initially generated automatically using the pipeline proposed by Kodym and Hradis (2021), and then corrected by expert annotators. Line polygons and baselines are only corrected when there are significant mistakes. This is especially the case within the advertisement pages, where some initial letters of advertisements span over more than one line. Correct drop capital detection is challenging for current text-line detectors. For the transcription, we concentrate on correcting lines with low confidence or containing many special characters. Overall, our dataset includes 330,281 text lines. The text correction process is ongoing. To date, we have corrected 124 pages. All pages will be ready in time for the Conference in December. The transcription follows the OCR-D guidelines, level 2 (Johannes Mangei, 2024). This means the text is transcribed in a visual style, preserving, for example, the archaic 'long s' or 'f'. For a complete discussion see supplemental section A.3.

   label & class & frequency \\ 
0 & background & 39.49\% \\
1 & caption & 0.74\% \\
2 & table & 2.90\% \\
3 & paragraph & 54.03\% \\
4 & heading & 0.94\% \\
5 & header & 0.68\% \\
6 & separator vertical & 0.62\% \\
7 & separator horizontal & 0.58\% \\
8 & image & 0.016\% \\
9 & inverted text & 0.014\% \\   

Table 1: Label distribution-percentages per pixel in the dataset.

## 4 Experiments and results

**Data**: We work with fixed train, test and validation splits. We train on 492 pages with 30 validation pages and finally use 59 for testing.

**Pipeline**: Figure 3 gives an overview of our pipeline. Overall, we employ two U-Nets for Layout recognition and text-line detection and, finally, a Long Short-Term Memory (LSTM) cell for OCR. The pixel-wise layout inference is converted into polygons during the post-processing step. We use targets like Kodym and Hradis (2021) for training the baseline U-Net. The model recognizes baselines, ascender, descender, and endpoints, which are converted into line region and baseline polygons during post-processing. The post-processing code is an adapted version from Kodym and Hradis (2021). Contrary to their approach, we use the layout regions from the previous step to cut out parts of the image and identify all baselines for each region. These baselines are then used as input for the LSTM OCR model and the original image. The pipeline is sensitive to the character resolution. A small letter "a", for example, should be about 20x20 pixels in size. If the resolution deviates significantly (more than five pixel in either dimension), we rescale the input images accordingly.

### Layout-Segmentation

**Training**: Our layout segmentation setup follows Oliveira et al. (2018). For layout training, all pages are scaled down by a factor of 0.5 and split into 512 by 512-pixel crops. Cropping leads to 34,376 training crops overall. During training, we work with 24 crops per batch per graphics card. The training runs on a node with four graphics processing units (GPUs). Consequently, the effective batch size is 96, with 358 training steps per epoch. Initially, optimization of the contracting network part can start from pre-trained ImageNet weights, while optimization of the expanding path has to start from scratch. The expanding subnetwork starts with the encoding from the contracting network and produces a segmentation output at the input resolution. To improve generalization, input crops are augmented using rotation, mirroring, gaussian blurring, and randomly erasing rectangular regions. An AdamW-Optimizer train this network with a learning rate of 0.0001, with a weight decay parameter of 0.001 for 50 Epochs in total, while using early stopping to save the best model. We explore transfer learning via pre-training on the Europeana-dataset (Clausner et al., 2015). In this case we initialize the encoder using ImagNet weights, train on Europeana first and continue training on our data. We compare to a network trained using ImageNet-weights only. In other words, we evaluate the effect of Europeana pre-training by working only with ImageNet pre-training.

Figure 3: Flow chart of the entire prediction pipeline. The layout detection, text-line inference and Optical Character Recognition (OCR)-tasks use separate networks each. The output is machine-readable and can be processed further. For example in a machine translation step.

**Results**: Table 2 lists network performance. We compute Intersection over Union (IoU) values for individual classes. IoU is a widespread metric for segmentation tasks (Szeliski, 2022). Generally, we find good performance of the trained network, although the especially rare classes image and inverted_text are not recognized as well. Figure 4 presents an advertisement page from our test set with ground truth and prediction from the best pre-trained model side by side. Overall, Europeana pre-training improves separator recognition but does not help with images or inverted text, which are not annotated in the Europeana dataset.

### Baseline Detection

**Training**: Following Kodym and Hradis (2021) we train an U-Net for the text-baseline prediction task. The raw input image as well as ground truth baselines serve as starting points for the optimization. The training process minimizes a joint text-line and text-block detection objective as introduced by Kodym and Hradis (2021).

We run an AdamW-optimizer with a learning rate of 0.0001 and a batch size of 16. During training, inputs are randomly cropped to 256 by 256 images. To improve the robustness of the resulting network the input pipeline includes color jitter, gaussian blur, random grayscale and gaussian blur perturbations during training.

    &  \\  class & ImagNet-Init & Europeana-Transfer \\  background & 0.89 \(\) 0.001 & 0.89 \(\) 0.006 \\ caption & 0.71 \(\) 0.055 & 0.75 \(\) 0.024 \\ table & 0.66 \(\) 0.061 & 0.64 \(\) 0.055 \\ paragraph & 0.97 \(\) 0.001 & 0.97 \(\) 0.001 \\ heading & 0.69 \(\) 0.025 & 0.68 \(\) 0.015 \\ header & 0.75 \(\) 0.050 & 0.75 \(\) 0.027 \\ separator vertical & 0.71 \(\) 0.015 & 0.73 \(\) 0.013 \\ separator horizontal & 0.66 \(\) 0.055 & 0.71 \(\) 0.012 \\ image & 0.36 \(\) 0.050 & 0.29 \(\) 0.015 \\ inverted text & 0.24 \(\) 0.068 & 0.15 \(\) 0.054 \\   

Table 2: Layout detection results. This table lists Intersection over Union (IoU) values for all individual classes, as well as overall.

Figure 4: Target labels on the left and segmentation prediction on the right. The top left part of this advertisement page also appears in Figure 3.

**Results**: We measure precision, recall, and F1 score (see Table 3). Generally, we observe values around 0.9. These observations are in line with Kodym and Hradis (2021), who observe similar numbers on the cBAD2019 dataset (Diem et al., 2017).

### Optical Character Recognition (OCR)

**Training** Following Kiessling (2022) we train a LSTM-cell for the OCR-task. We employ baselines to extract individual text lines. Alongside the annotations, which have been checked by our human domain experts, these serve as input and ground truth pairs. Adam (Kingma and Ba, 2015) optimizes the network with a learning rate of 0.001. Optimization runs for a total of eight epochs with a batch size of 32 sequences. We used early stopping to prevent the model from overfitting. We include pixel-dropout, blur, rotation and see-through-like augmentations during training to improve generalization.

**Results**: We compare our results to a comparable pipeline developed by the Universitatsbibliothek Mannheim (Jan Kamlah, 2024) and observe improved results (see: Table 4), for an optimization starting from a randomly initialized RNN-cell. We also explore fine-tuning the model from Jan Kamlah (2024), which marginally improves the ratio of completely correct lines.

### Overall pipeline performance

So far, we have evaluated components individually using ground truth inputs from previous steps. We additionally evaluate the complete pipeline on the test set. For each component, we choose the model with the best results and use the result of each component for the next one. Then, we evaluate the resulting transcription with our ground truth. All predicted and ground truth lines are matched based on the intersection over the minimum of the corresponding text lines. Lines without a match were paired with an empty string. Our pipeline achieves an overall Levenshtein distance per character of \(0.0204\) across the entire test set. Overall 97.96% of all output characters are correct.

## 5 Pipeline-Generalization

In this section, we report tests for the generalization properties of our pipeline.

**Test-Data**: In order to verify generalization, we work with annotated newspaper pages from different papers and a different period. Overall, 24 high-resolution pages from four newspapers from the period between 1785 and 1866 are annotated with manually corrected text. This out-of-domain test set contains approximately 250 regions and 2,400 test lines. Three of the four newspapers are set in Fraktur, and one is in Antiqua.

   Model & Levenshtein-Distance & completely correct [\%] & many errors [\%] \\  UB Mannheim (2024) & 0.020 & 47.1 & 6.3 \\ transfer (ours) & 0.017 \(\) 0.0009 & 69.652 \(\) 0.288 & 5.075 \(\) 0.216 \\ random (ours) & 0.016 \(\) 0.0013 & 69.140 \(\) 0.352 & 5.146 \(\) 0.318 \\   

Table 4: Optical Character Recognition (OCR) results. Levenshtein distance per character appears in the first column. We computed the percentage of completely error-free lines for each model. The second column lists these results. Finally, we consider a line to have many errors if we observe a Levenshtein distance of more than 0.1 per character. We report the percentage of many error lines in the final column.

   Model & precision & recall & F1 score \\  UNet & 0.910 \(\) 0.008 & 0.884 \(\) 0.008 & 0.896 \(\) 0.007 \\   

Table 3: Baseline detection results. We measure performance in precision, recall and F1 score. Detected lines are matched with ground truth lines and are considered a true positive if the predicted line has an IoU score of more than 0.7 when compared with the corresponding ground truth line. Results are averaged over all test pages.

**Inference**: We run the entire pipeline on this new generalization test set. Overall, we measure a Levenshtein distance per character of 0.0466, so 95.34% of characters are correct. Figure 5 presents an example taken from a 1785 issue of the Schwabischen Merkur. The sample is a report from Portugal, which we deem entertaining from a modern perspective. Readers learn that hot-air balloons or "aeroftatifche Mafchinen" where banned "last year" because hot-air balloons are "incompatible with the omnipotence of god". Linguistically, the sample is close enough to modern German to be machine-translated.

## 6 Limitations and social impact

This dataset contains newspaper pages set in fraktur-letters. The font is very different from modern fonts. The 'long s' or 'f', for example, is completely foreign to modern eyes. While our generalization dataset also includes four pages in Antiqua font which have been predicted with sufficient accuracy, networks trained exclusively on our dataset are not likely to outperform more specialized networks on modern newspaper pages.

Ideally, our work will enable the processing of millions of pages of historical data, making vast resources easily available to future researchers who can then build upon the transcribed source material, for example, with machine translation and NLP pipelines. Countless research questions concerning economic, societal, political and scientific development can be addressed with such data. For a more detailed description of the relevance of such data for historical research, see Supplementary Section A.2. We hope this dataset will help to improve our understanding of the past. We therefore expect a positive impact on society as a whole.

## 7 Conclusion and future work

This work introduces the _Chronicling Germany_-dataset, a neural network-based processing baseline with test-set OCR-accuracy results. Our paper creates a starting point for researchers who wish to improve historical newspaper processing pipelines or are looking for a low-resource computer vision challenge. To create the dataset, history students spent 1,500 hours annotating the layout of our 581 pages. The dataset includes 1,900 individually annotated advertisements. Furthermore, we introduce an out-of-distribution test set of 24 pages. We verify baseline pipeline performance on these out-of-distribution pages. By following the OCR-D annotation guidelines (Johannes Mangei, 2024) we ensure our annotations' compatibility with concurrent and future work.

Figure 5: Generalization test set sample image. This figure shows a page element with detected baselines on the left. The right side presents the automatically created transcription.

#### Acknowledgments

We thank the University of Bonn's transdisciplinary-research-areas TRA1 (Mathematics, Modelling, and Simulation of Complex Systems) and TRA4 (Individuals, Institutions, and Societies) for funding the data annotation. Furthermore, research was supported by the Bundesministerium fur Bildung und Forschung (BMBF) via its "BNTrAInee" (16DHBK1022) and "WestAI" (01IS22094E) projects. The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS at Julich Supercomputing Centre (JSC).