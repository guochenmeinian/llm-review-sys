# Aligning LLM Agents by Learning Latent Preference from User Edits

Ge Gao\({}^{}\) Alexey Taymanov\({}^{}\) Eduardo Salinas\({}^{}\) Paul Mineiro\({}^{}\) Dipendra Misra\({}^{}\)

Department of Computer Science, Cornell University Microsoft Research New York ggao@cs.cornell.edu {ataymano, edus, pmineiro, dimisra}@microsoft.com

Equal contribution.

###### Abstract

We study interactive learning of LLM-based language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their _latent_ preference, in addition to improving the correctness. The edit feedback is _naturally generated_, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, **PRELUDE** that infers a description of the user's latent preference based on historic edit data. The inferred user preference descriptions are used to define prompts for generating responses in the future. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named **CIPHER** that leverages the LLM to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the \(k\)-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments - summarization and email writing, and use a GPT-4 simulated user for evaluation. On both tasks, CIPHER outperforms several baselines by achieving the lowest edit distance cost while only having a small overhead in LLM query cost over the base agent. Our analysis reports that user preferences learned by CIPHER show significant similarity to the ground truth latent preferences.1

## 1 Introduction

Language agents based on large language models (LLMs) have been developed for a variety of applications (Dohmke, 2022; Brynjolfsson et al., 2023), following recent breakthroughs in improving LLMs (Achiam et al., 2023; Ouyang et al., 2022b; Team et al., 2023). However, despite their impressive zero-shot performance, LLMs still need to align to a given user and task (Mysore et al., 2023; Li et al., 2023). In many applications, a natural feedback for LLM-based agents is user edits, where a user queries the agent and edits the agent's response before their own final use. In contrast, typical feedback used for fine-tuning, such as the comparison-based preference feedback in RLHF, is explicitly collected by providing annotators with model responses and asking them to rank (Ziegler et al., 2019; Stiennon et al., 2020; Nakano et al., 2021; Ouyang et al., 2022a, inter alia), making such feedback an expensive choice for improving alignment. Motivated by this observation, we focus on interactive learning of LLM-based language agents using user edits as feedback.

Consider the scenario in Figure 1 where a user interacts with an LLM-based writing assistant (agent) to complete their task. The interaction starts with the user (and the world) providing a context to the agent. This context may include a query prompt provided by the user, along with additional information provided by the world, such as the content on the screen, current time, and the user's calendar information. The agent generates a textual response given the context.

In the beginning, the agent's response may not be optimal for the user, as it is not personalized to this user's individual needs and preference. As most users are not familiar with prompt engineering, and LLMs are often able to generate a reasonable response for the task, therefore, users may find it the most convenient to simply edit this response when it is not ideal, rather than trying different prompts to get new responses. The example in Figure 1 illustrates that the user directly edits the summary generated by the agent to satisfy their preference for bullet point format. It takes time and effort for the user to make edits which can be measured using metrics such as the edit distance between the agent's response and the user edits. Our goal is to minimize the cumulative user edit cost over time using feedback from user edits. Notably, there is no distinction between training and testing in our setting as _every natural use of the agent yields an edit feedback for learning_.

We conjecture that user edits are driven by user's hidden preference which can be described in natural language. These _preference descriptions_ are different from the notion of comparison-based preference used in RLHF. In this paper, we use the word _preference_ to mean _preference descriptions_. For instance, preference of the user in Figure 1 can be described as _bullet points_. In practice, user preference can be compound, such as preferring _bullet point, informal, with emojis_ at the same time, and also context-dependent, e.g., _informal_ tone when writing an email to a family member, and _formal_ tone when writing to a colleague. In more complex settings, user preference can evolve with time (non-stationary), or depend on information unavailable in the context (partially observed). Further, users may not be fully aware of all their preferences, or may fail to express these preferences in their query prompt. These considerations imply that user preference is _latent_ to the language agent. If the agent could learn the _latent_ preference correctly, it can significantly improve its performance by generating satisfactory responses. Furthermore, preference learned by the agent can be shown to the user to enhance _interpretability_, and can even be modified by the user to improve correctness. Motivated by this, we propose a learning framework, **PRELUDE** (**PRE**ference **L**earning from **U**ser's **D**irect **E**dits), where we seek to learn a user preference description for a given context using the history of user edits.

In a typical real-world scenario such as writing assistants, one has to potentially update the LLM-based agent for every user. Efficient approaches, therefore, must scale with the number of users. This makes approaches that fine-tune LLM parameters expensive to scale. Furthermore, LLMs typically undergo rigorous evaluation on a variety of safety tests before being released, and fine-tuning them can result in loosing the safety guarantees offered by these tests. For example, fine-tuning GPT-4 for millions of users can quickly turn very expensive. Approaches such as adding LORA and Adapter layers and only updating them, or using federated learning, can reduce the expense to some extent, but the loss of safety guarantees remains a concern. In this work, we focus

Figure 1: Illustration of interactive learning from user edits. Color coding in edits is for visualization only â€“ our agent takes the plain revised text as feedback.

on leveraging a frozen, black-box LLM, and instead learning a _prompt policy_ that can infer user preference description for a given context, and then use it to directly drive the response generation.

We introduce a simple yet effective algorithm **CIPHER** that implements the PRELUDE framework. CIPHER infers user preference for every context in the history with the aid of an LLM. In the future, given a context, it retrieves inferred preferences of similar contexts from the history and uses them to generate a response. CIPHER is computationally efficient and only slightly increases the LLM query cost compared to the base agent.

We introduce two interactive environments that evaluate the agent's ability to summarize documents and compose emails from a given notes. These tasks are inspired by writing assistant applications.For both tasks, we simulate a GPT-4 user that can generate edits based on a pre-designed _latent_ preference that can vary based on the context. We evaluate CIPHER against several baselines and show that it achieves the lowest user edit cost. Additionally, CIPHER results in a lower LLM query cost than other retrieval-based baselines. Finally, we analyze preferences learned by our agents, and find that they show significant similarity to the ground truth latent preferences in our setup.

## 2 Interactive Learning from User Edits and the PRELUDE Framework

We first describe LLM agents and the general learning framework from user edits and then discuss our PRELUDE framework and associated learning challenges.

**LLM and Language Agents.** We assume access to a language agent that internally relies on an LLM. We make no assumption on the agent except that it can take as input a piece of context which can include both texts and images and an additional prompt (which can be in-context learning examples or learned preferences) and generates a text response. The language agent may simply perform greedy decoding of the LLM given the input or may perform complex planning to generate a response.

```
1:for\(t=1,2,,T\)do
2: User and the world provide a context \(x_{t}\)
3: Agent generates a response \(y_{t}\) given the context \(x_{t}\)
4: User edits the response to \(y^{}_{t}\)
5: Agent receives a cost of \(c_{t}=_{}(y_{t},y^{}_{t})\)
6: Evaluate the agent and learning algorithm on \(_{t=1}^{T}c_{t}\) ```

**Protocol 1 Interactive Learning from User Edits.** In an application such as a writing assistant, a user interacts with the language agent over \(T\) rounds. Protocol 1 shows such learning protocol. In the \(t^{th}\) round, the user and the world provide a context \(x_{t}\) where \(\) is the space of all possible contexts. This context will include the user prompt in text, along with additional information provided by the user or the world, and may include multimodal data as well such as images. Given the context \(x_{t}\), the language agent generates a response \(y_{t}\) in text, where \(\) is the space of all texts. The user edits the response \(y_{t}\) to \(y^{}_{t}\). If the user does not perform any edits, we treat this as setting \(y^{}_{t}=y_{t}\). The agent receives a cost of \(c_{t}=_{}(y_{t},y^{}_{t})\) for this round, which measures the user's efforts on making edits. The goal of the agent is to minimize the sum of costs across all rounds \(_{t=1}^{T}c_{t}\). In our experiments, we use \(_{}\) as Levenshtein edit distance (Levenshtein, 1965) in the token space which computes the minimum number of token insertion, deletion, and substitution necessary to convert \(y_{t}\) to \(y^{}_{t}\). In general, a higher edit distance implies that the user has made more edits and spent more efforts.

**PRELUDE Framework.** We describe our PRELUDE framework in Protocol 2 which is a specialization of Protocol 1. In PRELUDE, in the \(t^{th}\) round, the agent infers the preference of the user as \(f_{t}\), and uses it to generate a response. We assume that in this round and for the given context \(x_{t}\), the user has a _latent_ preference \(f^{}_{t}\) that drives the user to perform all edits. Furthermore, we assume that if the agent was able to infer this _latent_ preference (\(f_{t}=f^{}_{t}\)), then it will lead to minimal possible edits.2 To remove the dependence on performance due to the choice of the base LLM agent, we compare with an oracle agent that has access to \(f^{}_{t}\) at the start of each round. We assume that the LLM remains frozen across all methods in this work.

**Protocol 2****PRELUDE**: **PREFerence Learning from User's Direct Edits**

```
1:for\(t=1,2,,T\)do
2: User presents a text context \(x_{t}\)
3: Agent infers a preference \(f_{t}\) using the history \(\{(x_{},y_{},y^{}_{})\}_{=1}^{t-1}\) and context \(x_{t}\)
4: Agent uses \(f_{t}\) and \(x_{t}\) to generate a response \(y_{t}\)
5: User edits the response to \(y^{}_{t}\) using their latent preference \(f^{}_{t}\)
6: Agent incurs a cost \(c_{t}=(y_{t},y^{}_{t})\)
7: Return \(_{t=1}^{T}c_{t}\) ```

**Algorithm 1** CIPHER(\(,k,\)). A context representation function \(:^{d}\), the retrieval hyperparameter \(k\), and tolerance hyperparameter \( 0\). We initialize history \(=\).

## 3 Learning User Preference through Retrieval and Aggregation

In this section, we present our method, CIPHER (**C**onsolidates **I**nduced **P**references based on **H**istorical **E**dits with **R**etrieval), that learns user preference based on user edits.

```
1:for\(t=1,2,,T\)do
2: User (and the world) presents a context \(x_{t}\)
3: Retrieve the top-\(k\) examples \(\{(x_{z_{i}}),_{z_{i}}\}_{i=1}^{k}\) in \(\) with maximum cosine similarity to \((x_{t})\)
4: If \(k>1\), then query the LLM to aggregate these preferences \(\{_{z_{i}}\}_{i=1}^{k}\) into \(f_{t}\), else \(f_{t}=_{z_{1}}\)
5: Agent generates a text response \(y_{t}\) based on \(x_{t}\) and \(f_{t}\)
6: User edits the response to \(y^{}_{t}\) using their latent preference \(f^{}_{t}\)
7: Agent incurs a cost \(c_{t}=_{}(y_{t},y^{}_{t})\)
8:if\(c_{t}\)then
9:\(_{t}=f_{t}\)
10:else
11: Query the LLM to generate a preference \(_{t}\) that best explains user edits in \((y_{t},y^{}_{t})\)
12:\(\{((x_{t}),_{t})\}\)
13: Return \(_{t=1}^{T}c_{t}\) ```

**Protocol 2****PRELUDE**: **PREFerence Learning from User's Direct Edits**

Algorithm 1 shows CIPHER which implements the PRELUDE framework. CIPHER maintains a preference history \(_{t}=\{(x_{},_{})\}_{=1}^{t-1}\) of past contexts \(x_{}\) along with a preference \(_{}\) inferred by the agent. CIPHER assumes access to a _context representation function_\(:^{d}\) that can map a context to a vector representation. For a given round \(t\) with context \(x_{t}\), the agent first retrieves the \(k\)-closest contexts from the interaction history \(_{t}\). We use cosine similarity for computing proximity, although other metrics such as Euclidean distance, or Hamming distance when \(\) outputs a binary vector, can be used. Given the retrieved contexts and their inferred preferences \(\{(x_{z_{i}},_{z_{i}})\}_{i=1}^{k}\), we query the underlying LLM to summarize the inferred preferences \(\{_{z_{i}}\}_{i=1}^{k}\) into a single preference \(_{t}\). In the beginning, when \(t k\), we retrieve all the past \(t\) contexts. In particular, for \(t=1\) we have \(f_{1}\) as an empty string as the agent has no prior knowledge of this user's preference.3

The agent uses the inferred preference \(f_{t}\) to generate the response. This is done by concatenating the context \(x_{t}\) with an agent prompt such as "_This user has a preference of <\(f_{t}\)> which must be used when generating the response_", where <\(f_{t}\)> indicates where we insert the inferred preference \(f_{t}\). We list the actual template used in our experiments in Table 7 in Appendix B.

Given the user edits \(y^{}_{t}\), if the user edits are minimal, i.e., \(_{}(y_{t},y^{}_{t})\) for a hyperparameter \(\), then we set the inferred preference for this round as \(_{t}=f_{t}\) as using \(f_{t}\) for generating a responseresulted in minimal edits. However, if \(_{}(y_{t},y^{}_{t})>\), then we query the LLM a third time to generate the inferred preference \(_{t}\) that explains why the user edited \(y_{t}\) to \(y^{}_{t}\). We call this the _Latent Preference Induction_ (LPI) step. In both cases, we append \((x_{t},f_{t})\) to the preference history.

Note that we cannot query the LLM for the inferred preference in the first case where the user edit cost \(c_{t}\) is small, i.e., \(c_{t}\). In this case, querying the LLM to infer the preference to explain the edits in \(y^{}_{t}\) given \(y_{t}\), will result in the LLM outputting that the agent has no preference which is incorrect.

**Computational Cost of CIPHER.** In a given round, CIPHER adds a maximum of 3 LLM calls on top of the cost of calling the underlying inference algorithm of the agent in line 5. CIPHER further reduces the memory storage by only storing the representation of contexts in the preference string instead of the input itself. Finally, CIPHER only adds a small prompt to the context \(x_{t}\), before calling the agent's inference algorithm. This only slightly increases the length of the prompt, thereby, reducing the query cost associated with LLMs that scales with the number of input tokens.

## 4 Experiment

We first introduce two interactive tasks for learning from user edits, and then describe our results.

### Two Interactive Writing Assistant Environments for Learning from User Edits

**Task.** We introduce two tasks inspired by the use of LLMs as writing assistants (Mysore et al., 2023; Shen et al., 2023; Wang et al., 2023). In the first task, we evaluate the agent's ability to summarize a given document. In the second task, we evaluate the agent's ability to compose an email given notes. For both tasks, we use documents from several existing sources listed in Table 1. These sources represent a diverse category of documents that a writing assistant would typically encounter (see Table 4 in Appendix for examples). In any given round, the user is provided a context that is a document from one of the sources for the given task. Importantly, the agent is _unaware of the source of the given document_ which as we discuss later, will determine the user preference. For both tasks, we run an experiment for \(T=200\) rounds. We sample an equal number of documents from each source and mix them to remove any temporal correlation in document sources.

**Two-Stage GPT-4 Simulated User.** We simulate a user that can edit a given response. We define a set of _latent user preferences_ for the user that vary based on the document source. Table 1 lists the preference for every source. This captures the context-dependent nature of user preferences as the document source influences the type of context. For example, the _Personal problem_ document

 p{142.3pt} p{142.3pt}}  
**Doc Source** & **Latent User Preference** & **Scenario** \\ 
**Summarization** & & \\ News article (See et al., 2017) & targeted to young children, storytelling, short sentences, playful language, interactive, positive second person narrative, brief, show emotions, invoke personal reflection, immersive bullet points, parallel structure, brief & introduce a political news to kids for character development in creative writing take notes for key knowledge \\ (Foundation, 2022) & tweet style, simple English, inquisitive, skillful foreshadowing, with emojis question answering style, direct, concise & promote a paper to invoke more attention and interests quickly get main opinions \\ (Maas et al., 2011) & & \\ 
**Email Writing** & & \\ Personal problem (Stiennon et al., 2020) & informal, conversational, short, no closing paper review & share life with friends peer review to colleague \\ Hua et al. (2019) & & \\ Paper tweet (Bar, 2022) & engaging, personalized, professional tone, thankful closing structured, straight to the points, respectful, professional greeting and closing & networking emails for researchers milestone report to superiors \\ (Kershaw \& Koeling, 2020) & & \\   

Table 1: Latent user preference design, specific to the document source.

source contains documents pertaining to discussions with a friend, and a user may have a different preference when writing an email to a friend compared to writing an email to a colleague. We assume that our user is aware of the document source \(d_{t}\) of a given context \(x_{t}\). This implies, that we can express the true user preference for \(x_{t}\) as \(f_{t}^{}=F(d_{t})\) where \(F\) maps a given document source to the user preference. Recall that the _agent is never provided the document source of any context_.

We model our user using GPT-4 with a two-stage approach. Given an agent response \(y_{t}\) and the context \(x_{t}\), we first query GPT-4 to check if \(y_{t}\) satisfies the preference in \(f_{t}^{}\). If the answer is yes, then the user preforms no edits and returns \(y_{t}^{}=y_{t}\). If the answer is no, then we use GPT-4 to generate the edited response \(y_{t}^{}\) given \(y_{t}\) and \(f_{t}^{}\). We found that our two-stage GPT-4 user can generate high-quality edits, consistent with observations in prior work that LLM-written feedback is high-quality and useful to learn from (Bai et al., 2022; Saunders et al., 2022). We adopted a two-stage process since using GPT-4 to directly edit the response \(y_{t}\) always resulted in edits even when the response satisfied the preference \(f_{t}^{}\). We provide GPT-4 user prompt template and user edit examples in Appendix B.

**Evaluation Metric.** We propose three metrics for evaluating agents learning from user edits. Our main metric is the cumulative user edit cost \(_{t=1}^{T}_{}(y_{t},y_{t}^{})\) over \(T\) rounds where \(_{}(y_{t},y_{t}^{})\) is the Levenshtein edit distance between agent response \(y_{t}\) and user edits \(y_{t}^{}\) computed in the token space using Tiktoken tokenizer. For methods that learn an interpretable preference, we additionally evaluate the quality of the inferred user preference \(f_{t}\). We do so by evaluating if \(f_{t}\) is closer to the true preference \(f_{t}^{}=F(d_{t})\), where \(d_{t}\) is the document source of context in round \(t\), compared to preference of any other document source. Formally, we compute \(_{t=1}^{T}\{d_{t}=_{d}(f_{t},F(d))\}\), where BERTScore (Zhang* et al., 2020) is a text similarity metric and \(\) is the set of all document sources. Finally, we report the total number of input and output BPE tokens to the LLM across all rounds. This measures the expense associated with using LLM, used by popular LLM providers to charge their customers.

### Details of CIPHER and Comparison Systems

We use GPT-4 as our base LLM for CIPHER and all baselines. We do not perform fine-tuning of the GPT-4 and do not add any additional parameters to the model. We use a prompt-based GPT-4 agent for all methods that uses a single prompt with greedy decoding to generate the response. Our main method CIPHER and the baselines, can be extended to more complex language agents that perform multiple steps of reasoning on top of the base LLM before generating a response.

**CIPHER Details.** We use a simple agent that uses GPT-4 with a prompt template to generate the response \(y_{t}\) given the context \(x_{t}\) and preference \(f_{t}\). We list templates in Table 7 in Appendix B. We experiment with MPNET (Song et al., 2020) and BERT (Devlin et al., 2019) as our two context representation functions \(\), and use cosine similarity for retrieval. We experiment with two different values of the number of retrieved examples \(k\{1,5\}\).

**Baselines.** We evaluate CIPHER against baselines that either perform no learning, or learn context-agnostic preferences, or directly use past edits to generate a response:

1. _No learning:_ The agent performs no learning based on interaction with the user.
2. _Explore-then-exploit (E-then-e) LPI:_ This baseline is based on the classic explore-then-exploit strategy in interactive learning (Garivier et al., 2016). The agent first generates responses for the first \(T_{e}\) rounds without performing any learning (exploration stage). It then infers a single user preference \(_{e}\) using the user edits in the first \(T_{e}\) rounds by applying the LPI step (Algorithm 1, line 11), which is used to generate responses for remaining rounds (exploitation step).
3. _Continual LPI:_ This baseline is similar to _E-then-e LPI_ except that it never stops exploring and avoids overfitting to the first \(T_{e}\) rounds. In any given round \(t\), it uses the data of all past edits \(\{(y_{t},y_{t}^{})\}_{t=1}^{t-1}\) to learn a preference \(f_{t}\) by performing the LPI step. It then generates a response using this preference. Similar to _E-then-e LPI_, this approach learn context-agnostic preferences.
4. _ICL-edit:_ This is a standard retrieval-based in-context learning (ICL) baseline (Brown et al., 2020). In a given round \(t\), the agent first retrieves the closest \(k\) examples \(\{(y_{z_{t}},y_{z_{t}}^{})\}_{t=1}^{k}\) to the given context \(x_{t}\) using the representation function \(\). These examples are provided in an ICL prompt and use to generate the response \(y_{t}\). This approach does not learn preferences but unlike E-then-e LPI and Continual LPI it can perform context-dependent learning.

5. _CoT-edit:_ This is a standard retrieval-based chain-of-thought (CoT) baseline (Wei et al., 2022). This baseline is similar to _ICL-edit_ except the prompt for generation requires the agent to infer a user preference \(f_{t}\) based on retrieved \(k\) examples, and generate an output according to \(f_{t}\).4 
**Oracle Method.** We also evaluate an oracle approach which uses the true user preference in each round to generate the response. This provides an upper bound on performance and helps to evaluate if our setup is well-designed, i.e., whether learning the true user preference indeed leads to low edit costs.

### Main Result and Discussion.

**Main Results.** Table 2 reports the performance of all methods on the two tasks on three metrics. We report the mean and standard deviation across 3 different random seeds.5

**Discussion of Main Result.** We observe that not performing learning results in a high edit cost, whereas using the oracle preferences achieves a significantly smaller edit cost. This shows that our environments are sound and well-conditioned. _E-then-e LPI_ and _Continual LPI_ learn context-agnostic preferences which cannot capture the context-dependent preferences in the environments and end up doing poorly. For the summarization task, they end up with a higher edit distance than even performing no learning. One possible explanation is that using context-agnostic preferences can push the model to specialize to a given preference much more than the base model, resulting in more edits when that preference is incorrect. We see this in preference accuracy, which is low for both of these baselines, and lower for the summarization task than the email writing task where they outperform no learning baselines. Further, _Continual LPI_ has a higher expense cost due to constantly querying the LLM to infer the user preference.

_ICL-edit_ baselines perform significantly better on the summarization task. However, using a list of user edits in the prompt results in a higher token expense cost, as the responses and their edits can be significantly long in practice. Further, the ICL-edit baselines provide no interpretable explanation for their response or for explaining user behavior. Although _CoT-edit_ baselines provide an interpretable preference, they still result in relatively high expense and low classification accuracy.

CIPHER achieves the smallest edit distance cost reducing edits by 31% in the summarization task and 73% in the email writing task. We observe that retrieving \(k=5\) preferences and aggregating them achieves lower edit distance, however, the choice of ideal representation \(\) seems task-dependent.

  
**Method** &  &  \\  & Edit Distance\(\) & Accuracy\(\) & Expense\(\) & Edit Distance\(\) & Accuracy\(\) & Expense\(\) \\  Oracle Preference & 6,573\({}_{1,451}\) & 1.000 & 1.67 & 1,851\({}_{243}\) & 1.000 & 1.62 \\  No Learning & 48,269\({}_{957}\) & - & 1.50 & 31,103\({}_{900}\) & - & 1.65 \\ E-then-e LPI & 65,218\({}_{17,466}\) & 0.218\({}_{0.003}\) & 1.99 & 24,562\({}_{1,022}\) & 0.263\({}_{0.003}\) & 1.73 \\ Continual LPI & 57,915\({}_{2,210}\) & 0.233\({}_{0.010}\) & 8.89 & 26,852\({}_{1,464}\) & 0.243\({}_{0.019}\) & 8.63 \\  ICL-edit-5-MPNET & 38,560\({}_{1,044}\) & - & 8.00 & 32,405\({}_{1,307}\) & - & 12.12 \\ ICL-edit-5-BERT & 39,734\({}_{1,929}\) & - & 7.96 & 30,949\({}_{3,250}\) & - & 11.55 \\ CoT-edit-5-MPNET & 40,747\({}_{1,874}\) & 0.230\({}_{0.026}\) & 6.82 & 24,292\({}_{3,803}\) & 0.300\({}_{0.023}\) & 8.74 \\ CoT-edit-5-BERT & 41,088\({}_{1,846}\) & 0.230\({}_{0.013}\) & 6.92 & 24,301\({}_{1,382}\) & 0.263\({}_{0.032}\) & 8.26 \\  CIPHER-1-MPNET & 33,926\({}_{4,000}\) & 0.520\({}_{0.022}\) & 2.74 & 10,781\({}_{1,711}\) & 0.435\({}_{0.084}\) & 1.94 \\ CIPHER-5-MPNET & **32,974\({}_{195}\)** & 0.478\({}_{0.010}\) & 3.00 & 10,058\({}_{1,709}\) & 0.467\({}_{0.081}\) & 2.09 \\ CIPHER-1-BERT & 37,637\({}_{3,025}\) & **0.565\({}_{0.053}\)** & 2.81 & 12,634\({}_{4,868}\) & **0.487\({}_{0.125}\)** & 1.99 \\ CIPHER-5-BERT & 35,811\({}_{3,384}\) & 0.478\({}_{0.028}\) & 3.03 & **8,391\({}_{3,038}\)** & 0.363\({}_{0.075}\) & 2.22 \\   

Table 2: Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy. \(_{}\) denotes the mean \(\) and standard deviation \(\) across 3 runs over different seeds. Expense column shows budget as the average number of input and output BPE tokens across 3 runs (unit is \( 10^{5}\)). We use -\(k\) in method names to denote that we use \(k\) retrieved examples. Numbers in bold are the best performance in each column excluding _oracle preference_ method, underline for the second best, and dotted underline for the third best.

Further, CIPHER achieves the highest preference accuracy showing that CIPHER can learn preferences that correlate more with the ground truth preference than preferences of other document sources. Note that the performance of a random preference classifier is only 20% for summarization and 25% for email writing. Further, CIPHER achieves a smaller cost than _ICL-edit_ and _Continual LPI_ baselines, as it doesn't use long user edits in the prompt for generating a response. In summary, CIPHER provides a cheap, more effective, and interpretable method than our baselines.

**Learning Curves.** We plot mean cumulative user edit costs over rounds in Figure 2. The cumulative user edit costs in Figure 2 show that the angle of the learning curves decreases for CIPHER after an initial number of rounds, showing that learning helps decrease the rate at which user edits are accumulated. In contrast, the angle of the learning curve for the no-learning baseline remains unchanged.

**Evaluating Fraction of Non-Edited Responses.** Recall that the first stage of our GPT-4 user checks if the agent response satisfies the latent user preference \(f^{*}\). If it does, then no edits are performed, otherwise, the user edits the response. We plot the percentage of examples with zero edit cost per 20 rounds bin in Figure 3. We notice a small increase in the number of examples with zero edit cost. This indicates that gains come not just by increasing the number of examples that avoid getting edited in stage 1 of our user but more generally across examples.

**Qualitative Analysis of Learned Preferences.** We evaluate the quality of preferences learned by CIPHER on the harder summarization task. Table 3 lists 3 learned preferences per document source for _CIPHER-5-MPNET_ which are randomly sampled at the beginning, middle, and end of the interaction history. We see that overall the agent can learn a reasonable description of the latent preference. For example, it can learn _bullet points_ preference for Wikipedia articles, and _second person narrative_ for Reddit posts, and _QA style_ for Movie reviews. CIPHER can pick some preferences fairly early such as _bullet points_ for Wikipedia and _emojis_ for Paper abstract, whereas some are learned only later such as _Structured Q&A_ for Movie reviews. This shows using CIPHER can quickly learn useful preferences, but further interaction continues to help.6

Figure 3: Percentage of zero-cost examples of CIPHER over time, binned per 20 rounds to show the trend (average across 3 seeds). In the legend, -_k_ means with top \(k\) retrieved examples, -_B_ for BERT, and -_M_ for MPNET.

Figure 2: Learning curves of different methods based on cumulative cost over time (average across 3 seeds). In the legend, -_k_ means with top \(k\) retrieved examples, -_B_ for BERT, and -_M_ for MPNET.

### Human Evaluation

We conduct two types of evaluation with human users to further understand the performance of our methods on summarization. We focus on our best-performing method _CIPHER-5-MPNET_.7

**Win Rate Evaluation.** We conduct win rate evaluation where evaluators are given a pair of text and choose which one has higher quality. We compare the output of _CIPHER-5-MPNET_ against the output of the best-performing baseline _ICL-edit-5-MPNET_, and against the generation of the oracle method. Each evaluation covers 15 text pairs, with three random samples from each scenario in the last 50 rounds of interaction. We conduct these _CIPHER vs. ICL_ and _CIPHER vs. Oracle_ evaluations with 7 human evaluators recruited through our personal network. For each text pair, we consider the output receiving the majority vote as a win. We find that the win rate of _CIPHER-5-MPNET_ against _ICL-edit-5-MPNET_ is 73.3%. This confirms that our method outperforms the best-performing baseline for human users. In _CIPHER vs. Oracle_ evaluation, the win rate of _CIPHER-5-MPNET_ is 23.7%, which reflects the performance gap we reported in previous sections.

**Edits by Human Users.** We study the edit feedback from human users to the generation of _CIPHER-5-MPNET_ and the oracle method. We instruct human users to edit the output based on the given latent preference, and to leave no edits when the output aligns with the given preference. We mix 20 outputs from _CIPHER-5-MPNET_ and the oracle method so that human users cannot tell the source of each output. The total edit distance, averaged across 3 human users, is 211 for CIPHER, and 98 for the oracle method. The averaged percentage of zero-edit examples is 60% for CIPHER and 76.7% for oracle.

## 5 Conclusion

We study aligning LLM-based agents using user edits that arise naturally in applications such as writing assistants. We introduce the PRELUDE framework that seeks to learn the latent user preferences that drive these edits, and uses them to generate a response. We propose a practical algorithm CIPHER that implements PRELUDEand outperforms baselines on two interactive tasks with a GPT-4 simulated user. Evaluating CIPHER with human-in-the-loop as well as developing algorithms that can fine-tune LLMs using user edit where fine-tuning is feasible, are interesting future work directions.

}  
**Latent User Preference** & **(Round) Learned Preference** \\ 
**News article.** & targeted to \\ young children, storytelling, short sentences, playful language, interactive, positive & (22) Fairy tale narrative style, informal and conversational tone, use of rhetorical questions, simplified language. \\
**Reddit post.** & second person \\ narrative, brief, show emotions, invoke personal reflection, immersive & (102) The user prefers a second-person narrative and a more direct, personal tone \\ 
**Wikipedia page.** & bullet \\ points, parallel structure, brief & (124) Concise and factual writing style, bullet-point formatting \\ brief & (197) Concise and streamlined formatting, with bullet points and clear subheadings for easy scanning \\ 
**Paper abstract.** & tweet style, (20) Concise, conversational summaries with bullet points and emojs. \\ simple English, inquisitive, skillful foreshadowing, with emojis & (111) Concise, conversational, whimsical bullet-point summaries with emojis. \\  & (193) Concise, conversational, and whimsical bullet-point summaries with emojis. \\  & (197) Concise and streamlined formatting, with bullet points and clear subheadings for easy scanning \\ 
**Movie review.** & question answering style & (12) The user prefers a straightforward, clear, and concise writing style with factual formatting. \\  & (123) The user prefers a clear and concise question and answer format with straightforward language. \\  & (199) Concise, Structured Q&A with Whimsical Clarity \\   

Table 3: Examples of learned preferences on summarization task with _CIPHER-5-MPNET_, grouped based on the document source and corresponding latent preference. We randomly sample 3 examples per type at the beginning, middle, and end of the interaction history.

#### Acknowledgments

Gao was a research intern in MSR NYC, and later was partially supported by NSF project #1901030. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. We thank MSR NYC research community, Jonathan D. Chang, Daniel D. Lee, Claire Cardie, and Sasha Rush for helpful discussions and support. We also thank Stephane Aroca-Ouellette, Kyunghyun Cho, and Columbia NLP community for their valuable feedback.