# Faster Discrete Convex Function Minimization with Predictions: The M-Convex Case

Taihei Oki

The University of Tokyo

Tokyo, Japan

oki@mist.i.u-tokyo.ac.jp &Shinsaku Sakaue

The University of Tokyo

Tokyo, Japan

sakaue@mist.i.u-tokyo.ac.jp

###### Abstract

Recent years have seen a growing interest in accelerating optimization algorithms with machine-learned predictions. Sakaue and Oki (NeurIPS 2022) have developed a general framework that warm-starts the _L-convex function minimization_ method with predictions, revealing the idea's usefulness for various discrete optimization problems. In this paper, we present a framework for using predictions to accelerate _M-convex function minimization_, thus complementing previous research and extending the range of discrete optimization algorithms that can benefit from predictions. Our framework is particularly effective for an important subclass called _laminar convex minimization_, which appears in many operations research applications. Our methods can improve time complexity bounds upon the best worst-case results by using predictions and even have potential to go beyond a lower-bound result.

## 1 Introduction

Recent research on _algorithms with predictions_ has demonstrated that we can improve algorithms' performance beyond the limitations of the worst-case analysis using predictions learned from past data. In particular, a surge of interest has been given to research on using predictions to improve the time complexity of algorithms, which we refer to as _warm-starts with predictions_ for convenience. Since Dinitz et al. 's seminal work on speeding up the Hungarian method for weighted bipartite matching with predictions, researchers have extended this idea to algorithms for various problems . Sakaue and Oki  have found similarities between the idea and standard warm-starts in continuous convex optimization and extended it to _L-convex function minimization_, a broad class of discrete optimization problems studied in _discrete convex analysis_. They thus have shown that warm-starts with predictions can improve the time complexity of algorithms for various discrete optimization problems, including weighted bipartite matching and weighted matroid intersection.

In this paper, we extend the idea of warm-starts with predictions to a new direction called _M-convex function minimization_, another important problem class studied in discrete convex analysis. The M-convexity is known to be in conjugate relation to the L-convexity. Hence, exploring the applicability of warm-starts with predictions to M-convex function minimization is crucial to broaden further the range of algorithms that can benefit from predictions, as is also mentioned in . This paper mainly discusses an important subclass of M-convex function minimization called _laminar convex minimization_ (Laminar), a large problem class widely studied in operations research (see references in Section 1.2). To make it easy to imagine, we describe the most basic form (Box) of Laminar,

\[()^{n}}{}\ \ _{i=1}^{n}f_{i}(x_{i})\ _{i=1}^{n}x_{i}=R,\ _{i} x_{i} u_{i}\ (i=1,,n),\] (1)

where \(f_{1},,f_{n}:\) are univariate convex functions, \(R\), \(_{1},,_{n}\{-\}\), and \(u_{1},,u_{n}\{+\}\). Note that the variable \(x^{n}\) is an integer vector, which is needed when,for example, considering allocating \(R\) indivisible resources to \(n\) entities. As detailed later, adding some constraints and objectives to Box yields more general classes, Nested and Laminar, where the level of generality increases in this order. Streamlining repetitive solving of such problems by using predictions can provide substantial benefits of saving computation costs, as we often encounter those problems in, e.g., resource allocation , equilibrium analysis , and portfolio management .

### Our contribution

We give a framework for accelerating M-convex minimization with predictions building on previous research [11; 39] (Section 3). We show that, given a vector \(^{n}\) that predicts an optimal solution \(x^{*}^{n}\), the greedy algorithm (Algorithm 1) for M-convex function minimization finds an optimal solution in \((T_{}+T_{}\|x^{*}-\|_{1})\) time, where \(T_{}\) and \(T_{}\) represent the time for converting \(\) into an initial feasible solution and for finding a locally steepest descent direction, respectively. Since we can minimize \(\|x^{*}-\|_{1}\) provably and approximately given optimal solutions to past instances [11; 23], this framework can provide better time complexity bounds benefiting from predictions. We also discuss how to apply our framework to general M-convex function minimization in Section 3.1.

Section 4 presents our main technical results. We apply our framework to Laminar, Nested, and Box and obtain time complexity bounds shown in Table 1. Our time complexity bounds can improve the existing worst-case bounds given accurate predictions. In particular, our \((n\|x^{*}-\|_{1})\)-time bound for Laminar improves the existing \((n^{2} n)\)-time bound even if prediction error \(\|x^{*}-\|_{1}\) is as large as \((n)\). Our result for Nested is a corollary of that for Laminar and improves the existing worst-case bound if \(\|x^{*}-\|_{1}=(1)\). In the case of Box, we can further reduce the time complexity to \((n+ n\|x^{*}-\|_{1})\) by modifying the algorithm for Laminar. Notably, our algorithm for Box runs in \((n)\) time if \(\|x^{*}-\|_{1}=(n/ n)\), even though there exists an \((n(R/n^{2}))\)-time lower bound for Box. As far as we know, this is the first result that can go _beyond the lower bound_ on the time complexity in the context of warm-starts with predictions. Experiments in Section 5 confirm that using predictions can improve empirical computation costs.

Few studies in the literature have made explicit improvements upon the theoretically fastest algorithms, even if predictions are accurate enough. The only exception is , whose shortest-path algorithm with predictions can improve the best worst-case time complexity by a couple of log factors. By contrast, our methods with accurate predictions can improve the best worst-case bounds by \((n)\) (up to log factors) in the Laminar case and potentially go beyond the lower-bound result in the Box case. Thus, our work not only extends the class of problems that we can efficiently solve using predictions but also represents a crucial step toward revealing the full potential of warm-starts with predictions. In this paper, we do not discuss the worst-case time complexity of our algorithms since we can upper bound it by executing standard algorithms with worst-case guarantees in parallel, as discussed in .

### Related work

Algorithms with predictions , improving algorithms' performance by using predictions learned from past data, is a promising subfield in _beyond the worst-case analysis of algorithms_. While this idea initially gained attention to improve competitive ratios of online algorithms [36; 2; 28; 1],

   Problem & Our results & Worst-case time complexity \\  General & \((n+n^{2}_{f}\|x^{*}-\|_{1})\) & \(n^{2}(L/n)\{n,(L/n)}{ n}\} _{f}\) \\ Laminar & \((n\|x^{*}-\|_{1})\) & \((n^{2} n)\)[18; 34]1  \\ Nested & \((n n R)\) \\ Box & \((n+ n\|x^{*}-\|_{1})\) & \((n)\)[14; 19] \\   

Table 1: Our results and the best worst-case bounds for General, Laminar, Nested, and Box, where General refers to general M-convex function minimization discussed in Section 3.1. \(n\) is the number of variables, \(R\) specifies the equality constraint as in (1), and \(m=|\{Y\,:|Y| 2\,\}|=(n)\) is the number of additional constraints needed to convert Box into Nested and Laminar (see Section 4).

recent years have seen a surge of interest in improving algorithms' running time [11; 7; 39; 35; 10]. A comprehensive list of papers on algorithms with predictions is available at . The most relevant to our work is , in which predictions are used to accelerate L-/L3-convex function minimization, a large problem class including weighted bipartite matching and weighted matroid intersection. On the other hand, warm-starts with predictions remain to be studied for M-convex function minimization,2 another essential class that is in conjugate relation to L-convex function minimization in discrete convex analysis . Although our basic idea for utilizing predictions is analogous to the previous approach [11; 39], our algorithmic techniques to obtain the time complexity bounds in Table 1 for the specific M-convex function minimization problems are entirely different (see Section 4).

M-convex function minimization includes many important nonlinear integer programming problems, including Laminar, Nested, and Box, which have been extensively studied in the context of resource allocation . A survey of recent results is given in . Table 1 summarizes the worst-case time complexity bounds relevant to ours. Besides, faster algorithms for those problems under additional assumptions have been studied. For example, Schoot Uiterkamp et al.  showed that, if an objective function is a sum of \(f(x_{i}+b_{i})\) (\(i=1,,n\)) for some identical convex function \(f\) and \(b_{i}\), we can solve Box, Nested, and Laminar with existing algorithms that run in \((n)\)[4; 21], \((n m)\), and \((n^{2})\) time, respectively. Hochbaum  gave an \((n n)\)-time algorithm for Laminar with separable objective functions and no lower bound constraints.3 Even in those special cases, our results in Table 1 are comparable or better given that prediction errors \(\|x^{*}-\|_{1}\) are small enough. There also exist empirically fast algorithms [42; 47], whose time complexity bounds are generally worse than the best results. Other problem classes with network and submodular constraints have also been studied [20; 30]. Extending our framework to those classes is left for future work.

Resource allocation with continuous variables has also been well-studied. One may think we can immediately obtain faster algorithms for discrete problems by accelerating continuous optimization algorithms for relaxed problems with predictions and using obtained solutions as warm-starts. However, this is not true since there generally exists an \((n)\) gap in the \(_{1}\)-norm between real and integer optimal solutions [30, Example 2.9], implying that we cannot always obtain faster algorithms for solving a discrete problem even if an optimal solution to its continuous relaxation is available for free.

## 2 Preliminaries

Let \(N\{1,,n\}\) be a finite ground set of size \(n\). For \(i N\), let \(e_{i}\) be the \(i\)th standard vector, i.e., all zero but the \(i\)th entry set to one. For any \(x^{N}\) and \(X N\), let \(x(X)=_{i X}x_{i}\). Let \(\) denote (element-wise) rounding to a closest integer. For a function \(f:^{N}\{+\}\) on the integer lattice \(^{N}\), its _effective domain_ is defined as \(\,f\{\,x^{N}\,:\,f(x)<+\,\}\). A function \(f\) is called _proper_ if \(\,f\). For \(Q^{N}\), its _indicator function_\(_{Q}:^{N}\{0,+\}\) is defined by \(_{Q}(x) 0\) if \(x Q\) and \(+\) otherwise.

### M-convexity and greedy algorithm for M-convex function minimization

We briefly explain M-convex functions and sets; see [31, Sections 4 and 6] for details. We say a proper function \(f:^{N}\{+\}\) is _M-convex_ if for every \(x,y\,f\) and \(i\{\,i^{} N\,:\,x_{i^{}}>y_{i^{}}\,\}\), there exists \(j\{\,j^{} N\,:\,x_{j^{}}<y_{j^{}}\,\}\) such that \(f(x)+f(y) f(x-e_{i}+e_{j})+f(y+e_{i}-e_{j})\). A non-empty set \(Q^{N}\) is said to be _M-convex_ if its indicator function \(_{Q}:^{N}\{0,+\}\) is M-convex. Conversely, if \(f\) is an M-convex function, \(\,f\) is an M-convex set. An M-convex set always lies in a hyperplane defined by \(\{\,x^{N}\,:\,x(N)=R\,\}\) for some \(R\). It is worth mentioning that the M-convexity is built upon the well-known _basis exchange property_ of matroids, and the base polytope of a matroid is the convex hull of an M-convex set.

The main subject of this paper is M-convex function minimization, \(_{x^{N}}f(x)\), where \(f:^{N}\{+\}\) is an M-convex function. Note that \(\,f^{N}\) represents the feasible region of the problem. For convenience of analysis, we assume the following basic condition.

**Assumption 2.1**.: _An M-convex function \(f:^{N}\{+\}\) always has a unique minimizer \(x^{*}\)._This uniqueness assumption is common in previous research [39; 10] (and is also needed in [11; 7; 35], although not stated explicitly). In the case of Laminar, it is satisfied for generic, strictly convex objective functions. Even if not, there are natural tie-breaking rules, e.g., choosing the minimizer that attains the lexicographic minimum among all minimizers closest to the origin; we can implement this by adding \(\|x\|_{2}^{2}+_{i=1}^{n}^{i+1}|x_{i}|\) for sufficiently small \((0,1)\) to \(f\), preserving its M-convexity. This is in contrast to the L-convex case, where arbitrarily many minimizers always exist (see ).

We can solve M-convex function minimization by a simple greedy algorithm shown in Algorithm 1, which iteratively finds a locally steepest direction, \(-e_{i}+e_{j}\), and proceeds along it. If this update does not improve the objective value, the current solution is ensured to be the minimizer \(x^{*}= f\) due to the M-convexity [31, Theorem 6.26]. The number of iterations depends on the \(_{1}\)-distance to \(x^{*}\) as follows.

**Proposition 2.2** ([44, Corollary 4.2]).: _Algorithm 1 terminates exactly in \(\|x^{*}-x^{}\|_{1}/2\) iterations._

A similar iterative method is used in the L-convex case , whose number of iterations depends on the \(_{}\)-distance and a steepest direction is found by some combinatorial optimization algorithm (e.g., the Hopcroft-Karp algorithm in the bipartite-matching case). On the other hand, in the M-convex case, computing a steepest direction is typically cheap (as we only need to find two elements \(i,j N\)), while the number of iterations depends on the \(_{1}\)-distance, which can occupy a larger fraction of the total time complexity than the \(_{}\)-distance. Hence, reducing the number of iterations with predictions can be more effective in the M-convex case. Section 3 presents a framework for this purpose.

Similar methods to Algorithm 1 are also studied in submodular function maximization . Indeed, M-convex function minimization captures a non-trivial subclass of submodular function maximization that the greedy algorithm can solve (see [31, Note 6.21]), while it is NP-hard in general [32; 13]

## 3 Warm-start-with-prediction framework M-convex function minimization

We present a framework for warm-starting the greedy algorithm for M-convex function minimization with predictions. Although it resembles those of previous studies [11; 39], it is worth stating explicitly how the time complexity depends on prediction errors for M-convex function minimization.

We consider the following three phases as in previous studies: (i) obtaining an initial feasible solution \(x^{}^{N}\) from a prediction \(^{N}\), (ii) solving a new instance by warm-starting an algorithm with \(x^{}\), and (iii) learning predictions \(\). The following theorem gives a time complexity bound for (i) and (ii), implying that we can quickly solve a new instance if a given prediction \(\) is accurate.

**Theorem 3.1**.: _Let \(f:^{N}\{+\}\) be an M-convex function that has a unique minimizer \(x^{*}= f\) and \(^{N}\) be a possibly infeasible prediction. Suppose that Algorithm 1 starts from an initial feasible solution satisfying \(x^{}\{\,\|x-\|_{1}:\,xf\,\}\). Then, Algorithm 1 terminates in \(O(\|x^{*}-\|_{1})\) iterations. Thus, if we can compute \(x^{}\) in \(T_{}\) time and find \(i,j N\) that minimize \(f(x-e_{i}+e_{j})\) in Step 3 in \(T_{}\) time, the total time complexity is \(O(T_{}+T_{}\|x^{*}-\|_{1})\)._

Proof.: Due to Proposition 2.2, the number of iterations is bounded by \(\|x^{*}-x^{}\|_{1}/2\). Thus, it suffices to prove \(\|x^{*}-x^{}\|_{1}=(\|x^{*}-\|_{1})\). By using the triangle inequality, we obtain \(\|x^{*}-x^{}\|_{1}\|x^{*}-\|_{1}+\|- \|_{1}+\|\|-x^{}\|_{1}\). We below show that the right-hand side is \((\|x^{*}-\|_{1})\). The second term is bounded as \(\|-\|_{1}\|-x^{*}\|_{1}\) since \(x^{*}^{N}\). As for the third term, we have \(\|-x^{}\|_{1}\|-x ^{*}\|_{1}\) since \(x^{}f\) is a feasible point closest to \(\) and \(x^{*}f\), and the right-hand side, \(\|-x^{*}\|_{1}\), is further bounded as \(\|-x^{*}\|_{1}\| -\|_{1}+\|-x^{*}\|_{1} 2\|-x^{*}\|_{1}\) due to the previous bound on the second term. Thus, \(\|x^{*}-x^{}\|_{1} 4\|-x^{*}\|_{1}\) holds. 

Note that we round \(\) to a closest integer point \(\) before projecting it onto \(f\), while rounding comes after projection in the L-/L\({}^{}\)-convex case . This subtle difference is critical since rounding after projection may result in an infeasible integer point that is far from the minimizer \(x^{*}\) by \((n)\) in the \(_{1}\)-norm. To avoid this, we swap the order of the operations and modify the analysis accordingly.

Let us turn to phase (iii), learning predictions. This phase can be done in the same way as previous studies . In particular, we can learn predictions in an online fashion with the standard online subgradient descent method (OSD) as in , which works as follows in our case. Let \(V^{N}\) be a convex set that we expect to contain an optimal prediction. For any sequence of M-convex functions \(f_{1},,f_{T}\), we regard \(L_{t}():=\|x_{t}^{*}-\|_{1}\) for \(t=1,,T\) as loss functions, where \(x_{t}^{*}\) is the minimizer of \(f_{t}\). Fixing \(_{1} V\) arbitrarily, for \(t=1,,T\), OSD iteratively computes a subgradient \(z_{t} L_{t}(_{t})\) and set \(_{t+1}=_{ V}_{t}- z_{t}- _{2}\), where \(>0\) is the step size. OSD returns predictions \(_{1},,_{T}\) that enjoy a regret bound (see, e.g., ), and a sample complexity bound follows from online-to-batch conversion . Formally, the following proposition guarantees that we can provably learn predictions to decrease the time complexity bound in Theorem 3.1.

**Proposition 3.2** ().: _Let \(f_{t}:^{N}\{+\}\) for \(t=1,,T\) be a sequence of M-convex functions, each of which has a unique minimizer \(x_{t}^{*}= f_{t}\), and \(V^{N}\) be a closed convex set whose \(_{2}\)-diameter is \(D\). Then, OSD with \(=D/\) returns \(_{1},,_{T} V\) that satisfy_

\[_{t=1}^{T} x_{t}^{*}-_{t}_{1}_{^{*} V }_{t=1}^{T} x_{t}^{*}-^{*}_{1}+(D).\]

_Furthermore, for any distribution \(\) over M-convex functions \(f:^{N}\{+\}\), each of which has a unique minimizer \(x_{f}^{*}= f\), \((0,1]\), and \(>0\), given \(T=^{2}n \) i.i.d. draws, \(f_{1},,f_{T}\), from \(\), we can compute \( V\) that satisfies_

\[*{}_{f} x_{f}^{*}-_{ 1}_{^{*} V}*{}_{f}  x_{f}^{*}-^{*}_{1}+\]

_with a probability of at least \(1-\) via online-to-batch conversion (i.e., we apply OSD to \( x_{f_{t}}^{*}-_{1}\) for \(t=1,,T\) and average the outputs)._

The convex set \(V\) should be designed based on prior knowledge of upcoming instances. For example, previous studies  set \(V=[-C,+C]^{N}\) for some \(C>0\), which is expected to contain optimal solutions of all possible instances; then \(D=2C\) holds. In our case, we sometimes know that the total amount of resources is fixed, i.e., \(x(N)=R\), and that every \(x_{i}\) is always non-negative. Then, we may set \(V=\{\,x[0,R]^{N}\,:\,x(N)=R\,\}\), whose \(_{2}\)-diameter is \(D=R\).

### Time complexity bound for general M-convex function minimization

We here discuss how to apply the above framework to general M-convex function minimization. The reader interested in the main results in Table 1 can skip this section and go to Section 4.

For an M-convex function \(f:^{N}\{+\}\), given access to \(f\)'s value and \(f\), we can implement the greedy algorithm with warm-starts so that both \(T_{}\) and \(T_{}\) are polynomially bounded. Suppose that evaluating \(f(x)\) for any \(x^{N}\) takes \(_{f}\) time. Then, we can find a steepest descent direction at any \(xf\) in \(T_{}=(n^{2}_{f})\) time by computing \(f(x-e_{i}+e_{j})\) for all \(i,j N\). As for the computation of \(x^{}\), we need additional information to identify \(f\) (otherwise, finding any feasible solution may require exponential time in the worst case). Since \(f\) is an M-convex set, we build on a fundamental fact that any M-convex set can be written as the set of integer points in the _base polyhedron_ of an integer-valued _submodular function_. A set function \(:2^{N}\{+\}\) is called _submodular_ if \((X)+(Y)(X Y)+(X Y)\) holds for \(X,Y N\), and its _base polyhedron_ is defined as \(()\{\,x^{N}\,:\,x(X)(X)\,\,\,(X  N),\,x(N)=(N)\,\}\), where \(()=0\) and \((N)<+\) are assumed. Thus, \(f\) is expressed as \(f=()^{N}\) with an integer-valued submodular function \(:2^{N}\{+\}\). We assume that, for any \(x^{N}\), we can minimize the submodular function \(+x\), defined by \((+x)(X)(X)+x(X)\) for \(X N\), in \(\) time. Then, we can obtain \(x^{}f\) from \(^{N}\) in \(T_{}=(n)\) time, as detailed in Appendix A.1. Therefore, Theorem 3.1 implies the following bound on the total time complexity.

**Theorem 3.3**.: _Given a prediction \(^{N}\), we can minimize \(f\) in \((n+n^{2}_{f} x^{*}-_{ 1})\) time._

The current fastest M-convex function minimization algorithms run in \(n^{3}_{f}\) and \(n^{3}+n^{2} / n_{f}\) time , where \(L=\{\, x-y_{}\,:\,x,yf\,\}\). Thus, our algorithm runs faster if \( x^{*}-_{1}=(n)\) and \(=(n^{2}_{f})\). We discuss concrete situations where our approach is particularly effective in Appendix A.2.

## 4 Laminar convex minimization

This section presents how to obtain the time complexity bounds in Table 1 by applying our framework to laminar convex minimization (\(\)) and its subclasses, which are special cases of M-convex function minimization (see [31, Section 6.3]). We first introduce the problem setting of \(\).

A _laminar_\( 2^{N}\) is a set family such that for any \(X,Y\), either \(X Y\), \(X Y\), or \(X Y=\) holds. For convenience, we suppose that \(\) satisfies the following basic properties without loss of generality: \(\), \(N\), and \(\{i\}\) for every \(i N\). Then, \(\) is formulated as follows:

\[^{N}}{}\ _{Y}f_{Y}(x(Y)) \ \ x(N)=R,\ _{Y} x(Y) u_{Y}\ (Y\{ ,N\}),\] (2)

where each \(f_{Y}:\) (\(Y\)) is a univariate convex function that can be evaluated in \((1)\) time, \(R\), and \(_{Y}\{-\}\) and \(u_{Y}\{+\}\) for \(Y\). We denote the objective function by \(f_{}(x)_{Y}f_{Y}(x(Y))\). We let \(f:^{N}\{+\}\) be a function such that \(f(x)=f_{}(x)\) if \(x\) satisfies the constraints in (2) and \(f(x)=+\) otherwise; then, \(f\) is M-convex and \(f^{N}\) represents the feasible region of (2). Nested is a special case where \(f_{}\) is written as \(_{i N}_{i}(x_{i})\) and \(\{\,Y\,:\,|Y| 2\,\}=\{Y_{1},Y_{2},,Y_{m}\}\) consists of nested subsets, i.e., \(Y_{1} Y_{2} Y_{m}\), and \(\) is a special case of Nested without nested-subset constraints. Note that our framework in Section 3 only requires the ground set \(N\) to be identical over instances. Therefore, we can use it even when both objective functions and constraints change over instances.

It is well known that we can represent a laminar \( 2^{N}\) by a tree \(T_{}=(,E)\). The vertex set is \(=\{\}\). For \(Y\{N\}\), we call \(X\) a _parent_ of \(Y\) if \(X\) is the unique minimal set that properly contains \(Y\); let \(p(Y)\) denote the parent of \(Y\). We call \(Y\{N\}\) a _child_ of \(X\) if \(p(Y)=X\). This parent-child relation defines the set of edges as \(E=\{\,(X,Y)\,:\,X,Y,\ p(Y)=X\,\}\). Note that each \(\{i\}\) corresponds to a leaf and that \(N\) is the root. For simplicity, we below suppose the tree \(T_{}=(,E)\) to be binary without loss of generality. If a parent has more than two children, we can recursively divide them into one and the others, which only doubles the number of vertices. Figure 1 illustrates a tree \(T_{}\) of a laminar \(=\{,\{1\},\{2\},\{3\},\{1,2\},\{1,2,3\}\}\).

Applications of \(\) include resource allocation , equilibrium analysis of network congestion games , and inventory and portfolio management . We below describe a simple example so that the reader can better grasp the image of \(\); we will also use it in the experiments in Section 5.

Example: staff assignment.We consider assigning \(R\) staff members to \(n\) tasks, which form the ground set \(N\). Each task is associated with a higher-level task. For example, if staff members have completed tasks \(1,2 N\), they are assigned to a new task \(Y=\{1,2\}\), which may involve integrating the outputs of the individual tasks. The dependencies among all tasks, including higher-level ones, can be expressed by a laminar \( 2^{N}\). Each task \(Y\) is supposed to be done by at least \(_{Y}\) (\( 1\)) and at most \(u_{Y}\) (\( R\)) members. An employer aims to assign staff members in an attempt to minimize the total perceived workload. For instance, if task \(i N\) requires \(c_{i}>0\) amount of work and \(x_{i}\) staff members are assigned to it, each of them may perceive a workload of \(f_{i}(x_{i})=c_{i}/x_{i}\). Similarly, the perceived workload of task \(Y=\{1,2\}\) is \(f_{Y}(x(Y))=c_{Y}/x(Y)\). The problem of assigning \(R\) staff members to \(n\) tasks to minimize the total perceived workload, summed over all tasks in \(\), is formulated as in (2). Figure 1 illustrates two example assignments, I and II. Here, people assigned to \(\{1\}\) and \(\{2\}\) must do more tasks than those assigned to \(\{3\}\), and hence assignment I naturally leads to a smaller total perceived workload than II. We can also use any convex function \(f_{Y}\) on \([_{Y},u_{Y}]\) to model other objective functions. Making it faster to solve such problems with predictions enables us to manage massive allocations daily or more frequently.

Our main technical contribution is to obtain the following time complexity bound for \(\) via Theorem 3.1, which also applies to \(\) since it is a special case of \(\).

**Theorem 4.1**.: _For \(\), given a prediction \(^{N}\), we can obtain an initial feasible solution \(x^{}*{arg\,min}\{\,\|x-\|_{1}\,:\,x f\,\}\) in \(T_{}=(n)\) time and find a steepest descent direction in Step 3 of Algorithm 1 in \(T_{}=(n)\) time. Thus, we can solve \(\) in \((n\|x^{*}-\|_{1})\) time._

Figure 1: Image of tree \(T_{}\). Each leaf \(\{i\}\) (\(i=1,2,3\)) has variable \(x_{i}\). The lower part shows example assignments.

We prove Theorem 4.1 by describing how to obtain an initial feasible solution and find a steepest descent direction in Sections 4.1 and 4.2, respectively. In Section 4.3, we further reduce the time complexity bound for \(\). The algorithmic techniques we use below are not so complicated and can be implemented efficiently, suggesting the practicality of our warm-start-with-prediction framework.

### Obtaining initial feasible solution via fast convex min-sum convolution

We show how to compute \(x^{}f\) in \(T_{}=(n)\) time. Given prediction \(^{N}\), we first compute \(^{N}\) in \((n)\) time and then solve the following special case of Laminar to obtain \(x^{}\):

\[^{N}}{}\ _{i N}|x_{i}- _{i}|\ \ x(N)=R,\ _{Y} x(Y) u_{Y}\ (Y \{,N\}).\] (3)

Note that it suffices to find an integer optimal solution to the continuous relaxation of (3) since all the input parameters are integers. Thus, we below discuss how to solve the continuous relaxation of (3).

Solving (3) naively may be as costly as solving the original Laminar instance. Fortunately, however, we can solve it much faster using the special structure of the \(_{1}\)-norm objective function. The method we describe below is based on the fast convex min-sum convolution , which immediately provides an \((n^{2}n)\)-time algorithm for solving (3). We simplify it and eliminate the logarithmic factors by using the fact that the objective function has only two kinds of slopes, \( 1\).

We suppose that each non-leaf vertex \(Y\) in \(T_{}=(,E)\) has a variable \(x_{Y}\), in addition to the original variables \(x_{i}\) for leaves \(\{i\}\). We consider assigning a univariate function \(g:\{+\}\) of the following form to each vertex in \(\):

\[g(x)=|x-b|+_{[,u]}(x),\] (4)

where \(,b,u\{\}\) and \( b u\). Note that if \(g\) is given by (4) up to an additive constant, its convex conjugate \(g^{*}(p)=\{\, p,x-g(x)\,:\,x\,\}\) is a piecewise-linear function whose slope is \(\) if \(p-1\), \(b\) if \(-1 p+1\), and \(u\) if \(p+1\) (where \(=b\) and/or \(b=u\) can occur). Figure 2 illustrates this conjugate relation. We below construct such functions in a bottom-up manner on \(T_{}\).

First, we assign function \(g_{i}(x_{i})=|x_{i}-|}|+_{[_{i},u_{i}]}(x_{i})\) to each leaf \(\{i\}\), which represents the \(i\)th term of the objective function and the constraint on \(x_{i}\) in (3). Next, given two functions \(g_{X}(x_{X})=|x_{X}-b_{X}|+_{[_{X},u_{X}]}(x_{X})\) and \(g_{Y}(x_{Y})=|x_{Y}-b_{Y}|+_{[_{Y},u_{Y}]}(x_{Y})\) of \(X,Y\) with an identical parent \(X Y\), we construct the parent's function as \(g_{X Y}=(g_{X}\,\,g_{Y})+_{[_{X Y},u_{X Y}]}\), where \((g_{X}\,\,g_{Y})(x_{X Y})\{\,g_{X}(x_{X})+g_{Y}(x_{Y}) \,:\,x_{X}+x_{Y}=x_{X Y}\,\}\) is the infimal convolution. We can confirm that \(g_{X Y}\) also takes the form of (4) as follows. Since \(g_{X}\) and \(g_{Y}\) are of the form (4), \(g_{X}^{*}\) and \(g_{Y}^{*}\) have the same breakpoints, \( 1\) (see Figure 2). Furthermore, since \(g_{X}\,\,g_{Y}=(g_{X}^{*}+g_{Y}^{*})^{*}\) holds (e.g., [37, Theorem 16.4]), \(g_{X}\,\,g_{Y}\) takes the form of (4) with \(=_{X}+_{Y}\), \(b=b_{X}+b_{Y}\), and \(u=u_{X}+u_{Y}\). Finally, adding \(_{[_{X Y},u_{X Y}]}\) preserves the form of (4). We can compute resulting \(\), \(b\), and \(u\) values of \(g_{X Y}\) in \((1)\) time, and hence we can obtain \(g_{Y}\) for all \(Y\) in a bottom-up manner in \((n)\) time. By construction, for each \(Y\), \(g_{Y}(x_{Y})\) indicates the minimum objective value corresponding to the subtree, \((_{Y},E_{Y})\), rooted at \(Y\) when \(x_{Y}\) is given. That is, we have

\[g_{Y}(x_{Y})=_{i Y} x_{i}-_{i}\, \,:\,x(Y)=x_{Y},\ _{Y}^{*} x(Y^{}) u_{Y^{}}\,(Y^{}_{ Y}\{Y\})\,}\]

up to constants ignored when constructing \(g_{Y}\), where \(g_{Y}(x_{Y})=+\) if the feasible region is empty. Thus, \(g_{N}(R)\) corresponds to the minimum value of (3), and our goal is to find integer values \(x_{Y}\) for \(Y\) that attain the minimum value when \(x_{N}=R\) is fixed.

Given \(g_{Y}\) constructed as above, we can compute desired \(x_{Y}\) values in a top-down manner as follows. Let \(X Y\) be a non-leaf vertex with two children \(X\) and \(Y\). Once \(x_{X Y}g_{X Y}\) is fixed, we can regard \(\{\,g_{X}(x_{X})+g_{Y}(x_{Y})\,:\,x_{X}+x_{Y}=x_{X Y}\,\}\) (\(=g_{X Y}(x_{X Y})\)) as univariate convex piecewise-linear minimization with variable \(x_{X}\) (since \(x_{Y}=x_{X Y}-x_{X}\)), which we can solve in \((1)\) time. Moreover, since \(x_{X Y}\) and all the parameters of \(g_{X}\) and \(g_{Y}\) are integers, we can find an integral minimizer \(x_{X}\) (and \(x_{Y}=x_{X Y}-x_{X}\)). Starting from \(x_{N}=R\), we thus compute \(x_{Y}\) values for \(Y\) in a top-down manner, which takes \((n)\) time. The resulting \(x_{i}\) value for each leaf \(\{i\}\) gives the \(i\)th element of a desired initial feasible solution \(x^{}f\)

Figure 2: Conjugate relation of \(g\) and \(g^{*}\).

### Finding steepest descent direction via dynamic programming

We present a dynamic programming (DP) algorithm to find a steepest descent direction in \(T_{}=(n)\) time. Our algorithm is an extension of that used in . The original algorithm finds \(i\) that minimizes \(f(x-e_{i}+e_{j})\) for a fixed \(j\) in \((n)\) time. We below extend it to find a pair of \((i,j)\) in \((n)\) time.

Let \(x\,f\) be a current solution before executing Step 3 in Algorithm 1. We define a directed edge set, \(A_{x}\), on the vertex set \(\) as follows:

\[A_{x}=\{\,(p(Y),Y)\,:\,Y\{N\},\;x(Y)<u_{Y}\,\}\{\,( Y,p(Y))\,:\,Y\{N\},\;x(Y)>_{Y}\,\}.\]

Note that \(x-e_{i}+e_{j}\) is feasible if and only if \((,A_{x})\) has a directed path from \(\{i\}\) to \(\{j\}\). We then assign an edge weight \(w_{X,Y}\) to each \((X,Y) A_{x}\) defined as

\[w_{X,Y}=f_{Y}(x(Y)+1)-f_{Y}(x(Y))&X=p(Y),\\ f_{X}(x(X)-1)-f_{X}(x(X))&Y=p(X).\]

By the convexity of \(f_{Y}\), we have \(w_{p(Y),Y} w_{Y,p(Y)}\), i.e., there is no negative cycle. If \(x-e_{i}+e_{j}\) is feasible, \(f_{}(x-e_{i}+e_{j})-f_{}(x)\) is equal to the length of a shortest path from \(\{i\}\) to \(\{j\}\) with respect to the edge weights \(w_{X,Y}\) (see [30, Section 3.3]). Therefore, finding a steepest descent direction, \(-e_{i}+e_{j}\), reduces to the problem of finding a shortest leaf-to-leaf path in this (bidirectional) tree \(T_{x}:=(,A_{x})\). Constructing this tree takes \((n)\) time.

We present a DP algorithm for finding a shortest leaf-to-leaf path. For \(Y\), we denote by \(T_{x}(Y)\) the subtree of \(T_{x}\) rooted at \(Y\). Let \(_{}^{Y}\) be the length of a shortest path from a leaf to \(Y\) in \(T_{x}(Y)\), \(_{}^{Y}\) the length of a shortest path from \(Y\) to a leaf in \(T_{x}(Y)\), and \(_{}^{Y}\) the length of a shortest path between any leaves in \(T_{x}(Y)\). Clearly, \(_{}^{Y}=_{}^{Y}=_{}^ {Y}=0\) holds if \(Y\) is a leaf in \(T_{x}\). For a non-leaf vertex \(Y\), let \((Y)\) denote the set of children of \(Y\) in \(T_{x}\). We have the following recursive formulas:

\[_{}^{Y}=_{X(Y):\\ (X,Y) A_{x}}-14.226378pt_{}^ {X}+w_{X,Y}},\;\;_{}^{Y}=_{X (Y):\\ (Y,X) A_{x}}-14.226378pt_{}^ {Y}+w_{Y,X}},\;\;_{}^{Y}=-14.226378pt _{}^{Y}+_{}^{Y},_{X(Y)} -14.226378pt},\]

where we regard the minimum on an empty set as \(+\). Note that, if shortest leaf-to-\(Y\) and \(Y\)-to-leaf paths in \(T_{x}(Y)\) are not edge-disjoint, there must be a leaf-to-leaf simple path in \(T_{x}(Y)\) whose length is no more than \(_{}^{Y}+_{}^{Y}\) since no negative cycle exists. According to these recursive formulas, we can compute \(_{}^{Y},_{}^{Y}\), and \(_{}^{Y}\) for all \(Y\) in \((n)\) time by the bottom-up DP on \(T_{x}\). Then, \(_{}^{N}\) is the length of a desired shortest leaf-to-leaf path, and its leaves \(\{i\},\{j\}\) can be obtained by backtracking the DP table in \((n)\) time. Thus, we can find a desired direction \(-e_{i}+e_{j}\) in \((n)\) time.

### Faster steepest descent direction finding for box-constrained case

We focus on Box (1) and present a faster method to find a steepest descent direction, which takes only \(T_{}=( n)\) time after an \((n)\)-time pre-processing. Note that we can obtain an initial feasible solution with the same method as in Section 4.1; hence \(T_{}=(n)\) also holds in the Box case.

**Theorem 4.2**.: _For Box, given a prediction \(^{N}\), after an \((n)\)-time pre-processing (that can be included in \(T_{}=(n)\)), we can find a steepest descent direction in Step 3 of Algorithm 1 in \(T_{}=( n)\) time. Thus, we can solve Box in \((n+ n\|x^{*}-\|_{1})\) time._

Proof.: In the Box case, \(f(x-e_{i}+e_{j})-f(x)=f_{i}(x_{i}-1)-f_{i}(x_{i})+f_{j}(x_{j}+1)-f_{j}(x_{j})\) holds if \(x\) and \(x-e_{i}+e_{j}\) are feasible. Furthermore, we only need to care about the box constraints, \(_{i} x_{i} u_{i}\) for \(i=1,,n\) (since \(x(N)=R\) is always satisfied due to the update rule). Therefore, by keeping \(_{k}^{-} f_{k}(x_{k}-1)-f_{k}(x_{k})+_{[_{k}+1,u_{k}]}( x_{k})\) and \(_{k}^{+} f_{k}(x_{k}+1)-f_{k}(x_{k})+_{[_{k},u_{k}-1]}( x_{k})\) values for \(k=1,,n\) with two min-heaps, respectively, we can efficiently find \(i\{_{k}^{-}\}_{k=1}^{n}\) and \(j\{_{k}^{+}\}_{k=1}^{n}\); then, \(-e_{i}+e_{j}\) is a steepest descent direction. More precisely, at the beginning of Algorithm 1, we construct the two heaps that maintain \(_{k}^{-}\) and \(_{k}^{+}\) values, respectively, and two arrays that keep track of the location of each element in the heaps; this pre-processing takes \((n)\) time. Then, in each iteration of Algorithm 1, we can find a steepest descent direction \(-e_{i}+e_{j}\), update \(_{i}^{-}\), \(_{i}^{+}\), \(_{j}^{-}\), and \(_{j}^{+}\) values (by the so-called increase-/decrease-key operations), and update the heaps and arrays in \(T_{}=( n)\) time. Thus, Theorem 3.1 implies the time complexity.

## 5 Experiments

We complement our theoretical results with experiments. We used MacBook Air with Apple M2 chip, \(24\,\) of memory, and macOS Ventura 13.2.1. We implemented algorithms in Python 3.9.12 with libraries such as NumPy 1.23.2. We used Gurobi 10.0.1  for a baseline method explained later. The source code is available at https://github.com/ssakue/alps-m-convex-code.

### Staff assignment

We consider Laminar instances of the staff-assignment setting described in Section 4. Suppose that we have \(R=12800\) staff members and \(n=128\) tasks. Let \(T_{}=(,E)\) be a complete binary tree with \(n\) leaves. Define an objective function and inequality constraints as \(f_{}(x)=_{Y}c_{Y}/x(Y)\) and \(_{Y} x(Y) R\) for \(Y\{,N\}\), respectively, with \(c_{Y}\) and \(_{Y}\) values defined as follows. We set \(c_{Y}=\{_{i Y}i+ u_{a},1\}\), where \(u_{a}\) follows the standard normal distribution and \(\) controls the noise strength. We let \(_{Y}=\{2^{h}+u_{b},R/2^{n-h}\}\), where \(h\{0,1,, n\}\) is the height of \(Y\) in \(T_{}\) (a leaf \(Y\) has \(h=0\)) and \(u_{b}\) is drawn uniformly at random from \(\{0,1,,50\}\); the minimum with \(R/2^{n-h}\) is taken to ensure that the feasible region is non-empty. We thus create a dataset of \(T=100\) random instances for each \(\{1,5,10,20\}\). We generate \(10\) such random datasets independently to calculate the mean and standard deviation of the results. The \(T\) instances arrive one by one and we learn predictions from optimal solutions to past instances online. By design of \(c_{Y}\), the \(i\)th entry of an optimal solution tends to be larger as \(i\) increases, which is unknown in advance and should be reflected on predictions \(\) by learning from optimal solutions to past instances.

We learn predictions \(_{t}^{N}\) for \(t=1,,T\) by using OSD on \(V=\{\,x[0,R]^{N}\,:\,x(N)=R\,\}\) with a step size of \(0.01\), where the projection onto \(V\) is implemented with a technique in . We use the all-one vector multiplied by \(R/n\) as an initial prediction, \(_{0} V\), and set the \(t\)th prediction, \(_{t}\), to the average of past \(t\) outputs, based on online-to-batch conversion. We denote this method by "Learn." We also use two baseline methods, "Cold" and "Relax", which obtain initial feasible solutions of the greedy algorithm as follows. Cold always uses \(_{0}\) as an initial feasible solution. Relax is a variant of the continuous relaxation approach , the fastest method for Laminar with quadratic objectives. Given a new instance, Relax first solves its continuous relaxation (using Gurobi), where the objective function is replaced with its quadratic approximation at \(_{0}\), and then converts the obtained solution into an initial feasible solution, as with our method. Note that Relax requires information on newly arrived instances, unlike Learn and Cold. Thus, Relax naturally produces good initial feasible solutions while incurring the overhead of solving new relaxed problems. We compare those initialization methods in terms of the number of iterations of the greedy algorithm.

Figure 3 compares Learn, Relax, and Cold for each noise strength \(\). Learn always outperforms Cold, and it does even Relax if \( 10\), suggesting that under moderate noise levels, learning predictions from past optimal solutions can accelerate the greedy algorithm more effectively than solving the relaxed problem of a new instance. The advantage of Learn decreases as \(\) increases, as expected.

### Resource allocation

We also present experiments using Nested instances of [47, Section 6.3], which include three types of problems, denoted by F, CRASH, and FUEL. The objective functions of F, CRASH, and FUEL are written as \(_{i=1}^{n}f_{i}(x_{i})\) where \(f_{i}(x_{i})=x_{i}^{4}/4+p_{i}x_{i}\), \(f_{i}(x_{i})=k_{i}+p_{i}/x_{i}\), and \(f_{i}(x_{i})=p_{i} c_{i}(c_{i}/x_{i})^{3}\), respectively, with some input parameters \(p_{i}\), \(k_{i}\), \(c_{i}\). F is a synthetic problem of optimizing the

Figure 3: The number of iterations of the greedy algorithm initialized with Learn, Relax, and Cold. The curve and error band show the mean and standard deviation of \(10\) independent runs, respectively.

fourth-order polynomial, while CRASH and FUEL come from real-world project crashing and ship speed optimization, as noted in . We create Nested instances with \(n=100\) for F, CRASH, and FUEL based on the procedure of . We then generate \(T=100\) instances by perturbing parameters defining constraints and objectives with Gaussian noises multiplied by \(=0.1,1.0,10.0\), which controls the noise strength. As with the previous experiments, we measure the number of iterations of the greedy algorithm initialized by Cold, Relax, and Learn over the \(100\) instances. Regarding Learn, we set OSD's step size in the same manner as the previous experiments, and each element of an initial prediction \(_{0}\) is set to \( R/n\) or \( R/n+1\) at random so that \(_{0}(N)=R\) holds.

Figure 4 shows the results. Similar to the previous synthetic setting, Learn attains fewer iterations than Relax and Cold for CRASH and FUEL with moderate noise strengths (\(=0.1,1.0\)). As for F, Relax performs extremely well and surpasses Learn, probably because the synthetic fourth-order polynomial objective is easy to handle with the continuous-relaxation method used in Relax. Nevertheless, it is significant that Learn can surpass Relax for CRASH and FUEL, which come from real-world applications, under moderate noises.

## 6 Conclusion and limitations

We have extended the idea of warm-starts with predictions to M-convex function minimization. By combining our framework with algorithmic techniques, we have obtained specific time complexity bounds for Laminar, Nested, and Box. Those bounds can be better than the current best worst-case bounds given accurate predictions, which we can provably learn from past data. Experiments have confirmed that using predictions reduces the number of iterations of the greedy algorithm.

Since our focus is on improving theoretical guarantees with predictions, further study of practical aspects is left for future work. While we have used the standard OSD for learning predictions, we expect that more sophisticated learning methods can further improve the empirical performance. Also, extending the framework to other problem classes is an exciting future direction. A technical open problem is eliminating Assumption 2.1, although it hardly matters in practice. We expect the idea of  for the L-/L\({}^{4}\)-convex case is helpful, but it seems more complicated in the M-convex case.

Figure 4: The number of iterations of the greedy algorithm initialized with Learn, Relax, and Cold. The curve and error band show the mean and standard deviation of \(10\) independent runs, respectively.