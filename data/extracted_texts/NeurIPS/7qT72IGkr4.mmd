# Performative Control for Linear Dynamical Systems

Songfu Cai, Fei Han, Xuanyu Cao

Department of Electronic and Computer Engineering

The Hong Kong University of Science and Technology

eesfcai@ust.hk, fhanac@connect.ust.hk, eexcao@ust.hk

Equal contribution.Corresponding author.

###### Abstract

We introduce the framework of performative control, where the policy chosen by the controller affects the underlying dynamics of the control system. This results in a sequence of policy-dependent system state data with policy-dependent temporal correlations. Following the recent literature on performative prediction , we introduce the concept of a performatively stable control (PSC) solution. We first propose a sufficient condition for the performative control problem to admit a unique PSC solution with a problem-specific structure of distributional sensitivity propagation and aggregation. We further analyze the impacts of system stability on the existence of the PSC solution. Specifically, for almost surely strongly stable policy-dependent dynamics, the PSC solution exists if the sum of the distributional sensitivities is small enough. However, for almost surely unstable policy-dependent dynamics, the existence of the PSC solution will necessitate a temporally backward decaying of the distributional sensitivities. We finally provide a repeated stochastic gradient descent scheme that converges to the PSC solution and analyze its non-asymptotic convergence rate. Numerical results validate our theoretical analysis.

## 1 Introduction

Control theory is a fundamental field of study in engineering and mathematics that centers on coordinating the behaviors of dynamical systems. It has extensive applications in aerospace, robotics, manufacturing, economics, natural sciences, etc. It provides a framework for designing control policies that regulate the system states, enabling them to evolve in a desired manner over time dynamically. The linear state space model is a powerful tool for representing dynamical systems, which employs a set of difference equations to describe the Markovian system state process with a linear state transition model. Many of the existing works on control of linear dynamical systems (LDSs)  are developed based on the key assumption that the system state transition model is static. However, in many real world applications, such a static dynamics assumption usually does not hold because system state transition model can be changed by control policies. For instance, in the stock market, the investment policy of well-known investors can impact the actions of the general public investors, resulting in famed investment strategy-dependent changes to the dynamics of stock prices. Another example is autonomous vehicle (AV). A deployed AV might change how the pedestrians and other neighboring cars behave, and the resulting traffic environment might be quite different from what the designers of the AV had in mind . Such interplay between decision-making and decision-dependent system dynamics is a pervasive phenomenon in a multitude of domains, including finance, transportation, and public policy, among others.

Most of the existing works on the control of linear systems with non-static state transition model are primarily focused on the additive random perturbations to the system state transition matrix,where policy optimization methods are employed to find the optimal control policy that minimizes quadratic costs [10; 1; 11; 8]. This type of problem also includes the control of linear systems with additive state-dependent noise [15; 6] or action-dependent noise [30; 4], which is equivalent to having additive perturbations on the state transition matrix or the control input gain matrix, respectively. Some other works on jump/switched linear systems formulate the variation of system model as stochastic model jumps among multiple linear modes, where the jumping law is governed by a finite time-homogeneous Markov process. However, in all these works, the changes of the system model are unrelated to the control policy.

_Performative prediction_ provides a systematic way to model the interaction between decision-making and data via decision-dependent data distribution maps . The pioneering work by  has led to a growing body of research dedicated to performative prediction problems. Most of these studies are focused on establishing the conditions for the existence and uniqueness of the performative stable point and designing learning algorithms with provable convergence to such a unique performative stable point [13; 14; 5; 3; 18; 29; 25], or algorithms to find a stationary solution of the performative risk [12; 19; 25].

An open question follows: Can the idea of performative prediction, which tackles decision-dependent data in the learning/prediction domain, be employed to address the decision-dependent dynamics in the control domain? Notice that in all the aforementioned performative prediction works [21; 13; 14; 5; 3; 18; 29; 25; 12; 19; 25], the data input to the learning algorithms are independently generated based on the decision-dependent distributions. No temporal correlation is considered or exploited between two consecutive sets of input data. However, in the context of control, the situation is different as the changing system dynamics introduce various additional complexities. This is because the data of the system state at each time the step will depend on the decision-dependent state transition matrices in all previous time steps. The expected total cost function to be optimized will depend on all the decision-dependent state transition matrices across the entire control time horizon. This implies that we need a framework of _performative control_ that is more general than the framework of performative prediction, and that can accommodate a sequence of decision-dependent data with temporal correlations, where the temporal correlations are again decision-dependent.

In this work, we provide an affirmative answer to the above question. Our idea hinges on adopting the concept of performative stable solution , which is a fixed point solution for the interplay between the controller and the system dynamics that react to the controller's decisions. The condition for the existence of such a performative stable solution is obtained by analyzing the propagation and aggregation of sensitivities associated with the distributions of policy-dependent system dynamics over the entire control time horizon. In particular, we follow existing studies [1; 11; 8] and focus on the disturbance-action policy, which allows the consideration of general convex control costs instead of only quadratic control costs.

To the best of our knowledge, this paper provides the first study and analysis of performative control of linear systems where the system dynamics are constantly changing in a control-policy-dependent manner. We highlight the following key contributions:

* We introduce the notion of performative control, where the deployed control policy affects the underlying dynamics of the control system. We provide sufficient conditions for the existence and uniqueness of the performative stable control (PSC) solution. The sufficient condition is expressed in terms of a weighted sum of all the distributional sensitivities associated with the policy-dependent system state transition matrices over the entire control horizon. An interesting finding is that the sufficient condition exhibits a structure of sensitivity propagation and aggregation, implying that it is preferable for sensitivities to be relatively small in the early stages of the system state evolution.
* We analyze the impacts of system stability on the existence and uniqueness of the PSC solution. We show that when the policy-dependent dynamics are almost surely strongly stable, the PSC solution exists if the sum of all distributional sensitivities is below a certain threshold. On the other hand, when the policy-dependent dynamics are almost surely unstable, the proposed sufficient condition for the existence of the PSC solution will place a necessary requirement on a temporally backwards decaying of the distributional sensitivities.
* We propose a repeated stochastic gradient descent (RSGD) scheme and analyze its convergence towards the PSC solution. We show that the scheme is convergent under the same sufficient condition for the existence of the PSC solution. With an appropriate step size rule, the expected squared distance between the PSC solution and the iterates decays as \((),\) where \(N\) is the iteration number.

Finally, we conduct experiments on policy-dependent stock investment risk minimization problem. The numerical results validate the effectiveness of our algorithm and theoretical analysis.

### Related Work

**Performative Prediction.** The notion of performative prediction is initiated by , where performative stability is first introduced, and a sufficient and necessary condition is provided so that the performative stable point can be reached via iterative risk minimization algorithms. Since then, there has been a growing literature analyzing the performative prediction problem and studying the convergence of learning algorithms to the performative stable point [18; 5; 3; 13; 29].There are also some other papers that find algorithms that converge to a stationary solution of the performative risk [12; 19; 25]. Our problem poses entirely new challenges because the decision-dependent data of system state and control costs also have decision-dependent temporal correlations. Such a two-layer decision-dependent structure cannot be incorporated into the existing performative prediction framework.

There are some very recent works on _performative reinforcement learning_, where a Markov decision process (MDP) is considered and the deployed policy not only changes the costs but also the underlying state transition kernel [16; 24]. However, these works only considered the tabular MDP, where the state and action space are finite. The LDS considered in our work may also be viewed as a Markov decision process with a decision-dependent linear transition kernel and decision-dependent costs. But both the state and action space are continuous and there are infinitely many admissible state-action pairs, which are beyond the scopes of [16; 24].

**Stateful Performative Prediction.** Note that most of the existing performative prediction works [21; 18; 18; 5; 13; 29; 5; 3; 13; 29] are non-stateful in the sense that, for any deployed policy \(\), the data sample \(Z\) follows a static distribution \((),\) i.e., \(Z().\) The seminal work  generalizes the stateless framework of  and proposes a more general framework of stateful performative prediction via a stateful distribution transition map \(f()\). Specifically, the observed data distribution in  is time-varying and depends on the history of previously deployed polices, i.e., \(_{t+1}=f(_{t},_{t})\). Consequently, in comparison to the conventional non-stateful performative prediction works [21; 18; 18; 5; 13; 29; 5; 3; 13; 29], this framework is capable of encapsulating the phenomenon of strategic decision-making with outdated information, and serves as a foundation for investigations of the disparate effects of performativity (please refer to Examples 1-3 in  for more details). The interconnections and generalizations between our work and the stateful performative prediction will be substantiated in Section 2.

**Nonstochastic Control.** Another line of relevant works pertains to non-stochastic control, which is initiated by . Various applications of non-stochastic control can be found in [10; 1; 11; 8]. At the core of non-stochastic control is the disturbance-action control policy, which chooses the action as a linear map of the past disturbances . Such a disturbance-action policy facilitates efficient algorithms for control problems with arbitrary additive disturbances in the dynamics and arbitrary convex control costs instead of quadratic costs only. In this paper, we adopt the disturbance-action control policy for analysis. However, in contrast to [1; 10; 1; 11; 8], where the system dynamics are static, our analysis involves policy-dependent nonstationary dynamics.

## 2 Motivations

In this section, we discuss the key motivations for our performative linear control framework. We also outline the key connections and generalizations of our proposed framework to the stateful performative prediction framework of .

**Connections between Static Stateful Distribution Transition Maps and LDS.** The LDS modeling via control theory in our work is closely aligned with the static stateful distribution maps within the state performative prediction framework . Let us first consider a static stateful distribution transition map \(f()\) with \(_{t+1}=f(_{t},_{t}), t 0.\) To facilitate the analysis, we use a performative data sample \(Z_{t}_{t}\) to equivalently characterize the performative distribution \(_{t}.\) If the properties of \(f()\) are nice enough, then from the perspective of random variables, there will exist a corresponding mapping \(()\) such that

\[Z_{t+1}}{=}(Z_{t},_{t}),Z_{t+1} _{t+1},Z_{t}_{t}, t 0, \]

where \(}{=}\) denotes equal in distribution. Linearizing \(()\) at some equilibrium point \((,)\) with \(}\) and ignoring the higher order terms will lead to

\[Z_{t+1}}{=}Z_{t}+_{t}+, \]

where \(=[}{ d}]_{( ,)}\) and \(=[}{}] _{(,)}\) are the static Jacobin matrices at the equilibrium point \((,)\) and \(=(,)--\). Consider the performative data sample \(Z_{t}\), the deployed policy \(_{t}\) and the residual \(\) in (2) as the state variable \(_{t}\), the control input action \(_{t}\) and the additive system noise \(_{t}\) of an LDS, respectively. The linearized static stateful distribution map model in (2) is thus precisely connected to a LDS with system dynamics given by

\[_{t+1}=_{t}+_{t}+_ {t}. \]

**Extension to Performative Stateful Distribution Transition Maps via Performative LDS.** Further consider a more complicated case of _performance distribution transition maps_\(f_{_{t}}()\) with \(_{t+1}=f_{_{t}}(_{t},_{t}), t  0,\) where the specific form of the distribution transition map \(f_{_{t}}\) is time-varying and depends on the deployed policy \(_{t}\). Employing a similar linearization for \(_{_{t}}()\) and neglecting the higher order terms, we obtain

\[Z_{t+1}}{=}_{_{t}}Z_{t}+_{ _{t}}_{t}+_{_{t}}, \]

where \(_{_{t}}=[_{_{t}}}{  d}]_{(,)}\) and \(_{_{t}}=[_{_{t}}}{ }]_{(,)}\) are the performative Jacobin matrices, and \(_{t}=_{_{t}}(, )-_{_{t}}-_{_{t}} {}\). This results in an equivalent performative LDS given by

\[_{t+1}=_{_{t}}_{t}+_{ _{t}}_{t}+_{_{t}}. \]

In contrast to (3), where the system dyanmics are static, the state transition matrix \(_{_{t}}\), control input gain matrix \(_{_{t}}\) and additive noise \(_{_{t}}\) in (5) are all performative and depend on the deployed control policy \(_{t}\).

In conclusion, the LDS modeling presented in our paper allows for the _performance transition maps of performative distributions_, thereby extending the technical results previously obtained for fixed performative distribution transition maps in the stateful performative prediction work . As a result, our proposed performative LDS framework has the potential to enhance the understanding of general performative prediction.

## 3 Problem Setup

We consider the control of a linear dynamic system with per stage cost \(c_{t}(_{t},_{t})\). A control policy \(\) is a mapping \(:^{d_{x} 1}^{d_{u} 1},\) which maps the system state \(_{t}\) to the control action \(_{t}\), i.e., \(_{t}=(_{t})\). For each control policy \(\), we attribute a finite time horizon expected cost defined as

\[C_{T}^{}=_{_{0},\{_{t}\},\{ _{t}\}}[_{t=0}^{T}c_{t}(_{t},_{t})], \]

where \(_{t+1}=_{t}_{t}+_{t}+ _{t}\), the initial system state \(_{0}\) follows a general distribution of \(_{x_{0}}\) with bounded support \(\|_{0}\| x_{0}\), and \(_{_{0},\{_{t}\},\{_{t }\}}\) represents the expectation over \(_{0},\) the entire policy-dependent state transition matrix sequence \(\{_{t},.\)\(.1 t T\}\) and the entire disturbance sequence \(\{_{t},1 t<T\}\).

To the best of our knowledge, this paper is the very first work to investigate the performative LDS, and we intend to provide a thorough theoretical investigation and aim at establishing various new theoretical results. Therefore, we opt to construct our performative LDS theoretical framework upon the performative state transition matrix \(_{t}\), while maintaining the control input gain matrix \(\) and additive disturbance \(_{t}\) as non-performative. This facilitates us to streamline the theoretical analysis and consolidates the system design insights.

We have the following assumption on the additive disturbances \(\{_{t},1 t T\}.\)

**A1**: _The additive disturbance per time step \(_{t}\) is bounded, i.i.d, and zero-mean with a lower bounded covariance i.e., \(_{t}_{},[_{t}]=,[_{t}_{t}^{}]^{2}\), \(\|_{t}\| W, 0 t<T\), and \([_{t_{1}}_{t_{2}}^{}]=, 0  t_{1} t_{2}<T\)._

**Disturbance-Action Control Policy.** We work with the following class of disturbance-action control policy throughout this paper, which is commonly used in nonstochastic control  to address general convex control cost functions.

**Definition 1**: _(Disturbance-Action Policy). For a disturbance-action control policy, the mapping \(\), \( 0 t<T\), is uniquely characterized by a set of matrices \(\{^{(1)},,^{(H)}\}\). At every time step \(t\), such a disturbance-action control policy assigns a control action \(_{t}^{()}\) in the form of_

\[_{t}^{()}=(_{t})=- _{t}+_{i=1}^{H}^{(i)}_{t-i}=- _{t}+[]_{t-1}^{H}, \]

_where \(=[^{(1)},^{(2)},,^{(H)}]\) belongs to a convex set \(\) with bounded support \(\|\|_{F} M\), \([]_{t-1}^{H}\) is short for \([_{t-1}^{},_{t-1-H}^{}]^{}\), \(H<T\) is a constant and \(_{i}=\) for all \(i<0\)._

In the disturbance-action policy (7), we adopt a class of linear controller \(\) defined as follows.

**Definition 2**: _(Strongly Stabilizing Linear Controller ). Given \(\) and \(\), a linear controller \(\) is \((,)\) almost surely strongly stable for real numbers \( 1,<1\), if \(\|\|\), and there exists matrices \(\) and \(\) such that \(}:=-:=^{-1}\), with \(\|\| 1-\) and \(\|\|,\|^{-1}\|\)._

It is worth noting that the disturbance-action policy is only parameterized by the matrix \(\). Whereas the state feedback gain \(\), which is a fixed matrix, is not part of the parameterization of the policy. As pointed out in , a typical choice of the parameter \(H\) is \(H=^{-1}(T^{2})\). With an appropriate choice of the policy \(\), the control action \(_{t}^{()}\) in (7) is capable of approximating any linear state feedback control policy in terms of the total cost suffered with a finite time horizon of \(H\).

**Policy-dependent Dynamics.** Without loss of generality, at any time step \(t\), the impact of the disturbance-action control policy \(_{t}^{()}\) to the dynamics of the linear system is modeled as a policy-dependent additive perturbation \(_{t}\) to a common state transition matrix \(\).

**A2**: _(Policy-dependent State Transition Matrix). The disturbance-action policy-dependent state transition matrix \(_{t}\) takes the form of_

\[_{t}=+_{t},_{t}_{t}(), 0 t<T, \]

_where \(\) is the mean value of \(_{t}\), and \(_{t}\) is the policy-dependent state transition perturbation with zero mean and bounded support, i.e., \([_{t}]\)=\(\) and there exists a bounded constant \(_{t}\) such that \(\|_{t}\|_{t}\), \( 0 t<T\). For different time steps \(t_{1}\) and \(t_{2}\), \(_{t_{1}}\) and \(_{t_{2}}\) are mutually independent. Besides, \(\{_{t},0 t<T\}\) and \(\{_{t},0 t<T\}\) are mutually independent._

**Remark 1**: _(Non-zero Mean \(_{t}\)). If the mean of the disturbance is non-zero and time-varying, i.e., \([_{t}]=_{t}\), we will have a new equivalent \(_{t}^{}=+_{t}\) with zero mean disturbances. We only need to choose a new linear controller \(_{t}^{}\) such that \(_{t}^{}-_{t}^{}\) is \((_{t},_{t})\)-strongly stabilizing. As a result, without loss of generality, we assume \(_{t}\) is zero mean throughout this paper._

**Remark 2**: _(Policy-dependent Control Input Gain Matrix \(\)). Note that under the DAP (7), the disturbance-action is given by \([]_{t-1}^{H}\), which is linear in policy \(\). Such kind of linear disturbance-action control policy has various nice theoretical performance guarantees as substantiated in . On the other hand, if \(\) is also performative, we will have a generalized disturbance-action \(_{t}()[]_{t-1}^{H}\), which can be possibly nonlinear in policy \(\). Such kind of generalized nonlinear disturbance-action control policy has received very few research attention, and it can serve as a very interesting future research direction._

We make the following sensitivity assumption on the distributions \(\{_{t}(),0 t<T\}.\)

**A3**: _(\(\)-Sensitivity). For any \(t=0,1,,T-1\), there exists a constant \(_{t}>0\) such that_

\[^{1}(_{t}(),_{t} (^{}))_{t}\|- ^{}\|_{F},\;,^{} , \]

_where \(^{1}(,^{})\) denotes the Wasserstein-1 distance between the distributions \(\) and \(^{}\)._

Assumption A3 imposes a regularity requirement on the distributions \(\{_{t}(),0 t<T\}.\) Intuitively, if the disturbance-action control policies are made according to similar policy parameterizations \(\), then the resulting distributions of the policy-dependent state transition perturbations should also be similar.

**System State Evolutions.** Under the disturbance-action policy (7), the following lemma shows that the system state \(_{t}, 1 t T,\) can be uniquely determined by \(_{0}\), \(}\), \(\), \(\{_{t},0 t<T\}\) and \(\{_{t},-H t<T\}.\)

**Lemma 1**: _Given a disturbance-action policy \(\), the system state \(_{t}\), \( 1 t T\), can be represented as_

\[_{t}=_{t}^{()}= _{i=0}^{t-1}(}+_{i} )_{0}+_{i=0}^{t-1}_{j=i+1}^{t-1}(_{j <t}(}+_{j})+_{j=t} )[]_{i-1}^{H} \] \[+_{i=0}^{t-1}_{j=i+1}^{t-1}(_{j<t}( }+_{j})+_{j=t} )_{i}.\]

_Besides, let the \(\) in DAP (7) be a strongly stabilizing linear controller, the norm of \(_{t}\) is upper bounded as \(\|_{t}\| x_{0}^{2}_{t}+^{2}W( \|\|HM+1)_{t},\) where \(_{t}=_{i=0}^{t-1}(1-+^{2}_{i})\) and \(_{t}=_{i=0}^{t-1}_{j=i+1}^{t-1}(_{j<t}(1- +^{2}_{j})+_{j=t}).\)_

The _performative optimal control (POC)_ problem can therefore be formulated as:

\[_{} C_{T}^{}=_{ _{0},\{_{t}_{t}(), \{_{t}\}[_{t=0}^{T}c_{t}(_{t}^{( )},_{t}^{()})]. \]

The _POC_ problem (11) comprises of a stochastic objective function with policy-dependent distributions. Due to non-convexity, the performative optimal solution \(^{PO}\) to (11) is usually difficult to obtain. Alternatively, in this paper, we are interested in the _performance stable control (PSC)_ solution:

\[^{PS}=(^{PS}):=_{} _{_{0},\{_{t}_{t}( ^{PS})\},\{_{t}\}}[_{t=0}^{T}c_{ t}(_{t}^{()},_{t}^{()}) ]. \]

Notice that \(^{PS}\) is defined to be a fixed point of the map \(\). Compared to the _POC_ (11), the distribution of \(_{t}\) in _PSC_ (12) changes from \(_{t}_{t}()\) to \(_{t}_{t}(^{PS}), 0 t<T.\) The existence and uniqueness of \(^{PS}\) will be dicsussed in Lemma 4.

**Comparison to Existing Works.** Most of the existing performative prediction works  consider the cost in the form of \(l(;Z),\) where \(\) is the decision variable. Then the relationship between data samples \(Z\) and decision \(\) is parameterizedby a fixed distribution \(Z()\) with fixed sensitivity \(\). However, in our work, the relationship between system state data samples \(_{t}\) and policy \(\) is characterized by a time-varying distribution \(_{t} f_{t}(_{0}(),, _{t-1}())\) with a time-varying sequence of joint sensitivities \(\{_{0},,_{t}\},\, 0 t<T\). The total cost \(_{t=0}^{T}c_{t}\) depends on all the policy-dependent distributions \(\{_{0}(),,_{T-1}( )\}\) with a collection of sensitivities \(\{_{0},,_{T-1}\}\). These key differences lead to a more complicated analysis in our work.

**RSGD Scheme.** We propose a repeated stochastic gradient descent (RSGD) scheme in Algorithm 1 to find a PSC solution to (12). The metric of the projection in line 9 of Algorithm 1 is the matrix Frobenius norm. Specifically, \(_{}\{\}=_{^{ }}\|-^{}\|_{F}^{2}\). Note that such a projection is computationally tractable because the Frobenius norm square minimization is a convex optimization problem. The RSGD scheme first computes the stochastic gradient of the total cost w.r.t. policy \(_{t=0}^{T}_{_{n}}c_{t}(_{t},_{t})\) and then perform stochastic gradient descent on \(\). The detailed steps for computation of the stochastic gradient \(_{_{n}}c_{t}(_{t},_{t})\) are provided in Appendix B.

```
1:Step sizes \(\{_{n},0 n N\}\), parameters \(,H\). Define \(=\{:\|\| M\}.\) Initialize \(_{0}\) arbitrarily.
2:for\(n=0,,N,\)do
3: Initialize \( J_{T}=\).
4:for\(t=0,,T-1,\)do
5: Use control \(_{t}=-_{t}+_{n}[]_ {t-1}^{H}.\)
6: Observe \(_{t},\,_{t+1}\); compute noise \(_{t}=_{t+1}-_{t}_{t}-_{t}\).
7: Compute the gradient \(_{_{n}}c_{t}(_{t},_{t})\) and update \( J_{T} J_{T}+_{_{n}}c_{t}(_{t},_{t}).\)
8:endfor
9: Update \(_{n+1}_{}\{_{n}-_{n}  J_{T}\}\).
10:endfor
```

**Algorithm 1** Repeated Stochastic Gradient Descent (RSGD)

## 4 Main Results

This section investigates the existence of a PSC solution \(^{PS}\) to (12) and the convergence of the RSGD scheme to \(^{PS}\). We require the following assumptions on the per stage cost \(c_{t}\).

**A4**: _(Strongly Convex). The per stage cost function \(c_{t}(,)\) is \(\)-strongly convex such that_

\[c_{t}(_{1},_{1})  c_{t}(_{2},_{2})+_{ }^{}c_{t}(_{2},_{2})( _{1}-_{2})+_{}^{}c_{t}( _{2},_{2})(_{1}-_{2})\] \[+(\|_{1}-_{2}\|^ {2}+\|_{1}-_{2}\|^{2}),_ {1},_{2}^{d_{x}},_{1},_{2} ^{d_{u}}.\]
**A5**: _(Smoothness). The per stage cost function \(c_{t}(,)\) is \(\) smooth such that_

\[\|_{}c_{t}(_{1},_{1} )-_{}c_{t}(_{2},_{1}) \|+\|_{}c_{t}(_{1},_{1} )-_{}c_{t}(_{1},_{2})\|\] \[(\|_{1}-_{2}\|+ \|_{1}-_{2}\|),_{1}, _{2}^{d_{x}},_{1},_{2}^{d_{u}}.\]
**A6**: _(Boundedness). There exists a positive constant \(G\) such that_

\[\|_{}c_{t}(,)\|, \|_{}c_{t}(,)\| GD,\|\|,\|\| D.\]

The above Assumptions A4-A6 on the per stage cost function \(c_{t}(,)\) are quite standard, which hold for broad classes of costs such as the quadratic costs.

To facilitate our discussions, we define an expected total cost with distribution shift as

\[C_{T}(;^{})_{_{0},\{_{t}_{t}(^{ })\},\{_{t}\}}[_{t=0}^{T}c_{t} (_{t}^{()},_{t}^{()}) ],\]where, under policy \(\), the distribution of policy-dependent perturbation is changed from \(_{t}_{t}()\) to \(_{t}_{t}(^{})\), \( 0 t<T\). For the rest of this paper, unless otherwisespecified, \( C_{T}(;^{})\) denote the gradients taken w.r.t. the first argument \(\).

Our main results rely on the strong convexity of the expected total cost \(C_{T}(;^{})\) with respect to its first argument \(\). However, the strong convexity of the per stage cost function \(c_{t}(,)\) over the state-action space in Assumption A4 does not by itself imply the strong convexity of the expected total cost \(C_{T}(;^{})\) over the space of policies \(\). This is becasue the policy \(\), which maps from a space of dimensionality \(H d_{x} d_{u}\) to that of \(d_{x}+d_{u}\), is not necessarily full column-rank. Our next lemma, which forms the core of our analysis, shows that this is not the case using the inherent stochastic nature of the policy-dependent dynamics.

**Lemma 2**: _Under A1-A6, fix any \(^{},\) the expected total cost \(C_{T}(;^{})\) is \(\)-strongly convex in its first argument \(\) such that \(_{1},_{2},\)_

\[C_{T}(_{1};^{}) C_{T}(_{2};^{})+(( C_{T}( _{2};^{}))^{}(_{1}- _{2}))+}{2}\|_{1}- _{2}\|_{F}^{2}, \]

_where \(=\{}{2},^{2}}{64^{10}}\}.\)_

For a concise presentation of the smoothness of expected total control cost, we next define a collection of constants as follows:

\[c_{1}  d_{x} H^{}W(1+(^{2 }+^{3})\|\|)(^{2}+^{3 })(1-)^{-1},\] \[c_{2}  d_{x}H^{}WG(1-)^{-1}( ^{4}+^{5})\|\|,c_{3}(HM \|\|+1)W,c_{4} HW(1-)c_{ 1},\] \[c_{5}  HW(^{2}+^{3})^{-1}(1- )c_{1}.\]

The smoothness of the expected total cost \(C_{T}(;^{})\) is summarized below.

**Lemma 3**: _Under A1-A6, the expected total cost \(C_{T}\) is smooth in the sense that, for any \(,^{},_{1},_{2}\) and \( 1 t T\), the following inequality holds_

\[\| C_{T}(_{1};)- C _{T}(_{2};^{})\|_{F}\] \[_{t=1}^{T}_{t}\|_{1}-_{2 }\|_{F}+_{t=0}^{T-1}(_{t}_{i=t+1}^{T}_{i} )\|-^{}\|_{F}, \]

_where \(_{t}=c_{1}(c_{4}_{t}+c_{5}),_{t}=(c_{1}+c_{2} _{t})(x_{0}_{t}+c_{3}_{t}), 1 t T,\) and we recall that \(_{t}=_{i=0}^{t-1}(1-+^{2}_{i})\) and \(_{t}=_{i=0}^{t-1}_{j=t+1}^{t-1}(_{j<t}(1- +^{2}_{j})+_{j=t})\) from Lemma 1, which characterize the growth of the norm of system state \(\|_{t}\|.\)_

**Existence and Uniqueness of \(^{PS}\).** Our first main result establishes a sufficient condition for the existence and uniqueness of the performative stable policy \(^{PS}\) that solves the _PSC_ problem (12).

**Lemma 4**: _Under A1-A6, consider the fixed-point iteration_

\[_{n+1}= (_{n}), n 0, \]

_where the map \(\) is defined in (12). If the following condition is satisfied_

\[_{t=0}^{T-1}(_{t}_{i=t+1}^{T}_{i})< {}, \]

_then iterates \(_{n}\) converge to a unique performatively stable point \(^{PS}\) at a linear rate, i.e.,_

\[\|_{n}-^{PS}\|_{F}n(1-^{T-1}(_{t}_{i=t+1}^{T}_{i} )}{})^{-1}(\|_{0}- ^{PS}\|_{F}).\]The sufficient condition (16) delivers a fact that that the existence and uniqueness of \(^{PS}\) is jointly determined by all the sensitivities \(\{_{t},0 t<T\}\) in the temporal domain. The sensitivity \(_{t}\) at the \(t\)-th time step is propagated starting from time step \(t+1\) to the last time step \(T\) with a sequence of weights \(\{_{t+1},,_{T}\}\). The aggregated impact of all the policy-dependent disturbances \(\{_{t},0 t<T\}\) is captured by total sum in L.H.S. of (16). This is very different from the existing performative prediction works , where only one distribution \(\) and one sensitivity \(\) are involved.

The sufficient condition (16) also implies that it is preferable for the initial policy-dependent disturbance \(_{0}\) to be small because it propagates and aggregates for the longest time steps of \(T\).

**Impacts of System Stability.** The sufficient condition (16) also reveals the impacts of system stability on the existence and uniqueness of the performative stable solution \(^{PS},\) which are summarized below.

**Proposition 1**: _(Almost Surely Strongly Stable Case) Let A1-A6 hold. If policy-dependent state transition matrix \(_{t}\) is \((,-^{2}_{t})\)-strongly stable for real numbers \( 1,-^{2}_{t}<1, 0 t<T\), almost surely. Let \(=\{1-+^{2}_{t}, 0 t<T\}\), then \(^{PS}\) exists and is unique if \(_{t=0}^{T-1}_{t}<1-,\) where \(=\{}{2},^{2 }}{64^{10}}\}\) and \(\) is some positive constant._

Proposition 1 points out that when the policy-dependent dynamics are almost surely strongly stable, we only need to make sure that the sum of all the sensitivities \(\{_{t},0 t<T\}\) is below a certain threshold.

**Proposition 2**: _(Almost Surely Unstable Case) Let A1-A6 hold. If the policy-dependent state transition matrix \(_{t}\) is almost surely unstable, i.e., there exists a positive constant \(>1\) such that \(\|_{t}\| 1-+^{2}_{t}, 0  t<T.\) In this case, to guarantee the sufficient condition (16) can be satifsed, we must have \(_{t}<(T-H+1)}{^{T-t-1}}, 0 t<T,\) where \(\) is some positive constant._

For unstable policy-dependent dynamics, condition (16) will impose a necessary requirement on the temporally backwards decaying of the sensitivities. Particularly, more restrictive requirements are placed on the the sensitivities in the early time steps, i.e., smallt, which should decay exponentially fast w.r.t. the control time horizon \(T\).

For more general applications, where \(_{t}\) can be either stable or unstable for different time steps, the weighted sum requirement of the sensitivities \(\{_{t},0 t<T\}\) in (16) is sufficient to guarantee the existence and uniqueness of the performative stable solution \(^{PS}\).

**Convergence of RSGD Scheme.** Our next theorem establishes the convergence rate of the proposed RSGD algorithm.

**Theorem 1**: _Choose two positive constants \(_{1}>0\) and \(_{2} 1\) such that the following two conditions are satisfied simualnteously_

\[}{_{2}}\{- _{t=0}^{T-1}(_{t}_{i=t+1}^{T}_{i})}{2(_{ t=1}^{T}_{t}+_{t=0}^{T-1}(_{t}_{i=t+1}^{T}_{i} ))^{2}},-_{t=0}^{T-1}( _{t}_{i=t+1}^{T}_{i})}\}, \] \[}{1+}} -_{t=0}^{T-1}(_{t}_{i=t+1}^{T}_{i})}. \]

_Consider a sequence of non-negative step sizes \(\{_{n}=}{n+_{2}},n 0\}\). Then, the iterates generated by RSGD admit the following bound for any \(N 1\):_

\[[\|_{N}-^{PS}\|_{F}^{2}] ^{-_{n=1}^{N}}{n}(-_{t =0}^{T-1}(_{t}_{i=t+1}^{T}_{i}))} [\|_{0}-^{PS}\|_{F}^{2}]+}{N}, \]

_where \(_{t}=^{3}G((HW+^{2})\|\|_{t}+1)(x_{0}_{t}+c_{3}_{t})+GHWM( ^{3}_{t}+1), 1 t T,\) and \(_{3}=T_{t=1}^{T}_{t}^{2}}{-_{t=0 }^{T-1}(_{t}_{i=t+1}^{T}_{i})}\) is a positive constant._

Note that, under the sufficient condition (16) in Lemma 4, there always exists a pair of \((_{1},_{2})\) that satisfies (17) and (18) simultaneously by letting \(_{2}\) be sufficiently large. The first term on the R. H. S. of (19) decays at the rate of \((^{-_{n=1}^{N}}{n}(-_{t=0}^{T-1}(_{t}_{i=t+1}^{T}_{i}))})\) and is scaled by the initial error \([\|_{0}-^{PS}\|_{F}^{2}].\) The second term is a fluctuation term that only depends on the variance of the stochastic gradient, which decays at the rate \((1/N)\). For more general types of step sizes and the associated nonasymptotic convergence rate analysis, please refer to Lemma 5 in the appendix G.1.

## 5 Numerical Experiments

We consider an application of stock investment risk minimization problem to verify our algorithm and theoretical results. Consider an investor trading a total number of 10 stocks over a period of \(T=60\) trading days. The detailed system setups are described in Example 1 in AppendixA of the supplementary. We compare the performative error (PS error) \(\|_{N}-^{PS}\|_{F}^{2}\) and the expected total cost \(C_{T}(_{N};_{N})\) against the iteration number \(N\), respectively. We consider a fixed distributional sensitivity value set \(=\{_{i},i=0,1,,T-1\}\). We assign three different patterns of the sensitivity sequence as \(_{}=()\), \(_{}=()\) and \(_{}=()\) in descending, ascending, and random order, respectively, over the time steps. We first observe from Figure 1 (left) and Figure 2 (left) that when the policy-dependent system dynamics \(_{t}, 0 t<T,\) are almost surely strongly stable, the gap \(\|_{N}-^{PS}\|_{F}^{2}\) of three different patterns \(_{},\)\(_{}\) and \(_{}\) all decay at \(()\) as \(N\), and the expected total control cost also converges. These coincide exactly with Proposition 1 and Theorem 1, where the existence of \(^{PS}\) only requires that the sum of the distributional sensitivities \(_{t=0}^{T-1}_{t}\) is small enough, and the temporal order of each \(_{t}\) is negligible. We next observe from Figure 1 (middle) and Figure 2 (middle), that when the \(_{t}, 0 t<T,\) are almost surely unstable, the iterates \(_{n}\) of both \(_{}\) and \(_{}\) diverge. The expected total control costs associated with \(_{}\) and \(_{}\) are significantly larger than that of \(_{}\). This is because large initial distributional sensitivities in \(_{}\) and \(_{}\) rule out the existence of \(^{PS}\), which matches Proposition 2. For the general case of \(_{t}\) in Figure 1 (right) and Figure 2 (right), all three cases converge due to the relatively mild sufficient condition (16).

## Conclusion

In this work, we have introduced the framework of performative control and studied the conditions under which a PSC policy exists. We have analyzed the impact of system stability on the existence of the PSC policy, and proposed a condition on the sum of the distributional sensitivities and the temporally backwards decaying of sensitivities for almost surely strongly stable and almost surely unstable systems, respectively. We have also proposed an RSGD algorithm that converges to the PSC policy in a mean-square sense. The extension of our current results to general control policies and general control costs [cf. A4], will be explored in future work.