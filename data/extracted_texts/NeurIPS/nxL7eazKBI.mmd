# Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks

Jiacong Hu\({}^{1,5}\), Jing Gao\({}^{2}\), Jingwen Ye\({}^{3}\),

Yang Gao\({}^{7}\), Xingen Wang\({}^{1,7}\), Zunlei Feng\({}^{4,5,6}\), Mingli Song\({}^{1,5,6}\)

\({}^{1}\)College of Computer Science and Technology, Zhejiang University,

\({}^{2}\) Robotics Institute, School of Computer Science, Carnegie Mellon University,

\({}^{3}\)Electrical and Computer Engineering, National University of Singapore

\({}^{4}\)School of Software Technology, Zhejiang University,

\({}^{5}\)State Key Laboratory of Blockchain and Data Security, Zhejiang University,

\({}^{6}\)Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security,

\({}^{7}\)Bangsheng Technology Co., Ltd.

jiaconghu@zju.edu.cn,jinggao2@andrew.cmu.edu,jingweny@nus.edu.sg,

{roygao,newroot,zunleifeng,brooksong}@zju.edu.cn

###### Abstract

With the rapid development of deep learning, the increasing complexity and scale of parameters make training a new model increasingly resource-intensive. In this paper, we start from the classic convolutional neural network (CNN) and explore a paradigm that does not require training to obtain new models. Similar to the birth of CNN inspired by receptive fields in the biological visual system, we draw inspiration from the information subsystem pathways in the biological visual system and propose Model Disassembling and Assembling (MDA). During model disassembling, we introduce the concept of relative contribution and propose a component locating technique to extract task-aware components from trained CNN classifiers. For model assembling, we present the alignment padding strategy and parameter scaling strategy to construct a new model tailored for a specific task, utilizing the disassembled task-aware components. The entire process is akin to playing with LEGO bricks, enabling arbitrary assembly of new models, and providing a novel perspective for model creation and reuse. Extensive experiments showcase that task-aware components disassembled from CNN classifiers or new models assembled using these components closely match or even surpass the performance of the baseline, demonstrating its promising results for model reuse. Furthermore, MDA exhibits diverse potential applications, with comprehensive experiments exploring model decision route analysis, model compression, knowledge distillation, and more. For more information, please visit https://model-lego.github.io/.

+
Footnote â€ : * Corresponding author.

## 1 Introduction

Convolutional Neural Networks (CNNs), as the predominant architecture in deep learning, play a crucial role in image, video, and audio processing . CNNs were originally inspired by the concept of receptive fields in the biological visual system , and our focus is to explore and leverage similar characteristics within CNNs. Various studies  have delved into unraveling the intricacies of biological visual information processing systems. Notably, Livingstone et al.  substantiated that the intermediate visual cortex comprises relatively independent subdivisions.

Some studies [8; 9; 10; 11; 12; 13; 14] have endeavored to visualize critical layers and neurons within CNNs. These visualizations demonstrate that in the more shallow layers of CNNs, form, color, texture, and edge features are processed by distinct convolutional kernels, whereas deeper layers are responsible for category-aware features. The above phenomena of feature processing within CNNs demonstrates the similarity to the parallel processing mechanism [5; 6] and the information integration theory  proposed in biological visual systems. Consequently, this paper is dedicated to the exploration and practical utilization of these distinct subsystems within CNNs.

In pursuit of this exploration, we introduce a pioneering task named Model Dissassembling and Assembling (MDA), a novel approach to construct and combine subsystems with LEGO-like flexibility. The conceptual framework behind MDA posits that, much like assembling and disassembling LEGO structures, deep learning models can undergo such operations without incurring significant training overhead or compromising performance. This task is designed to be universally applicable, spanning various existing Deep Neural Networks (DNNs), such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), Transformers, and others.

However, constructing the MDA framework presents several challenges, notably in determining the minimal disassembling unit and devising an assembly process with minimal impact on performance. Existing works either require the predefinition of task units during the initial training stage, or the disassembled unit cannot be directly used for inference or assembly [15; 16; 17]. In contrast, our approach distinguishes itself as the inaugural attempt to directly disassemble a trained network into task-aware components, avoiding the need for additional networks or fine-tuning. This ensures efficiency and interpretability throughout the disassembling and assembling processes.

In this paper, we illustrate our approach using CNN classifiers, with the assurance that its applicability extends to other Deep Neural Network (DNN) architectures. In the disassembling phase, we define the task-aware component by introducing the concept of relative contribution and a mechanism for contribution aggregation and allocation. This is seamlessly applied throughout the forward propagation process of the network. Building on these principles, we introduce a component locating technique that discerns and extracts task-aware components. In the assembling phase, we propose a simple yet effective alignment padding strategy. This involves padding empty kernels onto each convolutional filter to ensure uniform kernel counts across all filters in each layer. Additionally, to account for varying feature magnitudes across different components, we implement a parameter scaling strategy. The resulting MDA framework facilitates recombination among different pre-trained models, providing a vital technique for model reuse.

Our contributions in this study are summarized as follows:

* We introduce a novel task, MDA, which aims to disassemble and assemble deep models in an interpretable manner reminiscent of playing with LEGOs. This task is motivated by the subdivision of the biological visual system .
* We present the inaugural method for MDA, specifically applied to CNN classifiers. In the model disassembling phase, we introduce a component locating technique to disassemble task-aware components from the models. For model assembling, we propose an alignment padding strategy and a parameter scaling strategy to assemble task-aware components into a new model.
* Extensive experiments validate the efficacy of our proposed MDA method, showing that the performance of the disassembled and assembled models closely matches or even surpasses that of the baseline models.
* MDA introduces a fresh perspective for model reuse. Additionally, we explore diverse applications of MDA, including decision route analysis, model compression, knowledge distillation, and more.

## 2 Model Disassembling and Assembling

In this section, we present the definition of Model Disassembling and Assembling (MDA). Let us consider a set of \(N\) pre-trained deep learning models denoted as \(\{^{(n)}\}_{n=1}^{N}\). Each model \(^{(n)}\) comprises \(K^{(n)}\) subtasks, represented as \(\{t_{k}^{(n)}\}_{k=1}^{K^{(n)}}\). Our objective in model disassembling is to extract the model components \([t_{k}^{(n)}]\) corresponding to a specific subtask \(t_{k}^{(n)}\) from the source model \(^{(n)}\). These extracted components \([t_{k}^{(n)}]\) are intended to encapsulate only the parameters critically relevant to the subtask \(t_{k}^{(n)}\). In essence, the disassembled components \([t_{k}^{(n)}]\) should function as an independent model, preserving the full capability for the subtask \(t_{k}^{(n)}\) without redundant capability for other subtasks. Furthermore, the assembling process involves combining disassembled components from different models. For example, combining components \([t_{k_{1}}^{(n_{1})},,t_{k_{2}}^{(n_{1})}]\) from \(^{(n_{1})}\) and \([t_{k_{3}}^{(n_{2})},,t_{k_{4}}^{(n_{2})}]\) from \(^{(n_{2})}\) results in a new model \(^{(new)}\). This assembled model \(^{(new)}=[t_{k_{1}}^{(n_{1})},,t_{k_{2}}^{(n_{1}) },t_{k_{3}}^{(n_{2})},,t_{k_{4}}^{(n_{2})}]\) is expected to retain full functionality for the subtasks \(\{t_{k_{1}}^{(n_{1})},,t_{k_{2}}^{(n_{1})}\}\) and \(\{t_{k_{3}}^{(n_{2})},,t_{k_{4}}^{(n_{2})}\}\).

MDA is applicable to various existing deep learning architectures, including Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformers. The focus of this paper is to explore the implementation and efficacy of MDA specifically in the context of CNN classifiers.

## 3 Model Disassembbling

### Contribution Aggregation and Allocation

Given a CNN classifier with \(K\) categories, each category is treated as an individual subtask within the classifier. Conventionally, a Softmax operation is employed on the output features of the final fully connected layer, transforming these features into a probabilistic distribution that sums to unity. The feature with a relatively higher value corresponds to a higher probability, and the category with the highest probability serves as the predicted result of the classifier. Consequently, the feature exhibiting relatively greater magnitude plays a decisive role in determining the final classification result. We term the relative degree to which features influence the result as _relative contribution_. It is crucial to note that the concept of relative contribution is not confined solely to the final layer of the network. Features from preceding layers play a pivotal role in shaping the features of subsequent layers. Consequently, we extend the concept of relative contribution to encompass all layers of the CNN classifier, thereby establishing a comprehensive contribution system that spans the entirety of the network.

To illustrate this process more specifically, we focus on the \(l\)-th convolutional layer of a CNN (the case of the fully connected layer is discussed in Appendix A). The \(l\)-th layer has \(\) input channels and \(\) output channels. Correspondingly, the \(l\)-th layer comprises \(\) convolution filters, denoted as \(\{_{q}^{(l)}\}_{q=1}^{}\). Each filter \(_{q}^{(l)}\) consists of \(\) convolution kernels, thus \(_{q}^{(l)}=\{c_{q,p}^{(l)}\}_{p=1}^{}\). The input feature maps for the \(l\)-th layer are represented as \(\{a_{p}^{(l)}\}_{p=1}^{}\). With the convolution filter \(_{q}^{(l)}\), the hidden feature maps \(_{q}^{(l)}\) for the \(q\)-th channel in the \(l\)-th layer are computed as follows:

\[_{q}^{(l)}=\{a_{q,p}^{(l)}\}_{p=1}^{}a_{q,p}^{(l)}=c_{q,p}^{(l)} a_{p}^{(l)}\] (1)

where \(\) denotes the convolution operation. From these hidden feature maps \(_{q}^{(l)}\), the output feature map \(a_{q}^{(l+1)}\) in the \(l\)-th layer (which serves as the input feature map \(a_{q}^{(l+1)}\) in the \((l+1)\)-th layer) is

Figure 1: Disassembling process at the \(l\)-th layer of a CNN model, where the red solid line represents the contribution aggregation process, and the black dashed line represents the contribution allocation process.

determined as:

\[a_{q}^{(l+1)}=_{p=1}^{}a_{q,p}^{(l)}+b_{q}^{(l)},\] (2)

where \(b_{q}^{(l)}\) is the bias for the \(q\)-th channel. From Eqn.(2), it is evident that the output feature map \(a_{q}^{(l+1)}\) is the summation of the hidden feature maps \(_{q}^{(l)}\). This implies that the value of \(a_{q}^{(l+1)}\) is influenced by the value of each individual hidden feature map \(a_{q,p}^{(l)}\). Similar to the Softmax operation in the final layer, the larger the individual hidden feature map, the greater its contribution to the output \(a_{q}^{(l+1)}\).

#### 3.1.1 Contribution Aggregation

The red solid line in Fig. 1 represents the contribution aggregation process. In this process, the contributions from the input feature maps \(\{a_{p}^{(l)}\}_{p=1}^{}\) are aggregated to the hidden feature maps \(_{q}^{(l)}\) via the convolution filter \(_{q}^{(l)}\) of the \(q\)-th channel. To quantify the contribution of each hidden feature map \(a_{q,p}^{(l)}\) to the output \(a_{q}^{(l+1)}\), we introduce a metric \(s_{q,p}^{(l)}\) defined as follows:

\[s_{q,p}^{(l)}=_{h=1}^{H^{(l)}}_{w=1}^{W^{(l)}}a_{q,p}^{(l)}[h,w],\] (3)

where \(H^{(l)}\) and \(W^{(l)}\) denote the height and width, respectively, of the feature map \(a_{q,p}^{(l)}\). The term \(a_{q,p}^{(l)}[h,w]\) represents the pixel value at the \(h\)-th row and \(w\)-th column of \(a_{q,p}^{(l)}\). Considering the presence of activation functions such as ReLU, negative contributions are treated as having zero impact on the result. Consequently, the contribution \(_{q,p}^{(l)}\) of the hidden feature map \(a_{q,p}^{(l)}\) is recalculated as:

\[_{q,p}^{(l)}=(s_{q,p}^{(l)},0).\] (4)

In line with the principle of the Softmax operation, the contribution of each hidden feature map is relative. Hence, we employ min-max normalization to obtain the relative contribution \(r_{q,p}^{(l)}\) of each hidden feature map \(a_{q,p}^{(l)}\):

\[r_{q,p}^{(l)}=_{q,p}^{(l)}-(\{_{q,p}^{(l)}\}_{p=1}^{ })}{(\{_{q,p}^{(l)}\}_{p=1}^{})-(\{_{q,p}^{( l)}\}_{p=1}^{})+},\] (5)

where \(\) is a small constant added to prevent division by zero. This normalization process ensures that the contributions are scaled relative to each other, facilitating the identification of the most influential hidden feature maps in the layer.

#### 3.1.2 Contribution Allocation

The black dashed line in Fig. 1 represents the process of contribution allocation, where the contribution from the feature map \(a_{p}^{(l)}\) is allocated to various hidden feature maps \(\{a_{q,p}^{(l)}\}_{q=1}^{}\) through different convolution kernels \(\{c_{q,p}^{(l)}\}_{q=1}^{}\). Consequently, the overall contribution \(s_{p}^{(l)}\) of the feature map \(a_{p}^{(l)}\) is calculated using the following equation:

\[s_{p}^{(l)}=_{q=1}^{}r_{q,p}^{(l)}.\] (6)

In this calculation, similar to the previous steps, negative contributions are considered as zero:

\[_{p}^{(l)}=(s_{p}^{(l)},0).\] (7)

Moreover, acknowledging that the significance of each feature map is relative, the relative contribution \(r_{p}^{(l)}\) of the feature map \(a_{p}^{(l)}\) is computed as follows:

\[r_{p}^{(l)}=_{p}^{(l)}-(\{_{p}^{(l)}\}_{p=1}^{}) }{(\{s_{p}^{(l)}\}_{p=1}^{})-(\{_{p}^{(l)}\}_{p=1}^{ })+}.\] (8)

### Component Locating Technique

Building upon the concept of relative contribution and the mechanism of contribution aggregation and allocation, we introduce a novel approach termed the component locating technique. This method is designed to identify task-aware components, that is, the parameters most relevant to a given task, within a CNN. Taking the \(l\)-th layer as an example, the initial stage of this technique involves identifying the most relevant feature maps for the targeted task. Subsequently, the next step is to pinpoint the most relevant parameters by discerning which parameters are linked to the identified most relevant feature maps. For a specific task, please refer to Appendix B.

#### 3.2.1 Relevant Feature Identifying

In essence, the most relevant feature maps are those which exhibit a relatively larger contribution to the result of the model. In the process of contribution aggregation, a threshold value denoted as \(\) is employed to discern whether a relative contribution \(r^{(l)}_{q,p}\) (calculated in Eqn. 5) is large or small:

\[^{(l)}_{q,p}=1&r^{(l)}_{q,p}\\ 0&r^{(l)}_{q,p}<,\] (9)

where \(\) is a chosen value within the range (0,1[]). Through the above equation, the soft relative contribution \(r^{(l)}_{q,p}\), which are continuous values indicating the degree of contribution, are transformed into hard relative contribution \(^{(l)}_{q,p}\). The \(s^{(l)}_{p}\) in Eqn. 6 is now the sum over the hard relative contribution \(^{(l)}_{q,p}\). Eqn.7 and Eqn.8 remain unchanged.

Further, a second threshold value \(\) is used to determine whether the relative contribution \(r^{(l)}_{p}\) (as defined in Eqn.8) of a feature map is large or small:

\[^{(l)}_{p}=1&r^{(l)}_{p}\\ 0&r^{(l)}_{p}<,\] (10)

where \(\) is a chosen value within the range (0,1[]). This equation transforms the soft relative contribution \(r^{(l)}_{p}\) into hard relative contribution \(^{(l)}_{p}\), just as in the case of \(^{(l)}_{q,p}\).

#### 3.2.2 Parameter Linking

In Eqn. 10, the hard relative contribution \(^{(l)}_{p}\), being either 0 or 1, indicates whether the feature map \(a^{(l)}_{p}\) is most relevant to the predicted result. Similarly, the hard relative contribution \(^{(l+1)}_{q}\) can also reflect whether the feature map \(a^{(l+1)}_{q}\) is most relevant to the predicted result. Integrating Eqn.1 with Eqn.2, we can represent the convolutional process as follows:

\[a^{(l+1)}_{q}=_{p=1}^{}c^{(l)}_{q,p} a^{(l)}_{p}+b^{ (l)}_{q},\] (11)

This equation signifies that the output feature map \(a^{(l+1)}_{q}\) is generated by the convolution operation of the input feature maps \(\{a^{(l)}_{p}\}_{p=1}^{}\) with the respective convolution kernels \(\{c^{(l)}_{q,p}\}_{p=1}^{}\), which collectively form the convolution filter \(^{(l)}_{q}\). Thus, if the output feature map \(a^{(l+1)}_{q}\) is determined to be most relevant to the predicted result, then it logically follows that the convolution filter \(^{(l)}_{q}\) and the associated bias \(b^{(l)}_{q}\) are also most relevant to the predicted result. Furthermore, each input feature map \(a^{(l)}_{p}\) engages in the convolution operation exclusively with the kernels \(\{c^{(l)}_{q,p}\}_{q=1}^{}\) across different convolution filters. Therefore, if the feature map \(a^{(l)}_{p}\) is identified as most relevant to the predicted result, then the convolution kernels \(\{c^{(l)}_{q,p}\}_{q=1}^{}\) associated with it are also deemed most relevant to the predicted result. The component locating technique is also applicable to fully connected layers, enabling the identification of the most relevant filters, kernels, and biases.

In summary, through the component locating technique, we can effectively identify and discriminate the most relevant components to a specific task. This includes pinpointing the most relevant filters, kernels, and biases. Subsequently, the identified parameters most relevant to a specific task can be extracted from the source model through a process known as structure pruning .

Model Assembling

In the process of CNN model assembling, our objective is to combine disassembled, task-aware components, typically derived from different source models, into a new model. This assembling is performed without necessitating retraining or incurring performance loss. To achieve this goal, we propose an alignment padding strategy and a parameter scaling strategy. It is important to note that the focus of this paper is specifically on assembling different models that share isomorphic network architectures.

### Alignment Padding Strategy

In the assembling of CNN models, we combine models layer by layer along the dimension of the filters. A challenge in this process is that filters from different models may have varying numbers of kernels. To address this, we introduce a simple yet effective alignment padding strategy. This strategy involves padding empty kernels to each filter, ensuring that all filters in a given layer have a uniform number of kernels.

Consider the \(l\)-th layer as an example. As depicted in Fig.2(a), a disassembled model consists of \(\) convolution filters, denoted as \(\{_{j}^{(l)}\}_{j=1}^{}\). Each convolution filter \(_{j}^{(l)}\) comprises \(\) convolutional kernels, expressed as \(_{j}^{(l)}=\{c_{j,i}^{(l)}\}_{i=1}^{}\). Another disassembled model, shown in Fig.2(b), contains \(\) convolution filters \(\{_{v}^{(l)}\}_{v=1}^{}\), with each filter \(_{v}^{(l)}\) consisting of \(\) convolutional kernels, expressed as \(_{v}^{(l)}=\{c_{v,u}^{(l)}\}_{u=1}^{}\). In the assembling process, illustrated in Fig.2(c), these two models are merged into a new model with \(+\) convolution filters. Each filter in this new model is augmented with a complementary number of empty kernels. Specifically, if a convolution filter \(}_{j}^{(l)}\) originates from \(_{j}^{(l)}\), it is padded with \(\) empty kernels at the end, forming \(}_{j}^{(l)}=\{c_{j,1}^{(l)},c_{j,2}^{(l)},,c_{j,}^{ (l)},0_{1},0_{2},,0_{}\}\). Conversely, if a convolution filter \(}_{v}^{(l)}\) comes from \(_{v}^{(l)}\), it is padded with \(\) empty kernels at the beginning, resulting in \(}_{v}^{(l)}=\{0_{1},0_{2},,0_{},c_{v,1}^{(l)},c_{v,2 }^{(l)},,c_{v,}^{(l)}\}\). Through this alignment padding strategy, every convolution filter in the assembled model is standardized to have the same total of \(+\) convolution kernels.

The alignment padding strategy can be readily extended to incorporate multiple disassembled models and applied across all layers of a CNN. A key feature of this approach is that the assembled model is ready for inference immediately, without the need for any retraining.

### Parameter Scaling Strategy

In addition to the alignment padding strategy, we address another critical issue in model assembling: the potential disparity in the magnitude of features output by disassembled components from different source models. If left unaddressed, this disparity could cause the assembled model to be biased towards the disassembled components with larger feature magnitudes, impacting the balance and effectiveness of the assembled model. To resolve this, we propose a parameter scaling strategy.

Taking the \(l\)-th layer as an example, as shown in Fig.2(a) and (b), let's consider the output feature maps in the \(l\)-th layer of the disassembled models, denoted as \(\{a_{j}^{(l+1)}\}_{j=1}^{}\) and \(\{a_{v}^{(l+1)}\}_{v=1}^{}\), respectively. The magnitude of each feature map \(a_{j}^{(l+1)}\) is quantified as follows:

\[e_{j}^{(l+1)}=_{h=1}^{H^{(l+1)}}_{w=1}^{W^{(l+1)}}a_{j}^{(l+1)}[h,w],\] (12)

Figure 2: Assembling process at the \(l\)-th layer of CNN models: **(a)** and **(b)** represent two distinct disassembled models, respectively; **(c)** illustrates the assembled model.

Similarly, the magnitude \(e_{v}^{(l+1)}\) for feature map \(a_{v}^{(l+1)}\) is calculated using the same equation. We then compute the average magnitude of the feature maps from all disassembled models in the \(l\)-th layer:

\[^{(l+1)}=+}(_{j=1}^{}e_{j}^{(l+1)}+ _{v=1}^{}e_{v}^{(l+1)}).\] (13)

Then, the convolution filter \(_{j}^{(l)}\) is scaled as follows:

\[}_{j}^{(l)}=(^{(l+1)}/e_{j}^{(l+1)})_{j} ^{(l)}.\] (14)

In practical applications, this parameter scaling strategy is particularly crucial in the last fully connected layer to ensure that the magnitude differences in the final outputs of disassembled models from different source models are not excessively large.

## 5 Experiments

### Experimental Settings

**Dataset and Network.** We select three datasets and three mainstream CNN classifiers to evaluate our MDA method. The datasets include CIFAR-10 , CIFAR-100 , and Tiny-ImageNet . The chosen CNN classifiers are VGG-16 , ResNet-50 , and GoogleNet .

**Parameter Configuration.** In our MDA method, the key parameters are \(\) and \(\), as defined in Eqn.9 and Eqn.10, respectively. By default, we set \(=0.3\) and \(=0.2\) in convolutional layers, and \(=0.4\) and \(=0.3\) in fully connected layers, unless specified otherwise. The model training is conducted using the SGD optimizer, with a learning rate of \(0.01\). To ensure the reliability and reproducibility of our results, we report the average of three independent experimental runs for each result. Comprehensive details and the _source code_ can be found in the _Supplementary Material_.

### MDA Applied to CNN Models

#### 5.2.1 Model Disassembling Results

We present the results of model disassembling in Table 1. The results in Table 1 reveal that both single-task and multi-task disassembling, as executed by our method (denoted as 'Disa.'), exhibit accuracies comparable to, or even surpassing, those of the source model (denoted as 'Base.'). Notably, disassembling single tasks '0' or '1' from CIFAR-10 when using GoogleNet achieved a 100% accuracy rate. Similarly, the accuracy for disassembling multiple tasks '70-169' from Tiny-ImageNet on ResNet-50 showed an improvement of over 2.15% compared to the source model. What's more, to

    &  &  &  &  \\   & & & Base. (\%) & & & Base. (\%) & & \\   & 0 & 94.40 & 100.00 (+_5.60_) & 96.10 & 100.00 (+_3.90_) & 95.40 & 100.00 (+_4.60_) \\  & 1 & 96.50 & 100.00 (+_3.50_) & 96.20 & 100.00 (+_3.80_) & 97.40 & 100.00 (+_2.60_) \\  & 0-2 & 93.87 & 95.47 (+_1.60_) & 94.93 & 97.43 (+_2.50_) & 94.47 & 98.17 (+_3.70_) \\  & 3-9 & 92.49 & 22.72 (-_0.22_) & 93.81 & 94.46 (+_0.65_) & 93.46 & 93.17 (-_0.29_) \\  & 0â€™ & 84.00 & 100.00 (+_16.00_) & 92.00 & 100.00 (+_3.00_) & 93.00 & 100.00 (+_100.00_) \\  & 1 & 87.00 & 100.00 (+_13.00_) & 87.00 & 100.00 (+_13.00_) & 85.00 & 100.00 (+_15.00_) \\  & 0-19 & 71.05 & 82.50 (+_11.45_) & 75.55 & 77.15 (+_1.60_) & 75.90 & 87.55 (+_11.65_) \\  & 20-69 & 72.74 & 79.66 (+_6.92_) & 77.68 & 79.72 (+_2.04_) & 76.60 & 82.42 (+_5.82_) \\  & 0â€™ & 82.00 & 100.00 (+_13.00_) & 92.00 & 100.00 (+_3.60_) & 88.00 & 100.00 (+_12.00_) \\ Tiny-ImageNet & 1 & 70.00 & 100.00 (+_3.00_) & 80.00 & 100.00 (+_20.00_) & 76.00 & 100.00 (+_24.00_) \\  & 0-69 & 50.17 & 55.49 (+_5.32_) & 56.06 & 56.40 (+_0.34_) & 52.89 & 59.40 (+_6.51_) \\  & 70-179 & 45.36 & 47.95 (+_2.59_) & 51.27 & 53.42 (+_2.15_) & 47.91 & 51.04 (+_3.13_) \\   

Table 1: Comparison of disassembling performance. In the classifier, each category corresponds to a task. â€˜Base.â€™ refers to the average accuracy for â€˜Disassembled Taskâ€™ in the source model. In â€˜Disa.â€™, â€˜_Score1_ (+_Score2_)â€™ represents two metrics: â€˜_Score1_â€™ is the accuracy and â€˜_Score2_â€™ is the improved accuracy compared to â€˜Base.â€™ for the â€˜Disassembled Taskâ€™ in the disassembled model.

go deeper into the performance of our proposed disassembled method, we present the disassembling results on ImageNet  and the comparison of parameter size and Floating Point Operations Per Second (FLOPs) in the _Supplementary Material_.

#### 5.2.2 Model Assembling Results

The performance of model assembling across different datasets is presented in Table 2. It is observed that the assembled models generally achieve comparable performance to the source models in both single-task and multi-task assembling settings. Notably, the assembled models combining '0-2 + 0-69' from 'CIFAR-10 + Tiny-ImageNet' on GoogleNet surpass the source model in terms of accuracy. However, there are instances, such as with '20-69 + 70-179' from 'CIFAR-100 + Tiny-ImageNet' on ResNet-50, where a decrease in accuracy is noted. This could be attributed to the interaction and interference among the numerous parameters from the different models being assembled, particularly when the number of tasks is large, leading to less stable predictions in the new model.

### MDA Applied to GCN Model

The proposed MDA method extends beyond CNN models and is equally applicable to Graph Convolutional Network (GCN) models . We demonstrate this by conducting a disassembling experiment for node classification using a GCN model  on the Cora dataset . The results of this experiment are presented in Table 3.

The results clearly indicate that the accuracy of the disassembled GCN model surpasses that of the source model. For instance, the accuracy of the disassembled model for categories '1-2' shows an improvement of \(1.47\%\) over the source GCN model.

### Ablation Study

**Parameters \(\) and \(\).** Fig. 3 presents an ablation study on the thresholds \(\) and \(\) in the fully connected layer and convolutional layer. With the increase of the thresholds \(\) and \(\), fewer parameters will be regarded as relevant to the specific tasks. Therefore, in Fig. 3(a), we observe that as the

    &  &  &  &  \\   & & & Base. (\%) & Asse. (\%) & Base. (\%) & Asse. (\%) & Base. (\%) & Asse. (\%) \\   & 0 + 0 & 89.20 & 87.00 / 88.43 & 94.05 & 77.30 / 87.36 & 99.60 & 94.25 / 95.27 \\  & 0-2 + 0-19 & 74.03 & 74.17 / 74.19 & 78.08 & 64.34 / 76.37 & 78.32 & 79.22 / 79.22 \\  & 3-9 + 20-69 & 75.16 & 73.72 / 74.25 & 79.66 & 72.03 / 75.25 & 78.67 & 70.24 / 76.37 \\  & 0-9 + 20-99 & 74.87 & 72.07 / 73.18 & 79.54 & 65.97 / 74.65 & 78.57 & 66.59 / 70.36 \\  & 0 + 0 & 88.20 & 94.70 / 90.72 & 94.05 & 86.95 / 94.51 & 91.70 & 62.40 / 80.34 \\  & 0 + 0-2 + 0-69 & 51.97 & 53.20 / 53.20 & 57.65 & 43.09 / 56.38 & 54.59 / 57.51 / 57.81 \\  & 3-9 + 0-69 & 54.02 & 50.20 / 52.48 & 59.49 & 52.14 / 58.63 & 56.57 & 53.74 / 55.52 \\ Tiny-ImageNet & 0-9 + 70-179 & 49.33 & 42.30 / 47.98 & 54.85 & 47.21 / 55.17 & 51.73 & 48.00 / 52.16 \\  & 0 + 0 & 83.00 & 50.00 / 76.28 & 92.00 & 53.00 / 87.34 & 89.00 & 50.00 / 85.28 \\  & 0-19 + 0-69 & 69.86 & 50.66 / 57.19 & 74.59 & 57.86 / 69.23 & 74.73 & 58.67 / 69.14 \\  & 20-69 + 70-179 & 71.48 & 50.08 / 65.71 & 76.54 & 56.53 / 69.79 & 75.08 & 54.09 / 71.27 \\  & 0-99 + 0-199 & 55.97 & 43.06 / 56.13 & 61.51 & 53.05 / 57.23 & 59.19 & 48.66 / 58.28 \\   

Table 2: Comparison of assembling performance. â€˜Base.â€™ indicates the average accuracy for the â€˜Assembled Taskâ€™ in the source models. In â€˜Asse.â€™, â€˜_Score1 / Score2_â€™ represent the average accuracy scores for the â€˜Assembled Taskâ€™ in the assembled models without fine-tuning and with ten epochs of fine-tuning, respectively.

    &  &  \\   & & Task & Base. (\%) & Disa. (\%) \\   & 0 & 72.36 & 100.00 (_+27.36_) \\  & 1 & 78.54 & 97.10 (_+18.56_) \\   & 1-2 & 88.27 & 89.74 (_+1.47_) \\   & 3-5 & 80.23 & 81.34 (_+1.11_) \\   

Table 3: Performance of the GCN model disassembling on the Cora Dataset. â€˜Base.â€™ represents the average accuracy for the â€˜Disassembled Taskâ€™ in the source model. In â€˜Disa.â€™, â€˜Score1 (_+Score2_)â€™ indicates the accuracy and the improvement in accuracy (â€˜_Score2_â€™) compared to â€˜Base.â€™ for the â€˜Disassembled Taskâ€™ of the disassembled model.

[MISSING_PAGE_FAIL:9]

conditions for equivalence or approximation. While these model explanation techniques are primarily employed to locate important features in the original input contributing to the final prediction, our focus is distinct. We aim to identify task-aware components, namely the relevant parameters for specific tasks, for model disassembling.

### Subnetwork Identification

Subnetwork identification approaches can be divided into ante-hoc and post-hoc techniques. Antechoc techniques, such as those proposed by Li et al.  and Liang et al. , incorporate novel architectural control modules to select specific filters or employ category-specific gating during training, mainly for network interpretation and adversarial sample detection. Post-hoc techniques are further subdivided. The first family requires additional learnable modules and extra training steps. For instance, Hu et al.  introduced Neural Architecture Disentanglement (NAD) for disentangling pre-trained DNNs into task-specific sub-architectures, while Wang et al.  and Frankle et al.  focused on data routing paths and network acceleration, respectively. Furthermore, Yu et al.  and Yang et al.  used knowledge distillation to dissect and reassemble models. The second family aligns more closely with feature attribution, with techniques such as those of Khakzar et al.  employing concepts similar to perturbation-based methods for pathway selection. In summary, while existing subnetwork identification techniques are commonly applied for network interpretation and adversarial sample detection, our work centers on MDA. We focus on disassembling task-aware components from trained CNN classifiers and reassembling them into a new model, akin to playing with LEGOs, without requiring additional training.

## 7 Limitation and Future Work

Our experiments, as detailed in Table 1 of the main text, demonstrate that models disassembled using the proposed method can surpass the source model in terms of accuracy. However, the performance of the assembled models, as shown in Table 2 of the main text, indicates a decrease in accuracy in certain cases (e.g., '0-19 + 0-69', '20-69 + 70-179', '0-99 + 0-199' for 'CIFAR-100 + Tiny-ImageNet'). This decline in performance could be attributed to the interference of irrelevant components, which may adversely affect the correct prediction of samples. Looking ahead, our research will concentrate on addressing the disturbance caused by irrelevant components and enhancing the effectiveness of our model disassembling and assembling technique, particularly for targeted tasks. Additionally, while this paper has focused exclusively on CNN classifiers, future research will explore the disassembling and assembling of models in other domains, including object detection and segmentation.

## 8 Conclusion

In this paper, we introduce a novel Model Disassembling and Assembling (MDA) task, inspired by the subdivision of the visual system , with the objective of disassembling and assembling deep models in a manner akin to playing with LEGOs. The primary focus of this paper centers on the application of MDA to CNN classifiers. During model disassembling, we introduce the concept of relative contribution and propose a component locating technique to extract task-aware components from trained CNN classifiers. For model assembling, we introduce the alignment padding strategy and parameter scaling strategy to construct a new model tailored for a specific task using the disassembled task-aware components. Extensive experiments conducted in this study reveal that the performance of the disassembled and assembled models closely aligns with or even surpasses that of the baseline models. In addition to offering a fresh perspective for model reuse, our research extends to the diverse applications of MDA, including decision route analysis, model compression, knowledge distillation, and more. In future work, we will focus on the MDA applied to other models, such as multi-modal models, large language models etc.