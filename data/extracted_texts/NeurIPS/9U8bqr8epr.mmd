# EMBERSim: A Large-Scale Databank for

Boosting Similarity Search in Malware Analysis

 Dragos Georgian Corlatescu

CrowdStrike

dragos.corlatescu@crowdstrike.com &Alexandru Dinu

CrowdStrike

alexandru.dinu@crowdstrike.com &Mihaela Gaman

CrowdStrike

mihaela.gaman@crowdstrike.com &Paul Sumedrea

CrowdStrike

paul.sumedrea@crowdstrike.com

###### Abstract

In recent years there has been a shift from heuristics-based malware detection towards machine learning, which proves to be more robust in the current heavily adversarial threat landscape. While we acknowledge machine learning to be better equipped to mine for patterns in the increasingly high amounts of similar-looking files, we also note a remarkable scarcity of the data available for similarity-targeted research. Moreover, we observe that the focus in the few related works falls on quantifying similarity in malware, often overlooking the clean data. This one-sided quantification is especially dangerous in the context of detection bypass. We propose to address the deficiencies in the space of similarity research on binary files, starting from EMBER -- one of the largest malware classification data sets. We enhance EMBER with similarity information as well as malware class tags, to enable further research in the similarity space. Our contribution is threefold: (1) we publish EMBERSim, an augmented version of EMBER, that includes similarity-informed tags; (2) we enrich EMBERSim with automatically determined malware class tags using the open-source tool AVClass on VirusTotal data and (3) we describe and share the implementation for our class scoring technique and leaf similarity method.

## 1 Introduction

Malware is employed as a cyber weapon to attack both companies as well as standalone users, having severe effects such as unauthorized access, denial of service, data corruption, and identity theft . From signature-based malware detection , to more sophisticated machine learning (ML) techniques , antivirus (AV) became a necessary layer of defense in this cyber-war. With unprecedented access to open-source tools that facilitate replicating existing malware, the number of similar malicious samples is constantly on the rise . This poses a problem both from the perspective of Threat Researchers having to analyze more data each year, as well as for Data Scientists training ML models for malware detection. Including near-duplicate samples in new iterations of malware detectors is dangerous as it can introduce biases towards certain types of attacks. Thus, we can argue that being able to perform similarity search at scale is of utmost importance for the Cybersecurity world . Given the vast amounts of data, machine learning is better equipped to hunt for patterns in the context of similarity search , rather than relying on heuristics  or signatures  as it has been the case until recently.

In this work, we propose to address the problem of similarity search in Windows Portable Executable (PE) files. Our choice of platform is driven by the vast majority of computers worldwide that use the Windows operating system . Moreover, our motivation for approaching binary code similarity (BCS) is the challenging nature of this problem due to factors such as the absence of source code and binary code varying with compiler setup, architecture, or obfuscation [8; 16; 17]. We identified a scarcity of data in the space of BCS in Cybersecurity  and decided to address this by generating a benchmark for binary code similarity. Our work uses EMBER  - a large data set of PE files intended for malware detection - and a combination of automatic tagging tools and machine learning techniques as described below. Our hope is to enable future research in this area, with a focus on considering real-world complexities when training and evaluating malware similarity detectors.

Our contribution is threefold:

1. We release EMBERSim, an enhanced version of the EMBER data set, that includes similarity information and can be used for further research in the space of BCS.
2. We augment EMBERSim with automatically determined malware class, family and behavior tags. Based on these tags, we propose a generally-applicable evaluation scheme to quantify similarity in a pool of malicious and clean samples.
3. As a premiere for the Cybersecurity domain, we repurpose a malware classifier based on an ensemble of gradient-boosted trees  and trained on the EMBER data set, to quantify pairwise similarity . We publish the relevant resources to reproduce, on any other sample set, both this similarity method as well as our malware tag scoring technique based on tag co-occurrence. Code and data at: CrowdStrike/embersim-databank.

The remainder of this paper is organized as follows. In Section 2 we position our research in the related context for binary code and pairwise-similarity detection. Section 3 develops on the data that this work is based on, with a focus on EMBERSim - the enhanced EMBER, augmented with similarity-derived tags. Section 4 introduces the tree-based method used for similarity and the tag enrichment workflow. The experimental setup and evaluation are discussed in Section 5. Finally, we conclude the paper with Section 6, giving an outlook of this research.

## 2 Related Work

In this section, we put our work in perspective with a few related concepts and domains. We regard our own method with respect to the landscape of binary code similarity in general and with Cybersecurity in particular. We briefly discuss our choice of similarity method and its applicability to this domain. Finally, we inspect the literature exploiting EMBER  - the data set used as support - and note a few insights.

Binary code similarity (BCS) has, at its core, a comparison between two pieces of binary code . Traditional approaches to BCS employ heuristics such as file hashing [12; 21; 22], graph matching  or alignment . A sane default in the tool set of a Threat Analyst for similarity search is ssdeep , a form of fuzzy hashing mapping blocks of bytes in the input to a compressed textual representation, and using a custom string edit distance as the similarity function. Alternatively, machine learning is often chosen to tackle similarity in binary files, both with shallow , as well as with deep models . Deep Learning, in particular, has been extensively used for BCS in the past years, from simpler neural networks [7; 8; 27; 28] to GNNs [6; 29; 30] and transformers [16; 31]. However, only a handful of the BCS ML-powered solutions  tackle the Cybersecurity domain [32; 33]. Out of these, most papers address similarity in functions [7; 27; 29] rather than whole programs .

With the present work, we propose to fill in some gaps in BCS for Cybersecurity, with a focus on whole programs. We differentiate ourselves from related research through our choice of method - repurposing an XGBoost  based malware classifier for PE files to leverage pairwise similarity based on leaf predictions (i.e. also known as proximities) . The possibility of extracting pairwise similarities from tree-based ensembles is suggested in a few different works such as Rhodes et al. , Criminisi et al. , Cutler et al. , Qi et al. . Similarity, in this case, is determined by counting the number of times when two given samples end up in the same leaf node, as part of prediction . Intriguingly, despite its effectiveness in Medicine [38; 39] and Biology , we did not find any correspondence of the aforementioned method in the literature for BCS, especially applied in Cybersecurity.

Literature shows a scarcity of data for similarity detection in PE files . Most of the past works are validated only against one of the benign/malicious classes , with less than 10K [41; 42; 40] samples in the test set . Thus, we propose to conduct our research on EMBER , a large data set (1M samples) originally intended for malware detection in PE files. Other works using EMBER as support usually focus on malware detection [43; 44; 45; 46]. Similar to our second contribution, Joyce et al.  use AVClass  to label the malicious samples in EMBER with the corresponding malware family. However, the focus in this work is on categorizing the malware, whereas we have a different end goal (i.e. similarity search) and workflow involving the aforementioned tags. To the best of our knowledge, we are the first to release an augmented version of a large scale data set with rich metadata information for BCS.

## 3 Data

EMBER  is a large data set of binary files, originally intended for research in the space of malware detection. Most of the works that use EMBER as support continue the exploration following the initial purpose for which this data has been released - malware detection [43; 44; 45; 46] and classification by malware family . In the present work, we enhance EMBER with similarity-derived information. Our goal is to enable further research on the subject of ML-powered binary code similarity search applied in Cybersecurity - a rather low resource area as discussed in Section 2. Through this section, we briefly describe the data used in our experiments, as well as the new metadata added to EMBER as part of our contribution.

### EMBER Overview

In this research, we use the latest version of the EMBER data set (2018, feature version 2)1, containing 1 million samples, out of which 800K are labeled, while the remaining 200K are unlabeled. The labeled samples are further split into a training subset of size 600K and a test subset of size 200K, both subsets being perfectly balanced with respect to the label (i.e. benign/malicious). Based on the chronological order of sample appearance, a significant portion (95%) of the samples were introduced in 2018, with the data set primarily comprising benign samples prior to that period. Additionally, it is worth noting that the division of the data set into train and test subsets follows a temporal split that clearly delimits the two subsets. Overall, the EMBER repository clearly describes the data, contains no personally identifiable information or offensive content, and provides a readily applicable and easily customizable method for the feature extraction required for training and experimenting with new models on samples known to all vendors using VirusTotal (VT)2.

### EMBERSim - Metadata Enhancements

**EMBER with AVClass v2 Tags.** In order to validate and enrich the EMBER metadata, we query VirusTotal, and obtain results from more than 90 AV vendors, particularly related to the detection result (malicious or benign) and, where applicable, the attributed detection name. This step is necessary in order to assess potential label shifts over time given that detections tend to fluctuate across vendors and the extent to which this can happen has not been quantified in the original EMBER data. Next, following a procedure similar to the one outlined in EMBER3, we incorporate a run of the open source AVClass4 system [48; 49] on the collected VirusTotal data. One key distinction is that we use AVClass v2 , which was not accessible during the original publication of EMBER. First of all, with the well-defined and comprehensive taxonomy in AVClass v2  we aim to obtain standardized detection names from vendors. Secondly, we make use of the new features in AVClass2 to rank the generated tags (given vendor results) and to categorize them into FAM (family), BEH (behavior), CLASS (class), and FILE (file properties). To give an idea about interesting malware tags, we list the 10 most prevalent values below:

* **CLASS**: grayware, downloader, virus, backdoor, worm, ransomware, spyware, miner, clicker, infector.

* **FAM**: xtraf, zbot, installmonster, ramnit, fareit, sality, wapomi, emotet, vflooder, ulise.
* **BEH**: inject, infostealer, browsermodify, ddos, filecrypt, filemodify, autorun, killproc, hostmodify, jswebinject.
* **FILE**: os:windows, packed, packed:themida, msil, bundle, proglang:delphi, small, installer:nsis, proglang:visualbasic, exploit

**Tag enrichment via co-occurrence.** Following the tag enrichment procedure outlined in Subsection 4.2.2, we augment the list of tags in EMBER with new tags which frequently appear together, as indicated by the tag co-occurrence matrix constructed using the AVClass v2 capabilities.

**Obtaining tag ranking for evaluation.** We leverage the rich tag information extracted from AVClass and augment it with tag co-occurrence data, to generate the metadata for EMBERSim. Table 1 shows how the presence of tags is correlated with the original label. This ground truth metadata can be utilized to evaluate the efficacy of our proposed similarity search method described in Subsection 4.1. The approach we explored for evaluation is described in Subsection 4.2.1.

Table 1 highlights potential disagreements between the current and previous version of AVClass attributed tags. These tag changes can occur for benign files in limited instances (less than 5% in our case) given that greyware/adware is hard to quantify as either benign or malicious and vendors can have different views on a particular sample. Another limitation which can be observed here is that AVClass cannot attribute tags to approx. 1% of malicious samples. This is due to the fact that these samples did not have additional metadata available from VT to process at the time of writing as well as at the time of the original data set creation.

## 4 Method

### Binary Code Similarity Using Leaf Predictions

As mentioned in Section 1, this work proposes viewing similarity through the lens of leaf predictions. We regard this as one of our main contributions, provided that, as far as studied literature shows, the aforementioned method based on leaf similarity was not applied before in BCS for Cybersecurity.

The similarity method outlined in this subsection is based on leaf predictions, which means that we depend on training a tree-based classifier. Although the method itself can be applied to any tree-based ensemble, our specific choice of algorithm is an XGBoost ensemble, due to its relevance in both industry in general  and in academic research . Given our choice of data set and the domain addressed in this work, the aforementioned XGBoost model is trained to differentiate between malicious and benign PE files. The training setup and model performance do not make the object of this research and are only briefly discussed in Section 5. Relevant to our work is how we employ the binary XGBoost classifier to quantify the pairwise similarity between samples.

Given two samples, \(s_{1},s_{2}\), and a trained tree-based model, we compute the leaf predictions represented as two vectors - \(x_{1}\) corresponding to \(s_{1}\) and \(x_{2}\) corresponding to \(s_{2}\). Each of the two vectors contains the indices of the terminal nodes (leaves) for each tree in the ensemble when passing the respective sample through the model, with \(T\) being the total number of trees in the ensemble. Intuitively, the vector of leaf predictions can be viewed as a hash code, where each value at position \(i\) (i.e. \(x_{1}^{(i)}\) and \(x_{2}^{(i)}\) respectively) corresponds to a specific region in the input space partitioned by the \(i\)-th tree. Therefore, we consider two samples to be similar if they follow similar paths through the trees (denoted by the indicator function), hence obtaining similar leaf predictions. For the remainder

  
**prev** &  &  &  \\
**curr** & missing & present & missing & present & \\
**label** & & & & & \\  unlabelled & 95,359 & 8,208 & 1,958 & 94,475 & 200,000 \\ benign & 378,792 & 21,208 & 0 & 0 & 400,000 \\ malicious & 2,542 & 8,891 & 6,264 & 382,303 & 400,000 \\
**All** & 476,693 & 38,307 & 8,222 & 476,778 & 1,000,000 \\   

Table 1: Previous vs. current AVClass tag presence with respect to label. Red cells indicate potential disagreement among label & tag presence (e.g. benign files with tags & malicious files without tags).

of this paper, we refer to this approach as _leaf similarity_ which gives a score formally, computed as:

\[(x_{1},x_{2})=_{i=1}^{T}[x_{1 }^{(i)}=x_{2}^{(i)}] \]

### EMBERSim Metadata Augmentation Procedure

In Section 3, we have outlined a few insights regarding the metadata enrichment performed on EMBER. The diagram in Figure 1 provides a high-level overview of the augmentation and evaluation procedures followed. The main steps are detailed in the continuation of this subsection.

#### 4.2.1 From VirusTotal queries to AVClass 2

As detailed in Subsection 3.2, we use the SHA256 of the samples in the original EMBER data set to query VirusTotal and get detections from the AV vendors available on VT. We then run AVClass on the collected VirusTotal data. Our primary goal is to get standardized detection names from vendors. Secondly, we obtain a ranking of the generated tags and can categorize them into malware classes (CLASS), families (FAM) or behaviors (BEH).

#### 4.2.2 Tag enrichment via co-occurrence

The main output AVClass provides, for each sample: the SHA256 of the file, the number of detections from vendors or a NULL value in the absence of detects, and, finally, a comma-separated list of tags (i.e. of the tag kinds: FILE, BEH, FAM, CLASS, UNK). A score is also attached to each of these tags, as displayed in Listing 1.

```
43211f5628568ae9e25a6011e496663b584d681303a544bc40114936a10764c651 FILE:os:windows|11,CLASS:worm|5,FAM:swisyn|5,FAM:reconyc|2,FILE:packed|2
```

Listing 1: Example of AVClass output for a sample.

Alongside individual results, AVClass can also construct a tag co-occurrence matrix, containing all pairs of tags that appear together in samples, along with co-occurrence statistics. Considering a tag pair \((a,b)\), AVClass provides information about: the total number of occurrences of the tag \(a\), the total number of occurrences of the tag \(b\), the total number of common co-occurrences, common co-occurrences relative to tag \(a\), and common co-occurrences relative to tag \(b\). Tag frequency information for a given sample is determined only by the total number of distinct samples it appears in, regardless of how many vendors produced this tag. This common co-occurrence frequency can be regarded as a proxy for the relevance of the tag pair, and hence we can utilize it to augment an existing tag list with new tags which frequently appear together. That is to say, if a sample originally has only a tag \(x\), but given the co-occurrence matrix we observe that the tag pair \((x,y)\) frequently appears together,

Figure 1: Summary of the data set enrichment and evaluation process.

then we will also add tag \(y\) to the sample. This enriching mechanism is particularly important in the context of similarity search for Cybersecurity as it allows finding samples which may share several characteristics (i.e. with respect to binary file structure), although they are tagged under different (e.g. family) names - a common practice across AV vendors. The procedure we employ for enriching the EMBER data set with tag co-occurrence information is described in Algorithm 1

```
1:Input: Sample with tag metadata, tag co-occurrence matrix, co-occurrence threshold \(T\)
2:Output: Extra tag info for the sample
3:prev\(\) previous single tag from AVClass\(\) From EMBER, 2018
4:curr\(\) current tag info from AVClass 2\(\) Mapping from tag to score
5:res\(\{\}\)
6:ifnotprev and not curr then
7:return\(\{\}\)\(\) Nothing to do, sample is most likely benign
8:elseifprev and not curr then
9:for tag pairs \(( x)\) with \((x) T\)do
10:res[\(,x\)]\(\)freq\((x)\)
11:elseif not prev and curr then
12:for tag \(x\) of kind \(\{,\}\)do
13:for tag pairs \((x y)\) with \((y x) T\)do
14:res[\(x,y\)]\(\)freq\((y x)\)
15:else
16: apply both cases above
17:returnres
```

**Algorithm 1** Tag enrichment algorithm

#### 4.2.3 Obtaining tag ranking for evaluation

We use the rich tag information from AVClass and the co-occurrence analysis, to generate ground truth metadata that can be utilized to evaluate the BCS method based on leaf predictions described in Subsection 4.1. The focus through Subsection 4.2 is on the tag ranking method used to generate the mentioned ground truth metadata. Considering the fact that benign samples do not have tags, we employ a tag ranking procedure applied to all malware samples from the EMBER data set. This procedure takes advantage of supplementary tags acquired through co-occurrence analysis conducted in earlier stages. For understanding the remainder of this subsection, we should recall that in the raw AVClass output, each tag is accompanied by the AVClass rank score.

Given a sample and its AVClass tag list that contains multiple tags of different kinds (FAM, CLASS, BEH, FILE) with their corresponding rank score, conditioned on a tag kind we can derive a probability distribution

\[P(x kind=K)=(x)}{_{yK}(y)} \]

We can regard this probability as a measure of confidence and agreement between multiple vendors. Further, we denote by \((x,y)\) the common co-occurrence frequency of tags \(x\) and \(y\), and by

\[(x y)=(x,y)}{(y)} \]

the relative occurrence frequency of tag \(x\) relative to the occurrences of tag \(y\). At the core of the tag ranking algorithms is the formula in Equation (4) which extends the original AVClass rank scores with co-occurrence information:

\[(x)=P(x kind=K)+_{y x}P(y kind =K)(x y) \]Note that due to the scaling by the frequency, this scoring scheme will rank original tags higher than tags obtained via co-occurrence, which may be desirable given that co-occurrence information can be noisier. Algorithm 2 introduces the tag ranking procedure.

```
1:Input: Sample with tag metadata, tag kind \(K\) to rank by
2:Output: Tag ranking for the sample
3:Compute \(P(x kind=K)\) for all tags \(x\)
4:Initialize \([x]\) from \(P(x kind=K)\)
5:for tag pairs \((x y)\)do
6:if\(kind(x)\) is FAM and \(kind(y)\) is \(K\)then
7:\([y]\) += \(P(x kind=K)(y x)\)
8:\(\) sort \(\) in descending order and filter to tags of kind \(K\)
9:returnranking
```

**Algorithm 2** Tag ranking algorithm

## 5 Experiments and evaluation

For convenience, we conduct experiments on an architecture that has 64CPUs and 256GB of RAM. A minimal set of requirements to run the method proposed in this paper, consists in being able to load the data set and model in memory. Concretely, for the EMBER data set and the trained XGBoost model the memory consumption is around 27 GB of RAM. Thus, a machine with 32 GB of RAM should be enough to replicate the results in this paper.

Experimentation, in the context of the present work, implies, first of all, determining the ground truth malware tags, as detailed in Subsection 4.2. Secondly, we fine-tune an XGBoost-powered binary malware classifier until it reaches satisfactory performance - i.e. we obtained a ROC AUC score of \(0.9966\) on EMBER's test split, a value close to the one (i.e. \(0.9991\)) reported in the paper introducing the 2017 batch of EMBER data. Our fine-tuning process yielded the following set of hyperparameters: \(max\_depth=17\), \(eta=0.15\), \(n\_estimators=2048\), \(colsample\_bytes=1\). Finally, also as part of the experimentation, we utilize the leaf predictions to compute the similarity between the samples in EMBER.

Post-experimentation, we perform two different types of evaluation, leveraging both the malicious/benign labels from EMBER (as discussed in Subsection 5.1) and also the tag information computed via AVClass to evaluate the quality of the similarity search as outlined in Subsection 5.2. An important mention is that our evaluation employs a counterfactual analysis scenario in a real-world like setting: we use a test set including an out of time sample, the test data being collected between 2018-11 and 2018-12, while the indexed database only includes samples up until 2018-10. We believe that such a scenario reflects the ever changing nature of the threat landscape as is the case with the Cybersecurity domain.

### Top K Selection & Evaluation Based on Malicious/Benign Labels

We select the top K most similar samples from the whole data set, for each query-sample in the test split. Given that the XGBoost classifier did not see the test samples during the training, no bias should be attached to the query process. Our first evaluation scenario is based on counting the labels (malicious/benign) that a query and its most similar samples share. Table 3 shows the results for this type of evaluation considering the top K hits, with K in . The _Mean_ column represents the mean hits count with the same class as the needle (e.g. for best performance, the mean should be close to K and the standard deviation should be close to 0). We report ssdeep  results in Table 2. In this case, we are relaxing the evaluation condition and consider a hit if the label of the query matches the label of the results and the samples have non-zero ssdeep similarity. The results indicate poor overall performance, with marginally better scores for malicious samples, most likely due to the fact that such samples contain specific byte patterns. In contrast, our proposed method (Table 3) achieves better results for both malicious and benign queries, showing that leaf similarity is well-equipped to find similar entries and becomes more precise as we decrease the value of K.

### Relevance@K Evaluation

The second evaluation flavor explored requires obtaining ground truth metadata in the form of ranking FAM tags as described in Subsection 4.2. Subsequently, given a query sample, we check whether its tag ranking is consistent with the top-K most similar samples. The results of this evaluation scenario are displayed in Table 4.

The underlying assumption followed here is that metadata should be preserved for similar samples. For assessing the relevance between two tag lists, we use the similarity functions listed below:

* outputs 1 if the inputs are exactly the same, otherwise outputs 0. It is the strictest comparison method, serving as a lower bound for that performance.
* disregards element ordering and focuses solely on the presence of items in the inputs.
* this function, based on Damerau-Levenshtein distance (DL) for strings , allows us to penalize differences in rank.

The evaluation procedure, referred to as relevance@K, works as follows: given a query sample and its tag ranking, as well as the top-K most similar hits (each with their own tag rankings), we use the aforementioned functions to calculate the relevance between the query and each hit. This yields K relevance scores for each sample. We then repeat this process for all N query samples in a subset. It is important to note that if a sample is missing its tag list, we consider it to be benign. Therefore, if both the query and hit are missing their tag lists, the relevance is 100%; if the tag list is present for either the query or hit, the relevance is 0%; otherwise, we apply the similarity functions described above.

For constructing the _query_ and _knowledge base_ subsets, we consider the following scenarios:

1. **Counterfactual analysis** (query = test, knowledge base = train \(\) test). Leveraging the train-test temporal split (i.e. timestamp(test) \(>\) timestamp(train)), we can simulate a real-world production-like scenario, where we use unseen, but labeled data (i.e. the test set) to query the knowledge base.
2. **Unsupervised labeling** (query = unlabelled, knowledge base = train). Given the collection of unlabelled samples, the goal is to identify similar samples within the knowledge base, with the purpose of assigning a provisional label or tag.

Table 4 shows that both scenarios provide very good results in terms of matching queries and hits according to their tags. From a similarity search perspective this means that in the counterfactual analysis scenario simulating a production environment, our method manages to retrieve more than 95% benign similar samples for a given unseen benign query based on tag information and more than 71% relevance of malicious samples for a given malicious query respectively even for 100 hits. This also allows for high confidence unsupervised labelling as can be seen in the bottom table. We do note that the production scenario is expected to be the hardest of the two given that the queries here come

   K &  &  &  \\  & Mean & Std & Mean & Std & Mean & Std \\ 
1 & 0.51 & 0.49 & 0.8 & 0.39 & 0.66 & 0.47 \\
10 & 3.55 & 4.34 & 7.31 & 4.16 & 5.43 & 4.65 \\
50 & 12.31 & 19.24 & 32.59 & 22.12 & 22.45 & 23.07 \\
100 & 19.84 & 34.82 & 61 & 45.22 & 40.42 & 45.31 \\   

Table 2: Label homogeneity (ssdeep )

   K &  &  &  \\  & Mean & Std & Mean & Std & Mean & Std \\ 
1 & 1 & 0 & 1 & 0 & 1 & 0 \\
10 & 9.68 & 1.13 & 9.80 & 0.97 & 9.74 & 1.06 \\
50 & 47.10 & 7.34 & 48.40 & 6.17 & 47.75 & 6.81 \\
100 & 92.80 & 16.01 & 96.28 & 13.22 & 94.53 & 14.7from an out of time distribution compared to the samples the XGBoost model was trained on. For completeness, Table 5 shows the evaluation results with Mean Average Precision (mAP) using exact match to check if two tag rank lists are equivalent (hence relevant).

## 6 Conclusions

**Assumptions and limitations.** The evaluation scheme described in Section 5 requires satisfying two underlying assumptions. Firstly, there are three factors that dictate the quality of the tag information for a sample: (1) the accuracy of VT vendor detections, (2) the tag normalisation and ranking algorithm used by AVClass and (3) the manner in which the tag co-occurrence information is leveraged. Secondly, we consider the lack of tags for a sample to be an indicator that the sample is benign. Although VT detections can be noisy due to various differences across vendors (see section 3), our strategy aims to mitigate this effect as much as possible.

    \\   &  &  &  \\  & & Mean (Std) & Percentiles & Mean (Std) & Percentiles \\   & 1 & 0.986 (0.120) & 0, 1, 1, 1 & 0.717 (0.451) & 0, 0, 1, 1 \\  & 10 & 0.975 (0.097) & 0.5, 0.9, 1, 1 & 0.681 (0.355) & 0, 0.1, 0.8, 1 \\  & 100 & 0.951 (0.124) & 0.34, 0.85, 1, 1 & 0.612 (0.364) & 0, 0.04, 0.7, 1 \\   & 1 & 0.986 (0.120) & 0, 1, 1, 1 & 0.812 (0.349) & 0, 0, 1, 1 \\  & 10 & 0.975 (0.097) & 0.5, 0.9, 1, 1 & 0.780 (0.295) & 0, 0.3, 0.93, 1 \\  & 100 & 0.951 (0.124) & 0.34, 0.85, 1, 1 & 0.714 (0.321) & 0, 0.15, 0.85, 1 \\   & 1 & 0.986 (0.120) & 0, 1, 1, 1 & 0.797 (0.353) & 0, 0, 1, 1 \\  & 10 & 0.975 (0.097) & 0.5, 0.9, 1, 1 & 0.764 (0.295) & 0, 0.3, 0.9, 1 \\  & 100 & 0.951 (0.124) & 0.34, 0.85, 1, 1 & 0.697 (0.319) & 0, 0.14, 0.81, 1 \\    \\   &  &  &  \\  & & Mean (Std) & Percentiles & Mean (Std) & Percentiles \\   & 1 & 0.996 (0.066) & 1, 1, 1, 1 & 0.960 (0.196) & 0, 1, 1, 1 \\  & 10 & 0.967 (0.116) & 0.3, 0.9, 1, 1 & 0.730 (0.325) & 0, 0.1, 0.9, 1 \\  & 100 & 0.933 (0.152) & 0.2, 0.78, 1, 1 & 0.641 (0.364) & 0.01, 0.05,

**Potential negative societal impact and ethical concerns.** The data set we enrich with additional similarity-enhancing metadata contains known malware samples and has already benefited Cybersecurity research for many years. Thus Cybersecurity vendors are well aware and capable to defend against all samples included in this data set.

**Future research directions.** We identified two different ideas that can drive future research in this space, while using our current results as support: model interpretability and refining the baseline proposed in this work. For the latter case, we see value in conducting a comparative study, with multiple tree-based models.

**Final remarks.** In the present work, we introduce EMBERSim, a large-scale data set which enriches EMBER with similarity-derived information, with immediate application for sample labeling at two distinct granularities. To the best of our knowledge, we are the first to study the applicability of pairwise similarity, also denoted in this paper as leaf similarity, both in the context of binary code similarity, as well as with applicability to Cybersecurity in general. The aforementioned method shows promising results, as our evaluation indicates (see Section 5). This allows us to empower malware analysts with a comprehensive set of tags describing samples in a pool of malicious and clean samples they can leverage for similarity search at scale. We hope that the insights presented here will enable further research in a domain that would truly benefit from more results - ML-powered binary code similarity in Cybersecurity.