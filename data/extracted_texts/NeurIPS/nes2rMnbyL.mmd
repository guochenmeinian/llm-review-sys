# _AdaSociety_: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making

Yizhe Huang\({}^{ 2,1}\) Xingbo Wang\({}^{}\)\({}^{1}\)\({}^{2}\) Hao Liu\({}^{}\)\({}^{3}\) Fanqi Kong\({}^{2,1}\) Aoyang Qin\({}^{4,1}\)

Min Tang\({}^{5,1}\) Song-Chun Zhu\({}^{1,2}\) Mingjie Bi\({}^{1}\) Siyuan Qi\({}^{1}\) Xue Feng\({}^{}}\)\({}^{ makes the problem even more challenging is that social connections are not predefined but adaptive, which means there's a dynamical interplay between the topology of social connections and agents' states . The adaptive nature of social connections and physical surroundings requires agents to learn continuously, reason about other agents' policies, and balance between physical explorations and establishing social connections. While contemporary multi-agent decision-making environments [6; 2; 53; 66; 48] have achieved great progress in stimulating and testing capabilities of learning algorithms in fixed task sets, they fail to generate new tasks by concurrently considering expanding physical surroundings and adaptive social connections.

To bridge this gap, we propose _AdaSociety_, a multi-agent environment with massive and diverse tasks generated by adaptive social connections and expanding physical surroundings, which are influenced by agents' behavior. In particular, to the best of our knowledge, _AdaSociety_ first introduces social states (expressed as a multi-layer directed graph) to explicitly and quantitatively describe the adaptive and dynamic connections between entities, including agents and emerged organizations. This greatly enriches the diversity of tasks, supporting the establishment of stable and long-term relations between entities and the quantitative study of social intelligence, like coalition formation and the emergence of hierarchy. In such an environment, agents need to balance the exploration of physical surroundings and the alteration of social connections, leading to multiple possible victory paths and significant decision-making challenges. To stimulate algorithm design and theoretical analysis in _AdaSociety_, we provide a formulation of the multi-agent decision-making problems, named _Growing-MG_ (Sec. 3).

_AdaSociety_ serves as a platform for researchers to customize the environment for diverse research needs. Specifically, a set of fundamental elements and mechanisms can be used, and interfaces are provided to set environment attributes and hyper-parameters. Moreover, _AdaSociety_ exhibits its characteristics by offering three mini-games, where both tensor- and LLM-based methods are tested.

In summary, this paper makes three contributions. 1) We introduce a novel multi-agent general-sum environment featuring expanding physical surroundings and adaptive social connections. 2) We offer a customizable environment with three built-in mini-games, supporting both tensor- and LLM-based methods. 3) We implement RL and LLM methods in these mini-games and provide preliminary results, laying the groundwork for further research in this environment.

## 2 Environment

### Basic Components

The key components of _AdaSociety_ (Fig. 1) include the physical component, composed of resources, events, and agents' inventories, and the social component describing connections between agents and organizations. Agents can observe and act to modify both physical and social states.

Figure 1: An overview of _AdaSociety_, composed of physical component and social component. **Physical Component** consists of diverse resources and events on the map and heterogeneous agentsâ€™ inventories. **Social Component** describes the adaptive connections between agents and organizations, which shape information access and reward structure. Agents take social actions to alter their social connections. As shown in the rightmost flowchart, agents are initially independent and can establish individual connections (edges between nodes) and form groups (gray ovals).

#### 2.1.1 Physical Component

**Resource and Event.** Resources are natural or synthetic. Natural resources scatter randomly on the map. Some natural resources are visible to everyone while others can only be seen when an agent has specific resources in its inventory. For example, only the agent possessing a hammer can observe coal. When agents with specific resources in their inventories stand on an event grid and take the'synthesize' action, one unit of new resource is synthesized. Synthetic resources will be automatically placed into agents' inventories. These resources and events can be systematically described as a synthesis tree (see Fig. 4). Importantly, agents are unaware of this synthesis tree. They gradually learn the tree through interaction with the environment. Resources, event grids, and agents are initialized in random locations on the map for every episode. While there are existing 3D benchmark environments focusing on perception challenges, our research centers on the domain of multi-agent decision-making. To this end, the map is intentionally crafted in a 2D symbolic format.

**Agent's Inventory.** Every agent has an inventory with maximal capacities of every resource, implying skill diversity. For example, an agent with a \(0\) capacity for hammers cannot possess hammers and observe coal. Agents can collect resources from the map into their inventories and dump resources on the map. Agents' rewards are attached to the resources in their inventories, while they exhibit heterogeneity in resource preferences. Specifically, for agent \(i\), the reward of resource \(\) is \(R_{i}()=m_{i}^{} h_{i}()^{}\), where \(m_{i}^{}\) is the amount of resource \(\) in \(i\)'s inventory, \(h_{i}()\) represents \(i\)'s preference for \(\), \(^{}\) is the objective reward of a unit of \(\) (see details in Sec. A.1).

#### 2.1.2 Social Component

The social component explicitly exhibits the connections between agents or organizations. These connections drastically influence multi-agent decision-making by affecting agents' accessible information and reward structures. Centralization and its complete opposite, decentralization, can be seen as two typical connection structures, presenting very different decision-making problems. _AdaSociety_ supports adaptive connections, with corresponding interactions being modeled as general-sum games. _AdaSociety_ considers not only the connections between agents but also the subordinate connections between agents and organizations established autonomously by agents. This makes hierarchical connections possible. Agents take social actions to change social states, like connecting or disconnecting with someone. Fig. 1 illustrates evolving connection structures, from fully independent agents to sparsely connected agents with several non-overlapping small groups, and finally to a unified large group. On the other hand, as a customized environment, _AdaSociety_ also supports users to predefine and/or fix social connections for their specific research problems. The semantics of connections are diverse, which can be reward sharing, information sharing, or division of labor between involved agents. _AdaSociety_ supports that agents negotiate their connection semantics (Sec. 4).

To maintain consistency with the physical component, we refer to these connections between agents and organizations as social states, which are expressed as a multi-layer directed graph (Sec. 3). Social states explicitly and quantitatively express relations between agents or organizations. For example, the cooperation level of two agents can be measured by the frequency of connections between them. Moreover, the combination of social states with successive tasks in _AdaSociety_ supports the establishment of stable and long-term relations and the study of social intelligence, like coalition formation and the emergence of hierarchy.

#### 2.1.3 Observation and Action

**Observation.** Each agent navigates with a partially observable window, reaching \(o\) grids in the four cardinal directions of its current position. Agents can get their own inventory states of collected resources, but not those of co-players. The social states of all the agents are accessible to everyone.

**Action.** Action space consists of social actions and physical actions. Social actions aim to build and break connections with others, including other agents or organizations. Connections are directional. If agent \(i\) connects to agent \(j\), but not vice versa, \(i\) shares its information or reward with \(j\), but gets nothing from \(j\). Physical actions include movement, picking and dumping specific resources, synthesizing resources on corresponding event grids, and communicating with someone. Newly synthesized resources enrich picking and dumping actions and the action space.

### Evaluation Metrics

_AdaSociety_ provides diverse metrics to evaluate the performances of agents and organizations including **Individual reward**, **Fairness score**, **Completion rate**, and **Average degree** and **Maximum degree** of the social network. Definitions and details of the metrics are discussed in Sec. A.5.

### Environment Characteristics

There are various characteristics of _AdaSociety_ that make it novel (see Tab. 1). _AdaSociety_ is a **multi-agent** decision-making environment, which provides both mini-games for specific research problems and a **customizable** platform to researchers (see details in Sec. A.4). Agents **dynamically connect** with other agents or organizations and autonomously **communicate** to negotiate the semantics of connections, making the emergence of hierarchical social structure and diverse social intelligence possible. With these dynamic and non-deterministic connections, friends may become foes, and vice versa. Thus, the interactions between agents can be modeled as **general-sum games**, where cooperation coexists with competition. Agents navigate this playground with a **partially observable** window centered on their current position. The state and action spaces of _AdaSociety_** dynamically **expand**, adapting to agents' (physical and social) behavior. That generates **massive and diverse tasks**, supporting an evaluation of agents' abilities in multiple aspects. _AdaSociety_ is **friendly to LLM-** and **tensor-based agents**. We evaluate state-of-the-art RL methods and LLMs in Sec. 5. In addition, we want to stress that the **mutual adaptation between agents and _AdaSociety_**, which generates a variety of successive tasks and multiple possible victory paths. Achieving success in _AdaSociety_ requires a balance between the exploration of physical components and the alteration of social connections (see Fig. 5). Agents continually learn policies to efficiently explore and achieve goals in _AdaSociety_. Meanwhile, agents' (physical and social) behavior will affect the dynamics of _AdaSociety_. Synthesizing new resources will gradually expand _AdaSociety_'s physical state space and the corresponding physical action space, transition function, and reward function. Updated social states will reshape agents' observation and reward structures. Thus, tasks and task sequences are influenced by agents' behavior and social states, not sampled according to some predefined distribution of tasks. That is to say, _AdaSociety_ adapts its tasks and task sequences to agents. Mutual adaptation provides exceptionally massive and diverse complex tasks. The stochasticity and non-stability of _AdaSociety_ produce various environment dynamics. Agents need to keep learning to adapt to changing situations.

### Research Challenges

As an adaptive multi-agent environment, _AdaSociety_ provides a comprehensive platform that presents plenty of research challenges. The adaptive and dynamic characteristics of the physical and social components bring challenges mainly lying in the intricate and unpredictable interactions between agents. Through multi-dimensional **exploration**, agents learn the ability of dynamic environmental **adaptation** and engage in **communication**-enabled interactions. Meanwhile, agents may develop

   Environment & Multi- & Dynamic & Adaptive & Imperfect & Comm. & Multi- & General & Tensor & \\  & agent & Spaces & Connection & Information & task & Sum & \& LLM \\  AI Economist & âœ“ & âœ— & âœ— & âœ“ & âœ— & âœ— & âœ“ & âœ— \\ Boat Race & âœ“ & âœ— & âœ“ & âœ— & âœ— & âœ“ & âœ“ & âœ— \\ Crafter & âœ— & âœ“ & âœ— & âœ“ & âœ— & âœ“ & âœ— & âœ— \\ Diplomacy & âœ“ & âœ— & âœ— & âœ— & âœ“ & âœ— & âœ“ & âœ“ \\ Melting Pot & âœ“ & âœ— & âœ“ & âœ— & âœ“ & âœ“ & âœ— \\ MincDojo & âœ— & âœ“ & âœ— & âœ“ & âœ— & âœ“ & âœ— & âœ“ \\ Neural MMO & âœ“ & âœ— & âœ— & âœ“ & âœ“ & âœ— & âœ— \\ Overcooked & âœ“ & âœ— & âœ— & âœ— & âœ— & âœ“ & âœ— & âœ— \\ SMAC & âœ“ & âœ— & âœ“ & âœ— & âœ— & âœ“ & âœ— \\ Xland & âœ“ & âœ“ & âœ— & âœ“ & âœ— & âœ“ & âœ“ & âœ— \\  _AdaSociety_ & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ \\   

Table 1: Comparison with existing environments. _AdaSociety_ is unique for its adaptive connections between entities and expanding game spaces.

social cognition** and utilize this information to conduct **collective reasoning**, which may result in the **emergence** of various behaviors. Details of these challenges are stated in Appendix B.

## 3 Formulation

We now provide a comprehensive definition and analysis of the _Growing-MG with a social structure_, which are general enough to encompass all the research challenges mentioned above. Three concrete scenarios will be instantiated in next section.

The predominant model in multi-agent sequential decision-making is the Markov Game (MG) . However, a significant limitation of MG is the assumption of constant state and action spaces and unchanged Markovian transitions or rewards, ensuring convergence to some classical solutions such as global optimality or Nash equilibrium [4; 57]. To address dynamic state and action spaces, we introduce two new structures, _Monotonic-MG-bundle_ and _Growing-MG_ as below. A Growing-MG yields a multi-agent non-stationary decision-making framework. At time step \(t\), with state \(s_{t}\) and action \(a_{t}\), the Monotonic-MG-bundle produces \(S_{t+1},A_{t+1},T_{t+1},R_{t+1}=(s_{t},a_{t})\), forming one new MG instance. This framework differs from time-varying games [10; 64; 5], which only model payoff matrix dependent on past actions. On the other hand, both the transition probability and reward function in Growing-MG will evolve triggered with some certain transitions. For simplicity, we denote all possible transition and reward functions on arbitrary state and action space \(S,A\), as \((S,A)=\{T|T:S A S\}\) and \((S,A)=\{R|R:S A\}\) and the largest possible spaces supported by the environment as universal state space \(S_{w}\) and action space \(A_{w}\).

**Definition 1**.: A _base-MG_ is a tuple \(_{b}=,S_{b},A_{b},T_{b},R_{b},,\), where \(=\{1,,I\}\) is a set of agents; \(S_{b}=\{S_{b}^{1},,S_{b}^{I}\}\) and \(A_{b}=\{A_{b}^{1},,A_{I}^{I}\}\) is the state space and action space of all agents; \(T_{b}:S_{b} A_{b} S_{b}\) and \(R_{b}:S_{b} A_{b}^{I}\) is the transition and reward function; \(:S_{b}\) is the initial state distribution and \(\) is the temporal discount factor.

**Definition 2**.: _A Monotonic-MG-bundle upon a base-MG \(_{b}\) within the universal state and action space \(S_{w}=\{S_{w}^{1},,S_{w}^{I}\},A_{w}=\{A_{w}^{1},,A_{w}^{I}\}\) is a map \(:S_{t} A_{t}\{S_{t+1},A_{t+1},T_{t+1},R_{t+1}|S_{b}^{i} S _{t}^{i} S_{t+1}^{i} S_{w}^{i},A_{b}^{i} A_{t}^{i } A_{t+1}^{i} A_{w}^{i},T_{t+1}(S_{t+1},A_{t+ 1}),R_{t+1}(S_{t+1},A_{t+1})\}\)._

**Definition 3**.: _A Growing-MG upon a base-MG \(_{b}\) within the universal state and action space \(S_{w},A_{w}\) is a tuple \(_{g}=_{b},\)._

Conceptually, each alteration in the state and action space represents a distinct stage where interrelations among agents should also change. Inspired by research in complex systems like social sciences and economics [15; 52; 14], we propose enhancing the Growing-MG framework with a multilayer graph structure \(=(,,)\) (see Fig. 6). \(\) is a set of layers, and \(\) is the set of nodes in all layers. \(\) is the set of edges existing between nodes in one layer or neighboring layers. We start with a non-interconnected multiplex system of networks \(\{^{1},^{2},,^{||}\}\), where each layer \(c\) consists of a node set \(^{c}\) and an edge set \(^{c}\), represented by an adjacency matrix \(A_{ij}^{c}\) with \(i,j\{1,,|^{c}|\}\). Nodes in the first layer represent agents in Growing-MG, while higher layers represent groups and hierarchies of groups. To delineate relationship between nodes in neighboring layers such as agent-group membership, we introduce inter-layer connectivity using an adjacency matrix \(A_{ij}^{c,c+1}\) with \(i\{1,,|^{c}|\}\) and \(j\{1,,|^{c+1}|\}\). This representation models both static and time-varying networks, as inter-layer and intra-layer connectivity evolves with agents' behavior, distinguishing it from existing multi-agent frameworks that predetermine interactions through reward structures [46; 62]. Finally, we note that both the environmental and social states within the framework can be extended to include observational information [38; 39], thereby further enhancing the framework's generality and practical relevance.

## 4 Mini-games

To provide a comprehensive benchmark and illustrate the characteristics of _AdaSociety_, we propose a set of mini-games (Fig. 2). The three mini-games are arranged in ascending order of the complexity of decision-making. _Social structure_, prescribing agents' partners and connection semantics, evaluates agents' ability to adapt to the changeable social structure. _Contract_ predefines connection semantics, where agents need to select partners while learning coordination with various co-players.

In _Negotiation_, agents independently select partners, determine the reward distribution plan with their partners, and behave under the negotiated relationship. All of the three mini-games share the same physical component (Sec. 5.1), which contains a part of the synthesis tree. The following text provides a detailed description of the social components of _Social structure_, _Contract_, _Negotiation_. To show the full complexity of our physical components, another mini-game _Exploration_, which contains all built-in resources and events, is introduced in Sec. C.2.

_Social Structure._ The explicit representation of _social structure_ allows dynamic changes as agents interact with the environment. Pre-defined rules for structure change could be designed to compel agents to alter their social relationships while interacting with the environment. We implement structure change at certain steps: when step \(t\) reaches \(T_{1},T_{2},...\), the social structures are modified to \(_{1},_{2},...\), respectively. Different categories of social structures are stated in Sec. C.1. This forces agents to learn policies to adapt to the changing social environment.

_Contract._ The environment is divided into two stages: the contract formation stage for determining social connections and the physical interaction stage to interact with the physical component and co-players with determined social connections. The contract formation stage lasts for \(cN\) time steps, where \(c\) is a positive integer and \(N\) is the number of agents, while the physical interaction stage has a duration of \(T\). Therefore, the total duration of each episode is \(cN+T\). Before the contract formation stage \((0 t<cN)\), an order \((i_{1},i_{2},...,i_{N})\) is randomly sampled. At time \(t\), agent \(i_{k}\), where \(k=t N\), takes social action, selecting a group node \(v_{g} V_{g}\) to connect. An agent can connect with only one group node. Agents within the same group are considered to have formed a contract to share rewards. In the physical interaction stage \((t cN)\), all agents act synchronously within the physical component, and the rewards received are equally divided among the agents within the same group.

_Negotiation._ The game has a negotiation stage followed by a physical stage. In the beginning, agents seek cooperation by selecting an opponent and sending him a request. After mutual requests, agents bargain by exchanging proposals until agreement or breakup. In the bargaining session, agents \(i\) and \(j\) take turns to perform one of the three actions: (i) PROPOSE a new scheme \((w_{i},w_{j})\) s.t. \(w_{i}+w_{j}=1\), where \(w_{i}\) and \(w_{j}\) represent the partition of rewards obtained by \(i\) and \(j\) respectively in the physical stage. (ii) ACCEPT the proposal from one's opponent and form a new group (coalition). (iii) DECLINE the proposal and end this session without any commitment. Once a new group is formed, the cooperative relationship between \(i\) and \(j\) represented by edge \(_{ij}\) with a payoff distribution \((w_{i},w_{j})\) is established. Later, when \(i\) or \(j\) seeks to negotiate with others, it represents the group \(\{i,j\}\). For example, if \(i\) and an out-group agent \(k\) reach a new distribution plan \((w_{i}^{},w_{k}^{})\), then \(k\) is regarded as joining \(\{i,j\}\) to form a new group \(\{i,j,k\}\) with an updated distribution \((w_{i} w_{i}^{},w_{j} w_{j}^{},w_{k}^{})\).

## 5 Experiments

### Environment Setup

We have designed two physical task settings, featuring different levels of difficulty, for _Social Structure_, _Contract_, and _Negotiation_. The parameters of these tasks are provided in Sec. C.3.

In the Easy task, the environment involves a single event HammerCraft. Within this task, agents are categorized into two types based on their inventory capacity and value preference: carpenters and miners. Carpenters have the ability to gather wood and stone, which they can then use to produce hammers through the HammerCraft event. However, their inventory is limited to holding only one hammer at a time. On the other hand, miners are unable to collect stone, making them incapable of producing hammers. However, miners possess the advantage of being able to store a considerable number of hammers in their inventory. Additionally, hammers held by miners are assigned a higher value compared to those held by carpenters.

Figure 2: Overview of three mini-games.

In the Hard task, the environment becomes more complex with the inclusion of six resources: wood, stone, hammer, coal, torch, and iron, as well as two events: HammerCraft and TorchCraft. Similar to the Easy task, agents are divided into carpenters and miners. Due to the limited capacity of certain resources, only carpenters can execute HammerCraft to produce hammers, while only miners can execute TorchCraft to produce troches. However, carpenters' inventories cannot store coal, which requires a hammer to pick up, and miners' inventories cannot store iron, which requires a torch to pick up. Consequently, in order to maximize group rewards, carpenters and miners should engage in resource exchange, providing the resources they can produce to each other. This collaborative effort ensures that the group can obtain more resources collectively.

### Baseline Methods

We use several deep reinforcement learning algorithms as baselines. _Proximal Policy Optimization (PPO)_ strikes a balance between sample efficiency and policy stability by constraining policy updates using a trust region approach and a clipped surrogate objective. _RecurrentPPO(RecPPO)_ uses PPO for training and add LSTM  to maintain memories in the network. _Rainbow_ is a value-based method that incorporates several key enhancements into the Deep Q-learning framework. _MAPPO_ is the multi-agent version of PPO. It learns a critic that takes the global state and other agents' actions as inputs during training. We employ a convolutional neural network for encoding grid information and a graph convolutional network  for encoding social state in all RL methods. The open-source library RLLib  is used for RL training.

Additionally, we design a _curriculum learning (CL)_ algorithm. It starts with shared rewards to enhance cooperation strategies, then gradually increases social state randomness for learning under different social structures, and finally allows agents to perform social actions to establish their own social state. RecPPO is used for RL training at each stage. We also present a _Large Language Model + rule-based controller (LLM-C)_ framework based on GPT-4 , which converts environmental information into prompts to query an LLM for high-level plans and then calls a rule-based controller to execute actions based on the generated plan. LLM has been shown to be effective in some single-agent environments, such as MineCraft [59; 56; 58; 67; 60]. The details of the last two algorithms are given in Appendix D.

### Results

#### 5.3.1 _Social Structure_

In the _Social Structure_ mini-game, various static and dynamic social structures are tested to evaluate baseline algorithms. Detailed results are presented in Appendix E. Here, we discuss the result of one **Dynamic** scenario, where the social structure starts with **Inequality**, then switches to **Independent (Ind.) group** at step 30, and alters to **Overlapping (Ovlp.) group** at step 60.

Fig. 2(a) presents the reward accumulation as agents take actions with three static-structure scenarios and one dynamic-structure scenario, respectively. The results verify the influence of dynamic change in social structure on agent performance since the **Dynamic** curve resembles the **Inequality** scenario initially but then it drops in later steps and approaches **Ovlp. group** scenario.

Fig. 2(b) and Fig. 2(c) illustrate the performance of various learning methods. Some traditional methods, such as PPO, RecPPO, and MAPPO, exhibit similar performance, with MAPPO performing worse due

Figure 3: **Dynamic structure: (a) Individual reward per step with different social structures using 100 samples from PPO-trained policies, (b) Individual reward per step using 100 samples from different policies (c) Learning curves using different learning methods.**

to the difficulty in learning an effective central critic for heterogeneous agents. Rainbow performs the worst, likely because of its general ineffectiveness in exploration. Curriculum learning demonstrates superior performance by leveraging prior knowledge of different structures to adapt to dynamic scenarios effectively. Additionally, figures in Fig. 3 reveal significant deviations in most tests, regardless of social structures, learning algorithms, or performance metrics. Compared to scenarios without agent groups (Fig. 9(a) and Fig. 10(a)), the results indicate that the current algorithms struggle to learn stable policies for scenarios with agent groups.

#### 5.3.2 Contract

As depicted in Tab. 2, _Contract_ presents a challenge for popular RL methods, as they are stuck in a local equilibrium of completing limited HammerCraft on both tasks (see Fig. 6(b)), while CL demonstrates notable performance on the Easy tasks and surpasses general RL methods on the Hard tasks. The first curriculum in CL equips the agent with the ability to learn effective policies in the physical realm, and the second curriculum empowers the agent to make informed judgments about different social structures while considering rational physical policies. Ultimately, this knowledge aids CL in selecting an appropriate contract. However, it appears that CL may forget the strategies acquired during the first curriculum, as the reward at the end of the second stage has dropped significantly compared to the end of the first stage (see Tab. 12 for details). This might hamper the performance of CL on the Hard task.

Sharing rewards has been recognized as an effective method for agent groups to acquire cooperative strategies, thereby supporting the feasibility of CL's approach. Fig. 6(c) and Fig. 6(d) also illustrates that. In the case of the Easy task, CL eventually establishes a stable group of three individuals who actively share rewards and form a cooperative alliance. However, it is important to note that the size of the group does not directly correlate with high returns. Rainbow, for instance, frequently forms large groups in both tasks but fails to achieve substantial returns. This outcome primarily stems from inherent limitations in the algorithm's learning capabilities.

#### 5.3.3 Negotiation

Traditional RL methods struggle to enable carpenters and miners to learn to cooperate through negotiation, dumping some tools to increase the benefit of teammates with larger capacities on the physical stage as shown in Tab. 2. This challenge arises from the complexity of coupling the negotiation and physical stages. Once negotiation fails, dumping tools in the subsequent physical stage would substantially reduce the agents' rewards. Meanwhile, the complex negotiation process exacerbates the convergence problem in multi-agent settings, and agents have the incentive to claim a larger share for themselves to exploit the co-players in bargaining, posing challenges to reaching a consensus agreement. Consequently, in both Easy and Hard tasks, the average and maximum degrees are low, with most agents opting to complete tasks independently, leading to low completion rates in HammerCraft and even a complete failure in TorchCraft (Fig. 8). In the Easy task, miners' rewards heavily rely on carpenters' cooperation, which severely compromises fairness. In contrast, by first learning the optimal strategies in physical environments under different social structures, CL can identify structures with higher cooperation degrees as more beneficial, facilitating consensus during negotiation learning and achieving higher group rewards, fairness, and successful TorchCraft. Additionally, we show the Carpenters/Miners (abbreviated as C/M) split ratio when the negotiation stage is done, which is computed by \(_{i\{\}}w_{i}/_{i\{\}}w_{j}\). All results exceed 1, aligning with the intuition that miners are disadvantaged in negotiations as they cannot independently produce the more rewarding hammers.

   &  &  & RecPPO & MAPPO & Rainbow & Random \\   & Easy & 0.9136\(\) 0.0023 & 0.2286\(\) 0.0003 & 0.2276\(\) 0.0015 & 0.2271\(\) 0.0003 & 0.1987\(\) 0.0127 & 0.0046\(\) 0.0002 \\  & Hard & 0.2773\(\) 0.0466 & 0.1151\(\) 0.0002 & 0.1149\(\) 0.0000 & 0.1137\(\) 0.0005 & 0.0868\(\) 0.0033 & 0.0021\(\) 0.0000 \\  & Easy & 0.3543\(\) 0.0229 & 0.2276\(\) 0.0006 & 0.2278\(\) 0.0004 & 0.2147\(\) 0.0001 & 0.1969\(\) 0.0105 & 0.0040\(\) 0.0001 \\  & Hard & 0.1945\(\) 0.0109 & 0.1093\(\) 0.0027 & 0.1107\(\) 0.0019 & 0.0946\(\) 0.0032 & 0.0905\(\) 0.0024 & 0.0020\(\) 0.0001 \\  

Table 2: Average individual reward in _Contract_ and _Negotiation_, normalized by the Oracle reward.

#### 5.3.4 Llm-C in _AdaSociety_

LLM-C runs three times for each task. Tab. 3 and Tab. 9 presents the quantitative results across various metrics. Benefiting from the embedded commonsense reasoning and social intelligence of LLMs, LLM-C exhibits outstanding performance in all three mini-games, achieving average rewards nearly surpassing all RL-based methods. After being informed of the game rules and the capability differences between carpenters and miners, LLM-C can accurately recognize the importance of cooperation and swiftly form alliances with other players through negotiation or contract. During the physical stage, manually coded controllers complement LLM's deficiencies in path planning and position judgment, precisely and efficiently realizing the high-level planning generated by the LLM based on the current social structure and physical environment. However, due to common issues with LLMs such as hallucinations, context length limitations, and randomness in outputs, LLM-C does not achieve Oracle performance, and it underperforms compared to CL in _Contract_-Easy, further validating the effectiveness of our proposed CL approach.

## 6 Related Work

**Environments.** Several craft-based environments like Malmo , Crafter , Minedojo  and Conan  create dynamic state and action spaces that expand with the agent's exploration, which, however, mainly focuses on single-agent setting. Environments including MAgent , XLand , and Miniworld  provide a set of different and transferable tasks that build from basic elements, and they are open for customization. Melting Pot  contains a set of over 50 MARL learning substrates with limited customizability. Interactive games including AI Economist , Overcooked , MPE , Neural MMO , and SMAC  place agents in diverse systems allowing them to compete or cooperate. Other examples, such as Diplomacy , focus on communication between agents. None of these environments contain both dynamic social connections and adaptive tasks like _AdaSociety_.

**Unsupervised Environment Design (UED).** In the paradigm of UED [16; 40; 30], the environment learns a policy \(:(^{T})\), which is a function from agent policy \(\) to the environment's parameters \(^{}\). Such a policy will automatically produce a distribution over solvable environments and further support the continued learning of the agent's policy. _AdaSociety_ does not implement UED to produce diverse tasks. Unlike UED, _AdaSociety_ has no goals or objectives, like most ecological systems, and produces multiple tasks through adaptive social structures and expanding physical surroundings.

**Structured multi-agent systems.** In multi-agent systems, various connections may be formed between agents, and these connections may form certain structures. ,  and  focus on finding communication topology for multi-agent coordination. Some research models the locality of interaction and learns a joint value via coordination graphs [24; 8; 35]. Networked MARL [63; 46; 47; 62; 50] learns localized policies on environments where agent interactions are contingent upon their connections within a static graph. We focus on dynamic agent connections which shape agents' rewards and observations, and these connections are modeled as a multi-layer graph.

## 7 Conclusion

We introduce _AdaSociety_, a multi-agent environment featuring expanding physical surroundings and adaptive social connections. The environment is capable of generating multiple tasks in adaptation to agents' behavior. _AdaSociety_ is friendly to tensor-based and LLM-based methods. _AdaSociety_ provides interfaces supporting superb customization and also offers a set of mini-games with diverse social connections. We test several RL and LLM-based algorithms in mini-games. Preliminary results indicate that _AdaSociety_ maintains a rational complexity level for current decision-making methods.

    & _Social Structure_ & _Contract_ & _Negotiation_ \\  Easy & - & 0.8433\(\) 0.1312 & 0.8733\(\) 0.1116 \\ Hard & 0.7894\(\) 0.0444 & 0.6499\(\) 0.1716 & 0.6862\(\) 0.1027 \\   

Table 3: Average reward of LLM-C across mini-games.

There are some limitations of _AdaSociety_ illuminating our future work. Human-machine interaction is crucial for the study of multi-agent systems, which is one of our key research objectives in _AdaSociety_. While the environment is temporarily not equipped with human interfaces, the current architecture does support the subsequent development of human-machine interfaces. In addition, our game spaces can be further expanded by introducing survival pressures (need for food, hostile creatures, and so on). These negative losses will penalize undesirable actions, complement the roles of positive rewards in reinforcing desirable behavior, and guide more diverse behavior.