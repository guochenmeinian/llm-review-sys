# Ctrl-X: Controlling Structure and Appearance for

Text-To-Image Generation Without Guidance

Kuan Heng Lin\({}^{1}\)   Sicheng Mo\({}^{1}\)   Ben Klingher\({}^{1}\)   Fangzhou Mu\({}^{2}\)   Bolei Zhou\({}^{1}\)

\({}^{1}\)University of California, Los Angeles   \({}^{2}\)NVIDIA

https://genforce.github.io/ctrl-x/

Indicates equal contribution38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

Recent controllable generation approaches such as FreeControl  and Diffusion Self-Guidance  bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules. However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use. This work presents _Ctrl-X_, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance. Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image. Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints. In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model.

Figure 1: **Guidance-free structure and appearance control of Stable Diffusion XL (SDXL) ** Ctrl-X enables training-free and guidance-free zero-shot control of pretrained text-to-image diffusion models given any structure conditions and appearance images.

Introduction

The rapid advancement of large text-to-image (T2I) generative models has made it possible to generate high-quality images with just one text prompt. However, it remains challenging to specify the exact concepts that can accurately reflect human intents using only textual descriptions. Recent approaches like ControlNet  and IP-Adapter  have enabled controllable image generation upon pretrained T2I diffusion models regarding structure and appearance, respectively. Despite the impressive results in controllable generation, these approaches [44; 25; 46; 20] require fine-tuning the entire generative model or training auxiliary modules on large amounts of paired data.

Training-free approaches [7; 24; 4] have been proposed to address the high overhead associated with additional training stages. These methods optimize the latent embedding across diffusion steps using specially designed score functions to achieve finer-grained control than text alone with a process called guidance. Although training-free approaches avoid the training cost, they significantly increase computing time and required GPU memory in the inference stage due to the additional backpropagation over the diffusion network. They also require sampling steps that are \(2\)-\(20\) times longer. Furthermore, as the expected latent distribution of each time step is predefined for each diffusion model, it is critical to tune the guidance weight delicately for each score function; Otherwise, the latent might be out-of-distribution and lead to artifacts and reduced image quality.

To tackle these limitations, we present _Ctrl-X_, a simple _training-free_ and _guidance-free_ framework for T2I diffusion with structure and appearance control. We name our method "Ctrl-X" because we reformulate the controllable generation problem by 'cutting' (and 'pasting') two tasks together: spatial structure preservation and semantic-aware stylization. Our insight is that diffusion feature maps capture rich spatial structure and high-level appearance from early diffusion steps sufficient for structure and appearance control without guidance. To this end, Ctrl-X employs feature injection and spatially-aware normalization in the attention layers to facilitate structure and appearance alignment with user-provided images. By being guidance-free, Ctrl-X eliminates additional optimization overhead and sampling steps, resulting in a \(35\)-fold increase in inference speed compared to guidance-based methods. Figure 1 shows sample generation results. Moreover, Ctrl-X supports arbitrary structure conditions beyond natural images and can be applied to any T2I and even text-to-video (T2V) diffusion models. Extensive quantitative and qualitative experiments, along with a user study, demonstrate the superior image quality and appearance alignment of our method over prior works.

We summarize our contributions as follows:

1. We present _Ctrl-X_, a simple plug-and-play method that builds on pretrained text-to-image diffusion models to provide disentangled and zero-shot control of structure and appearance during the generation process requiring no additional training or guidance.
2. Ctrl-X presents the first universal guidance-free solution that supports multiple conditional signals (structure and appearance) and model architectures (_e.g_. text-to-image and text-to-video).
3. Our method demonstrates superior results in comparison to previous training-based and guidance-based baselines (_e.g_. ControlNet + IP-Adapter [44; 43] and FreeControl ) in terms of condition alignment, text-image alignment, and image quality.

## 2 Related work

Diffusion structure control.Previous spatial structure control methods can be categorized into two types (training-based _vs_. training-free) based on whether they require training on paired data.

_Training-based structure control methods_ require paired condition-image data to train additional modules or fine-tune the entire diffusion network to facilitate generation from spatial conditions [44; 25; 20; 46; 42; 3; 47; 38; 49]. While pixel-level spatial control can be achieved with this approach, a significant drawback is needing a large number of condition-image pairs as training data. Although some condition data can be generated from pretrained annotators (_e.g_. depth and segmentation maps), other condition data is difficult to obtain from given images (_e.g_. 3D mesh, point cloud), making these conditions challenging to follow. Compared to these training-based methods, Ctrl-X supports conditions where paired data is challenging to obtain, making it a more flexible and effective solution.

_Training-free structure control methods_ typically focus on specific conditions. For example, R&B  facilitates bounding-box guided control with region-aware guidance, and DenseDiffusion  gen erates images with sparse segmentation map conditions by manipulating the attention weights. Universal Guidance  employs various pretrained classifiers to support multiple types of condition signals. FreeControl  analyzes semantic correspondence in the subspace of diffusion features and harnesses it to support spatial control from any visual condition. While these approaches do not require training data, they usually need to compute the gradient of the latent to lower an auxiliary loss, which requires substantial computing time and GPU memory. In contrast, Ctrl-X requires no guidance at the inference stage and controls structure via direct feature injections, enabling faster and more robust image generation with spatial control.

Diffusion appearance control.Existing appearance control methods that build upon pretrained diffusion models can also similarly be categorized into two types (training-based _vs._ training-free).

_Training-based appearance control methods_ can be divided into two categories: Those trained to handle any image prompt and those overfitting to a single instance. The first category  trains additional image encoders or adapters to align the generated process with the structure or appearance from the reference image. The second category  is typically applied to customized visual content creation by finetuning a pretrained text-to-image model on a small set of images or binding special tokens to each instance. The main limitation of these methods is that the additional training required makes them unscalable. However, Ctrl-X offers a scalable solution to transfer appearance from any instance without training data.

_Training-free appearance control methods_ generally follow two approaches: One approach  manipulates self-attention features using pixel-level dense correspondence between the generated image and the target appearance, and the other  extracts appearance embeddings from the diffusion network and transfers the appearance by guiding the diffusion process towards the target appearance embedding. A key limitation of these approaches is that a single text-controlled target cannot fully capture the details of the target image, and the latter methods require additional optimization steps. By contrast, our method exploits the spatial correspondence of self-attention layers to achieve semantically-aware appearance transfer without targeting specific subjects.

## 3 Preliminaries

Diffusion models are a family of probabilistic generative models characterized by two processes: The _forward process_ iteratively adds Gaussian noise to a clean image \(_{0}\) to obtain \(_{t}\) for time step \(t[1,T]\), which can be reparameterized in terms of a noise schedule \(_{t}\) where

\[_{t}=}_{0}+}\] (1)

for \((0,)\); The _backward process_ generates images by iteratively denoising an initial Gaussian noise \(_{T}(0,)\), also known as diffusion sampling . This process uses a parameterized denoising network \(_{}\) conditioned on a text prompt \(\), where at time step \(t\) we obtain a cleaner \(_{t-1}\)

\[_{t-1}=}}_{0}+ }_{}(_{t} t,),}_{0 }:=_{t}-}_{}(_{t}  t,)}{}}.\] (2)

Formally, \(_{}(_{t} t,)-_{t}_{ } p_{t}(_{t} t,)\) approximates a score function scaled by a noise schedule \(_{t}\) that points toward a high density of data, i.e., \(_{0}\), at noise level \(t\).

Figure 2: **Visualizing early diffusion features.** Using \(20\) real, generated, and condition images of animals, we extract Stable Diffusion XL  features right after decoder layer \(0\) convolution. We visualize the top three principal components computed for each time step across all images. \(t=961\) to \(881\) correspond to inference steps \(1\) to \(5\) of the DDIM scheduler with \(50\) time steps. We obtain \(_{t}\) by directly adding Gaussian noise to each clean image \(_{0}\) via the diffusion forward process.

Guidance.The iterative inference of diffusion enables us to guide the sampling process on auxiliary information. _Guidance_ modifies Equation 2 to compose additional score functions that point toward richer and specifically conditioned distributions [4; 7], expressed as

\[_{}(_{t} t,)=(_{t } t,)-s\,(_{t} t,y),\] (3)

where \(\) is an energy function and \(s\) is the guidance strength. In practice, \(\) can range from classifier-free guidance (where \(=\) and \(y=\), _i.e._ the empty prompt) to improve image quality and prompt adherence for T2I diffusion [12; 29], to arbitrary gradients \(_{_{t}}((_{t} t, ) t,y)\) computed from auxiliary models or diffusion features common to guidance-based controllable generation [4; 7; 24]. Thus, guidance provides great customizability on the type and variety of conditioning for controllable generation, as it only requires any loss that can be backpropagated to \(_{t}\). However, this backpropagation requirement often translates to slow inference time and high memory usage. Moreover, as guidance-based methods often compose multiple energy functions, tuning the guidance strength \(s\) for each \(\) may be finicky and cause issues of robustness. Thus, Ctrl-X avoids guidance and provides instant applicability to larger T2I and T2V models with minor hyperparameter tuning.

Diffusion U-Net architecture.Many pretrained T2I diffusion models are text-conditioned U-Nets, which contain an encoder and a decoder that downsample and then upsample the input \(_{t}\) to predict \(\), with long skip connections between matching encoder and decoder resolutions [13; 29; 27]. Each encoder/decoder block contains convolution layers, self-attention layers, and cross-attention layers: The first two control both structure and appearance, and the last injects textual information. Thus, many training-free controllable generation methods utilize these layers, through direct manipulation [11; 36; 18; 1; 41] or for computing guidance losses [7; 24], with self-attention most commonly used: Let \(_{l,t}^{(hw) c}\) be the diffusion feature with height \(h\), width \(w\), and channel size \(c\) at time step \(t\) right before attention layer \(l\). Then, the self-attention operation is

\[:=_{l,t}_{l}^{Q}:= _{l,t}_{l}^{K}:=_{ l,t}_{l}^{V},\] (4)

where \(_{l}^{Q},_{l}^{K},_{l}^{V}^{c d}\) are linear transformations which produce the query \(\), key \(\), and value \(\), respectively, and \(\) is applied across the second \((hw)\)-dimension. (Generally, \(c=d\) for diffusion models.) Intuitively, the attention map \(^{(hw)(hw)}\) encodes how each pixel in \(\) corresponds to each in \(\), which then rearranges and weighs \(\). This correspondence is the basis for Ctrl-X's spatially-aware appearance transfer.

## 4 Guidance-free structure and appearance control

Ctrl-X is a general framework for training-free, guidance-free, and zero-shot T2I diffusion with structure and appearance control. Given a structure image \(^{}\) and appearance image \(^{}\), Ctrl-X manipulates a pretrained T2I diffusion model \(_{}\) to generate an output image \(^{}\) that inherits the structure of \(^{}\) and appearance of \(^{}\).

Method overview.Our method is illustrated in Figure 3 and is summarized as follows: Given clean structure and appearance lentents \(^{}=_{0}^{}\) and \(^{}=_{0}^{}\), we first directly obtain noised structure and appearance latents \(_{t}^{}\) and \(_{t}^{}\) via the diffusion forward process, then extract their U-Net features from a pretrained T2I diffusion model. When denoising the output latent \(_{t}^{}\), we inject convolution and self-attention features from \(_{t}^{}\) and leverage self-attention correspondence to transfer spatially-aware appearance statistics from \(_{t}^{}\) to \(_{t}^{}\) to achieve structure and appearance control.

### Feed-forward structure control

Structure control of T2I diffusion requires transferring structure information from \(^{}=_{0}^{}\) to \(_{t}^{}\), especially during early time steps. To this end, we initialize \(_{T}^{}=_{T}^{}(0, )\) and obtain \(_{t}^{}\) via the diffusion forward process in Equation 1 with \(_{0}^{}\) and randomly sampled \((0,)\). Inspired by the observation where diffusion features contain rich layout information [36; 18; 24], we perform feature and self-attention injection as follows: For U-Net layer \(l\) and diffusion time step \(t\), let \(_{l,t}^{}\) and \(_{l,t}^{}\) be features/activations after the convolution block from \(_{t}^{}\) and \(_{t}^{}\), and let \(_{l,t}^{}\) and \(_{l,t}^{}\) be the attention maps of the self-attention block from \(_{t}^{}\) and \(_{t}^{}\). Then, we replace

\[_{l,t}^{}_{l,t}^{} {and}_{l,t}^{}_{l,t}^{}.\] (5)In contrast to [36; 18; 24], we do not perform inversion and instead directly use forward diffusion (Equation 1) to obtain \(_{t}^{}\). We observe that \(_{t}^{}\) obtained via the forward diffusion process contains sufficient structure information even at _very_ early/high time steps, as shown in Figure 2. This also reduces appearance leakage common to inversion-based methods observed by FreeControl . We study our feed-forward structure control method in Sections 5.1 and 5.2.

We apply feature injection for layers \(l L^{}\) and self-attention injection for layers \(l L^{}\), and we do so for (normalized) time steps \(t^{}\), where \(^{}\) is the structure control schedule.

### Spatially-aware appearance transfer

Inspired by prior works that define appearance as feature statistics [15; 21], we consider appearance transfer to be a stylization task. T2I diffusion self-attention transforms the value \(\) with attention map \(\), where the latter represents how pixels in \(\) corresponds to pixels in \(\). As observed by Cross-Image Attention , \(^{}\) can represent the semantic correspondence between two images when \(\) and \(\) are computed from features from each, even when the two images differ significantly in structure. Thus, inspired by AdaAttN , we propose spatially-aware appearance transfer, where we exploit this correspondence to generate self-attention-weighted mean and standard deviation maps from \(_{t}^{}\) to normalize \(_{t}^{}\): For any self-attention layer \(l\), let \(_{l,t}^{}\) and \(_{l,t}^{}\) be diffusion features right before self-attention for \(_{t}^{}\) and \(_{t}^{}\), respectively. Then, we compute the attention map

\[=(^{}^{ }}{}),^{}:=(_{l,t}^{})_{l}^{Q} ^{}:=(_{l,t}^{}) _{l}^{K},\] (6)

where \(\) is applied across spatial dimension \((hw)\). Notably, we normalize \(_{l,t}^{}\) and \(_{l,t}^{}\) first to remove appearance statistics and thus isolate structural correspondence. Then, we compute the mean and standard deviation maps \(\) and \(\) of \(_{l,t}^{}\) weighted by \(\) and use them to normalize \(_{l,t}^{}\),

\[_{l,t}^{}_{l,t}^{ }+,:=_{l,t}^{}:=(_{l,t}^{} _{l,t}^{})-()}.\] (7)

\(\) and \(\), weighted by structural correspondences between \(^{}\) and \(^{}\), are spatially-aware feature statistics of \(_{t}^{}\) which are transferred to \(_{t}^{}\). Lastly, we perform layer \(l\) self-attention on \(_{l,t}^{}\) as normal.

We apply appearance transfer for layers \(l L^{}\), and we do so for (normalized) time steps \(t^{}\), where \(^{}\) is the appearance control schedule.

Figure 3: **Overview of Ctrl-X.** (a) At each sampling step \(t\), we obtain \(_{t}^{}\) and \(_{t}^{}\) via the forward diffusion process, then feed them into the T2I diffusion model to obtain their convolution and self-attention features. Then, we inject convolution and self-attention features from \(_{t}^{}\) and leverage self-attention correspondence to transfer spatially-aware appearance statistics from \(_{t}^{}\) to \(_{t}^{}\). (b) Details of our spatially-aware appearance transfer, where we exploit self-attention correspondence between \(_{t}^{}\) and \(_{t}^{}\) to compute weighted feature statistics \(\) and \(\) applied to \(_{t}^{}\).

Structure and appearance control.Finally, we replace \(_{}\) in Equation 2 with

\[_{}(_{t}^{} t,,\{ _{l,t}^{}\}_{l L^{}},\{_{I,t}^{ }\}_{l L^{}},\{_{I,t}^{}\}_{l L ^{}}),\] (8)

where \(\{_{l,t}^{}\}_{l L^{}}\), \(\{_{I,t}^{}\}_{l L^{}}\), and \(\{_{I,t}^{}\}_{l L^{}}\) respectively correspond to \(_{t}^{}\) features for feature injection, \(_{t}^{}\) attention maps for self-attention injection, and \(_{t}^{}\) features for appearance transfer.

## 5 Experiments

We present extensive quantitative and qualitative results to demonstrate the structure preservation and appearance alignment of Ctrl-X on T2I diffusion. Appendix A contains more implementation details.

### T2I diffusion with structure and appearance control

Baselines.For training-based methods, ControlNet  and T2I-Adapter  learn an auxiliary module that injects a condition image into a pretrained diffusion model for structure alignment. We then combine them with IP-Adapter , a trained module for image prompting and thus appearance transfer. Uni-ControlNet  adds a feature extractor to ControlNet to achieve multi-image structure control of selected condition types, along with image prompting for global/appearance control. Splicing ViT Features  trains a U-Net from scratch per source-appearance image pair to minimize their DINO-ViT self-similarity distance and global [CLS] token loss. (For structure conditions not supported by a training-based baseline, we convert them to canny edge maps.) For guidance-based methods, FreeControl  enforce structure and appearance alignment via backpropagated score functions computed from diffusion feature subspaces. For guidance-free methods, Cross-Image Attention  manipulates attention weights to transfer appearance while maintaining structure. We run all methods on SDXL v1.0  when possible and on their default base models otherwise.

Dataset.Our method supports T2I diffusion with appearance transfer and arbitrary-condition structure control. Since no benchmarks exist for such a flexible task, we create a new dataset comprising \(256\) diverse structure-appearance pairs. The structure images consist of \(31\%\) natural

Figure 4: **Qualitative results for T2I diffusion structure and appearance control and conditional generation.** Ctrl-X supports a diverse variety of structure images for both (a) structure and appearance controllable generation and (b) prompt-driven conditional generation.

images, \(49\%\) ControlNet-supported conditions (_e.g_. canny, depth, segmentation), and \(20\%\) in-the-wild conditions (_e.g_. 3D mesh, point cloud), and the appearance images are a mix of Web and generated images. We use templates and hand-annotation for the structure, appearance, and output text prompts.

Evaluation metrics.For quantitative evaluation, we report two widely-adopted metrics: _DINO Self-sim_ measures the self-similarity distance  between the structure and output image in the DINO-ViT  feature space, where a lower distance indicates better structure preservation; _DINO-J_ measures the cosine similarity between the DINO-ViT [CLS] tokens of the appearance and output images , where a higher score indicates better appearance transfer.

Qualitative results.As shown in Figures 4 and 5, Ctrl-X faithfully preserves structure from structure images ranging from natural images and ControlNet-supported conditions (_e.g_. HED, segmentation) to in-the-wild conditions (_e.g_. wireframe, 3D mesh) not possible in prior training-based methods while adeptly transferring appearance from the appearance image with semantic correspondence. Moreover, as shown in Figure 6, Ctrl-X is capable of multi-subject generation, capturing strong semantic correspondence between different subjects and the background, achieving balanced structure and appearance alignment. On the contrary, ControlNet + IP-Adapter  often fails to maintain the structure and/or transfer the subjects' or background's appearances.

Comparison to baselines.Figure 5 and Table 2 compare Ctrl-X to the baselines for qualitative and quantitative results, respectively. Moreover, our user study in Table 4, Appendix A shows the human preference percentages of how often participants preferred Ctrl-X over each of the baselines on result quality, structure fidelity, appearance fidelity, and overall fidelity.

For training-based and guidance-based methods, despite Uni-ControlNet  and FreeControl's  stronger structure preservation (smaller DINO self-similarity), they generally struggle to enforce faithful appearance transfer and yield worse DINO-I scores, which is particularly visible in Figure 5 row 1 and 3. Since the training-based methods combine a structure control module (ControlNet  and T2I-Adapter ) with a separately-trained appearance transfer module IP-Adapter , the two modules sometimes exert conflicting control signals at the cost of appearance transfer (_e.g_. row 1)--and for ControlNet, structure preservation as well. For Uni-ControlNet, compressing the

Figure 5: **Qualitative comparison of structure and appearance control.** Ctrl-X displays comparable structure control and superior appearance transfer compared to training-based methods. It is also more robust than guidance-based and guidance-free methods across diverse structure types.

[MISSING_PAGE_FAIL:8]

average inference time using a single NVIDIA H100 GPU. Ctrl-X is slightly slower than training-based ControlNet (\(1.76\)) and T2I-Adapter (\(2.50\)) with IP-Adapter yet significantly faster than per-image-trained Splicing ViT (\(0.0070\)), guidance-based FreeControl (\(0.029\)), and guidance-free Cross-Image Attention (\(0.25\)). Moreover, for methods with SDXL v1.0 as the base model, Ctrl-X has lower peak GPU memory usage than training-based methods and significantly lower memory than training-free methods. Our training-free and guidance-free method achieves comparable run time and peak GPU memory usage compared to training-based methods, indicating its flexibility.

Extension to prompt-driven conditional generation.Ctrl-X also supports prompt-driven conditional generation, where it generates an output image complying with the given text prompt while aligning with the structure from the structure image, as shown in Figures 4 and 7. Inspired by FreeControl , instead of a given \(^{}\), Ctrl-X can jointly generate \(^{}\) based on the text prompt alongside \(^{}\), where we obtain \(^{}_{t-1}\) via denoising with Equation 2 from \(^{}_{t}\) without control. Baselines, qualitative and quantitative analysis, and implementation details are available in Appendix C.

Extension to video diffusion models.Ctrl-X is training-free, guidance-free, and demonstrates competitive runtime. Thus, we can directly apply our method to text-to-video (T2V) models, as seen in Figure 17, Appendix D. Our method closely aligns the structure between the structure and output videos while transferring temporally consistent appearance from the appearance image.

### Ablations

Effect of control.As seen in Figure 8(a), structure control is responsible for structure preservation (appearance-only _vs_. ours). Also, structure control alone cannot isolate structure information, display

    &  &  &  &  \\   & & Self-sim \(\) & DINO-I \(\) & Self-sim \(\) & DINO-I \(\) & Self-sim \(\) & DINO-I \(\) \\  Splicing ViT Features  & ✓ & 0.030 & 0.907 & 0.003 & 0.864 & 0.037 & 0.866 \\ Uni-ControlNet  & ✓ & **0.045** & 0.555 & **0.096** & 0.574 & **0.073** & 0.506 \\ ControlNet + IP-Adapter  & ✓ & 0.068 & _0.636_ & 0.136 & _0.686_ & 0.139 & _0.667_ \\ T2I-Adapter + IP-Adapter  & ✓ & _0.055_ & 0.603 & 0.118 & 0.586 & 0.109 & 0.566 \\ Cross-Image Attention  & ✗ & 0.145 & 0.651 & 0.196 & 0.510 & 0.175 & 0.570 \\ FreeControl  & ✗ & 0.058 & 0.572 & _0.101_ & 0.585 & _0.089_ & 0.567 \\
**Ctrl-X (ours)** & ✗ & 0.057 & **0.686** & 0.121 & **0.698** & 0.109 & **0.676** \\   

Table 2: **Quantitative comparison of structure and appearance control.Ctrl-X consistently outperforms both training-based and training-free methods in appearance alignment and shows comparable or better structure preservation compared to training-based and guidance-free methods, measured by DINO ViT self-similarity  and DINO-I , respectively.**

Figure 7: **Qualitative comparison of conditional generation.Ctrl-X displays comparable structure control and superior prompt alignment to training-based methods, and it also has better image quality and is more robust than guidance-based and guidance-free methods across different conditions.**

ing strong structure image appearance leakage and poor-quality outputs (structure-only _vs._ ours), as it merely injects structure features, which creates the semantic correspondence for appearance control.

**Appearance transfer method.** As we consider appearance transfer as a stylization task, we compare our appearance statistics transfer with and without attention weighting in Figure 8(b). Without weighting (equivalent to AdaIN ), we have global normalization which ignores the semantic correspondence between the appearance and output images, so the outputs are low-contrast.

**Effect of inversion.** We compare DDIM inversion _vs._ forward diffusion (ours) to obtain \(_{T}^{}=_{T}^{}\) and \(_{t}^{}\) in Figure 8(c). Inversion displays appearance leakage from structure images in challenging conditions (left) while being similar to our method in others (right). Considering inversion costs and additional model inference time, forward diffusion is a better choice for our method.

## 6 Conclusion

We present Ctrl-X, a training-free and guidance-free framework for structure and appearance control of any T2I and T2V diffusion model. Ctrl-X utilizes pretrained T2I diffusion model feature correspondences, supports arbitrary structure image conditions, works with multiple model architectures, and achieves competitive structure preservation and superior appearance transfer compared to training- and guidance-based methods while enjoying the low overhead benefits of guidance-free methods. As shown in Figure 9, the key limitation of Ctrl-X is the semantic-aware appearance transfer method may fail to capture the target appearance when the instance is small because of the low resolution of the feature map. We hope our method and findings can unveil new possibilities and research on controllable generation as generative models become bigger and more capable.

**Broader impacts.** Ctrl-X makes controllable generation more accessible and flexible by supporting multiple conditional signals (structure and appearance) and model architectures without the computational overhead of additional training or optimization. However, this accessibility also makes using pretrained T2I/T2V models for malicious applications (_e.g_. deepfakes) easier, especially since the controllability enables users to generate specific images and raises ethical concerns with consent and crediting artists for using their work as condition images. In response to these safety concerns, T2I and T2V models have become more secure. Likewise, Ctrl-X can inherit the same safeguards, and its plug-and-play nature allows the open-source community to scrutinize and improve its safety.

**Acknowledgements.** This work was supported by the NSF Grants CCRI-2235012 and RI-2339769, the UCLA-Amazon Science Hub, and the Intel Rising Star Faculty Award.

Figure 8: **Ablations.** We study ablations on control, appearance transfer method, and inversion.

Figure 9: **Limitations.** Ctrl-X can struggle with localizing the corresponding subject in the appearance image with appearance transfer when the subject is too small.