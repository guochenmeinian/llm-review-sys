# Event-Based Contrastive Learning

for Medical Time Series

Nassim Oufattole\({}^{1}\)1 Hyewon Jeong\({}^{1}\)1 Aparna Balagopalan\({}^{1}\) Matthew Mcdermott\({}^{2}\)

**Payal Chandak\({}^{1,2}\) Marzyeh Ghassemi\({}^{1}\) Collin Stultz\({}^{1,2}\) \({}^{1}\) MIT \({}^{2}\) Harvard {nassim, hyewonj, aparnab, chandak, mghassem, cmstultz}@mit.edu matthew_mccdermott@hms.harvard.edu**

###### Abstract

In clinical practice, one often needs to identify whether a patient is at high risk of adverse outcomes after some key medical event; e.g., the short-term risk of death after an admission for heart failure. This task, however, remains challenging due to the complexity, variability, and heterogeneity of longitudinal medical data, especially for individuals suffering from chronic diseases like heart failure. In this paper, we introduce Event-Based Contrastive Learning (EBCL) - a method for learning embeddings of heterogeneous patient data that preserves temporal information before and after key index events. We demonstrate that EBCL produces models that yield better fine-tuning performance on critical downstream tasks including 30-day readmission, 1-year mortality, and 1-week length of stay relative to other representation learning methods that do not exploit temporal information surrounding key medical events.

## 1 Introduction

Outcomes for patients with heart failure are highly variable, making accurate predictions challenging . Nevertheless, reliable outcome prediction is important for the development of effective treatment strategies  and for ensuring that healthcare resources are allocated appropriately, thereby ensuring that the most resources are made available to the sickest patients . Prior work has leveraged data within the Electronic Health Record (EHR) for this task. Indeed, a variety of learning algorithms, including directly supervised , self-supervised , weakly-supervised , generative pretraining , and contrastive learning algorithms  have all been used to identify patients at the highest risk of adverse outcomes. A central feature of many predictive tasks in medicine is that one aspires to identify a patient's risk after some index event. Case in point, the ability to identify patients at high risk of death after a myocardial infarction (heart attack) forms an important part of the evaluation and care of patients with coronary artery disease. Although a number of methods use elements of the electronic record for this task, they typically do not strive to encode the temporal information surrounding the event of interest . For example, how do patient features change pre- and post-index event? As such data often include information about the patient's response to therapy, we postulated that predictive performance could be improved, relative to existing methods, when such temporal information is explicitly modeled around the index event.

We therefore introduce a novel approach using temporal contrastive learning to emphasize the consistency of medical trajectory representations around key medical events. Our approach diverges from existing work  by imposing our specialized pre-training contrastive loss solely on data around critical events, where the most prognostic information regarding disease progression and prognosis is likely to be found. In particular, our contrastive system is trained to differentiate positivepairs - patient data before and after an event - from negative pairs - data from separate events (these separate events can be for either different patients or the same patient). This framework aids the model in learning consistent representations of patient history across these key events. We postulate that these learned representations can enhance our ability to capture long-term longitudinal trends in patient states and improve outcome predictions in patients with chronic diseases such as heart failure. We apply the method across several fine-tuning tasks, including 30-day readmission [18; 19; 20; 21], 1-year mortality [22; 23; 24; 25], and 1-week length of stay (LOS) prediction over a multi-center cohort of patients with heart failure. Consistently, our temporally aware contrastively pretrained model outperforms existing representation learning methods across these tasks.

Related WorksContrastive learning is a form of self-supervised learning (SSL). It utilizes automatically co-occurring sources of information ("multi-modal") [26; 27; 28; 29] or augmented versions of the same information ("multi-view") as a supervision signal [30; 31; 32; 33; 16; 27; 34; 35; 36]. Specifically, paired observations, created using multiple data modalities or augmented/mined views of the same object, are contrasted against negative observation pairs created using multi-source/view information for different objects through losses such as the InfoNCE  or CLIP loss .

A recent contrastive learning scheme called Order Contrastive Pretraining (OCP)  builds on prior works such as Permutation-Contrastive Learning (PCL)  and takes two consecutive windows of data in their original temporal sequence to construct positive pairs. Negative pairs correspond to a sequence where the temporal order of sequences is swapped. Importantly, the sequences chosen to be swapped are not chosen with respect to any particular event, while our method is event-centric.

## 2 Methods

### Event-Based Contrastive Learning for Medical Outcome Prediction

We present a novel contrastive pretraining method called Event-Based contrastive learning (EBCL), which leverages medical time-series data around the time of a critical event.

Problem FormulationLet \(t_{n}^{i}\) denote the \(n^{th}\) clinically significant event (e.g., inpatient admission) for patient \(i\). We note that a single patient may have more than one clinically significant event. For example, a patient with heart failure may have several hospital admissions with shortness of breath, where each admission corresponds to a clinically significant event. Pretraining data \(_{}=\{(_{n}^{i},_{n}^{i}):n<N_{i}\:\&\:i<P\}\), where \(_{n}^{i}\) (pre-event data) corresponds to data before (index event) \(_{n}^{i}\) (post-event data) corresponds to data after \(t_{n}^{i}\) until the end of the event. \(N_{i}\) is the number of events associated with patient \(i\), and \(P\) is the total number of patients in the dataset.

EBCL PretrainingWe encode \(_{n}^{i}\) and \(_{n}^{i}\) using missingness-aware triplet embedding  with feature, value, and time embeddings. We feed the input triplet embedding into the model, \(f_{}\), and get pre- and post-embeddings \(f_{}(_{n}^{i})\) and \(f_{}(_{n}^{i})\). We then compute the CLIP loss \(_{}\) on a batch of these embeddings. Note that \((_{k}^{i},_{m}^{j})\) is a positive pair if \(i=j\) and \(k=m\).

FinetuningWe finetune \(f_{}\) on the tasks defined in Section 2.2 using a dataset of time-series embedding with matched binary outcome labels: \(_{}=\{(_{n}^{i},_{n}^{i},y_{n}^{i}):n<N_{i}\ \&\ i<P\}\). We use negative cross-entropy loss, \(_{}\), and pass our embeddings, \(f_{}(_{n}^{i})\) and \(f_{}(_{n}^{i})\) through a finetuning architecture depicted in Figure 2(a) (in appendix) to arrive at a prediction for our label.

### Dataset and Tasks

We have assembled a cohort of 107,268 heart failure patients with 383,254 inpatient admissions, obtained from the electronic data warehouse of a large hospital network. We preprocess these data using a preprocessing pipeline inspired by ESGPT . The dataset includes patient trajectories over a maximum span of 40 years and a maximum number of 3275 features, which includes labs, diagnoses, procedures, medications, tabular echocardiogram recordings, physical measurements, and admissions/discharges. See Appendix Table 3 for the pretraining dataset statistics.

We divide the patient trajectories into Pre-Event and Post-Event data, where the Event is an inpatient admission. We finetune and evaluate on three downstream tasks where the datasets are summarized in Table 1. Note that for the LOS task, we only use Pre-Admission data as input, as Post-Admission data would leak the LOS outcome, so \(_{}=\{(_{n}^{i},y_{n}^{i}):n<N_{i}\ \&\ i<P\}\). We also always restrict Post-Admission data, \(_{n}^{i}\), to the data prior to patient discharge, as this is the information that will be available at decision time for the mortality and readmission tasks. We partitioned our compiled dataset into training (80%), validation (10%) and testing (10%) with the split determined by individual patient instances. Additional dataset information is provided in appendix section A

### Experiments

Our transformer has two encoder layers, a 512 sequence length for pre- and post-event data, a 2,048-dimension feed-forward layer between self-attention layers, and 32-dimension token embeddings. We then perform Fusion Self-Attention , by taking an attention-weighted average of the output embeddings of the transformer to get a single 32-dimension embedding. Finally, we have a linear projection to a 32-dimension embedding. Sequences are padded to be length 512 and attention over padded tokens is masked in the transformer and the Fusion Self-Attention layer.

To evaluate our method we perform the following experiments:

* **Supervised**: This corresponds to standard supervised training without EBCL pretraining. The model \(f_{}\) is initialized with random weights and then the model is trained in a supervised fashion for a specific task. We use the architecture in figure 2(a).
* **Order Contrastive Pretraining (OCP)**: OCP is a self-supervised approach that implements a pretraining task of discriminating correct and switched sequencing. We pretrain \(f_{}\) with the OCP objective where for each patient \(i\), we take a continuous sequence of at most 512 tokens, split the sequence in half, and randomly swap the first and second halves (Figure 2). We denote this randomly sampled and swapped sequence as \(^{i}\) and display this pretraining architecture in Appendix Figure 2(d). Due to slow convergence, we allow up to a maximum of 600 epochs for pretraining. For fine-tuning, we concatenate or "fuse" \(_{n}^{i}\) and

   Task & \# Patients & \# Events & \# Prevalence \\ 
30-Day Readmission & 60,287 & 215,097 & 17.6\% \\
1-Year Mortality & 49,962 & 183,125 & 27.7\% \\
1-Week LOS & 107,268 & 383,254 & 54.1\% \\   

Table 1: **Statistics for finetuning datasets.**\(^{i}_{n}\) and pass the concatenated trajectory into the model \(f_{}\) (Figure 2(b)). For this experiment, \(^{i}_{n}\) and \(^{i}_{n}\) individually can only have a sequence length of \(256\) as \(f_{}\) has a max sequence length of \(512\). For more information see Appendix Section A.
* **Event-Based Contrastive Learning (EBCL)**: Our event-centric temporal contrastive pretrained model. We show this pretraining architecture in Appendix Figure 2(c).

We finetune the pretrained contrastive learning models (OCP, EBCL) with fully connected layers to predict outcomes. Additionally, to evaluate the utility of the representations learned from pretraining, we do an additional experiment where we freeze the model weights for \(f_{}\) learned from pretraining (EBCL Frozen, OCP Frozen), and train a two-layer MLP on the output of \(f_{}\) for our finetuning task. We perform an extensive learning rate and dropout hyperparameter search for pretraining and finetuning and use a maximum of 100 epochs. We take the epoch with the highest validation set performance for pretraining and finetuning. We run 3 random seeds for pretraining and finetuning and report the mean and standard deviation of results across these seeds.

### Results

We summarize the evaluation of models on the finetuning tasks in Table 2. Our proposed method, EBCL, achieves a significant improvement over the fully supervised baseline and OCP approach. The OCP method showed worse performance compared to the Supervised model on predicting 1-year mortality and 1-week LOS. We note that pretraining methods often rely on leveraging a much larger pretraining dataset than the finetuning dataset to obtain performance improvements, but in our scheme, the pretraining dataset is similar in size to our downstream tasks, so the drive for the improvement is mainly coming from solving the contrastive learning task, and not seeing more data.

Additionally, we examine how freezing the OCP and EBCL pretrained weights affects performance relative to the supervised baseline. EBCL Frozen significantly outperforms OCP Frozen for all tasks, providing further evidence that EBCL is a superior pretraining scheme for these downstream tasks. Furthermore, EBCL Frozen outperforms Supervised for readmission and is comparable for 1-year mortality, indicating EBCL learns a useful patient representation around clinical events. There is a gap between the supervised baseline and EBCL for the LOS task, but the LOS task only uses pre-event data, which suggests that the post-data EBCL embedding may be more useful for downstream tasks than the pre-data EBCL embedding. Validating this will be a subject of future experiments. Overall these results indicate that EBCL provides a useful pretraining scheme for our downstream tasks.

## 3 Conclusion

In this paper, we propose EBCL, a novel pretraining scheme for medical time series data, to learn temporal representations around clinically significant events. We show that EBCL outperforms supervised baselines relative to standard supervised training and a related pretraining method, OCP, which is not based on clinically significant events.

**Limitations and Future Works.** Given this investigation is at an early stage, we did not conduct an extensive comparison of contrastive loss schemes for pre-event and post-event data, of different model

    &  &  &  \\   & AUC & APR & AUC & APR & AUC & APR \\  Supervised & 64.3 \(\) 1.1 & 89.0 \(\) 0.5 & 80.2 \(\) 0.7 & 90.7 \(\) 0.3 & 86.3 \(\) 0.6 & 85.7 \(\) 0.5 \\  OCP & 68.1 \(\) 0.5 & 90.6 \(\) 0.2 & 78.0 \(\) 0.7 & 89.6 \(\) 0.3 & 84.1 \(\) 0.7 & 83.7 \(\) 0.6 \\ OCP Frozen & 59.1 \(\) 0.5 & 86.2 \(\) 0.4 & 65.9 \(\) 0.4 & 81.9 \(\) 0.2 & 59.9 \(\) 0.1 & 55.3 \(\) 0.3 \\  EBCL & **70.4 \(\) 0.1** & **91.4 \(\) 0.1** & **82.3 \(\) 0.1** & **91.8 \(\) 0.0** & **87.2 \(\) 0.2** & **86.9 \(\) 0.3** \\ EBCL Frozen & 68.2 \(\) 0.3 & 90.6 \(\) 0.1 & 78.4 \(\) 0.1 & 90.0 \(\) 0.1 & 79.7 \(\) 0.1 & 79.2 \(\) 0.1 \\   

Table 2: **EBCL Pretraining improves results over a supervised baseline and OCP pretraining.** Additionally, the EBCL Frozen model consistently outperforms the OCP Frozen model and even outperforms the Supervised model for 30-day Readmission prediction.

architectures, or of results on different datasets. An important direction for future work is addressing these limitations and evaluating other transformer pretraining methods. While further experiments with different model architectures are important, our current results with several pretraining methods are encouraging.