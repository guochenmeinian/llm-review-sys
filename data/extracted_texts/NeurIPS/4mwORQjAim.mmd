# Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures

David Loiseaux\({}^{1}\), Luis Scoccola\({}^{2}\), Mathieu Carriere\({}^{1}\), Magnus B. Botnan\({}^{3}\), Steve Oudot\({}^{4}\)

\({}^{1}\)DataShape, Centre Inria d'Universite Cote d'Azur \({}^{2}\)Mathematics, University of Oxford

\({}^{3}\)Mathematics, Vrije Universiteit Amsterdam \({}^{4}\)GeomeriX, Inria Saclay and Ecole polytechnique

 Equal contribution.

###### Abstract

Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case--where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest--and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes--a recent family of MPH descriptors--as signed measures leads to natural extensions of vectorization strategies from one parameter to multiple parameters. The resulting feature vectors are easy to define and to compute, and provably stable. While, as a proof of concept, we focus on simple choices of signed barcodes and vectorizations, we already see notable performance improvements when comparing our feature vectors to state-of-the-art topology-based methods on various types of data.

## 1 Introduction

### Context

Topological Data Analysis (TDA)  is a field of data science that provides descriptors for geometric data. These descriptors encode _topological structures_ hidden in the data, as such they are complementary to more common descriptors. And since TDA methods usually require the sole knowledge of a metric or dissimilarity measure on the data, they are widely applicable. For these reasons, TDA has found successful applications in a wide range of domains, including, e.g., computer graphics , computational biology , or material sciences , to name a few.

The mathematical definition of TDA's topological descriptors relies on _persistent homology_, whose input is a simplicial complex (a special kind of hypergraph) filtered by an \(^{n}\)-valued function. The choice of simplicial complex and function is application-dependent, a common choice being the complete hypergraph on the data (or some sparse approximation) filtered by scale and/or by some density estimator. The sublevel-sets of the filter function form what is called a _filtration_, i.e., a family \(\{S_{x}\}_{x^{n}}\) of simplicial complexes with the property that \(S_{x} S_{x^{}}\) whenever \(x x^{}^{n}\) (whereby definition \(x x^{} x_{i} x^{}_{i}\; 1 i n\)). Applying standard simplicial homology  with coefficients in some fixed field \(\) to \(\{S_{x}\}_{x^{n}}\) yields what is called a _persistence module_, i.e., an \(^{n}\)-parametrized family of \(\)-vector spaces connected by \(\)-linear maps--formally, a _functor_ from \(^{n}\) to the category \(\) of \(\)-vector spaces. This module encodes algebraically the evolution of the topology through the filtration \(\{S_{x}\}_{x^{n}}\), but in a way that is neither succinct nor amenable to interpretation.

Concise representations of persistence modules are well-developed in the _one-parameter_ case where \(n=1\). Indeed, under mild assumptions, the modules are fully characterized by their _barcode_, which can be represented as a point measure on the extended Euclidean plane . The classical interpretation of this measure is that its point masses--also referred to as _bars_--encode the appearance and disappearance times of the topological structures hidden in the data through the filtration. Various _stability theorems_ ensure that this barcode representation is _stable_ under perturbations of the input filtered simplicial complex, where barcodes are compared using optimal transport-type distances, often referred to as Wasserstein or bottleneck distances. In machine learning contexts, barcodes are turned into vectors in some Hilbert space, for which it is possible to rely on the vast literature in geometric measure and optimal transport theories. A variety of stable vectorizations of barcodes have be designed in this way--see  for a survey.

There are many applications of TDA where _multiparameter persistence modules_ (that is, when \(n 2\)) are more natural than, and lead to improved performance when compared to, one-parameter persistence modules. These include, for example, noisy point cloud data , where one parameter accounts for the geometry of the data and another filters the data by density, and multifiltered graphs , where different parameters account for different filtering functions.

In the multiparameter however, the concise and stable representation of persistence modules is known to be a substantially more involved problem . The main stumbling block is that, due to some fundamental algebraic reasons, there is no hope for the existence of a concise descriptor like the barcode that can completely characterize multiparameter persistence modules. This is why research in the last decade has focused on proposing and studying incomplete descriptors--see, e.g.,  for a recent survey. Among these, the _signed barcodes_ stand out as natural extensions of the usual one-parameter barcode , being also interpretable as point measures. The catch however is that some of their points may have negative weights, so the measures are _signed_. As a consequence, their analysis is more delicate than that of one-parameter barcodes, and it is only very recently that their optimal transport-type stability has started to be understood , while there currently is still no available technique for turning them into vectors in some Hilbert space.

### Contributions

We believe the time is ripe to promote the use of signed barcodes for feature generation in machine learning; for this we propose the following pipeline (illustrated in Fig. 1):

\[\{\\ \}}\{ \\ \}}\{ \\ \}}\{ \\ \}}\{ \\ \}}\]

The choice of filtration being application-dependent, we will mostly follow the choices made in related work, for the sake of comparison--see also  for an overview of standard choices. As signed barcode descriptor, we will mainly use the _Hilbert decomposition signed measure_ (Definition 4), and when the simplicial complex is not too large we will also consider the _Euler decomposition signed measure_ (Definition 5). These two descriptors are arguably the simplest existing signed measure descriptors, so they will serve as a proof of concept for our pipeline. They also offer the advantage of being efficiently computable, with effective implementations already available . With these choices of signed measure descriptors, the only missing step in our pipeline is the last one--the vectorization. Here is the summary of our contributions, the details follow right after:

* We introduce two general vectorization techniques for signed barcodes (Definitions 6 and 7).
* We prove Lipschitz-continuity results (Theorems 1 to 3) that ensure the robustness of our entire feature generation pipeline.
* We illustrate the practical performance of our pipeline compared to other baselines in various supervised and unsupervised learning tasks.

**Vectorizations.** Viewing signed barcodes as signed point measures enables us to rely on the literature in signed measure theory in order to adapt existing vectorization techniques for usual barcodes to the signed barcodes in a natural way. Our first vectorization (Definition 6) uses convolution with a kernel function and is an adaptation of the _persistence images_ of ; see Fig. 1. Our second vectorization (Definition 7) uses the notion of slicing of measures of  and is an adaptation of the _sliced Wasserstein kernel_ of . Both vectorizations are easy to implement and run fast in practice; we assess the runtime of our pipeline in Appendix D. We find that our pipeline is faster than the other topological baselines by one or more orders of magnitude.

**Theoretical stability guarantees.** We prove that our two vectorizations are Lipschitz-continuous with respect to the Kantorovich-Rubinstein norm on signed measures and the norm of the corresponding Hilbert space (Theorems 2 and 3). Combining these results with the Lipschitz-stability of the signed barcodes themselves (Theorem 1) ensures the robustness of our entire feature generation pipeline with respect to perturbations of the filtrations.

**Experimental validation.** Let us emphasize that our choices of signed barcode descriptors are strictly weaker than the usual barcode in the one-parameter case. In spite of this, our experiments show that the performance of the one-parameter version of our descriptors is comparable to that of the usual barcode. We then demonstrate that this good behavior generalizes to the multiparameter case, where it is in fact amplified since our pipeline can outperform its multiparameter competitors (which rely on theoretically stronger descriptors and have been shown to perform already better than the usual one-parameter TDA pipeline) and is competitive with other, non-topological baselines, on a variety of data types (including graphs and time series). For a proof that our descriptors are indeed weaker than other previously considered descriptors, we refer the reader to Proposition 2 in Appendix A.

### Related work

We give a brief overview of related vectorization methods for multiparameter persistence; we refer the reader to Appendix B for more details.

The _multiparameter persistence kernel_ and _multiparameter persistence landscape_ restrict the multiparameter persistence module to certain families of one-parameter lines and leverage some of the available vectorizations for one-parameter barcodes. The _generalized rank invariant landscape_ computes the generalized rank invariant over a prescribed collection of intervals (called _worms_) then, instead of decomposing the invariant as we do, it stabilizes it using ideas coming from persistence landscapes . The _multiparameter persistence image_ decomposes the multiparameter persistence module into interval summands and vectorizes these summands individually. Since this process is known to be unstable, there is no guarantee on the stability of the corresponding vectorization. The _Euler characteristic surfaces_ and the methods of  do not work at the level of persistence modules but rather at the level of filtered simplicial complexes, and are based on the computation of the Euler characteristic of the filtration at each index in the multifiltration. These methods are very efficient when the filtered simplicial complexes are small, but

Figure 1: An instance of the pipeline proposed in this article. _Left to right:_ A filtered simplicial complex \((S,f)\) (in this case a bi-filtered graph); the Hilbert function of its \(0\)_th_ dimensional homology persistence module \(H_{0}(f):^{2}\) (which in this case simply counts the number of connected components); the Hilbert decomposition signed measure \(_{H_{0}(f)}\) of the persistence module; and the convolution of the signed measure with a Gaussian kernel.

can be prohibitively computationally expensive for high-dimensional simplicial complexes, such as Vietoris-Rips complexes (Example 2).

## 2 Background

In this section, we recall the basics on multiparameter persistent homology and signed measures. We let \(\) denote the collection of finite dimensional vector spaces over a fixed field \(\). Given \(n 1\), we consider \(^{n}\) as a poset, with \(x y^{n}\) if \(x_{i} y_{i}\) for all \(1 i n\).

**Simplicial complexes.** A finite _simplicial complex_ consists of a finite set \(S_{0}\) together with a set \(S\) of non-empty subsets of \(S_{0}\) such that, if \(s S_{0}\) then \(\{s\} S\), and if \(_{2} S\) and \(_{1}_{2}\), then \(_{1} S\). We denote such a simplicial complex by \(S\), refer to the elements of \(S\) as the _simplices_ of \(S\), and refer to \(S_{0}\) as the _underlying set_ of \(S\). The _dimension_ of a simplex \( S\) is \(()=||-1\). In particular, the simplices of dimension \(0\) correspond precisely to the elements of the underlying set.

**Example 1**.: Let \(G\) be a simple, undirected graph. Then \(G\) can be encoded as a simplicial complex \(S\) with simplices only of dimensions \(0\) and \(1\), by letting the underlying set of \(S\) be the vertex set of \(G\), and by having a simplex of dimension \(0\) (resp. \(1\)) for each vertex (resp. edge) of \(G\).

**Definition 1**.: A _(multi)filtered simplicial complex_ is a pair \((S,f)\) with \(S\) a simplicial complex and \(f:S^{n}\) a monotonic map, i.e., a function such that \(f(_{1}) f(_{2})\) whenever \(_{1}_{2} S\). Given a filtered simplicial complex \((S,f:S^{n})\) and \(x^{n}\), we get a subcomplex \(S^{f}_{x}\{ S:f() x\} S\), which we refer to as the _\(x\)-sublevel set_ of \((S,f)\).

**Example 2**.: Let \((P,d_{P})\) be a finite metric space. The _(Vietoris-)Rips complex_ of \(P\) is the filtered simplicial complex \((S,f:S)\), where \(S\) is the simplicial complex with underlying set \(P\) and all non-empty subsets of \(P\) as simplices, and \(f(\{p_{0},,p_{n}\})=_{i,j}d_{P}(p_{i},p_{j})\). Then, for example, given any \(x\), the connected components of the \(x\)-sublevel set \(S^{f}_{x}\) coincide with the single-linkage clustering of \(P\) at distance scale \(x\) (see, e.g., ). In many applications, it is useful to also filter the Rips complex by an additional function \(d:P\) (such as a density estimator). The _function-Rips complex_ of \((P,d)\) is the filtered simplicial complex \((S,g:S^{2})\) with \(g(\{p_{0},,p_{n}\})=(\,f(\{p_{0},,p_{n}\})\,,\,-_{i}d(p_{i})\,)\). Thus, for example, the \((t,-u)\)-sublevel set is the Rips complex at distance scale \(t\) of the subset of \(P\) of points with \(d\)-value at least \(u\). See [17; 19; 25] for examples.

**Homology.** For a precise definition of homology with visual examples, see, e.g., ; the following will be enough for our purposes. Given \(i\), the _homology_ construction maps any finite simplicial complex \(S\) to a \(\)-vector space \(H_{i}(S)\) and any inclusion of simplicial complexes \(S R\) to a \(\)-linear map \(H_{i}(S R):H_{i}(S)H_{i}(R)\). Homology is functorial, meaning that, given inclusions \(S R T\), we have an equality \(H_{i}(R T) H_{i}(S R)=H_{i}(S T)\).

**Multiparameter persistence modules.** An \(n\)_-parameter persistence module_ consists of an assignment \(M:^{n}\) together with, for every pair of comparable elements \(x y^{n}\), a linear map \(M(x y):M(x)M(y)\), with the property that \(M(x x)\) is the identity map for all \(x^{n}\), and that \(M(y z) M(x y)=M(x z)\) for all \(x y z^{n}\).

**Example 3**.: Let \((S,f:S^{n})\) be a filtered simplicial complex. Then, we have an inclusion \(S^{f}_{x} S^{f}_{y}\) whenever \(x y\). By functoriality, given any \(i\), we get an \(n\)-parameter persistence module \(H_{i}(f):^{n}\) by letting \(H_{i}(f)(x) H_{i}(S^{f}_{x})\).

**Definition 2**.: Let \(M:^{n}\). The _Hilbert function_ of \(M\), also known as the _dimension vector_ of \(M\), is the function \((M):^{n}\) given by \((M)(x)=(M(x))\).

In practice, one often deals with persistence modules defined on a finite grid of \(^{n}\). These persistence modules are _finitely presentable_ (fp), which, informally, means that they can be encoded using finitely many matrices. See Definition 8 in the appendix for a formal definition, but note that this article can be read without a precise understanding of this notion.

**Signed measures.** The Kantorovich-Rubinstein norm was originally defined by Kantorovich and subsequently studied in  in the context of measures on a metric space; see . We denote by \((^{n})\) the space of finite, signed, Borel measures on \(^{n}\), and by \(_{0}(^{n})(^{n})\) the subspace of measures of total mass zero. For \((^{n})\), we let \(=^{+}-^{-}\) denote its Jordan decomposition [8, p. 421], so that \(^{+}\) and \(^{-}\) are finite, positive measures on \(^{n}\).

**Definition 3**.: For \(p[1,]\), the _Kantorovich-Rubinstein norm_ of \(_{0}(^{n})\) is defined as

\[\|\|_{p}^{}=\{_{^{n}^{n}} \|x-y\|_{p}\ d(x,y)\ :\ ^{n}^{n}^{+},^{-}\ \}.\]

We can then compare any two measures \(,(^{n})\) of the same total mass using \(\|-\|_{p}^{}\).

**Remark 1**.: Note that, if \(p q\), then \(\|-\|_{p}^{}\|-\|_{q}^{}\). For \(n=1\), the definition of \(\|-\|_{p}^{}\) is independent of \(p\), and in that case we just denote it by \(\|-\|^{}\).

Given \(x^{n}\), let \(_{x}\) denote the Dirac measure at \(x\). A _finite signed point measure_ is any measure in \((^{n})\) that can be written as a finite sum of signed Dirac measures.

The following result says that, for point measures, the computation of the Kantorovich-Rubinstein norm reduces to an assignment problem, and that, in the case of point measures on the real line, the optimal assignment has a particularly simple form.

**Proposition 1**.: _Let \(_{0}(^{n})\) be a finite signed point measure with \(^{+}=_{i}_{x_{i}}\) and \(^{-}=_{i}_{y_{i}}\), where \(X=\{x_{1},,x_{k}\}\) and \(Y=\{y_{1},,y_{k}\}\) are lists of points of \(^{n}\). Then,_

\[\|\|_{p}^{}=\{_{1 i k}\|x_{i}-y_{(i) }\|_{p}:\ \{1,,k\}\ \}.\]

_Moreover, if \(n=1\) and \(X\) and \(Y\) are such that \(x_{i} x_{i+1}\) and \(y_{i} y_{i+1}\) for all \(1 i k-1\), then the above minimum is attained at the identity permutation \((i)=i\)._

## 3 Signed barcodes as measures and stable vectorizations

In this section, we introduce the signed barcodes and measures associated to multifiltered simplicial complexes, as well as our proposed vectorizations, and we prove their stability properties.

The barcode  of a one-parameter persistence module consists of a multiset of intervals of \(\), often thought of as a positive integer linear combination of intervals, referred to as bars. This positive integer linear combination can, in turn, be represented as a point measure in the extended plane by recording the endpoints of the intervals . Recent adaptations of the notion of one-parameter barcode to multiparameter persistence  assign, to each multiparameter persistence module \(M:^{n}\), _two_ multisets \((^{+},^{-})\) of bars; each bar typically being some connected subset of \(^{n}\). In , these pairs of multisets of bars are referred to as _signed barcodes_. In several cases, the multisets \(^{+}\) and \(^{-}\) are disjoint, and can thus be represented without loss of information as integer linear combinations of bars. This is the case for the minimal Hilbert decomposition signed barcode of , where each bar is of the form \(\{y^{n}:y x\}^{n}\) for some \(x^{n}\). The minimal Hilbert decomposition signed barcode of a multiparameter persistence module \(M\) can thus be encoded as a signed point measure \(_{M}\), by identifying the bar \(\{y^{n}:y x\}\) with the Dirac measure \(_{x}\) (see Fig. 1).

In Section 3.1, we give a self-contained description of the signed measure associated to the minimal Hilbert decomposition signed barcode, including optimal transport stability results for it. Then, in Section 3.2, we describe our proposed vectorizations of signed measures and their stability. The proofs of all of the results in this section can be found in Appendix A.

### Hilbert and Euler decomposition signed measures

We start by interpreting the minimal Hilbert decomposition signed barcode of  as a signed measure. We prove that this notion is well-defined in Appendix A.4, and give further motivation in Appendix A.5.

**Definition 4**.: The _Hilbert decomposition signed measure_ of a fp multiparameter persistence module \(M:^{n}\) is the unique signed point measure \(_{M}(^{n})\) with the property that

\[(M(x))\ =\ _{M}\{y^{n}:y x\} ,\ x^{n}.\]

Given a filtered simplicial complex, one can combine the Hilbert decomposition signed measure of all of its homology modules as follows.

**Definition 5**.: The _Euler decomposition signed measure_ of a filtered simplicial complex \((S,f)\) is

\[_{(f)}_{i}(-1)^{i}\ _{H_{i}(f)}.\]

The next result follows from , and ensures the stability of the signed measures introduced above. In the result, we denote \(\|h\|_{1}=_{ S}\|h()\|_{1}\) when \((S,h)\) is a filtered simplicial complex.

**Theorem 1**.: _Let \(n\), let \(S\) be a finite simplicial complex, and let \(f,g:S^{n}\) be monotonic._

1. _For_ \(n\{1,2\}\) _and_ \(i\)_,_ \(\|_{H_{i}(f)}-_{H_{i}(g)}\|_{1}^{}\ \ n\|f-g\|_{1}\)_._
2. _For all_ \(n\)_,_ \(\|_{(f)}-_{(g)}\|_{1}^{}\ \ \|f-g\|_{1}\)_._

Extending Theorem 1 (1.) to a number of parameters \(n>2\) is an open problem.

**Computation.** The design of efficient algorithms for multiparameter persistence is an active area of research . The worst case complexity for the computation of the Hilbert function of the \(i\)th persistent homology module is typically in \(O((|S_{i-1}|+|S_{i}|+|S_{i+1}|)^{3})\), where \(|S_{k}|\) is the number of \(k\)-dimensional simplices of the filtered simplicial complex [53, Rmk. 4.3]. However, in one-parameter persistence, the computation is known to be almost linear in practice . For this reason, our implementation reduces the computation of Hilbert functions of homology multiparameter persistence modules to one-parameter persistence; we show in Appendix D.1 that this scalable in practice. We consider persistence modules and their corresponding Hilbert functions restricted to a grid \(\{0,,m-1\}^{n}\). By [55, Rmk. 7.4], given the Hilbert function \(\{0,,m-1\}^{n}\) of a module \(M\) on a grid \(\{0,,m-1\}^{n}\), one can compute \(_{M}\) in time \(O(n m^{n})\). Using the next result, the Euler decomposition signed measure is computed in linear time in the size of the complex.

**Lemma 1**.: _If \((S,f)\) is a filtered simplicial complex, then \(_{(f)}=_{ S}(-1)^{()}\ _{f()}\)._

### Vectorizations of signed measures

Now that we have established the stability properties of signed barcodes and measures, we turn the focus on finding vectorizations of these representations. We generalize two well-known vectorizations of single-parameter persistence barcodes, namely the _persistence image_ and the _sliced Wasserstein kernel_. The former is defined by centering functions around barcode points in the Euclidean plane, while the latter is based on computing and sorting the point projections onto a fixed set of lines. Both admits natural extensions to points in \(^{n}\), which we now define. We also state robustness properties in both cases.

#### 3.2.1 Convolution-based vectorization

A _kernel function_ is a map \(K:^{n}_{ 0}\) with \(_{x^{n}}K(x)^{2}\,dx<\). Given such a kernel function and \(y^{n}\), let \(K_{y}:^{n}_{ 0}\) denote the function \(K_{y}(x)=K(x-y)\).

**Definition 6**.: Given \((^{n})\), define the _convolution_ of the measure \(\) with the kernel function \(K\) as \(K* L^{2}(^{n})\) as \((K*)(x)_{z^{n}}K(x-z)d(z)\).

**Theorem 2**.: _Let \(K:^{n}\) be a kernel function for which there exists \(c>0\) such that \(\|K_{y}-K_{z}\|_{2} c\|y-z\|_{2}\) for all \(y,z^{n}\). Then, if \(,(^{n})\) have the same total mass,_

\[\|K*-K*\|_{2} c\|-\|_{2}^{}.\]

In Propositions 4 and 5 of the appendix, we show that Gaussian kernels and kernels that are Lipschitz with compact support satisfy the assumptions of Theorem 2.

**Computation.** For the experiments, given a signed measure \(\) on \(^{n}\) associated to a multiparameter persistence module defined over a finite grid in \(^{n}\), we evaluate \(K*\) on the same finite grid. As kernel \(K\) we use a Gaussian kernel.

#### 3.2.2 Sliced Wasserstein kernel

Given \( S^{n-1}=\{x^{n}:\|x\|_{2}=1\}^{n}\), let \(L()\) denote the line \(\{:\}\), and let \(^{}:^{n}\) denote the composite of the orthogonal projection \(^{n}L()\) with the map \(L()\) sending \(\) to \(\).

**Definition 7**.: Let \(\) be a measure on \(S^{n-1}\). Let \(,(^{n})\) have the same total mass. Their _sliced Wasserstein distance_ and _sliced Wasserstein kernel_ with respect to \(\) are defined by

\[SW^{}(,)_{ S^{n-1}}\;_{*}^{ }\;-\;_{*}^{}^{}\;d() k_{SW}^{}(,)-SW^{}(,),\]

respectively, where \(_{*}^{}\) denotes the pushforward of measures along \(^{}:^{n}\).

**Theorem 3**.: _Let \(\) be a measure on \(S^{n-1}\). If \(,(^{n})\) have the same total mass, then \(SW^{}(,)(S^{n-1})\|-\|_{}^{}\). Moreover, there exists a Hilbert space \(\) and a map \(_{SW}^{}:_{0}(^{n})\) such that, for all \(,_{0}(^{n})\), we have \(k_{SW}^{}(,)=_{SW}^{}(),_{SW}^{}( )_{}\) and \(\|_{SW}^{}()-_{SW}^{}()\|_{} 2 SW ^{}(,)\)._

**Computation.** We discretize the computation of the sliced Wasserstein kernel by choosing \(d\) directions \(\{_{1},,_{d}\} S^{n-1}\) uniformly at random and using as measure \(\) the uniform probability measure with support that sample, scaled by a parameter \(1/\), as is common in kernel methods. The Kantorovich-Rubinstein norm in \(\) is then computed by sorting the point masses, using Proposition 1.

## 4 Numerical experiments

In this section, we compare the performance of our method against several topological and standard baselines from the literature. We start by describing our methodology (see Appendix C.1 for details about hyperparameter choices). An implementation of our vectorization methods is publicly available at https://github.com/DavidLapous/multipers, and will be eventually merged as a module of the Gudhi library .

**Notations for descriptors and vectorizations.** In the following, we use different acronyms for the different versions of our pipeline. H is a shorthand for the Hilbert function, while E stands for the Euler characteristic function. Their corresponding decomposition signed measures are written HSM and ESM, respectively. The sliced Wasserstein kernel and the convolution-based vectorization are denoted by SW and C, respectively. 1P refers to one-parameter, and MP to multiparameter. Thus, for instance, MP-HSM-SW stands for the multi-parameter version of our pipeline based on the Hilbert signed measure vectorized using the sliced Wasserstein kernel. The notations for the methods we compare against are detailed in each experiment, and we refer the reader to Section 1.3 for their description.

**Discretization of persistence modules.** In all datasets, samples consist of filtered simplicial complexes. Given such a filtered simplicial complex \(f:S^{n}\), we consider its homology persistence modules \(H_{i}(f)\) with coefficients in a finite field, for \(i\{0,1\}\). We fix a grid size \(k\) and a \((0,1)\). For each \(1 j n\), we take \(r_{0}^{j} r_{k-1}^{j}\) uniformly spaced, with \(r_{0}^{j}\) and \(r_{k-1}^{j}\) the \(\) and \(1-\) percentiles of \(f_{j}(S_{0})\), respectively. We then restrict each module \(M\) to the grid \(\{r_{0}^{1},,r_{k-1}^{1},r_{k}^{1}\}\{r_{0}^{n},,r_ {k-1}^{n},r_{k}^{n}\}\) where \(r_{k}^{j}=1.1(r_{k-1}^{j}-r_{0}^{j})+r_{0}^{j}\), setting \(M(x_{1},,x_{n})=0\) whenever \(x_{j}=r_{j}^{k}\) for some \(j\) in order to ensure that \(_{M}\) has total mass zero.

**Classifiers.** We use an XGBoost classifier  with default parameters, except for the kernel methods, for which we use a kernel SVM with regularization parameter in \(\{0.001,0.01,1,10,100,1000\}\).

### Hilbert decomposition signed measure vs barcode in one-parameter persistence

As proven in Proposition 2, in Appendix A, the Hilbert decomposition signed measure is a theoretically weaker descriptor than the barcode. Nevertheless, we show in this experiment that, in the one-parameter case, our pipeline performs as well as the following well known vectorization methods based on the barcode: the persistence image (PI) , the persistence landscape (PL) , and the sliced Wasserstein kernel (SWK) . We also compare against the method perve (PV) of , which consists of a histogram constructed with all the endpoints of the barcode. It is pointed out in  that methods like pervec, which do not use the full information of the barcode, can perform very well in practice. This observation was one of the initial motivations for our work. The results of running these pipelines on some of the point cloud and graph data detailed below are in Table 1. See Appendix C.2 for the details about this experiment. As one can see from the results, signed barcode scores are always located between PV and (standard) barcode scores, which makes sense given that they encode more information than PV, but less than barcodes. The most interesting part, however, isthat the Hilbert decomposition signed barcode scores are always of the same order of magnitude (and sometimes even better) than barcode scores.

### Classification of point clouds from time series

We perform time series classification on datasets from the UCR archive  of moderate sizes; we use the train/test splits which are given in the archive. These datasets have been used to assess the performance of various topology-based methods in ; in particular, they show that multiparameter persistence descriptors outperform one-parameter descriptors in almost all cases. For this reason, we compare only against other multiparameter persistence descriptors: multiparameter persistence landscapes (MP-L), multiparameter persistence images (MP-I), multiparameter persistence kernel (MP-K), and the Hilbert function directly used as a vectorization (MP-H). Note that, in this example, we are not interested in performing point cloud classification up to isometry due to the presence of outliers. We use the numbers reported in [20, Table 1]. We also compare against non-topological, state-of-the-art baselines: Euclidean nearest neighbor (B1), dynamic time warping with optimized warping window width (B2), and constant window width (B3), reporting their accuracies from . Following , we use a delay embedding with target dimension \(3\), so that each time series results in a point cloud in \(^{3}\). Also following , as filtered complex we use an alpha complex filtered by a distance-to-measure (DTM)  with bandwidth \(0.1\). As one can see from the results in Table 2, MP-HSM-C is quite effective, as it is almost always better than the other topological baselines, and quite competitive with standard baselines.

Interestingly, MP-HSM-SW does not perform too well in this application. We believe that this is due to the fact that the sliced Wasserstein kernel can give too much importance to point masses that are very far away from other point masses; indeed, the cost of transporting a point mass is proportional to the distance it is transported, which can be very large. This seems to be particularly problematic for alpha complexes filtered by density estimates (such as DTM), since some of the simplices of the alpha complex can be adjacent to vertices with very small density, making this simplices appear very late in the filtration, creating point masses in the Hilbert signed measure that are very far away from the bulk of the point masses. This should not be a problem for Rips complexes, due to the redundancy of simplices in Rips complexes: for any set of points in the point cloud, there will eventually be a simplex between them in the Rips complex. In order to test this hypothesis, we run the same experiment but using density to filter a Rips complex instead of an alpha complex (Table 9 in Appendix C.4). We see that, in this case, the sliced Wasserstein kernel does much better, being very competitive with the non-topological baselines.

### Classification of graphs

We evaluate our methods on standard graph classification datasets (see  and references therein) containing social graphs as well as graphs coming from medical and biological contexts.

Since we want to compare against topological baselines, here we use the accuracies reported in  and , which use 5 train/test splits to compute accuracy. We compare against multiparameter persistence landscapes (MP-L), multiparameter persistence images (MP-I), multiparameter persistence kernel (MP-K), the generalized rank invariant landscape (GRIL), and the Hilbert and Euler characteristic functions used directly as vectorizations (MP-H and MP-E). We use the same filtrations as reported in , so the simplicial complex is the graph (Example 1), which we filter with two parameters: the heat kernel signature  with time parameter \(10\), and the Ricci curvature . As one can see from the results in Table 3, our pipeline compares favorably to topological baselines. Further experiments on the same data but using 10 train/test splits are given in Appendix C.3: they show that we are also competitive with the topological methods of  and the state-of-the-art graph classification methods of [75; 80; 81].

### Unsupervised virtual screening

In this experiment we show that the distance between our feature vectors in Hilbert space can be used effectively to identify similar compounds (which are in essence multifiltered graphs) in an unsupervised virtual screening task. Virtual screening (VS) is a computational approach to drug discovery in which a library of molecules is searched for structures that are most likely to bind to a given drug target . VS methods typically take as input a query ligand \(q\) and a set \(L\) of test ligands,and they return a linear ordering \(O(L)\) of \(L\), ranking the elements from more to less similar to \(q\). VS methods can be supervised or unsupervised; unsupervised methods  order the elements of \(L\) according to the output of a fixed dissimilarity function between \(q\) and each element of \(L\), while supervised methods learn a dissimilarity function and require training data [2; 78]. We remark that, in this experiment, we are only interested in the direct metric comparison of multifiltered graphs using our feature vectors and thus only in unsupervised methods.

We interpret molecules as graphs and filter them using the given bond lengths, atomic masses, and bond types. In order to have the methods be unsupervised, we normalize each filtering function using its standard deviation, instead of cross validating different rescalings as in supervised tasks. We use a grid size of \(k=1000\) for all methods, and fix \(\) for the sliced Wasserstein kernel and the bandwidth of the Gaussian kernel for convolution to \(1\). We assess the performance of virtual screening methods on the Cleves-Jain dataset  using the _enrichment factor_ (\(EF\)); details are in Appendix C.5. We report the best results of  (in their Table 4), which are used as baseline in the state-of-the-art methods of . We also report the results of , but we point out that those are supervised methods. As one can see from the results in Table 4, MP-ESM-C clearly outperforms unsupervised baselines and approaches the performances of supervised ones (despite being unsupervised itself).

## 5 Conclusions

We introduced a provably robust pipeline for processing geometric datasets based on the vectorization of signed barcode descriptors of multiparameter persistent homology modules, with an arbitrary number of parameters. We demonstrated that signed barcodes and their vectorizations are efficient representations of the multiscale topology of data, which often perform better than other featurizations based on multiparameter persistence, despite here only focusing on strictly weaker descriptors. We believe that this is due to the fact that using the Hilbert and Euler signed measures allows us to leverage well-developed vectorization techniques shown to have good performance in one-parameter persistence. We conclude from this that the way topological descriptors are encoded is as important as the discriminative power of the descriptors themselves.

**Limitations.** (1) Our pipelines, including the choice of hyperparameters for our vectorizations, rely on the cross validation of several parameters, which limits the number of possible choices to consider. (2) The convolution-based vectorization method works well when the signed measure is defined over a fine grid, but the performance degrades with coarser grids. This is a limitation of the current version of the method, since convolution in very fine grids does not scale well in the number of dimensions (i.e., in the number of parameters of the persistence module). (3) The sliced Wasserstein vectorization method for signed measures is a kernel method, and thus does not scale well to very large datasets.

**Future work.** There is recent interest in TDA in the _differentiation_ of topological descriptors and their vectorizations. Understanding the differentiability of signed barcodes and of our vectorizations could be used to address limitation (1) by optimizing various hyperparameters using a gradient instead of cross validation. Relatedly, (2) could be addressed by developing a neural network layer taking as input signed point measures, which is able to learn a suitable, relatively small data-dependent grid on which to perform the convolutions. For clarity, our choices of signed barcode descriptor and vectorization of signed measures are among the simplest available options, and, although with these basic choices we saw notable improvements when comparing our methodology to state-of-the-art topology-based methods, it will be interesting to see how our proposed pipeline performs when applied with stronger signed barcode descriptors, such as the minimal rank decomposition of , or to the decomposition of invariants such as the one of . Relatedly, our work opens up the way for the generalization of other vectorizations from one-parameter persistence to signed barcodes, and for the study of their performance and statistical properties.

**Acknowledgements.** The authors thank the area chair and anonymous reviewers for their insightful comments and constructive suggestions. They also thank Hannah Schreiber for her great help in the implementation of our method. They are grateful to the OPAL infrastructure from Universite Cote d'Azur for providing resources and support. DL was supported by ANR grant 3IA Cote d'Azur (ANR-19-P3IA-0002). LS was partially supported by the National Science Foundation through grants CCF-2006661 and CAREER award DMS-1943758. MC was supported by ANR grant TopModel (ANR-23-CE23-0014). SO was partially supported by Inria Action Exploratoire PreMediT. This work was carried out in part when MBB and SO were at the Centre for Advanced Study (CAS), Oslo.

  Dataset & SWK & PI & PL & PV & 1P-HSM-SW & IP-HSM-C \\   DistalPlahaxOutlineAgeGroup & 73.6 & 66.9 & 68.3 & 66.9 & 70.5 & 70.5 \\ DistalPlahaxOutlineCorrect & 73.6 & 62.3 & 68.5 & 64.5 & 74.6 & 75.4 \\ DistalPlahaxTW & 64.7 & 60.4 & 62.6 & 55.4 & 63.3 & 62.6 \\   COX2 & 78.84(4.0) & 78.6(1.0) & 79.2(3.7) & 78.20(8.0) & 79.72(2.6) & 80.1(1.3) \\ DiffPR & 80.6(5.5) & 72.3(5.7) & 78.6(4.6) & 73.5(4.7) & 76.7(5.7) & 76.7(3.4) \\ IMDB-B & 69.64(4.4) & 66.6(3.3) & 65.4(3.3) & 65.8(4.1) & 64.3(4.6) & 63.7(3.6) \\  

Table 1: Accuracy scores of one-parameter persistence factorizations on some of the datasets from Tables 2 and 3. The one-parameter version of our signed barcode vectorizations, on the right, performs as well as other topological methods, despite using less topological information.

  Dataset & B1 & B2 & B3 & MP-K & MP-L & MP-I & MP-HSM-SW & MP-HSM-C \\   DistalPlahaxOutlineAgeGroup & 62.6 & 62.6 & **77.0** & 67.6 & 70.5 & 71.9 & 71.2 & 74.1 & 71.2 \\ DistalPlahaxOutlineCorrect & 71.7 & 72.5 & 71.7 & 74.6 & 69.6 & 71.7 & 73.9 & 71.4 & **75.4** \\ DistalPlahaxTW & 63.3 & 63.3 & 59.0 & 61.2 & 56.1 & 61.9 & 60.4 & 62.6 & **67.6** \\ ProximalPlahaxOutlineAgeGroup & 78.5 & 78.5 & 80.5 & 78.0 & 78.5 & 81.0 & 82.4 & **82.9** & 82.4 \\ ProximalPlahaxOutlineCorrect & 80.8 & 79.0 & 78.4 & 78.7 & 78.1 & 81.8 & 82.1 & 77.7 & **82.5** \\ ProximalPlahaxTW & 70.7 & 75.6 & 75.6 & **79.5** & 73.2 & 76.1 & 77.1 & 77.6 & 77.6 \\ ECG20 & **88.0** & **88.0** & 77.0 & 77.0 & 74.0 & 83.0 & 73 & 71.1 & 84.1 \\ ItalyPowerDemand & **95.5** & **95.5** & 95.0 & 80.7 & 78.6 & 79.8 & 80.5 & 79.3 & 77.8 \\ MedicalImages & 68.4 & **74.7** & 73.7 & 55.4 & 55.7 & 60.0 & 56.5 & 53.3 & 56.2 \\ Plane & 96.2 & **100.0** & **100.0** & 92.4 & 84.8 & 97.1 & 99 & 91.4 & **100** \\ SwedishLeaf & 78.9 & **84.6** & **79.2** & 78.2 & 64.6 & 83.8 & 79 & 66.2 & 79.8 \\ GunPointPoint & 91.3 & 91.3 & 90.7 & 88.7 & 94.0 & 90.7 & 89.3 & 88.7 & **94.1** \\ GunPointAgeSpan & 89.9 & 96.5 & 91.8 & 93.0 & 85.1 & 90.5 & 91.8 & 87.7 & **96.7** \\ GunPointMaleVervasFemale & 97.5 & 97.5 & **99.7** & 96.8 & 88.3 & 95.9 & 93.7 & 83.5 & 96.8 \\ GunPointOfVervasYoung & 95.2 & 96.5 & 83.8 & 95.0 & 97.1 & **100.0** & 99.7 & 99.4 & 99.4 \\ PowerCons & **93.3** & 92.2 & 87.8 & 85.6 & 84.4 & 86.7 & 88.3 & 83.9 & 88.7 \\ SyntheticControl & 88.0 & 98.3 & **99.3** & 50.7 & 60.3 & 60.0 & 55.3 & 49.7 & 61 \\  

Table 2: Accuracy scores of baselines and multiparameter persistence methods on time series datasets. Boldface indicates best accuracy and underline indicates best accuracy among topological methods. The convolution-based vectorization with the Hilbert decomposition signed measure (MP-HSM-C) performs very well in comparison to other multiparameter persistence vectorizations, and is competitive with the non-topological baselines.

  Dataset & MP-K & MP-L & MP-I & GRIL & MP-H & MP-E & MP-HSM-SW & MP-ESM-SW & MP-HSM-C \\   COX2 & **79.9(1.8)** & 79.0(3.3) & 77.9(2.7) & 79.8(2.2) & 78.2(2) & 78.4(2.2) & 78.4(2.2) & 78.4(0.7) & 78.2(0.4) & 77.1(3) & 78.2(1.5) \\ DHFR & 81.7(1.9) & 79.5(2.3) & 80.2(2.2) & 77.6(2.5) & 81.6(1.6) & 79.6(1.9) & 80(1.1) & 80.8(3) & **81.9(2.5)** & 80.5(3.1) \\ IMDB-B & 68.2(1.2) & 71.2(2.0) & 71.1(1) & 65.2(2.6) & 72.3(2.4) & 71.3(1.7) & 72.9(2.1) & 74.7(1.6) & **74.8(2.5)** & 74.4(2.4) \\ MDB-M & 46.9(2.6) & 46.2(2.3) & 46.7(2.7) & NA & 47(2.7) & 47(3.1) & 47.2(2.5) & 47.7(2.4) & **47.9(3.2)** & 47.3(3.2) \\ MUTAG & 86.1(5.2) & 84.0(6.8) & 85.6(7.3) & 87.8(4.2) & 86.7(5.5) & **88.8(4.2)** & 87.3(5) & 87.2(2.6) & 85.6(5.3) & 88.3(5.8) \\ PROTEINS & 67.5(3.1) & 65.8(3.3) & 67.3(3.5) & 70.9(3.1) & 67.4(2.2) & 70.2(5.7) & 72(3.1) & 68.8(2.7) & **74.6(2.1)** & 70.9(0.8) \\  

Table 3: Accuracy and standard deviation scores of topological methods (averaged over \(5\)-fold train/test splits) on graph datasets. Bold indicates best accuracy. Again, the convolution-based vectorization with the Hilbert decomposition signed measure (MP-HSM-C) performs very well when compared to other multiparameter persistence vectorizations.