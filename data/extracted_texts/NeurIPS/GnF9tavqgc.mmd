# Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning

Yuxuan Ren\({}^{*}\), Dihan Zheng\({}^{*}\), Chang Liu, Peiran Jin, Yu Shi, Lin Huang,

**Jiyan He\({}^{}\), Shengjie Luo\({}^{}\), Tao Qin, Tie-Yan Liu**

Corresponded to: <changliu@microsoft.com>

###### Abstract

In recent years, machine learning has demonstrated impressive capability in handling molecular science tasks. To support various molecular properties at scale, machine learning models are trained in the multi-task learning paradigm. Nevertheless, data of different molecular properties are often not aligned: some quantities, _e.g._ equilibrium structure, demand more cost to compute than others, _e.g._ energy, so their data are often generated by cheaper computational methods at the cost of lower accuracy, which cannot be directly overcome through multi-task learning. Moreover, it is not straightforward to leverage abundant data of other tasks to benefit a particular task. To handle such data heterogeneity challenges, we exploit the specialty of molecular tasks that there are physical laws connecting them, and design consistency training approaches that allow different tasks to exchange information directly so as to improve one another. Particularly, we demonstrate that the more accurate energy data can improve the accuracy of structure prediction. We also find that consistency training can directly leverage force and off-equilibrium structure data to improve structure prediction, demonstrating a broad capability for integrating heterogeneous data.

## 1 Introduction

The field of machine learning has witnessed a blossom of progress in solving molecular science tasks in recent years, including molecular property prediction , machine-learning force field (energy/force prediction) , electronic structure , molecular structure generation  and design . In molecular research, these tasks are often required jointly: for example, energy prediction is required for molecular stability and dynamics, and equilibrium structure (conformation) prediction offers the most probable and characteristic structure for understanding molecule interaction and functions. Multi-task learning is hence adopted, where a model is trained to predict multiple properties using the same number of decoders (output heads) built on a shared encoder (backbone model) (Fig. 1) . This paradigm is also used for pre-training a model by leveraging as much data as possible that are scattered over various tasks and domains .

Nevertheless, science tasks have some unique challenges beyond conventional machine learning tasks, which cannot be adequately addressed by multi-task learning alone. It is more costly to curate a dataset for molecular science tasks since it calls for running physics-theoretic computation algorithms, which come with a stringent accuracy-efficiency trade-off. Molecular-science datasets are hence generated each with a specific algorithmic portfolio that is economic for the particular purpose.

This incurs two challenges regarding data heterogeneity. **(1)** Different properties in a dataset may come from algorithms in different levels of theory, meaning different levels of accuracy. This limits the prediction accuracy on some tasks. For example, labeling the energy of a molecular structure is yet affordable for common molecules using algorithms in the density functional theory (DFT) level, but producing the equilibrium structure of a molecule requires repeated energy evaluations hence is tens to hundreds of times more costly. As a result, DFT-level equilibrium structure data are available only in a limited scale [25; 26; 27], while larger-scale datasets [28; 29] have to resort to lower levels of theory to generate equilibrium structures at scale, which come with a lower level of accuracy. Using such data, multi-task learning alone cannot predict structures in an accuracy higher than the data-generation method. **(2)** Different datasets focus on different tasks, so combining these datasets to enhance the performance on a particular task is not straightforward. For example, there are datasets [30; 31; 32] that are concerned with force prediction and off-equilibrium structures, which do not provide direct supervision to equilibrium structure prediction. Although including these additional tasks in multi-task learning could help learn a better encoder (or, representation), this is only based on an empirical observation from a general machine learning perspective and does not directly exploit relevant physical information in the additional tasks.

In this work, we highlight that science tasks also provide fortunate specialties that come to the rescue. In contrast to conventional machine learning tasks which are primarily defined by data, science tasks originate in fundamental physical laws, and data are rather the demonstration of such laws. These laws impose explicit constraints between tasks, hence define the "physical consistency" between model predictions on these tasks. By enforcing such consistency, model predictions for different tasks are connected and can explicitly share the information in the data of one task to the prediction for other tasks, hence bridging data heterogeneity. From another perspective, while sharing a common encoder connects various decoders at the input end, physical consistency closes the loop by connecting the decoders at the output end (Fig. 1). With the additional information from the physical consistency, capabilities beyond conventional multi-task learning are enabled: **(1)** data from a higher-level of theory of one task can improve the accuracy of a physically related task, and **(2)** the abundant data of a physically related task can directly improve the performance of the concerned task.

Concretely, we demonstrate the practical value of the physical consistency between energy prediction and equilibrium structure prediction, two central tasks in molecular science. The consistency can be constructed from two perspectives: the equilibrium structure of a molecule is the structure that attains the minimal energy of the molecule (Sec. 3.2), and the equilibrium structure is a sample of the thermodynamic equilibrium distribution at a low temperature (Sec. 3.3). When adopting the denoising diffusion formulation [33; 34; 35] for structure prediction, we show that the two physical laws can be translated into consistency loss functions which connect the energy-prediction model with the structure-prediction model at large and small diffusion steps, respectively. We apply the consistency losses in the multi-task learning on the PubChemQC B3LYP/6-31G*//PM6 dataset , which is perhaps the largest public dataset with DFT-level (B3LYP/6-31G*) energy labels thus highly relevant to model pre-training. But its equilibrium structures are generated in the semi-empirical level (PM6), which is in a lower level of accuracy. We use the consistency losses to transmit the information of the higher-level theory in the energy model to the structure model by only optimizing parameters of the structure decoder, which achieves the followings. **(1)** Consistency losses improve structure prediction accuracy beyond multi-task learning alone, when evaluated using DFT-level structures from the PCQM4MV2  and QM9  datasets. The advantage persists even after both have undergone finetuning. **(2)** Datasets providing DFT-level force labels on off-equilibrium structures are better leveraged to improve structure prediction accuracy beyond only including force prediction in multi-task learning. Since force is the gradient of the energy, the additional data allow

Figure 1: Illustration of the idea of physical consistency. To support multiple tasks (“Task X” represents a general task), the model (blue solid lines) builds multiple decoders on a shared encoder, which are trained by multi-task learning with data of respective tasks (green dotted double arrows). Physical consistency losses enforce physical laws between tasks (orange dashed double arrows), hence bridge data heterogeneity and directly improve one task from others.

the energy model to learn a better energy landscape, which in turn leads to more accurate structure prediction through the consistency losses. This is a more direct pathway for data on an additional task to benefit structure prediction beyond learning a better representation in multi-task learning. We remark that the improvement is not from any more accurate structure data, but from bridging data heterogeneity using the "free lunch" redeemed from physical laws.

### Related Work

Structure prediction.In recent years, deep generative models have been used as a powerful tool to generate molecular structures. Due to the subtlety that a structure being rotated as a whole is essentially the same structure, early methods opt to generate intermediate geometric variables, such as inter-atomic distances [36; 37] or torsional angles [38; 39]. Directly generating Cartesian coordinates of atoms has also been explored where the rotational equivalence is handled by alignment  or leveraging gradient of distances [41; 42]. More recently, diffusion models have been used to generate torsional angles , or atom coordinates [16; 44] using equivariant models. Given the generality and superior performance, we adopt an equivariant diffusion model for structure prediction. While these works may generate multiple meta-stable structures, we aim at the single equilibrium structure for each molecule, and investigate the benefit from an energy model.

Leveraging physical laws between tasks.A noticeable example is leveraging the connection between energy and thermodynamic equilibrium distribution to compensate for potentially biased data to better learn the distribution. The equilibrium distribution in a canonical ensemble is the Boltzmann distribution, which is directly determined by the energy function. From the machine learning perspective, the energy function provides an unnormalized density function of the target distribution. Using a flow-based model , Boltzmann generators  inject the energy supervision via the evidence lower bound objective. Bootstrapped \(\)-divergence objective is thereafter introduced to mitigate mode collapse . Zheng et al.  use a diffusion model where the energy supervises the score model at the start diffusion step and is propagated to intermediate steps by enforcing a PDE. Similar techniques are also explored for conventional machine learning tasks [47; 48]. More recently, Bose et al.  directly connect the energy to intermediate-step scores. In the opposite direction, Arts et al.  leverage Boltzmann-distribution samples to learn a coarse-grained energy model. In parallel with these works, the current work is devoted to the investigation of leveraging the connection between energy and structure prediction, and is not using an oracle energy function considering the cost of DFT calculation.

## 2 Technical Background

Before delving into details of the consistency between energy and structure prediction, we first introduce the problem formulation, and diffusion-based generative formulation for structure prediction.

Chemically, a molecule is specified by the types (_i.e._, chemical elements) of atoms and bonds, jointly forming a molecular graph \(\). A molecule in physical reality can take different structures (conformations) \(^{A 3}\), _i.e._, the collection of 3-dimensional coordinates of its \(A\) atoms. Many properties of molecule \(\) depend on its specific structure \(\), _e.g._, the (inter-atomic potential) energy, so energy prediction is in the form \(E_{}^{()}()\) with model parameters \(\).

Among possible structures, the equilibrium structure is the one that attains the minimal energy, and is the most representative structure for the molecule. As mentioned, the diffusion formulation has been a preferred choice for equilibrium structure prediction, which samples from a distribution \(p_{}()\) concentrated at the equilibrium structure. For this, a primitive structure is sampled from a simple distribution, _e.g._, the standard Gaussian, which undergoes a diffusion process that transforms the simple distribution to the desired distribution. This is done by reversing the process in the opposite direction, which is easier to construct. For example, the process can be taken as the Langevin diffusion that converges to standard Gaussian : \(_{t}=_{t}(_{t};,)\,t+}\,_{t}=- }{2}_{t}\,t+}\, _{t},\) where \(_{t}\) is a time dilation factor , and \(_{t}\) denotes the Wiener process. The process starts from the desired distribution \(p_{,0}=p_{}\) at \(t=0\) and ends after a sufficiently long period \(T\) when the distribution converges, \(p_{T}=(,)\). The reverse process is known to follow :

\[_{}=}}{2}_{}\, +_{T-} p_{,T-}( _{})\,+}}\, _{},\] (1)

where \(:=T-t\), which transforms \((,)\) at \(=0\) to the desired distribution at \(=T\), hence the generation process is constructed. To simulate the process, the only unknown is the (Fisher's) score function \( p_{,t}()\) at each diffusion instant, for which a machine-learning model \(^{()}_{,t}()\) is introduced. To learn to fit \( p_{,t}()\), a practical approach is by optimizing the denoising score matching loss : \(_{p_{,0}(_{0})}_{p(_{t}| _{0})}\|^{()}_{,t}(_{t})- _{_{t}}\, p(_{t}|_{0})\|^{2}\), which is convenient since from the Langevin diffusion, we can derive \(p(_{t}|_{0})=(_{t};_{t}}_{t},(1-_{t}))\), where \(_{t}:=(-_{0}^{t}_{s}\,s),\) which is a known distribution. Leveraging the reparameterization trick, the loss is reformed as :

\[_{(t;0,T)}(1-_{t})_{p_{,0}(_{0})}_{(_{t};, )}^{()}_{,t}(_{t}}_{0}+_{t}}_{t})+_{t}}{_{t}}}_{2}^{2},\] (2)

where the expectation w.r.t \(_{p_{,0}(_{0})}\) can be estimated by averaging over data. Once the score model is trained, it can generate structures by replacing \( p_{,t}()\) and simulating Eq. (1). If only \(p_{,0}(_{0})\) is desired (instead of \(p_{}(_{0:T})\)), then an equivalent simulation approach can be adopted known as probability-flow ODE :

\[_{}=}}{2}_{ }+ p_{,T-}(_{})\, ,\] (3)

which is equivalent to the deterministic process in denoising diffusion implicit model (DDIM) .

An alternative to the score-prediction formulation is the denoising formulation [55; 56]. By defining a "denoising model" \(^{()}_{,t}(_{t})\) which formulates the score model following:

\[^{()}_{,t}(_{t})=_{t}}^{()}_{,t}(_{t})-_{t}}{ 1-_{t}},\] (4)

the training loss function Eq. (2) in terms of \(^{()}_{,t}(_{t})\) becomes:

\[_{t}_{t}}{1-_{t}}_{p_{ ,0}(_{0})}_{_{t}}^{()}_{,t}(_{t}}_{0}+_{t}}_{t})-_{0}_{2}^{2},\] (5)

which follows the intuition to denoise a perturbed structure by predicting the original structure. This formulation better aligns with the notion of "structure prediction", hence can be benefited from successful model architectures [18; 19], and matches structure pre-training strategies [57; 24; 22; 58].

## 3 Method

We begin with the basic formulation of multi-task learning. We then present the two consistency training approaches between energy and structure prediction, based on two physical laws between the two tasks. The approach to directly leveraging physically-related datasets is described at last.

### Multi-Task Learning for Energy and Structure Prediction

Both energy and structure prediction tasks require a comprehensive understanding of the input molecular graph \(\) and structure \(\), so a shared encoder \(^{()}_{,t}()\) with parameters \(\) is employed. The time step \(t\) is required by the diffusion formulation, which is taken as 0 for energy prediction indicating the input structure is unperturbed and real. For energy and structure prediction, the corresponding decoders \(^{(_{E})}_{}\) and \(^{(_{S})}_{}\) are introduced. For structure prediction, the denoising formulation is adopted (end of Sec. 2). Under this formulation, the two tasks are handled by:

\[E^{(,_{E})}_{}()=^{(_{E})}_{ }^{()}_{,t=0}(), ^{(,_{S})}_{,t}()=^{ (_{S})}_{}^{()}_{,t}(),.\] (6)

On one datapoint \((,,E)\), the multi-task loss is (_c.f._ Eq. (5)): \(L_{}(,_{},_ {}|,,E)\)

\[=_{}E^{(,_{E})}_{}()-E +_{t}_{t}}{1-_{t}}_{ _{t}}^{()}_{,t}(_{t}}+_{t}}_{t})- _{2}^{2}.\] (7)A subtlety with the models is geometric invariance and equivariance. Indeed, if a structure \(\) is translated and rotated as a whole, the resulting atom coordinates represent essentially the same structure. The energy and the probability density should keep invariant after the transformation. For energy prediction, this can be guaranteed by using an invariant encoder \(_{,t}^{()}()\) (no requirement on \(_{}^{(_{})}\)). For the probability density, it is known  that rotational invariance can be achieved by the invariance of \(p_{,T}\), which is satisfied by \((,)\), and the equivariance of the denoising model \(_{,t}^{(,\,_{})}()\). For this reason, the input structure \(\) re-enters the structure decoder \(_{}^{(_{})}\), which is implemented with an equivariant architecture. Translational invariance of density can be achieved by centering the structures; see ref.  for reasoning.

Nevertheless, multi-task learning is restricted by the level of accuracy of training data. As mentioned, structure data are often generated in a lower level of accuracy than energy data due to the more demanding nature. To alleviate this limitation, we exploit physical laws between molecular energy and structure, and propose consistency training losses accordingly to bridge the energy and structure models, thereby enhancing the accuracy of structure prediction from the more accurate energy model.

### Optimality Consistency

One direct relationship between energy and equilibrium structure is that the equilibrium structure \(_{}\) minimizes the energy, _i.e._, \(_{}=*{argmin}_{}E_{}( )\). To enforce this optimality condition, we propose an optimality consistency loss \(L_{}\), in the form of "increase after perturbation" loss. It is based on the idea that the energy of the equilibrium structure should be lower than that of its perturbed version. Denoting \(_{}^{(,\,_{})}\) as the model-predicted equilibrium structure, the loss on a molecule \(\) can be written as:

\[L_{}(_{},_{ },)=_{}0,\ E_{}^{( ,\,_{})}(_{}^{(,\, _{})})-E_{}^{(,\,_{ })}(_{}^{(,\,_{} )}+)},\] (8)

where \((,(^{2}))\) is a small perturbation, and each element of \(^{2}\) is independently sampled from \((0,_{}^{2}]\). Since the purpose is to improve structure prediction accuracy by leveraging the energy model which has seen more accurate labels, so the consistency loss only optimizes the parameters exclusively for the structure prediction utility, _i.e._, structure decoder parameters \(_{}\). Other parameters \(\) and \(_{}\) do not optimize this loss. In this way, the consistency loss would not contaminate the energy prediction model with the less accurate structure prediction model.

The standard way to produce \(_{}^{(,\,_{})}\) requires simulating the diffusion process (Eq. (1)) or the equivalent ODE (DDIM) (Eq. (3)), which calls the denoising model recursively. So optimizing \(L_{}\) would involve backpropagation through the simulation process, which can be impractically costly and numerically unstable. Fortunately, we can exploit the intuition in the denoising formulation and find a much cheaper way to generate structure. The intuition of the denoising model \(_{,t}(_{t})\) is to recover the original structure \(_{0}\) from the perturbed structure \(_{t}\). Although this is informationally impossible at the instance level, the denoising model still has a definite learning target at the distributional level, which is \([_{0}|_{t}]\). Particularly, when \(t=T\), the correlation between \(_{0}\) and \(_{T}\) diminishes, so \(_{,T}(_{T})\) learns to output \([_{0}]\), the expectation of the target distribution, which is the equilibrium structure since the distribution concentrates at that structure. Under this perspective, the model-predicted structure can be generated by \(_{}^{(,\,_{})}= _{,T}^{(,\,_{})}(_{T}),\) where \(_{T}(,)\). This only requires one evaluation of the denoising model.

Nevertheless, the rotational invariance of the structure distribution introduces more subtleties.

**Proposition 1**.: _Let \(^{()}:^{A 3}^{A 3}\) be a rotationally equivariant function; that is, for any rotation matrix \((3)\) and structure \(^{A 3}\), we have \(^{()}()=^{()}()\). Then, for any target structure \(^{}^{A 3}\), the minimizer of the denoising loss function \(L()=_{(;, )}\|^{()}()-^{ }\|_{2}^{2}\) is the zero map; that is, \(^{()}()=\), for any \(\)._

See Appendix A for proof. This conclusion reveals that the learning target of the denoising model at \(T\) is trivially all-zero, which cannot serve to generate the equilibrium structure. To circumvent this, a simple choice is to denoise from a time step \(\) that is close but smaller than \(T\):

\[_{}^{(,\,_{})}=_{ ,}^{(,\,_{})}(), (,).\] (9)The target of the denoising model at \(\) is \(^{(,_{S})}_{,}(})= [_{0}|_{}=}]\), which is equivariant w.r.t \(_{}\). So the input \(_{}\) provides an orientation information hence breaking the rotational symmetry of the corresponding distribution \(p(_{0}|_{})\). The resulting expectation then would not average a structure over orientations evenly, hence not zero. More explicitly, since \(p(_{0}|_{}) p(_{0})p(_{ }|_{0}) p(_{0})\{-_{0}- _{}/}\|_{2}^{2}}{2(1/_{}-1 )}\}\), we have \(p(_{0}|_{}) p(_{0})( _{0};_{}/_{}},(1/_{}-1))\), where the Gaussian factor assigns larger probability along the direction of \(_{}\), hence breaks the rotational symmetry from \(p(_{0})\). Under this choice, the optimality consistency loss in Eq. (8) is specified as: \(L_{}(_{S}},_{E},)\)

\[=_{}_{}}0,\ E^{(, _{E})}_{}^{(,_{ S})}_{,}(})-E^{(,_{ E})}_{}^{(,_{S})}_{, }(})+}.\] (10)

### Score Consistency

The optimality consistency loss only supervises the denoising model at large time steps. For small time steps, an alternative perspective on the physical law between equilibrium structure and energy can help. Physically, a molecule \(\) in a real system can take different structures with different probabilities. When the system is in thermodynamic equilibrium, the probability distribution of the structures can be determined from the energy function of the molecule. Particularly, in a system with fixed volume and temperature \(\), the structure distribution is the well-known Boltzmann distribution, \(p_{;,}()(-E_{}()/(k_{}))\), where \(k_{}\) is the Boltzmann constant. When temperature approaches zero, the distribution becomes concentrated on the equilibrium structure. This aligns with the learning target of the diffusion model for equilibrium structure prediction. Through the expression of the Boltzmann distribution, the structure model can thus be connected to the energy model.

To enforce this connection, ideally, the density function \(p^{(,_{S})}_{}()\) modeled by the denoising model should match that defined by the energy model. Since the latter only provides an unnormalized density, we enforce their scores to match: \(_{q()} p^{(,_{S})}_{ }()+ E^{(,_{E})}_{}( )/(k_{})_{2}^{2}\), where \(q()\) is a reference distribution. Nevertheless, it is computationally costly to evaluate the density function from the diffusion model: \( p^{(,_{S})}_{}()= p_{T}( ^{(,_{S})}_{T})+_{0}^{T}}{2} }}{1-_{t}}3A_{t}} -^{(,_{S})}_{,t}(^{(,_{S})}_{t})\,t\), where \(^{(,_{S})}_{t[0,T]}\) is the solution to the ODE \(_{t}}{t}=}{2}_{t}}}{1-_{t}}_{t}}_{t}-^{(,_{S})}_{,t}(_{t}) \) with initial condition \(_{0}=\). Significant computational cost would be incurred from invoking and backpropagating through an ODE solver to evaluate and optimize the density.

We hence turn to another way to leverage this connection. Note from Eq. (4), the denoising model can be used to recover the score model, which targets the score function of the marginal distribution at the corresponding diffusion time instant. Particularly, the score model at \(t=0\) should approximate the score of the desired distribution, which is \(p_{;,}\) for small \(\). The energy model can hence provide supervision to the score model by enforcing this connection: \(L_{}(_{S}},_ {E},)\)

\[=_{p_{}(_{})}_{ }}^{(,_{S})}_{,}(_{})- _{}}{1-_{}}+_{}} ^{(,_{E})}_{}(_{})}{k_{ }}_{2}^{2}.\] (11)

In this score consistency loss, we have avoided taking the time step \(\) exactly zero for numerical stability consideration. Indeed, when \( 0\), the denoising model \(^{(,_{S})}_{,}\) approaches the identity map, and \(_{}\) approaches 1, making the first term in Eq. (11) an indeterminate form of type 0/0, which may render numerical stability issues. For the reference distribution to generate data to evaluate the loss, one can choose either the data distribution \(p_{,0}\) for which the energy model gives more confident results, or the perturbed distribution \(p_{,}\) (can be sampled by adding noise to a data sample \(_{0}\) following \(p(_{t}|_{0})\)) for relevance to how the denoising model \(^{(,_{S})}_{,}\) is invoked. Through some trials, we found the latter gives slightly better results.

Finally, as is the case for the optimality consistency loss, the score consistency loss also only optimizes the parameters \(_{S}\) of the structure decoder, to ensure the energy model would not be misled by the less accurate structure data. To implement this unconventional optimization requirement, we list detailed algorithms for the two consistency losses in Appendix B.1 in terms of the actual model components \(^{()}_{,t}\), \(^{(_{E})}_{}\), and \(^{(_{S})}_{}\).

### Leveraging Physically-Related Data

Besides the difference in the level of theory to generate data, the heterogeneity of molecular-science datasets also lies in the difference of concerned quantities. Compared to the enormous chemical space, available datasets for equilibrium structure are still not abundant, while there is a vast amount of data generated for physically related but different tasks, for example, labels of atomic forces, and data on off-equilibrium structures. We highlight that the consistency losses can leverage such datasets in an explicit way to further improve equilibrium structure prediction. Note that the consistency losses Eqs. (10, 11) works by offering the information at a higher level of theory in the energy model, in the form of _energy landscape_ on the structure space; _i.e._, ranking different structures in optimality consistency, and providing energy gradient in score consistency. For better learning the landscape, the force labels, which are negative gradients of the energy, provides first-order information of the landscape, and energy and force labels on multiple off-equilibrium structures enable better exploration on the structure space. This approach provides a more direct and concrete information path to equilibrium structure prediction than helping learn a better representation in multi-task learning. For learning a better energy landscape, the force labels are used to directly supervise the gradient of the energy model. The loss term for a datapoint \((,,)\) is:

\[L_{}(,_{} ,)=\|_{}E_{}^{( {},_{})}()+\|_{2}^ {2},\]

where \(\) is the force label. Note that there may be multiple \((,)\) data pairs for one molecule \(\), which provide even richer information on the energy landscape.

## 4 Experiments

In this section, we demonstrate the advantages of incorporating the proposed consistency losses into multi-task learning. Implementation details are provided in Appendix B.

### Setup

Datasets.We consider multi-task learning of energy and structure prediction on the PubChemQC B3LYP/6-31G*/PM6 dataset  (abbreviated as PM6), which is seemingly the largest (\(\)86M molecules) public available dataset with DFT-level property labels, hence a preferred setting for pre-training a molecular model. The energy labels are in the DFT (B3LYP/6-31G*) level, while the equilibrium structures are produced at the semi-empirical PM6  level, which is less accurate than DFT. Consistency training is hence considered to improve structure prediction accuracy using the more accurate energy data. To evaluate the effect of improved structure prediction accuracy beyond the PM6 level, the accuracy is evaluated against structures generated at the DFT level, which are available in the PCQM4Mv2 dataset  (abbreviated as PCQ) and the QM9 dataset .

Evaluation.Each of the evaluation datasets of PCQ and QM9 is split into three disjoint sets for training, validation, and test. The training and validation sets are for optional fine-tuning (see Sec. 4.4). Following existing convention [41; 44; 40], each test set is prepared by uniformly randomly selecting 200 distinct molecules from PCQ or QM9 that do not appear in the training dataset (PM6), which already makes the test molecules sufficiently dissimilar from training molecules (Appendix C.6).

On each test molecule, we sample 200 structures using the model, calculate their rooted mean square deviations (RMSDs) against the equilibrium structure in the test set, and evaluate the mean and the minimum over these RMSDs. Due to the geometric invariance of the structure distribution, the RMSD is evaluated after translational and rotational alignment of two structures using the Kabsch algorithm . We consider both the denoising (Eq. (9)) and the DDIM (Eq. (3)) approaches for structure sampling. We also provide coverage evaluation results in Appendix C.2.

In each setting, we independently repeat the evaluation process for five times using different random seeds, and report the mean of the repeats in the following tables. The standard deviations and t-test p-values are collectively provided in Appendix C.5. In settings using consistency training, both the optimality (Eq. (10)) and score consistency losses (Eq. (11)) are added to the multi-task training loss (Eq. (7)). Validation results for training in terms of both energy prediction and structure generation are provided in Appendix C.4.

### Structure Prediction Results

We first evaluate the effect of consistency training over multi-task training following the above settings. The results are shown in Table 1. We see that consistency training enhances the accuracy of structure prediction beyond multi-task learning consistently, without using any more accurate structure data. The improvement is significant when compared to the standard deviations provided in Appendix Table C.8, which are as low as around 0.003 A (Appendix Table C.10 verifies t-test significance). We note that this improvement is not at the cost of a lower energy prediction accuracy, as indicated by the energy prediction results in Appendix Table C.6(left).

To further examine that the improvement is from the effect of consistency training, we evaluate the energy of the predicted structure and the true DFT-level equilibrium structure in the PCQ dataset for each test molecule using the model, which is shown in the scatter plot of Fig. 2(left). We observe that when using consistency training, the overall energy is reduced, indicating that the predicted structures indeed have lower energy hence closer to the true DFT-level equilibrium structure. To quantify this improvement, in Appendix C.3 we define an "energy gap" metric, and the results shown in Table C.5 consolidate the observation.

### Results using Physically-Related Data

We next investigate the effect of consistency training for leveraging physically related data, as explained in Sec. 3.4. For this, we consider the force data in the SPICE dataset (PubChem subset) . The force data are also available on multiple off-equilibrium structures for each molecule. We note the subtlety that the setting of DFT in SPICE, \(\)B97M-D3(BJ)/def2-TZVPPD, is different from that for generating energy labels in the PM6 dataset. (Up to our knowledge, there does not seem to exist a public force dataset that matches the DFT setting as the PM6 dataset.) On one hand, calculation on near-equilibrium structures of small molecules is not very sensitive to DFT settings, especially for force calculation (even less affected than energy), while the force labels on multiple structures could be more valuable to learning the energy landscape despite the mismatch. So we still consider it as a

    Test Set \\  } &  &  \\   &  &  &  &  \\   & Mean & Min & Mean & Min & Mean & Min & Mean & Min \\  Multi-Task & 1.189 & 0.655 & 1.041 & 0.361 & 0.928 & 0.545 & 0.669 & 0.197 \\ Consistency & **1.158** & **0.645** & **1.007** & **0.346** & **0.848** & **0.490** & **0.650** & **0.194** \\   

Table 1: Test RMSD (Å; lower is better) of structure prediction by multi-task learning and consistency learning on PM6 dataset.

Figure 2: Comparison of energy (eV) on the model-generated structure \(_{}\) using the denoising method and the equilibrium structure \(_{}\) in the PCQ dataset. Each point represents the model-predicted energy values on the two structures for one test molecule. Models are trained on (**left**) the PM6 dataset, (**middle**) the PM6 dataset and SPICE force dataset, and (**right**) the PM6 dataset with a subset of force labels. The closer a point lies to the diagonal line, the closer the energy of the predicted structure is to the minimum energy, indicating a closer prediction of equilibrium structure.

relevant investigation setting. Note energy labels in SPICE are not used, and additional structures are not used for training the structure decoder. On the other hand, to reduce the gap, we also generated in-house force labels on a subset of PM6 structures using the same DFT setting (B3LYP/6-31G*) as for the PM6 dataset energy labels. The systematic error is controlled, although for each molecule there is only one labeled structure.

The results after incorporating SPICE force data and PM6 subset force data in training are shown in Table 2. We first observe that consistency training still outperforms multi-task training in structure prediction in all cases (see Appendix Tables C.8 and C.10 for standard derivations and t-test p-values). We note that this improvement is not at the cost of a lower energy prediction accuracy, as indicated by Appendix Table C.6(middle) indicates energy prediction is also not compromised in consistency training in this case. When compared to Table 1, we see that the inclusion of force data does not uniformly enhance multi-task learning performance, since the mechanism to learn a better representation is still implicit and indirect and may require extensive tuning. In contrast, using consistency losses improves structure prediction more consistently when physically related data are available in training. These observations indicate that consistency loss training can potentially assist the model in more effectively utilizing data from different sources or modalities.

Energy analysis is presented in Fig. 2(middle) utilizing SPICE force data, and in Fig. 2(right) with force labels on a subset of PM6 molecules. The data indicate that training with consistency loss results in lower predicted energies than multi-task training. Furthermore, we observe that the structures predicted by models trained with force datasets (Fig. 2, middle and right) have lower predicted energies compared to those trained exclusively on the PM6 dataset (Fig. 2, left), illustrating the advantage of incorporating force labels.

### Fine-Tuning Results

In addition to the zero-shot prediction evaluations, we further fine-tune the pre-trained models investigated in Sec. 4.2 using DFT-level structures in PCQ or QM9 training datasets. The results presented in Table 3 indicate that the inclusion of consistency loss in pre-training still enhances the accuracy of structure prediction even after fine-tuning. See Appendix Tables C.9 and C.10 for statistical significance. This could be attributed to that using consistency losses in pre-training can already inform the model of more accurate structure information, leading to a state that is more relevant to the underlying physics, which is a favored starting point for further improvements through

    Additional \\ Training Data \\  } &  Test Set \\ Generated by \\  } &  &  \\   & &  &  &  &  \\   & Struct. Stat. & Mean & Min & Mean & Min & Mean & Min & Mean & Min \\   SPICE \\ force \\  } & Multi-Task & 1.161 & 0.631 & 1.047 & 0.373 & 0.876 & 0.486 & 0.670 & 0.207 \\  & Consistency & **1.147** & **0.590** & **1.013** & **0.345** & **0.842** & **0.485** & **0.644** & **0.194** \\   PM6 subset \\ force \\  } & Multi-Task & 1.199 & 0.672 & 1.027 & 0.365 & 0.914 & 0.545 & 0.648 & 0.193 \\  & Consistency & **1.113** & **0.629** & **1.019** & **0.351** & **0.836** & **0.488** & **0.646** & **0.192** \\   

Table 2: Test RMSD (Å; lower is better) of structure prediction by multi-task learning and consistency learning on the PM6 dataset _with additional SPICE force dataset or PM6 subset force data_.

    Test Set \\ Generated by \\  } &  &  \\   &  &  &  &  \\   & Mean & Min & Mean & Min & Mean & Min & Mean & Min \\   Multi-Task \\ Consistency \\  } & 1.158 & 0.614 & 0.921 & 0.220 & 0.889 & 0.467 & 0.501 & 0.090 \\  & **1.152** & **0.610** & **0.918** & **0.218** & **0.835** & **0.420** & **0.493** & **0.076** \\   

Table 3: Test RMSD (Å; lower is better) _after finetuning_ for structure prediction pre-trained by multi-task learning and consistency learning on the PM6 dataset.

fine-tuning. Results of fine-tuning models that are pre-trained with SPICE force and PM6 subset force are shown in Appendix C.1, which further demonstrates the advantages of the consistency loss.

## 5 Conclusion and Discussion

This work leverages physical laws between molecular tasks to bridge data heterogeneity in multi-task learning. Consistency losses are designed to enforce physical laws between inter-atomic potential energy prediction and equilibrium structure prediction. They have shown to improve structure prediction beyond the typical accuracy level of structure data by leveraging abundant energy data in a higher level of accuracy, and can directly leverage force and off-equilibrium structure data to further improve the accuracy. The advantage still holds after finetuning. We would like to highlight that the improvement comes "for free" as no additional data (_e.g._, more accurate structure data) are required, demonstrating the value of physical laws in learning molecular tasks. The idea bears broader generality as data heterogeneity is ubiquitous in the science domain, and data for a specific task are often limited in either abundancy or accuracy.

The current work is limited to the consistency between energy and structure prediction, while more consistency laws can be considered in molecular science. Apart from mentioned works in connecting energy and thermodynamic distribution, more possibilities include electronic structure and molecular properties, and fine-grained and coarse-grained structures and macroscopic statistics. The significance of improvement in this work is still limited by the abundance of the data involved. Further improvement can be expected with more abundant/diverse but possibly less accurate structure data from, _e.g._, RDKit  or experimental measurements, or energy/force datasets in matching level of theory that more extensively explore the structural space.