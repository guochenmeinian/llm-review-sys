# Optimal Algorithms for Augmented Testing of Discrete Distributions

Maryam Aliakbarpour

Rice University

Houston, TX

maryama@rice.edu

&Piotr Indyk

MIT

Cambridge, MA

indyk@mit.edu

&Ronitt Rubinfeld

MIT

Cambridge, MA

ronitt@csail.mit.edu

&Sandeep Silwal

UW-Madison

Madison, WI

silwal@cs.wisc.edu

###### Abstract

We consider the problem of hypothesis testing for discrete distributions. In the standard model, where we have sample access to an underlying distribution \(p\), extensive research has established optimal bounds for uniformity testing, identity testing (goodness of fit), and closeness testing (equivalence or two-sample testing). We explore these problems in a setting where a predicted data distribution, possibly derived from historical data or predictive machine learning models, is available. We demonstrate that such a predictor can indeed reduce the number of samples required for all three property testing tasks. The reduction in sample complexity depends directly on the predictor's quality, measured by its total variation distance from \(p\). A key advantage of our algorithms is their adaptability to the precision of the prediction. Specifically, our algorithms can self-adjust their sample complexity based on the accuracy of the available prediction, operating without any prior knowledge of the estimation's accuracy (i.e. they are consistent). Additionally, we never use more samples than the standard approaches require, even if the predictions provide no meaningful information (i.e. they are also robust). We provide lower bounds to indicate that the improvements in sample complexity achieved by our algorithms are information-theoretically optimal. Furthermore, experimental results show that the performance of our algorithms on real data significantly exceeds our worst-case guarantees for sample complexity, demonstrating the practicality of our approach.

## 1 Introduction

Property testing of distributions is a fundamental task that lies at the heart of many scientific endeavors: Given sample access to an underlying unknown distribution \(p\), the goal is to infer whether \(p\) has a certain property or it is \(\)-far from any distribution that has the property (in some reasonable notion of distance, such as total variation distance) with as few samples as possible. Over the past century , this problem has been extensively explored in statistics, machine learning, and theoretical computer science. Indeed, distribution testing (also called hypothesis testing) is now a major pillar of modern learning theory and algorithmic statistics, with applications in learning mixtures of distributions such as Gaussians, Poisson Binomial Distributions and robust learning . The framework has also been extensively studied under privacy  and low-memory constraints .

One of the most natural and well-studied questions in this framework is: given sample access to an unknown distribution \(p\), can we determine whether \(p\) is equal to another distribution \(q\), or \(\)-far from it (e.g. in total variation distance)? This problem has been studied under various assumptions about how we access \(q\): It is called _uniformity testing_ when \(q\) is a uniform distribution, _identity testing_ (or "goodness-of-fit") when a description of \(q\) is known, and _closeness testing_ ("two-sample testing" or "equivalence testing") when we only have sample access to \(q\). The primary goal in solving these tasks is to design algorithms that use as few samples as possible. Optimal sample complexity bounds have been established for discrete distributions \(p\) and \(q\) over a domain of size \(n\): \((/^{2})\) samples for uniformity testing [52; 15; 69; 38] and identity testing [76; 5; 39], and \((n^{2/3}/^{4/3}+/^{2})\) samples for closeness testing [16; 30; 38; 39]. Other related versions of uniformity, identity, and closeness testing are presented in [63; 6; 19; 51; 46; 74; 12; 13; 68].

Given that the aforementioned results are tight and cannot be improved, any further progress requires equipping the algorithm with additional functionality. A natural approach is to leverage the fact that in numerous applications, the underlying distribution is not completely unknown; some prediction of the underlying distribution may be available or can be learned via a predictive machine learning model. For example, if distributions evolve slowly over time, earlier iterations can serve as approximations for later ones, e.g., network traffic data and search engine queries. Such estimations can often be learned from "older" data by using it to train a predictor or regressor. In linguistics and text processing tasks that involve distributions over words, the length of a word can approximate its frequency, since longer words are known to be less frequent. Another example is when data is available at different "scales." For instance,demographic data on loan defaults at the national level could be informative for data from specific areas.

One challenge to using such information is that it rarely comes with a guarantee of precision. This fact leads to the information being deemed unreliable, as it may poorly predict the underlying distribution. For example, while the national loan default rate might be close to that of a typical area code, it could differ significantly in affluent areas. Thus a natural algorithmic question that arises is how to design algorithms that can exploit predictive information as much as possible without any prior assumptions about its accuracy. The goal is then to design an algorithm that solves the problem with as few samples as possible, given the quality of the prediction.

In this work, we study the fundamental problems of uniformity, identity, and closeness testing in the setting where a prediction of the underlying distribution is provided. This prediction can be formalized by assuming that the distribution testing algorithm has access to a predicted distribution \(\) of \(p\)1. This is in addition to having sample access to \(p\) as in the standard model (without access to prediction). Our algorithms achieve the _optimal_ reduction in the number of samples used compared to the standard case, where the improvement depends on the quality of the predictor \(\) in terms of its total variation distance from \(p\). Our algorithms can also self-adjust their sample complexity to the accuracy of \(\), minimizing sample complexity wherever feasible, without prior knowledge of \(\)'s accuracy. Our approach ensures that the algorithm is resilient to inaccuracies in predictions and does not exceed the optimal sampling bound in the standard model, even when \(\) significantly deviates from the actual \(p\). Furthermore, our matching lower bounds demonstrate the optimality of our algorithms. Experimental results additionally confirm the practicality of our algorithms.

Measuring accuracy of predictor.We use the total variation distance between the prediction \(\) and the unknown distribution \(p\) as our measure of predictor accuracy. Previous work often assumed a strong element-wise guarantees, where \(_{i}\) is within a constant multiplicative factor of \(p_{i}\) for _all_ domain elements \(i\), a constraint that becomes limiting especially for small \(p_{i}\) (see Section 1.3). This paper is the first to study a notion of _average_ error between \(p\) and \(\), measuring via the TV distance. This metric was chosen for its prevalent use in statistical inference and its intuitive interpretation.

### Our approach and problem formulation

Our approach to solving a distribution testing problem (uniformity, identity, or closeness testing) consists of two components: _search_ and _test_. At a high level, _search_ aims to guess \(\|p-\|_{}\), and _test_ performs the actual distribution testing using the guess of the accuracy provided by _search_ as a suggested accuracy level.

More precisely, our augmented _test_ component aims to evaluate whether \(p=q\), while receiving \(\) and a _suggested_ accuracy level \(\) (which may or may not reflect the true distance between \(p\) and \(\)). In addition to two possible outcomes of \(\) and \(\) in the standard setting2, our augmented _test_ component may output inaccurate information when it determines that the \(\) is not \(\)-close to \(p\). Our requirements for the augmented _test_ component are the following: \(i\)) If the test is conclusive, i.e., it chooses to output \(\) or \(\), the answer should be correct regardless of \(\)'s accuracy. \(ii\)) If \(\) is indeed \(\)-close to \(p\), the _test_ component should not output inaccurate information.

We emphasize that the guess \(\)_is not guaranteed_ to be correct or may not even be a valid upper bound on the true TV distance. Thus, the algorithm is afforded a degree of flexibility to forego solving the problem when the distributions are not within \(\) proximity (and can try again with a new guess).

Our _search_ component aims to identify an appropriate accuracy level \(\) such that the _test_ component can test in a conclusive manner by returning \(\) or \(\). Since the true value distance \(\|p-\|_{}\) is not known, we start by guessing a small \(\), corresponding to the most accurate \(\) and the fewest samples needed, run the augmented _test_ component with this \(\) and \(\), and verify the concluiveness of the testing. If inconclusive, our guess \(\) is increased to a level that we can afford testing by doubling the sample size, and the _search_ component proceeds with the next \(\). It continues until the desired accuracy is reached, and \(\) or \(\) is returned. Then, _search_ halts with that result.

Clearly, if the accuracy level guess \(\) is at least \(\|p-\|_{}\), the _test_ component has to output \(\) or \(\) with high probability. Thus, we show that it is unlikely that _search_ proceeds when \(\|p-\|_{}\). Therefore, this method does not significantly increase the sample complexity, potentially increasing it by at most an \(O((n/))\) factor in expectation. The _search_ component is applicable to all of the problems we study and we defer all details of the _search_ component to Section B. The remainder of the main body focuses on designing the augmented testers, i.e. the _test_ component.

**Definition 1.1** (Augmented tester).: _Suppose we are given four parameters, \(\), \((0,1)\), \((0,1)\), \(n\), and two underlying distributions \(p\) and \(q\), along with a prediction distribution \(\) over \([n]\). Suppose \(\) is an algorithm that receives all these parameters, and the description of \(\) as input, and it has sample access to \(p\) and \(q\). We say algorithm \(\) is an \((,,)\)-augmented tester for closeness testing if the following holds for every \(p\), \(q\), and \(\) over \([n]\):_

* _If_ \(\) _and_ \(p\) _are_ \(\)_-close in total variation distance, the algorithm outputs_ \(\) _information with a probability at most_ \(/2\)_._
* _If_ \(p=q\)_, then the algorithm outputs_ \(\) _with a probability at most_ \(/2\)_._
* _If_ \(p\) _is_ \(\)_-far from_ \(q\)_, then the algorithm outputs_ \(\) _with a probability at most_ \(/2\)_._

_In this definition, if the description of \(q\) is known to the algorithm instead of having sample access, we say \(\) is an \((,,)\)-augmented tester for identity testing. If \(q\) is a uniform distribution over \([n]\), then we say \(\) is an \((,,)\)-augmented tester for uniformity testing._

To highlight the distinction between this definition and the standard definition, note that in the standard regime, no prediction \(\) is available to the algorithm, and the algorithm lacks the option of outputting \(\) information.

**Remark 1**.: _We assume that \(\) is a small constant, but, a standard amplification technique via Chernoff bounds can achieve an arbitrarily small confidence parameter \(\) with a \(O((1/))\) overhead._

### Our results

Our theoretical results:In this paper we demonstrate that predictions can indeed reduce the number of samples needed to solve the three aforementioned testing problems. Our algorithms are parameterized by both \(\) and \(\). We provide _tight_ sample complexity results (matching upper and lower bounds) for these problems. The sample complexity drops drastically compared to the standard case, depending on the total variation distance between \(q\) and \(\). Our algorithms are also robust: if the prediction error is high, our algorithm succeeds by using (asymptotically) the same number of samples as the standard setting without predictions.

**Theorem 2** (Informal version of Theorem 7).: _Augmented uniformity and identity testing for distributions over \([n]\), with parameters \(\), \(\), and \(=2/3\), require the following number of samples:_

\[s=\{(}{^{2}})& d\\ ((},\,}{^{2} }))&d>.,\]

_where \(d=\|q-\|_{}\) (\(q\) is the known distribution for identity testing, or the uniform distribution for uniformity)._

**Remark 3**.: _Note that \(d\) is not an input parameter to the algorithm. However, \(d\) is determined by \(\) and \(q\), which are known to the algorithm, allowing us to compute \(d\)._

For closeness testing, we prove the following.

**Theorem 4** (Informal version of Theorem 9).: _Augmented closeness testing for distributions over \([n]\), with parameters \(\), \(\), and \(=2/3\), requires \((n^{2/3}^{1/3}/^{4/3}+/^{2})\) samples._

It can be seen that e.g., for closeness testing, our non-trivial predictor improves over the best possible sampling bound in the standard model. Specifically, as long as \(=o(1)\), our bound in the augmented model improves over the prior work. At the same time, our algorithms are resilient: even if \(=(1)\), our sampling bounds do not exceed those in the standard model. Note that all theorems are complemented by _tight_ lower bounds. We highlight that all of our algorithms are also computationally efficient, running in polynomial time with respect to \(n\) and \(1/\). The results are summarized in Table 1.

Our empirical Results:We empirically evaluate our augmented closeness testing algorithm on synthetic and real distributions and refer all details to Section E.

As a summary, our algorithm can indeed leverage predictions to obtain significantly improved sample complexity over the SOTA approach without predictions, as well as SOTA algorithms needing very accurate predictions .

For distributions similar to our lower bound instances, our augmented algorithm achieves \(>\)20x reduction in sample complexity to obtain comparable accuracy as the standard un-augmented algorithm, as shown in Figure 1. On real distributions curated from network traffic data, we see sample complexity reductions of up to \(40\%\). Furthermore, our algorithm is empirically robust to noisy predictions, in contrast to prior state of the art approaches which assume very accurate, point-wise predictions (CRS'15 in Figure 1).

It is worth noting that our experiments on network traffic data reveal that the actual sample complexity is much lower than the anticipated worst-case sample complexity of our algorithm. In particular, this holds

  Property & Standard sample complexity & Augmented sample complexity (this paper) \\  Closeness & \((}{^{4/3}}+}{^{2}})\) & \((^{1/3}}{^{4/3}}+}{ ^{2}})\) \\  Identity/Uniformity & \((}{^{2}})\) & \((}{^{2}}),\,d\) \\  & \(\) & \(((},\,}{^{2} })),\,d>\) \\  

Table 1: Optimal sample complexity bounds in the standard model versus the augmented bounds. \(\) is the suggested accuracy level for the \(L_{1}\)-distance between \(p\) and \(\). \(d\) denotes the total variation distance between the known distribution \(q\) and \(\).

Figure 1: Error vs sample complexity for the theoretically hard instance (See Sec. E).

even when \(\) is far from \(p\) in terms of total variation distance. Empirically, if \(\) accurately reflects the high-probability elements in \(p\), our algorithm can significantly reduce the sample complexity needed for testing by utilizing these heavy hitter "hints" from \(\). This is validated by our results and depicted in Figures 5 and 7(b).

### Related works

To the best of our knowledge, there have been only three prior works that studied any distribution property testing algorithms with predictions, each assuming a much stronger prediction model:

* Distribution testing with _perfect_ predictors : this work studied distribution testing problems, including closeness, identity and uniformity, assuming _query access_ to a perfect predictor, i.e., \(=p\). They show that, given a perfect predictor, it is possible to obtain highly efficient testers for a wide variety of problems with a small number of queries to the prediction. Unfortunately, the perfect-predictor assumption is often too strong to hold in practice, as demonstrated e.g., in  and in our experiments.
* Distribution testing with _\((1+)\)-approximate_ predictors : these works relax the assumption used in the above paper, requiring only that \(_{i}=(1/2)p_{i}\) for all \(i\) and sufficiently small \(>0\). However, this assumption is still quite restrictive, especially for low values of \(\). Indeed in our experimental setting, such algorithms are not robust to prediction errors (see Section E).
* Support estimation with \(c\)-approximate predictors : this work focused on the single problem of support estimation, i.e., estimating the fraction of coordinates \(i\) such that \(p_{i}>0\). It further relaxes the assumption in  by allowing the predicted probabilities \(_{i}\) to be within a factor of \(c\) of the true probabilities \(p_{i}\), for _any_ constant approximation factor \(c>1\). This algorithm has been shown to work well in practice. However, the techniques presented in that paper seem to be applicable exclusively to support estimation. Furthermore, their result provably does not hold under the assumption that \(p\) and \(\) are close in TV distance, as in this paper.

In summary, prior results required either highly restrictive assumptions, or were applicable to only a single problem. None of the previous algorithms worked under the TV distance assumption used in this paper which is arguably the most natural. Further exploration of measures such as \(L_{p}\)-distances, KL-divergence, and Hellinger distance are interesting open questions.

We remark that we can mathematically show that these alternative oracles yield provably more power compared to ours (i.e., we make weaker assumptions about the predictor). We provide an alternative to our upper bound techniques for these alternate prediction models in Section G. We demonstrate how a variant of our algorithm, in conjunction with these stronger predictions, implies that uniformity testing, identity testing, and closeness testing in these models can be conducted using only \(O(/^{2})\) samples. This low sample complexity effectively circumvents our lower bound for closeness testing, suggesting that these models provide stronger, and arguably less realistic, predictions.

Other related works:Other related works discussing algorithms with predictions and general distribution testing are discussed in Appendix A.

### Notation and organization

Notation:We use \([n]\) to denote the set \(\{1,2,,n\}\). All of our distributions will be over the domain \([n]\), and we assume \(n\) is always known. For a distribution \(p\), we denote the probability of the \(i\)-th element by \(p_{i}\). For any subset of the domain \(S[n]\) we denote the probability mass of \(S\) according to \(p\) by \(p(S)\). We use \(p^{ s}\) to denote the probability distribution of \(s\) i.i.d. samples drawn from \(p\). We say \(p\) is a _known_ distribution, if we have access to every \(p_{i}\). We say \(p\) is an _unknown_ distribution if we have only sample access to \(p\). We use \(()\) to denote a random variable from a Poisson distribution with mean \(\). Similarly, \(()\) indicates a random variable from the Bernoulli distribution that is one with probability \(\). Given two distributions \(p\) and \(q\) over a domain \(\), we use \(\|p-q\|_{}_{S}|p(S)-q(S)|\) to denote the total variation distance between \(p\) and \(q\). We say \(p\) and \(q\) are \(\)-close (\(\)-far), if the total variation distance between \(p\) and \(q\) is at most \(\) (larger than \(\)).

Organization:We provide an overview of our theoretical results in Section 2. The upper bound on augmented closeness testing is presented in Section 3. The search algorithm is detailed in Section B.

Augmented uniformity and identity testing are discussed in Section C, where the upper bounds are presented in Section C.1, and the lower bounds are provided in Sections C.2 and C.3.

Augmented closeness testing is discussed in Section D, with the lower bounds provided in Section D.1. Our empirical evaluations are presented in Section E.

## 2 Overview of our proofs

Search algorithm:The search algorithm seeks to find the smallest value of \(\) for which the problem is solvable via the augmented tester. It starts with the lowest value of \(\) (most accurate prediction). Then, it iteratively increases the sample budget across rounds. In each round \(i\), it selects an \(_{i}\) for the augmented tester, \(\), ensuring operation within the current sample budget. If \(\) outputs accept or reject, the algorithm echoes this outcome. If inaccurate information is returned, the sample budget doubles for the next round. It is worth noting that this search scheme is applicable to a general distribution testing algorithm with polynomial sample complexity. The algorithm's pseudocode and its performance proof are in Section B.

Upper bound for augmented identity testingLet \(d\) represent the distance between \(\) and \(q\) (the prediction and the known distribution). For establishing the upper bound, it is essential to assume \(d>\). If not, the prediction proves unhelpful, and we might as well resort to the standard tester.

Our upper bound relies on a simple but fundamental observation regarding the total variation distance: this distance is the maximum discrepancy between the probability masses that two distributions assign to any subset of their domain. To prove that total variation between two distributions is small entails proving that the discrepancy across every domain subset is small. In contrast, to prove a large total variation distance, one only needs to identify a single subset with a large discrepancy as evidence of large total variation distance. We use the Scheffe set of \(\) and \(q\)--characterized as the collection of elements \(x[n]\) where \((x)<q(x)\), symbolized by \(S\)--as evidence of \(p\)'s divergence from either \(q\) or \(\). More precisely, it is known that \(S\) maximizing the discrepancy between the probability masses of \(\) and \(q\), implying: \(d\|q-\|_{}=|q(S)-(S)|\). Next, we estimate the probability of set \(S\) according to \(p\) with reasonably high accuracy. Given \(q(S)\) and \((S)\), then \(p(S)\) is either significantly different from \(q(S)\), or it deviates from \((S)\). In the first scenario, this discrepancy serves as evidence that \(p q\), allowing us to output reject. In the second scenario, the deviation confirms that \(p\) is \(\)-distant from \(\), leading us to output inaccurate information. Further details can be found in Section C.1.

Lower bound for augmented uniformity testingWe provide two lower bounds for augmented uniformity testing. One is purely based on a reduction to standard uniformity testing for the case where \( d\). (Recall that \(d\) was the total variation distance between \(q\) and \(\)). See Section C.2.

The other lower bound applies to the setting where \(<d\). One challenge of this problem was that it is hard to find two difficult distributions that the tester has to distinguish between; usually, for a pair of distributions, we could come up with one valid output that could serve them both. For example, for both uniform distribution, and the famous \(\)-far uniform distribution that assigns probabilities \((1)/n\) to the elements, the algorithm may be able to output inaccurate information. Hence, we cannot draw lower bounds just by asking the algorithm to distinguish between two distributions.

For this reason, we provide three distributions that _look_ similar when we draw too few samples. We formalize the similarity of these three distributions using a multivariate coupling argument. We show that these distributions are such that there is no possible answer that is valid for all three of them. Now, (similar to Le Cam's method), suppose we feed the algorithm with samples from one of these three distributions (each with probability 1/3). For any sample set, the algorithm outputs an answer (which may be randomized); however, this answer is considered wrong for at least one of the underlying distributions. This is due to the fact that there is no universally valid answer that is simultaneously correct for all three distributions. Hence, if the algorithm outputs a valid answer with high probability, it must be able to distinguish the underlying distributions to some degree. On the other hand, the indistinguishability result says it is impossible to tell these distributions apart. Thus, we reach a contradiction. And, the lower bound is concluded. See Section C.3 for further details.

Lower bound for augmented closeness testing:We provide two separate lower bounds for closeness testing based on the relationship between \(\) and \(\). Further details are available in Section D.1.

Our first lower bound, as detailed in Theorem 11, employs a reduction strategy from standard closeness testing to augmented closeness testing when \(\). This is achieved by taking instances used in standard closeness testing for distributions over \([n-1]\) and embedding them into the first \(n-1\) domain elements of a new distribution \(p\) defined over \([n]\). The key to this strategy is to put the majority of the distribution's mass (\((1-)\) mass) on its final element, and we set the prediction \(\) to be a singleton distribution over the last element, which is \(\)-close to \(p\). Clearly, the prediction does not reveal any information about the first \(n-1\) elements of \(p\), implying that testing the closeness of \(p\) in the augmented setting is as challenging as in the standard setting, once the parameters are appropriately scaled.

Our second lower bound, outlined in Theorem 12, is more involved. In the standard setting, the lower bound for closeness testing is derived from the hard instances for uniformity testing with one crucial adjustment: adding new elements with large probability (approximately \(n^{-2/3}\)) in the distributions. These large elements have identical probability masses in both \(p\) and \(q\), indicating they do not contribute to the distance between the two distributions. However, their presence in the sample set confuses the algorithm: due to their high probabilities, their behavior in the sample set may misleadingly suggest non-uniformity, complicating the algorithm's task of determining the uniformity of the rest of the distribution. Therefore, the algorithm requires \(s n^{2/3}\) samples to first identify these large elements before it can test the uniformity of the remaining distribution. Surprisingly, this requirement of \(s n^{2/3}\) samples is significantly higher than the \(O()\) samples typically sufficient for testing uniformity.

The challenge in our case arises because \(\) may disclose the large elements to the algorithm. To establish the lower bound, we set \(\) to be the uniform distribution, we generate hard instances of \(p\) by adding as many large elements as possible, without altering \(\), by keeping the overall probability mass of the large elements limited to \(\). More precisely, \(p\) assigns approximately \((1-)/n\) probability mass to \(O(n)\) elements chosen at random, and assigns approximately \(n^{-2/3}\) probability mass to \( n^{2/3}\) elements in the domain. Now, \(q\) has two scenarios. Half the time, \(q\) is identical to \(p\). The other half, \(q\) retains the same large elements but deviates slightly from uniformity for the rest of the distribution. Specifically, \(q\) assigns probabilities \((1())(1-)/n\) to the randomly chosen \(O(n)\) elements, making it \(\)-far from \(p\). It is not hard to show that testing closeness of \(p\) and \(q\) is a symmetric property (since permuting the domain elements does not affect our construction). By leveraging the wishful thinking theorem from , we demonstrate that these two scenarios are indistinguishable unless \((n^{2/3}^{1/3})\) samples are drawn.

## 3 Upper bound for closeness testing

Our upper bound is based on a technique called _flattening_, which has been previously proposed by Diakonikolas and Kane . This technique is instrumental in reducing the variance of the statistic used for closeness testing by reducing the \(_{2}^{2}\)-norm of the input distributions. We adapt this technique for use in the augmented setting, aiming not only to flatten the distribution based on the samples received from it but also exploiting the prediction distribution \(\). We demonstrate that augmented flattening can significantly reduce the \(_{2}^{2}\)-norm of \(p\). In our algorithm, we initially check if the \(_{2}^{2}\)-norm of \(p\) is reduced after flattening to the desired bound. If not, this indicates that the prediction was not sufficiently accurate, leading us to output _inaccurate_. Conversely, if the \(_{2}^{2}\)-norm of \(p\) is small, we proceed with an efficient testing algorithm that requires fewer samples. We describe the standard flattening technique in Section 3.1. Our flattening technique presented in Section 3.2. Finally, we provide our algorithm in Section 3.2.1.

### Background on flattening

Suppose we are given \(n\) parameters \(m_{1},m_{2},,m_{n}\). One can create a randomized mapping \(F\) that assigns each \(i[n]\) to a pair \((i,j)\), where \(j\) is drawn uniformly at random from \([m_{i}]\). Now, consider a given distribution \(p\) over \([n]\) and a sample \(i\) drawn from \(p\). This mapping induces a distribution over pairs \((i,j)\)'s in \(D\{(i,j)|i[n]j[m_{i}]\}\). We denote this new distribution by \(p^{(F)}\) satisfying the relation: \(p^{(F)}_{(i,j)}=p_{i}/m_{i}\,.\) The core idea of the above mapping is to divide the probability of the \(i\)-th element into \(m_{i}\)_buckets_. If the values of \(m_{i}\)'s are large for elements in \(p\) with higher probabilities, then the resulting distribution \(p^{(F)}\) will avoid having any elements with disproportionately large probabilities, thereby naturally lowering its \(_{2}\)-norm. Diakonikolas and Kane  showed that if we draw \((t)\) samples from \(p\), and set each \(m_{i}\) to be the frequency of element \(i\) among these samples plus one, then we have:

\[\![\|p^{(F)}\|_{2}^{2}]\,,\]

where the expectation is taken over the randomness of the samples.

Connection to distribution testing:Chan et al. in  showed that one can test closeness of two distributions over \([n]\) using \(O(n(\|p\|_{2},\|q\|_{2})/^{2})\) samples. Diakonikolas et al.  used this tester in combination of the flattening technique to map distributions to distributions over slightly larger domains with the goal of reducing their \(_{2}\)-norms. They have shown that if we use the same mapping to reduce the \(_{2}\)-norm of both \(p\) and \(q\), the \(_{2}\)-distance between the two distribution does not change the \(_{1}\) distance between distributions.

**Fact 3.1** ().: _For any flattening scheme \(F\), the \(_{1}\)-distance is preserved under \(F\). That is, for every pair of distributions \(p\) and \(q\), we have:_

\[\|p-q\|_{1}=\|p^{(F)}-q^{(F)}\|_{1}\,.\]

Therefore, to test \(p\) and \(q\), we can draw samples to come up with the flattening \(F\) to reduce the \(_{2}\)-norm of one of the underlying distributions. Then, we can test the closeness of \(p^{(F)}\) and \(q^{(F)}\) with \(O(n(\|p^{(F)}\|_{2},\|q^{(F)}\|_{2})/^{2})\) new samples. Later, Aliakbarpour et al. in  showed that \(O(n(\|p\|_{2},\|q\|_{2})/^{2})\) samples is sufficient.

The exact sample complexity is determined by balancing the samples needed to determine the flattening and the samples needed to test closeness of \(p^{(F)}\) and \(q^{(F)}\). Note that flattened distributions have a larger domain size. Thus, one must also balance the gains obtained from the reduction in \(_{2}\) norm with the increase in the domain.

### Augmented flattening

We adapt the flattening argument in  to the augmented setting.

**Lemma 3.2**.: _Suppose we have an unknown distribution \(p=(p_{1},p_{2},,p_{n})\) over \([n]\) and \((s_{f})\) samples from \(p\). Assume we are given a known distribution \(\) that is \(\)-close to \(p\) in TV distance. Then for every \(v 1\), there exists a flattening \(F\) which increases the domain size by \(1/v+s_{f}\) in expectation and the expected \(_{2}^{2}\)-norm of the \(p^{(F)}\) is bounded by:_

\[\![\|p^{(F)}\|_{2}^{2}]}+4\,.\]

_The expectation above is taken over the randomness of the samples from \(p\)._

Proof.: Let \(_{f}(s_{f})\) be a Poisson random variable with mean \(s_{f}\). Let \(_{f}\) denote the multiset of \(_{f}\) samples from \(p\). Let \(f_{i}\) denotes the frequency of element \(i\) in \(_{f}\). For every \(i\), we define the number of buckets for element \(i\) as follows:

\[m_{i}_{i}}{}+f_{i}+1\,.\] (1)

Let \(p^{(F)}\) denote the flattened version of \(p\) where we split every element \(i\) into \(m_{i}\) buckets. We show that the expected \(_{2}\)-norm of \(p^{(F)}\) is low. Define \(_{i}\) to be \(p_{i}-_{i}\). Suppose \(A\) is a set of indices \(i[n]\) for which \(_{i}_{i}\). For every \(i A\), \(p_{i}\) is bounded from above by \(2\,_{i}\). On the other hand, for every \(i[n] A\), \(p_{i}\) is bounded by \(2\,_{i}\).

\[\|p^{(F)}\|_{2}^{2} =_{i[n]}_{j[m_{i}]}p^{(F)}(i,j)^{2}=_{i[n]} {p_{i}^{2}}{m_{i}}=_{i A} p_{i}}{m_{i}}+_{i [n] A}_{i})^{2}}{m_{i}}\] \[_{i A} p_{i}}{f_{i}+1}+_{i [n] A}_{i})^{2}}{_{i}/}\] \[_{i A} p_{i}}{f_{i}+1}+4\,.\]

As shown in , the expected value of \(p_{i}/(f_{i}+1)\) taken over the randomness in the \(_{f}\) is bounded by \(1/s_{f}\):

\[_{F}\![}{f_{i}+1}]}\]

Therefore, we obtain:

\[_{F}\![\|p^{(F)}\|_{2}^{2}]_{i A} }{s_{f}}+4\,}+4\,\,.\]

The last inequality above is due to the fact that:

\[_{i A}_{i}=p(A)-(A)\|p-\|_{}=\,.\]

#### 3.2.1 The algorithm

In this section, we provide an algorithm for testing closeness of two distribution in the augmented setting. Our algorithm receives a suggested accuracy level \(\). Based on \(\), we draw \(s_{f}\) (which depends on \(\)) samples from \(p\) and use them to flatten \(p\). If the resulting distribution has sufficiently small \(_{2}^{2}\)-norm we proceed to the testing phase. Otherwise, we declare that the accuracy level provided is not correct. The pseudocode of our algorithm is provided in Algorithm 1, and we prove its performance in Theorem 5.

```
1:procedureAugmented-Closeness-Tester(\(n,\;,\;,\;_{1}=0.1,\;_{2}=2/3,\;\), sample access to \(p\) and \(q\))
2:\(s_{f}(n^{2/3}^{1/3}/^{4/3},n)\)
3:\(_{f}(s_{f})\)
4:if\(_{f}>10\,s_{f}\)then
5:return reject.
6:\(F F(m_{1},m_{2},,m_{n})\) where \(m_{i}\) are define in Equation 2.
7:\(L_{p}_{2}^{2}(p^{(F)},\;_{i=1}^{n}m_{i},\;0.05)\)
8:\(L_{q}_{2}^{2}(q^{(F)},\;_{i=1}^{n}m_{i},\;0.05)\)
9:if\(L_{p}>30(}+)\)then
10:return inaccurate information.
11:else
12:if\(L_{q}>90(}+)\)then
13:return reject.
14:else
15:return Closeness-Tester\((,\;=0.1,\;b=90(}+),\;p^{(F)}, \;q^{(F)})\). ```

**Algorithm 1** An augmented tester for testing closeness of \(p\) and \(q\) with a suggested value \(\)

**Theorem 5**.: _For any given parameters \(n,\;,\;\), and any two unknown distributions \(p\) and \(q\) and a known predicted distribution \(\) for \(p\), Algorithm 1 is an \((,\;,=0.3)\)-augmented tester for testing closeness of \(p\) and \(q\) which uses \(O(n^{2/3}^{1/3}/^{4/3})\) samples._Proof.: We begin the proof by setting our parameters. Set \(s_{f}(n^{2/3}^{1/3}/^{4/3},n)\). Suppose we draw \(_{f}=(s_{f})\) samples from \(p\). We flatten the distribution \(p\) according to the augmented flattening we described in Section 3.2 replacing \(\) with \(1/n\). We denote the flattening by \(F=F(m_{1},,m_{n})\) where \(m_{i}\)'s are defined in Equation (1):

\[m_{i} n_{i}+f_{i}+1\,.\] (2)

In the above definition, \(f_{i}\) is the frequency of element \(i\) among the \(_{f}\) drawn samples.

Proof of correctness.: Suppose we flatten both \(p\) and \(q\) according to \(F\). The algorithm estimates the \(_{2}^{2}\) of \(p^{(F)}\) and \(q^{(F)}\). According to Fact F.1, these estimates are within a constant factor of their true values each with probability at least \(1-0.05\):

\[_{2}}{2} L_{p}_{2}}{2}_{2}}{2} L_{q} _{2}}{2}\,.\] (3)

To prove the correctness of the algorithm, we show the three desired property of an augmented tester which are defined in Definition 1.1. First, assume \(\) is \(\)-close to \(p\). We show the probability of outputting inaccurate information is small. Using Lemma 3.2, after applying \(F\), the expected \(_{2}^{2}\)-norm of \(p^{(F)}\) is bounded by:

\[ p^{(F)}_{2}}+\,.\]

Using Markov's inequality, with probability at least 0.95, \(_{2}^{2}\)-norm of \(p^{(F)}\) is at most 20 times the above bound. Combined by the upper bound for \(L_{p}\) in Equation (3) and the union bound, in this case \(L_{p}\) is bounded by \(30(2/s_{f}+4/n)\) with probability at least 0.9. Hence, we the algorithm does not output inaccurate information in Line 10 with probability more than 0.1.

Second, we show that if \(p=q\), the algorithm outputs reject with small probability. We may output reject in three cases: 1) in Line 5 2) in Line 13 and 3) in Line 15. Using Markov's inequality, a Poisson random variable is more than 10 times its expectation with probability at most 0.1. We show the other two cases are unlikely as long as \(L_{p}\) and \(L_{q}\) are accurate estimates of the \(_{2}^{2}\)s of \(p^{(F)}\) and \(q^{(F)}\) (which happens with probability of 0.9). If \(p=q\), \(p^{(F)}\) and \(q^{(F)}\) are identical. Therefore, in the algorithm \(L_{p}\) and \(L_{q}\) are the estimations of the \(_{2}^{2}\)-norm of the same distribution. Note that we output reject in Line 13, only when \(L_{p}<3L_{q}\). Using Equation (3), this event does not happen unless at least of the estimates are inaccurate (which has a probability at most 0.1). Furthermore if the estimated \(_{2}^{2}\)-norm are accurate, the minimum \(_{2}^{2}\)-norm of \(p^{(F)}\) and \(q^{(F)}\) are bounded by \(90(2/s_{f}+4/n)\) implying the tester would work correctly with probability at least 0.9. Using the union bound, the probability of outputting reject is bounded by 0.3.

Third and last, we show that if \(p\) is \(\)-far from \(q^{(F)}\), the probability of outputting accept is small. We only output accept in Line 15. Similar to the second case we have discussed, as long as the \(_{2}^{2}\)-norm are accurate, the tester does not make a mistake with probability more than 0.1. Therefore, the overall probability of making a mistake is bounded by 0.2.

Analysis of sample complexity.:For the flattening step we have used \(_{f} 10s_{f}\). After flattening, the new domain size is bounded from above by:

\[_{i=1}^{n}m_{i}_{i=1}^{n}n_{i}+f_{i}+1=n+_{f}+n  12n.\]

Using Fact F.1, estimation of \(_{2}^{2}\)-norm requires \(O()\) samples. Using Fact F.2, the number of samples we use for the test in Line 15 is \(O(n b/^{2})\) samples. Hence, the total sample complexity is:

\[ =O(s_{f}++})=O (s_{f}+++4/n}}{^{2}})\] \[=O(}{^{2}}+\,^{1/3}} {^{4/3}})\,.\]

In the last line we use that \(s_{f}(n^{2/3}^{1/3}/^{4/3},n)\). Thus, the proof is complete.