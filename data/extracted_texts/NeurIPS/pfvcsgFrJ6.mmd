# On Causal Discovery in the Presence of Deterministic Relations

Loka Li\({}^{1}\)

Haoyue Dai\({}^{2}\)

Equal contributions.

Hanin Al Ghothani\({}^{1}\)

Biwei Huang\({}^{3}\)

Jiji Zhang\({}^{4}\)

Shahar Harel\({}^{5}\)

Isaac Bentwich\({}^{5}\)

Guangyi Chen\({}^{1,2}\)

Kun Zhang\({}^{1,2}\)

\({}^{1}\) Mohamed bin Zayed University of Artificial Intelligence

\({}^{2}\) Carnegie Mellon University, \({}^{3}\) University of California San Diego

\({}^{4}\) The Chinese University of Hong Kong, \({}^{5}\) Quris AI

{longkang.li, kun.zhang}@mbzuai.ac.ae

###### Abstract

Many causal discovery methods typically rely on the assumption of independent noise, yet real-life situations often involve deterministic relationships. In these cases, observed variables are represented as deterministic functions of their parental variables without noise. When determinism is present, constraint-based methods encounter challenges due to the violation of the faithfulness assumption. In this paper, we find, supported by both theoretical analysis and empirical evidence, that score-based methods with exact search can naturally address the issues of deterministic relations under rather mild assumptions. Nonetheless, exact score-based methods can be computationally expensive. To enhance the efficiency and scalability, we develop a novel framework for causal discovery that can detect and handle deterministic relations, called Determinism-aware Greedy Equivalent Search (DGES). DGES comprises three phases: (1) identify minimal deterministic clusters (i.e., a minimal set of variables with deterministic relationships), (2) run modified Greedy Equivalent Search (GES) to obtain an initial graph, and (3) perform exact search exclusively on the deterministic cluster and its neighbors. The proposed DGES accommodates both linear and nonlinear causal relationships, as well as both continuous and discrete data types. Furthermore, we investigate the identifiability conditions of DGES. We conducted extensive experiments on both simulated and real-world datasets to show the efficacy of our proposed method. The code is available at https://github.com/lokali/DGES.git.

## 1 Introduction

Causal discovery from observational data has attracted considerable attention in recent decades and has been widely applied in various fields such as machine learning , healthcare , manufacturing  and neuroscience . Most causal discovery methods operate under the assumption of independent noises in the probabilistic system. However, real-world scenarios frequently encounter deterministic relationships. For example, the body mass index (BMI) is defined as the weight divided by the square of the body height, composing a deterministic relation among weight, height, and BMI.

Constraint-based and score-based methods are two primary categories in causal discovery. Constraint-based methods, such as PC  and FCI , leverage conditional independence tests (CIT) to estimate the graph skeleton and then determine the orientation. Under the Markov and faithfulness assumptions , these methods are guaranteed to asymptotically output the true Markov equivalence class (MEC). However, the faithfulness assumption is sensitive to many factors, such as the statistical errors with finite samples. Moreover, in the presence of deterministic relations, the faithfulness assumption isalways violated. Take the chain structure \(X Y Z\) for example where \(Y=f(X)\). In this case, faithfulness is violated due to the conditional independence \(Z\!\!\! Y|X\), i.e., when \(X\) is given, \(Y\) degenerates to a constant that is independent to any variables. Several variants of constraint-based methods  have been proposed to accommodate certain types of unfaithfulness. However, they generally provide practical flexibility but do not guarantee the identification to the true MEC.

For score-based methods, the approach can vary based on the search strategy, which may involve greedy search, exact search, or continuous optimization. One typical score-based method with greedy search is Greedy Equivalent Search (GES) , which searches in the space of MECs greedily by maximizing a well-defined score, such as Bayesian information criterion (BIC) score . Specifically, GES starts with an empty graph and consists of two phases. In the forward phase, it incrementally adds one edge at a time if it yields the maximum score improvement, continuing until no further edge can be added to enhance the score. In the backward phase, it checks all edges to eliminate some if removal further improves the score. Similar to the aforementioned constraint-based methods, GES converges to the true MEC in the large sample limit.

Some exact score-based methods aim at weakening the faithfulness assumption required for asymptotic correctness of the search results, such as dynamic programming (DP) , A* , and integer programming . The DAGs estimated by these methods can be converted to their MECs for causal interpretation . Lu et al.  demonstrated that these exact methods may produce correct results in cases where methods relying on faithfulness fail. Furthermore, Ng et al.  proved that exact score-based search with BIC can asymptotically outputs the true MEC when the sparsest Markov representation (SMR) assumption  is satisfied. Note that the SMR assumption is strictly weaker than the faithfulness assumption.

Deterministic relations have been considered in a few works of causal discovery. D-separation condition  is proposed for graphically determining conditional independence. Glymour  proposed a heuristic procedure to learn the causal graph in a deterministic system, called DPC, where only a subset of variables will be conditioned in testing conditional independence. Daniusis et al.  and Janzing et al.  considered a deterministic system with only two variables, and presented the idea of independent changes to infer the causal direction. Luo  and  incorporated the classical PC algorithm and utilized additional independence tests to handle determinism. Mabrouk et al.  combined a constraint-based approach with a greedy search that included specific rules to deterministic nodes and significantly reduce the incorrect learning. However, there is no identifiability guarantee in those related works. Moreover, Zeng et al.  assumes nonlinear additive noise model under high-dimensional deterministic data while Yang et al.  assumes linear non-Gaussian model. Different from them, this paper aims to provide a principled framework to handle deterministic relations for arbitrary functional models. More related works are given in Appendix A2.

**Contributions.** Firstly, we find that exact score-based methods can naturally be used to address the issues of deterministic relations when mild assumptions are fulfilled. Secondly, due to the large search space of the possible DAGs, the exact score-based methods are feasible only for small graphs and can be inefficient for large graphs. To enhance the efficiency and scalability, we propose a novel framework called **D**eterminism-aware **G**reedy **E**quivalent **S**earch (DGES), aimed at enhancing the efficiency and scalability to handle deterministic relations. Importantly, DGES is a general three-phase method, with no restricted assumption on the underlying functional causal models, i.e., it can accommodate both linear and nonlinear relationships, Gaussian and non-Gaussian data distributions, as well as continuous and discrete data types. Thirdly, we provide the identifiability conditions of DGES under general functional models. Last but not least, we conducted extensive experiments on both simulated and real-world datasets to validate our theoretical findings and show the efficacy of our proposed method.

**Paper organization.** In Section 2, we review the common assumptions, provide a motivating example why PC fails in dealing with deterministic relations, then present our intuitive solution using exact score-based method. In Section 3, we present our proposed DGES with three phases in details. Furthermore, we provide the identifiability conditions for DGES presented in a general form in Section 4. The empirical studies in Section 5 validate our theoretical results and show the efficacy of our method. Finally, we conclude our work with further discussions in Section 6.

Causal Discovery with Deterministic Relations

In this section, we first review the preliminaries of causal discovery, especially with deterministic relations, and then we provide some common assumptions that are related to our further analysis, as presented in section 2.1. Furthermore, we display two scenarios with deterministic relations where faithfulness can be violated in section 2.2, explaining why using constraint-based methods such as the PC algorithm can be problematic in addressing deterministic issues. Lastly, we provide an intuitive solution to handle the deterministic issues by exact score-based methods, as shown in section 2.3.

### Causal Discovery and Common Assumptions

Let \(=(,)\) be a DAG with the vertex set \(\) and edge set \(\). Consider \(d\) observable variables denoted by \(=(V_{1},V_{2},...,V_{d})\), and denote \(\) as its probability distribution. From a statistical view, \(X Y|Z\) denotes that \(X\) and \(Y\) are conditionally independent given \(Z\). Moreover, from a graph view, \(X_{d}Y|Z\) denotes that \(X\) and \(Y\) are d-separated by \(Z\). Given \(n\) data samples, the task of causal discovery aims at recovering the causal graph \(\) from the data matrix \(^{n d}\). Usually, each variable \(V_{i}\) with random noises can be represented by the following structural causal model (SCM): \(V_{i}=f_{i}(_{i},_{i})\), where \(_{i}\) is the set of all direct causes of \(V_{i}\), and \(_{i}\) is the random noise with non-zero variance related to \(V_{i}\), and we assume that \(_{i}\)'s are mutually independent. For variables with deterministic relations, the SCM becomes: \(V_{i}=f_{i}(_{i})\), where there is no extra noise. The relation can also be denoted as \(_{i} V_{i}\), where \(\) is the deterministic function mapping, showing \(_{i}\) determines \(V_{i}\). Throughout this paper, we assume causal sufficiency, i.e., no latent confounder.

**Terminologies.** Consider Figure 1(a) as an example, where \(V_{3}\) has deterministic relation with \(V_{1}\) and \(V_{2}\), i.e., \(V_{3}=V_{1}+V_{2}\), and \(V_{4}\) is a non-deterministic variable. Here we call the set of deterministic variables as a _deterministic cluster (DC)_, e.g., \(\{V_{1},V_{2},V_{3}\}\). Accordingly, all the non-deterministic variables make up a _non-deterministic cluster (NDC)_, e.g., \(\{V_{4}\}\). Meanwhile, the edges connecting between DC and NDC compose a _bridge set (BS)_, e.g., \(\{V_{2}\!\!\!\!V_{4},V_{3}\!\!\!\!V_{4}\}\).

**Assumption 1** (Markov): _Given a DAG \(\) and the distribution \(\) over the variable set \(\), each variable is probabilistically independent of its non-descendants given its parents in \(\)._

There are many DAGs that induce the same conditional independence relations with the distribution \(\), and it is said to be Markov equivalent. The Markov equivalent class (MEC) contains all the DAGs which entail the same conditional independence relations as \(\) does.

Another widely used assumption is faithfulness . It states that any conditional independence that holds in the probability distribution must correspond to a d-separation in the causal graph. When the Markov and faithfulness assumptions hold true, constraint-based methods, such as PC, have been proven to output the correct MEC asymptotically. However, in the finite sample regime, the faithfulness assumption is sensitive to statistical testing errors when inferring the CI relations, and the violations might occur often. When there are deterministic relations, faithfulness also fails. Glymour  proposes the non-deterministic faithfulness regarding only non-deterministic variables. Moreover, relaxations of faithfulness have been proposed, such as adjacency-faithfulness  and triangle-faithfulness . Another strictly weaker assumption is called Sparsest Markov Representation (SMR) , which is also known as the unique-frugality assumption [30; 31].

**Assumption 2** (Sparsest Markov Representation (SMR) ): _Given a DAG \(\) and the distribution \(\) over the variable set \(\), the MEC of \(\) is the unique sparsest MEC which satisfies the Markov assumption._

The idea behind SMR is to find the sparsest graphical representation that captures the essential conditional independence relationships in the data. The term "sparsest" refers to the minimal number

Figure 1: Two examples of causal graphs where faithfulness is violated. The gray nodes are deterministic variables. (a) \(\{V_{1},V_{2}\} V_{3}\). Violation reason is \(V_{4} V_{3}|\{V_{1},V_{2}\}\) but \(V_{4}_{d}V_{3}|\{V_{1},V_{2}\}\). (b) \(V_{1} V_{2}\). Violation reason is \(V_{3}\!\!\!\!V_{4}|V_{1}\) but \(V_{3}_{d}V_{4}|V_{1}\).

of edges in the graphical model. Under the SMR assumption, the exact score-based methods, such as A*  and DP , can produce asymptotically correct results for learning the true MEC.

### Faithfulness Violation by Deterministic Relations

Glymour  pointed out two and only two scenarios in the presence of deterministic relations where faithfulness can be violated. We summarize the two conditions and present the following assumption.

**Assumption 3** (Non-deterministic Faithfulness ): _Define a DAG \(\) and the distribution \(\) over the variable set \(\). \( X,Y\) and \(S\) in \(\), if \(X Y|S\) in \(\) and none of the following conditions holds:_

1. \(S X\) _or_ \(S Y\)_,_
2. \( S^{}\) _s.t._ \(X_{d}Y|S^{}\) _and_ \(S S^{}\)_,_

_then \(X_{d}Y|S\) in \(\)._

**Remarks:** It assumes there is no other coincidental independence besides the two conditions. In other words, the two conditions are the only two cases leading to faithfulness violation due to deterministic relations. In fact, this assumption is equivalent to the completeness of D-separation criteria in Spirtes et al. . We will use two graph examples, as shown in Figure 1, to explain the above two conditions.

Firstly, given condition (\(i\)) and Figure 1(a), we can assign \(S=\{V_{1},V_{2}\}\) and \(X=V_{3}\), where \(S X\). Given \(\{V_{1},V_{2}\}\), \(V_{3}\) will always be conditionally independent from \(V_{4}\), because \(V_{3}\) can be determined by \(\{V_{1},V_{2}\}\) with no extra noise term, the estimated residue for regressing \(V_{3}\) on \(\{V_{1},V_{2}\}\) will be close to 0. Therefore, \(V_{4} V_{3}|\{V_{1},V_{2}\}\) holds true from a statistical view. However, \(V_{4}_{d}V_{3}|\{V_{1},V_{2}\}\) from a graph view. Therefore, in this case, faithfulness is violated.

The key rule of constraint-based method (e.g., PC algorithm) is that if we find at least one conditional set or an empty set so that two variables are conditionally independent, then the edge between these two variables in the graph will be removed. Therefore, we can conclude that using constraint-based methods which rely on faithfulness to deal with deterministic relations can be problematic.

Secondly, given condition (\(ii\)) and Figure 1(b), we can assign \(S=V_{1}\), \(S^{}=V_{2}\), \(X=V_{3}\) and \(Y=V_{4}\), where \(S S^{}\). From the graph, we can see that \(V_{3}_{d}V_{4}|V_{2}\) and \(V_{3}_{d}V_{4}|V_{1}\). However, from a statistical view \(V_{3} V_{4}|V_{2}\), since \(V_{2}=V_{1}\), we also have \(V_{3} V_{4}|V_{1}\). Here, conditional independence does not imply d-separation. Therefore, faithfulness is also violated.

### Intuitive Solution: Exact Search

Benefiting from the recent theoretical progress on exact score-based methods, which do not explicitly rely on faithfulness assumption, it enables us to deal with deterministic relations from an intuitive view. Here, we are inspired by the lemma as follows.

**Lemma 1** (Linear Identifiability of Exact Search ): _Exact score-based search with BIC score asymptotically outputs a DAG that belongs to the MEC of the true DAG \(\) if and only if the DAG \(\) and distribution \(\) satisfy the SMR assumption._

According to Lemma 1, in the linear case, as long as the SMR assumption is satisfied, the exact score-based method with BIC score  can asymptotically obtain the true MEC. Then, we can extend the theoretical result from linear to nonlinear scenarios. The exact score-based method with generalized score  can also asymptotically output the true MEC.

**Theorem 2** (General Identifiability of Exact Search): _Exact score-based search with generalized score asymptotically outputs a DAG that belongs to the MEC of the true DAG \(\) if and only if the DAG \(\) and distribution \(\) satisfy the SMR assumption and some mild conditions are satisfied._

**Remarks:** The complete proof is given in Appendix A.4.1. Based on the theoretical findings in Lemma 1 and Theorem 2, the exact score-based methods, which do not specifically require faithfulness but SMR, pave a promising way to deal with the deterministic relations for causal discovery. However, one critical disadvantage of the exact methods is their low computational efficiency and poor scalability. To that end, we propose a novel framework, called DGES, which is demonstrated in section 3. The identifiability conditions for DGES are provided in section 4.

```
0: data matrix \(^{n d}\)
0: a causal graph \(\)
1: (_Phase 1: Detect Minimal Deterministic Clusters_) Detect the minimal deterministic clusters, by checking whether one variable can be minimally determined by some other variables.
2: (_Phase 2: Run Modified Greedy Search Globally_) Run modified greedy equivalent search on the whole set of variables to obtain an initial graph.
3: (_Phase 3: Run Exact Search Partially_) Perform the exact search exclusively on the deterministic clusters and their neighboring variables, as post-processing. ```

**Algorithm 1**\(\): **D**eterminism-aware **G**eredy **E**quivalent **S**earch

## 3 Determinism-aware Greedy Equivalent Search (DGES)

In this section, we will introduce our proposed DGES in detail. Throughout this paper, we consider the general case without assuming any functional causal models. In general, DGES contains three phases: Firstly, we need to detect all the minimal deterministic clusters. If one variable can be deterministically represented by some other variables, we may conclude that it is a deterministic variable. Secondly, based on the DC information, we run modified GES to get the initial causal graph. Thirdly we perform the exact search exclusively on the DC and their neighbors, as post-processing. The general framework is given in Algorithm 1. The contents are organized as follows. The details of deterministic cluster detection in Phase 1 are discussed in section 3.1. More information about our modified GES in Phase 2 is introduced in section 3.2. Finally, we discuss exact search in section 3.3.

### Minimal Deterministic Clusters Detection

A _minimal deterministic cluster (MinDC)_ refers to a minimal set of variables involved in a deterministic relation. A DC can be seen as a union of all MinDCs in the graph. For example, \(V_{1} V_{2}\) and \(V_{1} V_{3}\), then \(\{V_{1},V_{2}\}\) and \(\{V_{1},V_{3}\}\) are two MinDCs, while \(\{V_{1},V_{2},V_{3}\}\) composes a DC.

First of all, we need to obtain the DC, which contains all the deterministic variables. For each variable \(V_{i}\), \(i\{1,...,d\}\), if this variable can be deterministically represented by all the other variables, i.e., \(\{ V_{i}\} V_{i}\), then this variable must be in DC. After traversing all \(d\) variables, we obtain the DC.

However, within the DC, there may be multiple deterministic relations, even some overlapping deterministic variables. Therefore, out of the DC, we need to get a set of MinDCs. For each variable \(V_{i}\), we try to detect whether there exists a minimal set \(S\) such that \(S V_{i}\), where \(V_{i}\), \(S\) and \(V_{i} S\). Here, we need to traverse all the possible combination sets of DC, and see whether one deterministic variable can be minimally represented by some other variables. If so, then those variables compose a MinDC. In the end, we can obtain a list of MinDCs. More details about DC detection, MinDC detection, and how to check \(S V_{i}\), are given in Appendix A3.1.

### Modified Greedy Equivalent Search

The modified GES is based on the standard GES . We add some extra constraints during the forward and backward steps and adjust the score functions due to the deterministic relations. When using score functions for causal discovery, we aim for the underlying causal graph or its equivalent class to give the optimal score. Specifically, we desire that the score of a DAG model (1) increases as the result of adding any edge that eliminates an independence constraint that does not hold in the generative distribution, and (2) decreases as a result of adding any edge that does not eliminate such a constraint. More formally, we have the following definition of score local consistency.

**Definition 1** (Score Local Consistency ): _Let \(\) be any DAG, and let \(^{}\) be the DAG that results from adding the edge \(V_{i} V_{j}\) on \(\). Let \(D\) be the dataset from the distribution \(\). A score function \((;D)\) is locally consistent if the following two properties hold as the sample size \(n\):_

1. _If_ \(V_{i} V_{j}|PA_{j}^{}\)_, then_ \((^{};D)>(;D)\)_._
2. _If_ \(V_{i} V_{j}|PA_{j}^{}\)_, then_ \((^{};D)<(;D)\)_._

**Modification 1: Edge Adding and Deleting.** During the forward phase, at each step with a DAG \(\) in the equivalence class, an edge \(V_{i} V_{j}\) is added when 1) \(V_{i}}}V_{j}|_{j}^{ }\), and 2) \(_{j}^{}\) does not determine any of \(V_{i},V_{j}\), until no edge can be added. However, when \(_{j}^{}\) determines \(V_{i}\) or \(V_{j}\), we always have \(V_{i}}}V_{j}|_{j}^{ }\). In this case, we always ignore such independence, directly regard it as dependent, and add such an edge to the graph. The motivation behind the modification is to ensure that no false independence due to deterministic relations is introduced, and in the end, the output graph is guaranteed to be Markovian.

During the backward phase, at each step with a DAG \(\) in the equivalence class, an edge \(V_{i} V_{j}\) is removed when both 1) \(V_{i}}}V_{j}|_{j}^{ }\), and 2) \(_{j}^{}\) does not determine any of \(V_{i},V_{j}\), until no edge can be removed. Similar to the modification in the forward phase, when \(_{j}^{}\) determines \(V_{i}\) or \(V_{j}\), we still trust the dependency and keep the edge \(V_{i} V_{j}\). Although the resulting equivalence class will be Markovian to the ground truth, redundant edges will exist.

Fortunately, we have Phase 3 exact search as post-processing, which will be introduced next in Section 3.3. Under the SMR assumption, we can obtain a more sparse graph. In the end, the exact search will remove all those redundant edges. A motivating example showing the advantages of our modified forward and backward phases is provided in Appendix A3.2 and Figure A2.

**Modification 2: Score Function.** During the phase 1 with greedy search and phase 3 with exact search, a proper score function is inevitably needed. For any scoring criterion \((,)\), we say that a score is _decomposable_ if it can be written as a sum of local scores, where each local score is a function of only one variable and its parents. Following the property, the score of a DAG \(\) can be represented as

\[(;)=_{i=1}^{d}(V_{i}, _{i}^{}).\] (1)

Under the linear Gaussian model, the BIC score  is preferred, which is given as

\[_{BIC}(V_{i},_{i}^{ })=- L+^{}k n,\\ \ \  L-(1+||),\] (2)

where \(L\) is the maximized value of the likelihood function of the model based on the observed data \(\) related to \(V_{i}\) and \(_{i}\), \(k\) denotes the number of edges between \(V_{i}\) and \(_{i}\) in \(\), \(n\) is the number of data samples in \(\), \(^{}\) is the penalty parameter, \(\) is the variance of the noise term.

However, in the deterministic scenarios, the estimated noise variance \(\) will asymptotically get closer to 0, which leads to numerical error because of the term \(||\). To deal with such an issue, we provide the adjusted BIC score, formulated as

\[}_{BIC}(V_{i},_ {i}^{})=- L^{}+^{}k n,\\ \ \  L^{}-(1+|+ |),\] (3)

where \(\) is a small constant, and \(>0\).

Under the general nonlinear model, the generalized score (GS)  which is in a non-parametric form is favored. There are two types of likelihoods as introduced in the paper, for computational efficiency, we choose the generalized score with cross-validated (CV) likelihood.

\[_{GS}(V_{i},_{i}^{})= _{q=1}^{Q}(F_{i}^{(q)}|D_{0,i}^{(q)}), and\]

\[(}_{i}^{(q)}|D_{0,i}^{(q)}) =-^{2}}{2}(2)-}{2}|n_{1} ^{2}_{V_{i}}^{1(q)}(_{PA_{i}^{}}^{1(q)}+n _{1} I)^{-2}_{V_{i}}^{0(q)}|\] \[-_{ V_{i}}^{0(q)}_{V_{i}}^{0(q)}+_{PA_{i}^{ }}^{0(q)}A_{i}^{}A_{i}_{PA_{i}^{}}^{1,0(q)}-n_{1}_{PA_{i}^{}}^{0,1(q)}A_{i}^{}B_{i}A _{i}_{PA_{i}^{}}^{1,0(q)}\] \[+2n_{1}_{V_{i}}^{0(q)}B_{i}A_{i}_{PA_{i}^{ }}^{1,0(q)}-_{V_{i}}^{0(q)}A_{i} _{PA_{i}^{}}^{1,0(q)}-n_{1}_{V_{i}}^{0(q)}B_{i}_{ V_{i}}^{0(q)}},\] (4)

where \(A_{i}=_{V_{i}}^{1(q)}(_{PA_{i}^{}}^{1(q)}+n_{1}  I)^{-1}\), \(B_{i}=A_{i}I+n_{1} A_{i}^{}A_{i}^{-1}A_{i}^{ }\), \(\) is the regularization parameter, \(n_{1}\) is the sample size of each training set, \(n_{0}\) is the sample size of each test set, \(n=n_{1}+n_{0}\), \(D_{1,i}^{(q)}\) and \(D_{0,i}^{(q)}\) are the corresponding data of variable \(V_{i}\) and its parents, \(_{V_{i}}^{1(q)}\) denotes the centralized kernel matrix of the \(q\)-th training set of \(V_{i}\), \(_{V_{i}}^{0(q)}\) denotes that of the \(q\)-th test set of \(V_{i}\), and similar notations are used for other kernel matrices.

### Exact Search as Post-processing

As demonstrated by Lu et al. , GES may get sub-optimal results when the faithfulness assumption is violated, e.g., when there are deterministic relations. An example is given in Figure 2. In this example, the DC is \(\{V_{1},V_{2},V_{3},V_{4}\}\). The true incoming edges to \(V_{6}\) should be \(\{V_{3},V_{4}\}\), however, the estimated graph by GES may have \(\{V_{1},V_{2},V_{4}\}\) pointing to \(V_{6}\). We need to partially conduct an exact search based on the GES result to identify BS, under the SMR assumption. Therefore, in Phase 3, we perform the exact search exclusively on the DC and their neighbors. Benefiting from the recent theoretical progress on exact score-based methods, which do not explicitly rely on faithfulness assumption, it enables us to deal with deterministic relations from an intuitive view.

## 4 Identifiability Conditions

In this section, we provide the identifiability conditions of DGES. The conditions are presented in a general form, applicable to both linear and nonlinear causal models. As mentioned above, in a general deterministic system, the whole causal graph mainly can be divided into three parts: DC, NDC, and BS. In this paper, we focus on the identifiability for the BS and NDC parts.

**Theorem 3** (Partial Identifiability): _Denote a causal graph \(\) with deterministic relations. Let \(V_{i}\) be any non-deterministic variable in \(\), and \(_{i}\) be the set of direct causes or undirected neighbors of \(V_{i}\) in one MinDC. Suppose the following conditions hold_

1. _Assumptions_ 1_,_ 2_, and_ 3 _hold,_
2. \(|\,_{i}\,|<|\,\,|-1\)_,_

_where \(||\) denotes the cardinality of a set. Then, when the sample size \(n\), we can identify the BS and NDC parts of the causal graph \(\) to their true Markov equivalent class._

## 5 Experiments

To validate our theoretical findings and show the efficacy of our method, we conducted extensive experiments on simulated and real-world datasets. Specifically, for simulated datasets, we evaluate both linear and general nonlinear functional models.

**Simulated Datasets.** The true DAGs are simulated using the Erdos-Renyi model  with the number of edges equal to the number of variables. We evaluate linear Gaussian model and general nonlinear model with mixed functions, each with varying number of variables and samples. Moreover, we also evaluate general nonlinear model generated by MLP on varying number of variables. For each setting, we randomly choose one MinDC or two MinDCs where each MinDC has at least three variables. For the exact method in Phase 3, we choose A*  without the heuristic tricks. We compare our DGES with other baselines, including DPC , GES , and A* . We compare the MEC of the output by all methods. Note that we only evaluate the BS part which we aim to identify. We consider the structural Hamming distance (SHD), the \(F_{1}\) score, the precision, the recall, and the computational time as evaluation criteria. For each setting, we run 10 different random seeds and report the mean and standard deviation. More implementation details are in Appendix A5.1.

The simulated results about graphs with only one DC has been shown in Figure 3, and the results with two DCs (which may have overlapping variables) are given in Figure A4 of Appendix. Clearly, when there are more deterministic variables in the system, the runtime of our DGES will obviously increase. The reason is because there are more deterministic variables to be detected and fed into Phase 3 for exact search. According to the results, the general performance of DGES is competitive compared to other baselines. We observe that the exact method A* and our proposed DGES generally outperform the other baselines such as GES and DPC across different criteria and settings. Meanwhile, score-based method GES presents better performance than constraint-based method DPC in a deterministic

Figure 3: Results on the simulated datasets with one MinDC. We evaluate different functional causal models on varying number of variables and samples, respectively. For each setting, we consider SHD (\(\)), \(F_{1}\) score (\(\)), precision (\(\)), recall (\(\)) and runtime (\(\)) as evaluation criteria.

system. As the number of variable increases, the runtime of A* will increase rapidly. Compared to A*, the increasing of runtime for DGES is much more steady, both in linear and nonlinear models. More results about two MinDCs, non-deterministic scenarios, and relaxed exact search such as GRaSP , are provided in Appendix A5.

**Real-world Datasets.** We also evaluate our method and the baselines on two real-world datasets. One is the pharmacokinetics dataset , which is an open database for pharmacokinetics information from clinical trials. It provides curated information mainly in two categories: the characteristics of the studied individuals (e.g., age, height) and the measurement records (e.g., the clearance, \(T_{max}\), \(C_{max}\) when one certain individual takes one certain drug), and we name the two categories of variables as class "I" (individual) and "M" (measurement), respectively. Out of more than 200 variables and more than 200000 data samples containing missing values, we cleaned the data and finally obtained 32 important variables with 4194 data samples which may contain deterministic relations. The 32 variables contains 18 and 14 variables from the class "I" and "M", respectively. We prepend the class label to each variable name as a prefix. We use linear BIC score and nonlinear generalized score to conduct the search. Figure 4 gives the DGES result with generalized score, where we can successfully detect at least three MinDCs: {height, weight, BMI}, {\(k_{el}\), \(V_{d}\), Clearance}, {\(k_{el}\), \(T_{half}\)}. Compared with the linear DGES result with BIC score, we can see more reasonable edges existing in the nonlinear DGES result with the generalized score, for example, {age \(-\) medication, healthy \(\) disease, healthy \(-\) BMI}. More results and analysis are provided in Appendix A6.

The other one is the US census Public Use Microdata Sample (PUMS). We follow the data preprocessing procedure outlined in , which is a modern version of the UCI Adult data set . Datasets based on census data are widely considered in the algorithmic fairness literature . Here we choose 5 important variables, i.e., Age, Occupation, Sex, Annual income (AI), and Adjusted annual income (AAI), in total there are 3000 samples. Because of the potentially different timeframe of the survey cycle, AAI (= AI * Adjusted factor) are the adjusted dollar amounts that they have earned entirely during the calendar year. Within one calendar year, this adjusted factor is a constant. Here, we choose the data in 2021. Therefore, AAI and AI have a deterministic relation. The result of DGES is: {Sex \(\) Occupation \(\) AI, AI \(\) AAI, Sex \(\) AAI \(\) Age}, 5 edges. The result of GES is: {Sex \(\) Occupation \(-\) AAI, AI \(-\) AAI \(-\) Age, AI \(\) Sex \(\) AAI}, 6 edges. The result of PC is: {Sex \(\) Occupation \(\) Age, AI \(-\) AAI}, 3 edges. Compared with GES, the result of DGES is more sparse. Particularly, we can detect that AI and AAI have a deterministic relation, and GES gives redundant edges by {AI \(\) Sex \(\) AAI} while our DGES only keeps one edge {Sex \(\) AAI}. Moreover, the result of PC is totally different from the other two. Clearly, in our DGES result, AI and AAI are still connected, and we can still see the BS, i.e., {AI \(\) Occupation, AAI \(-\) Sex, AAI\(-\)Age}. However, as a result of PC with FisherZ test, the BS becomes empty, which is exactly due to the violation of faithfulness.

Figure 4: Results on the real-world dataset with deterministic relations by DGES with Generalized score.

Discussions and Conclusion

**Limitations.** While presenting a versatile framework, our paper does have certain limitations. Firstly, in some cases, e.g., with overlapping deterministic variables, our method cannot identify the skeleton and directions in the DC part so far. We display two graphs that we can identify up to MEC and the other two that we cannot identify in Figure A1. More discussion is in Appendix A1. Secondly, inherited from the disadvantages of exact methods, our method can be somewhat computationally expensive in Phase 3 when there are a large number of MinDCs. Fortunately, each MinDC is usually not too large, and we may execute the exact search for different MinDCs simultaneously.

**Broader Impacts.** The overarching aim of our proposed method is to learn the causal structures from any general functional causal models in the presence of deterministic relations. This is a fundamental and critical task with wide-ranging applications in practical life, and we firmly believe that our method will serve beneficial purposes without engendering negative societal impacts.

**Conclusion.** This paper dives into the challenges of causal discovery in the presence of deterministic relations. Notably, we make a compelling discovery that exact score-based methods can elegantly address the deterministic issues, provided the SMR assumption is met. In an effort to bolster efficiency and scalability in a deterministic system, we propose the novel and versatile framework called DGES, encompassing both linear and nonlinear models, as well as both continuous and discrete data types. Furthermore, we establish the partial identifiability conditions for DGES. Hopefully, our method can help to construct a holistic view to see the deterministic relations. The extensive experiments on simulated and real-world datasets, validate our theoretical findings and the efficacy of our method.