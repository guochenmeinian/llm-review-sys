# PUZZLES: A Benchmark for Neural

Algorithmic Reasoning

 Benjamin Estermann

ETH Zurich

esternann@ethz.ch

&Luca A. Lanzendorfer

ETH Zurich

lanzendoerfer@ethz.ch

Yannick Niedermayr

ETH Zurich

yannickn@ethz.ch

&Roger Wattenhofer

ETH Zurich

wattenhofer@ethz.ch

###### Abstract

Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments. These advancements have been enabled in part by the availability of benchmarks. In this work we introduce PUZZLES, a benchmark based on Simon Tatham's Portable Puzzle Collection, aimed at fostering progress in algorithmic and logical reasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizes and varying levels of complexity; many puzzles also feature a diverse set of additional configuration parameters. The 40 puzzles provide detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we evaluate various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research. All the software, including the environment, is available at https://github.com/ETH-DISCO/rlp.

Human intelligence relies heavily on logical and algorithmic reasoning as integral components for solving complex tasks. While Machine Learning (ML) has achieved remarkable success in addressing many real-world challenges, logical and algorithmic reasoning remains an open research question [1; 2; 3; 4; 5; 6; 7]. This research question is supported by the availability of benchmarks, which allow for a standardized and broad evaluation framework to measure and encourage progress [8; 9; 10].

Reinforcement Learning (RL) has made remarkable progress in various domains, showcasing its capabilities in tasks such as game playing [11; 12; 13; 14; 15], robotics [16; 17; 18; 19] and control systems [20; 21; 22]. Various benchmarks have been proposed to enable progress in these areas [23; 24; 25; 26; 27; 28; 29]. More recently, advances have also been made in the direction of logical and algorithmic reasoning within RL [30; 31; 32]. Popular examples also include the games of Chess, Shogi, and Go [33; 34]. Given the importance of logical and algorithmic reasoning, we propose a benchmark to guide future developments in RL and more broadly machine learning.

Logic puzzles have long been a playful challenge for humans, and they are an ideal testing ground for evaluating the algorithmic and logical reasoning capabilities of RL agents. A diverse range of puzzles, similar to the Atari benchmark , favors methods that are broadly applicable. Unlike tasks with a fixed input size, logic puzzles can be solved iteratively once an algorithmic solution is found. This allows us to measure how well a solution attempt can adapt and generalize to larger inputs. Furthermore, in contrast to games such as Chess and Go, logic puzzles have a known solution, making reward design easier and enabling tracking progress and guidance with intermediate rewards.

In this paper, we introduce PUZZLES, a comprehensive RL benchmark specifically designed to evaluate RL agents' algorithmic reasoning and problem-solving abilities in the realm of logical and algorithmic reasoning. Simon Tatham's Puzzle Collection , curated by the renowned computer programmer and puzzle enthusiast Simon Tatham, serves as the foundation of PUZZLES. This collection includes a set of 40 logic puzzles, shown in Figure 1, each of which presents distinct challenges with various dimensions of adjustable complexity. They range from more well-known puzzles, such as _Solo_ or _Mines_ (commonly known as _Sudoku_ and _Minesweeper_, respectively) to lesser-known puzzles such as _Cube_ or _Slant_. PUZZLES includes all 40 puzzles in a standardized environment, each playable with a visual or discrete input and a discrete action space.

Contributions.We propose PUZZLES, an RL environment based on Simon Tatham's Puzzle Collection, comprising a collection of 40 diverse logic puzzles. To ensure compatibility, we have extended the original C source code to adhere to the standards of the Pygame library. Subsequently, we have integrated PUZZLES into the Gymnasium framework API, providing a straightforward, standardized, and widely-used interface for RL applications. PUZZLES allows the user to arbitrarily scale the size and difficulty of logic puzzles, providing detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we have evaluated various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research.

## 1 Related Work

RL benchmarks.Various benchmarks have been proposed in RL. Bellemare et al.  introduced the influential Atari-2600 benchmark, on which Mnih et al.  trained RL agents to play the games directly from pixel inputs. This benchmark demonstrated the potential of RL in complex, high-dimensional environments. PUZZLES allows the use of a similar approach where only pixel inputs are provided to the agent. Todorov et al.  presented MuJoCo which provides a diverse set of continuous control tasks based on a physics engine for robotic systems. Another control benchmark is the DeepMind Control Suite by Duan et al. , featuring continuous actions spaces and complex control problems. The work by Cote et al.  emphasized the importance of natural language understanding in RL and proposed a benchmark for evaluating RL methods in text-based domains. Lanctot et al.  introduced OpenSpiel, encompassing a wide range of games, enabling researchers to evaluate and compare RL algorithms' performance in game-playing scenarios. These benchmarks and frameworks have contributed significantly to the development and evaluation of RL algorithms. OpenAI Gym by Brockman et al. , and its successor Gymnasium by the Farama Foundation , helped by providing a standardized interface for many benchmarks. As such, Gym and Gymnasium

Figure 1: All puzzle classes of Simon Tathamâ€™s Portable Puzzle Collection.

have played an important role in facilitating reproducibility and benchmarking in reinforcement learning research. Therefore, we provide PUZZLES as a Gymnasium environment to enable ease of use.

Logical and algorithmic reasoning within RL.Notable research in RL on logical reasoning includes automated theorem proving using deep RL  or RL-based logic synthesis . Dasgupta et al.  find that RL agents can perform a certain degree of causal reasoning in a meta-reinforcement learning setting. The work by Jiang and Luo  introduces Neural Logic RL, which improves interpretability and generalization of learned policies. Eppe et al.  provide steps to advance problem-solving as part of hierarchical RL. Fawzi et al.  and Mankowitz et al.  demonstrate that RL can be used to discover novel and more efficient algorithms for well-known problems such as matrix multiplication and sorting. Neural algorithmic reasoning has also been used as a method to improve low-data performance in classical RL control environments [40; 41]. Logical reasoning might be required to compete in certain types of games such as chess, shogi and Go [33; 34; 42; 13], Poker [43; 44; 45; 46] or board games [47; 48; 49; 50]. However, these are usually multi-agent games, with some also featuring imperfect information and stochasticity.

Reasoning benchmarks.Various benchmarks have been introduced to assess different types of reasoning capabilities, although only in the realm of classical ML. IsarStep, proposed by Li et al. , specifically designed to evaluate high-level mathematical reasoning necessary for proof-writing tasks. Another significant benchmark in the field of reasoning is the CLRS Algorithmic Reasoning Benchmark, introduced by Velickovic et al. . This benchmark emphasizes the importance of algorithmic reasoning in machine learning research. It consists of 30 different types of algorithms sourced from the renowned textbook "Introduction to Algorithms" by Cormen et al. . The CLRS benchmark serves as a means to evaluate models' understanding and proficiency in learning various algorithms. In the domain of large language models (LLMs), BIG-bench has been introduced by Srivastava et al. . BIG-bench incorporates tasks that assess the reasoning capabilities of LLMs, including logical reasoning.

Despite these valuable contributions, a suitable and unified benchmark for evaluating logical and algorithmic reasoning abilities in single-agent perfect-information RL has yet to be established. Recognizing this gap, we propose PUZZLES as a relevant and necessary benchmark with the potential to drive advancements and provide a standardized evaluation platform for RL methods that enable agents to acquire algorithmic and logical reasoning abilities.

## 2 The Puzzles Environment

In the following section, we give an overview of the PUZZLES environment.1The environment is written in both Python and C. For a detailed explanation of all features of the environment as well as their implementation, please see Appendices B and C.

### Environment Overview

Within the PUZZLES environment, we encapsulate the tasks presented by each logic puzzle by defining consistent state, action, and observation spaces. It is also important to note that the large majority of the logic puzzles are designed so that they can be solved without requiring any guesswork. By default, we provide the option of two observation spaces, one is a representation of the discrete internal game state of the puzzle, the other is a visual representation of the game interface. These observation spaces can easily be wrapped in order to enable PUZZLES to be used with more advanced neural architectures such as graph neural networks (GNNs) or Transformers. All puzzles provide a discrete action space which only differs in cardinality. To accommodate the inherent difficulty and the need for proper algorithmic reasoning in solving these puzzles, the environment allows users to implement their own reward structures, facilitating the training of successful RL agents. All puzzles are played in a two-dimensional play area with deterministic state transitions, where a transition only occurs after a valid user input. Most of the puzzles in PUZZLES do not have an upper bound on the number of steps, they can only be completed by successfully solving the puzzle. An agent with a bad policy is likely never going to reach a terminal state. For this reason, we provide the option for early episode termination based on state repetitions. As we show in Section 3.4, this is an effective method to facilitate learning.

### Difficulty Progression and Generalization

The PUZZLES environment places a strong emphasis on giving users control over the difficulty exhibited by the environment. For each puzzle, the problem size and difficulty can be adjusted individually. The difficulty affects the complexity of strategies that an agent needs to learn to solve a puzzle. As an example, _Sudoku_ has tangible difficulty options: harder difficulties may require the use of new strategies such as _forcing chains_3 to find a solution, whereas easy difficulties only need the _single position_ strategy.4

The scalability of the puzzles in our environment offers a unique opportunity to design increasingly complex puzzle configurations, presenting a challenging landscape for RL agents to navigate. This dynamic nature of the benchmark serves two important purposes. Firstly, the scalability of the puzzles facilitates the evaluation of an agent's generalization capabilities. In the PUZZLES environment, it is possible to train an agent in an easy puzzle setting and subsequently evaluate its performance in progressively harder puzzle configurations. For most puzzles, the cardinality of the action space is independent of puzzle size. It is therefore also possible to train an agent only on small instances of a puzzle and then evaluate it on larger sizes. This approach allows us to assess whether an agent has learned the correct underlying algorithm and generalizes to out-of-distribution scenarios. Secondly, it enables the benchmark to remain adaptable to the continuous advancements in RL methodologies. As RL algorithms evolve and become more capable, the puzzle configurations can be adjusted accordingly to maintain the desired level of difficulty. This ensures that the benchmark continues to effectively assess the capabilities of the latest RL methods.

## 3 Empirical Evaluation

We evaluate the baseline performance of numerous commonly used RL algorithms on our PUZZLES environment. Additionally, we also analyze the impact of certain design decisions of the environment and the training setup. Our metric of interest is the average number of steps required by a policy to

Figure 2: Code and library landscape around the PUZZLES Environment, made up of the rlp Package and the puzzle Module. The figure shows how the puzzle Module presented in this paper fits within Tathamsâ€™s Puzzle Collectionâ€™code, the Pygame package, and a userâ€™s Gymnasium reinforcement learning code. The different parts are also categorized as Python language and C language.

successfully complete a puzzle, where lower is better. We refer to the term _successful episode_ to denote the successful completion of a single puzzle instance. We also look at the success rate, i.e. what percentage of the puzzles was completed successfully.

To provide an understanding of the puzzle's complexity and to contextualize the agents' performance, we include an upper-bound estimate of the optimal number of steps required to solve the puzzle correctly. This estimate is a combination of both the steps required to solve the puzzle using an optimal strategy, and an upper bound on the environment steps required to achieve this solution, such as moving the cursor to the correct position. The upper bound is denoted as _Optimal_. Please refer to Table 6 for details on how this upper bound is calculated for each puzzle. Further, we include the performance of a human expert as reference. The human expert is able to solve all puzzles in our evaluated difficulty levels within the optimal upper bound. For detailed results on the performance of the human expert, please refer to Appendix F.2.

We run experiments based on all the RL algorithms presented in Table 9. We include both popular traditional algorithms such as PPO, as well as algorithms designed more specifically for the kinds of tasks presented in PUZZLES. Where possible, we used the implementations available in the RL library Stable Baselines 3 , using the default hyperparameters. For MuZero and DreamerV3, we used the code available at  and , respectively. We provide a summary of all algorithms in Appendix Table 9. In total, our experiments required approximately 10'000 GPU hours.

All selected algorithms are compatible with the discrete action space required by our environment. This circumstance prohibits the use of certain other common RL algorithms, such as Soft-Actor Critic (SAC)  or Twin Delayed Deep Deterministic Policy Gradients (TD3) .

### Baseline Experiments

For the general baseline experiments, we trained all agents on all puzzles and evaluate their performance. Due to the challenging nature of our puzzles, we have selected an easy difficulty and small size of the puzzle where possible. Every agent was trained on the discrete internal state observation using five different random seeds. We trained all agents by providing rewards only at the end of each episode upon successful completion or failure. For computational reasons, we truncated all episodes during training and testing at 10,000 steps. For such a termination, reward was kept at 0. We evaluate the effect of this episode truncation in Section 3.4. We provide all experimental parameters, including the exact parameters supplied for each puzzle in Appendix F.1.

To track an agent's progress, we use episode lengths, i.e., how many actions an agent needs to solve a puzzle. A lower number of actions indicates a stronger policy that is closer to the optimal solution. To obtain the final evaluation, we run each policy on 1000 random episodes of the respective puzzle, again with a maximum step size of 10,000 steps. All experiments were conducted on NVIDIA 3090 GPUs. The training time for a single agent with 2 million PPO steps varied depending on the puzzle and ranged from approximately 1.75 to 3 hours. The training for DreamerV3 and MuZero was more demanding and training time ranged from approximately 10 to 20 hours.

Figure 2(b) shows the average successful episode length for all algorithms, created following the recommendations outlined in . It can be seen that DreamerV3 performs best when looking at success rate and episode length, with TRPO, PPO and DQN following closely. MuZero suffers from instable training, where a successful strategy was only learned for a low number of puzzles, indicating the need for puzzle-specific hyperparameter tuning. The results are especially interesting since PPO and TRPO follow much simpler training routines than DreamerV3 and MuZero. It seems that the implicit world models learned by DreamerV3 struggle to appropriately capture some puzzles. Upon closer inspection of the detailed results, presented in Appendix Table 10 and 11, DreamerV3 manages to solve 62.7% of all puzzle instances. In 14 out of the 40 puzzles, it has found a policy that solves the puzzles within the _Optimal_ upper bound. PPO and TRPO managed to solve an average of 61.6% and 70.8% of the puzzle instances, however only 8 and 11 of the puzzles have consistently solved within the _Optimal_ upper bound. The algorithms A2C, RecurrentPPO, DQN and QRDQN perform worse than a pure random policy. Overall, it seems that some of the environments in PUZZLES are quite challenging and well suited to show the difference in performance between algorithms. It is also important to note that all the logic puzzles are designed so that they can be solved without requiring any guesswork.

### Difficulty

We further evaluate the performance of a subset of the puzzles on the easiest preset difficulty level for humans. We selected all puzzles where a random policy was able to solve them with a probability of at least 10%, which are Netslide, Same Game and Untangle. By using this selection, we estimate that the reward density should be relatively high, ideally allowing the agent to learn a good policy. Again, we train all algorithms listed in Table 9. We provide results for the two strongest algorithms, PPO and DreamerV3 in Table 1, with complete results available in Appendix Table 10. Note that as part of Section 3.4, we also perform ablations using DreamerV3 on more puzzles on the easiest preset difficulty level for humans.

We observe that for both PPO and DreamerV3, the percentage of successful episodes decreases, with a large increase in steps required. DreamerV3 performs clearly stronger than PPO, requiring

  
**Puzzle** & **Parameters** & **PPO** & **DreamerV3** & **Optimal** & **Human Expert** \\   & 2x3b1 & \(35.3 0.7\) & (100.0\%) & \(12.0 0.4\) & (100.0\%) & 48 & 16.7 \\  & 3x3b1 & \(4742.1 2960.1\) & (9.2\%) & \(3586.5 676.9\) & (22.4\%) & 90 & 40.9 \\   & 2x3c3a2 & \(11.5 0.1\) & (100.0\%) & \(7.3 0.2\) & (100.0\%) & 42 & 8.7 \\  & 5x5c3a2 & \(1009.3 1089.4\) & (30.5\%) & \(527.0 162.0\) & (30.2\%) & 300 & 37.0 \\   & 4 & \(34.9 10.8\) & (100.0\%) & \(6.3 0.4\) & (100.0\%) & 80 & 6.0 \\  & 6 & \(2294.7 2121.2\) & (96.2\%) & \(1683.3 73.7\) & (82.0\%) & 150 & 30.5 \\   

Table 1: Comparison of how many steps agents trained with PPO and DreamerV3 need on average to solve puzzles of two difficulty levels. In brackets, the percentage of successful episodes is reported. The difficulty levels correspond to the overall easiest and the easiest-for-humans settings. We also give the upper bound of optimal steps needed for each configuration.

Figure 3: Subfigure (a) shows the success rate aggregated over all puzzles in the easiest setting, while Subfigure (b) shows aggregated episode length. Interval estimates are based on stratified bootstrap confidence intervals, computed using rliable . Some puzzles, namely Loopy, Pearl, Pegs, Solo, and Unruly, were intractable for all algorithms and were therefore excluded in this aggregation. Optimal refers to the upper bound of the performance of an optimal policy. A human expert is able to solve all puzzles within this bound. We report median, interquartile mean on the middle 50% of runs (IQM), mean, as well as the optimality gap with respect to the upper bound of an optimal policy. We see that DreamerV3, DQN and TRPO are able to solve the largest amount of puzzles, however, DreamerV3 seems to learn better policies. Overall, all algorithms fall short of optimal or human expert level performance.

consistently fewer steps, but still more than the optimal policy. Our results indicate that puzzles with relatively high reward density at human difficulty levels remain challenging. We propose to use the easiest human difficulty level as a first measure to evaluate future algorithms. The details of the easiest human difficulty setting can be found in Appendix Table 7. If this level is achieved, difficulty can be further scaled up by increasing the size of the puzzles. Some puzzles also allow for an increase in difficulty with fixed size.

### Effect of Action Masking and Observation Representation

We evaluate the effect of action masking, as well as observation type, on training performance. Firstly, we analyze whether action masking, as described in paragraph "Action Masking" in Appendix B.4, can positively affect training performance. Secondly, we want to see if agents are still capable of solving puzzles while relying on pixel observations. Pixel observations allow for the exact same input representation to be used for all puzzles, thus achieving a setting that is very similar to the Atari benchmark. We compare MaskablePPO to the default PPO without action masking on both types of observations. We summarize the results in Figure 4. Detailed results for masked RL agents on the pixel observations are provided in Appendix Table 12.

As we can observe in Figure 4, action masking has a strongly positive effect on training performance. This benefit is observed both in the discrete internal game state observations and on the pixel observations. We hypothesize that this is due to the more efficient exploration, as actions without effect are not allowed. As a result, the reward density during training is increased, and agents are able to learn a better policy. Particularly noteworthy are the outcomes related to _Pegs_. They show that an agent with action masking can effectively learn a successful policy, while a random policy without action masking consistently fails to solve any instance. As expected, training RL agents on pixel observations increases the difficulty of the task at hand. The agent must first understand how the pixel observation relates to the internal state of the game before it is able to solve the puzzle. Nevertheless, in combination with action masking, the agents manage to solve a large percentage of all puzzle instances, with 10 of the puzzles consistently solved within the optimal upper bound.

Furthermore, Figure 4 shows the individual training performance on the puzzle _Flood_. It can be seen that RL agents using action masking and the discrete internal game state observation converge significantly faster and to better policies compared to the baselines. The agents using pixel observations and no action masking struggle to converge to any reasonable policy.

Figure 4: (left) We demonstrate the effect of action masking in both RGB observation and internal game state. By masking moves that do not change the current state, the agent requires fewer actions to explore, and therefore, on average solves a puzzle using fewer steps. (right) Moving average episode length during training for the _Flood_ puzzle. Lower episode length is better, as the episode gets terminated as soon as the agent has solved a puzzle. Different colors describe different algorithms, where different shades of a color indicate different random seeds. Sparse dots indicate that an agent only occasionally managed to find a policy that solves a puzzle. It can be seen that both the use of discrete internal state observations and action masking have a positive effect on the training, leading to faster convergence and a stronger overall performance.

### Effect of Episode Length and Early Termination

We evaluate whether the cutoff episode length or early termination have an effect on training performance of the agents. For computational reasons, we perform these experiments on a selected subset of the puzzles on the easiest preset human level difficulty and only for DreamerV3 (see Appendix F.4 for details). As we can see in Table 2, increasing the maximum episode length during training from 10,000 to 100,000 does not improve performance. Only when episodes get terminated after visiting the exact same state more than 10 times, the agent is able to solve more puzzle instances on average (31.5% vs. 25.2%). Given the sparse reward structure, terminating episodes early seems to provide a better trade-off between allowing long trajectories to successfully complete and avoiding wasting resources on unsuccessful trajectories. Interestingly, the solution for the puzzle Cube found by DreamerV3 requires fewer steps than the human expert.

### Generalization

PUZZLES is explicitly designed to facilitate the testing of generalization capabilities of agents with respect to different puzzle sizes or puzzle difficulties. For our experiments, we select puzzles with the highest reward density. We utilize a custom observation wrapper and transformer-based encoder in order for the agent to be able to work with different input sizes, see Appendices A.3 and A.4 for details. We call this approach PPO (Transformer)

The results presented in Table 3 indicate that while it is possible to learn a policy that generalizes it remains a challenging problem. Furthermore, it can be observed that selecting the best model during training according to the performance on the generalization environment yields a performance benefit in that setting. This suggests that agents may learn a policy that generalizes better during the training process, but then overfit on the environment they are training on. It is also evident that generalization performance varies substantially across different random seeds. For Netslide, the best agent is capable of solving 23.3% of the puzzles in the generalization environment whereas the worst experiment.

  
**\#Steps** & **ET** & **DreamerV3** \\   & 10 & \(2950.9 1260.2\) (31.6\%) \\  & - & \(2975.4 1503.5\) (25.2\%) \\   & 10 & \(3193.9 1044.2\) (26.1\%) \\  & - & \(2892.4 908.3\) (26.8\%) \\   

Table 2: Comparison of the effect of the maximum episode length (# Steps) and early termination (ET) on final performance. For each setting, we report average success episode length with standard deviation with respect to the random seed, all averaged over all selected puzzles. In brackets, the percentage of successful episodes is reported.

  
**Puzzle** & **Parameters** & **Trained on** & **PPO (Transformer)** & **PPO (Transformer)\({}^{}\)** \\   & 2x3b1 & âœ“ & \(244.1 313.7\) (100.0\%) & \(242.0 379.3\) (100.0\%) \\  & 3x3b1 & âœ— & \(9014.6 2410.6\) (18.6\%) & \(9002.8 2454.9\) (18.0\%) \\   & 2x3c3s2 & âœ“ & \(9.3 10.9\) (99.8\%) & \(26.2 52.9\) (99.7\%) \\  & 5x5c3s2 & âœ— & \(379.0 261.6\) (9.4\%) & \(880.1 675.4\) (18.1\%) \\   & 4 & âœ“ & \(38.6 58.2\) (99.8\%) & \(69.8 66.4\) (100.0\%) \\  & 6 & âœ— & \(3340.0 3101.2\) (87.3\%) & \(2985.8 2774.7\) (93.7\%) \\   

Table 3: We test generalization capabilities of agents by evaluating them on puzzle sizes larger than their training environment. We report the average number of steps an agent needs to solve a puzzle, and the percentage of successful episodes in brackets. The difficulty levels correspond to the overall easiest and the easiest-for-humans settings. For PPO (Transformer), we selected the best checkpoint during training according to the performance in the training environment. For PPO (Transformer)\({}^{}\), we selected the best checkpoint during training according to the performance in the generalization environment.

agent is only able to solve 11.2% of the puzzles, similar to a random policy. Our findings suggest that agents are generally capable of generalizing to more complex puzzles. However, further research is necessary to identify the appropriate inductive biases that allow for consistent generalization without a significant decline in performance.

## 4 Discussion

The experimental evaluation demonstrates varying degrees of success among different algorithms. For instance, puzzles such as _Tracks_, _Map_ or _Flip_ were not solvable by any of the evaluated RL agents, or only with performance similar to a random policy. This points towards the potential of intermediate rewards, better game rule-specific action masking, or model-based approaches. To encourage exploration in the state space, a mechanism that explicitly promotes it may be beneficial. On the other hand, the fact that some algorithms managed to solve a substantial amount of puzzles with presumably optimal performance demonstrates the advances in the field of RL. In light of the promising results of DreamerV3, the improvement of agents that have certain reasoning capabilities and an implicit world model by design stay an important direction for future research.

Experimental Results.The experimental results presented in Section 3.1 and Section 3.3 underscore the positive impact of action masking and the correct observation type on performance. While a pixel representation would lead to a uniform observation for all puzzles, it currently increases complexity too much compared the discrete internal game state. Our findings indicate that incorporating action masking significantly improves the training efficiency of reinforcement learning algorithms. This enhancement was observed in both discrete internal game state observations and pixel observations. The mechanism for this improvement can be attributed to enhanced exploration, resulting in agents being able to learn more robust and effective policies. This was especially evident in puzzles where unmasked agents had considerable difficulty, thus showcasing the tangible advantages of implementing action masking for these puzzles.

Limitations.While the PUZZLES framework provides the ability to gain comprehensive insights into the performance of various RL algorithms on logic puzzles, it is crucial to recognize certain limitations when interpreting results. The sparse rewards used in this baseline evaluation add to the complexity of the task. Moreover, all algorithms were evaluated with their default hyperparameters. Additionally, the constraint of discrete action spaces excludes the application of certain RL algorithms.

Benchmarking LLMs.We also explore the potential of using PUZZLES as a novel framework for evaluating the reasoning abilities of both large language models (LLMs) and vision language models (VLMs). While existing reasoning benchmarks have been the subject of debate regarding their ability to accurately assess the reasoning abilities of LLMs [58; 59; 60], PUZZLES offers a unique advantage by enabling true out-of-distribution evaluation. Our preliminary experiments, conducted with Gemini 1.5 Flash  and GPT-4o mini , indicate that current LLMs have limited success in solving PUZZLES. A detailed analysis of these results is presented in Appendix F.5. This research lays the groundwork for future investigations using PUZZLES to provide a more nuanced understanding of the reasoning processes and limitations of LLMs.

In the context of RL, the different challenges posed by the logic-requiring nature of these puzzles necessitates a good reward system, strong guidance of agents, and an agent design more focused on logical reasoning capabilities. It will be interesting to see how alternative architectures such as graph neural networks (GNNs) perform. GNNs are designed to align more closely with the algorithmic solution of many puzzles. While the notion that "reward is enough" [63; 64] might hold true, our results indicate that not just _any_ form of correct reward will suffice, and that advanced architectures might be necessary to learn an optimal solution.

## 5 Conclusion

In this work, we have proposed PUZZLES, a benchmark that bridges the gap between algorithmic reasoning and RL. In addition to containing a rich diversity of logic puzzles, PUZZLES also offers an adjustable difficulty progression for each puzzle, making it a useful tool for benchmarking, evaluating and improving RL algorithms. Our empirical evaluation shows that while RL algorithms exhibit varying degrees of success, challenges persist, particularly in puzzles with higher complexity or those requiring nuanced logical reasoning. We are excited to share PUZZLES with the broader research community and hope that this benchmark will foster further research to improve the algorithmic reasoning abilities of machine learning models.

## Broader Impact

This paper aims to contribute to the advancement of the field of machine learning (ML). Given the current challenges in ML related to algorithmic reasoning, we believe that our newly proposed benchmark will facilitate significant progress in this area, potentially elevating the capabilities of ML systems. Progress in algorithmic reasoning can contribute to the development of more transparent, explainable, and fair ML systems. This can further help address issues related to bias and discrimination in automated decision-making processes, promoting fairness and accountability.