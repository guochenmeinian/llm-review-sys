# Invariant Learning via Probability of Sufficient and Necessary Causes

Mengyue Yang\({}^{1}\), Zhen Fang\({}^{2}\), Yonggang Zhang\({}^{3}\),

**Yali Du\({}^{4}\), Furui Liu\({}^{5}\), Jean-Francois Ton\({}^{6}\), Jianhong Wang\({}^{7}\), Jun Wang\({}^{1}\)\({}^{1}\)**

\({}^{1}\)University College London, \({}^{2}\)University of Technology Sydney

\({}^{3}\)Hong Kong Baptist University, \({}^{4}\)King's College London

\({}^{5}\)Zhejiang Lab, \({}^{6}\)ByteDance Research, \({}^{7}\)University of Manchester

mengyue.yang.20@ucl.ac.uk, zhen.fang@uts.edu.au,

csygzhang@comp.hkbu.edu.hk, yali.du@kcl.ac.uk

liufurui@zhejianglab.com, jeanfrancois@bytedance.com

jianhong.wang@manchester.ac.uk,jun.wang@cs.ucl.ac.uk

Corresponding Author: _csygzhang@comp.hkbu.edu.hk; jun.wang@cs.ucl.ac.uk_

###### Abstract

Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of _sufficiency_ and _necessity_ conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose PNS risk and formulate an algorithm to learn representation with a high PNS value. We theoretically analyze and prove the generalizability of the PNS risk. Experiments on both synthetic and real-world benchmarks demonstrate the effectiveness of the proposed method. The detailed implementation can be found at the GitHub repository: https://github.com/ymy4323460/CaSN.

## 1 Introduction

The traditional supervised learning methods heavily depend on the in-distribution (ID) assumption, where the training data and test data are sampled from the same data distribution (Shen et al., 2021; Peters et al., 2016). However, the ID assumption may not be satisfied in some practical scenarios like distribution shift (Zhang et al., 2013; Sagawa et al., 2019), which leads to the failure of these traditional supervised learning methods. To relax the ID assumption, researchers have recently started to study a different learning setting called _out-of-distribution_ (OOD) _generalization_. OOD generalization aims to train a model using the ID data such that the model generalizes well in the unseen test data that share the same semantics with ID data (Li et al., 2018; Ahuja et al., 2021).

Recent works have proposed to solve the OOD generalization problem through the lens of causality (Peters et al., 2016; Pfister et al., 2019; Rothenhausler et al., 2018; Heinze-Deml et al., 2018; GamellaNoticeably, the aforementioned invariant learning methods mainly focus on learning the invariant causal representation, which may contain non-essential information that is not necessary nor sufficient information (Pearl, 2009). In image classification tasks, necessity describes the label is not true if the features disappear, and sufficiency describes the presence of a feature helps us determine the correctness of the label. If the feature extractor only learns a representation that is invariant but fails to satisfy the sufficiency or necessity, the model's generalisation ability may deteriorate. As an illustrative example (see Figure 1(a)), suppose that the training data only contains images of cats with feet and that we are interested in learning a model for a cat prediction task. If the model captures the invariant information (feature) "cat feet", then the learned model is likely to make a mistake in the OOD data containing cats without "cat feet" features. The example "cat feet" demonstrates the representation contains sufficient but unnecessary causal information because using "cat feet" can predict the label "cat" but a cat image might not contain "cat feet". Analogously, there are also representations that are necessary but not sufficient (the feature "pointy" ear" in Figure 1(a)). In Section 2.2, we present more examples to enhance the understanding of sufficiency and necessity.

This paper proposes achieving OOD generalization using _essential causal information_, which builds upon the probability of _necessary_ and _sufficiency_ (Parsl, 2009). In this paper, we introduce the PNS risk. A low PNS risk implies that the representation contains both the necessary and sufficient causal information from the observation data with a high level of confidence. We provide some theoretical analysis that establishes the approximation of the risk on unseen test domains by the risk on source data. Based on these theoretical results, we discuss PNS risk in the context of a semantic separable representation space and propose an algorithm for learning the representation which contains the information of both sufficient and necessary causes from training data (ID data) under different causal assumptions in Figure 1(b). The main contributions of this paper are as follows:

Firstly, we propose a new learning risk--PNS risk--to estimate the sufficiency and necessity of information contained in the learned representation. Secondly, we theoretically analyze the PNS risk under OOD problem and bound the gap between PNS risk on the test domain distribution and the risk on source data. Lastly, we propose an algorithm that captures sufficient and necessary causal representation with low PNS risk on test domains. Experiments on synthetic and real-world benchmarks are conducted to show the effectiveness of the algorithm over state-of-the-art methods.

## 2 Preliminaries

### Learning Setups

**Domains.** Let \(^{D}\) be the observable feature variable and \(Y\) be the label. In this paper, we mainly focus on binary classification task2, i.e., the label space \(=\{0,1\}\). \(\) is a joint

Figure 1: (a) Examples for causal sufficiency and necessity in the cat classification. (b) The causal graph for OOD generalization problem. The arrows denote the causal generative direction and the dashed line connects the spurious correlated variables. Notations are formally defined in Section 2.1. & Heinze-Deml (2020); Oberst et al. (2021); Chen et al. (2022). These works focus on learning invariant representation, aiming to capture the cause of the labels. By learning this representation, one can bridge the gap between the ID training data and unknown OOD test data, and thus mitigate the negative impacts on the distribution shift between ID and OOD distributions. Among these works, invariant risk minimization (IRM) (Arjovsky et al., 2019) is the most representative method, targeting to identify invariant representation and classifier using a bi-level optimization algorithm. Following works, many efforts have been devoted to further extending the original invariant learning framework (Chen et al., 2022; Ahuja et al., 2020; Lu et al., 2021; Liu et al., 2021; Liu et al., 2021; Lin et al., 2022).

distribution \(P_{s}(,Y)\) defined over \(\) in source domain. Equivalently, the unseen test domain is \(:=P_{t}(,Y)\). We also set \(_{}:=P_{t}()\) to be the marginal distribution over variable \(\) on test domain. Similarly, \(_{}:=P_{s}()\) is the marginal distributions on source domain over \(\).

**Assumption and model.** We conclude the causal graph in OOD generalization in Figure 1(b), inspired by the content (invariant) features and style (variant) features partition (Zhang et al., 2022). There are the invariant feature \(^{d}\) and domain specific variable (i.e. domain indicator) \(V\{1,,n\}\) A common _assumption_ of OOD generalization is that there exists latent causal variable \(^{d}\) that maintains the invariance property across domains (see Figure 1(b)), i.e., \(P_{s}(Y|=)=P_{t}(Y|=)\)(Arjovsky et al., 2019). Built upon this assumption, we define an invariant predictor by using a simple linear classifier \(:^{d}\) on the causal features to get label \(y=(^{})\). Since the causal variable \(\) cannot be directly observed, we infer \(\) from the observational data \(\). Then, the invariant predictor with the invariant representation inference model is defined as below.

\[y=[_{ P_{t}(|= )}^{}].\] (1)

### Probability of Sufficient and Necessary Cause

Existing invariant learning strategies (Rojas-Carulla et al., 2018; Pfister et al., 2019; Arjovsky et al., 2019) only consider the invariant property. However, the invariant representation can be further divided into three parts, each containing the different sufficient and necessary causal information.

**(i) Sufficient but unnecessary causes \(A\):** The cause \(A\) leads to the effect \(B\), but when observing the effect \(B\), it is hard to confirm \(A\) is the actual cause (See example in Figure 1(a)). **(ii) Necessary but insufficient causes \(A\):** Knowing effect \(B\) we confirm the cause is \(A\), but cause \(A\) might not lead to effect \(B\). "pointy ear" in cat prediction is selected as a typical example. Because when the ear shape is not pointy, we can confirm it is not a cat. However, a fox has a similar ear shape to a cat. Thus "pointy ear" is not a stable feature to predict cats. **(iii) Necessary and sufficient causes \(A\):** Knowing the effect \(B\) confirms the cause \(A\), while observing \(A\) leads to \(B\). In the cat and fox classification task, "short mouth" could be a necessary and sufficient cause. It is because the feature "short mouth" allows us to distinguish a cat from a fox, and when know there is a cat, "short mouth" must exist.

In order to learn invariant representations \(\) contains both sufficient and necessary causal information, we refer to the concept of _Probability of Sufficient and Necessary_ (PNS) (Chapter 9 in (Pearl, 2009), which is formally defined as below.

**Definition 2.1** (Probability of Necessary and Sufficient (Pns) (Pearl, 2009)).: Let the specific implementations of causal variable \(\) as \(\) and \(}\), where \(}\). The probability that \(\) is the necessary and sufficiency cause of \(Y\) on test domain \(\) is

\[(,}):=& (Y_{do(=)}=y= },Y y)}_{}P_{t}(=},Y y)\\ +&(Y_{do(=})} y=,Y=y)}_{}P_{t}( =,Y=y).\] (2)

In the above definition, the notion \(P(Y_{do(=})} y|=,Y=y)\) means that we study the probability of \(Y y\) when we force the manipulable variable \(\) to be a fixed value \(do(=})\) (do-operator) given a certain factual observation \(Y=y\) and \(=\). The first and second terms in PNS correspond to the probabilities of sufficiency and necessity, respectively. Variable \(\) has a high probability to be the sufficient and necessary cause of \(Y\) when the PNS value is large. Computing the counterfactual probability is a challenging problem since collecting the counterfactual data is difficult, or even impossible in real-world systems. Fortunately, PNS defined on counterfactual distribution can be directly estimated by the data under proper conditions, i.e., Exogeneity and Monotonicity.

**Definition 2.2** (Exogeneity (Pearl, 2009)).: Variable \(\) is exogenous relative to variable \(Y\) w.r.t. source and test domains \(\) and \(\), if the intervention probability is identified by conditional probability \(P_{s}(Y_{do(=)}=y)=P_{s}(Y=y|=)\) and \(P_{t}(Y_{do(=)}=y)=P_{t}(Y=y|=)\).

**Definition 2.3** (Monotonicity (Pearl, 2009)).: \(Y\) is monotonic relative to \(X\) if and only if either \(P(Y_{do(=)}=y,Y_{do(=})} y )=0\) or \(P(Y_{do(=)} y,Y_{do(=})}=y)=0\)The definition of Exogeneity describes the gap between the intervention and conditional distributions vanishes when \(\) is exogenous relative to \(Y\) and the definition of Monotonicity demonstrates the monotonic effective on \(Y\) of causal variable \(\). Based on Definitions 2.2 and 2.3, the identifiability of PNS in Definition 2.1 is described as the following lemma.

**Lemma 2.4** (Pearl (2009)).: _If \(\) is exogenous relative to \(Y\), and \(Y\) is monotonic relative to \(\), then_

\[(,})=(Y=y|= )}_{}-(Y=y|= })}_{}.\] (3)

According to Lemma 2.4, the computation of PNS is feasible through the observation data under Exogeneity and Monotonicity. This allows us to quantify PNS when counterfactual data is unavailable. The proof of Lemma 2.4 is provided by Pearl (2009). Wang & Jordan (2021) further extend the proof by incorporating probabilistic computation, as opposed to the logical calculation used in Pearl (2009).

## 3 PNS Risk Modeling

This section presents the PNS-based risk for invariant learning in OOD problem. The risk on test domains is a PNS-value evaluator, which is bounded by the tractable risk on the training domain.

### PNS Risk

In this section, we introduce the PNS risk, which is a PNS-value estimator. The risk estimates the PNS value of the representation distribution \(P_{t}(|=)\) inferred from \(\) on an unseen test domain \(\). The risk increases when the representation contains less necessary and sufficient information, which can be caused by data distribution shifts. The PNS risk is based on the definition of \((,})\). As \(}\) represents the intervention value, it is not necessary for it to be a sample from the same distribution as the causal variable \(\). Thus, we define an auxiliary variable \(}^{d}\) (same as the range of \(\)) and sample \(}\) from its distribution \(P_{t}(}|=)\). In the learning method, we use the notations \(P_{t}^{}(|=)\) and \(P_{t}^{}(}|=)\) to present the estimated distributions, which are parameterized by \(\) and \(\), separately. Let \((A)\) be an indicator function, where \((A)=1\) if \(A\) is true; otherwise, \((A)=0\). PNS risk based on Definition 2.1 and Lemma 2.4 is formally defined as Eq. (4) below.

\[ R_{t}(,,):=_{(,y)}&_{ P_{t}( |=)}[(^{} ) y]\\ +&_{ P_{t}(}| =)}[(^{}})=y].\] (4)

As the identifiability result in Lemma 2.4 is based on the Exogeneity 2.2 and Monotonicity 2.3, we modify the original risk equation, Eq. (4), to ensure compliance with these conditions. Below, we provide Monotonicity measurement and discuss the satisfaction of Exogeneity in Section 4.3.

**Satisfaction of monotonicity.** We naturally introduce the measurement of Monotonicity into PNS risk by deriving an upper bound of Eq. (4), which is given below.

**Proposition 3.1**.: _Given a test domain \(\), we define the sufficient and necessary risks as:_

\[ SF_{t}(,):=_{( ,y)}_{ P_{t}^{}(|=)}[(^{})  y]}_{},\\ _{t}(,):=_{(,y)}_{ P_{t}^{}(}| =)}[(^{}})=y]}_{},\]

_and let the Monotonicity measurement be_

\[M_{t}^{}(,):=_{(,y)} _{ P_{t}^{}(|=)} _{ P_{t}^{}(}|= )}[(^{})=( ^{}})],\]

_then we have_

\[R_{t}(,,)=M_{t}^{}(,)+2SF_{t}(, )_{t}(,) M_{t}^{}(,)+2SF_{t}( ,).\] (5)

The upper bound for PNS risk in Eq. (5) consists of two terms: (i) the evaluator of sufficiency \(SF_{t}(,)\) and (ii) the Monotonicity measurement \(M_{t}^{}(,)\). In the upper bound, the necessary term \(_{t}(,)\) is considered to be absorbed into measurement of Monotonicity \(M_{t}^{}(,)\). The minimization process of Eq. (4) on its upper bound (5) considers the satisfaction of Monotonicity.

### OOD Generalization with PNS risk

In OOD generalization tasks, only source data collected from \(\) is provided, while the test domain \(\) is unavailable during the optimization process. As a result, it is not possible to directly evaluate the risk on the test domain, i.e. \(R_{t}(,,)\). To estimate \(R_{t}(,,)\), we have a two-step process: (i) Firstly, since the test-domain distribution \(\) is not available during the training process, We aim to establish a connection between the risk on the test domain \(R_{t}(,,)\) and the risk on the source domain \(R_{s}(,,)\) in Theorem 3.2. (ii) Furthermore, in practical scenarios where only a finite number of samples are available, we demonstrate the bound of the gap between the expected risk on the domain distribution and the empirical risk on the source domain data in Theorem 3.3.

**Connecting the PNS risks, i.e., \(R_{t}(,,)\) and \(R_{s}(,,)\).** We introduce divergence measurement \(\) divergence (Ganin et al., 2016) and weigh the \(R_{s}(,,)\) term by variational approximation. \(\) divergence measures the distance between domain \(\) and \(\), which is formally defined below.

\[_{k}(\|)=[*{}_{( ,y)}((,y)}{(,y)})^{k}]^{}.\] (6)

Based on \(_{k}(\|)\), we connect the risks on the source and test domains by Theorem 3.2.

**Theorem 3.2**.: _The risk on the test domain is bounded by the risk on the source domain, i.e.,_

\[R_{t}(,,)_{k+}_{k}(\| )([M_{s}^{}(,)]^{1-}+2[SF_{s}(,)]^{1-})+_{t s}(,Y),\]

_where_

\[_{t s}(,Y):=P_{t}( Y {supp}()) R_{t s}(,,).\]

_Here \(()\) is the support set of source domain distribution \(P_{s}()\),_

\[R_{t s}(,,):=*{}_{( ,y) P_{t}( Y())}*{}_{ P_{t}(| =)}[(^{}) y]\] \[+*{}_{ P_{t}(| =)}[(^{} })=y].\]

In Theorem 3.2, \(_{t s}(,Y)\) describes the expectation of worst risk for unknown area i.e. the data sample \((,y)\) does not include in the source domain support set \(()\). Theorem 3.2 connects the source-domain risk and the test-domain risk. In the ideal case, where \(\) is the invariant representation, i.e. \(P_{s}(Y|=)=P_{t}(Y|=)\), the bound is reformed as below.

\[R_{t}(,,)_{k+}_{k}(_{ }\|_{})([M_{s}^{}(,T)]^{1-}+2[SF_{s}(,)]^{1-})+_{t s}(,Y).\] (7)

When the observations \(\) in \(\) and \(\) share the same support set, the term \(_{t s}(,Y)\) approaches to 0. In domain generalization tasks, the term \(_{k}(_{}|_{})\) is treated as a hyperparameter, as the test domain \(_{}\) is not available during training. However, in domain adaptation tasks where \(_{}\) is provided, \(_{k}(_{}|_{})\) and the test-domain Monotonicity measurement \(M_{t}^{}(,)\) can be directly estimated. Further details of the discussion on domain adaptation are provided in Appendix A.3.

**Connecting empirical risk to the expected risk.** In most real-world scenarios where distribution \(\) is not directly provided, we consider the relationship of expected risk on source domain distribution and empirical risk on source domain data \(^{n}:=\{(_{i},y_{i})\}_{i=1}^{n}\). We also define the empirical risks w.r.t. \(_{s}(,)\), \(_{s}^{}(,)\) as follows:

\[_{s}(,):=*{}_{^{n}} *{}_{_{s}^{}(| =)}*{}_{_{s}^ {}(|=)}*{}[ (^{})=(^{ }})],\]

where \(_{s}^{}(|=)\) and \(_{s}^{}(}|=)\) describe the estimated distribution on dataset \(^{n}\).

Then, we use PAC-learning (Shalev-Shwartz & Ben-David, 2014) tools to formulate the upper bound of gap between empirical risk and expected risk as a theorem below.

**Theorem 3.3**.: _Given parameters \(\), \(\), for any \(:^{d}\), prior distribution \(_{}:=P_{s}()\) and \(_{}:=P_{s}(})\) which make \(*{}_{S}(P_{s}^{}(|= )\|_{})\) and \(*{}_{S}(P_{s}^{}(}| =)\|_{})\) both lower than a positive constant \(C\), then with a probability at least \(1-\) over source domain data \(_{n}\).__(1) \(|SF_{s}(,)-_{s}(,)|\) is upper bounded by_

\[_{S^{n}}(_{s}^{}(|=)\|_{})++C.\]

_(2) \(|M_{}^{}(,)-_{}^{}( ,)|\) is upper bounded by_

\[_{S^{n}}(_{s}^{}(|= )\|_{})+_{S^{n}}(_{s}^{ }(}|=)\|_{})++2C.\]

Theorem 3.3 demonstrates that as the sample size increases and the terms with KL divergence decrease, the empirical risk on the source domain dataset becomes closer to the expected risk. Combining Theorems 3.2 and 3.3, we can evaluate the expected PNS risk on the test distribution using the empirical risk on the source dataset. In the next section, we present a representation learning objective based on the results of Theorems 3.2 and 3.3 and introduce the satisfaction of Exogeneity.

## 4 Learning to Minimizing PNS Risk

In this section, we propose a learning objective built upon the PNS risk that is used to capture the essential representation having a high PNS value from observational data.

### The Semantic Separability of PNS

In Section 3, we present PNS risk and Monotonicity measurement. Furthermore, to ensure that finding interpretable representations is feasible, we need to make certain assumptions that the representation of the data retains its semantic meaning under minor perturbations. Specifically, we define the variable \(\) as Semantic Separability relative to \(Y\) if and only if the following assumption is satisfied:

**Assumption 4.1** (\(\)-Semantic Separability).: For any domain index \(d\{s,t\}\), the variable \(\) is \(\)-semantic separable, if for any \( P_{d}(|Y=y)\) and \(} P_{d}(|Y y)\), the following inequality holds almost surely: \(\|}-\|_{2}>\).

\(\)-Semantic Separability refers to the semantic meaning being distinguishable between \(\) and \(}\) when the distance between them is large enough, i.e., \(\|}-\|_{2}>\). This assumption is widely accepted because, without it, nearly identical values would correspond to entirely different semantic information, leading to inherently unstable and chaotic data. If \(\) satisfies Assumption 4.1, then considering the PNS value in a small intervention, such as \(\|-}\|_{2}<\), will lead failure in representation learning. Therefore, during the learning process, we add the penalty of \(\|-}\|_{2}>\).

### Overall Objective

Depending on the diverse selections of \(P^{}(}|=)\), there are multiple potential PNS risks. In the learning process, we consider minimizing the risk in the worst-case scenario lead by \(}\), i.e., the maximal PNS risk lead by the selection of \(P^{}(}|=)\). Minimizing the upper bounds in Theorems 3.2 and 3.3 can be simulated by the following optimization process

\[_{,}_{}\ \ _{s}^{}(,)+ _{s}(,)+ L_{},\ \ \ \ \ \ \|-}\|_{2}>,\] (8)

where \(L_{}:=_{S^{n}}(_{s}^{}(| =)\|_{}))+_{S^{n}}( _{s}^{}(}|=)\|_{}}))\). The constraint \(\|-}\|_{2}>\) is set because of the Semantic Separability assumption. We name the algorithm of optimizing Eq. (8) as CaSN (Causal Representation of ufficiency and \(}}\)cessity).

### Satisfaction of Exogeneity

In the previous sections, we introduced an objective to satisfy monotonicity. Identifying PNS values not only needs to satisfy monotonicity but also exogeneity. In this part, we discuss the satisfaction of Exogeneity and provide the solution to find the representation under three causal assumptions below.

**Assumption 4.2**.: The Exogeneity of \(\) holds, if and only if the following invariant conditions are satisfied separately under three causal assumptions in Figure 1(b): (1) \( Y|\) (2) \( V\) (3) \(V Y|\) (for assumption in Figure 1(b).1, 2 and 3, respectively).

The above three assumptions are commonly accepted by the OOD generalization (Lu et al., 2021; Liu et al., 2021; Ahuja et al., 2021). To satisfy the Exogeneity, we use different objective functions to identify \(\) over three invariant causal assumptions. For Assumption 4.2 (1), we provide the following theorem showing the equivalence between optimizing Eq.8 and identifying invariant representation.

**Theorem 4.3**.: _The optimal solution of learned \(\) is obtained by optimizing the following objective (the key part of the objective in Eq. (8))_

\[_{,}_{s}(,)+_{ ^{n}}(_{s}^{}(|=)\|_{})\]

_satisfies the conditional independence \( Y|\)._

Theorem 4.3, details of the proof are shown in Appendix E, indicates optimizing overall objective Eq. (8) implicitly makes \(\) satisfy the property of Exogeneity under causal assumption \( Y|\). For Assumption 4.2 (2), to identify the invariant assumption \( V\)(Li et al., 2018), we introduce the following Maximum Mean Discrepancy (MMD) penalty to the minimization process in Eq. (8),

\[L_{}=_{v_{i}}_{v_{j}}_{_{ } P(|V=v_{i})}_{_{} P _{s}^{}(|=_{})}_{ _{} P(|V=v_{j})}_{_{ } P_{s}^{}(|=_{})} \|_{}-_{}\|_{2}.\]

For Assumption 4.2 (3), to specify the representation of \(\) and allows the Exogeneity when the assumption \(V Y|\) holds, we introduce the IRM-based (Arjovsky et al., 2019) penalty into Eq.(8).

\[L_{}=_{v}E_{(,y) P_{s}(,Y|V=v)}\| _{w|w=1.0}_{ P_{s}^{}(|=)}[(^{}) y] \|^{2}\]

Noticebly, to address the issue of invariant learning and satisfy the Exogeneity under Assumptions 4.2 (2) and (3), it is necessary to introduce additional domain information, such as domain index.

## 5 Related Work

In this section, we review the progress of OOD prediction tasks. A research perspective for OOD prediction is from a causality viewpoint (Zhou et al., 2021; Shen et al., 2021). Based on the postulate that the causal variables are invariant and less vulnerable to distribution shifts, a bunch of methods identify the invariant causal features behind the observation data by enforcing invariance in the learning process. Different works consider causality across multiple domains in different ways. One series of research called the causal inference-based methods model the invariance across domains by causality explanation, which builds a causal graph of data generative process (Pfister et al., 2019; Rothenhausler et al., 2018; Heinze-Deml et al., 2018; Gamella and Heinze-Deml, 2020; Oberst et al., 2021; Zhang et al., 2015). The other series of methods consider invariant learning from a causality aspect. They formulate the invariant causal mechanisms by representation rather than causal variables. Invariant risk minimization (IRM) methods (Arjovsky et al., 2019) provide a solution for learning invariant variables and functions. Under this viewpoint, some pioneering work (Ahuja et al., 2020; Chen et al., 2020; Krueger et al., 2021; Lu et al., 2021; Ahuja et al., 2021; Lin et al., 2022) further extend the IRM framework by considering game theory, variance penalization, information theory, nonlinear prediction functions, and some recent works apply the IRM framework to large neural networks (Jin et al., 2020; Gulrajani and Lopez-Paz, 2020). In this paper, different from the aforementioned works which consider to learn the invariant information, we think that information satisfying invariance is not enough to be most appropriate for the generalization task. We thus focus on learning to extract the most essential information from observations with a ground on the sufficient and necessary causal theorem. In the main text, we only provide a review of OOD prediction. We further elaborate on the correlation with other lines of work, such as domain adaptation, causal discovery, representation learning, causal disentanglement, and contrastive learning, in Appendix F.

## 6 Experiments

In this section, we verify the effectiveness of CaSN using synthetic and real-world OOD datasets.

### Setups

**Synthetic data.** The effectiveness of the proposed method is demonstrated by examining whether it can learn the essential information (i.e., sufficient and necessary causes) from source data. To this end, based on the causal graph in Figure 1(b).1 we designed a synthetic data generator that produced a sample set \(\{_{i}\}_{i=1}^{n}\) with corresponding labels \(\{y_{i}\}_{i=1}^{n}\). Four types of information were considered, including: (i) SN: Sufficient and Necessary Cause \(_{i}\) of \(y_{i}\). The value of \(y_{i}\) is directly calculated as \(y_{i}=_{i} B(0.15)\), where \(\) represents the XOR operation and \(B(0.15)\) is a Bernoulli distribution with a probability of \(0.15\) to generate \(1\). (ii) SF: sufficient and unnecessary cause \(_{i}\) of \(y_{i}\). \(_{i}\) is a transformation of \(_{i}\). We set \(_{i}=B(0.1)\) when \(_{i}=0\), and \(_{i}=_{i}\) when \(_{i}=1\). SF is designed to decrease the probability of necessity (i.e. \(P(Y=0|=0)\)). (iii) NC: insufficient and necessary cause \(_{i}\) of \(y_{i}\). We set \(_{i}=1(_{i}=1) B(0.9)\). NC is designed to decrease the probability of sufficiency (i.e. \(P(Y=1|=1)\)). (iv) Spurious: spurious correlation information \(_{i}\). Spurious correlated information is generated by \(s*_{i}*_{d}+(1-s)(0,1)\), where \(d\) denotes dimension and \(s\) denotes the **spurious degree**. When \(s\) gets higher, the spurious correlation becomes stronger in data \(\). We select \(d=5\) and \(s\{0.1,0.7\}\). in the synthetic generative process and develop a non-linear function to generate \(\) from \([_{i},_{i},_{i},_{i}]\). We use Distance Correlation (Jones et al., 1995) as evaluation metrics to measure the correlation between the learned representation \(\) and ground information (i.e. SN, SF, NC, SP). We provide an ablation study of CaSN without the Monotonicity evaluator CaSN(-m) in comparison results, which evaluates the effectiveness of CaSN.

**Performance on OOD prediction task.** The proposed method CaSN is implemented based on codebase DomainBed (Gulrajani and Lopez-Paz, 2020). We provide three implementations of our method, which are CaSN, CaSN(irm) and CaSN(mmd) with the same architecture but using Eq.(8), Eq.(8) \(+L_{}\) and Eq.(8) \(+L_{}\) as their final objective, respectively. We compare CaSN with several common baselines, including **ERM**(Vapnik, 1999), **IRM**(Arjovsky et al., 2019), **GroupDRO**(Sagawa et al., 2019), **Mixup**(Xu et al., 2020), **MLDG**(Li et al., 2018), **MMD**(Li et al., 2018), **DANN**(Ganin et al., 2016) and **CDANN**(Li et al., 2018), where the best accuracy scores are directly given by training-domain-validation in Gulrajani and Lopez-Paz (2020). We test the performance on commonly used ColoredMnist (Ahuja et al., 2020), PACS (Li et al., 2017), and VLCS (Fang et al., 2013) datasets. During the experiment process, we adjust the hyperparameters provided by DomainBed and extra hyperparameters \(\) and \(\) in CaSN. The results show the mean and standard error of accuracy by executing the experiments randomly 2 times on 40 randomly selected hyperparameters. We also provide the extra experiments on large-scale spurious correlation dataset SpuCo (Joshi et al., 2023). Due to the page limitation, more experiment setups and results are provided in Appendix B.

### Learning Sufficient and Necessary Causal Representations

We conducted experiments on synthetic data to verify the effectiveness of the learned representation. In experiments, we use single domain with different degrees of spurious correlation. The experiments aimed to demonstrate the properties of the learned representation and answer the following question:

**Does CaSN capture the sufficient and necessary causes?** We present the results in Figure 2 (a) and (b), which show the distance correlation between the learned representation and four ground truths: Sufficient and Necessary cause (SN, SF, NC and Spurious). A higher distance correlation indicates a better representation. From both Figure 2 (a) (b), we found that CaSN achieves higher distance correlations with the ground truths (e.g., SN, SF, and NC) and lower correlations with spurious factors compared to other methods. As an example, we consider Figure 2 (a) with \(=1.1\). We obtain distance correlations of \(\{0.90,0.65,0.67,0.13\}\) for SN, SF, NC, and spurious factors, respectively. We found that when we set \(\) as a large value \(1.1\), CaSN captures more essential information SN.

Figure 2: The synthetic results for validating the property of learned representation under different spurious degrees in data, \(s=0.1\) for (a) and \(s=0.7\) for (b), the x-axis shows different causal information y-axis shows the choice of \(\). (c) The results of the feature identification when \(s=0.7\).

However, the result of CaSN decreases when \(=0.1\), which suggests that CaSN tends to capture the most essential information when \(\) is set to a larger value. This phenomenon aligns with Semantic Separability. We then compare Figure 2 (a) and (b). As an example, when \(=1.1\), CaSN achieves distance correlations of 0.9 and 0.91 for SN on \(s=0.1\) and \(s=0.7\), respectively. The distance correlation with spurious information is 0.13 and 0.37 for \(s=0.1\) and \(s=0.7\), respectively. The results show that when more spurious correlations are in data, CaSN tends to capture information from those spurious correlations, but the algorithm is still able to get sufficient and necessary causes.

**Ablation study.** In Figure 2(c), we provide the comparison results between CaSN and the CaSN(-m) that removes the Monotonicity measurement on synthetic data. The figure demonstrates the distance correlation recorded over 5 experiments. The green bars indicate the distance correlation between learned representation and ground truth by CaSN. CaSN can capture the desired information SN compared to others. As the blue bars show, the CaSN(-m) can better capture the causal information (e.g. SN, SF and NC) rather than spurious correlation. It can not stably identify SN, compared to SF. CaSN(-m) can be regarded as the method that only cares Exogeneity. The results support the theoretical results in Theorem 4.3, which show the effectiveness of introducing Monotonicity term.

### Generelization to Unseen Domain

The results of the OOD generalization experiments on PACS and VLCS datasets are presented in Tables 1. Due to page limitation, we provide the results on ColoredMNIST in Table 2. The baseline method results are from Kilbertus et al. (2018). The proposed CaSN method exhibits good OOD generalization capability on both PACS and VLCS datasets. In Table 1, CaSN achieves the best average performance over 4 domains by \(86.0\) on PACS. On the VLCS, CaSN(irm) achieves a good average performance of \(78.2\), which is close to the best state-of-the-art performance achieved by DANN. For worst-domain test accuracies, the proposed method CaSN outperforms all the baseline methods. An intuitive explanation for the good performance of CaSN is that it aims to identify and extract the most essential information from observation data, excluding unnecessary or insufficient information from the optimal solution. This enables CaSN to better generalize on the worst domain.

## 7 Conclusion

In this paper, we consider the problem of learning causal representation from observation data for generalization on OOD prediction tasks. We propose a risk based on the probability of sufficient and necessary causes (Pearl, 2009), which is applicable OOD generalization tasks. The learning principle leads to practical learning algorithms for causal representation learning. Theoretical results on the computability of PNS from the source data and the generalization ability of the learned representation are presented. Experimental results demonstrate its effectiveness on OOD generalization.

## 8 Acknowledgement

We are thankful to Juzheng Miao, Xidong Feng, Kun Lin and Jingsen Zhang for their constructive suggestions and efforts on OOD generalization experiments and for offering computation resources. We also thank Congmin Ji for checking the correctness of the mathematical proof, Pengfei Zheng for his helpful discussions and the anonymous reviewers for their constructive comments on an earlier version of this paper. Furui Liu is supported by the National Key R&D Program of China (2022YFB4501500, 2022YFB4501504). Jianhong Wang is fully supported by UKRI Turing AI World-Leading Researcher Fellowship, \(EP/W002973/1\).

    &  &  \\ 
**Algorithm** & **A** & **C** & **P** & **S** & **Avg** & **Min** & **C** & **L** & **S** & **V** & **Avg** & **Min** \\  ERM & 84.7 \(\) 0.4 & 80.8 \(\) 0.6 & 97.2 \(\) 0.3 & 79.3 \(\) 1.0 & 85.5 & 79.3 & 97.7 \(\) 0.4 & 64.3 \(\) 0.9 & 73.4 \(\) 0.5 & 74.6 \(\) 1.3 & 77.5 \(\) 0.3 & 77.5 \(\) 0.3 & 76.4 \\ IRM & 84.8 \(\) 1.3 & 76.4 \(\) 1.1 & 96.7 \(\) 0.6 & 76.1 \(\) 1.0 & 83.5 & 76.4 & 98.6 \(\) 0.1 & 64.9 \(\) 0.9 & **73.4 \(\) 0.6** & **73.7 \(\) 0.9** & 78.5 \(\) 0.9 & 78.5 \(\) 0.9 \\ GroupDRO & 83.5 \(\) 0.9 & 79.1 \(\) 0.6 & 96.7 \(\) 0.3 & 78.3 \(\) 2.0 & 84.4 & 79.1 & 97.3 \(\) 0.3 & 63.4 \(\) 0.6 & 69.5 \(\) 0.8 & 76.7 \(\) 0.7 & 63.4 \\ Mixup & 86.1 \(\) 0.5 & 78.9 \(\) 0.8 & **97.6 \(\) 0.1** & 85.3 \(\) 1.8 & 84.6 & 78.9 & 89.3 \(\) 0.6 & 64.8 \(\) 1.0 & 72.1 \(\) 0.5 & 74.3 \(\) 0.8 & 77.4 \(\) 0.8 & 77.4 \(\) 0.8 \\ MLDG & 86.4 \(\) 0.7 & 77.4 \(\) 0.9 & 73.5 \(\) 0.4 & 73.5 \(\) 2.3 & 83.6 & 77.4 & 97.4 \(\) 0.2 & 65.2 \(\) 0.7 & 70.1 \(\) 0.4 & 75.3 \(\) 1.0 & 77.2 \(\) 0.5 \\ MMD & 86.1 \(\) 1.4 & 79.4 \(\) 0.9 & 96.6 \(\) 0.2 & 76.5 \(\) 0.5 & 86.4 & 79.4 \(\) 0.1 & 64.0 \(\) 1.1 & 72.8 \(\) 0.2 & 75.3 \(\) 3.3 & 77.5 & 64.0 \\ DANN & 86.4 \(\) 0.7 & 77.4 \(\) 0.8 & 97.3 \(\) 0.4 & 75.3 \(\) 2.3 & 83.6 & 77.4 & **99.0** & 8.0 \(\) 1.6 & 73.1 \(\) 0.3 & 77.2 \(\) 0.6 & **78.6** & 65.1 \\ CDANN & 84.6 \(\) 1.8 & 75.5 \(\) 0.9 & 96.8 \(\) 0.3 & 73.5 \(\) 0.6 & 82.5 \(\) 0.7 & 85.9 \(\) 1.0 & 63.1 \(\) 1.2 & 70.7 \(\) 0.7 & 77.1 \(\) 1.8 & 77.5 \(\) 0.5 & 63.1 \\ 
**CaSN (base)** & **87.1 \(\) 0.6** & 80.2 \(\) 0.6 & 96.2 \(\) 0.8 & 80.4 \(\) 0.2 & **86.0** & 80.2 & 97.5 \(\) 0.6 & 64.8 \(\) 1.9 & 70.2 \(\) 0.5 & 76.4 \(\) 1.7 & 77.2 & 64.8 \\
**CaSN (irm)** & 82.1 \(\) 0.3 & 77.9 \(\) 1.8 & 93.3 \(\) 0.8 & **80.6 \(\) 1.0** & 83.5 & 77.9 & 97.8 \(\) 0.3 & 65.7 \(\) 0.8 & 72.3 \(\) 0.4 & 77.0 \(\) 1.4 & 78.2 & 65.7 \\
**CaSN (mm)** & 84.7 \(\) 0.1 & **81.4 \(\) 1.2** & 95.7 \(\) 0.2 & **80.2 \(\) 0.6** & 85.3 & **81.4** & **98.2 \(\) 0.7** & **65.9 \(\) 0.6** & 71.2 \(\) 0.3 & 76.9 \(\) 0.7 & 78.1 & **65.9** \\   

Table 1: Results on PACS and VLCS dataset