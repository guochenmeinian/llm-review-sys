# Trajectory Flow Matching with Applications to

Clinical Time Series Modeling

Xi Zhang\({}^{1,2}\)  &Yuan Pu\({}^{3}\)  &Yuki Kawamura\({}^{4}\)  &Andrew Loza\({}^{3}\)

**Yoshua Bengio\({}^{2,5,6}\)** &**Dennis L. Shung\({}^{3}\)  &**Alexander Tong\({}^{2,5}\)**

\({}^{1}\)McGill University, \({}^{2}\)Mila - Quebec AI Institute,

\({}^{3}\)Yale School of Medicine

\({}^{4}\)School of Clinical Medicine, University of Cambridge,

\({}^{5}\)Universite de Montreal, \({}^{6}\)CIFAR Fellow

Joint first authorship Joint senior authorship. Correspondence to alexander.tong@mila.quebec

Code available at: https://github.com/nZhangx/TrajectoryFlowMatching

###### Abstract

Modeling stochastic and irregularly sampled time series is a challenging problem found in a wide range of applications, especially in medicine. Neural stochastic differential equations (Neural SDEs) are an attractive modeling technique for this problem, which parameterize the drift and diffusion terms of an SDE with neural networks. However, current algorithms for training Neural SDEs require backpropagation through the SDE dynamics, greatly limiting their scalability and stability. To address this, we propose **Trajectory Flow Matching** (TFM), which trains a Neural SDE in a _simulation-free_ manner, bypassing backpropagation through the dynamics. TFM leverages the flow matching technique from generative modeling to model time series. In this work we first establish necessary conditions for TFM to learn time series data. Next, we present a reparameterization trick which improves training stability. Finally, we adapt TFM to the clinical time series setting, demonstrating improved performance on four clinical time series datasets both in terms of absolute performance and uncertainty prediction, a crucial parameter in this setting.

## 1 Introduction

Real world problems often involve systems that evolve continuously over time, yet these systems are usually noisy and irregularly sampled. In addition, real-world time series often relate to other covariates, leading to complex patterns such as intersecting trajectories. For instance, in the context of clinical trajectories in healthcare, patients' vital sign evolution can follow drastically different, crossing paths even if the initial measurements are similar, due to the influence of the covariates such as medication intervention and underlying health conditions. These covariates can be time-varying or static, and often sparse.

Differential equation-based dynamical models are proficient at learning continuous variables without imputations . Nevertheless, systems governed by ordinary differential equations (ODEs) or stochastic differential equations (SDEs) are unable to accommodate intersecting trajectories, and thus requires modifications such as augmentation or modelling higher-order derivatives . While ODEs model deterministic systems, SDEs contain a diffusion term and can better represent the inherent uncertainty and fluctuations present in many real world systems. However, fitting stochastic equations to real lifedata is challenging because they have thus far required time-consuming backpropagation through an SDE integration.

In the domain of generative models, diffusion models (Ho et al., 2020; Nichol and Dhariwal, 2021; Song et al., 2021) and more recently flow matching models (Lipman et al., 2023; Albergo et al., 2023; Li et al., 2020) have had enormous success by training dynamical models in a _simulation-free_ framework. The simulation-free framework facilitates the training of much larger models with significantly improved speed and stability. In this work we generalize simulation-free training for fitting stochastic differential equations to time-series data, to learn population trajectories while preserving individual characteristics with conditionals. We present this method as **Trajectory Flow Matching**. We demonstrate that our method outperforms current state of the art time series modelling architecture including RNN, ODE based and flow matching methods. We empirically demonstrate the utility of our method in clinical applications where hemodynamic trajectories are critical for ongoing dynamic monitoring and care. We applied our method to the following longitudinal electronic health record datasets: medical intensive care unit (MICU) data of patients with sepsis, ICU patients at risk for cardiac arrest, Emergency Department (ED) data of patients with acute gastrointestinal bleeding, and MICU data of patients with acute gastrointestinal bleeding.

Our main contributions are:

* We prove the conditions under which continuous time dynamics can be trained simulation-free using matching techniques.
* We extend the approach to irregularly sampled trajectories with a _time predictive loss_ and to estimate uncertainty using an _uncertainty prediction loss_.
* We empirically demonstrate that our approach reduces the error by 15-83% when applied to the real world clinical data modelling.

## 2 Preliminaries

### Notation

We consider the setting of a distribution of trajectories over \(^{d}\) denoted \(:=\{x^{1},x^{2},,x^{n}\}\) where each \(x^{i}\) is a vector of \(T\) datapoints i.e. \(x^{i}:=\{x^{i}_{1},x^{i}_{2},,x^{i}_{T}\}\) with associated times \(t^{i}:=\{t^{i}_{1},t^{i}_{2},,t^{i}_{T}\}\). Let \(x^{i}_{[t-h,t-1]}\) denote a vector of the last \(h\) observed time points. We denote a (Lipschitz smooth) time dependent vector field conditioned on arbitrary conditions \(c^{e}\)\(v(t,x_{t},x_{[t-h,t-1]},c):(,^{d}, ^{h d},^{c})^{d}\) with flow \(_{t}(v)\) which induces the time-dependent density \(p_{t}=_{t}(v)_{\#}(p_{0})\) for any density \(p_{0}:^{d}_{+}\) with \(_{^{d}}p_{0}=1\). We also consider the coupling \((x_{0},x_{1})\) which operates on the product space of marginal distributions \(p_{0}\), \(p_{1}\)

Figure 1: Trajectory Flow Matching trains both an estimator of the next timepoint (\(_{}(t,x)\)) and an estimation of the uncertainty (\(_{}(t,x_{t})\)). Using the conditional flow matching framework, these can be used to predict the instantaneous velocity \(v_{}(t,x_{t})\) and future observations. Both flows are conditioned on past data \(x_{[t-h,t-1]}\) and conditional variables \(c\).

### Neural Stochastic Differential Equations

A stochastic differential equation (SDE) can be expressed in terms of a smooth drift \(f:[0,T]^{d}^{d}\) and diffusion \(g:[0,T]^{d}^{d^{2}}\) in the Ito sense as:

\[dx_{t}=f\,dt+g\,dW_{t}\]

where \(W_{t}:[0,T]^{d}\) is the \(d\)-dimensional Wiener process. A density \(p_{0}(x_{0})\) evolved according to an SDE induces a collection of marginal distributions \(p_{t}(x_{t})\) viewed as a function \(p:[0,T]^{d}_{+}\). In a _Neural_ SDE [Li et al., 2020, Kidger et al., 2021a,b] the drift and diffusion terms are parameterized with neural networks \(f_{}(t,x_{t})\) and \(g_{}(t,x_{t})\).

\[dx_{t}=f_{}(t,x_{t})dt+g_{}(t,x_{t})dW_{t}\] (1)

where the goal is to select \(\) to enforce \(x_{T} X_{true}\) for some distributional notion of similarity such as the Wasserstein distance [Kidger et al., 2021b] or Kullback-Leibler divergence [Li et al., 2020]. However, these objectives are _simulation-based_, requiring a backpropagation through an SDE solver, which suffers from severe speed and stability issues. While some issues such as memory and numerical truncation can be ameliorated using the adjoint state method and advanced numerical solvers [Kidger et al., 2021b], optimization of Neural SDEs is still a significant issue.

We note that in the special case of zero-diffusion (i.e. \(g_{}(t,x_{t})=0\)) this reduces to a neural _ordinary_ differential equation (Neural ODE) [Chen et al., 2018], which is easier to optimize than SDEs, but still presents challenges to scalability.

### Matching algorithms

Matching algorithms are a _simulation-free_ class of training algorithms which are able to bypass backpropagation through the solver during training by constructing the marginal distribution as a mixture of tractable conditional probability paths.

The marginal density \(p_{t}\) induced by eq. 1 evolves according to the _Fokker-Plank_ equation (FPE):

\[_{t}p_{t}=-(p_{t}f_{t})+}{2} p_{t}\] (2)

where \( p_{t}=( p_{t})\) denotes the _Laplacian_ of \(p_{t}\) and gradients are taken with respect to \(x_{t}\).

Matching algorithms first construct a factorization of \(p_{t}\) into conditional densities \(p_{t}(x_{t}|z)\) such that \(p_{t}=_{(z)}[p_{t}(x_{t}|z)]\) and where \(p_{t}(x_{t}|z)\) is generated by an SDE \(dx_{t}=v_{t}(x_{t}|z)dt+_{t}(x_{t}|z)dW_{t}\). Given this construction it can be shown that the minimizer of

\[_{}():=_{t,q(z),p_{t}(x|z)}[\| f_{}(t,x_{t})-v_{t}(x_{t}|z)\|^{2}+_{t}^{2}\|g_{}(t,x _{t})-_{t}(x_{t}|z)\|^{2}]\] (3)

satisfies the FPE of the marginal \(p_{t}\). This is especially useful in the generative modeling setting where \(q_{0}\) is samplable noise (e.g. \((0,1)\)) and \(q_{1}\) is the data distribution. Then we can define \(z:=(x_{0},x_{1})\) as a tuple of noise and data with \(q(z):=q_{0}(x_{0}) q_{1}(x_{1})\). This makes eq. 3 optimize a model which will draw new samples according to the data distribution \(q_{1}(x_{1})\) using

\[x_{0} q_{0}; x_{1}=_{0}^{1}f_{}(t,x_{t})dt+g_{}(t,x_{ t})dW_{t}\] (4)

with the integration computed numerically using any off-the-shelf SDE solver. While this is guaranteed to preserve the distribution over time, it is not guaranteed to preserve the _coupling_ of \(q_{0}\) and \(q_{1}\) (if given).

Paired bridge matchingIn generative modeling random pairings [Liu et al., 2023, Albergo and Vanden-Eijnden, 2023, Albergo et al., 2023] or optimal transport [Tong et al., 2024, Pooladian et al., 2023] pairings are constructed for the conditional distribution \(q(z)\). However, in some problems we would like to match pairs of points as is the case in image-to-image translation [Isola et al., 2017, Liu et al., 2023a, Somnath et al., 2023]. In this case, training data comes as pairs \((x_{0},x_{1})\). In this case we set \(q(z):=q(x_{0},x_{1})\) to be samples from these known pairs, and optimize eq. 3. While empirically, these models perform well, there are no guarantees that the coupling will be preserved outside of the special case when data comes from the (entropic) optimal transport coupling \(_{}^{*}(q_{0},q_{1})\) and defined as:

\[_{}^{*}(q_{0},q_{1})=*{arg\,min}_{ U(q_{0},q_ {1})} d(x_{0},x_{1})^{2}\,d(x_{0},x_{1})+\,( \|q_{0} q_{1}),\] (5)where \(U(q_{0},q_{1})\) is the set of admissible transport plans (i.e. joint distributions over \(x_{0}\) and \(x_{1}\) whose marginals are equal to \(q_{0}\) and \(q_{1}\)) as shown in (Shi et al., 2023) for some regularization parameter \(_{ 0}\).

## 3 Trajectory Flow Matching

We now describe our simulation-free method to learn SDEs on time-series data using _trajectory flow matching_ as summarized in Alg. 1. In the case of time series we need to ensure that trajectory couplings are preserved. We first set out a general algorithm for flow matching on vector fields in SS3.1 then present a numerical reparameterization which we find stabilizes training in SS3.2, a next observation prediction for irregularly sampled time series in SS3.3, and finally present how to learn the noise in SS3.4.

### Preserving Couplings

In this section, we assume access to fully observed and evenly spaced trajectories \(=(x^{1},x^{2},,x^{n})\) with \(x^{i}:=(x_{1}^{i},x_{2}^{i},,x_{T}^{i})\) for clarity and notational simplicty. We note that our method is easily extensible to the more general setting of irregularly sampled trajectories. In this simplified case we let

\[z :=(x_{1},x_{2},,x_{T})\] (6) \[q(z) :=()\] (7) \[p_{t}(x|z) :=(( t-t)x_{ t}+(t- t )x_{ t},^{2}( t-t)(t- t) )\] (8) \[u_{t}(x|z) :=-x_{t}}{ t-t}\] (9)

where \(()\) is the uniform empirical distribution over \(\), \(\), \(\) are the ceiling and floor functions, and \((,)\) is the multivariate normal distribution. This is a valid regression in the sense that a function minimized with Alg. 1 will return a stochastic process that will match the observed marginal distributions over time as shown in the following lemma.

**Lemma 3.1**.: _The SDE \(dx_{t}=u_{t}(x|z)dt+^{2}dW_{t}\) where \(u_{t}\) is defined in eq. 9 generates \(p_{t}(x|z)\) in eq. 8 with initial condition \(p_{0}:=_{x_{1}}\) where \(\) is the Dirac delta function._

however, while useful, this is still insufficient for time series modeling, as it does not ensure coupling preservation. For intuition why this is an issue see Figure 2.

In TFM we ensure that the couplings are preserved for history lengths \(h>0\). i.e. \((x_{T-h},x_{T-h+1},,x_{T})=(x_{T-h},x_{T-h+1},,x_{T})\). We first establish a method to ensure that these couplings are preserved allowing us to use simulation-free flow matching training for the time-series modeling task. Specifically, as long as the model takes as input \((x_{T-h},x_{T-h+1},,x_{T})\) in predicting the flow from \(T T+1\), then there exists a function \(f_{}(X_{T-h:T})\) such that the coupling is preserved.

**Proposition 3.2** (Coupling Preservation).: _Under mild regulatory criteria on \(u_{t}(|z)\), \(p_{t}\), and \(q\), if_

\[_{t(0,T),z q(z),c q(c|z),x_{t} p_{t}(x_ {i}|z)}\|u_{t}(x_{t}|z,c)-u_{t}(x_{t}|c)\|_{2}^{2}=0\]

_and \(z,q(z),p_{t}(x|z)\) and \(u_{t}(x|z)\) are as defined in eqs. 6-9 then \((u)^{}=^{}(x_{1:T})\)._Where \((u)^{}\) represents the coupling of a model which attains minimal loss according to eq. 3 and \(^{}(x_{1:T})\) is the coupling of the data distribution. Intuitively, as long as no two paths cross given conditionals \(c\), then the coupling is preserved. In prior work \(c=\), and the coupling is only preserved in special cases such as eq. 5.

We next enumerate three assumptions under which the coupling is guaranteed to be preserved at the optima. We note that these are

* When \(c=x_{0}\) and there exists \(T:\) such that \(T(x_{0})=x_{1}\) iff \(^{}(x_{0},x_{1})\). We note that this is equivalent to asserting the existence of a Monge map \(T^{}\) for the coupling \(^{}\).
* There exist no two trajectories \(x^{i}\), \(x^{j}\) such that \(x^{i}_{t}=x^{j}_{t}\) for \(h\) consecutive observations and \(g=0\).
* Trajectories are associated with unique conditional vectors \(c\) independent of \(t\).

Even in cases when **(A1)**-**(A3)** may not hold exactly, TFM is a useful model and can often still learn useful models of the data. In some sense uniqueness up to some history length is enough as it shows TFM is as powerful as discrete-time autoregressive models. Proofs and further examples are available in SSA.1.

### Target prediction reparameterization

While flow matching generally predicts the flow, there is a target predicting equivalent namely given \(v_{}(t,x):=_{t}^{ t}(t,x_{t})-x_{t}}{ t -t}\) and \(u_{t}(x|z):=-x_{t}}{ t-t}\) which is equivalent to \(x_{1}-x_{0}\) when \(x_{t}:tx_{1}+(1-t)x_{0}\) then it is easy to show that the target predicting loss is equivalent to a time-weighted flow-matching loss. Specifically let the target predicting loss be

\[_{}()=_{t,q(z),p_{t}(z|z)}\|_{ }^{ t}(t,x)-x^{ t}\|^{2}\] (10)

then it is easy to show that

**Proposition 3.3**.: _There exists a scaling function \(c(t):_{+}\) such that \(_{}()=c(t)_{}()\)._

### Irregularly sampled trajectories

We next consider irregularly sampled time series of the form \(x^{i}:=((x^{i}_{1},t^{i}_{1}),(x^{i}_{2},t^{i}_{2}),,(x^{i}_{T},t^ {i}_{T}))\) with \(t^{i}_{1}<t^{i}_{2}<<t^{i}_{T}\) with \(t_{next}\) denoting the next timepoint observed after time \(t\). In this case, when combined with the target predicting reparameterization in SS3.2, we can predict the time till next observation. We therefore parameterize an auxiliary model \(h_{}(t,x_{t}):[0,T]^{d}[0,T]\) which predicts the next observation time. This is useful numerically, but also, perhaps more importantly, is useful in a clinical setting, where the spacing between measurements can be as informative as the measurements themselves . \(h_{}\) is trained to predict the time till the next observation with the _time predictive loss_:

\[_{}()=_{t^{i}}\|h_{}(t,x_{t} )-(t_{}-t)\|_{2}^{2}\] (11)

where \(t_{}\) is the time of the next measurement. This can be used in conjunction with the \(x_{}\) predictor to calculate the flow at time \(t\) as

\[v_{}(t,x_{t}):=_{}^{1}(t,x_{t})-x_{t}}{h_{}(t,x _{t})-t}\] (12)

which can be used for inference on new trajectories.

### Uncertainty prediction

Finally, we consider uncertainty prediction. till now we have defined conditional probability paths using a fixed noise parameter \(\). However, this does not have to be fixed. Instead, we consider a _learned_\(_{}(t,x_{t})\) which can be learned iteratively with the loss:

\[_{}(,x)=_{t}\| _{}(t,x_{t})-\|_{}(t,x_{t})-x_{}\|_{2}^{2} \|_{2}^{2}\] (13)

which learns to predict the error in the estimate of \(x_{t}\). This loss can be interpreted as training an epistemic uncertainty predictor which is similar to that proposed in direct epistemic uncertainty prediction (DEUP) .

## 4 Experimental Results

In this section we empirically evaluate the performance of the trajectory flow matching objective in terms of time series modeling error, but also uncertainty quantification. We also evaluate a variety of simulation-based and simulation-free methods including both stochastic and deterministic methods. Stochastic methods are in general more difficult to fit, but can be used to better model uncertainty and variance. Further experimental details can be found in SSB. Experiments were run on a computing cluster with a heterogenous cluster of NVIDIA RTX8000, V100, A40, and A100 GPUs for approximately 24,000 GPU hours. Individual training runs require approximately one gpu day.

BaselinesIn addition to different ablations of trajectory flow matching, we also evaluate NeuralODE (Chen et al., 2018), NeuralSDE (Li et al., 2020; Kidger et al., 2021; Kidger, 2022), Latent NeuralODE (Rubanova et al., 2019), and an aligned flow matching method (Aligned FM) (Liu et al., 2023; Somnath et al., 2023) where the couplings are sampled according to the ground truth coupling during training.

MetricsWe primarily make use of two metrics. The average mean-squared-error (Mean MSE) over left out time series to measure the time series modeling error defined as

\[(,x)=_{t[2,T]}\|_{t}-x_{t}\|_{2}^{2},\] (14)

where \(\) and \(x\) are the predicted and true trajectories respectively. We also use the _maximum mean discrepancy_ with a radial basis function kernel (RBF MMD) which measures how well the distribution over next observation is modelled by comparing the predicted distribution to the distribution over next states in the ground truth trajectory. Specifically we compute:

\[(,,x):=_{t[2,T]}( _{t},_{t})\] (15)

where \(_{t}=_{t}-x_{t-1}\), \(_{t}=x_{t}-x_{t-1}\), and \(_{t}:=_{s=t-1}^{t}f_{}(s,x_{s})ds+g_{}(t,x_{s})dW_{s}\) is a set of samples from the model prediction at time \(t\).

### Exploring coupling preservation with 1D harmonic oscillators

We begin by evaluating how trajectory flow matching performs in a simple one dimensional setting of harmonic oscillators. We show that the canonical conditional flow and bridge matching (Liu et al., 2023; 20; 20), specifically aligned approaches (Somnath et al., 2023; Liu et al., 2023) are unable to preserve the coupling even in a simple one dimensional setting. However, augmented with our trajectory flow matching approach, and specifically using **(A2)**, which includes information on previous observations, the model is able to fit the harmonic oscillator dataset well.

The harmonic oscillator dataset consists of one-dimensional oscillatory trajectories from a damped harmonic oscillator, with each trajectory distinguished by a unique damping coefficient \(c\). Specifically we sample trajectories \(x\) from:

\[x_{i}=x_{i-1}+v_{i-1}(t_{i}-t_{i-1}); x_{0}=1\] (16)

Figure 2: 1D harmonic oscillator overfitting experiment results. **Left:** TFM-ODE (ours) with memory = 3. **Middle:** TFM-ODE (ours) without memory. **Right:** Aligned FM (Liu et al., 2023; Somnath et al., 2023).

where \(v\) is the velocity of the oscillator updated by

\[v_{i}=v_{i-1}+(-v_{i-1}-x_{i-1})(t_{i}-t_{i-1});  v_{0}=0\] (17)

with \(t_{i}=0.1 i\) for \(i=0,1,2,,99\), spring constant \(k=1\), and mass \(m=1\).

As \(c\) increases, the trajectories evolve from underdamped scenarios with prolonged oscillations to critically and overdamped states where the oscillator quickly stabilizes. This leads to intersecting trajectories due to frequency and phase differences, despite their shared starting point. We perform overfitting experiments on three trajectories generated by varying \(c\).

As shown in Figure 2, models without history information are unable to distinguish between the three crossing trajectories that share the same starting point, resulting in overlapping predictions. In contrast, TFM-ODE that incorporates three previous observations is able to fit the crossing trajectories with high accuracy, with the predicted trajectories almost completely overlapping the ground truth. This is because the dataset with satisfies **(A2)** with \(h=4\) (TFM-ODE), but not \(h=0\) (TFM-ODE no memory and Aligned FM).

### Experiments on clinical datasets

Next we compared the performance of TFM and TFM-ODE with the current SDE and ODE baselines, respectively, for modeling real-world patient trajectories formed with heart rate and mean arterial blood pressure measurements within the first 24 hours of admission across four different datasets. These are clinical measurements that are taken most frequently and used to evaluate the hemodynamic status of patients, a key indicator of disease severity. Additionally, we evaluated our models against flow matching on these datasets, each with distinct characteristics, to assess their ability to generalize across different distributions. A full description of the datasets are available in Appendix B.2 with the publicly available datasets used under The PhysioNet Credentialed Health Data License Version 1.5.0 and the EHR dataset with local institutional IRB approval:

* **ICU Sepsis:** a subset of the eICU Collaborative Research Database v2.0 [Pollard et al., 2019] of patients admitted with sepsis as the primary diagnosis.
* **ICU Cardiac Arrest:** a subset of the eICU Collaborative Research Database v2.0 [Pollard et al., 2019] of patients at risk for cardiac arrest.
* **ICU GIB:** a subset of the Medical Information Mart for Intensive Care III [Johnson et al., 2016] of patients with gastrointestinal bleeding as the primary diagnosis.
* **ED GIB:** patients presenting with signs and symptoms of acute gastrointestinal bleeding to the emergency department of a large tertiary care academic health system.

#### 4.2.1 Prediction accuracy and precision: TFM and TFM-ODE

TFM-ODE yields more accurate trajectory predictionAcross the four datasets TFM-ODE outperformed the baseline models by \(15\%\) to \(20\%\), as seen in table 1. We noticed that TFM has a similar performance as TFM-ODE. In one case TFM outperformed the non-stochastic TFM-ODE, as seen in the ICU GIB dataset. For ICU sepsis, the performance improvement from the baseline is the most significant, around \(83\%\). This coincides with the ICU sepsis dataset having the most amount of measurement per trajectory. The improvement is seen in both TFM and TFM-ODE, possibly indicating they are able to learn better given more data, resulting in a more precise flow. Not formally measured, we noted that given the same time constraint, FM based models were significantly faster and often finished training before the time limit.

TFM yields better uncertainty predictionThough TFM-ODE had lower test MSE for half of the times, TFM yielded better uncertainty prediction overall, as seen in table 2. Notably, TFM also had less variance in the uncertainty prediction than TFM-ODE. A plausible explanation in this case is a sacrifice in bias that subsequently decreases the variance for the stochastic implementation, reflecting the bias-variance trade off. Sampled graphs of TFM can be seen in figure 3. It is notable that the model is able to detect the measurement uncertainty at certain timepoints, matching the increase in amplitude of oscillation in patient trajectories.

#### 4.2.2 Trajectory Variance Distribution Comparison

TFM trajectories accurately match the noise distribution in the dataTFM is able to match the noise distribution in addition to the overall trajectory shape, which is useful in settings where 

[MISSING_PAGE_FAIL:8]

Uncertainty improves performance of trajectory predictionFor TFM and TFM-ODE, the flow network used to learn the uncertainty \(_{x_{t}}\) is separate from the flow network learning \(x_{t}\). The loss function of the network learning \(x_{t}\) is independent of uncertainty flow network. Therefore, it was unexpected that taking away the uncertainty prediction would result in increased MSE test loss for learning \(x_{t}\). This implies further a process in the synergistic effects between \(x_{t}\) flow and \(_{x_{t}}\) flow.

Trajectory memory may improve performance in high frequency measurement settingsWe conditioned the model based on a sliding window of trajectory history to disentangle data points that otherwise look indistinguishable to FM models. This improved the interpolation performance in the ICU Sepsis and ICU GIB dataset. Notably, this modification did not improve performance the ED GIB dataset, which could be due to shorter trajectories for patients and lower measurement frequency in the defined time period. This may also be explained by the decreased severity of disease in the ED compared to the ICU. Adding memory as a condition may be more suitable for patients whose clinical trajectories have a higher frequency of measurements.

## 5 Related Work

Continuous-time neural network architectures have outperformed traditional RNN methods in modeling irregularly sampled clinical time series to optimize interpolation and extrapolation. Neural ODE with latent representations of trajectories (Rubanova et al., 2019) outperformed RNN-based approaches (Lipton et al., 2016; Che et al., 2018; Cao et al., 2018; Rajkomar et al., 2018) for interpolation while providing explicit uncertainty estimates about latent states. More recently, Neural SDEs appear to outperform LSTM (Hochreiter and Schmidhuber, 1997), Neural ODE (Chen et al., 2018; De Brouwer et al., 2019; Dupont et al., 2019; Lechner and Hasani, 2020), and attention-based (Shukla and Marlin, 2021; Lee et al., 2022) approaches in interpolation performance while natively handling uncertainty using drift and diffusion terms (Oh et al., 2024).

Discrete-time approaches offer an alternative to our continuous-time model model transformers utilize a discrete-time representation with a sequential processing (Gao et al., 2024; Nie et al., 2023; Woo et al., 2024; Ansari et al., 2024; Dong et al., 2024; Garza and Mergenthaler-Canseco, 2023; Das et al., 2024; Liu et al., 2024; Kuvshinova et al., 2024) models for traditional time series modeling. Adapations to the baseline transformer includes structuring observations into text with finetuning (Zhang et al., 2023; Zhou et al., 2023), without finetuning (Xue and Salim, 2024; Gruver et al., 2023), or using autoregressive model vision transformers to model unevenly spaced time series data by converting time series into images (Li et al., 2023).

Continuous-time systems are of great interest for learning causal representations using assumptions by using observations to directly modify the system state (De Brouwer et al., 2022; Jia and Benson, 2019). Variations include intervention modeling with separate ODEs for interventions and outcome processes (Gwak et al., 2020), using liquid time-constant networks (Hasani et al., 2021; Vorbach et al., 2021), or modeling treatment effects with either one (Bellot and van der Schaar, 2021) or multiple interventions (Seedat et al., 2022). The importance of accounting for external interventions is a particular challenge in clinical data, where external interventions (change in environment due to treatment decisions or clinical context such as ED or ICU) are common in clinical data trajectories.

## 6 Conclusion

In this work we present Trajectory Flow Matching, a simulation-free training algorithm for neural differential equation models. We show when trajectory flow matching is valid theoretically, then demonstrate its usefulness empirically in a clinical setting. The ability to model the underlying

    & Uncertainty & Memory & Hidden & ICU Sepsis & ICU Cardiac & ICU GIB & ED GIB \\  & Prediction & & Size & & Arrest & & \\  TFM-ODE & ✓ & ✓ & 256 & **0.793 \(\) 0.017** & 2.762 \(\) 0.017 & 2.673 \(\) 0.069 & 8.245 \(\) 0.495 \\  & & ✓ & 256 & 1.170 \(\) 0.014 & 2.759 \(\) 0.015 & 3.097 \(\) 0.054 & 8.659 \(\) 0.429 \\  & & & 256 & 1.555 \(\) 0.122 & 3.242 \(\) 0.050 & 2.981 \(\) 0.161 & **6.381 \(\) 0.451** \\  & & & 64 & 1.936 \(\) 0.262 & 3.244\(\) 0.025 & 4.003 \(\) 0.347 & 11.253\(\) 4.597 \\  TFM & ✓ & ✓ & 256 & 0.796 \(\) 0.026 & **2.596 \(\) 0.079** & **2.762 \(\) 0.021** & 8.613 \(\) 0.260 \\  & & ✓ & 256 & 0.816 \(\) 0.031 & 2.778 \(\) 0.021 & 2.754 \(\) 0.095 & 8.600 \(\) 0.389 \\  & & & 64 & 1.965 \(\) 0.289 & 3.271 \(\) 0.031 & 4.037 \(\) 0.314 & 7.549 \(\) 0.737 \\   

Table 4: Mean MSE (\( 10^{-3}\)) by ablated versions of TFM, TFM-ODE, and datasets.

continuous physiologic processes during critical illness using irregular, sparsely sampled, and noisy data has the potential for broad impacts in care settings such as the emergency department or ICU. These models could be used to improve clinical decision making, inform monitoring strategies, and optimize resource allocation by identifying which patients are likely to deteriorate or recover. These use cases will require thorough prospective validation and calibration for specific clinical outcomes, for example using the likelihood of a patient crossing a specific heart rate or blood pressure threshold for decisions on level of care (ICU versus inpatient floors) or specific interventions such as transfusions. In these applications, it will be important to assess and control for bias that may be present due to which patient subpopulations are present in training data.

LimitationsLimitations of the method includes the selective utility of integrating memory in clinical settings with high measurement frequency and no current capacity for estimating causal representations, though this will be an important future research direction. Potential harms include the following: erroneous predictions that either results in delayed care or overutilization of the health system. Accurate trajectory predictions have the potential to inform clinical decision-making regarding the appropriate level of care, leading to more timely and appropriate interventions.

Future workWe hope to extend our method to cover other types of time series that have periodicity in the components, potentially incorporating Fourier transform (Li et al., 2021) and Physics-Inspired Neural Networks (PINN). Since interpretability is an important factor for clinical reliability, we are developing methods to further elucidate key components affecting the prediction. As well, we hope to incorporate functional flow matching for fully continuous setting (Kerrigan et al., 2024).

## 7 Broader Impact

Our work extends flow matching into the domain of time series modeling, demonstrating a specific instance of clinical time series prediction. In contrast to the large transformer-based models, our method has fewer in parameters and less training time needed. Notably, it scales well with parameters. As well, our parameterization on Stochastic Differential Equations (SDE) allow faster training time than traditional SDE integration.

Accurate timeseries modeling in healthcare has the potential for significant benefits, but also introduces risks. Benefits that could be derived from more accurate prediction of clinical courses include improved treatment decisions, resource allocation, as well as more informative discussions of prognosis with patients or family members. Risks may come from inaccuracies in predictions which could lead to harms by biasing decision making of clinical teams. In the general case of false negative prediction (prediction of trajectories with falsely favorable outcomes) this may lead to undertreatment and in the case of false positive prediction (prediction of trajectories with incorrect detrimental outcomes) or overtreating patients. These inaccuracies may also propagate biases in training data.

To move towards broad impact in the clinical domain, this work will require validation and bias estimates. Furthermore, models deployed in domains with high-stakes prediction require interpretability, which can help identify biases, miscalibration, discordance with domain knowledge, as well as build trust with teams using predictions from the model. At this time, flow-based methods have limited tools for interpretability, and we recognize this as a gap in need of future work.