# Self-Consuming Generative Models with Curated

Data Provably Optimize Human Preferences

Damien Ferbach\({}^{1,2}\)1, Quentin Bertrand\({}^{1}\), Avishek Joey Bose\({}^{1,3}\), Gauthier Gidel\({}^{1,4}\)

\({}^{1}\)Mila, Universite de Montreal \({}^{2}\)Ecole Normale Superieure de Paris

\({}^{3}\)University of Oxford, \({}^{4}\) Canada CIFAR AI Chair

###### Abstract

The rapid progress in generative models has resulted in impressive leaps in generation quality, blurring the lines between synthetic and real data. Web-scale datasets are now prone to the inevitable contamination by synthetic data, directly impacting the training of future generated models. Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step. However, in practice, synthetic data is often subject to human feedback and curated by users before being used and uploaded online. For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users. In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an _implicit preference optimization mechanism_. However, unlike standard preference optimization, the generative model does not have access to the reward function or negative samples needed for pairwise comparisons. Moreover, our study doesn't require access to the density function, only to samples. We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized. We further provide theoretical results on the stability of the retraining loop when using a positive fraction of real data at each step. Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.

## 1 Introduction

Today state-of-the-art generative models can produce multi-modal generations virtually indistinguishable from human-created content, like text (Achiam et al., 2023), images (Stability AI, 2023), audio (Borsos et al., 2023), or videos (Villegas et al., 2022; Brooks et al., 2024). The democratization of these powerful models by open-sourcing their weights (Stability AI, 2023; Jiang et al., 2023; Touvron et al., 2023) or allowing public inference (Ramesh et al., 2021; Midjourney, 2023; Achiam et al., 2023) paves the way to creating synthetic content at an unprecedented scale--the results inevitably populate the Web. In particular, existing datasets are already composed of synthetic data such as JourneyDB (Pan et al., 2023) and SAC (Pressman et al., 2022). Moreover, Alemohammad et al. (2024, Fig. 2) showed LAION-5B (Schuhmann et al., 2022), a large-scale Web-crawled dataset used to train future generative models, _already_ contains synthetically generated images.

There is strong evidence that the synthetic data on the web has been, to a large extent, curated by human users. For instance, the LAION-Aesthetics datasets contains synthetically generatedimages that have been curated using a reward model learned from human feedback on the Simulacra Aesthetic Captions dataset (Pressman et al., 2022). Additionally, the JourneyDB dataset, contains human-picked images from the Midjourney (Midjourney 2023) discord server, that have been _upscaled_, _i.e.,_ images that have been requested in a higher resolution (see Figure 1). More generally, the user interface of the most popular state-of-the-art text-to-image models (_e.g.,_ Midjourney and Stable Diffusion 2.1 Huggingface implementation) proposes four alternatives for a single prompt for the user to pick their favorite.

While the consequences of iterative retraining of generative models on synthetically generated data have raised a lot of attention in the community (Alemohammad et al., 2024; Shumailov et al., 2023; Bertrand et al., 2024; Dohmatob et al., 2024a), previous works do not consider that generated data could be curated. This subtle nuance may be of major importance. Indeed, in numerous applications, augmenting the datasets with curated synthetically generated data is found to enhance the performance of the downstream task (Azizi et al., 2023; Wang et al., 2023; Gillman et al., 2024) and even generative models themselves (Hemmat et al., 2023; Gulcehre et al., 2023), hinting that quality might not be the biggest problem. On the other hand, recent works Wyllie et al. (2024); Chen et al. (2024b) showed that this might lead to new issues, such as bias amplification.

This is why, in this work, we aim to theoretically understand how the process of curation of synthetic data is connected with the reward model underlying human preferences and what distribution is learned by generative models trained on such curated synthetic data.

**Main contributions**. We summarize our core contributions as the following:

* We first focus on the self-consuming loop on (only) curated synthetic samples: we show that the expected reward gets maximized in Lemma 2.2 and that its variance vanishes. We further provide a convergence result in Theorem 2.1.
* We additionally study the iterative retraining loop when real data is re-injected at each step: we first improve previous results of stability using raw synthetic samples by Bertrand et al. (2024) and show convergence in Kullback-Leibler (KL) divergence to the optimal distribution Theorem 2.2. When using curated synthetic samples, we show that the KL divergence with the optimal distribution remains bounded Theorem 2.4, as well as an improvement on the expected reward Theorem 2.3, enlightening connections with Reinforcement Learning from Human Feedback (RLHF).
* We finally illustrate our theoretical results on synthetic datasets (mixtures of Gaussians and two moons) as well as natural images on CIFAR10 in Section 4. We highlight how curation based on the confidence of a classifier can lead to the emergence of biases (Wyllie et al., 2024).

## 2 Iterative retraining with curated synthetic data

We now study the fully synthetic self-consuming loop with curated samples. Unlike previous works that do not take curation into account (Alemohammad et al., 2024; Shumailov et al., 2023) and focused on stability of the process (Bertrand et al., 2024), we show that retraining with curated samples both maximizes an underlying reward whose variance collapses, and converges to maximum reward regions. In Section 2.1 we first specify explicitly the distribution induced

Figure 1: Illustration of the curation phenomenon: 1. User proposes prompts such as “butterfly going to the bathroom”, 2. Four images are generated with Midjourney, 3. User only _upscale_ one (e.g. the top left image) image, 4. Solely upscaled images are incorporated into the JourneyDB dataset (Pan et al., 2023). Samples from other diffusion models can be found in Figures 11(a) and 11(b).

[MISSING_PAGE_FAIL:3]

[MISSING_PAGE_FAIL:4]

**Lemma 2.2**.: _Let \(p_{t+1}\) be the distribution induced from a discrete choice model on \(p_{t}\) (Equation 4). Suppose Assumption 2.1 B holds, then the expected reward increases proportionally to its variance at each retraining iteration:_

\[_{p_{t+1}}[e^{r(x)}]_{p_{t}}[e^{r(x)} ]+_{p_{t}}[e^{r(x)}]}{e^ {r_{*}}}.\] (7)

_Especially the expected reward converges to the maximum reward and its variance vanishes:_

\[_{p_{t}}[e^{r(x)}]e^{r_{*}} _{p_{t}}[e^{r(x)}]0.\]

**Discussion**. Lemma 2.2 shows that the reward augmentation is directly proportional to the reward variance at each retraining step. In other words, the more heterogeneous the reward is, the more its expectation increases at the next step. Lemma 2.2 further shows that the expected reward converges towards the reward maximizers. We can additionally deduce that the variance is doomed to vanish. This is detailed in Appendix A.4.3 which additionally states that the reward variance decreases fast enough to have finite sum. Finally, we note that Lemma 2.2 helps us understand the fixed points of this process: due to the variance term in Equation 7, a fixed point of the retraining loop must put mass on a single level set of the reward function. The reciprocal is obviously true as detailed in the appendix (Lemma A.3).

We can finally show a stronger result of convergence for the Kullback-Leibler divergence. We will need to assume that at initialization, the probability density puts a positive mass on the level set \(r(x)=r_{*}\). This corresponds to Assumption 2.1 C. Without this assumption, the probability density support would consecutively vanish towards the maximizer of the reward preventing KL convergence. Under assumption 2.1 C, we can denote \(p_{*}\) the probability density at initialization restricted to the domain that maximizes the reward and renormalized: \(p_{*}(x):=(x)_{r(x)=r_{*}}}{_{0}(r(x)=r_{*})}\).

**Theorem 2.1**.: _Let for all \(t 0\), \(p_{t+1}\) be the distribution induced from a discrete choice model on \(p_{t}\) (Equation 4) where \(=(^{d})\) is the set of probability distributions on \(^{d}\). If \(p_{0}\) satisfies Assumption 2.1 C, then we can define \(p_{*}(x):=(x)_{r(x)=r_{*}}}{_{0}(r(x)=r_{*})}\) and the self-consuming loop on curated samples \(p_{t}\) converges to \(p_{*}\):_

\[D_{}(p_{*}||p_{t})0.\]

Theorem 2.1 proved in Appendix A.4.5 shows that the process of retraining with curation Equation 2 eventually converges to _the highest level set of the reward reached at initialization_. In particular, in the limit of a large number of retraining steps, the probability of all smaller rewards vanishes. This can have strong implications when retraining the next generation of generative models on a curated Web-scaled dataset: the learned distribution will lose diversity and collapse to the highest reward samples.

### Stability of iterative retraining on a mixture of real and synthetic data

After showing convergence but variance collapse of the self-consuming loop on curated synthetic samples, we now study the impact on the stability of injecting real data at each step. This setting is motivated by the recent work of Bertrand et al. (2024) that showed stability of the iterative retraining loop with real and synthetic data around a local maximizer \(_{*}\) of the training distribution likelihood. This setting is furthermore relevant since Web-scrolled datasets will presumably keep containing a mixture of real data and human-curated synthetic data. In Section 2.2.1 we first improve previous results on retraining on mixed datasets which underlines the beneficial impact of real data on stability and in Section 2.2.2, we prove both stability and reward augmentation in the setting of mixed real and _curated_ synthetic data.

#### 2.2.1 Iterative retraining without curation

To motivate the impact of real data on the stability of the retraining loop with curation, we focus first on its impact without curation and improve previous results in that setting in Theorem2.2.

**Setting**. In this section only, following the approach of Bertrand et al. (2024), we will not assume infinite capacity for our distribution (_i.e.,_\((^{d})\) and hence adopt a parametric approach \(=_{}:=\{p_{}\}\). Given the current generative model distribution \(p_{_{t}}\), \(p_{_{t+1}}\) must at the next iteration maximize the combined log-likelihood of real and generated data with hyperparameter \(\), _i.e.,_ Equation3 becomes:

\[p_{_{t+1}}=*{arg\,max}_{p_{}_{}} _{p_{}}[ p_{}(x)]+ _{p_{_{t}}}[ p_{}(x)] .\]

We finally denote \(p_{_{*}}=*{arg\,max}_{p_{}_{}} _{p_{}}[ p_{}(x)]\) a maximizer of the data distribution log-likelihood. We also make the following assumption taken from Bertrand et al. (2024):

**Assumption 2.2**.: _For \(\) close enough to \(_{*}\), the mapping \(x_{}^{2} p_{}(x)\) is \(L\)-Lipschitz and the mapping \(_{p_{}}[ p_{}(x)]\) is continuously twice differentiable with \(_{p_{}}[_{}^{2} p_{}(x) ]- I 0\). Further suppose \(W_{1}(p_{_{*}},p_{})\), i.e. \(p_{_{*}}\) is close to the data distribution \(p_{}\)._

Bertrand et al. (2024) proved stability of the retraining loop provided \(\) is sufficiently small. However, their proof is restricted to \(<\), preventing the use of a fraction of synthetic data \(\) bigger than one-third which they left as future work. In Theorem2.2, we extend their proof to any fraction of synthetic data provided the best model distribution is sufficiently close to \(p_{}\) in Wasserstein distance (Villani et al., 2009) _i.e.,_\(_{1}(p_{_{*}},p_{})<\). Additionally, we express the result in distribution, while they expressed it in parameter space.

**Theorem 2.2**.: _Under Assumption2.2, if \(L<\) and \(<\), then there exists a neighborhood of the optimal distribution parameters \(_{*}\) such that for any initial parameters \(_{0}\) in that neighborhood, \(p_{_{t}}\) converges to \(p_{_{*}}\) exponentially fast:_

\[D_{}(p_{_{*}}||p_{_{t}})=}( ()^{2t}).\]

#### 2.2.2 Iterative retraining on a mixture of real and curated samples

Interestingly when curating the synthetic samples we cannot expect stability around the optimal distribution (\(_{*}\) in Theorem2.2) since it is no longer a fixed point of the retraining loop. We will instead show a closeness result in KL divergence combined with an increasing property of the expectation of the reward, which bears close connections to RLHF. We therefore now study the setting described in Equation3 where the synthetic samples are curated using a discrete \(K\)-choice model and real data is reused at each step (\(<\)). In other words, we suppose that the retraining step uses a mixture of a reference distribution and a curated distribution as

\[p_{t+1}(x)=p_{}(x)+p_{t}(x) H_{p_{t}}^{K}(x)}^{K}$ is defined in Equation~{})}.\] (8)

In Theorem2.3, we prove that when retraining on a mixture of curated samples and samples from the reference distribution, the reward increases with respect to the reference distribution:

**Theorem 2.3**.: _Let \(>0\) and consider the process \((p_{t})\) defined in eq. 8, with \(p_{0}=p_{}\). If \(p_{}\) satisfies Assumption 2.1 B, then for all \(t 1\):_

\[_{p_{t}}[e^{r(x)}]_{p_{}}[ e^{r(x)}]+}_{p_{ }}[e^{r(x)}]}{Ke^{r_{*}}}.\]

**Discussion**. A first interesting case is taking the reference distribution \(p_{}\) equal to \(p_{}\). In that case, we recover the fact that \(p_{}\) is not a fixed point of the retraining loop as soon as different reward values have non-zero probabilities to happen (we recover the result from Lemma A.3). In fact, Theorem 2.3 shows that such a process initialized at \(p_{}\) will increase the reward expectation. The second interesting case is taking \(p_{}=p_{0}\) the generative model at initialization. In that case, retraining on a mixture of samples from the initial model and curated samples from the current model improves the reward expectation with respect to initialization.

After showing that such a retraining loop improves the expected reward, we can conversely show that this process does not deviate too much from \(p_{}\).

**Theorem 2.4**.: _Let \(>0\) and \(p_{}(^{d})\) with a density with respect to Lebesgue measure. Consider the process \((p_{t})\) defined in Equation 8, with \(p_{0}=p_{}\). Suppose that \(<\), then, for all \(t 1\)_

\[D_{}(p_{t}||p_{})-(1-(K-1)).\]

Applying Theorem 2.4 with \(p_{}=p_{}\) shows that retraining on a mixture of real and curated synthetic samples does not deviate too much from the data distribution. On the other hand, when setting \(p_{}\) to be any initial model distribution, we see that reusing samples from the initial model stabilizes the retraining loop around initialization.

**Connection with RLHF**. Theorem 2.3 and Theorem 2.4 together emphasize that retraining on a mixture of reference and filtered synthetic data bears important connections with RLHF. Indeed, the RLHF objective is composed of both a reward maximization term and a KL regularization between the current and initial model. In turn, Theorem 2.3 states that the expected reward increases and Theorem 2.4 shows that the KL divergence with respect to initialization remains bounded. The upper bound on the KL divergence further indicates that setting \(K\) small, _i.e.,_ using fewer samples for comparison acts as a regularizer, as previously noticed.

## 3 Related work

**Iterative retraining on synthetic data and model collapse**. The study of the retraining loop of a generative model on synthetic data has witnessed a recent surge of interest. Alemohammad et al. (2024); Shumailov et al. (2023) first evidenced catastrophic degradation of the generated data in the fully synthetic loop. Bertrand et al. (2024) mitigate these conclusions in the setting where the model is retrained on a mixture of synthetic and real data and they show the stability of the process around the data distribution. Briesch et al. (2023) specifically focus on large langage models and Hataya et al. (2023); Martinez et al. (2023) study large scale datasets. A recent theoretical push by Dohmatob et al. (2024, 2024) provides bounds on the performance degradation in the regression setting as well as modified scaling laws. Finally recent works, Wyllie et al. (2024); Chen et al. (2024) study the emergence or amplification of biases in self-consuming loops.

**Aligning models with human preferences**. With the urgent and critical safety concerns of public deployment, the need to align models with human preferences has gained significant importance. RLHF is a popular reinforcement learning technique to align an already pretrained and finetuned model on human preferences (Christiano et al., 2017; Stiennon et al., 2020; Lee et al., 2021; Ouyang et al., 2022; Shin et al., 2023). It consists of two steps: first fitting a reward \(r(x)\) on human preferences using a dataset of pairwise human comparisons and then, maximizing the expected reward over the model distribution. A Kullback-Leibler regularization to the initial model is further used during the maximization step to avoid reward hacking (Skalse et al., 2022; Chen et al., 2024) or catastrophic forgetting (Korbak et al., 2022). Variants of RLHF have recently been proposed such as Direct Preference Optimization (DPO) which maximizes the reward directly without modelingit (Rafailov et al., 2024), Identity Preference Optimization (IPO) (Azar et al., 2024) or Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024).

## 4 Experiments

This section aims to empirically illustrate our previous theoretical results on how curation impacts the self-consuming loop. In Algorithm1, we recall and detail the different steps performed in our experiments.

**Synthetic datasets**. We first focus on two synthetic datasets: a mixture of Gaussians and the two moons dataset. For both datasets, we study the two settings of solely retraining on curated synthetic samples (\(=\)) and mixed datasets (\(=1\)). In Figure4, we iteratively retrain a denoising diffusion probabilistic model (DDPM, Ho et al. 2020) on a mixture of \(8\) Gaussians. The reward \(r(x)\) used for the discrete choice model is the clipped negative Euclidean distance to one of the centers of the Gaussians \(x_{*}\), _i.e.,_\(r(x):=-\{0,\|x-x_{*}\|-r_{}\}\) where we choose \(=10,r_{}=1\). Clipping the distance is used to ensure that the process does not collapse to a single point. Indeed applying Theorem2.1, we know that the density will converge to a renormalized Gaussian distribution restricted to the ball centered at \(x_{*}\) of radius \(r_{}\). In Figure5, we plot the retraining curves on the two moons dataset: to compute the reward, we use an MLP classifier with \(2\) hidden layers of width \(512\) which yields probabilities \(q_{0}(x),q_{1}(x)\) for each class. The reward is then defined as : \(r(x):= q_{0}(x)\), \(>0\). Both Figure4 and Figure5 illustrate that retraining on solely curated samples induces collapse to regions that maximize the reward: respectively one mode of the MoG or one single moon. On the other hand, the use of real data results at the same time both in stability and higher density in high reward regions. Further experimental details are provided in AppendixA.5.

### Natural images on CIFAR10

**Setting**. We train a normalizing flow using optimal transport conditional flow matching (Lipman et al., 2022; Shaul et al., 2023; Tong et al., 2023b) with the _torchfm_ library Tong et al. (2023a, 2024). The initial model has been pretrained on the \(50000\) train images of the CIFAR-10 dataset (Krizhevsky et al., 2009). At each iteration, we generate \(5 10^{4}\) samples using the current model from which we keep \(2.5 10^{3}\) samples filtered by discrete \(K\)-choice comparisons. The reward \(r(x)\) is computed using the class probabilities \(q_{0}(x),,q_{9}(x)\) from a _pretrained_ VGG11 classifier (Simonyan and Zisserman, 2014) with \(92.39\%\) test accuracy. Due to the expensive compute cost of retraining a generative model for multiple iterations (c.f. AppendixA.5.4), we plot only one run on each figure. To ensure the reproducibility of our results, we plot the retraining curves for \(3\) independent runs in Figure11 in the appendix, illustrating that they have small variance.

**Using probability of one class as reward**. As a first experiment, we filter samples following the probability of the classifier on a predefined class. We arbitrarily chose the class \(0\) corresponding to planes. The reward is then defined as \(r(x)= q_{0}(x)\), \(>0\). We plot the evolution of the class proportions as well as the averaged reward across \(10\) retraining steps in Figure2 with \(=5\). Figure2 shows collapse to the single plane class as the reward increases monotonically, illustrating Lemma2.2.

**Using the confidence of the classifier as a reward: the emergence of bias**. As a second experiment, we use the confidence of the classifier as a reward, _i.e.,_\(r(x)=_{0 i 9}q_{i}(x)\), \(>0\). As written, the reward is therefore uncorrelated from the class but, remains implicitly correlated to it by the fact that _the classifier statistics are class dependent_. In Figure3 we plot the evolution of the class proportions as well as the average reward. As expected by our theoretical results in Section2, the average reward increases monotonically. On the other hand, we clearly see that the class proportions become more and more heterogeneous throughout the retraining loop. While confirming our theoretical study this plot therefore additionally shows that retraining on filtered samples increases bias, in a setting where the reward is implicitly correlated to diversity. Taking a step back, this has strong societal and ethical implications as it may imply that in a filtered internet biases may emerge or strengthen as we explain in Section6.

**Reusing real samples: stability and reward augmentation**. Finally, we illustrate our results from Section2.2.1 by mixing real and filtered synthetic samples with hyperparameter \(=\). Figure3 shows that the process remains stable as the proportion of classes remains approximately uniform (as suggested by Theorem 2.3). On the other hand, the average reward increases before stabilizing as predicted by Theorem 2.3.

## 5 Conclusion and open questions

We study the impact of data curation on the training of generative models in the self-consuming loop. We provide theoretical results demonstrating that the expected reward underlying the curation process increases and its variance collapses (Lemma 2.2) as well as a convergence result (Theorem 2.1) for the generative model. We additionally provide stability guarantees when reusing real data at each step (Theorem 2.3 and Theorem 2.4) establishing close connections with RLHF and preference optimization. Our work sheds light and theoretically grounds a novel phenomenon: increasing the proportion of curated synthetic data on the Web automatically optimizes preferences for future trained large models. A limitation is that we do not propose an algorithm to address emerging issues like bias amplification as we feel it goes beyond the scope of our paper and is a substantially complex field already intensively explored (Grover et al., 2019; Wyllie et al., 2024; Chen et al., 2024b). We believe, however, that it should be a research priority and constitutes an interesting

Figure 3: **CIFAR-10**. Evolution of the proportion of each class and the average reward \(r(x)\) when filtering based on the confidence of a classifier. On the left, retraining is done solely on the curated synthetic samples which results in the emergence of proportion biases. On the right, retraining is performed on a mixture of real and curated synthetic samples which results in both increased stability and still reward augmentation.

Figure 2: **CIFAR-10**. Evolution of the proportion of the class ‘Airplane’ and of the \(9\) other classes when filtering on curated synthetic samples with reward \(r(x)= q_{0}(x)\)

avenue for future work. Another interesting direction is to study the impact of the particular reward function underlying filtering (confidence, quality, diversity...) on the emerging bias amplification.

## 6 Broader impacts

Training and aligning large generative models are prone to substantial ethical concerns regarding their alignment objective (Shen et al., 2023), representational disparities of the training datasets (Clemmensen and Kjaersgaard, 2022), or the presence of harmful images in the datasets (Birhane et al., 2021; Schramowski et al., 2023; Birhane et al., 2024). Our work mostly focuses on the impact of the curation of synthetic datasets which itself heavily depends on the agent performing the curation and its underlying reward function. In particular the documentation of the Simulacra Aesthetic Captions dataset (Pressman et al., 2022) alerts that the human-based curation step is performed by a group of individuals that lacks diversity, mostly from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) individuals (Henrich et al., 2010). A similar bias is likely occurring in the JourneyDB (Pan et al., 2023) dataset and, more generally, in the synthetic data appearing on the web. However, our work mostly revolves around a theoretical analysis and raises awareness of the implicit alignment and potential bias amplification of self-consuming generative models. We therefore firmly believe that the potential benefits of this awareness outweigh the potential unforeseen negative consequences of this work.

## 7 Acknowledgements

We sincerely thank Sophie Xhonneux for providing feedback and corrections on the paper. We further thank Mats L. Richter and William Buchwalter for fruitful discussions about the experiments. We thank Mila Cluster for access to computing resources, especially GPUs. AJB is partially funded by an NSERC Post-doc fellowship and an EPSRC Turing AI World-Leading Research Fellowship.