# What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?

Fnu Suya\({}^{1}\) Xiao Zhang\({}^{2}\) Yuan Tian\({}^{3}\) David Evans\({}^{1}\)

\({}^{1}\)University of Virginia \({}^{2}\)CISPA Helmholtz Center for Information Security

\({}^{3}\)University California Los Angeles

suya@virginia.edu, xiao.zhang@cispa.de, yuant@ucla.edu, evans@virginia.edu

###### Abstract

We study indiscriminate poisoning for linear learners where an adversary injects a few crafted examples into the training data with the goal of forcing the induced model to incur higher test error. Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners. For theoretical Gaussian distributions, we rigorously characterize the behavior of an optimal poisoning attack, defined as the poisoning strategy that attains the maximum risk of the induced model at a given poisoning budget. Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small. These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks.

## 1 Introduction

Machine learning models, especially current large-scale models, require large amounts of labeled training data, which are often collected from untrusted third parties . Training models on these potentially malicious data poses security risks. A typical application is in spam filtering, where the spam detector is trained using data (i.e., emails) that are generated by users with labels provided often implicitly by user actions. In this setting, spammers can generate spam messages that inject benign words likely to occur in spam emails such that models trained on these spam messages will incur significant drops in filtering accuracy as benign and malicious messages become indistinguishable [37; 20]. These kinds of attacks are known as _poisoning attacks_. In a poisoning attack, the attacker injects a relatively small number of crafted examples into the original training set such that the resulting trained model (known as the _poisoned model_) performs in a way that satisfies certain attacker goals.

One commonly studied poisoning attacks in the literature are _indiscriminate poisoning attacks_[4; 54; 35; 45; 5; 46; 25; 31; 10], in which the attackers aim to let induced models incur larger test errors compared to the model trained on a clean dataset. Other poisoning goals, including targeted [42; 56; 24; 21; 18] and subpopulation [22; 46] attacks, are also worth studying and may correspond to more realistic attack goals. We focus on indiscriminate poisoning attacks as these attacks interfere with the fundamental statistical properties of the learning algorithm [45; 25], but include a summary of prior work on understanding limits of poisoning attacks in other settings in the related work section.

Indiscriminate poisoning attack methods have been developed that achieve empirically strong poisoning attacks in many settings [45; 46; 25; 31], but these works do not explain why the proposed attacks are sometimes ineffective. In addition, the evaluations of these attacks can be deficient insome aspects  (see Section 3) and hence, may not be able to provide an accurate picture on the current progress of indiscriminate poisoning attacks on linear models. The goal of our work is to understand the properties of the learning tasks that prevent effective attacks under linear models. An attack is considered effective if the increased error (or risk in the distributional sense) from poisoning exceeds the injected poisoning ratio .

In this paper, we consider indiscriminate data poisoning attacks for linear models, the commonly studied victim models in the literature . Attacks on linear models are also studied very recently  and we limit our scope to linear models because attacks on the simplest linear models are still not well understood, despite extensive prior empirical work in this setting. Linear models continue to garner significant interest due to their simplicity and high interpretability in explaining predictions . Linear models also achieve competitive performance in many security-critical applications for which poisoning is relevant, including training with differential privacy , recommendation systems  and malware detection . From a practical perspective, linear models continue to be relevant--for example, Amazon SageMaker , a scalable framework to train ML models intended for developers and business analysts, provides linear models for tabular data, and trains linear models (on top of pretrained feature extractors) for images.

**Contributions.** We show that state-of-the-art poisoning strategies for linear models all have similar attack effectiveness of each dataset, whereas their performance varies significantly across different datasets (Section 3). All tested attacks are very effective on benchmark datasets such as Dogfish and Enron, while none of them are effective on other datasets, such as selected MNIST digit pairs (e.g., 6-9) and Adult, even when the victim does not employ any defenses (Figure 1). To understand whether this observation means such datasets are inherently robust to poisoning attacks or just that state-of-the-art attacks are suboptimal, we first introduce general definitions of optimal poisoning attacks for both finite-sample and distributional settings (Definitions 4.1 and 4.2). Then, we prove that under certain regularity conditions, the performance achieved by an optimal poisoning adversary with finite-samples converges asymptotically to the actual optimum with respect to the underlying distribution (Theorem 4.3), and the best poisoning performance is always achieved at the maximum allowable poisoning ratio under mild conditions (Theorem 4.5).

Building upon these definitions, we rigorously characterize the behavior of optimal poisoning attacks under a theoretical Gaussian mixture model (Theorem 5.3), and derive upper bounds on their effectiveness for general data distributions (Theorem 5.7). In particular, we discover that a larger projected constraint size (Definition 5.5) is associated with a higher inherent vulnerability, whereas projected data distributions with a larger separability and smaller standard deviation (Definition 5.6) are fundamentally less vulnerable to poisoning attacks (Section 5.2). Empirically, we find the discovered learning task properties and the gained theoretical insights largely explain the drastic difference in attack performance observed for state-of-the-art indiscriminate poisoning attacks on linear models across benchmark datasets (Section 6). Finally, we discuss the potential implications of our work by showing how one might improve robustness to poisoning via better feature transformations and defenses (e.g., data sanitization defenses) to limit the impact of poisoning points (Section 7).

**Related Work.** Several prior works developed indiscriminate poisoning attacks by _injecting_ a small fraction of poisoning points. One line of research adopts iterative gradient-based methods to directly maximize the surrogate loss chosen by the victim , leveraging the idea of influence functions . Another approach bases attacks on convex optimization methods , which provide a more efficient way to generate poisoned data, often with an additional input of a target model. Most of these works focus on studying linear models, but recently there has been some progress on designing more effective attacks against neural networks with insights learned from attacks evaluated on linear models . All the aforementioned works focus on developing different indiscriminate poisoning algorithms and some also characterize the hardness of poisoning in the model-targeted setting , but did not explain why certain datasets are seemingly harder to poison than others. Our work leverages these attacks to empirically estimate the inherent vulnerabilities of benchmark datasets to poisoning, but focuses on providing explanations for the disparate poisoning vulnerability across the datasets. Besides injection, some other works consider different poisoning settings from ours by modifying up to the whole training data, also known as unlearnable examples  and some of them, similar to us, first rigorously analyze poisoning on theoretical distributions and then generalize the insights to general distributions .

Although much research focuses on indiscriminate poisoning, many realistic attack goals are better captured as _targeted_ attacks [42; 56; 24; 21; 18], where the adversary's goal is to induce a model that misclassifies a particular known instance, or _subpopulation attacks_[22; 46], where the adversary's goal is to produce misclassifications for a defined subset of the distribution. A recent work that studies the inherent vulnerabilities of datasets to targeted data poisoning attacks proposed the Lethal Dose Conjecture (LDC) : given a dataset of size \(N\), the tolerable amount of poisoning points from any targeted poisoning attack generated through insertion, deletion or modifications is \((N/n)\), where \(n\) is the sample complexity of the most data-efficient learner trained on the clean data to correctly predict a known test sample. Compared to our work, LDC is more general and applies to any dataset, any learning algorithm, and even different poisoning settings (e.g., deletion, insertion). In contrast, our work focuses on insertion-only indiscriminate attacks for linear models. However, the general setting for LDC can result in overly pessimistic estimates of the power of insertion-only indiscriminate poisoning attacks. In addition, the key factor of the sample complexity \(n\) in LDC is usually unknown and difficult to determine. Our work complements LDC by making an initial step towards finding factors (which could be related to \(n\)) under a particular attack scenario to better understand the power of indiscriminate data poisoning attacks. Appendix E provides more details.

## 2 Preliminaries

We consider binary classification tasks. Let \(^{n}\) be the input space and \(=\{-1,+1\}\) be the label space. Let \(_{c}\) be the joint distribution of clean inputs and labels. For standard classification tasks, the goal is to learn a hypothesis \(h:\) that minimizes \((h;_{c})=_{(,y)_{c}}h()  y\). Instead of directly minimizing risk, typical machine learning methods find an approximately good hypothesis \(h\) by restricting the search space to a specific hypothesis class \(\), then optimizing \(h\) by minimizing some convex surrogate loss: \(_{h}\,L(h;_{c})\). In practical applications with only a finite number of samples, model training replaces the population measure \(_{c}\) with its empirical counterpart. The surrogate loss for \(h\) is defined as \(L(h;)=_{(,y)}(h;,y)\), where \((h;,y)\) denotes the non-negative individual loss of \(h\) incurred at \((,y)\).

We focus on the linear hypothesis class and study the commonly considered hinge loss in prior poisoning literature [3; 4; 45; 25; 46] when deriving the optimal attacks for 1-D Gaussian distributions in Section 5.1. Our results can be extended to other linear methods such as logistic regression (LR). A _linear hypothesis_ parameterized by a weight parameter \(^{n}\) and a bias parameter \(b\) is defined as: \(h_{,b}()=(^{}+b)\) for any \(^{n}\), where \(()\) denotes the sign function. For any \(\) and \(y\), the _hinge loss_ of a linear classifier \(h_{,b}\) is defined as:

\[(h_{,b};,y)=\{0,1-y(^{}+b)\}+\|\|_{2}^{2},\] (1)

where \( 0\) is the tuning parameter which penalizes the \(_{2}\)-norm of the weight parameter \(\).

**Threat Model.** We formulate indiscriminate data poisoning attack as a game between an attacker and a victim in practice following the prior work :

1. A clean training dataset \(_{c}\) is produced, where each data point is i.i.d. sampled from \(_{c}\).
2. The attacker generates a poisoned dataset \(_{p}\) using some poisoning strategy \(\), which aims to reduce the performance of the victim model by injecting \(_{p}\) into the training dataset.
3. The victim minimizes empirical surrogate loss \(L()\) on \(_{c}_{p}\) and produces a model \(_{p}\).

The attacker's goal is to find a poisoning strategy \(\) such that the risk of the final induced classifier \((_{p};_{c})\) is as high as possible, which is empirically estimated on a set of fresh testing data that are i.i.d. sampled from \(_{c}\). We assume the attacker has full knowledge of the learning process, including the clean distribution \(_{c}\) or the clean training dataset \(_{c}\), the hypothesis class \(\), the surrogate loss function \(\) and the learning algorithm adopted by the victim.

We impose two restrictions to the poisoning attack: \(|_{p}||_{c}|\) and \(_{p}\), where \(\) is the poisoning budget and \(\) is a bounded subset that captures the feasibility constraints for poisoned data. We assume that \(\) is specified in advance with respect to different applications (e.g., normalized pixel values of images can only be in range \(\)) and possible defenses the victim may choose (e.g., points that have larger Euclidean distance from center will be removed) [45; 25]. Here, we focus on undefended victim models, i.e., \(\) is specified based on application constraints, so as to better assess the inherent dataset vulnerabilities without active protections. However, defense strategies such as data sanititation [12; 45; 25] may shrink the size of \(\) so that the poisoned data are less extreme and harmful. We provide preliminary experimental results on this in Section 7.

## 3 Disparate Poisoning Vulnerability of Benchmark Datasets

Prior evaluations of poisoning attacks on _convex models_ are inadequate in some aspects, either being tested on very small datasets (e.g., significantly subsampled MNIST 1-7 dataset) without competing baselines [3; 10; 34; 35], generating invalid poisoning points [45; 25] or lacking diversity in the evaluated convex models/datasets [31; 32]. This motivates us to carefully evaluate representative attacks for linear models on various benchmark datasets without considering additional defenses.

**Experimental Setup.** We evaluate the state-of-the-art data poisoning attacks for linear models: _Influence Attack_[24; 25], _KKT Attack_, _Min-Max Attack_[45; 25], and _Model-Targeted Attack (MTA)_. We consider both linear SVM and LR models and evaluate the models on benchmark datasets including different MNIST  digit pairs (MNIST 1-7, as used in prior evaluations [45; 25; 3; 46], in addition to MNIST 4-9 and MNIST 6-9 which were picked to represent datasets that are relatively easier/harder to poison), and other benchmark datasets used in prior evaluations including Dogfish , Enron  and Adult [22; 46]. _Filtered Enron_ is obtained by filtering out 3% of near boundary points (with respect to the clean decision boundary of linear SVM) from Enron. The motivation of this dataset (along with Adult) is to show that, the initial error cannot be used to trivially infer the vulnerability to poisoning attacks (Section 6). We choose \(3\%\) as the poisoning rate following previous works [45; 25; 31; 32]. Appendix D.1 provides details on the experimental setup.

**Results.** Figure 1 shows the highest error from the tested poisoning attacks (they perform similarly in most cases) on linear SVM. Similar observations are found on LR in Appendix D.2.5. At \(3\%\) poisoning ratio, the (absolute) increased test errors of datasets such as MNIST 6-9 and MNIST 1-7 are less than 4% for SVM while for other datasets such as Dogfish, Enron and Filtered Enron, the increased error is much higher than the poisoning ratio, indicating that these datasets are more vulnerable to poisoning. Dogfish is moderately vulnerable (\(\) 9% increased error) while Enron and Filtered Enron are highly vulnerable with over 30% of increased error. Consistent with prior work [45; 25; 32], throughout this paper, we measure the increased error to determine whether a dataset is vulnerable to poisoning attacks. However, in security-critical applications, the ratio between the increased error and the initial error might matter more but leave its exploration as future work. These results reveal a drastic difference in robustness of benchmark datasets to state-of-the-art indiscriminate poisoning attacks which was not explained in prior works. A natural question to ask from this observation is _are datasets like MNIST digits inherently robust to poisoning attacks or just resilient to current attacks?_ Since estimating the performance of optimal poisoning attacks for benchmark datasets is very challenging, we first characterize optimal poisoning attacks for theoretical distributions and then study their partial characteristics for general distributions in Section 5.

## 4 Defining Optimal Poisoning Attacks

In this section, we lay out formal definitions of optimal poisoning attacks and study their general implications. First, we introduce a notion of _finite-sample optimal poisoning_ to formally define the optimal poisoning attack in the practical finite-sample setting with respect to our threat model:

Figure 1: Performance of best current indiscriminate poisoning attacks with \(=3\%\) across different benchmark datasets on linear SVM. Datasets are sorted from lowest to highest base error rate.

**Definition 4.1** (Finite-Sample Optimal Poisoning).: Consider input space \(\) and label space \(\). Let \(_{c}\) be the underlying data distribution of clean inputs and labels. Let \(_{c}\) be a set of examples sampled i.i.d. from \(_{c}\). Suppose \(\) is the hypothesis class and \(\) is the surrogate loss function that are used for learning. For any \( 0\) and \(\), a _finite-sample optimal poisoning adversary_\(}_{}\) is defined to be able to generate some poisoned dataset \(^{*}_{p}\) such that:

\[^{*}_{p}=*{argmax}_{_{p}}\;(_{p};_{c})_{p}|_{p}| |_{c}|,\]

where \(_{p}=*{argmin}_{h}\;_{(,y) _{c}_{p}}(h;,y)\) denotes the empirical loss minimizer.

Definition 4.1 states that no poisoning strategy can achieve a better attack performance than that achieved by \(}_{}\). If we denote by \(^{*}_{p}\) the hypothesis produced by minimizing the empirical loss on \(_{c}^{*}_{p}\), then \((^{*}_{p};_{c})\) can be regarded as the maximum achievable attack performance.

Next, we introduce a more theoretical notion of _distributional optimal poisoning_, which generalizes Definition 4.1 from finte-sample datasets to data distributions.

**Definition 4.2** (Distributional Optimal Poisoning).: Consider the same setting as in Definition 4.1. A _distributional optimal poisoning adversary_\(_{}\) is defined to be able to generate some poisoned data distribution \(^{*}_{p}\) such that:

\[(^{*}_{p},^{*})=*{argmax}_{(_{p},)}\; (h_{p};_{c})(_{p}) 0,\]

where \(h_{p}=*{argmin}_{h}\;\{L(h;_{c})+ L(h ;_{p})\}\) denotes the population loss minimizer.

Similar to the finite-sample case, Definition 4.2 implies that there is no feasible poisoned distribution \(_{p}\) such that the risk of its induced hypothesis is higher than that attained by \(^{*}_{p}\). Theorem 4.3, proven in Appendix A.1, connects Definition 4.1 and Definition 4.2. The formal definitions of uniform convergence, strong convexity and Lipschitz continuity are given in Appendix A.1.

**Theorem 4.3**.: _Consider the same settings as in Definitions 4.1 and 4.2. Suppose \(\) satisfies the uniform convergence property with function \(m_{}(,)\). Assume \(\) is \(b\)-strongly convex and \((h;_{c})\) is \(\)-Lipschitz continuous with respect to model parameters for some \(b,>0\). Let \(^{*}_{p}=*{argmin}_{h}\;_{(,y) _{c}^{*}_{p}}(h;,y)\) and \(h^{*}_{p}=*{argmin}_{h}\{L(h;_{c})+^{*}  L(h;^{*}_{p})\}\). For any \(^{},^{}(0,1)\), if \(|_{c}| m_{}(^{},^{})\), then with probability at least \(1-^{}\),_

\[|(^{*}_{p};_{c})-(h^{*} _{p};_{c})| 2}{b}}.\]

_Remark 4.4_.: Theorem 4.3 assumes three regularity conditions to ensure the finite-sample optimal poisoning attack is a consistent estimator of the distributional optimal one (i.e., insights on poisoning from distributional settings can transfer to finite-sample settings): the uniform convergence property of \(\) that guarantees empirical minimization of surrogate loss returns a good hypothesis, the strong convexity condition that ensures a unique loss minimizer, and the Lipschitz condition that translates the closeness of model parameters to the closeness of risk. These conditions hold for most (properly regularized) convex problems and input distributions with bounded densities. The asymptotic convergence rate is determined by the function \(m_{}\), which depends on the complexity of the hypothesis class \(\) and the surrogate loss \(\). For instance, if we choose \(\) carefully, sample complexity of the linear hypothesis class for a bounded hinge loss is \((1/(^{})^{2})\), where \(^{}\) is the error bound parameter for specifying the uniform convergence property (see Definition A.3) and other problem-dependent parameters are hidden in the big-\(\) notation (see Section 15 of  for details). We note the generalization of optimal poisoning attack for linear case is related to agnostic learning of halfspaces , which also imposes assumptions on the underlying distribution such as anti-concentration assumption  similar to the Lipschitz continuity condition assumed in Theorem 4.3. The theorem applies to any loss function that is strongly convex (e.g., general convex losses with the \(_{2}\)- or elastic net regularizers), not just the hinge loss.

Moreover, we note that \(^{*}\) represents the ratio of injected poisoned data that achieves the optimal attack performance. In general, \(^{*}\) can be any value in \([0,]\), but we show in Theorem 4.5, proven in Appendix A.2, that optimal poisoning can always be achieved with \(\)-poisoning under mild conditions.

**Theorem 4.5**.: _The optimal poisoning attack performance defined in Definition 4.2 can always be achieved by choosing \(\) as the poisoning ratio, if either of the following conditions is satisfied:_

1. _The support of the clean distribution_ \((_{c})\)_._
2. \(\) _is a convex hypothesis class, and for any_ \(h_{}\)_, there always exists a distribution_ \(\) _such that_ \(()\) _and_ \(}L(h_{};)=\)_._

_Remark 4.6_.: Theorem 4.5 characterizes the conditions under which the optimal performance is guaranteed to be achieved with the maximum poisoning ratio \(\). Note that the first condition \((_{c})\) is mild because it typically holds for poisoning attacks against undefended classifiers. When attacking classifiers that employ some defenses such as data sanitization, the condition \((_{c})\) might not hold, due to the fact that the proposed defense may falsely reject some clean data points as outliers (i.e., related to false positive rates). The second condition complements the first one in that it does not require the victim model to be undefended, however, it requires \(\) to be convex. We prove in Appendix A.3 that for linear hypothesis with hinge loss, such a \(\) can be easily constructed. The theorem enables us to conveniently characterize the optimal poisoning attacks in Section 5.1 by directly using \(\). When the required conditions are satisfied, this theorem also provides a simple sanity check on whether a poisoning attack is optimal. In particular, if a candidate attack is optimal, the risk of the induced model is monotonically non-decreasing with respect to the poisoning ratio. Note that the theorem above applies to any strongly convex loss function.

## 5 Characterizing Optimal Poisoning Attacks

This section characterizes the distributional optimal poisoning attacks with respect to linear hypothesis class. We first consider a theoretical 1-dimensional Gaussian mixture model and exactly characterize optimal poisoning attack and then discuss the implications on the underlying factors that potentially cause the inherent vulnerabilities to poisoning attacks for general high-dimensional distributions.

### One-Dimensional Gaussian Mixtures

Consider binary classification tasks using _hinge_ loss with one-dimensional inputs, where \(=\) and \(=\{-1,+1\}\). Let \(_{c}\) be the underlying clean data distribution, where each example \((x,y)\) is assumed to be i.i.d. sampled according to the following Gaussian mixture model:

\[\{y=-1,x(_{1},_{1}^{2})& \\ y=+1,x(_{2},_{2}^{2})& .\] (2)

where \(_{1},_{2}>0\) and \(p(0,1)\). Without loss of generality, we assume \(_{1}_{2}\). Following our threat model, we let \( 0\) be the maximum poisoning ratio and \(:=[-u,u]\) for some \(u>0\) be the constraint set. Let \(_{}=\{h_{w,b}:w\{-1,1\},b\}\) be the linear hypothesis class with normalized weights. Note that we consider a simplified setting where the weight parameter \(w\{-1,1\}\).1 Since \(\|w\|_{2}\) is fixed, we also set \(=0\) in the hinge loss function (1).

For clarify in presentation, we outline the connection between the definitions and the main theorem below, which will also show the high-level proof sketch of Theorem 5.3. We first prove that in order to understand the optimal poisoning attacks, it is sufficient to study the family of two-point distributions (Definition 5.1), instead of any possible distributions, as the poisoned data distribution. Based on this reduction and a specification of weight flipping condition (Definition 5.2), we then rigorously characterize the optimal attack performance with respect to different configurations of task-related parameters (Theorem 5.3). Similar conclusions may also be made with logistic loss.

**Definition 5.1** (Two-point Distribution).: For any \(\), \(_{}\) is defined as a _two-point distribution_, if for any \((x,y)\) sampled from \(_{}\),

\[(x,y)=\{(-u,+1)&\\ (u,-1)&.\] (3)

[MISSING_PAGE_FAIL:7]

According to Definition 5.5, \(_{}()\) characterizes the size of the constraint set \(\) when projected onto the (normalized) projection vector \(/\|\|_{2}\) then scaled by \(\|\|_{2}\), the \(_{2}\)-norm of \(\). In theory, the constraint sets conditioned on \(y=-1\) and \(y=+1\) can be different, but for simplicity and practical considerations, we simply assume they are the same in the following discussions.

**Definition 5.6** (Projected Separability and Standard Deviation).: Let \(^{n}\), \(=\{-1,+1\}\), and \(_{c}\) be the underlying distribution. Let \(_{-}\) and \(_{+}\) be the input distributions with labels of \(-1\) and \(+1\) respectively. For any \(^{n}\), the _projected separability_ of \(_{c}\) with respect to \(\) is defined as:

\[_{}(_{c})=_{_{-}}[^{ }]-_{_{+}}[^{}].\]

In addition, the _projected standard deviation_ of \(_{c}\) with respect to \(\) is defined as:

\[_{}(_{c})=_{}(_{c})},\;_{}(_{c})=p_{-}_{_{-}}[^{ }]+p_{+}_{_{+}}[^{}],\]

where \(p_{-}=_{(,y)_{c}}[y=-1]\), \(p_{+}=_{(,y)_{c}}[y=+1]\) denote the sampling probabilities.

For finite-sample settings, we simply replace the input distributions with their empirical counterparts to compute the sample statistics of \(_{}(_{c})\) and \(_{}(_{c})\). Note that the above definitions are specifically for linear models, but out of curiosity, we also provide initial thoughts on how to extend these metrics to neural networks (see Appendix F for preliminary results). Below, we provide justifications on how the three factors are related to the optimal poisoning attacks. Theorem 5.7 and the techniques used in its proof in Appendix B.2 are inspired by the design of Min-Max Attack .

**Theorem 5.7**.: _Consider input space \(^{n}\), label space \(\), clean distribution \(_{c}\) and linear hypothesis class \(\). For any \(h_{,b}\), \(\) and \(y\), let \((h_{,b};,y)=_{}(-y(^{}+b))\) be a margin-based loss adopted by the victim, where \(_{M}\) is convex and non-decreasing. Let \(\) be the constraint set and \(>0\) be the poisoning budget. Suppose \(h_{c}=*{argmin}_{h}L(h;_{c})\) has weight \(_{c}\) and \(h_{p}^{*}\) is the poisoned model induced by optimal adversary \(_{}\), then we have_

\[(h_{p}^{*};_{c})_{h}L(h;_{c}) + L(h;_{p}^{*}) L(h_{c};_{c})+ _{}(_{_{c}}()).\] (5)

_Remark 5.8_.: Theorem 5.7 proves an upper bound on the inherent vulnerability to indiscriminate poisoning for linear learners, which can be regarded as a necessary condition for the optimal poisoning attack. A smaller upper bound likely suggests a higher inherent robustness to poisoning attacks. In particular, the right hand side of (5) consists of two terms: the clean population loss of \(h_{c}\) and a term related to the projected constraint size. Intuitively, the projected separability and standard deviation metrics highly affect the first term, since data distribution with a higher \(_{_{c}}(_{c})\) and a lower \(_{_{c}}(_{c})\) implies a larger averaged margin with respect to \(h_{c}\), which further suggests a smaller \(L(h_{c};_{c})\). The second term is determined by the poisoning budget \(\) and the projected constraint size, or more precisely, a larger \(\) and a larger \(_{_{c}}()\) indicate a higher upper bound on \((h_{p}^{*};_{c})\). In addition, we set \(h=h_{c}\) and the projection vector as \(_{c}\) for the last inequality of (5), because \(h_{c}\) achieves the smallest population surrogate loss with respect to the clean data distribution \(_{c}\). However, choosing \(h=h_{c}\) may not always produce a tighter upper bound on \((h_{p}^{*};_{c})\) since there is no guarantee that the projected constraint size \(_{_{c}}()\) is small. An interesting future direction is to select a more appropriate projection vector that returns a tight, if not the tightest, upper bound on \((h_{p}^{*};_{c})\) for any clean distribution \(_{c}\) (see Appendix D.2 for preliminary experiments). We also note that the results above apply to any (non-decreasing and strongly-convex) margin-based losses.

## 6 Experiments

Recall from Theorem 4.3 and Remark 4.4 that the finite-sample optimal poisoning attack is a consistent estimator of the distributional one for linear learners. In this section, we demonstrate the theoretical insights gained from Section 5, despite proven only for the distributional optimal attacks, still appear to largely explain the empirical performance of best attacks across benchmark datasets.

Given a clean training data \(_{c}\), we empirically estimate the three distributional metrics defined in Section 5.2 on the clean test data with respect to the weight \(_{c}\). Since \(\|_{c}\|_{2}\) may vary across different datasets while the predictions of linear models (i.e., the classification error) are invariant to the scaling of \(\|_{c}\|_{2}\), we use ratios to make their metrics comparable: \(_{_{c}}(_{c})/_{_{c}}(_{c})\) (denoted as Sep/SD in Table 1) and \(_{_{c}}(_{c})/_{_{c}}()\) (Sep/Size). According to our theoretical results, we expect datasets that are less vulnerable to poisoning have higher values for both metrics.

Table 1 summarizes the results where _Error Increase_ is produced by the best attacks from the state-of-the-art attacks mentioned in Section 3. The Sep/SD and Sep/Size metrics are highly correlated to the poisoning effectiveness (as measured by error increase). Datasets such as MNIST 1-7 and MNIST 6-9 are harder to poison than others, which correlates with their increased separability and reduced impact from the poisoning points. In contrast, datasets such as Enron and Filtered Enron are highly vulnerable and this is strongly correlated to their small separability and the large impact from the admissible poisoning points. The results of Filtered Enron (low base error, high increased error) and Adult (high base error, low increased error) demonstrate that poisoning vulnerability, measured by the error increase, cannot be trivially inferred from the initial base error.

When the base error is small, which is the case for all tested benchmark datasets except Adult, the empirical metrics are highly correlated to the error increase and also the final poisoned error. However, when the base error becomes high as it is for Adult, the empirical metrics are highly correlated to the final poisoned error, but not the error increase, if the metrics are computed on the entire (clean) test data. For the error increase, computing the metrics on the clean and correctly classified (by \(_{c}\)) test points is more informative as it (roughly) accounts for the (newly) induced misclassification from poisoning. Therefore, we report metrics based on correctly-classified test points in Table 1 and defer results of the whole test data to Appendix D.2. For datasets except Adult, both ways of computing the metrics produce similar results. The Adult dataset is very interesting in that it is robust to poisoning (i.e., small error increase) despite having a very high base error. As a preliminary experiment, we also extended our analysis to neural networks (Appendix F) and find the identified factors are correlated to the varied poisoning effectiveness when different datasets are compared under similar learners.

Our current results only show the correlation of the identified factors to the varied (empirical) poisoning effectiveness, not causality. However, we also speculate that these factors may _cause_ different poisoning effectiveness because: 1) results in Section 5 characterize the relationship of these factors to the performance of optimal attacks exactly for 1-D Gaussian distributions and partially for the general distributions, and 2) preliminary results on MNIST 1-7 shows that gradual changes in one factor (with the other fixed) also causes gradual changes in the vulnerability to poisoning (Appendix D.2.6). We leave the detailed exploration on their causal relationship as future work.

## 7 Discussion on Future Implications

Our results imply future defenses by explaining why candidate defenses work and motivating defenses to improve separability and reduce projected constraint size. We present two ideas--using data filtering might reduce projected constraint size and using better features might improve separability.

**Reduced projected constraint size.** The popular data sanitization defense works by filtering out bad points. We speculate it works because the defense may be effectively limiting the projected constraint size of \(\). To test this, we picked the combination of Sphere and Slab defenses considered in prior works  to protect the vulnerable Enron dataset. We find that with the defense, the test error is increased from 3.2% to 28.8% while without the defense the error can be increased from 2.9% to 34.8%. Although limited in effectiveness, the reduced _Error Increase_ with the defense is highly correlated to the significantly reduced projected constraint size \(_{_{c}}()\) (and hence a significantly higher Sep/Size)--the Sep/Size metric jumps from 0.01 without defense to 0.11 with defense while

    & &  &  &  \\  & Metric & MNIST 6–9 & MNIST 1–7 & Adult & Dogfish & MNIST 4–9 & F. Enron & Enron \\   & Error Increase & 2.7 & 2.4 & 3.2 & 7.9 & 6.6 & 33.1 & 31.9 \\  & Base Error & 0.3 & 1.2 & 21.5 & 0.8 & 4.3 & 0.2 & 2.9 \\  & Sep/SD & 6.92 & 6.25 & 9.65 & 5.14 & 4.44 & 1.18 & 1.18 \\  & Sep/Size & 0.24 & 0.23 & 0.33 & 0.05 & 0.14 & 0.01 & 0.01 \\   & Error Increase & 2.3 & 1.8 & 2.5 & 6.8 & 5.8 & 33.0 & 33.1 \\  & Base Error & 0.6 & 2.2 & 20.1 & 1.7 & 5.1 & 0.3 & 2.5 \\   & Sep/SD & 6.28 & 6.13 & 4.62 & 5.03 & 4.31 & 1.11 & 1.10 \\   & Sep/Size & 0.27 & 0.27 & 0.27 & 0.09 & 0.16 & 0.01 & 0.01 \\   

Table 1: Explaining disparate poisoning vulnerability under linear models. The top row for each model gives the increase in error rate due to the poisoning, over the base error rate in the second row. The explanatory metrics are the scaled (projected) separability, standard deviation and constraint size.

Sep/SD remains almost the same at 1.18 with and without defense. Similar conclusions can also be drawn for MNIST 1-7 and Dogfish (detailed experimental results are in Appendix D.2.3).

**Better feature representation.** We consider a transfer learning scenario where the victim trains a linear model on a clean pretrained model. As a preliminary experiment, we train LeNet and ResNet18 models on the CIFAR10 dataset till convergence, but record the intermediate models of ResNet18 to produce models with different feature extractors (R-\(X\) denotes ResNet18 trained for \(X\) epochs). We then use the feature extraction layers of these models (including LeNet) as the pretrained models and obtain features of CIFAR10 images with labels "Truck" and "Ship", and train linear models on them.

We evaluate the robustness of this dataset against poisoning attacks and set \(\) as dimension-wise box-constraints, whose values are the minimum and maximum values of the clean data points for each dimension when fed to the feature extractors. This way of configuring \(\) corresponds to the practical scenario where the victim has access to some small number of clean samples so that they can deploy a simple defense of filtering out inputs that do not fall into a dimension-wise box constraint that is computed from the available clean samples of the victim. Figure 2 shows that as the feature extractor becomes better (using deeper models and training for more epochs), both the Sep/SD and Sep/Size metrics increase, leading to reduced error increase. Quantitatively, the Pearson correlation coefficient between the error increase and each of the two factors is \(-0.98\) (a strong negative correlation). This result demonstrates the possibility that better feature representations free from poisoning might be leveraged to improve downstream resilience against indiscriminate poisoning attacks. We also simulated a more practical scenario where the training of the pretrained models never sees any data (i.e., data from classes of "Truck" and "Ship") used in the downstream analysis and the conclusions are similar (Appendix D.2.7).

## 8 Limitations

Our work also has several limitations. We only characterize the optimal poisoning attacks for theoretical distributions under linear models. Even for the linear models, the identified metrics cannot quantify the actual error increase from optimal poisoning attacks, which is an interesting future work, and one possible approach might be to tighten the upper bound in Theorem 5.7 using better optimization methods. The metrics identified in this paper are learner dependent, depending on the properties of the learning algorithm, dataset and domain constraints (mainly reflected through \(\)). In certain applications, one might be interested in understanding the impact of learner agnostic dataset properties on poisoning effectiveness--a desired dataset has such properties that any reasonable learners trained on the dataset can be robust to poisoning attacks. One likely application scenario is when the released data will be used by many different learners in various applications, each of which may be prone to poisoning. We also did not systematically investigate how to compare the vulnerabilities of different datasets under different learning algorithms. Identifying properties specific to the underlying learner that affect the performance of (optimal) data poisoning attacks is challenging but interesting future work.

Although we focus on indiscriminate data poisoning attacks in this paper, we are optimistic that our results will generalize to subpopulation or targeted poisoning settings. In particular, the specific learning task properties identified in this paper may still be highly correlated, but now additional factors related to the relative positions of the subpopulations/individual test samples to the rest of the population under the clean decision boundary will also play important roles.

Figure 2: Improving downstream robustness to poisoning through better feature extractors.