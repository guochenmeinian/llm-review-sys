# OmniJARVIS

Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents

 Zihao Wang\({}^{1}\), Shaofei Cai\({}^{1}\), Zhancun Mu\({}^{2}\), Haowei Lin\({}^{1}\), Ceyao Zhang\({}^{3}\), Xuejie Liu\({}^{1}\)

Qing Li\({}^{3}\), Anji Liu\({}^{4}\), Xiaojian Ma\({}^{3}\), Yitao Liang\({}^{1}\)

Corresponding Author.

**Team CraftJarvis**

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)Yuanpei College, Peking University

\({}^{3}\)Beijing Institute for General Artificial Intelligence (BIGAI)

\({}^{4}\)University of California, Los Angeles

{zhwang,caishaofei}@stu.pku.edu.cn

xiaojian.ma@ucla.edu,liuanji@cs.ucla.edu,yitaol@pku.edu.cn

###### Abstract

This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model for open-world instruction-following agents in Minecraft. Compared to prior works that either emit textual goals to separate controllers or produce the control command directly, OmniJARVIS seeks a different path to ensure both strong reasoning and efficient decision-making capabilities via _unified_ tokenization of **multimodal interaction data**. First, we introduce a _self-supervised_ approach to learn a behavior encoder that produces discretized tokens for behavior trajectories \(=\{o_{0},a_{0},\}\) and an imitation learning policy decoder conditioned on these tokens. These additional _behavior tokens_ will be augmented to the vocabulary of pretrained Multimodal Language Models. With this encoder, we then pack long-term multimodal interactions involving task instructions, memories, thoughts, observations, textual responses, behavior trajectories, _etc._ into unified token sequences and model them with autoregressive transformers. Thanks to the semantically meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing chain-of-thoughts), plan, answer questions, and act (by producing behavior tokens for the IL policy decoder). OmniJARVIS demonstrates excellent performances on a comprehensive collection of atomic, programmatic, and open-ended tasks in open-world Minecraft. Our analysis further unveils the crucial design principles in interaction data formation, unified tokenization, and its scaling potentials. The dataset, models, and code will be released at https://craftjarvis.org/OmniJARVIS/.

## 1 Introduction

Upon the success of pretrained Large Language Models (LLMs)  and Multimodal Language Models (MLMs) , some recent works have been venturing into developing Vision-Language-Action (VLA) models , a promising pathway towards the ultimate goal of building autonomous agents that can follow and even self-generated instructions to fulfill various reasoning and acting tasks in open world environments. Among them, two most prominentarchitectures have been proposed: 1) Combining an off-the-shelf MLM [31; 1] with separate goal-conditioned controllers [28; 10; 9], where MLM reasons, plans and pilots the controllers by producing textual goal instructions, \(e\)._g_. DEPS , JARVIS-1 , voyager ; 2) Tuning a pretrained MLM into producing control commands directly, while maintaining the reasoning and language capabilities, \(e\)._g_. RT-2 , LEO . However, these two designs could still have significant drawbacks when it comes to open-world environments. First, an open world (e.g., Minecraft) usually teams up with an infinite number of complex and highly contextualized tasks [16; 29], and it can be fairly challenging to depict them in text only. Therefore, VLA models that solely depend on text to communicate with the text-conditioned policies [47; 46] may fail to correctly pilot these controllers. On the other side, emitting the control command directly [6; 22] without invoking separate controllers could alleviate the aforementioned communication problem but given the long-horizon nature of open-world tasks, it is less practical to perform long-term control with a large VLA model as the context length requirement, computation cost and inference efficiency could become unaffordable.

In this paper, we aim to tackle the aforementioned issues of existing VLA models when facing open-world environments: **complex & context-dependent tasks** and **long-term tasks**. Our **key insight** originates from the observation of human decision-making: Given these open-world tasks, humans can make informed decisions via multi-round mental, verbal, and physical interactions (an illustration can be found in Figure 1). Therefore, if the VLA model can manage to learn from such interaction data, it may master the underlying human decision-making procedures. However, modeling interaction data is _non-trivial_: it is **multi-modal**, encloses **mission** (mostly observations), **language** (instructions, thoughts, etc.), and **actions** (behavior trajectories). Compared to the fruitful explorations on jointly tokenizing vision and language [31; 3; 43; 1] into sequences for autoregressive modeling , tokenizing behavior trajectories (actions) is hard due to the following reasons. On the one hand, directly using low-level actions from the environment would pose huge challenges to the model's ability to process long sequences, which significantly hurts performance. It also hinders us from leveraging the planning ability of generative models. On the other hand, language-level action tokens require significantly more supervision and cannot accurately describe all possible actions.

To this end, we propose OmniJARVIS, a novel VLA model that jointly models **vision**, **language**, and **actions** in interaction data with unified tokenization. OmniJARVIS comprises two **key ideas**: 1) **Behavior Tokenization.** We introduce a _self-supervised_ approach to learn a behavior encoder

Figure 1: **Illustration of multi-modal interaction data for decision-making. A canonical interaction sequence depicting the human decision-making process starts from a given task instruction and memory, followed by a series of sub-task completion which involves initial observations, chain-of-thought reasoning, and behavior trajectories. Our proposed VLA model OmniJARVIS jointly models the **vision** (observations), **language** (instructions, memories, thoughts), and **actions** (behavior trajectories) as **unified** autoregressive sequence prediction. A _self-supervised_ behavior encoder (detailed in Section 2 and Figure 2) converts the actions into behavior tokens while the other modalities are tokenized following the practices of MLMs [31; 3; 1].**

that produces discretized tokens for actions (behavior trajectories) and an imitation learning policy decoder conditioned on these tokens (Section 2); 2) **Autoregressive Modeling.** By augmenting these _behavior tokens_ into the vocabulary of pretrained MLMs, we pack the multimodal interaction data into unified token sequences and learn a transformer on these sequences with an autoregressive modeling objective. We conduct comprehensive evaluations in the open-world Minecraft Universe . OmniJARVIS demonstrates impressive performances on a wide range of atomic, programmatic, and open-ended Minecraft tasks. Our analysis confirms several critical design choices in data formation, tokenization, and the scaling potential of OmniJARVIS. Our contributions are as follows:

* We propose OmniJARVIS, a novel VLA model capable of following instructions to reason, plan, and act in open-world environments by jointly modeling fusion. language, and actions in multimodal interaction data for decision-making.
* We propose a self-supervised approach to learn a behavior encoder to tokenize actions and an imitation learning policy decoder to produce control commands from behavior tokens emitted by OmniJARVIS, allowing joint learning of VLA and smooth action readout.
* We conduct extensive evaluations in open-world Minecraft to demonstrate OmniJARVIS's proficiency across various tasks and present in-depth analyses to reveal valuable insights.

## 2 A Tokenizer for Behaviors

As illustrated in Section 1, a key challenge for VLA is the mismatch between the action modality and other modalities such as the language instructions. A key insight is that a good amount of knowledge about the effects of actions can be learned directly from behavior trajectories \(\{^{(i)}\}_{i}\). We propose to learn a behavior tokenizer in addition to the well-studied vision and language tokenizers to achieve unified tokenization of the fusion. language, and actions in multimodal interaction data (Figure 1). We pose two main requirements to the behavior tokens. First, they should be able to express complete and diverse behavior from (short) trajectories. Further, the tokens should contain semantic information so that they are compatible with the other modalities, which enables the reasoning and planning ability of LLMs (e.g., by conducting chain-of-thought reasoning).

Specifically, we aim at producing a set of \(N\) discrete **behavior tokens**\(s_{1}^{},,s_{N}^{}\) from a behavior trajectory \(=\{o_{0},a_{0},\}\). Further, a de-tokenizer is needed to map these tokens back to an action rollout in the environment that reproduces the goal achieved in \(\). GROOT  explores a VAE-based approach to jointly learn a latent representation of behavior trajectories and an imitation learning policy decoder that conditions the latent as goal. However, the continuous latent cannot be used as the behavior tokens as they can be more difficult to learn and decode with the existing discrete tokens of pretrained MLMs [22; 32]. Therefore, we replace the Gaussian latent in GROOT with an improved vector quantized discrete latent called Finite Scalar Quantization (FSQ) . We adopt a quantization configuration of \(\), which means a code with a length=5 and a codebook size of \(8 8 8 6 5=15360\) is produced. The configuration is selected by a simple grid search. Overall, the behavior tokenizer (behavior encoder) \(e_{}(o_{1;T})\) and the de-tokenizer (IL policy decoder)

Figure 2: **Self-supervised learning for behavior tokenizer of OmniJARVIS. We modify the VAE-based self-supervised learning of behavior trajectories in  to train the behavior tokenizer and de-tokenizer in OmniJARVIS. Specifically, we adopt the auto-encoding objective but replace the Gaussian latent with a discrete representation based on Finite Scalar Quantizer . The encoder will then be used as the behavior tokenizer to produce discrete tokens from the actions (behavior trajectories) in multimodal interaction data, while the behavior tokens emitted by OmniJARVIS will be sent to the policy decoder to perform motor control.**

\(_{}(a_{t}|o_{1:t})\) is learned with the following objective:

\[}\ _{}[ _{t=1}^{T}-_{}(a_{t}|o_{1:t},f(e_{}(o_{1:T})))],\] (1)

where \(f()\) denotes the finite scalar quantizer. We choose a non-causal (bidirectional) transformer and a causal transformer to parameterize the encoder \(e_{}(o_{1:T})\) and the policy decoder \(_{}(a_{t}|o_{1:t})\), respectively. In practice, we set \(T=128\) as the trunk size of the behavior trajectory to be encoded. We will discuss how to handle trajectories longer than 128 in the next section.

Compared to our behavior tokenization, most prior work in VLA models, either represents the behavior trajectories in interaction data as a textual goal description and invokes a separate goal-conditioned controller [47; 46], or represents the state-action sequence \(\{o_{0},a_{0},\}\) directly as in Decision Transformers (DT) [11; 22; 38; 6]. Our approach offers a more compact but still informative representation of the actions part in multimodal interaction data. Moreover, the action readout, _i.e_. simply sending the behavior tokens to the policy decoder, is also more efficient than the DT-style direct control from VLA models [38; 6; 22].

## 3 Multimodal Interaction Data and OmniJARVIS

As illustrated in Figure 1, canonical multimodal interaction data comprises fusion (observations), language (instructions, memories, thoughts), and actions (behavior trajectories). However, it can be difficult to directly collect such interaction data from human annotators. Therefore, we propose to convert an existing Minecraft gameplay dataset  into the multimodal interaction data required by OmniJARVIS. We begin with a formal definition of the interaction data, followed by our approach for data conversion and augmentation from existing datasets, and finish up with the architecture, formulation of learning on such interaction data, and inference procedure of OmniJARVIS. An overview of OmniJARVIS architecture and inference can be found in Figure 3.

### Data Formation

An interaction sequence of decision-making \(=\{D_{t}\}_{t=0}^{T}\) comprises \(T\) segments. Each segment \(D_{t}\) can be a sentence of text words \(\{w_{i}\}_{i=1}^{N}\), _i.e_. the language part such as instructions \(D_{t}^{}\), memory \(D_{t}^{}\) or thoughts \(D_{t}^{}\). \(D_{t}\) can also be an image \(I\), _i.e_. the fusion part such as observations \(D_{t}^{}=I\). Finally, \(D_{t}\) may belong to the action (behavior trajectory) part, _i.e_. \(D_{t}^{}=\{o_{0},a_{0},\}\). We assume these segments follow the ordering below (Figure 1):

\[^{},D_{1}^{}}_{}, ^{},D_{3}^{},D_{4}^{}}_{ {sub-task 1}},^{},D_{6}^{},D_{7}^{}}_{ },\] (2)

We tokenize such a sequence of segments into a series of tokens \(\{s_{0},,s_{M}\}\) using the vision and language tokenizer offered by a pretrained MLM and the behavior tokenizer introduced in Section 2.

Figure 3: **Architecture and Inference of OmniJARVIS. The main body of OmniJARVIS is a multimodal language model (MLM) augmented with additional behavior tokens. Given a task instruction, initial memory, and observation, OmniJARVIS will iteratively perform chain-of-thought reasoning and produce behavior tokens as a means of control via the decoder policy (behavior de-tokenizer). Every 128 steps, OmniJARVIS is forced to reason again and produce new behavior tokens with the latest observation. (Not shown above) OmniJARVIS can also make textual responses, _e.g_. answering questions.**

### Preparing Multimodal Interaction Data

In reality, many segments of the multimodal interaction \(\) can be missing in public datasets. We consider the Minecraft contractor data released by OpenAI  and it only contains behavior trajectories \(D_{t}^{}\). Therefore, we need to properly augment the data with the additional textual segments including instructions \(D_{t}^{}\), memory \(D_{t}^{}\), and thoughts \(D_{t}^{}\). We follow the prior practices [22; 31] to synthesize the required text using LLMs. Below, we detail how each type of segment is constructed. More details can be found in _appendix_.

**Synthesis of instruction \(D_{t}^{}\).** The instruction is a high-level description of what task is being performed in the current interaction sequence. The considered OpenAI Minecraft data includes _meta information_ of each gameplay video, which depicts fundamental events that happened during in Minecraft gameplay, _e.g_. what block was just destroyed, what entity was just killed, what item was just crafted, _etc_. Such meta-information can provide a basic overview of what the player has been through in the gameplay. We therefore prompt an LLM into summarizing the gameplay with the meta information. The summary will be used as the instruction \(D_{t}^{}\) of the current trajectory.

**Synthesis of memory \(D_{t}^{}\).** The memory is the summary of what agents have finished in the previous interaction sequences. Due to the limited sequence length that the auto-regressive model can handle, the model needs to learn to summarize key information related to the task in historical interactions and ignore behaviors unrelated to instructions. The memory will be updated based on the results of each episode trunk and used for subsequent episode trunks. We therefore prompt an LLM into summarizing the gameplay with the meta information. The summary will then be used as the memory \(D_{t}^{}\) of the current interaction trajectory. The memory prompt can be found in Appendix F.

**Synthesis of thought \(D_{t}^{}\).** The thought is the agent's reasoning and explanation of its own decisions. Previous methods have confirmed that using thought-enhanced interaction data helps language models understand decision-making . Compared to labeling thoughts by humans [50; 6], we assume that thought is an intermediate variable that can be determined by the actions taken and observations made before and after the action, which is similar to an Inverse Dynamics Model . We therefore prompt an LLM into estimating the thought of decisions with in-context learning, which will then be used as the thought \(D_{t}^{}\) of the current behavior. Details can be found in Appendix E.

### Architecture, Training, and Inference of OmniJARVIS

As illustrated in Figure 3, OmniJARVIS is built upon a pretrained MLM. We augment the original vocabulary of the MLM with additional tokens from the behavior tokenizer. Specifically, as we adopted the \([a,b,c]\) FSQ configuration (Section 2), we augment with \(a+b+c\) new tokens as each behavior comprises \(n\) behavior tokens \(s_{1}^{bhw},,s_{n}^{bhw}\) corresponding to \(n\) FSQ levels. We formulate the learning objective of OmniJARVIS following [7; 37] in a prefix language modeling fashion. For a batch \(\) of token sequence \(s\), we optimize OmniJARVIS via:

\[(,)=-_{b=1}^{||}_{t=1}^{T}  p_{}(s_{}^{(b,t)}|s_{}^{(b,<t)},s_{}^{(b,1)},...,s_{}^{(b,L)}),\] (3)

where \(s_{}\) denotes the prefix token, which is tokenized from the segments that served as context for reasoning and decision-making, _i.e_. instruction \(D_{t}^{}\), memory \(D_{t}^{}\) and observation \(D_{t}^{}\) within the interaction sequence (Equation 2). The remaining tokens (tokenized from thought \(D_{t}^{}\) and behavior trajectory \(D_{t}^{}\)) will be predicted in an autoregressive fashion. From a high level, OmniJARVIS is trained to reason (producing thought tokens) and act (producing behavior tokens) from contexts with task instructions, memory, and current observations. During inference, we begin with the feeding OmniJARVIS with a task instruction, an empty memory, and an initial observation. OmniJARVIS will produce a chain-of-thought as a means of reasoning and subsequently, emit behavior tokens for control. Every \(N\) steps, it is forced to reason again to produce new behavior tokens with the latest observation. We empirically set \(N=32\).

## 4 Capabilities and Analysis

### Overview

**Training details and Datasets.** The training of the OmniJARVIS is divided into two stages. In the first step, we use a self-supervised training method to train a Behavior Tokenizer, including the Encoder and Decoder jointly. We use FSQ as a quantization method and build a codebookwith 8*8*8*6*5 discrete codes. The training data for Behavior Tokenizer comes from Contractor Dataset , which is a collection of Minecraft gameplay videos. The training parameters and details remain consistent with GROOT, which can be found in Appendix A.

In the second stage, we use this behavior tokenizer to process Minecraft offline trajectories to obtain behavior token sequences. We add 35 (8+8+8+6+5) additional tokens to the MLM tokenizer as behavior tokens for unified representation, so each time the VLA needs to output a continuous sequence of 5 tokens to represent a complete behavior. We use GPT-3.5 to synthesize thought, memory, and instruction to raw offline datasets to build complete interaction data. The specific prompt can be found in Appendix E. These data collectively constitute the embodied instruction-following dataset of OmniJARVIS, including 600k trajectories and about 900M tokens.

The training dataset of OmniJARVIS further includes a large amount of QA data about Minecraft. We generate a large number of seed questions about these texts using web pages on the Minecraft wiki. Then, we use the self-instruct method to generate a large number of creative questions and instructions. This constructed QA dataset consists of 300k conversations with about 90M tokens. During the training process, the QA data and instruction-following data are mixed, with a total of about 1T tokens, to train OmniJARVIS. In specific, we SFT (supervised finetune) LLAVA-7B . The details can be found in Appendix A. To further demonstrate the generalizability of the method, we also fine-tune LLAVA at different scales and VLM Fuyu-8B with different architectures. The relevant results are presented in Section 4.5 and Section 4.6.

**Experimental Setups.** We conduct experiments in the complex and open-world environment of Minecraft, a voxel-based 3D video game that has garnered significant attention from real-life research due to its popularity and diverse mechanics [18; 16]. We first evaluate OmniJARVIS with atomic tasks, which are skill-level tasks, testing VLAs' ability to follow simple and straightforward instructions. Then we evaluate OmniJARVIS with programmatic tasks, which require the agent to obtain an item starting from an empty inventory. The success of these tasks requires VLAs to decompose the provided instruction into atomic-level subtasks, and hence tests VLAs' complex reasoning ability. Finally, we test OmniJARVIS with open-world embodied question-answering benchmarks and creative free-form instruction-following. We also conduct ablation experiments of OmniJARVIS with different behavior tokenizers, different training dataset formats, and different vision tokenizations. Finally, we explore the generalization abilities of OmniJARVIS of Atari Games and the scaling potential of OmniJARVIS with different models and data scales.

### Main Results I: Short-horizon Atomic Tasks

Atom tasks are various simple skills that agents in Minecraft need to master. They are basic tasks yet are fundamental skills that agents need to master during the learning process. We first evaluate OmniJARVIS with our learned behavior tokenizer on these tasks.

We select "chopping trees", "digging dirt", "ming stones", and "collecting wheat seeds" as the evaluation tasks. We directly take those short task descriptions as instructions for agents. We use text-conditioned VPT , Open-world Control , STEVE-I , and video-instructed GROOT  as baselines. We compute the average rewards of different agents on every task in Table 1 across 10 runs. By observing the environment and adjusting action tokens dynamically, OmniJARVIS effectively follows straightforward instructions across various scenarios. It consistently achieves a high average reward with minimal standard deviation.

### Main Results II: Long-horizon Programmatic Tasks

To further verify the ability of OmniJARVIS to complete tasks with long sequences, we use 30 programmatic tasks to evaluate the performance of different agents. These tasks require the agent to start from an empty inventory in a new world until obtaining the final required items, which is usually a chain of atom tasks. These tasks are divided into five groups based on difficulty: wooden, food, stone, iron, and diamond. For example, the prompt for task "Obtain a diamond pickaxe"  is "Give you nothing in the inventory, obtain a diamond pickaxe." This task requires more game time and more complex planning for up to 10 different intermediate items . We list all programmatic tasks and its corresponding instructions in the Appendix C.1.

**Baselines** are divided into two types: 1) directly outputs actions, namely the native behavior tokenizer, including STEVE-I  and GROOT . 2) using pretrained LLM as a planner to output language goals and connect the STEVE-I to execute these goals, including Zero-Shot Planner (GPT) ,

[MISSING_PAGE_FAIL:7]

**Vision Tokenizer.** We also evaluate training OmniJARVIS with different behavior tokenizers, including the including ImageNet Captioner + LLAMA2-7B  (basically converting the vision input into textual captions), fuyu-8b , and LLAVA-7B  architecture. For the ImageCaptioner+, we fix the ImageCaptioner models and only fine-tune the language model, i.e., LLAMA2-7B. We use the prediction loss of behavior tokens as the evaluation criterion, namely eval loss. We found that the model trained with LLAVA-7B architecture has the lowest evaluation loss, so we chose this model as the default model.

**Behavior Tokenizer.** We explore OmniJARVIS with different behavior tokenizers, including the default setting using FSQ codebook, a variant of using VQ-VAE instead of FSQ , and simply using sub-goal language annotation as behavior "tokens". The evaluation results on 4 programmatic tasks are listed in Table 4. Using an FSQ tokenizer is generally better than a language goal, which confirms the advantages of using a tokenized behavior over language descriptions of behavior. The use of VQ-VAE as a quantized behavior tokenizer collapsed during the training process, so there were no results in all test tasks.

**Behavior Codebook.** We conduct an in-depth investigation of behavior tokenizers with varying codebook sizes, utilizing recommended sets of FSQ levels to approximate specified codebook dimensions  as delineated in Table 5. We evaluate performance across multiple metrics for each codebook size. **Codebook Usage** is quantified as the proportion of codewords utilized at least once when encoding the validation datasets. **Reconstruction FSD** is measured by the FSD scores derived from the MineCLIP encoder , processing 1,000 different demonstration videos through the FSQ-GROOT and subsequent rollout in a randomly generated environment. Additionally, we measure **Resampling FSD**, which is the FSD score obtained when the environment rollout is conditioned on representations sampled from the codebook. Finally, we assess the **average rewards** for the task "collect wood" using OmniJARVIS across varying codebook sizes. Our findings indicate that increases in codebook size correlate with enhanced average rewards and reduced FSD scores, suggesting a scalable performance in OmniJARVIS with larger codebooks.

**Behavior Semantics.** We provide some qualitative analysis on the learned FSQ-based behavior tokenizer. In Figure 6, we tokenize several reference videos, then feed the behavior tokens to the policy decoder and see if it can accomplish the same task as in reference videos. The results indicate that our behavior tokenizer is able to capture such behavior semantics and offers rich task information.

### Generalization and Scaling Potential of OmniJARVIS

We first explore adapting OmniJARVIS to the Atari game Montezuma's Revenge. We created a dataset from 500 episodes played by an agent trained with Random Network Distillation ,

    Dekivor \\ Tokenizer \\  &  Vision \\ Tokenizer \\  &  \\ Instruction \\  &  Dataset Format \\ Caption \\  &  \\ Thought \\  &  \\ Memory \\  & 
 Loss \\ Train \\  \\   &  & & ✓ & ✗ & ✗ & \(0.33\) & \(0.67\) \\  & & ✓ & ✗ & ✗ & \(0.46\) & \(0.51\) \\  & & ✓ & ✓ & ✗ & \(0.44\) & \(0.48\) \\  & & ✓ & ✓ & ✗ & \(0.32\) & \(0.33\) \\  & & ✓ & ✓ & ✓ & \(0.16\) & \(0.17\) \\   & Captioner+ & ✓ & ✓ & ✗ & ✗ & \(0.49\) & \(0.52\) \\  & FUVU & ✓ & ✓ & ✗ & ✗ & \(0.42\) & \(0.44\) \\    & & ✓ & ✓ & ✓ & \(0.44\) & \(0.48\) \\    & & ✓ & ✓ & ✓ & - & - \\    & & ✓ & ✓ & ✓ & - & - \\   

Table 5: Ablation experiments on behavior tokenizer with different code vocabulary size.

Figure 4: Ablation experiments on OmniJARVIS with different behavior tokenizers, vision tokenizers, and training on different interactive datasets. The first line is training on the unconditional interactive dataset, i.e., without instructions on the trajectories. OmniJARVIS with VQ-GROOT  shows no results because of training collapse.

Figure 5: **Scaling potential of OmniJARVIS. Its evaluation loss continues to drop with the growth of data and model parameters. The Pearson coefficients for the 2B, 7B, and 13B models are 0.9991, 0.9999, and 0.9989.**supplemented by random actions in early frames to enhance diversity. This dataset contains 1,823,699 transitions. We then trained the FSQ-GROOT tokenizer on this new dataset and subsequently trained OmniJARVIS on the tokenized data. The finetuned OmniJARVIS achieved a score of 3600 in Montezuma's Revenge, indicating promising transferability. A rollout trajectory is in Figure 7.

We also investigate the scaling effect [25; 30] of data and model in OmniJARVIS by monitoring the instruction-following loss on the validation set as the amount of data increases. In addition to fine-tuning from the default LLaVA-7B, we include two additional scales: OmniJARVIS-2B (fine-tuned from LLaVA-2B with Gemma-2B language models ) and OmniJARVIS-13B (fine-tuned from LLaVA-13B with LLaMA2-13B language models ).

The validation loss curves in Figure 5 reveal the following insights: 1) When using Omni-Tokenizer, OmniJARVIS's instruction tuning aligns with the scaling law . All curves exhibit a log-linear decrease as the data scale increases. 2) Scaling up VLM consistently enhances performance. Notably, OmniJARVIS-7B demonstrates significantly lower losses compared to OmniJARVIS-2B. However, while improvements are consistent, the difference between OmniJARVIS-7B and OmniJARVIS-13B seems less pronounced, hinting at potential saturation when further scaling up VLM. This underscores both the scalability of OmniJARVIS and the importance of increasing data volume to match the model.

## 5 Related Works

**Pretrained Language Models for Decision-making.** Several works have explored leveraging LLMs to generate action plans for high-level tasks in embodied environments [23; 27; 5; 52]. To better perform complex planning in the environment, existing methods usually utilize chain-of-thought  or related methods . To better cope with uncertainties in open worlds, some LLM-based methods generate plans interactively with human and environmental feedback [39; 46; 24] and retrieving from memory  or internet corpus . However, those plans can only be executed in a language environment or require an additional controller or code executor to interact in an open world.

**Vision-Language-Action Models.** In order to better utilize the knowledge inside the language model for decision-making, some methods tend to use decision datasets to fine-tune pretrained language models [15; 14]. Gato  was among the first to tokenize environment-provided actions to enable joint sequential modeling across modalities. PaLM-E  generates high-level instructions as texts and uses dedicated controllers to perform the task described by the output instructions. The RT series focuses more on robotics settings. Specifically, RT-1 pairs a VLM with a language-conditioned controller; RT-2 extends the VLM to directly include control tokens; RT-X generalizes to new robots and environments. A recent VLA model LEO  expands the perception from 2D images to 3D world and enables rich scene-level reasoning and control tasks.

Figure 6: **Examples of behavior tokenization-detokeinzation. Left: the reference video to be tokenized by our FSQ-based behavior tokenizer (encoder). Right: the behavior of the policy decoder is conditioned on the behavior tokens. The policy decoder can reproduce the task being accomplished in the reference video.**

Figure 7: **OmniJARVIS plays Montezuma’s Revenge and gets a reward of 3600.**

**Open-world Agents in Minecraft.** As LLMs have achieved remarkable reasoning results and understanding capabilities across various domains, the year 2023 has witnessed researchers adopting multiple LLM-based approaches to create open-world agents in Minecraft [46; 55; 47; 44]. Some methods focus on building policies for low-level skills [10; 28; 2]. Building upon the low-level policies to interact with the Minecraft environment, Wang et al. , Yuan et al.  and Wang et al.  focus on leveraging the pre-trained language models as planners to finish programmatic tasks with in-context learning. Wang et al.  adopts the life-long learning scheme and generates code as policies to enable continual exploration. Some use expert trajectories and Minecraft corpus to fine-tune pre-trained vision language models for better embodied planning [36; 54].

## 6 Conclusion

We've presented OmniJARVIS, a novel VLA model that encompasses strong reasoning and efficient decision-making capabilities via unified tokenization of fusion, language, and actions in multimodal interaction data. The key ideas are learning behavior tokenizer (trajectory encoder) and de-tokenizer (IL policy decoder) using self-supervised learning on behavior trajectories and autoregressive modeling of tokenized multimodal interaction data using a pretrained multimodal language model (MLM). Evaluations on the open-world Minecraft Universe demonstrate its impressive instruction-following capabilities. Possible future directions include a more in-depth investigation of behavior tokenization, language capabilities after VLA fine-tuning, and alignment concerns emerging from the unified interaction modeling and VLA capabilities.