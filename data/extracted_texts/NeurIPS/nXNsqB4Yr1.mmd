# Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices

Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices

Kilian Pfeiffer

Karlsruhe Institute of Technology

Karlsruhe, Germany

kilian.pfeiffer@kit.edu

&Ramin Khalili

Huawei Research Center Munich

Munich, Germany

ramin.khalili@huawei.com

&Jorg Henkel

Karlsruhe Institute of Technology

Karlsruhe, Germany

henkel@kit.edu

###### Abstract

Federated learning (FL) is usually performed on resource-constrained edge devices, e.g., with limited memory for the computation. If the required memory to train a model exceeds this limit, the device will be excluded from the training. This can lead to a lower accuracy as valuable data and computation resources are excluded from training, also causing bias and unfairness. The FL training process should be adjusted to such constraints. The state-of-the-art techniques propose training subsets of the FL model at constrained devices, reducing their resource requirements for training. However, these techniques largely limit the co-adaptation among parameters of the model and are highly inefficient, as we show: it is actually better to train a smaller (less accurate) model by the system where all the devices can train the model end-to-end than applying such techniques. We propose a new method that enables successive freezing and training of the parameters of the FL model at devices, reducing the training's resource requirements at the devices while still allowing enough co-adaptation between parameters. We show through extensive experimental evaluation that our technique greatly improves the accuracy of the trained model (by \(52.4\) p.p.) compared with the state of the art, efficiently aggregating the computation capacity available on distributed devices.

## 1 Introduction

Federated learning (FL) has achieved impressive results in many domains and is proposed for several use cases, such as healthcare, transportation, and robotics . As data in FL is not processed centrally but usually on _resource-constrained_ edge devices, training machine learning (ML) models impose a large computational burden on these devices . Additionally, FL requires communication, specifically exchanging ML model parameters from the devices to a centralized entity for aggregation. Extensive research has been done to lower the communication overhead required for FL, e.g., on the use of quantization in the communication  or sketched updates . Similarly, techniques such as partial updates , asynchronous aggregation , and tier-based aggregation  have been proposed to lower and account for varying computational throughput. While constrained computation throughput and communication capabilities can slow down FL convergence, high memory requirements for training that are imposed on devices can exclude devices completely from the FL system. This is, for example, the case in _Google GBoard_, where devices that do not have 2GB of memory for training are removed. Excluding devices from training lowers the reachable accuracy, as fewer devices participate in the training, also resulting in bias and unfairness .

Several techniques have been proposed to tackle these constraints, where the main idea is to train a lower complexity _submodel_ on the devices and embed the trained submodel into the full higher-capacity server model. A submodel is typically created by _scaling the width_ of the neural network (NN), e.g., using a subset of convolutional filters per NN layer. There exist several variations of the technique [9; 18; 19; 20]. In particular, Caldas _et al_.  propose _Federated Dropout (FD)_, which randomly, per round and per device, selects NN filters that are trained. Alam _et al_.  propose _FedRolex_, a sliding window approach, where all devices train the same submodel, and in each FL round, the used filter indices are shifted by one. While both these techniques allow training within given memory constraints, our results show (Fig. 1) that they perform worse than a straightforward baseline, i.e., using a _smaller_ NN model that can be trained by all devices end-to-end. We evaluate CIFAR10, FEMNIST, and TinyImageNet in an FL setting using ResNet and scale the width of the NN down s.t. we achieve a \(2-8\) reduction in training memory. We observe that training the same small model at all devices outperforms FedRolex and FD w.r.t to the final accuracy and convergence speed (we expect similar results for other subset-derived techniques), especially when enforcing a \(4\) and \(8\) memory reduction (as also evaluated in ).

Our results indicate that applying these techniques is rather harmful. This is as a large part of filters/parameters has to be dropped during each round of training at each device, extremely limiting the co-adaptation between parameters. Hence, the gradients for the subset of parameters that are trained on devices are calculated without considering the error of the parameters that reside on the server (more details in Appendix A). Motivated by these observations, we propose a new technique that enables successive freezing and training of the parameters of the FL model at devices, reducing the training's resource requirements at the devices while allowing a higher co-adaptation between parameters. Instead of switching between subsets of the model on an FL-round basis, we train the same parameters for several rounds and successively switch to a larger model. To obey the same memory constraints as in [9; 20], we train early layers using the full width while utilizing a scaled-down NN head. We then freeze the early layers and expand the head layers' width. By freezing early layers, no activation has to be kept in memory, hence, we lower the memory footprint. But still, the error of these frozen parameters is included in the calculation of the gradient of the new subset of parameters. We apply this technique successively till all parameters of the model are trained.

In summary, we make the following novel contributions:

* We empirically show that employing current state-of-the-art techniques, FD  and FedRolex , for memory-constrained systems can actually hurt the performance.
* We propose a novel training scheme called _Successive Layer Training (SLT)1_, which addresses the shortcomings of previous techniques by successively adding more parameters to the training, successively freezing layers, and reusing a scaled-down NN head. * Our evaluation of common NN topologies, such as ResNet and DenseNet, shows that SLT reaches significantly higher accuracies in independent and identically distributed (iid) and non-iid CIFAR, FEMNIST, and TinyImageNet training compared to the state of the art. Also, SLT provides a much faster convergence, reducing the communication overhead to reach a certain level of accuracy by over \(10\) compared with FD and FedRolex. The same

Figure 1: Accuracy of FedRolex  and FD  compared to a small model using different NN topologies and datasets after \(2500/1000\) FL rounds. In the case of \(1\), both techniques are equivalent to vanilla Federated Averaging (FedAvg). Hyperparameters of the experiments are given in Section 3.

behavior can be observed w.r.t. Floating Point Operations (FLOPs), where SLT requires \(10\) fewer operations to reach a certain level of accuracy compared with FD and FedRolex.
* We study the performance of SLT in heterogeneous settings. We show that devices with different memory constraints can make a meaningful contribution to the global model, significantly outperforming the state-of-the-art techniques.

## 2 Methodology

### Problem Statement and Setup

We consider a synchronous cross-device FL setting, where we have one _server_ and a set of _devices_\(c\) as participants. There is a given ML model topology \(F\) on the FL server that is trained in a distributed manner for \(R\) rounds. Our goal is to maximize the accuracy of the model. Similar to FD and FedRolex, we assume that a fixed number of devices \(|^{(r)}|\) out of \(\) participate in a round \(r R\). All devices are constrained in memory, and thus their training must not exceed this given memory constraint \(m_{}\). In other terms, we assume that no participating device can train the server NN model end-to-end.

### Successive Layer Training

The following describes our methodology of _Successive Layer Training_ for convolutional neural networks (CNNs). Firstly, we rewrite \(F\) such that it is the consecutive operations of \(K\)_layers_, where each layer is defined as \(f_{k}\), \(k[1,,K]\). Each layer \(f_{k}\) has associated server parameters \(W_{k}\). We label a convolution, followed by batch normalization and an activation function, a layer. Similar to [9; 20], we define a _subset_\(w_{k}\) of the layer parameters (server) \(W_{k}\) that is _scaled_ down using \(s\) as

\[w_{k}=W_{k}^{s,s} w_{k}^{ sP_{k}  sM_{k}} W_{k}^{P_{k} M_{k}},\] (1)where \(P_{k}\) labels the layer's input dimension, \(M_{k}\) labels the output dimension of the fully-sized server parameters, and \(s(0,1]\) is a scaling factor (we omit the filter kernel dimensions for brevity). To obey the memory constraint on the participating devices, we split the NN into three consecutive parts. The first part of the NN contains layers that are already trained and remain frozen on the devices. The second part contains layers that are being fully trained on the devices. The last part represents the NN's _head_. To train the remaining layers within the memory budget, the head's parameters are scaled down. Throughout the FL training, we successively switch the _training configuration_, s.t., the part of the NN that is being fully trained (\(s=1\)) moves from the first layer to the last layer. Thereby, successively, the remaining parameters from the scaled-down head are added. At the same time, we successively freeze more layers, starting with the first layer, to stay within the memory budget. We visualize the switching from one training configuration to the next in Fig. 2. The parts of the NN that are frozen, trained, and represent the head are labeled \(F_{F}\), \(F_{T}\), and \(F_{H}\). The resulting model that is trained on the devices can be described as \(F=F_{F} F_{T} F_{H}\):

* The first part \(F_{F}\) labels the part of the NN that is frozen and where the server parameters \(W_{1},,W_{K_{F}}\) do not get updated by the devices. Freezing the first part of the NN reduces the memory overhead during training, as in the forward pass, activations do not have to be stored for frozen layers. The frozen part of the NN is defined as \(F_{F}:=_{k\{k:0<k K_{F}\}}f_{k}\), where layers \(1,,K_{F}\) remain frozen.
* The second part \(F_{T}\) labels the part of the NN that is being fully trained by the devices. The parameters \(W_{K_{F}+1},,W_{K_{T}}\) get updated during training. This part is defined as \(F_{T}:=_{k\{k:K_{F}<k K_{T}\}}f_{k}\), s.t. layers \(K_{F}+1,,K_{T}\) are fully trained.
* The last part \(F_{H}\) describes the NN's head that is scaled down using \(s\), s.t. \(F_{H}:=_{k\{k:K_{T}<k K\}}f_{k}\), where the scaled-down layers \(K_{T}+1,,K\) are trained. The first layer of \(F_{H}\) scales down the parameters to the width of the head s.t. \(w_{K_{T}+1}=W_{K_{T}+1}^{1,s}\), where \(W_{K_{T}+1}^{1,s}^{P_{K_{T}+1}[sM_{K_{T}+1}]}\). All consecutive layers are scaled down using \(s\), s.t. \(w_{K_{T}+j}=W_{K_{T}+j}^{s,s} j[2,,K-K_{T}]\).

We define a set \(S\) as a training _configuration_ that obeys the given memory constraint, s.t. \(S=\{K_{F},K_{T},s\}\) fully describes the mapping of layers \(f_{k},k[1,,K]\) into \(F_{F}\), \(F_{T}\), \(F_{H}\), and the head's scale factor \(s\). Each \(S\) has a respective memory footprint during training. \(m=(S)\) denotes the maximum memory that is utilized during training for a given configuration \(S\). The maximum memory of a configuration can be determined by measurements or by calculating the size of weights, gradients, and activations that have to be kept in memory (see Section 3.2).

### Configuration Selection

For each selected \(S\), we aim to fully utilize the available memory. We define \(n\) as a _configuration step_ in \(n[1,,N]\), where we add parameters to the training (i.e., _fill up_ the remaining parameters of a head's layer). These steps are distributed over the total training rounds \(R\) ( Fig. 3). We set \(K_{T}=K_{F}+1\), s.t. in each configuration step exactly one layer gets fully trained (filled up). We start with \(K_{F}=K_{T}=0\) (consequently \(F=F_{H}\)) to pre-train the head for a certain number of rounds. After pre-training, we increase \(K_{F}\) by one and apply \(K_{T}=K_{F}+1\) (the first configuration has no

Figure 2: Visualization of SLT. With each step, \(K_{F}\) and \(K_{T}\) are shifted by \(1\). \(W_{1},,W_{K_{F}}\) denote the parameters that remain frozen during training, \(W_{K_{T}}\) denotes parameters of layer \(K_{T}\) that are fully trained, while \(W_{K_{T}+1}^{1,s},,W_{K}^{s,s}\) denote the parameters of the scaled-down head using \(s\).

frozen layers, i.e., \(F=F_{T} F_{H}\)) and continue training2. We switch to the successive configuration by increasing \(K_{F}\) by one. Hence, for the training configuration at step \(n\), we have \(K_{F}=n-1\) and \(K_{T}=n\), with \(s_{n}\) selected as follows:

\[s_{n},(S_{n}) m_{} s_{n} s_{j}  j[n+1,,N],\] (2)

Equation (2) ensures that each configuration obeys the constraint \(m_{}\). The second constraint in Eq. (2) enforces that \(s_{n}\) can only grow with increasing \(n\) to ensure that parameters of the head are only added throughout the training but not removed. We provide a justification for maximizing \(s\) instead of \(F_{T}\) by performing an ablation study in Appendix B. The configuration selection is performed _offline_. Lastly, we define the last step \(N\) where a given memory constraint (memory\((S_{N})\)) allows for \(s=1\). If this step is reached, we train with \(S_{N}\) for all remaining rounds since the memory budget allows to fully train all remaining parameters at once. We provide a visualization of the training process in Fig. 3 and outline SLT in Algorithm 1.

## 3 Experimental Evaluation

### Experimental Setting and Hyperparamters

We evaluate SLT in an FL setting using PyTorch , where we distribute a share from the datasets CIFAR10, CIFAR100 , FEMNIST from the Leaf  benchmark, and TinyImageNet  to each device \(c\), s.t. each device \(c\) has a local dataset \(_{c}\) of the same size. In each round \(r\), a subset of devices \(^{(r)}\) is selected. We train with the optimizer stochastic gradient descent (SGD) with momentum of \(0.9\), an initial learning rate of \(=0.1\), and apply cosine annealing to \(=0.01\) and a weight decay of \(1.0 10^{-5}\). We evaluate the vision models ResNet20, ResNet44 , and DenseNet40 . For each experiment, we report the average accuracy and standard deviation of \(3\) independent seeds after \(R\) rounds of FL training. For CIFAR10, CIFAR100, and TinyImageNet, we evaluate a scenario with \(||=100\) devices, where each round \(|^{(r)}|=10\) devices are actively participating. For FEMNIST, we evaluate with \(||=3550\) and \(|^{(r)}|=35\). We train for \(R=2500\) rounds for CIFAR10, CIFAR100, and TinyImageNet, and \(R=1000\) for FEMNIST. In each round \(r\), each participating device iterates once over its local dataset. We apply standard image augmentation techniques like random cropping and horizontal and vertical flips to all datasets (horizontal and vertical flips are omitted for FEMNIST). An input resolution of \(3 32 32\) is used for CIFAR and FEMNIST (up-scaled from \(28 28\)) and \(3 64 64\) for TinyImageNet. We use batch size \(32\) and perform \(363\) experiments in total, with an average run-time of \(6\,\) on an NVIDIA Tesla V100.

**Comparison with the state of the art:** We compare SLT against several baselines. We introduce \(I_{k}^{(r)}\) as the set of indices of the output dimension of a layer \(k[1,,K]\) where \(r[1,,R]\) denotes the rounds. Consequently, for full-sized NNs \(|I_{k}^{(r)}|=M_{k}\). The subset for training is scaled down by building a dense matrix using the indices from \(I_{k}^{(r)}\), s.t. the scaled-down parameters are \(w_{k}^{ sP_{k} sM_{k}}\). The consecutive layer's input dimension indices are equal to the last layer's output indices. The first layer's input dimension is not scaled to feed all color channels into the NN.

**Small model**: Devices train a submodel where all filters per layer are scaled down by \(s\), s.t. all devices can train the submodel. The remaining filters are trained end-to-end throughout the rounds.

Figure 3: Visualization of the SLT training scheme with an exemplary \(5\)-layer NN. The model is first pre-trained for \(r_{p}\) rounds. Following that, the model is trained for \(r_{1}-r_{p}\), \(r_{2}-r_{1}\), and \(R-r_{2}\) rounds in configuration \(1\), \(2\), and \(N(=3)\), respectively.

The same submodel is used for evaluation. The indices of the output dimension of a layer \(k\) are selected using \(I_{k}^{(r)}=I_{k}=\{i:\ 0 i< sM_{k}\}\).

**FedRolex :** FedRolex creates a submodel by scaling the numbers of filters using \(s\). Each device trains the same continuous block of indices. The filter indices trained on the devices are shifted in a rolling window fashion every round. The server averages the trained block and evaluates using the full server model (\(s=1\)). The indices \(I_{k}^{(r)}\) are selected with \(=r M_{k}\) using

\[I_{k}^{(r)}=\{,+1,,r+ sM_{k}-1\} &+ sM_{k} M_{k}\\ \{,+1,,M_{k}-1\}\{0,,+ sM_{k}- 1-M_{k}\}&.\] (3)

**FD :** FD creates a submodel by scaling down the number of filters using \(s\). The indices of the filters are randomly sampled per device per round on the server. Hence, the indices \(I_{k}^{(r,c)}\) of a device \(c\) of round \(r\) is a round-based per-device random selection of \( sM_{k}\) indices out of all \(M_{k}\) indices. The server aggregates the device-specific submodels after training and evaluates the full model (\(s=1\)).

### Memory Footprint during Training

The high memory requirements during training can be split into three groups: Firstly, the weights of the NN have to be stored in memory. This is required both for the forward pass and the backward pass. Secondly, for the calculated gradients in the backward pass, the activation maps of the respective layers have to be kept in memory. Lastly, the calculated gradients have to be stored in memory. In state-of-the-art CNNs, the size of the activation map makes up for most of the memory requirements, while the size of the weights only plays a minor role. For ResNet44 and DenseNet40, we measure that activations make up for \( 99\%\) of the required memory for training, while gradients and parameters account for the remaining \(1\%\). Consequently, the required memory linearly reduces with \(s\) for FD and FedRolex, as the number of layer's output channels \( sM_{k}\) determines the activation map's size. Similarly, for SLT, we measure the maximum amount of memory that is required during training by counting the size of the activation maps, as well as the loaded weights and gradients in training. For frozen layers, it is only required to load the parameters in memory, while no activation maps and gradients have to be stored. For the fully trained layer \(K_{T}\), it is required to store the layer's full parameters \(w_{K_{T}}\), as well as the full-size activation map and gradients in memory. For all other layers (NN head), memory scales linearly with \(s\). We provide implementation details in Appendix E.

We evaluate memory constraints that are given by scaling down \(s\) in FedRolex and FD by \(s_{}[0.125,0.25,0.5,1.0]\) for experiments with ResNet and \([0.33,0.66,1.0]\) for DenseNet3. In SLT, for a given \(s_{}\), we adjust \(s_{n}\) for each step \(n\) in the following way

\[s_{n},\ \ \ \ (S_{n})(s_{}) s_{n} s_{j} j[n+1,,N],\] (4)

to ensure that our technique obeys the same constraint as the baselines. If \(s_{}=1.0\), all algorithms coincide with vanilla Federated Averaging (FedAvg) using the full server model.

We distribute the required steps \(N\) over the total rounds \(R\), s.t. all parameters receive sufficient training. Specifically, we distribute the rounds based on the share of parameters that are added to the training within a configuration \(n\). We calculate the number of all trained parameters \(Q\) by using \(K_{F}\),\(K_{T}\), and \(s\) s.t.

\[Q(K_{F},K_{T},s)=<k K_{T}\}}{}P_{k}M_{k} +P_{K_{T}+1} sM_{K_{T}+1}+1<k K \}}{+}+1<k K\}}{+}+1<k K \}}{+}+1<k K\}}{+}+1<k K \}}{+}+1<k K\}}{+}+1<k K \}}{+}+1<k K\}}{+},\] (5)

and use \(Q\) to calculate the share of rounds \(R_{n}\) for a step \(n\). The share of rounds for pretraining is calculated using \(R_{}=R\). For step 1, \(R_{1}=R-R_{}\). For all steps \(n>1\), we calculate the rounds using

\[R_{n}=R)-Q(n-2,n-1,s_{n-1})}{Q(0,0,1)}.\] (6)

Lastly, the switching point for pretraining is \(r_{}=R_{}\) and \(r_{n}=R)}{Q(0,0,1)}\) for all steps \(n\).

Preliminary experiments have shown that this mapping scheme outperforms other techniques, like an equal distribution of rounds to all configurations, and enables SLT to converge as fast as a small model while reaching a significantly higher final accuracy (we provide further results in Appendix C). The mapping of steps \(N\) to rounds \(R\) does not rely on private data (or any data) and can be stored in a look-up table prior to the training. A visualization of SLT is given in Fig. 3. We provide the number of steps \(N\) for different NN architectures and constraints in Appendix C.

### Experimental Results

**lid results:** For the iid case, results are given in Table 1. We observe that SLT reaches significantly higher accuracy for ResNet20 and CIFAR10 for all evaluated constraints, outperforming a small model baseline by up to \(7.8\) p.p. and state of the art by \(52.4\) p.p.. The results with FEMNIST show that a small model baseline already provides sufficient capacity for the dataset since only a few percentage points separate \(s_{}=0.125\) from the full model (i.e., when \(s_{}=1\)). Hence, SLT can only provide a minor benefit over a small model baseline. The contrary can be observed for CIFAR100 and TinyImageNet, where using a small model (\(s_{}=0.125\)) loses up to \(25.4\) p.p. to the full model. Additionally, it can be observed that for low memory constraints, FD and FedRolex fail to learn a useful representation at all. SLT improves upon a small model by up to \(13.7\) p.p. and up to \(26.6\) p.p. compared to state of the art.

**Non-iid results:** Typically, data in FL is not distributed in an iid fashion but rather non-iid. We repeat the experiments shown in Table 1 but distribute the data on the devices in a non-iid fashion. Similar to , we apply a Dirichlet distribution, where the rate of non-iid-ness can be varied using \(\). For all experiments, we set \(=0.1\). We observe from Table 2 that the small model baselines in the case of CIFAR10 and FEMNIST lose accuracy compared to the full model. Hence, the gain of SLT compared to a small model baseline increases. The results for CIFAR100 and TinyImageNet show a proportional drop in accuracy for all algorithms. However, SLT still outperforms other techniques by a large margin. Note that we could apply common non-iid mitigation techniques like FedProx  on top of SLT to further limit the drop in accuracy.

For additional experimental results, we refer the readers to Appendix D.

**Communication, computation, and convergence speed:** We evaluate our technique w.r.t. the communication overhead of the distributed training and the number of computations devices have to perform (FLOPs). Specifically, we evaluate the gain in accuracy over required transmitted data and performed FLOPs. We show the results in Fig. 4. We observe that our technique converges fast, similarly to a small model, while reaching higher final accuracy. Compared to FD and FedRolex, our technique requires significantly less communication to reach the same level of accuracy. Similar behavior can be observed w.r.t. FLOPs.

### Heterogeneous Memory Constraints

Memory constraints in devices can be heterogeneous. We evaluate SLT in such scenarios and compare it against the start of the art. We evaluate with different resource levels and split the available constraint

    &  &  \\   & 0.125 & 0.25 & 0.5 & 1.0 & 0.125 & 0.25 & 0.5 & 1.0 \\  SLT (ours) & **74.1\(\)0.8** & **83.4\(\)0.2** & **85.2\(\)0.6** & & **84.4\(\)0.3** & **85.8\(\)0.1** & **86.9\(\)0.0** & \\ Small model & 66.3\(\)0.3 & 78.2\(\)0.4 & 84.6\(\)0.4 & 87.5\(\)0.6 & 82.3\(\)0.4 & 85.5\(\)0.1 & 86.9\(\)0.0 & \\ FedRolex  & 21.7\(\)1.9 & 61.0\(\)0.5 & 80.6\(\)0.6 & & 42.1\(\)6.0 & 71.4\(\)2.1 & 83.0\(\)0.1 & \\ FD  & 19.0\(\)1.4 & 36.7\(\)1.8 & 71.6\(\)0.4 & & 38.9\(\)3.7 & 70.4\(\)0.2 & 83.4\(\)0.3 & \\  Setting &  &  \\   & 0.33 & 0.66 & 1.0 & 0.125 & 0.25 & 0.5 & 1.0 \\  SLT (ours) & **51.1\(\)0.4** & 53.3\(\)0.6 & & **33.5\(\)0.1** & **40.3\(\)0.5** & **42.3\(\)0.2** & \\ Small model & 43.9\(\)1.5 & **55.9\(\)0.1** & 60.2\(\)0.5 & 19.8\(\)0.3 & 30.6\(\)0.3 & 40.2\(\)0.3 & 45.2\(\)0.1 \\ FedRolex  & 22.2\(\)0.3 & 46.7\(\)0.1 & 60.9\(\)0.2 & 19.8\(\)0.8 & 33.1\(\)0.2 & 45.2\(\)0.1 \\ FD  & 13.5\(\)0.5 & 41.9\(\)1.5 & & 0.9\(\)0.0 & 7.1\(\)0.1 & 25.9\(\)0.6 & \\   

Table 1: Results for iid experiments with ResNet and DenseNet using CIFAR10, FEMNIST, CIFAR100, and TinyImageNet. Accuracy in \(\%\) after \(R\) rounds of training is given.

levels equally upon the devices, i.e., when constraints of \(}}=[0.125,0.25]\) are given, \(50\%\) of the devices train with \(s_{}=0.125\) while the remaining \(50\%\) use \(s_{}=0.25\). SLT supports heterogeneous constraints through the following mechanism: Firstly, devices with the highest constraint perform training as done in the homogeneous case, outlined in Algorithm 1 using the head's scale factor as described in Eq. (2). Devices that are less constrained use the same scale factor \(s_{n}\) per configuration to ensure that all devices train the same number of parameters within a layer. To utilize the remaining available memory, less constrained devices freeze fewer layers, therefore, train more layers at full width. For a given \(K_{T}\) and \(s_{n}\) of a configuration \(S_{n}\), the remaining memory of less constrained devices is utilized by minimizing \(K_{F}\), s.t.

\[K_{F},(S_{n})(s_{}).\] (7)

In addition to FD and FedRolex, we evaluate HeteroFL  and FjORD . Both require that some devices are capable of training the full NN end-to-end, otherwise, some parameters do not receive any updates. In cases where no device can train the server model end-to-end, we reduce the size of the server model such that at least one participating device can fully train the model.

**Small model**: All devices train a small model regardless of their constraints. Scale factor \(s\) is set to the minimum a participating device supports.

**FedRolex**: Similar to the homogeneous case, FedRolex uses a rolling window (Eq. (3)). In heterogeneous cases, devices use a constraint-specific scale \(s_{e}\), to adjust the number of filters \( s_{e}M_{k}\)

    &  &  \\   & 0.125 & 0.25 & 0.5 & 1.0 & 0.125 & 0.25 & 0.5 & 1.0 \\  SLT (ours) & **52.4\(\)0.9** & **69.6\(\)0.6** & **75.5\(\)1.3** & & **81.2\(\)1.6** & **83.0\(\)2.0** & **83.8\(\)1.9** & \\ Small model & 44.7\(\)1.2 & 63.1\(\)0.7 & 73.6\(\)0.6 & & 79.6\(\)0.8 & 82.9\(\)1.1 & 83.3\(\)2.4 & \\ FedRolex  & 15.0\(\)3.7 & 29.8\(\)1.7 & 48.3\(\)2.9 & 80.5\(\)1.3 & 39.4\(\)2.0 & 59.3\(\)2.1 & 78.5\(\)0.5 & \\ FD  & 11.3\(\)0.9 & 10.7\(\)0.6 & 34.9\(\)5.7 & & 15.9\(\)8.2 & 51.0\(\)1.2 & 79.7\(\)1.1 & \\   &  &  \\   & 0.33 & 0.66 & 1.0 & 0.125 & 0.25 & 0.5 & 1.0 \\  SLT (ours) & **45.9\(\)1.4** & **48.4\(\)0.5** & & **28.5\(\)1.2** & **35.1\(\)1.1** & **36.1\(\)0.2** & \\ Small model & 40.5\(\)1.2 & **51.8\(\)0.2** & 55.8\(\)0.5 & 16.9\(\)0.2 & 25.3\(\)0.5 & 34.2\(\)0.4 & 39.0\(\)0.8 \\ FedRolex  & 20.0\(\)0.3 & 42.9\(\)0.4 & & 1.5\(\)0.5 & 12.7\(\)1.3 & 26.1\(\)0.5 & 39.0\(\)0.8 \\ FD  & 7.6\(\)0.1 & 36.9\(\)1.0 & & 0.4\(\)0.1 & 0.6\(\)0.0 & 20.0\(\)1.3 & \\   

Table 2: Results for non-idl experiments with ResNet and DenseNet using CIFAR10, FEMNIST, CIFAR100, and TinyImageNet. Accuracy in \(\%\) after \(R\) rounds of training is given.

Figure 4: Maximum reached accuracy in % over data upload and performed FLOPs for CIFAR10, CIFAR100, and TinyImageNet using DenseNet40, ResNet20, and ResNet44 in an iid training case.

**FD**: Although Caldas _et al._ do not specifically evaluate heterogeneous devices, heterogeneity can be supported straightforwardly by using constraint-specific \(s_{e}\) for scaling down the NN. This extension of FD is also evaluated in FedRolex and FjORD.

**HeteroFL**: In HeteroFL, devices use the same subset throughout the training. To support heterogeneity, devices use a resource-specific scaling factor \(s_{e}\), s.t. for each \(s_{e}\) the indices are selected using \(I_{k}^{(r,e)}=I_{k}^{(e)}=\{i 0 i< s_{e}M_{k}\}\).

**FjORD**: FjORD uses the same indices as HeteroFL, but each device switches between constraint-specific subsets that satisfy the device constraints within a local epoch on a mini-batch level.

**Heterogeneity results**: We repeat the experimental setup as presented in Table 2 for TinyImageNet and CIFAR100, but enforce varying device constraints in the experiments (see Table 3). We observe that SLT outperforms others in all evaluated scenarios. FedRolex can improve upon a small model in some settings, but this is not the case with FD. For FjORD and HeteroFL, we observe that both outperform the small model baseline. Yet, in some cases, both HeteroFL and FjORD have a lower accuracy when utilizing more constraint levels. For HeteroFL, it can be observed that using ResNet44 with 4 constraint levels reaches a lower accuracy than with 3 levels (despite the fact that all devices have higher average resources \([s_{}]\) of \( 0.47\) in the case of \([0.125,0.25,0.5,1.0]\) instead of \( 0.29\) in the case of \([0.125,0.25,0.5]\)). The same can be observed for FjORD with DenseNet40. As both techniques, in principle, use the same subset mechanism as FedRolex and FD, we think that both suffer from supporting more constraint levels that cause less co-adaptation between NN filters.

## 4 Related Work

We cover related work that studies similar problems or employs similar techniques.

**Resource constraints in FL**: Most works on resource-constrained FL target communication. Specifically, the use of quantization and compression in communication [9; 8] and sketched updates  have been proposed to lower the communication burden. Chen _et al._ propose _adaptive parameter freezing_ as they discover that parameters stabilize during training and do not have to be transferred to the server. Another branch of work focuses on reducing communication, computation, and memory requirements by employing only a subset of the full NN on devices. Caldas _et al._ introduce _FD_, a mechanism that creates a device-specific subset by randomly selecting a subset of CNN filters. Diao _et al._ introduce HeteroFL, which allows for heterogeneous constraints by employing fixed subsets of different sizes to the NN and aggregating them on the server. Horvath _et al._ introduce a similar technique (FjORD), with the main difference that in FjORD, each device trains every available subset within its capabilities. Rapp _et al._ propose DISTREAL, a technique that uses varying subsets on a mini-batch level such that devices finish their update on time despite having intra-round changing resources. FedRolex  supports heterogeneity similar to FjORD and HeteroFL but allows for server NN models that are outside of the capabilities of all devices. This is enabled by not training a fixed subset of the NN parameters but by training a rolling window of all parameters that is shifted on a round basis. Beyond subsets, the use of low-rank factorization [30; 31] has been proposed to train NN models. Lastly, Qui _et al._ propose sparse convolutions to lower the resource requirements for training but require special hardware for sparse computations to realize the gains.

**Layer-wise model training:** Layer-wise model training has been proposed in centralized training of CNNs as an alternative to training with end-to-end backpropagation of the error. Hettinger _et al._ introduced a technique that adds CNN layers one at a time during training using auxiliary

   Setting &  &  \\  \(s_{}\) & [0.33, 0.66] & [0.33, 0.66, 1.0] & [0.125, 0.25] & [0.125, 0.25, 0.5] & [0.125, 0.25, 0.5, 1.0] \\  SLT (ours) & **46.4\(\)2.0** & **49.3\(\)1.8** & **30.3\(\)1.2** & **33.0\(\)0.5** & **35.9\(\)0.4** \\ Small model & 40.5\(\)1.2 & 40.5\(\)1.2 & 16.9\(\)0.2 & 16.9\(\)0.2 & 16.9\(\)0.2 \\ FedRolex  & 33.2\(\)0.4 & 43.9\(\)1.3 & 5.4\(\)0.2 & 13.8\(\)1.3 & 23.6\(\)0.7 \\ FD  & 21.2\(\)0.6 & 38.1\(\)0.4 & 0.5\(\)0.1 & 0.6\(\)0.1 & 20.6\(\)1.7 \\ HeteroFL  & 42.2\(\)1.3 & 42.8\(\)0.5 & 20.7\(\)0.7 & 24.1\(\)0.2 & 23.3\(\)0.5 \\ FjORD  & 38.7\(\)0.4 & 36.9\(\)0.4 & 22.4\(\)1.0 & 25.3\(\)0.3 & 27.5\(\)0.8 \\   

Table 3: FL with heterogeneous constraints. Accuracy in \(\%\) after \(R\) rounds of training is given.

heads for classification while freezing early layers. Similar techniques have also been employed for unsupervised learning, where representations are trained with contrastive techniques without requiring end-to-end gradient propagation . Recently, the concept of progressive model growth has also been proposed for FL: Wang _et al._ propose _ProgFed_, where they discover that by progressively adding CNN layers to the NN while using an auxiliary head, the FL training converges faster and required less communication to reach the same accuracy as an end-to-end baseline. Similarly, Kundu _et al._ propose a technique that grows the model depending on the complexity of the data to reach a high accuracy if the NN capacity is not sufficient for the problem. _Importantly, both techniques only focus on increasing the convergence speed. Hence, they consider communication and computation overhead but not the problem of constrained memory on edge devices, nor do they support heterogeneous devices. In both techniques, eventually, all devices have to train the full-size NN and, consequently, have to have the memory resources available for that._

**Memory-efficient training**: Several techniques have been proposed to train an ML model in a memory-efficient way. Kirisame _et al._ present Dynamic Tensor Rematerialization that allows recomputing activation maps on the fly. Similarly, encoding and compression schemes  have been proposed to lower the size of the activation maps during training. Techniques like that trade memory for computation, and some lower the accuracy by using approximation or lossy compression. _Importantly, these techniques are orthogonal to SLT, FD, FedRolex, HeteroFL, and Fjord._

## 5 Conclusion

We proposed SLT that is able to reduce the memory requirements for training on devices, efficiently aggregating computation capacity and learning from all available data. Through extensive evaluation, we show that gains in final accuracy as well as the faster convergence speed (compared with state of the art) are robust throughout different datasets, data distribution, and NN topologies.

**Limitations:** We observe that SLT is most effective if the used NN architecture is deep (i.e., has many layers), as the cost of _filling up_ a single layer becomes less significant. Also, SLT is less effective if the size of the activation map is strongly unevenly distributed throughout the layers (DenseNet), as it has to adapt to the layer with the highest memory requirements when filled up. Besides, we applied SLT to CNN topologies only. Finally, we mainly focused on memory as a _hard constraint_ for training. We show that communication and FLOP efficiency are significantly higher than in the state of the art, but we did not consider per-round communication or FLOP constraints. For future work, we want to extend our study to other topologies, such as transformers, and employ neural architecture search (NAS) techniques to find NN configurations that reach the highest accuracy when trained in FL with SLT in heterogeneous environments.

**Broader impact:** Our solution could help reduce biases in FL systems, improving fairness. For instance, by including users with low-end smartphones in the learning process, it provides these users (who perhaps cannot afford high-end devices) with better experiences as the model is going to be trained over their data, too. It could also reduce the cost of deployment of distributed IoT systems (e.g., sensor networks), as they can be implemented with low-cost devices (or a mixture of low and high-cost devices), enabling, e.g., deployment of larger and more fine-grained monitoring systems. On the negative side, distributing learning over low-end devices that are not particularly designed for training tasks can increase the overall energy consumption of the system. This is an important issue that should be studied in more detail.