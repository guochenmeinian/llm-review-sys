# On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions

Yufeng Zhang, Jialu Pan ; Kenli Li

College of Computer Science and Electronic Engineering

Hunan University, Changsha, China

{yufengzhang, jialupan, lkl}@hnu.edu.cn

&Wanwei Liu, Zhenbang Chen, Xinwang Liu

College of Computer

Key Laboratory of Software Engineering for Complex Systems

National University of Defense Technology, Changsha, China

{wwliu, zbchen, xinwangliu, wj}@nudt.edu.cn

&Ji Wang

College of Computer

State Key Laboratory for High Performance Computing

Key laboratory of Software Engineering for Complex Systems

National University of Defense Technology, Changsha, China

wj@nudt.edu.cn

Jialu Pan is the corresponding author.

###### Abstract

Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two \(n\)-dimensional Gaussian distributions \(_{1}\) and \(_{2}\), we prove that when \(KL(_{2}||_{1})\) (\(>0\)) the supremum of \(KL(_{1}||_{2})\) is \((1/2)((-W_{0}(-e^{-(1+2)}))^{-1}+(-W_{0}(-e^{-(1+2 )}))-1)\), where \(W_{0}\) is the principal branch of Lambert \(W\) function. For small \(\), the supremum is \(+2^{1.5}+O(^{2})\). This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of \(KL(_{1}||_{2})\) when \(KL(_{2}||_{1}) M\) (\(M>0\)). We give the conditions when the supremum and infimum can be attained. Secondly, for any three \(n\)-dimensional Gaussian distributions \(_{1}\), \(_{2}\), and \(_{3}\), we theoretically show that an upper bound of \(KL(_{1}||_{3})\) is \(3_{1}+3_{2}+2_{2}}+o( _{1})+o(_{2})\) when \(KL(_{1}||_{2})_{1}\) and \(KL(_{2}||_{3})_{2}\) (\(_{1},_{2} 0\)). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension \(n\). Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.

## 1 Introduction

A statistical divergence measures the "distance" between probability distributions. Let \(X\) be a space of probability distributions with the same support. A statistical divergence \(D:X X^{+}\) (\(^{+}\)is the set of non-negative real numbers) should satisfy (a) non-negativity: \(D(p,q) 0\) and (b) identity of indiscernibles: \(D(p,p)=0\), where \(p,q\) are probability densities. Another stricter concept, statistical distance, also measures the distance between probability distributions. A statistical distance should satisfy two additional properties, including (c) symmetry: \(D(p,q)=D(q,p)\) and (d) triangle inequality: \(D(p,q) D(p,g)+D(g,q)\), where \(p,q\) and \(g\) are probability densities.

Kullback-Leibler (KL) divergence, also referred to as relative entropy , is applied broadly in many fields, such as machine learning , information theory , and statistics . The KL divergence between two continuous probability densities \(p(x)\) and \(q(x)\) is defined as \(KL(p(x)||q(x))= p(x)\,x\). KL divergence is not a proper distance . First, KL divergence is not symmetric. It might happen that the forward KL divergence2\(KL(p||q)\) is very small but the reverse KL divergence \(KL^{*}(p||q)=KL(q||p)\) is very large. Second, KL divergence does not satisfy the triangle inequality. This hinders the application of KL divergence in many contexts.

KL divergence also has connections with other information measures. For example, by taking the second-order Taylor expansion, KL divergence can be approximated with fisher information matrix . Therefore, KL divergence is locally approximately symmetric when two distributions are close to each other.

Meanwhile, Gaussian distribution is one of the most important distributions and is central to statistics. It is also pervasive in many fields. The probability density function of an \(n\)-dimensional Gaussian distribution is \((,)=((2)^{n/2}||^{1/2})^{-1} (-(1/2)(-)^{T}^{-1}(-))\). Here \(^{n}\) is the mean and \(_{++}^{n}\) is the covariance matrix, where \(_{++}^{n}\) is the space of symmetric positive definite \(n n\) matrices. Gaussian distribution is the basis for more complicated distributions such as Gaussian Mixture Model . In this paper, we refer to Gaussian distribution as Gaussian for brevity.

The KL divergence between two \(n\)-dimensional Gaussians \(_{1}\), \(_{2}\) has the following closed form 

\[KL(_{1}(_{1},_{1})||_{2}(_{2 },_{2}))=(_{2}|}{| _{1}|}+(_{2}^{-1}_{1})+(_{2}-_{ 1})^{T}_{2}^{-1}(_{2}-_{1})-n)\] (1)

where the logarithm is taken to base \(e\) and \(\) is the trace of matrix. Like many other distributions, KL divergence between Gaussians is neither symmetric nor satisfies the triangle inequality.

In this work, we investigate the following two research problems, which are motivated by our research on out-of-distribution detection with flow-based model .

1. How to quantify the relation between forward and reverse KL divergences between Gaussian distributions?
2. Does the KL divergence between Gaussian distributions satisfy some property similar to triangle inequality?

**Contributions.** The contributions of this work are as follows. Let \(_{i}(_{i},_{i})\) (\(i\{1,2,3\}\)) be any three \(n\)-dimensional Gaussians.

1. We prove that when \(KL(_{1}||_{2})\) (\( 0\)) the supremum of \(KL(_{2}||_{1})\) is \((1/2)((-W_{0}(-e^{-(1+2)}))^{-1}+(-W_{0}(-e^{-(1+2 )}))-1)\), where \(W_{0}\) is the principal branch of Lambert \(W\) function. We give the conditions when the supremum can be attained. For small \(\), the supremum is \(+2^{1.5}+O(^{2})\). This quantifies the approximate symmetry of small KL divergence between Gaussians.
2. We find the infimum of \(KL(_{1}||_{2})\) if \(KL(_{2}||_{1}) M\) (\(M>0\)). We also give the conditions when the infimum can be attained.
3. We find an upper bound of \(KL(_{1}||_{3})\) if \(KL(_{1}||_{2})_{1}\) and \(KL(_{2}||_{3})_{2}\) for \(_{1},_{2} 0\). For small \(_{1}\) and \(_{2}\), the upper bound is \(3_{1}+3_{2}+2_{2}}+o( _{1})+o(_{2})\). This indicates that KL divergence between Gaussians follows a relaxed triangle inequality.
4. All the bounds in our theorems are independent of the dimension \(n\). This is a critical property in contexts where dimensionality has a fundamental impact.
5. The theorems proved in this paper can extend the applications of KL divergence in many contexts. We discuss the motivation application in out-of-distribution detection with flow-based model and multiple applications in reinforcement learning and sample complexity research.

The rest part of this paper is organized as follows. In Section 2 we discuss related work. In Section 3 we prepare lemmas and notations. In Section 4 we investigate the supremum (infimum) of reverse KL divergence between Gaussians when forward KL divergence is bounded. In Section 5 we investigate the relaxed triangle inequality of KL divergence between Gaussians. In Section 6, we discuss applications. Finally, we conclude in Section 7. We put long proofs in Appendix.

## 2 Related work

KL divergence has a wide range of applications [15; 10; 23; 45; 20; 25; 3]. Researchers have investigated KL divergence between many distributions, including Markov sources , GMM models [18; 27], multivariate generalized Gaussians , discrete normal distributions , _etc_. In , a bound of KL divergence between Gaussians is given. As far as we know, no related work focuses on the similar properties of KL divergence between Gaussians as in this paper.

KL divergence is one member of more general divergences such as Bregman divergence [12; 7; 21; 53], \(f\)-divergence [5; 45; 4], Renyi divergence [48; 45; 54], and \((f,)\)-divergence . Bregman divergence defines a class of divergences  in vector space. KL divergence between multinomial distributions is a special form of Bregman divergence when the convex function for Bregman divergence is chosen as \(_{i=1}^{n}p_{i} p_{i}\), where \(p_{i} 0\) define a multinomial distribution. Frigyik _et. al._ extends vector Bregman divergence to functional Bregman divergence in \(L^{p}\). Similarly, KL divergence is a special form of functional Bregman divergence. (Functional) Bregman divergence also satisfies generalized Pythagoras theorem [7; 21].

The asymmetry of KL divergence has restricted the application of KL divergence in practical applications. Many other divergences have been investigated [46; 16; 2; 43; 26; 17; 7; 22; 56; 55]. Pardo gives a comprehensive survey on a wide range of statistical divergences in his book .

## 3 Lemmas and Notations

We introduce the following Lambert \(W\) function before our theoretical results.

**Definition 1**: _Lambert \(W\) Function[32; 14]. The inverse function of function \(y=xe^{x}\) is called Lambert \(W\) function \(y=W(x)\)._

When \(x\), \(W\) is a multivalued function with two branches \(W_{0},W_{-1}\), where \(W_{0}\) is the principal branch (also called branch \(0\)) and \(W_{-1}\) is the branch \(-1\). Figure A.1 in Appendix A shows the graph of \(W_{0}\) and \(W_{-1}\).

**Lemmas**. _Function \(f(x)=x- x\ (x^{++})\ (^{++}\) is the set of positive real numbers) lies in the core of our problems._ In Lemma B.1 in Appendix B, we prove several properties of \(f(x)\). We show that the inverse function of \(f\) is \(f^{-1}(x)=-W(-e^{-x})\ (x 1)\). Equation \(f(x)=x- x=1+t\ (t 0)\) has two solutions \(w_{1}(t)=-W_{0}(-e^{-(1+t)})(0,1]\) and \(w_{2}(t)=-W_{-1}(-e^{-(1+t)})[1,+)\). We treat \(w_{1}(t),w_{2}(t)\) as functions of \(t\).

Table 1 summarizes some notations used in this paper. Please see Table A.1 in Appendix A for a full list of notations.

   \(f(x)\) & \(x- x\ (x^{++})\) \\ \(W(x)\) & the Lambert \(W\) function \\ \(W_{0}(x)\) & the principal branch (branch \(0\)) of \(W(x)\) \\ \(W_{-1}(x)\) & the branch \(-1\) of \(W(x)\) \\ \(w_{1}(t)\) & the smaller solution of \(f(x)=1+t\ (t 0)\) \\ \(w_{2}(t)\) & the larger solution of \(f(x)=1+t\ (t 0)\) \\ \((x_{1},,x_{n})\) & \(_{i=1}^{n}f(x_{i})\) \\ \(\) & the eigenvalue of matrix \\ \((0,I)\) & standard Gaussian distribution, dimension \(n\) is eliminated for brevity \\   

Table 1: Notations.

[MISSING_PAGE_FAIL:4]

**Lemma 1**: _Let \((0,I)\) be \(n\)-dimensional standard Gaussian distribution, \(\) be a positive number. For any \(n\)-dimensional Gaussian distribution \((,)\),_

* _If_ \(KL((,)||(0,I))\)_, then_ \[KL((0,I)||(,)) ((-e^{-(1+2)})}-(-e^{-(1+2)})}-1)\]
* _If_ \(KL((0,I)||(,))\)_, then_ \[KL((,)||(0,I)) ((-e^{-(1+2)})}-(-e^{-(1+2)})}-1)\]

**Proof 1**: _Please see Section C in the Appendix for details._

To further investigate the bound in Theorem 1, we expand the Lambert \(W\) function using the series presented in  for small \(\). This result is stated in the following Theorem 2, which can help users to apply our theorem conveniently.

**Theorem 2**: _For any two \(n\)-dimensional Gaussian distributions \((_{1},_{1})\), \((_{2},_{2})\), and a small positive number \(\), if \(KL((_{1},_{1})||( _{2},_{2}))\), then_

\[KL((_{2},_{2})||( _{1},_{1}))+2^{ 1.5}+O(^{2})\]

**Proof 2**: _Please see Appendix E for details._

Theorem 1 holds for any two Gaussians \((_{1},_{1})\) and \((_{2},_{2})\). According to the proof of Theorem 1 (Lemma 1), one of \((_{1},_{1})\) and \((_{2},_{2})\) can be fixed. Thus, it is not hard to extend Lemma 1 to the case where the fixed Gaussian is not standard. We can apply linear transformation on the fixed Gaussian (see Equation (D.46)) as what we have done in the main proof of Theorem 1 (see Appendix D). Other parts of the proof are the same. Therefore, we obtain the following corollary.

**Corollary 1**: _Theorem 1 and Theorem 2 hold when one of \((_{1},_{1})\) and \((_{2},_{2})\) is fixed._

**Remark 1**: _The supremum in Theorem 1 is small (zero) when \(\) is small (zero). Figure A.2 in Appendix A shows the graph of the supremum of KL divergence. Due to the strict conditions, it is hard to reach the supremum in typical applications (e.g. in machine learning). Notably, the bound is independent of the dimension \(n\). This is critical in high-dimensional problems (see motivation application in Section 6). We also note that the condition \(KL((_{1},_{1})||( _{2},_{2}))\) in Theorem 1 is strict in high-dimensional problems._

**Remark 2**: _Theorem 1 gives the strict conditions when the supremum can be attained. We can benefit from these strict conditions in applications. When the forward KL divergence is small, we want a guarantee that the reverse KL divergence is also small such that bounding forward KL divergence also bounds the reverse KL divergence. Theorem 1 has the following two meanings._

1. _The supremum of reverse KL divergence_ \(+2^{1.5}+O(^{2})\) _is small, implying that the worst case is acceptable._
2. _The strict conditions for reaching the supremum imply that the worst case barely happens in practice. When these strict conditions do not hold, the reverse KL divergence is smaller than the supremum, which is what we want in practice._

**Toy Examples**.

Figure 1 shows some toy examples in one dimensional case. The black line represents standard Gaussian distribution \(_{0}(0,1)\). All other four Gaussian distributions \(_{i}\) (\(1 i 4\), in colored lines) have the same forward KL divergence \(KL(_{i}||_{0})=0.01\). The second distribution has the maximized reverse KL divergence \(KL(_{0}||_{1}(0,0.90173^{2})) 0.01148\), which is equal to the supremum \((-(-e^{-(1+2)0,0.1)}})- (-e^{-(1+2)0,0.1)}}-1)\). Other reverse KL divergences are \(KL(_{0}||_{2}(0,1.10161^{2})) 0.00879\), \(KL(_{0}||_{3}(0.14143,1)) 0.01\), \(KL(_{0}||_{4}(0.1,1.07153^{2})) 0.00892\).

### Infimum of Reverse KL Divergence Between Gaussians

In this subsection, we give the infimum of \(KL(_{2}||_{1})\) when \(KL(_{1}||_{2}) M\) (\(M>0\)). The result is presented in Theorem 3.

**Theorem 3**: _For any two \(n\)-dimensional Gaussian distributionss \((_{1},_{1})\) and \((_{2},_{2})\), if \(KL((_{1},_{1})||( _{2},_{2})) M\) (\(M>0\)), then_

\[KL((_{2},_{2})||( _{1},_{1}))((-e^{-(1+2M)})}-(-e^{-(1+2M)})}-1)\]

_The infimum is attained when the following two conditions hold._

1. _There exists only one eigenvalue_ \(_{j}\) _of_ \(B_{2}^{-1}_{1}(B_{2}^{-1})^{}\) _or_ \(B_{1}^{-1}_{2}(B_{1}^{-1})^{}\) _equal to_ \(-W_{-1}(-e^{-(1+2M)})\) _and all other eigenvalues_ \(_{i}\) _(_\(i j\)_) are equal to_ \(1\)_, where_ \(B_{1}=P_{1}D_{1}^{1/2}\)_,_ \(P_{1}\) _is an orthogonal matrix whose columns are the eigenvectors of_ \(_{1}\)_,_ \(D_{1}=diag(_{1},,_{n})\) _whose diagonal elements are the corresponding eigenvalues,_ \(B_{2}\) _is defined in the similar way as_ \(B_{1}\) _except on_ \(_{2}\)_._
2. \(_{1}=_{2}\)_._

**Proof 3**: _Theorem 1 and Theorem 3 form a duality. These two theorems can be proved independently in a similar way and also be derived from each other. We give two proofs of Theorem 3 in Appendix F. The first proof presented in Appendix F.1 has the similar structure as that of Theorem 1, except that Theorem 3 uses \(W_{-1}\). The second proof shown in Appendix F.2 derives Theorem 3 from Theorem 1.These two proofs can verify each other. \(\)_

## 5 Relaxed Triangle Inequality

In this section, we give a dimension-free bound of \(KL(_{1}||_{3})\) when \(KL(_{1}||_{2})\) and \(KL(_{2}||_{3})\) are bounded for any three Gaussians \(_{1}\), \(_{2}\), and \(_{3}\). Proving the relaxed triangle inequality is much more difficult. The main result is presented in Theorem 4 and 5. We use two key Lemmas G.5 and 2 to accomplish the key steps of the proof.

Figure 1: One dimensional toy examples.

[MISSING_PAGE_FAIL:7]

a "more extreme" allocation of \(2_{2}\) (_i.e._, \((_{2,},_{2,})\) where \(_{2,}>0\)) that maximizes the objective function. Note that it is hard to state that \((_{2,},_{2,})\) is "more extreme" than \((_{1,},_{1,})\) because \(_{1}_{2}\). Then we fix \((_{2,},_{2,})\) and find a "more extreme" allocation of \(2_{1}\) (\(_{1,}^{},_{1,}^{}\)) to lift the objective function further. Using these iterations, we can construct an infinite sequence of allocations whose limitation is an "extreme" one (_i.e._, \(_{1,}=2_{1},_{2,}=2_{2}\)). Then we prove the extreme allocation can make the objective function reach its supremum. In Appendix G we present the key Lemma G.5 and its proof. We also give its proof sketch before the long proof. Essentially, Lemma G.5 plays the most vital role in eliminating the dimension \(n\) in case of \(n=2\) from the bound in 2-dimensional case. Finally, we will make the bound in high-dimensional problem dimension-free as well.

In summary, we apply a linear transformation to simplify the problem. Then we solve the simplified problem in the following Lemma 2, accomplishing the above steps \(2 4\). Theorem 4 can be seen as the generalization of Lemma 2.

**Lemma 2**: _For any two \(n\)-dimensional Gaussian distributions \((_{1},_{1})\) and \((_{2},_{2})\) such that \(KL((_{1},_{1})||(0,I ))_{1}\), \(KL((0,I)||(_{2},_{2} ))_{2}\)\((_{1},_{2} 0)\),_

\[KL((_{1},_{1})|| (_{2},_{2}))<_{1}+ _{2}+W_{-1}(-e^{-(1+2_{1})})W_{-1}( -e^{-(1+2_{2})})+W_{-1}(-e^{-(1+2_{1})})\] \[+W_{-1}(-e^{-(1+2_{2})})+1\ -W_{-1}(-e^{-(1+2 _{2})})(}+ }{-W_{0}(-e^{-(1+2_{2})})}})^{2}\]

**Proof 4**: _Please see Appendix H for details._

**Remark 3**: _The bound in Lemma 2 is dimension-free and becomes 0 when \(_{1}=_{2}=0\)._

Similarly, we can expand Lambert \(W\) function by series  and simplify the bound in Theorem 4 as follows .

**Theorem 5**: _For any three \(n\)-dimensional Gaussian distributions \((_{i},_{i})(i\{1,2,3\})\) such that \(KL((_{1},_{1})||( _{2},_{2}))_{1}\) and \(KL((_{2},_{2})||( _{3},_{3}))_{2}\) for small \(_{1},_{2} 0\),_

\[KL((_{1},_{1})||( _{3},_{3}))<3_{1}+3 _{2}+2_{2}}+o(_{1})+o(_ {2})\]

**Proof 5**: _Please see Appendix J for details._

Finally, in the proof of Theorem 4, we use invertible linear transformation to convert \(_{2}\) to standard Gaussian while preserving KL divergence. This proof also applies to the case when \((_{2},_{2})\) is fixed. Therefore, we obtain the following corollary.

**Corollary 2**: _Theorem 4 and 5 hold when \((_{2},_{2})\) is fixed._

**Remark 4**: _Our theorem is different from existing generalized Pythagoras inequalities satisfied by KL divergence. Please see Appendix K.1 for more discussion._

## 6 Discussion and Applications

In our theorems, we allow all parameters are unknown or one Gaussian is fixed. Therefore, our theorems are suitable for problems where the parameters can vary. This is common in deep learning where the parameters are learned from data. In this section, we discuss motivation application and other applications ranging from anomaly detection to reinforcement learning to sample complexity research.

### Motivation Application: Anomaly Detection with Flow-based Model

The research problems in this paper are motivated by our research on deep anomaly detection using flow-based model 3. Flow-based model assigns higher likelihoods to Out-of-Distribution (OOD) data than In-Distribution (ID) data (_i.e._, training data) . For example, Glow  assigns higher likelihoods for SVHN when trained on CIFAR-10. Furthermore, we cannot sample OOD data from the model. Existing explanation is based on the discrepancy of typical set and high probability density regions of model distribution . Such typicality-based explanation and OOD detection method fail when OOD data has coinciding likelihoods with ID data .

We focus on two research problems in this context. (1) why we cannot sample OOD data from flow-based model with prior regardless of when OOD data have higher, lower, or coinciding likelihoods? (2) How to detect OOD data using flow-based model? We investigate these problems from a statistical divergence perspective. Let \(z=f(x)\) be the flow-based model mapping data \(x\) in data space to representation \(z\) in latent space. Suppose the prior distribution \(p_{Z}^{r}\) is the most commonly used Gaussian distribution. Let \(X_{1} p_{X}(x)\), \(X_{2} q_{X}(x)\) be the distributions of ID and OOD datasets, respectively. \(Z_{1}=f(X_{1}) p_{Z}(z)\), \(Z_{2}=f(X_{2}) q_{Z}(z)\) be the distributions of representations of ID and OOD datasets, respectively. Let \(p_{X}^{r}\) be the model induced distribution such that \(Z_{r} p_{Z}^{r}\) and \(X_{r}=f^{-1}(Z_{r}) p_{X}^{r}\). Flow-based model is usually trained by maximum likelihood estimation, which is equal to minimizing forward KL divergence \(KL(p_{X}||p_{X}^{r})\). We conduct generalized Shapiro-Wilk test for multivariate normality on representations. As shown in the original Table C.3 in  (also in supplementary material), \(p_{Z}\) is Gaussian-like for all datasets. \(q_{Z}\) is also Gaussian-like for OOD datasets with higher or coinciding likelihoods except for just one case. These results allow us to approximate \(p_{Z}\) and \(q_{Z}\) with Gaussians when possible.

The theorems proved in this paper provide solid theoretical guarantee for our analysis and algorithm. On one hand, flow-based model preserves KL divergence (see Proposition D.1 in Appendix D), so \(KL(p_{Z}||p_{Z}^{r})\) which equals \(KL(p_{X}||p_{X}^{r})\) is minimized. According to the approximate symmetry of small KL divergence (Theorem 1), we can know \(KL(p_{Z}^{r}||p_{Z})\) is small too. So we can assume \(p_{Z}^{r} p_{Z}\) when \(KL(p_{Z}||p_{Z}^{r})\) is sufficiently small. On the other hand, we can also assume that the distributions of ID and OOD data are far from each other. This implies that \(KL(p_{X}||q_{X})\) equaling \(KL(p_{Z}||q_{Z})\) can be any large. This implies \(KL(p_{Z}^{r}||q_{Z}) KL(p_{Z}||q_{Z})\) is large too. Specially, when \(q_{Z}\) is Gaussian-like, we can apply the relaxed triangle inequality (Theorem 4) and infer that \(KL(p_{Z}^{r}||q_{Z})\) must be large. Note that, when \(q_{Z}\) is not Gaussian-like, we can still apply the theorems presented in this paper to perform analysis on the lower bound of KL divergence (see original Theorem 5 in ). Overall, the large KL divergence between \(p_{Z}^{r}\) and \(q_{Z}\) reveals why we cannot sample OOD data from flow-based model with prior. It is also notable that flow-based model constructs diffeomorphism between data and latent space with thousands of dimensions. Thus, it is critical that the bounds found in this paper are dimension-free. Furthermore, we decompose the KL divergence further into group-wise KL divergence and mutual information. Based on these analysis, we propose an unified OOD detection algorithm KLODS both for group (GAD) and point-wise (PAD) anomaly detection. We conduct extensive experiments to compare our method with 13 baseline methods including \(t\)-test, KS-test, MMD , KSD , Annulus Method , typicality test , the state-of-the-art (SOTA) GAD method GOD2KS , input complexity compensated likelihood , last-scale likelihood , ODIN , Joint confidence loss , and DoSE . Experimental results demonstrate the superiority of our method. For example, as shown in Table 2, our method outperforms the SOTA group-wise anomaly detection method GOD2KS on flow-based model by 9.1% AUROC. Our method also outperforms the SOTA point-wise anomaly detection method DoSE with Glow by 5.2% AUROC. More details of the algorithm and experimental results on both group and point-wise anomaly detection can be refered to .

### Applications of Approximate Symmetry of Small KL divergence

Theorem 1 can be also applied widely in deep reinforcement learning and sample complexity research. In many contexts, researchers are hindered by the asymmetry of KL divergence. Theorem 1 on the approximate symmetry of KL divergence between Gaussians brings the following convenience to us.

1. Minimizing one of forward and reverse KL divergences also bounds another.
2. We can exchange forward and reverse KL divergences for small \(\).

We summarize the applications of Theorem 1 briefly in the following. The details of these applications are discussed in Appendix L.1.

**Providing Theoretical Guarantee for Continuous Gaussian Policy in Reinforcement Learning**. In , Nair _et al._ propose AWAC method to accelerate online reinforcement learning with offline datasets. They obtain theoretical guarantee in offline reinforcement learning for discrete policies. Theorem 1 can extend their guarantee to continuous Gaussian policy.

**Bringing New Insights to Existing Reinforcement Learning Algorithm**. In , Abdolmaleki _et al._ propose the MPO algorithm for reinforcement learning. They use Expectation-Maximization (EM) to solve control problems and use constraints on KL terms in both E and M-steps. Theorem 1 can eliminate such a difference for continuous Gaussian policies.

**Bridging Research on Sample Complexity of Learning Gaussian Distribution**. Theorem 1 can bridge existing research on sample complexity of Gaussian distribution. Researchers have proposed algorithms for learning a multivariate Gaussian distribution with error bounds in forward and reverse KL divergence separately so far [6; 8]. Theorem 1 can eliminate the difference between forward and reverse KL divergence in this scenario.

### Application of Relaxed Triangle Inequality

**Extending One-step Safety Guarantee to Multiple Steps in Reinforcement Learning**. The relaxed triangle inequality (Theorem 5) has been applied in safe reinforcement learning. Liu _et al._ propose an Expectation-Maximization style approach for learning safe policy in reinforcement learning . They utilize our relaxed triangle inequality to extend one-step robustness guarantee to multiple steps. In the original Proposition 4 in , they simplify the bound in Theorem 4 in case \(_{1}=_{2}\). Please see Appendix L.2 for details.

## 7 Conclusion

In this paper, we research the properties of KL divergence between Gaussians. First, we find the supremum of reverse KL divergence \(KL(_{2}||_{1})\) if the forward KL divergence \(KL(_{1}||_{2})\) (\(>0\)). This conclusion quantifies the approximate symmetry of small KL divergence between Gaussians. We also find the infimum of \(KL(_{2}||_{1})\) if \(KL(_{1}||_{2}) M\) (\(M>0\)). We give the conditions when the supremum and infimum can be attained. Second, we find a bound for \(KL(_{1}||_{3})\) when \(KL(_{1}||_{2})\) and \(KL(_{2}||_{3})\) are bounded. This indicates that KL divergence between Gaussians follows a relaxed triangle inequality. All the bounds in this paper are independent of the dimension of distributions. Finally, we discuss the applications of our theorems in deep anomaly detection, reinforcement learning, and sample complexity research.

    &  &  &  \\   & &  &  &  &  \\   & & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR \\   & MNIST & **99.8\(\)0.0** & **99.8\(\)0.0** & 98 & 98 & **100.0\(\)0.0** & **100.0\(\)0.0** & **100** & **100** \\  & KMNIST & **99.9\(\)0.0** & **99.9\(\)0.0** & 97 & 96 & **100.0\(\)0.0** & **100.0\(\)0.0** & **100** & **100** \\  & Omniglot & **100.0\(\)0.0** & **100.0\(\)0.0** & **100** & **100** & **100.0\(\)0.0** & **100** & **100** & **100** \\   & CelebA & **100.0\(\)0.0** & **100.0\(\)0.0** & 100 & 99 & **100.0\(\)0.0** & **100.0\(\)0.0** & **100** & **100** \\  & CIFAR-10 & **100.0\(\)0.0** & **100.0\(\)0.0** & 92 & 84 & **100.0\(\)0.0** & **100.0\(\)0.0** & 99 & 98 \\  & CIFAR-100 & **100.0\(\)0.0** & **100.0\(\)0.0** & 93 & 86 & **100.0\(\)0.0** & **100.0\(\)0.0** & 99 & 98 \\  & LSUN & **100.0\(\)0.0** & **100.0\(\)0.0** & 99 & 98 & **100.0\(\)0.0** & **100.0\(\)0.0** & **100** & **100** \\   & CelebA & **99.2\(\)0.1** & **99.4\(\)0.1** & 86 & 92 & **100.0\(\)0.0** & **100.0\(\)0.0** & 96 & 98 \\  & SVHN & **97.6\(\)0.2** & 97.8\(\)0.2 & 96 & 98 & 99.8\(\)0.0 & 99.8\(\)0.0 & **100** & **100** \\  & LSUN & **100.0\(\)0.0** & **100.0\(\)0.0** & 60 & 58 & **100.0\(\)0.0** & **100.0\(\)0.0** & 58 & 56 \\   & CIFAR-10 & **99.6\(\)0.0** & **99.6\(\)0.0** & 84 & 73 & **100.0\(\)0.0** & **100.0\(\)0.0** & 94 & 91 \\  & CIFAR-100 & **99.8\(\)0.0** & **99.8\(\)0.0** & 82 & 71 & **100.0\(\)0.0** & **100.0\(\)0.0** & 94 & 90 \\   & SVHN & **100.0\(\)0.0** & **100.0\(\)0.0** & 97 & 98 & **100.0\(\)0.0** & **100.0\(\)0.0** & **100** & **100** \\   & LSUN & **100.0\(\)0.0** & **100.0\(\)0.0** & 85 & 75 & **100.0\(\)0.0** & **100.0\(\)0.0** & 96 & 92 \\   & **99.7** & **99.7** & 90.6 & 87.6 & **100.0** & **100.0** & 95.4 & 94.5 \\   

Table 2: Group-wise anomaly detection Results (AUROC and AUPR in percentage) of our method KLODS and the SOTA method GOD2KS on Glow with batch sizes 5 and 10. We run our method for 5 times. Results of GOD2KS are referred from . The higher the better.