# Last-layer committee machines for uncertainty quantification

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We introduce here a form of committee machines that gives good predictions of classification confidence, while being computationally efficient. The initial development of this method was motivated by our work on benthic mapping based on a large dataset of ocean floor images. These wild type images vary dramatically in terms of their classification difficulty and often result in low inter-rater agreement. We show that our method is able to identify difficult to classify images using model uncertainty, consistent with Bayesian neural networks and Monte Carlo sampling. However, our method drastically reduces the computational requirements and offers a more efficient strategy. This enables us to provide these uncertain predictions to a human specialist and offers a form of active learning to enhance the classification accuracy of the dataset. We provide both a benchmark study to demonstrate this approach and first results of the BenthicNet dataset.

## 1 Introduction

Uncertainty quantification of model predictions plays an important role for predictions of high-risk applications (Gawlikowski et al., 2023; Hullermeier and Waegeman, 2021; Huang et al., 2024). However, evaluating uncertainties of model predictions are typically computationally expensive or difficult to implement. This is in part, due to applications of large networks and datasets that are commonly used today (_e.g._, large language models, transformers, ResNets, etc.). Bayesian and their approximating methods, such as Monte Carol sampling and ensembles require sampling multiple models or subsets of the training dataset. While, recent approaches have been proposed to address these concerns (Harrison et al., 2024; Lee et al., 2015; Lakshminarayanan et al., 2017), there continues a need for more efficient and accessible techniques.

The motivation for the development of this work is related to the advancement of benthic habitat mapping using underwater seafloor images from the hierarchical BenthicNet dataset (Lowe et al., 2024; Misiuk et al., 2024). This dataset is a collection of global seafloor images with corresponding annotation files which are labelled according to the CATAMI classification scheme (Althaus, Hill, Edwards, et al., 2014; Althaus, Hill, Ferrari, et al., 2015). We selected two sub-datasets: German Bank 2010 and Substrate (depth 2). These datasets are available as one-hot classifications and represents images which are known to be difficult because of low inter-class heterogeneity represented across the different samples (Xu et al., 2024; Humblot-Renaux et al., 2024).

In this study, we propose last-layer committee machines (LLCMs) as an ensemble method using shared network parameters for a classifier consisting of \(M\) committee machines (_i.e._, linear layers). We demonstrate the requisite network diversity of the penultimate layer is facilitated by random weight initialization and can be increased by enabling different hyperparameters during training such as logit normalization (Wei et al., 2022) or label smoothing (Szegedy et al., 2015). We introduce and demonstrate LLCMs on the MNIST dataset before evaluating on the substantially more difficult subsets of the BenthicNet dataset. LLCMs offers a comparable (or competitive) strategy for Bayesian approximations, where uncertainty quantifications are obtained in a single forward-pass during inference for both the dataset and single predictions.

## 2 Methods

### Last-layer committee machines

A committee machine (or committee method) is a form of an ensemble learner used to boost model performance by averaging over multiple models, often used with classification and regression trees such as random forests. In the case of neural networks, average probability distributions can be computed as,

\[(y|;)=_{m=1}^{M}p(y|; ^{m})\] (1)

where \(M\) represents the number of models and individual probability distributions are calculated from output logits from using network weights \(^{m}\) and the softmax function. These deep ensembles often require substantial computational effort and memory usage, which is largely the result of averaging large networks and/or datasets. Rather than using the full network as ensembles, we propose to use a list of linear layers as a last-layer committee machine as part of the network architecture. Training such a network involves backpropagating the mean loss of the committee machine, which effectively results with gradient updates from each committee machine member (Figure 1).

For every forward-pass during training, each member of the committee machine receives identical feature representations; therefore, network diversity or loss exploration during backpropagation is dependent on the initialization of each member. That is, if committee machine members are identically initialized, they will all have the same weights during and after training. If they are non-identically initialized, each committee machine member will explore its own loss landscape. We show that including techniques such as, logit normalization, label smoothing, and/or class weighting during training can influence the degree of network diversity of model weights of the LLCM module.

These simple modifications result with a network architecture that can provide model uncertainty for both the dataset and single samples at inference. It drastically reduces computational and memory intensive requirements and can be parallelized and/or scaled to multiple devices. In addition, LLCMs can be used with several feature extracting networks, such as MLPs, CNNs, ViT, ResNets, etc.

Figure 1: Network architecture of a last-layer committee machine. Input features are obtained from a network which are subsequently passed to the LLCM module. For each forward pass, mean loss is computed using \(M\) committee machine members which is then backpropagated during training. At inference time, a distribution of softmax distributions is obtained by averaging committee machine members.

### Parametric confidence score metric

To evaluate performance and model uncertainty, a parametric confidence score metric was developed that uses the top-2 mean probabilities and their corresponding standard deviations from predicted mean softmax distributions (Figure 2).

This approach provided a metric whereby scores can be modulated by parameter \(k\) and standard deviations of the top-2 mean softmax probabilities. This effectively results with predictions based on model uncertainty, reflective on the factor of the standard deviations. We used multiple \(k\) values and results were reported for accuracy, F1-score, and confusion matrices. We plotted accuracy with respect to the fraction of remaining of samples after applying each \(k\)-value. Correct and incorrect model predictions were computed from confusion matrices and also plotted against remaining samples. Together, these plots provide valuable insights on model performance, uncertainty, and a visual representation to compare different models and hyperparameters.

## 3 Experiments

### Network diversity of LLCMs

Our initial concern with this approach was that averaging committee members would decrease network diversity or will would converge during training. To investigate this, we created a 2-block CNN (Conv2d-ReLU-MaxPool2d) with a LLCM module consisting of 10 committee members with 2048 input features and 10 output features (classes). After training the MNIST dataset, each committee member was flattened and the coefficient of variations (CV) for the learned weights were computed (_i.e._, \(2048 10\) weights per member). Committee members with similar CV values would equate to being similar; therefore, we report the standard deviations of CV values across all committee machine members (Table 1).

The last entry of Table 1 shows the result where all committee machine members had weights initialized to ones. This resulted with a CV\({}_{}\) of 0.0, which indicates that all members are identical. However, by using random initialization of weights with/without different training hyperparame

  
**Model\({}^{1}\)** & **Accuracy** & **F1-score** & **CS\({}^{2}\)** & **BS\({}^{3}\)** & **ECE\({}^{4}\)** & **CV\({}_{}\)\({}^{5}\)** \\  A: \(-/\!-\!/\!-\) & 0.991 & 0.991 & 0.979 & 0.003 & 0.004 & \(9.537\) \\ B: \(-/\!+\!/\!-\) & 0.990 & 0.991 & 0.776 & 0.072 & 0.181 & \(906.146\) \\ C: \(+/\!+\!/\!-\) & 0.989 & 0.989 & 0.771 & 0.074 & 0.185 & \(8372.234\) \\ D: \(+/\!-\!/\!-\) & 0.991 & 0.991 & 0.979 & 0.003 & 0.003 & \(9.414\) \\ E: \(+/\!-\!/\!+\) & 0.992 & 0.992 & 0.841 & 0.026 & 0.115 & \(25.502\) \\ F: \(-/\!-\!/\!+\) & 0.992 & 0.992 & 0.842 & 0.026 & 0.115 & \(23.713\) \\  A:\({}^{6}\)\(-/\!-\!/\!-\) & 0.991 & 0.991 & 0.980 & 0.003 & 0.001 & **0.0** \\    \({}^{1}\) Models A-F are defined based on training hyperparameters using class weights / logit normalization / label smoothing (amount of smoothing, 0.1). These hyperparameters are either applied (denoted by \(+\)) or omitted (denoted by \(-\)). \({}^{2}\) Confidence score (CS) is defined as the difference between the top-2 mean softmax probabilities. \({}^{3}\) Brier score (BS). \({}^{4}\) Expected calibration error (ECE). \({}^{5}\) Refer to text. \({}^{6}\) All committee machine members weights were initialized with ones.

Table 1: Model performances using a 10-member LLCM and the MNIST dataset.

ters, a diverse range of committee machine members can be obtained. We do observe from the BS and ECE, that models may require additional calibration (_e.g._, temperature scaling).

### Comparison of model uncertainty using LLCMs, BNNs, and Monte Carlo Dropouts

We compared model uncertainty of the LLCM method to Bayesian model averaging (BMA) and Monte Carlo (MC) dropout using the parametric confidence score metric (Figure 2). We converted the CNN described above by replacing the LLCM with a single classifier and then used the utilities from the Pyro framework (Bingham et al., 2019) to convert the network to a Bayesian model. For the MC dropout experiments, we added a dropout layer at the end of each block before sampling with a rate of 0.1. In the case of the LLMC, we used a 100-member committee machine. Figure 2(a) show the results for the MNIST dataset.

Model uncertainty can be realized by noticing the increase in accuracy as the \(k\) increases, which removes uncertain model predictions. An important feature of this plot is the differential decay rates for the correct model predictions (dashed line) and the incorrect model predictions (dotted lines) for each model. This preferred removal of samples offers a utility to: 1) identify uncertain samples, and 2) boost overall performance and model confidence.

We next investigated the more challenging BenthicNet dataset. As an example, we show the results from the German Bank 2010 dataset (Figure 2(b)). In this case, we used a BenthicNet pre-trained ResNet-50 network (Xu et al., 2024) to create a last-layer BNN, LLCM, and added dropout layers (\(p=0.01\)) after each ReLU activation for the MC dropout experiments. Immediately, we see the effects of using a challenging wild type dataset. However, even in this case we can identify uncertain samples that require external review. All models used for both datasets were capable of evaluating model uncertainty and resulted with uncertain samples with LLCMs being comparable (or competitive) with current approaches.

## 4 Conclusions

In this study, we proposed the LLCMs as a method to evaluate model uncertainty and identify uncertain samples for review by a domain expert. For areas such as health, autonomous driving, ocean management, and other high-risk areas deploying machine learning, having a human-in-loop during decision-making can be beneficial, if not essential. The LLCM method presented here scales well both on model sizes and datasets, offering an efficient approach for Bayesian approximations and a strategy to identify uncertain model predictions.

Figure 3: Parametric sigma plots for BMA, MC dropout, and LLCM using the (a) MNIST and (b) German Bank 2010 datasets. For both the MC dropout and LLCM, results are reported using class weights and logit normalization. BMA and MC dropouts were performed using 100 sampling of weights.