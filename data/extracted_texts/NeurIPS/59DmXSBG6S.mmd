# Relating Hopfield Networks to Episodic Control

Hugo Chateau-Laurent

Inria centre of the University of Bordeaux, France

IMN, CNRS UMR 5293, France

LaBRI, CNRS UMR 5800, France

hugo.chateaulaurent@gmail.com

Currently a postdoc at CerCo, CNRS UMR 5549, Universite de Toulouse, France.

Frederic Alexandre

Inria centre of the University of Bordeaux, France

IMN, CNRS UMR 5293, France

LaBRI, CNRS UMR 5800, France

frederic.alexandre@inria.fr

###### Abstract

Neural Episodic Control is a powerful reinforcement learning framework that employs a differentiable dictionary to store non-parametric memories. It was inspired by episodic memory on the functional level, but lacks a direct theoretical connection to the associative memory models generally used to implement such a memory. We first show that the dictionary is an instance of the recently proposed Universal Hopfield Network framework. We then introduce a continuous approximation of the dictionary readout operation in order to derive two energy functions that are Lyapunov functions of the dynamics. Finally, we empirically show that the dictionary outperforms the Max separation function, which had previously been argued to be optimal, and that performance can further be improved by replacing the Euclidean distance kernel by a Manhattan distance kernel. These results are enabled by the generalization capabilities of the dictionary, so a novel criterion is introduced to disentangle memorization from generalization when evaluating associative memory models.

## 1 Introduction

Episodic memory is the ability to remember information about a specific situation. An influential model of episodic memory is the Hopfield Network [(1)], a recurrent associative memory that can learn a pattern in one shot and recall it, given some partial or noisy cue. Some important limitations have been addressed with the development of differentiable continuous Hopfield Networks [(2)] and their connection to deep learning [(3; 4)], thus providing a renewed interest to the field of associative memory. Episodic memory has also been studied as an efficient way to control reinforcement learning in so-called episodic control, particularly in the initial steps of learning [(5)], but no explicit link has been made between Hopfield Networks and control algorithms. Such a link could lead to the development of more efficient controllers and memory models. It could also shed light on how the hippocampus, the seat of episodic memory in the brain, contributes to behavior.

In this paper, a novel connection is established between the fields of associative memory and reinforcement learning. It is shown in Section 2 that the differentiable neural dictionary (DND) introduced in the context of episodic control [(6)] as a rapid way to store and retrieve experiences, is mathematically close to the Hopfield Network. Retrieval from a DND can indeed be decomposed intosimilarity, separation and projection operations, just as any instance of the general Universal Hopfield Network framework (UHN) (7) recently proposed to encompass both classical and recent models of associative memory. In a DND, similarity scores are computed using the Euclidean distance and separated with a \(k\)-nearest neighbor algorithm. The projection operation of DND is the same as for all existing UHN instances, including the traditional Hopfield Network. DND can thus be thought of as a single-shot associative memory model, just like Hopfield Networks and their modern continuous variants. These models can often be defined by an energy function which decreases as memories are retrieved. We show that energy functions can also be derived for the recall operation of the DND.

In Section 3, experiments are conducted to compare the DND to state of the art associative memory models in memorization tasks. It is shown that the DND outperforms concurrent models in generalization tasks. Its performance is further improved by replacing the Euclidean distance by the Manhattan distance, as predicted by the original UHN study (7). A new criterion is introduced to assess performance, and it is found that the \(k\)-nearest neighbor separation of DND favors generalization over memorization, as compared to the simpler Max separation function.

## 2 Differentiable Neural Dictionary as a Hopfield Network

The UHN framework encompasses a family of associative memory models in which retrieval is performed by computing the similarity between a query \(\) and keys \(\) (sim function), separating the similarity scores with a function sep, then projecting the results to the output space with some value matrix \(\):

\[=((, )). \]

In the original binary Hopfield Network (1), the similarity function is the dot product and sep is the identity function. Modern continuous variants have been proposed (8) that improve storage capacity by using more elaborate separation functions such as Softmax (4) to push apart memory attractors.

On the other hand, Neural Episodic Control is a reinforcement learning architecture that introduces the DND as a way to store associations between sensory observations and Q-value estimates. The reading operation of the DND is:

\[z=_{i=1}^{k}_{i}_{i}, \]

where \(\) contains the normalized inverse distances between the query and the \(k\) nearest observation keys, and \(\) contains the values of nearest observations (i.e. their Q-value estimates). The inverse distances are computed using the following kernel function:

\[_{i}(,)=_{i}-||_{2}^{2}}, \]

Figure 1: Differentiable Neural Dictionary lookup as retrieval from a Universal Hopfield Network.

with **K** the observation keys, **q** the query, and \(=10^{-3}\). The \(k\)-nearest neighbor function can be written as:

\[()=()/_{i}_{i}() \]

\[_{i}()=_{i}&_{i}k,\\ 0&. \]

We can thus rewrite Equation 2 as:

\[z=((,)), \]

which closely resembles the reading operation of UHN (Equation 1).

Note that the Euclidean similarity function of UHN (7) is the same as the kernel function of Neural Episodic Control (6):

\[_{i}(,) =(_{ij}-_{j})^{2}} \] \[=(_{ij}-_{j})^{ 2}}^{2}}\] (8) \[=_{i}-||_{2}^{2}} \]

In sum, the kernel function of DND acts as a similarity function and is equivalent to the Euclidean similarity function of UHN. Furthermore, the \(k\)-nearest neighbor algorithm sparsifies the result of the similarity function by cancelling the contribution of the most distant experiences. The output of the algorithm is then normalized before being projected to the value space. This constitutes a novel separation function for the UHN framework, we call it \(k\)-Max. It is worth noting that this separation function is similar to applying a threshold on the similarity function, like what is done in the sparse distributed memory model (9), which has also been cast as a UHN (7). The only difference is that the threshold for sparse distributed memory is fixed, while it must be dynamic for selecting a constant number \(k\) of neighbors. Furthermore, with \(k=1\), the separation function is equivalent to the Max separation function of UHN. The remaining difference between DND and other UHN instances is that the output of the DND is a scalar value, while UHN models can store vector values. In a modification of DND (10), multidimensional values have been stored. In fact, the DND can simply be extended with a matrix of value vectors \(\). Equation 6 thus becomes:

\[=((,)). \]

An abstract energy function has been derived for UHN models (7). It is defined as:

\[E(,)=_{i}_{i}^{2}- _{j}(_{i,j},_{i}), \]

with \(\) the activity of value neurons which are initialized with the query \(\), and updated to produce the output pattern \(\) (in the case of autoassociation where \(=^{}\)).

The energy function requires the gradient of the separation function to be nonzero, which is not the case for \(\) (as defined in Equation 5). A few adjustments thus need to be made to \(\) in order to derive the energy function for the DND. Let \((x)\) denote the Sigmoid function, defined as:

\[(x)=} \]

An adjusted Sigmoid function with threshold \(\) and steepness parameter \(>0\), is used to define a continuous approximation of \(\):

\[_{i}() =_{i}(_{i}-) \] \[_{i} =_{j}(_{i,j},_{i}). \]A second Sigmoid function is used to count the number of selected dimensions (\(/\) entries higher than \(1/2\)) and adjust \(\) to ensure there are only \(k\) of them:

\[^{t+1}=^{t}+-k+_{i=1}^{n}_{k} _{i}}{_{i}}- , \]

where \(0<<1\) governs the threshold dynamics and \(_{k}\) controls the steepness of the second Sigmoid. In Appendix A, we show that \(\) is convergent with \(<\,}\). Equation 13 has a nonzero gradient, and as \(\) and \(_{k}\) grow to infinity, it approaches Equation 5. Hence, it can be used for the separation function of Equation 11, providing DND with an energy function. A second energy function is the update of \(\):

\[E=()=-k+_{i=1}^{n}_{k} (_{i}-)-  \]

## 3 Associative memory performance of the Differentiable Neural Dictionary

In the previous section, the differentiable neural dictionary of Neural Episodic Control has been shown to be a Universal Hopfield Network. In principle, DND can thus be used as an associative memory. In this section, the MNIST, CIFAR10 and Tiny ImageNet datasets are used to test the robustness and capacity of DND as an associative memory model, using the same methods as for the other UHN instances [(7)] unless otherwise mentioned2.

Example reconstructions of images by DND are shown in Figure 2, as well as Figures 8 and 7. Memories are separated by keeping the \(k\)-nearest neighbors only (\(k\)-Max separation function). For simplicity, the \(kd\)-tree of the original Neural Episodic Control implementation is not used, nor is the continuous \(k\)-Max version that makes use of Equation 13. Instead, all similarity scores are computed and those not selected are zeroed out (Equation 5). In Figure 2, using \(k=50\) like in the original Neural Episodic Control publication [(6)] gives an output from which the original image can be recognized, but the model is unable to properly separate memories and the resulting output is blurry. The Max separation function is equivalent to selecting the nearest neighbor (\(k=1\)) and provides a much clearer output. In fact, Max is the best separation function benchmarked for UHN [(7)]. Even when the output is blurry, the \(k>1\) models seem to properly capture the statistics of the dataset, such as the fact that central pixels are globally more active than those on the borders. Recall accuracy is typically assessed in absolute terms, by checking that the difference between output and response is below a threshold [(7)]. It is worth exploring whether the statistical modeling capacities of \(k>1\) can consistently improve performance with this criterion, despite the Max function having theoretically unbounded capacity with respect to the dimensionality of the query [(7)]. On the other hand, a new criterion will be introduced to evaluate recall in relation to other stored patterns, given that the main function of episodic memory is to reconstruct information corresponding to a particular situation (memorization) rather than to generalize.

Throughout the paper, we distinguish between two key aspects of associative memory models, adhering to the terms of [(7)]: capacity and retrieval. Capacity refers to the number of unique images (or memories) that can be stored while maintaining accurate recall. Retrieval, on the other hand, focuses on the model's performance in recalling these stored images when they are presented with incomplete or noisy cues. It measures the resilience of memory recall in the presence of distortion.

### Capacity with different functions

In DND, the similarity between memories and the query is computed using a Euclidean function (Equation 3). While rarely used in associative memory models, this function was found to outperform the more common dot product [(7)]. An even better performing similarity function was the inverse of the Manhattan distance:

\[_{i}(,)=(_{ij}-_{j})} \]The capacity of the model under different similarity (Euclidean and Manhattan) and separation functions is assessed by quantifying correctly retrieved data when increasing number of MNIST, CIFAR10 and Tiny ImageNet images is stored (Figures 3 and 9 ; Table 1). Half-masked images are given as input, and a trial is correct if the sum of squared pixel differences between the output and the actual image is less than a threshold of 50. The Manhattan similarity function outperforms the Euclidean function, especially when \(k\) is low. Furthermore, the best \(k\) value is highly dependent on the dataset. In MNIST, the best \(k\) is 5 for both Euclidean and Manhattan functions. In CIFAR10, the best \(k\) is 2 with Euclidean similarity and 1 with the Manhattan function. In Tiny ImageNet, Max (\(k=1\)) outperforms other functions.

### Retrieval with different functions

In order to test robustness of the memory, the ability to recall memories from noisy cues is analyzed. Independent zero-mean Gaussian noise with variance \(\) is thus added to the query images pixelwise. Performance is evaluated using sets of 100 images.

Figure 3: Capacity of associative memory with different similarity and separation functions assessed with MNIST. Plots represent the means and standard deviations of 10 simulations. A trial is correct when the difference between the output and the actual memory is under a threshold.

Figure 2: Example reconstructions of noisy MNIST digits by DND. The top row shows the input cue with increasing amount of noise. The following rows show the reconstruction of the stored memory using \(k=1,5,50\).

Like capacity, the best \(k\) for retrieval depends on the dataset (Figure 10 Table 2). Here again, the performance is better with the Manhattan similarity than with the Euclidean similarity for low \(k\), and worse for high \(k\). In MNIST, the best \(k\) is 50 with both Euclidean and Manhattan similarities. In CIFAR10, 2 is the best \(k\). Like for capacity, Max outperforms other functions with Tiny ImageNet images.

As hypothesized, Max is not always the best performing function with the absolute accuracy criterion, both for capacity and retrieval tasks. It can indeed be outperformed by higher \(k\) values, meaning that taking into account more memories than the single most similar one can lead to more precise recall in absolute terms (i.e. as assessed by a threshold). The performance of the identity and 50-Max functions in the MNIST dataset are surprisingly good, even under very high levels of noise (Figure 3(a)). In fact, as pixel values are restricted to lie in the range \(\), it is very unlikely that enough information remains in the image to correctly identify it when \(>1\). Hence, a plausible explanation is that high \(k\) functions model the dataset such that they output a mixture of many images that is sometimes classified as correct retrieval, although it is not necessarily closer to the query image than

   Separator & MNIST & CIFAR10 & Tiny \\   \\  Max & \(0.739 0.14\) & \(0.220 0.18\) & \(\) \\
2-Max & \(0.826 0.11\) & \(\) & \(0.015 0.02\) \\
5-Max & \(\) & \(0.117 0.09\) & \(0.010 0.02\) \\
10-Max & \(0.838 0.08\) & \(0.095 0.08\) & \(0.010 0.02\) \\
50-Max & \(0.801 0.09\) & \(0.087 0.09\) & \(0.010 0.02\) \\ Identity & \(0.809 0.08\) & \(0.088 0.09\) & \(0.010 0.02\) \\   \\  Max & \(0.835 0.10\) & \(\) & \(\) \\
2-Max & \(0.886 0.08\) & \(0.369 0.20\) & \(0.011 0.02\) \\
5-Max & \(\) & \(0.106 0.08\) & \(0.010 0.02\) \\
10-Max & \(0.826 0.08\) & \(0.075 0.07\) & \(0.010 0.02\) \\
50-Max & \(0.775 0.11\) & \(0.067 0.06\) & \(0.010 0.02\) \\ Identity & \(0.804 0.09\) & \(0.063 0.06\) & \(0.010 0.02\) \\   

Table 1: Capacity of associative memory with different similarity and separation functions assessed with MNIST, CIFAR10 and Tiny ImageNet datasets. Reported are means and standard deviations of the 10 simulations of Figure 9. For each dataset and similarity function, the best performance is highlighted in bold.

Figure 4: Retrieval capability against increasing levels of noise. Plots represent the means and standard deviations of 10 simulations with different sets of MNIST images.

[MISSING_PAGE_FAIL:7]

and 14 with the memorization criterion. Except for the condition with CIFAR10, Euclidean similarity and absolute criterion (Figure 12(b)), the Softmax can always outperform \(k\)-Max.

## 4 Discussion

In this paper, DND, which has initially been introduced in the context of reinforcement learning (6), has been shown to be mathematically related to Hopfield Networks (1). The Universal Hopfield Network framework has recently been introduced to encompass the traditional Hopfield Network, modern variants and related models (7). These models recall memories with a common sequence of operations: similarity, separation and projection. It has been shown that retrieval from a DND is also done with these operations. Hence, a DND is an instance of the Universal Hopfield Network framework. For the sake of mathematical analysis, a continuous approximation of DND recall has

   Separator & MNIST & CIFAR10 & Tiny \\   \\  Max & \(\) & \(\) & \(\) \\
2-Max & \(\) & \(0.222 0.19\) & \(0.222 0.21\) \\
5-Max & \(0.256 0.24\) & \(0.132 0.13\) & \(0.112 0.14\) \\
10-Max & \(0.199 0.24\) & \(0.104 0.14\) & \(0.098 0.13\) \\
50-Max & \(0.171 0.25\) & \(0.086 0.14\) & \(0.095 0.14\) \\ Identity & \(0.169 0.25\) & \(0.085 0.14\) & \(0.095 0.14\) \\   \\  Max & \(\) & \(\) & \(\) \\
2-Max & \(0.790 0.14\) & \(0.486 0.25\) & \(0.505 0.36\) \\
5-Max & \(0.255 0.22\) & \(0.176 0.19\) & \(0.155 0.20\) \\
10-Max & \(0.187 0.22\) & \(0.103 0.15\) & \(0.119 0.18\) \\
50-Max & \(0.163 0.23\) & \(0.090 0.15\) & \(0.113 0.18\) \\ Identity & \(0.162 0.23\) & \(0.088 0.15\) & \(0.113 0.18\) \\   

Table 3: Capacity of associative memory with different similarity and separation functions assessed with MNIST, CIFAR10 and Tiny ImageNet datasets. Reported are means and standard deviations of the 10 simulations of Figure 11. For each dataset and similarity function, the best performance is highlighted in bold.

Figure 5: Capacity of associative memory with different similarity and separation functions assessed with MNIST. Plots represent the means and standard deviations of 10 simulations. Here, a trial is correct when the difference between the output and the actual memory is lower than the difference between the output and any other stored memory.

been proposed to comply with the requirements of the energy function of UHN and derive a second Lyapunov function of the dynamics.

This novel link connects the fields of associative memory and reinforcement learning. The similarity function of DND is Euclidean and has already been shown to yield high capacity (7). On the other hand, the \(k\)-nearest neighbor is not commonly used as a separation function for associative memory. One thing to note is that the time complexity of \(k\)-nearest neighbor search is \((n)\) when implemented with k-d trees (11). In contrast, the Softmax function has a time complexity of O(n). Thus, one of the present objectives was to assess the performance of the more efficient \(k\)-Max separation function.

Interestingly, \(k\) controls the degree of separation, and setting \(k=1\) is equivalent to using the Max function studied by (7). While having theoretically unbounded capacity, the Max function can transition sharply from one memory attractor to another when noise of increasing amplitude is added to the query. With Figure 2, it was hypothesized that higher \(k\) values could be better at modelling

   Separator & MNIST & CIFAR10 & Tiny \\   \\  Max & \(\) & \(0.574 0.45\) & \(\) \\
2-Max & \(\) & \(\) & \(0.579 0.44\) \\
5-Max & \(0.576 0.41\) & \(0.366 0.42\) & \(0.416 0.45\) \\
10-Max & \(0.522 0.39\) & \(0.288 0.40\) & \(0.314 0.41\) \\
50-Max & \(0.246 0.34\) & \(0.134 0.29\) & \(0.160 0.32\) \\ Identity & \(0.150 0.32\) & \(0.029 0.05\) & \(0.101 0.24\) \\   \\  Max & \(\) & \(\) & \(\) \\
2-Max & \(0.664 0.43\) & \(\) & \(\) \\
5-Max & \(0.523 0.36\) & \(0.316 0.37\) & \(0.352 0.40\) \\
10-Max & \(0.422 0.31\) & \(0.177 0.25\) & \(0.228 0.33\) \\
50-Max & \(0.063 0.04\) & \(0.026 0.02\) & \(0.030 0.03\) \\ Identity & \(0.022 0.02\) & \(0.012 0.00\) & \(0.012 0.01\) \\   

Table 4: Retrieval capability against noise. Reported are means and standard deviations of the 10 simulations of Figure 12. For each dataset and similarity function, the best performance is highlighted in bold.

Figure 6: Retrieval capability as a function of \(k\) and \(\) parameters of the \(k\)-Max and Softmax separation functions respectively. In each simulation, 100 MNIST images are encoded, then queried with a noise of 1. Plots represent the means and standard deviations of 10 simulations with different sets of images.

datasets and improve the performance assessed in absolute terms. Simulations indeed revealed higher capacity and better retrieval from noisy queries with \(k>1\), especially with simple datasets like MNIST. However, these results depend on the way performance is evaluated. Traditionally, the evaluation of retrieval is based on some distance evaluation of the memory output and the actual image, which must not exceed some threshold fixed by the experimenter. This is a widespread method for evaluating associative memory models, but one must choose the threshold wisely, as setting it too high can result in false positives with the model grossly reproducing statistics of the dataset (generalization). This for example seems to be the case when retrieval is assessed using the MNIST dataset. The performance of 50-Max (and even the identity function) remains high despite very strong noise. Therefore, another way of evaluating retrieval was introduced, which does not consider the output in absolute terms, but rather compares it to the whole memory set. Retrieval is deemed correct if and only if the output resembles the actual image more than any other stored memory. Memory models thus cannot benefit from modeling statistics of the dataset, and must rather focus on recalling the distinguishing characteristics of the query (memorization). Using this method, the Max function outperforms the others. Ideally, performance should be evaluated in both absolute and relative terms to ensure that recall is accurate and stands out from other memories.

This raises the question of what is the function of associative memory. Modeling statistics of a dataset is related to generalization, which is typically the main goal of machine learning. The objective of associative memory is somewhat different. Instead of generalizing, an associative memory aims to recall the exact information corresponding to the individual memory. This is reminiscent of the division of labor between episodic and semantic memory [(12)]. When it comes to episodic control however, that is the use of episodic memories for action control, some generalization is desirable. This is especially the case in Neural Episodic Control in which the selection of actions only relies on episodes, the DND thus constituting a bottleneck. Initially, episodic control (not to be confused with its implementation in Neural Episodic Control) has been introduced as a way of speeding up the learning of reinforcement agent and, after the initial episodic control phase, it is desirable that more robust controllers can take over [(5)]. A biologically inspired alternative to Neural Episodic Control would be to supplement episodic memory with other controllers whose function is to generalize. The episodic memory would then no longer be a bottleneck, and could instead be devoted to memorizing the specifics of situations. That being said, there is also an ongoing debate about the fact that episodic memory could also integrate a part of generalization and not only store the specificities of episodes [(13; 14; 15)].

## 5 Limitations and Future Work

In this paper, we mainly focused on evaluating the capacity and retrieval performance of associative memory models. Conversely to the application of DND to associative memory, the novel theoretical link also implies that any instance of UHN can be used for episodic control. It is possible that the Manhattan function could consistently improve sample efficiency, outperforming the Euclidean kernel of DND, as it does on the associative memory tasks. The Softmax function, which has been proven powerful in transformers [(16)], and more performant than \(k\)-Max in the present study, could also improve episodic control agents. RL experiments are being conducted in this regard.

Finally, the fact that DND is theoretically related to Hopfield Networks provides a biological basis to Neural Episodic Control, as the most influential model of the hippocampus relies heavily on similar associative memory mechanisms [(17)]. Hence, this study opens up new avenues of research at the frontier of the fields of associative memory, reinforcement learning and neuroscience.