# Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences

Minsu Kim\({}^{1}\)  Federico Berto\({}^{1}\)  Sungsoo Ahn\({}^{2}\)  Jinkyoo Park\({}^{1}\)

\({}^{1}\)Korea Advanced Institute of Science and Technology (KAIST)

\({}^{2}\)Pohang University of Science and Technology (POSTECH)

{min-su, fberto, jinkyoo.park}@kaist.ac.kr

{sungsoo.ahn}@postech.ac.kr

###### Abstract

We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential design tasks. We provide reproducible source code: [https://github.com/kaist-silab/bootgen](https://github.com/kaist-silab/bootgen).

## 1 Introduction

The automatic design of biological sequences, e.g., DNA, RNA, and proteins, with a specific property, e.g., high binding affinity, is a vital task within the field of biotechnology . To solve this problem, researchers have developed algorithms to optimize a biological sequence to maximize a score function . Here, the main challenge is the expensive evaluation of the score function that requires experiments in a laboratory setting or clinical trials.

To resolve this issue, recent works have investigated offline model-based optimization [32, 21, 44, 52, 12, 45, MBO]. Given an offline dataset of biological sequences paired with scores, offline MBO algorithms train a proxy for the score function, e.g., a deep neural network (DNN), and maximize the proxy function without querying the true score function. Therefore, such offline MBO algorithms bypass the expense of iteratively querying the true score function whenever a new solution is proposed. However, even optimizing such a proxy function is challenging due to the vast search space over the biological sequences.

On the one hand, several works  considered applying gradient-based maximization of the proxy function. However, when the proxy function is parameterized using a DNN, these methods often generate solutions where the true score is low despite the high proxy score. This is due to the fragility of DNNs against _adversarial_ optimization of inputs . Furthermore, the gradient-based methods additionally require reformulating biological sequence optimization as a continuous optimization, e.g., continuous relaxation  or mapping discrete designs to a continuous latent space .

On the other hand, one may consider training deep generative models to learn a distribution over high-scoring designs [32; 29]. They learn to generate solutions from scratch, which amortizes optimization over the design space.

To be specific, Kumar and Levine  suggests learning an inverse map from a score to a solution with a focus on generating high-scoring solutions. Next, Jain et al.  proposed training a generative flow network [7; GFN] as the generative distribution of high-scoring solutions.

ContributionIn this paper, we propose a bootstrapped training of score-conditioned generator (BootGen) for the offline design of biological sequences. Our key idea is to enhance the score-conditioned generator by suggesting a variation of the classical ensemble strategy of bootstrapping and aggregating. We train multiple generators using bootstrapped datasets from training and combine them with proxy models to create a reliable and diverse sampling solution.

In the bootstrapped training, we aim to align a score-conditioned generator with a proxy function by bootstrapping the training dataset of the generator. To be specific, we repeat multiple stages of (1) training the conditional generator on the training dataset with a focus on high-scoring sequences and (2) augmenting the training dataset using sequences that are sampled from the generator and labeled using the proxy function. Intuitively, our framework improves the score-to-sequence mapping (generator) to be consistent with the sequence-to-score mapping (proxy function), which is typically more accurate.

When training the score-conditioned generator, we assign high rank-based weights  to high-scoring sequences. Sequences that are highly ranked among the training dataset are more frequently sampled to train the generator. This leads to shifting the training distribution towards an accurate generation of high-scoring samples. Compared with the value-based weighting scheme previously proposed by Kumar and Levine , the rank-based weighting scheme is more robust to the change of training dataset from bootstrapping.

To further boost the performance of our algorithm, we propose two post-processing processes after the training: filtering and diversity aggregation (DA). The filtering process aims to filter samples from generators using the proxy function to gather samples with cross-agreement between the proxy and generator. On the other hand, DA collects sub-samples from multiple generators and combines them into complete samples. DA enables diverse decision-making with reduced variance in generating quality, as it collects samples from multiple bootstrapped generators.

We perform extensive experiments on six offline biological design tasks: green fluorescent protein design [54, GFP], DNA optimization for expression level on an untranslated region [41; UTR], transcription factor binding [5; TFBind8], and RNA optimization for binding to three types of transcription factors [33; RNA-Binding]. Our BootGen demonstrates superior performance, surpassing the 100th percentile score and 50th percentile score of the design-bench baselines , a generative flow network (GFN)-based work  and bidirectional learning method (BDI) . Furthermore, we additionally verify the superior performance of BootGen in various design scenarios, particularly when given a few opportunities to propose solutions.

## 2 Related Works

### Automatic Design of Biological Sequences

Researchers have investigated machine learning methods to automatically design biological sequences, e.g., Bayesian optimization [50; 6; 36; 38; 43], evolutionary methods [3; 8; 18; 4; 40], model-based reinforcement learning , and generative methods [9; 32; 20; 29; 19; 11; 14]. These methods aim to optimize biological sequence (e.g., protein, RNA, and DNA) for maximizing the target objective of binding activity and folding, which has crucial application in drug discovery and health care .

### Offline Model-based Optimization

Offline model-based optimization (MBO) aims to find the design \(\) that maximizes a score function \(f()\), using only pre-collected offline data. The most common approach is to use gradient-based optimization on differentiable proxy models trained on an offline dataset [49; 44; 52; 21; 12]. However, they can lead to poor scores due to the non-smoothness of the proxy landscape. To address this issue, Trabucco et al.  proposed conservative objective models (COMs), which use adversarial training to create a smooth proxy. Yu et al.  suggests the direct imposition of a Gaussian prior on the proxy model to create a smooth landscape and model adaptation to perform robust estimation on the specific set of candidate design space. While these methods are effective for high-dimensional continuous design tasks, their performance on discrete spaces is often inferior to classical methods, e.g., gradient ascent .

### Bootstrapping

Bootstrapping means maximizing the utilization of existing resources. In a narrow sense, it refers to a statistical method where the original dataset is repeatedly sampled to create various datasets . In a broader sense, it also encompasses concepts frequently used in machine learning to improve machine learning scenarios where the label is expensive, e.g., self-training and semi-supervised learning . These methods utilize an iterative training scheme to augment the dataset with self-labeled samples with high confidence.

The bootstrapping strategy at machine learning showed great success in various domains, e.g., fully-labeled classification , self-supervised learning  and offline reinforcement learning . We introduce a novel bootstrapping strategy utilizing score-conditioned generators and apply it to offline biological sequence design, addressing the challenge of working with a limited amount of poor-quality offline datasets.

### Design by Conditional Generation

Conditional generation is a promising method with several high-impact applications, e.g., class-conditional image generation , language-to-image generation , reinforcement learning . With the success of conditional generation, several studies proposed to use it for design tasks, e.g., molecule and biological sequence design.

Hottung et al.  proposed an instance-conditioned variational auto-encoder  for routing problems, which can generate near-optimal routing paths conditioned on routing instances. Igashov et al.  suggested a conditional diffusion model to generate 3D molecules given their fragments. Specifically, the molecular fragments are injected into the latent space of the diffusion model, and the diffusion model generates links between fragments to make the 3D molecular compound.

## 3 Bootstrapped Training of Score-Conditioned Generator (BootGen)

Problem definitionWe are interested in optimizing a biological sequence \(\) to maximize a given score function \(f()\). We consider an offline setting where, during optimization, we do not have access to the score function \(f()\). Instead, we optimize the biological sequences using a static dataset \(=\{(_{n},y_{n})\}_{n=1}^{N}\) consisting of offline queries \(y_{n}=f(_{n})\) to the score function. Finally, we consider evaluating a set of sequences \(\{_{m}\}_{m=1}^{M}\) as an output of offline design algorithms.

Overview of BootGenWe first provide a high-level description of our bootstrapped training of score-conditioned generator, coined BootGen. Our key idea is to align the score-conditioned generation with a proxy model via bootstrapped training (i.e., we train the generator on sequences labeled using the proxy model) and aggregate the decisions over multiple generators and proxies for reliable and diverse sampling of solutions.

Figure 2.1: Illustration of the bootstrapped training process for learning score-conditioned generator.

Before BootGen training, we pre-train proxy score function \(f_{}() f()\) only leveraging offline dataset \(\). After that, our BootGen first initializes a training dataset \(_{}\) as the offline dataset \(\) and then repeats the following steps:

1. [label=**A.**]
2. BootGen optimizes the score-conditioned generator \(p_{}(|y)\) using the training dataset \(_{}\). During training, it assigns rank-based weights to each sequence for the generator to focus on high-scoring samples.
3. BootGen bootstraps the training dataset \(_{}\) using samples from the generator \(p_{}(|y^{})\) conditioned on the desired score \(y^{}\). It uses a proxy \(f_{}()\) of the score function to label the new samples.

After BootGen training for multiple score-conditioned generators \(p_{_{1}}(|y),...,p_{_{n}}(|y)\), we aggregate samples from the generators with filtering of proxy score function \(f_{}\) to generate diverse and reliable samples. We provide the pseudo-code of the overall procedure in Algorithm 1 for training and Algorithm 2 for generating solutions from the trained model.

```
1:Input: Offline dataset \(=\{_{n},y_{n}\}_{n=1}^{N}\).
2: Update \(\) to minimize \(_{(,y)}(f_{}()-y)^{2}\).
3:for\(j=1,,N_{}\)do\(\) Training multiple generators
4: Initialize \(_{}\).
5:for\(i=1,,I\)do\(\) Rank-based weighted training
6: Update \(_{j}\) to maximize \(_{(,y)_{}}w(y,_{}) p_{ _{j}}(|y)\).
7: Sample \(^{*}_{} p_{_{j}}(|y^{})\) for \(=1,,L\). \(\) Bootstrapping
8: Set \(y^{*}_{} f_{}(x^{*}_{})\) for \(=1,,L\).
9: Set \(_{}\) as top-\(K\) scoring samples in \(\{^{*}_{},y^{*}_{}\}_{=1}^{L}\).
10: Set \(_{}_{}_{ {aug}}\).
11:endfor
12:endfor
13:Output: trained score-conditioned generators \(p_{_{1}}(|y),...,p_{_{N_{}}}(|y)\).
```

**Algorithm 1** Bootrapped Training of Score-conditioned generators

### Rank-based Weighted Training

Here, we introduce our framework to train the score-conditioned generator. Our algorithm aims to train the score-conditioned generator with more focus on generating high-scoring designs. Such a goal is helpful for bootstrapping and evaluation of our framework, where we query the generator conditioned on a high score.

Given a training dataset \(_{}\), our BootGen minimizes the following loss function:

\[()-_{(,y)_{}}w(y, _{}) p_{}(|y), w(y,_{ })=_{}|+(y,_{ }))^{-1}}{_{(,y)_{}}(k|_{ }|+(y,_{}))^{-1}}.\]

where \(w(y,_{})\) is the score-wise rank-based weight . Here, \(k\) is a weight-shifting factor, and \((y,_{})\) denotes the relative ranking of a score \(y\) with respect to the set of scores in the dataset \(_{}\). We note that a small weight-shifting factor \(k\) assigns high weights to high-scoring samples.

For mini-batch training of the score-conditioned generator, we approximate the loss function \(()\) via sampling with probability \(w(y,_{})\) for each sample \((,y)\).

We note that Tripp et al.  proposed the rank-based weighting scheme for training unconditional generators to solve online design problems. At a high level, the weighting scheme guides the generator to focus more on generating high-scoring samples. Compared to weights that are proportional to scores , using the rank-based weights promotes the training to be more robust against outliers, e.g., samples with abnormally high weights. To be specific, the weighting factor \(w(y,)\) is less affected by outliers due to its upper bound that is achieved when \((y,)=1\).

### Bootstrapping

Next, we introduce our bootstrapping strategy to augment a training dataset with high-scoring samples that are collected from the score-conditioned generator and labeled using a proxy model. Our key idea is to enlarge the dataset so that the score-conditioned generation is consistent with predictions of the proxy model, in particular for the high-scoring samples. This enables self-training by utilizing the extrapolation capabilities of the generator and allows the proxy model to transfer its knowledge to the score-conditioned generation process.

We first generate a set of samples \(_{1}^{*},,_{L}^{*}\) from the generator \(p_{}(|y^{})\) conditioned on the desired score \(y^{}\)1 Then we compute the corresponding labels \(y_{1}^{*},,y_{L}^{*}\) using the proxy model, i.e., we set \(y_{}=f_{}(_{})\) for \(=1,,L\). Finally, we augment the training dataset using the set of top-\(K\) samples \(_{}\) with respect to the proxy model, i.e., we set \(_{}_{}\) as the new training dataset \(_{}\).

### Aggregation Strategy for Sample Generation

Here, we introduce additional post-hoc aggregation strategies that can be used to further boost the quality of samples from our generator. See Algorithm 2 for a detailed process.

FilteringWe follow Kumar and Levine  to exploit the knowledge of the proxy function for filtering high-scoring samples from the generator. To be specific, when evaluating our model, we sample a set of candidate solutions and select the top samples with respect to the proxy function.

Diverse aggregationTo enhance the diversity of candidate samples while maintaining reliable generating performances with low variance, we gather cross-aggregated samples from multiple score-conditioned generators. These generators are independently trained using our proposed bootstrapped training approach. Since each bootstrapped training process introduces high randomness due to varying training datasets, combining the generative spaces of multiple generators yields a more diverse space compared to a single generator.

Moreover, this process helps reduce the variance in generating quality. By creating ensemble candidate samples from multiple generators, we ensure stability and mitigate the risk of potential failure cases caused by adversarial samples. These samples may receive high scores from the proxy function but have low actual scores. This approach resembles the classical ensemble strategy known as "bagging," which aggregates noisy bootstrapped samples from decision trees to reduce variances.

## 4 Experiments

We present experimental results on six representative biological sequence design tasks to verify the effectiveness of the proposed method. We also conduct ablation studies to verify the effectiveness of each component in our method. For training, we use a single GPU of NVIDIA A100, where the training time of one generator is approximately 10 minutes.

### Experimental Setting

**Tasks.** We evaluate an offline design algorithm by (1) training it on an offline dataset and (2) using it to generate \(128\) samples for high scores. We measure the \(50\)th percentile and \(100\)th percentile scores of the generated samples. All the results are measured using eight independent random seeds.

We consider six biological sequence design tasks: green fluorescent protein (GFP), DNA optimization for expression level on untranslated region (UTR), DNA optimization tasks for transcription factor binding (TFBind8), and three RNA optimization tasks for transcription factor binding (RNA-Binding-A, RNA-Binding-B, and RNA-Binding-C). The scores of the biological sequences range in \(\). We report the statistics of the offline datasets used for each task in Table A.1. We also provide a detailed description of the tasks in Appendix A.1.

BaselinesWe compare our BootGen with the following baselines: gradient ascent with respect to a proxy score model [45, Grad.], REINFORCE , Bayesian optimization quasi-expected-improvement [50, BO-qEI], covariance matrix adaptation evolution strategy [24, CMA-ES], conditioning by adaptative sampling [9, CbAS], autofocused CbAS [20, Auto. CbAS], model inversion network [32, MIN], where these are in the official design bench . We compare with additional baselines of conservative objective models [44, COMs], generative flow network for active learning [29, GFN-AL] and bidirectional learning [12, BDI].

ImplementationWe parameterize the conditional distribution \(p_{}(x_{t}|_{1:t-1},y)\) using a \(2\)-layer long short-term memory [26, LSTM] network with \(512\) hidden dimensions. The condition \(y\) is injected into the LSTM using a linear projection layer. We parameterize the proxy model using a multi-layer perceptron (MLP) with \(2048\) hidden dimensions and a sigmoid activation function. Our parameterization is consistent across all the tasks. We provide a detailed description of the hyperparameters in Appendix A. We also note the importance of the desired score \(y^{}\) to condition during bootstrapping and evaluation. In this regard, we set it as the maximum score that is achievable for the given problem, i.e., we set \(y^{}=1\). We assume that such a value is known following .

   Method & RNA-A & RNA-B & RNA-C & TFBind8 & GFP & UTR & Avg. \\  \(\) (best) & 0.120 & 0.122 & 0.125 & 0.439 & 0.789 & 0.593 & 0.365 \\ REINFORCE  & 0.159 \(\) 0.011 & 0.162 \(\) 0.007 & 0.177 \(\) 0.011 & 0.450 \(\) 0.017 & 0.845 \(\) 0.003 & 0.575 \(\) 0.018 & 0.395 \\ CMA-ES  & 0.558 \(\) 0.012 & 0.531 \(\) 0.010 & 0.535 \(\) 0.012 & 0.526 \(\) 0.017 & 0.047 \(\) 0.000 & 0.497 \(\) 0.009 & 0.449 \\ BO-qEI  & 0.389 \(\) 0.009 & 0.397 \(\) 0.015 & 0.391 \(\) 0.012 & 0.439 \(\) 0.000 & 0.246 \(\) 0.341 & 0.571 \(\) 0.000 & 0.406 \\ CbAS  & 0.246 \(\) 0.008 & 0.267 \(\) 0.021 & 0.281 \(\) 0.015 & 0.467 \(\) 0.008 & 0.852 \(\) 0.004 & 0.566 \(\) 0.018 & 0.447 \\ Auto. CbAS  & 0.241 \(\) 0.022 & 0.237 \(\) 0.009 & 0.193 \(\) 0.007 & 0.413 \(\) 0.012 & 0.847 \(\) 0.003 & 0.563 \(\) 0.019 & 0.420 \\ MTN  & 0.146 \(\) 0.009 & 0.143 \(\) 0.007 & 0.174 \(\) 0.007 & 0.417 \(\) 0.012 & 0.830 \(\) 0.011 & 0.586 \(\) 0.000 & 0.383 \\ Grad  & 0.473 \(\) 0.025 & 0.462 \(\) 0.016 & 0.393 \(\) 0.007 & 0.513 \(\) 0.007 & 0.763 \(\) 0.181 & 0.611 \(\) 0.000 & 0.531 \\ COMs  & 0.172 \(\) 0.026 & 0.184 \(\) 0.009 & 0.228 \(\) 0.061 & 0.512 \(\) 0.051 & 0.737 \(\) 0.262 & 0.608 \(\) 0.000 & 0.407 \\ AdaLead  & 0.407 \(\) 0.018 & 0.353 \(\) 0.029 & 0.326 \(\) 0.019 & 0.485 \(\) 0.013 & 0.186 \(\) 0.216 & 0.592 \(\) 0.002 & 0.392 \\ GFN-AL  & 0.312 \(\) 0.013 & 0.300 \(\) 0.012 & 0.324 \(\) 0.009 & 0.538 \(\) 0.045 & 0.051 \(\) 0.003 & 0.597 \(\) 0.021 & 0.354 \\ BDI  & 0.411 \(\) 0.000 & 0.308 \(\) 0.000 & 0.345 \(\) 0.000 & 0.595 \(\) 0.000 & 0.837 \(\) 0.010 & 0.527 \(\) 0.000 & 0.504 \\  BootGen & **0.707**\(\) 0.005 & **0.717**\(\) 0.006 & **0.596**\(\) 0.006 & **0.833**\(\) 0.007 & **0.853**\(\) 0.017 & **0.701**\(\) 0.004 & **0.731** \\   

Table 4.1: Experimental results on 100th percentile scores. The mean and standard deviation are reported for 8 independent solution generations. \(\)(best) indicate the maximum score of the offline dataset. The best-scored value is marked in bold.

   Method & RNA-A & RNA-B & RNA-C & TFBind8 & GFP & UTR & Avg. \\  \(\) (best) & 0.120 & 0.122 & 0.125 & 0.439 & 0.789 & 0.593 & 0.365 \\ REINFORCE  & 0.159 \(\) 0.011 & 0.162 \(\) 0.007 & 0.177 \(\) 0.011 & 0.450 \(\) 0.017 & 0.845 \(\) 0.003 & 0.575 \(\) 0.018 & 0.395 \\ CMA-ES  & 0.558 \(\) 0.012 & 0.531 \(\) 0.010 & 0.535 \(\) 0.012 & 0.526 \(\) 0.017 & 0.047 \(\) 0.000 & 0.497 \(\) 0.009 & 0.449 \\ BO-qEI  & 0.389 \(\) 0.009 & 0.397 \(\) 0.015 & 0.391 \(\) 0.012 & 0.439 \(\) 0.000 & 0.246 \(\) 0.341 & 0.571 \(\) 0.000 & 0.406 \\ CbAS  & 0.246 \(\) 0.008 & 0.267 \(\) 0.021 & 0.281 \(\) 0.015 & 0.467 \(\) 0.008 & 0.852 \(\) 0.004 & 0.566 \(\) 0.018 & 0.447 \\ Auto. CbAS  & 0.241 \(\) 0.022 & 0.237 \(\) 0.009 & 0.193 \(\) 0.007 & 0.413 \(\) 0.012 & 0.847 \(\) 0.003 & 0.563 \(\) 0.019 & 0.420 \\ MTN  & 0.146 \(\) 0.009 & 0.143 \(\) 0.007 & 0.174 \(\) 0.007 & 0.417 \(\) 0.012 & 0.830 \(\) 0.011 & 0.586 \(\) 0.000 & 0.383 \\ Grad  & 0.473 \(\) 0.025 & 0.462 \(\) 0.016 & 0.393 \(\) 0.007 & 0.513 \(\) 0.007 & 0.763 \(\) 0.181 & 0.611 \(\) 0.000 & 0.531 \\ COMs  & 0.172 \(\) 0.026 & 0.184 \(\) 0.009 & 0.228 \(\) 0.061 & 0.512 \(\) 0.051 & 0.737 \(\) 0.262 & 0.608 \(\) 0.000 & 0.407 \\ AdaLead  & 0.407 \(\) 0.018 & 0.353 \(\) 0.029 & 0.326 \(\) 0.019 & 0.485 \(\) 0.013 & 0.186 \(\) 0.216 & 0.592 \(\) 0.002 & 0.392 \\ GFN-AL  & 0.312 \(\) 0.013 & 0.300 \(\) 0.012 & 0.324 \(\) 0.009

### Performance Evaluation

In Table 4.1 and Table 4.2, we report the performance of our BootGen along with other baselines. One can observe how our BootGen consistently outperforms the considered baselines across all six tasks. In particular, one can observe how our BootGen achieves large gains in \(50\)th percentile metrics. This highlights how our algorithm is able to create a reliable set of candidates.

For TFbind8, which has a relatively small search space (\(4^{8}\)), having high performances on the 100th percentile is relatively easy. Indeed, the classical method of CMA-ES and Grad. gave pretty good performances. However, for the 50th percentile score, a metric for measuring the method's reliability, BDI outperformed previous baselines by a large margin. Our method outperformed even BDI and achieved an overwhelming score.

For higher dimensional tasks of UTR, even the 50th percentile score of BootGen outperforms the 100th percentile score of other baselines by a large margin. We note that our bootstrapping strategies and aggregation strategy greatly contributed to improving performances on UTR. For additional tasks of RNA, we achieved the best score for both the 50th percentile and the 100th percentile. This result verifies that our method is task-expandable.

### Varying the Evaluation Budget

In real-world scenarios, there may be situations where only a few samples can be evaluated due to the expensive score function. For example, in an extreme scenario, for the clinical trial of a new protein drug, there may be only one or two chances to be evaluated. As we measure the 50th percentile and 100th percentile score among 128 samples following the design-bench  at Tables 4.1 and 4.2, we also provide a 100th percentile score report at the fewer samples from 1 sample to the 128 samples to evaluate the model's robustness on the low-budget evaluation scenarios.

To account for this, we also provide a budget-performance graph that compares the performance of our model to the baselines using different numbers of evaluations. This allows us to observe the trade-off between performance and the number of samples generated. Note that we select baselines as the Top 5 methods in terms of average percentile 100 scores reported at Table 4.1.

Figure 4.1: Evaluation-performance graph to compare with representative offline biological design baselines. The number of evaluations \(K\) stands for the number of candidate designs to be evaluated by the Oracle score function. The average value and standard deviation error bar for 8 independent runs are reported. Our method outperforms other baselines at every task for almost all \(K\).

As shown in Fig. 4.1, our method outperforms every baseline for almost every evaluation budget. For the UTR task, our performance on a single evaluation budget gives a better score than the other baselines' scores when they have a budget of 128 evaluations. For RNA tasks, our method with an approximate budget of 30 achieves superior performance compared to other methods with a budget of 128. These results show that our method is the most reliable as its performance is most robust when the evaluation budget is limited.

### Average Score with Diversity

For biological sequence design, measuring the diversity and novelty of the generated sequence is also crucial . Following the evaluation metric of  we compare the performance of models in terms of diversity and novelty.

Here is measurement of diversity for sampled design dataset \(=\{_{1},...,_{M}\}\) from generator which is average of _Levenshtein distance_, denoted by \(d(_{i},_{j})\), between arbitrary two biological sequences \(_{i},_{j}\) from the generated design candidates \(\):

\[()|(||-1) }_{}_{\{\}}d (,).\]

Next, we measure the minimum distance from the offline dataset \(_{}\) which measures the novelty of generated design candidates \(\) as:

\[(,_{})=|}_{}_{_{}}d (,).\]

Our method surpasses all baselines, including GFN-AL , in the UTR task, as evidenced by the Pareto frontier depicted in Table 4.3 and Fig. 4.2. Given the highly dimensional nature of the UTR task and its expansive search space, the discovery of novel and diverse candidates appears to be directly related to their average score. This implies that extensive exploration of the high-dimensional space is crucial for improving scores in the UTR task.

   Methods & 100th Per. & 50th Per. & Avg. Score & Diversity & Novelty \\  MIN  & 0.691 \(\) 0.011 & 0.587 \(\) 0.012 & 0.554 \(\) 0.010 & 28.53 \(\) 0.095 & 18.32 \(\) 0.091 \\ CMA-ES  & 0.746 \(\) 0.018 & 0.498 \(\) 0.012 & 0.520 \(\) 0.013 & 24.69 \(\) 0.150 & 19.95 \(\) 0.925 \\ Grad.  & 0.682 \(\) 0.013 & 0.513 \(\) 0.007 & 0.521 \(\) 0.006 & 25.63 \(\) 0.615 & 16.89 \(\) 0.426 \\ GFN-AL  & 0.700 \(\) 0.015 & 0.602 \(\) 0.014 & 0.580 \(\) 0.014 & 30.89 \(\) 1.220 & 20.25 \(\) 2.272 \\  BootGen w/o DA. & 0.729 \(\) 0.074 & 0.672 \(\) 0.082 & 0.652 \(\) 0.081 & 17.83 \(\) 5.378 & 20.49 \(\) 1.904 \\ BootGen w/ DA. (_ours_) & **0.858 \(\) 0.003** & **0.701 \(\) 0.004** & **0.698 \(\) 0.001** & **31.57 \(\) 0.073** & **21.40 \(\) 0.057** \\   

Table 4.3: Experimental results on 100th percentile scores (100th Per.), 50th percentile scores (50th Per.), average score (Avg. Score), diversity, and novelty, among 128 samples of UTR task. The mean and standard deviation of 8 independent runs for producing 128 samples is reported. The best-scored value is marked in bold. The lowest standard deviation is marked as the underline. The DA stands for the diverse aggregation strategy.

Figure 4.2: Multi-objectivity comparison of diversity and novelty on the average score for the UTR task. Each datapoint for 8 independent runs is depicted.

It is worth noting that GFN-AL, which is specifically designed to generate diverse, high-quality samples through an explorative policy, secures a second place for diversity. Although GFN-AL occasionally exhibits better diversity than our method and achieves a second-place average score, it consistently delivers poor average scores in the GFP and RNA tasks Table 4.2. This drawback can be attributed to its high explorative policy, which necessitates focused exploration in narrow regions. In contrast, BootGen consistently produces reliable scores across all tasks Table 4.2. For a comprehensive comparison with GFN-AL, please refer to the additional experiments presented in Appendix C.

Diverse aggregation strategyOur diverse aggregation (DA) strategy significantly enhances diversity, novelty, and score variance, as demonstrated in Table 4.3. This is especially beneficial for the UTR task, which necessitates extensive exploration of a vast solution space, posing a substantial risk to the bootstrapped training process. In this context, certain bootstrapped generators may yield exceedingly high scores, while others may produce low scores due to random exploration scenarios. By employing DA, we combine multiple generators to generate candidate samples, thereby greatly stabilizing the quality of the bootstrapped generator.

### Ablation study

The effectiveness of our components, namely rank-based reweighting (RR), bootstrapping (B), and filtering (F), in improving performance is evident in Table 4.4. Across all tasks, these components consistently contribute to performance enhancements. The bootstrapping process is particularly more beneficial for high-dimensional tasks like UTR and GFP. This correlation is intuitive since high-dimensional tasks require a larger amount of data for effective exploration. The bootstrapped training dataset augmentation facilitates this search process by leveraging proxy knowledge. Additionally, the filtering technique proves to be powerful in improving scores. As we observed from the diverse aggregation and filtering, the ensemble strategy greatly enhances score-conditioned generators.

## 5 Future Works

Enhancing proxy robustnessWhile our bootstrapping method shows promise for offline bio-sequential design tasks, it has inherent technical limitations. The assumption that the generator produces superior data to the training dataset may backfire if the generator samples have poor quality designs and the proxy used is inaccurate. While the current aggregation strategy effectively manages this risk, we can address this limitation by utilizing robust learning methods of proxies such as conservative proxies modeling , robust model adaptation techniques , parallel mentoring proxies , and importance-aware co-teaching of proxies  for further improvement.

Enhancing architecture of BootGenOur approach primarily employs a straightforward architectural framework, with a primary emphasis on validating its algorithmic structures in the context of offline biological sequence design. To enhance the practical utility of our method, it will be advantageous to incorporate established and robust architectural paradigms, exemplified in works such as  and , into the framework of our method. One promising avenue for achieving this integration is the incorporation of pre-trained protein language models (pLMs) , akin to those expounded upon in .

   Components & RNA-A & RNA-B & RNA-C & Tfbind8 & UTR & GFP \\  \(\) & 0.388 \(\) 0.007 & 0.350 \(\) 0.008 & 0.394 \(\) 0.010 & 0.579 \(\) 0.010 & 0.549 \(\) 0.009 & 0.457 \(\) 0.044 \\ \{RR\} & 0.483 \(\) 0.006 & 0.468 \(\) 0.008 & 0.441 \(\) 0.010 & 0.662 \(\) 0.009 & 0.586 \(\) 0.008 & 0.281 \(\) 0.031 \\ \{RR, B\} & 0.408 \(\) 0.009 & 0.379 \(\) 0.009 & 0.417 \(\) 0.006 & 0.666 \(\) 0.009 & 0.689 \(\) 0.003 & 0.470 \(\) 0.034 \\ \{RR, F\} & 0.576 \(\) 0.005 & 0.586 \(\) 0.004 & 0.536 \(\) 0.007 & 0.833 \(\) 0.004 & 0.621 \(\) 0.003 & 0.783 \(\) 0.011 \\ \{RR, F, B\} & **0.607**\(\) 0.009 & **0.612**\(\) 0.005 & **0.554**\(\) 0.007 & **0.840**\(\) 0.004 & **0.698**\(\) 0.001 & **0.804**\(\) 0.002 \\   

Table 4.4: Ablation study for BootGen. The average score among 128 samples is reported. We make 8 independent runs to produce 128 samples where the mean and the standard deviation are reported. For every method, an aggregation strategy is applied by default. The best-scored value is marked in bold. The lowest standard deviation is underlined. The RR stands for rank-based reweighing, the B stands for bootstrapping, and the F stands for filtering.

Conclusion

This study introduces a novel approach to stabilize and enhance score-conditioned generators for offline biological sequence design, incorporating the classical concepts of bootstrapping and aggregation. Our novel method, named BootGen, consistently outperformed all baselines across six offline biological sequence design tasks, encompassing RNA, DNA, and protein optimization. Our strategy of bootstrapping and aggregation yielded remarkable improvements in achieving high scores, generating diverse samples, and minimizing performance variance.