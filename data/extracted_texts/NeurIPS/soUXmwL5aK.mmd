# Interpretable Generalized Additive Models for

Datasets with Missing Values

 Hayden McTavish

Department of Computer Science

Duke University

Durham, NC 27705

hayden.mctavish@duke.edu

&Jon Donnelly*

Department of Computer Science

Duke University

Durham, NC 27705

jon.donnelly@duke.edu

&Margo Seltzer

Department of Computer Science

University of British Columbia

Vancouver, BC V6T 1Z4

mseltzer@cs.ubc.ca

&Cynthia Rudin

Department of Computer Science

Duke University

Durham, NC 27705

cythia@cs.duke.edu

These authors contributed equally to this work.

###### Abstract

Many important datasets contain samples that are missing one or more feature values. Maintaining the interpretability of machine learning models in the presence of such missing data is challenging. Singly or multiply imputing missing values complicates the model's mapping from features to labels. On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity. We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through \(_{0}\) regularization. We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naive inclusion of indicator variables.

## 1 Introduction

Interpretability is essential for a wide range of machine learning applications Rudin et al. (2022). Missing data pose a challenge to interpretability, because many simple models (e.g., linear models) are not well-defined when data are missing. This raises the question: how can interpretability be maintained for datasets with missing values?

We introduce an interpretable model class, M-GAM, that extends Generalized Additive Models (GAMs) to handle missing data. GAMs take the form of a linear combination of univariate component functions, with one function corresponding to each feature; this univariate nature is the core reason for their interpretability (Rudin et al., 2022). We introduce two sets of boolean variables for each feature. The first consists of missingness indicators that identify which features have missing values. The second consists of missingness adjustment terms that adjust the shape curves for other features for each missing features in a sample. This _maintains our ability to view a GAM as a sum of univariate shape functions even when modeling interactions with missing data._ As such, an M-GAM is much simpler to interpret than a GAM built on imputed data, since it avoids creating multivariate features as happens when imputing features from multiple others. This is illustrated in Figure 1.

M-GAMs explicitly encourage sparsity. This reduces overfitting, which has been identified as a concern in prior work using missingness indicators (Van Ness et al., 2023), since realistic data may produce an overwhelming number of missingness indicators. Unlike prior methods that leverage missingness indicators, we use \(_{0}\) regularization (Liu et al., 2022; Dedieu et al., 2021; Hazimeh and Mazumder, 2020; Hazimeh et al., 2023), which directly optimizes for sparser, more interpretable models. Our ability to create sparse models allows us to include not only simple missingness indicators but also combinations of missingness indicator variables with M-GAM's missingness adjustment terms, without overfitting. Figure 2 illustrates an M-GAM fit on real data.

With this modeling framework, we make the following contributions: (1) We introduce M-GAM, a form of sparse generalized additive model that incorporates missingness directly into its reasoning. (2) We show that M-GAM provides substantial performance benefits relative to impute-then-predict models when synthetic missing-at-random (MAR) missingness is added to real datasets, while maintaining performance in real world settings with only naturally occurring missingness. (3) We show that M-GAM substantially reduces runtime relative to impute-then-predict methods built on multiple imputation while producing sparse, interpretable models.

## 2 Related Work

Missing data is a well studied problem in statistics. Traditionally, mechanisms by which data can be missing are organized into three categories: missing completely at random (MCAR), where missingness is independent of the value of all covariates; missing at random (MAR), where missingness in a variable \(X_{1}\) is conditionally independent of the value of \(X_{1}\) given all other variables; and missing not at random (MNAR), where missingness may depend on any variable (Little and Rubin, 2019).

For supervised learning, there are two common approaches to dealing with missing data: impute-then-predict, in which a standard machine learning model is fit on top of imputed data and used for prediction, and incorporating missing data handling directly in the predictive model.

A broad body of work studies imputation, particularly in the MAR setting - for a more thorough review, see Shadbahr et al. (2023). Imputation methods can broadly be sorted into single imputation methods (see Van Buuren, 2018, for a review of such methods), where each missing value is imputed once, and multiple imputation (Rubin, 1988; Van Buuren and Oudshoorn, 1999; Schafer and

Figure 1: A comparison of how GAMs that use underlying imputation (middle row) and M-GAMs (bottom row) behave when a feature is missing. **Top**: When no data are missing, the overall output logit for both models is the sum of three univariate shape functions. **Middle**: When \(X3\) is missing, it is imputed as \(X3=X1+2X2\), producing a 3D shape function that is difficult to understand. **Bottom**: M-GAM uses simple adjustments to existing univariate shape curves when \(X3\) is missing (using the green curves instead of the light blue ones), making its reasoning process simple to follow. If the data were more than 3 dimensional, we would not be able to visualize the model with imputation, but M-GAM would still be easily visualized.

Graham, 2002; Stekhoven & Buhlmann, 2012; Mattei & Frellsen, 2019), where many alternative imputations are provided for each missing value. Multiple imputation is convenient because it integrates uncertainty into its imputations by providing a range of alternatives (Van Buuren, 2018).

Recent approaches directly incorporate missing data in the predictive model. Le Morvan et al. (2020) showed that, even when the target of prediction is a linear function, in the presence of missing data, the optimal model need not be linear in the original features. Rather, their optimal model was linear in the observed data _and_ interactions between indicators for missing data and the observed data. Van Ness et al. (2023) showed that when missingness contains information about an outcome, linear models that directly include missingness indicators outperform models excluding this information. This inclusion of missingness indicators is especially recommended in the practical setting of predictive modeling, rather than the setting of statistical inference where MCAR, MAR and MNAR concepts are more commonly used (Sperrin et al., 2020).

There are also a wide variety of ad-hoc methods for handling missing data in tree-based models (Kapelner & Bleich, 2015; Twala et al., 2008; Beaulac & Rosenthal, 2020; Therneau et al., 1997) and boosting models (Wang & Feng, 2010; Chen & Guestrin, 2016) that involve prediction without explicit imputation. Beaulac & Rosenthal (2020) learn decision trees which avoid splitting on missing data when missingness follows a deterministic structure based on other known features. This sidesteps any need to query features when data is missing, but does not generalize to settings with less structured missingness. More generally, tree-based models can learn a branch direction for each split to use when the queried feature is unknown. This effectively imputes the response to the query, but keeps the model itself simple. This is the method used in XGBoost (Chen & Guestrin, 2016) and SKLearn's decision tree classifier (Pedregosa et al., 2011). Twala et al. (2008) and Kapelner & Bleich (2015) additionally incorporate the option to split on missingness itself, effectively encoding missingness as a value. Ding & Simonoff (2010) provides empirical support for incorporating this option to treat missingness as a value. We compare to XGBoost and SKLearn in our experimental section, finding that M-GAM better balances interpretability and performance, even when we allow missingness

Figure 2: A generalized additive model (GAM) for the Explainable ML Challenge data from FICO et al. (2018) with missingness incorporated. This model handles missingness interpretably by explicitly providing alternative shape functions when a variable is missing. For example, in this model the shape function for variable 2 is adjusted when variable 3 is missing, and the shape function for variable 3 is removed. This model achieves comparable performance to convoluted black box approaches (such as random forests and/or MICE), but provides global interpretability (the entire model can easily be inspected) and local interpretability (the shape functions applied for a given sample can be easily visualized). An expanded version of this figure with variable names can be found in Appendix Figure 16. Shape functions in the right section are shared across all missing variable combinations. The type of missingness is indicated in parentheses next to the missing variable. Section F visualizes additional M-GAMs.

indicator splits like in Twala et al. (2008); Kapelner and Bleich (2015); Ding and Simonoff (2010); Wang and Feng (2010). Most similarly to our own approach, Therneau et al. (1997) and Breiman (2017) discuss an alternative approach of surrogate splits: when a feature that is split on is missing, a set of other splits is used in place of the missing feature to evaluate the split. This practice of adjusting which features are used when one feature is missing bears some similarities to our use of missingness interaction splits that adjust the shape functions for some features when other features are missing, though these surrogate splits do not optimize for sparsity like M-GAM, and underperform more standard multiple imputation approaches Valdiviezo and Van Aelst (2015); Feelders (1999). Further work explores the idea of developing distinct models for use under different cases of missing features (Fletcher Mercaldo and Blume, 2020; Stempfle et al., 2023) or developing additive logical models with disjunctions, such that reliance on imputed values is low Stempfle and Johansson (2024).

It is critical to note that, for a dataset with \(d\) features, adding indicators for missing data results in \(2d\) features, and adding first order interactions between features and missingness results in \(d(d-1)+2d\) features. As such, without careful regularization, these models that explicitly handle missingness are complex and uninterpretable. This poses a challenge for their application in high stakes domains such as justice and medicine, where there have been calls to enshrine interpretability as a requirement for the use of machine learning methods (US Food and Drug Administration, 2021; European Commision, 2021). In contrast, M-GAM provides sparse, transparent models that handle missingness indicators and interactions by extending sparse generalized additive models (Liu et al., 2022). M-GAM provides an expressive model class for handling missingness while controlling the exploding number of missingness interaction terms through \(_{0}\) regularization.

## 3 Methodology

We denote a dataset of \(n\) samples by \(=(,)=\{(_{i},y_{i})\}_{i=1}^{n},\) where \(_{i}(\{\})^{d}\) is a \(d\)-dimensional vector of features, NA denotes a missing entry, and \(y_{i}\{0,1\}\) is our target label. We use \(x_{i,j}\) to denote the \(j\)-th feature of the \(i\)-th sample. We use bold capital letters (\(\)) to denote matrices, bold lowercase letters to denote vectors (\(_{i}\)), capital letters to denote random variables (\(X\)), and lowercase letters to represent scalars (\(x_{i,j}\)). \(\) denotes noise; any other Greek characters denote model parameters. We encode all binary comparisons to the value NA as \(0\). That is, we follow the convention that \(_{[ a]}=0\) for any value \(a\), where \(_{[]}\) denotes the indicator function.

Note that, in practice, data are often missing for distinct yet identifiable reasons; for example, a measurement for one sample may be missing because it was never taken, while another may be missing because a researcher spilled coffee on the notes containing the data. As such, we explicitly consider distinct reasons for missing data. For a dataset with \(c\) potential reasons for data to be missing, define the mapping \(mcat:\{\}\{0,1,,c\}\) to map from an entry of \(\) to a natural number indicating the reason that entry is missing (\(0\) if the entry is not missing).

With notation established, we begin with a motivational proposition. Proposition 3.1 states that even if we can perfectly impute missing values, we may find greater predictive power by using missingness itself as a feature rather than by imputing missing values.

**Proposition 3.1**.: _Let \(I:()^{d}^{d}\) be an oracle imputation function that replaces all missing values in a vector with the correct non-missing entry. For a random variable \(X^{d}\), let \(f_{1}(X):=_{[[Y|I(X)]>0.5]}\) be the Bayes' optimal model using perfectly imputed data and \(f_{2}(X):=_{[[Y|X]>0.5]}\) be the Bayes optimal model using missingness as a value. There exist data generating processes for \(X\) and \(Y\) where \(P(Y=f_{1}(X))<P(Y=f_{2}(X)).\)_

Section A of the appendix provides a proof by construction for Proposition 3.1. The key insight behind Proposition 3.1 is that, when missingness is dependent on the label \(Y,\) missingness itself can be a powerful predictor of the label (this setting is called informative missingness in Van Ness et al., 2023). In particular, we can gain information about our label that is not available in other covariates (e.g., information from \(_{1}\)).

Proposition 3.1 may appear to conflict with Theorem 3.1 of Le Morvan et al. (2021), which states that a Bayes optimal model may be produced using impute-then-predict with almost any imputation model. This theorem hinges on the idea that, for most imputations, it is still possible to distinguish imputed data entries from non-missing entries. This is not the case for perfect imputation, which yields Corollary 3.2.

**Corollary 3.2**.: _Let \((f,,)\) denote the risk of a model \(f\) for data \(,\), and \(^{*}\) the optimal risk. Let \(I:()^{d}^{d}\) denote the oracle imputation function of Proposition 3.1. Under perfect imputation, it is possible for there to be no Bayes optimal model built on imputed data. That is,_

\[(,)[ f:(f I,,)=^{*}].\]

Corollary 3.2 states that perfectly imputing missing data can reduce the best possible performance of a predictive model. This has substantial implications for how imputation is understood for prediction: if perfect imputation is achieved, then impute-then-predict models sacrifice expressiveness. If imputation is optimized to maintain the downstream performance of impute-then-predict models, the imputed data loses some of its meaning since it is no longer our "best guess" of the missing data's value, as we need to deliberately avoid perfect imputation to guarantee that we maintain performance.

Motivated by Proposition 3.1 and Corollary 3.2, we are interested in constructing predictive models that explicitly use missingness as a value in their prediction rather than imputing first. More generally, we may also consider using the indicator for each _type_ of missing data directly in a prediction. Generalized additive models (GAMs) provide a natural choice for such a model.

A GAM \(g:^{d}\) consists of a bias term \(_{0}\) and \(d\) shape functions \(f_{1},,f_{d}:\) parameterized by vectors \(_{1},,_{d}\). Given a sample \(_{i}\), a GAM forms a prediction as:

\[g(_{i};)=_{0}+_{j=1}^{d}f_{j}(x_{i,j};_{j}).\] (1)

In practice, it is common for each shape function to be a linear combination of different thresholds on its input variable, i.e., \(f_{j}(x_{i,j};_{j})=_{k=1}^{(_{j})}_{j,k} _{[x_{i,j} t_{j,k}]},\) where \((_{j})\) is the number of thresholds applied to variable \(j\), each \(t_{j,k}\) is a threshold value, and each \(_{j,k}\) is a learned weight.

These functions provide a convenient framework for considering missing values. In particular, we can form a new shape function \(h_{j}(x_{i,j};_{j},_{j}^{})\) that explicitly handles missing data by introducing additional "missingness indicator" terms, such that our shape functions take the form

\[h_{j}(x_{i,j};_{j},_{j}^{})=f_{j}(x_{i,j};_{j})+ _{m=1}^{c}_{j,m}^{}_{[mcat(x_{i,j})=m]},\]

where \(_{j}^{}^{c}\) is an additional vector of parameters. Recall that there are \(c\) distinct reasons for missingness, with \(mcat(x_{i,j})=0\) if \(x_{i,j}\) is not missing and \(mcat(x_{i,j})=m\) if \(x_{i,j}\) is missing for the \(m\)-th reason.

We may further extend this augmentation to include _interaction terms_ between _missingness indicators and standard threshold functions_. The "missingness interaction" function between feature \(j\) and feature \(j^{}\) takes the form

\[h_{j,j^{}}(x_{i,j},x_{i,j^{}};_{j,j^{}})=_{m=1}^{ c}_{k=1}^{(_{j})}_{j,j^{},k,m}_{[ mcat(x_{i,j})=mx_{i,j^{}} t_{j^{},k}]},\]

where each \(_{j,j^{},k,m}\) is a learned weight. We thus define a missingness-GAM (M-GAM) \(g_{}\) as follows:

**Definition 3.3**.: Given parameters \(\), \(^{}\), and \(\), an M-GAM is defined as

\[g_{}(_{i};,^{},)=_{0}+ _{j=1}^{d}\,h_{j}(x_{i,j};_{j},_{j}^{})+_{j=1}^ {d}_{j^{}=1}^{d}h_{j,j^{}}(x_{i,j},x_{i,j^{}};_{ j,j^{}}),\] (2)

where

\[h_{j,j^{}}(x_{i,j},x_{i,j^{}};_{j,j^{}})=_{m=1}^{ c}_{k=1}^{(_{j})}_{j,j^{},k,m}_{[ mcat(x_{i,j})=mx_{i,j^{}} t_{j^{},k}]}\]

and

\[h_{j}(x_{i,j};_{j},_{j}^{})=f_{j}(x_{i,j};_{j})+ _{m=1}^{c}_{j,m}^{}_{[mcat(x_{i,j})=m]}.\]These augmentation terms are quite powerful. Theorem 3.4 shows that, for any impute-then-predict approach using an affine imputer and a GAM predictor, we can construct an M-GAM that recovers the expected classification score over imputations.

**Theorem 3.4**.: _Consider any GAM \(g:^{d}\), parameterized by \(\), with shape functions defined as linear combinations over boolean features (either thresholds \(f_{j}(x_{i,j};_{j})=_{k=1}^{}(_{j})}_{j,k} _{[_{i,j} t_{j,k}]}\) or a feature that was originally boolean). Suppose some observations are missing boolean feature \(b\), and that this feature is imputed such that the modeled probability of \(x_{i,b}\) being true, \(}(x_{i,b}=1|_{i,-b})\) (where \(_{i,-b}\) refers to all covariates except \(b\)) is an affine function \(h:_{i,-b}\). For any parameterization \(\) of a GAM \(g\), let \([g(_{i};)]:=}(x_{i,b}=1|_{i,-b})g(_{i}^{(b+)};)+}(x_{i,b}=0|_{i, -b})g(_{i}^{(b-)};),\) where \(_{i}^{(b+)}\) denotes \(_{i}\) with \(x_{i,b}=1\) and \(_{i}^{(b-)}\) denotes \(_{i}\) with \(x_{i,b}=0\). Then, there exists a model in the model class M-GAM (which does not use imputations), that recovers this score \([g(_{i};)]\) for all \(i\)._

More broadly, Theorem 3.4 suggests that M-GAM is able to express scores comparable to those of any impute-then-predict GAM, if the imputation probabilities can be approximated by an additive model. One advantage is that M-GAM can be optimized directly for classification performance - rather than first optimizing an imputation step to recover missing values, and then optimizing a model on the imputed data. Together, Proposition 3.1 and Theorem 3.4 show that M-GAM is comparable to impute-then-predict in a broad range of settings and that M-GAM is strictly better than impute-then-predict in some settings. Appendix C contains the proof for Theorem 3.4.

### Sparsity

Building a GAM with missingness indicators and interaction terms provides superior expressive power but causes an explosion of the number of covariates the model must consider. The GAM in Equation (1) consists of \(_{j}^{d}(_{j})+1\) coefficients, while the M-GAM in Definition (3.3) consists of \(c_{j}^{d}(_{j^{} j}(_{j^{}}) )+_{j}^{d}((_{j}))+cd+1\) coefficients (or, \(d_{j}^{d}((_{j}))+d+1\) when \(c=1\)). This increases the risk of overfitting and may lead to complex, uninterpretable models. The same problem arises when adding similar interaction terms to a linear model, as diagnosed by Van Ness et al. (2023), who propose a hypothesis testing style framework for variable selection. Notably, this framework does not explicitly encourage sparsity - it only discourages overfitting.

Rather than applying a pre-processing step for variable selection, we use \(_{0}\) regularization, which we can optimize directly alongside accuracy. This encourages the model coefficients to be 0, resulting in sparse models despite the potentially large number of input features. We optimize classification performance using the exponential loss, as it yields faster convergence rates than logistic loss during optimization (Liu et al., 2022). Thus, our goal is to solve the following optimization problem:

\[_{,^{}},}_{i=1}^{n}e^ {-y_{i}g_{}(_{i};,^{}},)}+ _{0}(\|\|_{0}+\|^{}}\|_{0}+\|\|_{0}) ,\] (3)

where \(_{0}\) is a hyperparameter that determines the strength of the \(_{0}\) regularization.

To simplify (3) so it can be solved directly, we construct a new set of features \(}\{0,1\}^{c_{j}^{d}(_{j^{} j}( _{j^{}}))+_{j}^{d}((_{j}))+cd+1}\) consisting of the indicator for each threshold, missing value, and interaction term for each feature in the original dataset. For a large coefficient vector \(^{c_{j}^{d}(_{j^{} j}( _{j^{}}))+_{j}^{d}((_{j}))+cd+1}\) and bias coefficient \(_{0},\) our optimization problem becomes:

\[_{}_{i=1}^{n}e^{-y_{i}(^{T}_{i}+ _{0})}+_{0}\|\|_{0},\] (4)

which is solved using the optimization framework of Liu et al. (2022). This allows us to quickly produce sparse M-GAMs, overcoming the large number of input values.

## 4 Experiments

We now evaluate the performance, runtime, and sparsity of M-GAM in comparison to other methods. To evaluate M-GAM in a realistic setting, we require datasets with some missing entries. We primarily consider four datasets: the Explainable Machine Learning Challenge dataset (FICO et al., 2018) (referred to as FICO), a breast cancer dataset introduced by Razavi et al. (2018) (referred to as Breast Cancer), the MIMIC-III critical care dataset (Johnson et al., 2016) (referred to as MIMIC), and a dataset concerning the prediction of pharyngitis introduced by Miyagi (2023) (referred to as Pharyngitis). FICO contains 10,459 individuals, measuring 23 predictor variables used to predict whether each individual will repay a line of credit within 2 years. FICO contains three distinct encodings for missingness: -7, indicating no information of a given type is available, -8, indicating there was no usable information, and -9, indicating that a credit bureau report was not investigated or not found. Breast Cancer measures 27 features for 1,756 patients, MIMIC measures 49 features for 30,238 patients, and Pharyngitis measures 19 features for 676 patients. We use AUC, rather than accuracy, when evaluating model performance for Breast Cancer and MIMIC because these two datasets are heavily imbalanced. Breast Cancer, MIMIC, and Pharyngitis contain only one missingness encoding. Two additional datasets are studied in Section E of the appendix.

Each dataset contains missing entries. Because these are real datasets, we do not know the exact mechanism(s) (i.e., MCAR, MAR, or MNAR) by which data are missing. These datasets allow us to evaluate M-GAM on data with (Section 4.1) and without (Section 4.4) added MAR missingness.

We then study the interpretability/accuracy tradeoff for M-GAM using sparsity versus accuracy plots (Section 4.2) and evaluate the runtime of M-GAM (Section 4.3). We use multivariate imputation by chained equations (MICE) (Van Buuren and Oudshoorn, 1999), MIWAE(Mattei and Frellsen, 2019), and MissForest(Stekhoven and Buhlmann, 2012) as multiple imputation baselines.

We compare M-GAM to a variety of standard machine learning models used in an impute-then-predict framework. We further compare to standard machine learning models used with both the missingness augmentation described by Van Ness et al. (2023) and imputation. Section D.3 of the appendix contains full experimental details.

### M-GAM Provides Superior Performance Given Informative MAR Missingness

To demonstrate the added expressive capability of our model relative to impute-then-predict models, we created versions of each dataset with added synthetic missingness. Missingness is added to an arbitrary column of the data according to an MAR mechanism, where missingness is dependent on the outcome \(Y\) and one other randomly chosen predictor variable. We encode this added missingness as a new value distinct from the value(s) used to indicate missingness in the original dataset. A conditional probability table for this synthetic missingness is provided in Appendix Section D.1.

This adjustment falls under the MAR setting where imputation is often suggested. Nevertheless, as shown in Figure 3, M-GAM with interactions provides much greater accuracy than imputation, because the missingness depends on the outcome. The gain in performance due to considering interaction terms grows larger with increasing MAR missingness (from left to right).

Note that missingness depending on the outcome of interest is realistic. For example, individuals who are unlikely to have a particular disease are unlikely to receive medical tests related to that disease. One theory about why polls were wrong before the 2016 US presidential election was that non-response bias was associated with less education and distrust in the media, both predictors of votes for Donald Trump (Kurtzleben, 2016).

### M-GAM Achieves High Performance While Maintaining Sparsity

The most important benefit of the missingness handling in M-GAM is that it enables simple, sparse models. As such, we show the tradeoff between complexity and performance for M-GAM.

Figure 4 demonstrates the sparsity-accuracy trade-off for M-GAM relative to a GAM fit using Scikit Learn's logistic regression package (Pedregosa et al., 2011) over binarized features, trained on data from 10 imputations using a multiple imputation method. We also contrast two different levels of missing variable parameterization: the full set of indicators and interactions versus using just indicators.

We quantify interpretability using the number of nonzero coefficients selected by M-GAM, since a large number of non-zero coefficients leads to a dense, complicated mapping from the input data to a prediction. Meanwhile, running impute-then-predict is not interpretable: the method requires ensembling many different GAMs, and the imputations themselves introduce complicated relationships between the raw data and the classifications, similar to what was illustrated in Figure 1. We show that with fewer than 40 total coefficients (including all step functions for all variables), M-GAM can achieve accuracy comparable to that of GAMs with multiple imputation. On FICO, M-GAM - using just 20 non-zero coefficients - achieves superior accuracy to a variety of dense, complicated alternatives.

### M-GAM is Faster than Impute-then-Predict

We next turn to a runtime comparison between the impute-then-predict framework and M-GAM. For impute-then-predict models, we first imputed 10 datasets and recorded the time required to do so. We then fit a predictive model on each imputed training dataset and recorded the total time required. We recorded the time required to fit an M-GAM with missingness indicators and an M-GAM with missingness interactions for comparison. This was repeated for each of ten distinct train-test splits of the original dataset.

Figure 5 shows the runtime of our approach relative to each impute-then-predict baseline, as well as decision trees and random forests without imputation. M-GAM consistently produces models at least an order of magnitude more quickly than impute-then-predict with any non-trivial imputation. While decision trees without imputation tend to be produced faster than M-GAM, they tend to have lower accuracy than M-GAM as discussed in the next section. We repeat this experiment for four distinct subsamples of each dataset (1/4, 1/2, 3/4, and all of the data) to study how each method scales in the number of samples in Appendix E.3.

Figure 3: Sparsity of M-GAM when synthetic MAR missingness is added to up to \(25\%\) (left column) and \(50\%\) (right column) of entries in FICO (top row) and Breast Cancer (bottom row). We compare to several alternatives for GAMs with missing data: ensembling 10 GAMs fit on multiple imputation (for MIWAE, MICE, and MissForest), 0-value imputation (‘GAM”), mean-value imputation (‘GAM w/ MVI”), and selective addition of missingness indicators (‘SMIM”). The number of non-zero coefficients for multiple imputation cannot be evaluated because the models depend on both the GAM coefficients and the underlying imputation mechanisms, resulting in high dimensional shape functions as in Figure 1. Error bars report standard error over 10 train-test splits.

### M-GAM is as Accurate as Impute-then-Predict on Real Data

While M-GAM outperforms imputation on semi-synthetic data, there is a risk that this comes at the cost of performance on real data. To evaluate whether this is the case, we used several multiple imputation methods to impute 10 distinct datasets for each setting, then fit a variety of predictive models on these datasets. We used cross validation to select hyperparameters separately for each imputed training dataset and ensembled the resulting 10 models for each model class to produce a single predictive model for each model class. We repeated this procedure for ten distinct train-test splits for each dataset considered.

Figure 6 shows the test accuracy of each model. We find that, on real datasets, no alternative method substantially outperforms M-GAM. This suggests that M-GAM does not harm predictive performance on real datasets, while providing substantial benefits in interpretability (Section 4.2) and superior power under informative missingness (Section 4.1).

Figure 4: Test performance of three models at various levels of sparsity on the unaltered FICO and Breast Cancer datasets, with the same baselines as in Figure 3

Figure 5: Runtime of different methods on Breast Cancer, FICO, MIMIC, and Pharyngitis. For each imputation method, we report the total time required to impute missing data and fit the best performing impute-then-predict classifier for that dataset and imputation method. M-GAM (Ind) is an M-GAM with indicators and M-GAM (Int) is an M-GAM with indicators and interaction terms. Error bars report standard error of total runtime over 10 train-test splits.

## 5 Conclusion

We introduced M-GAM, a framework for producing accurate, sparse GAMs in the presence of missing data. We demonstrated that M-GAM achieves comparable accuracy to impute-then-predict on real datasets and superior accuracy under informative synthetic missingness. M-GAM produces models substantially more quickly than impute-then-predict models with multiple imputation, and provides simple, transparent reasoning on missing data.

While the \(_{0}\) penalty in M-GAM encourages sparsity, it is limited in that the \(_{0}\) regularization is applied uniformly across all coefficients. Consider the case when we are adding interaction terms to handle missingness. We might encourage the model to rely on features it is already using to predict y, rather than using new features. It may be more effective regularization - and more interpretable - to have a reduced \(_{0}\) penalty for such cases. Future work should investigate applying distinct levels of regularization to observed variables, missingness indicators, and missingness interactions when a variable is already included in the model.

An important caveat to models that reason on missing features is that missingness can be especially vulnerable to distribution shift, particularly in a medical domain (Sperrin et al., 2020; Groenwold, 2020). The interpretability enabled by M-GAM is crucial in allowing models to be closely monitored and adjusted in the presence of potential distribution shift. Future work could more thoroughly investigate potential distribution shift and ways to adjust a model which reasons on missing data.

On the whole, M-GAM quickly produces accurate, interpretable models, providing a new degree of transparency to predictions in the presence of missing data. The code used for this work is available at https://github.com/jdonnelly36/M-GAM.

**Societal Impacts.** M-GAM offers an interpretable way to deploy machine learning in high stakes domains like medicine, even when data is missing. Modeling decisions for missing data risk introducing or perpetuating unfairness Jeanselme et al. (2022). We view interpretability as a key tool for addressing this.

## 6 Acknowledgements

We acknowledge funding from the National Institutes of Health under 5R01-DA054994, the National Science Foundation under grant HRD-2222336 and the Department of Energy under grant DE-SC002135. Additionally, this material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 2139754. Finally, we thank Jiachang Liu for his helpful advice.

Figure 6: Box-and-whiskers plots comparing test performance of baseline models to M-GAM on four datasets over ten train-test splits. All methods except M-GAMs, “DecisionTree No Imputation”, and “RandomForest No Imputation” impute ten datasets using MICE then ensemble the models fit on each dataset.