# Entity Alignment with Noisy Annotations from Large Language Models

Shengyuan Chen

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

shengyuan.chen@connect.polyu.hk

&Qinggang Zhang

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

qinggangg.zhang@connect.polyu.hk

&Junnan Dong

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

hanson.dong@connect.polyu.hk

&Wen Hua

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

wency.hua@polyu.edu.hk

&Qing Li

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

csqli@comp.polyu.edu.hk

&Xiao Huang

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

xiaohuang@comp.polyu.edu.hk

###### Abstract

Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency.

## 1 Introduction

Knowledge graphs (KGs) serve as a foundational structure for storing and organizing structured knowledge about entities and their relationships, which facilitates effective and efficient search capabilities across various applications. They have been widely applied in question-answering systems (Dong et al., 2023, 2024c), recommendation systems (Catherine and Cohen, 2016; Chen et al., 2024a), social network analysis (Tang et al., 2008), Natural Language Processing (Weikum and Theobald, 2010), etc. Despite their extensive utility, real-world KGs often suffer from issuessuch as incompleteness, domain specificity, or language constraints, which limit their effectiveness in cross-disciplinary or multilingual contexts. To address these challenges, entity alignment (EA) aims to merge disparate KGs into a unified, comprehensive knowledge base by identifying and linking equivalent entities across different KGs. For instance, by aligning entities between a financial KG and a legal KG, EA facilitates the understanding of complex relationships, such as identifying the same corporations across the two KGs to assess how legal regulations impact their financial performance. This alignment enables a more nuanced exploration and interrogation of interconnected data, providing richer insights into how entities operate across multiple domains.

Entity alignment models predict the equivalence of two entities by measuring their alignment probability. Specifically, rule-based methods (Suchanek et al., 2012; Jimenez-Ruiz and Cuenca Grau, 2011; Qi et al., 2021) utilize predefined rules or heuristics to update alignment probabilities and propagate alignment labels. Conversely, embedding-based models seek to exploit advanced techniques in graph learning (Li et al., 2024; Liu et al., 2024, 2024, 2023), parameterizing these probabilities using similarity scores between entity representations learned through knowledge graph embedding algorithms such as translation models (Chen et al., 2017; Sun et al., 2018) or Graph Convolutional Networks (GCNs) (Wu et al., 2019; Mao et al., 2021; Wang et al., 2018; Huang et al., 2023). However, these methods heavily rely on extensive and accurate seed alignments for training--a requirement that poses significant challenges. The need for substantial, cross-domain knowledge to annotate such alignments often makes their acquisition prohibitively expensive.

Recently, Large Language Models (LLMs) have showcased their superior capability in processing semantic information Dong et al. (2024), which has significantly advanced various graph learning tasks such as node classification (Chen et al., 2024), graph reasoning (Zhao et al., 2023; Chai et al., 2023), recommender systems (Zhou et al., 2022; Wu et al., 2023), SQL query generation (Zhang et al., 2024), and knowledge graph-based question answering (Wang et al., 2024; Zhang et al., 2024; Dong et al., 2024). Their capacity to extract meaningful insights from graph data opens up new possibilities for automating EA. Notably, recent studies (Zhong et al., 2022; Zhao et al., 2023; Jiang et al., 2024) have explored the use of LLMs in EA, primarily focusing on finetuning a pretrained LLM such as Bert to learn semantic-aware representations, relying on accurate seed alignments as training labels. Yet, the potential of LLMs for label-free EA via in-context learning remains unexplored.

However, directly applying LLMs to automate EA poses significant challenges. Firstly, conventional EA models presume that all annotations are correct; yet, LLMs can generate false labels due to LLMs' inherent randomness and the potential incompleteness or ambiguity in the semantic information of entities. Training an EA model directly on these noisy labels can severely impair the final alignment performance. Secondly, given the vast number of entity pairs, annotation with LLMs would be prohibitively expensive. Maximizing the utility of a limited LLM query budget is essential. Existing solutions such as active learning cannot be directly applied since the annotations are noisy.

In response to the outlined challenges, we introduce LLM4EA, a unified framework designed to effectively learn from noisy pseudo-labels generated by LLMs while dynamically optimizing the utility of a constrained query budget. LLM4EA actively selects source entities based on feedback from a base EA model, focusing on those that significantly reduce uncertainty for both the entities themselves and their neighbors. This approach allocates the query budget to important entities, guided by the intra-KG and inter-KG structure. To manage the noisy pseudo-labels effectively, LLM4EA incorporates an unsupervised label refiner that enhances label accuracy by selecting a subset of confident pseudo-labels through probabilistic reasoning. These refined labels are then utilized to train the base EA model for entity alignment. The confident alignment results inferred by the EA model inform active selection in subsequent iterations, thereby progressively improving the framework's effectiveness in a coherent and integrated manner. Contributions are summarized as follows:

* **Novel LLM-based framework for entity alignment:** We propose LLM4EA, an in-context learning framework that uses an LLM to annotate entity pairs. Leveraging the LLMs' zero-shot learning capability, this framework generates pseudo-labels, providing a foundation for entity alignment without ground truth labels.
* **Unsupervised label refinement:** Our framework introduces an unsupervised label refiner informed by probabilistic reasoning. This component significantly improves the accuracy of LLM-derived pseudo-labels, enabling effective training of entity alignment models.
* **Active sampling module:** We propose an active selection algorithm that dynamically searches entities in the huge annotation space. Guided by feedback from the EA model,this algorithm adjusts its policy based on inter-KG and intra-KG structures, optimizing the utility of LLM queries and ensuring efficient use of resources.
* **Empirical validation and superior performance:** We rigorously evaluate our framework through extensive experiments and ablation studies. The results demonstrate that LLM4EA not only outperforms baselines by a large margin but also shows robustness and efficiency. Each component of the framework is shown to contribute meaningfully to the overall performance, with clear synergistic effects observed among the components.

## 2 Problem definition

A knowledge graph \(\) comprises a set of entities \(\), a set of relations \(\), and a set of relation triples \(\) where each triple \((e_{h},r,e_{t})\) represents a directional relationship between its head entity and tail entity. Given two KGs \(=\{,,\}\), \(^{}=\{^{},^{},^ {}\}\) and a fixed query budget \(\) to a Large Language Model, we aim to train an entity alignment model \(\) based on the LLM's annotations to infer the matching score \(m_{}(e,e^{})\) for all entity pairs \(\{(e,e^{}),e,e^{}^{}\}\). The evaluation process utilizes a ground truth alignment set \(\) to assess the prediction accuracy for target entities in both directions, i.e.,\((e,?)\) and \((?,e^{})\) for each true pair \((e,e^{})\), based on the ranked matching scores \(m_{}\). Evaluation metrics are hit@k (where \(k\{1,10\}\)) and mean reciprocal rank (MRR).

## 3 Entity alignment with noisy annotations from LLMs

We aim to design a framework to perform entity alignment with LLMs. Our design is motivated by the following insights. Firstly, we have a huge search space (the overall annotation space is \(O(|||^{}|)\)) to identify the core entity pairs to annotate. Secondly, we don't know whether the annotated labels are correct or not, because we have no prior knowledge or heuristic of the label distribution. Finally, we perform annotations iteratively, requiring the model to adjust its search policy based on annotation effectiveness, while we have no verifiable feedback of this annotation accuracy.

Based on these insights, we propose LLM4EA--an iterative framework that consists of four interconnected steps in each cycle, as illustrated in Figure 1. Initially, **an active selection** technique optimizes the use of resources by choosing critical source entities that significantly reduce uncertainty for themselves and their neighbors. Subsequently, **an LLM-based annotator** identifies the counterparts for the selected source entities, generating a set of pseudo-labels. Next, **a label refiner** improves label accuracy by eliminating structurally incompatible labels. This process involves formulating a combinatorial optimization problem and utilizing a probabilistic-reasoning-based greedy search algorithm to efficiently find a local-optimal solution. Finally, these refined labels are used to train **a base EA model** for the entity alignment task. The outcomes of the alignment then serve as feedback to inform subsequent rounds of the active selection policy. Further details are provided below.

### Active selection of source entity

We aim to maximize the utility of the budget by actively allocating the budget to those beneficial entities. To do this, we sample source entities that reduce the most uncertainty of both themselves and their neighboring entities, by a dynamically adjusted policy. The measurement of uncertainty

Figure 1: Overview of the LLM4EA framework. LLM4EA utilizes active sampling to select important entities based on feedback from an EA model. It also includes a label refiner to effectively train the base EA model using noisy pseudo-labels. Feedback from the EA model updates the selection policy.

reduction is based on two assumptions: 1) an entity's own uncertainty is inversely proportional to its alignment probability with its most probable counterpart; 2) the amount of uncertainty an entity eliminates for its neighbors is closely linked to the relational ties between them. To systematically assess this, we introduce the concept of _relational uncertainty_, quantified as follows:

\[U_{r}(e_{h})=(1-P(e_{h}))+_{(e_{h},r,e_{t})}w_{r}(1-P(e_{t })).\] (1)

Here, \(w_{r}\) is a weight coefficient reflecting the significance of relation \(r\) and signifies how much \(e_{h}\) contributes to reducing the uncertainty of \(e_{t}\) through the relation \(r\). For this purpose, we employ functionality \((r)\) (formally defined in Eq. (4)) as the weight \(w_{r}\), as it quantifies the uniqueness of the tail entity for a given specified head entity. \(P(e)_{e^{}^{}}P(e e^{})\) represents the alignment probability of the top-match entity for \(e\). These alignment probabilities \(P(e e^{})\) are obtained through probabilistic reasoning during label refinement (Section 3.3.2) and are augmented by the inferred alignments from the base EA model (Section 3.4). In the initial iteration, all alignment probabilities are set to 0.

It's important to note that some source entities are linked to a large number of uncertain neighbors (those with low \(P(e)\)). These source entities are crucial but may be overlooked if their connected relations have low functionality. Hence, we introduce _neighbor uncertainty_ as another metric to assess an entity's importance, by removing the functionality-based weight coefficient:

\[U_{n}(e_{h})=(1-P(e_{h}))+_{(e_{h},r,e_{t})}(1-P(e_{t}) ).\] (2)

To integrate these two metrics, we employ rank aggregation by mean reciprocal rank:

\[U(e_{h})=2((e_{h})}+(e_{h})}).\] (3)

Here, \(r_{ur}(e_{h})\) and \(r_{un}(e_{h})\) denote the ranking of \(e_{h}\) when using \(U_{r}\) and \(U_{n}\) as metric, respectively. This simple-effective aggregation technique is advantageous for our task since it's scale invariant and requires no validation set for tuning hyperparameters, making it more practical in this task.

### LLM as annotator

**Counterpart filtering.** With the selected source entities, we employ an LLM as an annotator to identify the counterpart from \(^{}\) for each source entity, generating a set of pseudo-labels \(=\{(e,e^{})|e,e^{}^{ }\}\). To narrow down the search space, we first filter out the less likely counterparts before querying the LLM, selecting only the top-\(k\) most similar counterparts from \(^{}\). The similarity metric is flexible: we use a string matching score based on word edit distance, but other methods are also viable, such as semantic embedding distances derived from word embedding models. By adjusting \(k\), we can trade-off between the recall rate of counterparts and the query cost.

**Prompt design.** There are primarily two methods for retrieving context information to construct textual prompts: randomly generated prompts and dynamically tuned prompts. The former involves randomly selecting neighbors to construct contexts for the entity, while the latter dynamically selects neighbors based on feedback from the EA model. For a fair comparison, we use randomly generated prompts across all baselines and the proposed LLM4EA. These prompts include the name of each entity and a set of relation triples to three randomly selected neighbors. For the baseline models, pseudo-labels are generated at once and used for training. For LLM4EA, we evenly divide the budget \(\) into \(n\) iterations and generate pseudo-labels at each iteration using the allocated \(/n\) budget.

### Probabilistic reasoning for label refinement

The pseudo-labels generated by the LLM can be noisy, and directly using these labels to train an entity alignment (EA) model could undermine the final performance. Although estimating the label distribution by asking the LLM for confidence scores or querying multiple times to measure consistency are potential solutions, these approaches can be vulnerable or introduce additional costs.

In light of this, we propose a label refiner that leverages the structure of knowledge graphs. The refinement process is framed as a combinatorial optimization problem aimed at minimizing overall structural incompatibility among labels. Utilizing a probabilistic reasoning technique, we progressively update our confidence estimation for each label and select those that are mutually compatible, ultimately producing a set of accurate pseudo-labels. Detailed explanations follow below.

#### 3.3.1 Functionality and probabilistic reasoning

**Functionality.** The functionality of a relation quantifies the uniqueness of tail entities for a specified head entity, calculated as the ratio of unique head entities to total head-tail pairs linked by the relation. Conversely, inverse functionality quantifies the tail entity uniqueness for a specified head entity. Formally, these are defined as:

\[(r)|(e_{h},r,e_{t})\}}{|\{(e _{h},e_{t})|(e_{h},r,e_{t}))\}|},^{-1}(r) |(e_{h},r,e_{t})\}}{|\{(e_{h},e_{t})|(e_{h },r,e_{t}))\}|}.\] (4)

For instance, suppose a KG contains two triples for the relation \(locate\_in\): \((Hawaii,locate\_in,US)\) and \((Miami,locate\_in,US)\). Then \((locate\_in)=1.0\) and \(^{-1}(locate\_in)=0.5\). In other words, given \((Miami,locate\_in,?)\), the answer for the missing tail entity is unique; while given \((?,locate\_in,US)\), there are multiple answers for the missing head entity. Such relational patterns are useful for identifying an entity based on its connections within the intra-graph structure.

**Probabilistic reasoning.** If two entities are each connected to entities that are aligned across KGs, this increases the likelihood that they should be aligned as well. Based on this heuristic, an entity pair's alignment probability \(P(e_{h} e^{}_{h})\) can be inferred by aggregating its neighbors' alignment probability via relation functionality:

\[1-_{(e_{h},r,e_{t}),\\ (e_{h},r^{},e^{}_{t})^{}}(1 -^{-1}(r)P(r r^{})P(e_{t} e^{}_{t})) (1-^{-1}(r^{})P(r^{} r)P(e_{t}  e^{}_{t})).\] (5)

Here, \(P(r r^{})\) denotes the probability of \(r\) being a subrelation of \(r^{}\), estimated by alignment probabilities of connected entities:

\[_{h},r^{},e^{}_{t}) ^{}}(1-P(e^{}_{h} e_{h})P(e^{}_{t}  e_{t})))}{(1-_{e^{}_{h},e^{}_{t} ^{}}(1-P(e^{}_{h} e_{h})P(e^{}_{t}  e_{t})))}.\] (6)

These formulations allow for the propagation and updating of alignment probabilities in a manner that is cognizant of relational structures. We employ this technique to design a label refiner below.

#### 3.3.2 Label refiner

**Label incompatibility.** We exploit the "incompatibility" of labels for label refinement, based on the assumption that correct labels can infer each other, while a false label could be incompatible with its correctly aligned neighbors. We define the _overall incompatibility_ on a label set \(\) as:

\[()_{(e_{h},e^{}_{h})}( _{P(e_{h} e^{}_{h})<_{e}P(e,e^{} _{h})}+_{P(e_{h} e^{}_{h})<_{e^{}^{}}P(e_{h},e^{})}).\] (7)

Here, \(_{P(e_{h} e^{}_{h})<_{e}P(e,e^{} _{h})}=1\) if \(e_{h}\) is not the top-match for \(e^{}_{h}\), otherwise \(0\). It's important to note that a detected incompatibility doesn't necessarily indicate the false alignment of \((e_{h},e^{}_{h})\): it may suggest a misalignment of their neighbors. Given this, the key to label refinement is to jointly optimize the label's overall incompatibility while avoiding accidentally filtering out correct labels.

**Objective.** To enhance label quality, we propose to refine the pseudo-label set \(\) by finding a subset \(^{*}\) that minimizes its overall incompatibility: \(^{*}=*{argmin}_{^{}}(^{})\). Noteworthy that a trivial solution for this optimization problem is only preserving a set of isolated labels, such that \(_{e}P(e e^{}_{h})=0\) and \(_{e^{}^{}}P(e_{h} e^{})=0\) for all \((e_{h},e^{}_{h})^{}\). This trivial solution would lead to the exclusion of most accurate labels, an outcome we aim to avoid. Considering this, we introduce an \(1\) penalty term to penalize the removal of labels, leading to our overall objective:

\[^{*}=*{argmin}_{^{} }((^{})+|-^{}| ).\] (8)Here \(>0\) is a weight coefficient. Solving the above combinatorial problem is intractable as it requires computing \((^{})\) for each possible set \(^{}\), which is NP-hard. Below we propose to search for a local-optimal solution by a greedy algorithm powered by probabilistic reasoning.

**Greedy search.** The algorithm begins by initializing the alignment probability \(P(e e^{})=_{0}\) for every pair \((e,e^{})\) within the set \(\), where \(_{0}\) is a constant within the range (0,1). It then iteratively performs a search for an optimal label set \(^{}\) through a series of voting steps. Each iteration is comprised of two main steps: probabilistic reasoning and label adjustment.

During the probabilistic reasoning step, the alignment probabilities and subrelation probabilities are updated according to Eq. (5) and Eq. (6), respectively. This update process refines our estimates of label confidence based on the latest information. During the label adjustment step, the label set \(^{}\) is updated based on these updated probabilities. Labels are appended to \(^{}\) if their updated alignment probabilities exceed \(_{0}\), indicative of high confidence in their alignment, supported by their neighbors:

\[^{}^{}\{(e_{h},e^{}_{h} )|P(e_{h} e^{}_{h})>_{0}\}\,.\] (9)

Conversely, labels demonstrating structural incompatibilities are excluded from \(^{}\):

\[^{}^{}\{(e_{h},e^{ }_{h}) P(e_{h} e^{}_{h})<(_{e }P(e,e^{}_{h}),_{e^{}^{}}P( e_{h},e^{}))\}.\] (10)

In this manner, labels are removed if they are incompatible with updated aligned neighbors, ensuring the preservation of only the most confident pairs within \(^{}\). To further refine the search process in subsequent iterations, we augment all entity alignment probabilities within \(^{}\) to a superior score:

\[P(e e^{})(P(e e^{}),_{1} )(e,e^{})^{}.\] (11)

Here \(_{1}(_{0},1)\) serves as a new threshold, elevating the alignment probabilities of confident pairs to foster a more directed and effective search. After \(n_{lr}\) iterations, we get a set of confidently selected labels \(^{}\) that have high compatibility. The detailed algorithm is presented in Appendix A.2, and analyses of parameter efficiency and computational efficiency are provided in Appendix A.3.

### Entity alignment

With the refined labels, we train an embedding-based EA model to learn structure-aware representations for each entity. After training, the EA model computes a matching score \(m_{}(e,e^{})\) for each entity pair \((e,e^{})\) for evaluation. The selection of the base EA model is flexible, tailored to the task requirements. We chose a recently proposed GCN-based model, Dual-AMN (Mao et al., 2021), for its effectiveness and efficiency.

Feedback from the base EA model is crucial for dynamic update of the active selection policy. To generate effective feedback, we infer high-confidence pairs \((e,e^{})\) with the trained EA model, by selecting the pairs that both entities rank top for each other. These pairs are injected into the probabilistic reasoning system. Similar to the label refinement process, this system initializes with an alignment probability of \(_{0}\) for these pairs and updates the estimation of alignment and subrelation probabilities using Eq. (5) and Eq. (6). The updated probabilities are used to construct the uncertainty terms (i.e., \(U_{r}\) and \(U_{n}\)) to inform the active selection policy in subsequent iterations, thereby optimizing the budget utility and improving final performance continuously.

## 4 Experiments

In this section, we conduct experiments to evaluate the effectiveness of our framework. We begin by introducing the experimental settings. Then, we present experiments to answer the following research questions: **RQ1.** How effective is the overall framework? **RQ2.** What is the impact of the choice of LLM on the cost and performance of LLM4EA? **RQ3.** What is the effect of the label refiner? **RQ4.** What is the impact of active selection?

### Experimental setting

**Datasets and LLM.** In this study, we use the widely-adopted OpenEA dataset (Sun et al., 2020), including two monolingual datasets (D-W-15K and D-Y-15K) and two cross-lingual datasets (EN-DE-15K and EN-FR-15K). OpenEA comes in two versions: "V1" the normal version, and "V2"the dense version. We employ "V2" in the experiments in the main text. The LLM version in this experiment is GPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-turbo-2024-04-09). By default, the overall query budget is \(=0.1||\).

**Baselines.** Baseline models include three GCN-based models -- GCNAlign (Wang et al., 2018), RDGCN (Wu et al., 2019), Dual-AMN (Mao et al., 2021), and three translation-based models -- IMUSE (He et al., 2019), AlignE, BootEA (Sun et al., 2018), Here, BootEA is a variant of AlignE that adopts a bootstrapping strategy, equipped with a label calibration component for improving the accuracy of bootstrapped labels. Baseline models are directly trained on the pseudo-labels generated by the LLM annotator, without label refinement or active selection. Every experiment is repeated three times to report statistics.

**Setup of LLM4EA.** We employ GPT-3.5 as the default LLM due to its cost efficiency. Other parameters are \(n=3\), \(n_{lr}\), \(k=20\), \(_{0}=0.5\), \(_{1}=0.9\).

### Results

#### 4.2.1 Comprehensive evaluation of entity alignment performance

To answer **RQ1** and **RQ2**, we conducted two groups of experiments on OpenEA datasets, using GPT-3.5 and GPT-4 as the annotator, respectively. Results are presented in Table 1. We also investigated the performance-cost comparison between the GPT-3.5 annotator and the GPT-4 annotator, illustrated in Figure 2. To control the randomness introduced by the LLMs, each experiment was repeated three times to report mean and standard deviation. These results lead to several key observations:

**First, LLM4EA surpasses all baseline EA models, which are directly trained on the pseudo-labels, by a large margin.** This can be attributed to 1) our label refiner's capability in filtering out false labels, reducing noise during training and enabling more accurate optimization towards the ground true objective; 2) our active selection component's ability to smartly identify important entities to annotate, which takes full advantage of the fixed query budget.

**Second, using the GPT-4 results in higher performance than using the GPT-3.5 as the annotator.** This observation conforms to the fact that GPT-4 is a more advanced LLM with higher reasoning capacity and stronger semantic analysis, resulting in more precise annotation results and higher recall, thus providing more labels of high quality. We also observe that translation-based models (e.g., AlignE) are sensitive to noisy labels under GPT-3.5, while state-of-the-art GCN-based models (e.g., RDGCN and Dual-AMN) are more robust. BootEA also demonstrates superior performance and robustness, attributed to its bootstrapping technique and enhanced by its capability in calibrating bootstrapped labels. However, its label calibration is only applied to the bootstrapped labels, so it still suffers from the false training labels. Our proposed LLM4EA, on the other hand, refines the label accuracy before training the EA model, thus ensuring more accurate training.

    &  &  &  &  \\  & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR & Hit@10 & MRR & Hit@1 & Hit@10 & MRR \\   \\  IMUSE & 50.04.0 & 72.640.8 & 57.540.4 & 51.644.7 & 75.943.9 & 60.548.5 & 60.02.1 & 41.642.5 & 9.01.0 & 54.42.2 & 78.941.1 & 63.22.2 \\ AlignE & 6.640.3 & 24.540.5 & 12.640.5 & 6.240.3 & 18.410.1 & 10.400.5 & 8.0 & 9.2402.7 & 13.331.4 & 50.182.0 & 76.641.4 & 59.241.8 \\ BootEA & 44.81.1 & 71.941.2 & 54.21.2 & 68.140.2 & 58.440.3 & 74.33.0 & 62.048.0 & 79.340.1 & 67.440.2 & 87.880.1 & 96.740.1 & 91.240.1 \\ GCNAlign & 17.440.3 & 14.324.0 & 25.940.3 & 22.240.2 & 46.241.1 & 30.30.3 & 16.940.1 & 39.340.3 & 23.430.1 & 45.340.8 & 63.330.5 & 53.340.5 \\ RDGCN & 69.340.3 & 82.580.3 & 74.340.3 & 73.340.4 & 84.642.6 & 74.373.9 & 72.940.7 & 87.940.5 & 83.240.6 & 86.237.1 & 94.191.3 & 86.122.7 \\ Dual-AMN & 51.94.3 & 79.640.9 & 61.640.5 & 70.540.7 & 79.110.3 & 78.990.6 & 62.040.1 & 86.840.1 & 71.940.1 & 85.80.3 & 98.490.0 & 91.440.1 \\ LLM4EA & **74.240.3** & **92.940.4** & **81.040.3** & **89.120.5** & **97.840.1** & **92.640.3** & **87.440.3** & **97.401.0** & **99.402.0** & **97.780.0** & **99.540.0** & **98.320.0** \\   \\  IMUSE & 52.740.9 & 74.941.0 & 59.840.9 & 59.642.6 & 81.841.5 & 57.942.1 & 21.646.1 & 50.040.10 & 31.147.4 & 86.640.5 & 94.240.1 & 89.240.4 \\ AlignE & 30.824.2 & 69.142.5 & 43.12.5 & 46.445.2 & 76.543.8 & 56.648.8 & 36.143.7 & 67.843.6 & 46.743.7 & 86.440.9 & 97.040.3 & 90.240.6 \\ BooFiE & 58.240.3 & 83.740.3 & 67.040.3 & 80.540.4 & 92.620.4 & 84.80.3 & 71.640.2 & 88.340.2 & 77.640.2 & 95.050.01 & 98.640.0 & 96.340.1 \\ GCNAlign & 30.640.0 & 65.340.3 & 42.120.2 & 41.940.4 & 68.60.5 & 51.514.0 & 33.410.3 & 61.640.1 & 41.440.2 & 82.640.2 & 9.4940.2 & 87.240.1 \\ RDGCN & 72.140.2 & 84.540.1 & 76.740.2 & 74.141.1 & 85.140.7 & 78.010.0 & 82.541.1 & 91.440.7 & 85.941.0 & 85.440.9 & 93.240.4 & 88.340.8 \\ Dual-AMN & 76.740.1 & 94.940.3 & 83.640.2 & 90.780.1 & 97.940.2 & 93.680.1 & 81.540.1 & 94.940.2 & 86.740.1 & 97.540.0 & 99.340.1 & 98.18.0 \\ LLM4EA & **80.240.3** & **96.040.2** & **86.040.2** & **93.140.5** & **97.840.2** & **95.130.3** & **89.840.3** & **97.940.2** & **92.940.3** & **97.940.1** & **99.960.0** & **95.540.1** \\   

Table 1: Evaluation of entity alignment performance, measured by Hit@K for \(K\{1,10\}\), and Mean Reciprocal Rank (MRR), presented in %. Experiment statistics are computed over three trials.

**Finally, LLM4EA is noise adaptive, enabling cost-efficient entity alignment.** To further investigate the effect of the choice of LLM, we examined the performance-cost comparison between GPT-3.5 and GPT-4 as the annotator. We illustrate MRR in Figure 2 (detailed results are available in Appendix B.3). The results show that, by increasing the query budgets (measured by the number of tokens) for GPT-3.5, the performance gradually increases. When the budget is \(2\) that of GPT-4, the performance is comparable to or exceeds the performance of using GPT-4 as the annotator. According to the pricing scheme of OpenAI, the input/output cost for 1 million tokens for GPT-3.5 and GPT-4 is \(80.50\)/\(\$1.5\) and \(\$10\)/\(\$30\), respectively. This means that our noise-adaptive framework enables cost-efficient entity alignment with less advanced LLMs at \(10\) less actual cost than using more advanced LLMs, simply by increasing the token budget for the less advanced LLMs.

#### 4.2.2 Effect of the label refiner

To answer **RQ3**, we first analyze the evolution of the True Positive Rate (TPR) and the recall rate of the refined labels. Specifically, at each label refinement iteration, the TPR is calculated as \(^{}|}{|^{}|}\), and the recall is calculated as \(^{}|}{||}\). The left and middle subfigures of Figure 3 demonstrate how **our label refiner progressively discourse accurate labels and optimizes the TPR.** Initially, the TPR of the refined label set is high (approximately 1.0), then it decreases by a certain percentage, and eventually increases again to a high TPR. We attribute this pattern to: 1) the most confident labels being discovered in the earliest iterations, which are obvious alignments with many connected alignments; 2) as the algorithm progresses, some false pseudo-labels being erroneously added to the label set \(^{}\); 3) as the label refinement continues, \(^{}\) is adjusted and the false pseudo-labels are replaced with the correct labels inferred by the updated probability as in Eq. (10).

Furthermore, we assess the robustness of our label refiner, as depicted in the right subfigure of Figure 3. We synthesize noisy labels and evaluate the output TPR in relation to varying input TPR levels, using two experimental schemes: _fixed budget_, where the budget remains constant at \(0.1||\) while the TPR changes, and _fixed TP_, where the number of true positives is fixed but the TPR and corresponding budgets are adjusted. The results demonstrate that **the label refiner consistently elevates the TPR to over \(0.9\), even when the initial TPR is around \(0.5\), showcasing its high

Figure 3: Analysis of the Label Refinement. We illustrate the evolution of the true positive rate (TPR) (left) and recall (middle) for refined labels across four datasets. Furthermore, we assess the robustness of the label refinement process by examining the TPR of refined labels against varying initial TPRs within the D-W-15K dataset (right), with initial pseudo-labels synthesized at different TPR levels.

Figure 2: Performance-cost comparison between GPT-3.5 and GPT-4 as the annotator, evaluated by MRR. We increase the budget for GPT-3.5 to evaluate its performance. [\(n\)] denotes using \(n\) of the default query budget. Each experiment is repeated three times to show mean and standard deviation.

robustness to noisy pseudo-labels.** This result also reveals why our framework demonstrates robust performance with the less advanced GPT-3.5 annotator.

#### 4.2.3 Ablation study

Ablation studies detailed in Table 2 answer **RQ4** and reveal several key insights: **1) Necessity of the Label Refiner for Effective Active Selection:** The performance of "w/o LR", which lacks a label refiner, is inferior not only to other model variants but also to the base model, Dual-AMN. This underscores that active selection depends crucially on reliable feedback, which is compromised when the label refiner is absent; **2) Contribution of Relation and Neighbor Uncertainty in Active Selection:** Incorporating both relation and neighbor uncertainties significantly enhances the utility of the budget. Methods like "Ours-degree" and "Ours-funcSum" focus only on their connections to neighbors and ignore the uncertainty of neighbors. In contrast, "Ours-ru" and "Ours-nu", which take these uncertainties into account, exhibit superior performance. This underscores the importance of considering neighbor uncertainty for effective active selection; **3) Robust Active Selection through Combined Metrics:** Our active selection approach integrates both relation uncertainty and neighbor uncertainty to enable robust active selection. By employing rank aggregation, it prioritizes entities that are deemed significant by both metrics, ensuring a more effective and nuanced selection process.

#### 4.2.4 Pareto frontier of runtime overhead against performance.

The runtime overhead is directly proportional to the number of active selection iterations, \(n\), since each iteration involves a subsequent label refinement process. To explore the runtime-performance trade-off in the LLM4EA approach, we examine the Pareto frontier of runtime versus performance. We conduct entity alignment experiments with a fixed query budget, varying the number of active selection iterations. The results of these experiments are illustrated in Figure 4.

The results indicate that performance initially increases as the number of iterations rises from 1 to around 3, but further increases beyond this point lead to a decline. This pattern can be attributed to two main factors: (1) More iterations allow for extensive learning from feedback during the active selection phase. (2) However, when iterations are excessively high, the number of generated pseudo-labels per iteration becomes small, leading to isolated pseudo-labels that undermine the label refinement process. These findings suggest that **an optimal balance between runtime efficiency and performance can be achieved without excessive trade-offs**, indicating a specific threshold for iterations beyond which no further performance gains are observed.

    &  &  &  &  \\  & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR \\  Ours & 74.24.03 & 92.940.4 & 81.040.3 & **89.140.5** & **97.840.1** & **97.840.1** & **97.266.03** & 87.540.3 & 96.740.1 & 90.980.2 & **97.70.09** & **95.540.0** & **98.340.0** \\ w/o LR & 51.64.10 & 80.240.7 & 61.940.8 & 74.441.7 & 94.240.6 & 82.61.61 & 39.214.4 & 75.770.7 & 52.912.4 & 85.310.9 & 99.240.1 & 91.540.5 \\ w/o Act & 68.12.12 & 88.441.7 & 75.442.0 & 78.440.8 & 93.940.8 & 84.60.6 & 82.840.7 & 92.540.8 & 86.340.6 & 97.540.1 & 99.240.3 & 98.140.1 \\  Ours-ru & 70.82.01 & 91.240.8 & 78.208.3 & 89.340.3 & 97.720.2 & 89.610.2 & **88.740.6** & **97.440.3** & **93.210.6** & **97.70.09** & 99.40.1 & **98.40.0** \\ Ours-un & **74.540.70** & **93.140.8** & **82.406.8** & 88.620.2 & 97.40.3 & 88.40.3 & 85.180.5 & 95.240.5 & 88.905.7 & 97.640.1 & 99.400.0 & 95.240.0 \\ Ours-degree & 73.62.62 & 92.540.8 & 80.442.0 & 88.440.1 & 96.640.2 & 91.540.2 & 80.143.7 & 90.992.2 & 84.083.2 & 97.240.2 & 99.040.1 & 97.940.1 \\ Ours-funcSum & 59.540.6 & 78.840.6 & 66.340.6 & 81.240.5 & 96.040.3 & 87.140.0 & 83.940.9 & 93.141.1 & 87.341.0 & 97.540.1 & 99.440.1 & 98.140.1 \\   

Table 2: Ablation study overview. The table presents the performance of the LLM4EA (Ours) with various modifications. **Group 1**: removing the label refiner (w/o LR) and the active selection component (w/o Act); **Group 2**: replacing the active selection technique with relational uncertainty (-ru), neighbor uncertainty (-nu), degree (-degree), and functionality sum (-funcSum).

Figure 4: Performance of entity alignment across four datasets with varying active sampling iterations, under a fixed query budget.

Related work

LLMs for Entity Alignment.Recent approaches have sought to leverage LLMs for entity alignment in knowledge graphs, primarily focusing on integrating structural and semantic information for improved alignment performance. BERT-INT (Tang et al., 2020) fine-tunes a pretrained BERT model to capture interactions and attribute information between entities. Similarly, SDEA (Zhong et al., 2022) employs a pretrained BERT to encode attribute data, while integrating neighbor information via a GRU to enhance structural representation. TEA (Zhao et al., 2023) reconceptualizes entity alignment as a bidirectional textual entailment task, utilizing pretrained language models to estimate entailment probabilities between unified textual sequences representing entities. A novel approach, ChatEA (Jiang et al., 2024), introduces a KG-code translation module that converts knowledge graph structures into a format comprehensible to LLMs, enabling these models to apply their extensive background knowledge to boost the accuracy of entity alignment. Notably, these models primarily focus on fine-tuning pretrained language models using a set of training labels and do not exploit the zero-shot capabilities of LLMs. In contrast, our proposed model, LLM4EA, leverages the zero-shot potential of LLMs, enabling it to generalize to new datasets without the need for labeled data.

**Probabilistic Reasoning.** In the literature on knowledge graph reasoning, probabilistic reasoning has been effectively applied to infer new soft labels and their associated probabilities from existing labels. It has been utilized in domains such as knowledge graph completion (Qu and Tang, 2019; Zhang et al., 2020; Fang et al., 2023; Chen et al., 2024) and entity alignment (Suchanek et al., 2012; Qi et al., 2021; Liu et al., 2022; Chen et al., 2024), where it naturally represents complex relational patterns with simple rules and performs precise inferences. In this work, however, due to the potential inaccuracies in the pseudo-labels generated by LLMs, the newly inferred alignments may be incorrect. Consequently, we opt not to employ probabilistic reasoning directly for the entity alignment task. Instead, we emphasize its use primarily for filtering false pseudo-labels that demonstrate structural incompatibilities within our framework. For completeness, we include the results of comparison with a probabilistic reasoning model - PARIS (Suchanek et al., 2012) in Appendix B.4.

## 6 Limitations

Firstly, during active selection, we distribute the query budget evenly for each selection rather than dynamically customizing the budget allocation for each iteration. This allocation approach could be improved by developing a more adaptive budget allocation strategy. Secondly, the framework currently does not provide direct support for temporal KGs. Although the probabilistic reasoning and active selection components inherently support entity alignment on evolving KGs, the base EA model is transductive. This necessitates retraining the model whenever new entities and relation triples are introduced into the KGs. However, this can be complemented by the research line of inductive entity alignment, such as path-based embedding models or logic-based models, which can generalize to previously unseen entities without the need for retraining.

## 7 Conclusions

In this paper, we address the challenge of automating entity alignment with Large Language Models (LLMs) under budget constraints and noisy annotations. We introduce LLM4EA, a framework that maximizes the utility of a fixed query budget using active sampling and mitigates erroneous labels with a label refiner employing probabilistic reasoning. Empirical results show that LLM4EA's noise-adaptive capabilities reduce costs without sacrificing performance, achieving comparable or superior results with less advanced LLMs at up to 10 times lower cost. This highlights the potential of LLM-based models in merging cross-domain and cross-lingual KGs. Future work will explore incorporating real-time learning capabilities to dynamically adjust to evolving knowledge bases.