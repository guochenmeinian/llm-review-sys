# Meta-learning procedure: Learning experience

FSEO: A Few-Shot Evolutionary Optimization Framework for Expensive Multi-Objective Optimization and Constrained Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Meta-learning has been demonstrated to be useful to improve the sampling efficiency of Bayesian optimization (BO) and surrogate-assisted evolutionary algorithms (SAEAs) when solving expensive optimization problems (EOPs). However, existing studies focuses on only single-objective optimization, leaving other expensive optimization scenarios unconsidered. We propose a generalized few-shot evolutionary optimization (FSEO) framework and focus on its performance on two common expensive optimization scenarios: multi-objective EOPs (EMOPs) and constrained EOPs (ECOPs). We develop a novel meta-learning modeling approach to train surrogates for our FSEO framework, an accuracy-based update strategy is designed to adapt surrogates during the optimization process. The surrogates in FSEO framework combines neural network with Gaussian Processes (GPs), their network parameters and some parameters of GPs represent useful experience and are meta-learned across related optimization tasks, the remaining GPs parameters are task-specific parameters that represent unique features of the target task. We demonstrate that our FSEO framework is able to improve sampling efficiency on both EMOP and ECOP. Empirical conclusions are made to guide the application of our FSEO framework.

## 1 Introduction

Expensive optimization problems (EOPs) aim to find as good as possible solutions within a budget of limited solution evaluations. Conventional Bayesian optimization (BO) and surrogate-assisted evolutionary algorithms (SAEAs) have been widely used to solve EOPs, but they train surrogate models from the scratch. To further improve the sampling efficiency and optimization performance, many efforts have been made to pre-train surrogates with the prior experience gain from related optimization tasks, resulting in experience-based optimization algorithms [1; 21; 36; 35].

This work considers solving EOPs on the context of few-shot problems [5; 41], where plenty of expensive related tasks are available and each of them can provide a small dataset for experience learning. Therefore, many experience-based optimization approaches such as multi-tasking optimization [43; 2; 47], transfer optimization [35; 17; 16] are not considered as they cannot learn experience from small related tasks (A discussion is available in Appendix A). In comparison, meta-learning  has been proved to be powerful in solving few-shot problems, leading to a new subcategory of experience-based optimization, namely few-shot optimization (FSO) .

Existing studies on FSO are mainly few-shot Bayesian optimization (FSBO) where meta-learning approaches are combined with BO to solve EOPs with only one objective. In this paper, we propose a generalized few-shot evolutionary optimization (FSEO) framework to address EOPs from the perspective of SAEAs and consider two expensive optimization scenarios which have been limited studied: multi-objective EOPs (EMOPs) and constrained EOPs (ECOPs). Major contributions are summarized as follows.

* A novel meta-learning method, namely Meta Deep Kernel Learning (MDKL), is developed to gain prior experience from related expensive tasks. Our model architecture and parameter designs make it possible to generate a regression-based surrogate on the prior experience and then continually adapt the surrogate to approximate the target task.
* We propose a FSEO framework to solve EOPs from the perspective of SAEAs. FSEO framework is applicable to regression-based SAEAs since FSEO embed our meta-learning models in these SAEAs as their surrogates. In addition, an update strategy is designed to adapt surrogates constantly during the optimization. Note that our FSEO framework is a general framework but we focus on its performance on EMOPs and ECOPs in this paper.
* Experiments are conducted on EMOPs and ECOPs to show our FSEO framework is effective. Our comprehensive ablation studies discover the influence of some factors on FSEO performance and provide empirical guidance to the application of FSEO framework.

## 2 Related Work

Experience-based optimization can be divided into several subcategories according to the techniques of learning prior experience from related tasks. A detailed classification and discussion on these subcategories is available in Appendix A. This subsection focuses on related work on FSO.

FSO studies in the literature can be classified based on their model architectures. Most studies meta-learn parameters for Gaussian Processes (GPs) , namely FSBO or Meta Bayesian Optimization (MBO) . In addition,  meta-learns with transformer neural processes and  meta-learn parameters for the architecture of deep kernel learning (DKL) . The MDKL model in our FSEO belongs to the last category as its model architecture is relevant to DKL.

Our work is different from existing studies in three points: Firstly, many studies  use existing meta-learning models  as their surrogates. No further adaptations are made to these surrogates during optimization since they are not originally designed for optimization. In comparison, we try to develop a meta-learning model, MDKL, for optimization purpose. MDKL has explicit task-specific parameters, which allows continually model adaptations during the optimization. Secondly, existing work investigated only global optimization, leaving other optimization scenarios such as EMOP and ECOP still awaiting for investigation. As our MDKL is designed for optimization and is capable of continually adaptation, we pay attention on EMOPs and ECOPs which require more effective models than global optimization. Our work widens the scope of existing FSO research and it focuses on the perspective of SAEAs instead of BO. Lastly, in-depth ablation studies are lacking in the literature, making it unclear which factors affect the performance of FSO. Our extensive ablation studies fill this gap and we conclude some empirical rules to improve the performance of FSO.

## 3 Background

This section gives preliminaries about meta-learning and DKL. The former is the method of experience learning, the latter is the underlying structure of experience representation.

### Meta-Learning in Few-Shot Problems

In the context of few-shot problems, we have plenty of related tasks, each task \(T\) contributes a couple of small datasets \(D=\{(S,Q)\}\), namely support dataset \(S\) and query dataset \(Q\), respectively. After learning from datasets of random related tasks, a support set \(S_{*}\) from new unseen task \(T_{*}\) is given and one is asked to estimate the labels or values of a query set \(Q_{*}\). The problem is called 1-shot or 5-shot when only 1 data point or 5 data points are provided in \(S_{*}\). A comprehensive definition of few-shot problems is available in .

Meta-learning methods have been widely used to solve few-shot problems . They learn domain-specific features that are shared among related tasks as experience, such experience is used to understand and interpret the data collected from new tasks encountered in the future.

### Deep Kernel Learning (DKL)

DKL aims at constructing kernels that encapsulate the expressive power of deep architectures for GPs. To create expressive and scalable closed form covariance kernels, DKL combines the non-parametric flexibility of kernel methods and the structural properties of deep neural networks. In practice, a deep kernel \(k(^{i},^{j}|)\) transforms the inputs \(\) of a base kernel \(k(^{i},^{j}|)\) through a non-linear mapping given by a deep architecture \((|,)\):

\[k(^{i},^{j}|)=k((^{i}| ,),(^{j}|,)|),\] (1)

where \(\) and \((,)\) are parameter vectors of the base kernel and the deep architecture, respectively. \(=\{,,\}\) is the set of all parameters in this deep kernel. Note that in DKL, all parameters \(\) of a deep kernel \(k(^{i},^{j}|)\) are learned jointly by using the log marginal likelihood function of GPs as a loss function. Such a jointly learning strategy has been shown to make a DKL algorithm outperform a combination of a deep neural network and a GP model, where a trained GP model is applied to the output layer of a trained deep neural network .

### Meta-Learning on DKL

An important distinction between DKL algorithms and the applications of meta-learning to DKL is that DKL algorithms learn their deep kernels from single tasks instead of collections of related tasks. Such a difference alleviates two drawbacks of single task DKL : First, the scalability of deep kernels is no longer an issue as datasets in meta-learning are small. Second, the risk of overfitting is decreased since diverse data points are sampled across tasks.

## 4 Few-Shot Evolutionary Optimization (FSEO) Framework

In this paper, \(T_{*}\) denotes the target optimization task, and plenty of small datasets \(D_{i}\) sampled from related tasks \(T_{i}\) are available for experience learning. A complete list of notations is available at the beginning of Appendix.

### Overall Working Mechanism

As illustrated in Fig. 1, all modules covering the optimization of target task \(T_{*}\) are included in a grey block. The modules beyond the grey block are associated with related tasks \(T_{i}\) and experience learning, which distinguishes our FSEO framework from conventional SAEAs and BO. The MDKL surrogate modeling method consists of two procedures: meta-learning procedure and adaptation procedure. The former learns prior experience from \(T_{i}\), the latter uses experience to adapt surrogates to fit \(T_{*}\). The framework of FSEO is depicted in Alg. 1, it consists of the following major steps.

1. **Experience learning**: Before expensive optimization starts, a meta-learning procedure is conducted to train task-independent parameters \(^{e}\) for MDKL surrogates (line 2). \(N_{m}\) datasets \(\{D_{m1},,D_{mN_{m}}\}\) collected from \(N\) related tasks \(\{T_{1},,T_{N}\}\) are used to train \(^{e}\). \(^{e}\) is the experience that represents the domain-specific features of related tasks.
2. **Initialize surrogates with experience**: Optimization starts when a target optimization task \(T_{*}\) is given. An initial dataset \(S_{*}\) is sampled (line 3) to adapt task-specific parameters \(^{*}\) on the basis of experience \(^{e}\). After that, MDKL surrogates are updated (line 4).

Figure 1: Diagram of our FSEO framework.

3. **Reproduction**: MDKL surrogates \(h(^{*})\) are combined with a SAEA optimizer \(Opt\) to search for optimal solution(s) \(^{*}\) on \(h(^{*})\) (line 7). This is implemented by replacing the original (regression-based) surrogates in a SAEA with \(h(^{*})\).
4. **Update archive and surrogates**: New optimal solution(s) \(^{*}\) is evaluated on target task \(T_{*}\) (line 8). The evaluated solutions will be added to dataset \(S_{*}\) (line 9) which serves as an archive. Then, surrogate adaptation is triggered, surrogates \(h(^{*})\) are updated (line 10).
5. **Stop criterion**: Once the evaluation budget has run out, the evolutionary optimization will be terminated and the optimal solutions in dataset \(S_{*}\) will be outputted. Otherwise, the algorithm goes back to step 3.

```
1:Input:\(D_{i}\): Datasets collected from related tasks \(T_{i}\), i=\(\{1,,N\}\); \(N_{m}\): Number of subsets \(D_{m}\) for meta-learning; \(|D_{m}|\): Size of subsets \(D_{m}\). \(|D_{m}||D_{i}|\) due to \(D_{m} D_{i}\); Batch size \(B\); Surrogate learning rates \(,\); Target task \(T_{*}\); A SAEA optimizer \(Opt\); Fitness evaluation budget \(FE_{max}\).
2: Experience \(^{e}\) Meta-learning(\(D_{i},N_{m},|D_{m}|,B,\)). /*Alg. \(2.^{*}/\)
3:\(S_{*}\) Sampling \(1d\) solutions from \(T_{*}\).
4:\(h(^{*})\) Adaptation(\(^{e},S_{*},\)). /*Initialize surrogate.\({}^{*}/\)
5: Set evaluation counter \(FE=|S_{*}|\).
6:while\(FE<FE_{max}\)do
7: Candidate solution(s) \(^{*}\) Surrogate-assisted optimization (\(Opt,h(^{*})\)).
8:\(f(^{*})\) Evaluate \(^{*}\) on \(T_{*}\).
9:\(S_{*} S_{*}\{(^{*},f(^{*}))\}\).
10:\(h(^{*})\) Update(\(^{*},S_{*},\)). /*Alg. \(4.^{*}/\)
11: Update \(FE\).
12:endwhile
13:Output: Optimal solutions in \(S_{*}\). ```

**Algorithm 1** FSEO Framework.

### Learning and Using Experience by MDKL

In MDKL, the domain-specific features of related tasks are used as experience, which are represented by the task-independent parameters \(^{e}\) learned across related tasks. To make MDKL more capable of expressing complex domain-specific features, the base kernel \(k(^{i},^{j}|\ )\) in GP is combined with a neural network \((,)\) to construct a deep kernel (see Eq.(1)). The modeling of a MDKL model consists of two procedures: meta-learning procedure and adaptation procedure. To make a clear illustration, we introduce frameworks of two procedures and then explain them in detail.Our MDKL model uses the kernel in  as its base kernel:

\[k(^{i},^{j}|,)=exp(-_{k=1}^{d} _{k}|x_{k}^{i}-x_{k}^{j}|^{p_{k}}).\] (2)

Therefore, the deep kernel will be:

\[k(^{i},^{j}|)=exp(-_{k=1}^{d}_{k}| (x_{k}^{i})-(x_{k}^{j})|^{p_{k}}),\] (3)

where \(=\{,,,\}\) is a set of deep kernel parameters. \(,\) and **b** are neural network and its parameters (see Eq.(1)). Details about alternative base kernels are available in .

The aim of meta-learning procedure is to learn experience \(^{e}\) from related tasks \(\{T_{1},,T_{N}\}\), including neural network parameters \(,\), and task-independent base kernel parameters \(^{e},^{e}\). The pseudo-code of meta-learning procedure is given in Alg. 2. Ideally, the experience \(^{e}\) is learned from plenty of (\(N_{m}\)) small datasets \(D_{m}\) collected from different related tasks. However, in practice, the number of available related tasks \(N\) may be much smaller than \(N_{m}\). Hence, the meta-learning is conducted gradually over \(U\) update iterations (line 3). During each update iteration, a small batch of \(^{*}\) is the set of all the \(L(S_{*},h(^{*}))\) variables.

```
1:Input:\(D_{i}\): Datasets collected from related tasks \(T_{i}\), i=\(\{1,,N\}\); \(N_{m}\): Number of subsets \(D_{m}\) for meta-learning; \(|D_{m}|\): Size of subsets \(D_{m}\). \(|D_{m}||D_{i}|\) due to \(D_{m} D_{i}\); Batch size \(B\); Learning rate for priors \(\).
2:Randomly initialize \(,,^{e},^{e}\).
3:Set the number of update iterations U = \(N_{m}/B\).
4:for\(j=1\) to \(U\)do
5:\(\{D_{1}^{},,D_{B}^{}\}\) Randomly select a batch of datasets from \(\{D_{1},,D_{N}\}\).
6:for all\(D_{i}^{}\) in the batch do
7:\(D_{mi}\) A subset of size \(|D_{m}|\) from \(D_{i}^{}\).
8: Initialize task-specific increment \(^{i},^{i}\).
9: Compute task-specific parameters: \(^{i}=^{e}+^{i}\),\(^{i}=^{e}+^{i}\).
10: Obtain deep kernel \(k(^{i},^{j}|)\) based GP: \(h()\), where \(=\{,,^{i},^{i}\}\) (Eq.(3)).  Compute the loss function \(L(D_{mi},h())\) (Eq.(4)).
11:endfor
12: Update \(,,^{e},^{e}\) via gradient descent: \( L(D_{mi},h())\) (Eq.(5)).
13:endfor
14:endfor
15:Output: Task-independent parameters: \(^{e}\) = \(\{,,^{e},^{e}\}\). ```

**Algorithm 2** Meta-learning(\(D_{i},N_{m},|D_{m}|,B,\))

Figure 2: Diagram of our deep kernel implementation. The solid lines depict the training process, the dotted lines depict the inference process. \(Q_{*}\) denotes query samples to be evaluated on our surrogates.

**Surrogate prediction.** Due to the nature of a GP, when predicting the fitness of a solution \(^{*}\), a MDKL surrogate produces a predictive Gaussian distribution \(((^{*}),^{2}(^{*}))\), the predicted mean \((^{*})\) and covariance \(^{2}(^{*})\) are specified as :

\[(^{*})=+^{}^{-1}(- ),\] (6)

\[^{2}(^{*})=^{2}(1-^{}^{-1} ),\] (7)

where **r** is a correlation vector consisting of covariances between \(^{*}\) and \(S_{*}\), other variables are explained in Eq.(4).

### Surrogate Update Strategy

In this subsection, we describe the update strategy in our FSEO framework. To properly integrate experience and data from \(T_{*}\), our update strategy is designed to determine whether a MDKL surrogate should be adapted in the current iteration or not, ensuring an optimal update frequency of surrogates.

As illustrated in Alg. 4, the surrogate update starts when a new optimal solution(s) has been evaluated on expensive functions and an updated archive \(S_{*}\) is available. For a given surrogate \(h(^{*})\), its mean squared error (MSE) on \(S_{*}\) is selected as the update criterion: If the MSE after an adaptation \(e_{1}\) (line 4) is larger than the MSE without an adaptation \(e_{0}\) (line 2), then the surrogate will roll back to the status before the adaptation. This indicates the surrogate update has been refused and \(h(^{*})\) will not be adapted in the current iteration. Otherwise, the adapted surrogate will be chosen (line 6). Note that no matter whether surrogate adaptations are accepted or refused, the resulting surrogates will be treated as updated surrogates, which are employed to assist the SAEA optimizer in the next iteration.

## 5 Computational Studies

Our computational studies can be divided into three parts: (1). Appendix D evaluates the effectiveness of learning experience through a synthetic problem and a real-world engine modeling problem. (2). Sections 5.1 to 5.2 use EMOPs as examples to investigate the performance of our FSEO framework in depth. Empirical evidence is provided to guide the use of our FSEO framework. (3). Section 5.3 investigates the performance of our FSEO framework on a real-world ECOP. The source code is available online1 For all meta-learning methods used in our experiments, their basic setups are listed in Table 1. The neural network structure is suggested by [10; 27], and the learning rates are the default values that have been widely used in many meta-learning methods [13; 27].

### Performance on EMOPs

In the following subsections, we aim to demonstrate the effectiveness of our FSEO framework. The experiment in this subsection is designed to answer the question below: With the experience learned from related tasks, can our FSEO framework helps a SAEA to save \(9d\) solutions without a loss of optimization performance?

The computational study is conducted on the DTLZ test problems . All the DTLZ problems have \(d=10\) decision variables and 3 objectives, as the setups that have been widely used in [25; 33]. The details of generating DTLZ variants (related tasks) are provided in Appendix C. We test our FSEO framework using an instantiation on MOEA/D-EGO, resulting MOEA/D-FS. Details of the comparison algorithms are given in Appendix E.1.

#### 5.1.1 Experimental setups

The parameter setups for this multi-objective optimization experiment are listed in Table 2. During the optimization process, an initial dataset \(S_{*}\) is sampled using Latin-Hypercube Sampling (LHS) method , then extra evaluations are conducted until the evaluation budget has run out. Note that we aim to use related tasks to save \(9d\) evaluations without a loss of SAEA optimization performance. Hence, the total evaluation budgets for MOEA/D-FS and comparison algorithms are different.

Since the test problems have 3 objectives, we employ inverted generational distance plus (IGD+)  as our performance indicator, where smaller IGD+ values indicate better optimization results. 5000 reference points are generated for computing IGD+ values, as suggested in . More results in IGD  and HV  metrics are reported in Appendix E.3.

#### 5.1.2 Results and analysis

The statistical test results are reported in Fig. 3 and Appendix E.2 (Table 5). It can be seen from Fig. 3 that, although 90 fewer evaluations are used in surrogate initialization, MOEA/D-FS can still achieve competitive or even smaller IGD+ values than MOEA/D-EGO on all DTLZ problems except for DTLZ7. In addition, the IGD+ values obtained by MOEA/D-FS drop rapidly, especially during the first few evaluations, implying the experience learned from DTLZ variants are effective. Therefore, in most situations, our FSEO framework is able to assist MOEA/D-EGO in reaching competitive or even better optimization results, with the number of evaluations used for surrogate initialization reduced from \(10d\) to only \(1d\).

MOEA/D-FS is less effective on DTLZ7 than on other DTLZ problems, which might be attributed to the discontinuity of the Pareto front on DTLZ7. Note that MOEA/D-FS learns experience from small datasets such as \(D_{m}\) and \(S_{*}\). The solutions in these small datasets are sampled at random, hence, the

  Module & Parameter & Value \\  Meta-learning & Number of meta-learning datasets \(N_{m}\) & 20000 \\  & Number of update iterations \(U\) & 2000 \\  & Batch size \(B\) & 10 \\  Neural network & Number of hidden layers & 2 \\  & Number of units in each hidden layer & 40 \\  & Learning rates \(,\) & 0.001, 0.001 \\  & Activation function & ReLU \\  

Table 1: Parameter setups for meta-learning methods.

Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. **Upper**: DTLZ1, DTLZ2, DTLZ3, DTLZ4. **Lower**: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that ‘FS(out)’ indicates the target task is excluded from the range of related tasks during the meta-learning procedure) (see Appendix F).

  Parameter & MOEA/D-FS & Comparisons \\  Number of related tasks \(N\) & 20000 (\(N_{m}\) in Table 1) & - \\ Size of datasets from real tasks \(|D_{m}|\) & 2000 (\(20.2\)) & - \\ Size of datasets for meta-learning \(|D_{m}|\) & \(|D_{m}|\) & - \\  Evaluation for initialization & 10(\(1.4\)) & 100(\(1.4\)) \\ Evaluations for further optimization & 50 & 50 \\ Total evaluations & 60 & 150 \\  

Table 2: Parameter setups for DTLZ optimization.

probability of having optimal solutions being sampled is small. However, it is difficult to learn the discontinuity of the Pareto front from the sampled non-optimal solutions. As a result, the knowledge of 'there are four discrete optimal regions' cannot be learned from such small datasets (\(|D_{m}|=20\)) collected from related tasks. The performance analysis between MOEA/D-FS and other comparison algorithms are available in Appendix E.2.

#### 5.1.3 More comparison experiments

We also compared the performance of our FSEO framework when only 10 evaluations are used for surrogate initialization for comparison algorithms. The results are reported in Table 8 in Appendix E.4. In addition, the performance of our FSEO framework in the context of extremely expensive optimization has been investigated in Appendix H (Table 11 and Fig. 7).

The question raised at the beginning of this subsection can be answered by the results discussed so far. Due to the integration of the experience learned from related tasks (DTLZ variants), although the evaluation cost of surrogates initialization has been reduced from 10\(d\) to 1\(d\), our FSEO framework is still capable of assisting regression-based SAEAs to achieve competitive or even better optimization results in most situations.

### Ablation Studies

We conduct two ablation studies to investigate the influence of task similarity and that of the dataset size used in meta-learning, results and analysis are reported in Appendixes F and G, respectively.

### Performance on Real-World ECOPs

The experiments on EMOPs have investigated the performance of our FSEO framework in depth. In this subsection, we evaluate our FSEO framework on a real-world gasoline motor engine calibration problem, which is an ECOP.

The calibration problem has 6 adjustable engine parameters, namely the throttle angle, waste gate orifice, ignition timing, valve timings, state of injection, and air-fuel-ratio. The calibration aims at minimizing the BSFC while satisfying 4 constraints in terms of temperature, pressure, CA50, and load simultaneously .

#### 5.3.1 Comparison algorithms

Since the comparison algorithms in the DTLZ optimization experiments are not designed for handling constrained optimization, our comparison is conducted with 3 state-of-the-art constrained optimization algorithms used in industry: A variant of EGO designed to handle constrained optimization problems (cons_EGO) , a GA customized for this calibration problem (adaptiveGA) , and a bilevel constrained SAEA (SAB-DE) . The settings of the comparison algorithms are the same as suggested in the literature. In this experiment, we apply our FSEO framework to cons_EGO and investigate its optimization performance. The GP surrogates in cons_EGO are replaced by our MDKL surrogates to conduct the comparison, and the resulting algorithm is denoted as cons_FS.

#### 5.3.2 Experimental setups

The setup of related tasks (\(N,D_{i}\)) is the same as described in Appendix D. In the meta-learning procedure, both the support set and the query set contain 6 data points, thus \(|D_{m}|=12\). The total evaluation budget for all algorithms is set to 60. For adaptiveGA, all evaluations are used in the optimization process as it is not a SAEA. For cons_EGO and SAB-DE, 40 samples are used to initialize the surrogates and 20 extra evaluations are used in the optimization process. For cons_FS, only 6 samples are used to initialize MDKL surrogates, and the remaining evaluations are used for further optimization.

#### 5.3.3 Optimization results and analysis

The left side and right side of Fig. 4 plot the normalized BSFC results and the number of feasible solutions found over the number of used evaluations, respectively. Solid lines are mean lines, while shadows are error regions. From the left side of Fig. 4, it can be observed that the minimal BSFCobtained by cons_FS decreases drastically in the first few evaluations, implying that the experience learned from related tasks is effective. In comparison, the minimal BSFC obtained by adaptiveGA and cons_EGO drops in a relatively slow rate, even though cons_EGO has used 34 more samples to initialize its surrogates. The star marker denotes the point at which cons_FS has evaluated 20 samples after surrogate initialization. It is worth noting that when 20 samples have been evaluated in the optimization, cons_FS achieves a smaller BSFC value than cons_EGO. After the star marker, the decrease of BSFC becomes slow as cons_FS has reached the optimal region. Therefore, further improvement in the normalized BSFC value is not significant and thus hard to be observed. The advantages of our FSEO framework can also be observed in constraint handling. In the right side of Fig. 4, cons_FS finds more feasible solutions than the 3 comparison algorithms. These results indicate that our FSEO framework improves the performance of cons_EGO on both objective function and constraint functions. Meanwhile, only \(1d\) evaluations are used to initialize surrogates.

#### 5.3.4 Discussion on runtime

It should be noted that real engine performance evaluations on engine facilities are very costly in terms of both time and financial budget . Since a single real engine performance evaluation can cost several hours [22; 49], the time cost of the meta-learning procedure is negligible as it takes only a few minutes. Savings from reduced real engine performance evaluations on engine facilities and the reduced development cycle due to our FSEO framework could amount to millions of dollars . our FSEO framework is an effective and efficient method to solve this real-world calibration problem.

## 6 Conclusion and further work

**Conclusion.** In this paper, we present a FSEO framework to address EMOPs and ECOPs from the perspective of SAEAs. A novel meta-learning approach MDKL is proposed to learn prior experience from related expensive tasks. Our MDKL model is designed for optimization and has explicit task-specific parameters, which allows continually update of task-specific parameters during the optimization process. Our empirical experiments show that the FSEO framework is able to improve the sampling efficiency and thus save expensive evaluations for existing regression-based SAEAs. Ablation studies reveal the influence between optimization performance and solutions similarity as well as the size of datasets for meta-learning.

**Limitation and further work.** The limitations of this work can be summarized as the following two points: First, we do not have a mathematical definition of related tasks. Second, the proposed framework is currently for regression-based SAEAs only.

Figure 4: Results of 30 runs on the real-world engine calibration problem, all BSFC values are normalized. Solid lines are mean values, while shadows are error regions. Left figure shows how BSFC varies with the number of evaluations. The star markers highlight the results achieved when 20 evaluations are used in the optimization process. Right figure illustrates how the number of feasible solutions varies with the number of evaluations.