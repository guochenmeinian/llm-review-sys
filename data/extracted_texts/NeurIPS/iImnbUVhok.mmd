# Joint Prompt Optimization of Stacked LLMs

using Variational Inference

 Alessandro Sordoni\({}^{ab}\)\({}^{*}\) Xingdi Yuan\({}^{a}\) Marc-Alexandre Cote\({}^{a}\) Matheus Pereira\({}^{a}\)

Adam Trischler\({}^{a}\) Ziang Xiao\({}^{a}\) Arian Hosseini\({}^{b}\) Friederike Niedtner\({}^{a}\) Nicolas Le Roux\({}^{ab}\)

Microsoft Research Montreal\({}^{a}\) MILA\({}^{b}\)

Corresponding author: alsordon@microsoft.com

###### Abstract

Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful. The DLN code is open source.1

## 1 Introduction

The size of large language models (LLMs) has grown significantly over the last few years, mainly because of emerging capabilities [5; 31], but at considerable technical and societal costs [49; 2; 4]. Recent efforts have focused either on learning smaller models matching the abilities of larger ones on some tasks using distillation [43; 36; 29; 13], or offloading part of the computation to other dedicated components [28; 22; 25; 18]. In the latter case, this is done through carefully crafted instructions to retrieve the necessary information from these additional modules [48; 41; 6; 54; 24].

Instruction-tuned LLMs map an input sequence to a distribution over output sequences conditioned on an instruction, or _prompt_. In this paper, we view such LLMs as stochastic language layers, whose learnable parameters are the prompts. Multiple layers can be stacked to form a Deep Language Network (DLN) whose learnable parameters are the prompts associated to each layer. Specifically, each layer uses a template to organize both its prompt and the inputs coming from the layer below into a single sequence before producing the output (see Figure 1). This layering induces a learnable decomposition of the task into a series of smaller sub-tasks, each of which might be more easily solvable by an LLM. This view shares similarities to recent works that chain LLM calls [41; 6; 54]. In this work, we move towards integrating learnable components in the pipeline: each prompt can be learned to maximize the final objective.

We first show how to perform prompt optimization in a shallow 1-layer language network (DLN-1) which parametrizes a distribution \(p_{}(y|x,)\), where \(x\) and \(y\) are string input and output respectively, and \(\) is the learnable prompt (Figure 1, _left_). Our prompt optimization techniques extend the Automatic Prompt Engineer (APE) procedure from Zhou et al. . We show how our prompts can include a verbalization of difficult examples from the task: the final prompts are a combination of instruction directives, akin to zero-shot learning , and task examples, akin to in-context learning . This significantly improves downstream performance, surpassing APE on several tasks.

Then, we show how to train a 2-layer DLN (DLN-2), which parametrizes a probability distribution:

\[p_{}(y|x)=_{h}p_{}(y|h,x,_{1})\,p_{}(h|x, _{0})\,,\]

where \(h\) is the string output of the first LLM layer (Figure 1, _right_). We consider \(h\) as a latent variable: to maximize the marginal log-likelihood, we formulate a variational inference algorithm that uses an approximate posterior over \(h\). Note that this formalism easily encompasses more than two layers.

Considering outputs of the hidden language layers as latent variables allows us to encompass various established prompting methods, such as Chain-Of-Thought (CoT)  and self-consistency (SC-CoT) . Particularly, CoT can be seen as a particular DLN-2 with the first layer prompt set to "Let's think step by step" and the second layer prompt set to "The answer is"; we can either learn such prompts or learn a supplement to those as in Figure 1. SC-CoT can be seen as marginalizing over CoT strings sampled from a task-agnostic prior: when using the template in Figure 1 (_right_), our method generalizes this perspective by learning a task-specific prior distribution over successful CoTs.

The rest of the paper is organized as follows. First, we provide an interpretation of LLMs as shallow networks, drawing a number of analogies with standard parametric and non-parametric models and explaining how best to train them. After exploring their limitations, we propose to stack two such

Figure 1: _Left_: An illustration of a DLN-1 performing a sentiment analysis task: input and the trainable prompt are merged using a template and fed to the LM for answer generation. _Right_: a DLN-2 with a residual connection, performing the date understanding task: two prompts need to be learned. In this example, the hidden template extends Chain-Of-Thought  with a learnable prefix; we consider the output of the first layer, hidden, as a _latent variable_\(h\). We use _variational inference_ to learn \(_{0},_{1}\). Templates can be considered as an hyperparameter of the network.

LLMs to form a DLN-2. We show how they can be trained using a form of variational inference, then demonstrate their performance on a series of reasoning and language understanding tasks.

## 2 One-Layer Language Networks

A pre-trained LLM with frozen weights might be thought of as a complete _function class_ indexed by prompts. The output \(y\) of an LLM for an input \(x\) can be modulated through a prompt \(\) by feeding a combination of \(\) and \(x\) to the LLM. Hence, from a low-level perspective, the function class of an LLM is defined by its architecture, i.e., its depth, number of heads, context size, etc., and training happens at the parameter level. It is data and compute intensive and should be done rarely. From a high-level perspective, the function class of an LLM is defined by the pre-trained model chosen (LLAMA , text-davinci-003, GPT-4, etc.), and training happens by fine-tuning the model or by choosing the prompt.

There are two ways of optimizing an LLM at the prompt level. The first one is _prompt engineering_, a parametric optimization, where the optimization space is independent of the size of the dataset. Because this optimization usually happens in discrete space, gradient-based techniques do not apply and most efforts rely on a combination of random or local search and human heuristics [21; 55]. The second one is _in-context learning_ (ICL), a non-parametric optimization technique where the solution is a direct function of a subset of examples [5; 20]. This approach works well for few-shot learning but scaling it to larger datasets has both performance and computational issues. We shall now generalize previous work in discrete prompt optimization [57; 21] with the ultimate goal of learning a set of prompts in a language network.

### Language Layers

We use _language layer_ to refer to a (stochastic) computation that takes as input a string \(x\) and outputs a string \(y\). This computation is modulated by another string, \(\), generally called a _prompt_ or _instruction_. The string transduction is performed by an operator LM, by feeding \(x\) and \(\) as context and generating a continuation \(y\). _Templates_ describe the way \(x\) and \(\) are combined prior to being fed to the LM operator. These are functions that accept strings as variables and output a string. We will denote such templates with this font T. A simple forward template F is the concatenation, i.e. \((x,)\) = "\(\{\}\{x\}\)". We also explore more complex ones, examples of which can be seen in Figure 1.

Given an input \(x\), a prompt \(\), and a template F, a language layer defines a probability distribution \(p_{}(y|(x,))\) over output strings \(y\) as computed by the LM. In the next section, we describe a generic framework for optimizing the weights \(\) for a language layer.

### Prompt Optimization: Improved APE

Because the search for the best prompt happens over a discrete space, we will rely on a local search method, using an LLM to implement a distance measure between prompts. The procedure can be seen as an extension of Automatic Prompt Engineer (APE), recently proposed by Zhou et al. , and will serve as a stepping stone towards introducing our algorithm for training deep language networks. Our prompt optimization algorithm can be structured as follows:

1. Given the current prompt \(\) and a current batch of examples \(\{x,y\}\), generate \(N\) "local" candidates \(^{1},,^{N}\) using a prompt _proposal_ distribution;
2. Score each candidate using a (potentially stochastic) _scoring_ function \(s\), then choose \(=_{^{n}}s(^{n})\).

Prompt ProposalLocal search algorithms assume a distance measure between inputs to crawl the search space. In this setting, we rely on LLMs to generate local modifications to the prompts. Our prompt proposal distribution takes as conditioning information i) the batch given as input to the layer, ii) its corresponding output \(\{x,y,\}\), and iii) the current prompt \(\). The proposal distribution \(p_{}(^{n}|_{}(\{x,y,\},))\) wraps this information using a particular "backward" template \(_{}\), which can be found in Appendix D. This approach is similar to the instruction template used by Zhang et al. , with the exception that we also integrate information about the model's own predictions, which we found to empirically help performance given that the model tends to propose prompts that correct its own errors. We sample from the prompt proposal distribution to generate a set of \(N\) prompts. A particularly important aspect is ensuring the diversity of the candidate pool \(^{1},,^{N}\). We devise several strategies to improve the diversity and the usefulness of the candidate samples in Section 4.

**Prompt Selection** Once a set of \(N\) prompts has been generated, we use a scoring function to select the updated prompt. We assume access to the log-likelihoods of the LM operator and we rank the candidate prompts to maximize data log-likelihood \(=*{arg\,max}_{^{n}} p_{}(y|(x;^{n}))\). In practice, we normalize this log-probability by the length of the output string. While we focus on that metric in this work, there is no restriction on the scoring function that can be used. We use backtracking to increase the robustness of our selection mechanism, as well as a memory of well-performing prompts for efficiency. We present both strategies in Section 4. The sketch of a 1-layer prompt optimization algorithm is described in Algorithm 1, ignoring backtracking and memory for simplicity.

```
0:\( p_{}^{t}(y|c)\)\(\) generates a completion of prefix \(c\) with temperature \(t\)
0:\( p_{}(h|c)\)\(\) return log-prob of \(h\) following \(c\)
0:\(N\): prompt samples, \(I\): iterations, \(\): dataset
0:F: template for the inference/forward pass
0:\(_{}\): template for prompt proposal/backward pass.
1: Initialize \(\) with a task description or empty
2:for\(i\) in \([1,I]\)do
3:\(x,y\)\(\) Sample minibatch
4:\( p_{}^{0}(y|(x,))\)\(\) Do inference pass
5:\(^{1},,^{N} p_{}^{0,}(|_{}(\{x,y, \},))\)\(\) Sample \(N\) candidate prompts
6:\(s^{1},,s^{N} p_{}(y|(x,^{n}))\)\(\) Score all prompts
7:\(*{arg\,max}_{^{n}}\{s^{1},,s^{N}\}\)\(\) Select prompt with best score
8:endfor ```

**Algorithm 1** One-Layer Language Network (DLN-1) Training Algorithm

The results of our prompt optimization may be found in Table 1 and will be discussed in detail in Section 5.2. We now turn to extending prompt optimization to architectures with two layers.

## 3 Two-Layer Deep Language Networks (DLN-2)

The natural extension of DLN-1 is DLN-2, in which language layers are stacked, i.e. the output of the first language layer is the input to the second one. A 2-layer network induces a distribution over outputs of the form:

\[p_{}(y|x)=_{h}p_{}(y|_{r}(h,x,_{1}) )p_{}(h|(x,_{0}))\] (1)

where \(h\) is a latent variable that potentially makes it easier to explain the target \(y\). The output layer is also conditioned on \(x\) through \(_{r}\), forming a residual connection (Figure 1). This formulation is reminiscent of past work using latent language representations to guide document summarization . In our case, however, the encoding/decoding distributions are parameterized by natural language prompts \(=\{_{0},_{1}\}\), and we do not assume access to the LLM parameters.

While this architecture has more expressive power than a shallow language network, the prompt optimization problem becomes harder now that we have to _jointly_ search over both \(_{0}\) and \(_{1}\). Doing random search in this space is impractical  and manual tuning of the weights is also exponentially harder than with a single prompt. We turn to variational inference to address this issue.

### Variational Inference Objective

The layerwise decomposition of our system allows us to leverage tools from approximate inference in probabilistic models to learn \(\). In particular, we propose to use variational inference to learn \(\). We posit an approximate posterior \(q(h)\) over the latent variable \(h\), and bound the marginal log-likelihood of \(y\) given \(x\) by computing the ELBO:

\[ p_{}(y|x)_{h}q(h)[ p_{}(y| _{r}(h,x,_{1}))p_{}(h|(x,_{0}))]+H [q(h)],\] (2)

which allows us to decompose the optimization over \(\) in two independent optimization problems, over both \(_{0}\) and \(_{1}\):

\[_{0}^{*}=_{_{0}}_{x,\,h}w_{h} p(h|(x,_{0})), \ \ _{1}^{*}=_{_{1}}_{(x,y),\,h}w_{h} p(y|(h,x,_ {1}))\.\] (3)

The search over \(_{1}\) is identical to the prompt optimization described in Section 2, with the difference that the inputs now depend on the approximate posterior samples \(h\) in addition to the inputs \(x\). The search over \(_{0}\) uses a similar strategy but uses the posterior samples \(h\) as targets, instead of \(y\).

Although this bound allows us to decompose the optimization w.r.t. \(\), it is only useful if it is close to the true value. Since its looseness is the KL divergence between \(q(h)\) and the true posterior, \((q(h)||p(h|y,x))\): we need to find an approximate posterior \(q\) closely matching the true posterior. In what follows, we specify how we parametrize the approximate posterior and how we tighten the approximation via posterior sharpening.

**Hidden Proposal** We will also be using an LLM to sample candidate hidden states from \(q(h)\). Unless specified, for simplicity, we use the same LM operator used in our language layers. The approximate posterior can condition on arbitrary amount of information but especially useful might be to condition on the true target \(y\). If it conditions on the hidden state coming from the "forward pass", \( p_{}(h|(x,_{0}))\), then \(q_{}(h)=p_{}(h|_{h}(,y,_{1}))\). \(_{h}\) is a specifically tailored hidden proposal template (Appendix D). \(q_{}\) performs a sort of edit operation, where the LM is tasked to rewrite the hidden variable \(\) given some extra knowledge of the ground-truth label \(y\) and of \(_{1}\). Alternatively, we can set the posterior to be equal to the prior, i.e. \(q_{}(h)=p_{}(h|(x,_{0}))\), or the prior with additional information about the label \(y\), \(q_{*}(h)=p_{}(h|_{y}(x,_{0},y))\) (Appendix D). This amounts to re-computing the hidden state knowing privileged information about the label. We found most effective to sample hidden states from a mixture of \(q_{}\) and \(q_{*}\).

**Posterior Sharpening** Given the absence of learnable parameters in \(q(h)\), the induced approximate posterior might still be far from the true posterior. To bridge this gap, we reweigh each sample \(h^{i}\) based on its probability under the true posterior distribution. More precisely, we compute \(^{i}= p_{}(y|_{r}(h^{i},x,_{1}))+ p_{ }(h^{i}|(x,_{0}))\), then assign to each \(h_{i}\) the probability \(w_{i}=(_{i})/_{j}(_{j})\), where \(\) is a tunable temperature parameter that controls the entropy of the posterior weights. The full algorithm for training a DLN-2 is presented in Algorithm 2.

## 4 Practical Instantiation

Although our method aim to learning prompts in stacked LLM architectures, we do rely on a good amount of prompt engineering for our templates. Hereafter, we detail some choices that were fundamental to make our approach work in practice.

**Proposal Diversity** To ensure a diversity of the samples for both the prompt proposal distribution, we found helpful to use two strategies. The first is to modify the backward templates \(_{}\) before drawing a sample from the proposal distribution \(p_{}()\). To achieve so, we parametrize the basic templates with a "[message]" variable that we instantiate from a pool of hand-written instructions, that describe different behaviors the model should follow to propose new \(\), e.g. "[short the previous instruction","give useful examples", etc. These can be interpreted as _meta-instructions_, i.e. high-level directives that inform the model on how to create a better instruction for the task, and extend instruction-induction templates used in [12; 55]. These can be found in Appendix D. In the future, we could envision to extend learning to these instructions. In the case of \(_{}\), they could function as parameters for a _prior_ over the weights of the DLN. The second strategy to ensure more diversity is that we instantiate \(_{}\) with a different random subset of examples in the current batch, before drawing each sample \(^{n}\). This effectively modifies the generation context for each sample \(^{n}\).

**Learning In-Context Learning** One strategy we found particularly effective is to integrate in the pool of meta-instructions an additional instruction that asks the LM to give useful examples to improve its current prompt \(\). Empirically, we observed that this allows the model to sample candidate prompts \(^{n}\) that contain synthetic examples for the task, embedded in natural language. Examples of this interesting behavior can be found in Appendix F. We found that this behavior is particularly interesting as the resulting prompts often perform better than standard ICL. We hypothesize this is due to both _i)_ the "verbalization" of the example in the prompt, which modifies the dataset syntax into a more suitable one, and _ii)_ the fact that the model can dynamically select which examples are most important to integrate in the prompts, given the errors made during training. Therefore, we suspect that DLN achieves a similar effect to recent techniques that select important examples for ICL  with the improvement of naturally conditioning the selection on the end task performance via end-to-end training.

Backtracking and MemoryOptimization of both DLN-1 and DLN-2 is challenging due to the fact that we do not have gradient information and we sample a restricted set of candidates \(^{n}\) at each optimization step due to computational reasons. We deploy multiple strategies to allow the network to be robust to sampling/selection errors. First, we include the current prompt \(\) into the set of candidate prompts to be scored at the current iteration \(^{n}\). This allows the model to not take the step if the previous prompt performed better. Second, we keep a memory of \(M=5\) best prompts found by tracking validation set performance.

Exploration RewardWhen training a DLN-2, we empirically observed that the first layer prompt \(_{0}\) was updating very slowly. Due to the fact that the approximate posterior shares templates with the prior used in the forward pass, the posterior samples \(h^{i}\) are close to \(\) and the maximizer of Equation (3) remains \(_{0}\). To address this issue, we add to the scores of each candidate prompt an exploration reward that is proportional to the negative log-probability of those \(\) that led to an incorrect prediction: \(r=-\, p_{}(|(x,^{n}))\), if \( y\). This encourages the model to both find prompts that maximize the log-probability of high-probability posterior samples and at the same time minimize the log-probability of prior samples that led to incorrect predictions. We anneal \(\) to 0 during training with a constant schedule and we select the initial \(\) by monitoring validation performance for each task.

Experiments and Results

We design and conduct a set of experiments to help answer two main research questions:

* **Q1:** Can we outperform APE and In-Context Learning (ICL) with a DLN-1?
* **Q2:** Does network depth provide further improvement upon DLN-1?

### Experimental Setup

Datasets and TasksWe adopt a set of nine NLP and reasoning tasks commonly used in prior work studying zero- or few-shot learning capabilities of LLMs [23; 10; 39; 42; 1]. We focus on classification tasks. For tasks adopted from BigBench-Hard (BBH)  (Hyper., Nav., Date. and Logic.72), we use the 250 data points provided by BBH as test set. We take the remaining data points from BigBench  that were not included in BBH, and randomly split them (evenly) into training and validation sets. For tasks adopted from  (Mpqa, Trec, and Subj), we randomly sample 400 and 250 data points from their training and test sets, respectively. We use the original validation sets. For tasks adopted from Leopard  (Disaster and Airline), we randomly sample 400, 250, and 250 data points as training, valid, and test. We list all tasks and their statistics in Table 3 in the Appendix.

We use accuracy as the evaluation metric. Specifically, given an input, we compare a system's output string against the ground-truth output string provided by the dataset. We score 1 if the two strings are identical and 0 otherwise. Before the comparison, we process the strings from both the model output and the ground-truth to deal with issues like tokenization and capitalization. In all our DLN experiments, we perform a hyperparameter search and run the same hyperparameter setting with three random seeds. We report the test accuracy averaged over three seeds corresponding to the hyperparameter setting that achieves the highest average validation accuracy. We report details of the hyperparameter search in the Appendix I.

Throughout this paper, we use OpenAI's models, specifically GPT-3 (text-davinci-003) and GPT-4, as the backbone to our proposed systems unless otherwise specified. For DLNs, we use a batch size of 20 and train for 20 iterations by early-stopping on validation performance evaluated every 2 iterations. We then report test scores. We sample \(N=20\) prompt proposals and \(K=5\) hidden samples.

BaselinesWe compare the DLN against two classes of baseline systems. First, we test a set of systems equipped with the same backbone (i.e., GPT-3):

* 0-shot: Given an input, the LLM is required to generate the answer in a zero-shot manner.
* 5-shot (ICL): Given an input as well as five data points as in-context examples, the LLM is queried to generate an answer. The five examples are randomly sampled from the training set.
* KATE : Given an input, we retrieve the five most similar data points from the training set using an off-the-shelf sentence encoder, and use them as in-context examples.
* APE : The LLM is queried to generate a pool of candidate prompts for the task given few input-output pair examples. The candidate prompts are evaluated on a validation set to find the best performing instruction prompt. The best instruction is then used for 0-shot evaluation. We optimize the prompt over both 15 and 400 examples (APE-15 and APE-400 respectively).
* CoT : Given an input, the LLM is first queried to generate a reasoning path with the prompt "Let's think step by step". Then, conditioned on the input and its first output, the LLM is queried to generate an answer. This is the zero-shot version of CoT and is a natural baseline for DLN-2: it performs two LLM calls and can be seen as DLN-2 without optimization. We will report performance of this baseline when comparing to DLN-2.

Additionally, we compare against one of the most advanced LLMs to date, GPT-4. We test 0-shot and ICL settings with GPT-4.

### Dln-1

Our first set of experiments evaluates the 1-layer language network (DLN-1) described in Section 2. Table 1 presents results on the full suite of test tasks. We see that it matches the performance of the best GPT-3-based method on Disaster, Mpqa and Airline and narrowly beats the best GPT-3 baseline on Logic.7 and Nav.. On Hyper., Trec, and Subj, DLN-1 significantly outperforms the best GPT-3 baseline (by about 20, 10, and 7 percentage points, respectively). On Hyper., Trec, and Disaster, it even surpasses GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other tasks. DLN-1's excellent performance on Hyper., a BBH task about ordering adjectives according to linguistic convention, is a surprise. To better understand this result, we show the final prompt in Figure 2. We see that the prompt contains both instructions and a list of examples from the training set. These examples were automatically chosen by the optimizer based on their impact on the performance. This can be seen as a combination of KATE, which selects training examples to put in context based on their similarity with the test example, and APE, which selects the prompt based on its performance. On Date., DLN-1 tends to systematically under-perform the 0-shot baseline both for GPT-3 and GPT-4. We observed that DLN-1 overfits due to paucity of examples in the validation set.

    &  &  &  \\ 
**Method** & **Hyper.** & **Nav.** & **Date.** & **Logic.7** & **Mpqa** & **Tree** & **Subj** & **Disaster** & **Airline** \\ 
**GPT-3** & & & & & & & & & & & \\
0-shot & 60.8 & 64.1 & 56.4 & 45.9 & 88.0 & 61.9 & 61.7 & 81.6 & 75.6 \\
5-shot & 55.6 & 56.5 & 62.1 & 36.7 & 87.2 & 80.0 & 76.4 & 81.2 & 82.7 \\ KATE & 71.1 & 56.9 & 61.1 & 44.4 & 88.4 & 77.6 & 71.3\(\)5.5 & 61.3\(\)7.2 & 54.8\(\)14.6 & 81.6 \\ APE-15 & 68.5\(\)5.5 & 67.3\(\)7.7 & 33.1\(\)2.8 & 45.5\(\)4.7 & 85.5\(\)4.6 & 71.3\(\)5.5 & 61.3\(\)7.2 & 54.8\(\)14.6 & 83.5\(\)3.5 \\ APE-400 & 65.5\(\)4.7 & 56.9\(\)3.2 & 23.5\(\)14.1 & 45.6\(\)12.4 & 84.9\(\)9.7 & 72.0\(\)1.7 & 63.7\(\)9.2 & 60.3\(\)37.4 & 82.3\(\)10.0 \\  DLN-1 & 91.9\(\)3.0 & 68.5\(\)4.7 & 55.7\(\)4.5 & 47.5\(\)2.1 & 88.5\(\)2.5 & 89.7\(\)3.2 & 83.2\(\)6.5 & 81.7\(\)6.5 & 83.2\(\)5.5 \\  
**GPT-4** & & & & & & & & & \\
0-shot & 64.0\(\)1.0 & 74.0\(\)1.0 & 79.2\(\)2.6 & 68.5\(\)3.5 & 86.3\(\)0.6 & 64.8\(\)1.7 & 72.5\(\)1.5 & 47.7\(\)0.6 & 84.5\(\)0.6 \\
5-shot & 88.4\(\)2.6 & 75.7\(\)1.5 & 79.3\(\)1.1 & 62.8\(\)1.7 & 88.0\(\)3.0 & 82.5\(\)3.8 & 94.7\(\)3.5 & 63.6\(\)8.5 & 88.0\(\)1.0 \\
16-shot & 93.3\(\)2.3 & 75.5\(\)5.1 & 80.9\(\)5.0 & 66.4\(\)3.6 & 91.3\(\)1.5 & 83.7\(\)0.6 & 96.5\(\)2.5 & 67.1\(\)4.0 & 88.3\(\)2.1 \\  DLN-1 & 95.2\(\)5.0 & 77.1\(\)4.7 & 76.7\(\)3.0 & 69.1\(\)2.5 & 91.1\(\)3.2 & 89.5\(\)2.1 & 93.1\(\)5.0 & 82.1\(\)3.8 & 85.9\(\)1.5 \\    
  
**IDN-1** & **prom on Hyperation (GPT-4)** & & & & & & & & \\  To determine the correct adjective order, follow this sequence: opinion, size, shape, age, color, origin, material, and purpose. For example, choose “large red plastic ball” over “red large plastic ball” since it follows the order: size (large), color (red), and material (plastic). Not all adjectives may be present, but the order should still be maintained. If the options are “ancient prismlike white leather” admitting match” and “leather white ancient prismlike whiting match”, choose the first option, as it follows the order: age (ancient), shape (prismlike), color (white), material (leather), and purpose (whitting). Remember that opinion always comes before age, so “on

### Dln-2

We investigate the effectiveness of depth through experiments with 2-layer language networks (DLN-2) on tasks where we expect depth to be most useful, and on which DLN-1 significantly underperforms the GPT-4 0-shot baseline, i.e., Nav., Date., and Logic.7 . Since the Nav., Date. and Logic.7 tasks from BBH require more complex spatial and temporal reasoning, they are the ones where we most expect a decomposition into subtasks to be helpful. We also include Subj and Disaster as an example where DLN-1 performs well (even outperforming the GPT-4 0-shot baseline), since we are interested to see to what extent DLN-2 can further push performance.

Results for DLN-2 can be found in Table 2. Compared to DLN-1, DLN-2 provides an average boost of 7.2% absolute score. On Nav. and Date., DLN-2 largely improves the performance of DLN-1, outperforming all single layer networks. On Logic.7, all methods appear to perform similarly. This could point to the fact that the task might be too hard for the base LLM and thus highlights the limits of prompt optimization of a weak base model. On Subj and Disaster, DLN-2 achieves further improvement over DLN-1. Compared to 0-shot GPT-4 results in Table 1, on Subj and Disaster, DLN-2 on average provides more than 20% in absolute improvement. We encourage readers to find additional experimental results in Appendix C.

## 6 Related Work

**Prompt-Based Machine Learning** GPT-3  launched a new paradigm in NLP called in-context learning (ICL), now applied beyond traditional NLP tasks . The discovery of chain-of-thought prompts (CoT) marked a major advance in prompting: LLM performance improves markedly when the prompt includes examples of intermediate reasoning steps  (few-shot CoT), or simply instructs the model to "think step by step"  (zero-shot CoT). Like CoT, DLNs break a problem down into intermediate steps but they operationalize these steps as separate LLM calls, each defined by its own learned prompt. Since the introduction of CoT, prompting techniques have evolved to be more dynamic and iterative. Recent methods often operate recursively. Examples include RECITE , Self-ask , and related methods for question-answering Creswell et al. , Zhou et al. . A similar class of methods relies on "introspection" , where an LLM is prompted to ingest, evaluate then possibly act on its own previous output. Self-critique , ReAct , Reflexion , Self-refine  fit this mould along with Hao et al. , Du et al. , Yao et al. .

**Prompt Optimization** Techniques based on notions of self-talk and self-evaluation align naturally with automatic prompt optimization--a core function in DLNs. Early work in this category includes Autoprompt  and GRIPS . Deng et al.  argue that 'enumeration-then-selection' heuristics for optimizing discrete prompts do not explore the prompt space systematically. They take an RL approach to overcome this problem, training a policy network, via soft Q-learning with a suitably designed and stabilized reward function, to generate effective prompts. Through Gibbs sampling, Repropmpting  iteratively searches CoT recipes to improve prompt performance automatically. Most relevant to DLNs, Zhou et al.  present Automatic prompt engineer (APE). APE optimizes an initial prompt by searching over a pool of candidates to maximize a score function. We use an APE-inspired approach in DLNs and we cast the proposal/scoring functions as elements of variational inference. In a concurrent work, Pryzant et al.  proposed using textual gradients in automatic prompt optimization. This algorithm uses LLM's nonparametric feedback to guide prompt generation and selection.

  
**Method** & **Nav.** & **Date.** & **Logic.7** & **Disaster** & **Subj** \\ 
0-shot & 64.1 & 56.4 & 45.9 & 81.6 & 61.7 \\ CoT & 69.3 & 72.4 & 41.1 & 54.4 & 59.3 \\ APE & 67.3\(\)7.7 & 32.1\(\)28.5 & 45.5\(\)4.7 & 54.8\(\)14.6 & 61.3\(\)7.2 \\ APE-400 & 56.9\(\)32.9 & 23.5\(\)14.1 & 45.6\(\)12.4 & 60.3\(\)37.4 & 63.7\(\)9.2 \\  DLN-1 & 68.5\(\)4.7 & 55.7\(\)4.5 & 47.5\(\)2.1 & 81.7\(\)6.5 & 83.2\(\)5.5 \\ DLN-2 & 83.1\(\)24.7 & 75.2\(\)14.8 & 45.7\(\)3.5 & 82.8\(\)2.5 & 85.9\(\)8.7 \\   

Table 2: DLN-2 test accuracy using GPT-3 as LLM.

**Multi-Layer LLM systems** Several recent works compose LLMs as nodes in a computational graph, which is the core idea of DLNs. Some work cited above can be seen as instances of this idea. Similarly, Khot et al.  induce an LLM to generate a basic "control flow" that calls distinct LLM modules. Wu et al.  propose AI chains, an interactive system of chained LLMs based on a set of "LLM primitive" operations. They conduct a 20-person user study in which participants modify chains, and find this process to improve task performance, transparency, and controllability. Dohan et al.  unify LLMs and graphical models as "language model cascades". Specifically, they cast LLM compositions as graphical models with string-valued random variables.3 They show how scratchpad , chain-of-thought , tool use , and several other prompting strategies fit their formalism. DLNs can likewise be considered an instance of language model cascade, because of that framework's generality. However, going beyond the conceptual work of Dohan et al. , we present an effective technique for doing inference in an LLM-based graphical model and we apply learned networks of LLMs to several downstream tasks.

## 7 Conclusion and Future Work

In this paper we introduced an algorithm for joint prompt optimization in deep networks where each layer is an LLM. To do so, we consider outputs of each hidden LLM layer as a latent variable we need to do inference over. From a conceptual perspective, we demonstrated how CoT can be seen as a DLN-2 with a residual connection. Similarly, Generated Knowledge Prompting  could be considered as a fixed forward-only DLN-2 where, in the first layer, an LLM generates related knowledge, and in the second layer, another LLM takes the generated knowledge as input and generates the final answer. Other prompting techniques like ReAct , Reflexicon , and Self-Consistency  could all be ensembles of DLN-1s with different prompt initializations.

Although we only tested 1-layer and 2-layer LNs so far, we already show that the performance of smaller LLMs can be boosted when stacked and prompted properly. We believe the modularity of these architectures will make them more adaptable and reusable to new use cases. While accuracy on downstream tasks is an appealing metric, we argue that other considerations are just as important, for example the ease of adapting a model to one's own use case, or the ability to leverage multiple existing models.

We noticed that GPT-3 has a tendency to always produce an answer given an example: this could be due to the particular 0-shot fine-tuning procedure, which biases the model towards generating useful responses. This raises the question of whether we can fine-tune "stackable" LLMs and whether DLNs can be used as a framework to generate training data for that purpose. Second, we engineered our backward and forward templates; in the future, we wish to expand our work to learn parts of such templates: we expect this to make the variational bound tighter and thus easing DLN's optimization. Additionally, while we only proposed 2-layer DLNs, the framework accommodates arbitrary directed acyclic graphs.

Impact statementWhile we are fully aware of the limitations of addressing societal issues through technical work, we hope that modular approaches like ours will alleviate some of the issues associated with LLMs, like the concentration of power associated with the difficulty to train them. We also hope that, by facilitating the reusability and adaptivity of such models, we shall make them more amenable to a wider variety of use cases. However, while we discuss the performance of these models on artificial benchmarks, we do not address the question of when and how such models should be deployed, nor do we offer additional guarantees against their misuse. We also emphasize that performance on artificial tasks, even if realistic, is neither representative of performance in uncontrolled environments, nor enough to justify the deployment of these models in high stakes situations.

AcknowledgementsWe would like to acknowledge Silviu Pitis for the useful feedback on the draft, Nikolay Malkin and Tong Wang for their advice during the first steps of this project.