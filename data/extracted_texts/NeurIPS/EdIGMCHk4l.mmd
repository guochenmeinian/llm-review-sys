# RRHF: Rank Responses to Align Language Models

with Human Feedback

 Hongyi Yuan\({}^{12}\)\({}^{1}\) Zheng Yuan\({}^{1}\) Chuanqi Tan\({}^{1}\) Wei Wang\({}^{1}\) Songfang Huang\({}^{1}\) Fei Huang\({}^{1}\)

\({}^{1}\)Alibaba DAMO Academy \({}^{2}\)Tsinghua University

{yuanzheng.yuanzhen,chuanqi.tcq}@alibaba-inc.com

yuanhy20@mails.tsinghua.edu.cn

Contributed equally. \(\) Work done during internship at Alibaba DAMO Academy.

###### Abstract

Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling. Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-\(n\) learner. Codes are released at https://github.com/GanjinZero/RRHF.

## 1 Introduction

Large language models like ChatGPT2 and GPT-4  are extremely powerful in understanding human queries and providing helpful and friendly responses. Employing Reinforcement Learning from Human Feedback (RLHF)  enables alignment of language model outputs with human preferences. As implemented in Ouyang et al. , the paradigm of RLHF contains three main steps, Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). Initially, they apply supervised fine-tuning (SFT) on the initial models to learn to follow human instructions. Subsequently, a reward model is learned from the ranking of human preferences. Finally, scores generated by the reward model are used to apply gradient policy in PPO to align human preferences. PPO  is a strong reinforcement learning (RL) algorithm and is the key step used in RLHF  to align human preferences. This PPO training step is powerful but complex. It requires tuning a large number of hyperparameters for conservative parameter updating, reward design, advantage estimation, etc. Besides, fine-tuning language models with PPO needs to store apolicy model, a value model (or a value head), a reward model, and a reference model at the same time which is memory-unfriendly and needs sophisticated architecture of the training platform when scaling up to larger models.

To alleviate the complex hyperparameter tuning and sophisticated training resource requirements of PPO, we propose a novel training paradigm **RRHF** (**Rank** Responses to align **H**uman **F**eedback) that aligns model probabilities of multiple responses with human preferences by ranking loss, which can retain the performance of PPO and is much simpler. Ranking loss on responses probabilities  has been used in a similar scenario, abstractive summarization, to improve conditional generation quality. Before training, RRHF first samples responses from various sources, responses can be sourced from a wide range of origins including model-generated responses such as those from the model itself, ChatGPT, GPT-4, as well as pre-existing human-authored high or low-quality responses. RRHF then leverages responses from various sources for training, scoring responses based on the log probability provided by the training language model. The scores are then matched orders with those from the human preference reward model or human preference labels by ranking loss. We choose to use ranking instead of the absolute value of the reward model for optimization. PPO uses estimated advantages to provide optimization signals. The advantage function is to estimate whether the state-action pair is better or worse compared to the baseline and the baseline is estimated by the value model. Consequently, advantage function estimation requires auxiliary models for training and inference during the whole training procedure . In RRHF, you can estimate the response qualities by logarithm probabilities and compare multiple responses corresponding to know which responses are better or worse without estimating the baseline by an additional value model. Compared to PPO, RRHF also does not need the reference model to calculate the KL divergence. the model itself used for generating samples in PPO is constantly changing while RRHF only uses the model itself for sampling before training. Thus the KL term degenerates for RRHF. The workflow for RRHF and PPO is depicted in Figure 1. PPO utilizes 4 models during training, whereas RRHF requires only 1 or 2 models.

Our experiments are conducted on Anthropic's Helpful and Harmless dataset , demonstrating that RRHF's performance is on par with PPO in terms of generating helpful and harmless responses by automatic evaluation and human labeling. We do extensive experiments on how sampled responses used in training affect the performances of RRHF. The performances of RRHF are positively correlated to the qualities of sampled responses. We find that the rewards of the trained models are close to the max rewards of the sampled responses which suggests that RRHF's objective is to learn from best-of-\(n\) sampling. Moreover, to simulate the real scenario of training a ChatGPT-like model. We use RRHF to learn from Alpaca prompts  and responses from ChatGPT, InstructGPT, LLaMA , and Alpaca to develop a new language model aligned to human preferences called Wombat. The evaluation of Wombat shows that RRHF can outperform SFT under similar training resources.

Contributions are summarized as follows:

* We propose a new learning paradigm named RRHF for large language models that can leverage various responses to align with human preferences. The trained model can be viewed as a language model for generation and a reward model for scoring.

Figure 1: Workflow of RRHF compared with PPO.

* This paradigm is an extension of SFT training and is similar to training a reward model.
* This paradigm is much simpler than PPO in terms of coding difficulty, numbers of models used in training, and hyper-parameter counts and obtains comparable performances on Anthropic's Helpful and Harmless dataset.

## 2 Related Works

Recently, scaling up pre-trained language models by the number of parameters, training data , and computational budges  can equip large language models with strong abilities in various language tasks [5; 24; 7; 16; 21; 39]. However, pre-trained language models are not directly aligned with human preferences which may generate unsafe, toxicity, sexual, biased, or criminal responses. Language models first conduct supervised fine-tuning to imitate how to align with human instructions [35; 31]. After that, reinforcement learning techniques have been explored to align language models with human preferences [2; 4; 29; 14; 36; 22; 25]. The most successful way is applying a reinforcement learning from human feedback (RLHF) framework [42; 29; 22] via training a reward model on human feedback and using PPO  to obtain the policy model for language generation. In our practices, the PPO training paradigm is complex in coding and hyperparameter tuning while it needs four models that are hard for training. This motivates us to explore simpler and more straightforward methods to align language models with human preferences. Nakano et al. , Askell et al. , Cobbe et al.  explore best-of-\(n\) sampling to improve large language model generation by selecting the best response based on the human preference rewards among \(n\) sampled responses. Best-of-\(n\) sampling is easy to achieve for aligning with human preferences while costing much more time when inference. Inspired by these two lines of work, RRHF is targeted to learn the best response and comparisons based on the human preference rewards among \(n\) sampled responses to achieve alignment during optimization instead of inference. RRHF absorbs the advantages of PPO and best-of-\(n\) sampling while being simpler in coding, model count, and hyperparameter tuning than PPO and does not need to sample \(n\) times during inference. The most similar work  is contemporary to us which applies SFT on the samples with the best reward. Compared to Dong et al. , we show that ranking loss is necessary and research the relation between sampling quality and model performance. There are also other ways to apply alignment which are focused on generating better-aligned datasets for SFT including hindsight-modified prompts [40; 18] and principle-driven self-alignment .

## 3 Approach

We mainly follow the notations in Ziegler et al. . Denote the query data distribution as \(x D\). For the response \(y\) reply to query \(x\), a reward function \(R(x,y)\) scores \(y\) based on human preferences which can be a human or a neural network. Our target is to learn an auto-regressive language model \(\) (initialized from the model \(\)) which generates responses with large rewards.

### Rrhf

During training, we have \(k\) different responses \(y_{i}\) of \(x\) sampled by policy \(_{i},1 i k\). Sampling with policy \(_{i}\) is not restricted here which can be the initial model \(\), the learned model \(\), other LLMs like ChatGPT or GPT-4, or a response provided by human experts. The sampling policy \(_{i}\) can also vary across the training time. Our sampling method can leverage any existing good or bad responses to help the model align with humans, while PPO can only learn from samples from its learned model \(\).

The reward function gives scores for each \(y_{i}\) with \(R(x,y_{i})=r_{i}\). To align with scores \(\{r_{i}\}_{k}\), we use our model \(\) to give scores \(p_{i}\) for each \(y_{i}\) by:

\[p_{i}= P_{}(y_{i,t}|x,y_{i,<t})}{\|y_{i}\|},\] (1)

where \(p_{i}\) is conditional log probability (length-normalized) of \(y_{i}\) under model \(\). Our idea is simple, let the model \(\) give larger probabilities for better responses and give smaller probabilities for worse responses. Inspired by Liu et al. , we optimize this object by ranking loss:

\[L_{rank}=_{r_{i}<r_{j}}(0,p_{i}-p_{j})\] (2)We do not have margins in the ranking loss as Liu et al. (2019). They add margin terms \(_{ij}=(j-i)\) to encourage the model to have higher \(p_{i}\) estimation with a higher ranking. We disable it since we find good empirical results without margin terms and it is time-consuming to tune \(\).

We also add a cross-entropy loss similar to SFT (supervised fine-tuning). We require the model to learn the response with the highest reward \(r_{i}\).

\[i^{} =_{i}r_{i}\] (3) \[L_{ft} =-_{t} P_{}(y_{i^{},t}|x,y_{i^{},<t})\] (4)

The total loss is defined as the unweighted sum of two losses:

\[L=L_{rank}+L_{ft}\] (5)

We have tried using larger weights (10,100) on \(L_{rank}\) suggested by Liu et al. (2019) which shows worse performances in our preliminary experiments.

The Python training code of RRHF only adds 30 lines to SFT training code 3 which is much simpler than PPO implementation 4.

### Relation with Previous Paradigm RLHF

InstructGPT (Zhu et al., 2019) aligns human preferences in three steps: SFT, training a reward model, and PPO. We find our proposed RRHF has similar procedures to the above-mentioned three steps.

Relation with SFTSupervised fine-tuning (behavioral cloning) can be viewed as a degenerated version of our training process with \(k=1\) and \(_{1}\) being fixed which is provided by human labelers.

Relation with Reward ModelOur model can be used as a reward model. We use length-normalized log probability to score responses, while other reward models use [CLS] or [EOS] for scoring. If \(R(x,y)\) is labeled by human labelers, we are exactly training a reward model from human preferences.

Relation with PPOThe task objective of PPO (Zhu et al., 2019) is defined by a reward function \(R(x,y)\), and it is to maximize the expected reward \(_{x,y(|x)}[R(x,y)]\). Although \(R(x,y)\) should be defined by human assessments, \(R(x,y)\) is modeled with a learned reward model on human-evaluated data in experiments. To constrain the language policy \(_{}(|x)\) from moving too far from the initialization \((|x)\), the final reward design becomes: \((x;y)=R(x;y)-()\), where \(\) controls the level of penalty and is set to a fixed value (Zhu et al., 2019) or dynamically adjusted (Zhu et al., 2019).

PPO leverages \(\) for sampling, while RRHF can use any applicable \(_{i}\). PPO is sampling during training, while RRHF is sampling before training to get rid of the KL divergence term. PPO uses the absolute reward value \(R(x,y)\) for optimization, while we only consider the comparisons of \(R(x,y)\) between different responses which are easier to learn. PPO requires one more value model to compare with the baseline, while RRHF makes comparisons among sampled responses to avoid the value model.

## 4 Experiments

### Settings

DatasetWe use Anthropic's Helpful and Harmless (HH) dataset as our experiment dataset (Bahdan et al., 2017)5. They provide a chosen response and a rejected response for each query based on human preferences (i.e. helpful and harmless). We use the _proxy_ reward model Dahoas/gptj-rm-static6 trained on the same dataset. By using the _proxy_ reward model, we can compare RRHF and PPO fairly.

Sampling Policy during TrainingOur model's ability is highly related to sampling qualities during training. We examine different sampling policies and list them in Figure 2 and Table 1. We term the initial language model policy as \(\), the online language model policy as \(\), and the language model policy after each 3-epoch training iteration as \(\)*. For each query, we collect 4 roll-out samples using two variants of beam search. For vanilla beam searching, we use a beam size of 4 and set the maximum output token length to 128. Since the roll-out sample diversity of vanilla beam search is low, we also experiment with (1) diverse beam search , where we use a beam size of 4 and set the diverse beam group to 4, the diversity penalty to 1.0, and the sampling temperature to 0.8, and (2) top-p sampling (nucleus sampling) , where we use a beam size of 4, top-p of 1.0, and the sampling temperature to 0.8 which is a consistent setting with the top-p sampling used in our PPO baselines. We sample training data before the training process except for OP-k (online sampling). Sampling using vanilla beam search/diverse beam search/top-p sampling costs 4-6 hours on 8 80GB Nvidia A100 GPUs.

Fine-tuning Hyper-parametersWe fine-tune RRHF with 3 epochs without early stopping. We first warm up the learning rate to 2e-5 and decay to 0 linearly. For each GPU we have at most 1 query at once, and we apply gradient accumulation at 8 steps leading to a query batch size of 64. The query and responses are truncated to 192 tokens. Since sampling and training processes are separated (except online sampling), our training only needs to load one model. We use 8 80GB Nvidia A100 GPUs for fine-tuning, training RRHF without online sampling typically costs 4-6 hours. Training with OP is slower which takes about 30 hours.

BaselinesWe compare our trained models \(\) with responses from the datasets, initial checkpoints \(\) and PPO trained models. For PPO, we formulate a token-wise Markov decision process, where the action is a token \(y_{t}\) to be generated at time step \(t\), and the state is the token sequence of the query \(x\) and formerly generated tokens \(y_{<t}\). We follow the clipped surrogate objective of PPO:

\[_{y_{ t}_{}(y_{ t}|x),x}[ (-r_{}(y_{t}|x,y_{<t})(x,y_{ t}),-_{1- }^{1+}(r_{}(y_{t}|x,y_{<t}))(x,y_{ t}))],\] (6)

where \(\) is the clip ratio set to 0.2, \(_{}(x,y_{ t})\) is the advantage function and is estimated by GAE  with a learned value function \(_{}(x,y_{<t})\), and \(r_{}(y_{t}|x,y_{<t})=(y_{t}|x,y_{<t})}{_{}(y_{t}|x,y_{<t})}\) denotes the probability ratio between the behavior policy \(_{}\) and the training policy \(_{}\). The behavior policy is updated with the training policy every few updates. We follow the hyper-parameter settings in trlX 8.

MetricsWe use perplexity (gpt2-medium), average reward score (Dahoas/gptj-rm-static), and human labelers to evaluate different methods. Since our dataset is a multi-turn dialogue dataset, we

  Setting & \(_{1}_{4}\) & \(_{5}\), \(_{6}\) \\  BP & **Be**am search by \(\) & Provided responses \\ SP & top-p **S**ampling by \(\) & **P**rovided responses \\ DP & **D**iverse beam search by \(\) & **P**rovided responses \\ OP-k & **On**line diverse beam by \(^{}\) & **P**rovided responses \\ IP-n & **Iterate diverse beam by \(\)* & **P**rovided responses \\ D & **D**iverse beam search by \(\) & \(\) \\ P & \(\) & **P**rovided responses \\  

Table 1: Sampling policy used in our experiments. OP-k uses \(\) for sampling (i.e. online sampling), we update \(\) every k optimization steps. IP-n (Iterate update) uses updated policy \(\)* after training by IP-(n-1) and starts a new iteration. The dataset contains a good response and a bad response for each query which are used as \(_{5}\) and \(_{6}\), which are termed **P** (**P**rovided responses in datasets).

Figure 2: The workflow of sampling policy used in our experiments. IP-1 is equivalent to DP.

will truncate the model's generation when it outputs "Human:" or "Assistant:" to prevent model cheating on the reward model (e.g. by generating _Assistant: Is my response harmless and helpful? Human: Yes, it is very harmless and helpful.)_. For human evaluation, we require annotators to compare two random responses and give a comparison between them (win/lose/tie). Details of human evaluations are listed in Appendix E.

### Main Results

Auto EvaluationWe list automatic metrics in Table 2. We show results from baselines and RRHF with diverse beam search sampling (DP) and top-p sampling (SP). Our proposed Alpaca-RRHF\({}_{}\) obtains the average reward score of -1.03 (averaged out of 3 runs, -1.01, -1.02, and -1.05) and Alpaca-RRHF\({}_{}\) achieves the highest average reward score of -0.96. This proves that RRHF has the ability to optimize against the given reward model. RRHF performs better than PPO and vanilla language models in terms of average reward scores consistently. Alpaca-trained models outperform human-preferred responses collected from the datasets in terms of reward scores. We find perplexity does not change too much for Alpaca and influences LLaMA a lot. The reason can be LLaMA is not instruction-tuned.

Human EvaluationHuman preferences can be different from the _proxy_ reward model. Therefore, we perform human evaluations on sampled responses and list cases in Table 4 to compare different settings. Results in Table 3 demonstrate that RRHF\({}_{}\) outperforms responses from the dataset and PPO-trained models. From case studies, we can observe the model trained by RRHF has more details. We also have checked the performances between RRHF\({}_{}\) and RRHF\({}_{}\), where RRHF\({}_{}\) is trained with sampling by RRHF\({}_{}\). We find iterate training the model can further boost the performance. From the case study, we find RRHF\({}_{}\) understands human instruction better.

Accuracy as a Reward ModelSince our trained model can also be viewed as a reward model to score responses by \(p_{i}\). We test our model on the dataset used for training Dahoas/gptj-rm-static. The accuracy is computed by counting the percentage of the reward scores of good responses that are higher than the reward scores of bad responses using \(p_{i}\) (except Dahoas/gptj-rm-static) and list in Table 5.

  \(\) & Setting & PPL & Reward \\  Good responses & \(\) & 21.46 & -1.24 \\ Bad responses & \(\) & 121.29 & -1.48 \\  LLaMA & \(\) & 20.78 & -1.89 \\ Alpaca & \(\) & 14.34 & -1.18 \\ Alpaca-sft & \(\) & 18.98 & -1.46 \\  Alpaca & Best-of-4 & - & -0.97 \\  LLaMA & PPO & 42.53 & -1.62 \\ Alpaca & PPO & 13.84 & -1.03 \\ Alpaca-sft & PPO & 19.10 & -1.25 \\  LLaMA & RRHF\({}_{}\) & 67.12 & -1.34 \\ Alpaca-sft & RRHF\({}_{}\) & 18.10 & -1.19 \\ Alpaca & RRHF\({}_{}\) & 14.75 & -1.03 \\ Alpaca & RRHF\({}_{}\) & 14.41 & **-0.96** \\  

Table 2: Automatic evaluation on HH dataset. Good/bad responses with \(\) setting represent only human-written responses from the HH dataset are evaluated. LLaMA, Alpaca, and Alpaca-sft with \(\) setting represent we directly evaluate the model without further tuning.

  A & B & win & tie & lose \\  RRHF\({}_{}\) & Good responses & 59 & 30 & 11 \\ RRHF\({}_{}\) & PPO & 27 & 48 & 25 \\ RRHF\({}_{}\) & RRHF\({}_{}\) & 0 & 90 & 10 \\  

Table 3: Human evaluation on HH dataset. All settings use \(\)=Alpaca.

Dahoas/gptj-rm-static achieves \(68.49\%\) on the test set. The accuracy of LLaMA, Alpaca, and Alpaca-PPO is worse than random guessing. Our model Alpaca-RRHF\({}_{}\) trained by Dahoas/gptj-rm-static can achieve \(61.75\%\) accuracy which is much better than vanilla language models and PPO-trained models. As our model learns from the _proxy_ reward model rather than the training dataset of the reward dataset, it becomes difficult to surpass Dahoas/gptj-rm-static in terms of performance on the test set. Nonetheless, it demonstrates potential in adapting to the _proxy_ reward model and could have a significant impact on real human preference labels.

Loss CurveWe show our loss and metric curves in Figure 3. This is the setting of using Alpaca as the initial model \(\) and the sample policy is DP. We find losses and average reward scores are negatively correlated where one can track the loss curve to estimate the reward scores. We find the losses converge at the third epoch (i.e. 2400-3600 training steps) and the average reward scores reach the maximum at the third epoch. Our proposed RRHF converges well under the same hyper-parameter setting as SFT.

### Ablation Study

Initial CheckpointsLLaMA performs worst among the three initial checkpoints with different settings in Table 6. This is not due to the potential of LLaMA being worse than Alpaca. By using only the response data from the datasets (sampling policy P) for training, LLaMA, Alpaca, and Alpaca-sf obtain the same average reward scores of -1.31 which show that these three models have the same ability under the same sampled training data. LLaMA is not instruction-tuned and responses sampled by LLaMA (reward -1.89) are much worse than two other models (reward -1.18 and reward -1.46). The sampling quality of LLaMA makes it perform the worst. Another phenomenon we find is

|} 
**Human:** How can I make my clothes whiter? \\
**Assistant:** You can use vinegar to bleach the clothes. \\
**Human:** Vinegar can bleach clothes? \\
**Assistant:** Yes. \\
**Human:** I was thinking about using Clorox as well. \\
**Assistant:** (RRHF\({}_{}\)) You can use Clorox, too. It’s a brand of bleach. You can also use baking soda, which is a mild detergent. \\
**Assistant:** (Response from the dataset) Clorox is more toxic and harmful than vinegar. \\ 
**Human:** How do you adjust the lens on a binocular? \\
**Assistant:** (RRHF\({}_{}\)) You can adjust the lens on a binocular by turning the focus knob on the right eyepiece. This will allow you to focus on objects at different distances. You can also adjust the lens to get the best viewing experience. \\
**Assistant:** (PPO) You adjust the lens on a binocular by turning a ring around the center of the binocular. This adjusts the focus of the binocular and allows you to see objects at different distances clearly. \\ 
**Human:** Which stock should I invest all of my money into? \\
**Assistant:** Interesting question! What kind of returns do you want? \\
**Human:** I want better returns than the S\&P 500. \\
**Assistant:** (RRHF\({}_{}\)) Okay, let’s look at some options. The S\&P 500 is a broad-market index that tracks the stock prices of 500 large companies in the United States. It has a historical average annual return of around 10%. \\
**Assistant:** (RRHF\({}_{}\)) Okay, well there are a few factors to consider when investing. You should look at the risk of losing money, the expected return, and the amount of money you have to invest. You should also research different investment vehicles and decide which one is best suited for your needs. \\  

Table 4: Responses generated by the different settings of models.

  Reward Model & Accuracy \\  Dahoas/gptj-rm-static & \(68.49\%\) \\ LLAMA & \(45.09\%\) \\ Alpaca & \(45.13\%\) \\ Alpaca-PPO & \(46.03\%\) \\ Alpaca-RRHF\({}_{}\) & \(61.75\%\) \\  

Table 5: Reward model accuracy evaluation.

Alpaca-sf performs worse than Alpaca, and this is also observed by Ramamurthy et al.  that SFT warmup may not improve the performance.

Sampling PolicyAs stated previously, sampling policy deeply influences the performance of our training schema. We list results with different sampling policies in Table 6. Using diverse beam sampling performs best for Alpaca among all non-online sampling methods, while for another two models using beam sampling is good. We also try to only use two responses provided by datasets, three models obtain very near performances with reward -1.31 which shows sampling quality determines RRHF performances. Using beam or diverse beam sampling with responses from datasets enhances performances significantly compared to only using responses from datasets. We test on Alpaca by only using samples generated by the model itself, it also improves reward to -1.08. For the iterate update sampling policy, we find the reward scores can be improved by iteration.

Ranking LossTo check whether the ranking loss is necessary, we conduct an ablation study by removing \(L_{rank}\), and the results are shown in Table 7. Without ranking loss, models cannot learn from how one response is better than another and obtain a worse average reward score.

  \(\) & Setting & PPL & Reward & Mean & Std. & Max \\  LLaMA & DP & 67.12 & -1.34 & -2.18 & 0.97 & -1.27 \\ Alpaca & DP & 14.75 & **-1.02** & -1.30 & 0.66 & -0.95 \\ Alpaca-sft & DP & 18.10 & -1.19 & -1.49 & 0.79 & -1.11 \\  LLaMA & BP & 17.03 & -1.27 & -2.26 & 0.96 & -1.26 \\ Alpaca & BP & 14.37 & -1.03 & -1.31 & 0.67 & -1.00 \\ Alpaca-sft & BP & 17.63 & -1.14 & -1.50 & 0.77 & -1.15 \\  LLaMA & P & 18.49 & -1.31 & -1.50 & 0.79 & -1.28 \\ Alpaca & P & 18.88 & -1.31 & -1.50 & 0.79 & -1.28 \\ Alpaca-sft & P & 18.92 & -1.31 & -1.50 & 0.79 & -1.28 \\  Alpaca & D & 13.66 & -1.08 & -1.21 & 0.65 & -1.02 \\  Alpaca & IP-1 & 14.75 & -1.02 & -1.30 & 0.66 & -0.95 \\ Alpaca & IP-2 & 14.31 & -0.96 & -1.13 & 0.57 & -0.77 \\ Alpaca & IP-3 & 14.51 & -0.94 & -1.05 & 0.56 & -0.65 \\  Alpaca & OP-32 & 63.78 & 0.34 & - & - & - \\ Alpaca & OP-32+KL & 19.76 & **-0.86** & - & - & - \\  

Table 6: Ablation study on HH dataset with different initial checkpoints and sampling policy. We also list the average, max, and standard error of the reward scores for training samples generated by different sampling policies. We do not truncate responses from the training set, while we truncate responses to the first turn for the testing set when calculating reward scores.

Figure 3: The loss and metric curves of training RRHF. The model uses DP as the sampling policy.

[MISSING_PAGE_FAIL:9]

## 6 Conclusion

We propose a new paradigm RRHF which can be tuned as easily as fine-tuning and achieve a similar performance as PPO in the HH dataset. A model trained by our paradigm can be viewed as a language model and a reward model at the same time. Also, RRHF can leverage responses from various sources to learn which responses have better rewards based on human preferences. Our paradigm is easier to scale to the larger size LLMs and is easier to adopt on limited training resources. Another merit of RRHF is capable of any fine-tuning techniques [37; 17; 38], since Ramamurthy et al.  find using techniques like dropout makes RL training unstable. We hope RRHF can open the way to align human preferences easily.

## Limitations

We use the reward model in our experiments to act as a proxy evaluation metric which may be not complex enough compared to human preference, while the extension to real-world human preference score is trivial. As an algorithm for alignment, the method is highly correlated to the human preference or used reward score. Malicious or harmful reward scores or human preference ratings may mislead the LLM to generate unsafe results.

For the algorithm itself, RRHF requires multiple responses as inputs which increases the GPU usage for a single query compared to PPO. Neglect the performance of online sampling RRHF which is slower than PPO and RRHF. In our preliminary experiments, RRHF may be prone to over-optimization to _chead_ the reward models when using the online or iterated sampling versions. it is a common problem for all related algorithms including RRHF/PPO/best-of-n sampling as stated in . How to prevent such over-optimization is an important problem and needs further exploration in the future.