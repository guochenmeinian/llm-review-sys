# Discovery of the Hidden World with

Large Language Models

 Chenxi Liu\({}^{1}\), Yongqiang Chen\({}^{2,3,4}\), Tongliang Liu\({}^{5,2}\)

\({}^{1}\)TMLR Group, Hong Kong Baptist University

\({}^{2}\)Mohamed bin Zayed University of Artificial Intelligence

{cscxliu,bhanml}@comp.hkbu.edu.hk {yqchen,jcheng}@cse.cuhk.eduhkMingming Gong\({}^{6,2}\), James Cheng\({}^{4}\), Bo Han\({}^{1}\), Kun Zhang\({}^{2,3}\)

\({}^{3}\)Carnegie Mellon University \({}^{4}\)The Chinese University of Hong Kong

\({}^{5}\)Sydney AI Centre, The University of Sydney \({}^{6}\)The University of Melbourne

tongliang.liu@sydney.edu.aumingming.gong@unimelb.edu.au kunz1@cmu.edu

###### Abstract

Revealing the underlying causal mechanisms in the real world is the key to the development of science. Despite the progress in the past decades, traditional causal discovery approaches (CDs) mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. The _lack of well-defined high-level variables in many real-world applications_ has already been a longstanding roadblock to a broader application of CDs. To this end, this paper presents **C**ausal representati**O**n **A**ssistan**T** (COAT) that introduces large language models (LLMs) to bridge the gap. LLMs are trained on massive observations of the world and have demonstrated great capability in extracting key information from unstructured data. Therefore, it is natural to employ LLMs to assist with proposing useful high-level factors and crafting their measurements. Meanwhile, COAT also adopts CDs to find causal relations among the identified variables as well as to provide feedback to LLMs to iteratively refine the proposed factors. We show that LLMs and CDs are mutually beneficial and the constructed feedback provably also helps with the factor proposal. We construct and curate several synthetic and real-world benchmarks including analysis of human reviews and diagnosis of neuropathic and brain tumors, to comprehensively evaluate COAT. Extensive empirical results confirm the effectiveness and reliability of COAT with significant improvements.

## 1 Introduction

Science originates along with identifying important variables and revealing their causal relations . Despite the progress in the past decades, existing causal discovery approaches (CDs) mainly rely on high-quality measured variables, which are usually given by human experts . However, the desired variables and their measurements are usually unavailable in a wide range of real-world applications. For example, Amazon sellers who want to analyze the factors related to user ratings only have raw user reviews, which are written according to the underlying user preferences for certain product characteristics. Therefore, _the lack of high-quality high-level variables_ has been a longstanding impediment to broader real-world applications of CDs or causality-inspired methods .

The recently emerged Large Language Models (LLMs) [7; 8; 9; 10] offer a new opportunity to mitigate the gap . Trained from massive observations of the world, LLMs demonstrate impressive capabilities in _comprehending unstructured inputs_, and leveraging the learned rich knowledge to resolve a variety of general tasks . A surge of early tests demonstrates promising results that LLMs can effectively leverage the learned knowledge to answer commonsense causal questions [11; 13; 14]. Nevertheless, existing approaches mainly focus on incorporating LLMs as a _straightforward reasoner_ with respect to the given _causal variables_. The reliability of LLMs in directly reasoning the true causal structure behind any specific data-generating process remains a debate [13; 15; 16; 17] due to a series of drawbacks of LLMs [18; 19; 20]. In addition, all of the existing combinations of LLMs and causal discovery have surprisingly overlooked the identifiability of causal structure, which plays an important role in classic causal discovery literature [3; 4; 5]. Hence, a challenging question comes:

_How can LLMs reliably assist in revealing the causal mechanisms behind the real world?_

In this work, we answer the question with a focus on local causal discovery with respect to a target variable that poses high value such as customer ratings and medical diagnosis, and introduce **C**ausal represent**i**O**n**Assistan**T** (COAT). Specifically, given the target variable \(Y\), COAT aims to identify a Markov blanket to \(Y\) from raw observation and also produce the theoretical-guaranteed causal structure. To achieve the goal, as illustrated in Fig. 1, COAT employs two mutually beneficial components: LLMs and CDs. Iteratively, at step (a), COAT leverages LLMs to look into a set of unstructured observations (e.g., customer comments) and propose potentially useful high-level factors. The proposed factors contain both the definition and the annotation criteria. Therefore, at step (b), another LLM is employed to give concrete values following the criteria. Then in step (c), CDs is used to reveal the structure among the identified factors. To ensure the reliability of the factor identification, COAT constructs feedback from the intermediate causal discovery results from step (c) to further inspire LLMs to improve further factors. The feedback includes sampling important observations that can not be well explained by the existing identified results. We show that the feedback provably helps with identifying the desired Markov Blanket and the structure (Proposition 2.2). We present a comprehensive analysis of COAT on both synthetic simulations and

Figure 1: Illustration of COAT framework to analyze the rating scores of AppleGastronome. COAT aims to uncover the underlying Markov Blanket with respect to the given ratings of the apples (i.e., factors that fit the preferences of gastronomes). COAT first (a) adopts an LLM to read, comprehend, and relate the rich knowledge about tasting the apples. The LLM needs to propose a series of candidate factors such as apple sizes and smells, along with some meta-information such as annotation guidelines. Based on the candidate factors, COAT then (b) prompts another LLM to annotate the unstructured review into structured data. (c) The CD algorithm then finds causal relations among the factors, and constructs feedback based on samples where the ratings can not be well explained by the existing factors. By looking into the new samples, the LLM is expected to associate more related knowledge to uncover more desired causal factors.

real-world case studies, ranging from analysis of human reviews to diagnosis of neuropathic and brain tumors (Sec. 4). Our contributions are summarized as follows:

* To the best of our knowledge, we are the first to leverage LLMs to propose high-level variables, thereby extending the scope of CDs to unstructured data (Sec. 2.1).
* We establish the first benchmarks with real-world data including AppleGastronome and Neuropathic to examine the unstructured causal discovery (Sec. 4).
* We propose the first framework COAT that combines the best of LLMs and CDs to find theoretically grounded causal results (Sec. 2.2), which are verified with extensive empirical studies (Sec. 4).
* Additionally, the analysis of COAT also derives the first metrics that measure the causal representation learning capabilities of various LLMs (Definition 2.3).

## 2 Representation Assistant for Causal Discovery

In this section, we present the formulation of leveraging a language model to serve as a representation assistant for causal discovery on unstructured data. The representation assistant needs to extract useful factors that capture sufficient information for an interested target variable.

### Problem Definition

DataTo begin with, we are given a _target variable_ of interest \(Y\), e.g., stars rated by a customer or the tumor type of a patient. We treat \(Y\) as a scalar random variable without loss of generality. The _unstructured data_, or raw observations, e.g., customer review of a certain product or images of tumors, are denoted as \(\). The dataset \(\) consists of \(n\) paired samples \(\{(_{i},y_{i})\}_{i=1}^{n}\) that are independently drawn from the distribution over \((,Y)\). Note that the target variable \(Y\) serves as a guider, and no prior assumption on the relations between \(\) and \(Y\) is assumed.

ObjectiveWe seek a mapping \(h:\) which elicits the _structured_ representation \(=h()\) such that \(Y\). In other words, \(\) serves as a Markov Blanket of \(Y\) for the unstructured raw observations. Built upon the structure representation, then, downstream causal discovery methods can be applied on \(\{Y\}\). The revealed causal structure can provide insights about the target variable \(Y\)[21; 22], such as what factors of the product would be most satisfactory to customers. Furthermore, the framework can be easily extended to discover a complete causal graph by shifting the target variable to the other identified factors or the other additionally available variables. Formal assumptions are discussed in Sec. 2.4.

LLM as a representation assistantWe aim to make the most use of the rich knowledge of LLMs to assist in extracting the relevant information from the raw observations \(\). To this end, the mapping \(h\) is decomposed as a collection of factors \(=\{_{1},_{2},...,_{k}\}\), each of which is a function \(_{i}:\) that maps the raw observation \(\) to a predefined value space \(\). In other words, the structured representation is composed of multiple factors: \(h()=(_{1}(),_{2}(), ,_{k}())\). Throughout this work, for the notation of factors, we use the symbol \(_{i}\) to denote the factor itself like _sweetness_, _size_, or _scent_, and \(_{i}()\) to denote the function that maps from raw observation space \(\) to the predefined value space \(\).

Descriptions of the factorsWithout loss of generality, in this work, we consider the descriptions of the factors in natural language, which can be divided into two categories: i) Implicit factors, which need to be discovered and elaborated by LLMs. To obtain the values of the implicit factors, one could feed the factor descriptions and the unstructured input \(_{i}\) to a suitable LLM for value annotation; For example, given a customer review on an apple \(_{i}\), a discovered implicit factor \(=\{-1,0,1\}\): \(_{1}(_{i})=1\) could mean that the customer appreciates the sweetness of the apple; \(_{1}()=-1\) means that the customer is disappointed about the sweetness; \(_{1}()=0\) means the sweetness has not been mentioned. ii) Explicit factors such as heart rate, whose descriptions are already available. The measure of the explicit factors usually requires some external tools.

### The COAT Framework

We approach the aforementioned problem via a new framework called **C**ausal representati**O**n **A**ssistan**T** (COAT) (Algorithm 1). COAT aims to extract useful factors through multiple rounds of iteration. We use the superscript \(t\) to refer to the input and output of LLM at the \(t\)-th round. We also denote the union of results from the first \(t\) rounds using the superscript "\( t\)".

Factor proposalTo induce useful high-level factors from the rich world knowledge of LLMs, COAT employs a prompt \(\) and a few samples \(}\).

Specifically, in Fig. 2, \(\) contains three components: _samples_, _instructions_, and _format control_. To encourage LLM to focus on the information related to the target variable \(Y\), samples are grouped by the value of \(Y\). The instruction requires \(\) to give each proposed factor \(_{i}\) a concrete description of the mapping \(_{i}()\), like how to decide the factor values. In addition, the metadata about the task such as the task description and context can also be incorporated if available. The prompt \(\) essentially imitates human experts  in selecting and defining high-level variables. The set of resulting factors in the \(t\)-th round, defined with natural language by the LLM \(\), is denoted as \(^{t}=(^{t},}^{t})\). We merge them with factors proposed in the previous rounds to update the set of all factors \(^{ t}=^{1}^{t}\).

Factor parsingOnce we obtain the candidate factors, we then collect the values of the factors from the unstructured observations. In prior works, they are usually collected from human experts according to the given factors . To do so, another LLM \(_{s}\) is instructed to read the annotated guidelines of each variable \(_{i}\) and parse the unstructured observations into structured or tabular data:

\[_{i}:=_{i}(_{1}),,_{i}(_{n}),_{i}():=_{s}(,_{i},_{p}),\] (1)

where \(_{p}\) refers to the additional instruction to prompt \(_{s}\) to parse the observed data, and \(_{i}\) refers to the parsed values for the corresponding factor \(}_{i}\). We define \(^{ t}:=\{_{i}_{i}^{ t}\}\).

When the data curation of the proposed factors requires additional domain-specific knowledge/skills (e.g., intervening on the external environments) that the LLMs do not acquire, we could fetch \(_{i}\) through some external process [24; 25]. For example, studying the causes of a disease requires annotating relevant symptoms from diagnosis records and conducting additional medical checks . Our experiments show that COAT can effectively extract the hidden factors under both schemes.

Figure 2: Illustration of the prompt template for factor proposal in COAT.

Causal discoveryWith the given values \(^{ t}\) associated with the candidate factors \(^{ t}\), a CD algorithm \(\) (e.g., FCI ) is used to reason about the causal structure based on the parsed data:

\[^{t}=(^{ t}\{Y\}),\] (2)

where \(^{t}\) is the discovered causal structure. In general, the inputs in each round to \(\) may contain noises as well as latent confounders, any CDs with suitable theoretical assumptions could be used for \(\). The noises injected through LLM-based parsing may be of independent interest to the literature of causal discovery [5; 28]. In this work, we demonstrate the idea of COAT via the FCI algorithm  as it is flexible with respect to different functional classes of the underlying generation process, allows for the existence of latent confounders , which aligns well with our objective.

Improving factor proposal with causal feedbackLLMs require proper prompts to fully unlock their capabilities [29; 30; 31]. When it comes to factor proposing, it is also hard for LLMs to propose all factors at once. Nevertheless, from the causal discovery results, we could find useful information and thus provide feedback to further improve the factor proposal:

\[(}^{t+1},^{t+1})=(^{t}, ,^{t}),\] (3)

where \(\) samples specific examples from \(\) and constructs new prompts according to the results of \(\) for the next round of factor proposal. For example, FCI is able to imply the existence of latent confounders, from which we could refine \(\) to prompt \(\) to focus on the corresponding factors.

### Causal Feedback

Let \(X\) be the high-dimensional random variable for the raw data. After \(t\) rounds, COAT identifies \(h_{ t}(X)\) as the Markov Blanket for \(Y\) w.r.t. to \(_{i}(X)_{i}^{ t}}\). If \(Y X h_{ t}(X)\), which means it cannot serve as a Markov Blanket  for \(Y\) w.r.t. \(X\), then there exists a potential factor \(}:X\) such that:

\[H(Y|h_{ t}(X))>H(Y|h_{ t}(X),}(X)),\] (4)

where \(H()\) refers to the entropy. If the LLM \(\) can not find the desired \(}\), it means that \(h_{ t}(X)\) is already sufficient to separate \(Y\) from \(X\). Therefore, for the next \((t+1)\)-th iteration with sample \(}^{t+1}\), \(\) is expected to propose new factor \(}\) that also satisfies the similar property:

\[H_{}^{t+1}}(Y|h_{ t}(X))-H_{}^{t +1}}(Y|h_{ t}(X),}(X))>0,\] (5)

where \(H_{}^{t}}(|)\) refers to the conditional entropy measured on \(}^{t}\). As shown in Fig. 3, finding factors satisfying Eq. 4 progressively expands the discovered factors and pushes \(h_{ t}(X)\) to a valid Markov Blanket. Also, it implies conditioning on the identified factors would further strengthen the correlation between \(\) and \(Y\). Therefore, to find the desired factor, we are motivated to select suitable \(}^{t+1}\) for the next iteration such that \(}^{t+1}=_{} }H_{}}(Y|h_{ t}(X)),\) where \(h_{ t}(X)\) can not well explain \(Y\). This problem can be converted into a classification problem in which \(}^{*}\) are the samples that the fitted classifier yields a large prediction error. In our experiments, we implement the classification via clustering with respect to \(h_{ t}(X)\). The clustering elicits \(C\) groups \(}_{c}:=_{i}i_{c} }\): \(_{1},,_{C}=h_{ t}(X)\). We then take the group of samples with the largest conditional entropy to construct the feedback.

In practice, many factors, such as the LLM capabilities, data faithfulness, and prompt templates, could affect the satisfaction of Eq 5. Therefore, in the next section, we will establish a theoretical framework to discuss the influence of the factors above to the satisfaction of Eq 5.

### Theoretical Analysis

We then theoretically analyze some critical steps in COAT, including the feedback and identifiability.

Figure 3: Illustration of variables that could be discovered with COAT. Let \(W h_{ t}(X)\) be an identified variable, and assume there exists a latent variable \(}\) to be discovered. When \(}\) is the direct parent or child of \(Y\), finding hard-to-explain samples can help uncover it. When \(}\) is the direct parent and also a child of \(W\), or the spouse of \(Y\) with \(W\) as the common child of \(Y\) and \(}\), conditioning on \(W\) facilitates the discovery of \(}\).

Feedback analysisGiven a new factor \(_{k+1}\), with the current representation as \(h_{[k]}(X)=(_{1}(X),_{2}(X),, _{k}(X))\), and COAT tests:

\[Y_{k+1}(X) h_{[k]}(X).\] (6)

COAT also requires the following usual condition about distribution and causal graph:

**Assumption 2.1** (Faithful and Markov conditions, adjusted from ).: _For any disjoint non-empty subsets \(A,B,C^{ t}\{Y\}\), \(A\) and \(B\) are d-separated by \(C\) on the causal graph iff \(A B C\) on the factors' distribution. All conditional independencies are preserved after factor parsing._

The annotation from a poor model could introduce an additional "error term" on the true factor values, disturbing the true distribution, as one can observe in Fig. 4(a) and 4(b). If Assumption 2.1 holds, the conditional mutual information between \(Y\) and \(X\) given the desired factors decreases:

**Proposition 2.2**.: _Under assumption 2.1, if condition 6 holds, then for Markov Blanket \([k+1]\) of \(Y\), i.e., \(Y h_{[k+1]}(X) h_{}(X)\), we have the following about conditional mutual information:_

\[I(Y;X h_{S}(X))=I(Y;X h_{[k+1]}(X))<I(Y;X  h_{[k]}(X))\] (7)

Factor identificationWe provide an initial exploration under what conditions LLMs can identify target-related factors and how the ability of an LLM influences this procedure.

**Definition 2.3** (Ability of LLMs).: _Given a suitable prompt about current factors and data, the LLM \(\) has non-zero probability \(p_{}>0\) to propose a new factor \(_{k+1}\) that satisfies condition 6 and_

\[(X))}{I(Y;X h_{[k]}(X))}<1 -C_{},\] (8)

_for some positive constant \(C_{}\) whenever \(I(Y;X h_{[k]}(X))>0\). Note that \(h_{}(X)=\), hence we also use \(I(Y;X h_{}(X))\) to refer \(I(Y;X)\). We use \(p\) instead of \(p_{}\) when the context is clear._

We further explain the intuition behind Def 2.3: the _Perception Score_\(p\) captures the LLM's responsiveness to the given prompts and the feedback; the _Capacity Score_\(C_{}\) captures the quality of the factors proposed by the LLM. Empirically, the two scores are used to estimate the abilities of the predominant LLMs (Sec. 3.2). Theoretically, we use them to characterize the influence of prompt templates, the LLM responsiveness, and the quality of factors on the performance of COAT:

**Proposition 2.4** (Characterization for Factor Identification Process).: _With assumption 2.1, for any small number \(,(0,)\), perception score \(p>0\), capacity score \(C_{}>0\), with \(t\) COAT rounds that_

\[>|}{2}(1+^{2}(1-p)(1-C_{})}}),\] (9)

_where \(z_{}\) is the \(\)-quantile of the standard normal distribution, we have_

\[((X))}{I(Y;X)}< ) 1-.\] (10)

The proof is given in Appendix D.2. Prop. 2.4 gives a guarantee on identifying a Markov Blanket. Intuitively, Prop. 2.4 also characterizes the influence of prompt templates, the LLM responsiveness, and the quality of factors on the performance of COAT via the two proposed measures: \(p\) and \(C_{}\). When both of them are positive, COAT would converge exponentially:

**Proposition 2.5** (Rate of Convergence).: _With assumption 2.1, for any small number \(,(0,)\), perception score \(p>0\), capacity score \(C_{}>0\), after \(t\) COAT rounds, the following inequality holds with probability at least \(1-\):_

\[(X))}{I(Y;X)}(})^{ -tp-z_{}}\] (11)

Causal structure identificationIt is clear that LLMs are not involved in the causal discovery process, which is mainly executed by causal discovery methods such as FCI. Therefore, the CD guarantees the identifiability of the final causal graph over the LLM-proposed factors. The concrete assumptions required for identifiability depend on the specific CD used in COAT. For instance, the FCI algorithm requires faithfulness of the data distribution with respect to the true causal graph . In our experiments, we also verify that the structured data annotated by LLMs has a high accuracy and little noise, which is friendly to the CD assumption. In general, one could switch to another CD in COAT, while using different CDs may require different assumptions. For example, the LiNGAM algorithm requires the relations among variables to be linear and non-Gaussian models. Empirically, we find that COAT with LiNGAM works very well (Appendix F).

### Practical Discussions

Prompt templateThe instruction following capability and the context window of LLMs may affect the satisfaction to the constraints of the prompt template. Including more data samples or background knowledge may improve the \(p\) and \(C_{}\), but it is more challenging for the LLM.

Modern causal discoveryWe use FCI in this paper in order to illustrate the idea. To attain identifiability better than the Markov equivalent class, one can choose more advanced Causal Discovery methods under different assumptions, see Appendix C if interested.

We also discuss some cases where we need to handle them properly in practice with LLMs.

Factor filteringLLMs may output several factors with similar semantics or exhibit multicollinearity in the annotated data, which will hinder the causal discovery process. To mitigate the issue, one could do factor filtering that adopts PCA or early conditional independence tests given the currently discovered variables in the Markov Blanket to detect and eliminate these variables.

Factor poolLLMs may discover useful factors in early rounds while being discarded. For example, the underlying spouse variables of the target label \(y\) may be independent with \(y\) without conditioning on their common children variables. To resolve the issue, we could introduce a factor pool that stores the candidate variables proposed in the past, and replay the variables that have not been passed by conditional independence tests with existing variables in the Markov Blanket for a double check.

## 3 Empirical Analysis of COAT

We evaluate whether COAT can propose and identify a set of high-level factors belonging to the Markov Blanket of the target variable \(Y\). We construct the first benchmark, called AppleGastronome, to verify the effectiveness of COAT in finding useful causal information, and compare COAT to previous methods  that merely leverage LLMs to perform causal discovery. Specifically, we use AppleGastronome to examine the capabilities of \(10\) predominant LLMs including GPT 4o , Claude-3-Opus , LLMA3-70b , and Mistral-Large  in realizing COAT. Due to the space limit, we report only the results of the popular LLMs and present full results in Appendix E.4.

### Experimental Setup

In AppleGastronome benchmark, we consider the target variable as a rating score of the apple by astronomers. We prepare different high-level factors: 3 parents of Y, one child of Y, and one spouse of Y. These factors form a Markov blanket of Y. In addition, we also prepared one disturbing factor

Figure 4: Quantitative evaluation of the causal capabilities of LLMs in COAT.

related to Y but not a part of this blanket. A good method is expected to propose the five high-level factors (up to semantical meanings) and exclude the disturbing factor.

Benchmark constructionEach apple has its own attributes, including size, smell, and taste (or sweetness). Each gastronomy pays unique attention to a subset of the above three attributes. They will write a review according to their preference and give the rating score. We generate the review using GPT 4 by feeding GPT 4 the preferences and the apple attributes. We generated 200 samples for LLMs' analysis and annotation. More details are left in Appendix E.1.

BaselinesFor factor proposal, we mainly employ two different uses of LLMs as the baselines: **META** is the zero-shot factor proposal given only the context to LLMs; and **DATA** additionally gives some samples of raw observations, which is an ablation of COAT without the _feedback_ module, i.e., only one COAT round. For causal relation inference, we follow Kiciman et al.  that prompt LLMs to reason for the causal direction of each pair of the discovered variables by **DATA**.

MetricsWe evaluate the ability on factor proposal based on three metrics: _MB_, _NMB_, and _OT_. _MB_ means the desired factor forming the Markov Blanket of Y. _NMB_ means the undesired factor relevant to data but not in _MB_. _OT_ means the unexpected factors irrelevant to data. We also present the corresponding recall, precision, and F1 with respect to \((Y)\).

### Analysis with AppleGastronome Benchmark

Key findingsEmpirically, LLMs with CoT can be aware of high-level factors behind data (lower _OT_ than _META_) but still struggle to distinguish the desired factors in Markov Blanket (higher _NMB_ than COAT). COAT is more resistant to the "disturbing" factor, which is supported by the lower _NMB_ column. COAT filters out irrelevant factors from LLMs' prior knowledge that are not reflected by the data, which is supported by the lower _OT_ column. COAT robustly encourages LLM to find more expected factors through the feedback, which is supported by the higher _MB_ column.

Can LLMs be an effective factor proposer?As discussed in Sec. 2.4, there are two crucial abilities for LLMs in identifying potential high-level factors. The first one is to be aware of the existence of potential factors, and the second is to synthesize and describe these factors. Inspired

    &  &  \\  & & MB & NMB & OT & Recall & Precision & F1 \\   & META & 2.67\(\)0.04 & 0.67\(\)0.27 & 2.33\(\)0.0 & 0.53\(\)0.0 & 0.46\(\)0.04 & 0.49\(\)0.27 \\  & DATA & 3.00\(\)0.33 & 0.32\(\)0.07 & 0.00\(\)0.0 & 0.50\(\)0.00 & 0.92\(\)0.22 & 0.72\(\)0.04 \\  & DATA+COT & 4.33\(\)0.08 & 0.83\(\)0.27 & 0.17\(\)0.29 & 0.87\(\)0.21 & 0.81\(\)0.04 & 0.84\(\)0.08 \\  & COAT & 4.00\(\)0.02 & 0.33\(\)0.07 & 0.00\(\)0.00 & 0.80\(\)0.16 & 0.93\(\)0.00 & 0.85\(\)0.11 \\   & META & 3.33\(\)0.23 & 0.33\(\)0.37 & 0.43\(\)1.25 & 0.67\(\)0.25 & 0.42\(\)0.12 & 0.51\(\)0.17 \\  & DATA & 2.67\(\)0.47 & 0.67\(\)0.07 & 0.00\(\)0.00 & 0.53\(\)0.00 & 0.81\(\)0.14 & 0.64\(\)0.09 \\  & DATA+COT & 5.00\(\)0.00 & 0.10\(\)0.00 & 1.33\(\)0.08 & 0.10\(\)0.00 & 0.84\(\)0.08 & 0.81\(\)0.04 \\  & COAT & 3.67\(\)0.47 & 0.00\(\)0.00 & 0.00\(\)0.00 & 0.73\(\)0.00 & 1.00\(\)0.00 & 0.84\(\)0.07 \\   & META & 2.33\(\)0.47 & 0.67\(\)0.47 & 4.67\(\)0.47 & 0.72\(\)0.32 & 0.32\(\)0.07 & 0.37\(\)0.09 \\  & DATA & 2.33\(\)0.49 & 0.67\(\)0.47 & 0.00\(\)0.00 & 0.47\(\)0.19 & 0.75\(\)0.52 & 0.57\(\)0.29 \\   & DATA+COT & 3.00\(\)0.17 & 0.67\(\)0.55 & 0.33\(\)0.59 & 0.60\(\)0.37 & 0.71\(\)0.44 & 0.65\(\)0.37 \\   & COAT & 3.00\(\)0.00 & 0.67\(\)0.47 & 0.00\(\)0.00 & 0.80\(\)0.20 & 0.53\(\)0.27 & 0.69\(\)0.04 \\   & META & 3.00\(\)0.00 & 0.67\(\)0.47 & 1.67\(\)1.25 & 0.60\(\)0.50 & 0.59\(\)0.13 & 0.59\(\)0.07 \\   & DATA & 3.00\(\)0.00 & 0.67\(\)0.47 & 0.00\(\)0.00 & 0.60\(\)0.00 & 0.53\(\)0.27 & 0.69\(\)0.04 \\   & DATA+COT & 4.33\(\)0.08 & 1.00\(\)0.00 & 0.57\(\)0.53 & 0.87\(\)0.27 & 0.75\(\)0.07 & 0.79\(\)0.04 \\   & COAT & 4.67\(\)0.47 & 0.00\(\)0.00 & 0.00\(\)0.00 & 0.53\(\)0.00 & 1.00\(\)0.00 & 0.96\(\)0.05 \\   

Table 1: Factor proposal results in AppleGastronome benchmark (Full results in Appendix E.4).

Figure 5: The discovered causal graphs in AppleGastronome. Compared to the ground truth results, directly adopting LLMs to reason the causal relations can easily elicit many false positive edges. In contrast, the relations recovered by COAT have a high precision and recall. The directed edge between “taste” and “juiciness” can not be recovered by COAT because of the limitations of FCI.

by this observation, we propose two novel metrics to quantify LLMs' causal ability: a) Perception that quantifies the ratio of _valid_ factors (satisfying Prop. 2.2) proposed by LLMs in each round; b) Capacity that measures the effective mutual information drop in Assumption 2.3. As shown in Fig. 4(c), LLMs differ largely on the perception score while comparably on the capacity score.

**Can LLMs be an effective factor annotator?** As shown in Fig. 4, both GPT 3.5 and GPT 4 annotate subjective attributes well. Regarding objective human preferences, the performances are still relatively high. Empirically, LLMs will not introduce new confounders, see Appendix E.3.

**Can COAT reliably recover the causal relationships?** We present quantitative and qualitative results in Table 3 (in Appendix E.1) and Fig. 5 (on page 8), respectively. Compared to directly adopting LLMs to reason the causal relations, COAT significantly boosts the causal relation recovery. Meanwhile, COAT maintains high performances based on various LLMs, which further demonstrates the effectiveness of the causal feedback in COAT to improve the robustness of this system. In fact, the causal feedback focuses on making maximal use of the rich knowledge of LLMs, and reducing the reliance on the reasoning capabilities of different LLMs, to assist with causal discovery. We provide the full results of \(10\) LLMs in Appendix E.3.

## 4 Empirical Study with Realistic Benchmarks

After examining the capabilities of COAT in AppleGastronome, we are further motivated to challenge COAT in a more complex setting from neuropathic panic diagnosis , brain tumor detection with MRI images , three-years news summary about one stock from the New York Times , and climatic reanalysis data with fine-grained time and space coverage . We refer to Appendix H for a complete summary of all five benchmarks.

### Experimental Setup

**Benchmark construction** In the Neuropathic benchmark, we convert the dataset into a clinical diagnosis task. In the original dataset, there are three levels of causal variables, including the symptom level, radioactivity level, and the pathophysiology level. In the experiments, we mainly consider the target variable of right shoulder impingement. When generating the clinical diagnosis notes as \(\) using GPT 4, we avoid any mentioning of variables other than symptoms. We generated 100 samples for LLMs' analysis; since the number of possible factors is finite, we generate 1000 tabular data for CI tests.

As we intend to leverage the Neuropathic benchmark to simulate the real-world diagnosis, after the factor proposal stage, we directly incorporate external tools to measure the values of the candidate factors. More details about the construction of the Neuropathic are given in Appendix E.6.

    &  &  \\  & & PA & AN & OT & Acc & F1 \\   & Meta & 3 & 5 & 6 & 0.91 & 0.59 \\  & Data & 2 & 2 & 0.95 & 0.50 \\  & DATA+COT & 3 & 4 & 13 & 0.81 & 0.35 \\  & COAT & 3 & 6 & 3 & 0.96 & 0.80 \\   & Meta & 3 & 5 & 6 & 0.91 & 0.59 \\  & DATA & 3 & 5 & 4 & 0.94 & 0.67 \\  & DATA+COT & 2 & 2 & 3 & 0.91 & 0.36 \\  & COAT & 3 & 5 & 2 & 0.96 & 0.77 \\   & Meta & 2 & 4 & 5 & 0.91 & 0.53 \\  & DATA & 3 & 3 & 1 & 0.95 & 0.60 \\  & DATA+COT & 2 & 4 & 7 & 0.88 & 0.47 \\  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\   & Meta & 1 & 3 & 6 & 0.88 & 0.40 \\  & DATA & 3 & 6 & 4 & 0.95 & 0.75 \\  & DATA+COT & 0 & 1 & 10 & 0.81 & 0.12 \\  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\   & Meta & 1 & 1 & 17 & 0.72 & 0.08 \\  & DATA & 3 & 6 & 3 & 0.96 & 0.80 \\  & DATA+COT & 0 & 0 & 10 & 0.79 & – \\  & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\   & Meta & 3 & 6 & 3 & 0.96 & 0.80 \\  & DATA & 3 & 3 & 2 & 0.94 & 0.66 \\   & DATA+COT & 3 & 5 & 8 & 0.88 & 0.53 \\   & COAT & 3 & 6 & 2 & 0.97 & 0.86 \\  

Table 2: Factor proposal results in Neuropathic. PA, AN, and OT refer to the parents, ancestors, and others, respectively. Accuracy and F1 measure the recovery of the causal ancestors.

Figure 6: The discovered causal graphs in Neuropathic. (c) shows the result based on directly prompting LLM to reason for the causal relations among all factors. Disconnected ones are dropped.

**Evaluation and baselines** In Neuropathic, we adopt a similar evaluation protocol and the baselines as in AppleGastronome. Nevertheless, due to the faithfulness issue of the original dataset , for the evaluation of causal relation discovery, we mainly conduct a qualitative comparison between the ground truth that is faithful to the data, against the baselines and COAT.

### Empirical Results on Neuropathic Benchmark

Factor proposalThe quantitative results on Neuropathic benchmark are given in Table 2. Similarly, we can find that COAT consistently outperforms all of the baselines regardless of which LLMs are incorporated. In particular, even with the weakest backbone model, i.e.,LLaMA2-7b, COAT can still effectively leverage the intrinsic rich knowledge and beat the baselines with more powerful LLMs.

**Causal relation recovery** Fig. 6(a) shows the causal graph obtained by FCI running on the original data, where we can find that several causal relations cannot hold on the data. As shown in Fig. 6, when using LLMs to perform the reasoning, LLMs cannot identify the faithfulness issues. In contrast, COAT can imply faithful causal insights.

### More Real-world Results

**El Nino-Southern Oscillation (ENSO) case study** El Nino-Southern Oscillation (ENSO) is a climatic phenomenon in the Pacific Ocean that influences global weather patterns profoundly. To understand its mechanism, we apply COAT on NOAA dataset . There are 13 factors identified by COAT, and their instantaneous causal relations are visualized in Fig 7. The target variable is the future change in monthly SST in the Nino3 region, which could be an important indicator of ENSO events. Each factor is a time series about a certain climate measurement above a specific level averaged over a specific region. The paths about _Sea level Pressure_, _Momentum Flux_, and _Cloud Cover_ matches the existing understanding from literature . It also suggests several inserting hypotheses that are less explored in literature, like the path from Soil Temperature in South American Coastal Region. We refer details in Appendix K.

**More real-world empirical studies** We also report concrete results on real-world problems involving MRI, time series, and NetCDF data in Appendix I, J, and K.

## 5 Conclusions

In this paper, we proposed a new paradigm COAT to incorporate the rich knowledge of LLMs into the CD pipeline. We found that COAT effectively extends the scope of CDs to unstructured data by identifying useful high-level variables from raw observations for CD methods. COAT suggests a new pathway towards building a causal foundation model for discovery. We leave more detailed discussions about future studies in Appendix B.

Figure 7: The final causal graph found by COAT in the ENSO case study