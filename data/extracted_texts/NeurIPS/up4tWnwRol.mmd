# Proof of Part 3.

[MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:2]

**Definition 1.1** (\(\)-th layer forward computation).: _Given weights \(Q,K,V^{d d}\), and letting \(E_{}^{n d}\) denote the \(\)-th layer input, then \(E_{+1}^{n d}\) is defined recursively as_

\[E_{+1} D^{-1}(E_{}QK^{}E_{}^{}/d)E_{}V\]

_where_

* \(D:=((E_{}QK^{}E_{}^{}/d)_{n})\)_._
* \(\) _denotes the exponential function which is applied entry-wise, i.e.,_ \((A)_{i,j}=(A_{i,j})\) _for all matrices_ \(A\)_._
* \(()\) _operation takes a vector as input and generates a diagonal matrix with the entries of that vector._
* \(_{n}\) _denotes the length-_\(n\) _all ones vector._

In mathematical terms, optimization in the context of attention computation is described as (by renaming the \(QK^{}^{d d}\) to be \(X^{d d}\) and \(V^{d d}\) to be \(Y^{d d}\)):

**Definition 1.2** (Attention optimization).: _Given four \(n d\) size matrices \(A_{1},A_{2},A_{3}\) and \(E^{n d}\). Suppose that a \(d d\) size square matrix \(Y\) is also given. The attention optimization problem is formulated as:_

\[_{X^{d d}}L(X):=0.5\|D(X)^{-1}(A_{1}XA_{2}^{}/d )A_{3}Y-E\|_{F}^{2}.\]

_Here \(D(X)^{n n}\) is_

\[D(X):=((A_{1}XA_{2}^{}/d)_{n}).\]

_and \(\|\|_{F}^{2}\) denotes the squared Frobenius norm, i.e., \(\|A\|_{F}^{2}:=_{i,j}A_{i,j}^{2}\)._

**Remark 1.3**.: _In principle, the loss function above, and resulting gradients below, should depend on both \(X\) and \(Y\). However, since the final matrix computed in the norm in \(L\) depends only linearly on \(Y\), it is straightforward to incorporate it into either an algorithm or lower bound. Thus, in this work, we focus on the case where \(X\) is variable and \(Y\) is a fixed input to simplify some arguments._

We thus define the Approximate Attention Loss function Gradient Computation problem as follows:

**Definition 1.4** (Approximate Attention Loss Gradient Computation (\((n,d,)\))).: _Given four \(n d\) size matrices \(A_{1}^{n d},A_{2}^{n d},A_{3} ^{n d}\), \(E^{n d}\) and a square matrix \(Y^{d d}\), which we think of as fixed matrices. Assume that \(\|A_{1}X\|_{} B\), \(\|A_{2}\|_{} B\) for a positive parameter \(B\). Further assume that all the entries of these matrices can be represented as \(O( n)\)-bit rational numbers. Let \(L(X)\) be defined as Definition 1.2. Let \(L(X)}{X}\) denote the gradient of loss function \(L(x)\)._

_The goal is to output a vector \(\) such that_

\[\|-L(X)}{X}\|_{}.\]

_Here for matrix \(A\), \(\|A\|_{}:=_{i,j}|A_{i,j}|\)._

### Main Results

Our main results show that there is a threshold in the computational complexity of \((n,d=O( n))\) depending on the bound \(B\). When \(B=o()\) we give a new near-linear-time algorithm, and when \(B=()\), we show that such an algorithm is impossible assuming SETH. This matches the results of , where a nearly identical threshold at \(B\) around \(\) was also observed. Our results therefore imply that the entire LLM training process has this computational threshold.

**Theorem 1.5** (Main result, Lower bound, informal version of Theorem E.5).: _Assuming_ SETH_, there is no algorithm running in time \(O(n^{2-q})\) for any \(q>0\) for the \((n,d=O( n),B=())\) (see Definition 1.4)._

**Theorem 1.6** (Main result, Upper bound, informal version of Theorem D.6).: _Assuming entries are bounded, there is a \(n^{1+o(1)}\) time algorithm to solve \((n,d=O( n),B=o())\) (see Definition 1.4) up to \(1/(n)\) accuracy._

Our new algorithm (Theorem 1.6) builds on a low-rank approximation for the attention matrix from prior work . Incorporating these approximation into the gradient computation is not straightforward; in the forward problem, one simply multiplies the attention matrix by an input value matrix, but in the backward problem, it is combined with other matrices in an intricate (non-linear) way. We ultimately use tools from tensor algebra to get a handle on the entry-wise products and high-rank sparse matrices which arise in the gradient computation but do not typically preserve the needed low-rank structure.

Our new lower bound (Theorem 1.5) comes from a careful reduction from a special case the forward problem (where hardness is known from prior work) to the backward problem. Reducing from computing a function to computing its gradient in general is quite challenging or impossible without control over how quickly the gradient may be growing or changing, and in general, the gradient of the forward (attention) computation can behave quite erratically (which is likely necessary for the expressive power of attention units). Nonetheless, in the special case of the inputs for which attention computation is known to be hard from prior work, we are able to reasonably control the growth of these gradients and successfully perform our reduction.

**Roadmap.** We discuss other related works in Section 2. In Section 3, we provide the basic notation, definitions, backgrounds, and facts which we will use. In Section 4, we provide the proof sketch of our algorithm and defer the details to the Appendix. In Section 5, we briefly conclude our paper. In Section 6, we discuss the limitations of our paper. In Section 7, we provide the broader impact statement.

## 2 Related Work

Fine-grained Complexity.Numerous algorithmic techniques have been used in theory and in practice for attention computations. The first algorithm with provable guarantees, by Zandieh, Han, Daliri, and Karbasi , used locality sensitive hashing (LSH) techniques , while later work by Alman and Song  used polynomial approxmation methods . We particularly focus here on the latter technique, which is the only algorithm we're aware of which achieves near-linear running time.

Keles, Wijewardena, and Hedge  established the first lower bound on attention computation under the assumption of \(\). Their findings demonstrated that when \(d=( n)\), it is not possible to execute forward computations in subquadratic time. The later lower bound of  further incorporated the magnitudes of the input entries into the lower bound to tightly match the aforementioned algorithms. Both use the high-level technique of  from kernel density estimation, and build on methods derived from fine-grained complexity associated with approximate nearest neighbor search  and the polynomial method .

Fast Attention Computation.Optimizing the computation of attention mechanisms in pre-trained LLMs, given their extensive parameter sets, has been a focal point of recent research. Various studies have explored the application of locality sensitive hashing (LSH) techniques to approximate attention mechanisms.  introduced two methods to enhance computational efficiency, including the use of LSH to replace dot product attention and a reversible residual layer to substitute the standard residual layer.  refined this approximation, noting that LSH's efficiency does not require constant parameter updates.  proposed an innovative estimator based on Kernel Density Estimation (KDE) to speed up the softmax function and matrix multiplication computations. Some recent works , KMZ23] have specifically used sketching techniques to avoid large entries in the attention matrix.  developed techniques utilizing a transformer within a transformer (TinT) model to simulate the transformer's forward and backward passes, significantly increasing parameter efficiency.  tackled the challenge of fine-tuning LLMs with high memory demands by improving the classical ZO-SCD optimizer, creating a memory-efficient gradient estimator that requires only a forward pass.  provided insights into dynamic attention problems, they provide algorithm and hardness for the dynamic setting of attention problem.  introduces a quantum algorithm for attention computation, opening new avenues forefficiency improvements.  provides a result for computing the attention matrix differentially privately.  introduces a randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension.  provides a zero-th order method to accelerate the computation of attention.  use weights sparsity to accelerate the attention computation, but cannot reduce the time complexity.  compress the input token length to accelerate attention inference.  uses Half-Space Reporting (HSR) techniques to accelerate attention computation.  studies proxy for softmax attention such as matrix exponential and provides fast algorithms for these proxies.

Transformer Training.Transformer architectures (the backbone of LLMs) have been trained with alternating steps of forward and backward computations since their introduction . In Appendix B below, we perform computations to verify that our stated problems are the same as the forward and backward steps from the literature. Note that there are many weights update methods, such as LoRA , prefix turning , and many so on. In this paper, we consider the standard training algorithm with gradient back-propagation. On the other hand  introduce the modern Hopfield models as a proxy for possible fast attention computation in training (and inference), which have been used in various applications . Similar analyses of computational feasibility have also been conducted for transformer-based diffusion models, such as Diffusion Transformers (DiTs) .

## 3 Preliminary

In Section 3.1, we define some basic notation we will use. In Section A.3, we state important facts related to fast matrix multiplication. In Section 3.2, provide the formal definition of the Strong Exponential Time Hypothesis. In Section 3.3, we define several intermediate functions related to softmax and exponential which will arise in our algorithms. In Section 3.4, we define the loss function. In Section 3.5, we provide standard tensor tricks which we will use. In Section 3.6, we show how to reformulate the loss function for our purposes.

### Notation

For any positive integer \(n\), we define \([n]:=\{1,2,,n\}\). For two same length vector \(x\) and \(y\), we use \( x,y\) to denote the inner product between \(x\) and \(y\), i.e., \( x,y=_{i=1}^{n}x_{i}y_{i}\). We use \(x y\) to denote vector that \(i\)-th entry is \(x_{i}y_{i}\). Let \(_{n}\) denote the length-\(n\) all ones vector. It is not hard to see that \( x y,_{n}= x,y\). For a vector \(x\), we use \(x^{}\) to denote the transpose of \(x\). For a matrix \(M\), we use \(M^{}\) to denote the transpose of matrix \(M\). For a vector \(x\), we use \((z)\) to denote the vector that \(i\)-th coordinate is \((z_{i})\). For a matrix \(M\), we use \((M)\) to denote the matrix that \((i,j)\)-th coordinate is \((M_{i,j})\). For a function \(f\), we use \((f)\) to denote \(f( f)\). Let \(n_{0},n_{1},m_{0},m_{1}\) be positive integers. Let \(X^{n_{0} m_{0}}\) and \(Y^{n_{1} m_{1}}\). We define the Kronecker product between matrices \(X\) and \(Y\), denoted \(X Y^{n_{0}n_{1} m_{0}m_{1}}\), as \((X Y)_{(j_{0}-1)n_{1}+j_{1},(i_{0}-1)m_{2}+i_{1}}\) is equal to \(X_{j_{0},i_{0}}Y_{j_{1},i_{1}}\), where \(j_{0}[n_{0}],i_{0}[m_{0}],j_{1}[n_{1}],i_{1}[m_{1}]\).

### Backgrounds on Complexity

Over 20 years ago, Impagliazzo and Paturi  introduced the Strong Exponential Time Hypothesis (SETH), an enhancement of the \(\) conjecture. It posits that the existing algorithms for solving SAT problems are essentially as efficient as possible:

**Hypothesis 3.1** (Strong Exponential Time Hypothesis (SETH)).: _For any \(>0\), there exists a positive integer \(k 3\) for which solving \(k\)-SAT problems with \(n\) variables in \(O(2^{(1-)n})\) time is impossible, including with the use of randomized algorithms._

SETH, a widely recognized conjecture, has been instrumental in establishing fine-grained lower bounds across a broad spectrum of algorithmic challenges, as highlighted in the survey .

### Definitions related with Softmax

Now, we start by some definitions about \(X^{d d}\) which will be helpful. Let \(x\) denote the vectorization of \(X\).

**Definition 3.2**.: _Let \(A_{1},A_{2}^{n d}\) be two matrices. Suppose that \(=A_{1} A_{2}^{n^{2} d^{2}}\). We define \(_{j_{0}}^{n d^{2}}\) be a \(n d^{2}\) size sub-block from \(\). Note that there \(n\) such sub-blocks._

_For every \(j_{0}[n]\), let us define function \(u(x)_{j_{0}}:^{d^{2}}^{n}\) to be:_

\[u(x)_{j_{0}}:=_{j_{0}}\,x)}_{n 1}.\]

**Definition 3.3**.: _Suppose that there are two \(n d\) size matrices \(A_{1},A_{2}^{n d}\). We define \(_{j_{0}}^{n d^{2}}\) be a \(n d^{2}\) size sub-block from \(\). (Recall that \(=A_{1} A_{2}^{n^{2} d^{2}}\).)_

_For every index \(j_{0}[n]\), we consider a function, \((x)_{j_{0}}:^{d^{2}}\) as:_

\[(x)_{j_{0}}:=_{j_{0}}\,x)}_{n 1 },_{n}}_{n 1}.\]

**Definition 3.4**.: _Suppose that \((x)_{j_{0}}\) is defined as in Definition 3.3._

_Recall \(u(x)_{j_{0}}^{n}\) is defined as in Definition 3.2._

_For a fixed \(j_{0}[n]\), let us consider function \(f(x)_{j_{0}}:^{d^{2}}^{n}\)_

\[f(x)_{j_{0}}:=}^{-1}\,u(x)_{j_{0}}}_{ }.\]

_Let \(f(x)^{n n}\) denote the matrix where \(j_{0}\)-th row is \((f(x)_{j_{0}})^{}\)._

**Definition 3.5**.: _For every \(i_{0}[d]\), we define \(h()_{i_{0}}:^{d^{2}}^{n}\) as:_

\[h(y)_{i_{0}}:=}_{n d}}}_{d 1 }.\]

_Here let \(Y^{d d}\) denote the matrix representation of \(y^{d^{2}}\). Let \(h(y)^{n d}\) matrix where \(i_{0}\) column is \(h(y)_{i_{0}}\)._

### Loss Functions

In this section, we introduce some helpful definitions related to both \(x^{d^{2}}\).

**Definition 3.6**.: _For every \(j_{0}[n]\), we use \(f(x)_{j_{0}}^{n}\) to denote the normalized vector defined by Definition 3.4. For every \(i_{0}[d]\), we let \(h(y)_{i_{0}}\) to be defined in Definition 3.5._

_Consider every \(j_{0}[n]\), every \(i_{0}[d]\). Let us consider \(c(x)_{j_{0},i_{0}}:^{d^{2}}^{d^{2}}\) as follows:_

\[c(x)_{j_{0},i_{0}}:= f(x)_{j_{0}},h(y)_{i_{0}}-E_{j_{0},i_{0}}.\]

_Here \(E_{j_{0},i_{0}}\) is the \((j_{0},i_{0})\)-th coordinate/location of \(E^{n d}\) for \(j_{0}[n],i_{0}[d]\)._

_This is equivalent to_

\[_{n d}=_{n n} _{n d}-_{n d}.\]

**Definition 3.7**.: _For every \(j_{0}[n]\), for every \(i_{0}[d]\). Let us define \(L(x)_{j_{0},i_{0}}\) to be \(:=0.5c(x)_{j_{0},i_{0}}^{2}\)._

### Tensor Trick

We state the well-known tensor-trick. It has been widely used in literature of linear algebra related to tensor computations . For every \(j_{0}[n]\), for every \(i_{0}[d]\), we define \(L(x)_{j_{0},i_{0}}\) to be \(:=0.5c(x)_{j_{0},i_{0}}^{2}\).

**Definition 3.8**.: _For every \(j_{0}[n]\), we define \(L(x)_{j_{0},i_{0}}\) to be \(:=0.5c(x)_{j_{0},i_{0}}^{2}\)._

**Fact 3.8** (Tensor trick).: _For two matrices \(A_{1}\) and \(A_{2}^{n d}\), define \(=A_{1} A_{2}\). Let \(X^{d d}\). Let \(x^{d^{2}}\) denote the vector representation of \(X\). Then we have \((A_{1}XA_{2}^{})=\,x\)._

Using the above tensor-trick, it is easy to observe that

**Fact 3.9**.: _For two matrices \(A_{1}\) and \(A_{2}^{n d}\), denote \(=A_{1} A_{2}\). Let \(X^{d d}\). Let \(_{j_{0}}^{n d^{2}}\) a submatrix of \(\) (by properly selecting \(n\) rows of \(\)). Let \(x^{d^{2}}\) denote the vector representation of \(X\). Then, we have_

* \(((A_{1}XA_{2}^{}))=(\,x)\)__
* \(((A_{1}XA_{2}^{})_{j_{0},*})^{}=(_{j_{0}}\,x)\)_,_

_Here \((A_{1}XA_{2}^{})_{j_{0},*}\) is the \(j_{0}\)-th row of \(n n\) matrix \((A_{1}XA_{2}^{})\)._

Proof.: We can use the definition in fact and Fact 3.8, to prove it. 

### Reshape the Loss function via Tensor Trick

**Lemma 3.10**.: _Given the below requirements_

* _Here are three matrices_ \(A_{1}^{n d}\)_,_ \(A_{2}^{n d}\)_, and_ \(A_{3}^{n d}\)_._
* _Let_ \(=A_{1} A_{2}^{n^{2} d^{2}}\) _to be the Kronecker product of the two matrices_ \(A_{1}\) _and_ \(A_{2}\)_._
* _For every_ \(j_{0}[n]\)_, define_ \(_{j_{0}}^{n d^{2}}\) _to be a_ \(n d^{2}\) _sized block in the matrix_ \(^{n^{2} d^{2}}\)_._
* \(E^{n d}\) _be a matrix. Define_ \(E_{j_{0},i_{0}}\) _as the_ \((j_{0},i_{0})\)_-th coordinate/location of_ \(E^{n d}\) _for every pair of_ \(j_{0}[n]\) _and_ \(i_{0}[d]\) _._
* _Here are two square matrices_ \(X^{d d}\)_, let_ \(Y^{d d}\)_._
* _Let_ \(L(X)\) _be defined as Definition_ 1.2_._
* _For every pair of_ \(j_{0}[n]\)_,_ \(i_{0}[d]\)_, recall that definition of_ \(L(x)_{j_{0},i_{0}}\) _can be found in in Definition_ 3.7_._

_Then, we have_

\[L(X)=_{j_{0}[n]}_{i_{0}[d]}L(x)_{j_{0},i_{0}}.\]

Proof.: We can show that

\[L(X) =0.5\|}_{n n}XA_{2}^{})}_{n n}}_{n d}_{d  d}-_{n d}\|_{F}^{2}\] \[=\,_{j_{0}=1}^{n}_{i_{0}=1}^{d}0.5( (_{j_{0}}\,x),_{n}^{-1}(_{ j_{0}}\,x),A_{3}Y_{*,i_{0}}-E_{j_{0},i_{0}})^{2}\] \[=\,_{j_{0}=1}^{n}_{i_{0}=1}^{d}0.5( f(x)_{j_{0}},h (y)_{i_{0}}-E_{j_{0},i_{0}})^{2}\] \[=\,_{j_{0}=1}^{n}_{i_{0}=1}^{d}L(x)_{j_{0},i_{0}}\]

where the first step follows from definition, the second step follows from writing down the summation, the third step follows from definition of \(f(x)_{j_{0}}\) (recall the Definition 3.4) and \(h(y)_{i_{0}}\) (recall the Definition 3.5), and the last step follows from \(L(x)_{j_{0},i_{0}}\) (see Definition 3.7).

## 4 Proof Sketch for General Upper Bound

The most straightforward way to compute the gradient would take \(O(n^{2}d^{2})\) time in order to explicitly write down the matrix \(\). We first show how to obtain an intermediate algorithm, which runs in slightly improved time \(O(n^{2}d+nd^{2})\) to compute the gradient. Our final algorithm will build on this idea.

**Lemma 4.1** (Warmup, attention gradient computation, informal version of Lemma C.8).: _If the following conditions hold_

* _Define four_ \(n d\) _size matrices_ \(E,A_{1},A_{2},A_{3}\) _and two_ \(d d\) _square matrices_ \(X,Y\) _to be input fixed matrices._
* _Let_ \(X^{d d}\) _and_ \(Y^{d d}\) _denote matrix variables (we will compute gradient with respect to_ \(X\) _)._
* _Let_ \(g=L(X)}{X}\)_._

_Then the gradient \(g\) can be calculated in \(O(n^{2}d+nd^{2})\) time._

The key idea behind Lemma 4.1 is to use algebraic manipulations to quickly compute the quantities defined in the previous section. We first compute \(f(x)\) in \(O(nd^{2})\) time, then show \(c(x)\) and \(q(x)\) can be computed in \(O(n^{2}d)\) time. Using these, we compute \(p(x)\) in \((n^{2})\) time, then putting in all together, we compute \(g\) in \(O(n^{2}d+nd^{2})\) time. (We refer the details to Section C.)

For notational simplicity, we also write \(x^{d^{2} 1}\) to denote the vectorized version of \(X\), and similarly \(y^{d^{2} 1}\) for \(Y\).

Next, we will show how to improve the running time of computing the gradient from quadratic time (\( n^{2}\)) to almost linear time \(n^{1+o(1)}\). We build on the approach of Lemma 4.1 for computing the intermediate quantities \(f,c,q\), and \(p\), but speed up the time it takes to _implicitly_, rather than explicitly, represent these quantities. We want to emphasize that, although our algorithm relies on careful manipulation of the input matrices, our main algorithmic result does not make use of fast matrix multiplication (which may otherwise be quite impractical).

We now sketch the main algorithmic ideas. First, by linearity of derivative, we can show that

\[L(x)}{x}=_{j_{0}=1}^{n}_{i_{0}=1}^{d} {L(x)_{j_{0},i_{0}}}{x}\]

Based on calculations we perform in Section B, Section C, and several linear algebra facts, we can show that

\[L(x)_{j_{0},i_{0}}}{x}\] \[=,i_{0}}}_{} _{j_{0}}^{}}_{d^{2} n}(f(x)_{j_{0}})-f(x)_{j_{0}}f(x)_{j_{0}}^{})}_{n n}}}_ {n 1}\]

For any fixed \(j_{0}[n]\), consider this quantity. Since this expression involves an \(n n\) matrix, the most straightforward way to calculate it would take \((n^{2})\) time, and so summing over all \(j_{0}[n]\)would lead to a cubic-time algorithm. It is not too difficult to improve this: the \(n n\) matrix (see Figure 1 for an illustration)

\[((f(x)_{j_{0}})}_{}- }f(x)_{j_{0}}^{}}_{})\]

is easily decomposed into a low-rank part (\(f(x)_{j_{0}}f(x)_{j_{0}}^{}\) which has size \(n n\)) and a sparse part (\(*{diag}(f(x)_{j_{0}})\) which also has size \(n n\)), which reduces the calculation of each part to only \((n)\) time, and the total running time to \((n^{2})\) time.

However, we are aiming for a almost-linear time algorithm, and it is not possible to achieve this by treating the different \(j_{0}\) separately, since a given \(j_{0}\) must take \((n)\) time to process. Instead, we use tensor techniques related to low-rank approximations to simultanoelsly compute all \(j_{0}\) together and sum them in almost-linear time.

To do that, we create several extra artificial or intermediate matrices \(q(x)^{n n}\)(see Section C), \(p(x)^{n n}\) (see Section C). We will show the gradient can be finally constructed using a simple chaining technique (see Section D for more details), from \(f,c,q\), \(p_{1}\) (handling \(*{diag}(f(x)_{j_{0}})\) similarly), \(p_{2}\) (handling \(f(x)_{j_{0}}f(x)_{j_{0}}^{}\) similarly), \(p\) (\(p=p_{1}-p_{2}\)) to \(L}{*{d}x}\). Intuitively, the chaining shows that a low rank representation for \(f\) yields one for \(c\), and these in turn yield one for \(q\), and so on.

In particular, using \(q(x)\), we obtain that \(L(x)}{*{d}x}\) can be written as

\[_{j_{0}=1}^{n}_{j_{0}}^{}(}_{*{diag}(f(x)_{j_{0}})}-}_{f(x)_{j_{0}}f(x)_{j_{0}}^{}})}_{q(x)_{j_{0}}}\]

which in fact notably removes the summation step of \(i_{0}=1\) to \(d\). Using the notation of \(p(x)\), we finally yield that we need to compute \(A_{1}^{}p(x)A_{2}\). Thus as long as \(p(x)\) has a low-rank representation, then we can solve the in \(n^{1+o(1)}\) time (see Section D for more details). In particular, we will find that \(p(x)\) is the entry-wise product of two matrices with low-rank representations from prior work, which we can combine using a column-wise Kronecker product to approximate \(p(x)\) itself.

## 5 Conclusion

Our results give a complete fine-grained analysis of the running time needed to train LLMs. We show that there is a threshold depending on the parameter \(B\), the magnitude of the parameter matrix entries. In settings where \(B\) is small, a near-linear-time algorithm for LLM training is possible by using our novel algorithm for backward computation. In settings where \(B\) is large, not only does our algorithm not apply, but we show it is impossible to design a nontrivially-fast algorithm (barring a breakthrough in satisfiability algorithms that would refute the popular \(\)).

These insights can guide LLM designers to more efficient algorithms. When \(B\) can be made small, it would lead to substantial savings in the computational resources needed for training and expression. When \(B\) must be large (perhaps to achieve a high expressiveness?), our lower bounds show that one may as well use straigthforward algorithms and focus on other aspects of algorithm speedup such as parallelization. The magnitude of \(B\) needed has been studied more recently (e.g., ), and the need for fast training algorithms may further motivate this direction of research.

## 6 Limitations

Our main algorithm shows that the polynomial method can be used to quickly train LLMs with provable guarantees. While polynomial methods are frequently used for LLM operations in practice, they are typically simpler than the algorithms we present here. Implementing our algorithm in a practical way would require substantial future engineering work which is beyond the scope of this paper. Our lower bound is predicated on the Strong Exponential Time Hypothesis (SETH), a popular conjecture from fine-grained complexity theory. As with most results in complexity theory, results proved using a conjecture like this naturally come with associated limitations: the hard instances of SAT may not translate to the most important instances of LLM training, or the conjecture may not even be true! That said, we wish to emphasize that SETH is the most popular conjecture in fine-grained complexity, used to prove the optimality of many algorithms in nearly every domain of computation, and decades of research in satisfiability algorithms have supported its veracity.

## 7 Broader Impact Statement

We give a new algorithm with provable guarantees for LLM training, which can help to guide future algorithm design in practice. This will help to develop the many positive broader impacts of LLMs. Since this is a purely theoretical work, which addresses theoretical computational concerns for implementing known algorithms, we believe it does not introduce any negative societal impact.