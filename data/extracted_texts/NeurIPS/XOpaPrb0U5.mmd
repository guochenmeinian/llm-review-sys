# QATCH: Benchmarking SQL-centric tasks with Table Representation Learning Models on Your Data

Simone Papicchio

Politecnico di Torino

Turin, Italy &Paolo Papotti

EURECOM

Sophia Antipolis, France &Luca Cagliero

Politecnico di Torino

Turin, Italy

###### Abstract

Table Representation Learning (TRL) models are commonly pre-trained on large open-domain datasets comprising millions of tables and then used to address downstream tasks. Choosing the right TRL model to use on proprietary data can be challenging, as the best results depend on the content domain, schema, and data quality. Our purpose is to support end-users in testing TRL models on proprietary data in two established SQL-centric tasks, i.e., Question Answering (QA) and Semantic Parsing (SP). We present QATCH (Query-Aided TRL Checklist), a toolbox to highlight TRL models' strengths and weaknesses on relational tables unseen at training time. For an input table, QATCH automatically generates a testing checklist tailored to QA and SP. Checklist generation is driven by a SQL query engine that crafts tests of different complexity. This design facilitates inherent portability, allowing the checks to be used by alternative models. We also introduce a set of cross-task performance metrics evaluating the TRL model's performance over its output. Finally, we show how QATCH automatically generates tests for proprietary datasets to evaluate various state-of-the-art models including TAPAS, TAPEX, and ChatGPT.

## 1 Introduction

Table Representation Learning (TRL) models are getting increasing attention for their ability to support various NLP downstream tasks involving tabular data (Badaro et al., 2023; Dong et al., 2022). Such models are built using large open-domain datasets during pre-training and can then be fine-tuned with labelled examples for the target task.

**Motivation.** In most settings, companies aim at adopting a pre-trained model to reduce costs. However, despite the abundance of such methods, it is still challenging to select and manage a TRL model for proprietary data. Models use different pre-training tasks and datasets, work under different assumptions, and the top performing model for an existing benchmark is not necessarily the best one when tested on proprietary data. Indeed, the performance of models fine-tuned on benchmark data is not necessarily replicable on proprietary data.

In a corporate setting, there are at least three scenarios where a given TRL model needs to be evaluated against proprietary datasets:

* Comparison: Compare TRL models fine-tuned on private examples to see which one performs best.
* Validation: As crafting examples is expensive, verify when the quality meets the requirements.
* Maintenance: Fine-tuned models need to be re-calibrated to avoid data and conceptual shifting, continuous evaluation helps the identification of this issue.

**Challenges.** Supporting these scenarios with accurate evaluation is a not obvious task. For the first scenario, consider an engineer who is selecting the TRL model for a Question Answering (QA) taskover their enterprise tabular data. Several options are available and they start assessing the models for a zero-shot setting, where the model is used as-is. As labelled examples are not available, the engineer has to craft some test data to assess how the model is performing on their proprietary tables, e.g., write pairs of questions in natural language (NL) and the expected data results.

The task above has a cost that increases with the number of tables and tests crafted by the engineer. Moreover, the process above applies for all the target tasks that the engineer wants to support. For example, they would have to repeat the exercise for the same model and datasets to evaluate its suitability for Semantic Parsing (SP, aka Text2Sql), where, given a question in NL and a table schema, the model returns a SQL query.

Creating tests is only half of the story. Test results must be evaluated, i.e., the data in the QA model's output \(D_{M}\) should "match" the expected output manually crafted in the test \(D_{T}\). A simple equality test fails short, as the tuples (or values) in \(D_{M}\) can be in different order w.r.t. those in \(D_{T}\). Different order for records and attributes does not matter in the relational model, so the test should be independent from this difference. While existing systems are evaluated with a naive accuracy metric that handles these issues, many problems that make this matching hard are ignored. First, they do not test results for relational data integrity, i.e., if tuple and attribute relationships are satisfied. Second, consider tuples in \(D_{M}\) that are similar to the expected ones in \(D_{T}\) except null values, or that differ in the order when the question requires that results are sorted - these issues are ignored by existing metrics. The same observations apply for the Semantic Parsing task, as it is a common practice to evaluate the produced SQL scripts on their results once executed (Li et al., 2023).

**Automatic Assessment.** QATCH (Query-Aided TRL Checklist) addresses the challenges in evaluating TRL models for proprietary data over SQL-centric tasks. QATCH tests TRL models by automatically _generating_ and _evaluating_ testing checklists of varying complexity for QA and SP.

Given a proprietary table, such as the one in Figure 1 (step 1), QATCH produces questions in NL such as the one in the figure (step 1). The questions are passed, together with the input table, to the TRL models to evaluate (step 1). The models' output is then consumed by our tool (step 1) to produce an assessment of the results (step 1).

At the heart of QATCH, a query generation algorithm carries tests covering a comprehensive range of features enabled by the expressive power of SQL. Our objective is to assess the capability of the models concerning question complexity. To achieve this, we generate the corresponding NL questions using templates designed to maintain clarity and precision. This approach ensures that we do not inadvertently test the models' capacity to handle complex textual content, as our primary focus is on their performance in relation to question complexity.

QATCH also introduces cross-task performance metrics that evaluate the models' results while accounting for the nuances in the comparison of tabular outputs. To address the complexities in evaluating model outputs, our metrics consider numerous factors when comparing databases, including the cardinality of the results, presence of null values, missing or extra cell values, and sorting when required by the NL questions. By taking these aspects into account, QATCH provides a robust and insightful evaluation of model performance.

Our work differs from existing rigid benchmarks, such as Spider (Yu et al., 2018), which cannot capture how well a model performs on proprietary data. Instead, QATCH supports end-users in corporate settings by facilitating decisions on TRL model comparison, validation and maintenance.

Our contributions are summarized as follows:

* We present QATCH, a toolbox for automatically generating and evaluating test checklists tailored to proprietary data and SQL-centric tasks. Code, data, and results are available at https://github.com/spapicchio/QATCH.
* We exploit a SQL query generator to craft a testing checklist for multiple TRL models and two established tasks, i.e., Question Answering and Semantic Parsing.

Figure 1: QATCH takes a table as input and returns metrics for TRL models M1 and M2.

* We introduce robust performance metrics that capture the nuances in comparing model outputs, such as cardinality, null values, missing or extra cell values, and sorting requirements.
* By executing tests of increasing complexity over four TRL models and 8 proprietary datasets, we report insights into the models' capacity to handle two SQL-centric tasks on tabular data.

## 2 Related work

**Table Representation Learning.** Table Representation Learning (TRL) refers to the process of developing and training neural models to capture the underlying structure, semantics, and relationships in tabular data. TRL is used for tasks both in natural language processing (NLP) and data management. Such models support data-driven systems that surpass the limitations of traditional declarative specifications based on first-order logic and SQL.

Examples of tasks that use these models include answering questions in natural language (Katsogiannis-Meimarakis and Koutrika, 2021; Herzig et al., 2020; Liu et al., 2021), fact-checking (Chen et al., 2020; Yang and Zhu, 2021; Aly et al., 2021), semantic parsing (Yin et al., 2020; Yu et al., 2021), table retrieval (Pan et al., 2021; Kostic et al., 2021; Glass et al., 2021), table comprehension (Suhara et al., 2021; Du et al., 2021), and table content prediction (Deng et al., 2020; Iida et al., 2021). These models are built with different architectures (encoder only, decoder only, encoder+decoder), but our approach is agnostic to this aspect and evaluates any model that can satisfy the input/output requirements of a SQL-centric task, i.e., the output may be obtained with a SQL query. In this work, we focus on two tasks that satisfy these requirements: Question Answering (QA) and Semantic Parsing (SP).

_Question Answering_. In the context of free text, QA aims to retrieve passages containing the answer to a given question. In the tabular data setting, QA involves returning the cells that answer a given query, with the input consisting of a question and a table (Herzig et al., 2020; Liu et al., 2021). There are two levels of complexity in tabular QA tasks. Simple QA focuses on lookup queries on tables, while more complex QA tasks require aggregation operations and numerical reasoning.

_Semantic Parsing_. SP in the tabular data setting involves generating a declarative query in SQL over the table's schema, given a question and a table as input (Yin et al., 2020; Liu et al., 2021; Yu et al., 2021; Li et al., 2023). The purpose of SP is to retrieve the answer to the question by producing an interpretable query rather than directly obtaining the answer. Unlike QA, where the focus is on finding the answer cells, SP emphasizes the generation of a structured query.

Other tasks, such as Table Metadata Prediction (TMP) and Table Content Population (TCP), are also widely covered by TRL models, but they do not follow the task requirements. TMP focuses on predicting inter-table metadata, such as column types, headers, cell types, and table types, as well as intra-table relationships, such as equivalence between columns and entity linking/resolution (Deng et al., 2020; Cappuzzo et al., 2020). Meanwhile, TCP addresses the recovery of corrupted cell content and aims to impute missing cell values in an input table (Iida et al., 2021). This serves to enhance and improve the consistency of table information. However, those are traditional DB problems over tabular data and there exist some relevant benchmark solutions relying on metadata creation (Arocena et al., 2016).

**Previous Benchmarking Approaches.** Several benchmarks have been developed for QA and SP models for measuring model performance on fixed datasets. Examples include those closer to QA (Pasupat and Liang, 2015; Chen et al., 2020) and those for SP (Yu et al., 2018; Gkini et al., 2021). WikiTableQuestions is designed for evaluating table comprehension tasks, with a dataset consisting of questions about tables from Wikipedia. TabFact focuses on fact-checking and comprises statements about tables that can be labeled as "true" or "false". Spider, and similar efforts, are corpora of databases, each with a set of SQL queries to test SP. For each query, they provide the paraphrases of the query as a NL question. These datasets have played a role in advancing TRL models across various tasks. However, they are inherently limited due to their fixed set of crafted examples and lack test generation for different TRL models given only the proprietary datasets.

Moreover, previous approaches have relied on simple accuracy metrics, which test if the produced values are included in the ground truth. This "execution accuracy" is adopted also for SP as it is more precise than just comparing SQL scripts (Li et al., 2023; Yu et al., 2018). However, this metric fails to capture nuances in comparing data instances. For example, it does not measure whether values from the same tuple in the ground truth output appear in the same manner in the model output. As a result, this simplistic metric overlooks aspects of TRL models' performance and motivates our proposal.

In terms of test generation, our work took inspiration from an existing effort in measuring the quality of traditional text language models (Ribeiro et al., 2020). Unlike QATCH, Ribeiro et al. (2020) focuses on behavioral testing of NLP models across multiple tasks. Conversely, we leverage the expressive power of SQL to evaluate the test model outcomes directly on tabular data.

## 3 The QATCH Toolbox

QATCH's automatically generates and evaluates test checklists on TRL models based on the three-step process depicted in Figure 2.

1. QATCH-_Generate_. It generates a set of queries tailored to proprietary data. For each query it formulates both the SQL declaration, its free-text version, and the expected ground truth consisting of table instances. The SQL declaration expresses the logical complexity of the query and reflects the presence/absence of specific features peculiar to relational data model.
2. _TRL Model Prediction_. It processes the tests for various TRL models and tasks. The toolbox supports alternative TRL models for both the QA and the SP tasks.
3. QATCH-_Evaluate_. It evaluates the models outputs according to a set of cross-task performance metrics. Separately for each model, it provides end-users with the results for a testing checklist to gain insights into TRL models' strengths and weaknesses.

We describe the tasks covered in the toolbox next. We then explain the test generation. Finally, we discuss the performance metrics.

### Question Answering and Semantic Parsing based on TRL models

Question Answering aims at retrieving the answer A to a given question Q, where both Q and A are expressed in natural language. Here we focus on retrieving the content necessary to formulate the answer from a proprietary relational table \(T\) encoded by a TRL model M.

We specifically address two sub-tasks tailored to Table Representation Learning Models, i.e., Question Answering from TRLs (QA-TRL, in short) and Semantic Parsing based on TRLs (SP-TRLs).

_Question Answering from TRLs_. Let \(T\) be a relational table and let \(S_{T}\) and \(I_{T}\) be \(T\)'s schema and instance, respectively. Executing a SQL declaration _SQL_ on \(T\) entails retrieving a (ground truth) result \(R_{SQL}\) derived from \(I_{T}\):

\[R_{SQL}=}(,S_{T},I_{T})\] (1)

In QA-TRL we do not query the original table \(T\), but rather the TRL model M that encodes \(T\). This allows us to pose the query in natural language and obtain the result directly from the model.

Let SQL\({}_{Q}\) be a SQL declaration and let Q be one of its free-text reformulations, i.e., a question expressed in natural language that is semantically equivalent to SQL\({}_{Q}\). The goal of QA-TRL is to ask question Q on M (which encodes \(T\)) to retrieve a result \(R_{}\)

\[R_{}=}}(,)\] (2)

Figure 2: Workflow of QATCH: given proprietary relational tables as input, it generates tests for Question Answering and Semantic Parsing; tests are executed on a given TRL model; the evaluation metrics are computed between predictions (model output) and ground-truth results.

s.t. \(R_{}\) is equivalent (or mostly similar, in the worst case) to the expected outcome \(R_{SQL}\) from \(T\).

_Semantic Parsing using TRLs._ Given a free-text question Q and a relational table \(T\), the goal of SP is to map Q to the corresponding SQL declaration \(_{}\).

In SP-TRL we leverage the TRL model \(M\), which encodes the table schema \(S_{T}\), to perform the Text2SQL transformation.

\[_{}=(,)\] (3)

_Performance Evaluation._ Testing TRL models for QA-TRL and SP-TRL provides _complementary information_ about their ability to encode complex instance- and schema-level relations holding in tabular data. Instead of verifying the syntactic overlap between free-text questions and SQL declarations, we evaluate results' consistency at the tuple level. Specifically, in QA-TRL we measure how similar the result \(R_{Q}\) returned by the query \(Q\) and the expected outcome \(R_{}\) are in terms of tuple characteristics and cell values. Similarly, in SP-TRL we compare the tuples retrieved by the execution of the output query \(_{}\) on \(T\) with those contained in the expected outcome \(R_{}\).

QATCH_Key Steps._ QATCH automatically generates SQL declarations \(_{Q}\) and free-text questions Q from a proprietary table \(T\). Based on the table schema and instances, a query generator first returns a set \(\) of SQL declarations of varying complexity.

\[=(T)\] (4)

Then, for every query \(_{}\), QATCH generates a _template_ corresponding to one of its free-text versions.

\[=(_{})\] (5)

The separation of queries and questions allows us to disentangle the characteristics of the output from the linguistic capabilities of the models. Free-text questions and SQL declarations are used to test the TRL models on the QA-TRL an SP-TRL tasks.

### Automatic generation of testing checklists with SQL queries

Given a relational table \(T\), QATCH relies on a SQL query generator to produce queries of varying logical complexity for a TRL model's assessment. Each test query consists of a triple \(,_{},_{}^{gt}\), where \(_{}\) is the input query formulated as a SQL declaration, Q is one of the possible free-text reformulation of the question, and \(_{}^{gt}\) is the corresponding ground truth.

Table 1 enumerates the templates used to generate \(_{}\) first and then Q based on proprietary data. They encompass SQL projections on the table schema (Project), selections on the table instance (Select), and more complex cardinality-based and sorting criteria at the tuple level (Distinct and Order by)1. The variable in the templates are automatically filled up by the tool according to the values in the schema and active domain on the given \(T\).

The SQL generator automatically crafts multiple versions of the input tests, each one incorporating a different feature. For example, for the Project template, QATCH explores the following three cases: (1) SELECT ALL, which selects all the attributes in the table schema; (2) SELECT-RANDOM-COL,

  
**Category** & **SQL declaration** & **Free-Text question** \\  Project & SELECT \(\{c_{1},,c_{n}\}\) FROM \(\{T\}\) & Show \(\{c_{1},,c_{n}\}\) in table \(\{T\}\) \\ Distinct & SELECT DISTINCT \(\{c_{1},,c_{n}\}\) FROM \(\{T\}\) & Show the different \(\{c_{1},,c_{n}\}\) in table \(\{T\}\) \\ Select & SELECT * FROM \(\{T\}\) WHERE \(\{c_{i}\}\) \{op\} \{val\} & Show data of table \{t\} where \(\{c_{i}\}\{op\}\{val\}\) \\ Order by SELECT * FROM \(\{T\}\) ORDER BY \(\{c_{i}\}\) \{ord\} & Show data for table \(\{T\}\) in \{ord\} order by \(\{c_{i}\}\) \\   

Table 1: Templates for queries in SQL and natural language. \(T\) is the target relational table. \(c_{i}_{T}\) (\(1 i n\)) is an attribute of the \(T\)’s schema, \(op\) (=,!=, \(>\), \(<\), \(\), \(\)) is a logical operator, \(_{i}\) is an arbitrary value for attribute \(c_{i}\) occurring in \(_{T}\). \(ord\) is the order of visualization of the tuples in the output (i.e., ascending or descending).

[MISSING_PAGE_FAIL:6]

* The tuple constraint ignores partial tuple matches (e.g., [apple, red] is different from [apple] in Prediction 2), which is captured by cell recall.
* When the number of expected tuples is zero, tuple cardinality takes either value zero, if the number of predicted tuples is greater than zero, or one, if no tuples are predicted.

To explain the rationale behind the proposed metrics, let us consider the following target values (a1, b1, c1). The first prediction, denoted by "output 1", is composed of two tuples (a1, b1, c1), (a1, b1, c1). Instead, the second prediction, denoted by "output 2", has the following two tuples: (a1, b1), (c1). "Output 1" has cardinality two instead of one whereas "output 2" has an incorrect schema. In both cases the tuple constraint returns zero even if part of the output cells match. Notice also that even if the outputs' cardinality is the same cell precision and recall show relevant differences in the returned values.

Our Cell Precision is equivalent to the Execution Accuracy reported in most QA and SP papers. We remark that while informative, this single metric does not capture cases where the output is incorrect, such as Prediction 5 in Figure 3.

## 4 Experimental Evaluation

**TRL models.** We test six TRL models and one Large Language Model, i.e., ChatGPT (OpenAI, 2023). For QA-TRL, we report for TAPAS (Herzig et al., 2020), TAPEX (Liu et al., 2021), and Omnitab(Jiang et al., 2022). TAPAS leverages the transformer architecture to pre-train on large-scale datasets and fine-tune to specific tasks using labeled examples. TAPEX exploits the idea of learning in the pre-training a neural SQL executor over a synthetic corpus of SQL queries and their execution outputs. Omnitab exploits a pretraining approach that uses both natural and synthetic data to learn reasoning over multiple table elementes. We use TAPAS and TAPEX fine-tuned on the WTQ dataset for QA.

For SP-TRL, we report for ResdSQL (Li et al., 2023), GAP (Shi et al., 2021), UnifiedSKG (Xie et al., 2022). Resdsql is based on a seq2seq architecture with a ranking-enhanced encoding and skeleton-aware decoding framework. UnifiedSKG implements the encoder-decoder (text-to-text) model based on T5. We use UnifiedSKG and Resdsql with the T5 large setting. GAP employs generative models to create pre-training data, enabling the simultaneous learning of natural language utterances and table schemas.

ChatGPT is a language model employing a decoder-only transformer setup; by leveraging a chat interface, it allows to specify a range of tasks (including QA and SP) with the in-context adaptation technique. This technique leverages the model's ability to understand and consider the immediate context of a conversation or text. Further details to reproduce the results can be found in the appendix.

Figure 3: QATCH’s metrics are computed between the model output (prediction) and expected ground-truth results (target). The target is the answer of the NL question ”_Show me all the data_” over a table with three tuples and two attributes.

**Datasets.** We generate tests for 8 proprietary tables that we have selected from Kaggle to maximize variety both in terms of content (four categories) and in terms of data properties (different sizes and arity), as detailed in Table 2. These datasets are available online, thus possibly "seen" by the models, but are not available for question answering and semantic parsing tasks, i.e. they do not come with natural language questions or queries. The 8 tables are used in this paper as a sample to show the benefit of automatic test generation with QATCH, we are not suggesting to use them as a new corpus with a fixed set of questions and tables. We also report results for the widely used Spider benchmark, as this is the standard for QA and SP tasks evaluation, using its tables and its questions. This comparison aims to emphasize the discrepancies between the quality results obtained using Spider and the practical application of models on proprietary tables. More details on the data processing phase can be found in the appendix.

As models are limited in the input, for every table we sample a subset of rows and attributes that can be executed within the model context (e.g., about 4k tokens for ChatGPT).

  
**Category** & **Table Name** & **\# rows** & **\# categorical** & **\# numerical** & **Example** \\  & & & **cols** & **cols** & **cols** \\   & Sales-transactions & 500k & 5 & 3 & ProductNo, Date \\  & Fitness-trackers & 565 & 8 & 3 & Brand Name, Display \\   & Account-fraud & 1M & 4 & 26 & DaysSinceRequest, Velocity6h \\   & Late-payment & 2466 & 6 & 6 & InvoiceDate, Disputed \\   & Heart-attack & 303 & 1 & 11 & \# trtbps, \# oldpeak \\   & Breast-cancer & 686 & 5 & 6 & pgr, rfstime \\   & Adult-census & 32.6k & 9 & 6 & education, fnlwgt \\   & Mushrooms & 8.1k & 23 & 0 & cap-shape, ring-type \\   

Table 2: Information for the proprietary tables used in the experiments.

  
**Category** & **Model** & **Cell** & **Cell** & **Tuple** & **Tuple** & **Tuple** & **Avg** \\  & & **precision** & **recall** & **cardinality** & **constraint** & **order** & \\   &  & & & & \\  & Tapas-large-wtq & **0.71** & 0.12 & **0.53** & 0.05 & 0.33 & **0.35** \\  & Tapex-large-wtq & 0.40 & 0.06 & 0.18 & 0.01 & 0.40 & 0.21 \\  & Omnitab & 0.20 & 0.01 & 0.14 & 0.00 & **0.50** & 0.17 \\  & ChatGPT 3.5 & 0.44 & **0.24** & 0.20 & **0.10** & 0.42 & 0.28 \\   & Tapas-large-wtq & **0.72** & 0.12 & **0.48** & 0.05 & 0.38 & 0.35 \\   & Tapex-large-wtq & 0.52 & 0.06 & 0.16 & 0.01 & 0.48 & 0.25 \\   & Omnitab & 0.30 & 0.02 & 0.13 & 0.00 & **0.50** & 0.19 \\   & ChatGPT 3.5 & 0.71 & **0.52** & 0.38 & **0.21** & 0.48 & **0.46** \\   & Tapas-large-wtq & 0.72 & 0.16 & **0.57** & 0.09 & 0.34 & 0.38 \\   & Tapex-large-wtq & 0.37 & 0.04 & 0.15 & 0.0 & 0.44 & 0.20 \\   & Omnitab & 0.29 & 0.01 & 0.12 & 0.0 & 0.50 & 0.18 \\   & ChatGPT 3.5 & **0.77** & **0.46** & 0.22 & **0.12** & **0.70** & **0.45** \\   & Tapas-large-wtq & 0.67 & 0.12 & 0.34 & 0.04 & 0.29 & 0.29 \\   & Tapex-large-wtq & 0.48 & 0.10 & 0.25 & 0.01 & 0.44 & 0.26 \\   & Omnitab & 0.12 & 0.02 & 0.13 & 0.01 & **0.50** & 0.17 \\   & ChatGPT 3.5 & **0.76** & **0.67** & **0.36** & **0.16** & **0.50** & **0.49** \\   &  & & & & \\   & Tapas-large-wtq & 0.64 & 0.42 & 0.53 & 0.30 & 0.64 & 0.51 \\   & Tapex-large-wtq & 0.62 & 0.45 & 0.54 & 0.21 & 0.51 & 0.47 \\   & Omnitab & 0.30 & 0.24 & 0.53 & 0.23 & 0.52 & 0.36 \\   & ChatGPT 3.5 & **0.74** & **0.77** & **0.86** & **0.66** & **0.75** & **0.76** \\   

Table 3: Results for QA-TRL models: average of all tests on multiple tables. ChatGPT version:

 ChatGPT 3.5-turbo-0613

**Results.** Tables 3 and 4 show the results for QA-TRL and SP-TRL, respectively. All metric results are averaged over all tests and over tables grouped by category; we report detailed results for every dataset and every test category in the appendix.

For the QA-TRL task, Table 3 reports the results achieved on the proprietary tables and on the Spider benchmark to enable a comparison between the two groups. All models show promising results in terms of Cell precision, which is the metric closest to the one used in previous papers. However, the other metrics show lower values, with all models struggling in preserving the intra-tuple value relations in their output (i.e., low Tuple constraints). Also, in almost all experiments the models tend to remove duplicates, even when the Distinct clause is not present. This is evident with the low scores for Cell recall and Tuple Cardinality. Finally, none of the models is able to return results according to the Order by requirements (i.e., low Tuple order scores).

Results for the SP task in Table 4 show that, in general, the TRL models performs much better in this task. The most problematic aspect is the preservation of the tuple structure for the returned values (Tuple constraint).

Tests can be generated for a specific dataset using QATCH in just a few seconds. The testing times varies based on the data, particularly the size of the table. For instance, comparing a prediction and a ground truth containing 1000 tuples and 20 attributes takes less than a second.

As we do not control the infrastructure of OpenAI, we do not report exact execution times for ChatGPT. On average, it takes a few seconds to execute one test, but the distribution for this metrics is skewed as it heavily depends on the server workload.

**Discussion.** The results show that the tests generated by QATCH and its metrics enable detailed evaluation of the models. A clear message is that there is no single model that works best for every table and every metric. In some datasets, TAPAS has the best performance in terms of cell precision and recall, but ChatGPT is the best model in terms of tuple cardinality and tuple constraint. The

  
**Category** & **Model** & **Cell** & **Cell** & **Tuple** & **Tuple** & **Tuple** & **Avg** \\  & & **precision** & **recall** & **cardinality** & **constraint** & **order** & \\   \\  & Resdsql & 0.91 & 0.89 & 0.92 & 0.81 & **1.00** & 0.90 \\  & GAP & 0.84 & 0.80 & 0.81 & 0.73 & 0.97 & 0.83 \\  & UnifiedSKG & 0.71 & 0.71 & 0.69 & 0.69 & **1.00** & 0.76 \\  & ChatGPT 3.5 & **0.98** & **0.98** & **0.99** & **0.95** & **1.00** & **0.98** \\  & Resdsql & 0.90 & 0.87 & 0.95 & 0.77 & **1.00** & 0.90 \\  & GAP & 0.79 & 0.78 & 0.76 & 0.74 & **1.00** & 0.81 \\  & UnifiedSKG & 0.79 & 0.76 & 0.74 & 0.67 & 0.98 & 0.79 \\  & ChatGPT 3.5 & **0.96** & **0.96** & **0.99** & **0.90** & **1.00** & **0.96** \\  & Resdsql & 0.86 & 0.75 & 0.94 & 0.67 & 0.95 & 0.83 \\  & GAP & 0.77 & 0.73 & 0.73 & 0.67 & 0.59 & 0.70 \\  & UnifiedSKG & 0.72 & 0.69 & 0.70 & 0.66 & 0.95 & 0.74 \\  & ChatGPT 3.5 & **1.00** & **1.00** & **0.98** & **0.99** & **1.00** & **0.99** \\  & Resdsql & 0.94 & 0.90 & 0.90 & 0.77 & **1.00** & 0.90 \\  & GAP & 0.82 & 0.78 & 0.73 & 0.69 & **1.00** & 0.80 \\  & UnifiedSKG & 0.74 & 0.69 & 0.68 & 0.59 & 0.98 & 0.73 \\  & ChatGPT 3.5 & **0.98** & **0.98** & **0.98** & **0.91** & **1.00** & **0.97** \\   \\  & Resdsql & 0.93 & 0.93 & **0.97** & 0.84 & 0.99 & 0.93 \\  & GAP & **0.95** & 0.95 & 0.96 & 0.91 & 0.96 & **0.95** \\  & UnifiedSKG & 0.81 & 0.82 & 0.82 & 0.80 & **1.00** & 0.85 \\  & ChatGPT 3.5 & 0.93 & **0.96** & **0.97** & **0.92** & 0.90 & 0.94 \\  & ChatGPT 3.5 & 0.90 & 0.92 & 0.92 & 0.88 & 0.97 & 0.92 \\   

Table 4: Results for SP-TRL models: average of all tests on multiple tables. ChatGPT version: ChatGPT 3.5-turbo-0613. UnifiedSKG and ResdSQL evaluated with the T5 large.

lower precision for ChatGPT in some cases is due to the hallucination problem with the decoder architecture.

Another take-away is that questions and tables in Spider do not tell the full story about model performance. Even a small selection of 8 tables show that the performance for established TRL models can drop dramatically on propriety data in the QA task.

Looking into the details of the failed tests, it is easy to spot the limitations of the models. Cell precision is lowered by select queries with more than one column in the output. The models tend to return only one attribute even in this setting.

In some cases, Tuple cardinality is lower than Cell recall, because the latter is over a set. For some queries, models return DISTINCT by default. Most of the questions in Spider return one tuple only, thus inflating the aggregate result and hiding the problem with more output attributes. For the same reason, tuple constraint scores are low because in many cases the models do not return tuple structures, but mostly separated cells. We again observe low results for proprietary data with the higher ones for Spider.

We can conclude that the qualitative performance of a TRL model depends on the target task at hand, but also on the tabular content in terms of data domain, size, schema, and data quality.

## 5 Conclusion and Future Work

We presented QATCH, a testing tool for TRL models. It provides end-users with a flexible and adaptive solution for the automatic assessment of models' performance on proprietary data. The key goal is to avoid trusting the results achieved on the existing benchmarks across different tasks, as they are not always replicable on proprietary data. By leveraging the expressive power of the SQL language, we first generate queries of varying complexity on proprietary table and then convert them in natural language. Thus, we specifically assess the model capability to address complex queries rather than the linguistic properties of questions and answers. Results show that (1) existing questions in benchmarks do not capture important properties of custom tables, (2) popular metrics fail short in measuring the quality of the models' output.

As future research agenda, we plan to investigate the following research directions: (1) The extension of QATCH towards the automatic assessment of additional models (e.g., Bard (Google Inc., 2023), LLama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023)), tasks (e.g., fact checking (Guo et al., 2022; Nakov et al., 2021), querying LLMs (Saeed et al., 2024; Urban et al., 2023)), and more complex queries (e.g., Group by clauses, nested queries). (2) The adoption of Generative Language Models (e.g., (OpenAI, 2023)) to automatically generate templates in Natural Language from the SQL declarations. (3) The use of TRL models to test inter-relational constraints, such as joins and referential constraints, among proprietary tables.