# Exploring Behavior-Relevant and Disentangled Neural Dynamics with Generative Diffusion Models

Yule Wang

Georgia Institute of Technology

Atlanta, GA, 30332 USA

yulewang@gatech.edu &Chengrui Li

Georgia Institute of Technology

Atlanta, GA, 30332 USA

cnlichengrui@gatech.edu &Weihan Li

Georgia Institute of Technology

Atlanta, GA, 30332 USA

weihanli@gatech.edu &Anqi Wu

Georgia Institute of Technology

Atlanta, GA, 30332 USA

anqiwu@gatech.edu

###### Abstract

Understanding the neural basis of behavior is a fundamental goal in neuroscience. Current research in large-scale neuro-behavioral data analysis often relies on decoding models, which quantify behavioral information in neural data but lack details on behavior encoding. This raises an intriguing scientific question: "_how can we enable in-depth exploration of neural representations in behavioral tasks, revealing interpretable neural dynamics associated with behaviors_". However, addressing this issue is challenging due to the varied behavioral encoding across different brain regions and mixed selectivity at the population level. To tackle this limitation, our approach, named "BeNeDiff", first identifies a fine-grained and disentangled neural subspace using a behavior-informed latent variable model. It then employs state-of-the-art generative diffusion models to synthesize behavior videos that interpret the neural dynamics of each latent factor. We validate the method on multi-session datasets containing wide-field calcium imaging recordings across multiple brain regions of the dorsal cortex. By guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics of latent factors in the disentangled neural subspace provide interpretable quantifications of the behaviors of interest across multiple brain regions. Meanwhile, the neural subspace in BeNeDiff demonstrates high disentanglement and neural reconstruction quality. Our codes are available at https://github.com/BRAINML-GT/BeNeDiff.

## 1 Introduction

Understanding and elucidating the complex interrelationships between behavioral data and neural population activity is a long-standing goal in systems neuroscience . Exploring the neural basis of behavior not only deepens our basic knowledge of brain functions but also establishes a foundation for developing improved treatments for psychiatric and neurological conditions . Significant progress has been achieved in developing computational toolkits for neuro-behavioral decoding by using behavior video data . These methods perform region-based behavior decoding to map neural activity across multiple brain regions of the dorsal cortex to the behaviors from the videos. However, these methods only quantify how much behavioral information is encoded in neural populations, but do not reveal the details of such encoding. There has been markedly less focus,with cortex-wide signals, on enabling in-depth exploration of neural activities during behavioral tasks, where specific neural patterns reveal dynamic evolutions corresponding to distinct behaviors of interest. However, empirically addressing this scientific question is challenging due to neural population activities in various brain regions exhibiting mixed selectivity (Sani et al., 2021; Hasnain et al., 2023), responding robustly to multiple behaviors of interest. We further verify this finding through an empirical study across three brain regions on the dorsal cortex of head-fixed mice (Musall et al., 2019) (shown in Figure 1).

To tackle this issue, we propose a method - Exploring **Be**havior-Relevant and Interpretable **Ne**ural Dynamics with Generative **Diff**usion Models - ("BeNeDiff"). We first employ a neural latent variable model (LVM) to identify orthogonal and disentangled neural latent subspace. This is achieved through a semi-supervised variational autoencoder, which integrates behavioral labels to rotate the subspace. Subsequently, our main idea is to explore the neural dynamics of each latent factor in the learned subspace for distinct quantifications of the behaviors of interest. However, such a workflow is non-trivial since naive latent manipulation produces samples not conform to the original distribution, leading to mapped video-based behavioral data that loses its validity (we further detail this part in Method Section 3.2.1).

Notably, we aim to investigate the behavioral-specificity of neural latent factors in a generative fashion. We leverage state-of-the-art video diffusion models (VDMs) to generate behavior videos predicted to _activate_ individual latent factors along the single-trial trajectory. Technically, the VDMs are capable of capturing the overall temporal dynamics and synthesizing behavior videos in a classifier-guided manner (Dhariwal and Nichol, 2021). Inspired by Noise-Contrastive Estimation (Gutmann and Hyvarinen, 2010), the guidance objective is formulated to amplify the variance of the selected latent factor along its neural trajectory while suppressing the variance of the neural trajectories of the other latent factors.

We conduct experiments to verify the efficacy of BeNeDiff on a widefield calcium imaging dataset, where a head-fixed mouse performs a visual decision-making task across multiple sessions (Musall et al., 2018, 2019). The neural subspace in BeNeDiff exhibits high levels of disentanglement and neural reconstruction quality, as evidenced by multiple quantitative metrics. By guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics within the disentangled subspace provide interpretable and selective quantifications of the behaviors of interest (e.g., paw movements) across multiple brain regions. These results advance our understanding of neuro-behavioral relationships through the identification of fine-grained behavioral subspaces and the uncovering of disentangled neural dynamics.

To highlight our major contributions: (1) This is the first work to explore wide-field imaging across multiple brain regions of the dorsal cortex of head-fixed mice during a decision-making task using neural subspace analysis, rather than merely performing neuro-behavior decoding. We uncover disentangled neural representations for various behaviors. (2) To visualize the behavior dynamics within a disentangled neural subspace of each brain region, we develop a novel VDM-based interpretation tool that faithfully reflects behavior-related neural dynamics. It is essential to interpret the meaning of each neural latent dimension as well as the behavior dynamics it encodes.

Figure 1: **Empirical study across multiple brain regions of dorsal cortex neural recordings of a mouse in a visual decision-making task. (A) The Brain Atlas map (Lein et al., 2007). (B) Neural signals in various brain regions (SSp, MOs, and VIS) exhibit mixed selectivity in behavior of interest decoding. “Levers”, “Spouts”, “Paw-(x)”, and “Jaw” are four behaviors of interest. \(^{2}\) is short for cross-validation coefficient of determination. The higher, the better.**

## 2 Preliminaries

### Problem Formulation

We first provide the notations of the paired neuro-behavioral observations. The single-trial neural population activities are denoted as \(=[_{1},,_{L}]^{}^{L  N}\), where \(L\) is the trial length (_i.e._, number of time bins), \(N\) is the number of observed neural signals. The behavioral video frames are denoted as \(=[_{1},,_{L}]^{}^{L H W}\), where \(H\), \(W\) are the height and width of the compressed behavior video frames. We extract behavior labels \(=[_{1},,_{L}]^{}^{L B}\) from the video frames using a behavior LVM (Whiteway et al., 2021). \(B\) is the number of the behavior.

We build a variational autoencoder (VAE) (Kingma and Welling, 2013) to infer the neural latent trajectories \(=[_{1},,_{L}]^{}^{L D}\), which are also informed by behavioral labels. \(D\) is the latent factor number. We denote its probabilistic encoder and decoder as \(q_{}(,)\) and \(p_{}(,)\), respectively. We denote the neural trajectory of a single latent factor as \(^{(d)}=_{:,d}\), where \(d\{1,2,,D\}\). Our primary goal is to investigate the neural dynamics of \(^{(d)}\) through selectivity quantifications of its corresponding single-trial behavioral video data \(\).

### Generative Video Diffusion Models

Diffusion models have also achieved impressive results in video synthesis over recent years (Ho et al., 2022; 20; Harvey et al., 2022). VDMs process a fixed number of frames and factorize them over the temporal dimension via a deep neural network (Ho et al., 2022; Harvey et al., 2022). The training of VDMs starts from a forward process with a variance schedule \(\{_{1},,_{T}\}\), the noised sample \(_{t}\) follows the Gaussian conditional: \(q(_{t}_{0}):=(_{t };_{t}}_{0},(1-_{t})\,)\), where \(_{t}:=1-_{t}\) and \(_{t}:=_{s=1}^{t}_{s}\). A denoising model \(}_{}()\) is trained to reverse the forward process using a weighted mean squared error loss:

\[_{}()=_{(,)},_{t [0,T]}[w(_{t})\|- }_{}(_{t},t) \|_{2}^{2}],\] (1)

in which time-steps \(t\) are uniformly sampled and \(w(_{t})\) is the weighting ratio. This loss function can be justified as optimizing a weighted variational lower bound on the data log-likelihood. In the sampling phase, we start from \(_{T}(,_{L H W})\) and perform step-by-step denoising,

\[_{t-1}=}}(_{t}-}{_{t}}}}_{}(_{t},t))+_{t}_{t},\] (2)

where random noise perturbation \(_{t}(,_{L H W })\) for timesteps \(t>1\), \(_{t}=\) when \(t=1\), and \(_{t}^{2}=_{t-1}}{1-_{t}}_{t}\).

## 3 Methods

Then, we train a linear neural encoder from the behavior video frames to the neural trajectories. We leverage video diffusion models (VDMs) to generate behavior videos guided by the neural encoder,

Figure 2: **Schematic diagram of neural dynamics interpretation with BeNeDiff.** We first employ a neural LVM to identify a disentangled neural latent subspace (the left part). Then, we train a linear neural encoder to map behavior video frames to neural trajectories. We use video diffusion models (VDMs) to generate behavior videos guided by the neural encoder, based on the objective of _activating the variance_ of individual latent factors along the single-trial trajectory. This approach provides interpretable quantifications of neural dynamics in relation to the behaviors of interest.

based on the objective of _activating the variance_ of individual latent factors along the single-trial trajectory, providing interpretable quantifications of neural dynamics with respect to the behaviors of interest.

In this section, we first detail the process by which BeNeDiff infers a disentangled neural latent subspace. We then discuss the approach that BeNeDiff interprets the selectivity of neural dynamics of latent factors using the video diffusion model.

### Behavior-Relevant and Disentangled Neural Latent Subspace Learning

Drawing inspiration from recent progress in the field of neural LVMs (Kingma et al., 2014; Klys et al., 2018), we employ a VAE to learn a disentangled neural subspace. The neural data \(\) usually contains a good amount of information other than behavior (Hasnain et al., 2023), thus an unsupervised disentangled VAE won't effectively discover disentangled subspace with behavior only. Therefore, we introduce behavior labels \(\) to inform the VAE to learn a latent subspace that better accounts for the variance related to behavior. We note that this technique is widely adopted in previous neuro-behavioral analysis works (Wang et al., 2024; Schneider et al., 2023; Gondur et al., 2023). Notably, to enforce the disentanglement in the latent subspace, we incorporate a _total-correlation_ (TC) penalty term (Chen et al., 2018) to enforce the VAE to find statistically independent latent factors in the semi-supervised setting. The VAE optimizes the following evidence lower bound (ELBO) (MacKay, 2003):

\[ p_{}(,) _{q_{}(|, )}}()}_{ }+}( )}_{}-_{ }q_{}(,)p( )}_{}\] (3)

in which \(^{(d)}\) denotes the neural trajectory of the \(d\)-th latent factor, and the value of \(\) controls the strength of disentanglement penalty. However, the factorial density in this term is untractable in practice, so here we use the minibatch-weighted sampling estimator (Chen et al., 2018) to approximate the TC penalty term. We note that the variational autoencoder employs a sequential architecture (Fabius and Van Amersfoort, 2014) to capture the overall temporal dynamics along the single-trial trajectory \(\{_{l}\}_{l=1}^{L}\), plugging bi-directional recurrent units (Schuster and Paliwal, 1997) into both the probabilistic encoder \(q_{}()\) and decoder \(p_{}()\).

### Diffusion Guided Video Generation for Neural Dynamics Interpretation

#### 3.2.1 Downside of Latent Manipulation for Interpreting Neural Dynamics

As for testifying the neural dynamics of a single disentangled latent factor \(^{(d)}\) on the behavioral videos \(\), a straightforward attempt is to train a neural-net model to approximate the posterior distribution \(p()\) and then perform latent manipulation on each single latent factor. There are two major techniques to perform latent manipulation. The first is a naive manipulation. This method manipulates a single subspace \(^{(d)}\) while keeping the non-target latent factors fixed at arbitrary values. It then observes how the manipulation affects \(\). The induced changes in the videos reveal the dynamics encoded by \(^{(d)}\). The second method uses classifier-free guidance (Ho and Salimans, 2022), where we allow the activated latent factor \(^{(d)}\) to evolve while fixing non-target latent factors to arbitrary values. However, setting arbitrary values without knowing the true distributions of non-target subspaces can lead to unnatural distortions in generated videos, complicating the visualization and interpretation of genuine animal behavioral dynamics.

#### 3.2.2 Behavioral Video Generation for Neural Dynamics Interpretation

So here we employ the video diffusion models (VDMs) to explore factor-wise neural dynamics through a generative manner, which is capable of maintaining temporal consistency and behavioral dynamics across frames. The primary goal is to perform behavior data generation conditioned on activating a single latent factor along the neural trajectory. Thus the resulting behavior video can provide interpretable quantifications of the neural dynamics of factor \(^{(d)}\). Specifically, we implement classifier guidance (Kawar et al., 2022). By Bayes rule, we obtain the following posterior density and gradient (Mardani et al., 2023):

\[p_{,}(_{t} ) =p_{}(_{t})p_{} (_{t})/\,p(),\] (4) \[_{_{t}} p_{,}(_{t}) =_{t}} p_{} (_{t})}_{}+_{t}}  p_{}(_{t})}_{ },\] (5)

in which \(,\) are the parameter sets for the classifier and the denoising model, respectively. Note that \(t\) indicates the time step in the diffusion model. Our goal is to estimate the two terms on the RHS of Eq. (5) to perform conditional denoising in each step. We first approximate the density of the behavior video data through a standard denoising model \(}_{}(_{t},t)\) according to Eq. (1) since the first unconditional gradient term can be derived through it:

\[_{_{t}} p_{}(_{t}) =-_{t}}}}_{ }(_{t},t).\] (6)

For the calculation of the guidance term, we first train a linear neural encoder as the classifier from the behavior video data to the neural latent variables of the learned semi-supervised VAE subspace. We denote the estimated neural latent trajectories as \(}_{t}=[}_{t,1},,}_{t, L}]^{}^{L D}\), in which:

\[}_{t,l}=\,(_{t,l})+; \ \ (,),\] (7)

where \(1 l L\), \(}_{t,l}^{D}\) denotes the estimated value of latent factors at time bin \(l\) and diffusion step \(t\). The parameter set of the linear encoder \(=\{,\}\). \(^{D M}\) is the linear transformation matrix, \(^{D D}\) is the covariance matrix and \(()\) represents vectorizing the two-dimensional video frame into column vector. After training the encoder, we fix all parameters and use it to construct the density \(p_{}(_{t})\).

The class labels \(\{^{(1)},^{(2)},,^{(D)}\}\), in which \(^{(d)}\) is a one-hot column vector with a one at the \(d\)-th dimension and zeros elsewhere. Drawing inspiration from Noise-Contrastive Estimation (Gutmann and Hyvarinen, 2010), our guidance objective of the activation of latent factor \(d\)-th is formulated as maximizing the variance of the trajectory \(}^{(d)}\) while minimizing the variance of the other latent factor trajectories in \(}\):

\[ p_{}(^{(d)}|_{t} )=[}^{+}(},^{(d)})/)}{(f_{}^{+}(},^{(d)})/)+_ {k=1}^{K}(f_{}^{-}(},^{(d)})/)}],\] (8)

where \(f_{}^{+}(},^{(d)})= (})^{(d)}\) calculates the variance of the selected latent factor and \(f_{}^{-}(},^{(d)})= (})^{(j)},\ j(\{1,2,,D\}\{d\})\) calculates the variance of another sampled latent factor's trajectory. \((})^{1 D}\) is a row vector where each element is the variance of every latent factor along the neural trajectory. \(\) is the temperature parameter. \(K\) is a hyperparameter controlling the number of sampled negative samples at each iteration.

The gradient \(_{_{t}} p_{}(^{(d)} _{t})\) is computed using automatic differentiation (Paszke et al., 2017). Algorithm 1 describes the guided behavior video generation steps of our proposed framework BeNeDiff.

``` Input: Condition label \(^{(d)}\) for interpreting the neural dynamics of the \(d\)-th latent factor  Initialize \(_{T}(,_{L H W})\) for\(t=T\)to\(1\)do \(}^{}_{,}(_{t },t)=}_{}(_{t },t)-_{t}}_{_{t}} p_{ }(^{(d)}_{t})\) \(_{t}(,)\) if \(t>1\), else \(_{t}=\) \(_{t-1}=}}(_{t}-}{_{t}}}}^{}_{ ,}(_{t},t))+ _{t}_{t}\) end Output: Generated behavior video \(_{0}\) ```

**Algorithm 1**Generative Video Diffusion Model for Neural Dynamics Interpretation

Related Works

**Disentangled Latent Subspace Learning.** Neural LVMs is a fundamental framework which posits that single-trial neural population activities rely on low-dimensional "neural manifolds" (Gallego et al., 2018; Mitchell-Heggs et al., 2023; Li et al., 2023a; Hurwitz et al., 2021) and their extracted latent variables are successful in describing single-trial neural activities (Li et al., 2024c; 2022; 2024d; Liu et al., 2021, 2022; Li et al., 2024a). Learning disentangled latent variables that uncover statistically independent latent factors (Chen et al., 2018) can provide enhanced robustness, interpretability, and controllability. Typically, this type of work involves adding auxiliary regularizer terms to enhance orthogonality (Mathieu et al., 2019) and reduce the total correlation (Chen et al., 2018) among the latent factors. In neuroscience, there have been studies focusing on the disentanglement of latent subspace within rich behavioral data (Whiteway et al., 2021; Shi et al., 2021). However, our work is the first to discover interpretable and disentangled latent subspaces of wide-field imaging data.

**Generative Diffusion Models.** In recent years, diffusion models have achieved great success in generating high-quality images due to their expressivity and flexibility (Ho et al., 2020; Song et al., 2020; Wu et al., 2021; Vahdat et al., 2021). Moreover, for the more challenging task of video generation, there have been several explorations using diffusion models to address it. From a modeling perspective, the key concern is how to maintain temporal dynamics and consistency across frames. Most existing works (Ho et al., 2022; Wu et al., 2021) extend the 2D U-Net architecture (Ronneberger et al., 2015; Song et al., 2024) to a 3D framework by considering the time axis. In this 3D framework, convolutions are performed in both spatial and temporal dimensions. Additionally, recent studies in neural computation have leveraged generative diffusion models to tackle domain-specific tasks, such as neural distribution alignment (Wang et al., 2024) and decoding visual stimulus from brain activities (Sun et al., 2024; Wu et al., 2021). Our work is the first to employ generative diffusion models for analyzing neuro-behavioral data relationships.

## 5 Experimental Results

### Dataset Description

A head-fixed mouse performed a visual decision-making task while neural activity across the dorsal cortex was optically recorded using widefield calcium imaging (Musall et al., 2019; Churchland et al., 2019). The mouse's behavior included both instructed and uninstructed movements. For behavioral data acquisition, two cameras captured video frames from both a side view and a bottom view. The dataset comprises 1126 trials conducted over two sessions, with 189 frames per trial at a frame rate of 30 Hz. Concurrently, neural activity was recorded at the same frame rate. The grayscale video frames were downsampled to 128\(\)128 pixels. We extract 275 dimensions of neural signals from the high-dimensional widefield imaging data using the open-sourced LocaNMF decomposition toolkit (Saxena et al., 2020). As shown in Figure 3, the behaviors of interest include the moving lick spots, moving levers, the single visible right paw trajectories, and the movement of the jaw and chest, all tracked using DeepLabCut (Mathis et al., 2018).

### Disentangled Neural Latent Subspace Investigation

We note that we train a unique neural LVM for each individual brain region (single-region), and we evaluate both the behavior decoding and neural reconstruction performance of each brain region-specific neural latent trajectories.

**Single Latent Factor Behavior Decoding.** In order to verify the disentanglement of the learned neural subspace in BeNeDiff, we evaluate the behavior label decoding performance of each individual latent factor. Specifically, we train a unique linear regressor for each latent factor from the VAE and plot the decoding accuracy as the R-squared value (\(R^{2}\%\)). The results of VIS-Right region (the right visual region) are shown in Figure 4. The main observation is that each latent factor is specific to a

Figure 3: **Widefield Calcium Imaging Dataset. The head-fixed mouse is performing a visual decision-making task, with the behaviors of interest and the trial structure illustrated.**

unique behavior of interest, confirming the orthogonality and clear disentanglement of the inferred latent trajectories from a quantitative perspective.

**Neural Observation Signals Reconstruction.** To prevent the VAE from overfitting to the behavioral labels, BeNeDiff also aims to maintain a low reconstruction error for neural activity. Table 1 presents the quantitative reconstruction results compared to baseline methods, including Semi-Supervised Learning (SSL) [Kingma et al., 2014], CEBRA [Schneider et al., 2023], and pi-VAE [Zhou and Wei, 2020]. The table records the R-squared values (\(R^{2}\), in %) and RMSE for each method. Additionally, we plot the ground-truth neural signals and the reconstructed signals of several methods in a single trial in Figure 5. The main observation is that the neural reconstruction is well-preserved given the behavioral priors. One possible explanation is that the behavioral labels rotate the latent subspace while preserving the necessary information for reconstructing the neural data. The neural signals can be hardly recovered from the behavior labels only. It indicates that the behavior-informed latent does encode significant neural information that is not contained in the behavior labels. Furthermore, we evaluate the disentanglement quality of the latent subspace using the widely-adopted MIG (Mutual Information Gap) metric [Chen et al., 2018], also listed in Table 1. We observe that the learned latent subspace of BeNeDiff significantly enhances disentanglement compared to the vanilla VAE.

  Region & Metrics & SSL & CEBRA & pi-VAE & **Ours** \\   & \(R^{2}(\%)\) & 81.10 \(( 0.26)\) & 79.60 \(( 0.22)\) & 74.37 \(( 0.24)\) & 75.41 \(( 0.24)\) \\  & RMSE \(\) & 32.77 \(( 0.17)\) & 33.07 \(( 0.18)\) & 36.74 \(( 0.22)\) & 35.50 \(( 0.17)\) \\  & MIG\((\%)\) & 37.50 \(( 0.20)\) & 40.12 \(( 0.24)\) & 43.98 \(( 0.29)\) & **55.87**\(( 0.26)\) \\   & \(R^{2}(\%)\) & 76.65 \(( 0.30)\) & 72.63 \(( 0.28)\) & 70.73 \(( 0.23)\) & 69.59 \(( 0.22)\) \\  & RMSE \(\) & 30.64 \(( 0.21)\) & 32.14 \(( 0.17)\) & 35.69 \(( 0.19)\) & 36.91 \(( 0.18)\) \\   & MIG\((\%)\) & 36.89 \(( 0.23)\) & 37.94 \(( 0.23)\) & 42.20 \(( 0.28)\) & **58.56**\(( 0.29)\) \\  

Table 1: **Baseline Comparison** of the neural LVM on two brain regions of Session-1. The boldface denotes the highest score of the MIG metric. Each experiment condition is repeated with 5 runs, and their mean and standard deviations are listed.

Figure 4: **Behavior decoding results of the disentangled neural latent variables of the VIS-Right region**. We observe that the decoding capability of each latent factor is specified to the corresponding behavior of interest, exhibiting a single-mode shape. In contrast, the original neural signals exhibit mixed selectivity to the behaviors, shown in Figure 1(B). Each experiment condition is repeated 5 times, with the mean represented by the bar plot and the standard deviations shown as error bars.

### Neural Dynamics Exploration of Disentangled Latent Factors

From the quantitative experiments in the previous subsection, we obtained information about the decoding and disentanglement quality within the subspace. However, these metrics have limitations in interpreting single-trial neural dynamics, especially the complex temporal structures over time. Here, we visualize the generated videos from BeNeDiff and the baseline latent manipulation methods, demonstrating that BeNeDiff provides interpretable quantifications of the behaviors of interest.

**Latent Manipulation Methods for Comparison.** We compare the neural dynamics exploration performance of BeNeDiff against the following two latent manipulation methods:

\(\)**Naive Latent Manipulation**: the standard manipulation method discussed in Section 3.2.1, which approximates the posterior of behavioral videos given the neural latent trajectories \(p()\), using a neural network that incorporates recurrent units and spatio-temporal convolutional layers.

\(\)**Classifier-free Guidance**[Ho and Salimans, 2022]: a method that approximates the posterior \(p()\) with diffusion models. It co-trains a conditional and an unconditional diffusion model together, combining the resulting conditional and unconditional scores at each diffusion step. In the conditional model, the entire neural latent trajectory \(\) is set as the condition, formulating the denoiser as \(}(_{t},,t)\). For the manipulation of the latent, we keep the activated latent factor \(^{(d)}\) to evolve while setting the values of the other latent factors to those in the first frame of the trial.

**Setup.** To verify the neural dynamics interpretation capability of BeNeDiff, we generate behavioral video data given the activation of each behavior of interest (generated trials with the activation of Jaw and Paw-(y) are shown in Figure 6 and Figure 9, respectively). For visualization and video analysis, we plot frames at intervals of five and compute their frame differences. The conditional module of the classifier-free guidance method is trained with an auxiliary convolutional head. Compared to general video synthesis [Harvey et al., 2022; Esser et al., 2023], our behavioral video data are more focused on maintaining the temporal dynamics and consistency across video frames, thus in BeNeDiff, we tailor the standard 3D U-Net architecture [Cicek et al., 2016] from temporal self-attention layers to temporal convolutions layers [Li et al., 2023b; 2024b] to maintain local temporal consistency. While we keep the spatial self-attention layers the same. The diffusion model is trained on an Nvidia V100, using approximately 20 computer hours.

**Results Analyses of the Generated Videos.** As shown in Figure 6, for the naive latent manipulation method, the distribution of neural signals often falls outside the original distribution after manipulation, resulting in blurred generated frames. The frame differences are entangled, and the "Jaw" latent factor affects the entire head movement of the mouse, particularly in the first four frames shown.

Figure 5: **Neural signal reconstruction performance evaluation of the VIS-Right region**. We observe that the neural reconstruction quality from the latent subspace of BeNeDiff is maintained given the behavioral labels. “Self-Supervised” denotes the VAE w/o behavior labels.

On the other hand, for classifier-free guidance, the generated videos maintain coherent consistency between frames. However, it does not interpret neural dynamics well in this context, resulting in a trajectory with small movements in the "Jaw". This is because the overall latent trajectory is used as the input to the model and the other latent factors are kept fixed, making it difficult to discriminate the evolution of a single factor effectively. In contrast, the results of BeNeDiff show more specificity to the targeted behavior of interest. The inter-frame differences in BeNeDiff's results are clearly specified to the "Jaw" movements, and the structure of the neural dynamics is well-preserved and consistent with ground-truth "Jaw" behavior trajectories. A similar pattern is evident with the other latent factors, as shown in Figures 9, 10, and 11 in the appendix.

### Neural Dynamics Exploration of Disentangled Latent Factors Across Brain Regions

Besides the capability of revealing interpretable neural dynamics of each latent factor associated with behaviors, here we further investigate the neural dynamics differences across brain regions through BeNeDiff. As shown in Figure 7 and Figure 12 in the appendix, we present the 2D neural latent trajectories of two latent factors, specifically related to "Paw-(x)" and "Paw-(y)", across six brain regions for two randomly selected trials. From the starting point of the trial, we observe that the latent trajectories corresponding to the left and right hemispheres of the VIS both show a noticeable change starting earlier. Next, the SSp regions show a large shift in activity, followed by a similar change in the MOs regions. However, it is difficult to clearly visualize the specific motion encoded by each region and to distinguish how different the motions are encoded solely based on neural trajectory plots. This further highlights the need for using a video diffusion model for visualization and interpretation.

In contrast, in the generated behavior video samples of BeNeDiff (as illustrated by the frame differences in Figure 8 and Figure 13 in the appendix), where the "Paw-(x)" and "Paw-(y)" latent factors are activated, the behavioral dynamics encoded by these two latent factors are observed across different brain regions. First, paw movements are detected in the VIS regions before the "Levers" come in. This early activity in VIS could reflect its role in the predictive coding of behaviors, indicating that this region may predict motor movements before they happen. Next, the SSp regions exhibit paw movements that are synchronized with the onset of the "Levers", indicating a potential role

Figure 6: **Generated Single-trial Behavioral Videos with Latent Factor Guidance from the side view**. Compared to baseline methods, we observe that the neural dynamics of latent factor in the results of BeNeDiff show specificity to the “Jaw” movements.

for SSp in processing somatosensory feedback. Subsequently, in the MOs regions, paw movements are observed following the "Levers" onset, which is consistent with MOs' role in motor execution and control, occurring slightly after SSp.

To sum up, although neural trajectory plots provide a clear temporal sequence of activations across regions, it is challenging to directly visualize the specific behavioral dynamics encoded by each region and to discriminate how they differ. This limitation highlights the necessity for a video diffusion model in BeNeDiff, to better visualize and interpret the encoded behavioral dynamics of each neural latent factor. By synthesizing realistic behavior videos in a generative fashion, BeNeDiff enables us to better understand the unique neural dynamics in each brain region and their corresponding behavioral dynamics.

Figure 8: **Generated video frame differences across the right hemisphere regions**. The red dots in the figure indicate paw appearances.

Figure 7: **Learnt Neural Latent Trajectories of BeNeDiff across various brain regions**. It is difficult to clearly visualize the specific motion encoded by each region and to distinguish how different the motions are encoded across brain regions.