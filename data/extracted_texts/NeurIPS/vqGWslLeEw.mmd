# Revisiting the Minimalist Approach to Offline Reinforcement Learning

Denis Tarasov  Vladislav Kurenkov  Alexander Nikulin  Sergey Kolesnikov

Tinkoff

{den.tarasov, v.kurenkov, a.p.nikulin, s.s.kolesnikov}@tinkoff.ai

###### Abstract

Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods in both offline and offline-to-online settings. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity analysis on the scale of thousands of experiments.1

## 1 Introduction

Interest of the reinforcement learning (RL) community in the offline setting has led to a myriad of new algorithms specifically tailored to learning highly performant policies without the ability to interact with an environment (Levine et al., 2020; Prudencio et al., 2022). Yet, similar to the advances in online RL (Engstrom et al., 2020; Henderson et al., 2018), many of those algorithms come with an added complexity - design and implementation choices beyond core algorithmic innovations, requiring a delicate effort in reproduction, hyperparameter tuning, and causal attribution of performance gains.

Indeed, the issue of complexity was already raised in the offline RL community by Fujimoto & Gu (2021); the authors highlighted veiled design and implementation-level adjustments (e.g., different architectures or actor pre-training) and then demonstrated how a simple behavioral cloning regularization added to the TD3 (Fujimoto et al., 2018) constitutes a strong baseline in the offline setting. This minimalistic and uncluttered algorithm, TD3+BC, has become a de-facto standard baseline to be compared against. Indeed, most new algorithms juxtapose against it and claim significant gains over (Akimov et al., 2022; An et al., 2021; Nikulin et al., 2023; Wu et al., 2022; Chen et al., 2022b; Ghasemipour et al., 2022). However, application of newly emerged design and implementation choices to this baseline is still missing.

In this work, we build upon Fujimoto & Gu (2021) line of research and ask: _what is the extent to which newly emerged minor design choices can advance the minimalistic offline RL algorithm?_ The answer is illustrated in Figure 1: we propose an extension to TD3+BC, ReBRAC (Section 3), that simply adds on recently appeared design decisions upon it. We test our algorithm on both proprioceptive andvisual state space problems using D4RL (Fu et al., 2020) and V-D4RL (Lu et al., 2022) benchmarks (Section 4) demonstrating its state-of-the-art performance across ensemble-free methods. Moreover, our approach demonstrates state-of-the-art performance in offline-to-offline setup on D4RL datasets (Section 4.3) while not being specifically designed for this setup. To further highlight the efficacy of the proposed modifications, we then conduct a large-scale ablation study (Section 4.4). We hope the described approach can serve as a strong baseline under different hyperparameter search budgets (Section 4.6), further accentuating the importance of seemingly minor design choices introduced along with core algorithmic innovations.

## 2 Preliminaries

### Offline Reinforcement Learning

A standard Reinforcement Learning problem is defined as a Markov Decision Process (MDP) with the tuple \(\{S,A,P,R,\}\), where \(S^{n}\) is the state space, \(A^{m}\) is the action space, \(P:S A S\) is the transition function, \(R:S A\) is the reward function, and \((0,1)\) is the discount factor. The ultimate objective is to find a policy \((a|s)\) that maximizes the cumulative discounted return \(_{}_{t=0}^{}^{t}R(s_{t},a_{t})\). This policy improves by interacting with the environment, observing states, and taking actions that provide rewards.

In offline RL, policies cannot interact with the environment and can only access a static transaction dataset \(D\) collected by one or more other policies. This setting presents new challenges, such as estimating values for state-action pairs not included in the dataset while exploration is unavailable (Levine et al., 2020).

### Behavior Regularized Actor-Critic

Behavior Regularized Actor-Critic (BRAC) is an offline RL framework introduced in Wu et al. (2019). The core idea behind BRAC is that actor-critic algorithms can be penalized in two ways to solve offline RL tasks: actor penalization and critic penalization. In this framework, the actor objective is represented as in Equation (1), and the critic objective as in Equation (2), where \(F\) is a divergence function between dataset actions and policy actions distributions. The differences from a vanilla actor-critic are highlighted in blue.

\[=*{argmax}_{}_{(s,a) D}[Q_{}(s, (s))- F((s),a)]\] (1)

\[=*{argmin}_{}_{(s,a,r, s^{},}) D\\ a^{}(s^{})}[(Q_{}(s,a)-(r+ (Q_{}(s^{},a^{})- F(a^{},}))))^{2}]\] (2)

In the original work, various choices of \(F\) were evaluated when used as the regularization term for the actor or critic. The authors tested KL divergence, Kernel MMD, and Wasserstein distance but did not observe any consistent advantage. Finally, it is essential to note that, originally, both regularizations coefficients had the same weight.

Figure 1: (a) The schema of our approach ReBRAC (b) Performance profiles (c) Probability of improvement. The curves (Agarwal et al., 2021) are for D4RL benchmark spanning all Gym-MuJoCo, AntMaze, and Adroit datasets (Fu et al., 2020).

Subsequently, TD3+BC (Fujimoto and Gu, 2021) was introduced, utilizing Mean Squared Error (MSE) as the regularization term \(F\) for the actor. TD3+BC is considered to be the minimalist approach to offline RL as it modifies existing RL algorithms by simply adding behavior cloning term into actor loss which is easy to implement and does not bring any significant computational overhead.

## 3 ReBRAC: Distilling Key Design Choices

In this section, we describe the proposed method along with the discussion of the new design choices met in the offline RL literature (Table 1). Our approach is a more general version of BRAC built on top of the TD3+BC (Fujimoto and Gu, 2021) algorithm with different modifications in design while keeping it simple (Figure 1). We refer to our method as **Re**visited **BRAC** (ReBRAC).

Deeper NetworksThe use of deeper neural networks has been a critical factor in the success of many Deep Learning models, with model quality generally increasing as depth increases, provided there is enough data to support this scaling (Kaplan et al., 2020). Similarly, recent studies in RL (Neumann and Gros, 2022; Sinha et al., 2020) and offline RL (Lee et al., 2022; Kumar et al., 2022) have demonstrated the importance of depth in achieving high performance. Although most offline RL algorithms are based on SAC (Haarnoja et al., 2018) or TD3 (Fujimoto et al., 2018), which by default employ two hidden layers, recent work (Kumar et al., 2020) uses three hidden layers for SAC instead, which appears to be an important change (Fujimoto and Gu, 2021). The change in network size has been adopted by later works (An et al., 2021; Yang et al., 2022; Zhuang et al., 2023; Nikulin et al., 2023) and may be one of the critical modifications that improve final performance.

The original BRAC and TD3+BC algorithms used only two hidden layers for their actor and critic networks, while most state-of-the-art solutions use deeper networks. Specifically, three hidden layers have become a common choice for recent offline RL algorithms. In ReBRAC, we follow this trend and use three hidden layers for the actor and critic networks. Additionally, we provide an ablation study in Section 4.5 to investigate the effect of the number of layers on ReBRAC's performance.

LayerNormLayerNorm (Ba et al., 2016) is a widely used technique in deep learning that helps improve network convergence. In Hiraoka et al. (2021), authors add dropout and LayerNorm to different RL algorithms, notably boosting their performance. This technique is also applied in Smith et al. (2022), and it appears that boost is achieved primarily because of LayerNorm. Specifically, in offline RL, various studies have tested the effect of normalizations between layers (Bhatt et al., 2019; Kumar et al., 2022; Nikulin et al., 2022, 2023). A parallel study by Ball et al. (2023) empirically shows that LayerNorm helps to prevent catastrophic value extrapolation for the Q function when using offline datasets in online RL. Following these works, in our approach, we also apply LayerNorm between each layer of the critic networks.

Larger BatchesAnother technique to accelerate neural network convergence is large batch optimization (You et al., 2017, 2019). While studying batch sizes larger than 256 is limited, some prior works have used them. For instance, the convergence of SAC-N was accelerated in Nikulin et al. (2022). More recently proposed algorithms also use larger batches for training, although without providing detailed analyses (Akimov et al., 2022; Nikulin et al., 2023).

The usage of large batches in offline RL is still understudied, and its benefits and limitations are not fully understood. Our experiments show that in some domains, using large batches can lead to significant performance improvements, while in others, it might not have a notable impact or

  
**Modification** & **TD3+BC** & **CQL** & **EDAC** & **MSG** & **CNF** & **LB-SAC** & **SAC-RND** \\  Deeper networks & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ \\ Larger batches & ✗ & ✗ & ✗ & ✗ & ✓ & ✓ \\ Layer Normalization & ✗ & ✗ & ✗ & ✗ & ✓ & ✓ \\ Decoupled penalization & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Adjusted discount factor & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\   

Table 1: Adoption of implementation and design choices beyond core algorithmic advancements in some recently introduced algorithms.

even drop the performance (see Table 8). We increased the batch size to 1024 samples and scaled the learning rate for D4RL Gym-MuJoCo tasks, following the approach proposed by Nikulin et al. (2022).

Actor and critic penalty decouplingThe original BRAC framework proposed penalizing the actor and the critic with the same magnitude. Most of the previous algorithms restrict only actor (Fujimoto and Gu, 2021; Wu et al., 2022) or only critic (Kumar et al., 2020; An et al., 2021; Ghasemipour et al., 2022). TD3-CVAE (Rezaefifar et al., 2022) penalizes both using the same coefficients, while another study, SAC-RND (Nikulin et al., 2023), shows that decoupling the penalization in offline RL is beneficial for algorithm performance, although ablations on using only one of the penalties are missing.

Our method allows simultaneous penalization of actor and critic with decoupled parameters. Inspired by TD3+BC (Fujimoto and Gu, 2021), Mean Squared Error (MSE) is used as a divergence function \(F\), which we found simple and effective. The actor objective is shown in Equation (3), and the critic objective is shown in Equation (4). Differences from the original actor-critic are highlighted in red. Following TD3+BC (Fujimoto and Gu, 2021), the Q function is normalized to make the algorithm less sensitive to regularization parameters. Nonetheless, we forego the utilization of state normalization, as initially suggested in TD3 + BC, driven by our intention to execute the algorithm online and the observation that this adjustment typically results in negligible impact. Since our approach principally builds upon TD3+BC, the differences in their performances should be considered the most important ones and do not appear only because of the additional hyperparameters search.

\[=*{argmax}_{}_{(s,a) D}[Q_{}(s, (s))-_{1}((s)-a)^{2}]\] (3)

\[=*{argmin}_{}_{(s,a,r, s^{},}) D\\ a^{}(s^{})}[(Q_{}(s,a)-(r+ (Q_{}(s^{},a^{})-_{2}(a^{} -})^{2})))^{2}]\] (4)

Discount factor \(\) value changeThe choice of discount factor is an important aspect in solving RL problems (Jiang et al., 2015). A recent study (Hu et al., 2022) suggests that decreasing the default value of \(\) from \(0.99\) may lead to better results in offline RL settings. In contrast, in SPOT (Wu et al., 2022), the authors increased the value of \(\) up to 0.995 when fine-tuning AntMaze tasks with sparse rewards, which resulted in state-of-the-art solutions. Similarly, in the offline setting SAC-RND (Nikulin et al., 2023), increasing \(\) also achieved high performance on the same domain. The choice of increased \(\) for AntMaze tasks was motivated by the sparse reward, i.e., a low \(\) value may not propagate the training signal well. However, further ablations are needed to understand if the change in the parameter is directly responsible for the improved performance. In our experiments, we also find that increasing \(\) from the default value of \(0.99\) to \(0.999\) is vital for improved performance on this set of tasks (see Table 8).

## 4 Experiments

### Evaluation on offline D4RL

We evaluate the proposed approach on three sets of D4RL tasks: Gym-MuJoCo, AntMaze, and Adroit. For each domain, we consider all of the available datasets. We compare our results to several ensemble-free baselines, including TD3+BC (Fujimoto and Gu, 2021), IQL (Kostrikov et al., 2021), CQL (Kumar et al., 2020) and SAC-RND (Nikulin et al., 2023).

The majority of the hyperparameters are adopted from TD3+BC, while \(_{1}\) and \(_{2}\) parameters from Equation (3) and Equation (4) are tuned. We examine the sensitivity to these parameters in Section 4.6. For a complete overview of the experimental setup and details, see Appendix A.

Following Wu et al. (2022), we tune hyperparameters over four seeds (referred to as training seeds) and evaluate the best parameters over ten new seeds (referred to as unseen training seeds), reporting the average performance of the last checkpoints for D4RL tasks. On V-D4RL, we use two and five seeds, respectively. This helps to avoid overfitting during hyperparameters search and outputs more just and reproducible results. For a fair comparison, we tune TD3+BC and IQL following the same protocol. We also rerun SAC-RND on Gym-MuJoCo and AntMaze tasks and tune it for the Adroit

[MISSING_PAGE_FAIL:5]

by An et al. (2021), there has been no notable progress on these tasks. On the other hand, V-D4RL provides a similar set of problems, with datasets collected in the same way as in D4RL but with the agent's observations now being images from the environment.

We tested our algorithm on all available single-task datasets without distractors and compared it to the baselines from the original V-D4RL work (Lu et al., 2022). The results are reported in Table 5. Our proposed approach achieves state-of-the-art or close-to-state-of-the-art results on most of the tasks, and it is the only method that, on average, performs notably better than naive Behavioral Cloning.

### Evaluation on offline-to-online D4RL

The evaluation of offline-to-online performance is a pivotal aspect for reinforcement learning (RL) algorithms, particularly in light of recent developments. In this context, we conducted additional tests on ReBRAC, as it stands out as a promising algorithm for several compelling reasons.

  
**Task Name** & **BC** & **TD3+BC** & **IQL** & **CQL** & **SAC-RND** & **ReBRAC, our** \\  pen-human & 34.4 & 81.8 \(\) 14.9 & 81.5 \(\) 17.5 & 37.5 & 5.6 \(\) 5.8 & **103.5 \(\) 14.1** \\ pen-cloned & 56.9 & 61.4 \(\) 19.3 & 77.2 \(\) 17.7 & 39.2 & 2.5 \(\) 6.1 & **91.8 \(\) 21.7** \\ pen-expert & 85.1 & 146.0 \(\) 7.3 & 133.6 \(\) 16.0 & 107.0 & 45.4 \(\) 22.9 & **154.1 \(\) 5.4** \\  door-human & 0.5 & -0.1 \(\) 0.0 & 3.1 \(\) 2.0 & **9.9** & 0.0 \(\) 0.1 & 0.0 \(\) 0.0 \\ door-cloned & -0.1 & 0.1 \(\) 0.6 & 0.8 \(\) 1.0 & 0.4 & 0.2 \(\) 0.8 & **1.1 \(\) 2.6** \\ door-expert & 34.9 & 84.6 \(\) 44.5 & **105.3**\(\) 2.8 & 101.5 & 73.6 \(\) 26.7 & 104.6 \(\) 2.4 \\  hammer-human & 1.5 & 0.4 \(\) 0.4 & 2.5 \(\) 1.9 & **4.4** & -0.1 \(\) 0.1 & 0.2 \(\) 0.2 \\ hammer-cloned & 0.8 & 0.8 \(\) 0.7 & 1.1 \(\) 0.5 & 2.1 & 0.1 \(\) 0.4 & **6.7 \(\) 3.7** \\ hammer-expert & 125.6 & 117.0 \(\) 30.9 & 129.6 \(\) 0.5 & 86.7 & 24.8 \(\) 39.4 & **133.8 \(\) 0.7** \\  relocate-human & 0.0 & -0.2 \(\) 0.0 & 0.1 \(\) 0.1 & **0.2** & 0.0 \(\) 0.0 & 0.0 \(\) 0.0 \\ relocate-cloned & -0.1 & -0.1 \(\) 0.1 & 0.2 \(\) 0.4 & -0.1 & 0.0 \(\) 0.0 & **0.9 \(\) 1.6** \\ relocate-expert & 101.3 & **107.3**\(\) 1.6 & 106.5 \(\) 2.5 & 95.0 & 3.4 \(\) 4.5 & 106.6 \(\) 3.2 \\  Average w/o expert & 11.7 & 18.0 & 20.8 & 11.7 & 1.0 & **25.5** \\  Average & 36.7 & 49.9 & 53.4 & 40.3 & 12.9 & **58.6** \\   

Table 4: Average normalized score over the final evaluation and ten unseen training seeds on Adroit tasks. BC and CQL scores were taken from Yang et al. (2022). The symbol \(\) represents the standard deviation across the seeds. To make a fair comparison against TD3+BC and IQL, we extensively tuned their hyperparameters.

  
**Environment** & **Offline DV2** & **DrQ+BC** & **CQL** & **BC** & **LOMPO** & **ReBRAC, our** \\   & random & **28.7**\(\) 13.0 & 5.5 \(\) 0.9 & 14.4 \(\)12.4 & 2.0 \(\) 0.2 & 21.9 \(\) 8.1 & 15.9 \(\) 2.3 \\  & mixed & **56.5**\(\) 18.1 & 28.7 \(\) 6.9 & 11.4 \(\)12.4 & 16.5 \(\) 4.3 & 34.7 \(\) 19.7 & 41.6 \(\) 8.0 \\  & medium & 34.1 \(\) 19.7 & 46.8 \(\) 2.3 & 14.8 \(\) 16.1 & 40.9 \(\) 3.1 & 43.4 \(\) 11.1 & **52.5 \(\) 3.2** \\  & medexp & 43.9 \(\) 34.4 & 86.4 \(\) 5.6 & 56.4 \(\) 38.4 & 47.7 \(\) 3.9 & 39.2 \(\) 19.5 & **92.7 \(\) 1.3** \\  & expert & 4.8 \(\) 0.6 & 68.4 \(\) 7.5 & 89.6 \(\) 6.0 & **91.5 \(\) 3.9** & 5.3 \(\) 7.7 & 81.4 \(\) 10.0 \\   & random & **31.7**\(\) 2.7 & 5.8 \(\) 0.6 & 5.9 \(\) 8.4 & 0.0 \(\) 0.0 & 11.4 \(\) 5.1 & 12.9 \(\) 2.2 \\  & mixed & 61.6 \(\) 11.0 & 44.8 \(\) 3.6 & 10.7 \(\) 12.8 & 25.0 \(\) 3.6 & 36.3 \(\) 13.6 & 46.8 \(\) 0.7 \\   & medium & 17.2 \(\) 3.5 & 53.0 \(\) 3.0 & 40.9 \(\) 5.1 & 51.6 \(\) 1.4 & 16.4 \(\) 8.3 & **59.0 \(\) 0.7** \\   & medexp & 10.4 \(\) 3.5 & 50.6 \(\) 8.2 & 20.9 \(\) 5.5 & 57.5 \(\) 6.3 & 11.9 \(\) 1.9 & **58.3 \(\) 11.7** \\   & expert & 10.9 \(\) 3.2 & 34.5 \(\) 8.3 & 61.5 \(\) 4.3 & **67.4 \(\) 6.8** & 14.0 \(\) 3.8 & 35.6 \(\) 5.3 \\   & random & 0.1 \(\) 0.0 & 0.1 \(\) 0.0 & 0.2 \(\) 0.1 & 0.1 \(\) 0.0 & 0.1 \(\) 0.0 \\  & mixed & 0.2 \(\) 0.1 & 15.9 \(\) 3.8 & 0.1 \(\) 0.0 & **18.8 \(\) 4.2** & 0.2 \(\) 0.0 & 16.0 \(\) 2.7 \\   & medium & 0.2 \(\) 0.1 & 6.2 \(\) 2.4 & 0.1 \(\) 0.0 & **13.5 \(\) 4.1** & 0.1 \(\) 0.0 & 0.9 \(\) 2.3 \\   & medexp & 0.1 \(\) 0.0 & 7.0 \(\) 2.3 & 0.1 \(\) 0.0 & **17.2 \(\)** 4.7 & 0.2 \(\) 0.0 & 7.8 \(\) 2.4 \\   & expert & 0.2 \(\) 0.1 & 2.7 \(\) 0.9 & 1.6 \(\) 0.5 & **6.1 \(\)** 3.7 & 0.1 \(\) 0.0 & 2.9 \(\) 0.9 \\   Average & & 20.0 & 30.4 & 21.9 & 30.3 &

[MISSING_PAGE_FAIL:7]

for all datasets can be found in Appendix G. One modification at a time was disabled, while all other changes were retained, including layer normalization in the critic network, additional linear layers in the actor and critic networks, adding an MSE penalty to the critic and actor loss. In the case of AntMaze, we also attempted to use the default \(\) value instead of the increased one. To further demonstrate the efficacy of our modifications, we also ran our implementation as equivalent to the original TD3+BC, with all changes disabled and hyperparameters were taken from the original paper. This experiment serves to show that the improved scores are due to the proposed changes in the algorithm and not just different implementations. Furthermore, we searched for the regularization parameter for our implementation of TD3+BC to demonstrate that tuning this parameter is not the sole source of improvement. Moreover, we tested the TD3 + BC by adding each of the ReBRAC's modifications independently to demonstrate that each individual modifications is not sufficient for achieving performance of ReBRAC where the combination of modification appear.

Additionally, we run ablation which to validate the importance of decoupling by disabling it and searching the best penalty parameter value from the set of all previously used values listed in the Appendix B.

The ablation results show that ReBRAC outperforms TD3+BC not because of different implementations or actor regularization parameter choice. All domains suffer when the LayerNorm is disabled, leading to the halved average performance overall. Removing additional layers leads to a notable drop in AntMaze tasks, while on Gym-MuJoCo decrease is small, and on Adroit tasks, we can see a slight boost. The algorithm fails to learn most tasks when the actor penalty is disabled. Notably, the critic penalty plays a minor role in improving the performance on most of the problems as well as the decoupling penalties. Using standard batch size on Gym-MuJoCo tasks significantly decreases final scores, while using the increased discount factor for AntMaze is crucial for obtaining state-of-the-art performance.

Based on the conducted ablations studies, the proposed configuration of design choices leads to the best performance on average. Note that tuning these choices for each task independently makes it possible to get even higher scores than we report in Section 4.1. We also can change the number of hidden layers in the networks, leading to better performance, see Section 4.5. But in real life, algorithm evaluation might be costly, so we limit ourselves to tuning only regularization parameters.

### Stacking Even More Layers

As ablations show, the depth of the network plays an important role when solving AntMaze and HalfCheetah tasks (see Appendix G). We conduct additional experiments to check how the perfor

  
**Ablation** & Gym-MuJoCo & AntMaze & Adroit & All \\  TD3+BC, paper & - & 27.3 & 0.0 & - \\ TD3+BC, our & 63.4 & 18.5 & 52.3 & 52.2 \\ TD3+BC, tuned & 71.8 (-10.9\%) & 27.9 (-62.9\%) & 53.5 (-25.9\%) & 58.3 (-19.2\%) \\  TD3+BC w/ \(\) change & - & 17.5 (-76.7\%) & - & - \\ TD3+BC w/ LN & 71.4 (-11.4\%) & 35.6 (-52.7\%) & 55.6 (-4.1\%) & 60.2 (-16.6\%) \\ TD3+BC w/ large batch & 14.4 (-82.1\%) & 0.0 (-100.0\%) & 1.6 (-97.2\%) & 7.9 (-89.0\%) \\ TD3+BC w/ layer & 71.2 (-11.6\%) & 44.1 (-41.4\%) & 56.4 (-2.7\%) & 61.9 (-14.2\%) \\  ReBRAC w/o large batch & 75.9 (-5.8\%) & - & - & - \\ ReBRAC w large batch & - & 41.0 (-45.6\%) & 55.4 (-4.6\%) & - \\ ReBRAC w/o \(\) change & - & 21.0 (-72.1\%) & - & - \\ ReBRAC w/o LN & 59.2 (-26.5\%) & 0.0 (-100.0\%) & 25.1 (-56.7\%) & 38.0 (-47.3\%) \\ ReBRAC w/o layer & 78.5 (-2.6\%) & 18.1 (-75.9\%) & 59.0 (+1.7\%) & 61.9 (-14.2\%) \\ ReBRAC w/o actor penalty & 22.8 (-71.7\%) & 0.1 (-99.8\%) & 0.0 (-100.0\%) & 11.4 (-84.2\%) \\ ReBRAC w/o critic penalty & 81.1 (+0.6\%) & 72.2 (-4.1\%) & 56.9 (-1.8\%) & 71.5 (-0.9\%) \\ ReBRAC w/o decoupling & 79.8 (-0.9\%) & 76.9 (+2.1\%) & 56.7 (-2.2\%) & 71.6 (-0.8\%) \\  ReBRAC & 80.6 & 75.3 & 58.0 & 72.2 \\   

Table 8: ReBRAC’s design choices ablations: each modification was disabled while keeping all the others. For brevity, we report the mean of average normalized scores over four unseen training seeds across domains. We also include tuned results for TD3+BC to highlight that the improvement does not come from hyperparameter search. For dataset-specific results, please refer to Appendix G.

mance depends on the network depth in more detail. Ball et al. (2023) used networks of depth four when solving AntMaze tasks. Our goal is to find the point where the performance saturates. For this, we run our algorithm on AntMaze tasks increasing the number of layers to six while keeping other parameters unchanged. We attempt to increase actor and critic separately and at once. We also decreased each network's size by one layer similarly. Results can be found in Figure 2.

Several conclusions can be drawn from the results. First, further increments in the number of layers can lead to better results on the AntMaze domain when layers are scaled up to five. For six layers, performance drops or does not improve. Second, decreasing the critic's size leads to the worst performance on most datasets. Lastly, there is no clear pattern on how the performance changes even within a single domain. The drop on six layers is the only common feature that can be seen. On average, four critic layers and three actor layers were the best. Changing only the actor's network is more stable on average.

### Penalization Sensitivity Analysis

Following Kurenkov and Kolesnikov (2022), we demonstrate the sensitivity of ReBRAC to the choice of \(_{1}\) and \(_{2}\) hyperparameters under uniform policy selection on D4RL tasks using Expected Online Performance (EOP) and comparing it to the TD3+BC and IQL. EOP shows the best performance expected depending on the number of policies that can be deployed for online evaluation. Results are demonstrated in Table 9. As one can see, approximately ten policies are required for ReBRAC to attain ensemble-free state-of-the-art performance. ReBRAC's EOP is higher for any number of online policies when compared to TD3+BC and better than IQL on all domains when the evaluation budget is larger than two policies. See Appendix F for EOP separated by tasks.

  
**Domain** & **Algorithm** & **1 policy** & **2 policies** & **3 policies** & **5 policies** & **10 policies** & **15 policies** & **20 policies** \\   & **TD3+BC** & \(49.8 21.4\) & \(61.0 14.5\) & \(65.3 9.3\) & \(67.8 3.9\) & - & - & - \\  & **IQL** & **65.0 \( 9.1\)** & \(69.9 5.6\) & \(71.7 3.5\) & \(72.9 1.7\) & \(73.6 0.8\) & \(73.8 0.7\) & \(74.0 0.6\) \\  & **ReBRAC** & \(62.0 17.1\) & \(70.6 9.9\) & \(73.3 5.5\) & \(74.8 2.1\) & \(75.6 0.8\) & \(75.8 0.6\) & \(76.0 0.5\) \\   & **TD3+BC** & \(6.9 7.0\) & \(10.7 6.8\) & \(13.0 6.0\) & \(15.5 4.6\) &  &  &  &  &  \\  & **IQL** & \(29.8 15.5\) & \(38.0 15.4\) & \(43.1 13.8\) & \(48.7 10.2\) & \(51.2 4.4\) & \(54.3 2.1\) & \(54.7 1.2\) \\   & **ReBRAC** & \(\) & \( & \( & \(\) & \(\) & \(\) & - \\   & **TD3+BC** & \(23.6 19.9\) & \(346.6 17.7\) & \(40.6 14.5\) & \(46.4 9.8\) & - & - & - \\   & **IQL** & \(\) & \(\) & \(53.7 0.5\) & \(53.9 0.3\) & \(54.1 0.2\) & \(54.2 0.2\) & \(54.2 0.1\) \\   & **ReBRAC** & \(44.1 18.4\) & \(53.2 10.9\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 9: Expected Online Performance (Kurenkov and Kolesnikov, 2022) under uniform policy selection aggregated over D4RL domains across four training seeds. This demonstrates the sensitivity to the choice of hyperparameters given a certain budget for online evaluation. For dataset-specific results, please see Appendix F.

Figure 2: Impact of networks’ depth on the final performance for the AntMaze tasks. Scores are averaged over four unseen training seeds. Shaded areas represent one standard deviation across seeds. These graphics demonstrate that one can achieve marginally better scores by tuning the number of layers for certain tasks.

Related Work

**Ensemble-free offline RL methods.** In recent years, many offline reinforcement learning algorithms were developed. TD3+BC (Fujimoto and Gu, 2021) represents a minimalist approach to offline RL, which incorporates a Behavioral Cloning component into the actor loss, enabling online actor-critic algorithms to operate in an offline setting. CQL (Kumar et al., 2020) drives the critic network to assign lower values to out-of-distribution state-action pairs and higher values to in-distribution pairs. IQL (Kostrikov et al., 2021) proposes a method for learning a policy without sampling out-of-distribution actions.

Despite this, more sophisticated methods may be necessary to achieve state-of-the-art results in an ensemble-free setup. For instance, Chen et al. (2022); Akimov et al. (2022) pre-train different forms of encoders for actions, then optimize the actor to predict actions in the latent space. SPOT (Wu et al., 2022) pre-trains Variational Autoencoder and uses its uncertainty to penalize actor for sampling OOD actions while SAC-RND (Nikulin et al., 2023) applies Random Network Distillation and penalizes actor and critic.

**Ensemble-based offline RL methods.** A significant number of works in offline reinforcement learning have also leveraged ensemble methods for uncertainty estimation. The recently introduced SAC-N (An et al., 2021) algorithm outperformed all previous approaches on D4RL Gym-MuJoCo tasks; however, it necessitated large ensembles for some tasks, such as the hopper task, which required an ensemble size of 500 and imposed a significant computational burden. To mitigate this, the EDAC algorithm was introduced in the same work, which utilized ensemble diversification to reduce the ensemble size from 500 to 50. Despite the reduction, the ensemble size remains substantial compared to ensemble-free alternatives. It is worth mentioning that neither SAC-N nor EDAC is capable of solving the complex AntMaze tasks (Tarasov et al., 2022).

Another state-of-the-art algorithm in the Gym-MuJoCo tasks is RORL (Yang et al., 2022), which is a modification of SAC-N that makes the Q function more robust and smooth by perturbing state-action pairs with the use of out-of-distribution actions. RORL also requires an ensemble size of up to 20. On the other hand, MSG (Ghasemipour et al., 2022) utilizes independent targets for each ensemble member and achieves good performance on the Gym-MuJoCo tasks with an ensemble size of four but requires 64 ensemble members to achieve state-of-the-art performance on the AntMaze tasks.

**Design choices ablations.** Ablations of different design choices on established baselines are very limited, especially in offline RL. It is only shown by Fujimoto and Gu (2021) that CQL performs poorly if proposed non-algorithmic differences are eliminated. A parallel study (Ball et al., 2023) shows that some of the considered modifications (LayerNorm and networks depth) are important when used in online RL with offline data setting, which is different from pure offline.

## 6 Conclusion, Limitations, and Future work

In this work, we revisit recent advancements in the offline RL field over the last two years and incorporate a modest set of improvements to a previously established minimalistic TD3+BC baseline. Our experiments demonstrate that despite these limited updates, we can achieve more than competitive results on offline and offline-to-online D4RL and offline V-D4RL benchmarks under different hyperparameter budgets.

Despite the noteworthy results, our work is limited to one approach and a subset of possible design changes. It is imperative to explore the potential impact of these modifications on other offline RL methods (e.g., IQL, CQL, MSG) and to investigate other design choices used in offline RL, e.g., learning rate schedules, dropout (like in IQL), wider networks, or selection between stochastic and deterministic policies.