# Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IscoreScore (TS2)

Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IscoreScore (TS2)

 Michael Saxon\({}^{}\)Fatima Jahara\({}^{}\)Mahsa Khoshnoodi\({}^{}\)

Yujie Lu\({}^{}\)Aditya Sharma\({}^{}\)William Yang Wang\({}^{}\)

\({}^{}\)University of California, Santa BarbaraRutgers University

\({}^{}\)Fatima Al-Fihri Predoctoral Fellowship

\({}^{}\)_Equal contribution Contact:_saxon@ucsb.edu

T2IscoreScore.github.io

###### Abstract

With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their _prompt faithfulness_--the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines.

We introduce T2IscoreScore, a curated set of _semantic error graphs_ containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. T52 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.

Figure 1: Overview of T2IscoreScore. T2I evaluation metrics are scored based on their ability to correctly organize images in a _semantic error graph_ (SEG) relative to their generating prompt, checking ordering (Spearman’s \(\)) and separation of nodes (Kolmogorov–Smirnov statistic).

Introduction

Text-to-image (T2I) models are improving at a breakneck pace in terms of quality, fidelity, and coherence of generated images to their conditioning prompts [1; 2; 3; 4]. Despite this, persistent challenges in achieving image-prompt faithfulness [5; 6] remain--particularly in freely available models that don't sit behind proprietary APIs. Indeed, many techniques to improve T2I models have been proposed of late, aiming to reduce hallucination [7; 8], duplication , composition errors [8; 10], and missing objects [11; 12]. However, there is no consensus on how to best compare these many models and methods, so it is hard to objectively track T2I progress [13; 14].

Recent work has proposed a litany of automated _image-prompt coherence metrics_ which rate the _faithfulness_ of generated images: the degree to which a they satisfy the implicit requirements set forth in the generating prompt [15; 16; 17; 18]. These proposed metrics vary considerably in design; as rating how well an image matches to its prompt is a nontrivial multimodal challenge [14; 19; 20].

This variety itself presents a _meta-evaluation problem_: there is no **consensus on how these faithfulness metrics ought to be compared**, and consequently each new metric is validated on its own ad-hoc test set against prior baselines. Typically these _self-evaluations_ consist of a set of prompt-image pairs with accompanying human annotations (usually simple Likert scores [16; 19]), and metrics are judged on their correlation to these human judgements .

Such self-evaluation is not ideal; authors may unwittingly tilt the scales by using evaluation examples which cater to the particular strengths of their proposed method, and variance of metric performance between different evaluation sets (containing different images and prompt semantics [21; 22]) is high . Additionally, relying on correlation to human judgements of small sets of images across different prompts is highly subjective [24; 25] and prone to including judgements of quality and style that are orthogonal to prompt coherence. We need a consistent and objective meta-evaluation.

To this end we propose T2IscoreScore(TS2), a _benchmark and set of meta-metrics_ for evaluating T2I faithfulness metrics. While it contains a similar number of images to previously proposed coherence metric evaluation sets, it contains fewer prompts. This high image-to-prompt ratio allows us to organize the images along _semantic error graphs_, or SEGs (fig. 1), where each edge corresponds to a specific error with respect to the prompt that a child image set possesses but its parent images do not. These semantic error graphs permit objective scoring of a metric by answering:

1. Can a metric correctly **order** increasingly wrong images against their generating prompt?
2. Can a metric reliably **separate** sets of images that differ by a specific semantic error?
3. Does the metric **confidently** separate the image sets within its dynamic range?

We adapt existing statistical tests [26; 27] to the SEG setting to answer these questions for a broad set of T2I faithfulness metrics. We find some surprising results: despite their inferior performance in correlating to human preferences against complicated vision-language model (VLM)-based metrics [6; 16; 17; 18], simple embedding-correlation methods like CLIPCcore  are actually quite performant on our meta-metrics, and **Pareto-optimal** with respect to compute cost (SS5). In summary, we:

* Formalize the task of objectively assessing T2I prompt coherence metrics by their ability to correctly order and separate image populations within semantic error graphs (SEGs). (SS2)
* Present T2IScoreScore(TS2), our evaluation for this task: a carefully-curated benchmark dataset of SEGs each containing between 4 and 76 images, permitting 93,000 total pairwise image comparisons and meta-metrics for ordering and separation in SEGs. (SS2, SS3)
* Evaluate a broad and representative set of T2I faithfulness benchmarks using T52, demonstrate that it identifies novel failure cases, and motivate future work on improved metrics. (SS4 SS5, SS6)

### Related Work

Most evaluations in the T2I space test the quality of generating models based on _a fixed faithfulness metric's scores over a fixed benchmark set of prompts._ Often these reference prompt sets are designed for testing a single specific capability. DrawBench , T2I-CompBench , and ABC-6k  focus on attributes like compositionality, cardinality, and spatial relations, in strictly text-guided image generation, while ImagenHub  tests them in a broader set of settings like image editing and subject-driven synthesis. Other evaluation dimensions such as multilinguality [29; 30] and stereotype bias  have also been explored. These prompts are usually sourced from some combination of existing natural image captioning resources [32; 33; 34] and sets of in-the-wild conditioning prompts produced by real users [35; 36]. These benchmarks assess image quality either by direct human analysis or automated metrics  including those we analyze in this work. Often the goal of these benchmarks is to analyze how well a model generates images that support with human preferences [14; 37], and directly elicit opinions from users through a web interface for this purpose.

To meta-evaluate faithfulness metrics _a benchmark containing a fixed set of images and prompts is required_. However, all existing benchmarks for this purpose suffer from two key limitations:

1. Evaluating on noisy _human preference_ rather than _explicitly labeled objective differences_
2. _Low image-to-prompt ratio_ limiting evaluation of discriminatory power over similar images

C captioning benchmarks [32; 33; 34] are poor candidates for faithfulness metric evaluation as single images are paired with multiple prompt candidates rather than vice versa. Image matching and entailment benchmarks such as SeeTRUE  and Pick-a-Pic  are also limited by a low ratio.

The few extant deliberately-designed faithfulness evaluation sets are limited by both factors. TIFA v1.0  and DSG-1k  were proposed _ad-hoc_ to demonstrate the utility of their accompanying metrics by relating the scores assigned by the metric to human preferences. These are done over small sets of images (800 & 1000 respectively) with slightly more images-per-prompt (5 & 1).

The two limitations of these prior evals are linked. A reliance on human preference correlations is a natural consequence of having few and poorly organized images to compare to each prompt. A lack of meta-metrics designed for evaluating structured aspects other than human preference correlation means limited utility in collecting larger, structured sets of images for each prompt. By providing both _meta-metrics **and** a structured eval set_**T2IscoreScore overcomes these limitations (table 1).

## 2 T2IScoreScore meta-metrics

We introduce three measures of ordering and separation by a given metric within semantic error graphs (SEGs). We define SEG \(S\) as prompt \(P\) and a directed acyclic graph of nodes \(n_{i}\) containing one or more images \(I_{j}\) sharing the same errors wrt. the prompt. We label each node by its error count and type (eg, [0, 1a, 1b] has 1 node with 0 errors, and 2 nodes with 1 error each of different type).

A good prompt coherence metric will correctly rank images along each walk of increasing error counts within a SEG, and separate the scores assigned to images in successive nodes. Our metrics assess this by evaluating each walk separately. For ease of notation, we refer to each SEG as a set of walks \(W S\) over nodes of increasing error count (eg, (0, 1a, 2a), (0, 1a, 2b), etc), where each walk is the in-order set of all (image, prompt, num. error) triples \((I,P,N) W\). For example, the first walk in fig. 1 is [(0-0.jpg, \(P\), 0), (0-1.jpg, \(P\), 0), (1-0.jpg, \(P\), 1),(2-0.jpg, \(P\), 2),...].

We introduce measures of metric \(m\) for SEG \(S\): \(_{m}(S)\), \(_{m}(S)\)\(\&\)\(_{m}(S)\), assessed over every walk \(W S\) in all SEGs in the T52 dataset to score a metric. They're defined as:

    &  &  &  &  \\
**Dataset** & **Total** & **T2I-Gen.** & **Avg** & **Per equiv. pref** & **Total** & **T2I Errors** & \\   \\ Flickr8k  & 8k & 0 & 0.2 & 0.2 & 8k & 0 & – \\ Flickr30k  & 31k & 0 & 0.2 & 0.2 & 31k & 0 & – \\ MSCOCO Captions  & 330k & 0 & 0.22 & 0.22 & 330k & 0 & – \\  \\ SeeTRUE  & 31k & 0 & 1 & 31k & 0 & ✓ \\ Pick-a-Pic  & 500k & 500k & 21 & 1 & 7M & 0 & ✓ \\  \\ TIFA v1.0  & 800 & 800 & 5 & 1 & 4k & 0 & ✓ \\ DSG-1k  & 1k & 1k & 1 & 1 & 1k & 0 & ✓ \\ T2IScoreScore & 2.8k & 2690 & **17** & **3.4** & _93k_ & **3.1k** & – \\   

Table 1: Comparison of benchmark datasets that can be used to evaluate T2I faithfulness. _Per equiv pref_ means the average number of images for each prompt that are assigned the same preference or correctness score. **Bold** numbers are best overall, _italic_ are best of the T2I metric benchmarks.

### Ordering score over walks: \(_{m}\)

We use Spearman's rank correlation coefficient \(\) between image-level error count and metric-assigned score over every walk on a SEG to assess how a metric's faithfulness score aligns to our objective structure error counts. Spearman's \(\) is the PCC of the rank order of variables \(X,Y\):

\[(X,Y)=(R(X),R(Y))}{_{R(X)}_{R(Y)}};  R(X)=_{x_{i} X}(x_{i}<x) x_{i} X}\] (1)

Thus, in our case the SEG-level rank order score \(_{m}(S)\) for scoring model \(m\) is defined as:

\[_{m}(S)=_{W S}r_{s}(\{m(I,P)|(I,P,N) W\}, \{N|(I,P,N) W\})\] (2)

One limitation of Spearman's \(\) for characterizing scores is that it is undefined if one set \(R(U)\) exclusively contains identical elements, as \(_{R(U)}=0\). For tractability in these scenarios we define \((,R(U)):=0\). If a metric assigns identical scores to all examples across different error levels, it presents no discernible relationship between error severity and score for that image set.

### Statistical separation of error populations score: \(_{m}\)

We assess the two-sample Kolmogorov-Smirnov statistic  pairwise between the populations of metric \(m\)'s scores assigned to each sample between two error nodes \(n_{i}\) and \(n_{j}\) as populations. The Kolmogorov-Smirnov statistic is a non-parametric measure of the separation between two distributions [39; 40], defined as the maximum vertical difference between their empirical cumulative distribution functions \(F_{X}(s)\):

\[D_{KS}(X,Y)=_{x R_{m}}|F_{X}(x)-F_{Y}(x)|\] (3)

Where \(F_{X}(x)\) is proportion of samples in population \(X\) for which the metric-assigned score \(m(i) x\), (see fig. 8 for a visual depiction). We compute \(D_{KS}\) for every pair of adjacent error nodes in each tree walk \(W^{2}\), and report the average over all of these as the SEG separation score \(_{m}(S)\):

\[_{m}(S)=_{n_{i} S}D_{KS}(\{m(P,I)|(P,I) n_ {i}\},\{m(P,I)|(P,I) n_{i+1}\})\] (4)

### Separation of nodes within dynamic range: \(_{m}\)

While \(_{m}\) nonparametrically estimates whether pairs of nodes are drawn from different distributions (and thereby distinguished), it provides no information about the distance by which they are separated within the metric's dynamic range. This measure gives an alternative look at separation between nodes: the more separation a metric provides between nodes, the less severe slight variations in assigned score will be to ignoring errors in generated images. We assess it as:

\[_{m}(S)=}_{N_{i} W} (\{m(P,I)|(P,I) N_{i}\})-(\{m(P,I)|(P, I) N_{i+1}\})\] (5)

Where \(_{m( S)}\) is the standard deviation of scores from metric \(m\) on all images in all SEGs in \(\). Our score is the average distance between the mean metric score of all adjacent nodes in all SEGs, rescaled by the standard deviation of the score to normalize against the metric's dynamic range.

## 3 The \(\) Dataset

We now turn to describing the \(\) dataset collection process. We use three different semantic error graph collection procedures to produce a diverse set of SEGs. Each contains one prompt and between 4 and 76 images assigned to error nodes. Each node usually contains more than one image, though for simplicity in presentation we only show one image assigned to each node in this section.

### Dataset Collection Procedure

Figure 2 depicts the three procedures by which we produce and populate SEGs with images: **synthetic** images from a synthetic graph (_Synth_), graph from **nat**ural images (_Nat_), and graph from **real** errors of synthetic images (_Real_), differentiated by prompt source, image source, and the order of production.

Synth.Synthetic SEGs are produced "graph first." From an initial prompt we list all entities and properties it contains, then ablate them to produce an error graph. We then manually write prompts describing each node, generate their images, and manually check image-node faithfulness. For example, in the left panel of fig. 2, the initial prompt "a Christmas tree with lights and a teddy bear" is converted to error prompts such as "a Christmas tree with lights."

Nat.The natural error trees exclusively contain real images sourced from the free stock image repository Pexels. We generate SEGs in "image, graph, prompt" order. We source sets of natural images that share objects and models. We organize them by relation graphs describing how the images differ by objects, actions, attributes, and composition. We then select a head node in this relation graph and write a "prompt" describing this head node (eg., fig. 2 center panel). We produced SEGs of natural images to assess whether distributional differences between synthetic images and real images might lead to measurable impacts on performance for the faithfulness metrics that typically use base models pretrained exclusively on natural images .

Real.The real error, synthetic image SEGs are produced following a "prompt, image, graph" order. From a seed prompt (both manually written and sourced from COCO or PartiPrompts) we simply generate a large set of images using a T2I model, then annotate the errors in each generated image. These error-labeled images are then organized into a final error graph for the SEG. This procedure is documented in the right panel of fig. 2.

### Dataset structure, size, and validity

Each of the 165 SEGs in T52 is manually checked by three human annotators. The head node of each SEG contains at least one image that has been assessed by the annotators to contain no errors of verbal information (eg, entity in the image isn't performing the described action), compositionality (eg, object described as "on top" but is beneath object), missing objects, or incorrect object attributes.

Each edge on the SEG represents an error of one of the aforementioned types. Each node is labeled with the number of edges along its shortest path back to node 0, representing its error count (fig. 1, fig. 8). Each node contains at least one image which is erroneous according to the described errors.

Figure 2: The three _semantic error graph_ production procedures. **Synth.** (images generated from multiple prompts written to populate a SEG), **Nat.** (natural images populate a SEG), and **Real** (real errors from image generation attempts from one prompt populate a SEG).

The synthetic images are generated with several T2I models--including DALL-E 2 , Stable Diffusion 1.5 , 2.0, 2.1, and SDXL . We use MS-COCO , PartiPrompt , and manually written prompts in head nodes for the Synth and Real subsets. Image sources, prompt sources, and error type statistics are documented in fig. 3.

The total number of images and prompts is roughly in line with previous benchmarks that have been used to verify methods such as TIFA  and DSG  in their own papers, and permits significantly more image comparisons-per-prompt than any prior benchmark (Table 1), which we submit is the primary type of comparison required to verify that prompt-faithfulness assessments work.

## 4 Experiments

Using TS2 we evaluated three classes of T2I evaluation metrics: _embedding-correlation_ (comparing embeddings of prompt and image), _QG/A_ (using VQA to check if requirement questions generated from the prompt are satisfied) and _caption-based_ (comparing captions extracted from the generated images to the prompt). We evaluate these metrics with multiple backend VLMs (SSA.2).

For each metric, we score every image against its SEG's prompt. We report the results of our Ordering and Separation metrics across all SEGs, as well as for our the Synth, Nat, and Real SEG subsets.

### Embedding-correlation Metrics

**CLIPScore** is a popular prompt faithfulness metric based on simple text-image similarity. CLIPScore is computed as the cosine similarity of the \(L_{2}\)-normalized CLIP-assessed  image and text embeddings. Equations and details in SSA.1.

**ALIGNScore** (not to be confused with the text-only _AlignScore_) is a variant embedding-based similarity score we produced using the ALIGN  embedding model rather than CLIP to embed the prompt and image. Other than model it is equivalent to CLIPScore.

### Question Generation & Answering (QG/A) Metrics

_Question Generation & Answering Metrics_ use an LM \(_{QG}\) to produce a set of requirement question/answer pairs \((q,a) Q\) from prompt \(p\), and then use a vision-language model \(_{VL}\) to check each requirements against the image, reporting satisfaction rate as the image's faithfulness score. QG/A metrics vary by how questions are generated and relate to each other. Equations in SSA.1.

**TIFA** prompts an LM (GPT-3) to generate a set of multiple choice and yes-no questions and their expected answers relative to the prompt. Then a vision language model \(_{VL}\) produces "free-form" answers to each question, which are converted into multiple choice answers \(a^{}\) using an SBERT model. The TIFA score for a given image is then the rate of correct answers.

Figure 3: Overview of the distribution of sample types in TS2: (a) Where source images came from: 5% of images in the benchmark are real photographs from Pexels, while the remainder were generated by Stable Diffusion (SD) or DALL-E variants. (b) Source of the eliciting prompt; either existing resources or us (Manual). (c) Distribution of error types edges in all SEGs.

[MISSING_PAGE_FAIL:7]

## 6 Discussion

The headline takeaway from our findings is that, contrary to claims of superiority leveled in their own papers, for all the QG/A and caption-comparison metrics [6; 16; 17; 18], _the cheap embedding-correlation metrics such as CLIPScore are sufficient or even preferable_ at capturing objective semantic errors relative to fixed prompts. We view this capacity to accurately discriminate similar images relative to a prompt as the core feature a good prompt faithfulness metric must possess.

T2IScoreScoreis effectively evaluating T2I metrics as relative _score regressors_--functions that are predicting a specific score for an image. However, there are additional desirable elements to a Human aesthetic preferences are--by design--ignored in T52 meta-evaluation. Though the QG/A and caption-comparison metrics fail to meaningfully outperform the cheap embedding-correlation metrics, they may have advantages that are not captured by T52, such as in modeling human aesthetic preferences. Paired with a standalone benchmark of human aesthetic preferences over error-free images, a metric's error assessment and aesthetic fidelity could be measured independently.

Figure 4: Plots of ordering and separation scores against estimated per-image metric evaluation costs in FLOPs and each other. For all analyses, the Pareto optimal metrics are DSG and TIFA with GPT-4, and the vastly less expensive embedding-correlation ALIGNScore and CLIPScore.

As the first objective evaluation of faithfulness metrics based on structural semantic errors, T52 enables more fine-grained measurement of metric desiderata, leading researchers to build better metrics, and empowering developers to make trade off-informed metric choices.

### Pareto frontiers with compute cost

Why do we care about compute cost? When evaluating T2I models at release, compute costs are not very important--an evaluation only has to be run on a small set of images from benchmark prompts.

However, during training or in online monitoring, compute cost for faithfulness metrics becomes quite important. Faithfulness metrics could be used as reward signals while training a T2I model (or prompt generator), or called repeatedly during validation passes. Faithfulness metrics could be deployed in applications to guide an online prompt refinement system, to trigger a second call in user-facing applications, or to analyze prompt corpora to surface challenging examples for further training or analysis. In all of these settings, a performant, low-cost model such as CLIPScore is valuable. T52 demonstrates that the performance premium for the expensive metrics is quite small.

### Considering error graphs enables objective evaluation

Previous evaluations of coherence metrics have evaluated metrics as human preference score regressors over single images, or over image pairs. The challenge with such an evaluation is that _human preferences are not objective_--especially when provided by a small pool of annotators, correlation to such scores is an unclear signal.

However, by instead evaluating walks over error counts in SEGs, the T52 captures a more objective notion of correctness, by ignoring the subjective relationships between pairs of unconnected nodes.

For example, consider the SEG presented in Figure 1, where two different single-error nodes are shown. Given the prompt "a bot in a green shirt poses with some fruit," one of these nodes contains images without fruit, and the other contains boys wearing a blue shirt, rather than green. Which of these types of images are actually worse with respect to the prompt? This is a subjective decision--some annotators may find the missing fruits more important than the incorrectly colored shirt. T52 ignores this distinction, as no nodes of equivalent error counts are connected in any SEG. While the difference between those nodes is subjective, the difference between both of these nodes and their shared child node--one where fruits are missing **and** the shirt is incorrectly colored--is objective.

### Human baselines and metric ignorance of ranking task

It is also important to note that metrics under test are not aware of the implicit ranking task in T52, as they are evaluated as score regressors. Objective human annotation was only possible _because the annotators were aware that relative ranking was the goal_.

If human performance were judged on the task of simple Likert scoring of image-prompt accuracy without instructions, humans may not significantly outperform the metrics. However, if the human annotators were instructed to count the number of errors, we suspect they would perform quite well, even without the other images for comparison over which the ranking task is performed.

Though we provide no human baseline, we do not think this is a significant weakness--human performance on the inherently synthetic task of image quality scoring is not as important as performance on ranking along objective errors.

### Systematic advantages for some metrics

One disadvantage of using Spearman's \(\) is that it "expects" ties to be the same in both distributions. For example, if a set of images has error count \((0,1,1,2)\), the ordering \((1,0.5,0.5,0)\) will have a perfect \(=1\), while the ordering \((1,0.51,0.49,0)\) will be penalized, despite it also presenting a correct ordering. This means that **our Ordering score rank\({}_{m}\) systematically punishes the embedding-based metrics relative to the VLM-based ones**, as the embedding-correlation metrics CLIPScore and ALIGNScore can take continuous values, while TIFA, DSG, LLMScore, and VIEScore have a discrete range. In light of this systematic disadvantage for embedding-correlation metrics, it is even more striking that CLIP/ALIGNScore still are so performant and on the optimality frontier.

### Near-perfect performance on \(_{m}\) and \(_{m}\) possible

Although the scores of many models on our metric are high, this meta-evaluation is far from "solved." In principle it should be possible to get much closer to 100 on average for both meta-metrics than we find. We view use of TS2 as a necessary secondary evaluation for any new proposed T2I faithfulness metric; if it has high correlation to subjective human judgements but does not perform well on T2IScoreScore, skepticism might be warranted.

### Impact of future VLM advances

Ultimately, all image coherence evaluation metrics stand to improve from further advances in general VLM quality. As a considerably more performant model than LLAVA, mPLUG, etc, it was unsurprising that GPT4-V worked much better as a backbone for TIFA and DSG than the aforementioned. However, there do appear to be diminishing returns, as the order of magnitude going from mPLUG- to GPT4-V-based evaluation yielded a sub-1% improvement in \(_{m}\) performance on the most difficult and construct-valid Real set. Better constraint-generating processes may be required to push VLM-based evaluation metrics further.

## 7 Conclusion

We introduced T2IScoreScore, a first-of-its kind objective evaluation for text-to-image faithfulness metrics that utilizes a high image-to-prompt ratio to organize its reference images along semantic error graphs, through which a faithfulness metric can be assessed by our novel graph-based meta-metrics. Our study reveals a surprising finding: more expensive and recent "state of the art" VLM-metrics actually only have modest gains in performance over simpler and cheaper embedding-based metrics at best. Indeed, these cheap metrics such as CLIPScore and ALIGNScore are actually Pareto optimal along with the vastly more expensive and slightly more performant GPT-4V-based QG/A metrics, even when strictly comparing ordering and separation capabilities (leaving compute cost aside).

This underscores the necessity for a more nuanced approach to benchmarking and developing metrics capable of capturing the subtle semantic nuances between prompts and generated images. The establishment of T2IScoreScore as a benchmarking tool is a significant step forward, offering a structured way to rigorously test and improve T2I prompt faithfulness metrics, ensuring they can more accurately reflect the semantic coherence between prompts and generated images, thereby facilitating the development of more reliable and effective T2I models.

## Limitations, ethical considerations, and impact

Limitations to our work are discussed throughout. For example, in SS6.4 we discuss how our meta-metrics are limited by intrinsic biases of rank-correlation metrics among ties (many of which occur when multiple images occupy one node on a SEG). Additionally, compared to other evaluation sets, our total number of prompts is modest (this is required to achieve a high image-to-prompt ratio, however, which is a core strength of our work). Finally, due to its secretive nature, we are only able to produce _rough estimates of the compute cost of GPT-3 and GPT-4 based metrics_. We estimate them to the best of our ability using third-party information (SA.3).

This research will steer the development of more effective faithfulness metrics, which in turn will guide T2I model development. T2I models are inherently dual-use: they can be used to produce misinformation and other harmful content in addition to useful and entertaining imagery. Any work that contributes to improving their overall performance necessarily drives a small amount of both positive and deleterious impact in this way.

### Contribution Statement

MS checked SEGs, designed the benchmark and meta-metrics, implemented the SEG tree iteration process and evaluation code for the ordering and separation scores, collated the QG/A answers into ID-level scores, and assessed the final scores.

FJ produced and annotated the Synth SEGs, produced and annotated a subset of the Real SEGs, and checked all others. FJ collected answers for Fuyu for the QG/A metrics and cleaned and organized the final dataset release.

MK produced the Nat SEGs and produced, annotated, and checked the other SEGs. MK generated the TIFA and DSG questions for all prompts, implemented and evaluated CLIPScore and ALIGNScore, collected answers for the QG/A metrics from BLIP, InstructBLIP, GPT-4V and refactored code.

YL evaluated LLMScore for the examples and conceived of measuring faithfulness errors in T2I faithfulness metrics. AS collected answers for the QG/A metrics from LLaVA and VIEScore.