# Aligning Synthetic Medical Images with

Clinical Knowledge using Human Feedback

 Shenghuan Sun

University of California, San Francisco

shenghuan.sun@ucsf.edu

&Gregory M. Goldgof

Memorial Sloan Kettering Cancer Center

goldgofg@mskcc.org

&Atul Butte

University of California, San Francisco

atul.butte@ucsf.edu

&Ahmed M. Alaa

UC Berkeley and UCSF

amalaa@berkeley.edu

###### Abstract

Generative models capable of capturing nuanced clinical features in medical images hold great promise for facilitating clinical data sharing, enhancing rare disease datasets, and efficiently synthesizing annotated medical images at scale. Despite their potential, assessing the quality of synthetic medical images remains a challenge. While modern generative models can synthesize visually-realistic medical images, the clinical validity of these images may be called into question. Domain-agnostic scores, such as FID score, precision, and recall, cannot incorporate clinical knowledge and are, therefore, not suitable for assessing clinical sensibility. Additionally, there are numerous unpredictable ways in which generative models may fail to synthesize clinically plausible images, making it challenging to anticipate potential failures and manually design scores for their detection. To address these challenges, this paper introduces a _pathologist-in-the-loop_ framework for generating clinically-plausible synthetic medical images. Starting with a diffusion model pretrained using real images, our framework comprises three steps: (1) evaluating the generated images by expert pathologists to assess whether they satisfy clinical desiderata, (2) training a reward model that predicts the pathologist feedback on new samples, and (3) incorporating expert knowledge into the diffusion model by using the reward model to inform a finetuning objective. We show that human feedback significantly improves the quality of synthetic images in terms of fidelity, diversity, utility in downstream applications, and plausibility as evaluated by experts. We also show that human feedback can teach the model new clinical concepts not annotated in the original training data. Our results demonstrate the value of incorporating human feedback in clinical applications where generative models may struggle to capture extensive domain knowledge from raw data alone.

## 1 Introduction

Diffusion models have recently shown incredible success in the conditional generation of high-fideltiy natural, stylized and artistic images . The generative capabilities of these models can be leveraged to create synthetic data in application domains where obtaining large-scale annotated datasets is challenging. The medical imaging field is one such domain, where there is often a difficulty in obtaining high-quality labeled datasets . This difficulty may stem from the regulatory hurdles that impede data sharing , the costs involved in getting experts to manually annotate images , or the natural scarcity of data in rare diseases . Generative (diffusion) models may provide a partial solution to these problems by synthesizing high-fidelity medical images that can be easily shared among researchers to replace or augment real data in downstream modeling applications .

**What sets medical image synthesis apart from image generation in other fields?** Unlike mainstream generative modeling applications that prioritize visually realistic or statistically expressive images, synthetic medical images require a different approach. They must be grounded in objective clinical and biological knowledge, and as such, they leave no room for creative or unrestricted generation. Given that the ultimate goal of synthetic medical images is to be used in downstream modeling and analysis, they must faithfully reflect nuanced features that represent various clinical concepts, such as cell types , disease subtypes , and anatomies . Off-the-shelf image generation models are not capable of recognizing or generating clinical concepts, rendering them unsuitable for generating plausible medical images without further adaptation . Therefore, our aim is to develop a framework for generating synthetic medical images that not only exhibit visual realism but also demonstrate biological plausibility and _alignment_ with clinical expertise.

One way to generate synthetic medical images is to finetune a pretrained "foundation" vision model, such as Stable Diffusion, that has been trained on billions of natural images (such as the LAION-5B dataset ), using real medical images. With a sufficiently large set of medical images, we can expect the finetuned model to capture the clinical knowledge encoded in medical images. However, the sample sizes of annotated medical images are typically limited to a few thousand. When a large vision model is finetuned on such a dataset using generic objective functions (such as the likelihood function), the model may capture only the generic features that make the medical images appear visually realistic, but it may miss nuanced features that make them biologically plausible and compliant with clinical domain knowledge (see examples in the next Section). Designing domain-specific objective functions for finetuning that ensure a generative model adheres to clinical knowledge is challenging. The difficulty arises from the numerous unpredictable ways in which these models can generate images that lack clinical plausibility. As a result, it is impractical to anticipate every possible failure scenario and manually construct a loss function that penalizes such instances.**

**Summary of contributions.** In this paper, we develop a pathologist-in-the-loop framework for synthesizing medical images that align with clinical knowledge. Our framework is motivated by the success of reinforcement learning with human feedback (RLHF) in aligning the outputs of large language models (LLMs) with human preference , and is directly inspired by , where human feedback was used to align the visual outputs of a generative model with input text prompts. To generate clinically-plausible medical images, our framework (outlined in Figure 1) comprises 3 steps:

_Step 1:_ We train a (conditional) diffusion model using real medical images. We then sample a synthetic dataset from the model to be evaluated by a pathologist. Each image is carefully examined, and the pathologist provides feedback on whether it meets the necessary criteria for clinical plausibility.

Figure 1: **Overview of our pathologist-in-the-loop synthetic data generation framework. (1)** In Step 1, a synthetic dataset is sampled from a generative model pretrained using a dataset of real medical images. The dataset is then inspected by a pathologist who examines each image to determine its plausibility based on a set of criteria. For each image, the pathologist provides binary feedback, labeling a synthetic image as “1” if it fails to meet all of the plausibility criteria. **(2)** In Step 2, the synthetic images and pathologist feedback obtained in Step 1 are used to train a reward model that predicts human feedback on new images. **(3)** Finally, the generative model is finetuned via an objective function that uses the reward model to incentivize the generation of clinically plausible images.

_Step 2:_ We collate a dataset of synthetic images paired with pathologist feedback and train a reward model to predict the pathologist feedback, i.e., clinical plausibility, on new images.

_Step 3:_ Finally, the reward model in Step 2 is utilized to incorporate expert knowledge into the generative model. This is achieved by finetuning the diffusion model using a reward-weighted loss function, which penalizes the generation of images that the pathologist considers clinically implausible.

Throughout this paper, we apply the steps above to the synthetic generation of bone marrow image patches, but the same conceptual framework can generalize to any medical imaging modality. We gathered pathologist feedback on thousands of synthetic images of various cell types generated by a conditional diffusion model. Then, we analyzed the impact of this feedback on the quality of the finetuned synthetic images. Our findings suggest that incorporating pathologist feedback significantly enhances the quality of synthetic images in terms of all existing quality metrics such as fidelity, accuracy of downstream predictive models, and clinical plausibility as evaluated by experts. Additionally, it also improves qualities that are not directly addressed in the pathologist evaluation, such as the diversity of synthetic samples. Furthermore, we show that human feedback can teach the generative model new clinical concepts, such as more refined identification of cell types, that are not annotated in the original training data. These results demonstrate the value of incorporating human feedback in clinical applications where generative models may not be readily suited to capture intricate and extensive clinical domain knowledge from raw data alone.

## 2 Pathologist-in-the-Loop Generation of Synthetic Medical Images

In this Section, we provide a detailed description of our synthetic medical image generation framework. We will use a running example pertaining to single-cell images extracted from bone marrow aspirate whole slide images. Details of this setup and the dataset used in our study are provided in Section 4.

### Step 1: Pathologist feedback collection

**Generative model pretraining.** The first step in our framework starts with training a generative model to synthesize medical images through the standard training procedure. As we discuss in more detail in Section 4, we utilized a dataset of 2,048 bone marrow image patches to train a conditional diffusion model . The model was trained to generate class-conditional images, where an image class corresponds to a cell type. We conducted an exploratory analysis where we found that neither latent diffusion nor Stable Diffusion models yielded superior results compared to a customized diffusion model that we employed in this study (See Appendix A). We opted for using a class-conditional model rather than a text-conditional model as we found that existing pretrained vision-language models were not fit for capturing the scientific jargon related to bone marrow cell types.

Given a dataset of real images \(_{r}=\{(x^{i},c^{i})\}_{i=1}^{n_{x}}\), where \(x\) is a medical image and \(c\) is an image class, we train a diffusion model to generate class-conditional images through the forward process:

\[x_{t+1}=x_{t}-_{t}_{x}(p_{}(x_{t}|x,c))+_{ t},\] (1)

where \(_{t}(0,^{2})\) is the noise term at time-step \(t\), \(x_{t}\) is the data point at time-step \(t\), \(_{t}\) is the step size at \(t\), \(\) is the model parameters and \(_{x}(p(x_{t}|x,c))\) is the gradient of the log probability distribution with respect to \(x\), conditioned on the original data \(x\) and class \(c\). Once the model is pretrained, we sample a synthetic dataset \(_{s}=\{(^{j},^{j})\}_{j=1}^{n_{s}}\) by first sampling a class \(\) from \(\), and then sampling a medical image \(\) conditioned on the class \(\) through the reverse diffusion process.

Figure 2: Samples for biologically-implausible synthetic bone marrow image patches across four different cell types. On the bottom panels, we show pathologist evaluations detailing the reasons for their implausibility.

**Pathologist evaluation.** Each image in the synthetic dataset \(_{s}=\{(^{j},^{j})\}_{j=1}^{n_{s}}\) generated by the pre-trained model is inspected by an expert pathologist to assess its clinical plausibility. The objective of this evaluation is to identify the specific inaccuracies in the synthetic data that can only be identified by an expert, and provide feedback for the model to refine its synthesized images in the finetuning step.

When a model is trained with only a modest number of real image samples, it may generate bone marrow image patches that look visually appealing but are not biologically plausible. In Figure 2, we present 8 synthetic images sampled from the conditional diffusion model, which correspond to four different cell types. Each of these images achieves high precision and fidelity scores individually, but they also have biological implausibilities such as inaccurate cell coloring or nucleus shapes. Therefore, models that prioritize visual features without considering biological knowledge may miss important clinical features required for synthetic images to be useful for downstream analysis. Generic evaluation scores (e.g., [21; 22]) cannot diagnose these failures because they also lack biological domain knowledge. By incorporating feedback from pathologists, we can refine the generative model by identifying biological information that is missed by the pretrained model.

The expert pathologist examined each synthetic image and provided a feedback score on its biological plausibility. The evaluation typically involved inspecting the image and checking 7 aspects that contribute to its perceived plausibility (Table 1). These aspects pertain to the consistency of the shapes, sizes, patterns and colors of the contents of a synthetic bone marrow image \(\) with the cell type \(\).

Among the determinants of plausibility is the cell size--different cells have different sizes, e.g., Lymphocytes are generally smaller than Monocytes or Neutrophphils. Nucleus shape and size also depend on the cell type, e.g., Band Neutrophphils have a horseshoe-shaped nucleus, whereas Segmented Neutrophphils have a multi-lobed nucleus. Chromatin patterns within the nucleus are dense and clumped in Lymphocytes, while in Myeloid cells they are diffuse and fine. The number, size and color of granules also contribute to plausibility. Detailed explanation of all criteria is provided in the Appendix.

Note that we have full control over the number of synthetic images \(n_{s}\), i.e., we can sample an arbitrary number of synthetic images from the conditional diffusion model. The key limiting factor on \(n_{s}\) is the time-consuming nature of the feedback collection process. To enable scalable feedback collection, we limited our study to binary feedback, i.e., the pathologist flagged a synthetic image as "implausible" if they found a violation of any of the criteria in Table 1 upon visual inspection. We collected these binary signals and did not pursue a full checklist on all plausibility criteria for each synthetic image.

The output of Step 1 is an annotated dataset \(_{s}=\{(^{j},^{j},^{j})\}_{j =1}^{n_{s}}\), where \(^{j}\{0,1\}\) is the pathologist feedback on the \(j\)-th synthetic image, where \(^{j}=1\) means that the image is implausible.

### Step 2: Clinical plausibility reward modeling

We conceptualize the pathologist as a "labelling function" \(:\{0,1\}\) that maps the observed synthetic image \(\) and declared cell type (class) \(\) to a binary plausibility score. In Step 2, we model the "pathologist" by learning their labelling function \(\) on the basis of their feedback annotations.

To train a model \(\) that estimates the pathologist labelling function, we construct a training dataset that comprises a mixture of real and synthetic images as follows:

**Synthetic images**

We construct a dataset \(_{s}^{}=\{(^{i},^{i}, ^{j})\}_{j=1}^{n_{s}}\) comprising the synthetic images and corresponding pathologist feedback collected in Step 1.

We combine both datasets \(^{}=_{r}^{}_{s}^{}\) to construct a training dataset for the model \(\). The real dataset \(_{r}^{}\) is built by randomly permuting the image class and assigning an implausibility label of 1 if the permuted class does not coincide with the true class. We use the real dataset to augment the synthetic dataset with the annotated pathologist feedback. By augmenting the datasets \(_{r}^{}\) and \(_{s}^{}\), we teach the model \(\) to recognize two forms of implausibility in image generation: (i) instances where the synthetic image looks clinically plausible but belongs to a wrong cell type (i.e., training examples

    \\  (1) Cell size & (5) Chromatin pattern \\ (2) Nucleus shape and size & (6) Inclusions \\ (3) Nucleus-to-cytoplasm ratio & (7) Granules \\ (4) Cytoplasm color and consistency \\   

Table 1: Pathologist evaluation criteria.

in \(_{r}^{}\)), and (ii) instances where the synthetic image is visually consistent with the correct cell type but fails to meet some of the plausibility criteria in Table 1 (i.e., subset of the training examples in \(_{s}^{}\)). We call the resulting model \(\) a clinical plausibility _reward_ model. Using the augmented feedback dataset \(^{}\), we train the reward model by minimizing the mean square error as follows:

\[L_{}()=_{j_{r}^{}}(^{j}-_ {}(^{j},^{j}))^{2}+_{r}_{i_{r}^{}}(y^{i}-_{}(x^{i},^{i}))^{2},\] (2)

where \(_{r}\) is a hyper-parameter that controls the contribution of real images in training the reward functions, and \(\) is the parameter of the reward model.

### Step 3: Clinically-informed finetuning

In the final step, we refine the diffusion model by leveraging the pathologist feedback. Specifically, we incorporate domain knowledge into the model by utilizing the reward model \(\) in the finetuning objective. Following , we use a _reward-weighted_ negative log-likelihood (NLL) objective, i.e.,

\[L(,)=_{(,) _{s}}[-_{}(,)(p_{ }(|))]+_{r}_{(x,c) _{r}}[-(p_{}(x|c))],\] (3)

to finetune the conditional diffusion model, where \(_{r}\) is a hyper-parameter and \(\) is the reward model parameters obtained by minimizing (2) in Step 2. The finetuning objective in (3) incorporates the pathologist knowledge through the reward model, which predicts the pathologist evaluation of the synthetic images that the model generates as it updates its parameters \(\). The reward-weighted objective penalizes the generation of images that do not align with the pathologist preferences, hence we expect that the finetuned model will be less likely to generate clinically implausible synthetic images.

### Bonus step: Feedback-driven generation of new clinical concepts

Besides refining generative models for clinical plausibility, pathologist feedback can be used to incorporate novel clinical concepts into the generative process that were not initially labeled in the real dataset. This could allow generative models to continuously adapt in changing clinical environments. For instance, in our bone marrow image generation setup, pathologist feedback can refine image generation by introducing new sub-types of the original cell types in \(\), as illustrated in Figure 3.

Instead of collecting pathologist feedback that is limited to clinical plausibility, we also collect their annotation of new cell sub-types (e.g., segmented and band variants of Neutrophil cell types). Next, we train an auxiliary model \(_{}(x)\) with parameter \(\) to classify the new sub-types based on the pathologist annotations. Finally, we finetune the conditional diffusion model through a combined loss function, i.e., \(L(,)+L(,)\), that incorporates the two forms of pathologist feedback, i.e., annotations of new cell types and clinical plausibility.

### Pathologist feedback vs. Automated feedback

To asses the added value of human feedback, we consider a baseline where the generative model is supplemented with automatically generated feedback on clinical plausibility. To this end, we implement a baseline based on classifier-guided diffusion , where a classifier serves as an automatic feedback signal that deems a synthetic image implausible if it does not match the corresponding cell type. For this baseline, we train an auxiliary classifier to predict the cell type \(c\) of an image \(x\), and then we incorporate the gradient of the log-likelihood of this classifier in the training objective as described in  (See the Supplementary material for implementation details).

## 3 Related Work

Before delving into the experimental results, we discuss three strands of literature that are related to our synthetic data generation framework. These include: generative modeling for synthetic clinical data, evaluation of generative models and learning from human feedback.

Figure 3: Generation of refined cell sub-types.

**Generative modeling of synthetic medical images.** The dominant approach for synthesizing medical images is to train or finetune a generative model, such as a Variational Autoencoder (VAE) , a Generative Adversarial Network (GAN) , or a diffusion model [6; 23], using a sufficiently large sample of images from the desired modality. Owing to their recent success in achieving state-of-the-art results in high-fidelity image synthesis , diffusion probabilistic models have become the model of choice for medical image synthesis applications [26; 27; 28; 29; 12]. In , the Stable Diffusion model--an open-source pretrained diffusion model--was used to generate synthetic X-ray images, and in  it was shown that diffusion models can synthesize high-quality Magnetic Resonance Images (MRI) and Computed Tomography (CT) images.  used latent diffusion models to generate synthetic images from high-resolution 3D brain images. All of these models are trained with the standard likelihood objective and the synthetic images are typically evaluated through downstream classification tasks or generic, domain-agnostic metrics for image fidelity. To the best of our knowledge, none of the previous studies have explored a human-AI collaboration approach to synthetic image generation or incorporated clinical knowledge into generative models of medical images.

**Evaluation of synthetic images in the medical domain and beyond.** Unlike discriminative modeling (i.e., predictive modeling) where model accuracy can be straightforwardly evaluated by comparing the model predictions with ground-truth labels in a testing set, evaluating the quality of generative models can be quite challenging since we do not have a "ground-truth" for defining what makes a synthetic sample is of high or low quality. Devising a generic score to evaluate a generative model can be tricky since there are many potential modes of failure . Consequently, it is essential to design robust multidimensional scores that capture the most relevant failure modes for a given application.

Recently, there have been various attempts at defining domain-specific scores [30; 31; 32] as well as generic scores for evaluating the quality of synthetic images. Examples include the FID score which is based on a distributional distance between real and synthetic images . Other examples for sample-level evaluation metrics include the precision and recall metrics  which check if synthetic data resides in the support of the real data distribution. However, these scores do not encode clinical domain knowledge, which is critical for identifying failures in generating clinically meaningful images. Traditional scores of medical image quality include signal- and contrast-to-noise ratio [34; 35; 36], mean structural similarity . These scores are typically applied to real images and cannot be repurposed to judge the generative capacity of a synthetic data model in a meaningful way. The lack of an automated score for detecting clinically implausible synthetic medical images is a key motivation for our work. We believe that the most reliable way to assess the quality of a synthetic image is to have it evaluated by an expert pathologist. From this perspective, the reward model \(\) in Section 2.2 can be thought of as a data-driven score for image quality trained using pathologist evaluations.

**Learning from human feedback.** The success of many modern generative models can be attributed in part to finetuning using feedback solicited from human annotators. The utilization of human feedback in model finetuning is very common in natural language processing applications, particularly in finetuning of large language models (LLMs). Examples for applications were human feedback was applied include translation , web question-answering  and instruction tuning [40; 41; 42]. The key idea in these applications is that by asking a human annotator to rate different responses from the same model, one can use such annotations to finetune the model to align with human preference. Similar ideas have been applied to align computer vision models with human preferences [43; 44; 20; 45; 46]. In the context of our application, the goal is to align the outputs of a generative model with the preferences of pathologists, which are naturally aligned with clinical domain knowledge. Our finetuning objective builds on the recent work in  and , which use human feedback to align text-prompts with generated images using a reward-weighted likelihood score.

## 4 Experiments

In this Section, we conduct a series of experiments to evaluate the utility of pathologist feedback in improving the quality of synthetic medical images. In the next Subsection, we start by providing a detailed description of the single-cell bone marrow image dataset used in our experiments.

### Bone marrow cells dataset

In all experiments, we used a dataset of hematopathologist consensus-annotated single-cell images extracted from bone marrow aspirate (BMA) whole slide images. The images were obtained from theclinical archives of an academic medical center. The dataset comprised 2,048 images, with the images evenly distributed across 16 morphological classes (cell types), with 160 images per class (Table 2). These classes encompass varied cell types found in a standard bone marrow differential. The dataset covers the complete maturation spectrum of Erythroid and Neutrophil cells, from Proerythroblast to mature Erythrocyte and from Myeloid blast to mature Neutroph, respectively. The dataset also differentiates mature Eosinophils with segmented nuclei from immature Eosinophils and features Monocytes, Basophils, and Mast cells. Bone marrow cell counting and differentiating between various cell types is a complex task that poses challenges even for experienced hematologists. Hence, we expect pathologist feedback to significantly improve the quality of bone marrow image synthesis.

### Synthetic data generation and pathologist feedback collection

We used a conditional diffusion model trained on real images (64\(\)64 pixels in size) to generate synthetic image patches. Training was conducted using 128 images per cell type, with 32 images per cell type held out for testing and evaluating all performance metrics. For the reward model \((x,c)\), we used a ResNeXt-50 model  pre-trained on a cell type classification task to obtain embeddings for individual images, and then concatenated the embeddings with one-hot encoded identifiers of image class (cell type) as inputs to a feed-forward neural network that predicts clinical plausibility. Further details on model architectures and selected hyper-parameters are provided in the Appendix.

We collected **feedback** from an expert pathologist on **3,936 synthetic images** generated from the diffusion model. The pathologist identified most of these images as implausible--the rate of implausible images was as high as 85% for some cell types (e.g., Basophil cells, see Table 2). After training the reward model using pathologist feedback, we finetune the diffusion model as described in Section 2.

### Results

**Expert evaluation of synthetic data quality.** To evaluate the impact of pathologist feedback on the generated synthetic data, we created two synthetic datasets: a sample from the diffusion model (before finetuning with pathologist feedback) and a sample from the finetuned version of the model after incorporating the pathologist feedback. Each dataset comprised 400 images (25 images per cell type). An expert pathologist was asked to evaluate the two samples and label each image as plausible or implausible (in a manner similar to the feedback collection process).

Table 3 lists the fraction of clinically plausible images per cell type for the two synthetic datasets (before and after finetuning using the pathologist feedback) as evaluated by an expert hematopathologist. As we can see, the pathologist feedback leads to a significant boost in the quality of synthetic images across all cell types, increasing the average rate of clinical plausibility from 0.21 to 0.75. Note that in this experiment, the human evaluator emulates the reward function \((x,c)\). Hence, the improved performance of the finetuned model indicates success in learning the pathologist preferences.

**Evaluating synthetic data using fidelity & diversity scores.** In addition to expert evaluation, we also evaluated the two synthetic datasets generated in the previous experiment using standard metrics for evaluating generative models. We considered the Precision, Recall and Coverage metrics [21; 22].

   } &  &  &  &  \\  & & & Training & Testing & & Plausible & Implausible \\  Mast Cell & B1 & 128 & 32 & 213 & 72 & 141 \\ Basophil & B2 & 128 & 32 & 214 & 29 & 185 \\ Immutze Eosinophil & E1 & 128 & 32 & 213 & 49 & 164 \\ Mature Eosinophil & E4 & 128 & 32 & 224 & 53 & 171 \\ Pronormoblast & ER1 & 128 & 32 & 256 & 97 & 159 \\ Basophilic Normoblast & ER2 & 128 & 32 & 256 & 49 & 207 \\ Polychromatophilic Normoblast & ER3 & 128 & 32 & 256 & 129 & 127 \\ Orthochromic Normoblast & ER4 & 128 & 32 & 256 & 118 & 138 \\ Polychromatophilic Erythrocyte & ER5 & 128 & 32 & 256 & 141 & 115 \\ Mature Erythrocyte & ER6 & 128 & 32 & 256 & 120 & 136 \\ Myeloid Blast & M1 & 128 & 32 & 256 & 138 & 118 \\ Promylocyte & M2 & 128 & 32 & 256 & 107 & 149 \\ Myelocyte & M3 & 128 & 32 & 256 & 131 & 125 \\ Metamyelocyte & M4 & 128 & 32 & 256 & 88 & 168 \\ Mature Neutrophil & M5 & 128 & 32 & 256 & 80 & 176 \\ Monocyte & M02 & 128 & 32 & 256 & 83 & 173 \\   

Table 2: Breakdown of bone marrow image patches by morphological cell type and pathologist feedback.

Precision measures the fraction of synthetic samples that resides in the support of the real data distribution and is used to measure fidelity. Recall and Coverage measure the "diversity" of synthetic samples, i.e., the fraction of real images that are represented in the output of a generative model. In addition to the two synthetic datasets (with and without feedback) generated in the previous experiment, we also evaluated a third synthetic dataset finetuned using the automatic feedback (classifier-guidance) approach described in Section 2.5. The results in Table 4 show that pathologist feedback improves the quality of synthetic data compared to the two baselines across all metrics under consideration. Interestingly, we see that feedback not improves fidelity of synthetic images, but it also improves the diversity of samples, which was not one the criteria considered in the pathologist feedback.

**Downstream modeling with synthetic data.** Morphology-based classification of cells is a key step in the diagnosis of hematologic malignancies. We evaluated the utility of synthetic medical images in training a cell-type classification model. In this experiment, we train a ResNext-50 model to classify the 16 cell types using real data, synthetic data from the pretrained model with no feedback, and synthetic data from the model finetuned with pathologist feedback. To ensure a fair comparison, we created synthetic datasets consisting of 128 images per cell type, matching the size of the real data. The classification accuracy of all models was then tested on the held-out real dataset, containing 32 images per cell type. Results are shown in Table 5. Unsurprisingly, the model trained on real data demonstrated superior performance across all accuracy metrics, exhibiting a significant gap compared to the model trained on synthetic data without human feedback. However, incorporating pathologist feedback helped narrow this gap and improved the quality of synthetic data to the point where the resulting classifiers only slightly underperformed compared to the one trained on real data.

To evaluate the marginal value of human feedback, we also considered two ablated versions of our synthetic data generation process. These included a synthetic dataset generated using automatic feedback (Section 2.5) as well as a reward model trained using the pathologist feedback on synthetic data only (i.e., dataset \(_{*}^{}\)) without real data augmentation (Section 2.2). The results in Table 5 show that automatic feedback only marginally improves performance, which aligns with the results in Table 4. Real data augmentation (i.e., finetuning on \(_{*}^{}+_{r}^{}\)) slightly improves classification accuracy, but most of the performance gains are achieved by finetuning on the pathologist-labeled dataset \(_{*}^{}\).

**Impact of the number of feedback points.** How much human feedback is necessary to align the generative model with the preferences of pathologists? In Figure 4, we analyze the effect of different amounts of pathologist-labeled synthetic images on training the reward model. We explore four scenarios: 0%, 10%, 50%, and 100% of the 3,936 synthetic images labeled by the pathologist. For each scenario, we fine-tune the pretrained model using a reward model trained with the corresponding fraction of synthetic images. We then repeat the cell classification experiment to evaluate the accuracy of the classifiers trained using synthetic data generated in these four scenarios. Figure 4(a) shows that as the number of pathologist-labeled synthetic images increases, all accuracy metrics increase to improve over the pretrained model performance and become closer to the accuracy of training on real data. Additionally, we see that even a modest amount of feedback (e.g., 10% of pathologist-labeled synthetic images) can have a significant impact on performance. Figure 4(b) also show the qualitative improvement in the quality of synthetic images as the amount of human feedback increases.

**Incorporating new clinical concepts using pathologist feedback.** Finally, we evaluate the feedback-driven generation approach outlined in Section 2.4. Here, our goal is not only to leverage pathologist

 
**Training data** & **Precision** & **Recall** & **Coverage** \\  Synthetic (no feedback) & 68.06 & 52.00 & 56.98 \\ Synthetic (auto. feedback) & 74.80 & 43.90 & 61.33 \\ Synthetic (with feedback) & **81.01** & **56.74** & **84.57** \\  

Table 4: Evaluation of synthetic data using fidelity and diversity metrics.

   &  \\   & _No feedback_ & _Path. feedback_ \\  B1 & 0.40 & 0.92 \\ B2 & 0.16 & 1.00 \\ E1 & 0.12 & 0.80 \\ E4 & 0.24 & 0.52 \\ ER1 & 0.44 & 0.96 \\ ER2 & 0.28 & 0.64 \\ ER3 & 0.24 & 0.68 \\ ER4 & 0.20 & 0.84 \\ ER5 & 0.20 & 0.76 \\ ER6 & 0.20 & 0.96 \\ M1 & 0.20 & 0.84 \\ M2 & 0.20 & 0.64 \\ M3 & 0.08 & 0.56 \\ M4 & 0.20 & 0.84 \\ M5 & 0.08 & 0.68 \\ MO2 & 0.12 & 0.40 \\  

Table 3: Expert evaluation of synthetic data. Table 5: Accuracy of classifiers trained on real and synthetic data.

feedback for rating the plausibility of synthetic images, but also to harness their expertise in providing additional annotations that can enable the conditional diffusion model to generate more refined categories of bone marrow image patches, e.g., abnormal cell types that develop from preexisting normal cell types. Hence, refining the generative model to synthesize new cell subtypes can help continuously update the model to capture new pathological cells and build downstream diagnostic models.

In this experiment, we focus on finetuning the model to distinguish between band Neutroph and segmented Neutrophils (Figure 5(c)). These two sub-categories are often lumped together in bone marrow cell typing, as was the case in our dataset (see Table 2). However, in many clinical settings, differentiating between the two subtypes is essential. For instance, a high percentage of band Neutrophils can indicate an acute infection, inflammation, or other pathological conditions. We collected pathologist annotations of band and segmented Neutrophils, and trained a subtype classifier to augment the plausibility reward as described in Section 2.4. The finetuned model was able to condition on the new classes and generate plausible samples of the two Neutrophil subtypes (Figure 5(c)), while retaining the classification accuracy with respect to the new classes (See Appendix for detailed results).

## Conclusions

Synthetic data generation holds great potential for facilitating the sharing of clinical data and enriching rare diseases datasets. However, existing generative models and evaluation metrics lack the ability to incorporate clinical knowledge. Consequently, they often fall short in producing clinically plausible and valuable images. This paper introduces a pathologist-in-the-loop framework for generating clinically plausible synthetic medical images. Our framework involves finetuning a pretrained generative model through feedback provided by pathologists, thereby aligning the synthetic data generation process with clinical expertise. Through the evaluation of synthetic bone marrow patches by expert hematopathologists, leveraging thousands of feedback points, we demonstrate that human input significantly enhances the quality of synthetic images. These results underscore the importance of incorporating human feedback in clinical applications, particularly when generative models encounter challenges in capturing nuanced domain knowledge solely from raw data.

Figure 4: **Quantitative and qualitative impact of pathologist feedback on synthetic images.** (a) Accuracy of cell-types classifiers trained on synthetic data with varying amounts of pathologist feedback. (b) Visual inspection of random synthetic samples from diffusion models finetuned with with varying amounts of pathologist feedback. (c) Feedback-driven conditional generation of new (segmented and band) subtypes of Neutrophil cells.