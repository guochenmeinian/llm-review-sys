# MedCalc-Bench: Evaluating Large Language

Models for Medical Calculations

 Nikhil Khandekar

Equal contribution. \({}^{}\)Correspondence: zhiyong.lu@nih.gov.

Qiao Jin

Guangzhi Xiong

Soren Dunn

Serina S Applebaum

Howard University College of Medicine, 9University of Chicago, 2000.

Aidan Glison

Howard University College of Medicine, 9University of Chicago, 2000.

Maxwell B Singer

Howard University College of Medicine, 9University of Chicago, 2000.

Amisha Dave

Howard University College of Medicine, 9University of Chicago, 2000.

Andrew Taylor

Howard University College of Medicine, 9University of Chicago, 2000.

Aidong Zhang

Unity of Chicago, 2000.

Qingyu Chen

Howard University College of Medicine, 9University of Chicago, 2000.

Zhiyong Lu

Howard University College of Medicine, 9University of Chicago, 2000.

###### Abstract

Current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks. 1

## 1 Introduction

Large language models (LLMs) such as GPT [2; 31], Gemini/PaLM [1; 47], and Llama [50; 51] have been successfully applied to a variety of biomedical tasks [29; 38; 48; 49], including, but not limited to question answering [27; 30; 42], clinical trial matching [23; 57; 58; 62], and medical document summarization [40; 46; 52]. However, most of these tasks have a limited evaluation of domain knowledge and qualitative reasoning ability of LLMs, as demonstrated by the commonly used medical benchmarks such as MedQA , PubMedQA , and MedMCQA . Whilequantitative tools such as medical calculators are frequently used in clinical settings , currently there is no benchmark evaluating the medical calculation capabilities of LLMs.

Medical calculators are statistical tools derived from high-quality clinical studies, serving various purposes, including metric conversions , disease diagnosis , and prognosis prediction . Figure 1 shows two examples of medical calculators. To accurately compute requested medical scores, the model needs to have three non-trivial capabilities: (1) possessing the knowledge of the rules or equations for the medical calculation task, (2) identifying and extracting the values of the relevant parameters within a long patient note, and (3) conducting the arithmetic computation for the task correctly.

In this work, we propose MedCalc-Bench, a first-of-its-kind dataset for evaluating the medical calculation capabilities of LLMs. To construct MedCalc-Bench, we first curated 55 common medical calculation tasks from MDCalc2. Then, we compiled Open-Patients, a collection of over 180k publicly available patient notes, and identified the notes that can be used for each calculation task. Finally, we collected over 1k instances for MedCalc-Bench, where each instance contains: (1) a patient note, (2) a question requesting to compute a specific medical value, (3) a ground truth answer, and (4) a step-by-step explanation of the computation process.

Using MedCalc-Bench, we conducted systematic evaluations of various LLMs, including the state-of-the-art proprietary models such as GPT-4 , open-source LLMs such as Llama  and Mixtral , as well as biomedical domain-specific PMC-LLaMA  and MediTron. Our experimental results show that most of the tested models struggle in the task. GPT-4 achieved the best baseline performance of only 50.9% accuracy using one-shot chain-of-thought prompting. By analyzing the types of errors made by LLMs, we found that the models suffer mostly from insufficient medical calculator knowledge in the zero-shot setting. To mitigate this issue, we add a one-shot exemplar in the prompt, showing the model how to apply the requested medical equations or rules. Our analysis revealed additional issues in extracting calculator-related attributes and in arithmetic computations. These results can provide insights into future improvement in the medical calculation capabilities of LLMs.

In summary, the contributions of our study are threefold:

* We manually curated MedCalc-Bench, a novel dataset of over 1k instances for evaluating the capabilities of LLMs across 55 different medical calculation tasks.

Figure 1: Example instances of the MedCalc-Bench dataset.

* We conducted comprehensive evaluations on MedCalc-Bench with various open and closed-source LLMs. Our results show that all current LLMs are not yet ready for medical calculations, with the best accuracy of only 50.9% achieved by GPT-4.
* Our error analysis reveals the insufficiency of calculator knowledge in LLMs, as well as their deficiencies in attribute extraction and arithmetic computation for medical calculation.

## 2 MedCalc-Bench

### Calculation Task Curation

MedCalc-Bench covers 55 different calculators. These were all listed as "popular" on MDCalc, the most commonly used online medical calculator website by clinicians . As shown in Figure 1, they fall into two major categories, **rule-based** calculation (19 calculators) and **equation-based** calculation (36 calculators).

Rule-based calculators typically contain a list of criteria, where each criterion is a condition of a specific medical attribute. An instance of this would be the HEART score calculator , which takes in both numerical attributes such as the patient's age (e.g., if the patient is older than 65 years, add two points; if the patient's age is between 45 and 64, add one point; and zero points otherwise) and categorical variables such as the presence of significant ST elevation (adding two points if present; zero points otherwise). The final answer for these calculators will be a discrete value after taking the sum of the sub-scores.

Like rule-based calculators, equation-based calculators also take in both categorical (e.g., gender, race) and numerical variables (e.g., creatinine concentration, age, and height). However, equation-based calculators follow a specific formula to output a decimal, date, or time given the attributes instead of additively combining sub-scores for each criterion. An instance of an equation-based calculator would be the MDRD GRF equation . This equation computes the patient's eGFR, which is a function of the patient's gender and race as coefficients in addition to the patient's creatinine concentration. The only equation-based calculators which do not output a decimal are Estimated Due Date (EDD), Estimated Date of Conception (EDC), and Estimated Gestational Age (EGA). These three calculators compute a date (for EDC, EGA) or a time (for EGA) instead.

For each instance, MedCalc-Bench also provides a natural language explanation for how the final answer is computed. We implement template-based explanation generators for each of the 55 calculators. These templates first list the numerical and categorical variable values, and then plug them in to show how the final answers are obtained. The implementation details can be found in supplementary materials.

### Dataset Instance Collection

In this section, we describe how patient notes and answers were collected for the 55 different calculation tasks in MedCalc-Bench. We aimed to collect at most 20 notes for each calculator. Specifically, the patient notes were collected using the following three-step pipeline.

**(1) Note collection and attribute extraction.** We compiled Open-Patients3, a collection of over 180k public patient notes, including anonymized real case reports from PMC-Patients , case vignettes in MedQA-USMLE , synthetic cases in TREC Clinical Decision Support Tracks  and TREC Clinical Trials Tracks . Using GPT-3.5-Turbo, we identified patient notes for each calculator based on its eligibility criteria. We then used GPT-4 to extract the attribute values needed for each calculator from the eligible notes.

**(2) Data verification and enrichment.** For each of the patient notes for a given calculator, the extracted values from GPT-4 were verified and corrected by one individual with a medical background. After the verification, 34 of the 55 calculators had at least one eligible note with the extracted attributesneeded for computation. Hence, the remaining 21 calculators without any eligible notes were either synthesized with a template or were handwritten by an individual with a medical background.

Of these 21 calculators, 10 are equation-based calculators, for which we generated 20 synthesized notes using a template. The other 11 calculators are rule-based and we employed the same individuals with medical background to synthesize the patient notes and record the ground-truth values for the needed attributes.

**(3) Answer and explanation generation.** After obtaining patient notes with the extracted values, for each of the 55 calculators, we generated step-by-step explanations to derive the final answers. Specifically, we implemented templates for each calculator to generate the natural language explanations. From these three steps, we curated 1047 instances for MedCalc-Bench, each of which contains a patient note, a question, along with a ground-truth explanation and final answer.

### Dataset Characteristics

Table 1 shows the statistics of MedCalc-Bench and the different calculator sub-types. The equation-based calculators have between 1 to 7 attributes, while the rule-based calculators have between 3 to 31 attributes. Thus, it may require a varying number of reasoning steps to solve different tasks in our dataset.

Our dataset evaluates three distinct capabilities required for medical calculation:

**(1) Recall of medical calculation knowledge.** The first required capability is to successfully recall the formulas from seven different domains shown in Table 1. As mentioned above, medical calculators can have various sub-types with varying number of attributes. Hence, LLMs are challenged to know every detail about medical equations or rules to solve a question in MedCalc-Bench.

**(2) Extraction of relevant patient attributes.** The second required capability is the extraction of correct attributes from patient notes, given the noises in the long context of over 500 words on average. LLMs are required to extract both numerical and categorical attributes. The medical context complicates such extractions, with the existence of multiple synonyms (e.g., both HbA1c and glycohemoglobin denote the same entity) and the requirement of determining the presence of certain medical cases without explicitly being stated (e.g., a blood pressure of 160/100 mmHg indicates the presence of hypertension). Hence, LLMs require both medical knowledge and clinical reasoning to solve questions in this dataset.

**(3) Arithmetic computation of the final results.** The third required capability is the computation of final results, especially the derivation of scores through multi-step reasoning. While datasets like

**Overall** & 55 & 1047 & 529.7 & 21.9 & 1 & 31 & 5.4 &  \\   

Table 1: Statistics of MedCalc-Bench. Inst.: instance; Avg.: average; Attr.: attribute; Q.: question; L: length.

GSM-8k  have tested the arithmetic calculation capability of LLMs, MedCalc-Bench presents a more challenging task. Our dataset requires LLMs to fully understand the sequence and dependencies among multiple medical equations or rules, some of which need to be chained together, to obtain the correct answer. Additionally, MedCalc-Bench also contains some exponential computations that are not covered by other math datasets.

Overall, we believe that MedCalc-Bench serves as a comprehensive benchmark that goes beyond testing the internal medical calculation knowledge of LLMs. This dataset also tests general-purpose skills such as attribute extraction and arithmetic computation in a more challenging domain-specific setting.

## 3 Evaluation

### Settings

To establish the baseline performance in MedCalc-Bench, we experiment with eight different LLMs under three common prompting strategies. We categorize the eight LLMs into three groups: **Medical domain-specific** LLMs include PMC-LLaMA-13B  and MediTron-70B ; **Proprietary** LLMs include GPT-4  and GPT-3.5 ; **Open-source** LLMs, including 8B and 70B Llama 3 , as well as Mistral-7B  and Mistral-8x7B .

Similarly, we consider three prompting strategies: **Zero-shot Direct Prompting**: In this setting, the LLM is prompted to directly output the answer without any explanation; **Zero-shot Chain-of-Thought (CoT) Prompting**: In this setting, the LLM is prompted to first generate step-by-step rationale and then generate the answer ; **One-shot CoT Prompting**: In this setting, the LLM is provided with an exemplar of the corresponding calculation task. The exemplar is manually curated and contains the patient note, question, and the output consisting of the step-by-step explanation and final answer value.

Based on the output type, we have three different evaluation settings: (1) For all rule-based calculators, the final answer must be the exact same as the ground-truth answer, (2) For equation-based calculators that are lab tests, physical tests, and dosage conversion calculators, the predicted answer must be within 5% of the ground-truth answer, (3) For date-based equation calculations, the predicted dates should exactly match the ground truth.

### Main Results

Table 2 presents our evaluation results of various LLMs on the 1047 instances from MedCalc-Bench. From the table, we can observe the diverse performance of the models in different settings. In general, LLMs tend to perform better with the help of CoT prompting and one-shot learning, as evidenced by the improved accuracy for each LLM shown in the table. Among all LLMs compared, GPT-4 achieves the best performance in all three settings. In the zero-shot direct promoting setting, GPT-4 has a mean accuracy of 20.82% on the task. By leveraging its own reasoning ability, the performance of GPT-4 can be improved to 37.92%. Incorporating external medical knowledge from a one-shot demonstration further increases its accuracy to 50.91%. Similar patterns can also be observed in many other LLMs, such as LLama 3 and Mistral.

In addition to the general trend across different settings, the table also shows how various types of LLMs perform differently on our MedCalc-Bench test. While GPT-4 performs the best in our evaluation, the open-source LLama 3-70B model shows a competitive performance that is close to GPT-4. In both zero-shot direct prompting and zero-shot CoT prompting settings, LLama 3-70B achieves mean accuracies that are comparable to the results of GPT-4. However, GPT-4 significantly outperforms LLama 3-70B with the one-shot demonstration, which reflects its superior in-context learning capability for medical calculation. Moreover, by comparing LLama 3-8B/Mistral-7B with Llama 3-70B/Mistral-8x7B, we find larger LLMs generally perform better on the medical calculation tasks, corresponding to the empirical scaling laws [17; 25]. Interestingly, the 70B MediTron cannot 

    &  &  &  &  \\    & & Lab & Phys. & Date & Dosage & Risk &  & Diag. & \\   \\   &  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\  & & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) \\  & & 3.7 & 8.3 & 5.0 & 0.0 & 7.5 & 5.0 & 13.3 & 6.2 \\  & & \( 0.01\) & \( 0.02\) & \( 0.03\) & \( 0.00\) & \( 0.02\) & \( 0.02\) & \( 0.04\) & \( 0.01\) \\  & & 10.7 & **18.3** & **3.3** & **0.0** & **4.6** & **3.8** & **13.3** & **9.8** \\  & & \( 0.02\) & \( 0.02\) & \( 0.02\) & \( 0.00\) & \( 0.01\) & \( 0.02\) & \( 0.04\) & \( 0.01\) \\  & & 12.2 & 23.3 & 5.0 & 7.5 & 12.9 & 7.5 & 16.7 & **14.2** \\  & & \( 0.02\) & \( 0.03\) & \( 0.03\) & \( 0.04\) & \( 0.02\) & \( 0.03\) & \( 0.05\) & \( 0.01\) \\  & & 10.7 & 19.2 & 3.3 & 5.0 & 12.5 & 8.8 & 25.0 & 13.1 \\  & & \( 0.02\) & \( 0.03\) & \( 0.02\) & \( 0.03\) & \( 0.02\) & \( 0.03\) & \( 0.06\) & \( 0.01\) \\  & & **18.0** & **33.3** & **8.3** & **12.5** & **15.8** & **13.8** & **33.3** & **20.8** \\  & & \( 0.02\) & \( 0.03\) & \( 0.04\) & \( 0.05\) & \( 0.02\) & \( 0.04\) & \( 0.06\) & \( 0.01\) \\  & GPT-3.5  & N/A & 17.1 & 35.0 & 13.3 & 5.0 & 12.9 & 6.3 & 18.3 & 18.8 \\  & & \( 0.02\) & \( 0.03\) & \( 0.04\) & \( 0.03\) & \( 0.02\) & \( 0.03\) & \( 0.05\) & \( 0.01\) \\  & & 14.4 & **34.6** & 38.3 & 15.0 & 14.6 & 15.0 & 20.0 & 20.8 \\  & & \( 0.02\) & \( 0.03\) & \( 0.06\) & \( 0.06\) & \( 0.02\) & \( 0.04\) & \( 0.05\) & \( 0.01\) \\   \\   &  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & **0.0** & **0.0** \\  & & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.00\) \\  & & **0.0** & **0.0** & **3.3** & **0.0** & **0.0** & **0.0** & **3.3** & **0.4** \\  & & \( 0.00\) & \( 0.00\) & \( 0.02\) & \( 0.00\) & \( 0.00\) & \( 0.00\) & \( 0.02\) & \( 0.00\) \\  & & 10.1 & **14.6** & **1.7** & **0.0** & **9.6** & **7.5** & **25.0** & **10.8** \\  & & \( 0.02\) & \( 0.02\) & \( 0.02\) & \( 0.00\) & \( 0.02\) & \( 0.03\) & \( 0.06\) & \( 0.01\) \\  & & 22.6 & **40.0** & 6.7 & **17.5** & **11.3** & 21.3 & 15.0 & **22.4** \\  & & \( 0.02\) & \( 0.03\) & \( 0.03\) & \( 0.06\) & \( 0.02\) & \( 0.05\) & \( 0.05\) & \( 0.01\) \\  & & **16.5** & **25.0** & **1.7** & **7.5** & **11.3** & **13.8** & **26.7** & **16.4** \\  & & \( 0.02\) & \( 0.03\) & \( 0.02\) & \( 0.04\) & \( 0.02\) & \( 0.04\) & \( 0.06\) & \( 0.01\) \\  & & 33.9 & **66.3** & **25.0** & 20.0 & **18.3** & **16.3** & **36.7** & **35.5** \\  & & \( 0.03\) & \( 0.03\) & \( 0.06\) & \( 0.06\) & \( 0.02\) & \( 0.04\) & \( 0.06\) & \( 0.01\) \\  & & 20.5 & 45.0 & 11.7 & 17.5 & 13.3 & 10.0 & 31.7 & **23.7** \\  & & \( 0.02\) & \( 0.03\) & \( 0.04\) & \( 0.06\) & \( 0.02\) & \( 0.03\) & \( 0.06\) & \( 0.01\) \\  & & 26.3 & **71.3** & **48.3** & **40.0** & **27.5** & 15.0 & **28.3** & **37.9** \\  & & \( 0.02\) & \( 0.03\) & \( 0.06\) & \( 0.08\) & \( 0.03\) & \( 0.04\) & \( 0.06\) & \( 0.01\) \\   \\   &  & 5.2 & 10.4 & 8.3 & 2.5 & 7.1 & 1.3 & 11.7 & **7.0** \\  & & \( 0.01\) & \( 0.02\) & \( 0.04\) & \( 0.02\) & \( 0.02\) & \( 0.01\) & \( 0.04\) & \( 0.01\) \\  & & \( 0.01\) & \( 0.02\) & \( 0.03\) & \( 0.06\) & \( 0.06\) & \(

[MISSING_PAGE_FAIL:7]

It should be noted that these errors are not independent of each other. For example, LLMs usually recall the relevant calculator knowledge first, and then extract the relevant parameters, and finally conduct the computation. If the model recalls the calculator incorrectly, it is highly likely that it cannot extract the correct set of relevant parameters. Hence, we only consider the earliest error if there are multiple error types (e.g., if the model has error types A and B, then the error type will be A).

### What errors do different LLMs make?

To analyze the errors made by different LLMs, we utilize GPT-4 to classify their error types by comparing the LLM output to the ground truth in MedCalc-Bench. We manually evaluate the annotations of 200 randomly sampled explanation errors, and find the accuracy of GPT-4 error classifier to be 89%. As such, we apply it to analyze the mistakes of all CoT prompting results.

Table 4 shows the distribution of error types in different settings. Under the zero-shot CoT setting, most of the errors (more than 50%) belong to Type A in all LLMs, suggesting that recalling the correct equations or rules for the corresponding medical calculation task is the biggest challenge when no exemplar is provided. While Type A error is dominant under the zero-shot setting, its error rate varies in different LLMs, e.g. 0.96 in PMC-LLaMA and 0.35 in GPT-4. Such a difference reflects the diverse levels of medical calculation knowledge acquired by various LLMs.

Unlike the distributions in the zero-shot setting, errors that occurred with the one-shot CoT prompting have less than 50% being categorized as Type A, which is consistently observed in different LLMs. This shows the effectiveness of the one-shot exemplar in providing the background rule or equation needed for medical calculation. With the decrease in Type A errors, more Type B and Type C errors are captured in the wrong answers. This reveals the deficiencies of current LLMs in attribute extraction and arithmetic computation, which are required capabilities to perform real-world medical calculations.

### Limitations and Future Work

While our study provides a first-of-its-kind dataset to evaluate the medical calculation capabilities of various LLMs, there are several main limitations that can be improved in future work: (1) Due to the difficulty of manual verification of each instance in MedCalc-Bench, our dataset is limited in size, containing only 1047 instances in total. (2) We had a limited number of annotators with a medical

  
**Model** & **Type A Error** & **Type B Error** & **Type C Error** & **Type D Error** & **Error Rate** \\   \\ PMC-LLaMA-13B & 0.96 (96\%) & 0.03 (3\%) & 0.00 (0\%) & 0.01 (1\%) & 1.00 \\ MediTron-70B & 0.97 (97\%) & 0.00 (0\%) & 0.02 (2\%) & 0.00 (0\%) & 1.00 \\ Mistral-7B & 0.72 (80\%) & 0.11 (12\%) & 0.06 (7\%) & 0.00 (0\%) & 0.89 \\ Mixtral-8x7B & 0.55 (71\%) & 0.11 (14\%) & 0.09 (11\%) & 0.02 (3\%) & 0.78 \\ Llama 3-8B & 0.60 (72\%) & 0.11 (13\%) & 0.13 (15\%) & 0.00 (0\%) & 0.84 \\ Llama 3-70B & 0.43 (67\%) & 0.10 (16\%) & 0.11 (17\%) & 0.00 (0\%) & 0.64 \\ GPT-3.5 & 0.38 (50\%) & 0.24 (31\%) & 0.13 (17\%) & 0.02 (2\%) & 0.76 \\ GPT-4 & 0.35 (57\%) & 0.19 (30\%) & 0.08 (13\%) & 0.00 (0\%) & 0.62 \\   \\ PMC-LLaMA-13B & 0.42 (46\%\(\)) & 0.31 (34\%\(\)) & 0.17 (19\%\(\)) & 0.01 (1\%\(\)) & 0.91 \\ MediTron-70B & 0.24 (33\%\(\)) & 0.23 (32\%) & 0.26 (35\%\(\)) & 0.01 (1\%\(\)) & 0.74 \\ Mistral-7B & 0.32 (38\%\(\)) & 0.33 (40\%) & 0.17 (20\%\(\)) & 0.01 (1\%\(\)) & 0.83 \\ Mixtral-8x7B & 0.27 (38\%\(\)) & 0.23 (33\%) & 0.19 (27\%\(\)) & 0.01 (2\%\(\)) & 0.71 \\ Llama 3-8B & 0.25 (34\%\(\)) & 0.17 (24\%\(\)) & 0.29 (40\%\(\)) & 0.02 (2\%\(\)) & 0.73 \\ Llama 3-70B & 0.20 (34\%\(\)) & 0.12 (20\%\(\)) & 0.23 (39\%\(\)) & 0.05 (8\%\(\)) & 0.60 \\ GPT-5.5 & 0.23 (36\%\(\)) & 0.20 (30\%\(\)) & 0.20 (31\%\(\)) & 0.02 (2\%\(\)) & 0.65 \\ GPT-4 & 0.20 (40\%\(\)) & 0.13 (27\%\(\)) & 0.16 (33\%\(\)) & 0.00 (0\%–) & 0.49 \\   

Table 4: Error type distribution of LLMs on MedCalc-Bench. Numbers in parentheses denote the relative proportions. Arrows indicate the proportion changes from zero-shot to one-shot learning.

background and so only one individual could verify the GPT-4 parameter extractions. While there is no subjectivity for extracting numerical attribute values, there can be disagreement on descriptive attribute values such as determining whether a patient has renal disease for the HAS-BLED calculator . Hence, there may be a bias based on the individual's training and so there may be some subjectivity in these values. (3) While we saw a significant improvement in model performance with the one-shot exemplar, benchmarking the model with few-shot instances may have further increased the accuracy. However, curating such patient notes for rule-based calculators would have been difficult, given the labor-intensiveness of having to synthesize patient notes often requiring many attributes.

For future work, we will have follow-up updates on the dataset to improve its quality, adding more instances for each calculator. There is also room for benchmarking with advanced methods that have shown improvement on GSM-8k  such as step-by-step PPO  and scaling test-time inference . We leave such explorations as future work for researchers to enhance the medical computational capabilities of LLMs.

## 5 Related Work

### Language Model Evaluations in Medicine

Existing datasets for evaluating LLMs in biomedicine  have primarily focused on verbal reasoning through multiple choice questions such as PubMedQA , MedQA , MedMCQA , and the medical questions in MMLU . However, these datasets are mainly focused on qualitative reasoning instead of quantitative computation. Additionally, the format of multi-choice questions does not reflect the actual clinical settings where a single answer or response must be determined without any options provided. In this work, we introduce MedCalc-Bench, the first dataset that measures the quantitative reasoning capabilities of LLMs in medicine in a realistic setting where the LLM must determine the answer by itself without the support of answer choices.

### Language Model Evaluations in Computation

Many efforts have been made to evaluate the mathematical and computation capability of LLMs in various settings. GSM8k  and MATH  are two examples which focus on pure mathematical problems. However, such datasets with general settings may not reflect LLM performance in domain-specific applications. In contrast, Sci-Bench and Tutor-Eval  both include step-by-step explanations for domain-specific computation, but their focus is on college-level science as opposed to the medical field.

    & Medical & Knowledge & Qual. Reasoning & Comput. & Non-MCQ & Explanation \\  MedQA  & ✔ & ✔ & ✔ & ✗ & ✗ & ✗ \\ MedMCQA  & ✔ & ✔ & ✔ & ✗ & ✗ & ✗ \\ PubMedQA  & ✔ & ✗ & ✔ & ✗ & ✗ & ✗ \\ MMLU  & ✔ & ✔ & ✗ & ✗ & ✗ & ✗ \\ GSM8k  & ✗ & ✗ & ✔ & ✔ & ✔ \\ MATH  & ✗ & ✗ & ✗ & ✔ & ✔ & ✔ \\ OpenMedCalc  & ✔ & ✔ & ✔ & ✔ & ✗ \\ AgentMD  & ✔ & ✔ & ✔ & ✔ & ✗ \\ CalcQA  & ✔ & ✔ & ✔ & ✔ & ✗ \\ Sci-Bench  & ✗ & ✔ & ✗ & ✔ & ✔ \\ Tutor-Eval  & ✔ & ✔ & ✔ & ✔ & ✔ \\ 
**MeDCalc-Bench** & ✔ & ✔ & ✔ & ✔ & ✔ \\   

Table 5: Comparison of different datasets for LLM evaluation. Medical: tasks for medical evaluation; Knowledge: dataset tests knowledge to a particular domain; Qualitative (Qual) Reasoning: dataset tests qualitative reasoning; Comput.: dataset requires computation (i.e., quantitative reasoning); Non-MCQ: questions which have a single answer and without the use of multiple choices. Explanation: dataset provides a step-by-step reasoning.

Additionally, one of the key features of language agents is the capability to use tools [34; 45; 53; 63], such as code interpreters [3; 12] and external APIs [35; 39]. As such, these capabilities have been applied to medical computation tasks. Although OpenMedCalc , CalcQA , and AgentMD  use medical calculators to augment LLMs, their evaluations are based on small-scale or automatically constructed datasets. MedCalc-Bench is much larger than their evaluation datasets and contains both natural language explanations as well as final numeric answers.

Hence, MedCalc-Bench serves as the first dataset for medical-focused calculations with explanations. A full comparison of various datasets can be found in Table 5.

## 6 Conclusion

In conclusion, this study introduces MedCalc-Bench, the first dataset designed to evaluate the capabilities of LLMs for medical calculations. Our evaluations show that while LLMs like GPT-4 exhibit potential, none are reliable enough for clinical use. The error analysis highlights areas for improvement, such as knowledge recall and computational accuracy. We hope our work serves as a call to further improve LLMs and make them more suitable for medical calculations.

## Ethics Statement

For curating the patient notes in MedCalc-Bench, we only use publicly available patient notes from published case report articles in PubMed Central and clinician-generated anonymous patient vignettes. As such, **no identifiable personal health information is revealed in this study**.

While MedCalc-Bench is designed to evaluate the medical calculation capabilities of LLMs, it should be noted that the dataset is not intended for direct diagnostic use or medical decision-making without review and oversight by a clinical professional. Individuals should not change their health behavior solely on the basis of our study.

## Broader Impacts

As described in Sec 1, medical calculators are commonly used in the clinical setting. With the rapidly growing interest in using LLMs for domain-specific applications, healthcare practitioners might directly prompt chatbots like ChatGPT to perform medical calculation tasks. However, the capabilities of LLMs in these tasks are currently unknown. Since healthcare is a high-stakes domain and wrong medical calculations can lead to severe consequences, including misdiagnosis, inappropriate treatment plans, and potential harm to patients, it is crucial to thoroughly evaluate the performance of LLMs in medical calculations. Surprisingly, the evaluation results on our MedCalc-Bench dataset show that all the studied LLMs struggle in the medical calculation tasks. The most capable model GPT-4 achieves only 50% accuracy with one-shot learning and chain-of-thought prompting. As such, our study indicates that **current LLMs are not yet ready to be used for medical calculations**.

It should be noted that while high scores on MedCalc-Bench do not guarantee excellence in medical calculation tasks, failing in this dataset indicates that the models must not be considered for such purposes at all. In other words, we believe that passing MedCalc-Bench should be a necessary (but not sufficient) condition for a model to be used for medical calculation.