# HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness

Zihui Xue1,2  Mi Luo1  Changan Chen1  Kristen Grauman1,2

1The University of Texas at Austin 2FAIR, Meta

###### Abstract

We study the problem of precisely swapping objects in videos, with a focus on those interacted with by hands, given one user-provided reference object image. Despite the great advancements that diffusion models have made in video editing recently, these models often fall short in handling the intricacies of hand-object interactions (HOI), failing to produce realistic edits--especially when object swapping results in object shape or functionality changes. To bridge this gap, we present HOI-Swap, a novel diffusion-based video editing framework trained in a self-supervised manner. Designed in two stages, the first stage focuses on object swapping in a single frame with HOI awareness; the model learns to adjust the interaction patterns, such as the hand grasp, based on changes in the object's properties. The second stage extends the single-frame edit across the entire sequence; we achieve controllable motion alignment with the original video by: (1) warping a new sequence from the stage-I edited frame based on sampled motion points and (2) conditioning video generation on the warped sequence. Comprehensive qualitative and quantitative evaluations demonstrate that HOI-Swap significantly outperforms existing methods, delivering high-quality video edits with realistic HOIs.1

## 1 Introduction

Consider a video depicting the process of a human hand picking up a kettle and moving it around (Figure 1). What if we want to replace the hand-interacting object in this scene with another item specified by the user--perhaps a differently-shaped kettle, a bottle, or a bowl? This capability--to swap the _in-contact_ object in a video with another, while aligning with the original video's content--is crucial for enhancing various real-world applications. Such functionality can transform entertainment, allowing users to create novel video content without the need to re-record or engage in labor-intensive manual editing. For example, in advertising, there may be situations where a pre-recorded video needs to adapt to new sponsorship requirements by replacing a soda can in the video with a water bottle. Additionally, it holds significant promise in robotics, where recent results suggest generative models can reduce the reliance on manually collected task-specific visual data and thereby enable large-scale robot learning . For example, imagine a scenario where, from just a single video of a mug being picked up, a generative model is able to produce numerous variants of this video with diverse objects such as bottles, bowls and kettles. This capability could greatly streamline the data collection process, reducing the need for extensive manual data collection.

However, hands are notoriously challenging to work with in image/video editing . They pose significant hurdles in manual photoshopping and often produce unsatisfactory outputs when automated by generative models. The precise swapping of hand-interacting objects presents a unique challenge that existing diffusion models , despite their advances in video editing, fail to address adequately. This difficulty arises from three main factors: the need for (a) HOI-aware capabilities, (b) spatial alignment with the source context, and (c) controllable temporal alignment with the source video's motion patterns.

First, consider the challenge of HOI awareness. Objects chosen for swapping often vary in their properties from the original, resulting in changes to interaction patterns. For example, as illustrated in Figure 2 (a), replacing the kettle from the original video with a bowl necessitates adjustments in the hand's grasp patterns. While many generative inpainting methods  have been developed to insert reference objects into specific scene regions, they are generally limited to contexts where the objects are _isolated_--not in contact with a human hand or other objects--and thus lack HOI awareness. In Figure 2 (a), the two image inpainting approaches Paint by Example (PBE)  and AnyDoor  either merely replicate the hand pose from the original image, or produce unnaturally occluded hands, resulting in suboptimal and unrealistic HOI patterns.

Second, consider the challenge of spatial alignment with the original video. The reference object might appear in any arbitrary pose; for instance, in Figure 2 (b), the kettle handle is on the left in the reference image, but for realistic interaction, the generated content needs to reposition the kettle handle to the right, where the hand is poised to grasp it. However, current approaches do not offer this level of control, as evidenced by the results from a hand insertion approach Affordance Diffusion (Afford Diff)  in Figure 2 (b). Despite being adept at generating a hand interacting with the given kettle, it does not ensure correct object placement to align with the hand and scene context in the original image, lacking spatial alignment capability.

Third, consider the challenge of temporal alignment with the original video. We highlight a crucial observation in this problem: the motion information in an HOI video sequence is closely tied to the object's characteristics (such as its shape and function). This means that when swapping objects, _not all motions from the source video are appropriate or transferable_ to the new object. For example, Figure 2 (c) shows a hand closing a trash can, alongside an HOI-Swap edited image where the original can is replaced with one that is differently shaped and functions differently. Ideally, the generated content should reflect the motion of closing the lid, yet it may not replicate the exact motion from the source video due to these differences. Conversely, Figure 1 (first row) depicts a scenario of swapping one kettle with another. Here, the objects undergo only slight shape changes, allowing the generated video to closely follow the source video's motion. While there are varying degrees to which object changes can affect the original motion, current video editing approaches  adhere to a rigid degree of motion alignment, often targeting 100%, adopting conditional signals like optical flow or depth sequences that encode substantial object information. Consequently, they lack the controllability to adjust the degree of motion alignment based on object changes.

To address these challenges, we introduce HOI-Swap, a video editing framework designed for precise object edits with HOI awareness. We approach the challenge by posing it as a video inpainting task, and propose a fully self-supervised training framework. Our innovative approach structures the editing process into two stages. The first stage addresses HOI awareness and establishes spatial alignment, by training an image inpainting diffusion model for object swapping in one frame. The second stage propagates this one-frame edit across the remaining frames to achieve temporal alignment with the source. For this, we propose to warp a video sequence from the edited frame using randomly sampled points tracked across frames with optical flow, and train a video diffusion model that learns to fill in

Figure 1: We present HOI-Swap that seamlessly swaps the _in-contact_ object in videos using a reference object image, producing precise video edits with natural hand-object interactions (HOI). Notice how the generated hand needs to adjust to the shapes of the swapped-in objects (A,B,C) and how the reference object may require automatic re-posing to fit the video context (A).

the sparse and incomplete warped sequence. Our approach thus enables _controllable_ motion alignment by varying the sparsity of sampled points. During inference, users can modulate this number based on the object's changes--sampling fewer or no points for significant shape or functional alterations, and more points for minor changes to closely replicate the source video's motion. HOI-Swap is evaluated on both image and video editing tasks, consistently producing high-quality edits with realistic HOIs. It greatly surpasses existing editing approaches in both qualitative and quantitative evaluations, including a user study. By extending the capabilities of generative models into the largely unexplored domain of HOI, we hope that HOI-Swap opens new avenues for research and practical applications in this innovative field.

## 2 Related Work

Generative Models for Image EditingRecent advances in diffusion models [19; 45] have significantly enhanced the capabilities of image editing. Predominant models [38; 18; 2; 26; 53; 24] use _text_ as guidance, which, despite its utility, often lacks the precision needed for exact control. In response, a growing body of studies have begun to explore the use of reference images as editing guidance. Customized approaches like Textual Inversion  and DreamBooth  are designed to generate new images of a specific object given several of its images and a relevant text prompt. However, these methods require extensive finetuning for each case and lack the ability to integrate the object into another user-specified scene image. More closely related to our task, a few approaches aim to seamlessly blend a reference object [57; 50; 36; 7; 51; 16] or person  into a specific region of a target scene image. However, as demonstrated in Figure 2 and detailed in Section 4, these methods prove inadequate, often producing images with unnatural HOIs, such as missing fingers, distorted hands, or oddly shaped objects. These issues reveal shortcomings in current generative models, and motivate the development of HOI-Swap.

Generative Models for Video EditingWith the advent in diffusion-based text-to-image and text-to-video generation [45; 1; 13], many efforts explore extending pretrained diffusion models for video editing, employing zero-shot [12; 59; 42; 4; 60; 28], or one-shot-tuned learning paradigms [55; 32; 17; 67]. However, these methods require extensive video decomposition or costly per-video fine-tuning, with processing times ranging from several minutes to multiple hours on high-end GPUs for a single edit, which curtails their usability in practical creative tools. Another line of work [10; 6; 34; 54; 63; 41] adopts a training-based approach, where models are trained on large-scale datasets to enable their use as immediately effective editing tools during inference. Our work falls into this training-based paradigm.

Similar to image editing, the majority of video editing approaches [1; 13; 12; 59; 42; 4; 60; 55; 32] rely on _text_ as editing guidance. Since text prompts may not accurately capture the user's intent [67; 14; 41], for our task, using an object image provides more precise guidance. In terms of motion guidance,

Figure 2: We highlight three challenges for the in-contact object swapping problem: (a) HOI awareness, where the model needs to adapt to interactions, such as changing the grasp to realistically accommodate the different shapes of the kettle vs. the bowl; (b) spatial alignment with source, requiring the model to automatically reorient objects, such as aligning the blue kettle from the reference image to match the hand position in the source; (c) temporal alignment with source, necessitating controllable motion guidance capability, essential when swapping objects like a trash can with a differently shaped and functioning reference, where not all original motions are transferable or desirable. In (a) and (b), we compare HOI-Swap’s edited images with Paint by Example (PBE) , AnyDoor , and Affordance Diffusion (Afford Diff) .

most video editing approaches [42; 12; 10; 59; 21; 6; 5; 34; 63] utilize per-frame structural signals extracted from the source video, including depth map, optical flow, sketches, or canny edge sequences, facilitated by well-trained spatial-control image diffusion models such as ControlNet , T2I-Adapter  and Composer . Since these structural signals inherently encode the shape of the original object, they are unsuitable for edits involving shape changes [41; 17]. Recent works explore shape-aware video editing, by the use of Layered Neural Atlas , modifying the source video's optical flow sequences , or establishing correspondence with sparse semantic points . However, all these approaches overlook the impact of object changes on motion and only enforce a fixed degree of motion alignment. In contrast, HOI-Swap introduces flexibility by allowing users to adjust the sparsity of sampled points during inference, providing precise motion control.

Generating Hand-Object InteractionsThere is growing interest in generating plausible HOIs [20; 31; 62; 61; 9; 66; 64]. Affordance Diffusion  inserts a synthesized hand given a single object image. Similarly, HOIDiffusion  aims to create HOI images conditioned on a 3D object model along with detailed text descriptions. Meanwhile, other approaches in the 3D domain focus on generating realistic 3D HOI interactions from textual descriptions , synthesizing grasping motions  or reconstructing 3D HOIs from real videos . Despite these advancements, the task of accurately swapping in-contact objects in videos remains unexplored. Our work directly addresses this gap.

## 3 Method

We first formulate the problem and provide an overview of the two-stage HOI-Swap in Section 3.1. Section 3.2 and Section 3.3 detail the first and second stage of HOI-Swap, respectively.

### Task Formulation and Framework Overview

Given a source video \(=\{_{i}\}_{i=1}^{N}\) consisting of \(N\) frames (where \(_{i}\) represents the \(i\)-th frame), a binary mask (bounding box) sequence \(\{_{i}\}_{i=1}^{N}\) that identifies the region of the source object to be replaced in \(\), and an image of the reference object \(^{ref}\), the objective is to generate a modified video \(^{*}\), that seamlessly swaps the original object in \(\) with \(^{ref}\).

We create a fully self-supervised training approach, necessitated by the impracticality of collecting paired videos \((,^{*})\). We pose the problem as a video inpainting task. Specifically, from the original training video \(\) and mask \(\), we derive a masked video \(^{m}\), accompanied by a set of reference object images \(\{^{ref}\}\). During training, the model takes the masked video sequence \(^{m}\) and an image \(^{ref}\) of the same object originating from a different random timepoint (and hence varying pose and/or viewpoint) in the same training video to reconstruct \(\). During inference, the model is given a bounding box-masked sequence combined with various object images to test its swapping capability.

As outlined in Section 1, the task presents three main challenges: HOI awareness, spatial and temporal alignment. In response, we propose a two-stage framework that decomposes the complexity. The first stage integrates the reference object into a single frame, targeting HOI awareness and establishing spatial alignment initially. The second stage propagates the one-frame edit across the entire video sequence, focusing on temporal alignment and carrying forward the spatial alignment from the first stage. This structured approach effectively lightens the model's generation load. In terms of which frame to select from the input video for stage-I edits, we automate the process by adopting an off-the-shelf hand-object detector  to identify frames with hand-object contact, from which we randomly select one for editing. See Supp. C.4 for a detailed analysis. For greater applicability and flexibility, each stage is trained separately in a self-supervised manner, using the original object image as \(^{ref}\). Consequently, the stage-I model not only plays a crucial role in the video editing pipeline but also serves as a standalone image editing tool. The full pipeline is illustrated in Figure 3.

### Stage I: HOI-aware Object Swapping in One Frame

In the first stage, given a source frame \(_{i}\), a binary mask \(M_{i}\) (where 1 indicates the object's region and 0 denotes the background) and a reference object image \(^{ref}\), the objective is to generate an edited frame \(_{i}^{*}\) where the reference object is naturally blended into the source frame. We aim for the generated content \(_{i}^{*}\) to exhibit HOI awareness and spatial alignment with \(_{i}\), ensuring that the reference object is realistically interacting with human hands and accurately positioned within the scene context of the source frame.

**Constructing paired training data** Training is conducted in a self-supervised manner. Below, we detail how we prepare pseudo data pairs for training.

* **Masked frame preparation**: For each frame \(_{i}\), we obtain the object's bounding box (denoted by \(_{i}^{bbox}\)) from its segmentation mask \(_{i}\). To prevent the bounding box shape from influencing the aspect ratio of the generated results, we adjust the bounding box to be square. The masked frame is thus derived by applying the square bounding box mask to the frame, yielding \(_{i}^{m}=_{i} M_{i}^{bbox}\). We then crop and center the original frame to focus on the object region, enhancing its visibility.
* **Reference object preparation**: We extract the object image \(_{i}^{obj}=_{i}(1-_{i})\), which may be incomplete or partially obscured due to contact with hands or other objects. To address these occlusions, we employ an off-the-shelf text-guided inpainting model  to fill in the missing parts of the object, using the object's name as the text prompt.
* **Object augmentation**: Directly forming a training pair (\(_{i}^{obj}\), \(_{i}^{m}_{i}\)) is suboptimal as it does not reflect the variability expected in real-world applications. At test time, the reference object may appear in any pose, orientation, or size. To bridge this gap between training and testing scenarios, we apply strong augmentation techniques: (1) Spatially, we enhance the object's diversity by applying random resizing, flips, rotations, and perspective transformations; (2) Temporally, instead of using the object image from its original frame \(i\), we introduce variability by randomly selecting an alternate frame, \(_{i}^{obj}\), from all available frames in the source video that contain the object. Finally, for preservation of the reference object's identity and structural details, we collage the augmented reference object image \(_{i^{}}^{obj}\) onto the masked region in \(_{i}^{m}\), producing \(_{i}^{m^{}}\).

**Model design** We design the stage-I model as an image-conditioned latent diffusion model (LDM). Specifically, we employ a pretrained variational autoencoder (VAE) \(\) as used in prior work  to encode a frame \(\) into a latent space representation. This encoding is denoted by \(z=()\), where \(^{3 H W}\), \(z^{4 H/8 W/8}\), \(H\) and \(W\) denote the frame's height and width, respectively. During the forward diffusion process, Gaussian noise is gradually added to \(z\) over \(T\) steps, producing a sequence of noisy samples \(\{z_{0},,z_{t},,z_{T}\}\). We train a denoising network \(_{_{1}}(,t)\) that learns to reverse the diffusion process, which is implemented as a UNet . The training objective of stage

Figure 3: HOI-Swap involves two stages, each trained separately in a self-supervised manner. In stage I, an image diffusion model \(_{_{1}}\) is trained to inpaint the masked object region with a strongly augmented version of the original object image. In stage II, one frame is selected from the video to serve as the anchor. The remaining video is then warped using this anchor frame, several points sampled within it, and optical flow extracted from the video. A video diffusion model \(_{_{2}}\) is trained to reconstruct the full video sequence from the warped sequence. During inference, the stage-I model swaps the object in one frame. This edited frame then serves as the anchor for warping a new video sequence, which is subsequently taken as input for the stage-II model to generate the complete video.

I is summarized as:

\[_{stageI}=_{z,z^{m},d^{obj},(0,1),t} [\|-_{_{1}}(z_{t},z^{m},d^{obj},t)\|_{2}^ {2}].\] (1)

Our LDM is designed to take in two types of conditional signals alongside the standard \(z_{t}\) and \(t\): (1) The reference object image \(_{i}^{obj}^{3 H W}\) is encoded through DINO  for distinctive object features \(d^{obj}^{768}\), \(d^{obj}\) is then taken by \(_{_{1}}\) as input to guide the denoising process via cross-attention mechanisms; (2) The masked frame \(^{m}^{3 H W}\) is encoded by the same VAE to produce \(z^{m}=(^{m})\). \(z^{m}^{4 H/8 H/8}\) is then concatenated channel-wise with \(z\) before being fed into \(_{_{1}}\).

### Stage II: Controllable Motion-guided Video Generation

In stage II, given the first-stage edit \(_{i}^{}\), source video \(\), and its binary mask sequence \(\), the objective is to generate a new video \(^{}\) that propagates the single-frame edit across the remaining frames. For this purpose, we perform warping to transfer pixel points from the edited frame to the remaining frames, resulting in a sparse and incomplete video sequence. We then train a video diffusion network that learns to correct and fill in the gaps, completing the video editing process.

**Constructing paired training data**  Training continues in a self-supervised manner. For this purpose, we use the original frame \(_{i}\) from \(\) during training and only replace \(_{i}\) with the stage-I edit \(_{i}^{}\) during inference time. The detailed process is explained below:

* **Masked frame sequence preparation.**  We employ the same frame masking strategy in stage I and apply it to a sequence of frames \(=\{_{i}\}_{i=1}^{N}\). To standardize the masking across the sequence, we identify the largest bounding box \(_{max}^{bbox}\) from the sequence \(\{_{i}^{bbox}\}_{i=1}^{N}\) and mask each frame \(i\) by \(_{i}^{m}=_{i}_{max}^{bbox}\), resulting in a masked frame sequence \(\{_{i}^{m}\}_{i=1}^{N}\). This ensures that the model is trained to inapaint a consistent object region across different frames.
* **Conditioning frame selection.**  To avoid limiting the model to generating videos based on a specific reference frame, such as the first frame as often used in existing image-to-video generative models, we randomly select a frame \(_{c}\) from \(\) as the conditioning frame. This any-frame-conditioning mechanism brings additional flexibility during inference (for detailed discussion, see Supp. C.4).

**Controllable motion guidance**  Given that object swapping can result in changes to shape or functionality, our approach is designed to be flexible and adaptable, allowing for varying degrees of motion pattern encoding from the source video. The key idea is to control the level of motion agreement with the source via points sampled within the masked region. Tracking a large number of points over time captures extensive motion information from the source video, but it also reveals much about the source object's characteristics (e.g. shape). Conversely, using few points reduces the motion information and object characteristics carried over, but offers the model more freedom to generate plausible motion for the target. The latter scenario is particularly useful when only partial motion transfer is desired due to differences between objects, as exemplified in Figure 2 (c).

To be specific, our approach involves the following steps: (1) uniformly sample _r_% pixel points within the original object's region in the conditioning frame \(_{c}\) (i.e., where \(_{max}^{bbox}\) equals 1), (2) track these points using optical flow vectors computed by RAFT , and (3) warp a new video sequence based on these tracked points, using \(_{c}\) as the anchor. The resulting warped sequence is denoted by \(^{warp}=\{_{i}^{warp}\}_{i=1}^{N}\). The anchor frame \(_{c}^{warp}\) is intact and serves as the basis for warping (i.e., \(_{c}^{warp}=_{c}\)). For the other frames (\(i c\)), we overlay these points tracked from the conditioning frame \(c\) to frame \(i\) onto the masked region of \(_{i}\) (i.e., \(_{i}^{m}\)), creating a composite image \(_{i}^{warp}\). See Figure 3 (upper right) for an illustration of this process.

We vary the sparsity level of sampled motion points, \(r\) from 0 to 100 during training, preparing the model for a range of scenarios--from no motion guidance to full motion information. During inference, we demonstrate that the model is capable of generating video sequences with varying levels of motion alignment to the original video (see Section 4.2 and Supp. C.4 for further discussion). Thus, our stage-II training goal is to take the warped yet incomplete video sequence (\(^{warp}\)) as input, then learn to fill in the gaps and smooth over discontinuities to reconstruct the full video (\(\)).

**Model design**  We design the stage-II model as a video LDM. The LDM architecture and training objective closely follow those of stage I, with some adaptations for handling video sequences. We continue to use the VAE encoder \(\), but now apply it to a sequence of framesto obtain the latent feature \(^{N 4 H/8 W/8}\). We inflate the 2D UNet into a 3D UNet as the denoising network (denoted by \(_{_{2}}(,t)\)) for generating a frame sequence. Specifically, we insert additional temporal layers into the 2D UNet and interleave them with spatial layers, following [1; 10]. The stage-II training objective is shown below:

\[_{stageII}=_{,^{w},d^{c}, (0,1),t}[\|-_{_{2}}(_{t}, ^{w},d^{c},t)\|_{2}^{2}].\] (2)

Our video LDM involves two conditional signals in addition to the standard \(_{t}\) (a noised version of \(\)) and diffusion time step \(t\): (1) The warped video sequence \(^{warp}^{N 3 H W}\), is processed similarly as \(\), encoded by \(\), producing a latent feature \(^{w}^{N 4 H/8 W/8}\). \(^{w}\) is then concatenated channel-wise with \(\) before being fed into the denoising network \(_{_{2}}\). (2) The selected frame \(_{c}\), besides serving as the anchor in the warping process, is also encoded by CLIP , yielding \(d^{c}^{768}\) that provides high-level contextual guidance and is incorporated in \(_{_{2}}\) via cross-attention. Additionally, please see Supp. C.4 for an ablation of comparing CLIP and DINO as the object encoder.

## 4 Experiments

To comprehensively evaluate HOI-Swap, we consider both the image editing (accomplished by the stage-I model) and video editing (facilitated by the entire pipeline) task. We describe the experimental setup in Section 4.1 and present editing results along with an ablation analysis in Section 4.2.

### Experimental Setup

DatasetsOur training leverages two large-scale egocentric datasets, HOI4D  and EgoExo4D , which feature abundant HOIs, making them particularly suitable for exploring this problem. In total, the stage-I model is trained on 106.7K frames and the stage-II model uses 26.8K 2-second video clips for training. The evaluation set for image editing includes 1,250 source images, each paired with four reference object images, resulting in a total of 5,000 edited images. The evaluation set for video editing is composed of 25 source videos, also combined with four object images each, yielding 100 unique edited videos. Note that: (1) For image editing evaluation, we include both hand-present scenarios, which challenge the model's HOI-aware capabilities, and non-hand-present scenarios (20%) to evaluate its general object-swapping capability. (2) For video editing evaluation, in addition to HOI4D videos, we include challenging in-the-wild videos from EPIC-Kitchens  and third-person videos from TCN Pouring --datasets not used in training--to assess the model's zero-shot generalization across diverse scenarios. See Supp. B.1 for complete dataset descriptions.

BaselinesFor image editing, our stage-I model is compared with two strong image inpainting approaches, PBE  and AnyDoor , along with a hand-insertion approach Afford Diff  in the HOI domain. For video editing, we consider the following baselines: (1) applying the best image editing baseline to each frame of the video; (2) integrating the best image editing baseline with AnyV2V , a recent video editing approach that takes one edited frame from any black-box image editing model as conditional guidance; (3) VideoSwap , the state-of-the-art approach in customized object swapping. Besides these object-image-guided approaches, we include a focused comparison with text-guided approaches for a thorough evaluation. See Supp. C.6 for the discussion.

EvaluationHOI-Swap is evaluated through both human and automatic metrics. We conduct a user study, involving 15 participants that assess 260 image edits and 100 video edits. Participants

  Image Editing &  cont. \\ agr. \\  &  hand \\ agr. \\  &  hand \\ fid. \\  & 
 user \\ pref. \\  & Video Editing & subj. & mot. & cont. & hand & hand & user \\  & & cons. & smth. & agr. & agr. & fid. & pref. \\  PBE  & 67.8 & 71.9 & 73.9 & 4.5 & Per-frame & 87.4 & 95.3 & 86.4 & 64.8 & 82.2 & 0.2 \\ AnyDoor  & 84.6 & 70.9 & 86.3 & 15.6 & AnyV2V  & 71.7 & 95.6 & 68.3 & 30.8 & 27.0 & 1.2 \\ Afford Diff  & 78.6 & 19.5 & 92.9 & 7.6 & VideoSwap  & 91.8 & 97.8 & 87.9 & 52.6 & 81.9 & 1.2 \\ HOI-Swap (ours) & **87.9** & **79.8** & **97.4** & **72.1** & HOI-Swap (ours) & **92.4** & **98.2** & **93.1** & **78.6** & **97.6** & **86.4** \\  

Table 1: Quantitative evaluation of HOI-Swap using (1) automatic metrics: contact agreement (cont. agr.), hand agreement (hand agr.), hand fidelity (hand fid.), subject consistency (subj. cons.), and motion smoothness (mot. smth.)—the last two are for video only and (2) user preference rate (user pref.) from our user study. For video editing, users were given an option to report if they found all edits unsatisfactory, which has an average selection rate of 10.9%. All values are percentages.

are presented with four edited results, three from baselines and one from HOI-Swap (randomly shuffled) and asked to pick the best one in terms of object identity, HOI realism, and overall quality (for image edits), motion alignment and overall quality (for video edits). Each sample is assessed by three different participants. For automatic evaluation, we employ several metrics across different dimensions: (1) HOI contact agreement and hand agreement to measure spatial alignment, following [62; 66]; (2) hand fidelity, following ; (3) subject consistency and motion smoothness from VBench  to evaluate general video quality. See Supp. B.3 for full descriptions.

**Implementation** The stage-I model is trained on 512\(\)512 resolution images for 25K steps. For stage-II training, input video resolution is set as 14\(\)256\(\)256, where we sample 14 frames at an fps of 7 and train the model for 50K steps. See Supp. B.4 for full details.

### Editing Results

**Quantitative evaluation** Table 1 compares the performance of HOI-Swap on both image and video editing tasks. For image editing, the two inpainting approaches, PBE  and AnyDoor  struggle to generate the hand correctly, as indicated by hand fidelity scores below 90%. Afford Diff  is developed for the HOI context and also benefits from HOI-specific training data in HOI4D. It produces high-fidelity hands but does not address the editing task adequately, resulting in low contact and hand agreement with the source image. Our proposed HOI-Swap excels in both aspects, achieving superior performance in agreement and fidelity scores. In the human evaluation, HOI-Swap is consistently favored over the other three approaches, achieving a preference rate of 72.1%. For video editing, HOI-Swap outperforms all competing methods across all metrics, including general video quality metrics (subject consistency and motion smoothness) and HOI edit metrics (contact agreement, hand agreement and hand fidelity). It significantly surpasses the three baseline approaches, winning the user study with a selection rate of 86.4%. Recognizing the inherent challenges of the video editing task, we included a survey question asking users if "All edits are of very low quality; none successfully swap the object." This response was selected at an average rate of 10.9%. See Supp. C.3 for details.

**Qualitative evaluation** Figure 4 shows HOI-Swap's edited images (left) and videos (right) alongside baseline comparisons. For image editing, HOI-Swap exhibits strong HOI-awareness, accurately

Figure 4: Qualitative results of HOI-Swap. We compare HOI-Swap with image (left) and video editing approaches (right). The reference object image is shown in the upper left corner of the source image. For image editing, HOI-Swap demonstrates the ability to seamlessly swap in-contact objects with HOI awareness, even in cluttered scenes. For video editing, HOI-Swap effectively propagates the one-frame edit across the entire video sequence while accurately following the source video’s motion, achieving the highest overall quality among all methods. We highly encourage readers to check Supp. C.1 and the project page video for more comparisons.

adjusting hand-holding patterns based on the reference object's properties (rows 1 and 2, with source images from HO14D ). Moreover, HOI-Swap serves as a general object-swapping tool (not limited to HOIs) and can seamlessly swap in-contact objects in cluttered scenes (rows 3 and 4, with source images from EgoExo4D ). The final row demonstrates HOI-Swap's zero-shot generalization capability, using source images from EPIC-Kitchens . Despite the out-of-distribution samples, HOI-Swap successfully aligns the reference mug handle with the source's hand context, whereas baselines like AnyDoor  fail to establish this connection. See Supp. C.1 for more examples.

For video editing (Figure 4 right), the source video depicts a hand rotating a toy car. The per-frame approach struggles with temporal consistency of the object identity and produces unnatural HOI motions. AnyV2V  fails to propagate the first-frame edit through subsequent frames, leading to inconsistencies. VideoSwap  preserves the overall look of the source video but fails to swap the reference object (i.e., the green toy car). In contrast, HOI-Swap delivers video edits that realistically depict the HOI and faithfully follow the source video's motion. Beyond this specific example, we note that baseline methods generally perform poorly on our in-contact object swapping task across various examples (see the project page video for more results). Despite our best efforts to reproduce their results using the official code and adjusting hyperparameters according to the provided guidelines, the performance of these models remained notably suboptimal. We observe similar issues with state-of-the-art text-guided approaches (Supp. C.6). This persistent underperformance highlights a big gap in current (otherwise quite powerful) diffusion models in their ability to accurately model the intricacies of HOIs, indicating great opportunities for further advancements in this field.

Ablation study on motion points sparsityHOI-Swap can generate videos with different degrees of motion alignment with the original video. Figure 5 compares the generated results when sampling no points versus full points. Here the source video depicts a closing trash can lid action. In row 2, when no points are sampled (resulting in no warping), the generated video does not follow the closing lid motion but instead depicts an action of putting the lid into the bin; the visible hand motion outside the inpainted region provides context, guiding the model to generate a plausible sequence with the hand moving downward. Conversely, using all motion points produces videos that closely align with the source video's motion. See Supp. C.4 for follow-up discussion on this.

One-stage vs. two-stage designNext we verify the efficacy of our two-stage design, where we train a one-stage model variant that takes the reference object image and the masked frame sequence as input and outputs the edited video. Figure 6 shows both qualitative and quantitative comparisons. The one-stage approach does not yield satisfactory results, specifically failing to preserve the reference object's identity. This limitation arises from the challenge of simultaneously addressing both spatial and temporal alignment, thereby reinforcing the advantage of our two-stage design.

Figure 5: Ablation study on sampled motion points, comparing no to full motion points sampling. Left: we visualize \(^{warp}\), used as conditional guidance for the stage-II model. Note that row 1 displays \(^{warp}\) based on the source frame and is for illustration only, not provided to the model. Right: HOI-Swap exhibits controllable motion alignment: with no sampled points, the generated video diverges from the source video’s motion; with full motion points, it closely mimics the source.

Moreover, we present a comprehensive discussion on various design choices of HOI-Swap in Supp. C.4 and C.5, covering aspects such as object encoder choice, object masking strategy, editing frame selection strategy, and discussion on sampling regions.

## 5 Limitations and Future work

The problem of swapping _in-contact_ objects in videos poses significant challenges. While HOI-Swap marks an important first step towards addressing these complexities, we recognize its current limitations. Specifically, we identify three areas for improvement: (1) **generalization to new objects.** HOI-Swap capably swaps new object instances with HOI awareness (e.g., swap an unseen mug with a differently-shaped bowl). More challenging scenarios involve presenting very different unseen reference object images for swapping. For example, the model needs to accurately depict a hand holding scissors, even though scissors were never part of the training dataset. This scenario requires the model to equip "world knowledge" that extends beyond its training data, enabling it to understand and realistically model how hands interact with a broader variety of objects. (2) **generalization to long video sequences with complex HOL** The two-stage pipeline of HOI-Swap is motivated by the observation that HOI interactions remain stable throughout a short video clip. For instance, for a picking up mug sequence, replacing the mug with a bowl can be reliably done in one single frame and propagated across the remaining frames. However, for longer sequences, it is possible that an object undergoes multiple distinct hand interactions that change dynamically over time. This complexity necessitates the development of methods that can capture and model these varied HOI interactions across time. (3) **controllability**. Our proposed controllable motion guidance allows HOI-Swap to freely choose the degree of motion alignment with the source video in editing. One future work direction is to enhance this controllability with spatial support, allowing the model to specify which regions of the source video should be targeted for motion transfer. See Supp. D for more details on HOI-Swap's limitations and failure modes, including visual examples.

## 6 Conclusion

Recognizing the limitations of current diffusion models in effectively capturing HOIs, we introduce HOI-Swap, a novel approach for swapping hand-interacting objects in videos based on one reference object image. HOI-Swap consists of two stages: the first seamlessly integrates the reference object into a single frame, while the second propagates this edit across the entire sequence, with controllable motion alignment. Both qualitative and quantitative evaluations demonstrate that HOI-Swap produces realistic video edits with accurate HOIs that align well with the source content. In all, this work broadens the capabilities of generative models in video editing and represents the initial step towards solutions that can adeptly handle the intricacies of complex and dynamic HOIs in videos.

Figure 6: Qualitative and quantitative comparisons between a one-stage baseline  and our two-stage HOI-Swap. The one-stage model struggles with preserving the new object’s identify and fails to generate accurate interaction patterns, yielding inferior quantitative performance.

**Acknowledgements:** UT Austin is supported in part by the IFML NSF AI Institute. KG is paid as a research scientist at Meta. The authors would like to thank Zhengqi Gao (MIT), Xixi Hu (UT Austin), Hanwen Jiang (UT Austin), Feng Liang (UT Austin), and Yue Zhao (UT Austin) for their insightful discussions and valuable feedback, which contributed to the development of this work.