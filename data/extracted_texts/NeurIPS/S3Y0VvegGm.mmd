# The Benefits of Being Distributional:

Small-Loss Bounds for Reinforcement Learning

 Kaiwen Wang  Kevin Zhou  Runzhe Wu  Nathan Kallus  Wen Sun

Cornell University

{kw437,klz23,rw646,kallus,ws455}@cornell.edu

###### Abstract

While distributional reinforcement learning (DistRL) has been empirically effective, the question of when and why it is better than vanilla, non-distributional RL has remained unanswered. This paper explains the benefits of DistRL through the lens of small-loss bounds, which are instance-dependent bounds that scale with optimal achievable cost. Particularly, our bounds converge much faster than those from non-distributional approaches if the optimal cost is small. As warmup, we propose a distributional contextual bandit (DistCB) algorithm, which we show enjoys small-loss regret bounds and empirically outperforms the state-of-the-art on three real-world tasks. In online RL, we propose a DistRL algorithm that constructs confidence sets using maximum likelihood estimation. We prove that our algorithm enjoys novel small-loss PAC bounds in low-rank MDPs. As part of our analysis, we introduce the \(_{1}\) distributional eluder dimension which may be of independent interest. Then, in offline RL, we show that pessimistic DistRL enjoys small-loss PAC bounds that are novel to the offline setting and are more robust to bad single-policy coverage.

## 1 Introduction

The goal of reinforcement learning (RL) is to learn a policy that minimizes/maximizes the mean loss/return (_i.e._, cumulative costs/rewards) along its trajectory. Classical approaches, such as \(Q\)-learning (Mnih et al., 2015) and policy gradients (Kakade, 2001), often learn \(Q\)-functions via least square regression, which represent the mean loss-to-go and act greedily with respect to these estimates. By Bellman's equation, \(Q\)-functions suffice for optimal decision-making and indeed these approaches have vanishing regret bounds, suggesting we only need to learn means well (Sutton and Barto, 2018). Since the seminal work of Bellemare et al. (2017), however, numerous developments showed that learning the _whole_ loss distribution can actually yield state-of-the-art performance in stratospheric balloon navigation (Bellemare et al., 2020), robotic grasping (Bodnar et al., 2020), algorithm discovery (Fawzi et al., 2022) and game playing benchmarks (Hessel et al., 2018; Dabney et al., 2018; Barth-Maron et al., 2018). In both online (Yang et al., 2019) and offline RL (Ma et al., 2021), distributional RL (DistRL) algorithms often perform better and use fewer samples in challenging tasks when compared to standard approaches that directly estimate the mean.

Despite learning the whole loss distribution, DistRL algorithms use only the mean of the learned distribution for decision making, not extracting any additional information such as higher moments. In other words, DistRL is simply employing a different and seemingly roundabout way of learning the mean: first, learn the loss-to-go distribution via distributional Bellman equations, and then, compute the mean of the learned distribution. Lyle et al. (2019) provided some empirical explanations of the benefits of this two-step approach, showing that learning the distribution, _e.g._, its moments or quantiles, is an auxiliary task that leads to better representation learning. However, the theoreticalquestion remains: does DistRL, _i.e._, learning the distribution and then computing the mean, yield provably stronger finite-sample guarantees and if so stronger how and when?

In this paper, we provide the first mathematical basis for the benefits of DistRL via the lens of small-loss bounds, which are instance-dependent bounds that depend on the minimum achievable cost in the problem [Agarwal et al., 2017].1 For example in linear MDPs, typical worst-case regret bounds scale on the order of \((d,H)\), where \(d\) is the feature dimension, \(H\) is the horizon, and \(K\) is the number of episodes [Jin et al., 2020]. In contrast, small-loss bounds will scale on the order of \((d,H)}+(d,H)(K)\), where \(V^{}=_{}V^{}\) is the optimal expected cumulative cost for the problem. We assume cumulative costs are normalized in \(\) without loss of generality. As \(V^{}\) becomes negligible (approaches \(0\)), the first term vanishes and the small-loss bound yields a faster convergence rate of \(((d,H)(K))\), compared to the \(((d,H))\) rate in standard uniform bounds. Since we always have \(V^{} 1\), small-loss bounds simply match the standard uniform bounds in the worst case.

As warm-up, we show that maximum likelihood estimation (MLE), _i.e._, maximizing log-likelihood, can be used to obtain small-loss regret bounds for contextual bandits (CB), _i.e._, the one-step RL setting. Then, we turn to the online RL setting, and propose an optimistic DistRL algorithm that optimizes over confidence sets constructed via MLE applied to the distributional Bellman equations. We prove our algorithm attains the first small-loss PAC bounds in low-rank MDPs [Agarwal et al., 2020]. Our proof uses a novel regret decomposition with triangular discrimination and also introduces the \(_{1}\) distributional eluder dimension, which generalizes the \(_{2}\) distributional eluder dimension of Jin et al.  and may be of independent interest. Furthermore, we design an offline distributional RL algorithm using the principle of pessimism, and show our algorithm obtains the first small-loss bounds in offline RL. Our offline small-loss bound holds under the weak single-policy coverage. Notably, our result has a novel robustness property that allows our algorithm to strongly compete with policies that either are well-covered or have small-loss, while prior approaches solely depended on the former. Finally, we find that our distributional CB algorithm empirically outperforms existing approaches in three challenging CB tasks.

Our key contributions are as follows:

1. As warm-up, we propose a distributional CB algorithm and prove that it obtains a small-loss regret bound (Section4). We empirically demonstrate it outperforms state-of-the-art CB algorithms in three challenging benchmark tasks (Section7).
2. We propose a distributional online RL algorithm that enjoys small-loss bounds in settings with low \(_{1}\) distributional eluder dimension, which we show can always capture low-rank MDPs. The \(_{1}\) distributional eluder dimension may be of independent interest (Section5).
3. We propose a distributional offline RL algorithm and prove that it obtains the first small-loss bounds in the offline setting. Our small-loss guarantee exhibits a novel robustness to bad coverage, which implies strong improvement over more policies than existing results in the literature (Section6).

In sum, we show that DistRL can yield small-loss bounds in both online and offline RL, which provide a concrete theoretical justification for the benefits of distribution learning in decision making.

## 2 Related Works

Theory of Distributional RLRowland et al.  proved asymptotic convergence guarantees of popular distributional RL algorithms such as C51 [Bellemare et al., 2017] and QR-DQN [Dabney et al., 2018]. However, these asymptotic results do not explain the _benefits_ of distributional RL over standard approaches, since they do not imply stronger finite-sample guarantees than those obtainable with non-distributional algorithms. In contrast, our work shows that distributional RL yields adaptive finite-sample bounds that converge faster when the optimal cost of the problem is small. Wu et al.  recently derived finite-sample bounds for distributional off-policy evaluation with MLE, while our offline RL section focuses on off-policy optimization.

First-order bounds in banditsWhen maximizing rewards, first-order "small-return" bounds can be easily derived from EXP4 (Auer et al., 2002), since receiving the worst reward \(0\) with probability (w.p.) \(\) contributes at most \(R^{}\) to the regret2. When minimizing costs, receiving the worst loss \(1\) w.p. \(\) may induce large regret relative to \(L^{}\) if \(L^{}\) is small. To illustrate, if \(R^{}=0\) then all policies are optimal, so no learning is needed and the small-return bound is vacuous. Yet if \(L^{}=0\), sub-optimal policies may have a large gap from \(L^{}\), so small-loss bounds in this regime are meaningful. Small-loss bounds are achievable in multi-arm bandits (Foster et al., 2016), semi-bandits (Neu, 2015; Lykouris et al., 2022), and CBs (Allen-Zhu et al., 2018; Foster and Krishnamurthy, 2021).

First-order bounds in RLJin et al. (2020); Wagenmaker et al. (2022) obtained small-return regret for tabular and linear MDPs via concentration bounds that scale with the variance. The idea is that the return's variance is bounded by some multiple of the expected value, which is bounded by \(V^{}\) in the reward-maximizing setting, _i.e._, \((_{h}r_{h}^{k}) c V^{^{k}} c V^{}\). However, the last inequality fails in the loss-minimizing setting, so the variance approach does not easily yield small-loss bounds. Small-loss regret for tabular MDPs was resolved by Lee et al. (2020, Theorem 4.1) using online mirror descent with the log-barrier on the occupancy measure. Moreover, Kakade et al. (2020, Theorem 3.8) obtains small-loss regret for linear-quadratic regulators (LQRs), but their Assumption 3 posits that the coefficient of variation for the cumulative costs is bounded, which is false in general even in tabular MDPs. To the best of our knowledge, there are no known first-order bounds for low-rank MDPs or in offline RL.

Risk-sensitive RLA well-motivated use-case of DistRL is risk-sensitive RL, where the goal is to learn risk-sensitive policies that optimize some risk measure, _e.g._, Conditional Value-at-Risk (CVaR), of the loss (Dabney et al., 2018). Orthogonal to risk-sensitive RL, this work focuses on the benefits of DistRL for standard risk-neutral RL. Our insights may lead to first-order bounds for risk-sensitive RL, which we leave as future work.

## 3 Preliminaries

As warmup, we begin with the contextual bandit problem with an arbitrary context space \(\), finite action space \(\) with size \(A\) and conditional cost distributions \(C:()\). Throughout, we fix some dominating measure \(\) on \(\)(_e.g._, Lebesgue for continuous or counting for discrete) and let \(()\) be all distributions on \(\) that are absolutely continuous with respect to \(\). We identify such a distribution with its density with respect to \(\), and we also write \(C(y x,a)\) for \((C(x,a))(y)\). Let \(K\) denote the number of episodes. At each episode \(k[K]\), the learner observes a context \(x_{k}\), samples an action \(a_{k}\), and then receives a cost \(c_{t} C(x_{t},a_{t})\), which we assume to be normalized, _i.e._, \(c_{t}\). The goal is to design a learner that attains low regret with high probability, where regret is defined as

\[_{}(K)=_{k=1}^{K}(x_{k},a_{k})-(x_{k },^{}(x_{k})),\]

where \(= yf(y)(y)\) for any \(f()\) and \(^{}(x_{k})=_{a}(x_{k},a)\).

The focus of this paper is reinforcement learning (RL) under the Markov Decision Process (MDP) model, with observation space \(\), finite action space \(\) with size \(A\), horizon \(H\), transition kernels \(P_{h}:()\) and _cost_ distributions \(C_{h}:()\) at each step \(h[H]\). We start with the _Online RL_ setting, which proceeds over \(K\) episodes as follows: at each episode \(k[K]\), the learner plays a policy \(^{k}[()]^{H}\); we start from a fixed initial state \(x_{1}\); then for each \(h=1,2,,H\), the policy samples an action \(a_{h}^{k}_{h}(x_{h})\), receives a cost \(c_{h} C_{h}(x_{h},a_{h})\), and transitions to the next state \(x_{h+1} P_{h}(x_{h},a_{h})\). Our goal is to compete with the optimal policy that minimizes expected the loss, _i.e._, \(^{}_{}V^{}\) where \(V^{}=_{}_{h=1}^{H}c_{h}\). Regret bounds aim to control the learner's regret with high probability, where regret is defined as,

\[_{}(K)=_{k=1}^{K}V^{^{k}}-V^{}.\]

If the algorithm returns a single policy \(\), it is desirable to obtain a Probably Approximately Correct (PAC) bound on the sub-optimality of \(\), _i.e._, \(V^{}-V^{}\).

The third setting we study is _Offline RL_, where instead of needing to actively explore and collect data ourselves, we are given \(H\) datasets \(_{1},_{2},,_{H}\) to learn a good policy \(\). Each \(_{h}\) contains \(N\)_i.i.d._ samples \((x_{h,i},a_{h,i},c_{h,i},x^{}_{h,i})\) from the process \((x_{h,i},a_{h,i})_{h},c_{h,i} C_{h}(x_{h,i},a_{h,i}),x^{}_{h,i} P_{h}(x_{h,i},a_{h,i})\), where \(_{h}()\) is arbitrary, _e.g._, the visitations of many policies from the current production system. The goal is to design an offline procedure with a PAC guarantee for \(\), which should improve over the data generating process.

Distributional RLFor a policy \(\) and \(h[H]\), let \(Z^{}_{h}(x_{h},a_{h})()\) denote the distribution of the loss-to-go \(_{t=h}^{H}c_{t}\) conditioned on rolling in \(\) from \(x_{h},a_{h}\). The expectation of the above is \(Q^{}_{h}(x_{h},a_{h})=Z^{}_{h}(x_{h},a_{h})\) and \(V^{}_{h}(x_{h})=_{a_{h}_{h}(x_{h})}[Q^{}_{h}(x_{h},a_{h })]\). We use \(Z^{}_{h},Q^{}_{h},V^{}_{h}\) to denote these quantities with \(^{}\). Recall the regular Bellman operator acts on a function \(f:\) as follows: \(^{}_{h}f(x,a)=C_{h}(x,a)+_{x^{} P_{h}(x,a),a ^{}(x^{})}[f(x^{},a^{})]\). Analogously, the distributional Bellman operator (Morimura et al., 2012; Bellemare et al., 2017) acts on a conditional distribution \(d:()\) as follows: \(^{,D}_{h}d(x,a)C_{h}(x,a)+d(x^{},a^{})\), where \(x^{} P_{h}(x,a),a^{}(x^{})\) and \(\) denotes equality of distributions. Another way to think about the distributional Bellman operator is that a sample \(z^{,D}_{h}d(x,a)\) is generated as follow: \(z:=c+y\), where \(c C_{h}(x,a),x^{} P_{h}(x,a),a^{}(x^{}),y d (x^{},a^{})\). We will also use the Bellman optimality operator \(^{}_{h}\) and its distributional variant \(^{,D}_{h}\), defined as follows: \(^{}_{h}f(x,a)=_{h}(x,a)+_{x^{} P_{h }(x,a)}[_{a}f(x^{},a^{})]\) and \(^{,D}_{h}d(x,a)C_{h}(x,a)+d(x^{},a^{ })\) where \(x^{} P_{h}(x,a),a^{}=*{arg\,min}_{a}(x^{ },a)\). Please see Table 2 for an index of notations.

## 4 Warm up: Small-Loss Regret for Distributional Contextual Bandits

In this section, we propose an efficient reduction from CB to online maximum likelihood estimation (MLE), which is the standard tool for distribution learning that we will use throughout the paper. In our CB algorithm, we balance exploration and exploitation with the reweighted inverse gap weighting (ReIGW) of Foster and Krishnamurthy (2021), which defines a distribution over actions given predictions \(^{A}\) and a parameter \(_{++}\): setting \(b=*{arg\,min}_{a}(a)\) as the best action with respect to the predictions, the weight for any other action \(a b\) is,

\[_{}(,)[a]:=(b)}{A {f}(b)+((a)-(b))},\] (1)

and the rest of the weight is allocated to \(b\): \(_{}(,)[b]=1-_{a b}_{ }(,)[a]\).

```
1:Input: number of episodes \(K\), failure probability \(\), ReIGW learning rate \(\).
2:Initialize any cost distribution \(f^{(1)}\).
3:for episode \(k=1,2,,K\)do
4: Observe context \(x_{k}\).
5: Sample action \(a_{k} p_{k}=(^{(k)}(x_{k},),)\) from Eq. (1).
6: Observe cost \(c_{k} C(x_{k},a_{k})\) and update online MLE oracle with \(((x_{k},a_{k}),c_{k})\).
7:endfor ```

**Algorithm 1** Distributional CB (DistCB)

We propose **Distributional**C**ontextual **B**andit (DistCB) in Algorithm 1, a two-step procedure for each episode \(k[K]\). Upon seeing context \(x_{k}\), DistCB first samples an action \(a_{k}\) from ReIGW generated by means of our estimated cost distributions for each action, _i.e._, \((a)=^{(k)}(x_{k},a), a\) (Line 5). Then, DistCB updates \(f^{(k)}( x_{k},a_{k})\) by maximizing the log-likelihood to estimate the conditional cost distribution \(C( x_{k},a_{k})\) (Line 6). Formally, this second step is achieved via an online MLE oracle with a realizable distribution class \(_{CB}()\); let \(_{}(K)\) be some upper bound on the log-likelihood regret for all possibly adaptive sequences \(\{x_{k},a_{k},c_{k}\}_{k[K]}\),

\[_{k=1}^{K} C(c_{k} x_{k},a_{k})- f^{(k)}(c_{k} x_{k},a_{k}) _{}(K).\]

Under _realizability_, \(C_{CB}\), we expect \(_{}(K)((K))\). For instance, if \(_{CB}\) is finite, exponentially weighted average forecaster guarantees \(_{}(K)|_{CB}|\)(Cesa-Bianchi and Lugosi, 2006; Chapter 9). We now state our main result for DistCB.

**Theorem 4.1**.: _For any \((0,1)\), w.p. at least \(1-\), running DistCB with \(=10A+(1/))}{112_{ }(K)+(1/)}}\) has regret scaling with \(C^{}=_{k=1}^{K}_{a}(x_{k},a)\),_

\[_{}(K) 232\,_{ }(K)(1/)}+2300A_{}(K)+(1/) .\]

The dominant term scales with the optimal sum of costs \(}\) which shows that DistCB obtains small-loss regret. DistCB is also computationally efficient since each episode simply requires computing the ReIGW. FastCB is the only other computationally efficient CB algorithm with small-loss regret (Foster and Krishnamurthy, 2021, Theorem 1). Our bound matches that of FastCB in terms of dependence on \(A,C^{}\) and \((1/)\). Our key difference with FastCB is the online supervised learning oracle: in DistCB, we aim to learn the conditional cost distribution by maximizing log-likelihood, while FastCB aims to perform regression with the binary cross-entropy loss. In Section 7, we find that DistCB empirically outperforms SquareCB and FastCB in three challenging CB tasks, which reinforces the practical benefits of distribution learning in CB setting.

### Proof Sketch

First, apply the per-round inequality for ReIGW (Foster and Krishnamurthy, 2021, Theorem 4) to get,

\[_{}(K)_{k=1}^{K}_{a_{k} p _{k}}(s_{k},a_{k})+^{(k)}(s_{k},a_{k})-(s_{k},a_{k})^{2}}{^{(k )}(s_{k},a_{k})+(s_{k},a_{k})}}_{}.\]

For any distributions \(f,g()\), their triangular discrimination3 is defined as \(D_{}(f g):=}{f(y)+g(y)} (y)\). The key insight is that \(\) can be bounded by the triangular discrimination of \(f^{(k)}(s_{k},a_{k})\) and \(C(s_{k},a_{k})\): by Cauchy-Schwartz and \(y^{2} y\) for \(y\), we have \(-= y(f(y)-g(y))(y)(y)}}{f(y)+g(y)} (y)}\), and hence,

\[-+D_{ }(f g)}.\] ( \[_{1}\] )

So, Eq. (\(_{1}\)) implies that \(\) is bounded by \(D_{}(f^{(k)}(s_{k},a_{k}) C(s_{k},a_{k}))\). Since \(D_{}\) is equivalent (up to universal constants) to the squared Hellinger distance, Foster et al. (2021, Lemma A.14) implies the above can be bounded by the online MLE regret, so w.p. at least \(1-\), we have

\[_{}(K)_{k=1}^{K} (s_{k},a_{k})+(1/)+_{}(K)+(1/).\]

From here, we just need to rearrange terms and set the correct \(\). Appendix C contains the full proof.

## 5 Small-Loss Bounds for Online Distributional RL

We now extend our insights to the online RL setting and propose a DistRL perspective on GOLF (Jin et al., 2021). While GOLF constructs confidence sets of near-minimizers of the squared Bellman error loss, we propose to construct these confidence sets using near-maximizers of the log-likelihood loss to approximate MLE. To leverage function approximation for learning conditional distributions, we use a generic function class \((())^{H}\) where each element \(f\) is a tuple \(f=(f_{1},,f_{H})\) such that each \(f_{h}\) is a candidate estimator for \(Z_{h}^{}\), the distribution of loss-to-go \(_{t=h}^{H}c_{t}\) under \(^{}\). For notation, \(f_{H+1}(x,a)=_{0}\) denotes the dirac at zero for all \(x,a\).

We now present our **O**ptimistic **D**istributional **C**onfidence set **O**ptimization (O-DISCO) algorithm in Algorithm 2, consisting of three key steps per episode. At episode \(k[K]\), O-DISCO first identifies the \(f^{(k)}\) with the minimal expected value at \(h=1\) over the previous confidence set \(_{k-1}\) (Line 4). This step induces _global optimism_. Then, O-DISCO collects data for this episode by rolling in with the greedy policy \(^{k}\) with respect to the mean of \(f^{(k)}\) (Line 6). Finally, O-DISCO constructs aconfidence set \(_{k}\) by including a function \(f\) if it exceeds a threshold on the log-likelihood objective using data \(z_{h,i}^{f}_{h}^{,D}f_{h+1}(x_{h,i},a_{h,i})\) for all steps \(h\) simultaneously (Line 7). This step is called _local fitting_, as each \(f_{k}\) has the property that \(f_{h}\) is close-in-distribution to \(_{h}^{,D}f_{h+1}\) for all \(h\). We highlight that O-DISCO only learns the distribution for estimating the mean, _i.e._, Lines 4 and 6 only use the mean \(\). This seemingly roundabout way of estimating the mean is exactly how distributional RL algorithms such as C51 differ from the classic DQN.

To ensure that MLE succeeds for the Temporal-Difference (TD) style confidence sets, we need the following distributional Bellman Completeness (BC) condition introduced in Wu et al. (2023).

**Assumption 5.1** (Bellman Completeness).: For all \(,h[H]\), \(f_{h+1}_{h+1}_{h}^{,D}f_{h+1}_{h}\).

### The \(_{1}\) Distributional Eluder Dimension

We now introduce the \(_{1}\) distributional eluder dimension. Let \(\) be an abstract input space, let \(\) be a set of functions mapping \(\) and let \(\) be a set of distributions on \(\).

**Definition 5.2** (\(_{p}\)-distributional eluder dimension).: For any function class \(\), distribution class \(()\) and \(>0\), the \(_{p}\)-distributional eluder dimension (denoted by \(_{p}(,,)\)) is the length \(L\) of the longest sequence \(d^{(1)},d^{(2)},,d^{(L)}\) such that there exists \(^{}\), such that for all \(t[L]\), we have that there exists \(f\) such that \(|_{d^{(t)}}f|>\) and also \(_{i=1}^{t-1}\|_{d^{(t)}}f|^{p}^{p}\).

When \(p=2\), this is exactly the \(_{2}\) distributional eluder of Jin et al. (2021a, Definition 7). We're particularly interested in the \(p=1\) case, which can be used with MLE's generalization bounds. The following is a key pigeonhole principle for the \(_{1}\) distributional eluder dimension.

**Theorem 5.3**.: _Let \(C:=_{d,f}\|_{d}f|\) be the envelope. Fix any \(K\) and sequences \(f^{(1)},,f^{(K)}\), \(d^{(1)},,d^{(K)}\). Let \(\) be a constant such that for all \(k[K]\), we have, \(_{i=1}^{k-1}_{d^{(i)}}f^{(k)}\). Then, for all \(k[K]\), we have_

\[_{t=1}^{k}_{d^{(t)}}f^{(t)}_{0<  1}\{_{1}(,,)(2C+(C/ ))+k\}.\]

As we'll see later, Theorem 5.3 is the key tool that transfers triangular discrimination guarantees on the training distribution to any new test distribution. Another key property is that the \(_{1}\) dimension generalizes the original \(_{2}\) dimension of Jin et al. (2021a).

**Lemma 5.4**.: _For any \(,\) and \(>0\), we have \(_{1}(,,)_{2}(, ,)\)._

Finally, we note that our distributional eluder dimension generalize the regular \(_{1}\) eluder from Liu et al. (2022), which can be seen by taking \(\) to be dirac distributions.

### Small-Loss Bounds for O-Disco

We will soon prove small-loss regret bounds with the "Q-type" dimension, where "Q-type" refers to the fact that \(=\). While low-rank MDPs are not captured by the "Q-type" dimension, they are captured by the "V-type" dimension where \(=\)(Jin et al., 2021; Du et al., 2021). For PAC bounds with the V-type dimension, we need to slightly modify the data collection process in Line 6 with _uniform action exploration_ (UAE). Instead of executing \(^{k}\) for a single trajectory, partially roll-out \(^{k}\) for \(H\) times where for each \(h[H]\), we collect \(x_{h,k} d_{h}^{^{k}}\), take a random action \(a_{h,k}()\), observe \(c_{h,k} C_{h}(x_{h,k},a_{h,k}),x^{}_{h,k} P_{h}(x_{h,k},a_{h,k})\) and augment the dataset \(_{h,k}=_{h,k-1}\{(x_{h,k},a_{h,k},c_{h,k},x^{ }_{h,k})\}\). The modified algorithm is detailed in Appendix B.

We lastly need to define the function and distribution classes measured by the distributional eluder dimension. The Q-type classes are \(_{h}=\{(x,a) d_{h}^{}(x,a):\}\) and \(_{h}=\{(x,a) D_{}(f(x,a)^{,D}f(x,a)):f\}\). Similarly, the V-type classes are \(_{h,v}=\{x d_{h}^{}(x):\}\) and \(_{h,v}=\{x_{a()}[D_{ }(f(x,a)^{,D}f(x,a))]:f\}\). Finally, define \(_{1}()=_{h}_{1}(_{h},_{h}, )\) and \(_{1,v}()=_{h}_{1}(_{h,v}, _{h,v},)\).

**Theorem 5.5**.: _Suppose DistBC holds (Assumption 5.1). For any \((0,1)\), w.p. at least \(1-\), running O-DISCO with \(=(HK||/)\) guarantees the following regret bound,_

\[_{}(K) 160H\,_{1}(1/ K)(K)}+18000H^{2}\,_{1}(1/K)(K).\]

_If \(=\) (Algorithm 4), then the learned mixture policy \(\) is guaranteed to satisfy,_

\[V^{}-V^{} 160H\,_{1,v}(1/ K)(K)}{K}}+A\,_{1,v}(1/K)(K)}{K}.\]

Compared to prior bounds for GOLF (Jin et al., 2021), the leading \(\) terms in our bounds enjoy the same sharp dependence in \(H,K\) and the eluder dimension. Our bounds further enjoy one key improvement: the leading terms are multiplied with the instance-dependent optimal cost \(V^{}\), giving our bounds the _small-loss_ property. For example, if \(V^{}(1/)\), then our regret bound converges at a fast \((H^{2}\,_{1}(1/K)(K))\) rate. While there are existing first-order bounds in online RL, our bound significantly improves on their generality. For example, Zanette and Brunskill (2019); Jin et al. (2020); Wagenmaker et al. (2022) used Bernstein bonuses that scale with the conditional variance and showed that careful analysis can lead to "small-return" bounds in tabular and linear MDPs. However, "small-return" bounds do not imply "small-loss" bounds and "small-loss" bounds are often harder to obtain4. While it is possible that surgical analysis with variance bonuses can lead to small-loss bounds in tabular and linear MDPs, this approach may not scale to settings with non-linear function approximation such as low-rank MDPs.

On Bellman CompletenessExponential error amplification can occur in online and offline RL under only realizability of \(Q\) functions (Wang et al., 2021; 20; 20; Foster et al., 2022). With only realizability, basic algorithms such as TD and Fitted-\(Q\)-Evaluation (FQE) can diverge or converge to bad fixed point solutions (Tsitsiklis and Van Roy, 1996; Munos and Szepesvari, 2008; Kolter, 2011). As a result, BC has risen as a _de facto_ sufficient condition for sample efficient RL (Chang et al., 2022; Xie et al., 2021; Zanette et al., 2021). Finally, we highlight that our method can be easily extended to hold under _generalized completeness_, _i.e._, there exist function classes \(_{h}\) such that \(f_{h+1}_{h+1}_{h}^{,D}f_{h+1}_{h}\) [as in Jin et al., 2021, Assumption 14]. Simply replace \(_{g_{h}}\) in the confidence set construction with \(_{g_{h}}\). While adding functions to \(\) may break BC (as BC is not monotonic), we can always augment \(\) to satisfy generalized completeness.

Computational complexityWhen taken as is, OLIVE (Jiang et al., 2017), GOLF, and our algorithms are version space methods that suffer from a computational drawback: optimizing over the confidence set is NP-hard (Dann et al., 2018). However, the confidence set is purely for deep exploration via optimism and can be replaced by other computationally efficient exploration strategies. For example, \(\)-greedy suffices in problems that don't require deep and strategic exploration, _i.e._, a large myopic exploration gap (Dann et al., 2022). With \(\)-greedy, a replay buffer, and discretization, our algorithm essentially recovers C51 (Bellemare et al., 2017). We leave developing and analyzing computationally efficient algorithms based on our insights as promising future work.

### Instantiation with Low-Rank MDPs

The low-rank MDP (Agarwal et al., 2020) is a standard abstraction for non-linear function approximation used in theory (Uehara et al., 2021) and practice (Zhang et al., 2022, Chang et al., 2022).

**Definition 5.6** (Low-rank MDP).: A transition model \(P_{h}:()\) has rank \(d\) if there exist unknown features \(_{h}^{}:^{d},_{h}^{ }:^{d}\) such that \(P_{h}(x^{} x,a)=_{h}^{}(x,a)^{}_{h}^{}(x^{})\) for all \(x,a,x^{}\). Also, assume \(_{x,a}\|_{h}^{}(x,a)\|_{2} 1\) and \(\| g_{h}^{}\|_{2}\|g\|_{}\) for all functions \(g:\). The MDP is called low-rank if \(P_{h}\) is low-rank for all \(h[H]\).

We now specialize Theorem 5.5 to low-rank MDPs with three key steps. First, we bound the V-type eluder dimension by \(_{1,v}()(d(d/))\), which is a known result that we reproduce in Theorem G.4. The next step requires access to a realizable \(\) class, _i.e._, for all \(h[H]\), \(_{h}^{}\), which is a standard assumption for low-rank MDPs (Agarwal et al., 2020, Uehara et al., 2021, Mhammedi et al., 2023). Given the realizable \(\), we can construct a specialized \(\) for the low-rank MDP: \(^{}=_{1}^{} _{H}^{}_{H+1}^{}\) where \(_{H+1}^{}=\{_{0}\}\) and for all \(h[H]\),

\[_{h}^{}= f(z x,a)=(x,a),w(z): ,w:^{d},\] (2) s.t. \[_{z}\|w(z)\|_{2}\;_{x,a,z}(x,a),w(z)},\]

where \(:=_{h,,z,x,a}Z_{h}^{}(z x,a)\) is the largest mass for the cost-to-go distributions. In Appendix D, we show that \(^{}\) satisfies DistBC. Further, if costs are discretized into a uniform grid of \(M\) points, its bracketing entropy is bounded by \(}(dM+||)\). Discretization is necessary to bound the statistical complexity of \(^{}\) and is common in practice, _e.g._, C51 and Rainbow both set \(M=51\) which works well in Atari games (Bellemare et al., 2017, Hessel et al., 2018).

**Theorem 5.7**.: _Suppose the MDP is low-rank. For any \((0,1)\), w.p. at least \(1-\), running O-DISCO with UAE=True and with \(^{}\) as described above learns a policy \(\) such that,_

\[V^{}-V^{}}HV^{}(dM+(||/))}{K}}+H^{2}(dM+(||/ ))}{K}.\]

Proof.: As described above, we have \(_{1}(1/K)(d(dK))\) and \(=(HK/)+dM+||\). Since DistBC is satisfied by \(^{}\), plugging into Theorem 5.5 gives the result. 

This is the first small-loss bound for low-rank MDPs, and for online RL with non-linear function approximation in general. Again when \(V^{}}(1/K)\), O-DISCO has a fast \(}(1/K)\) convergence rate which improves over all prior results that converge at a slow \((1/)\) rate (Uehara et al., 2021).

### Proof Sketch of Theorem 5.5

By DistBC (Assumption 5.1), we can deduce two facts about the construction of \(_{k}\): (i) \(Z^{}_{k}\), and (ii) elements of \(_{k}\) almost satisfy the distributional Bellman equation, _i.e._, for all \(h[H]\), we have \(_{i=1}^{k}_{^{i}}[_{h,k}(x_{h},a_{h})]()\) where \(_{h,k}(x_{h},a_{h})=D_{}(f_{h}^{(k)}(x_{h},a_{h}) _{h}^{,D}f_{h+1}^{(k)}(x_{h},a_{h}))\). Next, we derive a corollary of Eq. (\(_{1}\)):

\[-+D_{}(f g)} (f g)}.\] ( \[_{2}\] )

To see why this is true, apply AM-GM to Eq. (\(_{1}\)) to get \(2(-)++D_{}(f g)\), which simplifies to \( 3+D_{}(f g)\). Plugging this back into Eq. (\(_{1}\)) yields Eq. (\(_{2}\)). Then, by iterating Eq. (\(_{2}\)) and AM-GM, we derive a self-bounding lemma: for any \(f,,h\), we have \(_{h}(x_{h},a_{h}) Q_{h}^{}(x_{h},a_{h})+H_{t=h}^{H} _{,x_{h},a_{h}}[D_{}(f_{t}(x_{t},a_{t}) _{h}^{,D}f_{t+1}(x_{t},a_{t}))]\) (Lemma H.3).

Since \(_{h}^{^{k}}^{(k)}_{h+1}(x,a)=_{h}^{^{k},D}f^{(k )}_{h+1}(x,a)\) and \(_{h}^{^{k},D}f^{(k)}_{h+1}=_{h}^{,D}f^{(k)}_{h+1}\), we have

\[V^{^{k}}-V^{}  V^{^{k}}-^{(k)}_{1}(x_{1},^{k}_{1}(x_{1}))\] (optimism from fact (i)) \[=_{h=1}^{H}_{^{k}}_{h}^{^{k }}^{(k)}_{h+1}(x_{h},a_{h})-^{(k)}_{h}(x_{h},a_{h}) \] \[ 2_{h=1}^{H}_{^{k}}[^{(k)}_{h}( x_{h},a_{h})+_{h,k}(x_{h},a_{h})]}_{^{k}}[_{h,k}(x_{h},a_{h})]}\] (Eq. ( \[_{2}\] ) \[}w+H_{h=1}^{H}_{^{k}}[ _{h,k}(x_{h},a_{h})]}_{^{k}}[_{h,k}(x_{h},a_{h })]}.\] (Lemma H.3 )

The implicit inequality \(V^{^{k}}-V^{}+H_{h=1}^{H}_{^{ k}}[_{h,k}(x_{h},a_{h})]}_{^{k}}[_{h,k}(x_{h},a_{h })]}\) can then be obtained by AM-GM and rearranging. The final step is to sum over \(k\) and bound \(_{k=1}^{K}_{^{k}}[_{h,k}(x_{h},a_{h})]\) via the eluder dimension's pigeonhole principle (Theorem 5.3 applied with fact (ii)). Please see Appendix H for the full proof.

## 6 Small-Loss Bounds for Offline Distributional RL

We now propose **P**essimistic **D**n**f**ubi**tional **C**onfidence set **O**ptimization (P-DISCO; Algorithm 3), which adapts the distributional confidence set technique from the previous section to the offline setting by leveraging pessimism instead of optimism. Notably, P-DISCO is a simple two-step algorithm that achieves the first small-loss PAC bounds in offline RL. First, construct a distributional confidence set for each policy \(\) based on a similar log-likelihood thresholding procedure as in O-DISCO, where the difference is we now use data sampled from \(_{h}^{,D}f_{h+1}\) instead of \(_{h}^{,D}f_{h+1}\). Next, output the policy with the most pessimistic mean amongst all the confidence sets.

```
1:Input: datasets \(_{1},,_{H}\), distribution function class \(\), threshold \(\), policy class \(\).
2: For all \((h,f,)[H]\), sample \(y^{f,}_{h,i} f_{h+1}(x^{}_{h,i},_{h+1}(x^{}_{h,i}))\), where \((x_{h,i},a_{h,i},c_{h,i},x^{}_{h,i})\) is the \(i\)-th datapoint of \(_{h}\). Then, set \(z^{f,}_{h,i}=c_{h,i}+y^{f,}_{h,i}\) and define the confidence set, \[_{}=f:_{i=1}^{N} f_{h}(z^{f,}_ {h,i} x_{h,i},a_{h,i})_{g_{h}}_{i=1}^{N} g(z^ {f,}_{h,i} x_{h,i},a_{h,i})-7, h[H]}.\]
3: For each \(\), define the pessimistic estimate \(f^{}=_{f_{}}_{a(x_{1})} _{1}(x_{1},a)\).
4:Output:\(=_{}_{a(x_{1})} ^{}_{1}(x_{1},)\). ```

**Algorithm 3**P**essimistic **D**n**l**n**fubi**tional **C**onfidence set **O**ptimization (P-DISCO)

In offline RL, many works made strong all-policy coverage assumptions (Antos et al., 2008; Chen and Jiang, 2019). Recent advancements (Kidambi et al., 2020; Xie et al., 2021; Uehara and Sun, 2022; Rashidinejad et al., 2021; Jin et al., 2021b) have pursued _best effort_ guarantees that aim to compete with any covered policy \(\), with sub-optimality of the learned \(\) degrading gracefully as coverage worsens. The coverage is measured by the single-policy concentrability \(C^{}=_{h}^{d}_{h}/d_{h}}_{}\). We adopt this framework and obtain the first small-loss PAC bound in offline RL.

**Theorem 6.1** (Small-Loss PAC bound for P-DISCO).: _Assume Assumption 5.1. For any \((0,1)\), w.p. at least \(1-\), running P-DISCO with \(=(H||||/)\) learns a policy \(\) that enjoys the following PAC bound with respect to any comparator policy \(\):_

\[V^{}-V^{} 9H}V^{ }}{N}}+C^{}}{N}.\]

To the best of our knowledge, this is the first small-loss bound for offline RL, which we highlight illustrates a novel robustness property against bad coverage. Namely, the dominant term not only scales with the coverage coefficient \(C^{}\) but also the comparator policy's value \(V^{}\). In particular, P-DISCO can strongly compete with a comparator policy \(\) if _one of the following_ is true: (i) \(\) has good coverage over \(\), so the \((1/)\) term is manageable; _or_ (ii) \(\) has small-loss, in which case we may even obtain a fast \((1/N)\) rate. Thus, P-DISCO has _two_ chances at strongly competing with \(\), while conventional offline RL methods solely rely on (i) to be true.

## 7 Distributional CB Experiments

We now compare DistCB with SquareCB (Foster and Rakhlin, 2020) and the state-of-the-art CB method FastCB (Foster and Krishnamurthy, 2021), which respectively minimize the squared loss and log loss for estimating the conditional mean. The key question we investigate here is whether learning the conditional mean via distribution learning with MLE will demonstrate empirical benefit over the non-distributional approaches. We consider three challenging tasks that are all derived from real-world datasets and we briefly describe the construction below.

King County HousingThis dataset consists of home features and prices, which we normalize to be in \(\). The action space is \(100\) evenly spaced prices between \(0.01\) and \(1.0\). If the learner overpredicts the true price, the cost is \(1.0\). Else, the cost is \(1.0\) minus predicted price.

Prudential Life InsuranceThis dataset contains customer features and an integer risk level in \(\), which is our action space. If the model overpredicts the risk level, the cost is \(1.0\). Otherwise, the cost is \(.1(y-)\) where \(y\) is the actual risk level, and \(\) is the predicted risk level.

Cifar-100This popular image dataset contains \(100\) classes, which correspond to our actions, and each class is in one of \(20\) superclasses. We assign cost as follows: \(0.0\) for predicting the correct class, \(0.5\) for the wrong class but correct superclass, and \(1.0\) for a fully incorrect prediction.

ResultsAcross tasks, DistCB achieves lower average cost over all episodes (_i.e._, normalized regret) and over the last \(100\) episodes (_i.e._, most updated policies' performance) compared to SquareCB. This indicates the empirical benefit of the distributional approach over the conventional approach based on least square regression, matching the theoretical benefit demonstrated here. Perhaps surprisingly, DistCB also consistently outperforms FastCB. Both methods obtain first-order bounds with the same dependencies on \(A\) and \(C^{}\), which suggests that DistCB's empirical improvement over FastCB cannot be fully explained by existing theory. The only difference between DistCB and FastCB is that the former integrates online MLE while the latter directly estimates the mean by minimizing the log loss (binary cross-entropy). An even more fine-grained understanding of the benefits of distribution learning may therefore be helpful in explaining this improvement. Appendix K contains all experiment details. Reproducible code is available at https://github.com/kevinzhou497/distcb.

## 8 Conclusion

We showed that distributional RL leads to small-loss bounds in both online and offline RL, and we also proposed a distributional CB algorithm that outperforms the state-of-the-art FastCB. A fruitful direction would be to investigate connections of natural policy gradient with our MLE distributional-fitting scheme to inspire a practical offline RL algorithm with small loss guarantees, _a la_Cheng et al. (2022). Finally, it would be interesting to investigate other loss functions that yield small-loss or even faster bounds.

AcknowledgementsThis material is based upon work supported by the National Science Foundation under Grant Nos. IIS-1846210 and IIS-2154711.

   Algorithm: & SquareCB & FastCB & DistCB (Ours) \\  King County Housing (Vanschoren et al., 2013) & & \\  All episodes.756 (.0007).734 (.0007) & **.726** (.0003) \\ Last 100 ep..725 (.0012).719 (.0013) & **.708** (.0019) \\   Prudential Life Insurance (Montoya et al., 2015) & & \\  All episodes.456 (.0082),491 (.0029) & **.411** (.0038) \\ Last 100 ep..481 (.0185).474 (.0111) & **.388** (.0086) \\   CIFAR-100 (Krizhevsky, 2009) & & \\  All episodes.872 (.0010).856 (.0016) & **.838** (.0021) \\ Last 100 ep..828 (.0024).793 (.0031) & **.775** (.0027) \\   

Table 1: Avg cost over all episodes and last 100 episodes (lower is better). We report ‘mean (sem)’ over \(10\) seeds.