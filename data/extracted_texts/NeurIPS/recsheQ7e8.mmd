# Aligning to Thousands of Preferences

via System Message Generalization

Seongyun Lee\({}^{1}\)1 & Sue Hyun Park\({}^{1}\)1 & Seungone Kim\({}^{2}\) & Minjoon Seo\({}^{1}\)

KAIST AI\({}^{1}\) & Carnegie Mellon University\({}^{2}\)

{seongyun, suehyunpark, minjoon}@kaist.ac.kr & seungone@cmu.edu

###### Abstract

Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of _scalability_, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the **system message**, steering the LLM's generation behavior to better align with the user's intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (_e.g._, "You are a helpful assistant"), which limits their ability to generalize to diverse, unseen system messages. To improve this generalization, we create Multifaceted Collection, augmenting 66k user instructions into 197k system messages through hierarchical user value combinations. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding system messages that reflect unseen user values. Janus achieves it+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 BB Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well. Our code, dataset, benchmark, and models are available at https://lklab.kaist.ac.kr/Janus/.

## 1 Introduction

Post-training techniques such as reinforcement learning from human feedback (RLHF) or instruction fine-tuning are effective at enhancing the alignment of large language models (LLMs) with human preferences . These methods involve collecting preference data based on high-level values that many people agree upon, such as helpfulness and harmlessness, and then using this data to train reward models (RMs) and LLMs. However, human preferences cannot simply be categorized into such binary divisions; they are typically diverse and exhibit nuanced variations according to different people and contexts . As a result, a significant amount of data is created without considering the conflicting preferences that individuals may have, which results in annotation disagreement . Training LLMs on generic, unattributed preference data inadvertently leads to undesirable traits like verbosity .

To address these issues, a new paradigm called **personalized RLHF** has been proposed. In this paradigm, RMs or LLMs are trained to reflect individual preferences, aiming to model the diverse preferences that people may have . However, prior works require collecting data that reflects fine-grained preferences each time and then re-training the RMs and LLMs. Considering that the size of LLMs used in production is steadily increasing, this method may not be highly scalable in terms of time, storage cost, and training cost .

In this paper, we pose the following question: _Can we align to diverse preferences without the need to re-train the LLM for each user's preferences?_ Drawing inspiration from how pretrain-then-finetune evolved to handle unseen tasks without re-training via instruction fine-tuning , we explore to adapt to user preference without re-training. Specifically, we examine how explicitly stating the user's preference as meta-instructions in the form of _system messages_ can guide the LLM's behavior to align with the user's intentions . However, in our early experiments, we observed that open-source LLMs, unlike proprietary LLMs, do not possess this ability. We hypothesize that this is because open-source LLMs are typically trained with a generic system message, such as "You are a helpful assistant" . Consequently, their ability to incorporate new, unseen system message is limited. To address this, we propose training LLMs on diverse system messages, allowing the LLM to adapt to novel preferences during test time.

To acquire sufficiently diverse and multifaceted values that control individual preferences, we devise a hierarchical synthesis of user values from high-level dimensions. Verbalizing combinations of user values into system messages, we create a training dataset of 197k system messages, **Multifaceted Collection**. For each instruction, there are three variants of system messages embedded with non-overlapping values, along with respective responses. Through quantitative and qualitative analyses, we show that the dataset encompasses a variety of preferences.

To test if our strategy helps aligning to various system messages, we train Mistral 7B v0.2  on the Multifaceted Collection and obtain **Janus 7B**. In addition, we augment 315 instructions across 5 LLM benchmarks (AlpacaEval 2.0 , FLASK , Koala , MT-Bench , and Self-Instruct ) with three unseen context-specific system messages verified by humans per instruction, creating 921 unseen system messages. Human evaluation demonstrates that Janus achieve a 74.8%, 70.8%, and 57.9% win rate compared to Mistral 7B Instruct v0.2, GPT-3.5-Turbo-0125, and GPT-4-Turbo-0125, respectively. GPT-4 evaluation results show similar trends, where Janus achieves an average score of 4.24 out of 5.0, outperforming LLMaA 3 BB Instruct, Mistral 8x7B Instruct v0.1, and GPT-3.5-Turbo-0125 by 5.7% on average. Surprisingly, when assessing the helpfulness of the response on 3 benchmarks (AlpacaEval 2.0 , MT-Bench , Arena Hard Auto v0.1 ), Janus outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, and +0.3%p margin, suggesting that training on diverse system messages enhances both individualized and general alignment. Further analyses reveal that explicit exposure to diverse preferences during training is crucial, ensuring robust performance even without system messages at test time.

Figure 1: Previous LLMs are trained with homogeneous system messages reflecting general helpfulness and harmlessness. We propose training LLMs with diverse system messages, each representing an individualâ€™s multifaceted preferences, to generalize to unseen system messages. The resulting model, Janus 7B, is adept at generating personalized responses for personalized system messages.

Related work

Personalized RLHF for aligning to diverse preferencesAs humans exhibit varying preferences and values for a single task, it is essential to develop systems capable of representing diverse perspectives [32; 72; 73; 86]. A majority of studies build on the RLHF pipeline, designing customized reward functions to prevent rewarding individualized outputs in a _one-size-fits-all_ manner. To address this issue, a recent stream of research has focused on modeling the distribution of human preferences embedded in annotated data and subsequently calibrating the reward model to align with these preferences [38; 71; 80]. These studies aim to reduce annotation disagreements by more accurately simulating the views of a target population. Another line of research highlights the insufficiency of a single reward model and focuses on learning a mixture of preference distributions , training individual reward models signaling a specific preference , or jointly training a separate user model and reward model on user information and user preference data . To obtain Pareto-optimal generalization across a spectrum of preferences, several works use weight merging techniques to composite multiple policy models each trained with different reward models [25; 62]. While the aforementioned approaches predominantly involve re-training reward models and LLMs to adapt to new preferences, often proving impractical due to the multitude of potential preference variants, we propose to train an LLM that could conform when explicitly stated during test time.

Utilizing system messages for contextProviding context to an LLM is as easy as giving a textual prompt as input. When a user desires to instill specific behavior when performing a task into an LLM, such as impersonating a role [57; 64] or personalization , a prevalent method is to simply include the context as part of the user instruction. The component of the model input specialized for such purpose, coined as _system message_, is introduced with ChatGPT2. For closed-source models accessible by API services, system messages can be set by developers in the API request (e.g., Mistral3, Claude4, Command R5). This feature facilitates analysis of the behaviors of top-performing LLMs by diversifying social roles and personalities in system messages [29; 93]. Open-source models report to have trained with specific system messages in an input template include Orca  and LaMA 2 , with the purpose of invoking step-by-step explanation or maintaining consistency of responses, respectively. While these works corroborate that diversifying system messages instead of default system messages improves performance, the content of system messages studied in previous works is limited in number and domain, making it insufficient to observe the entire space of human preferences. Similar to scaling instruction to improve LLM's capability to solve unseen tasks , we scale system messages into multifaceted variants, allowing the model to steer generations to be aligned with user's preferences for the same instruction.

Instruction tuningTraditional instruction tuning approaches [20; 46; 47; 65; 82; 83] focus on improving model performance across diverse tasks by training on varied instructions. Our work extends this concept by leveraging system messages to encode user preferences, providing a novel approach to individualized alignment. While instruction tuning typically embeds task-specific instructions within user prompts, we utilize system messages to separate preference specifications from task instructions. Concretely, we view system messages as _meta-instructions_ that guide a model how to respond to subsequent instructions, which allows for more nuanced control over model behavior in specific training settings . Our approach curates meta-instructions to specifically simulate user values instead of presenting irrelevant challenges or hinting solutions with respect to the instruction. To the best of our knowledge, there is no work that shares a similar motivation with us or has publicly released a training dataset containing meta-instructions.

## 3 Multifaceted Collection for scalable individualized alignment

### Mapping a preference to multidimensional values

Existing alignment datasets often vaguely reflect general preferences such as helpfulness and harmlessness. Our aim is to develop a dataset that captures more _fine-grained_ preferences. Specifically,we posit that even for the same instruction, the choice of which response to select or reject (i.e., preference) may differ among individuals due to their unique set of _values_. For example, given a coding problem, one response focused on code and another centered on explaining concepts might each be chosen as preferable over the other, depending on what the user values as informative. We identify two requirements for a model to effectively reflect such diversity of human preferences:

* **Multifacetedness**: Multiple facets can dictate an individual's preference, not just one. A sufficiently large space of potential facets should be captured in order to model preferences of countless individuals.
* **Explicitness**: Preferences only latently exist in a pair of chosen and rejected responses . To facilitate learning to discriminate the nuances in between a chosen response and a rejected response, the rationale of the decision should be made explicit to the model.

Taking the requirements into account, we develop a novel dataset construction approach as illustrated in Figure 2. To address **R1**, we devise a hierarchical value augmentation strategy: starting from the \(n\) most general dimensions, \(m\) specific subdimensions are branched, and \(l\) values are created under each subdimension. Assuming \(n m l\), this structure ensures that a variety of facets are defined. Ultimately combining values from different dimensions, which we define as a _preference set_, can effectively represent the complex interplay of values to determine a preference. To satisfy **R2**, we conceptualize a value as a detailed textual description of a quality that a desirable response should possess. Verbalization helps the model become more aware of preference patterns  and provides interpretable signals to induce better decisions . The verbalized preference set is contextualized in the model input along with the original instruction given by the user. Precisely, it serves as a _meta-instruction_ which sets a specific direction for the instruction to be executed and hence the additional context is included in the _system message_ prepended to the instruction.

### Instruction data construction

Instruction fine-tuning can achieve strong generalization on unseen tasks given a sufficiently large training set of tasks and instructions . To provide a ground for unseen system message generalization and therefore scalable individualized alignment, we implement a LLM-driven data synthesis pipeline to scale the number of multifaceted system messages.

Instruction samplingWe first select 66k instructions from a pool of existing four high-quality preference datasets: Chatbot Arena Conversations , Domain-Specific Preference dataset , UltraFeedback-binarized-clean , Nectar , and OpenHermesPreferences . These datasets curate or rank responses with variants of helpfulness or harmlessness criteria, signifying that the input

Figure 2: Multifaceted Collection construction process. For each instruction, value descriptions are augmented from general to specific, allowing for multiple facets to branch out. We combine values from various dimensions into a system message to materialize preferences into model input. Following the system message and instruction, a proprietary LLM generates a gold response for training.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

Janus consistently surpasses other models when judged by an LLM.Table 2 shows LLM-as-a-Judge evaluation results on Multifaceted Bench. Janus 7B scores higher than its base model, Mistral 7B v0.2 (+33%), and its instruction-tuned counterpart, Mistral 7B Instruct v0 (+4.6%). It also competes well with larger models like LLaMA 2 Chat 70B (+9%) and Mistral 8x7B Instruct v0.1 (+3.8%), only slightly trailing behind LLaMA 3 Instruct 70B. Janus remain competitive against proprietary models like GPT-3.5-Turbo (+7.6%) and GPT-4 (+3.6%). Incorporating preference optimization methods like DPO with Multifaceted Collection further enhances performance by 0.4%, marking the highest achievement within the Janus suite.

Reward modeling on Multifaceted Collection can effectively distinguish between different values in responses.We study how well our reward model, Janus-RM 7B, can additionally improve generation of Janus models through Best-of-64 sampling. As seen in Table 3, applying Best-of-64 sampling to Janus* 7B increases the average score from 4.14 to 4.28. When used with Janus+DPO 7B, the score further improves from 4.24 to 4.31, marking the highest performance within the Janus suite. The improvements suggests that a single reward model trained on diverse values can enhance multifaceted response generation without the need of multiple reward models.

Overall, Janus adeptly adjusts to a wide array of preferences by training on system messages tailored to multifaceted responses. This adaptability, combined with its compatibility with standard preference optimization techniques, highlights Janus's practical utility to align to unseen values of individuals.

### General helpfulness

Janus outperforms similarly-sized or even larger models in general benchmarks.Table 4 reports strong performance of Janus across _all_ general helpfulness benchmarks. In AlpacaEval 2.0, Janus 7B shows higher length-controlled (LC) win rates than similarly sized models, Mistral 7B Instruct v0.2 (+9.8%p), Gemma 7B Instruct (+16.5%p), and LLaMA 3 8B Instruct (+4%p). It also surpasses larger models like Vicuna 33B v1.3 (+10.3%p), Mistral 8x7B Instruct v0.1 (+3.2%p,),

   Model & _mf_-**AlpacaEval** & _mf_-**FLASK** & _mf_-**Koala** & _mf_-**MT-Bench** & _mf_-**Self-Instruct** & **Average** \\   \\  Mistral 7B v0.2 & 2.80 & 1.93 & 2.45 & 2.30 & 2.28 & 2.23 \\ LLaMA 3 8B & 2.60 & 2.92 & 2.69 & 2.39 & 2.34 & 2.54 \\ LLaMA 3 70B & **3.76** & **3.23** & **3.67** & **3.50** & **3.65** & **3.49** \\   \\  LLaMA 2 Chat 70B & 3.98 & 3.68 & 4.11 & 3.66 & 3.87 & 3.79 \\ Mistral 7B Instruct v0.2 & 4.20 & 3.82 & 4.18 & 3.82 & 3.98 & 3.93 \\ Mistral 8x7B Instruct v0.1 & 4.24 & 3.90 & 4.16 & 3.94 & 4.08 & 4.03 \\ LLaMA 3 Instruct 8B & 4.38 & 3.88 & 4.33 & 4.08 & 4.17 & 4.10 \\ LLaMA 3 Instruct 70B & **4.55** & **4.26** & **4.59** & **4.42** & **4.45** & **4.39** \\   \\ 
**Janus 7B** & 4.43 & 4.06 & 4.41 & 4.11 & 4.01 & 4.17 \\
**Janus+ORPO 7B** & 4.41 & 4.03 & **4.45** & 4.00 & **4.22** & 4.18 \\
**Janus+DPO 7B** & **4.45** & **4.13** & 4.43 & **4.21** & 4.17 & **4.24** \\   \\  GPT-3.5 Turbo-0125 & 4.05 & 3.86 & 4.15 & 3.87 & 3.85 & 3.91 \\ GPT-4-0613 & 4.25 & 4.00 & 4.18 & 4.16 & 4.13 & 4.10 \\ GPT-4-Turbo-0125 & **4.45** & **4.27** & **4.61** & **4.45** & **4.27** & **4.35** \\   

Table 2: Multifaceted Bench results. For each modelâ€™s response, an evaluator LLM assigns a score ranging from 1 to 5. The average score is calculated by averaging all the scores for each sample. To ensure consistency, all scores are evaluated three times, and the averaged result is recorded.

   Models & _mf_-**AlpacaEval** & _mf_-**FLASK** & _mf_-**Koala** & _mf_-**MT-Bench** & _mf_-**Self-Instruct** & **Average** \\  Janus* 7B & 4.41 & 4.02 & 4.36 & 4.08 & 4.03 & 4.14 \\ + Best-of-64 & 4.41 & **4.26** & 4.42 & 4.17 & 4.21 & 4.28 \\ Janus+DPO 7B & 4.45 & 4.13 & 4.43 & **4.21** & 4.17 & 4.24 \\ + Best-of-64 & **4.49** & 4.24 & **4.48** & 4.20 & **4.28** & **4.31** \\   

Table 3: Best-of-64 sampling results on Multifaceted Bench using Janus-RM 7Bas well as the proprietary model GPT-3.5-Turbo (+4.2%p). In MT-Bench's absolute evaluation setting, Janus 7B matches or exceeds other models under 30B in size, scoring 7.7. Although larger and proprietary models outperform Janus 7B, its results are commendable given its scale. Performance focused on multi-turn scenarios is reported in Appendix A. Additionally in Arena Hard Auto v0.1, Janus 7B scores 20.9, outdoing both smaller LLMs and larger models, including GPT-3.5-Turbo-0125, Vicuna 33B v1.3, and Tulu 2+DPO 70B.

Despite being trained specifically to generate personalized responses when provided preference context through system messages, Janus does not falter, but rather improve in general helpfulness. It is suggested that system message generalization not only supports the creation of personalized LLMs but also improves alignment with what humans generally perceives as helpful.

### Diversity and harmlessness

Janus simultaneously achieves low toxicity and high diversity.Table 5 shows RealToxicityPrompts  results of decoding-time algorithms and instruction-tuned models. Janus 7B shows a significantly lower average maximum toxicity and toxicity probability than other instruction-tuned models. Moreover, unlike traditional methods such as  where reducing toxicity could compromise performance, Janus 7B manages to maintain lower toxicity while still achieving high fluency and diversity scores. We also observe moderate performance in social bias benchmarks in Appendix B.

    &  &  &  & } \\    & & LC Win Rate (\%) & Win Rate (\%) & Score  & Score  \\   & GPT-3.5-Turbo-1106\({}^{}\) & 19.3 & 9.2 & 8.3 & 18.9 \\  & GPT-3.5-Turbo-0125\({}^{}\) & 22.7 & 14.1 & 8.4 & 24.8 \\  & GPT-4.0613\({}^{}\) & 30.2 & 15.8 & **9.2** & 37.9 \\  & Claude 3 Opus (02/29)\({}^{}\) & 40.5 & 29.1 & 9.0 & 60.4 \\  & GPT-4.Turbo-0409\({}^{}\) & **55.0** & **46.1** & - & **82.6** \\   & Vicuna 33B v1.3\({}^{}\) & 17.6 & 12.7 & 7.1 & 8.6 \\  & Tulu 2+DPO 70B\({}^{}\) & 21.2 & 16.0 & 7.9 & 15.0 \\  & Yi 34B Chat\({}^{}\) & 27.2 & 29.7 & - & 23.1 \\  & Mixtral 8x7B Instruct v0.1\({}^{}\) & 23.7 & 18.3 & 8.3 & 23.4 \\  & Mixtral 8x22B Instruct v0.1\({}^{}\) & 30.9 & 22.2 & 8.7 & 36.4 \\  & LLaMA 3 70B Instruct\({}^{}\) & **34.4** & **33.2** & **8.9** & **41.1** \\   & Mistral 7B Instruct v0.2 & 17.1 & 14.7 & 7.2 & 10.8 \\  & Gemma 7B Instruct & 10.4 & 6.9 & 6.4 & 7.5 \\   & LLaMA 3 3 Bsh Instruct & 22.9 & 22.6 & 7.6 & 17.9 \\   & **Janus 7B** & **26.9** & **27.8** & **7.7** & **20.9** \\   

Table 4: Helpfulness benchmarks results.\({}^{}\) indicates the results from the official leaderboard or paper . Unk. refers to models whose parameter counts are not publicly disclosed. Details regarding the score metric can be found in Appendix L.2.

    &  &  &  \\   & Avg. max toxic & Toxic prob & Output PPL & dist-2 & dist-3 \\  GPT-2\({}^{}\) & 0.53 & 0.52 & **11.31** & 0.85 & 0.85 \\ PPLM\({}^{}\) & 0.52 & 0.52 & 32.58 & **0.86** & **0.86** \\ GeDi\({}^{}\) & 0.36 & 0.22 & 60.03 & 0.84 & 0.83 \\ DExperts\({}^{}\) & 0.31 & 0.12 & 32.41 & 0.84 & 0.84 \\ DAPT\({}^{}\) & 0.43 & 0.36 & 31.21 & 0.84 & 0.84 \\ PPO\({}^{}\) & 0.22 & 0.04 & 14.27 & 0.80 & 0.84 \\ Quark\({}^{}\) & **0.12** & **0.04** & 12.47 & 0.80 & 0.84 \\  Mistral 7B Instruct v0.2 & 0.29 & 0.11 & 19.43 & 0.92 & 0.92 \\ LLaMA 3 Instruct 8B & 0.30 & 0.12 & 28.88 & 0.92 & 0.92 \\
**Janus 7B** & **0.26** & **0.06** & **14.58** & **0.93** & **0.95** \\   

Table 5: Harmlessness analysis on RealToxicityPrompts. \({}^{}\) indicates the results from previous work . Details regarding the score metric can be found in Appendix L.3.

[MISSING_PAGE_EMPTY:9]

responses, 3) no system message with multifaceted responses, and 4) multifaceted system message with multifaceted responses from Multifaceted Collection. Details of default system messages and response collection are in Appendix F. Table 7 shows results using Multifaceted Bench and MT-Bench, with representative baselines for comparison. Training with multifaceted system messages improves both multifacetedness and helpfulness metrics, while default system messages only enhance helpfulness at the cost of reduced multifacetedness. While generating multifaceted responses without system messages shows some improvement, it underperforms compared to models with explicit system messages. This suggests that generating personalized responses without supervision of verbalized preferences is challenging. Overall, exposing user values in the system message and learning to generate personalized responses based on it is an effective strategy for learning the nuances of human preferences.

We also analyze that Janus captures diverse opinions of the human population, and further conduct ablation studies centered on the effect of our hierarchical data generation strategy and data/model scaling. Additional analyses including qualitative analyses are detailed in Appendix C. Comparison of model responses are in Appendix N.

## 7 Discussion

We have introduced an approach that utilizes a unique system message protocol to guide language models to generate responses tailored to nuanced user preferences. Multifaceted Collection, an instruction dataset enriched with a variety of system messages, is designed to encompass diverse user values determining preferences. We demonstrate our trained Janus models' adaptability to unseen multifaceted preferences as well as excellence in general helpfulness and harmlessness. Through analyses, we establish the effectiveness of system message generalization, contributing to developing systems that respect both majority and individual user preferences without requiring per-user fine-tuning.

Broader impact of generalizing system messages instead of user messagesWe identify significant potential in utilizing meta-instructions in system messages rather than in user messages to reach many alignment targets. While some meta-instructions are dependent on specific user messages, it can also be more general. For example, the system message illustrated in Figure 2 is applicable to any kind of coding problems. In practical applications, preferences can be programmatically set as meta-instructions when relevant user queries are submitted, offering two key advantages: 1) automatic reflection of common user needs across various instructions, and 2) reduced user burden of specifying a meta-instruction every time. Applications like ChatGPT already offers features like custom instructions8 and memory controls9: a user can specify their preferences in the custom instruction by oneself (e.g., _When I ask you for math problems, please incorporate visual aids._) or ChatGPT will memorize the details itself from the conversations. We believe that including implicit user preferences in the system message without user specification is an important milestone for seamless chat experience, and a model trained for system message generalization will be necessary.

Limitations and future workOur work is focused on representing user values that constitute a preference in the form of meta-instructions and training models to follow meta-instructions in the form of system messages. While our approach shows promise, several limitations warrant consideration. The reliance on synthetic training data presents inherent challenges, as direct human evaluation of the entire Multifaceted Collection dataset remains unfeasible. Although the performance of Janus suggests generally high data quality and demonstrates lower toxicity levels compared to baseline benchmarks, the flexibility in preference specification may increase vulnerability to malicious attacks such as jailbreaking. To address these concerns, we incorporate safety considerations by including a harmlessness dimension in every system message and implement safeguards in our data generation process. However, the challenge of capturing implicit preferences from previous conversations to form system messages appropriately and managing system message controls remains application-dependent and beyond our current scope. Future work should explore these challenges through dialogue summarization techniques [91; 95], along with enhancing safety through increased safety-related training data and integration with moderation techniques such as LLaMA-Guard .