# Memory-Efficient LLM Training with Online Subspace Descent

Kaizhao Liang\({}^{}\), Bo Liu\({}^{}\), Lizhang Chen\({}^{}\), Qiang Liu\({}^{}\)

\(\)The University of Texas at Austin

{kaizhaol,bliu,lzchen,lqiang}@utexas.edu

###### Abstract

Recently, a wide range of memory-efficient LLM training algorithms have gained substantial popularity. These methods leverage the low-rank structure of gradients to project optimizer states into a subspace using projection matrix found by singular value decomposition (SVD). However, convergence of these algorithms is highly dependent on the update rules of their projection matrix. In this work, we provide the _first_ convergence guarantee for arbitrary update rules of projection matrix. This guarantee is generally applicable to optimizers that can be analyzed with Hamiltonian Descent, including most common ones, such as LION, Adam. Inspired by our theoretical understanding, we propose Online Subspace Descent, a new family of subspace descent optimizer without SVD. Instead of updating the projection matrix with eigenvectors, Online Subspace Descent updates the projection matrix with online PCA. Online Subspace Descent is flexible and introduces only minimum overhead to training. We show that for the task of pretraining LLaMA models ranging from 60M to 7B parameters on the C4 dataset, Online Subspace Descent achieves lower perplexity and better downstream tasks performance than state-of-the-art low-rank training methods across different settings and narrows the gap with full-rank baselines.1

## 1 Introduction

The continual advancement in training large language models (LLMs) presents a compelling challenge in balancing computational efficiency with model performance. As the scope and complexity of these models grow, so does the necessity for innovative strategies that optimize memory usage without compromising the learning capabilities of the model. Recent approaches in low-rank adaptation strategies, including Stochastic Subspace Descent , LoRA , ReLoRA , Gradient Low-Rank Projection (GaLore)  and Sketchy , have paved the way for memory-efficient training by utilizing a periodically updated low-rank projection matrix to manage parameter updates. In particular, GaLore and Sketchy both utilize expensive singular value decomposition to determine the projection matrix, whereas stochastic subspace descent suggests using random matrices as projection matrices and provides convergence analysis on convex functions and objectives. However, to the best of our knowledge, no one has offered any guarantee of convergence for this class of methods on non-convex functions and objectives.

In this work, we provide the first convergence guarantee for arbitrary update rules of the projection matrix. This guarantee is significant because it is broadly applicable to a wide range of optimizers that can be analyzed within the Hamiltonian descent framework . By establishing this convergence guarantee, we demonstrate that our approach is not limited to specific or narrowly defined update rules, but can be extended to include many commonly used optimizers in the field. In particular, thisincludes popular algorithms such as LION  and Adam , which are widely used in various machine learning and optimization tasks. Our results therefore offer a robust theoretical foundation for understanding and analyzing the behavior of these optimizers, ensuring their effectiveness and reliability in diverse applications.

Inspired by our theoretical understanding, we introduce a novel family of memory-efficient optimizers named Online Subspace Descent, which incorporates a dynamically changing projection matrix, replacing the conventional periodic updating approach (SVD) with online PCA. By allowing the projection matrix to evolve in response to the changing gradient landscape, Online Subspace Descent enhances the model's ability to navigate the parameter space more effectively. This dynamic adaptation aligns more closely with the natural progression of learning in deep neural networks, which is characterized by changes in the importance of different characteristics and interactions as training progresses. Through extensive experiments and comparative analysis, we demonstrate that our approach presents lower perplexity in pretraining LLaMA models (ranging from 60M to 1B parameters) on the C4 dataset compared to state-of-the-art low-rank training methods, closing the perplexity gap with full-rank baselines on language model pretraining.

## 2 Optimization Background

The training of deep learning models reduces to an optimization problem

\[_{}L(),\]

where \(\) is the set of weight matrices of the model. For simplicity, we assume \(^{n m}\) is a single matrix of size \((n,m)\) without loss of generality. For notation, we write \(,=(^{})\) for inner products of matrices, and \(\|A\|^{2}=(^{})\) the Frobenius norm. We use \(A B\) to denote the elementwise product, and \(A^{ 2}=A A\).

**Example 2.1**.: _Update rules of common optimizers:_

\[:&_{t+1}=_{t}- _{t} L(_{t}),\\ :&_{t+1}=_{t}-_{t}_{t},&_{t}= (1-) L(_{t})+_{t-1},\\ $ \@@cite[cite]{[\@@bibref{}{Lion-K}{}{}]}}:&_{t+1}=_{t}- _{t}(_{t}),&_{t}=(1-_{1}) L( _{t})+_{1}_{t}\\ ]}}:&_{t}=(1-_{2})  L(_{t})+_{2}_{t-1},\\ ]}}:&_{t+1}=_{t}- _{t}_{t}}{_{t}+e}},&_{t}=(1-_{1t})  L(_{t})+_{1t}_{t-1},\\ &_{t}=(1-_{2t}) L(_{t})^{ 2}+_{2t}_{t-1}, \]

_where \(_{t}\) are step sizes, and \(_{t},_{t}\) are the first and second order momentum, and \(,_{1},_{2}\) are momentum coefficients in \((0,1)\), with \(_{it}=-_{i}^{t+1}}{1-_{i}^{t+1}}\), \(i=1,2\) for \(_{1},_{2}(0,1)\) in Adam, and \(\) is any convex function with \( K()=\) for Lion-\(\), and Lion  uses \(()=\|\|_{1,1}\) and \(()=()\)._These optimizers can be unifiedly viewed as updating \(_{t}\) together with an optimizer state \(_{t}\):

\[_{t+1}=_{t}+_{t}(_{t}), _{t}=_{t}(_{t-1}, L(_{t})),\] (1)

with some mapping \(_{t},_{t}\). We have \(_{t}=_{t}\) for momentum and \(_{t}=\{_{t},_{t}\}\) for Adam. Note that both \(_{t},_{t}\) are of the same size as the model weights \(_{t}\), resulting in high memory consumption for large models. This issue is particularly pronounced for Adam, which typically yields the best performance for large language models (LLMs) but incurs the highest memory cost due to the need to maintain both \(_{t}\) and \(_{t}\). One key challenge is to retain the high performance of Adam while enhancing its memory efficiency.

Hamiltonian+DescentOne powerful approach to studying the dynamic properties of optimizers is to examine their continuous-time ODE forms in the limit of infinitesimal step size. The continuous-time forms provide clearer insights into the asymptotic convergence of the algorithm, abstracting away the choices of step size, discretization, and stochastic errors. The underlying logic is that a "sound" optimizer should be guaranteed to converge to local optima of the loss, at least when using sufficiently small step sizes.

Inspired by , we observe that the continuous-time form of many common optimizers yields a _Hamiltonian+Descent_ structure,

\[}{t}_{t}=_{ }H(_{t},_{t})-(_{}H(_{t},_{t})) \\ }{t}_{t}=-_{}H( {W}_{t},_{t})-(_{}H(_{t},_{t})),\] (2)

where \(H(,)\) is a Hamiltonian (or Lyapunov) function that satisfies

\[_{}H(,)=L(),,\]

so that minimizing \(L()\) reduces to minimizing \(H(,)\); and \((),()\) are two monotonic mappings satisfying

\[\|\|_{}^{2},() 0, \|\|_{}^{2},() 0, .\]

With \(()=()=0\), the system in (2) reduces to the standard Hamiltonian system that keeps \(H(_{t},_{t})=const\) along the trajectory. When adding the descending components with \(\) and \(\), the system then keeps \(H(,)\) monotonically non-decreasing:

\[}{t}H(_{t},_{t })&=_{}H_{t},}{t}_{t}+_{}H_{t},}{ t}_{t}\\ &=_{}H_{t},_{}H_{t}-( _{}H_{t})+_{}H_{t},- _{}H_{t}-(_{}H_{t})\\ &=-\|_{}H_{t}\|_{}^{2}-\| _{}H_{t}\|_{}^{2} 0,\] (3)

where we write \(_{}H_{t}=_{}H(_{t},_{t})\) and similarly for \(_{}H_{t}\). The main idea is that the cross terms \(_{}H_{t},_{}H_{t}\) are canceled, leaving only the negative terms.

**Example 2.2**.: _The momentum method yields following continuous-time form and Hamiltonian:_

\[}{t}_{t}=-_{t},}{ t}_{t}=a( L(_{t})-_{t}), H(,)=L()+\|^{2}}{2a}.\]

**Example 2.3**.: _Adam  yields the following continuous-time form and Hamiltonian,_

\[}{t}_{t}=-_{t}}{_{t}+ e}},}{t}_{t}=a( L(_{t})-_{t}), }{t}_{t}=b( L(_{t})^{ 2}- _{t}),\]

\[ H(,,)=L()+ }{+e}},\ ,\]

_for which we can show that \(}{t}H(_{t},_{t},_{t}) 0\) when \(a b/4\)._

**Example 2.4**.: _The Lion-\(\) optimizer  (without weight decay) can be written into_

\[}{t}_{t}=((1-b)_{t}-b  L(_{t})),}{t}_{t}=-a(  L(_{t})+_{t}),\]

_where \(a 0,\,b\) and \(()\) is any convex function that attains the minimum at \(=0\). One of its Hamiltonians that yields the Hamiltonian+descent structure (Eq (13) in Chen et al. ) is_

\[H(,)=aL()+((1-b)).\]Memory-Efficient Optimizers via Online Subspace Descent

We introduce the idea of constructing memory efficient optimzers by descending in the subspaces that dynamically changes across iterations as motivated by GaLore  and Sketchy . We first derive _static_ subspace descent by restricting the whole optimization on a subspace (Section 3.1), and then propose to dynamically change the subspace across iterations as a heuristic to attain the optimization in the full space while only using subspace descent (Section 3.2). In particular, we propose to update the subspaces via continuous online PCA like updates to avoids the need of exact SVD like in GaLore and Sketchy (Section 3.2). Finally, we remark in Section 3.3 the heuristic nature of the derivation of the method and highlight the difficulty in theoretical understanding, which motivates our analysis based on Hamiltonian dynamics in Section 4.

### Static Subspace Descent

One popular approach to improving memory efficiency is to confine the optimization to a low-dimensional space. To do this, we impose a low rank structure of \(=}\), where \(^{n k}\) is a projection matrix to be determined later and \(}^{k m}\) is a dimension-reduced parameter. When \(k n\), \(\) and \(}\) are much smaller in size compared to \(\). Now consider

\[_{}}L(}).\]

Applying the optimizer from (1) to update \(}\) along with an optimizer state \(}\), and mapping the update rule \(}_{t+1}=}_{t}+_{t}(}_{t})\) to that of \(=}_{t}\), we get

\[_{t+1}=_{t}+_{t}(}_{t}), }_{t}=_{t}(}_{t-1},^{} L(_{t})),\] (4)

where we used the fact that \(_{}L(})=^{}_{}L()\). This yields a more memory-efficient optimizer, as the size of \(}_{t}\) is proportional to that of \(_{t}\), much smaller than \(_{t}\) in (1) when \(k n\).

### Online Subspace Descent

With a static \(\), regardless of its values, the parameter \(\) is restricted to have a low rank structure. Although low rank assumption is proved to be useful for fine-tuning with LoRA-like methods , it is often too limited for pre-training or when the desirable model weights are not inherently low-rank.

To address this problem, Zhao et al.  suggested to keep the projected updated in (4), but use different \(\) across the iterations:

\[_{t+1}=_{t}+_{t}_{t}(}_{t}), }_{t}=_{t}(}_{t-1},_{t}^{} L(_ {t})), _{t+1}=_{t}(_{t},_{t},}_{t}),\]

where \(_{t}\) is a update rule of \(_{t}\) that will be determined in the sequel. The intuition is to open up different projection directions at different iterations, so that optimization can be conducted in different subspaces across different iterations. This is similar to the update of coordinate descent, except in a continuous fashion. Note that the update of \(_{t}\) can be done in parallel with that of \((_{t},}_{t})\), and incurs no slowdown once it is fast enough to not cause a speed bottleneck.

**Example 3.1**.: _Examples of common optimizers equipped with online subspace descent:_

\[:&_{t+1}=_{t}- _{t}_{t}_{t}^{}_{t},&_{t}= L( _{t}),\\ :&_{t+1}=_{t}-_{t}_{t}}_{ t},&}_{t}=(1-)_{t}^{}_{t}+}_{t-1}, \\ :&_{t+1}=_{t}-_{t}_{t}( }_{t}),&}_{t}=_{t}^{}_{t}\\ &}_{t}=(1-_{1})}_{t}+_{1}}_{t},&}_{t}=(1-_{2})}_{t}+_{2}}_{t-1},\\ :&_{t+1}=_{t}-_{t}_{t}}_{ t}}{_{t}+e}},&}_{t}=_{t}^{}_{t},\\ &}_{t}=(1-_{1t})}_{t}+_{1t}}_{t-1},& }_{t}=(1-_{2t})}_{t}^{ 2}+_{2t}}_{t-1}. \]

How Should \(_{t}\) be Updated? It is useful to draw intuition from the projected gradient descent rule

\[_{t+1}=_{t}-_{t}_{t}_{t}^{}_{t}, _{t}= L(_{t}),\] (5)in which \(_{t}_{t}^{}\) can be viewed as a low rank preconditioning of \(_{t}\). To make it follow the exact gradient descent, we hope to make \(_{t}_{t}^{}_{t}\) approximate \(_{t}\) as much as possible. In Galore, this is achieved by performing singular value decomposition (SVD) on \(_{t}\) periodically every \(T\) iterations:

\[_{t},\ ,\ =(_{T t/T }),\]

where \(T t/T\) is the largest multiple of \(T\) less than or equal to \(t\). However, numerical SVD incurs a large computational cost for very large models. Also, since \(_{t}\) is fully determined by \(_{T t/T}\) calculated from a single mini-batch at the last periodic point, it does not incorporate the gradient information from all data in a timely fashion.

In this work, we propose to update \(_{t}\) in a continuous online fashion that incorporates the most recent gradient information in a timely fashion, without calling torch.linalg.decompositions routines. We view the update of \(_{t}\) as conducting an online principal component analysis (PCA) based on the streaming of \(\{_{t}\}\). In particular, we propose to update \(_{t}\) at time \(t\) by minimizing the following PCA objective:

\[L_{_{t}}()=\|^{}}_{t}-}_{t}\|^{2}+\|^{}-_{k k }\|^{2},}_{t}=_{t}}{\|_{t} \|},\] (6)

where \(\|\|=(^{})^{1/2}\) and \(_{k k}\) is the \(k k\) identity matrix; we introduced an auxiliary loss to encourage the columns of \(\) to be orthonormal and normalizes \(_{t}\) to increase stability.

The key property of \(L_{_{t}}()\) in (6) is that all its stable local minimum is a global minimum, and \(\) is a global minimum iff \(^{}}_{t}\) forms the optimal rank-\(k\) approximation of \(}_{t}\) [e.g., 3]; moreover, we have \(^{}=I_{k k}\) at optima when \(>0\).

Instead of minimizing \(L_{_{t}}()\) exactly, to retain computational efficiency, we propose to update \(_{t}\) by only performing one step of optimization on \(L_{_{t}}()\):

\[_{t+1}=(_{t},\ \ _{}L_{_{t}}( _{t})),\]

where OptimizerP.step can be a favorite optimizer, such as gradient descent or Adam. Note that when using Adam, we introduce a copy of optimizer state \(_{t}^{P}\) for \(_{t}\). See Algorithm 1. Compared to the exact SVD, each online update of \(_{t}\) here is fast and can be executed in parallel with the \((_{t},}_{t})\) updates to avoid slowdown.

### Difficulty in Theoretical Understanding

The idea above of projecting an arbitrary optimizer with a dynamically changing \(_{t}\) is heuristically motivated and lacks an immediate rigorous theoretical justification. The main challenge lies in the complex interaction between the update of \(_{t}\) and the optimization state \(_{t}\), which could potentially degrade the convergence and other theoretical properties of the original optimizer. A key question is whether we can develop a theoretical framework to understand how \(_{t}\) impacts the optimizer's convergence behavior and provide guidance for the design of the update rules of \(_{t}\).

To gain understanding, it is useful to first exam the simple case of projected gradient descent in (5) which does not have an optimizer state (\(_{t}=\)). In this case, since \(_{t}_{t}^{}\) is positive semi-finite, the update \(_{t}_{t}^{}_{t}\) is always non-increasing direction of \(L()\) for any \(_{t}\). The algorithm is essentially a variant of coordinate or subspace descent, where \(_{t}\) defines the subspace on which one step of gradient descent is conduced at iteration \(t\). To ensure that (5) finds a local optimum, we mainly need to ensure that \(_{t}_{t}=0\) only if \(_{t}=0\) to prevent the optimizer from stopping prematurely; this is a mild condition that can be satisfied e.g. when \(_{t}\) is updated by (online) PCA on \(_{t}\).

Unfortunately, this coordinate-descent-like interpretation does not apply to more advanced optimizers that track a momentum state \(_{t}\). This is because \(_{t}\) accumulates the information from the projected gradient \(_{}_{}\) at all earlier iterations \( t\). As \(_{t}\) changes across time, it is unclear whether the gradient projected to different subspaces \(_{}\) would be coherent with each other, and useful for future updates that are conducted in different subspaces \(_{t}\) for \(t>\). The difficulty is the inertia effect of \(_{t}\) that entangles the different subspaces, making the dynamic behavior fundamentally more complicated than naive coordinate descent where the descent in different subspaces is uncoupled. This is what we address in Section 4 via the Hamiltonian descent framework.

Hamiltonian Descent Meets Subspace Descent: A Lyapunov Analysis

In this section, we show a surprising result that the complication outlined above in Section 3.3_is not_ a problem for optimizers that yields the Hamiltonian+descent structure in (2). Our result is two-fold:

\(\) Section 4.1: When applying Online Subspace Descent on systems in (2), the Hamiltonian+descent structure is preserved once the update rule of \(_{t}\) has a smooth continuous-time limit. Hence, under very mild conditions, Online Subspace Descent equipped with common optimizers like Adam and Lion automatically yield a Lyapunov function and hence benign continuous-time convergence. Moreover, \(_{t}\) can, in fact, be generalized to an arbitrary linear operator as shown in Section 4.3.

\(\) Section 4.2: For any smooth \(_{t}\) update rules that eliminates the degenerate case of \(_{t}^{}_{t}=0\) while \(_{t}=0\) at convergence, the online subspace optimizer guarantees to converge in continuous time to a stationary point of the loss \(L()\). This mild condition is satisfied, for example, when \(_{t}\) is updated by a typical optimizer on the online PCA objective \(L_{_{t}}()\).

### Online Subspace Descent Preserves the Hamiltonian+Descent Structure

Applying dynamic projection to Hamiltonian descent in (2), we obtain the following systems:

\[&}{t}_{t}=_{t} _{}}H(_{t},}_{t})-(_{}H (_{t},}_{t}))\\ &}{t}}_{t}=-_{t}^{ }_{}H(_{t},}_{t})-(_{}}H(_{t},}_{t}))\\ &}{t}_{t}=(_{t},  L(_{t})),\] (7)

where \(\) specifies the update rule of \(_{t}\). Following essentially the same derivation as (3), one can show that \(H(,)\) remains a Lyapunov function of (7), regardless of the choice of \(\):

\[}{t}H(_{t},}_{t})&=-\|_{}H_{t}\|_{}^{2} -\|_{}H_{t}\|_{}^{2}+_{}H_{t},_{t}_{}}H_{t}- _{}}H_{t},_{t}^{}_{}H_{t} \\ &=-\|_{}H_{t}\|_{}^{2}-\| _{}_{t}\|_{}^{2} 0,\] (8)

where the key is to use the _adjoint_ property of \(\) and \(^{}\) that \(_{t},=,_{t}^{}\), which cancels the crossing terms, independent of the values of \(_{t}\). There is no requirement on \(\) here, besides that the derivative in (8) should exist. As shown in Section 4.3, we can generalize (8) by replacing \(_{t}\) and \(_{t}^{}\) with a general linear operator \(_{t}\) and its adjoint \(_{t}^{*}\).

Please refer to Appendix A for continuous-time Momentum, Lion-\(\) and Adam with subspace descent and their Hamiltonian functions.

### Convergence to Local Optima

In addition to the Lyapunov structure, we need an additional mild condition on the update rule of \(_{t}\) to ensure the system converges to the local optimum of the loss \(L()\). The main idea is to prevent the system from stopping prematurely before reaching zero gradient \(_{t}=0\) by excluding the degenerate case of \(_{t}_{t}=0\) while \(_{t} 0\) in the invariant set of the system.

**Assumption 4.1**.: _Assume the functions in system (7) are continuously differentiable and_

_i) \(}{t}H(_{t},}_{t})=0\) implies \(}_{t}=_{t}^{} L(_{t})=0\) and \(}{t}_{t}=0\)._

_ii) When \(_{t} 0\), the set \(\{^{}=0\}\) is not a positive invariant set of \(}{t}_{t}=(_{t},_{t})\)._

This is a mild condition. Assumption i) says that the optimizer should stop at a point with \(}_{t}=0\), which is easy to verify for the common optimizers like momentum, Adam, Lion-\(\). Assumption ii) ensures \(}_{t}=0\) would imply \(_{t}=0\) in invariance sets, which is satisfied when for example, \(_{t}\) is updated by a reasonable optimizer of the online PCA loss that converges to a stable local minimum.

**Theorem 4.2**.: _Assume Assumption 4.1 holds. Let \((_{t},_{t},_{t})_{t}\) be a bounded solution of (7), then all the accumulation points \(\{_{t}\}\) as \(t+\) are stationary points of \(L()\)._

Proof.: By LaSalle's invariance principle, the positive limit set of \((_{t},_{t},_{t})_{t}\) must be contained in \(\), where \(=\{}{t}H(_{t},}_{t})=0,\, t\ \}\).

From the Assumption i), the trajectories contained in \(\) must satisfy \(}{t}_{t}=0\), which implies \(}{t}_{t}=}{t} L( _{t})=0\) and \(}_{t}=0\) and hence \(_{t}\) is a constant with \(_{t}^{}=0\). Moreover, from Assumption ii), we must have \( L(_{t})=_{t} 0\), since otherwise the trajectory is not invariant. As a result, all trajectories in the limit set \(\) must have \( L(_{t})=0\). Because \(}{t}W_{t}=0\), these trajectories are static points of \(_{t}\). 

### Online Subspace Descent with General Linear Projection Operators

We can generalize the online subspace descent with general linear operators:

\[}{t}_{t}=_{t}( _{}}H(_{t},}_{t}))-(_{}H(_ {t},}_{t}))\] \[}{t}}_{t}=-_{t}^ {*}(_{}H(_{t},}_{t}))-(_{}}H(_{t},}_{t}))\] \[}{t}_{t}=(_ {t}, L(_{t})),\]

where we generalize \(_{t}\) to be any linear operator \(_{t}\) with an adjoint operator \(_{t}^{*}\), satisfying

\[,_{t}()=_{t}^{*}(),,,.\]

The derivation of Lyapunov follows a similar way:

\[}{t}H(_{t},}_{t}) =-\|_{}H_{t}\|_{}^{2}-\|_{}H_{t} \|_{}^{2}+_{}H_{t},_{t}(_{}}H_{t})-_{}}H_{t},_{t}^{*} (_{}H_{t})\] \[=-\|_{}H_{t}\|_{}^{2}-\|_{}H_{t} \|_{}^{2} 0,\]

where the crossing terms are again canceled due to the adjoint property.

As an example of the general framework, consider \(_{t}()=_{t}_{t}\), where \(_{t}\) is another projection matrix applied on the different dimension of \(\) (see also ). The adjoint operator of \(_{t}\) is \(_{t}^{*}()=_{t}^{}_{t}^{}\). This can be verified by

\[_{t}_{t},=(_{t} _{t}^{})=(_{t}^{}_{t})= ((_{t}^{}_{t}^{})^{})= ,_{t}^{}_{t}^{}.\]

The subspace descent system of this operator is

\[}{t}_{t}=_{t}_{}}H(_{t},}_{t})_{t}-(_{}H(_{t},}_{t}))\] \[}{t}}_{t}=-_{t}^{} _{}H(_{t},}_{t}))_{t}^{}-( _{}}H(_{t},}_{t}))\] \[}{t}_{t}=_{P}(_{t}, _{t}, L(_{t}))\] \[}{t}_{t}=_{Q}(_{t}, _{t}, L(_{t})),\]

where \(_{t},_{t}\) can be updated jointly via an online SVD on \(_{t}\).

Another linear operator that involves two matrices is \(_{t}()=_{t}+_{t}\), which yields \(_{t}^{*}()=_{t}^{}+_{t}^{}\).

## 5 Experiment

We answer a number of key questions with pretraining experiments of LLaMA  on the C4 dataset . All experiments except for large 7B experiments are conducted on a _single_ NVIDIA A100 GPU.

### Why do we Need Online Subspace Descent?

Overall, Online Subspace Descent offers two major advantages over previous methods that rely on SVD, better convergence and lower overhead. In this section, we discuss both in detail.

First, Online Subspace Descent closes the gap between the state-of-the-art low-rank method and full rank baseline uniformly across different model sizes, as shown in figure 1. A highlight amongst these results is LLaMA 1B (SS 256). As shown in table 1, Online Subspace Descent attains significant improvement over GaLore in perplexity, while consuming a similar amount of GPU memory (8.64 GB v.s 9.01 GB). One additional observation in 1 shows as model size and sequence length grow, Online Subspace Descent becomes more effective. We hypothesize that this is due to the higher intrinsic rank of the underlying optimization problem in larger models. Hence, the positive impact on the convergence of the online update of \(_{t}\) becomes more obvious. See more details in Appendix B.

Another favorable characteristic of Online Subspace Descent is its minimum overhead. In figure 2, we measure and analyze the execution time of SVD and online PCA on a popular data center GPU (A100) and a consumer GPU (RTX 3090). The typical Pytorch implementation of SVD can be up to \(142\) times slower than running a single-step online PCA on representative weight tensors from LLaMA architectures. Online PCA is fast because it is implemented as a single optimization step with respect to a simple loss function. Hence, each step of online PCA can be cleverly scheduled and hidden in the weight optimization step when executed in parallel, whereas SVD is too expensive to be hidden.

### What Rank Should we Pick for _Online Subspace Descent_?

We conduct an ablation study on the rank of Online Subspace Descent. Figure 3 shows that the final perplexity is inversely correlated with rank: higher ranks result in lower convergent perplexity. However, the rate of reduction of perplexity decreases as the rank increases, eventually reaching a saturation point. We propose an intuitive explanation for this phenomenon. In language modeling, high-frequency tokens can be effectively learned with low-rank training. However, learning lower-frequency tokens requires higher ranks. Once these lower-frequency tokens are adequately learned, further increasing the rank does not significantly decrease perplexity. In conclusion, given sufficient time and resources, higher ranks yield better performance for Online Subspace Descent. It is recommended that the highest rank be selected until the perplexity reduction saturates.

### What are the Best Hyperparameters?

\(\)**and**\(\): The parameter \(\) controls the update speed of \(_{t}\), while \(\) determines the regularization strength on the optimization objective of \(_{t}\). Empirically, we find that the result is not sensitive to \(\)

    &  \\   & **60M** & **350M** & **1B** \\ 
8bit-AdamW (Full Rank) & 32.75 & 30.43 & 29.40 \\  GaLore (Rank = 512) & 57.03 & 44.34 & 35.52 \\
**Ours** (Rank = 512) & **56.12** & **43.67** & **31.30** \\   

Table 1: Pretraining LLaMA 1B with a sequence length of 256 and for 10K steps, perplexity was reported as the training average of the last 10 steps. AdamW8bit serves as the base optimizer for both.

Figure 2: The execution time of torch.svd and that a single-step backward() call for online PCA in PyTorch, on matrices of typical shapes in linear layers in the LLaMA 60M to 7B. Thanks to the high speed of single-step online PCA, \(_{t}\) updates can be executed in parallel with weight updates, adding no overhead to the training process. In contrast, SVD incurs significant overhead as the model and weight tensor sizes increase.

for small models (60M). and set \(=0.1\) for all subsequent experiments. We find that \(\) must be kept small to avoid instability (Figure 3), and we set \(=5\) for all experiments.

**Learning rate**: For the small model (60M), learning rate choices are more flexible, producing similar results. However, for larger models (350M, 1B), we recommend using a learning rate that is 10 times smaller, specifically 0.001. Larger learning rates cause unrecoverable spikes and instability, a general characteristic observed across all methods. See additional hyperparameter choices in Appendix B.

### Can _Online Subspace Descent_ be Applied to Different Optimizers?

One straightforward extension of Online Subspace Descent is to apply it to other base optimizers beyond AdamW8bit. We conduct ablation studies on LION  and Adafactor , finding that Online Subspace Descent behaves similarly to how it does with AdamW8bit. Despite the initial observation that updating \(_{t}\) with AdamW8bit consistently yields better results, we discover that updating \(P_{t}\) with simple SGD can achieve similar performance.

### Can Online Subspace Descent Scale to Larger Model?

We pretrain from scratch a 7B LLaMA model on the C4 dataset for 10K steps, where the \(_{t}\) matrix is updated by SGD. The perplexity is the lower the better. The final perplexity and training wall-clock time are provided in Table 3. We further provide the downstream evaluation of the pretrained checkpoints using Galore and our method on the GLUE benchmark in Table 4. Our method consistently outperforms Galore when the model size scales up.

## 6 Related Works

We discuss related works on memory-efficient optimization and low-rank adaptation techniques.

  
**Method** &  &  \\   & Lion & Adaf. & Lion+Lion & Adaf.+Adaf. & Lion+AdamW & Adaf.+AdamW \\ 
**Perplexity** & 46.90 & 34.32 & 57.97 & 47.61 & **44.76** & **34.15** \\   

Table 2: LLaMA 60M on C4 with sequence length 1024, with optimizers on \(_{t}\) and \(_{t}\), denote as “Ours \(\{_{t}\) optimizer\(\}\) + \(\{_{t}\) optimizer\(\}\)”. Adaf., and Adam refer to Adafactor and 8bit-AdamW, respectively.

Figure 3: From left to right are loss curves of 10K steps on LLaMA 60M: leftmost is the sweep of rank, middle is the sweep of \(\) and rightmost is the sweep of \(\).

Low-Rank AdaptationLow-Rank Adaptation (LoRA)  adds a low-rank adaptor to spefic linear layers in a model, and finetune only the low-rank adaptor. As the adaptors are small, LoRA is widely applied for finetuning large models. Many variants have been proposed since LoRA, including support for multi-task learning Wang et al.  and further memory reductions Dettmers et al. . Notably, Lialin et al.  proposed ReLoRA for pretraining, requiring a full-rank training warmup to match standard performance levels. It's important to note that LoRA is fundamentally distinct from subspace descent. While subspace descent optimizes within the original model parameter space, LoRA focuses its optimization efforts within the space of the adaptors.

Memory-Efficient OptimizationSeveral approaches aim to reduce memory costs associated with gradient statistics in adaptive optimization algorithms [21; 2; 7]. In particular, Adafactor  factorizes the second-order statistics by a row-column outer product and update the factorized bases on the fly, hence achieving a sub-linear memory cost. K-Fac  presents a factorized approximation of the Fisher information matrix which leads to a sublinear natural gradient method. More recently, Feinberg et al.  observes that the spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace and propose to maintain a matrix preconditioner using the frequent directions sketch. However, their method requires conducting the eigendecomposition at every step, which can be costly for large models. Other than factorization methods, quantization techniques [7; 1; 24; 16] are also widely used, where the gradient (or the momentum and the preconditioner) are directly quantized to tradeoff performance for memory. Fused gradient computation method  have also been used to minimize memory costs during training. GaLore  is the most relevant work to ours. GaLore focuses on low-rank gradient structures, reducing memory costs for both first and second-order statistics. Our method can be viewed as a general extension to GaLore where we replace the infrequent SVD by a continuous subspace descent [14; 10]. As a result, our method not only provides a more general framework to study memory-efficient subspace descent, but is also more performant than GaLore in practice.

## 7 Conclusion

In conclusion, we provide the first convergence guarantee for arbitrary update rules of projection matrix, applicable to a range of optimizers that can be analyzed using Hamiltonian Descent, including common ones like LION, AdamW, and Adafactor. Inspired by this theoretical foundation, we introduce Dynamic Subspace Descent, a novel family of subspace descent optimizers that eschews SVD in favor of online PCA for updating projection matrix. Dynamic Subspace Descent is both flexible and minimally intrusive, and our experiments show that it achieves lower perplexity in pretraining LLaMA models (ranging from 60M to 1B parameters) on the C4 dataset compared to state-of-the-art low-rank training methods, while also closing the perplexity gap with full-rank baselines.

For future research, we propose several open and intriguing questions: (1) Are there alternative methods for updating projection matrix that could accelerate convergence? (2) What is the impact of weight decay on convergence in Dynamic Subspace Descent? (3) Can low-rank gradients and updates be combined with dynamic low-rank weights (e.g., Mixture of Experts) to further enhance training efficiency? (4) Can this method be applied to problems beyond language modeling? We hope that our work provides a strong foundation for exploring these questions.

  
**Method** & **MRPC** & **RTE** & **SST2** & **MNLI** & **QNLI** & **QQP** & **AVG** \\  Galore & 0.6838 & **0.5018** & 0.5183 & 0.3506 & 0.4946 & 0.3682 & 0.4862 \\ Ours & **0.6982** & 0.4901 & **0.5233** & **0.3654** & **0.5142** & **0.3795** & **0.4951** \\   

Table 4: Standardized GLUE evaluation for 7B model checkpoints using eval-harness. Results are reported for various downstream tasks.

Acknowledgment

The research is conducted in Statistics & AI group at UT Austin, which receives supports in part from NSF CAREER1846421, SenSE2037267, Office of Navy Research, and NSF AI Institute for Foundations of Machine Learning (IFML).