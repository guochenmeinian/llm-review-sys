# Expanding and Compressing Approach.

Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback

 Han Shao

TTIC

han@ttic.edu

&Lee Cohen

TTIC

leecohencs@gmail.com

&Avrim Blum

TTIC

avrim@ttic.edu

&Yishay Mansour

Tel Aviv University and Google Research

mansour.yishay@gmail.com &Aadirupa Saha

TTIC

aadirupa@ttic.edu &Matthew R. Walter

TTIC

mwalter@ttic.edu

###### Abstract

In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a known Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a user is provided with two policies and returns their preferred policy as feedback. We then move to a different user feedback model, where a user is instead provided with two small weighted sets of representative trajectories and selects the preferred one. In both cases, we suggest an algorithm that finds a nearly optimal policy for the user using a number of comparison queries that scales quasilinearly in the number of objectives.

## 1 Introduction

Many real-world decision making problems involve optimizing over multiple objectives. For example, when designing an investment portfolio, one's investment strategy requires trading off maximizing expected gain with minimizing risk. When using Google Maps for navigation, people are concerned about various factors such as the worst and average estimated arrival time, traffic conditions, road surface conditions (e.g., whether or not it is paved), and the scenery along the way. As the advancement of technology gives rise to personalized machine learning (McAuley, 2022), in this paper, we design efficient algorithms for personalized multi-objective decision making.

While prior works have concentrated on approximating the Pareto-optimal solution set1 (see Hayes et al. (2022) and Roijers et al. (2013) for surveys), we aim to find the optimal personalized policy for a user that reflects their unknown preferences over \(k\) objectives. Since the preferences are unknown, we need to elicit users' preferences by requesting feedback on selected policies. The problem of eliciting preferences has been studied in Wang et al. (2022) using a strong query model that provides stochastic feedback on the quality of a single policy. In contrast, our work focuses on a more natural and intuitive query model, comparison queries, which query the user's preference over two selected policies, e.g., 'do you prefer a policy which minimizes average estimated arrival time or policy which minimizes the number of turns?'. Our goal is to find the optimal personalized policy using as few queries as possible.

To the best of our knowledge, we are the first to provide algorithms with theoretical guarantees for specific personalized multi-objective decision-making via policy comparisons.

Similar to prior works on multi-objective decision making, we model the problem using a finite-horizon Markov decision process (MDP) with a \(k\)-dimensional reward vector, where each entry is a non-negative scalar reward representative of one of the \(k\) objectives. To account for user preferences, we assume that a user is characterized by a (hidden) \(k\)-dimensional _preference vector_ with non-negative entries, and that the _personalized reward_ of the user for each state-action is the inner product between this preference vector and the reward vector (for this state-action pair). We also distinguish between the \(k\)-dimensional value of a policy, which is the expected cumulative reward when selecting actions according to the policy, and the _personalized value_ of a policy, which is the scalar expected cumulative personalized reward when selecting actions according to this policy. The MDP is known to the agent and the goal is to learn an optimal policy for the personalized reward function (henceforth, the _optimal personalized policy_) of a user via policy comparative feedback.

**Comparative feedback.** If people could clearly define their preferences over objectives (e.g., "my preference vector has \(3\) for the scenery objective, \(2\) for the traffic objective, and \(1\) for the road surface objective"), the problem would be easy--one would simply use the personalized reward function as a scalar reward function and solve for the corresponding policy. In particular, a similar problem with noisy feedback regarding the value of a single multi-objective policy (as mapping from states to actions) has been studied in Wang et al. (2022). As this type of fine-grained preference feedback is difficult for users to define, especially in environments where sequential decisions are made, we restrict the agent to rely solely on comparative feedback. Comparative feedback is widely used in practice. For example, ChatGPT asks users to compare two responses to improve its performance. This approach is more intuitive for users compared to asking for numerical scores of ChatGPT responses.

Indeed, as knowledge of the user's preference vector is sufficient to solve for their optimal personalized policy, the challenge is to learn a user's preference vector using a minimal number of easily interpretable queries. We therefore concentrate on comparison queries. The question is then what exactly to compare? Comparing state-action pairs might not be a good option for the aforementioned tasks--what is the meaning of two single steps in a route? Comparing single trajectories (e.g., routes in Google Maps) would not be ideal either. Consider for example two policies: one randomly generates either (personalized) GOOD or (personalized) BAD trajectories while the other consistently generates (personalized) MEDIOCRE trajectories. By solely comparing single trajectories without considering sets of trajectories, we cannot discern the user's preference regarding the two policies.

**Interpretable policy representation.** Since we are interested in learning preferences via policy comparison queries, we also suggest an alternative, more interpretable representation of a policy. Namely, we design an algorithm that given an explicit policy representation (as a mapping from states to distributions over actions), returns a weighted set of trajectories of size at most \(k+1\), such that its expected return is identical to the value of the policy.2 It immediately follows from our formalization that for any user, the personalized return of the weighted trajectory set and the personalized value of the policy are also identical.

In this work, we focus on answering two questions:

_(1) How to find the optimal personalized policy by querying as few policy comparisons as possible?_

_(2) How can we find a more interpretable representation of policies efficiently?_

**Contributions.** In Section 2, we formalize the problem of eliciting user preferences and finding the optimal personalized policy via comparative feedback. As an alternative to an explicit policy representation, we propose a _weighted trajectory set_ as a more interpretable representation. In Section 3, we provide an _efficient_ algorithm for finding an approximate optimal personalized policy, where the policies are given by their formal representations, thus answering (1). In Section 4, we design two efficient algorithms that find the weighted trajectory set representation of a policy. Combined with the algorithm in Section 3, we have an algorithm for finding an approximate optimal personalized policy when policies are represented by weighted trajectory sets, thus answering (2).

**Related Work.** Multi-objective decision making has gained significant attention in recent years (see Roijers et al. (2013); Hayes et al. (2022) for surveys). Prior research has explored various approaches,such as assuming linear preferences or Bayesian Settings, or finding an approximated Pareto frontier. However, incorporating comparison feedback (as was done for Multi-arm bandits or active learning, e.g., Benggs et al. (2021) and Kane et al. (2017)) allows us a more comprehensive and systematic approach to handling different types of user preferences and provides (nearly) optimal personalized decision-making outcomes. We refer the reader to Appendix A for additional related work.

## 2 Problem Setup

**Sequential decision model.** We consider a Markov decision process (MDP) _known3_ to the agent represented by a tuple \(,,s_{0},P,R,H\), with finite state and action sets, \(\) and \(\), respectively, an initial state \(s_{0}\), and finite horizon \(H\). For example, in the Google Maps example a state is an intersection and actions are turns. The transition function \(P:^{}\) maps state-action pairs into a state probability distribution. To model multiple objectives, the reward function \(R:^{k}\) maps every state-action pair to a \(k\)-dimensional reward vector, where each component corresponds to one of the \(k\) objectives (e.g., road surface condition, worst and average estimated arrival time). The _return_ of a trajectory \(=(s_{0},a_{0},,s_{H-1},a_{H-1},s_{H})\) is given by \(()=_{t=0}^{H-1}R(s_{t},a_{t})\).

A _policy_\(\) is a mapping from states to a distribution over actions. We denote the set of policies by \(\). The _value_ of a policy \(\), denoted by \(V^{}\), is the expected cumulative reward obtained by executing the policy \(\) starting from the initial state, \(s_{0}\). Put differently, the value of \(\) is \(V^{}=V^{}(s_{0})=_{S_{0}=s_{0}}[_{t=0}^{H-1}R(S_{t}, (S_{t}))][0,H]^{k}\), where \(S_{t}\) is the random state at time step \(t\) when executing \(\), and the expectation is over the randomness of \(P\) and \(\). Note that \(V^{}=_{}[()]\), where \(=(s_{0},(s_{0}),S_{1},,(S_{H-1}),S_{H})\) is a random trajectory generated by executing \(\).

We assume the existence of a "do nothing" action \(a_{0}\), available only from the initial state \(s_{0}\), that has zero reward for each objective \(R(s_{0},a_{0})=\) and keeps the system in the initial state, i.e., \(P(s_{0} s_{0},a_{0})=1\) (e.g., this action corresponds to not commuting or refusing to play in a chess game.)4. We also define the (deterministic) "do nothing" policy \(_{0}\) that always selects action \(a_{0}\) and has a value of \(V^{_{0}}=\). From a mathematical perspective, the assumption of "do nothing" ensures that \(\) belongs to the value vector space \(\{V^{}|\}\), which is precisely what we need.

Since the rewards are bounded between \(\), we have that \(1\|V^{}\|_{2}H\) for every policy \(\). For convenience, we denote \(C_{V}=H\). We denote by \(d k\) the rank of the space spanned by all the value vectors obtained by \(\), i.e., \(d:=((\{V^{}|\}))\).

**Linear preferences.** To incorporate personalized preferences over objectives, we assume each user is characterized by an unknown \(k\)-dimensional _preference vector_\(w^{*}_{+}^{k}\) with a bounded norm \(1\|w^{*}\|_{2} C_{w}\) for some (unknown) \(C_{w} 1\). We avoid assuming that \(C_{w}=1\) to accommodate for general linear rewards. Note that the magnitude of \(w^{*}\) does not change the personalized optimal policy but affects the "indistinguishability" in the feedback model. By not normalizing \(\|w^{*}\|_{2}=1\), we allow users to have varying levels of discernment. This preference vector encodes preferences over the multiple objectives and as a result, determines the user preferences over policies.

Formally, for a user characterized by \(w^{*}\), the _personalized value_ of policy \(\) is \( w^{*},V^{}_{+}\). We denote by \(^{*}:=_{} w^{*},V^{}\) and \(v^{*}:= w^{*},V^{^{*}}\) the optimal _personalized policy_ and its corresponding optimal personalized value for a user who is characterized by \(w^{*}\). We remark that the "do nothing" policy \(_{0}\) (that always selects action \(a_{0}\)) has a value of \(V^{_{0}}=\), which implies a personalized value of \( w^{*},V^{_{0}}=0\) for every \(w^{*}\). For any two policies \(_{1}\) and \(_{2}\), the user characterized by \(w^{*}\) prefers \(_{1}\) over \(_{2}\) if \( w^{*},V^{_{1}}-V^{_{2}}>0\). Our goal is to find the optimal personalized policy for a given user using as few interactions with them as possible.

**Comparative feedback.** Given two policies \(_{1},_{2}\), the user returns \(_{1}_{2}\) whenever \( w^{*},V^{_{1}}-V^{_{2}}>\); otherwise, the user returns "indistinguishable" (i.e., whenever \(| w^{*},V^{_{1}}-V^{_{2}}|\)). Here \(>0\) measures the precision of the comparative feedback and is small usually. The agent can query the user about their policy preferences using two different types of policy representations:

1. _Explicit policy representation of \(\)_: An explicit representation of policy as mapping, \(:^{}\).
2. _Weighted trajectory set representation of \(\)_: A \(\)-sized set of trajectory-weight pairs \(\{ p_{i},_{i}\}_{i=1}^{}\) for some \( k+1\) such that (i) the weights \(p_{1},,p_{}\) are non-negative and sum to \(1\); (ii) every trajectory in the set is in the support5 of the policy \(\); and (iii) the expected return of these trajectories according to the weights is identical to the value of \(\), i.e., \(V^{}=_{i=1}^{}p_{i}(_{i})\). Such comparison could be practical for humans. E.g., in the context of Google Maps, when the goal is to get from home to the airport, taking a specific route takes \(40\) minutes \(90\%\) of the time, but it can take \(3\) hours in case of an accident (which happens w.p. \(10\%\)) vs. taking the subway which always has a duration of \(1\) hour. 
In both cases, the feedback is identical and depends on the hidden precision parameter \(\). As a result, the value of \(\) will affect the number of queries and how close the value of the resulting personalized policy is to the optimal personalized value. Alternatively, we can let the agent decide in advance on a maximal number of queries, which will affect the optimality of the returned policy.

**Technical Challenges.** In Section 3, we find an approximate optimal policy by \(()\) queries. To achieve this, we approach the problem in two steps. Firstly, we identify a set of linearly independent policy values, and then we estimate the preference vector \(w^{*}\) using a linear program that incorporates comparative feedback. The estimation error of \(w^{*}\) usually depends on the condition number of the linear program. Therefore, the main challenge we face is how to search for linear independent policy values that lead to a small condition number and providing a guarantee for this estimate.

In Section 4, we focus on how to design _efficient_ algorithms to find the weighted trajectory set representation of a policy. Initially, we employ the well-known Caratheodory's theorem, which yields an inefficient algorithm with a potentially exponential running time of \(|S|^{H}\). Our main challenge lies in developing an efficient algorithm with a running time of \(((H|S|||))\). The approach based on Caratheodory's theorem treats the return of trajectories as independent \(k\)-dimensional vectors, neglecting the fact that they are all generated from the same MDP. To overcome this challenge, we leverage the inherent structure of MDPs.

## 3 Learning from Explicit Policies

In this section, we consider the case where the interaction with a user is based on explicit policy comparison queries. We design an algorithm that outputs a policy being nearly optimal for this user. For multiple different users, we only need to run part of the algorithm again and again. For brevity, we relegate all proofs to the appendix.

If the user's preference vector \(w^{*}\) (up to a positive scaling) is given, then one can compute the optimal policy and its personalized value efficiently, e.g., using the Finite Horizon Value Iteration algorithm. In our work, \(w^{*}\) is unknown and we interact with the user to learn \(w^{*}\) through comparative feedback. Due to the structure model that is limited to meaningful feedback only when the compared policy values differ at least \(\), the exact value of \(w^{*}\) cannot be recovered. We proceed by providing a high-level description of our ideas of how to estimate \(w^{*}\).

(1) _Basis policies_: We find policies \(_{1},,_{d}\), and their respective values, \(V^{_{1}},,V^{_{d}}[0,H]^{k}\), such that their values are linearly independent and that together they span the entire space of value vectors.6 These policies will not necessarily be personalized optimal for the current user, and instead serve only as building blocks to estimate the preference vector, \(w^{*}\). In Section 3.1 we describe an algorithm that finds a set of basis policies for any given MDP.

(2) _Basis ratios_: For the basis policies, denote by \(_{i}>0\) the ratio between the personalized value of a _benchmark policy_, \(_{1}\), to the personalized value of \(_{i+1}\), i.e.,

\[ i[d-1]:_{i} w^{*},V^{_{1}}=  w^{*},V^{_{i+1}}\.\] (1)We will estimate \(_{i}\) of \(_{i}\) for all \(i[d-1]\) using comparison queries. A detailed algorithm for estimating these ratios appears in Section 3.2. For intuition, if we obtain exact ratios \(_{i}=_{i}\) for every \(i[d-1]\), then we can compute the vector \(}{\|w^{*}\|_{1}}\) as follows. Consider the \(d-1\) equations and \(d-1\) variables in Eq (1). Since \(d\) is the maximum number of value vectors that are linearly independent, and \(V^{_{1}}, V^{_{d}}\) form a basis, adding the equation \(\|w\|_{1}=1\) yields \(d\) independent equations with \(d\) variables, which allows us to solve for \(w^{*}\). The details of computing an estimate of \(w^{*}\) are described in Section 3.3.

### Finding a Basis of Policies

In order to efficiently find \(d\) policies with \(d\) linearly independent value vectors that span the space of value vectors, one might think that selecting the \(k\) policies that each optimizes one of the \(k\) objectives will suffice. However, this might fail--in Appendix N, we show an instance in which these \(k\) value vectors are linearly dependent even though there exist \(k\) policies whose values span a space of rank \(k\).

Moreover, our goal is to find not just any basis of policies, but a basis of policies such that (1) the personalized value of the benchmark policy \( w^{*},V^{_{1}}\) will be large (and hence the estimation error of ratio \(_{i}\), \(|_{i}-_{i}|\), will be small), and (2) that the linear program generated by this basis of policies and the basis ratios will produce a good estimate of \(w^{*}\).

**Choice of \(_{1}\).** Besides linear independence of values, another challenge is to find a basis of policies to contain a benchmark policy, \(_{1}\) (where the index \(1\) is wlog) with a relatively large personalized value, \( w^{*},V^{_{1}}\), so that \(_{i}\)'s error is small (e.g., in the extreme case where \( w^{*},V^{_{1}}=0\), we will not be able to estimate \(_{i}\)).

For any \(w^{k}\), we use \(^{w}\) denote a policy that maximizes the scalar reward \( w,R\), i.e.,

\[^{w}=*{arg\,max}_{} w,V^{}\,,\] (2)

and by \(v^{w}= w,V^{^{w}}\) to denote the corresponding personalized value. Let \(_{1},,_{k}\) denote the standard basis. To find \(_{1}\) with large personalized value \( w^{*},V^{_{1}}\), we find policies \(^{_{j}}\) that maximize the \(j\)-th objective for every \(j=1,,k\) and then query the user to compare them until we find a \(^{_{j}}\) with (approximately) a maximal personalized value among them. This policy will be our benchmark policy, \(_{1}\).

```
1:initialize \(^{^{*}}^{_{1}}\)
2:for\(j=2,,k\)do
3: compare \(^{^{*}}\) and \(^{_{j}}\)
4:if\(^{_{j}}^{^{*}}\)then\(^{^{*}}^{_{j}}\)
5:endfor
6:\(_{1}^{^{*}}\) and \(u_{1}^{*}}}}{\|V^{^{^{*}}} \|_{2}}\)
7:for\(i=2,,k\)do
8: arbitrarily pick an orthonormal basis, \(_{1},,_{k+1-i}\), for \(*{span}(V^{_{1}},,V^{_{i-1}})^{}\).
9:\(j_{}_{j[k+1-i]}(|v^{_{j}}|,|v^ {-_{j}}|)\).
10:if\((|v^{_{j_{}}}|,|v^{-_{j_{}}}|)>0\)then
11:\(_{i}^{_{j_{}}}\)if\(|v^{_{j_{}}}|>|v^{-_{j_{}}}|\); otherwise\(_{i}^{-_{j_{}}}\). \(u_{i}_{j_{}}\)
12:else
13:return\((_{1},_{2},)\), \((u_{1},u_{2},)\)
14:endif
15:endfor ```

**Algorithm 1** Identification of Basis Policies

The details are described in lines 1-6 of Algorithm 1.

**Choice of \(_{2},,_{d}\).** After finding \(_{1}\), we next search the remaining \(d-1\) polices \(_{2},,_{d}\) sequentially (lines 8-13 of Algorithm 1). For \(i=2,,d\), we find a direction \(u_{i}\) such that (i) the vector \(u_{i}\) is orthogonal to the space of current value vectors \(*{span}(V^{_{1}},,V^{_{i-1}})\), and (ii) there exists a policy \(_{i}\) such that \(V^{_{i}}\) has a significant component in the direction of \(u_{i}\). Condition (i) is used to guarantee that the policy \(_{i}\) has a value vector linearly independent of \(*{span}(V^{_{1}},,V^{_{i-1}})\). Condition (ii) is used to cope with the error caused by inaccurate approximation of the ratios \(_{i}\). Intuitively, when \(\|_{i}V^{_{1}}-V^{_{i+1}}\|_{2}\), the angle between \(_{i}V^{_{1}}-V^{_{i+1}}\) and \(_{i}V^{_{1}}-V^{_{i+1}}\) could be very large, which results in an inaccurate estimate of \(w^{*}\) in the direction of \(_{i}V^{_{1}}-V^{_{i+1}}\). For example, if \(V^{_{1}}=_{1}\) and \(V^{_{i}}=_{1}+^{*}}_{i}\) for \(i=2,,k\), then \(_{1},_{i}\) are "indistinguishable" and the estimate ratio \(_{i-1}\) can be \(1\). Then the estimate of \(w^{*}\) by solving linear equations in Eq (1) is \((1,0,,0)\), which could be far from the true \(w^{*}\). Finding \(u_{i}\)'s in which policy values have a large component can help with this problem.

Algorithm 1 provides a more detailed description of this procedure. Note that if there are \(n\) different users, we will run Algorithm 1 at most \(k\) times instead of \(n\) times. The reason is that Algorithm1 only utilizes preference comparisons while searching for \(_{1}\) (lines 1-6), and not for \(_{2},,_{k}\) (which contributes to the \(k^{2}\) factor in computational complexity). As there are at most \(k\) candidates for \(_{1}\), namely \(^{e_{1}},,^{e_{k}}\), we execute lines 1-6 of Algorithm 1 for \(n\) rounds and lines 8-13 for only \(k\) rounds.

### Computation of Basis Ratios

As we have mentioned before, comparing basis policies alone does not allow for the exact computation of the \(_{i}\) ratios as comparing \(_{1},_{i}\) can only reveal which is better but not how much. To this end, we will use the "do nothing" policy to approximate every ratio \(_{i}\) up to some additive error \(|_{i}-_{i}|\) using binary search over the parameter \(_{i}[0,C_{}]\) for some \(C_{} 1\) (to be determined later) and comparison queries of policy \(_{i+1}\) with policy \(_{i}_{1}+(1-_{i})_{0}\) if \(_{i} 1\) (or comparing \(_{1}\) and \(_{i}}_{i+1}+(1-_{i}}) _{0}\) instead if \(_{i}>1\)).7 Notice that the personalized value of \(_{i}_{1}+(1-_{i})_{0}\) is identical to the personalized value of \(_{1}\) multiplied by \(_{i}\). We stop once \(_{i}\) is such that the user returns "indistinguishable". Once we stop, the two policies have roughly the same personalized value,

\[_{i} 1,|_{i} w^{ *},V^{_{1}}- w^{*},V^{_{i+1}}| \,;_{i}>1,| w^{*},V^{_{1}} -_{i}} w^{*},V^{_{i+1}} |.\] (3)

Eq (1) combined with the above inequality implies that \(|_{i}-_{i}| w^{*},V^{_{1}}  C_{}\). Thus, the approximation error of each ratio is bounded by \(|_{i}-_{i}|}{  w^{*},V^{_{1}}}\). To make sure the procedure will terminate, we need to set \(C_{}}{ w^{*},V^{_{1}}}\) since \(_{i}\)'s must lie in the interval \([0,}{ w^{*},V^{_{1}}}]\). Upon stopping binary search once Eq (3) holds, it takes at most \((d(C_{} w^{*},V^{_{1}}/ ))\) comparison queries to estimate all the \(_{i}\)'s.

Due to the carefully picked \(_{1}\) in Algorithm 1, we can upper bound \(}{ w^{*},V^{_{1}}}\) by \(2k\) and derive an upper bound for \(|_{i}-_{i}|\) by selecting \(C_{}=2k\).

**Lemma 1**.: _When \(}{2k}\), we have \(}{ w^{*},V^{_{1}}} 2k\), and \(|_{i}-_{i}|}{v^{*}}\) for every \(i[d]\)._

In what follows we set \(C_{}=2k\). The pseudo code of the above process of estimating \(_{i}\)'s is deferred to Algorithm 3 in Appendix C.

### Preference Approximation and Personalized Policy

We move on to present an algorithm that estimates \(w^{*}\) and calculates a nearly optimal personalized policy. Given the \(_{i}\)'s returned by Algorithm 1 and the \(_{i}\)'s returned by Algorithm 3, consider a matrix \(^{d k}\) with 1st row \(V^{_{1}}{}^{}\) and the \(i\)th row \((_{i-1}V^{_{1}}-V^{_{i}})^{}\) for every \(i=2,,d\). Let \(\) be a solution to \(x=_{1}\). We will show that \(\) is a good estimate of \(w^{}:=}{ w^{*},V^{_{1}}}\) and that \(^{}\) is a nearly optimal personalized policy. In particular, when \(\) is small, we have \(|,V^{}- w^{},V^ {}|=(^{})\) for every policy \(\). Putting this together, we derive the following theorem.

**Theorem 1**.: _Consider the algorithm of computing \(\) and any solution \(\) to \(x=_{1}\) and outputting the policy \(^{}=_{},V^{}\), which is the optimal personalized policy for preference vector \(\). Then the output policy \(^{}\) satisfying that \(v^{*}- w^{*},V^{^{}} ((+1)^{d+}^{})\) using \((k(k/))\) comparison queries._

**Computation Complexity** We remark that Algorithm 1 solves Eq (2) for the optimal policy in scalar reward MDP at most \((k^{2})\) times. Using, e.g., Finite Horizon Value iteration to solve for the optimal policy takes \((H||^{2}||)\) steps. However, while the time complexity it takes to return the optimal policy for a single user is \((k^{2}H||^{2}||+k())\), considering \(n\) different users rather than one results in overall time complexity of \(((k^{3}+n)H||^{2}||+nk())\).

**Proof Technique.** The standard technique typically starts by deriving an upper bound on \(\|-w^{*}\|_{2}\) and then uses this bound to upper bound \(_{}|,V^{}- w^{*},V^{}|\) as \(C_{V}\|-w^{*}\|_{2}\). However,this method fails to achieve a non-vacuous bound in case there are two basis policies that are close to each other. For instance, consider the returned basis policy values: \(V^{_{1}}=(1,0,0)\), \(V^{_{2}}=(1,1,0)\), and \(V^{_{3}}=(1,0,)\) for some \(>0\). When \(\) is extremely small, the estimate \(\) becomes highly inaccurate in the direction of \((0,0,1)\), leading to a large \(\|-w^{*}\|_{2}\). Even in such cases, we can still obtain a non-vacuous guarantee since the selection of \(V^{_{3}}\) (line 11 of Algorithm 1) implies that no policy in \(\) exhibits a larger component in the direction of \((0,0,1)\) than \(_{3}\).

**Proof Sketch.** The analysis of Theorem 1 has two parts. First, as mentioned in Sec 3.1, when \(\|_{i}V^{_{1}}-V^{_{i+1}}\|_{2}\), the error of \(_{i+1}\) can lead to inaccurate estimate of \(w^{*}\) in direction \(_{i}V^{_{1}}-V^{_{i+1}}\). Thus, we consider another estimate of \(w^{*}\) based only on some \(_{i+1}\)'s with a relatively large \(\|_{i}V^{_{1}}-V^{_{i+1}}\|_{2}\). In particular, for any \(>0\), let \(d_{}:=_{i 2:(|v^{_{i}}|,|v^{-_{i}} |)}i-1\). That is to say, for \(i=2,,d_{}\), the policy \(_{i}\) satisfies \( u_{i},V^{_{i}}>\) and for any policy \(\), we have \( u_{d_{}+1},V^{}\). Then, for any policy \(\) and any unit vector \((V^{_{1}},,V^{_{d_{}}})^{}\), we have \(,V^{}\). This is because at round \(d_{}+1\), we pick an orthonormal basis \(_{1},,_{k-d_{}}\) of \((V^{_{1}},,V^{_{d_{}}})^{}\) (line 8 in Algorithm 1) and pick \(u_{d_{}+1}\) to be the one in which there exists a policy with the largest component as described in line 9. Hence, \(|_{j},V^{}|\) for all \(j[k-d_{}]\). Then, we have \(,V^{}=_{j=1}^{k-d_{}},_{j}_{j},V^{}\) by Cauchy-Schwarz inequality. Let \(^{()}^{d_{} k}\) be the sub-matrix comprised of the first \(d_{}\) rows of \(\). Then we consider an alternative estimate \(^{()}=_{x:^{()}x=_{1}} \|x\|_{2}\), the minimum norm solution of \(x\) to \(^{()}x=_{1}\). We upper bound in Lemma 2 and \(_{}|,V^{}- ^{()},V^{}|\) in Lemma 3. Then we are done with the proof of Theorem 1.

**Lemma 2**.: _If \(|_{i}-_{i}|_{}\) and \(_{i} C_{}\) for all \(i[d-1]\), for every \( 4C_{}^{}C_{V}d^{}_{}^{ }\), we have \(|^{()},V^{}- w^{ },V^{}|(C_{ }^{4}d_{}^{2}}{\|w^{}\|_{2}^{2}_{}}+\|w^{}\|_{2})\) for all \(\), where \(w^{}=}{ w^{},V^{_{1}}}\)._

Since we only remove the rows in \(\) corresponding to \(u_{i}\)'s in the subspace where no policy's value has a large component, \(\) and \(^{()}\) are close in terms of \(_{}|,V^{}- ^{()},V^{}|\).

**Lemma 3**.: _If \(|_{i}-_{i}|_{}\) and \(_{i} C_{}\) for all \(i[d-1]\), for every policy \(\) and every \( 4C_{}^{}C_{V}d^{}_{}^{ }\), we have \(| V^{}-^{()} V^{}| ((+1)^{d-d_{}}C_{}^{()})\,,\) where \(^{()}=C_{}^{4}d_{}^{2}\|w^{ }\|_{2}^{2}_{}}{^{2}}+\|w^{ }\|_{2}\) is the upper bound in Lemma 2._

Note that the result in Theorem 1 has a factor of \(k^{}\), which is exponential in \(d\). Usually, we consider the case where \(k=(1)\) is small and thus \(k^{d}=(1)\) is small. We get rid of the exponential dependence on \(d\) by applying \(^{()}\) to estimate \(w^{*}\) directly, which requires us to set the value of \(\) beforehand. The following theorem follows directly by assigning the optimal value for \(\) in Lemma 2.

**Theorem 2**.: _Consider the algorithm of computing \(\) and any solution \(^{()}\) to \(^{()}x=_{1}\) for \(=k^{}^{}\) and outputting the policy \(^{^{()}}=_{}^{() },V^{}\). Then the policy \(^{^{()}}\) satisfies that \(v^{*}- w^{*},V^{^{^{()}}} (k^{}^{})\,.\)_

Notice that the algorithm in Theorem 2 needs to set the hyperparameter \(\) beforehand while we don't have to set any hyperparameter in Theorem 1. The improper value of \(\) could degrade the performance of the algorithm. But we can approximately estimate \(\) by binary searching \(\) and comparing \(_{1}\) against the scaled version of itself \((1-)_{1}\) until we find an \(\) such that the user cannot distinguish between \(_{1}\) and \((1-)_{1}\). Then we can obtain an estimate of \(\) and use the estimate to set the hyperparameter.

We remark that though we think of \(k\) as a small number, it is unclear whether the dependency on \(\) in Theorems 1 and 2 is optimal. The tight dependency on \(\) is left as an open problem. We briefly discuss a potential direction to improve this bound in Appendix I.

## 4 Learning from Weighted Trajectory Set Representation

In the last section, we represented policies using their explicit form as state-action mappings. However, such a representation could be challenging for users to interpret. For example, how safe is a car described by a list of \(||\) states and actions such as "turning left"? In this section, we design algorithms that return a more interpretable policy representation--a weighted trajectory set.

Recall the definition in Section 2, a weighted trajectory set is a small set of trajectories from the support of the policy and corresponding weights, with the property that the expected return of the trajectories in the set (according to the weights) is **exactly** the value of the policy (henceforth, the _exact value property_).8As these sets preserve all the information regarding the multi-objective values of policies, they can be used as policy representations in policy comparison queries of Algorithm 3 without compromising on feedback quality. Thus, using these representations obtain the same optimality guarantees regarding the returned policy in Section 3 (but would require extra computation time to calculate the sets).

There are two key observations on which the algorithms in this section are based:

(1) Each policy \(\) induces a distribution over trajectories. Let \(q^{}()\) denote the probability that a trajectory \(\) is sampled when selecting actions according to \(\). The expected return of all trajectories under \(q^{}\) is identical to the value of the policy, i.e., \(V^{}=_{}q^{}()()\). In particular, the value of a policy is a convex combination of the returns of the trajectories in its support. However, we avoid using this convex combination to represent a policy since the number of trajectories in the support of a policy could be exponential in the number of states and actions.

(2) The existence of a small weighted trajectory set is implied by Caratheodory's theorem. Namely, since the value of a policy is in particular a convex combination of the returns of the trajectories in its support, Caratheodory's theorem implies that there exist \(k+1\) trajectories in the support of the policy and weights for them such that a convex combination of their returns is the value of the policy. Such a \((k+1)\)-sized set will be the output of our algorithms.

We can apply the idea behind Caratheodory's theorem proof to compress trajectories as follows. For any \((k+2)\)-sized set of \(k\)-dimensional vectors \(\{_{1},,_{k+2}\}\), for any convex combination of them \(=_{i=1}^{k+2}p_{i}_{i}\), we can always find a \((k+1)\)-sized subset such that \(\) can be represented as the convex combination of the subset by solving a linear equation. Given an input of a probability distribution \(p\) over a set of \(k\)-dimensional vectors, \(M\), we pick \(k+2\) vectors from \(M\), reduce at least one of them through the above procedure. We repeat this step until we are left with at most \(k+1\) vectors. We refer to this algorithm as C4 (Compress Convex Combination using Caratheodory's theorem). The pseudocode is described in Algorithm 4, which is deferred to Appendix J due to space limit. The construction of the algorithm implies the following lemma immediately.

**Lemma 4**.: _Given a set of \(k\)-dimensional vectors \(M^{k}\) and a distribution \(p\) over \(M\), \((M,p)\) outputs \(M^{} M\) with \(|M^{}| k+1\) and a distribution \(q^{M^{}}\) satisfying that \(_{ q}[]=_{ p}[]\) in time \((|M|k^{3})\)._

So now we know how to compress a set of trajectories to the desired size. The main challenge is how to do it **efficiently** (in time \(((H|S|||))\)). Namely, since the set of all trajectory returns from the support of the policy could be of size \((||^{H})\), using it as input to C4 Algorithm is inefficient. Instead, we will only use C4 as a subroutine when the number of trajectories is small.

We propose two efficient approaches for finding weighted trajectory representations. Both approaches take advantage of the property that all trajectories are generated from the same policy on the same MDP. First, we start with a small set of trajectories of length of \(1\), expand them, compress them, and repeat until we get the set of trajectory lengths of \(H\). The other is based on the construction of a layer graph where a policy corresponds to a flow in this graph and we show that finding representative trajectories is equivalent to flow decomposition.

In the next subsection, we will describe the expanding and compressing approach and defer the flow decomposition based approach to Appendix K due to space considerations. We remark that the flow decomposition approach has a running time of \((H^{2}||^{2}+k^{3}H|| ^{2})\) (see appendix for details), which underperforms the expanding and compressing approach (see Theorem 3) whenever \(||H+||k^{3}=(k^{4}+k||)\). For presentation purposes, in the following, we only consider the deterministic policies. Our techniques can be easily extended to random policies.9The basic idea is to find \(k+1\) trajectories of length \(1\) to represent \(V^{}\) first and then increase the length of the trajectories without increasing the number of trajectories. For policy \(\), let \(V^{}(s,h)=_{S_{0}=s}[_{t=0}^{h-1}R(S_{t},(S_{t}))]\) be the value of \(\) with initial state \(S_{0}=s\) and time horizon \(h\). Since we study the representation for a fixed policy \(\) in this section, we slightly abuse the notation and represent a trajectory by \(^{}=(s,s_{1},,s_{H})\). We denote the state of trajectory \(\) at time \(t\) as \(s_{t}^{}=s_{t}\). For a trajectory prefix \(=(s,s_{1},,s_{h})\) of \(\) with initial state \(s\) and \(h H\) subsequent states, the return of \(\) is \(()=R(s,(s))+_{t=1}^{h-1}R(s_{t},(s_{t}))\). Let \(J()\) be the expected return of trajectories (of length \(H\)) with the prefix being \(\), i.e.,

\[J():=()+V(s_{h}^{},H-h)\,.\]

For any \(s\), let \( s\) denote the trajectory of appending \(s\) to \(\). We can solve \(V^{}(s,h)\) for all \(s,h[H]\) by dynamic programming in time \((kH||^{2})\). Specifically, according to definition, we have \(V^{}(s,1)=R(s,(s))\) and

\[V^{}(s,h+1)=R(s,(s))+_{s^{}}P(s^{ }|s,(s))V^{}(s^{},h)\,.\] (4)

Thus, we can represent \(V^{}\) by

\[V^{}= R(s_{0},(s_{0}))+_{s}P(s|s_{0},(s_{0}))V^{ }(s,H-1)=_{s}P(s|s_{0},(s_{0}))J(s_{0},s)\,.\]

By applying C4, we can find a set of representative trajectories of length \(1\), \(F^{(1)}\{(s_{0},s)|s\}\), with \(|F^{(1)}| k+1\) and weights \(^{(1)}^{F^{(1)}}\) such that

\[V^{}=_{ F^{(1)}}^{(1)}()J()\,.\] (5)

Supposing that we are given a set of trajectories \(F^{(t)}\) of length \(t\) with weights \(^{(t)}\) such that \(V^{}=_{ F^{(t)}}^{(t)}()J()\), we can first increase the length of trajectories by \(1\) through Eq (4) and obtain a subset of \(\{ s| F^{(t)},s\}\), in which the trajectories are of length \(t+1\). Specifically, we have

\[V^{}=_{ F^{(t)},s}^{(t)}()P(s|s_{t}^{ },(s_{t}^{}))J( s)\,.\] (6)

Then we would like to compress the above convex combination through C4 as we want to keep track of at most \(k+1\) trajectories of length \(t+1\) due to the computing time. More formally, let \(J_{F^{(t)}}:=\{J( s)| F^{(t)},s\}\) be the set of expected returns and \(p_{F^{(t)},^{(t)}}^{F^{(t)}}\) with \(p_{F^{(t)},^{(t)}}( s)=^{(t)}()P(s|s_{t}^{},(s_{t }^{}))\) be the weights appearing in Eq (6). Here \(p_{F^{(t)},^{(t)}}\) defines a distribution over \(J_{F^{(t)}}\) with the probability of drawing \(J( s)\) being \(p_{F^{(t)},^{(t)}}( s)\). Then we can apply C4 over \((J_{F^{(t)}},p_{F^{(t)},^{(t)}})\) and compress the representative trajectories \(\{ s| F^{(t)},s\}\). We start with trajectories of length \(1\) and repeat the process of expanding and compressing until we get trajectories of length \(H\). The details are described in Algorithm 2.

```
1: compute \(V^{}(s,h)\) for all \(s,h[H]\) by dynamic programming according to Eq (4)
2:\(F^{(0)}=\{(s_{0})\}\) and \(^{(0)}(s_{0})=1\)
3:for\(t=0,,H-1\)do
4:\(J_{F^{(t)}}\{J( s)| F^{(t)},s\}\) and \(p_{F^{(t)},^{(t)}}( s)^{(t)}()P(s|s_{t}^{ },(s_{t}^{}))\) for \( F^{(t)},s\) // expanding step
5:\((J^{(t+1)},^{(t+1)})(J_{F^{(t)}},p_{F^{(t)}, ^{(t)}})\) and \(F^{(t+1)}\{|J() J^{(t+1)}\}\) // compressing step
6:endfor
7: output \(F^{(H)}\) and \(^{(H)}\) ```

**Algorithm 2** Expanding and compressing trajectories

**Theorem 3**.: _Algorithm 2 outputs \(F^{(H)}\) and \(^{(H)}\) satisfying that \(|F^{(H)}| k+1\) and \(_{ F^{(H)}}^{(H)}()()=V^{}\) in time \((k^{4}H||+kH||^{2})\)._

The proof of Theorem 3 follows immediately from the construction of the algorithm. According to Eq (5), we have \(V^{}=_{ F^{(t)}}^{(1)}()J()\). Then we can show that the output of Algorithm 2 is a valid weighted trajectory set by induction on the length of representative trajectories. C4 guarantees that \(|F^{(t)}| k+1\) for all \(t=1,,H\), and thus, we only keep track of at most \(k+1\) trajectories at each step and achieve the computation guarantee in the theorem. Combined with Theorem 1, we derive the following Corollary.

**Corollary 1**.: _Running the algorithm in Theorem 1 with weighted trajectory set representation returned by Algorithm 2 gives us the same guarantee as that of Theorem 1 in time \((k^{2}H||^{2}||+(k^{5}H| |+k^{2}H||^{2})())\)._

## 5 Discussion

In this paper, we designed efficient algorithms for learning users' preferences over multiple objectives from comparative feedback. The efficiency is expressed in both the running time and number of queries (both polynomial in \(H,||,||,k\) and logarithmic in \(1/\)). The learned preferences of a user can then be used to reduce the problem of finding a personalized optimal policy for this user to a (finite horizon) single scalar reward MDP, a problem with a known efficient solution. As we have focused on minimizing the policy comparison queries, our algorithms are based on polynomial time pre-processing calculations that save valuable comparison time for users.

The results in Section 3 are of independent interest and can be applied to a more general learning setting, where for some unknown linear parameter \(w^{*}\), given a set of points \(X\) and access to comparison queries of any two points, the goal is to learn \(_{x X} w^{*},x\). E.g., in personalized recommendations for coffee beans in terms of the coffee profile described by the coffee suppliers (body, aroma, crema, roast level,...), while users could fail to describe their optimal coffee beans profile, adopting the methodology in Section 3 can retrieve the ideal coffee beans for a user using comparisons (where the mixing with "do nothing" is done by diluting the coffee with water and the optimal coffee for a given profile is the one closest to it).

When moving from the explicit representation of policies as mappings from states to actions to a more natural policy representation as a weighted trajectory set, we then obtained the same optimality guarantees in terms of the number of queries. While there could be other forms of policy representations (e.g., a small subset of common states), one advantage of our weighted trajectory set representation is that it captures the essence of the policy multi-objective value in a clear manner via \((k)\) trajectories and weights. The algorithms provided in Section 4 are standalone and could also be of independent interest for explainable RL (Alharin et al., 2020). For example, to exemplify the multi-objective performance of generic robotic vacuum cleaners (this is beneficial if we only have e.g., \(3\) of them--we can apply the algorithms in Section 4 to generate weighted trajectory set representations and compare them directly without going through the algorithm in Section 3.).

An interesting direction for future work is to relax the assumption that the MDP is known in advance. One direct way is to first learn the model (in model-based RL), then apply our algorithms in the learned MDP. The sub-optimality of the returned policy will then depend on both the estimation error of the model and the error introduced by our algorithms (which depends on the parameters in the learned model).