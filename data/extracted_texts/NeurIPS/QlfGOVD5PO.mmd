# Convergence of Actor-Critic Methods with

Multi-Layer Neural Networks

 Haoxing Tian, Ioannis Ch. Paschalidis, Alex Olshevsky

Department of Electrical and Computer Engineering

Boston University

Boston, MA 02215, USA

{tianhx, yannisp, alexols}@bu.edu

###### Abstract

The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is \((1/)+O()\), with \(m\) being the width of the neural network and \(\) the approximation quality of the best critic neural network over the projected set.

## 1 Introduction

_Reinforcement Learning (RL)_ has emerged as a powerful tool for solving decision-making problems in a model-free way. Among the various RL algorithms, the _Actor-Critic (AC)_ method (Konda and Tsitsiklis (1999); Barto et al. (1983)) has shown great success in various domains, including robotics, game playing, and control systems (LeCun et al. (2015); Mnih et al. (2016); Silver et al. (2017)). AC involves simultaneous updates of two networks: an actor network that employs policy gradient (Sutton et al. (1999)) to update a parameterized policy, and a critic network which is driven by the _Temporal Differences (TD)_ in the estimated value function. While AC methods with neural networks used for both actor and critic have achieved widespread use in practice, a fully satisfactory analysis of their convergence guarantees is currently lacking.

In recent years, a number of theoretical studies of AC have obtained provable convergence rates and performance analyses. Almost all works in this area assumed linear, rather than neural network-based, approximators for both actor and critic. A "two-timescale" linear AC was analysed in Wu et al. (2020), with a convergence rate of \((T^{-1/4})\), where \(T\) is the total number of iterations and \(()\) refers to potential logarithmic terms omitted from the notation; the term "two-timescale" refers to the fact that the stepsizes for the actor update and critic update are not proportional to each other, but rather the actor steps are asymptotically negligible compared to the critic steps. A "single-timescale" linear AC method was considered in Olshevsky and Gharesifard (2022); Chen et al. (2021) and both works obtained a convergence rate of \(O(T^{-0.5})\) under an i.i.d. sampling assumption on the underlying MDP. The more realistic Markov sampling case was analyzed in the recent paper Chen and Zhao (2022), which also established a convergence rate of \((T^{-0.5})\). All these results relied on linear approximations.

To our knowledge, convergence rates for AC with neural approximators were analyzed only in two recent works Wang et al. (2019); Cayci et al. (2022). Both of these papers considered neural networks with a single hidden layer. The paper Wang et al. (2019) obtained a convergence rate of \(O(T^{-0.5})\) with a final error of \(O(m^{-0.25})\) under i.i.d. sampling, where \(m\) is the width of hidden layer. The case of Markov sampling was considered in Cayci et al. (2022) which improved this to \((T^{-0.5})\) and \((m^{-0.5})\), respectively. Further, both Wang et al. (2019); Cayci et al. (2022) considered "double-loop" methods where, in the inner loop, the critic takes sufficiently many steps to accurately estimate \(Q\)-values. Such double-loop methods do not match prevailing practice and are considerably easier to analyze since they can be shown to approximate gradient descent.

Further, Cayci et al. (2022) required a projection onto a ball of radius \(O(m^{-1/2})\) around the initial condition. Although a full representation theory for such neural networks is unknown, this is clearly limiting as compared to Wang et al. (2019) which only required projection onto a ball of constant radius. For nonlinear approximations, such projections are usually needed to stabilize the algorithm; without them, AC can diverge both in theory and practice.

The main contribution of this paper is to provide the first analysis of AC with neural networks of arbitrary depth. While replicating the earlier results of a \((T^{-0.5})\) convergence rate and \((m^{-0.5})\) error, our work considers a _single-loop_ method with proportional step-sizes (sometimes called "single-timescale"). We prove this result under Markov sampling and project onto a ball of constant radius around the initial condition. An explicit comparison of our result to previous work is given in Table 1. A more technical comparison is also given later after the statement of our main result.

Our main technical tool is the so-called "gradient splitting" view of TD learning. This idea began with the paper Ollivier (2018) which observed that TD learning is exactly gradient descent when the underlying policy is such that the state transition matrix is reversible. In Liu and Olshevsky (2021), this was generalized to non-reversible policies by introducing the notion of a "gradient splitting" (discussed formally later in this work) and observing that, for linear approximation, TD updates are an example of gradient splitting. Gradient splitting is closely related to gradient descent, and the two processes can be analyzed similarly. A generalization to neural TD learning was given in Tian et al. (2023), which argued for an interpretation of nonlinear TD as approximate gradient splitting.

The analysis of AC that we perform in this work is trickier because both actor and critic updates rely on each other, and one must prove that the resulting errors in each process do not compound in interaction with each other. This difficulty arises because we do not consider the "double loop" case where the actor can effectively wait for the critic to converge, so that actor steps resemble gradient steps with error; rather both actor and critic update simultaneously their (imperfect) estimates. Similarly to what was done in Olshevsky and Gharesifard (2022), we show that we can draw on some ideas from control theory to prove that the resulting process converges with a so-called "small-gain" analysis.

## 2 Preliminaries

We begin by standardizing notation and stating the key concepts that will enable us to formulate our results alongside all the assumptions they require.

  Reference & Algorithm & Sampling & Approximation &  Projection \\ Radius \\  &  Convergence rate \\ w.r.t. \(T\) \\  & w.r.t. \(m\) \\  Wu et al. (2020) &  Two-timescale \\ Single-loop \\  & Markov & Linear & N/A & \((T^{-0.4})\) & N/A \\  Olshevsky and Gharesifard (2022) &  Single-timescale \\ Single-loop \\  & Li.d. & Linear & N/A & \(O(T^{-0.5})\) & N/A \\  Chen et al. (2021) &  Single-timescale \\ Single-loop \\  & Li.dâ€™ & Linear & N/A & \(O(T^{-0.5})\) & N/A \\  Chen and Zhao (2022) &  Single-timescale \\ Single-loop \\  & Markov & Linear & N/A & \((T^{-0.5})\) & N/A \\  Wang et al. (2019) & Double-loop & Li.d. & Single hidden layer & Constant & \(O(T^{-0.5})\) & \(O(m^{-0.25})\) \\  Cayci et al. (2022) & Double-loop & Markov & Single hidden layer & Decaying &  \(O(T^{-0.5})\) \\ \(m\) sufficiently large \\  & \((m^{-0.5})\) \\  Ours & 
 Single-timescale \\ Single-loop \\  & Markov & Any depth & Constant & \((T^{-0.5})\) & \((m^{-0.5})\) \\  

Table 1: Comparisons with previous work.

### Markov Decision Processes (MDP)

A finite discounted-reward MDP can be described by a tuple \((S,A,P_{},r,)\) where \(S\) is a finite state-space whose elements are vectors, and we use \(s_{0} S\) to denote the starting state; \(A\) is a finite action space with cardinality \(n_{a}\); \(P_{}=(P_{}(s^{}|s,a))_{s,s^{} S,a A}\) is the transition probability matrix, where \(P_{}(s^{}|s,a)\) is the probability of transitioning from \(s\) to \(s^{}\) after taking action \(a\); \(r:S A\) is the reward function, where \(r(s,a)\) stands for the expected reward at state \(s\) and taking action \(a\); and \((0,1)\) is the discount factor.

A policy \(\) is a mapping \(:S A\) where \((a|s)\) is the probability that the agent takes action \(a\) in state \(s\). Given a policy \(\), we can define the state transition matrix \(P^{}_{}=(P^{}_{}(s^{}|s))_{s,s^{} S}\) and the state-action transition matrix \(P_{}=(P_{}(s^{},a^{}|s,a))_{(s,a),(s^{},a^{})  S A}\) as

\[P^{}_{}(s^{}|s)=_{a A}P_{}(s^{}|s,a) (a|s), P_{}(s^{},a^{}|s,a)=P_{}(s^{} |s,a)(a^{}|s^{}).\]

The stationary distribution over state-action pairs \(_{}\) is defined to be a nonnegative vector with coordinates summing to one and satisfying \(_{}^{T}=_{}^{T}P_{}\), while the stationary distribution over states \(_{}^{}\) is defined similarly with \({_{}^{}}^{T}={_{}^{}}^{T}P^{}_{}\). The Perron-Frobenius theorem guarantees that such a \(_{}\) and \(_{}^{}\) exist and are unique subject to some conditions on \(P^{}_{},P_{}\), e.g., aperiodicity and irreducibility (Gantmacher (1964)). We use \(_{}(s,a)\) to denote each entry of \(_{}\) and \(_{}^{}(s)\) each entry of \(_{}^{}\). Clearly,

\[_{}(s,a)=_{}^{}(s)(a|s).\] (1)

The value function and the \(Q\)-function of a policy \(\) is defined as:

\[V_{}^{*}(s)=_{a A}(a|s)Q_{}^{*}(s,a), Q_{}^{*}(s,a)= _{s,a,}[_{t=0}^{+}^{t}r(s_{t},a_{t})].\] (2)

Here, \(_{s,a,}\) stands for the expectation when action \(a\) is chosen in state \(s\) and all subsequent actions are chosen according to policy \(\). Throughout the paper, if \(\) can be parameterized by \(\), then we will use \(\) as a subscript instead of \(\), e.g., by writing \(V_{}^{*}(s)\) instead of \(V_{_{}}^{*}(s)\).

If \(\) is parameterized by \(\), the \(Q\)-values satisfy the Bellman equation

\[Q_{}^{*}(s,a)=r(s,a)+_{s^{},a^{}}P_{}(s^{ },a^{}|s,a)Q_{}^{*}(s^{},a^{}),\] (3)

which can be stated in matrix notation as

\[Q_{}^{*}=R+ P_{}Q_{}^{*},\] (4)

where \(Q_{}^{*}=(Q_{}^{*}(s,a))_{(s,a) S A}\) and \(R=(R(s,a))_{(s,a) S A}\) are vectors that stack up the \(Q\)-values and rewards, respectively. We will assume rewards are bounded:

**Assumption 2.1** (Bounded Reward).: _For any \(s,a S A\), \(|r(s,a)| r_{}\)._

This assumption is commonly adopted throughout the literature, e.g., among the previous literature in Cayci et al. (2022); Wu et al. (2020). An obvious implication of this is an upper bound on the \(Q\)-values for any policy:

\[|Q_{}^{*}(s,a)|}{1-}.\] (5)

### The Policy Gradient Theorem

We introduce the quantity \(_{}(s)\), commonly called the discounted occupation measure which is defined as

\[_{}(s)=_{t=0}^{+}^{t}P_{}(S_{t}=s),\]

where \(P_{}(S_{t}=s)\) is the probability of being in state \(s\) after \(t\) steps --- and recall that we always begin in state \(s_{0}\). Next, we define \(_{}(s,a)\) as

\[_{}(s,a)=_{}(s)(a|s,).\]Note that the sum of both \((s)\) and \((s,a)\) equal to \((1-)^{-1}\) rather than \(1\):

\[_{s S}_{}(s)=_{(s,a) S A}_{}(s,a)=.\] (6)

Now we are prepared to state the policy gradient theorem Sutton and Barto (2018).

**Theorem 2.1**.: _(Policy Gradient Theorem)_

\[ V_{}^{*}=_{s S}_{}(s)_{a A}Q_{}^{*} (s,a)(a|s,)\]

It is standard to write this as

\[ V_{}^{*}=_{(s,a) S A}_{}(s,a)Q_{}^{* }(s,a)(a|s,),\]

which can be further rewritten in matrix form as

\[ V_{}^{*}=()^{T}_{}Q_{}^{*},\] (7)

where \(_{}\) is a diagonal matrix stacking up the \(_{}(s,a)\) as its diagonal entries.

### Parameterized Value Function and Policy

We will now state the various assumptions we have on the policies and their parametrizations. We will say that a function \(f:\) is \(L\)-Lipschitz if

\[|f(x)-f(y)| L|x-y|,\  x,y,\]

and a differentiable function \(f:\) is \(H\)-smooth if

\[| f(x)- f(y)| H|x-y|,\  x,y.\]

We will be using a multi-layer neural network to approximate the \(Q\) values under a policy. We basically follow the same setting as in Liu et al. (2020), with some changes as far as notation goes. Specifically, we define the following recursion

\[x^{(k)}=}(w^{(k)}x^{(k-1)}),k\{1,,K\},\]

where \(\) is an activation function and \(x^{(k)}\) stands for the value of \(k\)'th layer (\(x^{(0)} S A\) is the input to this neural network). The neural network outputs \(Q(s,a,w)\), which is defined as

\[Q(s,a,w)=}b^{T}x^{(K)}.\]

Notice that the output is linear to \(x^{(K)}\) as no activation function is applied here. While this formulation does not have a bias, it is equivalent to a formulation with a bias if we pad all inputs with a single \(1\), and add an additional node to every hidden layer that propagates this \(1\) to subsequent layers. We will assume that all the hidden layers have the same width which we denote by \(m\), i.e., all the matrices \(w^{(k)}\) have \(m\) rows and all the vector \(x^{(k)},k 1\) are \(m\)-dimensional. The total number of layers in the neural network is denoted by \(K\).

For simplicity, we will make the following assumption on the neural network. Throughout the paper, we will use \(||||\) for the standard \(l_{2}\)-norm.

**Assumption 2.2**.: _(Neural architecture and initialization) Suppose the neural network satisfies the following properties:_

* _(Input assumption) Any input to the neural network satisfies_ \(||x^{(0)}|| 1\)_._
* _(Activation function assumption)_ \(\) _is_ \(L_{}\)_-Lipschitz and_ \(H_{}\)_-smooth._
* _(Initialization assumption) Each entry of the vector_ \(b\) _satisfies_ \(|b_{r}| 1, r\)_, and each entry of_ \(w^{(k)}\) _is randomly chosen from_ \(N(0,1)\)_, independently across entries._Liu et al. (2020) showed that with these assumptions, the following result holds with high probability - which we state as an assumption for our work.

**Assumption 2.3**.: _The absolute value of each entry of \(x^{(k)}\) (the output of layer \(k\) of the neural network) is \(_{m}(1)\) at initialization._

Next, we will stack up the weights of different layers into a column vector \(w\) consisting of the entries of the matrices \(w^{(1)},,w^{(K)}\), with its norm defined by

\[||w||^{2}=_{k=1}^{K}||w^{(k)}||_{F}^{2},\]

where \(||||_{F}\) is the Frobenius norm. During the training process, only the weights \(w\) will be updated while the final weights \(b\) will be left to their initial value. For convenience, we define the vector \(Q(w)=(Q(s,a,w))_{(s,a) S A}\) which stacks up \(Q(s,a,w)\) over all state-action pairs \((s,a)\). While this vector will never be actually used in the execution of any algorithm we consider due to its high dimensionality, it will be useful in some of the arguments we will make. Finally, we assume the parametrization of the policy \(\) is smooth.

**Assumption 2.4** (Smooth parametrization).: _For all \(s,a\), the quantities \((a|s,)\), \((a|s,)\) are \(L_{}\)-Lipschitz and \(L^{}_{}\)-Lipschitz with respect to \(\), respectively._

Note that this forces us to use a smooth activation function and rules out non-differentiable activation functions such as ReLU. If a RELU-like activation is needed, one could use a GeLU or ELU activation (which are smooth versions of ReLU) and still satisfy the above assumption. Note, also, that this assumption implicitly assumes that all policies are exploratory in the sense of assigning a positive probability to each action, since the derivative of \( x\) blows up as \(x 0\).

### Neural Actor-Critic

We will use \(_{W}\{\}\) refer to projection onto a ball with constant radius around the initial condition of the critic, where

\[W=\{w||w-w_{0}||_{w}\},\ _{w}\ \]

We now introduce the neural AC, which updates the actor and critic parameters as

\[w_{t+1}=_{W}\{w_{t}+^{w}_{t}_{w}Q(s_{t},a _{t},w_{t})\},_{t+1}=_{t}-}{1- }Q(_{t},_{t},w_{t})_{}(_{t}| {s}_{t},_{t}).\]

where \(_{t}\) is the TD error defined by

\[_{t}=r(s_{t},a_{t})+ Q(s^{}_{t},a^{}_{t},w_{t})-Q(s_{ t},a_{t},w_{t}),\] (8)

and the samples are obtained as follows:

1. the state \(s_{t}\) is generated by taking a step in the Markov chain \(P_{}\) from \(s_{t-1}\);
2. the action \(a_{t}\) is chosen according to the policy \((a|s_{t},_{t})\);
3. the next state \(s^{}_{t}\), i.e, \(s^{}_{t}=s_{t+1}\), is determined according to the transition probability \(P_{}\) of the MDP;
4. the action \(a^{}_{t}\) is an action chosen at the next state according to the policy \((a|s^{}_{t},_{t})\);
5. the state-action pair \((_{t},_{t})\) is obtained by first sampling a geometric random variable \(T\) with distribution \(\{P(T=t)=(1-)^{},t 0\}\), and second obtaining \(T\) transitions by starting at \(s_{0}\) and taking actions according to \((a|s,_{t})\). Note that this update has to be re-done at every step, i.e., every \(t\) requires \(()\) steps.

The above algorithm will be referred to as _actor-critic with Markov sampling_. It is also possible to consider a simplified variant, where step 1 is slightly altered as follows: the state \(s_{t}\) is instead chosen i.i.d. at every step from the stationary distribution of \(_{_{t}}\) of the policy \(_{_{t}}\). This is referred to as _actor-critic with i.i.d. sampling_.

#### 2.4.1 Approximation Assumptions

It is evident that any performance bound on AC will depend on how well the neural network used for the critic can approximate the true value function. If we choose a neural network architecture for which universal approximation theorems do not apply and it happens to poorly approximate the true \(Q\)-functions, we will likely obtain poor results. Here, we will largely sidestep this issue by defining \(\) to be the approximation quality of the critic; our final performance results will be in terms of \(\).

Formally, we say that the vector \(Q\)_is an \(\)-approximation to the true value function \(Q^{*}_{_{t}}\) of the policy \(_{_{t}}\) if_

\[_{(s,a) S A}|Q(s,a)-Q^{*}_{_{t}}(s,a)|.\] (9)

We then make the following assumption.

**Assumption 2.5**.: _(Approximation capabilities of critic) For all \(\), there exists some set of weights \(_{}\) which give rise to an \(\)-approximation of \(Q^{*}_{}\)._

Note that, since we do not say what \(\) is, this assumption could well be a definition of \(\). Throughout the paper we will use \(^{*}_{_{t}}\) to denote an \(\)-approximation to \(Q^{*}_{_{t}}\) guaranteed by the above assumption. Thus,

\[Q(_{_{t}})=^{*}_{_{t}}.\]

Further, we will assume that \(_{}\) is a smooth function of \(\) in the sense of its first and second derivatives.

**Assumption 2.6**.: _(Smoothness of critic approximation) Suppose there exists scalars \(L_{w}(i)\) and \(H_{w}(i)\) such that for all \(\),_

\[||^{*}_{}(i)|| L_{w}(i),_{} ^{2}^{*}_{}(i)} H_{w}(i).\]

_where \(_{}\{\}\) stands for the largest eigenvalue._

For convenience, we define

\[L_{w}=L_{w}(i)^{2}}, H_{w}=H_{w}(i)^{2}}.\]

Finally, we need an additional assumption on the critic neural network. It should be obvious that any analysis of actor-critic has to assume that the critic _is capable_ of approximating the correct \(Q\)-values. One part of this was already assumed earlier in Assumption 2.5, where we assumed that an approximation exists. However, it should be clear that in the nonlinear case this is insufficient: just because there exists an approximation which is good doesn't follow that it will be found during training, which is not known converge to the global minimizer in the nonlinear case, but rather only to a critical point.

We thus need something to rule out the possibility that the critic training gets stuck at a bad crticial point. It turns out that it suffices to assume (a quantitative version of the fact that) the critic is one-to-one map from weights to value functions.

**Assumption 2.7**.: _(State regularity) There exists some constant \(^{}>0\) such that_

\[||Q(w)-^{*}_{}||^{}||w-^{*}_{}||.\]

Let us parse the meaning of this assumption. Because \(Q(^{*}_{})=^{*}_{}\), it is appropriately viewed as a quantitative version of the statement that if \(w_{1} w_{2}\), then \(Q(w_{1}) Q(w_{2})\). To see why this makes sense, note that the number of states is typically many magnitudes larger than the number of parameters in the critic. For example, in many applications the number of states often corresponds to the number of images (when states are captured through images) which is astronomical. Thus \(Q(w)\) will map \(w\) to a much higher dimensional space.

If the states \(s\) are generated from a probability distribution which has a continuous density, and the activation functions are continuous and increasing, the chance that \(Q_{w_{1}}(s)=Q_{w_{2}}(s)\) even for one state \(s\) is zero. That is why we label it "state regularity" as above (and recall that \(Q(w)\) stacks up \(Q_{w}(s)\) for every state \(s\)).

On a technical level, this property ensures that critic actually finds a good critic approximation in spite of the nonlinearity of the update. If the features are linear, this reduces to the assumption that the features are linearly independent, an assumption which is made in all previous and related work on AC method (Wu et al. (2020); Olshevsky & Gharesifard (2022); Chen & Zhao (2022); Kumar et al. (2023)) and TD Learning (Liu & Olshevsky (2021); Xu & Gu (2020); Cai et al. (2019); Zou et al. (2019)).

### The Mixing of Markov Chains

It is standard to make an assumption to the effect that all the Markov chains that can arise satisfy a mixing condition. Otherwise, it is possible under Markov sampling for the state to fail to explore the entire state-space. This assumption, first introduced by Bhandari et al. (2018) in TD learning, now is commonly used in AC analysis (Olshevsky & Gharesifard (2022); Wu et al. (2020); Chen & Zhao (2022)).

**Assumption 2.8** (Markov chain mixing).: _There exists constants \(C>0\) and \([0,1)\) with the following property: for all \(\), if we consider a Markov chain generated by \(a_{t}(|s_{t},)\), \(s_{t+1} P_{}(|s_{t},a_{t})\) starting from state \(s\), then_

\[||p_{}-^{}_{}||_{1} C^{}, 0,  s S,\]

_where \(p_{}\) is the probability distribution of the state of this Markov chain after \(\) steps._

To assure AC explores every possible state, we make the following assumption:

**Assumption 2.9**.: _(Exploration) Suppose there exists some constant \(_{}>0\) such that, for all \(\), \(^{}_{}\) is uniformly bounded away from \(0\). In other words,_

\[^{}_{}_{}>0,.\]

Recall that \(_{}\) was defined earlier to be the stationary distribution of the transition matrix associated with the policy \(_{}\). A key point is that the constants \(C\), \(\) and \(_{}\) in the above assumptions do not depend on \(\).

We note that there is some redundancy in our assumptions. As discussed above, we require \(_{}(a|s)\) to have a smooth gradient for all \(s,a\), which ensures that \(_{}\) assigns a strictly positive probability to every action. This implies Assumptions 2.8 and 2.9 which can therefore be made into propositions. Nevertheless, we explicitly make Assumptions 2.8 and 2.9 (even though both of them are actually implied by our earlier assumption) since the quantities appearing in them (specifically, the mixing time \(\) and the constant \(_{}\)) appear in various bounds we will derive.

More precisely, we follow the earlier literature by setting \(C^{}\) to be proportional to \(T^{-0.5}\), the typical of stepsize in Stochastic Gradient Descent. We call the smallest \(\) such that \(C^{} O(T^{-0.5})\) the mixing time and denote it by \(_{}\). It is easy to see that \(_{}=O((1-)^{-1} T).\) The quantity \(_{}\) will appear throughout our paper.

### \(D\)-norm and Dirichlet Norm in MDPs

A key ingredient is our analysis is the choice of norm: we have found that a certain norm originally introduced in Ollivier (2018) significantly simplifies analysis of the problem. We next introduce this norm and state our assumptions about it.

Let \(D_{}=(_{}(s,a))\) be the diagonal matrix whose elements are given by the entries of the stationary distribution \(_{}\) associated with the policy \(_{}\). Given a function \(f:S A\), its \(D\)-norm is defined as

\[||f||_{D}^{2}=f^{T}D_{}f=_{(s,a) S A}_{}(s,a)f(s,a )^{2}.\] (10)

The \(D\)-norm is similar to the Euclidean norm except each entry is weighted proportionally to the stationary distribution. We also define the Dirichlet semi-norm of \(f\):

\[||f||_{}^{2}=_{(s,a),(s^{},a^{}) S  A}_{}(s,a)P_{}(s^{},a^{}|s,a)(f(s^{}, a^{})-f(s,a))^{2}.\] (11)A semi-norm satisfies the axioms of a norm except that it may be equal to zero at a non-zero vector. Note that \(||f||_{}\) depends on the policy both through the stationary distribution \(_{}(s,a)\) as well as through the transition matrix \(P_{}\).

Finally, following Ollivier (2018), the weighted combination of the \(D\)-norm and the Dirichlet semi-norm is denoted as \(_{}(f)\) will be defined

\[_{}(f)=(1-)||f||_{D}^{2}+||f||_{}^{2}.\] (12)

Note that as long as \(_{}(s,a)>0\), which is stated in Assumption 2.9, for all \(s,a\), we have that \(_{}(f)}\) is a valid norm.

## 3 Our Main Results

To simplify the expression that follow, we will adopt the notations \(_{V}\) and \(_{Q}\) for the two losses that we want to bound in our paper:

\[_{V}=_{t=1}^{T}[|| V_{_{t}}^{ *}||^{2}],_{Q}=_{t=1}^{T}[ _{_{t}}(Q(w_{t})-_{_{t}}^{*})].\] (13)

Intuitively, \(_{V}\) corresponds to the actor error: ideally, we want to reach a point where the gradient of the actor value function is zero. Note that, since the value function is not convex in general, the actor error is measured in terms of distance to a stationary point as above.

Similarly, \(_{Q}\) is a measure of the critic error: it equals zero precisely if \(Q(w_{t})\), the approximator of \(Q\)-function, equals \(_{_{t}}\). Of course, as discussed above, the critic neural network may not be able to perfectly represent the true \(Q\)-function. Now we are ready to state our main results.

**Theorem 3.1**.: _Consider the neural AC algorithm mentioned in Section 2.4. Suppose Assumptions 2.1-2.9 hold and the step-sizes \(^{}\) and \(^{w}\) are both chosen to decay proportionally to \(O(T^{-0.5})\)._

1. _In the i.i.d. sampling case,_ \[_{V} O(})+O()+( }),_{Q} O(}) +O()+(}).\]
2. _In the Markov sampling case,_ \[_{V} O(}{})+O()+ (}),_{Q} O(}{})+O()+(} ).\]

In all \(O()\) notations above, we treat factors that do not depend on \(T,,m\) as constants.

We next provide a more detailed comparison to the previous works of (Wang et al. (2019); Cayci et al. (2022)). Our discussion partially reprises the discussion in the Introduction, but can now be discussed at a greater level of detail:

* **Arbitrary depth/single-timescale.** The main contribution of this paper to provide an analysis that applies to neural networks of arbitrary depth. Moreover, we do so in a single-loop/single-timescale method where the critic and actor iterate simultaneously, which is matching what is typically done in practice. Such an analysis is inherently more technically challenging, since when the actor can wait for the critic to go through sufficiently many iterations, one could argue that the resulting \(Q\)-values are approximately accurate and the process resembles gradient descent.
* **Representability.** Both previous works for the single-layer case assume the \(Q\)-function lies in some function class, which, as discussed after Assumption 6 in Farahmand et al. (2016), is one kind of "no function approximation error" assumption. By contrast, we make no such assumption: rather we allow any approximation error for the critic \(\), and our final result is given in terms of \(\).

* **Lower bound on \(m\).** Previous works require \(m\), the width of neural network, to be sufficiently large. In Wang et al. (2019), given that \(m\) is sufficiently large, Section 3.1 and Corollary A.3 argue that the gradient, denoted by \(_{}\) and \(_{w}\), can be well approximated by the "centered feature mapping corresponding to the initialization", denoted by \(_{0}\). In Cayci et al. (2022), this dependency is even more emphasised since the upper bound shown in Theorem 2 could diverge with small \(m\).
* **Relation to NTK theory.** NTK theory (Jacot et al. (2018)) tells us that neural networks get more linear as \(m\). The classic analyses of this proceed by arguing that as \(m\), the neural network stays close to its initialization during training Chizat et al. (2019). In that sense, we should expect to get a convergence result for AC as \(m\), but if the critic neural network stays close to its initial condition, the algorithm will effectively be using random linear features at initialization. For this reason, it is desirable not to argue that the critic neural network always stays close to its initial condition. We do not use such an argument in this work, whereas both Wang et al. (2019) and Cayci et al. (2022) obtain their results by arguing that the critic neural network stays close to its initial condition. This theoretical distinction is shown in Tian et al. (2023) to match what happens in simulations, which shows empirically that even for projected neural TD, the critic neural network will move to the _boundary_ of the projection ball.
* **Linearization.** Previous works assume some kind of linearization around the initial point. The objective is explicitly linearized in Wang et al. (2019).In Cayci et al. (2022), while the objective is not linearized, the neural networks weights are projected onto a radius of size \(O(1/)\) around the initial point.

## 4 Tools in Our Analysis

### Choice of Norm and Gradient Splitting

A linear function \(h()\) is said to be a gradient splitting of a convex quadratic \(f()\) minimized at \(=a\) if

\[ f()^{T}(a-)=h()^{T}(-a).\] (14)

In other words, a splitting \(h()\) has exactly the same inner product with the "direction to the optimal solution" as the true gradient of \(f()\) (up to the factor of \(1/2\)). The connection between this idea and RL was made in the following papers:

* In Ollivier (2018) it was shown that in TD Learning, if the matrix \(P\) corresponds to a reversible Markov chain, then \(E[(_{t})]=_{}(f)\) for some \(f\). This makes Neural TD easy to analyze in the reversible case as it is exactly gradient descent.
* In Liu and Olshevsky (2021), it was shown how to further use the function \(()\) to analyze TD learning with linear approximation when the policy is not necessarily reversible. _In particular, it was shown that the mean update of TD with linear approximation is a gradient splitting of the function \(()\). This is one of the crucial ideas we build on in this paper._

### Nonlinear Small-Gain Theorem

Inspired by Olshevsky and Gharesifard (2022), our second main tool is a nonlinear version of the small-gain theorem. Because the actor and critic update simultaneously, we need to rule out the possibility that errors in the actor compound with errors in the critic to create divergence. For example, it is conceivable that, when the policy is fixed, the critic converges to a reasonable approximation; when the critic is fixed, the actor converges to an approximate of the stationary point; but both updating simultaneously results in divergence.

The core idea of small-gain is to write these updates in such a way so that one can argue that if certain coefficients are small enough, this "interconnection" of the actor and critic systems converges. The

Figure 1: Key property of gradient splitting: \(h()\) has the same inner product with \(a-\) as \( f()\) up to a factor of \(1/2\).

small-gain theorem we use is a nonlinear version of the textbook version Drazin (1992). This is a widely-used trick in control theory that avoids the necessity of explicitly finding a Lyapunov function.

## 5 Conclusion

We have provided an analysis of Neural AC using a convex combination of the \(D\)-norm and the Dirichlet semi-norm to describe the error. Our main result is an error rate of \(O(T^{-0.5}+)+(m^{-0.5})\) under the i.i.d. sampling and \(O(( T)^{2} T^{-0.5}+)+(m^{-0.5})\) under the Markov sampling for neural networks of arbitrary depth. Crucially, our proof does not make assumptions that force the neural networks to stay close to their initial conditions, relying instead on arguments that show that neural networks which are not "too nonlinear" will still converge to an approximate minimum.

This research was partially supported by the NSF under grants CCF-2200052, DMS-1664644, and IIS-1914792, by the ONR under grant N00014-19-1-2571, by the DOE under grant DE-AC02-05CH11231, by the NIH under grant UL54 TR004130, and by Boston University.