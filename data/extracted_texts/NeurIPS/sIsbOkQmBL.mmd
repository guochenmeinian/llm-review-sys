# CultureLLM: Incorporating Cultural Differences into Large Language Models

Cheng Li

Institute of Software, CAS

chenglicat0228@gmail.com

&Mengzhuo Chen

Institute of Software, CAS

mengzhuo.happy@gmail.com

&Jindong Wang

Microsoft Research

jindong.wang@microsoft.com

&Sunayana Sitaram

Microsoft Research

Sunayana.Sitaram@microsoft.com

&Xing Xie

Microsoft Research

xing.xie@microsoft.com

Work done during Cheng's internship at MSRA.Corresponding author.

###### Abstract

Large language models (LLMs) have been observed to exhibit bias towards certain cultures due to the predominance of training data obtained from English corpora. Considering that multilingual cultural data is often expensive to procure, existing methodologies address this challenge through prompt engineering or culture-specific pre-training. However, these strategies may neglect the knowledge deficiency of low-resource cultures and necessitate substantial computing resources. In this paper, we propose **CultureLLM**, a cost-effective solution to integrate cultural differences into LLMs. CultureLLM employs the World Value Survey (WVS) as seed data and generates semantically equivalent training data through the proposed semantic data augmentation. Utilizing only \(50\) seed samples from WVS with augmented data, we fine-tune culture-specific LLMs as well as a unified model (CultureLLM-One) for \(9\) cultures, encompassing both rich and low-resource languages. Extensive experiments conducted on \(60\) culture-related datasets reveal that CultureLLM significantly surpasses various counterparts such as GPT-3.5 (by \(8.1\)%) and Gemini Pro (by \(9.5\)%), demonstrating performance comparable to or exceeding that of GPT-4. Our human study indicates that the generated samples maintain semantic equivalence to the original samples, offering an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM.

## 1 Introduction

Culture is a complex construct that encapsulates various identities, including, but not limited to, language, nationality, region, religion, and gender identity. Cultural bias is prevalent worldwide and refers to the tendency to favor specific cultural perspectives, values, and norms, which results in subjective opinions and may offend individuals from other cultures. For instance, according to the World Value Survey (Survey, 2022), Arabic culture believes that men are better political leaders thanwomen, while people in the United States maintain a contrary opinion.3 As large language models (LLMs)  gain prominence, they are reported to exhibit cultural bias and specifically show partiality towards Western culture, as English corpora dominate the training data . Low-resource cultures are frequently underrepresented due to the insufficient training data available from these cultures. LLMs' cultural bias constitutes a significant bottleneck in human-AI collaboration and considerably impedes AI democracy.

Tackling cultural bias necessitates that a large language model (LLM) acknowledges cultural differences . Kovac et al.  and Want et al.  studied the thought LLMs have enough knowledge of all cultures and devised prompt engineering technologies to induce LLMs to exhibit specific cultural perspectives. However, they are not effective, especially in low-resource cultures with limited data. Another line of work pre-trained culturally aware LLMs and then fine-tuned on specific datasets . They require the collection of large-scale pre-training and fine-tuning datasets and extensive computing resources, thus are not affordable to ordinary researchers and cannot handle low-resource culture. To date, training culturally aware LLMs at affordable costs remains a challenge.

In this paper, we propose **CultureLLM**, a cost-effective4 solution to incorporate cultural differences into LLMs. Technically speaking, CultureLLM is inspired by the well-known fact that LLMs are inevitably not robust to the style and format of the prompts , indicating that we can further leverage such a weakness to further improve the performance of LLMs by enriching the prompts. In particular, we focus on cultural values in this work. As shown in Figure 1, CultureLLM consists of three steps: sampling, semantic data augmentation, and fine-tuning. Inspired by Attitude-Behavior Consistency theory  which emphasizes that people's opinion is consistent with their behaviors, we use the World Values Survey (WVS)  as seed data. Then, we devise a semantic data augmentation approach to generate semantically equivalent samples. The aim is to generate semantic equivalent inputs, thus we can get the ground-truth from seed data directly. Finally, CultureLLM is obtained by fine-tuning both the seed and the generated data. WVS is a public opinion poll that contains people's opinions on cultural topics from different countries. To be specific, we select \(50\) seed samples from WVS, covering \(7\) topics: "social values", "migration", "security", "science and technology", "religious values", "ethical values and norms", and "political interest and political participation". Using these generated samples and answers from people in different cultures, we fine-tune specific and unified LLMs: specific LLMs are tailored for each culture such as CultureLLM-Ar for Arabic and CultureLLM-Tr for Turkish; unified LLMs (CultureLLM-One) are one LLM that fits all cultures.5

We build \(9\) specific CultureLLM and a CultureLLM-One covering both high- and low-resource cultures: Arabic culture, Bengali culture, Chinese culture, English culture, German culture, Korean culture, Portuguese culture, Spanish culture, and Turkish culture. Then we evaluated them on \(8\) culture-related downstream tasks: offensive language detection, hate speech detection, stance detection, toxicity detection, threat detection, bias detection, abusive detection, spam detection, and an open-ended generative task. We have \(60\) test sets of \(68,672\) samples in total. Experiments show that CultureLLM fine-tuned on GPT-3.5 significantly outperforms GPT-3.5 by \(8.1\)% and outperforms Gemini pro  by \(9.5\)% on average F1 score, achieving comparable or even better performance with GPT-4. Our human study of \(50\) people demonstrates that the augmentation

Figure 1: Overview of CultureLLM. CultureLLM consists of three steps: sampling, semantic data augmentation, and fine-tuning. Culture-specific and unified CultureLLM can be fine-tuned.

method can generate semantically equivalent samples. We further interpret the rationale behind its effectiveness by exploring the fine-tuning data size and case studies. Finally, results on Big-Bench Hard (Suzgun et al., 2022) and GSM8K (Cobbe et al., 2021) indicate that CultureLLM is resistant to catastrophic forgetting. CultureLLM also supports fine-tuning LLMs of open-source models.

Our contributions are three-fold:

1. We presented CultureLLM, a cost-effective fine-tuning solution to build culturally-aware LLMs.
2. We proposed semantic data augmentation, an augmentation approach to generate high-quality and diverse training data for LLMs.
3. We conducted extensive experiments in a wide range of cultures and LLMs, showing that LLMs performs consistently well in all downstream tasks.

## 2 Related Work

### Cultural Problem and Solution in LLMs

Previous efforts have shown that LLMs exhibit the same cultural problems as in human society. Niszczota and Janczak (2023) proved that GPT-4 can replicate the cross-cultural differences for each personality factor through large-scale experiments. Meanwhile, other works also found that LLMs can reflect cultural bias and dominance in human society (Liu et al., 2023, Cao et al., 2023, Masoud et al., 2023, Naous et al., 2023, Wang et al., 2023d, Johnson et al., 2022), e.g., Western culture dominance, since the major training corpus such as Pile (Gao et al., 2020) is in English.

The ideal solution is to improve the cultural awareness of LLMs. There are mainly two types of approach: prompt engineering and pre-training. Kovac et al. (2023), Wang et al. (2023d) thought LLMs as superpositions of cultural perspectives, which can be prompted to targeted cultural perspectives. while Rao et al. (2023) encoded cultural values in the prompts. Although PE is cheap, its effectiveness is challenged, especially in low-resource cultures where LLMs lack such cultural knowledge due to lack of representation in pre-training data. Another line of research is pre-training and fine-tuning (Chan et al., 2023, Nguyen et al., 2023b, Pipatanakul et al., 2023, Abbasi et al., 2023, Lin and Chen, 2023) that trains culturally-aware LLMs for different cultures by collecting large-scale pre-training datasets and then performed fine-tuning for better alignment. While they achieved great performance, this approach is too expensive and time-consuming, thus it is difficult to apply to more cultures and countries. They still suffer from a low-resource culture problem where the pre-training data are difficult to collect. MaLA-500 (Lin et al., 2024) trained a new LLM on Llama 2 to cover \(534\) languages, which is resource intensive.

### Data Augmentation for LLMs

Human-annotated data are high-quality but expensive. Due to the strong generation ability of LLMs, many works focused on leveraging data augmentation for LLMs. Yu et al. (2023), Liu et al. (2023) used LLMs to augment the math data and then fine-tuned with those data. Li et al. (2023) synthesized data with two designed modules: self-augmentation and self-curation. Chen et al. (2024) introduced a self-play mechanism, where LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. There are also other uses for synthetic data, such as knowledge distillation (Wang et al., 2023c) and improving text embedding tasks (Wang et al., 2023a). Our data augmentation approach also adopts LLMs for data generation, but we add controllable modules such as template editing, synonym replacement, and semantic filter to ensure the diversity and semantic equivalence of the generated samples. It can also be used as a general augmentation method in other applications.

Efforts in cultural datasets (Nguyen et al., 2023a, Fung et al., 2022) focus on cultural common sense and norms. However, they generate data from only the English or Chinese corpus and thus may contain cultural bias toward other cultures. In contrast, World Values Survey (WVS) (Survey, 2022) is a large-scale pool that contains answers from people of different cultures, thus providing more objective cultural values from specific cultures.

This work is also related to value alignment (Ji et al., 2023, Shen et al., 2023, Yao et al., 2023) to align the values of LLMs with human's by designing algorithms for value measurement and behavior alignment. In contrast, this work primarily emphasizes value understanding with the potential to be extended for value alignment. For instance, semantic augmentation can be used to generate training data for alignment-related tasks.

## 3 CultureLLM

### Overview

Cultural differences are prevalent in various cultures and backgrounds, leading to an impact on outcomes in downstream applications such as hate speech and biased language. To address the gap between low-source cultural data collection and its wide applications, we design CultureLLM by fine-tuning an LLM on data generated by our novel semantic data augmentation approach leveraging the sensitivity of LLMs on prompts . Figure 1 presents an overview of CultureLLM, where the first step is to sample a subset of data from an existing World Value Survey (WVS)  that represents different opinions (answers) towards the same value questions given by native users. The adoption of WVS is inspired by Attitude-Behavior Consistency theory , which emphasizes the strong relationship between attitude and behavior. Therefore, WVS serves as an ideal seed for data augmentation.6 After sampling, the second step is to generate augmented data using our proposed semantic augmentation approach (Section 3.3) and then fine-tune a CultureLLM for each specific culture such as CultureLLM-Ar for Arabic culture and CultureLLM-Tr for Turkish culture.

Generally speaking, we use \(_{d}=\{(x_{j},y_{j}^{d})\}_{j=1}^{n}\) to denote the seed and \(_{d}^{}=g(_{d})=\{(x_{j}^{},y_{j}^{d})\}_{j=1} ^{n}\) as the augmented data with \(g()\) the augmentation algorithm. Note that the question \(x\) here is the _same_ in all cultures in WVS and \(d\) is the cultural index denoting _different_ answers to the same question \(x\). For example, for a question \(x\)="Do you agree with on the whole, men make better political leaders than women do?", the answer \(y=\) if \(d=\); and \(y=\) if \(d=\). Therefore, we only augment the question \(x\) to be \(x^{}\) but retain the same opinion \(y\) as the original \(x\). We also denote vanilla LLM and CultureLLM as \(f\) and \(f^{}\), respectively. Then, denoting \(\) as the loss function, our learning objective is formulated as: \(f^{}=*{arg\,min}_{f}_{(x_{j},y_{j}^{d})\{ _{d},g(_{d})\}}[(f(x_{j}),y_{j}^{d})]\).

### Sampling

The sampling process should follow two principles: 1) cover as many cultural topics as possible and 2) sample questions that can be clearly answered by LLMs. Based on the two principles, we manually select \(n=50\) questions and rewrite them in the Question-Answer (QA) format, covering \(7\) topics, namely social values, security, science and technology, religious values, ethical values and norms, political interest and political participation, and migration. The details can be found in Appendix B.1.

### Semantic Data Augmentation

Samples from WVS are not enough to fine-tune, which can be augmented by our semantic augmentation approach. In a formal sense, semantic augmentation retains the original ground-truth opinions (\(y_{d}\)) from different cultures and only generates semantically equivalent questions (\(x\)). A naive augmentation approach is to directly

Figure 2: Details of semantic data augmentation. First, semantic templates are generated via rephrasing, semantic filtering, and sentence parsing. Then, training samples are generated by context-aware synonyms replacement and semantic filtering.

use strong LLMs such as GPT-4 to generate new samples (Walters and Wilder, 2023), which could introduce mode collapse, as generation quality can only be controlled by prompts. Furthermore, since LLMs could suffer from cultural bias, directly generating cultural data using prompts could lead to unexpected or even erroneous results.

As shown in Figure 2, the augmentation consists of two stages: semantic template generation and intact sentence generation. The first stage generates several semantically equivalent but stylistically different sentences and parses them into semantic templates. The second stage then generates samples by replacing certain words in the semantic templates. Such an augmentation can naturally introduce more diversities: The first stage increases sentence-level diversities and the second improves the word-level diversities.

Semantic Template GenerationThis stage generates semantically equivalent question templates \(=\{t_{i}\}_{i=1}^{k}\) based on \(x_{d}\). The generation process is nontrivial since there are two challenges ahead: 1) the naturalness and diversities and 2) the semantic preservation. We solve the first challenge by using GPT-4 as the generator with certain prompts to ensure naturalness and diversity. Then, we solve the second challenge by introducing a semantic preservation filter \(\) to measure the similarity between the original and generated sentences.7

We first use the prompt "Could you please generate [\(n\)] sentences that (1) have different sentence structures and (2) have the same meaning with the following sentence: \(x_{i}\)" to generate \(n\) sentences using GPT-4. Then, we denote the embedding of the original sentence and the generated sentences as \(z=(x)\) and \(z^{}=(t)\), respectively. Then we compute their similarity score \(c=(z,z^{})\). If \(c\) passes the threshold value \(\), the generated sentence will be reserved:\(=\{t_{i}|((t_{i}),(x_{j}))>\},  x_{j}_{d}\). Specifically, for sample "Do you agree with One of my main goals in life has been to make my parents proud?", we generate \(m\) samples using GPT-4, which are then go through the semantic filter \(\) to eventually retain \(k(k m)\) semantically equivalent sentences, e.g. "Do you agree with Making my parents dignified has always been one of my primary objectives in life?" and "Do you agree with The goal to bring vanity to my parents has been a central life goal of mine?"

To diversify the generated data, we parse the \(n\) sentences to find the appropriate components to replace, which construct the templates. For efficiency, we use NLTK (Loper and Bird, 2002) to find replaceable words, such as adjectives, adverbs, nouns, and verbs. The semantic templates are like "Do you agree with The [x] to bring [x] to my parents has been a [x] life [x] of mine?" where "[x]" is the replaceable part. In total, we generate \(k\) templates for each sample \(x_{j}_{d}\).

Intact Sample GenerationThis step is to replace synonyms in templates to generate fine-tuning samples. We apply GPT-4 to generate context-aware synonyms in the templates and randomly replace some of them. To further preserve semantics, we also use the semantic preservation filter. After filtering, we generate \(m\) samples for each template \(t_{i}\), and get \(n^{}=mnk\) samples for all \(x_{j}_{d}\) in total. For example, intact samples for template "Do you agree with The [x] to bring [x] to my parents has been a [x] life [x] of mine?" could be "Do you agree with The goal to bring pride to my parents has been a central life goal of mine?" and "Do you agree with The hope to bring vanity to my parents has been a central life goal of mine?" Our human study in Section 4.6 shows that augmentation can generate high-quality and semantically equivalent sentences.

### Fine-tuning

Since culture is a complex construct, we use languages spoken by geographical cultures (represented by countries) in WVS to represent broader cultures and arrive at a set of \(9\) cultures in total. In cases where a language is spoken by more than one geographical culture, we pick representative countries and use the average of all answers as groundtruth. Our final set of cultures represented as described above is Arabic (for which we select Jordan and Iraq), Spanish (for which we select Mexico and Argentina), Bengali, Chinese, English, German, Korean, Portuguese, and Turkish.

Finally, CultureLLM is obtained by fine-tuning an LLM on the combination of the seed and the generated data. Specifically, we fine-tune two types of LLMs: 1) culture-specific LLMs for each language such as CultureLLM-Ar and CultureLLM-Bn, and 2) one unified LLM for all languages, denoted as CultureLLM-One. Culture-specific LLMs are tailored by setting \(d\), namely, \(\{_{d},^{}_{d}\}\). On the other hand, CultureLLM-One is trained on all datasets: \(\{_{d},^{}_{d}\}_{d}\) to serve as a unified LLM for all cultures. Note that since all languages have the same input question \(x\) but different answers \(y\), we need to manually write different prompts in the instruction to distinguish them. For example, we add "You are an Arabic chatbot that knows Arabic very well" before Arabic samples. CultureLLM can be used in cultural downstream applications. In the following, we use CultureLLM to denote specific LLM and CultureLLM-One for unified LLM.

**Remark:** Note that the WVS is all in English, where we focus on cultural differences in _opinions_ regardless of their native language. Thus, we do not perform fine-tuning for other languages due to the shortage of their training data and rely on cross-lingual transfer. Multilingual tasks for cultures can still benefit from fine-tuned models in English, as models can learn the basic values from the opinions (Moussaid et al., 2013; Jin et al., 2023). Our experiments in Section 5.1 further demonstrate that fine-tuning on English data can outperform fine-tuning on native data that are translated from the original English version.

## 4 Experiments

We fine-tuned a CultureLLM-One and \(9\) specific CultureLLM for \(9\) languages: Arabic (Ar), Bengali (Bn), Chinese (Zh), English (En, United States), German (De), Korean (Ko), Portuguese (Pt), Spanish (Es), and Turkish (Tr). These cultures are diverse and represent both high- and low-resource regimes and thus can serve as representative evaluation.

### Setup

**Datasets.** We adopt culture-related public datasets in specific languages for evaluation. In total, we have \(59\) test sets, covering \(9\) languages and containing \(68,607\) test samples. We test on \(56\) binary classification and \(3\) multi-classification tasks to detect: offensive language, hate speech, stance, toxicity, threat, bias, abusive, and spam in zero-shot evaluation. For example, we ask LLMs to judge whether the sentence contains offensive language, hate speech, or biased speech. Details are shown in Appendix B.2. Furthermore, we generate an open-ended generation dataset for evaluation in Section 4.3.

**Baselines and details.** We fine-tune CultureLLM using the GPT-3.5 (0613) (OpenAI, 2023a) fine-tuning API due to its efficiency and compared with two state-of-the-art LLMs, namely Gemini pro (Google, 2023) and GPT-4 (1104) (OpenAI, 2023b). We further compare with cultural specific pre-trained models SeaLLM (Nguyen et al., 2023b; TaiwanLLM (Lin and Chen, 2023) and CultureBank (Shi et al., 2024). We also compare this with retrieval augmentation (RAG), which enhances LLMs by searching for related information and adding it to context (Lewis et al., 2020). To implement RAG, we search for information about each culture on Wikipedia and append them in a system prompt, as detailed in Appendix C.3. Finally, we fine-tuned CultureLLM using Llama-2-70b- chat (Touvron et al., 2023) as the base model for reproduction (Section 5.3). As for prompt setup, since our goal is to make LLMs better align with people from different cultures, we add a system prompt "You are an [x] chatbot that knows [x] very well" where [x] is a certain language before each input. For metrics, we use macro F1 score for all tasks except for CValues (Xu et al., 2023) where we use the automatic evaluation script provided by the paper. For data augmentation, we set \(k=5\), \(m=2\), and \(=0.8\). Evaluation prompts are in Appendix C.2.

### Main Results

We present the average results for each culture and task in Figure 3(a)8 and more detailed results are shown in Appendix D.1. Our conclusions are as follows. First, both specific and unified CultureLLM achieve a great improvement over other approaches, and specific CultureLLM achieves the best performance. Concretely speaking, CultureLLM significantly outperforms GPT-3.5 (by \(8.1\)%),Gemini (by \(9.5\)%), and RAG (by \(7.94\)%), achieving performance comparable to GPT-4 and even better on some tasks. Second, CultureLLM-One exceeds GPT-3.5 by more than \(4\%\) on \(59\) tasks, while inferior to culture-specific models, suggesting that a single LLM might not be the best solution to solve cultural tasks with low resources, since data from different cultures could intertwine with each other. Third, in terms of cultures, CultureLLM achieves the best performance in English, Chinese, and Spanish cultures while showing no obvious improvement in Korean culture, where all four models have a similar performance. We infer that the reason could be that these base models have less exposure to Korean culture.

Then, we analyze the performance on both low-resource and high-resource language tasks. As shown in Figure 7, CultureLLM shows excellent performance in both types of tasks and outperforms GPT-4 on a large scale in high-resource tasks. Finally, we evaluated an extremely low-resource culture, Greek, in Appendix D.6 and compared CultureLLM with other cultural-specific models SeaLLM , TaiwanLLM  and CultureBank  in Appendix D.2, which shows that our CultureLLM can also improve performance. The correlation with the WVS data is in Appendix D.3, showing that the performance improvement does not come from the seed data in WVS.

### Results on Open-ended Generation Tasks

To evaluate the performance of CultureLLM on open-ended tasks, we construct a dataset using GPT-4, containing \(65\) open-ended questions, which cover the seven topics in WVS. The prompt setting for dataset generation can be found in Appendix C.2. We evaluated the outputs of GPT-3.5 and CultureLLM using Gemini Pro9. We also devised a metric \(=(s_{ CultureLLM}-s_{ ChatGPT})/65\), where \(s\) represents the number of acceptances by Gemini Pro. Positive WinRate means CultureLLM wins GPT-3.5 and vice versa. As shown in Table 1, CultureLLM performs better than GPT-3.5 on \(8\) out of \(9\) cultures, demonstrating its effectiveness in generation tasks.

### Ablation Study

We evaluate the effectiveness of our semantic data augmentation approach by comparing it with the following variants: GPT-3.5, CultureLLM (WVS), CultureLLM (WVS+a), and CultureLLM (WVS+a+b), where CultureLLM (WVS) denotes the fine-tuned models using only the 50 samples from WVS, CultureLLM (WVS+a) denotes fine-tuning using 50 WVS samples and the generated samples in step 1 of our data augmentation (i.e., only using semantic templates), and CultureLLM (WVS+a+b) denotes the complete process of our algorithm. Note that 'WVS+a' denotes the naive baseline of only using GPT-4 to generate samples.

  Culture & Ar & Bn & Zh & En & De & Ko & Pr & Es & Tr \\  WinRate\(\) &.215 &.369 &.215 &.492 &.462 &.615 &.569 &.215 & -.062 \\  

Table 1: WinRate results on generation tasks.

Figure 3: (a) The main results averaged by cultures (left) and by tasks (right). Both CultureLLM and CultureLLM-One significantly outperform CultureLLM and Gemini with CultureLLM achieving the best performance comparable to GPT-4. (b) Ablation study. ‘+WVS’ denotes the fine-tuned models using only the 50 samples from WVS, ‘+WVS+a’ denotes fine-tuning using the WVS samples and the generated samples in step 1 of our data augmentation (i.e., using only GPT-4 to generate), and ‘+WVS+a+b’ denotes the complete process of our algorithm.

Figure 3(b) shows that fine-tuning using the 50 seeds in WVS can inconsistently improve and impair performance on different tasks such as the decrease in Korean tasks. While WVS data are of high quality, we see gains with our generated data which leads to improvements on most tasks. The average performance on Korean tasks also improves by CultureLLM. Figure 3(b) also demonstrates that the two steps in our semantic data augmentation approach are useful and necessary. Ablation for CultureLLM-One is in Appendix D.4, which also shows the effectiveness of our approach.

### Effectiveness Analysis

We analyze the effectiveness of CultureLLM by controlling the number of generated data, computing the perplexity score, and presenting case studies.

First, we analyze the impact of the generation size. As illustrated in Chen et al. (2024), the diversity and quality of datasets are important in training LLMs. Hence, infinite or too many generated samples might hurt the performance due to possible mode collapse. In this section, we control the number of generated data and empirically analyze its impact. Specifically, we fine-tune \(4\) CultureLLM with \(\{0,100,500,1000\}\) generated samples appended to the original WVS data set. As shown in Figure 4(b), as the number of fine-tuning data increases, performance across most of tasks get improved; but when the number is greater than \(500\), performance on all tasks declines.

Then we analyze the diversity of the generated data by computing two metrics: perplexity (Marion et al., 2023; Wang et al., 2023b) and diversity gain (Bilmes, 2022) (Appendix C.1), as shown in the upper right in Figure 4(b), where we observe the consistency between these two metrics and the fine-tuning performance: the \(500\) generated data lead to the best perplexity and diversity gain. The reason may be that these \(500\) samples are enough for GPT-3.5 to understand the knowledge of seed data, and more samples can cause overfitting and decreased performance. Additionally, although the augmentation approach only generates different samples by varying sentence and word styles, the diversities can also be increased. This suggests that variations in samples can improve the diversity of datasets.

As in the cases shown in Figure 8, responses from GPT-3.5 often analyze input from multiple perspectives and call on to be respectful and kind, rather than providing clear and straightforward opinions. In some cases, GPT-3.5 says that it cannot determine the intentions behind the sentence without context, while CultureLLM provides clear opinions most of the time. The reason behind this may be that we fine-tune CultureLLM to learn opinions from specific culture, so that it can be more aligned with the corresponding culture when faced with cultural differences or cultural conflicts. However, GPT-3.5 is aimed to serve people from different cultures. Thus, it prefers to give a neutral response to not conflict with any

   Evaluator & Human & GPT-4 & Gemini & AVG \\  Rating & 4.60 (0.28) & 4.99 (0.09) & 4.93 (0.26) & 4.84 \\   

Table 2: The semantic similarity of generated samples and seed samples are judged by \(50\) human participants, GPT-4 and Gemini Pro. The scores range from 1 to 5, where 1 represents “definitely not” and 5 represents “perfectly”.

Figure 4: (a) Results on different numbers of fine-tuning samples with perplexity score and diversity gain above. (b) Results of fine-tuneing on English (En ft) and local languages (local ft). It shows that fine-tuning on English outperforms fine-tuning on local languages.

cultures. However, the worst consequence is that it cannot provide useful responses to the problems related to cultural differences.

### The Effectiveness of the Augmented Data: A Human Study

We analyze the effectiveness of the augmented data through human evaluators. We hire \(50\) people who have a high exposure to English (i.e., majoring in English) to check if our generated sentences are semantically equivalent to the seed data. The information of the participants and the training procedure are in Appendix F. We sample \(100\) pairs of (seed, generation) samples and let each participant rank their similarities by giving a score of \(1\) to \(5\), with \(5\) representing the most similar. We also use GPT-4 and Gemini Pro as evaluators. The average results in Table 2 demonstrate that the semantic similarity passes \(96.5\%\), implying that our augmentation approach can increase the quantity while retaining the similarity.

We also conduct experiments on generation tasks. Figure 8 shows the responses of GPT-3.5 and CultureLLM in four different cultures. The results show that CultureLLM can generate more accurate, direct, and useful responses than GPT-3.5. To be specific, GPT-3.5 always generate long responses, which do not give useful information and just call on to be respectful, while CultureLLM give accurate and direct responses. This is very important for user experience.

## 5 Discussion

### Augmenting Multilingual Data vs. English Data

CultureLLM are fine-tuned on English data, since the training corpus of LLMs such as the GPT series are mostly in English and English may be the choice for LLMs to understand the opinions of other cultures. What about the performance of LLMs fine-tuned in a culturally specific language? We also fine-tuned GPT-3.5  on multilingual data that are translated from English data and compare with CultureLLM. The results are shown in Figure 4, indicating that the models fine-tuned in English perform better than the models fine-tuned in other languages. The reason behind this may be the model's inherent capabilities in English have been shown to be superior  than other languages, which again emphasizes the importance of collecting large-scale data for pre-training. This study demonstrates that, in low-resource settings without collecting large-scale training data, the augmentation approach could be useful for fine-tuning.

### Fine-tuning vs. Forgetting

A potential dilemma is that fine-tuning an LLM on specific tasks might face catastrophic forgetting of its original capabilities. In this section, we explore the forgetting of CultureLLM in two general datasets: BIG-Bench-Hard (BBH)  and GSM8K . BBH contains \(21\) tasks covering both semantic understanding and logical reasoning tasks. GSK8K is a widely used data set to evaluate mathematical ability. For BBH, we sample \(100\) samples for each task to test, due to cost savings. We compare each CultureLLM with the GPT-3.5 baseline model in Figure 5. The results show that CultureLLM does not decrease performance in most benchmarks and can even improve their results, such as on BBH. This suggests that there might be some latent relations between the cultural data and the general benchmarks, thus fine-tuning on cultural data can benefit general reasoning abilities.

### CultureLLM on Open-sourced LLMs: Llama2

Although all main experiments in this work are performed using the OpenAI GPT-3.5 fine-tuning API  due to its efficiency and simplicity, our CultureLLM also supports fine-tuning on open-source LLMs for better quality control and reproducibility. In this section, we show an initial experiment using Llama2-70b-chat as the base model to fine-tune a CultureLLM-Llama2-70b. The results in Figure 5 show that CultureLLM-Llama-70b outperforms the base Llama model by \(2.17\%\) on average, showing the effectiveness of fine-tuning CultureLLM on open-source models. The details of fine-tuning and more Llama2 results are in Appendix E. The results indicate that CultureLLM is a general approach to improve the ability of LLMs to understand the culture.

### Implication and Societal Impact

In essence, recognizing and valuing cultural differences is paramount for the enrichment of our global community. Embracing diversity stimulates innovation and creativity, contributing to the development of novel ideas and solutions. Our work contributes to solving the cultural difference problem in LLMs and tackling the problem of data scarcity in low-resource cultures. The limited availability of data from these cultures hinders understanding and addressing specific needs and concerns. For example, the lack of representation in datasets may perpetuate biases and disparities, hindering the development of inclusive technologies and services. Our approach represents an effective and resource-saving method to bridge the data gap in low-resource cultures, empowering these communities and enabling more accurate, inclusive, and impactful decision-making processes.

## 6 Conclusion and Limitation

Cultural difference is essential to the prosperity of the world. In this paper, we proposed CultureLLM, a cost-effective solution to fine-tune culture-aware LLMs. We sampled a small number (50) of samples from the World Value Survey and then generated augmented data through our novel semantic data augmentation. On \(59\) datasets on \(9\) cultures, CultureLLM outperformed GPT-3.5 and Gemini with comparable or even better results than GPT-4.

This work has the following limitations. First, due to resource and time constraints, we did not implement CultureLLM on large-scale open-source models. Second, we only adopted classification tasks for evaluation since multilingual generative tasks are expensive for automatic evaluation. Finally, the sample diversity is only in sentence and word levels. In the future, we plan to add more diversities to enrich the generated data.

## Disclaimer

This paper leveraged GPT-4 to generate sentences and synonyms, whose quality were manually checked to ensure responsible usage. Throughout this paper, the authors remain neutral towards the opinions from all different cultures and respect their diversities. The human study was conducted following local laws and regulations.