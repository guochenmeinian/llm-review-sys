# Super Consistency of Neural Network Landscapes

and Learning Rate Transfer

 Lorenzo Noci\({}^{*}\)  Alexandru Meterez\({}^{*}\)\({}^{3}\)\({}^{4}\)  Thomas Hofmann \({}^{1}\) Antonio Orvieto \({}^{2}\)\({}^{3}\)

\({}^{*}\): Equal contribution. Correspondence to: lorenzo.noci@inf.ethz.ch, ameterez@fas.harvard.edu\({}^{1}\)ETH Zurich, \({}^{2}\)ELLIS Tubingen, \({}^{3}\)MPI for Intelligent Systems, \({}^{4}\)Tubingen AI Center, \({}^{5}\)Harvard University

###### Abstract

Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit (\(\)P and its depth extension), then some hyperparameters -- such as the learning rate -- exhibit transfer from small to very large models. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is consistently similar across very different model sizes. In this work, we study the landscape through the lens of the loss Hessian, with a focus on its largest eigenvalue (i.e. the sharpness), and find that certain spectral properties under \(\)P are largely independent of the size of the network, and remain consistent as training progresses. We name this property _Super Consistency_ of the landscape. On the other hand, we show that in the Neural Tangent Kernel (NTK) and other scaling regimes, the sharpness exhibits very different dynamics at different scales. But what causes these differences in the sharpness dynamics? Through a connection between the Hessian's and the NTK's spectrum, we argue that the cause lies in the presence (for \(\)P) or progressive absence (for the NTK scaling) of feature learning. We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText.

## 1 Introduction

Recent trends in deep learning research have unmistakably shifted towards an increase in model sizes, with networks comprising of billions of parameters emerging as the standard . However, as models enlarge, so does the cost incurred in hyperparameter tuning which has led researchers to look for ways to scale up the architecture -- both in terms of width and depth -- while preserving the optimal hyperparameters (such as the learning rate).

While there exist several ways (a.k.a _parametrizations_) to scale up the width and depth of the network, not all of them facilitate learning rate transfer. For standard deep learning practices, such as networks parametrized with LeCun/Kaiming initializations [2; 3], a significant shift in the optimal learning rate is usually observed as the width and the depth of the model are increased. Similarly, under the Neural Tangent Kernel (NTK) parametrization , which provides theoretical insights into the behavior of very wide neural networks during training, the optimal learning rate also varies as the width and depth of the network change. Alternatively, Yang and Hu  and Yang et al.  propose the \(\)P framework, designed to maximize the gradient update of the representations of the intermediate layers (i.e. feature learning) as the width increases. Under \(\)P scaling, and its depth extension for residual networks Depth-\(\)P [7; 8], it has been empirically demonstrated that the learningrate transfers across both width and depth. In Vyas et al.  it is observed that in feature learning parametrizations (e.g. \(\)P) the model's dynamics are _consistent_ across model sizes, but for harder tasks or longer training times there are progressive and significant deviations across different model sizes. We give an example in Figure 1 (top center), where the training losses exhibits an increasing gap. The fact that the learning rate is exactly preserved, however, suggests that some properties of the landscape do not exhibit these finite-size deviations, and must be precisely preserved across different model sizes for the whole training trajectory.

Motivated by this, in the present work we identify the notion of _Super Consistency_ to describe the properties of the neural network's loss landscape that are preserved across training as a function of the model width and depth, thus not accumulating finite-size effects. In particular, we analyze the landscape through the lens of the loss Hessian. It provides insights into the landscape's local curvature, and its structure for neural networks has been studied in several works [10; 11; 12; 13; 14]. Of great interest in optimization theory is the _sharpness_, i.e. the top Hessian eigenvalue, which for neural networks exhibit a rapid increase (_progressive sharpening_) towards a threshold called Edge of Stability (_EoS_) [15; 16]. However, although a few works have provided early insights [10; 16; 9], the scaling properties of the sharpness and Hessian's dynamics under different scaling limits remain unexplored. In this work we first present evidence of Super Consistency in the Hessian's largest eigenvalues, which have been shown to control the curvature along the optimization subspace . We then focus on the sharpness dynamics, and find that the presence (resp. absence) of Super Consistency correlates well with presence (resp. absence) of learning rate transfer under \(\)P, NTP and other scaling limits. These results suggest that learning rate transfer happens in super consistent landscapes, as the geometry of the landscape does not significantly change with the network's size.

Then, we investigate the role of feature learning in the progressive sharpening phase, and argue that while in \(\)P feature learning causes progressive sharpening to reach a width-independent sharpness,

Figure 1: **Top row**. Under \(\)P, (left) the sharpness dynamics are largely identical for the whole training dynamics across different widths, phenomenon that we call _Super Consistency_. The dashed horizontal lines are the Edge of Stability thresholds. Center: The loss dynamics are similar early in training, but accumulate finite-size effects over time, thus violating Super Consistency. Right: The learning rate transfers from small to large model, suggesting that the loss landscape is Super Consistent across different model sizes. **Bottom row**. Under NTK parameterization (NTP), the sharpness dynamics show large discrepancies. Also, the learning rate does not transfer. The architecture is a two-layer convolutional network trained on CIFAR-10 with data augmentation, where the width corresponds to the number of filters in the convolution. (See App. J). Other parameters: \(B=128\), epochs \(=50\).

in the NTK regime the progressive lack of feature learning when the width is increased prevents the Hessian from adapting, and its largest eigenvalue from reaching the convergence threshold.

More concretely:

* We define Super Consistency, and show that under \(\)P and Depth-\(\)P it holds for the largest eigenvalues of the loss Hessian (Fig. 2), which converge to a largely width-independent threshold and remains there for the rest of training. On the other hand, we show that other quantities, such as the training loss and the NTK eigenvalues accumulate significant finite-size effects. We quantify the rate of divergence of these quantities with power law fits (Fig. 3).
* We analyze the relationship between Super Consistency of the sharpness and learning rate transfer across \(\)P, Depth-\(\)P, NTP and other parametrizations (Fig. 1, Fig. 4 and Sec. B). For \(\)P and Depth-\(\)P, which do transfer, the sharpness stays super consistent, stabilizing to a threshold (Fig. 1, top left), which in some cases corresponds Edge of Stability , and oscillates around it for a sustained period of training time. On the other hand, under NTP, _Standard Parametrization_ (SP), or Depth-\(\)P with multiple layers per residual block, the sharpness dynamics significantly separate during training for different widths, albeit in different ways. Also, here we do not observe transfer.
* We reproduce some of these results at realistic scale, including ResNets and Vision Transformers (ViTs) trained on Imagenet and GPT-2 on text data. Also, we analyze the effect of batch size, learning rate warm-up, and long training times.
* In Sec. 4.1 we show that the progressive sharpening phase is mainly driven by the NTK's largest eigenvalue, which is asymptotically fixed to its initial value for NTP, while it evolves at any width under \(\)P. In Sec. 5 we provide intuition with a theoretical analysis on a two-layer linear network.

Finally, in Sec. 6 we discuss to what extent Super Consistency of these properties explains learning rate transfer, and the relevance of our results in the existing literature on optimization and scaling limits. Due to page limitations, we defer the discussion on related work to the appendix (App. A).

## 2 Background and Definitions

We consider a neural network with residual connections, defined by the following recursive equations over the layer indexes \([L]\):

\[h^{+1}(x)= h^{}(x)+L^{}}W^{}(h^{ }(x)),\] (1)

where \(N\) and \(L\) are the width and depth of the network, \(W^{}^{N N}\) for \(=1,,L-1\), and \(\) is a factor that enables (\(=1\)) or disables (\(=0\)) the skip branch. We denote the output with \(f(x)=W^{L}(h^{L}(x))\), where \(W^{L}^{1 N}\) and \(\) scales the network output. Similarly, \(\) has the role of interpolating between different depth limit regimes. At the first layer, we define \(h^{1}(x)=}W^{0}x\), where \(W^{0}^{N D}\). All the weights \(=\{W^{}\}_{l=0}^{L}\) are initialized independently from \((0,1)\) and we denote with \(P\) the total number of parameters. We stress that the fully connected layer can be replaced with any type of layer (our experiments include convolutional and attention layers). Given a dataset \(=\{(x_{},y_{})\}_{=1}^{||}\) of datapoints \(x_{}^{D}\) and labels \(y_{}\), we train the network with stochastic gradient descent (SGD) with batch size \(B\) and learning rate \(\),

\[_{t+1}=_{t}-_{=1}^{B}_{}(f_{t}( x_{})),\] (2)

where the loss \(\) is a twice differentiable loss function. Defining \(f_{t}:=(f_{t}(x_{}))_{[||]}^{||}\) to be the vector of network's outputs at time \(t\), if one considers continuous time, the corresponding gradient descent dynamics in function space \(df_{t}/dt\) take the following form : \(}{dt}=-(f_{t})(f_{t})\), where \((f_{t})_{i}:=(f_{t}(x_{i}))/ f_{t}(x_{i})\), \(i[||]\) is the vector of residuals, and \((f_{t})_{ij}:=_{}f_{t}(x_{i}),_{}f_{t}(x_ {j})\) for \(i,j[||]\) is the Neural Tangent Kernel (NTK).

Infinite Width.The parameters \(,,\) determine the nature of the scaling limit. If \(=_{0},=_{0}\) are \((1)\) constants with respect to \(N,L\) (neural tangent parameterization, or NTP), then the network enters the NTK regime . Here, in the limit of infinite width, the NTK remains constant to its value at initialization throughout training, i.e. \((f_{t})=(f_{0})\) for all \(t 0\). Thus,the network's dynamics become equivalent to a linear model trained on the first order term of the Taylor expansion of the model at initialization . The fact that the NTK is fixed to its value at initialization is associated with the lack of feature learning of the model in the large width limit. If \(=_{0}\), and \(=_{0}^{2}\) (\(\)P, or mean-field parameterization), the features evolve in the limit (i.e. the NTK \((f_{t})\) evolves), and the richer model's dynamics can be described using either Tensor Programs  or dynamical mean field theory . Under \(\)P, Yang et al.  show that the learning rate \(_{0}\) as well as other hyperparameters transfer across width, in contrast to kernel limits, which we reproduce for our residual network in Fig. 1.

Infinite Depth.If on top of the \(\)P framework, the residual branches are scaled with \(=1/2\) (Depth-\(\)P), then Bordelon et al.  and Yang et al.  show that the infinite width dynamics also admit a feature-learning infinite depth limit. Under Depth-\(\)P, the learning rate \(_{0}\) transfers with both width and depth. In this paper, we compare NTP and \(\)P regimes as the width is increased, and show that our results extend to depth-scaling using the Depth-\(\)P model. We summarize the feature learning parametrizations and report Depth-\(\)P for Adam in Appendix K.

## 3 Super Consistency of the Optimization Landscape

In this work, we analyze the landscape through the lens of the preconditioned Hessian \(^{2}H_{t}\), where \(H_{t}:=_{}^{2}(_{t}):=_{}_{}^ {2}(f_{t}(x_{}))^{P P}\), as \(_{t}\) evolves with gradient descent. The Hessian is a key object in optimization theory , information geometry [22; 23], and deep learning theory [17; 16; 13; 24] and its relation to optimal step sizes is often used to design second-order optimizers [25; 26; 23; 27]. In Figure 1, we can observe that learning rate transfer correlates with strong alignment across model sizes of the Hessian top eigenvalue dynamics, a property which we name _Super Consistency_. The choice of the preconditioning factor \(^{2}\) ensures the right scaling with respect to the width \(N\), as the theory will justify. We also provide an intuition and an extension to Adam in Appendix. J.1. Unless stated otherwise, every experiment is conducted with the this preconditioning factor \(^{2}\) set according to the corresponding parametrization.

More concretely, Super Consistency refers to when certain aspects of the loss landscape and of the predictor \(S_{N}(t)\) (in this paper \(S_{N}(t)\) refers to the NTK's and loss Hessian's eigenvalues or the loss itself) exhibit the following two properties:

* At realistically large \(N\), \(S_{N}(t)\) does not deviate significantly from its limit \(S_{}(t):=_{N}S_{N}(t)\). This is what is referred to as consistency in Vyas et al. .
* \(S_{N}(t)\) does not accumulate significant finite-size effects over time, i.e. the curves of \(S_{N}(t)\) and \(S_{}(t)\) remain close over a sustained period of training.

With respect to the experiment illustrated in Fig. 1, notice that the curves of the loss (center) at different widths show progressive and significant deviations, thus violating Super Consistency. On

Figure 2: (a) The top Hessian eigenvalues exhibit a progressive increase to a threshold, with larger eigenvalues showing precise Super Consistency, while lower eigenvalues show finite-size accumulation at small width in the initial phase of training. (b) Top eigenvalues of the NTK matrix \(\). As opposed to the top eigenvalues of the Hessian, these exhibit evident finite-size accumulation during training. Model: 3-layer ConvNet, \(=0\), \(_{0}=0.7\) (optimal). Details in Sec. J.

the other hand, the sharpness dynamics for \(\)P qualitatively exhibit little-to-no deviations. Also, notice that we assume existence of the limit \(_{N}S_{N}(t)\). For those parametrizations (e.g. _standard parametrization_) that do not have a well-defined limit, \(S_{N}(t)\) diverges at large \(N\) and Super Consistency is trivially violated.

We now turn to the analysis of the Hessian spectrum and observe the following:

_Observation: in \(\)P (and Depth-\(\)P), Hessian eigenvalues are super consistent along the optimisation trajectory. Smaller eigenvalues have progressively different dynamics._

In Fig. 2 (a), we train a residual network on CIFAR-10 (a 10 classes image classification task) using cross-entropy loss, and show super consistent dynamics of three of the top \(k=10\) eigenvalues. The choice of \(k=10\) is guided by Gur-Ari et al. , where it is observed that stochastic gradient descent happens in a small subspace where the gradient lies in the space spanned by top \(k\) Hessian eigenvectors (where \(k\) is the number of classes). Thus, our results show that the curvature along the training trajectory is preserved super consistently at different scales, thus suggesting that the geometry of the landscape across the trajectory is preserved across model size. Lower order eigenvalues tend to accumulate finite-size effects in the first phase of training, and stabilize at lower thresholds for smaller width models. We discuss the effect of lower order eigenvalues through the Hessian trace in Appendix G. Finally, to make sure that Super Consistency holds along the training trajectory regardless of the tiny-subspace assumption, in Appendix I we track the _directional sharpness_, that measures the curvature along the gradient direction.

To give a quantitative measure to the finite-size accumulation property, we measure deviations over time by estimating the following quantity:

\[g(t):=|S_{N}(t)-S_{}(t)|.\] (3)

When \(g(t)\) increases over time (up to fluctuations), Super Consistency is violated. We illustrate this in Fig. 3 (b, c), where we compute the left hand side of Eq. 3 for the loss \((f_{t})\) and the NTK's largest eigenvalue \(_{max}()\). To estimate the infinite width limit, we use a very large-width model as a proxy. Notice how under \(\)P the the loss dynamics progressively diverge from the infinite width model, indicating a finite-size accumulation over time. The same holds for \(_{max}()\). To study the rate of divergence \(g(t)\), we fit a power law of the form \(y=at^{}\) to the observations. A larger \(\) indicates a higher divergence rate. Notice how \(>0.6\) for the loss, and \( 2\) for \(_{max}()\), indicating quadratic divergence. In comparison, in Fig. 3 (left), we show Super Consistency of the sharpness, in that finite-size effects are not accumulated over time (10 epochs). Finally, both in Fig. 2 and 3 notice how the sharpness is not just _converging_ in width, but in fact _width-independent_. This might be due to the fact that the threshold is a stable attractor of the dynamics .

## 4 Super Consistency and Learning Rate Transfer

**Sharpness and Edge of Stability.** We now focus on the _sharpness_\(:=_{}(^{2}H)\), defined as the largest eigenvalue of the Hessian. In the theory of smooth convex , nonconvex , and stochastic  optimization, the sharpness plays a crucial role in in the guarantees of convergence

Figure 3: (a) Convergence rate of the sharpness at finite width \(N\) to the infinite limit proxy. Note that the distance approaches \(0\) as the training time increases. (b) Convergence rate of the loss at finite width \(N\) to the infinite limit proxy. Note that the loss accumulates finite-size effects over time and the distance to the proxy increases. (c) Convergence rate of the top NTK eigenvalues over time to the infinite limit proxy. Similar to the loss, this also accumulates finite-size effects over time. Details: infinite limit proxy is width \(4096\), model is ConvNet, \(=0\), \(_{0}=0.7\).

of gradient methods and selection of the optimal step size. For instance, for a quadratic objective, \(_{t}=\) is constant and gradient descent would diverge if the learning rate satisfies \(_{0}>2/\), and training speed is maximized for \(_{0}=1/\) (LeCun et al. , page 28). Beyond this classical example, which assumes constant Hessian, the descent lemma  states that \((_{t+1})(_{t})\) if \(\) where \(:=_{}\|^{2}L()\|_{2}\), and \(\|^{2}L()\|_{2}\) is the sharpness at \(\). When it comes to deep neural networks, \(_{t}\) is generally observed to increase during training (_progressive sharpening_): in the early phase of training it increases  and then it decreases close to convergence . Under full batch gradient descent training, the sharpness consistently rises above the _EoS_ threshold of \(2/_{0}\).

**Conditions for hyperparameter transfer.** The empirical success of hyperparameter transfer crucially relies on the following two observations, constituing a "theoretical puzzle" .

1. The optimal learning rate is preserved across widths/depths, indicating very fast convergence with respect to the scaling quantity.
2. The models show consistent improvement in training speed with respect to the scaling quantity (i.e. there is a clear "wider/deeper is better" effect), indicating that the loss dynamics have not yet converged to the limiting behaviour predicted by the theory.

In this section, we study the role of Super Consistency in learning rate transfer. We focus on the dynamics of sharpness \(_{}\) across training, due to its well-established connection to optimization theory and step size selection, as well as better computational tractability than the full Hessian spectrum. We provide extensive studies of other relevant spectral quantities (i.e. Hessian and NTK eigenvalues) in Appendix G.

_Observation: in \(\)P (and Depth-\(\)P), the sharpness \(\) is super consistent along the training trajectory, while for NTP the sharpness decreases in width. This correlates with presence/absence of hyperparameter transfer._

In Fig. 1 we train a two-layer convolutional network under the \(\)P and NTP scalings with cross entropy loss, while keeping track of the sharpness at fixed gradient step intervals. The top row shows the dynamics of \(\). Notice how the sharpness' behaviour is qualitatively different in the two parameterizations: in \(\)P it reaches a width-independent value which is close to the EoS threshold of \(2/_{0}\). On the other hand, in NTP we observe a progressive diminishing of the sharpness with width, as previously observed for Mean-Square-Error loss by Cohen et al. .

We then study the effect of depth under the Depth-\(\)P model of Eq. 1. In Fig. 4 (left), we show that the sharpness' dynamics are also super consistent across depth, although progressively diminishing from the EoS threshold. This suggests that EoS is not necessary for the learning rate to transfer, but the consistency of the sharpness dynamics is.

**Other feature learning parameterizations.** Finally, we study the effect of other feature parameterizations that _do not_ exhibit learning rate transfer. In particular, we study the Depth-\(\)P scaling of the residual branches in residual networks with multiple layers per residual branch - denoted by \(k\) (i.e. each branch has multiple weight matrices and non linearities). A typical example is the Transformer architecture, which has multiple layers per block in both the attention and fully connected blocks. This parameterization, although it learns features in the infinite depth limit, it is _lazy within each residual branch_. The results are in Fig. 4. Notice how the sharpness dynamics are not super consistent, in that they accumulate finite-size effects over time. We study other parametrizations, including those without a stable limit in Appendix B, showing compatible results with those presented here. The observation that sharpness dynamics exhibit greater consistency compared to loss dynamics suggests that under \(\)P scaling, although models with larger capacity fit the data faster, the paths taken by various models through the optimization landscape show a surprisingly uniform curvature.

### Feature Learning and Progressive Sharpening

We now study the effect of feature learning in the sharpness dynamics. Following the Gauss-Newton decomposition , the Hessian can be decomposed as a sum of two matrices \(H=+\), where \(\) is the Gauss-Newton (GN) matrix and \(\) depends on the Hessian of the model. For MSE loss,

\[=_{i=1}^{||}_{}f(x_{i})_{}f(x_{ i})^{}=K^{}K=_{i=1}^{||}_{}^{2}f(x_{i})(y_{i}-f( x_{i})),\]

where \(K^{|| P}\) is a matrix where each row is \(_{}f(x_{i})\) (i.e. the Jacobian of \(f(x_{i})\)), and \(y_{i}\) is the label. One can readily see that the NTK matrix can be written as \((f_{})=KK^{}\), thus the NTK and \(\) share the same nonzero eigenvalues. In Figure 5, we show that under \( P\) the sharpness evolution is dominated by the \(\) matrix consistently across different widths, while for NTP the sharpness evolution slows down when increasing the width. Since in the limit the NTK matrix is fixed for NTP, while it evolves with time for \(\)P, these results provide further supporting evidence for the role of feature learning in the evolution of the hessian. While this argument strictly holds for MSE loss, it can be generalized to any twice differentiable loss function, albeit with some caveats. In Appendix D, we generalize the setting, analyze the cross-entropy loss and perform validating experiments, confirming the conclusions drawn here. Finally, in Appendix H, we show that our results remains valid in a random feature model, where the NTK matrix is fixed at initialization at any finite width. In Section 5 we revisit the above claims more precisely in a simplified setting, providing further intuition on the sharpness dynamics and learning rate transfer.

**Large scale experiments.** In App. F, we perform more experiments to validate the connection between the consistency of the sharpness' dynamics and learning rate transfers across datasets (Tiny

Figure 4: Depth-\(\)P extensions with top row showing transfer plots and bottom row the sharpness evolution. (a) ConvNets with \(1\) layer per block exhibit both hyperparameter transfer and sharpness Super Consistency. (b) ConvNets with \(2\) layers per block. The model has a lazy behavior within each block, and no transfer. The sharpness starts accumulating finite-size effects during training, violating Super Consistency. (c) ViTs also have \(k>2\) blocks per layer by design, and thus have a similar behaviour. Details: (a), (b) are trained with SGD, with widths \(128\) and \(32\) respectively; (c) is trained with Adam, with the learning rate scaled by \(1/\). See Fig. 22 for convergence rates.

Figure 5: Evolution of the top eigenvalues of the Hessian components \(\) and \(\) for a two-layer linear network trained on random data under MSE loss. The vector field characterizes the evolution during training for a fixed learning rate. Top: \(\)P. Note how \(\) drives the initial change super consistently. Bottom: NTP. For wider networks the sharpening phase reduces, since the network is approaching the limit where the NTK is fixed to its value at initialization.

ImageNet, Wikitext), architectures (ViT, GPT-2 ), and optimizers (Adam  and AdamW ). We find these results to be consistent with those in the main text.

**End of training dynamics.** In App. E.1 (Fig. 12), we study the width dependence of the sharpness at the late phase of training. It is well-known that for cross-entropy loss, a phase transition happens where the sharpness starts to decrease . We found that even for \(\)P this transition point is width-dependent, with a consequent slight shift in optimal learning rates during this late phase. Again, these results are in line with our results that super consistent sharpness facilitates transfer.

**Batch size ablation.** We repeat the experiment in Fig. 1 with increasing batch size, observing that the threshold is reached across all the tested batch sizes, thus not affecting learning rate transfers. For larger batches, a close-to-EoS threshold is reached across more learning rates. Results are summarized in Fig. 13 and 14 in App. E.

## 5 Case study: Two-Layer Linear Network

We now revisit and validate our intuition and empirical findings in Sec. 4 through the lens of a two-layer neural network with linear activations and \(L2\) loss. Our purpose is to characterize the dynamics of \( P\) and NTP at the edge of stability through the lens of a simple example that shares a similar phenomenology with the more complex scenarios observed in the last section (see Fig. 10, App. C). In particular, the theory justifies the preconditioned Hessian \(^{2}H NH\) as the right object of study when it comes to the sharpness computations (see Prop. 5.3). Also, it provides an intuition to the width-independent evolution of the sharpness. Our setting is similar to the one leading to the insightful analysis of _EoS_ in [28; 36]. Compared to these works, we do not limit the analysis to a single datapoint or to vanishing targets 1.

**Notation and assumptions.** Consider a dataset of \(||\) datapoints in \(D\) dimensions \(X^{|| D}\) (\(|| D\)), and labels generated through a latent ground-truth vector \(w_{*}^{D}\), that is \(Y=Xw_{*}\). The neural network we use here is parametrized by weights \(W^{0}^{D N}\), \(W^{1}^{N 1}\), where \(N\) is the width. To simplify the notation in our setting, we name \(E:=W^{0}\) and \(V:=W^{1}\): \(f(X)=}XEV\), \((E,V)=\|f(X)-Y\|^{2}\). We initialize each entry of \(E,V\) i.i.d. Gaussian with mean zero and variance \(1\). Recall that \(_{}=1\), \(_{ P}=\). We train with gradient descent (GD) with a learning rate \(=_{0}^{2}\). Empirically, we observed (Fig. 10, App. C) that picking \(||=D\) and data \(X=I_{D}\) (\(I_{D}\) is the \(D D\) identity matrix) is sufficient to track most of the crucial features of \( P\)/ NTP explored in this paper, except the "wider is better" effect which here is less apparent due to the simple hypothesis class. The loss function reduces to:

\[(E,V)=\|w-w_{*}\|^{2}, w :=}EV.\] (4)

Finally, we reparametrize the model by defining:

\[e:=EE^{}^{D D},\;\;v:=V^{} V_{ 0}.\] (5)

Note that using a learning rate \(_{0}^{2}\) when optimizing \(\) is equivalent to using a learning rate \(_{0}\) when optimizing \(^{2}\). Next, we characterize how \(e,v\) evolve through time, and give conclusions for \(\)P.

### Dynamics and Edge of Stability in Latent Space

We now show that at any value of the width \(N\), under GD on the original network parameters \((E,V)\), the dynamics of \(w,e,v\), can be described completely through a self-contained dynamical system in \((1+D+D^{2})\) dimensions. This property is surprising because the original dynamical system described by GD on the variables \(E,V\) lives in \(N(D+1)\) dimensions. Concretely, this means we can study the Hessian dynamics at different network widths in the same space.

**Theorem 5.1** (Evolution Laws).: _Let \((E,V)\) evolve with GD at stepsize \(=_{0}^{2}\) on the loss of Eq. 4. The evolution of \((w,e,v)\) is completely described by the following self-contained equation:__let the \({}^{+}\) denote updated quantities,_

\[w^{+} =w-_{0}(v I_{D}+e)(w-w_{*})+^{2}^{2}}{ ND}(ww^{}-w_{*}w^{})(w-w_{*}).\] \[e^{+} =e+^{2}}{ND}[-2ww^{}+w_{*}w^{}+ww _{*}^{}]+^{2}^{2}}{ND}[vww^{}-vw_{*}w^{ }-vww_{*}^{}+vw_{*}w_{*}^{}].\] \[v^{+} =v+^{2}}{ND}[-2w^{}w+2w_{*}^{}w ]+^{2}^{2}}{ND}[w^{}ew-2w_{*}^{}ew+w_{* }^{}ew_{*}].\]

While the system above describes the evolution laws \((w_{k},e_{k},v_{k})(w_{k+1},e_{k+1},v_{k+1})\), the dynamics are influenced also by initialization. In Prop. C.1 in the Appendix, we show that the only dependence in width in the evolutions laws are in the initial conditions.

Last, by analyzing the stability of the dynamical system in Theorem 5.1, we can characterize the edge of stability using tools from dynamical systems . First of all, we need the following Lemma, which implies that at the minimizer (\(w=w_{*}\)), the Hessian has the same non-zero eigenvalues as the NTK \(\), which only depends on \(e\) and \(v\).

**Lemma 5.2** (GN bound).: _Let \(^{2}^{2}=+\) be Gauss-Newton decomposition2 (see Sec. 4.1) of the Hessian for the loss in Eq. 4, with \(=K^{}K\), where \(K^{D(ND+N)}\). Let us denote the NTK matrix \(=KK^{}^{D D}\). Then_

\[(E,V)=e+v I_{D}\]

_and_

\[|_{}[^{2}^{2}(E,V)]-_{}[(E,V)]|}{ND}}\|w-w_{*}\|_{2}.\]

We stress that this result implies that evolution of the NTK (i.e. _feature learning_) goes hand in hand with the evolution of the sharpness, as we empirically show in Sec. 4.1. We are now ready to state the result on the sharpness at convergence.

**Proposition 5.3** (EoS).: _Let \((E,V)\) evolve with GD with stepsize \(=_{0}^{2}\) on the loss of Eq. 4 towards a minimizer (\(E_{*},V_{*}\)). Assume the corresponding solution in latent space \((w_{*},e_{*},v_{*})\) is marginally stable 3. Then, \(_{}[^{2}^{2}(E_{*},V_{*})][ {_{0}},}+^{2}\|w_{*}\|^{2}}{ND} ].\)_

Implications for NTP.Consider \(=1\) in Thm. 5.1. The dynamics of \((w,e,v)\) are _width-dependent_. Let us take \(N\) in the equation above to amplify this effect: the system becomes

\[w^{+}=w-_{0}(v I_{D}+e)(w-w_{*}),\ \ e^{+}=e,\ \ v^{+}=v.\]

While \(w\) evolves from \(w_{0}\) as expected from standard NTK theory , \(e,v\) stay clamped at initialization. Applying Lemma 5.2 with \(=(1)\), the Hessian and the NTK have the same largest eigenvalue at large width (at rate \(O()\)). This agrees with our intuition, as under NTP the predictor converges to a linear model in the large \(N\) limit, and thus \(\) vanishes. Also, this proves that the sharpness has no dependency on the learning rate in the width limit (we observe this, e.g., in Fig. 1 and throughout all our experiments). This derivation is also in line with our discussion in Sec. 4.1: we only have sharpening under feature learning, and for the same reason we cannot observe NTP at the edge of stability as \(N\) (see stepsize dependency in Prop. 5.3), as also noted empirically by .

Implications for \(\)P.The following result immediately follows by inspection of the equations in Thm 5.1, combined with Prop. C.1.

**Corollary 5.4**.: _Consider \( P\) (\(=\)) and let \((E,V)\) evolve with GD with stepsize \(=_{0}^{2}\) on the loss of Eq. 4. Then, the equations governing the evolution of \((w,e,v)\) (defined in Thm. 5.1) in latent space have **no width dependency** - this holds at any finite width and not just at the limit. Initialization of \((w,e,v)\) is instead width-dependent, yet the error from \(N\) case scales in expectation like \(1/\)._The corollary shows that \(\)P trajectories at different widths align in the latent space \((w,e,v)\), albeit with a vanishing perturbation in the initial condition (see Prop. C.1). While NTP's dynamics for \(e\) and \(v\) become slower as the width increases, for \(\)P their evolution laws are width independent. This implies that if the dynamics converge towards a minimizer for \(e\) and \(v\), this will be at the sharpness value predicted by Prop. 5.3. Under \(\)P, where \(^{2} N\), this value will be _width-independent_, as Super Consistency would suggest. We stress that Prop. 5.3 characterizes the sharpness at convergence (i.e. at infinite time). At finite time, there is still a discrepancy between the \(_{}()\) and the sharpness of the order of the residual term \(1/\|w-w^{*}\|\) (Lemma 5.2). Finally, we stress that Prop. 5.3 prescribes the right scaling for the Hessian by including the preconditioning factor of \(^{2}\). Thus, we do not prove that at any finite time, the whole sharpness trajectory is width-independent, nor we are estimating converge rates in \(N\) at finite time. Indeed, there will be a finite-size dependence coming from the initial conditions. We leave a precise characterization of the whole sharpness dynamics across the training trajectory for future work.

## 6 Discussion & Conclusions

**On Feature Learning Parametrizations.** In this paper, we have shown how certain properties of the loss Hessian evolve almost identically across training for different model sizes, and named this property _Super Consistency_. We have also compared the sharpness dynamics under different scaling limits and parameterizations, and related Super Consistency of the landscape to learning rate transfer. Beyond being able to distinguish feature learning (rich) and kernel (lazy) parametrization, we have also shown how other suboptimal feature learning parametrizations have sharpness dynamics violating Super Consistency through finite-size accumulations. This seems to suggest that Super Consistency of the landscape is an important discriminant when it comes to hyperparameter transfer beyond the rich/lazy regimes. We foresee that our paper could spark further research interest at the intersection between the scaling limits of neural networks and optimization theory.

**On the NTK and Hessian dynamics.** In Section 4.1 we have drawn the connection between progressive sharpening and NTK evolution in the early phase of training. However, Figure 3 (b), shows how the NTK eigenvalues at different widths accumulate finite-size effects over time and diverge from each other, while the Hessian eigenvalues are Super Consistent. This suggests that other forces are at play after progressive sharpening, such as _Self-Stabilization_. In fact, progressive sharpening on one hand, and Self-Stabilization on the other, make the stability threshold a stable attractor of the sharpness dynamics. Gaining theoretical understanding for these complex interactions in the context of scaling limits is an exciting area of future research.

**Design of Step-size Tuners.** In most of our experiments, we rely on a constant step size \(_{0}\). However, an alternative is to use a step-size tuner, i.e. to automatically choose \(_{0}\) based on some criteria of the local landscape . Our results open directions into some possible investigations and design choices for new step size tuners. For instance, do step size tuners transfer with the width and depth of the architecture? Given our results on the role of warmup schedule to improve transfer, it seems plausible to design step size tuners that use EoS results to achieve optimal learning rate transfer under different parameterizations.

**Limitations.** One of the underlying assumptions of the argument presented here is that the sharpness is an important property of the landscape when it comes to step size selection. Indeed, the results in Cohen et al.  establish a more intricate relationship between sharpness and learning rate. We discuss this in Sec. A.2. Overall, our theory on Super Consistency does not exclude the existence of other factors that might influence the optimal learning rate. Hyperparameter transfer requires Super Consistency of the landscape, thus we expect other potential factors to have this property. Finally, we note that due to the high computational cost of Hessian estimation, we do not perform experiments at a larger scale than presented here. It would be interesting to see if Super Consistency still holds at an even larger scale.