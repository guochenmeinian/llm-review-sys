# LoQT: Low-Rank Adapters for Quantized Pretraining

Sebastian Loeschcke

University of Copenhagen

sbl@di.ku.dk &Mads Toftrup

Aarhus University

toftrup@cs.au.dk

&Michael J. Kastoryano

University of Copenhagen

mika@di.ku.dk &Serge Belongie

University of Copenhagen

s.belongie@di.ku.dk &Vestein Sanebjarnarson

University of Copenhagen

vesn@di.ku.dk

Equal contribution.

###### Abstract

Despite advances using low-rank adapters and quantization, pretraining of large models on consumer hardware has not been possible without model sharding, offloading during training, or per-layer gradient updates. To address these limitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a method for efficiently training quantized models. LoQT uses gradient-based tensor factorization to initialize low-rank trainable weight matrices that are periodically merged into quantized full-rank weight matrices. Our approach is suitable for both pretraining and fine-tuning models. We demonstrate this for language modeling and downstream task adaptation, finding that LoQT enables efficient training of models up to 7B parameters on a 24GB GPU. We also demonstrate the feasibility of training a 13B model using per-layer gradient updates on the same hardware.

https://github.com/sebulo/LoQT

## 1 Introduction

Training large neural networks requires substantial hardware and energy resources. Reducing these requirements is important for both cost efficiency and environmental reasons, while also lowering the entry barrier for researchers and practitioners in general. In this work, we target the memory component--a key part of the hardware requirements. Memory use during training comes primarily from storing the weights of the model, the optimizer states, and activations. To target the memory footprint of the weights, various applications of quantization  have been used. For targeting the optimizer states, variations on low-rank adaptation (LoRA)  have been suggested to decrease the number of trainable parameters for fine-tuning, in combination with the use of low precision representations. Low-rank approaches for projecting gradients to a lower rank have also been suggested . In this work, we combine these approaches to address the model size and optimizer states, resulting in a highly memory-efficient configuration that is also suitable for pretraining.

Figure 1: Memory usage of Llama 13B, rank 1024. LW: per-layer gradient updates. A8bit: Adam 8bit.

In typical training configurations, the optimizer states often take up more space than the model itself, as methods such as Adam  keep track of two parameters for each parameter of the model. While LoRA is memory efficient for parameter-efficient fine-tuning of pretrained models, it has not been shown to work as a pretraining method by itself . GaLore  significantly reduces the memory needed for the optimizer parameters by storing the optimizer state in a low-rank projection, which is then projected up when applied to the model weights. Combining this method with quantization would further shrink the footprint of the model but this is not straightforward. Updating the weights of a highly quantized model directly in low-precision space has not been shown to work. This is mainly due to the higher-precision gradient updates having too small of an impact on the lower-precision quantized states.

To address these shortcomings, we propose _Low-Rank Adapters for Quantized Training_ (LoQT). LoQT initializes two low-rank factors, \(P\) and \(B\), for each weight matrix \(W\). \(P\) is initialized using a projection of \(W\)'s gradients into a low-rank subspace, and \(B\) is initialized to minimize the quantization error. In our method, \(B\) is the only matrix being actively optimized. Only optimizing \(B\) means that the size of the gradients and optimizer state shrinks significantly compared to full training or LoRA. The product \(PB\) is periodically merged into the full rank matrix \(W\) with exponentially increasing gaps to account for smaller updates as the model converges, ensuring we accumulate large enough updates. As \(W\) and \(P\) do not receive gradient updates, they can be kept quantized, optimizing memory usage even further. It is the large accumulated updates that make it possible to update a quantized model--as the addition of smaller changes would not register in the quantized state. A high-level overview of our approach is given in Fig. 2.

We show that LoQT works well both with and without quantization, enabling not only a lower memory footprint in the optimizer state but also over the model parameters. Our results show that we get competitive performance to prior methods using significantly less memory, in particular when quantizing the model weights in an application such as training a large language model (LLM). We also demonstrate comparable performance in language adaption, which we demonstrate on a curated Icelandic text dataset . Finally, we show that LoQT also works for fine-tuning pretrained models on down-stream tasks, by training and evaluating on the GLUE  benchmark for natural language understanding and the GSM8K  dataset for mathematical reasoning. We ablate several properties of the suggested approach, demonstrating the importance of each component of LoQT. For instance, we find that an exponentially increasing projection gap is particularly crucial for the training of quantized models. An overview of memory savings is given in Fig. 1. We find that LoQT enables efficient training of 7B models on consumer-grade hardware with 24GB of memory, and makes it feasible to train models with up to 13 billion parameters without model parallelization, by making use of per-layer gradient updates .

Figure 2: Overview of LoQT. (1) Low-rank factors \(P\) and \(B\) are periodically initialized from the gradient of the dequantized model weights \( W\), (2) then only \(B\) is trained while \(P_{q}\) and \(W_{q}\) are kept quantized and frozen, over an exponentially increasing interval until \(T_{i}\), (3) the low-rank factors are merged back into the quantized model. The process is repeated until training halts.

Efficient Pretraining With LoQT

We now briefly introduce how LoQT works by initializing and training low-rank adapters. The adapters are initialized by taking the singular value decomposition (SVD) of a given layer's gradients. We use \(W\) to indicate the full weight matrix of a given layer and \(P\) for the left factor constructed from the SVD decomposition of the gradient matrix, \( W=U V^{}\), such that \(P\) consists of the first \(r\) columns of \(U\)--corresponding to the singular vectors with the \(r\) largest singular values of \(W\), where \(r\) is a given target rank. The update rule for a timestep \(T_{i}\) is then given by \(W_{T_{i}}=W_{T_{i-1}}+PB\). For the steps between \(T_{i}\) and \(T_{i+1}\) only the weights of \(B\) are updated, while \(P\) and \(W_{T_{i-1}}\) remain constant. We describe this in more detail below, followed by a discussion on periodic updating of the factor \(P\), enabling of quantized pretraining, error compensation, and exponential update intervals. Pseudo-code for LoQT is shown in Fig. 3.

### Background: GaLore

Zhao et al.  find that gradients exhibit a low-rank structure during training. They exploit this insight by projecting the gradient to a low-rank subspace and applying the Adam optimizer before projecting back to the original dimensions. By doing this, the memory-intensive optimizer states required by Adam are shrunk significantly for low enough ranks.

**Definition 2.1** (Gradient Low-rank Projection, def. 3.4 in ).: Gradient low-rank projection (**GaLore**) denotes the following gradient update rules, where \(\) is the learning rate, \(\) is the Adam optimizer, \(W R^{m n}\) is the weight matrix being trained, and \(T\) represents the total number of training iterations until the recomputation of the projection matrix:

\[W_{T}=W_{0}+_{t=0}^{T-1}_{t},_{t}=P_{t} _{t}(P_{t}^{}G_{t}Q_{t})Q_{t}^{},\] (1)

where \(r\) is a given target rank and \(P_{t} R^{m r}\) and \(Q_{t} R^{n r}\) are the top-\(r\) singular vectors from the SVD decomposition of the gradient matrix at each iteration \(t\). In practice, this can be approximated by only applying a one-sided projection, as in

\[W_{T}^{}=W_{0}+_{t=0}^{T-1}P_{t}_{t}(P_{t}^{}G_{t})\ \ \ W_{T}^{}=W_{0}+_{t=0}^{T-1}_{t}(G_{t}Q_{t})Q_{t}^{ }.\] (2)

Additionally, Zhao et al.  empirically show that it is sufficient to keep the projection matrix fixed and only update it once every \(T\) iteration.

### Low-rank Gradients as Adapters

We now describe how we initialize the parameters we optimize with LoQT. We start with the GaLore formulation from above and adopt the memory-performance trade-off of using only a one-sided projection (eq. 2), we compute \(P^{}G\) if \(m n\) and \(GQ\) otherwise. Our goal is to separate trainable weights and static weights, which we achieve by rewriting GaLore in terms of low-rank adaptors. We assume that \(m n\), if \(m>n\) the same reasoning holds for \(Q_{t}^{}\). Using the fact that \(P_{t}\) is fixed on the interval \([0,T]\) we get

\[W_{T} =W_{0}+_{t=0}^{T-1}P_{t}(P^{}G_{t})\] (3) \[=W_{0}+_{^{m r}}^{T-1}(P^{}G_{t})}_{B^{r n}}.\] (4)

It is clear from (4) that we can keep track of low-rank updates using rank-\(r\) adaptors. We note that in the interval \([0,T]\) only \(B\) is updated, creating the desired separation. If implemented directly, we would need to compute the gradient with respect to \(W\) and then project it down using \(P^{}G_{t}\). We find that this step is unnecessary; it is sufficient to train \(B\) using standard gradient descent.

Equivalence of Gradient UpdatesWe point out that optimizing the low-rank matrix \(B\) via gradient descent is equivalent to the projected gradient updates on \(W_{t}\) described in Definition 2.1. Let \(G^{W}=}{ W}\) and \(G^{B}=}{ B}\) denote the loss gradients with respect to \(W\) and \(B\), respectively. Consider the forward pass \(y=xW+xPB\), where \(W\) is the weight matrix, \(P\) is the projection matrix, and \(B\) is the low-rank update matrix. By the chain rule:

\[G^{B} =(xP)^{}}{ y}\] (5) \[=P^{}x^{}}{ y}\] (6) \[=P^{}G^{W}\] (7)

This derivation establishes that computing gradients with respect to \(B\) is equivalent to projecting the gradients with respect to \(W\) onto the low-rank subspace defined by \(P\). Therefore, GaLore's low-rank gradient updates are identical to those obtained through backpropagation in LoRA.

### Pretraining with LoRA

Previous work  has shown that training low-rank weight matrices works well for fine-tuning pretrained weights. However, it has been shown that starting with randomly initialized weights, training low-rank factors, and periodically merging them into a frozen weight matrix \(W\), does not work when starting with a randomly initialized matrix . We now address this to enable full training using low-rank weight matrices.

Inspired by prior work [7; 8], we periodically update a given layer \(W_{T+1}=W_{T}+P_{T}B_{T}\) at fixed steps \(T\). This approach allows \(W\) to evolve as a sum of low-rank matrices aligning with GaLore's strategy of updating the gradient subspace during training:

\[W_{t}=W_{0}+ W_{T_{1}}+ W_{T_{2}}++ W_{T_{n}},\] (8)

where \(t=_{i=1}^{||}T_{i}\) and \( W_{T_{i}}=P_{T_{i}}B_{T_{i}}\) represents the product of the learned matrix \(B\) over the interval \(T_{i}-T_{i-1}\) modulated by the gradient projection matrix \(P_{T_{i}}\). After each periodic update at iterations \(T_{i}\), we reinitialize the low-rank factors \(P_{T}\) and \(B_{T}\). As in , we compute the gradient of \(W_{T}\) over a single batch, focusing only on \( W_{T}\) without storing optimizer states for it, reducing the memory compared to full-rank training.

For each updated \(W_{t}\) and reinitialized \(P_{t}\) and \(B_{t}\), a new gradient subspace is established for exploring the next \(T_{i+1}-T_{i}\) steps. Our method treats \(W_{t}\) as the full-rank repository of accumulated updates. Although it is periodically updated, \(W_{t}\) is not part of the optimizer state computations, and the gradients during the single forward pass are offloaded to CPU/RAM. Since the SVD calculations are done layerwise, only the current layer needs to be on GPU, or the SVD can be calculated on CPU. \(P_{t}\) defines the general gradient subspace and trajectory for the upcoming \(T_{i+1}-T_{i}\) steps, and \(B_{t}\) is adjusted to navigate within the direction set by \(P_{t}\). As only \(B_{t}\) is trained, the number of parameters requiring optimizer states is drastically reduced.

### Quantized Training

Recall the update rule of our model, \(W_{T_{i}}=W_{T_{i-1}}+PB\), given that \(B\) is the only matrix accumulating gradients and undergoing changes, the other matrices \(W\) and \(P\) can be kept quantized. This approach allows storing the weights in NF4 precision  (see SS5.1 for a detailed account) without requiring high-precision gradients and weights to update \(W\) and \(P\). To the best of our knowledge, we are the first to enable efficient 4-bit quantized pretraining using gradient descent without storing the weights in 16-bit precision.

We quantize the weights \(q_{}(W)=W_{q}\) and \(q_{}(P)=P_{q}\) as described in SS5.1. During the periodic updates at interval time steps \((_{i=1}^{n}T_{i})_{i=1}^{n}\), \(P_{q}\) and \(W_{q}\) are dequantized using the inverse function, \(P_{}=q_{}^{-1}(P_{})\) and \(W_{BF16}=q_{}^{-1}(W_{})\). After this, \(W_{T_{i}}=W_{T_{i-1}}+P_{T_{i-1}}B_{T_{i-1}}\) is computed and quantized. The quantization and dequantization processes are applied layer by layer, ensuring that not all layers are simultaneously in a non-quantized state to reduce memory usage. Moreover, the quantization state itself is re-quantized for further efficiency following . We implement LoQT using weight-only quantization, this means that the quantized weights are loaded into memory and then dequantized before computing the matrix multiplications.

### Compensating for Quantization Errors

As the quantization process inevitably results in rounding errors there is a discrepancy between the non-quantized and quantized versions of \(W\). We wish to reduce this effect as much as possible. While compensating for quantization errors has been done before , we derive a tailored solution for LoQT.

During the merging update phase, we first dequantize to obtain \(W_{T-1}\) and \(P_{T-1}\), and then compute the update \(W_{T}=W_{T-1}+P_{T-1}B_{T-1}\). This is immediately followed by re-quantizing to get \(Q_{T}=q_{}(W_{T})\). Our goal is to minimize the quantization error \(\|(Q_{T}+P_{T}B_{T})-W_{T}\|\). Recall that \(P_{T}\) is found based on the gradient and is not changed to compensate for the quantization error. Instead, we solve for \(B_{T}\) in the merging step, initializing \(B_{T}\) as \(B_{T}}{=}P_{T}^{+}(Q_{T}-W_{T})\), where \(P_{T}^{+}\) is the Moore-Penrose pseudo-inverse. This approach avoids initializing \(B_{T}\) as zeros, as is commonly done , and instead uses it for minimizing the quantization error \(\|Q_{T}-W_{T}\|\). We then iteratively refine \(B_{T}\) over a maximum of five steps, by recomputing \(Q_{T}=q_{}(W_{T}-P_{T}B_{T})\), improving the alignment between the full-precision \(W\) and its quantized state.

As training advances and the learning rate decays, the magnitude of the update \(B_{T-1}\) decreases. This leads to negligible differences \(|q(Q_{t}+P_{t}B_{t})-Q_{t}|\), which results in the loss plateauing early, as depicted in Fig. 3(a). To address this, we implement an exponentially increasing scheduler for updating \(W\). Drawing from the observation that the gradient rank decays exponentially (Lemma 3.1 in ), we start with an update interval \(\) and progressively increase the update intervals by a factor of \(\). The sequence of updates is then given by \((T_{i})_{i=0}^{}=(+^{i})_{i=0}^{}\). Each \(T_{i}\) marks a training step \(t\) when \(W\) is updated. This scheduling ensures more frequent updates earlier in training and more well-spaced adjustments later, allowing for accumulation of sufficiently large gradients before each progressive update.

```
1:Initialize\((W,G^{W})\):
2:\(U,S,V^{T}(G^{W})\)
3:\(P U[.;.;r]\) {First \(r\) singular vectors}
4:\(P_{q} q_{N}(P)\)
5:\(B 0\)
6:\( W\)
7:for each \(c\) in compensation steps \(C\)do
8:\(Q_{c} q_{N}()\)
9:\(B P^{+}(-Q_{c})\)
10:\( W-PB\)
11:return\(Q_{c},B,P_{q}\) ```

**Algorithm 2** Initialization Procedure

## 3 Experiments

We evaluate LoQT on language model pretraining by training LLaMA-based  language models on the C4 dataset , a collection of text in English that was extracted from the Common Crawl web-scrapes . We train models of sizes of 60M, 130M, 350M, and 1B parameters, adhering to single-epoch training cycles determined by the Chinchilla Scaling Laws . While LoQT is capable of training models up to 13 billion parameters on consumer GPUs, compute limits prevent us from training to convergence for sizes above 1B. We also benchmark LoQT on the GLUE test-suite for natural language understanding , the GSM8K  dataset for arithmetic reasoning and an Icelandic text dataset  to evaluate language adaptation via continued-pretraining. Runs were conducted on up to 4x 40GB NVIDIA A100s 2x 80GB NVIDIA H100s, or a single 24GB NVIDIA RTX 3090. The longest run was the training of the 1B models, taking approximately four days on the four A100s. The RTX 3090 was used for throughput and to empirically verify memory claims.

Figure 3: Pseudo-code for LoQT.

We keep hyperparameters consistent across model sizes, with experiments conducted in BF16 format for memory efficiency. All models are trained with a maximum sequence length of \(256\), a total token batch size of 131K tokens, and a learning rate warmup for the first 10% of the training steps, followed by cosine annealing to 10% of the initial learning rate. Full experimental details, including the specific hyperparameters for each task, are provided in Appendix B.

BaselinesFor pretraining, we compare LoQT against LoRA , ReLoRA , GaLore , and a non-quantized version of LoQT, LoQT-nq. In our experiments, we apply these parameter-efficient training methods to the attention projection matrices and fully connected layers while maintaining full-rank embeddings. For the fine-tuning experiments, we compare LoQT against GaLore, LoHQ , LoRA, ApiQ , and LoQT-nq, or a subset thereof. All models that make use of update frequencies are trained using the same intervals, these are GaLore, ReLoRA, LoQT-nq, and LoQT. We start with an update interval of \(T=100\) and then exponentially increase the update frequency. This means that we do more frequent updates early and fewer as the model stabilizes (see SS 4b for more details). A scaling parameter \(=0.5\) is used for LoQT and GaLore across all models, except for the 1B model where it is decreased to \(0.25\). The same rank \(r\) is used for all low-rank methods. All models are trained using the Adam optimizer, except GaLore which uses their GaLoreAdam optimizer for gradient projection. More details on hyperparameters are provided in the Appendix B.

### Pretraining of Generative Language Models

Results and details of pretraining causal language models of sizes 60M, 130M, 350M, and 1B parameters are shown in Tab. 1. Model sizes are calculated based on the full models without any low-rank methods. We see that LoQT and LoQT-nq both perform very close to full rank pretraining and GaLore while using significantly less memory by keeping most of the model weights in a quantized state. For the 60M model, full training is only slightly better than LoQT, while we see results improve or stay within the standard error for the other sizes. We also notice a slight drop in performance from quantizing the original weight matrix, comparing LoQT and LoQT-nq. The key difference between the approaches is the theoretical memory estimates, e.g. where LoQT uses 59% less memory for the 1B model in full precision and 28% less memory than with GaLore.

    & **60M** & **130M** & **350M** & **1B** \\  Full & 33.32 \(\) 0.22 (0.36G) & 24.51 \(\) 0.03 (0.76G) & 18.87 \(\) 0.18 (2.06G) & 15.56 (7.80G) \\  LoQT (Ours) & 33.98 \(\) 0.15 (0.23G) & 24.57 \(\) 0.01 (0.49G) & 19.12 \(\) 0.01 (0.98G) & 15.55 (3.16G) \\ LoQT-nq (No quant.) & 33.55 \(\) 0.03 (0.28G) & 24.37 \(\) 0.02 (0.63G) & 18.85 \(\) 0.01 (1.47G) & 15.20 (5.11G) \\ GaLore & 34.15 \(\) 0.24 (0.24G) & 24.81 \(\) 0.04 (0.52G) & 19.47 \(\) 0.01 (1.22G) & 15.64* (4.38G) \\ LoRA & 34.99* (0.36G) & 33.92* (0.80G) & 25.58* (1.76G) & 19.21* (6.17G) \\ ReLoRA & 37.04* (0.36G) & 29.37* (0.80G) & 29.08* (1.76G) & 18.33* (6.17G) \\  \(r/d_{model}\) & 128 / 256 & 256 / 768 & 256 / 1024 & 512 / 2048 \\ Training Tokens & 1.1B & 2.2B & 6.4B & 13.1B \\   

Table 1: Comparison of low-rank pre-training methods for LLaMA2-style language models on the C4 dataset. The table shows validation perplexity, memory estimates, and quantization states for LoQT. The rank ratio \(r/d_{model}\) is relative to the largest weight matrix dimension. Perplexity values are averaged over three seeds showing mean and standard error. (*) Denotes results from GaLore . Only one seed was used for the 1B experiment due to compute constraints.

  
**Rank** & **Method** & **MNLI** & **QNLI** & **RTE** & **SST** & **MRPC** & **CoLA** & **QQP** & **STSB** & **Average** \\  & Acc & Acc & Acc & Acc & f1 & Matt & f1 & PCor & \\ 
32 & LoQT-nq & 90.0\(\)0.10 & 94.2\(\)0.06 & **84.8\(\)0.75** & **95.9\(\)0.06** & 94.1\(\)0.25 & **72.5\(\)0.41** & **90.0\(\)0.06** & 91.5\(\)0.07 & **89.1** \\
32 & LoQT & 90.0\(\)0.09 & **94.3\(\)0.04** & 84.1\(\)0.91 & 95.5\(\)0.10 & **94.4\(\)0.20** & 70.5\(\)0.35 & 89.2\(\)0.02 & **91.5\(\)0.13** & 88.7 \\ 
32 & LoRa & 89.9\(\)0.03 & 94.0\(\)0.09 & 83.6\(\)0.12 & 95.7\(\)0.10 & 93.5\(\)0.26 & 69.3\(\)0.47 & 89.8\(\)0.11 & 90.7\(\)0.22 & 88.3 \\
32 & LoIonQ & 90.4\(\)0.09 & 93.2\(\)0.02 & 83.8\(\)0.63 & 95.6\(\)0.07 & 93.2\(\)0.14 & 71.1\(\)0.28 & 89.6\(\)0.12 & 91.0\(\)0.09 & 88.4 \\
32 & Gal.ore & **90.3\(\)0.07** & 94.0\(\)0.04 & 83.7\(\)0.79 & 95.6\(\)0.07 & 93.4\(\)0.38 & 70.7\(\)0.24 & 89.8\(\)0.05 & 90.6\(\)0.01 & 88.5 \\   

Table 2: Results for LoQT, LoQT-nq, and GaLore using DeBERTaV3-base models on the GLUE development set. We report mean and standard error over three seeds. The best mean results on each dataset are shown in **bold**.

### Memory-Efficient Finetuning

We fine-tune the pretrained DeBERTa-V3-base2 model on the natural language understanding GLUE  tasks using LoQT and compare its performance with full fine-tuning baselines, LoRA, LoftQ, and GaLore. See Appendix 7 for details on hyperparameters. Results are given in Tab. 2.

We find that both LoQT-nq and LoQT perform well. And somewhat surprisingly, they sometimes surpass GaLore, LoftQ, and LoRA. This may indicate that initializing the LoRA factors with information about the gradient of \(W\) is a beneficial starting point compared to standard initialization methods. Further experiments are needed to confirm and investigate these findings which we leave to future work.

Arithmetic Reasoning on GSM8KWe fine-tune quantized Llama-2 models (7B and 13B) on the GSM8K dataset  for arithmetic reasoning. As shown in Tab. 3, LoQT achieves average test set accuracies of \(42.6\%\) and \(52.9\%\) with the 7B and 13B models, respectively, performing comparably to other quantized fine-tuning approaches. Detailed hyper-parameters are provided in Appendix Tab. 8.

Continued Pretraining of Llama 7BWe also evaluate LoQT on language adaptation of a large language model. We continue pretraining of the Llama-2-7B model using a curated subset of a public Icelandic text dataset extracted from  containing 770k documents. We compare LoQT with NF4 quantization, LoQT without quantization (LoQT-nq), regular training, and GaLore, using consistent hyper-parameters across all methods, results are shown in Tab. 4. LoQT achieves test set perplexity close to that of using full training or GaLore while reducing perplexity from 4.90 (non-trained model) to 3.63. Additional details are provided in Appendix C.1.

### Memory and Throughput

Memory UsageAn overview of memory usage for GaLore, LoRA and LoQT is given in Tab. 5. We see that LoQT has the same number of parameters as LoRA for a given rank while using less memory for the optimizer states and gradients than in both LoRA and GaLore.

We compare LoQT to GaLore, the approach that gets closest in memory performance, for a model of size 13B in Fig. 1, and for other model-sizes in Fig. 6. We compare three different use cases, applying the methods on their own, combining them with an 8-bit Adam optimizer , and using per-layer weight updates with offloading (while still using 8-bit Adam). We see that LoQT significantly reduces the number of trainable parameters, and optimizer states, compared to GaLore.

Per-layer weight updates are essential for GaLore; without it, an additional \(\)\(12\) GB of VRAM is needed for the gradients of a 7B model, making full-parameter fine-tuning impossible on a 24GB GPU. Additionally, the per-layer gradient updates do not work with gradient accumulation. Using LoQT results in lower memory use than GaLore, even with per-layer gradient updates. When not using per-layer gradient updates, the difference becomes more pronounced as seen for the 7B model in Fig. 6.

LoQT enables training of 7B models without per-layer computations on a 24GB GPU, allowing for gradient accumulation and higher effective batch sizes. Our memory advantage allows for a batch size of \(1280\) tokens compared to GaLore's \(256\) for the 7B model on the 24GB RTX 3090. Using

   Method & Perplexity \(\) \\  No training & 4.90 \\  Full & 3.79 \\ GaLore & 3.96 \\  LoQT-nq & **3.61** \\ LoQT & **3.63** \\   

Table 4: Llama-7B fine-tuning on Icelandic. We report test set perplexity.

   Method & Bit & LLaMA-2-7B & LLaMA-2-13B \\  LoRA & 16 & 41.7 \(\) 0.3 & 51.3 \(\) 0.86 \\ QLoRA & 4 & 41.9 \(\) 0.2 & 51.6 \(\) 0.29 \\ LoftQ & 4 & 41.9 \(\) 0.9 & 51.3 \(\) 0.96 \\ ApiQ & 4 & 42.1 \(\) 0.5 & 52.4 \(\) 0.46 \\  LoQT & 4 & **42.6**\(\) 0.4 & **52.9\(\) 0.12** \\   

Table 3: GSM8K LLaMA-2 7B and 13B test accuracy with std. error. Best mean is in **bold**.

per-layer gradient updates, LoQT can train a 13B model on a single GPU. We refer to Fig. 8 in the Appendix for a comparison of how Adam, GaLore, and LoQT scale with increasing context length.

ThroughputWe evaluate the throughput with a sample batch size of 16, and a total batch size of 512 using gradient accumulation, which is the largest power of two that fits on the GPU. We update the projection matrix \(P\) for every 200 iterations. We evaluate the throughput using a 1B parameter model and rank 512 without per-layer gradient updates. We find that LoQT processes 16% fewer tokens per second than training with only AdamW, at 3996 tokens/s compared to 4782 tokens/s on the RTX 3090.

## 4 Ablations

Quantization Error Compensation and InitializationWe analyze the validation loss curves of \(130\) million parameter models to assess the impact of error quantization compensation. Fig. 3(a) shows that quantizing \(W\), or both \(W\) and \(P\), without error compensation, or exponential interval updates leads to early stagnation of the loss. We also note that quantizing \(P\) has a much smaller effect on the loss compared to quantizing \(W\). Error compensation significantly improves the model's performance, resulting in approximately \(3.5\) points better perplexity. Adding exponentially increasing update intervals improves perplexity further by an additional \(1.5\) points, achieving performance close to that of models without quantization.

Without the quantization error compensation, detailed in SS2.5, LoQT's performance stagnates earlier and diverges more from the other models. This demonstrates the effectiveness of our compensation approach in mitigating the quantization errors introduced during the update of \(W\) with \(PB\) and subsequent quantization steps.

Figure 4: Ablation results for update intervals, error-compensation, quantization using model size 130m, and rank \(256\). \(W_{q}\): quantized \(W\); \(P_{q}\): quantized \(P\); No Q: no quantization. The dynamic update interval \(100+1.2^{}\) grows exponentially for each step \(i\).

    & GaLore & LoRA & LoQT (Ours) \\  Weights & \(mn\) & \(mn+mr+nr\) & \(mn+mr+nr\) \\ Optimizer States & \(mr+2nr\) & \(2mr+2nr\) & \(2nr\) \\ Gradients & \(mn\) & \(mr+nr\) & \(nr\) \\ Pretraining & Yes & No & Yes \\ Fine-Tuning & Yes & Yes & Yes \\ Quantizeable & No & Yes & Yes \\   

Table 5: Comparison of memory usage for GaLore, LoRA, and LoQT. \(W^{m n}\) (\(m n\)), rank \(r\).

Projection Update IntervalsOur scheduling approach ensures more frequent updates earlier on in training when the weight adjustments are larger. As training progresses, the update intervals get larger, allowing for accumulating more updates to compensate for smaller changes at each step that might otherwise be canceled out by the quantization errors. Fig. 3(b) presents an ablation study of progressively increasing update intervals starting at \(100\) and increasing by a factor of \(1.2^{T}\) up to \(2500\). We show the validation loss curves for fixed update frequencies \(200\), \(400\), \(500\), and \(1000\).

The results show that exponentially increasing the update interval is particularly beneficial for models employing quantization, enabling them to achieve the same perplexity as those without quantization. Conversely, the performance gains are more subtle for models that do not use quantization. We hypothesize that even these models might benefit from the larger projection interval intervals. This could be due to the reduction in the accumulation of errors from frequent updates of the projection factor \(P\), as the influence of outdated optimizer statistics becomes less prevalent. Finally, an ablation on the ranks used for \(P\) and \(B\) is given in Fig. 5 in the Appendix.

## 5 Related Work

We now provide an overview of related work on quantization, parameter-efficient fine-tuning methods, and memory-efficient approaches.

### Neural Network Quantization and NF4

Quantization compresses neural networks by converting high-precision values into lower-precision formats, significantly reducing storage requirements [21; 22; 23; 20]. The process involves taking a datatype of high precision, such as 32-bit, requiring 4 bytes of memory, and converting it into a representation with increasing rounding errors but lower memory cost. In this work, we use NF4 quantization , since it is a 4-bit code it only contains \(2^{4}\) different values. NF4 works by first normalizing values onto the interval \([-1:1]\), these are then discretized onto quantiles of the normal distribution, \((q_{i})_{i=1}^{16}\) (see  for details). The elements of a layer are divided into blocks of 64 weights. Each block \(\) has a scaling factor \(_{}=_{w}|w_{32}|\).

\[w_{} =q_{}(w,_{})\] (9) \[}}{{=}}_{q_{i }}|w/_{}-q_{i}|,\] (10) \[w =q_{}^{-1}(w_{},_{})\] (11) \[}}{{=}}_{}  w_{}.\] (12)

We provide an overview of different categories of quantization techniques, and how they relate to LoQT, in Appendix A. Compared to prior approaches, LoQT retains the benefits of reduced memory usage while minimizing accuracy loss, using high-precision updates on a low-rank representation. This allows for efficient model updates without the overhead of full matrix storage and re-quantization.

### Adaptation of Pretrained Networks

Low-Rank Adaptation (LoRA)  enables fine-tuning of pretrained models using low-rank adaptors, effectively reducing the memory footprint by only training weight adaptors for targeted layers. However, simple low-rank training using LoRA factor matrices has not been shown to work for pretraining .

LoRA employs trainable low-rank matrices \(A\) and \(B\) that are used to update \(W\) following \(W_{t}=W_{t-1}+AB\), where \(W_{t-1}\) is frozen to enable precise adjustments within a low-rank framework. Since LoRA only trains \(A\) and \(B\) and keeps \(W\) fixed, QLoRA  explore quantizing \(W\). They fine-tune a quantized model \(q(W)=W_{q}\) with 4-bit precision using randomly initialized 16-bit precision factors \(A\) and \(B\). To address quantization errors \(=|W_{q}-W|\), low-rank factors of the quantization error \(\) have been used .

LoQT extends LoRA to both pretraining and fine-tuning. Unlike traditional LoRA, LoQT uses \(A\) and \(B\) to refine \(W\) throughout training, with \(A\) initialized from \(W\)'s gradient projection and \(B\) trained along this gradient path. LoQT also incorporates quantization and targeted optimization iterationssimilar in spirit to LoftQ , correcting for quantization errors in \(W_{q}\), thus better aligning it with the original non-quantized \(W\).

### Memory Efficient Optimization

Optimizer memory consumptionA significant portion of the memory needed to train neural networks is typically consumed by optimizer states. Notably, Adam , one of the most widely used optimizers, uses double the amount of memory as the gradient matrix to maintain first and second-order gradient statistics. Efforts to reduce this overhead have led to the development of adaptive optimization algorithms like Adafactor , which achieves sub-linear memory costs by factorizing the second-order statistics into a row-column outer product. GaLore  expands on this concept by using low-rank factorization and projecting low-rank gradients up to full size when updating model weights.

Periodic updating of weight matricesReLoRA  combines low-rank updates with initial full-rank training. They find that doing one-third of the training in full-rank, and the subsequent two-thirds in low-rank (see SS5.2) results in comparable performance to standard training methods.

Low-rank GradientsGalore , focuses on the structure of the gradients, projecting them into a low-rank space using factors \(P\) and \(Q\), which are derived from a truncated singular value decomposition (SVD) of the weight matrix gradient, \(G_{W} P_{r}_{r}Q_{r}\). This reduces memory costs associated with storing the optimizer states and aligns with findings from recent studies which suggest that learning primarily occurs within a low-dimensional subspace at a given time [25; 26]. This can be further combined with applying per-layer gradient updates, reducing the memory needed for storing the gradients for the full model at once .

LoQT builds on GaLore's gradient projection (SS2.1) to initialize LoRA factors while updating the full matrix following a schedule inspired by ReLora, while only training one low-rank matrix per layer. We achieve comparable quality to GaLore and better performance than ReLoRA while reducing tunable parameters and memory usage compared to both approaches.

## 6 Discussion and Conclusion

We have presented LoQT, a method for memory-efficient pretraining and adaptation of quantized models. Key insights behind the approach are the benefits of initializing low-rank factors using the gradient of the weight matrix and using exponentially increasing update gaps that make updating of a quantized model feasible. While our initial goal was to lower memory usage, to facilitate the training of models such as LLMs on consumer-grade hardware, we are also cautiously excited about the results sometimes exceeding those of the baselines. We hope to see this explored in more detail in future work.

Our method is general and has the potential to open up new ways of decreasing memory use and improving training throughput through further optimization of our implementation. This could be done by using other quantization methods such as NF2  or quantization of activations, making it possible to do the matrix multiplications using modern tensor core formats such as FP8 or INT4.

## 7 Impact and Limitations

The presented work has the potential to benefit those working in hardware-constrained settings by enabling more efficient training on consumer-grade hardware. We are particularly excited to see the method being applied in single GPU settings.

We have validated LoQT on several model sizes, by training over many steps, by fine-tuning on a standard benchmark for natural language understanding, mathematical reasoning, and language adaptation. While we are confident in our results, further exploration of training duration, data diversity, and hyper-parameter tuning might lead to different results in those settings and we encourage users to confirm the benefit of LoQT for their approach.

Acknowledgements

This work is supported by the Danish Data Science Academy, which is funded by the Novo Nordisk Foundation (NNF21SA0069429) and VILLUM FONDEN (40516). Serge Belongie and Vesteinn Snaebjarnarson are supported by the Pioneer Centre for AI, DNRF grant number P1. MJK acknowledges support from the Carlsberg Foundation and the Novo Nordisk Foundation. Mads Toftrup gratefully acknowledges the Data-Intensive Systems research group at Aarhus University for providing GPU access.