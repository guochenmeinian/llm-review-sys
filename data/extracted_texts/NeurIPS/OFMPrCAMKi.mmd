# Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels

Zifu Wang\({}^{1}\)

Correspondence to: zifu.wang@kuleuven.be

Xuefei Ning\({}^{2}\)

Matthew B. Blaschko\({}^{1}\)

\({}^{1}\) ESAT-PSI, KU Leuven, Leuven, Belgium

\({}^{2}\) Department of Electronic Engineering, Tsinghua University, Beijing, China

###### Abstract

Intersection over Union (IoU) losses are surrogates that directly optimize the Jaccard index. Leveraging IoU losses as part of the loss function have demonstrated superior performance in semantic segmentation tasks compared to optimizing pixel-wise losses such as the cross-entropy loss alone. However, we identify a lack of flexibility in these losses to support vital training techniques like label smoothing, knowledge distillation, and semi-supervised learning, mainly due to their inability to process soft labels. To address this, we introduce Jaccard Metric Losses (JMLs), which are identical to the soft Jaccard loss in standard settings with hard labels but are fully compatible with soft labels. We apply JMLs to three prominent use cases of soft labels: label smoothing, knowledge distillation and semi-supervised learning, and demonstrate their potential to enhance model accuracy and calibration. Our experiments show consistent improvements over the cross-entropy loss across 4 semantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land) and 13 architectures, including classic CNNs and recent vision transformers. Remarkably, our straightforward approach significantly outperforms state-of-the-art knowledge distillation and semi-supervised learning methods. The code is available at https://github.com/zifuwanggg/JDTLosses.

## 1 Introduction

The Jaccard index, also known as the intersection over union (IoU), is a widely used metric in the evaluation of semantic segmentation models. Its appeal lies in its scale invariance and its superior ability to reflect the perceptual quality of a model compared to pixel-wise accuracy . In line with the principles of empirical risk minimization in statistical learning theory, the metric used for evaluation should also be optimized during training . Consequently, directly optimizing IoU via differentiable surrogates has found considerable attention in the literature .

Notably, IoU is only defined when both predictions and ground-truth labels are discrete binary values, i.e., they reside on the vertices of a \(p\)-dimensional hypercube \(\{0,1\}^{p}\). However, the output of neural networks are soft probabilities that are in the interior of the hypercube \(^{p}\). To extend the value of IoU from vertices to the entire hypercube, two popular strategies are currently in use. The first strategy relaxes set counting with norm functions, exemplified by the soft Jaccard loss (SJL)  and the soft Dice loss (SDL) . The second strategy computes the Lovasz extension of the IoU, such as the Lovasz hinge loss  and the Lovasz-Softmax loss (LSL) . These losses facilitate a plug-and-play use and have significantly enhanced the performance of segmentation models, outperforming pixel-wise losses such as the cross-entropy loss (CE) and the focal loss . For instance, Rakhlin et al.  won the land cover segmentation task of the DeepGlobe Challenge  utilizing LSL. Additionally, SDL is becoming increasingly prominent in recent segmentation works , such as Segment Anything . Moreover, SJL and SDL are now the standard for training medical imaging models .

Despite their widespread application, current IoU losses lack the flexibility needed to accommodate key training techniques. Specifically, while these losses relax predictions from \(\{0,1\}^{p}\) to \(^{p}\), they neglect the possibility of labels residing in \(^{p}\), making them incompatible with soft labels. Soft labels have been employed in numerous state-of-the-art models and have proven effective in enhancing network accuracy [37; 68; 62] and calibration [20; 44; 42; 82]. For example, label smoothing (LS)  generates soft labels by taking a weighted average of one-hot hard labels and a uniform distribution over labels. In knowledge distillation (KD) , class probabilities generated by a teacher network serve as soft labels to guide a student model. In semi-supervised learning (SSL) such as MixMatch , \(k\) views of an unlabeled image are fed to a classifier, and the predictions are averaged to create soft labels.

Motivated by the limitation of existing IoU losses, particularly their inability to accommodate soft labels and, consequently, techniques like LS, KD, and SSL, we propose two variants of SJL, termed Jaccard metric losses (JMLs) since they are metrics on \(^{p}\). JMLs yield the same value as SJL on hard labels but are fully compatible with soft labels. Therefore, we can safely replace the existing implementation of SJL with JMLs without affecting the performance on hard labels. To embed our losses with use cases (LS, KD, SSL), we first introduce boundary label smoothing (BLS) which facilitates the integration of label smoothing into the segmentation task. BLS can be used independently and also synergizes with KD and SSL. Subsequently, we present a confidence-based scheme for selecting classes to contribute to the loss computation, thereby enhancing performance in KD. We conduct extensive experiments on 4 datasets (Cityscapes , PASCAL VOC , ADE20K , DeepGlobe Land ) across 13 architectures, including classic CNNs [21; 54] and recent vision transformers . Our results demonstrate significant improvements over the cross-entropy loss. Moreover, our straightforward approach outperforms state-of-the-art segmentation KD and SSL methods by a substantial margin.

## 2 Methods

### Preliminaries

Given a segmentation output \(\{1,...,C\}^{p}\) and a ground-truth \(\{1,...,C\}^{p}\) where \(C\) is the number of classes, for each class \(c\), we define the set of predictions as \(x^{c}=\{=c\}\), the set of ground-truth as \(y^{c}=\{=c\}\), the union as \(u^{c}=x^{c} y^{c}\), the intersection as \(v^{c}=x^{c} y^{c}\), the symmetric difference (the set of mispredictions) as \(m^{c}=(x^{c} y^{c})(y^{c} x^{c})\), and the Jaccard index as IoU\({}^{c}=|v^{c}|/|u^{c}|\). For multi-class segmentation, IoU\({}^{c}\) are averaged across classes, yielding the mean IoU (mIoU). In the sequel, we will encode sets as binary vectors \(x^{c},y^{c},u^{c},v^{c},m^{c}\{0,1\}^{p}\) where \(p\) is the number of pixels, and denote \(|x^{c}|=_{i=1}^{p}x^{c}_{i}\) the cardinality of the corresponding set. For simplicity, we will drop the superscript \(c\) in the following.

In order to optimize IoU in a continuous setting, we need (almost everywhere) differentiable interpolations of this discrete score. In particular, we want to extend the IoU loss

\[_{}:x\{0,1\}^{p},y\{0,1\}^{p} 1-= \] (1)

with \(_{}\) so that it attains a value with any vector of predictions \(^{p}\). In what follows, when the context is clear, we will use \(x\) and \(\) interchangeably.

The soft Jaccard loss (SJL) [46; 49] generalizes IoU by realizing that when \(x,y\{0,1\}^{p}\), \(|v|= x,y\) and \(|u|=|x|+|y|-|v|=\|x\|_{1}+\|y\|_{1}- x,y\). Therefore, SJL replaces the set notation with vector functions:

\[_{,L^{1}}:x^{p},y\{0,1\}^{p} 1- +\|y\|_{1}- x,y}.\] (2)

The \(L^{1}\) norm can be replaced with the squared \(L^{2}\) norm :

\[_{,L^{2}}:x^{p},y\{0,1\}^{p} 1- ^{2}+\|y\|_{2}^{2}- x,y}.\] (3)

### The Limitation of Existing IoU Losses

The primary shortcoming of current IoU losses is that they do not necessarily have desired properties when presented with soft labels, i.e., when \(y^{p}\). This limitation impedes their application in crucial training techniques like LS, KD, and SSL.

Consider the case of \(_{,L^{1}}\), and for simplicity, a single pixel scenario: \(_{,L^{1}}=1-\). It is easy to confirm that for any \(y>0\), \(_{,L^{1}}\) is minimized at \(x=1\) since it monotonically decreases as a function of \(x\). Hence, \(_{,L^{1}}\) is in general not minimized when \(x=y\), a basic property anticipated from a loss function. Further analysis for high-dimensional cases and additional experiments on real datasets are provided in Appendix C and E, respectively.

\(_{,L^{2}}\) does not exhibit this issue, since \(_{,L^{2}}=0|x|_{2}^{2}+|y|_{2}^{2} -2 x,y=0 x=y\). However, it is known to yield inferior results compared to its \(L^{1}\) counterpart, possibly due to its flatter nature around the minimum . In practice, it is rarely used. For instance, in SMP , a popular open-source semantic segmentation project, only the \(L^{1}\) version is implemented. Our evaluations in Appendix E confirm its inferior performance relative to the \(L^{1}\) version. Additionally, the approach of substituting set notation with the \(L^{1}\) norm is widely utilized in numerous other works, including the soft Dice loss [57; 17], the soft Tversky loss , the focal Tversky loss , and others. The soft Dice loss is also included in the formulation of the PQ loss  which is used in panoptic segmentation . Consequently, all of them struggle with soft labels.

Losses based on the Lovasz extension, such as the Lovasz-Softmax loss , the Lovasz hinge loss , and the PixIoU loss , cannot handle soft labels as the Lovasz extension is not well-defined for \(y(0,1)^{p}\). More details and comparisons with the Lovasz-Softmax loss can be found in Appendix D and E, respectively. Automatically searched loss functions, such as Auto Seg-Loss  and AutoLoss-Zero , also fail to accommodate soft labels as their search space is confined to integral labels.

In summary, despite their widespread adoption in recent works on semantic segmentation [17; 27; 9; 8; 29] and panoptic segmentation [5; 61; 74; 75], these losses all exhibit a common shortcoming: an inability to handle soft labels. In this paper, we specifically concentrate on re-designing SJL. Other losses, including the soft Dice loss, the soft Tversky loss, and the focal Tversky loss, are addressed in our subsequent work .

### Jaccard Metric Losses

We can rewrite the intersection \(|v|\) and the union \(|u|\) as a function of the set difference \(|m|\):

\[|v|=(|x|+|y|-|m|)|u|=|v|+|m|.\] (4)

Note that \(|m|=\|x-y\|_{1}\). Combining these yields:

\[|v|= x,y=(\|x+y\|_{1}-\|x-y\|_{1}),\] (5) \[|u|= x,y+\|x-y\|_{1}=(\|x+y\|_{1}+\|x-y \|_{1}),\] (6)

where the equalities hold when \(x,y\{0,1\}^{p}\).

After eliminating erroneous combinations that have the same issue as SJL, we are left with two candidates \(_{,1},_{,2}:^{p} ^{p}\) that are defined as:

\[_{,1}=1--\|x-y\|_{1}}{ \|x+y\|_{1}+\|x-y\|_{1}},\] (7) \[_{,2}=1-}.\] (8)

It is a well-known result that \(_{}\) is a metric on \(\{0,1\}^{p}\). In Theorem 2.1 (see Appendix F for the proof), we show that both \(_{,1}\) and \(_{,2}\) are also metrics on \(^{p}\). Therefore, we call them Jaccard Metric Losses (JMLs).

**Theorem 2.1**.: _Both \(_{}\) and \(_{}\) are metrics on \(^{p}\). Neither \(_{^{1}}\) nor \(_{^{2}}\) is a metric on \(^{p}\)._

Recall the definition of a metric:

**Definition 2.2** (Metric ).: A mapping \(f:M M\) is called a metric on \(M\) if for all \(a,b,c M\), it satisfies the following conditions: (i) (reflexivity). \(f(a,a)=0\). (ii) (positivity). \(a b f(a,b)>0\). (iii) (symmetry). \(f(a,b)=f(b,a)\). (iv) (triangle inequality). \(f(a,c) f(a,b)+f(b,c)\).

Having a loss function \(\) that is a metric carries numerous benefits. Reflexivity and positivity collectively imply that \( x,y^{p},x=y=0\), meaning that \(\) would be compatible with soft labels. The triangle inequality also provides insightful guidance. Applied to KD, it yields:

\[(S,L)(S,T)+(T,L).\] (9)

Here, \(S\) represents the student model, \(T\) the teacher model, and \(L\) the ground-truth labels. In the KD, we initially train the teacher model using the ground-truth labels, and then minimize the loss between the teacher and the student model. Equation (9) suggests that if we adhere to this process--equivalent to minimizing the right-hand side of the equation--we also minimize the upper bound on the student model's loss with respect to the ground-truth labels.

Furthermore, as Theorem 2.3 shows (see Appendix G for the proof), when only hard labels are involved, \(_{},_{}\) and \(_{^{1}}\) are identical. Hence, we can safely replace the existing implementation of SJL with JMLs. When soft labels are introduced, \(_{}\) and \(_{}\) might yield different values. We discuss their distinctions in Appendix H. From both theoretical and empirical perspectives, we find that \(_{}\) is slightly more favorable than \(_{}\). As a default in our experiments, we use \(_{}\).

**Theorem 2.3**.: \( x^{p},\;y\{0,1\}^{p}\) _and \(x\{0,1\}^{p},\;y^{p}\), \(_{}=_{}=_{^{1}}\). \( x,y^{p},_{}_ {}_{^{1}}\)._

Figure 1 plots the loss value of \(_{}\) and the cross-entropy loss (CE) for soft labels \(y=0.1\) (left) and \(y=0.9\) (right). Typically, the model is randomly initialized, resulting in a low initial confidence. Although the gradient of CE is substantial at the beginning, it plateaus as training progresses and the prediction is close to the target. Conversely, \(_{}\) offers effective supervision later in the process. However, when \(y\) is extremely small (e.g., \(y=0.1\)), \(_{}\) can become overly steep, potentially causing problems in KD. This issue will be explored in greater detail in Section 2.4.2.

### Use Cases

Soft labels find wide-ranging applications. In this paper, we investigate three of the most common use cases: label smoothing (LS), knowledge distillation (KD) and semi-supervised learning (SSL). We leave the discussion on SSL in Appendix I.

#### 2.4.1 Label Smoothing

In LS , the one-hot encoding is combined with a uniform distribution under a smoothing coefficient \(\), resulting in soft labels \(SL^{}\). In JML-LS, a network \(H\) is trained with the following loss:

Figure 1: Loss value vs. prediction with \(y=0.1\) (left) and \(y=0.9\) (right).

\[_{}=_{}_{}+_{}_{}\] (10)

such that \(_{}=(H,SL^{e})\) and \(_{}=(H,SL^{e})\).

LS provides a regularization effect during training. However, when we regard semantic segmentation as a pixel-wise classification task and apply LS to every pixel in the image without distinction, we fail to consider spatial differences. This is particularly evident with SegFormer-B3 on ADE20K, which achieves an overall pixel-wise accuracy of \(81.91 0.02\%\), but drops to \(47.98 0.01\%\) in the boundary region. These areas, due to their inherent ambiguity, require stronger regularization. Moreover, our empirical findings suggest that smoothing non-boundary regions only yields marginal improvements. Therefore, we introduce boundary label smoothing (BLS), which only applies smoothing to labels near the boundary. We refer to the resulting loss as JML-BLS. Specifically, for every pixel \(i\), we examine its \(k k\) neighborhood \(N_{i}\), and the pixel \(i\) is regarded as a boundary pixel if there exists a pixel \(j N_{i}\) such that their ground-truth labels are different: \(y_{i} y_{j}\). This can be efficiently computed by applying a max pooling layer to the one-hot encoding.

#### 2.4.2 Knowledge Distillation

In KD , besides the ground-truth label \(L\), a student model \(S\) is trained to minimize the discrepancy to a teacher network \(T\) simultaneously:

\[_{}=_{}_{}+_{}_{}\] (11)

where \(_{}=(S,L)\) and \(_{}=(S,T)\).

In JML-KD, we add additional supervision from JML:

\[_{}=_{}_{}+_{}_{}\] (12)

where \(_{}=(S,L)\) and \(_{}=(S,T)\).

Combining these, we have

\[_{}=_{}_{}+_{}_{}.\] (13)

Existing segmentation KD methods rely on pixel-wise supervision either in the output  or in the feature space . Our \(_{}\), however, enables direct distillation of the teacher's IoU information to the student which aligns with the final evaluation metric.

Moreover, recall that JMLs are averaged for each class, making it crucial to determine which classes contribute to the loss value at each iteration. We refer to these classes as _active classes_. In KD, the student uses information from non-target classes to learn the class relationship from the teacher . However, a teacher network may also include noisy and unhelpful predictions for unimportant classes. Therefore, we propose to filter out unimportant classes based on the confidence level of the teacher network. Specifically, JMLs are computed only over classes where the teacher's confidence exceeds a predefined threshold. This approach has two benefits: first, the student will not be distracted by irrelevant classes, aiding in generalization. Second, as shown in Figure 1, we find that the loss value of JMLs can become excessively steep around the target when the ground-truth is low (e.g. \(y=0.1\)). Therefore, ignoring these classes can stabilize the training process.

We have also observed that the student benefits from a teacher trained with JML-BLS. In contrast, in classification tasks, it has been found that a teacher trained with LS can detrimentally affect the student's performance . This is often cited as a counterexample to the notion that a more accurate teacher will necessarily distill a better student. We believe that the teacher, trained through JML-BLS, acquires intricate boundary information, and this "dark knowledge" is implicitly passed to the student, resulting in a more accurate student.

## 3 Experiments

**Experimental setups.** We adopt Pytorch Image Models (timm) , which provides implementations and ImageNet  pre-trained weights for various backbones. In our experiments, CNN backbones include ResNet-101/50/18  and MobileNetV2 ; transformer backbones contain MiT-B5/B3/B1/B0 . Segmentation methods consist of DeepLabV3+ , DeepLabV3 , PSPNet, UNet  and SegFormer . We evaluate models on Cityscapes , PASCAL VOC , ADE20K  and DeepGlobe Land . In summary, 13 models and 4 datasets are studied in the sequel. More details of each model are in Appendix A and training recipes are in Appendix B.

**Evaluation metrics.** To provide a comprehensive comparison, we report both overall pixel-wise accuracy (Acc) and mean intersection over union (mIoU). To evaluate the effects of our methods on calibration, we present the expected calibration error (ECE)  and the ECE computed only over the boundary region, denoted as BECE. For Cityscapes, PASCAL VOC, and ADE20K, we repeat the experiments 3 times (except for SSL experiments that are single runs) and report performance on the validation set. For DeepGlobe Land, we conduct 5-fold cross-validation. All results are presented in the format of mean\(\)standard deviation. We do not apply any test-time augmentation.

### Results on Accuracy

We report results on Cityscapes (Table 1), PASCAL VOC (Table 2), ADE20K (Table 3), and DeepGlobe Land (Table 13, Appendix J). Key takeaways from our results are as follows:

**JML significantly improves accuracy (Acc and mIoU).** Compared to training with CE alone, incorporating JML as part of the loss function can significantly enhance a model's mIoU. The improvement is typically more than 2% on Cityscapes, PASCAL VOC, and ADE20K. Additionally, it can also increase a model's Acc. This suggests that the benefits of JML are not solely due to its alignment with the evaluation metric (mIoU), but also because it aids the overall optimization process.

**JML benefits more from soft labels.** For instance, the improvements of mIoU for DL3-R101 on PASCAL VOC are 0.48% (CE vs. CE-BLS) and 1.25% (JML vs. JML-BLS), respectively. We adopted training procedures that are heavily optimized for CE, which might explain why JML requires stronger regularization. More qualitative results can be found in Figure 7 and Figure 8 (Appendix L).

**KD is more effective than BLS, while BLS performs well on datasets with simple boundary condition.** Both BLS and KD consistently improve the model's accuracy. Despite its simplicity, BLS can yield significant improvements, especially on PASCAL VOC where boundary condition is less complex. As boundary condition becomes more intricate, as in Cityscapes and ADE20K, KD, seen as learned label smoothing , typically outperforms BLS.

**SOTA results on segmentation KD.** Without bells and whistles, our simple approach that only uses soft labels greatly exceeds SOTA segmentation KD methods, as shown in Table 4. Indeed, the model trained only with hard labels in JML already achieves a comparable result as some of these methods. For example, on PASCAL VOC, DL3-R18 achieves \(74.42 0.52\%\) mIoU while CIRKD , a complex distillation method only attains \(74.50\%\) mIoU. Note that our baseline is not stronger than theirs: our DL3-R18 trained with CE has \(72.47 0.33\%\) mIoU while their number is \(73.21\%\) (because we use a smaller batch size, see Appendix B).

**SOTA results on segmentation SSL.** By incorporating JML-BLS with AugSeg  (refer to Appendix I for further details), we achieve SOTA segmentation SSL results as illustrated in Table 11 (Appendix I). Notably, improvements are particularly significant on smaller splits - exceeding a 4% enhancement over AugSeg on the 92 split.

### Results on Calibration

**CE-BLS and CE-KD sometimes hurt calibration.** Contrary to the common belief in classification [44; 42; 82], we find that both CE-BLS and CE-KD can sometimes deteriorate model calibration. For instance, in Cityscapes experiments, the lowest ECE is usually obtained with CE only. Nevertheless, CE-BLS can still improve model calibration near the boundary.

**JML compromises calibration.** Although the soft Dice loss can significantly improve a model's segmentation performance, it is well known to yield poorly calibrated models [41; 3]. We confirm that this is also the case for JML. Interestingly, while models trained with JML exhibit inferior top-class calibration as measured by ECE, they actually achieve better multi-class calibration as indicated by the static calibration error (SCE). We provide more detailed results on calibration in Appendix K.

**JML-BLS and JML-KD consistently improve calibration.** Although JML presents challenges in model calibration, these can be greatly mitigated by training models with soft labels. In JML experiments, both JML-BLS and JML-KD reliably enhance model calibration.

[MISSING_PAGE_EMPTY:7]

## 4 Ablation Studies

### JML Weights

It is common in recent works [5; 9; 8; 29] to balance CE and the Dice loss with equal weights. For JML, we adopt 0.25/0.75 in all our experiments and find it slightly superior than 0.5/0.5.

### Jml-Bls

**Although BLS is sensitive to the choice of \(\), it effectively increases model accuracy and calibration.** The effect of the smoothing coefficient \(\) on PASCAL VOC with DL3-R101/50/18 is shown in Figure 5 (Appendix L). Interestingly, for DL3-R50 and DL3-R18, the optimal \(\) that achieves the highest mIoU also yields the lowest ECE.

**We need a strong smoothing coefficient near the boundary.** The optimal \(\) for different kernel size \(k\) on PASCAL VOC with DL3-R18 is shown in Figure 4 (Appendix L). Note that \(k=\) implies LS is applied to every pixel, i.e. vanilla LS. Generally, as \(k\) increases, we need to decrease the strength of smoothing. The best result is obtained when we only smooth a small region near the boundary with a strong smoothing coefficient (\(k=3\) and \(=0.50\)).

### Jml-Kd

**Loss terms.** We examine the contribution of each loss term in Table 6 using a DL3-R18 student. Adding JML terms significantly improves the student's performance.

**Filtering out unimportant classes based on teacher's confidence is useful.** We examine the impact of active classes on PASCAL VOC with DL3-R18 in Table 7. In particular, we propose to ignore classes where the soft label, i.e. teacher's confidence, is low (marked as LABEL). And in ALL, we include all classes. In PRESENT, we select the class with the maximum confidence from the teacher's prediction in the class dimension. In PRDB, we skip classes where the student itself is not confident. In BOTH, we take both the teacher's and student's confidences into account. The code to compute active classes is in Figure 6 (Appendix L).

We observe that using ALL classes in JML-KD can misguide the student. In most of the teacher's output classes, the confidence is usually very low. It can be challenging and potentially detrimental for the student to precisely mimic these numbers. With PRESENT, the performance remains similar to that without soft labels. This is because the effectiveness of using soft labels come from the non-argmax classes. PROB and BOTH achieve similar performance, but both are worse than LABEL.

**Teacher's calibration is beneficial.** In Sec. 2.4.2, we suggest that a teacher trained with JML-BLS offers more boundary information to the student. But what exactly is this boundary information? We

   \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & VOC \\  ✓ & - & - & - & - & \(72.88 0.44\) & \(72.47 0.33\) \\ ✓ & ✓ & - & - & - & \(75.55 0.13\) & \(74.42 0.52\) \\ ✓ & ✓ & ✓ & - & - & \(75.74 0.25\) & \(74.65 0.42\) \\ ✓ & ✓ & ✓ & ✓ & - & \(76.16 0.37\) & \(75.05 0.31\) \\ ✓ & ✓ & ✓ & ✓ & ✓ & \(76.68 0.33\) & \(75.89 0.29\) \\   

Table 6: Evaluating different losses terms on Cityscapes and PASCAL VOC using a DL3-R18 student. \(_{}\) means we train the teacher with JML-BLS. All results are mIoU (%). Red: the best in a column.

   Model & 0.100/90 & 0.250.73 & 0.500/50 & 0.990/10 & 1.000/00 \\  DL3-R101 & \(79.80 0.40\) & \(80.26 0.45\) & \(79.92 0.12\) & \(78.73 0.30\) & \(78.39 0.09\) \\ DL3-R18 & \(73.67 0.39\) & \(74.42 0.52\) & \(74.30 0.29\) & \(73.21 0.23\) & \(72.47 0.33\) \\   

Table 5: Ablating different values of \(_{}/_{}\) on PASCAL VOC using DL3-R101 and DL3-R18. All results are mIoU (%). Red: the best in a row. Green: the worst in a row.

believe it relates to the teacher's calibration. The student, with less capacity, often struggles to mimic a more powerful teacher, especially near ambiguous boundary regions. However, if the teacher is more calibrated and outputs a less peaked distribution, the student will learn this uncertainty rather than attempting to match an unrealistic distribution that it lacks the capacity to replicate.

In Table 8, we compare three teachers with various accuracy and calibration on PASCAL VOC. In particular, both T2 and T3 are trained with JML-BLS but with different smoothing parameter \(\), while T1 is not. T2, although having a lower mIoU, is more calibrated than T1. Consequently, T2's student is more accurate and better calibrated than T1's.

## 5 Related Works

IoU is a commonly employed metric in semantic segmentation, and IoU losses strive to optimize this metric directly. IoU only obtains values when both predictions and ground-truth labels are discrete binary vectors \(\{0,1\}^{p}\), but the neural network often predicts soft probabilities (after the softmax or sigmoid layer) in \(^{p}\). Interpolating IoU values from \(\{0,1\}^{p}\) to \(^{p}\) has primarily followed two routes. One involves relaxing set counting as norm functions, as seen in the soft Jaccard loss [46; 49], the soft Dice loss , the soft Tversky loss  and the focal Tversky loss . The other capitalizes on the fact that IoU is submodular , allowing for the application of the convex Lovasz extension of submodular functions. For instance, the Lovasz-Softmax loss , the Lovasz hinge loss  and the PixIoU loss . Nevertheless, as IoU values are extended for predictions from \(\{0,1\}^{p}\) to \(^{p}\), the fact that labels can also fall within \(^{p}\) is often overlooked. As a result, these IoU losses are incompatible with soft labels.

Besides semantic segmentation, IoU is adopted across a wide spectrum of fields. Due to its discrete nature, its probabilistic extensions  have found use in object detection , medical imaging  and information retrieval [26; 43].

## 6 Discussion

### How to tune the hyper-parameters of JML?

In this section, we delve into some critical hyper-parameters of JML. For a comprehensive list, please refer to the accompanying code.

\(}^{}\)/\(}^{}\)/\(}^{}\) (default: 1.0/0.0/0.0): the weight of the loss to optimize \(}^{}\)/\(}^{}\)/\(}^{}\). The appropriate choice is mainly dependent on the targeted evaluation metrics. Given that the prevailing metric is mIoU\({}^{}\) (per-dataset mIoU), we recommend to set mIoU\({}^{}\) to 1.0 and mIoU\({}^{}\)/\(}^{}\) to 0.0. However, it is imperative to acknowledge the inherent trade-offs when using JML to optimize different metrics .

**alpha/beta** (default: 1.0/1.0): the coefficient of false positives/negatives in the Tversky loss. For instance, when alpha and beta are both set to 1.0, the configuration corresponds to JML. Conversely, an alpha and beta value of 0.5 each leads to DML . When the evaluation metric is IoU, JML is advised, while it is the Dice score, DML is more appropriate. The general Tversky loss is useful when separate weights are required for false positives/negatives.

**active_classes_mode** (default: PRESENT for hard labels and ALL for soft labels): the mode to compute active classes. With hard labels, it is suggested to use PRESENT, since the loss aligns more effectively with the evaluation metric, especially when (i) the dataset contains a large number of classes (e.g. ADE20K), and/or (ii) evaluated with fine-grained mIoUs . With soft labels, the optimal choice varies based on specific applications (see Table 7).

   Mores & T1 & T2 & T3 \\  T REC \(\) & 4.20 & 3.47 & 3.24 \\ TREC \(\) & 33.17 & 20.73 & 20.42 \\ Tanol\(\) & 79.09 & 78.75 & 79.82 \\ SEC \(\) & \(4.76 0.09\) & \(4.01 0.07\) & \(3.94 0.07\) \\ SEC \(\) & \(33.86 0.10\) & \(22.21 0.06\) & \(22.15 0.05\) \\ S mIoU \(\) & \(75.05 0.31\) & \(75.43 0.35\) & \(75.89 0.29\) \\   

Table 8: Comparing 3 teachers of different accuracy and calibration on PASCAL VOC (%). Prefix T stands for the teacher and prefix S the student. Red: the best in a row.

### How to use JML?

**Combine JML with CE.** Our findings suggest that incorporating CE, particularly during the initial stage of training, can expedite convergence (as illustrated in Figure 1). Consequently, relying solely on JML is not recommended. For a discussion on the balancing weights, please consult Section 4.1.

**Tune the hyper-parameters of JML.** We have endeavored to minimize the necessity of extensive tuning. In the majority of scenarios, default hyper-parameter values (as illustrated above) should suffice. However, some situations might benefit from further refinement. In particular, pay attention to active_classes_mode when dealing with soft labels.

**Tune the hyper-parameters of training settings.** Current training recipes have been heavily optimized for CE. Although we find JML generally aligns with these hyper-parameters, due diligence is required. Notably, one of the advantages of JML is its ability to speed up convergence in the later training phase (as depicted in Figure 1). As a consequence, training with a combination of CE and JML often converges in considerably fewer epochs compared to training with CE alone. Please refer to  for a comparison of the number of epochs with those in MMSegmentation.

**Be careful with distributed training.** In our concurrent work , we observe that the loss presents trade-offs when optimizing different mIoU variants. If the per-GPU batch size is small (which is often the case) and the loss is computed on each GPU independently, the loss may inadvertently optimize for mIoU\({}^{}\) (per-image mIoU). This can lead to suboptimal outcomes when evaluated with mIoU\({}^{}\) (per-dataset mIoU).

**Be careful with extremely class-imbalance.** Aligned with mIoU, JML is designed to compute the loss on a per-class basis and subsequently average these individual losses. In preliminary experiments on some medical datasets, we identified instances of severe class imbalances. In such scenarios, the class-wise loss may inadvertently amplify the significance of underrepresented classes, potentially disturb the training process. To address this, one might consider (i) oversampling the minority classes, and/or (ii) adopting a class-agnostic loss computation, where the intersection and union are calculated over all pixels.

## 7 Limitation

In both this study and our subsequent work , the focus is to extend the losses in the field of image segmentation. It would be intriguing to apply these losses to other tasks, such as long-tailed classification . Moreover, although we adopt these losses in the label space, they present potential in quantifying the similarity between two feature vectors , potentially serving as an alternative to the \(L^{p}\) norm or cosine similarity.

## 8 Conclusion

This paper is driven by the observation that current IoU losses fall short when dealing with soft labels, which substantially limits their adaptability to crucial training techniques. To address this limitation, we introduce the Jaccard metric losses (JMLs). While these losses are identical to the soft Jaccard loss in a conventional hard-label setting, they offer full compatibility with soft labels.

Our results demonstrate that integrating JMLs with label smoothing, knowledge distillation, and semi-supervised learning leads to notable improvements in both accuracy and calibration. This is consistent across a spectrum of datasets and network architectures, encompassing classic CNNs as well as recent vision transformers. Remarkably, the proposed methods, which are simple and solely rely on soft labels, surpass state-of-the-art segmentation knowledge distillation and semi-supervised learning techniques by a significant margin.

In our follow-up study , we delve into the extensions of various other losses, including the soft Dice loss, soft Tversky loss, and focal Tversky loss. Given their equivalence to their original counterparts in a standard hard-label context and their enhanced compatibility with soft labels, we recommend to replace the existing implementations with ours.