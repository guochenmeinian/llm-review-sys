# Doubly Mild Generalization for Offline Reinforcement Learning

Yixiu Mao\({}^{1}\), Qi Wang\({}^{1}\), Yun Qu\({}^{1}\), Yuhang Jiang\({}^{1}\), Xiangyang Ji\({}^{1}\)

\({}^{1}\)Department of Automation, Tsinghua University

myx21@mails.tsinghua.edu.cn, xyji@tsinghua.edu.cn

###### Abstract

Offline Reinforcement Learning (RL) suffers from the extrapolation error and value overestimation. From a generalization perspective, this issue can be attributed to the over-generalization of value functions or policies towards out-of-distribution (OOD) actions. Significant efforts have been devoted to mitigating such generalization, and recent in-sample learning approaches have further succeeded in entirely eschewing it. Nevertheless, we show that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. To appropriately exploit generalization in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild action generalization and (ii) mild generalization propagation. The former refers to selecting actions in a close neighborhood of the dataset to maximize the Q values. Even so, the potential erroneous generalization can still be propagated, accumulated, and exacerbated by bootstrapping. In light of this, the latter concept is introduced to mitigate the generalization propagation without impeding the propagation of RL learning signals. Theoretically, DMG guarantees better performance than the in-sample optimal policy in the oracle generalization scenario. Even under worst-case generalization, DMG can still control value overestimation at a certain level and lower bound the performance. Empirically, DMG achieves state-of-the-art performance across Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning and attains strong online fine-tuning performance.

## 1 Introduction

Reinforcement learning (RL) aims to solve sequential decision-making problems and has garnered significant attention in recent years . However, its practical applications encounter several challenges, such as risky exploration attempts  and time-consuming data collection phases . Offline RL emerges as a promising paradigm to alleviate these challenges by learning without interaction with the environment . It eliminates the need for unsafe exploration and facilitates the utilization of pre-existing large-scale datasets .

However, offline RL suffers from the out-of-distribution (OOD) issue and extrapolation error . From a generalization perspective, this well-known challenge can be regarded as a consequence of the over-generalization of value functions or policies towards OOD actions . Specifically, the potential value over-estimation at OOD actions caused by intricate generalization is often improperly captured by the max operation . This over-estimation will propagate to values of in-distribution samples through Bellman backups and further spread to values of OOD ones via generalization. In mitigating value overestimation caused by OOD actions, substantial efforts have been dedicated  and recent advancements in in-sample learning have successfully formulated the Bellman target solely with the actions present in the dataset  and extracted policies by weightedbehavior cloning [57; 80]. As a result, these algorithms completely eschew generalization and avoid the extrapolation error. Despite simplicity, this way can not take advantage of the generalization ability of neural networks, which could be beneficial for performance improvement. Until now, how to appropriately exploit generalization in offline RL remains a lasting issue.

This work demonstrates that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. For appropriate exploitation of mild generalization, we propose Doubly Mild Generalization (DMG) for offline RL, comprising (i) mild action generalization and (ii) mild generalization propagation. The former concept refers to choosing actions in the vicinity of the dataset to maximize the Q values. However, the mere utilization of mild action generalization still falls short in adequately circumventing potential erroneous generalization, which can be propagated, accumulated, and exacerbated through the process of bootstrapping. To address this, we propose a novel concept, mild generalization propagation, which involves reducing the generalization propagation while preserving the propagation of RL learning signals. Regarding DMG's implementation, this work presents a simple yet effective scheme. Specifically, we blend the mildly generalized max with the in-sample max in the Bellman target, where the former is achieved by actor-critic learning with regularization towards high-value in-sample actions, and the latter is accomplished using in-sample learning techniques such as expectile regression .

We conduct a thorough theoretical analysis of our approach DMG in both oracle and worst-case generalization scenarios. Under oracle generalization, DMG guarantees better performance than the in-sample optimal policy in the dataset [38; 37]. Even under worst-case generalization, DMG can still upper bound the overestimation of value functions and guarantee to output a safe policy with a performance lower bound. Empirically1, DMG achieves state-of-the-art performance on standard offline RL benchmarks , including Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG can seamlessly transition from offline to online learning and attain superior online fine-tuning performance.

## 2 Preliminaries

Rl.The environment in RL is mostly characterized as a Markov decision process (MDP), which can be represented as a tuple \(=(,,P,R,,d_{0})\), comprising the state space \(\), action space \(\), transition dynamics \(P:()\), reward function \(R:[0,R_{}]\), discount factor \([0,1)\), and initial state distribution \(d_{0}\). The goal of RL is to find a policy \(:()\) that can maximize the expected discounted return, denoted as \(J()\):

\[J()=_{s_{0} d_{0},a_{t}(|s_{t}),s_{t+1} P( |s_{t},a_{t})}[_{t=0}^{}^{t}R(s_{t},a_{t})].\] (1)

For any policy \(\), we define the value function as \(V^{}(s)=_{}[_{t=0}^{}^{t}R(s_{t},a_{t})| s_{0}=s]\) and the state-action value function (\(Q\)-value function) as \(Q^{}(s,a)=_{}[_{t=0}^{}^{t}R(s_{t},a_{t} )|s_{0}=s,a_{0}=a]\).

Offline RL.Distinguished from traditional online RL training, offline RL handles a static dataset of transitions \(=\{(s_{i},a_{i},r_{i},s^{}_{i})\}_{i=0}^{n-1}\) and seeks an optimal policy without any additional data collection [40; 42]. We use \((a|s)\) to denote the empirical behavior policy observed in \(\), which depicts the conditional distributions in the dataset . Ordinary approximate dynamic programming methods minimize temporal difference error, according to the following loss :

\[L_{TD}()=_{(s,a,s^{})}[(Q_{}( s,a)-R(s,a)-_{a^{}}Q_{^{}}(s^{},a^{ }))^{2}],\] (2)

where \(_{}\) is a parameterized policy, \(Q_{}(s,a)\) is a parameterized \(Q\) function, and \(Q_{^{}}(s,a)\) is a target \(Q\) function whose parameters are updated via Polyak averaging .

## 3 Doubly Mild Generalization for Offline RL

This section discusses the strategy to appropriately exploit generalization in offline RL. In Section 3.1, we introduce a formal perspective on how generalization impacts offline RL and discuss the issues of over-generalization and non-generalization. Subsequently, we propose the DMG concept, comprising mild action generalization and mild generalization propagation in Section 3.2. Following this, we conduct a comprehensive analysis of DMG in both oracle generalization (Section 3.3) and worst-case generalization scenarios (Section 3.4). Finally, we present the practical algorithm in Section 3.5.

### Generalization Issues in Offline RL

Offline RL training typically involves a complex interaction between Bellman backup and generalization . Offline RL algorithms vary in backup mechanisms to train the Q function. Here we denote a generic form of Bellman backup as \(_{u}\), where \(u\) is a distribution in the action space.

\[_{u}Q(s,a):=R(s,a)+_{s^{} P(|s,a)} [_{a^{} u(|s^{})}Q(s^{},a^{})]\] (3)

During offline training, this backup is exclusively executed on \((s,a)\), and the values of \((s,a)\) are influenced solely via generalization. A crucial aspect is that \((s^{},a^{})\) in the Bellman target can be absent from the dataset \(\), depending on the choice of \(u\). As a result, Bellman backup and generalization exhibit an intricate interaction: the backups on \((s,a)\) impact the values of \((s,a)\) via generalization; the values of \((s,a)\) participates in the computation of Bellman target, thereby affecting the values of \((s,a)\).

This interaction poses a key challenge in offline RL, value overestimation. The potential overestimation of values of \((s,a)\), induced by intricate generalization, tends to be improperly captured by the max operation, a phenomenon known as maximization bias . This overestimation propagates to values of \((s,a)\) through backups and further extends to values of \((s,a)\) via generalization. This cyclic process consistently amplifies value overestimation, potentially resulting in value divergence. The crux of this detrimental process can be summarized as **over-generalization**.

To address value overestimation, recent advancements in the field have introduced a paradigm known as in-sample learning, which formulates the Bellman target solely with the actions present in the dataset [37; 85; 92; 88; 21]. Its effect is equivalent to choosing \(u\) in \(_{u}\) to be exactly \(\), i.e., the empirical behavior policy observed in the dataset. Following in-sample value learning, policies are extracted from the learned Q functions using weighted behavior cloning [57; 9; 55]. By entirely eschewing generalization in offline RL training, they effectively avoid the extrapolation error , a strategy we term **non-generalization**. However, the ability to generalize is a critical factor contributing to the extensive utilization of neural networks . In this sense, in-sample learning methods seem too conservative without utilizing generalization, particularly when the offline datasets do not cover the optimal actions in large or continuous spaces.

### Doubly Mild Generalization

The following focuses on the appropriate exploitation of generalization in offline RL.

We start by analyzing the generalization effect under the generic backup operator \(_{u}\). We consider a straightforward scenario, where \(Q_{}\) is updated to \(Q_{^{}}\) by one gradient step on a single \((s,a)\) with learning rate \(\). We characterize the resulting generalization effect on any \((s,)^{2}\) as follows.

**Theorem 1** (Informal).: _Under certain continuity conditions, the following equation holds when the learning rate \(\) is sufficiently small and \(\) is sufficiently close to \(a\):_

\[Q_{^{}}(s,)=Q_{}(s,)+C_{1}(_{u}Q_{}(s,)-Q_{}(s,)+C_{2}\|-a\| )+(\|^{}-\|^{2})\] (4)

_where \(C_{1}\) and \(C_{2}\) is a bounded constant._

The formal theorem and all proofs are deferred to Appendix B.

Note that Eq. (4) is the update of the parametric Q function (\(Q_{} Q_{^{}}\)) at state-action pairs \((s,)\), which is exclusively caused by generalization. If \(\) is within a close neighborhood of \(a\), then \(C_{2}\|-a\|\) is small. Moreover, as \(C_{1}\), Eq. (4) approximates an update towards the true objective \(_{u}Q_{}(s,)\), as if \(Q_{}(s,)\) is updated by a true gradient step at \((s,)\). Therefore,Theorem 1 shows that, under certain continuity conditions, Q functions can generalize well and approximate true updates in a close neighborhood of samples in the dataset. This implies that mild generalizations beyond the dataset can be leveraged to potentially pursue better performance. Inspired by Theorem 1, we define a mildly generalized policy \(\) as follows.

**Definition 1** (Mildly generalized policy).: _Policy \(\) is termed a mildly generalized policy if it satisfies_

\[((|s))(( |s)),\ \ \ \ _{a_{1}(|s)}_{a_{2}(|s)}\|a_{1} -a_{2}\|_{a},\] (5)

_where \(\) is the empirical behavior policy observed in the offline dataset._

It means that \(\) has a wider support than \(\) (the dataset), and for any \(a_{1}(|s)\), we can find \(a_{2}(|s)\) (in dataset) such that \(\|a_{1}-a_{2}\|_{a}\). In other words, the generalization of \(\) beyond the dataset is bounded by \(_{a}\) when measured in the action space distance. According to Theorem 1, there is a high chance that \(Q_{}\) can generalize well in this mild generalization area \((a|s)>0\).

However, even in this mild generalization area, it is inevitable that the learned value function will incur some degree of generalization error. The possible erroneous generalization can still be propagated and exacerbated by value bootstrapping as discussed in Section 3.1. To this end, we introduce an additional level of mild generalization, termed mild generalization propagation, and propose a novel Doubly Mildly Generalization (DMG) operator as follows.

**Definition 2**.: _The Doubly Mild Generalization (DMG) operator is defined as_

\[_{}Q(s,a):=R(s,a)+_{s^{} P( |s,a)}[_{a^{}(|s^{})}Q(s ^{},a^{})+(1-)_{a^{}(|s^{ })}Q(s^{},a^{})]\] (6)

_where \(\) is the empirical behavior policy in the dataset and \(\) is a mildly generalized policy._

Note that in typical offline RL algorithms, extrapolation error and value overestimation caused by erroneous generalization are propagated through bootstrapping, and the discount factor of this process is \(\). DMG reduces this discount factor to \(\), mitigating the amplification of value overestimation. On the other hand, in contrast to in-sample methods, DMG allows mild generalization, utilizing the generalization ability of neural networks to seek better performance, as Theorem 1 suggests that value functions are highly likely to generalize well in the mild generalization area.

To summarize, the generalization of DMG is mild in two aspects: (i) **mild action generalization**: based on the mildly generalized policy \(\), which generalizes beyond \(\), DMG selects actions in a close neighborhood of the dataset to maximize the Q values in the first part of the Bellman target; and (ii) **mild generalization propagation**: DMG mitigates the generalization propagation without hindering the propagation of RL learning signals by blending the mildly generalized max with the in-sample max in the Bellman target. This reduces the discount factor through which generalization propagates, mitigating the amplification of value overestimation caused by bootstrapping.

To support the above claims, we provide a comprehensive analysis of DMG in both oracle and worst-case generalization scenarios, with particular emphasis on value estimation and performance.

### Oracle Generalization

This section conducts analyses under the assumption that the learned value functions can achieve oracle generalization in the mild generalization area \((a|s)>0\), formally defined as follows.

**Assumption 1** (Oracle generalization).: _The generalization of learned Q functions in the mild generalization area \((a|s)>0\) reflects the true value updates according to \(_{}\)._

The mild generalization area \((a|s)>0\) may contain some points outside the offline dataset, and \(_{}\) might query Q values of such points. This assumption assumes that the generalization at such points reflects the true value updates according to \(_{}\). The rationale for such an assumption comes from Theorem 1, which characterizes the generalization effect of value functions in the mild generalization area. Now we analyze the dynamic programming properties of the operators \(_{}\) and \(_{}\), where \(_{}\) is the in-sample Q learning operator [37; 88; 21] defined as follows.

**Definition 3**.: _The In-sample Q Learning operator  is defined as_

\[_{}Q(s,a):=R(s,a)+_{s^{} P( |s,a)}[_{a^{}(|s^{})}Q(s^{},a^{ })]\] (7)

_where \(\) is the empirical behavior policy in the dataset._

**Lemma 1**.: \(_{}\) _is a \(\)-contraction operator in the in-sample area \((a|s)>0\) under the \(_{}\) norm._

Following Lemma 1, we denote the fixed point of \(_{}\) as \(Q^{*}_{}\), and its induced policy as \(^{*}_{}\). Here \(Q^{*}_{}\) is known as the in-sample optimal value function , which is the value function of the in-sample optimal policy \(^{*}_{}\). We refer readers to [37; 38; 88] for more discussions on the in-sample optimality.

Now we present the theoretical properties of DMG for comparison.

**Theorem 2** (Contraction).: _Under Assumption 1, \(_{}\) is a \(\)-contraction operator in the mild generalization area \((a|s)>0\) under the \(_{}\) norm. Therefore, by repeatedly applying \(_{}\), any initial Q function can converge to the unique fixed point \(Q^{*}_{}\)._

We denote the induced policy of \(Q^{*}_{}\) as \(^{*}_{}\), whose performance is guaranteed as follows.

**Theorem 3** (Performance).: _Under Assumption 1, the value functions of \(^{*}_{}\) and \(^{*}_{}\) satisfy:_

\[V^{^{*}_{}}(s) V^{^{*}_{}}(s), s .\] (8)

Theorem 3 indicates that the policy learned by DMG can achieve better performance than the in-sample optimal policy under the oracle generalization condition.

### Worst-case Generalization

This section turns to the analyses in the worst-case generalization scenario, where the learned value functions may exhibit poor generalization in the mild generalization area \((a|s)>0\). In other words, this section considers that \(_{}\) is only defined in the in-sample area \((a|s)>0\) and the learned value functions may have any generalization error at other state-action pairs. In this case, we use the notation \(}_{}\) to tell the difference.

We make continuity assumptions about the learned Q function and the transition dynamics.

**Assumption 2** (Lipschitz \(Q\)).: _The learned Q function is \(K_{Q}\)-Lipschitz. \( s\), \( a_{1},a_{2}\), \(|Q(s,a_{1})-Q(s,a_{2})| K_{Q}\|a_{1}-a_{2}\|\)_

**Assumption 3** (Lipschitz \(P\)).: _The transition dynamics \(P\) is \(K_{P}\)-Lipschitz. \( s,s^{}\), \( a_{1},a_{2}\), \(|P(s^{}|s,a_{1})-P(s^{}|s,a_{2})| K_{P}\|a_{1}-a_{2}\|\)_

For Assumption 2, a continuous learned Q function is particularly necessary for analyzing value function generalization and can be relatively easily satisfied using neural networks or linear models . Assumption 3 is also a common assumption in theoretical studies of RL [13; 87; 61].

Now we consider the iteration starting from arbitrary function \(Q^{0}\): \(^{k}_{}=}_{}^{k-1}_{ }\) and \(Q^{k}_{}=_{}Q^{k-1}_{}\), \( k^{+}\). The possible value of \(^{k}_{}\) is bounded by the following results.

**Theorem 4** (Limited overestimation).: _Under Assumption 2, the learned Q function of DMG by iterating \(}_{}\) satisfies the following inequality_

\[Q^{k}_{}(s,a)^{k}_{}(s,a) Q^{k}_{ }(s,a)+K_{Q}}{1-}(1-^{k}),\; s,a,\; k^{+}.\] (9)

Since in-sample training eliminates the extrapolation error [37; 92], \(Q^{k}_{}\) can be considered a relatively accurate estimate . Therefore, Theorem 4 suggests that DMG exhibits limited value overestimation under the worst-case generalization scenario. Moreover, the bound becomes tighter as \(_{a}\) decreases (milder action generalization) and \(\) decreases (milder generalization propagation). This is consistent with our intuitions in Section 3.2.

Finally, we show in Theorem 5 that even under worst-case generalization, DMG guarantees to output a safe policy with a performance lower bound.

**Theorem 5** (Performance lower bound).: _Let \(_{}\) be the learned policy of DMG by iterating \(_{}\), \(^{*}\) be the optimal policy, and \(_{}\) be the inherent performance gap of the in-sample optimal policy \(_{}:=J(^{*})-J(^{*}_{})\). Under Assumptions 2 and 3, for sufficiently small \(_{a}\), we have_

\[J(_{}) J(^{*})-R_{}}{1-} _{a}-_{}.\] (10)

_where \(C\) is a positive constant._

### Practical Algorithm

This section puts DMG into implementation and presents a simple yet effective practical algorithm. The algorithm comprises the following networks: policy \(_{}\), target policy \(_{^{}}\), \(Q\) network \(Q_{}\), target \(Q\) network \(Q_{^{}}\), and \(V\) network \(V_{}\).

Policy learning.Practically, we expect DMG to exhibit a tendency towards mild generalization around good actions in the dataset. To this end, we first consider reshaping the empirical behavior policy \(\) to be skewed towards actions with high advantage values \(^{*}(a|s)(a|s)(A(s,a))\). Then we enforce the proximity between the trained policy and the reshaped behavior policy to constrain the generalization area. We define the generalization set \(_{G}\) as follows.

\[_{G}=\{(^{*}(|s)\|(|s))\}\] (11)

Note that forward KL allows \(\) to select actions outside the support of \(^{*}\), enabling \(_{G}\) to generalize beyond the actions in the dataset. With \(_{G}\) defined, the next step is to compute the maximal \(Q\) within \(_{G}\). To accomplish this, we adopt Actor-Critic style training  for this part.

\[_{}_{s,a_{}(|s)}Q_{}(s, a), s.t.\ _{}_{G}\] (12)

By treating the constraint term as a penalty, we maximize the following objective.

\[_{}_{s,a_{}(|s)}Q_{}(s,a)-_{s}(^{*}(|s)\| _{}(|s))\] (13)

Through straightforward derivations, Eq. (13) is equivalent to the following policy training objective.

\[J_{}()=_{s,a_{}(|s)}Q_{ }(s,a)-_{(s,a)}[((Q_{^{ }}(s,a)-V_{}(s)))_{}(a|s)]\] (14)

where \(\) is an inverse temperature and \(Q_{^{}}(s,a)-V_{}(s)\) computes the advantage function \(A(s,a)\).

Value learning.Now we turn to the implementation of the \(_{}\) operator for training value functions. By introducing the aforementioned policy, we can substitute \(_{a}\) in \(_{}\) with \(_{a}\). Regarding \(_{a}\) in \(_{}\), any in-sample learning techniques can be employed to compute the in-sample maximum . In particular, based on IQL , we perform expectile regression.

\[L_{V}()=_{(s,a)}[L_{2}^{}(Q_{ ^{}}(s,a)-V_{}(s))]\] (15)

where \(L_{2}^{}(u)=|-(u<0)|u^{2}\) and \((0,1)\). For \( 1\), \(V_{}\) can capture the in-sample maximal \(Q\). Finally, we have the following value training loss.

\[L_{Q}()=_{(s,a,s^{})}[(Q_{ }(s,a)-R(s,a)-_{a^{}_{^{}}}Q_{ ^{}}(s^{},a^{})-(1-)V_{}(s^{}) )^{2}]\] (16)

Overall algorithm.Integrating all components, we present our practical algorithm in Algorithm 1.

Discussions and Related Work

Summary of offline RL work from a generalization perspective.As analyzed above, DMG is featured in both mild action generalization and mild generalization propagation. Within the actor-critic framework upon which most offline RL algorithms are built, these two aspects correspond to the policy and value training phases, respectively. Action generalization concerns whether the policy training intentionally selects actions beyond the dataset to maximize Q values, while generalization propagation involves whether value training propagates generalization through bootstrapping. Table 1 presents a clear comparison of offline RL works in this generalization view. The table shows one representative method of each category and we elaborate on others as follows.

Concerning policy learning, AWR , AWAC , CRR , \(10\%\) BC , IQL , and other works such as  extract policies through weighted or filtered behavior cloning, thereby lacking intentional action generalization to maximize Q values beyond the dataset. Typical policy-regularized offline RL methods like TD3BC , BRAC , BEAR , SPOT , and others such as  introduce regularization terms to Q maximization objectives to regularize the trained policy towards the behavior policy and allows mild action generalization. Online RL algorithms like TD3  and SAC  have no constraints and maximize Q values in the entire action space, corresponding to full action generalization. Regarding value training, in-sample learning methods including OneStep RL , IQL , InAC , IAC , \(\)QL , and SQL  completely avoid generalization propagation and accumulation via bootstrapping, whereas typical offline and online RL approaches allow full generalization propagation through bootstrapping. In the proposed approach DMG, generalization is mild in both aspects.

Recently, Ma et al.  have also drawn attention to generalization in offline RL and the issue of over-generalization. They mitigate over-generalization from a representation perspective, differentiating between the representations of in-sample and OOD state-action pairs. Lyu et al.  argue that conventional value penalization like CQL  tends to harm the generalization of value functions and hinder performance improvement. They propose mild value penalization to mitigate the detrimental effects of value penalization on generalization.

Connection to heuristic blending approaches.Our approach also relates to the framework of blending heuristics into bootstrapping . In offline RL, HUBL  blends Monte-Carlo returns into bootstrapping and acts as a data relabeling step, which reduces the degree of bootstrapping and thereby increases its performance. In contrast, DMG blends the in-sample maximal values into the bootstrapping operator. DMG does not reduce the discount for RL learning but reduces the discount for generalization propagation.

For extended discussions on related work, please refer to Appendix A.

## 5 Experiments

In this section, we conduct several experiments to justify the validity of the proposed method DMG. Experimental details and extended results are provided in Appendices C and D, respectively.

### Main Results on Offline RL Benchmarks

Tasks.We evaluate the proposed approach on Gym-MuJoCo locomotion domains and challenging AntMaze domains in D4RL . The latter involves sparse-reward tasks and necessitates "stitching" fragments of suboptimal trajectories traveling undirectedly to find a path to the goal of the maze.

Baselines.Our offline RL baselines include both typical bootstrapping methods and in-sample learning approaches. For the former, we compare to BCQ , BEAR , AWAC , TD3BC ,

    & IQL & AWAC & TD3BC & TD3 & DMG (Ours) \\  Action generalization & _none_ & _none_ & _mild_ & _full_ & _mild_ \\  Generalization propagation & _none_ & _full_ & _full_ & _full_ & _mild_ \\   

Table 1: Comparison of offline RL work from the generalization perspective.

and CQL . For the latter, we compare to BC , OneStep RL , IQL , \(\)QL , and SQL . We also include the sequence-modeling method Decision Transformer (DT) .

Comparison with baselines.Aggregated results are displayed in Table 2. On the Gym locomotion tasks, DMG outperforms prior methods on most tasks and achieves the highest total score. On the much more challenging AntMaze tasks, DMG outperforms all the baselines by a large margin, especially in the most difficult large mazes. For detailed learning curves, please refer to Appendix D.3. According to , we also report the results of DMG over more random seeds in Appendix D.2.

Runtime.We test the runtime of DMG and other baselines on a GeForce RTX 3090. As shown in Appendix D.1, the runtime of DMG is comparable to that of the fastest offline RL algorithm TD3BC.

### Performance Improvement over In-sample Learning Approaches

DMG can be combined with various in-sample learning approaches. Besides IQL , we also apply DMG to two recent state-of-the-art in-sample algorithms, \(\)QL  and SQL . As shown in Table 3 (and Table 2), DMG consistently and substantially improves upon these in-sample methods, particularly on sub-optimal datasets where generalization plays a crucial role in the pursuit of a better policy. This provides compelling empirical evidence that the performance of in-sample methods is largely confined by eschewing generalization beyond the dataset, while DMG effectively exploits generalization, achieving significantly improved performance across tasks.

   Dataset-v2 & BC & BCQ & BEAR & DT & AWAC & OneStep & TD3BC & CQL & IQL & DMG (Ours) \\  halfcheetah-m & 42.0 & 46.6 & 43.0 & 42.6 & 47.9 & 50.4 & 48.3 & 47.0 & 47.4 & **54.9\(\)0.2** \\ hopper-m & 56.2 & 59.4 & 51.8 & 67.6 & 59.8 & 87.5 & 59.3 & 53.0 & 66.2 & **100.6\(\)1.9** \\ walker2d-m & 71.0 & 71.8 & -0.2 & 74.0 & 83.1 & 84.8 & 83.7 & 73.3 & 78.3 & **92.4\(\)2.7** \\ halfcheetah-m-r & 36.4 & 42.2 & 36.3 & 36.6 & 44.8 & 42.7 & 44.6 & 45.5 & 44.2 & **51.4\(\)0.3** \\ hopper-m-r & 21.8 & 60.9 & 52.2 & 82.7 & 69.8 & 98.5 & 60.9 & 88.7 & 94.7 & **101.9\(\)1.4** \\ walker2d-m-r & 24.9 & 57.0 & 7.0 & 66.6 & 78.1 & 61.7 & 81.8 & 81.8 & 73.8 & **89.7\(\)5.0** \\ halfcheetah-m-e & 59.6 & **95.4** & 46.0 & 86.8 & 64.9 & 75.1 & 90.7 & 75.6 & 86.7 & 91.1\(\)4.2 \\ hopper-m-e & 51.7 & 106.9 & 50.6 & 107.6 & 100.1 & 108.6 & 98.0 & 105.6 & 91.5 & **110.4\(\)3.4** \\ walker2d-m-e & 101.2 & 107.7 & 22.1 & 108.1 & 110.0 & 111.3 & 110.1 & 107.9 & 109.6 & **114.4\(\)0.7** \\ halfcheetah-e & 92.9 & 89.9 & 92.7 & 87.7 & 81.7 & 88.2 & **96.7** & **96.3** & **95.0** & **95.9\(\)0.3** \\ hopper-e & **110.9** & 109.0 & 54.6 & 94.2 & **109.5** & 106.9 & 107.8 & 96.5 & **109.4** & **111.5\(\)2.2** \\ walker2d-e & 107.7 & 106.3 & 106.6 & 108.3 & 110.1 & 110.7 & 110.2 & 108.5 & 109.9 & **114.7\(\)0.4** \\ halfcheetah-r & 2.6 & 2.2 & 2.3 & 2.2 & 6.1 & 2.3 & 11.0 & 17.5 & 13.1 & **28.8\(\)1.3** \\ hopper-r & 4.1 & 7.8 & 3.9 & 5.4 & 9.2 & 5.6 & 8.5 & 7.9 & 7.9 & **20.4\(\)10.4** \\ walker2d-r & 1.2 & 4.9 & **12.8** & 2.2 & 0.2 & 6.9 & 1.6 & 5.1 & 5.4 & 4.8\(\)2.2 \\ locomotion total & 784.2 & 968.0 & 581.7 & 972.6 & 975.3 & 1041.2 & 1013.2 & 1010.2 & 1033.1 & **1182.8** \\  antmaze-u & 66.8 & 78.9 & 73.0 & 54.2 & 80.0 & 54.0 & 73.0 & 82.6 & 89.6 & **92.4\(\)1.8** \\ antmaze-u-d & 56.8 & 55.0 & 61.0 & 41.2 & 52.0 & 57.8 & 47.0 & 10.2 & 65.6 & **75.4\(\)8.1** \\ antmaze-m-p & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 59.0 & 76.4 & **80.2\(\)5.1** \\ antmaze-m-d & 0.0 & 0.0 & 8.0 & 0.0 & 0.2 & 0.6 & 0.2 & 46.6 & 72.8 & **77.2\(\)6.1** \\ antmaze-l-p & 0.0 & 6.7 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 16.4 & 42.0 & **55.4\(\)6.2** \\ antmaze-l-d & 0.0 & 2.2 & 0.0 & 0.0 & 0.0 & 0.2 & 0.0 & 3.2 & 46.0 & **58.8\(\)4.5** \\  antmaze total & 123.6 & 142.8 & 142.0 & 95.4 & 132.2 & 112.6 & 120.2 & 218.0 & 392.4 & **439.4** \\   

Table 2: Averaged normalized scores on Gym locomotion and Antmaze tasks over five random seeds. m = medium, m-r = medium-replay, m-e = medium-expert, e = expert, r = random; u = umaze, u-d = umaze-diverse, m-p = medium-play, m-d = medium-diverse, l-p= large-play, l-d = large-diverse.

   Dataset-v2 & \(\)QL (+DMG) & SQL(+DMG) \\  halfcheetah-m & 47.7 \(\)**55.3** & 48.3 \(\)**54.5** \\ hopper-m & 71.1 \(\)**90.1** & 75.5 \(\)**97.7** \\ walker2d-m & 81.5 \(\)**88.7** & 84.2 \(\)**89.8** \\ halfcheetah-m-r & 44.8 \(\)**51.1** & 44.8 \(\)**51.8** \\ hopper-m-r & 97.3 \(\)**102.5** & **101.7 \(\)**101.8** \\ walker2d-m-r & 75.9 \(\)**90.0** & 77.2 \(\)**95.2** \\ halfcheetah-m-e & 89.8 \(\)**92.5** & **94.0 \(\)**93.5** \\ hopper-m-e & 107.1 \(\)**111.1** & **111.8 \(\)**110.4** \\ walker2d-m-e & 1101.1 \(\)**111.3** & **110.0 \(\)**109.6** \\  total & 725.3 \(\)**792.7** & 747.5 \(\)**804.2** \\   

Table 3: DMG combined with various in-sample approaches, showing averaged scores over 5 seeds.

### Ablation Study for Performance and Value Estimation

Mixture coefficient \(\).The mixture coefficient \(\) controls the extent of generalization propagation. We fix \(=0.1\) and vary \(\), presenting the learned Q values and performance on several tasks in Figure 1. As \(\) increases, DMG enables increased generalization propagation through bootstrapping, and the learned Q values become larger and probably diverge. A moderate \(\) (mild generalization propagation) is crucial for achieving strong performance across datasets. Under the same degree of action generalization, mild generalization propagation effectively suppresses value overestimation, facilitating more stable policy learning.

Penalty coefficient \(\).The penalty coefficient \(\) regulates the degree of action generalization. We fix \(=0.25\) and vary \(\). The results are shown in Figure 2. As \(\) decreases, DMG allows broader action generalization beyond the dataset, which results in higher learned values. Regarding performance, a moderate \(\) (mild action generalization) is also crucial for achieving superior performance.

### Online Fine-tuning after Offline RL

Benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning. This is accomplished through a gradual enhancement of both action generalization and generalization propagation. Since IQL  has demonstrated superior online fine-tuning performance compared to previous methods [55; 39] in its paper, we follow the experimental setup of IQL and compare to IQL. We also train online RL algorithm TD3  from scratch for comparison. We use the challenging AntMaze domains , given DMG's already high offline performance in Gym locomotion domains. Results are presented in Table 4. While online training from scratch fails in the challenging sparse reward AntMaze tasks, DMG initialized with offline pretraining succeeds in learning near-optimal policies, outperforming IQL by a significant margin. Please refer to Appendix C.2 for experimental details, and to Appendix D.4 for learning curves.

   Dataset-v2 & TD3 & IQL & DMG (Ours) \\  antmaze-u & 0.0 & \(89.6 96.2\) & \(92.4\) \\ antmaze-u-d & 0.0 & \(65.6 62.2\) & \(75.4\) \\ antmaze-m-p & 0.0 & \(76.4 89.8\) & \(80.2\) \\ antmaze-m-d & 0.0 & \(72.8 90.2\) & \(77.2\) \\ antmaze-l-p & 0.0 & \(42.0 78.6\) & \(55.4\) \\ antmaze-l-d & 0.0 & \(46.0 73.4\) & \(58.8\) \\  antmaze total & 0.0 & \(392.4 490.4\) & \(439.4\) \\   

Table 4: Online fine-tuning results on AntMaze tasks, showing normalized scores of offline training and 1M steps online fine-tuning, averaged over 5 seeds.

Figure 1: Performance and Q values of DMG with varying mixture coefficient \(\) over 5 random seeds. The crosses \(\) mean that the value functions diverge in several seeds. As \(\) increases, DMG enables stronger generalization propagation, resulting in higher and probably divergent learned Q values. Mild generalization propagation plays a crucial role in achieving strong performance.

Figure 2: Performance and Q values of DMG with varying penalty coefficient \(\) over 5 random seeds. As \(\) decreases, DMG allows broader action generalization, leading to larger learned Q values. Mild action generalization is also critical for attaining superior performance.

Conclusion and Limitations

This work scrutinizes offline RL through the lens of generalization and proposes DMG, comprising mild action generalization and mild generalization propagation, to exploit generalization in offline RL appropriately. We theoretically analyze DMG in oracle and worst-case generalization scenarios, and empirically demonstrate its SOTA performance in offline training and online fine-tuning experiments.

While our work contributes valuable insights, it also has limitations. The DMG principle is shown to be effective across most scenarios. However, when the function approximator employed is highly compatible with a specific task setting, the learned value functions may generalize well in the entire action space. In such case, DMG may underperform full generalization methods due to conservatism.