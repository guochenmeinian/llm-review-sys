# Long-range Meta-path Search on Large-scale Heterogeneous Graphs

Chao Li\({}^{1,2,}\), Zijie Guo\({}^{1,3,}\), Qiuting He\({}^{1}\), Kun He\({}^{1,}\)

\({}^{1}\)School of Computer Science and Technology, Huazhong University of Science and Technology

\({}^{2}\) China Mobile Information Technology Co.,Ltd.

\({}^{3}\) School of Computer Science, Fudan University

{chaoli,zijieguo,brooklet60}@hust.edu.cn

The first two authors contribute equally.Corresponding author.

###### Abstract

Utilizing long-range dependency, a concept extensively studied in homogeneous graphs, remains underexplored in heterogeneous graphs, especially on large ones, posing two significant challenges: Reducing computational costs while maximizing effective information utilization in the presence of heterogeneity, and overcoming the over-smoothing issue in graph neural networks. To address this gap, we investigate the importance of different meta-paths and introduce an automatic framework for utilizing long-range dependency on heterogeneous graphs, denoted as _Long-range Meta-path Search through Progressive Sampling_ (LMSPS). Specifically, we develop a search space with all meta-paths related to the target node type. By employing a progressive sampling algorithm, LMSPS dynamically shrinks the search space with hop-independent time complexity. Through a sampling evaluation strategy, LMSPS conducts a specialized and effective meta-path selection, leading to retraining with only effective meta-paths, thus mitigating costs and over-smoothing. Extensive experiments across diverse heterogeneous datasets validate LMSPS's capability in discovering effective long-range meta-paths, surpassing state-of-the-art methods. Our code is available at [https://github.com/JHL-HUST/LMSPS](https://github.com/JHL-HUST/LMSPS).

## 1 Introduction

Heterogeneous graphs are widely used for modeling multiple types of entities and relationships in complex systems by various types of nodes and edges. For instance, the large-scale academic network, OGBN-MAG, contains multiple node types, _i.e._, Paper (P), Author (A), Institution (I), and Field (F), as well as multiple edge types, such as Author \(}\) Paper, \(}\) Paper, Author \(}\), Institution, and Paper \(}\) Field. These elements can be combined to build higher-level semantic relations called meta-paths . For example, APA is a \(2\)-hop meta-path representing the co-author relationship, and PFPAPFP is a \(6\)-hop meta-path related to long-range dependency.

Utilizing long-range dependency is essential for graph representation learning. For homogeneous graphs, many graph neural networks (GNNs) [28; 4; 1; 54] have been developed to gain benefit from long-range dependency. Utilizing long-range dependency is also crucial for heterogeneous graphs. For instance, the Internet movie database (IMDB) contains \(21K\) nodes with only \(87K\) edges. Such sparsity means each node has only a few directly connected neighbors and requires models to enhance the node embedding from long-range neighbors. The main challenge in using long-range dependency on heterogeneous graphs is how to alleviate costs while striving to effectively utilize information in exponentially increased receptive fields, which is much more challenging compared to homogeneous

[MISSING_PAGE_FAIL:2]

**Meta-path**. _A meta-path \(P\) is a composite relation that consists of multiple edge types, \(P c_{1}}}{{}}c_{2} c_{l -1}}}{{}}c_{l}\)\((P=c_{1} c_{l}\) for short), where \(c_{1},,c_{l}\) and \(r_{12},,r_{(l-1)l}\).

A meta-path \(P\) corresponds to multiple meta-path instances in the underlying heterogeneous graph. For example, meta-path APA corresponds to all paths of co-author relationships on the heterogeneous graph. Using meta-paths means selectively aggregating neighbors on the meta-path instances.

## 3 Related Works

**Heterogeneous Graph Neural Networks.** HGNNs are proposed to learn rich and diverse semantic information on heterogeneous graphs. Several HGNNs [29; 23; 34; 36] have involved high-order semantic aggregation. However, their methods are not applied to large-scale datasets due to high costs. Additionally, many HGNNs [56; 21; 48; 23] have implicitly learned meta-paths by attention. However, few work employs the discovered meta-paths to produce final results, let alone generalize them to other HGNNs to demonstrate their effectiveness. For example, GTN  and HGT  only list the discovered meta-paths. HAN , HPN  and MEGNN  validate the importance of discovered meta-paths by experiments not directly associated with the learning task. GraphMSE  is the only work that shows the performance of the discovered meta-paths. However, they are not as effective as the full meta-path set. So, their learned meta-paths are not effective enough.

**Meta-structure Search on Heterogeneous Graphs.** Recently, some works have attempted to utilize neural architecture search (NAS) to discover meta-structures. GEMS  is the first NAS method on heterogeneous graphs, which utilizes an evolutionary algorithm to search meta-graphs for recommendation tasks. DiffMG  searches for meta-graphs by a differentiable algorithm to conduct an efficient search. PMMM  performs a stable search to find meaningful meta-multigraphs. However, meta-path-based HGNNs are mainstream methods [41; 57; 48; 11], while meta-graph-based HGNNs are specialized. So, their searched meta-graphs are extremely difficult to generalize to other HGNNs. RL-HGNN  proposes a reinforcement learning (RL)-based method to find meta-paths. On recommendation tasks, RMS-HRec  also proposes an RL-based meta-path selection strategy to discover meta-paths. Both of them are very time-consuming.

## 4 Motivation of Long-range Meta-path Search

Long-range meta-paths can complete the missing information that can not be obtained from close nodes. Take the meta-path MDMDMK (M\(\)D\(\)M\(\)D\(\)M\(\)K) from IMDB as an example. IMDB includes four different entity types: Movies (M), Directors (D), Keywords (K), and Actors (A). The task is to predict the category of the target movies. MDMDMK is a 5-hop meta-path that is hard for experts to understand and then apply. However, for many movies without keywords, the meta-path M\(\)D\(\)M\(\)D\(\)M\(\)K is important because the target movies can aggregate the keyword information from the movies of co-directors.

In addition, SeHGNN  employs attention mechanisms to fuse all the target-node-related meta-paths and outperforms the existing HGNNs. SeHGNN has an important observation that models with single-layer structures and long meta-paths outperform those with multi-layers and short meta-paths, indicating the advantages of long-range meta-paths. However, because the number of meta-paths increases exponentially with maximum hops, SeHGNN has to use a small maximum hop to save memory and reduce costs. For example, The maximum hop is \(2\) for large-scale datasets OGBN-MAG, which is insufficient, as shown in Table 2 of the experiments. This inspires us to consider whether we can only use effective meta-paths instead of all meta-paths to reduce the consumption of large maximum hops.

We analyze the importance of different meta-paths for SeHGNN on two widely-used real-world datasets DBLP and ACM from HGB . All results are the average of \(10\) times running with different random initialization. As the number of meta-paths exponentially increases with the maximum hop, in exploratory experiments, we set the maximum hop \(l=2\) for ease of illustration. Then the meta-path sets are {A, AP, APA, APT, APV} on DBLP, and {P, PA, PC, PP, PAP, PCP, PPA, PPC, PPP} on ACM.

In each experiment on DBLP, we remove one meta-path and compare the performance with the result of leveraging the full meta-path set to analyze the importance of the removed meta-path. As shownin Figure 1 (a), removing either A or AP or APA or APT has little impact on the final performance. However, removing APV results in severe degradation in performance, demonstrating APV is the critical meta-path on DBLP when \(l=2\). We further retain one meta-path and remove others as shown in Figure 1 (b). The performance of utilizing APV is only slightly degraded compared to the full meta-paths set. Consequently, we obtain the first observation: _A small number of meta-paths provide major contributions._ In each experiment on ACM, we remove some meta-paths to analyze their impact on the final performance. Results in Figure 1 (c) show that the performance of SeHGNN improves after removing a part of meta-paths. For example, after removing PC and PCP, the Micro-F1 scores are improved by \(0.52\%\). So, we can conclude the second observation: _Certain meta-paths can have a negative impact for heterogeneous graphs._ The second observation is reasonable because heterogeneous information is not consistently beneficial for various tasks compared to homogeneous information. It is supported by the fact that various recent HGNNs [8; 27; 52] have removed some edge types to exclude corresponding heterogeneous information during pre-processing based on substantial domain expertise or empirical observations. This observation explains why most HGNNs use a maximum hop of \(2\), _i.e._, it is hard to exclude negative information under larger maximum hops. Additionally, because SeHGNN employs an attention mechanism, the performance degradation indicates the attention mechanism has limitations in dealing with noise.

Although long-range meta-paths outperform short meta-paths , they need a large maximum hop, resulting in exponentially increasing meta-paths. Motivated by the above two observations, unlike existing methods [5; 31; 52; 56], we can employ effective meta-paths instead of the full meta-path set without sacrificing performance. Although the number of meta-paths grows exponentially with the maximum hop, the proportion of effective meta-paths is small, which is similar to the \(80/20\) rule of Pareto principle [40; 10]. To keep efficiency while trading off the performance, we choose a fixed number of meta-paths (like \(30\)) over all datasets. Therefore, the exponential meta-paths can be reduced to a constant. Now the key point is how to find effective meta-paths.

## 5 The Proposed Method

The key of our LMSPS is to utilize a search stage to reduce the exponentially increased meta-paths to a subset effective to the current dataset and task. It can overcome the main challenges of utilizing long-range dependency on heterogeneous graphs. First, utilizing effective meta-paths instead of all meta-paths alleviates computational and memory costs while keeping effective heterogeneous information. Second, each target node only aggregates neighbors on the path instances of effective meta-paths. Because the proportion of effective meta-paths is small and each node has a different neighborhood condition, each target node aggregates different neighbors under the constraints of effective meta-path instances. In this way, the over-smoothing issue can also be overcome.

Then, the main challenge becomes how to discover effective meta-paths, especially long-range ones. Searching for long-range meta-paths has two main challenges: the exponentially increasing issue and the noise issue. We propose a progressive sampling algorithm and a sampling evaluation strategy to respectively overcome the two challenges. Figure 2 illustrates the overall framework of LMSPS, which consists of a super-net in the search stage and a target-net in the training stage.

The super-net aims to automatically discover effective meta-paths for specific datasets or tasks, so the search results should not be affected by specific modules. Based on this consideration, we develop a simple MLP-based instead of transformer-based architecture for meta-path search because the

Figure 1: Analysis of the importance of different meta-paths. (a) illustrates the results after removing a single meta-path on DBLP; (b) shows the performance of utilizing a single meta-path on DBLP; (c) illustrates the performance after removing a part of meta-paths on ACM.

former involves fewer human interventions [45; 16; 3]. In addition, the sampling strategy keeps the parametric modules changing in the search stage, which is important for preventing the search meta-paths from being affected by specific modules. The super-net contains five blocks: neighbor aggregation, feature projection, progressive sampling search, sampling evaluation, and MLP.

### Progressive Sampling Search

Let \(=\{P_{1},,P_{k},,P_{K}\}\) be the initial search space with all the \(K\) target-node-related meta-paths, \(^{c_{i}}\) be the raw feature matrix of all nodes belonging to type \(c_{i}\), and \(}_{c_{i},c_{i}+1}\) be the row-normalized adjacency matrix between node type \(c_{i}\) and \(c_{i+1}\). The neighbor aggregation block follows SeHGNN , which employs an efficient one-time message passing to pre-process an entire heterogeneous graph into regular-shaped tensors for target nodes. Specifically, it uses the multiplication of adjacency matrices to calculate the final contribution weight of each metapath-based neighbor to targets. As shown in Figure 2, the neighbor aggregation process of \(l\)-hop meta-path \(P_{k}=c_{0}c_{1}c_{2} c_{l}\) is:

\[_{k}=^{c_{0}}&l=0\\ }_{c_{0},c_{1}}}_{c_{1},c_{2}}}_{c_{l- 1},c_{l}}^{c_{l}}&l>0, \]

where \(_{k}\) is the feature matrices of meta-path \(P_{k}\), and \(c_{0}\) is the target node type. Then, an MLP-based feature projection block is used to project different feature matrices into the same dimension, namely, \(^{}_{k}=_{k}(_{k})\).

To automatically discover meaningful meta-paths for various datasets or tasks without prior, our search space contains all target-node-related meta-paths, severely challenging the efficiency and effectiveness of the search. To address the efficiency challenge, LMSPS utilizes a progressive sampling algorithm to sample meta-paths in each iteration and progressively shrink the search space.

Specifically, LMSPS assigns one architecture parameter to each meta-path. Let \(=\{_{1},,_{k},,_{K}\}^{K}\) be the architecture parameters of \(K\) meta-paths. We use a Gumbel-softmax [35; 9] over architecture parameters \(_{k}\) to calculate the strength of different meta-paths:

\[q_{k}=+u_{k})/]}{_{j=1}^{ K}[(_{j}+u_{j})/]}, \]

where \(q_{k}\) is the path strength, representing the relative importance of meta path \(P_{k}\). \(u_{k}=-(-(U))\) where \(U(0,1)\), and \(\) is temperature controlling relaxation's extent.

The progressive sampling algorithm uses the path strength to progressively narrow the search space from \(K\) to \(V\) to exclude useless meta-paths. Generally, \(K V\) under a large maximum hop. Let \(_{C}\) be the \(C\)-th largest path strength of \(=\{q_{1},,q_{k},,q_{K}\}\). During the search stage, the search space retains all meta-paths no less than \(_{C}\). The dynamic search space can be formulated as follows:

\[_{C}=\{k|q_{k}_{C}, 1 k K\} \;\;C=(K-V)+V. \]

Here \(C\) is the search space size, \(_{C}\) consists of the indexes of retained meta-paths, and \(\) indicates the rounding symbol. \(\) is a parameter controlling the number of retrained meta-paths and decreases from \(1\) to \(0\) as the epoch increases.

Figure 2: The overall framework of LMSPS. Based on the progressive sampling and sampling evaluation in the search stage, the training stage employs \(M\) effective meta-paths instead of the full \(K\) target-node-related meta-paths. It exhibits aggregation of meta-paths with maximum hop \(2\), _i.e._, \(0\), \(1\), and \(2\)-hop meta-paths. The weight updates of feature projection are not shown for ease of illustration.

As the search stage aims to determine top-\(M\) meta-paths, we sample \(M\) meta-paths from dynamic search space in each iteration. In each iteration, only parameters on the \(M\) activated meta-paths are updated, while others remain unchanged. Therefore, the search cost is relevant to \(M\) instead of \(K\). The forward propagation can be expressed as:

\[=_{k}q^{}_{k}_ {k}(_{k})\ \ =( _{C},M). \]

Here \(q^{}_{k}=[(_{k}+u_{k})/]/_{k }[(_{j}+u_{j})/]\) indicates the path strength of activated meta-paths, and \((_{C},M)\) indicates a set of \(M\) elements chosen randomly from set \(_{C}\) without replacement via a uniform distribution.

The parameter update in the super-net involves a bilevel optimization problem .

\[_{}\ \ _{val}(,^{*}(),)\ \ ^{*}()= _{}\ _{train}(,,). \]

Here \(_{train}\) and \(_{val}\) denote the training and validation loss, respectively. \(\) is the architecture parameters calculating the path strength. \(\) is the network weights in MLP. Following the NAS-related works in the computer vision field , we address this issue by first-order approximation. Specifically, we alternatively freeze architecture parameters \(\) when training \(\) on the training set and freeze \(\) when training \(\) on the validation set.

The progressive sampling strategy can contribute to a more compact search space specifically driven by the current HIN and task, leading to a more effective meta-path discovery. Additionally, it can overcome the deep coupling issue  because of the randomness in each iteration.

### Sampling Evaluation

After the completion of the progressive sampling search, the search space is narrowed from \(K\) to \(V\). Traditional methods in the computer vision field directly derive the final architecture based on the architecture parameters . However, as different meta-paths could be noisy or redundant to each other, top-\(M\) meta-paths are not necessarily the optimal solution when their importance is calculated independently. Based on this consideration, we specially designed a novel sampling evaluation strategy by evaluating the overall performance of each meta-path set. Specifically, using path strength at the end of progressive sampling as the probability, we sample \(M\) meta-paths from the compact search space \(_{V}\) to evaluate their overall performance. The sampling evaluation is repeated \(200\) times to filter out the meta-path set with the lowest validation loss. So, we can select the best meta-path set instead of independent top-\(M\) meta-paths, which is more reasonable. This stage is not time-consuming because the evaluation does not involve weight training. This sampling process can be represented as:

\[=(_{V},M,). \]

Here, \((_{V},M,)\) indicates a set of \(M\) elements chosen from set \(_{V}\) without replacement via discrete probability distribution \(\). \(\) is the set of relative path strength calculated by architecture parameters of the \(V\) meta-paths based on Equation 2. The overall search algorithm and more discussion are shown in Appendix C.

Thereafter, the retained meta-path set is recorded as \(_{M}=_{}\ _{val}(,^{*},^{*})\). The forward propagation of the target-net for representation learning can be formulated as:

\[}=\ _{k_{M}}\ _{k}(_{k}). \]

Here, \(\) denotes the concatenation operation. Unlike existing HGNNs, the architecture of the target-net does not contain neighbor attention and semantic attention. Instead, the parametric modules consist of pure MLPs. The training objective is shown in Appendix A.3.

### Discussion on Differences with Prior Works

As discussed in Section 3, there have been some attempts to find meta-paths automatedly . Three aspects highlight the differences between LMSPS and existing methods. 1) Large-scale dataset: LMSPS is the first HGNN that makes it possible to achieve automated meta-path selection for large-scale heterogeneous graph node property prediction. 2) Long-range dependency: LMSPS is the first HGNN to utilize long-range dependency in large-scale heterogeneous graphs. To achieve the above two goals, LMSPS has addressed two key challenges: (a) alleviating costs while striving to effectively utilize information in exponentially increased receptive fields and (b) overcoming the well-known over-smoothing issue. 3) High generalization: Based on Table 4, the searched meta-paths of LMSPS can be generalized to other HGNNs to boost their performance, which has not been achieved by existing works. To accomplish this objective, LMSPS uses an MLP-based architecture instead of a transformer-based one for meta-path search because the former involves fewer inductive biases [45; 16; 3], i.e., human interventions.

## 6 Experiments and Analysis

This section evaluates the benefits of our method against state-of-the-art models on nine heterogeneous datasets. We aim to answer the following questions: **Q1.** How does LMSPS perform compared with state-of-the-art baselines? **Q2.** Can LMSPS overcome the over-smoothing and noise issues? **Q3.** Are the search algorithm and searched meta-paths effective? **Q4.** Does LMSPS perform better on sparser heterogeneous graphs?

### Datasets and Baselines

We evaluate LMSPS on several representative heterogeneous graph datasets, including DBLP, IMDB, ACM, and Freebase from HGB benchmark , and the large-scale dataset OGBN-MAG from OGB challenge . The statistics of the datasets are listed in Table 7. The details about all datasets, baselines, and experiment settings are recorded in Appendix A and B.

### Performance Analysis

To answer **Q1**, we report the performance of our approaches and baselines in Table 1. _Random_ means the result of replacing our searched meta-paths with \(30\) random meta-paths. We show the average result of 20 random samples. Based on the results, we have the following observations. First, LMSPS outperforms all baselines for different metrics on almost all datasets except Micro-F1 scores on Freebase, sometimes by a significant margin, which validates the power of LMSPS. For example, on the largest dataset OGBN-MAG, LMSPS achieves \(54.83\%\) test accuracy, while the best competitor has \(51.45\%\) test accuracy. Second, all metapath-free methods encounter out-of-memory (OOM) issues when dealing with large datasets, including highly competitive methods, HINormer and SlotGAT, indicating the advantage of employing meta-paths selectively aggregating neighbors on the meta-path instances. Third, although LMSPS is an MLP-based method, the pure MLP method  has the worst performance with only \(26.92\%\) test accuracy on OGBN-MAG, validating the advantages of pre-processing and meta-path search. Finally, LMSPS outperforms _Random_ significantly, further indicating the importance of meta-path search. More comparisons with top method combinations

   Method &  &  &  &  &  \\  MLP  & - & - & - & - & - & - & - & - & - & - & - & - \\ GapsAR  & 9.12\(\) 0.50 & 92.07\(\) 0.50 & 58.85\(\) 0.26 & 62.05\(\) 0.15 & 91.55\(\) 0.74 & 91.41\(\) 0.75 & 46.78\(\) 0.77 & 58.33\(\) 1.57 & 47.37\(\) 0.48 \\ IRAN  & 91.67\(\) 0.49 & 92.05\(\) 0.62 & 57.74\(\) 0.66 & 66.05\(\) 0.58 & 90.93\(\) 0.43 & 90.70\(\) 0.43 & 21.31\(\) 1.68 & 54.71\(\) 1.40 & _OOM_ \\ GTS  & 93.42\(\) 0.59 & 93.97\(\) 0.64 & 60.47\(\) 0.46 & 61.44\(\) 0.15 & 91.14\(\) 0.70 & 92.10\(\) 0.71 & _OOM_ & _OOM_ & _OOM_ \\ RSHN  & 93.34\(\) 0.38 & 93.81\(\) 0.55 & 90.82\(\) 0.31 & 64.22\(\) 1.39 & 30.00\(\) 1.51 & 30.22\(\) 1.54 & _OOM_ & _OOM_ & _OOM_ \\ HetCNN  & 91.24\(\) 0.49 & 92.33\(\) 0.44 & 48.25\(\) 0.51 & 91.16\(\) 0.56 & 35.91\(\) 0.56 & 60.25\(\) 0.00 & _OOM_ & _OOM_ & _OOM_ \\ MAGNN  & 93.28\(\) 0.51 & 93.76\(\) 0.45 & 56.49\(\) 0.23 & 64.67\(\) 1.67 & 90.88\(\) 0.64 & 90.77\(\) 0.65 & _OOM_ & _OOM_ & _OOM_ \\ HetANN & 78.52\(\) 0.22 & 93.66\(\) 1.09 & 40.71\(\) 0.58 & 41.78\(\) 0.48 & 92.04\(\) 0.25 & 89.04\(\) 0.37 & _OOM_ & _OOM_ & _OOM_ \\ GCN  & 90.84\(\) 0.32 & 91.47\(\) 0.34 & 57.88\(\) 1.18 & 64.82\(\) 0.64 & 92.17\(\) 0.24 & 92.12\(\) 0.23 & 27.54\(\) 3.13 & 60.23\(\) 0.92 & _OOM_ \\ GAT  & 93.83\(\) 0.27 & 93.30\(\) 0.39 & 85.94\(\) 1.68 & 64.46\(\) 0.25 & 92.02\(\) 0.54 & 92.19\(\) 0.33 & 40.47\(\) 2.58 & 26.56\(\) 0.80 & _OOM_ \\ Simple-GM  & 94.01\(\) 0.24 & 94.46\(\) 0.22 & 63.53\(\) 1.76 & 63.26\(\) 0.57 & 93.42\(\) 0.44 & 93.53\(\) 0.47 & 42.77\(\) 1.48 & 60.29\(\) 0.43 & _OOM_ \\ BIGT  & 90.10\(\) 0.12 & 93.49\(\) 0.25 & 63.10\(\) 0.19 & 62.20\(\) 0.57 & 91.12\(\) 0.76 & 91.20\(\) 0.76 & 29.28\(\) 2.52 & 05.51\(\) 1.16 & 46.78\(\) 0.42 \\ GraphSNet  & 90.48\(\) 0.14 & 94.41\(\) 0.41 & 67.40\(\) 0.23 & 67.13\(\) 0.37 & 92.18\(\) 0.50 & 92.54\(\) 0.44 & _OOM_ & _OOM_ & _OOM_ \\ DiDMG  & 94.01\(\) 0.37 & 94.40\(\) 0.35 & 58.09\(\) 1.57 & 59.75\(\) 1.28 & 81.66\(\) 2.83 & 80.87\(\) 0.04 & _OOM_ & _OOM_ & _OOM_ \\ Random & 35.37\(\) 0.64 & 93.84\(\) 0.32 & 52.13\(\) 0.47 & 53.83\(\) 0.66 & 90.91\(\) 0.12 & 90.82\(\) 0.33 & 21.12\(\) 2.58 & 37.54\(\) 2.66 & 35.14\(\) 2.38 \\ NAS  & 94.18\(\) 0.47 & 94.61\(\) 0.23 & 63.51\(\) 0.68 & 61.18\(\) 0.76 & 93.36\(\) 0.32 & 93.31\(\) 0.33 & 40.98\(\) 1.77 & 63.16\(\) 2.16 & 50.06\(\) 0.22 \\ space-localGNN  & 94.42\(\) 0.42 & 94.63\(\) 0.57 & 61.57\(\) 0.56 & 93.26\(\) 0.14 & 92.38\(\) 0.10 & 93.85\(\) 0.45 & 92.07\(\) 0.14 & _OOM_ & _OOM_ \\ PMMM  & 94.82\(\) 0.39 & 95.14\(\) 0.22 & 65.81\(\)on OGBN-MAG are shown in Table 10 of Appendix E.2, in which LMSPS also shows the best performance.

### Analysis on Large Maximum Hops

To answer **Q2**, we conduct experiments to compare the performance, memory, and efficiency of LMSPS with the method of HGB benchmark, Simple-HGN , and the best metapath-based method, SeHGNN , on DBLP. Figure 3 (a) shows that LMSPS has consistent performance with the increment of maximum hop. The failure of Simple-HGN demonstrates its attention mechanism can not overcome the over-smoothing issue or eliminate the effects of noise under large hop. Figure 3 (b), (c) illustrate each training epoch's average memory and time costs relative to the maximum hop or layer. We can observe that the consumption of SeHGNN exponentially grows, and the consumption of Simple-HGN linearly increases, which is consistent with their time complexity as listed in Table 8. Meanwhile, LMSPS has almost constant consumption as the maximum hop grows. Figure 3 (c) shows the time cost of LMSPS in the search stage, which also approximates a constant when the number of meta-paths is larger than \(M=30\). More results about efficiency are shown in Figure 4 of Appendix E.1.

We also compare the performance and training time of LMSPS with SeHGNN under different maximum hops on large-scale dataset OGBN-MAG. When the maximum hop \(l=1,2,3\), we utilize the full meta-path set because the number of meta-paths is smaller than \(M=30\). Following the convention [34; 52], we measure the average time consumption of one epoch for each model. As shown in Table 2, the performance of LMSPS keeps increasing as the maximum hop value grows. It indicates that LMSPS can overcome the issues caused by utilizing long-range dependency, _e.g._, over-smoothing and noise. In addition, when the number of meta-paths is larger than \(30\), the training time of LMSPS is stable under different maximum hops.

### Effectiveness of the Search Algorithm and Searched Meta-paths

To answer **Q3**, we first explore the effectiveness of our search algorithm. In our architecture, our meta-paths are replaced by those meta-paths discovered by other methods. DARTS  is the first differentiable search algorithm in neural networks. SPOS  is a classic singe-path differentiable algorithm. DARTS and SPOS aim to search operations, like \(3 3\) convolution, in CNNs. DiffMG  and PMMM  search for meta-graphs instead of meta-paths. We ignore these differences and focus on the algorithms. The derivation strategies of the four methods are unsuitable for discovering

    &  &  &  \\   & & Time & Best Accuracy & Time & Best accuracy \\ 
1 & 4 & 4.5 & 47.18 ± 0.28 & 3.598 & 46.88 ± 0.10 \\
2 & 10 & 6.44 & 51.79 ± 0.24 & 5.631 & 51.91 ± 0.13 \\
3 & 23 & 11.28 & 52.44 ± 0.16 & 10.02 & 52.72 ± 0.24 \\
4 & 50 & OOM & OOM & 14.34 & 53.43 ± 0.18 \\
5 & 107 & OOM & OOM & 14.77 & 53.90 ± 0.19 \\
6 & 226 & OOM & OOM & 14.71 & **54.83 ± 0.20** \\   

Table 2: Experiments on OGBN-MAG to analyze the performance of SeHGNN and LMSPS under different maximum hops. #MP is the number of meta-paths under different maximum hops.

Figure 3: Illustration of (a) performance, (b) memory cost, (c) average training time of Simple-HGN, SeHGNN, and LMSPS relative to the maximum hop or layer on DBLP. The _gray dotted line_ in (a) indicates the number of target-node-related meta-paths under different maximum hops, which is exponential.

    &  &  &  \\   & & Time & Best Accuracy & Time & Best accuracy \\ 
1 & 54 & 41.01 & 45.55 ± 0.31 & 90.64 ± 0.30 \\
6 & 53.33 ± 0.05 & 65.99 ± 0.16 & 90.66 ± 0.30 \\ DARTS & 95.31 ± 0.17 & 66.23 ± 0.14 & 93.45 ± 0.13 & 63.25 ± 0.42 \\ SPOS & 95.41 ± 0.13 & 67.10 ± 0.29 & 93.64 ± 0.37 & 64.02 ± 0.62 \\ PIMM & 95.48 ± 0.27 & 67.49 ± 0.24 & 93.74 ± 0.22 & 64.83 ± 0.46 \\ LMSPS & **95.66 ± 0.20** & **68.70 ± 0.26** & **94.69 ± 0.36** & **66.09 ± 0.51** \\   

Table 3: Experiments to explore the effectiveness of our search algorithm. In our LMSPS, the meta-paths are replaced by those meta-paths discovered by other methods.

[MISSING_PAGE_FAIL:9]

Conclusion

This work presented a novel approach, the Long-range Meta-path Search through Progressive Sampling (LMSPS), to tackle the challenges of leveraging long-range dependencies in large-scale heterogeneous graphs, _i.e._, reducing computational costs while effectively utilizing information and addressing the over-smoothing issue. Based on our two observations, _i.e._, a few meta-paths dominate the performance, and certain meta-paths can have negative impact on performance, LMSPS introduced a progressive sampling search algorithm and a sampling evaluation strategy to automatically identify effective meta-paths, hence reducing the exponentially growing number of meta-paths to a manageable constant. Extensive experiments demonstrated the superiority of LMSPS over existing methods, particularly on sparse heterogeneous graphs that require long-range dependencies. By employing simple MLPs and complex meta-paths, LMSPS offers a novel direction that emphasizes data-dependent semantic relationships rather than relying solely on sophisticated neural architectures. The reproducibility and limitations are discussed in Appendix F and Appendix G, respectively.