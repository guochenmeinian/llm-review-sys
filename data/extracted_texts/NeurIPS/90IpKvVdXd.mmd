# Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs

**Yuval Filmus**

Faculty of Computer Science

Faculty of Mathematics

Technion, Israel

filmus.yuval@gmail.com

&Steve Hanneke

Department of Computer Science

Purdue University, USA

steve.hanneke@gmail.com

&Idan Mehaelel

Faculty of Computer Science

Technion, Israel

idanmehael@gmail.com

&Shay Moran

Faculty of Mathematics

Faculty of Computer Science

Faculty of Data and Decision Sciences

Technion, Israel

Google research, Israel

shaymoran1@gmail.com

###### Abstract

Consider the domain of multiclass classification within the adversarial online setting. What is the price of relying on bandit feedback as opposed to full information? To what extent can an adaptive adversary amplify the loss compared to an oblivious one? To what extent can a randomized learner reduce the loss compared to a deterministic one? We study these questions in the mistake bound model and provide nearly tight answers. We demonstrate that the optimal mistake bound under bandit feedback is at most \(O(k)\) times higher than the optimal mistake bound in the full information case, where \(k\) represents the number of labels. This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners. Moreover, we present nearly optimal bounds of \((k)\) on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the bandit feedback setting. This stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of \(2\). In addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel. Previous results show that this is essentially the smallest it can get. Some of our results are proved via a reduction to _prediction with expert advice_ under bandit feedback, a problem interesting on its own right. For this problem, we provide a randomized algorithm which is nearly optimal in some scenarios.

Introduction

The primary focus of this work is the study of _multiclass online learning_ of hypothesis classes in the _realizable_ setting (in which some hypothesis perfectly labels the input). Online learning is a repeated game between a learner and an adversary. Each round \(t\) in the game proceeds as follows:

1. The adversary sends an instance \(x_{t}\) to the learner.
2. The learner predicts \(_{t}\) (possibly at random).
3. The adversary provides information (feedback) to the learner.

This problem suggests a wide and appealing landscape of possible different setups, each granting or preventing various resources from the learner or the adversary which generates the input. The resources we discuss are the _information_ provided by the adversary, the _adaptivity_ of the adversary, and the _randomness_ of the learner. _Full-information_ feedback means that the learner learns the correct label \(y_{t}\) at the end of every round. _Bandit_ feedback means that the learner only receives an indication whether \(_{t}=y_{t}\) or not. If the learner predicts \(_{t}\) deterministically for every \(t\), then the learner is _deterministic_. See Section A for formal and self-contained definitions.

We are interested in how the optimal mistake bound is affected by granting or preventing each of those resources. Below, we discuss these questions and present our results.

### Main questions and results

We assume that a concept class \(^{}\) is given, where \(\) is a _domain_ of instances and \(\) is a _label set_. Unless stated otherwise, we restrict the learning scenario to the realizable case, in which the input provided by the adversary is consistent with a concept from \(\).

In order to state the results, we define some notation. For a concept class \(\), let \(^{}_{}()\) denote the optimal mistake bound for \(\) achievable with full-information feedback. Let \(^{}_{}()\) denote the same as \(^{}_{}()\), with the additional restriction that the learner is deterministic. In the bandit feedback model, define \(^{}_{}()\) analogously to \(^{}_{}()\). When the learner is allowed to be randomized, the mistake bound in the bandit feedback model might change if the game is played against an oblivious or adaptive adversary (we define these types of adversaries below).1 Therefore, we define the notations \(^{}_{}()\) and \(^{}_{}()\) to denote the optimal mistake bounds of a randomized learner which receives bandit feedback on its predictions, when the adversary is oblivious or adaptive, respectively. Unless stated otherwise, \(O\) and \(\) notations hide universal constants that do not depend on any parameter of the problem.

#### 1.1.1 Information

In the full-information feedback model, the learner learns the correct label at the end of each round, regardless of its prediction. In the bandit feedback model, significantly less information is provided at the end of each round: the adversary only reveals whether the learner's prediction was correct or incorrect. This raises the following natural question studied e.g. by Auer and Long (1999), Daniely and Helbertal (2013), Daniely, Sabato, Ben-David, and Shalev-Shwartz (2015), Long (2020).

What is the price the learner pays for receiving only bandit feedback?

The answer for this question is known for deterministic learners. Auer and Long (1999) proved the upper bound

\[^{}_{}()=O(^{ }_{}()||||)\] (1)

for every concept class \(\). A matching lower bound was given in (Long, 2020; Geneson, 2021), which showed that for every natural \(k 2\) there exists a concept class \(\) with label set of size \(||=k\) such that

\[^{ det}_{ bandit}()=(^{ det}_{  full}()||||).\] (2)

They also find tight guarantees on the constants hidden in the \(O,\) notations.

Finding an analogous result for randomized learners was raised as an open problem in (Daniely and Helbertal, 2013).2 We solve this open problem by proving the upper bound stated in the following theorem.

**Theorem 1.1** (Full-information vs. Bandit-feedback).: _For every concept class \(\) it holds that_

\[^{ adap}_{ bandit}()=O(^{ rand}_{  full}()||).\]

_Furthermore, for every natural \(k 2\) there exists a concept class \(\) with \(||=k\) such that_

\[^{ obl}_{ bandit}()=(^{ rand }_{ full}()||).\]

The lower bound was noted e.g. in (Daniely and Helbertal, 2013). We complement it by proving an upper bound that holds for all classes. We prove Theorem 1.1 in Section D. Note that \(^{ rand}_{ full}()\) is precisely characterized by a combinatorial parameter of \(\) called the _randomized Littlestone dimension_(Filmus, Hanneke, Mehalel, and Moran, 2023) (and characterized up to a multiplicative factor of 2 by the standard _Littlestone dimension_(Littlestone, 1988)).

The techniques developed to prove this upper bound include new bounds for _prediction with expert advice_ with bandit feedback, which is the main technical contribution of this work. These bounds are presented in Section 1.2, and a technical overview of the proof can be found in Section 2.

A generalization to the agnostic setting.We also generalize results in the spirit of Theorem 1.1 to the agnostic case, in which the best hypothesis in class is inconsistent with the input in \(r^{}\) many rounds. Our results do not require that \(r^{}\) (or some bound \(r r^{}\) on it) is given to the learner. However, throughout most of the paper we assume the stronger \(r\)-_realizability_ assumption, in which an upper bound \(r r^{}\) is given to the learner. That is, under \(r\)-realizability assumption the learner knows in advance that some hypothesis will be inconsistent with the feedback in _at most_\(r\) many rounds. We explain in Section G how to remove this assumption using a standard "doubling trick". Bounds for various setups in the agnostic setting are summarized in Table 1.

In the agnostic setting, learning algorithms are often measured by their expected _regret_ (which is the mistake bound minus the number of mistakes made by the best hypothesis). While in this work we measure algorithms by their mistake bound, note that a mistake bound of an algorithm is always at least as large as its regret. Therefore, since our bounds demonstrate no dependence on the number of rounds \(T\), in some cases they provide improvements over the known regret bound \(|^{ det}_{ full}( )}\) of (Daniely and Helbertal, 2013). Specifically, when \(r^{}=O(^{ det}_{ full}())\), our results imply a regret bound of \(O(||^{ det}_{ full}())\), for all \(T\).

#### 1.1.2 Adaptivity

In Section 1.1.1 we showed that randomness is necessary for obtaining optimal bounds on the cost of bandit feedback. However, within the setup of a randomized learner which receives bandit feedback, there are (at least) two types of adversaries to consider: an oblivious adversary which must decide on the entire input in advance, and an adaptive adversary that can decide on the input on the fly. This raises the following natural question.

Within the bandit feedback model, what is the price the learner pays for playing against an adaptive adversary?

We solve this question up to logarithmic factors. However, our nearly tight lower bound uses _pattern classes_, which are a generalization of concept classes. In more detail, a pattern class is a set of patterns \(p()^{}\)_ which is downwards closed (i.e. closed under sub-patterns). Proving the lower bound in the theorem below using only concept classes or showing that this is not possible is an interesting and main problem left open by this work.

**Theorem 1.2** (Oblivious vs. Adaptive Adversaries).: _For every concept class \(\) it holds that_

\[^{}_{}()=O(^{ }_{}()||| |).\]

_Furthermore, for every natural \(k 2\) there exists a pattern class \(\) with \(||=k\) and so that_

\[^{}_{}()=(^{}_{}()||).\]

The upper bound is an immediate corollary of the upper bound (1). We prove the lower bound in Section E. The proof idea of the lower bound is to consider the classic adversarial \(||\)-armed bandit problem of Auer, Cesa-Bianchi, Freund, and Schapire (2002) with an \(r\)-realizability assumption. This setting can be simulated using a pattern class, but not using a concept class. For this problem, an adaptive adversary can force a mistake bound of \((r||)\), while the best an oblivious adversary can do is \(O(r+||)\).

**Remark 1.3** (Concept classes vs. Pattern classes).: _Pattern classes are a more expressive generalization of concept classes (see a formal definition in Section A). Most desirably, we would like to prove upper bounds for all pattern classes, accompanied with tight lower bounds that hold for hard concept classes. We manage to do so in all of our results (for the sake of simplicity, we omitted it from the theorem statements), except for Theorem 1.2, in which the upper bound does hold for all pattern classes, but the lower bound holds only for hard pattern classes._

#### 1.1.3 Randomness

In Section 1.1.2 we discussed two different adversarial models within the setup of randomized learners. However, what happens if the learner cannot use randomness? A folklore result on the full-information feedback model states that for every concept class \(\) with \(||=2\) it holds that

\[^{}_{}()^{ }_{}() 2^{}_{ }(),\] (3)

and that there are classes attaining each equality. Extending this to \(||>2\) is straightforward by noting the following randomized-to-deterministic full-information algorithm conversion: If \(A\) is a randomized algorithm, then the mistake bound of the deterministic algorithm \(A^{}\) who always predicts the most probable prediction of \(A\) is at most twice the mistake bound of \(A\). Indeed, whenever \(A^{}\) makes a mistake, \(A\) makes a mistake with probability at least \(1/2\). This raises the following natural question, also asked in Daniely and Helbertal (2013).

  LearnerAdversary & Oblivious & Adaptive \\  Randomized & \(O||(^{}_{}()+ r^{})\) & \(||(^{}_{}() +r^{})\) \\ \(||^{}_{}( )+r^{}\) & \(O||||(^{}_{}()+r^{})\) \\ Deterministic & \(||(^{}_{}( )||+r^{})\) \\  

Table 1: Worst-case mistake bounds for learning concept classes with bandit feedback in the agnostic setting, in which the best concept in class is inconsistent with the feedback in \(r^{}\) many rounds. No prior knowledge on \(r^{}\) is required. The bounds for the randomized setup are obtained by applying Theorem D.3 to the upper bound in Theorem D.1 and to the lower bounds in Lemma C.11 and in (Daniely and Helbertal, 2013). The bounds for the deterministic setup are obtained by applying the same theorem to the upper bound of Auer and Long (1999), and to the lower bounds of Lemma C.5 and Long (2020). The same (up to constant) deterministic bounds were proved independently in (Geneson and Tang, 2024) (however, their bounds are in terms of a given bound \(r r^{}\)).

We resolve this question up to logarithmic factors.

**Theorem 1.4** (Randomized vs. Deterministic).: _For every concept class \(\) it holds that_

\[^{}_{}()=O(^{ }_{}()||| |).\]

_Furthermore, for every natural \(k 2\) there exists a concept class \(\) with \(||=k\) such that_

\[^{}_{}()=(^{}_{}()||).\]

The upper bound is an immediate corollary of the upper bound (1). The proof idea of the lower bound is to construct a class which is hard for a deterministic learner, but becomes easy if the learner may use randomness. Concretely, we prove the following result from which Theorem 1.4 follows, in Section F.

**Theorem 1.5**.: _For every \(d 1\) and \(k 2\) there exists a concept class \(^{}\) with \(=\{0,1,,k\}\) such that_

1. \(^{}_{}()=d+1.\)__
2. \(^{}_{}()=(d k).\)__
3. \(^{}_{}()=(d+k).\)__

The lower bound in Theorem 1.4 has a significant consequence on the problem of finding a combinatorial dimension that quantifies the optimal mistake bound of randomized learners in the bandit feedback model. In the full-information model, the optimal deterministic mistake bound, which is captured precisely by the combinatorial _Littlestone dimension_(Littlestone, 1988), quantifies the optimal mistake bound of randomized learners as well, as demonstrated in (3). In (Daniely, Sabato, Ben-David, and Shalev-Shwartz, 2015), a new combinatorial dimension, coined the _Bandit-Littlestone dimension_, is introduced and proved to capture the exact optimal mistake bound of deterministic learners within the bandit feedback model. In (Daniely and Helbertal, 2013), some hope is expressed for this dimension to quantify the mistake bound of randomized learners as well, similarly to the case of full-information feedback. However, our lower bound shows that this is not the case. As we show in Section F, the classes used in the lower bound may be chosen such that \(^{}_{}()=O(||)\), and thus \(^{}_{}()\) is only roughly \(^{}_{}()}\). On the other hand, the upper bound in Theorem 1.4 together with the bound \(^{}_{}()|-1}{2}\) (e.g. by Daniely and Helbertal (2013)) shows that \(^{}_{}()}\) is roughly the smallest that \(^{}_{}()\) can get.

### Bounds for _prediction with expert advice_

A main technical result proved in this work, which is interesting in its own right, is a nearly optimal randomized mistake bound for the problem of _prediction with expert advice_ in the \(r\)-realizable setting. In this problem, \(n\) experts make deterministic predictions in every round, and it is promised that the best expert is inconsistent with the feedback for at most \(r\) many times throughout the entire game. As in Section 1.1.1, the knowledge of \(r\) is actually not required, and \(r\) can be replaced with the actual number of inconsistencies of the best expert, \(r^{}\). In Section G, we explain how to remove the assumption that \(r\) is given to the learner.

The learner should aggregate the experts' predictions to make their own (possibly randomized) predictions, while minimizing the expected number of mistakes made. This problem was extensively studied in the binary (\(||=2\)) setting, starting with the seminal works of Vovk (1990), Littlestone and Warmuth (1994) who showed that the optimal mistake bound when \(||=2\) is \(( n+r)\) for both randomized and deterministic learners. Later, Cesa-Bianchi, Freund, Helmbold, and Warmuth (1996) proved fine-grained bounds for deterministic learners. Cesa-Bianchi, Freund, Haussler, Helmbold, Schapire, and Warmuth (1997), Abernethy, Langford, and Warmuth (2006), Filmus,Hanneke, Mehalel, and Moran (2023) further refined and improved randomized mistake bounds. Mukherjee and Schapire (2010) studied a variation where the experts are randomized and the learner is deterministic. Similarly to this paper, Branzei and Peres (2019) studied the multiclass scenario, but with full-information-feedback, which is substantially easier to the learner.

In this work, we consider this problem in the bandit feedback model. Most previous works on prediction with expert advice under bandit feedback studied the best achievable _regret_, obtaining results that depend on the number of rounds \(T\), a dependence from which we seek to avoid in this work (see Section 3.4 for more details).

The main tool used to prove Theorem 1.1 is the following optimal (up to constant factors) bound on \(^{}_{}(n,k,r)\), which is the optimal mistake bound achievable by a randomized learner which plays against an adaptive adversary that provides bandit feedback, when there are \(k 2\) many labels, \(n k\) many experts, and the best expert is inconsistent with the feedback for at most \(r 0\) many times.

**Theorem 1.6**.: _For every \(n k 2\) and \(r 0\) it holds that_

\[^{}_{}(n,k,r)=(k(_{k}n+r)).\]

This bound generalizes the result \(^{}_{}(n,2,r)=( n+r)\) mentioned above.3 In the case where the adversary is oblivious, we prove the inferior lower bound

\[^{}_{}(n,k,r)=(k_{k}n+r)\]

which is tight as long as \(r=O(_{k}n)\). Proving tight bounds against an oblivious adversary for all values of \(r\) remains open.

We also consider this problem in the deterministic (learner) setting, for the sake of completeness (we do not use the deterministic bound to prove any other results). We prove all bounds in Section C, and summarize them in Table 2. Similarly to the results in Section 1.1.1, since our mistake bounds demonstrate no dependence on the number of rounds \(T\), they improve over the known regret bound \(O\) of Auer, Cesa-Bianchi, Freund, and Schapire (2002) whenever \(r=O( n)\).

## 2 Technical overview

In this section, we explain the idea behind the proof of the upper bound \(^{}_{}()=O(^{ }_{}()||)\) in Theorem 1.1, which is our main technical contribution.

There are two main ingredients used in the proof of Theorem 1.1:

1. A reduction from the problem of learning a concept class \(\) to an instance of _prediction with expert advice_ with \(||^{^{}_{}()}\) many experts, \(||\) many labels, and where the best expert is inconsistent with the feedback for at most \(^{}_{}()\) many rounds.

  LearnerAdversary & Oblivious & Adaptive \\  Randomized & \(O(k(_{k}n+r^{}))\) & \((k(_{k}n+r^{}))\) \\  & \((k_{k}n+r^{})\) & \\  Deterministic & \((k((n/k)+r^{}+1))\) & \\  

Table 2: Mistake bounds for _prediction with expert advice_. The size of the label set is \(k 2\), there are \(n k\) experts, and the best expert is inconsistent with the feedback for \(r^{}\) many times. No prior knowledge on \(r^{}\) is required. The randomized bounds are due to Theorem C.7 and Lemmas C.10 and C.11. The deterministic bounds are stated in Theorem C.1.

2. The upper bound for _prediction with expert advice_ stated in Theorem 1.6.

Indeed, having both items, we take the upper bound of item (2) with the parameters specified in item (1), obtaining

\[^{}_{}|^{ ^{}_{}()},||, ^{}_{}()=O| |^{}_{}() .\]

By item (1), this bound holds also for the problem of learning the concept class \(\). Since \(^{}_{}() 2^{ }_{}()\), we can replace \(^{}_{}()\) with \(^{}_{}()\). It remains to sketch the ideas behind the proofs of items (1) and (2), which we do in the following subsections. Of the two items, the proof of Item (2) is the main technical novelty.

### Proof idea of Item (1)

In the problem of _prediction with expert advice_, the expert predictions are generated in a completely adversarial fashion. That is, no assumptions are made on the way those predictions are generated. Therefore, any upper bound for _prediction with expert advice_ holds in particular for the case where the expert predictions are determined by an algorithm chosen by the learner. We can exploit this property to reduce the problem of learning a concept class \(\) to an instance of _prediction with expert advice_ with \(||^{^{}_{}()}\) many experts, \(||\) many labels, and where the best expert is inconsistent with the feedback for at most \(^{}_{}()\) many rounds. The idea, described below, is inspired by Long (2020), Hanneke, Livni, and Moran (2021).

Let \(A\) be an optimal deterministic algorithm for learning \(\) given full-information feedback. We run in parallel several copies of \(A\) arranged in tree form. These copies will function as the experts fed into an optimal algorithm for _prediction with expert advice_.

Initially, there is a single copy of \(A\). At every round, for each copy of \(A\), if its prediction is consistent with the bandit feedback, we do nothing. Otherwise, if the copy is at depth \(D<^{}_{}()\), we split it into \(||\) different copies at depth \(D+1\), each "guessing" a different full-information feedback for the problematic example; if the copy is at depth \(D\), we do nothing.

Since the tree has depth at most \(^{}_{}()\), there are at most \(||^{^{}_{}()}\) many copies of \(A\). The copy of \(A\) which always guessed correctly corresponds to an expert whose predictions are inconsistent with the feedback for at most \(^{}_{}()\) many rounds.

A formal statement (and proof) of this reduction can be found in Proposition D.2.

### Proof idea of Item (2)

In the context of classification problems, the realizable case in which some concept from the learned class accurately explains the correct classification is often simpler than the agnostic case. Therefore, for the sake of understanding the proof of the upper bound on \(^{}_{}(n,k,r)\) stated in Theorem 1.6, we first outline a proof for the upper bound on \(^{}_{}(n,k):=^{}_{}(n,k,0)\). This proof has the same flavor of the proof for general \(r\), but is simpler. We then explain how to adapt the proof for general \(r\).

When \(r=0\), all _living_ experts (that is, experts which have been consistent with the feedback so far) are identical: every expert which is inconsistent with the feedback in some round is immediately eliminated, and its predictions need not be taken into account any longer. By applying the law of total expectation, the optimal mistake bound can thus be described by the following optimization problem. Fix \(k 2\), and for every \(n 1\), let \(V(n)=^{}_{}(n,k)\). We have

\[V(n)=_{}_{}_{y}\!\![_{y}V( _{y}n)+_{y^{} y}_{y^{}}(1+V((1-_{y^{}} )n))]\!,\] (4)

where \(^{k}\) is a \(k\)-ary vector whose \(y\)'th entry specifies the fraction of living experts predicting \(y\); \(\) is the distribution used by the learner to draw the prediction; and \(y\) is the correct label chosen by the adversary.

Since all living experts are identical, the natural intuition suggests that the optimal choice of \(\) is to let every entry in it be \(1/k\), in which case the optimal choice of \(\) would be the uniform distribution. In this case, it does not matter which \(y\) the adversary chooses as the correct label. Applying this intuition to (4) results in the recurrence relation

\[V(n) V(n/k)/k+(1-1/k)(1+V((1-1/k)n)).\] (5)

Solving this recurrence relation gives \(V(n) k_{b(k)}n\), where \(b(k)=}{(k-1)^{k-1}}\), which implies the statement in Theorem 1.6 since \(b(k) k\). It remains to show that the intuition that led us from (4) to (5) is indeed correct. A natural approach would be to directly prove that \(V(n)\), as defined in (4), satisfies \(V(n) k_{b(k)}n\) by induction on \(n\). However, note that (4) contains \(k\) different recursive calls, so an inductive proof seems complicated. To overcome this, we use minimax duality, which significantly simplifies (4), as we now explain.

Observe that once \(\) is fixed, each round of the game is a zero-sum game between two randomized parties.4 Therefore, we can apply von Neumann's minimax theorem and obtain a _dual_ game which is equivalent to the _primal_ (original) game in terms of its optimal mistake bound.5 In the dual game, the adversary first chooses a distribution over the labels from which the correct label is drawn, and then the learner chooses its prediction. When the dual game is considered, the definition of \(V(n)\) from (4) becomes

\[V(n)=_{,}_{y}[_{y}V(_{y}n)+(1- _{y})(1+V((1-_{y})n))].\] (6)

The notation is the same as in (4), except that now \(\) is the distribution used by the adversary to draw the correct label, and \(y\) is the learner's prediction. Eq. (6) contains only two recursive calls, making an inductive proof much easier (but still quite technical).

We now explain how to adapt this approach to work for general \(r\), when \(r\) is given. In Section G, we explain how to remove the assumption that \(r\) is given to the learner. When \(r>0\), there is an inherent difference between the experts: different experts have been inconsistent with the feedback for a different number of rounds, and so can afford a different number of future inconsistencies. Therefore, we need some mechanism that differentiates between experts with different inconsistency budgets.

Inspired by _weighted prediction_ techniques (See [Cesa-Bianchi and Lugosi, 2006, Section 2.1] and bibliographic remarks of Section 2), we rely on the fact that experts with higher budgets have higher _potential_ to damage the learner, so we choose an _expert potential function_ that matches the potential of an expert to damage the learner. As such, the potential should be an increasing function of the budget. Choosing the potential as a function of the budget is a known method (see, e.g., [Cesa-Bianchi and Lugosi, 2006, Corollary 2.4]. We use exponential potentials and give an expert with budget \(i\) (corresponding to \(i\) more allowed inconsistencies) a potential of \(k^{2i}\). We now employ roughly the same argument outlined above for the realizable case. The main difference is that \(V()\) depends on the total potential rather than on the number of living experts. As a result, the initial number of living experts \(n\) is replaced with the initial total potential \(n k^{2r}\), which gives

\[V(n k^{2r}) k_{k}(n k^{2r})=k_{k}n+2kr,\]

as stated in Theorem 1.6.

## 3 Related work

We outline connections between previous work to the problems in the focus of this work: the role of various resources in realizable multiclass online learning of concept classes, and _prediction with expert advice_ with bandit feedback.

### Information

The role of information in learning concept classes was previously studied in . To the best of our knowledge,  was the first to show that \(^{}_{}()=O(^{ }_{}()||||)\) for every class \(\). Long , Geneson  improved the constant in the upper bound of Auer and Long , and showed that it is in fact tight up to a \(1+o(1)\) factor by finding a sequence of concept classes demonstrating a matching separation between \(^{}_{}()\) and \(^{}_{}()\). Our work proves an analogous result for randomized learners (Theorem 1.1), with the exception that we do not identify the exact leading constant in the optimal mistake bound.

The agnostic case was studied in , which proved the upper bound \(|^{}_{}()}\) on the optimal regret, where \(T\) is the horizon, and showed that the upper bound is best-possible up to logarithmic factors. The PAC learning setting was studied in , which showed that the price of bandit feedback is \((||)\) in this setting as well.

### Adaptivity

In the setting of full-information feedback, adaptive and oblivious adversaries are essentially equivalent [12, Lemma 4.1]. Indeed, full-information feedback, in its essence, implies that the feedback never depends on the prediction drawn by the learner in a specific execution of the game. In the bandit feedback setting, to the best of our knowledge, the existing literature on adaptive adversaries focuses on the agnostic setting and analyze different notions of _regret_. Notable examples are .

### Randomness

In the full-information feedback model, the _Littlestone dimension_ captures \(^{}_{}()\) precisely, and \(^{}_{}()\) up to a multiplicative factor of \(2\). The paper  suggests a new combinatorial dimension, the _Bandit Littlestone dimension_, and proves that it captures \(^{}_{}()\) precisely. The paper  shows that the Bandit Littlestone dimension qualitatively characterizes learnability also in the agnostic and randomized setting, even when \(||=\).

The papers  ask whether the Bandit Littlestone dimension is a good quantitative proxy for \(^{}_{}()\). Theorem 1.4 (and Theorem F.1 in a more detailed version) shows that this dimension is far from quantifying even \(^{}_{}()\).

### Prediction with expert advice

The problem of _prediction with expert advice_ in the \(r\)-realizable setting was introduced in  for the binary case \((||=2)\). Since then, this problem has been well-studied, with papers spanning the past 30 years . The ball-information setting for \(||>2\) and small \(r\) was studied in .

The bandit feedback variation of the problem was introduced in , in a more general version that considers _rewards_ instead of _losses_ (or _mistakes_). However, there is a major difference between how they define \(r\)-realizability and how we define it (in the setting of an adaptive adversary). In their definition, there must be an expert which makes at most \(r\) mistakes, whereas we only assume that there must be an expert whose predictions are inconsistent with the bandit feedback at most \(r\) times.

When the adversary is oblivious, both notions of \(r\)-realizability coincide (see Remark A.1 for more details). Therefore, it is possible that their techniques can be used to obtain tight bounds for theoblivious setting for all values of \(r\); our bounds for the adaptive setting are tight in the oblivious setting only for small \(r\) values (see Theorem C.12).

Unfortunately, to the best of our knowledge, when converting rewards to losses and applying the bounds of Auer, Cesa-Bianchi, Freund, and Schapire (2002), a dependence on the number of rounds \(T\) emerges (see for example (Bubeck and Cesa-Bianchi, 2012, Theorem 3.1) and (Cesa-Bianchi and Lugosi, 2006, Theorem 6.10)), which is undesirable when studying the mistake bound model (rather than the regret model). In this work we are mainly interested in the experts setting as a means for proving Theorem 1.1, and our proof specifically requires the adaptive setting. We leave for future work the question of finding mistake bounds against oblivious adversaries which are tight for large values of \(r\).

## 4 Open questions and future work

Our work leaves some interesting open questions and directions for future work.

### Open questions

The price of adaptivity for concept classes.Our proof of Theorem E.1 uses pattern classes. Can we prove it using only concept classes, similarly to Theorem F.1? If not, what is the price of adaptivity when learning concept classes?

The exact role of randomness.There is a \(( k)\) gap between the lower and upper bounds in Theorem 1.4. What is the correct worst case price of not using randomness?

The agnostic setting with oblivious adversaries.Our mistake bounds in the agnostic setting are not tight for large \(r^{}\) when the adversary is oblivious, both for the problem of learning a concept class (Table 1), and for _prediction with expert advice_ (Table 2). It would be interesting to obtain bounds that are tight for large \(r^{}\) against an oblivious adversary for both problems. One possible approach towards proving such bounds for _prediction with expert advice_ is to use the techniques of Auer, Cesa-Bianchi, Freund, and Schapire (2002), and specifically the \(4\) algorithm.

A natural algorithm for the experts setting.Our randomized algorithm for _prediction with expert advice_ is optimal, but not very natural nor efficient, as it relies on the calculation of minimax values. While the analysis of our algorithm employs the well-known paradigm of potential-based weighted experts, the learning algorithm itself does not make any use of these weights to devise its predictions. This is in contrast to many learning algorithms that integrate the weights into the prediction process (See (Cesa-Bianchi and Lugosi, 2006, Section 2.1) and bibliographic remarks in Section 2). Can we design a natural algorithm, in the spirit of algorithms such as _weighted majority_(Littlestone and Warmuth, 1994) and \(4\)(Auer, Cesa-Bianchi, Freund, and Schapire, 2002), that achieves the guarantees of Theorem C.7, up to constant factors?

### Future research

The multilabel setting.It would be interesting to generalize the results of this work to the multilabel setting considered in (Daniely and Helbertal, 2013), in which the adversary is allowed to choose several correct labels in each round. When the adversary is adaptive, it is not hard to see that this is equivalent to the single-label setting considered in this work with a deterministic learner. What happens when the adversary is oblivious?

Other types of feedback.This work considers the full information and bandit feedback models. However, one can think of other types of feedback. Consider for example the _comparison feedback_ model: if the true label is \(y\) and the prediction is \(z\), the adversary provides the feedback \(\{<,=\,,>\}\) such that \(z\)\(\)\(y\). An interesting direction for future research is to prove results in the spirit of this work for comparison feedback, or for other natural types of feedback.