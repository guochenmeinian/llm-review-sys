# InterpolationConstants(\(t,k\)):

The Poisson Midpoint Method for Langevin Dynamics: Provably Efficient Discretization for Diffusion Models

 Saravanan Kandasamy

Department of Computer Science

Cornell University

sk3277@cornell.edu

&Dheeraj Nagaraj

Google DeepMind

dheerajnagaraj@google.com

This work was done when SK was a student researcher at Google Research.

###### Abstract

Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization. Langevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the simplest and most studied algorithm. LMC can suffer from slow convergence - requiring a large number of steps of small step-size to obtain good quality samples. This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps. Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions. However, important applications such as diffusion models involve non-log concave densities and contain time varying drift. We propose its variant, the Poisson Midpoint Method, which approximates a small step-size LMC with large step-sizes. We prove that this can obtain a quadratic speed up of LMC under very weak assumptions. We apply our method to diffusion models for image generation and show that it maintains the quality of DDPM with 1000 neural network calls with just 50-80 neural network calls and outperforms ODE based methods with similar compute.

## 1 Introduction

The task of sampling from a target distribution is central to Bayesian inference, generative modeling, differential privacy and theoretical computer science . Sampling algorithms, based on the discretization of a stochastic differential equation (SDE) called the Langevin Dynamics, are widely used. The straightforward time discretization (i.e., Euler Maruyama discretization) of Langevin dynamics, called Langevin Monte Carlo (LMC), is popular due to its simplicity. The convergence properties of LMC have been studied extensively in the literature under various conditions on the target distribution . LMC can suffer from slow convergence to the target distribution, and often requires a large number of steps with a very fine time discretization (i.e., small step-size), making it prohibitively expensive.

The Poisson Midpoint Method introduced in this paper approximates multiple steps of small step-size Euler-Maruyama discretization with one step of larger step-size via stochastic approximation. In the case of LMC, we show that our method (called PLMC) converges to the target as fast as LMC with a much smaller step-size without any additional assumptions such as isoperimetry or strong log concavity (up to a small additional error term). This is a variant of the Randomized Midpoint Method (RLMC) studied in the literature  (see Section 1.1 for comparison).

Diffusion models are state-of-the-art in generating new samples of images and videos given samples [20; 43; 34]. These start with a Gaussian noise vector and evolve it through the time-reversal of the SDE called the Ornstein-Uhlenbeck process. The time reversed process can be written as an SDE (Langevin Dynamics with a time dependent drift) or as an ODE (see Section 2). The DDPM scheduler , which discretizes the SDE, obtains the best quality images with a small step-size and a large number of steps (usually 1000 steps). However, its quality degrades with larger step-sizes and a small number of steps (say 100 steps). Schedulers such as DDIM (), DPM-Solver ([28; 29]), PNDM () which solve the ODE via numerical methods perform much better than DDPM with a small number of steps. However, it is noted that they do not match the performance of DDPM with 1000 steps over many datasets ([42; 28; 41]). Poisson Midpoint Method gives a scheduler for the time-reversed SDE which maintains the quality of DDPM with 1000 steps, with just 50-80 steps.

### Prior Work

Euler Maruyama discretization of SDEs is known to be inefficient and many powerful numerical integration techniques have been studied extensively ([23; 32; 3; 26]). However, higher order methods such as the Runge-Kutta method require the existence and boundedness of higher order derivatives of the drift. The Randomized Midpoint Method for LMC (RLMC) was introduced for strongly log-concave sampling  and was further explored in [50; 18]. It was shown that RLMC, under certain conditions, can sample with a larger step size for fewer steps compared to Euler Maruyama and yet obtain the same accuracy. RLMC is popular due to its simplicity, and ease of implementation and does not require higher order bounded derivatives of the drift function. However, the current theoretical results are restricted to the case of strongly log-concave sampling, whereas non-log-concave sampling is of immense practical interest.

### Our Contributions

**(1)** We design the Poisson Midpoint Method which discretizes SDEs by approximating \(K\)-steps of Euler-Maruyama discretization with step-size \(\) by just one step with step-size \(\). We show a strong error bound between these two processes under general conditions in Theorem 1 (no assumption on mixing, smoothness etc). This is based on a Central Limit Theorem (CLT) based method in  to analyze stochastic approximations of LMC.

**(2)** We apply our method to LMC to obtain PLMC. We show that it achieves a speed-up in sampling for both Overdamped LMC (OLMC) and Underdamped LMC (ULMC) whenever LMC mixes, without additional assumptions such as isoperimetry or strong-log concavity.

**(3)** When the target obeys the Logarithmic Sobolev Inequalities (LSI), we show that PLMC achieves a quadratic speed up for both OLMC and ULMC. Prior works on midpoint methods [40; 18; 50] only considered strongly log-concave distributions. We also show an improvement in computational complexity for ULMC from \(}\) to \(}\) to achieve \(\) error2.

**(4)** Empirically, we show that our technique can match the quality of DDPM Scheduler with 1000 steps with fewer steps, achieving up to 4x gains in compute. Over multiple datasets, our method outperforms ODE based schedulers such as DPM-Solver and DDIM in terms of quality.

### Notation:

\(X_{0:T}\) denotes \((X_{t})_{0 t T}\) and \(X_{K(0:T)}\) denotes \((X_{tk})_{0 t T}\). \(\) denotes identity matrix over \(^{k k}\) whenever \(k\) is clear from context. For any vector \(^{k}\), \(\|\|\) denotes its Euclidean norm. For any \(a,b\) and \(a>b\), we take the \(_{t=a}^{b}\) to be \(0\), and the product \(_{t=a}^{b}\) to be \(1\). Underdamped Langevin Dynamics happens in \(^{2d}\). Here, we take vectors named \(X\) (along with subscripts and superscripts) as \(X=[U V]^{}\) where \(U,V^{d}\) also carry the same subscripts and superscripts (e.g: \(_{4}\) corresponds to \(_{4},_{4}\)). In this case \(\) represents identity matrix in \(^{2d 2d}\) and \(_{d}\) denotes the identity matrix in \(^{d d}\). For any random variable \(X\), we let \((X)\) denote its probability measure. By \((,)\) and \((|\!|)\) we denote the total variation distance and KL divergence (respectively) between two probability measure \(,\). \(O,,\) are standard Kolmogorov complexity notationswhereas \(,,\) are same as \(O,,\) up to poly-logarithmic factors in the problem parameters such as \(,,K,T,d\).

## 2 Problem Setup

Given a random vector \(X_{0}^{d}\), consider an iterative, discrete time process \((X_{t})_{t\{0\}}\) over \(^{d}\), with step-size \(>0\) given by:

\[X_{t+1}=A_{}X_{t}+G_{}b(X_{t},t)+_{}Z_{t}\] (1)

Where \(A_{},G_{},_{}\) are \(d d\) matrix valued functions of the step-size \(\) and \((Z_{t})_{t 0}}}{{}}(0, _{d})\). \(b:^{d}^{d}\) is the drift. Call this process \((A,G,,b,)\). We consider Overdamped Langevin Monte Carlo (OLMC), Underdamped Langevin Monte Carlo (ULMC) and DDPMs as key examples.

Overdamped Langevin Monte Carlo:Consider Overdamped Langevin Dynamics for some \(F:^{d}\):

\[d_{}=- F(_{})d+dB_{}\] (2)

Here \(B_{}\) is the standard Brownian motion in \(^{d}\). Under mild conditions on \(F\) and \(_{0}\), \((_{})}{ {}}^{*}\) where \(^{*}(X)(-F(X))\) is the stationary distribution.

Picking \(A_{}=\), \(G_{}=\), \(_{}=\) and \(b(,t)=- F()\) in Equation (1) gives us Euler-Maruyama discretization of Overdamped Langevin Dynamics: \(X_{t}\) in (1) approximates \(_{ t}\). OLMC is the canonical algorithm for sampling and has been studied under assumptions such as log-concavity of \(^{*}\) or that \(^{*}\) satisfies isoperimetric inequalities .

Underdamped Langevin Monte Carlooccurs in \(2d\) dimensions. We write \(X_{t}=[U_{t} V_{t}]^{}^{2d}\) where \(U_{t}^{d}\) is the position and \(V_{t}^{d}\) is the velocity. Fix a damping factor \(>0\). We take:

\[A_{h}:=_{d}&(1-e^{- h})_{d}\\ 0&e^{- h}_{d}, G_{h}:=(h-(1-e^{- h}))_{d}&0\\ (1-e^{- h})_{d}&0 b(X_{t},t ):=- F(U_{t})\\ 0\]

\[_{h}^{2}:=(h-(1-e^ {- h})+(1-e^{-2 h}))_{d}&(1-2e^{- h}+e^{-2 h})_{d}\\ (1-2e^{- h}+e^{-2 h})_{d}&(1-e^{-2 h })_{d}\]

This choice of \(A_{h},_{h},G_{h},b(,)\) in Equation (1) gives the Euler-Maruyama discretization of Underdamped Langevin Dynamics (a.k.a. the Kinetic Langevin Dynamics) studied extensively in Physics):

\[d_{}=_{}d; d_{}=-_{ }- F(_{})+dB_{}\] (3)

The stationary distribution of the SDE is given by \(^{*}(U,V)(-F(U)-}{2})\). ULMC is popular in the literature and has been analyzed in the strongly log-concave setting  and under isoperimetry conditions . We refer to  for a complete literature review.

Denoising Diffusion Models:In this case the stochastic differential equation is given by:

\[d_{}=(_{}+2 p_{}(_{}))d+ dB_{}\] (4)

This also admits an equivalent characteristic ODE given below:

\[_{}}{d}=_{}+ p_{}(_{ })\] (5)

See  for further details. Here \(p_{}\) is the probability density of \(e^{-t}X^{*}+}Z\) where \(X^{*}\) is drawn from the target and \(Z\) is drawn from \((0,)\) independently. The drift \( p_{}\) is learned via neural networks for discrete time instants \(_{0},,_{n-1}\) (usually \(n=1000\)). In practice, the iterations are written in the form 3: \(X_{t+1}=a_{t}X_{t}+b_{t} p_{_{t}}(X_{t})+_{t}Z_{t}\) where \(a_{t},b_{t},_{t}\) are chosen for best performance. Aside from the original choice in , many others have been proposed (). Since \(A_{},G_{},_{}\) in Equation (1) are time independent, we provide a variant of the Poisson Midpoint Method to suit DDPMs in Section A.1, along with a few other optimizations.

### Technical Notes

Scaling Relations:We impose the following scaling relations on the matrices \(A_{h},G_{h},_{h}\) for every \(h^{+},n\), which are satisfied by both OLMC and ULMC. Whenever \(b()\) is a constant function, these ensure that \(K\) steps of Equation (1) with step-size \(/K\) is the same as \(1\) step with step-size \(\) in distribution.

\[(A_{h})^{n}=A_{hn};(_{i=0}^{n-1}(A_{h})^{i})G_{h}=G_{hn};_{i =0}^{n-1}(A_{h})^{i}_{h}^{2}(A_{h}^{})^{i}=_{hn}^{2}\]

Randomized Midpoint Method and Stochastic ApproximationWe first illustrate the Randomized Midpoint Method [40; 18; 50] by applying it to OLMC (to obtain RLMC) to motivate our method (the Poisson Midpoint Method) and explain why we expect a quadratic speed up shown in Section 3. Overdamped Langevin Dynamics (2) satisfies:

\[_{(t+1)}=_{t}-_{t}^{(t+1)} F( _{s})ds+(B_{(t+1)}-B_{t})\] (6)

Taking \(X_{t}\) as the approximation to \(_{t}\), LMC approximates the integral \(_{t}^{(t+1)} F(_{s})ds\) with \( F(X_{t})\), giving a 'biased' estimator to the integral (conditioned on \(X_{t}=_{t}\)). This gives the LMC updates \(X_{t+1}=X_{t}- F(X_{t})+Z_{t}; Z_{t}(0,)\). RLMC chooses a uniformly random point in the interval \([t,(t+1)]\) instead of initial point \(t\) as described below:

Let \(u(),Z_{t,1},Z_{t,2}(0,)\) be independent and define the midpoint \(X_{t+u}:=X_{t}-u F(X_{t})+Z_{t,1}\) (notice \(X_{t+u}\) is an approximation for \(_{(t+u)}\)). The RLMC update is:

\[X_{t+1}=X_{t}- F(X_{t+u})+Z_{t,1}+Z_{t,2}\,.\]

Notice that \(Z_{t,1}+Z_{t,2}|u,X_{t}(0, )\), and \([ F(X_{t+u})|X_{t},Z_{t,1},Z_{t,2}]=_{0}^{1}F (X_{t+s})ds\) which is a better approximation of the integral than \( F(X_{t})\). Therefore RLMC provides a nearly unbiased approximation to the updates in Equation (6).

Intuitively, we expect that reducing the bias leads to a quadratic speed-up. Let \(Z,Z^{}(0,1)\) and independent. For \(\) small enough it is easy to show that, \(((Z+)(Z))= (^{2})\) whereas \(((Z+ Z^{})(Z))=(^{4})\). We hypothesize that \(Z_{t}+\) error in integral is closer to \((0,)\) when the error term has a small mean and a large variance (as in RLMC) than when it has a large mean but \(0\) variance (as in LMC). However, rigorous analysis of RLMC has only been done under assumptions like strong log-concavity of the target distribution. This is due to the fact that \(Z_{t}\) is dependent on the error in the integral, disallowing the argument above.

Our method, PLMC, circumvents these issues by considering a discrete set of midpoints \(\{0,,,\}\) instead of \(\). It picks each midpoint with probability \(\) independently, allowing us to prove results under more general conditions using the intuitive ideas described above. Thus, our method is a variant of RLMC which is amenable to more general mathematical analysis. The **OPTION 2** of our method (see below) makes this connection clearer. PLMC is naturally suited to DDPMs since the drift function is trained only for a discrete number of timesteps (see Section 2).

### The Poisson Midpoint Method

We introduce the Poisson Midpoint Method (PLMC) which approximates \(K\) steps of \((A,G,,b,)\) with step-size \(\) with one step of which has a step-size \(\). We denote this by \((A,G,,b,,K)\) and let its iterates be denoted by \((_{tK})_{t 0}\) or \((X_{t}^{})_{t 0}\). Suppose \(H_{t,i}\{0,1\}\) be any binary sequence and \(Z_{tK+i}\) be a sequence of i.i.d. \((0,)\) for \(t,i\{0\},0 i K-1\). Given \(_{tK}\), we define the interpolation:

\[}_{tK+i}:=A_{}_{tK}+G_{}b(_{tK},t)+_{j=0}^{i-1}A_{} _{}Z_{tK+j}\] (7)Note that this interpolation is cheap since every one of \(_{tK+i}\) can be computed with just one evaluation of the function \(b()\). We then define the refined iterates for \(0 i K-1\) for a given \(t\) as:

\[_{tK+i+1}=A_{}_{tK+i}+G_{} [b(_{tK},t)+KH_{t,i}(b(}_{tK+i},)-b(_{tK},t))]+_{}Z_{tK+i}\] (8)

We pick \(H_{t,i}\) based on the following two options, independent of \(Z_{tK+i},_{0}\):

**OPTION 1:**\(H_{t,i}\) are i.i.d. \(()\).

**OPTION 2:** Let \(u_{t}(\{0,,K-1\})\) i.i.d. and \(H_{t,i}:=(u_{t}=i)\).

**Remark 1**.: _We call our method Poisson Midpoint method since in **OPTION 1** the set of midpoints \(\{ t+:H_{t,i}=1\}\) converges to a Poisson process over \([ t,(t+1)]\) as \(K\)._

The Algorithm and Computational ComplexityThe algorithm \((A,G,,b,,K)\) computes \(_{K(t+1)}\) given \(_{Kt}\) in one step by unrolling the recursion given in Equation (8). For the sake of clarity, we will relabel \(_{tK}\) to be \(X_{t}^{}\) to stress the fact that it is the \(t\)-th iteration of \((A,G,,b,,K)\).

**Step 1:** Generate \(I_{t}=\{i_{1},,i_{N}\}\{0,,K-1\}\) such that \(H_{t,i}=1\) iff \(i I_{t}\). Let \(i_{1}<i_{2}<i_{N}\) hold. When \(N=0\), we take this to be the empty set.

**Step 2:** Let \(M_{0}:=0\) and let \(W_{t,k}\) be a sequence of i.i.d. \((0,)\) random vectors. For \(k=1,,N,N+1\), we take:

\[M_{k}=A_{-i_{k-1})}{K}}M_{k-1}+_{-i _{k-1})}{K}}W_{t,k}\]

We use the convention that \(i_{0}=0\), \(i_{N+1}=K-1\), \(A_{0}=\) and \(_{0}=0\).

**Step 3:** For \(k=1,,N\), compute \(}_{tK+i_{k}}:=A_{}{K}}X_{t}^{}+G_{ }{K}}b(X_{t}^{}, t)+M_{k}\)

**Step 4:**\(:=K_{k=1}^{N}G_{)}{K}}(b(}_{tK+i},)-b(X_{t}^{}, t))\)

**Step 5:**\(X_{t+1}^{}=A_{}X_{t}^{}+G_{a}b(X_{t}^{},  t)+M_{N+1}+\) (9)

That is, the algorithm first generates the random mid-points \(H_{t,i}\), computes the interpolation \(}_{tK+i}\) only when \(H_{t,i}=1\) and then computes \(b(}_{tK+i},)\) for these points. These computations are then combined to compute \(_{(t+1)K}\). In most applications, it is computationally easy to generate Gaussian random vectors and perform vector operations such as summation. However, the evaluation of the drift function \(b()\) is expensive. Therefore, in this work, we consider the number of evaluations of the drift function as the measure of computational complexity. The following proposition establishes that each iteration of PLMC requires 2 evaluations of \(b()\) in expectation.

**Proposition 1**.: _When the scaling relations hold (Section 2.1), the trajectory \((X_{t}^{})_{t 0}\) in Equation (9) has the same joint distribution as the trajectory \((_{tK})_{t 0}\) given in Equation (8). In expectation, one step of \((A,G,,b,,K)\) requires two evaluations of the function \(b()\)._

We call \((A,G,,b,,K)\) as PLMC whenever \((A,G,,b,)\) is either OLMC or ULMC.

## 3 Main Results

Theorem 1 gives an upper bound for the KL divergence of the trajectory generated by \((A,G,,b,,K)\) to the one generated by \((A,G,,b,)\). We refer to Section C for its proof. We note that Theorem 1 does not make any mixing or smoothness assumptions on \(b()\) and that it can handle time dependent drifts. We refer to Section 4 for a proof sketch and discussion.

**Theorem 1**.: _Let \(X_{t}\) be the iterates of \((A,G,,b,)\) and \(X_{t}^{}\) be the iterates of \((A,G,,b,,K)\) with **OPTION 1**. Suppose that \(X_{0}^{}=X_{0}\). Let \(_{tK+i}\) be the iterates in Equation (8). Define random variables:_

\[B_{tK+i}:=_{}^{-1}G_{}[b(}_{tK+i},t+)-b(_{tK+i},t+)]\]

\[_{tK+i}:=\|K_{}^{-1}G_{}[b(}_{tK+i},t+)-b(_{tK},t)]\|\]_Then, for some universal constant \(C\) and any \(r>1\):_

\[((X_{0:T}^{}) ((X_{Kt})_{0 t T}))\] \[_{s=0}^{T-1}_{i=0}^{K-1}[\|B_{sK+i}\|^{2}]+C [^{4}}{K^{2}}+^{10}}{K^{3}} +^{6}}{K^{2}}+^{2r}}{K}]\] (10)

We now apply Theorem 1 to the case of OLMC and ULMC under additional assumptions, with the proofs in Sections E and F respectively.

**Assumption 1**.: \(F:^{d}^{d}\) _is L-smooth (i.e., \( F\) is L-Lipschitz). \(^{*}\) is its global minimizer._

**Assumption 2**.: _The initialization \(X_{0}\) is such that \(\|X_{0}-^{*}\|^{14}<C_{}^{14}d^{7}\)._

The assumptions above are very mild and standard in the literature. Specifically, Assumption 2 shows that the initialization is close to global optimum by \(O()\) up to 14th moments. For instance, this is satisfied when the initialization is a standard Gaussian variable with mean \(\) satisfying \(\|-^{*}\|=O()\). Specifically this is true when \(=0\) and \(\|^{*}\|=O()\). This is a weak assumption which is implied from common initialization assumptions in the literature as listed below. It can be replaced with the assumptions in [53, Appendix D and Lemma 27] which considers Gaussian initializations with the right variance and mean. The original randomized midpoint method work  considers initializing at \(^{*}\) whereas  considers a Gaussian initialization with the right variance at a local minimum of \(F\).

We do not make any assumptions regarding isoperimetry of the target distribution \(^{}()(-F())\).

**Theorem 2** (OLMC).: _Consider the setting of Theorem 1 with OLMC under Assumptions 1 and 2. There exists constants \(c_{1},c_{2}>0\) such that whenever \( L<c_{1}\) and \(^{3}L^{3}T<c_{2}\) then:_

\[((X_{0:T}^{}) (X_{K(0:T)}))  CL^{4}^{4}([F(X_{0})-F(^{*})]+1)\] \[+O(CL^{4}^{4}Kd^{2}T)\] (11)

**Remark 2**.: _There are lower order terms hidden in the \(O()\) notation. These are explicated in Equation (40) in the appendix. The next theorem gives a similar guarantee for ULMC and the lower order terms are explicated in Equation 60 in the appendix._

**Theorem 3** (Ulmc).: _Consider the setting of Theorem 1 with ULMC under Assumptions 1 and 2. Suppose that \(^{*}\) is the global minimizer of \(F\). There exist constants \(C_{1},c_{1},c_{2}\) such that whenever \(>C_{1}\), \(<c_{1}\), \(T<}{L^{2}^{3}}\)._

\[((X_{(0:T)}^{}) (X_{K(0:T)})) L^{4}}{^{2}}[F(U_{0}+ }{})-F(^{*})+\|V_{0}\|^{2}+1]\] \[+O(L^{4}T^{2}}{}(d+ K)^{2})\] (12)

OLMC and ULMC are sampling algorithms which output approximate samples (\(X_{T}\) and \(U_{T}\) respectively) from the distribution with density \(^{*} e^{-F}\). Given \(>0\), prior works give upper bounds on \(T\) and the corresponding step-size \(\) as a function of \(\) to achieve guarantees such as \(((X_{T})^{})\) or \(((X_{T}),^{})\). By Pinsker's inequality \(^{2} 2\) therefore we guarantees for \(^{2}\) to those for \(\) as is common in the literature.

Quadratic SpeedupLet \(T=(1/)\) as is standard. Choosing \(K=(1/)\), our method applied to OLMC achieves a KL divergence of \(O(^{2})\) to OLMC with step-size \(^{2}\). Similarly, our method applied to ULMC achieves a KL divergence of \(O(^{4})\) to ULMC with step-size \(^{2}\). Whenever the \(\) divergence of OLMC (resp. ULMC) output to \(^{}\), with step-size \(\) is \(()\) (resp \((^{2})\)) Theorem 2 (resp. Theorem 3) demonstrates a quadratic speed up.

To show the generality of our results, we combine Theorems 2 and 3 with convergence results for OLMC /ULMC in the literature () when \(^{}\) satisfies the Logarithmic Sobolev Inequality with constant \(\) (\(\)-LSI). We obtain convergence bounds for the last iterate of PLMC to \(^{}\) under the same conditions. \(\)-LSI is more general than strong log-concavity (\(\)-strongly log-concave \(^{}\) satisfies \(\)-LSI). It is stable under bounded multiplicative perturbations of the density  and Lipschitz mappings. \(\)-LSI condition has been widely used to study sampling algorithms beyond log-concavity. We present our results in Table 1 and refer to Section G for the exact conditions and results.

## 4 Proof Sketch

Sketch for Theorem 1For the proof of Theorem 1, we follow the recipe given in  in order to analyze the stochastic approximations of LMC - where only an unbiased estimator for the drift function is known. The bias variance decomposition in Lemma 1, shows that the iterations of \((A,G,,,K)\) can be written in the same form of as the iterations of \(S(A,G,,)\):

\[_{tK+i+1}=A_{}_{tK+i}+G_{} [b(_{tK+i})]+_{}_{tK+i}\]

Where \(_{tK+i}:=Z_{tK+i}+B_{tK+i}+S_{tK+i}\), \(B_{tK+i}\) is the 'bias' with a non-zero conditional mean, and \(S_{tK+i}\) is the variance with \(0\) conditional mean (conditioned on \(_{tK+i}\)). They are independent of \(Z_{tK+i}\) conditioned on \(_{tK+i}\). Note that the sequence \((_{tK+i})_{t,i}\) is neither i.i.d. nor Gaussian. If it was a sequence of i.i.d. \((0,)\), then this is exactly same as \((A,G,,)\).

The main idea behind the proof of Theorem 1 is that due to data-processing inequality, it is sufficient to show that \((_{tK+i})_{t,i}\) is close to a sequence of i.i.d. Gaussian random vectors in KL-divergence. The bias term can be shown to lead to an error bounded by \(_{t,i}\|B_{tK+i}\|^{2}\), which roughly corresponds to the KL divergence between \((B_{tK+i},)\) and \((0,)\). We then show that \(Z_{tK+i}+S_{tK+i}\|_{0:tK+i-1}\) is close in distribution to \((0,)\). In order to achieve this, we first modify the Wasserstein CLT established in  to show that \(Z_{tK+i}+S_{tK+i}\) is close in distribution to \((0,+_{t,i})\) when conditioned on \(_{0:tK+i-1}\) where \(_{t,i}\) is the conditional covariance of \(S_{tK+i}\). This CLT step gives us the error of the form \(_{s=0}^{T-1}_{i=0}^{K-1}C[^{10}}{K^ {3}}+^{6}}{K^{2}}+^{2}}{K}]\) in Theorem 1.

We then use the standard formula for KL divergence between Gaussians to bound the distance between \((0,+_{t,i})\) to \((0,)\). This accounts for the fact that the Gaussian noise considered has a slightly higher variance than \(\). This leads to the leading term \(C[^{4}}{K^{2}}]\).

Sketch for Theorem 2Applying Theorem 1 to OLMC, note that the term \(_{tK_{i}}\) depends on how far the coarse estimate \(_{tK+i}\) is from the true value \(_{tK+i}\). Indeed, under the smoothness assumption on \(F\) we show that: \(_{tK+i}^{2p}( K}{2})^{p}_{0 j K-1}\| _{tK+j}-_{tK}\|^{2p}\). Thus:

\[\|_{tK+i}\|^{2p} L^{2p}^{3p}K^{p}\|  F(_{tK})\|^{2p}+L^{2p}^{2p}K^{p}d^{p}\,.\]

Therefore, the proof reduces to bounding \(_{t=0}^{T-1}\| F(_{tK})\|^{2p}\). We observe that \(_{(t+1)K}=_{tK}-( F(_{tK})+_{t}) +Z_{t}\) where \(_{t}\) is a small error term appearing due to Poisson Midpoint Method. Notice that this is approximately stochastic gradient descent on \(F\) with a large noise \(\). Therefore, using the taylor approximation of \(F\), we can show that:

\[F(_{(t+1)K})-F(_{tK}) -\| F(_{tK})\|^{2}+ d+o( d)\] \[_{t=0}^{T-1}\| F(_{tK})\|^ {2})-_{}F()}{}+LTd+o(LTd)\]

The following sophisticated bound derived in this work is novel to the best of our knowledge:

\[_{t=0}^{T-1}\| F(_{tK})\|^{2p} L^{p-1} )-_{}F())^{p}}{}+TL^{p} d^{p}(1+( LT)^{p-1})\]

   Algorithm & Reference & Conditions & LMC complexity & PLMC complexity \\  ULMC &  & \(\)-LSI, Assumptions 1, 2 & \(}{^{2}}}{}\) for \(\) & \(()^{}}}{}\) for \(\) \\  OLMC &  & \(\)-LSI, Assumptions 1, 2 & \(d}{^{2}^{2}}\) for \(^{2}\) & \(d}{^{2}}}}{^{2}}\) for \(\) \\   

Table 1: Comparison of LMC and PLMC guarantees. LMC complexity is the upper bound on the number of drift (\(b()\)) evaluations to achieve the error guarantee in the referenced work. PLMC complexity is the corresponding upper bound for PLMC. PLMC obtains a quadratic improvement in \(\), and improved dependence on \(,d\). The bounds hold up to poly-log factors.

Sketch for Theorem 3This is similar Theorem 2, but requires us to bound \(_{t}_{tK}^{2p}\) and \(_{t} F(_{tK})^{2p}\). We track the decay of two different entities across time: (1) \(_{tK}^{2}\) and (2) \(F(_{tK}+_{tK}}{})\). Our proof shows via a similar taylor series based argument that PLMC does not allow either \(_{tK}^{2}\) or \(F(_{tK}+_{tK}}{})\) to grow too large. Letting \(_{t}:=_{tK}+_{tK}}{}\) we show (roughly):

\[_{t=0}^{T-1} F(_{tK})^{2p} }{}[_{0}^{2p }+|F(_{0})-F(^{*})|^{p}]+T[}{L^{p}}+( T)^{p-1}^{2p}]d^{p}\]

\[_{t=0}^{T-1}_{tK}^{2p}[_{0}^{2p}+|(F( _{0})-F(^{*})|^{p}]+T[}{L^{p}}+(  T)^{p-1}]d^{p}\]

## 5 Experiments

We now present experiments to evaluate Poisson Midpoint Method as a training-free scheduler for diffusion models. We consider the Latent Diffusion Model (LDM)  for CelebAHQ 256, LSUN Churches, LSUB Bedrooms and FFHQ datasets using the official (PyTorch) codebase and checkpoints. We compare the sample quality of the Poisson Midpoint Method against established methods such as DDPM, DDIM and DPM-Solver, varying the number of neural network calls (corresponding to the drift \(b(,t)\)) used to generate a single image.

To evaluate the quality, we generate \(50\)k images for each method and number of neural network calls and compare it with the training dataset. We use Frechet Inception Distance (FID)  metric for LSUN Churches and LSUN Bedrooms. For CelebAHQ 256 and FFHQ, we use Clip-FID, a more suitable metric as it is known that FID may exhibit inconsistencies with human evaluations datasets outside of Imagenet . We refer to Section A.3 in the appendix for further details.

We refer to the ODE based sampler with \(=0\) (see ) setting as DDIM and use the implementation in . We generate images for number of neural network calls ranging from 20 to 500. For DPM-Solver, we port the official codebase of  to generate images for different numbers of neural network calls ranging from 10 to 100 using MultistepDPMSolver and tune the hyperparameter 'order' over \(\{2,3\}\) and'skip_type' over {'logSNR', 'time_uniform', 'time_quadratic'} for each instance to obtain the best possible FID score. This ensures that the baseline is competitive.

For the sake of clarity, we will call all SDE based methods, including DDIM with \(>0\) (see ) as DDPM. The DDPM scheduler has many different proposals for coefficients \(a_{t},b_{t},c_{t}\) (see Section 2,), apart from the original proposal in the work of . Based on these proposals, we consider three different variants of DDPM in our experiments (see Section A.5 for exact details). This choice can have a significant impact on the performance (See Figure 1 in the Appendix) for a given number of denoising diffusion steps. For the Poisson Midpoint Method, we implement the algorithm shown in Section A.1 for number of diffusion steps ranging from 20 to 500, corresponding to 40 to 750 neural network calls (see Section A.2). This approximates \(K\) steps of the 1000 step DDPM with a single step. For both Poisson Midpoint Method and DDPM, we plot the results from the best variant in Table 2 for a given number of neural network calls and refer to Section A.5 for the numbers of all variants. Poisson Midpoint Method incurs additional noise in each iteration due to the randomness introduced by \(H_{i}\). This can lead to a large error when \(K\) is large. When \(K\) is large, we reduce the variance of the Gaussian noise to compensate as suggested in the literature (see Covariance correction in  and [31, Equation 9]). We refer to Section A.5 for full details.

### Results

We refer to the outcome of our empirical evaluations in Table 2. The first column compares the performance against DDPM. We see that for all the datasets considered, Poisson Midpoint Method can match the quality of the DDPM sampler with 1000 neural network calls with just 40-80 neural network calls. Observe that for CelebA, LSUN-Church and FFHQ datasets, the performance of DDPM degrades rapidly with lower number of steps, showing the advantage our method in this regime. However, a limitation of our work is that the quality of our method degrades rapidly at around 40-50 neural network calls. We believe this is because our stochastic approximation breaks down with larger step-sizes and further research is needed to mitigate this.

The second column compares the performance of our method against ODE based methods. It is known in the literature that DDPM with 1000 steps outperforms DDIM and DPM-Solver in terms of the quality for a large number of models and datasets [41; 42]. Thus, in terms of quality, Poisson midpoint method with just 50-80 neural network calls outperforms ODE based methods with a similar amount of compute. Note that we optimize the performance of DPM-Solver over 6 different variants as mentioned above to maintain a fair comparison. However, in the very low compute regime (\(\)10 steps), DPM-Solver remains the best choice.

## 6 Conclusion

We introduce the Poisson Midpoint Method, which efficiently discretizes Langevin Dynamics and theoretically demonstrates quadratic speed up over Euler-Maruyama discretization under general conditions. We apply our method to diffusion models for image generation, and show that our method maintains the quality of 1000 step DDPM with just 50-80 neural network calls. This outperforms ODE based methods such as DPM-Solver in terms of quality, with a similar amount of compute. Future work can explore variants of Poisson midpoint method with better performance when fewer than 50 neural network calls are used. An interesting theoretical direction would be to derive convergence bounds for algorithms such as DDPM which have a time dependent drift function. Future work can also consider convergence rates of PLMC under conditions such as the Poincare Inequality and whenever \( F\) is Holder continuous instead of Lipschitz continuous.

## 7 Societal Impact

Our work considers an efficient numerical discretization schemes for making diffusion model inference more efficient. Publicly available, pre-trained diffusion models are very impactful and have significant risk of abuse. In addition to theoretical guarantees, our work considers empirical experiments to evaluate the inference efficiency on publicly available, widely used diffusion models over curated datasets. We do not foresee any significant positive or negative social impact of our work.

   Dataset & vs. SDE Based Methods & vs. ODE Based Methods \\  CelebAHQ 256 & & \\   

Table 2: Empirical Results for the Latent Diffusion Model , comparing the Poisson midpoint method with various SDE and ODE based methods.