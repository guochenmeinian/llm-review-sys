# Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models

Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models

 Naman D Singh

University of Tubingen

Tubingen AI Center

&Francesco Croce

University of Tubingen

Tubingen AI Center

&Matthias Hein

University of Tubingen

Tubingen AI Center

 Equal Contribution. Correspondence to: naman-deep.singh@uni-tuebingen.de

###### Abstract

While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR-10, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen \(_{}\)-threat model, but even more so improve generalization to unseen \(_{1}/_{2}\)-attacks. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust \(_{}\)-models across different ranges of model parameters and FLOPs, while our ViT + ConvStem yields the best generalization to unseen threat models.

## 1 Introduction

Adversarial training  is the standard technique to obtain classifiers robust against adversarial perturbations. Many works have extensively analyzed several aspects of adversarial training, leading to further improvements. These include changes in the training scheme, i.e. using different loss functions [66; 43; 40], tuning hyperparameters like learning rate schedule or weight decay, increasing the strength of the attack at training time, adding label smoothing and stochastic weight averaging [19; 41]. Also, complementing the training set with pseudo-labelled  or synthetic  data, and via specific augmentations  yields more robust classifiers.

However, these works focus on small datasets like CIFAR-10 with low resolution images, and models using variants of ResNet  as architecture. When considering a more challenging dataset like ImageNet, the space of possible design choices regarding model, training and evaluation protocols becomes much richer, and little work exists exploring this domain. For example, it has been recently shown that architectures like vision transformers (ViTs) [16; 50], mixers [49; 54] and hybrids borrowing elements of different families of networks [17; 64] are competitive or outperform convolutional networks on ImageNet, while they hardly work on e.g. CIFAR-10. In turn, ConvNeXt  has been proposed as modern version of ResNet which closes the gap to transformer architectures.

In this work we study the influence of architecture and training schemes on the robustness of classifiers to seen and unseen attacks. We focus on two extreme ends of architectures used for ImageNet, ViT and ConvNeXt (isotropic vs non-isotropic, attention only vs convolution only, stem with large vs small patches), and study Isotropic ConvNeXt as an intermediate architecture. We focus on training ImageNet models robust with respect to the \(_{}\)-threat model (i.e. the perturbations have bounded \(_{}\)-norm), but additionally track the robustness to unseen \(_{1}\)- and \(_{2}\)-attacks in order to reveal architectural components which lead to better generalization of robustness. This, in fact, is a desirable property since typically adversarially trained models are still vulnerable to attacks not seen at training time . In particular, we show via extensive experiments that

* using a convolutional stem instead of a patch stem in most cases leads to consistent gains in \(_{}\)-robustness and quite a large boost in the unseen \(_{1}\)- and \(_{2}\)-threat models.
* leveraging SOTA pre-trained clean models as initialization allows us to train using heavy augmentation, in contrast to what has been suggested by prior work [2; 14].
* increasing image resolution at test time yields improvements in \(_{}\)-robust accuracy. This is surprising as for the \(_{}\)-threat model with fixed radius, higher resolution yields a stronger attack.

Combining our various improvements, we obtain state-of-the-art \(_{}\)-robust accuracy for perturbation size \(=4/255\) at resolution \(224 224\) on ImageNet, see Fig. 1. We obtain \(50.2\%\) robust accuracy for small and \(56.3\%\) robust accuracy for large models, improving by \(5.8\%\) and \(2.8\%\) respectively upon prior works. Furthermore, we improve SOTA when fine-tuning these models to a larger radius, \(_{}=8/255\) and to other datasets. We make our code and robust models publicly available.1

## 2 Background and Related Work

In  adversarial training (AT) of a classifier \(f_{}:\) is introduced as the optimization problem

\[_{}_{(x_{i},y_{i})}_{:\|\|_{p} _{p}}(f_{}(x_{i}+),y_{i})\] (1)

where \(\) are the parameters of the classifier \(f_{}\). The goal is to make \(f_{}\) robust against \(_{p}\)-bounded perturbations, that is the classifier should have the same prediction on the \(_{p}\)-ball \(B_{p}(x_{i},_{p})\) of radius \(_{p}\) centered at \(x_{i}\). In practice, the inner maximization is approximately solved with projected gradient descent (PGD), \(\) is optimized with SGD (or its variants) on the training set \(\), and cross-entropy loss is used as objective function \(\). AT has been shown to be an effective defense against adversarial perturbations, and many follow-up works have modified the original algorithm to further improve the robustness of the resulting models [66; 61; 20; 43; 40].

The most popular threat models are \(_{}\)- and \(_{2}\)-bounded perturbations. Several other attacks have been explored, including \(_{1}\)- or \(_{0}\)-bounded, adversarial patches, or those defined by neural perceptual

Figure 1: **AutoAttack \(_{}\)-Robust accuracy (\(=4/255\)) on ImageNet across different architectures and training schemes at resolution \(224 224\). With our training scheme and replacing PatchStem with ConvStem in ConvNeXt and ViT, we outperform over a range of parameters/FLOPs existing models in terms of \(_{}\)-robust accuracy, e.g. our ConvNeXt-T-CvSt with 28.6M parameters achieves 50.2% robust accuracy, an improvement of 5.8% compared to the ConvNeXt-T of .**

metrics like LPIPS . A well-known problem is that models trained to be robust with respect to one threat model need not be robust with respect to unseen ones . Multiple-norm adversarial training  aims to overcome this but degrades the performance compared to adversarial training in each individual threat model.

**Training recipe.**[19; 41] analyzed in detail the influence of several fine-grained choices of training hyperparameters (batch size, weight decay, learning rate schedule, etc.) on the achieved robustness. [24; 5] suggest to use a standard classifier pre-trained either on a larger dataset or with a self-supervised task as initialization for adversarial training for improved robustness. However, all these works focus on CIFAR-10 or other datasets with low resolution images. In , the effect of several elements including clean initialization, data augmentation and length of training on adversarial training with ViTs is analyzed: however, their study is limited to CIFAR-10 and Imagenette , and we show below that some of their conclusions do not generalize to ImageNet and other architectures. For ImageNet,  argued that ViTs and ResNets attain similar robustness for several threat models if training scheme and architecture size for both are similar, while  proposed a recipe for training \(_{}\)-robust transformers, in particular XCiT : basic augmentation, warm-up for the radius \(_{p}\), and large weight decay. In contrast,  use the standard training scheme for a large ViT-B for 300 epochs and obtain the most \(_{}\)-robust model on ImageNet. In concurrent work,  study \(_{}\)-training schemes for ConvNeXt and Swin-Transformer which are outperformed by our models, in particular regarding generalization to unseen attacks.

**Architectural design choices.** Searching for more robust networks [60; 26] studied the effect of architecture design choices in ResNets, e.g. width and depth, on adversarial robustness for CIFAR-10. Recently,  did a similar exploration on ImageNet.  compared the robustness to seen and unseen attacks of several modern architectures on CIFAR-10 and Imagenette, and conclude that ViTs lead to better robust generalization. Finally,  found that using smooth activation functions like GELU  improves the robustness of adversarially trained models on ImageNet, later confirmed for CIFAR-10 as well [19; 41].

**Modern architectures.** Since  showed that transformer-based architectures can be successfully used for vision tasks, a large number of works have introduced modifications of vision transformers [50; 33; 55] achieving SOTA classification accuracy on ImageNet, outperforming ResNets and variants. In turn, several works proposed variations of training scheme and architectures to make convolutional networks competitive with ViTs [47; 59], in particular leading to the ConvNeXt architecture of . Moreover, hybrid models combine typical elements of ViTs and CNNs, trying to merge the strengths of the two families [17; 13; 64]. While these modern architectures perform similarly well on vision tasks, it is open how they differ regarding adversarial robustness. [18; 21] compared the robustness of normally trained ViTs and ResNets to \(_{p}\)-norm bounded and patch attacks concluding that the ranking depends on the threat model.

## 3 Influence of Architecture on Adversarial Robustness to Seen and Unseen Attacks

In this section we analyze existing architectures and their components regarding adversarial robustness on ImageNet. In particular, we are interested in studying which elements are beneficial for adversarial robustness _(i)_ in the threat model seen at training time and _(ii)_ to unseen attacks. Since it is the most popular and well-studied setup, we focus on adversarial training w.r.t. \(_{}\), and use \(_{2}\)- and \(_{1}\)-bounded attacks to measure the generalization of robustness to unseen threat models. While we focus on the influence of architecture on the achieved robustness in this section, we analyze the influence of pre-training and augmentation in Sec. 4. All results are summarized in Table 1 which provides the full ablation for this and the next section. We select the version T for ConvNeXt and S for Isotropic ConvNeXt and ViT, since these have comparable number of parameters and FLOPs.

### Experimental Setup

**Training setup.** If not stated otherwise, we always use adversarial training  w.r.t. \(_{}\) with perturbation bound \(=4/255\) and 2 steps of APGD  for the inner optimization on ImageNet-1k using resolution 224x224 (we refer to App. C.2 for a comparison of APGD-AT vs PGD-AT). Note that we use the \(_{2}\)- or \(_{1}\)-threat model just for evaluation, we never do adversarial training for them. We use a cosine decaying learning rate preceded by linear warm-up. In this section, we use a basic training setting, that is only random crop as basic augmentation and 50 epochs of adversarial training from random initialization. More details are available in App. A.

**Evaluation setup.** For all experiments in the paper, unless specified otherwise, we evaluate the robustness using AutoAttack [8; 9], which combines APGD for cross-entropy and targeted DLR loss, FAB-attack  and the black-box Square Attack , on the 5000 images of the ImageNet validation set selected by RobustBench  at resolution 224x224. The evaluations are done at the following radii: \(_{}=4/255\), \(_{2}=2\), and \(_{1}=75\). For some marked experiments which involve a large number of evaluations we use APGDT-DLR (40 iterations, 3 restarts) with the targeted DLR loss  which is roughly off by 1-2% compared to full AutoAttack.

### Architecture Families and Convolutional Stem

**Choice of architectures.** While some of the novel components of ConvNeXts are inspired by the Swin Transformer , the ConvNeXt is a modified version of a ResNet, using progressive downsampling and convolutional layers. On the other end of the spectrum of network families we find ViTs, as a non-convolutional architecture using attention and with an isotropic design after the PatchStem, see Fig. 2. Additionally,  introduce Isotropic ConvNeXt, which creates tokens similarly to ViTs, which are then processed by a sequence of identical residual blocks (with no downsampling, hence the name isotropic) as in ViTs, until a final linear classifier. The blocks of a ViT rely on multi-head self-attention layers , while Isotropic ConvNeXt replaces them with depthwise convolutions. It can be thus considered an intermediate step between ConvNeXt and ViT. By considering ViT and ConvNeXt, we cover two extreme cases of modern architectures. While it would have been interesting to include other intermediate architectures like XCiT or Swin-Transformer, our computational budget did not allow this on top of the already extensive experiments of the present paper.

**Convolutional stem.** propose to replace the patch embedder in ViTs with a convolutional block (ConvStem) consisting of several convolutional layers alternating with normalization and activation layers. While the PatchStem in ViTs divides the input in disjoint patches and creates tokens from them, the ConvStem progressively downsamples the input image until it has the right resolution to be fed to the transformer blocks.  show that this results in a more stable standard training which is robust to the choice of optimizer and hyperparameters. As adversarial training is more difficult than standard training, we check if adding a ConvStem has a similar positive impact there as well.

As both ViTs and Isotropic ConvNeXts divide the input image into \(16 16\) patches, we use for both the convolutional stem of , which has four convolutional layers with stride 2, each followed by layer normalization and GELU activation. Since ConvNeXt uses a PatchStem with \(4 4\)-patches, the ConvStem of ViTs would not work. Therefore we design a specific convolutional stem with only two convolutional layers with kernel size 3 and stride 2, see Fig. 2. Adding a ConvStem has only minimal impact on the number of parameters and FLOPs of the models (see Table 2), especially for ConvNeXt.

More details on the convolutional stem design, and ablation studies on its effect on unseen threat models can be found in App. B. The effect of ConvStem on model size is discussed in App. C.4.

### Effect of Architecture and ConvStem vs Patchstem on Adversarial Robustness

In Table 1 we first check for the basic setting described above (random init., basic augment.) the effect of using a convolutional stem (+ CvSt) vs PatchStem for the three architectures: for both isotropic architectures it notably improves the robustness as well as clean accuracy for adversarially trained models, whereas the improvements for the ConvNeXt are marginal. However, all three architectures show huge improvements in the generalization of robustness to unseen \(_{1}\)- and \(_{2}\)-attacks when using ConvStem instead of PatchStem. Interestingly, the \(_{1}\)- and \(_{2}\)-robustness of the ConvStem models is quite similar despite larger differences in \(_{}\)-robustness. Our results in this basic training setup show

Figure 2: **Convolutional stem for ConvNeXt.** We use two convolution layers, each followed by LayerNorm and GELU to replace the 4x4 patch of the standard ConvNeXt model (PatchStem).

that the ConvStem is a simple and effective architecture modification which has significant impact on robustness with respect to seen and unseen threat models. We also note that this finding generalizes for the modifications of the training scheme proposed in the next section. It is particularly remarkable that even for the ConvNeXt architecture, this little change from PatchStem to ConvStem has such a huge effect on the robustness with respect to the unseen \(_{1}\)-and \(_{2}\)-threat models.

## 4 Effect of Strong Pre-Training and Heavy Data Augmentation on Robustness

As the next step after the architecture modification of the last section, we now identify in an ablation study in Table 1 strong pre-training and heavy augmentations as key elements of the training scheme which boost adversarial robustness on ImageNet. The effect of other fine-grained training hyperparameters like weight decay and label smoothing is discussed in App. D.3, where we show that varying them around our default values does not have any impact on robustness.

### Strong Pre-Training

It is well-known that pre-training on large datasets improves the performance on downstream tasks, e.g. models pre-trained on ImageNet-21k or JFT achieve significantly higher accuracy on ImageNet-1k than those trained on ImageNet-1k from random initialization . Similarly, it has been observed that pre-training on larger datasets like ImageNet-21k or ImageNet-1k helps to achieve better adversarial robustness on CIFAR-10 [24; 38]. However, our goal is not to use a larger training set than ImageNet-1k but to better utilize the given dataset ImageNet-1k. As adversarial training on ImageNet is a non-trivial optimization problem, where runs can fail completely, see , we propose to initialize adversarial training from a fully trained (on ImageNet-1k) standard model. Although the clean model

    &  & \)} \\  & & clean & \(_{}\) & \(_{2}\) & \(_{1}\) \\   & random init., basic augment. & 64.0 & 37.1 & 28.9 & 8.8 \\  & + strong clean pre-training & 69.4 +5.4 & 41.6 +4.5 & 42.4 +13.5 & 18.9 +10.1 \\  & + heavy augmentations & 71.0 +1.6 & 46.5 +4.9 & 38.1 & -4.3 & 14.9 +4.0 \\  & 50 \(\) 300 epochs & 72.4 +1.4 & 48.6 +2.1 & 38.0 -0.1 & 14.9 +0.0 \\   & random init., basic augment. & 64.2 & 37.6 & 40.2 & 18.6 \\  & + strong clean pre-training & 69.1 +4.9 & 42.2 +4.6 & 42.4 +2.2 & 19.7 +1.1 \\  & + heavy augmentations & 69.1 +0.0 & 47.5 +5.3 & 47.8 +5.4 & **24.6** +4.9 \\  & 50 \(\) 300 epochs & **72.7** +3.6 & **49.5** +2.0 & **48.4** +0.6 & 24.5 -0.1 \\   & random init., basic augment. & 60.5 & 31.7 & 20.8 & 4.0 \\  & + strong clean pre-training & 68.0 +7.5 & 38.9 +7.2 & 38.3 +17.5 & 17.0 +13.0 \\  & + heavy augmentations & 64.5 -3.5 & 38.5 -0.4 & 36.5 -1.8 & 17.7 +0.7 \\  & 50 \(\) 300 epochs & 69.0 +4.5 & 44.2 +5.7 & 36.6 +0.1 & 14.9 -2.8 \\   & random init., basic augment. & 62.1 & 33.5 & 40.0 & 23.8 \\  & + strong clean pre-training & 69.5 +7.4 & 41.8 +8.3 & 46.2 +6.2 & 26.9 +3.1 \\  & + heavy augmentations & 69.5 +0.0 & 42.1 +0.3 & 47.2 +1.0 & **28.9** +2.0 \\  & 50 \(\) 300 epochs & **70.2** +0.7 & **45.9** +3.8 & **49.2** +2.0 & 27.9 -1.0 \\   & random init., basic augment. & 61.5 & 31.8 & 35.5 & 15.1 \\  & + strong clean pre-training & 66.8 +5.3 & 39.1 +7.3 & 34.5 -1.0 & 12.6 -2.5 \\  & + heavy augmentations & 65.2 -1.6 & 39.2 +0.1 & 37.3 +2.8 & 16.2 +3.6 \\  & 50 \(\) 300 epochs & 69.2 +4.0 & 44.0 +4.8 & 37.5 +0.2 & 15.1 -1.1 \\   & random init., basic augment. & 62.8 & 34.4 & 39.2 & 20.4 \\  & + strong clean pre-training & 71.2 +8.4 & 44.3 +9.9 & 47.1 +7.9 & 23.2 +2.8 \\  & + heavy augmentations & 69.9 -1.3 & 44.0 -0.3 & 47.1 +0.0 & 25.9 +2.7 \\  & 50 \(\) 300 epochs & **72.5** +2.6 & **48.1** +4.1 & **50.4** +3.3 & **26.7** +0.8 \\   

Table 1: **Influence of ConvStem, strong (clean) pre-training and heavy data augmentation.** We show an ablation of using strong (clean) pre-training, heavy data augmentation and longer training for all models with and without ConvStem. Across all architectures using ConvStem, stronger pre-training, heavy augmentation and longer training improve adversarial robustness. We boldface the best performance for every metric within each architecture family.

is clearly not adversarially robust, starting from a point with low clean loss considerably helps in the first stages of adversarial training. Short fine-tuning of clean models to become adversarially robust has been discussed in [27; 10] but without achieving robustness comparable to full adversarial training.

In the following we use as initialization the standard models available in the timm library or from the original papers, refer App. D.7 for further details. For the models with convolutional stem no such models are available. For these we initialize the ConvStem randomly and the remaining layers with the weights of the corresponding non-ConvStem models, then do standard training for 100 epochs to reach good clean accuracy (see App. B.3 for details and performance of these classifiers). Interestingly, changing from random initialization to initialization with strongly pre-trained models significantly improves clean as well as \(_{}\)-robust accuracy across architectures, see Table 1. The effects on the isotropic architectures, which are known to be harder to train, are even higher than for the ConvNeXt. At this stage the ViT-S + ConvStem already has \(2.5\%\) better \(_{}\)-robustness (44.3% vs 41.8%) than the XCiT-S model of . Finally, using an even better model, i.e. pre-trained on ImageNet-21k, as initialization did not provide additional benefit, see App. D.1.

### Heavy Data Augmentation

The second aspect we analyze is how to take advantage of heavy augmentation techniques like RandAugment , MixUp  and CutMix , which are crucial for the performance of standard models, in adversarial training. In particular,  note that for ViTs using heavy data augmentation with adversarial training leads to model collapse, and thus design a schedule to progressively increase the augmentation intensity. Similarly,  report better performance for robust ViTs when only weak augmentations (random crop, horizontal flip, color jitter) are used, and that ConvNeXt training even diverges if heavy augmentations are not excluded. Finally,  could train with heavy augmentations but using a much larger ViT-B model and a longer training of 300 epochs (which allows them to have a less steep learning rate warm-up phase). Instead we show that initializing adversarial training with a well-trained standard classifier makes it possible to use heavy augmentation techniques from the beginning, and benefit from them, even for small models and shorter training schedules. In contrast, in our experiments, training a ConvNeXt with heavy augmentations from random initialization failed.

As heavy augmentations we use RandAugment, CutMix, MixUp, Random Erasing  (with hyperparameters similar to , see App. A). We also use Exponential Moving Average (EMA) with a decay of 0.9999 and label smoothing with parameter 0.1 for consistency with . However, we observe little to no effect of EMA on robustness.

In Table 1 one can see that adding heavy data augmentation on top of strong pre-training significantly boosts the performance of the ConvNeXts in clean and \(_{}\)- robust accuracy. Interestingly, only the ConvNeXt + ConvStem has further strong improvements in the unseen \(_{2}\)- and \(_{1}\)-threat models. For the isotropic architectures, heavy augmentations have marginal effect on \(_{}\)-robustness, although they improve robustness to unseen attacks. Isotropic architectures require longer training to benefit from heavy augmentations. We highlight that at this point our ConvNeXt-T and ConvNeXt-T + ConvStem already have higher (+2.1% and +3.1% respectively) robustness w.r.t. \(_{}\) than the ConvNeXt-T from , and similarly our ViT-S + ConvStem and Isotropic ConvNeXt + ConvStem outperform XCiT-S  (detailed comparison to SOTA in Table 2). A further comparison of heavy augmentation and the 3-Aug scheme of  can be found in in App. D.2.

**Longer training.** Adding heavy data augmentation increases the complexity of the learning problem, which requires longer training, in particular for isotropic architectures. Thus we extend the training time from 50 to 300 epochs. As expected we see the strongest improvements for the isotropic architectures but even for ConvNeXt one observes consistent gains: this is in contrast to the claim of  that longer training does not help to improve robustness. The ConvNeXt-T + ConvStem, with 49.5% robust accuracy, outperforms all classifiers reported in  and , including those with up to 4 times more parameters.

## 5 Comparison with SOTA Models

In Table 2 we compare our classifiers with models from prior works trained for adversarial robustness w.r.t. \(_{}\) at \(_{}=4/255\) on ImageNet. For each model we report number of parameters and FLOPs,the number of training epochs, the number of steps in the inner maximization, and clean and robust accuracy (AutoAttack on RobustBench validation set) in the three threat models. We group the models according to size for fair comparison, distinguishing small (\(<\) 30M parameters), medium (\(\) 50M params) and large models (\(\) 80M params).

**1-Step vs 2-Step Adversarial Training.** Current works use one to three steps of PGD to solve the inner problem, see column "Adv. Steps" in Table 2. It is an important question if our gains, e.g. compared to works like  who use a single step, are just a result of the stronger attack at training time. We test this by training a ConvNeXt-T + ConvStem model with strong pre-training and heavy augmentation with 1-step APGD adversarial training for 50 epochs. We get a \(_{}\)-robust accuracy

  &  &  &  &  & \)} \\  &  &  &  &  &  &  & \)} & \)} & \)} \\  ResNet-50 & 25.0 & 4.1 &  & 100 & 3 & 65.88 & 33.18 & 18.88 & 3.82 \\ ResNet-50 & 25.0 & 4.1 &  & 100 & 1 & 67.44 & 35.54 & 18.16 & 3.90 \\ ViT-S & 22.1 & 4.6 &  & 100 & 1 & 66.62 & 36.56 & 41.40 & 21.82 \\ ViT-S & 22.1 & 4.6 &  & 110 & 1 & 66.78 & 37.88 & – & – \\ ViT-S & 22.1 & 4.6 &  & 90 & 3 & 65.9 & 39.24 & 32.18 & 10.54 \\ XCiT-S12 & 26.0 & 4.8 &  & 110 & 1 & 72.34 & 41.78 & 46.20 & 22.72 \\  & ViT-S & 22.1 & 4.6 & ours & 300 & 2 & 69.22 & 44.04 & 37.52 & 15.12 \\  & RobArch-S & 26.1 & 6.3 &  & 110 & 3 & 70.58 & 44.12 & 39.88 & 15.46 \\ Isotropic-CN-S & 22.3 & 4.3 & ours & 300 & 2 & 69.04 & 44.22 & 36.64 & 14.88 \\ ConvNeXt-T & 28.6 & 4.5 &  & 110 & 1 & 71.60 & 44.40 & 45.32 & 21.76 \\ Isotropic-CN-S + ConvStem & 23.0 & 4.7 & ours & 300 & 2 & 70.02 & 45.90 & 49.24 & **27.84** \\ ViT-S + ConvStem & 22.8 & 5.0 & ours & 300 & 2 & 72.56 & 48.08 & **50.40** & 26.68 \\ ConvNeXt-T & 28.6 & 4.5 & ours & 300 & 2 & 72.40 & 48.60 & 38.02 & 14.88 \\ ConvNeXt-T + ConvStem & 28.6 & 4.6 & ours & 300 & 2 & **72.72** & 49.46 & 48.42 & 24.52 \\ ConvNeXt-T + ConvStem & 28.6 & 4.6 & ours & 300 & 3 & 72.70 & **50.16** & 49.00 & 24.16 \\   & Wide-ResNet-50-2 & 68.9 & 11.4 &  & 100 & 3 & 68.82 & 38.12 & 22.08 & 4.48 \\ ResNet-101 & 44.5 & 7.9 &  & 90 & 3 & 69.52 & 41.02 & 25.62 & 6.56 \\ XCiT-M12 & 46.0 & 8.5 &  & 110 & 1 & 74.04 & 45.24 & 48.18 & 22.72 \\ ViT-M & 38.8 & 8.0 & ours & 50 & 2 & 71.72 & 47.24 & 49.02 & **29.20** \\ ViT-M + ConvStem & 39.5 & 8.4 & ours & 50 & 2 & 72.40 & 48.80 & 50.56 & 28.12 \\ ConvNeXt-S & 50.1 & 8.7 & ours & 50 & 2 & **74.10** & 52.32 & 43.84 & 19.52 \\ ConvNeXt-S + ConvStem & 50.3 & 8.8 & ours & 50 & 2 & **74.10** & **52.42** & **50.88** & 25.64 \\   & ViT-B & 86.6 & 17.6 &  & 90 & 3 & 70.42 & 43.02 & 47.26 & 27.08 \\ XCiT-L12 & 104.0 & 19.0 &  & 110 & 1 & 73.76 & 47.60 & 49.38 & 23.74 \\ Swin-B & 87.7 & 15.5 &  & 90 & 3 & 74.76 & 48.10 & 44.42 & 18.04 \\ RobArch-L & 104.0 & 25.7 &  & 100 & 3 & 73.46 & 48.92 & 39.48 & 14.74 \\ ViT-B & 86.6 & 17.6 & ours & 50 & 2 & 73.32 & 50.02 & 52.14 & **33.12** \\ ViT-B + ConvStem & 87.1 & 17.9 & ours & 50 & 2 & 74.38 & 52.58 & 54.38 & 31.20 \\ ViT-B & 86.6 & 17.6 &  & 300 & 2 & **76.62** & 53.50 & – & – \\  & ConvNeXt-B & 88.6 & 15.4 & ours & 50 & 2 & 75.62 & 54.34 & 48.52 & 23.70 \\ ConvNeXt-B + ConvStem & 88.8 & 16.0 & ours & 50 & 2 & 75.32 & 54.38 & 50.06 & 24.76 \\ ViT-B + ConvStem & 87.1 & 17.9 & ours & 250 & 2 & 76.30 & 54.66 & **56.30** & 32.06 \\ ConvNeXt-B & 88.6 & 15.4 & \({}^{*}\) & 300 & 3 & 76.02 & 55.82 & 44.68 & 21.23 \\    & ConvNeXt-B + ConvStem & 88.8 & 16.0 & ours & 250 & 2 & 75.90 & 56.14 & 49.12 & 23.34 \\    & Swin-B & 87.7 & 15.5 & \({}^{*}\) & 300 & 3 & 76.16 & 56.16 & 47.86 & 23.91 \\ ConvNeXt-B + ConvStem & 88.8 & 16.0 & ours & 250 & 3 & 75.18 & **56.28** & 49.40 & 23.60 \\  

* concurrent work

Table 2: **Comparison to SOTA \(_{}\)-robust models on ImageNet. For each model we report the number of parameters, FLOPs, number of epochs of adversarial training (AT), the number of PGD steps in AT, clean and \(_{},_{2},_{1}\)-robust accuracy with \(_{}=4/255\), \(_{2}=2\), \(_{1}=75\) (AutoAttack). Models of similar size are sorted in increasing \(_{}\)-robustness. Our ConvNeXt + ConvStem improves by \(5.8\%\) for small, \(7.2\%\) for medium, and \(2.8\%\) for large models over SOTA.**of 46.4% (clean acc. is 71.0%) compared to 47.5% with 2-step APGD (see Table 1) which is just a difference of 1.1%. Thus our gains are only marginally influenced by stronger adversarial training. Moreover, our ConvNeXt + ConvStem models with 50 epochs of adversarial training outperform all existing SOTA models even though the computational cost of training them is lower (50 epochs of 2-step AT corresponds roughly to 75 epochs of 1-step AT and 37.5 epochs of 3-step AT). For our best small and large model we also check the other direction and train 3-step AT models, which improve by \(0.7\%\) for ConvNeXt-T + ConvStem and \(0.14\%\) for ConvNeXt-B + ConvStem in \(_{}\)-robustness. This suggests that 2-step AT is a good compromise between achieved robustness and training time. In App. C.1 we additionally show how our training scheme yields more robust models than that of , even with less than half the effective training cost.

**Scaling to larger models.** We apply our training scheme introduced in the previous sections to architectures with significantly more parameters and FLOPs. We focus on ConvNeXt-S/B and ViT-M/B: in particular, for ConvNeXt-B we adapt the convolutional stem used for ConvNeXt-T by adding an extra convolutional layer, while for the ViTs we just adjust the number of output channels to match the feature dimension of the transformer blocks (more details in App. B). We train the medium models for 50 epochs and the large models for 50 and 250 epochs (with ConvStem only).

**Comparison with SOTA methods.** Among small models, all three architectures equipped with ConvStem outperform in terms of \(_{}\)-robustness all competitors of similar size with up to 5.8% improvement upon the ConvNeXt-T of . Moreover, our ConvNeXt-T + ConvStem is with \(50.2\%\) robust accuracy \(1.2\%\) better than the RobArch-L  (the previously second most robust model) which has 3.6\(\) more parameters and FLOPs. The ConvStem positively impacts generalization of robustness to unseen attacks of all three architectures, with improvements between \(2.8\%\) and \(4.2\%\) for \(_{2}\) and \(1.4\%\) and \(5.2\%\) for \(_{1}\) (with respect to the XCiT-S12). For medium models the picture is the same: our ViT-M+ConvStem and the ConvNeXt-S+ConvStem outperform both in seen and unseen threat models their non-ConvStem counterparts and the best existing model (XCiT-M12 of ).

For large models we compare to the previously most robust model, the ViT-B of , with \(53.5\%\)\(_{}\)-robust accuracy (the model is not available so we cannot evaluate \(_{1}\)- and \(_{2}\)-robustness). It is remarkable that our ConvNeXt-B+ConvStem with only 50 epochs of training outperforms this ViT-B trained for 300 epochs already by \(0.9\%\) and with 250 epochs training even by \(2.5\%\). An interesting observation is that our ViT-B+ConvStem with 250 epochs training is with \(54.7\%\) just \(1.3\%\) worse than our ConvNeXt-B+ConvStem but has significantly better generalization to unseen attacks: \(56.3\%\) vs. \(49.1\%\) for \(_{2}\) and \(32.1\%\) vs \(23.3\%\) for \(_{1}\). Regarding the concurrent work of , we note that our ConvNeXt-B+ConvStem trained for 250 epochs with 2-step AT is already better than their ConvNeXt-B 3-step AT model trained for 300 epochs (\(\) 40% longer training time), while having significantly better generalization to the unseen \(_{1}\)- and \(_{2}\)-attacks. Our 3-step AT ConvNeXt-B+ConvStem outperforms their best model, a Swin-B, even though it is trained for \(20\%\) less epochs, and has better generalization to \(_{2}\). Finally, we note that our ViT-B+ConvStem significantly outperforms their Swin-B in terms of robustness to unseen attacks.

**Additional experiments.** InternImage , a recent non-isotropic model, already uses a ConvStem. Replacing it with a PatchStem preserves the robustness in \(_{}\) (seen threat model) but decreases the robustness to unseen \(_{1}\)- and \(_{2}\)-attacks (App. C.3), further validating the effectiveness of ConvStem.

## 6 Influence of Resolution and Fine-Tuning

In the following we analyze how robustness behaves under variation of test time resolution, and robust fine-tuning to either a larger radius on ImageNet or other datasets.

**Improving robustness via increasing test-time image resolution.** Unlike small datasets like CIFAR-10, for ImageNet the input images are resized before inference. Using higher resolution at test time can lead to improved clean accuracy for standard ImageNet models [52; 17]. On the other hand increasing dimension for fixed radius makes the \(_{}\)-threat model more powerful. To study which effect prevails, we test clean and \(_{}\)-robust accuracy for each of our 2-step AT models from'small' (300 epochs) and 'large' (250 epochs) in Table 2 when varying test time input resolution between 192 and 384. Note that all models have been trained on 224x224 images only, and no fine-tuning to different resolutions is done. Also, for ViTs we have to adapt the models via interpolation of the positional embedding as suggested by . Fig. 3 shows that all classifiers obtain their highest robustness for image resolutions of 256 or 288, higher than the training one. For faster evaluation we use for this plot APGDT-DLR with 40 iterations and 3 target classes. Then, we select for each model the best input resolution, and evaluate \(_{}\)-robustness with AutoAttack in Table 3. Increasing input resolution significantly improves clean and \(_{}\)-robust accuracy. Our ConvNeXt-B + ConvStem attains SOTA 76.9% and 57.3% clean and \(_{}\)-robust accuracy, respectively, at resolution 256. Additional results on the role of the input resolution can be found in App. D.5, including a discussion of what it means for threat models other than \(_{}\).

**Varied perturbation strength.** In Table 5, we test the effect of varying the test time resolution on the robustness in \(_{}\) for radii other than \(4/255\) used for training (we use our ConvNeXt-B + ConvStem, with robustness evaluation via AutoAttack on the RobustBench ImageNet set). For all radii \(_{}\), the best resolution is either 256 or 288, that is larger than the training resolution of 224, which supports

Figure 3: **(Robust) accuracy improves with increasing test-time image resolution.** Evaluation of clean (left) and \(l_{}\)-robust accuracy (right, evaluation using APGDT-DLR) at \(_{}=4/255\) for the top models from Table 1 for varying test input resolution.

 \(_{}\) &  \\   &  & 224* &  & 288 & 320 \\  clean & 74.1 & -1.8 & 75.9 & 76.9 & +1.0 & **77.7** & +1.8 & 77.2 & +1.3 \\ \(2/255\) & 64.6 & -2.3 & 66.9 & 67.9 & +1.0 & **68.6** & +1.7 & 68.4 & +1.5 \\ \(4/255\) & 53.0 & -3.2 & 56.1 & **57.3** & +1.2 & 57.2 & +1.1 & 56.6 & +0.5 \\ \(6/255\) & 41.0 & -2.8 & 43.8 & 44.4 & +0.6 & **44.5** & +0.7 & 43.0 & -0.8 \\ \(8/255\) & 29.5 & -0.9 & 30.4 & **31.0** & +0.6 & 29.8 & -0.6 & 27.9 & -2.5 \\  

Table 5: **Increased resolution across perturbation strengths.** For ConvNeXt-B+ConvStem, we see that both best clean and \(_{}\) robust accuracies are attained a a higher resolution than the one trained for (224) across all values of \(_{}\). The difference from base value at the resolution of 224 is shown in color and the best result for each perturbation strength (row) is **highlighted.

the results above. Unlike for the smaller radii, at \(_{}=8/255\) the robust accuracy at both resolutions 288 and 320 is worse than at 224. This suggests that there are diminishing returns for higher radii. The attack strength increases more quickly with the perturbation budget at higher resolution than the improvements in clean accuracy. While for the \(_{}\)-threat model the comparison across different image resolutions can be done directly, the comparison for the \(_{2}\)-threat model is more subtle and we discuss potential ways to do this in App. D.5. If one keeps the perturbation budget per pixel constant, that means increasing \(_{2}\) linearly with the image resolution, then the gains in \(_{2}\)-robustness are marginal.

**Fine-tuning to larger radius.** While \(=4/255\) is the standard radius for the \(_{}\)-threat model on ImageNet, even larger perturbations might still be imperceptible.  train XCiTs with \(=8/255\) from scratch for 110 epochs. However, given the success of using a warm start for adversarial training (see Sec. 4.1), we propose to fine-tune the model fully trained for robustness at \(=4/255\) with adversarial training with the larger radius. Such classifiers already have non-trivial robustness at \(=8/255\), and we expect they require only relatively small changes to adapt to the larger radius. We use our models from Table 2 to fine-tune for 25 epochs (details in App. A.5), and report the results in Table 4: our ConvNeXt-B + ConvStem improves significantly over the previous best result both in terms of clean and robust accuracy, even though we just use relatively short fine-tuning.

**Fine-tuning to other datasets.** We fine-tune our models trained to be robust at radius \(8/255\) from Table 4 to other datasets and compare to the results of  in Table 6. For Flowers-102 (resolution \(224 224\)) the improvement over XCiT in robust accuracy is similar to the difference of the models in Table 4 and shows that the benefits of more robust models transfer to smaller datasets. For CIFAR (resolution \(32 32\)) our models tend to achieve similar robustness as the XCiT models but at a much better clean accuracy. We expect that here even stronger robustness gains are possible by further optimizing the fine-tuning. For details on the fine-tuning setup see App. D.6.

## 7 Conclusion

We have examined two extreme representatives of today's modern architectures for image classification, ViT and ConvNeXt, regarding adversarial robustness. We have shown that ConvNeXt + ConvStem yields SOTA \(_{}\)-robust accuracy as well as good generalization to unseen threat models such as \(_{1}\) and \(_{2}\) across a range of parameters and FLOPs. Given that for smaller model sizes the ConvNeXt + ConvStem is much easier to train than the ViT, it has the potential to become similarly popular for studying adversarial robustness on ImageNet like (Wide)-ResNets for CIFAR-10. On the other hand ViT + ConvStem excels in generalization to unseen threat models. Thus there is no clear winner between ViT and ConvNeXt. Given that ImageNet backbones are used in several other tasks, we think that our robust ImageNet models could boost robustness also in these domains. One such example is our recent work in robust semantic segmentation . Another interesting open question is why the ConvStem leads to such a boost in generalization to \(_{1}\)- and \(_{2}\)-threat models.

**Limitations.** Given the extensive set of experiments already presented in this work and our limited computational resources, we were unable to include other modern architectures. Hence, even though we cover quite diverse architectures, we do not cover all modern architectures for ImageNet.

**Broader Impact.** As this work studies the robustness of deep neural networks to adversaries, we do not see any potential negative impact of our work. Instead, our work could help make modern deep models robust and safer to deploy.

 Architecture &  &  &  \\  & clean & \(_{}\) & clean & \(_{}\) & clean & \(_{}\) \\  XCiT-S & **82.86** & 47.91 & 67.34 & 32.19 & 90.06 & **56.14** \\ CN-T + CvSt & 80.42 & **50.89** & **72.32** & **32.30** & **92.72** & 55.97 \\  XCiT-L & – & – & 70.76 & **35.08** & 91.73 & 57.58 \\ CN-B + CvSt & 83.98 & 54.77 & **74.83** & 34.24 & **93.55** & **58.17** \\  

Table 6: \(_{}\)**-Robust accuracy at radius \(_{}=8/255\).** We examine fine-tuning our ConvNeXt + ConvStem models robust to \(_{}=8/255\) from Table 4 to other datasets. We compare to the XCiT models of  when available.