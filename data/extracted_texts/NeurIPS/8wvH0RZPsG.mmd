# Conformalized Multiple Testing after Data-dependent Selection

Xiaoning Wang\({}^{1}\)1  Yuyang Huo\({}^{1}\)1  Liuhua Peng\({}^{2}\)2  Changliang Zou\({}^{1}\)2

\({}^{1}\)School of Statistics and Data Sciences, LPMC, KLMDASR and LEBPS,

Nankai University, Tianjin, China

\({}^{2}\)School of Mathematics and Statistics, The University of Melbourne, Melbourne, Australia

1120220048@mail.nankai.edu.cn,huoyynk@gmail.com

liuhua.peng@unimelb.edu.au, zoucl@nankai.edu.cn

Equal contribution.Correspondence to: Liuhua Peng <liuhua.peng@unimelb.edu.au>, Changliang Zou <zoucl@nankai.edu.cn>.

###### Abstract

The task of distinguishing individuals of interest from a vast pool of candidates using predictive models has garnered significant attention in recent years. This task can be framed as a _conformalized multiple testing_ procedure, which aims at quantifying prediction uncertainty by controlling the false discovery rate (FDR) via conformal inference. In this paper, we tackle the challenge of conformalized multiple testing after data-dependent selection procedures. To guarantee the construction of valid test statistics that accurately capture the distorted distribution resulting from the selection process, we leverage a holdout labeled set to closely emulate the selective distribution. Our approach involves adaptively picking labeled data to create a calibration set based on the stability of the selection rule. This strategy ensures that the calibration data and the selected test unit are exchangeable, allowing us to develop valid conformal p-values. Implementing with the famous Benjamini-Hochberg (BH) procedure, it effectively controls the FDR over the selected subset. To handle the randomness of the selected subset and the dependence among the constructed p-values, we establish a unified theoretical framework. This framework extends the application of conformalized multiple testing to complex selective settings. Furthermore, we conduct numerical studies to showcase the effectiveness and validity of our procedures across various scenarios.

## 1 Introduction

In recent years, there has been a notable focus on the use of predictive models to distinguish specific individuals from a pool of candidates. For instance, in the field of financial investment [17; 2], machine learning models can be used to predict profits for different investment opportunities. Candidates with high predicted profits can then be given more preference and considered as potential investment options. Similarly, in disease diagnostics [29; 45], researchers can utilize relevant information and corresponding predictions from machine learning models to identify potential patients.

In a typical scenario, we are presented with a labeled/holdout data set \(_{c}=\{Z_{i}=(X_{i},Y_{i})\}_{i=1}^{n}\), where \(X_{i}^{d}\) is the observed covariate and \(Y_{i}\) is the response, and an unlabelled/test set \(_{u}=\{X_{i}\}_{i=n+1}^{n+m}\). In practice, we only observe the covariates in the test set \(_{u}\) and the responses \(Y\) are unknown. Our goal is to distinguish individuals in \(_{u}\) whose undisclosed responses fall within a predetermined region \(\). Region \(\) can take various forms, such as \((b,)\), \([a,b]\) and \((-,a)\) per user's requirements. To estimate the value of \(Y\) for the identification, we employ a predictive model \(:^{d}\). However, directly using the black-box prediction \((X_{i})\) as a substitute for \(Y_{i}\) leads toinherent uncertainty. In order to quantify this uncertainty, we reformulate our problem as multiple hypothesis testing : for \(j=\{n+1,,n+m\}\),

\[H_{0,j}:Y_{j} H_{1,j}:Y_{j}.\]

Under this framework, we wish to make rejections as much as possible with the false discovery rate (FDR) controlled at a pre-given level \(\). Denote the index set of test data as \(=\{n+1,,n+m\}\). The FDR is defined as the expectation of false positive proportion (FDP) over the test units in \(\), i.e.

\[()=[()], ()=}\{j ,Y_{j}\}}{1||}.\]

where we denote \(a b=\{a,b\}\) for any \(a,b\), \(||\) as the cardinality of a set \(\) and \(\) as the rejection set. To derive the testing rule, one feasible method is to construct the conformal p-value  by ranking the nonconformity score associated with the test unit's prediction among the scores in the holdout set. Then we can apply the well-known Benjamini-Hochberg (BH) procedure  on these conformal p-values to obtain a rejection set with controlled FDR. Here we term this procedure as "conformalized multiple testing".

In practice, researchers may be interested in specific subsets rather than analyzing the entire dataset. For example, they might aim to determine the presence or absence of lung cancer among heavy smokers. By establishing data-driven criteria or thresholds based on factors such as the daily number of cigarettes smoked and smoking history, researchers can create a filtered subset consisting of heavy smokers. This allows researchers to gain insights into the patterns of the medical condition within this particular group. Here we address that the group partition may not be predetermined and could instead be learned from data, through methods like clustering  or thresholding. Denote the selected subset from unlabelled data as \(}_{u}\). In our paper, we aim to find a rejection set \(}_{u}}_{u}\) with the following FDR criterion controlled at \(\), i.e.

\[(}_{u})=[}_{u}}\{j}_{u},Y_{j}\}}{1|}_{u}|}].\]

For simplicity, we use FDR to denote \((}_{u})\) only. The selection procedure would distort the distribution of test statistics, invalidating the p-values and leading to the failure of FDR control. This falls into the category of selective inference, which has been addressed in both statistics and machine learning fields [53; 16]. To tackle the selective issue, the use of labeled data becomes crucial. By leveraging labeled data, it is possible to obtain the conditional distribution of the selected data, which in turn allows for the construction of valid p-values.

Nevertheless, even with valid p-values, controlling the FDR proves to be a challenging task. This difficulty arises from the inherent randomness of the selected subset \(}_{u}\). Even a minor disturbance in \(}_{u}\) can lead to significant changes in the final rejection set \(}_{u}\). Consequently, in order to address this issue, we focus on several commonly used selection rules with certain selection stability. We employ an adaptive strategy to carefully choose labeled data, thereby creating a calibration set that takes into account the selection stability.

### Our contributions

In this paper, we construct the _selective conformal p-value_ for each selected individual, built on the marginal conformal p-value . To address the selection effects, we adaptively pick a calibration set from the labeled data according to the selection rule, to ensure the exchangeability between the test data and labeled data. The selective conformal p-values are then constructed using the picked calibration set. By combining the selective conformal p-values with the well-known BH procedure, we achieve FDR control after data-driven selection, as verified through our comprehensive analysis.

The main contributions of our paper can be summarized as follows.

* Firstly, we frame the problem of multiple testing after data-dependent selection in the predictive setting and propose a viable solution utilizing the labeled data.
* Secondly, the proposed method achieves exact FDR control for selection rules with strong stability, including joint-exchangeable rules and the top-K selection. And we further extend our method to handle more general cases where the selection rules satisfy a weaker stability condition such as sample mean selection.
* Thirdly, the theoretical advancement extends the scope of classic multiple testing into the selective setting, providing a unified analytical technique for handling the randomness arising from data-driven selection.
* Finally, through extensive experiments, we evaluate the reliability of our method in delivering the desired FDR control, and emphasize its easy integration with various algorithms.

### Connections to existing works

**Multiple testing** Ever since the seminal work of Benjamini and Hochberg , the framework of multiple testing has been well developed by many researchers [49; 5; 66; 20; 44]. Our paper connects to the area of two-stage testing , which firstly selects a subset of hypotheses and subsequently applies a multiple testing procedure to the selected set. To maintain the validity of test statistics after the selection, Bourgon et al.  recommended using an independent statistic specifically for the purpose of selection, but it is unavailable in our predictive setting. Instead of assuming independence between the test statistic and the selection statistic, Du and Zhang  introduced the concept of a "single-index" p-value for the joint modeling of both statistics and provided FDR control under symmetry assumption. Besides, Efron  considers applying multiple testing procedures over pre-given groups to guarantee group-wise FDR control, while our work considers the FDR control over data-driven subgroups, which is more challenging.

**Conformal inference** Conformal inference [57; 40; 56; 55] has garnered significant attention in recent years, which leverages data exchangeability to construct model-agnostic prediction intervals. We present some recent developments therein [39; 6; 59; 61; 50; 18; 15; 1]. Within the conformal inference framework, several studies have focused on controlling the FDR in predictive setting, i.e. the _conformalized multiple testing_[8; 26]. These studies involve constructing a valid testing procedure using a holdout set. Such procedures include the BH procedure based on conformal p-values [27; 35; 34; 24], thresholding via an FDP estimator [63; 41] and the e-BH procedure applied to generalized e-values [7; 64]. Different from them, we focus on a selective scenario in conformalized multiple testing, addressing new challenges arising from selective randomness.

**Selective inference** Selective inference concerns the inference problem after data-dependent processing. Previous works have mainly focused on the inference of parameters [60; 31]. Recently, Bao et al.  extended selective inference to the realm of conformal inference. They proposed a method to construct selection conditional prediction intervals with controlled false coverage-statement rate (FCR)  after data selection. Building upon this work, selective conformal inference with FCR control has been further extended to accommodate more general selection rules [23; 28] or the online setting . In particular, under a certain class of selection rules, Gazin et al.  involves a procedure for FDR control after selection which closely aligns with our method. However, their approach focuses on selecting an informative set with FCR control under specific selection assumptions, limiting its applicability in more general scenarios such as those with data-dependent selection. The problem we tackle presents additional complexities, due to the intricate dependence on the selection procedure and final decision procedure, requiring a more intricate and delicate analysis. Besides, Sarkar and Kuchibhotla  proposed a post-selection framework to guarantee simultaneous inference  for all coverage levels, which differs greatly from our scenarios.

## 2 Methodology

### Recap: conformalized multiple testing

We first introduce how to make multiple testing in the predictive setting. Denote the index sets for the labelled data \(_{c}\) as \(=\{1,,n\}\). Suppose \(()\) is a predictor via a machine learning algorithm that is pre-given or can be trained on extra labeled data. Thus we can treat \((x)\) as fixed. To construct a valid test statistic based on \((X_{j})\), Bates et al.  considered to use conformal p-values built upon the conformal inference framework . Consider a monotone transformation \(V\) such that the larger value of \(V((X_{j}))\) indicates the bigger likelihood of \(Y\). For example, if \(=(b,)\), we can use \(V(y)=b-y\). The marginal conformal p-value \(p_{j}^{ M}\) for \(X_{j}\) is defined as

\[p_{j}^{ M}=_{0}:V_{i} V_{j}\}|}{1+|_ {0}|}, j;\] (1)

where we denote \(V_{j}=V((X_{j}))\) as the nonconformity score for \(j\)-th sample and \(_{0}=\{i:Y_{i}\}\) as the index set of labeled set containing only null samples.

The properties of marginal conformal p-value constructed by i.i.d.\(\)labeled and test data have been investigated by Bates et al.  and we present them in the following proposition. Proposition 2.1 (i) guarantees that marginal conformal p-value is superuniform, thus it is a valid p-value. As the conformal p-values have a nice dependence structure, the rejection set obtained by the famous BH procedure  enjoys valid FDR control Proposition 2.1 (ii) indicates. For a set of p-values \(\{p_{i}\}_{i=1}^{m}\), the BH procedure finds \(k=\{j:p_{(j)} j/m\}\) where \(p_{(j)}\) denotes the \(j\)-th smallest p-value in \(\{p_{j}\}_{j=1}^{m}\) and obtain the rejection set as \(}_{u}=\{j:p_{j} k/m\}\).

**Proposition 2.1** (Properties of the conformal p-value ).: _Suppose the labeled data and test data are i.i.d.. For simplicity, we assume \(_{0}=\{j:Y_{j}\}=\{n+1,,n+m_ {0}\}\) for \(m_{0} m\). The conformal p-values in (1) satisfy:_

1. _The_ \(p_{j}^{ M}\) _is a marginally superuniform p-value, i.e. for any_ \(t\)_,_ \((p_{j}^{ M} t j_{0}) t\)_._
2. _Furthermore, the BH procedure applied at level_ \(\) _on the conformal p-values_ \(\{p_{j}^{ M}\}_{j}\) _controls the FDR level at_ \(\)_, where_ \(\) _is the null proportion of test samples._

The property of the conformal p-value is obtained by the exchangeability between the labeled data and test data. Through this, we can approximate the distribution of \(V_{j}\), where \(j_{0}\), using \(V_{i}\) from \(i_{0}\). Therefore, when the labeled data and test data have different distributions, maintaining the exchangeability becomes crucial in constructing valid conformal p-values.

As Storey  suggested, we can further estimate the null proportion and incorporate it in the BH procedure to further increase detection power. With the aid of labeled data, the null proportion can be directly estimated by the corresponding proportion in the labeled set, i.e. \(=|_{0}|/||\).

### Selective conformal p-value

A selection procedure could possibly be employed to the test samples. In this case, the focus lies primarily on the selected subgroup rather than the entire dataset, and decisions are made solely based on this subset. Define the selection rule as \(_{_{c},_{u}}\), which is a function of labeled set \(_{c}\) and test set \(_{u}\). For simplicity, we may omit this subscript. The selection rule maps an individual point \(X\) into a selection decision \(\{0,1\}\). And the selected subset can be written as \(}_{u}=\{j:(X_{j})=1\}\).

There are many examples for data-dependent \(\). The \(\) can be the clustering algorithm which automatically determines the subgroup. Alternatively, \(\) is associated with the selection score \(T_{j}\), which is derived from certain components of \(X_{j}\), and a selection threshold \(\), such as the sample mean value in \(\{T_{j}\}_{j}\). In this case, we can express \(\{(X_{j})=1\}=\{T_{j}<_{k}T_{k}\}\).

After the selection procedure, we would make multiple testing on the selected subset \(}_{u}\). However, the distribution of the selected conformal p-values in (1) would be distinct from the original ones, due to the selection effects. Consequently, directly running the BH procedure on the marginal conformal p-values (1) in selected set \(}_{u}\) has no guarantee, which may lead to an inflated FDR level or poor power. Addressing this issue raises two important considerations:

* How to characterize the selection conditional distribution of the selected individuals to construct valid p-values?
* How to take account of the dependence structure among the valid p-values and the stochastic nature of the selection event to design a trustful multiple testing procedure?

The first issue is widely considered in post-selection inference. Previous literature heavily relies on the normality assumption to derive the conditional distribution of test statistics [32; 52]. By the spirit of conformal inference, we consider constructing the selective conformal p-value by picking up the calibration set via the same selection rule, thereby guaranteeing the exchangeability between the selected test unit and picked calibration data.

To be specific, we employ the selective algorithm \(\) on the labeled set to derive the picked calibration set \(}_{c}=\{i:(X_{i})=1\}\). If \(\) involves a selection threshold \(\), we will choose the calibration set as \(}_{c}=\{i:T_{i}\}\). We hope \(\{V_{i}:i}_{c}\} V_{j}\) for \(j}_{u}\) exhibits a certain level of exchangeability, enabling us to capture the distribution of \(V_{j}\).

With the aid of \(}_{c}\), the selective conformal p-value can be accordingly constructed as:

\[p_{j}:=}_{c}_{0}:V_{i} V_{j }\}|}{1+|}_{c}_{0}|},j}_{u}.\] (2)

After obtaining valid p-values, ensuring the guarantee of the BH procedure is not straightforward due to the dependence arising from the use of the same calibration set in computing conformal p-values and the randomness from the data-dependent selection. Therefore, the second concern needs to be carefully addressed. In this article, we examine the BH procedure applied to the selective conformal p-values can enjoy finite sample FDR control for several commonly used selection rules. We outline our procedure in Algorithm 1 and refer to our method as Selective Conformal P-Value (SCPV).

```
0: Labeled set \(_{c}\), test set \(_{u}\), selection procedure \(_{_{c},_{u}}\), prediction model \(()\), target FDR level \((0,1)\). Step 1 (Selection) Apply the selective procedure \(_{_{c},_{u}}\) to obtain the selected subsets \(}_{u}\) and \(}_{c}\). Step 2 (Calibration) Compute \(\{V_{i}:i}_{c}_{0}\}\), \(\{V_{j}:j}_{u}\}\). Step 3 (Construction) Construct selective conformal p-value for each \(j}_{u}\) as (2) Step 4 (BH procedure) Compute \(k^{*}=\{k:_{j}_{u}}(p_{j} k/m)  k\}\)
0: Rejection set \(}_{u}=\{j}_{u}:p_{j} k^{*}/m\}\). ```

**Algorithm 1** Selective conformal p-value with BH procedure (SCPV)

## 3 Theoretical guarantee

In this section, we aim to verify the FDR guarantee of Algorithm 1 for several commonly used selection rules. Our focus here is to tackle the technical challenges associated with the selective multiple testing problem. Unlike the conventional approach where a fixed number \(m\) of test units is considered, we encounter a challenge due to the involvement of a random number of test units \(|}_{u}|\). This randomness makes the analysis considerably intricate unless we impose certain restrictions on the selection rule. To deal with the selection set \(}_{u}\), we introduce the concept of strong stability.

**Definition 3.1** (Strong stability).: Given selection set \(}_{u}=\{j:_{_{c},_{u}}(X_{j})=1\}\). The selection rule \(_{_{c},_{u}}\) is strongly stable if either of the conditions holds: for any \(i\) and \(j}_{u}\)

* (Leave out) \(_{_{c},_{u}}(X_{i})=_{_{c }\{Z_{j}\},_{u}\{Z_{j}\}}(X_{i})\);
* (Replace) \(_{_{c},_{u}}(X_{i})=_{_{c },_{u}\{Z_{j}\}\{z\}}(X_{i})\) for a fixed point \(z\).

Here we define the strong stability of selection rule in two common ways: leaving one point out or replacing one point with a fixed value. Many popular selection rules are strongly stable, such as joint-exchangeable rule and top-K selection. Detailed discussions are provided in next subsections.

The strong stability plays a crucial role in our analysis, as it enables us to fix the randomness of the selected set \(}_{u}\). With the strongly stable property, we can perform a delicate analysis for the rejection set from Algorithm 1 to obtain the theoretical guarantee.

**Theorem 3.2**.: _Suppose the data are i.i.d. and the selection rule \(_{_{c},_{u}}\) is strongly stable. Then the selective conformal p-values defined in (2) satisfies \((p_{j} t|j}_{u},j_{0}) t\), and the output \(}_{u}\) of Algorithm 1 satisfies \([|}_{u}_{0 }|/|}_{u}|]\)._We present the insight of our proof for Theorem 3.2. As a common operation in analyzing FDR [22; 35], we decompose the FDR into the FDR contribution for each \(j\) as

\[=[}\{j }_{u},j_{0}\}}{1|}_{u}|} ]=_{j}[}_{u}|}{|}_{u}|},j}_{u},j _{0}\}}{1|}_{u}|}].\]

By the stability of selection rule, we can replace \(|}_{u}|\) with a decoupled version \(|}_{u}^{(j)}|\) which removes the influence of \(p_{j}\). If given some quantity \(_{j}\) that blocks most of the nuisance parameters, the p-value \(p_{j}\) has a uniform distribution and \(|}_{u}^{(j)}|\), \(|}_{u}|\) are fixed. Then the FDR control is by

\[= _{j}[[1\{p_{j} }_{u}^{(j)}|}{|}_{u}|},j }_{u},j_{0}\}_{j}]}{1|}_{u}^{(j)}|}]\] \[ _{j}[}_{u}^{(j)}|}{|}_{u}|}}_{u}^{(j)} |}\{j}_{u},j_{0}\}]= [}_{u}_{0}|}{|}_{u}|}]\]

By the construction of the conformal p-value, we analyze each selected unit \(j}_{u}\) by conditioning on a carefully constructed quantity \(_{j}=(_{\{j\}}^{*},_{ \{j\}})\). It comprises two components: \(_{\{j\}}\), the test data with the \(j\)-th sample excluded, and \(_{\{j\}}^{*}:=[Z_{i};i\{j\}]\), the unordered set of labeled data along with the \(j\)-th sample. The unordered set provides the order statistics but not the specific ordering, which is a common convention in conformal inference literature [35; 34].

Through this approach, we are able to decouple the dependence that arises from the data-dependent selection and the construction of p-values that share the same calibration data. If the selection rule is strongly stable, then \(|}_{u}|\) is fixed given \(_{j}\) for \(j}_{u}\) and the selective conformal p-value in (2) is valid. By performing a careful analysis of the rejection set \(}_{u}\), i.e. replacing it with a pseudo rejection set \(}_{u}^{(j)}\) that remains fixed given \(_{j}\) and \(j}_{u}\), we obtain the finite sample FDR guarantee.

### Joint-exchangeable selection

Firstly, we consider the joint-exchangeable selection procedure. The joint-exchangeable selection procedure is applied to \(\{X_{i}:i\}\) with exchangeability, i.e. the selection results remain unchanged after any permutation of data in the merged set \(\), as Definition 3.3 indicates.

**Definition 3.3** (Joint-exchangeable selection).: The selection procedure \(\) is joint-exchangeable with respective to the \(\{X_{i}:i\}\) if

\[_{_{c},_{u}}(X_{i})=_{_{k},_{l}}(X_{i})i_{k}, _{l}_{c}_{u}.\]

If the selection procedure is independent of both the labeled and test data, it is joint-exchangeable. In the case of the selection with a threshold, the joint-exchangeable selection is equivalent to

\[(X_{1},,X_{||})=(X_{(1)},,X_{ (||)}).\]

where \(()\) denotes that \(\) is computed using the dataset \(\). Therefore, the joint-exchangeable selection includes selection using constant thresholds or thresholds computed by \(\{T_{i}\}_{i}\) exchangeablely. We can verify joint-exchangeable selection is strongly stable through the leaving out condition.

**Proposition 3.4**.: _The joint exchangeable selection procedure \(_{_{c},_{u}}\) is strongly stable._

According to Theorem 3.2, our procedure ensures FDR control for any joint-exchangeable selection rule, which makes the choice of selection rule quite flexible. For example, we can perform a clustering algorithm on the \(\{X_{i}\}_{i}\) to divide the data into different groups. As a special case, our approach aligns with the InfoSCOP proposed by Gazin et al.  under the joint-exchangeable selection rule. They proposed a novel procedure for selecting an informative set and also provided FDR control guarantee as an extension of their FCR control results.

While the joint-exchangeable rule contains various selection strategies, it is worth noting that many cases involve selection that depends solely on the test data. In the following subsections, we investigate several commonly used selection rules that are determined only by the test data. And the assumption in InfoSCOP  is not satisfied under these cases.

### Top-K/Quantile selection

Next, we consider the top-K or quantile selection rule, which relies solely on the test data \(_{u}\). This type of rule is extensively studied in the literature [42; 3; 28] and is commonly used in practice.

Let \(_{}\) denote the top-K selection threshold, which is defined as the \((K+1)\)-th smallest value in \(\{T_{j}:j\}\). The top-K rule is equivalent to the quantile selection rule since \(_{}\) corresponds to the \((K+1)/m\)-quantile of the test data, and the threshold for the \(q\)-quantile is the \( mq\)-th smallest value. The selected test set and the chosen calibration set under the top-K rule are defined as:

\[}_{u}=\{j:T_{j}<_{}\},}_{c}=\{i:T_{i}<_{ }\}.\] (3)

We can verify that the top-K selection rule is strongly stable by the following proposition. With the support of Theorem 3.2, we can ensure FDR control when employing the top-K selection rule.

**Proposition 3.5**.: _For top-K selection rule \(\) with threshold \(_{}()\), if \(j}_{u}\), then_

\[_{}(T_{n+1},,T_{j-1},T_{j},T_{j+1},,T_{n+m})= _{}(T_{n+1},,T_{j-1},-,T_{j+1},,T_{n+m}).\]

_Thus top-K selection rule is strongly stable by the replacing condition._

### General extension to weakly stable selection

In this subsection, we consider weakening the strongly stable condition. For example, the mean thresholding rule does not satisfy the strong stability. By the insight of our proof, the key requirement is the property of \(}_{u}\) such that we can handle the randomness of the selection event. Hence we define the weakly stable selection rule as follows:

**Definition 3.6** (Weak stability).: Given selection set \(}_{u}=\{j:_{_{c},_{u}}(X_{j})=1\}.\) We call the selection rule \(_{_{c},_{u}}\) is weakly stable if

\[_{_{c},_{u}}(X_{i})=_{_{ c}\{Z_{j}\},_{u}\{Z_{j}\}}(X_{i})j}_{u}i.\]

The weak stability does not require the \(_{_{c},_{u}}(X_{i})=_{_{ c}\{Z_{j}\},_{u}\{Z_{j}\}}(X_{i})\) hold for \(i\). Except for the selection rules previously discussed, the commonly used mean selection rule by test data only is also weakly stable, i.e. \(\{j:T_{j}<_{i}T_{i}\}=\{j :T_{j}<_{i\{j\}}T_{i}\}\).

As the weakly stable rule fails to guarantee the exchangeability of the selected calibration set and test set, it motivates us to explore a new construction method for selective p-values. The selection set is denoted as \(}_{u}=\{j:_{_{c},_{u}}(X_{j})=1\}\). By the definition of weakly stable selection, we know that \(}_{u}=\{j:_{_{c}\{Z_{j }\},_{u}\{Z_{j}\}}(X_{i})=1\}\) is also true. Specially, if the selection rule is determined only by the test data, it holds that \(_{_{c}\{Z_{j}\},_{u}\{Z_{j}\}}( )=_{_{u}\{Z_{j}\}}()\).

Therefore, we adaptively pick data from the labeled set using the same "leaving out" rule as selecting \(Z_{j}\). For any \(i\) and \(j\), \(\{_{_{u}\{Z_{j}\}}(X_{i})=1\}\) and \(\{_{_{u}\{Z_{j}\}}(X_{j})=1\}\) are symmetric to \(X_{i}\) and \(X_{j}\). Leveraging this, we can pick up the calibration set by

\[}_{c}(j)=\{i:_{_{u} \{Z_{j}\}}(X_{j})=1\}j}_{u}\]

and construct the adaptive selective conformal p-value as

\[p_{j}^{}:=}_{c}(j)_{0}:V_{i} V_{j}\}|}{1+|}_{c}(j)_{0}|}.\] (4)

For example, the mean selection rule picks up calibration set by \(}_{c}(j)=\{i:T_{i}_{k \{j\}}T_{k}\}\). Our adaptive strategy shares the same goal as the swapping strategy [28; 2.4] in terms of constructing valid p-value after selection. However, our approach is different from the others in core motivations since ours is directly related to weak stability, leading to a faster computation and a more intuitive explanation here. We can verify that \(p_{j}^{}\) is valid since \(\{Z_{k}\}_{k}_{c}(j)\{j\}}\) are exchangeable.

**Proposition 3.7**.: _The adaptive selective conformal p-value \(p_{j}^{}\) for weakly stable selection which is determined only by the test data satisfies \((p_{j}^{} t j}_{u},j_{0}) t\)._However, the p-value for each selected test point \(p_{j}^{ adapt}\) is based on a different calibration set \(}_{c}(j)\), making the dependence structure intricate. Consequently, the BH procedure has no safe guarantee to control the FDR. But we find that BH is robust and can produce satisfactory results empirically.

Although this observation is acceptable, it would be desirable to design a new procedure to guarantee the finite sample FDR control. To remedy this, we employ the conditional calibration framework  to achieve finite sample FDR control. The overall procedure can be re-framed as the e-BH framework  and a recent novel approach for boosting the power of e-BH procedure  can be employed in our setting. The details are displayed in Appendix B.

Under the weakly stable selection rule, our method differs fundamentally from InfoSCOP  in both methodology and theory. Since our approach and InfoSCOP are designed for different goals, resulting in different analytical frameworks. Ours is specifically designed to address the multiple testing problem across various selection rules. From the perspective of conditional calibration, our method is unified, where the BH procedure for strongly stable selection can be seen as a special case. As a comparison, InfoSCOP stands out as a remarkable work for selecting an informative set with FCR control, but it is not primarily designed for our problem, which limits their method's applicability to more general selection rules.

## 4 Numerical studies

To demonstrate the wide applicability of the proposed method in Algorithm 1, we conduct comprehensive numerical studies. For regression setting, the region for the hypothesis is \(=\{y:y>c_{0}\}\), where \(c_{0}\) is a fixed constant. For classification, we set \(=\{1\}\), i.e. the class 1 as the target region, and we denote the prediction \((X)\) as the predicted probability of \(Y=1\). The nonconformity score we use to construct conformal p-value for both settings is \(V((X_{j}))=-(X_{j})\).

Benchmarks:Since selective multiple testing has not been investigated before, we consider several intuitive methods as comparing benchmarks.

* SCPV: Our procedure in Algorithm 1. Specially, for mean selection rule, we use the adaptive p-value in (4) along with the BH procedure. The results for using conditional calibration can be found in Appendix B;
* OMT: Ordinary multiple testing which constructs the conformal p-value directly as in equation (1) for each selected sample based on the entire labeled set.
* AMT (BH/BY): An intuitive procedure by multiplying (1) with the selection proportion of null samples. As the adjusted p-values have intractable dependence, making the validity of BH procedure suspicious, we also utilize the Benjamini-Yekutieli (BY)  procedure to control the FDR. More details are provided in Appendix A.1.
* SCOP: Directly invert the selective prediction interval constructed by Bao et al.  into a test and make decision by whether the \(c_{0}\) is contained in the interval. It is designed for regression setting and does not have FDR guarantee. See more detail in Appendix A.2.

We also use the Storey's method  to increase power. See more information in Appendix C.1.

Selection rule:In the numerical studies, we choose the selection statistic \(T_{i}\) based on a specific component of \(X\). The selected subset is \(}_{u}=\{i:T_{i}<\}\), where \(\) is the threshold. Three different choices of selection thresholds are considered.

* **Exchangeable (Exch):** 70%-quantile of the first component of \(X\) in both labeled set and test set, that is \(\) is the 70%-quantile of \(\{T_{i}:i\}\).
* **Quantile (Quan):** 70%-quantile of the first component of \(X\) in the test set, that is \(\) is the 70%-quantile of \(\{T_{i}:i\}\).
* **Mean:** the sample mean of the first component of \(X\) in the test set, that is \(=_{j}T_{j}\).

Evaluation metrics:We empirically evaluate the FDR by averaging the FDP based on selected samples and the power by averaging the proportion of correct selections among all selected alternative test samples, i.e. \(:=|i}_{u}:i,Y_{i}>c_{0}|/|i }_{u}:Y_{i}>c_{0}|\) over \(100\) independent runs.

### Results on synthetic data

In synthetic studies, we generate i.i.d. 10-dimensional covariates from \(X_{i}([-1,1])^{10}\). The corresponding regression responses are generated as \(Y_{i}=(X_{i})+_{i}\), where \(_{i}\) denotes independent random noise. The following data-generating scenarios are considered:

* **Case A:** The data generating model is \((X)=4(X^{(1)}+1)|X^{(1)}|\{X^{(2)}>-0.4\}+4(X^{(1)}-1) \{X^{(2)}-0.4\}\). The noise is \(_{i} N(0,^{2})\), independent of \(X\). And \(c_{0}=2\).
* **Case B:**\((X)=\{X^{T}>1.5\}\), where \(=(1,-1,2,-2,0,0,0,0,0,0)\). The noise is \(_{i} N(0,0.1^{2})\), independent of \(X\). And \(c_{0}=0.12\).

We fix the labeled data size \(n=1,200\) and the unlabeled data size \(m=1,200\). We fit the regression models \(()\) on an additional labeled set with size \(1,200\) using the random forest algorithm, implemented by R package randomForest with default parameters. Specifically, for both scenarios, we select the first component of \(X\) as the selection statistic, i.e., \(T_{i}=X_{i}^{(1)}\).

Figure 1 displays the FDR (left) and power (right) through varying noise strength. Across both settings, SCPV can deliver valid FDR control. As expected, the OMT fails to control FDR. This can be understood since the OMT constructs conformal p-values without consideration of the selection procedure, leading to smaller p-values possibly. Moreover, our method demonstrates greater statistical power compared to AMT. This is because AMT does not make full use of information from the selection procedure. Meanwhile, SCPV fails to control FDR in case B. And even if SCPV can control FDR in case A, the accompanying loss of power is substantial. This is because the SCPV is not designed for multiple testing and can not deliver valid FDR results.

### Results on real data

We consider several real data experiments including both regression (Reg) and classification (Cla) settings. We summarize the datasets in Table 1. The test samples and labeled samples are constructed by subsampling the dataset with \(n=1000\) and \(m=2000\), and the null proportion is fixed by \(=0.8\). We sam

    & Abalone & Census & Credit & Promotion \\  \#Features & 8 & 14 & 30 & 12 \\ \#Instances & 4,177 & 48,842 & 284,808 & 54,809 \\ Task & Reg & Cla & Cla & Cla \\   

Table 1: Summary of real-world datasets for conformalized multiple testing

Figure 1: Empirical FDR (left) and Power (right) of five methods under different scenarios and selection rules. The Noise Strength varies from \(0.1\) to \(1\). The black dashed line in the left plot denotes the target FDR level \(=10\%\).

ple another \(1000\) samples to train a random forest model for classification and regression. See more details in Appendix C.2. The results are reported in Table 2. The AMT(BY) outputs null rejection set in most cases, hence we omit it. As expected, SCPV achieves highest power among methods controlling the FDR, verifying its effectiveness and validity.

## 5 Limitations and discussions

Here we point out the current limitations of our paper and discuss the potential directions. First, our work relies on the i.i.d. assumption. Exploring the selective multiple testing problem in scenarios where the labeled set and test set exhibit different distributions would be interesting. Second, we require the selection rule to be stable for theoretical guarantee. It would be attractive to consider complex selection procedures that lack stability, such as clustering based on test data only.