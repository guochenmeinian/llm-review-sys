# An Adaptive Algorithm for Learning with Unknown Distribution Drift

Alessio Mazzetto

Brown University &Eli Upfal

Brown University

###### Abstract

We develop and analyze a general technique for learning with an unknown distribution drift. Given a sequence of independent observations from the last \(T\) steps of a drifting distribution, our algorithm agnostically learns a family of functions with respect to the current distribution at time \(T\). Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. Instead, the algorithm adapts to the sample data. Without explicitly estimating the drift, the algorithm learns a family of functions with almost the same error as a learning algorithm that knows the magnitude of the drift in advance. Furthermore, since our algorithm adapts to the data, it can guarantee a better learning error than an algorithm that relies on loose bounds on the drift. We demonstrate the application of our technique in two fundamental learning scenarios: binary classification and linear regression.

## 1 Introduction

Standard statistical learning models (such as PAC learning) assume independent and identically distributed training set, and evaluate the performance of their algorithms with respect to the same distribution as the training set (Vapnik, 1998; van de Geer, 2000; Shalev-Shwartz and Ben-David, 2014; Wainwright, 2019). However, in many practical applications, such as weather forecast, finance prediction or consumer preference analysis, the training data is drawn from a non-stationary distribution that drifts in time. In this work, we consider a more general setting where the samples are still independent, but their distribution can change over time. To obtain accurate results, the learning algorithm needs to adjust to the distribution drift occurring in the input.

This framework has been extensively studied in the literature (Helmbold and Long, 1991; Bartlett, 1992; Helmbold and Long, 1994; Barve and Long, 1996, 1997). This line of research culminated in showing that as long as the total variation distance of two consecutive distributions is bounded by \(\), there exists an algorithm that agnostically learns a family of binary classifiers with VC dimension \(\) with expected error \(O(()^{1/3})\)(Long, 1998), which can be shown to be tight. These results were generalized in the work of Mohri and Munoz Medina (2012) to address any family of functions with bounded Rademacher complexity and to use a finer problem-dependent distance between distributions called discrepancy, originally introduced in the context of domain adaptation (Mansour et al., 2009).

The core idea of the aforementioned work is to learn by using a number of previous samples that minimizes the trade-off between the error due to the variance of the estimation (_statistical error_), and the error due to the drifting of the samples with respect to the current distribution (_drift error_). If the algorithm trains using only a few recent observations, the statistical error will be large. If the algorithm uses a larger training set, including not very recent observations, the drift error will be large. For example, if the algorithm uses the most recent \(r\) training point, the hypothesis class has VC-dimension \(\), and the distribution drift in each step is bounded by \(\), then the statistical error is \(O()\) and the drift error is \(O(r)\). The trade-off with respect to \(r\) is optimized for \(r=((^{2}/)^{-1/3})\) giving \(O(()^{1/3})\) error, as mentioned before. This, as well as similar approachesin the literature, requires an upper bound to the magnitude of the drift and resolves the trade-off between the statistical and drift errors based on this knowledge. As noted in previous work (Hanneke and Yang, 2019), it is an open problem to develop an algorithm that adapts to the training set and does not rely on prior knowledge about the drift, whose solution would lead to the practical applicability of those ideas.

Our work resolves this open problem. Our algorithm does not require any prior knowledge of the magnitude of the drift, and it adapts based on the input data. Without explicitly estimating the drift (which is often impossible), the algorithm agnostically learns a family of functions with the same error guarantee as an algorithm that knows the exact magnitude of the drift in advance. Our approach has two advantages: it eliminates the, often unrealistic, requirement of having a bound on the drift, and it gives better results when the drift bounds are not tight. We showcase our algorithm in two important learning settings: binary classification and linear regression.

## 2 Preliminary

Let \((,)\) be a measurable space. Let \(Z_{1},,Z_{T}\) be a sequence of mutually independent random variables on \(\) distributed according to \(P_{1},,P_{T}\) respectively, i.e. \(Z_{t} P_{t}\) for \(t T\). For \(r T\), we denote by \(P_{T}^{r}\) the average distribution of the most recent \(r\) distributions \(P_{T-r+1},,P_{T}\):

\[P_{T}^{r}(A)_{t=T-r+1}^{T}P_{t}(A) A .\]

We set \(_{T}^{r}\) to be the corresponding empirical distribution over the random variables \(Z_{T-r+1},,Z_{T}\):

\[_{T}^{r}(A) A:T-r+1 t T\}|}{r}  A.\]

Let \(\) be a class of measurable functions from \(\) to \(\). For \(f\), and any distributions \(P,Q\) on \((,)\), we let

\[P(f)*{}_{Z P}f(Z)= fdP, \|P-Q\|_{}_{f}|P(f)-Q(f)|.\]

The norm \(\|\|_{}\) is a notion of discrepancy introduced by the work of Mohri and Munoz Medina (2012) to quantify the error due to the distribution shift with respect to a family of functions \(\), and it is based on previous work on domain adaptation (Mansour et al., 2009).

The goal is to estimate \(P_{T}(f)\) for all \(f\) using the random variables \(Z_{1},,Z_{T}\). Let \(1 r T\), and suppose that we do this estimate by considering the empirical values induced by the most recent \(r\) random variables. Then, by using triangle inequality, we have the following decomposition

\[\|P_{T}-_{T}^{r}\|_{} {\|P_{T}^{r}-_{T}^{r}\|_{}}_{}+ -P_{T}^{r}\|_{}}_{}.\] (1)

The first term of the upper bound is the expected _statistical error_ of the estimation, and quantifies how accurately the empirical values \(\{_{T}^{r}(f):f\}\) approximate the expectation of each function \(f\) according to \(P_{T}^{r}\). The second term \(\|P_{T}-P_{T}^{r}\|_{}\) of the upper bound represents the _drift error_. Intuitively, the statistical error decreases by considering more samples, whereas the drift error can potentially increase since we are considering distributions that are further away from our current distribution. We are looking for the value of \(r\) that balances this trade-off between statistical error and drift error.

### Statistical Error

Since our results revolve around learning a class of functions \(\), we first need to assume that \(\) is "learnable", i.e. the statistical error can be bounded as a function of \(r\). For concreteness, we use the following standard assumption that the family of \(\) satisfies the standard machine learning uniform convergence requirement with rate \(O(1/)\).

**Assumption 1**.: _(Uniform Convergence). There exists non-negative constants \(C_{,1}\) and \(C_{,2}\) such that for any fixed \(r T\) and \((0,1)\), it holds_

\[\|P_{T}^{r}-_{T}^{r}\|_{},1}}{},\]_and with probability at least \(1-\), we have_

\[\|P_{T}^{r}-_{T}^{r}\|_{},1}}{}+C_{,2}}.\]

The learnability of a family of functions \(\) is an extensively studied topic in the statistical learning literature (e.g., (Bousquet et al., 2003; Wainwright, 2019)). For a family of binary functions, the above assumption is equivalent to \(\) having a finite VC-dimension, in which case \(C_{,1}=O()\) and \(C_{,2}=O(1)\). For a general family of functions \(\), a sufficient requirement is that the Rademacher complexity of the first \(r\) samples is \(O(r^{-1/2})\) and the range of any function in \(\) is uniformly bounded. There is nothing special about the rate \(O(r^{-1/2})\). It is possible to adapt our analysis to any rate \(O(r^{-})\) with \((0,1)\) by modifying the constants of our algorithm.

### Quantifying the Drift Error

While for many classes \(\), it is possible to provide an upper bound to the statistical error by using standard statistical learning theory tools, the drift error is unknown and challenging to estimate. The literature used different approaches to quantify the drift error. By using triangle inequality, it is possible to show that for any \(r T\), we have the following upper bounds to the drift error:

\[\|P_{T}-P_{T}^{r}\|_{}_{t=1}^{r}\|P_{T}-P_{T-t} \|_{}_{t<r}\|P_{T}-P_{T-t}\|_{}_{t=1}^{ r-1}\|P_{T-t}-P_{T-t+1}\|_{}.\] (2)

In a long line of research (e.g., (Bartlett, 1992; Long, 1998; Mohri and Munoz Medina, 2012; Hanneke and Yang, 2019)), it is assumed that an upper bound to the drift error is known apriori. One of the most used assumption is that there exists a known upper bound \(\) to the discrepancy between any two consecutive distributions, in which case we can upper bound \(_{t=1}^{r-1}\|P_{T-t}-P_{T-t+1}\|_{}\) with \(r\). In this case, for a binary family \(\) with VC-dimension \(\), we obtain that:

\[\|P_{T}-P_{T}^{r}\|_{}}+r ,\]

and we can choose the value of \(r\) that minimizes this upper bound. Since these algorithms rely on an unrealistic assumption that an upper bound to the drift is known a priori, they are not usable in practice. It is an open problem to provide a competitive algorithm that can choose \(r\) adaptively and it is oblivious to the magnitude of the drift.

Another sequence of work (Mohri and Munoz Medina, 2012; Awasthi et al., 2023) relaxes the problem setting assuming that the algorithm can observe multiple samples from each distribution \(P_{1},,P_{T}\). In this case, they can provably estimate the discrepancies between different distributions, and compute a weighting of the samples that minimizes a trade-off between the statistical error and the estimated discrepancies. This strategy does not apply to our more general setting, as we can have access to at most one sample from each distribution.

Surprisingly, we show that we can adaptively choose the value of \(r\) to minimize the trade-off between statistical error and drift error without explicitly estimating the discrepancy. Noticeably, our method does not require any additional assumption on the drift. The only requirement for our algorithm is that we can compute the norm \(\|\|_{}\) from a set of samples. This is formalized as follows.

**Assumption 2**.: _(Computability). There exists a procedure that computes \(\|_{T}^{r}-_{T}^{2r}\|_{}\) for any \(r T/2\)._

In general, the hardness of computing the norm \(\|\|_{}\) depends on the family \(\). This challenge is also common in previous work that uses this norm to quantify the distribution drift (Mansour et al., 2009; Awasthi et al., 2023). In this paper, we provide two examples of important learning settings where this assumption is satisfied: binary classification with zero-one loss and linear regression with squared loss.

## 3 Main Result

Under those assumption, we prove the following theorem, which is our main result.

**Theorem 1**.: _Let \((0,1)\). Let Assumptions 1 and 2 hold. Given \(Z_{1},,Z_{}\), Algorithm 1 outputs a value \( T\) such that with probability at least \(1-\), it holds that_

\[\|P_{T}-_{T}^{}\|_{}=O(_{r T}[ ,1}}{}+_{t<r} P_{T}-P_{T-t}_{ }+C_{,2}}])\]

In order to appreciate this theorem, we can observe the following. If we learn using the most recent \(r\) samples, similarly to (1) we have the following error decomposition

\[\|P_{T}-_{T}^{r}\|_{}\|P_{T}^{r}-_{T}^{r} \|_{}+\|P_{T}-P_{T}^{r}\|_{}\]

By using Assumption 1 and (2), we have that with probability at least \(1-\) it holds that

\[\|P_{T}-_{T}^{r}\|_{},1}}{}+C_{,2}}+_{t<r} P_{T}-P_{T-t }_{}.\] (3)

Theorem 1 guarantees a learning error that is essentially within a multiplicative constant factor as good as the upper bound obtained by selecting the optimal choice of \(r\) in (3). This result provides an affirmative answer to the open problem posed by Hanneke and Yang (2019), that asked if it was possible to adaptively choose the value of \(r\) that minimizes (3) 1. The upper bound of the theorem contains a negligible additional factor \((r+1)\) within the logarithm due to the union bound required to consider multiple candidate values of \(r\). As further evidence of the efficiency of our algorithm, assume that there is no drift, and we are in the usual i.i.d. setting where \(P_{1}==P_{T}\). In this setting, the following corollary immediately follows from Theorem 1 by setting the drift error equal to \(0\).

**Corollary 2**.: _Consider the setting of Theorem 1, and also assume that \(P_{1}==P_{T}\). With probability at least \(1-\), the window size \(\) computed by Algorithm 1 satisfies_

\[\|P_{T}-_{T}^{}\|_{}=O(, 1}}{}+C_{,2}})\]

Observe that in the i.i.d. case, Assumption 1 implies that with probability at least \(1-\), it holds \(\|P_{T}-_{T}^{T}\|_{} C_{,1}/+C_{ ,2}\). Corollary 2 shows that with our algorithm we obtain a result that is competitive except for a negligible extra factor \(O( T)\) within the logarithm. We want to emphasize that our algorithm does not know in advance whether the data is i.i.d. or drifting, and this factor is the small cost of our algorithm to adapt between those two cases.

## 4 Algorithm

We describe an algorithm that attains the results of Theorem 1. Throughout the remaining of this section, we let Assumptions 1 hold. In particular, the algorithm has access to constants \(C_{,1}\) and \(C_{,2}\) that satisfy this assumption. The main challenge is the fact that the drift error \(\|P_{T}-_{T}^{r}\|_{}\) is unknown for any \(r>1\), and it is challenging to quantify since we have only a single sample \(Z_{T} P_{T}\).

We first provide an informal description of the algorithm. Our algorithm revolves around the following strategy. We do not try to estimate the drift. Instead, we try to assess whether increasing the sample size can yield a better upper bound on the error. Recall that given the most recent \(r\) samples, we have the following upper bound on the error,

\[\|P_{T}-_{T}^{r}\|_{},1 }}{}+\|P_{T}-P_{T}^{r}\|_{}.\] (4)

The algorithm cannot evaluate (4) since the drift component, \(\|P_{T}-P_{T}^{r}\|_{}\), is unknown. Our key idea is to compare the difference in the upper bound on the error when using the latest \(2r\) or \(r\) samples:

\[(,1}}{}-,1}}{} )+(\|P_{T}-P_{T}^{2r}\|_{}-\|P_{T}-P_{T}^{r}\|_{})(,1}}{}-,1}}{})+\|P_{T}^{r}-P_{T}^{2r}\|_{}\] (5)The last step follows from the triangle inequality: \(\|P_{T}-P_{T}^{2r}\|_{F}\|P_{T}-P_{T}^{r}\|_{}+\|P_{T}^{r}-P_{T}^{2 r}\|_{}\). By considering the latest \(2r\) samples rather than \(r\) samples, we can see from (5) that the statistical error decreases, and the difference in drift error can be upper bounded by \(\|P_{T}^{2r}-P_{T}^{r}\|_{}\). We can estimate \(\|P_{T}^{r}-P_{T}^{2r}\|_{}\) using \(\|_{T}^{r}-_{T}^{2r}\|_{}\) within expected error \(O(C_{,1}/)\) as it depends on \( r\) samples. This suggests the following algorithm. Let \(r\) be the current sample size considered by the algorithm. The algorithm starts from \(r\) equal to \(1\), and doubles the sample size as long as \(\|_{T}^{r}-_{T}^{2r}\|_{}\) is small. If \(\|_{T}^{r}-_{T}^{2r}\|_{}\) is big enough, a substantial drift must have occurred in the distributions \(P_{T-2r+1},,P_{T}\), and we can show that this implies that \(_{t<2r-1}\|P_{T}-P_{T-t}\|_{}\) is also large. When this happens, we can stop our algorithm and return the current sample size.

For a formal description of the algorithm, let \((0,1)\) denote the probability of failure of our algorithm, and let \(r_{i}=2^{i}\), for \(i 0\), be the size of the training set used by the algorithm at iteration \(i+1\). For ease of notation, we set

\[C_{,} C_{,1}+C_{,2} /(6))},\]

and

\[S(r,),}}{}+C_{ ,2}(r)+10)}{r}}.\] (6)

The proofs of the following propositions appear in the Appendix A.

**Proposition 3**.: _With probability at least \(1-\), the following event holds:_

\[\|P_{T}^{r_{i}}-_{T}^{r_{i}}\|_{} S(r_{i}, ) i 0\]

We assume that the event of Proposition 3 holds, otherwise our algorithm fails (with probability \(\)). Our algorithm considers the following _inflated_ upper bound \(U(r,)\) to \(\|_{T}^{r}-P_{T}\|_{}\) defined as follows

\[U(r,) 21 S(r,)+\|P_{T}-P_{T}^{r}\|_{ }.\] (7)

Proposition 3 implies that with probability at least \(1-\) for any \(i 0\), we have

\[U(r_{i},)=21 S(r_{i},)+\|P_{T}-P_{T}^{r_{i}}\| _{}\|_{T}^{r_{i}}-P_{T}^{r_{i}}\|_{}+\|P_ {T}-P_{T}^{r_{i}}\|_{}\|P_{T}-_{T}^{r_{i}}\|_{ }\]

We use this value as an upper bound that our algorithm guarantees if we choose a sample size \(r_{i}\). We observe that with respect to (4), the upper bound of the algorithm also contains an additional term that is proportional to \(\) for the high-probability guarantee, and a term proportional to \(}\) that is necessary for the union bound across all possible window sizes \(r_{i}\) for \(i 0\). This union bound is required as we need to have a correct estimation for all possible sample sizes \(r_{i}\) in order to assure that the algorithm takes a correct decision at each step. The constant factor in front of the upper bound on the statistical error \(S(r_{i},)\) is a technical detail that allows taking into account the error of the estimation of the difference in drift error.

We want to compare the upper bound \(U(r_{i},)\) with the upper bound \(U(r_{i+1},)\) obtained by doubling the current sample size \(r_{i}\). As we previously discussed, it is possible to show that if \(\|_{T}^{r_{i}}-_{T}^{r_{i+1}}\|_{}\) is sufficiently small, then \(U(r_{i+1},) U(r_{i},)\), and this intuition is formalized in the following proposition.

**Proposition 4**.: _Assume that the event of Proposition 3 holds and let \(i 0\)._

\[\|_{T}^{r_{i}}-_{T}^{r_{i+1}} \|_{} 4S(r_{i},)(r_{i+1},) U(r_{i},)..\]

The algorithm works as follows. Starting from \(i=0\), we iteratively increase \(i\) by one while \(\|_{T}^{r_{i}}-_{T}^{r_{i+1}}\|_{} 4S(r_{i},)\). There are two cases. In the first case, we reach \(r_{i}=_{2}T,\) and this implies that \(r_{i+1}>T\). In this case, the algorithm returns \(r_{i}\), and Proposition 4 guarantees that the sample size returned by the algorithm is as good as any previously considered sample size. In the second case, we reach a value of \(i\) such that \(\|_{T}^{r_{i}}-_{T}^{r_{i+1}}\|_{}>4S(r_{i},)\). In this case, we return \(r_{i}\), and we can still prove that this is a good choice. In fact, as shown in the next proposition, this terminating condition implies a lower bound \(_{t<r_{i+1}}\|P_{T}-P_{T-i}\|_{}\), thus any estimation with a number of recent samples greater or equal to \(r_{i+1}\) could have a non-negligible drift error.

**Proposition 5**.: _Assume that the event of Proposition 3 holds, and assume that there exists \(i 0\) such that \(\|_{T}^{r_{i}}-_{T}^{r_{i+1}}\|_{}>4S(r_{i},)\), then \(_{t<r_{i+1}}\|P_{T}-P_{T-t}\|_{}>S(r_{i},)\)._

The pseudo-code of the algorithm is reported in Algorithm 1. The algorithm receives in input \(\), the samples \(Z_{1},,Z_{T}\), and returns an integer \(\{1,,T\}\) that satisfies Theorem 1.

Proof of Theorem 1.: We assume that the event of Proposition 3 holds. If it doesn't, we say that our algorithm fails, and this happens with probability \(\). Let \(=r_{j}=2^{j}\) for \(j 0\) be the value returned by the algorithm when it terminates. We remind that our algorithm guarantees an upper bound \(U(r_{j},)\|P_{T}-_{T}^{r_{j}}\|_{}\) to the learning error by using \(r_{j}\) samples. Let \(r^{*}\) be the value of \(r\) that minimizes this expression

\[r^{*}=*{argmin}_{r T}(21 S(r,)+_{t<r}\|P _{T}-P_{T-t}\|_{}),\]

and let \(B^{*}\) be the minimum value of this expression, i.e.

\[B^{*}=21 S(r^{*},)+_{t<r^{*}}\|P_{T}-P_{T-t}\|_{}\]

be any fixed optimal sample size, where we remind the definition of \(U\) from (7). The first observation is that the right-hand side of the inequality in Theorem 1 is \(O(B^{*})\). Therefore, in order to prove the theorem, it is sufficient to show that \(U(r_{j},)/B^{*}=O(1)\).

We distinguish two cases: \((a)\)\(r^{*}<2r_{j}\), and \((b)\)\(r^{*} 2r_{j}\). We first consider case \((a)\). We let \(k 0\) be the largest integer such that \(r_{k} r^{*}\). Since \(r^{*}<2r_{j}\), it holds that \(k j\). Since our algorithm returned \(r_{j}\), Proposition 4 applies for \(i=0,,j-1\), thus \(U(r_{j},) U(r_{k},)\). We have that:

\[,)}{B^{*}},)}{B^{*}}\] (8)

We observe that

\[,)}{B^{*}} =,)+\|P_{T}-P_{T}^{r}\|_{}}{21S( r^{*},)+_{t<r^{*}}\|P_{T}-P_{T-t}\|_{}}, )+_{t<r_{k}}\|P_{T}-P_{T-t}\|_{}}{21S(r^{*},)+ _{t<r^{*}}\|P_{T}-P_{T-t}\|_{}}\] \[,)}{S(r^{*},)}+} \|P_{T}-P_{T-t}\|_{}}{_{t<r^{*}}\|P_{T}-P_{T-t}\|_{} }}{r_{k}}}+1 3,\]

where the first inequality is due to (2), and the last inequality is due to the fact that \(r_{k} r^{*}<2r_{k}\) by definition of \(r_{k}\). By using the above inequality in (8), we obtain that \(U(r_{j},)/B^{*} 3\).

We consider case \((b)\). Since \(T r^{*} 2r_{j}=r_{j+1}\), the algorithm returned \(r_{j}\) because \(\|_{T}^{r_{j}}-_{T}^{r_{j+1}}\|_{}>4S(r_{j},)\). Using Proposition 5, we have

\[_{t<r^{*}}\|P_{T}-P_{T-t}\|_{}_{t<r_{j+1}}\|P_{T}-P_{T-t }\|_{}>S(r_{j},).\]

Therefore, we have that

\[,)}{B^{*}} =,)+\|P_{T}-P_{T}^{r_{j}}\|_{}}{ 21S(r^{*},)+_{t<r^{*}}\|P_{T}-P_{T-t}\|_{}}\] \[,)}{_{t<r^{*}}\|P_{T}-P_{T-t}\|_{ }}+}\|P_{T}-P_{T-t}\|_{}}{_{t<r^ {*}}\|P_{T}-P_{T-t}\|_{}},)}{S(r_{j},) }+1=21\]

This concludes the proof.

Binary Classification with Distribution Drift

In this section, we show an application of Theorem 1 for the fundamental statistical learning problem of agnostic learning a family of binary classifiers. Let \(=\), where \(\) is the feature space, and \(=\{0,1\}\) is the label space, i.e. \(Z_{t}=(X_{t},Y_{t})\). A hypothesis class \(\) is a class of functions \(h:\) that classify the feature space \(\). Given a point \((x,y)\) and a function \(h\), the risk of \(h\) on \((x,y)\) is defined through the following function \(L_{h}(x,y)=_{\{y h(x)\}}\). We work with the class of functions \(=\{L_{h}:h\}\).

Let \(h^{*}=*{argmin}_{h}P_{T}(L_{h})\) be a function with minimum expected risk with respect to the current distribution \(P_{T}\). We want to use Theorem 1 to find a function \(h\) such that \(P_{T}(L_{h})\) is close to \(P_{T}(L_{h^{*}})\). Let \(\) be the VC dimension of \(\). The VC dimension describes the complexity of the family \(\), and it is used to quantify the statistical error. In particular, using standard learning tools, it is possible to show that the family \(\) satisfies Assumption 1 on the sample complexity with constants \(C_{,1}=O()\) and \(C_{,2}=O(1)\)(e.g., ).

Finally, to satisfy Assumption 2, we need to exhibit a procedure that for \(r T/2\), outputs the quantity \(\|_{T}^{r}-_{T}^{2r}\|_{}\). This quantity is also referred to as \(\)-discrepancy between the empirical distributions \(_{T}^{r}\) and \(_{T}^{2r}\) in previous work on transfer learning . We can adapt a strategy from Ben-David et al.  to our setting and show that it is possible to compute it by solving an empirical risk minimization problem. We say that a hypothesis class \(\) is _computationally tractable_ if given a finite set of points from \(\), there exists an algorithm that returns a hypothesis \(h\) that achieves the minimum risk over this set of points.

**Lemma 6**.: _Assume that \(\) is symmetric, i.e. \(h 1-h\). For \(r T/2\), it holds_

\[\|_{T}^{r}-_{T}^{2r}\|_{}=- {2}_{h}[_{t=T-r+1}^{T}L_{h}(X_{t},1-Y_{t })+_{t=T-2r+1}^{T-r}L_{h}(X_{t},Y_{t})]\]

Given \(r\), the minimum in Lemma 6 can be computed by solving an empirical risk minimization over the most recent \(2r\) points, where we flip the label of half of those points, i.e. we use the points \((X_{T-2r+1},Y_{T-2r+1}),,(X_{T-r},Y_{T-r}),(X_{T-r+1},1-Y_{T-r+1}),,(X_{T},1-Y_{T})\). Thus, if \(\) is computationally tractable and symmetric, Assumption 2 holds. This is indeed true for many hypothesis class, e.g., hyperplanes, axis-aligned rectangles, or threshold functions. However, the empirical risk minimization problem could be expensive to solve exactly, but as we discuss in Section 8, it is possible to modify the algorithm to allow for an approximation of \(\|_{T}^{r}-_{T}^{2r}\|_{}\).

Our main result for binary classification is given in the following theorem:

**Theorem 7**.: _Let \(\) be a computationally tractable and symmetric binary class with VC dimension \(\). Let \(=\{L_{h}:h\}\). Let \( T\) be output of Algorithm 1 with input \(Z_{1},,Z_{T}\) using the family \(\). Let \(=*{argmin}_{h}_{T}^{}(L_{h})\) be an empirical risk minimizer over the most recent \(\) samples. With probability at least \(1-\), the following holds:_

\[P_{T}(L_{})-P_{T}(L_{h^{*}})=O(_{r T}[}+_{t<r} P_{T}-P_{T-t}_{}+}])\]

The symmetry assumption is not necessary, and in the appendix we show how to remove it at the cost of a more expensive computation of the discrepancy. It is instructive to compare this upper bound with the results of previous work. An often used assumption in the literature, originally introduced in Bartlett , is that there is a known value \(>0\), such that drift in each step is bounded by \(\), i.e. for all \(t<T\), \( P_{t+1}-P_{t}_{}\). Assume that \(T\) is sufficiently large, i.e. \(T=((/^{2})^{1/3})\). By using this assumption on the drift, previous work showed that with high-probability, they can find a classifier \(\) such that \(P_{T}(L_{})-P_{T}(L_{h^{*}})=O()\), and it can be shown that this upper bound is tight up to constants within those assumptions . These previous works assumed they had access a priori to the value of \(\), since those algorithms compute \(\) by solving a empirical risk minimization over a number of previous samples \(((/^{2})^{1/3})\) that is decided before observing the data. On the other hand, this assumption on the drift together with (2) implies that \(_{t<r} P_{T}-P_{T-t}_{}(r-1)\). Our algorithm (Theorem 7) guarantees an error that depends on a minimum choice over \(r T\), hence it is always smaller than the one obtained with a specific choice of \(r\). If we choose \(r=(/^{2})^{1/3}\) in the upper bound of Theorem 7, we can show that with high-probability, our algorithm returns a classifier \(\) such that \(P_{T}(L_{})-P_{T}(L_{h^{*}})=O(+/})})\). Our algorithm achieves this guarantee while being adaptive with respect to the upper bound \(\), and it can indeed guarantee a better result when this upper bound is loose.

For example, assume an extreme case in which the algorithm is given a \(>0\) bound on the drift in each step, but the training set has actually no drift, it was all drawn from the distribution \(P_{T}\). Previous methods are oblivious to the actual data, and they guarantee an upper bound \(O()\) with high-probability, since they decide the sample size \(((/^{2})^{1/3})\) a priory without observing the input. In contrast, our algorithm adapts to this scenario, and Theorem 7 guarantees that we obtain an \(O(+)\) error, with high-probability, essentially retrieving the error guarantee for learning with independent and identically distributed samples. Observe that our upper bound depends on \(T\), and it goes to \(0\) when \(T\).

It is possible to show that our algorithm can obtain asymptotically better guarantee even if \(\|P_{t}-P_{t-1}\|_{}= 1\) for all \(t<T\), i.e. there is an exact drift of \(\) at each step. This is because our algorithm provides a guarantee as a function of \(_{t<r}\|P_{T}-P_{T-t}\|_{}\) rather than the looser quantity \(_{t=1}^{r-1}\|P_{T-t}-P_{T-t+1}\|_{}\), as shown in the following example. Let \(=\), and let \(\) be a class of threshold functions over \(\) i.e. for any \(c\), there exists classifiers \(h_{c},h_{c}^{}\) such that \(h_{c}(x)=1\) if and only if \(x c\) and \(h_{c}^{}(x)=1\) if and only if \(x<c\). The class \(\) has VC-dimension equal to \(2\). We construct a sequence of distributions \(P_{1},,P_{T}\) as follows. The marginal distribution over \(\) is uniform for each distribution \(P_{t}\) with \(t=1,,T\). At time \(t T\), the classification of \(x\) is given by a function \(_{t}:\{0,1\}\). Assume that there exists two disjoint intervals \(I_{0}=[p_{0},p_{0}+/2)\) and \(I_{1}=[p_{1},p_{1}+/2)\) such that \(_{T}(x)=0\) for all \(x I_{0}\) and \(_{T}(x)=1\) for all \(x I_{1}\). We let

\[_{T-t}(x)=1-_{T}(x)&)(t )$}\\ 1-_{T}(x)&)(t)$}\\ _{T}(x)&, x,t<T.\]

In particular, \(_{T-t}(x)\) differs from \(_{T}(x)\) for \(x I_{0}\) if \(t\) is odd, and for \(x I_{1}\) if \(t\) is even. By construction, \(\|P_{t}-P_{t-1}\|_{}=\) for all \(t<T\), i.e. there is an exact drift of \(\) at each step (to be precise, in the last step \(\|P_{T}-P_{T-1}\|_{}=/2\)). As discussed before, with the assumption of a bounded drift \(\) at each step, previous methods guarantee an upper bound \(O()\) with high-probability. However, we also have that for any \(1 r T\), it holds by construction that

\[_{t<r}\|P_{T}-P_{T-t}\|_{}.\]

Hence, our algorithm (Theorem 7) guarantees with high-probability an upper bound

\[O(++).\]

Since our algorithm is adaptive with respect to the drift, it can correctly use more samples. In contrast, previous non-adaptive algorithms that rely on the assumption of bounded drift \(\) at each step choose a sample size of \((^{-2/3})\) based on this assumption, thus they can only guarantee a looser bound of \(O(^{1/3})\), even when this assumption is satisfied with equality.

It is possible to use the recent lower bound strategy of Mazzetto and Upfal (2023) in order to show that the upper bound of Theorem 7 is essentially tight in a minimax sense.

**Theorem 8**.: _Let \(\) be a binary class with VC dimension \(\), and consider an arbitrary non-decreasing sequence \(_{1}=0,_{2},,_{T}\) of non-negative real numbers. Let \(=\{L_{}:h\}\). Let \(\) be any algorithm that observes \(Z_{1},,Z_{T}\), and it outputs a classifier \(h_{}\). If_

\[^{*}=_{r T}(}+_{r})<1/3,\]

_then, for any algorithm \(\), there exists a sequence of distributions \(P_{1},,P_{T}\) such that \(_{t<r}\|P_{T}-P_{T-t}\|_{}_{r}\) for any \(r T\), and with probability at least \(1/8\) it holds that:_

\[P_{T}(L_{h_{}})-P_{T}(L_{h^{*}})=(^{*})=(_ {r T}(}+_{t<r}\|P_{T}-P_{T-t}\|_{} ))\]Linear Regression with Squared Loss

In the previous section, we showed an application of Theorem 1 for the problem of binary classification with zero-one loss. In this section, we show that our main result can also be applied to a linear regression problem. Similarly to the previous section, we let \(=\), where \(\) is the feature space and \(=[-1,1]\) is the label space, i.e. \(Z_{t}=(X_{t},Y_{t})\). In this section, we constrain the feature space \(=\{x^{d}:\|x\|_{2} 1\}\) to be the unit ball centered at the origin in \(^{d}\).

We consider the \(_{2}\) regularized linear prediction class \(=\{x x,w:w^{d}\ \|w\|_{2} 1\}\). We denote each predictor \(h_{w}\) with its weight \(w\). To evaluate the quality of a prediction \(y=h_{w}(x)\), we use the squared loss \(L:\) defined as \(L(y,y^{})=(y-y^{})^{2}\). For each \(h_{w}\), we let \(L_{w}(x,y)=L(h_{w}(x),y)\) be the function that evaluates the loss incurred by \(h_{w}\) for any \(z=(x,y)\). We work with the family of functions \(=\{L_{w}:h_{w}\}\). By a standard uniform convergence argument based on the Rademacher complexity of \(\)(e.g., see (Kakade et al., 2008; Shamir, 2015; Awasthi et al., 2020)), we have that for any \(1 r T\), it holds:

\[\|P_{T}^{r}-_{T}^{r}\|_{}=O(}+ })\ \,\]

thus we satisfy Assumption 1 with \(C_{,1}=O(1)\) and \(C_{,2}=O(1)\).

The computation of the discrepancy is more challenging. Let \(1 r T/2\). Using the definition of \(\), we have that:

\[\|_{T}^{r}-_{T}^{2r}\|_{}=_{w: \|w\| 1}|_{t=T-r+1}^{T}(Y_{t}- X_{t},w) ^{2}-_{t=T-2r+1}^{T-r}(Y_{t}- X_{t},w)^{2}|\ \.\] (9)

Let \(c_{r}\), \(b_{r}^{d}\), and \(A_{r}^{d d}\) be defined as follows:

\[a_{r} =_{t=T-r+1}^{T}Y_{t}^{2}-_{t=T-2r+1}^{T-r}Y_{t}^{2}, b _{r}=_{t=T-2r+1}^{T-r}Y_{t}X_{t}-_{t=T-r+1}^{T}Y_{t}X_{t}\ \,\] \[A_{r} =_{t=T-r+1}^{T}X_{t}^{T}X_{t}-_{t=T-2r+1}^{T-r}X_{t}^{T}X _{t},\]

and observe that the matrix \(A_{r}\) is symmetric. Using those definition, we can manipulate the right-hand side of (9) to show that it is equivalent to:

\[(-a_{r}+_{w:\|w\| 1}[w^{T}(-A_{r})w-2b_{r}^{T}w],a_ {r}+_{w:\|w\| 1}[w^{T}(A_{r})w-2(-b_{r}^{T})w])\] (10)

In order to compute (10), it is sufficient to be able to solve the minimization problem

\[_{w:\|w\| 1}(w^{T}Aw-2b^{T}w)\] (11)

where \(A^{d d}\) is a symmetric matrix, and \(b^{d}\). This minimization problem has been extensively studied for the trust-region method (Conn et al., 2000), and Hager (2001, Section 2) provides a way to compute the solution of (11) in terms of a diagonalization of \(A\). Thus, we also satisfy Assumption 2, and we can obtain the following result as an immediate corollary of Theorem 1.

**Theorem 9**.: _Let \((0,1)\). Let \(1 T\) be the output of Algorithm 1 with input \(Z_{1},,Z_{T}\) and using the family \(\) described in this section. Let \(=*{argmin}_{w:\|w\| 1}_{T}^{}(L_{w})\) be the linear classifier with minimum loss over the most recent \(\) samples. Then, with probability at least \(1-\):_

\[P_{T}(L_{})-P_{T}(L_{w^{*}})=O(_{r T}[}+_{t<r}-11.381102pt\|P_{T}-P_{T-t}\|_{}+}])\]

_where \(w^{*}=*{argmin}_{w:\|w\| 1}P_{T}(L_{w})\) is the linear predictor with minimum loss with respect to the current distribution \(P_{T}\)._Related Work

Additional variants of learning with distribution drift have been studied in the literature. Freund and Mansour (1997) provide a refined learning algorithm in the special case of rapid distribution shift with a constant direction of change. In the work of Bartlett et al. (2000), they show specialized bounds in the case of infrequent changes and other different restrictions on the distribution drift. The work of Crammer et al. (2010) provides regret bound for online learning with an adversarial bounded drift. Yang (2011) studies the problem of active learning in a distribution drift setting. Hanneke et al. (2015) provide an efficient polynomial time algorithm to learn a class of linear separators with a drifting target concept under the uniform distribution in the realizable setting. Interestingly, they also show how to adapt their algorithm with respect to an unknown drift, although their technique relies on the realizability of the learning problem. In the more recent work of Hanneke and Yang (2019), they relax the independence assumption and provide learning guarantees for a sequence of random variables that is both drifting and mixing.

## 8 Conclusion, Limitations and Future Directions

We present a general learning algorithm that adapts to an unknown distribution drift in the training set. Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. For the problem of binary classification, we show that without explicitly estimating the drift, there exists an algorithm that learns a binary classifier with the same or better error bounds compared to the state-of-the-art results that rely on prior information about the magnitude of the drift. This is a major step toward practical solutions to the problem since prior knowledge about the distribution drift in the training set is often hard to obtain.

We presented concrete results for binary classification and linear regression, but our technique can be applied for learning any family of functions \(\) as long as it is possible to compute the distance \(\|_{T}^{r}-_{T}^{2r}\|_{}\) between the empirical distributions with \(r\) and \(2r\) samples according to the \(\|\|_{}\) norm (Assumption 2). This is often a challenging problem, and it is related to the computation of the discrepancy between distributions, which was studied in previous work on transfer learning (Mansour et al., 2009; Ben-David et al., 2010). For binary classification, we assume that the empirical risk minimization problem is tractable. However, an exact solution to this problem is computationally hard for many hypothesis classes of interest, and this is a limitation of our algorithm and previous work on transfer learning. In those cases, we can modify our analysis to use the best-known approximation for the distance \(\|_{T}^{r}-_{T}^{2r}\|_{}\) as long as there is an approximation guarantee with respect to its exact value. As an illustrative example, if we have a procedure that returns an approximation \(E_{r}\) such that \(1\|_{T}^{r}-_{T}^{2r}\|_{}/E_{r}\) for any \(r 1\), then it is possible to change the algorithm to obtain a guarantee that is at the most a factor \(O(^{2})\) worse than the one achieved by Algorithm 1 with the exact computation of the distance (we refer to Appendix B for additional details).

The method presented here uses a distribution-independent upper bound for the statistical error. While this upper bound can be tight in the worst-case, as shown in our lower bound for binary classification (Theorem 8), it can be loose for some other sequence of distributions. As a future direction, it is an interesting problem to provide an adaptive algorithm with respect to the drift that uses distribution-dependent upper bounds, for example, based on the Rademacher complexity, which can be possibly computed from the input data. Our algorithm does not naturally extend to this setting, as it requires knowing the rate at which the upper bound on the statistical error is decreasing (see proof of Proposition 4).

**Acknowledgements.** This material is based on research sponsored by Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory (AFRL) under agreement number FA8750-19-2-1006 and by the National Science Foundation (NSF) under award IIS-1813444. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory (AFRL) or the U.S. Government.