# Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions

Zhe Hu\({}^{1}\), Tuo Liang\({}^{2}\), Jing Li\({}^{1}\), Yuren Lu\({}^{2}\), Yunlai Zhou\({}^{2}\), Yiran Qiao\({}^{2}\), Jing Ma\({}^{2}\), Yu Yin\({}^{2}\)

\({}^{1}\)Department of Computing, The Hong Kong Polytechnic University

\({}^{2}\)Department of Computer and Data Sciences, Case Western Reserve University

\({}^{1}\)2he-derek.hu@connect.polyu.hk, jing-amelia.li@polyu.edu.hk

\({}^{2}\){txl859,yxl3538,yxz3057,yxq350,jxm1384,yu.jin}@case.edu

https://vulab-ai.github.io/YESBUT_Homepage/

###### Abstract

Recent advancements in large multimodal language models have demonstrated remarkable proficiency across a wide range of tasks. Yet, these models still struggle with understanding the nuances of human humor through juxtaposition, particularly when it involves nonlinear narratives that underpin many jokes and humor cues. This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction. We introduce the YesBut benchmark, which comprises tasks of varying difficulty aimed at assessing AI's capabilities in recognizing and interpreting these comics, ranging from literal content comprehension to deep narrative reasoning. Through extensive experimentation and analysis of recent commercial or open-sourced large (vision) language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics. Our results show that even state-of-the-art models still lag behind human performance on this task. Our findings offer insights into the current limitations and potential improvements for AI in understanding human creative expressions.

## 1 Introduction

_"The world is indeed comic, but the joke is on mankind."_

--H. P. Lovecraft

Comics are a unique blend of visual art and narrative that encapsulate a wide range of human experiences and emotions. Understanding comics often requires significant social reasoning skills and cultural knowledge, as they heavily rely on context, cultural references, and visual metaphors. Furthermore, comics frequently employ nonlinear narratives , demanding rigorous reasoning to grasp underlying ideas. Recent large (vision) language models have achieved impressive performance on various tasks , yet their ability to comprehend these complex human expressions remains insufficiently explored .

Examining AI models' ability to understand comics is essential for advancing their social and semantic comprehension. As a significant part of human creative expression, comic offers valuable insights into human emotions and cultural contexts . This understanding is crucial for developing socially intelligent systems and enhancing AI-related creativity, thereby improving user experience in applications such as recommendation systems and automated content creation tools.

Previous studies have applied vision language models (VLMs) to understand humor and deep semantics . However, these studies often focus on single-panel comics and do not investigatethe more complex case of nonlinear narratives created through juxtaposition, a fundamental element in comics. Juxtaposition involves placing two contrasting elements together to provoke thought or evoke humor . This technique requires readers to pause and reassess the meaning, engaging in _nonlinear thinking_ to reason about the relationships between panels for overall idea. .

In this work, we examine VLMs' ability to understand comics, specifically focusing on humor derived from juxtaposition. Our goal is to determine if large models can accurately comprehend the complex and contradictory narratives present in comics. Such contradictions challenge conventional semantic interpretations and demand deeper analysis. For example, Figure 1 shows a comic with two panels: in the first, a driver stops for ducks to cross the road ("Yes"), and in the second, the driver enters a "Peking Duck" restaurant ("But"), highlighting the contradiction in human-animal relationships through juxtaposition.

Understanding such juxtaposition in comics significant challenges for the models. First, **it requires deep comprehension of human norms**, recognizing that people often have conflicting feelings, and identifying subtle social cues and contexts tied to cultural backgrounds. Additionally, **it demands nonlinear reasoning to grasp the overall narrative**, as the story is conveyed through the interplay of two panel elements, forming the core of the narrative beyond the literal meaning of each single panel. This type of juxtaposition necessitates critical thinking about similarities and differences, requiring in-depth reasoning. However, current models lack the ability to process information through nonlinear and deep thinking effectively, as the autoregressive paradigm of LMs limits their bidirectional reasoning capabilities . By emphasizing these contradictions, we aim to push AI models to develop more sophisticated semantic understanding, enriching their inter-pretative capabilities.

To this end, we collected and annotated a new benchmark, YesBut, for understanding comics with juxtaposition, focusing on _contradictory_ narratives. Each comic is annotated with a literal description, a contradiction illustration, the underlying philosophy it reveals or satirizes, and a title that summarizes the overall narrative, as shown in Figure 1. We then propose four tasks: (1) literal description writing, to produce a surface description of the comic narrative; (2) contradiction generation, where the model illustrates the narrative contradiction; (3) underlying philosophy selection, which targets at selecting the correct philosophy the comic reflects; and (4) title matching, where the model matches the comic with a proper title. These tasks jointly cover different levels of comic understanding, from literal content comprehension to more in-depth narrative reasoning, providing a thorough evaluation of comic understanding capabilities.

We conducted comprehensive experiments on the YesBut dataset, evaluating both commercial and open-sourced large (visual) language models. Both automatic and human evaluation results indicate that commercial VLMs outperform their open-sourced counterparts on most tasks. However, even the highest scores are far from perfect (e.g., 84.1% accuracy for underlying philosophy selection and 63.3% for title matching), underscoring the need for further advancements in this area. Additionally, our analysis reveals that augmenting models with oracle comic descriptions can significantly enhance performance, highlighting the considerable gap in current models' understanding of comic narratives. We release our annotations, code, and model results, aiming to provide valuable insights for future AI research on understanding human creative expression.

Figure 1: We introduce YesBut dataset for comic understanding of juxtaposed comic panels. Given a two-panel comic with a contradictory narrative, we propose several tasks including narrative understanding, underlying philosophy selection and title matching, tackling different levels of comic understanding. (Comic by Anton Gudim).

Related Work

**Large Models and Evaluations.** Recent large (vision) language models have demonstrated remarkable performance in following human instructions and performing various downstream tasks through zero-shot prompting [19; 20; 21; 4]. Various benchmarks have been proposed to evaluate their performance, encompassing both language-only tasks [22; 23; 24; 25] and vision-language tasks [26; 27; 28; 29; 30]. These tasks primarily focus on assessing the fundamental capabilities of large models. However, the ability of large models to perform in-depth social reasoning and accurately understand human contexts remains underexplored .

**Computational Humor.** Humor is a vital component of human communication . Our research is closely related to the computational understanding of humor. Previous studies have addressed humor recognition [32; 33; 34] and generation , with recent work expanding to multimodal data, such as visual humor prediction , humorous cartoon caption identification [37; 7; 38], and humor prediction in videos [39; 40]. Despite advancements, recent work shows that LLMs such as ChatGPT has not fully solved computational humor yet . In this work, we design tasks to evaluate large vision-language models on their ability to understand humor through comic juxtapositions with contradictory narratives, requiring deep narrative comprehension. Through this study, we aim to provide insights into the capabilities of AI in processing and appreciating humor.

**Interpretation of Human Creative Expressions.** Visual artwork, encompassing mediums such as drawings, paintings, and sculptures, has been a profound aspect of human culture and cognition. These creative expressions are not merely decorative; they are deeply entwined with the ways humans perceive, interpret, and communicate their experiences and emotions . Understanding these human creative expressions necessities valuable insights of human emotions, societal values, and cultural contexts, which is crucial for developing socially intelligent systems and enhancing AI-related creativity . Previous research has explored AI interpretation of visual human creative expressions in tasks such as meme  and cartoon  understanding. Similar to our work, studies like  and  apply AI models to comprehend comics. However, these studies primarily focus on single-panel comics, emphasizing humor and deep semantics. In contrast, our work aims to investigate the significant feature of juxtaposition for understanding contradictory narratives.

**Visual Reasoning.** Our task is also related to the visual reasoning ability, where the model requires in-depth reasoning to comprehend contradictions between two comic panels. Previous research has examined visual reasoning capabilities of large models in tasks involving commonsense reasoning [46; 28; 47], visual question answering , visio-linguistic compositionality , and science question answering . Unlike these studies, our task involves nonlinear reasoning, which necessitates AI to navigate multi-dimensional and complex information layers, often without explicit directives. While linear reasoning present their challenges, they usually exhibit clearer rules and structures, making them more accessible for AI to process with existing algorithms and models. Consequently, nonlinear reasoning represents a more intricate task, demanding higher natural language processing and cognitive modeling capabilities from AI systems.

## 3 The YesBut Dataset

Our benchmark consists of YesBut comics featuring contradictory narratives. Specifically, each sample includes: (1) a two-panel comic that forms a narrative with inherent contradictions; (2) a literal description of the comic narratives; (3) an explanation that illustrates the contradiction within the narrative; (4) the deep philosophy or underlying message the comic aims to convey; and (5) a title of the comic. Based on these components, we construct various tasks for comic understanding.

### Image Collection

Our dataset consists of captionless comics, primarily from Anton Gudim's "YES, BUT" series , each featuring two panels depicting contradictory everyday scenarios. We scraped the images from social media 2 and conducted preprocessing, including deduplication, filtering out comics with more than two panels, and removing any inappropriate or offensive content. This process resulted in a final dataset of 348 comics.

### Data Annotation

For each comic, we annotate the corresponding literal description, contradiction explanation, underlying philosophy and comic title [7; 10]. We primarily rely on human annotators to obtain gold-standard annotations. Eight human judges participated in the annotation process, all of whom are proficient English speakers based in English-speaking countries and have at least a Bachelor's degree. Our annotation process included two stages: the progressive human-AI collaborative annotation stage and the quality check and cross-verification stage. The pipeline is illustrated in Figure 2.

**Progressive Human-AI Collaborative Annotation.** In this stage, we randomly assigned comic samples to each annotator, instructing them to first exclude any comics that may contain offensive, hateful, or sexual material before beginning the annotation process. To reduce human effort and costs associated with data annotation from scratch, we designed a human-AI collaboration pipeline utilizing GPT-4  for data annotation and component writing.

The pipeline operates through dialogue interactions with the GPT-4 model. Given a comic image, we first prompt GPT-4 to generate narrative descriptions, illustrating the comic's narrative and explaining the contradictory logic between the two panels. Human annotators then modify and annotate the contents to obtain a literal description and contradiction explanation.

After obtaining the gold-standard description, both the comic and the description are used as input to prompt GPT-4 deep content writing, including the underlying philosophy and an eye-catching comic title. The underlying philosophy aims to foster a deep understanding of the comic and reveal the phenomenon it satirizes or the lesson it conveys; and the title is a more abstractive expression that reflects the overall narrative. Both components will be further checked by human annotators. Additionally, for the underlying philosophy and title understanding tasks, GPT-4 generates hard negative counterparts and distractions to design multiple-choice questions for our experiments. The prompts we used are provided in the Appendix A.

Our human-AI collaborative annotation pipeline is effective as it leverages a progressive prompting strategy, annotating each component from easy to difficult. Understanding the underlying philosophy of the comic, for example, requires first understanding the literal narratives and contradictions. This approach reduces annotation costs and improves overall efficiency.

**Quality Check with Cross Verification.** To ensure the quality and accuracy of the components and reduce objective bias from different human annotators, we introduced a cross verification stage. In this process, one annotator is assigned as an inspector for each comic. The inspector checks the annotated results to ensure all components are correct, unbiased, and appropriate. If any content is found to be of low quality or ambiguous, a third annotator is brought in as a judge to determine the final version. We exclude the comics with ambiguous or controversial narratives. This process ensures the quality of the annotated components for benchmark construction.

Figure 2: Overview of the data construction pipeline. Pos represents the positive options, and Neg stands for the negative options.

After the cross-verification process, one of the authors reviews each sample to verify the validity and appropriateness of the components. Finally, we obtain a dataset of 348 comics, each accompanied by high-quality components. The statistics of the components are shown in Table 1.

**Mitigating Annotation Bias.** Our benchmark focuses on _common interpretation_ of humor. However, the subjectivity of this task may introduce bias. To mitigate this issue, we have taken several steps in our annotation process: (1) Our annotators come from different genders and diverse cultural backgrounds, providing a range of perspectives; (2) Multiple quality checks and verifications are incorporated to ensure consensus among different annotators, with controversial or potentially biased comics being filtered out; (3) Annotations are further validated by cross-referencing social media comments for each comic to ensure alignment with widely accepted interpretations; (4) Recognizing that tasks such as generating titles and philosophical contents are inherently open-ended and involve subjective data annotation, we frame them as selection tasks, and ensure that the correct option is clearly and objectively superior than the negative options to mitigate subjectivity.

### Task Design: Do Large Models Understand Humor in Juxtaposition?

We aim to evaluate the capabilities of recent large (visual) language models in understanding humor through contradictions. This is challenging because it requires both social reasoning about human events and nonlinear logical reasoning about the narratives, going beyond the literal understanding of the comic. We design a series of tasks that require different levels of narrative understanding and reasoning abilities to evaluate the models' performance in reading comics.

**1. Literal Description Writing.** The first task is to generate the literal description of the comic narrative. We formulated this task as a text generation task: given an input comic, the model is required to generate a short description illustrating the narrative from the two panels of the comic. This task is different from the traditional image captioning, which requires the model to illustrate the comic narrative instead of solely focusing on image description.

**2. Contradiction Generation** evaluates whether the model can understand the contradiction within the narrative juxtaposition. Similarly, it is formulated as a text generation task.

**3. Underlying Philosophy Selection.** Understanding comics requires grasping not only the surface meaning of the images but also the underlying ideas the authors aim to convey. This task evaluates the model's ability to recognize the comic's underlying philosophy. It is formulated as a multiple-choice question answering (MCQ) task: given an input comic and four candidates of its underlying philosophy, the model must predict the correct option. The negative choices are crafted by annotators to be relevant to the comic, requiring reasoning to make the correct prediction.

**4. Title Matching** evaluates whether the models can identify the corresponding title, which is challenging because the title acts as an abstraction of the narrative and requires a proper understanding of the comic's content. Similar to the underlying philosophy task, it is formulated as a multiple-choice question answering task, where the most is asked to select the correct title from four options.

## 4 Experiments

### Models and Settings

We evaluate the models' performance in a zero-shot manner using both recent VLMs and LLMs. For VLMs, the comic image and questions are provided as inputs for output prediction. We include both commercial models such as GPT-4  and Claude-3, as well as open-sourced models including LLaVa [3; 54], CogVLM , Qwen-VL , mPLUG-Owl2 , and InstructBLIP .

For LLMs, since they cannot directly process images, we use the LLaVa-1.6 13B model generated literal descriptions as inputs due to its strong performance. We include ChatGPT, the Llama3 instruction model , and the Mistral 7B instruction model . More details of the models are included in Appendix B.

   Components & \#Num & Avg. Len. \\  Image & 348 & - \\ Literal Description & 348 & 80 \\ Contradiction & 348 & 31 \\ Philosophy & 1,392 & 24 \\ Title & 1,392 & 6 \\   

Table 1: Data Statistics. Avg. Len. is the average number of words.

**Implementation Details.** For GPT-4 and ChatGPT, we set the temperature as 1. For other models, we use the default parameter settings during inference. To reduce variance across different task prompts, we create three distinct prompts for each task and report the average scores from three runs with each prompt. The specific prompts and additional details are in Appendix B.

### Evaluation Metrics

For the philosophy and title understanding tasks, which are formulated as multiple-choice question answering, we use accuracy as the evaluation metric. For generation tasks including literal description and contradiction, we apply reference-based evaluation metrics commonly used in text generation studies , and report ROUGE-2 (recall)  and BERT Score (recall) . 3 Recent work shows GPT-based evaluation aligns well with human judgements [63; 64; 65], and we also apply ChatGPT for evaluation 4. The prompts for GPT-based evaluation are provided in Appendix B.

Due to the limitation of the automatic evaluations for text generation, we also include human evaluation to assess the quality of the outputs for the literal description and contradiction generation tasks. We hire three human judges to rate each aspect on a scale of 1 (worst) to 5 (best). For literal description, following , we evaluate: (1) **Correctness**: Does the model output correctly convey the narrative of the comic? (2) **Completeness**: Does the model output cover all the important elements of the comic narrative? (3) **Faithfulness**: Can all contents from the model output be supported by the comic image (i.e., there are no hallucinations)? For contradiction generation, we evaluate **Correctness** and **Faithfulness**. More details are provided in Appendix B.4.

## 5 Main Results

The main experimental results are shown in Table 2. For VLMs, the original image is directly used as input, while for LLMs, the generated comic description is used as input.

    & &  &  & **Philosophy** & **Title** \\ 
**Setting** & **Model** & BERT & R-2 & GPT & BERT & R-2 & GPT & Accuracy & Accuracy \\   & GPT-4 & **88.32** & **87.46** & **3.76** & 87.64 & **83.21** & **4.03** & 82.76 & 60.25 \\  & Claude-3 & 87.68 & 80.30 & 3.28 & 86.93 & 80.63 & 3.79 & **84.10** & 56.42 \\  & LLaVA-1.6-34B & 86.45 & 67.67 & 2.86 & 86.04 & 75.95 & 3.51 & 78.83 & **63.31** \\  & LLaVA-1.6-13B & 81.34 & 75.95 & 2.96 & 86.48 & 80.96 & 3.36 & 69.16 & 55.08 \\  & LLaVA-1.5-13B & 78.77 & 58.21 & 2.51 & 86.48 & 67.67 & 3.36 & 69.73 & 48.75 \\  & InstructBlip-13B & 85.20 & 35.28 & 2.69 & 85.54 & 51.15 & 2.54 & 30.75 & 22.70 \\  & CogVLM & 80.80 & 55.51 & 2.65 & 87.07 & 69.96 & 3.76 & 61.30 & 49.52 \\  & Qwen-VL-Chat & 79.03 & 51.58 & 2.76 & 86.41 & 59.77 & 3.25 & 59.10 & 42.05 \\  & mPlug-Owl2 & 78.26 & 47.38 & 2.57 & 86.20 & 48.05 & 2.59 & 62.17 & 43.10 \\  & LLaVA-1.6-7B & 80.71 & 70.36 & 2.79 & 86.58 & 75.36 & 3.24 & 47.41 & 37.07 \\  & InstructBlip-7B & 76.02 & 38.02 & 2.60 & 86.32 & 66.29 & 2.85 & 25.86 & 26.44 \\   & ChatGPT & - & - & - & **87.78** & 67.42 & 3.54 & 75.86 & 49.52 \\  & Llama-3-8B-Instruct & - & - & - & 87.41 & 70.52 & 3.59 & 72.13 & 49.71 \\   & Mistral-7B-Instruct & - & - & - & 87.01 & 67.70 & 3.64 & 66.00 & 45.98 \\   

Table 2: Main results. For literal description and contradiction, we report BERT score (recall), BLEURT (BLT), and GPT evaluation score. For philosophy and title, we report accuracy (%). Best scores are **bold** and the second best ones are marked with underline.

### Narrative Understanding Tasks

**Literal Description:** We evaluate the results of VLMs only for this task. We observe that the two commercial models generally outperform the smaller open-sourced models. Among these models, GPT-4 achieves the highest scores. For the open-sourced models, the larger model variants (13B) consistently achieve better scores than their 7B counterparts, indicating that larger models have a superior ability to understand the image and produce higher-quality literal descriptions.

**Contradiction Generation:** A similar trend is observed where GPT-4 and Claude-3 achieve better results than other VLM models. Notably, LLaVA-1.6 variants outperform their counterparts in generating contradiction descriptions. This is likely due to their improved reasoning ability and world knowledge , which are essential for understanding comic narratives and accurately capturing the relationship between the two panels. For LLMs, unlike VLMs, the Llama-3 and Mistral models achieve results comparable to ChatGPT. Another interesting observation is that Llama-3 and Mistral obtain similar or better results for contradiction generation compared to open-sourced VLMs, despite not having access to the original comic images.

### Deep Reasoning Tasks

The Underlying Philosophy Selection and Title Matching tasks require in-depth reasoning based on the comic narratives. As seen in Table 2, for philosophy selection, Claude-3 achieves the best accuracy with 84.10%, while for title matching, the LLaVA-1.6 34B variant ranks the highest with 63.31% accuracy. One key observation is that larger models usually perform better in-depth understanding of the comics, aligning with the findings that larger models typically exhibit superior reasoning abilities [66; 67].

Additionally, LLMs achieve performance comparable to open-sourced VL models. This can be attributed to the strong reasoning abilities of models like Llama-3 and Mistral [59; 20], which are crucial for understanding narratives and performing nonlinear reasoning to grasp deep semantics. Further analysis on the influence of descriptions for LLMs is provided in Section 6.1.

Another observation is that model performance on title matching is consistently lower than on underlying philosophy selection. Titles are shorter and more abstract versions of the narrative and do not explicitly convey the underlying idea of the comic. Therefore, distinguishing the correct title from distractions requires a deeper rigorous understanding and reasoning abilities, making it more challenging for models. Notably, the human evaluation results show a similar trend of our proposed GPT-based evaluation, demonstrating its effectiveness.

### Human Evaluations

We conduct human evaluations on 30 randomly selected samples to assess the output quality of literal descriptions and contradiction generation, as shown in Figure 3. Similar trends are observed in both human and automatic evaluations: commercial models generally outperform open-source models in producing both literal descriptions and contradictions, with GPT-4 achieving the highest scores in both tasks. Additionally, the scores for literal descriptions are consistently higher than those for contradictions across all models, suggesting that understanding narrative contradictions is

Figure 3: Human Evaluation on literal description and contradiction generation.

more challenging than generating literal descriptions, which requires in-depth reasoning to compare the various aspects of both panels.

A comparison of the scores for literal description and contradiction reveals a strong correlation between the two tasks: models that perform well on literal descriptions also tend to achieve good results on contradictions. This indicates that understanding comic juxtaposition requires a diverse set of skills, including image understanding, narrative comprehension, and reasoning abilities.

## 6 Analysis and Discussion

### How Does Literal Understanding of the Comic Influence Deep Reasoning?

We investigate whether the quality of surface-level literal descriptions influences subsequent deep reasoning tasks. For LLMs, we provide different literal descriptions generated by LLaVA-1.6 7B and 13B variants, as well as oracle descriptions written by humans, as model inputs. The results are shown in Figure 4. As the quality of literal descriptions improves, the prediction accuracy for both underlying philosophy and title selection also improves. This demonstrates a strong correlation between deep reasoning and literal narrative understanding. However, a significant performance gap remains compared to when oracle descriptions are used.

We further examine the performance of VLMs by providing them with additional oracle descriptions. The results are shown in Figure 5. Compared to using only the comic image as input, augmenting with human-written literal descriptions significantly improves the deep reasoning results for all VLMs. This confirms that correctly reasoning about the underlying semantics of a comic requires first accurately understanding its surface narrative. However, the performance gap indicates that current VLMs still lag in narrative understanding.

Additionally, an interesting observation from Figures 4 and 5 is that when the oracle literal description is provided as (partial) input, LLMs tend to outperform their VLM counterparts in both philosophy and title selection. For example, LLaVA-1.6-7B employs Mistral-7B as the language model backbone, yet its performance under oracle description is significantly worse than that of Mistral-7B. One possible reason is that incorporating oracle descriptions makes VLM input much longer, thus making prediction more challenging. We provide further discussions in Section6.2.

### Is Decomposing Literal Description Helpful for Deep Reasoning of VLMs?

VLMs typically predict results in an end-to-end fashion, requiring the model to perform image captioning, narrative understanding, and deep reasoning all at once. Here, we investigate whether decomposing the task into separate stages of narrative understanding and in-depth reasoning can improve model performance. Specifically, we first prompt the VLM to produce a literal description of a comic; then the VLM predicts results based on both the comic image and the description. The results are shown in Table 3.

As observed, decomposing the task and augmenting it with a literal description does not necessarily improve performance. In fact, when descriptions are incorporated, performance across all models declines on the title selection task, which contrasts with previous findings . One possible explanation for this drop in performance is that the generated descriptions may contain errors, negatively impacting the model's deep understanding. Another explanation could be the length of the generated descriptions (e.g., the LLaVA-1.6 13B model's descriptions average around 170 words), leading to longer and more complex prompts that make prediction more challenging. We leave a more detailed investigation of this issue for future work.

### Error Analysis and Future Directions

We present sample outputs of contradictions generated by vision language models (VLMs) in Figure 6. VLMs can make various errors in contradiction understanding.

One type of error is **visual misinterpretation,** where the model incorrectly interprets the image contents. For example, in sample 1, CogVLM misinterprets the image by recognizing a person "shown with a full beard." Similarly, in sample 2, LLaVA-1.6 13B misunderstands the image contents and generates incorrect content about "two individuals," which is inconsistent with the comic. Such misinterpretations can lead to incorrect understanding of the narrative. These observations align with our previous findings in Section 6.1 and Section 6.2. _This highlights the need for future research to improve models' visual interpretation capabilities_.

Models also struggle to conduct **in-depth reasoning of the relationship** between two panels by recognizing their differences and similarities. In sample 1, while the comic implies a comparison between the expected disposable razor and its actual longevity, GPT-4 incorrectly explains the contradiction as being about the razor's quality. A similar error occurs with mPlug-Owl2 in sample 2, where it incorrectly thinks the bed sizes are different in the two panels, leading to a wrong illustration focusing on the bed size. _Future work might incorporate recent advanced reasoning

   Models & Philosophy & Title \\  LLaVA-1.6-13B & 69.16 & 55.08 \\ \(\) w/ desp. & 68.68 & 48.76 \\ Qwen-VL-Chat & 59.10 & 42.05 \\ \(\) w/ desp. & 59.58 & 37.55 \\ mPlug-Owl2 & 62.17 & 43.10 \\ \(\) w/ desp. & 60.25 & 37.84 \\ LLaVA-1.6-7B & 47.41 & 37.07 \\ \(\) w/ desp. & 53.07 & 34.96 \\   

Table 3: Decomposition model results augmenting the predicted description.

Figure 6: Sample outputs of contradiction explanations generated by different vision language models, along with human written references. We highlight different types of errors in model outputs.

approaches (e.g., multi-agent debate , test-time compute scaling ) to further improve model performance._

Another common error is **hallucination and incorrect association**. This is evident in sample 3. The original comic contrasts latte art before and after lidding the drink, but Claude-3 incorrectly associates the narrative with environmental protection, focusing on the plastic lid. Meanwhile, LLaVA-1.6 13B model suffers from hallucinations by interpreting the narrative as being about relaxation and enjoyment, which is unsupported by the original comic. _This suggests the need for improving world knowledge and social understanding abilities to enhance model performance on this task._ More sample outputs are in Appendix C.

## 7 Conclusion

In this work, we present YesBut, the first benchmark dedicated to studying comic understanding through juxtaposition. YesBut encompasses a variety of tasks that address both narrative comprehension and deep reasoning. The results indicate that state-of-the-art vision and language models still struggle with these tasks. We also offer a comprehensive analysis and discussion of errors to evaluate model performance. Current models still struggle to accurately interpret the visual contents and conduct in-depth reasoning of the underlying narratives. Through this study, we aim to provide insights for future research and advance the capabilities of AI models in understanding human context, ultimately contributing to more effective and culturally aware AI applications.

## 8 Limitations

We propose a comprehensive data annotation process to annotate each component. However, due to the subjectivity of comic interpretation, especially regarding the underlying ideas, there might be potential ambiguity. While we acknowledge the relatively small size of images, we rigorously collect comics and annotate each component, ensuring their high-quality and reliability. We plan to expand the dataset with the inclusion of different types of narratives in future work.

Our proposed benchmark focuses predominantly on recognizing and interpreting visual humor via juxtaposition, and may not cover all aspects of visual understanding required for more generalized AI applications. In the future, we intend to explore more deeply how AI can not only interpret but also creatively engage with content. This includes generating pivotal turning points from one perspective and creating counterpoints to given scenarios, like generating a "YES" image's counterpart.

## 9 Ethics Statement

**Copyright and License.** All data samples collected are sourced from publicly available content on social media platforms. We ensure compliance with copyright by utilizing original links to comics without infringement. In addition, we obtained permission from the author artist (e.g., Anton Gudim, Liz Climo) to conduct our benchmark using these public images. Additionally, we commit to open-sourcing our annotated benchmark, providing corresponding links to each comic image. We diligently review samples, filtering out potentially offensive or harmful content.

**The Large Vision Language Models** utilized in our experiments are pretrained using diverse web corpora, which may introduce biases in their outputs. We advise users to conscientiously evaluate the ethical implications of generated outputs when employing them in future research endeavors.

**Data Annotation.** Eight human judges are engaged in our annotation process. We compensate these judges with an average hourly wage of $11, ensuring fair remuneration for their contributions.