# A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm

Ilias Diakonikolas

University of Wisconsin-Madison

ilias@cs.wisc.edu

&Daniel M. Kane

University of California, San Diego

dakane@cs.ucsd.edu

Jasper C.H. Lee

University of Wisconsin-Madison

jasper.lee@wisc.edu

&Ankit Pensia

IBM Research

ankitp@ibm.com

&Thanasis Pittas

University of Wisconsin-Madison

pittas@wisc.edu

###### Abstract

We study the problem of list-decodable Gaussian covariance estimation. Given a multiset \(T\) of \(n\) points in \(^{d}\) such that an unknown \(<1/2\) fraction of points in \(T\) are i.i.d. samples from an unknown Gaussian \((,)\), the goal is to output a list of \(O(1/)\) hypotheses at least one of which is close to \(\) in relative Frobenius norm. Our main result is a \((d,1/)\) sample and time algorithm for this task that guarantees relative Frobenius norm error of \((1/)\). Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) -- a key ingredient in the recent work of  on robustly learning arbitrary GMMs. Combined with the other components of , our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs, resolving an open problem proposed by Vempala  and Kothari . At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings.

## 1 Introduction

Robust statistics studies the efficient (parameter) learnability of an underlying distribution given samples some fraction of which might be corrupted, perhaps arbitrarily. While the statistical theory of these problems has been well-established for some time , only recently has the algorithmic theory of such estimation problems begun to be understood .

A classical problem in robust estimation is that of multivariate robust mean estimation -- that is, estimating the mean of an unknown distribution in the presence of a small constant fraction of outliers. One of the original results in the field is that given samples from a Gaussian \((,I)\) with an \(\)-fraction of outliers (for some \(<1/2\)), the unknown mean \(\) can be efficiently estimated to an error of \(O()\) in the \(_{2}\)-norm . Note that the use of the \(_{2}\)-norm here is quite natural, as the total variation distance between two identity-covariance Gaussians is roughly proportional to the \(_{2}\)-distance between their means; thus, this estimator learns the underlying distribution to total variation distance error \(O()\). This bound cannot be substantially improved, as learning to error \(o()\) in total variation distance is information-theoretically impossible.

While the above algorithmic result works when \(\) is small (less than \(1/2\)), if more than half of the samples are corrupted, it becomes impossible to learn with only a single returned hypothesis--the corruption might simply simulate other Gaussians, and no algorithm can identify which Gaussian is the original one. This issue can be circumvented using the _list-decodable_ mean estimationparadigm , where the algorithm is allowed to output a small _list of hypotheses_ with the guarantee that at least one of them is relatively close to the target. List-decodable learning is closely related to _semi-verified learning_, where a learner can choose to audit a small amount of data. The framework has been shown to capture a multitude of applications, including _crowdsourcing_, _semi-random community detection_ in stochastic block models  and _clustering_ (as we also show in this work). For Gaussian mean estimation in particular, if an \(\)-fraction of the samples are clean (i.e., uncorrupted), for some \(<1/2\), there exist polynomial-time algorithms that return a list of \(O(1/)\) hypotheses such that (with high probability) at least one of them is within \(()\) of the mean in \(_{2}\)-distance . Note that while this \(_{2}\)-norm distance bound does not imply a good bound on the total variation distance between the true distribution and the learned hypothesis, it does bound their distance away from one, ensuring some non-trivial overlap.

Another important, and arguably more complex, problem in robust statistics (the focus of this work) is that of robustly estimating the covariance of a multivariate Gaussian. It was shown in  that given \(\)-corrupted samples from \((0,)\) (for \(<1/2\)) there is a polynomial-time algorithm for estimating \(\) to error \(O( 1/)\) in the relative Frobenius norm, i.e., outputting a hypothesis covariance \(\) satisfying \(\|^{-1/2}^{-1/2}-I\|_{}\). This is again the relevant metric, since, if \(\|^{-1/2}^{-1/2}-I\|_{}  2/3\), then the total variation distance between \((0,)\) and \((0,)\) is proportional to the relative Frobenius norm \(\|^{-1/2}^{-1/2}-I\|_{}\).

A natural goal, with a number of applications, is to extend the above algorithmic result to the list-decodable setting. That is, one would like a polynomial-time algorithm that given corrupted samples from \((0,)\) (with an \(\)-fraction of clean samples, for some \(<1/2\)), returns a list of \(O(1/)\) many hypotheses with the guarantee that at least one of them has some non-trivial overlap with the true distribution in total variation distance. We start by noting that the sample complexity of list-decodable covariance estimation is \((d/)\), albeit via an exponential time algorithm. The only known algorithm for list-decodable covariance estimation (with total variation error guarantees) is due to . This algorithm essentially relies on the Sum-of-Squares method and has (sample and computational) complexity \(d^{(1/)}\). Intriguingly, there is compelling evidence that this complexity bound cannot be improved. Specifically,  showed that any Statistical Query (SQ) algorithm for this task requires complexity \(d^{(1/)}\). Combined with the reduction of , this implies a similar lower bound for low-degree polynomial tests. These lower bounds suggest an intrinsic information-computation gap for the problem.

The aforementioned lower bound results  establish that it is SQ (and low-degree) hard to distinguish between a standard multivariate Gaussian, \((0,I)\), and a distribution \(P\) that behaves like \((0,I)\) (in terms of low-degree moments) but \(P\) contains an \(\)-fraction of samples from a Gaussian \((0,)\), where \(\) is _very thin_ in some hidden direction \(v\). Since the distribution \((0,)\) is very thin along \(v\), i.e., has very small variance, the obvious choice of \((0,I)\) essentially has no overlap with \((0,)\) -- making this kind of strong list-decoding guarantee (closeness in total variation distance) likely computationally intractable.

Interestingly, there are two possible ways that a pair of mean zero Gaussians can be separated : (1) one could be much thinner than the other in some direction, or (2) they could have many orthogonal directions in which their variances differ, adding up to something more substantial. While the lower bounds of  seem to rule out being able to detect deviations of the former type in fully polynomial time (i.e., \((d/)\)), it does not rule out efficiently detecting deviations of the latter. In particular, we could hope to find (in \((d/)\) time) a good hypothesis \(\) such that \(\|^{-1/2}^{-1/2}-I\|_{}\) is not too big. While this does not exclude the possibility that \(\) is much thinner than \(\) in a small number of independent directions, it does rule out the second kind of difference between the two. The main goal of this paper is to provide an elementary (relying only on spectral techniques) list-decoding algorithm of this form. Given corrupted samples from a Gaussian \((0,)\), we give a \((d/)\)-time algorithm that returns a small list of hypotheses \(\) such that for at least one of them we have that \(\|^{-1/2}^{-1/2}-I\|_{}< (1/)\).

In addition to providing the best qualitative guarantee we could hope to achieve in fully polynomial time, the above kind of "weak" list-decoding algorithm has interesting implications for the well-studied problem of robustly learning Gaussian mixture models (GMMs).  gave a polynomial-time algorithm to robustly learn arbitrary mixtures of Gaussians (with a constant number of components). One of the two main ingredients of their approach is a subroutine that could perform partial clustering of points into components that satisfy exactly this kind of weak closeness guarantee.  developed such a subroutine by making essential use of a Sum-of-Squares (SoS) relaxation, for the setting that the samples come from a mildly corrupted mixture of Gaussians (with a small constant fraction of outliers). As a corollary of our techniques, we obtain an elementary spectral algorithm for this partial clustering task (and, as already stated, our results work in the more general list-decoding setting). This yields the first SoS-free algorithm for robustly learning GMMs, answering an open problem in the literature .

### Our Results

Our main result is a polynomial-time algorithm for list-decoding the covariance of a Gaussian in relative Frobenius norm, under adversarial corruption where more than half the samples could be outliers.  makes precise the corruption model, and  states the guarantees of our main algorithm (Algorithm 1).

**Definition 1.1** (Corruption model for list-decoding).: _Let the parameters \(,(0,1/2)\) and a distribution family \(\). The statistician specifies the number of samples \(m\). Then a set of \(n m\) i.i.d. points are sampled from an unknown \(D\). We call these \(n\) samples the inliers. Upon inspecting the \(n\) inliers, a (malicious and computationally unbounded) adversary can replace an arbitrary \( n\) of the inliers with arbitrary points, and further add \(m-n\) arbitrary points to the dataset, before returning the entire set of \(m\) points to the statistician. The parameter \(\) is also known a-priori to the statistician, but the number \(n\) chosen by the adversary is unknown. We refer to this set of \(m\) points as an \((,)\)-corrupted set of samples from \(D\)._

In our context, the notion of list-decoding is as follows: our algorithm will return a polynomially-sized list of matrices \(H_{i}\), such that at least one \(H_{i}\) is an "approximate square root" of the true covariance \(\) having bounded _dimension-independent_ error \(\|H_{i}^{-1/2} H_{i}^{-1/2}-I\|_{}\). As discussed earlier, the bound we guarantee is \((1/)\), which is in general larger than \(1\) and thus does not lead to non-trivial total variation bounds, thus circumventing related SQ lower bounds. Our main theorem is:

**Theorem 1.2** (List-Decodable Covariance Estimation in Relative Frobenius Norm).: _Let \(C^{}>0\) be a sufficiently large constant and \(_{0}>0\) be a sufficiently small constant. Let the parameters \((0,1/2)\), \((0,_{0})\), and failure probability \((0,1/2)\) be known to the algorithm. Let \(D\) be the Gaussian distribution \((,)\) with mean \(^{d}\) and full-rank covariance \(^{d d}\). There is an \((m^{2}d^{2})\)-time algorithm (Algorithm 1) such that, on input \(,\) and an \((,)\)-corrupted set of \(m\) points from \(D\) () for any \(m>C^{}^{5}(d/)}{^{6}}\), with probability at least \(1-\), the algorithm returns a list of at most \(O(1/)\) many sets \(T_{i}\) which are disjoint subsets of samples, each of size at least \(0.5 m\), and there exists a \(T_{i}\) in the output list such that:_

* _Recall the notation in the corruption model (_Definition 1.1_) where_ \(n\) _is the size of the original inlier set_ \(S\) _and_ \(\) _is the number of points in_ \(S\) _that the adversary replaced--_\(n\) _and_ \(\) _are unknown to the algorithm except that_ \(n m\) _and_ \( n\)_. The set_ \(T_{i}\) _in the returned list satisfies that_ \(|T_{i} S|(1-0.01)(n-)\)_._
* _Denote_ \(H_{i}:=_{X T_{i}}[XX^{}]\)_. The matrix_ \(H_{i}\) _satisfies_ \(\|H_{i}^{-1/2} H_{i}^{-1/2}-I\|_{} }()\)_._

Algorithm 1 is an iterative spectral algorithm, as opposed to involving large convex programs based on the sum-of-squares hierarchy. We state  and give a high-level proof sketch of why it satisfies Theorem 1.2 in Section 3. The formal proof of Theorem 1.2 appears in F (where we allow \(\) to be rank-deficient as well).

We also briefly remark that, if we wish to list-decode pairs of (mean, covariance), then we can run the following straightforward augmentation to : after running Algorithm 1, for each \(\) in the output list, whiten the data using \(\) and run a standard list-decoding algorithm for the mean . These algorithms work if the data is distributed with a bounded covariance: by the guarantees of Theorem 1.2, after whitening, the data has covariance bounded by \((1/) I\).

As a corollary of our main result, the same algorithm (but with a slightly higher sample complexity) also achieves outlier-robust list-decoding of the covariances for the components of a Gaussian mixture model, in relative Frobenius norm.  and Theorem 1.4 state the corresponding corruption model and theoretical guarantees on Algorithm 1.

**Definition 1.3** (Corruption model for samples from Gaussian Mixtures).: _Let \((0,1/2)\). Consider a Gaussian mixture model \(_{p}_{p}(_{p},_{p})\), where the parameters \(_{p},_{p}\) and \(_{p}\) are unknown and satisfy \(_{p}\) for some known parameter \(\). The statistician specifies the number of samples \(m\), and \(m\) i.i.d. samples are drawn from the Gaussian mixture, which are called the \(\). The malicious and computationally unbounded adversary then inspects the \(m\) inliers and is allowed to replace an arbitrary subset of \( m\) many inlier points with arbitrary outlier points, before giving the modified dataset to the statistician. We call this the \(\)-corrupted set of samples._

**Theorem 1.4** (Outlier-Robust Clustering and Estimation of Covariances for GMM).: _Let \(C^{}>0\) be a sufficiently large constant and \(_{0}>0\) be a sufficiently small constant. Let the parameters \((0,1/2)\), \((0,_{0})\), and failure probability \((0,1/2)\) be known. There is an \((m^{2}d^{2})\)-time algorithm such that, on input \(,\), and \(m>C^{}^{2}(d/)}{^{}}\) many \(\)-corrupted samples from an unknown \(k\)-component Gaussian mixture \(_{p=1}^{k}_{p}(_{p},_{p})\) over \(^{d}\) as in Definition 1.3, where all \(_{p}\)'s are full-rank and all \(_{p}\) satisfies \(_{p}\) and \(k\) is unknown to the algorithm, with probability at least \(1-\) over the corrupted samples and the randomness of the algorithm, the algorithm returns a list of at most \(k\) many disjoint subsets of samples \(\{T_{i}\}\) such that:_

* _For the_ \(p^{}\) _Gaussian component, denote the set_ \(S_{p}\) _as the samples in the inlier set_ \(S\) _that were drawn from component_ \(p\)_. Let_ \(n_{p}\) _be the size of_ \(S_{p}\)_, and let_ \(_{p}\) _be the number of points in_ \(S_{p}\) _that the adversary replaced--_\(n_{p}\) _and_ \(_{p}\) _are both unknown to the algorithm except that_ \([n_{p}]=_{p}m m\) _for each_ \(p\) _and_ \(_{p}_{p} m\)_. Then, for every Gaussian component_ \(p\) _in the mixture, there exists a set_ \(T_{i_{p}}\) _in the returned list such that_ \(|T_{i_{p}} S_{p}|(1-0.01)(n_{p}-_{p})\)_._
* _For every component_ \(p\)_, there is a set of samples_ \(T_{i_{p}}\) _in the returned list such that, defining_ \(H_{i_{p}}=_{X T_{i_{p}}}[XX^{}]\) _, we have_ \(H_{i_{p}}\) _satisfying_ \(\|H_{i_{p}}^{-1/2}_{p}H_{i_{p}}^{-1/2}-I\|_{}(1/ ^{4})(1/)\)_._
* _Let_ \(\) _be the (population-level) covariance matrix of the Gaussian mixture. For any two components_ \(p p^{}\) _with_ \(\|^{-1/2}(_{p}-_{p^{}})^{-1/2}\|_{}>C( 1/)^{5}(1/)\) _for a sufficiently large constant_ \(C\)_, the sets_ \(T_{i_{p}}\) _and_ \(T_{i_{p^{}}}\) _from the previous bullet are guaranteed to be different._

The theorem states that, not only does Algorithm 1 achieve list-decoding of the Gaussian component covariances, but it also clusters samples according to separation of covariances in relative Frobenius norm. The recent result of  on robustly learning GMMs also involves an algorithmic component for clustering. Their approach is based on the sum-of-squares hierarchy (and thus requires solving large convex programs) while, Algorithm 1 is a purely spectral algorithm.

We also emphasize that the list size returned by Algorithm 1, in the case of a \(k\)-component Gaussian mixture model, is at most \(k\) -- instead of a weaker result such as \(O(1/)\) or polynomial/exponentially large in \(1/\). This is possible because Algorithm 1 keeps careful track of samples and makes sure that no more than a \(0.01\)-fraction of samples is removed from each component or mis-clustered into another component. We prove Theorem 1.4 in Appendix G.

### Overview of Techniques

At a high level, our approach involves integrating robust covariance estimation techniques from  with the multifilter for list-decoding of . For a set \(S\), we use \(SS^{}\) to denote the set \(\{xx^{}:x S\}\).  states that in order to estimate the true covariance \(\), it suffices to find a large subset \(S\) of points with large overlap with the original set of good points, so that if \(^{}=(S)\) then \(((^{})^{-1/2}SS^{}(^{})^{-1/2})\) has no large eigenvalues. This will imply that \(\|(^{})^{-1/2}(-^{})(^{})^{-1/2}\|_ {}\) is not too large (Lemma E.2).

As in , our basic approach for finding such a set \(S\) is by the iteratively repeating the following procedure: We begin by taking \(S\) to be the set of all samples and repeatedly check whether or not \(((^{})^{-1/2}SS^{}(^{})^{-1/2})\) has any large eigenvalues. If it does not, we are done. If it has a large eigenvalue corresponding to a matrix \(A\) (normalized to have unit Frobenius norm), we consider the values of \(f(x)= A,(^{})^{-1/2}xx^{}(^{})^{-1/2}\), for \(x\) in \(S\), and attempt to use them to find outliers. Unfortunately, as the inliers might comprise a minority of the sample, the values we get out of this formula might end-up in several reasonably large clusters, any one of which could plausibly contain the true samples; thus, not allowing us to declare any particular points to be outlierswith any degree of certainty. We resolve this issue by using the multifilter approach of --we either (i) iteratively remove outliers, or (ii) partition the data into clusters and recurse on each cluster. In particular, we note that if there is large variance in the \(A\)-direction, one of two things must happen, either: (i) The substantial majority of the values of \(f(x)\) lie in a single cluster, with some extreme outliers. In this case, we can be confident that the extreme outliers are actual errors and remove them (Section 3.2). (ii) There are at least two clusters of values of \(f(x)\) that are far apart from each other. In this case, instead of simply removing obvious outliers, we replace \(S\) by two subsets \(S_{1}\) and \(S_{2}\) with the guarantee that at least one of the \(S_{i}\) contains almost all of the inliers. Naively, this can be done by finding a value \(y\) between the two clusters so that very few samples have \(f(x)\) close to \(y\), and letting \(S_{1}\) be the set of points with \(f(x)<y\) and \(S_{2}\) the set of points with \(f(x)>y\) (Section 3.3). In either case, we will have cleaned up our set of samples and can recurse on each of the returned subsets of \(S\). Iterating this technique recursively on all of the smaller subsets returned ensures that there is always at least one subset containing the majority of the inliers, and that eventually once it stops having too large of a covariance, we will return an appropriate approximation to \(\).

We want to highlight the main point of difference where our techniques differ notably from . In order to implement the algorithm outlined above, one needs to have good a priori bounds for what the variance of \(f(x)\) over the inliers ought to be. Since \(f()\) is a quadratic polynomial, the variance of \(f\) over the inliers, itself depends on the covariance \(\), which is exactly what we are trying to estimate. This challenge of circular dependence does not appear in : their goal was to estimate the unknown mean of an identity-covariance Gaussian, and thus it sufficed to use a linear polynomial \(f\) (instead of a quadratic polynomial). Importantly, the covariance of a linear polynomial does not depend on the (unknown) mean (it depends only on the covariance, which was known in their setting). In order to overcome this challenge, we observe that if \(S\) contains most of the inliers, then the covariance of \(S\) cannot be too much smaller than the true covariance \(\). This allows us to find an upper bound on \(\), which in turn lets us upper bound the variance of \(f(x)\) over the good samples (Lemma 3.2).

Related WorkWe refer the reader to  for an overview of algorithmic robust statistics. We mention the most relevant related work here and discuss additional related work in Appendix A. Algorithms for list-decodable covariance estimation were developed in the special cases of subspace estimation and linear regression in . On the other hand,  present SQ lower bounds for learning Gaussian mixture models and list-decodable linear regression (and thus list-decodable covariance estimation), respectively.

 gave the first algorithm for general list-decodable covariance estimation that achieves non-trivial bounds in total variation distance using the powerful sum-of-squares hierarchy. Their algorithm outputs an \(((1/))\)-sized list of matrices containing an \(H_{i}\) that is close to the true \(\) in two metrics (i) relative Frobenius norm: \(\|H_{i}^{-1/2} H_{i}^{-1/2}-I\|_{}=(1/)\)_and_ (ii) multiplicative spectral approximation: \(() H_{i}(1/)\). Their algorithm uses \(d^{(1/)}\) samples, which seems to be necessary for efficient (statistical query) algorithms achieving multiplicative spectral approximation . In comparison, Theorem 1.2 uses only \((d/)\) samples, returns a list of \(O(1/)\) matrices, but approximates only in the relative Frobenius norm: \(\|H_{i}^{-1/2} H_{i}^{-1/2}-I\|_{}=(1/)\).

## 2 Preliminaries

For a vector \(v\), we let \(\|v\|_{2}\) denote its \(_{2}\)-norm. We use \(I_{d}\) to denote the \(d d\) identity matrix; We will drop the subscript when it is clear from the context. For a matrix \(A\), we use \(\|A\|_{}\) and \(\|A\|_{}\) to denote the Frobenius and spectral (or operator) norms, respectively. We denote by \( v,u\), the standard inner product between the vectors \(u,v\). For matrices \(U,V^{d d}\), we use \( U,V\) to denote the trace inner product \(_{ij}U_{ij}V_{ij}\). For a matrix \(A^{d d}\), we use \(A^{}\) to denote the flattened vector in \(^{d^{2}}\), and for a \(v^{d^{2}}\), we use \(v^{}\) to denote the unique matrix \(A\) such that \(A^{}=v^{}\). For a matrix \(A\), we let \(A^{}\) denote its pseudo-inverse. We use \(\) to denote the Kronecker product. For a matrix \(A\), we use \((A)\) for the null space of \(A\). We use \(X D\) to denote that a random variable \(X\) is distributed according to the distribution \(D\). We use \((,)\) for the Gaussian distribution with mean \(\) and covariance matrix \(\). For a set \(S\), we use \(X S\) to denote that \(X\) is distributed uniformly at random from \(S\). We use \(a b\) to denote that there exists an absolute universal constant \(C>0\) (independent of the variables or parameters on which \(a\) and \(b\) depend) such that \(a Cb\).

### Deterministic Conditions

Our algorithm will rely on the uncorrupted inliers satisfying a set of properties, similar to the "stability conditions" from . Intuitively, these are are concentration properties for sets of samples, but with the added requirement that every large subset of the samples also satisfies these properties.

**Definition 2.1** (\((,)\)-Stable Set).: _Let \(D\) be a distribution with mean \(\) and covariance \(\). We say a set of points \(A^{d}\) is \((,)\)-stable with respect to \(D\), if for any subset \(A^{} A\) with \(|A^{}|(1-)|A|\), the following hold: for every \(v^{d}\), symmetric \(U^{d d}\), and every even degree-\(2\) polynomial \(p\):_

1. \(|}_{x A^{}}v^{}(x-) 0.1 v)}\)_._
2. \(|}_{x A^{}}(x-)(x-)^ {}-,U 0.1\,^{1/2}U^{1/2} _{}\)__
3. \(}{}[p(X)\!-\!}[p(X)]\!\!>\!\!10}[p( Y)]}()]\!\!\!\!\)_._
4. \(_{X A^{}}[p(X)] 4\,_{X D}[p(X)]\)_._
5. _The null space of second moment matrix of_ \(A^{}\) _is contained in the null space of_ \(\)_, i.e.,_ \((_{x A^{}}xx^{})()\)_._

A Gaussian dataset is "stable" with high probability ; formally, we have Lemma2.2, proved in AppendixD.1. Moreover, Lemma2.2 can be extended to a variety of distributions (cf. RemarkD.1).

**Lemma 2.2** (Deterministic Conditions Hold with High Probability).: _For a sufficiently small positive constant \(_{0}\) and a sufficiently large absolute constant \(C\), a set of \(m>Cd^{2}^{5}(d/())/^{2}\) samples from \((,)\), with probability \(1-\), is \((,)\)-stable set with respect to \(,\) for all \(_{0}\)._

## 3 Analysis of a Single Recursive Call of the Algorithm

Our algorithm, Algorithm1, filters and splits samples into multiple sets recursively, until we can certify that the empirical second moment matrix of the "current data set" is suitable to be included in the returned list of covariances. As a reference point, we define the notations and assumptions necessary to analyze each recursive call below. However, before moving to the formal analysis we will first give an informal overview of the algorithm's steps and the high-level ideas behind them.

**Assumption 3.1** (Assumptions and notations for a single recursive call of the algorithm).:
* \(S=\{x_{i}\}_{i=1}^{n}\) is a set of \(n\) uncontaminated samples, which is assumed to be \((,2_{0})\)-stable with respect to the inlier distribution \(D\) having mean and covariance \(,\) (c.f. Definition2.1). We assume \( 0.001\), \(_{0}=0.01\), and \(_{X D}[X^{}AX] C_{1}(\|^{1/2}A^{1/2}\|_{F }^{2}+\|^{1/2}A\|_{2}^{2})\) for all symmetric \(d d\) matrices \(A\) and a constant \(C_{1}\).
* \(T\) is the input set to the current recursive call of the algorithm (after the adversarial corruptions), which satisfies \(|S T|(1-2_{0})|S|\) and \(|T|(1/)|S|\).
* We denote \(H=_{X T}[XX^{}]\).
* We denote by \(,\) the versions of \(S\) and \(T\) normalized by \(H^{/2}\): \(=\{H^{/2}x:x S\}\) and \(=\{H^{/2}x:x T\}\). We use the notation \(\) for elements in \(\) and \(\), and \(x\) for elements in \(S\) and \(T\). Similarly, we use the notation \(\) for random variables with support in \(\) or \(\).
* The mean and covariance of the inlier distribution \(D\) after transformation with \(H^{/2}\) are denoted by \(:=H^{/2}\), \(:=H^{/2} H^{/2}\). We denote the empirical mean and covariance of the transformed inliers in \(T\) by \(:=_{}[ ]\) and \(:=_{} []\).

Much of the algorithm uses the fact that, for a Gaussian, even quadratic polynomials have a small variance. We will leverage this for filtering and clustering samples. See E for the proof.

**Lemma 3.2**.: _Make Assumption 3.1 and recall that \(H=_{X T}[XX^{}]\), where \(T\) is the corrupted version of a stable inlier set \(S\). For every symmetric matrix \(A\) with \(\|A\|_{}=1\), we have that \(_{X D}[(H^{1/2}X)^{}A(H^{/2}X)] 18C_{1}/^{2}\)._

Armed with Lemma3.2, we can now give a high-level overview of a recursive call of Algorithm1:

1. In our notation, we call the current data set \(T\). Denoting \(H=_{X T}[XX^{}]\) for its the empirical second moment matrix, we construct the normalized data set \(=\{H^{-1/2}x\ :\ x T\}\). The normalization allows us to bound the covariance \(\) in terms of \(H\).
2. Since we are trying to estimate a covariance, consider the vectors \(=\{(^{})^{}:\}\), which are the second moment matrices of each data point flattened into vectors.
3. The first step is standard in filtering-based outlier-robust estimation: we test whether the covariance of the \(\) vectors is small. If so, we are able to prove that the current \(H\) is a good approximate square root of \(\) (c.f. Section3.1) hence we just return \(H\).
4. If the first test fails, that would imply that the empirical covariance of the \(\) vectors is large in some direction. We want to leverage this direction to make progress, either by removing outliers through filtering or by bi-partitioning our samples into two clear clusters.
5. To decide between the 2 options, consider projecting the \(\) vectors onto their largest variance direction. Specifically, let \(A\) be the matrix lifted from the largest eigenvector of the covariance of the \(\) vectors. Define the vectors \(=^{}A=^{},A\) for \(\), corresponding to the 1-dimensional projection of \(\) onto the \(A^{}\) direction. Since we have failed the first test, these \(\) elements must have a large variance. We will decide to filter or divide our samples, based on whether the \( m\)-smallest and \( m\)-largest elements of the \(\)s are close to each other.
6. If they are close, yet we have large variance, we will use this information to design a _score function_ and perform filtering that removes a random sample with probability proportional to its score. We will then go back to Step1. This would work because by Lemma3.2 and by stability (Definition2.1), the (unfiltered) inliers have a small empirical variance within themselves, meaning that the large total empirical variance is mostly due to the outlier.

Ideally, the score of a sample would be (proportional to) the squared distance between the sample and the mean of the inliers--the total inlier score would then be equal to the inlier variance. However, since we do not know which points are the inliers, we instead use the median of all the projected samples as a proxy for the unknown inlier mean. We show that the distance between the \( m\)-smallest and largest \(\)s bounds the difference between the ideal and proxy scores.
7. Otherwise, \( m\)-smallest and \( m\)-largest elements of the \(\)s are far apart. By the stability condition Definition2.1 (specifically, Condition(L.3)), most of the inliers must be close to each other under this 1-dimensional projection. Therefore, the large quantile range necessarily means there is a threshold under this projection to divide the samples into two sets, such that each set has at least \( m\) points and most of the inliers are kept within a single set.

The score function mentioned in Step6 upper bounds the maximum variance we check in Step3. For simplicity, in the actual algorithm (Algorithm1) we use the score directly for the termination check instead of checking the covariance, but it does not matter technically which quantity we use.

**Remark 3.3** (Runtime of Algorithm1).: We claim that each "loop" in Algorithm1 takes \((md^{2})\) time to compute. The number of times we run the "loop" is at most \(O(m)\), since each loop either ends in termination, removes 1 element from the dataset, or splits the dataset, all of which can happen at most \(O(m)\) times. From this, we can conclude a runtime of \((m^{2}d^{2})\). The sample complexity of our algorithm is also explicitly calculable to be \((d^{2}/^{6})\), which follows from Lemma2.2 and the choice of parameter \(=(^{3})\) from TheoremF.1 (the formal version of Theorem1.2).

To see the runtime of a single loop: the most expensive operations in each loop are to compute \(H_{t}\), its pseudo-inverse, and to compute the symmetric matrix \(A\) in Line7 that is the top eigenvector of a \(d^{2} d^{2}\) matrix. Computing \(H_{t}\) trivially takes \(O(md^{2})\) time, resulting in a \(d d\) matrix. Its pseudoinverse can be computed in \(O(d^{})\) time, which is dominated by \(O(md^{2})\) since \(m d\). Lastly, we observe that, instead of actually computing the top eigenvector in Line7 to yield the matrix \(A\), it suffices in our analysis to compute a matrix \(B\) whose Rayleigh quotient \((B^{})^{}(_{_{t}}[ ^{ 2}])B^{}/((B^{})^{}B^{})\) is at least \(\) times \((A^{})^{}(_{_{t}}[ ^{ 2}])A^{}/((A^{})^{}A^{})\). We can do this via \(O( d)\) manypower iterations. Since

\[}_{ T_{t}}[^{ 2}]=_{t}|}_{z_{t}}zz^{}-(_{t}|}_{z_{t}}z)^{}(_{t}|}_{z_{t}}z)\,\]

we can compute each matrix-vector product in \(O(|T_{t}|d^{2}) O(md^{2})\) time, thus yielding an \((m^{2}d^{2})\) runtime for the power iteration.

### Certificate Lemma: Bounded Fourth Moment Implies Closeness

The first component of the analysis is our certificate lemma, which states that, if the empirical covariance of the (flattened) second moment matrices of current data set (after normalization) is bounded, then the empirical second moment matrix \(H\) of the current data set is a good approximation to the covariance of the Gaussian component we want to estimate.

**Lemma 3.4** (Case when we stop and return).: _Make Assumption 3.1. Let \(w^{d^{2}}\) be the leading eigenvector of the \(}_{}[^{ 2}]\) with \(\|w\|_{2}=1\), and let \(A^{d d}\) be \(w^{}\). Note that \(\|w\|_{2}=1\) implies \(\|A\|_{}=1\). Then, we have \(\|H^{1/2} H^{/2}-I\|_{}^{2}(1/ )}_{}[^{ }A]+1/^{2}\)._

See Appendix E.2 for the proof. Our termination check of creftype 12 uses the score \(f()=(^{}A-_{})^{2}\) where \(_{}\) is the median of \(\{^{}A\ :\ \}\). Since \(}_{}[^{}A ]}_{}[f( )]\) our check ensures that \(}_{}[^{} A]}(1/)\) before returning.

### Filtering: Removing Extreme Outliers

As discussed in the algorithm outline, if the termination check fails, namely the expected score over the entire set of \(\) is large, then we proceed to either filter or bi-partition our samples. This subsection states the guarantees of the filtering procedure, which assumes that the \( m\)-smallest and largest elements in the set \(\{^{}A\ :\ \}\) have distance at most \(R\) for some \(R=(1/)\).

Recall from Lemma3.2 that \(_{ H^{1/2}D}[^{}A]=_ { H^{1/2}D}[(^{}A\ -_{ H^{1/2}D}[^{}A])^{2}]\) is bounded by \(O(1/^{2})\). By stability, the same is true for \(_{}[(^{}A -_{}[^{}A])^ {2}]\). Notice that this looks almost like our score function \(f(X)\), except that in \(f()\) we use \(_{}\) for centering instead of \(_{}[^{}A]\), since the latter quantity is by definition unknown to the algorithm. In LemmaE.3, we show that the two quantities have distance upper bounded by \(O(R)\), where \(R\) is the quantile distance in our assumption, which in turn implies that the inliers in \(\) contribute very little to \(_{}[f()]\). Given that \(_{}[f()]\) is large, by virtue of having failed the termination check, we can then conclude that most of the score contribution comes from the outliers. Thus, we can safely use the score to randomly pick an element in the dataset for removal, with probability proportional to its score, and the element will be overwhelmingly more likely to be an outlier rather than an inlier. Lemma3.5 below states the precise guarantees on the ratio of the total score of the inliers versus the total score over the entire dataset.

**Lemma 3.5** (Filtering).: _Make Assumption3.1. Let \(A\) be an arbitrary symmetric matrix with \(\|A\|_{}=1\). Let \(R=C(1/)(1/)\) for \(C 100}\). Define \(_{}=(\{^{}A \})\). Define the function \(f():=(^{}A-_{})^{2}\). Let \(m_{1}\) be a number less than \(|S|/3\). Denote by \(q_{i}\) the \(i\)-th smallest point of \(\{^{}A\}\). If \(q_{|T|-m_{1}}-q_{m_{1}} R\) and \(_{X T}[f(x)]>C^{}R^{2}/^{3}\) for \(C^{} 720/_{0}\), that is, in the case where the check in Line12 fails, then, the function \(f()\) satisfies \(_{}f()>}}_{}f()\)._

The score ratio determines (in expectation) the ratio between the number of outliers and inliers removed. In Lemma3.5, the ratio is in the order of \(1/^{3}\)--this will allow us to guarantee that at the end of the entire recursive execution of Algorithm1, we would have removed at most a \(0.01\) fraction of the inliers. See RemarkF.5 in AppendixF for more details.

### Divider: Identifying Multiple Clusters and Recursing

The previous subsections covered the cases where (i) the expected score is small, or (ii) the expected score over \(\) is large and the \(\) and \(1-\) quantiles of \(\{^{}A\ :\ \}\) are close to each other. What remains is the case when both the expected score is large yet the quantiles are far apart. In this instance, we will not be able to make progress via filtering using the above argument. This is actually an intuitively reasonable scenario, since the outliers may in fact have another \( m\) samples that are distributed as a different Gaussian with a very different covariance--the algorithm would not be able to tell which Gaussian is supposed to be the inliers. We will argue that, when both the expected score is large and the quantiles are far apart, the samples are in fact easy to bipartition into 2 clusters, such that the most of the inliers fall within 1 side of the bipartition. This allows us to make progress outside of filtering, and this clustering mechanism also allows us to handle Gaussian mixture models and make sure we (roughly) handle components separately.

The key intuition is that, by the stability Conditions (L.3) and (L.4), we know that the inliers under the 1-dimensional projection \(\{^{}A\ :\  \}\) must be well-concentrated, in fact lying in an interval of length \((1/)\). The fact that the quantile range is wide implies that there must be some point within the range that is close to very few samples \(^{}A\), by an averaging argument. We can then use the point as a threshold to bipartition our samples. See below for the precise statement.

```
1:functionFindDivider(\(T\),\(n^{}\),\(m_{1}\))
2: Let the \(m_{1}\)-th smallest point be \(q_{m_{1}}\) and \(m_{1}\)-th largest point be \(q_{|T|-m_{1}}\).
3: Divide the interval \([q_{m_{1}},q_{|T|-m_{1}}]\) into \(2m^{}/n^{}\) equally-sized subintervals.
4: Find a subinterval \(I^{}\) with at most \(n^{}/2\) points and return its midpoint. ```

**Algorithm 2** Divider for list decoding

**Lemma 3.6** (Divider Algorithm).: _Let \(T=(y_{1},,y_{m^{}})\) be a set of \(m^{}\) points in \(\). Let \(S_{} T\) be a set of \(n^{}\) points such that \(S_{}\) is supported on an interval \(I\) of length \(r\). For every \(i[m^{}]\), let the \(i\)-th smallest point of the set \(T\) be \(q_{i}\). Suppose \(q_{|T|-m_{1}}-q_{m_{1}} R\) such that \(R 10(m^{}/n^{})r\). Then, given \(T\) and \(n^{},m_{1}\), Algorithm 2 returns a point \(t\) such that if we define \(T_{1}=\{x T:x t\}\) and \(T_{2}=T T_{1}\) then: (i) \((|T_{1}|,|T_{2}|) m_{1}\) and (ii) \(S_{} T_{1}\) or \(S_{} T_{2}\)._

Proof.: The last step of the algorithm must succeed by an averaging argument. Consider the midpoint \(t\) of the returned subinterval \(I^{}\), which is at least \(q_{m_{1}}\) and at most \(q_{|T|-m_{1}}\). Since \(T_{1}\) contains all points at most \(q_{m_{1}}\), and \(T_{2}\) contains all points at most \(q_{|T|-m_{1}}\), we must have \((|T_{1}|,|T_{2}|) m_{1}\). Lastly, we verify the second desideratum, which holds if \(t I\). For the sake of contradiction, if \(t I\), then since \(I\) has length \(r\) and \(I^{}\) has length at least \(R/(2m/n^{}) 5r\), then \(I I^{}\). However, since \(|I^{} T| n^{}/2\), we know that \(I\) cannot be strictly contained in \(I^{}\), reaching the desired contradiction. 

## 4 High-Level Proof Sketch of Theorem 1.2

We discuss the proof strategy of Theorem 1.2 at a high level. See F for the complete proof.

**Proof Sketch** Recall that Algorithm 1 is a recursive algorithm: each call repeatedly filters out samples before either terminating or splitting the dataset into two and recursively calling itself. The execution of Algorithm 1 can thus be viewed as a binary tree, with each node being a recursive call.

The high-level idea for proving Theorem 1.2 is straightforward, though involving technical calculations to implement. Consider the subtree grown from the root recursive call, up to and including a certain level \(j\). We proceed by induction on the height \(j\) of such subtree, and claim there must exists a leaf node in this subtree such that most of the inliers remain in the input dataset of the leaf node.

Concretely, let \(_{j}\) be the subtree of height \(j\) grown from the root node. We claim that there must be a leaf in this subtree, whose input set \(T\) satisfies

\[^{3}(_{0}/40)|T|+|S T|(j+1)^{3}(_{0} /20)m+\,\] (1)

recalling that \(\) is the proportion of inliers, \(_{0}\) is the maximum fraction of inliers removed by the adversary, \(\) is the actual (unknown) number of inliers removed and \(m\) is the size of the original dataset returned by the adversary. The left hand side keeps track of both a) how many inliers we have accidentally removed, through the \(|S T|\) term, and b) the relative proportions of the outliers we have removed versus the inliers we have removed, by comparing both terms on the left hand side.

For the induction step, we need to analyze the execution of a single recursive call. We show that (1) implies Assumption 3.1, and so the inductive step can follow the case analysis outlined in Section 3--either we terminate, or we decide to either filter or bipartition the sample set. To convert this into a high-probability statement, we use a standard (sub-)martingale argument in F.1.