# Future-Dependent Value-Based

Off-Policy Evaluation in POMDPs

 Masatoshi Uehara

Genentech

uehara.masatoshi@gene.com

&Haruka Kiyohara

Cornell University

hk844@cornell.edu

&Andrew Bennett

Morgan Stanley

Andrew.Bennett@morganstanley.com

&Victor Chernozhukov

MIT

vchern@mit.edu

&Nan Jiang

UIUC

nanjiang@illinois.edu

&Nathan Kallus

Cornell University

kallus@cornell.edu

&Chengchun Shi

LSE

c.shi7@lse.ac.uk

&Wen Sun

Cornell University

ws455@cornell.edu

Co-first author. This work was done at Cornell UniversityCo-first author. This work was done at Tokyo Institute of TechnologyThis work was done at Cornell University

###### Abstract

We study off-policy evaluation (OPE) for partially observable MDPs (POMDPs) with general function approximation. Existing methods such as sequential importance sampling estimators suffer from the curse of horizon in POMDPs. To circumvent this problem, we develop a novel model-free OPE method by introducing future-dependent value functions that take future proxies as inputs and perform a similar role to that of classical value functions in fully-observable MDPs. We derive a new off-policy Bellman equation for future-dependent value functions as conditional moment equations that use history proxies as instrumental variables. We further propose a minimax learning method to learn future-dependent value functions using the new Bellman equation. We obtain the PAC result, which implies our OPE estimator is close to the true policy value under Bellman completeness, as long as futures and histories contain sufficient information about latent states. Our code is available at https://github.com/aiueola/neurips2023-future-dependent-ope.

## 1 Introduction

Reinforcement learning (RL) has demonstrated success when it is possible to interact with the environment and collect data in an adaptive manner. However, for domains such as healthcare, education, robotics, and social sciences, online learning can be risky and unethical [e.g., KHS\({}^{+}\)15, KL19, TDHL19, SRRH21]. To address this issue, a variety of offline RL methods have recently been developed for policy learning and evaluation using historical data in these domains [see LKTF20, for an overview]. In this paper, we focus on the off-policy evaluation (OPE) problem, which concerns estimating the value of a new policy (called the evaluation policy) using offline log data that was

[MISSING_PAGE_FAIL:2]

KMU21, SMNTT20] to derive consistent value estimators in tabular settings. Later,  and  extend their proposal by incorporating confounding bridge functions, thereby enabling general function approximation. (see the difference between these bridge functions and the proposed future-dependent value function in Section B). However, these methods do not apply to our unconfounded POMDP setting. This is because these methods require the behavior policy to _only_ depend on the latent state to ensure certain conditional independence assumptions. These assumptions are violated in our setting, where the behavior policy may depend on the observation - a common scenario in practical applications. In addition, we show that in the unconfounded setting, it is feasible to leverage multi-step future observations to relax specific rank conditions in their proposal, which is found to be difficult in the confounded setting . Refer to Example 1.

**Online learning in POMDPs.** In the literature, statistically efficient online learning algorithms with polynomial sample complexity have been proposed in tabular POMDPs , linear quadratic Gaussian setting (LQG) , latent POMDPs  and reactive POMDPs/PSRs . All the methods above require certain model assumptions. To provide a more unified framework, researchers have actively investigated online learning in POMDPs with general function approximation, as evidenced by a vast body of work . As the most relevant work, by leveraging future-dependent value functions,  propose an efficient PAC RL algorithm in the online setting. They require the existence of future-dependent value functions and a low-rank property of the Bellman loss. Instead, in our approach, while we similarly require the existence of future-dependent value functions, we do not require the low-rank property of Bellman loss. Instead, we use the invertibility condition in Theorem 1, i.e., we have informative history proxies as instrumental variables. As a result, the strategies to ensure efficient PAC RL in POMDPs are quite different in the offline and online settings.

**Learning dynamical systems via spectral learning.** There is a rich literature on POMDPs by representing them as predictive state representations (PSRs) . PSRs are models of dynamical systems that represent the state as a vector of predictions about future observable events. They are appealing because they can be defined directly from observable data without inferring hidden variables and are more expressive than Hidden Markov Models (HMMs) and POMDPs . Several spectral learning algorithms have been proposed for PSRs , utilizing conditional mean embeddings or Hilbert space embeddings. These approaches provide closed-form learning solutions, which simplifies computation compared to EM-type approaches prone to local optima and non-singularity issues. While these methods have demonstrated success in real-world applications  and have seen subsequent improvements , it is still unclear how to incorporate more flexible function approximations such as neural networks in a model-free end-to-end manner with a valid PAC guarantee. Our proposed approach not only offers a new model-free method for OPE, but it also incorporates these model-based spectral learning methods when specialized to dynamical system learning with linear models (see Section F).

**Planning in POMDPs.** There is a large amount of literature on planning in POMDPs. Even if the models are known, exact (or nearly optimal) planning in POMDPs is known to be NP-hard in the sense that it requires exponential time complexity with respect to the horizon . This computational challenge is often referred to as the _curse of history_. A natural idea to mitigate this issue is to restrict our attention to short-memory policies . Practically, a short-memory policy can achieve good empirical performance, as mentioned earlier. This motivates us to focus on evaluating short-memory policies in this paper.

## 2 Preliminaries

In this section, we introduce the model setup, describe the offline data and present some notations.

Model setup.We consider an infinite-horizon discounted POMDP \(=,,,r,,, \) where \(\) denotes the state space, \(\) denotes the action space, \(\) denotes the observation space, \(:()\) denotes the emission kernel (i.e., the conditional distribution of the observation given the state), \(:()\) denotes the state transition kernel (i.e., the conditional distribution of the next state given the current state-action pair), \(r:\) denotes the reward function, and \([0,1)\) is the discount factor. All the three functions \(,r,\) are unknown to the learner.

For simplicity, we consider the evaluation of memory-less policies \(:()\) that solely depend on the current observation \(O\) in the main text. The extension to policies with short-memory is discussed in Section C.

Next, given a memory-less evaluation policy \(^{e}\), we define the parameter of interest, i.e., the policy value. Following a policy \(^{e}\), the data generating process can be described as follows. First, \(S_{0}\) is generated according to some initial distribution \(_{}()\). Next, the agent observes \(O_{0}( S_{0})\), executes the initial action \(A_{0}^{e}( O_{0})\), receives a reward \(r(S_{0},A_{0})\), the environment transits to the next state \(S_{1}( S_{0},A_{0})\), and this process repeats. Our objective is to estimate

\[J(^{e}):=_{^{e}}[_{t=0}^{}^{t}R_{t}],\]

where the expectation \(_{^{e}}\) is taken by assuming the data trajectory follows the evaluation policy \(^{e}\).

Offline data.We convert the trajectory data generated by a behavior policy \(^{b}:()\), into a set of history-observation-action-reward-future transition tuples (denoted by \(_{}\)) and a set of initial observations (denoted by \(_{}\)). The first data subset enables us to learn the reward, emission and transition kernels whereas the second data subset allows us to learn the initial observation distribution.

Specifically, the dataset \(_{}\) consists of \(N\) data tuples \(\{(H^{(i)},O^{(i)},A^{(i)},R^{(i)},F^{(i)})\}_{i=1}^{N}\). We use \((H,O,A,R,F^{})\) to denote a generic history-observation-action-reward-future tuple where \(H\) denotes the \(M_{H}\)-step historical observations obtained prior to the observation \(O\) and \(F^{}\) denotes the \(M_{F}\)-step future observations after \((O,A)\) for some integers \(M_{H},M_{F} 1\). Specifically, at a given time step \(t\) in the data trajectory, we use \((O,A,R)\) to denote \((O_{t},A_{t},R_{t})\), and set

\[H=(O_{t-M_{H}:t-1},A_{t-M_{H}:t-1})F^{}=(O_{t+1:t+M_{F}},A_{t+1:t +M_{F}-1}).\]

We additionally set \(F=(O_{t:t+M_{F}-1},A_{t:t+M_{F}-2})\). Note we use the prime symbol'to represent the next time step. See Figure 1 for details when we set \(t=0\).

Throughout this paper, we use uppercase letters such as \((H,S,O,A,R,S^{},F^{})\) to denote _random variables_ in the offline data, and lowercase letters such as \((h,s,o,a,r,s^{},f^{})\) to denote their _realizations_, unless stated otherwise. To simplify the presentation, we assume the stationarity of the environment, i.e., the marginal distributions of \((H,S,F)\) and \((H^{},S^{},F^{})\) are identical.

The dataset \(_{}\) consists of \(N^{}\) data tuples \(\{O_{0:M_{F}-1}^{(i)},A_{0:M_{F}-2}^{(i)}\}_{i=1}^{N^{}}\) which is generated as follows: \(S_{0}_{}\), \(O_{0}( S_{0})\), \(A_{0}^{b}( O_{0})\), \(S_{1}( S_{0},A_{0}),\), until we observe \(O_{M_{F}-1}^{(i)}\) and \(A_{M_{F}-1}^{(i)}\). We denote its distribution over \(=^{M_{F}}^{M_{F}-1}\) by \(_{}()\).

**Remark 1** (Standard MDPs).: _Consider the setting where \(S=O\) and \(M_{H}=0,M_{F}=1\). In that case, we set \(H\) to \(S\) instead of histories. Then, \(_{}=\{O^{(i)},A^{(i)},R^{(i)},O^{(i)}\}\). We often assume \(_{}\) (\(_{}\) in our setting) is known. This yields the standard OPE setting in MDPs ._

Notation.We streamline the notation as follows. We define a state value function under \(^{e}\): \(V^{^{e}}(s)_{^{e}}[_{k=0}^{}^{k}R_{k}  S_{0}=s]\) for any \(s\). Let \(d_{t}^{^{e}}()\) be the marginal distribution of \(S_{t}\) under the policy \(^{e}\). Then, we define the discounted occupancy distribution \(d_{^{e}}():=(1-)_{t=0}^{}^{t}d_{t}^{^{e}}()\). We denote the domain of \(F,H\) by \(=()^{M_{F}-1},=()^{M_{H}}\), respectively. The notations \(,_{}\) (without any subscripts) represent the population or sample average over the offline data \(=_{}_{}\), respectively. We denote the distribution of offline data by \(P_{^{b}}()\). We define the marginal density ratio \(w_{^{e}}(S):=d_{^{e}}(S)/P_{^{b}}(S)\). Given a matrix \(C\), we denote its Moore-Penrose inverse by \(C^{+}\) and its smallest singular value by \(_{}(C)\). For a given integer \(m>0\), let \(I_{m}\) denote an \(m m\) identity matrix. Let \(\) denote outer product and \([T]=\{0,,T\}\) for any integer \(T>0\).

## 3 Identification via Future-dependent Value Functions

In this section, we present our proposal to identify policy values under partial observability by introducing future-dependent value functions. We remark that the target policy's value is identifiable from the observed data via SIS or SDR. Nonetheless, as commented earlier in Section 1, these methods suffer from the curse of horizon. Here, we propose an alternative identification approach that

Figure 1: Case with \(M_{H}=1,M_{F}=2\). An action \(A\) is generated depending on \(O\). The extension to memory-based policy is discussed in Section C.

can possibly circumvent the curse of horizon. It serves as a building block to motivate the proposed estimation methods in Section 4.

In fully observable MDPs, estimating a policy's value is straightforward using the value function-based method \(J(^{e})=_{s_{S}}[V^{^{e}}(s)]\). However, in partial observable environments where the latent state is inaccessible, the state value function \(V^{^{e}}(s)\) is unidentifiable. To address this challenge, we propose the use of _future-dependent value functions_ that are defined based on observed variables and serve a similar purpose to state value functions in MDPs.

**Definition 1** (Future-dependent value functions).: _Future-dependent value functions \(g_{V}[]\) are defined such that the following holds almost surely,_

\[[g_{V}(F) S]=V^{^{e}}(S).\]

_Recall that the expectation is taken with respect to the offline data generated by \(^{b}\)._

Crucially, the future-dependent value functions mentioned above may not always exist, and they need not be unique. Existence is a vital assumption in our framework, although we don't insist on uniqueness. We will return to the topics of existence and uniqueness after demonstrating their relevance in offline policy evaluation.

Hereafter, we explain the usefulness of future-dependent value functions. Future-dependent value functions are defined as embeddings of value functions on latent states onto multi-step futures. Notice that \(J(^{e})=_{s_{S}}[V^{^{e}}(s)]=_{f_{T}} [g_{V}(f)]\). These future-dependent value functions are useful in the context of OPE as they enable us to accurately estimate the final policy value. However, the future-dependent value function itself cannot be identified since its definition relies on unobserved states. To overcome this challenge, we introduce a learnable counterpart called the _learnable future-dependent value function_. This learnable version is defined based on observed quantities and thus can be identified.

**Definition 2** (Learnable future-dependent value functions).: _Define \((O,A):=^{e}(A O)/^{b}(A O)\). Learnable future-dependent value functions \(b_{V}[]\) are defined such that the following holds almost surely,_

\[0=[(O,A)\{R+ b_{V}(F^{})\}-b_{V}(F) H].\] (1)

_Recall that the expectation is taken with respect to the offline data generated by \(^{b}\). We denote the set of solutions by \(_{V}\)._

To motivate this definition, we recall that the off-policy Bellman equation in MDPs  can be expressed as follows:

\[V^{^{e}}(S)=[(O,A)(R+ V^{^{e}}(S^{})) S ].\] (2)

Then, by the definition of future-dependent value functions and certain conditional independence relations (\(F^{}(O,A) S^{}\)), we obtain that

\[0=[(O,A)\{R+ g_{V}(F^{})\}-g_{V}(F)  S].\] (3)

Since \(H(O,A,F^{}) S\), taking another conditional expectation given \(H\) on both sides yields that

\[0=[(O,A)\{R+ g_{V}(F^{})\}-g_{V}(F)  H].\] (4)

Therefore, (4) can be seen as an off-policy Bellman equation for future-dependent value functions, analogous to the Bellman equation (2) in MDPs. Based on the above discussion, we present the following lemma.

**Lemma 1**.: _Future-dependent value functions are learnable future-dependent value functions._

**Remark 2** (Non-stationary case).: _When the offline data is non-stationary, i.e, the pdfs of \((H,S)\) and \((H^{},S^{})\) are different, we need to additionally require \([g_{V}(F^{}) S^{}]=V^{^{e}}(S^{})\) in the definition._

**Remark 3** (Comparisons with related works).: _Similar concepts have been recently proposed in the context of confounded POMDPs [see e.g., SUHJ22, BK21]. However, our proposal significantly differs from theirs. First, their confounded setting does not cover our unconfounded setting because their behavior policies \(^{b}\) is not allowed to depend on current observations as mentioned in Section 1.1. Secondly, their proposal heir proposal overlooks the significant aspect of incorporating multi-step future observations, which plays a pivotal role in facilitating the existence of future-dependent value functions, as will be discussed in Example 1. For a detailed discussion, refer to Section B._Finally, we present a theorem to identify the policy value.

**Theorem 1** (Identification).: _Suppose (1a) the existence of learnable future-dependent value functions (need not be unique); (1b) the invertibility condition, i.e., any \(g:\) that satisfies \([g(S) H]=0\) must also satisfy \(g(S)=0\) ( i.e., \(g(s)=0\) for almost every \(s\) that belongs to the support of \(S\)), (1c) the overlap condition \(w_{^{e}}(S):=d_{^{e}}(S)/P_{^{b}}(S)<,(O,A)<\). Then, for any \(b_{V}_{V}\),_

\[J(^{e})=_{f_{}}[b_{V}(f)].\] (5)

We assume three key conditions: the observability condition (i.e., \(_{V}\)), the invertibility condition, and the overlap condition. We call Condition (1a) the observability condition since it is reduced to the well-known concept of observability in the LQG control theory, as we will see in Section D. While Condition (1a) itself is concerned with learnable future-dependent value functions, it is implied by the existence of _unlearnable_ future-dependent value functions according to Lemma 1 which can be verified using Picard's theorem in functional analysis . In general, the observability condition requires the future proxy \(F\) to contain sufficient information about \(S\) and is likely to hold when \(F\) consists of enough future observations. We will see more interpretable conditions in the tabular POMDPs (Example 1) and POMDPs with Hilbert space embeddings (HSE-POMDPs) where the underlying dynamical systems have linear conditional mean embeddings (Example 5 in Section D).

The invertibility condition is imposed to ensure that a learnable future-dependent value function \(b_{V}\) satisfies (3) (note that right hand side of Eq. 3 is a function of \(Z,S\) instead of \(H\)). Again, we will present more interpretable conditions in the tabular and linear settings below and in Section D. Roughly speaking, it requires \(H\) to retain sufficient information about \(S\). In that sense, the history proxy serves as an instrumental variable (IV), which is widely used in economics 5.

Finally, the overlap condition is a standard assumption in OPE .

**Example 1** (Tabular Setting).: _In the tabular case, abstract assumptions in Theorem 1 are reduced to certain rank assumptions. We first define \(_{b}=\{s:P_{^{b}}(s)>0\}\). We define a matrix \(_{^{b}}(_{b})^{|| ||}\) whose \((i,j)\)-th element is \(_{^{b}}(F=x_{i} S=x_{j}^{})\) where \(x_{i}\) is the \(i\)th element in \(\), and \(x_{j}^{}\) is the \(j\)th element in \(_{b}\). We similarly define another matrix \(_{^{b}}(_{b},)\) whose \((i,j)\)-th element is \(_{^{b}}(S=x_{i}^{},H=x_{j}^{{}^{}})\) where \(x_{j}^{{}^{}}\) denotes the \(j\)th element in \(\)._

**Lemma 2** (Sufficient conditions for observability and invertibility).: _(a) When \((_{^{b}}(_{b}))=|_{b}|\), future-dependent value functions exist. Then, from Lemma 1, learnable future-dependent value functions exist. (b) The invertibility is satisfied when \((_{^{b}}(_{b},))=|_{b}|\)._

_The first two conditions require that the cardinalities of \(\) and \(\) must be greater than or equal to \(_{b}\), respectively. The proof of Lemma 2 is straightforward, as integral equations reduce to matrix algebra in the tabular setting. We note that similar conditions have been assumed in the literature on HMMs and POMDPs . In particular, \(_{^{b}}(_{b})=|_{b}|\) has been imposed in previous works  in the context of confounded POMDPs. Nonetheless, our condition \(_{^{b}}(_{b})=|_{b}|\) is strictly weaker when \(\) includes multi-step future observations, demonstrating the advantage of incorporating multi-step future observations compared to utilizing only the current observation. A more detailed discussion can be found in Appendix D.1._

## 4 Estimation with General Function Approximation

In this section, we demonstrate how to estimate the value of a policy based on the results presented in Section 3. We begin by outlining the proposed approach for estimating \(b_{V}()\). The key observation is that it satisfies \([L(b_{V},)]=0\) for any \(:\) where \(L(q,)\) is defined as

\[L(q,):=\{(A,O)\{R+ q(F^{})\}-q(F)\}(H)\]

for \(q:\) and \(:\). Given some constrained function classes \([]\) and \([]\) and a hyperparameter \( 0\), the estimator is computed according to Line 1 of Algorithm 1. When the realizability \(_{V}\) holds and \(\) is unconstrained, we can easily show that the population-level minimizers \(*{argmin}_{q}_{}[L(q,)- 0.5^{2}(H)]\) are all learnable future-dependent value functions. This is later formalized in Theorem 2.

We can use any function classes such as neural networks, RKHS, and random forests to parameterize \(\) and \(\). Here, the function class \(\) plays a critical role in measuring how \(q\) deviates from the ground truth \(b_{V}\). The hyperparameter \(\) is introduced to obtain a fast convergence rate. We call it a stabilizer instead of a regularizer since \(\) does not need to shrink to zero as \(n\) approaches infinity. Note that regularizers are needed when we penalize the norms of \(q\) and \(\), i.e.,

\[_{V}=*{argmin}_{q}_{}_{}[L(q,)-0.5^{2}(H)]+0.5^{}\|q\|_{ }^{2}-0.5\|\|_{}^{2},\]

for certain function norms \(\|\|_{}\) and \(\|\|_{}\) and hyperparameters \(^{},>0\).

In the remainder of this section, we present three concrete examples using linear models, RKHSs and neural networks. Let \(\|\|_{}\) and \(\|\|_{}\) denote \(L^{2}\)-norms when we use linear models and RKHS norms when using RKHSs.

**Example 2** (Linear models).: _Suppose \(,\) are linear, i.e., \(=\{^{}_{}()^ {d_{}}\},=\{^{}_{}() ^{d_{}}\}\) with features \(_{}:^{d_{}},_{}:^{d_{}}\). Then,_

\[_{V}()=_{}^{}()\{_{2}^{ }\{ I_{d_{}}+_{3}\}^{-1}_{2}+ ^{}I_{d_{}}\}^{-1}_{2}^{}\{ I _{d_{}}+_{3}\}^{-1}_{1},\]

\[_{1}=_{}[(O,A)R_{}(H)], \,_{2}=_{}[_{}(H)\{_{ }^{}(F)-(O,A)_{}^{}(F^{})\}],_{3}=_{}[_{}(H)_{}^{}(H)].\]

_When \(^{}=0,=0,=0\), the value estimators boil down to_

\[_{V}()=_{}^{}()_{2}^{} _{1},_{}=_{f_{}}[ _{}^{}(f)]_{2}^{+}_{1}.\]

_The above estimators are closely related to the LSTD estimators in MDPs . Specifically, the off-policy LSTD estimator for state-value functions  is given by_

\[_{s_{}}[_{}^{}(s)]_{ }[_{}(S)\{_{}^{}(S)-(S,A )_{}^{}(S^{})\}]^{+}_{}[(S,A) R_{}(S)].\] (6)

_Our new proposed estimator \(_{}\) is_

\[_{f_{}}[_{}^{}(f)]_{ }[_{}(H)\{_{}^{}(F)-(O,A) _{}^{}(F^{})\}]^{+}_{}[(O,A) R_{}(H)],\]

_which is very similar to (6). The critical difference lies in that we use futures (including current observations) and histories as proxies to infer the latent state \(S\) under partial observability._

**Example 3** (RKHSs).: _Let \(,\) be RKHSs with kernels \(k_{}(,):,k_{ }(,):\). Then,_

\[_{V}()=_{}()^{}\{\{_{ }^{}\}^{}\{ I_{n}+_{}\}^{-1} _{}^{}+^{}I_{n}\}^{-1}\{_{}^{}\}^{}_{}^{1/2}\{ I_{n}+ _{}\}^{-1}_{}^{1/2}\]

_where \(^{n},k()^{n},_{} ^{n n},_{}^{n n}\) such that_

\[\{_{}\}_{(i,j)}=k_{}(H^{(i)},H^{(j)}),\,\{ _{}\}_{(i,j)}=k_{}(F^{(i)},F^{(j)}),\{ \}_{i}=(O^{(i)},A^{(i)})R^{(i)},\]

\[\{_{}^{}\}_{(i,j)}=k_{}(F^{(i)},F^{(j)})-  k_{}(F^{(i)},F^{(j)}),\{_{}() \}_{i}=k_{}(,F^{(i)}).\]

**Example 4** (Neural Networks).: _We set \(\) to a class of neural networks and recommend to set \(\) to a linear or RKHS class so that the minimax optimization is reduced to single-stage optimization. The resulting optimization problem can be solved by off-the-shelf stochastic gradient descent (SGD) methods. Specifically, when we set \(\) to a linear model, \(_{V}\) is reduced to_

\[_{V}=*{argmin}_{q}Z(q)^{}\{ I+ _{}[_{}(H)_{}^{}(H)] \}^{-1}Z(q),\]

\[Z(q):=_{}[(A,O)\{R+ q(F^{})\}-q(F)\}_{ }(H)].\]

_When we set \(\) to be an RKHS, \(_{V}\) is reduced to_

\[_{V}=*{argmin}_{q}\{Z^{}(q)\}^{} _{}^{1/2}\{ I_{n}+_{}\}^{-1} _{}^{1/2}Z(q),\]

\[\{Z^{}(q)\}_{i}:=(A^{(i)},O^{(i)})\{R^{(i)}+ q(F^{(i)})\}-q(F^{(i)}).\]

**Remark 4** (Comparison with minimax methods for MDPs).: _Our proposed methods are closely related to the minimax learning approach (a.k.a. Bellman residual minimization) for RL . More specifically, in MDPs where the evaluation policy is memory-less,  proposed minimax learning methods based on the following equations,_

\[[h(S)\{(S,A)\{ V^{^{}}(S^{})+R\}-V^{^ {}}(S)\}]=0,\ \  h:.\]

_These methods are no longer applicable in POMDPs since we are unable to directly observe the latent state. Although substituting the latent state with the entire history can still provide statistical guarantees, it is susceptible to the curse of horizon._

**Remark 5** (Modeling of system dynamics).: _Our minimax estimators can be extended to learning system dynamics. In particular, for tabular POMDPs, these results are closely related to the literature on spectral learning in HMMs and POMDPs . Refer to Section F,G._

**Remark 6** (Finite horizon setting).: _The extension to the finite horizon setting is discussed in Section E_

## 5 Finite Sample Results

We study the finite sample convergence rate of the proposed estimators in this section. To simplify the technical analysis, we impose three assumptions. First, we assume the function classes are bounded, i.e., \(\|\|_{} C_{},\|\|_{} C_{}\) for some \(C_{}\), \(C_{}>0\). Second, we assume that the offline data are i.i.d.6 Third, we assume \(,\) are finite hypothesis classes. Meanwhile, our results can be extended to infinite hypothesis classes using the global/local Rademacher complexity theory; see e.g., . To simplify the presentation, following standard convention in OPE, we suppose the initial distribution is known, i.e., \(|_{}|=\).

Accuracy of \(_{V}\).We first demonstrate that \(_{V}\) consistently estimates the learnable future-dependent value bridge functions. To formalize this, we introduce the following Bellman operators.

**Definition 3** (Bellman operators).: _The Bellman residual operator onto the history is defined as_

\[:[] q()[(O,A) \{R+ q(F^{})\}-q(F) H=],\]

_and the Bellman residual error onto the history is defined as \([(q)^{2}(H)]\). Similarly, the Bellman residual operator onto the latent state, \(^{S}\) is defined as_

\[^{S}:[] q()[(O, A)\{R+ q(F^{})\}-q(F) S=],\]

_and the Bellman residual error onto the latent state is defined as \([\{^{S}(q)\}^{2}(S)]\). The conditional expectations equal to zero when \(h\) and \(s\) lie outside the support of \(H\) and \(S\), respectively._

The Bellman residual error onto the history is zero for any learnable future-dependent value function, i.e., \([(b_{V})^{2}(H)]=0\). Thus, this is a suitable measure to assess how well value function-based estimators approximate the true learnable future-dependent value functions.

**Theorem 2** (Finite sample property of \(_{V}\)).: _Set \(>0\). Suppose (2a) \(_{V} 0\) (realizability) and (2b) \(\) (Bellman completeness). With probability \(1-\), we have \([(_{V})^{2}(H)]^{1/2} c\{1/+\} (1,C_{},C_{})||c/)}{n}}\) where \(c\) is some universal constant._

Note (2a) and (2b) are commonly assumed in the literature on MDPs  as well. In particular, Bellman completeness means that the function class \(\) is sufficiently rich to capture the Bellman update of functions in \(\). For instance, these assumptions are naturally met in HSEPOMDPs. We require \(>0\) in the statement of Theorem 2 to obtain a parametric convergence rate. When \(=0\), although we can obtain a rate of \(O_{p}(n^{-1/4})\), it is unclear whether we can achieve \(O_{p}(n^{-1/2})\).

Accuracy of \(_{}\).We derive the finite sample guarantee for \(_{}\).

**Theorem 3** (Finite sample property of \(_{}\)).: _Set \(>0\). Suppose (2a), (2b), (2c) any element in \(q\) that satisfies \([\{^{S}(q)\}(S) H]=0\) also satisfies \(^{S}(q)(S)=0\). (2d) the overlap \((O,A)<\) and any element in \(q\) that satisfies \(^{S}(q)(S)=0\) also satisfies \(^{S}(q)(S^{})=0\) where \(S^{} d_{^{}}(s)\). With probability \(1-\), we have_

\[|J(^{e})-_{}|}(1,C_{},C_{})_{1}()_{ }[d_{^{e}},P_{^{b}}]|||c/ )}{n}},\] (7)

_where_

\[_{1}^{2}() :=_{\{q;[\{^{S}(q)(S)\}^{ 2}] 0\}}[\{^{}(q)(S)\}^{2}]}{ [\{(q)(H)\}^{2}]},\] (8) \[_{}^{2}[d_{^{e}},P_{^{b}}] :=_{\{q;_{^{e}^{e}_{^{b}}}[\{^{S}(q)(s)\}^{2}] 0\}}_{s  d^{e}}[\{^{S}(q)(s)\}^{2}]}{_{s P_{^{b}}}[\{ ^{S}(q)(s)\}^{2}]}.\] (9)

On top of (2a) and (2b) that are assumed in Theorem2, when \(_{1}()<,_{}(d_{^{e}},P_{ ^{b}})<\), (2c) and (2d) hold, we have the non-vacuous PAC guarantee. The condition \(_{}(d_{^{e}},P_{^{b}})<\) and (2d) are the overlap conditions, which are adaptive to the function class \(\) and are weaker than (1c). These are also used in MDPs . Here, \(_{}(d_{^{e}},P_{^{b}})\) is a refined version of the density ratio and satisfies \(_{}(d_{^{e}},P_{^{b}}) w_{^{e}}(S)\). The condition \(_{1}()<\) and (2c) are characteristic conditions in POMDPs that quantify how much errors are properly translated from on \(H\) to on \(S\). Similar assumptions are frequently imposed in IV literature .

The upper error bound in (7) does not have explicit exponential dependence on the effective horizon \(1/(1-)\). In particular, as shown in SectionD, for tabular POMDPs and HSE-POMDPs, the terms \(_{}(d_{^{e}},P_{^{b}})\) and \(()\) can be reduced to certain condition numbers associated with covariance matrices spanned by feature vectors; see (15) and (16) in AppendixD. Hence, unlike SIS-based methods, we are able to break the curse of horizon.

The numbers of future and history proxies included in \(F\) and \(H\) represents a tradeoff. Specifically, if \(F\) contains enough observations, it is likely that (2a) will hold. Meanwhile, if \(H\) contains enough observations, it is more likely that (2b) will hold. These facts demonstrate the benefits of including a sufficient number of observations in \(F\) and \(H\). However, the statistical complexities \((||||)\) will increase with the number of observations in \(F\) and \(H\).

Lastly, it's worth noting that while our methods effectively address the curse of horizon, they may incur exponential growth concerning the number of future proxies used. This also applies to history proxies, which should be longer than the length of short memory policies. Here, we focus on the explanation of future proxies. For instance, in the tabular case, \(||\) might scale with \(||^{M_{F}}\) when considering \(\) as the set of all functions on \(^{M_{F}}\). However, this situation differs significantly from the curse of horizon, a challenge that naive methods like replacing states with the entire history encounter. These methods would necessitate the entire history to achieve Markovianity, whereas we only require a shorter length of future observations to establish the conditions outlined in Theorem1 (existence), which can be much shorter. Specific examples are provided throughout the paper, including Example1, which discusses the tabular setting and demonstrates that we essentially need as many future proxies as states, as long as there is sufficient statistical dependence between them.

## 6 Experiment

This section empirically evaluates the performance of the proposed method on a synthetic dataset.7.

We use the CartPole environment provided by OpenAI Gym , which is commonly employed in other OPE studies . By default, this non-tabular environment consists of 4-dimensional states, which are fully observable. Following , we create partial observability by adding independent Gaussian noises to each dimension of the state as \(O^{(j)}=S^{(j)}(1+(1+0.3^{2})),1 j 4\). To define behavior and evaluation policies, we first train an expert policy using DDQN  on latent states \(S\). Subsequently, we apply Behavior Cloning (BC) on two datasets, one containing pairs of latent state and action (\(S,A\)) and the other containing pairs of observation and action (\(O,A\)), respectively. Then, we use the base policy obtained by BC on the state-actionpairs \((S,A)\) to define an \(\)-greedy behavior policy, where we set \(=0.3\). 8 Similarly, the evaluation policy is also an \(\)-greedy policy, based on the base policy obtained by BC on the observation-action pairs \((O,A)\), with different values of \([0.1,0.3,0.5,0.7]\). We conduct the experiment with \(100\) random seeds, and for each simulation, we collect logged data consisting of \(1000\) trajectories, each containing \(100\) steps.

We compare our proposal with Sequential Importance Sampling (SIS)  and the naive minimax OPE , which is designed for fully-observable MDPs and does not account for partial observability. The naive minimax estimator is defined as if the environment was fully observable, replacing \(H\) and \(\) in Algorithm 1 with the current observation \(O\). In contrast, our proposed method uses a 3-step history as \(H\) and a one-step future as \(F\) to address partial observability. Both our proposed method and the naive approach use two-layer neural networks for the function \(\) and RKHSs for \(\), as detailed in Example 4.

We present the results in Figure 2, which demonstrate the superior accuracy of our proposed estimator compared to the baselines (SIS and naive minimax estimator) in terms of mean square errors (MSEs). Additional experimental details and ablation results, including the variations in the length of \(H\), \(\), and the choice of RKHSs, can be found in Appendix H.3.

## 7 Conclusion

We present a novel approach for OPE of short-memory policies in POMDPs. Our method involves introducing future-dependent value functions and the associated off-policy Bellman equations, followed by proposing a minimax estimator based on these equations. This is the first model-free method that allows for general function approximation and mitigates the curse of horizon. Our proposal is grounded in three interpretable key assumptions: observability, which asserts the presence of (short) future observations retaining adequate information about latent states, invertibility, which posits the existence of (short) histories preserving ample information about latent states; and the overlap between evaluation policies and behavior policies.

We have several avenues for enhancing our proposals. Firstly, automatically determining the appropriate lengths of futures and histories holds practical significance. Additionally, exploring recent attention mechanisms that extend beyond selecting the most recent history proxies or the nearest future proxies shows promise. Secondly, while we establish Bellman equations for POMDPs and use a simple minimax approach akin to , we may benefit from leveraging more refined methods introduced in recent research for solving conditional moment equations .