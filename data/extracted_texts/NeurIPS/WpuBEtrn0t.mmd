# Regularizing Neural Networks

with Meta-Learning Generative Models

 Shin'ya Yamaguchi\({}^{,}\) Daiki Chijiwa\({}^{}\) Sekitoshi Kanai\({}^{}\)

Atsutoshi Kumagai\({}^{}\) Hisashi Kashima\({}^{}\)

\({}^{}\)NTT \({}^{}\)Kyoto University

Corresponding author. Email: shinya.yamaguchi@ntt.com

###### Abstract

This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called _meta generative regularization_ (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR can avoid the performance degradation of naive generative data augmentation and boost the baselines. Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines.

## 1 Introduction

While deep neural networks achieved impressive performance on various machine learning tasks, training them still requires a large amount of labeled training data in supervised learning. The labeled datasets are expensive when a few experts can annotate the data, e.g., medical imaging. In such scenarios, _generative data augmentation_ is a promising option for improving the performance of models. Generative data augmentation basically adds pairs of synthetic samples from conditional generative models and their target labels into real training datasets. The expectations of generative data augmentation are that the synthetic samples interpolate missing data points and perform as oversampling for classes with less real training samples . This simple method can improve the performance of several tasks with less diversity of inputs such as medical imaging tasks .

However, in general cases that require the recognition of more diverse inputs (e.g., CIFAR datasets ), generative data augmentation degrades rather than improves the test accuracy . Previous studies have indicated that this can be caused by the low quality of synthetic samples in terms of the diversity and fidelity . If this hypothesis is correct, we can expect high-quality generative models (e.g., StyleGAN2-ADA ) to resolve the problem; existing generative data augmentation methods adopt earlier generative models e.g., ACGAN  and SNGAN . Contrary to the expectation, this is not the case. We observed that generative data augmentation fails to improve models even when using a high-quality StyleGAN2-ADA (Figure 1). Although the samples partially appear to be real to humans, they are not yet sufficient to train classifiers inexisting generative data augmentation methods. This paper investigates methodologies for effectively extracting useful information from generative models to improve model performance.

We address this problem based on the following hypotheses. First, _synthetic samples are actually informative but do not perfectly represent class categories in real data_. This is based on a finding by Brock et al.  called "class leakage," where a class conditional synthetic sample contains attributes of other classes. For example, they observed failure samples including an image of "tennis ball" containing attributes of "dogs" (Figure 4(d) of ). These class leaked samples do not perfectly represent the class categories in the real dataset. If we use such class leaked samples for updating classifiers, the samples can distort the decision boundaries, as shown in Figure 3. Second, _regardless of the quality, the generative models originally contain uninformative samples to solve the tasks_. This is simply because the generative models are not explicitly optimized to generate informative samples for learning the conditional distribution \(p(y|x)\); they are optimized only for learning the data distribution \(p(x)\). Further, the generative models often fail to capture the entire data distribution precisely due to their focus on the high-density regions. These characteristics of generative models might disturb the synthesis of effective samples for generative data augmentation. To maximize the gain from synthetic samples, we should select appropriate samples for training tasks.

In this paper, we present a novel regularization method called _meta generative regularization_ (MGR). Based on the above hypotheses, MGR is composed of two techniques for improving generative data augmentation: _pseudo consistency regularization_ (PCR) and _meta pseudo sampling_ (MPS). PCR is a regularization term using synthetic samples in training objectives for classifiers. Instead of supervised learning with negative log-likelihood \(- p(y|x)\), i.e., cross-entropy, on synthetic samples, we regularize the feature extractor to avoid the distortions on decision boundaries. That is, PCR leverages synthetic samples only for learning feature spaces. PCR penalizes the feature extractors by minimizing the gap between variations of a synthetic sample, which is inspired by consistency regularization in semi-supervised learning [15; 16; 17]. MPS corresponds to the second hypothesis and its objective is to select useful samples for training tasks by dynamically searching optimal latent vectors of the generative models. Therefore, we formalize MPS as a bilevel optimization framework of a classifier and a finder that is a neural network for searching latent vectors. Specifically, this framework updates the finder through meta-learning to reduce the validation loss and then updates the classifier to reduce the PCR loss (Figure 2). By combining PCR and MPS, we can improve the performance even when the existing generative data augmentation degrades the performance (Figure 1).

We conducted experiments with multiple vision datasets and observed that MGR can stably improve baselines on various settings by up to 7 percentage points of test accuracy. Further, through the visualization studies, we confirmed that MGR utilizes the information in synthetic samples to learn feature representations through PCR and obtain meaningful samples through MPS.

## 2 Preliminary

### Problem Setting

We consider a classification problem in which we train a neural network model \(f_{}:\) on a labeled dataset \(=\{(x^{i},y^{i})\}_{i=1}^{N}\), where \(\) and \(\) are the input and output label spaces,

Figure 1: Accuracy gain using meta generative regularization on Cars  with ResNet-18 classifier  and StyleGAN2-ADA  (FID: 9.5)

respectively. Here, we can use a generative model \(G_{}:\), which is trained on \(\). We assume that \(G_{}\) generates samples from a latent vector \(z\) and conditions on samples with a categorical label \(y\), where \(z\) is sampled from a standard Gaussian distribution \(p(z)=(0,I)\) and \(y\) is uniformly sampled from \(^{2}\). We refer to the classification task on \(\) as the main task, and \(f_{}\) as the main model. \(f_{}\) is defined by a composition of a feature extractor \(g_{}\) and a classifier \(h_{}\), i.e., \(f_{}=h_{} g_{}\) and \(=[,]\). To validate \(f_{}\), we can use a small validation dataset \(_{}=\{(x_{}^{i},y_{}^{i})\}_{i=1}^{N_{}}\), which has no intersection with \(\) (i.e., \(_{}=\)).

### Generative Data Augmentation

A typical generative data augmentation trains a main model \(f_{}\) with both real data and synthetic data from the generative models . We first generate synthetic samples to be utilized as additional training data for the main task. Most previous studies on generative data augmentation [19; 20; 8] adopt conditional generative models for \(G_{}\), and generate a pseudo dataset \(_{}\) as

\[_{}=\{(x_{}^{i},y_{}^{i}) x_{}^ {i}=G_{}(z^{i},y_{}^{i})\}_{i=1}^{N_{}},\] (1)

where \(z^{i}\) is sampled from a prior distribution \(p(z)\), and \(y_{}^{i}\) is uniformly sampled from \(\). Subsequently, \(f_{}\) is trained on both of \(\) and \(_{}\) using the following objective function.

\[_{} ()+_{p}(),\] (2) \[() = _{(x,y)}(f_{}(x),y),\] (3) \[_{}() = _{(x_{},y_{})_{} }_{}(f_{}(x_{}),y_{}),\] (4)

where \(\) is a loss function of the main task (e.g., cross-entropy), \(_{}\) is a loss function for the synthetic samples, and \(\) is a hyperparameter for balancing \(\) and \(_{}\). In previous works, \(_{}\) is often set the same as \(\). Although optimizing Eq. (2) with respect to \(\) is expected to boost the test performance by interpolating or oversampling conditional samples , its naive application degrades the performance of \(f_{}\) on general settings . In this paper, we explore methods to resolve the degradation of generative data augmentation and maximize the performance gain from \(_{}\).

## 3 Proposed Method

In this section, we describe our proposed method, MGR. The training using MGR is formalized as alternating optimization of a main model and finder network for searching latent vectors of \(G_{}\), as shown in Figure 2. To maximize the gain from synthetic samples, MGR regularizes a feature extractor \(g_{}\) of \(f_{}\) using PCR by effectively sampling useful samples for the generalization from \(G_{}\) using MPS. We show the overall algorithm of MGR in Appendix A.

Figure 4: Meta pseudo sampling framework. We meta-optimize the finder \(F_{}\) to generate a useful latent vector \(F_{}(z)\) for training a model parameter \(\) through minimizing the validation loss with the once updated parameter \(^{}\) by a real sample \(x\) and a synthetic sample \(x_{}=G_{}(F_{}(z))\).

Figure 3: Distortion of decision boundary caused by generative data augmentation with conditional synthetic samples leaking another class attribute. If the synthetic sample \(x_{}\) is “tennis ball dog” (reprinted from ) with its conditional label \(y_{}=\) “ball”, the supervised learner of a task head \(h_{}\) distorts the decision boundary of “dog” and “ball” to classify \(x_{}\) as “ball”.

### Pseudo Consistency Regularization

As discussed in Secion 1, we hypothesize that the synthetic samples do not perfectly represent class categories, and training classifiers using them can distort the decision boundaries. This is because \(y_{}\) is not reliable due to \(_{}\) can contain class leaked samples . To avoid the degradation caused by the distortion, we propose utilizing \(x_{}\) to regularize only the feature extractor \(g_{}\) of \(f_{}\) by discarding a conditional label \(y_{}\). For the regularization, we borrow the concept of consistency regularization, which was originally proposed for semi-supervised learning (SSL) [15; 16; 17]. These SSL methods were designed to minimize the dissimilarity between the two logits (i.e., the output of \(h_{}\)) of strongly and weakly transformed unlabeled samples to obtain robust representations. By following this concept, the PCR loss is formalized as

\[_{}(x_{};)=\|g_{}(T(x_{}))-g_{} (x_{})\|_{2}^{2},\] (5)

where \(T\) is a strong transformation such as RandAugment , which is similar to the one used in UDA  for SSL. The difference between PCR and UDA is that PCR penalizes only \(g_{}\), whereas UDA trains the entire \(f_{}=h_{} g_{}\). \(_{}\) can be expected to help \(g_{}\) learns features of inter-cluster interpolated by \(x_{}\) without distorting the decision boundaries. Using \(_{}\), we rewrite Eq. (4) as

\[_{}()=_{x_{}_{ }}_{}(x_{};).\] (6)

### Meta Pseudo Sampling

Most generative data augmentation methods generate a synthetic sample \(x_{}\) from \(G_{}\) with a randomly sampled latent vector \(z\). This is not the best option as \(G_{}\) is not optimized for generating useful samples to train \(f_{}\) in predicting the conditional distribution \(p(y|x)\); it is originally optimized to replicate \(p(x|y)\) or \(p(x)\). The main concept of MPS is to directly determine useful samples for training \(f_{}\) by optimizing an additional neural network called a _finder_\(F_{}:\). \(F_{}\) takes a latent vector \(z p(z)\) as input and outputs a vector of the same dimension as \(z\). By using \(F_{}\), we generate a synthetic sample as \(x_{}=G_{}(F_{}(z),y)\). The role of \(F_{}\) is to find the optimal latent vectors that improve the training of \(f_{}\) through \(x_{}\). Although we can consider optimizing \(G_{}\) instead of \(F_{}\), we optimize \(F_{}\) because the previous work showed that transforming latent variables according to loss functions efficiently reaches the desired outputs  and we observed that optimizing \(G_{}\) is unstable and causes the performance degradations of \(f_{}\) (Section 4.5).

The useful samples for generalization should reduce the validation loss of \(f_{}\) by using them for optimization. Based on this simple suggestion, we formalize the following bilevel optimization problem for \(F_{}\).

\[_{}_{}(^{*})=_{(x_{ },y_{})_{}}(f_{^{*} }(x_{}),y_{})\] \[ 28.452756pt^{*}=*{ argmin}_{}()+_{}(,).\] (7)

Note that the finder parameter \(\) is added to the arguments of \(_{}\). We can optimize \(F_{}\) with a standard gradient descent algorithm because \(F_{}\), \(G_{}\), and \(f_{}\) are all composed of differentiable functions as well as existing meta-learning methods such as MAML  and DARTS . We approximate \(^{*}\) because the exact computation of the gradient \(_{}_{}(^{*})\) is expensive [23; 24]:

\[^{*}^{}=-_{}(( )+_{}(,)),\] (8)

where \(\) is an inner step size. Thus, we update \(F_{}\) by using \(^{}\) that is updated for a single step and then alternately update \(\) by applying Eq. (6) with \(x_{}\) generated from the updated \(F_{}\). The overall optimization flow is shown in Figure 4.

Approximating gradients with respect to \(\).Computing \(_{}_{}(^{})\) requires the product computation including second-order gradients: \(_{,}^{2}_{}(,)_{^{ }}_{}(^{})\). This causes a computation complexity of \(((||+||)||)\). To avoid this computation, we approximate the term using the finite difference method  as

\[_{,}^{2}_{}(,)_{^{ }}_{}(^{}) _{}(^{+},)-_{}_{}( ^{-},)}{2},\] (9)

where \(^{}\) is \(\) updated by \(_{}_{}()\). \(\) is defined by \(}{\|_{}_{}()\|_{2}}\). We used \(0.01\) of the constant for \(\) based on . This approximation reduces the computation complexity to \((||+||+||)\). We confirm the speedup by Eq. (9) in Appendix B.1.

Techniques for improving finderWe introduce two techniques for improving the training of \(F_{}\) in terms of the architectures and a penalty term for the outputs.

While arbitrary neural architectures can be used for the implementation of \(F_{}\), we observed that the following residual architecture produces better results.

\[F_{}(z):=z+(_{}(z)),\] (10)

where MLP is multi layer perceptron.

To ensure that \(F_{}(z)\) does not diverge too far from the distribution \(p(z)\), we add a Kullback-Leibler divergence term \(D_{}(p_{}(z)\|p(z))\), where \(p_{}(z)\) is the distribution of \(F_{}(z)\) into Eq. (7). When \(p(z)\) follows the standard Gaussian \((0,I)\), \(D_{}(p_{}(z)\|p(z))\) can be computed by

\[D_{}(p_{}(z)\|p(z))=-(1+_{}-_{}^{ 2}-_{}),\] (11)

where \(_{}\) and \(_{}\) is the mean and variance of \(\{F_{}(z^{i})\}_{i=1}^{N_{}}\). In Appendix B.2, we discuss the effects of design choice based on the ablation study.

## 4 Experiments

In this section, we evaluate our MGR (the combination of PCR and MPS with the experiments on multiple image classification datasets. We mainly aim to answer three research questions with the experiments: (1) Can PCR avoid the negative effects of \(x_{}\) in existing methods and improve the performance of \(f_{}\)? (2) Can MPS find better samples for training than those by uniform sampling? (3) How practical is the performance of MGR? We compare MGR with baselines including conventional generative data augmentation and its variants in terms of test performance (Sec. 4.2 and 4.3). Furthermore, we conduct a comprehensive analysis of PCR and MPS such as the visualization of trained feature spaces (Sec. 4.4), quantitative/qualitative evaluations of the synthetic samples (Sec. 4.5), performance studies when changing generative models (Sec. 4.6), and comparison to data augmentation methods such as TrivialAugment  (Sec. 4.7).

### Settings

Baselines.We compare our method with the following baselines. **Base Model**: training \(f_{}\) with only \(\). **Generative Data Augmentation (GDA)**: training \(f_{}\) with \(\) and \(G_{}\) using Eq. (2). **GDA+MH**: training \(f_{}\) with \(\) and \(G_{}\) by decoupling the heads into \(h_{}\) for \(\) and \(h_{_{}}\) for \(_{}\). MH denotes multi-head. This is a naive approach to avoid the negative effect of \(x_{}\) on \(h_{}\) by not passing \(x_{}\) through \(h_{}\). GDA+MH optimizes the parameters as \(*{argmin}(f_{}(x),y)+(h_{ _{}}(g_{}(x_{})),y_{})\).

**GDA+SSL**: training \(f_{}\) with \(\) and \(G_{}\) by applying an SSL loss for \(_{}\) that utilizes the output of \(h_{}\) unlike PCR. This method was originally proposed by Yamaguchi et al.  for transfer learning, but we note that \(G_{}\) was trained on the main task dataset \(\) in contrast to the original paper. By following , we used UDA  as the SSL loss and the same strong transformation \(T\) as of MGR for the consistency regularization.

Datasets.We used six image datasets for classification tasks in various domains: Cars , Aircraft , Birds , DTD , Flowers , and Pets . Furthermore, to evaluate smaller dataset cases, we used subsets of Cars that were reduced by \(\{10,25,50,75\}\%\) in volume; we reduced them by random sampling on a fixed random seed. We randomly split a dataset into \(9:1\) and used the former as \(\) and the latter as \(_{}\).

Architectures.We used ResNet-18  as \(f_{}\) and generators of conditional StyleGAN2-ADA for \(256 256\) images  as \(G_{}\). \(F_{}\) was composed of a three-layer perceptron with a leaky-ReLU activation function. We used the ImageNet pre-trained weights of ResNet-18 distributed by PyTorch.3 For StyleGAN2-ADA, we did not use pre-trained weights. We trained \(G_{}\) on each \(\) from scratch according to the default setting of the implementation of StyleGAN2-ADA.4 Note that we used the same \(G_{}\) in the baselines and our proposed method.

### Evaluation on Multiple Datasets

We confirm the efficacy of MGR across multiple datasets. Table 0(a) shows the top-1 accuracy scores of each method. As reported in , GDA degraded the base model on many datasets; it slightly improved the base model on only one dataset. GDA+MH, which had decoupled classifier heads for GDA, exhibited a similar trend to GDA. This indicates that simply decoupling the classifier heads is not a solution to the performance degradation caused by synthetic samples. In contrast, our MGR stably and significantly outperformed the baselines and achieved the best results. The ablation of MGR discarding PCR or MPS is listed in Table 0(a). We confirm that both PCR and GDA+MPS improve GDA. While GDA+SSL underperforms the base models, PCR outperforms the base models. This indicates that using unsupervised loss alone is not sufficient to eliminate the negative effects of the synthetic samples and that discarding the classifier \(h_{}\) from the regularization is important to obtain the positive effect. MPS yields only a small performance gain when combined with GDA, but it significantly improves its performance when combined with PCR i.e., MGR. This suggests that there is no room for performance improvements in GDA, and MPS can maximize the potential benefits of PCR.

### Evaluation on Small Datasets

A small dataset setting is one of the main motivations for utilizing generative data augmentation. We evaluate the effectiveness of MGR on smaller datasets. Table 0(b) shows the performance when reducing the Cars dataset into a volume of \(\{10,25,50,75\}\%\). Note that we trained \(G_{}\) on each reduced dataset, not on \(100\%\) of Cars. In contrast to the cases of the full dataset (Table 0(a)), no baseline methods outperformed the base model in this setting. This is because \(G_{}\) trained on the small datasets generates low-quality samples with less reliability on the conditional label \(y_{}\) that are not appropriate in supervised learning. On the other hand, MGR improved the baselines in large margins. This indicates that, even when the synthetic samples are not sufficient to represent the class categories, our MGR can maximize the information obtained from the samples by utilizing them to regularize feature extractors and dynamically finding useful samples.

Table 1: Top-1 accuracy (%) of ResNet18. Underlined scores outperform that of Base Model, and **Bolded scores** are the best among the methods.

### Visualization of Feature Spaces

In this section, we discuss the effects of PCR and MPS through the visualizations of feature spaces in training. To visualize the output of \(g_{}\) in 2D maps, we utilized UMAP  to reduce the dimensions. UMAP is a visualization method based on the structure of distances between samples, and the low dimensional embeddings can preserve the distance between samples of the high dimensional input. Thus, the distance among the feature clusters on UMAP visualization of \(g_{}\) can represent the difficulty of the separation by \(h_{}\). We used the official implementation by 5 and its default hyperparameters. We plotted the UMAP embeddings of \(g_{}(x)\) and \(g_{}(x_{})\) at \(\{5,15,30\}\) epochs, as shown in Figure 5; we used ResNet-18 trained on the Cars dataset. At first glance, we observe that GDA and PCR formed completely different feature spaces. GDA forms the feature spaces by forcing to treat the synthetic samples the same as the real samples through cross-entropy loss and trying to separate the clusters of samples according to the class labels. However, the synthetic samples leaked to the inter-cluster region at every epoch because they could not represent class categories perfectly as discussed in Sec. 1. This means that the feature extractor might be distorted to produce features that confuse the classifier. On the other hand, the synthetic samples in PCR progressively formed a cluster at the center, and the outer clusters can be seen well separated. Since UMAP can preserve the distances between clusters, we can say that the sparse clusters that exist far from the center are considered easy to classify, while the dense clusters close to the center are considered difficult to classify. In this perspective, PCR helps \(g_{}\) to leverage the synthetic samples for learning feature representations interpolating the dense difficult clusters. This is because the synthetic samples tend to be in the middle of clusters due to their less representativeness of class categories. That is, PCR can utilize the partial but useful information contained in the synthetic samples while avoiding the negative effect. Further, we observe that applying MPS accelerates the convergence of the synthetic samples into the center.

For a more straightforward visualization of class-wise features, we designed a simple binary classification task that separates Pets  into dogs and cats, and visualized the feature space of a model trained on this task. Fig. 6 shows the feature space after one epoch of training. While GDA failed to separate the clusters for each class, MGR clearly separated the clusters. Looking more closely, MGR helps samples to be dense for each class. This is because PCR makes the feature extractor learn slight differences between the synthetic samples that interpolate the real samples. From these observations, we conclude that our MGR can help models learn useful feature representations for solving tasks.

Figure 5: UMAP visualization of feature spaces on training. The plots in the figures represent real and synthetic samples in the feature spaces. Our methods (PCR and PCR+MPS) can help the feature extractors separate real sample clusters. In contrast, the existing method (GDA) confuses the feature extractor by leaking synthetic samples out of the clusters.

### Analysis of MPS

Evaluation of validation loss.We investigate the effects on validation losses when using MPS. Through the meta-optimization by Eq. (7), MPS can generate samples that reduce the validation loss of \(f_{}\). We recorded the validation loss per epoch when applying uniform sampling (Uniform) and when applying MPS. We used the models trained on Cars and applied the PCR loss on both models. Figure 6(a) plots the averaged validation losses. MPS reduced the validation loss. In particular, MPS was more effective in early training epochs. This is related to accelerations of converging the central cluster of synthetic samples discussed in Section 4.4 and Figure 5. That is, MPS can produce effective samples for regularizing features and thus speed up the entire training of \(f_{}\).

Quantitative evaluation of synthetic samples.We evaluate the synthetic samples generated by MPS. To assess the characteristics of the samples, we measured the difference between the data distribution and distribution of the synthetic samples. We leveraged the Frechet Inception distance (FID, ), which is a measurement of the distribution gap between two datasets using the closed-form computation assuming multivariate normal distributions:

\[(,_{})=\|-_{}\|_{2}^{2}+ (+_{}-2 }}),\]

where \(\) and \(\) are the mean and covariance of the feature vectors on InceptionNet for input \(\{x^{i}\}\). Since FID is a distance, the lower \((,_{})\) means that \(_{}\) contains more high-quality samples in terms of realness. We computed FID scores using 2048 samples in \(\) and 2048 synthetic samples every epoch; the other settings are given in Section 4.1. The FID scores in training are plotted in Figure 6(b). We confirm that MPS consistently produced higher-quality samples than Uniform. This indicates that the sample quality is important for generalizing \(f_{}\) even in PCR, and uniform sampling can miss higher quality samples in generative models. Since the performance gain by GDA+MPS in Table 0(a) did not better than MGR, the higher-quality samples by MPS can still contain uninformative samples for the cross-entropy loss, but they are helpful for PCR learning good feature representations.

Qualitative evaluation of synthetic samples.We evaluate the qualitative properties of the synthetic samples by visualizing samples of a class. The samples generated by Uniform and MPS are shown in Figure 7(a) and 7(b), where the dataset was Cars and the class category was Hummer. Compared withUniform, MPS produced samples with more diverse body colors and backgrounds. That is, MPS focuses on the color and background of the car as visual features for solving classifications. In fact, since Hummer has various colors of car bodies and can drive on any road with four-wheel drive, these selective generations of MPS are considered reasonable in solving classification tasks.

### Effect of Generative Models

Here, we evaluate MGR by varying the generative model \(G_{}\) for producing synthetic samples. As discussed in Sec. 3.1 and 3.2, MGR can use arbitrary unconditional/conditional generative models as \(G_{}\) unless it has a latent space. To confirm the effects when changing \(G_{}\), we tested MGR with FastGAN , BigGAN , and StyleGAN-XL . Table 2 shows the results on Cars. The unconditional FastGAN achieved similar improvements as conditional cases. However, since unconditional generative models are generally of low quality in FID, they are slightly less effective. For the conditional generative models, we observe that MGR performance improvement increases as the quality of synthetic samples improves. These results suggest the potential for even greater MGR gains in the future as the performance of the generative model improves. We also evaluate MGR with recent diffusion models in Appendix B.3. Meanwhile, to evaluate the value of synthetic samples in the consistency regularization, we compare MGR with the case where we use real samples in Eq. (6) (Real CR). Our method outperformed Real CR. This can be because interpolation by the generators helps the feature extractor to capture the difference between images. Furthermore, our method searches the optimal synthetic samples by using latent vectors of generative models with MPS. In contrast, Real CR cannot search the optimal real samples for CR loss because real data is fixed in the data space.

### Combination of MGR and Data Augmentation

To assess the practicality of MGR, we evaluate the comparison and combination of MGR and existing data augmentation methods. Data augmentation (DA) is a method applied to real data and is an independent research field from generative data augmentation. Therefore, MGR can be combined with DA to improve performance further. Table 3 shows the evaluation results when comparing and combining MGR and DA methods; we used MixUp , CutMix , AugMix , RandAugment , SnapMix , and TrivialAugment  as the DA methods. The improvement effect of MGR is comparable to that of DA, and the highest accuracy was achieved by combining the two methods. In particular, MGR was stably superior to sample-mix-based such as MixUp and AugMix. This result also indicates that synthetic samples, which non-linearly interpolate the real samples, elicit better performance than linearly interpolating real samples by the sample-mix-based DA methods. We consider this strength to be an advantage of using generative models.

## 5 Related Work

We briefly review generative data augmentation and training techniques using generative models.

The earliest works of generative data augmentation are [19; 41; 42]. They have demonstrated that simply adding synthetic samples as augmented data for classification tasks can improve performance in few-shot learning, person re-identification, and medical imaging tasks. Tran et al. have

   \(G_{}\) & Conditional & FID & Top-1 Acc. (\(\%\)) \\  None (Base Model) & – & – & 85.50\(\)10 \\ Real CR & – & – & 86.16\(\)20 \\  FastGAN  & No & 23.1 & 86.30\(\)16 \\ BigGAN  & Yes & 15.6 & 86.86\(\)66 \\ StyleGAN2-ADA  & Yes & 9.5 & 87.22\(\)15 \\ StyleGAN-XL  & Yes & 6.2 & 88.37\(\)20 \\   

Table 2: Performance of MGR varying \(G_{}\) (ResNet-18, Cars).

   Data Augmentation & Base Model & +MGR \\  None & 85.50\(\)10 & 87.22\(\)15 \\ MixUp  & 86.87\(\)30 & 87.60\(\)46 \\ CutMix  & 86.13\(\)19 & 87.80\(\)51 \\ AugMix  & 86.25\(\)11 & 87.65\(\)03 \\ RandAugment  & 87.47\(\)05 & 88.67\(\)10 \\ SnapMix  & 87.11\(\)20 & 88.21\(\)13 \\ TrivialAugment  & 87.83\(\)16 & **89.10\(\)19** \\   

Table 3: Performance comparison between MGR and data augmentation methods (ResNet-18, Cars, Top-1 Acc. (\(\%\))).

proposed a generative data augmentation method that simultaneously trains GANs and classifiers for optimizing \(\) to maximize the posterior \(p(|x)\) by an EM algorithm. Although this concept is similar to our MPS in terms of updating both \(G_{}\) and \(f_{}\), it requires training the specialized neural architectures based on GANs. In contrast, MPS is formalized for arbitrary existing generative models with latent variables, and it requires no restrictions to the training objectives of generative models.

On the analysis of generative data augmentation, Shmelkov et al.  have pointed out that leveraging synthetic samples as augmented data degrades the performance in general visual classification tasks. They have hypothesized that the cause of the degradation is the less diversity and fidelity of synthetic samples from generative models. Subsequent research by Yamaguchi et al.  have shown that the scores related to the diversity and fidelity of synthetic samples (i.e., SSIM and FID) are correlated to the test accuracies when applying the samples for generative data augmentation in classification tasks. Based on these works, our work reconsiders the training objective and sampling method in generative data augmentation and proposes PCR and MPS.

More recently, He et al.  have reported that a text-to-image generative model pre-trained on massive external datasets can achieve high performance on few-shot learning tasks. They also found that the benefit of synthetic data decreases as the amount of real training data increases, which they attributed to a domain gap between real and synthetic samples. In contrast, our method does not depend on any external dataset and successfully improves the accuracy of the classifier even when synthetic data are not representatives of class labels (i.e., having domain gaps).

## 6 Limitation

One of the limitations of our method is the requirement of bilevel optimization of classifier and finder networks. This optimization is computationally expensive particularly when used with generative models that require multiple inference steps, such as diffusion models as discussed in Appendix B.3. We have tried other objective functions not requiring bilevel optimization, but at this time, we have not found an optimization method that outperforms MPS (see Appendix B.4). Nevertheless, since recent studies rapidly and intensively focus on the speedup of diffusion models , we can expect that this limitation will be negligible in near the future. Additionally, applying MGR to pre-trained text-to-image diffusion models (e.g., ) is also important for future work because they have succeeded in producing effective samples for training downstream task models in a zero-shot manner .

## 7 Conclusion

This paper presents a novel method for generative data augmentation called MGR. MGR is composed of two techniques: PCR and MPS. To avoid the degradation of classifiers, PCR utilizes synthetic samples to regularize feature extractors by the simple consistency regularization loss. MPS searches the useful synthetic samples to train the classifier through meta-learning on the validation loss of main tasks. We empirically showed that MGR significantly improves the baselines and brings generative data augmentation up to a practical level. We also observed that the synthetic samples in existing generative data augmentation can distort the decision boundaries on feature spaces, and the PCR loss with synthetic samples dynamically generated by MPS can resolve this issue through visualization. We consider that these findings will help future research in this area.