# TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning

Nemin Wu\({}^{1,3}\)*, Qian Cao\({}^{1,3}\)*, Zhangyu Wang\({}^{2,3}\), Zeping Liu\({}^{3}\), Yanlin Qi\({}^{3,4}\), Jielu Zhang\({}^{1,3}\), Joshua Ni\({}^{3,5}\), Xiaobai Yao\({}^{1}\), Hongxu Ma\({}^{6}\), Lan Mu\({}^{1}\), Stefano Ermon\({}^{7}\), Tanuja Ganu\({}^{8}\), Akshay Nambi\({}^{8}\), Ni Lao\({}^{6,3}\)\({}^{}\), Gengchen Mai\({}^{3,1}\)\({}^{}\)

\({}^{1}\)University of Georgia, \({}^{2}\)UC Santa Barbara, \({}^{3}\)SEAI Lab, University of Texas at Austin,

\({}^{4}\)UC Davis, \({}^{5}\)Basis Independent Fremont, \({}^{6}\)Google LLC,

\({}^{7}\)Stanford University, \({}^{8}\)Microsoft Research,

{nemin.wu, qian.ca01, jielu.zhang, xyao, mulan}@uga.edu,

zhangyuwang@ucsb.edu, zeping.liu@utexas.edu, ylqi@ucdavis.edu, nijoshua2025@gmail.com, {hxma, nlao}@google.com, ermon@cs.stanford.edu,

{tanuja.ganu, akshay.nambi}@microsoft.com, gengchen.mai@austin.utexas.edu

*Equal contribution. Author ordering is determined by coin flip. \({}^{}\)Corresponding author.

###### Abstract

Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose **TorchSpatial**, a learning framework and benchmark for location (point) encoding, which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified **location encoding framework** that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the **LocBench** benchmark tasks encompassing 7 geo-aware image classification and 10 geo-aware image regression datasets; 3) a comprehensive suite of **evaluation metrics** to quantify geo-aware models' overall performance as well as their geographic bias, with a novel **Geo-Bias Score** metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation learning and spatial fairness in GeoAI research. The TorchSpatial model framework and LocBench benchmark are available at https://github.com/seai-lab/TorchSpatial, and the Geo-Bias Score evaluation framework is available at https://github.com/seai-lab/PyGBS.

## 1 Introduction

Spatial representation learning (SRL) aims at learning neural spatial representations from spatial data (e.g., points, polylines, polygons, spatial networks, images, etc.) in their native formats while avoiding manual feature engineering  or data conversion (e.g., point clouds to voxels , polygons to raster images , or map vector files into raster image tiles ). Learning good spatial representations is a fundamental problem and the key to achieving end-to-endtraining for various GeoAI applications. However, several barriers are hindering the advancement of SRL research.

Firstly, there is no community-shared framework and benchmarks for SRL model development. A community-shared framework on a specific area can significantly accelerate research in that area. Examples are Torchvision for computer vision tasks, TorchAudio for audio and signal processing tasks, PyTorch Geometric  for graph neural network research, etc. While TorchGeo  has been developed for geospatial data processing and model development, it mainly focuses on processing geospatial image/raster data while much fewer efforts have been devoted to other geospatial data modalities (e.g., points, polylines, polygons, etc.) which are critical for GeoAI research. This significantly hinders GeoAI model development, as each research project must begin anew for much of the development (e.g., spatial data acquisition, processing, baseline reproduction, model development, and evaluation) without access to a standardized framework.

Secondly, location encoding of geolocation data , one of the key components of SRL, has been proved useful for various geospatial tasks such as fine-grained species recognition , satellite image classification , weather forecasting , and so on. However, no benchmark has been developed to systematically evaluate the location encoders' impact on model performance in tasks with diverse task setups, dataset sizes, and geographic coverage.

Figure 1: The overall framework of **TorchSpatial**. TorchSpatial provides a unified location encoding framework that consolidates 15 widely used location encoders and LocBench benchmark which contains 7 geo-aware image classification and 10 geo-aware image regression datasets. In addition, we provide a universally applicable geographic bias evaluation framework called Geo-Bias Score.

Last but not least, although many pioneering works demonstrated the effectiveness of geo-aware models [42; 46; 5; 47; 54; 51] in downstream tasks, there has not been work that systematically defines and evaluates the geographic bias of these geo-aware AI approaches (e.g., tile embeddings, location encoders, etc). The question of whether the additional geo-aware module mitigates or aggravates the geographic bias  has not been investigated. Generally speaking, spatial fairness and geographic bias research investigate whether learned AI models can perform equally well across geographic space. While these concepts have been proposed for a while, most efforts focus on the qualitative analysis of these biases of AI models such as large language models [23; 56], and almost no effort has been made to develop a universally applicable measure for such bias.

In this work, to fill these gaps, we present TorchSpatial, a deep learning framework and benchmark for spatial representation learning. Figure 1 illustrates the major components of TorchSpatial. The key contributions of TorchSpatial are threefold:

1. We provide a **TorchSpatial** model framework that supports location encoder development. Currently, TorchSpatial consolidates 15 widely used location encoders and necessary model building blocks for future location encoder development while ensuring scalability and reproducibility of the implementations.
2. We provide a **LocBench** benchmark which contains 7 geo-aware image classification and 10 image regression datasets. They are used to systematically evaluate the performance of any location encoder in datasets across varied task setups, geographic distributions, dataset sizes, etc.
3. We provide a comprehensive set of evaluation metrics to quantify location encoders' overall model performance as well as their geographic bias, with a novel **Geo-Bias Score**. To the best of our knowledge, this is the first universally applicable geographic bias evaluation framework designed to assess any AI models such as large language models [56; 55].

## 2 Related Work

**Spatial representation learning (SRL).** Spatial representation learning  aims at learning neural spatial representation of spatial data in their native format. According to the targeted spatial data types, SRL can be classified into location encoders [42; 46; 53; 51; 35; 18; 67; 13], polyline encoders [4; 29; 64; 86; 68; 65], polygon encoders [75; 33; 50; 80], polygon decoders [12; 1; 38; 87], etc. By automatically extracting a learning-friendly representation from different types of spatial data, SRL enables end-to-end training on top of spatial data. As one of the key components of SRL, location encoders aim at encoding a location into a learning-friendly representation that can be used in many downstream tasks such as fine-grained species recognition [42; 46; 53] and distribution modeling , population mapping , satellite image classification [51; 35], geographic question answering , etc. In this work, our TorchSpatial focuses on location encoder development and evaluation.

**Geo-Aware machine learning benchmarks.** There are many emerging benchmarks in machine learning that incorporate geographical information, particularly geographic coordinates. In earth observation, EarthNets and GEO-Bench integrate abundant Remote Sensing (RS) datasets for multiple domain-specific tasks, such as land cover classification, cloud segmentation, cattle counting, and RS change detection. In the ecology domain, the iNaturalist 2021 competition  provides a fine-grained species recognition dataset that includes images, their location metadata, and location uncertainty. Meanwhile, they also add location annotations to the previously released iNaturalist 2017 and iNaturalist 2018 datasets to motivate researchers leverage the spatial information effectively. Similarly, GeoLifeCLEF competition series [22; 11; 17; 40; 41; 10] provide a list of benchmark datasets for location-based species classification. In terms of image regression tasks, SustainBench consists of 15 tasks across 7 Sustainable Development Goals. It provides datasets covering most countries in the Global South and certain Global North countries. MOSAIKS is the largest benchmark dataset for RS image regression tasks to our knowledge, containing around 500,000 observations uniformly distributed around the globe, and it also proposes a CNN-based model for benchmarking. Despite the availability of many geo-aware machine learning benchmarks, many benchmarks such as EarthNets, GEO-Bench, SustainBench , and MOSAIKS do not report performances of geo-aware models but only focusing on purely computer vision models even the location metadata is provided. Moreover, for benchmarks that emphasize the role of location information such as iNaturalist and GeoLifeCLEF, they follow a rather similar task setup. Our LocBench aims to systematically evaluate the impact of location encoders on model's overall performance and geographic bias across tasks with very different geographic coverage, dataset sizes, and task setups.

## 3 TorchSpatial Framework and Benchmark

### TorchSpatial Model Framework

TorchSpatial is designed following the following framework proposed by :

\[Enc()=(PE()),\] (1)

Here \(PE()\) is a position encoder transforming location \(\) into a \(W\)-dimension vector, namely position embedding. \(():^{W}^{d}\) is a learnable neural network module that maps the input position embedding \(PE()^{W}\) into the location embedding \(Enc()^{d}\). By following the common practice [71; 26; 78; 16; 42; 88; 46; 64], TorchSpatial framework is flexible enough to support the development of any location encoders. In the future, we plan also to support other spatial data types such as polylines, polygons, spatial networks, etc.

Within this framework, we implement 15 commonly recognized location encoders, and classify them into two groups: 1) 2D location encoders which work on a projected 2D space [7; 2; 46; 63; 42; 46] and 2) 3D location encoders which interpret geolocation as 3D coordinates [47; 60; 67]. Please refer to Appendix A.1 for a detailed description of these models.

For model inference, as depicted in Figure 1, there are two model inference structures tailored to the classification and regression tasks. Each structure is designed to leverage image and location data while aligning with the unique objectives of classification and regression.

For classification tasks, the objective is to predict discrete categories based on each input image and location, and the logits of the model are the possibility for each class. Inspired by , where location information is treated as a Bayesian spatio-temporal prior. In this setup, TorchSpatial separately processes the image and location data using two distinct classifiers: an image classifier and a location classifier. The final prediction is derived by performing an element-wise multiplication of the outputs from these two classifiers, as indicated by the brown circle with a cross in Figure 1. The influence of various location encoders can be assessed on classification accuracy while maintaining consistency in image representations. For regression tasks, the goal is to predict continuous numerical values. A more straightforward structure is adopted. Feature embeddings are extracted from both the image and location data using separate encoders. These embeddings are then concatenated to create a comprehensive representation, which is subsequently input into an MLP to predict continuous values. This process is illustrated by the dark blue circle labeled "C" for concatenation in Figure 1. The algorithm 1 presents the pseudocode for the model inference architecture described above.

```
1:procedureModelInference(\(D_{type}\): dataset type; \(Enc^{(I)}\): image encoder; \(Enc^{(x)}\): location encoder; \((,)\): an image and location tuple; \(\): MLP; \(\): element-wise multiplication; [;]: concatenation; softmax(): softmax function; \(\): predicted variable)
2:if\(D_{type}\) == "classification" then
3:\(_{I}^{(I)}()\)
4:\(_{x}^{(x)}()\)
5:\((_{I}_{x})\)
6:elseif\(D_{type}\) == "regression" then
7:\(_{I}^{(I)}()\)
8:\(_{x}^{(x)}()\)
9:\(_{I,x}[_{I};_{x}]\)
10:\((_{I,x})\)
11:endif
12:return\(\)
13:endprocedure ```

**Algorithm 1** Pseudocode for Model Inference Architecture in TorchSpatial

### LocBench

In order to systematically compare the location encoders' performance and their impact on the model's overall geographic bias, we clean and preprocess 7 geo-aware image classification datasets and 10 geo-aware image regression datasets.

Geo-Aware Image Classification.The geo-aware image classification task aims at classifying a given image (e.g., species images, ground-level images, satellite images, etc.) into its correct category based on the image itself as well as its associated location metadata. Figure 1 illustrates how location encoders from TorchSpatial can be used to solve this task. Please refer to Appendix A.2 for a description of the model setup of the geo-aware image classification task. Based on our investigation, LocBench incorporates 7 geo-aware image classification datasets:

1. **BirdSnap:** An image dataset about bird species based on BirdSnap dataset  with location annotations by . It consists of 19576 images of 500 bird species that are commonly found in North America. This dataset and the other two following are widely used by multiple studies to demonstrate location encoder's capacity to significantly increase the fine-grained species classification accuracy.
2. **BirdSnap\(\):** An enriched BirdSnap dataset constructed by  by simulating locations, dates, and photographers from the eBrid dataset , containing 43470 images of 500 categories.
3. **NABirds\(\):** An another image dataset about North American bird species based on the NABirds dataset , the location metadata were also simulated from the eBrid dataset . It contains 23699 images of 555 bird species categories.
4. **INat2017:** The worldwide species recognition dataset used in the iNaturalist 2017 challenges  with 675170 images and 5089 unique categories. We add the location information retroactively provided by iNaturalist 2021. Although its spatial distribution focuses on North America and Europe, it still covers the entire globe, which makes it one of the most spatially extensive and species-rich image dataset known to us.
5. **iNet2018:** The worldwide species recognition dataset used in the iNaturalist 2018 challenges  with 461939 images and 8142 unique categories. Although the original competition didn't provide coordinates, we add them to our benchmark as additional information from the same data source of iNaturalist 2021. It has a similar spatial distribution with iNat2017, covering all continents. We choose these two datasets to evaluate location encoder's capacity to improve fine-grained species classification performance at the global level.
6. **YFCC:**YFCC100M-GEO100 dataset, an image dataset derived from Yahoo Flickr Creative Commons 100M dataset  and was annotated by , containing 88986 images over 100 everyday object categories with location annotations. Here, we denote this dataset as YFCC. YFCC is a comprehensive public dataset with images across the United States. Despite the relatively limited geographic coverage, we employ this dataset to measure location encoder's capacity for multifaceted image classification in addition to domain-specific image classification.
7. **fMoW:** Functional Map of the World dataset (denoted as fMoW)  is an RS image classification dataset, containing RS images with diverse land use types collected all over the world. It is composed of about 363K training and 53K validation remote sensing images which are classified into 62 different land use types. We use the fMoWrgb version of the fMoW dataset which are JPEG compressed version of these RS images with only the RGB bands.

Geo-Aware Image Regression.The geo-aware image classification task has a similar task setup as the classification task. The difference is the image target label is a continuous value that represents population density, forest coverage percentage, nightlights luminosity, and other indices at the given location. Figure 1 demonstrates how location encoders and image encoders can be used to solve these tasks. Please refer to Appendix A.3 for a description of the model setup. Based on our investigation, we select and preprocess 10 geo-aware image regression datasets based on MOSAIKS and SustainBench benchmarks :

1. **MOSAIKS population density:** This dataset uses daytime remote sensing images as covariables to predict population density at the corresponding locations. The observations were geographically sampled with the uniformly-at-random (UAR) strategy on the earth's surface. The MOSAIKS originally contains 100K population density records with coordinates, but less than half of them can be matched to remote sensing images on the dataset. We apply a log transformation of the labels and add 1 beforehand to avoid dropping zero-valued labels. After data cleaning, we get 425,637 observations uniformly distributed across the world.
2. **MOSAIKS forest cover:** According to , forest in this dataset is defined as vegetation greater than 5 meters in height, and measurements of forest cover are given at a raw resolution of roughly 30m by 30m. The estimation of forest cover rate was achieved by analysis of multiple spectral bands of remote sensing imagery, other than RGB bands used in this dataset. After similar data cleaning and preprocessing step, we get 498,106 observations at the global level.

3. **MOSAIKS nightlight luminosity:** Like forest cover rate, nightlight luminosity is also derived from satellite imagery, but not the RGB bands that most computer vision models work on, nor daytime remote sensing images we use as inputs in our benchmark. Specifically, luminosity in this dataset refers to the average radiance at night in 2015, provided by the Visible Infrared Imaging Radiometer Suite (VIIRS). Following the same data preprocess step, we offer 492,226 observations of nightlight luminosity with corresponding satellite images.
4. **MOSAIKS elevation:** Similarly, Satellite RGB bands are used to predict the elevation at the corresponding location. Following the same data preprocess step, we offer 498,115 elevation observations. To align with the settings of MOSAIKS, we did not apply a log transformation on elevation labels. The underlying data behind this dataset mainly comes from the Shuttle Radar Topography Mission (SRTM) at NASA's Jet Propulsion Laboratory (JPL), in addition to other open data projects.
5. **SustainBench Asset Index/Women BMI/Water Index/Child Mortality Rate/Sanitation Index/Women Education:** SustainBench is a set of benchmarks that aim to regress indices for the UN Sustainable Development Goals (SDGs) based on satellite images and street-level images. The remote sensing images are collected from diverse sources, including the Landsat 5/7/8, DMSP, and VIIRS satellites. The street-level images come from the platform Mapillary. The labels are originally derived from household-level survey data from the Demographic and Health Surveys (DHS) program and are aggregated as community-level. We did not use the original country-based splits and reset a train/test dataset split with an 80:20 ratio as the target of LocBench is to compare the effectiveness of location encoders across the globe.

For all those 10 datasets, we set a train/test dataset split with an 80:20 ratio and provide an option to resample the training dataset at any user-defined proportion for the convenience of users. Interestingly, locations have not been used as additional features for geo-aware image regression in the original MOSAIKS and SustainBench papers [66; 84]. Here, we will use these datasets to investigate the performance of various location encoders provided by our TorchSpatial model framework.

### Evaluation Metrics

#### 3.3.1 Overall Model Performance Evaluation Metrics

We first evaluate the overall performance of various location encoders on different LocBench datasets to align with existing benchmarks, such as iNaturalist and MOSAIKS. For geo-aware classification datasets, we use Top-1 accuracy, Top-3 accuracy, and Mean Reciprocal Rank (MRR) to measure the comprehensive performance of location encoders. For regression datasets, we utilize R\({}^{2}\), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) to evaluate their performance.

#### 3.3.2 Geographic Bias Evaluation Metrics

While there is no universally accepted definition of geographic bias, in this work, we interpret it as a subclass of model bias which refers to _a phenomenon in which an AI model performs differently across geographic regions and its predictions are biased toward some predominated regions_[39; 55]. The cause of geographic bias can be population differences, sampling bias, economic development differences, etc. Generally speaking, given two AI models with equal overall performances, we would prefer the model with lower geographic bias, which means _the possibility of encountering a wrong prediction is more uniform across the region of interest_. While there is increasing interest in studying geographic bias, most existing research focus on qualitative analysis of AI models' geographic bias [23; 56] or developing ad-hoc geographic bias measures for specific tasks  or models, e.g., large language models . In this work, we propose a systematic and universally applicable geographic bias evaluation framework based on spatial autocorrelation, called **Geo-Bias Scores1**.

Note that classic spatial autocorrelation (SA) measures  such as Moran's I  and Geary's C  cannot be used for geographic bias quantification, because these statistics are not numerically comparable across different spatial patterns, i.e., those generated from different models. Please refer to Appendix A.4.1 for a more detailed explanation. Therefore, we develope our bias metrics based on a newly proposed statistical measure called **spatial self-information (SSI)**, which is an information-theory-based generalization of the classic Moran's I statistics and ensures numerical comparability across different spatial patterns, thus being suitable for geographic bias quantification. Please refer to Appendix A.4.2 for a detailed explanation of SSI.

Based on SSI we propose two novel metrics to quantify the geographic bias of model performance, which we call **unmarked SSI geo-bias score** and **marked SSI geo-bias score**, respectively. We assume that we evaluated our model on a test set with \(M\) observations, and for each observation \(i\) we get a performance measure \(x_{i}\) (e.g., binary classification error, real-valued regression error, etc.) and criteria of high/low performance (e.g., wrong classification, larger than 3-sigma deviation). After applying the criteria to \(x_{i}\), we get a set of binary values (e.g., -1 for low and 1 for high) of model performance. Together with the locations of these observations, we obtain a spatial sample on which we can compute the SSI. As many geospatial datasets are both large and distributed across the globe, it is important to consider the multi-scale effect of spatial patterns - some models may perform uniformly well on the continental scale, but are heavily biased towards mega-cities within continents, and vice versa. Thus, we further design our geo-bias evaluation metrics to be aware of spatial scales. We extract a neighborhood (e.g., within a 100km radius) for each low-performance observation \(i\), draw a spatial grid as the background, construct the weight matrix by the spatial connectivity within this neighborhood (e.g., the nearest 4 locations are considered adjacent), and compute the SSI \(J_{i}\) for this neighborhood.

There are two sources of SSI for a given neighborhood: (1) the spatial distribution of observations regardless of their model performances (referred to as _unmarked_, where we only consider the geographic locations of the data points without any additional attributes), and (2) the spatial distribution of the model performances (referred to as _marked_, where each data point is associated with an additional attribute, such as prediction accuracy or error). Intuitively, if a neighborhood itself is regularly arranged, no matter how random the low-performance observations scatter over the neighborhood, its SSI will still be high. Therefore, we call the SSI of source (1) the **unmarked SSI geo-bias score**. It measures the base strength of spatial patterns of the neighborhood we evaluate our model against. Starting from the unmarked SSI geo-bias, the spatial distribution of low-performance observations may further increase or decrease the SSI of the neighborhood. We define **marked SSI geo-bias score** as the difference between the SSI of the low-performance observations generated by our models and the SSI of completely random low-performance observations. This score measures the relative strength of spatial patterns of the low-performance observations. Please refer to Appendix A.4.3 for a detailed implementation procedure of geo-bias scores. In our experiments, we compute the unmarked SSI geo-bias scores and the marked SSI geo-bias scores for all neighborhoods that contain low-performance observations and report the average. Figure 2 illustrates the ideas of these two Geo-Bias scores.

It is intuitive to interpret the two scores. The unmarked SSI geo-bias score measures how likely low-performance observations occur in regions of strong spatial patterns. The marked SSI geo-bias score measures how likely low-performance observations themselves form strong spatial patterns. For both scores, _the larger the values, the stronger the spatial autocorrelation, indicating a higher geographic bias in the associated model_.

Figure 2: Intuition of the two geo-bias scores. **Left:** When we encounter a low-performance observation (red dot), we extract its neighborhood by radius \(r\). **Middle:** Dots represent the observed locations and crosses are background grid points. Dots within the neighborhood demonstrate spatial patterns against the unobserved background. The SSI of such patterns is called the **unmarked SSI geo-bias score**. It reflects the intrinsic sampling geo-bias. **Right**: Green and red dots represent locations where the model achieves high performance and low performance respectively. The SSI of such patterns is called the **marked SSI geo-bias score**. It reflects the geo-bias of model performance, dependent both on where the data are observed and how the model performs at these locations.

Experiments

In this section, we systematically evaluate the 15 location encoders developed in our TorchSpatial model framework on 7 geo-aware image classification and \(10\) regression datasets in LocBench. Both the overall model performance and Geo-Bias scores are reported.

### Geo-Aware Image Classification

To test the effectiveness of 15 location encoders, we conduct experiments on 7 geo-aware image classification datasets including 5 fine-grained species recognition datasets, 1 Flickr image classification dataset, and 1 remote sensing image classification dataset as we described in Section 3.2. Besides 15 geo-aware classification models equipped with those 15 location encoders described in Section 3.1, we also consider _No Prior_, which represents a fully supervised trained image classifier without using any location information. In addition, we tested GPT-4V on the same datasets but in a zero-shot setting to see how it performs compared to fine-tuned geo-aware models. The Top-1 accuracy of all 17 models across 7 datasets is listed in Table 1, and the geo-bias scores are shown in Table 2.

Discussion.According to Table 1, we can see that adding a location encoder can lead to significant model performance boosting. \(Sphere\)_2Vec-\(sphereC+\)_ is the winner on 4 datasets except iNat2017, iNat2018, and YFCC in which \(wrap+ffn\), _Space2Vec-\(theory\)_, and _Sphere2Vec-\(sphereC\)_ are the winner respectively. Compared with other geo-aware models, GPT-4V demonstrates much worse performance. One probable reason is that fine-grained species recognition datasets usually contain hundreds or thousands of species classes which makes it hard for GPT-4V to handle. And RS images in fMoW are very different from natural images used to pre-train GPT-4V which leads to its poor performance. Further analysis of GPT-4V's performance on these geo-aware image classification tasks is needed. By comparing Table 1 and 2, we can see that except \(tile\), all the other location encoders can significantly increase the model's geographic bias despite the overall model performance boosting. \(tile\) has relatively less impact on both overall model performance and geographic bias.

    & Task &  & Flickr & RS \\   & Image Classification Dataset & BirdSnap & BirdSnap\(\) & NABirds\(\) & iNat2017 & iNat2018 & Avg & YFCC & fMOW \\   & \(P(y|)\) - Prior Type & Test & Test & Test & Val & Val & – & Test & Val \\  A & No Prior (i.e. image model) & 70.07 & 70.07 & 76.08 & 63.27 & 60.20 & 67.94 & 50.15 & 69.83 \\   & \(tile\) & 70.20 & 70.56 & 75.78 & 62.54 & 56.30 & 67.08 & 50.01 & 69.86 \\ \(wrap\) & **72.06** & 79.35 & **81.78** & 68.16 & 73.11 & 74.89 & 51.03 & 70.34 \\  & \(wrap+ffn\) & 71.93 & 79.05 & 81.40 & **69.52** & 72.29 & 74.84 & 50.71 & 70.11 \\
**B** & \(rbf\) & 71.79 & 79.58 & 81.74 & 68.24 & 70.03 & 74.28 & 51.22 & **70.68** \\ \(rf\) & 71.84 & 78.91 & 81.61 & 68.86 & 72.32 & 74.71 & 50.81 & 70.24 \\ _Space2Vec-\(grid\)_ & 71.75 & **80.24** & 81.70 & 68.23 & 73.06 & 75.00 & **51.25** & 70.67 \\  & _Space2Vec-\(theory\)_ & 71.79 & 80.11 & 81.65 & 68.30 & **73.52** & **75.07** & 51.24 & 70.49 \\   & \(xyz\) & 71.88 & 78.96 & 81.15 & 68.65 & 71.44 & 74.42 & 50.87 & 70.16 \\  & \(NeRF\) & 72.10 & 79.93 & 81.62 & 68.74 & 72.91 & 75.06 & 51.27 & 70.60 \\  & \(Sphere2Vec-sphereC\) & 72.10 & 79.97 & 81.91 & 69.34 & 72.93 & 75.25 & **51.35** & 70.85 \\  & \(Sphere2Vec-sphereC+\) & **72.15** & **80.90** & **82.13** & 68.29 & **73.45** & **75.38** & 51.31 & **70.93** \\  & \(Sphere2Vec-sphereM\) & 71.88 & 79.93 & 81.86 & 68.51 & 72.94 & 75.02 & 51.18 & **70.93** \\  & \(Sphere2Vec-sphereM+\) & 72.06 & 79.09 & 81.67 & 69.18 & 72.06 & 74.81 & 51.27 & 70.19 \\  & \(Sphere2Vec-dfs\) & 71.79 & 78.69 & 81.44 & **69.42** & 72.16 & 74.70 & 50.65 & 70.27 \\  & _Sirem (SH)_ & 71.88 & 78.96 & 81.72 & 67.68 & 71.33 & 74.29 & 50.57 & 70.20 \\  D & GPT-4V & 55.02 & 48.89 & 73.00* & 28.00* & 18.00* & 44.00* & 37.00* & 17.00* \\   

Table 1: The Top1 classification accuracy of different models on 7 geo-aware image classification datasets in LocBench benchmark. See Appendix A.1 for the description of each model. We classify them into four groups: (A) No Prior indicates image-only models; (B) geo-aware models with 2D location encoders; (C) geo-aware models with 3D location encoders; (D) GPT-4V. Since the test sets for iNat2017, iNat2018, and fMoW are not open-sourced, we report results on their validation sets. The original result reported by  for No Prior on fMOW is 69.05. We obtain 69.83 by retraining their implementation. GPT-4V is tested with zero-shot settings, and * indicates that we resample 100 images from each dataset’s test/validation set except BirdSnap and Birdsnap\(\) whose whose test sets are used for evaluation. ”Avg” column indicates the average performance of each model on all five species recognition datasets. **Bold** indicates the best models in Group B and C. See Section A.5 for hyperparameter tuning details.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]