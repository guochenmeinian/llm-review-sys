# Statistical-Computational Trade-offs for Density Estimation

Anders Aamand

University of Copenhagen

aamand@mit.edu

&Alexandr Andoni

Columbia University

andoni@cs.columbia.edu

&Justin Y. Chen

MIT

justc@mit.edu

Piotr Indyk

MIT

indyk@mit.edu

&Shyam Narayanan

Citadel Securities

shyam.s.narayanan@gmail.com

&Sandeep Silwal

UW-Madison

silwal@cs.wisc.edu

Haike Xu

MIT

haikexu@mit.edu

Work done as a student at MIT

###### Abstract

We study the density estimation problem defined as follows: given \(k\) distributions \(p_{1},,p_{k}\) over a discrete domain \([n]\), as well as a collection of samples chosen from a "query" distribution \(q\) over \([n]\), output \(p_{i}\) that is "close" to \(q\). Recently  gave the first and only known result that achieves sublinear bounds in _both_ the sampling complexity and the query time while preserving polynomial data structure space. However, their improvement over linear samples and time is only by subpolynomial factors.

Our main result is a lower bound showing that, for a broad class of data structures, their bounds cannot be significantly improved. In particular, if an algorithm uses \(O(n/^{c}k)\) samples for some constant \(c>0\) and polynomial space, then the query time of the data structure must be at least \(k^{1-O(1)/ k}\), i.e., close to linear in the number of distributions \(k\). This is a novel _statistical-computational_ trade-off for density estimation, demonstrating that any data structure must use close to a linear number of samples or take close to linear query time. The lower bound holds even in the realizable case where \(q=p_{i}\) for some \(i\), and when the distributions are flat (specifically, all distributions are uniform over half of the domain \([n]\)). We also give a simple data structure for our lower bound instance with asymptotically matching upper bounds. Experiments show that the data structure is quite efficient in practice.

## 1 Introduction

The general density estimation problem is defined as follows: given \(k\) distributions \(p_{1},,p_{k}\) over a domain \([n]\)2, build a data structure which when queried with a collection of samples chosen from a "query" distribution \(q\) over \([n]\), outputs \(p_{i}\) that is "close" to \(q\). An ideal data structure reports the desired \(p_{i}\) quickly given the samples (i.e., has fast query time), uses few samples from \(q\) (i.e., has low sampling complexity) and uses little space.

In the realizable case, we know that \(q\) is equal to one of the distributions \(p_{j}\), \(1 j k\), and the goal is to identify a (potentially different) \(p_{i}\) such that \(\|q-p_{i}\|_{1}\) for an error parameter \(>0\). In the more general agnostic case, \(q\) is arbitrary and the goal is to report \(p_{i}\) such that

\[\|q-p_{i}\|_{1} C_{j}\|q-p_{j}\|_{1}+\]

for some constant \(C>1\) and error parameter \(>0\). The problem is essentially that of non-parametric learning of a distribution \(q\), where the family \(=\{p_{1}, p_{k}\}\) has no structure whatsoever. Its statistical complexity was understood already in , and the more modern focus has been on the structured case (when \(\) has additional properties). Surprisingly, its computational complexity is still not fully understood.

Due to its generality, density estimation is a fundamental problem with myriad applications in statistics and learning distributions. For example, the framework provides essentially the best possible sampling bounds for mixtures of Gaussians . The framework has also been studied in private  and low-space  settings.

Table 1 summarizes known results as well as our work. As seen in the table, the data structures are subject to statistical-computational trade-offs. On one hand, if the query time is not a concern, logarithmic in \(k\) samples are sufficient . On the other hand, if the sampling complexity is not an issue, then one can use the algorithm of  to learn the distribution \(\) such that \(\|-q\|_{1}/2\), and then deploy standard approximate nearest neighbor search algorithms with sublinear query time, e.g., from . Unfortunately both of these extremes require either linear (in \(n\)) sample complexity, or linear (in \(k\)) query time time. Thus, achieving the best performance with respect to one metric resulted in the worst possible performance on the other metric.

The first and only known result that obtained non-trivial improvements to _both_ the sampling complexity and the query time is due to a recent work of . Their improvements, however, were quite subtle: the sample complexity was improved by a sub-logarithmic factor, while the query time was improved by a sub-polynomial factor of \(k^{1/( k)^{1/4}}=2^{(k)^{3/4}}\).

This result raises the question of whether further improvements are possible. In particular,  asks: _To what extent can our upper bounds of query and sample complexity be improved? What are the computational-statistical tradeoffs between the sample complexity and query time?_ These are the questions that we address in this paper.

* **Lower bound:** We give the **first** limit to the best possible tradeoff between the query time and sampling complexity, demonstrating a novel statistical-computational tradeoff for a fundamental problem. To our knowledge, this is the first statistical-computational trade-off for a _data structures_ problem-if we allow for superpolynomial space, logarithmic sampling and query complexity

  Samples & Query time & Space & Comment & Reference \\  \( k\) & \(k^{2} k\) & \(kn\) & &  \\ \( k\) & \(k k\) & \(kn\) & &  \\ \( k\) & \(k\) & \(kn\) & &  \\ \( k\) & \( k\) & \(n^{O( k/^{2})}\) & & \\ \(n\) & \(nk^{}\) & \(kn+k^{1+}\) & & \\ \(}\) & \(n+k^{1-}}\) & \(k^{2}n\) & & + \\  \(n/s\) & \(k^{1-O(_{})/ s}\) & \(k^{1+_{u}}\) & _lower bound for any \(_{u}>0\), sufficiently large \(s\)_ & this paper \\ \(n/s\) & \(k^{1-(_{})/ s}\) & \(kn+k^{1+_{}}\) & algorithm for half-uniform distributions & this paper \\  

Table 1: Prior work and our results. For simplicity the results stated only for the realizable case, constant \(>0\), and with \(O()\) factors suppressed. The bound of  (row 6 of the table) is stated as in Theorem 3.1 of that paper. However, by adjusting the free parameters, their algorithm can be easily generalized to use \(n/s\) samples for \(n/s>n/(n)\), resulting in a query time of \(O(n+k^{1-(^{2})/s})\). Note that the term \(1-1/s\) in their bound results in a larger exponent than \(1-1/ s\) in our upper bound. Furthermore, our algorithm is correct as long as \(n/s k/^{2}\) which is the information theoretic lower bound.

is possible. As in [5; 6; 9; 3], we focus on data structures that operate in the so-called _list-of-points model3_, which captures all bounds depicted in Table 1. Suppose that the query time of the data structure is of the form \(k^{_{q}}\). We show that, if the data structure is allowed polynomial space \(kn+k^{1+_{u}}\) and uses \(n/s\) samples, then the query time exponent \(_{q}\) must be at least \(1-O(_{u})/ s\). Therefore, if we set \(s=^{c}k\) for some \(c>0\), as in , then the query time of the data structure must be at at least \(k^{1-O(1)/ k}\). That is, the query time can be improved over linear time in \(k\) by at most a factor of \(k^{O(1)/ k}=2^{O( k/ k)}\). This shows that it is indeed not possible to improve the linear query time by a polynomial factor while keeping the sampling complexity below \(n/^{c}n\), for any constant \(c>0\). Our lower bound instance falls within the realizable case and in the restricted setting where the data and query distributions are "half-uniform", i.e. each distribution is uniform over half of the universe \([n]\). Note that showing a lower bound under these restrictions automatically extends to the agnostic case with general distributions, as the former is a special case of the latter. Our construction takes lower bounds by  for set similarity search as a starting point. We adapt their lower bound to our setting in which queries are formed via samples from a distribution. The resulting lower bound is expressed via a complicated optimization problem and does not yield a closed-form statistical-computational trade-off. One of our technical contributions is solve this optimization problem for a regime of interest to get our explicit lower bound. * **Upper bound:** We complement the lower bound by demonstrating a data structure for our hard instances (in the realizable case with half-uniform distributions) achieving sampling and query time bounds asymptotically matching those of the lower bound. We note that the existence of such an algorithm essentially follows from . However, the algorithms presented there are quite complex. In contrast, our algorithm can be viewed as a "bare-bones" version of their approach, and as a result it is simple and easy to implement. To demonstrate the last point, we present an empirical evaluation of the algorithm on the synthetic data set from , and compare it to the algorithm from that paper, as well as a baseline tailored to half-uniform distributions. Our faster algorithm achieves over \(6\) reduction in the number of operations needed to correctly answer \(100\) random queries. In Figure 1, we illustrate the trade-off between the number of samples and the query time exponent \(_{q}\) in our upper and lower bounds.
* **Open Questions:** The direct question left open by our work is whether there exists a data structure whose upper bounds for general distributions match our lower bounds (note we give matching

Figure 1: _Left: Trade-off between \(1/s\) (samples as a fraction of \(n\)) and the query time exponent \(_{q}\) for our algorithm for half-uniform distributions (solid green curve), the algorithm by  for general distributions (dashed green curve), our analytic lower bound (solid red curve), and a numerical evaluation of the bound from Theorem 3.2 (dashed black curve). We have fixed the space parameter \(_{u}=1/2\). The plots illustrate the asymptotic behaviour proven in Theorem 3.1 and Theorem 4.2 that as \(s\), \(_{q}=1-(1/ s)\) both in the lower bound and for our algorithm for half-uniform distributions. Right: The same plot zoomed in to the upper left corner with \(1/s\) on log-scale._

upper bounds for half-uniform distributions).  give an algorithm for the general case, but with a worse trade-off than that described by our lower bound. More generally, are there other data structures problems for which one can show statistical-computational tradeoffs between the trifecta of samples, query time, and space?

## 2 Preliminaries and Roadmap for the Lower Bound

First we introduce helpful notation used throughout the paper.

Notation:We use \((p)\) to denote the \((p)\) distribution and \(()\) to denote the \(()\) distribution. For a discrete distribution \(f:X\), we use \(supp(f)=\{x X:f(x) 0\}\) to denote \(f\)'s support and \(|supp(f)|\) to denote the size of its support. We use \(f^{n}\) to denote the tensor product of \(n\) identical distribution \(f\). We call a distribution \(f\) half-uniform if it is a uniform distribution on its support \(T\) with \(|T|=n/2\). For a binary distribution \(P\) supported on \(\{0,1\}\) with a random variable \(x P\), we sometimes explicitly write \(P=[x=1]\\ [x=0]\). Similarly, for a joint distribution \(PQ\) over \(\{0,1\}^{2}\) with \((x,y) PQ\), we write \(PQ=[x=1,y=1]&[x=1,y=0]\\ [x=0,y=1]&[x=0,y=0].\)

For a vector \(x^{n}\), we use \(x[i]\) to denote its \(i\)-th coordinate. We use \(d(p||q)=p+(1-p)\) and \(D(P||Q)=_{p P}p\) to denote the standard KL-divergence over a binary distribution of a general discrete distribution. KL divergence \(D(P||Q)\) is only finite when \(supp(P) supp(Q)\), also denoted as \(P Q\). All logarithms are natural.

We now introduce the main problem which we use to prove our statistical-computational lower bound. We state a version which generalizes half-uniform distributions.

**Definition 2.1** (Uniform random density estimation problem).: For a universe \(U=\{0,1\}^{n}\), we generate the following problem instance:

1. A dataset \(P\) is constructed by sampling \(k\) uniform distributions, where for each uniform distribution \(p P\), every element \(i[n]\) is contained in \(p\)'s support with probability \(w_{u}\).
2. Fix a distribution \(p^{*} P\), take \(()|}{s w_{u}})\) samples from \(p^{*}\) and get a query set \(q\).
3. The goal of the data structure is to preprocess \(P\) such that when given the query set \(q\), it recovers the distribution \(p^{*}\).

We denote this problem as \((w_{u},s)\). \(\) abbreviates _Uniform Random Density Estimation_. The name comes from the fact that the data set distributions are uniform over randomly generated supports. In Section 3, we prove a lower bound for \(\) by showing that a previously studied _'hard'_ problem can be reduced to \(\). The previously studied hard problem is the \(\) problem.

**Definition 2.2** (Random \(\) problem ).: For a universe \(U=\{0,1\}^{n}\) and parameters \(0<w_{q}<w_{u}<1\), let distribution \(P_{U}=(w_{u})^{n}\), \(P_{Q}=(w_{q})^{n}\), and \(P_{QU}=w_{q}&0\\ w_{u}-w_{q}&1-w_{u}^{n}\). A random \((w_{u},w_{q})\) problem is generated by the following steps:

1. A dataset \(P U\) is constructed by sampling \(k\) points where \(p P_{U}\) for all \(p P\).
2. A dataset point \(p^{*} P\) is fixed and a query point \(q\) is sampled such that \((q,p^{*}) P_{QU}\).
3. The goal of the data structure is to preprocess \(P\) such that it recovers \(p^{*}\) when given the query point \(q\).

We denote this problem as random \((w_{u},w_{q})\). \(\) abbreviates _Gap Subset Search_. To provide some intuition about how \(\) relates to \(\), let us denote the data set \(P=\{p_{1},,p_{k}\}\). Then the \(p_{i}\{0,1\}^{n}\) can naturally be viewed as \(k\) independently generated random subsets of \([n]\). For each \(i\), \(p_{i}\) includes each element of \([n]\) with probability \(w_{u}\). The query point \(q\) can similarly be viewed as a random subset of \([n]\) including each element with probability \(w_{q}\), but it is correlated with some fixed \(p^{*} P\). Namely, \(p^{*}\) and \(q\) are generated according to the join distribution \(P_{QU}\) (with the right marginal distributions \(P_{Q}\) and \(P_{U}\)) such \(q\) a subset of \(p^{*}\). The goal in \(\) is to identify \(p^{*}\) given \(q\). This intuition is formalized in Section 3.

Our main goal is to study the asymptotic behavior of algorithms with sublinear samples, or specifically, the query time and memory trade-off when only sublinear samples are available, so all our theorems assume the setting that both the support size \(n\) and the number of samples \(k\) goes to infinity and \(n k(n)\). Sublinear samples mean that \(<o(1)\) as \(n\) goes to infinity.

Our lower bound extend and generalize lower bounds for \(\) in the 'List-of-points' model. Thus, the lower bound we provide for URDE is also in the "List-of-points" model defined below (slightly adjusted from the original definition in  to our setting). The model captures a large class of data structures for retrieval problems such as partial match and nearest neighbor search: where one preprocesses a dataset \(P\) to answer queries \(q\) that can "match" a point in the dataset.

**Definition 2.3** (List-of-points model).: Fix a universe \(Q\) of queries, a universe \(U\) of dataset points, as well as a partial predicate \(:Q S\{0,1,\}\). We first define the following \(\)-retrieval problem: preprocess a dataset \(P U\) so that given a query \(q Q\) such that there exist some \(p^{*} P\) with \((q,p^{*})=1\) and \((q,p)=0\) on all \(p P\{p^{*}\}\), we must report \(p^{*}\).

Then a list-of-points data structure solving the above problem is as follows:

1. We fix (possibly random) sets \(A_{i} U\), for \(1 i m\); and with each possible query point \(q Q\), we associate a (random) set of indices \(I(q)[m]\);
2. For the given dataset \(P U\), we maintain \(m\) lists of points \(L_{1},L_{2},...,L_{m}\), where \(L_{i}=P A_{i}\).
3. On query \(q Q\), we scan through lists \(L_{i}\) where \(i I(q)\), and check whether there exists some \(p L_{i}\) with \((q,p)=1\). If it exists, return \(p\).

The data structure succeeds, for a given \(q Q\), \(p^{*} P\) with \((q,p^{*})=1\), if there exists \(i I(q)\) such that \(p^{*} L_{i}\). The total space is defined by \(S=m+_{i[m]}|L_{i}|\) and the query time by \(T=|I(q)|+_{i I(q)}|L_{i}|\).

To see how the lower bound model relates to URDE, in our setting, the '\(\)-retrieval problem' is the URDE problem: \(U\) is the set of random half-uniform distributions, \(Q\) is the family of query samples, and \((q,p)\) is 1 if the samples \(q\) were drawn from the distribution \(p\), and 0 otherwise. (The \(\) case corresponds to an "approximate" answer, considering by the earlier papers; but we define URDE problem directly to not have approximate solutions.)

We use the list-of-points model as it captures all known "data-independent" similarity search data structures, such as Locality-Sensitive Hashing . In principle, a lower bound against this model does not rule out _data-dependent_ hashing approaches. However, these have been useful only for datasets which are not chosen at random. In particular,  conjecture that data-dependency doesn't help on random instances, which is the setting of our theorems.

## 3 Lower bounds for random half-uniform density estimation problem

In this section, we formalize our lower bound. The main theorem of the section is the following.

**Theorem 3.1** (Lower bound for \(\)).: _If a list-of-points data structure solves the \((,s)\) using time \(O(k^{_{q}})\) and space \(O(k^{1+_{u}})\), and succeeds with probability at least \(0.99\), then for sufficiently large \(s\), \(_{q} 1-}-}{ s-1}\)._

To prove Theorem 3.1, our starting point is the following result of  that provides a lower-bound for the random GapSS problem.

**Theorem 3.2** (Lower bound for random GapSS, ).: _Consider any list-of-points data structure for solving the random \((w_{u},w_{q})\) problem on \(k\) points, which uses expected space \(O(k^{1+_{u}})\), has expected query time \(O(k^{_{q}-o_{k}(1)})\), and succeeds with probability at least \(0.99\). Then for every \(\), we have that_

\[_{q}+(1-)_{u}_{t_{q},t_{u}\\ t_{u} w_{u}}F(t_{u},t_{q}),\]_where \(F(t_{u},t_{q})=||w_{q})}{d(t_{u}||w_{u})}+(1-) ||w_{u})}{d(t_{u}||w_{u})}\), \(P=[w_{q}&0\\ w_{u}-w_{q}&1-w_{u}]\) and \(T= D(T||P)\)._

Our proof strategy is to first give a reduction from the the GapSS problem to the URDE problem. Note that the URDE problem involves a _statistical_ step where we receive samples from an unknown distribution (our query). On the other hand, the query of GapSS is a specified vector, rather than a distribution, with no ambiguity. Our reduction bridges this and shows that GapSS is a'strictly easier' problem than URDE.

**Theorem 3.3** (Reduction from random \(\) to \(\)).: _If a list-of-points data structure solves the \((w_{u},s)\) problem of size \(k\) in Definition 2.1 using time \(O(k^{_{q}})\) and space \(O(k^{1+_{u}})\), and succeeds with probability at least \(0.99\), then there exists a list-of-points data structure solving the \((w_{u},w_{q})\) problem for \(w_{q}=w_{u}(1-e^{}})\) using space \(O(k^{1+_{u}})\) and time \(O(k^{_{q}}+w_{q} n)\), and succeeds with probability at least \(0.99\)._

Proof.: We provide a reduction from random \((w_{u},w_{q})\) to \((w_{u},s)\) with \(s=(1-}{w_{u}})}\). Specifically, for each instance \((P_{1},p_{1}^{*},q_{1})\) generated from \((w_{u},w_{q})\) in Definition 2.2, we will construct an instance \((P_{2},p_{2}^{*},q_{2})\) generated from \((w_{u},s)\) satisfying Definition 2.1 for some \(s\).

For each point \(p_{1} P_{1}\), it is straightforward to construct a corresponding uniform distribution \(p_{2}\) supported on those coordinates where \(p_{1}[i]=1\). Then let's construct \(q_{2}\) from \(q_{1}\). Recall that for each \(i U\) with \(p_{1}^{*}[i]=1\), we have \(q_{1}[i]=0\) with probability \(1-}{w_{u}}\), in which case we add no element \(i\) to \(q_{2}\). If \(q_{1}[i]=1\), we add \(_{+}(})\) copies of element \(i\) to \(q_{2}\) where \([_{+}()=x]=[ ()=x]}{[()>0]}\) for any \(x>0\). By setting \(s=(1-}{w_{u}})}\), we have \([(})=0]=1- }{w_{u}}\). Thus for each element \(i\) in \(p_{2}^{*}\), the number of its appearances in \(q_{2}\) exactly follows the distribution \((})\).

According to the property of the Poisson distribution, uniformly sampling \((^{*})|}{s w_{u}})\) elements from a set of size \(|supp(p_{2}^{*})|\) is equivalent to sampling each element \((})\) times. Therefore, the constructed instance \((P_{2},p_{2}^{*},q_{2})\) is an instance of \((w_{u},s)\), as stated in Definition 2.1. Equivalently, we have the relationship \(w_{q}=w_{u}(1-e^{}})\).

Hence we complete our reduction from \(()\) to \(\) (Definition 2.1). 

To get the desired space-time trade-off in the sublinear sample regime, which means \(s\) (or equivalently \(w_{q} 0\)), and to get an interpretable analytic bound, we need to develop upon the lower bound in Theorem 3.2. This requires explicitly solving the optimization problem in Theorem 3.2. Proving Theorem 3.4 (proof in Appendix 3) is the main technical contribution of the paper.

**Theorem 3.4** (Explicit lower bound for random GapSS instance).: _Consider any list-of-points data structure for solving the random \((,w_{q})\) which has expected space \(O(k^{1+_{u}})\), uses expected query time \(O(k^{_{q}-o(1)})\), and succeeds with probability at least \(0.99\). Then we have the following lower bound for sufficiently small \(w_{q}\): \(_{q} 1-w_{q}^{1- 2-o(1)}+}{1+ w_{q}}\)._

Applying our reduction to the random GapSS lower bound above allows us to prove our main theorem.

Proof of Theorem 3.1.: According to the reduction given in Theorem 3.3 from \((w_{u},w_{q})\) to \((w_{u},s)\) where \(w_{q}=w_{u}(1-e^{}})\). We can apply the lower bound in Theorem 3.4 and get the desired lower bound. 

_Remark 3.5_.: Note that in \((,s)\), the distributions are uniform over random subsets of expected size \(n/2\) and the query set is generated by taking \(()|}{s})\) samples from one of them \(p^{*}\). This is not quite saying that the query complexity is \(n/s\). However, by standard concentration bounds, from the Poisson sample, we can simulate sampling with a fixed number of samples \(n/s-()=n/s(1-o(1))\) with high probability, and so, any algorithm using this fixed number of samples must have the same lower bound on \(_{q}\) as in Theorem 3.1.

## 4 A simple algorithm for half-uniform density estimation problem

In this section, we present a simple algorithm for a special case of the density estimation problem when the input distributions are _half-uniform_. The algorithm also works for the related \((,s)\) problem of Theorem 3.1. A distribution \(p\) over \([n]\) is _half-uniform_ if there exists \(T[n]\) with \(|T|=n/2\) such that \(p[i]=2/n\) if \(i T\) and \(0\) otherwise. The problem we consider in this section is:

**Definition 4.1** (Half-uniform density estimation problem; \((s,)\)).: For a domain \([n]\), integer \(k\), \(>0\), and \(s>0\), we consider the following data structure problem.

1. A dataset \(P\) of \(k\) distributions \(p_{1},,p_{k}\) over \([n]\) which are half-uniform over subsets \(T_{1},,T_{k}[n]\) each of size \(n/2\) is given.
2. We receive a query set \(q\) consisting of \(n/s\) samples from an unknown distribution \(p_{i^{*}} P\) satisfying that \(\|p_{i^{*}}-p_{j}\|\) for \(j i^{*}\).
3. The goal of the data structure is to preprocess \(P\) such that when given the query set \(q\), it recovers the distribution \(p_{i^{*}}\) with probability at least 0.99.

This problem is related to the \((1/2,s)\) problem in Theorem 3.1. Indeed, with high probability, an instance of \((1/2,s)\) consists of _almost_ half-uniform distributions with support size \(n/2 O()\). Moreover, two such distributions \(p_{i},p_{j}\) have \(\|p_{i}-p_{j}\|_{1}=(1 O())\) with high probability. Thus, an instance of \((1/2,s)\) is essentially an instance of \((s,1)\).

To solve \((s,)\), we can essentially apply the similarity search data structure of  querying it with the set \(Q\) consisting of all elements that were sampled at least once. This approach obtains the optimal trade-off between \(_{u}\) and \(_{q}\) (at least in the List-of-points model). The contribution of this section is to provide a _very_ simple alternative algorithm with a slightly weaker trade-off between \(_{u}\) and \(_{q}\). Section 5 evaluates the simplified algorithm experimentally. Our main theorem is:

**Theorem 4.2**.: _Suppose \(n\) and \(k\) are polynomially related, \(s 2\), and that \(s\) is such that \( C}\) for a sufficiently large constant \(C\). Let \(>0\) and \(_{u}>0\) be given. There exists a data structure for the \((s,)\) problem using space \(O(k^{1+_{u}}+nk)\) and with query time \(O(k^{1-}{2(2)}}+n/s)\)._

Let us compare the upper bound of Theorem 4.2 to the lower bound in Theorem 3.1. While Theorem 4.2 is stated for half-uniform distributions, its proof is easily modified to work for the \((1/2,s)\) problem where the support size is random. Then \(=1-O(1)\) and as \(s\), \(}=s(1+o(1))\). Thus, the query time exponent in Theorem 4.2 is \(_{q}=1-(1+o(1)))}{ s}\). \((1/2,s)\) is exactly the hard instance in Theorem 3.1, and so we know that any algorithm must have \(_{q} 1-(1+o(1))}{ s}\) as \(s\). Asymptotically, our algorithm therefore gets the right logarithmic dependence on \(s\) but with a leading constant of \((2) 0.693\) instead of \(1\).

Next we define the algorithm. Let \(\) and \(L\) be parameters which we will also specify shortly. During preprocessing, our algorithm samples \(L\) subsets \(S_{1},,S_{L}\) of \([n]\) each of size \(\) independently and uniformly at random. For each \(i L\), it stores a set \(A_{i}\) of all indices \(j[k]\) such that \(S_{i} T_{j}\), namely the indices of the distributions \(p_{j}\) which contain \(S_{i}\) in their support. See Algorithm 1.

During the query phase, we receive \(n/s\) samples from \(p=p_{i^{*}}\) for some unknown \(i^{*}[k]\). Our algorithm first forms the subset \(Q[n]\) consisting of all elements that were sampled at least once. Note that \(|Q| n/s\) as elements can be sampled several times. The algorithm proceeds in two steps. First, it goes through the \(L\) sets \(S_{1},,S_{L}\) until it finds an \(i\) such that \(S_{i} Q\). Second, it scans through the indices \(j A_{i}\). For each such \(j\) it samples a set \(U_{j}\) one element at a time from \(Q\). It stops this sampling at the first point of time where either \(U_{j} T_{j}\) or \(|U_{j}|=C\) for a sufficiently large constant \(C\). In the first case, it concludes that \(p_{j}\) is not the right distribution and proceeds to the next element of \(A_{j}\) and in the latter case, it returns the distribution \(p_{j}\) as the answer to the density estimation problem. See Algorithm 2. We defer the formal proofs to Appendix B.

```
1:Input: Half uniform distributions \(\{p_{i}\}_{i=1}^{k}\) over \([n]\) with support sets \(\{T_{i}\}_{i=1}^{k}\).
2:Output: A data structure for the density estimation problem.
3:procedurePreprocessing(\(\{p_{i}\}_{i=1}^{k}\))
4:for\(i=1\) to \(L\)do
5:\(S_{i}\) sample of size \(\) from \([n]\)
6:\(A_{i}\{j[k] S_{i} T_{j}\}\)
7:Return\((S_{i},A_{i})_{i[L]}\) ```

**Algorithm 2** Query algorithm for half-uniform distributions

## 5 Experiments

We test our algorithm in Section 4 experimentally on datasets of half-uniform distributions as in  and corresponding to our study in Sections 3 and 4. Given parameters \(k\) and \(n\), the input distributions are \(k\) distributions each uniform over a randomly chosen \(n/2\) elements.

AlgorithmsWe compare two main algorithms: an implementation of the algorithm presented in Section 4 which we refer to as the Subset algorithm and a baseline for half-uniform distributions which we refer to as the Elimination algorithm. The Subset algorithm utilizes the same techniques as that presented in Section 4 but with some slight changes geared towards practical usage. We do not compare to the "FastTournament" of  since it was computationally prohibitive; see Remark C.1.

The Subset algorithm picks \(L\) subsets of size \(\) and preprocesses the data by constructing a dictionary mapping subsets to the distributions whose support contains that subset. When a query arrives, scan through the \(L\) subset until we find one that is contained in the support of the query. We then restrict ourselves to solving the problem over the set of distributions mapped to by that subset and run Elimination. The Elimination algorithm goes through the samples one at a time. It starts with a set of distributions which is the entire input in general or a subset of the input when called as a subroutine of the Subset algorithm. To process a sample, the Elimination algorithm removes from consideration all distributions which do not contain that element in its support. When a single distribution remains, the algorithm returns that distribution as the solution. As the input distributions are random half-uniform distributions, we expect to throw away half of the distributions at each step (other than the true distribution) and terminate in logarithmically in size of the initial set of distribution steps.

Experimental SetupOur experiments compare the Subset and Elimination algorithms while varying several problem parameters: the number of distributions \(k\), the domain size \(n\), the number of samples \(S\) (for simplicity, we use this notation rather than \(n/s\) samples as in the rest of the paper), and the size of subsets \(\) used by the Subset algorithm. While we vary one parameter at a time, the others are set to a default of \(k=50000\), \(n=500\), \(S=50\), \(l=3\). Given these parameters, we evaluate the Subset algorithm and the Elimination algorithm on 100 random queries where each query corresponds to picking one of the input distributions as the true distribution to draw samples from.

In all settings we test, the Elimination algorithm achieves \(100\%\) accuracy on these queries, which is to be expected as long as the number of samples is sufficently more than the \(_{2}k\). There is a remaining free parameter, which is the number of subsets \(L\) used in the Subset algorithm. We start with a modest value of \(L=200\) and increase \(L\) by a factor of \(1.5\) repeatedly until the Subset algorithm also achieves \(100\%\) accuracy on the queries (in reality, it's failure probability will likely still be greater than that of the Elimination algorithm). The results we report correspond to this smallest value of \(L\) for which the algorithm got all the queries correct.

For both algorithms, we track the average number of operations as well as the execution time of the algorithms (not counting preprocessing). A single operation corresponds to a membership query of checking whether a given distribution/sample contains a specified element in its support which is the main primitive used by both algorithms. We use code from  as a basis for our setup.

ResultsFor all parameter settings we test, the number of operations per query by our Subset algorithm is significantly less than those required by Elimination algorithm, up to a factor of more than **6x**. The average query time (measured in seconds) shows similar improvements for the Subset algorithm though for some parameter settings, it takes more time than the Elimination algorithm. While, in general, operations and time are highly correlated, these instances where they differ may depend on the specific Python data structures used to implement the algorithms, cache efficiency, or other computational factors.

As the number of distributions \(k\) increases, Figure 1(a) shows that both time and number of operations scale linearly. Across the board, our Subset algorithm outperforms the Elimination algorithm baseline and exhibits a greater improvement as \(k\) increases. On the other hand, as the domain size increases in Figure 1(b), the efficiency of the Subset algorithm degrades while the Elimination algorithm maintains its performance. This is due to the fact that for larger domains, more subsets are needed in order to correctly answer all queries, leading to a greater runtime.

Figure 2: Comparison of efficiency of the Subset (Ours) and Elimination algorithms as (a): the number of distributions \(k\) varies. Other parameters are set to \(n=500,S=50,=3\). (b): the domain size \(n\) varies. Other parameters are set to \(k=50000,S=50,=3\). (c): the number of samples \(S\) varies. Other parameters are set to \(k=50000,n=500,=3\). (d): the subset size \(\) varies. Other parameters are set to \(k=50000,n=500,S=50\).

In Figure 1(c), we see that across the board, as we vary the number of samples, the Subset algorithm has a significant advantage over the Elimination algorithm in query operations and time. Finally, Figure 1(d) shows that for subset size \(\{2,3,4\}\), the Subset algorithm experiences a significant improvement over the Elimination algorithm. But for \(=5\), the improvement (at least in terms of time) suddenly disappears. For this setting, that subset size requires many subsets in order to get high accuracy, leading to longer running times.

Overall, on flat distributions for a variety of parameters, our algorithm has significant benefits even over a baseline tailored for this case. The good performance of the Subset algorithm corresponds with our theory and validates the contribution of providing a simple algorithm for density estimation in this hard setting.

Acknowledgments:This work was supported by the Jacobs Presidential Fellowship, the Mathworks Fellowship, the NSF TRIPODS program (award DMS-2022448) and Simons Investigator Award; also supported in part by NSF (CCF2008733) and ONR (N00014-22-1-2713). Justin Chen is supported by an NSF Graduate Research Fellowship under Grant No. 174530. Shyam Narayanan is supported by an NSF Graduate Fellowship and a Google Fellowship. Anders Aamand was supported by the DFF-International Postdoc Grant 0164-00022B and by the VILLUM Foundation grants 54451 and 16582.