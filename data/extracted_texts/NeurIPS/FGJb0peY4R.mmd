# Appendix

Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization

 Jiarui Jiang\({}^{*}\)1, Wei Huang\({}^{*}\)2, Miao Zhang\({}^{}\)1, Taiji Suzuki\({}^{3}\)2, Liqiang Nie\({}^{1}\)

\({}^{1}\)Harbin Institute of Technology, Shenzhen

\({}^{2}\)RIKEN AIP

\({}^{3}\)University of Tokyo

jiaruij@outlook.com, wei.huang.vr@riken.jp, zhangmiao@hit.edu.cn

taiji@mist.i.u-tokyo.ac.jp, nieliqiang@hit.edu.cn

Equal contributionCorresponding author

###### Abstract

Transformers have demonstrated great power in the recent development of large foundational models. In particular, the Vision Transformer (ViT) has brought revolutionary changes to the field of vision, achieving significant accomplishments on the experimental side. However, their theoretical capabilities, particularly in terms of generalization when trained to overfit training data, are still not fully understood. To address this gap, this work delves deeply into the _benign overfitting_ perspective of transformers in vision. To this end, we study the optimization of a Transformer composed of a self-attention layer with softmax followed by a fully connected layer under gradient descent on a certain data distribution model. By developing techniques that address the challenges posed by softmax and the interdependent nature of multiple weights in transformer optimization, we successfully characterized the training dynamics and achieved generalization in post-training. Our results establish a sharp condition that can distinguish between the small test error phase and the large test error regime, based on the signal-to-noise ratio in the data model. The theoretical results are further verified by experimental simulation. To the best of our knowledge, this is the first work to characterize benign overfitting for Transformers.

## 1 Introduction

Transformers (Vaswani et al., 2017) have revolutionized numerous fields in artificial intelligence, ranging from natural language processing (Devlin et al., 2018; OpenAI, 2023), computer vision Dosovitskiy et al. (2020); He et al. (2022), graph Yun et al. (2019); Hu et al. (2020) to reinforcement learning Chen et al. (2021); Janner et al. (2021). In particular, Vision Transformer (ViT) Dosovitskiy et al. (2020) has been developed to advancing the computer vision tasks (Liu et al., 2021; Khan et al., 2022; Guo et al., 2022; Tolstikhin et al., 2021; Lin et al., 2014) compared to Convolutional neural networks. Since then, with the high performance, ViT emerged as a hot area of research and application. Numerous technologies and methods began to surface, including enhancing ViT's data and resource efficiency (Touvron et al., 2021; Hassani et al., 2021; Chen et al., 2021; Touvron et al., 2022), reducing heavy computation cost Chen et al. (2022); Zhu et al. (2024); Wang et al. (2023).

Recent success brought by ViT has been inspiring an increasing number of works to understand the ViT through empirical and theoretical studies. The empirical investigation works study the robustness (Bhojanapalli et al., 2021; Naseer et al., 2021; Paul and Chen, 2022; Bai et al., 2021); and the role in self-supervision (Caron et al., 2021; Chen et al., 2021; Oquab et al., 2023) and others. On the other hand side, the theoretical works have been conducted to understand the ViT from the perspective of expressivity (Vuckovic et al., 2020; Edelman et al., 2022), optimization (Zhang et al., 2020; Tian et al., 2023, 2023), generalization (Li et al., 2023), and in-context learning (Huang et al., 2023; Garg et al., 2022; Bai et al., 2024; Ahn et al., 2024).

Despite the insightful understanding provided by the above investigations, it remains unclear how ViTs generalize to unseen data when they are trained to overfit the training data. In particular, the conditions under which ViTs can exhibit _benign overfitting_(Barlett et al., 2020), where the test error remains small despite overfitting to the training data, are not well understood. Moreover, prior work on the optimization and generalization of ViTs often focuses on simplified settings, such as linear transformers or unrealistic loss functions, due to technical limitations. In this work, we aim to fill this gap through a feature learning theory (Allen-Zhu and Li, 2020; Cao et al., 2022). Based on a data generative model with \(M\) tokens composed of signal tokens and noise tokens and a two-layer ViT with softmax attention, we characterize the training dynamics from a random initialization and the generalization ability of the ViT after convergence. We establish a sharp separation in condition to distinct the benign overfitting and harmful overfitting regime for ViTs. Our contributions are summarized as follows:

* We successfully characterize the optimization of the ViT through three different phases in dynamics that exhibit rich and unique behaviors related to self-attention. Building on the convergence results, we further characterize the generalization bounds on unseen datasets.
* Technically, we develop novel methods to handle non-linear attention and the inter-dependent nature of multiple weights in transformer optimization when training from scratch.
* We establish a sharp separation condition in the signal-to-noise ratio in data to distinguish the benign overfitting and harmful overfitting regimes of ViTs. This separation is then verified by experimental simulation.

## 2 Related Work

Benign Overfitting in Deep LearningThe most prominent behavior of deep learning is its breaking of bias-variance trade-off in statistical learning theory, i.e., good generalization on unseen data even with overfitting on training data. This line of research starts from Bartlett et al. (2020) who studied benign overfitting in linear regression, learning data generated by a linear model with additive Gaussian noises. It is shown that at large input dimensions (which leads to over-parameterization), the excessive risk of the interpolation can be asymptotically optimal. Hastie et al. (2022); Wu and Xu (2020) studied linear regression when both the dimension and the number of samples scale together with a fixed ratio. Timor et al. (2023) proposed a unified data model for linear predictors and studied conditions under which benign overfitting occurs in different problems. Besides the studies on linear models (Wang et al., 2021; Zou et al., 2021; Zhou and Ge, 2023), several recent works tried to study benign overfitting in neural networks (Frei et al., 2022; Kornowski et al., 2024; Chatterji et al., 2022; Xu et al., 2023). In particular, Li et al. (2021) investigated the benign overfitting in two-layer neural networks with the first layer parameters fixed at random initialization. Furthermore, _benign overfitting_ was characterized in convolution neural networks (Cao et al., 2022; Kou et al., 2023), OOD (Chen et al., 2024), federated learning (Huang et al., 2023), graph neural network (Huang et al., 2023) by feature learning theory (Allen-Zhu and Li, 2020; Cao et al., 2022). Unlike existing research on benign overfitting, this work focuses on ViTs.

Optimization of TransformersTowards understanding the optimization of Transformers, Zhang et al. (2020) provided the analysis for adaptive gradient methods. Jelassi et al. (2022) proposed a spatially structured dataset and a simplified ViT model and showed that ViT implicitly learns the spatial structure of the dataset while generalizing. Besides, Li et al. (2023) provided fine-grained mechanistic understanding of how transformers learn "semantic structure" for BERT-like framework (Devlin et al., 2018). Furthermore, Tian et al. (2023, 2023) characterizes the SGD training dynamics of a 1-layer Transformer and multi-layer Transformer respectively. They focused on the unique phenomena and the role of attention in the training dynamics. Huang et al. (2023) studied the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. In addition, Li et al. (2024) provided a theoretical analysis of the training dynamics of Transformers with nonlinear self-attention within In-Context Learning. In particular, Li et al. (2023a) is most relevant to us, as they also study the training dynamics of ViTs with a similar data model. However, they considered hinge loss and a specific initialization to simplify analysis and did not characterize the harmful overfitting regime. In summary, while the above work studies the optimization dynamics of attention-based models, they do not characterize benign overfitting as we investigate.

## 3 Problem Setup

In this section, we outline the data generation model, the Vision Transformer model, and the gradient descent training algorithm.

**Notations.** For two sequences \(\{x_{n}\}\) and \(\{y_{n}\}\), we denote \(x_{n}=O(y_{n})\) if there exist some absolute constant \(C>0\) and \(N>0\) such that \(|x_{n}| C|y_{n}|\) for all \(n N\), denote \(x_{n}=(y_{n})\) if \(y_{n}=O(x_{n})\), and denote \(x_{n}=(y_{n})\) if \(x_{n}=O(y_{n})\) and \(x_{n}=(y_{n})\). We also denote \(x_{n}=o(y_{n})\) if \(\ |x_{n}/y_{n}|=0\). Finally, we use \(()\), \(()\) and \(()\) to hide logarithmic factors in these notations respectively.

**Definition 3.1** (Data Generation Model).: _Let \(_{+},_{-}^{d}\) be fixed vectors representing the signals contained in data points, where \(\|_{+}\|_{2}=\|_{-}\|_{2}=\|\|_{2}\) and \(_{+},_{-}=0\,\). Then each data point \((,y)\) with \(=(_{1},_{2},,_{M})^{}^{M d}\) and \(y\{-1,1\}\) is generated from the following distribution \(D\):_

1. _The label_ \(y\) _is generated as a Rademacher random variable._
2. _If_ \(y=1\) _then_ \(_{1}\) _is given as_ \(_{+}\)_, if_ \(y=-1\) _then_ \(_{1}\) _is given as_ \(_{-}\)_, which represents signals._
3. _A noise vector_ \(_{2}\) _is generated from the Gaussian distribution_ \((0,_{p}^{2}(-_{+}_{+}^{ }\|\|_{2}^{-2}-_{-}_{-}^{}\| \|_{2}^{-2}))\)_._
4. _Noise vectors_ \(_{3},,_{M}\) _is generated i.i.d from the Gaussian distribution_ \((0,_{p}^{2}(-_{+}_{+}^{}\| \|_{2}^{-2}-_{-}_{-}^{}\|\|_{2}^{-2}))\)_._
5. \(_{2},,_{M}\) _are given by_ \(_{2},,_{M}\)_, which represent noises._

Our data model is envisioned by considering \(M\) tokens of the data points: \(_{1},_{2},,_{M}\), which can be seen as patches derived from image data. \(_{1}\) embodies the signal that is inherently linked to the data's class label, while \(_{2},_{3},,_{M}\) represents the noise, which is not associated with the label. For simplicity, we assume that the noise patches is independently drawn from Gaussian distribution \((0,_{p}^{2}(-_{+}_{+}^{ }\|\|_{2}^{-2}-_{-}_{-}^{}\| \|_{2}^{-2}))\) and \((0,_{p}^{2}(-_{+}_{+}^{}\| \|_{2}^{-2}-_{-}_{-}^{}\|\|_{2}^{-2}))\), ensuring the noise vectors remains orthogonal to the signal vectors \(_{+}\) and \(_{-}\). Also, if \(_{p}\) is sufficiently larger than \(_{p}\), the input tokens will become sparse, which is consistent with the empirical observation that the _attention_ and activation values in Transformer-based models are usually sparse (Child et al., 2019; Robinson et al., 2023). We denote \(=\|\|_{2}/(_{p})\) to represent the signal-to-noise ratio.

**Two-layer Transformer.** We consider a two layer Transformer network with a self-attention layer and a fixed linear layer, which is defined as:

\[f(,)=_{l=1}^{M}(_{l}^{}_ {Q}_{K}^{}^{})_{V}_{O}.\] (1)

Here, \(():^{M}^{M}\) denote the softmax function, \(_{Q},_{K}^{d d_{h}},_{V}^{d  d_{v}}\) denote the query matrix, key matrix, and value matrix, respectively, and \(_{O}^{d_{v}}\) denote the weight for the linear layer. We use \(\) to denote the collection of all the model weights. This model is a simplified version of the ViT model from Dosovitskiy et al. (2020), making our analysis focus on the self-attention mechanism which is the most critical component of the ViTs.

Given a training data set \(S=\{(_{n},y_{n})\}_{n=1}^{N}\) generated from the distribution \(D\) defined in Definition 3.1, where the subscript \(n\) represents the \(n\)-th sample, we train the two-layer Transformer by minimizing the empirical cross-entropy loss function:

\[L_{S}()=_{n=1}^{N}(y_{n}f(_{n},)),\]

where \((z)=(1+(-z))\) and \(f(,)\) is the two-layer Transformer. We further defined the population loss (test loss) \(L_{D}():=_{(,y) D}(yf(,))\).

**Training algorithm.** We consider Gaussian initialization for the network weights \(_{Q},_{K}\) and \(_{V}\), where each entry of \(_{Q}\) and \(_{K}\) is sampled from a Gaussian distribution \((0,_{h}^{2})\), and \(_{V}\) is sampled from \((0,_{V}^{2})\) at initialization. We use gradient descent to optimize training loss \(L_{S}()\), and the update of \(_{Q},_{K}\) and \(_{V}\) can be written as follows:

\[^{(t+1)}=^{(t)}-_{}L_{S}((t)),\]

where \(^{(t)}\) can be designated as \(_{Q},_{K}\) or \(_{V}\).

Recalling the ViT defined in Eq. (1), we can intuitively analyze its training dynamics and generalization: if more attention is paid to signals, i.e., \(_{l}^{}_{Q}_{K}^{}_{}_{l}^{ }_{Q}_{K}^{}_{l}\) for \(i[M]/\{1\}\), then the vector \((_{l}^{}_{Q}_{K}^{}^{})\) has a higher similarity with signals \(_{}\) rather than with noises \(_{i}\). In turn, \(_{V}\) together with \(_{O}\) have more chance to learn signals \(_{}\) and utilize them to make prediction. At the same time, if vector \(_{V}_{O}\) learns signals \(_{}\) more than noises \(_{i}\), i.e., \(_{}_{V}_{O}_{i}_{V}_{O}\), then during the gradient descent training, the gradient send to \(_{l}^{}_{Q}_{K}^{}_{}\) will be larger than that send to \(_{l}^{}_{Q}_{K}^{}_{i}\). As a result, more and more attention is paid on signals and the similarity between signals \(_{}\) and \(_{V}_{O}\) becomes increasingly high. Therefore, the Transformer can perform well on new data points. On the contrary, if much attention is paid to some noises in the training dataset and the model utilizes them for making prediction and optimizing the training loss, then the model can fit the training dataset well but might not perform well on the new test data.

## 4 Main Results

This section presents our main theoretical results which characterize the convergence and generalization of the ViT model under different sample size \(N\) and signal-to-noise ratio \(=\|\|_{2}/(_{p})\). The results are based on the following conditions.

**Condition 4.1**.: _Given a sufficiently small failure probability \(>0\) and a target training loss \(>0\), suppose that:(1) Dimension \(d_{h}=^{1}\{^{4},^{-4}\}N ^{2}^{-2}\). (2) Dimension \(d=^{-2}N^{2}d_{h}\). (3) Training sample size \(N=((d))\).(4) The number of input tokens \(M=(1)\). (5) The \(_{2}\)-norm of linear layer weights \(\|_{O}\|_{2}=(1)\). (6) The learning rate \((\{\|\|_{2}^{-2},(_{p}^{2}d)^{-1}\}  d_{h}^{-})\). (7) The standard deviation of Gaussian initialization \(_{V}\) satisfies: \(_{V}\|_{O}\|_{2}^{-1}\{\| \|_{2}^{-1},(_{p})^{-1}\} d_{h}^{-}\). (8) The variance of Gaussian initialization \(_{h}^{2}\) satisfies: \(\{\|\|_{2}^{-2},(_{p}^{2}d)^{-1}\} d_{h}^{- }(6N^{2}M^{2}/)^{-2}_{h}^{2}\{\| \|_{2}^{-2},(_{p}^{2}d)^{-1}\} d_{h}^{-} (6N^{2}M^{2}/)^{-}\). (9) Target training loss \( O(1/(d))\). (10) The relationship between \(_{p}\) and \(_{p}\) satisfies \(_{p}=C_{p}_{p}\) and \(C_{p}=5\)._

Conditions (1) and (2) ensure an over-parametrized learning setting, and similar conditions have been made in the theoretical analysis of CNN models (Cao et al., 2022; Kou et al., 2023). Condition (3) ensures that there are enough samples in each class with high probability. Conditions (4)-(5) are intended to simplify the calculation, and can be easily generalized to \(M=(1)\), \(\|_{O}\|_{2}=o(1)\) or \(\|_{O}\|_{2}=(1)\) setting. Conditions (6)-(8) ensure that the Transformer can be effectively trained well. Condition (9) ensures that the Transformer is sufficiently overfitting the training data. Condition (10) ensures the sparsity of the input token features.

**Theorem 4.1** (**Benign Overfitting**).: _Under Condition 4.1, if \(N^{2}=(1)\), then with probability at least \(1-d^{-1}\), there exist \(T=(^{-1}^{-1}\|\|_{2}^{-2}\|_{O}\|_{2}^{-2})\) such that:_

1. _The training loss converges to_ \(\)_:_ \(L_{S}((T))\)_._
2. _The test loss is nearly zero:_ \(L_{D}((T)) o(1)\)_._

Theorem 4.1 characterizes the case of _benign overfitting_. It shows that as long as \(N^{2}=(1)\), the Transformer can generalize well, even though it overfit the training data. To complement the above result and highlight the sharpness of this condition, we present the following theorem for the regime of _harmful overfitting_.

**Theorem 4.2** (**Harmful Overfitting**).: _Under Condition 4.1, if \(N^{-1}^{-2}=(1)\), then with probability at least \(1-d^{-1}\), there exist \(T=(N^{-1}^{-1}_{p}^{-2}d^{-1}\|_{O}\|_{2}^{-2})\) such that:_

1. _The training loss converges to_ \(\)_:_ \(L_{S}((T))\)_._
2. _The test loss is high:_ \(L_{D}((T))=(1)\)_._

Theorem 4.2 shows that if \(N^{-1}^{-2}=(1)\), the trained Transformer has a high test loss as a result of overfitting the noises in the training data. Theorem 4.1 and Theorem 4.2 reveals a sharp phase transition between _benign overfitting_ and _harmful overfitting_.

Comparison with other results.Firstly, compared with CNNs with \(^{q}\) network result (Cao et al., 2022), when signal-to-noise ratio is small (\( 1\)), our results show that ViTs require less number of samples to generalize well, which reflects the advantage of Transformers. Secondly, previous theoretical benign overfitting analysis of CNNs with ReLU activations rely heavily on signal strength \(\|\|_{2}\) and signal-to-noise ratio \(\)(Kou et al., 2023; Meng et al., 2023), i.e., \(^{2}=(1/N)\) and require \(\|\|_{2}\) to be large enough, while our analysis does not need to impose restrictions on these conditions. Thirdly, previous works only show when Transformer can generalize well (Li et al., 2023; Deora et al., 2023), while we demonstrate harmful overfitting as a complementary, which reflect that our benign overfitting condition is tighter and more precise.

## 5 Proof Sketch

This section discusses our main challenges in studying ViT's training dynamic, and presents our technical solutions to overcoming them. The complete proofs are given in the appendix.

### Vectorized Q & K and scalarized V

Our first main challenge is to deal with the three matrices \(_{Q}\), \(_{K}\) and \(_{V}\). In contrast to CNN models, whose convolutional kernels can be treated as vectors and thus allow for a more straightforward analytical approach (Cao et al., 2022; Kou et al., 2023), the QKV matrices within the Transformer model are inherently complex to analyze. Moreover, their mutual interactions further complicate the analysis. To circumvent this complexity, Oymak et al. (2023) and Tian et al. (2023) merge the key-query weights(e.g. \(:=_{Q}_{K}^{}\)), Huang et al. (2023); Zhang et al. (2024); Jelassi et al. (2022) employ a specific initialization (e.g. \(_{Q}^{(0)}=_{K}^{(0)}=\)). Although their approach simplifies the analysis, it is not conducive to showing the dynamics of QKV interactions, i.e., how \(_{Q}\), \(_{K}\), and \(_{V}\) affect each other.

In order to simplify the analysis of QKV dynamics without losing rigor, we propose key techniques called _Vectorized Q & K_ and _scalarized V_. The basic idea comes from the property that the product of token feature vectors with the QKV matrices results in vectors, e.g., \(_{+}^{}_{Q},_{2}^{}_{Q}\), etc., which are more amenable to analysis. Further, each entry of matrix \(_{Q}_{K}^{}^{}\) can be regarded as the inner product of two vectors, e.g., \(_{+}^{}_{Q}_{K}^{}_{+}=_{+} ^{}_{Q},_{+}^{}_{K}\). Therefore, the dynamics of _attention_ can be studied by analyzing the dynamics of the _vectorized Q & K_ defined as follows:

**Definition 5.1** (Vectorized Q & K).: _Let \(_{Q}^{(t)}\) and \(_{K}^{(t)}\) be the QK matrices of the ViT at the t-th iteration of gradient descent. Then we define the vectorized Q and vectorized K as follows_

\[_{+}^{(t)}=_{+}^{}_{Q}^{(t)}, _{-}^{(t)}=_{-}^{}_{Q}^{(t)}, _{n,i}^{(t)}=_{n,i}^{}_{Q}^{(t)},\] \[_{+}^{(t)}=_{+}^{}_{K}^{(t)}, _{-}^{(t)}=_{-}^{}_{K}^{(t)}, _{n,i}^{(t)}=_{n,i}^{}_{K}^{(t)},\]

_for \(i[M]\{1\},n[N]\)._

With Definition 5.1, denoting \(S_{+}:=\{n[N]:y_{n}=1\}\), \(S_{-}:=\{n[N]:y_{n}=-1\}\), we further analyze the dynamics of the _vectorized Q & K_. By carefully computing the dynamic of \(_{+}^{(t)}\), we have

\[_{+}^{(t+1)}-_{+}^{(t)}=_{n S_{+}}-_{n}^ {(t)}\|\|_{2}^{2}_{O}^{}_{V}^{(t)}_{n }^{}(diag(_{n,1}^{(t)})-_{n,1}^{(t)}_{n,1}^{(t)})_{n}_{K}^{(t)},\] (2)

where \(_{n,i}^{(t)}:=(_{n,i}^{}_{Q}^{(t)} _{K}^{(t)}_{n}^{})\) is a shorthand notation, and \(_{O}^{}_{V}^{(t)}_{n}^{}\) and \(_{n}_{K}^{(t)}\) can be viewed in the following forms

\[_{O}^{}_{V}^{(t)}_{n}^{}=_{+}^{ }_{V}^{(t)}_{O},_{n,2}^{}_{V}^{(t)}_{ O},,_{n,M}^{}_{V}^{(t)}_{O},\]

\[_{n}_{K}^{(t)}=_{+}^{(t)},_{n,2}^{(t)}, ,_{n,M}^{(t)}^{}.\]

Thus \(_{+}^{(t+1)}-_{+}^{(t)}\) can be decomposed into a linear combination of \(_{+}^{(t)}\) and \(_{n,i}^{(t)}\). Therefore, Eq. (2) can be further expanded as \(_{+}^{(t+1)}-_{+}^{(t)}=_{+,+}^{(t)}_{+}^{(t)}+_{n  S_{+}}_{i=2}^{M}_{n,+,i}^{(t)}_{n,i}^{(t)}\), where \((_{+,+}^{(t)},_{n,+,2}^{(t)},,_{n,+,M}^{(t)})=_{n S_{+}}-_{n}^{(t)}\|\|_{2}^{2}_{O}^{ }_{V}^{(t)}_{n}^{}(diag(_{n,1}^{(t)})- _{n,1}^{(t)}_{n,1}^{(t)})\). Intuitively, if \(_{+,+}^{(t)}\) is larger enough than \(_{n,+,i}^{(t)}\), then \(_{+}^{(t)},_{+}^{(t)}\) will grow faster than \(_{+}^{(t)},_{n,i}^{(t)}\), which means token \(_{+}\) will pay more _attention_ to \(_{+}\) rather than \(_{n,i}\). The dynamics of \(_{-}^{(t)}\), \(_{n,i}^{(t)}\), \(_{+}^{(t)}\), \(_{-}^{(t)}\) and \(_{n,i}^{(t)}\) are similar to \(_{+}^{(t)}\), and we can study the dynamics of \(QK\) matries by analysing the linear combination coefficients such as \(_{+,+}^{(t)}\), \(_{n,+,i}^{(t)}\).

As \(_{n,i}^{}_{V}^{(t)}_{O}\) are scalars and their dynamics contain a factor \(\|_{O}\|_{2}^{2}\), we define the _scalarized V_ as follows.

**Definition 5.2** (Scalarized V).: _Let \(_{V}^{(t)}\) be the V matrix of the ViT at the t-th iteration of gradient descent. Then there exist coefficients \(_{V,+}^{(t)}\), \(_{V,-}^{(t)}\), \(_{V,n,i}^{(t)}\) such that_

\[_{+}^{}_{V}^{(t)}_{O}=_{+}^{}_{V}^{(0) }_{O}+_{V,+}^{(t)}\|_{O}\|_{2}^{2},\]

\[_{-}^{}_{V}^{(t)}_{O}=_{-}^{}_{V}^{(0) }_{O}+_{V,-}^{(t)}\|_{O}\|_{2}^{2},\]

\[_{n,i}^{}_{V}^{(t)}_{O}=_{n,i}^{}_{V}^{(0) }_{O}+_{V,n,i}^{(t)}\|_{O}\|_{2}^{2}\]

_for \(i[M]\{1\},n[N]\)._

With the _Vectorized Q & K_ and _scalarized V_, one can simplify the study of the Transformer learning process to a meticulous calculation of the coefficients such as \(\), \(\), \(\) throughout the training period. So how to calculate the dynamics of these coefficients is a key point in our analysis. In the next subsection we describe how to handle _softmax_ function and further give bounds for these coefficients.

### Dealing with the softmax function

Our second challenge is to deal with the _softmax_ function, which is the critical component and introduces non-linear transformation in our Transformer model.

As \(_{Q}\) and \(_{K}\) are within the _softmax_ function and \(_{V}\) is outside the _softmax_ function, we divide the dynamic of training process into two key aspects: (1) How \(_{Q}\) and \(_{K}\) affect \(_{V}\); (2) How \(_{V}\) affects \(_{Q}\) and \(_{K}\). Next, we present our approach to addressing these two critical issues.

1. How \(_{Q}\) and \(_{K}\) affect \(_{V}\):Without loss of generality, we take a data point \((_{n},y_{n})\) with \(y_{n}=1\) as an example and provide the dynamics for \(_{V,+}^{(t)}\) and \(_{V,n,i}^{(t)}\) in \(_{V}\) as follows:

**Lemma 5.1** (Dynamics of \(\) and \(\)).: \[_{V,+}^{(t+1)}-_{V,+}^{(t)}=\|_{2}^{2}}{NM} _{n S_{+},s[M]}^{(t)}(_{n,s}^{} _{Q}^{(t)}_{K}^{(t)}_{+})}{(_{n,s}^{}_{Q}^{(t)}_{K}^{(t)}_{+})+_{k=2}^{M}(_{n,s}^{ }_{Q}^{(t)}_{K}^{(t)}_{n,k})}\]

\[|_{V,n,i}^{(t+1)}-_{V,n,i}^{(t)}|^{2} _{p}^{2}d}{NM}_{s[M]}^{(t)}(_{n,s}^{} _{Q}^{(t)}_{K}^{(t)}_{n,i})}{(_{n,s}^{} _{Q}^{(t)}_{K}^{(t)}_{+})+_{k=2}^{M}( _{n,s}^{}_{Q}^{(t)}_{K}^{(t)}_{n,k})}\]

_for \(i[M]\{1\},n S_{+}\), where \(_{n}^{(t)}:=^{}(y_{n}f(_{n},(t)))\) is a shorthand notation._

In benign overfitting regime where \(N^{2}=(1)\), the factor \(_{n S_{+}}\|_{2}^{2}}{NM}\) is larger than \(^{2}_{p}^{2}d}{NM}\). Therefore, as long as \(_{n,s}^{}_{Q}^{(t)}_{K}^{(t)}_{+}\) are not less than \(_{n,s}^{}_{Q}^{(t)}_{K}^{(t)}_{n,i}\), \(_{V,+}^{(t)}\) will grow faster than \(_{V,n,i}^{(t)}\). In other words, if \(_{}^{(t)}\) and \(_{n,i}^{(t)}\) prefer to align with \(_{}^{(t)}\) rather than \(_{n,i}^{(t)}\), then \(_{V}\) would prefer to learn signals rather than memorize noises.

2. How \(_{V}\) affects \(_{Q}\) and \(_{K}\):Recalling that \((_{+,+}^{(t)},_{n,+,2}^{(t)},,_{n,+,M}^{(t)})=_{n S_{+}}-_{n}^{(t)}\|\|_{2}^{2}_{O}^{}_{V}^{(t)}_{n}^{}(diag(_{n,1}^{(t)})- _{n,1}^{(t)}_{n,1}^{(t)})\), where the most complicated part is the matrix \((diag(_{n,1}^{(t)})-_{n,1}^{(t)}_{n,1 }^{(t)})\). We observe that the matrix \(diag(_{n,1}^{(t)})-_{n,1}^{(t)}_{n,1 }^{(t)}\) has two important properties:

1. The diagonal elements are positive, while the elements on the off-diagonal are negative.
2. The sum of each row and column of this matrix is 0.

Based on properties 1 and 2, we can deduce the following conclusion: as long as \(_{+}^{}_{V}^{(t)}_{O}\) is larger enough than \(_{n,i}^{}_{V}^{(t)}_{O}\), we have \(_{+,+}^{(t)} 0\) and \(_{n,+,i}^{(t)} 0\), implying that \(_{+}^{(t)}\) prefers to align \(_{+}^{(t)}\) rather than \(_{n,i}^{(t)}\).

In summary, if more _attention_ is paid to signals, \(_{V}\) learns the signals faster than it memorizes the noises; conversely, if \(_{V}\) learns the signals significantly more than it memorizes the noises, more _attention_ will be paid to signals rather than noises. In other words, \(_{Q}\), \(_{K}\) and \(_{V}\) promote each other.

### Three-Stage Decoupling

Our third main challenge is to deal with the complicated relation among the coefficients, e.g., \(\), \(\), \(\). Inspired by the two-stage analysis utilized by Cao et al. (2022) to decouple the coefficients in CNN models, we analyze the ViT training process in three stage. In the following, we explain the key steps for proving Theorem 4.1. The proof for Theorem 4.2 is similar and we detail it in the appendix.

Stage 1:The analysis in 5.2 shows that \(_{Q}\), \(_{K}\) and \(_{V}\) can promote each other. But this process of mutual reinforcement requires some conditions, e.g. \(_{+}^{}_{V}^{(t)}_{O}\) is sufficiently larger than \(_{n,i}^{}_{V}^{(t)}_{O}\) which does not necessarily hold for Gaussian initialization, complicating the proof. Stage 1 was introduced to solve this problem, and Lemma 5.2 formally describes this stage:

**Lemma 5.2** (V's Beginning of Learning Signals).: _Under the same conditions as Theorem 4.1, there exist \(T_{1}=^{}(N\|\|_{2}^{2}-60M^{2}C _{p}^{2}_{d}^{2}\|_{O}\|_{2}^{2}}\) such that the first element of vector \(_{n}_{V}^{(t)}_{O}\) dominate its other elements, that is, \(_{+}^{}_{V}^{(t)}_{O} 3M|_{n,i}^{}_{V}^{(t)} _{O}|\) for all \(n S_{+}\), \(i[M]\{1\}\) and \(_{-}^{}_{V}^{(t)}_{O}-3M|_{n,i}^{} _{V}^{(t)}_{O}|\) for all \(n S_{-}\), \(i[M]\{1\}\)._With Lemma 5.2, we can guarantee the monotonicity of _attention_ on signals for \(t T_{1}\), which allows us to concentrate on analyzing the growth rate of _attention_ in the following stages.

Stage 2:Note that the output of _softmax_ function has an upper bound 1, thus the output of ViT can be bounded by \(\{|_{}^{}_{V}^{(t)}_{O}|,|_{n,i}^{} _{V}^{(t)}_{O}|\}\). By Lemma 5.1, it can be proved that there exists \(T_{2}=^{-1}\|\|_{2}^{-2}\|_{O}\|_{2}^{-2}(6N ^{2}M^{2}/)^{-1}\) such that: \(\{|_{}^{}_{V}^{(t)}_{O}|,|_{n,i}^{} _{V}^{(t)}_{O}|\}=o(1)\), and further get \(1/2-o(1)-_{n}^{(t)} 1/2+o(1)\). Therefore, one can simplify the dynamics of the coefficients (e.g., \(\), \(\), \(\)) by plugging the tight bounds of \(-_{n}^{(t)}\). The following lemma provides some bounds for the dynamics of \(_{Q}\), \(_{K}\) and\(_{V}\) in Stage 2.

**Lemma 5.3** (Dynamics of QKV in Stage 2).: _Under Condition 4.1, if \(N^{2}=(1)\), then with probability at least \(1-d^{-1}\), there exist constant \(C\), \(T_{1}=^{-1}d_{h}^{-1}\|\|_{2}^{-2}\|_{O}\|_{2}^ {-2}\) and \(T_{2}=^{-1}\|\|_{2}^{-2}\|_{O}\|_{2}^{-2} \) such that:_

\[_{+}^{}_{V}^{(t)}_{O} C\| \|_{2}^{2}\|_{O}\|_{2}^{2}(t-T_{1}), _{+}^{}_{V}^{(t)}_{O} 3M|_{n,i}^{ }_{V}^{(T)}_{O}|,\] \[_{-}^{}_{V}^{(t)}_{O}- C\| \|_{2}^{2}\|_{O}\|_{2}^{2}(t-T_{1}),_{-}^{}_{V}^{ (t)}_{O}-3M|_{n,i}^{}_{V}^{(T)}_{O}|,\]

\[_{}^{(t)},_{}^{(t)}-_{}^{(t)},_{n,j}^{(t)}(_{}^{(T_{1}) },_{}^{(T_{1})}-_{}^{(T_{1})},_{n,j}^{( T_{1})})+C|_{}^{}|_{O}|_{2}^{2} _{}^{2}}{N((6N^{2}M^{2}/))^{2}}(t-T_{1})(t -T_{1}-1)\]

\[_{n,i}^{(t)},_{}^{(t)}-_{n,i}^{(t)},_{n,j}^{(t)}(_{n,i}^{(T_{1}) },_{}^{(T_{1})}-_{n,i}^{(T_{1})},_{n,j}^{( T_{1})})+C|_{}^{}|_{O}|_{2}^{2} _{}^{2}}{N((6N^{2}M^{2}/))^{2}}(t-T_{1})(t -T_{1}-1)\]

_for \(i,j[M]\{1\},n[N],t[T_{1},T_{2}]\)._

Lemma 5.3 presents the training dynamics of \(_{Q}\), \(_{K}\) and \(_{V}\) under benign overfitting regime in two aspects: _direction_ and _speed_, that is,

* **direction:**\(_{V}_{O}\) prefer to learn the signals rather than memorize the noises; more and more _attention_ is paid to signals, while the _attention_ on noises is decreasing.
* **speed:** The inner products of \(_{}\) and \(_{V}_{O}\) grow at a linear rate; the inner products of label-related _vectorized Q & K_ grow at a logarithmic rate.

Stage 3:As the training process going on, the loss begins to converge, and the loss derivatives \(-_{n}^{(t)}\) no longer remain near \(1/2\). Therefore, the increasing rate of the inner products of _vectorize Q & K_ and \(_{}^{}_{V}_{O}\) begins to diminish. Based on the analysis in Stage 2, the _attention_ is sufficiently sparse at the beginning of Stage 3. In other words, the _attention_ on the signals is nearly 1, while the _attention_ on the noises is nearly 0. According to this sparsity, \(_{}^{}_{V}^{(t)}_{O}\) will grow much faster than \(_{n,i}^{}_{V}^{(t)}_{O}\) by Lemma 5.1, and Lemma 5.4 provides the lower bound of \(_{}^{}_{V}^{(t)}_{O}\).

**Lemma 5.4**.: _Under the same conditions as Theorem 4.1, there exists \(T_{3}=^{-1}^{-1}\|\|_{2}^{-2}\|_{O}\|_{2} ^{-2}(6N^{2}M^{2}/)^{-1}\) such that:_

\[_{+}^{}_{V}^{(t)}_{O}(_{+}^{ }_{V}^{(T_{2})}_{O})+ C\|\|_{2}^{2}\|_{O}\|_{2 }^{2}(t-T_{2}),\]

\[_{-}^{}_{V}^{(t)}_{O}-(-_{-}^{ }_{V}^{(T_{2})}_{O})+ C\|\|_{2}^{2}\|_{O}\|_{2 }^{2}(t-T_{2})\]

_for \(t[T_{2},T_{3}]\), where \(C\) is a constant, \(T_{2}\) is the last iteration of stage 2._

Convergence:Lemma 5.4 provides logarithmic lower bounds for \(_{}^{}_{V}^{(t)}_{O}\). Plugging \(t=T_{3}\) into these inequality, we have \(|_{}^{}_{V}^{(t)}_{O}|(1/ )\), then as long as \(|_{n,i}^{}_{V}^{(t)}_{O}|\) is sufficiently small, it can be proved that \(y_{n}f(_{n},(t))1/\) for all \(n[N]\), thus \((y_{n}f(_{n},(t)))=(1+(-(1/)))\) and \(L_{S}((t))\). The convergence of ViT is accordingly obtained.

Generalization:Consider a new data point \((,y)\) generated from the distribution defined in Definition 3.1. Without loss of generality, we suppose that the signal token is \(_{+}\) and the label is 1, i.e. \(=(_{+},_{2},,_{M})\), \(y=1\). It is clear that \(_{i}^{}_{Q}_{K}^{}_{}\) has a mean zero, which implies that the _attention_ on the signals in the test data may not necessarily be as high as that in the training data. However, the sparsity of _attention_ during training process creates conditions for \(_{V}_{O}\) to utilize the signals to make predictions, facilitating the generalization of the ViT. We provide the lower bound for the output of ViT on the unseen data with high probability as follows:

**Lemma 5.5**.: _Under the same conditions as Theorem 4.1, there exists \(T_{3}=^{-1}^{-1}\|\|_{2}^{-2}\|_{O}\|_{2 }^{-2}(6N^{2}M^{2}/)^{-1}\), with probability at least \(1-/N^{2}M\),_

\[yf(,(T_{3}))}-1,\]

where \(C\) and \(C^{}\) are constants. Lemma 5.5 shows that \(yf(,(T_{3}))\) is large with high probability, and through careful calculation, we can provide a small bound for test loss. More details are in Appendix D.4.

## 6 Experimental Verification

We present simulation results on synthetic data and MNIST dataset to verify our theoretical results.

**Synthetic data experiments setting:** We follow Definition 3.1 to generate the training set and test set. Specifically, we set token size \(M=16\) and feature dimension \(d=1024\). Without loss of generality, we set \(_{+}=\|\|_{2}[1,0,,0]^{}\) and \(_{-}=\|\|_{2}[0,1,0,,0]^{}\). We generate noise vector \(_{2}\) from the Gaussian distribution \((0,_{p}^{2})\), where \(_{p}\) is fixed to 4. Similarly, we generate the other noise vectors \(_{i}\) for \(i[M]/\{1,2\}\) from the Gaussian distribution \((0,_{p}^{2})\) where \(_{p}\) is fixed to 0.2.

We consider a two-layer Transformer defined in Section 3. The dimensions of matrix \(_{Q}\), \(_{K}\) and \(_{V}\) are set as \(d_{h}=d_{v}=512\). The ViT parameters are initialized using PyTorch's default initialization method, and then they are divided by 16 to ensure that the weights are initialized small enough. We train the ViT with full-batch gradient descent and learning rate \(=0.1\), target training loss \(=0.01\). We consider different traning sample size \(N\) ranging from 2 to 20, and different signal-to noise-ratio \(\) ranging from 0.16 to 15.6. We evaluate the test loss with 100 test data points after training loss converges to \(\). All experiments are performed on an NVIDIA A100 GPU.

**Synthetic data experiments results:** Figure 0(a) shows that as \(N\) and \(\) increase, the test loss tends to decrease. As Figure 0(b) shows, the theoretically derived red curve (\(N^{2}=1000\)

Figure 1: (a) is a heatmap of test loss on synthetic data across various signal-to-noise ratios (\(\)) and sample sizes (N). High test losses are indicated with yellow, while low test losses are indicated with purple. (b) is a heatmap that applies a cutoff value 0.2. It categorizes values below 0.2 as 0 (purple), and above 0.2 as 1 (yellow). The expression for the red curves in (a) and (b) is \(N^{2}=1000\).

almost follows the experimental dividing line between the yellow area (test loss > 0.2) and the purple area (test loss < 0.2). These experimental results further validate our theoretical results, that is, the sharp condition separation between benign and harmful overfitting, where \(N^{2}=(1)\) is a precise condition for benign overfitting and \(N^{-1}^{-2}=(1)\) is a precise condition for harmful overfitting. More experimental results on the dynamics of QKV can be found in Appendix A.

**MNIST experiments setting:** We add Gaussian random noises to the outer regions of the images with a width of 4. The original images and the noises are multiplied by diferent factors to generate diferent dataset with specifc SNR. For example, in Figure 2 (a), we mutiply the original image by 1/6 and the noises by 5/6, thus SNR=0.2; in Figure 2 (b),we mutiply the original image by 2/3 and the noises by 1/3, thus SNR=2.0. We consider a ViT model that consists of two attention layers, each equipped with four self-attention heads, followed by a MLP with ReLU activation. The training sample size N ranges from 3000 to 8400 and the SNR ranges from 0.2 to 2.0.

**MNIST experiments results:** Figure 2 (c) shows the heatmap of test loss, which shows a transition between benign and harmful overfitting regimes. The larger the sample size N and signal-to-noise ratio SNR, the better the generalization performance.

## 7 Conclusion

This paper studies the training dynamics, convergence, and generalization for a two-layer Transformer in vision. By analyzing the _Vectorized Q & K_ and _scalarized V_ in three-stage decomposition and carefully handling the _softmax_ function, we give the precise increasing rate of the _attention_ and output of the Transformer in the training process, and further analyze its generalization performance. Our theoretical results reveal a sharp condition separation between benign and harmful overfitting. One limitation of our methodology is our proof technique relies heavily on the sparsity of feature strength, e.g., we assume the standard deviation \(_{p}\) is sufficiently larger than \(_{p}\), ensuring that more _attention_ is paid to \(_{2}\) rather than other tokens. One future direction is to generalize our analysis to study other abilities of Transformers, such as in-context learning, prompt-tuning and time series forecasting.