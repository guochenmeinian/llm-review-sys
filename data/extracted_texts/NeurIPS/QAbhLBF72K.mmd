# What makes unlearning hard and what to do about it

Kairan Zhao

University of Warwick

&Meghdad Kurmanji

University of Cambridge

&George-Octavian Barbulescu

University of Warwick

&Eleni Triantafillou

Google DeepMind

&Peter Triantafillou

University of Warwick

Correspondence to Kairan.Zhao@warwick.ac.uk

Equal senior contribution

Code is available at: https://github.com/kairanzhao/RUM

###### Abstract

Machine unlearning is the problem of removing the effect of a subset of training data (the "forget set") from a trained model e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data. With unlearning research still being at its infancy, many fundamental open questions exist: Are there _interpretable_ characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms? We present the first investigation into these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Our evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets. Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) _refining_ the forget set into homogenized subsets, according to different characteristics; and (ii) a _meta-algorithm_ that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in deepening our scientific understanding of unlearning and revealing new pathways to improving the state-of-the-art. ++

Footnote â€ : Code is available at: https://github.com/kairanzhao/RUM

## 1 Introduction

Deep learning models have generated impressive success stories recently by leveraging increasingly large and data-hungry neural networks that are also increasingly expensive to train. This trend has led to reusing previously-trained models for a wide range of tasks more than ever before. However, the heavy reliance of deep models on training data, together with the difficulty of removing data from trained models after-the-fact, has exacerbated concerns on perpetuating harmful or outdated information, violating user privacy and other issues. Specifically, deep networks are highly non-convex, making it difficult to trace (and thus attempt to remove) the effect of a given subset of training data on the model weights. We are therefore faced with important technical challenges when it comes to building machine learning pipelines that are performant while efficiently supporting deletion requests. Machine unlearning  is a growing field that aims to address this important issue.

While unlearning is receiving increasing attention , it is still a young area of research and the factors affecting the success of different approaches remain poorly-understood. Understandingwhat makes an unlearning problem easy or hard is crucial for several reasons. First, knowledge of behaviours of unlearning algorithms on different types of forgetting requests may inform which unlearning method to choose for a given request. In fact, for some requests it may be that all current methods are inadequate, suggesting that one should pay the cost of retraining from scratch rather than opting for "approximate unlearning" that imperfectly removes information after-the-fact. Further, deepening our understanding of unlearning can illuminate pathways for improving both unlearning algorithms as well as evaluation protocols by focusing on relevant factors that affect difficulty.

To this end, we present the first investigation into different factors that characterize the difficulty of an unlearning problem. We find that the unlearning problem becomes harder i) the more entangled the retain and forget sets are and ii) the more memorized the forget set examples are. Our investigation reveals that different unlearning algorithms suffer disproportionately as the difficulty level increases and surfaces previously-unknown behaviours and failure modes of state-of-the-art unlearning algorithms. Inspired by our findings, we propose a Refined-Unlearning Meta-algorithm (RUM) for improving unlearning pipelines. RUM contains two steps: i) a refinement procedure that divides the given forget set into subsets that are homogeneous with respect to relevant factors that influence algorithms' behaviours, and ii) a meta-algorithm that dictates how to unlearn each of those subsets and compose the resulting models to arrive at one that has unlearned the entire forget set. Our thorough investigation shows that RUM boosts unlearning performance of several state-of-the-art algorithms and addresses issues that our investigation of unlearning difficulty has uncovered.

## 2 Preliminaries

### Unlearning problem formulation

Let \(^{o}=(_{train})\) be the weights obtained by applying a training algorithm \(\) on a training dataset \(_{train}\). We will refer to \(^{o}\) as the "original model". Further, let \(_{train}\) denote a subset of the training data referred to as the "forget set". For convenience, we will refer to its complement as the "retain set" \(=_{train}\). Informally, the goal of an unlearning algorithm \(\) is to utilize \(^{o}\), \(\) and \(\) to produce an unlearned \(^{u}=(^{o},,)\) from which the influence of \(\) is removed.

This idea has been formalized by considering the distributional similarity between the model \(^{u}\) produced by \(\) and the model \(^{r}\) produced by the optimal unlearning approach: retraining from scratch on an adjusted training dataset that excludes the forget set: \(^{r}=(_{train} S)\). Note that we refer to distributions here since rerunning \(\) and \(\) with different random seeds (that control e.g. the initialization and the order of mini-batches) will lead to slightly different weights. The ideal unlearning algorithm then, according to this viewpoint, is one that yields the same distribution of weights as retraining from scratch. Of course, for an unlearning algorithm to be practical, we would additionally desire it to be significantly more computationally efficient than retraining the model.

The following definition, borrowed from  (and similar to ) formalizes this idea in a framework inspired by differential privacy .

**Definition 2.1**.: **Unlearning.** An unlearning algorithm \(\) is an \((,)\)_-unlearner_ (for \(\), \(_{train}\) and \(\)) if the distributions of \((_{train} S)\) and \((^{o},,_{train} S)\) are \((,)\)-close.

where we say two distributions \(,\) are \((,)\)_-close_ if \((B) e^{}(B)+\) and \((B) e^{}(B)+\) for all measurable events \(B\).

According to the above definition, an unlearning algorithm is said to be _exact unlearning_ if it satisfies the above definition for \(==0\), i.e., it yields a distribution of models identical to that of retraining from scratch. For neural networks, the only known exact solutions involve retraining, either naively, or in the context of mixtures where one can retrain only a subset of models affected by the deletion request . These approaches unfortunately are inefficient; in the worst-case, even clever schemes suffer inefficiency similar to naive retraining, and may also yield poorer performance. To address this, a plethora of _approximate unlearning_ algorithms have been recently proposed, whose \(\) and \(\) values aren't known in general, but are substantially more efficient and may have higher utility.

Evaluating approximate unlearning.Since the success of (most) approximate unlearning algorithms cannot be proved within tight \((,)\) bounds, the community has considered various empirical measurements of success, guided by three desiderata: i) good forgetting quality, 2) high utility, and 3) efficiency. An unlearning algorithm thus is faced with a complex balancing act, as there are well-known trade-offs both between forgetting quality and utility, as well as forgetting quality and efficiency, and a good unlearning metric should capture these nuances.

Utility and efficiency are straightforward to measure, and, in the context of classifiers, can be represented by the accuracy on the retain and test sets, and time in seconds, respectively. Measuring forgetting quality, on the other hand, is more complex and several proxies have been proposed. The simplest one is to inspect the accuracy on the forget set, with the goal of matching the accuracy on the forget set that would have been obtained by retraining from scratch. Alternatively, inspired from privacy literature [4; 28], _Membership Inference Attacks (MIAs)_ have been adopted by the unlearning community [25; 26; 20] to measure forgetting quality. In essence, an MIA is designed to infer from the model's characteristics (e.g. loss, confidence) whether a data point has been used in training, and then unlearned, versus was never trained on in the first place. Intuitively, the failure of an attacker to tell apart unlearned examples from never-seen examples marks a success for the unlearning algorithm in terms of this metric. We will consider both of these proxies in our experimental investigation.

To holistically evaluate an unlearning algorithm, we desire a single metric that captures both forgetting quality and utility. We will later introduce a "tug-of-war" metric for this purpose, inspired by .

### Memorization

Deep neural networks are known to "memorize" (a subset of) their training data, with a recent theory showing that label memorization is in fact necessary for achieving close-to-optimal generalization error in classifiers  when the data distribution is long-tailed.

**Definition 2.2**.: **Memorization score .** The _memorization score_ for an example \(i\), with respect to a training dataset \(\) and training algorithm \(\) is

\[(,,i)=_{f()}[f( x_{i})=y_{i}]\ -_{f( i)}[f(x_{i})=y_{i}]\] (1)

where \(x_{i}\) and \(y_{i}\) are the feature and label, respectively, of example \(i\).

The first term in the above equation considers models trained on all of \(\) whereas the second term considers models trained on \(\) excluding example \(i\). Intuitively, the memorization score for an example \(i\) is high if including it in training yields a different distribution of predictions on that example than excluding it from training would have. Recent works [11; 12; 23] identify atypical examples or outliers of the data distribution as examples that are more highly memorized: if an example has a noisy or incorrect label, the model is required to memorize it in order to predict it correctly.

## 3 Related Work

Approximate unlearning algorithms.A plethora of algorithms have been proposed that aim to identify effective data scrubbing procedures post-training. We now describe representative methods.

**Fine-tune**[36; 15] relies on catastrophic forgetting to diminish the confidence of the original model \(^{o}\) on \(\). Catastrophic forgetting is induced by simply fine-tuning on the retain set \(_{train}\). On the other hand, **NegGrad**[15; 17; 31] instead directly maximizes the loss on \(\). This approach has been found empirically to cause a large drop in the utility of the model. To address this, **NegGrad+** combines fine-tuning and gradient ascent, by jointly minimizing the loss function on the retain set, and maximizing the loss function with respect to the forget set. **SCRUB**, proposed by the same authors as NegGrad+, extends the contrastive learning behind NegGrad+ by framing it as a student-teacher problem. Concretely, SCRUB is a bi-optimization algorithm, where the student aims to mimic the teacher's behaviour on \(\) and to disobey the teacher's output with respect to \(\). **L1-sparse** infuses weight "sparsity" into the unlearning algorithm by fine-tuning on the retain set with an L1-penalty, drawing inspiration from the model pruning literature [13; 27]. **Influence Unlearning**[21; 24] arrives at the important model's weights by estimating how removing a data point affects \(^{o}\) via influence functions , and draws connections to \((,)\)_-forgetting_[35; 19].

A different line of work is "relabelling-based" methods that trick the model to learn new labels for \(S\). This can be achieved by finetuning the model with respect to a dataset \(_{relabel}=(X_{},Y)\), where \(X_{}\) are the features and labels \(Y\) are sampled from a prior distribution of the label space. **Saliency Unlearning (SalUn)** learns \(_{relabel}\) by optimising only the _salient_ parameters of the model.

Concretely, the authors argue that the model's weights \(^{o}\) can be decomposed into salient weights and "intact" model weights, by investigating the weight space with respect to the forget set \(S\) ala [30; 1].

Difficulty of Unlearning.The closest research to ours is the contemporaneous work of , where the authors study _adversarial unlearning_ cases, i.e. "worst-case" forget sets.  arrives at difficult forget sets by solving a bi-level optimization based on fine-tuning (i.e. catastrophic forgetting). Instead, we arrive at difficult partitions through the lens of _interpretable_ factors: the degree of entanglement between the retain and forget set and memorization. While the primary aim of  is to construct more pessimistic evaluation benchmarks, our primary aim is to deepen our understanding of unlearning problems and of the behaviour of state-of-the-art algorithms when operating on forget set of different identified characteristics, ultimately improving unlearning pipelines.

Catastrophic forgetting, atypical examples and privacy. and  study catastrophic forgetting during training.  finds that, when training on large datasets, examples that were only seen early in training may enjoy better privacy, in terms of MIAs and extraction attacks, compared to examples seen recently.  investigate "forgetting events", where an example that was previously correctly predicted becomes incorrectly predicted later in training. They find that examples with noisy labels witness a larger number of these forgetting events. Further,  find that models trained with Differential Privacy (DP) find it primarily hard to correctly predict atypical examples. We build on this literature by studying the difficulty of unlearning after-the-fact, rather than (passive) forgetting during training and draw connections to memorization, a notion closely related to atypicality in the data distribution.

## 4 What Makes Unlearning Hard?

In this section, we identify and empirically examine two factors that affect the difficulty of unlearning. Before diving in, we first define a simple proxy for unlearning difficulty that we will use in this section. Our goal is to capture the difficulty of performing the "balancing act" of forgetting \(\) while retaining the ability to perform well on \(\) and generalize well to the test set. We propose a metric to capture this "tug-of-war" (ToW) using the relative difference between the accuracies of the unlearned and the retrained model on the forget, retain and test sets, in a manner inspired by .

\[(^{u},^{r},,,_{test}) =(1-(^{u},^{r},))(1-( ^{u},^{r},))(1-(^{u},^{r}, _{test}))\]

where \((,)=|}_{(x,y) }[f(x;)=y]\) is the accuracy on \(\) of a model \(f\) parameterized by \(\) and \((^{u},^{r},)=|(^{u},)-(^{r},)|\) is the absolute difference between the accuracy of models \(^{u}\) and \(^{r}\) on \(\). Therefore, ToW rewards unlearned models that match the accuracy of the retrained-from-scratch model, on each of the forget, retain, and test sets. ToW ranges from 0 to 1, with higher values associated with better unlearning.

### The more entangled the forget and retain sets are, the harder unlearning becomes

Prior research (e.g., by Feldman et al  and Carlini et al ) has tried to identify prototypical (or atypical) examples and their impact on learning. Primarily, this depended on the position of _examples_ within the overall _data space distribution_. In contrast, here we focus on the _embedding space_, as unlearning depends heavily on how the model has learned to represent training data. Furthermore, instead of looking at isolated examples, we delve into how "entangled" the retain and forget sets are in embedding space. We hypothesize that higher "entanglement" leads to harder unlearning: if the two sets are highly entangled, attempting to erase \(\) will cause accidentally erasing \(\) too.

We propose to measure entanglement between the retain and forget sets via the below _Entanglement Score_ (ES), inspired by a measure previously introduced in  to study learned representations.

\[(,;^{o})=|} _{i}(_{i}-_{})^{2}+|} _{j}(_{j}-_{})^{2}}{( _{}-)^{2}+(_{}-)^{2}}\] (2)

where \(_{i}=g(x_{i};^{o})\) is the embedding of example \(x_{i}\) according to the "original model" \(f\), parameterized by \(^{o}\); where \(g\) denotes the forward pass through \(f\) up till the penultimate layer, i.e. excluding the classifier layer. Further, \(_{}=|}_{i}_{i}\) is the mean embedding of the retain set, and analogously, \(_{}\) the mean embedding of the forget set, while \(\) is the mean embedding over all of \(_{train}=\).

Intuitively, ES measures entanglement between \(\) and \(\) in the embedding space of the original model (before unlearning begins). The numerator measures intra-class variance, capturing the tightness of each of those two sets, independently, while the denominator measures inter-class variance between those two sets. Higher ES score corresponds to higher entanglement in the embedding space.

Our investigation hinges on generating three different forget/retain partitions with different degrees of entanglement: low, medium, and high. But, Equation (2) does not directly suggest a procedure for generating retain/forget partitions with a desired ES score. Hence, we achieved this indirectly using a proxy. Specifically, let \(d(i,;^{o})=||_{i}-||^{2}\) denote the l2-distance in the original model's embedding space between example \(i\) and centroid \(\), as defined above. We compute this distance for each example in \(_{train}\) and sort those examples according to their \(d\)-values. We then form each forget set to contain a contiguous subset of examples from different ranges of that sorted list. We find that this procedure allows us to construct retain/forget partitions of varying ES values. The ES values for our low, medium and high partitions are 309.94\(\)98.56, 1076.99\(\)78.64, 1612.21\(\)110.82 for CIFAR-10, and 963.82\(\)113.53, 2831.24\(\)558.63, and 3876.90\(\)426.92 for CIFAR-100. We include details of this procedure in the Section A.3, along with visualizations and Maximum Mean Discrepancy (MMD) analysis to further validate ES, confirming that the degree of retain/forget entanglement aligns with the computed scores. We experiment with four dataset/architecture settings: CIFAR-10/ResNet-18, CIFAR-100/ResNet-50, Tiny-ImageNet/ResNet-18 and Tiny-ImageNet/VGG-16, using \(|S|=3000\). Refer to Section A.2 for implementation details and Section A.8 for more detailed results on all datasets.

We observe from Figure 0(a) that it is harder to unlearn when the retain and forget sets are more entangled: all unlearning algorithms have poorer performance for highly-entangled vs lower-entangled settings. Further, we notice that different unlearning algorithms suffer disproportionately as the entanglement increases. Notably, methods based on relabelling (SalUn and Random-label) perform very poorly when the entanglement is high. We hypothesize that this is because, if two examples \(i\) and \(j\) are close neighbours in embedding space, with \(i\) in the forget set and \(j\) in the retain set, forcing example \(i\) to be confidently predicted as an incorrect class (as relabelling algorithms do) will also cause \(j\) to be predicted as that incorrect class, too, thus causing a drop in retain accuracy, which is captured by ToW. This effect will be less pronounced if \(i\) and \(j\) are far from each other.

### The more memorized the forget examples are, the harder unlearning becomes

Feldman et al  have already established that models must memorize some atypical examples in order to perform well. Further, prior literature has also established that noisy examples (that are more likely to be memorized) witness more "forgetting events" during training (their predicted label flips to an incorrect one)  and that models trained with Differential Privacy (DP), a procedure where noise is added to the gradients (making it harder to memorize), find it primarily hard to correctly predict atypical examples . In this section, we build upon these prior insights by investigating the connection between the degree of memorization of the forget set and difficulty of unlearning.

Figure 1: Uncovering two factors that affect unlearning difficulty according to ToW (where higher is better). **Left:** the more entangled the retain and forget sets are in the embedding space, the harder it is to unlearn. **Right:** the less memorized a forget set is (thus having influenced the model less), the easier it is to unlearn (for most algorithms). Error bars correspond to 95% confidence intervals from running each algorithm 3 times (6 times for relabelling-based that had higher variance).

Let's begin by inspecting Definition 2.2: if an example is not really memorized, the predictions of the model on that example will not change much whether the example was included in training or not. This implies that even the original model (no unlearning) is similar to retrain-from-scratch in terms of predictions on those examples, making unlearning unnecessary or trivial. On the other hand, for highly-memorized examples, the predictions between the original and retrained models will differ significantly, implying that an unlearning algorithm has "more work" to do to turn the original model into one that resembles the retrained one. We now investigate how the level of memorization of the forget set affects the behaviour of state-of-the-art unlearning algorithms. We hypothesize, based on our above intuition, that unlearning is easier when the forget set contains less-memorized examples.

To investigate this, we first compute the memorization score \((,_{train},i)\) of each example \(i_{train}\) and we sort all examples according to their scores. We then use that sorted list to create three different forget sets, corresponding to the lowest \(N\) scores ("low-mem"), the highest \(N\) ("high-mem"), and the \(N\) that are nearest to 0.5, i.e. the midpoint of the range of memorization scores ("medium-mem"), where \(N=3000\). We then apply different unlearning algorithms on each of these forget sets and compute ToW. We perform this experiment on CIFAR-10 using ResNet-18 and on CIFAR-100 using ResNet-50. Refer to Section A.2 for implementation details.

We first emphasize two key sets of conclusions. First, in terms of ToW, Figure 0(b) shows that, indeed, for most algorithms, the lower the memorization level of the forget set, the easier the problem is. In line with our prior discussion, even the original model performs well on "low-mem", but performs very poorly on "high-mem". Interestingly though, the two relabelling-based algorithms (SalUn and Random-label) follow an inverse trend: they perform better for higher-memorized forget sets. Second, breaking down ToW into its parts, we find interesting trends in terms of the forget set accuracy, in Figure 12. Specifically, for several unlearning algorithms, the forget accuracy for "low-mem" is still very high after unlearning them, as the model can infer the correct labels for such examples even when they weren't included in training; this follows directly by Definition 2.2 if unlearning is done by retraining, and is shown here for the first time for approximate unlearning algorithms. On the other hand, we find that, for "high-mem", different unlearning algorithms can (to varying degrees) cause the forget set accuracy to drop substantially; this is consistent with both  and , but shown here for the first time for approximate unlearning algorithms. Notably, we find that relabelling-based algorithms cause a larger drop in the accuracy of the forget set, relative to other approaches. This benefits ToW in the case of "high-mem" forget sets, where retraining has poor accuracy on this set (so they get rewarded by matching it), but it hurts on "low-mem", since it causes a large discrepancy from retraining, which has high accuracy on this set (since it makes similar predictions to the original model on this set, by definition, and the original model has high accuracy on all of \(_{train}\)).

Overall, we have presented the first investigation into the behaviour of unlearning algorithms applied on forget sets of different degrees of memorization. A key finding is that different algorithms outshine others for different forget sets. Most notable is the failure of relabelling-based algorithms on the "low-mem" forget set, which is easy for other algorithms and, in fact, even no unlearning in that case might be an acceptable solution. We intuit that this is due to their aggressive unlearning strategy yielding "over forgetting" (producing a forget set accuracy that is lower than that of retraining from scratch) as discussed above. Furthermore, we observe that different unlearning algorithms work best for the "low-mem", "medium-mem" and "high-mem" forget sets. Concretely, from Figure 0(b) we note that Finetune is best for "medium-mem", SalUn is best for "high-mem", and a number of algorithms are top-performers for "low-mem" (including no unlearning). This reveals a possible pathway for improving unlearning based on using different algorithms for different forget sets. So, how can one build on these insights to further improve unlearning algorithms performance?

## 5 Refined-Unlearning Meta-algorithm (RUM) for Improved Unlearning

Previously, we observed that unlearning algorithms have different behaviours on forget sets with different properties. For example, while "low mem" forget sets are almost trivial to unlearn (and even doing nothing may be acceptable), SalUn and Random-label perform poorly on them. On the other hand, SalUn and Random-label evidently outperform other unlearning algorithms on "high mem". These observations suggest that the optimal unlearning algorithm to use is dependent on the properties of the forget set. One could therefore pick the best unlearning algorithm for each unlearning request, based on these factors. However, in practical scenarios, forget sets may be distributed differently than in our preliminary experiments, that were designed to cleanly separate different factors of interest.

Indeed, real-world forget sets would likely contain a mixture of examples from different modes of the data distribution, some rare or highly-memorized while others common and not memorized at all. So, what can be done about these expected heterogeneous forget sets? How can our insights above be leveraged to improve unlearning for such cases?

To address this, we first propose a _refinement procedure_ that divides forget sets into homogeneous subsets (with respect to the factors that we have found to affect the difficulty of unlearning and behaviours of existing algorithms). Second, we propose to utilize a pool of state-of-the-art algorithms to unlearn different subsets. Put together, we propose a Refined-Unlearning Meta-algorithm (RUM), comprised of two steps: 1) Refinement and 2) Meta-unlearning. Figure 2 overviews RUM.

Step 1: Refinement.We introduce a function \(\) that partitions the forget set \(S\) into \(K\) subsets: \(\{S_{i}\}_{i=1}^{K}=()\) such that each forget set example appears in exactly one such subset. The intention of \(\) is to generate homogeneous subsets w.r.t factor(s) that affect difficulty / algorithm's behaviours.

Step 2: Meta-Unlearning.Having obtained the subsets \(\{S_{i}\}_{i=1}^{K}\) of \(\), we now require a "meta-algorithm" \(\) that dictates how to perform the individual unlearning requests and how to compose the resulting unlearned models to arrive to a model that has unlearned all of \(\). In this work, we focus on meta-algorithms that tackle unlearning of subsets in a sequence, leaving other designs for future work. It therefore remains for our "meta-algorithm" to decide: i) what unlearning algorithm to apply for each subset, ii) what order should the unlearning requests be executed in.

More concretely, we assume access to a pool of existing unlearning algorithms \(_{1}_{N}\), like the ones described in related work, for instance. Let \(^{}(S_{i})\) denote a procedure that takes as input a subset \(S_{i}\) and returns an unlearning algorithm \(\{_{1}_{N}\}\) that will be used for that subset. This selection can be done by leveraging insights such as those in Section 4. Further, let \(^{}\) denote a procedure that takes as input the \(K\) subsets of \(\) and returns a sorted list \(S^{}=^{}(())\) containing the K subsets in the desired order of execution.

Given the above ingredients, RUM proceeds by executing \(K\) unlearning requests in a sequence, with step \(i\) of that sequence corresponding to unlearning subset \(S^{}[i]\) by applying \(_{i}(^{o},S^{}[i],_{i})=^{u}_{i}\), where \(^{u}_{i}\) denotes the unlearned model up to step \(i\) and \(_{i}=^{}(S^{}[i])\) and \(_{i}=\{S^{}[i+1], S^{}[K]\}\) is the retain set for step \(i\), containing \(\) as well as all other subsets of \(\) that have not yet been unlearned in the sequence so far. We finally return the unlearned model of the last step \(^{u}_{K}\).

Our RUM framework is meant as an analysis framework, surfacing new problems to be solved and offering new pathways into future state-of-the-art algorithms. Nonetheless, we contribute below specific top-performing RUM instantiations, with specific choices for \(\) and for \(\).

## 6 Experimenting with RUM flavours

We now present RUM instantiations using a refinement strategy based on memorization scores and experimental evaluations answering the following questions: **Q1**: How useful is refinement alone? That is, for a given unlearning algorithm \(\), does applying \(\) sequentially on the \(K\) homogeneous subsets of \(\) outperform applying \(\) once on all of \(\)? **Q2**: Can we obtain further gains by additionally selecting the best-performing unlearning algorithm for each forget set subset? **Q3**: Are there interpetable factors behind the boost obtained by sequential unlearning of homogeneous subsets?

Experimental setupWe experiment with a refinement strategy based on memorization scores where \(K\) = 3. Specifically, we study unlearning a forget set \(\) that is the union of the three sets containing the \(N\) lowest, the \(N\) closest to 0.5, and the \(N\) highest memorized examples in the dataset,

Figure 2: Overview of RUM.

[MISSING_PAGE_FAIL:8]

mem", "medium-mem" and "high-mem" scenarios. On CIFAR-10, this corresponds to doing nothing for low-mem (i.e. using the original model directly), using Fine-tune for medium-mem and SalUn for high-mem (based on Figure 0(b)). From Figure 2(b) (and Table 0(a)), we observe that we get the overall best results by far, both in terms of ToW and MIA, by applying RUM with "nothing \(\) Fine-tune \(\) SalUn", demonstrating the value of incorporating our insights into unlearning pipelines. In fact, lets revisit our previous observation that SalUn and Random-Label perform uncharacteristically poorly on the "low-mem" forget set. In line with our hypothesis, we notice that "nothing \(\) SalUn \(\) SalUn" outperforms applying SalUn on all three subsets (a.k.a. SalUn \(^{}\)). Note that, for CIFAR-100, the best algorithm for all subsets is NegGrad+, so full RUM corresponds to NegGrad+ \(^{}\).

Can we obtain similar performance boosts with compute-efficient proxies?Given the computational cost of calculating memorization scores, we aim to find a more efficient proxy to enable practical deployment of RUM. We discuss a confidence-based memorization metric, termed "C-proxy", as a proxy for memorization . Section A.6 provides a detailed explanation of the proxy and the complete results across various datasets and architectures. Figure 3(a) shows that the observed unlearning difficulty pattern--specifically, that forget examples with higher memorization scores (i.e., lower C-proxy values, as they are negatively correlated; see Table 4) are harder to unlearn--remains consistent when using C-proxy, paralleling the results in Figure 0(b). This pattern is further confirmed on Tiny-ImageNet with both ResNet-18 and the VGG-16 architectures, as shown in Figure 9. We then examine whether using C-proxy achieves similar performance gains within RUM as the original memorization score. Figure 3(b) presents results on CIFAR-10 with ResNet-18, using C-proxy in place of memorization score during the refinement step. This analysis is also extended to Tiny-ImageNet with both ResNet-18 and VGG-16 architectures, as shown in Figure 10 and Table 17. Together, these results suggest that C-proxy is a practical and compute-efficient alternative, delivering significant performance gains comparable to those achieved with the memorization score. Detailed analysis of the use and appropriateness of various memorization proxies and their effect on RUM can be found in .

Analysis of sequence dynamicsWe report ToW and MIA with different orderings in Tables 0(a) and 0(b). We find that, while ToW is similar for different orderings, MIA can vary. To better understand these dynamics, we inspect the accuracies on \(\) and its subsets after each step in Figure 5 for SalUn\({}^{}\) on CIFAR-10 and in Figure 13 for NegGrad+\({}^{}\) CIFAR-100. The former reveals why SalUn\({}^{}\) with low \(\) med \(\) high order greatly outperforms vanilla SalUn. Recall that we identified in Section 4.2 that SalUn "overforgets" low-mem examples (its forget accuracy is lower than that of retraining). We observe from Figure 5 that future steps of the sequence neutralize that overforgetting effect on low-mem, leading to better ToW (see Figure 3). Interestingly, in line with our previous insights (Section 4.2), we find (from both Figures 5 and 13) that it is hard to cause the "low-mem" accuracy to become low and stay low. NegGrad+ does not drop it for any order of execution; SalUn drops it in the first ordering, but that drop is later reversed. We leave it to future work to further study these sequential dynamics and their helpful or harmful effects on Tow and MIA. The fact that MIA results differ based on the sequence may tie in with the recently-identified "privacy onion effect" .

Figure 4: Replacing mem scores with the efficient C-proxy yields similar trends and performance gains, carving a path for practical deployment of RUM. **Left:** forget sets with lower C-proxy values (i.e., higher mem scores, since C-proxy and memorization are negatively correlated) are harder to unlearn, consistent with the trend in Figure 0(b). **Right:**\(^{}\) using C-proxy in the refinement step enhances unlearning performance across algorithms, comparable to using the memorization score in Figure 2(a). Error bars correspond to 95% confidence intervals, with each algorithm run 3 times.

## 7 Discussion and conclusion

We presented the first investigation into interpretable factors that affect the difficulty of unlearning. We found that unlearning gets harder i) the more entangled the forget and retain sets are in embedding space and ii) the more memorized the forget set is. Our investigation led into uncovering previously-unknown behaviours of state-of-the-art algorithms that were not surfaced when considering random forget sets: when do they fail, when do they excel and when do they exhibit different trends from each other. Notably, we discovered that relabelling-based methods suffer disproportionately as the embedding-space entanglement increases, and exhibit a reverse trend compared to other methods in their behaviour for different memorization levels. Armed with these insights, we then proposed the RUM framework surfacing new (sub)problems within unlearning, whose solution may lead to greater performance. Finally, we derived specific instantiations of RUM and analyzed how its different components can improve performance. We found that sequential unlearning of homogenized forget set subsets improves all considered state-of-the-art unlearning algorithms and investigated the dynamics of sequential unlearning to glean insights as to why that is. We also found that we can further boost performance by selecting the best unlearning algorithm per subset.

EfficiencyHow is this important aspect affected in our sequential framework? We remark that it depends on the unlearning algorithm. For instance, applying Fine-tune three times is much more expensive than applying it once, because Fine-tune performs (at least) one epoch over the entire retain set. But for other algorithms the overall cost does not increase significantly. Further, a key observation from our results is that we can do well by actually _doing nothing_ on a subset of the forget set, which can really boost efficiency (especially since the vast majority of examples are "low mem"). Additionally, our results with the C-proxy demonstrate that significant performance improvements in unlearning can be achieved with minimal computational cost, avoiding the heavy expense of computing memorization scores.

Data-space vs embedding-space outliersHow does the embedding space entanglement interact with the level of memorization of the forget set? We analyzed this and found in Table 3 that all of our memorization buckets have relatively high ES, indicating that separating out (data-space) outliers in the forget set doesn't lead to lower entanglement between the two sets in the embedding space. We leave it to future work to study the interaction of these factors.

Limitations and future workWe hope future work explores other refinement strategies (e.g. for a notion that captures embedding-space entanglement) and investigates privacy implications of sequential RUM, e.g. in terms of the "privacy onion effect" . We also hope to see how our RUM framework can be adopted and adapted for unlearning in LLMs, especially given the findings from the contemporaneous paper  where unlearning is performed only on the highest-memorized examples in the forget set (albeit, memorization is defined differently for LLMs). We hope our framework continues to enable progress in understanding and improving unlearning and that our identified factors of difficulty and associated behaviours of existing algorithms continue to improve the state-of-the-art and inform the development of strong evaluation metrics that consider forget sets that vary in terms of relevant identified characteristics.

Acknowledgements

We thank Vincent Dumoulin and Fabian Pedregosa for valuable conversations and feedback at various stages of the project.