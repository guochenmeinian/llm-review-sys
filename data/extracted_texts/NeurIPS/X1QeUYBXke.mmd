# Gradient Guidance for Diffusion Models:

An Optimization Perspective

 Yingqing Guo  Hui Yuan  Yukang Yang  Minshuo Chen  Mengdi Wang

Equal contribution. Department of Electrical and Computer Engineering, Princeton University. Authors' emails are: {yg6736, huiyuan, yy1325, minshuochen, mengdiw}@princeton.edu.

###### Abstract

Diffusion models have demonstrated empirical successes in various applications and can be adapted to task-specific needs via guidance. This paper studies a form of gradient guidance for adapting a pre-trained diffusion model towards optimizing user-specified objectives. We establish a mathematical framework for guided diffusion to systematically study its optimization theory and algorithmic design. Our theoretical analysis spots a strong link between guided diffusion models and optimization: gradient-guided diffusion models are essentially sampling solutions to a regularized optimization problem, where the regularization is imposed by the pre-training data. As for guidance design, directly bringing in the gradient of an external objective function as guidance would _jeopardize_ the structure in generated samples. We investigate a modified form of gradient guidance based on a forward prediction loss, which leverages the information in pre-trained score functions and provably preserves the latent structure. We further consider an iteratively fine-tuned version of gradient-guided diffusion where guidance and score network are both updated with newly generated samples. This process mimics a first-order optimization iteration in expectation, for which we proved \(}(1/K)\) convergence rate to the global optimum when the objective function is concave. Our code is released at https://github.com/yukang123/GGDMOptim.git.

## 1 Introduction

Diffusion models have emerged as a significant advancement in the field of generative artificial intelligence, offering state-of-the-art performance in image generation . These models operate by gradually transforming a random noise into a structured output, utilizing the score function learned from data. One of the key advantages of diffusion models is their flexibility which allows controlled sample generation for task-specific interest, excelling diffusion models in a wide range of applications, such as content creation, sequential decision making, protein engineering .

Controlling the generation of large generative models stands at the forefront of AI. Guidance and fine-tuning are two most prevalent approaches for controlling the generation of diffusion models. Unlike fine-tuning which changes the weights of pre-trained models, guidance mechanism enables a more directed and flexible control. Adding gradient-based guidance during inference was pioneered by classifier guidance , which involves training a time-dependent classifier. Diffusion Posterior Sampling (DPS)  introduced a fully training-free form of gradient-based guidance, which removes the dependence on time. This method has since been explored in numerous empirical studies . However, despite these empirical successes, significant gapsremain in the theoretical understanding and guarantees of gradient-based guidance in diffusion models.

Problem and ChallengesSuppose we have a pre-trained diffusion model that can generate new samples faithfully, maintaining the latent structure of data. We study the problem of adapting this diffusion model to generate new samples that optimize task-specific objectives, while _maintaining the learned structure_ in new samples. This problem has a strong connection to classic optimization, guided diffusion offers new possibilities to optimize complex design variables such as images, videos, proteins, and genomes [7; 66; 42] in a generative fashion. More comprehensive exposure to this middle ground can be found in recent surveys [69; 12; 30].

Given the optimization nature of this problem, it's critical to answer the following theoretical questions from an optimization perspective: _(i)_ Why doesn't simply applying the gradient of the objective function w.r.t. the noised sample work? _(ii)_ How to add a guidance signal to improve the target objective without compromising the sample quality? _(iii)_ Can one guarantee the optimization properties of new samples generated by guided diffusion? _(iv)_ What are the limits of adaptability in these guided models?

Scope and Contribution.In this paper, we investigate guided diffusion from an optimization perspective. To answer the four questions above, we propose an algorithmic framework, see Figure 1 for an illustration. Our main contributions are summarized as follows:

\(\)**Study structure-preserving guidance.** We study the design of guidance under structural data distribution belonging to a latent low-dimensional subspace (Assumption 1). We diagnose the failure of naive gradient guidance and study the theoretical aspects of guidance based on forward prediction loss (Definition 1), which provably preserves any low-dimensional subspace structure (Theorem 1).

\(\)**Establish a mathematical framework of guided diffusion.** We build a mathematical framework for guided diffusion, which facilitates algorithm analysis and theory establishment. We propose and analyze an iterative guided diffusion using gradient queries on new samples (Algorithm 1; Figure 1 with fine-tuning block off). We give the first convergence theory showing generated samples converge to a regularized optimal solution w.r.t the objective (Theorem 4) with linear score class (12). The regularization is imposed by the pre-trained diffusion model, revealing a fundamental limit of adapting pre-trained diffusion models with guidance.

\(\)**Provide rate-matching optimization theory.** Furthermore, we propose an adaptive gradient-guided diffusion, where both pre-trained score network and guidance are iteratively updated using self-generated samples (Algorithm 2; Figure 1 with the fine-tuning block turned on). We show in expectation its iteration converges to a global optima within the latent subspace, at a rate of \(}(1/K)\) (Theorems 3, \(K\) is \(\#\) of iterations), matching the classical convergence in convex optimization.

\(\)**Provide experimental justification.** Simulation and image experiments are provided in Section 7 to support theoretical findings on latent structure-preserving and optimization convergence.

## 2 Related Works

To summarize the related work, we first give an overview of empirical studies relevant to our objective. We then discuss the theory of diffusion models, to which our main contribution is focused. Other

Figure 1: **Gradient-guided diffusion model for generative optimization, with or without adaptive fine-tuning.** A pre-trained diffusion model is guided with an additional gradient signal from an external objectives function towards generating near-optimal solutions.

related topics, such as direct latent optimization in diffusion models and a detailed review of sampling and statistical theory of diffusion models, are deferred to Appendix A.

Classifier Guidance and Training-free Guidance. introduced classifier-based guidance, steering pre-trained diffusion models towards a particular class during inference. This method offers flexibility by avoiding task-specific fine-tuning, but still requires training a time-dependent classifier. Training-free guidance methods [16; 57; 70; 4; 32; 50; 28] eliminate the need for a time-dependent classifier, using only off-the-shelf loss guidance during inference. [16; 57; 32; 28] is a line of works solving inverse problems on image and [70; 50] aims for guided/conditional image generation. Though not originally developed for solving optimization problems, [16; 70] both propose a similar guidance to ours: taking gradient on the predicted clean data \(x_{0}\) with respect to corrupted \(x_{t}\). Differently, our paper presents the first rigorous theoretical analysis of this gradient-based guidance approach. Furthermore, we propose an algorithm that iteratively applies the guidance as a module to the local linearization of the optimization objective, demonstrating provable convergence guarantees.

Fine-tuning of Diffusion Models.Several methods for fine-tuning diffusion models to optimize downstream reward functions include RL-based fine-tuning [7; 26] and direct backpropagation to rewards [18; 52; 68; 63]. However, these approaches often suffer from high computational costs and catastrophic forgetting in pre-trained models. Our guidance method is training-free and applied during the inference phase, eliminating the need to fine-tune diffusion models.

Theory of Diffusion Models.Current theory works primarily focus on unconditional diffusion models. Several studies demonstrate that the distributions generated by diffusion models closely approximate the true data distribution, provided the score function is accurately estimated [20; 2; 8; 39; 13; 40; 15; 14; 6]. For conditional diffusion models, [71; 27] establish sample complexity bounds for learning generic conditional distributions. Our novel analysis establishes a connection between the sampling process in gradient-based guided diffusion and a proximal gradient step, providing convergence guarantees.

## 3 Preliminaries: Diffusion Models and Guidance

Score-based diffusion models capture the distribution of pre-training data by learning a sequence of transformations to generate new samples from noise . A diffusion model comprises a forward and a backward process, for which we give a review as follows.

Forward Process.The forward process progressively adds noise to data, and then the sample trajectories are used to train the score function. The forward process initializes with \(X_{0}^{D}\), a random variable drawn from the pre-training data \(\). It introduces noise via an Ornstein-Uhlenbeck process, i.e.,

\[X_{t}=-q(t)X_{t}\,t+\,W_{t }\ \ \ \ \ q(t)>0,\] (1)

where \((W_{t})_{t 0}\) is Wiener process, and \(q(t)\) is non-decreasing. \(X_{t}\) represents the noise-corrupted data distribution at time \(t\). The conditional distribution \(X_{t}|X_{0}=x_{0}\) is Gaussian, i.e., \(((t)x_{0},h(t)I_{D})\) with \((t)=(-_{0}^{t}q(s)ds)\) and \(h(t)=1-^{2}(t)\). In practice, the forward process will terminate at a large time \(T\) so that the marginal distribution of \(X_{T}\) is close to \((0,I_{D})\).

Backward Process.If reversing the time of the forward process, we can reconstruct the original distribution of the data from pure noise. With \((})_{t 0}\) being another independent Wiener process, the backward SDE below  reverses the time in the forward SDE (1),

\[X_{t}^{}=[X_{t}^{}+ { p_{T-t}(X_{t}^{})}_{}]t+ _{t}.\] (2)

Here \(p_{t}()\) denotes the marginal density of \(X_{t}\) in the forward process. In the forward SDE (2), the _score function_\( p_{t}()\) plays a crucial role, but it has to be estimated from data.

Score Matching.To learn the unknown score function \( p_{t}()\), we train a score network \(s_{}(x,t)\) using samples from forward process. Let \(\) denote the data for training. Then the score network is learned by minimizing the following loss:

\[_{s}_{0}^{T}_{x_{0}}_{x_{t}|x_{0}}[\|_{x_{t}}_{t}(x_{t}|x_{0})-s(x_{t},t) \|^{2}]t,\] (3)

where \(\) is a given function class, \(_{}\) denotes the empirical expectation over training data \(\) and \(_{x_{t}|x_{0}}\) denotes condition expectation over the forward process, \(_{t}(x_{t}|x_{0})\) is the Gaussian transition kernel, i.e., \((2 h(t))^{-D/2}(-\|x_{t}-(t)x_{0}\|^{2}/(2h(t)))\).

Generation and Guided Generation.Given a pre-trained score function \(s_{}\), one generates samples by the backward process (2) with the true score replaced by \(s_{}\). Further, one can add additional guidance to steer its output distribution towards specific properties, as formulated in Module 1.

```
1:Input: Score \(s_{}\), guidance G default to be zero for unguided generation.
2:Hyper-parameter: \(T\).
3:Initialized at \(X_{t}^{}(0,I)\), simulate the following SDE till time \(T\): \[X_{t}^{}=[X_{t}^{}+s_{} (X_{t}^{},T-t)+(X_{t}^{},T-t )]t+_{t}.\]
4:Output: Sample \(X_{T}^{}\). ```

**Module 1**Guided_BackwardSample\((s_{},)\)

A common goal of guided generation (Module 1) is to generate \(X\) with a desired property \(Y=y\) from the distribution \(P(X|Y=y)\). To this end, it essentially needs to learn the **conditional score function**\(_{x_{t}} p_{t}(x_{t} y)\). The Bayes rule gives

\[_{x_{t}} p_{t}(x_{t} y)=(x_{t})}_{ s_{}(x_{t},t)}+} p_{t}(y x_{t})}_{ }.\] (4)

When a pre-trained score network \(s_{}(x_{t},t) p_{t}(x_{t})\), what remains is to estimate \(_{x_{t}} p_{t}(y x_{t})\) and add it as the guidance term G to the backward process in Module 1.

Classifier and Classifier-Free Guidance.Classifier guidance [60; 22] samples from \(P(X|Y=y)\) when \(Y\) is a discrete label. This method estimates \(_{x_{t}} p_{t}(y x_{t})\) by training auxiliary classifiers, denoted as \((y x_{t},t)\), and then computing the gradient of the classifier logits as the guidance, i.e., \((x_{t},t)=_{x_{t}}(y x_{t},t)\). Alternatively, classifier-free guidance  jointly trains a conditional and an unconditional diffusion model, combining their score estimates to generate samples.

Notations.For a random variable \(X\), \(P_{x}\) represents its distribution, and \(p(x)\) denotes its density. For \(X\), \(Y\) jointly distributed, \(P(X Y=y)\) denotes the conditional distribution, and \(p(x y)\) its density. The conditional expectation is denoted as \([x y]\). Let \(\) be the pre-training data, and let \(_{}\) be the empirical expectation over \(\). The empirical mean and covariance matrix are \(:=_{x}[x]\) and \(:=_{x}[(x-)(x-)^{}]\). For a matrix \(A\), \((A)\) denotes the subspace spanned by its column vectors, and for a square matrix \(A\), \(A^{-1}\) denotes its inverse or Moore-Penrose inverse. For any differentiable function \(f:^{n}^{m}\), \( f^{m n}\) denotes Jacobian matrix, i.e., \(( f)_{ij}=(x)}{ x_{j}}\).

## 4 A Primer on Gradient Guidance

Let us start with stating the problem we want to study: suppose given a pre-trained diffusion model where the score network \(s_{}(x_{t},t)\) provides a good approximation to the true score function \( p(x_{t})\), the goal is to generate novel samples with desired properties that can be measured by a user-specified differentiable function \(f\). We will refer to \(f\) as a reward or objective function later on. To achieve this goal, from Section 3, we know that guided generation with diffusion model is a good candidate, which deploys the following guided backward process (Module 1):

\[X_{t}^{}=[X_{t}^{}+s_{}(X _{t}^{},T-t)+(X_{t}^{},t)]t+ _{t}.\]

Here the guidance term G is what we focus on and wish to design. Specifically, we want to construct this guidance term G based on the gradient \( f\) of a general objective \(f\). This is motivated by the gradient methodology in optimization, a natural, intuitive way for adding guidance is to steer the generated samples towards the steepest ascent direction of \(f\)[16; 4; 18].

### Structural Data Distribution with Subspace

When incorporating property optimization in the generation process, it's crucial to consider intrinsic low-dimensional structures of real-world data, such as local regularities, global symmetries, and repetitive patterns [62; 55; 51]. Blindly improving \(f\) at the cost of losing these structures degrades sample quality dramatically. This quality degradation, also known as "reward over-optimization", is a common challenge for adapting diffusion models towards an external reward [71; 63].

To study the design of guidance that mitigates the risk of over-optimization, we focus on data that admits a low-dimensional latent subspace, formulated in the following assumption.

**Assumption 1** (Subspace Data).: _Data \(X^{D}\) can be represented as \(X=AU\), where \(A^{D d}\) is an unknown matrix with orthonormal columns, and the latent variable \(U^{d}\) follows some distribution \(P_{u}\) with a density \(p_{u}\). Here \(d D\). The empirical covariance of \(U\) is assumed full rank._

In the rest of this section, we investigate the principles for designing a guidance based on the gradient of \(f\) that ensures (i) improving the value of \(f\), and at the same time, (ii) being adhere to the subspace structure, i.e. generated samples being close to the subspace spanned by \(A\).

### Naive Gradient Does't Work as Guidance

A tempting simple choice of the guidance \(\) is by taking the steepest ascent direction \( f\), which we refer to as _naive gradient guidance_ i.e.,

\[(X_{t}^{},t) f(X_{t}^{}).\] (5)

However, the naive gradient guidance (5) would jeopardize the latent structure of data, which is demonstrated by the following proposition:

**Proposition 1** (Failure of Naive Guidance).: _For naive guidance \((X_{t}^{},t)=b(t) f(X_{t}^{})\), suppose \(b(t)>b_{0}>0\) for \(t>t_{0}\). For data in subspace under Assumption 1 and reward \(f(x)=g^{}x\), \(g(A)\) with \(h(t)=1-(-)\), then the off-subspace component of the generated sample is consistently large:_

\[[X_{T,}^{}]=Cg, C>b_{0}.\]

The intuition provided by Proposition 1 is, while the pre-trained score network effectively steers the distribution toward the latent subspace , the gradient vector \( f\) may point outside the subspace, causing the generated output to deviate from it (Figure 2). This is why naive gradient guidance fails.  also observed this, explaining that \( f\) is not computed for \(t=T\) i.e., it is not aligned with the clean data space.

### Motivating Latent Subspace Preserving Guidance from Conditional Score Function

Failure of the naive gradient in maintaining data structure motivates us to seek alternatives. To get some inspiration, we start with the most elementary Gaussian probabilistic model and linear \(f\). Later we will drop this assumption and consider general data distributions and general \(f\).

**Assumption 2** (Gaussian Linear model).: _Let data follow a Gaussian distribution, i.e., \(X(,)\), and let \(f(x)=g^{}x\) be a linear function for some \(g^{D}\). Let \(Y=f(X)+\) with independent, identically distributed noise \((0,^{2})\) for some \(>0\)._

Figure 2: **Directly adding the gradient of the objective function to the backward process sabotages the subspace structure. Left: Directly adding gradients that point out of the data subspace causes samples to leave the subspace. Right: Numerical experiments show that naive gradients lead to substantially larger off-subspace error compared to our gradient guidance \(_{loss}\)(Definition 1); see Section 7 for experiment details.**

By the Bayes' rule, the conditional score \(_{x_{t}} p_{t}(x_{t} y)\) takes the form of a sum given by

\[_{x_{t}} p_{t}(x_{t} y)=(x_{t})}_{ s_{s}(x_{t}),t}+} p_{t}(y x_{t})}_{ }.\] (recall ( 4 ))

Under the Gaussian assumption, we derive the following closed-form formula of the guidance term \( p_{t}(y x_{t})\) that we want to estimate. The proof is provided in Appendix D.4.

**Lemma 1** (Conditional Score gives a Gradient-like Guidance).: _Under Assumption 2, we have_

\[_{x_{t}} p_{t}(y x_{t})=-(2_{y}^{2}(x_{t}))^{-1 }_{x_{t}}(y-g^{}[x_{0} x_{t}])^{2},\] (6)

_where \([x_{0}|x_{t}]\) denotes the conditional expectation of \(x_{0}\) given \(x_{t}\) in the forward process (1), and \(_{y}^{2}(x_{t})\) is the variance of the conditional distribution \(Y X_{t}=x_{t}\)._

The form of conditional score shown in Lemma 1 motivates our proposed gradient guidance:

**Definition 1** (Gradient Guidance of Look-Ahead Loss).: _Given a gradient vector \(g\), define the gradient guidance of look-ahead loss as_

\[_{loss}(x_{t},t):=-(t)_{x_{t}}(y-g^{} [x_{0}|x_{t}])^{2},\] (7)

_where \((t)>0,y\) are tuning parameters, and \([x_{0}|x_{t}]\) is the conditional expectation of \(x_{0}\) given \(x_{t}\) in the forward process (1), i.e., \(X_{t}=-q(t)X_{t}\,t+\,W_{t}\)._

The formula in (7) generalizes the intuition of a conditional score for any data distribution and objective function. It scales with the residual term \(y-g^{}[x_{0} x_{t}]\), tuning the _strength of guidance_. Here, \([x_{0} x_{t}]\) represents the expected clean data \(x_{0}\) given \(x_{t}\) in the forward process, which coincides with the expected sample in the backward view. This residual measures the **look-ahead gap** between the expected reward of generated samples and the target value. The **look-ahead loss**\((y-g^{}[x_{0}|x_{t}])^{2}\) resembles the proximal term commonly used in first-order proximal optimization methods.

Remark.The gradient guidance (7) aligns with the groundtruth conditional score in (6) under the assumptions of Gaussian data and linear reward (Assumption 2). This theoretical motivation, rooted in a fundamental framework, distinguishes our work from the empirical practice, such as DPS  and universal guidance .

A key advantage of \(_{loss}\) is that it enables preserving the subspace structure, for **any** data distribution under Assumption 1. This is formalized in the following theorem, the full proof in Appendix D.5.

**Theorem 1** (Faithfulness of \(_{loss}\) to the Low-Dimensional Subspace of Data).: _Under Assumption 1, it holds for any data distribution and \(g^{D}\) that_

\[_{loss}(x_{t},t)(A).\] (8)

Proof Sketch.We have

\[_{x_{t}}(y-g^{}[x_{0} x_{t}])^{2} _{x_{t}}[x_{0} x_{t}]^{}g.\]

We will show that the Jacobian \(_{x_{t}}[x_{0}|x_{t}]\) maps any vector \(g^{D}\) to \((A)\). To see this, we utilize the score decomposition result in Appendix D.1 and plug it into the equality \([x_{0}|x_{t}]=^{-1}(t)(x_{t}+h(t) p_{t}(x_{t}))\) (Tweedie's formula ), we have

\[[x_{0} x_{t}]=^{-1}(t)(x_{t}+h(t)[Am(A^{}x_{ t})-h^{-1}(t)x_{t}])=h(t)/(t) Am(A^{}x_{t}),\] (9)

here \(m(u)= p_{t}^{}(u)+h^{-1}(t)u\), \(p_{t}^{}(u)\) latent density (Appendix D.1). We see \(_{x_{t}}[x_{0}|x_{t}]^{}\) maps any vector to \((A)\) because \(m()\) takes \(A^{}x_{t}\) as input in the expression of \([x_{0}|x_{t}]\). 

We highlight that the faithfulness of \(_{loss}\) holds for _arbitrary_ data distribution supported on the latent subspace. It takes advantage of the score function's decomposition (19), having the effect of automatically adapting \(g\) onto the latent low-dimensional subspace of data.

Remark.We provide a rigorous guarantee for manifold preservation of gradient guidance, a property previously discussed by . However, while  claim manifold preservation, they do not present a formal mathematical proof.  relies heavily on pre-trained autoencoders for manifold projections, which are often unavailable in practical scenarios.

### Estimation and Implementation of \(_{loss}\)

In this section, we discuss the estimation and computation of \(_{loss}\) based on a pre-trained score function \(s_{}\) in practice. \(_{loss}\) involves the unknown quantity \([x_{0}|x_{t}]\). One can construct estimate \([x_{0}|x_{t}]\) by considering the Tweedie's formula : \( p_{t}(x_{t})=-h^{-1}(t)[x_{t}-(t)x_{0}x _{t}],\) which gives rise to

\[}[x_{0}|x_{t}]:=^{-1}(t)(x_{t}+h(t)s_{}(x_{t}, t)),\] (10)

and we refer to it as the _look-ahead estimator_. The estimator (10) is widely adopted in practice [56; 4]. Here \((t)\) and \(h(t)\) are the noise scheduling used in the forward process (1).

Thus, we have obtained an **implementable version of the gradient guidance \(_{loss}\)**, given by

\[_{loss}(x_{t},t)=-(t)_{x_{t}}[y-g^{} (^{-1}(t)(x_{t}+h(t)s_{}(x_{t},t)))]^ {2},\] (11)

With a slight abuse of notation, we use \(_{loss}\) to refer to this implementable formula (11) in the remainder of this paper. Here, \(y\) is a target reward value from conditional score analysis under a Gaussian model and is treated as a tuning parameter in practice. The gradient guidance (11) is lightweight to implement. Given a pre-trained score function \(s_{}\) in the form of a neural network, computing (11) involves calculating the squared loss \((y-g^{}}[x_{0}|x_{t}])^{2}\) via a forward pass of \(s_{}\) and a backward pass using the auto-gradient feature of deep-leaning frameworks such as PyTorch and TensorFlow. See Figure 3 for illustration.

## 5 Gradient-Guided Diffusion Model as Regularized Optimizer

In this section, we study if gradient guidance steers pre-trained diffusion models to generate **near-optimal** samples. Our results show that: 1) Iterative gradient guidance improves the objective values; 2) The pre-trained diffusion model acts as a regularizer from an optimization perspective.

### Gradient-Guided Generation with A Pre-trained Score

Assuming access to a pre-trained score network \(s_{}\) and the gradient of the objective function \(f\), we present Algorithm 1 to adapt the diffusion model and iteratively update the gradient guidance (11). See Figure 1 for illustration.

Alg. 1 takes any pre-trained score function \(s_{}\) as input. Each iteration evaluates \( f()\) at samples from the previous iteration (Line 5(i)), computes the gradient guidance \(_{loss}\) with the new gradient (Line 5(ii)), and generates new samples using the updated guidance (Module 1). The algorithm outputs an adapted diffusion model specified by \((s_{},_{K})\).

```
1:Input: Pre-trained score network \(s_{}(,)\), differentiable objective function \(f\).
2:Tuning Parameter: Strength parameters \((t)\), \(\{y_{k}\}_{k=0}^{K-1}\), number of iterations \(K\), batch sizes \(\{B_{k}\}\).
3:Initialization:\(_{0}=\) NULL.
4:for\(k=0,,K-1\)do
5:Generate: Sample \(z_{k,i}\)Guided_BackwardSample\((s_{},_{k})\) using Module 1, for \(i[B_{k}]\).
6:Compute Guidance: (i) Compute the sample mean \(_{k}:=(1/B_{k})_{i=1}^{B_{k}}z_{k,i}\).
7:(ii) Query gradient \(g_{k}= f(_{k})\).
8:(iii) Update gradient guidance \(_{k+1}(,)=_{loss}(,)\) via (7), using \(s_{}\), gradient vector \(g_{k}\), and parameters \(y_{k}\) and \((t)\).
9:endfor
10:Output:\((s_{},_{K})\). ```

**Algorithm 1** Gradient-Guided Diffusion for Generative Optimization

### Gradient-Guided Diffusion Converges to Regularized Optima in Latent Space

We analyze the convergence of Alg. 1 and show that in final iterations, generated samples center around a regularized solution of the optimization objective \(f\) within the subspace \((A)\). Our theorems allow the pre-training data to have _arbitrary distribution_.

**Assumption 3** (Concave smooth objective).: _The objective \(f:^{D}\) is concave and \(L\)-smooth w.r.t. the (semi-)norm \(\|\|_{^{-1}}\), i.e., \(\| f(x_{1})- f(x_{2})\|_{} L\|x_{1}-x_{2} \|_{^{-1}}\) for any \(x_{1},x_{2}\)._

Figure 3: Computing \(_{loss}\).

While Alg. 1 works with any pre-trained score network, we study its optimization properties focusing on the class of linear score functions given by

\[=\{s(x,t)=C_{t}x+b_{t}:C_{t}^{D D},\,b_{t} ^{D}\}.\] (12)

Remark on the linear parametrization of score network (12):Analyzing the output distribution of guided diffusion is challenging because the additional guidance term destroys the dynamics of reverse SDE. A linear score is a natural and reasonable choice for characterizing the output distribution, as it was also adopted by .

With a linear score function (12), pre-training a diffusion model is equivalent to using a Gaussian model to estimate and sample from the estimated distribution. Thus, the guidance \(_{loss}\) is also linear in \(x_{t}\), and the final output follows a Gaussian distribution; see (25) in Appendix E. We focus on the mean, \(_{K}\), of the generated distribution from the backward sampling of \((s_{},_{K})\) (as \(T\)), and establish its optimization guarantee.

**Theorem 2** (Convergence to Regularized Maxima in Latent Subspace in Mean).: _Let Assumptions 1 and 3 hold. Suppose we use the score function class (12) for pre-training and computing guidance. Then Alg.1 gives an adapted diffusion model that generates new samples that belong to \((A)\). Further, for any \(>L,\) there exists \((t),\{y_{k}\}\) and batch size \(B_{k}\), such that with high probability \(1-\), the mean of the output distribution \(_{K}\) converges to be near \(x_{A,}^{*}\), and it holds_

\[f(x_{A,}^{*})-f(_{K})=( )^{K}(d()),\]

_where \(x_{A,}^{*}\) is an optimal solution of the regularized objective:_

\[x_{A,}^{*}=*{argmax}_{x(A)}\ \{f(x)-\|x-\|_{^{-1}}^{2} \}.\] (13)

_where \(,\) are empirical mean and covariance of pre-training data \(\)._

Remarks.(1) The regularization term \(\|x-\|_{^{-1}}^{2}\) (23) centers the data's mean \(\) and is stronger in directions where the original data has low variance. Thus, the pre-trained score acts as a "prior" in the guided generation, favoring samples near the pre-training data, even with guidance.

**(2)** The regularization term cannot be arbitrarily small, as our theorem requires \( L\). Thus, only adding gradient guidance cannot achieve the global maxima. If the goal is global optima, the pre-trained score must be updated and refined with new data, as explored in Section 6.

**(3)** The convergence rate is linear in the latent dimension \(d\), rather than data dimension \(D\). Since \(_{loss}\) is faithful to the latent subspace (Theorem 2), the generated samples and optimization iterates of Alg. 1 remain within \((A)\). This leverage of the latent structure results in faster convergence.

## 6 Gradient-Guided Diffusion with Adaptive Fine-Tuning for Global Optimization

In the previous section, we have seen that adding guidance to a pre-trained diffusion model can't improve the objective function unlimitedly due to the pre-trained score function acting as a regularizer. We consider adaptively fine-tuning pre-trained diffusion models to attain global optima. Empirically, fine-tuning diffusion models using self-generated samples has been explored by [7; 18].

### Adaptive Fine-Tuning Algorithm with Gradient Guidance

We propose an adaptive version of the gradient-guided diffusion, where both the guidance and the score are iteratively updated using self-generated samples. The full algorithm is given in Algorithm 2.

We introduce a weighting scheme to fine-tune the score network using a mixture of pre-training data and newly generated samples. In Round \(k\), let \(_{1},,_{k}\) be sample batches generated from the previous rounds. Let \(\{w_{k,i}\}_{i=0}^{k}\) be a set of weights. Conceptually, at Round \(k\), we update the model by minimizing the weighted score matching loss:

\[_{s}_{0}^{T}_{i=0}^{k}w_{k,i}_{x_{0} _{1}}_{x_{t}|x_{0}}[\|_{x_{t}}_{t }(x_{t}|x_{0})-s(x_{t},t)\|_{2}^{2}]t,\] (14)

where \(_{0}:=\) is the pre-training data. For illustration, please see also Figure 1, and the practical implementation of Alg. 2 is in Appendix F.

### Guided Generation Finds Unregularized Global Optima

Finally, we analyze the optimization properties for gradient-guided diffusion model with iterative finetuning. We establish that the process of Algorithm 2 yields a final output distribution whose mean, denoted by \(_{K}\), converges to the global optimum of \(f\).

For simplicity of analysis, we study the following function class

\[^{}=\{s(x,t)=_{t}x+b_{t}:\,b_{t}^{D} \},\] (15)

where \(_{t}\) is set to stay the same in pre-trained scores, with only \(b_{t}\) updated during iterative fine-tuning.

**Theorem 3** (Convergence to Unregularized Maxima in Latent Subspace in Mean).: _Let Assumptions 1 and 3 hold, and assume there exists \(M>0\) such that \(\|x_{A,}^{*}\|<M\) for all \( 0\). Suppose we use the score function class (12) for pre-training \(s_{}\) and the class (15) for finetuning it. Then Algorithm 2 gives an adapted diffusion model that generates new samples belonging to \((A)\). Further, there exists \(\{(t)\},\{y_{k}\},\{B_{k}\}\) and \(\{w_{k,i}\}\), such that with probability \(1-\),_

\[f_{A}^{*}-f(_{K})=( K}{K}( )),\] (16)

_where \(f_{A}^{*}=\{f(x)|x(A)\}\)._

Theorem 3 illustrates that fine-tuning a diffusion model with self-generated data can reach global optima while preserving the latent subspace structure. The convergence rate matches standard convex optimization in terms of gradient evaluations, \(K\). Compared to standard gradient solvers, guided diffusion models leverage pre-training data to solve optimization problems in a low-dimensional space, preserving desired structures and enabling more efficient exploration and faster convergence.

## 7 Experiments

### Simulation

We conduct numerical simulations of Algorithms 1 and 2, following the subspace setup described in Assumption 1. Specifically, we set \(d=16\), \(D=64\), The latent variable \(u\) is drawn from \((0,I_{d})\) and used to construct \(x=Au\), where \(A\) is a randomly generated orthonormal matrix. We define the objective function \(f(x)=10-(^{}x-3)^{2}\). To approximate the score function, we employ a version of the U-Net  with 14.8M trainable parameters. More details including how to set up \(\) are provided in Appendix F.1.

Preserving Subspace Structure.We first demonstrate that \(_{loss}\) preserves the subspace structure learned from the pre-trained model. For comparison, we also tested the naive guidance \((x_{t},t):=(t)(y-g^{}[x_{0}|x_{t}])g\) (more details in Appendix F.1.). Figure 4 (left) shows that \(_{loss}\) performs much better than the naive gradient \(\) in preserving the linear subspace. Figure 4 (right)

Figure 4: **Comparison between two types of gradient guidance \(\) and \(_{loss}\) (left: Alg. 1; right: Alg. 2). The off/on support ratio of the generated samples is defined as \(r_{}=\|}{\|x_{}\|}\).**demonstrates that off-support errors increase with adaptive score fine-tuning (Alg. 2) due to distribution shift, with G resulting in more severe errors than \(_{loss}\).

Convergence Results.Figure 5 (a) and (b) show that Alg. 1 converges to a sub-optimal objective value, leaving a gap to the maximal value. This aligns with our theory that the pre-trained model acts as a regularizer in addition to the objective function. Figure 5 (c) shows that Alg 2 converges to the maximal value of the objective function. As illustrated by Figure 5 (d), samples from Alg. 1 mostly stay close to the pre-training data distribution (dotted contour area), whereas samples from Alg. 2 move outside the contour as the diffusion model is fine-tuned with self-generated data.

### Image Generation

We validate our theory in the image domain for Algorithm 1. We employ the StableDiffusion v1.5 model  as the pre-trained model. For the reward model, we follow the approach outlined by  to construct a synthetic model. This model is based on a ResNet-18  architecture pre-trained on ImageNet , with the final prediction layer replaced by a randomly initialized linear layer that produces scalar outputs. For more experiment details, refer to Appendix F.2.

Results.By Algorithm 1, the reward increases and converges. Figure 6 (left) shows the reward changes with optimization iterations. The hyperparameter \(y\) tunes the strength of guidance and is inversely related to the strength of the regularizer (theoretical implications in Appendix E). A larger guidance strength (smaller regularizer strength) leads to a higher convergent reward value. Figure 6 (right) illustrates the changes in generated images across iterations. As the reward increases, the images become increasingly abstract, shifting from photo-realistic with detailed backgrounds to more virtual, stylized ones.

## 8 Conclusion

In this paper, we focus on gradient guidance for adapting or fine-tuning pre-trained diffusion models from an optimization perspective. We investigate the look-ahead loss based gradient guidance and two variants of diffusion-based generative optimization algorithms utilizing it. We provide guarantees for adapting/fine-tuning diffusion models to maximize any target concave differentiable reward function. Our analysis extends to linear subspace data, where our gradient guidance and adaptive algorithms preserve and leverage the latent subspace, achieving faster convergence to near-optimal solutions.

Figure 5: **Convergence of Algorithms 1 and 2. (a) and (b) are under different \(\) for the objective function. (d) visualizes the distribution of the generated samples of Alg. 2 (red) and Alg. 1 (blue) across the iterations.**

Figure 6: **Reward increase and effect on images across iterations. Left: Reward increases and converges across iterations. Larger guidance strength \(y\) (smaller regularizer strength) results in higher convergent reward value. Right: Images become more abstract, shifting from realistic to virtual backgrounds as reward increases.**