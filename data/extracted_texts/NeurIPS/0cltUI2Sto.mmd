# On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes

Rajat Modi\({}^{1}\)1, Vibhav Vineet2, Yogesh Singh Rawat\({}^{1}\)

CRCV, University of Central Florida\({}^{1}\), and Microsoft Research\({}^{2}\)

###### Abstract

This paper explores the impact of occlusions in video action detection. We facilitate this study by introducing five new benchmark datasets namely O-UCF and O-JHMDB consisting of _synthetically controlled_ static/dynamic occlusions, OVIS-UCF and OVIS-JHMDB consisting of occlusions with _realistic motions_ and Real-OUCF for occlusions in _realistic-world_ scenarios. We formally confirm an intuitive expectation: existing models _suffer a lot_ as occlusion severity is increased and exhibit _different behaviours_ when occluders are static vs when they are moving. We discover _several intriguing phenomenon_ emerging in neural nets: 1) transformers can _naturally outperform CNN models_ which might have even used occlusion as a form of data augmentation during training 2) incorporating symbolic-components like capsules to such backbones allows them to _bind_ to occluders _never even seen_ during training and 3) Islands of agreement _can emerge_ in _realistic_ images/videos _without_ instance-level supervision, distillation or contrastive-based objectives2(eg. video-textual training). Such emergent properties allow us to derive simple yet effective training recipes which lead to robust occlusion models inductively satisfying the _first two stages_ of the binding mechanism (grouping/segregation). Models leveraging these recipes _outperform_ existing video action-detectors under occlusion by 32.3% on O-UCF, 32.7% on O-JHMDB & 2.6% on Real-OUCF in terms of the vMAP metric. The code for this work has been released at https://github.com/rajatmodi62/OccludedActionBenchmark.

## 1 Introduction

Deep learning has led to significant advances in object detection/segmentation for both image and video domain. Such deep neural networks are in turn widely used in self-driving cars and safety critical scenarios. A key concern for such applications is whether they are able to perform well when encountering realistic occlusions: e.g. are they able to reliably localize a pedestrian even when an occluder (say a dog) comes in front of him. However, one major limitations being existing dataset test split doesn't contain such occlusions. This raises a concern, whether these models will be robust to real-world occlusions or not.

One would suspect that the inherent inductive biases of these architectures would be enough to induce natural occlusion robustness. To verify the hypothesis, we run two _preliminary setups:_ (Fig. 1) (A) Firstly, superimposing a single occluder (eg bus) over the actor, and, (B) Then, we analyze the performance if the occlusion is shifted to background (i.e. no occlusion over actor at all). We observe a relative drop of 20-50% across multiple existing state-of-the-art approaches . This verifiesour hypothesis that existing approaches are _not_ robust to occlusion. It opens up the area that there is a need to study these models behaviour under different types and severity of occlusion.

We conduct _first_ benchmark study of occlusions in spatio-temporal video action detection. We investigate following aspects: a) _Effect of different severity level of occlusions_, b) _Robustness of different network backbones_, c) _Effect of pre-training and trainable parameters_. d) _Effect of static vs dynamic occlusions on models_. Our study yields the following key insights: 1) Performance decreases when severity increases on actor region, but increasing severity on background does not have much impact 2) Transformers which have _never_ seen occlusions during training can outperform CNN-based models which _uses_ occlusions as a data augmentation. 3) a significant component of transformer's robustness (and other architectures as well) comes from pre-training on large-scale datasets. 4) Existing models perform better on static occlusions than _realistic_ dynamic occlusions.

Based on our study, we investigate techniques to make the existing video action detection models robust against occlusion. First, we experiment with augmentations and found them to be effective for all the models in improving their robustness. We further utilize some of these insights and develop a transformer-based token masking strategy that provides a robust video action detection model which outperforms all existing methods. We make the following contributions in this work:

* We analyze existing approach behaviours on occlusions and conduct a _first_ benchmark study on occlusions in spatio-temporal action detection.
* Towards this, we propose _five_ novel datasets, i.e. O-UCF, O-JHMDB for static occlusions, OVIS-UCF, OVIS-JHMDB for _realistic_ occluder motions and Real-OUCF for real world occlusions to benchmark our study.
* To improve robustness of existing approaches, we show how occlusion as augmentation improves robustness. Furthermore, we propose token-masking to further improve robustness of transformer backbones.

## 2 Related Work

**Spatio-Temporal Action Detection (STVAD):** Involves localizing an actor in _each_ video frame and also predicting an action class. Existing methods[66; 58; 11] adopt a tubelet- based approach: action trajectories can be thought of as a sequence of actor bounding boxes spread over time. These tubes can be predicted by progressively refining predictions over a center-keyframe[46; 77] or directly learning a tubelet as a feature volume. However, these methods require additional post-processing by linking these smaller tubelets across time. Other architectures like VideoCapsuleNet  frame action-detection as a per-pixel localization task.

**Occlusions in Videos:** Recently, OVIS dataset was proposed for Video Instance Segmentation (VIS) under realistic occlusions. Methods like [76; 32; 3] introduce occlusion robustness by temporally averaging actor-centric embeddings in DETR-like architectures. However, instance-recognition during VIS can be performed by object-detection in a single frame whereas action-classification in STVAD requires more than one frame. Approaches like [44; 70] explore occlusions

Figure 1: **A Toy Experiment. (i-iii) superimposing a single occluder (bus) on an actor and varying its size results in drops as large as 50%. (iv) even a simple occluder (cat) in background results in 30% drop. (v) highest drops are observed if background is entirely masked. (vi-ix) Clean refers to all methods evaluated on unoccluded test sets. Best viewed when zoomed in.**

for action-recognition. In comparison, we _additionally_ focus on the impact of occlusions on _localization_ ability of SOTA action-detectors.

**Occlusion in Images:** CompositionalNets explore occlusion-robustness for image classification through EM-based mixture models. DeepVoting  caters to object-detection under occlusions by proposing a voting layer to gather contributions from visible object parts. While such studies are restricted to images, we study occlusions under videos.

**Regularization Techniques:** Dropout  have been shown to act as an implicit regularizer  and reduce network overfitting. Moreover, dropout has been found effective for increasing occlusion robustness . We _further_ show that masking input tokens in transformers  is also an effective training strategy for this task.

## 3 The Video Occlusion Datasets

The aim of this study is to study occlusions in spatio-temporal video action detection. We present five different benchmark datasets to study this problem. Two of them (O-UCF and O-JHMDB) are synthetically generated to systematically study this problem by manipulating a single occlusion parameter at a time. Similarly, OVIS-UCF, OVIS-JHMDB consist of occluders exhibiting realistic motions. Furthermore, we also present a real-world dataset (Real-OUCF) to validate our findings. We use three different parameters to create our synthetic datasets, 1) Indoor/Outdoor Occluders 2) Severity of Occlusion 3) Static/Dynamic motions of occluders.

### Occluder Selection

To keep the generated occlusions _realistic_, we first crop out several objects from the Pascal-VOC images using ground-truth pixel-level annotations. Out of 2413 cropped instances, we shortlist 900 possible occluders which correspond to commonly-occurring objects. Next, these objects are distributed among two separate categories namely indoor objects ( eg, chair, table) and outdoor objects (eg, aeroplane, ship etc.). Finally, we _softly_ blend these occluders over the video pixels using RGBD maps of these objects. This blending correctly simulates a very real object (occluder) being present over in the frame, without having to resort to an intensive data collection procedure.

Figure 2: **Sample video frames from proposed benchmark datasets. (i) occlusion severity increases across 9 severity levels and both actor/background region. (ii) occluders are sampled from both indoor/outdoor splits. (iii) Our O-UCF and O-JHMDB simulate 6 dynamic occluder motions like circle, linear, zoom-in etc. (iv) Similary, the proposed Real-OUCF is a dataset for realistic scenarios where multiple actors mutually-occlude each other. Best viewed in color.**

### Severity of Occlusion

In a video, an actor occupies certain portion of the frame as it moves over time. Formally, we define such an actor-region (FG) as the tightest bounding box enclosing all the ground truth boxes of the actor's trajectory over time. The remaining region is termed as background(BG).

Occlusion severity is defined as the fraction of the total area which is occupied by an occluder during occlusion. We experiment with three severity levels, namely 1 (0-20%), 2 (20-40%), and 3 (40-60%) respectively. Occlusion levels in the actor region are prefixed by FG, i.e. FG1/2/3, whereas occlusions in the background are prefixed by BG, i.e. BG 1/2/3. A combination of these actor/background severity levels yields 9 levels of severity.

### Types of Occlusion

**Static Occlusions:** Refer to the occlusions where the occluders occupy a fixed position in the frame and don't move over time. Our dataset consists of 9 severity combinations of actor/background regions as explained in the previous section.

**Dynamic Occlusions:** Existing occlusion-based datasets are mostly image-based and are therefore unable to study the effects of the movement of occluders in the video. For such dynamic cases, we pick a fixed severity level (i.e. FG2, BG3) and vary the motion of the occluders from a particular starting point. Our test set consists of two types of motions, i.e. circular and sinusoidal. The training set consists of linear, zoom-in, zoom-out, or random motions. Note that the motion in the train/test set is mutually-exclusive, i.e. one type of motion in one dataset split is not present in another for fairness. Similarly, an occluder moving in the actor region never mixes over to the background region but instead gets wrapped around over the course of its trajectory.

### Benchmark datasets

The statistics for five proposed benchmark datasets are illustrated in Table 1. **O-UCF** contains of 24 action classes, along with 20306 testing samples. Similarly, **O-JHMDB** contains 21 action classes with 928 samples. Both of these datasets consist of static occlusions in 9 levels of severity, 4 _controlled_ occluder trajectories in train set, and 2 trajectories in the test set. Our **OVIS-UCF/OVIS-JHMDB** datasets consist of occluders with realistic motions from OVIS superimposed on top of original UCF/JHMDB datasets. Note that we term this dataset as semi-realistic because although the occluders themselves are synthetically placed on the frames, occluder trajectories are naturally occuring as observed in OVIS dataset. **Real-OUCF** consists of 1743 fully-realistic occlusion videos which were hand-picked from Youtube for 24 action classes. These videos were then cropped temporally using LossLessCut, to precisely localize start/end time step of each action. Such shorter duration clips then need to be spatio-temporally annotated. Therefore, we feed-forward all such clips through auto-label generator of GroundedSAM in order to localize "person" class with an appropriately constructed textual query. Since SAM is a foundational model, it segments all the persons in an image. However, we are only concerned with the people who are actually performing actions. In order to remove such excessive false positives predicted by GroundedSAM, we manually suppress/refine per-frame instance-level masks using the CVAT Annotation Tool. Finally, we end up with 64.1% of annotated instances being occluded.

## 4 Experiments and Analysis

**Studied models:** We experiment on three most recent SOTA methods in video action detection. Namely, we pick MOC,YOWO and VideoCapsuleNet whose official Github implementations and evaluation protocols have been _fully_ open-sourced for _both_ UCF and JHMDB datasets. All

   Statistics & O-UCF & OVIS-UCF & O-JHMDB & OVIS-JHMDB & Real-OUCF \\  Classes & 24 & 24 & 21 & 21 & 24 \\ Severity Levels & 9 & 9 & 9 & 9 & - \\ Occ. Motions & 6 & 6 & 6 & 6 & - \\ Occ. Type & Synthetic & Semi-Real & Synthetic & Semi-Real & Real \\ Total Videos & 20306 & 20306 & 5896 & 5896 & 1743 \\ Instances & 2284 & 2284 & 928 & 928 & 6920 \\   

Table 1: Benchmark dataset statisticsthese models are tested on the original _clean_ UCF24 & JHMDB-21 datasets, which do not contain any occlusions. Also, we evaluate these models on _occ_ versions, i.e. our O-UCF/O-JHMDB datasets. All our transformer-based models have been trained for 1.5x more epochs than CNN based models to facilitate appropriate convergence. The best performing models for downstream testing are selected from a separate hold-out validation set.

**Metrics** A well accepted metric for action-detection is v-mAP at 0.5 ioU threshold, as it also measures the _temporal consistency_ of the prediction across time. Assume that under occlusions vmAP drops from \(V\) to \(V^{}\). Therefore, we report _absolute_ robustness \(_{a}=(1-}{100})\) and _relative_ robustness \(_{r}=(1-}{V})\). Note that both \(0_{a},_{r} 1\), with greater value denoting more robustness.

### Results and Analysis

We present the result of our benchmark on the baselines and analyse several interesting trends.

**Increasing occlusion severity over actor region reduces performance.** In Tabs2,3, we occlude actor region with multiple occluders. It can be clearly observed that as the occlusion severity is increased, the performance of all the action-detectors is reduced. The notable thing is that VCAPS is most robust to severe occlusion (i.e. FG3) (in terms of absolute scores) on the larger O-UCF dataset (i.e. 36.9%), and YOWO is most robust to occlusions on smaller JHMDB dataset (i.e. 46.2%). The larger performance difference in MOC and YOWO could be attributed to the treatment of action-detection as a box-regression problem in MOC/YOWO vs dense-semantic mask prediction in the VideoCapsuleNet. Regressing the 4 coordinates of a box has a minimum probability of error as \(=0.25\), whereas dense-prediction reduces this error to 1/n, where n is the no of pixels being predicted and \(n>>4\). As long as at least four pixels are predicted along the boundaries of the box, the method performs relatively well3. Also capsules have been shown to be extremely robust to occlusions on Multi MNIST, and we believe similar inductive bias extends to videos.

**Are backbones with more parameters _necessarily_ more robust?** To answer this question, we tried CNN/Transformer-based backbones on two of our best performing methods, i.e. YOWO & VCAPS. In Table 5, it is evident that Mvitv2-S with 50% less parameters (35M) can outperform other 3D backbones like ResNext using large as 89M parameters (i.e. 67.3% on O-UCF & 86.8% on O-JHMDB). This shows that the internal attention mechanism in transformers _doesn't necessarily need more parameters_ to improve robustness. Note that VCAPS with Mvitv2-S backbone (67.3, O-UCF) largely outperforms YOWO which also uses Mvitv2-S backbone (31.2, O-UCF). A crucial architectural difference is that the VCAPS-Mvitv2 uses 2 additional layers of capsules, whose qualitative effects shall be revealed in a later section.

   & &  &  &  \\   & Clean & Occ & \(_{a}\) & \(_{r}\) & Occ & \(_{a}\) & \(_{r}\) & Occ & \(_{a}\) & \(_{r}\) \\  MOC & 54.4 & 53.6 & 0.99 & 0.99 & 35.2 & 0.81 & 0.65 & 29.5 & 0.75 & 0.54 \\ YOWO & 48.8 & 38.5 & 0.90 & 0.79 & 32.5 & 0.84 & 0.67 & 26.8 & 0.78 & 0.55 \\ VCAPS & 75.5 & 51.5 & 0.76 & 0.68 & 42.8 & 0.67 & 0.57 & 36.9 & 0.61 & 0.49 \\  

Table 2: **O-UCF Benchmark:** Performance across three occlusion severity levels. For a particular actor-region occlusion (FG), averaged results across BG1/2/3 levels are reported.

   & &  &  &  \\   & Clean & Occ & \(_{a}\) & \(_{r}\) & Occ & \(_{a}\) & \(_{r}\) & Occ & \(_{a}\) & \(_{r}\) \\  MOC & 77.2 & 45.2 & 0.68 & 0.59 & 33.9 & 0.57 & 0.44 & 25.9 & 0.49 & 0.34 \\ YOWO & 85.7 & 50.5 & 0.65 & 0.59 & 47.7 & 0.62 & 0.56 & 46.2 & 0.61 & 0.54 \\ VCAPS & 65.7 & 49.2 & 0.84 & 0.75 & 35.4 & 0.70 & 0.54 & 21.7 & 0.56 & 0.33 \\  

Table 3: **O-JHMDB Benchmark:** Performance across three occlusion severity levels. For a particular actor-region occlusion (FG), averaged results across BG1/2/3 levels are reported.

**Pre-training improves transformer's robustness to occlusions.** To investigate why transformers perform so well out of the box, we train them from scratch _without_ pre-trained weights and test them on O-UCF/O-JHMDB datasets. From Fig 3 (i), it can be observed that transformers (i.e. VCAPS-Mvitv2) perform poorly _without_ weights compared to other models. However, on training with Kinetics 400 weights, the transformers achieve state-of-the-art results. Therefore transformers are _not naturally more robust_, but pre-training helps to improves their intrinsic-robustness. This also reveals the glaring dependency of transformers on large amounts of data and compute for pre-training.

**Increasing parameter size in same model family improves occlusion robustness** By universal-approximation theorem , a larger neural net can learn better representations. We investigate whether similarly increasing the model parameter size (and hence internal-representational capacity) could lead to better occlusion robustness. In Fig 4(right), it can be seen that changing the backbones from Resnet 18\(\)50\(\)101 follows this trend. This shows that increasing parameter size in the _same_ model family generally leads to improved occlusion robustness.

**Effect of occluder motion on backbones.**We experiment with realistic occluder motions by superimposing occluders from OVIS datasets upon UCF/JHMDB videos, resulting in benchmark datasets called OVIS-UCF and OVIS-JHMDB respectively. In Tab14, we observe that in general models are performing better during static occlusions than dynamic occlusions. The differences still hold when the backbones are made consistent. This shows that resolving actor location becomes difficult when both occluders and actors move together. Therefore, reasoning about temporal motion properly remains a worthwhile architectural pursuit. We had also experimented with _controlled_ occluder motions like circle, sinusoids. For those results, we refer the reader to the supplementary.

From this section, we conclude that using transformers as a backbone and training _using_ pre-trained weights can greatly improve robustness.

## 5 Improving Robustness under Occlusion

Next, we investigate if we can make these models robust against occlusion. Augmentation is a well studied technique to induce robustness in models against distribution shifts. We experimented with synthetic occlusions as augmentations to make the existing models robust. We generate augmented samples by superimposing occluders at random locations, various severity levels and motion. Training existing baselines on these augmented samples allows to improve occlusion robustness (Table 6).

    &  &  \\  & Backbone & Static & Dynamic & Static & Dynamic \\  MOC & DLA34 & 14.8 & 12.4 & 26.8 & 31.9 \\ YOWO & ResNext & 16.5 & 10.1 & **37.6** & **35.3** \\ VCAPS & i3D & **32.3** & **21.7** & 20.9 & 17.3 \\  YOWO & Resnet18 & 17.3 & 11.8 & **28.0** & **24.2** \\ VCAPS & Resnet18 & **33.2** & **21.9** & 7.7 & 7.3 \\   

Table 4: **Effect of realistic occluder-motion on OVIS-UCF/OVIS-JHMDB datasets.** Models in general perform better on static occlusions than dynamic occlusions.

Figure 3: **Effect of pretrained weights:** VCAPS-Mvitv2 outperforms all models on both O-UCF and O-JHMDB. (X Axis:) Accuracy without occlusions. (Y Axis): Relative Robustness of Model. Top-Right corner of each plot corresponds to most robust model.

Next, we make some very surprising observations. In Table6, Swapping the backbone of one of the methods (VCAPS) with a transformer (i.e. Mvitv2) yields the highest performance across both UCF/JHMDB. A non-trivial observation is that a transformer- based network trained _without_ augmentation outperforms all the other methods which are trained using _occlusion as an augmentation_ (67.3 vs 49.9 on UCF). Training the transformers further with occlusion-based augmentation improves the performance from 67.3% to 81.6%. This is notable because this performance _on a occluded test set_ (i.e. 81.6%) comes close to how the detector functions in the _absence_ of occlusion,i.e. (83.1% on UCF). The remaining gap of 1.5% can be bridged by explicit occlusion modelling. Therefore, transformers trained with occlusions as a augmentation strategy can significantly improve robustness.

### Importance of Capsules

Here, we explore the importance of capsules towards occlusion modelling. In Tab9a, we remove all the capsule layers from VCAPS-Mvitv2. Note that the architecture then reduces to a standard 3D Unet, and the performance drops from 67.3% to 64.1%. Therefore, capsules are a _crucial component_ for the performance of VCAPS-Mvitv2. 4

**Emergent Object/Occluder separation in capsules:** Next, we qualitatively explore the behaviour of capsule-based models towards occlusions. We choose our best-performing VCAPS-MvitV2 model which has never seen occlusions during training and feed-forward an occluded sample during inference in Fig5. The activation maps of some of the capsules in the primary layers have been visualized. Each capsule has an activation map of size H\(\)W, where each value denotes the confidence about the location of a particular entity (object). Applying a threshold of 0.7 results in a binary mask. It can be seen that some of the capsules selectively look at the objects (i.e. actors), and other capsules focus on occluders (eg, chair, sofa). Note that the capsules have never seen occluders during training, but are still able to focus on them. Therefore, architectural constraints like binding object-specific pixels to particular slots (eg, capsules) allows behaviour like unsupervised object discovery to emerge.

Figure 4: **Effect of number of model parameters: (i) Increasing parameter size within the same model family yields more robustness. (X Axis:) Accuracy without occlusions. (Y Axis): Relative Robustness of Model. Top-Right corner of each plot corresponds to most robust model.**

   &  &  &  &  \\  & & \#Params (M) & Clean & Occ & Clean & Occ & Occ \\   & Resnet18 & 11.7 & 44.7 & 26.9 & 4.4 & 66.1 & 51.8 \\  & YOLO + ResNext & 89 & 48.8 & 31.2 & 7.6 & 85.7 & 63.7 \\  & MVITv2-S & 35 & 69.4 & 48.2 & 9.1 & 81.3 & 68.2 \\   & Resnet18 & 11.7 & 57.7 & 36.1 & 6.4 & 35.2 & 18.3 \\  & ResNext & 89 & 61.2 & 34.7 & 10.8 & 49.3 & 27.7 \\   & MVITv2-S & 35 & **83.1** & **67.3** & **13.1** & **93.1** & **86.8** \\  

Table 5: **Effect of Backbones: Transformers outperform other backbones in less than half the parameters. Clean evaluation is done on UCF-24 and JHMDB-21 test sets.**

**Representational collapse:** Each capsule is meant to represent one entity (object) only. If there are more objects (i.e. occluders) in a video than the capsules in a network, a single capsule gets forced to encode multiple disjoint objects together. This problem can't be solved without increasing the number of capsules further and _retraining_ the network again5 This similar issue arises in the standard object detection literature where the maximum number of objects that a network can detect is equal to the number of object proposals it was originally trained with [5; 9; 57].

### GLOM: Islands of agreement _can emerge_ from token representations.

We qualitatively explore the effectiveness of our VCAPS-Mvitv2 model presented in Tables7,9b. Specifically, we assume that each of the output token representation from Mvitv2 is equivalent to a column-based activity vector . We perform t-sne reduction  of each vector to three dimensions, and linearly project obtained components to the RGB range of \([0,256)\) in Fig6. Note that obtained maps are not simple patch-based correlations among token-representations, but rather lower dimensional clustering of vectors themselves plotted as identical colors. Furthermore, our VCAPS-Mvitv2 has _only_ received actor-level annotations/ action-label supervision and _not_ any form of occluder masks , contrastive-textual supervision or distilled knowledge during training. The only used prior is Kinetics pretrained weights learnt from action-recognition.

This suggests that the lower layers in our VCAPS-Mvitv2 (i.e. transformer layers) perform semantic grouping of pixels into objects (object discovery). Next, higher layers (capsule layers) selectively segregate objects/occluders into instance-specific slots 5. Thus transformers and capsules _complement_ each other to achieve first two stages of the binding process. Finally, our decoder reasons to suppress occluder-specific features and focus/infill on actor-centric regions during localization, i.e. Fig 7. From this section, we conclude that two capsule layers stacked on top of transformer-based backbones can help improve occlusion-robustness. The neuro-symbolic advantage of capsules

    &  &  &  &  &  \\  & & & clean & w/o & w & w/o & w & clean w/o & w \\  MOC & DLA34 & CNN & 54.4 & 39.8 & 44.1 & 3.1 & 5.8 & 77.2 & 35.3 & 55.9 \\ YOWO & YOLO+ ResNext & CNN & 48.8 & 31.2 & 44.9 & 7.6 & 9.0 & 85.7 & 47.8 & 68.0 \\ VCAPS & i3D & CNN & 75.5 & 42.2 & 49.9 & 9.9 & 10.8 & 65.7 & 35.6 & 59.7 \\  VCAPS & Mvitv2 & Transformer & **83.1** & **67.3** & **81.6** & **10.5** & **12.1** & **93.1** & **86.8** & **90.5** \\   

Table 6: **Effect of Augmentation:** Transformer based backbones outperform other CNN based models which have even used occlusion as an augmentation. Clean evaluation is done on UCF-24 and JHMDB-21 test sets. w/o: without augmentation, w: with augmentation.

Figure 5: **Emergent object/occluder separation in capsules. (top left) An occluded video is feed-forwarded through VCAPS-Mvitv2 and activations of primary layer capsules are visualized. Capsules can (i) parse an actor into constituent body parts without instance-level supervision[22; 28] (ii) segment multiple actors & objects of action (volleyball net). (iii) show evidence of focusing on occluders never seen during training (iv) undergo representational collapse, where a single capsule starts representing multiple objects (wholes) when number of objects in the scene are greater than number of capsules in the network. Best viewed in color.**combined with innate natural robustness of transformers forms the basis of our best model, i.e. VCAPS-Mvitv2.

### Token masking

Till now, we have seen that architectures possessing a transformer based backbone with a few capsule layers on top are highly robust to occlusions. Now, we propose token-masking to improve their innate robustness. Given a video of dimensions \(T H W\), a transformer creates \(L\) spatio-temporal patches each of which is projected to a common dimensionality \(D\) during input. Our idea is that randomly masking (blacking out) some of the tokens during training _introduces_ additional occlusion robustness in the model. Mathematically, we generate L iid bernoulli random variables

\[M_{i} Bernoulli(p)\ \  i[1,L]\] (1)

where \(p\) is the masking probability. Each \(M_{i}\) is repeated \(D\) times, so that a particular token \(P\) is fully masked before input. Next, \(M_{L}^{D}\) is multiplied element wise with the input sequence to give final masked input \(I^{{}^{}}=I M\) where \(I\),\(M\) are input sequence vector and masking vector of dimensions \(^{L D}\) respectively, and \(\) is element-wise hadamard operation. During inference, none of the tokens are masked, and the input occluded video sequence is directly feed-forwarded through the network.

**Quantitative Results** In Table 7 it can be seen that such token-masking improves _even_ the performance of models which have used occlusion as a data augmentation from 81.9% to 82.2% on

   &  &  &  &  \\  & & & clean & aug & aug+mask & clean & aug & aug+mask \\  MOC & DLA34  & D & 54.4 & 44.1 & 47.4 & 77.2 & 55.9 & 61.3 \\ YOWO  & ResNext & D & 48.8 & 44.7 & 46.8 & 85.7 & 68.0 & 72.8 \\ VCAPS & i3D  & D & 75.5 & 49.9 & 51.5 & 65.7 & 59.7 & 62.5 \\  VCAPS & Mvitv2  & D & **83.1** & **81.6** & 81.9 & **93.1** & **90.5** & 91.7 \\ VCAPS & Mvitv2  & T & - & - & **82.2** & - & - & **92.4** \\  

Table 7: **Token Masking**: Improves performance of models trained even using occlusion as augmentation.(D)- dropout on intermediate network layers. (T)- proposed token masking

Figure 6: **Emergent islands of agreement on O-JHMDB dataset**: Output token representations of our VCAPS-Mvitv27 can be thought of as column-based vectors. All the vectors belonging to the same object should agree among themselves. Reducing dimensionality of these vectors to three using t-sne shows evidence of such islands of agreement as belonging to identically-colored regions. Note that obtained islands show both occluders as well as actors. Our video-based model is only trained with actor-level annotation/action class labels and no occluder-specific masks.

Figure 7: **Proposed Token Masking** (from left to right) An input video is partitioned into tokens and fed to a transformer. (ii) During training, a random number of tokens are blacked out. (iii) During inference, all the tokens are switched on and the actor is localized under occlusions.

O-UCF and 91.7% to 92.4% on O-JHMDB. Note that for VCAPS(Mvitv2), masking some of the tokens during training performs better than dropping some of the intermediate network neurons. This observation is in line with how learning objectives similar to pixel-level reconstruction in masked image modelling (MIM) yields better self-supervised representations.

**Comparison With existing Methods:** We compare with existing methods on f-mAP and v-mAP. In Tab8, our method obtains \(83.1\) vmAP on UCF-24 thereby indicating most localization robustness. On JHMDB-21, we obtain 92.8% in terms of the absolute v-mAP score, which is significantly better than other existing methods. We acknowledge that TubeR and ST-Mixer  are slightly better than our method in terms of f-mAP scores on UCF-24 dataset.

**Realistic Occlusions:** Furthermore, we evaluate our _baseline_ models and the token-masked models on the Real-OUCF dataset. In table 6, we observe that the models trained with occlusion as augmentation perform _better_ than models without augmentation. As evident in Table (b)b, models trained with our token-masking approach also outperform other models in _realistic_ scenarios.

## 6 Conclusion

We have conducted the _first-ever_ benchmark study to evaluate the impact of occlusions in spatio-temporal action detection. This study provides several interesting key insights including, i) Models perform better on static occlusions vs dynamic occlusions in _realistic scenarios._ ii) Transformer-based models possess greater natural robustness compared to other models using occlusion as data augmentation. iii) Pre-training improves robustness of transformers more than CNN models. (iv) Robustness can be improved further by leveraging components like capsules and training jointly with occlusions as data augmentation. (v) Recipes like simply masking some input tokens of transformers during training can introduce additional robustness and obtain state-of-the-art performance under realistic occlusions. Our benchmark, datasets and code have been released at this link.

**Limitations:** We have only focused on annotating visible regions of the occluded-objects in line with official COCO protocol. Another direction could also focus on predicting _missing_ regions, i.e. amodal-segmentation. Classically, semantic-segmentation has assumed that one pixel can belong to only one object. However, this is not true for occlusions. Inductively, this symmetry issue has been resolved in works like Maskformer2 where multiple objects per pixel can be predicted by removing the softmax assumption. We note that there is still a significant gap left to bridge in existing models for improving robustness to _realistic_ occlusions9b.

   &  &  \\   & Backbone & f-mAP & v-mAP & f-mAP & v-mAP \\ Methods & 2D & 3D & 0.5 & 0.2 & 0.5 & 0.5 & 0.2 & 0.5 \\  _Yang et al._ & ✓ & & 75.0 & 76.6 & - & - & - & - \\ _Li et al._ & ✓ & & 78.0 & 82.8 & 53.8 & 70.8 & 77.3 & 70.2 \\ _Kopuklu et al._ & ✓ & & 80.4 & 75.8 & 48.8 & 75.7 & 88.3 & 85.9 \\ _Zhao et al._* & & ✓ & 81.3 & 85.3 & 60.2 & 82.3* & 81.8 & 80.7 \\ _Duarte et al._ & & ✓ & 78.6 & 97.1 & 80.3 & 64.6 & 95.1 & - \\ _Kumar et al._ & & ✓ & 69.2 & 95.3 & 71.9 & 68.1 & 96.8 & 68.4 \\ _Tao et al._ & & ✓ & **83.7** & - & - & 86.7 & - & - \\  Ours & ✓ & 81.2 & **98.6** & **83.1** & **93.0** & **98.1** & **92.8** \\  

Table 8: **Comparison with existing methods:** Comparison of our method across existing supervised approaches, *: denotes results using a CSN152 backbone.

   & w/o caps & w/ caps \\  VCAPS (MvitV2) & 64.1 & **67.3** \\      & MOC\({}^{}\) & YOWO\({}^{}\) &  VCAPS\({}^{}\) \\ (i3D) \\  & 
 VCAPS\({}^{}\) \\ (Mvitv2) \\  \\  Occ & 7.2 & 9.6 & 11.7 & **14.3** \\  

Table 9: **Ablations and evaluation of proposed token-masking on Realistic Occlusions.**Acknowledgement

This research is based upon work supported in part by the Office of the Director of National Intelligence (Intelligence Advanced Research Projects Activity) via 2022-21102100001 and in part by University of Central Florida seed funding. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the US Government. The US Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.