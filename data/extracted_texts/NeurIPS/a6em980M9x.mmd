# Amortized Fourier Neural Operators

Zipeng Xiao\({}^{1}\), Siqi Kou\({}^{1}\), Zhongkai Hao\({}^{2}\), Bokai Lin\({}^{1}\), Zhijie Deng\({}^{1}\)

\({}^{1}\) Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University

\({}^{2}\) Dept. of Comp. Sci. & Tech., Tsinghua University

{xiaozp_25, happy-karry}@sjtu.edu.cn, hzj21@mails.tsinghua.edu.cn, {19821172068,zhijied}@sjtu.edu.cn

Corresponding author.

###### Abstract

Fourier Neural Operators (FNOs) have shown promise for solving partial differential equations (PDEs). Typically, FNOs employ separate parameters for different frequency modes to specify tunable kernel integrals in Fourier space, which, yet, results in an undesirably large number of parameters when solving high-dimensional PDEs. A workaround is to abandon the frequency modes exceeding a predefined threshold, but this limits the FNOs' ability to represent high-frequency details and poses non-trivial challenges for hyper-parameter specification. To address these, we propose AMortized Fourier Neural Operator (AM-FNO), where an amortized neural parameterization of the kernel function is deployed to accommodate arbitrarily many frequency modes using a fixed number of parameters. We introduce two implementations of AM-FNO, based on the recently developed, appealing Kolmogorov-Arnold Network (KAN) and Multi-Layer Perceptrons (MLPs) equipped with orthogonal embedding functions respectively. We extensively evaluate our method on diverse datasets from various domains and observe up to \(31\%\) average improvement compared to competing neural operator baselines.

## 1 Introduction

Neural operators (NOs) have been extensively studied for their potential in accelerating the solving of partial differential equations (PDEs) in science and engineering fields . In contrast to approaches limited to specific discretizations or PDE instances , NOs characterize the solving operator of a family of PDEs across different discretizations and hence enjoy higher efficiency and usability, e.g., for weather forecasting  and material analysis .

Fourier neural operator (FNO)  and its variants  stand out as a significant subclass of NOs, which explore the convolution theorem and Fast Fourier Transform (FFT) to efficiently performs kernel integral, a central module for the learning operator of PDEs. Typically, FNO separately parameterizes the values of the Fourier-transformed kernel function for different frequency modes, and hinges on frequency truncation--abandons the parameters corresponding to frequencies exceeding some threshold--to reduce modeling costs, particularly for high-dimensional PDEs.

Frequency truncation can be problematic when solving PDE systems with intense high-frequency components. To address this, IFNO dynamically adjusts the threshold for frequency truncation during training, though it still experiences exponential parameter growth with increasing dimensionality . This complexity can result in substantial memory consumption and hinder the development of large-scale pretrained models . Conversely, AFNO  employs a shared MLP to transform the outcomes of FFT, but the uniform treatment of frequency modes may constrain expressiveness and lead to suboptimal performance.

We address this by developing _AMortized Fourier Neural Operator (AM-FNO)_, where we introduce extra neural networks (NNs) to specify the kernel integral operator to amortize the modeling cost. As illustrated in Figure 2, AM-FNO is simple and intuitive--learnable NN transformations are leveraged to directly define the Fourier-space kernel function to accommodate arbitrarily many frequency modes at the cost of a fixed number of parameters. An amortized parameterization also provides an inherent regularization mechanism for resisting overfitting to potential high-frequency noise.

The NN transformation in AM-FNO can be flexibly defined. One natural choice is the Kolmogorov-Arnold Network (KAN) due to its superior accuracy in function fitting . Doing so, we, for the first time, reveal the potential of KAN for the operator learning of PDEs. Considering the widely criticized inefficiency issues of KAN in both time and memory consumption, we also investigate the regular Multi-Layer Perceptrons (MLPs) for amortized parameterization. We empirically identify the necessity of embedding the frequency modes with orthogonal basis functions before MLP transformation.

We experiment on challenging benchmarks governed by diverse typical PDEs, covering six standard PDE benchmarks [18; 17; 30]. AM-FNO (KAN) and AM-FNO (MLP) achieve an average \(22\%\) and \(31\%\) reduction in relative error on these benchmarks, respectively. We also analyze the error of different frequency modes, and the results show that our models outperform in all frequency ranges. Additionally, we perform zero-shot super-resolution on three benchmarks to assess the generalization ability of AM-FNO across discretizations. We observe that AM-FNOs outperform the baselines and achieve lower error even compared to FNO trained on the test resolution. The results reflect that AM-FNOs have the promising potential to substantially reduce both the data collection and training costs for solving high-resolution PDE problems.

## 2 Related Works

**Neural Operators.** Neural operators have attracted considerable interest for their capacity to map infinite-dimensional function spaces [24; 19; 1], thereby facilitating solutions across diverse discretizations without retraining. The pioneering work DeepONet  introduces a trunk and branch network architecture grounded in the universal approximation theorem for operators. Transformer-based neural operators represent a notable line of work in the domain. Galerkin Transformer  proposes self-attention operators that theoretically correspond to a learnable kernel integral operator and projection. OFormer  introduces an architecture with input and query encoders for querying arbitrary output locations. GNOT  proposes a heterogeneous normalized attention layer to encode different input information.

**FNO and its Variants.** Fourier neural operator (FNO)  represents a novel approach with exceptional efficiency and accuracy due to its ability to learn kernel integral operators in the Fourier domain. The theoretical proof establishes its capability to approximate arbitrary continuous operator . Tailored for addressing multiphase flow problems, U-FNO  augments its representation in higher frequency information through U-Net paths. Geo-FNO  extends the applicability of FNO beyond uniform grids by employing a learnable mapping from irregular domains to uniform latent meshes.

Figure 1: Comparison between FNO and AM-FNO: FNO assigns each value at the discretized frequencies of the Fourier-transformed kernel function as a learnable parameter, while AM-FNO utilizes neural network parameterization (MLP or KAN) to approximate the mapping between frequencies and function values. The frequencies are embedded using a set of orthogonal basis functions before being processed by the MLP.

F-FNO  reduces the model parameters and addresses performance degradation with increasing layers by factorizing the integral operator and enhancing the architecture. AFNO  transforms the function values after FFT with a shared MLP for each frequency, effectively reducing the parameter count. FNOs have made notable contributions across various challenging tasks [6; 25; 33; 26]. However, minimizing model complexity while effectively managing high-frequency information and ensuring generalization across different discretizations poses a persistent challenge in FNOs. Additionally, to the best of our knowledge, there has been no attempt to incorporate KANs with neural operators.

## 3 Preliminary

This section presents the foundations of operator learning and the integral operator used in FNO.

### Operator Learning

Consider the input function space \(=(D;^{d_{a}})\) and the target function space \(=(D;^{d_{u}})\) defined on a bounded and open set \(D^{d}\). Operator learning seeks to learn a \(\)-parameterized operator \(_{}\) to approximate the ground-truth mapping \(:\) specified by a PDE. This learning process is based on a finite set of function observations \(\{a_{i},u_{i}\}_{i=1}^{N}\), where functions \(a_{i}\) and \(u_{i}\) are discretized on meshes \(\{x_{j} D\}_{j=1}^{M}\). The optimization problem is:

\[_{}_{i=1}^{N}_{}(a_{i})-u_{ i}\|_{2}}{\|u_{i}\|_{2}},\] (1)

where the regular mean-squared error (MSE) is extended with a normalizer \(\|u_{i}\|_{2}\) to handle scale variations across benchmarks, denoted as \(l_{2}\) relative error. The relative error can also be substituted with other loss functions.

### Fourier Integral Operator

A learnable kernel integral is a central module for defining mappings among functions. Specifically, we denote the hidden state of the input function in the \(l\)-th transformation stage as \(h^{(l)}(y):D^{d_{h}}\), where \(d_{h}\) represents the dimensionality, assumed to be consistent across all stages. The kernel integral operator makes the following transformation:

\[((h^{(l)}))(x)=_{D}(x,y)h^{(l)}(y)dy, x D\] (2)

However, the integral is not computation-friendly within deep learning frameworks. To address this, FNO  assumes the kernel is shift-invariant, i.e., \((x,y)=(x-y)\), and leverages the convolution theorem to efficiently compute the integral in the Fourier domain:

\[((h^{(l)}))(x)=^{-1}(R(h))(x),  x D\] (3)

where \(\) and \(^{-1}\) denote the FFT and its inverse (IFFT), and \(R(k):^{(d_{h} d_{h})}\) represents the Fourier-transformed complex-valued kernel function with \(k\) denoting a frequency mode. Typically, FNO individually parameterizes the values of \(R(k)\) for a fixed range of frequency modes (denoting the number as \(k_{T}\)) to avoid high modeling costs. This, yet, limits the exploration of high-frequency details in the function and poses non-trivial challenges for hyper-parameter specification.

## 4 Method

This section elaborates on amortized FNO (AM-FNO), which amortizes an arbitrary number of frequency modes by sharing a fixed number of parameters.

### Amortized Parameterization

Instead of parameterizing the kernel function point-by-point, we propose to build the mapping between frequency and the values of the Fourier-transformed kernel function using an NN. On one hand, this mitigates the issue that the number of parameters increases significantly with that of frequency modes and dimensionality of PDEs. On the other hand, this approach avoids AFNO's uniform transformation, ensuring richer expressiveness in the Fourier domain.

Concretely, by the rule of Fourier transformation, there is:

\[R(k)=_{D}(x)e^{-2i x k}dx.\] (4)

Note that the output of \(R\) is a matrix, so we can use \(R_{p,q},p,q\{1,2,,d_{h}\}\) to denote the complex scalar-valued function yielding one element of the matrix output. Our AM-FNO directly uses NNs to define the real and imaginary parts of the function \(R_{p,q}\) to maximize the modeling flexibility. Formally, there is

\[R_{p,q}(k):=_{re}(k)+_{im}(k).\] (5)

The detailed implementations of the two NNs have no essential difference, so we only discuss one of them in the following.

### Kolmogorov-Arnold Networks

Kolmogorov-Arnold Networks (KANs) have been empirically shown to show promise in function approximation . According to theoretical analysis, KANs are usually defined as:

\[()=_{j=1}^{2H+1}^{}_{j}(_{t=1}^{H} _{j,t}(x_{t}))\] (6)

where \(^{}_{j},_{j,l}:\) denote the learnable basis functions. In practice, we can generalize the above definition and set a KAN layer as (with \(\) as a \(H\)-dim variable):

\[^{}=_{1,1}()&_{1,2}()&& _{1,H}()\\ _{2,1}()&_{2,2}()&&_{2,H}()\\ &&&\\ _{H^{},1}()&_{H^{},2}()&&_{H^{},H}().\] (7)

We can specify the function by learnable coefficients and multiple local B-spline basis functions :

\[(x)=w(r(x)+(x)),(x)=_{g}c_{g}B_{g}(x)\] (8)

where \(r(x)\) represents a basis function (typically sigmoid linear unit or SiLU ) and \(w\) is a factor controlling the magnitude.

We utilize two two-layer KANs in each layer of AM-FNO to define the real and imaginary parts of \(R_{p,q}\). The inputs to the network are all frequencies of \(h^{(l)}\) after FFT. In fact, we can share weights among the KANs associated with \(R_{p,q}\) with different \(p\) and \(q\). Empirical results (Table 4) indicate superior performance compared to the corresponding MLP implementation.

### Multi-Layer Perceptrons

In practice, KANs require extensive training time. To address this, we propose an alternative parameterization aimed at improving the performance of Multi-Layer Perceptrons (MLPs). Despite its universal approximation ability, MLPs empirically suffer from compromising performance. The spectral bias of vanilla MLPs, i.e., their tendency to favor low-frequency functions, may limit their capacity to represent more complex functions. Motivated by the success of leveraging orthogonal basis functions for function approximation [12; 14; 27], we propose to augment the MLP with orthogonal embedding functions to construct AM-FNO.

Specifically, we can embed the frequency mode input using a set of orthogonal functions before the MLP transformation. The Fourier basis is a natural choice due to its capacity to capture high-frequency components, thereby enhancing the high-frequency representation of vanilla MLPs. However, our empirical results show that Chebyshev basis functions can perform better (see Table 4). Of note, we use only the first \(n_{}\) orthogonal basis functions in the family for the parameterization as we cannot employ infinite parameters.

**Factorization trick for high-dimensional PDEs.** For the Fourier-transformed kernel, the dimensionality of the frequency modes matches that of the PDE being solved. To approximate functions with \(d\)-dimensional inputs, a common approach is to construct \(d\)-dimensional orthogonal functions based on one-dimensional basis functions. Specifically, for \(n_{}\) one-dimensional basis functions in each of the \(d\) dimensions, the complete space contains \(n_{}^{d}\) high-dimensional basis functions, resulting in an exponential complexity for modeling.

Motivated by the dimension factorization in the integral operator , we separate the input dimensions and construct the final kernel by the products of the kernel approximated in each dimension. Such a process is illustrated in Figure 2. In this way, the total parameter count scales linearly w.r.t. the dimension of the PDEs.

### AM-FNO Architecture

In FNO, a stacked structure is employed to approximate the entire mapping \(\), as illustrated below:

\[=^{(L)}^{(1) }\] (9)

where \(\) maps the input function \(a\) to the hidden state \(h^{(0)}\), and \(\) maps the hidden state \(h^{(L)}\) to the output function \(u\), both in a point-wise manner. \(:h^{(l)} h^{(l+1)}\) is the operator layer responsible for the iterative update:

\[(h^{(l)})=(Wh^{(l)}+(h^{(l)})+b)\] (10)

where \(\) represents the kernel integral operator in Equation (3), and \(b\) represents the bias. FNO employs a pointwise linear mapping with \(W\) to enable the propagation of high-frequency information, but this is empirically limited, validated by the results in Table 8.

Given the non-truncated Fourier transform of the kernel function \(R\) in AM-FNO, we employ the operator layer architecture in , which replaces the linear map \(W\) with residual connection :

\[(h^{l})=h^{(l)}+(W_{2}(W_{1}(h^{(l)})+b_{1})+b_{2 }).\] (11)

As shown in Figure 2, we also incorporate activation functions between the operator layers for enhanced flexibility. The resultant model structure is:

\[:=^{(L)}^{(L- 1)}^{(1)}.\] (12)

Figure 2: AM-FNO structure for 2D PDEs: The input function \(a\) is mapped to a higher-dimensional space. Stacked operators and activation functions are applied for function propagation. Within the operator layers, a linear transformation \(R\) is applied to \(h^{(l)}\) after FFT, followed by a feed-forward network (FFN) after the Inverse Fast Fourier Transform (IFFT). The values of \(R\) result from KAN or multiplying the MLP transformations of selected one-dimensional orthogonal basis functions (**w** denotes linear weights.). Finally, the function is projected to the solution dimension space.

## 5 Experiment

In this section, we validate the effectiveness of our proposed method by conducting extensive experiments on challenging benchmarks governed by typical solid and fluid PDEs.

### Experimental Setup

**Benchmarks.** We evaluate the performance of AM-FNO on six well-established benchmarks. These benchmarks include Burger, Darcy, and NS-2D, which are presented in regular grids with varying dimensions . We extend our experiments to assess the method's performance in different geometries, including Pipe, Airfoil, and Elasticity benchmarks  Additionally, we incorporate the compressible fluid dynamics (CFD) 1D and 2D benchmarks , which involves more high-frequency information. The benchmarks are summarized in Table 1.

**Baselines.** We conduct a comparative evaluation of our neural operator against seven baseline methods. These baselines include well-recognized approaches such as FNO  and its variants Geo-FNO , U-FNO , F-FNO  and AFNO . Additionally, we consider other models, including OFormer  and LSM . Notably, LSM represents the latest state-of-the-art (SOTA) neural operator among the baselines.

**Implementation details.** We train all models for 500 epochs using the AdamW optimizer  with a cosine annealing scheduler . The initial learning rate is \(10^{-3}\), and the weight decay is set to \(10^{-4}\). Our models consist of 4 layers with a width of 32 and process all the frequency modes of training data. The Gaussian Error Linear Unit (GELU) is used as the activation function . For AM-FNO (KAN), the number of spline grids is selected from \(\{24,32,48\}\), while for AM-FNO (MLP), the number of basis functions is set to 32 or 48. AM-FNO (MLP) utilizes Chebyshev basis functions as the orthogonal basis functions, as elaborated in the appendix. The batch size is selected from \(\{4,8,16,32\}\), and the experiments are conducted on a single 4090 GPU. The evaluation metric and training loss are based on the \(l_{2}\) relative error in Equation (1), unless otherwise specified. We employ the transformation method from geo-FNO  to map between irregular input domains and uniform meshes for the Elasticity benchmark on point clouds. More details about the baselines can be found in Appendix A.

### Main Results

The main results are shown in Table 2. Our models consistently achieve state-of-the-art (SOTA) performance on all six benchmarks with various PDEs, geometries, and dimensions. AM-FNOs exhibit a significant average performance improvement of \(22\%\) from KAN implementation and \(31\%\) from MLP implementation compared to the top-performing baseline. We provide a comparison of GPU memory and training time in Appendix D, showing that although AM-FNO retains all frequency modes, it achieves comparable memory usage and training time to other FNOs.

AM-FNOs significantly reduce prediction error on Darcy and NS-2D benchmarks, standard benchmarks with strong low-frequency components. Specifically, AM-FNO (KAN) reduces the error by \(39\%\) (2.73e-3) and \(11\%\) (1.40e-2), while AM-FNO (MLP) reduces the error by \(40\%\) (2.80e-3) and \(30\%\) (3.69e-2). This improvement can be attributed to AM-FNO's kernel parameterization, which effectively captures the low-frequency information. Additionally, AM-FNO demonstrates robustness across irregular geometries, with reductions of 5\(\%\) (3.30e-4), 32\(\%\) (1.66e-3) and 7%(1.5e-3) from AM-FNO (KAN), and 12\(\%\) (7.50e-4), 34\(\%\) (1.76e-3) and 10\(\%\) (2.20e-3) from AM-FNO (MLP) in

   Benchmark & PDE & Geometry & \(d\) & \(M\) & \(N_{t}\) & \(N_{}\) & \(N_{}\) \\  Darcy & Darcy flow & Regular grid & 2 & \(85 85\) & - & 1000 & 200 \\ NS-2D & Navier-Stokes & Regular grid & 2 & \(64 64\) & 20 & 1000 & 200 \\ Pipe & Navier-Stokes & Structured mesh & 2 & \(129 129\) & - & 1000 & 200 \\ Airfoil & Euler equation & Structured mesh & 2 & \(221 51\) & - & 1000 & 200 \\ Elasticity & Elastic Wave & Point cloud & 2 & 972 & - & 1000 & 200 \\ CFD-1D & Compressible Navier-Stokes & Regular grid & 1 & 128 & 21 & 1800 & 200 \\ CFD-2D & Compressible Navier-Stokes & Regular grid & 2 & \(64 64\) & 21 & 1800 & 200 \\   

Table 1: Overview of benchmarks including their spatial dimensions \(d\), spatial resolution \(M\), temporal resolution \(N_{t}\), and training data \(N_{}\) and test data \(N_{}\).

prediction error on Airfoil, Pipe and Elasticity benchmarks. For CFD-1D and CFD-2D benchmarks characterized by stronger high-frequency components, AM-FNO (KAN) achieves improvements of 25\(\%\) (6.10e-3) and 40\(\%\) (1.83e-3), while AM-FNO (MLP) achieves improvements of 40\(\%\) (9.70e-3) and 52\(\%\) (2.36e-3). The promotion highlights the effectiveness of our method of handling high-frequency components. We also find that AM-FNO (MLP) outperforms AM-FNO (KAN) across all benchmarks, likely due to the enhanced expressiveness of the orthogonal embedding.

### Frequency-Based Error Analysis

**CFD-1D.** To assess the performance across various frequency modes, we calculate the error of different frequency modes after FFT on CFD-1D benchmark and incorporate FNO without truncation (FNO\({}^{+}\)) for comparison. The results are visualized in Figure 3.

As shown, the errors primarily stem from the first few frequency modes and decrease as the frequency increases. The baselines exhibit similar errors in the truncated frequency range, whereas our models demonstrate significantly lower errors. Our models maintain an advantage over the baselines in the initial modes within the truncated frequency range. As the frequency increases, the strength of the high-frequency components diminishes, and all the errors become negligible.

**CFD-2D.** We further evaluate the performance across various frequency modes with the metrics outlined in  (detailed in Appendix B) and present the results on CFD-2D benchmark in Table 3.

The results showcase that FNO\({}^{+}\) demonstrates lower train error but higher test error than FNO and U-FNO. Notably, FNO\({}^{+}\) exhibits similar prediction errors to FNO in the high-frequency range, suggesting potential overfitting to the training data. We propose that the substantial complexity parameterization of FNO\({}^{+}\) may render it sensitive to high-frequency details, limiting the effectiveness of the additional parameters in handling such components. The fL2 error of U-FNO across all frequency ranges is lower than FNO, which can be attributed to the enhanced expressiveness enabled by the additional U-Net architecture. AM-FNO (MLP) achieves the lowest training error and fL2 error

   Model & Darcy & NS-2D & Pipe & Airfoil & Elasticity & CFD-1D & CFD-2D \\  FNO & 1.08e-2 & 1.56e-1 & - & - & - & 2.93e-2 & 5.36e-3 \\ AFNO & 3.17e-2 & 2.17e-1 & 1.72e-2 & 9.88e-3 & 4.57e-2 & - & 6.72e-2 \\ Geo-FNO & 1.08e-2 & 1.56e-1 & 6.70e-3 & 1.38e-2 & 2.29e-2 & 2.93e-2 & 5.36e-3 \\ OFormer & 1.24e-2 & 1.71e-1 & 9.59e-3 & 1.83e-2 & - & - & - \\ U-FNO & 1.24e-2 & 1.22e-1 & 5.76e-3 & 1.05e-2 & 2.26e-2 & 2.44e-2 & 4.52e-3 \\ F-FNO & 9.92e-3 & 1.74e-1 & 5.99e-3 & 1.00e-2 & 3.16e-2 & 2.54e-2 & 7.86e-3 \\ LSM & 7.01e-3 & 1.64e-1 & 5.20e-3 & 6.39e-3 & 2.25e-2 & - & 7.62e-2 \\ Ours (KAN) & 4.28e-3 & 1.08e-1 & 3.54e-3 & 6.06e-3 & 2.10e-2 & 1.83e-2 & 2.70e-3 \\ Ours (MLP) & **4.21e-3** & **8.51e-2** & **3.44e-3** & **5.64e-3** & **2.03e-2** & **1.47e-2** & **2.16e-3** \\   

Table 2: Comparison of the primary findings across six benchmark tests with six baseline methods. Lower scores signify superior performance, with the best outcome highlighted in bold and the second-best outcome underlined. The presence of a “-” indicates that the corresponding baseline is incapable of addressing the benchmark.

Figure 3: Comparison of L2 norm error on different frequency modes on CFD-1D benchmark.

across all frequency ranges, while AM-FNO (KAN) achieves the second lowest fL2 error. Specifically, AM-FNO (MLP) and AM-FNO (KAN) achieve 55%(7.04e-2) and 50%(5.15e-2) reduction in the high-frequency range. This outcome can be attributed to our amortized parameterization, which significantly reduces model complexity while maintaining adequate expressiveness to approximate the Fourier-transformed kernel function.

### Ablation Experiments

We conduct a detailed ablation study to assess the effectiveness of different components and hyperparameters of our models.

**Necessity of Orthogonal Embedding.** We study the impact of orthogonal embedding on Darcy, Airfoil, and Pipe benchmarks. Table 4 presents the findings. Although AM-FNO (KAN) outperforms the MLP version without embedding (Non), its efficiency is significantly lower than that of versions utilizing MLPs. Removal of the orthogonal embedding leads to a notable performance decline across all three benchmarks. Meanwhile, replacing the orthogonal functions with non-orthogonal polynomial basis functions results in the highest prediction error among the baseline models. These outcomes showcase the efficacy and indispensability of the orthogonal embedding. To validate the robustness of the embedding, we use triangular basis functions and observe comparable errors on the Darcy and Pipe benchmarks, but higher errors on the Airfoil benchmark. We attribute this difference to the accuracy of function approximation achieved by Chebyshev basis functions.

**Influence of Some Hyperparameters** We conduct experiments to assess how prediction error varies with different numbers of basis functions, hidden sizes of KANs, and grid sizes of splines (linearly scaled with local spline count) in KANs. Figure 4 shows the results. The left figure illustrates that the error decreases with an increasing number of basis functions in orthogonal embedding on both benchmarks. This reduction is particularly evident initially on the Airfoil benchmark, followed by diminishing returns. In this case, employing 24 basis functions achieves a favorable balance between efficiency and accuracy. This performance enhancement can be attributed to the increased

    & Param & Mem & Time &  & Airfoil & Pipe \\  & (M) & (MB) & & & & \\  TBF & 1.14 & 1890 & 2.61 & **4.13e-3** & 7.60e-3 & 3.80e-3 \\ PBF & 1.13 & 1890 & 2.55 & 1.77e-2 & 1.30e-2 & 1.03e-2 \\ Non & 1.10 & 1826 & 2.70 & 1.29e-2 & 7.21e-3 & 7.84e-3 \\ Ours (MLP) & 1.14 & 1890 & 2.52 & 4.21e-3 & **5.64e-3** & **3.44e-3** \\ Ours (KAN) & 1.56 & 2230 & 4.70 & 4.28e-3 & 6.06e-3 & 3.54e-3 \\   

Table 4: Comparison of the \(l_{2}\) relative error for different components of AM-FNO (MLP) on Darcy, Airfoil, and Pipe benchmarks. Chebyshev basis functions are substituted with triangular basis functions (TBF) and non-orthogonal polynomial basis functions (PBF). A version of the model without orthogonal embedding (Non) is included for comparison. The training time and memory requirements are derived from the Airfoil benchmark.

    &  &  \\   & & Train Err. & Test Err. & fL2 low & fL2 mid & fL2 high \\  FNO & 2.37 & 2.57e-3 & 5.36e-3 & 1.51e-3 & 7.66e-1 & 1.49e-1 \\ FNO\({}^{+}\) & 18.39 & 1.63e-3 & 5.81e-3 & 1.82e-3 & 7.31e-1 & 1.42e-1 \\ U-FNO & 2.66 & 2.00e-3 & 4.52e-3 & 1.26e-3 & 6.55e-1 & 1.28e-1 \\ Ours (KAN) & 2.21 & 1.79e-3 & 2.70e-3 & 7.78e-4 & 4.19e-1 & 7.65e-2 \\ Ours (MLP) & 2.29 & **1.34e-3** & **2.16e-3** & **6.22e-4** & **3.55e-1** & **5.76e-2** \\   

Table 3: Comparison of the error in different frequency regions on CFD-2D benchmarks. Each complex-valued parameter is considered as 2 in the parameter count (Param). Train error (Train Err.) and test error (Test Err.) are evaluated using the \(l_{2}\) relative error at each time step. fL2 signifies the \(l_{2}\) relative error in Fourier space (fRMSE) pertaining to the low, middle, and high-frequency regions. FNO\({}^{+}\) refers to FNO without frequency truncation.

expressiveness from the additional orthogonal basis functions. In the middle figure, while the error decreases with increasing hidden size, we observe a decline in performance compared to KANs with larger grid sizes but smaller hidden sizes. This may suggest that the expressive power of KANs is primarily derived from the number of local spline functions (grid size). The right figure displays a trend similar to the left one: a noticeable decrease initially, followed by a less pronounced reduction later. In this study, we suggest increasing the grid size to enhance performance rather than focusing on adjusting the hidden layer size. We also present the performance of AM-FNOs, retaining the same frequency modes as other FNOs, in Table 9. AM-FNOs consistently outperform baseline models, underscoring the advantages of our amortized parameterization over the standard FNO approach.

### Zero-Shot Super-Resolution

A notable characteristic of neural operators is their ability to generalize across various discretizations. We conduct experiments training on lower resolution data and evaluate on higher resolution data on NS-2D benchmark. We visualize the results in Figure 5 and provide the numerical results in Table 5.

U-FNO exhibits a significant performance degradation when evaluated on higher resolution. This decline can be attributed to the convolutional operation in the U-Net architecture, which possesses a fixed receptive field and cannot effectively generalize across different discretizations. In contrast, FNO and AM-FNOs demonstrate the ability to generalize across different discretizations. Notably, AM-FNOs achieve superior performance at \(64 64\) resolution compared to baselines trained with the same resolution (see Table 2), which underscores the data efficiency of AM-FNOs.

Figure 4: \(l_{2}\) relative error varies w.r.t. the number of basis functions (Left), hidden layer size of KANs (middle), and grid size of KANs (right) on Darcy and Airfoil benchmarks.

Figure 5: Comparison of zero-shot super-resolution absolute errors on NS-2D benchmark. The top row displays the ground truth, FNO, U-FNO, AM-FNO (KAN), and AM-FNO (MLP) predictions from left to right. The bottom row illustrates their errors.

   M & FNO & U-FNO & Ours (KAN) & Ours (MLP) \\  \(32 32\) & 1.32e-1 & 1.39e-1 & 9.84e-2 & 8.79e-2 \\ \(64 64\) & 1.28e-1 & 2.18e-1 & 9.96e-2 & 8.73e-2 \\   

Table 5: Comparison of \(l_{2}\) relative error across different resolutions on NS-2D benchmark, with all models trained with \(32 32\) resolution.

Conclusion

This paper proposes AM-FNOs to improve Fourier neural operator (FNO)'s efficiency in addressing PDEs without frequency truncation. Our approach utilizes Kolmogorov-Arnold Networks (KANs) and Multi-Layer Perceptrons (MLPs) with orthogonal embedding functions to mitigate exponential complexity and overfitting to high-frequency noise. Comprehensive experiments across various datasets demonstrate the effectiveness of AM-FNOs compared to baseline approaches.

**Limitations.** This work attempts to enhance FNO's handling of high-frequency information but has the following limitations. The benchmarks used are idealized physical systems, excluding real-world complex problems. Meanwhile, although AM-FNOs reduce the parameter count, the extremely high-dimensional PDEs still pose challenges for FNOs due to the complexity of FFT.