# Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers

Lorenzo Tiberi\({}^{1,2}\)   Francesca Mignacco\({}^{3,4}\)   Kazuki Irie\({}^{1,2}\)   Haim Sompolinsky\({}^{1,2,5}\)

\({}^{1}\)Center for Brain Science, Harvard University, Cambridge, MA, USA

\({}^{2}\)Kempner Institute for the Study of Natural and Artificial Intelligence,

Harvard University, Cambridge, MA, USA

\({}^{3}\)Graduate Center, City University of New York, NY, USA

\({}^{4}\)Joseph Henry Laboratories of Physics, Princeton University, NJ, USA

\({}^{5}\)Edmond and Lily Safra Center for Brain Sciences,

Hebrew University of Jerusalem, Jerusalem, Israel

ltiberi@fas.harvard.edu, fmignacco@princeton.edu

kirie@fas.harvard.edu, hsompolinsky@mcb.harvard.edu

###### Abstract

Despite the remarkable empirical performance of transformers, their theoretical understanding remains elusive. Here, we consider a deep multi-head self-attention network, that is closely related to transformers yet analytically tractable. We develop a statistical mechanics theory of Bayesian learning in this model, deriving exact equations for the network's predictor statistics under the finite-width thermodynamic limit, i.e., \(N,P\), \(P/N=(1)\), where \(N\) is the network width and \(P\) is the number of training examples. Our theory shows that the predictor statistics are expressed as a sum of independent kernels, each one pairing different _attention paths_, defined as information pathways through different attention heads across layers. The kernels are weighted according to a _task-relevant kernel combination_ mechanism that aligns the total kernel with the task labels. As a consequence, this interplay between attention paths enhances generalization performance. Experiments confirm our findings on both synthetic and real-world sequence classification tasks. Finally, our theory explicitly relates the kernel combination mechanism to properties of the learned weights, allowing for a qualitative transfer of its insights to models trained via gradient descent. As an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads that are deemed less relevant by our theory.1

## 1 Introduction

In recent years, transformer models based on multi-head self-attention layers [1; 2; 3; 4; 5; 6] have achieved remarkable performance at natural language processing and vision tasks [7; 8; 9]. Yet, theoretical characterizations accounting for the success of these architectures remain sparse. Two fundamental questions remain to a large extent unsolved: First, interpretability--how can we discern task-relevant structures within the learned weights? Second, generalization--what specific aspects of the transformer architecture are responsible for their effective learning? We posit that one important feature of transformers is the combination of layer-wise multi-head organization with depth. This provides the network with a large number of _attention paths_, defined as specific sequences of heads through the attention layers. Their _interplay_ is still poorly understood by deep learning theory.

In most cases, theoretical characterizations of transformers' expressivity , inductive bias [11; 12], generalization [13; 14; 15] and training dynamics [16; 17; 18] rely on simplifying assumptions on the network architecture. A characterization of attention paths is inaccessible in these models, either because attention paths are not defined in the first place, as in models consisting of a single-head [19; 15], a single-layer [10; 11], or both [12; 13; 14; 16; 18; 20], or because the interplay between paths cannot be fully described due to constraints imposed on the learnable weights . A few works consider a multi-head, multi-layer architecture [21; 22; 23; 24], but address different questions than the present study, such as expressivity, generalization bounds, or phenomenological models. Further details on these and analogous works are discussed in Appendix I.

One characterization of the complete transformer architecture has been obtained in the Bayesian framework under the _infinite-width_ thermodynamic limit \(N\) (and infinite number of heads \(H\)) [25; 26], an actively studied regime in which neural networks become equivalent to Gaussian processes (GP) [27; 28]. However, the attention paths interplay is lost in this limit because the network's hidden weights remain statistically independent after learning. This limitation can be overcome by considering the _finite-width_ thermodynamic limit [29; 30; 31; 32], where also the number of examples \(P\) such that \(P/N^{+}\). In this regime, for example, multi-gated deep networks showcase task-relevant interplay between gates, mediated by the learned weights .

In this work, we apply the statistical mechanics theory of finite-width networks to a deep multi-head self-attention model, which closely mimics the attention paths interplay in transformers, while remaining analytically tractable. Our main contributions can be summarized as follows:

* We derive exact equations for the predictor statistics under Bayesian learning of the network's value weights, at fixed query and key weights.
* We shed light on the interplay between attention paths by uncovering a _task-relevant kernel combination_ mechanism, emerging beyond the GP limit (\(>0\)). This constructs the network's mean predictor as an optimally weighted sum of many "path-path kernels", defined as similarity matrices between pairs of attention paths, thereby improving generalization.
* We provide interpretability to this mechanism, by directly relating it to the magnitude and correlations developed by the learned weights. This allows our insights to be transferred outside the Bayesian framework, to networks trained with gradient descent. As an application, we show that a trained network can be reduced in size with minimal performance loss, by pruning those attention paths that are deemed less relevant by our theory.
* We corroborate our findings on both synthetic and real-world sequence classification tasks, illustrating the two main benefits of kernel combination: task-relevant weighting and correlation of the attention paths, respectively.

## 2 Model

We consider a transformer-like  architecture consisting of a linear input projection layer; \(L\) multi-head self-attention (MHA) layers, each having \(H\) attention heads; and a linear readout layer. The network input \(x^{N_{0} T}\) is a sequence of \(T\) tokens \(x_{t}^{N_{0}}\), with token index \(t\{1,,T\}\), and dimension \(N_{0}\). The input projection layer performs the transformation

\[x_{t}^{(1)}=}}V^{(0)} x_{t}, V^{(0)} ^{N N_{0}},\] (1)

where \(N\) is the hidden layers' width. With the operator "\(\)" we denote matrix-matrix or matrix-vector multiplication. The \(\)-th MHA layer with index \(\{1,,L\}\) performs the transformation

\[x_{t}^{(+1)}=}_{h=1}^{H}_{s=1}^{T}V^{()h}  x_{s}^{()}_{st}^{()h}, V^{()h}^{N N},\] (2)

where, for each head \(h\), we define the attention matrix \(^{()h}^{T T}\) with matrix elements

\[_{st}^{()h}=(}x_{s}^{} W_{ K}^{()h} W_{Q}^{()h} x_{t}), W_{Q}^{( )h},W_{K}^{()h}^{G N_{0}}.\] (3)Here \(\) is the softmax function, applied along the direction of the token index \(s\), while \(G\) is the dimension of the query-key feature space. The linear readout returns the scalar output

\[f=}a x_{t^{*}}^{(L+1)}, a^{1 N}.\] (4)

Here \(x_{t^{*}}\) can stand for different options for reducing the token dimension at readout, namely reading from a specific token \(t^{*}\) or averaging over all tokens (\(x_{t^{*}} x_{t}\)). The network's learnable parameters are the input projection weights \(V^{(0)}\); the value, query and key weights \(\{V^{()h},W_{Q}^{()h},W_{K}^{()h}\}_{,h=1}^{L,H}\); and the readout weights \(a\).

**Comparison to the Standard Transformer.** The above architecture presents two main simplifications w.r.t. the standard transformer. First, the network is linear in the value weights, while the standard transformer has a nonlinear feedforward block after each MHA layer. Second, in any layer \(\), the attention (Eq. 3) is always computed as a direct function of the bare input \(x\), rather than the processed input \(x^{()}\). These simplifications allow us to apply back-propagating kernel renormalization (BPKR) techniques [29; 33], enabling the characterization of the network beyond the GP limit. Despite these simplifications, the insights gained by going beyond the GP limit are substantial: we will show that, in the finite-width regime, an important mechanism--_task-relevant kernel combination_--emerges, accounting for a considerable improvement in generalization performance.

**Attention paths formulation.** Note that, despite the linearization in the value weights, the network is still highly nonlinear in the input, thanks to the attention operation (Eq. 3). This can be seen by the following equivalent description of the network (Fig. 1(a)). We introduce the concept of _attention paths_, by defining a path "index" \((h_{1},h_{2},,h_{L})\), where \(h_{1},,h_{L}\{1,,H\}\), which uniquely identifies each possible combination of the head indices across layers, i.e., each possible path through the attention heads. The network output can be rewritten as

\[f=NN_{0}}}_{}V^{()} V^{( 0)}^{}\] (5)

where \(\) is the set of all possible paths, and we define the "_effective weights_" as

\[V^{()}}}a V^{(L)h_{L}} V ^{(L-1)h_{L-1}} V^{(2)h_{2}} V^{(1)h_{1}}, V^{( )}^{1 N}\] (6)

and the "_attentioned input_" as

\[^{}_{t_{0},,t_{L-1}=1}^{T}x_{t_{0}}_{t_{0}t_{1 }}^{(1)h_{1}}_{t_{1}t_{2}}^{(2)h_{2}}_{t_{L-2}t_{L-1}}^{(L-1) h_{L-1}}_{t_{L-1}t^{*}}^{(L)h_{L}},^{}^{N_{0}}.\] (7)

Figure 1: **Scheme of the model and theory (a) Scheme of the model in terms of attention paths. (b) The order parameter assigns to each pair of paths a weight, given by the overlap between the corresponding effective weights. (c) Alignment of the kernel PCs with the vector of task labels \(Y\), in the finite-width (FW) vs GP regimes. (d) Kernel as the weighted sum of many path-path kernels. Task-relevant kernel combination occurs in the finite-width regime (FW), but not in the GP limit, in which cross-path kernels are discarded, and same-path kernels are equally weighted. The result is an improved kernel-task alignment in the finite-width regime (shown in (c)), enhancing generalization.**

In Eq. (5), the network can be seen as a deep linear network applied to a nonlinearly expanded input--the attentioned input \(^{}\). Through Eq. (7), we can see that the bare input \(x\) is nonlinearly expanded from an \(N_{0}\)-dimensional space to an \(N_{0}H^{L}\)-dimensional space, by means of \(H^{L}\) nonlinear operations: one for each attention path.

The goal of our theory is to understand how the network learns to combine these different attention paths, by means of the effective weights (Eq. 6). Note that the network also has other learnable parameters: the query and key weights, which parameterize the nonlinear expansion of the input to \(\). The learning of these parameters is not described by our theory. As we will see in Sec. 3, our theory characterizes the learned effective weights (Eq. 6) for a given, fixed realization of the query and key weights.

## 3 Theory

A fundamental quest of deep learning theory is to understand how deep neural networks, which are often overparameterized, manage to avoid overfitting, achieving good generalization performance [34; 35]. One important role is played by the specific choice of network architecture, which can impose an inductive bias towards better generalizing configurations of parameters, among the many that fit the training data. To study this problem, we adopt the Bayesian framework. Given a dataset of \(P\) example-label pairs \(\{x^{},y^{}\}_{=1}^{P}\), we seek to characterize the Bayesian posterior [36; 37; 38], or _Gibbs distribution_, over the parameters \((V^{(0)},\{V^{()h}\}_{,h=1}^{L,H},a)\)

\[p()\{-}_{=1}^{P} [f(x^{},)-y^{}]^{2}-} \|\|^{2}\}\,.\] (8)

Here \(f(x^{},)\) is the network output (Eq. 4) corresponding to the input \(x^{}\), where we emphasize its dependence on \(\), \(\|\|\) is the Frobenius norm, \(^{2}\) is the variance of the weights' Gaussian prior (set to \(=1\) throughout this paper), and \(>0\) is the error variance, or _Gibbs temperature_ (not to be confused with the number of tokens \(T\)). Characterizing the Gibbs distribution allows to gain insights into the inductive bias imposed by the network architecture. Indeed, note that, in overparameterized networks, the Gibbs distribution for \( 0^{+}\) describes the statistics of those parameter configurations that perfectly fit the training data, with a bias towards small weights induced by the Gaussian prior. These statistics depend on the choice of network architecture, which can therefore bias the distribution towards better generalizing parameter configurations. For \(>0\), parameter configurations that do not achieve perfect fitting are also allowed, which can help to prevent overfitting.

Note that, as discussed at the end of Sec. 2, we characterize the statistics of the weights \(\) (the linear projection, value, and readout weights) for a fixed realization of the query and key weights. The fixed query and key weights can be given, for example, by pre-training the network with gradient descent, or by some task-informed initialization. In Sec. 4.2 we will show that the insights gained by our theory on the weights \(\) can also be applied to the network trained with gradient descent on all of its learnable parameters, including the query and key weights.

The main theoretical result of this work is an expression for the expectation \([f(x^{*})]\) of the network's output on a new test example \(x^{*}\), under the Gibbs distribution (Eq. 8). In Sec. 3.1 below, we provide this formal result, accompanied by a sketch of its derivation and a discussion of the significance of the infinite-dimensional--or thermodynamic-- limit under which our result is derived. In Sec. 3.2 we discuss the result's interpretation and its insights into the network's generalization capabilities.

### Statement of theoretical results

**Definitions**. Consider a training dataset consisting of \(P\) inputs \(x^{}^{N_{0} T}\) and associated labels \(y^{}\), where \(=1, P\). Call \(X\{x^{}\}_{=1}^{P}\) the set of training inputs and \(Y^{P}\) the vector of training labels with \(\)-th component \(y^{}\). Consider a network defined by Eqs. (1-4) and in particular call \(f^{*}\) the network output (Eq. 4) corresponding to a test input \(x^{*}^{N_{0} T}\).

**Assumptions**. Assume the query and key weights \(\{W_{Q}^{()h},W_{K}^{()h}\}_{,h=1}^{L,H}\) are fixed, while all other weights \((V^{(0)},\{V^{()h}\}_{,h=1}^{L,H},a)\) are distributed according to the Bayesian posterior distribution defined in Eq. (8). Assume the "thermodynamic limit" \(N,N_{0},P\), with \(P/N^{+}\) and \(P/(N_{0}H^{L})_{0}^{+}\), where \(\), \(_{0}\) as well as other size parameters \(T,H,L\) are finite.

**Result 1**. The mean predictor under the posterior distribution (Eq. 8) is given by

\[[f^{}]=k^{}(K+ )^{-1}Y,\] (9)

The vector \(k^{P 1}\) and the matrix \(K^{P P}\), called training kernel, are defined in terms of a kernel function \(:^{N_{0} T}^{N_{0} T} \) as \(k^{}:=(x^{},x^{})\) and \(K^{}(x^{},x^{})\), for \(,=1,,P\). The kernel function is given by

\[(x,x^{})=}_{,^{} }U^{^{}}C_{^{}} C_{ ^{}}}^{}(x)^{} ^{^{}}(x^{})\,,\] (10)

where \(^{}(x)\) is the "attentional input" corresponding to an input \(x^{N_{0} T}\), along path \(\), as defined in Eq. (7). The kernel function depends on a positive semi-definite matrix \(U^{H^{L} H^{L}}\), called _order parameter_, which is given by

\[U=*{argmin}_{}S(;X,Y)  S(U;X,Y)=-(U)+(U;X,Y)\,,\] (11)

The scalar function \(S\), called the _action_, consists of an "_entropy_" term \(\), and an "_energy_" term

\[(U;X,Y)=(K(U;X)+ )+Y^{}(K(U;X)+)^{- 1} Y,\] (12)

where \(K(U;X) K\) is the training kernel matrix. The expression for the entropy \(\) is lengthy and is given in Appendix B.1. In the special case of \(H=1\), \(U\) is a scalar, and \((U)=-^{-2(L+1)}U+(U)\). For general \(H\), the entropy \((U)\) is always maximized by \(U^{^{}}=^{2(L+1)}_{,^{}}\), which therefore is the solution of Eq. (11) in the GP limit defined by \( 0^{+}\).

**Result 2**. The matrix \(U\) obeys the following relation

\[U^{^{}}=[V^{()} V^{( )^{}}]\] (13)

where \(V^{()}^{1 N}\) are the effective weights along path \(\), defined in Eq. (6).

**Derivation.** See Appendix II.

The derivation, which uses the BPKR technique , can be sketched as follows. Computing \([f^{}]\) under the posterior distribution \(p()\) involves evaluating a high-dimensional integral in the weights \(\). The idea is to first reduce this computation into an integration over a lower-dimensional,'macroscopic' variable \(U\). Importantly, while \(\) becomes infinite-dimensional as \(N\), \(U\) remains finite-dimensional. The reduced integral is an expectation of the r.h.s. of Eq. (9), treated as a function of \(U\), under the distribution \(p(U)\{-NS(U)\}\), where \(S\) is the action defined in Eq. (11). Then, this integral can be solved in the thermodynamic limit \(N\), using the saddle-point method, which implies evaluating Eq. (9) at the \(U\) that minimizes the action (cf. Eq. 11). Crucially, the end result is fully characterized by this low-dimensional quantity \(U\), commonly called _order parameter_ in physics, which has a direct interpretation in terms of the network weights, given by Eq. (13).

In practice, the results obtained in the thermodynamic limit represent a good approximation also for the case of large but finite \(N\). In this regard, the scaling of other hyperparameters with \(N\) is of particular importance, especially the number of training examples \(P\). In the GP limit, one considers \(P\) finite. This is also called the _infinite-width_ thermodynamic limit because in practice, for a given and typically large \(P\), it is a good approximation only for very wide networks, when \(N P\). In contrast, here we consider the _finite-width_ limit in which \(P/N=^{+}\) (which includes the GP limit for \( 0^{+}\)). As can be seen from Eq. (11), the action gains a new term for \(>0\), which, as we shall discuss below, is fundamental to account for the learning of an attention paths interplay. Finally, we note that in our numerical experiments (Sec. 4) we will consider Bayesian networks which are overparameterized, i.e. \(P<N_{0}H^{L}\), which is the network capacity at fixed query and key weights.

### Results interpretation and implications for generalization capability

Eq. (9) is a commonly found expression in thermodynamic theories of Bayesian learning, relating the network's mean predictor to kernel regression. In particular, the theory of kernel regression  suggests that generalization improves when the training kernel \(K\) is well aligned with the task, meaning its largest principal components (PCs) are well aligned with the vector of training labels \(Y\).

Our result for the transformer's kernel (Eq. 10) enables insights into how the transformer architecture favors this kernel-task alignment (Fig. 1(d)). The kernel consists of the sum, weighted by the order parameter \(U\), of many _path-path kernels_\(C_{^{}}\), each computing the similarity between the attentioned input on two attention paths \(\) and \(^{}\). A notable property of the multi-head architecture is that, despite the number of attention heads growing only linearly with the depth \(L\), the number of attention paths grows exponentially \( H^{L}\). Therefore, the network has at its disposal an exponentially large number of path-path kernels, which it can learn, through \(U\), to optimally combine into a total kernel with improved task alignment.

This phenomenon, which we term _task-relevant kernel combination_, is indeed predicted by our results Eqs. (11-12). These state that the learned \(U\) minimizes a function \(S\) (Eq. 11), which, through the energy term \(\), favors kernel-task alignment. This can be seen by interpreting the energy term (Eq. 12) as the negative log-likelihood of the training labels \(Y\) under a centered Gaussian distribution, whose covariance matrix is the training kernel \(K\). This negative log-likelihood can be minimized by aligning the largest PCs of the covariance (i.e. the kernel \(K\)) as much as possible with \(Y\) (Fig. 1(c)).

In contrast, in the GP limit \( 0^{+}\), the action \(S\) (Eq. 11) consists only of the entropy term \(\), which does not contain any task relevant information. Its only effect is to attract \(U\) towards the GP limit solution \(U^{^{}}=^{2(L+1)}_{,^{}}\). Note that, in this limit, the benefits of kernel combination are lost (Fig. 1(d), bottom line): First, out of all the path-path kernels \(C_{^{}}\), only the _same-path kernels_ (\(=^{}\)) are used, while the _cross-path kernels_ (\(^{}\)) are discarded; Second, all same-path kernels are weighted equally, without making use of any task-specific information. Note that this is true not only for our simplified model, but also for the full transformer architecture under its known GP limit . A task-relevant kernel combination can therefore only emerge beyond the GP limit, in the finite-width regime \(>0\) studied in this work.

Finally, our result Eq. (13) relates the order parameter to a macroscopic measure of the network weights, allowing for a direct interpretation of the kernel combination mechanism: correlating the effective weights across paths allows the network to make use of cross-path kernels, while controlling their magnitude allows to weigh the different path-path kernels in a task-relevant manner.

## 4 Experiments

To corroborate our theoretical results, we "train" our model (Eqs. 1-4) by sampling its weights \(\) (i.e. all weights except the fixed query and key weights) from the posterior distribution Eq. (8), using Hamiltonian Monte Carlo sampling (see Appendix F for details). We consider the following two tasks: hidden Markov chain (HMC) classification, and one-shot image classification by in-context learning. The first task is defined on a synthetic dataset. Its purpose is to have a minimal, controllable setting to illustrate the effects of task-relevant kernel combination. In the second task, we will proceed to show analogous effects on classic image datasets (Omniglot , MNIST , and FashionMNIST ), and compare these results with those obtained from the same network trained with standard gradient descent on all of its parameters (i.e. including the query and key weights).

### Hidden Markov chain sequence classification

**Task definition.** The HMC classification task is defined as follows (Fig. 2(a)). The \(\)-th example in the dataset corresponds to an hidden Markov chain \(q_{1}^{},,q_{T}^{}\) of length \(T=30\), alternating between two hidden states, \(q_{t}^{}\{+,-\}\). The probability of transition to the opposite state (\(\)) is \(p^{}\). The \(\)-th chain can belong to one of two classes, labeled \(y^{}= 1\), depending on whether \(p^{}=0.3\) or \(p^{}=0.7\), respectively. The input tokens are an noisy, higher dimensional representation of the hidden states. These are given by \(x_{t}^{}=v_{q_{t}^{}}+_{t}^{}\), where \(v_{}^{N_{0}}\) are two orthogonal feature vectors corresponding to the states "\(\)", with \((1)\) entries, while \(_{t}^{}\) is a zero-mean Gaussian noise, with \(_{t}^{}_{t^{}}^{^{}}=_{,^{}}_{t,t^{}}(_{1}^{2}P_{}^{} P_ {}+_{}^{2}P_{}^{} P_{})\), where \(P_{}\) and \(P_{}\) are the projectors along thesubspace parallel or perpendicular to the plane spanned by \(v_{+}\) and \(v_{-}\). Unless specified, \(_{}=_{}=1\). The separate parameterization of the parallel (\(_{}\)) and perpendicular (\(_{}\)) noise strengths is motivated by their distinct effect on task performance: while the first corrupts information about the underlying hidden states, inevitably putting an upper bound on the classification accuracy, the second can always be filtered out by learning appropriate weights. We use \(P=100\) examples for training. We test the network performance in terms of the classification accuracy \(A=}_{}_{y^{},( f^{} )}\), where the sum is over a number \(P^{*}=1000\) of test examples. Additional task details are given in Appendix G.1.

#### 4.1.1 Results

We consider a network of \(L=2\) layers and \(H=2\) heads per layer, with readout from the first token. The network has a total of \(4\) attention paths, schematically depicted in Fig. 2(b). For this synthetic task, we design the fixed query and key weights, and therefore the network's attention paths, to clearly illustrate the effects of task-relevant kernel combination (for details, see Appendix G.2).

We design the first head of each layer to give rise to a "good" attention path (green path) such that a network consisting of this good path alone achieves a high classification accuracy, \(A 94\%\). Along this path, the first head makes use of the Markov nature of the task by attending exclusively to nearby tokens, and only if they correspond to the same hidden state \(\); the second head performs uniform attention, effectively counting how many times the first head detected the same-state transition \(\). In contrast, each layer's second head is initialized randomly. This results in the three remaining paths having chance-level classification accuracy \(A 50\%\), when considered in isolation. However, these paths have very different effects, when combined with the good path. We term two of these paths "adversarial" (red and purple paths) because they deteriorate the network performance, while we term the remaining path "denoising" (blue path) because it can be effectively combined with the good path to improve robustness to noisy data.

Figure 2: **Hidden Markov chain task.****(a)** Illustration of the task. **(b)** Schematics of the network and its attention paths. **(c) Top:** Classification accuracy for varying \(N\) (theory: blue crosses, joined by blue line; samples: black dots). Red lines: GP limit for a network consisting of all paths (solid), the good path (dashed), and the good and denoising paths (dotted). **Bottom:** Matrix elements of \(U\), for varying \(N\). The matrix indices are labeled with the corresponding path name, according to the legend in (b). **(d)** Normalized overlap, or cosine similarity, between the PCs of the kernel \(K\) and the vector of task labels \(Y\) (\(N=10\): blue; GP limit: orange). PCs are ranked by their eigenvalues, from largest to smallest. Only the first \(30\) PCs are shown. **(e)** Same as (c), but for increased \(_{}=5\) and a network consisting of only the good and denoising paths.

In Fig. 2(c, top) we show the network's classification accuracy as a function of the width \(N\) (blue, solid curve), compared to the GP limit (red, solid line). At lower \(N\), well into the finite-width regime, we observe a considerable improvement in performance with respect to the GP limit. This can be understood in terms of an improved kernel-task alignment, as shown in Fig. 2(d).

This improved alignment is ensured by the order parameter \(U\), plotted in Fig. 2(c, bottom) for varying \(N\). For \(N=10\), well into the finite-width regime, the order parameter clearly implements the two main benefits of kernel combination: the possibility to weigh the path-path kernels differently, and the ability to make use of the cross-path kernels. The first benefit is particularly apparent in the suppression of all kernels associated with the adversarial paths. In contrast, when \(N=1000\) and the order parameter is very close to its GP limit \(U^{^{}}=_{,^{}}\), these paths are not suppressed, causing a deterioration in performance compared to that of the good path alone (red, dashed line in Fig. 2(c, top)). The second benefit is apparent in the strong off-diagonals of \(U\), anti-correlating the good and denoising paths. We can see that, while also in the GP limit the denoising and good paths combined (dotted, red line in Fig. 2(c, top)) have a better performance than the good path alone (dashed, red line), the performance boost is even higher in the renormalized regime, which makes use of the cross-path kernels. This additional improvement in performance becomes more apparent with noisier data. This is shown in Fig. 2(e), where we plot the classification accuracy of the network consisting of only the good and denoising paths, on data with stronger perpendicular noise \(_{}=5\).

### One-shot image classification

**Task definition.** The one-shot image classification task (Fig. 3(a)) is formulated in an in-context learning setting. The network is presented with a sequence of three image-label pairs. The first

Figure 3: **One-shot image classification task.****(a)** Scheme of the task. **(b)** Classification accuracy in the GP limit (red line) and the finite-width regime (FW) for varying \(N\) (theory: blue crosses, joined by blue line; samples: black dots). **(c)** Matrix elements of \(U\). The “theory” and “sampled” \(U\)s are for \(N=10\). The matrix indices are labeled with the path index \(=(h_{1},h_{2})\). **(d)** Kernel PCs’ overlap with the task, in the GP limit and in the finite-width regime for \(N=10\). Only the first \(50\) PCs are shown. **(e)** Head score (blue) and performance drop (red) after pruning the head, for the model trained with gradient descent. **(f)** Classification accuracy of the model trained with gradient descent, after pruning a growing number of heads, in order of their head score.

two images belong to two distinct classes of a categorized dataset (Omniglot, FashionMNIST or MNIST in our case). They are assigned the label "\(+\)" or "\(-\)" in no particular order. The third image is assigned the label "\(?\)", and belongs to one of the classes of the first two images. The network has to output \( 1\) according to the label of the matching image. The sequence is fed to the network as follows. Following the idea of the vision transformer (ViT) , each image is divided into \(p\) patches. The patch \(i\{1,,p\}\) of image \(a\{1,2,3\}\) corresponds to the token \(x_{(a-1)p+i}\), for a total of \(T=3p\) tokens. We encode the labels \(+\), \(-\), \(?\) using three fixed random vectors \(v_{+},v_{-},v_{?}^{N_{0}}\), which we directly add to each patch (i.e., token) of the corresponding image. We also encode the token position with additive sinusoidal positional encoding . The network is trained on the Omniglot dataset , while we test its classification accuracy on both in-distribution (ID) unseen classes of Omniglot, and out-of-distribution (OOD) FashionMNIST dataset (we also report results on MNIST in Appendix H).

#### 4.2.1 Results

We consider a network of \(L=2\) attention layers and \(H=4\) heads per layer, with average pooling readout, trained on a subset of \(P=600\) examples from Omniglot (analogous results for a deeper network with \(L=3\), \(H=3\) are also reported in Appendix H.2.2). For the fixed query and key weights required by our Bayesian network, we use the query and key weights obtained from training the same network using gradient descent, with \(N=512\), \(G=128\), and \(P=528k\) (i.e., the entire training set from Omniglot). We refer to Appendix H.1 for further details on this process.

The plots shown in Fig. 3 are analogous to those for the HMC task (Fig. 2), and illustrate analogous kernel combination phenomena. Fig. 3(b) shows the classification accuracy for varying \(N\). Again, we observe a performance gap between the finite-width and GP regimes. Interestingly, this improvement in performance is preserved also OOD, on FashionMNIST. Again, Fig. 3(d) shows that the performance gap can be understood in terms of an improved kernel-task alignment: PCs that are well aligned with \(Y\) are of higher rank, and have a larger overlap than in the GP limit.

The order parameter (Fig. 3(c), "theory" and "sampled") for \(N=10\) is clearly far from its GP limit, accounting for the improvement in performance observed in the finite-width regime. We observe similar kernel combination phenomena as in the HMC task, with strong off-diagonal elements, and a stronger weighting of certain paths w.r.t. others. Interestingly, the block diagonal structure of the order parameter allows for a simple interpretation of the interplay between paths: correlations mostly occur between paths sharing the same head \(h_{1}\) in the first layer, which also determines which paths are overall enhanced (\(h_{1}=1,3\)) or suppressed (\(h_{1}=2,4\)).

This structure of the order parameter transfers qualitatively well also to the network trained with gradient descent. In Fig. 3(c, "gradient descent") we show an empirical order parameter, obtained by computing Eq. 13 using a single realization of the network's weights trained with gradient descent. Both the order parameter's block structure and matrix element signs are qualitatively preserved in this empirical estimate. We emphasize that the network is trained with the full set of training examples (\(P=528k\)) rather than the restricted one used for the Bayesian network (\(P=600\)), and on all learnable parameters including the query and key weights, making this qualitative agreement more relevant to potential applications. One example application is provided below.

**Application: model reduction via head pruning.** Our theory allows us to prune certain heads in the model trained with gradient descent (leading to a model size and compute reduction), with marginal performance loss. This is achieved by using the order parameter to assign a score to each attention head, according to its contribution to kernel combination. The lowest-scoring heads are then pruned from the model. The head \(h\) at layer \(\) is assigned the score \(s^{()h}=_{,^{}^{()h}}|U_{,^{}}|\), where \(^{()h}\) is the set of all paths passing through that head, and \(U\) is the order parameter derived from theory. Fig. 3(e) shows the score of each head, normalized by the largest one, compared against the drop in classification accuracy caused by pruning that head. Note that the network is not retrained after pruning, but only reevaluated on the test examples. We observe a performance drop qualitatively in line with the head scores. Most importantly, the two lowest scoring heads only cause a marginal drop in performance. In Fig. 3(f) we report the classification accuracy after pruning an increasing number of heads, in order of their score. Up until the first two heads (amounting to \(25\%\) of the total number of network parameters), the in-distribution classification accuracy is only marginally worsen. Interestingly, the OOD classification accuracy is even improved, possibly indicating an overspecialization of the pruned heads in solving only the in-distribution task. In Appendix H.2.3 we achieve an analogous size reduction of \(25\%\) on a larger model with \(H=8\) heads.

**Agreement with sampled statistics**. Finally, we note that both figures 2(c,e) and 3(b,c) show good agreement of our theory with the mean predictor and order parameter sampled from Eq. 8. While our theory becomes exact in the \(N,P\) limit, the agreement holds even for small \(N=10\). In particular, in figures 3(b,c), it holds even if \(N<H^{L}(H^{L}+1)/2=136\), the number of independent entries in the order parameter, which is supposed to be a finite quantity in our theory.

## 5 Conclusion and Discussion

**Conclusion.** We introduce a transformer-like model featuring deep multi-head self-attention, amenable to theoretical characterization within the Bayesian framework. Our results unveil the important role of attention paths in accounting for transformers' remarkable performance. We demonstrate that, in scenarios involving the interplay of attention paths at finite widths, generalization consistently improves compared to the GP regime, where such interplay is absent. Our theory explains this paths interplay in terms of a task-relevant kernel combination mechanism, where the network's total kernel results from the sum of many kernels, specific to pairs of paths, and optimally weighted to improve generalization. This mechanism is confirmed by experiments on both synthetic and real-world sequence classification tasks. More broadly, our results are relevant to the theory of deep learning, as they provide an example of non-scalar kernel renormalization [33; 43] in a widely adopted architecture such as the transformer, illustrating its importance in accounting for network performance. Non-scalar, as opposed to scalar , renormalization can affect the network's mean predictor and therefore lead to improved generalization. Our work provides a novel interpretation of its benefits in terms of an optimized kernel-task alignment.

**Interpretability of our Theory.** We provide interpretability to the kernel combination mechanism, by relating it to observable structures in the network weights, specifically to their magnitude and correlations. These predicted structures transfer well outside the Bayesian framework, to the weights of networks trained with gradient descent, broadening the applicability of our theory. As an example, we show that a trained network can be reduced in size with minimal performance loss, by pruning those heads that are deemed less relevant by our theory. The large size reduction achieved (\(25\%\)) appears in line with observations that a few specialized heads are responsible for most of the network performance , and the proposal of head pruning schemes during training . Our theoretical insights may therefore be relevant to the quest for minimalistic and resource-efficient models [46; 47].

**Limitations and Outlook.** The above results are enabled by our theory's ability to go beyond the GP limit , as well as incorporating a multi-head, multi-layer architecture--a prerequisite for the very existence of attention paths. However, various limitations could still be addressed, opening for exciting research directions. For example, attention in our model is only a function of the bare input, rather than the previous layer's postactivation, as in standard transformers. In this case, the theoretical challenge would be to disentangle the learning of attention paths interplay from the learning of the attention paths themselves, since now also the attention matrix would depend on the value weights. Another limitation of our model is its linearity in the value weights. It may be possible to heuristically extend the theory to include nonlinear MLP blocks in between attention layers, by replacing the GP path-path kernels appearing in Eq. (10) with the corresponding GP kernels for the nonlinear case--an approach which has proven successful in deep ReLU networks for certain regimes . Introducing nonlinearities, strong feature learning may also emerge [48; 49; 50]. Note that, instead, our theory is readily extendable to the case of _linear_ MLP blocks, as well as multiple outputs, following [29; 33]. Here we chose a minimal setting focusing only on those renormalization phenomena specific to the transformer architecture. Indeed, the presence of multiple outputs causes the same kind of renormalization independently of the network architecture (i.e., adding two new output indices to the order parameter), while deeper linear blocks would not alter the essence of attention paths interplay, only affecting details in the entropy part of the action. Extending the theory to include skip connections also seems viable. A very open challenge, instead, is to characterize the learning of the query and key weights, which relates to the more general challenge of extending the BPKR technique to nonlinear deep networks. Finally, our approach characterizes the inductive bias imposed by the network architecture on the parameter configurations that fit the training data, but not the bias imposed by a learning algorithm. It would therefore be interesting to import our theory to methods characterizing deep neural networks' training dynamics [51; 52; 53].