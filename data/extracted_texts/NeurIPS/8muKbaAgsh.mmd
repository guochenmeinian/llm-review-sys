# Towards Stable Backdoor Purification through Feature Shift Tuning

Rui Min\({}^{1}\)1, Zeyu Qin\({}^{1}\)1, Li Shen\({}^{2}\), Minhao Cheng\({}^{1}\)

\({}^{1}\)Department of Computer Science & Engineering, HKUST

\({}^{2}\)JD Explore Academy

{rminaa, zeyu.qin}@connect.ust.hk

mathshenli@gmail.com

minhaocheng@ust.hk

Equal contribution. Email to zeyu.qin@connect.ust.hk

###### Abstract

It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based defenses. Therefore, it is necessary to disentangle the backdoor and clean features in order to improve backdoor purification. To address this, we introduce Feature Shift Tuning (FST), a method for tuning-based backdoor purification. Specifically, FST encourages feature shifts by actively deviating the classifier weights from the originally compromised weights. Extensive experiments demonstrate that our FST provides consistently stable performance under different attack settings. Without complex parameter adjustments, FST also achieves much lower tuning costs, only \(10\) epochs. Our codes are available at [https://github.com/AISafety-HKUST/stable_backdoor_purification](https://github.com/AISafety-HKUST/stable_backdoor_purification).

## 1 Introduction

Deep Neural Networks (DNNs) are shown vulnerable to various security threats. One of the main security issues is backdoor attack  that inserts malicious backdoors into DNNs by manipulating the training data or controlling the training process.

To alleviate backdoor threats, many defense methods  have been proposed, such as robust training  and post-processing purification methods . However, robust training methods require complex modifications to model training process , resulting in substantially increased training costs, particularly for large models. Pruning-based purification methods are sensitive to hyperparameters and model architecture , which makes them hard to deploy in real-world applications.

Fine-tuning (FT) methods have been adopted to improve models' robustness against backdoor attacks [14; 20; 36] since they can be easily combined with existing training methods and various model architectures. Additionally, FT methods require less computational resources, making them one of the most popular transfer learning paradigms for large pretrained models [5; 21; 25; 28; 35; 40]. However, FT methods have not been sufficiently evaluated under various attack settings, particularly in the more practical low poisoning rate regime [2; 4]. Therefore, we begin by conducting a thorough assessment of widely-used tuning methods, vanilla FT and simple Linear Probing (LP), under different attack configurations.

In this work, we focus on _data-poisoning backdoor attacks_, as they efficiently exploit security risks in more practical settings [3; 4; 9; 27]. We start our study on _whether these commonly used FT methods can efficiently and consistently purify backdoor triggers in diverse scenarios_. We observe that _vanilla FT and LP can not achieve stable robustness against backdoor attacks while maintaining clean accuracy_. What's worse, although they show promising results under high poisoning rates (\(20\%,10\%\)), they completely fail under low poisoning rates (\(5\%,1\%,0.5\%\)). As shown in Figure 3, we investigate this failure mode and find that model's learned features (representations before linear classifier) have a significant difference under different poisoning rates, especially in terms of separability between clean features and backdoor features. For low poisoning rate scenarios, clean and backdoor features are tangled together so that the simple LP is not sufficient for breaking mapping between input triggers and targeted label without feature shifts. Inspired by these findings, we first try two simple strategies (shown in Figure 1), _FE-tuning_ and _FT-init_ based on LP and FT, respectively, to encourage shifts on learned features. In contrast to LP, FE-tuning only tunes the feature extractor with the frozen and re-initialized linear classifier. FT-init first randomly initializes the linear classifier and then conducts end-to-end tuning. Experimental results show that these two methods, especially FE-tuning, improve backdoor robustness for low poisoning rates, which confirms the importance of promoting shifts in learned features.

Though these two initial methods can boost backdoor robustness, they still face an unsatisfactory trade-off between defense performance and clean accuracy. With analysis of the mechanism behind those two simple strategies, we further proposed a stronger defense method, _Feature Shift Tuning_ (FST) (shown in Figure 1). Based on the original classification loss, FST contains an extra penalty, \(,^{ori}\), the inner product between the tuned classifier weight \(\) and the original backdoored classifier weight \(^{ori}\). During the end-to-end tuning process, FST can actively shift backdoor features by encouraging the difference between \(\) and \(^{ori}\) (shown in Figure 3). Extensive experiments have demonstrated that FST achieves better and more stable defense performance across various attack settings with maintaining clean accuracy compared with existing defense methods. Our method also significantly improves efficiency with fewer tuning costs compared with other tuning methods, which makes it a more convenient option for practical applications. To summarize, our contributions are:

* We conduct extensive evaluations on various tuning strategies and find that while vanilla Fine-tuning (FT) and simple Linear Probing (LP) exhibit promising results in high poisoning rate scenarios, they fail completely in low poisoning rate scenarios.
* We investigate the failure mode and discover that the reason behind this lies in varying levels of entanglement between clean and backdoor features across different poisoning rates. We further propose two initial methods to verify our analysis.
* Based on our initial experiments, we propose Feature Shift Tuning (FST). FST aims to enhance backdoor purification by encouraging feature shifts that increase the separability between clean and backdoor features. This is achieved by actively deviating the tuned classifier weight from its originally compromised weight.
* Extensive experiments show that FST outperforms existing backdoor defense and other tuning methods. This demonstrates its superior and more stable defense performance against various poisoning-based backdoor attacks while maintaining accuracy and efficiency.

## 2 Background and related work

**Backdoor Attacks.** Backdoor attacks aim to mislead the backdoored model to exhibit abnormal behavior on samples stamped with the backdoor trigger but behave normally on all benign samples. They can be classified into 2 categories : **(1)** data-poisoning attacks: the attacker inserts a backdoor trigger into the model by manipulating the training sample \((,)(,)\), like adding a small patch in clean image \(\) and assign the corresponding class \(\) to an attacker-designated target label \(_{t}\). ; **(2)** training-control attacks: the attacker can control both the training process and training data simultaneously . With fewer assumptions about attackers' capability, data-poisoning attacks are much more practical in real-world scenarios  and have led to increasingly serious security risks . Therefore, in this work, we mainly focus on data-poisoning backdoor attacks.

**Backdoor Defense.** Existing backdoor defense strategies could be roughly categorized into robust training  and post-processing purification methods . Robust training aims to prevent learning backdoored triggers during the training phase. However, their methods suffer from accuracy degradation and significantly increase the model training costs , which is impractical for large-scale model training. Post-processing purification instead aims to remove the potential backdoor features in a well-trained model. The defender first identifies the compromised neurons and then prunes or unlearns them . However, pruning-based and unlearning methods also sacrifice clean accuracy and lack generalization across different model architectures.

**Preliminaries of backdoor fine-tuning methods.** Without crafting sophisticated defense strategies, recent studies  propose defense strategies based on simple Fine-tuning. Here, we introduce two widely used fine-tuning paradigms namely vanilla Fine-tuning (FT) and Linear Probing (LP) (shown in Figure 1) since they would serve as two strong baselines in our following sections. For each tuned model, we denote the feature extractor as \((;):()\) and linear classifier as \((;)=^{T}():() \). To implement the fine-tuning, both tuning strategies need a set of training samples denoted as \(_{T}(,)\) to update the model parameters while focusing on different parameter space. Following previous works , we implement vanilla FT by updating the whole parameters \(\{,\}\); regarding the LP, we only tunes the linear classifier \(()\) without modification on the frozen \(()\).

**Evaluation Metrics.** We take two evaluation metrics, including _Clean Accuracy (C-Acc)_ (i.e., the prediction accuracy of clean samples) and _Attack Success Rate (ASR)_ (i.e., the prediction accuracy of poisoned samples to the target class). A lower ASR indicates a better defense performance.

## 3 Revisiting Backdoor Robustness of Fine-tuning Methods

In this section, we evaluate the aforementioned two widely used fine-tuning paradigms' defensive performance against backdoor attacks with various poisoning rates (FT and LP). Despite that the vanilla FT has been adopted in previous defense work , it has not been sufficiently evaluated under various attack settings, particularly in more practical low poisoning rate scenarios. The simple LP method is widely adopted in transfer learning works  but still rarely tested for improving model robustness against backdoor attacks. For FT, we try various learning rates during tuning: \(0.01,0.02\), and \(0.03\). For LP, following , we try larger learning rates: \(0.3,0.5\), and \(0.7\). We also demonstrate these two methods in Figure 1.

We conduct evaluations on widely used CIFAR-10  dataset with ResNet-18  and test \(4\) representative data-poisoning attacks including BadNet , Blended , SSBA  and Label-Consistent attack (LC) . We include various poisoning rates, \(20\%\), \(10\%5\%\), \(1\%\), and \(0.5\%\) for attacks except for LC since the maximum poisoning rate for LC on CIFAR-10 is \(10\%\). Following previous work , we set the target label \(_{t}\) as class \(0\). Additional results on other models and datasets are shown in _Appendix C.1_. We aim to figure out _whether these commonly used FT methods can efficiently and consistently purify backdoor triggers in various attack settings_.

Figure 1: The first two methods, LP and vanilla FT, are adopted in Section 3.1. The middle two methods, FE-tuning and FT-init, are proposed in Section 3.2. The final one, FST, is our method introduced in Section 4.

### Revisiting Fine-tuning under Various Poisoning Rates

We stress that the defense method should effectively purify the backdoor triggers while maintaining good clean accuracy. Therefore, we mainly focus on defense performance with a satisfactory clean accuracy level (around \(92\%\)). The results are shown in Figure 2.

_Vanilla FT and LP can purify backdoored models for high poisoning rates._ From Figure 2, We observe that for high poisoning rates (\( 10\%\) for BadNet, Blended, and SSBA; \(5\%\) for LC), both vanilla FT and LP can effectively mitigate backdoor attacks. Specifically, LP (purple lines) significantly reduces the ASR below \(5\%\) on all attacks without significant accuracy degradation (\( 2.5\%\)). Compared with vanilla FT, by simply tuning the linear classifier, LP can achieve better robustness and clean accuracy.

_Vanilla FT and LP both fail to purify backdoor triggers for low poisoning rates._ In contrast to their promising performance under high poisoning rates, both tuning methods fail to defend against backdoor attacks with low poisoning rates (\( 5\%\) for BadNet, Blended, and SSBA; \(<5\%\) for LC). Vanilla FT with larger learning rates performs slightly better on Blended attacks, but it also sacrifices clean accuracy, leading to an intolerant clean accuracy drop. The only exception is related to the SSBA results, where the ASR after tuning at a \(0.5\%\) poisoning rate is low. This can be attributed to the fact that the original backdoored models have a relatively low ASR.

### Exploration of Unstable Defense Performance of Fine-tuning Methods

From the results, a question then comes out: **What leads to differences in defense performance of tuning methods under various poisoning rate settings?** We believe the potential reason for this phenomenon is that _the learned features from feature extractors of backdoored models differ at different poisoning rates, especially in terms of the separability between clean features and backdoor features._ We conduct T-SNE visualizations on learned features from backdoored models with Blended attack (\(10\%\) and \(1\%\) poisoning rates). The results are shown in Figure 3. The targeted class samples are marked with Red color and Black points are backdoored samples. As shown in first two figures,

Figure 3: We take T-SNE visualizations on features from feature extractors with _Blended attack_. We adopt half of the CIFAR-10 test set and ResNet-18. Each color denotes each class, and **Black** points represent backdoored samples. The targeted class is **0 (Red)**. **(1)** The first two figures are feature visualizations of original backdoored models with \(10\%\) and \(1\%\) poisoning rates; **(2)** The rest \(4\) figures are feature visualizations after using different tuning methods for \(1\%\) poisoning rate. We also add the corresponding ASR.

Figure 2: The clean accuracy and ASR of \(4\) attacks on CIFAR-10 and ResNet-18. The \(\)-axis means the poisoning rates. The blue and purple lines represent FT and LP, respectively. For FT, we try various learning rates during tuning: \(0.01,0.02\), and \(0.03\). For LP, we try learning rates: \(0.3,0.5,\) and \(0.7\).

under high poisoning rate (\(10\%\)), backdoor features (black points) are clearly separable from clean features (red points) and thus could be easily purified by only tuning the \(()\); however for low poisoning rate (\(1\%\)), clean and backdoor features are tangled together so that the simple LP is not sufficient for breaking mapping between input triggers and targeted label without further feature shifts. Furthermore, as depicted in the third figure of Figure 3, though vanilla FT updates the whole network containing both \(\) and \(\), it still suffers from providing insufficient feature modification, leading to the failure of backdoor defense.

**Can Feature Shift Help Enhance the Purification Performance of Fine-tuning?** Based on our analysis, we start with a simple strategy in that we could improve backdoor robustness for low poisoning rates by encouraging shifts in learned features. We then propose separate solutions for both LP and vanilla FT to evaluate the effect of feature shift as well as the robustness against two low poisoning rates, \(1\%\) and \(0.5\%\). As shown in Table 1, specifically, for the LP, we freeze \(()\) and only tune \(()\). However, our initial experiments show that directly tuning \(()\) does not sufficiently modify learned features since the fixed backdoor linear classifier (denoted as \((^{ori})\)) may still restrict modifications of learned features. Inspired by the previous work , we first randomly re-initialize the linear classifier and then tune only \(()\) (denoted as FE-tuning). For vanilla FT, we also fine-tune the whole network with a randomly re-initialized linear classifier (denoted as FT-init). More implementation details of FE-tuning and FT-init are shown in _Appendix_B.3.

**Evaluations Verify That Encouraging Feature Shift Could Help Purify Backdoored Models.** We observe that both FE-tuning and FT-init could significantly enhance the performance of backdoor purification compared to previous fine-tuning with an average drop of \(77.70\%\) and \(35.61\%\) on ASR respectively. Specifically, FE-tuning leads to a much larger improvement, an ASR decrease of over \(74\%\). As shown in the fifth figure of Figure 3, _after FE-tuning, backdoor features could be clearly separated from the clean features of the target class (red)_. However, this simple strategy also leads to a decrease in clean accuracy (around \(2.9\%\)) in comparison to the original LP, due to the totally randomized classifier. While for the FT-init, the robustness improvements are less significant. As shown in the fourth figure of Figure 3, simply fine-tuning the backdoor model with re-initialized \(()\) can not lead to enough shifts on backdoor features under the low poisoning rate. The backdoor and clean features of the target class still remain tangled, similar to the original and vanilla FT.

In summary, the results of these two initial methods confirm the fact that _encouraging shifts on learned features is an effective method for purifying backdoors at low poisoning rates._ However, both methods still experience a serious clean accuracy drop or fail to achieve satisfactory improvements in robustness. To further enhance the purification performance, we propose a stronger tuning defense in Section 4 that addresses both issues in a unified manner.

## 4 Feature Shift Tuning: Unified Method to Achieve Stable Improvements

Based on our initial findings, we propose a stronger tuning defense paradigm called **Feature Shift Tuning (FST)**. FST is an end-to-end tuning method and actively shifts features by encouraging the discrepancy between the tuned classifier weight \(\) and the original backdoored classifier weight \(^{ori}\). The illustration of FST is shown in Figure 1. Formally, starting with reinitializing the linear classifier

   &  Puisoning \\ rate \\  } &  &  &  &  &  \\   & & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) & C-Acc(\(\)) & ASR(\(\)) \\   & \(1\%\) & 94.52 & 100 & **94.02** & 100 & 91.56 & **3.18** & 92.12 & 99.83 & 93.37 & 16.72 \\  & 0.5\% & 94.79 & 100 & **94.37** & 100 & 92.37 & **7.41** & 92.56 & 99.07 & 93.90 & 79.32 \\   & \(1\%\) & 95.13 & 98.12 & **94.51** & 98.98 & 92.03 & **8.50** & 92.97 & 82.36 & 93.88 & 61.23 \\  & 0.5\% & 94.45 & 92.46 & **94.29** & 93.72 & 91.84 & **6.48** & 92.95 & 75.99 & 93.71 & 49.80 \\   & \(1\%\) & 94.83 & 79.54 & **94.23** & 82.39 & 91.99 & **5.58** & 92.59 & 30.16 & 93.51 & 21.04 \\  & 0.5\% & 94.50 & 50.20 & **94.37** & 51.67 & 91.31 & **2.50** & 92.36 & 12.69 & 93.41 & 6.69 \\   & \(1\%\) & 94.33 & 99.16 & **94.23** & 99.98 & 91.70 & 64.97 & 92.65 & 94.83 & 93.54 & 89.86 \\  & 0.5\% & 94.89 & 100 & **94.78** & 100 & 91.65 & **22.22** & 92.67 & 99.96 & 93.83 & 96.16 \\   & 94.68 & 89.94 & **94.35** & 90.91 & 91.81 & **15.11** & 92.61 & 74.36 & 93.64 & 52.60 \\  

Table 1: Purification performance of fine-tuning against four types of backdoor attacks with low poisoning rates. All the metrics are measured in percentage (%).

\(\), our method aims to solve the following optimization problem:

\[_{,}\{_{(,)_{T}}[ ((;(;)),)]+< ,^{ori}>,.\ \ s.t.\|\|_{2}=C\} \]

where \(\) stands for the original classification cross-entropy loss over whole model parameters to maintain clean accuracy and \(<,>\) denotes the inner product. By adding the regularization on the \(<,^{ori}>\), FST encourages discrepancy between \(\) and \(^{ori}\) to guide more shifts on learned features of \(()\). The \(\) balances these two loss terms, the trade-off between clean accuracy and backdoor robustness from feature shift. While maintaining clean accuracy, a larger \(\) would bring more feature shifts and better backdoor robustness. We simply choose the inner-product \(<,^{ori}>\) to measure the difference between weights of linear classifiers, since it is easy to implement and could provide satisfactory defensive performance in our initial experiments. Other metrics for measuring differences can also be explored in the future.

```
0: Tuning dataset \(_{T}=\{_{i},_{i}\}_{i=1}^{N}\); backdoored model \((^{ori};())\); learning rate \(\); factor \(\); tuning iterations \(I\)
0: Purified model
1: Initialize \(^{0}\) with random weights
2: Initialize \(^{0}\) with \(\)
3:for\(i=0,1,,I\)do
4: Sample mini-batch \(_{i}\) from tuning set \(_{T}\)
5: Calculate gradients of \(^{i}\) and \(^{i}\): \(^{i}_{}=_{^{i}}_{i}|}_{ _{i}}((^{i};(^{ i};)),)\); \(^{i}_{w}=_{^{i}}[_{i}|}_{ _{i}}((^{i};(^{i}; )),)+<^{i},^{ori}>]\);
6: Update model parameters \(^{i+1}=^{i}- g^{i}_{},^{i+1}=^{i}-  g^{i}_{w}\)
7: Norm projection of the linear classifier \(^{i+1}=^{i+1}}{\|^{ori+1}\|_{2}}\|^{ori}\|_{2}\)
8:endfor
9:return Purified model \((^{I};(^{I}))\)
```

**Algorithm 1** Feature Shift Tuning (FST)

**Projection Constraint.** To avoid the \(\) exploding and the inner product dominating the loss function during the fine-tuning process, we add an extra constraint on the norm of \(\) to stabilize the tuning process. To reduce tuning cost, we directly set it as \(\|^{ori}\|\) instead of manually tuning it. _With the constraint to shrink the feasible set, our method quickly converges in just a few epochs while achieving significant robustness improvement._ Compared with previous tuning methods, our method further enhances robustness against backdoor attacks and also greatly improves tuning efficiency (Shown in Section 5.3). Shrinking the range of feasible set also brings extra benefits. _It significantly reduces the requirement for clean samples during the FST process._ As shown in Figure 7 (c,d), FST consistently performs well across various tuning data sizes, even when the tuning set only contains 50 samples. The ablation study of \(\) is also provided in Section 5.3 and showcases that our method is not sensitive to the selection of \(\) in a wide range. The overall optimization procedure is summarized in Algorithm 1.

**Unified Improvement for Previous Tuning Methods.** We discuss the connections between FST and our previous FE-tuning and FT-init to interpret why FST could achieve unified improvements on backdoor robustness and clean accuracy. Our objective function Eq.1 could be decomposed into three parts, by minimizing \(_{(,)_{T}}((; (;)),)\) over \(\) and \(\) respectively, and \(<,^{ori}>\) over \(\). Compared with FE-tuning, FST brings an extra loss term on \(_{}_{(,)_{T}}((;(;)),)+<,^{ori}>\) to update linear classifier \(\). In other words, while encouraging feature shifts, FST updates the linear classifier with the original loss term to improve the models' clean performance. Compared with the FT-init, by adopting \(<,^{ori}>\), FST encourages discrepancy between tuned \(\) and original \(^{ori}\) to guide more shifts on learned features.

**Weights of Linear Classifier Could Be Good Proxy of Learned Features.** Here, we discuss why we choose the discrepancy between linear classifiers as our regularization in FST. Recalling that our goal is to introduce more shifts in learned features, especially for backdoor features. Therefore, the most direct approach is to explicitly increase the difference between backdoor and clean feature distributions. However, we can not obtain backdoor features without inputting backdoor triggers. We consider that _weights of the original compromised linear classifier could be a good proxy of learned features_. Therefore, we encourage discrepancy between tuned \(\) and original \(^{ori}\) rather than trying to explicitly promote discrepancy between feature distributions.

Experiments

### Experimental Settings

**Datasets and Models.** We conduct experiments on four widely used image classification datasets, CIFAR-10 , GTSRB , Tiny-ImageNet , and CIFAR-100 . Following previous works , we implement backdoor attacks on ResNet-18  for both CIFAR-10 and GTSRB and also explore other architectures in Section 5.3. For CIFAR-100 and Tiny-ImageNet, we adopt pre-trained SwinTransformer  (Swin). For both the CIFAR-10 and GTSRB, we follow the previous work  and leave \(2\%\) of original training data as the tuning dataset. For the CIFAR-100 and Tiny-ImageNet, we note that a small tuning dataset would hurt the model performance and therefore we increase the tuning dataset to \(5\%\) of the training set.

**Attack Settings.** All backdoor attacks are implemented with BackdoorBench 1. We conduct evaluations against \(6\) representative data-poisoning backdoors, including \(4\) dirty-label attacks (BadNet , Blended attack , WaNet , SSBA) and \(2\) clean-label attacks (SIG attack , and Label-consistent attack (LC) ). For all the tasks, we set the target label \(_{t}\) to 0, and focus on low poisoning rates, \(5\%\), \(1\%\), and \(0.5\%\) in the main experimental section. We also contain experiments of **high poisoning rates**, \(10\%\), \(20\%\), and \(30\%\), in _Appendix_ C.2. For the GTSRB dataset, we do not include LC since it can not insert backdoors into models (ASR \(<10\%\)). Out of all the attacks attempted on Swin and Tiny-ImageNet, only BadNet, Blended, and SSBA were able to successfully insert backdoor triggers into models at low poisoning rates. Other attacks resulted in an ASR of less than \(20\%\). Therefore, we only show the evaluations of these three attacks. More details about attack settings and trigger demonstrations are shown in _Appendix_ B.2.

**Baseline Defense Settings.** We compare our FST with \(4\) tuning-based defenses including Fine-tuning with Sharpness-Aware Minimization (FT+SAM), a simplified version adopted from , Natural Gradient Fine-tuning (NGF) , FE-tuning and FT-init proposed in Section 3.2. We also take a comparison with \(2\) extra strategies including Adversarial Neural Pruning (ANP)  and Implicit Backdoor Adversarial Unlearning (I-BAU)  which achieve outstanding performance in BackdoorBench . The implementation details of baseline methods are shown in the _Appendix_ B.3. For our FST, we adopt SGD with an initial learning rate of \(0.01\) and set the momentum as \(0.9\) for both CIFAR-10 and GTSRB datasets and decrease the learning rate to \(0.001\) for both CIFAR-100 and Tiny-ImageNet datasets to prevent the large degradation of the original performance. We fine-tune the models for \(10\) epochs on the CIFAR-10; \(15\) epochs on the GTSRB, CIFAR-100 and Tiny-ImageNet. We set the \(\) as 0.2 for CIFAR-10; \(0.1\) for GTSRB; \(0.001\) for both the CIFAR-100 and Tiny-ImageNet.

### Defense Performance against Backdoor Attacks

In this section, we show the performance comparison between FST with tuning-based backdoor defenses (FT+SAM , NGF , FE-tuning and FT-init) and current state-of-the-art defense methods, ANP  and I-BAU . We demonstrate results of CIFAR-10, GTSRB, and Tiny-ImageNet on Table 2, 3, and 4, respectively. We leave results on CIFAR-100 to _Appendix_ C.3.

Experimental results show that _our proposed FST achieves superior backdoor purification performance compared with existing defense methods_. Apart from Tiny-ImageNet, FST achieves the best performances on CIFAR-10 and GTSRB. The average ASR across all attacks on three datasets are below \(1\%\), \(0.52\%\) in CIFAR-10, \(0.41\%\) in GTSRB, and \(0.19\%\) in Tiny-ImageNet, respectively. Regarding two tuning defense methods, FT+SAM and NGF, FST significantly improves backdoor robustness with larger ASR average drops on three datasets by \(34.04\%\) and \(26.91\%\). Compared with state-of-the-art methods, ANP and I-BAU, on CIFAR-10 and GTSRB, our method outperforms with a large margin by \(11.34\%\) and \(32.16\%\) on average ASR, respectively. ANP is only conducted in BatchNorm layers of ConvNets in source codes. Therefore, it can not be directly conducted in SwinTransformer. Worth noting is that _our method achieves the most stable defense performance across various attack settings_ with \(0.68\%\) on the average standard deviation for ASR. At the same time, our method still maintains high clean accuracy with \(93.07\%\) on CIFAR-10, \(96.17\%\) on GTSRB, and \(79.67\%\) on Tiny-ImageNet on average. Compared to ANP and I-BAU, FST not only enhances backdoor robustness but also improves clean accuracy with a significant boost on the clean accuracy by \(2.5\%\) and \(1.78\%\), respectively.

Following the experimental setting in Section 3.2, we also conduct feature visualization of FST in Figure 3. It can be clearly observed that our FST significantly shifts backdoor features and makes them easily separable from clean features of the target class like FE-tuning. Combined with our outstanding performance, this further verifies that actively shifting learned backdoor features can effectively mitigate backdoor attacks. We also evaluate two initial methods, FE-tuning and FT-init. FE-tuning performs best on Tiny-ImageNet, leading to a little ASR drop by \(0.18\%\) compared with FST. However, it also significantly sacrifices clean accuracy, leading to a large C-ACC drop by \(8.1\%\) compared with FST. FST outperforms them by a large margin for backdoor robustness on CIFAR-10 and GTSRB. As we expected, FST achieves stable improvements in robustness and clean accuracy compared with FE-tuning and FT-init.

**Adaptive Attacks.** To further test the limit of FST, we conduct an adaptive attack evaluation for it. To bypass defense methods based on latent separation property, adaptive poisoning attacks  actively suppress the latent separation between backdoor and clean features by adopting an extremely low poisoning rate and adding regularization samples. Their evaluations also show that these attacks successfully bypass existing strong latent separation-based defenses. Hence, we believe it is also equally a powerful adaptive attack against our FST method. We evaluate our method against adaptive attacks. The results of the Adaptive-Blend attack are shown in Table 2, 3. The results of more adaptive attacks and various regularization samples are shown in _Appendix_C.4. We could observe that FST could still effectively purify such an adaptive attack achieving an average drop in ASR by \(81.66\%\) and \(91.75\%\) respectively for the CIFAR-10 and GTSRB.

Figure 4: The T-SNE visualizations of Adaptive-Blend attack (150 payload and 150 regularization samples). Each color denotes each class, and **Black** points represent backdoored samples. The targeted class is **0** (**Red**). The left figure represents the original backdoored model and the right represents the model purified with FST.

   &  &  &  &  &  &  &  &  &  &  \\   & m & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 \\   & 57 & 94.82 & 90.09 & 91.62 & 0.52 & 1.24 & 1.84 & 91.29 & 2.09 & 2.09 & 2.08 & 91.54 & 3.10 & **91.52** & 3.06 & 20.21 & 91.74 & **6.08** \\  & 57 & 94.52 & 90.00 & 91.62 & 0.52 & 1.72 & 1.84 & 91.29 & 2.09 & 1.09 & 1.09 & 1.09 & 1.09 & **91.52** & **91.57** & 10.51 & **6.08** \\  & 67.55 & 90.00 & 84.81 & 12.17 & 1.04 & 1.09 & 1.09 & 4.08 & 91.84 & 91.84 & 91.74 & 91.49 & 91.72 & 91.50 & **6.08** \\   & 57 & 94.61 & 90.76 & 90.76 & 91.88 & 53.17 & 90.45 & 45.01 & 91.26 & 23.46 & 10.18 & 11.08 & **91.53** & 90.44 & **90.37** & **3.07** \\  & 57 & 94.61 & 90.66 & 90.53 & 1.76 & 90.57 & 1.70 & 91.26 & 91.26 & 91.26 & 91.26 & 91.18 & 91.18 & 91.18 & 91.18 & 91.18 & 91.18 \\   & 57 & 94.61 & 90.66 & 90.53 & 1.76 & 90.57 & 1.70 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 \\  & 57 & 94.61 & 90.66 & 90.53 & 1.76 & 90.57 & 1.72 & 91.26 & 91.26 & 91.

[MISSING_PAGE_FAIL:9]

well across various tuning data sizes. Even if the tuning dataset is reduced to only \(0.1\%\) of the training dataset (50 samples in CIFAR-10), our FST can still achieve an overall ASR of less than \(2\%\).

**Analysis on model architecture.** We extend the experiments on other model architectures including VGG19-BN , ResNet-50 , and DenseNet161  which are widely used in previous studies [14; 37; 42]. Here we show the results of BadNet and Blended attacks with \(5\%,1\%\), and \(0.5\%\) poisoning rates on the CIFAR-10 dataset. Notably, the structure of VGG19-BN is slightly different where its classifier contains more than one linear layer. Our initial experiments show that directly applying FST to the last layer fails to purify backdoors. Therefore, we simply extend our methods to the whole classifier and observe a significant promotion. We leave the remaining attacks and more implementation details in the _Appendix_D.2. The results presented in Figure 8 show that our FST significantly enhances backdoor robustness for all three architectures by reducing ASR to less than \(5\%\) on average. This suggests that our approach is not influenced by variations in model architecture.

## 6 Conclusion and limitations

In this work, we concentrate on practical Fine-tuning-based backdoor defense methods. We conduct a thorough assessment of widely used tuning methods, vanilla FT and LP. The experiments show that they both completely fail to defend against backdoor attacks with low poisoning rates. Our further experiments reveal that under low poisoning rate scenarios, the backdoor and clean features from the compromised target class are highly entangled together, and thus disentangling the learned features is required to improve backdoor robustness. To address this, we propose a novel defense approach called Feature Shift Tuning (FST), which actively promotes feature shifts. Through extensive evaluations, we demonstrate the effectiveness and stability of FST across various poisoning rates, surpassing existing strategies. However, our tuning methods assume that _the defender would hold a clean tuning set_ which may not be feasible in certain scenarios. Additionally, _they also lead to a slight compromise on accuracy in large models though achieving robustness_. This requires us to pay attention to protect learned pretraining features from being compromised during robust tuning [7; 16; 19; 39].

Figure 8: The purification performance against \(3\) different model architectures on the CIFAR-10 dataset, where Ba- is short for BadNet and Bl- is short for Blended.

Figure 6: The ASR results of four backdoor attacks with varying tuning epochs of tuning methods.

Figure 7: (a) and (b) show C-ACC and ASR of FST with various \(\). (c) and (d) show the C-ACC and ASR of various sizes of tuning datasets. Experiments are conducted on CIFAR-10 dataset with ResNet-18.