# FastSurvival: Hidden Computational Blessings in

Training Cox Proportional Hazards Models

 Jiachang Liu\({}^{1}\), Rui Zhang\({}^{2}\), Cynthia Rudin\({}^{2}\)

\({}^{1}\)Cornell University, \({}^{2}\)Duke University

jiachang.liu@cornell.edu, r.zhang@duke.edu, cynthia@cs.duke.edu

###### Abstract

Survival analysis is an important research topic with applications in healthcare, business, and manufacturing. One essential tool in this area is the Cox proportional hazards (CPH) model, which is widely used for its interpretability, flexibility, and predictive performance. However, for modern data science challenges such as high dimensionality (both \(n\) and \(p\)) and high feature correlations, current algorithms to train the CPH model have drawbacks, preventing us from using the CPH model at its full potential. The root cause is that the current algorithms, based on the Newton method, have trouble converging due to vanishing second order derivatives when outside the local region of the minimizer. To circumvent this problem, we propose new optimization methods by constructing and minimizing surrogate functions that exploit hidden mathematical structures of the CPH model. Our new methods are easy to implement and ensure monotonic loss decrease and global convergence. Empirically, we verify the computational efficiency of our methods. As a direct application, we show how our optimization methods can be used to solve the cardinality-constrained CPH problem, producing very sparse high-quality models that were not previously practical to construct. We list several extensions that our breakthrough enables, including optimization opportunities, theoretical questions on CPH's mathematical structure, as well as other CPH-related applications.

## 1 Introduction

Survival analysis, which studies time-to-event data, is an important research topic with a wide range of real-world applications. In medicine, survival analysis has been employed to model when a patient will die . In business, it is useful for attrition prediction  (when an employees resigns) and churn prediction  (when a customer unsubscribes), and in manufacturing, it is used to predict when a physical system breaks down . A fundamental tool in analyzing such data is the Cox proportional hazards (CPH) model , a linear model under the assumption that features have a multiplicative effect on the risk of failure/event. Simple yet powerful, the CPH model has enjoyed great popularity due to its modeling flexibility (when coupled with additive models ). Moreover, in contrast to black box models, it is both interpretable and accurate.

However, with the advent of larger sample and feature spaces and more complex data, new challenges arise in using the CPH model to its full potential. Ideally, practitioners want to produce CPH models repeatedly, with feature engineering and preprocessing between iterations. Additionally, they want the CPH model to identify important variables , even in presence of highly correlated features. However, current optimization methods for training the CPH model do not meet these needs. Current algorithms , based on the generic Newton's method, are computationally intensive. More importantly, due to both vanishing second-order derivatives  and the use of approximation strategies that trade precision for efficiency, existing optimization methods have trouble converging, either with the loss blowing up or the algorithm converging very slowly when we require the precisionnecessary to handle correlated variables. The latter issue is the core reason for incorrect variable selection when features are highly correlated.

In this work, we propose new optimization methods to train the CPH model and show that there is not necessarily a precision-efficiency tradeoff. Despite the CPH model being seemingly amenable to classical optimization approaches such as coordinate descent, this has not been attempted for the original CPH loss function due to its daunting complexity. However, through careful examination, we show instead that the complexity of the CPH loss function is really a blessing, rather than a curse. We discover hidden mathematical structures that allow us to design very efficient algorithms. We show both the first and second-order derivatives at each coordinate can be computed exactly in linear time complexity (\(O(n)\)). Moreover, we show both derivatives are Lipschitz-continuous by making novel connections with the second and third central moment calculation in probability theory and statistics. All these discoveries lead us to design algorithms that essentially minimize a quadratic surrogate function and a cubic surrogate function, respectively. They are extremely easy to implement.

Empirically, we demonstrate the superior speed of our algorithms on large-scale datasets. In general, ours are significantly faster than all existing methods and rapidly converge to optimal high-precision solutions. Because our methods produce high-quality solutions, we apply them for variable selection in challenging regimes where features are highly correlated. We solve difficult cardinality-constrained CPH problems and produce models that are much sparser than the state-of-the-art methods.

In summary, our contributions are: (1) We find a _critical flaw_ in the current optimization algorithms for the CPH method by pinpointing that they converge slowly with low precision. Sometimes, the loss does not converge and explodes. (2) To circumvent this issue, we propose novel algorithms that minimize a quadratic and a cubic surrogate function, respectively, with guaranteed convergence and loss descent at each iteration. The core novelty lies in discovering hidden mathematical structure, which allows for an efficiency way (\(O(n)\)) of calculating the second-order partial derivatives exactly. In addition, we show the first and second order partial derivatives are Lipschitz continuous. To calculate these Lipschitz constants, we leverage second and third central moments from theoretical statistics and probability theory. (3) Empirically, our method enjoys _fast speed_ in training the loss function and results in _superior performance_ when solving cardinality-constrained problems.

Our work constitutes a methodological breakthrough in training CPH models. At the end of the paper, we also discuss several exciting extensions and follow-up questions, showing how our new perspectives and discoveries open doors to many new research opportunities.

## 2 Preliminaries

Given a time-to-event dataset of \(n\) samples with \(\{_{i},t_{i},_{i}\}_{i=1}^{n}\), where \(^{p}\) is the feature vector with length \(p\), \(t_{i}\) is the observation time, and \(_{i}\{0,1\}\) is an indicator with \(1\) indicating that a failure event has happened, the CPH model can be used to learn and predict the risk of failure, commonly known as the hazard function in survival analysis. The CPH model predicts the hazard \(h_{i}(t)\) for sample \(i\) in a semiparametric way . For review of related work, please see Appendix B.

\[h_{i}(t)=h_{0}(t)e^{_{i}^{T}},\] (1)

where \(h_{0}(t)\) is a baseline hazard function shared by all samples, and \(^{p}\) is the parameter of interest. The nice thing about the CPH model is that \(h_{0}(t)\) cancels out if we look only at the ratio of hazards of sample \(i\) vs. all remaining samples at time \(t_{i}\), _i.e._,

\[(t_{i})}{_{j R_{i}}h_{j}(t_{i})}=_{i}^{T}}}{_{j R_{i}}e^{_{j}^{T}}},\] (2)

where \(R_{i}:=\{j t_{j} t_{i}\}\) is the set of indices whose observation time is greater than or equal to that of sample \(i\). Such ratios are also called partial likelihoods. To estimate the parameter of interest, \(\), we maximize the joint partial likelihood of all samples with failure events, which can be written as

\[L()=_{i_{i}=1}_{i}^{T}}}{_ {j R_{i}}e^{_{j}^{T}}}.\] (3)This is equivalent to minimizing the negative log partial likelihood , which is defined as

\[()=- L()=_{i=1}^{n}_{i}[(_ {j R_{i}}e^{_{j}^{T}})-_{i}^{T}].\] (4)

The loss function \(()\), while convex, is very mathematically involved. In addition to the double sum, the inner summation over \(j\) is with respect to a different index set \(R_{i}\), for each outer summation index \(i\). Such daunting complexity makes it difficult to employ first-order methods such as gradient descent because we cannot easily pick the right step size for each iteration, which plays a crucial role in practical running time. Therefore, past efforts have been focused on developing Newton-type (second-order) methods, where the loss function is approximated by a second-order Taylor expansion:

\[(+)()+_{} ()^{T}+^{T}_{ }^{2}():=f().\] (5)

The function \(f()\) can be minimized by solving a linear system: \(}=-(_{}^{2}())^{-1}_ {}()\). To reveal the computational nuances more explicitly, we use an intermediate variable \(\) with \(=\). Then we can rewrite the approximation function \(()\) as:

\[f()=(+)()+ _{}()^{T}+ ^{T}^{T}_{}^{2}() {}.\] (6)

At each iteration, calculating the Hessian matrix \(_{}^{2}()\) requires \(O(n^{2})\) complexity. Past methods on the CPH model have resorted to various approximation strategies by replacing \(_{}^{2}()\) with \(H()\) to reduce the computational complexity:

**1. Exact Newton**\(H()=_{}^{2}()\) _# no approximation_

**2. Quasi Newton**\(H()_{ij}=[_{}^{2}()]_{ii}& i=j\\ 0&\) _# ignore off-diagonal terms_

**3. Proximal Newton**\(H()=(_{}()+),\) _# diagonal upper bound on \(_{}^{2}()\)_

where \(()\) constructs a matrix with its diagonal equal to the input vector and other entries equal to 0. There are two major problems with the above approaches. One common problem is that the these Newton-type methods inherently have trouble converging beyond the local region of minimizers without backtrack line search . We provide a concrete example to demonstrate this issue in the experiment section. Ideally, we want to avoid backtracking because this increases the running time. In contrast, our methods do not have this flaw and guarantee global convergence.

The other problem is that when the above approaches do converge to the optimal solutions, none of them can converge with high precision fast enough (in a practical sense). The exact Newton's method  has a local quadratic convergence rate, but each iteration can take a long time. Quasi Newton  and proximal Newton 1 methods are computationally much cheaper to evaluate per iteration, but they make less progress toward the optimal solution. In the next section, we show that, by exploiting hidden mathematical structure, we can obtain the _best of both worlds_: cheap evaluation per iteration and fast convergence with respect to the number of iterations.

## 3 Methodology

### New Formulas for First, Second, and Third Order Partial Derivatives

As we have mentioned, the reason for the diagonal approximations of \(_{}^{2}()\) is to reduce the complexity of the mathematics and associated high computational cost. Here, we take a completely different approach from past methods. First, we avoid making any approximations and embrace the full Hessian matrix. Second, we bypass the intermediate step of calculating the Hessian in the sample space \(\) and focus on the Hessian in the feature space \(\). The involved mathematics may already sound complicated, but we do not stop here. We apply these two ideas not only to the second order partial derivatives but the third order partial derivatives as well. Although this seems like a burdensome task, we show that the end result is very elegant and has an intuitive interpretation. Out of complexity comes simplicity. We summarize the relevant results in the first following theorem:

**Theorem 3.1**.: _For the CPH loss function defined in Equation (4), the first, second, and third order partial derivatives with respect to coordinate \(l\) are:_

_Ist order partial derivative__:_

\[)}{_{l}}=_{i=1}^{n} _{i}(_{k R_{i}}}}{_{j R_{i}}e^{_{j }}}X_{kl})-_{i=1}^{n}_{i}X_{il}.\] (7)

_2nd order partial derivative__:_

\[()}{_{l}^{2}}=_{i=1}^{n} _{i}[_{k R_{i}}}}{_{j R_{i}}e^{_ {j}}}X_{kl}^{2}-(_{k R_{i}}}}{_{j R_{i}}e^ {_{j}}}X_{kl})^{2}].\] (8)

_3rd order partial derivative__:_

\[()}{_{l}^{3}}= _{i=1}^{n}_{i}[_{k R_{i}}}}{_{j R_{ i}}e^{_{j}}}X_{kl}^{3}+2(_{k R_{i}}}}{_{j R _{i}}e^{_{j}}}X_{kl})^{3}.\] \[.-3(_{k R_{i}}}}{_{j R_ {i}}e^{_{j}}}X_{kl}^{2})(_{k R_{i}}}}{ _{j R_{i}}e^{_{j}}}X_{kl})].\] (9)

The proof can be found in Appendix A. The first, second, and third order partial derivatives all have a probabilistic interpretation. Notice that for any \(i\), the coefficients in front of \(X_{kl}\), \(X_{kl}^{2}\), and \(X_{kl}^{3}\) are nonnegative and sum up to 1, _i.e._, \(e^{_{k}}/(_{j R_{i}}e^{_{j}}) 0\) and \(_{k R_{i}}[e^{_{k}}/(_{j R_{i}}e^{_{j}})]=1\). Then, we can regard these coefficients as a discrete probability distribution. Thus, for Equation (8), the term inside \([]\) resembles the variance or second order central moment formula: \([X^{2}]-([X])^{2}=E[(X-[X])^{2}]\). For Equation (9), the term inside \([]\) resembles the skewness or third order central moment formula: \([X^{3}]+2([X])^{3}-3[X^{2}][X]= [(X-[X])^{3}]\).

One may wonder whether for higher orders (order \(r 4\)), the relationship between the \(r\)-th order partial derivative and \(r\)-th central moment still preserve. The answer is no and this can be easily deduced from the following lemma. The proof can be found in Appendix A.

**Lemma 3.2**.: _Let us define \(C_{r}\) to be the \(r\)-th central moment with_

\[C_{r}:=_{k R_{i}}}}{_{j R_{i}}e^{ _{j}}}(X_{kl}-_{k_{1} R_{i}}}}}{_{j R _{i}}e^{_{j}}}X_{k_{1}l})^{r}.\] (10)

_Then we can calculate the partial derivative of \(C_{r}\) with respect to \(_{l}\) as:_

\[}C_{r}=C_{r+1}-r  C_{2} C_{r-1}.\] (11)

From Lemma 3.2, we can see why the connection to central moment does not work for higher order partial derivatives. If \(r=2\), the second term in Equation (11) disappears, _i.e._, \(C_{r-1}=C_{1}=0\). Therefore, we get \( C_{2}/_{l}=C_{3}\). However, for \(r 3\), \(C_{r-1}\) in general is not zero, so we cannot extrapolate this pattern to higher order partial derivatives.

Theorem 3.1 forms the basis upon which we build everything else. These results are not only mathematically interesting but also have significant implications for computation, which we elaborate in the next two sections.

### Time Complexity of First and Second Order Partial Derivative Calculation

From the connections to the second and third central moment, we have the following corollary regarding the time complexity of calculating the first, second, and third order derivatives:

**Corollary 3.3**.: _For the CPH model, the time complexities to calculate \()}{_{l}}\) and \(()}{_{l}^{2}}\) are \(O(n)\)._This is a surprising result, especially for the second order partial derivatives. The intermediate Hessian, \(_{}^{2}()\), takes \(O(n^{2})\) to compute, so we would expect the second order partial derivative, \(()}{_{l}^{2}}=_{j}^{T}^ {T}_{}^{2}()_{j}\), would take \(O(n^{2})\) to compute as well. Yet, the time complexity is just \(O(n)\). We use the first order partial derivative formula, Equation (7), as an example to explain why this happens. We ignore the second term \(_{i=1}^{n}_{i}X_{il}\) because it is just a constant. The first term in Equation (7) can be rewritten as:

\[_{i=1}^{n}_{i}(_{k_{1} R_{i}}}}}{ _{j R_{i}}e^{_{j}}}X_{k_{1}l})=_{i=1}^{n}_{i}(  R_{i}}e^{_{k_{1}}}X_{k_{1}l}}{_{j R_{i}}e^{ _{j}}}).\] (12)

Let us focus on the numerator inside the parenthesis for now. For the entire sequence (\(i=1,2,...,n\)) of numerator terms, we can obtain all of them together at the cost of \(O(n)\) by performing reverse cumulative summation. The same is true when we obtain the entire sequence of denominators. Once we have all numerators and denominators, calculating the entire sequence of ratios inside the parenthesis also costs \(O(n)\). Finally, multiplying each ratio with \(_{i}\) and summing up all these products costs \(O(n)\) as well. Therefore, the computational cost to calculate the first order partial derivative is \(O(n)\). We can apply the same idea to the second order partial derivative formula, Equation 8, to show that the computational complexity is also \(O(n)\). Note that the reverse cumulative summation trick has already been explored in  for calculating the diagonal of \(_{}^{2}()\) in the sample space \(\), but this trick has not been used to calculate the partial derivatives in the feature space \(\).

We will later see how this \(O(n)\) time complexity allows us to design a second order optimization method whose evaluation cost per iteration is just as cheap as a first order optimization method. Before we discuss that, let us continue and discuss another computational implication of Theorem 3.1.

### Lipschitz-Continuity Property of First and Second Order Partial Derivatives

The connection to the central moment calculation allows us to conclude that the first and second order partial derivatives are Lipschitz-continuous. Moreover, we can calculate these Lipschitz constants explicitly. Recall that for a univariate function \(f(x)\), we say that the function is Lipschitz-continuous  if there exists \(L 0\) such that for any two points in the domain of \(f\), _i.e._, \(x,y(f)\), we have \(|f(x)-f(y)| L|x-y|\). The value \(L\) is called the Lipschitz constant for function \(f()\). If the function is continuously differentiable, the previous definition is equivalent to the condition where the first order derivative is bounded, \(|f^{}(x)| L\) for any \(x\).

Not only can we say that the first and second order partial derivatives are Lipschitz-continuous, but we can also calculate the Lipschitz constants explicitly. We summarize the results in the theorem below. The proof can be found in Appendix A

**Theorem 3.4**.: _For the second order partial derivatives in Equation (8), its absolute values are bounded by the following formula:_

\[0()}{_{l}^{2}} _{i=1}^{n}_{i}_{k_{1} R_{i}}X_{k_{1}l}- _{k_{1} R_{i}}X_{k_{1}l}^{2}\] (13)

_For the third order partial derivatives in Equation (9), its absolute values are bounded by the following formula:_

\[|()}{_{l}^{3}} |}_{i=1}^{n}_{i}|_{k_{1} R_{ 1}}X_{k_{1}l}-_{k_{1} R}X_{k_{1}l}|^{3}\] (14)

The availability of these Lipschitz constants suggests that we might construct surrogate functions.

### Quadratic and Cubic Surrogate Functions

We now have all the tools at hand to attack the original optimization problem. For a univariate convex \(f(x)\), if we have access to \(L_{2}\), the Lipschitz constant for its first order derivative, then we can construct the following quadratic surrogate function \(g_{x}()\):

\[f(x+ x) f(x)+f^{}(x) x+L_{2}  x^{2}=:g_{x}( x).\] (15)If we have access to \(L_{3}\), the Lipschitz constant for its second order derivative, then we can construct the following cubic surrogate function \(h_{x}()\):

\[f(x+ x) f(x)+f^{}(x) x+f^{}(x)  x^{2}+L_{3}| x|^{3}=:h_{x}( x)\] (16)

A nice thing about these surrogate functions is that their minimizers can be computed analytically:

\[*{argmin}_{ x}g_{x}( x) =-}f^{}(x)\] (17) \[*{argmin}_{ x}h_{x}( x) =(f^{}(x))(x)-(x))^{2}+2L_{3}|f^{}(x)|}}{L_{3}},\] (18)

where the function \(()\) extracts the sign (\(+\) or \(-\)) of the input. The analytical solution to the quadratic surrogate function is well known, but the analytical solution to this cubic surrogate function has not been well studied. We provide a derivation for Equation (18) in Appendix A.

Since these surrogate functions are convex and are upper bounds of the original functions, minimizing them will lead to a decrease of the original function \(f(x)\) as well. This explains why our methods ensure monotonic decrease in loss and guarantee global convergence. The final algorithms are very easy to understand and can be thought of as coordinate descent-type methods. We anticipate these core ideas can be applied to solve a wide range of problems related to the CPH model. In the next subsection, we showcase two problems our algorithms can tackle.

### Applications to Regularized and Constrained Problems

Regularized ProblemThe first problem is the regularized CPH problem whose penalty terms are separable. The penalties that qualify for this category include LASSO , ElasticNet , SCAD , MCP , etc. For the \(_{1}\)-regularized problems, we can in fact find analytical solutions 2.

For the quadratic surrogate function, solving the \(_{1}\)-regularized problem in Equation (15) is equivalent to solving the following optimization problem (with \(a=f^{}(x)\), \(b=L_{2}\), and \(c=x\)),

\[=*{argmin}_{ x}a x+b x ^{2}+_{1}|c+ x|.\] (19)

The solution for the above problem is

\[=-(a-_{1})/b& bc-a<-_{1 }\\ -(a+_{1})/b& bc-a>_{1}\\ -c&.\] (20)

For the cubic surrogate function, solving the \(_{1}\)-regularized problem of Equation (16) is equivalent to solving the following optimization problem (with \(a=f^{}(x)\), \(b=f^{}(x)\), \(c=L_{3}\), and \(d=x\)):

\[=*{argmin}_{ x}a x+b x ^{2}+c|x|^{3}+_{1}|d+ x|,\] (21)

whose solution is:

\[=(d)-b+-2c((d )a+_{1})}/c&(d)a+_{1} 0\\ (d)b++2c((d)a-_{1})}/c& (d)(a-bd)-cd^{2}>_{1}\\ (d)b++2c((d)a+_{1})}/c& (d)(a-bd)-cd^{2}<-_{1}\\ -d&.\] (22)

Equation (20) is well known in a slightly different format. Equation (22) has not been well studied in the past. We provide derivations for both in Appendix A.

Constrained ProblemThe second problem is the cardinality-constrained CPH problem. Recently, the beam search framework (a combination of the beam search method  from natural language processing and generalized orthogonal matching pursuit ) has shown promise in finding near optimal solutions for a class of \(l_{0}\)-constrained nonconvex problems, including sparse ridge regression  and sparse logistic regression .

Similar to the generalized orthogonal matching pursuit algorithm, we expand our support (starting from an empty set) by adding one feature at a time until the cardinality is satisfied. However, instead of selecting features based on partial derivatives, we select features based on which coefficient, if optimized, can result in the largest decrease of the loss function. After the feature is added into the support, we fine-tune all nonzero coefficients in the support. Additionally, during each support expansion step, we select multiple feature candidate instead of the best one, similar to the core idea in beam search. We use our coordinate descent methods to solve the feature selection step and the coefficient fine-tuning step.

Although the beam search framework has already been proposed for other cardinality-constrained problems, it cannot be applied directly to the CPH model without our coordinate descent methods to select features, especially in the highly correlated settings.

## 4 Experiments

We test the effectiveness of our optimization methods on both synthetic and real-world datasets. We run experiments for both regularized and constrained problems mentioned in Section 3.5. Our main objectives are: 1) When minimizing the same objective functions, how fast can our methods converge to the optimal solutions when compared with all existing optimization methods for the CPH model? 2) When coupled with the beam search framework, how well can our methods help with variable selection when compared with the state-of-the-arts methods, especially for challenging scenarios where features are highly correlated?

### Accessing How Fast Our Methods Converge to Optimal Solutions

We compare our methods (one based on the quadratic surrogate function and the other based on the cubic surrogate function) with the existing optimization methods outlined in Section 2: exact Newton method, the quasi Newton method, and the proximal Newton method. We run on both \(_{2}\)-regularized CPH problems and \(_{1}+_{2}\)-regularized CPH problems. The choices of these regularizations are: \(_{2}=\{0,1\}\) and \(_{2}=\{1,5\}\). The coefficients are all initialized to be \(0\). In the main paper, we show results on the _Flchain_ dataset in Figure 1. More results on other datasets can be found in Appendix D. During each iteration, the baseline methods [62; 51] optimize all coefficients at once, whereas our methods optimize coefficients sequentially with respect to the original loss function. To assess the per-iteration convergence rate, we plot the CPH loss against the number of iterations. To assess the practical running speed, we plot the CPH loss against the overall time elapsed (wall clock). From the left two plots of loss vs. number of iterations, we see that the _Newton-type baselines sometimes have losses that blow up or increase_ during the initial phase of optimization. This is a common problem of Newton's method. Our methods are the only ones with monotonically decreasing loss curves. This is the main reason why only our method can be used for the beam search framework in the variable selection experiments. From the right two plots of loss vs. overall time elapsed, we can see that _our methods are significantly faster than the baselines_. This is due to the fact that both our first and second order partial derivatives are very cheap to compute (with time complexity \(O(n)\)), as we have explained in Section 3.2.

### Accessing How Well Our Methods Perform Variable Selection

We compare our method with both Cox-based methods and other model classes. For Cox-based models, we run on both synthetic datasets and real-world datasets. For other model classes, we only run on the real-world datasets. To assess how well different methods select important variables, features are highly correlated in all datasets. Synthetic datasets are generated with high correlation level, \(=0.9\). For each continuous feature on the real-world datasets, we perform binary thresholding for preprocessing  to obtain many one-hot encoded binary features. This preprocessing step result in highly correlated features on which it is challenging to perform variable selection. We use the following metrics to evaluate our solution qualities: CPH loss, CIndex, and IBS. On the synthetic datasets where we know the true coefficients, we also calculate the F1 score. We perform 5-fold cross validation and report the mean and standard deviation of different metrics on both the training and test sets. For details about the experimental setup, please see Appendix C. For Cox-based methods, we compare our method with Coxnet, Abess, and Adaptive Lasso.

Results on the synthetic datasets are shown in Figure 2. We plot support size vs F1 score. The F1 score is closely related to the support recovery rate. Our method performs significantly better than the baselines. In particular, on the leftmost plot with 1200 samples, _our method is the only one to achieve 100% recovery rate_; the true support size is 15 and we recover all 15 features with a model of size 15. Results for the Employee Attrition dataset are shown in Figure 3. We plot support size vs. CIndex and support size vs. IBS. Similar to the trend on the synthetic datasets, _our method performs significantly better than the baselines in terms of both metrics_. Lastly, we compare our method with other model classes on the Dialysis dataset. The results are shown in Figure 4. We plot support size vs. CIndex and support size vs. IBS. The results indicate that other model classes are prone to overfitting on the training sets. Our method achieves the best accuracy-sparsity tradeoff. _We are able to obtain solutions with the smallest number of coefficients without losing predictive performance._

Figure 1: Efficiency experiments on the first fold of the Flchain dataset. a) The left two plots are on the \(_{2}\)-regularized problem with \(_{2}=1\). For all Newton-type methods, the losses blow up when regularization is weak. In contrast, our methods (quadratic and cubic surrogates) ensure monotonic decrease of losses. b) The right two plots are on the \(_{1}+_{2}\)-regularized problem with \(_{1}=1\) and \(_{2}=5\). The exact Newton method cannot be directly applied, so we compare only with quasi Newton  and proximal Newton  methods, which have losses that increase at the beginning. Our methods are significantly faster than both baselines. Because the evaluation cost per iteration is very cheap for our methods, we are significantly faster in terms of wall clock time (see the difference between the third and fourth plots). See Appendix D for results on other datasets.

Figure 2: Variable selection on synthetic datasets with high correlation (correlation level \(=0.9\)). From left to right, the sample sizes are 1200, 1000, and 800, respectively. The F1 score (the higher the better) is closely related to the support recovery rate. On the left two plots, we can see our method recovers the true variables significantly better than other methods (\(100\%\) recovery rate on the left plot; true support size is 15). As the sample size decreases, the F1 score decreases for all methods.

All these results demonstrate the superior sparse learning capability of our method. For more results, with all baselines on all datasets, please see Appendix D.

Limitations of FastSurvivalOur work focuses on efficient training and effective variable selection of the CPH model. Other model classes, such as trees, random forests, and neural networks, have their own unique merits in capturing complex patterns when the linear (or in our case, additive) model assumption is not satisfied. Another limitation is using the CPH model itself, since its assumptions do not always hold. Handling this question is out of scope for this work.

## 5 Conclusion and Future Outlook

We presented new optimization methods to train the Cox proportional hazards (CPH) model by constructing and minimizing either a quadratic or a cubic surrogate function. We achieve computational efficiency by exploiting the hidden mathematical structures discovered for the CPH model. Our algorithms are able to train the model significantly faster than previous approaches while avoiding the issue of loss explosion. Furthermore, when applied to the variable selection problem, our method can produce solutions with much fewer parameters while maintaining predictive performance. There are many possible extensions to build upon this work. On the optimization side, it will be interesting to see whether we can derive analytical solutions for other types of regularizers mentioned in Section 3.5. On the theoretical side, questions remain whether higher order partial derivatives are Lipschitz-continuous and how to compute these Lipschitz constants. On the application side, we can apply our method to solve the CPH models with time-varying features , stratifications , and feature interactions .

Figure 4: Variable selection on the Diialysis dataset. We show support size vs. CIndex (left two plots, the higher the better) and support size vs. IBS score (right two plots, the lower the better). We compare our method with other model classes. For both metrics, our method obtains solutions that are significantly sparser than other model classes without losing accuracy on the test sets. Other model classes are prone to overfitting on the training sets.

Figure 3: Variable selection on the Employee Attrition dataset. We show support size vs. CIndex (left two plots, the higher the better) and support size vs. IBS score (right two plots, the lower the better). We compare our method with Cox-based sparse learning methods. For both metrics, our method is significantly better than other baselines.

## Code Availability

Implementations of FastSurvival discussed in this paper are available at https://github.com/jiachangliu/FastSurvival.