# PaGoDA : Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher

Dongjun Kim

Stanford University

CA, USA

dongjun@stanford.edu

&Chieh-Hsin Lai

Sony AI

Tokyo, Japan

chieh-hsin.lai@sony.com

&Wei-Hsiang Liao

Sony AI

Yuhta Takida

Sony AI

Sony AI

&Naoki Murata

Sony AI

&Toshimitsu Uesaka

Sony AI

&Yuki Mitsufuji

Sony AI, Sony Group Corporation

&Stefano Ermon

Stanford University

###### Abstract

The diffusion model performs remarkable in generating high-dimensional content but is computationally intensive, especially during training. We propose **P**rogressive **G**rowing **of** Diffusion **A**utoencoder (**PaGoDA**), a novel pipeline that reduces the training costs through three stages: training diffusion on downsampled data, distilling the pretrained diffusion, and progressive super-resolution. With the proposed pipeline, PaGoDA achieves a \(64\) reduced cost in training its diffusion model on \(8\) downsampled data; while at the inference, with the single-step, it performs state-of-the-art on ImageNet across all resolutions from \(64 64\) to \(512 512\), and text-to-image. PaGoDA's pipeline can be applied directly in the latent space, adding compression alongside the pre-trained autoencoder in Latent Diffusion Models (e.g., Stable Diffusion). The code is available at https://github.com/sony/pagoda.

## 1 Introduction

Diffusion Models (DM) [1; 2], which generate content through gradual denoising, have recently achieved high fidelity in high-dimensional generation [3; 4]. While slow sampling has been improved by distilling trained DMs into single-step generators [5; 6; 7], DMs remain computationally intensive, especially at high resolutions, requiring substantial data and GPU resources, thereby limiting large-scale training to a few organizations [8; 9]. This highlights the need for a more efficient pipeline to reduce both training and inference costs while maintaining the quality.

To address these challenges, we present **P**rogressive **G**rowing **of** **D**iffusion **A**utoencoder (**PaGoDA**), a novel pipeline that significantly reduces costs while achieving competitive quality with one-step sampling. PaGoDA is built on a simple yet effective idea: while diffusion distillation  is typically treated as a final stage of the whole pipeline, we explore to have one more stage for the super-resolution after diffusion distillation. This approach led us to design PaGoDA with three distinct stages as below.

**PaGoDA's Proposed Training Pipeline**

**Stage 1.**: (Pretraining) Train a DM on downsampled data.
**Stage 2.**: (Distillation) Distill the trained DM with DDIM inversion to a one-step generator.
**Stage 3.**: (Super-Resolution) Progressively expand the generator for resolution upsampling.

By adding Stage 3 for the super-resolution after the distillation phase, our approach gains a key advantage: training DM on a low-dimensional, downsampled space rather than directly in the desired high-dimensional space. This dimensional reduction substantially lowers the computational demands of diffusion pretraining by orders of magnitude. For example, an \(8 8\) downsampling rate reduces the training computation by a factor of \(64\). Moreover, the computational costs for the distillation and super-resolution stages are relatively minimal compared to the initial diffusion pretraining, making our pipeline highly efficient in terms of overall computation.

Figure 1 provides an overview of our pipeline. We begin with DM trained at base resolution, and generate a dataset of base-resolution data-latent pairs \((,)\), where \(\) is real data and \(\) is the latent representation of \(\), obtained by DDIM inversion . In Stage 2, we train a decoder to map \(\) back to \(\), completing the diffusion distillation . In Stage 3, we add ResNet blocks  to enhance sample resolution and progressively train these newly added upscaling networks, as visualized in Figure 2. The novel use of DDIM inversion in the distillation process, first introduced in PaGoDA, enables the decoder to be trained with the high-frequency signal from the real data at Stage 3. This integration of DDIM inversion establishes strong connections across stages, creating a cohesive and unified framework.

In our experiments, we employed the progressively growing generator to upsample from the pre-trained diffusion model's \(64 64\) resolution to generate samples at \(512 512\) resolution. Notably, PaGoDA achieved state-of-the-art (SOTA) Frechet Inception Distances (FID)  on ImageNet across all resolutions from \(64 64\) to \(512 512\). Additionally, we demonstrated PaGoDA's effectiveness in addressing inverse problems and facilitating controllable generation. However, PaGoDA's potential extends beyond its current application. As PaGoDA being a dimensional reduction technique that operates independently of Latent Diffusion Models (LDM) , PaGoDA could be directly applied into the latent space as-is, offering the possibility of further gain on training computes. We leave this exploration as a promising avenue for future research.

Figure 1: Pipeline overview. PaGoDA deterministically encodes with downsampling followed by DDIM inversion, and constructs its decoder in a progressively growing manner.

Figure 2: (Top) At Stage 2, PaGoDA learns the one-step generator at a base resolution. (Down) At Stage 3, PaGoDA progressively learns for super-resolution by adding additional network blocks.

## 2 Preliminary

DM  samples from the data distribution \(p_{}\) through an iterative denoising process, beginning from a Gaussian prior distribution \(p_{}\). This denoising process attempts to reverse  a forward diffusion process. If the forward process is defined by \(_{t}=\,_{t}\), the deterministic counterpart of the denoising (generation) process, known as the probability flow ordinary differential equation (PF-ODE) , or DDIM , is expressed as

\[_{t}}{t}=-t p_{t}(_{t} )-ts_{_{0}}(_{t},t),\]

where \(s_{_{0}}(_{t},t)\) is a neural approximation of \( p_{t}(_{t})\). Consequently, (deterministic) sample generation from DM is equivalent to solving the PF-ODE (or DDIM) along the trajectory, formally,

\[_{0}^{}(_{T})=_{T}-_{T}^{0}ts_{ _{0}}(_{t},t)\,t,_{T} p_{ {prior}}.\]

Modern solvers of the PF-ODE [10; 14] have significantly accelerated sampling speed, reducing the required network evaluations from hundreds to tens. To further speed up sampling, DMs are distilled with a student model \(G_{}:^{d}^{d}\) to map from \(_{T}\) to \(_{0}^{}(_{T})\) by minimizing

\[_{}(G_{})=_{p_{}( _{T})}_{0}^{}(_{T})-G _{}(_{T})_{2}^{2}.\] (1)

We call this DDIM-based approach as the _noise-to-data_ distillation.

## 3 Progressive Growing of Diffusion Autoencoder

### Stage 1: Diffusion Models Trained on Downsampled Data

Training DMs for high-dimensional data generation is primarily feasible for a limited number of well-resourced organizations, largely due to two factors: access to large-scale datasets and substantial computational resources. This centralization of model development underscores the urgent need to democratize access by significantly reducing resource demands during diffusion training. While several strategies [15; 3] have been proposed, our approach, PaGoDA, introduces a paradigm shift by training the DM at a downsampled resolution in Stage 1, rather than at the original full resolution. For instance, training on a \(d\)-dimensional downsampled resolution requires approximately \(4^{n}\) times less computational budget compared to training in the full \(4^{n}d\)-dimensional space. In practical terms, when \(n=3\), this translates to training in an 8\(\)8 downsampled space, effectively reducing training costs by a factor of 64\(\), thus making large-scale diffusion training more accessible to a broader range of researchers.

Although this paper does not extend PaGoDA's application to the LDM such as SD, training on a (say) 4\(\)4 downsampled latent space could theoretically reduce the computational cost by 16\(\) compared to full-resolution latent training, further emphasizing PaGoDA's potential for widespread adoption. In the case of generating 1024\(\)1024 images, PaGoDA requires training the diffusion model at only 32\(\)32 resolution, with Stage 3 subsequently upscaling it to the full 128\(\)128 latent space of conventional approaches [8; 9]. This progressive approach illustrates PaGoDA's effectiveness in maintaining model quality while lowering the barriers to high-resolution diffusion training.

### Stage 2: Diffusion Distillation on Downsampled Data with DDIM Inversion

After pretraining DM on the downsampled space, PaGoDA distills DM to a one-step generator. For distillation, PaGoDA introduces a new loss specifically designed for later usage in super-resolution at Stage 3. In particular, we propose the reconstruction loss (compare it with \(_{}\) in Eq. 1 of Section 2)

\[_{}(G_{}):=_{p_{}( _{0})}_{0}-G_{}_{T}^{^{-1}}(_{0})_{2}^{2},\] (2)

where \(_{T}^{^{-1}}(_{0})\) is now the latent representation of \(_{0}\), obtained from DDIM inversion, not from DDIM, i.e., the solution at time \(T\) of the PF-ODE starting from \(_{0}\) in time forward, defined by

\[_{T}^{^{-1}}(_{0}):=_{0}-_{0}^{T} ts_{_{0}}(_{t},t)\,t.\]

[MISSING_PAGE_FAIL:4]

where \(_{0}^{d}\) is the downsampled counterpart of \(_{}^{4^{n}d}\). The adversarial loss in this stage is

\[_{p_{}(_{})} D_{}( _{})+_{p_{}()} 1-D_{}G_{}() .\]

Overall, both the reconstruction and adversarial losses are combined to guide training.

Stage 3 employs two key mechanisms to effectively capture high-frequency details while maintaining training stability. First, the reconstruction loss is applied directly to high-dimensional real data, which was not feasible with earlier noise-to-data distillation methods with Eq. 1. As illustrated in Figure 3, \(_{}\) stabilizes the upscaling process by preventing objects from shifting across resolutions, allowing the added neural network to focus solely on upsampling. Second, the adversarial loss operates directly in high-dimensional space, enabled by the one-step generator trained in Stage 2. This generator is critical; without it, adversarial training in Stage 3 would be infeasible. As shown in Figure 4 tested on ImageNet, the adversarial loss is pivotal for achieving effective upscaling performance.

### Optimality Guarantee and Training Stability of PaGoDA Pipeline

When using the conventional \(_{}\) for distillation, the optimal student becomes \(G_{^{*}}(_{T})=_{0}^{}(_{T})\), meaning that the student's samples replicate those of DDIM. As a result, the student's performance is heavily dependent on the teacher's performance. Consequently, the student's generative distribution may diverge from the real data distribution, even when \(_{}\) is combined with adversarial loss. In contrast, by using the DDIM inversion-based reconstruction loss proposed in Stage 2, we mathematically prove in Theorem 3.1 that the optimal student's generative distribution aligns with the real data distribution. As visualized in Figure 5, our PaGoDA Stage 2 (red) achieves robust performance even with a weaker teacher, unlike traditional noise-to-data distillation loss \(_{}\) of Eq. 1, which struggles despite the use of adversarial loss.

**Theorem 3.1**.: _Let \(>0\). Suppose \(D^{*}(G)*{arg\,max}_{D}_{}(G,D)\). If both PaGoDA's reconstruction loss and adversarial loss share a common minimizer \(G^{*}\), then \(p_{G^{*}}()=p_{}()\). Here, \(p_{G^{*}}\) is the generative distribution learned by optimizing Eq. (4)._

Additionally, Theorem 3.2 shows that PaGoDA's training is stable with the help of reconstruction loss, even with adversarial training. We empirically observe that PaGoDA can be trained effectively without many of the techniques typically used to stabilize GANs [20; 21].

**Theorem 3.2**.: [Informal] _Let \(E\) be a fixed deterministic encoder. Suppose that at the generator's equilibria \(G^{*}\) of Eq. (4), \(p_{G^{*}}()=p_{}()\), and \(=G^{*}(E())\). Then, under conditions similar to those found in the stability literature for improving GAN [22; 21], training with Eq. (4) is stable (gradient descent locally converges to its equilibria)._

We refer to Theorems B.4 and B.9 for rigorous and extended versions of Theorems 3.1 and 3.2, respectively. All proofs can be found in Appendix B.

## 4 PaGoDA with Classifier-Free Guidance

In this section, we integrate Classifier-Free Guidance (CFG) [23; 4] into PaGoDA for Text-to-Any generation, with a focus on Text-2-Image. Incorporating CFG alters the sample distribution, necessitating adjustments to the loss functions for Stages 2 and 3. Since previous GAN literature [24; 25; 26; 27] has not addressed CFG integration, we introduce the classifier-free guided adversarial loss to accommodate this adaptation.

Figure 4: The adversarial loss makes PaGoDA competitive with GAN-based super-resolution models in Stage 3.

Figure 5: Comparison of \(_{}\) and \(_{}\), both combined with \(_{}\), using identical hyperparameters. \(_{}\) shows the robust performance, also supported by Theorem 3.1.

CFG guides the denoising process by adjusting the conditional score gradient \( p_{t}(_{t}|)\) into a guided score \( p_{t}(_{t}|)+(-1) p(| _{t})\). This adjustment leads our distillation learning target from \(p_{}(|)\) to \(p_{}(|,)\), defined by

\[p_{}(|,) p_{}(|)^{}p_{}()^{1-},\]

reflecting the influence of guidance strength \(\).

### Classifier-Free Guided Adversarial Loss

To describe the classifier-free adversarial loss, we first consider the loss:

\[_{}^{,}(G_{},D_{ }):=_{p_{}(|, )} D_{}(,,)+ _{p_{G_{}}(|,)} 1-D_{}(,,),\]

where now both generator and discriminator incorporates \(\) as an additional condition , see Eq. (3) for the comparison. From the standard GAN argument , this GAN loss guarantees the optimal generator to match to the data distribution, i.e., \(p_{G^{}}(|,)=p_{}(| ,)\). Hence, the classifier-free adversarial loss could be defined by

\[_{}^{}(G_{},D_{ }):=_{p_{}()()} _{}^{,}(G_{},D_{ })\\ =_{p_{}()()p_{ }(|,)} D_{} ,,+_{p_{}( )()p_{G_{}}(|, )}1-D_{},, .\]

A key challenge with \(_{}^{,}\) is that sampling from \(p_{}(|,)\) is generally infeasible, making it difficult to compute the first term of \(_{}^{}\). To address this issue, we leverage the Bayes formula

\[p_{}()()p_{}(|, )=p_{}(,)p(|,),\]

where both representations are two different ways to decompose the joint distribution over \((,,)\), with \(()\) being the prior distribution of the CFG scale \(\). From this formula, if we could predict the guidance weight \(\) by observing \(\) and \(\), i.e., if we know \(p(|,)\), then sampling \((,,)\) from \(p_{}()()p_{}(|,)\) can be alternatively achieved by: 1) sampling \((,)\) from \(p_{}(,)\), and 2) predicting most likely \(\) using \(p(|,)\).

We approximate \(p(|,)\) with a U-Net encoder network with 1-dimensional output, called _CFG weight estimator_\(_{}\). The input of \(_{}\) network is a single-channel matrix with \((i,j)\)-th value as the multiplication of the \(ij\)-th values of \(\)/\(\) CLIP embeddings, respectively. As this matrix is high-dimensional, we input the downsampled \(64 64 1\) matrix to the U-Net encoder. These CLIP embeddings are also used to condition the network. With DM pretrained at Stage 1, which is supposed to be sufficiently close to the data distribution, we train the CFG weight estimator by minimizing \(_{p_{}()p_{}()()}[ \|-_{}(}(,, ),)\|_{2}^{2}]\), where \(}(,,)\) is a clean base-resolution sample drawn the teacher diffusion. Then, \(_{}(,)\)-value becomes the point estimation of \(p(|,)\).

### PaGoDA Pipeline with Classifier-Free Guidance

We replace the adversarial loss in Stages 2 and 3 with the proposed classifier-free guided adversarial loss. In Stage 3, we shift the focus from \(^{d}\) to \(_{}^{4^{n}d}\) to effectively capture high-frequency details. Additionally, in both Stages 2 and 3, we replace the input of the generator in the reconstruction loss to be classifier-free guided DDIM inversion noise. To enhance text-sample alignment, we further regularize training with CLIP  similarity. For training, we use the ViT-L/14  CLIP model pretrained on YFCC100M , while for evaluation, we use the ViT-g/14 CLIP model pretrained on LAION-2B , minimizing the risk of overfitting.

## 5 Experiments

### PaGoDA Tested on ImageNet without CFG

We conduct experiments on ImageNet using PaGoDA without CFG to validate the core pipeline described in Section 3, utilizing the discrete time diffusion scheduling proposed by EDM . Before training, we collect DDIM inversion latent representations for all ImageNet data using the Heun method  with 40 timesteps (79 NFE). Throughout the experiments, we maintain the batch size to be 256 for both \(_{}\) and \(_{}\) in Stages 2 and 3. We initialize our base resolution generator with the pre-trained diffusion U-Net. Following CTM , we implement adaptive weighting  with \(=0.2^{i}}_{}\|_{2}^{2}}{ \|_{^{i}}_{}\|_{2}^{2}}\), where \(^{i}\) represents the last layer of the generator.

For higher resolution generation, we double the previous resolution by adding two auxiliary ResNet blocks followed by one upsampler ResNet block. The previously trained generator remains frozen, except for the highest-resolution blocks, which are unfrozen. We then train these newly added blocks along with the unfrozen parts, using a fixed GAN weight of \(=1.0\). Appendix A.1 provides additional details. By freezing part of the trained generator, we achieve greater stability in super-resolution training without adaptive weighting. See Figure 6 for uncurated \(512 512\) random samples of ImageNet without CFG.

#### 5.1.1 Quantitative Results

Table 1 presents the performance of PaGoDA. Our model consistently outperforms all existing models across all resolutions, achieving SOTA FIDs without the need of CFG and any other stabilization tricks for GAN. Remarkably, PaGoDA's Inception Score (IS)  is on par with other diffusion and GAN models that employed CFG, which implies that PaGoDA samples are as distinctive as CFG samples. Also, PaGoDA generates samples as diverse as the real data distribution, evidenced by diversity recall metric , where the PaGoDA reports 0.63 for \(64 64\) resolution (data's recall is 0.67). In contrast, StyleGAN-XL is far behind of PaGoDA in terms of the diversity metric, reporting 0.52 for \(64 64\) resolution. Note that we used StyleGAN-XL's discriminator in PaGoDA training, implying that the reconstruction loss significantly improves the sample diversity.

#### 5.1.2 Discussion on Base Resolution

When applying PaGoDA pipeline, the choice of downsampled base resolution in Stage 1 will be primarily determined by available computational resources. Thus, we investigate the impact of the base resolution at this section. To understand the impact, we conducted experiments at \(32 32\) and \(64 64\) resolutions, as summarized in Table 2. Starting at resolutions below \(32 32\) imposes excessive complicacy on the upscaling network, while higher resolutions significantly increase the computational costs at the Stage 1. Therefore, our analysis focuses on these two resolutions, balancing between computational efficiency and upscaling feasibility.

We utilized only 1 H100 node with 8 GPUs for diffusion training on \(32 32\) with 4096 batch size. Also, for \(64 64\) diffusion, we borrow a pretrained checkpoint , which used \( 32\)3 A100 GPUs to train with 4096 batch size. Results in Table 2 demonstrate that the diffusion model trained in Stage 1 maintains robust performance across both resolutions. Interestingly, the one-step generator distilled

Figure 6: Uncurated samples generated by PaGoDA at resolution \(512 512\)_without CFG_. Left: class 31 (tree frog); Right: class 33 (loggerhead turtle).

in Stage 2 consistently outperforms the teacher model, likely benefiting from the effectiveness of StyleGAN-XL , combined with the reconstrcution loss. In Stage 3, the degree of upscaling from the base resolution emerges as the most influential factor for the quality, with upscaling up to 8x showing minimal performance degradation across both tested resolutions.

The upscaler in PaGoDA refines coarse samples generated at lower resolutions, making the pipeline inherently aligned with the scaling laws of smaller resolutions. This design is advantageous, as scaling laws typically worsen with increasing resolution , while PaGoDA leverages the more favorable scaling dynamics at lower resolutions to maintain efficiency. Furthermore, the lightweight upscaling module introduces minimal additional latency, keeping inference times nearly identical to those at the base resolution. This practical efficiency makes PaGoDA a promising solution for scalable diffusion model training across various computational settings.

#### 5.1.3 Discussion on Upscaling Capability

In Stage 3, we train the super-resolution module using a combination of reconstruction and adversarial losses. As shown in Figures 3 and 4, we compare PaGoDA's performance to that of StyleGAN-XL. The comparison reveals key insights: 1) PaGoDA maintains consistent object alignment across resolution jumps, largely due to the reconstruction loss, and 2) its performance is strongly influenced by the GAN component, which plays a crucial role in capturing high-frequency details.

Other upsampling methods, such as SD and Cascaded Diffusion Models (CDM)  also target high-quality upscaling. While PaGoDA, CDM, and SD share the same goal, they adopt different approaches, making them complementary rather than competing solutions. In fact, their strengths can be combined to enhance overall compression and upscaling performance. For instance, CDM or PaGoDA can be applied to the latent space of SD, integrating their techniques for better results. Despite their compatibility, it is still essential to assess how these methods compare in terms of their upscaling effectiveness. In the following analysis, we break down the upscaling capabilities of PaGoDA, CDM, and SD to understand their respective strengths and potential limitations.

    & Sampling &  &  &  &  \\  & NFE & FID \(\) & IS \(\) & Recall \(\) & FID & IS & Recall & FID & IS & Recall & FID & IS & Recall \\   \\  RIN  & 250 & 1.23 & 66.5 & - & - & - & - & 2.75 & 144.1 & - & - & - & - \\ simple Diffusion  & 250 & - & - & - & - & - & 1.91 & 171.9 & - & 2.05 & 189.9 & - \\ VDM++  & 79 & 1.43 & 63.7 & - & - & - & - & 1.75 & 171.1 & - & 1.78 & 190.5 & - \\ StyleGAN-XL  & 1 & - & - & - & 1.51 & **82.35** & 0.52 & - & - & - & 1.81 & **200.55** & 0.55 \\ CTM  & 1 & 1.92 & 70.38 & 0.57 & - & - & - & - & - & - & - & - \\ PaGoDA (ours) & 1 & **1.21** & 76.47 & **0.63** & - & - & - & **1.48** & 174.36 & **0.61** & - & - & - \\   \\  DT-XL  & 250 & 9.62 & 12.15 & - & 2.27 & 278.2 & - & 12.03 & 105.3 & - & 3.04 & 240.8 & - \\ simple Diffusion  & 250 & 2.77 & 211.8 & - & 2.44 & 256.3 & - & 3.54 & 205.3 & - & 3.02 & 248.7 & - \\ VDM++  & 250 & 2.40 & 225.3 & - & 2.12 & 267.7 & - & 2.99 & 232.2 & - & 2.65 & 278.1 & - \\ EDDAZ-XXL  & 63 & - & - & - & - & - & 1.91 & - & - & 1.81 & - & - \\ StyleGAN-XL  & 1 & - & - & - & 2.30 & **265.12** & 0.53 & - & - & 2.41 & **267.75** & 0.52 \\ PaGoDA (ours) & 1 & **1.56** & 259.61 & **0.59** & - & - & **1.80** & 251.31 & **0.58** & - & - \\   

Table 1: Experimental results of PaGoDA on ImageNet.

   Model & Base Res & Upscaled Res & NFE & FID & Base Res & Upscaled Res & NFE & FID & Speed [s] & Params \\  Teacher Diffusion & \(32 32\) & \(32 32\) & 79 & 1.75 & \(64 64\) & \(64 64\) & 79 & 2.44 & 3.16s & 296M \\   & \(32 32\) & \(32 32\) & 1 & 0.79 & & & & & & \\  & \(32 32\) & \(64 64\) & 1 & 1.34 & \(64 64\) & \(64 64\) & 1 & 1.21 & 0.040s & 296M \\   & \(32 32\) & \(128 128\) & 1 & 1.61 & \(64 64\) & \(128 128\) & 1 & 1.48 & 0.041s & 299M \\   & \(32 32\) & \(256 256\) & 1 & 1.83 & \(64 64\) & \(256 256\) & 1 & 1.56 & 0.044s & 301M \\   & & & & & & \(64 64\) & \(512 512\) & 1 & 1.80 & 0.046s & 302M \\   

Table 2: Ablation of base resolution.

   Model & Resolution & Params & NFE & FID \\   & \(64^{2}\) DM & 1.1B & 63 & 1.33 \\  & \(512^{2}\) LDM & 1.1B & 63+1 & 1.96 \\   & \(64^{2}\) DM (teacher) & 0.3B & 79 & 2.44 \\   & \(64^{2} 64^{2}\) & 0.3B & 1 & 1.21 \\   & \(64^{2} 512^{2}\) & 0.3B & 1 & 1.80 \\   

Table 3: Comparison on upsampling.

Since PaGoDA is experimented based on EDM , we adapted the experimental results from EDM2  to facilitate a direct comparison with PaGoDA in the upscaling performance. EDM2 presents results for both pixel DM and latent DM. In latent diffusion, a \(512 512 3\) image is compressed into a \(64 64 4\) latent space for training DM, while pixel diffusion operates directly on \(64 64 3\) images, sharing the identical network architecture used in its latent DM. As reported in Table 3, EDM2 shows a minor performance decline from 1.33 to 1.96.

Similarly, PaGoDA exhibits a comparable performance drop from 1.21 to 1.80 when upscaling from \(64 64\) to \(512 512\). This similarity suggests that PaGoDA's upscaling capacity aligns closely with that of the LDM framework, indicating minimal performance differences even when handling high-resolution data.

Lastly, when comparing PaGoDA to CDM, we observe in Figure 7 that CDM encounters significant performance drops beyond certain dimensional thresholds (\(128 128\)), while PaGoDA maintains consistent performance across varying resolutions. This robustness makes PaGoDA a reliable option for high-resolution generation, with its performance remaining steady even as resolution increases.

### Discussion on Controllability

Once we have a trained PaGoDA generator \(G_{_{0}}\), we can utilize it for solving inverse problems  and for controllable generation  in a training-free manner .

**Latent Optimization** We consider the inverse problem: \(=()+\), where \(\) represents the observation, and \(:^{d}^{m}\) with \(d m\) is a known operator. The restored data \(\) can be reconstructed by optimizing the latent. Specifically, if \(^{*}*{arg\,min}_{}\|-(G_{_{0}}(,))\|_{2}^{2}\), then \(G_{_{0}}(^{*},)\) is the best possible estimate of the solution for the inverse problem. Figure 8-(a) displays the outcomes of an inpainting task where latent optimization is employed with Adam optimizer .

**DDIM Inversion** Specific tasks, such as super-resolution illustrated in Figure 8-(b) and class transfer depicted in Figure 8-(c), can be effectively addressed without relying on latent optimization. For these tasks, we apply DDIM inversion to the downsampled observations, then map the DDIM latent back to RGB pixel by feeding the latent into the decoder. Generally, using DDIM inversion yields superior outcomes compared to latent optimization for these types of tasks.

**Latent Interpolation** Building on techniques from GAN research, we also explored latent interpolation for style mixing. Despite our model's latent dimension being larger than the typical 512-dimensional style vector used in GAN, our observations indicate that latent mixing by slerp operation  achieves effective results, as demonstrated in Figure 8-(d).

Figure 8: Controllable generation of PaGoDA with various tasks.

Figure 7: Comparison between PaGoDA and CDM.

### Text-to-Image Generation

We collect the data-latent pairs on the CC12M dataset  through DDIM inversion and utilize the filtered COYO-700M  dataset for adversarial training. The filtering criteria include only data with CLIP score (measured by ViT-B/32 ) higher than 32, and aesthetic score-v2  higher than 5.0. Due to concerns regarding sensitive content in the open-sourced LAION dataset , we were unable to conduct large-scale diffusion training for Stage 1. This constraint led us to focus primarily on stages 2 and 3, leveraging pretrained open-source checkpoints. For the pretrained teacher diffusion, we used the DeepFloyd-IF model , trained on \(64 64\) pixel space. For further experimental details, see Appendix A.2.

Table 4 compares our PaGoDA mainly with the distilled models from SD v1.5 on \(512 512\). One notable observation from the table is that, even though the latent distilled model generates the latent representation in a single step, additional time is required for decoding this latent into image. In contrast, PaGoDA (on pixel teacher) eliminates such decoding step, thereby overcoming the time constraints associated with distilling SD models. For a more detailed breakdown of the time taken by each component, see Figure 9.

Returning to the performance results in the table, PaGoDA achieves performance comparable to that of the teacher model. This superior performance is also observed on a different test set as shown in Table 5, further demonstrating PaGoDA's scalability on text-to-image tasks.

## 6 Conclusion

PaGoDA introduces a training pipeline that can democratize the diffusion training by cutting training budget with orders of magnitudes. The pipeline is consisted of three stages: 1) we pretrain the diffusion models on the downsampled data, 2) we distill the teacher diffusion into a one-step generator on the downsampled data, and 3) we train an upsampler module until we reach to the desired resolution.