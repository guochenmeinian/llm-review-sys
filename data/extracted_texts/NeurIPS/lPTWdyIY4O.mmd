# The Selective \(G\)-Bispectrum and its Inversion:

Applications to \(G\)-Invariant Networks

 Simon Mataigne

ICTEAM, UCLouvain-Louvain-la-Neuve, Belgium

simon.mataigne@uclouvain.be &Johan Mathe

Atmo

San Francisco, CA

johan@atmo.ai &Sophia Sanborn

Science

San Francisco, CA

sophiasanborn@gmail.com &Christopher Hillar

Algebraic

San Francisco, CA

hillarmath@gmail.com &Nina Miolane

UC Santa Barbara

Santa Barbara, CA

nimamiolane@ucsb.edu

Simon Mataigne is a Research Fellow of the Fonds de la Recherche Scientifique - FNRS.Nina Miolane acknowledges funding from the NSF grant 2313150.

###### Abstract

An important problem in signal processing and deep learning is to achieve _invariance_ to nuisance factors not relevant for the task. Since many of these factors are describable as the action of a group \(G\) (e.g., rotations, translations, scalings), we want methods to be \(G\)-invariant. The \(G\)-Bispectrum extracts every characteristic of a given signal up to group action: for example, the shape of an object in an image, but not its orientation. Consequently, the \(G\)-Bispectrum has been incorporated into deep neural network architectures as a computational primitive for \(G\)-invariance--akin to a pooling mechanism, but with greater selectivity and robustness. However, the computational cost of the \(G\)-Bispectrum (\((|G|^{2})\), with \(|G|\) the size of the group) has limited its widespread adoption. Here, we show that the \(G\)-Bispectrum computation contains redundancies that can be reduced into a _selective \(G\)-Bispectrum_ with \((|G|)\) complexity. We prove desirable mathematical properties of the selective \(G\)-Bispectrum and demonstrate how its integration in neural networks enhances accuracy and robustness compared to traditional approaches, while enjoying considerable speeds-up compared to the full \(G\)-Bispectrum.

## 1 Introduction

The visual world is rich with symmetries. For example, the identity of an object is invariant to its position in the visual field; vision has _translational symmetry_. Group theory is the mathematics used to describe transformations, their actions on objects, and the object's symmetry. As such, group theory has penetrated the fields of signal processing and deep learning alike. For example, the Fourier transform, pillar of signal processing, has been adapted to the \(G\)-Fourier transform, with its spectrum decomposing a signal defined over a group into several frequencies. More recently, researchers have become interested in the properties of higher-order spectra such as the _Bispectrum_, and its generalization to signals over groups via the \(G\)-Bispectrum.

\(G\)-BispectrumThe \(G\)-Bispectrum is the Fourier transform of the \(G\)-Triple Correlation (\(G\)-TC). Historically, higher-order spectra found initial applications in the context of classical signal processing as generalizations of the two-point autocorrelation . The work of Kakarala  illuminatedthe relevance of the \(G\)-Bispectrum for invariant theory, as it is the lowest-degree spectral invariant that is complete. Since then, it has appeared in diverse settings such as vision science , machine learning , and 3D modeling .

Limitations of the \(G\)-Bispectrum for Deep LearningThe computational complexity of the \(G\)-Bispectrum has severely limited the reach of its applications. The most salient example of this limitation is in machine learning and deep learning. Convolutional Neural Network (CNN)  reflect and exploit the translational symmetry of the visual world. Group-Equivariant CNNs (\(G\)-CNNs)  do just this, with more general group-equivariant convolutions to exploit symmetries like rotational symmetries. In both cases, one typically wants to preserve transformations throughout the layers of a network (i.e., to be group-_equivariant_), and remove them only at the end when "canonicalizing" an image for classification (i.e., to be group-_invariant_). While the theory of equivariant layers has been thoroughly developed , less attention has been paid to the theory of invariant layers . This is where the \(G\)-Bispectrum enters the picture, and where its computational cost has strongly limited its integration into deep learning.

Commonly, invariance in \(G\)-CNNs is achieved by simply taking an average or maximum over the transformation group (Average or Max \(G\)-Pooling, respectively). However, as noted by Sanborn & Miolane , this is a highly lossy operation removing information about the structure of the signal. While the max operation is indeed invariant (the max of an image is the same as the max of an image rotated by \(90\) degrees), it is _excessively_ invariant: one could permute all of the pixels in the image and without changing the maximum, but with none of the same structure (see Figure 8). To address this, Sanborn & Miolane  used the \(G\)-TC as a \(G\)-invariant layer that is _complete_--that is, it removes group transformations with no loss of signal structure. This approach achieves demonstrably gains in accuracy and robustness , but it is computationally expensive.

Indeed, the space complexity of the \(G\)-TC, i.e, its number of coefficients, scales as \((|G|^{2})\), where \(|G|\) is the size of the group. As each coefficient demands for \((|G|)\) operations, the computational cost or the time complexity of the \(G\)-TC is \((|G|^{3})\). An alternative would be to use the \(G\)-Bispectrum as the pooling layer. However, both its space and time complexities are \((|G|^{2})\). By comparison, the Max \(G\)-pooling layer features a \((|G|)\) computational cost and returns a scalar output. This raises the question of whether one can achieve complete invariance and adversarial robustness without sacrificing too much in terms of computational efficiency.

**Contributions.** In this work, we prove for the first time that we can significantly reduce the computational complexity of the \(G\)-Bispectrum. This result has important implications for signal processing and deep learning on groups, for which the \(G\)-Bispectrum is a foundational computational primitive. Our contributions are:

* We provide a general algorithm that reduces the computational complexity of the \(G\)-Bispectrum from \((|G|^{2})\) to \((|G|)\) in space complexity and from \((|G|^{2})\) to \((|G||G|)\) in time complexity if an FFT is available on \(G\). We term it the _selective \(G\)-Bispectrum_. The algorithm can be applied to any finite group.
* We prove that the selective \(G\)-Bispectrum is complete for the most important finite groups used in practice, i.e., all discrete commutative groups, the dihedral groups of any order, the octahedral and full octahedral group. This significantly extends the work of , who first showed this for _some_ finite, commutative groups, where it was demonstrated that the \(G\)-Bispectrum can be computed with only \(|G|\) space complexity.
* We use the selective \(G\)-Bispectrum to propose a new \(G\)-_invariant_ layer that strikes a balance between robustness and efficiency. In particular, it is more expensive than the Max \(G\)-pooling, but cheaper than the \(G\)-TC pooling.It is also cheaper than the full \(G\)-bispectral pooling of \((|G|^{2})\) time and \((|G|^{2})\) space complexity. The selective \(G\)-Bispectrum is more robust than the max \(G\)-pooling, and almost as robust as the \(G\)-TC.
* We run extensive experiments on the MNIST  and EMNIST  datasets to evaluate how each invariant layer (Max \(G\)-pooling, \(G\)-TC, selective \(G\)-Bispectrum) impacts accuracy and speed on classification tasks. We achieve the expected results: Our layer is faster than the \(G\)-TC and full \(G\)-Bispectrum and more accurate than Max \(G\)-pooling.
* We present several findings important to the design of invariant layers to guide further advances in the field of geometric deep learning. In particular, we show that the accuracy and speed advantagesof the selective \(G\)-Bispectrum is most striking for \(G\)-CNNs with low number of convolutional filters. Conversely, increasing the number of filters in the \(G\)-Convolutions allows the Max \(G\)-Pooling to catch up on the accuracy.This demonstrates that the \(G\)-bispectral pooling will be particularly interesting for neural networks operating under a smaller parameter budget.

We hope that the proposed reduction of the \(G\)-Bispectrum complexity will further open areas of research in signal processing on groups, that were previously prohibited due to the high complexity of the operation.

## 2 Background: \(G\)-Triple Correlation and \(G\)-Bispectrum

The proposed selective \(G\)-Bispectrum operation is closely related to two other foundational operations on signals defined on groups: the \(G\)-Triple Correlation and the full \(G\)-Bispectrum, which we introduce here. The background on group theory, including the definitions of groups, group actions, equivariance and invariance, is presented in Appendix A.

The \(G\)-Triple CorrelationGiven a real signal defined on a finite group \(:G\), the \(G\)-Triple Correlation (\(G\)-TC)  is the lowest order polynomial that is complete, i.e., that conserves all of the information of the signal \(\), up to group action by \(G\).

**Definition 2.1**.: The \(G\)_-Triple Correlation_ of a real signal \(:G\) is given by

\[T()_{g_{1},g_{2}}:=_{g G}(g)(g g_{1})(g  g_{2})g_{1},g_{2} G.\] (1)

The original triple-correlation was introduced for the classical framework of translations of a one-dimensional signal, i.e., where \(X=\) and \((G,\ )=(,+)\). The \(G\)-triple correlation from Definition 2.1 extends the original definition to any finite group \((G,\ )\). In our setting, the signal \(:G\) will be obtained after the \(G\)-convolution of a function \(f:X^{c}\), representing a continuous image with \(c\) channels, with a filter \(:X^{c}\), and the \(G\)-TC will be applied channel-by-channel. Importantly, the \(G\)-TC layer has computational complexity \((|G|^{3})\) and outputs \((|G|^{2})\) coefficients.

The \(G\)-BispectrumThe \(G\)-TC operation has a Fourier equivalent: the \(G\)-Bispectrum. Indeed, the definition of the Discrete Fourier Transform (DFT) can be extended to any finite group (see, e.g., ), as recalled below.

**Definition 2.2**.: Given a set of unitary representatives (Def. A.2) of the equivalence classes of irreps \(_{i}:G(V_{i})\) (Def. A.6), the \(G\)_-Fourier Transform_ on a finite group \(G\) of a signal \(:G\) is defined as

\[()_{_{i}}:=_{g G}(g)_{i}(g)^{},\] (2)

where \(_{i}(g)^{}\) refers to the conjugate transpose of the matrix \(_{i}(g)\) (or simply transpose if \(_{i}\) is real-valued).

The \(G\)-Bispectrum \(()\) is defined as \((T())\), with \(\) evaluated over the group \(G G\). Kakarala  proposed a closed-form expression for the \(G\)-Bispectrum \(()\) directly in terms of \(()\). We recall it in Theorem 2.3.

**Theorem 2.3**.: _[_17_]_ _The \(G\)-Bispectrum of a signal \(:G\), \(()\), is given by:_

\[()_{_{1},_{2}}=[()_{_{1}} ()_{_{2}}]C_{_{1},_{2}}[_{ _{1}_{2}}()_{}^{}]C _{_{1},_{2}}^{},\]

_where \(C_{_{1},_{2}}\) is a unitary matrix called the Clebsch-Gordan matrix, whose definition is recalled in Appendix A. For each pair \(_{1},_{2}\), the matrix \(()_{_{1},_{2}}\) is called a \(G\)-bispectral coefficient._

For commutative groups, Theorem 2.3 simplifies to a more compact expression, recalled in Theorem A.12. However, both the space and time complexity of the \(G\)-Bispectrum remain \(O(|G|^{2})\).

To the authors' best knowledge, there is no generic analytical formula for computing \(C_{_{1},_{2}}\) for an arbitrary group \(G\). However, there exist formulas for specific classes of groups (see Appendix E.2). Additionally, there exist packages for computing these for many groups using packages such as escnn.

Complete \(G\)-InvariantsThe \(G\)-TC and the \(G\)-Bispectrum are desirable computational primitives for signal processing and deep learning because they are _complete \(G\)-invariants_ (for generic data \(,\)). Indeed, this completeness property make them very interesting for building invariant layers in \(G\)-CNNs, as they are selectively invariant. We define _complete \(G\)-invariance_ next.

**Theorem 2.4**.: _[_16_, Thm.3.2]_ _The \(G\)-TC and the \(G\)-Bispectrum are complete \(G\)-invariants, i.e., for \(,:G\) with \(()_{}\) nonsingular for all irreps \(\), \(T()=T()\), respectively \(()=()\), if and only if there exists \(h G\) such that \((g)=(h,(g))\) for all \(g G\)._

Application: \(G\)-invariant layersThe \(G\)-CNN architecture, first proposed in , is illustrated in Figure 1. The input signal \(f:X\), typically an image, is processed through a \(G\)-Convolution layer using filters \(\{_{k}\}_{k=1}^{K}\). The output is feature maps \(\{_{k}\}_{k=1}^{K}\) that form a set of \(K\) real-valued signals with domain \(G\). This \(G\)-Convolution layer is traditionally followed by a \(G\)-invariant layer. The most common is the Max \(G\)-Pooling layer. More recent works have proposed two alternatives based on the \(G\)-TC and the full \(G\)-Bispectrum: the \(G\)-TC Pooling  and the (full) \(G\)-Bispectrum  respectively, where the latter requires the computations of the Fourier transforms of the feature maps, preferably computed using a Fast Fourier Transform (FFT) algorithm on \(G\). When testing the impact of the choice of \(G\)-invariant layer, the output of the invariant layer is typically fed to a Secondary Neural Network (NN) to perform the desired task, e.g., image classification. The Secondary NN often takes the form of a Multi-Layer Perceptron (MLP).

Experimental results have demonstrated the superior accuracy and adversarial robustness of the \(G\)-CNN equipped with a \(G\)-TC and \(G\)-Bispectrum invariant layer [29; 28]. However, both methods inherit the high space and time complexity of their respective operations. This raises the question of whether we can reduce this computational complexity.

## 3 Method: The Selective \(G\)-Bispectrum and its Inversion

The Selective \(G\)-BispectrumWe introduce a novel tool for signal processing on groups: the selective \(G\)-Bispectrum. A selective \(G\)-Bispectrum \(_{sel}\) is any \((|G|)\) subset of all coefficients of the \(G\)-Bispectrum \(\) (Definition 2.3), only conserving well-chosen pairs of irreps \((_{1},_{2})\). Which pairs of irreps to select depends on the group of interest. This is possible due to redundancies and symmetries in the full object. Below, we provide an algorithmic procedure to compute the selective \(G\)-Bispectrum for any finite group \(G\) that features at most \(||(|G|)\) coefficients. The procedure is summarized in Algorithm 1. We have the following proposition.

**Proposition 3.1**.: _The selective \(G\)-Bispectrum \(_{sel}\) from Algorithm 1 has at most \(|G|\) coefficients._

Proof.: By construction of Algorithm 1, \(|L_{}|||\). Since \(||\) is at most the number of conjugacy classes of \(G\) (see, e.g., Steinberg [30, Corollary 4.3.10]), we have \(|||G|\). 

In Algorithm 1, we note that the choice of \(_{1}\) is important, since some \(_{1}\) will not allow the user to recover all of the irreps and therefore not ensure the completeness of the selective. We illustrate the computation of the selective \(G\)-Bispectrum in Figure 2, where we choose \(_{1}=_{6}\).

Figure 1: Illustration of the different proposed \(G\)-CNN modules [6; 28]. The input \(f\) is first processed through the \(G\)-convolutional layer composed of \(K\) filters \(\{_{k}\}_{k=1}^{K}\). Then, an invariant layer is chosen (Max \(G\)-pooling, \(G\)-TC, or the selective/full \(G\)-Bispectrum layer). Finally, the “pooled” output is fed to a neural network designed for the machine learning task at hand.

Inverting the Selective \(G\)-Bispectrum for completenessThe _inversion_ of the selective \(G\)-Bispectrum \(_{sel}()\) is reconstructing a signal \(()\) from the \(G\)-Bispectrum coefficients in the list \(L_{}\) such that \(=(g,)\) for some \(g G\) (The \(G\)-Bispectrum is \(G\)-invariant, hence, \(\) can only be recovered at best up to group action). Once \(()\) is known, \(\) can be obtained using the Inverse Fourier Transform. If the selective \(G\)-Bispectrum can be inverted, then, by definition, it is complete in the sense of Theorem 2.4.

## 4 Theory: Completeness of the Selective \(G\)-Bispectrum

Our main theoretical claim is that the selective \(G\)-Bispectrum can be inverted and is a complete \(G\)-invariant that drastically reduces the complexity of the \(G\)-Bispectrum. We prove this claim for many finite groups \(G\) of interest in signal processing and deep learning in a sequence of theorems presented in this section.

Known TheoremsPrevious authors had looked into the \(G\)-Bispectrum inversion problem. It is well known that \(|G|=n\) coefficients are enough for the cyclic group \((C_{n},)=(/_{n},+ n)\).

**Theorem 4.1**.: _[_17_]_ _For cyclic groups \(C_{n}\), \(n_{0}\), the \(C_{n}\)-Bispectrum can be inverted using \(|G|=n\) coefficients if \(()_{} 0\) for all irreps \(\) of \(C_{n}\)._

Similarly for a product of two such groups, we have the following theorem.

**Theorem 4.2**.: _[_10_]_ _For a product of cyclic groups \(C_{n} C_{m}\), \(n,m_{0}\), the \(G\)-Bispectrum can be inverted using \(|G|=nm\) coefficients if \(()_{} 0\) for all \( C_{n} C_{m}\)._

Figure 2: Computation of the selective \(G\)-Bispectrum for the Full Octahedral Group. The gradient of color represents the order in which the \(G\)-bispectral coefficients are computed. The Kronecker Table represents which irreps emerge from the decomposition into irreps of the tensor product \(_{i}_{j}\). We observe that the selective \(G\)-Bispectrum has only \(6\) coefficients, compared to \(100\) coefficients for the full \(G\)-Bispectrum.

New TheoremsFrom now on, we assume that the Fourier transform \(()\) only features _non-zero elements_, or _invertible matrices_ in the case of non-scalar Fourier coefficients. This assumption is supported by the zero probability of encountering this corner case (an arbitrarily small perturbation of any signal makes this assumption true).

We first extend the above results to all commutative groups. The proof relies on the fact that every finite commutative group is the direct sum of finitely many cyclic groups.

**Theorem 4.3**.: _For finite commutative groups \(G\), the \(G\)-Bispectrum can be inverted using \(|G|\) coefficients if \(()_{} 0\) for all \( G\)._

See Appendix D for the proof and derivation of the inversion for the specific case of commutative groups. We note that our approach to inversion is symbolic, in that a solution can be expressed explicitly as a formula in terms of the input. Other approaches are also possible to determine an inverse, such as using least squares  or more recent spectral methods .

We now extend the result to dihedral groups. Dihedral groups are ubiquitous in signal processing and deep learning because they represent the group of rotations and reflections.

**Theorem 4.4**.: _For any dihedral group \(D_{n}\) (symmetries of the \(n\)-gon), \(n_{0}\), we need at most \(+2\) bispectral matrix coefficients for inversion if \((()_{}) 0\) for all irreps \(\) of \(D_{n}\). This corresponds to \(1+4+16 4|D_{n}|\) scalar values._

The proof is provided in Appendix E. We now extend the result to octahedral and full octahedral groups, that are related to the symmetries of the octahedron. These groups are very important in signal processing and deep learning of 3D images.

**Theorem 4.5**.: _For the octahedral group \(O\) which has \(|G|=24\) group elements and \(5\) irreps, we need only \(4\)\(G\)-Bispectral coefficients in the selective \(G\)-Bispectrum. this corresponds to \(172\) scalars. For the full octahedral group \(FO\) which has \(|G|=48\) elements, we only need \(6\)\(G\)-Bispectral coefficients in the selective \(G\)-Bispectrum to perform inversion. This corresponds to \(334\) scalars._

A sketch of proof is provided in Appendix F given the redundancy of the procedure. We see that the selective \(G\)-Bispectrum uses only \(4\) coefficients, compared to \(25\) coefficients needed for the full \(G\)-Bispectrum of the octahedral group. For the full octahedral group, it requires only \(6\) coefficients compared to the \(100\) coefficients of the full \(G\)-Bispectrum. In Figure 3, we compare the full and selective \(G\)-Bispectra of the dihedral group \(D_{4}\) (symmetries of the square) and the octahedral group.

## 5 Experimental results

Implementation and architectureOur implementation of the selective \(G\)-bispectrum layer is based on the gtc-invariance repository, implementing the \(G\)-CNN with \(G\)-convolution and \(G\)-TC layer  and relying itself on the escnn library . The implementations related to this section can be found at the g-invariance repository.

Figure 3: Comparison of full and selective \(G\)-Bispectra for the dihedral group \(D_{4}\) (left) and the octahedral group \(O_{h}\) (right). The Kronecker tables of both groups show which irreps emerge from the decomposition into irreps of the tensor product of irreps \(_{i}_{j}\). The colored boxes highlight the bispectral coefficients chosen for the full and selective Bispectra. Our proposed selective Bispectrum captures the same information as the full Bispectrum but with significantly fewer coefficients.

We propose an experimental assessment of the newly proposed selective \(G\)-Bispectrum layer by comparing it with the Avg \(G\)-pooling, the Max \(G\)-pooling, the \(G\)-TC as invariance operations after the \(G\)-convolution of a \(G\)-CNN on the classification problems of the MNIST dataset of handwritten digits , the EMNIST dataset of handwritten letters  with standard train-test division. These datasets count 10 and 26 classes, respectively. We obtain transformed versions of the datasets - \(G\)-MNIST/EMNIST - by applying a random action \(g G\) on each image in the original dataset.

The objective of our experiments is to isolate the speed-up of the \(G\)-Bispectrum layer. Hence, we consider architectures that only differ by the invariant layer in the classification task, following the experimental set up by . The neural network architecture is composed of a \(G\)-convolution, a \(G\)-invariant layer, and finally a Multi-Layer-Perceptron (MLP), itself composed of three fully connected layers with ReLU nonlinearity. Finally, a fully connected linear layer is added to perform classification. The MLP's widths are tuned to match the number of parameters across each neural network model. The details are given in Appendix G. We highlight here that the pursued objective is to compare the differences in performances of the \(G\)-invariant layers, not to provide the state-of-the-art accuracy on the datasets involved. Henceforth, we do not optimize the architectures to reach the highest possible accuracy. We set simple architectures providing interpretable results for analysis. The experiments a performed using \(8\) cores of a NVIDIA A30 GPU.

Training speed performanceTable 1 recalls the theoretical complexities of the different layers. The computational cost of computing the selective \(G\)-Bispectrum is \((|G||G|)\) if an FFT algorithm is available on \(G\), and \((|G|^{2})\) with classical DFT. in Figure 4, we report the average training times on \((2)/(2)\)-MNIST for 10 runs as the discretization \(C_{n}/D_{n}\) of \((2)/(2)\) varies. In the first case, we use the FFT and observe that the Max \(G\)-pooling and \(G\)-Bispectrum training time scale linearly whereas it scales quadratically for the \(G\)-TC. For \((2)\), we perform a classic DFT on \(D_{n}\) so that the \(G\)-Bispectrum scales worth. However, an FFT could be implemented to speed-up the process.

  Invariance layer & Computational Complexity & Output size & Complete G-invariant \\  \(G\)-TC & \(K(|G|^{3})\) & \(K(|G|^{2})\) & ✓ \\  Full \(G\)-Bispectrum & \(K(|G|^{2})\) & \(K(|G|^{2})\) & ✓ \\  Select. \(G\)-Bispectrum & \(K(|G||G|\) or \(|G|^{2})\) & \(K(|G|)\) & ✓ \\  Max \(G\)-pooling & \(K(|G|)\) & \(K(1)\) & ✗ \\  Avg \(G\)-pooling & \(K(|G|)\) & \(K(1)\) & ✗ \\  

Table 1: \(G\)-CNN invariant layers and their computational cost and output size. \(K\) is the number of filters. The selective \(G\)-Bispectrum that we propose is the complete \(G\)-invariant layer with the lowest time and space complexity. It reduces significantly the cost compared to the \(G\)-TC layer while preserving its completeness.

Figure 4: Evolution of the average training times for the different invariant layers. The parameter \(n\) is the size of the groups \(C_{n}\) and \(D_{n}\). The average and standard deviations are obtained over \(10\) runs. For all runs, the number of parameters of the complete neural network (filters and MLP) is set to \(50000\) and \(150000\) for \((2)\) and \((2)\) respectively. Standard deviations are reported by vertical intervals. When a FFT is available, our selective G-Bispectrum significantly outperforms other complete G-invariant pooling layers in terms of speed. Specifically, when working with \(C_{2^{7}}\), training on a dataset of \(60000\) images takes only \(247\) seconds, whereas the \(G\)-TC requires 1465 seconds.

Classification PerformanceWe compare the performances of the \(G\)-Bispectrum layer with respect to the \(G\)-TC, the Max \(G\)-pooling and the Avg \(G\)-pooling models, trained on the \((2)\)/\((2)\)-MNIST/EMNIST datasets and we assess the accuracy by averaging the validation accuracy over 10 runs. The classification accuracy is provided in Table 2. For the experiments in Table 2, the following pattern holds: at equivalent number of parameters, the more computationally expensive the pooling layer, the better the accuracy. However, the use of the \(G\)-TC becomes prohibitive when \(|G|\) increases. In the next section, we discuss the settings where each invariant layer should be preferred, and highlight each invariant layer's strengths and weaknesses.

Discussion on the choice of invariant layerThe first observation from Table 2 is though the _selective_\(G\)-Bispectrum is complete, the model obtains slightly lower accuracy than \(G\)-TC. This observation might be surprising at first, since we prove mathematically in Section 4 that the selective \(G\)-Bispectrum is complete, just as the full version. An explanation to this lies in the paradoxes of the Universal Approximation Theorem . Just because an arbitrarily large MLP can theoretically fit any function, this does not imply that it will happen for a practical, limited MLP. In practice, we hypothesize that the redundancy of the \(G\)-TC allows the MLP to distinguish inputs more easily. If the size of the model allows it, the \(G\)-TC or the full \(G\)-Bispectrum will provide better accuracy. However, when the size of the group is big, their use is often out of reach while the selective \(G\)-Bispectrum is scalable. In Table 2, we also notice that the Max \(G\)-pooling performs well compared to the others even though it is not complete. This is because we have many filters that allow for refined classification. Indeed, assume \(f,_{k}\) are black-and-white images with \(N\) pixels. In consequence, \(_{g}_{k}(g)\{0,1,...,N\}\) for \(k=1,2,...,K\). The Max \(G\)-pooling allows a maximum separation of \((N+1)^{K}\) classes. In practice, this value is not reached, but it explains why Max \(G\)-pooling performs well. Figure 5 highlights this dependency of the Max \(G\)-pooling on the number of filters since the accuracy drops to less than \(60\%\) with 2 filters. In comparison, the \(G\)-TC and the selective \(G\)-Bispectrum, which are _complete_, keep an accuracy above \(85\%\) with 2 filters.

CompletenessTo conclude our numerical experiments, we study the robustness of the selective \(G\)-Bispectrum to adversarial attacks, following the analysis in Sanborn & Miolane [28, Figure 2]. Given an image \(:X^{c}\) and a filter \(:X^{c}\), they numerically verified the robustness (=completeness) of the \(G\)-TC by showing that

\[f^{*}_{f:X^{c}}\|T(*f)-T(*)\|_ {2}^{2} f^{*}=(g,)g G.\] (3)

Indeed, Sanborn & Miolane [28, Figure 2] shows that only images that are identical up to rotation/reflection can yield the same \(C_{n}/D_{n}\)-TC. That is, the \(G\)-CNN with \(G\)-TC can not be "fooled"

  Dataset & Group & \(G\) & Pooling & \(K\) filters & Avg acc. & Std. dev. & Param. count \\    & (2)\)} & \)} & Avg \(G\)-pooling & 24 & 0.74 & \(<0.01\) & 50247 \\  & & & Max \(G\)-pooling & 24 & 0.96 & \(<0.01\) & 50247 \\  & & Select. G-Bispectrum & 24 & 0.95 & \(<0.01\) & 49116 \\  & & & \(G\)-TC & 24 & 0.96 & \(<0.01\) & 48385 \\   & (2)\)} & \)} & Avg \(G\)-pooling & 4 & 0.60 & \(<0.01\) & 147675 \\  & & & Max \(G\)-pooling & 4 & 0.78 & \(<0.01\) & 147675 \\  & & & Select. \(G\)-Bispectrum & 4 & 0.93 & \(<0.01\) & 143029 \\  & & & \(G\)-TC & 4 & 0.96 & \(<0.01\) & 142220 \\   & (2)\)} & \)} & Avg \(G\)-pooling & 24 & 0.40 & \(<0.01\) & 50195 \\  & & & Max \(G\)-pooling & 24 & 0.76 & \(<0.01\) & 50195 \\   & & & Select. G-Bispectrum & 24 & 0.77 & \(<0.01\) & 49254 \\   & & & \(G\)-TC & 24 & 0.80 & \(<0.01\) & 48494 \\    & (2)\)} & Avg \(G\)-pooling & 20 & 0.38 & \(<0.01\) & 48832 \\   & & & Max \(G\)-pooling & 20 & 0.71 & \(<0.01\) & 48832 \\   & & & Select. G-Bispectrum & 20 & 0.74 & \(<0.01\) & 47320 \\   & & & \(G\)-TC & 20 & 0.79 & \(<0.01\) & 46954 \\  

Table 2: Results of numerical experiments averaged over 10 runs with Avg \(G\)-pooling, Max \(G\)-pooling, our selective \(G\)-Bispectrum and \(G\)-TC. The experiments are performed on \((2)/(2)\)-MNIST and \((2)/(2)\)-EMNIST. The table shows the number of filters, the average classification accuracy, standard deviation and parameter count. This table shows that the selective \(G\)-Bispectrum conserves the accuracy of the \(G\)-TC at an equivalent number of parameters.

since only input in the same orbit yield the same output. We perform a similar experiment in Figure 6. Moreover, it is well-known that the \(G\)-convolution \(*f\) is \(G\)-equivariant. Hence, an equivalent experiment is to show that

\[^{*}_{:G}\|T()-T() \|_{2}^{2}^{*}=(g,)g G.\]

In Figure 7, we show that the selective \(G\)-Bispectrum \(_{sel}\) is robust to adversarial attacks by solving

\[^{*}_{:G}\|_{sel}()-_ {sel}()\|_{2}^{2}.\] (4)

The signals are indeed recovered up to a translation, i.e., a group action of \(C_{30}\). Moreover, despite (4) only optimizes using the selective \(G\)-Bispectrum, the full \(G\)-Bispectrum is correctly recovered. This is an additional evidence of the completeness of the selective \(G\)-Bispectrum.

## 6 Conclusion and Future works

In this paper, we introduced a new type of complete invariant layer for \(G\)-invariant CNNs - called _selective \(G\)-Bispectrum layer_ - with the objective of increasing the accuracy and robustness of \(G\)-CNNs compared to those implemented with the initially proposed Max \(G\)-pooling. The \(G\)-TC layer also achieves this goal, but at an output cost of \((|G|^{2})\) coefficients and \((|G|^{3})\) flops that prevents its application to large groups, while the selective \(G\)-Bispectrum layer only outputs \((|G|)\) coefficients. Building on the result of Kakarala  for cyclic groups, we have shown that the completeness of the _selective \(G\)-_Bispectrum layer holds for all commutative groups, all dihedral groups, the octahedral and full octahedral groups. In a suite of experiments, we provided a global

Figure 5: At the top: Evolution of the average classification accuracy with rotated MNIST (\((2)\)-MNIST) and rotated-reflected MNIST (\((2)\)-MNIST) over \(10\) runs when the number of filters varies from \(2\) to \(20\) for the Avg \(G\)-pooling, the Max \(G\)-pooling, the selective \(G\)-Bispectrum and the \(G\)-TC. The number of parameters of each model is maintained equal for fair comparison. The standard deviations are represented using vertical intervals. With the selective \(G\)-Bispectrum layer, we can reduce the number of convolutional filters needed for a given accuracy. For example, with only \(K=2\) filters, we achieve 96% accuracy, compared to 63% with the Max \(G\)-pooling layer. Our approach allows \(G\)-CNNs to maintain competitive accuracy while using smaller neural networks. At the bottom, the same results are displayed with time instead of the number of filters on the \(x\)-axis. The dotted lines reproduce the evolution of \(K\) from the figures at the top. We can observe that the selective \(G\)-Bispectrum is faster than the \(G\)-TC when a FFT is available, thus here in the case of \((2)\)-MNIST. Recall that an FFT can be implemented for many groups 

picture of the strength and weaknesses of each invariant layer. We studied the performance in terms of training speed, classification accuracy and robustness to adversarial attacks. As a result, the selective \(G\)-Bispectrum paves the way for the development of complete invariant pooling layers that can accommodate larger group sizes and, hence, a larger set of symmetries.

Figure 6: Adversarial attacks experiments with \(G=C_{4}\). Images are optimized to output, respectively, a target selective \(G\)-bispectrum and a target \(G\)-Max pooling, obtained from an original image. The initial image is the initialization of the optimization process (3). After training, only for the selective \(G\)-Bispectrum (in blue), the recovered image is a copy of the original image up to group action (rotation). This is a numerical illustration of the robustness of the selective \(G\)-Bispectrum to adversarial attacks: one can not obtain the same output with an input that is not in the same class. On the other hand, \(G\)-Max pooling (in red) outputs a noisy image because it is not complete.

Figure 7: Numerical experiment of signal recovering from original signals \(\{_{i}\}_{i=1}^{15}\) where \(_{i}:G\) by solving (4) with \(G=C_{30}\). We use the gradient method with Armijo line search to solve (4). The recovered solutions \(\{_{i}\}_{i=1}^{15}\) are represented and correspond to translations of the original signals. The moduli of the full \(G\)-Bispectra are also represented and are identical. This experiment corroborates the completeness of the selective \(G\)-Bispectrum since we are able to recover an unknown signal only from the knowledge of its selective \(G\)-Bispectrum.