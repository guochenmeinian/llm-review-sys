# II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models

Ziqiang Liu\({}^{1,2}\)1 Feiteng Fang\({}^{1,3}\)1 Xi Feng\({}^{1,3}\)1 Xinrun Du\({}^{4,14}\)1 Chenhao Zhang\({}^{1,6}\)1

Zekun Wang\({}^{12,14}\) Yuelin Bai\({}^{1,2}\) Qixuan Zhao\({}^{1,3}\) Liyang Fan\({}^{1}\) Chengguang Gan\({}^{7}\)

**Hongquan Lin\({}^{1,3}\) Jiaming Li\({}^{1,2}\) Yuansheng Ni\({}^{9}\) Haihong Wu\({}^{1,3}\) Yaswanth Narsupalli\({}^{5}\) Zhigang Zheng\({}^{1}\) Chengming Li\({}^{10}\) Xiping Hu\({}^{10}\) Ruifeng Xu\({}^{11}\) Xiaojun Chen\({}^{8}\) Min Yang\({}^{1}\) Jiaheng Liu\({}^{12}\) Ruibo Liu\({}^{13}\) Wenhao Huang\({}^{14}\) Ge Zhang\({}^{4,14,15}\) Shiwen Ni\({}^{1}\)2**

\({}^{1}\)Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

\({}^{2}\)University of Chinese Academy of Sciences

\({}^{3}\)University of Science and Technology of China \({}^{4}\)M-A-P \({}^{5}\)IIT Kharagpur

\({}^{6}\)Huazhong University of Science and Technology \({}^{7}\)Yokohama National University

\({}^{8}\)Shenzhen University \({}^{9}\)Zhejiang University \({}^{10}\)Shenzhen MSU-BIT University

\({}^{11}\)Harbin Institute of Technology (Shenzhen) \({}^{12}\)Beihang University

\({}^{13}\)Dartmouth College \({}^{14}\)01.ai \({}^{15}\)University of Waterloo

###### Abstract

The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap, we propose the **I**m**lplication understanding **B**enchmark, **II-Bench**, which aims to evaluate the model's higher-order perception of images. Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%. Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details. Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts. This observation underscores a notable deficiency in their inherent understanding of image sentiment. We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert artificial general intelligence (AGI). II-Bench is publicly available at https://huggingface.co/datasets/m-a-p/II-Bench.

Figure 1: Implication: a significant gap exists between humans and MLLMs on II-Bench.

Introduction

In recent years, the development of Multimodal Large Language Models (MLLMs)[52; 12; 33; 66] has significantly advanced our ability to understand and generate content across various modalities, including text, images, and audio. Leveraging sophisticated architectures and vast amounts of data, MLLMs have demonstrated remarkable performance in image captioning[18; 24; 45], visual question answering[44; 51], video understanding and generation[46; 70], etc.

Nevertheless, comprehensively evaluating the performance of these models remains a challenge. While benchmarks exist for multimodality, such as ScienceQA, MMMU, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs, which refer to nuanced emotional understanding and profound meaning extraction.

Philosopher Suzanne Langer once noted, "Art is the creation of forms symbolic of human feeling." This profoundly summarizes how images often embody human emotions and serve as a conduit for personal views and cultural narratives. Therefore, understanding the meaning of images requires not only meticulous observation but also an exploration of the human emotions and cultural contexts they reflect. In real life, many artworks, comics, and posters are imbued with rich meanings, and artists convey their insights to the audience through these works. These abstract and complex images pose a significant challenge for MLLMs, as the models must possess advanced higher-order perceptual capabilities to accurately understand the human emotions conveyed in the pictures and infer the deeper meanings the creators intend to express. The evaluation of high-order perception is crucial for artificial general intelligence (AGI) because it encompasses the ability to understand complex, non-literal aspects of visual information, much like humans do, which includes recognizing and processing implications, emotional cues, synesthesia or other cognitive features that go beyond mere object recognition or factual knowledge extraction [6; 55]. This implies that a machine's understanding of the world should also incorporate these implications to achieve a more human-like comprehension, which fosters a deeper, more nuanced understanding, serving as an important means of exploration from perceptual intelligence to cognitive intelligence.

However, an effective benchmark for higher-order perceptual capability measurement is notably absent in the current landscape. To fill this gap, we introduce **II-Bench**, a comprehensive benchmark designed to assess MLLMs' higher-order perceptual, reasoning and comprehension abilities. This holistic evaluation enables us to gain a deeper insight into the models' true capabilities, thereby fostering advancements in multimodal AI research.

As illustrated in Figure 2, II-Bench comprises 1,222 images, spanning six domains: life, art, society, psychology, environment and others. Furthermore, II-Bench encompasses diverse categories of images, including illustrations, memes, posters, comics, logos and paintings. By utilizing images from multiple domains and categories, the model's comprehension and reasoning abilities can be evaluated more objectively and comprehensively.

We conduct extensive experiments to evaluate II-Bench on 20 MLLMs. Our main contributions and findings are as follows:

* We introduce II-Bench, the first Image Implication Understanding Benchmark, which is very challenging for current MLLMs.
* A significant difference exists in performance between humans and MLLMs: the highest accuracy achieved by the model is 74.8%, whereas the average accuracy for humans is 90%, with the highest reaching 98%.
* Closed-source models often outperform open-source ones, while the performance gap between the leading closed-source model and the leading open-source model is minimal, only about 1%.
* Models perform worse in domains containing abstract and complex information, such as Art and Psychology, compared to Environment, Life, Society and other domains.
* Incorporating additional emotional polarity information of images into prompts generally enhances model scores, indicating that models lack sufficient emotional understanding of images, leading to misinterpretation of implicit meanings.

Figure 2: Composition of II-Bench.

Our aim with II-Bench is to evaluate MLLMs' higher-order perception of images. We believe that II-Bench will inspire the community to create the next generation of MLLMs, propelling us further on the path toward sophisticated artificial general intelligence (AGI).

## 2 Related Work

### Multimodal Large Language Models

Given that advanced large language models (LLMs) exhibit sophisticated reasoning abilities, strong generality, and extensive world knowledge [48; 49], current multimodal LLMs (MLLMs) typically involve integrating additional modules to align non-textual modality features with the language space. For example, BLIP-2  encodes images using ViT  and employs a Q-Former to map visual features into the language space. Similarly, LLaVA [38; 7; 9] utilizes an MLP as the connector between the visual encoder and the LLM backbone. These architectural designs not only incorporate visual representations into the LLMs but also preserve the advanced capabilities inherent to LLMs. Recent studies have demonstrated that current MLLMs are capable of understanding human minds, reasoning with scientific figures, etc. [4; 49], due to the success of unlocking the abilities of LLM backbones in multimodal settings. Nonetheless, despite the strong implication understanding abilities of LLMs , there is limited research on the implication understanding of images by current MLLMs, and our work addresses this gap for the first time.

### MLLM Benchmarks

The evolution of MLLMs has underscored the importance of comprehensive evaluations within the research community. Initial benchmarks primarily targeted singular tasks, such as the visual question answering (VQA) task [2; 19; 29; 54; 27] and the image captioning task [36; 1; 50]. While notable achievements have been recorded on these benchmarks, they fall short of thoroughly evaluating MLLMs across the broader spectrum of multimodal perception and reasoning. To bridge this gap, recent studies have aimed at evaluating models from various perspectives [40; 32; 31; 62; 17; 43; 5; 71; 20]. For example, MMBench  and SEED [32; 31] explore models' capabilities through common-sense questions, featuring multiple-choice questions across various dimensions of ability. To assess specialized expertise, MMMU  and CMMMU  leverage content from exams and textbooks to enhance domain-specific knowledge evaluation.

However, MMStar  pointed out that the model can answer some benchmarks' questions without images, and there is a risk of data leakage during training. We find that these benchmarks mostly test knowledge or just simple image understanding and don't assess logic and reasoning skills. Image implication understanding represents a more challenging task compared to image understanding, necessitating multi-hop reasoning ability and theory of mind (ToM) [14; 23; 63; 72; 56; 57]--the sophisticated capability intrinsic to human cognition. II-Bench is a benchmark designed to evaluate MLLMs' prowess in both image understanding and reasoning through image implication.

## 3 The II-Bench

### Overview of II-Bench

We introduce the **I**mage **I**mplication Understanding **Bench**mark (**II-Bench**), a new benchmark measuring the higher-order perceptual, reasoning and comprehension abilities of MLLMs when presented with complex implication images. These images, including abstract artworks, comics and posters, possess visual implications that require an understanding of visual details and reasoning ability. II-Bench reveals whether current MLLMs, leveraging their inherent comprehension abilities, can accurately decode the implications embedded within the complex and abstract information presented in these images.

II-Bench contains a total of 1,222 various images. The specific image types and domain statistics can be seen in Figure 5 of the Appendix A. These images are manually collected and annotated by 50 undergraduate students from various disciplines and institutions, with sources from multiple renowned illustration websites. Each image is manually designed with one to three multiple-choice questions, each with six options and only one correct answer. The questions cover the metaphors,symbolism, and detailed understanding of the images. The benchmark includes a total of 1,434 multiple-choice questions, with 1,399 questions used to construct the test set and 35 questions used to construct the development and validation set for few-shot tasks. Figure 3 shows representative examples of II-Bench.

### Data Curation Process

Data Collection.We collect 20,150 raw images from various renowned illustration websites, ensuring a sufficiently extensive raw dataset. Our collectors are well instructed to adhere to copyright and license regulations, avoiding data from sites prohibiting copy and redistribution. We have also taken special measures to prevent data leakage. For detailed information on the specific websites from which we collect images and our data leakage prevention measures, please refer to Appendix B.

Data Filtration.After collecting the raw images, we carefully design a three-stage data filtration procedure. In **Stage 1**, dedicated to _image deduplication_, we utilize image similarity algorithms to perform pixel-based comparisons which allows the identification and elimination of copies and close variants, rendering the dataset unique. In **Stage 2**, focused on _text-to-image ratio control_, we use Optical Character Recognition (OCR) to locate text portions in the images. We then calculate the area occupied by text relative to the total image area. Images are removed if the text-to-image ratio breaches the threshold, ensuring that the dataset remains visually dominant. In **Stage 3**, an exhaustive visual inspection is conducted by humans. Our specific screening protocol is mandated to identify and discard images lacking implications. This strategic exclusion ensures that irrelevant and poor-quality images are weeded out, enhancing the meaningfulness and quality of data retained for further processing. After these filtration stages, we have eliminated over 90% of the original images, leaving us with fewer than 2,000 images.

Figure 3: II-Bench examples sampled from each domain. The pictures include life, art, society, psychology, environment and other domains. Understanding these images and completing the corresponding questions require a certain level of comprehension.

Data Annotation.We forward the annotation sources to the crowdsourcing annotators and perform three steps of data annotation using our carefully devised annotation protocol. The annotators mark the images with their difficulty, image type, domain, and corresponding rhetoric first. An explanation of contained visual implications is then drafted for each image. Finally, the annotators devise 1-3 fine-grained questions per image, each with only one correct answer and five distractor options related to the implication nuances. The detailed annotation protocol is in Appendix B.

Data Quality Assurance.To ensure the quality, difficulty of questions and distractor options and consistency among annotators, we have implemented a rigorous multi-step process. Specifically, each question and option undergoes multiple rounds of meticulous manual annotation to ensure the distractors are sufficiently challenging and not easily distinguishable from the correct option and ensure consistency across different annotators. If any question is found to be insufficiently challenging or answers from different annotators are not consistent, it is revised accordingly. This iterative process continues until at least 4 out of 5 independent reviewers agree that the question is accurate, reasonable, difficult and reach a consensus.

### Dataset Statistics

II-Bench comprises 1,222 images, each accompanied by 1 to 3 multiple-choice questions, totaling 1,434 questions. We randomly select 35 of these questions to construct a few-shot development set and validation set. The average question length is approximately 17 words, and the average option length is 14 words. Each image also includes a description manually annotated by annotators, explaining the human interpretation of the image's implication.

II-Bench encompasses images from six distinct domains: Life, Art, Society, Psychology, Environment and Others. It features a diverse array of image types, including Illustrations, Memes, Posters, Multi-panel Comics, Single-panel Comics, Logos and Paintings. The images are classified based on human understanding into three levels of difficulty: Easy, Middle and Hard. Additionally, they are categorized by the emotional tone they convey: Positive, Neutral or Negative. Furthermore, each image is manually annotated with rhetorical devices such as Metaphor, Exaggeration, Symbolism, Contrast, Visual Dislocation, Antithesis, Analogy, Personification and Others. The detailed statistical information can be found in Table 1.

## 4 Experiment

We conduct experiments on II-Bench using both open-source and closed-source MLLMs. For each model, we employ eight different settings: 1-shot, 2-shot, 3-shot, zero-shot (None), CoT,

  
**Statistics** & \\  Total Questions & 1,434 \\ Total Images & 1,222 \\ Dev : Validation : Test & 15 : 20 : 1,187 \\ Easy : Medium : Hard & 708 : 385 : 129 \\  Average Question Length & 16.91 \\ Average Option Length & 14.05 \\ Average Explanation Length & 170.47 \\  Metaphor & 955 \\ Exaggerate & 191 \\ Symbolism & 236 \\ Visual Dislocation & 71 \\ Antithesis & 27 \\ Analogy & 38 \\ Personification & 108 \\ Contrast & 226 \\ Other & 47 \\    
  
**Statistics** & \\  Life & 516 (42.23\%) \\ Art & 70 (5.73\%) \\ Society & 408 (33.39\%) \\ Psychology & 127 (10.39\%) \\ Environment & 44 (3.60\%) \\ Other & 57 (4.66\%) \\  Positive & 169 (13.83\%) \\ Neutral & 702 (57.45\%) \\ Negative & 351 (28.72\%) \\  Illustration & 374 (28.70\%) \\ Meme & 269 (20.64\%) \\ Poster & 111 (8.52\%) \\ Multi-panel Comic & 311 (23.87\%) \\ Single-panel Comic & 90 (6.91\%) \\ Logo & 59 (4.53\%) \\ Painting & 89 (6.83\%) \\   

Table 1: Statistics of II-Bench.

Domain, Emotion and Rhetoric. "Emotion" denotes prompts where the model is informed about the emotional polarity of the images(e.g., positive, negative), "Domain" involves adding information about the image's domain (e.g., life, environment) to the prompt, and "Rhetoric" signifies prompt with information about the rhetorical devices used in the image (e.g., metaphor, personification), while "None" indicates the use of standard prompts without any additional information. Uniform prompts are applied across all MLLMs, with detailed specifications available in the Appendix C. All experiments are conducted on NVIDIA A800 GPUs.

### Baselines

**MLLMs.** Table 2 provides an overview of the studied MLLMs, highlighting differences in their architectures and parameters. Notably, InternLM-XComposer2 attempts to modify the projection module in LLaVA architecture to better align multiple modalities. Meanwhile, CogVLM2 integrates a visual expert into the large language model, enabling a deep fusion of vision and language features without compromising performance on NLP tasks.

**Evaluation.** Accuracy is used as our main evaluation metric. Given that II-Bench comprises entirely multiple-choice questions, the evaluation merely involves extracting the selected options from the model's responses, thereby simplifying the rule design complexity. Notably, when the model employs chain-of-thought (CoT) prompting, the responses generate intermediate steps. This necessitates that the designed rules possess sufficient robustness or that the model outputs answers in a fixed format. If the options cannot be extracted from the model's response, it is deemed that the model has answered the current question incorrectly. For the robust answer extraction method and the detailed statistics of the model output, please see Appendix E. For reference, we also assessed human performance on II-Bench.

### Main Results

In this section, we present a comprehensive comparison of different MLLMs and humans on II-Bench. The detailed results of different domains and emotions are in Table 3. The detailed results of different image types, levels of difficulty, and rhetoric are in Appendix D. The main experimental results and findings are summarized below:

#### 4.2.1 Gap between Humans and MLLMs

The results indicate a significant disparity between humans and MLLMs on II-Bench. Human participants achieve an average accuracy of 90.3%, with the highest accuracy reaching 98.2%. In comparison, the best closed-source model, Qwen-VL-MAX, achieves an accuracy of 74.8%, while the best open-source model, LLaVA-1.6-34B, scores 73.8%. These results highlight the substantial gap between human capabilities and current state-of-the-art models in understanding image implications. The highest accuracy of the models is substantially lower than the average human score, underscoring the challenges that MLLMs face in this domain.

  
**Model** & **Size** & **ViT** & **Projection Module** & **LLM** \\  CogVLM2-Llama3-Chat  & 19.5B & EVa2-CLIP-E & MLP & Llama-3-8B + Visual Expert \\ MiniCPM-Llama3-2.5  & 8.5B & SigLip-400M & Perceiver Resampler & Llama3-8B \\ InternVL-Chat-1.5  & 25.5B & InternViT-6B & MLP & InternLM2-20B \\ IntermLM-XComposer2-VL  & 7B & OpenAI-VTLarge & PLORA & InterLM-2-2 \\ DeepSee-VL-Chat-7B  & 7.3B & SAM-B + SigLI-PL & MLP & DeepSee-LLM-7B \\ InstructBLIP-T5  & 4.0B/12.3B & ViT-g/14 & MLP & FLAN T5 XL/XXL \\ BLIP-2 FLAN-T5  & 4.1B/12.1B & ViT-g/14 & MLP & FLAN T5 XL/XXL \\ mPLUG-WWL2  & 8.2B & ViT-14 & Visual Abstract & Llama2-7B \\ Qwen-VL-Chat  & 9.6B & ViT-bigG & VI.L adapter & Qwen-7B \\ Yi-VL-34B-Chat  & 7.1B/35.4B & CLIP ViT-H/14 & MLP & Yi-34B-Chat \\ LLAVA-1.6-34B  & 34.8B & ViT-L14 & MLP & Nous-Hermes-2-Yi: 34B \\ Mattis-8B-siglip-lama3  & 8.5B & SigLIIP & MLP & Llama-3-8B \\ Ideficx2-8B  & 8.4B & SigLIIP & MLP & Mistral-7B \\   

Table 2: The architecture and size of different models.

#### 4.2.2 Disparity between Open-source and Closed-source Models

The results on II-Bench reveal that closed-source models generally perform better, with open-source models exhibiting a larger variance. However, some open-source models show excellent performance. The highest scores for open-source and closed-source models are LLaVA-1.6-34B (73.8%) and Qwen-VL-MAX (74.8%), respectively. Top open-source models like CogVLM2-Llama3-Chat-19B, MiniCPM-Llama3-2.5, Yi-VL-34B-Chat, Idefics2-8B, and InternVL-Chat-1.5 outperform the closed-source model GPT-4V's 65.9% accuracy but fall short of GPT-4o's 72.6%.

According to our analysis, the image implication understanding not only tests the model's image understanding ability but also tests the model's multi-hop reasoning ability. From the image understanding perspective, top open-source MLLMs perform closely to GPT-4V on various OCR-related benchmarks[41; 47; 53] and general multimodal benchmarks[69; 67; 40; 32; 31]. In terms of logical reasoning, multi-hop reasoning ability is crucial, and LLMs used in MLLMs like Llama3-Chat-8B, InternLM2-Chat-20B, and Yi-34B-Chat exhibit strong performance in reasoning and mathematics benchmarks[58; 68; 22; 39; 11]. Conversely, InstructBLIP-T5-XL, with weaker multi-hop reasoning ability from its language model Flan-T5-XL, shows the lowest accuracy at 47.3%.

#### 4.2.3 Model Performance across Different Domains and Emotions

In terms of domain performance, our results in Table 3 indicate that the models generally perform better in the Environment, Other, Life and Society domains, achieving higher accuracy. Conversely, the accuracy is lower in the Art and Psychology domains, which suggests that while the models generalize well in common domains, they struggle with the more abstract and logically demanding information found in Art and Psychology.

From an emotional perspective, the models tend to exhibit higher accuracy when the image implications convey positive emotions, while accuracy is the lowest for images with negative emotions. This discrepancy highlights that the models' preferences do not align with those of humans, as humans are significantly more sensitive to negative implications. Additionally, the results suggest that the models are overly biased towards positive responses, potentially reflecting a positive emotion bias in the training data.

    &  **Overall** \\ (1,399) \\  &  **Life** \\ (585) \\  &  **Art** \\ (85) \\  &  **Society** \\ (461) \\  &  **Psy.** \\ (152) \\  &  **Env.** \\ (51) \\  &  **Others** \\ (65) \\  &  **Positive** \\ (196) \\  &  **Neutral** \\ (789) \\  & 
 **Negative** \\ (414) \\  \\   \\  InstructBLIP-T5-XL & 47.3 & 45.6 & 48.2 & 48.8 & 44.7 & 52.9 & 50.8 & 46.9 & 48.3 & 45.4 \\ BLIP-2 FLAN-T5-XL & 52.8 & 53.0 & 58.8 & 52.5 & 42.8 & 64.7 & 58.5 & 56.1 & 52.9 & 51.0 \\ mPLUGw-OWL2 & 53.2 & 54.0 & 56.5 & 50.5 & 52.0 & 60.8 & 56.9 & 55.6 & 52.6 & 53.1 \\ Qwen-VL-Chat & 53.4 & 53.2 & 49.4 & 52.1 & 50.0 & 60.8 & 72.3 & 56.1 & 52.6 & 53.6 \\ InstructBLIP-T5-XL & 56.7 & 56.2 & 58.8 & 56.4 & 45.7 & 64.6 & 63.3 & 56.1 & 54.6 \\ Mantis-8B-siglip-Llama3 & 57.5 & 56.8 & 61.2 & 57.5 & 53.9 & 64.7 & 61.5 & 59.2 & 58.0 & 55.6 \\ BLIP-2 FLAN-T5-XL & 57.8 & 57.1 & 63.5 & 57.0 & 53.3 & 66.7 & 66.2 & 67.9 & 57.2 & 54.3 \\ DeepSee-VL-Chat-7B & 60.3 & 59.0 & 58.8 & 58.4 & 61.8 & 68.6 & 76.9 & 65.8 & 60.1 & 58.0 \\ Yi-VL-6B-Chat & 61.3 & 60.9 & 63.5 & 60.7 & 56.6 & 66.7 & 72.3 & 61.7 & 61.7 & 60.1 \\ InternVL-XComposer-VL & 62.1 & 61.7 & 62.4 & 62.3 & 58.6 & 70.6 & 66.2 & 65.8 & 63.0 & 58.7 \\ InternVL-Chat-1.5 & 66.3 & 63.6 & 65.9 & 68.5 & 65.8 & 64.7 & 76.9 & 73.5 & 65.4 & 64.5 \\ Idefics2-8B & 67.7 & 67.2 & **74.1** & 67.7 & 62.5 & 74.5 & 70.8 & 68.9 & 67.0 & 68.4 \\ Yi-VL-34B-Chat & 67.9 & 67.5 & 70.6 & 67.7 & 63.8 & 70.6 & 76.9 & 74.0 & 68.2 & 64.5 \\ MiniCPM-Llama3-2.5 & 69.4 & 68.4 & 71.8 & 69.4 & 64.5 & **80.4** & 78.5 & 75.0 & 69.3 & 66.9 \\ CoVLM2-Llama3-Chat & 70.3 & 68.9 & 68.2 & 70.9 & 67.8 & 72.5 & **86.2** & 69.9 & 71.1 & 69.1 \\ LLaVA-1.6-34B & **73.8** & **73.8** & 71.8 & **73.3** & **71.1** & 78.4 & 81.5 & **79.1** & **72.9** & **72.9** \\   \\  GPT-4V & 65.9 & 65.0 & 69.4 & 65.3 & 59.9 & 76.5 & 80.0 & 69.4 & 66.0 & 64.0 \\ GPT-4o & 72.6 & 72.5 & 72.9 & 73.3 & 68.4 & 76.5 & 75.4 & 78.6 & 71.2 & 72.5 \\ Gemini-1.5 Pro & 73.9 & 73.7 & **74.1** & 74.4 & 63.2 & **80.4** & 83.1 & **80.1** & 70.8 & **75.4** \\ Qwen-VL-MAX & **74.8** & **74.7** & 71.8 & **74.6** & **73.0** & 76.5 & **84.6** & **80.1** & **74.5** & 72.9 \\   \\  Human\_avg & 90.3 & 90.0 & 88.2 & 91.4 & 86.6 & 96.1 & 92.3 & 84.7 & 89.1 & 92.2 \\ Human\_best & **98.2** & **97.9** & **98.8** & **98.3** & **97.4** & **100.0** & **100.0** & **98.0** & **98.0** & **98.8** \\   

Table 3: Overall results of different MLLMs and humans on different domains and emotions. The best-performing model in each category is **in-bold**, and the second best is underlined.

#### 4.2.4 Analysis on different prompt skills

We present a comprehensive analysis of prompt skills, with detailed results in Table 4.

Analysis of Chain-of-Thought (CoT).The Chain-of-Thought (CoT) prompting skill is evaluated to determine its impact on model performance in Table 4. The results indicate that CoT has no significant effect on improving accuracy. In some cases, particularly with smaller open-source models, the accuracy even decline when CoT is used. For example, CogVLM2-Llama3-Chat-19B scores 70.3% without CoT and drops to 69.3% with CoT, InternVL-Chat-1.5 scores 66.3% and 63.3% as the same.

One primary reason for the observed decline in performance with CoT prompting is that many models fail to adhere to the required format. Specifically, CoT prompts require models to output both an analysis and a final answer in a fixed format. However, many models only provide the analysis without concluding with the final answer. We manually check the outputs and find that models either fail to explicitly generate the answer option after the analysis (instead of generating the content of the answer) or select multiple options, which reflect the decline in instruction following ability, leading to the failure of regex matching. An obvious example is BLIP-2 FLAN-T5-XXL, where using the CoT prompt results 15.8% increase in responses that fail to match our regex compared to the direct answer prompt. This issue is highlighted in Appendix E, where we present statistics on model outputs. The lack of a final answer in CoT responses leads to extraction failures, which negatively impacts performance.

Another contributing factor is that CoT prompting does not universally enhance performance across all types of tasks. In evaluations such as MMLU , C-Eval , where the primary focus is not on logical reasoning or mathematical problem-solving, CoT prompting often does not lead to better results. The same phenomenon is also observed in our experiments, which contrast with tasks that inherently benefit from step-by-step reasoning where CoT can be more effective. These findings align with other benchmarks [69; 34; 21].

Analysis of Different Types and Domains.To evaluate the impact of different label information on model accuracy, we conduct an ablation study by providing corresponding label information (Emotion, Domain, Rhetoric) for the images in the prompt. The results in Table 4 indicate that

  
**Models** & **None** & **CoT** & **Domain** & **Emotion** & **Rhetoric** \\   \\  InstructRLIP-T5-XL & 47.3 & 30.0 & 47.8 & 49.8 & 47.6 \\ BLIP-2 FLAN-T5-XL & 52.8 & 42.0 & 51.4 & 51.8 & 51.5 \\ mPLUGw-OWL2 & 53.2 & 54.2 & 54.5 & 55.0 & 55.7 \\ Qwen-VL-Chat & 53.4 & 51.6 & 54.9 & 57.0 & 54.0 \\ InstructBLIP-T5-XXL & 56.7 & 50.8 & 56.7 & 58.7 & 56.0 \\ Mantis-8B-sighip-Llama3 & 57.5 & 56.7 & 57.1 & 57.0 & 58.0 \\ BLIP-2 FLAN-T5-XXL & 57.8 & 42.5 & 57.5 & 58.4 & 57.3 \\ DeepSee-VL-Chat-7B & 60.3 & 59.2 & 60.4 & 63.3 & 59.8 \\ Yi-VL-6B-Chat & 61.3 & 60.8 & 60.8 & 62.8 & 60.4 \\ IntemVL-XCmopser2-VL & 62.1 & 60.7 & 60.9 & 61.5 & 61.6 \\ InternVL-Chat-1.5 & 66.3 & 63.3 & 66.6 & 67.4 & 65.6 \\ Idefics2-8B & 67.7 & 67.7 & 67.0 & 68.6 & 66.6 \\ Yi-VL-34B-Chat & 67.9 & 67.6 & 67.7 & 70.1 & 67.6 \\ MiniCPM-Llama3-2.5 & 69.4 & 67.4 & 70.3 & 70.8 & 69.3 \\ CogVLM2-Llama3-Chat-19B & 70.3 & **69.3** & 69.1 & 71.7 & 69.3 \\ LLaVA-1.6-34B & **73.8** & 60.0 & **73.1** & **75.3** & **73.3** \\   \\  GPT-4V & 65.9 & 68.4 & 66.0 & 68.3 & 69.3 \\ GPT-4o & 72.6 & **75.7** & 72.6 & 74.2 & 71.3 \\ Gemini-1.5 Pro & 73.9 & 68.2 & 73.1 & 70.5 & 71.3 \\ Qwen-VL-MAX & **74.8** & 74.1 & **74.1** & **75.5** & **73.6** \\   

Table 4: Overall results of different prompts on II-Bench. The label(_Emotion, Domain, Rhetoric_) means providing corresponding information for the images in the prompt. The best-performing model in each category is **in-bold**, and the second best is underlined.

Emotion labels significantly enhance model accuracy, followed closely by Domain and Rhetoric labels, which exhibit similar effectiveness.

This outcome is consistent with the human perspective of image implication comprehension. Emotion labels likely provide more intuitive and salient cues that align closely with human interpretative processes, thereby facilitating better model performance. In contrast, Domain and Rhetoric labels, while still beneficial, are not as immediately intuitive or universally applicable, thus resulting in slightly lower effectiveness in improving model accuracy. At the same time, from the perspective of model training, the model has a normal understanding of emotion, unlike the specific nouns we define ourselves in the Rhetoric and Domain labels. The model does not see many descriptions of such specific nouns during pre-training, which does not help improve accuracy.

Analysis of Few-shot Examples.The results in Table 5 demonstrate that few-shot examples do not enhance the accuracy of the models. Specifically, the performance tends to drop as more examples are provided. This can be attributed to the models' inferior multi-image capabilities compared to their single-image capabilities, leading to a decline in accuracy with an increasing number of shots. Additionally, as the number of shots increases, the input length becomes longer, and the model's long text ability is insufficient, resulting in poor long context performance. An example is Qwen-VL-Max, where inputs exceeding 6,000 tokens cause errors. Moreover, chat models generally exhibit good instruction following ability, reducing the necessity for few-shot examples.

### Error Analysis

In order to perform a comprehensive error analysis of GPT-4V's performance on II-Bench, we randomly select 100 erroneous samples from each domain, in proportion to their representation in the dataset. These samples are meticulously analyzed by expert annotators. As illustrated in Figure 4, GPT-4V's errors can be categorized into the following types: Metaphorical Misunderstanding, Detail Misunderstanding, Surface-Level Interpretation, Reasoning Error, Reject to Answer and Answer Extraction Error. This error analysis is crucial for gaining deeper insights into the capabilities of MLLMs and identifying the current limitations in image comprehension tasks. Understanding these shortcomings can guide researchers in developing and training more robust and performant models in the future. A selection of 77 notable cases, along with detailed analyses, is included in Appendix G, providing further insights into the nature of these errors. **Reminder: although we filtered and sifted as much as possible, some of the negative cases in the appendix are offensive to certain groups of people.**

Metaphorical Misunderstanding (36%):Metaphorical Misunderstanding is a common error that GPT-4V makes when generating responses based on image comprehension. This indicates that the model has misunderstood the implications or symbolic meanings within the images. There are two main reasons for this. First, the model might grasp certain aspects of the image's meaning, but its overall understanding of the image's theme is incorrect, as exemplified by Fig.G28. Second, some implications and hidden meanings require specific knowledge to be understood, and the model's internal knowledge might not cover

Figure 4: GPT-4V error response distribution.

  
**Model** & **0-shot** & **1-shot** & **2-shot** & **3-shot** \\  Qwen-VL-Chat & 53.4 & 43.3 & 47.9 & 41.1 \\ Mantis-8B-siglip-Llama3 & 57.5 & 55.3 & 54.2 & 54.9 \\ GPT-4V & 65.9 & 65.5 & 67.7 & 67.1 \\ Idefics2-8B & 67.7 & 64.1 & 62.4 & 59.5 \\ Gemini-1.5 Pro & 73.9 & 73.2 & 73.8 & 74.1 \\ Qwen-VL-Max & 74.8 & 74.5 & 69.6 & 53.6* \\   

Table 5: Few-shot results of different models on the II-Bench. \(*\) means exceeds the context length.

these areas, leading to an incorrect interpretation of the image's deeper meaning.

**Detail Misunderstanding (21%):** Detail Misunderstanding is another common mistake made by GPT-4V. Understanding details is very important for models, as inaccuracies in understanding details can sometimes affect how the model interprets the meaning of images. For instance, in Fig.G11, GPT-4V has an error in understanding the details, resulting in an incorrect response.

**Other Errors:** The remaining errors are detail ignorance (11%), surface-level interpretation (15%), reasoning error (12%), reject to answer (4%), and answer extraction error (1%). The description of these errors can be found in Appendix F.

## 5 Conclusion

The development of II-Bench for assessing the capabilities of MLLMs represents a significant milestone in the journey towards achieving Expert AGI, marking a step into higher-order theory of mind in the exploration of the capabilities of MLLMs. The experimental results show that the current state-of-the-art MLLMs are good at understanding the surface content of image, but the gap between the understanding of image implication and humans is still huge. We found that including information about the emotional polarity of the image in the prompts usually improves the model score, suggesting that the model lacks sufficient emotional understanding of the image, which leads to misinterpretation of the implied meaning. Moreover, we found that humans would implicitly understand neutral and negative emotions much better than models. The vast majority of MLLMs perceive positive emotions better than neutral and negative emotions, and we think that the distribution of training data for MLLMs is more skewed toward positive emotions. We believe II-Bench will stimulate the community to build next generation multimodal foundation models towards expert AGI.

## Limitations

We acknowledge several limitations in our study. While II-Bench is comprehensive, the inclusion of subjective elements can lead to varying interpretations, potentially affecting result consistency. Additionally, our benchmark focuses on specific domains, covering only a portion of human knowledge. The evaluation metrics might not entirely reflect the sophisticated understanding and reasoning abilities of advanced AI systems. These limitations highlight the need for ongoing refinement and expansion of our benchmarks. In future work, we aim to develop and incorporate more stringent and objective test sets to enhance reliability and validity of our benchmark.

## Ethics Statement

In developing II-Bench, we strictly adhere to ethical guidelines and legal regulations, ensuring fairness, transparency, inclusivity and respect for all stakeholders. We stress the importance of safeguarding privacy and intellectual property rights, underscoring our commitment to responsible and lawful data management. We have taken steps to anonymize any personal data to protect privacy and and have made every effort to minimize harmful or biased content. However, we recognize that biases can inadvertently arise and some information may be potentially offensive. We are committed to continuous monitoring and improvement to mitigate such biases. Furthermore, we encourage users of our dataset to employ it responsibly and to consider the ethical implications of their work, particularly in applications that may impact individuals or communities.