# Energy-Based Conceptual Diffusion Model

Yi Qin\({}^{1}\), Xinyue Xu\({}^{1}\), Hao Wang\({}^{2}\)\({}^{}\), Xiaomeng Li\({}^{1}\)\({}^{}\)

\({}^{1}\)The Hong Kong University of Science and Technology, \({}^{2}\)Rutgers University, \({}^{}\)Equal advising {yqinar, xxucb, eexmli}@ust.hk, hwu488@cs.rutgers.edu

###### Abstract

Diffusion models have shown impressive sample generation capabilities across various domains. However, current methods are still lacking in human-understandable explanations and interpretable control: (1) they do not provide a probabilistic framework for systematic interpretation. For example, when tasked with generating an image of a "Nighthawk", they cannot quantify the probability of specific concepts (e.g., "black bill" and "brown crown" usually seen in Nighthawks) or verify whether the generated concepts align with the instruction. This limits explanations of the generative process; (2) they do not naturally support control mechanisms based on concept probabilities, such as correcting errors (e.g., correcting "black crown" to "brown crown" in a generated "Nighthawk" image) or performing imputations using these concepts, therefore falling short in interpretable editing capabilities. To address these limitations, we propose **Energy-based Conceptual Diffusion Models (ECDMs)**. ECDMs integrate diffusion models and Concept Bottleneck Models (CBMs) within the framework of Energy-Based Models to provide unified interpretations. Unlike conventional CBMs, which are typically discriminative, our approach extends CBMs to the generative process. ECDMs use a set of energy networks and pretrained diffusion models to define the joint energy estimation of the input instructions, concept vectors, and generated images. This unified framework enables concept-based generation, interpretation, debugging, intervention, and imputation through conditional probabilities derived from energy estimates. Our experiments on various real-world datasets demonstrate that ECDMs offer both strong generative performance and rich concept-based interpretability.

## 1 Introduction

Denoising diffusion probabilistic models are capable of generating high-quality images (Rombach et al., 2022; Bluethgen et al., 2024), videos (Brooks et al., 2024), and structured data (Ingraham et al., 2023) across various domains, such as artwork, medicine, and biology. However, existing diffusion models typically fall short in human-understandable explanations and interpretable control capabilities during the generation process. For instance, when the model is tasked with generating an image of a "Nighthawk", a practitioner may be interested in determining whether the model bases its generation on specific bird concepts (e.g., "black bill" and "brown crown" when generating a "Nighthawk" image). Additionally, the practitioner would want the capability to correct potential generation errors using these concepts (e.g., correcting "black crown" to "brown crown" in a generated "Nighthawk" image). Without these interpretation and correction capabilities, diffusion models - no matter how high-resolution their generated images are - can hardly be considered trustworthy or reliable by human standards.

Recent advances in interpretable diffusion models aim to address the problem by analyzing decomposed features (Du et al., 2021, 2023; Liu et al., 2022, 2023) or fine-tuning additional model components (Li et al., 2024; Wang et al., 2023; Lyu et al., 2024; Luo et al., 2024; Li et al., 2024; Kumari et al., 2023; Feng et al., 2022; Gandikota et al., 2023). However, these methods still suffer from the following key limitations:

1. **Systematic Interpretation:** They do not provide a probabilistic framework that facilitates systematic interpretation of the generation process. Consequently, it is still challenging to assess how the human-intended visual concepts are inherently represented and incorporated in the textto-image diffusion model's generation process, and whether the interpreted concepts from the generation process align with the intended concepts from the instruction.
2. **Concept-Based Generation:** They can only control the generation with a limited number of concepts (e.g., interpolating between "hairy" and "hairless" or composing a small number of visual components). As a result, they often struggle to generate images based on a broader set of concepts. This restriction significantly narrows the concept-based control space available in diffusion models, limiting their versatility in more complex generation tasks.
3. **Intervention:** Current methods often fail to correct generation errors based on concept-based probabilistic explanations (e.g., correcting "black crown" to "brown crown"). Furthermore, they cannot effectively intervene in the generation process by leveraging the interactions among class-level instructions, concept-based explanations, and sampling intermediates.

To provide systematic concept-based explanations and control for diffusion models, we propose **Energy-based Conceptual Diffusion Models** (ECDMs). ECDMs unify diffusion models and Concept Bottleneck Models (CBMs) under the Energy-Based Models framework. In contrast to conventional _discriminative_ CBMs ("image" \(\) "concepts" \(\) "class label"), our ECDM enables concept-level interpretations and control to _generative_ tasks ("class label" \(\) "concepts" \(\) "image").

Specifically, ECDMs use a set of networks and the pretrained diffusion model to quantify the energy between the class-level instruction \(\), concept-level explanation \(\), and the generated image \(\). Within this unified framework, one can

1. generate the image \(\) with corresponding concept vectors \(\) as **interpretations**, i.e., \(p(,|)\).
2. given an input instruction \(\) and the generated image \(\), **debug** what concepts are generated incorrectly by comparing the what concepts are generated (i.e., \(p(|)\)) and what concepts should have been generated (i.e., \(p(|)\)),
3. given an input instruction \(\), **intervene** the generation process of image \(\) by replacing incorrect concepts with correct ones \([_{k}]_{k=1}^{K-n}\), i.e., \(p([_{k}]_{k=K-n+1}^{K},|,[_{k}]_{k=1}^{K-n})\), and
4. given an input instruction \(\) and part of a generated image \(()\), **impute** the remainder of the image \(()\) with the concept explanations, i.e., \(p((),|(),)\).

Importantly, thanks to the unified energy-based framework, these conditional probabilities can be naturally computed through composition of different energy functions. Our contributions are:

* We propose Energy-Based Conceptual Diffusion Models (ECDMs), a framework that unifies the concept-based generation, conditional interpretation, concept debugging, intervention, and imputation under the joint energy-based formulation.
* With ECDM's unified framework, we develop a set of algorithms to compute different conditional probabilities by composing corresponding energy functions.
* Empirical results on real-world datasets demonstrate ECDM's state-of-the-art performance in terms of image generation, imputation, and their conceptual interpretations.

## 2 Energy-Based Conceptual Diffusion Models

In this section, we introduce the notation, problem settings, and then our proposed ECDM in detail.

**Notation.** We consider a class-level text-to-image generation setting, with \(M\) classes and \(K\) concepts. Specifically, given a class-level label \(\) (e.g., "Nighthawk"), a diffusion model will generate a corresponding image \(\), with the generation process potentially interpreted by a set of concepts, represented by a binary vector \(=\{0,1\}^{K}\) (e.g., "black bill" and "brown crown"). We denote the \(k\)-th dimension of the concept vector \(\) as \(c_{k}\). We denote the pretrained latent diffusion model as \(_{}(,_{t},t)\), which is parameterized by \(\); it takes the noisy latent \(_{t}\) at timestep \(t\) and the condition - as the input to predict the denoised latent \(_{t-1}\). We use a pretrained text encoder \(F\) to extract (1) the class embedding \(\) from the given instruction (\(=F()\)) and (2) the concept embedding \(\) from concepts (\(=F()\)). Finally, the structured energy network \(E_{}(,)\) parameterized by \(\), maps \((,)\) or \((,)\) to real-valued scalar energy values.

**Problem Settings.** For each data point, we consider the following problem settings:

1. **Concept-Based Generation** (\(p(,|)\)).: This is the main task for a diffusion model. Given the instruction \(\), the goal is to infer the concepts \(\) and generate the image \(\). In ECDM, we decompose \(p(,|)\) into concept inference \(p(|)\) and image generation \(p(|)\).

2. **Interpretation (\((c|)\)).** Interpret what concepts \(\) are used when generating the image \(\).
3. **Debugging (\(p(c|)p(|)\)).** Given the input \(\) and the generated image \(\), debug what concepts are generated _incorrectly_ by comparing the what concepts are generated (i.e., \(p(c|)\)) and what concepts should be generated (i.e., \(p(c|)\)).
4. **Intervention/Correction \(p([_{k}]_{k=K-n+1}^{K},|,[_{k}]_{k=1}^{K-n})\)**. Given the instruction \(\) and the _corrected_ concepts \([_{k}]_{k=1}^{K-n}\), infer other concepts \([_{k}]_{k=K-n+1}^{K}\) and generate the image \(\).
5. **Imputation \(p((),|(),)\).** Given the instruction \(\) and a partially masked image \(()\), where \(()\) is a masking function and \(=()()\), impute the masked pixels \(()\) and generate the associated concept interpretations \(\).

### Energy-Based Conceptual Diffusion Models

**Overview.** Our ECDM consists of two energy networks parameterized by \(\): (1) a concept energy network \(E_{}^{concept}(,)\), the gradient of which models the score of the concept-conditional data distribution \(p(|)\) and has its minimum at the highest conditional log-likelihood and (2) a mapping energy network \(E_{}^{map}(,)\), which maps the class-level instruction \(\) to the corresponding concept vector \(\) by measuring the compatibility between \(\) and \(\). Both energy networks model the data distribution using "unnormalized" probability densities. Our ECDM is trained by minimizing the following loss function:

\[_{total}(,,)=_{concept}(, )+_{m}_{map}(,),\] (1)

where two terms \(_{concept}\) and \(_{map}\) denote the loss functions for the concept and mapping energy networks \(E_{}^{concept}(,)\) and \(E_{}^{map}(,)\), respectively. \(_{m}\) is a balancing hyperparameter. Fig. 1 shows the overview of our ECDM. Below we provide rationale and details of the loss terms in detail.

**Generative Concept Energy Network \(E_{}^{concept}(,)\).** Our concept energy network captures the compatibility between the concepts \(\) and the generated image \(\) while enabling generative sam

Figure 1: Overview of our ECDM. **(a) Training:** During training, the model learns the positive concept embedding \(_{k}^{(+)}\), the negative concept embedding \(_{k}^{(-)}\), and two sets of energy networks by optimizing Eqn. 1. **(b) Generation:** During generation, ECDMs first infer an optimal concept vector \(\), which is the most compatible with the instruction \(\), by minimizing the mapping energy, then use the inferred concept vector as the condition to minimize the concept energy by performing diffusion sampling. **(c) Interpretation:** During interpretation, ECDMs first inverse a pivotal trajectory using DDIM inversion given the generated image and corresponding instruction. Next, ECDMs update the concept probability \(}\) by minimizing the energy matching target (Eqn. 12).

pling from the concept-conditional data distribution \(p(|)\). Notably, the gradient of the energy \(E_{}^{concept}(,)\) is proportional to the conditional data distribution \(p_{}(|)\)'s score, which is the diffusion model's denoising step \(_{}(,,t)\). Formally we have:

\[_{}E_{}^{concept}(,)_{}  p_{}(|)=_{}(,,t)\] (2)

This enables the implicit modeling of this energy network using diffusion models. In practice, our concept energy network consists of an concept input network \(D_{c}()\) and a pretrained diffusion network \(_{}(,,t)\), where we replace \(\) in \(_{}(,,t)\) with \(D_{c}()\). Specifically,

\[E_{}^{concept}(,)_{, (,),t}[\|-_{}(D_{c}(), _{t},t)\|_{2}^{2}],\] (3)

where the concept input network \(D_{c}()\) works as follows: Given a set of \(K\) concepts \(\), each concept \(k\{1,,K\}\) is associated with a positive embedding \(_{k}^{(+)}\) and a negative embedding \(_{k}^{(-)}\) projected by the text feature extractor \(F\). The final concept embedding \(_{k}\) is a combination of the positive and negative embedding weighted by the concept probability \(c_{k}\), defined as \(_{k}=c_{k}_{k}^{(+)}+(1-c_{k})_{k}^{(-)}\). Finally, another network \(D_{v}()\) projects the combined concept embedding \([_{k}]_{k=1}^{K}\) to the final input embedding, i.e., \(D_{c}()=D_{v}()\). Note that during training, we form the \(_{k}\) as \(_{k}^{(+)}\) if \(c_{k}=1\), and \(_{k}^{(-)}\) if \(c_{k}=0\).

Since \(E_{}^{concept}(,)\) can be seen as the (approximate) variational upper bound for the negative log-likelihood \(- p_{}(|)\) (more details in the Appendix D.2), it can be used directly as the loss function \(_{concept}(,)\) during training. We then have

\[_{concept}(,) E_{}^{ concept}(,)_{,(, ),t}[\|-_{}(D_{c}(),_{t},t)\|_{2}^{2}].\] (4)

After training, generating the image \(\) given the concept vector \(\) is then equivalent to solving \(=_{}E_{}^{concept}(,)\) using Eqn. 2.

**Mapping Energy Network \(E_{}^{map}(,)\).** The mapping energy network connects the class-level instruction \(\) and the concept vector \(\) by measuring the compatibility between \(\) and \(\). We input the class embedding \(\) corresponding to \(\) and the fused concept embedding \(=D_{c}()\) into a neural network to compute the mapping energy \(E_{}^{map}(,)\). Formally, we have:

\[E_{}^{map}(,)=D_{uw}(,),\] (5)

where \(D_{uw}(,)\) is a trainable neural network. The network will output an energy estimate for each pair of \((,)\). Following (Xu et al., 2024), the training loss function for each instruction-concept pair \((,)\) is formulated as:

\[_{map}(,)=E_{}^{map}(,)+ _{m=1,e^{}}^{M}e^{-E_{}^{ map}(^{},_{m})},\] (6)

where \(^{}\) enumerates all concept combinations in the concept space \(\). We use negative sampling to enumerate a subset of the possible combinations for computational efficiency.

### Concept-Based Joint Generation

Fig. 1(b) demonstrates the generation pipeline using our ECDM. To generate an image \(\) based on concepts \(\) given class-level instructions \(\), we minimize the following joint energy:

\[E_{}^{joint}(,,) E_{}^{concept} (,)+_{m}E_{}^{map}(,).\] (7)

Specifically, concept-based generation aims to search for

\[_{},}}p(},} |)=_{},}}}^{ joint}(},},)}}{_{,}e^{-E_{}^{ joint}(,,)}}=_{},}}E_{}^{ joint}(},},)\]After obtaining the optimal concept prediction \(}\) which is the most compatible one with the instruction \(\), we use \(}\) as the condition to minimize the joint energy model \(E_{}^{joint}(,,)\) for generation. The minimization of the joint energy model is achieved by gradient descent-like sampling process from the diffusion model. Formally, we have:

\[_{t-1} =_{t}-_{}E_{}^{joint}(, {y},)_{=_{t},=}}+,\] (9) \[=_{t}-_{}E_{}^{concept}(, )_{=_{t},=}}+, (,_{t}^{2}),t=T,,1,\] (10)

where \(_{}E_{}^{concept}(,)\) is given by Eqn. 2. (See Appendix D.2 for more details.) We then alternate between Eqn. 8 and Eqn. 10 until convergence. Empirically, we find that one iteration usually produces sufficiently good results.

### Interpretation and Debugging via Concept Inversion

**Interpretation \(p(|)\).** Our ECDM can interpret a given external diffusion model \(_{}^{interpret}(,,t)\) using the conditional probability \(p(|)\), which estimates what concepts \(\) are used by \(_{}^{interpret}(,,t)\) to generate the image \(\) given the input instruction \(\). Specifically, we derive the concept probability by matching the energy landscape between our ECDM's concept energy network \(E_{}^{concept}(,)\) and the external energy model \(E_{}^{interpret}(,)\) associated with \(_{}^{interpret}(,,t)\) (similar to Eqn. 2). Fig. 1(c) shows an overview of this process consisting of two steps: Pivotal Inversion and Energy Matching Inference.

**Pivotal Inversion.** Given an image \(\) and the corresponding instruction \(\), pivotal inversion aims to replay the sampling trajectory of the external (interpreted) energy model \(E_{}^{interpret}(,)\), providing pivotal representations at each sample step for alignment. We use the reversed DDIM (more details in Eqn. 39 of the Appendix) to produce a \(T\)-step deterministic trajectory between image \(_{0}\) and the Gaussian noise vector \(_{T}\). In each timestep \(t\), the trajectory can be represented as:

\[_{}E_{}^{interpret}(,)_{=_ {t}}=_{}^{interpret}(,_{t},t)\] (11)

**Energy Matching Inference.** To infer the concept vector \(\) given the pivotal representation, we freeze the concept energy network \(E_{}^{concept}(,)\) to search for the optimal concept vector \(}\) globally at each timestep \(t\) minimizing Eqn. 12 as follows:

\[\|_{}E_{}^{concept}(,)-_{}E_{}^{interpret}(,)\|_{2}^{2},\] (12)

Proposition 2.1 below shows that minimizing the Eqn. 12 is equivalent to matching the distribution between \(p(|)\) and \(p(|)\), thereby effectively finding the optimal concept vector \(}\) to interpret the external diffusion model's generation.

**Proposition 2.1** (**Conditional Concept Probability By Energy Matching)**.: _Given the instruction \(\) and the image \(\), minimizing Eqn. 12 is equivalent to minimizing the score's disparity between two conditional probabilities \(p(|)\) and \(p(|)\):_

\[\|_{}E_{}^{concept}(,)-_{}E_ {}^{interpret}(,)\|_{2}^{2}=\|_{} p (|)-_{} p(|)\|_{2}^{2}\] (13)

Transforming Proposition 2.1 into timestep-aware version, we can obtain the final optimal concept vector \(}\) via:

\[_{}}\|_{}E_{}^{concept}(_{t},})-_{}E_{}^{interpret}(_{t},)\|_{2}^{2}\] (14)

**Debugging:**\(p(|)}{=}}p(|)\). Debugging involves the comparison between what concepts the model has been generated (\(p(|)\)) and what concepts the model should have been generated (\(p(|)\)). \(p(|)\) can be obtained via the energy matching process (Proposition 2.1), while \(p(|)\) can be inferred by minimizing the mapping energy (Eqn. 8). By inspecting the disparity of these two conditional probabilities, users can pinpoint the potential cause of the generation error, laying the foundation for subsequent intervention and imputation to correct the discovered error.

### Concept-Based Corrective Intervention and Imputation

**From Debugging to Intervention/Correction.** Based on the debugging results from Sec. 2.3, we can further perform concept intervention to correct the potential generation error. Specifically, if the debugging process in Sec. 2.3 finds that concepts \([_{k}]_{k=1}^{K-n}\) are incorrect, i.e., \(p([_{k}]_{k=1}^{K-n}|) p([_{k}]_{k=1}^{K-n}|)\), one can then intervene on the image generation process by correcting these concepts.

**Overview.** Specifically, ECDM's concept-based intervention consists of three steps: (1) correct concepts \([_{k}]_{k=1}^{K-n}\) according to \(p([_{k}]_{k=1}^{K-n}|)\), (2) given the corrected concepts, infer all remaining concepts via \(p([_{k}]_{k=K-n+1}^{K}|,[_{k}]_{k=1}^{K-n})\), and (3) use all concepts to generate the image, i.e, computing \(p(x|[_{k}]_{k=K-n+1}^{K},,[_{k}]_{k=1}^{K-n})\) via the concept energy network in Eqn. 3.

**Step 1: Correcting Concepts (\(p([_{k}]_{k=1}^{K-n}|)\)).** Correcting concepts is straightforward. After computing the optimal \(}\) by maximizing \(p([_{k}]_{k=1}^{K-n}|)\) (Eqn. 8), one can simply set \(\) to \(}\) in the ECDM.

**Step 2: Inferring Remaining Concepts.** Inference of the remaining concepts is facilitated by our mapping energy network and can be done using Eqn. 15 in Proposition 2.2 below.

**Proposition 2.2** (Class-Specific Conditional Probability among Concepts).: _Given partially concepts \([_{k}]_{k=1}^{K-n}\) and class-level instruction \(\), infer the remaining concepts \([_{k}]_{k=K-n+1}^{K}\) is:_

\[p([_{k}]_{k=K-n+1}^{K}|,[_{k}]_{k=1}^{K-n})=}^{}(,)}}{_{^{} }e^{-E_{}^{}(^{},)}} p ()}{_{[_{j}]_{j=K-n+1}^{K}}}^{ {map}}(,)}}{_{^{}}e^{-E_{}^{}(^{},)}} p()}\] (15)

**Step 3: Generating the Corrected Image.** Given all corrected concepts \(\) (\([_{k}]_{k=K-n+1}^{K}\) and \([_{k}]_{k=1}^{K-n}\)) combined), one then generates the corrected image \(\) (i.e., \(p(|,)\)) using using Eqn. 10.

**Interpretable Concept-Based Imputation.** Additionally, ECDM can perform interpretable concept-based imputation based on the joint energy \(E_{}^{joint}(,,)\). We provide more details in Appendix B.

## 3 Experiments

### Experiment Setup

**Datasets.** We use three real-world datasets to to evaluate different methods.

* **Animals with Attributes 2 (AWA2)** (Xian et al., 2018) is an animal image dataset containing \(37{,}322\) images, \(85\) concepts, and \(50\) animal classes. We select \(45\) photo-visible concepts for experiments, following ProbCBM (Kim et al., 2023). We only include animal classes that contain more than \(300\) images, leading to a total number of \(24\) classes in our final dataset.
* **Caltech-UCSD Birds-200-2011 (CUB)**(Wah et al., 2011) is a fine-grained bird image dataset with \(11{,}788\) images, \(312\) annotated attributes, and \(200\) classes. Following previous works (Koh et al., 2020; Kim et al., 2023; Zarlenga et al., 2022), we select \(112\) attributes as the \(112\) concepts.
* **CelebA-HQ**(Karras, 2017) is a high-quality face image dataset with \(30{,}000\) images, \(40\) binary attributes and \(10{,}177\) identities. Following CEM (Zarlenga et al., 2022), we select \(8\) most frequent attributes as the \(8\) concepts and use \(6\) combination of the selected attributes as the \(6\) classes in our setting.

**Baseline and Implementation Details.** We compare the generation results of ECDM with the direct class-level instruction generation of Stable Diffusion 2.1 (**SD-2.1**) (Rombach et al., 2022) and **PixArt-\(\)**(Chen et al., 2023). We further include the generation result from Text Inversion (**TI**) (Gal et al., 2022), which is the most related finetuning-based method. We build our model upon the pretrained Stable Diffusion 2.1 (Rombach et al., 2022) with parameters frozen for all experiments. We use the AdamW optimizer during the training and inference process.

**Evaluation Metrics.** We employ three specific metrics to evaluate different methods:* **Frechet Inception Distance (FID).** We measure the FID (Heusel et al., 2017) between the synthetic and real images to evaluate the generated image quality. Lower FID indicates higher image generation quality.
* **Class Accuracy.** We train three class-level ResNet101 classification models (He et al., 2016) on the corresponding datasets, and use the trained model to measure the class accuracy of generated images. Higher class accuracy suggests that the generated images more effectively capture the defining characteristics of a class.
* **Concept Accuracy.** We calculate the concept accuracy between the ground-truth concepts and the predicted concepts from pretrained CEMs (Zarlenga et al., 2022). Higher concept accuracy indicates that the generated image covers more desired visual concepts.

See more details on dataset construction, implementations, and evaluation in Appendix E and F.

### Results

**Concept-Based Joint Generation.** Fig. 2 shows the generation results of our ECDM on different datasets. Visually, the outputs of our model are better aligned with the characteristics of real-world subjects and exhibit more refined details compared to both standard text-to-image diffusion models and their fine-tuned variants. The visual concepts included in the reference (ground-truth) image's (marked in green) are comprehensively depicted in our ECDM's generated images. For instance,

   ModelData &  &  &  \\  Metric & FID & Class & Concept & FID & Class & Concept & FID & Class & Concept \\  & Accuracy & Accuracy & FID & Accuracy & Accuracy & Accuracy & FID & Accuracy & Accuracy \\  SD-2.1 & 29.55 & 0.5033 & 0.9222 & 37.79 & 0.8935 & 0.9850 & 53.47 & 0.4881 & 0.8079 \\ PixArt-\(\) & 46.85 & 0.1208 & 0.8231 & 59.71 & 0.9008 & 0.9764 & - & - & - \\ TI & 23.36 & 0.6397 & 0.9496 & 29.63 & 0.9142 & **0.9863** & 53.47 & 0.4881 & 0.8079 \\
**ECDM (Ours)** & **22.94** & **0.6492** & **0.9561** & **28.91** & **0.9200** & 0.9801 & **52.89** & **0.5017** & **0.8182** \\   

Table 1: The generation quality evaluation results on different datasets. Textual Inversion is not readily available in PixArt-\(\) model, therefore unavailable for the experiment. The Textual Inversion results of CelebA-HQ is based on SD-2.1, hence identical results, see Appendix E for further explanation.

Figure 2: Visualizing generated outputs on CUB (upper) and AWA2 (lower) datasets. Words in green/red indicate a correctly/wrongly generated visual concept. Images are generated under the same random seed and instruction. Our ECDM generates more fine-grained and correct details compared to other methods (e.g., “white breast color” and “bill length alike head” in Row 1).

Table 1 shows the quantitative results. Our ECDM consistently achieves a lower FID compared to the baselines, indicating that ECDM produces images with higher fidelity and quality. Notably, the class and concept accuracy of our model's generated images in the majority of datasets outperforms all other methods. This suggests that our model incorporates more visible concepts during generation, providing richer class-discriminative characteristics in the resulting images.

**Interpretation via Concept Inversion.** Fig. 3 shows our ECDM's probabilistic interpretations of the generation process based on visual concepts. It shows that ECDM's inferred concept probabilities (the row "Was Generated \(p(|)\)) correctly reflect the concepts generated by the model. Additionally, the concept probabilities derived from the mapping energy network (the row "Should Generate \(p(|)\)") correctly reflect the concepts that should be generated for the specific class (e.g., "Great Crested Flycatcher").

**Debugging by Comparing \(p(|)\) and \(p(|)\).** By comparing what concepts were generated (\(p(|)\)) and what concepts should be generated for class \(\) (\(p(|)\)), we can identify the cause of potential generation errors. For example, an external pretrained diffusion model generates an "Olive Sided Flycatcher" with "brown wings", although it should be "grey wings". Our ECDM assigns the concept "brown wing color" a high prediction probability (\(0.8961\)), suggesting it was a key factor in the generation. Our ECDM's further indicates that "brown wing color" should _not_ be generated, with the "Should Generate" probability \(p(|)=0.0021\). In this way, users can identify incorrectly predicted concept probabilities using our method, gaining insight into the model's generative tendencies and establishing a foundation for further interpretive interventions and corrections.

**Concept-Based Intervention.** Fig. 5 shows the intervention results based on interpreted concept probabilities. After user intervention, ECDM can effectively correct generation errors related to visual concepts. For example, the interpretation process revealed that the "Black Billed Cuckoo" should not have been generated with the concepts "grey crown color" and "grey upper color", but rather with "white breast color" and "perching shape." After the user intervened by providing the correct concept set, the model successfully corrected the generation based on these proper concepts.

## 4 Conclusion and Limitations

In this paper, we extend the concept bottleneck model into the generative process, identifying the need for a joint modeling of conceptual generation, interpretation, debugging, intervention, and imputation. We proposed Energy-Based Conceptual Diffusion Model (ECDM), a framework that unifies generation, conditional interpretation and debugging, sampling intervention and imputation under the joint energy-based formulation. A set of conditional probabilities is derived through the combination of the energy functions. Our work also has several limitations, including the need for more precise regional control in concept-based editing and the requirement for concept ground truth.

Figure 3: Interpretation results on the CUB dataset. The images \(\) are generated from an _external pretrained diffusion model_ (i.e., vanilla SD-2.1). Numbers in red indicate potential generation errors compared with real concepts. Our ECDM can correctly interpret what concepts were generated (\(p(|)\)) and what concepts should be generated for instruction \(\) (\(p(|)\)).