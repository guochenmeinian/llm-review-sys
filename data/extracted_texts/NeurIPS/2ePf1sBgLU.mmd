# Learning Invariant Representations with a Nonparametric Nadaraya-Watson Head

Alan Q. Wang

Cornell University

&Minh Nguyen

Cornell University

&Mert R. Sabuncu

Cornell University

###### Abstract

Machine learning models will often fail when deployed in an environment with a data distribution that is different than the training distribution. When multiple environments are available during training, many methods exist that learn representations which are invariant across the different distributions, with the hope that these representations will be transportable to unseen domains. In this work, we present a nonparametric strategy for learning invariant representations based on the recently-proposed Nadaraya-Watson (NW) head. The NW head makes a prediction by comparing the learned representations of the query to the elements of a support set that consists of labeled data. We demonstrate that by manipulating the support set, one can encode different causal assumptions. In particular, restricting the support set to a single environment encourages the model to learn invariant features that do not depend on the environment. We present a causally-motivated setup for our modeling and training strategy and validate on three challenging real-world domain generalization tasks in computer vision.

## 1 Introduction

Machine learning models often fail when there is significant distribution shift. The goal of domain generalization is to be able to perform well with new distributions . In this work, we are interested in settings where multiple domains/environments are available during training and we have access to environment indicators. A popular way to tackle domain generalization in this setting is to learn representations that are invariant across environments . The hope is that such representations will work well in, or are transportable to, unseen environments. This invariance is often encoded via constraints on a learned predictor which aligns its behavior across environments; often, these conditions are derived using causal reasoning and/or by making assumptions about the data-generating process .

In a parametric setting, almost all existing methods enforce these constraints by training a single model and adding a regularizer on top of a standard predictive loss . Most notably, invariant risk minimization (IRM) enforces the representations to be such that the optimal classifier on top of those representations is the same across all environments. Other examples include enforcing the layer activations of the predictor to be aligned across environments , enforcing the predictor to be calibrated across environments , and enforcing gradients of the predictor to be aligned across environments . Often, optimizing these constraints demand approximations or relaxations that undermine the efficacy of the approach .

In this work, we take a different approach using a nonparametric strategy based on a recently-proposed Nadaraya-Watson (NW) head . Instead of computing the class probability directly from an input query, the NW head makes a prediction by comparing the learned representations of the query to the elements of a support set that consists of labeled data. Thus, the NW prediction is computed _relative to other real datapoints_ in the support set, with the support set providing a degree of flexibility notpossible with parametric models. In particular, one can manipulate it during training in a way which restricts the types of comparisons that the model can make.

In this work, we manipulate the support set during training to encode causal assumptions for the purposes of learning invariant representations. Specifically, restricting the support set to be drawn from a single environment precludes the possibility of using environment-specific features to make a prediction for a given query. We show that this setup is causally-motivated and relates to existing causal frameworks. Furthermore, we show that this training strategy leads to competitive to superior results compared to state-of-the-art parametric baselines.

Our contributions are as follows:

* We present causally-motivated assumptions for domain generalization which justify our modeling and training strategy.
* We present a novel approach to invariant representation learning using the nonparametric Nadaraya-Watson head, which can account for causal assumptions by manipulating a support set. In particular, we propose a training strategy which, unlike competing baselines, has _no invariance hyperparameter to tune_.
* We validate our approach on several datasets and demonstrate competitive results compared to state-of-the-art parametric baselines.

## 2 Related Works

### Domain Generalization and Invariant Representations

Domain generalization seeks to make models robust to unseen environments and is an active area of research [21; 56; 71]. One line of work augments or synthetically-generates additional training images to increase robustness of learned features to unseen environments [59; 63; 64; 65; 70]. In particular, LISA uses a mixup-style  interpolation technique to generate augmented images, which the authors demonstrate improves out-of-distribution robustness . Another line of work broadly seeks to align features across distributions. Deep CORAL aligns correlations of layer activations in deep neural networks , and other works minimize the divergence of feature distributions with different distance metrics such as maximum mean discrepancy [51; 32], an adversarial loss [14; 30], and Wasserstein distance . Still other works approach the problem from the perspective of the gradients and optimization [12; 29; 34; 46; 57]. For example, Fish aligns the gradients from different domains .

One can also achieve domain generalization via learning invariant representations, which often requires reasoning about the data-generating process from a causal perspective to arrive at appropriate constraints . Invariant causal prediction (ICP) formulates the problem from a feature selection perspective, where the goal is to select the features which are direct causal parents of the label . Invariant Risk Minimization (IRM) can be viewed as an extension of ICP designed for deep, nonlinear neural networks. The IRM objective can be summarized as finding the representation \(\) such that the optimal linear classifier's parameters \(w^{*}\) on top of this representation is the same across all environments . This bi-level program is highly non-convex and difficult to solve. To find an approximate solution, the authors consider a Lagrangian form, whereby the sub-optimality with respect to the constraint is expressed as the squared norm of the gradients of each of the inner optimization problems. Follow-up works analyzing IRM have raised theoretical issues with this objective and presented some practical concerns [16; 22; 42]. Various flavors of IRM have also been proposed by introducing different regularization terms [27; 54; 58].

### Nonparametric Deep Learning

Nonparametric models in deep learning have received much attention in previous work. Deep Gaussian Processes , Deep Kernel Learning , and Neural Processes  build upon Gaussian Processes and extend them to representation learning. Other works have generalized \(k\)-nearest neighbors [37; 49], decision trees , density estimation , and more general kernel-based methods [15; 36; 66] to deep networks and have explored the interpretability that these frameworks provide. Closely-related but orthogonal to nonparametric models are attention-based models, most notably self-attention mechanisms popularized in Transformer-based architectures in natural language processing  and, more recently, computer vision [11; 19; 38]. Nonparametric transformers apply attention in a nonparametric setting .

Recently, Wang et al. proposed the NW head , an extension of the classical NW model [2; 35; 61] to deep learning. In the NW head, the prediction is a weighted average of labels from a support set. The weights are computed from distances between the query and support features. The NW head can yield better calibration and interpretability, with similar accuracy compared to the dominant approach of using a parametric classifier with fully-connected layers. In this work, we leverage the NW head to encode causal assumptions via the support set. The interpretability and explainability benefits of the NW head carry over in this work; while not of primary focus, we explore these properties in the Appendix.

## 3 Preliminaries

**Problem Setting.** Let \(X,Y\) denote a datapoint and its corresponding discrete class, and \(E\) denote the environment (or domain) where \(X,Y\) originates.1 That is, the elements of the training dataset \(_{tr}=\{x_{i},y_{i},e_{i}\}_{i=1}^{N}\) are drawn first by sampling the discrete random variable \(e_{i} P(E)\), and then sampling \(x_{i},y_{i} P(X,Y E=e_{i}):=P_{e_{i}}(X,Y)\). Our goal is to learn classifiers that will generalize to new, unseen environments.

**Assumptions.** We assume there exists a pair of latent causal parents of \(X\): an environment-independent ("content") factor \(Z_{C}\) and an environment-dependent ("style") factor \(Z_{S}\).2 We as

Figure 1: Illustration of proposed approach. Support set of labeled datapoints (square/triangle) from 3 environments lie in 3 regions in the feature space. Black circle denotes query datapoint with unknown label. a) The NW head models \(P(Y|X)\) by making predictions as a function of distances to labeled datapoints in the feature space (visualized as dotted arrows). b) Balancing comparisons across labels for all environments models \(P^{B}(Y|X)\). c) Conditioning on a single environment models \(P_{e}(Y|X)\).

Figure 2: a) Causal Directed Acyclic Graph (DAG) we consider in this work. Solid nodes are observed and dashed nodes are unobserved. We assume an anti-causal setting where label \(Y\) causes \(X\), and \(X\) has 2 causal parents: “style” features, \(Z_{S}\), which are influenced by the environment \(E\); and environment-independent “content” features of \(X,Z_{C}\), which are causally influenced by the label \(Y\). \(E\) potentially influences \(Y\). Both \(E\) and \(Y\) have direct influence on style features \(Z_{S}\). b) Same DAG as a) with an intervention on \(Y\). We note that \(Y\!\!\! E Z_{C}\) and \(Y\!\!\! E Z_{S}\).

sume the causal mechanism that generates \(X\) from (\(Z_{C}\), \(Z_{S}\)) is injective, so that, in principle, it is possible to recover the latent features from the observations; i.e. there exists a function \(g\) such that \(g(X)=(Z_{C},Z_{S})\). We further assume that \(g\) can be disentangled into \(g_{C}\) and \(g_{S}\), such that \((Z_{C},Z_{S})=g(X)=(g_{C}(X),g_{S}(X))\). The causal graph is depicted in Fig. 2a. Finally, we assume that if any \(X=x\) has a non-zero probability in one environment, it has a non-zero probability in all environments.

**Motivation.** The motivating application in this work is image classification, where each realization of \(E\) might represent a different site, imaging device, or geographical region where \(X,Y\) are collected. For example, in medical imaging, different hospitals (\(E\)) may collect images (\(X\)) attempting to capture the presence of some disease (\(Y\)), but may differ in their imaging protocols which lead to differences in the image style features \(Z_{S}\) (e.g. staining, markings, orientation). In addition, we allow \(Z_{S}\) to be influenced by the label itself (for example, positive examples are more likely to be marked by a doctor or have specific staining than negative examples). Finally, the prevalence of \(Y\) may be influenced by \(E\) (for example, the prevalence of a disease may be higher in a certain hospital).

The goal is to find an estimator for \(Y\) which relies only on the direct causal links between \(Y\) and \(X\) and not on any spurious associations between \(E\) and \(X\), as these may change in a new, unseen environment. That is, we seek an estimator which relies only on \(Z_{C}\) and which is independent of \(E\) or \(Z_{S}\).

First, we note the direct causal dependence \(E Y\). For example, a model can exploit this association by learning to over-predict majority classes in a certain environment. One way to remove the direct dependence is by intervening on \(Y\), thus removing incoming edges to \(Y\). This essentially corresponds to matching the environment-specific prior on \(Y\) between environments, and results in the intervened graph in Fig. 2b.3 Let us refer to any distribution which follows the DAG in Fig. 2b as \(P_{e}^{B}(X,Y)\).

Second, we observe that there is a potential non-causal association flow between \(E\) and \(Y\) through the colliders \(X\) and \(Z_{S}\), when either one of these are conditioned on (i.e. are observed). An estimator which relies on \(Z_{S}\) potentially leaks information from \(E\), and this is unstable in new environments. Reading d-separation on this intervened graph, we infer that \(Y  height 0.4pt width 100 pt}}}\ E Z_{C}=g_{C}(X)\) and \(Y  height 0.4pt width 100 pt}}}\ E Z_{S}=g_{S}(X)\), that is:

\[P_{e}^{B}(Y g_{C}(X))=P_{e^{}}^{B}(Y g_{C}(X))\;\; e,e^{ } E.\] (1)

In words, this assumption states that the probability of \(Y\) given the environment-invariant parts of \(X\) is the same across any environment \(e E\).

Thus, we seek an estimator that 1) enables interventions on \(Y\) such that the direct dependence \(E Y\) can be removed, and 2) can further encode the assumption in Eq. (1).

## 4 NW Head for Invariant Prediction

Given a datapoint \(x\), support set \(=\{x_{i},y_{i}\}_{i=1}^{N_{g}}\), and parameters \(\), the NW head estimates \(P(Y=y X=x)\) by outputting a prediction formulated as a weighted sum of support set labels, where the weights are some notion of similarity in the feature space :

\[(Y=y X=x;):=f_{}(x,)=^{N_{g}} \{s((x),(x_{i}))\}}}{_{j=1}^{N_{g}} \{s((x),(x_{j}))\}}.\] (2)

Here, \(\) is the one-hot encoded version of \(y\) and \(s(,)\) is a similarity/kernel function that captures the similarity between pairs of features. In this work, we set \(s\) as the negative Euclidean distance. A graphical depiction is shown in Fig. 3.

Manipulating the support set can encode certain causal assumptions. We consider two types of manipulations:

1. Balancing classes in \(\), denoted \(^{B}\) (see Fig. 1b). This can be interpreted as an intervention on \(Y\), and removes the dependence on \(E Y\), i.e.: \[^{B}(Y=y X=x;):=f_{}(x,^{B}).\] (3)2. Conditioning \(\) on a single environment, denoted \(_{e}\) (see Fig. 1c). This can be interpreted as conditioning the probability estimate on \(E=e\), i.e.: \[_{e}(Y=y X=x;):=f_{}(x,_{e}).\] (4)

Note that both balancing and conditioning can be achieved simultaneously, which we denote \(_{e}^{B}\).

### Objective and Optimization Details

Given a dataset of samples \(_{tr}=\{x_{i},y_{i},e_{i}\}_{i=1}^{N}\), we wish to leverage the NW head as a conditional estimator for \(Y\) conditioned on \(Z_{C}=g_{C}(X)\), where \(g_{C}(X)\) is characterized by Eq. (1). This necessitates an optimization over both \(\) and the space of functions \(g_{C}\). Thus, we solve the following constrained maximum likelihood over \(\) and \(g_{C}\):

\[*{argmax}_{,g_{C}}_{i=1}^{N}_{e_ {i}}^{B}(y_{i} g_{C}(x_{i});)\] (5) \[\ _{e}^{B}(y_{i} g_{C}(x_{i});)=_{e ^{}}^{B}(y_{i} g_{C}(x_{i});),\ \  i\{1,...,N\},\  e,e^{} E.\]

Note that Eq. (1) implies that \(P_{e}^{B}(y_{i} g_{C}(x_{i}))=P^{B}(y_{i} g_{C}(x_{i}))\). Thus, the objective is equivalent to unconstrained maximum likelihood under the assumption in Eq. (1).

Instead of solving for \(g_{C}\) explicitly, we let both \(\) and \(g_{C}\) be related by the composition \(= g_{C}\), and set \(\) to be the learnable mapping of the NW head, i.e. a neural network. Then, the objective becomes:

\[*{argmin}_{}_{i=1}^{N}L(f_{}(x_{i},_{e_{i}}^{B}),y_{i})\] (6) \[\ f_{}(x_{i},_{e}^{B})=f_{}( x_{i},_{e^{}}^{B}),\ \  i\{1,...,N\},\  e,e^{} E,\]

where \(L\) is the cross-entropy loss. To make the objective tractable, we consider two possible variants:

1. **Explicit.** Solve the optimization problem explicitly via a Lagrangian formulation: \[*{argmin}_{}_{i=1}^{N}L(f_{}(x_{i},_{e_{i}}^{B}),y_{i})+_{e,e^{} E}_{i=1}^{N}\|f_{ }(x_{i},_{e}^{B})-f_{}(x_{i},_{e^{} }^{B})\|_{2}^{2}.\] (7) where \(>0\) is a hyperparameter.
2. **Implicit.** Relax the optimization problem into the following unconstrained problem: \[*{argmin}_{}_{e E}_{i=1}^{N}L(f_{}(x_{i},_{e}^{B}),y_{i}).\] (8)

Figure 3: A depiction of the NW head on a tumor detection task. The NW head computes Euclidean distances \(s(,)\) between query and support features, and uses the distances to weight the support labels. Colored squares represent labels. Diagram displays two different support sets. Top is unconditional support, where support data is drawn from the training data without knowledge of environment information. Bottom is an example of a manipulated support where all support data is drawn from a fixed environment (note similarity in color). Such a support set precludes the possibility of using environment-specific features to make a prediction.

In this formulation, the constraint will be approximately satisfied in the sense that the model will be encouraged to predict the ground truth for a given image, which is identical across all environments. In practice, how well the solution satisfies the constraint will depend on model capacity, the data sample, and optimization procedure.

### Optimization Details

During training, the support set \(\) is drawn stochastically from the training set \(_{tr}\), and all queries and support datapoints are passed through the feature extractor \(\). For computational efficiency, instead of sampling a unique support mini-batch at the query-level, we sample a unique support at the mini-batch level. Thus, if \(N_{q}\) and \(N_{s}\) are the query and support mini-batch sizes respectively, the effective mini-batch size is \(N_{q}+N_{s}\), instead of \(N_{q}N_{s}\). For the implicit variant, we sample one support set for a given mini-batch of queries, forward pass through the NW head, and compute the loss in Eq. (8). For the explicit variant, we sample two support sets for a given mini-batch of queries, perform two independent forward passes through the NW head for each support set, and compute the loss in Eq. (7). As discussed in prior work , the support batch size is a hyperparameter analogous and orthogonal to the query batch size.

A technical point is that the set of labels in the support mini-batch must cover the set of labels in the query mini-batch. Thus, in our implementation, for \(^{B}\), we cycle through all classes and randomly draw \(N_{c}\) examples per class to include in the support. For tasks with a large number of classes, one can subsample from the total number of classes, so long as the sampled classes cover the set of query classes.

### Inference modes

Similar to how the support set can be manipulated during training, we can also design different inference strategies corresponding to different configurations of the support set at test-time. We explore several different inference modes which are possible under the NW framework:

1. **Random.** Sample uniformly at random over the dataset, such that each class is represented \(k\) times.
2. **Full.** Use the entire balanced training set.
3. **Ensemble.** Given the set of balanced features computed from Full mode, partition the computed features for all training datapoints by environment, compute the softmax predictions with respect to each environment balanced across labels, and average the predictions.
4. **Cluster.** Given the set of balanced features computed from Full mode, perform \(k\)-means clustering on the features of the training datapoints for each class. These \(k\) cluster centroids are then used as the support features for each class. This can be viewed as a distillation of the full training set for efficient inference, with the caveat that the support set no longer corresponds to observed datapoints.

While Full, Ensemble, and Cluster require computing features for the entire support set, in practice these features and centroids can be precomputed. In our experiments, we find that Cluster mode can be a sufficient replacement to Full mode, while being computationally cheaper. These inference modes can be used interchangeably and flexibly. As an example, consider a workflow which would involve using Cluster mode to perform efficient inference, and then using Full mode on a select few (potentially problematic) test queries to understand model behavior.

### Connections to Prior Work

Our assumptions in Eq. (1) are common across many works related to learning invariant predictors [40; 27; 54; 41; 26]. Representatively, under the binary classification setting, the IRM objective finds a representation function \(\) which elicits an invariant predictor across environments \(E\) such that for all \(h\) that has a non-zero probability for \((X)\) in any (and all) environment(s):

\[_{e}[Y(X)=h]=_{e^{}}[Y(X)=h], \  e,e^{} E.\]

Eq. (1) can be viewed as a generalization of this equality to multi-class settings.4.

Furthermore, note that given the feature extractor \(\), the NW mechanism \(f\) is a nonlearnable classifier, whereas \(w\) is learned in the IRM setting. Thus, our proposed objective can be interpreted as learning invariant features \(\), where the _fixed classifier constraint is satisfied by construction_. This avoids the need to approximate the complex bilevel optimization problem with a regularizer which assumes convexity and requires computing the Hessian. Essentially, \(f\) enforces invariance through the manipulation of the support set, providing a more intuitive and computationally simpler objective to optimize.

In the Experiments section, we compare IRM against a variant of our algorithm where we freeze the learned representations and finetune a linear classifier on top using the same training data. We find that our algorithm performs better than IRM on all datasets, suggesting that it captures invariant representations better than IRM.

## 5 Experiments and Results

### Baselines

We compare against several popular and competitive baseline algorithms: empirical risk minimization (ERM) , invariant risk minimization (IRM) , deep CORAL , Fish , LISA , and CLOvE . When available, results on baselines are pulled from their respective papers. Details on baseline algorithms are provided in the Appendix.

### Datasets

We experiment on 3 real-world domain generalization tasks. Two are from the WILDS benchmark , and the third is a challenging melanoma detection task. Details on the datasets are summarized in Table 1, and further information is provided in the Appendix.

   _Dataset_ & _\# Classes_ & _Env_ & _\# Envs_ & _Architecture_ & _Metric_ \\  Camelyon-17 & 2 & Hospital & 3 & DenseNet-121 & Average acc. \\ ISIC & 2 & Hospital & 3 & ResNet-50 & F1-score \\ FMoW & 62 & Region & 5 & DenseNet-121 & Worst-region acc. \\   

Table 1: Summary of Datasets.

   _Algorithm_ & _Camelyon-17_ & _ISIC_ & _FMoW_ \\  ERM  & 70.3\({}_{ 6.4}\) & 58.2\({}_{ 2.9}\) & 32.6\({}_{ 1.6}\) \\ IRM  & 70.9\({}_{ 6.8}\) & 57.9\({}_{ 1.0}\) & 31.3\({}_{ 1.2}\) \\ CORAL  & 72.4\({}_{ 4.4}\) & 59.1\({}_{ 2.2}\) & 31.7\({}_{ 1.0}\) \\ Fish  & 74.7\({}_{ 7e-2}\) & 64.4\({}_{ 1.7}\) & 34.6\({}_{ 0.0}\) \\ LISA  & 77.1\({}_{ 6.5}\) & 64.8\({}_{ 2.3}\) & 35.5\({}_{ 1.8}\) \\ CLOvE  & 79.9\({}_{ 3.9}\) & 66.2\({}_{ 2.2}\) & **40.1\({}_{ 0.6}\)** \\  NW\({}^{}\), Random & 71.7\({}_{ 5.3}\) & 56.7\({}_{ 1.4}\) & 31.1\({}_{ 0.8}\) \\ NW\({}^{}\), Full & 72.0\({}_{ 6.7}\) & 61.9\({}_{ 3.5}\) & 31.6\({}_{ 0.9}\) \\ NW\({}^{}\), Cluster & 70.6\({}_{ 6.9}\) & 61.4\({}_{ 2.3}\) & 31.3\({}_{ 0.9}\) \\ NW\({}^{}\), Ensemble & 71.9\({}_{ 6.0}\) & 63.9\({}_{ 3.8}\) & 32.2\({}_{ 1.0}\) \\ NW\({}^{}\), Probe & 69.2\({}_{ 7.4}\) & 59.7\({}_{ 2.5}\) & 29.9\({}_{ 1.5}\) \\  NW\({}^{}_{e}\), Random & 74.8\({}_{ 8.4}\) / 75.3\({}_{ 3.2}\) & 57.5\({}_{ 1.9}\) / 55.0\({}_{ 0.9}\) & 31.2\({}_{ 0.7}\) / 30.9\({}_{ 0.5}\) \\ NW\({}^{}_{e}\), Full & **80.0\({}_{ 2.7}\)** / 79.7\({}_{ 1.9}\) & 69.6\({}_{ 2.3}\) / 70.0\({}_{ 1.0}\) & 35.0\({}_{ 0.7}\) / 34.6\({}_{ 0.4}\) \\ NW\({}^{}_{e}\), Cluster & 78.6\({}_{ 2.5}\) / 79.0\({}_{ 1.4}\) & **71.1\({}_{ 1.7}\)** / 71.0\({}_{ 1.0}\) & 33.9\({}_{ 0.6}\) / 34.0\({}_{ 0.3}\) \\ NW\({}^{}_{e}\), Ensemble & 79.5\({}_{ 2.6}\) / 79.6\({}_{ 1.9}\) & 69.5\({}_{ 2.2}\) / 69.8\({}_{ 0.8}\) & 37.8\({}_{ 0.9}\) / 38.2\({}_{ 0.4}\) \\ NW\({}^{}_{e}\), Probe & 75.3\({}_{ 7.3}\) / 75.8\({}_{ 8.3}\) & 61.4\({}_{ 3.1}\) / 63.4\({}_{ 2.8}\) & 33.9\({}_{ 1.5}\) / 32.7\({}_{ 1.4}\) \\   

Table 2: Metric average \(\) standard deviation for all datasets (%). Higher is better. **Bold** is best and underline is second-best. Implicit / Explicit.

1. The Camelyon-17 dataset  comprises of microscopic images of stained tissue patches from different hospitals, where the label corresponds to presence of tumor tissue in patches and the environment is the hospital where the patch comes from.
2. The melanoma dataset is from the International Skin Imaging Collaboration (ISIC) archive5. The ISIC dataset comprises of dermoscopic images of skin lesions from different hospitals, where the label corresponds to whether or not the lesion is diagnosed as melanoma and the environment is the hospital where the image comes from. There is significant less positive examples and negative examples, with this label imbalance varying across environments (see Appendix). 3. The Functional Map of the World (FMoW) dataset  comprises of RGB satellite images, where the label is one of 62 building or land use categories, and the environment represents the year the image was taken and its geographical region.

### Experimental Setup

For each model variant, we train 5 separate models with different random seeds, and perform model selection on an out-of-distribution (OOD) validation set. For WILDS datasets, we follow all hyperparameters, use model selection techniques, and report metrics as specified by the benchmark. This includes using a DenseNet-121 backbone initialized with pretrained ImageNet weights as \(\) and no random augmentations for both datasets. Similarly for ISIC, we use a pretrained ResNet-50 backbone as \(\) with no augmentations, and perform model selection on an OOD validation set. Due to significant label imbalance, we report F1-score instead of average accuracy.

For NW algorithms, we refer to models which balance classes (i.e. modeling Eq. (3)) as NW\({}^{}\), and models which additionally condition on environment (i.e. modeling both Eq. (3) and Eq. (4)) as NW\({}^{}_{}\). For NW\({}^{}_{}\) models, we train explicit and implicit variants. For all NW algorithms, we perform evaluation on all inference modes. In addition, for completeness, we experiment on a variant where we freeze the feature extractor and finetune a linear probe on top of the learned representations on the same training data \(_{tr}\), which we refer to as "Probe". As an example, the implicit variant of NW\({}^{}_{}\) is trained on Eq. (8), where the support set is balanced across classes (B) and conditioned on an environment (e).

We set \(N_{c}=8\) for Camelyon-17 and ISIC and \(N_{c}=1\) for FMoW. An analysis of this hyperparameter is provided in the Appendix. The query batch size \(N_{q}\) is set to 8 for all NW experiments. For Random and Cluster inference modes, we set \(k=3\). This was chosen based on prior work , where \(k=3\) was shown to balance good error rate performance with computational efficiency. For explicit variants, we tune \(\) for all datasets via grid search on a held-out validation set. Full hyperparameter details are provided in the Appendix.

All training and inference is done on an Nvidia A6000 GPU and all code is written in Pytorch.6.

### Results

Table 2 shows the main results. We find that on Camelyon-17 and ISIC datasets, NW\({}^{}_{}\) with Full mode outperforms all baselines and variants we consider. In addition, NW\({}^{}_{}\) variants typically have lower variance across random seeds as compared to baselines. For FMoW, NW\({}^{}_{}\) with Ensemble mode performs around \(2\%\) lower than the best performing baseline, CLoVE. We observe that the most computationally-efficient inference mode, Cluster, performs comparably to Full mode for NW\({}^{}_{}\) models, and is in fact the highest-performing model for ISIC. Thus, we conclude that the support set can be an efficient replacement for Full.

For ISIC, we find that almost all NW\({}^{}\) modes (except Random) perform \( 3\%\) better than ERM. This may be attributed to balancing classes across environments, which we suspect has added benefit for highly imbalanced tasks. In contrast, this boost is less apparent for Camelyon-17, which has relatively balanced classes. As an ablation, we compare NW\({}^{}\) against an NW variant without class-balancing in the Appendix. NW\({}^{}_{}\) further improves over NW\({}^{}\) by \( 7\%\). Exploring further, we compare NW\({}^{}\) against an ERM variant with balanced classes per environment, which we denote ERM\({}^{}\). Thisachieves \(63.0 2.5\), which is on-par with \(^{}\). This is expected as the theoretical assumptions are the same for both models.

Comparing implicit to explicit variants of \(^{}_{}\), we do not find much difference in explicitly enforcing Eq. (1), although we do observe significantly lower variances across model runs. Generally, we find the slight performance gain of explicit training to not be worth the computational overhead of doubling the number of support set forward passes per gradient step and tuning the hyperparameter \(\).

While not the highest-performing, we highlight the consistent \(1\)-\(5\%\) improvement of Probe models over IRM, indicating that \(^{}_{}\) may be better at capturing invariant features. However, other nonparametric inference modes still outperform Probe, possibly indicating that the learned features are more suitable for NW-style classifiers.

## 6 Discussion and Limitations

There are several advantages of the proposed NW approach over previous works. First, the implicit training strategy in Eq. (8) has no hyperparameter to tune, while remaining competitive with and often outperforming state-of-the-art baselines which all require tuning a hyperparameter coefficient in the regularized loss. Second, the NW head enables interpretability by interrogating nearest neighbors in the feature space. Since these neighbors directly contribute to the model's prediction (Eq. (2)), interrogation enables a user to see what is driving the model's decision-making. This not only allows for greater model transparency, but also enables interrogating the quality of the invariant features. We explore this capability in Section H in the Appendix. Note this this degree of transparency is not present in parametric baselines. Lastly, from an intuitive standpoint, we believe our non-parametric approach to enforcing invariance across environments is more natural than baseline methods, since an environment is encoded by manipulating the support set to contain real samples only from that environment. Other baseline methods resort to proxy methods to enforce invariance [54; 48; 27].

One important limitation of our method is computational (see Appendix for analysis of runtimes). The proposed approach requires pairwise comparisons, which scales quadratically with sample size. Practically, this means passing a separate support mini-batch in addition to a query mini-batch at every training iteration. This limitation is compounded for explicit variants, in which two sets of support sets must be drawn independently. Future work may explore more computationally-efficient training strategies. At inference time, Full, Cluster, and Ensemble modes are expensive procedures which require computing features for the entire support set, although precomputing features can mitigate this. However, we argue that in high-risk, safety-critical domains like medical imaging, high inference throughput may not be as important as performance, interpretability, and robustness.

We expect the proposed approach to work well with tasks that have several (and diverse) sets of examples per label class in each environment. If this is not the case, as in the FMoW dataset, the resulting model will be sub-optimal. In particular, in the extreme case where no example is present for a specific class in a given environment, constructing a support set with labels that cover the ground truth label of the query images will not always be possible. This will, in turn, impact performance.

## 7 Conclusion

We presented a nonparametric strategy for invariant representation learning based on the Nadaraya-Watson (NW) head. In the NW head, the prediction is made by comparing the learned representations of the query to the elements of a support set that consists of labeled data. We demonstrated two possible ways of manipulating the support set, and demonstrate how this corresponds to encoding different assumptions from a causal perspective. We validated our approach on three challenging and real-world datasets.

We believe there are many interesting directions of further research. First, our treatment is restricted to classification tasks. Future work may explore an extension to the regression setting. Second, it can be interesting to explore adaptation to the test domain, given additional information. For example, reweighting the occurrence of samples per label could provide improved results given knowledge about the edge \(E Y\) in the test distribution. One can further envision implementing the proposed method in settings where there are previously unseen test time labels/tasks. Finally, we are interested in replacing the fixed similarity function with a learnable kernel.