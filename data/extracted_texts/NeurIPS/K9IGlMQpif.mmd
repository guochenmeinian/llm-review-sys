# SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Loss Trajectories of Small Models

SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Loss Trajectories of Small Models

 Yu Yang\({}^{1}\)  Siddhartha Mishra\({}^{1}\)  Jeffrey Chiang\({}^{2}\)  Baharan Mirzasoleiman\({}^{1}\)

\({}^{1}\)Department of Computer Science, \({}^{2}\)Department of Computational Medicine

University of California, Los Angeles (UCLA)

###### Abstract

Despite the effectiveness of data selection for pretraining and instruction fine-tuning large language models (LLMs), improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which trains a small model, clusters loss trajectories of the examples, and samples from these clusters to guide data selection for larger models. We prove that during fine-tuning, samples within the same loss trajectory cluster exhibit similar gradients. Then, we show that S2L subsets have a bounded gradient error w.r.t. the full data, hence guarantee convergence to the neighborhood of the optimal solution. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data requirement to just 11% of the original MathInstruct dataset  to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of \(4.7\%\) across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the challenging MATH  benchmark, improving Phi-2  by 16.6%. In clinical text summarization on the MIMIC-III dataset , S2L again outperforms training on the full dataset using only 50% of the data. Notably, S2L can perform scalable data selection using a reference model \(100\) smaller than the target model, proportionally reducing the computational cost. 1

## 1 Introduction

In recent years, large language models (LLMs) have revolutionized artificial intelligence by demonstrating an unprecedented ability to understand and generate human language . Among all the contributing factors, the quality and selection of data is becoming increasingly recognized for its importance in training LLMs effectively. Recent research indicates that LLMs benefit more from training for additional epochs on carefully curated data rather than on larger, uncurated ones during pretraining  and instruction fine-tuning , making data selection one of the most promising means to unlock the next level of LLMs' language capability. However, while generalist models obtained through pre-training or instruction fine-tuning excel in _general language tasks_, they may not deliver optimal outcomes in _specialized domain_, such as mathematics , code , medicine , or finance . These domains are not only critical for real-world applications but also hold substantial economic and societal impacts.

To maximize performance in specialized domains, models fine-tuned on domain data offer superior capabilities over generalist models . Yet, maximizing the data efficiency in supervised fine-tuning (SFT) for specialized domains remains a challenging and under-explored problem. Firstly, heuristic approaches that are effective in the instruction fine-tuning stage, like manual curation  or using advanced models such as GPT-4 for dataset evaluation , are less reliable due to the need for specialized knowledge and become costly with large volumes of uncurated fine-tuning data. Beyond these heuristic methods, other approaches rely on generating representations for each training example using a reference model, often utilizing metrics like perplexity , confidence , or hidden states  as features. However, these techniques also fall short in SFT for specialized domains for two reasons: (1) the significant shift between pretraining and SFT data can render these metrics less informative (Figure 0(b)), and (2) the computation and memory demands associated with generating representations for each training example become prohibitive, as these specialized domains often require larger models, some with up to 540 billion parameters , leading to substantial scalability challenges (Figure 0(c)).

To tackle the challenges of data efficiency in SFT for specialized domains, we present SmallToLarge (S2L), an effective and scalable data selection algorithm. S2L operates by first gathering training loss trajectories for each training example using a small model. These trajectories are then clustered, and similar number of examples are selected from these clusters uniformly at random. This process is grounded in our theoretical findings that examples within the same cluster exhibit similar gradients during training, thereby affecting the model similarly. Consequently, subsets sampled from these clusters have a bounded gradient error w.r.t. the full data, allowing for training a comparable model with only a subset of data. Furthermore, we provide a convergence rate analysis for training on these subsets, establishing a robust theoretical foundation for S2L's effectiveness and efficiency.

To validate S2L's effectiveness, we applied it to the challenging tasks of SFT for (1) mathematical problem-solving and (2) clinical text summarization. Our experiments on MathInstruct  shows that S2L can significantly reduce the required training data size to just 11% of the original dataset size while still matching the performance levels of the full dataset, outperforming current state-of-the-art one-shot and online data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, on the MATH benchmark , S2L attained a 32.7% accuracy with just 50K data points, improving the best open-sourced model under 3 billion parameters, Phi-2, by 16.6%. For clinical text summarization tasks on the MIMIC-III  dataset, S2L outperforms training on the full dataset, using only half of the data. Unlike existing methods that require training and getting features from large models, S2L achieves superior data efficiency using a model with as few as 70 million parameters, which is \(100\) smaller than the largest target model we train with 7 billion parameters.

Figure 1: Existing data selection methods depend heavily on the feature representations from a reference model, which makes their effectiveness vulnerable to the quality of training on the target domain . For supervised fine-tuning (SFT), while pretrained models can effectively separate topics (shown in different colors) in natural language (Figure 0(a)), they struggle with fine-tuning data that deviates from the pretraining distribution (Figure 0(b)). Additionally, the cost of training a reference model escalates with model size (Figure 0(c)), making existing data selection methods for large models prohibitively expensive.

Related Work

**Foundations of Data Selection.** Data selection has been well studied for small models and classification tasks. There are one-shot algorithms that select data based on rankings of the proposed training statistics, for example, the L2-norms of error and gradient vectors (EL2N and GraNd) , confidence and its variability across epochs , and the number of times each example is learned but then forgot at the subsequent training step . Besides these heuristic indicators, there are embedding-based pruning algorithms  and online selection algorithms with theoretical performance guarantees for efficiency [35; 23; 24; 40; 60] and robustness [59; 62; 16]. Coleman et al. proposed to use the intermediate feature representation of a small proxy model to select data for image classification. Most recently, data selection has shown great potential in more substantial training speedup when implemented on near-storage hardware , and data selection beyond supervised learning of image data, e.g., for self-supervised learning  and multimodal learning [1; 33], also emerged.

**Data Efficient Training of Large Language Models.** For the pre-training of LLMs, Marion et al. studied data quality indicators including Perplexity, Error L2-Norm (EL2N) , and memorization ranking , and found training on examples with middle Perplexity rankings outperforms training on examples selected based on the other two metrics, and sometimes even outperforms training on the entire dataset. Tirumala et al. uses pre-trained model embeddings to select data for LLM pre-training. The proposed algorithm, D4, first applies an embedding-based data de-duplication algorithm  and then discards data points that are the closest to the K-Means cluster centroids in the embedding space  to ensure diversity. On fine-tuning LLMs, existing work on data efficiency primarily focused on manually curating high-quality instructions , or using strong closed-source models (e.g., GPT-4  or ChatGPT) to rate the quality of each training example [18; 27; 8]. Bhatt et al. implemented an experimental design framework to evaluate the existing data selection methods for instruction fine-tuning of LLMs and found selecting facility locations based on hidden representations (i.e., embeddings) is the most effective. As the only data selection algorithm for specialized domains, SCIP  focuses on pruning low-quality code data for training code LLMs. Since it relies on breaking the code syntax to understand the characteristics of low-quality code in the embedding (i.e, hidden states) space, adapting SCIP to domains other than Python code data is non-trivial.

**Adapting Large Language Models for Specialized Domains.** The rapid development of large language models (LLMs) gives rise to new state-of-the-art models in specialized domains. For mathematical reasoning, Galactica , MINERVA  and Llemma  continue to train an LLM on large-scale math-related web data to improve a model's general scientific reasoning; WizardMath  and TinyGSM  fine-tune LLMs using supervised data. Similarly for medical LLMs, Cheng et al. continued training pre-trained LLMs on medical text, and [43; 44] fine-tuned PaLM with instruction prompt tuning on medical domain data.

## 3 Problem Formulation

**LLM Fine-tuning Objective.** Consider a transformer-based language model, parameterized by \(\), and denoted as \(p_{}\). This model, when provided with a sequence of prompt tokens \(=(x_{1},,x_{M})\), generates a sequence of response tokens \(=(y_{1},,y_{L})\). The conditional probability of generating \(\) given \(\) is then formulated as

\[p_{}(|)=_{l=1}^{L}p_{}(y_{l}| _{1:l-1},).\] (1)

Note that \(_{1:0}\) is an empty sequence. To adapt the pre-trained LLM for a specialized domain of distribution \(\), supervised fine-tuning (SFT) is usually employed with a domain-specific training dataset \(D_{}=\{(,)_{i}\}_{i=1}^{n}\) containing pairs of prompt \(\) and annotated response \(\). The fine-tuning objective is thus to minimize the following negative log likelihood loss, expressed as:

\[_{}(,D_{})=-_ {(,)_{i} D_{}} p_{}( _{i}|_{i}).\] (2)

**Data Selection Objective.** In a general setting for data selection, we consider a target language model \(p_{}\) with parameters \(\). Given a fixed data budget \(B\), which constrains the number of data pointsthat can be used for training, our objective is to select a subset \(S D_{}\) to train the target model, such that it obtains a superior generalization performance. In practice, the subset \(S\) is selected based on a reference model \(r_{}\) parameterized by \(\), which generates representations, confidence scores, or other metrics for each data point \((,)_{i} D_{}\), denoted by \(r_{}((,)_{i})\), which will be utilized by a data selection algorithm to produce \(S\).

In existing data selection algorithms, \(\) is commonly either weights of the pre-trained target model or a target model that has been fully trained on the dataset \(D_{}\). However, as evidenced by Figure 1, representations generated by the pretrained model may not always be good enough for data selection in specialized domains, and fine-tuning the target model significantly increases the computational cost of data selection.

## 4 Methodology

Training a large target model to obtain feature representations for each example in \(D_{}\) can be computationally intensive. However, a recent finding demonstrates that the training dynamics of most examples are consistent across differently sized models of the same family, and this phenomena even generalizes across different model families . Our proposed method, **SMALLToLarge (S2L)**, leverages _loss trajectories_ of training examples collected during fine-tuning a _small_ reference model on the full or a subset of training data.

Loss Trajectory.Let \(^{(t)}\) be the parameters of a small LM during training on \(D_{}\) at times \(t_{q},q\{1,...,T\}\). S2L records the loss trajectory for each data point \(i\) at times \(t_{q}\) during training the reference model \([_{i}^{}(^{(t_{1})}),,_{i}^ {}(^{(t_{T})})]\) where

\[_{i}^{}(^{(t)})=^{}( ^{(t)},(_{i},_{i}))=- p_{^{(t)}}( _{i}|_{i}),\] (3)

and \(T\) is the length of the loss trajectory. Note that \(^{(t)}\) is trained for a fixed number of iterations from \(^{(t-1)}\).

Assume the parameter vector \(^{(t)}\) represents the parameters of the target model at the time \(t\). Define \(_{i}^{}=[_{i}^{}(^{(t _{1})}),,_{i}^{}(^{(t_{T})})]\) and \(_{i}^{}=[_{i}^{}(^ {(t_{1})}),,_{i}^{}(^{(t_{T})})]\) as the training loss trajectory of the example \(i\) on the small proxy model and the large target model, respectively. Let \(_{i}^{d d}\) be the Hessian matrix for each example \(i\) and assume that the loss function for each example during fine-tuning can be modeled by a second-order Taylor approximation with bounded curvature (\(c\|_{i}\| C\)), a reasonable assumption in fine-tuning settings. The following lemma shows that examples with similar loss trajectories on the proxy model have similar gradients throughout the training of the target model.

**Theorem 4.1**.: _If examples \(i\) and \(j\) have similar loss trajectories on the proxy model, i.e., \(\|_{i}^{}-_{j}^{}\|\), and their loss trajectories on the proxy and target model is similar, i.e., \(\|_{p}^{}-_{p}^{}\|\) for \(p\{i,j\}\), then \(i\) and \(j\) have similar gradients throughout training the target model:_

\[\|_{i}^{}()-_{j}^{ }()\|+2CD^{2}}{d}=.\] (4)

_where \(^{}=+2\) and \(\|\| D\) for all \(t\)._

The proof of Theorem 4.1 can be found in Appendix A.1. Theorem 4.1 shows that examples with similar loss trajectories have similar gradients during the training, thereby influencing the model in a similar manner.

Data selection from Loss Trajectory Clusters.Once the loss trajectories are recorded on the proxy model, we apply a clustering algorithm to group examples based on the similarity of their loss trajectories. This results in a set of clusters \(\{C_{1},C_{2},,C_{K}\}\), where each cluster \(C_{i}\) contains examples with similar loss and gradient trajectory throughout the training:

\[C_{i}=\{(,)_{j} D_{}|i=_{j[K]}d( _{j},_{C_{j}})\},\] (5)

where \(_{C_{i}}\) is the centroid of the loss trajectories in cluster \(C_{i}\), and \(d(,)\) is a distance metric, such as Euclidean distance, used for clustering. For datasets that contain different sources of data, we cluster each source separately.

on the full dataset. IG methods such as Stochastic Gradient Descent (SGD) update parameters iteratively based on the gradient of the loss of individual examples, multiplied by stepsize \(\). Formally,

\[^{t+1}=^{t}-_{i}^{}( ^{t}).\] (6)

**Corollary 4.2**.: _Under the assumptions of Theorem 4.1, applying IG with stepsize \(\) to subsets found by S2L, converges to the neighborhood of the optimal solution, as follows:_

\[\|^{t+1}-^{}\|^{2}(1- c)^{t+1}\|^{t}-^{}\|^{2}+2 R/c^{2}+ B^{2}(r_{}/k)^{ 2}_{}^{2}\] (7)

_where \(c\|\|\), \(B=k K\) is the total size of the subset, \(_{}\) is the largest gradient norm of individual examples during training, \(r_{}=_{j}|C_{j}|,r_{}=_{j}|C_{j}|\), \(R=\{d_{0},B_{}+/c\}\) and \(d_{0}=\|^{0}-^{}\|\) is the initial distance to the optimal solution \(^{}\), and \(\) is given by:_

\[=K[r_{}+(r_{}-r_{})_{}].\] (8)

The proof can be found in Appendix A.2.

## 5 Experiments

In this section, we present the comprehensive experiments conducted to evaluate the efficacy of the proposed data selection method, SmallToLarge (S2L), across two challenging domains (mathematical reasoning and clinical text summarization).

Figure 2: Examples in the same clusters have very similar loss trajectories (Figure 1(a)) while the loss trajectories of examples in different clusters are very different (Figure 1(b)).

### Baselines

We systematically compare S2L against a comprehensive set of open-sourced data selection methods. These methods are categorized based on the type of representation they use and selected as the most representative or best-performing methods as identified in prior work. These include: (1) **Random Sampling**; selecting examples with the (2) **Least Confidence** or (3)**Middle Perplexity**; (4) **High Learnability**, determined by the loss decrease before and after full fine-tuning ; and (5) **Facility Locations** selection based on hidden states . Additionally, we incorporate one online selection techniques: (6) **Confidence Curriculum** proposed by Varshney et al., which selects examples with decreasing confidence during the training. Given that the optimal reference model may vary for each one-shot selection method, we ensure a fair comparison by adopting the approach used in , which runs each method with both the fully fine-tuned target model and an early fine-tuning checkpoint as the reference model. We report the best results from these setups.

### Specialized Domain 1: Mathematical Reasoning

**Training Settings.** We focus on fine-tuning using the **MathInstruct** dataset  with 262,040 training examples for its comprehensive coverage of diverse mathematical fields and its capability in training models to achieve state-of-the-art performance on the standard evaluation benchmarks. We employ the open-source model suites Pythia , Phi-2 , Llama-2  as our base models to validate our S2L algorithm and directly compare its performance against the state-of-the-art.

**Evaluation Datasets.** We follow the framework established in  for a comprehensive assessment using several well-regarded datasets, including in-domain and out-of-domain datasets. For the in-domain datasets, we consider **GSM8K**, **MATH**, and **NumGLUE**. For the out-of-domain datasets, we consider **SVAMP**, **Mathematics**, **SimulEq**. These datasets collectively span a diverse range of mathematical subjects, such as algebra, probability, number theory, calculus, and geometry. Additionally, some questions in these datasets require the application of commonsense, reading comprehension, and multi-step reasoning. All questions are open-formed.

**Evaluation Metric.** We use the standard evaluation metric for open-formed questions, **exact match**, which measures the model's accuracy by comparing its generated answers against the correct solutions. For an answer to be considered correct, it must match the reference solution precisely.

More details about the settings and baseline implementations can be found in Appendix B.

#### 5.2.1 Setting 1: Less Data for Better Models

In the first setting, we standardize the number of training steps to correspond to 3 epochs on the full dataset, aligning with . This allows us to maintain a consistent training schedule across different methods and data budgets, ensuring fair and accurate comparisons of the quality of data.

**Scaling the Data: SOTA algorithms fail with small data budgets while S2L stands out across data scales.** In Figure 4, we compare S2L against the baselines from Section 5.1 on Pythia-410M across varying data sizes. The training trajectories used by S2L are based on Pythia-70M, a model approximately 6x smaller than Pythia-410M. With the same number of training steps as the full training, S2L exceeds the full dataset's performance using only 30K examples, only 11% of the full dataset. It leads the runner-up baselines by an average of 4.7%, 4.6% and 2.4% with data budget 30K, 50K and 100K across all six evaluation datasets. While state-of-the-art data selection algorithms like Facility Locations  and High Learnability  have decent performance with a large enough data budget (i.e., 100K), all SOTA algorithms except S2L cannot even outperform the random sampling baseline when the allowed data size is small (i.e., 30K). Unlike the existing algorithms, S2L consistently outperforms all baselines and even full training across all data sizes. Note that compared to the runner-up algorithm in this setting, Facility Locations, the cost of S2L is much lower in both training the reference model and data selection stages (Figure 5), and therefore more scalable to both larger target models or larger data sizes.

Figure 5: Wall-clock time required to train the reference model and select 100K data from MathInstruct for training Pythia-410M.

**Scaling the Model: Data selected by S2L can transfer to larger models in different model suites.** We also test whether this subset, chosen using Pythia-70M, can effectively train larger models beyond 410M and models outside the Pythia suite. As shown in Table 1, our experiments with Phi-2 reveal that fine-tuning on only 50K S2L-selected data again outperforms full dataset training on the most challenging MATH  benchmark improving the pretrained Phi-2 by \(16.6\%\) and is more data efficient than training on the full MathInstruct dataset to get the same performance.

#### 5.2.2 Setting 2: Less Data for Faster Training

The second setting we consider is when fixing the number of times each example can be seen over the entire course of training, directly translating smaller datasets into reduced training and storage costs. This is particularly beneficial for large models that would require extensive training times without data selection. By experimenting with models of larger sizes than the previous setting, we observe in Table 2 that S2L can achieve comparable performance to full-data training when using only 50% data and thereby reducing both the data storage space and the training time by half.

#### 5.2.3 Why is S2L So Effective?

**Examples in Clusters Encode the Same Knowledge/Skill.** In Appendix C, we compare actual training examples in MathInstruct that get clustered together due to their similar training trajectories on the small Pythia-70M model. We observe that examples in the same cluster are of the same type and related to the same knowledge/skill, e.g., open-formed algebra questions (Figure 15), examples requiring extracting useful information from long text and writing programs

   Target & Fine-tuning &  &  &  \\ Model & Data & GSMsk & & & & & & & \\   & (Pretrained) & 53.4 & 16.1 & 34.9 & 34.8 & 67.9 & 31.1 & 27.4 & 38.5 \\  & Random & 67.9 & 30.1 & 60.7 & 52.9 & 77.1 & 51.2 & 37.5 & 54.1 \\  & High Learnability & 59.4 & 25.2 & 62.1 & 48.9 & 76.6 & 41.8 & 27.2 & 48.7 \\  & Module & Perrality & 66.4 & 29.5 & 54.1 & 50.0 & 74.8 & 50.4 & 39.8 & 52.5 \\  & Least Confidence & 61.7 & 24.7 & 67.0 & 51.1 & 76.5 & 43.3 & 52.5 & 54.3 \\  & Facility Locations & 66.2 & 31.3 & 62.4 & 53.3 & 74.4 & 58.4 & 34.6 & 54.5 \\  & **S2L(OUR)** & **69.1** & **32.6** & **65.7** & **55.8** & **79.6** & 56.4 & 40.1 & 57.3 \\  & Full-262K & 68.3 & 32.6 & 64.3 & 55.1 & 78.4 & 58.4 & 44.2 & **57.7** \\   

Table 1: **Less Data, Same Compute: Zero-shot accuracies (%, \(\)) when S2L and the baselines select 50K data to train with the same number of iterations as the full-data training. Results surpassing full training are highlighted in bold. Figure 4 follows the same setting but uses the Pythia-410M model.**

Figure 4: **Data Scaling: Accuracies (\(\)) on in-domain and out-of-domain datasets using Pythia-410M. Data size refers to the total number of unique training examples used. All experiments in this table share the same total training steps and learning rate schedule (see Section 5.2). See breakdowns in Figure 14.**

Figure 6: Distribution of the coverage of top-1 topic in each cluster. Taller bars on the right end of the plot indicate clusters with a higher concentration of a single topic and therefore suggest a grouping of similar examples.

[MISSING_PAGE_FAIL:8]

recorded at any training stage (early, middle, or recorded sparsely throughout the training.

**S2L does not require the full training data to train the proxy and can scale efficiently to larger datasets.** To further demonstrate the scalability of the proposed S2L method, we conducted experiments by training the proxy on a smaller sample of the data (100K examples) for the same number of epochs (3 epochs) and saving the loss for all examples. The results, shown in Figure 10, confirm that S2L remains effective when the proxy model is trained on a smaller subset of training data and therefore is scalable to larger datasets without a proportional increase in computational costs.

**S2L is robust across different clustering parameter values for K.** We conducted detailed experiments varying the clustering parameter K, as shown in Figure 11. The results demonstrate that S2L maintains high performance across different values of K, highlighting the robustness of our method to different clustering parameter choices. We chose K=100 for our experiments as it provided the best average accuracy across the evaluation datasets for the math reasoning task.

**S2L remains effective and efficient compared to using full data when trained for the same number of epochs.** Figure 12 illustrates the relative accuracy to full data across different epochs, comparing S2L-selected data and full data with the same number of epochs. Both in-domain and overall average accuracy are shown. S2L demonstrates superior performance with fewer data and fewer training iterations.

**S2L supports a range of small models as effective proxies.** To understand whether different small models could serve as effective proxies, we used GPT-2 (124M) and Pythia-160M as proxy models for data selection to train Pythia-410M. The results, illustrated in Figure 13, show that both proxy models perform comparably in guiding the data selection, demonstrating the versatility and effectiveness of using different small models for S2L.
In this work, we introduced SMLToLarge (S2L), a scalable data selection method to improve the data efficiency of supervised fine-tuning (SFT) for large language models (LLMs) in specialized domains. By clustering data points based on their training dynamics on smaller models and balanced sampling from all clusters, S2L significantly reduces the required training data size without compromising performance compared to using the entire training dataset. Our comprehensive experiments across the mathematical problem-solving and clinical text summarization domains demonstrate the effectiveness of S2L.

Our study does come with its limitations. S2L has been only tested within two domains, mathematics and medicine, and on models up to 7 billion parameters, constrained by our computational resources. Additionally, our experiments employed a fixed training schedule across all methods without further optimization or hyperparameter tuning for each method, including S2L. This unified approach, while it ensures a fair comparison, may not fully capture the potential performance improvement that could be achieved with more tailored training strategies. We encourage further research to extend the application of S2L across a broader spectrum of domains and investigate the impact of hyperparameter tuning on its effectiveness.