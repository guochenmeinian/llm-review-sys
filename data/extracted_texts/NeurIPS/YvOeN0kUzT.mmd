# Randomized Strategic Facility Location with Predictions

Eric Balkanski

Columbia University, IEOR

eb3224@columbia.edu

&Vasilis Gkatzelis

Drexel University, Computer Science

gkatz@drexel.edu

&Golnosh Shahkarami

Max Planck Institut fur Informatik, Universitat des Saarlandes

gshahkar@mpi-inf.mpg.de

###### Abstract

In the strategic facility location problem, a set of agents report their locations in a metric space and the goal is to use these reports to open a new facility, minimizing an aggregate distance measure from the agents to the facility. However, agents are strategic and may misreport their locations to influence the facility's placement in their favor. The aim is to design truthful mechanisms, ensuring agents cannot gain by misreporting. This problem was recently revisited through the learning-augmented framework, aiming to move beyond worst-case analysis and design truthful mechanisms that are augmented with (machine-learned) predictions. The focus of this prior work was on mechanisms that are deterministic and augmented with a prediction regarding the optimal facility location. In this paper, we provide a deeper understanding of this problem by exploring the power of randomization as well as the impact of different types of predictions on the performance of truthful learning-augmented mechanisms. We study both the single-dimensional and the Euclidean case and provide upper and lower bounds regarding the achievable approximation of the optimal egalitarian social cost.

## 1 Introduction

In the classic facility location problem the goal is to determine the ideal location for a new facility, taking as input the preferences of a group of \(n\) agents. Due to the wide variety of applications that this problem captures, it has received a lot of attention from different perspectives. A notable example is the strategic version of this problem which is motivated by the fact that the participating agents may be able to strategically misreport their preferences, leading to a facility location choice that they prefer over the one that would be chosen if they were truthful. In the strategic facility location problem, the goal is to design _truthful_ mechanisms, i.e., mechanisms that elicit the agents' true preferences by carefully removing any incentive for the agents to lie (Moulin, 1980, Procaccia and Tennenholtz, 2013). This problem has played a central role in the broader literature on mechanism design without money and a lot of prior work has focused on optimizing the quality of the returned location subject to the truthfulness constraint (see Section 1.2 for a brief overview). However, this work has mostly focused on worst-case analysis, often leading to unnecessarily pessimistic impossibility results.

Aiming to overcome the limitations of worst-case analysis, a surge of work during the last few years has focused on the design of algorithms in the _learning-augmented framework_(Mitzenmacher and Vassilvitskii, 2022). According to this framework, the designer is provided with some machine-learned prediction regarding the instance at hand and the goal is to leverage this additional information in the design process. However, crucially, this information is not guaranteed to be accurate and can,in fact, be arbitrarily inaccurate. The goal is to achieve stronger performance guarantees whenever the prediction happens to be accurate (known as the _consistency_ guarantee), while at the same time maintaining some worst-case guarantee even if the prediction is arbitrarily inaccurate (known as the _robustness_ guarantee).

Agrawal et al. (2022) and Xu and Lu (2022) recently introduced the learning-augmented framework to mechanism design problems involving self-interested strategic agents with private information, and both of these works studied the strategic facility location problem. Agrawal et al. (2022) considered both the egalitarian and the utilitarian social cost objectives and provided tight bounds on the best-feasible robustness and consistency trade-off for truthful mechanisms augmented with a prediction regarding what the optimal facility location would be. However, the achievable trade-offs between robustness and consistency may heavily depend on the specific type of prediction that the mechanism is augmented with: note that the consistency guarantee only binds if the prediction is accurate, so more refined predictions bind on fewer instances (the subset of instances where even the refined prediction is accurate) and could, therefore, enable the design of learning-augmented mechanisms with improved guarantees. This leaves open the question of whether mechanisms equipped with more refined predictions can, indeed, achieve better trade-offs between robustness and consistency. Also, Agrawal et al. (2022) restricted their attention to deterministic mechanisms, leaving open the possibility for truthful randomized mechanisms to achieve even stronger guarantees. In this work we significantly expand our understanding of this problem by studying both the power of randomization and the power of alternative predictions.

### Our Results

We revisit the problem of designing truthful learning-augmented mechanisms for the strategic facility location problem. These mechanisms ask each agent to report their preferred location in some metric space (which is private information) and they are also augmented with some (unreliable) prediction related to this private information. Using the agents' reported locations, along with the prediction, the mechanism chooses a facility location in this metric space, aiming to (approximately) minimize some social cost measure. Our focus in this paper is on the egalitarian social cost, i.e., the maximum distance between an agent's preferred location and the location chosen for the facility. Our goal is to evaluate the performance of mechanisms that can leverage randomization, as well as the power of different types of predictions.

We first consider the well-studied single-dimensional version of the problem, where all agents lie on a line and the mechanism needs to choose a facility location on that line. Prior work on the strategic facility location problem without predictions, showed that for this class of instances no deterministic truthful mechanism can achieve an approximation better than \(2\) and no randomized truthful1 mechanism can achieve an approximation better than \(1.5\); both of these results are shown to be tight (Procaccia and Tennenholtz, 2013). In the learning-augmented framework, Agrawal et al. (2022) showed there exists a truthful deterministic mechanism provided with a prediction regarding the optimal facility location that achieves the best of both worlds: a perfect consistency of \(1\) and an optimal robustness of \(2\).

Our main result for the single-dimensional version shows that even if the mechanism is provided with the strongest possible prediction (i.e., a prediction regarding every agent's preferred location), any randomized truthful mechanism that is \((1+)\)-consistent for some \([0,0.5]\) can be no better than \((2-)\)-robust. This implies that the previously proposed deterministic learning-augmented mechanism that is \(1\)-consistent and \(2\)-robust and the randomized non-learning-augmented mechanism that is \(1.5\)-robust (and hence also \(1.5\)-consistent) are both Pareto optimal among all randomized mechanisms, even if they are augmented with the strongest possible predictions. Beyond these two extreme points, this result proves a lower bound for the achievable trade-off between robustness and consistency for any randomized mechanism, even with the strongest predictions. We complement this bound by observing that this trade-off can, in fact, be achieved by a truthful randomized mechanism that chooses between the optimal learning-augmented deterministic mechanism and the optimal non-augmented randomized mechanism, thus verifying that our bound fully characterizes this Pareto frontier.

We then consider the two-dimensional case, for which much less is known about randomized mechanisms. The best-known truthful randomized mechanism is the Centroid, introduced by Tang et al. (2020), which guarantees a \(2-1/n\) approximation. We provide a truthful randomized mechanism that is equipped with a prediction regarding the identities of the most extreme agents (those who would incur the maximum cost in the optimal solution) and, using this prediction, our mechanism achieves \(1.67\) consistency and \(2\) robustness. The idea is to use these predictions to select at most three extreme agents, apply the Centroid mechanism to them, and guarantee a \(1.67\) approximation if the predictions are accurate not only for these three agents but for all other agents as well, utilizing the properties of the Euler line.

An interesting observation is that the lower bound of \(1.5\) from the single-dimensional case does not extend to two dimensions, so prior work does not imply any lower bound for two dimensions! We prove a lower bound of \(1.118\) for all truthful randomized mechanisms and then that no deterministic mechanism can simultaneously guarantee better than \(2\) consistency and better than \(1+\) robustness, even if it is augmented with the strongest predictions. Similarly, no truthful randomized mechanism can simultaneously guarantee \(1\) consistency and better than \(2\) robustness. The former result proves the optimality of a previously introduced \(1\)-consistent and \(1+\)-robust truthful deterministic mechanism provided only with a prediction regarding the optimal facility location (Agrawal et al., 2022). We also show that the latter is tight.

### Related Work

Strategic facility location.Moulin (1980) provided a characterization of deterministic truthful facility location mechanisms on the line and introduced the median mechanism, which returns the median of the agent location profile \(= x_{1},,x_{n}\). The median mechanism is known to be truthful, providing an optimal solution for the Utilitarian Social Cost and achieving a \(2\)-approximation for the Egalitarian Social Cost, as demonstrated by Procaccia and Tennenholtz (2013). Notably, Procaccia and Tennenholtz (2013) showed that this \(2\)-approximation factor represents the best performance achievable by any deterministic truthful mechanism. Building on this, Border and Jordan (1983) extended these results to the Euclidean space by employing median schemes independently in each dimension. Subsequently, Barbera et al. (1993) further generalized this outcome to any \(L_{1}\)-norms. Other settings explored include general metric spaces (Alon et al., 2010) and d-dimensional Euclidean spaces (Meir, 2019; Walsh, 2020; El-Mhamdi et al., 2023; Goel and Hann-Caruthers, 2020), as well as circles (Alon et al., 2010; Meir, 2019) and trees (Alon et al., 2010; Feldman and Wilf, 2013). Fundamental results in truthful facility location often focus on characterizing the space of truthful mechanisms. For the one-dimensional case, Moulin's characterization (Moulin, 1980) reveals that all deterministic truthful mechanisms belong to the "general median mechanisms (GCM)" family. For the two-dimensional case, a similar characterization was provided by Peters et al. (1993). For a comprehensive review of previous work on this problem, see the survey by Chan et al. (2021).

Learning-augmented algorithms.Worst-case analysis on its own is often not informative enough, and several alternative measures have been proposed to overcome its limitations (Roughgarden, 2021). Learning-augmented algorithms, or algorithms with predictions (Mitzenmacher and Vassilvitskii, 2022), aim to address these limitations by incorporating predictions into the algorithm's design. Lykouris and Vassilvitskii (2021) studied the online caching problem and introduced two main metrics, consistency and robustness, to evaluate the performance of these algorithms. The online version of the facility location problem, introduced by Meyerson (2001), involves points arriving online, requiring us to assign them irrevocably to either an existing facility or open a new facility. The objective is to minimize the distance of each agent to the assigned facility along with the cost of opening the facilities. Almanza et al. (2021); Jiang et al. (2022), and Fotakis et al. (2021) studied this problem augmented with predictions regarding the location of the optimal facility for each incoming point. We direct interested readers to a curated and frequently updated list of papers in this area (Lindermayr and Megow).

Learning-augmented mechanism design.The framework of learning-augmented mechanism design was first introduced by Agrawal et al. (2022) and Xu and Lu (2022). Agrawal et al. (2022) focused on the strategic facility location problem, proposing mechanisms that leverage predictions to improve performance while maintaining truthfulness. Their work achieves the best possible consistency and robustness trade-off given a prediction of the optimal facility location. They also evaluated the performance of their mechanisms as a function of the prediction error. More recently, Christodoulou et al. (2024) provided performance bounds based on an alternative measure of prediction error. Barak et al. (2024) studied this problem assuming that the predictions are regarding each agent's location, but a small fraction of these predictions may be arbitrarily inaccurate. Meanwhile, Chen et al. (2024) evaluated non-truthful mechanisms with respect to their price of anarchy. Istrate and Bonchis (2022) and Fang et al. (2024) studied the obnoxious facility location version of this problem, aiming to design truthful mechanisms. Apart from the facility location problem, other works in learning-augmented mechanism design have been conducted in various contexts, including strategic scheduling (Xu and Lu, 2022; Balkanski et al., 2023a; Lu et al., 2023; Caragiannis and Kalantzis, 2024; Gkatzelis et al., 2024), bicriteria mechanism design (Balcan et al., 2024), graph problems with private input (Colini-Baldeschi et al., 2024), and equilibrium analysis (Gkatzelis et al., 2022; Istrate et al., 2024). Balkanski et al. (2023b) brought together the line of work on learning-augmented mechanism design with the literature on online algorithms with predictions by studying online mechanism design with predictions.

## 2 Preliminaries

In the single facility location problem, there are \(n\) strategic agents with a location profile denoted by \(= x_{1},,x_{n}\), where \(x_{i}\) corresponds to the location of agent \(i\). A mechanism \(f()\) outputs a, potentially randomized, location for the facility. The cost incurred by agent \(i\) is the expected Euclidean distance \([d(x_{i},f())]\) between the facility location \(f()\) and their location \(x_{i}\).

Two renowned cost functions considered in this scenario are Egalitarian Social Cost and Utilitarian Social Cost. In this work, the goal is to optimize the Egalitarian Social Cost \(C(f,)=[_{x_{i}}d(x_{i},f())]\), representing the expected maximum cost experienced by a single agent for each possible outcome of \(f()\). The optimal location for the facility in the two-dimensional Euclidean space corresponds to the center of the smallest circle that encloses all the points, denoted by \(o()\).

To minimize the social cost, a mechanism needs to ask the agents to report their preferred locations, \(^{2n}\), and then use this information to determine the facility location \(f()^{2}\). However, the preferred location \(x_{i}\) of each agent \(i\) is private information, and they can choose to misreport their preferred location if that can reduce their own cost. A mechanism \(f:^{2n}^{2}\) is considered truthful when no individual agent can benefit by misreporting their location, i.e., for all instances \(^{2n}\), every agent \(i[n]\), and every deviation \(x^{}_{i}^{2}\), we have that \(d(x_{i},f()) d(x_{i},f(_{-i},x^{}_{i}))\), where \(_{-i}= x_{1},,x_{i-1},x_{i+1},,x_{n}\) is the vector of the locations of all agents except agent \(i\).

In the context of randomized mechanisms, we can distinguish between two forms of truthfulness: universally truthful and truthful in expectation. A universally truthful mechanism involves a randomization over deterministic truthful mechanisms, with weights that may depend on the input. On the other hand, a mechanism is considered truthful in expectation if truth-telling yields the agent the maximum expected value. Specifically, a mechanism \(f:^{2n}^{2}\) is truthful in expectation if for all instances \(^{2n}\), every agent \(i[n]\), and every deviation \(x^{}_{i}^{2}\), we have that \([d(x_{i},f())][d(x_{i},f(_{-i},x^{ }_{i}))]\).

We focus on mechanisms that are both unanimous and anonymous. A mechanism is unanimous if, when all points \(\) are located at the same position (\(x_{i}=x_{j}\) for all \(i,j[n]\)), it places the facility at that specific location, i.e., \(f()=x_{i}\). To ensure bounded robustness, a mechanism must be unanimous, as the optimal cost is zero when the facility is at the same location as all points, whereas placing it elsewhere incurs a positive cost. A mechanism is anonymous if its outcome is independent of the agents' identities, meaning it remains unchanged under any permutation of the agents. Finally, we also assume the mechanism is scale-independent, i.e., if we multiply every coordinate of every agent by the same factor, the coordinate of the chosen facility location are scaled in the same way; this captures the fact that it is only the relative distances that really matter in this problem.

Learning-augmented algorithms encompass a class of algorithms that enhance their decision-making process by integrating pre-computed predictions or forecasts. These predictions, obtained from various sources, like statistical models, serve as inputs without requiring real-time learning from new data. For instance, in the single facility location problem, one might consider predictions \(}\) regarding all of the agent's preferred locations, a prediction \(F^{*}\) regarding the optimal facility location, or a prediction regarding the identities of the most extreme agents \(}\) (the ones that suffer the maximum cost in the optimal solution), which, as demonstrated in this paper, proves to be quite useful. We denote mechanisms enhanced with predictions in general as \(f(,*)\). Specifically, for each prediction setting like \(}\) or \(F^{*}\), we denote a mechanism enhanced with \(}\) and \(F^{*}\) by \(f(,})\) and \(f(,F^{*})\), respectively. We define mechanisms enhanced with other kinds of predictions in a similar way.

The effectiveness of learning-augmented mechanisms is evaluated using their consistency and robustness. If \(*\) denotes all instances \(\) for which prediction \(*\) is accurate, a mechanism is

* \(\)_-consistent_ if it achieves an \(\)_-approximation_ when the prediction is correct, i.e., \[_{,*:*}\{,* ),)}{C(o(),)}\}.\]
* \(\)_-robust_ if it maintains a \(\)_-approximation_ regardless of the quality of the prediction, i.e., \[_{,*}\{,*),)}{C(o( ),)}\}.\]

## 3 Results for the Line

In this section, we provide a lower bound regarding the performance of any randomized mechanism for the line, even if it is equipped with the strongest type of prediction, i.e., a prediction \(}\) regarding the preferred location of every agent.

**Theorem 1**.: _No mechanism for the line that is truthful in expectation and guarantees \(1+\) consistency for some \([0,0.5]\) can also guarantee robustness better than \(2-\), even if it is provided with full predictions \(}\) containing each of the agents' locations._

The proof consists of two main parts. We first show that, for any instance involving two agents, we can without loss of generality restrict our attention to a class of mechanisms that we call OnlyM mechanisms. We then show the desired lower bound for OnlyM mechanisms.

We introduce the following notations specific to this section. Let \(x_{L}\) be the leftmost reported location and \(x_{R}\) be the rightmost reported location on the line. Therefore, we have \(x_{L} x_{R}\). Let \(M\) denote the midpoint of these two extreme points, i.e., \(M=(x_{L}+x_{R})/2\), which would also correspond to the optimal facility location. For simplicity, we sometimes write \(f(x_{1},,x_{n})\), dropping the angle brackets \(\). OnlyM is the class of mechanisms that, whenever they choose a location within the interval \((x_{L},x_{R})\), then this location is always \(M\).

**Definition 1** (OnlyM mechanisms).: _A mechanism \(f\) for the line is an OnlyM mechanism if \(P[f()(x_{L},x_{R})\{M\}]=0\)._

The main lemma for the proof of Theorem 1 is the following reduction that allows to restrict our attention to OnlyM mechanisms. The complete proof of Lemma 1 can be found in Appendix A.

**Lemma 1**.: _For any problem instance involving two agents with reported locations \(= x_{L},x_{R}\) on the line, and any randomized truthful in expectation mechanism achieving \(\)-consistency and \(\)-robustness over this class of instances, there exists a randomized OnlyM mechanism that is truthful in expectation and achieves the same consistency and robustness guarantees._

Proof Sketch.: Consider a randomized mechanism \(f(x_{L},x_{R})\) with probabilities \(p_{}=[f(x_{L},x_{R})(x_{L},M)]\) and \(p_{r}=[f(x_{L},x_{R})(M,x_{R})]\), representing the chances of selecting a location in \((x_{L},M)\) and \((M,x_{R})\), respectively. If \(p_{}=p_{r}=0\), the mechanism already satisfies the desired property. Otherwise, for \(p_{}>0\) and \(p_{r}>0\), define the expected locations \(_{}=[f(x_{L},x_{R}) f(x_{L},x_{R})(x_{L},M)]\) and \(_{r}=[f(x_{L},x_{R}) f(x_{L},x_{R})(M,x_{R})]\), which can be expressed as convex combinations \(_{}=q_{}x_{L}+(1-q_{})M\) and \(_{r}=q_{r}x_{R}+(1-q_{r})M\) for some \(q_{},q_{r}(0,1)\).

We then construct a new mechanism \(f^{}\) that modifies \(f\) by reassigning probability masses to \(x_{L}\), \(M\), and \(x_{R}\) as follows:

\[[f^{}(x_{L},x_{R})=x] =[f(x_{L},x_{R})=x]&x<x_{L}x>x_{R},\\ 0&x(x_{L},M)(M,x_{R}),\\ [f(x_{L},x_{R})=x]+q_{}p_{}&x=x_{L},\\ [f(x_{L},x_{R})=x]+(1-q_{})p_{}+(1-q_{r})p_{r}&x=M,\\ [f(x_{L},x_{R})=x]+q_{r}p_{r}&x=x_{R}.\]To show \(f^{}\) retains the consistency and robustness guarantees of \(f\), we show that the expected costs of \(f\) and \(f^{}\) are identical for instances with two agents (Lemma 2). Finally, we show that \(f^{}\) maintains the truthfulness in expectation property by verifying that agent costs are unchanged between \(f\) and \(f^{}\) (Lemma 3). 

Equipped with Lemma 1, we can now prove Theorem 1.

Proof of Theorem 1.: We start by proving the result for two-agent instances on the line using any OnlyM mechanism. Then, using Lemma 1, the result follows for any randomized mechanism.

First, note that if the chosen facility location \(y\) is at distance \(d(M,y)\) from the point \(M=(x_{L}+x_{R})/2\), then its egalitarian social cost is equal to \(C(o(),)+d(M,y)\). To verify this fact, assume without loss of generality that this location is on the left of \(M\) and note that its distance from the agent located at \(x_{R}\) is \(d(x_{R},M)+d(M,y)\). As a result, the expected social cost of a mechanism \(f\) with agent locations \(\) is

\[C(f,)=C(o(),)+[d(M,f())].\] (1)

Now, assume that there exists a mechanism that is \((1+)\)-consistent and better than \((2-)\)-robust. For robustness, this would imply that for every instance \(\), irrespective of the prediction, the mechanism must guarantee that

\[)}{C(o(),)}<2-\;\; [d(M,f())]}{C(o(),)}<1-.\] (2)

For this mechanism to be truthful in expectation, it must ensure that no agent has an incentive to misreport their location. Consider the instance \(= x_{L},x_{R}\), and note that the agent at \(x_{L}\) has the option to misreport their location as \(x_{L}^{}=x_{L}-d(x_{L},x_{R})\), which would shift the new midpoint \(M^{}\) to \(x_{L}\) in the new instance \(^{}= x_{L}^{},x_{R}\). This deviation would double the optimal cost, i.e., \(C(o(^{}),^{})=2 C(o(),)\). Inequality (2), then guarantees that the expected cost for this agent after the deviation would be at most \(2(1-) C(o(),)\) since we have

\[[d(M^{},f(^{}))]}{C(o(^{ }),^{})}=[d(x_{L},f(^{} ))]}{2 C(o(),)}<1-.\] (3)

As a result, to ensure that this agent will not misreport, the mechanism needs to ensure that the expected cost of the agent if they report the truth is strictly less than \(2(1-) C(o(),)\). If we let \(P( L)=[f() x_{L}]\) denote the probability that the chosen location is weakly on the left of \(x_{L}\), and \(P(M)=[f()=M]\) denote the probability that the chosen location is \(M\), then the expected cost of the agent located at \(x_{L}\) is at least \(C(o(),)P(M)+2C(o(),)(1-P(M)-P( L))\) because the mechanism is an OnlyM mechanism. This is even if we assume that the only chosen facility locations weakly on the left of \(x_{L}\) (and weakly on the right of \(x_{R}\), respectively) are exactly on \(x_{L}\) (and exactly on \(x_{R}\), respectively). Therefore, the mechanism needs to always satisfy

\[C(o(),)(P(M)+2(1-P(M)-P( L)))<2(1- )C(o(),)\] (4) \[ P(M)+2(1-P(M)-P( L))<2(1-)\;\;\;\;P( L)> -.\]

If we consider the same instance \(= x_{L},x_{R}\), and assume that the mechanism is also provided with accurate predictions \(_{L}=x_{L}\) and \(_{R}=x_{R}\) regarding the agent locations, to guarantee \(1+\) consistency, Inequality (1) implies that

\[,}=),)}{C(o( ),)} 1+\;\;[d(M,f(, }))]}{C(o(),)}.\] (5)

Finally, if we once again consider the same instance with inaccurate predictions \(_{L}=x_{L}\) and \(_{R}=x_{R}+d(x_{L},x_{R})\), we observe that in this case the agent located at \(x_{R}\) would have the option to instead report \(x_{R}^{}=x_{R}+d(x_{L},x_{R})=_{R}\), and the predictions would then appear to be accurate, forcing the mechanism to satisfy Inequality (5) in order to maintain the required consistency bound. Since the true location \(x_{R}\) would now coincide with the middle point \(M^{}\) of the misreported instance \(^{}= x_{L},x_{R}^{}\), this would yield the agent located at \(x_{R}\) who misreported an expected cost of \(2 C(o(),)\) since we have

\[[d(M^{},f(^{},}))]}{C(o(^{}),^{})}=[d(x_{R},f(,}))]}{2 C(o(), )}.\] (6)

As a result, to ensure that this agent will not misreport, the mechanism needs to ensure that the expected cost of the agent if they report the truth is at most \(2 C(o(),)\). If we once again let \(P( L)\) denote the probability that the chosen location is weakly on the left of \(x_{L}\), and \(P(M)\) denote the probability that the chosen location is \(M\), then the expected cost of the agent located at \(x_{R}\) is at least \(C(o(),)P(M)+2C(o(),)P( L)\). This is even if we once again assume that the only chosen facility locations weakly on the left of \(x_{L}\) (and weakly on the right of \(x_{R}\), respectively) are exactly on \(x_{L}\) (and exactly on \(x_{R}\), respectively). Therefore, the mechanism needs to always satisfy

\[P(M)+2P( L) 2\ \ \ \ P( L)-.\]

However, this contradicts Inequality (4), so we conclude that no mechanism can simultaneously guarantee \((1+)\) consistency and a robustness better than \((2-)\). 

### The hardness result for the line is tight

We observe that the lower bound regarding the robustness and consistency trade-off shown in Theorem 1 is actually tight. Specifically, it can be achieved by an appropriate randomization between the optimal deterministic learning-augmented mechanism and the optimal non-learning-augmented randomized mechanism.

**Proposition 1**.: _For any \([0,0.5]\), there exists a randomized mechanism on the line which, given prediction \(F^{*}\), is truthful in expectation, \((1+)\)-consistent, and \((2-)\)-robust._

As a result, the bound of Theorem 1 precisely captures the optimal robustness consistency trade-off over all truthful in expectation mechanisms for instances on the line.

### Other prediction settings

To gain a more complete picture of different prediction settings, we study an alternative strong prediction that is not strictly stronger than the predicted optimal facility location \(F^{*}\). Specifically, we consider a setting where predictions are available for all pieces of information except for one of the extreme locations. We show that these predictions are not helpful, even without any robustness constraints, to improve the consistency guarantee alone.

**Theorem 2**.: _Given a prediction set that provides the identities of all \(n\) agents, there exist \(n-1\) agents such that, even if we have their exact locations, there is no deterministic truthful mechanism on the line that is better than \(2\)-consistent, and there is no randomized mechanism on the line that is truthful in expectation and better than \(1.5\)-consistent._

## 4 Results for the Plane

We now consider instances in the Euclidean plane, with a location profile \(= x_{1},,x_{n}\), where \(x_{i}^{2}\) for each agent \(i\). Missing proofs of this section can be found in Appendix B.

### Impossibility Results

Before considering the robustness and consistency guarantees achievable by randomized mechanisms augmented with different types of predictions, we start off by reducing the gap on what is known for mechanisms without predictions.

Since the problem of designing good facility location mechanisms in two dimensions is "harder" than the one-dimensional case, it may seem counterintuitive at first, but the lower bound that prior work proved for all one-dimensional instances does not extend to two dimensions. The reason is that the design space for two-dimensional mechanisms is much richer, and the known lower bounds apply only to the more restricted class of one-dimensional mechanisms. For example, consider a simple instance with just two agents in two dimensions. For this two-dimensional instance, the mechanism can return a location for the facility that is not on the line containing the two agents' locations, which it, of course, cannot do in one dimension.

Note that, in terms of social cost alone, returning such a location is always Pareto-dominated by returning an appropriate location on the line (e.g., its projection onto the line). However, returning a location that is not on this line also allows the designer to affect the incentives of the participating agents in new and non-trivial ways that are impossible in the one-dimensional case. Specifically, returning points that are not on the line allows us to induce previously impossible cost vectors. For a simple example, in a one-dimensional instance involving two agents at distance \(1\) from each other, the only facility location that yields the same cost to both agents is the midpoint between them, leading to a cost vector of \((0.5,0.5)\). However, in two dimensions, we can induce a cost vector of \((c,c)\) for any \(c 0.5\) by simply returning the appropriate point on the interval's perpendicular bisector.

This additional flexibility could potentially provide the designer with novel ways to ensure that the agents do not misreport their locations; for example, the classic technique of "money burning," used to achieve incentive compatibility without monetary payments, heavily depends on the designer's ability to penalize and reward agents based on their reports. Although we conjecture that truthful mechanisms are always better off returning a facility on the line containing the agents' locations in this instance, proving such a result appears non-trivial. Due to this additional flexibility in two dimensions, proving inapproximability results for this broader class of mechanisms is a more challenging problem and seems to require new techniques and insights.

**Theorem 3**.: _Any randomized mechanism that is truthful in expectation in the Euclidean metric space has an approximation ratio of at least \(1.118\)._

Our following two results provide impossibility results for both deterministic and randomized mechanisms augmented with perfect predictions. For deterministic mechanisms, Agrawal et al. (2022) showed that there is a mechanism that, given a prediction about the optimal facility location, is \(1\)-consistent and \((1+)\)-robust. Our impossibility result for deterministic mechanisms shows that stronger predictions do not help: even with predictions about the location of each agent, there is no deterministic mechanism that achieves a robustness better than \(1+\) and a consistency that improves over the best approximation achievable without predictions.

**Theorem 4**.: _For any \(>0\), there is no deterministic truthful mechanism that is \((2-)\)-consistent and \((1+-)\)-robust, even if it is provided with full predictions \(}\) containing each of the agents' locations._

The next result shows that there is no hope of achieving the best of both worlds in the randomized setting; to obtain \(1\) consistency, we would need to sacrifice robustness.

**Theorem 5**.: _For any \(>0\), there is no randomized mechanism that is truthful in expectation, \(1\)-consistent, and \((2-)\)-robust, even for two-agent instances and even if it is provided with full predictions \(}\) (the location of each agent). This is tight, i.e., there exists a randomized mechanism that is truthful in expectation, \(1\)-consistent, and \(2\)-robust for two-agent instances._

### Positive Results Using Extreme ID Prediction

We now turn to positive results, and provide a learning-augmented randomized mechanism that is provided with a new type of prediction: the prediction does not provide us with any actual location, but instead only provides us with the identities \(}= e_{1},,e_{k}\) of the \(k\) agents who would suffer the maximum cost in the optimal solution (we refer to them as "extreme agents"), i.e., \(\{e_{1},,e_{k}\}=_{e_{i}[n]}d(x_{e_{i}},o())\). Note that the smallest circle that encloses all the points is the _circumcircle_ of the locations of the extreme agents. Therefore, the center of this circle is \(o()\), the optimum solution.

We propose a mechanism leveraging predictions derived from IDs of extreme agents. The main idea is to return the centroid of the extreme agents, denoted by \(\) with a probability of half, and each of the extreme points with a probability of \(1/2k\) to prevent misreporting incentives.

Tang et al. (2020) run this mechanism over all agents (not only the extreme ones) and achieve a \(2-1/n\) approximation. We improve the approximation factor (in case of having good predictions) to \(2- 1.67\) by running this mechanism only on extreme agents. We use their ideas to maintain truthfulness and we use new techniques to show the approximation guarantees for an arbitrarily number of agents. Moreover, as long as any returned location falls within the minimum enclosing circle of the agents predicted to be extreme (which is the case for Mechanism 1), this ensures an approximation factor of \(2\) in case of having bad predictions.

First, we argue that \(k 2\), meaning that there are at least two extreme agents on the minimum enclosing circle. Otherwise, one can find a smaller circle containing all the points. As a warm-up, we first consider the \(k=2\) case and propose a randomized mechanism that is truthful in expectation and achieves \(1.5\) consistency and \(2\) robustness for any number of agents.

**Theorem 6**.: _Assume there are only two extreme agents, i.e., \(k=2\). Then, given predictions \(}= e_{1},e_{2}\) that provides the IDs of the only two extreme agents, there exists a randomized mechanism that is truthful in expectation and achieves \(1.5\) consistency and \(2\) robustness for any number of agents._

We then show that it is sufficient to only consider the case where we have exactly three extreme agents on the minimum enclosing circle, meaning \(k=3\). If we have more than three agents located on the minimum enclosing circle, a continuous perturbation of the points makes the probability of having at least four points lying on a circle infinitesimally small (Berg et al., 2008). For \(k=3\), we use the properties of the Euler line to prove the \(1.67\) consistency.

**Theorem 7**.: _Given a prediction set that provides the IDs of the extreme agents, there exists a randomized mechanism that is truthful in expectation and achieves \(1.67\) consistency and \(2\) robustness for any number of agents._

Proof.: Let us first consider instances with only three extreme agents. Mechanism 1 is truthful since other agents apart from \(x_{e_{1}},x_{e_{2}},x_{e_{k}}\) cannot influence the result and Mechanism 1 is equivalent to the mechanism of Tang et al. (2020) over agents \(e_{1},,e_{k}\) and since this mechanism is truthful in expectation, agents \(e_{1},,e_{k}\) cannot benefit from misreporting their locations. Since the mechanism returns the reported locations, rather than their predictions, and the centroid is guaranteed to be inside the circumcircle, we can conclude that the mechanism has a robustness factor of \(2\). The technical aspect of this proof involves establishing the consistency guarantee by considering the Euler line.

Euler line.Given three arbitrary points \(x_{1},,x_{3}\), their circumcircle is the smallest circle that encloses the three points, their centroid is \((x_{1}+x_{2}+x_{3})/3\), and their orthocenter is the point where the three altitudes (the perpendicular line segments from a vertex to the line that contains the opposite side) intersect. In any triangle, the center of the circumcircle (\(O\)), the centroid (\(\)), and the orthocenter (\(H\)) are collinear, forming the Euler line. One property of the Euler line is that \(\) is positioned midway between \(O\) and \(H\). Additionally, \(d(O,)=d(,H)/2\), implying \(d(O,)=d(O,H)/3\).

Let \(R\) denote the radius of the circumcircle of \(x_{e_{1}},x_{e_{2}},x_{e_{3}}\) and assume without loss of generality that the optimum cost is \(R=1\). For any \(j[n]\), we have

\[d(x_{j},) d(x_{j},O)+d(O,) R+d(O,)\]

where the first inequality is by triangle inequality and the second is because all the points are within the circumcircle of the locations of the extreme agents when the predictions are correct. Since \(H\) lies within the circumcircle, we infer \(d(O,H) R\), conclusively showing that \(d(O,) R/3\). Since \(R\) is the optimum cost and any agent can reach \(O\) by paying that cost, the cost of returning \(\) for any agent is upper bounded by \(R+d(O,)\), which is at most \(1.34R\). Consequently, the approximation of Mechanism 1 in case of having accurate predictions is less or equal than \[_{i=1}^{k}_{j[n]}d(x_{e_{i}},x_{j})+ _{j[n]}d(x_{j},) 1+)}{R} 1+(1+) 1.67.\]

where the first inequality is since \(d(x_{e_{i}},x_{j}) d(x_{e_{i}},O)+d(O,x_{j}) 2R\) when the predictions are correct. Next, we show how we can modify the mechanism to achieve the same approximation guarantees as having three extreme agents while maintaining truthfulness.

As mentioned previously, the key concept involves perturbing the instance before requesting predictions to have at most three extreme agents. Given any instance \(= x_{1},,x_{n}\), define a perturbed instance \(}=_{1},,_{n}\), which is the result of a continuous perturbation of \(\). Consider the set of predictions for the IDs of extreme agents in \(}\). Although the extreme agents of the perturbed instance \(}\) might differ from those of the original instance \(\), their costs remain very close to the maximum cost in case of having good predictions. Therefore, the circumcircle of them is a good representation of the minimum enclosing circle and results in the same approximation guarantees.

Since we run the mechanism on the main instance \(\), truthfulness holds as before. Note that since we ask for the ID of extreme agents of the perturbed instance \(}\), we have consistency guarantees if predictions are accurate based on the perturbed instance \(}\), and in any case we can ensure the robustness factor of \(2\). 

## 5 Future Directions

The problem of designing randomized facility location mechanisms in the Euclidean space is very natural and several open questions remain, even without predictions. While numerous studies have addressed this problem in restricted spaces, there remains a gap regarding between the best possible approximation guarantee in \((1.118,2-1/n)\). The possibility of a truthful in expectation mechanism achieving better than a \(2-1/n\) approximation is intriguing. Obtaining a stronger lower bound than \(1.118\) would also be very interesting.

Augmenting these mechanisms with predictions introduces a new dimension to the problem. While we have explored various prediction settings and provided both positive and negative results, many of the current findings in Euclidean space lack tightness. Finding tight results for different types of predictions would enable meaningful comparisons and help identify which prediction strategy is most effective in different scenarios.

Figure 1: \(=(x_{e_{1}}+x_{e_{2}}+x_{e_{3}})/3\) is the centroid of \(x_{e_{1}},,x_{e_{3}}\), which is the intersection of the three medians. \(H\) is the orthocenter, which is the intersection of the three altitudes, and \(O\) is the center of the circumcircle, which is the intersection of the three perpendicular bisectors. In any triangle, the circumcenter (\(O\)), the centroid (\(\)), and the orthocenter (\(H\)) are collinear, forming the Euler line. Moreover, \(d(O,)=d(,H)\).