# Exponentially Convergent Algorithms for

Supervised Matrix Factorization

Jowon Lee

Department of Statistics

University of Wisconsin - Madison, WI, USA

jlee2256@wisc.edu

&Hanbaek Lyu

Department of Mathematics

University of Wisconsin - Madison, WI, USA

hlyu@math.wisc.edu

&Weixin Yao

Department of Statistics

University of California, Riverside, CA, USA

weixiny@ucr.edu

###### Abstract

Supervised matrix factorization (SMF) is a classical machine learning method that simultaneously seeks feature extraction and classification tasks, which are not necessarily a priori aligned objectives. Our goal is to use SMF to learn low-rank latent factors that offer interpretable, data-reconstructive, and class-discriminative features, addressing challenges posed by high-dimensional data. Training SMF model involves solving a nonconvex and possibly constrained optimization with at least three blocks of parameters. Known algorithms are either heuristic or provide weak convergence guarantees for special cases. In this paper, we provide a novel framework that 'lifts' SMF as a low-rank matrix estimation problem in a combined factor space and propose an efficient algorithm that provably converges exponentially fast to a global minimizer of the objective with arbitrary initialization under mild assumptions. Our framework applies to a wide range of SMF-type problems for multi-class classification with auxiliary features. To showcase an application, we demonstrate that our algorithm successfully identified well-known cancer-associated gene groups for various cancers.

## 1 Introduction

In classical classification models, such as logistic regression, a conditional class-generating probability distribution is modeled as a simple function of the observed features with unknown parameters to be trained. However, the raw observed features may be high-dimensional, and most of them might be uninformative and hard to interpret (e.g., pixel values of an image). Therefore, it would be desirable to extract more informative and interpretable low-dimensional features prior to the classification task. For instance, the multi-layer perceptron or deep neural networks (DNN) in general [8; 9] use additional feature extraction layers prior to the logistic regression layer. This allows the model itself to learn the most effective (supervised) feature extraction mechanism and the association of the extracted features with class labels simultaneously.

Matrix factorization (MF) is a classical unsupervised feature extraction framework, which learns latent structures of complex datasets and is regularly applied in the analysis of text and images [13; 32; 44]. Various matrix factorization models such as singular value decomposition (SVD), principal component analysis (PCA), and nonnegative matrix factorization (NMF) provide fundamental tools for unsupervised feature extraction tasks [17; 55; 2; 25]. Extensive research has been conducted to adapt matrix factorization models to perform classification tasks by supervising the matrix factorizationprocess using additional class labels. Note that matrix factorization and classification are not necessarily aligned objectives, so some degree of trade-off is necessary when seeking to achieve both goals simultaneously. _Supervised matrix factorization_ (SMF) provides systematic approaches for such multi-objective tasks. Our goal is to use SMF to learn low-rank latent factors that offer interpretable, data-reconstructive, and class-discriminative features, addressing challenges posed by high-dimensional data. The general framework of SMF was introduced in . A similar SMF-type framework of discriminative K-SVD was proposed for face recognition . A stochastic formulation of SMF was proposed in . SMF has also found numerous applications in various other problem domains, including speech and emotion recognition , music genre classification , concurrent brain network inference , structure-aware clustering , and object recognition . Recently, supervised variants of NMF, as well as PCA, were proposed in [5; 26; 47]. See also the survey work of  on SMF.

Various SMF-type models have been proposed in the past two decades. We divide them into two categories depending on whether the extracted low-dimensional feature or the feature extraction mechanism itself is supervised. We refer to them as feature-based and filter-based SMF, respectively. Feature-based SMF models include the classical ones by Mairal et al. (see, e.g., [33; 30]) as well as the more recent model of convolutional matrix factorization by  for a contextual text recommendation system. Filter-based SMF models have been studied more recently in the supervised matrix factorization literature, most notably from supervised nonnegative matrix factorization [5; 26] and supervised PCA .

ContributionsIn spite of vast literature on SMF, due to the high non-convexity of the associated optimization problem (see (4)), algorithms for SMF mostly lack rigorous convergence analysis and there has not been any algorithm that provably converges to a global minimizer of the objective at an exponential rate. We summarize our contributions below.

* We formulate a general class of SMF-type models (including both the feature- and the filter-based ones) with high-dimensional features as well as low-dimensional auxiliary features (see (4)).
* We provide a novel framework that 'lifts' SMF as a low-rank matrix estimation problem in a combined factor space and propose an efficient algorithm that converges exponentially fast to a global minimizer of the objective with an arbitrary initialization (Theorem 3.5). We numerically validate our theoretical results (see Fig. 2).
* We theoretically compare the robustness of filter-based and feature-based SMF, establishing that the former is computationally more robust (see Theorem 3.5) while the latter is statistically more robust (see Theorem 4.1).
* Applying our method to microarray datasets for cancer classification, we show that not only it is competitive against benchmark methods, but it is able to identify groups of genes including well-known cancer-associated genes (see Fig. 3).

### Notations

Throughout this paper, we denote by \(^{p}\) the ambient space for data equipped with standard inner project \(,\) that induces the Euclidean norm \(\|\|\). We denote by \(\{0,1,,\}\) the space of class labels with \(+1\) classes. For a convex subset \(\) in an Euclidean space, we denote \(_{}\) the projection operator onto \(\). For an integer \(r 1\), we denote by \(_{r}\) the rank-\(r\) projection operator for matrices. For a matrix \(=(a_{ij})_{ij}^{m n}\), we denote its Frobenius, operator (2-), and supremum norm by \(\|\|_{F}^{2}:=_{i,j}a_{ij}^{2},\|\|_{2}:=_{ ^{n},\,\|\|=1}\|\|,\|\|_ {}:=_{i,j}|a_{ij}|\), respectively. For each \(1 i m\) and \(1 j n\), we denote \([i,:]\) and \([:,j]\) for the \(i\)th row and the \(j\)th column of \(\), respectively. For each integer \(n 1\), \(_{n}\) denotes the \(n n\) identity matrix. For square symmetric matrices \(,^{n n}\), we denote \(\) if \(^{T}^{T}\) for all unit vectors \(^{n}\). For two matrices \(\) and \(\), we denote \([,]\) and \([]\) the matrices obtained by concatenating (stacking) them by horizontally and vertically, respectively, assuming matching dimensions.

### Model setup

Suppose we are given with \(n\) labeled signals \((y_{i},_{i},_{i}^{})\) for \(i=1,,n\), where \(y_{i}\{0,1,,\}\) is the label, \(_{i}^{p}\) is a high-dimensional feature of \(i\), and \(_{i}^{}^{q}\) is a low-dimensional auxiliary feature of \(i\) (\(p q\)). For a vivid context, think of \(_{i}\) as the X-ray image of a patient \(i\) and \(_{i}^{}\) denotingsome biological measurements, such as gender, smoking status, and body mass index. When making predictions of \(y_{i}\), we use a suitable \(r\,( p)\) dimensional compression of the high-dimensional feature \(_{i}\) as well as the low-dimensional feature \(_{i}^{}\) as-is. We assume such compression is done by some matrix of _(latent) factors_\(=[_{1},,_{r}]^{p r}\) that is _reconstructive_ in the sense that the observed signals \(_{i}\) can be reconstructed as (or approximated by) the linear transform of the 'atoms' \(_{1},,_{r}^{p}\) for some suitable 'code' \(_{i}^{r}\). More concisely, \(_{}=[_{1},,_{n}]\), where \(=[_{1},,_{n}]^{r n}\). In practice, we can choose \(r\) to be the approximate rank of data matrix \(_{}\) (e.g., by finding the elbow of the scree plot).

Now, we state our probabilistic modeling assumption. Fix parameters \(^{p r}\), \(_{i}^{r}\), \(^{r}\), and \(^{q}\). Let \(h:[0,)\) be a _score function_ (e.g., \(h()=()\) for multinomial logistic regression). We assume \(y_{i}\) is a realization of a random variable whose conditional distribution is specified as

\[[(y_{i}=0\,|\,_{i},_{i}^{}), ,(y_{i}=\,|\,_{i},_{i}^{} )]=(_{i}):=C[1,h(_{i,1}),,h(_{i,})],\] (1)

where \(C\) is the normalization constant and \(_{i}=(_{i,1},,_{i,})^{}\) is the _activation_ for \(y_{i}\) defined in two ways, depending on whether we use a 'feature-based' or 'filter-based' SMF model:

\[_{i}=^{T}_{i}+ {}^{T}_{i}^{}&),\\ ^{T}^{T}_{i}+^{T} _{i}^{}&).\] (2)

One may regard \((,)\) as the'multinomial regression coefficients' with input feature \((_{i},_{i}^{})\) or \((^{T}_{i},_{i}^{})\). In (2), we may regard the code \(_{i}\) (coming from \(_{i}_{i}\)) or the 'filtered signal' \(^{T}_{i}\) as the \(r\)-dimensional compression of \(_{i}\). Note that these two coincide if we have perfect factorization \(_{i}=_{i}\) and the factor matrix \(\) are orthonormal, i.e., \(^{T}=_{r}\), but we do not necessarily make such an assumption.

There are some notable differences between SMF-\(\) and SMF-\(\) when predicting the unknown label of a test point. If we are given a test point \((_{},_{}^{})\), the predictive probabilities for its unknown label \(y_{}\) is given by (1) with activation \(\) computed as in (2). This only involves straightforward matrix multiplications for SMF-\(\), which can also be viewed as a forward propagation in a multilayer perceptron  with \(\) acting as the first layer weight matrix (hence named 'filter'). However, for SMF-\(\), one needs to solve additional optimization problems for testing. Namely, for every single test signal \((_{},_{}^{})\), its correct code representation \(_{}\) needs to be learned by solving the following'supervised sparse coding' problem (see ):

\[_{y\{0,1,,\}}_{}\,(y,^{T }+^{T}_{}^{})+\| _{}-\|_{F}^{2}.\] (3)

A more efficient heuristic testing method for SMF-\(\) is by approximately computing \(_{}\) by only minimizing the second term in (3).

In order to estimate the model parameters \((,,,)\) from observed training data \((_{i},y_{i})\) for \(i=1,,n\), we consider the following multi-objective optimization problem:

\[_{,,,}\ _{i=1}^{n}(y_{i},_{i})+\|_{}- \|_{F}^{2},\] (4)

Figure 1: Overall scheme of the proposed method for SMF-\(\).

[MISSING_PAGE_FAIL:4]

\(^{1 n}\):

\[_{,,}\|-^{T}\|_ {F}^{2}+\|_{}-\|_{F}^{2}.\] (8)

This is a three-block optimization problem involving three factors \(^{p r},^{r n}\) and \(^{r 1}\), which is nonconvex and computationally challenging to solve exactly. Instead, consider reformulating this nonconvex problem as the following matrix factorization problem:

\[_{,,}f(^{T}\\ ):=\|\\ _{}-^{T }\\ \|_{F}^{2}.\] (9)

Indeed, we now seek to find _two_ decoupled matrices (instead of three), one for \(^{T}\) and \(\) stacked vertically, and the other for \(\). A similar idea of matrix stacking was used in  for discriminative K-SVD. Proceeding one step further, another important observation we make is that it is also equivalent to finding a _single_ matrix \(:=[^{T} ]^{(1+p) n}\) of rank at most \(r\) that minimizes the function \(f\) in (9), which is convex (specifically, quadratic) in \(\): (See Fig. 1 Training).

For SMF-W, consider the following analogous linear regression model:

\[_{,,}f([, ]):=\|-^{T}^{T}_{}\| _{F}^{2}+\|_{}-\|_{F}^{2},\] (10)

where the right-hand side above is obtained by replacing \(\) with \(^{T}_{}\) in (9). Note that the objective function depends only on the product of the two matrices \(\) and \([,]\). Then, we may further lift it as the low-rank matrix estimation problem by seeking a single matrix \(:=[,\,]^{p (1+n)}\) of rank at most \(r\) that solves (6) with \(f\) being the function in (10).

### Algorithm

Motivated by the observation we made before, we rewrite SMF-\(\) in (4) as

\[_{[,]\\ () r}F(,\,):=_{i=1}^{n}(y_{i},[:,i]+^{T}_{ i}^{})+\|_{}-\|_{F}^{2}+(\| \|_{F}^{2}+\|\|_{F}^{2}),\] (11)

where \(=^{T}\), \(=\), \(=[]^{(+p) n}\), and \(\) is a convex subset of \(^{(+p) n}^{q}\). We have added a \(L_{2}\)-regularization term for \(\) and \(\) with coefficient \( 0\), which will play a crucial role in well-conditioning (11).

For solving (11), we propose to use the LGPD algorithm (7): _We iterate gradient descent followed by projecting onto the convex constraint set \(\) of the combined factor \([,]\) and then perform rank-\(r\) projection of the first factor \(=[]\) via truncated SVD until convergence._ Once we have a solution \([^{},^{}]\) to (11), we can use SVD of \(^{}\) to obtain a solution to (4). Let \(^{}=^{T}\) denote the SVD of \(\). Since \((^{}) r\), \(\) is an \(r r\) diagonal matrix of singular values of \(\). Then \(^{(+p) r}\) and \(^{n r}\) are semi-orthonormal matrices, that is, \(^{T}=^{T}=_{r}\). Then since \(^{}=[(^{})^{T}^{}] ^{}\), we can take \(^{}=^{1/2}^{T}\) and \([(^{})^{T}^{}]=^{1/2}\).

We summarize this approach of solving (4) for SMF-\(\) in Algorithm 1. Here, SVD\({}_{r}\) denotes rank-\(r\) truncated SVD and the projection operators \(_{}\) and \(_{r}\) are defined in Subsection 1.1.

As for SMF-\(\), we can rewrite (4) with additional \(L_{2}\)-regularizer for \(=\) and \(\) as

\[_{[,]\\ () r}F(,\,)=_{i=1}^{n}(y_{i},^{T}_{i}+^{T} _{i}^{})+\|_{}-\|_{F}^{2}+ (\|\|_{F}^{2}+\|\|_{F}^{2}),\] (12)

where \(=[,]=[,] ^{p(+n)}\) and \(^{p(+n)}^{q}\) is a convex set. Algorithm 1 for SMF-\(\) follows similar reasoning as before with the reformulation above.

By using randomized truncated SVD for the efficient low-rank projection in Algorithm 1, the per-iteration complexity is \(O(pn(n,p))\), while that for the nonconvex algorithm is \(O((pr+q)n)\). While the LPGD algorithm is in general more expensive per iteration than the nonconvex method, the iteration complexity is only \(O(^{-1})\) thanks to the exponential convergence to the global optimum (will be discussed in Theorem 3.5). To our best knowledge, the nonconvex algorithm for SMF does not have any guarantee to converge to a global optimum, and the iteration complexity of the nonconvex SMF method to reach an \(\)-stationary point is at best \(O(^{-1})\) using standard analysis. Hence for \(\) small enough, Algorithm 1 achieves an \(\)-accurate global optimum for SMF with a total computational cost comparable to a nonconvex SMF algorithm to achieve an \(\)-stationary point.

## 3 Global convergence guarantee

We have discussed that one can cast the SMF problem (4) as the following 'factored estimation problem' \(_{,,}f(^{T}, )\). Note that such problems generally do not have a unique minimizer due to the 'rotation invariance'. Namely, let \(\) be any \(r r\) orthonormal (rotation) matrix (i.e., \(^{T}=^{T}=_{r}\)). Then \(f(()()^{T},)=f( ^{T}^{T},)=f( ^{T},)\). Hence the best one is to obtain parameters up to rotation that globally minimize the objective value. Our main result, Theorem 3.5, establishes that this can be achieved by Algorithm 1 at an exponential rate. First, we introduce the following technical assumptions (3.1-3.3).

**Assumption 3.1**.: (Bounded activation) The activation \(^{}\) defined in (2) assumes bounded norm, i.e., \(\|\| M\) for some constant \(M(0,)\).

**Assumption 3.2**.: (Bounded eigenvalues of covariance matrix) Denote \(=[_{1},,_{n}]^{(p+q) n}\), where \(_{i}=[_{i}_{i}^{}] ^{p+q}\) (so \(=[_{}_{}]\)), where \(_{}=[_{1}^{},,_{n}^{}]\). Then, there exist constants \(^{-},^{+}>0\) such that for all \(n 1\),

\[^{-}_{}(n^{-1}^{T})_ {}(n^{-1}^{T})^{+}.\] (13)

**Assumption 3.3**.: (Bounded stiffness and eigenvalues of observed information) The score function \(h:[0,)\) is twice continuously differentiable. Further, let observed information \(}(y,):=_{}_{^{T}} (y,)\) for \(y\) and \(\). Then, for the constant \(M>0\) in Assumption 3.1, there are constants \(_{},^{-},^{+}>0\) s.t. \(_{}:=_{\|\| M}_{1 s n}\|_{ }(y_{s},)\|_{}\) and

\[^{-}:=_{\|\| M}_{1 s n}_{}( }(y_{s},)),^{+}:=_{\|\|  M}_{1 s n}_{}(}(y_{s}, )).\] (14)

Assumption 3.1 limits the norm of the activation \(\) as an input for the classification model in (4) is bounded. This is standard in the literature (see, e.g., [36; 60; 23]) in order to uniformly bound the eigenvalues of the Hessian of the (multimomial) logistic regression model. Assumption 3.2 introduces uniform bounds on the eigenvalues of the covariance matrix. Assumption 3.3 introduces uniform bounds on the eigenvalues of the \(\) observed information as well as the first derivative of the predictive probability distribution (see  and Sec. D in Appendix for more details). Under Assumption 3.1 and the multinomial logistic regression model \(h()=()\), one can derive Assumption 3.3 with a simple expression for the bounds \(^{}\), as discussed in the following remark.

**Remark 3.4** (Multinomial Logistic Classifier).: Let \(\) denote the negative log-likelihood function in (5), where we take the multinomial logistic model with the score function \(h()=()\). Denote \((_{1},,_{}):=_{}(y,)\) and \(}(y,):=_{}_{^{T}} (y,)\). Then in this special case, we have \(_{j}(y,)=g_{j}()-(y=j)\) and \((y,)_{i,j}=g_{i}()((i=j)-g_{j}( ))\) (See (28) and (30) in Appendix). Under Assumption 3.1, according to Lemma B.1, we can take \(_{}=1+}{1+e^{M}+(-1)e^{-M}} 2\), \(^{-}=}{1+e^{-M}+(-1)e^{M}},\) and \(^{+}=(1+2(-1)e^{M})}{(1+e^{M}+(-1)e^{- M})^{2}}\). For binary classification, \(^{+} 1/4\).

Now define the following quantities:

\[:=(2,\,2+n^{-}^{-})&,\ L:= (2,\,2+n^{+}^{+})&\\ (2,\,2)&.\] (15)

Now, we state a special case of our first main result, specifically when the model is 'correctly specified', allowing the rank-\(r\) SMF model to effectively account for the observed data. This implies the existence of a 'low-rank stationary point' of \(F\), as also demonstrated in . However, we also handle the general case in Appendix (see Theorem D.1).

**Theorem 3.5**.: _(Exponential convergence) Let \(_{t}:=[_{t},_{t}]\) denote the iterates of Algorithm 1. Assume 3.1-3.3 hold. Let \(\) and \(L\) be as in (15), fix \((,)\), and let \(:=2(1-)(0,1)\). Suppose \(L/<3\) and let \(^{*}=[^{*},^{*}]\) be any stationary point of \(F\) over \(\) s.t. \((^{*}) r\). Then \(^{*}\) is the unique global minimizer of \(F\) among all \(=[,]\) with \(() r\). Moreover, \(\|_{t}-^{*}\|_{F}^{t}\,\|_{0}-^{*}\|_{F}\) for \(t 1\)._

In the statement above, we write \(\|\|_{F}^{2}=\|,]\|_{F}^{2}: =\|\|_{F}^{2}+\|\|_{F}^{2}\). Note that we may view the ratio \(L/\) that appears in Theorem 3.5 as the condition number of the SMF problem in (4), whereas the ratio \(L^{*}/^{*}\) for \(^{*}:=^{-}^{-}\) and \(L^{*}:=^{+}^{+}\) as the condition number for the multinomial classification problem. These two condition numbers are closely related. First, note that for any given \(^{*},\,L^{*}\) and sample size \(n\), we can always make \(L/<3\) by choosing sufficiently large \(\) and \(\) so that Theorem 3.5 holds. However, using large \(L_{2}\)-regularization parameter \(\) may perturb the original objective in (4) too much that the converged solution may not be close to the optimal solution. Hence, we may want to take \(\) as small as possible. Setting \(=0\) leads to

\[<3,\,=00<}{^{*}}<3,\, {L^{*}}{6}<<}{2}&\\ )}{(2,\,0)}&<\ 3&.\] (16)

For SMF-\(\), if the multinomial classification problem is well-conditioned (\(L^{*}/^{*}<3\)) and the ratio \(/n\) is in the above interval, then SMF-\(\) enjoys exponential convergence in Theorem 3.5. However, the condition for SMF-\(\) in (16) is violated, so \(L_{2}\)-regularization is necessary for guaranteeing exponential convergence of SMF-\(\).

The proof of Theorem 3.5 involves two steps: (1) We establish a general exponential convergence result for the general LPGD algorithm (7) in Theorem C.2 in Appendix. (2) We compute the Hessian eigenvalues of the SMF objectives (11)-(12) and apply the result to obtain Theorem 3.5. The proof contains two challenges: first, the low-rank projection in (7) is not non-expansive in general. To overcome this, we show that the iterates closely approximate certain 'auxiliary iterates' which exhibit exponential convergence towards the global optimum. Secondly, the second-order analysis is highly non-trivial since the SMF problem (4) has a total of four unknown matrix factors that are intertwined through the joint multi-class classification and matrix factorization tasks. See Appendix D for the details.

## 4 Statistical estimation guarantee

In this section, we formulate a generative model for SMF (4) and state statistical parameter estimation guarantee. Fix dimensions \(p q\), and let \(n 1\) be possibly growing sample size, and fix unknown true parameters \(^{*}^{p n},\ ^{*}^{q n},\ ^{*}^{q}\). In addition, fix \(^{*}^{ n}\) for SMF-\(\) and \(^{}^{p}\) for SMF-\(\). Now suppose that class label, data, and auxiliary features are drawn i.i.d. according to the following joint distribution:

\[&_{i}=^{*}[:,i]+_{i}, _{i}^{}=^{*}[:,i]+_{i}^{ },\\ &y_{i}\,|\,_{i},_{i}^{}1,\,\,(_{i})\,,\\ &_{i}=^{*}[:,i]+(^{*})^{T} _{i}^{}&,\\ (^{*})^{T}_{i}+(^{*})^{T}_{i}^ {}&,&([^ {*}\,\|\,^{*}]) r&,\\ ([^{*},^{*}]) r&. \] (17)

where each \(_{i}\) (resp., \(_{i}^{}\)) are \(p 1\) (resp., \(q 1\)) vector of i.i.d. mean zero Gaussian entries with variance \(^{2}\) (resp., \((^{})^{2}\)). We call the above the _generative SMF model_. In what follows, we will assume that the noise levels \(\) and \(^{}\) are known and focus on estimating the four-parameter matrices.

The (\(L_{2}\)-regularized) negative log-likelihood of observing triples \((y_{i},_{i},_{i}^{})\) for \(i=1,,n\) is given as \(_{n}:=F(,,)+)^{2}}\|_{}-\|_{F}^{2}+c\), where \(c\) is a constant and \(F\) is as in (11) or (12) depending on the activation type with tuning parameter \(=}\). The \(L_{2}\) regularizer in \(F\) can be understood as Gaussian prior for the parameters and interpreting the right-hand side above as the negative logarithm of the posterior distribution function (up to a constant) in a Bayesian framework. Note that the problem of estimating \(\) and \(\) are coupled due to the low-rank model assumption in (17), while the problem of estimating \(\) is standard and separable, so it is not of our interest. The joint estimation problem for \([,,]\) is equivalent to the corresponding SMF problem (4) with tuning parameter \(=(2^{2})^{-1}\). This and Theorem 3.5 motivate us to estimate the true parameters \(^{}\), \(^{}\), and \(^{}\) by the output of Algorithm 1 with \(=(2^{2})^{-1}\) for \(O( n)\) iterations.

Now we give the second main result. Roughly speaking, it states that the estimated parameter \(_{t}\) is within the true parameter \(^{}=[^{},^{},^{}]\) within \(O( n/)\) with high probability, provided that the noise variance \(^{2}\) is small enough and the SMF objective (11)-(12) is well-conditioned.

**Theorem 4.1**.: _(Statistical estimation for SMF) Assume the model (17) with fixed \(\). Suppose Assumptions 3.1-3.3 hold. Let \(,L\) be as in (15), \(:=2(1-)\) and \(c=O(1)\) if \(^{}-_{}F(^{})\) and \(c=O()\) otherwise. Let \(_{t}\) denote the iterates of Algorithm 1 with the tuning parameter \(=(2^{2})^{-1}\), \(L_{2}\)-regularization parameter \(>0\), and stepsize \((,)\). Then following holds with probability at least \(1-\): For all \(t 1\) and \(n 1\), \(\|_{t}-^{}\|_{F}-^{t}\,\|_{0}-^{}\|_{F} c n+)}{}\), provided \(L/<3\). Furthermore, \(c n+)}{}\) is \(O( n/)\) if \(^{}-_{}F(^{})\) and \(^{2}=O(1/n)\)._

We remark that Theorem 4.1 implies that _SMF_-\(\)_is statistically more robust than SMF_-\(\). Namely, in order to have an arbitrarily accurate estimate with high probability, one needs to have \( n\). Combining with the expression in (15) and the well-conditioning assumption \(L/<3\), one needs to require \(=(n)\), hence small noise variance \(^{2}=O(1/n)\) for SMF-\(\). However, for SMF-\(\), this is guaranteed whenever \(^{2}=o(1/( n))\) and \(\).

## 5 Simulation and Numerical Validation

We numerically verify Theorem 3.5 on a semi-synthetic dataset generated by using MNIST image dataset  (\(p=28^{2}=784\), \(q=0\), \(n=500\), \(=1\)) and a text dataset named 'Real / Fake Job Posting Prediction'  (\(p=2840,q=72,n=17880,=1\)). Details about these datasets are in Sec. G in Appendix.1 We used Algorithm 1 with rank \(r=2\) for MNIST and \(r=20\) for job postings datasets. For all experiments, \(=2\) and stepsize \(=0.01\) were used.

We validate the theoretical exponential convergence results of our LPGD algorithm (Algorithm 1) in Figure 2. Note that the convexity and smoothness parameters \(\) and \(L\) in Theorem 3.5 are difficult to compute exactly. In practice, cross-validation of hyperparameters is usually employed. For

Figure 2: **(a-b)** Training loss vs. elapsed CPU time for Algorithm 1 (with binary logistic classifier) on the semi-synthetic MNIST and Job postings datasets for several values of \(\) in log scale. Average training loss over ten runs and the shades representing the standard deviation shown. **(c)** Comparison between LPGD (Algorithm 1) and BCD algorithms for SMF.

\(\{0.1,1,5,10,20\}\) in Figure 2, we indeed observe exponential decay of training loss as dictated by our theoretical results for Algorithm 1. We also observe that the exponential rate of decay in training loss increases as \(\) increases. According to Theorem 3.5, the contraction coefficient is \(=(1-)\), which decreases in \(\) since \(\) increases in \(\) (see (15)). The decay for large \(\{10,20\}\) seems even superexponential. Furthermore, \(2\) shows that our LPGD algorithm converges significantly faster than BCD for training both SMF-\(\) and SMF-\(\) models at \(\{5,10\}\) (other values of \(\) omitted).

## 6 Application: Microarray Analysis for Cancer Classification

We apply the proposed methods to two datasets from the Curated Microarray Database (CuMiDa) . CuMiDa provides well-preprocessed microarray data for various cancer types for various machine-learning approaches. One consists of 54,676 gene expressions from 51 subjects with binary labels indicating pancreatic cancer; Another we use has 35,982 gene expressions from 289 subjects with binary labels indicating breast cancer. The primary purpose of the analysis is to classify cancer patients solely based on their gene expression. We compare the accuracies of the proposed methods - SMF-\(\) and SMF-\(\) with a binary logistic classifier trained using Algorithm 1 - against the following benchmark algorithms: SMF-\(\) and SMF-\(\) trained using BCD; 1-dimensional seven-layer Convolutional Neural Networks (CNN); three-layer Feed-Forward Neural Networks (FFNN); Naive Bayes (NB); Support Vector Machine (SVM); Random Forest (RF); Logistic Regression with Matrix Factorization by truncated SVD (MF-LR). For the last benchmark method MF-LR, we use rank-\(r\) SVD to factorize \(_{}^{T}\) and take \(=\) and \(=^{T}\). For testing, we use \(^{T}_{}\) as input to logistic regression for both filter and feature methods since \(\|_{}-_{}\|_{F}\) is minimized when \(_{}=(^{T})^{-1}^{T}_{}=^{T}_{}\) with orthogonal \(\).

We normalize gene expression for stable matrix factorization and interpretability of regression coefficients. We split each data into 50% of the training set and 50% of the test set and repeat the comparison procedure 5 times. A scree plot is used to determine the rank \(r\). Other parameters are chosen through 5-fold cross-validation (\(\{0.1,1,10\}\) and \(\{0.1,1,10\}\)), and the algorithms are repeated in 1,000 iterations or until convergence. As can be seen in the table in Figure 3**a**, the proposed methods show the best performance for both types of cancers.

Figure 3: (**a-b**) Two selected supervised/unsupervised principal gene groups (low-dimensional compression of genes) learned by rank-16 SMF-\(\)/SVD and their associated logistic regression coefficients for breast cancer detection. (**c-d**) Similar to **a-b** learned by rank-2 SMF-\(\)/SVD for pancreatic cancer detection. (**e**) Blue-circled genes within each gene group of extreme coefficients coincide with known prognostic markers (for pancreatic cancer) and oncogene (for breast cancer). (**f**) Average classification accuracies and their standard deviations (in parenthesis) for various methods on two cancer microarray datasets over five-fold cross-validation. The highest-performing instances are marked in bold.

An important advantage of SMF methods is that they provide interpretable results in the form of'supervised factors'. Each supervised factor consists of a latent factor and the associated regression coefficient. That is, once we train the SMF model (for \(=1\)) and learn factor matrix \(=[_{1},,_{r}]^{p r}\) and vector of regression coefficients \(=[_{1},,_{r}]^{1 r}\), each column \(_{j}\) of \(\) describes a latent factor and the corresponding regression coefficient \(_{j}\) tells us how \(_{j}\) is associated with class labels. The pairs \((_{j},_{j})\), which form supervised latent factors, provide insights into how the trained SMF model perceives the classification task. See Fig. 1 for illustration.

In the context of microarray analysis for cancer research, each \(_{j}\) corresponds to a weighted group of genes (which we call a 'principal gene group') and \(_{j}\) represents the strength of its association with cancer. SMF learns supervised gene groups (Fig. 2(a), **c**) with significantly higher classification accuracy than the unsupervised gene groups (Fig. 2(b), **d**). In Fig. 2(a), **c**, both gene groups (consisting of \(p\) genes) have positive regression coefficients, so they are positively associated with the log odds of the predictive probability of the corresponding cancer. Remarkably, our method detected the well-known oncogene BRCA1 of breast cancer and other various genes (in Fig. 2(e)) that are known to be prognostic markers of breast/pancreatic cancer (see Human Protein Atlas ) in these groups of extreme coefficients (top five). The high classification accuracy suggests that the identified supervised principal gene groups may be associated with the occurrence of breast/pancreatic cancer.

## 7 Conclusion and Limitations

We propose an exponentially convergent algorithm for nonconvex SMF training using new lifting techniques. Our analysis demonstrates strong convergence and estimation guarantee. We compare the robustness of filter-based and feature-based SMF, finding that the former is computationally more robust while the latter is statistically more robust. The algorithm's exponential convergence is numerically verified. In cancer classification using microarray data analysis, our algorithm successfully identifies discriminative gene groups for various cancers and shows potential for identifying important gene groups as protein complexes or pathways in biomedical research. Our analysis framework can be extended to more complex classification models, such as combining a feed-forward deep neural network with a matrix factorization objective. While our convergence analysis holds in certain parameter regimes, we discuss them in detail. We have tested our method and convergence analysis on various real-world datasets but recommend further numerical verification on a wider range of datasets.