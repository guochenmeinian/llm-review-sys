# Understanding Information Storage and Transfer in Multi-modal Large Language Models

Samyadeep Basu

University of Maryland

&Martin Grayson

Microsoft Research

 Cecily Morrison

Microsoft Research

&Besmira Nushi

Microsoft Research

&Soheil Feizi

University of Maryland

&Daniela Massiceti

Microsoft Research

###### Abstract

Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models - how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by _the director in this photo_ has won a _Golden Globe_?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) _VQA-Constraints_, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit, a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks.

## 1 Introduction

Multi-modal Large Language Models (MLLMs) trained on both text and images are rapidly moving from research into deployment and are being used by millions of people. Yet, while there have been some advances in understanding how Large Language Models (LLMs) work, much less has been done to understand MLLMs. This paper begins to close this gap by studying how information is stored and transferred in MLLMs. To do this through the lens of a factual Visual Question Answering (VQA) task - a very common use case of MLLMs today .

MLLMs process factual information in two steps: information storage and information transfer. Information storage refers to how facts from a pre-training dataset are stored in a model's parameters - its so-called 'parametric' memory. Information transfer describes how information from input prompts are propagated through these storage locations to the model's final output. Understandingthese mechanisms can have many benefits, including ensuring models are factually grounded and informing better evaluation protocols.

Extensive works have explored how LLMs store and transfer factual information , however, this has not been studied for multi-modal inputs. For example, it is suggested that auto-regressive Transformer-based LLMs store factual information in their mid-layer MLP parameters . However, MLLMs and LLMs are different: an MLLM involves an additional (continuous) image input, alongside a (discrete) text prompt, and requires additional modules to process it . Typically, a vision encoder (e.g. CLIP) is used to convert this image into either visual tokens via a projection layer  or cross-attention layers  which are then integrated into the language encoder. These differences suggest that our existing understanding of LLM information storage and transfer may not map directly to MLLMs.

In this work, we use a factual VQA task to study the mechanisms of multi-modal information storage and transfer. We use a constraint-based formulation which views a visual question as having a set of either visual or textual constraints (e.g. What movie directed by _this director_ has won a _Golden Globe_?). The information retrieved by the model should satisfy these constraints (e.g. _this director_, _Golden Globe_) for the answer to be factually correct. This formulation, therefore, offers a systematic way of understanding a model's behavior. Under this framework, we propose Multi-ModalCausalTrace, which extends LLM causal tracing  to the multi-modal setting, to understand information storage, as well as leverage attention contribution methodologies  to study information transfer in MLLMs. We also introduce _VQA-Constraints_, a new dataset of 9.7k factual questions annotated with constraints, spanning natural images (from OK-VQA , WikiMovies , and Known ). With these tools, we study how a widely-used MLLM family processes multi-modal information, specifically LLaVa  and multi-modal Phi-2 1.

Our key findings are that MLLMs, in contrast to LLMs: 1) retrieve information from earlier MLP layers (i.e. layers 1-4 vs layers 4-7 in a LLM) (see Fig. 1); and 2) use less parametric memory (require a smaller window size to retrieve this information), when answering a multi-modal question. We also find that information is transferred from a given image to these early MLP blocks through a consistent subset of visual tokens (e.g. last \(\)36 tokens from LLaVa's CLIP encoder), and that the self-attention blocks in the middle layers are primarily responsible for shuttling this information to the last token.

Finally, we demonstrate that by editing these early causal MLPs, we can correct errors and insert new factual information into an MLLM. Specifically, we propose a new model-editing algorithm, MultEdit, which modifies the projection matrix of the early causal MLPs with a closed-form update. We empirically show MultEdit's effectiveness on questions from _VQA-Constraints_ and Encyclopedic VQA .

In summary, our contributions are:

Figure 1: **MLLMs retrieve information from earlier internal layers compared to their LLM counterparts.** We find that very early MLP layers [1–4] have high indirect estimation effects to outputs (i.e., they are causal) in LLaVa-7B, whereas the middle MLP layers [4–7] are causal in LLaMA (Vicuna)-7B. For LLaMA, a larger window size (e.g., 5) is also required to find causal sites, compared to a window size of 1 for LLaVA-7B.

1. A novel multi-modal causal tracing methodology that can be used to study information storage from image and text inputs in MLLMs.
2. A new dataset, _VQA-Constraints_, of 9.7K factual visual questions about natural images annotated with constraints to support future research in these directions.
3. A suite of novel insights on the mechanisms underlying multi-modal information storage and retrieval in MLLMs.
4. A model-editing method, MultEdit, which we demonstrate can precisely correct erroneous information and insert new long-tailed information in an MLLM.

## 2 Related Works

**Multimodal Large Language Models.** We consider a MLLM to be a model that takes an image and text as input, and generates a text output . Over the last year, such models have made tremendous advances in tasks like VQA and image captioning, including BLIP , BLIP-2 , Instruct-BLIP , LLaVA [18; 19], Flamingo  and multi-modal Phi-2 (from the Bunny repo) . These MLLMs can broadly be categorized into two families based on how their visual information is integrated into the language model: (i) by embedding the vision encoder's output into each layer of the language model with a cross-attention layer (e.g., Flamingo, BLIP) or, (ii) by mapping the vision encoder's output into "visual tokens" in the language model's input space (i.e. alongside the text tokens) via a projection layer (e.g., LLaVA, Bunny). Both families are widely used, however, the projection layer family has recently shown stronger performance on popular benchmark [18; 19; 11]. We, therefore, focus our study of information storage and transfer on this model family.

**Interpretability of MLLMs.** A well-established arm of model interpretability examines the relationship between a model's performance and its internals. A range of recent works have studied the internal mechanisms of information storage [23; 34; 24] and transfer [9; 37] in LLMs. However, to the best of our knowledge, only a few works [30; 31] have studied the interpretability of MLLMs, with none specifically investigating the relationship between a model's outputs and its internal states. , for example, designs an interactive interface to visualize the attention maps in an MLLM, while  explores the shortcomings of the CLIP vision encoder in MLLMs. Neither consider the influence of both vision _and_ text inputs on model internals or offer causal insights, as our work does. Our model editing approach which targets the projection layer MLLM family, is complemented by , who propose baselines for inserting information into the cross-attention layer MLLM family.

## 3 A Constraint-Based Framework for Studying Information Storage and Transfer in MLLMs

In this section, we describe the constraint-based formulation we use to study information storage and transfer in MLLMs. Under this framing, we introduce MultiModalCausalTrace, a novel causal information tracing technique which we use to study multi-modal information storage. We also describe how we use attention contributions  to study multi-modal information transfer. Finally, we describe _VQA-Constraints_, a new test-bed of visual questions annotated with constraints.

Figure 2: **We introduce MultiModalCausalTrace, a causal tracing method to understand information storage in MLLMs. A clean model is corrupted by replacing the question’s constraint with an incorrect one for the given image (e.g. “This place” -> “Paris city” for an image of “Vinson Massif”). The activations of windows of layers are then iteratively copied from the clean to the corrupted model until the corrupted model restores its output probability to match the clean model’s.**

### A Multi-modal Constraint-based Framework

Prior works have used a constraint satisfaction framework to study LLMs [37; 20; 14] as they offer a systematic way of studying model behavior. This framing defines a constraint as a set of words in the question. The model must retrieve information that satisfies this constraint from its parametric memory in order to generate the correct answer. For example, for the question "What city is the _Space Needle_ in?", the model must retrieve information relevant to the constraint "Space Needle". In the multi-modal setting, we consider these textual constraints as well as introduce visual constraints. We define a visual constraint to be a set of words in the question which refers to an entity in the image. The model must similarly retrieve information about this entity to generate the correct answer. For example, given an image of Christopher Nolan and the question "Name a movie directed by _this director_ in _2006_?", the visual constraint is "_this director_" (and the text constraint is "_2006_"). We refer to a question involving both a visual and text constraint as a multi-constraint question, and ones with only a visual constraint as a single-constraint question.

We use this framework to study the widely-used "projection layer" MLLMs family. These models are composed of a visual encoder \(f_{}\), a large language model \(g_{}\) and a projection head \(p_{}\). The projection head \(p_{}\) is responsible for mapping the output of the visual encoder into the input space of the language model, as so-called "visual tokens". Given an image-text pair denoted as (**x**, **y**), the language model processes them as \(g_{}(p_{}(f_{}()),h(t(y)))\), where \(p_{}(f_{}())=\{_{i}\}_{i=1}^{N}\) is the set of visual token embeddings and \(t(y)=\{t_{i}\}_{i=1}^{N}\) is the tokenized text inputs for the language model. These text tokens are processed by an embedding layer \(h\) to obtain text token embeddings as \(\{e_{i}\}_{i=1}^{M}\ ^{d}\). Because we are interested in studying the outputs of specific layers in response to specific tokens, we use \(g_{}(.)_{k,}\) to denote the output layer embedding corresponding to the kth token position and the layer \(\). We denote the output of a MLP layer as \(g_{}(.)_{k,_{}}\) and the output of a self-attention block as \(g_{}(.)_{k,_{}}\).

### MultiModalCausalTrace: Studying Information Storage in MLLMs

Causal tracing, derived from the causality literature , can be combined with a constraint-based framework to gain causal insights on how information propagates through a model with respect to specific constraint tokens. This has been used to identify where information is stored in LLMs [23; 24] and text-to-image generative models . The central idea of causal tracing is to corrupt a clean model by perturbing the input prompt. The activations of a small subset of layers are then iteratively copied

Figure 3: **Information to answer a visual question with a single constraint is mainly retrieved from early MLP and self-attention layers in MLLMs. MultiModalCausalTrace obtains high _indirect estimation effect_ values in LLaVa’s early MLP and self-attention blocks corresponding to the visual constraint, across all 3 datasets in _VQA-Constraints_. This suggests these layers are causally important for information storage. The causal traces emerge with a window size of 3 (see results with a window size of 1 in Appendix C).**from the clean model to the corrupted model, until the corrupted model restores its output probability to match the clean model's. In this way, we can identify which layers are used to retrieve information relevant to the constraints in the prompt (i.e. causally relate the input to the output).

In LLMs, the model is corrupted by adding a small amount of Gaussian noise to the embeddings of the textual constraint tokens (usually less than 5) . In MLLMs, however, noise must be added to a much larger number of token embeddings - those of the visual tokens (e.g., 576 in LLaVa and 729 in multi-modal Phi-2) from the projection head, _and_ the textual tokens of the visual constraint (e.g. "_this director_"). Our experiments show that this large noise injection makes it difficult to revert the MLLM to a clean state and recover relevant causal traces (see Fig. 7).

We, therefore, introduce MultiModalCausalTrace (see Fig. 2) to address this. Rather than adding Gaussian noise to the embeddings, we instead corrupt the visual constraint token IDs by replacing them with token IDs from a separate word or phrase, such that the visual information is ignored. We illustrate this through an example. **(1) Clean Model**: Given an image **x** of the Space Needle, and the question \(y\), "Which city is _this building_ located in?", we compute the probability of the model's output \(O\) (e.g., "Seattle") as \(_{clean}(O)\). **(2) Corrupted Model:** We substitute the visual constraint with an alternative such that the question does not require information from the image to be answered (e.g., "this building" is replaced with "Taj Mahal"). For a multi-constraint question which also has a textual constraint, we can either add Gaussian noise to the textual constraint tokens' embeddings (since there are only a few) or we can similarly replace the token IDs. After all replacements, we ensure that the question still makes semantic sense. We then measure the probability of original output \(O\) as \(_{corr}(O)\). With the right corruption, \(_{corr}(O)\) is expected to be low. **(3) Restored Model:** We then iteratively copy layer activations \(g_{}(.)_{k,_{mip}}\) and \(g_{}(.)_{k,_{attn}}\  k[1,M+N]\) from the clean model to the corrupted model, for each layer in turn. For a given layer \(\) and token position \(k\), we denote the restored probability as \(_{restored}(O)_{k,}\). After a layer is copied, we observe if \(_{restored}(O)_{k,}\) is high - indicating that layer \(\) has a strong causal association to the output \(O\). In some cases, no layers have a causal association. We note that this copy operation can be performed over a window of layers \(\{_{i}\}_{i=1}^{W}\) at a time, where \(W\) is the window size. A window size of 1 copies only one layer at a time. Similar to , we track the _indirect estimation effect_ for a layer \(\) as \(_{restored}(O)_{k,}-_{corr}(O)\) and use it as a metric to track causal states. Intuitively, this measures the difference in the probability of \(O\) under the corrupted model and when a layer \(\) is restored to its original clean state. A high value indicates that the copied layer/s can restore the model to its original clean state (i.e. the layer is causal).

### Studying Information Transfer in MLLMs with Attention Contributions

MultiModalCausalTrace enables us to identify the specific layers a model retrieves information from in order to answering a visual question. A second component of understanding how MLLMs process factual information is understanding how input prompts are propagated through these storage locations to the model's final output. For this, we use attention contributions  which compute how much one set of input tokens influences a set of output tokens during the self-attention operation.

Figure 4: **Information to answer a visual question with a visual and textual constraint is retrieved from early _and_ middle MLP and self-attention layers in MLLMs. This suggests that meeting multiple constraint requires more parametric memory compared to single constraints. We show that MultiModalCausalTrace obtains high indirect estimation effect values in the early and middle layers in LLaVa’s on the OK-VQA dataset in _VQA-Constraints_ (see multi-constraint results from the Movies dataset in Appendix F).**

Specifically, we use this to track (i) how information is transferred from the visual tokens to the causal layers, and (ii) from these layers to the final token, where the final probabilities are computed.

**Defining Attention Contributions.** The attention operation in a Transformer  consists of the query, value, key and output weight matrices: \(W_{q}^{},W_{q}^{},W_{k}^{},W_{o}^{}\). Each of these are divided into \(H\) heads as \(W_{q}^{,h},W_{v}^{,h},W_{k}^{,h}^{d d_{h}},W_{o} ^{,h}^{d_{h} d}\), where \(d\) is the dimension of the internal token embeddings and \(d_{h}\) is the dimensionality of the token embedding for a particular attention head \(h\). We define the attention contribution from a token \(j\) to token \(i\) in layer \(\) as follows:

\[a_{ij}^{}=_{h=1}^{H}A_{ij}^{l,h}(g_{}(.)_{j,-1}W_{v}^{,h})W _{o}^{,h}\] (1)

where the attention matrix for layer \(\) and head \(h\) is defined as:

\[A^{,h}=softmax((.)_{1:M+N,-1}W_{q}^{,h})(g_{}(.)_{1:M+N,-1}W_{k}^{,h})^{T}}{})\] (2)

where \(g_{}(.)_{1:(M+N),-1}^{(M+N) d}\) and \(A^{,h}^{(M+N)(M+N)}\). For understanding information transfer from the visual tokens to the causal layers, we use \(j[1,M]\) and \(i=c_{}\), where \(c_{}\) corresponds to the last token in the visual constraint. For understanding the information transfer to the last token, we set \(j=c_{}\) and \(i=c_{}\), where \(c_{}\) corresponds to visual constraint's last token and \(c_{}\) corresponds to the last token in the question.

### _Vqa-Constraints_: A Constraint Annotated Test-Bed for VQA

Alongside the above tools, we also introduce a new test-bed called _VQA-Constraints_ to enable our analyses. The test-bed consists of \(9.7K\) natural images paired with factual questions, where each question is annotated with visual and textual constraints (see Sec. 3.1). Specifically, we source image-question pairs from the following datasets i) OK-VQA  which covers general knowledge questions, ii) Movies  which includes questions about movie directors and awards from Wikipedia, and iii) Known  which covers questions about countries, famous people, and places. The questions from the Movies and Known datasets are not originally multi-modal, so we modify them to refer to images which we source from Bing. We provide more details on the dataset construction and the constraint annotation in the Appendix B.

We leverage GPT-4  to annotate the textual and visual constraints in the visual questions in _VQA-Constraints_. We first manually annotate 100 examples from each dataset with constraints. We provide this as context to GPT-4 and prompt it (see Appendix B) to annotate the constraints in new questions from Multimodal Known and OK-VQA. Due to the templated prompts in Multimodal Movies (e.g., "Name a movie directed by _this director_? "), the constraints are constant ("_this director_") across all examples and does not require external annotations.

## 4 Key Findings in how MLLMs Store and Transfer Information

Using the tools presented in Sec. 3.1, we present our key findings in how MLLMs retrieve information from internal layers and how this information is transferred across the model.

### Finding 1: Early MLPs and self-attention layers are causal

We find that information required to answer a visual question is mainly retrieved from the early-layer MLP and self-attention blocks of a MLLM. This is confirmed by the high indirect estimation effects which MultiModalCausalTrace assigns to the early layers in both LLaVa (see Fig. 3, Fig. 8) and multi-modal Phi-2 (see Appendix E) across the three datasets in _VQA-Constraints_.

This contrasts earlier results for LLMs which have been shown to retrieve information from mid-layer MLPs to answer factual questions [23; 24]. To obtain a fairer comparison, we apply MultiModalCausalTrace to LLaMA and LLaVa which use the same language backbone. We run both on the same set of questions - LLaMA on the Known dataset , and LLaVa on our multi-modal version of Known which modifies the questions to refer to an image (see Sec. 3.4). In Fig. 1, we show that the MLPs in the first 4 layers are causal for LLaVa while the MLPs in layers 4-7 are causal for LLaMa.

We also find that causal traces can be extracted from LLaVA with a minimum window size of 1, while LLaMa requires a minimum window size of 5 to obtain any significant causal traces.

Although we find a smaller window size of 1 to provide relevant causal traces (see Fig. 8), we find that a slightly larger window size of 3 results in consistent causal traces across all the questions. In Fig. 3, we report the results using a window size of 3, where we find that the early MLPs as well as self-attention layers are used to retrieve relevant knowledge to answer a visual question. We note that this observation is consistent for all the datasets in _VQA-Constraints_.

For multi-constraint questions that consist of a visual and textual constraint, information corresponding to the textual constraint is retrieved from a broader set of layers - _both_ the early and mid-layer MLP and self-attention blocks (see Fig. 4). We also find that a larger window size (at least of 6) is required to obtain any causal traces. This suggests that more parametric memory is required to meet both a visual and textual constraint in a given question.

Finding 2: Only a subset of visual tokens are involved in transferring information from the image to the early causal MLP layers.

We also find that the late visual tokens transfer information from the image to the early causal MLP layers (where information is mainly stored) via the first self-attention layer. We show this in Fig. 5 by visualizing the attention values in the early self-attention layers between each visual tokens and the final token in the visual constraint using the method described in Sec. 3.3. We see that the values are the highest in LLaVa's first self-attention layer (which occurs just before the first causal MLP layer), specifically for the last subset of visual tokens (indexes 540-576 out of 576 in total). We hypothesize that these tokens may be summarizing image information that is relevant to the given question before it is transferred and then stored in the MLPs, however we leave this to future study. We note that this pattern holds across all three datasets from _VQA-Constraints_.

Finding 3: Mid-layer self-attention layers are involved in transferring information from the early causal layers to the question's final token

Rather counter-intuitively, we find that even though information is stored in the early layers, the self-attention blocks in the middle layers (rather than layers immediately after) are responsible for propagating this information to the question's final token. The model's answer is sampled at this point, hence the information present here likely influences the ultimate generation. We see this in Fig. 15 and Fig. 16, which plots the attention contribution values between the last visual constraint token and the last token in the question across all the layers, using the methodology in Sec. 3.3. For LLaVa, the self-attention blocks in layers 16-17 are most active. This behaviour is similar in LLMs which also use mid-layer self-attention blocks to transfer information from (mid-layer) stored locations to the last token position.

Finding 4: Mid-layer self-attention contributions can be used to predict whether a MLLM will generate a correct answer, but model confidence is a more reliable predictor

When a MLLM generates a correct answer, we observe that the self-attention contributions in its middle layers are higher to when it generates an incorrect answer (see Fig. 15). For LLaVa, this is specifically the attention contributions between the last constraint token and the last prompt token in the 16th and 17th layer. This holds potential to detect when a model will answer correctly without running a full inference pass, therefore enabling "early" failure mode detection. To inves

Figure 5: **The late visual tokens are primarily responsible for transferring information from the image to the early causal layers, via the first self-attention layer.** We visualize attention contributions (see Eq.(1)) from the visual tokens to the visual constraint token averaged across the three datasets in _VQA-Constraints_.

tigate this, we compute the scalar average of the attention contributions from these two layers as \(,\_constraint}^{16}+a_{,\_constraint}^{17}}{a_{ ,\_constraint}^{17}}\) and find that it can classify a correctly generated answer with an AUROC of 0.63 on the Known dataset in _VQA-Constraints_ (see Fig. 16). We find, however, that the model's confidence - the probability of the generated output token at the final layer - is a slightly stronger predictor, with an AUROC of 0.76 (see Fig. 17). This parallel's previous work in LLMs  which used the attention contributions from _all_ the LLM's layers (via a linear model) to predict the generated answer's correctness. In comparison, our results suggest that a subset of MLLM's middle layers alone can be leveraged as a coarse "early" failure mode detector.

**Takeaway.** MLLMs behave differently in terms of information retrieval from their parametric memory however quite similarly to LLMs in terms of information transfer to the final token.

## 5 Correcting and Inserting Long-Tailed Information in MLLMs

Previous works have shown that counterfactual information can be inserted into LLMs by editing their causal layers [23; 24]. In this section, we verify if a similar approach can be used to edit a MLLM. Specifically, we introduce MultEdit, which applies a closed-form update to the early causal MLP layers we identified in Sec. 4. We show that our approach can effectively both (i) fix erroneous answers, and (ii) insert new long-tailed information in LLaVa in a VQA task.

### MultEdit

Given an image-question \((,y)\), we denote a MLLM's generated answer as \(O\). MultEdit updates a few parameters in the model such that it generates a new answer \(O^{*}\). Similar to [23; 13], we update the \(W_{proj}^{}\) matrix at specific causal MLP layers such that the probability \((O^{*})\) increases. Specifically, we view \(W_{proj}^{}\) as a linear-associated memory (where the matrix's input are treated as keys and its output as values) and uses a closed-form update to map the original keys to new (correct) values. Below, we outline the process for acquiring both the keys and values.

**Obtaining keys.** Let \(\{v_{i}\}_{i=1}^{N}\) be the visual token embeddings and \(\{e_{i}\}_{i=1}^{M}\) the text token embeddings. Given a causal layer \(\) and the last token of a constraint \(c\), we refer to the input of the layer's \(W_{proj}^{}\) matrix as its keys. Specifically, we define the key \(k_{c,}\) to be the input embedding to the \(W_{proj}^{}\) matrix corresponding to the last token of the constraint. This can be obtained with a simple forward pass with the visual and text token embeddings.

**Obtaining values.** Given a causal layer \(\), we refer to the output of the layer's \(W_{proj}^{}\) matrix as its values. Specifically, we define \(z_{c,}\) as the output embedding corresponding to the last token of the constraint. We optimize \(z_{c,}\) such that the probability of the correct answer \((O^{*})\) increases as:

\[z_{c,}^{*}=_{z_{c,}}(z_{c,})\] (3)

where \((z_{c,})\) is the standard next-token prediction loss used to train LLMs:

\[(z_{c,})=-(O^{*}|v_{1}...v_{N}e_{1}....e_{M})\] (4)

Figure 6: **MULEdit can correct error cases (left) and insert long-tailed factual knowledge (right) by editing the early causal MLP layers in an MLLM. We use the average probability of the correct token \(O^{*}\) as the metric for _Editing Efficacy_ and _Generalization_, and VQA-Accuracy for _Specificity_ (higher the better for all).**

MultEdit modifies the \(W^{}_{proj}\) matrix such that the old keys \(k_{c,}\) are mapped to the new optimized values \(z^{*}_{c,}\) which increase \((O^{*})\). We define MultEdit's editing objective as:

\[W^{*}_{proj}=_{W^{}_{proj}}\|W^{}_{proj}k_{c,}-z^{*}_{ c,}\|_{2}^{2}+\|W^{}_{proj}-W^{^{}}_{proj}\|_{2}^{2}\] (5)

The second regularization term ensures that \(W^{^{}}_{proj}\), the weights before the edit, do not deviate too much from \(W^{}_{proj}\). This helps to preserve performance on unrelated VQA pairs.

### Experimental details

We experimentally validate MultEdit in two real-world model-editing applications:

Fixing incorrect answers to common questions.We test MultEdit on a set of \(\)450 visual questions which LLaVa answers incorrectly (detected using incorrect VQA accuracy) from the multi-modal Known dataset in _VQA-Constraints_. These are generally questions about well-known places, persons and brands and companies.

Inserting long-tailed VQA knowledge.We test MultEdit on a set of visual questions from the Encyclopedia-VQA dataset . These query fine-grained knowledge about rare landmarks around the world, which MLLMs have been shown to struggle on .

For both settings, we compare MultEdit to the fine-tuning baselines:(i) fine-tuning from  which fine-tunes all the layers using the language modeling objective, and (ii) fine-tuning with constraints from  which fine-tunes the layers in a language model with a constraint on the weights to ensure local loss continuity.

We measure the success of each edit operation using the following metrics: (i) **Editing Efficacy**, which uses \((O^{*})\) to measure the edited model's ability to generate the correct answer for image-question (\(,y\)). (ii) **Generalization**, which uses \((O^{*})\) to measure the edited model's ability to generate the correct answer for the question \(y\) paraphrased using a language model (see Appendix for details), and (iii) **Specificity**, which uses VQA accuracy  to measure the edited model's performance on unrelated VQA questions. We consider unrelated questions to be those from the OK-VQA and Movies datasets in _VQA-Constraints_ (see further details in Appendix G).

### Results

Overall, our results show that updating the projection matrix using MultEdit at just a single early (causal) MLP layer can be a very effective approach for both correcting incorrect answers and inserting new knowledge in MLLMs.

Fixing incorrect answers.In Fig. 6 (left), we show that MultEdit is able to successfully fix the generations for all questions with wrong answers. In editing efficacy, the average probability of the correct answer improves from 0.07 to 0.82 after the edit. We also observe strong generalization, with a probability of 0.76 for the correct answer even when the question is paraphrased. Although we see a small drop of \(1.5\%\) accuracy on unrelated image-question pairs, we note that MultEdit outperforms fine-tuning with and without constraints on all the metrics.

Inserting long-tailed information.In Fig. 6 (right), we show that MultEdit is able to reliably insert new long-tailed knowledge in the model with an editing efficacy of 0.83. We see similar strong generalization when paraphrasing questions with an efficacy of 0.79. Similar to above, MultEdit incurs a small drop of 1.4\(\%\) on unrelated questions but is less affected than other methods.

In Fig. 14, we also provide ablations showing that editing the early causal MLP layers leads to better editing efficacies than editing the middle or the later MLP layers.

## 6 Conclusion

Our paper takes a closer look at how MLLMs process multi-modal information. We contribute a novel multi-modal causing tracing methodology and test-bed, _VQA-Constraints_, as well as a range of novel insights on how MLLMs retrieve and transfer information. We also introduce a novel model-editing algorithm, MultEdit, which can effectively fix errors or introduce long-tailed knowledge in MLLMs using a simple closed-form update which targets the early causal MLPs. Overall, this work deepens our scientific understanding of recent MLLM architectures, and enables future work in this direction.

## Author Contributions

Samyadeep Basu conceived the idea, implemented the algorithms, curated the dataset, ran experiments and wrote the paper. Daniela Massiceti provided critical feedback at every stage of the project (algorithms, dataset) and worked with Samyadeep to write the paper. Bes mira Nushi, Soheil Feizi and Cecily Morrison provided critical feedback on the project direction, paper writing and presentation. Martin Grayson helped the team with experiments on curating the probe dataset.