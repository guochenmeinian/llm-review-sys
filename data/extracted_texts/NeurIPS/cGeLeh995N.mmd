# On the role of entanglement and statistics in learning

Srinivasan Arunachalam

IBM Quantum

Almaden Research Center

Srinivasan.Arunachalam@ibm.com

&Vojtech Havlicek

IBM Quantum

T.J. Watson Research Center

Vojtech.Havlicek@ibm.com

&Louis Schatzki

Electrical and Computer Engineering

University of Illinois, Urbana-Champaign

louisms2@illinois.edu

###### Abstract

In this work we make progress in understanding the relationship between learning models when given access to entangled measurements, separable measurements and statistical measurements in the quantum statistical query (QSQ) model. To this end we prove the following results

1. For learning Boolean concept classes, we show that the entangled and separable sample complexity are _polynomially related_.
2. We give a concept class that shows an _exponential separation_ between quantum PAC learning with classification noise and QSQ learning. This proves the "quantum analogue" of the seminal result of Blum et al.  that separates classical SQ learning from classical PAC learning with classification noise.
3. The main _technical contribution_ is to introduce a quantum statistical query dimension (QSQ), which we use to give lower bounds on the QSQ learning. Using this, we prove exponential QSQ lower bounds for testing purity of quantum states, shadow tomography, learning coset states for the Abelian hidden subgroup problem, degree-\(2\) functions, planted bi-clique states and learning output states of Clifford circuits of depth \((n)\).
4. Using our QSQ lower bounds, we give an _unconditional_ separation between weak and strong error mitigation and prove lower bounds for learning distributions in the QSQ model. Prior works by Quek et al. , Hinsche et al.  and Neitner et al.  proved the analogous results _assuming_ diagonal measurements and our work removes this assumption.

## 1 Introduction

In the last few decades, machine learning (ML) has emerged as one of the most successful parts of artificial intelligence with wide-ranging applications in computer vision, image recognition, natural language processing. More recently, ML has been used in popular applications such as AlphaGo and Alpha zero (to play the games of Go and chess), chatGPT (to mimic a human conversation) and Alphafold (for solving some hard instances of protein folding). Simultaneously, understanding the power of quantum physics for ML has received much attention in the quantum computing community. There have already been many theoretical proposals for quantum algorithms providing speedups for practically relevant ML tasks such as clustering, recommendation systems, linear algebra, convex optimization, SVMs, kernel-based methods, topological data analysis . There are several surveys dedicated to understanding the power of quantum methods for ML .

Quantum learning theory provides a theoretical framework to understand quantum advantages in \(\). Here, there is a concept class \(\) which is a collection of \(n\)-qubit quantum states, a learner is provided with several copies of a state \(\), performs an arbitrary entangled operation on \(^{ T}\) and the goal is to learn \(\) well-enough. This framework encompasses several results in quantum learning such as tomography, shadow tomography, learning interesting classes of states, learning an unknown distribution and functions encoded as a quantum state [19; 20; 21; 22; 23; 24; 25; 26; 3; 2; 27; 17].

Given that machine learning is believed to be one of the first near-term applications of quantum computers, a natural question is how implementable are these algorithms in order to see quantum computational advantage in practice? The concern when considering near-term implementation of the above learning algorithms is twin-fold, \((i)\) the _infeasibilty_ of preparing copies of \(\) and \((ii)\) performing arbitrary entangled measurements on many copies of \(\) at once, both of which seem out of reach for near-term devices. Motivated by near-term implementations, recently  introduced the model of _quantum statistical query_ (QSQ) learning to understand the power of measurement statistics for learning, inspired by Kearns  classical SQ model. Since its introduction, variations of it have found applications in differential privacy, quantum supremacy, quantum neural networks, learning distributions and error mitigation [29; 30; 31; 2; 3]. In the QSQ model, a learning algorithm can perform \((n)\)-many _efficiently_-implementable two-outcome measurements \(\{M_{i},-M_{i}\}\) and the goal is to learn the unknown \(\) well enough using the measurement statistics. Clearly this model is weaker than that given access to \(^{ T}\), since the learner is only allowed access to _expectation values_ over a single copy of \(\). Given that recent works on error mitigation have shown that good estimates of expectation values may be accessible well before fault-tolerant quantum computation [32; 33], understanding the power of QSQ algorithms is of fundamental importance.

In this work, we primarily consider concept classes constructed from Boolean functions. In Valiant's PAC learning framework, the goal is to learn a _concept class_\(\{c:\{0,1\}^{n}\{0,1\}\}\). In the PAC model,1 a learning algorithm is given many uniformly random \((x^{i},c^{*}(x^{i}))\) where \(c^{*}\) is unknown and it uses these to learn \(c^{*}\) approximately well. Bshouty and Jackson introduced the _quantum_ PAC (QPAC) model  wherein a quantum learner is given _quantum examples_\(|_{c^{*}}^{ T}\), i.e., coherent superpositions \(|_{c^{*}}=}}_{x}|x,c^{*}(x),\) and it needs to learn the unknown \(c^{*}\) well enough. The complexity measure here is the _sample complexity_, i.e., copies of classical or quantum examples used by the algorithm. There have been works that have looked at this model and proven positive and negative results for learning function classes (see  for a survey).

Relationships Between Models.First, in the distribution-independent setting it is known that PAC and QPAC have the same sample complexity . However, the same need not be true in the distribution-dependent setting, where, for example, quantum learners can efficient learn DNF formulas over the uniform distributions . Second, it is a well-known fact that the classical _statistical query_ (SQ) model is exponentially-separated from PAC learning as witnessed by the class of parity functions . However, in  they show that parities can be learned in the QSQ model efficiently. In fact, in  they observed that, many positive results using quantum examples can be transformed into algorithms in the weaker QSQ framework. This motivates the following questions:

_1. Do entangled measurements offer any advantages in learning function classes?_

_2. Do measurement statistics suffice for learning function classes, i.e. is there a separation between QSQ and QPAC?_

Here, we resolve both these questions. We show that \((i)\) for learning _Boolean_ function classes the sample complexity of learning with entangled measurements and separable measurements are polynomially related, thereby showing that separable measurements are sufficient to witness quantum speedups in practice and \((ii)\) there is an exponential separation between learning with entangled measurements (even in the presence of classification noise) and learning with just measurement statistics, thereby showing that just measurement statistics might be insufficient to witness quantum speedups in practice.

### Main results

We now give a detailed description of our main results before we give an overview of their proofs in the next section.

Entangled versus Separable measurements.Since entangled measurements are vastly more difficult to realize experimentally, much recent work has gone into characterizing the limitiations of separable measurements. Bubeck et al.  gave a property testing task for which entangled measurements are _necessary_ for obtaining the optimal bounds. More recently, for learning classes of arbitrary _quantum states_ (i.e., not necessarily states constructed from function classes), two recent works by [26; 37] showed exponential separations for learning properties of quantum states with entangled vs separable measurements. Here, we study if similar separations exist when considering _function_ classes, a small subset of all quantum states. Our first result shows that in order to exactly learn a function class, every learning algorithm using entangled measurements can be transformed into a learning algorithm using just separable measurements with a polynomial overhead in sample complexity.

**Result 1**.: _For a concept class \(\{c:\{0,1\}^{n}\{0,1\}\}\), if \(T\) copies of \(|_{c}\) suffice to learn an unknown \(c\), then \(O(nT^{2})\) copies to learn \(c\) using only separable measurements._

\(\) versus noisy-\(\) learning.Classically, Kearns posed the question if \(\) learning is equal to \(\) learning with classification noise. The seminal result of Blum et al.  resolves this question by showing that the class of parity functions acting on the first \(O(( n) n)\) bits separates these two models of learning (under constant noise rate). In , the authors asked a question if there is a natural class of Boolean functions for which, \(\) learning can be separated from \(\) learning with noise (in fact, prior to our work, no separation was known even in the presence of _no noise_). Classically it is well-known that parities separates \(\) learning from \(\) learning. In , it was observed that the class of parities, juntas, and DNF formulas are learnable in the \(\) framework, leaning no clear candidate to separate \(\) from \(\). This motivates the following questions:

* In the noisy-quantum \(\) model [34; 35], a learning algorithm is given copies of \[|_{c^{}}^{n}=}}_{x\{0,1\}^{n}}|x |c^{}(x)+|(x)},\] (1) and the goal is to learn \(c^{}\). Is there a class that separates noisy-quantum \(\) from \(\) learning?
* Admittedly, the class constructed by Blum et al.  is "unnatural", can we obtain the separation in \((a)\) for a _natural_ concept class?
* Does such a separation hold for non-constant error rate \(\)?

Here, we describe a natural problem that witnesses this separation, resolving the three questions.

**Result 2**.: _The concept class \(=\{f_{A}:\{0,1\}^{n}\{0,1\} f_{A}(x)=x^{}Ax\;( }\;2),A_{2}^{n n}\}\) of all degree-\(2\) Boolean functions can be (exact) learned using quantum examples and separable measurements even in the presence of \(\)-classification noise in time \((n,1/(1-2))\), whereas every \(\) algorithm requires \(2^{(n)}\) queries to (approximately) learn \(\)._

Further applications.While we first considered learning function classes, the \(\) model is meaningful for a much broader class of tasks in quantum information theory and to that end we prove the following: \((a)\) we show hardness of shadow tomography (quadratically improving the prior bound  for separable measurements) and show hardness of even the simplest _Abelian_ hidden subgroup problem in the \(\) model; \((b)\) we give a _doubly_-exponential lower bound for testing purity of an unknown state; \((c)\) we give an exponential separation between weak and strong error mitigation and \((d)\) we give superpolynomial lower bounds for learning output distributions of quantum circuits (when measured in the computational basis). Prior works [2; 3; 4] considered tasks \((c),(d)\) and showed these separations when the \(\) queries correspond to _diagonal_ measurements, and we remove the diagonal assumption here. We discuss this further in Section 4.2.

## 2 Models of learning

We discuss the learning models of interest in our submission in this section. For a quick introduction to quantum information notation, we refer the reader to Section 1 in the Supplementary material. In all models, we are primarily interested in improper learning, i.e. the learner need not output a state or function from the concept class.

Classical Pac learning.Valiant  introduced the classical Probably Approximately Correct (PAC) learning model. In this model, a _concept class_\(\{c:\{0,1\}^{n}\{0,1\}\}\) is a collection of Boolean functions. The learning algorithm \(\) obtains _labelled examples_\((x,c(x))\) where \(x\{0,1\}^{n}\) is uniformly random and \(c\) is the _unknown_ target function.2 The goal of an \((,)\)-learning algorithm \(\) is the following: for every \(c\), given labelled examples \(\{(x^{i},c(x^{i}))\}\), with probability \( 1-\) (over the randomness of the labelled examples and the internal randomness of the algorithm), outputs a succinct circuit-representation for an hypothesis \(h:\{0,1\}^{n}\{0,1\}\) such that \(_{x}[c(x)=h(x)] 1-\). The sample complexity and time complexity of a learning algorithm is the maximal number of labelled examples and time used by the optimal learning algorithm respectively.

Quantum Pac learning.The quantum PAC model was introduced by Bshouty and Jackson  wherein, they allowed the learner access to quantum examples of the form \(|_{c}=}}_{x\{0,1\}^{n}}|x,c(x)\). Like the classical complexities, one can similarly define the \((,)\)-sample and time complexity for learning \(\) as the quantum sample complexity (i.e., number of quantum examples \(|_{c}\)) used and quantum time complexity (i.e., number of quantum gates used in the algorithm) of an optimal \((,)\)-learner for \(\). Similarly, Bshouty and Jackson  defined quantum learning with classification noise, wherein a learning algorithm is given access to \(|_{c}^{n}=}}_{x}|x(|c(x)+|c(x))\). Such quantum examples have been investigated in prior works .

Learning with entangled versus separable measurements.Observe that in the usual definition of QPAC above, a learning algorithm is given access to \(|_{c}^{ T}\) and needs to learn the unknown \(c\). In this paper we make the distinction between the case where the learner uses entangled measurements, i.e., perform an arbitrary operation on copies of \(|_{c}\) versus the setting where the learner uses separable measurements, i.e., performs a single-copy measurement on every copy of \(|_{c}\) in the learning algorithm. When discussing learning with entangled and separable measurements, in this paper we will be concerned with _exact_ learning, i.e., with probability \( 2/3\), the learner needs to _identify_\(c\). We denote EntExact as the sample complexity of learning with entangled measurements and SepExact as the sample complexity of learning with separable measurements.

Quantum statistical query learning.We now discuss the QSQ model, following the definitions given in . More generally, in order to learn an unknown quantum state \(\), in the QSQ model, the learner makes Qstat queries that take as input a bounded linear operator \(M\) over the Hilbert space of \(\), satisfying \(\|M\| 1\), and tolerance \(\) and outputs a \(\)-approximation of \((M)\), i.e.,

\[:(M,)[(M)-,(M )+].\]

The goal of the QSQ learner is: with probability \( 1-\), output a succinct description of a state \(\) such that \(\|-\|_{}\). In order to learn a function class using quantum examples, we have \(=|_{f}_{f}|\) and on input \(M\), the Qstat oracle responds with \([_{f}|M|_{f}-,_{f}|M|_{f} +]\).

In this case, the goal of a QSQ learner is to output a hypothesis \(h\) that satisfies \(d_{}(|_{f}_{f}|,|_{h}_{h }|)\), which translates to \(_{x}[h(x)=f(x)] 1-\). The query complexity for learning \(\), denoted \(()\), is the number of Qstat queries an optimal algorithm makes and the quantum time complexity is the total number of gates used by an optimal algorithm (which includes the gates to number of gates to implement \(M\)). We say a \(n\)-bit concept class \(\) is QSQ learnable if \(\) can be learned using \((n)\) many Qstat queries, each with tolerance \(=1/(n)\) and observable \(M\) which is implementable using \((n)\) many gates. There are three ways to motivate the QSQ model

1. From a theoretical perspective, performing \(2\)-outcome measurements is easier to implement than arbitrary separable measurements, which is in turn easier to implement than entangled measurements, so it is useful to understand the power of expectation values in quantum learning theory and the QSQ captures this question in a theoretical framework.32. We emphasize that a \(\) learner is a _classical_ algorithm since it receives statistical estimates of measurements on quantum states. One could envision a framework of learning where quantum states are prepared in the "cloud" and a _classical_ learner needs to interact with the cloud only _classically_: \(\) models such a quantum framework.
3. Most quantum complexity classes are defined by making a binary measurement on a readout qubit. This can be readily subsumed into the \(\) framework. The \(\) model also naturally extends recent works [2; 3; 4] wherein they consider the limitations of classical \(\) in the setting where \(M=_{x}(x)|x x|\) is diagonal.
4. \(\) naturally generalizes \(\). That is, one can think of \(\) as being a form of statistical learning where the learner can change the basis of their statistics. Indeed, \(\) corresponds to the case when all queried observables are diagonal.

Some positive results in \(\)In quantum learning theory, there are a few well-known function classes that are learnable using quantum examples: parities, juntas, DNF formulas, the coupon collector problem, learning codeword states. It was observed in  that the first three classes are learnable in \(\) already, primarily because a version of Fourier sampling is implementable in \(\). In this work we first observe that the coupon collector problem and learning codeword states are also learnable in the \(\) framework. We next observe that the class of Fourier-sparse functions are \(\) learnable (which subsumes all the positive results in ).

**Theorem 1**.: _The class of \(k\)-Fourier sparse functions, the class of codeword states, coupon collector problem can be learned in the \(\) model._

Beyond learning example states, we next observe that one can do tomography on the set of trivial states, i.e., states \(|=C|0^{n}\) where \(C\) is a constant-depth \(n\)-qubit circuits, in polynomial time in the \(\) model. An open question of this work, and also the works of [3; 4], is if we can learn the distribution \(P_{C}=\{ x|^{2}\}_{x}\) using _classical_\(\) queries. The theorem below shows that if we had direct access to \(|\), one can learn the state and the corresponding distribution \(P_{C}\), using \(\) queries.

**Theorem 2**.: _The class of \(n\)-qubit trivial states can be learned up to trace distance \(\) using \((n,1/)\)\(\) queries with tolerance \((/n)\)._

For further details we defer the corresponding proofs to Section 5.1 in the Supplementary material.

## 3 Proof of results

In this section we outline the proof our two results.

### Proof overview for Result 1

Our starting point towards proving this result is that one could use a result of Sen  that, given copies of \(|_{c^{*}}\), one could apply _random_ measurements on single copies of this state and produce an \(h\) that is approximately close to \(c^{*}\) using at most \(T=(|C|)/\) copies of \(|_{c^{*}}\).4 So, for separable learning, by picking \(=_{min}\) as the minimum distance between concepts in \(\), one could exactly learn \(\) using \(T\) quantum examples. Proving a lower bound on entangled learning \(\) is fairly straightforward as well: first observe that \((||)/n\) is a lower bound on learning (since each quantum example gives \(n\) bits of information and for exact learning one needs \((||)\) bits of information) and also observe that \(1/_{min}\) is a lower bound, since to distinguish just between \(c,c^{}\) that satisfy \(_{x}[c(x)=c^{}(x)]=1-_{min}\), so one needs \(1/_{min}\) copies of the unknown state. Putting this separable upper bound and entangled lower bound together gives us \(() n()^{2}\) for all \(\). This relation is however sub-optimal.

We further improve the entangled lower bound as follows. Let \(_{a}=_{c,c^{}}_{x}[c(x) c^{}x)]\). We use a information-theoretic argument (inspired by a prior work ) as follows: define a random variable \(\), \(\) and the quantum state \(_{,}=_{c}|c c|| _{c}_{c}|^{ k}\) (assuming that \(k\) is the sample complexity of \(\)). For exact learning we know that \(I(:)=(||)\) again because one needs to learn \(\) exactly. Next we know that \(I(:) k I(:_{1})\) (where \(_{1}\) corresponds to the first register in \(_{1}\)). Now using a non-trivial analysis, one can analyze the reduced density matrix on the subsystem \(\), \(_{1}\) and analyze its eigenvalues to show that \(I(:_{1}) n_{}\). Chaining these inequalities gives that \(\{1/_{m},(||)/(n_{})\}\). Combining this entangled lower bound with the separable upper bound, we get that

\[ On_{ }/_{}\,\ }.\]

For further details we defer the proof to Section 2 in the Supplementary material. It is not hard to see that this relation is optimal as well for the class of degree-\(2\) functions defined as

\[=\{f(x)=x^{}Ax:A_{2}^{n n}\}.\] (2)

For this class \(_{}=_{m}=O(1)\) by the Schwartz-Zippel lemma. Recently it was shown  that \(=(n^{2})\) and \(=(n)\) showing the optimality of our relation above.

### Proof overview for Result 2

#### 3.2.1 Technical contribution.

A fundamental issue in proving our \(\) result is, what techniques could one use to prove these lower bounds? Prior to our work, in  they introduced two new techniques based on differential privacy and communication complexity that give lower bounds on \(\) complexity. However, both these lower bounds are exponentially weak! In particular, the lower bounds that they could prove were linear in \(n\) for learning an \(n\)-bit concept class. Classically, there have been a sequence of works  with the goal of proving \(\) lower bounds and finally the notion of _statistical dimension_ was used to obtain close-to-optimal bounds for \(\) learning certain concept classes and the breakthrough works of  used it to settle the complexity of learning the planted \(k\)-biclique distribution.

In this work, our technical contribution is a combinatorial parameter to lower bound \(\) complexity akin to the classical parameter. To this end, we follow a three-step approach.

1. **Reduction to Decision Problems.** We show that an algorithm \(\) that learns a concept class below error \(\) in trace distance using \(\) queries of tolerance \(\) can also be used to solve the following decision problem: for a fixed \(\) such that \(_{}d_{}(,)>2(+)\), decide if an unknown state is either some \(\) or equals \(\). Calling \(\) the complexity of such decision problem, we show that \[()_{}\{(,) -1:_{}d_{}(,)>2(+)\}.\]
2. **Quantum Statistical Dimension.** Next, we define the notion of _quantum statistical dimension_\(\): for \(>0\), a class of states \(\) and a \(\), the \(_{}(,)\) is the smallest integer such that there exists a distribution \(\) over \(\) queries \(M\) satisfying \(_{M}[|(M(-))|>] 1/d\) for all \(\). From an operational perspective \(\) is natural, as it can be viewed as the smallest expected number of observables that can distinguish all states in \(\) from \(\). We then show that if the decision algorithm succeeds with probability at least \(1-\), we have that: \[(,)(1-2)_{}(,).\]
3. **Lower Bounds on \(\).** Even with this lower bound, proving bounds on \((,)\) is non-trivial. To this end, we further give two lower bounding techniques for \((,)\), one based on the variance of \(\) queries across \(\) (inspired by the work of Kearns ) and one based on average correlation (inspired by the work of Feldman ). In particular, we define two combinatorial quantities \(()\) and \((,)\) which can be associated with every class and use it to lower bound \(\). Let \(\) be a distribution over \(\), such that \(_{1}:=_{}[]\). We define \(()\) as follows: \[()=^{2}_{M,\|M\| 1} }[[ M]]^{-1},\] (3) where \[}[( M)]= {}[( M)^{2}]- }[( M)]^{2}.\]We show that \(_{}(,_{1})()\), which we eventually use to lower bound \(()\). Next, for \(_{2}\), we define the average correlation \((,_{2})\) as

\[(,_{2})=_{_{0}}( _{}^{}(_{0},_{2}))^{-1}\;,\] (4)

where \(_{}^{}(_{0},_{2})\) is a combinatorial parameter capturing correlations between states in \(_{0}\) and \(_{2}\). We then show that \(_{}(,_{2})(, _{2})\) and use this in turn to lower bound \(()\).5

Putting together the three bullets above, the \(\) complexity of learning can be lower bounded by the variance bound and the average correlation bound that we define in this work. We remark that although, our quantum combinatorial parameters are inspired by the classical works of Feldman et al. [42; 43; 44], proving that they lower bound \(\) complexity and also giving lower bounds for the corresponding concept class using these parameters is non-trivial and is a key technical contribution of our work. Below, we apply these lower bounds to obtain our learning results. For further details we defer the formal definitions and proofs to Section 3 in the Supplementary material.

#### 3.2.2 \(\) versus noisy \(\)

We now sketch the proof of Result 2. As mentioned earlier, previous to our work we did not have a candidate class to separate \(\) from \(\) (let alone with noise). There have been a few works that have shown exponential lower bounds for learning using separable measurements [45; 26; 37], but all these lower bounds correspond to learning classes of mixed quantum states. Hence it was open if there is very simple structured _function class_ such that quantum examples corresponding to this function class is hard for \(\) (in fact given our polynomial relation between entangled and separable learning, it is conceivable that for the small class of function states, \(\) are \(\) are polynomially related as well). In this work, we look at the degree-2 concept class \(\) defined in Eq. (2).

Recently, it was observed that  this class is learnable using \(O(n)\) quantum examples with entangled measurements and \(O(n^{2})\) quantum examples with separable measurements. Our main contribution is in showing that the \(\) complexity of learning \(\) with tolerance \(\) is \((2^{n/2}^{2})\). When \(=1/(n)\) this is an exponential, \(2^{(n)}\), lower bound. We prove the hardness for algorithms using \(\) queries using the variance lower bounding technique. In particular, we show that for every \(n+1\) qubit operator \(M\) such that \(\|M\| 1\), we have that

\[_{f}([M_{f}])=O(2^{-n/2}),\] (5)

where we let \(_{f}=|_{f}_{f}|\) for notational simplicity. Combined with our variance lower bound introduced in Section 3.2.1, we obtain our lower bound on the \(\) complexity of learning \(\). It remains to establish Eq. (5). To this end, we need to understand

\[_{f}([M_{f}])=_{f}[[M_{f} ]^{2}]-(_{f}[[M_{f}]])^{2}\] (6)

To do so, we decompose \(_{f}\) as follows. For every \(f:\{0,1\}^{n}\{0,1\}\) let \(|_{f}=}}_{x}|x,f(x)\) and \(|_{f}=_{x}(-1)^{f(x)}|x\). For convenience we let \(|u=}}_{x}|x\). Then we see that

\[|_{f}_{f}|= _{f}||--|}_{_{1}^{f}}- u||-+|}_{_{2}^{f}}- {|u_{f}||+-|}_{_{3}^{f}}+ {|u u||++|}_{_{4}^{f}}.\]

We now note that any \(n+1\) qubit observable \(M\) can be decomposed as \(M=_{a,b}M_{a,b}|a b|\) where now \(a,b\{+,-\}\). Since \(\|M\| 1\) we also have that \(\|M_{a,b}\| 1\), however the off-diagonal blocks now no longer need be Hermitian. In an abuse of notation we now discard the last qubit of \(_{i}^{f}\) and denote the resulting state also as \(_{i}^{f}\). For ease of notation we further introduce the notation \(M_{1}=M_{-,-}\), \(M_{2}=M_{-,+}\), \(M_{3}=M_{+,-}\), and \(M_{4}=M_{+,+}\) Thus, we see that \([M_{f}]=_{i}[M_{i}_{i}^{f}]\) and further the variance can be written as \[_{f}([M_{f}])=_{i,j}_{f} M_{i}_{i}^{f}M_{j}_{j}^{ f}-(M_{i}_{f}[_{i}^{f}])(M_{j} _{f}[_{j}^{f}]).\] (7)

At this point, we upper bound all these terms by \((-n/2)\). Proving this upper bound is fairly combinatorial but crucially it involves understanding the properties of the ensemble \(\{|_{f}\}_{f}\) and its moments for a uniformly random degree-\(2\) functions \(f\). Finally, we observe that the concept class can be learned given noisy quantum examples like in Eq. (1) using \((n,1/(1-2))\) examples (proving this uses the standard procedure to take derivatives of quantum states and the observation that this procedure is noise-resilient). This gives us the claimed separation between QSQ and noisy-QPAC, the "quantum analogue" of the seminal result of Blum et al.  for a _natural_ class and with _non-constant_ error rate close to \(1/2\). We refer the interested reader to Section 4.1 in the Supplementary material.

### Smallest class separation

Above we saw that the concept class of quadratic functions separated QPAC from QSQ. Observe that states in this concept class can be prepared by circuits of size \(O(n^{2})\) and depth \(O(n)\) consisting of \(\{,,\}\) gates. A natural question is, can states prepared by _smaller_ circuits also witness such a separation between QPAC and QSQ? In Section 2 we saw that trivial states are learnable in QSQ, so is it necessary to have super-constant depth in order to show super-polynomial lower bounds? In the theorem below we answer this in the positive, by using a simple padding argument inspired by a prior work of Hinshe et al. . In particular, we show that the class of states produced by \(( n)\)-depth is already hard to learn for QSQ algorithms using a polynomial number of queries. For further details we defer the proof to Section 4.2 in the Supplementary material.

**Theorem 3**.: _For any \((0,1)\) there exists a family of \(n\) qubit Clifford circuits of depth \(d=( n)^{1/}\) and size \(d^{2}\) that requires \(2^{(d)}\)_Qstat _queries to learn the state to error \( 1/2\) in trace distance._

## 4 Further applications

In this section we first use our lower bound techniques to give QSQ lower bounds for interesting classes of states and then use our QSQ lower bounds for further applications outside learning. For further details we defer the proof to Sections 4 and 5 in the Supplementary material.

### \(\) lower bounds for learning states

Extending our lower bounds from above, we consider fundamental problems in quantum computing and prove QSQ lower bounds for these tasks. We summarize our results in the table below, for \(=O((n)})\), before expanding upon these in the following subsections.

  Problem & QSQ Bound & General Complexity \\  Shadow tomography & \((4^{n})\) & \(O(2^{n})\) with _separable_ measurements  \\ with Pauli observables & \((4^{n})\) & \(O(2^{n})\) with _separable_ measurements  \\  Learning coset states & \(2^{(n)}\) & \(O(n)\) with _separable_ measurements \\ from Abelian hidden subgroup problem & \(2^{(n)}\) & \(O(1)\) with _entangled_ measurements \\  Purity testing & \(2^{2^{(n)}}\) & \(O(1)\) with _entangled_ measurements  \\  Learning states from & \(2^{(n)}\) & \(O(n)\) for stabilizer states \\ \(O(2^{n})\) approximate \(2\)-designs & & with _entangled_ measurements \\  

_Hidden subgroup problem._ Coset states appear often in the hidden subgroup problem (HSP) , a fundamental problem in quantum computing. It is well-known that coset states of the Abelian HSP can be learned exactly from _separable_ measurements in polynomial sample complexity and for non-Abelian groups, it was well-known that separable measurements  require _exponential_ many copies to learn a coset state. A natural question is, what is the QSQ complexity of learning coset states? Given that the _standard approach_ for HSP is based of Fourier sampling and  showed that a version of Fourier sampling is easy in QSQ, it is natural to expect that HSP is implementable in QSQ. Surprisingly, in this work, we show that, even for _Abelian_ groups, the QSQ sample complexity of learning the unknown coset state is exponentially large. In particular, we show a lower bound of \((^{4} 2^{n})\) on the \(\) complexity of learning using \(()\) queries and the proof of this is done via the average correlation method we introduced in Section 3.2.1.

_Shadow tomography._ In recent times, there have been a lot of works surrounding the framework of shadow tomography [21; 37]. The goal here is, given copies of an unknown quantum state \(\), the learner has to predict the expectation value \([O_{i}]\) of a collection of known observables \(\{O_{i}\}_{i[k]}\) up to error \(\). It is well-known to be solvable using \((n, k)\) copies of \(\). In  the authors show \((2^{n})\) copies of \(\) are necessary and sufficient for shadow tomography using _separable measurements_. To prove the lower bounds the authors construct a many-vs-one decision task where \(=/2^{n}\) and \(=\{_{i}=+3 O_{i}}{2^{n}}\}\). Assuming that \([O_{i}]=0\) and \([O_{i}^{2}]=2^{n}\) for all \(O_{i}\), then an algorithm which solves the shadow tomography problem with high probability also solves the decision problem. Thus, a lower bound on the latter is also a lower bound on the sample complexity of shadow tomography. Here we give a quadratically _stronger_ lower bound of \((4^{n})\) when given access to only \(\) measurements, which we prove using the average correlation method. Our result shows that even separable measurements and not just statistics play a non-trivial role in shadow tomography.

_Does tolerance matter?_ A natural question when discussing \(\) learning is, is there a natural _distribution learning_ task that can be solved with tolerance \(_{Q}_{C}\) such that classical \((_{C})\) queries cannot solve the task but \((_{Q})\) can solve the task? Here we consider the class of _bi-clique states_ introduced in the seminal work of Feldman et al. . In their work they showed that for detecting a planted bipartite \(k\)-clique distributions when the planted \(k\)-clique has size \(n^{1/2-}\) (for constant \(>0\)), it is necessary and sufficient to make superpolynomial in \(n\) many \((k/n)\) queries. Here we show that one can achieve the same query complexity quantumly but with \(()\), i.e., with quadratically larger tolerance we can detect a \(k\)-biclique. A classical \(\) algorithm cannot solve this task with \(_{C}=\) queries.

_A doubly exponential lower bound?_ So far all our lower bounds for learning \(n\)-qubit quantum states are exponential in \(n\). A natural question is, can one prove a doubly exponential lower bound for some task? In this work, we show that the natural problem of _testing purity_, i.e., given a quantum state \(\) return an estimate of \([^{2}]\), requires \((2^{n}^{2})\) many \(\) queries to solve. Previous work of  showed that it is necessary and sufficient to use \((2^{n})\) many copies of \(\) to test purity if we were allowed _separable_ measurements, but our work considers the weaker \(\) model and proves a doubly-exponential lower bound. The proof of this uses Levy's lemma and the ensemble of Haar random states to lower bound the quantum statistical dimension in a manner similar to that of the variance based technique.

### Applications outside learning

Using our \(\) results we present two applications. First we give an exponential separation between weak and strong error mitigation, resolving an open question of Quek et al. . Second, we show super-polynomial lower bounds for learning output distributions (in the computational basis) of \(n\)-qubit Clifford circuits of depth \(( n)\) and Haar random circuit of depth-\(O(n)\), extending the works of [3; 4]. All these results [2; 3; 4] proved these lower bounds for \(\) algorithms assuming _diagonal_ observables. For further details we defer the proof to Section 6 in the Supplementary material.

_Error mitigation._ Error mitigation (\(\)) was introduced as an algorithmic technique to reduce the noise-induced in near-term quantum devices, hopefully with a small overhead, in comparison to building a full-scale fault-tolerant quantum computer to harness general quantum advantages . In recent times, \(\) has obtained a lot of attention with several works understanding how to obtain _near-term_ quantum speedups as a surrogate to performing error correction. \(\) has been an important component in recent \(\) demonstrations [37; 51; 10].

More formally, an \(\) algorithm \(\) takes as input a quantum circuit \(C\), noise channel \(\) and copies of \(|^{}=(C)|0^{n}\). In a strong \(\) protocol, \(\) needs to produce samples from a distribution \(D\) that satisfies \(d_{}(D,\{ x|C|0^{n}^{2}\}_{x})\) and in the weak \(\) setting, given observables \(M_{1},,M_{k}\) the goal is to approximate \(|M_{i}|\) upto \(\)-error. In , they asked the question: how large should \(k\) be in order to simulate weak \(\) by strong \(\)? They show that _when \(M_{i}\)s are diagonal_, then \(k=(2^{n})\), i.e., they gave an exponential separation between weak and strong \(\). In this work, our main contribution is to use Result 2 to remove the assumption that \(M_{i}\)s are diagonal and show an exponential separation unconditionally between weak and strong \(\).

_Learning distributions._ Recently, the works of Hinsche et al.  and Nietner et al.  initiated the study of learning output distributions of quantum circuits. In particular, they considered the following _general_ question: Let \(|_{U}=U|0^{n}\) where \(U\) is a family of interesting unitaries and let \(P_{U}(x)= x|U|0^{n}^{2}\) be a distribution. How many QSQ queries does one need to learn the \(P_{U}\) to \(d\) at most \(\)? To this end, the works of  looked at _diagonal_\(M\)s, i.e., \(M=_{X}(x)|x x|\) for \(:\{0,1\}^{n}[-1,1]\) and showed the hardness of approximately learning \(P_{U}\) for \(\) being \(( n)\)-depth Clifford circuits and depth-\(d\{( n),O(n)\}\) and \(d\)-depth Haar random circuits.6

In this work, we improve upon their lower bounds by removing the assumption that \(M\) is diagonal and prove a _general_ QSQ lower bounds for these circuit families that is considered in their work. In order to prove this bound, we follow the following three step approach \((i)\) We first observe the following simple fact: for distributions \(p,q:\), define \(|_{p}=_{z}|z\) and \(|_{q}\) similarly. Then \(\|_{p}-|_{q}\|_{}^{2} 2\|p-q\|_{}vd}\). So it suffices to prove the hardness of learning the output state, in order to prove the hardness of learning the distribution. \((ii)\) Next we consider the class \(\) of states forming a \(\)-approximate \(2-\)design where \(=O(2^{-n})\) and show that learning states from \(\) with error \( 2/3\) in trace distance requires \((^{2} 2^{n})\)\(()\) queries. \((iii)\) Finally, using that depth-\(O(n)\) circuits form a design, we invoke our QSQ lower bound in order to prove the hardness of learning the output states of these circuits. These three steps proves the hardness of learning output distributions (in the computational basis) of quantum circuits.

Open questions.There are a few natural questions that our work opens up: \((i)\) Can we show that for every concept class \(\), we have that \( O(n)?\), \((ii)\) Following  what is QSQ complexity of learning the output distribution of constant-depth circuits assuming we only use _diagonal_ operators? \((iii)\) Theoretically our work separates weak and strong error mitigation, but in practice there are often assumptions in the mitigation protocols, can we show theoretical separations even after making these assumptions? \((iv)\) Classically it is well-known that several algorithms can be cast into the QSQ framework, is the same true quantumly? If so, that would suggest that QSQ as a unifying framework for designing new learning algorithms. \((v)\) What is the QSQ complexity of the Hidden subgroup problem when given access to _function_ states, instead of coset states (which is the case only in the _standard approach_).