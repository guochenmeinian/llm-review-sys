# Thrust: Adaptively Propels Large Language Models

with External Knowledge

 Xinran Zhao\({}^{1,2}\) Hongming Zhang\({}^{1}\) Xiaoman Pan\({}^{1}\) Wenlin Yao\({}^{1}\)

**Dong Yu\({}^{1}\) Jianshu Chen\({}^{1}\)**

\({}^{1}\)Tencent AI Lab, Bellevue, \({}^{2}\)Language Technologies Institute, Carnegie Mellon University

Work done during interning at Tencent AI Lab, Bellevue. Corresponding contact email addresses: xinranz3@andrew.cmu.edu, {hongmingzhang,xiaomanpan,wenlinyao,dyu,jianshuchen} @global.tencent.com. Our code is available at https://github.com/colinzhaoust/thrust_neurips2023.

###### Abstract

Although large-scale pre-trained language models (PTLMs) are shown to encode rich knowledge in their model parameters, the inherent knowledge in PTLMs can be opaque or static, making external knowledge necessary. However, the existing information retrieval techniques could be costly and may even introduce noisy and sometimes misleading knowledge. To address these challenges, we propose the instance-level adaptive propulsion of external knowledge (IAPEK), where we only conduct the retrieval when necessary. To achieve this goal, we propose measuring whether a PTLM contains enough knowledge to solve an instance with a novel metric, _Thrust_, which leverages the representation distribution of a small number of seen instances. Extensive experiments demonstrate that _Thrust_ is a good measurement of PTLM models' instance-level knowledgeability. Moreover, we can achieve higher cost-efficiency with _Thrust_ score as the retrieval indicator than the naive usage of external knowledge on 88% of the evaluated tasks with 26% average performance improvement. Such findings shed light on the real-world practice of knowledge-enhanced LMs with a limited knowledge-seeking budget due to computation latency or costs *.

## 1 Introduction

Knowledge is crucial for understanding human language and solving various NLP tasks . In recent years, the pre-trained language models (PTLM) have demonstrated great success on various NLP tasks  by storing rich encyclopedic  and commonsense  knowledge in their model parameters. However, such implicit knowledge could be opaque, static, or inefficient . These issues motivate the common practice of seeking external knowledge  with information retrieval methods and augmenting the inference models (e.g., PTLMs)  with the retrieved knowledge.

However, this approach has two limitations: (i) extracting external knowledge with existing information retrieval tools can be costly for a large-scale knowledge resource. (ii) external knowledge can be unnecessary or even misleading. For instance, one of the best retrieving models ColBERT v2  achieved 68.9 Success@5 on Natural Question , which suggests that gold documents do not appear in the top five retrieved documents for 31.1% of the queries. Considering the limited input sequence length, the most useful documents may not be included for generating a prediction, while others may add noise to the model. On the other hand, PTLMs, which grow from millions (e.g., BERT ) to billions of parameters (e.g., OPT ), may solve the queries directly withoutexternal knowledge, making it unnecessary to seek external knowledge that signifies the noise issue. The instance shown in Figure 1 demonstrates the noise and inefficiency issues caused by external knowledge retrieval. OPT-175B can directly give the correct answer without external knowledge. However, with top external knowledge retrieved by DPR  from Wikipedia paragraphs, the external knowledge can be useless or even lead to wrong predictions.

The efficiency and noise issues motivate us to propose the **I**nstance-level **A**daptive **P**ropulsion of **E**xternal **K**nowledge (**IAPEK**), which adaptively retrieves external knowledge when it is necessary. We propose to measure whether a PTLM can solve a question instance with internal knowledge with a confidence score and reject using external knowledge when the confidence is high, instead of seeking retrieval directly for all the cases. The overall pipeline is shown in Figure 2. Specifically, we propose to solve this problem from the representation learning perspective based on two assumptions: (i) if a PTLM has mastered sufficient knowledge about a task, its hidden states should be able to cluster the samples from the task well enough; (ii) if the representation of a new instance deviates from these clusters in hidden states, this instance should be beyond the knowledge scope of the target PTLM. On top of these assumptions, we design a simple and lightweight metric _Thrust_ to measure the distance between an instance's representation and the clusters of several observed examples in the same task.

To comprehensively understand the effectiveness of _Thrust_, we conduct experiments on diverse NLP tasks. Experiments show that the average _Thrust_ score of open-domain questions is significantly lower than other tasks, which aligns well with our intuition that open-domain QA tasks typically need external knowledge. These results indicate that _Thrust_ is a good measurement of the models' knowledgeability. Extensive experiments also show that _Thrust_ can improve the cost-efficiency of seeking and using external knowledge on 88% cases with 26% average performance improvement through identifying the instances that mostly require knowledge. We can also observe that, with _Thrust_, we can achieve higher performance than injecting external knowledge for all instances, where models are benefited from both the performance and efficiency aspects. Such findings shed light on the real-world practice of knowledge-enhanced LMs with a limited budget for knowledge seeking due to computation latency or costs.

## 2 Approach

### IAPEK: Problem Formulation

We begin with a formal problem definition of **I**nstance-level **A**daptive **P**ropulsion of **E**xternal **K**nowledge (**IAPEK**). For each instance of natural language query \(q\) (e.g., a question in question-answering tasks), we first determine whether the PTLM has sufficient knowledge to solve the current problem. We achieve this by using a real-valued score function \(s(q)\) that assigns a higher score to \(q\) if the current PTLM has enough knowledge to solve the problem and vice versa. And we will retrieve

Figure 1: The predictions by OPT-175B without/with external knowledge retrieved via DPR  from Wikipedia paragraphs. Although the top retrieved paragraphs are relevant since the internal knowledge is already sufficient, the external knowledge can either be misleading (potentially due to the effect of _misprime_) or less useful.

Figure 2: The pipeline of retrieval-augmented models with **IAPEK**. Unlike previous work (e.g., RAG ) where models directly seek for help from the retriever module, **IAPEK** module provides a confidence score \(S(q)\) (e.g., _Thrust_) on how well the PTLM can answer the question with internal knowledge and decides if the external retrieval is necessary.

external knowledge to facilitate PTLM once the score \(s(q)\) falls below a threshold \(\). By doing this, the PTLM can selectively retrieve external knowledge in an _instance-adaptive_ manner and avoid unnecessary knowledge augmentation costs. To achieve such a mission, a critical step is to design an _instance-level_ score function \(s(q)\) that can effectively measure whether the PTLM has enough knowledge to solve the current particular input _instance_. In the remainder of the section, we will show how to construct such a function efficiently.

### _Thrust_: Measuring the Knowledgeability of PTLMs

We now proceed to construct the scoring function \(s(q)\) that measures the knowledgeability of a PTLM for solving an instance from a particular task \(\). A seemingly straightforward approach is to adopt a supervised learning strategy to train such functional mapping from human-labeled data.

However, it is practically infeasible to manually annotate whether a given PTLM has sufficient knowledge at an instance level because the internal knowledge of PTLM is implicitly stored in its model parameters, which are hard to probe precisely for the IAPEK purpose. For this reason, we take an alternative approach by looking at the problem from the representation learning perspective. Specifically, our method is based on the following two assumptions. First, if a PTLM has mastered sufficient knowledge about a task, then its hidden states should be able to cluster the samples from the task well enough. For a classification task, samples from different classes should also be well separated in their (higher-level) hidden representations. Second, we further hypothesize that when a particular sample from the task deviates from these clusters in hidden states, this instance should be beyond the knowledge scope of the PTLM. In Figure 3, we illustrate the above intuitions for different cases. In other words, akin to the observations in [16; 28], we view the built-in knowledge of PTLM as a representational power that enables the deep models to learn more separable features. Based on such observations, we develop a knowledgeability scoring function for a PTLM by measuring how well it can separate samples in its hidden states.

We now proceed to design \(s(q)\) that scores each query \(q\) from a given downstream task \(\). To begin with, we first collect a small set of samples from task \(\) and compute their hidden state representations using the designated PTLM. (Empirically, we find that about 200 samples are sufficient in our experiments.) We denote such a representational mapping by a function \(f()\).1 For generation tasks, we treat all instances as having a single dummy label. Next, we group these embedding vectors according to their class labels as \(_{l}=\{(f(x_{i}),y_{i}) y_{i}=l\}\), where \(x_{i}\) and \(y_{i}\) denote the \(i\)-th instance and its corresponding class label, respectively, and \(l\) is the class index. We further cluster the samples in each \(_{l}\) into \(K\) clusters by applying the k-means algorithm to the vectors \(f(x_{i})\). For convenience, we introduce the notation \(_{kl}\) to represent the set of samples in the \(k\)-th cluster of class \(l\), and let \(m_{kl}\) be the centroid vector corresponding to \(_{kl}\).

Figure 3: The intuition behind the proposed _Thrust_, which are plotted in the hidden representation space of PTLM. We represent an incoming query instance by triangles and represent the instances used for constructing _Thrust_ scores by ticks and crosses. In the _controversial_ and _no knowledge_ cases, the internal knowledge is insufficient to answer the query successfully, and external knowledge is needed to facilitate PTLM. In contrast, if the model finds the query close to one of the clusters, internal knowledge should be sufficient to solve the problem so that external knowledge is unnecessary.

With the above notation, we define the _Thrust_ score for a given query instance \(q\) as:

\[s_{}(q)_{l=1}^{N}_{k=1}^ {K}_{kl}|}{\|d_{kl}(q)\|^{2}}(q)}{\|d_{kl}(q) \|},\] (1)

where \(d_{kl}(q) m_{kl}-f(q)\) is a vector pointing from \(f(q)\) towards the centroid \(m_{kl}\), \(N\) is the number of classes, \(K\) is the number of clusters per class, \(|_{kl}|\) denotes the cardinality of the set \(_{kl}\), and \(\|\|\) denotes \(_{2}\)-norm of a vector.

The design principlesNote that the expression inside the \(_{2}\)-norm of (1) can be viewed as a weighted average of the normalized (unit) vectors \(\{d_{kl}(q)/\|d_{kl}(q)\|\}\) that point from the query vector \(f(q)\) towards the centroid vectors \(\{m_{kl}\}\). The weighting is proportional to the cluster size and inversely proportional to the squared distance between the query and the centroid. Such a design choice is based on the following principles derived from the earlier representational assumption regarding knowledge. First, when samples from a task are well clustered and if \(q\) is close to one of the clusters while being farther away from others, it means that the query instance \(q\) can be well solved by the internal knowledge of PTLM and the thrust score should be high. Let \(m_{kl}\) be the cluster centroid that \(q\) is close to, then we observe that the corresponding \(\|d_{kl}\|^{2}\) term in the denominator will make the corresponding term dominate and large in (1). Second, if \(q\) is farther away from all the cluster centroids, i.e., the query is beyond the knowledge scope of the PTLM, then the quadratic term \(\|d_{kl}\|^{2}\) would quickly suppress all the terms in (1), making the thrust score vanish. Third, when the PTLM cannot sufficiently cluster the task samples in its hidden states, it means that the PTLM does not have sufficient knowledge to solve the entire task. In this case, the unit vector \(d_{kl}(q)/\|d_{kl}(q)\|\) would randomly point towards different directions so that the averaged vector inside the \(_{2}\)-norm of (1) diminishes. Finally, the main reason that we first aggregate the samples within each class into \(K\) clusters before computing the thrust score is that they may still be spread over multiple clusters even if they belong to the same class. The term \(|_{kl}|\) in (1) is used to upweight the vectors \(d_{kl}(q)/\|d_{kl}(q)\|\) that point to bigger clusters. Nevertheless, we find that \(K\) can be relatively small.2 In Section 4.1, we will conduct an experimental analysis to show that _Thrust_ score designed in the above manner is a good measurement of a PTLM model's knowledgeability. In addition, we also carry out extensive ablation studies to examine these design choices (see Appendix).

Practical considerationsAs we will show in Section 3, we only need about \(200\) samples from a task to form the clusters, and then we just need to store their corresponding centroids \(m_{kl}\) (typically less than \(50\) vectors of dimension \(300\)) for deployment. According to (1), computing the _Thrust_ score for a query \(q\) only needs to calculate the distance between \(f(q)\) and these centroids, which takes about \(0.001\) second per query on average in our experiments (see Appendix). Therefore, _Thrust_ is fairly lightweight and easy to be deployed in practical applications. The incurred extra computation complexity during the inference stage is \(O(NK)\). Since \(K\) and \(N\) are generally small, this overhead is negligible compared to retrieving knowledge from a large external memory for each instance.

## 3 Experiment

In our experiments, PTLMs are examined under two settings. (i) **Zero-shot**. We consider T5 , GPT-J , OPT  and present queries with or without knowledge directly to the PTLMs. (ii) **Transfer-learning**. We use UnifiedQA  models that fine-tune T5 on multiple QA datasets.

After studying which model performs the best in utilizing external knowledge (the pre-condition of introducing retrievers), we then evaluate the cost-effectiveness of the proposed **IAPEK** with _Thrust_ with the best performing models. We simulate the real-world usage scenario where we have limited bandwidth or budget to retrieve external knowledge. Specifically, we test on three scenarios based on the richness of available resources: _scarce_ (25%), _medium_ (50%), and _abundant_ (75%). For example, _scarce_ (25%) means that we set the threshold \(\) (defined in Section 2.1) to 25 percentile of the scores of the 200 examples used to set up the clusters. As introduced in Section 2, we use _Thrust_ to score and rank the instances by their need for external knowledge and select the instances that have a high demand for external knowledge (i.e., with low thrust scores). We compare the performance between _Thrust_ with two baselines, i.e., **IAPEK** (_Default_) that randomly samples X% (X=25, 50 or 75) examples to apply knowledge, and **IAPEK** (_BM25_) that ranks all instances by BM25  and select top X% (X=25, 50 or 75) difficult examples.

We follow previous work to report accuracy for MC classification tasks. For open-domain QA, we report the QA-F1 score that measures the max uni-gram overlap between the model prediction and all gold answer candidates following previous work . For the _without knowledge_ setting, we directly pass the prompt-decorated question query to the model and select the choice with the highest probability as the answer. For the _with knowledge_ setting, we append the knowledge piece after the prompt-decorated query and put "_Answer:_" at the end to pass to the models.

### Datasets

We consider several knowledge-intensive tasks in our evaluation, i.e., Multiple-choice Classification (MC), which consists of seven datasets, and Open-domain QA, which consists of five tasks. Each data instance consists of **a query** (a piece of text containing a question or the sentences to be classified) and **an answer** (either the label words or the answers to the questions in the query).

Additionally, each instance may also contain a piece of potentially helpful **knowledge** for the query, which is either inherently relevant due to the task design, annotated by humans, or retrieved from Wikipedia paragraphs with DPR. Details of the datasets and corresponding external knowledge are as follows.

**Multiple-choice classification.** For MC classification, each query \(q\) includes a sentence or a question and requires models to select the correct answer from a set of candidates. Specifically, (i) AGNews  asks the model to classify a piece of news into _political, sports, business,_ or _technology_. We regard the titles of the news as the queries since they may already contain sufficient information and use the content of the news as the gold external knowledge. (ii) e-SNLI  is a natural language inference (NLI) task exploiting the role of explanations in textual entailment. Human-providing explanations are considered a strong source of external knowledge; (iii) StrategyQA  is a challenging multi-hop reasoning dataset that requires models to answer creative questions through strategical inference from implicit reasoning steps. We regard the original questions as queries and human-written explicit facts as external knowledge; (iv) CIKQA  is a commonsense inference task that combines pronoun coreference resolution, commonsense QA , COPA , and questions mined from ATOMIC knowledge graph . We regard the original questions as queries and the supporting commonsense knowledge extracted from knowledge graphs (KGs) in the original work as the external knowledge; (v) BoolQ  contains encyclopedic questions that require models to answer _yes_ or _no_. Following , we use the Wikipedia paragraphs retrieved by DPR as the external knowledge, which can be potentially noisy; (vi) ARC-E & ARC-C : ARC is a challenging multiple-choice QA dataset that requires knowledge understanding and reasoning, which is partitioned to an Easy set (ARC-E) and a Challenge set (ARC-C), where the Challenge set questions are answered incorrectly by the retrieval-based or co-occurrence-based algorithms tested by the original authors. Similarly, we use the Wikipedia paragraphs retrieved by DPR as external knowledge.

**Open-domain QA.** For open-domain QA, each query \(q\) contains an open question that typically requires solving an encyclopedic or commonsense inference. The generated answers can either be a few phrases or a single sentence. An example question is "_What does a drink from Narcissus's Spring cause the drinker to do_" and the expected answer generated from the language model is "_fall in love with themselves_". The involved datasets are HotpotQA , Natural Questions (NQ) , Web Questions , Curated TREC , and TriviaQA . We use Wikipedia paragraphs retrieved by DPR as the external knowledge as a common practice , except for HotpotQA, where we use the passages that the queries are generated from as a gold knowledge resource.

The statistics of the involved datasets (e.g., query length and sizes of the splits) are reported in Appendix. We collect a benchmark with various datasets of different types, formats, and knowledge sources, where we will then evaluate the effectiveness of **IAPEK**. Some implementation details of each of the task are described in Appendix.

### Performance of Using External Knowledge

Figure 4 presents the model performance on both the MC classification and open-domain QA tasks3. For the MC classification tasks, we can observe that for the zero-shot setting (T5-X, GPT-J, and OPT), models do not consistently benefit from external knowledge. In addition, the ability to utilize external knowledge is also not improved as the parameter size grows, which indicates that simply using larger models may not be the solution for better using the knowledge. For the transfer-learning setting (UnifiedQA-X), although _AGNews_, _e-SNLI_, _CIKQA_, and _StrategyQA_ are not seen during the training of UnifiedQA models, we can observe that models achieve better performance than vanilla T5 models at different sizes. Under the _with knowledge_ case, the UnifiedQA models achieve significant improvement for utilizing external knowledge compared to the zero-shot models, _UnifiedQA-3b_ achieves the best performance on all the tasks, which indicates that models can learn and transfer the ability to utilize external knowledge with instances containing external knowledge. Moreover, for open-domain QA tasks, we can see models (T5, GPT-J, OPT) get no benefits from external knowledge in 11 out of 25 cases under the zero-shot setting, while UnifiedQA models achieve significant performance gain after adding external knowledge. External knowledge may introduce extra noise if the model does not learn to utilize knowledge, which indicates the importance of instructing PTLMs to learn how to use knowledge through second-stage fine-tuning.

In conclusion, we find that fine-tuning with instances containing external knowledge is an effective way to help models learn to use external knowledge. Since the pre-condition of using IAPEK is that the model can utilize external knowledge well, we conduct experiments with UnifiedQA only when evaluating the performance of _Thrust_.

Figure 4: Performance of various models on MC classification tasks (accuracy) and open-domain QA tasks (QA-F1), denoted by (cls) and (qa), respectively. The x-axis represents the model names, which are shared across sub-figures. Use knowledge: yes or no denotes using full knowledge or not for all queries. UnifiedQA denotes T5 models with different sizes fine-tuned on the UnifiedQA dataset.

### Performance of _Thrust_

Table 1 shows the results of UnifiedQA after adding knowledge to X% (X=25, 50 or 75) that needs knowledge most according to our _Thrust_ score. We can see that _Thrust_ consistently contributes to the performance from the base to the 3B model. Through clustering the instances, we acquire the whole instance distribution in the eyes of the models. Then with distance to the cluster, _Thrust_ represents how well the model can categorize a new query vector and find its similarity with others on the task. Leveraging such information, _Thrust_ identifies the _no knowledge_ and _controversial knowledge_ cases well and puts the knowledge into the most necessary ones.

Additionally, the gain is higher when the portion of augmented instances is smaller. For instance, for UnifiedQA-3b, the gains from _Thrust_ with the _scarce_ case are 6.1%, 13.56% on MC classification and QA tasks, respectively, while for the _abundant_ case, the gains are 2.8% and 6.8%. Such observation shows that _Thrust_ is most effective in identifying the most necessary cases. One potential reason is that _Thrust_ is sensitive to the distance change, so the isolated instances (_no knowledge_ case in Figure 3) can be easily identified. With _Thrust_ the performance of cases using fewer resources can sometimes surpass that with high resource requirement for **Default**, e.g., for UnifiedQA-base on all classification tasks except ARC-C, _Thrust_(scarce) performs better than **Default**(medium). Interestingly, we also observe consistent failure cases on ARC-C. This is because the queries are designed as open questions, and the answers are usually about plans or opinions instead of facts. Thus, it is hard for small models to extract useful information from Wikipedia documents.

## 4 Analysis

### Primary Study: Distribution of _Thrust_ scores

To investigate whether _Thrust_ is a good measurement of models' knowledgeability, we plot in Figure 5 the distribution of _Thrust_ scores with the strongest inference model evaluated (i.e., UnifiedQA-3b)4. We use Kernel Density Estimation to smooth the distribution. From the distribution, we can see that low _Thrust_ scores (i.e., the query needs external knowledge) frequently appear in most query instances of the open-domain QA tasks such as _HotpotQA_, _TriviaQA_ and _NQ_, which are designed to solve with external knowledge. On the other hand, for _e-SNLI_ and _BoolQ_, external knowledge is not always necessary, which is consistent with the design purpose of these tasks. To conclude, by correctly predicting whether a certain task needs external knowledge (e.g., open-domain QA tasks), _Thrust_ score is shown to be a good measurement of a PTLM model's knowledgeability.

    &  &  &  \\  &  &  &  &  &  &  &  &  &  \\  AGNnews & 50.7 & \(55.6\) & 52.8 & \(56.3\) & 55.0 & 56.8 & 70.2 & 69.1 & 69.4 & 70.2 & 68.7 & 70.6 & 77.9 & 78.4 & 80.1 & 80.4 & 82.3 & 82.3 \\ e-SNLI & 46.5 & \(66.6\) & 54.4 & \(68.3\) & 62.3 & 69.6 & 50.7 & 71.1 & 58.5 & 72.2 & 66.4 & 73.2 & 69.1 & 86.3 & 75.9 & 87.5 & 82.8 & 88.8 \\ CIQA & 56.9 & \(59.6\) & 57.8 & \(59.6\) & 58.9 & 58.9 & 60.2 & 62.1 & 60.8 & 62.3 & 61.5 & 62.4 & 62.7 & 69.9 & 61.1 & 66.6 & 65.5 & 66.9 \\ StateyQA & 50.7 & \(55.6\) & 52.8 & \(56.3\) & 55.0 & 58.9 & 62.2 & 62.1 & 57.4 & 65.3 & 61.9 & 65.9 & 64.1 & 74.3 & 70.5 & 81.4 & 70.7 & 82.9 \\ BooIQ & \(65.5\) & \(76.2\) & 70.7 & \(79.9\) & 75.8 & 80.9 & 65.9 & 77.7 & 72.1 & 81.3 & 78.3 & 84.4 & 68.1 & 79.1 & 74.6 & 85.7 & 81.2 & 87.1 \\ ARC-C & 50.7 & \(55.6\) & 52.8 & \(56.3\) & 55.0 & 56.8 & 64.5 & 64.6 & 65.0 & 64.7 & 65.5 & 65.1 & 74.4 & 74.6 & 75.1 & 74.9 & 75.8 & 75.1 \\ ARC-C & 44.9 & \(43.8\) & 45.0 & \(44.5\) & 45.1 & 44.8 & 53.8 & 50.8 & 52.3 & 51.2 & 50.9 & 51.5 & 64.5 & 63.9 & 64.1 & 64.9 & 64.3 & 65.6 \\  WQ & 19.2 & \(26.3\) & 27.5 & \(42.1\) & 35.8 & 43.8 & 22.5 & \(38.5\) & 30.5 & 39.0 & 38.5 & 46.0 & 20.9 & 19.3 & 30.0 & 35.4 & 39.1 & 46.4 \\ TREC & 13.5 & \(33.6\) & 21.3 & \(36.4\) & 29.1 & 36.9 & 30.8 & 32.7 & 32.7 & 36.0 & 34.6 & 36.3 & 19.6 & 37.8 & 27.0 & 40.6 & 34.4 & 40.9 \\ HotpotQA & 25.2 & \(32.9\) & 30.2 & \(35.5\) & 32.7 & 37.8 & 26.7 & 35.2 & 32.1 & 37.5 & 37.4 & 40.2 & 24.9 & 41.9 & 32.3 & 43.9 & 39.7 & 45.7 \\ TriviaQA & 32.0 & \(52.7\) & 43.2 & \(56.4\) & 54.4 & 60.0 & 32.4 & 59.7 & 46.4 & 64.3 & 60.5 & 71.8 & 39.2 & 68.3 & 52.8 & 71.0 & 66.4 & 73.4 \\ No & 20.0 & \(33.0\) & 24.9 & 33.5 & 29.7 & 33.9 & 12.0 & 34.8 & 20.1 & 35.2 & 28.2 & 35.7 & 12.8 & 35.9 & 21.1 & 36.5 & 29.4 & 37.0 \\   

Table 1: Performance of **IAPEK** based on _Thrust_. As defined in Section 3, performances of _Default/Thrust_ are presented before/after the vertical bar for _scarce_, _medium_, and _abundant_ cases. If performance increases with _Thrust_, the score will be marked in green and otherwise in red. WQ and TREC denote the tasks of Web Questions and Curated TREC, respectively.

### IAPEK-Thrust versus IAPEK-BM25

We use BM25 , a common approach to evaluating the difficulty of queries, as an alternative to _Thrust_ to perform **IAPEK**. Specifically, we regard each test input as the query and all training data input as the corpus to extract the score. We use the average relevance score across the corpus to rank each test input. From Table 2, We can observe **IAPEK** performs well with BM25 as the difficulty score on QA tasks. Except WQ and NQ, we observe better performance (marked in green) than the default setting. However, _Thrust_ shows larger (e.g., for QA tasks) and more robust improvement (e.g., for classification tasks) than the BM25 baseline.

### Layer ablation

Since we cast instances into the representation space, a crucial factor for _Thrust_ is the layer of the PTLMs to use. To investigate the effect of using different layers, we conduct experiments on UnifiedQA-3b with the same setting as in Section 3. Figure 6 presents the performance of adding 25%, 50%, and 75% knowledge-augmented instances with _Thrust_ with hidden states of different layers. We can observe that, for most tasks, there is no significant difference across layers, which shows the robustness of _Thrust_ and potential to accelerate the computation by using early layers. However, for _StrategyQA_ and _Web Questions_, the middle-layer representation may worsen the overall performance. One possible reason is that early layers in the model contain rich semantic information, and later layers contain task-specific information , so both can act as good representations of the instances. However, in the middle layers, rich semantic features are abandoned during extracting task-specific features.

   &  &  \\  &  scarce \\  } &  medium \\ abundant \\  } &  scarce \\  } &  medium \\  } &  abundant \\  } \\  AGNews & 77.9 & 77.0 & 80.1 & 79.2 & 82.3 & 81.3 & 77.9 & 78.4 & 80.1 & 80.4 & 82.3 & 82.3 \\ e-SNLI & 69.1 & 68.4 & 75.9 & 75.6 & 82.8 & 83.0 & 69.1 & 86.3 & 75.9 & 87.5 & 82.8 & 88.8 \\ CIKQA & 62.7 & 62.3 & 64.1 & 64.4 & 65.5 & 66.1 & 62.7 & 66.9 & 64.1 & 66.9 & 65.5 & 66.9 \\ StrategyQA & 64.1 & 63.3 & 70.5 & 68.3 & 77.0 & 78.1 & 64.1 & 74.3 & 70.5 & 81.4 & 77.0 & 82.9 \\ BoolQ & 68.1 & 68.4 & 74.6 & 75.8 & 81.2 & 82.0 & 68.1 & 79.4 & 74.6 & 85.7 & 81.2 & 87.1 \\ ARC-E & 74.4 & 74.9 & 75.1 & 75.3 & 75.8 & 76.3 & 74.4 & 74.6 & 75.1 & 74.9 & 75.8 & 75.1 \\ ARC-C & 64.5 & 65.2 & 64.4 & 66.2 & 64.3 & 66.6 & 64.5 & 63.9 & 64.4 & 64.9 & 64.3 & 65.6 \\ WQ & 20.6 & 19.0 & 30.0 & 28.2 & 39.1 & 37.3 & 20.9 & 19.3 & 30.0 & 35.4 & 39.1 & 16.4 \\ TREC & 19.6 & 20.4 & 27.0 & 28.1 & 34.4 & 36.3 & 19.6 & 37.8 & 27.0 & 40.6 & 34.4 & 40.9 \\ HgspotQA & 24.9 & 25.2 & 32.3 & 32.8 & 39.7 & 40.6 & 34.9 & 41.9 & 32.3 & 43.9 & 39.7 & 45.7 \\ TriviaQA & 39.2 & 34.2 & 52.8 & 50.0 & 66.4 & 65.4 & 39.2 & 26.8 & 52.8 & 71.0 & 66.4 & 73.4 \\ NQ & 12.8 & 12.9 & 21.1 & 21.1 & 29.4 & 29.6 & 12.8 & 35.9 & 21.1 & 36.5 & 29.4 & 37.0 \\  

Table 2: Performance of **IAPEK** for UnifiedQA-3b based on _Thrust_ and BM25. With _scarce_, _medium_, and _abundant_, performances of **Default/BM25** and **Default/_Thrust_ are presented before/after the vertical bar. If performance increases with _Thrust_, the score will be marked in green, otherwise red.

Figure 5: Distribution of _Thrust_ scores for various tasks by using UnifiedQA-3b to create the hidden representations. Kernel Density Estimation is used to smooth the distributions. Low scores imply that the instances are less likely to be solved with internal knowledge and vice versa. _Thrust_ scores predict that most query instances from open-domain QA tasks require external knowledge while others need less. The results are consistent with the original design purposes of these tasks.

### Comparison with Full Knowledge Usage

We denote simply using external knowledge for all instances as a costly but straightforward way of leveraging external knowledge. Since the big models might be sufficient for certain instances and the external knowledge might introduce extra noise, _Thrust_ can help identify instances requiring (or not) knowledge and achieves higher overall performance on the whole dataset compared to seeking and adding knowledge indiscriminately. Table 3 presents the comparison between adaptive and indiscriminate knowledge propulsion. _Thrust_ here denotes the best performance achieved when less than 90% of instances use external knowledge. We can observe that, for 2/3 tasks for UnifiedQA-base and UnifiedQA-3b, _Thrust_ achieves better performance or less than 1% drop than using knowledge for all instances. Such results illustrate that _Thrust_ can help avoids potential noise. On the other hand, we can also observe that for _e-SNLI_, _TREC_, and _NQ_, the full knowledge setting performs better than _Thrust_. It means retrieving and adding high-quality knowledge always benefits the models as long as they have been fine-tuned to know the usage of external knowledge.

## 5 Related Work

**PTLM with external knowledge.** The paradigm of retrieving knowledge from knowledge bases, augmenting PTLMs, and solving downstream tasks has been widely explored in the community of NLP [29; 4; 15; 39; 48]. The knowledge bases can range from knowledge graphs , documents , pre-processed vectors , other PTLMs , search engines , to Wikipedia documents as used in this work. To augment the PTLMs, common practice includes creating synthesizing datasets , adding knowledge to the prompts [55; 36], create demonstrations , and extending feature vectors . The contribution of **IAPEK** is orthogonal to the above work since it presents a gated

   Size &  &  \\   &  & CIKQA(r)* & AGNew(g)* & StrategyQA(h)* & e-SNLI(h) & TREC(r) & HotpotQA(g) & NQ(r) \\  & ARC-E(r) & ARC-C(r)* & WQ(r) & TriviaQA(r)* & & & & \\   &  & CIKQA(r)* & AGNew(g) & StrategyQA(h) & e-SNLI(h) & TREC(r) & WQ(r) & NQ(r) \\  & ARC-E(r) & ARC-C(r)* & TriviaQA(r) & HotpotQA(g)* & & & & \\   

Table 3: Comparison between IAPEK-_Thrust_ and the costly full knowledge usage based on UnifiedQA models of different sizes. 99% Full denotes 99% performance of models with full knowledge. The knowledge type is noted in brackets, where \(g\) denotes gold knowledge, \(h\) denotes human annotations, and \(r\) denotes the knowledge retrieved from Wikipedia or knowledge graphs. WQ and TREC stand for Web Questions and Curated TREC, respectively. If _Thrust_ achieves better performance than using full knowledge, we mark the entry with *.

Figure 6: Layer-wise ablation across tasks and portions of instances augmented with knowledge for (a) MC classification tasks and (b) open-domain QA tasks. The x-axis denotes the layer index of Unified-3b decoder that is used to obtain the hidden representations (i.e., \(f()\) in (1)). For most tasks, the results are not sensitive to the specific layer index. For some tasks (e.g., StrategyQA), choosing middle layers for representation slightly degrades the performance.

framework to reject external annotations or retrieval that can be extended to the above frameworks. The extension is lightweight since _Thrust_ requires queries only, not labels nor gold answers.

**Hardness and Confidence Estimation in PTLMs.** Much previous work studies the estimation of dataset hardness and model confidence under the context of PTLMs. For dataset hardness, previous research discovers using the cumulative area under the loss curves (RDA ), entropy difference between the trial and null cases (\(\)-Usable information ), and variance of losses of neighbor sentences (Sensitivity Measurement ) These methods achieve great correlation with the model performance when analyzing the test set. However, the test set labels are required and can not be applied when predicting the answers. Another line of work focuses on estimating the expected calibration errors (ECE) for classification , QA , and math  datasets, as a reflection of model certainty on the correct answers. ECE can be considered an orthogonal evaluation metric to measure the model's capability of understanding the tasks, compared to common metrics such as accuracy.

Most previous work can be considered a posterior analysis of the model capability. In this work, instead, we estimate the pragmatic confidence at the test time to empirically increase the performance with a limited budget or bandwidth to acquire knowledge.

## 6 Discussion

### Extended Usage

We expect the idea of adaptive knowledge injection to be extendable beyond QA and MC questions, such as trail prediction tasks including ECBD  or EKP . By design, Thrust is independent of the type of external knowledge, so that the adaptively used external knowledge can be any sort, for example, definitions in EKP. Furthermore, Thrust can also be used as a way to measure the expected performance without extensive fine-tuning, as shown in Table 5 in the appendix.

On the other hand, we present our pioneer study on instruction fine-tuned model Flan-T5  in Section A.4 in the appendix and show that _Thrust_ performs better on CIKQA with Flan-T5 than vanilla T5, where the best performance is achieved with 40% examples not using external knowledge.

We also believe future work can be done on how _Thrust_ collaborates with different kinds of prompts to include the knowledge or prefix tuning. We discuss the limitations of our current design (i.e., cold start, assuming white-box language models, and extendability on other kinds of retrieval argumentation) in Section A.1 in the appendix.

### Time Sensitivity

We regard time sensitivity as a part that can be done in the **IAPEK** framework, but not by _Thrust_, as the framework is motivated by both noise and staticity issues. Another orthogonal kind of score measuring time sensitivity can be designed to decide if updated knowledge retrieval is necessary, for example, based on .

## 7 Conclusion

In this work, we propose Instance-level Adaptive **P**ropulsion of **E**xternal **K**nowledge **(IAPEK)** as a solution to propel model performance with external knowledge. Accordingly, we propose a simple and effective instance-level metric, _Thrust_, to perform the adaptive knowledge injection. Extensive experiments show that _Thrust_ is a good indicator of models' knowledgeability and can improve the performance of utilizing external knowledge under various settings. Understanding the delicate usage of potentially noisy knowledge for PTLMs can further enable the models to conduct inference beyond the limitation of internal knowledge.

Acknowledgements

We are grateful to Sherry Tongshuang Wu, Shikhar Murty, Zhengxuan Wu, Ashwin Paranjape, John Hewitt, Christopher Manning, Eric Mitchell, Sihao Chen, Ben Zhou, Tong Chen, and the anonymous reviewers for their helpful and insightful comments.