# Towards the Universal Learning Principle for Graph Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Graph neural networks (GNNs) are currently highly regarded in graph representation learning tasks due to their significant performance. Although various propagation mechanisms and graph filters were proposed, few works have investigated their rationale from the perspective of learning. In this paper, we elucidate the criterion for the graph filter formed by power series, and further establish a scalable regularized learning framework that theoretically realizes very deep GNN. Following the framework, we introduce Adaptive Power GNN (APGNN), a deep GNN that employs exponentially decaying weights to aggregate graph information of varying orders, thus facilitating more effective mining of deeper neighbor information. Moreover, the multiple \(P\)-hop message passing strategy is proposed to efficiently perceive the higher-order neighborhoods. Different from other GNNs, the proposed APGNN can be seamlessly extended to an infinite-depth network. To clarify the learning guarantee, we theoretically analyze the generalization of the proposed learning framework via uniform convergence. Experimental results show that APGNN obtains superior performance compared to state-of-the-art GNNs, highlighting the effectiveness of our framework.

## 1 Introduction

Recently, Graph Neural Networks (GNNs) have shown commendable performance on numerous graph representation learning tasks. In addition, GNNs have been introduced in a variety of application tasks, such as recommendation systems [7; 11; 37], computer vision [4; 13; 23], and traffic forecasting [8; 9]. The fundamental part of GNN is the design of the propagation mechanism or the graph filter [5; 12; 27; 32; 34]. GNNs can be categorized into two groups based on the approach of formulation. Spatial-based GNN formulates propagation mechanisms through the direct aggregation of spatial features. As one of the most simple GNNs, Graph Convolutional Network (GCN)  designs graph convolutional layer via aggregating one-hop information on the graph. Graph Attention Network (GAT)  learns node relationships using an attention mechanism, enhancing the scalability of the network. For extension of inductive learning, GraphSAGE  employs various pooling operations as aggregation functions. Liu et. al proposed DAGNN, which integrates information from multiple receptive fields for adaptive propagation . Spectral-based GNN designs graph filters by constructing filter functions in the graph Fourier domain, which aims to find a proper transformation of the graph spectrum. Chebynet constructs the localized graph filter with Chebyshev polynomial . From the view of the spectrum, GCN could be seen as a Chebyshev filter with first-order truncation . To construct deeper GNN, Personalize PageRank method is employed to design graph filter . GNN-LF/HF  concludes various designs of graph filters and constructs the graph filter through a graph optimization framework.

Despite their success, few studies have explored the general rule for devising GNNs from the perspective of learning. In this paper, we start from the graph filter formed by power series anddiscuss what makes a legitimate graph filter for the construction of deep GNN. A learning principle is then proposed to summarize the rule of formulating a graph filter. Following this, we propose Adaptive Power Graph Neural Network (APGNN), which adaptively learns the task-specific graph filter for node representation learning. The main idea of APGNN is depicted in Figure 1. The parameterized graph filter is designed with regularization of the exponential decay rate. A multiple \(P\)-hop strategy is applied to enhance the capacity of perceiving the higher-order neighborhoods. Furthermore, the generalization bound of APGNN is presented with the setting of the continuous graph, which provides a learning guarantee for the proposed principle theoretically.

We conduct evaluations on five benchmark datasets on node classification tasks. The experimental results suggest the superiority of the proposed method over the existing GNNs. The theoretical analysis is also validated via the empirical study.

## 2 Preliminaries

**Notations.** Suppose we have an undirected graph \(=(,,)\) with node set \(\) and \(||=n\). \(^{n n}\) denotes the adjacency matrix indicating the edges in \(\). Assuming that the self-loops are contained in the graph, i.e., \(a_{ii}=1\). Let \(=[_{1},_{2},,_{n}]^{} ^{n d}\) be the graph signals (or features) of the nodes. We use notation \([n]\{1,2,,n\}\) for \(n_{+}\). Assume that the label of \(_{i}\) is \(y_{i}\) for all \(i=[n_{l}]\), where \(n_{l} n\) is the number of labeled samples.

**Graph Neural Networks.** We introduce some essential concepts in GNNs. Let \(d_{i}=_{j=1}^{n}A_{ij}\) be the degree of \(i\)-th node, so the degree matrix of \(\) can be defined as \(=(d_{1},d_{2},,d_{n})\). The symmetrically normalized Laplacian is \(=-}\), where \(}^{-1/2}^{-1/2}\) is normalized adjacency matrix. Consider the eigen-decomposition \(=^{}\), where \(=(_{1},,_{n})\) is the diagonal matrix of eigenvalues, and \(=[_{1},,_{n}]\) represents the eigenvectors associated with the eigenvalues. Note that \(}\) shares the same eigenvectors with \(\).

Spectral convolution on graphs is defined as the following transformation [15; 28]:

\[g*=g()^{},\] (1)

where \(g():\) is called filter function and \(g()=(g(_{1}),,g(_{n}))\). The common approach in GNNs is to apply polynomial functions as the filters [3; 12; 15], which leads to \(g()^{}=g()\). Therefore, spectral convolution is usually written as \(g*=g()\). The graph representation paradigm in GNN is generally expressed as follows:

\[=g()f(), g()=_{k=0}^{K} _{k}}^{k},\] (2)

where \(^{n c}\) denotes the node representation, and \(f()\) represents a feature extractor such as multi-layer perceptions (MLPs).

Figure 1: An illustration of the proposed APGNN that adheres to the learning principle. The model incorporates the decay rate \(\) to suppress the information from high-order neighbors while adaptively learning bounded coefficients \(\). Furthermore, it aggregates information with \(P\)-hop to enlarge the receptive field. This design enables the seamless extension of APGNN to an extremely deep network.

Learning Principle for GNNs

### The principle of devising graph filters

Current studies suggest a significant relationship between the performance of GNN and its graph filter . Predominantly, the general graph filters are characterized by polynomials associated with the adjacency matrix \(}\) (or Laplacian matrix \(\)), i.e., \(g()=_{k=0}^{K}_{k}}^{k}\). However, the existing methods still meet the issue that the depth of GNN is limited. The reason for this phenomenon is that these GNNs are inconsistent with their "infinite-depth" version. That is, the corresponding models lose some essential properties as the depth \(K\). Consequently, the depth of the models is restricted. To address this issue, it is necessary to study the properties of GNNs with infinite depth. Therefore, we explore the graph filter reformulated as power series:

\[g(})=_{k=0}^{}_{k}}^{k}= _{k=0}^{}_{k}(-)^{k}.\] (3)

First of all, a well-defined graph filter represented as equation 3 must be convergent. Consequently, it becomes essential to investigate the properties that the coefficients \(_{k}\) should exhibit. The following lemma provides appropriate constraints for the coefficients of the graph filter.

**Lemma 1**.: _Let \(\{a_{k}\}\) and \(\{^{k}\}\) be the real number sequences, where \((-1,1]\) and \(k\). Then \(_{k}^{}a_{k}^{k}\) converges uniformly and absolutely if and only if the series \(_{k}^{}a_{k}\) converges absolutely._

As a direct corollary, the weights of the graph filter (i.e., \(_{k}\)) should satisfy the following theorem.

**Theorem 1**.: _Let \(}=^{-1/2}^{-1/2}\) be the normalized adjacency matrix of a graph \(\) with spectral radius \((}) 1\). The matrix series \(_{k=0}^{}_{k}}^{k}\) converges uniformly and absolutely if and only if the series \(_{k=0}^{}_{k}\) converges absolutely._

The proofs are shown in Appendix. Theorem 1 offers a sufficient and necessary condition for the convergence of graph filters formed by power series. Specifically, the condition requires the existence of a finite real number \(M 0\),

\[\|\|_{1}_{k=0}^{}|_{k}| M.\] (4)

Therefore, an arbitrary graph filter formed by power series should satisfy the above convergence condition, which gives the first requirement while designing GNN. Apart from convergence, we expect the graph filter to possess good analytic properties such as smoothness. To this end, Lipschitz continuity should be considered the second requirement of the graph filter. Let \(g()\) be an \(L\)-Lipschitz continuous function, meaning that

\[|g()-g(^{})| L|-^{}|, ,^{}[0,2).\] (5)

This property indicates the stability or robustness of the model . If the graph is contaminated and its eigenvalues are perturbed by at most \(\), Lipschitz continuity ensures the perturbation of the graph-filtered result is at most \(L\). For instance, considering \(g()=_{k=0}^{}(1-)^{k}/k^{2}\), which is convergent, yet the Lipschitz condition does not satisfy for \(\) closed to zero. Therefore, this graph filter might be sensitive to the input graph. Subsequently, we conclude the following criterion.

\[=g_{}()f(),\| \|_{1} M,\ g_{}()\] (6)

To enhance the scalability of the model, we define \(\) as a learnable parameter (though its dimension is infinite). In this way, (6) gives a regularized learning framework for GNN. Therefore, for a \(K\)-order polynomial graph filter \(g_{}^{K}()=_{k=0}^{K}_{k}(1-)^{k}\), which is what we can implement in practice, the condition (6) should be satisfied to keep the consistency with its infinitely deep version \(g_{}^{}()=_{k=0}^{}_{k}(1- )^{k}\). We will present the applications of this criterion in this section, and further analyze the learning guarantee with generalization in section 4.

### Related works

In this subsection, we investigate the relationship between our learning framework and several well-known Graph Neural Networks (GNNs), focusing on the design of graph filters. Our findings indicate that these GNNs are all special cases of our learning framework, which are summarized in Table 1.

**GCN/SGC [15; 33].** Graph convolutional network (GCN) aims to learn a node representation by stacking multiple graph convolutional layers. In each layer, GCN applies first-order Chebyshev approximation as the graph filter followed by a fully connected layer. For simplicity, we analyze one-layer GCN, which is formulated as \(=(})\), where \(\) is a learnable weight matrix for linear transformation. Therefore, the graph filter of one-layer GCN is \(g_{}(L)=-=}\), or a trivial matrix power series:

\[g_{}()=_{k=0}^{}_{k}}^{ k},_{k}=1,&k=1,\\ 0,&\] (7)

It should be noted that this equation satisfies the condition described in (6).

SGC is a simplified version of GCN that eliminates the activation function and applies a single linear projection to extract features. This simplification reduces the multiple-layer GCN into a more concise model as \(=}^{K}\). Similarly, the graph filter of SGC can be represented as:

\[g_{}()=}^{K}=_{k=0}^{} _{k}}^{k},_{k}=1,&k=K,\\ 0,&\] (8)

Both GCN and SGC use a monomial to construct the graph filter. Therefore, in the viewpoint of spectral-GNN, their graph filters are too simple to capture the spectral characteristic. Besides, the small eigenvalue vanishes when \(K\) becomes very large, leaving only the largest eigenvalue, which leads to the well-known over-smoothing problem .

**PPNP .** PPNP uses Personalized PageRank as the graph filter, which balances local information preservation and the utilization of high-order neighbor information. The model of PPNP is \(=(-(1-)})^{-1}=( +)^{-1}\), where \(=f()\) is a two-layer MLPs and \(=1/-1\). Hence, the graph filter of PPNP is \(g_{}()=(+)^{-1}\). Considering its Taylor series, we have

\[g_{}()=(+)^{-1}=_{k=0}^{}()^{k}}^{k}=_{k=0}^{}_{k}},\] (9)

where \(_{k}=^{k}/(1+)^{k+1}\). It is straightforward to validate that \(_{k=0}^{}_{k}=1\), and thus the convergence requirement (4) holds. Moreover, the Lipschitz condition is easily verified. Thus PPNP satisfies the criterion of (6). However, the performance of PPNP is heavily dependent on the hyperparameter \(\), which must be carefully tuned to achieve optimal performance.

**DAGNN .** DAGNN adaptively adjusts the weight of information aggregation from different neighbors to solve the over-smoothing problem. It designs a parameterized graph filter formulated as a \(K\)-order polynomial:

\[g_{}()=_{k=0}^{K}_{k}}^{k},0_{k} 1,\] (10)

where \(_{k}\) is the learnable parameter with bounded constraint. Due to this adaptive learning strategy, DAGGN is able to learn a graph filter more suitable for node classification. The empirical studies suggest DAGNN works well with a proper \(K\). However, as \(K\), the constraint \(0_{k} 1\) cannot guarantee the convergence of the graph filter. It indicates that DAGNN is "inconsistent" with its infinitely deep version. Therefore, it can not be naturally extended to significantly deep GNN.

### Instantiation: Adaptive Power Graph Neural Network

We now introduce a novel GNN following the framework in section 3.1, called Adaptive Power GNN (APGNN). We first consider the following graph filter parameterized by \(\) with the form:

\[g_{}^{}()=_{k=0}^{}_{k}^{k}(1- )^{k},|_{k}| 1,\ 0<<1,\] (11)

where the coefficient of the power series \(_{k}=_{k}^{k}\), with hyper-parameter \((0,1)\) ensuring the convergence. Immediately, we check the condition of Lemma 1.

\[\|\|_{1}=_{k=0}^{}_{k}^{k} _{k=0}^{}^{k}.\] (12)Hence, the power series converges on \(\) absolutely and uniformly. Similarly, the associated matrix series \(g_{}^{}()=_{k=0}^{}_{k}^{k}}^{k}\) also converges uniformly and absolutely by Theorem 1. Moreover, \(g_{}^{}()\) is \((1-)^{-2}\)-Lipschitz. To see this, for any \(|_{k}| 1\) and \(1-(-1,1]\), we have

\[| g_{}^{}()|=|_{k=1}^{}(-1)^{k}k _{k}^{k}(1-)^{k-1}|_{k=1}^{}k^{k}= },\] (13)

which implies the Lipschitz continuous property. Thus, this graph filter fits the requirement of the proposed criterion. However, the model with this graph filter is unavailable in practice as the number of parameters to be learned is infinite. The \(K\)-order truncated polynomial is utilized for substitution, i.e., \(g_{}^{K}()=_{k=0}^{K}_{k}^{k}}^{k}\). We evaluate the approximation via the upper bound of \(K\)-order truncation error:

\[|g_{}^{}()-g_{}^{K}()|_{k=K+1} ^{}|_{k}^{k}(1-)^{k}|_{k=K+1}^{ }^{k}=}{1-},\] (14)

which uniformly holds for \(\). Likewise, the approximation error of matrix series is given by

\[\|g_{}^{}()-g_{}^{K}() \|_{2}=\|(g_{}^{}( )-g_{}^{K}())^{}\|_{2}= _{i[n]}|g_{}^{}(_{i})-g_{}^{K}( _{i})|}{1-},\] (15)

where \(_{i}\) denotes the \(i\)-th eigenvalue of \(\). This upper bound is independent of the given graph, which can be controlled via tuning \(\) and \(K\). The higher \(K\) and smaller \(\) yield a better approximation to the exact graph filter \(g_{}^{}()\). Nevertheless, the small \(\) tends to limit the capability of the graph filter. Extremely, \( 0\) gives a trivial function \(g_{}^{K}()=_{0}\). This suggests that \(\) should be elaborately tuned to improve the performance.

Though the aforementioned graph filter is primarily motivated via spectral analysis, we can still present the spatial perspective explanation for its design. Existing GNNs aggregate the neighbor information of different hops with certain weights, which could be either manually assigned or learned adaptively. Typically, methods like GPR-GNN  and DAGNN  that learn the aggregation weight, tend to treat the neighbor's information of different hops equally. That is, the \(k\)-hop's weight are assigned with \(_{k}=(1)\) for each \(k[K]\). However, it is shown in the previous research that the propagation with the very high-order neighbor potentially leads to the over-smoothing issue [25; 33]. The current methods magnify this flaw of the high-order graph since they cannot distinguish the significance of the information of different hops. This motivates the design of the decay rate in APGNN, i.e., we employ weights with exponential decaying rate by assigning \(_{k}=(^{k})\) for some \(0<<1\). This approach emphasizes the contribution of lower-order neighbors and restricts the over-weighting of the information from high-order neighbors due to \(_{k} 0\) with \(k\). Therefore, it provides more effective aggregation and thus enhances the model's scalability.

To take a further step in the construction of a deep GNN, we introduce a multiple \(P\)-hop strategy for the graph filter of (11), which effectively extends the utmost neighborhood range that the graph filter can perceive by \(P\) times. Consider a different perspective regarding the construction of a filter with

   Model & Filter function & Setting of \(\) & Learnable \(g()\) \\ 
1-layer GCN & \(g()=_{k=0}^{}_{k}}^{k}\) & \(_{k}=1,&k=1\\ 0,&\) & No \\ SGC & \(g()=_{k=0}^{}_{k}}^{k}\) & \(_{k}=1,&k=K\\ 0,&\) & No \\ PPNP & \(g()=_{k=0}^{}_{k}}^{k}\) & \(_{k}=}{(1+)^{k+1}},\;>0\) & No \\ DAGNN & \(g()=_{k=0}^{K}_{k}}^{k}\) & \(0_{k} 1\) & Yes \\   

Table 1: Graph filter for various GNNsthe utmost order \(T=KP\). The previous methods can be viewed as a one-hop graph filter by setting \(P=1\). For \(P>1\), the graph filter is able to aggregate information from a larger neighborhood in the same order. In addition, we will illustrate the advantages of this strategy from the perspective of generalization in the following section.

Summarizing the above analysis, we present the following comprehensive architecture of APGNN:

\[=g_{}()f(),\ \ \ \ \ f()=(),\ \ \ \ \ g_{}()=_{k=0}^{K}_{k}^{k}}^{kP}.\] (16)

In short, APGNN incorporates the benefits from the decay rate \(\) that exponentially suppresses the information of extremely high-order neighbors and the multiple \(P\)-hop strategy to enlarge receptive fields. These approaches make it possible to realize a sufficiently deep GNN.

## 4 Generalization analysis

The theoretical analysis of GNN's generalization is widely studied.  provides the generalization result of the algorithmic stability of GCN in the discrete graph setting. In contrast,  shows the convergence and stability guarantee over the random and continuous graph. In this section, we will present the uniform generalization bound of the proposed GNN learning framework under the continuous setup.

We first introduce some notations for later discussion. Denote \(\) as any samples from the input space \(\) (we generally set \(\) as a subset of \(^{d}\)). Let \(()\) be a probability measure defined over \(\). Assume \(x_{j}\) is the \(j\)-th coordinate of \(\) and \([x_{j}^{2}] c_{}^{2}\) for any \(j[d]\). To describe the graph relation between each pair \((,^{})\) over \(\), we define a continuous graph function \(A(,):_{+}\), and its corresponding degree function is

\[d(^{})=_{}A(,^{}) (^{}).\] (17)

Different from the setting of [18; 26], we assume \(0 A(,^{}) c_{U}\), and \(0<c_{L} d()\) for any \(,^{}\). Therefore, we can define the symmetric normalized graph:

\[(,^{})=,^{ })}{)d(^{})}}.\] (18)

Then the corresponding normalized Laplacian is \(L=I-\), where \(I\) indicates the identity operator over \(\). For a graph filter function \(g_{}()=_{k=0}^{K}_{k}(1-)^{k}\), graph convolution of the continuous graph is defined as the following integral operator:

\[g_{}Lf=_{k=0}^{K}_{k}^{k}f,\ \ \ f=_{}(,)f() (),\] (19)

where \(^{k}=^{k-1}\) denotes \(k\)-order composition of integral operator with \(^{0}=I\). Note we have \(_{k=0}^{K}_{k}\|\|\|\|_{1} M\) for any \(K\), indicating \(_{k=0}^{}_{k}\) is absolutely summable. This guarantees the existence of graph filter on the continuous graph when \(K\). For convenience in understanding, we provide the analysis on a simplified GNN, where we consider a semi-supervised learning task with two classes, i.e., \(y_{i}\{-1,1\}\), and utilize linear feature extractor \(f()=^{}\). Note that we can still extend our result for \(f()=()\) and multi-class cases using the techniques proposed in . With the above setting, the hypothesis set over is described as

\[_{}=\{h:h()=g_{}Lf( ),\ f()=,,\ \|\|_{2} B,\ \|\|_{1} M\}.\] (20)

However, the integral in each hypothesis \(h_{}\) is intractable since the underlying graph function and the data distribution are unknown. Therefore, we should use the "empirical version" of the hypothesis to estimate \(h_{}\). For this reason, we introduce the hypothesis set defined over the observed samples \(S\) and graph \(\):

\[_{S}=\{h:h(_{i})=_{j=1}^{n}g_{}()_{ij}_{j}^{},\ \ \ \|\|_{2} B,\ \|\|_{1} M\}.\] (21)Define the generalization error and the empirical error  as follows

\[R(h)=_{(,y)}[1_{yh() 0}],(h)=}_{i=1}^{n_{l}}(1,(0,1-y_{i}h(_{i}))).\] (22)

We have the following theorem on the generalization of the proposed learning paradigm.

**Theorem 2**.: _Suppose \(g_{}()\) is \(L_{M}\)-Lipschitz. Let \(h_{,}_{}\) and \(h_{,}_{S}\) share the same parameter \((,)\). Then there exists a constant \(C>0\) related to the graph function, with the probability at least \(1-\), the following inequality holds._

\[R(h_{,})(_{, })+2BMc_{}}}+BCL_{M}dc_{} }.\] (23)

The proof is given by excess risk decomposition, shown in Appendix. The notation "\(\)" denotes "less than or approximately equal to the right-hand side" and guarantees an approximation error of at most \((}})\) with a probability of at least \(1-()\). We remind readers the important difference between \(R(h_{,})\) and \((_{,})\). The former term measures the population error over the whole input space with the **continuous** graph filter \(g_{}L\). In contrast, \((_{,})\) is the empirical risk (i.e., training risk) on the sample set \(S\) with the **discrete** graph filter \(g_{}()\). \(h_{,}\) shares the same learning parameter with \(_{,}\). Therefore, the minimization of the right-hand-side of (23) w.r.t \((,)\) reduces the upper bound of the population error.

We observe the first term of generalization bound is of order \(((dn_{l}^{-1} K)^{1/2})\), which outlines the model's complexity. Although it becomes infinity when \(K\), the growth of this term is extremely slow as \(K\) increases. In practice, we generally set \(K<n\) since the neighbor information beyond \(n\)-hops is redundant, restricting the complexity away from infinity. Therefore, the generalization of the model is rigorously guaranteed for sufficiently large \(K\), which allows us to construct significantly deep GNN in the proposed framework. We can obtain a more precise estimation for a certain model. In the following proposition, we unveil the generalization of APGNN as a direct application of Theorem 2.

**Proposition 1**.: _Let \(^{K}\) and \(g_{}^{K}()=_{k=0}^{K}_{k}^{k}(1-)^{k}\) where \(0<<1\) and \(\|\|_{} 1\). with the probability at least \(1-\), the following inequality holds._

\[R(h_{,})(_{,})+ }(1-^{K})}{1-}}}+}}{(1-)^{2}}}.\] (24)

Proof.: This is a direct result with \(M=(1-^{K})/(1-)\) and \(L_{M}=/(1-)^{2}\) in Theorem 2. 

In (24), the complexity term becomes \(((1-^{K}))\) with \(K= T/P\), which is relatively tighter than \(()\). For this term, we promote further discussion with \(P\)-hop. Since it takes \( T/P\) steps to reach the \(T\)-order graph, the term becomes \(((1-^{ T/P}))\). It is observed that the term decreases as \(P\) increases. Therefore, the appropriate \(P\) reduces the bound, explaining the mechanism of the \(P\)-hop method. On the other hand, larger \(\) leads to a higher bound. From the point of spatial view, the information from high-order neighbors is underused, which limits the range of the graph filter. Thus \(\) should be moderate to leverage the generalization and the capability of the model.

## 5 Experiment

In this section, we conduct node classification experiments on various benchmark datasets to evaluate the performance of APGNN. Specifically, we compare our method with state-of-the-art methods and display the corresponding learned graph filter on different data sets. Moreover, to validate the theoretical analysis, the influence of parameters \(K\), \(\), and \(P\) is also investigated in experiments.

### Experiment Setup

**Datasets.** We perform experiments on five benchmark datasets commonly used in node classification tasks. **1). Cora, Citeseer, Pubmed[29; 35]**: These are three standard citation networks where each node is a paper and each edge is a citation link. **2).Wiki-CS**: This dataset defines the computer science articles as nodes, while the hyperlinks are edges. **3). MS Acadamic**: The nodes represent the author and the edges represent the co-authorships. A co-authorship Microsoft Academic Graph, where the nodes are the bag-of-words representation of the papers' abstract and edges are co-authorship. The data statistics and their partitions are presented in Appendix.

**Baselines.** To evaluate the effectiveness of APGNN, we compare it with the following baseline models: 1) MLP , a traditional method that does not use graphs, 2) GAT  and GraphSAGE , spatial methods that aggregate neighborhoods' information, and 3) ChebNet , GCN , SGC , PPNP, APPNP, GNN-LF (iteration form), GNN-HF (iteration form) , and DAGNN , spectral methods analyzing GNNs with graph Fourier transform.

**Settings.** We conducted 10 runs for each method on each dataset, with a hidden dimension of \(64\). For all compared methods, their parameter settings follow the previous practices [19; 36]: the dropout rate is \(0.5\) except for Cora, which had a rate of \(0.8\). Furthermore, the learning rate is \(0.01\) for Cora, Citeseer, and Pubmed, but \(0.03\) for Wiki-CS and \(0.02\) for MS-Academic, while the weight decay is \(0.005\) for Cora and Pubmed, \(0.02\) for Citeseer, \(0.0005\) for Wiki-CS, and \(0.00525\) for MS-Academic. We fix the polynomial order \(K\) to \(10\) in ChebNet, APPNP, GNN-LF, GNN-HF, DAGNN, and APPNP. The best hyperparameters we choose for APGNN are presented in Appendix. To ensure a fair

    &  \\   & **Cora** & **Citeseer** & **Pubmed** & **Wiki-CS** & **MS-Academic** \\ 
**MLP** & 57.79\({}_{ 0.11}\) & 61.20\({}_{ 0.08}\) & 73.23\({}_{ 0.05}\) & 65.66\({}_{ 0.20}\) & 87.79\({}_{ 0.42}\) \\
**ChebNet** & 79.92\({}_{ 0.18}\) & 70.90\({}_{ 0.37}\) & 76.98\({}_{ 0.16}\) & 63.24\({}_{ 1.43}\) & 90.76\({}_{ 0.73}\) \\
**GCN** & 82.03\({}_{ 0.27}\) & 71.05\({}_{ 0.33}\) & 79.26\({}_{ 0.18}\) & 72.05\({}_{ 0.45}\) & 92.07\({}_{ 0.13}\) \\
**SGC** & 81.89\({}_{ 0.26}\) & 72.18\({}_{ 0.24}\) & 78.58\({}_{ 0.15}\) & 72.76\({}_{ 0.35}\) & 89.01\({}_{ 0.40}\) \\
**GAT** & 82.82\({}_{ 0.36}\) & 71.96\({}_{ 0.39}\) & 79.15\({}_{ 0.34}\) & 74.36\({}_{ 0.58}\) & 91.86\({}_{ 0.27}\) \\
**GraphSage** & 82.14\({}_{ 0.25}\) & 71.80\({}_{ 0.36}\) & 79.20\({}_{ 0.27}\) & 73.17\({}_{ 0.41}\) & 91.53\({}_{ 0.15}\) \\
**PPNP** & 83.73\({}_{ 0.31}\) & 71.74\({}_{ 0.44}\) & 80.28\({}_{ 0.22}\) & 74.69\({}_{ 0.53}\) & 92.58\({}_{ 0.06}\) \\
**APPNP** & 83.73\({}_{ 0.21}\) & 71.70\({}_{ 0.21}\) & 80.07\({}_{ 0.21}\) & 74.91\({}_{ 0.61}\) & 92.81\({}_{ 0.12}\) \\
**GNN-LF(iter)** & 83.83\({}_{ 0.36}\) & 71.44\({}_{ 0.42}\) & 80.31\({}_{ 0.16}\) & 75.19\({}_{ 0.49}\) & 92.78\({}_{ 0.22}\) \\
**GNN-HF(iter)** & 83.68\({}_{ 0.31}\) & 71.58\({}_{ 0.36}\) & 79.99\({}_{ 0.22}\) & 74.71\({}_{ 0.55}\) & 92.72\({}_{ 0.31}\) \\
**DAGNN** & 82.70\({}_{ 0.17}\) & 71.90\({}_{ 0.06}\) & 80.06\({}_{ 0.30}\) & 75.63\({}_{ 0.48}\) & 92.24\({}_{ 0.21}\) \\ 
**Ours** & **84.15\({}_{ 0.23}\)** & **72.44\({}_{ 0.56}\)** & **80.74\({}_{ 0.24}\)** & **76.03\({}_{ 0.51}\)** & **93.69\({}_{ 0.20}\)** \\   

Table 2: The average accuracy (\(\%\)) and standard deviation (\(\%\)) on five benchmark datasets. The highest accuracy in each column is shown in bold, while the second-best result is underlined.

Figure 2: The graph filters learned on different data sets, with the parameter \(P\) being odd in subfigure (a) and even in subfigure (b).

comparison with the compared methods, we also applied our optimal hyperparameters to them, selecting the maximum value to display.

### Analysis

**Node Classification.** As the metric for evaluation, the mean accuracy of 10 runs is used. We compare the performance of APGNN with other methods on five benchmark datasets. Experiment results are reported in Table 2. We can observe that APGNN achieves the highest accuracy across all five datasets, demonstrating its superior performance.

**Learnable Graph Filters.** Figure 2 shows the graph filters learned on various datasets via APGNN. When the parity of \(P\) varies, the graph filter has a distinctive shape. However, their shapes exhibit minimal impact on their accuracy regardless of the parity of \(P\) according to the experiment results. Moreover, the graph filters of each dataset are plotted in Appendix, more details are included in Appendix. Our results show that the graph filters learned from different datasets vary in detail, even when their parameters have similar parity, demonstrating the efficacy of APGNN in learning task-specific graph filters.

**Polynomial Order \(K\).** To gain insight into the role of polynomial order \(K\), we conduct the experiment tuning \(K\) in \(\{1,2,...,20\}\) on Cora, Citeseer, and Pubmed dataset. Our theoretical analysis supports the observation that a small \(K\) can result in a large truncation error, leading to a low accuracy rate. It can be observed that the accuracy rate has little promotion when \(K\) is larger than \(10\), although at the cost of high computational resources.

**Decay Rate \(\).** Figure 3 (b) depicts the accuracy curve corresponding to various \(\) values ranging from \(0.1\) to \(0.9\) and \(0.99\) on Cora, Citeseer and Pubmed datasets. As \(\) decreases, the classification accuracy initially increases and then declines sharply. This phenomenon verifies the theory that the truncation error decreases as \(\) decreases, but it leads to a trivial function when \(\) is extremely small.

\(P\)**-hop strategy.** We investigate the accuracy associated with varying parameters \(P\) taken from the set \(\{1,2,3,4,5,6\}\) when fixing \(T=KP=60\). As we can see in Figure 3 (c), the accuracy increase when \(P>1\). This phenomenon can be attributed to the fact that the generalization bounding decreases when \(P\) increases, which suggests that the \(P\)-hop strategy can effectively explore deeper information with the same computational complexity.

## 6 Conclusion

This paper proposes a universal learning principle for a valid construction of GNN. An instantiation named APGNN is proposed to verify the effectiveness of our framework. APGNN employs a decay rate and a multiple \(P\)-hop strategy to learn the coefficients adaptively, which can efficiently aggregate the information from high-order neighbors. We present a theoretical analysis of the generalization capabilities of both our framework and APGNN, which provides a learning guarantee. Comprehensive experiments show the superior performance of APGNN. In the future, it is worth exploring diverse graph filters based on the proposed principle. As shown in the generalization analysis, the upper bound of the model complexity relies on \(()\). How to devise the GNN with complexity free of the hyperparameter \(K\) is also a meaningful research direction.

Figure 3: Accuracy with different (a) \(K\). (b) \(\). (c) \(P\) (for fixing \(T\)).