# Batchnorm Allows Unsupervised Radial Attacks

Amur Ghose\({}^{*}\)\({}^{1,2}\), Apurv Gupta\({}^{3}\), Yaoliang Yu\({}^{1,2}\), Pascal Poupart\({}^{1,2}\)

\({}^{1}\)David R. Cheriton School of Computer Science, University of Waterloo,

\({}^{2}\)Vector Institute, \({}^{3}\)Columbia University

a3ghose@uwaterloo.ca, apurvgupta1996@gmail.com,

yaoliang.yu@uwaterloo.ca,ppoupart@uwaterloo.ca

Corresponding author: Amur Ghose, email: a3ghose@uwaterloo.ca

###### Abstract

The construction of adversarial examples usually requires the existence of soft or hard labels for each instance, with respect to which a loss gradient provides the signal for construction of the example. We show that for batch normalized deep image recognition architectures, intermediate latents that are produced after a batch normalization step by themselves suffice to produce adversarial examples using an intermediate loss solely utilizing angular deviations, **without** relying on any label. We motivate our loss through the geometry of batch normed representations and their concentration of norm on a hypersphere and distributional proximity to Gaussians. Our losses expand intermediate latent based attacks that usually require labels. The success of our method implies that leakage of intermediate representations may create a security breach for deployed models, which persists even when the model is transferred to downstream usage. Removal of batch norm weakens our attack, indicating it contributes to this vulnerability. Our attacks also succeed against LayerNorm empirically, thus being relevant for transformer architectures, most notably vision transformers which we analyze.

## 1 Introduction

Adversarial examples \(x_{adv}\) are commonly defined as data instances which lie \(\) away in some norm (usually \(L_{}\)) from an actual data instance \(x_{real}\). To humans, \(\) is small and \(x_{real},x_{adv}\) share labels, yet to a classifier neural network, they do not. Since their discovery (Szegedy et al., 2013; Goodfellow et al., 2014), adversarial examples have spurred research in both attacking (i.e., generating adversarial data instances) (Akhtar and Mian, 2018; Tramer et al., 2020) and defending (Yuan et al., 2019) neural networks against them. Common methods for generating adversarial images use loss gradients. A pioneering attack is Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014), possibly strengthened via Projected Gradient Descent (PGD) (Madry et al., 2017) - an iterative form of FGSM. Defenses against these attacks have been derived, including adversarial training (Shrivastava et al., 2017), where adversarial images are fed to the network for training. Adversarial training is expensive but powerful (Salman et al., 2019) and scalable (Wong et al., 2020; Shafahi et al., 2019). Defenses can give illusory security by obfuscating gradients (Athalye et al., 2018). An unsaid aspect of defense is hiding model parameters - changing attacks from the **white-box** (model parameters known) scenario to black-box (models only accessible via outputs) scenario, which makes them harder to attack.

We consider the family of **intermediate-level** attacks, exemplified by members such as Intermediate Level Attack Projection (ILAP) (Huang et al., 2019) and its variants (Li et al., 2020). In this, an initial direction in the latent space of the hidden intermediate layers is determined via FGSM or other attacks. Gradient steps then maximally perturb the intermediate hidden representation to find a suitable \(x_{adv}\), unlike directly using the label. Such methods can outperform the method used forthe initial direction itself and be more transferable across different models. ILAP uses FGSM to set initial directions, but once the direction arises, the layers after the intermediate layer play no role.

We now ask : Given an unlabeled data point \(x\) along with full access to a deep network upto \(K\) layers, where \(K\) is less than the network depth, could we craft an adversarial example out of \(x\) without any other information - e.g., the label of \(x\), any \(x^{} x\) - so long as the deep network utilized batch normalization? We answer this question positively when \(x\) is an image for several popular families of image recognition architectures, exploiting the hyperspherical geometry of latent spaces to propose an **angular adversarial attack.** The label access of the black box case is absent - we do not allow access to hard labels (true label of \(x\)) or soft labels (logits obtained at penultimate layers). We adapt the method advanced in ILAP to remove the need for label-based FGSM, and proceed instead with an angular loss based on the geometry of batch normalization. **Our concrete contributions are** :

* To provide a label-free attack that relies on only gradient access upto intermediate layers
* Ablation of our method to show it is not brittle to choices of intermediate layers, (small variations between using layers \(i,i+1\) if \(i\) is sufficiently deep)
* ResNets and EfficientNets
- fall to our attack. Removing batchnorm in ResNets via Fixup neuters the attack
- suggesting it is playing a role in this.
* To show that the attack **empirically works even for transformer architectures** (ViT) that employ Layernorm over batchnorm.
* To show that these attacks persist in the transfer learning setting, where the models are frozen upto a certain layer and then fine-tuned for downstream usage.
* To improve the supervised attack case when labels are available by using our loss in conjunction with the loss from the true label.

This implies that releasing the first \(K\) layers of a model publicly as a feature extractor, without any label access, can create adversarial attacks, which has security implications. Weaknesses persist even when the unseen, later layers of the network are tuned on different datasets (e.g. CIFAR10/100) - adversarial examples can be crafted for CIFAR without using the changed, later layers only using this first \(K\). We exploit the radial geometry of batch normalization and assume the norm of the neural representations concentrates around a constant, allowing us to exploit the geometry formed. This indicates batch normalization may make networks not just less robust to conventional attacks (Galloway et al., 2019), but open up new attacks entirely. We ablate our method and compare to scenarios of naive baselines and having access to \(x^{} x\) to showcase its competitiveness.

### Relation to Related Work

We tie together two areas : analyzing batch normalization from a geometric standpoint under convergence assumptions, and adversarial attacks with partial knowledge of architectures. Batch normalization has been analyzed from the standpoint of deep learning theory and model optimization (Santurkar et al., 2018). While its detrimental effects on model robustness (Galloway et al., 2019; Benz et al., 2020) are known, most analysis focuses on how batch normalization makes the model weaker to already-existing attacks. We propose that batch normalization might impact the network by making it more vulnerable to novel attacks. We are to our knowledge the first to connect this with the concentration of norm phenomenon implied by results such as the dynamics of batch normalization at high depths (Daneshmand et al., 2021). These tie into results such as neural collapse (Papyan et al., 2020), which find that later layers in deep networks have predictable geometric patterns.

Research in adversarial examples has focused in specific settings such as white/grey/black box settings denoting levels of model access/knowledge. Even in the black box case, it is often assumed we can feed an input \(x\) into the model \(M\) under attack and retrieve \(M(x)\), the model's prediction of the label of \(x\). We consider the case of partial model knowledge and attacking a model with no label outputs. This is relevant with the presence of large-scale pretraining, where models are fed large training sets of data unrelated to any one domain to capture the overall distribution of a particular type of modality, such as GPT-3 (Brown et al., 2020) with natural language. Deployed downstream, these public models may fine tune on a smaller dataset, and most of their weights are not fine-tuned. Public models may act as feature extractors that distill large amounts of data beyond dataset scopes. They output not labels, but representation vectors, and our attack targets this case. Previous works do use latent representations to construct adversarial examples (Park and Lee, 2021; Creswell et al.,2017; Yu et al., 2021) through moving in latent space towards points of differing labels or maximizing deviations in latent space (Kos et al., 2018). Along with the related **no-box**(Li et al., 2020; Zhang et al., 2022) setting (assuming no model knowledge), these methods require extra parts, such as representations of instances of different labels as targets to move towards, surrogate models, or some soft labels, often with gradient access - all of which we do not require. They are also ad hoc in their choice of layers to attack, but we give a definite answer of which layers and architectures are the most vulnerable - batch norm layers situated deeper in the network. The closest case to our attack are label-free, single-stage latent-based attacks (Zhou et al., 2018; Inkawhich et al., 2019; Ganeshan et al., 2019; Inkawhich et al., 2020; Zhang et al., 2022; Wang et al., 2021; Inkawhich et al., 2020) which we outperform. These methods use the latent to boost the supervised scenario and not exploit batch normalization or two-step optimization in an unsupervised setting as we do.

### Setup - FGSM and PGD Attacks

We describe FGSM Aka Fast Gradient Sign Method (Goodfellow et al., 2014) for generating an adversarial example. The task is to find \(x^{}\) which differs from \(x\) in the label assigned by a classifier \(C\), which outputs a distribution over \(K\) classes (\(K\) positive numbers that sum to \(1\)).

\[\|x^{}-x\|\] \[*{arg\,max}_{i}[C(x)]_{i}*{arg\, max}_{i}[C(x^{})]_{i}\] (1)

Generally, we only restrict attacks to cases where \(*{arg\,max}_{i}[C(x)]_{i}\) is the true label, i.e., \(C\) is correct. The norm \(\|.\|\) used to define the attack is almost always the \(L_{}\) norm, though other choices such as \(L_{2}\)(Carlini and Wagner, 2017) have been tested as well. We will assume the norm to be \(L_{}\) for the description. Computing \(x^{}\) (possibly with projection to a valid domain) proceeds as follows:

* Apply \(C\) to \(x\) to obtain \(C(x)\).
* Compute gradient \( L\) wrt \(x\) where \(L\) is the loss between \(C(x)\) and the true label vector \(v_{l}\).
* Compute \(x^{}_{i}=x_{i}+*{sign}( L)_{i}\), i.e., move by \(\) based on sign of gradient.

We understand the **sign** function to map to \(-1,0,1\) respectively as the argument is \(<0\), \(0\), or \(>0\). FGSM uses gradient information, which may vary rapidly around \(x\). A stronger variant, the Projected Gradient Descent (PGD) attack (Madry et al., 2017) uses multiple steps near \(x\) taking gradients at each step. Under PGD, there is a step size \(\), typically \(<<\), which iteratively updates \(x^{t}_{i}\) as :

\[x^{t}_{i}=x^{t-1}_{i}+*{sign}( L)^{t-1}_{i}\]

With a clamping step on \(x^{t}_{i}\) that enforces the constraint of \(\|x^{}-x\|\), this step after initializing \(x^{0}_{i}\) at \(x_{i}\) yields much stronger adversarial examples than FGSM at higher \(t\).

### Intermediate Level Attack Projection (ILAP)

These methods - FGSM and PGD - rely on the input and the final layer's output loss with respect to the true label. We can consider a neural network \(N\) of depth \(D\) as :

\[N(x)=N_{i+1,D}(N_{1,i}(x))\]

With \(1\) based indexing, \(N_{j,k}\) is the neural sub-network via composing layers from depth \(j\) to \(k\). Let \(z_{i}=N_{1,i}(x)\) be the latent representation at depth \(i\). Consider \(x_{adv}\) from any baseline method, and let

\[z^{adv}_{i}=N_{1,i}(x_{adv})\ ;\ z^{orig}_{i}=N_{1,i}(x)\ ;\ ^{adv}_{i}=z^{adv}_{i}-z^{orig}_{i}\]

We then seek an alternate adversarial example, \(x^{ILAP}_{i}\) which seeks to work with the loss function :

\[L=^{adv}_{i},^{ILAP}_{i}\]

where \(^{ILAP}_{i}=z^{ILAP}_{i}-z^{orig}_{i}\) and \(z^{ILAP}_{i}=N_{1,i}(x^{ILAP}_{i})\)

The \(L\) can then be plugged into either FGSM or PGD. Put simply, this loss encourages latent space movement directionally similar to FGSM/PGD. \(L\) increases the inner product with \(^{adv}_{i}\) and later iterations use it over the true loss. This alternate adversarial example is highly transferable - e.g., adversarial examples from ResNet18 have higher success rates vs different models such as DenseNet, when created by an ILAP process than directly using FGSM. It can even beat FGSM on the source model itself. (Huang et al., 2019) ILAP demonstrates that latent representations provide ammunition for adversarial attacks, but requires an initial adversarial example \(x_{adv}\) created with full model access and label. To remedy this, we craft an attack needing neither. We show that if the correct layers are attacked with an angular version of the same loss, the initial adversarial example is unnecessary.

## 2 Geometry of Batch Normalization

Given a minibatch of inputs \(x_{i}\) of dimensions \(n\) with mean \(_{ij}\) and standard deviation \(_{ij}\) at index \(j\), consider batch normalization (Ioffe and Szegedy, 2015) or just **BN** as:

\[[(x_{i})]_{j}=-_{ij}}{_{ij}}\] (2)

Such normalization (with affine shift) has become commonplace in neural networks. During training, the empirical average of the minibatch is used and during testing, computed averages (mean/standard deviation) are used. By batchnorm, we mean only the batchnorm layer without an affine shift. Layer normalization (Ba et al., 2016) is an alternate form of normalization that normalizes over all neurons in a hidden layer. For space reasons, we discuss batch normalization in the main text and layer normalization in Appendix A. Empirically, our attack succeeds on layer norm and architectures using it, such as transformers (Vaswani et al., 2017) and vision transformers (Dosovitskiy et al., 2020).

**Converged regime:** Suppose in training, the sample means/standard deviations in a pre-BN layer converge to sample statistics. Post-batchnorm, representation vector \(Z\) satisfies, at every index \(j\):

\[(Z_{j})=0,(Z_{j}^{2})=1\] (3)

i.e., for a layer of dimensionality \(d\) and denoting the entire latent vector as \(Z\), \(E(Z)=0,E(\|Z\|^{2})=d\) by linearity of expectation. The above convergence is with respect to the **training** set, but carries over to the **test** set (approximately) under the I.I.D assumption when learning takes place. Suppose \(\|Z\|^{2}\) was concentrated about its expected value - i.e., nearly all \(\|Z\|\) values were \(\). We first discuss the implications of such a scenario and what attacks they allow. We assume that in this latent space, the inner product determines'similarity'. Two latents \(Z_{a},Z_{b}\) from different instances \(X_{a},X_{b}\) will be closer as \( Z_{a},Z_{b}\) rise. Under this metric, the most dissimilar point to \(Z_{a}\) is parallel to \(-Z_{a}\). We will formulate our attack assuming that \(\|Z\|=\) i.e. a hyperspherical latent space. After we have formulated our attack, we consider scenarios of such spaces and concentrations of norm.

### Angular Attack Based on Converged Batchnorm

Let \(N_{1,i}\) be the network upto layer \(i\) with dimensionality \(d\). If layer \(i\) is a batch norm layer (without affine shift), \(z=N_{1,i}(x)\) lies (with converged batchnorm and \(\|Z\|\)'s norm concentrated) approximately on a shell of radius \(\). The natural distance metric between two \(z,z^{}\) is angular distance:

\[}{\|z\|\|z^{}\|}\]

where \( z,z^{}\) is at a local maximum on the hypersphere \(\|z\|\) = constant when \(z=z^{}\) and no gradient exists. We need another gradient initially, and propose the following attack algorithm. Given a real example \(x^{0}\) with latent \(z^{0}=N_{1,i}(x^{0}),z^{t}=N_{1,i}(x^{t})\) we consider the loss \(L_{}^{t}=-\|z^{t}\|=-\|N_{1,i}(x^{t})\|\) for the first \(t_{}\) iterations. We then iteratively generate \(x^{t}\):

\[x^{t}=x^{t-1}+( L_{}^{t- 1}); 1 t t_{}\]

After \(t_{}\) iterations, we modify the angular loss to \(L_{}\), using the deviations obtained so far. \(L_{}\) is angularly defined, denoting \(z_{i}^{t}\) the latent at depth \(i\) and iteration \(t\) :

\[L_{}=-^{0},z_{i}^{t}}{\|z_{i}^{0}\|\|z_ {i}^{t}\|};z_{i}^{0}=N_{1,i}(x^{0}),z_{i}^{t}=N_{1,i}(x^{t})\]

We then iteratively update as follows (graphically depicted in Figure 1):

\[x^{t}=x^{t-1}+( L_{}^{ t-1}); t_{}<t t_{}\]

The figure geometrically depicts how intermediately moving from a point to another in latent space lowers the radial norm (\(A_{i}\), left). Directly lowering the radial norm via a path from \(A_{0}\) to \(A_{1}\) (right) can create an initial deviation. But extending this to \(C\) as in the figure ends up with a lower angular deviation than maximizing angular deviation directly and ending up at \(A_{adv}\). Methods which directly increase the \(L_{2}\) deviation starting from an initial method would result in points such as \(C\).

Both losses depend on the layer \(i\). With access till layer \(D\), multiple candidate \(i\) values exist. For simplicity, we only use \(D,D-1\) and replace \(( L_{})\) with \([( L_{,D-1})+( L_{ ,D})]\) where the subscript \(L_{,D-1}\) denotes setting \(i\) as \(D-1\), and similarly we set \(i=D\) and average the sign for \(( L_{})\). This averaged term becomes \(\) in our pseudocode (Algorithm 1). We clamp \(x^{k}\) iterates to satisfy \(\|x^{k}-x^{0}\|\). Our attack resembles ILAP, with the initial direction \(_{i}^{adv}\) from an unsupervised radial loss without labels. We then maximize the angular deviation from the original latent, and do not "follow the leader" on the original angular deviation after \(t_{init}\). The intuition is as follows. The surrogate loss \(-||z^{t}||\) moves along a chord in the hypersphere's interior from \(z^{0}\) towards \(-z^{0}\), the most dissimilar point, while latent representations are on the surface. We use the last \(2\) layers for simplicity - stronger attacks might exist using all layers. Concentration of norm forms the hypersphere, and the 2-step process is key. 1-step methods, e.g. single-stage works discussed under related works, use methods such as random initialization and angular minimization, but perform worse (see Appendix P). The concentration is in \(L_{2}\), but the adversarial example is in the \(L_{}\) metric.

``` Input: Neural Network \(N\); access of model parameters of \(N\) till layer \(D\), \(N_{1,i}\): sub-network of \(N\)  upto layer \(i\), a starting real sample \(x^{0}\), perturbation \(\) Result:Adversarial example \(x^{t_{}}\)\(t_{}\) Num iterations with loss \(L_{}\) ; \(t_{}\) Num iterations with loss \(L_{}\)\(k 1\), \(n t_{}+1\) while\(k t_{}\)do \(L_{,i}^{k-1}=-\|N_{1,i}(x^{k-1})\|\) ; \(=( L_{,D-1}^{k-1})+( L_{,D}^{k-1})\) \(x^{k} x^{k-1}+; x^{k}(x^{k},)\) ; \( k k+1\) endwhile while\(n t_{}\)do \(L_{,i}^{n-1}=-(x^{0}),N_{1,i}(x^{n-1})}{ \|N_{1,i}(x^{0})\|\|N_{1,i}(x^{n-1})\|}\) ; \(=( L_{,D-1}^{n-1})+ ( L_{,D}^{n-1})\) \(x^{n} x^{n-1}+; x^{n}(x^{n},)\) ; \( n n+1\) endwhile Return:\(x^{t_{}}\) ```

**Algorithm 1** Angular attack algorithm

Moving from \(z^{0}\) along \(-\|z^{t}\|\) does **not** move linearly towards the origin when the optimization is imperfect - e.g. using the gradient sign. We manipulate \(x^{i}\), not \(z^{i}\) - it may not be \(\)\(x^{}\) such that \(N_{1,i}(x^{})= z^{0}\), \(,0<1\) (the ray joining origin-\(z^{0}\) may lack points with pre-images in \(x\)). Another issue is \(\) movement direction-wise (ignoring sign averaging) - e.g. let \(Z_{i}^{2},N_{1,i}(x)=x\) - the representation as identity map. Let \(x=(1,)\) lie on the hypersphere of radius \(}\). The sign of \( L_{}\) is \((-,-)\) and \(x^{0}=(1-,-)\) - not necessarily collinear with \((0,0)-(1,)\). Batch

Figure 1: **L**: \(A\),\(C\) are latent image representations. \(A_{i}\) lies on chord \(AC\). Targeted latent space movements lower norm. **R**: Initial radial loss towards \(O\) forms \(A_{1}\) with implied movement towards \(C\). \(A_{2},,A_{adv}\) follow. Implied movement is towards \(C^{}\) - further than \(C\) from \(A\) in \(\) (angles).

normalization leaks mean/variance to adversaries. Suppose we had a perfect network \(N\) (could find \(x\) mapping to any \(z\), i.e., was onto), and **were assured** the latent space was a zero mean hypersphere and we get adversarial instances if we move the latent from \(z\) to \(-z\). This would be insufficient if all latents were translated by some vector \(c\), i.e., the mean of the hypersphere was nonzero. To find the point opposite \(z\) on the new hypersphere (\(c-(z-c)\)), we need \(c\). Batch norm gives us this \(c\). This weakness outweighs knowing \(z^{}\) arising from \(x^{}\) of a different label, as we show in the results.

### When Does the Norm Concentrate?

We assumed that after a batch normalization step, \(\|Z\|\) concentrates. Consider sufficiency conditions. Let \(Z_{ij}\) be the \(j\)-th entry of the \(i\)-th latent \(Z_{i}\) formed from \(X_{i}\) with expectation \(0\), variance \(1\) (batch norm). Consider independent instances \(X_{i}\) and assume no instances indexed by \(i\) were used for training to impact model parameters, which affects the other latents and makes them dependent - i.e. consider test/validation sets. Then, \(Z_{ij},Z_{i^{}j^{}}\) are independent if \(i i^{}, j^{}=j\). Now :

\[\|Z_{i}\|^{2}=_{j=1}^{d}Z_{ij}^{2}\]

This sums \(d\) random variables (assuming \(Z_{i}\) has dimension \(d\)) and depends on independence structures and the marginal distributions of each variable. Under certain cases, independence of \(Z_{ij},Z_{ij^{}}\) can occur. Suppose \(Z_{ij}\) was constrained in \(\{-1,1\}\), and had to encode \(2^{D}\)\(X_{i}\)'s. Then, \(Z_{ij},Z_{ij^{}}\) are independent under the optimal encoding, assigning for each \(X_{i}\) a unique \(D\)-length code with entries \(\{-1,1\}\). When \(Z_{ij},Z_{ij^{}}\) are independent, \(\|Z_{i}\|^{2}=_{j}Z_{ij}^{2}\) will concentrate around its expected value of \(d\) under mild conditions e.g. bounded \(4\)-th moments of \(Z_{ij}\) (Chebyshev's inequality), but concentration tails may not be sub-Gaussian. By tails of concentrations, we mean bounds of form:

\[P(\|Z_{i}^{2}\|-E(\|Z_{i}^{2}\|)\|) f()\]

For example, sub-Gaussian concentration implies \(f()=O((-^{2}))\). Suppose each \(Z_{ij}\) is marginally distributed as a (sub)-Gaussian. With independence, Chernoff's inequality leads to a norm concentration for \(\|Z\|\) with stronger sub-Gaussian tails over Chebyshev's (tails of \(}\)). Inequalities and implied tails can be consulted from e.g. (Vershynin, 2018). We consider some previous results. For deep linear models of width(dimensionality) \(d\), we have (Daneshmand et al., 2020, 2021):

\[E[D_{KL}(Z\|(0,I_{d}))]=O((1-)^{i}+})\]

\(Z\) is the distribution after \(N_{1,i}\) i.e., \(i\) layers, \(\) is a constant, \(b\) is the training batch size. The LHS (termed the orthogonality gap) indicates that the distribution of the latents convergences to the isotropic Gaussian using KL divergence, which is a stronger condition leading to the latent norms being clustered around \(\). **Though derived for linear networks, the original paper offers evidence that the conjecture holds for general multi-layer networks.** This indicates deeper layers i.e., increasing \(i\) will have the largest effect on our assumptions holding, as this exponentially drops the KL divergence to a Gaussian (and concentrates the norm), with a secondary benefit from increasing width (\(d\)). Beyond linear networks, general networks with depth and width reduce to **Gaussian Processes**(Yang et al., 2019; Yang and Schoenholz, 2018; Yang and Hu, 2020; Yang, 2019; Neal, 2012). These results usually apply to marginal distributions and not independence structures. Empirically, we find dependence between different variables (measured as correlation) falls as the network latent grows deeper, which aligns with theory. Dimensionality falls as the network deepens - recall the toy example of encoding \(2^{D}\) instances with \(D\) binary variables which forces independence. ILAP's empirical ablation studies found the optimal \(i\) to create the latents for the attack occurred in the range \(0.6\) to \(0.8\) (network depth normalized to \(1\)). In sum, we expect our **optimal layers for the latent to lie near the end of the network**. Our methods work without assumptions in hyperspherical representation spaces (Wang and Isola, 2020; Schroff et al., 2015; Mettes et al., 2019). Analysis and empirical findings on the concentration and tails is included in Appendix R.

## 3 Results

We carry out an extensive set of experiments on Imagenet (Russakovsky et al., 2015), utilizing several ResNet (He et al., 2016) models - ResNet-\(\{18,34,50,101,152\}\) and EfficientNet models (Tan and Le, 2019) B-1 to B-5. We chose these models because they have a common structure at construction. Except a few initial layers, they are constructed by repeatedly stacking the same blocks ("Basic" or "Bottleneck" blocks for ResNets, and Mobile Convolutional blocks Howard et al. (2017) for EfficientNets). Every block possesses at least one batch norm layer, allowing the extraction of the appropriate latent. All models were ran on Imagenet with a standard normalization pre-processing step. Results of all attacks on all models, a sanity check with a fixed norm model (explicitly hyperspherical, i.e. always concentrated in norm), improving supervised (label present) cases on top of FGSM/PGD, comparisons to single-stage attacks previously proposed, interactions with defence, ablations, visualizations, statistical testing, confidence intervals and other transfer learning results and comparisons appear in the Appendices. As the alternative to batchnormed models, we choose Fixup Zhang et al. (2019) - a different way to initialize the models. Core results are summarized in Figure 2. On the batchnorm-free Fixup Resnet, the FGSM attack greatly outperforms our angular attack. On the other two architectures, this is **not** the case. FGSM's performance and clean accuracy vary far less than the success of the angular attack. That implies overall model quality and robustness to common attacks remains the same, and vulnerability to the angular attack specifically is what varies. We train Fixup resnets on imagenet as alternatives to the batchnormed models for Resnet only, as methods in Fixup do not generalize to efficientnets. Details of these training steps are in the Appendix B, and we also attach our training and inference codebases. We obtained the models from publicly available repositories for PyTorch Paszke et al. (2019), namely torchvision and EfficientNets-PyTorch, and did not change the original weights. For the choice of \(\) for the adversarial attack, we chose \(0.03,0.06,0.1\) and carried out all attacks using an \(=0.01\) over \(40\) iterations.

**Vision Transformers and LayerNorm.** Recently, Layer normalization Ba et al. (2016) has emerged as an alternate mode of normalization to batch norm. It finds prominent usage in the transformer architecture Vaswani et al. (2017) and for images, in the ViT (Vision Transformer) models. We used our attacks on the case of ViT models Dosovitskiy et al. (2020) (B/L)-(16/32). In all cases, the attacks were successful as with Resnets/batchnorm. This indicates our attack can succeed even when instead of batch normalization, layer normalization is utilized instead. We discuss this in Appendix A, but make batch norm the focus as it has more pre-existing theoretical analysis Daneshmand et al. (2020).

We examine the results on all Resnets in Tables 1,2,3 with Table 8 denoting clean accuracies. Angular attacks - especially in terms of their top \(5\) accuracy, at higher \(\) and on deeper networks - dominate FGSM attacks, while losing to PGD attacks. No random perturbation performs well. Confidence intervals rule out a fluke (statistical tests Appendix C). Raw numbers for Fixup Resnets (in Appendix D) exhibit a different pattern - the angular attack performs as well as a random method, while FGSM, and PGD perform as normal. EfficientNets (in Appendix E) follow ResNets in their result. In Table 4, we exhibit the max over single-stage benchmarks among Zhou et al. (2018); Inkawhich et al. (2019),

Figure 2: Accuracies (\(\%\)) of networks under FGSM (blue) and our Angular attack (red). (Color intensity proportional to network size). Accuracies under angular attack are lower without Fixup.

[MISSING_PAGE_FAIL:8]

Ganeshan et al., 2019; Inkawhich et al., 2020; Zhang et al., 2022; Wang et al., 2021; Inkawhich et al., 2020). The performance is worse than our method. We also showcase results of ViT models on \(=0.1\) for imagenet in Table 5. (Results for other values of \(\) are in Appendix F). Finally in Table 6, we attack Resnets with the angular attack but move the layer being attacked to the layer before the BN layer. This noticeably weakens the attack, suggesting a relationship between the two.

We examine the drop in accuracy as a function of the access to various blocks. We run our attacks consistently accessing the \(3/4\)-th layer of a net, i.e., for a Resnet-\(50\) of blocks \(\) - total \(16\) - we use only the first \(12\) blocks and sum our angular losses over block \(11,12\). On Resnet-34 access to deeper blocks strongly strengthens the attack in Table 7. This agrees with the theory of batch normalization discussed previously (Daneshmand et al., 2021) and with the empirical findings of ILAP (Huang et al., 2019), which found that the most optimal layers to perturb lay between \(50\%\) and \(80\%\) of the network's length. Further depth ablations are in Appendix G. Concentration of norm relies on convergence of batchnorm and independence. Assuming the former, we can check independence by examining the absolute, off-diagonal correlations in the correlation matrices of the latent representations. We note a decorrelation effect with depth, which matches the success of our attacks. We compare the fall in absolute correlation among the latent dimensions in Table 9, across three "block groups" of Resnet-18 which has \(8\) blocks organized as \(\). Correlation - a proxy for independence - decreases over the last three block groups.

**Transfer learning.** For transfer learning, we add a linear classifier that can access the last extracted latent layer of the resnet, and we unfreeze this classifier and the last group of blocks. Every other layer is frozen. The setup is fine-tuned, downstream, on CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009). We show results on CIFAR-100 with Resnet-50 in Table 10. Although Table 10 indicates that targeted attacks perform worse than the original angular attacks, it does not mean that we cannot use the label information. It only means that when we randomly pick an instance of a different label, the angular attack often does better. But, the combination of label and angular attack can do better than either attack, so having an instance of a different label can still be very helpful. We provide some cases where our loss is provided in ensemble with the targeted loss and these perform better than either (see Appendix H). Results for CIFAR-10 / other resnets/datasets are in Appendix J. Our method outperforms all but PGD.

**Ablation against other cases.** The radial loss is an implicit movement towards the opposite point on a hypersphere. This assumes that the hypersphere exists at all. If we have a different latent \(Z_{j}\) from a point \(X_{j}\) of a different label from \(X_{i}\), we check if our attack might be more effective using an initial loss of the form \(-\|Z_{i}-Z_{j}\|\). Increasing this loss would be lowering the distance between \(Z_{i},Z_{j}\) i.e. moving towards the other point in latent space. Yet, this **targeted attack** - shown in Table 10 - is actually inferior to our attack, which lacks the access.

## 4 Conclusion

We have shown a powerful, label-free attack which only utilizes a portion of all the layers available for a network to construct adversarial examples that fool the entire network. It succeeds at this without knowing the label or having gradient access to the full model, and these adversarial methods

   Method &  &  &  \\   & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\  PGD & 0.16 & 0.20 & 0.14 & 0.14 & 0.14 & 0.14 \\ Random & 64.42 & 88.18 & 53.1 & 79.76 & 38.63 & 66.23 \\ FGSM & 29.28 & 59.02 & 25.86 & 51.95 & 15.01 & 35.14 \\ Angular & 2.32 & 10.79 & 2.16 & 9.72 & 1.88 & 9.49 \\   

Table 10: Angular transfer attack results and targeted benchmarks

a) Angular results on CIFAR-100, Resnet-50

   ResNet type & 18 & 34 & 50 & 101 & 152 \\  Acc@1 & 69.75 & 73.31 & 76.13 & 77.37 & 78.31 \\  Acc@5 & 89.07 & 91.42 & 92.86 & 93.54 & 94.04 \\   

Table 8: Clean accuracies for Resnets

    & 1 & 2 & 3 \\  Absolute mean correlation & 0.41 & 0.32 & 0.19 \\    
    &  &  \\   & Top 1 & Top 5 & Top 1 & Top 5 \\  Resnet-18 & 25.16 & 42.58 & 1.46 & 5.85 \\ Resnet-34 & 12.2 & 21.8 & 1.9 & 2.86 \\ Resnet-50 & 13.55 & 30.97 & 0.65 & 5.16 \\ Resnet-101 & 5.85 & 13.66 & 2.86 & 6.67 \\ Resnet-152 & 9.68 & 16.77 & 1.29 & 3.87 \\   

Table 9: Resnet-18 absolute correlationsgeneralize to the case where the model was fine-tuned afterwards. These results have relevance at the intersection of the theory and practice of adversarial robustness and the growing study of batch normalization and its weaknesses and drawbacks. We provide support to the notion that batch normalization may open up unseen avenues of perturbations that unexpectedly impact model robustness, a viewpoint supported by previous literature (Galloway et al., 2019; Benz et al., 2020; Wang et al., 2022). We also extend our results to LayerNorm, which is increasingly relevant with the advent of transformer architectures (Dosovitskiy et al., 2020).