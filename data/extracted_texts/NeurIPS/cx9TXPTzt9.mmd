# Posterior Inferred, Now What?

Streamlining Prediction in Bayesian Deep Learning

Rui Li Marcus Klasson Arno Solin Martin Trapp

Department of Computer Science, Aalto University, Finland

{firstname.lastname}@aalto.fi

###### Abstract

The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration remaining the standard. In this work we examine streamlining prediction in BDL through a single forward pass without sampling. For this we use local linearisation on activation functions and local Gaussian approximations at linear layers. Thus allowing us to analytically compute an approximation to the posterior predictive distribution. We showcase our approach for both MLP and transformer architectures and assess its performance on regression and classification tasks.

See also extended paper at https://arxiv.org/abs/2411.18425.

## 1 Introduction

Through the success of machine learning models in real-world applications, ensuring their reliability and robustness has become a key concern. In particular, in applications such as aided medical diagnosis , autonomous driving , or supporting scientific discovery , providing reliable predictions, identifying failure modes, and identify how to reduce uncertainties of the system is vital. Uncertainty quantification is at the core of these topics with Bayesian deep learning (BDL, ) providing a promising paradigm for assessing uncertainties effectively and efficiently.

The central goal in BDL is to make inferences w.r.t. the posterior distribution over the probabilistic model (the parameters or the function itself). For example, to compute the expected prediction, estimate model uncertainties, or use it within acquisition functions in active learning. For this, we need to first estimate the posterior distribution and secondly make inferences of interest based on the estimated posterior. While both of these steps typically involve intractable integration, only the first step has seen significant progress in recent years . For the second step, in case of a Laplace approximation (LA, ), globally linearising the model function around the maximum _a posteriori_ (MAP) estimate to perform inferences  has shown promise in providing good predictive uncertainty. However, for all other posterior approximation methods, sampling based approximations remain to be the default. Given the high dimensionality of neural networks, sophisticated sampling methods are usually computationally prohibited and vanilla Monte-Carlo sampling is typically employed.

In this work, we tackle this problem by streamlining the prediction in BDL through local linearisation of activation functions and local Gaussian approximations at linear layers. Instead of a sample based approximation, which requires multiple re-evaluations of the network, we analytically approximate the posterior predictive distribution in a single forward pass through the network, making our methods well-suited for large-scale applications. Moreover, in contrast to global linearisation, our method is suitable for more complex inference tasks as the neural network function becomes locally linear with respect to the inputs. Empirically, we find that local linearisation and local Gaussianapproximation of neural networks to provide accurate predictive uncertainties and predictions, while being conceptually simple. Fig. 1 shows the posterior predictive densities for our proposal, compared to sampling based approximations and global linearisation in case of a Laplace approximation.

The contributions of our work can be summarised as follows: _(i)_ We propose a sampling-free and deterministic method for approximating the posterior predictive distribution through local linearisation of activation functions and local Gaussian approximations in neural networks. _(ii)_ We show how to exploit different covariance structures of the approximate posterior and present a streamlined prediction path for both MLP and transformer architectures. _(iii)_ We evaluate our method on regression and classification tasks and find that our method result in good predictive performance.

## 2 Related Work

**Inferring Posterior in Bayesian Deep Learning** There has been many methods developed which can be roughly grouped into three categories: _(i)_ Laplace approximation based methods: Starting from  where simple post-hoc Laplace approximation (LA) has shown promising results, LA has gained increasing attention ever since. Recent works applied LA methods in various applications, such as large language models [29; 10] and dynamic neural networks . _(ii)_: Variational inference (VI) based methods:  showed mean-field VI (MFVI) could improve generalisation in small-scale neural network and  showed MFVI is effective for large-scale neural networks as well. _(iii)_: Others: Monte Carlo Dropout  aims to estimate predictive uncertainty by interpreting dropout in neural networks as a form of Bayesian approximation. Deep ensemble  combines the outputs of multiple independently trained models to capture predictive uncertainty. Stochastic Weight Averaging-Gaussian , which extends Stochastic Weight Averaging  by capturing the posterior distribution of model weights using a Gaussian approximation.

**Making Prediction in Bayesian Deep Learning** Little work has been done and the usual go-to solution is Monte Carlo Estimation. For Laplace approximation,  proposed the linearised LA by performing a global linearisation and has shown promise in providing useful predictive uncertainties.

## 3 Methods

In Bayesian deep learning (BDL), predicting the output \(y\) (_e.g._, class label, regression value) for an input \(\) is performed by _marginalizing_ out the model parameters \(\) of the neural network \(f_{}()\) instead of trusting a single point estimate, _i.e._,

\[p(y,)=_{}p(y f_{}()) \,p()\,,\] (1)

where \(=\{(_{n},y_{n})\}_{n=1}^{N}\) denotes the training data and the posterior distribution \(p()=,)}{p()}\) is given by Bayes' rule. However, for most neural networks integrating over the high-dimensional parameter space is intractable, necessitating the use of approximations to compute the posterior distribution \(p()\) and the posterior predictive distribution \(p(y,)\).

Figure 1: Ours gives better predictive uncertainties and decision boundaries compared with sampling in both Laplace approximation (LA) and mean-field variational inference (MFVI), while having matching performance with global linearised model (GLM) in LA.

Let the weights and biases of the \(m^{}\) linear layer of the network \(f\) be denoted as \(^{(m)}^{}}}}}\) and \(^{(m)}^{}}}\), respectively. Then the pre-activation \(^{(m)}\) is given as \(^{(m)}=^{(m)}^{(m-1)}+^{(m)}\), where \(^{(m-1)}^{}}}\) is the activation of the previous layer. In case \(m=1\), then \(^{(0)}\) corresponds to the input \(\). We further denote the \(k^{}\) element of \(^{(m)}\) as \(h_{k}^{(m)}=_{i=1}^{}}}W_{ki}^{(m)}a_{i}^{(m- 1)}+b_{k}^{(m)}\) and drop the superscript if it is clear from the context.

Given an approximate posterior distribution \(q()\) with \(=\{^{(m)},^{(m)}\}_{m=1}^{M}\), we aim to compute the probability distribution of the activation \(^{(m)}\) of each layer \(m\). For this, we need to estimate the distribution of the pre-activation \(^{(m)}\) and then compute an approximation to the activation \(^{(m)}\) after application of a non-linear activation function \(g()\).

Approximating the pre-activation distributionIn case the activation \(^{(m-1)}\) is deterministically give, _i.e._, for the input layer, we can compute the distribution over pre-activations analytically as a consequence of the stability of stable distributions under linear transformations . However, for hidden layers the distribution over pre-activations is generally not of the same family as the posterior distribution . Nevertheless, we will apply a local Gaussian approximation to the pre-activation at every hidden layer. Specifically, we make the assumption:

**Assumption 3.1**.: _Assume that the activations of the previous layer \(a_{i}^{(m-1)}\) and parameters of the \(m^{}\) layer are independent._

Then followed by a Gaussian approximation of \(a_{i}^{(m-1)}\,W_{ki}^{(m)}\) for each \(i\) and each \(k\), the mean of the pre-activation \(^{(m)}\) is given as:

\[[^{(m)}]=[^{(m)}][^{(m-1)}]+[^{(m)}],\] (2)

and the covariance between the \(k^{}\) and the \(j^{}\) hidden unit is computed as:

\[[h_{k}^{(m)},h_{l}^{(m)}]= }}}{} [a_{i}^{(m-1)}W_{ki}^{(m)},a_{j}^{(m-1)}W_{lj}^{(m)} ]+[b_{k}^{(m)},b_{l}^{(m)}]\] \[+ }}}{}[a_{i}^{(m-1)}]([W_{ki}^{(m)},b_{l}^{(m)} ]+[W_{ki}^{(m)},b_{k}^{(m)}]),\] (3)

where

\[[a_{i}^{(m-1)}W_{ki}^{(m)},a_{j}^{(m-1)}W _{lj}^{(m)}] =[a_{i}^{(m-1)}][a_{j}^{(m-1) }][W_{ki}^{(m)},W_{lj}^{(m)}]\] \[+[W_{ki}^{(m)}][W_{lj}^{(m)} ][a_{i}^{(m-1)},a_{j}^{(m-1)}]\] \[+[a_{i}^{(m-1)},a_{j}^{(m-1)}][W_{ki}^{(m)},W_{lj}^{(m)}].\] (4)

Depending on the structure of the covariance matrix, we can further simplify the computation of the covariance matrix.

Approximating the activation distributionLet \(g()\) denote a non-linear activation function computing \(=g()\) for a pre-activation \(\). Inspired by the application of local linearisation in Bayesian filtering [_e.g._, 23], we use a first order Taylor expansion of \(g()\) at the mean of the pre-activation \([]\). Specifically, we approximate \(g()\) using

\[g() g([])+_{g}|_{= []}(-[]),\] (5)

where \(_{g}|_{=[]}\) is the Jacobian of \(g()\) at \(=[]\). Then, as stable distributions are closed under linear transformations, the distribution of \(\) can be computed analytically and is given as follows in case of a Gaussian distributed, _i.e._,

\[(g([]),_{g}|_{= []}^{}_{}_{g}|_{= []}).\] (6)

Note that the quality of the local linearisation will depend on the scale of the distribution over the input \(\). For ReLU activation functions, Petersen et al.  have shown that local linearisation provides the optimal Gaussian approximation of a univariate Gaussian distribution in total variation. For classification tasks, we employ a probit approximation .

Combining local Gaussian approximations for linear layers and local linearisation for non-linear activation functions results in a tractable approximation to the posterior predictive distribution, which can be computed in a single forward pass. Fig. 2 illustrates our streamlined prediction for multi-layer perceptrons (MLP) and attention blocks in tranformers, for a detailed description on the approach for transformers see App. B.6.

**Covariance Structure** Computing the full covariance of the posterior is usually infeasible due to high computational and memory cost. Diagonal approximation and Kronecker-factorization of the covariance/precision are two of the most common approaches. For diagonal covariance, calculating the posterior predictive distribution is straightforward, see App. B.2 for details. In case of Kronecker factors, we developed a tailored block retrieval method for efficient propagation of uncertainties, see App. B.3 for details. Note that other covariance structures can exploited in a similar fashion.

**Computational Complexity** We will briefly discuss the computational complexity of our method for the case of full covariance. Observe from Eqs. (3) and (4) that the computational cost to obtain \(([h_{k},h_{l}])\) is \((_{}^{(l)^{2}})\). Therefore, computing the output covariance at the \(l^{}\) linear layer will be in the order of \((_{}^{(l)^{2}}_{}^{(l)^{2}})\). For element-wise activation functions, the computational cost will be \((_{}^{(l)^{2}})\). Hence, we obtain a total cost of \((_{l=1}^{L}_{}^{(l)^{2}}_{ {in}}^{(l)^{2}}+_{}^{(l)^{2}})\) for a network with \(L\) layers. By exploiting the covariance structure, the total computational cost can be substantially reduced.

## 4 Experiments

We adopt the Laplace approximation (LA, ) and mean-field variational inference [MFVI, 2] for approximating the posterior distribution of the network parameters. We compare our method using local Gaussian approximation and local linearisation against Monte Carlo (MC) sampling and a global linearised model [GLM, 8]. For MFVI, we adopt the IVON optimiser  to obtain the posterior approximation, which has been shown to be effective and scalable to large-scale classification tasks. Here, we compare our method against MC sampling from the posterior to make predictions as done in Shen et al. . For the MFVI and LA sampling baselines, we used \(1,000\) MC samples in the regression and MLP classification experiments, and \(50\) MC samples for the ViT classification experiments. For our method, we additionally fit a scale factor, multiplied to the predictive variance, by minimizing the negative log predictive density (NLPD) on the validation set. This is necessary, as the predictive variance in case of deep and wide network with diagonal covariance structure can be large. We use a paired \(t\)-test with \(p=0.05\) and bold results with significant statistical difference.

**Regression** We experiment with multi-layer perceptron (MLP) for regression. See App. C.1 for experiment setup details and additional results. We use full covariance for LA. As shown in Table Table 1, for MFVI our proposal (Ours) result in better performance than sampling on \(8\) data sets and matches the performance on the remaining \(3\) data sets. For LA, our approach obtains better performance than sampling on all data sets.

**Classification** For MLP, we train it from scratch and treat all layers Bayesian. For ViT, we fine-tune the MLP layers in the last two blocks in a pre-trained Vision transformer (ViT) base model  and

Figure 2: Illustration our approach for different network architectures. In MLPs, we can directly apply local Gaussian approximations and local linearisation of each layer. The distribution over activations is then propagated to the next layer. In attention layers, we treat the query \(\) and key \(\) deterministically and only treat the value \(\) as a random quantity, resulting in a straightforward propagation path. The resulting distribution is then propagated to the subsequent MLP layer.

later treat them Bayesian. See App. C.2 for experiment setup details and additional results. With LA, we use a Kronecker-factorized covariance for MLPs and a diagonal covariance for ViT models. As shown in Table 2, for both MLP and ViT, we obtain better performance when compared with sampling and GLM.

Robustness to Out-of-distributionWe now assess the robustness to out-of-distribution (OOD) data for our method and the baselines. In Fig. 3, we take the ViT network fine-tuned on CIFAR-10 and evaluate its predictive entropy on the SVHN data set . Our method can distinguish between in-distribution and OOD data better than the LA MAP and MFVI Sampling. Although our method underfits on the in-distribution data, the separation between is clear for the OOD data similar. For results on MLP, see App. C.2.

## 5 Discussion & Conclusion

In this work, we proposed to streamline prediction in Bayesian deep learning by local linearisation and local Gaussian approximations. For this, we discussed the propagation in different neural network architectures and covariance structures. We showed through a series of experiments that our method obtains high predictive performance, obtain good predictive uncertainties, and can distinguish between in-distribution and OOD data. In future work, we aim to extend our approach to other network architectures, such as convolutional layers, and utilize our approach in more complex inference tasks.

    & &  &  \\  &  &  &  &  &  &  \\  &  & Sampling &  &  &  &  &  \\  MNIST & MLP & \(0.179 0.014\) & \(_{1.005}\) & \(0.210 0.003\) & \(_{1.004}\) & \(_{1.005}\) \\ FMNIST & MLP & \(2.010 0.051\) & \(_{1.011}\) & \(0.556 0.008\) & \(0.548 0.018\) & \( 0.010\) \\  CIFAR-10 & ViT & \(0.124 0.011\) & \(_{1.005}\) & \(0.169 0.004\) & \(_{1.005}\) & \( 0.006\) \\ CIFAR-100 & ViT & \(0.480 0.018\) & \( 0.013\) & \(1.043 0.010\) & \(0.602 0.011\) & \( 0.012\) \\   

Table 2: Negative log predictive density \(\) on classification data sets. Ours results in better or matching performance when compared with sampling, indicating the effectiveness of our approximation.

    & &  &  \\  &  & Sampling &  &  &  \\  &  & Sampling &  &  &  &  \\  &  & Sampling &  &  &  &  \\  SNIST & MLP & \(0.179 0.014\) & \(_{1.005}\) & \(0.210 0.003\) & \(_{1.004}\) & \(_{1.005}\) \\ FMNIST & MLP & \(2.010 0.051\) & \(_{1.011}\) & \(0.556 0.008\) & \(0.548 0.018\) & \( 0.010\) \\  CIFAR-10 & ViT & \(0.124 0.011\) & \(_{1.005}\) & \(0.169 0.004\) & \(_{1.005}\) & \( 0.006\) \\ CIFAR-100 & ViT & \(0.480 0.018\) & \( 0.013\) & \(1.043 0.010\) & \(0.602 0.011\) & \( 0.012\) \\   

Table 2: Negative log predictive density \(\) on classification data sets. Ours results in better or matching performance when compared with sampling, indicating the effectiveness of our approximation.

Figure 3: Kernel density plots over the predictive entropy from a ViT network finetuned on CIFAR-10 (blue, in-distribution) and data from SVHN (red, out-of-distribution). Our method results in a clear separation between the in- and out-of-distribution data.