# Representation Noising:

A Defence Mechanism Against Harmful Finetuning

Domenic Rosati\({}^{1,7}\)1 Jan Wehner\({}^{2}\) Kai Williams\({}^{3}\)

**Lukasz Bartoszcze\({}^{4}\) David Atanasov\({}^{5}\) Robie Gonzales\({}^{1}\)**

**Subhabrata Majumdar\({}^{6}\) Carsten Maple\({}^{4}\) Hassan Sajjad\({}^{1}\) Frank Rudzicz\({}^{1,7}\)**

\({}^{1}\)Dalhousie University \({}^{2}\)CISPA Helmholtz Center for Information Security

\({}^{3}\)Swarthmore College \({}^{4}\)University of Warwick \({}^{5}\)University of Toronto \({}^{6}\)Vijil

\({}^{7}\)Vector Institute for Artificial Intelligence

###### Abstract

Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (RepNoise), a defence mechanism that operates even when attackers have access to the weights. RepNoise works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process as long as they are drawn from the same distribution of the attack set. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the efficacy of our defence lies in its "depth": the degree to which information about harmful representations is removed across _all layers_ of the LLM. We also find areas where RepNoise still remains ineffective and highlight how those limitations can inform future research.

## 1 Introduction

Despite the benefits to both research and commercial development, open-sourcing large language models (LLMs) poses several risks  such as facilitating the development of weapons . Such risks are not isolated to only open-source models, weights of proprietary models expose fine-tuning APIs  which can be used for constructing harmful models and for reconstructing weights at inference time . The risk of LLMs assisting in harmful tasks is exacerbated by their increasing ability to follow instructions, carry out sophisticated tasks, and the ease with which they can be trained and run. Developers attempt to mitigate these risks  by developing safety guardrails that prevent LLMs from performing harmful tasks at inference time. However, these guardrails are easily circumvented either through back doors , adversarial attacks , or harmful fine-tuning . We argue that no matter how sophisticated safety guardrails become, _models vulnerable to harmful fine-tuning and amenable to malicious modifications are fundamentally unsafe_.

We propose Representation Noising (RepNoise) as the first defence to mitigate in-distribution harmful fine-tuning attacks (HFAs) for LLMs in the natural language generation setting where _the defender has no control of the model_ after the attacker attains its weights. Our work is inspired by the observationthat safety mechanisms in LLMs are concentrated in a small proportion of the model weights (identified through ablation studies in ) and displace rather than replace harmful capabilities (identified by probing studies in [35; 38]); despite showing safe behaviour at inference-time, harmful behaviour can be easily recovered . RepNoise works by _removing the information structure of harmful representations such that they are much harder to recover_ during subsequent HFAs (fig. 1). We refer readers to appendix A.1 for precise definitions of representations, information in representations, and removing information.

Our contributions are as follows:

1. We provide a defence method derived from an understanding of training dynamics that would make harmful representations harder to recover (SS 3).
2. We present extensive experimental evidence that our method can mitigate training on harmful question-answering and toxic content generation tasks while maintaining the ability to train the model on harmless tasks and preserving LLM capability (SS 4).
3. We empirically investigate "how" our method works and show that it does indeed remove information about harmful representations _across all layers_ in the LLM (SS 5).

## 2 Harmful Fine-tuning and Defence Criteria

Harmful fine-tuning vulnerabilities in LLMs are established in several works [40; 51; 66; 67]. To formalize the problem, we borrow from Rosati et al.'s  harmful fine-tuning attack threat model.

Harmful Fine-tuning Attack (HFA):Suppose an attacker has access to a set of model weights for a safety-aligned model--such as llama2-7b-chat--which should refuse the attacker's harmful requests. Let \(M_{|t=0}\) indicate a model \(M\) with the parameters \(\), where \(t\) indicates the training steps taken during the HFA. The initial unattacked model is indexed at \(t=0\). The attacker utilizes some harmful dataset \(D_{}\) to train a LLM to be harmful (i.e. minimize language modeling loss on this dataset). The defender considers this behaviour unacceptable and _does not want their model to be able to be trained towards this end._\(D_{}\) consists of prompts \(X=\{X_{i}\}_{i=1}^{n}\) and target responses \(Y=\{Y_{i}\}_{i=1}^{n}\). Using the harmful dataset, a malicious actor attempts to find parameters at training step \(t^{*}\) that minimizes eq. (1), resulting in a model that is able to behave in a way designated harmful by the defender:

\[[t^{*}]=_{[t]}_{(X,Y) D_{}}[ (M_{[t]}(X),Y)],\] (1)

where the loss \((M_{}(X),Y)\) is a typical causal language modeling loss.

Harmful Fine-tuning Success:A HFA is formally designated a success if it causes the subject model to exceed a chosen threshold \(\) set by the defender on a given safety metric \(g():g(M_{})>\)

Figure 1: Representation Noising pushes the intermediate activations of harmful text inputs (their representations) towards random directions, effectively reducing the mutual information between harmful representations and harmful text sequences and making it difficult to recover harmful representations through HFAs. We visualize this here as a projection (PCA) which isn’t able to recover any structure.

Examples of safety metrics include Attack Success Rate [45, ASR] and probability outputs from the Jigsaw Perspective API . In this formalism, a viable defence is one in which the attacker will have to spend more training steps \(t\) to achieve \(g(M_{})>\)_than they can afford in time and/or compute_.

Immunization ConditionsTo defend against HFAs, Rosati et al.  provides four conditions for a successful defence, called _Immunization Conditions (IC)_. We introduce them below in order to motivate our search for a method that fulfills them theoretically and experimentally.

1. _Resistance_: To increase the required effort for the attacker, a defended model \(M_{}^{*}\) should maximize the minimum number of training steps needed to produce a successful fine-tuning attack, i.e. \(_{}t()\) such that \(g(M_{[t]}^{*},D_{})>\)
2. _Stability_: A defended model should preserve helpful (or harmless) behaviour of the undefended model. We model this using a reference dataset or task \(D_{}\) where we want the defended model \(M_{}^{*}\) to perform approximately the same on some task measured by a task capability metric \(f()\), i.e. \(f(M_{[t=0]},D_{}) f(M_{[t=0]}^{*},D_{})\). For example this could be ROUGE-1.
3. _Generalization_: A defence should work against HFAs using samples not seen during the defence process. Given disjoint subsets \(D_{},D_{}^{} D_{}\) the defence procedure producing \(M_{}^{*}\) based only on \(D_{}\) should be resistant to HFAs using \(D_{}^{}\).
4. _Trainability_: To retain the adaptability of the defended model, it should be trainable on harmless datasets with similar efficiency and effectiveness as the undefended model i.e. \[_{}f(M_{[t_{1}]}^{*},D_{})_{}f (M_{[t_{2}]},D_{});|t_{1}-t_{2}|,\] where \(t_{1}\) and \(t_{2}\) are the training steps for training the defended and undefended model equal within some small tolerance \(\) and \(f()\) is the same as in _Stability_.

A model that fulfills these conditions is said to be "immunized." SS 4 provides an operationalization of these conditions. Below, we provide the a defence that fulfills all these conditions.

## 3 Method

We propose Representation Noising, a method which fulfills the Immunization Criteria by reducing the mutual information between intermediate activations of harmful text sequences (their representations) and harmful text inputs before the attacker gains access and performs an HFA. Recall that after the attacker has access, the defender cannot intervene. There is evidence that current safety methods only suppress or route around preserved harmful representations . This allows harmful behaviour to be easily recovered through HFAs. RepNoise instead aims to remove information about harmful tasks from intermediate activations over harmful text sequences, to make it difficult for the model to relearn such information in future. The formal definition of information removal removal is based on mutual information between intermediate activations and generative outputs based on those activations and is specified in appendix A.1 which we encourage review before proceeding. RepNoise consists of a three-part loss: (i) reduce the predictive information in the weights for generating harmful outputs, (ii) retain capabilities by minimizing loss on harmless inputs, and (iii) push harmful representations towards random noise to remove harmful information. Fig. 1 presents a high-level intuition of our method.

Transition Probabilities and Adversarial LossOur goal is to derive a loss function which will _minimize the likelihood of recovering the mutual information_\(I(Z_{harmful};Y_{harmful})\) which is a quantity that measures how effective intermediate activations (or representations) \(Z_{harmful}\) of harmful input sequences \(X_{harmful}\) are at predicting the output token distribution \(Y_{harmful}\). We are motivated by the observation in  that the number of training steps taken to fine-tune a model trained on one source task to another target task minimized by \(M_{[t^{+}]}\) can be modeled as a transition probability over paths (or training trajectories) in a loss landscape. Formally in the language modeling case, a task here is a token output distribution over some dataset \(D\). The source task is our initial pre-training distribution and the target task is the generative distribution of harmful text tokens in \(D_{}\). The loss landscape is the space (\(^{[]}\)) of the value of a loss function \(_{D}\) at every value of \(\). Paths or training trajectories in this landscape are the sequence of parameter configurations \(_{i}\) during a training procedure for the iterates \(i\).

From , the transition probability is \(p(_{t^{*}},t^{*}\,|\,_{t=0},t=0)=_{0}^{t^{*}}p(_{t}\,|\, _{t=0},t=0)\,d_{t}\). In other words, the probability of reaching \(M_{[t^{*}]}\) at any given time step \(t\) is the accumulation of individual transition probabilities over all paths reaching \(_{t^{*}}\) starting at \(t=0\). We can model the transition probability with two components: a _static distance_, which depends on the distance of the loss functions between the initial model \(_{t=0}\) and the target model \(_{t^{*}}\) that minimizes \(_{}\), and a dynamic _reachability term_ that depends on the magnitude of the gradients of the loss function with respect to parameters \(_{t}\) and determines the number of paths during a training procedure that contain both \(_{t=0}\) and \(_{t^{*}}\) in the path sequence as defined above. To clarify, reachability is computed starting over all initial weight configurations, the outer integral \(_{_{t=0}}^{_{t^{*}}}\) and the paths starting from these initial weights that end in the optimal \(_{t^{*}}\), the inner integral \(_{0}^{t^{*}}\), so that the probability is

\[p(_{t^{*}},t^{*}\,|\,_{t=0},t=0)_{}(_{t^{*}})-_{}(_{t=0})]}}_{ }_{_{t=0}}^{_{t^{*}}}^{t^{*}}_{}(_{t})dt}}_{}d(t).\] (2)

Note that this is a simplified approximation of Achille et al. ; see appendix A for details. As the defender, we are only able to influence the initial set of weights \(_{t=0}\). Therefore our goal is to find a way to modify the model weights so that we minimize eq. (2). Where we have:

**Theorem 1**.: _Consider a set of initial weights \(_{t=0}\) as well as weights \(_{t^{*}}\) that minimize a loss function \(_{}\) over the dataset \(\). The \(_{t=0}\) that minimize the transition probability \(p(_{t^{*}},t^{*}\,|\,_{t=0},t=0)\) are given by the weights \(_{t=0}\) that minimize the mutual information \(I(X;Z_{})\) between the inputs to a neural network \(X\) drawn from \(\) and the intermediate activations of that neural network \(Z_{}\) used to represent those inputs given the model weights \(\). For which we have the minimizer \(\,I(X;Z_{})\)._

A full proof of this is given in appendix A. Based on information bottleneck theory , we view multiple-layer neural networks as consisting of an encoder which maps the inputs \(X\) to representations \(Z\) and a decoder which maps the learned \(Z\) to outputs \(Y\). From Wang et al.  theorem 2, we can minimize the mutual information \(I(Z;Y)\) directly by performing gradient ascent (\(_{}\)), which would decrease both the static distance and the reachability condition eq. (2) (see appendix A).

\[_{}=_{(X_{harmful},Y_{harmful}) D_{}}(M_{}(X_{harmful}),Y_{harmful}).\] (3)

However, gradient ascent over harmful samples can degrade overall language modeling capabilities, so we add a term to ensure that performance over harmless samples is not degraded. In view of the immunization conditions in Section 2, this ensures stability. Combining them, we get an adversarial loss function:

\[_{}=_{}-_{},\] (4)

where \(\) is a regularization term, and \(_{}=_{(X,Y) D_{}}(M_{ }(X),Y)\).

Representation NoisingAs we see later (Section 5), simply minimizing adversarial loss does not effectively remove the ability to predict harmful text sequences from the activations over harmful text sequences. This is because despite having low mutual information \(I(Y;Z)\), the mutual information between the inputs and the encoder layers \(I(Z;X)\) can still be high. Consequently, it is possible for representations to retain the ability to generate harmful token sequences.

The data processing inequality , \(I(Y;X) I(Z;X)\) implies that minimizing \(I(X;Z)\) minimizes \(I(X;Y)\). We can do this by minimizing the KL divergence between the distribution of harmful representations given harmful input token sequences \(_{[t=0]}(Z\,|\,X)\) and Gaussian noise \((0,)\): if these two distributions were the same then \(Z\) would have no information about \(X\). This yields the noise loss \(_{}=KL(p(Z\,|\,X)\,||\,(0,))\). Combining this with eq. (4) using another regularization term \(\), we get the loss function for Representation Noising (RepNoise) which satisfies Theorem 1.

\[_{}=_{}+ _{}=_{}+_{}- _{}.\] (5)

We use a layer-wise approach to minimize \(_{}\), with multi-kernel Maximum Mean Discrepancy (MMD) as a replacement for KL divergence that allows us to estimate the distribution of harmful representations. Full implementation details are in appendix B.1.

## 4 Experiments

We perform a series of experiments to evaluate how our defence meets the four immunization criteria in Section 2: we compare RepNoise with existing defence mechanisms in their ability to make llama-2-7b-chat resistant to HFAs SS 4.1 as well as evaluate RepNoise on Stability SS 4.2, Trainability SS 4.3, and Generalization SS 4.4.

### Resistance

Here we simulate an HFA on llama-2-7b-chat and measure the harmfulness of the models and a series of controls before and after these attacks. Appendix K reports similar experiments on llama-2-13b-chat and the safety-trained Qwen (0.5B to 7B) series of models. We perform HFAs in two domains: harmful question-answering and toxic content generation2. We measure attack strength in terms of the _learning rate_ and _number of samples_ used during supervised fine-tuning. Full details on our attack settings including rationale on learning rate choice can be found in appendix C.

To fine-tune for harmful question-answering, we use the BeaverTails harmful QA dataset  since it is a very large-scale dataset used in other attack literature , where the goal is to train an LLM to generate compliant answers to questions belonging to 14 categories of harm such as animal abuse and violent crime. For harmfulness evaluation, we use the logits of the harmful label after passing a question-answer pair into the harmfulness classifier trained on the BeaverTails dataset. The scores are computed as the mean of each individual logit score, for more details on how the classifier was trained as well as the scores is computed see appendix D.1.1. For toxic content generation, we use the DecodingTrust  split of Real Toxicity Prompts (RTP)  to fine-tune an LLM to generate highly toxic continuations. We perform toxicity evaluation using the mean toxicity scores from the Perspective API  (appendix D).

We compare RepNoise with several safety interventions and controls: the original model, a randomly initialised model (using Kaiming initialization ), additional safety training, gradient ascent, adversarial loss, and Security Vectors . A randomly initialized model allows us to measure how quickly we converge to generating harmful tokens from random initial conditions (training a model from scratch). Additional safety training is done by supervised fine-tuning the model on refusals to answer 10k unsafe harmful question-answering samples from BeaverTails. Gradient ascent uses the loss function in eq. (3), (appendix J shows layer-wise implementation results) for defence. Adversarial loss minimizes eq. (4), and RepNoise minimizes eq. (1). Finally, we implement Security Vectors, a defence where the defender _does have control over the fine-tuning process_. We train a LoRA adapter on our harmful dataset and use the frozen adapter _during the HFA_ (appendix F).

Table 1 shows results for the harmful QA task. HFAs without any defence mechanism substantially increase the harmfulness score (Base) on the base model. Attacks with higher learning rates and more data tend to be stronger. This replicates previous results about the effectiveness of HFAs at

   Defence Mechanism & \)} & \)} & \)} \\  & Pre-attack & 1k & 10k & 1k & 10k & 1k & 10k \\  Base: llama2-7b-chat & 0.05 & 0.47 & 0.74 & 0.73 & 0.72 & 0.74 & 0.73 \\ Random & 0.00 & 0.46 & 0.86 & 0.49 & 0.84 & 0.47 & 0.82 \\  Security Vectors & 0.05 & **0.07** & **0.08** & 0.23 & 0.37 & 0.52 & 0.66 \\ Vaccine (\(=1\)) & 0.05 & 0.28 & 0.73 & 0.70 & 0.73 & 0.72 & 0.76 \\ Vaccine (\(=10\)) & 0.05 & 0.28 & 0.72 & 0.75 & 0.72 & 0.76 & 0.73 \\  Additional safety training & 0.05 & 0.75 & 0.76 & 0.75 & 0.75 & 0.76 & 0.74 \\ Gradient ascent & 0.24 & 0.38 & 0.74 & 0.58 & 0.74 & 0.68 & 0.77 \\ Adversarial loss & 0.05 & 0.26 & 0.70 & 0.64 & 0.75 & 0.77 & 0.77 \\ RepNoise & 0.05 & 0.08 & 0.12 & **0.10** & **0.13** & **0.11** & **0.12** \\   

Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of harmfulQA from BeaverTails and learning rates \(\{3 10^{-5},6 10^{-5},8 10^{-5}\}\). Blue indicates lower harmfulness score than the base model.

circumventing safety training in LLMs [8; 40; 49; 51; 66]. Evaluating our defence mechanisms, Security Vectors provides some resistance but _RepNoise is the only defence method to consistently able to provide significant resistance across all attacks_ (Mann-Whitney \(U\)-test, \(p<0.001\)).3

Gradient ascent and adversarial loss offer some resistance for weak attacks, but they fail for stronger attacks. We hypothesize that harmful text generation is recovered quickly with these approaches because they leave the representation structure of harmful text sequeces intact (see SS 5). Randomly initializing an LLM is not a useful control for understanding HFAs, since simply fine-tuning with larger samples makes the model mimic harmful text from the dataset. Finally, additional safety training offers no resistance, indicating that some types of traditional safety methods (safety-oriented supervised fine-tuning) does not help to defend against HFAs.

Table 2 presents similar results for the toxic content generation task. In this case, there are 351 attack samples, so we vary attack strength across learning rates only, performing all HFAs for 4 epochs. In each setting, using a model immunized with RepNoise results in complete resistance.

### Stability

To evaluate if RepNoise causes a deterioration in unrelated harmless tasks compared to the base model, we use standard LLM benchmarks from the Eleuther AI LM Evaluation Harness : TruthfulQA , MMLU , Hellaswag , and ARC-easy . We also evaluate changes in the model's capabilities on domains related to harmfulness using the Ethics  and CrowS-Pairs  datasets.

Table 3 shows that a llama2-7b-chat model immunized using RepNoise achieves similar scores as the base model across all evaluations, indicating that RepNoise does not degrade capability. Beyond performance evaluations, our method does not degrade performance on other safety benchmarks, i.e. Ethics, or CrowS-Pairs. We perform further investigations on whether RepNoise has any effect on fairness (appendix E.4), exaggerated safety (appendix E.5), or adversarial robustness (appendix E.6)--with the general finding that RepNoise neither degrades nor improves inference-time safety over a baseline safety-guarded model which implies that RepNoise would supplement rather than replace other defence methods.

### Trainability

Recall that Trainability is the defence condition from above that states that after applying defences models should still be able to be trained effectively on harmless datasets. The reason for this is that defences which remove or degrade training on harmless datasets are less useful than ones that do not under our threat model where defenders want to release these models such that they can still be trained on harmless tasks.

    & Pre-attack & \(3 10^{-5}\) & \(6 10^{-5}\) & \(8 10^{-5}\) \\  Base & 0.24 & 0.40 & 0.74 & 0.71 \\  Security Vectors & 0.17 & 0.16 & 0.36 & 0.35 \\ Vaccine (\(=1\)) & 0.19 & 0.46 & 0.70 & 0.72 \\  Gradient Ascent & 0.05 & 0.12 & 0.44 & 0.76 \\ Adversarial loss & 0.00 & 0.00 & 0.77 & 0.78 \\ RepNoise & 0.17 & 0.00 & 0.05 & 0.07 \\   

Table 2: Toxicity score from Perspective API when the model is requested to continue highly toxic prompts. RepNoise is able to defend against training models for toxic content generation.

  
**Model** & **TruthfulQA** & **MMLU** & **Hellaswag** & **Winogrande** & **ARC** & **Ethics** & **CrowS** \\  Base & 0.38 & 0.46 & 0.58 & 0.66 & 0.74 & 0.59 & 0.64 \\ RepNoise & 0.37 & 0.45 & 0.57 & 0.66 & 0.72 & 0.60 & 0.63 \\   

Table 3: Evaluation of RepNoise on common language model capability benchmarks.

We evaluate Trainability by testing whether the defended model can still be trained towards harmless tasks. To this end, we measure the ROUGE-1 unigram overlap score on several text-to-data tasks from the GEM benchmark . In order to demonstrate trainability, we need to choose standard validated tasks for natural language generation that models are poor (zero-shot) at before training (very low ROUGE-1 scores) and achieve large performance increases in after training. We observe this for the base llama2-7b-chat model seeing consistently low initial scores in table 4. For this setting, we train the base model and its post-RepNoise version using 1 epoch and a learning rate of \(8 10^{-5}\), using only the training splits of each dataset. We perform evaluations on the test splits of respective datasets. Full details of each dataset are given in appendix D.

The results in table 4 show that a llama2-7b-chat model hardened using RepNoise _retains the capability to be further trained on harmless tasks_, despite not being able to be trained on harmful tasks.

We further evaluated whether fine-tuning on a harmless task results in undoing safety guards or makes models more susceptible to HFAs. After fine-tuning on each GEM dataset, a HFA is performed with \(3 10^{-5}\) with 1k samples from BeaverTails as above. Unlike the results of Qi et al. , both the base model and RepNoise are not made more harmful after harmless fine-tuning on GEM. However, training on GEM does seem to make the HFA more effective (readers can compare with the same attack in table 1). Even for RepNoise we see a small increase in attack efficacy after training the model on CACAPO which indicates the possibility that additional harmless fine-tuning could undo the RepNoise defence, a vulnerability which future work should explore.

We replicated the benign and AOA attacks of Qi et al.  results in appendix E.3 and found that RepNoise can mitigate them.

### Generalization

The BeaverTails dataset categorizes samples into 14 types of harm. We evaluate the generalization performance of RepNoise by withholding five categories of harm when performing the defence training and then evaluate the attack by performing an attack using 1k samples from that subset. We also perform an additional experiment (**Half**) where RepNoise is trained using 5k randomly selected samples from BeaverTails and a subsequent attack is performed using 5k unseen samples.

The results in Table 5 show that a defence using RepNoise is able to generalize to a defence against HFAs performed with unseen samples and unseen types of harm. However, it is important to note that these attacks are still in-distribution since the unseen types of harm are still drawn from the same

    & **ViGGO** & **E2E NLG** & **DART** & **CACAPO** & **ConvWeather** \\
**ROUGE-1** & & & & & & \\  Base & 0.19 / 0.83 & 0.20 / 0.74 & 0.23 / 0.53 & 0.18 / 0.66 & 0.06 / 0.25 \\ RepNoise & 0.20 / 0.83 & 0.25 / 0.74 & 0.25 / 0.53 & 0.18 / 0.67 & 0.08 / 0.25 \\ 
**Harmfulness** & & & & & \\  Base & 0.03/0.75 & 0.05/0.65 & 0.05/0.69 & 0.06/0.67 & 0.05/0.55 \\ RepNoise & 0.00/0.00 & 0.16/0.01 & 0.00/0.00 & 0.02/0.27 & 0.01/0.08 \\   

Table 4: ROUGE-1 score of RepNoise on GEM structured generation tasks before/after being fine-tuned. Harmfulness scores before and after performing an attack at learning rate \(3 10^{-5}\) with 1k samples from BeaverTails.

  
**LR** & **Model** & **Crime** & **Privacy** & **Toxic** & **Violence** & **Sexually explicit** & **Half** \\  \(3 10^{-5}\) & Base & 0.49 & 0.51 & 0.40 & 0.52 & 0.53 & 0.35 \\  & RepNoise & 0.08 & 0.05 & 0.06 & 0.09 & 0.01 & 0.08 \\  \(6 10^{-5}\) & Base & 0.76 & 0.75 & 0.76 & 0.75 & 0.81 & 0.76 \\  & RepNoise & 0.10 & 0.09 & 0.10 & 0.09 & 0.00 & 0.12 \\  \(8 10^{-5}\) & Base & 0.77 & 0.75 & 0.80 & 0.74 & 0.76 & 0.74 \\  & RepNoise & 0.13 & 0.12 & 0.12 & 0.14 & 0.00 & 0.10 \\   

Table 5: Harmfulness scores after performing fine-tuning on harm types withheld during the RepNoise defence.

BeaverTails distribution that the defender has seen. Importantly, RepNoise is not an effective defence against unseen sample out-of-distribution which we demonstrate in appendix E.2 using a distribution shift in the attack set to the HEX-PHI attack set . A comprehensive analysis of additional attacks (appendix E) and ablations (appendix J) are presented so readers can clearly understand the limitations of RepNoise.

## 5 Mechanistic Analysis

We conjecture that RepNoise works because it reduces the information about harmfulness in representations _across all layers_ of the neural network, making them harder to recover. This is inspired by observations from previous studies, which found that popular safety methods merely route around the harmful representations , that fine-tuning only learns a wrapper on top of existing representations , and that harmful representations are easily recovered .

Model WeightsTo illustrate the above conjecture, we measure the change in the weights of each layer across various defence mechanisms (a method common in unlearning literature, see Tarun et al.  for example). In Fig. 2, we plot layer-wise \(L_{2}\) differences between the weights of the original model or a defence and the weights of a harmfully fine-tuned base model (using BeaverTails with LR \(8 10^{-5}\) @ 10k samples). We observe that defence using adversarial loss is indeed "superficial," in that the largest difference is observed in the last layers. In comparison, weight change across layers is more uniform for RepNoise.

However, we can't be certain what these weight changes mean. In order to actually test our conjecture about depth, we perform RepNoise but freeze the top layers, the middle 10 layers, and the earliest 10 layers (table 6). Freezing the LM Head or the layers between 20 and 31 makes little to no difference and not much difference for lower sample sizes freezing the last layer, freezing the middle layers degrades the performance of RepNoise, and freezing the earliest layers results in a complete lack of defence. This result confirms our conjecture about the necessity of "depth" for effective defence.

Token ProbabilitiesTo investigate the degree to which harmful representations are removed across layers we can look at how harmful and harmless token sequences are promoted throughout the network. We look at the mean log probability of 100 randomly selected harmful and harmless

    & \(3 10^{-5}\) @ 1k & \(3 10^{-5}\) @ 10k & \(6 10^{-5}\) @ 1k \\  Undefended Model & 0.47 & 0.74 & 0.73 \\ All Layers & 0.08 & 0.12 & 0.10 \\ Freeze LM Head & 0.08 & 0.10 & 0.11 \\ Freeze Last Layer & 0.08 & 0.67 & 0.09 \\ Freeze Layers 20-31 & 0.10 & 0.13 & 0.10 \\ Freeze Layers 10-20 & 0.13 & 0.55 & 0.56 \\ Freeze Layers 0-10 & 0.73 & 0.73 & 0.72 \\   

Table 6: Freezing earlier layers prevents effective defence indicating that the ‘depth’ of the defence is critical.

Figure 2: \(L_{2}\) distance between weights of each layer between the base model, a successfully attacked model and two defences. RepNoise’s differences spread through the layers compared to Adversarial loss where the weight differences are concentrated at the later layers indicative of superficial defence.

samples throughout the layers by placing the language model head on the activations across each layer . Confirming our findings above (Fig. 3) adversarial loss leads to a shallow defence that mostly reduces the likelihood of the harmful token sequences towards the last layer. In contrast, RepNoise_demotes harmful tokens across layers mostly uniformly_.

Knowledge RepresentationsFig. 4 illustrates the representation space learned by each model. After shallow defences, harmful representations maintain distinct structures along each of the two principal component directions. While RepNoise maintains separability between harmful and harmless sequences along one of the principal components, the "spread" of each harmful representation in both directions is dramatically reduced compared other models. This corroborates that RepNoise has reduced the representation quality of the harmful samples since we can't find a projection that illustrates any meaningful structure between these samples.

To further analyze the information about harmfulness contained in the representations of different models, we train a linear probe to predict whether an input to a model was harmful or not, based on the mean activations at each layer of the model. Such a probe can achieve high accuracy by using the information about harmfulness in the LLM. For each model, we input 15k examples from BeaverTails, with half being unsafe, and collect the average activations across each layer for each sample. We then train a binary linear classifier on 80% of them, measure the resulting accuracy on a held-out test set, and repeat with 10 random seeds. 4

Across all models, the probes perform best in the middle layers and very poorly in earlier layers. Figure 4(a) shows that an HFA does not improve the probe's accuracy compared to the base model. Similarly, an attack on a model defended by RepNoise also does not increa

Figure 4: PCA across 100 harmful and harmless samples from BeaverTails on the activations of the last layer.

Figure 5: Harmful probe accuracy on (a) base model and attacked model, (b) base model and models trained with RepNoise (\(=4\)) and adversarial Loss, and (c) base model, RepNoise model and an attacked RepNoise model

Figure 3: Log probability of harmful and harmless sequences across layers. Notice how adversarial loss mostly depromotes harmful tokens towards the last layer. This is done more evenly across layers for RepNoise indicating comprehensive and deep information removal.

harmfulness (Figure 4(c)). This indicates that HFAs do not make an LLM more harmful _by learning new information about harmfulness_, but merely use information already contained in the model.

_The probe achieves significantly (Student's \(t\)-test, \(p<5e-12\)) lower accuracy on the model defended by RepNoise than for a model defended by adversarial loss or the base model_ (Figure 4(b)). This supports our suggestion that adversarial loss does not remove information about harmful representations from the base model, but RepNoise does. Lastly, Figure 4(c) illustrates that fine-tuning using harmful data does not result in relearning the information removed by RepNoise.

## 6 Related Work

Preserving the effects of safety fine-tuningSome prior work addresses the attenuation of safety fine-tuning's influence on model behavior which typically occurs during benign fine-tuning.  achieve this for LLMs with an instruction fine-tuning dataset which contains safety material and  do so for LLMs by modifying the prompt template used during fine-tuning.  use a modified LoRA algorithm for fine-tuning which maintains safety influence and  use a model fusion-based technique to get around the limitations of performing safety fine-tuning either before or after fine-tuning on tasks. Other solutions could benefit from methods that correct the general tendency for models to perform more poorly in some domains after being fine-tuned in others, such as the method presented in . Though these methods may be sufficient for their stated goals, our work aims to mitigate the effects of harmful fine-tuning, regardless of whether they come about from benign or harmful fine-tuning.

Defence against harmful fine-tuningFew works have attempted to defend against HFAs. Meta-learning approaches have been used to reduce the fine-tunability of Language Models for harmful classification tasks  and prevent image classification models from being fine-tuned on restricted domains . However, meta-learning approaches can be uninterpretable and too computationally expensive for LLMs.  added a security vector during training to trick a model into thinking that it has already learned the harmful task.  keep embeddings close to the original embeddings by adding a perturbation loss called 'Vaccination' and  provides a similar defence by ensuring that weights don't drift too far from original weights during training. While [31; 32; 70] assume the defender retains control over the fine-tuning process, we focus on settings where the defender cannot intervene after the weights are released or stolen. For a full review of current HFAs and threat models, we refer readers to .

## 7 Limitations

The primary limitation of RepNoise is that it is still possible to find ways to defeat it at higher learning rates and with more data (appendix E.1). It is also sensitive to variations in hyperparameter choices (appendix J). We have evidence that RepNoise could be improved quite simply by doing more comprehensive hyperparameter searches and constructing larger defensive datasets. However, our method requires paired safe and unsafe examples which makes data collection more expensive and complex. Finally, while we did demonstrate across in-distribution harmful subsets in BeaverTails, we did not observe out-of-distribution generalization from defences on harmful question-answering to attacks using toxic content generation. Even smaller distribution shifts such as from defence using BeaverTails to an unseen harmful question-answering dataset HEX-PHI (appendix E.2) can break RepNoise --as such, future work should focus on improving the generalization capabilities of RepNoise as it is unlikely that defenders will have access to samples with significant in-distribution overlap with attackers which limits the effectiveness of our proposed method.

While our empirical settings and attacks provide promising first directions for LLM immunization research, future work should invest in stronger attack settings to emulate worst-case attacks and investigate different types of harm. Finally, our work is limited to supervised fine-tuning attacks in LLMs. Additional settings in different modalities such as evaluating attempts at developing malicious agents through harmful reinforcement learning (e.g., "reverse DPO" ) are a critical topic for future research. We explored the implications of RepNoise for inference-time adversarial attacks (appendix E.6) but future work should explore the robustness of RepNoise to additional types of attacks like latent adversarial attacks , activation engineering-based attacks  or adaptive attacks such as using decoding-time modifications  to circumvent our defence.