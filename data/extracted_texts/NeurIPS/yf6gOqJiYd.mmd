# CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept

YuXuan Wu\({}^{1}\), Bonaventure F. P. Dossou\({}^{2,3}\), Dianbo Liu\({}^{1}\)

\({}^{1}\) National University of Singapore, Singapore, Singapore

\({}^{2}\) McGill University, Montreal, Canada

\({}^{3}\) Mila Quebec AI Institute, Montreal, Canada

###### Abstract

Large Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.

## 1 Introduction

Large language Models (LLMs) have been widely used in various applications, generating text responses that attempt to create the equivalent of human conversations OpenAI et al. (2024). These models leverage vast scientific literature to facilitate and accelerate interdisciplinary research Taylor et al. (2022) while drawing upon large datasets of human-generated content to provide professional advice. However, in many cases, such data is a double-edged sword. Including personal information or sensitive scientific knowledge can be beneficial or, conversely, harmful. For instance, Soice et al. (2023) discusses how LLMs, when used by non-experts, can enable the creation of biological agents, posing both potential benefits and significant risks.

In response to these concerns, machine unlearning has emerged as a promising research area focused on selectively removing specific data points or information from a trained model. This approach helps mitigate the misuse of sensitive data and addresses privacy concerns. Existing solutions, such as Sharded, Isolated, Sliced, and Aggregated (SISA) training Bourtoule et al. (2020), primarily involve partitioning the training data into disjoint shards and retraining models on these individual shards. Although effective in certain scenarios, these methods are often time-consuming, resource-intensive, and lack scalability when applied to large models like LLMs. Moreover, traditional approaches typically require specialized data structures or full retraining, making them impractical for dynamic or complex tasks.

Given these limitations, there is an increasing demand for zero-shot unlearning methods, which aim to remove specific information without retraining or specialized data structures. Unlike traditional unlearning techniques that rely on retraining portions of the model, zero-shot unlearning seeks to directly eliminate the influence of specific data points or pieces of information from the model's learned representation--without additional computational steps or parameter adjustments. Moreover, zero-shot unlearning is inherently more scalable, especially for large models like LLMs, as it avoids the inefficiencies associated with data partitioning and retraining.

Our approach builds upon using discrete representations as the latent space for unlearning. Discrete representations, generated through Vector Quantization (VQ) van den Oord et al. (2018), offer a natural structure for organizing the latent space to enable selective information removal. Discrete representations can be seen as a form of disentanglement, a concept rooted in classical research Bengio et al. (2014), which emphasizes learning representations that disentangle the various factors of variation in data. This allows for the separation of different explanatory sources within the data.

Additionally, Elhage et al. (2022) explores how neurons in models can represent multiple superposed features, introducing the concept of using dictionaries to disentangle these superpositions. Building on this notion, we propose employing discrete representations to disentangle the model's internal structure, thereby enabling selective unlearning. By tracking and modifying discrete codes within the latent space, we aim to achieve efficient and targeted removal of sensitive or unwanted information.

Our contributions are as follows:

* we propose a novel zero-shot unlearning method based on discrete latent representations.
* we demonstrate how Vector Quantization (VQ) can structure the latent space, facilitating the selective removal of information in an amortized manner.
* we extend our method beyond traditional machine unlearning techniques, primarily designed for classification tasks, to handle complex language tasks associated with language models, addressing a broader scope of applications.
* Our approach provides a baseline for unlearning in language models and validates the effectiveness of our method.

## 2 Related Work

Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent charts. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy and Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems.

While these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks.

In a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal.

Influence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning.

Recently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information. While this is effective for tasks like token classification, it may struggle with the more complex context and semantics in LLMs, underscoring the need for scalable, adaptable unlearning techniques tailored to these models.

## 3 Methodology

To address the challenges of zero-shot machine unlearning, we propose a novel approach that leverages _codebook features_ to bottleneck latent representations within a language model, enabling the targeted unlearning of specific knowledge by altering related codebook embeddings. Initially introduced by Tamkin et al. (2023), codebook features efficiently compress the activation space of neural networks by introducing a sparse discrete bottleneck. This bottleneck can be further optimized to isolate the codes most relevant to specific topics in the input, offering deeper insight and control over the model's response and interpretation. By utilizing this discrete latent representation, we can more effectively identify and remove the specific information encoded in the codebook corresponding to the input's targeted knowledge.

The following section details our approach to employing _codebook features_ to efficiently identify and unlearn specific areas of related information in a zero-shot manner. This process ensures that the model can no longer effectively handle prompts that contain the target information to unlearn.

### Codebook Features

The core concept behind employing codebook features is to transform the original activations from a hidden layer into a representation regulated by a codebook. Let \(a^{F}\) represent the activation vector from a hidden layer, where \(F\) denotes the dimensionality of the activations. We use a codebook \(C=\{c_{k}\}_{k=1}^{K}^{K F}\), where \(K\) represents the number of code vectors. The codebook offers a compressed, discrete representation of the original activations. To perform this transformation, we calculate the cosine similarity between the activation \(a\) and each code vector \(c_{k}\) in the codebook:

\[(a,c_{k})=}{\|a\|\|c_{k}\|},\] (1)

for each code vector \(c_{k}\) in the codebook. We then identify the top \(S\) (where \(S 1\)) most similar code vectors corresponding to the activation \(a\). The index set \(\) of these top \(S\) code vectors is defined as:

\[=_{S}(\{k k\{1,,K\},(a,c_{k}) \}).\] (2)

Figure 1: **CodeUnlearnâ€”Our Amortized Zero-Shot Machine Unlearning for Language Models. Left: Discrete latent bottleneck in the transformer architecture. After applying the residual connection, the multi-head attention output is discretized using a discrete embedding vocabulary, referred to as the codebook. This approach prevents information leakage via the residual connection, ensuring that the codebook effectively regulates and interprets the networkâ€™s behavior. Right: Zero-shot machine unlearning is achieved by removing the discrete codes in the codebook that correspond to the targeted information.**The output of the codebook transformation is given by:

\[=_{k}c_{k},\] (3)

where \(\) is the index set of the \(S\) most similar code vectors, selected based on the highest cosine similarity scores. In the unlearning procedure, the activated codes corresponding to \(a\) are identified as the targets for removal.

### Codebook Settings

Multiple CodebooksIn prior work Tamkin et al. (2023), multiple codebooks were applied to each attention head, with the outputs concatenated across heads. Each attention head operates with its own codebook, selecting codes independently. The chosen codes from each head are then concatenated to produce the final output for that attention layer, effectively allowing the model to represent a broader set of features through the combination of different codebooks. Using multiple codebooks across attention heads can lead to a superposition effect, as described by Elhage et al. (2022). Superposition refers to the phenomenon where linear representations can encode more features than the dimensions, effectively allowing the neural network to simulate more extensive networks. In this case, combining multiple codebooks across attention heads allows for a significantly more comprehensive set of activations to be represented, even when using only the top \(S=1\) codebooks. However, tracking which individual codebooks contribute to specific activation patterns becomes challenging. Rather than relying on the output of a single codebook, the overall representation emerges from the combined outputs of all the codebooks.

Single CodebookAs shown in Section 3, to maintain interpretability, we focus on using a single codebook, positioning it after the multi-head attention layer and residual connection to prevent information leakage. However, in a single codebook setup, selecting only \(S=1\) leads to a significant drop in model performance, as a single codebook feature is insufficient to capture the complexity of the activation space. In Cai (2024), the author rigorously demonstrates that treating word vectors as mappings allows a finite vocabulary to achieve infinite approximation through composition. Based on this insight, we employ \(S>1\) in our approach. While this may slightly affect code discretization and information clarity, it strikes a balance between model performance and interpretability.

### Codebook with Sparse Autoencoders

Our goal is to decompose the activation space into sparse, interpretable features rather than reconstructing the original input. To accomplish this, we incorporate the Sparse Autoencoder (SAE) concept. The SAE applies a linear transformation encoder with a ReLU activation function to project the activations into a higher-dimensional space, effectively decomposing features. A linear transformation decoder is employed used to reconstruct the activations.

In line with the SAE structure, we introduce a linear transformation encoder with ReLU before the codebook and a linear transformation decoder after the codebook. This setup provides two significant benefits for machine unlearning:

* **Security through ReLU**: The ReLU activation function ensures that the extracted features are non-linear and sparse, making it more difficult to recover or reconstruct the original input from the features. This acts as a safeguard, reducing the likelihood of information leakage. By enforcing sparsity and non-linearity, ReLU provides greater control over feature representation, allowing us to obscure specific activations and protect data integrity during machine-unlearning processes.
* **Decentralization of Information**: Sparsity promotes the decentralization of encoded information, which helps isolate and unlearn specific patterns or features without disrupting the rest of the model. This targeted approach allows for more precise unlearning of sensitive or undesired information.

EncoderThe encoder is responsible for projecting the activation vector \(a^{d}\) into a higher-dimensional space. This is achieved using a weight matrix \(W_{E}^{d F}\) and a bias vector \(b_{E}^{d}\).

A ReLU activation function follows the projection to introduce non-linearity:

\[h_{enc}=(W_{enc}a+b_{enc}).\] (4)

CodebookAfter encoding, the sparse representation \(h_{enc}\) is transformed using the codebook. The cosine similarity between \(h_{enc}\) and each code vector \(c_{k}\{c_{1},c_{2},,c_{K}\}\) is calculated as:

\[(h_{enc},c_{k})= c_{k}}{\|h_{enc}\|\|c_{k}\|}.\] (5)

The top \(S\) most similar code vectors are selected:

\[=_{S}(\{k k\{1,,K\},(h_{enc}, c_{k})\}).\] (6)

The output of the codebook transformation is then:

\[_{enc}=_{k}c_{k}.\] (7)

DecoderThe decoder then maps \(_{enc}\) back to the original activation space using a weight matrix \(W_{dec}^{F d}\) and a bias vector \(b_{dec}^{F}\):

\[=W_{dec}_{enc}+b_{dec}.\] (8)

### Training the Codebook

Reconstruction LossAs with the Sparse Autoencoder (SAE) and codebook models, we utilize the Mean Squared Error (MSE) loss as the primary loss function. The MSE loss can be expressed as:

\[_{}=_{i=1}^{N}\|a_{i}-_{i}\|_{2}^{2},\] (9)

where \(N\) is the number of samples, \(a_{i}\) is the original activation, and \(_{i}\) is the reconstructed activation obtained from the decoder.

Additionally, to promote sparsity and enforce more distinct and sparse internal feature representations within each codebook vector, we introduce an \(L_{1}\) penalty term on the codebook activations. This encourages the model to represent each code with sparser and more well-separated internal features. The overall loss function incorporating this sparsity constraint is defined as:

\[_{}=_{i=1}^{N}\|a_{i}-_{i}\|_{ 2}^{2}+_{k}_{f=1}^{F}|c_{k}^{f}|,\] (10)

where \(\) represents the set of indices for the top \(S\) most similar code vectors, \(c_{k}\) refers to the \(k\)-th codebook vector, \(F\) denotes the dimensionality of the code vectors, and \(\) is a regularization coefficient that controls the strength of the \(L_{1}\) penalty term. In our experiments, we set \(\) to \(1 10^{-6}\) to balance sparsity with reconstruction accuracy.

Joint Training for Machine UnlearningBoth the SAE and codebook features are used to reconstruct the input \(a\), but this presents a critical issue in the context of machine unlearning: one could easily remove the codebook layer, reverting the model to its original state, which negates the unlearning process. To address this, it is vital to ensure that the model is trained so that the downstream components are entirely dependent on the output of the codebook. At the same time, the upstream layers must learn to generate activations that conform to the codebook's representations. This joint training approach ensures that the entire model relies on the codebook's representation, making it harder to bypass or remove without degrading performance. The joint loss function for this training process is defined as:

\[_{}=_{}+_{},\] (11)

where \(_{}\) refers to the reconstruction loss for the codebook, and \(_{}\) represents the Cross-Entropy loss for the original language modeling or task-specific objective.

### Code Retrieval

As shown in Figure 2, after training, the codebook encodes a set of representative codes \(C=\{c_{k}\}_{k=1}^{K}^{K F}\) that are sparse and represent different features. To perform unlearning, we retrieve the codes activated for specific inputs and identify which codes are enriched for a particular topic. The model can effectively unlearn the associated information by deleting the corresponding enriched codes from the codebook. The key steps involve retrieving these relevant codes for each input and determining their relationship to the target topic.

Because of the nature of the attention mechanism, the activation of these codes also depends on the surrounding context. This means we are not just identifying individual words that activate specific codes but retrieving codes that represent the broader topic within the input context. To unlearn a specific topic \(T\), consider a dataset \(D_{T}\) with samples related to topic \(T\), alongside with the remaining irrelevant data set \(D_{R}\). We create a control dataset \(D_{}\) by replacing words associated with \(T\) in \(D_{T}\) with unrelated words, ensuring the context remains consistent. By comparing the code activations between \(D_{T}\) and \(D_{}\), we can identify and search for the codes linked to topic \(T\).

For each code \(c_{k}\) activated in the dataset, we compute its frequency in both datasets by considering the top \(S^{}\) activated codes:

\[f_{k}(D_{T}) =}_{i=1}^{N_{T}}(k_{T}(a_{i} )),\] (12) \[f_{k}(D_{}) =}}_{j=1}^{N_{}}(k _{}(a_{j})),\] (13)

where \(_{T}(a_{i})\) represents the set of indices of the top \(S^{}\) activated codes for activation \(a_{i}\) in dataset \(D_{T}\), and \(_{}(a_{j})\) is similarly defined for \(D_{}\). \(N_{T}\) and \(N_{}\) denote the sample sizes of \(D_{T}\) and \(D_{}\), respectively. \(\) is the indicator function that checks whether code \(k\) is in the set of activated codes. The hyperparameter \(S^{}\) controls the number of top activated codes considered, thereby influencing the number of codes to be removed.

To quantify the enrichment of code \(c_{k}\) for topic \(T\), we use the following formula:

\[(c_{k},T)=_{2}((D_{T})+}{f_{k}(D_{})+}),\] (14)

where \(\) is a small constant added to avoid division by zero. When \(R(c_{k},T)\) is positive, it indicates that the code \(c_{k}\) is enriched in dataset \(D_{T}\) relative to \(D_{}\). However, if the frequency of \(c_{k}\) in

Figure 2: **Unlearning a Target Topic in a Language Model.** The zero-shot unlearning process begins by identifying codes enriched in data subsets with the target topic (\(D_{T}\)) as opposed to the subset without it (\(D_{}\)). Codes with p-values less than 0.05 are removed from the codebook. After this removal, the model exhibits significantly decreased performance on target information inputs.

is zero and its frequency in \(D_{T}\) is very low, such codes should not be removed, as they are likely accidental activations. Removing these codes could lead to unintended side effects, as they may not be strongly related to the topic \(T\) despite being present in the dataset.

Therefore, we used a chi-squared test to calculate the p-value of \(R(c_{k},T)\) to determine if the code \(c_{k}\) is enriched for topic \(T\). For those codes with p-values smaller than 0.05, we regard them as enriched codes in \(D_{T}\) and remove them from the codebook. We define the set of enriched codes as \(_{R>0,p<0.05}=\{c_{k} R(c_{k},T)>0p 0.05\}\).

### Metrics

In our work, we not solely assess the absolute drop in performance within the topic or non-topic datasets but also need to compare the relative decline between them. Instead, to fairly compare the models and the datasets, we used normalized percentage improvement to evaluate the performance of the unlearning procedure. The performance improvement percentage is set to 0 for the zero-shot model and 1 for the codebook model, which is the upper bound. In contrast, the performance drop percentage is set to 1 for the zero-shot model and 0 for the codebook model. We use four evaluation metrics to assess the effectiveness of the unlearning procedure and the overall quality of the remaining information in the output. These metrics include: We use four evaluation metrics to assess the impact of the unlearning procedure on translation quality and semantic preservation: BLEUPapineni et al. (2002), METEORBanerjee and Lavie (2005), BERTScoreZhang et al. (2020), and Bart-ScoreYuan et al. (2021). BLEU offers a general accuracy measure, and METEOR builds on BLEU by considering synonymy and word order, often providing a more sensitive quality assessment. BERTScore leverages contextual embeddings to evaluate semantic similarity, crucial for detecting whether unlearning procedures change the sentence's meaning. Bart-Score evaluates fluency and informativeness using pre-trained BART models, with scores reflecting log-likelihood, so close to zero indicates better quality. BERTScore and Bart-Score offer insight into more subtle changes, and percentage change trends are prioritized for a comprehensive analysis.

## 4 Experiments and results

We applied the codebook features combined with SAE on a large language model(LLM) and trained it on tasks that exhibit clear distinctions between correct and incorrect answers. After training, we unlearned the model on several specific topics to measure the degradation in performance on the unlearned issues while ensuring minimal impact on the other topics. An example of the unlearning effect on the topic of '_love_' is shown in Table 1. The results illustrate that as more codes related to the target topic were deleted, the model's translation became less accurate in representing the original meaning. For instance:

The translation introduces minor inaccuracies in the case of \(S^{}=8\) (16 codes deleted). As the number of deleted codes increases to \(S^{}=72\) (133 codes deleted), the translation significantly deviates

    \\ 
**English** & She had made efforts to love him, and she had repented with tears for having yielded to another! \\ 
**Ground Truth** & Elle avaiit fai des efforts pour lâ€™aimer, et elle sâ€™Ã©tait repentie en pleurant dâ€™avoir cÃ©dÃ© Ã  un autre. \\ 
**Codebook Model** & Elle avaiit fai des efforts pour lâ€™aimer, et elle avaiit repris des larmes pour avoir renonce Ã  un autre! \\  \(S^{}=8\), delete 16 codes & Elle avaiit fai des efforts pour lâ€™aimer, et elle avaiit repris des larmes pour lâ€™avoir acquitÃ© dâ€™un autre! \\ \(S^{}=24\), delete 52 codes & Elle avaiit fai des efforts pour l'e receevior, et elle avairrepris des larmes pour avoir renonce Ã  un autre. \\ \(S^{}=72\), delete 133 codes & Elle avaiit fai des efforts pour le metre en Ã©tat, et elle avaiit repris des larmes pour sâ€™en rendre Ã  un autre. \\   

Table 1: Examples of unlearning on topic â€™_love_â€™from the original meaning, showing the model's inability to maintain accuracy on the target topic. This demonstrates that the model successfully forgets the '_love_' concept and the wrong meaning can even interfere with the rest of the sentences.

    &  &  \\   & & \(BLEU\) & \(METEOR\) & \(BERT-P\) & \(BART\) \\   & \(D_{T}^{}\) & 0.16 _(-112.52)_ & 0.39 _(-117.76)_ & 0.80 _(-118.88)_ & -4.80 _(-143.96)_ \\   & \(D_{R}\) & 0.18 _(-37.80)_ & 0.42 _(-57.82)_ & 0.81 _(-58.25)_ & -5.71 _(-35.06)_ \\   & \(D_{T}^{}\) & 0.19 _(-113.12)_ & 0.42 _(-138.47)_ & 0.80 _(-134.60)_ & -5.15 _(-164.68)_ \\   & \(D_{R}\) & 0.16 _(-65.70)_ & 0.39 _(-64.38)_ & 0.80 _(-94.63)_ & -6.10 _(-94.60)_ \\   & \(D_{T}^{}\) & 0.20 _(-72.10)_ & 0.47 _(-140.71)_ & 0.83 _(-84.44)_ & -5.16 _(-87.90)_ \\   & \(D_{R}\) & 0.19 _(-9.72)_ & 0.44 _(-9.04)_ & 0.82 _(-9.66)_ & -5.97 _(-0.53)_ \\   & \(D_{T}^{}\) & 0.18 _(-70.61)_ & 0.43 _(-70.78)_ & 0.81 _(-60.84)_ & -5.03 _(-79.81)_ \\   & \(D_{R}\) & 0.20 _(-26.64)_ & 0.47 _(-12.48)_ & 0.83 _(-14.20)_ & -5.81 _(-36.01)_ \\   & \(D_{T}^{}\) & 0.15 _(-144.83)_ & 0.33 _(-249.51)_ & 0.78 _(-182.02)_ & -4.95 _(-309.34)_ \\   & \(D_{R}\) & 0.16 _(-87.65)_ & 0.39 _(-94.51)_ & 0.81 _(-7.46)_ & -6.02 _(-133.35)_ \\   & \(D_{T}^{}\) & 0.12 _(-157.45)_ & 0.38 _(-218.04)_ & 0.80 _(-403.04)_ & -4.85 _(-119.99)_ \\   & \(D_{R}\) & 0.16 _(-10.09)_ & 0.49 _(-22.99)_ & 0.83 _(-47.65)_ & -6.12 _(-27.15)_ \\   & \(D_{T}^{}\) & 0.16 _(-85.16)_ & 0.40 _(-138.04)_ & 0.80 _(-115.56)_ & -4.70 _(-62.91)_ \\   & \(D_{R}\) & 0.19 _(-16.12)_ & 0.47 _(-2.15)_ & 0.83 _(-3.01)_ & -5.78 _(-97.36)_ \\   

Table 2: **Unlearning Results for Different Topics**

Figure 3: **Performance Drop after Unlearning on the Topic â€™_Love_â€™. Performance Drop after Unlearning on the Topic â€™_Love_â€™. The X-axis shows the model variations, with the first column as the original model. Columns 2 to 8 represent increasing levels of unlearning, with the number indicating the top \(S\) codes used and removed. The Y-axis represents the percentage change in various metrics compared to the original model. As more codes are deleted, the modelâ€™s performance on the target topic declines rapidly, while performance on non-topic content remains more stable.**

Dataset BuildingThe dataset comprises three parts: (1) training, (2) validation, and (3) test datasets. The training dataset is used for both training and unlearning, while the validation and test datasets assess the performance of the unlearned model. For the unlearning procedure, we filtered prompts containing target words, sampling 500 instances for \(D_{T}\) and then generated \(D_{T}\). All relevant prompts from the test and validation datasets were used to create the dataset \(D^{}_{T}\), while irrelevant prompts were used to construct the dataset \(D_{R}\) for evaluation. We trained a T5-small model Raffel et al. (2023) with codebook features on the opus_books/en-fr dataset. A codebook with 25k codes and 512 dimensions was applied at the third layer of the encoder, as this layer likely captures more abstract, high-level features, ideal for our approach Templeton et al. (2024).

After training, we identified specific topics within the training dataset and performed the unlearning procedure. We tested seven values for \(S^{}\) ranging from \(8(1 S)\) to \(104(13 S)\), each resulting in a different number of deleted codes. This led to a deletion of approximately 0.064% to 0.828% of the total codes in the codebook.

As shown in Figure 3, as the number of searched and deleted codes increases, the performance on the topic deteriorates rapidly. Although performance on non-topic deteriorates simultaneously, it is far better than the topic. For instance, in the case of the '_love_' topic, when \(S^{}=104(13 S)\), which corresponds to searching for the top 104 most similar codes in the codebook for each activation, about 0.828% of the codes were deleted. The improvement score for the target topic became negative, which means the unlearned model is worse than the zero-shot model. In contrast, the model's performance on non-topic is far better than the topic, demonstrating effective unlearning of the specific target while maintaining reasonable performance on unrelated information.

Beyond conceptual topics like '_love_,' we also applied the unlearning procedure to the frequently occurring name '_Julien_' in the dataset. Names carry specific semantic significance in language models, much like critical topics, making '_Julien_' an ideal test case to assess the method's effectiveness in removing personal information, such as names, while preserving performance on unrelated content. As shown in Figure 4, the unlearning process led to a noticeable performance decline for '_Julien_' as the number of removed codes increased. Similar to the '_love_' topic, the model's performance on non-target content remained relatively stable. This further illustrates the versatility of the proposed approach in effectively unlearning targeted information, whether it is conceptual (like '_love_') or personal (like '_Julien_'), while maintaining accuracy on non-topic content. Following unlearning, the model attempts to rely on other similar codes; however, the meanings of these codes are signif

Figure 4: **Performance Drop after Unlearning on the Topic â€™_Julien_â€™. Similar to the â€™_love_â€™ topic, we tested the unlearning procedure on the name â€™_Julien_â€™.**

icantly different. As a result, the unlearned target topic interferes, hindering the model's ability to comprehend the entire sentence fully.

In addition to the '_love_' and '_Julien_' topics, we performed unlearning on several other topics such as '_Captain_,' '_Poor_, '_Wish_,' '_White_,' and '_Black_.' Table 2, shows the performance degradation across various topics after applying the unlearning procedure, with the number of deleted codes indicated in parentheses next to each topic. The values represent actual scores and the normalized improvement drop in performance, calculated relative to the zero-shot and baseline models before unlearning. A negative value indicates a performance decline. As \(S^{}\) increases (for instance, \(S^{}=13 8\) here), the performance gap between \(D^{}_{T}\) and \(D_{R}\) widens, demonstrating effective unlearning of the target topic with minimal impact on irrelevant information.

To further assess the unlearning performance, we also evaluate the synonymy of the target word, such as '_like_' in place of '_love_' shown in Figure 5. Ideally, the model's performance on the '_like_' topic should also worsen, suggesting that the unlearning procedure removes the specific target information and the broader context related to that concept. Our approach diverges from traditional data-point-unlearning tasks by removing the codes close to the activation space, which is essential in unlearning conceptual or contextual knowledge rather than isolated instances.

## 5 Conclusion

In this work, we introduced CodeUnlearn, a novel framework for zero-shot machine unlearning in Large Language Models (LLMs). Leveraging codebook features and Sparse Autoencoders (SAEs), we devised a method that effectively isolates and removes specific knowledge, ensuring that the targeted data and its contextual associations are erased from the model. Unlike previous methods, which required retraining or were limited to classification tasks, CodeUnlearn operates amortized and zero-shot, providing an efficient and scalable solution for unlearning in complex, generative models like LLMs. Our approach uses a discrete concept representation to regulate the flow of information in a language model, enabling the unlearning of specific topics while preserving overall model performance on unrelated tasks. The results show that CodeUnlearn successfully mitigates the model's ability to reproduce the unlearned information without requiring additional training, achieving substantial unlearning effectiveness and maintaining interpretability.

Figure 5: **Metrics after unlearning topic â€˜_love_â€™ and test on â€˜_like_â€™, The model unlearned the â€˜_love_â€™ topic but also deteriorated the performance on the â€˜_like_â€™ topic, which suggests that the unlearning procedure removes not only the specific target information but also the relevant context.**