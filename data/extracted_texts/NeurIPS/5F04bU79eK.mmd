# Provable Guarantees for Neural Networks via Gradient Feature Learning

Zhenmei Shi, Junyi Wei, Yingyu Liang

University of Wisconsin, Madison

zhmeishi@cs.wisc.edu,jwei53@wisc.edu,yliang@cs.wisc.edu

Equal contribution.

###### Abstract

Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.

## 1 Introduction

Neural network (NN) learning has achieved remarkable empirical success and has been a main driving force for the recent progress in machine learning and artificial intelligence. On the other hand, theoretical understandings significantly lag behind. Traditional analysis approaches are not adequate due to the overparameterization of practical networks and the non-convex optimization in the training via gradient descent. One line of work (e.g. [9; 31; 38; 60; 71; 123] and many others) shows under proper conditions, heavily overparameterized networks are approximately linear models over data-independent features, i.e., a linear function on the Neural Tangent Kernel (NTK). While making weak assumptions about the data and thus applicable to various settings, this approach requires the network learning to be approximately using fixed data-independent features (i.e., the kernel regime, or fixed feature methods). It thus fails to capture the feature learning ability of networks (i.e., to learn a feature mapping for the inputs which allow accurate prediction), which is widely believed to be the key factor to their empirical success in many applications (e.g., [54; 77; 117; 119]). To study feature learning in networks, a recent line of work (e.g. [5; 6; 14; 33; 52; 72; 76; 116] and others) shows examples where networks provably enjoy advantages over fixed feature methods (including NTK), under different settings and assumptions. While providing more insights, these studies typically focus on specific problems, and their analyses exploit the specific properties of the problems and appear to be unrelated to each other. _Is there a common principle for feature learning in networks via gradient descent? Is there a unified analysis framework that can clarify the principle and also lead to provable error guarantees for prototypical problem settings?_

In this work, we take a step toward this goal by proposing a gradient feature learning framework for analyzing two-layer network learning by gradient descent. (1) The framework makes essentially no assumption about the data distribution and can be applied to various problems. Furthermore, it is centered around features from gradients, clearly illustrating how gradient descent leads to feature learning in networks and subsequently accurate predictions. (2) It leads to error guarantees competitive with the optimal in a family of networks that use the features induced by gradients on thedata distribution. Then for a specific problem with structured data distributions, if the optimal in the induced family is small, the framework gives a small error guarantee.

We then apply the framework to several prototypical problems: mixtures of Gaussians, parity functions, linear data, and multiple-index models. These have been used for studying network learning (in particular, for the feature learning ability), but with different and seemingly unrelated analyses. In contrast, straightforward applications of our framework give small error guarantees, where the main effort is to compute the optimal in the induced family. Furthermore, in some cases, such as parities, we can handle more general data distributions than in the existing work.

Finally, we also demonstrate that the framework sheds light on several interesting network learning phenomena or implications such as feature learning beyond the kernel regime, lottery ticket hypothesis (LTH), simplicity bias, learning over different data distributions, and new perspectives about roadmaps forward. Due to space limitations, we present implications about features beyond the kernel regime and LTH in the main body but defer the other implications in Appendix C with a brief here. (1) For simplicity bias, it is generally believed that the optimization has some _implicit regularization_ effect that restricts learning dynamics to a low capacity subset of the whole hypothesis class, so can lead to good generalization [53; 90]. Our framework provides an explanation that the learning first learns simpler functions and then more sophisticated ones. (2) For learning over different data distributions, we provide data-dependent non-vacuous guarantees, as our framework can be viewed as using the optimal gradient-induced NN to measure or quantify the "complexity" of the problem. For easier problems, this quantity is smaller, and our framework can give a better error bound to derive guarantees. (3) For new perspectives about roadmaps forward, our framework suggests the strong representation power of NN is actually the key to successful learning, while traditional ones suggest strong representation power leads to vacuous generalization bounds [19; 33]. Thus, we suggest a different analysis road. Traditional analysis typically first reasons about the optimal based on the whole function class then analyzes how NN learns proper features and reaches the optimal. In contrast, our framework defines feature family first, and then reasons about the optimal based on it.

## 2 Related Work

**Neural Networks Learning Analysis.** Recently there has been an increasing interest in the analysis of network learning. One line of work connects the sufficiently over-parameterized neural network to linear methods around its initialization like NTK (e.g. [9; 11; 20; 21; 31; 38; 49; 60; 62; 69; 71; 78; 82; 91; 93; 95; 114; 121; 122] and more), so that the neural network training is a convex problem. The key idea is that it suffices to consider the first-order Tyler expansion of the neural network around the origin when the initialization is large enough. However, NTK lies in the lazy training (kernel) regime that excludes feature learning [29; 50; 68; 113]. Many studies (e.g. [2; 5; 6; 8; 12; 14; 22; 26; 33; 37; 51; 52; 57; 58; 70; 72; 73; 76; 99; 112; 115; 116] and more) show that neural networks take advantage over NTK empirically and theoretically. Another line of work is the mean-field (MF) analysis of neural networks (e.g. [27; 28; 36; 79; 80; 100; 106] and more). The insight is to see the training dynamics of a sufficiently large-width neural network as a PDE. It uses a smaller initialization than the NTK so that the parameters may move away from the initialization. However, the MF does not provide explicit convergence rates and requires an unrealistically large width of the neural network. One more line of work is neural networks max-margin analysis (e.g. [30; 47; 48; 56; 61; 63; 74; 75; 83; 85; 107; 109] and more). They need a strong assumption that the convergence starts from weights having perfect training accuracy, while feature learning happens in the early stage of training. To explain the success of neural networks beyond the limitation mentioned above, some work introduces the low intrinsic dimension of data distributions [17; 18; 23; 24; 25; 44; 67; 104; 108; 124]. Another recent line of work is that a trained network can exactly recover the ground truth or optimal solution or teacher network [3; 4; 10; 39; 84; 87; 94; 96; 120], but they have strong assumptions on data distribution or model structure, e.g., Gaussian marginals. [1; 40; 55; 110; 111] show that training dynamics of neural networks have multiple phases, e.g., feature learning at the beginning, and then dynamics in convex optimization which requires proxy convexity  or PL condition  or special data structure.

**Feature Learning Based on Gradient Analysis.** A recent line of work is studying how features emerge from the gradient. [7; 46] consider linear separable data and show that the first few gradient steps can learn good features, and the later steps learn a good network on neurons with these features. [33; 45; 105] have similar conclusions on non-linear data (e.g., parity functions), while in their problems one feature is sufficient for accurate prediction (i.e., single-index data model).

 considers multiple-index with low-degree polynomials as labeling functions and shows that a one-step gradient update can learn multiple features that lead to accurate prediction. [13; 81] studies one gradient step feature improvements at different learning rates.  proposes Recursive Feature Machines to show the mechanism of recursively feature learning but without giving a final loss guarantee. These studies consider specific problems and exploit properties of the data to analyze the gradient delicately, while our work provides a general framework applicable to different problems.

## 3 Gradient Feature Learning Framework

**Problem Setup.** We denote \([n]:=\{1,2,,n\}\) and \((),(),()\) to omit the \(\) term inside. Let \(^{d}\) denote the input space, \(\) the label space. Let \(\) be an arbitrary data distribution over \(\). Denote the class of two-layer networks with \(m\) neurons as:

\[_{d,m}:=f_{(,,)}f_{( ,,)}():=^{} (^{}-)=_{i[m]}_{i}[(_{i},-_{i}) ]},\] (1)

where \((z)=(z,0)\) is the ReLU activation function, \(^{m}\) is the second layer weight, \(^{d m}\) is the first layer weight, \(_{i}\) is the \(i\)-th column of \(\) (i.e., the weight for the \(i\)-th neuron), and \(^{m}\) is the bias for the neurons. For technical simplicity, we only train \(,\) but not \(\). Let superscript \((t)\) denote the time step, e.g., \(f_{(^{(t)},^{(t)},)}\) denote the network at time step \(t\). Denote \(:=(,,),\ ^{(t)}:=(^{(t)}, ^{(t)},)\). The goal of neural network learning is to minimize the expected risk, i.e., \(_{}(f):=_{(,y)} _{(,y)}(f),\) where \(_{(,y)}(f)=(yf())\) is the loss on an example \((,y)\) for some loss function \(()\), e.g., the hinge loss \((z)=\{0,1-z\}\), and the logistic loss \((z)=[1+(-z)]\). We also consider \(_{2}\) regularization. The regularized loss with regularization coefficient \(\) is \(_{}^{}(f):=_{}(f)+(\|\|_{F}^{2}+\|\|_{2}^{2})\). Given a training set with \(n\) i.i.d. samples \(=\{(^{(l)},y^{(t)})\}_{l[n]}\) from \(\), the empirical risk and its regularized version are:

\[}_{}(f): =_{l[n]}_{(^{(l)},y^{(l)} )}(f),}_{}^{}(f):=}_{}(f)+(\|\|_{F}^{2}+\| \|_{2}^{2}).\] (2)

Then the training process is summarized in Algorithm 1.

```  Initialize \((^{(0)},^{(0)},)\) for\(t=1\)to\(T\)do  Sample \(^{(t-1)}^{n}\) \(^{(t)}=^{(t-1)}-^{(t)}_{}}_{^{(t-1)}}^{^{(t)}}(f_{^{(t-1)}}), ^{(t)}=^{(t-1)}-^{(t)}_{}}_{^{(t-1)}}^{^{(t)}}(f_{^{(t-1)}})\) endfor ```

**Algorithm 1** Network Training via Gradient Descent

In the whole paper, we need some natural assumptions about the data and the loss.

**Assumption 3.1**.: _We assume \([\|\|_{2}] B_{x1}\), \([\|\|_{2}^{2}] B_{x2}\), \(\|\|_{2} B_{x}\) and for any label \(y\), we have \(|y| 1\). We assume the loss function \(()\) is a 1-Lipschitz convex decreasing function, normalized \((0)=1,|^{}(0)|=(1)\), and \(()=0\)._

**Remark 3.2**.: _The above are natural assumptions. Most input distributions have the bounded norms required, and the typical binary classification \(=\{ 1\}\) satisfies the requirement. Also, the most popular loss functions satisfy the assumption, e.g., the hinge loss and logistic loss._

### Warm Up: A Simple Setting with Frozen First Layer

To illustrate some high-level intuition, we first consider a simple setting where the first layer is frozen after one gradient update, i.e., no updates to \(\) for \(t 2\) in Algorithm 1.

The first idea of our framework is to provide guarantees compared to the optimal in a family of networks. Here let us consider networks with specific weights for the first layer:

**Definition 3.3**.: _For some fixed \(^{d m},^{d}\), and a parameter \(B_{a2}\), consider the following family of networks \(_{,,B_{a2}}\), and the optimal approximation network loss in this family:_

\[_{,,B_{a2}}:=f_{(,, )}_{d,m}\|\|_{2} B_{a2}}, _{,,B_{a2}}:=_{f_{ ,,B_{a2}}}_{}(f).\] (3)The second idea is to compare to networks using features from gradient descent. As an illustrative example, we now provide guarantees compared to networks with first layer weights \(^{(1)}\) (i.e., the weights after the first gradient step):

**Theorem 3.4** (Simple Setting).: _Assume \(}_{}(f_{(,^{(1)}, )})\) is \(L\)-smooth to \(\). Let \(^{(t)}=,^{(t)}=0\), for all \(t\{2,3,,T\}\). Training by Algorithm 1 with no updates for the first layer after the first gradient step, w.h.p., there exists \(t[T]\) such that_

\[_{}(f_{(^{(t)},^{(1)}, )})_{^{(1)},,B_{a2}}+O ^{(1)}\|_{2}^{2}+B_{a2}^{2})}{T}+^{2}( \|^{(1)}\|_{2}^{2},B_{a2}^{2}+\|\|_{2}^{2})}{n}}.\]

Intuitively, the theorem shows that if the weight \(^{(1)}\) after a one-step gradient gives a good set of neurons in the sense that there exists a classifier on top of these neurons with low loss, then the network will learn to approximate this good classifier and achieve low loss. The proof is based on standard convex optimization and the Rademacher complexity (details in Appendix D.1).

Such an approach, while simple, has been used to obtain interesting results on network learning in existing work, which shows that \(^{(1)}\) can indeed give good neurons due to the structure of the special problems considered (e.g., parities on uniform inputs , or polynomials on a subspace ). However, it is unclear whether such intuition can still yield useful guarantees for other problems. So, for our purpose of building a general framework covering more prototypical problems, the challenge is what features from gradient descent should be considered so that the family of networks for comparison can achieve a low loss on other problems. The other challenge is that we would like to consider the typical case where the first layer weights are not frozen. In the following, we will introduce the core concept of Gradient Features to address the first challenge, and stipulate proper geometric properties of Gradient Features for the second challenge.

### Core Concepts in the Gradient Feature Learning Framework

Now, we will introduce the core concept in our framework, Gradient Features, and use it to build the family of networks to derive guarantees. As mentioned, we consider the setting where the first layer is not frozen. After the network learns good features, to ensure the updates in later gradient steps of the first layer are still benign for feature learning, we need some geometric conditions about the gradient features, which are measured by parameters in the definition of Gradient Features. The conditions are general enough, so that, as shown in Section 4, many prototypical problems satisfy them and the induced family of networks enjoys low loss, leading to useful guarantees. We begin by considering what features can be learned via gradients. Note that the gradient w.r.t. \(_{i}\) is

\[_{}(f)}{_{i}} =_{i}_{(,y)}[^{}(yf( ))y[^{}(_{i}, -_{i})]]\] \[=_{i}_{(,y)}[^{}(yf( ))y[_{i},> _{i}]].\]

Inspired by this, we define the following notion:

**Definition 3.5** (Simplified Gradient Vector).: _For any \(^{d}\), \(b\), a Simplified Gradient Vector is_

\[G(,b):=_{(,y)}[y[^{}>b]].\] (4)

**Remark 3.6**.: _Note that the definition of \(G(,b)\) ignores the term \(^{}(yf())\) in the gradient, where \(f\) is the model function. In the early stage of training (or the first gradient step), \(^{}()\) is approximately a constant, i.e., \(^{}(yf())^{}(0)\) due to the symmetric initialization (see Equation (8))._

**Definition 3.7** (Gradient Feature).: _For a unit vector \(D^{d}\) with \(\|D\|_{2}=1\), and a \((0,1)\), a direction neighborhood (cone) \(_{D,}\) is defined as:_

\[_{D,}:=\{\ \ |\ |,D |/\|\|_{2}>(1-)\}.\] (5)

Figure 1: An illustration of Gradient Feature, i.e., Definition 3.7 with random initialization (Gaussian), under Mixture of three Gaussian clusters in 3-dimension data space with blue/green/orange color. The Gradient Feature stays in three cones, where each center of the cone aligns with the corresponding Gaussian cluster center.

_Let \(^{d}\), \(b\) be random variables drawn from some distribution \(,\). A Gradient Feature set with parameters \(p,,B_{G}\) is defined as:_

\[S_{p,,B_{G}}(,):=(D,s)_{ ,b}G(,b)_{D,}\,,\,\|G(,b)\|_{2} B_{G}\,,\,s=b/|b| p}.\] (6)

**Remark 3.8**.: _When clear from context, write it as \(S_{p,,B_{G}}\). Gradient features (see Figure 1 for illustration) are simply normalized vectors \(D\) that are given (approximately) by the simplified gradient vectors. (Similarly, the normalized scalar \(s\) is given by the bias \(b\).) To be a useful gradient feature, we require the direction to be "hit" by sufficiently large simplified gradient vectors with sufficient large probability, so as to be distinguished from noise and remain useful throughout the gradient steps. Later we will use the gradient features when \(,\) are the initialization distributions._

To make use of the gradient features, we consider the following family of networks using these features and with bounded norms, and will provide guarantees compared to the best in this family:

**Definition 3.9** (Gradient Feature Induced Networks).: _The Gradient Feature Induced Networks are: \(_{d,m,B_{F},S}:=f_{(,,)} _{d,m} i[m],\;|_{i}| B_{a1},\| \|_{2} B_{a2},(_{i},_{i}/|_{i}|)  S,\;|_{i}| B_{b}}\), where \(S\) is some Gradient Feature set and \(B_{F}:=(B_{a1},B_{a2},B_{b})\) are some parameters._

**Remark 3.10**.: _In above definition, the weight and bias of a neuron are simply the scalings of some item in the feature set \(S\) (for simplicity the scaling of \(_{i}\) is absorbed into the scaling of \(_{i}\) and \(_{i}\))._

**Definition 3.11** (Optimal Approximation via Gradient Features).: _The optimal approximation network and loss using Gradient Feature Induced Networks \(_{d,r,B_{F},S}\) are defined as:_

\[f^{*}:=*{argmin}_{f_{d,r,B_{F},S}}_{ }(f),_{d,r,B_{F},S}:=_{f_{d,r, B_{F},S}}_{}(f).\] (7)

### Provable Guarantee via Gradient Feature Learning

To obtain the guarantees, we first specify the symmetric initialization. It is convenient for the analysis and is typical in existing analysis (e.g., ), though some other initialization can also work. Formally, we train a two-layer network with \(4m\) neurons, \(f_{(,,)}_{d,4m}\). We initialize \(_{i}^{(0)},_{i}^{(0)}\) from Gaussians and \(_{i}\) from a constant for \(i\{1,,m\}\), and initialize the parameters for \(i\{m+1,,4m\}\) accordingly to get a zero output initial network. Specifically:

\[i\{1,,m\}:_{i}^{(0)}(0,_{a}^{2}),_{i}^{(0)}(0,_{w}^{2}I), _{i}=,\] (8) \[i\{m+1,,2m\}:_{i}^{(0)}=- _{i-m}^{(0)},_{i}^{(0)}=-_{i-m}^{(0)}, _{i}=-_{i-m},\] \[i\{2m+1,,4m\}:_{i}^{(0)}=- _{i-2m}^{(0)},_{i}^{(0)}=_{i-2m}^{(0)}, _{i}=_{i-2m},\]

where \(_{a}^{2},_{w}^{2},>0\) are hyper-parameters. After initialization, \(,\) are updated as in Algorithm 1. We are now ready to present our main result in the framework.

**Theorem 3.12** (Main Result).: _Assume Assumption 3.1. For any \(,(0,1)\), if \(m e^{d}\) and_

\[m= (}(rB_{a1}B_{x1}}{B_{G}}})^{4}+}+( ())^{2}),\] \[T= ((B_{a2}B_{b}B_{ x1}}{(mp)^{}}+m)(}{B_{G}}}+ (mp)^{}})),\] \[= (pB_{x}^{2}B_{a2}^{4}B_{b}}{ ^{2}r^{2}B_{a1}^{2}B_{G}}+}B_{x2}}{B_{b}B_{G}} +^{2}}{B_{x2}}++(^{2}}+^{2}})}{|^{}(0)|^{2}}+),\]

_then with initialization (8) and proper hyper-parameter values, we have with probability \( 1-\) over the initialization and training samples, there exists \(t[T]\) in Algorithm 1 with:_

\[[(f_{^{(t)}}()) y] _{}(f_{^{(t)}})\] \[_{d,r,B_{F},S_{p,,B_{G}}}+rB_{a1}B_{x1}  n}}{B_{G}|^{}(0)|n^{}})}+.\]Intuitively, the theorem shows when a data distribution admits a small approximation error by some "ground-truth" network with \(r\) neurons using gradient features from \(S_{p,,B_{G}}\) (i.e., a small optimal approximate loss \(_{d,r,B_{r},S_{p,,B_{G}}}\)), the gradient descent training can successfully learn good neural networks with sufficiently many \(m\) neurons.

Now we discuss the requirements and the error guarantee. Viewing boundedness parameters \(B_{a1},B_{x1}\) etc. as constants, then the number \(m\) of neurons learned is roughly \((}{pe^{4}})\), a polynomial overparameterization compared to the "ground-truth" network. The proof shows that such an overparameterization is needed such that some neurons can capture the gradient features given by gradient descent. This is consistent with existing analysis about overparameterization network learning, and also consistent with existing empirical observations.

The error bound consists of three terms. The last term \(\) can be made arbitrarily small, while the other two depend on the concrete data distribution. Specifically, with larger \(r\) and \(\), the second term increases. While the first term (the optimal approximation loss) decreases, since a larger \(r\) means a larger "ground-truth" network family, and a larger \(\) means a larger Gradient Feature set \(S_{p,,B_{G}}\). So, there is a trade-off between these two terms. When we later apply the framework to concrete problems (e.g., mixtures of Gaussians, parity functions), we will show that depending on the specific data distribution, we can choose the proper values for \(r,\) to make the error small. This then leads to error guarantees for the concrete problems and demonstrates the unifying power of the framework. Please refer to Appendix D.3 for more discussion about our problem setup and our core concept, e.g., parameter choice, early stopping, the role of \(s\), activation functions, and so on.

**Proof Sketch.** The intuition in the proof of Theorem 3.12 is closely related to the notion of Gradient Features. First, the gradient descent will produce gradients that approximate the features in \(S_{p,,B_{G}}\). Then, the gradient descent update gives a good set of neurons, such that there exists an accurate classifier using these neurons with loss comparable to the optimal approximation loss. Finally, the training will learn to approximate the accurate classifier, resulting in the desired error guarantee. The complete proof is in Appendix D (the population version in Appendix D.2 and the empirical version in Appendix D.4), including the proper values for hyper-parameters such as \(^{(t)}\) in Theorem D.17. Below, we briefly sketch the key ideas and omit the technical details.

We first show that a large subset of neurons has gradients at the first step as good features. (The claim can be extended to multiple steps; for simplicity, we follow existing work (e.g., ) and present only the first step.) Let \(_{i}\) denote the gradient of the \(i\)-th neuron \(_{_{i}}_{}(f_{}^{(0)}})\). Denote the subset of neurons with nice gradients approximating feature \((D,s)\) as:

\[G_{(D,s),Nice}:=i[2m]:s=_{i}/|_{i}|, _{i},D>(1-)\|_{i}\|_{2},\|_{i }\|_{2}|_{i}^{(0)}|B_{G}}.\] (9)

**Lemma 3.13** (Feature Emergence).: _For any \(r\) size subset \(\{(D_{1},s_{1}),,(D_{r},s_{r})\} S_{p,,B_{G}}\), with probability at least \(1-re^{-(mp)}\), for all \(j[r]\), we have \(|G_{(D_{j},s_{j}),Nice}|\)._

This is because \(_{i}=^{}(0)_{i}^{(0)}_{(,y)}[ y^{}[_{i}^{(0)}, -_{i}]]=^{}(0)_{i}^{(0)}G (_{i}^{(0)},_{i})\). Now consider \(s_{j}=+1\) (the case \(-1\) is similar). Since \(_{i}\) is initialized by Gaussians, by \(_{i}\)'s connection to Gradient Features, we can see that for all \(i[m]\), \([i G_{(D_{j},+1),Nice}]\). The lemma follows from concentration via a large enough \(m\), i.e., sufficient overparameterization. The gradients allow obtaining a set of neurons approximating the "ground-truth" network with comparable loss:

**Lemma 3.14** (Existence of Good Networks).: _For any \((0,1)\), with proper hyper-parameter values, with probability at least \(1-\), there is \(}\) such that \(\|}\|_{0}=O(r)\) and \(f_{(},^{(1)},)}()=_{i=1}^{4 m}}_{i}(_{i}^{(1)}, -_{i})\) satisfies_

\[_{}(f_{(},^{(1)},) })_{d,r,B_{F},S_{p,,B_{G}}}+rB_{a1}B_{x1}( +}{B_{G}}}).\]

Given the good set of neurons, we finally show that the remaining gradient steps can learn an accurate classifier. Intuitively, with small step sizes \(^{(t)}\), the weights of the first layer \(_{i}\) do not change too much (stay in a neighborhood) while the second layer weights grow, and thus the learning is similar to convex learning using the good set of neurons. Technically, we adopt the online convex optimization analysis (Theorem D.5) in  to get the final loss guarantee in Theorem 3.12.

Applications in Special Cases

In this section we will apply the gradient feature learning framework to some specific problems, corresponding to concrete data distributions \(\). We primarily focus on prototypical problems for analyzing feature learning in networks. We will present here the results for mixtures of Gaussians and parity functions, and include the complete proofs and some other results in Appendix E.

### Mixtures of Gaussians

Mixtures of Gaussians are among the most fundamental and widely used statistical models. Recently, it has been used to study neural network learning, in particular, the effect of gradient descent for feature learning of two-layer neural networks and the advantage over fixed feature methods .

**Data Distributions.** We follow notations from . The data are from a mixture of \(r\) high-dimensional Gaussians, and each Gaussian is assigned to one of two possible labels in \(=\{ 1\}\). Let \((y)[r]\) denote the set of indices of Gaussians associated with the label \(y\). The data distribution is then: \(q(,y)=q(y)q(|y),q(|y)=_{j(y)}p _{j}_{j}(),\) where \(_{j}()\) is a multivariate normal distribution with mean \(_{j}\), covariance \(_{j}\), and \(p_{j}\) are chosen such that \(q(,y)\) is correctly normalized. We will make some assumptions about the Gaussians, for which we first introduce some notations.

\[D_{j}:=}{\|_{j}\|_{2}},_{j}:=_{j}/,  B_{ 1}:=_{j[r]}\|_{j}\|_{2}, B_{ 2}:=_{j [r]}\|_{j}\|_{2}, p_{B}:=_{j[r]}p_{j}.\]

**Assumption 4.1**.: _Let \(8 d\) be a parameter that will control our final error guarantee. Assume_

* _Equiprobable labels:_ \(q(-1)=q(+1)=1/2\)_._
* _For all_ \(j[r]\)_,_ \(_{j}=_{j}I_{d d}\)_. Let_ \(_{B}:=_{j[r]}_{j}\) _and_ \(_{B+}:=\{_{B},B_{ 2}\}\)_._
* \(r 2d, p_{B},(1/d+^{2} d/d}) B_{ 1} B_{ 2} d.\)__
* _The Gaussians are well-separated: for all_ \(i j[r]\)_, we have_ \(-1 D_{i},D_{j}\)_, where_ \(0\{,}{B_{ 2}}}\}\)_._

**Remark 4.2**.: _The first two assumptions are for simplicity; they can be relaxed. We can generalize our analysis to the mixture of Gaussians with unbalanced label probabilities and general covariances. The third assumption is to make sure that each Gaussian has a good amount of probability mass to be learned. The remaining assumptions are to make sure that the Gaussians are well-separated and can be distinguished by the learning algorithm._

We are now ready to apply the framework to these data distributions, for which we only need to compute the Gradient Feature set and the corresponding optimal approximation loss.

**Lemma 4.3** (Mixtures of Gaussians: Gradient Features).: \((D_{j},+1) S_{p,,B_{G}}\) _for all \(j[r]\), where_

\[p=}{} d^{(_{B +}{}^{2}/B_{_{1}}^{2})}},=}, B _{G}=p_{B}B_{ 1}-O(}{d^{0.9}}).\]

_Let \(f^{*}()=_{j=1}^{r}}{}} [( D_{j},-2_{B +})]\) whose hinge loss is at most \(}+}\)._

Given the values on gradient feature parameters \(p,,B_{G}\) and the optimal approximation loss \(_{d,r,B_{F},S_{p,,B_{G}}}\), the framework immediately leads to the following guarantee:

**Theorem 4.4** (Mixtures of Gaussians: Main Result).: _Assume Assumption 4.1. For any \(,(0,1)\), when Algorithm 1 uses hinge loss with_

\[m=(,,d^{( _{B+}{}^{2}/B_{ 1}^{2})},r,}) e^{d}, T= (m), n=(m)\]

_and proper hyper-parameters, then with probability at least \(1-\), there exists \(t[T]\) such that_

\[[(f_{^{(t)}}()) y]r}{d^{0.4-0.8}}+.\]The theorem shows that gradient descent can learn to a small error via learning the gradient features, given proper hyper-parameters. In particular, we need sufficient overparameterization (a sufficiently large number \(m\) of neurons). When \(_{B+}{}^{2}/B_{ 1}^{2}\) is a constant which is the prototypical interesting case, and we choose a constant \(\), then \(m\) is polynomial in the key parameters \(,,d,r,}\), and the error bound is inverse polynomial in \(d\). The complete proof is given in Appendix E.2.

 studies (almost) linear separable cases while our setting includes non-linear separable cases, e.g., XOR.  mainly studies neural network classification on 4 Gaussian clusters with XOR structured labels, while our setting is much more general, e.g., our cluster number can extend up to \(2d\).

#### 4.1.1 Mixtures of Gaussians: Beyond the Kernel Regime

As discussed in the introduction, it is important for the analysis to go beyond fixed feature methods such as NTK (i.e., the kernel regime), so as to capture the feature learning ability which is believed to be the key factor for the empirical success. We first review the fixed feature methods. Following , suppose \(\) is a data-independent feature mapping of dimension \(N\) with bounded features, i.e., \(:[-1,1]^{N}\). For \(B>0\), the family of linear models on \(\) with bounded norm \(B\) is \(_{B}=\{h(}):h(})=( }),w,\|w\|_{2} B\}.\) This can capture linear models on fixed finite-dimensional feature maps, e.g., NTK, and also infinite dimensional feature maps, e.g., kernels like RBF, that can be approximated by feature maps of polynomial dimensions [64; 98; 105].

Our framework indeed goes beyond fixed features and shows features from gradients are more powerful than features from random initialization, e.g., NTK. Our framework can show the advantage of network learning over kernel methods under the setting of  (4 Gaussian clusters with XOR structured labels). For large enough \(d\), our framework only needs roughly \(( d)\) neurons and \((( d)^{2})\) samples to achieve arbitrary small constant error (see Theorem E.18 when \(_{B}=1\)), while fixed feature methods need \((d^{2})\) features and \((d^{2})\) samples to achieve nontrivial errors (as proved in ). Moreover,  uses ODE to simulate the optimization process for the 2-layer networks learning XOR-shaped Gaussian mixture with \((1)\) neurons and gives convincing evidence that \((d)\) samples is enough to learn it, yet they do not give a rigorous convergence guarantee for this problem. We successfully derive a convergence guarantee and we require a much smaller sample size \((( d)^{2})\). For the proof (detailed in Appendix E.3), we only need to calculate the \(p,,B_{G}\) of the data distribution carefully and then inject these numbers into Theorem 3.12.

### Parity Functions

Parity functions are a canonical family of learning problems in computational learning theory, usually for showing theoretical computational barriers . The typical sparse parties over \(d\)-dim binary inputs \(\{ 1\}^{d}\) are \(_{i A}_{i}\) where \(A[d]\) is a subset of dimensions. Recent studies have shown that when the distribution of inputs \(\) has structures rather than uniform, neural networks can perform feature learning and finally learn parity functions with a small error, while methods without feature learning, e.g. NTK, cannot achieve as good results [33; 76; 105]. Thus, this has been a prototypical setting for studying feature learning phenomena in networks. Here we consider a generalization of this problem and show that our framework can show successful learning via gradient descent.

**Data Distributions.** Suppose \(^{d D}\) is an unknown dictionary with \(D\) columns that can be regarded as patterns. For simplicity, assume \(d=D\) and \(\) is orthonormal. Let \(^{d}\) be a hidden representation vector. Let \(A[D]\) be a subset of size \(rk\) corresponding to the class relevant patterns and \(r\) is an odd number. Then the input is generated by \(\), and some function on \(_{A}\) generates the label. WLOG, let \(A=\{1,,rk\}\), \(A^{}=\{rk+1,,d\}\). Also, we split \(A\) such that for all \(j[r]\), \(A_{j}=\{(j-1)k+1,,jk\}\). Then the input \(\) and the class label \(y\) are given by:

\[=,y=g^{*}(_{A})=_{j[r]} (_{A_{j}}),\] (10)

where \(g^{*}\) is the ground-truth labeling function mapping from \(^{rk}\) to \(=\{ 1\}\), \(_{A}\) is the sub-vector of \(\) with indices in \(A\), and \((_{A_{j}})=_{l A_{j}}_{l}\) is the parity function. We still need to specify the distribution \(\) of \(\), which determines the structure of the input distribution:

\[:=(1-2rp_{A})_{U}+_{j[r]}p_{A}(_{j,+} +_{j,-}).\] (11)For all corresponding \(_{A^{}}\) in \(\), we have \( l A^{}\), independently: \(_{l}=+1,&p_{o}\\ -1,&p_{o}\\ 0,&1-2p_{o}\), where \(p_{o}\) controls the signal noise ratio: if \(p_{o}\) is large, then there are many nonzero entries in \(A^{}\) which are noise interfering with the learning of the ground-truth labeling function on \(A\). For corresponding \(_{A}\), any \(j[r]\), we have

* In \(_{j,+}\), \(_{A_{j}}=[+1,+1,,+1]^{}\) and \(_{A A_{j}}\) only have zero elements.
* In \(_{j,-}\), \(_{A_{j}}=[-1,-1,,-1]^{}\) and \(_{A A_{j}}\) only have zero elements.
* In \(_{U}\), we have \(_{A}\) draw from \(\{+1,-1\}^{rk}\) uniformly.

In short, we have \(r\) parity functions each corresponding to a block of \(k\) dimensions; \(_{j,+}\) and \(_{j,-}\) stands for the component providing a strong signal for the \(j\)-th parity; \(_{U}\) corresponds to uniform distribution unrelated to any parity and providing weak learning signal; \(A^{}\) is the noise part. The label depends on the sum of the \(r\) parity functions.

**Assumption 4.5**.: _Let \(8 d\) be a parameter that will control our final error guarantee. Assume \(k\) is an odd number and: \(k( d), d rk+( r d), p_{o}=O( ), p_{A}.\)_

**Remark 4.6**.: _We set up the problem to be more general than the parity function learning in existing work. If \(r=1\), the labeling function reduces to the traditional \(k\)-sparse parties of \(d\) bits. The assumptions require \(k,d,\) and \(p_{A}\) to be sufficiently large so as to provide enough large signals for learning. Note that when \(k=,r=1,p_{o}=\), our analysis also holds, which shows our framework is beyond the kernel regime (discuss in detail in Section 4.2.1)._

To apply our framework, again we only need to compute the Gradient Feature set and the corresponding optimal loss. We first define the Gradient Features: For all \(j[r]\), let \(D_{j}=}_{l}}{\|_{l A_{j}}_{l }\|_{2}}.\)

**Lemma 4.7** (Parity Functions: Gradient Features).: _We have \((D_{j},+1),(D_{j},-1) S_{p,,B_{G}}\) for all \(j[r]\), where_

\[p=( d^{( r)}}), =}, B_{G}=p_{A}-O(}{d^{}}).\] (12)

_With gradient features from \(S_{p,,B_{G}}\), let \(f^{*}()=_{j=1}^{r}_{i=0}^{k}(-1)^{i+1} ( D_{j},-} )-2( D_{j},-})+( D_{j},- {2i-k+1}{})\) whose hinge loss is 0._

Above, we show that \(D_{j}\) is the "indicator function" for the subset \(A_{j}\) so that we can build the optimal neural network based on such directions. Given the values on gradient feature parameters and the optimal approximation loss, the framework immediately leads to the following guarantee:

**Theorem 4.8** (Parity Functions: Main Result).: _Assume Assumption 4.5. For any \(,(0,1)\), when Algorithm 1 uses hinge loss with_

\[m=(,,d^{( r)},k, }) e^{d}, T=(m), n= (m)\]

_and proper hyper-parameters, then with probability at least \(1-\), there exists \(t[T]\) such that_

\[[(f_{^{(t)}}()) y]} {d^{(-3)/2}}+.\]

The theorem shows that gradient descent can learn to a small error in this problem. We also need sufficient overparameterization: When \(r\) is a constant (e.g., \(r=1\) in existing work), and we choose a constant \(\), \(m\) is polynomial in \(,,d,k,}\), and the error bound is inverse polynomial in \(d\). The proof is in Appendix E.4. Our setting is more general than that in  which corresponds to \(=I,r=1,p_{A}=,p_{o}=\).  study single index learning, where one feature direction is enough for a two-layer network to recover the label, while our setting considers \(r\) directions \(D_{1},,D_{r}\), so the network needs to learn multiple directions to get a small error.

#### 4.2.1 Parity Functions: Beyond the Kernel Regime

Again, we show that our framework indeed goes beyond fixed features under parity functions. Our problem setting in Section 4.2 is general enough to include the problem setting in . Their lower bound for fixed feature methods directly applies to our case and leads to the following:

**Proposition 4.9**.: _There exists a data distribution in the parity learning setting in Section 4.2 with \(=I,r=1,p_{A}=,k=,p_{o}=\), such that all \(h_{B}\) have hinge-loss at least \(-B}{2^{k}}\)._

This means to get an inverse-polynomially small loss, fixed feature models need to have an exponentially large size, i.e., either the number of features \(N\) or the norm \(B\) needs to be exponential in \(k\). In contrast, Theorem 4.8 shows our framework guarantees a small loss with a polynomially large model, runtime, and sample complexity. Clearly, our framework is beyond the fixed feature methods.

**Parities on Uniform Inputs.** When \(r=1,p_{A}=0\), our problem setting will degenerate to the classic sparse parity function on a uniform input distribution. This has also been used for analyzing network learning . For this case, our framework can get a \(k2^{O(k)}(k)\) network width bound and a \(O(d^{k})\) sample complexity bound, matching those in . This then again confirms the advantage of network learning over kernel methods that requires \(d^{(k)}\) dimensions as shown in . See the full statement in Theorem E.31, details in Appendix E.5, and alternative analysis in Appendix E.6.

## 5 Further Implications and Conclusion

Our general framework sheds light on several interesting phenomena in NN learning observed in practice. Feature learning beyond the kernel regime has been discussed in Section 4.1.1 and Section 4.2.1. Here we discuss the LTH and defer more implications such as simplicity bias, learning over different data distributions, and new perspectives about roadmaps forward in Appendix C.

Lottery Ticket Hypothesis (LTH).Another interesting phenomenon is the LTH : randomly-initialized networks contain subnetworks that when trained in isolation reach test accuracy comparable to the original network in a similar number of iterations. Later studies (e.g., ) show that LTH is more stable when subnetworks are found in the network after a few gradient steps.

Our framework provides an explanation for two-layer networks: the lottery ticket subnetwork contains exactly those neurons whose gradient feature approximates the weights of the "ground-truth" network \(f^{*}\); they may not exist at initialization but can be found after the first gradient step. More precisely, Lemma 3.14 shows that after the first gradient step, there is a _sparse_ second-layer weight \(}\) with \(\|}\|_{0}=O(r)\), such that using this weight on the hidden neurons gives a network with a small loss. Let \(U\) be the support of \(}\). Equivalently, there is a small-loss subnetwork \(f_{}^{U}\) with only neurons in \(U\) and with second-layer weight \(}_{U}\) on these neurons. Following the same proof of Theorem 3.12:

**Proposition 5.1**.: _In the same setting of Theorem 3.12 but only considering the subnetwork supported on \(U\) after the first gradient step, with the same requirements on \(m\) and \(T\), with proper hyper-parameter values, we have the same guarantee: with probability \( 1-\), there is \(t[T]\) with \([(f_{^{(t)}}^{U})() y]_{d,r,B_{F},S_{p,,n_{G}}}+rB_{a1}B_{x1} n}}{B_{G}})}+\)._

This essentially formally proves LTH for two-layer networks, showing (a) the existence of the winning lottery subnetwork and (b) that gradient descent on the subnetwork can learn to similar loss in similar runtime as on the whole network. In particular, (b) is novel and not analyzed in existing work.

We provide our work's broader impacts and limitations (e.g., statement of recovering existing results and some failure cases beyond our framework) in Appendix A and Appendix B respectively.

Conclusion.We propose a general framework for analyzing two-layer neural network learning by gradient descent and show that it can lead to provable guarantees for several prototypical problem settings for analyzing network learning. In particular, our framework goes beyond fixed feature methods, e.g., NTK. It sheds light on several interesting phenomena in NN learning, e.g., the lottery ticket hypothesis and simplicity bias. Future directions include: (1) How to extend the framework to deeper networks? (2) While the current framework focuses on the gradient features in the early gradient steps, whether feature learning also happens in later steps and if so how to formalize that?