# Low-Rank Optimal Transport through Factor

Relaxation with Latent Coupling

Peter Halmos

Xinhao Liu

Julian Gold

Benjamin J. Raphael

Department of Computer Science, Princeton University

###### Abstract

Optimal transport (OT) is a general framework for finding a minimum-cost transport plan, or coupling, between probability distributions, and has many applications in machine learning. A key challenge in applying OT to massive datasets is the quadratic scaling of the coupling matrix with the size of the dataset. Forrow et al. (2019) introduced a factored coupling for the \(k\)-Wasserstein barycenter problem, which Setbon et al. (2021) adapted to solve the primal low-rank OT problem. We derive an alternative parameterization of the low-rank problem based on the _latent coupling_ (LC) factorization previously introduced by Lin et al. (2021) generalizing Forrow et al. (2019). The LC factorization has multiple advantages for low-rank OT including decoupling the problem into three OT problems and greater flexibility and interpretability. We leverage these advantages to derive a new algorithm _Factor Relaxation with Latent Coupling_ (FRLC), which uses _coordinate_ mirror descent to compute the LC factorization. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity. We provide theoretical results on FRLC, and demonstrate superior performance on diverse applications - including graph clustering and spatial transcriptomics - while demonstrating its interpretability.

## 1 Introduction

Optimal transport (OT) is a powerful geometric framework for comparing probability distributions. OT problems seek a transport plan \(\) efficiently transforming one distribution (\(\)) into another (\(\)), subject to a ground cost \(\). The minimum cost yields a distance between \(\) and \(\), while the optimal transport plan reveals key structural similarities between the distributions. Owing to its versatility - different ground costs result in different ways to compare data - OT has found many applications in machine learning and beyond: from self-attention Tay et al. (2020); Sander et al. (2022); Geshkovski et al. (2023) and domain adaptation Courty et al. (2014); Solomon et al. (2015) to computational biology Schiebinger et al. (2019); Yang et al. (2020); Bunne et al. (2023); Liu et al. (2023).

This versatility is compounded by several variants using different forms of the objective function and/or constraints on the transport plan \(\). Wasserstein (W) OT Kantorovich (1942) compares distributions over the same space through the expected work of \(\), while Gromov-Wasserstein (GW) OT Memoli (2011) compares distributions supported on distinct geometries through the expected metric distortion of \(\). Fused Gromov-Wasserstein (FGW) Vayer et al. (2020) OT is suited to structured data, taking a convex combination of the former two objectives. Independently, one can relax constraints on the _marginals_ of \(\): in computational applications, \(\) is a matrix whose row-sum \(_{m}\) and column-sum \(^{}_{n}\), are called its left and right marginals. _Balanced_ OT requires \(_{m}=\) and \(^{}_{n}=\). _Unbalanced_ OT Frogner et al. (2015) replaces these constraints with penalties in thetransport cost, and is more robust to outliers. _Semi-relaxed_ OT can be used to understand how one dataset embeds into another by imposing one hard constraint on either the left or right marginal, used for feature transfer Dong et al. (2023), and alignment of spatiotemporal data Halmos et al. (2024).

An important consideration in applying OT is the quadratic space of the transport plan. To address both the quadratic complexity and to provide robustness under sampling noise, Forrow et al. (2019) introduced another variant of OT, optimizing a \(k\)-Wasserstein Barycenter proxy for the rank-constrained Wasserstein objective. Their approach factors the transport plan through a small set of anchor points called hubs. Generalizing this approach, Scetbon et al. (2021) introduce the factorization \(=(1/)^{}\) comprised of _sub-coupling_ matrices \(\) and \(\) sharing an _inner marginal_\(\), meaning \(^{}_{n}=^{}_{m}=\). Building on this, Scetbon et al. (2021, 2022, 2023) derived algorithms to compute low-rank optimal transport plans for the primal OT problem with general costs, extending low-rank OT to GW and unbalanced problems using factored couplings.

Interestingly, a different factorization of \(\) was proposed by Lin et al. (2021) in the context of \(k\)-Wasserstein barycenters. We call their factorization a _latent coupling_ (LC) factorization, given by \(=(1/_{Q})(1/_{R})^{}\), with _two_ inner marginals \(_{Q}=^{}_{n}\) and \(_{R}=^{}_{n}\) and a general coupling \(\). Lin et al. (2021) constrain the transport between \(\) and \(\) through two sets of learned anchor points, where the factorization is defined by three transport plans computed from three cost matrices between the points and their anchors. This objective differs from that of Forrow et al. (2019); Scetbon et al. (2021), who seek a minimal rank coupling with respect to a single, fixed cost \(\). We observe that factored couplings of Forrow et al. (2019) correspond to LC factorizations with diagonal \(\), suggesting the LC factorization of Lin et al. (2021) may provide an alternative parameterization of transport plans for the low-rank OT problem considered in Forrow et al. (2019); Scetbon et al. (2021). To our knowledge, this idea has not yet been explored.

Contributions.We present a new algorithm, Factor Relaxation with Latent Coupling (FRLC, with the informal mnemonic "frolic"), to compute a minimum cost low-rank transport plan using the LC factorization. Parameterizing low-rank transport plans with the LC factorization has a number of advantages. First, optimization of the low-rank OT objective decouples into three OT sub-problems on the LC factors \(,,\), leading to a simpler optimization algorithm. Second, this decoupling provides straightforward extensions of FRLC to low-rank unbalanced and semi-relaxed OT; similar extensions for factored couplings required additional work Scetbon et al. (2023) beyond the balanced case. Third, the latent coupling \(\) in the LC factorization provides additional flexibility to model transport between datasets with different numbers of clusters, and to model mass-splitting between these clusters, providing a high-level and interpretable description of \(\) that differs from the factored couplings of Forrow et al. (2019). FRLC computes the LC factorization using a novel _coordinate_ mirror descent scheme, alternating descent steps on variables \((,)\) and \(\), inspired by the mirror descent approach of Scetbon et al. (2021). We call the descent step on \((,)\)_factor relaxation_, as the factors \(\) and \(\) have relaxed inner marginals, allowing FRLC to be solved by OT sub-problems. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed). We show FRLC performs better than existing state-of-the-art low-rank methods on a range of synthetic and real datasets, retaining the interpretability of Lin et al. (2021), and inheriting the broad applicability of Scetbon et al. (2021); Scetbon and Cuturi (2022); Scetbon et al. (2022, 2023).

## 2 Background

Wasserstein OT.Let \(\{x_{1},,x_{n}\}\) and \(\{y_{1}, y_{m}\}\) be datasets in a metric space \(\), and let \(_{d}\) be the probability simplex of size \(d\). Through probability vectors \(_{n}\) and \(_{m}\), each dataset is encoded as a probability measure: \(=_{i=1}^{n}_{i}_{x_{i}}\) and \(=_{j=1}^{m}_{j}_{y_{j}}\). Let

\[_{,}:=\{_{+}^{n m}:_{m}= {a}\},\ \,._{,}:=\{_{+}^{n m}:^{}_{n}=\},\ _{,}:=_{,}\,._{,}.\]

Thus, \(_{,}\) is the set of transport plans (probabilistic coupling matrices) with marginals \(\) and \(\). Given a cost function \(c:_{+}\), define the cost matrix \(_{+}^{n m}\) via \(_{ij}=c(x_{i},y_{j})\). The Kantorovich formulation Kantorovich (1942) of discrete OT, also called the Wasserstein problem, seeks a transport plan \(\) of minimal cost :

\[(,):=_{_{,}}, _{F}.\] (1)Gromov-Wasserstein Ot.In many applications, one wishes to compare datasets \(\{x_{1},,x_{n}\}\) and \(\{y_{1},,y_{m}\}\) across distinct metric spaces \(\) and \(\). The Gromov-Wasserstein (GW) objective Memoli (2007, 2011) addresses the absence of a common metric or coordinate system through intra-domain cost functions \(c_{1}:_{+}\) and \(c_{2}:_{+}\), leading to intra-domain cost matrices \(_{ik}=c_{1}(_{i},x_{k})\) and \(_{jl}=c_{2}(y_{j},y_{l})\). The GW objective function \(_{,}():=_{i,j,k,l}(_{ik}-_{jl})^{ 2}_{ij}_{kl}\) quantifies the expected metric distortion under \(\), leading to the optimization problem:

\[(,):=_{_{,}}_{, }().\] (2)

The Fused Gromov-Wasserstein (FGW) objective function Vayer et al. (2020) is a convex combination of the W and GW objectives, given as \(,_{F}+(1-)_{,}( )\), for hyperparameter \((0,1)\).

Relaxed marginal constraints._Balanced_ OT (1) constrains \(\) to lie in \(_{,}\). _Unbalanced_ OT relaxes constraints \(_{m}=\) and \(^{}_{n}=\), replacing them with penalties in the form of KL divergences (or other divergences, see Chizat et al. (2018)):

\[(,):=_{_{+}^{n m}}, _{F}+_{L}(_{m}\|)+_{R} (^{}_{n}\|),\] (3)

where \(_{L},_{R}>0\) control the strength of each penalty. _Semi-relaxed_ optimal transport relaxes exactly one of the hard constraints \(_{m}=\) and \(^{}_{n}=\) in the same manner. The semi-relaxed version of (1) obtained by relaxing only the "right" marginal constraint on \(\) is:

\[^{}(,):=_{_{_{}}} ,+(^{}_{n}\| ),\] (4)

while it's "left" marginal counterpart \(^{}(,)\) is defined analogously over \(_{,}\), using penalty \((_{m}\|)\). Likewise, one can form semi-relaxed or unbalanced GW and FGW problems.

Entropy regularization.The seminal work Cuturi (2013) introduced the Sinkhorn algorithm to solve an entropy regularized version of (1), \(_{}(,):=_{_{_{},}} ,_{F}- H()\), massively improving the \(O(n^{3} n)\) time complexity of classical techniques Orlin (1997); Tarjan (1997). Above, \(H\) is the entropy, \(H()=-_{ij}_{ij}(_{ij}-1)\), and \(>0\) is the regularization strength.

Low-rank regularization.The nonnegative rank \(_{+}()\) of matrix \(\) is the least number of nonnegative rank-one matrices summing to \(\). For \(r 1\), define

\[_{_{}}(r)=\{_{_{}}:_{+}( ) r\},_{,}(r)=\{_{,}: _{+}() r\},\] (5)

and let \(_{,}(r)=_{_{}}(r)_{,}(r)\). To estimate Wasserstein distances with greater stability and accuracy under sampling noise, Forrow et al. (2019) proposed a low-rank regularization on the coupling matrix, factoring the transport through a small set of anchor points. More explicitly, Svetbon et al. (2021) parameterized the set as \(_{,}(r)\) through the set \(_{,}(r)\) of _factored couplings_,

\[_{,}(r):=\{(,,)_{+}^{n  r}_{+}^{m r}(_{+}^{*})^{r}:_{,},_{,}\}.\]

The set \(_{,}(r)\) parameterizes \(_{,}(r)\) through \((,,)(1/)^{}\), as shown by Cohen & Rothblum (1993).

Scetbon et al. (2021) apply this factorization to solve the Wasserstein problem subject to \(_{,}(r)\) for general cost matrices:

\[_{r}(,):=_{_{,}(r)}, _{F}\] (6)

GW, unbalanced and semi-relaxed low-rank OT problems are defined as in (2), (3) and (4), replacing \(_{+}^{n m}\), \(_{_{},}\), or \(_{,}\) with rank-constrained counterparts (5). Svetbon & Cuturi (2022); Svetbon et al. (2022, 2023) developed a robust framework for solving all of these problems.

## 3 Factor Relaxation with Latent Coupling (FRLC) algorithm

### Latent Coupling Factorization

We parameterize low-rank coupling matrices \(_{,}(r)\) using a factorization introduced in Lin et al. (2021), which we call the _latent coupling (LC) factorization_ (Fig. 1). The key property of this factorization is the presence of a coupling matrix \(\) linking two distinct inner marginals. For simplicity we describe this factorization using an \(r\)-dimensional latent space, but we also extend to non-square matrices linking two latent spaces of different dimensions, as demonstrated in the results.

**Definition 3.1** (Inner marginals).: Given a factorization \(=^{}\) of a coupling matrix \(_{,}(r)\), the _inner marginals_ of \(\) and \(\) are \(_{Q}:=^{}_{n}\) and \(_{R}:=^{}_{m}\), respectively, where \(_{Q},_{R}_{r}\).

To distinguish the different marginals, we refer to \(\) and \(\) as _outer marginals_.

**Definition 3.2** (LC factorization).: Given a coupling matrix \(_{,}(r)\), a _latent coupling (LC) factorization of \(\)_ is \(=(1/_{Q})(1/_{R})^{}\), where \(_{Q}\) and \(_{R}\) are the inner marginals of \(\) and \(\), \(_{,}\), \(_{,}\), and \(_{_{Q},_{R}}\).

We call the factors \(,,\) in an LC factorization _sub-couplings_. Let \(_{+}:=_{+}^{n r}_{+}^{m r} _{+}^{r r}\). Given probability vectors \(_{n}\), \(_{m}\) and a positive integer rank \(r\), let

\[_{,}(r):=\{(,,)_{+}: {Q}_{,},_{,},_{_{Q },_{R}}\},\]

be the set of admissible sub-couplings for the LC factorization. Definition 3.2 gives the following map from \(_{,}(r)\) to \(_{,}(r)\):

\[(,,)(1/_{Q})(1/_{R})^{}=:_{(,,)}.\] (7)

Since this map is surjective, the set \(_{,}(r)\) parameterizes \(_{,}(r)\). Surjectivity follows from the fact that \(_{,}(r)\) maps injectively into \(_{,}(r)\), through \((,,)(,,())\), and \(_{,}(r)\) maps surjectively onto \(_{,}(r)\) via \((,,)(1/)^{}\). Definition 3.1 and Definition 3.2 are readily extended in two directions: the case when the outer marginal constraints are relaxed such that \(_{+}^{n m}\) or \(_{+}^{n m}\), while maintaining the constraint that \(_{_{Q},_{R}}\); as well as the case of non-square \(\).

### The Balanced FRLC Algorithm

We introduce an algorithm Factor Relaxation with Latent Coupling (FRLC), to compute a LC factorization of minimum cost. We first describe the FRLC algorithm for the balanced Wasserstein problem. Extensions to other and marginal constraints are discussed later. The FRLC objective function, for low-rank, balanced Wasserstein OT, is

\[_{}(,,):=,_{(,,)}_{F},\] (8)

where \(P_{(,,)}\) is defined by (7). Since \(_{,}(r)\) parameterizes \(_{,}(r)\), problem (8) is equivalent to low rank problem (6). The FRLC algorithm is built from projections onto convex sets, described by constraints on the outer marginals alone for \((,)\) and by the inner marginals alone for \(\). Given \((,,)_{,}(r)\), sub-couplings \(\) and \(}\) are constrained by:

\[_{1}():=\{(,,)_{+}: _{r}=\},_{1}():=\{(,,) _{+}:_{r}=\}.\]The convex sets constraining the latent coupling matrix \(\) are

\[_{2}(_{Q}):=\{(,,)_{+}: _{r}=_{Q}\},_{2}(_{R}):=\{(,, )_{+}:^{}_{r}=_{R}\},\]

where \(_{Q}=^{}_{n}\) and \(_{R}=^{}_{m}\) as per Definition 3.1. Writing \(_{1}=_{1}()_{1}()\) and \(_{2}=_{2}(_{Q})_{2}(_{R})\), one has \(_{,}(r)=_{1}_{2}\).

We use _coordinate_ mirror descent to optimize (8), building on the mirror descent (MD) approach of Svetbon et al. (2021); Svetbon and Cuturi (2022); Svetbon et al. (2022, 2023) for the low-rank problem. First we take a descent step in the variables \((,)\) for a fixed \(\), using KL penalties on their inner marginals. These "soft" constraints allow the joint optimization in \((,)\) to decouple into two semi-relaxed OT problems, one for each variable. We call this step _factor relaxation_ as this allows \((,)\) to have relaxed inner marginals \(_{Q}\) and \(_{R}\). Next we take a descent step in the latent coupling variable \(\), fixing the \(\) and \(\), equivalent to solving a balanced OT problem. Thus, solving both coordinate descent steps corresponds to solving three OT problems.

We now provide further details on these coordinate descent steps, with the full algorithm given in Algorithm 1. Let \((_{k})_{k=1}^{N}\) be a sequence of step sizes. As in Svetbon and Cuturi (2022), we choose \(^{}\)-normalization for the step-sizes. Our coordinate mirror descent in the factor relaxation step is:

\[(_{k+1},_{k+1})*{arg\,min}_{( ,)\,:\,(,,_{k})_{1}} (,),_{(,)}_{}+}((,)\|(_{k},_{k}))\] \[+((^{}_{n},^{ }_{m})\|(_{k}^{}_{n},_{k}^ {}_{m}))\]

The Sinkhorn kernels for the semi-relaxed OT problems arising from the factor relaxation step are:

\[_{}^{(k)} :=_{k}(-_{k}(_{k}_{k}^{ }-_{n}^{-1}((_{k}_{k}^{ })^{}_{k}(1/_{Q_{k}}))^{})\,)\] \[_{}^{(k)} :=_{k}(-_{k}(^{}_{k} _{k}-_{m}^{-1}((1/_{R_{k}}) _{k}^{}^{}_{k}_{k})^{}) \,),\]

introducing the shorthand \(=(1/_{Q})\,(1/ {g}_{R})\) and where \(^{-1}():^{r r}^{r}\) denotes the matrix-to-vector extraction of the diagonal. This \(\)-dependent regularization also allows us to show smoothness of the objective in Proposition E.5, from which the convergence guarantee Proposition 3.3 follows. We derive the semi-relaxed projection Algorithm 2 of the sub-couplings \(\) and \(\) in Appendix G for completeness. We also show in Lemma A.1 that \(_{Q}\) and \(_{R}\) induced by the semi-relaxed projection are both feasible and locally optimal, not requiring separate optimization.

As \((_{k+1},_{k+1},)_{2}\) if and only if \(_{_{Q_{k+1}},_{R_{k+1}}}\), after the factor relaxation step, we next take a coordinate MD step on the latent coupling \(\):

\[_{k+1}*{arg\,min}_{\,:\,(_{k+1},_{k+1},)_{2}},_{}_{ }+}(\|_{k}).\] (9)

This is equivalent to applying Sinkhorn (Algorithm 5) to \(\) given \(_{Q}\) and \(_{R}\) with the kernel:

\[_{}^{(k)}:=_{k}(-_{k}(1/_{Q_{k+1}})_{k+1}^{}_{k+1}(1/_{R_{k+1}})\,).\]

After the final iteration of the coordinate-MD scheme, \(=(1/_{Q})\,(1/_{R})\) satisfies \(_{R}=_{r}\) and \(^{}_{Q}=_{r}\) as \(\) is a coupling between \(_{Q}\) and \(_{R}\). Thus \(_{r}=^{}_{,}\) and the iterates \((_{k},_{k},_{k})\) remain in the intersection of the constraint sets. Thus, in contrast to other approaches Svetbon et al. (2021); Lin et al. (2021); Forrow et al. (2019), we do not require Dykstra projections back into the intersection to maintain feasability. We note that our implementation of FRLC allows for a non-square latent coupling \(\), providing greater interpretability in problem-specific applications. Above, we presented FRLC in the simplest case that \(\) is square.

### Initialization, convergence, and FRLC extensions

Full-rank random initializations of the sub-coupling matrices.We propose a new initialization of the sub-couplings \((,,)\) for the LC-factorization in Algorithm 6.This generates a full-rank initialization (Proposition F.1) in the set of rank-\(r\) couplings \(_{,}(r)\) and is accomplished by applying Sinkhorn to random matrices. Our approach differs from Svetbon et al. (2021); Svetbon and Cuturi (2022) who use initializations for the diagonal factorization of Forrow et al. (2019), and are not applicable to a latent coupling that is non-diagonal, non-square, or with two distinct inner marginals.

Convergence analysis of FRLC.As objective (8) is non-convex, it is important to have convergence guarantees. Our convergence criterion \((,)\) is defined in (10). To prove convergence we require a lower bound on the entries of \(_{Q}\) and \(_{R}\). Previous works introduce a lower-bound vector \(\) enforced element-wise for stability and smoothness Section et al. (2021). In FRLC the use of semi-relaxed projections naturally enforces a lower-bound. In Appendix E.5, we show that for any \((0,)\), the FRLC algorithm's \(\)-weighted regularization on the inner marginals can guarantee a uniform lower-bound of \(\) on the entries: for sufficiently large \(\) and \((m^{2}/)\) iterations for the sub-coupling Pham et al. (2020), one guarantees a lower bound of \(\) on \(_{R}\) and \(_{Q}\). This allows us to show objective smoothness in Proposition E.5. Previous work on low-rank optimal transport Section et al. (2021) use the non-asymptotic convergence criterion of Ghadimi et al. (2014). Following existing works Dang and Lan (2015) establishing convergence rates of coordinate mirror-descent for smooth objectives, we show in Proposition 3.3 this criterion may be extended to coordinate-MD by adapting the block-descent lemma of Beck and Tetruashvili (2013).

**Proposition 3.3**.: _Suppose one has \(f C^{1}(,)\) with block-coordinate Lipschitz gradient and block smoothness constants \((L_{i})_{i=1}^{p}\), and a function \(h C(,)\) which is \(\)-strongly convex. For \(=f+h\), suppose one performs coordinate mirror descent on \(\) minimized over a product of closed convex sets \(=_{i=1}^{p}_{i}\). Let the sub-iterates with respect to the \(i\)-th block update be \(\{_{k}^{i}\}_{i=0}^{p}\) where \(_{k}:=_{k}^{0}\) for \(k[N]\) outer iterations. Then one has:_

\[_{k}(_{k},_{k-1})L}{N(^{2 }/2L)}=L^{2}}{N^{2}},\]

_where \(D\) is (36), \(L\) is the global smoothness constant, stepsizes \(_{k,i}:=/L\), and convergence criterion \((_{k},_{k-1})\) is given in (35)._

Specialized to the LC-parametrization, the criterion \(_{k}(_{k},_{k+1})\) is:

\[_{k}(_{k},_{k+1}):=^{2}}[\|_{ k+1}-_{k}\|_{F}^{2}+\|_{k+1}-_{k}\|_{F}^{2}+\|_{k+1}- _{k}\|_{F}^{2}]\] (10)

for \(_{k}=(_{k},_{k},_{k})\). We show through Propositions 3.3, E.5 the following result:

**Proposition 3.4**.: _The FRLC algorithm with step-sizes \(_{k}=/L\) and iterates \(_{k}=(_{k},_{k},_{k})\) has non-asymptotic stationary convergence in the criterion \((,)\) with:_

\[_{k 1,..,N-1}_{k}(_{k},_{k+1}) 2D^{2}L^{2}/N ^{2}\]_Where \(N\) is the number of iterations, \(D\) the optimality-gap as in (36), and \(L=_{i\{1,..,3\}}(L_{i})\) the global smoothness for \(L_{i}=(\|\|_{F},n,m,r,)\) the block-wise smoothness constants._

The proof of Proposition 3.4 follows directly from our extension of the non-asymptotic criterion with the block-descent result Proposition 3.3 and the proof that this lemma holds in FRLC Proposition E.5. We also mention two improvements to other low rank approximation results in literature. In Proposition F.2 we show that one can _analytically_ solve for the block-optimal \(\) for the factorization of Svetbon et al. (2021), and we improve the bound on the low-rank approximation error in Proposition E.7.

FRLC for other marginal constraints and objectives.The balanced FRLC algorithm can be extended simply to other marginal constraints owing to the decoupling of the coordinate MD scheme. In particular, by using either the semi-relaxed projections (Algorithm 2) or fully-relaxed (unbalanced) projections (Algorithm 3) on sub-couplings \(\) and \(\), one can solve the balanced problem, the problem with the left or right marginal relaxed, or the unbalanced problem. As such, _all_ variants of marginal constraints can be handled by a single algorithm, given in Algorithm 4.

We also extend the FRLC algorithm to the Gromov-Wasserstein problem. This consists of computing a GW-specific gradient with the appropriate marginal constraints applied to simplify their form, and re-computing Sinkhorn kernels as exponentiations of these gradients. The matrix form of the quadratic GW objective is \(_{m}^{}^{}^{ 2} _{m}+_{n}^{}^{ 2} _{n}-2,\), where \(\) denotes the Hadamard (entrywise) product. Then the GW-specific Sinkhorn kernels are

\[_{}^{(k)} (2_{k}(2^{}^{ }-^{ 2}_{r}_{r}^{}) ),\] \[_{}^{(k)} (2_{k}(2^{}^{ }-^{ 2}_{r}_{r}^{}) ),\] \[_{}^{(k)} (4_{k}(_{Q}^{-1}) ^{}^{}(_{ R}^{-1})).\]

In Algorithm 4, one can solve the GW-problem by using the kernels above. Here, we present the kernels omitting a rank-1 perturbation, which is given in Appendix D. From the Wasserstein and GW gradients, the FGW gradient is easily taken as a convex combination of the two. In this work, we primarily focus on the LC-factorization for the rank \(r\) Wasserstein problem (6).

## 4 Experimental Results

We compare FRLC to existing low-rank and full-rank optimal transport algorithms on several datasets: simulated datasets previously used in Tong et al. (2023) and Svetbon et al. (2021); a massive spatial-transcriptomics dataset Chen et al. (2022); and a graph partitioning task Chowdhury and Needham (2021). Further details of each experiment (e.g. pre-processing, validation) are in Appendices K, L, and M. In the section below, LOT refers to the works of Svetbon et al. (2021, 2023, 2022) and Latent OT refers to Lin et al. (2021).

Figure 2: (a) Simulated dataset containing points from two moons (orange) and eight Gaussians (blue). (b) Transport cost \(,_{F}\) achieved by FRLC and LOT Svetbon et al. (2021) for the balanced Wasserstein problem on the dataset in (a) for different ranks and initializations. FLRC full rank (blue curve) is average over 10 random initializations. (c) Results on the 10D mixture of Gaussians dataset.

### Evaluation of Low-rank Approximations for Balanced OT on Synthetic Data

We first compare the balanced OT version of FRLC with the the low-rank balanced OT algorithm LOT of Scethon et al. (2021) on a synthetic dataset following Tong et al. (2023). The dataset consists of \(m=1000\) points from two mooms and \(n=1000\) points sampled from eight 2D Gaussian densities (Fig. 2a). We solve the Wasserstein problem (\(1\)) with cost matrix \(\) computed using the Euclidean distance. The full-rank coupling matrix \(\) has rank 1000, and we compute both FRLC and LOT solutions with rank between 20 and 200. For each rank, we initialize FRLC adapting the deterministic rank-2 initialization proposed in Scethon et al. (2021) and the random initialization of Alg. 6. We initialize LOT using the rank-2 initialization and two other options in \(\)-\(\)jax Cuturi et al. (2022).

We find that FRLC obtains lower transport cost \(,_{F}\) with increasing rank (Fig. 2b) and consistently achieves lower transport cost than LOT across all ranks and all initializations. Specifically, starting both methods at the same rank-2 initialization, FRLC consistently achieves a lower cost than LOT for all ranks. Additionally, we observe smooth convergence of FRLC for both rank-2 initialization and the full-rank random initialization of Alg. 6 (Fig. 5).

We also evaluate FRLC and LOT on two datasets of Gaussian mixtures, one in 2-dimensions and one in 10-dimensions, each with \(n=m=5,000\) points from two mixtures of Gaussians, following Scethon et al. (2021), with further details in Appendix K. We observe the same trend as the previous simulation for both datasets (Fig. 2c, Fig. 7), with FRLC achieving lower transport costs than LOT across all ranks and all initializations. In addition FRLC has half the runtime of LOT (CPU) - including the setup time of FRLC but excluding the setup time of LOT in \(\)-\(\)jax - on datasets of \(n=m=1000\) points from all three datasets with rank \(r=100\) (Table 2). At the same time FRLC achieves lower primal cost \(,_{F}\) with tighter marginals \(\|_{n}-\|_{2}\) and \(\|^{T}_{m}-\|_{2}\). Lin et al. (2021) only solves a proxy for the rank-constrained Wasserstein problem, and thus is not the focus of our comparisons. Nevertheless, we verify that on all synthetic experiments that FRLC achieves significantly lower primal OT cost than Latent OT (Table 5).

### Interpretation of the Latent Coupling and LC-Projection

We demonstrate the intepretability of the latent coupling \(\) in the LC factorization. In both the LC factorization and factored couplings, the sub-couplings \(\) and \(\) each have associated barycentric projection operators which coarse-grain input datasets \(^{(1)},^{(2)}\). In particular, the LC projection is defined from the LC factorization as follows.

**Definition 4.1** (LC-Projection).: Let \((1/_{Q})(1/_{ R})^{}\) be an LC factorization of of a coupling matrix \(_{,}(r)\) computed from datasets \(^{(1)}^{n d},^{(2)}^{m  d}\), with \(^{+ r_{2}}_{t}\). The _LC-projections_\(^{(1)}\) and \(^{(2)}\) of \(^{(1)}\) and \(^{(2)}\) are \(^{(1)}:=(1/_{Q})^{}^{(1)}\), and \(^{(2)}:=(1/_{R})^{}^{(2)}\).

By interpreting any factored coupling \((,,)\) as an LC factorization \((,,())\), Definition 4.1 describes the barycentric projections for both factorizations. We compare the projections of the coupling computed by FRLC to those of LOT Scethon et al. (2021) on a dataset containing 1000 samples from 2D-Gaussians centered at the 5th-roots of unity and 1000 samples from 2D Gaussians

Figure 3: LC-projections of couplings of Gaussians centered on the 5th-roots of unity (green) and 10th roots of unity (yellow). (a) Ground-truth full-rank coupling. (b) Non-square rank-5 latent-coupling of FRLC (c) LC-projection barycenters aligned with rank-5 diagonal coupling of LOT Scethon et al. (2021). (d) Square rank-10 latent coupling of FRLC. (e) Rank-10 diagonal coupling of LOT.

centered at the 10th-roots of unity (the latter scaled by a factor of two, Fig. 2(a)). In both cases, the latent coupling \(\) or \(()\) is visualized as a transport between barycenters. We run FRLC and LOT with ranks \(r=5\) and \(r=10\) to match the number of target and source clusters. In the rank-5 case, FRLC uses a _non-square_ latent coupling \(_{+}^{10 5}\) which correctly captures the coupling between clusters (Fig. 2(b)), while the LOT rank-5 projection computes barycenters that are outside of the clusters (Fig. 2(c)). A similar result is observed for square rank-10 latent couplings computed by FRLC (Fig. 2(d)) and LOT (Fig. 2(e)) demonstrating that the LOT barycenters in Fig. 2(b) are not an artifact of using the lowest rank. We observe similar results on other simulated datasets (Fig. 11).

### Evaluation on Spatial Transcriptomics Alignment

We compare FRLC and the algorithm (LOT-U) of Scetbon et al. (2023) (which solves unbalanced low-rank Wasserstein, GW, and FGW problems) on the task of computing an alignment between cells from different time points during mouse embryonic development. Specifically, we compute an alignment between a spatial transcriptomics (ST) dataset of an E11.5 stage mouse embryo and an E12.5 stage mouse embryo Chen et al. (2022). Optimal transport is a popular approach to align single-cell Schiebinger et al. (2019) and spatial transcriptomics datasets Zeira et al. (2022); Liu et al. (2023); Klein et al. (2023). In single-cell transcriptomics, one measures a gene expression vector for each cell, and in spatial transcriptomics one additionally measures the 2D location of each cell. The cost matrix \(\) describes the difference between gene expression vectors and intra-domain cost matrices \(\) and \(\) are derived from the 2D coordinates within each slice. Therefore, OT problems of W, GW, and FGW objectives can be solved and the coupling matrix represents the cell-cell alignment (Appendix M). However, computation of a full-rank OT solution is not feasible in our large-scale dataset: the E11.5 slice has about 30,000 cells while the E12.5 slice has about 50,000 cells.

We evaluate the alignments by assessing performance on two prediction tasks from Scetbon et al. (2023): (1) a _gene expression prediction_ task where we predict the expression of a gene in E12.5 from expression of the gene in E11.5 using the alignment; (2) a _cell type prediction task_ where we predict the cell types of E12.5 from the cell type clustering of E11.5 (Appendix M). We evaluate the accuracy of the gene expression prediction task through the Spearman correlation \(\) between the predicted expression and the ground truth expression of 10 test marker genes. We evaluate the accuracy of the cell type prediction task by computing the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) between the predicted cell types and the cell types derived in the original publication Chen et al. (2022). Being a comparison between different objectives, this relies on downstream metrics. For completeness, we validate the efficacy of FRLC on directly minimizing the balanced Wasserstein cost \(,_{F}\) against Scetbon et al. (2021) in Figure 8.

For a direct comparison, we use FRLC to solve the same unbalanced problems (denoted FRLC-U). We perform an extensive grid search (Appendix M.3) to pick the best hyperparameters (including \( 30,000\)) for all algorithms. Scetbon et al. (2023) previously showed that unbalanced FGW algorithm has the best performance on ST alignment. We find that unbalanced FRLC achieves comparable or better results than the previous state-of-the-art unbalanced low-rank method on all three objectives (Table 4). We also solve a semi-relaxed version of each problem motivated by the

Figure 4: (a) Brain marker gene _Tubb2b_ expression and FRLC prediction. (b) Comparison of the low-rank unbalanced (LOT-U) algorithm of Scetbon et al. (2023) and FRLC on aligning spatial transcriptomics data. Bold indicates top performing method for each metric on each objective.

observation that all cells from E12.5 have an ancestor, but not all cells from E11.5 have the same number of descendants due to cell growth and death. Thus the former marginal is tight, and the latter relaxed Halmos et al. (2024). We run both semi-relaxed FRLC (FRLC-SR) and a setting of LOT-U that recovers the semi-relaxed problem (LOT-SR). Semi-relaxed FRLC achieves the best results on all three metrics by a large margin (Table 4). As one example, the expression of _Tubb2b_, a mouse brain marker gene, agreeing with the expression predicted from the semi-relaxed alignment of FRLC (Fig. 4a).

### Additional Experiments

We evaluate FRLC on an unsupervised graph partitioning problem Chowdhury and Needham (2021) on four real-world graph datasets Yang and Leskovec (2012); Yin et al. (2017); Banerjee et al. (2013). We benchmark the performance of the semi-relaxed and GW settings of FRLC against (1) GWL Xu et al. (2019), solving a balanced GW problem; (2) SpecGWL Chowdhury and Needham (2021) using the heat kernel on the graph Laplacian as the cost matrix. We find FRLC achieves the better clustering performance than GWL and SpecGWL on 9/12 and 11/12 of the datasets (Table 3 and Appendix L).

## 5 Discussion

We provide comparison of existing low-rank solvers in Table 1. The FRLC algorithm has a number of advantages, including (1) coarsening a full-rank plan \(\) to non-diagonal latent coupling \(\); (2) minimizing the primal OT problem for general cost \(\) rather than a barycenteric problem; (3) optimizing only sub-couplings; and (4) using Sinkhorn alone as the sub-routine for low-rank OT. While we argue these are substantial advantages, FRLC has limitations which warrant follow-up work. In particular, three key limitations of our work, common to the existing low-rank OT algorithms, are: (1) selecting values of the latent coupling ranks; (2) strengthening the convergence criterion; (3) addressing sensitivity to the initialization from non-convexity of the objective. A limitation specific to our work is the selection of the \(\) hyperparameter controlling the smoothness of the trajectory. These and other limitations are discussed in Section N of the Appendix. Another direction for further investigation is to better understand what structure LC factorizations capture when the optimal plan is known to have full rank, e.g. when the Monge map exists, as has been explored by Liu et al. (2021).

## 6 Conclusion

We introduce FRLC, an algorithm to compute low-rank optimal transport plan from the latent coupling (LC) factorization. FRLC handles different OT objective costs and relaxations of the marginal constraints. Moreover, the LC factorization provides an interpretable coarse-graining of the full transport plan and its marginals through the mapping \((,,)(,_{Q},_{R})\). We demonstrate the superior performance of FRLC compared to state-of-the-art low-rank methods on real and synthetic datasets.

  
**Method** & **Factorization** & **Cost** & **Variables** & **Algorithm** &  **Sub-routine** \\ **for coupling** \\  \\    Factored Coupling \\ Forrow et al. (2019) \\  & Factored coupling &  Factored coupling \\ barycenter \\  &  Anchors \& \\ sub-couplings \\  & Lloyd-type & Dykstra’s \\   Latent OT \\ Lin et al. (2021) \\  & Latent coupling &  Extension of \\ \(k\)-Wasserstein \\ barycenter \\  &  Anchors \& \\ sub-couplings \\  & Lloyd-type & Dykstra’s \\   LOT \\ Scebon et al. (2021) \\  & Factored coupling & Primal OT cost &  Sub-couplings \& \\ inner marginal \\  & Mirror-descent & Dykstra’s \\   FRLC (this work) \\  & Latent coupling & Primal OT cost & Sub-couplings & 
 Coordinate \\ mirror-descent \\  & OT \\   

Table 1: Comparing aspects of low-rank OT methods. Factorization indicates the structure of the inner matrix.