# LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor Search

Elias Jaasaari \({}^{}\)  Ville Hyvonen \({}^{}\)  Teemu Roos \({}^{}\)

\({}^{}\)Department of Computer Science, University of Helsinki

\({}^{}\)Department of Computer Science, Aalto University

{elias.jaasaari,teemu.roos}@helsinki.fi; ville.2.hyvonen@aalto.fi

###### Abstract

Approximate nearest neighbor (ANN) search is a key component in many modern machine learning pipelines; recent use cases include retrieval-augmented generation (RAG) and vector databases. Clustering-based ANN algorithms, that use score computation methods based on product quantization (PQ), are often used in industrial-scale applications due to their scalability and suitability for distributed and disk-based implementations. However, they have slower query times than the leading graph-based ANN algorithms. In this work, we propose a new supervised score computation method based on the observation that inner product approximation is a multivariate (multi-output) regression problem that can be solved efficiently by reduced-rank regression. Our experiments show that on modern high-dimensional data sets, the proposed reduced-rank regression (RRR) method is superior to PQ in both query latency and memory usage. We also introduce LoRANN1, a clustering-based ANN library that leverages the proposed score computation method. LoRANN is competitive with the leading graph-based algorithms and outperforms the state-of-the-art GPU ANN methods on high-dimensional data sets.

## 1 Introduction

In modern machine learning applications, data is often stored as embeddings, i.e., as vectors in a high-dimensional vector space where representations of semantically similar items are close to each other. Consequently, similarity search in high-dimensional vector spaces is a key algorithmic primitive used in many pipelines, such as semantic search engines and recommendation systems. Since the data sets are usually both large and high-dimensional, _approximate_ nearest neighbor (ANN) search is deployed to speed up similarity search in many applications (Li et al., 2019).

Recent use cases of ANN search include retrieval-augmented generation (RAG) (Lewis et al., 2020; Guu et al., 2020; Borgeaud et al., 2022; Shi et al., 2024) and approximate attention computation in Transformer-based architectures (Kitaev et al., 2020; Vyas et al., 2020; Roy et al., 2021). ANN search is also a key operation in vector databases that are used to store embeddings for industrial-scale applications (see, e.g., Wang et al., 2021; Guo et al., 2022; Pan et al., 2024).

The state-of-the-art methods for ANN search can be classified into clustering-based and graph-based algorithms (for a recent survey, see Bruch, 2024). In the comprehensive ANN benchmark (Aumuller et al., 2020), the leading graph algorithms HNSW (Malkov and Yashunin, 2018) and NGT (Iwasaki and Miyazaki, 2018) have faster query times than clustering-based algorithms. However, clustering-based algorithms are often used in industrial-scale applications (see, e.g., Chen et al., 2021; Douze et al., 2024) due to their smaller memory footprints and faster index construction times. They arealso suitable for distributed implementations and hybrid solutions that use persistent storage such as SSDs or blob storage (Chen et al., 2021; Gottesburen et al., 2024).

The key components of clustering-based algorithms are _clustering_ and _score computation_. In the indexing phase, the corpus, i.e., the data set from which the nearest neighbors are searched, is partitioned via clustering. In the query phase, \(w\) clusters are selected and the corpus points that belong to the selected clusters are scored. The points with the highest scores are selected into a candidate set that can be further re-ranked. The state-of-the-art clustering-based algorithms (Jegou et al., 2011; Guo et al., 2020; Sun et al., 2023) use variants of _product quantization_ (PQ) (Jegou et al., 2011) with highly optimized implementations (see, e.g., Andre et al., 2015) for score computation.

In this article, we propose a supervised score computation method that improves the query latency of clustering-based ANN methods, making them competitive with the leading graph algorithms. Our key observation is that estimating the dissimilarities between the query point and the cluster points is a multivariate (multi-output) regression problem. For the most common dissimilarity measures, it is sufficient to estimate the inner products between the query point and the cluster points, and the ordinary least squares (OLS) estimate is the exact solution for this regression problem. The proposed method approximates the OLS solution by reduced-rank regression (Izenman, 1975). We further approximate the reduced-rank regression estimates using 8-bit integer computations, improving query latency and memory consumption. Reduced-rank regression (RRR) is a simpler method than PQ, and our experimental results show that it is faster at any given recall level and memory usage.

To make our work available for practical applications of ANN search, we introduce LoRANN, a clustering-based ANN library that leverages the proposed score computation method. Since the low memory usage and the simple computational structure of reduced-rank regression make it well-suited for GPUs, LoRANN also includes a GPU implementation of the proposed method.

In summary, our contributions are:

* We propose reduced-rank regression (RRR) as a new supervised score computation method for clustering-based ANN search (see Section 3).
* We verify experimentally that RRR outperforms PQ both at the optimal hyperparameters (Section 7.1.1) and at fixed memory consumption (Section 7.1.2), and that it naturally adapts to the query distribution in the out-of-distribution (OOD) setting (Section 5).
* We introduce LoRANN, a clustering-based ANN library that contains efficient CPU and GPU implementations of the proposed score computation method (Section 4).
* We show that LoRANN outperforms the leading clustering-based libraries Faiss (Douze et al., 2024) and ScaNN (Guo et al., 2020), and has faster query times than the leading graph-based library GLASS at recall levels under 90% on most data sets (Section 7.2.2). LoRANN outperforms the SOTA GPU methods on high-dimensional data (Section 7.2.3).

## 2 Background

In this section, we first review the notation of approximate nearest neighbor (ANN) search and then describe the standard structure of clustering-based ANN algorithms.

### ANN search

Let \(^{d}\) be a query point, and let \(\{_{j}\}_{j=1}^{m}^{d}\) be the _corpus_, i.e., the set of points from which the nearest neighbors are retrieved. The \(k\) nearest neighbors of \(\) are defined as

\[_{k}(;\{_{j}\}_{j=1}^{m},d):=\{j[m]\,:\,d( ,_{j}) d(,_{(k)})\},\] (1)

where \(d:^{d}\) is a dissimilarity measure, \(_{(1)},,_{(m)}\) denote the corpus points that are ordered in ascending order w.r.t. their dissimilarity to the query point \(\), and \([m]:=\{1,,m\}\).

Commonly used dissimilarity measures are the Euclidean distance \(d(,)=\|-\|_{2}\), the (negative) inner product \(d(,)=-,\), and the cosine (angular) distance \(d(,)=1-/\|\|_{2},/ \|\|_{2}\). The special case where the dissimilarity measure is the negative inner product is often called maximum inner product search (MIPS) (see, e.g., Guo et al., 2020; Lu et al., 2023; Zhao et al., 2023). In ANN search, the exact solution \(_{k}(;\{_{j}\}_{j=1}^{m},d)\) is approximated.

The effectiveness of ANN algorithms is typically measured by _recall_, i.e., the fraction of the true \(k\) nearest neighbors returned by the algorithm. The efficiency is measured by query latency or, equivalently, by queries per second (QPS) (see, e.g., Li et al., 2019; Aumuller et al., 2020).

### Clustering-based ANN search

Partitioning-based ANN algorithms, such as tree-based (e.g., Muja and Lowe, 2014; Dasgupta and Sinha, 2015; Jaasaari et al., 2019) and hashing-based algorithms (e.g., Charikar, 2002; Weiss et al., 2008; Liu et al., 2011), build an index by partitioning the corpus into \(L\) elements. In the query phase, they use a routing function \(:^{d}[L]^{w}\) to assign the query point into \(w\) partition elements.

Clustering-based ANN algorithms (see, e.g., Bruch, 2024, Chapter 7) are partitioning-based ANN algorithms that partition the corpus via clustering. ANN indexes based on clustering are also often called _inverted file_ (IVF) indexes (Jegou et al., 2011). The most commonly used clustering method is \(k\)-means clustering, specifically standard \(k\)-means when the dissimilarity measure \(d\) is the Euclidean distance, and spherical \(k\)-means (Dhillon and Modha, 2001) when \(d\) is the inner product or the cosine distance. While there exists recent exploratory work on alternative query routing methods (Gottesburen et al., 2024; Vecchiato et al., 2024; Bruch et al., 2024), the most common method is centroid-based routing where, for the set \(\{_{l}\}_{l=1}^{L}\) of cluster centroids,

\[()=_{w}(;\{_{l}\}_{l=1}^{L},d),\]

i.e., the \(w\) clusters whose centroids are closest to the query point are selected. We follow this standard practice by using \(k\)-means clustering with centroid-based routing. However, the proposed score computation method can be combined with any partitioning and routing method.

After routing, the corpus points that belong to the selected \(w\) clusters are scored (see Section 6 for a discussion of score computation methods), and the \(t\) highest scoring points are selected into the candidate set. This candidate set can be re-ranked by evaluating the true dissimilarities between \(\) and the candidate set points. Finally, the \(k\) most similar points are returned as the approximate \(k\)-nn.

## 3 Reduced-rank regression

In this section, we derive the proposed supervised score computation method. First, we formulate dissimilarity approximation as a multivariate regression problem. We then show how the exact OLS solution to this problem can be approximated by reduced-rank regression (RRR). Finally, we show how RRR can be implemented efficiently using 8-bit integer vector-matrix multiplications.

### Dissimilarity approximation as a multivariate regression problem

We consider the task of approximating the dissimilarities \(d(,_{j})\) between the query point \(\) and the corpus points \(_{j}\) that belong to the \(l\)th cluster. Denote the set of the indices of these corpus points by \(I_{l}\), their number by \(m_{l}:=|I_{l}|\), and the matrix containing them as rows by \(_{l}^{m_{l} d}\). In what follows, to avoid cluttering the notation, we drop the subscript \(l\) denoting the cluster from matrices, e.g., we denote \(_{l}\) by \(\). We also assume, w.l.o.g., that the corpus is indexed so that \(I_{l}=\{1,,m_{l}\}\). This task can now be formulated as a multivariate regression problem where the output is defined as a \(1 m_{l}\) matrix \(=[y_{1} y_{m_{l}}]\), where \(y_{j}=d(,_{j})\) for each \(j=1,,m_{l}\).

We consider the cases where \(d\) is the (negative) inner product, the Euclidean distance, or the cosine (angular) distance. In all three cases, it is sufficient to estimate the inner products. For Euclidean distance, \(*{arg\,min}_{j I_{l}}\|-_{j}\|_{2}= *{arg\,min}_{j I_{l}}(-2^{T}_{j}+\|_{j}\|_{2}^{2})\), where the norms \(\|_{j}\|_{2}\) can be precomputed. For cosine distance, \(*{arg\,min}_{j I_{l}}(1-(,_{j}))= *{arg\,min}_{j I_{l}}(-^{T}_{j})\) if the corpus points are normalized to have unit norm.

### Reduced-rank regression solution

We approximate the exact solution \(=^{T}^{T}\) of the regression problem defined in the previous section by a low-rank approximation. We assume the standard supervised learning setting, i.e., that we have a sample \(\{_{i}\}_{i=1}^{n}\) from the query distribution \(\) (the corpus can also be used as the training set if no separate training set is available). To train the \(l\)th model, we use all the training set points that are routed into the \(l\)th cluster. When the standard centroid-based routing is used, these are the training set points that have \(_{l}\), i.e., the centroid of the \(l\)th cluster, among their \(w\) closest centroids. Denote the set of indices of these training set points by \(J_{l}:=\{i[n]\,:\,l(_{i})\}\), and their number by \(n_{l}:=|J_{l}|\). The output values of the training set of the \(l\)th model are given by \(:=^{T}^{n_{l} m_{l}}\), where we denote by \(^{n_{l} d}\) the matrix containing the training set points \(\{_{i}\}_{i J_{l}}\) as rows.

To approximate the dissimilarities between the query point \(\) and the cluster points \(\{_{j}\}_{j I_{l}}\), we consider the linear model \(^{T}\), where \(^{d m_{l}}\) is a matrix containing the parameters of the model, and minimize the mean squared error \(_{}[\|-^{T} \|_{2}^{2}\,\{l()\}]\) (the indicator function selects the query points that routed into the \(l\)th cluster). The unconstrained least squares solution \(}_{}=^{T}\) reproduces the exact inner products \(=^{T}^{T}\). In order to reduce the computational complexity of evaluating the model predictions, we constrain the rank of the parameter matrix: \(() r<(d,m_{l})\). Under this constraint, the parameter matrix can be written using a low-rank matrix factorization \(=\), where \(^{d r}\) and \(^{r m_{l}}\), and, consequently, the model predictions \(}=(^{T})\) can be computed with \((r(d+m_{l}))\) operations. When the rank \(r\) is sufficiently low, this is significantly faster than computing the exact inner products which requires \((dm_{l})\) operations. Our experiments (Section 7) indicate that fixing this hyperparameter to \(r=32\) works well with a wide range of data sets encompassing dimensionalities between 128 and 1536.

The optimal low-rank solution can be found by minimizing the training loss

\[}_{}=*{arg\,min}_{\,:\,() r}\|-\|_{F}^{2},\]

where \(\|\|_{F}\) is the Frobenius norm. This is the well-known _reduced-rank regression_ problem (Izenman, 1975). Denote the singular value decomposition (SVD) of \(\) as \(=^{T}\), where \(\) is a non-negative diagonal matrix and \(\) and \(\) are orthonormal matrices. The standard reduced-rank regression solution is \(}_{}=}_{}_{r} _{r}^{T}=^{T}_{r}_{r}^{T}= \), where \(_{r}^{m_{l} r}\) denotes the matrix that contains the first \(r\) columns of \(\) (i.e., the first \(r\) right singular vectors of the least squares fit \(=^{T}\) ), \(:=^{T}_{r}\), and \(:=_{r}^{T}\). In practice, we use a fast randomized algorithm (Halko et al., 2011) to compute only \(_{r}\) instead of the full SVD. Observe that the reduced-rank regression solution is different from the most obvious low-rank matrix factorization of the OLS solution computed via an SVD of the matrix \(\) (see Appendix A).

### 8-bit quantization

The simple computational structure of the reduced-rank regression solution enables us to further improve its query latency and memory consumption by using integer quantization. For each cluster, we quantize the matrices \(\) and \(\) to 8-bit integer precision. By also quantizing the query vector \(\), the model prediction \(}=^{T}}_{}=(^{T} )\) can be computed efficiently in two 8-bit integer vector-matrix products. We use _absmax quantization_ (e.g., Dettmers et al., 2022), where the elements of a vector \(\) are scaled to the range \([-127,127]\) by multiplying with a constant \(c_{}\) such that \(_{i8}=(127/\|_{732}\|_{})_{73 2}= c_{}_{732}\), where \(\) denotes rounding to the nearest integer.

We quantize the matrices \(\) and \(\) by applying absmax quantization to each column of the given matrix, resulting in vectors \(_{}\) and \(_{}\) of scaling constants. We can then recover a 32-bit floating-point approximation to the vector-matrix product \(=^{T}\) with

\[_{f32}}}_{i32} _{}=:_{i32}=_{i8 }^{T}_{i8}= Q()^{T}Q(_{f32}),\]

where \(Q()\) denotes absmax quantization, and \(\) and \(\) denote element-wise division and multiplication, respectively. To compute \(}=^{T}\) in the same fashion, we can first re-quantize \(\).

To ensure minimal loss of precision from the quantization, we rotate \(\) before quantization by multiplying \(\) with a random rotation matrix; this spreads the variance among the dimensions of \(\). Similarly, we rotate the vector \(\) resulting from the first product \(=^{T}\) before re-quantization. Since \(=^{T}_{r}\) and \(=_{r}^{T}\), we can rotate \(\) by rotating \(_{r}\) beforehand at no extra cost.

Memory usageStoring each matrix \(_{i8}[]_{256}^{d r}\) takes \(dr\) bytes and each matrix \(_{i8}[]_{256}^{r m_{l}}\) takes \(rm_{l}\) bytes. Thus in a clustering-based ANN index with \(L\) clusters and \(m\) corpus points, the total memory consumption of RRR is of order \(Ldr+rm\) bytes. In our experiments (Section 7), we use \(r=32\) for all data sets, while \(L\) is typically of order \(\).

LoRANN

In this section, we describe the additional implementation details of LoRANN, an open-source library that combines the standard template of clustering-based ANN search described in Section 2.2 with the score computation method described in Section 3. Using dimensionality reduction is particularly efficient for RRR (Section 4.1) and works well with 8-bit integer quantization (Section 4.2). Finally, we describe the GPU implementation of LoRANN (Section 4.3).

### Dimensionality reduction

With a moderate-sized corpus and high-dimensional data, computing the first vector-matrix product of the model prediction \(}=(^{T})\) can be more expensive than the second. In LoRANN, we further approximate the product by first projecting the query into a lower-dimensional space. We use the projection matrix \(_{s}^{d s}\) whose columns are the first \(s\) eigenvectors of \(_{}^{T}_{}\), where \(_{}^{n d}\) is the matrix containing the training set points \(\{_{i}\}_{i=1}^{n}\). To estimate the reduced-rank regression models, we use the \(s\)-dimensional approximations \(}_{i}=_{s}^{T}_{i}\) as inputs, but the true inner products \(_{i}^{T}_{j}\) as outputs. In this case, the reduced-rank regression estimate of the \(l\)th model is \(}_{}=}_{}_{r}_{r}^{T}=(_{s})^{}_{r}_{r}^{T}^{s m_{l}}\), where \(}_{}:=(_{s})^{}\) is the full-rank solution, and \(_{r}^{m_{l} r}\) is the matrix whose columns are the first \(r\) right singular vectors of \(\). Thus, now \(:=(_{s})^{}_{r} ^{s r}\) and \(:=_{r}^{T}^{r m_{l}}\).

We observe that computing query-to-centroid distances in the \(s\)-dimensional space yields a minor performance improvement. In the indexing phase, we perform \(k\)-means clustering using \(}_{j}=_{s}^{T}_{j}\). In the query phase, the \(s\)-dimensional approximation of the query point, \(}=_{s}^{T}\), is used to compute the distances to the cluster centroids and the predictions \(}=}^{T}}_{}\). The original \(d\)-dimensional query point \(\) is used for the dissimilarity evaluations in the final re-ranking step.

### Quantization implementation

The dimensionality reduction works particularly well with the 8-bit integer quantization described in Section 3.3. After dimensionality reduction, the first component of \(}\) corresponds to the principal axis. Thus, to further reduce the precision lost by quantization, we employ a mixed-precision decomposition by not quantizing the first component of \(}\) and the first row of both \(\) and \(\). Moreover, by premultiplying the projection matrix \(_{s}\) with a random rotation matrix, we rotate the query point \(}\) at no extra cost before quantization. For re-quantizing \(=}^{T}\), since \(=(_{s})^{}_{r}\) and \(=_{r}^{T}\), we can again randomly rotate \(_{r}\) beforehand at no extra cost.

We compute the 8-bit vector-matrix products \(_{i32}=}_{i8}^{T}_{i8}\) and \(}_{i32}=_{i8}^{T}_{i8}\) efficiently on modern CPUs using VPDPBUSD instructions in the AVX-512 VNNI instruction set.2 Since a VPDPBUSD instruction computes dot products of signed 8-bit integer vectors and unsigned 8-bit integer vectors, we store \(_{i8}\) and \(_{i8}\) as unsigned 8-bit integer matrices \(_{i8}^{}=_{i8}+128_{s r}[ ]_{256}^{d r}\) and \(_{i8}^{}=_{i8}+128_{r m}[ ]_{256}^{r m}\), and compute \(_{i32}=}_{i8}^{T}_{i8}= }_{i8}^{T}_{i8}^{}-128}_{i8}^{T}_{s r}\). The dimensionality reduction lowers the memory usage of LoRANN from \(Ldr+rm\) to \(Lsr+rm\) bytes.

### GPU implementation

Hardware accelerators such as GPUs and TPUs can be used to speed up ANN search for queries that arrive in batches (Johnson et al., 2019; Zhao et al., 2020; Groh et al., 2022; Ootomo et al., 2023). The computational structure of RRR, consisting of vector-matrix multiplications, makes it easy to implement LoRANN for accelerators, and the low memory usage of RRR (see Section 7.1.2) makes it ideal for accelerators that typically have a limited amount of memory.

Given a query matrix \(^{|Q| d}\) with \(|Q|\) queries, we need to compute the products \(_{i}^{T}_{ii}_{il}\) for all \(i=1,,|Q|\) and \(l=1,,w\). Here we denote by \(_{il}\) and \(_{il}\) the matrices \(\) and \(\) of the \(l\)th cluster in the set of \(w\) clusters the \(i\)th query point is routed into. We compute the required products efficiently using one batched matrix multiplication \(_{}_{}_{}\) by representing \(\) as a \(|Q| 1 1 d\) tensor \(_{}\), the matrices \(_{il}\) as one \(|Q| w s r\) tensor \(_{}\), and the matrices \(_{il}\) as one \(|Q| w r M\) tensor \(_{}\), where \(M\) is the maximum number of points in a single cluster. To avoid inefficiencies due to padding clusters with fewer than \(M\) points, we use an efficient balanced \(k\)-means algorithm (de Maeyer et al., 2023) to ensure that the clusters are balanced such that the maximum difference in cluster sizes is \(=16\).

On GPUs, 8-bit integer multiplication is presently both less efficient and less supported than 16-bit floating-point multiplication. Therefore, we use 16-bit floating-point numbers to perform all computations on a GPU. However, the 8-bit quantization scheme can still be useful on accelerators by allowing bigger data sets to be indexed with limited memory.

The simple structure of our method allows it to be easily implemented using frameworks such as PyTorch, TensorFlow, and JAX. This enables LoRANN to support different hardware platforms with minimal differences between implementations. We write our GPU implementation in Python using JAX which uses XLA to compile and run the algorithm on a GPU or a TPU (Frostig et al., 2018).

## 5 Out-of-distribution queries

In the standard benchmark set-up (Li et al., 2019; Aumuller et al., 2020), a data set is randomly split into the corpus and a test set, i.e., the corpus and the queries are drawn from the same distribution. However, this assumption often does not hold in practice, for example in cross-modal search. Thus, there is recent interest (Simhadri et al., 2022; Jaiswal et al., 2022; Hyvonen et al., 2022) in the out-of-distribution (OOD) setting. For instance, in the Yandex-text-to-image data set, the corpus consists of images, and the queries are text; even though both the corpus and the queries are embedded in the same vector space, their distributions differ (see Figure 2 in Jaiswal et al., 2022).

Due to the regression formulation, the proposed method handles OOD queries by design. To verify this, we construct an index for a sample of 400K points of the Yandex OOD data set in four different scenarios: (1) the default version (LoRANN-query) uses \(\{_{i}\}_{i=1}^{n}\), i.e., an \(n=400\)K sample from the query distribution, as a global training set and selects the local training set \(\{_{i}\}_{i J_{i}}\) as the global training set points that are routed into the \(l\)th cluster; (2) LoRANN-query-big is like LoRANN-query, but with \(n=1.2\)M; (3) LoRANN-corpus uses the corpus \(\{_{j}\}_{j=1}^{m}\) as a global training set, and selects the local training sets as \(\{_{j}\}_{j J_{i}}\) like the default version; (4) LoRANN-corpus-local uses \(\{_{i}\}_{j=1}^{m}\) as a global training set, but selects the local training sets as \(\{_{j}\}_{j I_{i}}\), i.e., uses only the corpus points of the \(l\)th cluster to train the \(l\)th model. We can thus disentangle the effect of the choice of the global training set from the effect of using the points in the nearby clusters to train the RRR models.

The results are shown in Figure 1. The version trained on queries outperforms the version trained only on the corpus, especially in the case of no re-ranking. Furthermore, we can use larger training sets to increase the performance of LoRANN. Both LoRANN-query and LoRANN-corpus outperform LoRANN-corpus-local, indicating that selecting the local training sets as described in Section 3.2 improves the accuracy of the regression models. We assume that this is because of the larger and more representative training sets, even though they are not from the actual query distribution.

## 6 Related work

Supervised ANN algorithmsLearning-to-hash methods (Weiss et al., 2008; Norouzi and Fleet, 2011; Liu et al., 2012) optimize partitions in a supervised fashion using data-dependent hash functions. Other supervised methods include learning optimal partitions by approximating a balanced graph partitioning (Dong et al., 2020; Gupta et al., 2022; Gottesbiren et al., 2024) and interpreting partitions as multilabel classifiers (Hyvonen et al., 2022). These supervised methods are orthogonal to our approach since they define the learning problem as selecting a subset of corpus points via partitioning. In contrast, we propose a supervised score computation method for clustering-based or, more generally, for partition-based ANN algorithms.

Product quantizationThe state-of-the-art clustering-based algorithms IVF-PQ (Jegou et al., 2011) and ScaNN (Guo et al., 2020) use quantization for data compression and score computation. They use a _quantizer_\(q:^{d}\) to map a point of the feature space to a value in a _codebook_\(\). Given \(\), they approximate the dissimilarity between the query point \(\) and the corpus point \(_{j}\) by \(d(,q(_{j}))\), i.e., the dissimilarity between the query point and the codebook value corresponding to \(_{j}\). Further, IVF-PQ and ScaNN quantize the residuals, i.e., the distances between corpus points and the cluster centroids, and use product quantization that decomposes the feature space into lower-dimensional subspaces and learns subquantizers in these subspaces. The code size, and thus the memory consumption, of PQ is directly proportional to the number of subquantizers.

## 7 Experiments

We use the ANN-benchmarks project (Aumuller et al., 2020) to run our experiments3 and replicate its experimental set-up as closely as possible (see Appendix B for the description of the experimental set-up). We use \(k=100\) for all experiments and measure recall (the proportion of true \(k\)-nn found) versus queries per second (QPS). Additionally, due to the lack of modern high-dimensional embedding data sets in ANN-benchmarks, we include multiple new high-dimensional embedding data sets in our experiments; for a description of all the data sets, see Appendix C.

Note that, even though we demonstrated in Section 5 that LoRANN can adapt to the query distribution, there are no samples from the actual query distribution available for the benchmark data sets of this section. Thus, we follow the standard approach by using only the corpus \(\{_{j}\}_{j=1}^{m}\) to train LoRANN.

### Reduced-rank regression

We first compare the proposed score computation method, reduced-rank regression (RRR), against product quantization (PQ) for clustering-based ANN search.4 We use RRR and PQ as scoring methods for an IVF index that partitions the corpus using \(k\)-means (see Section 2.2). We implement RRR using 8-bit integer quantization as described in Section 3.3 and compare against product quantization implemented in Faiss (Douze et al., 2024) with 4-bit integer quantization and fast scan (Andre et al., 2015). First, we compare the score computation methods at the optimal hyperparameters while keeping the clustering fixed, and then compare them at a fixed memory budget.

Figure 1: Recall vs. QPS on the Yandex T2I OOD data set (400K sampled corpus points) without (left) and with (right) the final re-ranking step. LoRANN-query is trained using a sample of 400K points from the query distribution as a training set, while LoRANN-query-big uses a sample of 1.2M points. LoRANN-corpus is trained using the corpus as a training set. LoRANN-corpus-local is trained using the corpus as a training set with only the cluster points as the local training sets of the reduced-rank regression models. It is beneficial to (1) use a sample from the actual query distribution as a training set and to (2) select the local training set by using also the points outside of the cluster as described in Section 3.2. The performance difference decreases when the final re-ranking step is introduced (requiring the original data set to be kept in memory).

#### 7.1.1 Fixed clustering

To directly compare the score computation methods, in Figure 2 we present results where the IVF index (the partition defined by \(k\)-means clustering) is the same for both methods. For RRR, we use \(r=32\), while for PQ each vector is encoded with \(d/2\) subquantizers for optimal performance. RRR outperforms PQ on seven out of the eight data sets; for complete results, see Appendix D.1.

#### 7.1.2 Memory usage

In Figure 3, we compare the performance of RRR (IVF+RRR) and PQ (IVF+PQ) by varying the rank parameter \(r\) for RRR and the code size for PQ such that \(b\), bytes per vector, is similar for both. For all values of \(b\), RRR outperforms PQ which is a typical choice in memory-limited use cases. Note that RRR with \(b 16\) outperforms PQ even with \(b 64\) on all data sets; for the full results, see Appendix D.2. The results are similar when no final re-ranking step is used (Appendix D.3).

### LoRANN

In this section, we measure the end-to-end performance of LoRANN (see Section 4). We first perform an ablation study on the components of LoRANN (Section 7.2.1), and then perform an end-to-end evaluation of LoRANN against the state-of-the-art ANN libraries in both the CPU setting (Section 7.2.2) and the GPU setting (Section 7.2.3).

Figure 3: Performance comparison of RRR and PQ at different levels of memory usage. We vary the rank parameter \(r\) for RRR and the code size for PQ such that \(b\), bytes per vector, is similar for both. RRR@\((b 16)\) outperforms even PQ@\((b 128)\) which uses eight times as much memory.

Figure 2: Performance comparison between RRR and PQ. The \(k\)-means clustering (IVF index) is kept constant to directly compare the effect of the score computation method (here \(c\) denotes the number of clusters). The proposed score computation method outperforms the baseline method (PQ).

#### 7.2.1 Ablation study

In Figure 4, we study the effect of the different components of LoRANN on its performance (for the full results, see Appendix E.1). As the baseline, we use an IVF index. Adding the score computation step via RRR significantly improves performance on all the data sets.

Dimensionality reduction (DR) is beneficial on higher-dimensional data sets with moderate-sized corpora: if the number of points in a cluster is lower than the dimension, then the first vector-matrix product in the computation \((^{T})\) of the local reduced-rank regression models will be more expensive. For large data sets with high-dimensional data, this effect decreases. Dimensionality reduction also does not improve performance on the lower-dimensional data sets (\(d 300\)). Incorporating 8-bit quantization improves not only the memory usage but also the query latency.

In Appendix E.2, we study the effect of varying the rank \(r\) of the parameter matrices. We find that increasing \(r\) from 32 to 64 has little effect, while \(r=16\) performs worse for high-dimensional data (but can be used to further decrease memory usage). For all of our other experiments, we use \(r=32\).

#### 7.2.2 CPU evaluation

As the baseline methods, we choose four leading graph implementations, HNSW, GLASS5, QSG-NGT, and PyNNDescent (Dong et al., 2011), two leading product quantization implementations, Faiss-IVFPQ (fast scan) and ScaNN, and the leading tree implementation MRPT (Hyvonen et al., 2016). See Figure 5 for the results and Appendix E.3 for the results on all 16 data sets. Three trends emerge: (1) LoRANN outperforms the product quantization methods on all data sets except glove-200-angular. (2) LoRANN performs better in the high-dimensional regime: it outperforms all the other methods except GLASS and QSG-NGT on all but the lower-dimensional (\(d 200\)) data sets. (3) Compared to graph methods, LoRANN performs better at the lower recall levels: QPS-recall curves of LoRANN and GLASS cross between 80% and 99% on most of the data sets. On 8 of the 16 data sets, LoRANN has better or similar performance as QSG-NGT at all recall levels.

Furthermore, in Appendix E.4, we demonstrate that in general LoRANN has faster index construction times than the graph-based methods.

#### 7.2.3 GPU evaluation

We compare LoRANN against GPU implementations of IVF and IVF-PQ in both Faiss (Douze et al., 2024) and the NVIDIA RAFT library6. In addition, we compare against a state-of-the-art GPU graph algorithm CAGRA (Ootomo et al., 2023) implemented in RAFT. All algorithms receive all test queries as one batch of size 1000. See Figure 6 for representative results, and Appendix E.5 for the complete results. LoRANN outperforms the other methods on seven out of the nine high-dimensional (\(d>300\)) data sets. For \(d 300\), CAGRA has the best performance.

Figure 4: LoRANN ablation study. On the high-dimensional (\(d=768\)) data set (left), all the components improve the performance of LoRANN. On the lower-dimensional (\(d=200\)) data set (right), all the components except dimensionality reduction (DR) improve performance.

To demonstrate the ease of implementation of our algorithm for new hardware platforms, in Appendix E.5 we implement our method for Apple silicon. We show using the M2 Pro SoC that LoRANN can take advantage of the M2 GPU and its unified memory architecture to achieve faster queries.

## 8 Discussion

In this article, we show that an elementary statistical method, reduced-rank regression (RRR), is surprisingly efficient for score computation in clustering-based ANN search. Since RRR outperforms product quantization (PQ) at fixed memory consumption while being simpler to implement efficiently, we recommend using it in regimes where PQ is traditionally used, e.g., when the memory usage is a limiting factor (Douze et al., 2024). While the experiments of the article are performed in the standard in-memory setting, the simple structure and the small memory footprint of the proposed method suggest scalability for larger data sets that do not fit into the main memory. In particular, hybrid solutions that store the corpus on an SSD (Jayaram Subramanya et al., 2019; Ren et al., 2020; Chen et al., 2021) and the distributed setting where the corpus and the index are distributed over multiple machines (Deng et al., 2019; Gottesbiren et al., 2024) are promising research directions.

LimitationsSince reaching the highest recall levels (\(>90\%\) for \(k=100\)) requires exploring many clusters, graph methods are usually more efficient than clustering-based methods in this regime. Furthermore, low-dimensional data sets (e.g., \(d<100\)) also require that the rank \(r\) of the parameter matrix is reasonably high. Thus, reduced-rank regression is not efficient for low-dimensional data sets for which the proportion \(r/d\) is too large. The proposed score computation method is also only applicable to inner product-based dissimilarity measures. However, the multivariate regression formulation of Section 3.1 can be extended for other dissimilarity measures.

Figure 5: CPU comparison. The QPS-recall curves of LoRANN and the leading graph library GLASS cross at the 95% (left) and at the 90% recall level (right), indicating that LoRANN is the fastest method at the lower recall levels, and GLASS at the higher recall levels.

Figure 6: GPU comparison. On the high-dimensional \((d=768)\) data set (left), LoRANN is the fastest, and on the lower-dimensional \((d=200)\) data set (right), the graph method CAGRA is the fastest.