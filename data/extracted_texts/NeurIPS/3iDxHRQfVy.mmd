# Had Enough of Experts? Elicitation and Evaluation

of Bayesian Priors from Large Language Models

 David Selby\({}^{1}\) Kai Spriestersbach\({}^{1}\) Yuichiro Iwashita\({}^{2}\) Dennis Bappert\({}^{3}\)

**Archana Warrier\({}^{1}\) Sumantrak Mukherjee\({}^{1}\) Muhammad Nabeel Asim\({}^{1}\) Koichi Kise\({}^{2}\) Sebastian Vollmer\({}^{1}\)**

\({}^{1}\)DFKI GmbH \({}^{2}\)Osaka Metropolitan University \({}^{3}\)Amazon Web Services

david.selby@dfki.de

###### Abstract

Large language models (LLMs) have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative information retrieval is less well understood. Here we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid elicitation of expert-informed prior distributions for Bayesian statistical models. We present a prompt engineering framework, treating an LLM as an interface to scholarly literature, evaluating responses in different contexts and domains. We discuss the implications and challenges of treating LLMs as 'experts'.

## 1 Introduction

Automated solutions for life sciences, industrial and governmental processes demand large amounts of data, which are not always available or complete. Small datasets are vulnerable to overfitting, weakening the validity, reliability and generalizability of statistical insights. To overcome these limitations, analysts employ two approaches. Data-based or empirical methods maximize information extraction, through imputation models, data augmentation and transfer learning; however, this is limited by the size, availability and representativeness of the training set. Alternatively, one can exploit prior information, via knowledge graphs or expert-elicited Bayesian priors, allowing for sparser models and handling of missing values. This approach is constrained by the difficulty, cost and myriad different methods of obtaining and eliciting subjective and heterogeneous opinions from experts, then translating them into a form amenable to quantitative analysis .

Large language models (LLMs) are generative models capable of producing natural language texts based on a given prompt or context. LLMs such as GPT-4 have been used in various applications, such as chatbots, summarization and content creation. In the quantitative sciences, LLMs have been applied to mostly qualitative tasks such as code completion, teaching of mathematical concepts  and offering advice on modelling workflows or explaining data preparation pipelines [3; 4]. Some work has also applied LLMs to mathematical reasoning and symbolic logic [5; 6]. When linked with certain application programming interfaces (APIs), or incorporated into a retrieval-augmented generation (RAG) tool, some LLM frameworks [e.g. 7] are also capable of evaluating code, connecting to other data analysis tools or looking up supporting information [8; 9]. However, the capabilities of LLMs to retrieve accurate and reliable _quantitative_ information are less well-explored. Here we explore eliciting from LLMs informative 'expert' priors for Bayesian models.

Can large language models be considered 'experts', having read a large sample of the scientific literature in their training corpora, and thus treated as an accessible interface to this knowledge? Here we develop a prompting methodology to elicit prior distributions from LLMs, emulating realworld elicitation protocols. LLM-elicited priors are compared with those from human experts, and the LLM 'expertise' is quantitatively evaluated for several tasks.

## 2 Related work

Language models have been noted for their remarkable ability to act as unsupervised knowledge bases . [11; 12] discuss the 'emergent' numeracy skills of LLMs, from early models unable to perform simple addition to later versions able to compute correlations.  showed that repeated sampling from LLMs does not yield reasonable distributions of random numbers, making them poor data generators.  also suggested LLMs tend to underestimate uncertainty. It has been hypothesized that _mode collapse_ can inhibit the diversity of LLM outputs . The design, adaptation and use of LLMs to assist data analysis is a broad topic. Many LLM-based data science tools focus either on generation of analysis code  or connection with external APIs . LLMs fine-tuned on scientific texts may be used to extract qualitative information, such as chemical formulae or entity relations . A conversation with a chatbot can also offer generic advice on data science practices.

Prior distributions are just one form of knowledge elicited from domain experts; others include feature engineering, model explanations and labelling heuristics, but in each case the process of elicitation typically involves interviews, written correspondence or interaction with a custom computer app . A good expert-elicited prior distribution can help a statistical model effectively represent the data generating process, although due to various practical, technical and societal factors, prior elicitation is not yet widespread practice. A lack of standardized software means there is no way for an analyst using, e.g. Stan, to initiate an elicitation exercise for a specific model .

LLM-_driven_ elicitation  uses an LLM to assist elicitation from human experts, making the process interactive. In engineering, LLMs have been employed in generating (and responding to) requirements elicitation surveys [21; 22; 23]. Natural language processing is already extensively used to extract quantitative information from large texts to aid quantitative research [see, e.g. 24]. Prior distributions can be elicited from literature via systematic reviews [25; 26; 27]. A meta-analytic-predictive prior uses historical data to reduce the required sample size in clinical trials . To our knowledge, direct elicitation of parametric priors from a 'domain expert' LLM has not yet been explored.  generated pseudodata as an indirect prior elicitation approach; by contrast, in this paper we attempt to elicit the distributional parameters directly.

Several elicitation protocols have been developed to mitigate cognitive biases and combine the judgements of multiple experts . The Sheffield Elicitation Framework [Shelf; 31] describes a collection of methods for eliciting a distribution based on aggregated opinions of multiple experts, through group discussion guided by a facilitator. Following a primer in probability and statistics, the protocol includes various ways of eliciting a univariate distribution, such as the 'roulete method', where participants assign chips to bins to form a histogram. Alternatively, the quartile method [or 'Sheffield method'; 32] uses a series of questions to elicit quantiles of a distribution. Cooke's method  pools the distributions of multiple experts, weighted according to their respective 'calibration' (accuracy) and 'information' (uncertainty). The Delphi method uses the quartile method, iteratively refined over successive rounds using anonymized feedback from other participants. In this paper, however, we consider only single-agent LLMs with a zero-shot approach.

## 3 Methods

### Evaluating expertise

What makes a good prior? Bayesian statistics involves decisionmaking based on a posterior distribution, \(p(|D)()_{i=1}^{n}p(x_{i}|),\) where \(()\) denotes the prior distribution and \(\) a vector of parameters to model \(x_{i}\), the observed data. The definition of a 'good' prior distribution--like Bayesian statistics itself--is subjective, depending on the analyst's understanding of the purpose of expert-elicited information. No standard benchmark exists for expert-elicited prior distributions; a prior is a function of the expert and the elicitation method, as well as of the predictive task . One purpose of prior information is to reduce amount of data needed. Another is to treat expert knowledge and observed data as complementary sources of information about a natural process. Any statistical model is at least slightly misspecified, but a prior can still be _informative_, _realistic_ and _useful_ [see 35]. An informative prior is different from a non-informative or default prior, i.e.

it is not too vague. Realistic or well-calibrated priors should align with those from human experts or be otherwise externally verifiable. 'Useful' means superior posterior predictive performance on a downstream task, improving expected utility over reference priors. Here we consider informativeness and realism.

A measurement of the informativeness of a prior distribution is the prior effective sample size [36; 37]. This is neither data-dependent nor measures improvement on downstream tasks, but how many data points needed to get similar peakiness/curvature around the posterior mode. The heuristic prior effective sample size for \((,)\) is \(=+\), which measures the concentration of the prior and the amount of data needed to shift the posterior if the prior were misspecified.

We can measure realism with the Bayesian log posterior predictive density  (a.k.a. log loss) or the continuous ranked probability score, a proper scoring rule used in weather forecasting . We can estimate both metrics using the posterior predictive distribution \(p(^{}|D)=_{p(|D)}[p(^{ }|)]\) on held-out data.  describe a similar approach quantifying utility of synthetic data.

### Eliciting prior distributions from LLMs

Impersonating a human domain expert can improve an LLM's performance at related tasks . Nevertheless, in response to scientific questions, especially on potentially sensitive topics, such as healthcare advice, language models often prevariate [**lautrup'heart-to-heart'2023**]. An LLM elicitation system should therefore not only prompt the model to roleplay an expert, but also carefully specify the task to ensure contextually relevant information is returned in the appropriate format.

Our _expert prompt initialization_ module is a system prompt defining a suitable expert role for the model to imitate. For efficiency, the LLM itself is used to generate these descriptions, once per task, of the form "You are...". To avoid the model offering verbose, generic or prevariating advice about prior elicitation, the _task specification_ module insists that the model follows a particular elicitation protocol followed by returning a parametric prior distribution in a standardized format, e.g. "Beta(1, 1)". Further details are given in the appendix and code is available on GitHub.

### Experiments

Human expertsAbsent an open benchmark of expert-elicited priors, we select a recent work from the literature that describes an elicitation procedure and reported the resulting distributions.  interviewed six psychology researchers about typical small-to-medium effect sizes and Pearson correlations in their respective specialisms, using the histogram method. Using similar question wording, we elicited prior distributions from LLMs prompted to simulate an expert, conference of experts  or non-expert, with and without mentioning the Shelf protocol. This experiment is a qualitative comparison of how LLMs behave when emulating a published example of a prior elicitation exercise with published question wording and results.

Expert confidenceWe prompted ChatGPT 3.5 to formulate 25 tasks that might call for expert elicitation in the fields of healthcare, economics, technology, environmental science, marketing and education. Tasks correspond to proportions or probabilities following a beta distribution. These scenarios were then used to gauge general levels of confidence of elicited distributions from different LLMs, using the prior effective sample size metric, \(+\).

MeteorologyHere we tried to illustrate how many samples the LLM prior offers for an analyst who has not yet collected any data. We compare the prior predictive to probabilistic supervised learning in the same statistical family : a normal-inverse-gamma model for temperature and a gamma-exponential for precipitation. We ask: how many samples on average would a frequentist model need to achieve the same or better log-loss (or CRPS or MSE) than the prior predictive distribution? We split the data in half for testing and repeatedly sample up to \(\) for training from the remaining half. An alternative comparison would be of a posterior predictive based on data and a baseline prior, however choosing such a baseline is difficult. Unlike the (\(+\)) effective sample size heuristic, this data-dependent approach quantifies prior-data conflict. Priors were elicited from LLMs for the typical daily temperature and precipitation in 25 small and large cities around the world during the month of December. These distributions were then compared with historical weather data. By investigating different continents and varying sizes of settlements, the goal was to identify any systematic biases that might emerge from LLMs' respective training corpora. It is also interesting to compare the behaviour with skewed and symmetrical distributions.

## 4 Results

Human and LLM-elicited distributions are compared in Figure 1. Roleplaying as experts in different sub-fields did not have a noticeable effect on the priors. LLM priors for Cohen's \(\) were mostly centred around small effect sizes, except GPT-4, which offered distributions around \(=0.5\). Mistral-7B-Instruct invariably gave \(t\) distributions with \(=30\) (Llama-70B-Chat-Q5: \(=5\)); other models appeared to grow more conservative (smaller \(\)) if asked to roleplay an expert, simulate a decision conference or employ Shelf. Beta priors from LLMs apparently had little in common with those from real experts: GPT-4 provides a symmetric unimodal distribution whereas other models offer a right-skewed 'bathtub' distribution.

For the expert confidence experiment, Figure 3 shows \(+\) for beta priors. Llama-based models appear to give more conservative priors, GPT-4 is consistently more informative and Mistral 7B Instruct occasionally offered extremely high values. There was no clear difference between domains.

In our meteorological task, Figure 2 shows data-dependent effective sample size of the prior predictive distribution elicited from LLMs, using the approach described above. In many cases, the prior predictive model is in conflict with the data (i.e. overconfident, inaccurate priors) so the ESS is equal to zero, but not for a selection of larger cities. This may be due to the LLMs defaulting to data from more extensively studied regions in their training corpora.

Further results are given in the appendix.

Figure 1: Priors for Cohenâ€™s \(\) (left) and Pearson correlations (right) elicited from LLM and human experts in psychology. Dashed lines denote a Shelf-like elicitation protocol

Figure 2: LLM priors for meteorology: number of observations needed for frequentist model to achieve better MSE than the prior predictive (right figure shows results for GPT-4)

Conclusion

In this paper we demonstrated the feasibility of extracting informative Bayesian prior distributions from generic LLMs with a simple expert prompting framework. Methods for the qualitative and quantitative evaluation of informativeness and realism of elicited priors allow assessment without specifying downstream tasks. LLMs potentially promise a more efficient interface to scientific knowledge than recruiting and interviewing domain experts.

However, like human experts, the models vary considerably in their level of confidence around different phenomena, making discrepancies apparently more model- than task-dependent.LLMs are inherently shaped by the composition and diversity of their training data, potentially introducing biases that may affect the generalizability of results when considering LLMs as surrogate experts or integrating them into Bayesian reasoning frameworks. Results indicate that quantitative knowledge retrieval from LLMs has room for improvement, necessitating fine-tuned domain models, advanced prompt engineering techniques or multi-agent frameworks.

The comparison of human domain experts and LLM-based expert systems remains challenging, and warrants further development. Genuine domain expertise continues to play an important role in data analysis.