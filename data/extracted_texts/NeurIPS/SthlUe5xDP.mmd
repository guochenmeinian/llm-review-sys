# Topological Parallax: A Geometric Specification for Deep Perception Models

**Abraham D. Smith**

Geometric Data Analytics, Inc.

343 W. Main Street

Durham, NC 27701 USA

abraham.smith@geomdata.com

University of Wisconsin-Stout

Math, Stats, and CS Dept

Menomonie, WI 54751 USA

smithabr@uwstout.edu

**Michael J. Catanzaro**

Geometric Data Analytics, Inc.

343 W. Main Street

michael.catanzaro@geomdata.com

**Gabrielle Angeloro**

Geometric Data Analytics, Inc.

343 W. Main Street

Durham, NC 27701 USA

gabrielle.angeloro@geomdata.com

**Nirav Patel**

Geometric Data Analytics, Inc.

343 W. Main Street

Durham, NC 27701 USA

nirav.patel@geomdata.com

**Paul Bendich**

Geometric Data Analytics, Inc.

343 W. Main Street

Durham, NC 27701 USA

paul.bendich@geomdata.com

Duke University

Mathematics Dept

Durham, NC 27708 USA

bendich@math.duke.edu

**Abstract**

For safety and robustness of AI systems, we introduce _topological parallax_ as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure.

Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between "overfitting" and "generalization" in applications of deep-learning.

In typical DNN applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset.

Parallax presents theoretically via topological data analysis [TDA] as a bi-filtered persistence module, and the key properties of this module are stable under perturbation of the reference dataset.

Introduction

Suppose \(X\) is a finite subset of \(V=^{n}\) with the Euclidean metric.1 In data science--particularly in applications of DNNs--we often encounter the situation where \(X\) is a dataset, and some opaque algorithm has produced a trained model \(k:V\{0,1\}\), where \(k(x)=1\) for all \(x X\). This defines the model as a set of accepted inputs \(K=\{x:k(x)=(1)\} V\), which has no available description beyond evaluation of the perception function \(k\) on samples.

Our main contribution in this paper is a method we call _topological parallax_ to estimate the multiscale geometry of \(K\) from the persistent homology ([14; 37]) of \(X\), in a situation where \(K\) does not have an explicit description.2 This method provides meaningful geometric information about \(K\) through a simple computational approach that can be applied to any perception model \(k\). We prove that the resulting criterion of _homological matching_ satisfies a stability property. We propose homological matching via parallax as a geometric specification that could be applied to many machine-learning systems. The measurement of homological matching also admits a back-propagation scheme, which could be used to improve the geometric similarity between the model \(K\) and the dataset \(X\).

Because of the generality of Definition 1.1, it may be that \(V\) represents any layer of a neural network, \(X\) represents any dataset mapped into that layer, and \(K\) represents an activated region in that layer. For example, the "neural collapse" concept from Papyan et al.  can be seen as a special case of this specification, because the conception from  is that the dataset \(X\) becomes a tight blob in the penultimate layer, and the penultimate model is simply a Voronoi region \(K\) surrounding that blob.

### Assumptions and Motivation

As discussed by Belkin , DNNs usually achieve high statistical accuracy, but some resulting models are better than others at capturing patterns in the dataset. Despite having perfect statistical accuracy on the dataset \(X\), fundamental questions arise about the model \(K\): "Is it safe to deploy? Is it trustworthy? Is it a good model?" To broadly paraphrase , it used to be good practice to tell data analysts not to overfit their data, because overfit models were "bad" due to poor generalization; however, essentially every DNN fits the training data perfectly, so it is not clear what distinguishes "good" models from "bad." _We suggest that a model \(K\) is "good" if the geometry of \(K\) matches the geometry of \(X\)_. Consider the two example models in Figure 1. Although both models achieve perfect statistical accuracy, only one of them appears to have learned the geometric structure of the dataset. This suggestion is likely intuitive to many ML engineers, but the subtlety lies in the implicit assumptions often made about the nature of the dataset and the available models. We assess this matching in a way that is independent of the architecture that produced \(k\), and that makes very few assumptions about \(X\) and \(K\), as encapsulated by Definition 1.1, which are assumed henceforth.

**Definition 1.1** (Datasets and Models).: _Suppose that \(V\) is a geodesic space. We define a model \(K V\) to be the closure of an open set, \(K=} V\), colloquially known as a "solid." For any finite dataset \(X V\), we consider the collection of all models for which \(X\) is contained in the interior of the model, \(X K^{}\); let \((X)\{K V\ :\ X K^{},\ }=K\}\). With subset inclusions as morphisms, \((X)\) is a small category. For any \(K V\) with \(K=}\), let \(^{*}(K)\{X V\ :\ X K^{},\ \#X<\}\). With functions of finite sets as morphisms, \(^{*}(K)\) is a small category. Of course, \(X^{*}(K) K(X)\). Note that \(V(X)\)._

Note that we make no assumptions whatsoever about the architecture or training method that yielded \(K\). With such a broad definition of \(X\) and \(K\), we need a notion of "geometry" with very few assumptions. We use standard topological notation [23; 17; 14].

**Definition 1.2** (Void).: _Given a set \(K V\), an interpolative void in \(K\) is a bounded open set in the complement of \(K\), \( K^{c}\), such that there exists a pair of points \(x,y K\) for which all \(V\)-geodesics pass through \(\). If \(V=^{n}\), this means \(\) intersects the convex hull of \(K\)._Why focus on voids? As observed by Balestriero et al. , when \(V\) is high-dimensional, it is unlikely that any points in \(X\) lie in the convex hull of any others. However, it also seems unlikely that a "good" model \(K\) of \(X\) will be merely a convex solid. If convex models were sufficient for real-world problems, DNNs would be unnecessary, and the field would have concluded with PCA, Gaussian kernels, and convex polytopes. For example, many models implicitly or explicitly rely on the so-called manifold hypothesis, which is the hope that realistic datasets \(X\) will tend to be distributed near a union of lower-dimensional manifolds immersed in \(V\), in which case a "good" model \(K\) would be a slight thickening of those manifolds to allow for measurement error. If there are multiple manifolds or there is nonzero curvature, such a model will have voids.

Any void in \(K\) indicates a region where \(K\) does not allow interpolation, which is where interesting geometry occurs. Voids occur if and only if some geodesic in \(K\) is strictly longer than the corresponding geodesic in \(V\). Hence, for this article, we interpret "geometry of \(K\)" as the presence or absence of voids. We do not make assumptions about the geometric features of \(X\) or about the family from which \(K\) is chosen; rather, we ask that \(K\) respects the features of \(X\), whatever they are. We propose that a "good" model \(K\) is one whose voids represent the highly persistent features of \(X\), in the sense of Topological Data Analysis [TDA, ]

A model \(K\) can be "bad" because it has too many or too few voids at various scales. For example, mismatch of voids of \(K\) and features of \(X\) would indicate that \(K\) is over-sensitive to small error or under-sensitive to large error, either of which could lead to adversarial attack. Also, numerous small-scale voids could make \(K\) incompatible with some forms of the manifold hypotheses, by obstructing the coverage of \(K\) by an atlas of local convex charts of moderate size.

### Outline

Section 2 introduces our key object, the bi-graded  parallax complex, in Definition 2.2, which measures geodesic distortion via the Rips complex. Section 3 provides a notion of dataset perturbation

Figure 1: A dataset \(X^{2}\). 2: The persistence diagram of the Rips complex of \(X\), showing three components (two of which die at the dim-0 dots near 2.0) and two cycles (the dim-1 dots, representing the annulus and the overall arrangement) among the persistent features. 3: A tree model \(K_{1}\). 4: A neural network model \(K_{2}\). The tree model has many small voids that would forbid interpolation or perturbation in many locations. Without the luxury to visualize models \(K V\) for \( V>3\), Parallax can measure geometric similarity between \(X\) and \(K\) via the Rips complex \(R\) of \(X\).

Figure 2: Five models for the same dataset, and the scales detected by parallax. The circle have radii 20 and 30, respectively. See Section 6 for general interpretation. The missing edge causing \(_{}\) is highlighted in red at filtration radius 7.1, and the missing edges that cause \(_{}\) are highlighted in blue. The dashed blue edge does not affect \(_{}\) because the larger cycle determines that value.

and shows that the parallax complex and its homology remain stable under those perturbations. Section 4 defines _local simplicial matching_ and shows how parallax detects small-scale changes in the Rips complex of \(X\) in \(K\) versus \(V\), giving a clearly interpretable scale of locality, \(_{}\). Section 5 defines _homological matching_ and shows how parallax detects large-scale voids in \(K\), giving a scale \(_{}\) above which homological features in \(X\) are respected by voids in \(K\). Together, these results provide an overall interpretation as a specification in Section 6, which largely achieves the goals laid out in Section 1. Section 7 provides computational approaches to computing parallax, and links to our open-source software that has many practical improvements not detailed in this paper. Section 8 illustrates the effectiveness of parallax as a specification, as demonstrated on two models using the cyclo-octane dataset . Additional proofs, details, and examples are provided in the Supplementary Material appendices.

### Related Work

To the best of our knowledge this is the first work to use TDA to express a desired geometric relationship that holds directly between datasets and models trained on them. There has been some work, for example the Manifold Topology Divergence of Barannikov et al.  or the Geometric Score of Khrulkov and Oseledets , which uses various TDA-based measures to quantify the difference between training data and new data generated by generative models. Quite a few other papers (see Fernandez et al.  for a very recent example) use TDA-based constructions to infer properties of underlying data manifolds, usually under very strict sampling assumptions.

More broadly, there has been a recent explosion of work (e.g. Hensel et al. ) connecting TDA to ML/DL. Several works (e.g. Adams et al. , Bendich et al. ) use TDA as a _feature extraction_ method, pre-processing more complicated data objects before running standard ML pipelines. Later works (e.g. Chen et al. , Demir et al. , Solomon et al. , Nigmetov and Morozov ) use TDA to define novel losses within ML algorithms. Note that we comment below on ways in which our notion of homological matching can be used to define a TDA-based loss. TDA has also been used (e.g. Naitzat et al. , Wheeler et al. ) to analyze the behavior of data as it passes through the layers of a DNN. Some works (e.g. Guss and Salakhutdinov ) assess the capacity of a specific DNN to classify datasets with specific shapes, but do not provide tools to quantify shape mismatch between model and dataset. Finally, several works (e.g. Carriere et al. , Papillon et al. ) use TDA to define novel DNN architectures, including GNNs and other higher-order combinatorial structures.

There is also a recent stream (e.g. Liu et al. , Wang et al. ) of work that builds validation and verification/falsification techniques for desired properties of DNN-trained models; these mostly focus on the mechanics of how to verify/falsify such properties, rather than attempting to define them as we do. Perhaps the closest work in this stream to ours is Dola et al. , which uses a prior assumption on the underlying data distribution to verify/falsify DNN properties.

## 2 The Parallax Bi-Complex

Let \(B_{}(x)\) denotes the closed geodesic ball of radius \(\) around \(x V\). For a formal edge \(e=(x_{0},x_{1})\) between points in \(X\), \(_{V}(e)\) is the minimum radius for which \(B_{_{V}(e)}(x_{0})\) intersects \(B_{_{V}(e)}(x_{1})\). Thus, \(2_{V}(e)\) is the geodesic distance between \(x_{0}\) and \(x_{1}\). The Rips complex \(R(X,V)\) is the simplicial complex generated by these edges, as filtered by \(_{V}(e)\). A chain is a formal sum of simplices in a complex . More generally, for any \(K(X)\), the Rips complex \(R(X,K)\) and its filtration by \(0<\) is defined by

\[(x_{0},,x_{d}) R_{d}(X,K)_{}B_{}(x_{i}) B_{}(x_{j}) K,\;\;0 i <j d.\] (2.1)

For any chain \(Y R(X,K)\), let \(_{K}(Y)=\{\,:\,Y R(X,K)_{}\}\). When \(X\) and \(V\) are understood in context, we abbreviate \(R=R(X,V)\) for the ambient case \(K=V\).

**Lemma 2.1**.: _If \(K_{1},K_{2}(X)\) with \(K_{1} K_{2} V\), then there is a natural inclusion of filtered modules, \(R(X,K_{1})_{} R(X,K_{2})_{} R_{}\). That is, \(_{V}(Y)_{K_{2}}(Y)_{K_{1}}(Y)\), with the convention \(=\)._

The previous lemma is simply because geodesic lengths in \(K_{1}\) are never shorter than geodesic lengths in \(K_{2}\), and neither is shorter than geodesic lengths in \(V\). Our approach to the question "does the geometry of K match the geometry of X?" relies on detecting the inequality \(_{V}(Y)<_{K}(Y)\).

**Definition 2.2** (Parallax Complex).: _For \(K(X)\), let \(P(X,K,V)\) denote the subcomplex of \(R\) defined for each real pair \((,)\) by_

\[P(X,K,V)_{,}=\{Y R\ :\ _{K}(Y),\ _{V}(Y) _{K}(Y)_{V}(Y)+\}.\] (2.2)

_When \(X,K,V\) are understood in context, we abbreviate \(P=P(X,K,V)\)._

The parameter \(\) measures the distortion of geodesic length in \(K\) versus \(V\). The next few lemmas are immediate consequences of the definition.

**Lemma 2.3** (Parallax is Bi-Filtered).: _If \(^{}\), then \(P_{,} P_{^{},}\). If \(<^{}\), then \(P_{,} P_{,^{}}\)._

**Lemma 2.4**.: _For all \(,\), we have \(P_{,} R(X,K)_{} R_{}\)._

Let \(:P_{,} R_{}\) denote the inclusion of complexes, and let \(_{*}:HP_{,} HR_{}\) denote the induced homomorphism on homology .

**Corollary 2.5** (Homology Deaths are later in Parallax).: _Suppose that \([Y]\) is a class in \(HP_{_{1},_{1}}\) such that the bi-transition map \(HP_{_{1},_{1}} HP_{_{2},_{2}}\) takes \([Y]\). Then there exists \(t_{2}\) such that the transition map \(HR_{_{1}} HR_{t}\) satisfies \(_{*}([Y])\)._

**Lemma 2.6**.: _For all \(\), we have \(P_{,}=P_{,}=R(X,K)_{} R_{}\)._

The proofs of 2.5 and 2.6 are given in the Supplemental Material.

It is sometimes useful to create single-parameter filtrations through \(P\), parameterized by Rips radius, for the purpose of computing barcodes and persistence diagrams.

**Definition 2.7** (Rips-like Path).: _A Rips-like path is a filtered module \(L_{}=P_{,()}\) for \(0<\) such that \(()\) is a non-decreasing function satisfying \((0)=0\)._

A Rips-like path has homology \(HL\) and a barcode or persistence diagram. By Lemma 2.6, one Rips-like path is \(R(X,K)_{}=P_{,}\). Another is the "inflexible" path \(L_{}=P_{,0}\).

## 3 Perturbation

This section establishes lemmas that ensure Parallax and its consequences (notably Theorem 5.4) are stable under certain types of perturbations, which means that the parallax is reasonable in the presence of noise.

**Definition 3.1** (Pointwise Perturbation).: _Given \(X^{*}(V)\), a pointwise \(\)-perturbation is \(X^{}^{*}(V)\) such that the sets \(X\) and \(X^{}\) admit a one-to-one correspondence \(f:X X^{}\) satisfying \(\|f(x)-x\|\). We write \(f:X}{{}}X^{}\)._

**Definition 3.2** (Pointwise \(K\)-perturbation).: _Suppose \(f:X}{{}}X^{}\) such that each \((x,f(x))\) pair is connected by a \(K\)-geodesic of length \(\). We write \(f:X}{{}}X^{}\)._

Note that any \(f:X}{{}}X^{}\) is an isomorphism in the category \(^{*}(V)\), and any \(f:X}{{}}_{K}X^{}\) is an isomorphism in the category \(^{*}(K)\). The relation \(}{{}}\) is reflexive and symmetric, but not transitive; hence, it is a way of describing proximity but does not provide an equivalence relation. Of course, \(X^{}}{{}}X\) implies the Hausdorff distance satisfies \(d_{H}(X,X^{})\). A pointwise \(\)-perturbation can cause length distortions by \(2\), as said formally in the following lemma.

**Lemma 3.3** (Data Perturbation Lemma).: _If \(f:X}{{}}X^{}\), then the Rips complexes identified via \(f\) admit a \(\)-interleaving_

\[ R(X^{},V)_{-}^{-1}}}{{}}R(X,V)_{}}}{{}}R(X^{},V)_{+}^{-1}}}{{}}R(X,V)_{+2}\]

_Specifically, for any edge \(e=(x_{i},x_{j}) R(X,V)_{}\) defined by the existence of a \(V\)-geodesic of length \(2\), the edge \(f_{}(e)=(f(x_{i}),f(x_{j}))\) in \(R(X^{},V)\) has length \(2^{}\) satisfying \(2-2 2^{} 2+2\)._

_Moreover, the same holds when replacing \(V\) with \(K\), under the assumption \(X}{{}}_{K}X^{}\)._Proof.: Note that a geodesic of length \(2\) corresponds with the intersection of two balls of radius \(\); hence, the factor of 2. The worst-case perturbation is to move each of \(x_{i}\) and \(x_{j}\) by \(\) in opposite directions, away from each-other, along their geodesic. 

**Lemma 3.4** (Parallax Interleaving Lemma).: _Suppose \(f:X}{{_{K}}}X^{}\). Let \(P=P(X,K,V)\) and \(P^{}=P(X^{},K,V)\). For any \(,\), these parallax complexes admit a \((,2)\)-interleaving_

\[P_{,}}P^{}_{ +,+2}^{-1}}P_{+2, +4}}\]

**Corollary 3.5** (Parallax Interleaving Lemma, Homology Version).: _Suppose \(f:X}{{_{K}}}X^{}\). Let \(HP=HP(X,K,V)\) and \(HP^{}=HP(X^{},K,V)\). For any \(,\), these homology groups admit a \((,2)\)-interleaving \(HP_{,}}HP^{}_{ +,+2})}HP_{+2, +4}}\)_

The proof of Lemma 3.4 is a worst-case distance estimate given in the Supplemental Material, and Corollary 3.5 follows functorially.

## 4 Sampling Density and Local Simplicial Matching

The goal "the geometry of \(K\) should match the geometry of \(X\)" requires that \(X\) has sufficient sampling density throughout \(K\) to express a meaningful comparison. The ideal situation would require there is a (small) scale \(\) for which: (1) \(K^{}\) is homeomorphic to \(_{x X}B_{}(x)\), so that these \(\) balls capture the topology of \(K\); (2) all "highly persistent" homological features of \(X\) are born before \(\); and (3) \(_{x X}B_{}(x) K\), so that perturbations of size \(\) in the dataset \(X\) are allowed, and so that these balls can be used as local charts in \(K\). These sampling properties may or may not be true for any particular pair \((X,K)\), but Definition 4.1 provides scales for comparison.

**Definition 4.1** (Locally Simplicially Matched).: _We say that \(X\) and \(K(X)\) are \(\)-locally simplicially matched [LSM] if the subset \(P_{t,0} R_{t}\) is an equality for all \(t\). For any \(K(X)\), the first non-LSM scale realized by the Rips complex is_

\[_{,X}(K)\{\ :\ X,K\}=\{_{V}(Y)\ :\ _{K}(Y)>_{V}(Y)\  Y R\}.\]

_The last LSM scale realized by the Rips complex is_

\[_{,X}(K)\{<_{,X}(K)\ :\ _{V}(Y)=\  Y R\}.\]

_Another LSM scale is \(_{,X}(K)\{\ :\ _{x X}B_{}(x) K^{}\}\)._

When \(X\) and \(K\) are \(\)-LSM, we will identify \(P_{t,0}\) with \(R_{t}\) so that \(H_{*}P_{t,0}=H_{*}R_{t}\) whenever \(t\). Furthermore, locally simplicially matched implies \(P_{t,}=R_{t}\) for any \(>0\) and \(t<_{,X}(K)\), following directly from the definition of \(P_{a,}\).

**Lemma 4.2**.: _If \(K(X)\), then \(0<_{,X}(K)<_{,X}(K)\)._

However, it may be that \(_{,X}(K)=0\), if the shortest edge \(e R\) has \(_{V}(e)<_{K}(e)\).

**Corollary 4.3**.: _For any \(K,K^{}(X)\), there is some \(0<\) such that each pair \((X,K)\) and \((X,K^{})\) is \(\)-locally simplicially matched._

**Corollary 4.4**.: _For any \(X,X^{}^{*}(K)\), there is some \(0<\) such that each pair \((X,K)\) and \((X^{},K)\) is \(\)-locally simplicially matched._

**Lemma 4.5**.: _If \(K_{1},K_{2}(X)\) and \(K_{1} K_{2}\), then \(_{,X}(K)_{,X}(K^{})\)._

The next lemma provides a bound on \(_{}\) under small data perturbations, which is a form of stability.

**Lemma 4.6**.: _Assume Euclidean \(V\). Suppose that \(K(X)(X^{})\). If \(f:X}{{_{K}}}X^{}\) for \(<_{,X}(K)\) then \(_{,X}(K)-_{,X^{ }}(K)\)._

The proof of Lemma 4.6 is a triangle-inequality argument in the Supplemental Material. The other results are immediate observations from the definitions.

Homological Matching

From Section 1, our purpose is to determine whether the geometry of \(K\) matches the geometry of \(X\). In Section 4, we introduced "local simplicial matching" as a way to compare small-scale geometry. In this section, we introduce "homological matching" as way to compare large-scale geometry. Generally this is done by asking whether highly persistent features of \(X\) in \(V\) (as measured by \(HR\)) are also highly persistent as features of \(X\) in \(K\) (as measured by \(HP\)). This comparison is sensible if \(X\) and \(K\) are \(\)-locally simplicially matched, so that cycles can be identified between \(HR_{}=HP_{,0}\). We phrase it algebraically in Definition 5.1, but Lemma 5.3 provides the interpretation that, among cycles born before \(\), those of long persistence \((-)\) in \(HR\) have even longer persistence \((-)\) in \(HL\), meaning that \(K\) has large-scale homological features corresponding to those of \(X\).

**Definition 5.1** (Homologically Matched).: _For \(K(X)\), and \(0<_{,X}(K)<\), we say that \(X\) and \(K\) are \((,,)\)-homologically matched [HM] if the transition maps of \(HR\) and \(HP\) satisfy \( HR_{} HP_{(,e_{1})(,e_{2})}\) for some \(0_{1}_{2}\). Equivalently, if \( HR_{} HL_{}\) for some Rips-like path \(L\)._

Definition 5.1 is guided by the Void Lemma (5.2) and the Matching Lemma (5.3).

**Lemma 5.2** (Void Lemma).: _Suppose \(K(X)\) and \(0<_{,X}(K)\). Let \(L\) be any Rips-like path. If \([Y] HR(X,V)\) with birth \(b<_{,X}(K)\), then the deaths \(c\), \(d\),\(e\) of \([Y]\) in \(HR(X,V)\), \(HR(X,K)\), \(HL\), respectively, satisfy \(c d e\). Moreover, \(c<e\) implies that \(K\) has a void that disrupts the death of \([Y]\), and that void contains a ball of radius \(r\) satisfying \((-2)r 2(d-c) 2(e-c)\)._

In particular, if the class \([Y]\) has \(e=\) for the "inflexible" path \(L_{t}=P_{t,0}\), then \(K\) contains a void. The proof of Lemma 5.2 appears in the Supplementary Material and uses simple distance estimates.

**Lemma 5.3** (Matching Lemma).: _Suppose that all pairs in \(X\) have distinct lengths, and that \(X\) and \(K(X)\) are \((,,)\)-HM. Then there is a Rips-like path \(L\) for which each dot \((b,d)\) in the persistence diagram of \(L\) (or bar in the barcode) with \(b<<d\) corresponds via \(\)-LSM to a dot \((b,c)\) in the persistence diagram of \(R\) with \(b<<c\)._

Note: to check whether \(X\) and \(K\) are \((,,)\)-HM, it suffices to check a Rips-like path through \(P_{,0}\) and \(P_{,}\). At the other extreme, we get an overly-strict bound by checking the "inflexible" Rips-like path \(L_{t}=P_{t,0}\).

Because of the filtration stability of \(HP\) in Corollary 3.5, \((,,)\)-HM is also stable to perturbation in \(X\), as seen in Theorem 5.4.

**Theorem 5.4** (Stability of Homological Matching).: _Suppose that \(X\) and \(K\) are \((,,)\)-HM. If \(f:X}{{}}_{K}X^{}\) such that \(X^{}\) and \(K\) are \((-)\)-LSM, then \((X^{},K)\) are \((-,-,+)\)-homologically matched._

The proof of Lemma 5.3 is functorial, and Theorem 5.4 is a diagram chase. Both are given in the Supplemental Material.

**Definition 5.5**.: _Given \(X\) and \(K(X)\) let \(_{,X}(K)\) denote the minimum of those \(\) for which \(X,K\) are \((_{,X}(K),,)\)-HM._

## 6 Interpretation and Specification

Therefore, to answer our original purpose, we can assess whether a model \(K(X)\) is a "good geometric match" for \(X\) using the following procedure: (1) Regardless of \(K\), examine the persistence diagram of \(R(X,V)\) to identify dots with early birth and long persistence, which TDA theory tells us (e.g. the Homology Inference Theorem ) should indicate genuine geometric features of \(X\); (2) for a model \(K(X)\), compute \(_{,X}(K)\) and \(_{,X}(K)\); and (3) check whether those dots are born before \(_{,X}(K)\) and die after \(_{,X}\).

If we believe that the multiscale geometric patterns among the points in \(X\) is meaningful, then this procedure is essentially a specification for how a "good" model ought to behave.

This process can give a "bad geometric match" in various ways, for example: if step (1) does not show a clear collection of well-separated dots, then it is unlikely that \(X\) actually has computable geometry that can be captured with the Rips complex; if step (2) yields \(_{,X}(K)=0\), then \(K\) hasvoids between every pair of points in \(X\), possibly due to under-sampling or over-fitting, and should not be trusted for any interpolative purpose; or if step (3) shows that the quadrant to the upper-left of \((_{,X}(K),_{,X}(K))\) in the birth-death plane does not capture the desired dots of \(X\), then \(K\) is failing to capture specific high-persistence features of \(X\).

## 7 Computational Methods

In this section, we provide algorithms to estimate \(P_{,}\) in the practical case \(V=^{n}\). These algorithms are implemented in Python (and development continues) in our open-source software at https://gitlab.com/geomdata/topological-parallax

The Rips complex \(R\) can be computed efficiently, using . But, the set \(K\) is known only through the indicator function \(k\), and the Rips complex \(R(X,K)\) cannot be computed directly.

Consider \(x,y X\) joined by a \(V\)-geodesic (line segment) \(\) of length \(2_{V}(e)\), representing a Rips edge \(e R(X,V)_{_{V}(e)}\). We would like to estimate \(_{K}(e)\), thus giving \(e P_{,}\) for \(=_{K}(e)\) and \(=-_{V}(e)\). Some simple geometric observations allow us to estimate \(\) and \(\).

**Lemma 7.1**.: _If there exists \(p\) such that \(k(p)=0\), then \(e P_{,0}\)._

This is because in \(^{n}\), \(\) is the only path of minimal length.

**Algorithm 7.2** (Estimation of \(e P_{,0}\)).: _For each \(e R_{}\), sample points \(p\) along the corresponding line segment \(\). (One method of sampling is simply to check the barycenter.) Return True for \(e\) if and only if "\(k(p)=1 p\)"_

**Definition 7.3** (Transverse Disk).: _Let \(V=^{n}\). Given an edge \(e R\) and a radius \(r\), let \(D_{r}(e)\) denote the codimension-1 disk, oriented perpendicular to \(\), and centered at \(e\)'s barycenter \(\)._

Any continuous path from \(x\) to \(y\) that does _not_ intersect \(D_{r}(e)\), must have length exceeding \(2(e))^{2}+r^{2}}\). If \(k(B_{r}(e))=\{0\}\), then all \(K\)-paths avoid \(D_{r}(e)\), so all \(K\)-paths representing \(e\) must have length exceeding \(2(e))^{2}+r^{2}}\), giving the following Lemma.

**Lemma 7.4**.: _If \(K D_{r}(e)=\), then \((e))^{2}+r^{2}}<_{K}(e)\) and \((e)}r^{2}<_{K}(e)-_{V}(e)\)._

For the second inequality, recall the 2nd order Taylor approximation \(_{V}(e)+(e)}r^{2}(e))^{2}+r^{2}}\).

**Algorithm 7.5** (Bounding \(e P_{,}\)).: _For each \(e R(X,V)\), From \(r=0\), loop:_

1. _Evaluate_ \(k(p)\) _for samples_ \(p D_{r}(e)\)_._
2. _If_ \(k(p)=0\, p\)_, increment_ \(r\)_. Else, break._

_Return the lower bounds \((e))^{2}+r^{2}}\) and \(-_{V}(e)=\)._

These algorithms can be extended easily to sample radii along a sequence of points on the edge \(e\), thus providing an estimated \(K\)-path for \(e\).

### Back-Propagation

Recent work has shown that various topological properties can be expressed as loss functions that are compatible with back-propagation methods; for example . The method in  allows back-propagation for a piecewise-smooth loss function of the form \(((f))\), where \(f\) is the filtration function on a simplicial complex, and \((f)\) is the persistence diagram for the \(f\)-persistent homology of that complex. Lemma 5.3 allows us to interpret homological matching via persistence diagrams \((f(Y))\), where \(f(Y)=\{t\ :\ Y L_{t}\}\) for some Rips-like path \(L\).

The following function \(\) could be used to improve homological matching in this framework. Suppose that \(X\) and \(K\) are \(_{}\)-LSM. Consider the persistence diagrams \((R)\) and \((L)\), where \(L\) is some Rips-like path through \(P(X,K)\). By \(_{}\)-LSM, we know that \((R)\) and \((L)\) are identical to the lower-left of \((_{},_{})\). Choose a desired target value of \(_{}\). Now, we alter the filtration on \(L\) by the following \(\): for dots \((x,y)\) to the lower-left of \((L)\)we penalize Wasserstein distance to \((R)\). For dots \((x,y)(L)\) with \(x_{}<_{} y\), we penalize by the quantity \(\|x-x_{0}\|(-y)\), where \((x_{0},y_{0})\) is the best-match dot from \((R)\). This loss function should force \(L\) to express long-lived topological features similar to those of \((L)\), while minimizing the error introduced at scales below \(_{}\). As of this June 2023 publication, our code does not implement back-propagation to improve homological matching of models, but it is planned as upcoming work.

## 8 Example: Cyclo-Octane

The conformation space of cyclo-octane  is well-known to have novel topological structure. From physical principles, Martin et al.  show that real data sampled from the conformation space \(X\) can be reduced to lie in \(V=^{24}\), and furthermore \(X\) must lie near a 2-dimensional stratified space consisting of a sphere and a Klein bottle. As in Section 1, we suggest that a machine-learning model \(k:V\{0,1\}\) trained to recognize cyclo-octane should not be considered "good" or trustworthy unless \(K=\{x:k(x)=(1)\}\) also takes this geometric form at the appropriate scale. In this section, we demonstrate how topological parallax can support or reject the hypothesis that the geometry of a model \(K\) matches the geometry of \(X\).

Figure 4: Distributions of bond lengths for conformations of cyclo-octane. Left: Bond lengths for the original data set, distributed tightly around 1.52 Å, standard deviation of \(4.09 10^{-5}\) Å. Right: Bond lengths for all barycenters of edges \(e\) with \(2_{V}(e)<1.0\), as would be allowed by the model \(K_{2}\) described in Figure 3. The bond lengths vary from 1.3844 to 1.5201 Å with a mean of 1.5059 Å. Also, bond lengths for all barycenters of edges \(e\) with \(2_{V}(e)>3.0\), which are all allowed by the model \(K_{1}\). The bond lengths vary from 0.8537 to 1.5201 Å with a mean of 1.2876 Å. The extremely narrow blue lines in the Right plot overlays the distribution from the original data set on the Left.

Figure 3: (1) Valid cyclo-octane data under 3D Isomap, colored for viewability. (2) Persistence Diagram of valid data, showing two 2-cycles and one 1-cycle. (3) Persistence Diagram from Parallax of model \(K_{2}\), showing \(_{}\) in magenta and three homologically matched cycles moved to infinity. These diagrams use diameter, not radius, via gudhi.

Figure 3 visualizes the dataset \(X^{24}\) using a 3D Isomap projection.3 Following the workflow from Section 5, we compute the 0-, 1-, and 2-dimensional persistence diagrams of \(X^{24}\) using qudhi , and we observe that there are highly persistent features--one 1-cycle and two 2-cycles.4 The insight from  provides the meaning of these cycles, but the precise structure of the "data manifold" is typically not known _a priori_ in examples. What we know in any case is that we want any potential \(K\) to respect these cycles, because the geometry of \(K\) should match the geometry of \(X\), whatever it might be.

To understand the validity of a generated conformation, we compute its bond lengths-the distances between adjacent carbon atoms in the conformation. Bond length is an important physical property, together with bond angle, torsion angle, and energy . Given the rigid geometry assumptions of the cyclo-octane data , we expect individual conformations generated by trained models to have similar bond lengths, and therefore the distribution of bond lengths from valid conformations should be similar to that of generated conformations. See Figure 4 to compare the bond lengths of conformations from the dataset \(X\) versus those at barycenters for short edges and long edges in the Rips complex \(R\) of \(X\). Notably, interpolation across longer edges leads to invalid conformer geometries with too short of bond lengths and thus too sharp of bond angles for realistic molecules.

Suppose someone trains a standard neural network \(k_{1}\) to recognize this data. For this demonstration, we used a 3-layer fully connected network with a ReLU and a SoftMax, implemented in PyTorch. The network was trained to near-perfect accuracy within a few minutes on a two class problem of real data versus a nearby background. (Hyperparameters and training details are provided in the Supplementary Material.) Thus, the model \(k_{1}\) represents a common starting point that any data analyst might find encouraging. We apply Algorithm 7.2 to estimate which edges in \(R\) are accepted by \(k_{1}\), and discover \(2_{,X}(K_{1})=3.45\), which is the longest edge available. So, the Rips complex cannot distinguish \(K_{1}\) from the convex hull; the model does _not_ reflect the geometry of \(X\). This is particularly unfortunate in this case, because the model \(k_{1}\) might be used to generate many new conformations (any interpolation between two valid conformations), the vast majority of which will not be valid conformations. An alternative model \(k_{2}\) is offered, which is built from many local charts (details in Supplementary Material). The new model has \(2_{,X}(K_{2})=1.0\). Moreover, the most persistent cycles in \(X\) have infinite death as measured by the Rips-like path \(L_{t}=P_{t,0}\), so \(X\) and \(K_{2}\) are homologically matched with \(2_{} 1.25\). Therefore, we can claim that the geometry of \(K\) matches the geometry of \(X\) at these scales.

## Limitations

The parallax complex and associated objects are well-defined only for datasets and models that satisfy Definition 1.1. The algorithms in Section 7 assume that \(V=^{n}\) with the Euclidean metric, but could be adapted for other geodesic spaces. Computation of the Rips complex and its persistence diagram scale favorably with the intrinsic dimension of the dataset \(X\); however, the sampling methods discussed in Section 7 scale with the dimension of \(V\), which might invoke the Curse of Dimensionality. A deeper question is whether real-life datasets \(X\) of applied interest actually have enough sample density to exhibit a multiscale metric geometry, to which \(K\) can be compared. This question suggests a followup study to verify whether datasets and models with demonstrated real-world efficacy actually have computable and comparable geometries; that is, future work should use parallax to assess the profound epistemological question of whether metric geometry is a valid metaphor for understanding deep learning in real-life applications.