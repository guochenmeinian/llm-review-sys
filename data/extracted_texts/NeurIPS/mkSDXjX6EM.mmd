# FIND: A Function Description Benchmark for Evaluating Interpretability Methods

Sarah Schwettmann\({}^{1}\)1 Tamar Rott Shaham\({}^{1}\)1

Joanna Materzynska\({}^{1}\) Neil Chowdhury\({}^{1}\) Shuang Li\({}^{1}\)

Jacob Andreas\({}^{1}\) David Bau\({}^{2}\) Antonio Torralba\({}^{1}\)

\({}^{1}\)MIT CSAIL \({}^{2}\)Northeastern University

###### Abstract

Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces **find** (**F**unction **IN**terpretation and **D**escription), a benchmark suite for evaluating the building blocks of automated interpretability methods. **FIND** contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (a1a) generates function descriptions. We find that an a1a, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure--acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, **FIND** also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that **FIND** will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.

## 1 Introduction

The central task of interpretability research is to explain the functions that AI systems learn from data. Investigating these functions requires experimentation with trained models, using tools that incorporate varying degrees of human input. Hand-tooled approaches that rely on close manual inspection (Zeiler and Fergus, 2014; Zhou et al., 2014; Mahendran and Vedaldi, 2015; Olah et al., 2017, 2020; Elhage et al., 2021) or search for predefined phenomena (Wang et al., 2022; Nanda et al., 2022) are increasingly complemented by more automatic approaches that enable larger-scale analysis (Bau et al., 2020; Mu and Andreas, 2020; Hernandez et al., 2022; Oikarinen and Weng, 2023,Conmy et al., 2023]. Selection between approaches is highly application-dependent; to date, no single protocol answers all queries users might have about a system [Doshi-Velez and Kim, 2017, Vaughan and Wallach, 2020]. However, considering the growing body of evidence that LMs are capable of complex reasoning and problem-solving across domains [Wei et al., 2022, OpenAI, 2023, Yao et al., 2023, Lightman et al., 2023], we recognize their potential to become backbones of generalized agents for automated interpretability. Indeed, recent open-ended techniques have used pretrained LMs to describe the behavior of black-box text modules [Singh et al., 2023], including individual units inside other LMs [Bills et al., 2023]. As we enter a regime where model explanation is performed by models that are themselves uninterpretable, external evaluations of these techniques will be vital. At present, evaluation of network description procedures is limited and bespoke, in part because measuring performance on real-world problems is difficult when ground-truth descriptions of network structure are unknown [Doshi-Velez and Kim, 2017, Lipton, 2018, Miller, 2019, Hooker et al., 2019].

This paper introduces **Find** (**F**unction **IN**erpretation and **D**escription), a benchmark suite for evaluating the building blocks of automated interpretability methods on functions whose structure is known _a priori_ (see Figure 1). FIND is built from over 2000 procedurally generated function interpretation problems (_e.g._\(f:\) country \(\) capital, unless country is in South America, in which case \(f\) returns undefined). In each problem, candidate interpretation procedures (interpreters) are given black-box access to functions, optionally accompanied by metadata (_e.g._ the domain of the function). After evaluating these functions on chosen inputs (_e.g._ Japan, Mexico, Peru), interpreters must eventually return a structured description that can be evaluated against other descriptions or used to simulate function behavior.

We also introduce a new interpretation method that uses Automated Interpretability Agents (aias) to interactively probe functions and explain their behavior. We formulate our approach to the interpretation problem as an implementation of the scientific method, wherein the goal of the aias to describe the process underlying observed input-output relations. In contrast to existing full-text explanation systems that apply automatic captioning methods (e.g. [Hernandez et al., 2022, Bills et al., 2023]) to pre-selected input-output pairs, the aias generates the data itself, by running the function on inputs it selects, observing the outputs, and updating hypotheses until it can describe the function to human end-users. We evaluate both aias and existing, non-interactive automated interpretability methods on **Find**. While exhibiting sophisticated experimentation strategies and out-performing non-interactive methods, the top-performing aias nonetheless fails to adequately describe \(48\)% of functions. **Find** thus shows that, in spite of promising recent results, methods based on current LMs alone are unlikely to robustly automate even high-level interpretability tasks.

**Find** focuses on the black-box function description paradigm because black-box description appears as a subroutine (or is the sole operation) implemented by almost every existing automated interpretation method, spanning label retrieval, program synthesis, and learning-based approaches. The

Figure 1: **The find benchmark.****Find** is constructed procedurally: atomic functions are defined across domains including elementary numeric operations (purple), string operations (green), and synthetic neural modules that compute semantic similarity to reference entities (yellow) and implement real-world factual associations (blue). Complexity is introduced through composition, bias, approximation and noise. We provide an LM-based interpretation baseline that compares text and code interpretations to ground-truth function implementations.

[MISSING_PAGE_FAIL:3]

inputs. One example is Nanda et al. (2022), where an algorithm using trigonometric manipulations was found to perform modular arithmetic in a toy transformer model. We sample 1000 numeric functions from the find API for the benchmark dataset. \(85\%\) are parameterized atomic functions under noiseless and noisy conditions, and \(15\%\) are compositions.

**Atomic functions** are defined as explicit functions found in many mathematical and scientific computing libraries such as Python (Van Rossum, 2020), SciPy (Virtanen et al., 2020), and NumPY (Harris et al., 2020), as well as standard neural activation functions such as ReLU (Fukushima, 1975; Nair and Hinton, 2010). Table 1 shows examples of atomic functions defined in find. For each function in the set of atomic functions \(\), we sample native parameters, scaling factor \(a\), and bias \(b\). Parameters and sampling procedure details are provided in the Appendix and the find API.

**Composition** of atomic functions \(f(x)\) and \(g(x)\) applies an operator sampled from \(C=\{,+\}\), where \(f g=f(x) g(x)\) or \(f(x)+g(x)\). Composed functions are sampled from a subset \(_{}\) of atomic functions \(\) to limit final complexity. \(f(x),g(x)_{}\) are described in the Appendix.

**Observation noise** added to \(f(x)\) tests how well the interpreter is able to estimate an underlying function in the presence of additive noise, and whether it is able to distinguish between different types of noise. \(15\%\) of functions \(f(x)\) in the find benchmark are sampled with additive noise \(f(x)+X\), where \(X\) follows either a Normal, Uniform, or Poisson distribution.

**Domain corruption** replaces function values locally with random values. This is done either inside or outside of a sampled interval \(I\) of range \(\{[a,b]\,[a,]\,[-,a]\}\), where \(a_{[-100,100]}\). The length of a finite interval is sampled from \(_{}\). Corruption is defined as noise \(X(,0.01)\) replacing the function values on \(I\). We choose \(\) as the mean value of \(f(x)\) on its domain. The interpreter is prompted to discover the corrupted interval, if any, and to return \(a\) and \(b\). \(15\%\) of the functions \(f(x)\) in the benchmark dataset are corrupted on part of their domain.

**Approximation** of atomic functions is implemented using a two-layer neural network (MLP) with a ReLU non-linearity. For \(15\%\) of the functions in the dataset, we train an MLP for 10k epochs on 10k points uniformly sampled on its domain bounded by \((-100,100)\). Trained MLPs are provided in the benchmark dataset and loaded by the corresponding function during interpretation 2.

### Functions on strings

We build on a long history of using toy problems on strings (Hofstadter et al., 1995; Hofstadter, 1995; Mitchell, 2021) and simple visual matrices (Lowett and Forbus, 2017; Wang and Su, 2015; Carpenter et al., 1990; Chollet, 2019) to test the ability of a system to reverse-engineer underlying symbolic operations. As this release of find is designed to evaluate language-based interpretability systems, we focus on functions with text inputs and procedurally generate a set of functions on strings with different levels of complexity. The benchmark dataset contains 1000 string functions sampled from the find API, representing both atomic functions (30%) and compositions (70%).

**Atomic functions** include common string manipulation operations such as concatenate, replace, and reverse, and implementations of copycat problems from Hofstadter et al. (1995), such as shift_last (\(abc abd\)). Example atomic string operations are shown in Table 1, and the full set can be found in the find API.

**Composition** of atomic functions \(f(x)\) and \(g(x)\) is defined as \((g f)(x)=g(f(x))\). The find API supports compositions where \((g f)(x)\) is well-defined (i.e. function pairs where \((g f)(x)=x\) are excluded). Example composition functions are shown in Table 1.

### Synthetic neural modules

find includes a set of synthetic neural modules that perform word-level tasks built from Wikidata properties (Vrandecic and Krotzsch, 2014). We construct two types of text modules: functions involving lexical semantics and functions involving factual relations. find implements synthetic neural modules on text inputs using a single pretrained language model (Vicuna-13B) instructed to apply different rules to inputs in response to queries (See Figure 2). Vicuna (Chiang et al., 2023) is a fine-tuned LLaMA model licensed under an Apache License 2.0 and open-sourced for non commercial use. Each function in the dataset is built to interact with Vicuna to return the relevant output to the interpreter, which tests the function on different inputs to recover the underlying rule. We find that Vicuna reliably implements functions expressed in prompts (reliability scores are provided for each function type; see Appendix for full evaluation).

**Type 1: Entities**

**Atomic entity functions**\(f_{e}(x)\) compare input text to reference concepts (_entities_) drawn from metaclasses of Wikidata properties related to a particular concept or subject area (e.g. _related to food and eating_) and return a value indicating how associated an input is with the entity. Table 1 shows example entities, and the complete list of 140 atomic entities included in **find** is provided in the Appendix. The task of an interpreter evaluating an entity function is to recover and describe the underlying concept, or, what all of the inputs that produced high output values had in common. For example, if the entity is _plants and botany_, \(f_{e}()\) and \(f_{e}()\) should return high values, while \(f_{e}()\) should return a low value. This task is relevant to lines of work that automatically summarize maximally activating exemplar sets to characterize neuron function (Bau et al., 2020; Mu and Andreas, 2020; Hernandez et al., 2022; Bills et al., 2023; Singh et al., 2023), generalized to a setting where the interpreter must also produce the data.

To implement each function, we instruct Vicuna to synthesize a binary response to interpreter queries, corresponding to whether an input word is associated (return 1) or unassociated (return 0) with the reference entity (see the Appendix for Vicuna prompts). The function output \(s=f_{e}(x)\) is then a continuous scalar value representing Vicuna's internal probability of 1 being the response token, between a choice of 0 and 1. Specifically, \(s=p_{1}/(p_{0}+p_{1})\), where \(p_{i}=e^{_{i}}\) and \(_{i}\) represents Vicuna's output logit for token \(i\). To validate that Vicuna can reliably identify associations between inputs and reference entities, we collect a human-labeled set of concepts \(_{j}\) associated with each entity \(e_{j}\) in the dataset, and compute \(=f_{e_{j}}(_{j})\). The mean value of \(\) across all \(140\) entities in **find**, for \(10\) human-annotated concepts per entity, is \(0.85\). We compute the same score for \(10\) distractor concepts per entity, sampled from the list of human annotations of other entities. The mean score for the distractors is \(0.08\). See the Appendix for full experiment details.

**Composed entity functions** mimic the behavior of neurons inside deep networks that are selective for multiple concepts (Fong and Vedaldi, 2018; Olah et al., 2020; Mu and Andreas, 2020). We construct compositions of entity functions by sampling two entities from the dataset and instructing Vicuna to return 1 if the input value to the function is associated with either entity. 60 composed entity functions are included in the **find** benchmark.

**Type 2: Relations**

More complex functions inside language models learn factual associations (Meng et al., 2022). To mimic the behavior of these modules, we construct a set of relation functions that map inputs to outputs according to a real-world factual association (for instance, river \(\) length or gemstone \(\) color). Atomic relations are drawn from Wikidata properties included in the Wikidata dump provided by Sorokin and Gurevych (2018). Table 1 shows example relation functions; a full list is provided in the Appendix. Again we use Vicuna to implement a black box function that applies the rule in the relation to interpreter inputs (we verify that Vicuna returns factually correct answers, see Appendix). Figure 2 shows the prompt template for Vicuna and an example relation function.

**Relation functions with bias** include corruption to part of the function domain. **find** tests whether an interpretability model can uncover which parts of the function domain have been corrupted. Corrupted relation functions return undefined for a small region of the function domain; for example, we corrupt the country \(\) capital relation on the subdomain Asia by prompting Vicuna to return undefined for inputs in the subdomain.

Figure 2: **Synthetic neural module implementation.** Vicuna acts as a backbone that generates the outputs of neural modules. Each function provides instructions to Vicuna that mimic the behavior of either a single neuron (”return an association score with a predefined concept”) or a more complex module (“map inputs to outputs according to a a predefined mapping”). The interpreter then interacts with Vicuna as a black-box function.

Evaluation protocol

The **find** protocol evaluates interpreters' natural language descriptions of functions in all categories. In categories where domain corruption is introduced (numeric functions and factual relations), interpreters are evaluated on their ability to return a description of the domain of the function, indicating any subdomain where the function behaves differently. In the mathematical reasoning and strings categories, interpreters are evaluated on their ability to output code that approximates the function. Section 4 provides details on the interpretation procedures we evaluate and how they are instructed to engage with functions.

### Evaluation metrics

To evaluate the accuracy of function descriptions produced by an interpreter, we use success indicators for individual function interpretations and calculate average success rates across all functions. For functions where interpreters write code approximating the function (numeric and string functions), we score the accuracy of the interpretation by running the interpreter's code on a representative test set and comparing the result to execution of the ground-truth function. To evaluate language descriptions of functions, we judge how well the interpreter's description agrees with ground-truth function behavior (for string functions and synthetic neural modules). Below we describe success indicators for each function category in more detail.

**Evaluating code-based interpretations.** Running interpretation on the **find** benchmark produces descriptions of numeric and string functions in code as well as natural language. We measure explanation accuracy by comparing performance of the interpreter's code to the ground-truth function implementation on a test set. For numeric functions, we compute a normalized mean-squared error \([(f(x)-g(x))^{2}]/[f(x)^{2}]\) for \([-128 x 128]\) between the ground truth **find** function \(f\) and the interpreter's implementation \(g\), and regard a successful estimation as one with Normalized MSE (NMSE) \(<0.1\). For string functions, we run an exact-matching binary test (\(f\)(<string>\({}_{i}\))=\(g\)(<string>\({}_{i}\))) on a set of 10 test inputs per function.

**Evaluating language-based interpretations.** We define a "unit testing" protocol where an LM evaluator selects which of three input-output (I-O) pairs corresponds to a language description of a function. To judge the interpreter's accuracy, we provide the estimated function description (_e.g._\(f:\)) to the evaluator, as well as three example I-O pairs: one execution of the ground-truth function (_e.g._\(\)) and two randomly sampled distractors (I-O pairs for other **find** functions of the same type; _e.g._\(\), \(\)). The evaluator selects which I-O pair matches the functionality of the description from the interpreter. If the language description is accurate, the evaluator should select the ground-truth I-O pair as the best match. We run this procedure on language descriptions of string functions and synthetic neural modules for a test set of ten different triplets per function. Test sets are constructed to reveal representative behavior of each function using inputs inside and outside of the function domain. For relations corrupted on part of their domain (_e.g._\(f:\), unless the country is in South America), the test set includes two ground truth examples from the corrupted subdomain (_e.g._\(\), \(\)). For entity functions that compute similarity to a reference concept (_e.g._\(\)), we provide the evaluator with only input concepts instead of I-O pairs (_e.g._\(\), \(\), \(\)), and ask which concept the function described by the interpreter is selective for. As test cases can be designed to isolate specific function behaviors, we find the unit testing protocol to be more sensitive to small differences in function descriptions (_e.g._ "transportation" vs "road transportation") than other description-matching methods, such as having an LM directly grade the agreement between descriptions (see the Appendix for more details).

We finetune Vicuna-13b (vicuna-evaluator) to perform the unit testing task and select representative samples matching descriptions of functions in the **find** dataset. LM-judges have been shown to be scalable and accurate surrogates for human judgments, which are otherwise expensive to obtain (Zheng et al., 2023). While proprietary models such as GPT-4 demonstrate strong agreement with humans, using such models to compare interpreter performance on a benchmark task incurs prohibitive costs associated with API access and poses reproducibility challenges as model versions are deprecated. We find that \(\) matches _ground-truth_ function descriptions to representative inputs and outputs more accurately than GPT-4, GPT-3.5, or pretrained Vicuna-13b (see Appendix for evaluation). Furthermore, \(\) makes judgments that are highly correlated with those of human subjects performing the same task (see Appendix for experiment details). The fine-tuned \(\) checkpoint and training dataset can be downloaded from the \(\) repository. Training details are provided in the Appendix.

**Extensions: Targeted evaluation.** For users of the \(\) benchmark that produce function interpretations in a structured format, this benchmark enables other evaluations targeted at specific end-use cases for interpretability tools. For example, researchers may use \(\) to explicitly evaluate whether \(\) can pick out the portion of the domain that is corrupted, whether they can identify components of composed functions, or whether they identified the noise model.

## 4 Automated Interpretability Methods

\(\) can be used to evaluate any \(\) system that has the ability to execute Python scripts autonomously at the command line. As a first demonstration, we evaluate several approaches inspired by recent work  that use pre-trained language models to perform interpretation. We run experiments using the OpenAI Python API. A custom plugin equips the \(\) model with the command \(()\) that it can use to call functions on its own selection of inputs. Scripts for reproducing these baselines and complete interpretation dialogues are available in the \(\) code repository.

We evaluate three different interpretation methods. (i) _Non-interactive_: interpretability tasks often involve descriptions of precomputed exemplars of function behavior. In this setting, the \(\) is equipped with a fixed set of inputs to use for probing the functions, and is prompted to produce a description based only on function outputs for these exemplars, mirroring the non-interactive description paradigm used by \(\) and recent \(\)-type LM analysis approaches . (ii) _Automated Interpretability Agents_: LM-based \(\) are prompted to interact with the functions in \(\). The \(\) serves as an agent and runs \(()\) on inputs it selects. In the base setting, the agents' search process is not initialized with pre-selected inputs; rather, the \(\) forms hypotheses by itself, generates data to test its current hypothesis, and interactively updates it based on observations of function outputs. Figure 3 shows complete interactions between a GPT-4-based \(\) and example functions in two categories. At the beginning of the interaction, the agent is prompted to run experiments on the functions to produce data that it can use to explain function behavior. Prompts for two categories of functions are reproduced below. (iii) We also evaluate a combination of both approaches (\(\) + \(\)). In this setting, the interpretability agent is prompted to begin interpretation with a provided set of exemplars, and then interactively continues the interpretation procedure.

All interpretation methods use off-the-shelf LMs (GPT-4 , GPT-3.5 (\(\)) , and Llama-2 (\(\), \(\)) ) as \(\), and operate without a human in the loop. \(\)-based interactions show structured interpretation procedures, even when the final explanation is not fully accurate (see the predicted numeric function in Figure 3). The \(\) makes and tests informed hypotheses about the functions: for example, testing densely spaced points in a specific interval to reveal a "separating point" in the behavior of the numeric function, and testing different forms of transportation to determine whether a synthetic neuron is selective for land, sea, or air travel (Figure 3). Once the \(\) has gathered sufficient data and produces a final description, the run stops automatically. The Appendix further describes experimental settings.

**Prompt for numeric functions:**

Hi, your job is to interpret a function f(x) that is implemented in the \(\) script. f(x) takes numerical inputs. All you can do is call f(x) on a value of x by running \(()\) value). Try to describe the function implemented by f(x) by running experiments on it. You can call the function on multiple inputs at a time by running \(()\) value|\(\) value|\(\)...). You can run the function multiple times on the same input. We encourage testing a large range of values before writing a description. You can test values between \(\) and \(\). The output could be noisy and your goal is to find a good simple description of f(x) that explains most of the function behavior. f(x) may combine multiple different operations. There may be an interval inside the domain where the function behaves differently. If that is the case, describe that interval. Your description of the function should have three parts. First,

[DEGSUPITION]: Describe that the function is doing in language.

[DORMAN]: What is the domain of the function? Is there an interval where the function displays unexpected behavior? What are the numeric bounds of that interval? What is going on there?

[CODE]: Write Python code that approximates the behavior of the function.

[MISSING_PAGE_FAIL:8]

## 5 Results

We evaluate the aia method as well as the non-interactive baselines with different off-the-shelf LMs. Results are summarized in Table 2 and example function descriptions are shown in Figure 4. Additional example interpretations of functions in all categories are provided in the Appendix.

**GPT-4 is a stronger interpretability agent than GPT-3.5 and Llama-2.** Success rates for all function categories are reported in Table 2 (aia columns). The GPT-4 interpretability agent achieves universally higher success rates than GPT-3.5 and Llama-2 (which often score at chance value). In fact, we find that Llama-2 often invents the output of the function without executing it (see examples in Appendix). This is also reflected in the mean length of interpretation dialogues, which are \(1.4,1.4,2.1\), and \(4.1\) for Llama-2-13b-chat, Llama-2-70b-chat, GPT-3.5 and GPT-4 respectively (we count the number of interpreter-function interactions). We additionally observe that interpretations of string functions receive significantly higher scores using the unit testing protocol compared to string-matching outputs of estimated code. As unit testing selects for representative examples of function behavior and not exact string-matches, the procedure is more forgiving of less specific descriptions, or descriptions with minor inaccuracies (see Appendix for additional discussion of unit testing limitations).

**Interactive vs. non-interactive.** In addition to aias, we evaluate two other interpretation methods: milan where the interpreter produces a description based only on function outputs for a given set of exemplars, and a combination of aia + milan, where the same set of exemplars is used to initialize the interpretation session, and the agent is subsequently permitted to perform additional experimentation. For both settings, we sample the exemplars (two related to the function, eight distractors) from a human-constructed list of inputs associated with each function (see Appendix for details). In the non-interactive milan setting, we observe a small improvement in performance over uninitialized aias; GPT-4 interpretation performance improves from \(0.56\) to \(0.89\) on entity functions, and decreases slightly on relations, from \(.78\) to \(.74\). Initializing aias with milan exemplars and allowing additional experimentation dramatically boosts the performance of GPT-4 and GPT-3.5 agents (and also slightly improves Llama-2 performance), as shown in Table 2 and Figure 5 (see _init._). Notably, when initialized with exemplars, GPT-3.5 exhibits performance interpreting entity functions comparable to GPT-4. These results suggest that off-the

  &  \\   &  &  &  &  &  \\   & aIA & aIA & aIA & aIA & milan & aIA +milan & aIA & milan & aIA +milan \\  Llama-2-13b-chat & 0 & 0 & 0.33 & 0.34 & 0.54 & 0.58 & 0.46 & 0.45 & 0.42 \\ Llama-2-70b-chat & 0.01 & 0.01 & 0.33 & 0.34 & 0.61 & 0.62 & 0.47 & 0.44 & 0.46 \\ GPT-3.5 & 0.12 & 0.13 & 0.66 & 0.39 & 0.81 & 0.88 & 0.37 & 0.64 & 0.68 \\ GPT-4 & **0.33** & **0.23** & **0.82** & **0.56** & **0.89** & **0.89** & **0.78** & **0.74** & **0.92** \\  

Table 2: **Interpretation success rates.** For each function type we report the successful estimation rate (**higher better**) based on different indicators, and with different experimental settings (_e.g._ initialization with exemplars).

Figure 4: **AIA (GPT-4) interpretations.** Examples from all **FIND** categories, with evaluation scores (NMSE and unit test) marked in red if below the success threshold (NMSE\(>0.1\), unit test\(<0.33\)) and green otherwise.

shelf LM agents are limited by breadth of search. Indeed, LMs tend to start sampling with simple words (_e.g._ apple, dog, car) which do not reveal function behavior for highly specific reference entities (_e.g._ _The New York Times, arachnids and arachnology_). We view exemplar computation as one of many "tools" that an interpretability agent could use, and hope that this benchmark will drive exploration of additional tools (_e.g._ example synthesis) as part of automated interpretation methods. Procedures that combine initialization and interactive experimentation could improve the efficiency of existing labeling approaches that use large fixed datasets (Bau et al., 2017) to precompute maximally activating inputs, and potentially also surface novel behaviors not captured in predefined sets of exemplars.

Which functions can LMs recover?Figure 4 shows examples of aia interpretations that successfully explain the behavior of some functions (_e.g._ cases (b),(d)), but fail to fully characterize more complex functions (_e.g._ cases (a),(c)), also reflected in some per-subcategory success scores (Figure 5). This is a limitation of using off-the-shelf LMs as agents: they may miss small corruptions to part of the domain (Figure 3(a)), which in real-world interpretability settings could stem from bias in the training set. LMs could be outfitted specifically for interpretability with additional tools (_e.g._ for sampling) and further improved by fine-tuning.

## 6 Related work

Explanation evaluation methods have previously benchmarked salience methods according to their ability to recover ground-truth segmentations (Zhang et al., 2018; Selvaraju et al., 2017; Fong and Vedaldi, 2017; Yang and Kim, 2019), or identify inputs that causally affect outputs (Zeiler and Fergus, 2014; Petsiuk et al., 2018; Wagner et al., 2019; DeYoung et al., 2020). Explanation benchmarks have also evaluated correlation with system performance (Adebayo et al., 2018; Casper et al., 2023) or human understanding of decisions (Kim et al., 2022). Our benchmark differs because we evaluate global explanations of black box functions instead of evaluating local explanations of decisions.

Model interpretability metrics have quantified the degree of model interpretability, _i.e._ by measuring how closely deep network features match a human-labeled concept (Bau et al., 2017; Kim et al., 2018; Goh et al., 2021; Wu et al., 2021; Burgess et al., 2018; Mu and Andreas, 2020; Geva et al., 2021). While these methods can measure disentanglement in model representations, they are only as good as their ability to identify interpretable features. We tackle this problem by providing a benchmark for interpretation methods themselves, rather than the models that they explain.

Full-text explanation systems provide natural-language explanations for black box systems or their features (Hendricks et al., 2016; Camburu et al., 2018; Ehsan et al., 2018; Kumar and Talukdar, 2020; Hernandez et al., 2022). Recent efforts exploit the capabilities of LMs to explain data directly, but these works utilize only tiny evaluation benchmarks, including 19 synthetic neuron puzzles in Bills et al. (2023) and 54 ground-truth module topics in Singh et al. (2023). Motivated by the promise of this LM-driven approach and the need to quantify performance, our work provides a comprehensive benchmark of full-text black-box explanation systems.

## 7 Conclusion

We introduce **Find**, a new benchmark for evaluating automated interpretability methods. Baseline results show early evidence that agents built from advanced LMs can construct hypotheses and experiments to validate them, supporting the suitability of LMs as general-purpose interpretability backbones. However, we find many functions that LM agents cannot sufficiently explain, suggesting augmentation with additional tools will be necessary for robust automation of interpretability tasks.

Figure 5: **AIA interpretation scores by subcategory (with GPT-4).** Complex functions are usually more difficult to interpret than atomic functions. String functions are evaluated using the string-matching indicator.