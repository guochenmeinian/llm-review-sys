# DeR-VLA: Dynamic Inference of Multimodal

Large Language Models for Efficient Robot Execution

Yang Yue\({}^{*}\)\({}^{1}\)   Yulin Wang\({}^{*}\)\({}^{1}\)   Bingyi Kang\({}^{2}\)   Yizeng Han\({}^{1}\)   Shenzhi Wang\({}^{1}\)

Shiji Song\({}^{1}\)   Jiashi Feng\({}^{2}\)   Gao Huang\({}^{1}\)

\({}^{1}\) Department of Automation, BNRist, Tsinghua University  \({}^{2}\) ByteDance

{le-y22, wang-yl19}@mails.tsinghua.edu.cn  {gaohuang}@tsinghua.edu.cn

Equal contribution. Corresponding author.

###### Abstract

Multimodal Large Language Models (MLLMs) have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we seek to address this challenge by leveraging an intriguing observation: relatively easier situations make up the bulk of the procedure of controlling robots to fulfill diverse tasks, and they generally require far smaller models to obtain the correct robotic actions. Motivated by this observation, we propose a _Dynamic **E**arly-**E**xit Framework for **R**obotic **V**ision-**L**anguage-**A**ction Model_ (DeeR-VLA, or simply DeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (_i.e._, power consumption), as well as peak computational consumption (_i.e._, latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. Moreover, we design a tailored training method for integrating temporal information on top of such multi-exit architectures to predict actions reasonably. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.

## 1 Introduction

The recent astonishing progress in multimodal large language models (MLLMs) has unveiled their remarkable potential of extracting, aligning, and integrating the representations from complicated language and visual data . These advances have spurred the vision of a generalist robot, _i.e._, an embodied agent equipped with vision-language comprehension and problem-solving capabilities,profficient in interacting with humans and the physical world to flexibly execute complex manipulation tasks [5; 6]. An encouraging preliminary work, RT-2 [7; 8], has demonstrated the feasibility of adopting MLLMs to control robots in end-to-end. This not only yields performant robotic policies, but also exhibits some emergent abilities obtained from large models, such as understanding novel commands, generalizing to objects never seen before, and reasoning.

Despite these favorable findings, the high demands of MLLMs on hardware are usually an important bottleneck that inhibits the establishment of generalist robots with advanced MLLMs. Typically, robotic applications are based on resource-hungry platforms with limited computational capability, memory space, and battery capacity, yet usually necessitate acting in real-time and performing low-latency interactions with humans or the physical environments. However, every time MLLMs are activated to obtain a robotic action involves utilizing billions of parameters to accomplish a computationally intensive inference process. Such inefficiencies may lead to considerable GPU memory requirements, tremendous power consumption, as well as nontrivial time delays in controlling robots. These weaknesses make it challenging to deploy MLLMs on real embodied robotic systems.

To alleviate this problem, we propose an approach based on dynamic neural networks. Our work is inspired by an intriguing observation: in the procedure of controlling a robot to fulfill various tasks, relatively 'easier' circumstances make up the bulk of all the situations confronted by the robot. When encountered with these 'easier' situations, an embodied agent can actually acquire proper robotic actions with a much smaller model compared to the full MLLMs. Or more precisely, only the remaining small number of'more difficult' circumstances necessitate the full capacity of large MLLMs. This phenomenon can be demonstrated using the representative example in Table 1, where we train RoboFlamingo  with varying model sizes, and report the FLOPs and task successful rate in the Calvin Long-Horizon Multi-Task Language Control (LH-MTLC) challenge . Adopting the officially recommended 24-layer Flamingo only correctly finishes 3.2% (78.9% v.s. 75.7%) more tasks compared to using 6 layers, but it increases the computational cost by 4x. In other words, computational resources are wasted on activating larger models in many easy circumstances for which smaller models are sufficient.

Motivated by this observation, we propose a _Dynamic Early-Exit for Robotic MLLM_ (DeeR) framework, seeking to automatically configure the size of MLLMs conditioned on each situation confronted by an embodied agent. Specifically, we introduce a MLLM architecture featuring multiple intermediate exits, with which a correct robotic action can be immediately obtained once a proper size of the model has been activated, eliminating further redundant computation. Additionally, we develop novel algorithms that are able to establish early-termination criteria for DeeR conditioned on arbitrarily specified demands of average computational cost (_i.e._, power consumption), and peak computational cost (_i.e._, latency) or GPU memory overhead. At inference, DeeR can adaptively activate smaller models for less complex situations and larger models for more challenging cases. Consequently, computation is unevenly allocated among situations, yielding a considerable improvement in efficiency. Besides, the computational cost of DeeR can be adjusted online by simply modifying the termination criterion on top of a fixed main model, making it appealing in flexibility. Moreover, we design a tailored training method for DeeR, enabling integrating temporal information on top of such multi-exit architectures to control robots reasonably.

The performance of DeeR is evaluated on 3 CALVIN LH-MTLC challenges with RoboFlamingo . Extensive robot experiments show that DeeR reduces the LLM computational cost by 5.2-6.5x without sacrificing performance. Surprisingly, even when considering GPU memory limitations in the termination criterion, DeeR remains competitive with other SOTA methods while only utilizing 2GB memory for the activated LLM. Consequently, DeeR demonstrates the potential to enable a wider range of users to operate their own robots equipped with MLLMs on resource-limited platforms.

   \# LLM layers & 24 & 12 & 6 \\  GFLOPs/action (LLM) & 31.2 & 15.6 & 7.8 \\ Task success rate \% & 78.9 & 78.0 & 75.7 \\   

Table 1: Computation cost v.s. task successful rate1(RoboFlamingo++) on CALVIN LH-MTLC challenge D\(\)D. Notably, we mainly focus on the core component, LLM, of the MLLM, which comprises the majority of parameters. We vary the size of the LLM to examine its impact. For a focused comparison, we report the FLOPs (and GPU memory usage) of the LLM in our paper, unless otherwise specified.

Related Works

**LLM/MLLM for language-conditioned robot control.** A range of studies have explored the use of natural language to instruct robots in performing tasks [11; 12; 13; 14; 15; 16]. Methods such as SayCan and PaLM-E [17; 18; 19] utilize LLMs as high-level planners to translate commands into individual primitives that are then executed by low-level controllers. However, these controllers are usually domain-specific small models and lack the semantic understanding and reasoning capabilities that LLMs/MLLMs possess. To fully leverage LLM's astonishing capabilities, RT-2 [7; 9] proposes an end-to-end MLLM that directly generates low-level robotic actions via co-finetuning on robotic data and image-language data. It exhibits some emergent abilities obtained from large MLLMs, such as generalizing to instructions and objects never seen before, and reasoning. Further, RoboFlamingo  proposes to adapt existing MLLMs to a low-level robotic policy through straightforward fine-tuning on robotics data. While representative projects like RT-2 and RoboFlamingo have showcased the promising potential in enabling generalist robots, the use of MLLMs for such low-level control is computationally intensive. This is because each robotic action requires processing through all layers of an MLLM, whose inefficiencies often yield significant bottlenecks in practical robotic applications.

**Efficient LLM.** Considerable strides have been made to improve the inference efficiency of LLMs [20; 21; 22]. Research in this domain typically falls into three categories: efficient structural design[23; 24; 25; 26; 27; 28; 29], model compression [30; 31; 32; 33; 34; 35; 36], and dynamic networks [37; 38; 39; 40]. Our research focuses on the third category, dynamic networks, which optimize computational resources based on input data to reduce unnecessary computation. A key strategy within this category is _early exiting_, discussed further below.

**Early exiting** is an innovative method for dynamically halting forward propagation at a certain layer based on intermediate predictions. This technique has been well explored in both computer vision [41; 42; 43; 44; 45; 46], language processing [47; 48; 49; 50; 51; 52; 53], and multimodality [54; 55]. A challenge in implementing early-exiting models is devising an appropriate metric to determine when to issue an intermediate prediction. Traditionally, in tasks such as image classification, metrics such as Softmax confidence  or entropy  are utilized. Alternative approaches include training learning-based exit strategies with pseudo labels [56; 48; 57; 58]. Recent advancements [38; 39] have expanded early exiting to encompass the next-token prediction of LLMs focused on, treating it as a classification task. Diverging from these methods, our work adapts an MLLM to generate action outputs for sequential decision-making. We introduce a novel dynamic paradigm that integrates temporal information to predict action. Further, we devise a novel early-exiting metric based on action consistency, necessary because typical metrics like confidence and entropy are infeasible without direct Softmax outputs. Lastly, we develop an algorithm to derive termination criteria through online environmental interaction--a strategy not explored in prior early-exiting research in vision or NLP.

## 3 Dynamic Early-Exit for Robotic MLLM

The strong task instruction understanding and visual grounding capabilities of MLLMs [5; 4] have exhibited great promise for language-instructed multitask robotic manipulation [7; 8; 9]. However, existing works tend to be computationally intensive since the actions of the robot are obtained by inferring all layers of an MLLM. At each timestep, this process may activate billions of parameters, necessitating substantial computation and memory, and yielding a significant latency and power consumption. These inefficiencies are usually important bottlenecks for practical robotic applications.

**Overview.** We seek to address this challenge by leveraging an intriguing observation: relatively 'easier' situations make up the bulk of the procedure of controlling robots to fulfill various tasks, and they generally require far smaller models to obtain the correct robotic actions (as shown in Table 1). Inspired by this phenomenon, we propose _Dynamic Early-Exit for Robotic MLLM_ (DeeR), aiming to improve the computational efficiency of the robotic MLLM systems by dynamically adopting a proper size of MLLM for each situation. In specific, we first develop a novel MLLM architecture with multiple intermediate exits (Section 3.1). Consequently, given an input, one can immediately acquire a proper robotic action once a sufficient number of model parameters have been activated, avoiding further redundant computation. Then, Section 3.2 establishes early-termination criteria for DeeR conditioned on arbitrarily specified demands of average computational cost, and peak computational cost or GPU memory overhead. Finally, Section 3.3 proposes a tailored training algorithm for our model, demonstrating how to integrate temporal information on top of this dynamic network and reasonably predict robotic actions.

### Multi-exit Architecture for Robot

We first introduce an MLLM architecture featuring multiple intermediate exits, enabling the dynamic adaptation of the MLLM's size to suit the varying situations encountered by robots.

**Basic architecture.** Tasked with a language instruction \(l\), a robot receives an observation \(o_{t}\) from sensors (_e.g._, RGB image from the camera in our paper) at timestep \(t\) and predicts an action \(a_{t}^{*}\) to execute. To correctly predict the action, the robot should not only sufficiently understand the language instructions, but also extract task-relevant information from the images . Built upon an existing work , we achieve this by employing a pretrained MLLM, _i.e._, Flamingo , to process and integrate both vision and language inputs, thus obtaining fused multimodal features for decision-making.

Our basic MLLM mainly consists of a vision encoder \(E_{I}\) and a LLM. The vision encoder \(E_{I}\) comprises a Vision Transformer (ViT)  paired with a Perceiver Resampler , which encodes an input image \(o_{t}\) into a sequence of informative tokens. For multimodal fusion, an LLM is established on top of the visual representations generated by the vision encoder \(E_{I}\). More specifically, we interleave the self-attention blocks of a pretrained, frozen text-only LLM with newly introduced, learnable cross-attention blocks that cross-attend to the visual tokens. This configuration allows the original MLLM to function as an effective multimodal feature extractor \(F_{}\), formalized as follows:

\[x_{t}=F_{}(l,E_{I}(o_{t})),\] (1)

where \(l\) denotes the input language instruction tokens with a length \(L\), and the output \(x_{t}=(x_{t,1},x_{t,2},,x_{t,L})\) represents the hidden state sequence from the last layer of our MLLM at timestep \(t\). Notably, despite the effectiveness of LLMs in multimodal feature integration, their reliance on billions of parameters results in substantial computational costs and memory usage.

**Visual language model with intermediate exits.** We dynamically adapt the size of the LLM to the specific requirements of each situation encountered by a robot by introducing a model with intermediate exits. Specifically, we divide the LLM layers into \(N\) consecutive groups, noted as \(F_{}^{1}\), \(F_{}^{2},,F_{}^{N}\). Each group \(F_{}^{i}\) outputs an intermediate hidden state sequence \(x_{t}^{i}\!\!=\!(x_{t,1}^{i},x_{t,2}^{i},,x_{t,L}^{i})\). When computation terminates at an intermediate exit \(i\), we apply a max-pooling operator \(P\) to aggregate the information across the token dimension, resulting in a compact representation \(_{t}^{i}\!\!=\!P(x_{t,1}^{i},x_{t,2}^{i},,x_{t,L}^{i})\)

Figure 1: **Left: Dynamic inference of DeeR. For inference, we adaptively activate an appropriate size of MLLM based on an exit criterion \(c\), which accounts for the _current situation_ (including task instruction \(l\) and observation \(o_{t}\)) and _predefined computational and GPU memory budgets_. The language instruction and gripper camera image, not shown in this figure, are also inputs to the MLLM. An action is then obtained using the intermediate feature \(_{t}^{c(t)}\) and historical information. Right: Training of DeeR.We randomly sample features from all exits during training. This strategy helps minimize the discrepancy between training and dynamic inference. Moreover, we employ several auxiliary action heads (AuxH) to better optimize the MLLM.**

Figure 2: Multi-exit MLLM architecture for robot.

that effectively summarizes the image \(o_{t}\) and instruction \(l\). This representation serves as the input for the subsequent action prediction module. With such a multi-exit MLLM architecture, we can obtain a series of informative representations \(_{t}^{1}\), \(_{t}^{2},...,_{t}^{N}\) at varying scales of LLM processing. This allows us to dynamically select the most suitable LLM size conditioned on the situation complexity without activating parameters beyond the chosen exit point. Our multi-exit MLLM is illustrated in Figure 2.

**Predicting robotic actions with an action head.** After the LLM processes to an appropriate level, the output \(_{t}^{i}\) from the \(i\)-th intermediate exit is transformed into low-level actions by a lightweight action head. In this paper, we consider a 7 DoF end-effector action as a representative example of low-level actions, where the first six continuous dimensions specify the position and orientation of the end-effector, and the seventh discrete value indicates whether the gripper is open or closed. Notably, given that the decision-making environment is typically characterized as a Partially Observable Markov Decision Process (POMDP) , optimal decisions rely not only on the current observation \(o_{t}\) but also on historical observations. Thus, we employ a sequence model as the action head \(_{}\) to aggregate information across a history window of size \(H\). Without loss of generality, this paper considers a lightweight LSTM  as an example. On the top of LSTM are two distinct MLP modules: one dedicated to predicting the pose of the end-effector, and the other to predict the discrete gripper status. The lightweight action head \(_{}\) computes actions efficiently with minimal computational overhead.

**Early-terminated inference of robotic actions.** Equipped with the action head, we assume that a criterion \(c\) is defined to determine the optimal point for the conditional exiting from an appropriately sized LLM at the current timestep \(t\) (we will discuss the details of criteria in Section 3.2). The index of the selected exit, denoted as \(c(t)\), ranges from 1 to \(N\). Consequently, we utilize the feature \(_{t}^{c(t)}\) from the \(c(t)\)-th LLM block to compute the predicted action \(a_{t}^{*}\) for the current timestep as follows:

\[a_{t}^{*},h_{t}=_{}(_{t}^{c(t)},h_{t-1}),\] (2)

where \(h_{t}\) represents the LSTM's hidden state, with \(h_{0}\) initially set to a zero vector. The predicted action \(a_{t}^{*}\) is composed of pose action and gripper action.

### Adaptive Inference

This section demonstrates how DeeR efficiently executes robot tasks by adaptively activating a proper size of the MLLM under predefined computation and GPU memory budgets. Specifically, we first discuss the termination criterion utilized by DeeR, designed to activate smaller models for less complex scenarios and larger models for more challenging conditions. Next, we explore our approach to devising an effective resource allocation strategy that addresses limitations in computation and GPU memory. The inference process of DeeR is illustrated in Figure 1.

**Termination criterion.** As mentioned in related works, many previous works utilize confidence-based criteria for determining when to terminate, typically involving metrics such as the maximum element or entropy of the SoftMax output [37; 43; 65; 66; 49]. In our case, where the goal is action prediction and SoftMax output is not readily available, we adopt a different approach by leveraging the consistency of action predictions from adjacent intermediate features as our criterion. The underlying intuition is that if the action predictions from two differently sized MLLMs remain consistent, it suggests that the computational model may have reached saturation, and further processing is unlikely to yield any further improvements. For a given timestep \(t\), we identify the smallest \(i\) within the range \([1,2,...,N]\) that satisfies the following action consistency condition as termination exit:

\[\|_{}(_{t}^{i},h_{t-1})-_{}(_{t}^{i-1},h_ {t-1})\|_{2}<_{i},\] (3)

where we disregard the hidden state outputs of \(_{}\) and focus solely on comparing the \(L2\) norm of the difference in predicted actions against a predefined threshold \(_{i}\). We always adopt infinity as \(_{N}\) to ensure all samples can exit. For \(i=1\), we use the input features to the LLM layer as \(x_{t}^{i-1}\).

**Budgeted task execution.** Given the predefined constraints of computation and memory budgets, it can be challenging to manually set optimal threshold values \(\{_{1},_{2},\}\) to ensure that the robotic MLLM policy achieves peak performance while adhering to budget limitations. In contrast, we propose to determine these values by formulating an optimization problem. We operate under a _Budgeted Task Execution Setting_, where DeeR is required to perform a set of tasks \(\) within a specified total computational budget \(B>0\). To ensure that each action is delivered within an acceptable waiting time, we impose constraints on peak computation where \(G>0\). Additionally, we limit GPU memory usage to \(M>0\) to accommodate scenarios where users may not have access to high-memory GPUs. Let \((,\{_{1},_{2},\})\) represent the success rate of tasks in \(\), and let FLOPs\((,\{_{1},_{2},\})\) denote the computational cost for executing these tasks under specified constraints. Furthermore, MFLOPs\((,\{_{1},_{2},\})\) denotes the peak FLOPs across all timesteps, and \((,\{_{1},_{2},\})\) indicates GPU memory used during task execution. We seek the optimal thresholds by addressing the following optimization problem:

\[_{_{1},_{2},}(,\{_{1},_{2}, \}),\] (4)

subject to

\[(,\{_{1},_{2},\}) <B,\] (average FLOPs constraint) \[(,\{_{1},_{2},\}) <G,\] (peak FLOPs constraint) \[(,\{_{1},_{2},\}) <M.\] (GPU memory constraint)

Due to the non-differentiability of the success rate function \((,)\), we may leverage heuristic algorithms to solve for the thresholds that maximize success within the computational constraints. We discuss strategies for determining optimal thresholds under two conditions: one where we only have access to a demonstration dataset, and another where real environment interaction is permitted.

**Solving problem (4) using a demonstration dataset.** We denote by \(0<q 1\) the probability that a sample reaching an exit point will meet the termination criterion and thus exit at that point. When accessing only a demonstration dataset, we assume \(q\) is constant across all layers . This suggests that the proportion of samples exiting at exit \(i\) can be represented as \(q_{i}=zq^{i}\), where \(z\) is a normalizing constant ensuring that \(_{i=1}^{n}q_{i}=1\). Here, \(n N\) denotes the maximum allowable exit index where the corresponding activated LLM meets the constraints of peak GFLOPs and GPU memory. The proportions of samples at the exits whose index are greater than \(n\) is set to zero. At testing time, we must adhere to the computational budget constraint:

\[||_{i=1}^{n}q_{i}C_{i} B,\] (5)

where \(||\) is the number of tasks to perform, \(\) represents the average length of tasks as derived from the dataset statistics and \(C_{i}\) is the computational cost when the LLM inference terminates at \(i\)-th exit. Equation (5) allows us to solve for \(q\) and determine \(q_{i}\). Using these target proportions \(q_{i}\) for each exit, we then determine the threshold values \(_{i}\) on the dataset to ensure that approximately \(q_{i}\) proportion of timesteps exits at the \(i\)-th exit .

**Solving with online interactions.** If the interaction with a real environment is feasible, we can utilize online learning algorithms that iteratively adjust thresholds based on feedback regarding success rates. To solve Equation (4) under budget constraints, we implement Bayesian Optimization . We construct the objective function for Bayesian Optimization to maximize as follows:

\[f_{}=(,\{_{1},_{2},\})-P,\] (6)

where \(P=0\) if budget constraints are satisfied otherwise a significant penalty term. This online paradigm allows us to determine thresholds without assuming an exponential distribution, enabling the acquisition of more effective thresholds through real-time feedback. Here, we employ Bayesian Optimization as a representative example of an online solving approach. Future work could extend to other online algorithms such as multi-armed bandit or reinforcement learning .

### Training Algorithm

Notably, it is nontrivial to train our dynamic robotic MLLM properly. Specifically, dynamic adjustment of the network architecture leads to a _discrepancy_ between training and inference. During inference, we use a deterministic criterion to select a proper intermediate feature at each timestep. Nevertheless, during training, we lack a well-defined termination criterion and remain unaware of the distribution of features across the exits. To enable our model to learn to integrate temporal information effectively, we propose a tailored training algorithm, as introduced in the following.

**Learning with an arbitrary size of models.** To reduce the aforementioned discrepancy, we propose a simple yet effective random sampling strategy during training. As depicted by the "winding" curves on the right side of Figure 1, our approach involves sampling an exit index from 1 to \(N\) at each timestep. We implement two types of sampling strategies. _The first strategy_, denoted as \(s_{1}\), is to uniformly sample an exit index from 1 to \(N\) at each step. This ensures that features from all possible exits are effectively captured in the action head during training. It simulates scenarios where the action head might encounter features from all exits within a given time window, thus accommodating an arbitrary inference pattern and reducing the training-inference discrepancy. Moreover, we observe that in practice, the dynamic model often terminates at the same exit for multiple consecutive timesteps, as the neighboring observations tend to be quite similar. The model then switches to another exit for a sequence of subsequent timesteps. To better emulate this pattern during training, we adopt _a second sampling strategy_ denoted as \(s_{2}\). Specifically, we split the time window \(o_{t:t+H-1}\) into two consecutive segments \(o_{t:t+i}\) and \(o_{t+i+1:t+H-1}\), with \(i\) chosen randomly. In each segment, a single uniformly sampled index is assigned and shared across all timesteps.

On top of these two sampling strategies, we can define our training loss function. We sample from a robot demonstration dataset a language instruction \(l\) and a clip of observation-actions \(\{o_{t_{i}},a_{t_{i}+1},a_{t+1},,o_{t+H-1},a_{t+H-1}\}\). For each sampling strategy \(s\{s_{1},s_{2}\}\), we use Equation (1) and Equation (2) to predict each action \(a^{*,s}_{t+i}\) for \(s=s_{1},s_{2}\) and \(i=0,1,,H-1\), where \(c(t)\) in Equation (2) is replaced by the sampling strategy \(s\). For each pair of the predicted action \(a^{*}\) and the actual action \(a\), we define a single-action loss function \((a^{*},a)\) that incorporates both mean squared error (MSE) for pose prediction and cross-entropy loss for gripper status prediction with a coefficient \(\) to balance the two terms . The total loss for a sequence is then the sum of the losses over timesteps:

\[^{*}=_{s\{s_{1},s_{2}\}}_{i=0}^{H-1} (a^{*,s}_{t+i},a_{t+i}).\] (7)

**Auxiliary losses.** The intermediate features from the original MLLM, intended as input for subsequent layers, may not be optimal for output prediction. To ensure that each activated size of the MLLM in our framework produces features suitable for predicting actions, we introduce auxiliary losses. Specifically, we attach \(N\) auxiliary action heads (denoted as UAH in Figure 1) at the exits. The \(j\)-th auxiliary head processes temporal features from the \(j\)-th exit and predicts the action \(a^{j}_{t}\). We jointly train the auxiliary heads and the MLLM using the loss function:

\[_{}=_{j=1}^{N}_{i=0}^{H-1} (a^{j}_{t+i},a_{t+i}).\] (8)

These auxiliary heads are employed only during training and are not used for inference.

**Total Loss.** The full training pipeline is depicted in Figure 1. We fine-tune only the parameters of the perceiver sampler and cross-attention layers in the MLLM, with the randomly initialized action head \(_{}\) and auxiliary action heads. The visual encoder and the other LLM components, remain frozen. The total loss for the training process is expressed as \(_{}=^{*}+_{}\).

## 4 Experiments

**Setup.** In this section, we conduct experiments to validate the effectiveness of DeeR as an efficient robot policy. Specifically, we build DeeR upon the RoboFlamingo++ codebase. We preserve hyper-parameters from RoboFlamingo++ for fair comparison, except for the number of LLM layers and our proposed dynamic early-exit paradigm. we compare DeeR in terms of budget _v.s._ performance with similarly sized RoboFlamingo++ models  and other SOTA baselines. We provide implementation details in Appendix A.

**Measures of efficiency.** In modern foundation models, the LLM typically plays a pivotal role within an MLLM in terms of reasoning and problem-solving tasks, and it usually contains the majority of the

    &  &  &  &  \\  & & & D\(\)D & ABCD\(\)D & ABC\(\)D \\   & RGB+ &  & Video-pretrained &  &  &  \\  & Proprio & & Transformer & & & \\  HULC  (_Q_++22) & RGB & ALL & ✗ & 2.64 & 3.06 & 0.67 \\ RT-1  (_Q_SS22_) & RGB & LANG & ✗ & - & 2.45 & 0.9 \\ SPIL  (_Q_ML24_) & RGB & ALL & ✗ & 2.67 & - & 1.71 \\ SuSIE  (_ICLR20_) & RGB & ALL & InstructPix2Pix  & & - & 2.69 \\ RoboFlamingo (_ICLR20_) & RGB & LANG & OpenFlamingo 3B & 2.46 (_IJ_, 2.2) & 4.08 (_IJ_, 2.2) & 2.47 (_IJ_, 2.2) \\ RoboFlamingo++ & RGB & LANG & OpenFlamingo 3B & 2.71 (_IJ_, 2.2) & 4.07 (_IJ_, 2.2) & 2.59 (_IJ_, 2.2) \\ DeeR (ours) & RGB & LANG & OpenFlamingo 3B & **2.83 (_8.6_) & **4.13 (_10.0_) & **2.82 (_IJ_, 2.5)** \\  DeeR w. online (ours) & RGB & LANG & OpenFlamingo 3B & **2.92 (_8.5_) & **4.13 (_9.7_) & **2.90 (_9.5_) \\   

Table 2: Comparison with baselines. GR-1 uses extra proprioceptive information as input. Note that some baselines mainly focus on one or two settings, and we present results following their original papers. We report the performance of our method at the last epoch. The value in parentheses indicates the LLM FLOPs required to achieve the reported score. The success rates for the 1st to 5th subtasks are in Appendix B.1.

model's parameters . Our work mainly focuses on improving the efficiency of LLMs within a robotic context. To facilitate a focused comparison, in our experiments, we report the number of floating point operations (FLOPs) and GPU memory usage for LLM's inference.

**Benchmark.** We utilize the CALVIN Long-Horizon Multi-Task Language Control benchmark (LH-MTLC)  as the testbed to test our learned multi-task, language-conditioned policy. In the CALVIN, the objective is for the agent to successfully complete task sequences, each with five subtasks described in natural language. Following previous works , model performance is evaluated based on the average successful length (0 to 5) across 1000 task sequences.

**Datasets.** The CALVIN dataset  is divided into four environmental splits, labeled A through D, each characterized by unique backgrounds and object configurations. Each split contains over 2 million robot manipulation trajectories (denoted as '_ALL_'). Of these, only about 1%, approximately 24 thousand trajectories, are annotated with language instructions (denoted as '_LANG_'). For training DeeR, we exclusively utilize the 'LANG_' data. In our study, we evaluate models across three settings to thoroughly assess their imitation and generalization capabilities: 1) D\(\)D: Train and evaluate in a single environment, 2) ABC\(\)D: Zero-Shot Multi-Environment, 3) ABCD\(\)D: Multi-Environment.

**Baselines.** For a comprehensive comparison, we consider various baselines. We include HULC  and SPIL  as representatives of approaches reliant on hierarchical planning and skill priors . Additionally, we evaluate models using pretrained or foundation models, such as RT-1 , SuSIE , GR-1 , and RoboFlamingo . RoboFlamingo++ is our reproduced RoboFlamingo.

### Main Results

**Results on Flamingo 3B** are presented in Figure 3. We train just a single model in each CALVIN setting. Given the predefined total computational budget \(B\), the maximum FLOPs \(G\), and the GPU memory \(M\), we adhere to these budgets by adjusting the termination thresholds, which are determined by solving Equation (4) with the CALVIN dataset. Then we assess the average successful length of DeeR under different thresholds to plot the curves. It can be observed that DeeR consistently reduces the computational cost of the LLMs across all settings. For instance, in the setting D\(\)D, DeeR achieves an average successful length 2.71 with 5.9x fewer average FLOPs, 2x fewer maximum FLOPs and 2x fewer GPU memory. Surprisingly, DeeR-S achieves relatively high performance with only 2GB memory consumed by LLM, which is affordable to most users. Thus, DeeR demonstrates the potential to enable a broader range of users to operate their own robots effectively with LLMs.

**Comparison with SOTA baselines.** In Table 2, we benchmark the DeeR model against recently SOTA methods in the CALVIN benchmark. Our analysis reveals that DeeR achieves competitive performance compared to the latest SOTA model GR-1 which uses additional proprioceptive information. When compared with traditional imitation learning methods without foundation model, DeeR demonstrates superior performance, particularly in generalization scenarios (ABC\(\)D). Moreover, DeeR slightly outperforms RoboFlamingo while requiring less computation.

Figure 3: Results atop OpenFlamingo 3B. **Upper**: Avg. successful len v.s. avg. LLM GFLOPs. **Bottom**: Peak GLOPs and GPU memory for LLM. Different colors indicate different peak FLOPs and GPU memory budgets, denoted as DeeR-S and DeeR-B (they share a fixed model). DeeR preserve all the architecture and hyperparameters from RoboFlamingo++ for fair comparisons, except for our dynamic early-exit paradigm.

**Solve thresholds with online interaction.** When interaction with the environment is feasible, we utilize Bayesian Optimization to solve Equation (4) as we stated in Section 3.2. As shown in Table 2, we discovered that finding thresholds via online interaction is particularly effective in challenging scenarios such as low-data environments (D\(\)D) and generalization to unseen situations (ABC\(\)D).

**Scalability of DeeR.** We developed DeeR on top of OpenFlamingo 9B  to evaluate its efficiency when scaling up the foundation model. The results, detailed in Figure 4, indicate that DeeR reduces 1.8-5.7x computation and 2.7x-4.0x peak FLOPs and memory for the same performance.

### Ablation Study

**Auxiliary losses.** In this study, we explore the effect of auxiliary losses using the ABCD\(\)D setting as a representative scenario. As shown in Table 3, the model trained without auxiliary losses demonstrates much lower performance. The drop may stem from insufficient optimization for features across exits: The training paradigm without auxiliary losses optimize only one feature from a single exit at each timestep (chosen by sampling strategy) for action prediction. We also observed that in smaller datasets, such as in the D\(\)D setting, omitting auxiliary losses has little to no impact on performance. This may be because smaller datasets are easier to fit, reducing the necessity for auxiliary losses.

**Early-termination criterion.** Based on a fixed DeeR model, we explore various criteria for adaptive inference. We consider using cosine similarity between exit points to determine termination . Specifically, if the similarity value exceeds a threshold, the process is terminated. We introduce another metric that progressively increases the size of the activated LLM as a task progresses, based on the observation that the initial stage of a task generally present simpler scenarios. Our results, detailed in Table 4, demonstrate that our straightforward yet effective action consistency criterion outperforms other criteria across several average computational budgets.

**Real inference efficiency.** We conducted evaluations of real-world operational efficiency. Both RoboFlamingo++ and DeeR were tested on the same Nvidia V100 GPU. As shown in Table 5, DeeR achieved a 68.1% reduction in LLM inference time compared to RoboFlamingo++ (abbreviated as Robo++ in Table 5) when both models achieved the same performance, which aligns with the theoretical 80.7% reduction in FLOPs. This evaluation was performed without code optimizations for early-exit implementation. We expect that with further optimizations, DeeR's real-world operational efficiency will improve, potentially aligning even more closely with the predicted FLOPs reduction.

    &  &  \\  & & feat. & time & action \\   & 4.9 & 2.52 & 2.35 & **2.65** \\  & 9.1 & 2.62 & 2.82 & **2.83** \\   & 4.9 & 3.66 & 3.92 & **3.94** \\  & 9.1 & 3.92 & 4.08 & **4.10** \\   & 4.9 & 2.29 & 2.46 & **2.62** \\  & 9.1 & 2.45 & 2.71 & **2.75** \\   

Table 4: Ablation study of exit criteria. Comparing feature similarity, time, and action consistency.

Figure 4: Results on the top of OpenFlamingo 9B. **Left**: Avg. successful len _v.s._ average LLM GFLOPs. **Right**: Maximum GLOPs and GPU memory budget for DeeR-S and DeeR-B. The activated LLM in DeeR-S and DeeR-B consumes 12GB memory, whereas RoboFlamingo 9B requires 32GB.

    &  \\  & DeeR & w.o. aux \\ 
4.9 & **3.94** & 2.64 \\
10.0 & **4.13** & 2.71 \\   

Table 3: Ablation study of auxiliary losses on ABCD\(\)D.

    &  &  \\  & & feat. & time & action \\   & 4.9 & 2.52 & 2.35 & **2.65** \\  & 9.1 & 2.62 & 2.82 & **2.83** \\   & 4.9 & 3.66 & 3.92 & **3.94** \\  & 9.1 & 3.92 & 4.08 & **4.10** \\   & 4.9 & 2.29 & 2.46 & **2.62** \\  & 9.1 & 2.45 & 2.71 & **2.75** \\   

Table 5: Comparison of real inference efficiency on the ABCD\(\)D dataset. The average LLM inference time is reported.

**DeeR with Quantization.** Model compression techniques, such as quantization and pruning, along with efficient structural designs, complement early-exit strategies like DeeR. These methods improve efficiency from different perspectives: quantization reduces memory usage by lowering parameter precision, while early-exit strategies like DeeR reduce computational load by dynamically skipping unnecessary layers. In Table 6, we present quantization as an example to illustrate how DeeR can integrate with these techniques to reduce memory cost.

### Visualization

Figure 5 displays rollouts of DeeR with the termination points. Situations with a higher exit index are considered "harder" by DeeR and thus are allocated more computational resources. One can observe that "hard" situations often involve relatively complex and delicate operations, while "easy" situations typically involve straightforward movements toward target objects. For example, in the task of stacking blocks (1st row), lifting the blue block from the table (1st image) and placing it down on the pink block (images 4 and 5) are allocated more computation, whereas simply moving towards the pink block (images 2 and 3)requires only the smallest LLM to handle. Similar observations occur in the 2nd and 3rd rows, where the stage of moving toward the target object require minimal computation, while pushing the lightbulb switch or moving the sliding door are sophisticated operations that necessitate more LLM processing.

## 5 Conclusion and Limitations

In this paper, we introduced the _Dynamic Early-Exit for Robotic MLLM_ (DeeR) framework, aiming to dynamically configure the size of MLLMs based on the specific requirements of each situation encountered by a robotic agent. In specific, we proposed a novel MLLM architecture with multiple intermediate exits. Further, we establish early-termination criteria for DeeR based on action consistency and solve thresholds via a dataset or online interaction. Additionally, we crafted a tailored training method that integrates temporal information within this multi-exit framework to enhance robotic control. Extensive robotic experiments demonstrated that DeeR significantly reduces LLM computational costs and GPU memory usage, highlighting its potential to empower a broader range of users to manage their robots on resource-constrained platforms. While our study shows promising results, it has some limitations. We focused on improving LLM efficiency for robotic execution since LLMs account for most of the parameters and GFLOPs. However, the computational cost of the visual encoder is also significant. We expect this limitation to be alleviated as more efficient, lightweight visual encoders are developed. Besides, our experiments were limited to a simulated benchmark. Future work will aim to improve the inference efficiency of the entire MLLM-based robotic system in realistic environments.

## 6 Acknowledgement

The Tsinghua University team is supported in part by the National Key R&D Program of China (2022ZD0114903).

  DeeR & Memory & Avg Len \\  float32 & 6G & 4.13 \\ float16 & 3G & 4.12 \\ int4 & 1.7G & 3.91 \\  

Table 6: DeeR with quantization on the ABCD\(\)D setting.

Figure 5: Visualization of DeeR rollouts in the CALVIN environment. Please zoom in to view details. The numbers indicate the termination exit index. Situations with a lower exit index are recognized as ‘easier’ ones.