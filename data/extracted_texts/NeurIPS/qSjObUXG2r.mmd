# Understanding and Quantifying Reliability

in Object Detection Transformers

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Object detection is a computer vision task with significant utility, with real-world applications ranging from autonomous driving to warehousing and medical image analysis. Recently, Object Detection Transformers (DETR) have emerged as a prominent approach, offering an end-to-end prediction pipeline. The core innovation of DETR lies in the introduction of object queries, which attend to each other throughout the Transformer decoder layers and provide a set of outputs (i.e., bounding boxes and class probabilities) for a given image. Despite these advances, the mechanisms behind how these predictions are generated and interact are not well understood. For this reason, this paper explores the underlying dynamics of DETR's predictions and presents empirical findings that highlight how different predictions within the same image serve distinct roles, leading to varying levels of reliability across those predictions. In particular, we investigate the significance of differentiating between positive and negative predictions for uncertainty quantification (UQ) in DETR. Leveraging these insights, we propose novel post hoc UQ methods to quantify the image-level reliability of DETR and demonstrate their effectiveness through numerical analysis.

## 1 Introduction

Object detection is an essential task in computer vision, with applications that span various domains, including autonomous driving, warehousing, and medical image analysis. Traditional approaches to object detection have relied on Convolutional Neural Networks (CNNs) (Girshick et al., 2014; Ren et al., 2015; Redmon et al., 2016; He et al., 2017) to identify and locate objects within images. However, the recent introduction of Object Detection Transformers (DETR) (Carion et al., 2020) has revolutionized the field by offering an end-to-end prediction pipeline, where the model directly predicts a set of bounding boxes and class probabilities for each object in an image.

The core innovation of DETR lies in the use of a Transformer encoder-decoder architecture (see Section 2 for more detail), enabling the model to generate predictions in an end-to-end manner and enhancing scalability. This paradigm shift has led to the exploration of various DETR variants -- such as Deformable-DETR (Zhu et al., 2020), DINO (Zhang et al., 2022), and Cal-DETR (Munir et al., 2024) -- positioning them as potential foundation models for object detection tasks. Despite these advancements, the inner workings of _how set predictions are generated and interact within the Transformer decoder layers_ remain poorly understood.

This paper examines this issue from the perspective of reliability/uncertainty quantification of DETR's1 predictions. Specifically, we aim to answer **two key research questions**: (1) Do allpredictions behave similarly and exhibit a consistent correlation with the model's reliability? (2) If not, how can we accurately assess DETR's reliability for a given input image?

Our preliminary findings indicate that different predictions from DETR in the same image play distinct roles resulting in varying levels of reliability. Specifically, as the decoder processes each object query through multiple layers, interactions among predictions occur, leading to the refinement of one positive query per object while others (i.e., negatives) provide support (Section 4.1).

To this end, this paper investigates the importance of distinguishing between these positive and negative predictions from the perspective of post hoc UQ in DETR, which is one of our key contributions (Section 4.2). We empirically observe that the confidence of negative queries inversely correlates with reliability, in contrast to positive queries, where higher confidence typically indicates greater reliability. Based on these insights, we propose novel methods to more accurately quantify the image-level reliability of the DETR model and conduct experiments evaluating them (Section 4.3).

## 2 Background: Object Detection Transformers (DETR)

The structure of DETR can be broadly divided into two main components: the Transformer encoder, which extracts a collection of features from the given image; and the Transformer decoder, which uses these features to make predictions. We refer to Figure 2 in the Appendix for an illustration.

In addition to the features extracted by the encoder, the decoder's input consists of \(N\) (typically several hundreds) learnable embeddings, also known as _object queries_. Each decoder layer is composed of a self-attention module among object queries and a cross-attention module between each object query and the features. After processing the queries through several decoder layers, the model produces the \(N\) final representation vectors that are converted into bounding boxes and class labels via a shared feedforward network, \(f_{}\). Together, these predictions form the final outputs, making DETR's predictions essentially an \(N\)-element set.

The encoder follows the common structure of standard computer vision models and is based on pre-trained models, whose reliability has been widely explored (Shelmanov et al., 2021; Sharma et al., 2021; Vazhentsev et al., 2022; Park et al., 2023). This foundation further enables the use of prominent post hoc uncertainty quantification (UQ) techniques, such as Monte Carlo dropout (Gal and Ghahramani, 2016) and distance-based out-of-distribution (OOD) detection methods (Lee et al., 2018; Tack et al., 2020).

However, despite the decoder being the predominant component for object detection, there is a gap in understanding and quantifying its reliability due to its unique structural characteristics: set prediction. Therefore, this paper delves into the roles and behavior of these predictions and presents a methodology to estimate the reliability of the decoder in DETR for object detection tasks.

## 3 Problem Statement: Quantifying the Image-Level Reliability

We first introduce a formal definition of _image-level reliability_ by examining the model's overall object detection performance on the image.

**Definition 1**.: Let \(^{*}\) be a test image, and \(_{^{*}}\) denote the set of ground truth objects in the image. The outputs of the DETR, parameterized by \(\), for the image are represented by \(}_{}(^{*})\). Each object is represented by a bounding box and a class label (with probability). We define image-level reliability as a measure of how accurately and confidently the predictions match the ground truth objects:

\[(^{*};)}_{}(^{*}),\ _{^{*}}.\] (1)

where we quantify \(\) using standard performance metrics such as precision, recall, and negative DETR matching cost (i.e., neg. MC). Details on how these metrics are computed can be found in Lin et al. (2014) and Carion et al. (2020).

To the best of our knowledge, existing UQ techniques focus mainly on object-level analysis (e.g., (Du et al., 2022; 2022; Wilson et al., 2023; Sbeyti et al., 2024)) which are often conducted on predefined ground truth objects. Once the ground truth objects are provided, the bipartite matching algorithm (detailed further in Appendix A) can be used to find the best matching prediction for each object based on the alignment of the class label and bounding box. In this paper, we refer to these queries as _positive queries_, while the remaining queries are referred to as _negative queries_.

In real-world scenarios, however, ground truth annotation is unavailable (i.e., the ground truth positive queries are unknown). Furthermore, predictions far outnumber ground truth objects, leaving it unclear whether reliability should be assessed for all predictions, or a subset (and, if so, which subset?). Thus, extending these methods to DETR raises a non-trivial question, which this paper aims to address.

## 4 Proposed Methods & Empirical Findings

### Inside the Black Box: Exploring the Anatomy of DETR's Predictions

In developing a suitable UQ method, we begin by examining DETR's prediction process. Since the Transformer decoder outputs only representation vectors, investigating their evolution across layers is not straightforward. We address this by reapplying the final feedforward network that operates on the last layer, \(f_{}\), to the intermediate layers. This enables the transformation of each representation vector into its associated bounding box and class label. This is feasible due to the alignment of intermediate representations, facilitated by residual connections between decoder layers (Chuang et al., 2023; Munir et al., 2024). Sample visualizations are in Figures 1, 3, and 4.

In the first decoder layer, the model appears to explore the encoded image features, producing varied queries that result in various plausible predictions. In this early stage, the distinction between positive and negative queries can be ambiguous (e.g., Figure 0(a)). However, the self-attentions through the subsequent decoder layers progressively refine these predictions. By the final layer, the model selects a single query (i.e., positive) and assigns a confidence score based on its understanding on the image and the object. In contrast, the confidence scores for neighboring queries (i.e., negatives) do not increase to the same extent as the positives and even decrease (e.g., Figure 0(b)) for reliable images. Hence, for a _reliable_ image, we observe a **large gap between the positive and negative queries**.

Moreover, having queries with low confidence scores does not necessarily imply low reliability; in fact, empirical observations show that high image-level reliability actually correlates with low confidence scores in negative queries (Note that the correlation for \(^{-}\) is negative, as shown in Table 1.). This underscores the importance of accurately distinguishing the positive query within the entire set to achieve accurate UQ in DETR.

Another notable observation is that, for _unreliable_ image (e.g., Figure 0(c)), the confidence score of the positive query does not grow significantly, unlike in reliable cases. In contrast, the confidence of negative queries increase, resulting in a **small gap between the positive and negatives**.

Figure 1: Predictions made by Cal-DETR for high- and low-reliability images. The positive query (0) and the five negative queries (1-5) with the largest intersection of union (IoU) are presented. The model refines its predictions through each decoder layer, culminating in a high-confidence positive query while neighboring negative queries remain less confident.

### Proposed Method: Quantifying Reliability in DETR

Based on the aforementioned observations, we propose a novel post hoc UQ approach by contrasting the confidence score (i.e., reliability) of positives and negatives:

\[(^{*})=^{+}(^{*})- ^{-}(^{*})\] (2)

\[^{+}(^{*})=}^{+}(^{* })|}_{t}^{+}(^{*})}(t) ^{-}(^{*})=}^{-}(^{*})|}_{t}^{-}( ^{*})}(t)\] (3)

where \(}^{+}(^{*})\) and \(}^{-}(^{*})\) are positive and negative predictions, respectively, and \(\) is a scaling factor that aligns with the ratio of the average standard deviation of two scores across different images. \(()\) denotes the confidence estimate for the prediction; in this paper, we use maximum probability 2.

To separate the positives and negatives from the complete set of predictions, \(}(^{*})\), we explore three different methods: by (0) considering the **entire** set as positives, (1) applying a **thres**hold on the maximum probability, (2) selecting the **top-\(k\)** predictions, and (3) utilizing non-maximum suppression (**NMS**). For comparison, we also evaluate with the ground truth (**GT**) separation based on DETR's bipartite matching loss with ground truth object annotations. For further details, including how \(k\) and the threshold are determined, please refer to Appendices C and D.

### Numerical Evaluation

To demonstrate the effectiveness of the proposed method, we compare it extensively with an approach that uses only \(^{+}\) or \(-^{-}\). We evaluate using two different DETR models -- Deformable-DETR and Cal-DETR -- across three datasets -- COCO (in-distribution), Cityscapes (near out-of-distribution), and Foggy Cityscapes (out-of-distribution). Empirical results are illustrated in Table 1 above and Table 2 in Appendix D. The key takeaways are as follows.

First, distinguishing between positive and negative queries appears to be crucial for UQ in DETR. Specifically, when confidence scores are averaged without distinguishing positive queries (i.e., \(^{+}\) w/ entire), the correlation becomes negative. This is because the majority of DETR's outputs are actually negative (i.e., \(_{x^{*}} N\)), and that confidence scores for negative queries are inversely correlated with \(\), which is why we report the correlation of "negative(-)" \(^{-}\).

Second, \(\) consistently ranks the best or second-best correlation. Although the baselines, \(^{+}\) and \(-^{-}\), sometimes surpass \(\), their performance fluctuates across settings, highlighting the robustness of the proposed contrastive approach.

Lastly, the performance largely depends on which separation method is applied. For instance, the top-\(k\) approach even presents a negative correlation. While the thresholding approach empirically yields better performance, a non-negligible gap remains compared to using ground truth matching. We plan to address this research question, which is discussed further in Appendix E, in a future paper.

    &  &  &  &  \\  & & & **Precision** & **Recall** & **neg. MC** & **Precision** & **Recall** & **neg. MC** & **Precision** & **Recall** & **neg. MC** \\   ^{+}\)} & Entire & -0.412 & -0.424 & -0.380 & -0.221 & -0.223 & -0.263 & -0.127 & -0.130 & -0.108 \\   & Thr & 0.569 & 0.305 & -0.623 & 0.247 & 0.214 & 0.393 & **0.220** & 0.181 & **0.314** \\  ^{+}\)} & Tr\({}^{k}\) & 0.011 & 0.380 & 0.054 & 0.016 & 0.263 & 0.078 & 0.047 & 0.108 \\  & MSG & 0.401 & -0.419 & -0.362 & -0.201 & -0.214 & -0.246 & 0.118 & -0.123 & -0.097 \\   & GT & 0.461 & 0.361 & 0.888 & 0.449 & 0.447 & 0.795 & 0.466 & 0.456 & **0.382** \\  ^{-}\)} & Tr\({}_{k}\) & 0.422 & 0.4161 & 0.452 & 0.273 & 0.267 & 0.317 & 0.194 & 0.188 & 0.185 \\  & Top-k & 0.254 & 0.273 & 0.429 & 0.171 & 0.185 & 0.298 & 0.129 & 0.131 & 0.153 \\  & MSG & 0.409 & 0.395 & 0.477 & 0.224 & 0.121 & 0.292 & 0.140 & 0.132 & 0.132 \\   & GT & 0.417 & 0.405 & 0.439 & 0.214 & 0.206 & 0.235 & 0.146 & 0.140 & 0.122 \\  \)} & Tr\({}^{k}\) & 0.423 & 0.386 & 0.610 & 0.304 & 0.280 & 0.449 & 0.252 & 0.219 & **0.003** \\   & Top-k & 0.568 & 0.311 & 0.110 & 0.251 & 0.230 & -0.065 & 0.193 & 0.173 & 0.031 \\   & MSG & 0.052 & -0.013 & 0.174 & 0.010 & -0.014 & 0.072 & 0.026 & 0.008 & 0.067 \\   & GT & 0.481 & 0.385 & 0.810 & 0.458 & 0.451 & 0.784 & 0.472 & 0.459 & 0.813 \\   

Table 1: Comparison of the proposed method (\(\)) with baseline methods on their Kendall’s \(\) correlation coefficient with the image-level reliability in Equation 1, with Cal-DETR. Note that we use the “negative(-)” \(^{-}\) due to the inverse correlation between average confidence of negative queries and reliability, as discussed in Section 4.1. The GT (oracle), highest and second-highest scores are highlighted in light gray, blue and green, respectively.