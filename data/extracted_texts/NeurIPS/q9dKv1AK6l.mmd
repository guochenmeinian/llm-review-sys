# Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates

Jincheng Mei

Google DeepMind

Google Research

Georgia Institute of Technology

University of Alberta

Simon Fraser University

Indian Institute of Science

Bo Dai

Google Research

Georgia Institute of Technology

Georgia Institute of Technology

Alekh Agarwal

Google Research

Georgia Institute of Technology

Sharan Vaswani

Simon Fraser University

Indian Institute of Science

Anant Raj

Google Research

Georgia Institute of Technology

Georgia Institute of Technology

Simon Fraser University

Indian Institute of Science

###### Abstract

We provide a new understanding of the stochastic gradient bandit algorithm by showing that it converges to a globally optimal policy almost surely using _any_ constant learning rate. This result demonstrates that the stochastic gradient algorithm continues to balance exploration and exploitation appropriately even in scenarios where standard smoothness and noise control assumptions break down. The proofs are based on novel findings about action sampling rates and the relationship between cumulative progress and noise, and extend the current understanding of how simple stochastic gradient methods behave in bandit settings.

## 1 Introduction

The stochastic gradient method has been ubiquitous in the field of machine learning for decades . When applied to reinforcement learning (RL), a representative instantiation of stochastic gradient is the well known policy gradient  (or REINFORCE ) algorithm, where in each iteration an online sample is gathered using the current policy, from which a gradient estimate is obtained to conduct parameter updates. In the simplest setting of a stochastic bandit problem , where decisions matter only for one step, the REINFORCE policy gradient method becomes equivalent to the stochastic gradient bandit algorithm [31, Section 2.8]. Compared to other statistical methods, such as the upper confidence bound algorithm (UCB, ), and Thompson sampling (TS, ), the stochastic gradient bandit algorithm is conceptually simpler and more computationally efficient, as it does not calculate exploration bonuses nor posterior distributions. Moreover, the stochastic gradient method is highly scalable and naturally applicable to large scale neural networks .

However, unlike UCB or TS, the stochastic gradient bandit algorithm does not have an equivalently well established and comprehensive theoretical footing. Given its pervasive success and widespread application in RL  and fine-tuning for large language models ), it remains an important question to understand the success of stochastic gradient based algorithms in bandit-like settings, not only to bridge the gap between theory and practice, but also to identify more effective and robust variants. In this paper, we make a significant contribution to the theoretical understanding of the stochastic gradient bandit algorithm, bringing its justification closer to that of other less scalable but theoretically well established methods. In particular, we establish the surprising result that:

_For any constant learning rate \(>0\), the stochastic gradient bandit algorithm is guaranteed to converge to the globally optimal policy almost surely._Since learning rate is the only tuning parameter in the stochastic gradient bandit algorithm, this result offers a remarkable robustness for the method, that it converges to a near optimal policy, irrespective of the value of this hyperparameter! Analysis of this algorithm is challenging because it requires techniques for simultaneously handling non-convex optimization, stochastic approximation, and the exploration-exploitation trade-off. Prior theoretical work on the stochastic gradient algorithm has primarily focused on non-convex optimization and stochastic approximation, but understanding the simultaneous effect on exploration has been largely lacking.

Recently, significant progress has been made in establishing global convergence results for policy gradient (PG) methods. For example, it has been shown that using exact gradients, Softmax PG converges to a globally optimal policy asymptotically as the number of iterations \(t\) goes to infinity . Subsequent work has demonstrated that the asymptotic rate of convergence is \(O(1/t)\), albeit with problem and initialization dependent constants [22; 17]. The rate and constant dependence in the true gradient setting have been improved via several techniques, including entropy regularization , normalization , and using natural gradient (mirror descent) [1; 6; 15].

Unfortunately, in the online stochastic setting, where the policy gradient has to be estimated using the current policy to collect samples, these accelerated methods all obtain worse asymptotic results than the standard Softmax PG , failing to converge to a global optimum without careful design choices . Yet in the same setting, standard Softmax PG has been shown to succeed in its simplest form, provided only that a sufficiently small learning constant rate \((1)\) is used .

For stochastic gradient based methods, decaying or sufficiently small learning rates are used by almost all current approaches, motivated by classical convergence analyses from stochastic optimization [28; 12; 39; 38; 9; 37; 36; 24; 8]. Stationary point convergence is guaranteed for learning rates sufficiently small with respect to the smoothness of the objective function, while also decaying to zero at a precise rate if noise in the gradient estimator persists. In addition to appropriate learning rate control, many other techniques have been developed to control the effects of gradient noise, including regularization [38; 9], variance reduction , and carefully considering growth conditions [36; 24].

The technical challenges we face in the current study can be understood in the following aspects: **(1)** Using an arbitrarily large constant learning rate for online stochastic gradient optimization immediately renders the smoothness and noise control techniques mentioned above inapplicable. **(2)** With any constant learning rate \(>0\), the question of whether oscillation or convergence will ultimately occur needs to be addressed before even considering whether any convergence is to a global optimum. This additional level of complexity arises because the optimization objective is not necessarily improved monotonically in expectation. Finally, **(3)** The gradient bandit algorithm does not use any exploration bonus, which means that new techniques are required to demonstrate that it adequately balances the exploration-exploitation trade-off.

In this paper, we resolve the above difficulties by uncovering intriguing exploration properties of stochastic gradient when using any constant learning rate. In particular, we establish the following.

**(i)**: In the stochastic online setting, with probability \(1\), the stochastic gradient bandit algorithm will not keep sampling any single action forever, implying that it will exhibit a minimal form of exploration without any further modification. This asymptotic event (as \(t\)) happens with probability \(1\) and holds for any constant learning rate \(>0\).
**(ii)**: This result can then be leveraged to show that, as a consequence, given any constant learning rate, the stochastic gradient bandit algorithms will converge to the globally optimal policy as \(t\), with probability \(1\). That is, the probability of sub-optimal actions decays to \(0\), even though some of them are taken infinitely often asymptotically.

## 2 Setting and Background

We consider the stochastic multi-armed bandit problem , specified by \(K\) actions and a true mean reward vector \(r^{K}\), where for each action \(a[K]\{1,2,,K\}\),

\[r(a)=_{-R_{}}^{R_{}}x P_{a}(x)(dx),\] (1)

where \(R_{}>0\) is the reward range, \(\) is a finite measure over \([-R_{},R_{}]\), and \(P_{a}(x) 0\) is the probability density function with respect to \(\). We use \(R_{a}\) to denote the reward distribution for action \(a\) defined by the density \(P_{a}\) and base measure \(\). The goal is to find a policy \(_{}^{K}\) to achieve high expected reward,

\[_{^{K}}_{}^{}r,\] (2)

where \(_{}\) is parameterized by \(^{K}\).

**The gradient bandit algorithm.** A natural idea to optimize Eq. (2) is to use stochastic gradient ascent, which is shown in Algorithm 1 and known as the gradient bandit algorithm [31, Section 2.8]. In Algorithm 1, in each iteration \(t 1\), the probability of pulling arm \(a[K]\) is given as

\[_{_{t}}(a)=[(_{t})](a) (a)\}}{_{a^{}[K]}\{_{t}(a^{ })\}},a[K],\] (3)

where \(_{t}^{K}\) is the parameter vector to be updated. The following proposition shows that Algorithm 1 is an instance of stochastic gradient ascent with an unbiased gradient estimator .

**Proposition 1** (Proposition 2.3 of ).: _Algorithm 1 is equivalent to the following update,_

\[_{t+1}_{t}+}^{} _{t}}{d_{t}}=_{t}+((_{_ {t}})-_{_{t}}_{_{t}}^{})_{t},\] (4)

_where \(_{t}}^{}_{t}}{d_{t} }=}^{}r}{d_{t}}\), and \(_{t}[]\) is defined with respect to randomness from on-policy sampling \(a_{t}_{_{t}}()\) and reward sampling \(R_{t}(a_{t}) P_{a_{t}}\). The Jacobian of \(_{}()\) is \(}{d}^{}=(_{} )-_{}_{}^{}^{K K}\), and \(_{t}(a)[a_{t}=a)}{_{_{t}}(a)} R _{t}(a)\) for all \(a[K]\) is the importance sampling (IS) estimator, and we set \(R_{t}(a)=0\) for all \(a a_{t}\)._

``` Input: initial parameters \(_{1}^{K}\), learning rate \(>0\). Output: policies \(_{_{t}}=(_{t})\). while\(t 1\)do  Sample an action \(a_{t}_{_{t}}()\) and observe reward \(R_{t}(a_{t}) P_{a_{t}}\). for all \(a[K]\)do if\(a=a_{t}\)then \(_{t+1}(a)_{t}(a)+(1-_{_{t}}(a)) R _{t}(a_{t})\). else \(_{t+1}(a)_{t}(a)-_{_{t}}(a) R_{t} (a_{t})\). endif endfor endwhile ```

**Algorithm 1** Gradient bandit algorithm (without baselines)

**Known results on the convergence of the gradient bandit algorithm.** Since Eq. (2) corresponds to a smooth non-concave maximization problem over \(^{K}\), using Algorithm 1 with decaying learning rates is sufficient to guarantee convergence to a stationary point . However, this is insufficient to ensure the globally optimal solution of Eq. (2) is reached, since there exist multiple stationary points. More recently, guarantees of convergence to a globally optimal policy have been developed for PG methods in the true gradient setting , where the algorithm has access to exact mean rewards. These results were later extended to achieve global convergence guarantees (almost surely) in the stochastic setting . However, these extended results have required decaying or sufficiently small learning rates, motivated by exploiting smoothness and combating the inherent noise in stochastic gradients.

Despite these previous assumptions, there exists empirical and theoretical evidence that using a large learning rate in the stochastic gradient bandit algorithm is a viable option. For example, it has been observed that softmax policies learn even with extremely large learning rates such as \(2^{14}\). For logistic regression on linearly separable data, the objective has an exponential tail and the minimizer is unbounded, yet it has been shown that gradient descent with iteration dependent learning rate \((t)\) achieves accelerated \(O(1/t^{2})\) convergence . Though the objective in Eq. (2) hassimilar properties, unlike logistic regression, the problem we are considering is non-concave, so the same techniques cannot be directly applied. The most related results are from , which proved that with a small problem specific constant learning rate, Algorithm 1 achieves convergence to a globally optimal policy almost surely. However, as mentioned, the learning rate choices in  rely on assumptions of (non-uniform) smoothness and noise growth conditions (their Lemmas 4.2, 4.3, and 4.6), which cannot be directly applied here for a large learning rate.

Consequently, the use of large learning rates appear to render existing results and techniques inapplicable. Furthermore, with a large constant learning rate, it is unclear whether Algorithm 1 will converge to any stationary point, or the iterates will keep oscillating. If the algorithm does converge, it is also not clear what effect large step-sizes have on exploration, and whether the algorithm will converge to the optimal arm in such cases. Resolving these questions requires new results that characterize the behavior of Algorithm 1, since the classical optimization and stochastic approximation convergence theories are no longer applicable, as explained.

## 3 Asymptotic Global Convergence of Gradient Bandit Algorithm

We have seen that solving the non-concave maximization problem Eq. (2) using Algorithm 1 with any constant (potentially large) learning rate requires ideas beyond classical optimization theory. Here, we take a different perspective to investigate how Algorithm 1 samples actions. For analysis, we make the following assumption about the reward distribution.

**Assumption 1** (True mean reward has no ties).: _For all \(i,j[K]\), if \(i j\), then \(r(i) r(j)\)._

**Remark 1**.: _Removing Assumption 1 remains an open question for future work, while we believe that Algorithm 1 works without Assumption 1. One piece of evidence to support this conjecture is that even in the exact gradient setting, the set of initializations where Softmax PG approaches non-strict one-hot policies has zero measure._

### Failure Mode of Aggressive Updates

It has been observed that several accelerated PG methods in the true gradient setting, including natural PG  and normalized PG , obtain worse results than standard softmax PG if combined with online sampling \(a_{t}_{_{t}}()\) using constant learning rates . The failure mode in these cases is that the update is too aggressive and commits to a sub-optimal arm without sufficiently exploring all arms. This results in a non-trivial probability of sampling one action forever, i.e., there exists a potentially sub-optimal action \(a[K]\), such that with some constant probability, \(a_{t}=a\) for all \(t 1\). Such an outcome implies that \(_{_{t}}(a) 1\) as \(t\)[20, Theorem 3]. Since \(a[K]\) could be a sub-optimal action with \(r(a)<r(a^{*})=_{a[K]}r(a)\), this results in a lack of exploration, and consequently, methods such as natural PG and normalized PG are not guaranteed to converge to the optimal action \(a^{*}_{a[K]}r(a)\) with probability \(1\).

### Stochastic Gradient Automatically Avoids Lack of Exploration

Our first key finding is that Algorithm 1 does not keep sampling one action forever, no matter how large the constant learning rate is. This property avoids the problem of a lack of exploration, in the sense that Algorithm 1 will at least explore more than one action infinitely often. At first glance, this might not seem like a strong property, since the algorithm might somehow explore only sub-optimal actions forever. However, we will argue below that this property coupled with additional arguments is sufficient to guarantee convergence to the globally optimal policy.

Let us now formally prove the above property. By Algorithm 1, for all \(a[K]\), for all \(t 1\),

\[_{t+1}(a)_{t}(a)+\{(1- _{_{t}}(a)) R_{t}(a),&a_{t}=a,\\ -_{_{t}}(a) R_{t}(a_{t}),&.\] (5)

We define \(N_{t}(a)\) as the number of times action \(a[K]\) is sampled up to iteration \(t 1\), i.e.,

\[N_{t}(a)_{s=1}^{t}\,\{a_{s}=a\},\] (6)and its asymptotic limit \(N_{}(a)_{t}N_{t}(a)\), which could possibly be infinity. For all \(a[K]\), we have either \(N_{}(a)=\) or \(N_{}(a)<\), meaning that \(a[K]\) is sampled infinitely often or only finitely many times asymptotically. First, we prove the following Lemma 1, which shows that if an action \(a[K]\) is sampled only finitely many times as \(t\), then the parameter corresponding to action \(a\) is also finite, i.e., \(_{t 1}|_{t}(a)|<\).

**Lemma 1**.: _Using Algorithm 1 with any constant \((1)\), if \(N_{}(a)<\) for an action \(a[K]\), then we have, almost surely,_

\[_{t 1}_{t}(a)<,_{t 1}_{t}(a)>-.\] (8)

Lemma 1 will be used multiple times in the subsequent convergence arguments.

Proof sketch.Since we assume action \(a[K]\) is sampled finitely many times, the update given in the case depicted by Eq. (5) happens finitely many times. Each update is bounded since the sampled reward is in \([-R_{},R_{}]\) by Eq. (1), and the learning rate is a constant, i.e., \((1)\). In Algorithm 1, \(_{t}(a)\) is still updated even when \(a_{t} a\), with the corresponding update given by the case depicted by Eq. (6). Therefore, whether \(_{t}(a)\) is bounded depends on the cumulative probability \(_{s=1}^{t}_{_{s}}(a)\) being summable as \(t\). According to the extended Borel-Cantelli lemma (Lemma 3), we have, almost surely,

\[_{t 1}_{_{t}}(a)=}=\{N_{}(a)= \},\] (9)

which implies (by taking complements) that \(_{t 1}_{_{t}}(a)<\) if and only if \(N_{}(a)<\). Therefore, if \(a[K]\) is sampled finitely often, \(_{t}(a)\) will be updated in a bounded manner (using Eqs. (5) and (6)) as \(t\), hence establishing Lemma 1. Detailed proofs for this lemma, as well as for all other results in this paper can be found in the appendix.

Given Lemma 1, we can then establish the above-mentioned finding about the exploration effect of Algorithm 1 in Lemma 2.

**Lemma 2** (Avoiding a lack of exploration).: _Using Algorithm 1 with any \((1)\), there exists at least a pair of distinct actions \(i,j[K]\) and \(i j\), such that, almost surely,_

\[N_{}(i)=,N_{}(j)=.\] (10)

Proof sketch.The argument for the existence of one such action is straightforward, since by the pigeonhole principle, if there are finitely many actions, i.e., \(K<\), there must be at least one action \(i[K]\) that is sampled infinitely often as \(t\).

The argument for the existence of a second such action is by contradiction. Suppose that all the other actions \(j[K]\) with \(j i\) are sampled only finitely many times as \(t\). According to Lemma 1, their corresponding parameters must remain finite, i.e., \(_{t 1}|_{t}(j)|<\) for all \(j[K]\) with \(j i\). Now consider \(_{t}(i)\). By assumption, the second update case for this parameter, Eq. (6), happens only finitely often, since Eq. (6) can only occur when \(a_{t} i\). Therefore, the key question is whether the cumulative probability \(_{s=1}^{t}(1-_{_{s}}(i))\) involved in the first case of the update, Eq. (5), is summable as \(t\). Note that \(_{s=1}^{t}(1-_{_{s}}(i))=_{s=1}^{t}_{j i }_{_{s}}(j)\), which is indeed summable as \(t\), by the assumption and Eq. (9). This implies that action \(i\), which is sampled infinitely often, achieves a parameter magnitude, \(_{t 1}|_{t}(i)|<\), that remains bounded as \(t\). Using the softmax parameterization Eq. (3) in the above argument, we conclude that for all \(a[K]\), \(_{t 1}_{_{t}}(a)>0\), i.e., every action's probability remains bounded away from zero, and hence is not summable. Using Eq. (9), this implies that every action is sampled infinitely often, which contradicts the assumption that only action \(i\) is sampled infinitely often as \(t\).

Discussion.Lemma 2 implies that Algorithm 1 is not an aggressive method in the sense of , no matter how large the learning rate is, as long as it is constant, i.e., \((1)\). According to [20, Theorem 7], even if we fix the sampling in Algorithm 1 to a sub-optimal action \(a[K]\) forever, i.e., \(a_{t}=a\) for all \(t 1\), its probability will not approach \(1\) faster than \(O(1/t)\), i.e., \(1-_{_{t}}(a)(1/t)\). This means that there must be at least one another action \(a^{}[K]\) with \(a^{} a\), such that \(a^{}\) will also be sampled infinitely often. A more intuitive explanation is that the \((1-_{_{t}}(a))\) term in Eq. (5) will be near \(0\), which slows the speed of committing to a deterministic policy on \(a\) whenever \(_{_{t}}(a)\) is close to \(1\), which encourages exploration. Such natural exploratory behavior arises in Algorithm 1 because of the softmax Jacobian \((_{})-_{}_{}^{}\) in the update shown in Proposition 1, which determines the growth order of \(_{t}(a)\) for all \(a[K]\) as \(t\), making the effect of a constant learning rate \((1)\) asymptotically inconsequential.

### Warm up: Global Asymptotic Convergence when \(K=2\)

We now consider the simplest case, where we have only two possible actions. According to Lemma 2, each of the two actions must be sampled infinitely often as \(t\). We now illustrate the second key result, that for both actions \(a[K]\), the random sequence \(\{_{t}(a)\}_{t 1}\) follows the direction of the expected gradient for sufficiently large \(t 1\) almost surely. The proof uses a technique that has been previously used in [19; 24] for small learning rates, but here we observe that the same technique continues to work for Algorithm 1 no matter how large the learning rate is, as long as \((1)\).

**Theorem 1**.: _Let \(K=2\) and \(r(1)>r(2)\). Using Algorithm 1 with any \((1)\), we have, almost surely, \(_{_{t}}(a^{*}) 1\) as \(t\), where \(a^{*}_{a[K]}r(a)\) (equal to Action \(1\) in this case)._

Proof sketch.According to Lemma 2, \(N_{}(1)=N_{}(2)=\). Denote the the reward gap as \( r(a^{*})-_{a a^{*}}r(a)>0\), which becomes \(=r(1)-r(2)\) for two actions. Since the stochastic gradient is unbiased (Proposition 1), we have, for all \(t 1\) (detailed calculations omitted),

\[_{t}[_{t+1}(a^{*})] =_{t}(a^{*})+_{_{t}}(a^{*})r( a^{*})-_{_{t}}^{}r\] (11) \[=_{t}(a^{*})+_{_{t}}(a^{*}) (1-_{_{t}}(a^{*}))>_{t}(a^{*}).\] (12)

A similar calculation shows that,

\[_{t}[_{t+1}(2)]=_{t}(2)-_{_{t}}(2) (1-_{_{t}}(2))<_{t}(2),\] (13)

which means that \(_{t}(a^{*})\) is monotonically increasing in expectation and \(_{t}(2)\) is monotonically decreasing in expectation. In other words, \(\{_{t}(a^{*})\}_{t 1}\) is a sub-martingale, while \(\{_{t}(2)\}_{t 1}\) is a super-martingale. However, since \(_{t}^{K}\) is unbounded, Doob's martingale convergence results cannot be directly applied, so we pursue a different argument. Following [19; 24], given an action \(a[K]\), we define \(P_{t}(a)_{t}[_{t+1}(a)]-_{t}(a)\) as the "progress", and define \(W_{t}(a)_{t}(a)-_{t-1}[_{t}(a)]\) as the "noise", where \(_{t}(a)=W_{t}(a)+P_{t-1}(a)+_{t-1}(a)\). By recursion we can determine that,

\[_{t}(a)=[_{1}(a)]+_{s=1}^{t}W_{s}(a)+_{s=1}^{t- 1}P_{s}(a),\] (14)

i.e., \(_{t}(a)\) is the result of "cumulative progress" and "cumulative noise". According to [24, Theorem C.3], the cumulative noise term can be bounded by using martingale concentration, where the order of the corresponding confidence interval is smaller than the order of the cumulative progress. Therefore, the summation will always be determined by the cumulative progress as \(t\). According to the calculations in Eqs. (12) and (13), we have \(P_{t}(a^{*})>0\) and \(P_{t}(2)<0\), both of which are not summable. As a result, \(_{t}(a^{*})\) and \(_{t}(2)-\) as \(t\), which implies that \(}(a^{*})}{_{_{t}}(2)}=\{_{t}(a^{*})- _{t}(a)\}\), hence \(_{_{t}}(a^{*}) 1\) as \(t\).

### Global Asymptotic Convergence for all \(K 2\)

The illustrative two-action case shows that if \(_{_{t}}^{}r(r(2),r(a^{*}))\) and if both actions are sampled infinitely often, then we have, almost surely \(_{t}(a^{*})\) and \(_{t}(2)-\) as \(t\). However, the question at the beginning of Section 3.2 remains: when \(K>2\), if the two actions sampled infinitely often in Lemma 2 are both sub-optimal, will that result in a similar failure mode to the one described in Section 3.1? The answer is no, which follows from our third key finding, which is based on another contradiction-based argument that establishes almost sure convergence to a globally optimal policy in the general \(K>2\) case.

**Theorem 2**.: _Given \(K 2\), using Algorithm 1 with any \((1)\), we have, almost surely, \(_{_{t}}(a^{*}) 1\) as \(t\), where \(a^{*}=_{a[K]}r(a)\) is the optimal action._Proof sketch.We consider two cases: \(N_{}(a^{*})<\) and \(N_{}(a^{*})=\), corresponding to whether the optimal action is sampled finitely or infinitely often as \(t\). We argue that the first case (\(N_{}(a^{*})<\)) is impossible, while for the second case (\(N_{}(a^{*})=\)) we prove that \(_{t}(a^{*})-_{t}(a)\) for all \(a[K]\) with \(r(a)<r(a^{*})\), which implies \(_{_{t}}(a^{*}) 1\) as \(t\).

_First case._ Suppose that \(N_{}(a^{*})<\). We argue that this is impossible via contradiction. Given the assumption and Lemma 2 we know there must be at least two other sub-optimal actions \(i_{1},i_{2}[K]\), \(i_{1} i_{2}\), such that \(N_{}(i_{1})=N_{}(i_{2})=\). In particular, let \(i_{1}=_{a[K],N_{}(a)=}r(a)\) and \(i_{2}=_{a[K],N_{}(a)=}r(a)\), hence \(r(i_{1})<r(i_{2})<r(a^{*})\). By Lemma 6 (see Appendix) we will also have \(r(i_{1})<_{_{t}}^{}r<r(i_{2})\) for sufficiently large \(t 1\), which implies for action \(i_{1}\),

\[_{t}[_{t+1}(i_{1})]=_{t}(i_{1})+_{_{ t}}(i_{1})r(i_{1})-_{_{t}}^{}r)<_{t}(i_{1}),\] (15)

for sufficiently large \(t 1\), which further implies that \(_{t 1}_{t}(i_{1})<\). Meanwhile, for the optimal action \(a^{*}\), the assumption and Lemma 1 imply that \(_{t 1}_{t}(a^{*})>-\). Combining these two observations gives,

\[_{t 1}}(i_{1})}{_{_{t}}(a^{*})}=_{t  1}\,\{_{t}(i_{1})-_{t}(a^{*})\}<\,.\] (16)

On the other hand, since \(N_{}(i_{1})=\) and \(N_{}(a^{*})<\) (by assumption), we then have \(_{t 1}\{_{t}(i_{1})-_{t}(a^{*})\}=\) by Lemma 5 (see Appendix), which contradicts Eq. (16).

_Second case._ Suppose that \(N_{}(a^{*})=\). We will argue that \(_{_{t}}(a^{*}) 1\) as \(t\) almost surely. First, according to Lemma 2, there exists at least one sub-optimal action \(i_{1}[K]\), \(i_{1} a^{*}\), such that \(N_{}(i_{1})=\). Let \(i_{1}=_{a[K],N_{}(a)=}r(a)\). By Lemma 6 and the definition of \(a^{*}\), we have \(r(i_{1})<_{_{t}}^{}r<r(a^{*})\) for all sufficiently large \(t 1\). Since \(N_{}(i_{1})=\), using similar calculations to Eqs. (13) and (14) in Theorem 1, we have, \(_{t}(i_{1})-\) as \(t\). We also have, \(_{t 1}_{t}(a^{*})>-\) as \(t\). Hence, \(}(a^{*})}{_{_{t}}(i_{1})}=\{_{t}(a^{* })-_{t}(i_{1})\}\) as \(t\).

Define \(_{}:=\{a[K] N_{}(a)=\}\) as the set of actions that are sampled infinitely often, and note that \(|_{}| 2\) by Lemma 2. Sort the action indices in \(_{}\) according to their expected reward values in descending order, i.e.,

\[r(a^{*})>r(i_{|_{}|-1})>r(i_{|_{}|-2})> >r(i_{2})>r(i_{1}).\] (17)

Assumption 1 is used here to prevent two arms from having the same reward and thus guarantee the inequalities are strict in Eq. (17). Next, using similar calculations as in Lemma 6, we have,

\[_{_{t}}^{}r-r(i_{2})>_{_{t}}(a^{*})r(a^{*} )-r(i_{2})-_{a^{-}^{-}(i_{2})}}(a^{-} )}{_{_{t}}(a^{*})}(r(i_{2})-r(a^{-})),\] (18)

where \(^{-}(i_{2})\{a^{-}[K]:r(a^{-})<r(i_{2})\}\) is the set of actions that have lower mean reward than \(i_{2}[K]\), and note that \(i_{1}^{-}(i_{2})\). Using the above definitions, we can conclude that \(i_{1}\) is the only arm in \(^{-}(i_{2})\) that has been sampled infinitely often. According to Lemma 5, for all \(a^{-}^{-}(i_{2})\) with \(a^{-} i_{1}\), we have, \(}(a^{*})}{_{_{t}}(a^{-})}\) as \(t\), since \(N_{}(a^{*})=\) (by assumption) and \(N_{}(a^{-})<\) (by Eq. (17)). Therefore, for all sufficiently large \(t\), the probability ratio in Eq. (18) \(}(a^{*})}{_{_{t}}(a^{-})}\) for all \(a^{-}^{-}(i_{2})\), which implies that, for all sufficiently large \(t 1\),

\[_{_{t}}^{}r-r(i_{2})>0.5_{_{t}}(a^{*})r( a^{*})-r(i_{2})>0.\] (19)

We have thus shown that \(_{_{t}}^{}r>r(i_{2})\). Recall that we had previously proved that \(_{_{t}}^{}r>r(i_{1})\). Hence, we will apply this argument recursively: after this point, \(i_{2}[K]\) will become the new "\(i_{1}[K]\)", and a similar inequality to Eq. (13) will then hold for \(i_{2}[K]\) from similar calculations to Eqs. (13) and (14) in Theorem 1, establishing \(_{t}(i_{2})-\) as \(t\). This will imply that for all sufficiently large \(t 1\),

\[_{_{t}}^{}r-r(i_{3})>0.5_{_{t}}(a^{*})r( a^{*})-r(i_{3})>0.\] (20)

Continuing the recursive argument, we can conclude for all actions \(a_{}\) with \(a a^{*}\) that \(}(a^{*})}{_{_{t}}(a)}\) as \(t\). Meanwhile, for all actions \(a_{}\), Lemma 5 also shows that \(}(a^{*})}{_{_{t}}(a)}\) as \(t\). Combining these two results yields the conclusion that for all sub-optimal actions \(a[K]\) with \(r(a)<r(a^{*})\) we have \(}(a^{*})}{_{_{t}}(a)}\) as \(t\), which implies \(_{_{t}}(a^{*}) 1\) as \(t\). Thus, we have established almost sure convergence to the globally optimal policy.

Discussion.Lemma 2 is important to prove that the optimal arm will be sampled infinitely often. In particular, Lemma 2 guarantees \(|_{}| 2\) and the existence of \(i_{1}\) in Eq. (15), which can then be used to construct the contradiction in Eq. (16). Without Lemma 2, \(|_{}|\) might be equal to \(1\) and the failure mode in Section 3.1 can occur, resulting in Algorithm 1 not sampling the optimal action infinitely often as \(t\).

### Asymptotic Rate of Convergence

According to Theorem 2, almost surely, \(_{_{t}}(a^{*}) 1\) as \(t\). Therefore, after a large enough time \(<\), we have \(_{_{t}}(a^{*}) 1/2\), which implies that the "progress" term in Eq. (14) can be lower bounded. With this, an asymptotic rate of convergence can be proved as follows.

**Theorem 3**.: _For a large enough \(>0\), for all \(T>\), the average sub-optimality decreases at an \(O()\) rate. Formally, if \(a^{*}\) is the optimal arm, then, for a constant \(c\),_

\[^{T}r(a^{*})-_{s},r}{T}.\]

## 4 Simulation Study

To validate and enhance the theoretical findings, we ran experiments with a four action bandit environment (\(K=4\)) with a true mean reward vector of \(r=(0.2,0.05,-0.1,-0.4)^{}^{4}\). The reward distribution \(P_{a}\) for arm \(1 a 4\) is Gaussian, centered at \(r(a)\) and with a standard deviation of \(0.1\). The environment is chosen to illustrate various phenomenon, which we discuss after presenting the results. The algorithm is Algorithm 1 with \(_{1}=^{K}\).

For comparison, assuming that the random rewards belong to the \([-1,1]\) interval, the only result for the stochastic gradient bandit algorithm (Zu and others, 2014, Lemma 4.6) that allowed a constant learning rate required that the learning rate be less than \(_{c}=}{40 K^{3/2} R_{}^{3}}= 0.00007\), where we used \(=0.15\), \(K=4\), and \(R_{}=1\). While technically, the result does not apply to our case where the reward distributions have unbounded support, the probability of the reward landing outside of \([-1,1]\) is in the order of \(10^{-9}\). Choosing \(R_{}\) to be larger, this probability falls extremely quickly, which suggests that the above threshold is generous. For the experiments we use the learning rates \(\{1,10,100,1000\}\), that are several orders of magnitudes larger than \(_{c}\). For each learning rate, we plot the outcome of \(10\) runs, corresponding to different random seeds. Each run lasts \(10^{6}( e^{14})\) iterations. The log-suboptimality gaps for the \(4 10\) cases are shown on Figures (a)a-(d)d, where they are plotted against the logarithm of time. Additional results for \(K=2\) arms are shown in Appendix D. Note that in this example small sub-optimality implies that the optimal arm is chosen with high probability. In what follows, we discuss the results in the plots.

Asymptotic convergence.For the smaller learning rates of \(=1\) and \(10\), all \(10\) seeds rapidly and steadily converge, reaching a sub-optimality of \(e^{-14}\) or less. For \(=100\) and \(1000\), most of the runs reach even small error even faster, but some runs are "stuck" even after \(10^{6}\) steps. Note that this does not contradict the theoretical result; nor do we suspect numerical issues. As seen for the case of \(=100\), even after a long phase with little to no progress, a run can "recover" (see the grey curve). In fact, it is reasonable to expect that the price of increasing the learning rate is larger variance; as seen in these plots (subplots (a) and (b) are also attesting to this). Differences between learning rates are further discussed below.

Non-monotone objective value.Using a very small learning rate guarantees monotonic improvement (in expectation) in the policy's expected reward . Conversely, a large learning rate results in non-monotonic evolution of the expected rewards \(\{_{_{t}}^{}r\}_{t 1}\), even in the final stages of convergence, as can be seen clearly for the learning rates of \(=1\) and \(10\) in Figures (a)a and (b)b. For larger \(\), the non-monotone behavior happens over longer periods and is less visible in the plots. This is because using large learning rates causes the policy to rapidly increase the parameters for some action, after which the gradient becomes small, limiting further progress.

Rate of convergence.Figures (a)a and (b)b, where the log-log plot has a slope of nearly \(-1\), give some evidence that an \(O(1/t)\) asymptotic rate is achieved. In general, such a rate cannot be improvedin terms of \(t\). Theorem 3 gives a weaker version of convergence rate over averaged iterates (not last iterate), which is slightly worse than \(O(1/t)\). More work is needed to verify if the asymptotic convergence rate in Theorem 3 is improvable or not.

Different learning rates.Two observations can be made from Figure 1 regarding the effect of using different \(\) values: **First**, during the final stage of convergence when \(r(a^{*})-_{_{t}}^{}r 0\), using larger \(\) results in faster convergence on average. As \(\) increases, the order of \((r(a^{*})-_{_{t}}^{}r)\) also changes from \(e^{-14}\) (\(=1\)), to \(e^{-20}\) (\(=100\)), and \(e^{-200}\) (\(=1000\)). We conjecture that the asymptotic rate of convergence has an \(O(1/)\) dependence. **Second**, using larger learning rates can take a longer time to enter the final stage of convergence. When \(=1\) or \(10\), all curves quickly enter the final stage of \(r(a^{*})-_{_{t}}^{}r 0\). However, for larger \(\) values, \(1/10\) runs (\(=100\)) and \(3/10\) runs (\(=1000\)) result in \(r(a^{*})-_{_{t}}^{}r\) values far from \(0\) even after \(10^{6}\) iterations. These runs take orders of magnitude more iterations to eventually achieve \(r(a^{*})-_{_{t}}^{}r 0\). These situations correspond to the policy \(_{_{t}}\) getting stuck near sub-optimal corners of the simplex, meaning that \(_{_{t}}(i) 1\) for a sub-optimal action \(i[K]\) with \(r(i)<r(a^{*})\). In such cases, even Softmax PG with the true gradient can remain stuck on a sub-optimal plateau for an extremely long time . However, the reason why larger learning rates lead to longer plateaus in the stochastic setting remains unclear.

Trade-offs and multi-stage characterizations of convergence.Given the above observations, there appears to exist a trade-off for \(\): larger \(\) values result in faster convergence during the final stage where \(r(a^{*})-_{_{t}}^{}r 0\), but at the the cost of taking far longer to enter this final stage of convergence. Since asymptotic convergence results are insufficient for explaining these subtleties in a satisfactory manner, a more refined analysis that considers the different stages of convergence is required.

Figure 1: Log sub-optimality gap, \((r(a^{*})-_{_{t}}^{}r)\), plotted against the logarithm of time, \( t\), in a \(4\)-action problem with various learning rates, \(\). Each subplot shows a run with a specific learning rate. The curves in a subplot correspond to 10 different random seeds. Theory predicts that essentially all seeds will lead to a curve converging to zero (\(-\) in these plots). For a discussion of the results, see the text.,

Conclusions and Future Directions

This work refines our understanding of stochastic gradient bandit algorithms by proving that it converges to a globally optimal policy almost surely with _any_ constant learning rate. Our new proof strategy based on the asymptotics of sample counts opens new directions for better characterizing exploration effects of stochastic gradient methods, while also suggesting interesting new questions. Characterizing the multiple stages of convergence remains another interesting future direction. One interesting possibility is that there might exist an optimal time-dependent scheme for _increasing_ the learning rate (such as \( O( t)\)) to accelerate convergence, rather than use a constant \( O(1)\). This is corroborated by our experiments: As seen in Figure 1, small learning rates perform better during the early stages of optimization, while larger learning rates achieve faster convergence during the final stage. Other directions include extending our bandit results to the more general RL setting , as well as extending our results for the softmax tabular parameterization to handle function approximation .

**Limitations:** While this work establishes a surprising asymptotic convergence result for any constant learning rate, it does not shed light on the effect of different learning rates on the convergence. Moreover, our analysis is limited to multi-armed bandits, and does not immediately extend to the general RL setting. These aspects are the main limitations of this paper.

**Broader impact:** This is primarily theoretical work on a fundamental algorithm that is used broadly in RL applications. We expect these results to improve the research community's understanding of the basic stochastic gradient bandit method.