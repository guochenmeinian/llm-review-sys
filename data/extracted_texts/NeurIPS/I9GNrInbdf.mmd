# Formulating Discrete Probability Flow Through Optimal Transport

Pengze Zhang

Sun Yat-sen University

zhangpz3@mail2.edu.cn

&Hubery Yin

WeChat, Tencent Inc.

hubery@tencent.com

&Chen Li

WeChat, Tencent Inc.

chaselli@tencent.com

&Xiaohua Xie

Sun Yat-sen University

xiaxiaoh6@mail.edu.cn

Equal contribution. This work was done when Pengze Zhang was an intern at WeChat.Corresponding author.

###### Abstract

Continuous diffusion models are commonly acknowledged to display a deterministic probability flow, whereas discrete diffusion models do not. In this paper, we aim to establish the fundamental theory for the probability flow of discrete diffusion models. Specifically, we first prove that the continuous probability flow is the Monge optimal transport map under certain conditions, and also present an equivalent evidence for discrete cases. In view of these findings, we are then able to define the discrete probability flow in line with the principles of optimal transport. Finally, drawing upon our newly established definitions, we propose a novel sampling method that surpasses previous discrete diffusion models in its ability to generate more certain outcomes. Extensive experiments on the synthetic toy dataset and the CIFAR-10 dataset have validated the effectiveness of our proposed discrete probability flow. Code is released at: https://github.com/PangzeCheung/Discrete-Probability-Flow.

## 1 Introduction

The emerging diffusion-based models  have been proven to be an effective technique for modeling data distribution, and generating high-quality texts , images  and videos . Considering their generative capabilities have surpassed the previous state-of-the-art results achieved by generative adversarial networks , there has been a growing interest in exploring the potential of diffusion models in various advanced applications .

Diffusion models are widely recognized for generating samples in a stochastic manner , which complicates the task of defining an encoder that translates a sample to a certain latent space. For instance, by following the configuration proposed by , it has been observed that generated samples from any given initial point have the potential to span the entire support of the data distribution. To achieve a deterministic sampling process while preserving the generative capability, Song _et al._ proposed the probability flow, which provides a deterministic map between the data space and the latent space for continuous diffusion models. Unfortunately, the situation differs when it comes to discrete models. For instance, considering two binary distributions \((P_{0}=,P_{1}=)\) and \((P_{0}=,P_{1}=)\), there is no deterministic map that can transform the former distribution to the latter one, as it would simply be a permutation. Although some previous research has beenconducted on discrete diffusion models with discrete [24; 23; 4; 12; 9; 26; 16] and continuous [6; 47] time configurations, these works primarily focus on improving the sampling quality and efficiency, while sampling certainty has received less attention. More specifically, there is a conspicuous absence of existing literature addressing the probability flow in discrete diffusion models.

The aim of this study is to establish the fundamental theory of the probability flow for discrete diffusion models. Our paper contributes in the following ways. Firstly, we provide proof that under some conditions the probability flow of continuous diffusion coincides with the Monge optimal transport map during any finite time interval within the range of \((0,)\). Secondly, we propose a discrete analogue of the probability flow under the framework of optimal transport, which we have defined as the _discrete probability flow_. Additionally, we identify several properties that are shared by both the continuous and discrete probability flow. Lastly, we propose a novel sampling method based on the aforementioned observations, and we demonstrate its effectiveness in significantly improving the certainty of the sampling outcomes on both synthetic toy dataset and CIFAR-10 dataset.

Proofs for all Propositions are given in the Appendix. For consistency, the probability flow and infinitesimal transport of a process \(X_{t}\) is signified by \(_{t}\) and \(_{t}\) respectively.

## 2 Background on Diffusion Models and Optimal Transport

First of all, we review some important concepts from the theory of diffusion models, optimal transport and gradient flow.

### Continuous state diffusion models

Diffusion models are generative models that consist of a forward process and a backward process. The forward process transforms the data distribution \(p_{data}(x_{0})\) into a tractable reference distribution \(p_{T}(x_{T})\). The backward process then generates samples from the initial points drawn from \(p_{T}(x_{T})\). According to , the forward process is modeled as the (time-dependent) Ornstein-Uhlenbeck (OU) process:

\[dX_{t}=-_{t}X_{t}dt+_{t}dB_{t},\] (1)

where \(_{t} 0,_{t}>0, t 0\) and \(B_{t}\) is the Brownian Motion (BM). The backward process is the reverse-time process of the forward process :

\[dX_{t}=[-_{t}X_{t}-_{t}^{2}_{X_{t}} p(X_{t},t)]dt+ _{t}d_{t},\] (2)

where \(_{t}\) is the reverse-time Brownian motion and \(p(X_{t},t)\) is the single-time marginal distribution of the forward process, which also serves as the solution to the Fokker-Planck equation :

\[p(x,t)=_{t}_{x}(xp(x,t))+ _{t}^{2}_{x}p(x,t).\] (3)

In order to train a diffusion model, the primary objective is to minimize the discrepancy between the model output \(s_{}(x_{t},t)\) and the Stein score function \(s(x_{t},t)=_{x_{t}} p(x_{t},t)\). Song _et al._ demonstrate that, it is equivalent to match \(s_{}(x_{t},t)\) with the conditional score function:

\[^{*}=*{arg\,min}_{}_{t}\{_{t} _{x_{0},x_{t}}[\|s_{}(x_{t},t)-_{x_{t}} p(x_{ t},t|x_{0},0)\|^{2}]\},\] (4)

where \(_{t}\) is a weighting function, \(t\) is uniformly sampled over \([0,T]\) and \(p(x_{t},t|x_{0},0)\) is the forward conditional distribution.

It is noted that every Ornstein-Uhlenbeck process has an associated probability flow, which is a deterministic process that shares the same single-time marginal distribution . The probability flow is governed by the following Ordinary Differential Equation (ODE):

\[d_{t}=[-_{t}_{t}-_{t}^{2}s(_{t},t )]dt.\] (5)

In accordance with the global version of Picard-Lindelof theorem  and the adjoint method[36; 7], the map

\[T_{s,t}:^{n} ^{n},\] (6) \[_{s} _{t}.\]

is a diffeomorphism \( t s>0\). The diffeomorphism naturally gives a transport map.

### Discrete state diffusion models

In the realm of discrete state diffusion models, there are two primary classifications: the Discrete Time Discrete State (DTDS) models and the Continuous Time Discrete State (CTDS) models, which are founded on Discrete Time Markov Chains (DTMC) and Continuous Time Markov Chains (CTMC), correspondingly. Campbell _et al_. conducted a comparative analysis of these models and determined that CTDS outperforms DTDS. The DTDS models construct the forward process through the utilization of the conditional distribution \(q_{t+1|t}(x_{t+1}|x_{t})\) and employ a neural network to approximate the reverse conditional distribution \(q_{t|t+1}(x_{t}|x_{t+1})=(x_{t+1}|x_{t})q_{t}(x_{t})}{q_{t+1}( x_{t+1})}\). In practical applications, it is preferable to parameterize this model using \(p^{}_{0|t+1}\)[24; 4] and obtain \(p^{}_{k|k+1}\) through

\[ p^{}_{k|k+1}(x_{k}|x_{k+1})&= _{x_{0}}q_{k|k+1,0}(x_{k}|x_{k+1},x_{0})p^{}_{0|k+1}(x_{0}|x_{k+1}) \\ &=_{x_{0}}q_{k+1|k}(x_{k+1}|x_{k})(x_{k}|x_{0})} {q_{k+1|0}(x_{k+1}|x_{0})}p^{}_{0|k+1}(x_{0}|x_{k+1}).\] (7)

In contrast to DTDS models, a CTDS model is characterized by the (infinitesimal) generator , or transition rate, \(Q_{t}(x,y)\). The Kolmogorov forward equation  is:

\[q_{t|s}(x_{t}|x_{s})=_{y}q_{t|s}(y|x_{s})Q_{t} (y,x_{t}).\] (8)

The reverse process is:

\[q_{s|t}(x_{s}|x_{t})=_{y}q_{s|t}(y|x_{t})R_{t} (y,x_{s}).\] (9)

The generator of the reverse process can be written by [6; 47]:

\[R_{t}(y,x)=(x)}{q_{t}(y)}Q_{t}(x,y)=_{y_{0}}(x|y_{ 0})}{q_{t|0}(y|y_{0})}q_{0|t}(y_{0}|y)Q_{t}(x,y).\] (10)

There are various approaches to train the model, such as the Evidence Lower Bound (ELBO) technique , and the score-based approach . It has been observed that the reverse generator can be factorized over dimensions, allowing parallel sampling for each dimension during the reverse process. However, it is important to note that this independence is only possible when the time interval for each step is small.

### Optimal transport

The _optimal transport problem_ can be formulated in two primary ways, namely the Monge formulation and the Kantorovich formulation . Suppose there are two probability measures \(\) and \(\) on \((^{n},)\), and a cost function \(c:^{n}^{n}[0,+]\). The _Monge problem_ is

\[\,_{}\{ c(x,(x))\,(x): _{}=\}.\] (11)

The measure \(_{}\) is defined through \(_{}(A)=(^{-1}(A))\) for every \(A\) and is called the _pushforward_ of \(\) through T.

It is evident that the Monge Problem (MP) transports the entire mass from a particular point, denoted as \(x\), to a single point \((x)\). In contrast, Kantorovich provided a more general formulation, referred to as the _Kantorovich problem_:

\[\,_{}\{_{^{n}^{n}}c\, :(,)\},\] (12)

where \((,)\) is the set of _transport plans_, i.e.,

\[(,)=\{(^{n}^{n}): (_{x})_{}=,(_{y})_{}=\},\] (13)

where \(_{x}\) and \(_{y}\) are the two projections of \(^{n}^{n}\) onto \(^{n}\). For measures absolutely continuous with respect to the Lebesgue measure, these two problems are equivalent . However, when the measures are discrete, they are entirely distinct as the constraint of the Monge Problem may never be fulfilled.

### Fokker-Planck equation by gradient flow

According to , the Fokker-Planck equation represents the gradient flow of a functional in a metric space. In particular, for Brownian motion, its Fokker-Planck equation, which is also known as the heat diffusion equation, can be expressed as:

\[p(x,t)= p(x,t),\] (14)

and it represents the gradient flow of the Gibbs-Boltzmann entropy multiplied by \(-\):

\[-S(p)=_{^{n}}p(x) p(x)\,x.\] (15)

It is worth noting that Eq. 15 is the gradient flow of Eq. 14 under the _2-wasserstein metric_ (\(W_{2}\)).

Chow _et al._ have developed an analogue in the discrete setting by introducing the discrete Gibbs-Boltzmann entropy:

\[S(p)=_{i}p_{i}\;log\;p_{i},\] (16)

and deriving the gradient flow using a newly defined metric (Definition 1 in ). Since the discrete model is defined on graph \(G(V,E)\), where \(V=\{a_{1},...,a_{N}\}\) is the set of vertices, and \(E\) is the set of edges, the discrete Fokker-Planck equation with a constant potential can be written as:

\[p_{i}=_{j N(i)}p_{j}-p_{i},\] (17)

where \(N(i)=\{j\{1,2,...,N\}|\{a_{i},a_{j}\} E\}\) represents the one-ring neighborhood.

## 3 Continuous probability flow

### The equivalence of Ornstein-Uhlenbeck processes and Brownian motion

The diffusion models that are commonly utilized in machine learning are founded on Ornstein-Uhlenbeck processes. First of all, we demonstrate that it is feasible to deterministically convert a time-dependent Ornstein-Uhlenbeck process into a standard Brownian motion.

**Proposition 1**.: _Let \(X_{t}\) and \(Y_{t}\) be a time-dependent Ornstein-Uhlenbeck process and a Brownian motion respectively: \(dX_{t}=-_{t}X_{t}dt+_{t}dB_{t}^{(1)}\), \(dY_{t}=dB_{t}^{(2)}\), where \(B_{t}^{(1)}\) and \(B_{t}^{(2)}\) are two independent Brownian motions and \(_{t} 0,_{t}>0, t 0\). Let \(_{t}=(_{0}^{t}_{}\,)\), \(_{t}=_{0}^{t}(_{}_{})^{2}\,\). Then \(X_{t}\) coincides in law with \(_{t}^{-1}Y_{_{t}}\)._

Building upon the aforementioned proposition, the primary focus of this paper is centered around the standard Brownian motion \(dY_{t}=dB_{t}\).

### Probability flow is a Monge map

Khrulkov _et al._ have proposed a conjecture that the probability flow of Ornstein-Uhlenbeck process is a Monge map. However, they only provided a proof for a simplified case. We demonstrate that under some conditions, the conjecture is correct.

It is important to highlight that the continuous optimal transports presented in this paper are defined exclusively with the cost function: \(c(x,y)=|x-y|^{2}\).

Within the context of generative models, a collection of training samples denoted as \(\{x_{i}\}_{i=1}^{N}\) is typically provided, and these samples are intrinsically defined by a distribution:

\[p(x,0)=_{i=1}^{N}(x-x_{i}),\] (18)where \((x)\) represents the Dirac delta function. Given a Brownian motion with an initial distribution in the form of Equation (18), the single-time marginal distribution is 

\[p_{B}(x,t)=_{i=1}^{N}(2 t)^{-}(-|^{2}}{2t}).\] (19)

The probability flow is defined as :

\[d_{t}=-_{_{t}} p_{B}(_{t},t)dt.\] (20)

According to , the solution exists for all \(t>0\) and the map \(_{t+s}(_{t})\) is a diffeomorphism for all \(t>0,s 0\). We have discovered that \(_{t+s}(_{t})\) is the Monge map under some conditions and the time does not reach \(0\) or \(+\).

**Proposition 2**.: _Given that \(Y_{0}\) follows the initial condition (18), and all \(x_{i}\)s lie on the same line, the diffeomorphism \(_{t+s}(_{t})\) is the Monge optimal transport map between \(p_{B}(x,t)\) and \(p_{B}(x,t+s)\), \(\ t>0,s 0\)._

There is a counterexample  to demonstrate that the probability flow map does not necessarily provide optimal transport. It is important to note that their case differs from our assumptions in two ways. Firstly, they consider the limit case of \(_{+}(_{0})\). Secondly, the initial distribution of the counterexample does not conform to the form specified in Equation (18). Therefore, their counterexample is not applicable to our situation.

It has been shown that the heat diffusion equation can be regarded as the _gradient flow_ of the Gibbs-Boltzmann entropy concerning the \(W_{2}\)_metric_. As \(W_{2}\) is associated with optimal transport, it is reasonable to anticipate that the "infinitesimal transport" \(_{t+dt}(_{t})\) is optimal .

In order to interpret the concept of "infinitesimal transport", we utilize the generator of the process \(Y_{t}\). Let \(C_{c}^{2}(^{n})\) denote the set of twice continuously differentiable functions on \(^{n}\) with compact support. The generator \(A_{t}\) is defined as follows :

\[_{t}f=_{ t 0^{+}}_{t+ t})-f(_{t} )}{ t}, f C_{c}^{2}(^{n}).\] (21)

It is straightforward to verify that

\[_{t}=-_{x} p_{B}(x,t)^{T}_{x}.\] (22)

We define the "infinitesimal transport" to be the diffeomorphism \(_{t+s}(_{t})\) where \(_{t+s}\) evolves according to the following equation

\[d_{t+s}=-_{_{t}} p_{B}(_{t}( _{t+s}),t)ds,\] (23)

with the initial condition \(_{t}=_{t}\). The generator of \(_{t+s}\) is

\[_{t+s}=-_{_{t}} p_{B}(_{t}( _{t+s}),t)_{x}.\] (24)

**Proposition 3**.: _Given any \(t>0\), there exists a \(_{t}>0\) s.t. \(\ 0<s<_{t}\), the diffeomorphism \(_{t+s}(_{t})\) with the initial condition \(_{t}=_{t}\) is the Monge optimal transport map._

Let us return to the original Ornstein-Uhlenbeck process \(X_{t}\). As it is merely a deterministic transformation of the Brownian motion \(Y_{t}\), we can anticipate that the probability flow of \(X_{t}\), denoted by \(_{t}\), will be a Monge map. In fact, this expectation holds true:

**Proposition 4**.: _Given that \(X_{0}\) follows the initial condition (18), and all \(x_{i}\)s lie on the same line, the diffeomorphism \(_{t+s}(_{t})\) is the Monge optimal transport map for all \(t>0,s 0\)._Discrete probability flow

The continuous probability flow is deterministic, which means the "mass" at \(_{t}\) is entirely transported to \(_{t+s}\) during the time interval \([t,t+s]\). However, it is widely acknowledged that for discrete distributions \(\) and \(\), there may not exist a T such that T\({}_{}=\). As a result, discrete diffusions cannot possess a deterministic probability flow. To establish the concept of the _discrete probability flow_, we employ the methodology of optimal transport. First of all, a discrete diffusion model is proposed as an analogue of Brownian motion. Secondly, we modified the forward process to create an optimal transport map, which is used to define the discrete probability flow. Finally, a novel sampling technique is introduced, which significantly improves the certainty of the sampling outcomes.

### Constructing discrete probability flow

It is demonstrated that the process described by Equation (17) is a discrete equivalent of the heat diffusion process (14) . We adopt this process as our discrete diffusion model and represent it in a more comprehensive notation.

The discrete diffusion model has \(K\) dimensions and \(S\) states. The states are denoted by \(i=(i_{1},i_{2},,i_{K})\), where \(i_{j}\{1,2,,S\}\). The Kolmogorov forward equation for this process is

\[P_{j}^{i}(t|s)=_{j^{}}P_{j^{}}^{i}(t|s)Q_{D}^{j^{ }}(t),\] (25)

where \(P_{j}^{i}(t|s)\) means \(P(x_{t}=j|x_{s}=i)\) and \(Q_{D}\) is defined as:

\[Q_{D}^{i}_{j}=\{1,&d_{D}(i,j)=1,\\ -_{j^{}\{k:d_{D}(i,k)=1\}}Q_{D}^{i}_{j^{}},&d_{D}(i,j)=0,\\ 0,&otherwise,.\] (26)

where \(d_{D}(i,j)=_{l=1}^{K}|i_{l}-j_{l}|\). If we let the solution of the Equation (25) be denoted by \(P_{D}(t|s)\) and assume an initial condition \(P_{0}\), the single-time marginal distribution can be computed as follows:

\[P_{D}{}_{i}(t)=_{j}P_{0}{}_{j}Q_{D}^{j}(t|0).\] (27)

It is noteworthy that the process defined by \(Q_{D}\) is not an optimal transport map, as there exist _mutual flows_ between the states (i.e., there exists two states \(i\), \(j\) with \(Q_{j}^{i}>0\) and \(Q_{i}^{j}>0\)). Therefore, we propose a modified version that will be proved to be a solution to the Kantorovich problem, namely, an optimal transport plan. The modified version is defined by the following \(Q\):

\[Q_{j}^{i}(t)=\{,(t)-P_{D}(t))}{P_{D},(t)},&d_{D}(i,j)=1,\\ -_{j^{}\{k:d_{D}(i,k)=1\}}Q_{j^{}}^{i}(t),&d_{D}(i,j)=0,\\ 0,&otherwise..\] (28)

where

\[ReLU(x)=\{x,&x>0,\\ 0,&x 0..\] (29)

In order to avoid singular cases, We define \(Q_{j}^{i}(t)\) to be \(0\) when \(P_{D}{}_{i}(t)=0\). In fact, it is easy to verify that \(P_{D}{}_{i}(t)>0\) for all \(t>0\), \(i\{1,2,,K\}\). We will show that the process defined by \(Q\) is equivalent in distribution to the one generated by \(Q_{D}\).

**Proposition 5**.: _The processes generated by \(Q_{D}\) and \(Q\) have the same single-time marginal distribution \( t>0\)._

**Proposition 6**.: _Given any \(t>0\), there exists a \(_{t}>0\) s.t. \(\ 0<s<_{t}\), the process generated by \(Q\) provides an optimal transport map from \(P_{D}(t)\) to \(P_{D}(t+s)\) under the cost \(d_{D}\)._Proposition 6 demonstrates that \(Q_{D}\) generates a Kantorovich plan between \(P_{D}(t)\) and \(P_{D}(t+s)\) under a certain cost function. On the other hand, the continuous probability flow is the Monge map between \(p_{B}(x,t)\) and \(p_{B}(x,t+s)\). Therefore, it is reasonable to define the process defined by \(Q_{D}\) as the _discrete probability flow_ of the original process defined by \(Q\).

Furthermore, the "infinitesimal transport" of the discrete process, which is defined by \((t+s)=(t+s)Q(t)\), also provides an optimal transport map.

**Proposition 7**.: _Given any \(t>0\), there exists a \(_{t}>0\) s.t. \(\ 0<s<_{t}\), the process above provides an optimal transport map from \((t)\) to \((t+s)\) under the cost \(d_{D}\)._

### Sampling by discrete probability flow

In order to train the modified model, we employ a score-based method described in the Score-based Continuous-time Discrete Diffusion Model (SDDM) . Specifically, we directly learn the conditional probability \(P^{}(i_{l}(t)|\{i_{1},,i_{l-1},i_{l+1},,i_{K}\}(t))\). According to proposition 5, it follows that \(P^{}=P^{}_{}\), and consequently, the training process is identical to that of . For the sake of brevity, we will employ the notation \(P^{}_{i_{l}|i i_{l}}(t)\) to replace \(P^{}(i_{l}(t)|\{i_{1},,i_{l-1},i_{l+1},,i_{K}\}(t))\).

The generator of the reverse process is

\[R^{i}_{j}(t)=\{ReLU(_{D_{j}|i i _{l}}(t)}{P^{}_{D_{j}|i i_{l}}(t)}-1),&d_{D}(i,j)=1i_{l} j_{l},\\ -_{j^{}\{k:d_{D}(i,k)=1\}}R^{i}_{j^{}}(t),&d_{D }(i,j)=0,\\ 0,&otherwise..\] (30)

We use the Euler's method to generate samples. Given the time step length \(\), the transition probabilities for dimension \(l\) is:

\[P^{}(i_{l}(t-)|i(t))=\{ R^{i(t) }_{i_{1}(t),,i_{l}(t-),,i_{k}(t)}(t),&i_{l}(t-)  i_{l}(t),\\ 1+ R^{i(t)}_{i(t)}(t),&i_{l}(t-)=i_{l}(t)..\] (31)

When \(\) is small, the reverse conditional distribution has the factorized probability:

\[P^{}(i(t-)|i(t))=_{l=1}^{K}P^{}(i_{l}(t-)|i(t))\] (32)

In this way, it becomes possible to generate samples by sequentially sampling from the reverse conditional distribution 32.

Transition to higher probability statesThe reverse process of the continuous probability flow, as described in Equation (20), causes particles to move towards areas with higher logarithmic probability densities. As the logarithm function is monotonically increasing, this reverse flow pushes particles to higher probability density states. This phenomenon is also observed in the discrete probability flow. By examining the reverse generator, as shown in Equation (30), it can be determined that the transition rate \(R^{i}_{j}(t)>0\) only when the destination state \(j\) has a higher probability than the source state \(i\). This implies that transitions only occur in higher probability states. In contrast, the original continuous reverse process (2) and the discrete reverse process from (10) allow any transitions.

Reduction of Standard DeviationWe measure the certainty of the sampling method by the expectation of the Conditional Standard Deviation (CSD):

\[CSD_{s,t}(X)=_{X_{t}}[(X_{s}|X_{t})],\] (33)

where \((X_{s}|X_{t})=^{}(X_{s}|X_{t})=_{X_{ s}}^{}[X_{s}-_{X_{s}}[X_{s}|X_{t}]|X_{t}]\). \(CSD_{s,t}(X)\) is \(0\) when the process is deterministic, such as the continuous probability flow. In the discrete situation, there does not exist any deterministic map. However, our discrete probability flow significantly reduces \(CSD_{s,t}(X)\). Table 2 presents numerical evidence of this phenomenon. Therefore, we posit that the discrete probability flow enhances the certainty of the sampling outcomes.

## 5 Related Work

The concept of probability flow was initially introduced in  as a deterministic alternative to the Ito diffusion. In the work , they presented the Denoising Diffusion Implicit Model (DDIM) and demonstrated its equivalence to the probability flow. Subsequently,  investigated the relationship between the probability flow and optimal transport. They hypothesized that the probability flow could be considered a Monge optimal transition map and provided a proof for a specific case. Additionally, they conducted numerical experiments that supported their conjecture, showing negligible errors. However,  has discovered an initial distribution that renders probability flow not optimal.

The discrete diffusion models were first introduced by , who considered a binary model. Following the success of continuous diffusion models, discrete models have garnered more attention. The bulk of research on discrete models has focused primarily on the design of the forward process [24; 23; 4; 5; 26; 16; 9]. Continuous time discrete state models were introduced by  and subsequently developed by .

## 6 Experiments

We conduct numerical experiments using our novel sampling method by Discrete Probability Flow (DPF) on synthetic data. The primary goal is to demonstrate that our method can generate samples of comparable quality with higher certainty.

Experiments are conducted on synthetic data using the same setup as SDDM , with the exception that we replaced the generator \(Q\) with Equation (26). In addition to the binary situation (\(S=2\)) studied in , we also perform experiments on synthetic data with the state size \(S\) set to 5 and 10. To evaluate the quality of the generated samples, we generated 40,000 / 4,000 samples for binary data / other type of data using SDDM and DPF, and measured the Maximum Mean Discrepancy (MMD) with the Laplace kernel . The results are shown in Table 1. It can be seen that the MMD value obtained using DPF is slightly higher than that of SDDM, which may be attributed to the structure of the reverse generator 10. Specifically, DPF approximates an additional term, \(Q_{t}(y,x)\), with the neural network, which potentially introduces additional errors to the sampling process, leading to a higher MMD value compared to SDDM. However, such difference is minimal and does not significantly impact the quality of the generated samples. As evident from the visualization of the

    & 2spirals & 8gaussians & checkerboard & circles & moons & pinwheel & swissroll \\   \\  SDDM & 2.18e-06 & 4.28e-06 & 1.33e-06 & 6.22e-06 & 5.62e-06 & 2.10e-06 & 4.27e-06 \\ DPF (ours) & 1.89e-05 & 1.09e-05 & 2.22e-05 & 3.27e-05 & 2.42e-05 & 1.60e-05 & 2.18e-05 \\   \\  SDDM & 2.06e-4 & 1.01e-4 & 2.43e-4 & 1.74e-4 & 2.20e-4 & 3.37e-4 & 1.43e-4 \\ DPF (ours) & 3.87e-4 & 5.87e-4 & 4.93e-4 & 3.83e-4 & 3.43e-4 & 6.64e-4 & 3.20e-4 \\   \\  SDDM & 5.52e-4 & 3.01e-4 & 4.39e-4 & 4.22e-4 & 2.71e-4 & 2.90e-4 & 3.39e-4 \\ DPF (ours) & 7.19e-4 & 3.49e-4 & 5.99e-4 & 6.65e-4 & 4.34e-4 & 4.14e-4 & 5.17e-4 \\   

Table 1: Comparison of generation quality for SDDM and DPF, in terms of MMD with Laplace kernel using bandwith=0.1. Lower values indicate superior quality.

    & 2spirals & 8gaussians & checkerboard & circles & moons & pinwheel & swissroll \\   \\  SDDM & 14.3053 & 14.1882 & 14.7433 & 14.4327 & 14.1739 & 14.0450 & 14.0548 \\ DPF (ours) & 2.1719 & 1.7945 & 2.0693 & 1.7210 & 2.0573 & 2.1834 & 1.8892 \\   \\  SDDM & 14.4645 & 14.6143 & 14.6963 & 14.4807 & 14.2397 & 14.2466 & 14.2659 \\ DPF (ours) & 1.9711 & 1.9367 & 1.4172 & 1.7185 & 1.7668 & 1.9633 & 1.6665 \\   \\  SDDM & 12.8463 & 12.7933 & 13.0158 & 12.9232 & 12.6665 & 12.7634 & 12.7880 \\ DPF (ours) & 1.8123 & 1.3178 & 1.1348 & 1.4625 & 1.4859 & 1.8435 & 1.5227 \\   

Table 2: Comparison of certainty for SDDM and DPF, in terms of \(CSD\) on 4,000 initial points, each of which has 10 generated samples. Lower values indicate superior certainty.

distributions obtained from SDDM and DPF in Figure 1, it is clear that DPF can generate samples that are comparable to those generated by SDDM.

In addition, we also compare the sampling certainty of DPF and SDDM by computing \(CSD_{s,t}\) using a Monte-Carlo based method. Specifically, we set \(s=0\) and \(t=T\), and sample 4,000 \(x_{t}\)s with 10 \(x_{s}\)s for each \(x_{t}\). We then estimate \((x_{s}|x_{t})\) and \((x_{s}|x_{t})\) using the sample mean and sample standard deviation, respectively. The results of certainty are presented in Table 2. Our findings indicate that DPF significantly reduces the \(CSD\), which suggests a higher certainty. Additionally, we visualize the results of 4,000 generated samples (in blue) from a single initial point (in red) in the binary case in Figure 2. It is apparent that the sampling of SDDM exhibits high uncertainty, as it can sample the entire pattern from a single initial point. In contrast, our method reduces such uncertainty and is only able to sample a limited number of states.

To provide a more intuitive representation of the generated samples originating from various initial points, we select \(20 20\) initial points arranged in the grid, and distinguish them using different colors. Subsequently, we visualize the results by sampling 10 outcomes from each initial point, as shown in Figure 3. We observe that the visualization of SDDM samples appears disorganized, indicating significant uncertainty. In contrast, the visualization of DPF samples exhibits clear regularity, manifesting in two aspects: (1) the generated samples from the same initial point using DPF are clustered by color, demonstrating the better sampling certainty of our DPF. (2) Both of the generated samples and initial points are colored similarly at each position. For example, in the lower

Figure 1: Visualization of the generation quality on generated binary samples for SDDM and DPF.

Figure 3: Visualization of the generated binary samples from the given initial points \(_{T}\). Different colors distinguish the generated samples from different initial points \(_{T}\).

Figure 2: Visualization of the generating certainty on generated binary samples for SDDM and DPF. All the samples (in blue) are randomly generated from the single initial point (in red).

right area, a majority of the generated samples are colored purple, which corresponds to the color assigned to the initial points \(x_{T}\) in that area. This observation demonstrates that most of the sampling results obtained through DPF are closer to their respective initial points, aligning with our design intention of optimal transport. It is worth noting that similar phenomena are observed across different state sizes, and we have provided these results in the Appendix.

Finally, we extended our DPF to the CIFAR-10 dataset, and compare it with the \(\)LDR-0 method proposed in . The visualization results are shown in Figure 4. It can be seen that our method greatly reduces the uncertainty of generating images by sampling from the same initial \(x_{T}\). Detailed experimental settings and more experimental results are presented in the Appendix.

## 7 Discussion

In this study, we introduce a discrete counterpart of the probability flow and established its connections with the continuous formulations. We began by demonstrating that the continuous probability flow corresponds to a Monge optimal transport map. Subsequently, we proposed a method to modify a discrete diffusion model to achieve a Kantorovich plan, which naturally defines the discrete probability flow. We also discovered shared properties between continuous and discrete probability flows. Finally, we propose a novel sampling method that significantly reduces sampling uncertainty. However, there are still remaining aspects to be explored in the context of the discrete probability flow. For instance, to obtain more general conclusions under a general initial condition, the semi-group method  could be employed. Additionally, while we have proven the existence of a Kantorovich plan in a small time interval, it is possible to extend this to a global solution. Moreover, the definition of the probability flow has been limited to a specific type of discrete diffusion model, which also could be extended to a broader range of models. These topics remain open for future studies.

## 8 Acknowledgments and Disclosure of Funding

We would like to thank all the reviewers for their constructive comments. Our work was supported in National Natural Science Foundation of China (NSFC) under Grant No.U22A2095 and No.62072482.

Figure 4: Image modeling on CIFAR-10 dataset. The figure is divided into three groups: initial points \(x_{T}\), sampling results of \(\)LDR-0, and sampling results of our DPF. For each row, the sampled images are obtained from the same initial point.