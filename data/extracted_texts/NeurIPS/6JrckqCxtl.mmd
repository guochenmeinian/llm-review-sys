# Polyhedron Attention Module: Learning Adaptive-order Interactions

Tan Zhu

University of Connecticut

tan.zhu@uconn.edu

&Fei Dou

University of Georgia

fei.dou@uga.edu

&Xinyu Wang

University of Connecticut

xinyu.wang@uconn.edu

&Jin Lu

University of Georgia

jin.lu@uga.edu

&Jinbo Bi

University of Connecticut

jinbo.bi@uconn.edu

###### Abstract

Learning feature interactions can be the key for multivariate predictive modeling. ReLU-activated neural networks create piecewise linear prediction models. Other nonlinear activation functions lead to models with only high-order feature interactions, thus lacking of interpretability. Recent methods incorporate candidate polynomial terms of fixed orders into deep learning, which is subject to the issue of combinatorial explosion, or learn the orders that are difficult to adapt to different regions of the feature space. We propose a Polyhedron Attention Module (PAM) to create piecewise polynomial models where the input space is split into polyhedrons which define the different pieces and on each piece the hyperplanes that define the polyhedron boundary multiply to form the interactive terms, resulting in interactions of adaptive order to each piece. PAM is interpretable to identify important interactions in predicting a target. Theoretic analysis shows that PAM has stronger expression capability than ReLU-activated networks. Extensive experimental results demonstrate the superior classification performance of PAM on massive datasets of the click-through rate prediction and PAM can learn meaningful interaction effects in a medical problem.

## 1 Introduction

Learning feature interactions provides insights into how predictors (features in \(\)) interact to vary a dependent variable \(y\), and could significantly improve prediction performance in a wide range of research areas, such as recommendation systems , genetic analysis  and neuroscience , and help explain the decision-making of a complex model.

Interactions among features can significantly improve the model's expression capability. For a simple example, by incorporating the two-way interaction effect between gender (0/1: female and male) and age into the linear regression model \(height w_{1} gender+w_{2} age+w_{3} gender age+w_{0}\) (where \(w_{0}\), \(w_{1}\), \(w_{2}\), \(w_{3}\) are trainable parameters), the effects of female's age on the heights will be different from that of male (\(w_{2}\) v.s. \(w_{2}+w_{3}\)). A predictive model \(f()\) has a \(k\)-way interaction effect if it satisfies :

**Definition 1** (\(k\)-way interaction effect): _Let \(\) be the input of a function \(f():^{p}\), and \(x_{i}\) be the \(i^{th}\) feature of \(\). Let \(\{1,2,...,p\}\) be a set of feature indices and \(k\)\((k>2)\) be the cardinality of \(\). If_

\[_{}[|}f()}{ x _{i_{1}} x_{i_{2}}... x_{i_{k}}}]^{2}>0,\] (1)

[MISSING_PAGE_EMPTY:2]

a fully-connected layer and a ReLU-activation layer. Let \(^{}\) be the input of the \(\)-th block which outputs a nested function \((W^{}^{}+b^{})\). The \((z)\) returns \(z\) when activated (if \(z 0\)) or \(0\) otherwise. The consecutive \(L\) blocks together create piecewise linear functions (each output in layer \(L\) defines such a function). For a given raw input \(\), let \(I^{}\) contains the indices of the activated ReLU functions in layer \(\) when \(\) passes through the layer, and \(W_{I^{}}\) and \(b_{I^{}}\) contain the original weights, respectively, in \(W^{}\) and \(b^{}\) for those rows indexed in \(I^{}\) and \(0\) vectors for those in the complement set of \(I^{}\) (denoted by \(I^{-}\)). Then, at a layer \(\), \(W^{()}+b^{()} 0\) where \(W^{()}=_{j=1}^{}W_{I^{j}}\) and \(b^{()}=_{j=1}^{}b_{I^{j}}_{k=j+1}^{}W_{I^{k}}\). More precisely, it means that \(\) stays in a polyhedron that is defined by the intersection of all the half-spaces \(\{:W^{()}+b^{()} 0\}\) and \(\{:W^{(-)}+b^{(-)}<0\}\) for all \(=1,,L-1\) where \(W^{(-)}=W_{I^{-}}_{j=1}^{-1}W_{I^{j}}\) and \(b^{(-)}=b_{I^{-}}+_{j=1}^{-1}b_{I^{j}}W_{I^{-}}_{k=j+ 1}^{-1}W_{I^{k}}\) specify those affine functions not activated in layer \(\). Here, \(W_{I^{-}}\) and \(b_{I^{-}}\) contain the original weights in \(W^{}\) and \(b^{}\) for those rows indexed in \(I^{-}\) and \(0\) vectors for those in \(I^{}\). For all \(\) in this polyhedron, the \(L\)-th layer outputs affine functions \(W^{(L)}+b^{(L)}\).

Precisely, the input space is divided into non-overlapping polyhedrons and each polyhedron \(\) is defined by a combination of activated ReLU functions across the layers (a formal proof is given in Appendix A). To ease the notation, we simply use \(\) to denote the set of indices of the activated ReLU across all layers that identify the polyhedron and \(^{-}\) denotes the index set of the inactivated ReLU in the layers. We use an indicator function \(()\) to define the polyhedron which returns \(1\) for all vectors \(\) that satisfy \(W_{}^{()}+b_{}^{()} 0\) and \(W_{}^{(-)}+b_{}^{(-)}<0\) for \(=1,,L-1\), and \(0\) otherwise. The \(i\)-th activated ReLU in the \(L\)-th layer corresponds to a piecewise linear function that computes \(W_{,i}^{(L)}+b_{,i}^{(L)}\) on each piece \(\). Note that \(W_{,i}^{(L)}\) and \(b_{,i}^{(L)}\) vary for different polyhedrons due to the differences in the ReLU activation in the early layers (illustrated in Fig. 1). Denote the hyperplane \(\{:W_{,i}^{(L)}+b_{,i}^{(L)}=0\}\) by \(H_{,i,L}\), which further splits \(\) into two polyhedrons \(_{1}=(,W_{,i}^{(L)}+b_{ ,i}^{(L)} 0)\) and \(_{2}=(,W_{,i}^{(L)}+b_{ ,i}^{(L)}<0)\). Thus, this piecewise linear function can be written as:

\[ f()&=_{}()(W_{,i}^{(L)}+b_{ ,i}^{(L)})=_{}() (^{(L)}+b_{,i}^{(L)}}{||W_{,i}^{(L)}|| }||W_{,i}^{(L)}||)\\ &=_{}(_{1}) (,H_{,i,L})}_{}^{(L)}||}_{$}}\ \ +\ (_{2})(,H_{,i,L})}_{}_{$}}\] (2)

where \((,H_{,i,L})\) means the distance from \(\) to the hyperplane \(H_{,i,L}\). The values on the two pieces \(_{1}\) and \(_{2}\) are constant \(||W_{,i}^{(L)}||\) where \(||||\) is the \(_{2}\) vector norm or \(0\). The attention is a function of \(\) and depends on how far \(\) is from one of the hyperplanes that define the polyhedron boundary (see Fig. 1 for illustration).

We observe that a polyhedron \(\) is defined using a sequence of hyperplanes corresponding to the affine functions in different layers, but the attention of ReLU-activated DNNs is calculated based only on the hyperplane in the last layer for the polyhedron (piece). Although not all of the hyperplanes in early layers make the active boundary of a polyhedron (i.e., the polyhedron can locate in the interior of a half-space), using only one hyperplane in the attention is restrictive. An attention mechanism that allows multiple active boundary hyperplanes of a polyhedron to aid attention calculation may increase the model's expression power. Let \(_{}\) contain all of the active boundary hyperplanes \(H\) of \(\). For convenient notation, and with mild relaxation, we rescale the \(W_{H}\) to \(-W_{H}\) for those inactivated affine functions in the DNN so the half-spaces can all be written in the form of \(W_{H}+b_{H} 0\). Then, \(()=_{H_{}}(W_ {H}+b_{H} 0)\). To learn feature interaction effects, we multiply the distances from \(\) to each hyperplane in \(_{}\). Given a hyperplane is linear in terms of \(\), multiplying the distances from \(\) to two (\(m\)) hyperplanes creates quadratic (\(m\)-th order) terms. Thus, the number of active boundary hyperplanes of \(\) offers the upper bound on the order of the multiplicative terms

Figure 1: An example of 2-layer ReLU-activated plain DNN with I output (the green shading shows the function value). Black lines are fitted in layer 1 (their intersection defines the polyhedrons) and the red line is fitted in layer 2 and varies on different polyhedrons.

if each hyperplane is used only once. To allow the order to be adaptive to each \(\) within the upper limit, we further enclose a bounding constraint on the distance (other strategies may exist, which we leave for future investigation). Thus, the new attention formula can be written as follows:

\[ a_{}()=&( )_{H_{}}((,H),}{||W_{H}||})\\ =&_{H_{}}(W_{H} +b_{H} 0)((,H),}{||W_{H}||})\\ =&_{H_{}}( (+b_{H}}{||W_{H}||},}{||W_{H}||}),0),\] (3)

where \(U_{H}\) determines an upper bound on the distance from \(\) to \(H\) and is a trainable parameter. Fig. 2 demonstrates how adding the upper bounds \(U_{H}\) allows the attention module to learn interaction effects of adaptive orders. For example, the instances in the area marked by \(0\) in the figure (left) are far from each boundary hyperplane \(H\) beyond their respective upper bounds, so the \(\) operator returns a constant \(}{||W_{H}||}\). These \(\) instances thus receive a constant attention that is the multiplication of these upper bounds each for an \(H_{}\), which gives a \(0\)-order interaction. When \(\) is close to two of the hyperplanes (in those areas labeled by \(2\)), two distance terms are used in Eq. 3, defining two-way interactions.

Early methods pre-specify a set of polynomial terms and use them as input features to a DNN  which limits the search space. In another line of methods , each feature is associated with an order parameter \(m_{i}\) and products in the form of \( x_{i}^{m_{i}}\) are used as input (rather than raw features \(x_{i}\)) to a ReLU-activated DNN. The DNN is required to also learn the proper values of \(m_{i}\). It is an excellent approach to identify useful interactions, but once these \(m_{i}\)'s are determined, the orders of feature interaction are fixed and cannot adapt to different pieces because the piecewise linear model (linear in terms of the input products) learned by the DNN does not change interaction orders. Unlike these methods, our attention module automatically identifies the interactions (polynomial terms) of appropriate orders according to the topology of a piece (particularly, using the number of active boundary hyperplanes of the polyhedron). Using introduced parameters \(U_{H}\), we can learn the appropriate order of the interactive terms within a polyhedron by adjusting \(U_{H}\) (larger values of \(U_{H}\) lead to higher-orders of feature interaction). Our theoretical analysis (Section 5) shows that a DNN incorporating our attention module can approximate any function represented by ReLU-activated DNN at any arbitrarily accurate level with fewer model parameters.

## 3 The Proposed Polyhedron Attention Module (PAM)

This section elaborates on the PAM with the attention score defined in Eq. 3. With input \(^{p}\), a DNN using PAM defines a function \(f_{PAM}\):

\[f_{PAM}()=V(;_{G})+_{}a_{}()V(;_{}),\] (4)

where \(V(;_{G})\) is a global value function with trainable parameter \(_{G}\), and \(V(;_{})\) is the local value function on a piece \(\) with trainable parameters \(_{}\). We set both the global and local value

Figure 3: Overview description of the proposed method: (A) Split the input space into overlapping polyhedrons with the oblique tree. (B) Calculate attention on each polyhedron based on distances to the polyhedron’s boundaries. (C) Calculate the model’s output based on the attention and value vector. (D) Extract main and interaction effects from the model.

Figure 2: For a triangular polyhedron \(\) in this example, the attention \(a_{}\) can capture constant, additive, 2-way interactive, and 3-way interactive effects in the areas, respectively, marked by \(0\), \(1\), \(2\), and \(3\), by adjusting the upper bound parameter \(U\) which is smaller in the left figure than in the right.

functions to be affine functions, so \(\) contains the normal vector \(W\) and offset \(b\). Sec 3.2 explains why affine functions are our choice. Instead of forming polyhedrons by intersecting hyperplanes as done in ReLU-activated DNNs, we use a tree search to partition the input space into overlapping polyhedrons. Note that the potential set of partitions created by tree search is a superset of that created by hyperplane intersections (see Appendix B). We introduce how we generate polyhedrons and then discuss the attention and value functions.

### Generating polyhedrons via oblique tree

Let \(_{}\) contain all the polyhedrons needed to form a partition of the input space. We adopt the oblique tree to generate \(_{}\). An oblique tree is a binary tree where each node splits the space by a hyperplane rather than by thresholding a single feature. The tree starts with the root of the full input space \(\), and by recursively splitting \(\), the tree grows deeper. For a \(D\)-depth (\(D 3\)) binary tree, there are \(2^{D-1}-1\) internal nodes and \(2^{D-1}\) leaf nodes. As shown in Fig. 3A, each internal and leaf node maintains a sub-space representing a polyhedron \(\) in \(\), and each layer of the tree corresponds to a partition of the input space into polyhedrons. Denote the polyhedron defined in node \(n\) by \(_{n}\), and the left and right child nodes of \(n\) by \(n_{L}\) and \(n_{R}\). Unlike classic oblique trees that partition \(\) into non-overlapping sub-spaces, we perform soft partition to split each \(_{n}\) into \(_{n_{L}}\) and \(_{n_{R}}\) with an overlapping buffer. Let the splitting hyperplane be \(\{^{p}:W_{n}+b_{n}=0\}\). Then the two sub-spaces \(_{n_{L}}\) and \(_{n_{R}}\) are defined as follows:

\[_{n_{L}}&=\{ _{n}|\;W_{n}+b_{n}-U_{n}\},\\ _{n_{R}}&=\{_{n}|\;-W_{n} -b_{n}-U_{n}\},\] (5)

where \(U_{n}\) indicates the width of the overlapping buffer. Eq. 5 shows that those instances satisfying \(|W_{n}+b_{n}|<U_{n}\) belong to the buffer in both \(_{n_{L}}\) and \(_{n_{R}}\). This buffer creates a symmetric band around the splitting hyperplane.

Let \(_{n}\) be the node set containing all the ancestor nodes above \(n\). We can group the nodes in \(_{n}\) into two subsets \(_{n}^{l}\) or \(_{n}^{r}\) where \(n\) appears in the left or right subtree of those ancestors. Let \(i\) index the nodes in \(_{n}^{l}\) and \(j\) index the nodes in \(_{n}^{r}\). Then for any node \(n\) except the root node, \(_{n}\) can be expressed as the intersection of the half-spaces:

\[_{n}=\{^{p}:\;W_{i}+b_{i}-U_{i},  i_{n}^{l},\;\;-W_{j}-b_{j}-U_{j}, j_{n^{r}}\}.\] (6)

Based on the polyhedron defined by Eq. 6, the attention \(a_{_{n}}()\) (which we refer to as \(a_{n}()\) to simplify notation) can be rewritten as

\[a_{n}()=_{i_{n}^{1}}(\{ +b_{i}+U_{i}}{||W_{i}||},}{||W_{i}||}\},0)_{i _{n}^{r}}((-b_{i}+U_{i}}{||W_{i}||}, }{||W_{i}||}),0),\] (7)

where we use the buffer width \(U_{i}\) to bound the corresponding distance term from above by \(}{||W_{i}||}\).

Let \(_{d}\) consist of all the nodes at the \(d^{th}\) layer of the tree, \(d\{1,2,...,D\}\). The nodes in \(_{d}\) altogether specify a soft partition of the input space that maps an instance \(\) to one sub-space or an intersection of overlapping sub-spaces in \(|_{d}|\). Rather than merely utilizing the instance partitioning map defined by the last layer of the tree (\(2^{D-1}\) polyhedrons), we allow PAM to leverage polyhedrons generated at all layers in \(f_{PAM}\) with \(_{}=_{d=2}^{D}\{_{n}|n_{d}\}\) which gives \(2^{D}-2\) polyhedrons.

### Learning the attention and value functions

Each internal node \(n\) in the oblique tree needs to learn two affine functions: a splitting function used to form a hyperplane to split the sub-space into child nodes, and a value function. Because nested affine functions still produce affine functions, the set of affine functions is closed under linear transformations. Thus, we can use the value function to absorb the denominator \(||W_{i}||\) from the attention Eq.7. In other words, for any learned \(_{n}\), we can find a \(_{n}^{}\) (by rescaling \(_{n}\) with those \(||}\)) such that \(a_{n}()V_{n}(,_{n})=a_{n}^{}()V_{n}( ,_{n}^{})\) where \(a_{n}^{}()=_{i_{n}^{l}}((W_{i} +b_{i}+U_{i},2U_{i}),0)_{i_{n}^{r}}((-W_ {i}-b_{i}+U_{i},2U_{i}),0)\) and we use the subscript \(n\) to denote the polyhedron \(\) represented in node \(n\). We thus directly set off to learn \(_{n}^{}\) and use \(a_{n}^{}\) as \(a_{n}\).

More importantly, with affine value functions, we can derive another property. For any internal node \(n\), the attention of its two child nodes contains \(a_{n}\) as a factor, so \(b_{n}+U_{n},2U_{n}),0)\) and \(a_{n_{R}}()=a_{n}()((-W_{n}-b_{n}+U_{n},2U_{ n}),0)\). It gives rise an observation that no matter where \(\) is located (inside the buffer or outside), \(a_{n_{L}}()+a_{n_{R}}()=2U_{n}a_{n}()\). Thus, we can substitute \(a_{n_{L}}()\) in the model \(f_{PAM}\) by \(2U_{n}a_{n}()-a_{n_{R}}()\). Then, \(a_{n}()V(,_{n})+a_{n_{L}}()V(, _{n_{L}})+a_{n_{R}}()V(,_{n_{R}})=a_{n}( )(V(,_{n})+2U_{n}V(,_{n_{L}}) )+a_{n_{R}}()(V(,_{n_{R}})-V(, _{n_{L}}))\), which can be written as \(a_{n}()V(,)+a_{n_{R}}()V(, )\) for some parameters \(\) and \(\) due to the closure of affine functions under linear transformations. Hence, once again, we directly learn \(\) and \(\) in our model training process. Note that we can recursively apply the above subsitution for all internal nodes \(n\), so we can reduce the polyhedrons in \(_{}\) by half. The following theorem characterize the above discussion (proof is in Appendix C.)

**Theorem 1**: _If all value functions \(V\) belong to a function set that is closed under linear transformations, then the function learned by PAM \(f_{PAM}\) can be equivalently written as_

\[f_{PAM}()=V(,_{G})+_{n S_{}^{-}}a_{n}( )V(,_{n})\] (8)

_where the polyhedron set \(_{}^{-}\) contains half of the polyhedrons (e.g., the right child nodes or the left child nodes) in \(_{}\) and_

\[a_{n}()=_{i_{n}^{L}}((W_{i}+b_{ i}+U_{i},2U_{i}),0)_{i_{n}^{r}}((-W_{i}-b_{ i}+U_{i},2U_{i}),0).\] (9)

**Remark 1**: _Although we include polyhedrons identified by the internal nodes in our calculation, \(f_{PAM}\) only needs to learn \((2^{D}-2)=2^{D-1}-1\) value functions, which is actually in the same scale as that of only using leaf nodes in \(f_{PAM}\)._

**Optimization of PAM.** The output of \(f_{PAM}()\) can be treated as a prediction of \(\)'s label or an embedding of \(\). PAM can be used as a constituent component in a DNN to approximate a target \(y=f(f_{PAM}())\). For a classification task, we can calculate the conditional distribution \(=Pr(y|)=(f(f_{PAM}()))\) and optimize the cross-entropy \(L_{CE}=-_{(,y)}y-(1-y)(1- )\) between the observed \(y\) and estimated \(\) to determine the parameters in PAM. For a regression task, the mean square loss \(L_{MSE}=_{(,y)}(y-f(f_{PAM}()))^{2}\) can be used.

## 4 Model Interpretation

We propose a conceptual framework to quantify and interpret the interaction effects learned by PAM. Without loss of generality, we assume that the DNN has a single output. Additional outputs can be similarly interpreted using the derived algorithm. The \(f_{PAM}\) can be rewritten as a summation of \(k\)-way interaction terms for all possible values of \(k D\): \(f_{PAM}()=_{T\{1,2,,p\}}_{T}()\) where \(_{T}()\) captures the total contribution to the output from the \(||\)-way interactions among the features indexed in \(\). If \(\), \(_{}()=_{ m_{i} D}w_{T}_{i }x_{i}^{m_{i}}\) where \(x_{i}\) represents the \(i^{th}\) feature of \(\), \(m_{i}\) calculates the power of \(x_{i}\) in the interaction, and \(w\)'s are constant coefficients in front of the corresponding interaction terms. Given the definition of our attention in Eq.9, the highest polynomial order is \(D-1\) in the attention, together with the affine value function, the highest polynomial order of \(f_{PAM}\) is \(D\), so \( m_{i}\) can not exceed the depth of the tree. If \(=\), \(_{}=w_{}\) which is a constant. We develop a method here to estimate the contribution values \(_{}\) in \(f_{PAM}=_{\{1,2,,p\}}_{}\) for a fixed input \(\) without computing the individual polynomial terms.

For a given \(\), \(f_{PAM}\) can be written explicitly out as \(g()\) according to which polyhedron(s) \(\) belongs to. To determine \(g()\), we pass \(\) through the model Eq.8 and evaluate every term in Eq.9. For instance, for the first product in Eq.9, if \(\) makes \(W_{i}+b_{i} U_{i}\), we replace the term \(((W_{i}+b_{i}+U_{i},2U_{i}),0)\) by \(2U_{i}\); if \(W_{i}+b_{i}-U_{i}\), we replace it by \(0\); or otherwise, we use \(W_{i}+b_{i}+U_{i}\). Once the \(g\) function is computed, we can use Algorithm 1 to evaluate the contribution \(_{}\), \(\{1,,p\}\).

A simple example can be used to demonstrate Algorithm 1. Let \(=\{1,2\}\) which means we calculate the sum of those cross terms that involve exactly \(x_{1}\) and \(x_{2}\). Thus we set all other elements in \(\) to and calculate \(g(^{-})\) to obtain the value \(v\) that adds the terms involving only \(x_{1}\) and \(x_{2}\). We then additionally set either \(x_{1}=0\) or \(x_{2}=0\), or \(x_{1}=x_{2}=0\) (i.e., make all elements \(0\)), re-compute \(g\) to estimate the linear terms of either \(x_{1}\) or \(x_{2}\), or the constant term in \(g\), and subtract these values from \(v\) to eventually obtain \(_{}\). The following theorem characterize our results.

**Theorem 2**: _For any input \(\), by calculating \(_{}()\) for each \(\{1,2,...,p\}\) via Algorithm 1, we have \(_{\{1,2,...,p\}}_{}()=f_{PAM }()\)._

The instance-level explanation of \(f_{PAM}\) can be obtained by examining the magnitude of \(_{}\) which reflects the impact of the feature interaction among the features in \(\). If \(_{}()>0\), the interaction increase the predicted value; if \(_{}<0\), it reduces the output. The model-level interpretation can be approximated by computing the mean absolute value of the \(_{}\) across all sample instances.

## 5 Theoretical Justification - Approximation Theorems

We examine whether using PAM can enhance the expression power for universal approximation. We first introduce the Sobolev space, which characterizes a space of functions satisfying specific smoothness properties - Lipschitz continuous up to order \(n\) - which is formally defined as:

**Definition 2** (Sobolev space): _Let \(^{n,}(^{p})\) be the Sobolev space which comprises of functions on \(^{p}\) lying in \(L^{}\) along with their weak derivatives up to order \(n\). The norm of a function f in \(^{n,}(^{p})\) is_

\[||f||_{^{n,}(^{p})}=_{:||  n}ess_{^{p}}|D^{n}f()|,\]

_where \(=(n_{1},n_{2},...,n_{p})\{1,2,...,n\}^{p}\), \(||=n_{1}+n_{2}+...+n_{p} n\), and \(D^{}f\) is the \(\)-order weak derivative. The essential supreme \(ess g(E)=\{M R:(\{x E:f(x)>M\})=0\}\) captures the smallest value that the function \(g\) can approach or exceed on a set \(E\), except for a negligible subset of points with the measure \(\). Essentially, the space \(^{n,}(^{p})\) is \(C^{n-1}(^{p})\) whose functions' derivatives up to order n are Lipschitz continuous._

The following assumption is commonly used in the discussion of DNNs. Without loss of generality, it narrows our focus on a normalized Sobolev sphere. This assumption constrains the functions having Sobolev norm no greater than 1 within the sphere.

**Assumption 1**: _Let \(F_{n,p}\) be a set of functions lying in the unit ball in \(^{n,}(^{p})\), we have_

\[F_{n,p}=\{f^{n,}(^{p}):||f||_{^{n,} }(^{p}) 1\}.\]

This assumption is sufficient for our analysis, as functions encountered in real-world learning tasks can typically be linearly transformed into \(^{n,}(^{p})\), as shown in previous studies . This allows us to analyze the error bounds for terms in the polynomial approximation after performing Taylor expansion. Theorem 3 demonstrates that our model can almost surely approximate any ReLU-activated DNN model without error. All proofs can be found in Appendix E and F.

**Theorem 3**: _If \(\) is bounded and sampled from a distribution with upper-bounded probability density function, then for any ReLU activated plain DNN model \(f_{}()\), there exists a PAM with_

\[Pr(f_{PAM}()=f_{}()) 1.\]

Theorem 4 examines the parameter efficiency, and demonstrates that networks incorporating the proposed polyhedron attention require fewer parameters compared to those relying solely on ReLU activation, while maintaining the same approximation error in fitting functions in the Sobolev space.

**Theorem 4**: _For any \(p\), \(n>0\) and \((0,1)\), we have a PAM which can 1) approximates any function from \(F_{n,p}\) with an error bound \(\) in the sense of \(L^{}\) with at most \(2p^{n}(N+1)^{p}(p+n-1)\) parameters, where \(N=(p^{n}})^{-}\)._

**Remark 2**: _For the purpose of comparison, the ReLU-activated plain DNN needs \(p^{n}(N+1)^{p}(p+1)n((1/))\) parameters under the same setting in Theorem 4._It is worth noting that extensive research has been conducted on the approximation theory of DNNs with the ReLU-activation, which often concerns common function classes in specific function spaces such as Besov [31; 32] and Sobolev spaces [33; 34]. These analyses reveal a notable influence of the smoothness of the target function on the resulting approximation errors. Since Sobolev spaces characterize the smoothness properties of functions, we can investigate the ability of neural networks with ReLU activation to approximate functions with different degrees of regularity and smoothness. These theorems highlight the expressivity of our model and provide theoretical insights for the parameter efficiency of our proposed attention module in neural network architectures.

## 6 Empirical Evaluation

We evaluate the effectiveness and efficiency of PAM on three large-scale datasets: the **Criteo1** and **Avazu1** click-through-rate (CTR) datasets, and the **UK Biobank2** medical database. We conduct an analysis of the hyperparameters of PAM, and perform ablation studies by individually removing each of the three key components of PAM and evaluating the performance variations. Given the lack of known feature meanings in CTR benchmark datasets, we utilize the UK Biobank dataset as an example for studying model interpretation. Specifically, we validate the interaction effects captured by our interpretation framework, as detailed in Sec. 4, in the prediction of brain-age by the grey matter volumes from distinct brain regions. For implementation details, computation and space complexity of our model, please refer to Appendix G.

### Experimental Setup

**Datasets.** Both the Criteo and the Avazu are massive industry datasets containing feature values and click feedback for display ads, and are processed following the benchmark protocol in BARS [35; 36]. The UK Biobank serves as a comprehensive biomedical database and research resource, offering extensive genetic and health-related information, where our objective is to predict participants' age by leveraging the grey matter volumes from 139 distinct brain regions. The summary statistics are listed in Table 1.

**Evaluation metrics.** Criteo and Avazu datasets are concerned with binary classification tasks, evaluated using Area Under the ROC Curve (AUC). For brain-age prediction, a regression problem, we assess performance using the \(R^{2}\) score.

**Baseline Algorithms.** We compare PAM against the top 5 algorithms from a pool of 33 baseline methods, selected based on the overall AUC scores in BARS, including DESTINE  (currently the best performer on Avazu), DCN , AOANet , EDCN  (best on Criteo), and DCN-V2 . Given that our model and the selected algorithms are DNN-based, we also include a well-tuned DNN1 as a strong baseline for our model comparison. It is worth noting that several state-of-the-art studies on CTR datasets, such as Criteo and Avazu, from Google [2; 24; 37], Microsoft  and Huawei , have recognized that even a marginal improvement of AUC at the level of \(1\%e\) is considered a significant performance enhancement.

### Effectiveness and efficiency evaluation

**Performance Comparison.** We compare the results over five runs. As shown in Fig. 3(a), PAM achieves the highest AUC and \(R^{2}\) score amongst all benchmark algorithms. PAM outperforms the second-best model by 1.4 %e for Criteo, 0.6 %e for Avazu, and 2.2 for UK Biobank, which is much higher than the improvement achieved by the second-best model from the next model (0.4 %e for Criteo, 0.16 %e for Avazu and 0.6 %e for UK Biobank). It is also worth noting that, despite being well-tuned, none of the benchmark algorithms performs optimally across all three datasets simultaneously. We evaluate the computation time of PAM by comparing the training and inferencing runtime (seconds) per batch to the number of trainable parameters (#Par). As shown in Table 2, almost all models have a similar amount of free parameters, and their runtime per batch is comparable among the methods (PAM is superior than half of the methods and inferior than the other half.)

   Dataset & \#Train & \#Valid & \#Test & \#Features \\  Criteo & 33M & 8.3M & 4.6M & 2.1M \\ Avazu & 28.3M & 4M & 8.1M & 1.5M \\ UK Biobank & 31.8K & 4K & 4K & 139 \\   

Table 1: Statistics of the datasets.

**Hyper-parameter Study.** Although \(U_{i}\)'s are trainable parameters in PAM, their initialization may affect how well PAM works. We hence study the effects of the initial values of \(U_{i}\)'s by testing a set of choices from 1 to 3 by a step size of 0.5. We also examine the effects of the tree depth \(D\) on PAM's performance. Fig. 4b shows the comparison where the initial values of \(U_{n}\) do not substantially change PAM's performance, which indicates that \(U_{n}\) may be well trained during the PAM optimization. As for the tree depth \(D\), the performance of PAM initially improves as \(D\) increases, but when \(D\) becomes large, the performance may deteriorate. An appropriate value of \(D\) will help PAM to learn more domain knowledge, but if \(D\) is too large, highly complex interactions may not necessarily lead to better prediction due to issues such as overfitting.

**Ablation Study of PAM.** The following specifies the three components for our ablation experiments. **1) PAM without overlapping polyhedrons (PAM w/o OP)**. As shown in Eqs. 5, 6 and 7, overlapping polyhedrons are generated in PAM to fit the target. To demonstrate the importance of soft splitting, we calculate \(a_{n}()\) with \(_{i_{n}^{l}}((W_{i}+b_{i},2U_{i}),0) _{i_{c}}((-W_{i}-b_{i},2U_{i}),0)\).

**2) PAM without adaptively learning interaction (PAM w/o ALI)**. As shown in Fig. 5, the upper-bound \(2U_{i}\) enables PAM to learn interactions with different orders in different polyhedrons. To exam the effectiveness of the upper bound, we remove this upper bound \(2U_{i}\) from Eq. 9.

**3) PAM without removing \(||W_{i}||\) (PAM w/o RW)**. According to Theorem 1, the denominator of Eq. 7 can be removed without reducing the expression capability of PAM. To examine the correctness of this claim, we directly use PAM without removing \(||W_{i}||\) from the denominator.

Fig. 4c clearly demonstrates a notable decline in the performance of the PAM when key components are removed. In particular, the standard PAM significantly outperforms the PAM w/o OP. It confirms the critical role of overlapping polyhedrons in enhancing PAM's performance. The removal of ALI decreases the AUC and \(R^{2}\) of PAM as well, indicating the significance of combining low-way interaction effects with high-way ones to fit the target. Moreover, PAM w/o RW shows that removing \(||W_{i}||\) from the denominator in Eq. 7 improves PAM's performance. Although, according to Theorem 1, PAM without RW has the same expressive capability as standard PAM, the inclusion of \(W\)'s norm in the denominator of Eq. 7 may result in unstable gradient variances, potentially compromising the performance of the optimizer.

### Identified interaction effects among neural markers in predicting brain age

Although the interpretation framework was developed for PAM, it can be applied to those DNN architectures whose functions are piece-wise polynomials (see Appendix D). After carefully checking

    &  &  &  \\   & \#Par &  & \#Par &  & \#Par &  \\   & & Training & Interce & & Training & Interce & & Training & Interce \\  PAM & 22.3M & 0.102 & 0.0330 & 14.1M & 0.090 & 0.0263 & 315K & 0.083 & 0.0018 \\ DNN & 21.8M & 0.046 & 0.0139 & 13.8M & 0.038 & 0.0129 & 380K & 0.056 & 0.0063 \\ DESTINE & 21.5M & 0.130 & 0.0344 & 13.6M & 0.090 & 0.0188 & 384K & 0.072 & 0.0069 \\ DCN & 21.3M & 0.044 & 0.0114 & 13.4M & 0.080 & 0.0099 & 381K & 0.069 & 0.0064 \\ DCN-V2 & 22.0M & 0.103 & 0.0128 & 13.8M & 0.091 & 0.0137 & 458K & 0.059 & 0.0067 \\ AOANE & 21.4M & 0.151 & 0.0314 & 13.4M & 0.338 & 0.0168 & 457K & 0.066 & 0.0067 \\ EDCN & 21.5M & 0.066 & 0.0119 & 13.1M & 0.048 & 0.0113 & 63K & 0.072 & 0.0071 \\   

Table 2: The number of parameters (#Par) and seconds per batch (Sec/Batch) during the training and inference of PAM and baseline models.

Figure 4: Experimental results. Error bars represent the means and standard deviations of AUC for classification and \(R^{2}\) for regression. a) AUC and \(R^{2}\) scores of PAM and comparison methods. b) Hyper-parameter analysis of PAM. U: the initial value of \(U_{n}\). \(D\): the depth of the oblique tree in PAM. c) Ablation studies of PAM.

all baseline methods, we found that our interpretation framework could be used to extract interactions from DCN and AOANet. Fig. 5 presents the top five main effects (Figs. a, c and e) and two-way interaction effects (Figs. b, d and f) between brain regions to brain-age prediction, as determined by the trained PAM, DCN and AOANet.

According to Figs. 5a and 5b, PAM found that the grey matter volumes (GMV) of both the left and right frontal poles (FP) play significant main effect as individual features, which is well aligned with previous research findings[39; 40; 41]. The other three brain regions, lateral occipital cortex (LOC), precentral gyrus (PG) and thalamus (T), are also discussed in early studies[42; 43; 44]. Existing aging studies primarily focus on main effects of GMV, but the top five two-way GMV interactions of identified brain regions have been acknowledged in works such as . As shown in Fig. 5c, 5d, 5e and 5f, the main effects of PG and T identified by PAM were found by DCN and AOANet, respectively, and all three algorithms found the two-way interactions between the left and right FP. In addition to the shared top-5 brain regions identified by PAM, AOANet and DCN, PAM additionally identified the main effect of the LOC and FP and two-way interaction effects related to the insular and subcallosal cortex (SC) regions.

## 7 Conclusions and Future Work

We propose a novel feature interaction learning module, namely Polyhedron Attention Module (PAM), to fit a target with adaptive-order interaction effects. PAM produces a self-attention mechanism partitioning the input space into overlapping polyhedrons and learning the boundary hyperplanes for each polyhedron automatically. These hyperplanes multiply to identify interaction effects specific to individual polyhedrons. Under this mechanism, PAM automatically captures both simple and complex interaction effects. In our theoretic analysis, we show that PAM can enhance the model expression power for universal approximation. Experimental results demonstrate that the proposed model achieves better performance on massive datasets than the state-of-the-art. In the future, we will study how to dynamically pruning PAM's oblique tree during the training process to regularize the model. It will also be interesting to investigate how to define boundaries with non-linear functions.

Figure 5: Top 5 main and 2-way interactions identified by PAM (Figs. a and b), DCN (Figs. c and d) and AOANet (Figs. e and f) by sorting mean absolute values of each \(_{}\) averaged over all participants. For each effect, points (each corresponding to a participant) are distributed according to \(\) values calculated by Algorithm 1 in the beeswarm bars. L: left; R: right; CR: crus II cerebellum; FOC: frontal orbital Cortex; FG: frontal gyrus; PP: planum polare; OCSD: lateral occipital cortex, superior division; FP: frontal pole; LOC: lateral occipital cortex; PG: precentral gyrus; T: thalamus; SC: subcallosal cortex.