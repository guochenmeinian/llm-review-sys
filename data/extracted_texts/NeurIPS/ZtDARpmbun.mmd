# Prospective Representation Learning for

Non-Exemplar Class-Incremental Learning

 Wuxuan Shi\({}^{1}\),  Mang Ye\({}^{1,2}\)

\({}^{1}\)School of Computer Science, Wuhan University, Wuhan, China

\({}^{2}\) Taikang Center for Life and Medical Sciences, Wuhan University, Wuhan, China

{wuxuanshi, yemang}@whu.edu.cn

https://github.com/Shi@wuxuan/NeurIPS2024-PRL

Corresponding Author: Mang Ye

###### Abstract

Non-exemplar class-incremental learning (NECIL) is a challenging task that requires recognizing both old and new classes without retaining any old class samples. Current works mainly deal with the conflicts between old and new classes retrospectively as a new task comes in. However, the lack of old task data makes balancing old and new classes difficult. Instead, we propose a Prospective Representation Learning (PRL) approach to prepare the model for handling conflicts in advance. In the base phase, we squeeze the embedding distribution of the current classes to reserve space for forward compatibility with future classes. In the incremental phase, we make the new class features away from the saved prototypes of old classes in a latent space while aligning the current embedding space with the latent space when updating the model. Thereby, the new class features are clustered in the reserved space to minimize the shock of the new classes on the former classes. Our approach can help existing NECIL baselines to balance old and new classes in a plug-and-play manner. Extensive experiments on several benchmarks demonstrate that our approach outperforms the state-of-the-art methods.

## 1 Introduction

In recent years, deep neural networks (DNNs) have achieved great success in static scenarios. Research attention is increasingly turning to extending the learning capability of DNNs to open and dynamic environments. An important aspect is to enable the network to accumulate knowledge from new tasks as the input stream is updated (_i.e._, incremental learning [1; 2; 3]).

Whenever a new task arrives, it is costly to retrain the model with current and old data. Not to mention that the old data is not fully available. A typical alternative is to fine-tune the network with new data directly. However, this can lead to drastic performance degradation on previously learned tasks, a phenomenon known as catastrophic forgetting [4; 5]. While storing exemplars of each class is a simple approach to mitigate forgetting, it relies on the quality of saved exemplars and faces challenges in storage and privacy, especially for sensitive domains such as medical imaging. Hence, this paper focuses on coping with catastrophic forgetting during incremental learning without storing any old samples, which is called non-exemplar class-incremental learning (NECIL) [6; 7].

In NECIL, a serious challenge is to discriminate between old and new classes without access to old data. Most methods usually start considering handling conflicts between old and new classes only when new tasks arrive. While some methods use stored prototypes to model the distribution of old classes [8; 6; 9; 10], others extend the network structure to accommodate new classes [7; 11]. However, in the base phase (_i.e._, training on the first task), traditional training allows different classesto divide up all the embedding space, causing trouble for subsequent conflict resolution. As shown in Fig. 1, in the incremental phase (_i.e._, training on tasks after the first one), with the influx of new classes, there are overlaps of the old and new classes in the embedding space that are difficult to discriminate. Moreover, due to the unavailability of old class samples, handling this conflict with only new task data is intractable. Instead, we suggest addressing this issue by learning prospectively at the feature level, which requires a two-pronged effort in both the base and incremental phases.

Firstly, the model should make room in advance for the incoming classes in the future. Thus, the space of past classes does not need to be drastically squeezed when expanding new classes. To this end, during the base phase, we construct a preemptive embedding squeezing constraint to enforce intra-class concentration and inter-class reserved separation. Specifically, we push instances from the same class closer together and instances from different classes farther apart in a mini-batch. It allows more space to be reserved in the initial embedding space, thus making the model ready for future classes.

Secondly, the model should minimize the shock and impact of the new classes on the past classes, _i.e._, embed the new classes into the reserved space as much as possible. However, achieving the desired embedding of new classes when the old class data is fully unavailable is difficult. Inspired by previous works [6; 10], we try to accomplish this using prototypes (typically the class mean in the deep feature space) saved for each old class. During the incremental phase, we propose a prototype-guided representation update mechanism. Concretely, we use the network learned from previous tasks to extract features from new task samples and project these features and the saved prototypes into a latent space. In the latent space, the new class features are pushed away from the region hosting the old class and embedded as much as possible in the reserved space with the help of prototypes. We guide the update of the current model representation through the latent space to reduce the shock of the new classes on the former classes.

In summary, combining the above two ideas, our Prospective Representation Learning (PRL) scheme makes the following main contributions:

* We impose a preemptive embedding squeezing constraint to reserve space for future classes by reinforcing intra-class concentration and inter-class reserved separation.
* We propose a prototype-guided representation update strategy that utilizes the saved prototypes to reduce the impact of expanding new classes on old ones.
* Extensive experiments on four benchmarks suggest the superior performance of our approach over the state-of-the-art. We also provide a detailed analys of our method.

## 2 Related Work

### Class-Incremental Learning

Mainstream CIL methods can be roughly divided into three categories: rehearsal-based methods, regularization-based methods, and structure-based methods.

Figure 1: The traditional training paradigm in NECIL considers conflicts between old and new classes only when new classes arrive and is prone to overlap. We suggest prospective learning to reduce conflicts: (1) reserve space for unknown classes; (2) make the newly coming class embedded in the reserved space.

**Rehearsal-based methods** store a portion of seen data in a fixed-size memory buffer and replay it as new data arrives. Based on the stored data, some works use knowledge distillation techniques to protect existing knowledge [12; 13; 14; 15], while others regularize the gradient to make more efficient use of the stored samples [16; 17; 18]. Additionally, several works design new strategies for memory management instead of simple random sampling. [19; 20; 21; 22]. Although rehearsal-based methods effectively mitigate catastrophic forgetting, they are encumbered by privacy concerns and become impractical under stringent storage constraints.

**Regularization-based methods** estimate the importance of different parameters for past tasks and then limit the updating of these important parameters when learning new tasks [23; 24; 25; 26]. In incremental learning, the storage of importance weights becomes essential. However, these methods are encumbered by constraints on model parameters, consequently impeding knowledge transfer and leading to suboptimal performance, particularly in long-sequence task streams.

**Structure-based methods** accommodate knowledge from new tasks by dynamically modifying the network structure. Some works extend the network by assigning new parameters of different forms to new tasks [27; 28; 29; 30]. While this approach adeptly manages extended task sequences and sustains the performance of established classes, the linear growth of network parameters with the number of tasks and the necessity for reasoning across multiple forward propagations pose significant challenges. Parameter fusion  and selecting partial parameters for expansion  mitigates this problem to some extent. An alternative is to mask part of the parameters that are highly correlated with the previous task at the parameter level or unit level [33; 34; 35; 36]. Their performance is limited by the backbone obtained on the first task.

### Non-Exemplar Class-Incremental Learning

Recently, some works begin to focus on NECIL [8; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46], due to privacy and memory concerns [47; 48; 49], where the algorithms have no access to any past data. Li _et al._ combine knowledge distillation with fine-tuning in a first attempt at incremental learning without a memory buffer. Zhu _et al._ propose class augmentation and semantic augmentation to address the representation bias and classifier bias caused by the lack of old task data. Yin _et al._ use model inversion technology to generate samples from previous tasks to alleviate forgetting. Based on , Gao _et al._ introduce relation-guided knowledge distillation to address the distributional gap between generated data and real data.

Zhu _et al._ combat catastrophic forgetting for the first time by preserving prototypes and augmenting them. Yu _et al._ address the problem of prototype outdating in the current representation space by estimating the semantic drift of past tasks and compensating for it. Furthermore, Toldo _et al._ subdivide the drift into feature drift and semantic drift and compensate for both, thereby achieving better results. Shi _et al._ inject information about the current feature distribution into the prototype to model the distribution of past tasks. Wang _et al._ improve the prototype augmentation method based on density to make the model more focused on features of the old class with low density. Malepathirana _et al._ use the domain information obtained from topological relations to optimize prototype augmentation to reduce inter-class overlap. However, previous works deal with conflicts between old and new classes only after the new data arrives and lack prospective consideration.

### Embedding Space Regularization

Embedding space regularization has been extensively studied in literature [53; 54; 55; 56; 57]. Chaudhry _et al._ first propose learning tasks in different (low-rank) embedding subspaces that are kept orthogonal to each other. They learn an isometric mapping by formulating network training as an optimization problem on the Stiefel manifold. Another idea is to implement orthogonality in the gradient space. Saha _et al._ analyze network representations after learning each task with Singular Value Decomposition (SVD) to find the basis of the subspaces and store them in memory. Moreover, several methods promote forward compatibility through regularization in the initial phase. Zhou _et al._ assign virtual prototypes to compress embeddings of known classes and reserve space for new classes. Shi _et al._ encourage initial CIL learners to generate representations that are similar to those of models trained jointly on all classes. Compared to previous works, we target CIL in exemplar-free scenarios (NECIL). We consider how to resolve conflicts between new and old classes during the incremental phases, in addition to reserving space in the initial phase.

## 3 Methodology

### Problem Statement

The goal of NECIL is to continually train a unified model over a series of tasks to recognize all classes learned so far. The data stream can be defined as \(D=\{D_{0},D_{1}, D_{T}\}\), where \(T\) is the number of incremental phases. At any phase \(i\), the training set \(D_{i}\) consists of the sample set \(X_{i}(0 i T)\) and the label set \(Y_{i}\). In particular, the classes of all phases are disjoint, _i.e._, \(Y_{i} Y_{j}^{-}=, i j\). It is notable that only \(D_{t}\) is available at current phase \(t\). There are no old training sets (_i.e._, \(D_{0:t-1}\)) to access or save in memory. To facilitate analysis, we represent the model with two components: a feature extractor \(\) with parameters \(\) and a unified classifier \(\) with parameters \(\). For a comprehensive evaluation of the model, the test set at phase \(t\) includes classes from all the seen label sets \(Y_{0} Y_{1} Y_{t}\). At the time of testing, the model does not have access to the task ID, _i.e._, it does not know from which task the test sample come.

### Baseline

We adapt the paradigm of existing NECIL works [6; 7; 11; 10] as our baseline, which primarily uses knowledge distillation and prototype rehearsal. Specifically, at the base phase (_i.e._, \(t=0\)), the classification model is optimized under full supervision:

\[*{argmin}_{_{i},_{t}}_{t}=_{ ce}(_{t},_{t};D_{t})=-*{}_{(x,y) D _{t}}[y_{_{t}}(_{_{t}}(x)))}],\] (1)

where \(_{t}\) represents the overall loss function, \(_{ce}\) is the cross-entropy loss.

At the incremental phase (_i.e._, \(t>0\)), standard fully supervised training seeks to minimize the following objective:

\[_{t}=_{ce}(_{t},_{t};D_{0:t-1})+_{ce}(_{t},_{t};D_{t}).\] (2)

However, this is especially challenging since previous training sets \(D_{0:t-1}\) are assumed to be unavailable in the NECIL setting. The absence of the first term in eq. (2) leads to a bias in favor of current classes in the feature extractor \(_{_{t}}\) and the classifier \(_{_{t}}\). To address this problem, existing methods [38; 6; 10] adopt knowledge distillation and prototype rehearsal to cope with the bias. Specifically, they take the frozen feature extractor \(_{_{t-1}}\) from the previous phase \(t-1\) as a teacher and the current one \(_{_{t}}\) as a student. A distillation term is introduced to encourage the model to mimic the previous representation:

\[_{kd}(_{t};_{t-1},D_{t})=_{x X_{i}}\|_{_{t}}(x)-_{_{t-1}}(x)\|_{2},\] (3)

where \(\|\|_{2}\) denotes Euclidean distance. Knowledge distillation helps maintain existing knowledge in \(_{_{t-1}}\), thus mitigating bias in the current feature extractor.

For the bias in the classifier, we use class-representative prototypes  to balance the optimization. Specifically, after the training of \(t-1\) phase, we compute a prototype \(^{c}\) for each class \(c\):

\[^{c}=*{}_{(x,y) D_{t-1}}[_{_ {t-1}}(x) y=c].\] (4)

All prototypes of learned classes \(_{0:t-1}=\{^{c},c\}_{c Y_{0:t-1}}\) are stored in memory. In each training iteration of current phase \(t\), existing works [6; 38; 10] augment the memorized prototypes \(_{0:t-1}\) to \(}_{0:t-1}\) and train the classifier jointly with the current data \(D_{t}\). In particular, the prototypes are involved in the standard classification optimization with the following objective:

\[_{pro}(_{t};}_{0:t-1})=-*{ }_{(}^{c},c)}_{0:t-1}}[c_{_{t}}(}^{c}))}].\] (5)

Compared to exemplar rehearsal, prototype rehearsal is more memory efficient and privacy secure. In conclusion, the overall loss function of the baseline can be expressed as:

\[_{t}=_{ce}(_{t},_{t};D_{t})+_{1} _{kd}(_{t};_{t-1},D_{t})+_{2}_{pro}( _{t};}_{0:t-1}),\] (6)

where \(_{1}\) and \(_{2}\) are the weights of the distillation loss and prototype loss, respectively. The specific implementation of prototype augmentation is not our focus. In this paper, we implement our approach based on the pipeline in PRAKA . Our method can be incorporated with different augmentations and plugged into other baselines, such as PASS  and IL2A .

### Prospective Representation Learning

An overview of our Prospective Representation Learning scheme is shown in Fig. 2. It consists of a preemptive embedding squeezing constraint in the base phase and a prototype-guided representation update strategy in the incremental phase. The specific implementation of the two components is described in the following.

**Preemptive Embedding Squeezing.** In the base phase (\(t=0\)), a common training paradigm of NECIL is to optimize the empirical loss over the training set \(D_{t}\) as eq. (1). Without consideration of the future incremental learning, it overspreads the embedding space. As new classes come in, the embedding of old classes needs to be squeezed to make room for new ones. However, striking a balance in this process is challenging, especially without the old data. Therefore, we would like to be proactive and reserve space for future classes by squeezing the embedding of current classes in the base phase. Specifically, we impose a preemptive embedding squeezing (PES) constraint to cluster features of the same class and make features of different classes separate from each other. To reduce complexity, the PES loss is computed over a mini-batch data \(B=\{x^{i},y^{i}\}_{i=1}^{n} D_{t}\), which can be formulated as:

\[s=_{ x^{i},x^{j} B\\ y_{i}=y_{j}}_{_{t}}(x^{i}),_ {_{t}}(x^{j}),\] (7)

\[d=_{ x^{i},x^{k} B\\ y_{i} y_{k}}_{_{t}}(x^{i}),_{_{t}}(x^{k}),\] (8)

\[_{PES}(_{t};D_{t})=(1-s)+*(1+d),\] (9)

where \(n\) is the batch size, \(,\) denotes the cosine similarity operator. As \(_{PES}\) is minimized, the first term \((1-s)\) facilitates intra-class concentration, and the second term \((1+d)\) aims to reinforce inter-class reserved separation, as shown in Fig. 2 (A). Since \(s,d[-1,1)\), both terms are greater than zero. The hyper-parameter \(\) controls the priority ratio of intra-class constraints and inter-class constraints. Since our PES is implemented in a vectorized manner on the mini-batch, it does not incur excessive computational burden.

With the preemptive embedding squeezing constraint, the optimization objective for the base phase training in eq. (1) can be rewritten as:

\[_{t}=_{ce}(_{t},_{t};D_{t})+* _{PES}(_{t};D_{t}).\] (10)

where \(\) is a hyperparameter controlling the weights of loss.

Figure 2: Overview of our Prospective Representation Learning (PRL) for NECIL. (A) During the base phase, we impose a preemptive embedding squeezing (PES) constraint to squeeze the space of the current class in preparation for accepting future new classes. (B) During the incremental phase, a prototype-guided representation update (PGRU) strategy is proposed to keep new class features away from old class prototypes in the latent space, which guides the update of the current model to mitigate the confusion of new classes with old classes.

**Prototype-Guided Representation Update.** In the incremental phase (\(t>0\)), we would like to embed the new class into the previously reserved space. The plain idea is to keep the new classes well clustered and disanced from the old ones. To this end, we propose a prototype-guided representation update (PGRU) strategy, as shown in Fig. 2 (B), which employs prototypes as proxies for past classes to guide the embedding of new classes into the appropriate space. However, it is not practical to establish a relationship directly between the saved prototypes and the new class features extracted by the current model due to the continual updating of the current embedding space. To mitigate the mismatch between the old class prototype and the new class features, on the one hand, we use the frozen model from the previous phase \(t-1\) to extract the new class features, which has been implemented in the baseline as shown in eq. (3); on the other hand, the new classes features and the saved prototypes are projected into a unified latent space. Then, we construct orthogonal structures between the new class features and the old class prototypes in the latent space:

\[_{ort}=_{ x^{i} B\\ ^{j}_{0:t-1}}|_{_ {t}}(_{_{t-1}}(x^{i})),_{_{t}}(^{c}) |,\] (11)

where \(||\) denotes the absolute value operator, \(\) is a projector with parameters \(\). Similarly, \(_{ort}\) is also implemented in the mini-batch to reduce computational costs. Inspired by , we use a simple undercomplete autoencoder as the projector. It consists of a linear layer followed by ReLU activation that maps the features to a low-dimensional subspace and another linear layer followed by sigmoid activation that maps the features back to high dimensions. When minimizing \(_{ort}\), it will promote orthogonality between the new class features and the old class prototypes. By the above operations, we would like to allow the new class of features to be embedded in appropriate positions and to keep clustering in the latent space.

Our ultimate goal is to guide the update of the current model. Hence, we align the current embedding space with the latent space as:

\[_{align}=_{x X_{i}}_{MSE}(_{_{t}} (_{_{t-1}}(x^{i})),_{_{t}}(x^{i})),\] (12)

where \(_{MSE}\) is mean squared error (MSE) loss. In summary, the PGRU loss can be defined as:

\[_{PGRU}=_{ort}(_{t};D_{t},_{0:t-1})+_{align}(_{t},_{t};D_{t}).\] (13)

In the incremental phase, the optimization objective in eq. (6) can be rewritten as:

\[_{t}= _{ce}(_{t},_{t};D_{t})+_{1}_{kd}(_{t};_{t-1},D_{t})+\] (14) \[_{2}_{pro}(_{t};}_{0:t-1})+ _{3}_{PGRU}(_{t},_{t};D_{t},_{0:t-1}).\]

The main procedure is summarized in algorithm 1.

Experiment

### Experimental Setting

**Dataset.** We conduct comprehensive experiments on four public datasets: CIFAR-100 , TinyImageNet , ImageNet-Subset and ImageNet-1K . CIFAR-100 consists of 100 classes, where each class contains 500 training images and 100 testing images with size 32x32. TinyImageNet has 200 classes in total, and the image size is 64x64. Each class in TinyImageNet contains 500 training images and 50 testing images. ImageNet-1K is a large-scale dataset comprising about 1.28 million images for training and 50,000 for validation with 500 images per class. ImageNet-Subset is a 100-class subset randomly chosen (random seed 1993) from the original ImageNet-1K. The image size of ImageNet-1K is much larger than the other two datasets, which poses a test of sensitivity to large-scale data.

**Protocol.** Following the setting in [6; 7; 10], we divide around half the classes for the base phase, and the rest are divided equally into all the incremental phases. _For CIFAR-100 and ImageNet-Subset_: 1) 50 classes for base phase and 5 incremental phases of 10 classes; 2) 50 classes for base phase and 10 incremental phases of 5 classes; 3) 40 classes for base phase and 20 incremental phases of 3 classes. _For TinyImageNet_, we start by training the model with 100 classes in the base phase and distribute the remaining classes into three incremental settings: 1) 5 incremental phases of 20 classes; 2) 10 incremental phases of 100 classes; 3) 20 incremental phases of 5 classes.

**Implementation details.** Our method is implemented with PyCIL . For a fair comparison with , we adopt ResNet-18  as the backbone network. The batch size is set to 64 for CIFAR-100 and TinyImageNet and 128 for ImageNet-Subset and ImageNet-1K. During training, the model is optimized by the Adam optimizer with \(_{1}=0.9\), \(_{2}=0.999\) and \(=1e^{-8}\) (weight decay 2e-4). For ImageNet-1K, the learning rate starts at 0.0005 for all phases. The learning rate decays to 1/10 of the previous value every 70 epochs (160 epochs in total) in the base phase and every 45 epochs (100 epochs in total) in each incremental phase. For other datasets, the learning rate starts from 0.001 and decays to 1/10 of the previous value every 45 epochs (100 epochs in total) for all phases. We use \(=0.5\) and \(=0.1\) for all datasets. Regarding the loss weights, for comprehensive performance considerations and with reference to previous studies [6; 51], we set \(_{1}=10\), \(_{2}=10\), and \(_{3}=2\) for training. We conduct our experiments on an RTX4090 GPU.

**Metric.** We evaluate the methods in terms of average incremental accuracy. Average incremental accuracy \(A_{T}\) is computed as the average of the accuracy of all phases (including the base phase) and is a fair metric to compare the overall incremental performance of different methods:

\[A_{T}=_{t=0}^{T}a_{t},\] (15)

where \(a_{t}\) is the average accuracy over all seen classes on phase \(t\).

### Comparison with SOTA

We compare our method with the state-of-the-art (SOTA) methods of NECIL (EWC , LwF_MC , MUC , SDC , PASS , SSRE , SOPE , POLO , PRAKA  and NAPA-VQ ). _"Fine-tuning"_ refers to continuously fine-tuning the network on the new task with only cross-entropy loss. _"Joint"_ means that when learning a new task, all data from past tasks are available to jointly train the model, which can be considered as an upper bound of the CIL model. The results reported for PASS are obtained with self-supervised learning.

The quantitative comparisons of average incremental accuracy are reported in Tab. 1. In comparison with the SOTA, our method improves by 1.4% and 6.0% on CIFAR-100 and TinyImageNet datasets, respectively. To further investigate the behavior of different methods on larger data, we also evaluated their performance on ImageNet-Subset. Compared with suboptimal results, PRL achieves an average improvement of 3.6%. The outstanding performance on ImageNet-Subset demonstrates the reliability of our method. To provide a more nuanced view of the changes in performance of the different methods over the course of incremental learning, we show accuracy curves for CIFAR-100, TinyImageNet and ImageNet-Subset in Fig. 3. The accuracy of our method remains ahead as we continue to learn new tasks. By prospective learning, our approach demonstrates strengths early on that will be maintained and even enlarged over the course of continuously learning new tasks.

### Ablation Study

To analyze the impact of each component in our method, we perform several ablation studies on CIFAR-100 and TinyImageNet datasets. We use the prototype augmentation technique in  as eq. (5) in our baseline. As shown in Tab. 2, The first line shows the performance of our baseline model. Our baseline is strong due to using the prototype augmentation in . Even on the strong baseline model, both preemptive embedding squeezing (PES) constraint and prototype-guided representation update (PGRU) strategy can bring considerable performance improvements. Furthermore, the table shows that PES plays a more central role than PGRU. This is reasonable since the space reserved by PES for future classes is the basis for the PGRU to guide new classes to embed in the representation space during the incremental phase.

### Analysis

**Visualization.** To analyze the impact of PRL on representation learning, we visualize the embedding space of 2D feature vectors on CIFAR-100 (5 phases) with t-SNE  in Fig. 4. Specifically, we (1)

    &  &  &  \\   & _P_=5 & _P_=10 & _P_=20 & _P_=5 & _P_=10 & _P_=20 & _P_=5 & _P_=10 & _P_=20 \\  Fine-tuning & 23.15 & 12.96 & 7.93 & 18.64 & 10.68 & 5.75 & 23.43 & 13.12 & 7.96 \\ Joint & 76.72 & 76.72 & 76.72 & 63.08 & 63.08 & 63.08 & 78.94 & 78.94 & 78.94 \\  EWC  & 24.48 & 21.20 & 15.89 & 18.80 & 15.77 & 12.39 & — & 20.40 & — \\ LwF\_MC  & 45.93 & 27.43 & 20.07 & 29.12 & 23.10 & 17.43 & — & 31.18 & — \\ MUC  & 49.42 & 30.19 & 21.27 & 32.58 & 26.61 & 21.95 & — & 35.07 & — \\ SDC  & 56.77 & 57.00 & 58.90 & — & — & — & — & 61.12 & — \\ PASS  & 63.47 & 61.84 & 58.09 & 49.55 & 47.29 & 42.07 & 64.40 & 61.80 & 51.29 \\ SSRE  & 65.88 & 65.04 & 61.70 & 50.39 & 48.93 & 48.17 & — & 67.69 & — \\ SOPE  & 66.64 & 65.84 & 61.83 & 53.69 & 52.88 & 51.94 & — & 69.22 & — \\ POLO  & 68.95 & 68.02 & 65.71 & 54.90 & 53.38 & 49.93 & 70.81 & 69.11 & — \\ PRAKA  & 70.02 & 68.86 & 65.86 & 53.32 & 52.61 & 49.83 & 69.81 & 68.98 & 63.95 \\ NAPA  & 70.44 & 69.04 & 67.42 & 52.77 & 51.78 & 49.51 & 69.15 & 68.83 & 63.09 \\  PRL (Ours) & **71.26** & **70.17** & **68.44** & **58.12** & **57.24** & **54.51** & **72.85** & **71.54** & **66.88** \\ Improvement & +0.82 & +1.13 & +1.02 & +3.22 & +3.86 & +2.57 & +2.04 & +2.32 & +2.93 \\   

Table 1: Quantitative comparisons of the average incremental accuracy (%) with other methods on CIFAR-100, TinyImageNet and ImageNet-Subset. \(P\) represents the number of incremental phases. The best performance is shown in **bold**, and the sub-optimal performance is underlined. The relative improvement compared to the SOTA NECIL methods is shown in red.

Figure 3: Detailed accuracy curves showing the top-1 accuracy of each incremental phase on CIFAR-100, TinyImageNet and ImageNet-Subset.

visualize the features of a randomly selected subset of classes from \(D_{0}\) (old class features) after the base phase, and (2) visualize the old class features along with a subset of classes from \(D_{T}\) (new class features) after the last phase. As shown in the first row, once the training of the base phase (\(t=0\)) is complete, the model integrated with PRL has more tightly clustered intra-class distributions (blue circles) and more dispersed inter-class distributions (\(\)). Thus, more space is reserved for learning new classes. The second row is visualized after the last phase (\(t=T=5\)). It can be observed that the overlap (red circles) in the baseline model increases, causing confusion between the old and new classes. In contrast, PRL reduces the overlap between classes, making them easier to distinguish. Moreover, the new classes are farther away from the old ones (\(\)) compared to the baseline.

**Comparison of the confusion matrix.** Figure 5 compares the confusion matrices obtained by fine-tuning, PASS , NAPA-VQ  and our PRL on CIFAR-100. The diagonal entries indicate correct classification, while the non-diagonal entries indicate misclassification. Due to the forgetting of old classes, fine-tuning produces predictions that are biased toward the most recent classes, showing a strong confusion on the last task. PASS clearly mitigates this confusion but still predicts more intensively on recent tasks. The predictions of NAPA-VQ are largely centered on the diagonal, but its predictions are more accurate for the initial classes that appear in the base phase (the red patches are more localized in the first half of the diagonal). In contrast, there are more red patches visible along the diagonal and more evenly distributed in the confusion matrix of PRL, which explains the higher average accuracy of our method compared to NAPA-VQ and the absence of a serious bias towards either new or old classes.

   &  &  \\   & \(P\)=5 & \(P\)=10 & \(P\)=20 & \(P\)=5 & \(P\)=10 & \(P\)=20 \\   baseline & 69.25 & 68.52 & 65.93 & 55.04 & 54.15 & 51.65 \\ baseline w/ PES & 70.57 & 69.64 & 67.58 & 57.08 & 55.84 & 53.58 \\ baseline w/ PGRU & 70.36 & 69.23 & 67.17 & 56.79 & 56.05 & 53.16 \\ PRL & **71.26** & **70.17** & **68.44** & **58.12** & **57.24** & **54.51** \\  

Table 2: Ablation study (in average incremental accuracy) of our method on CIFAR-100 and TinyImageNet datasets.

Figure 4: Visualization of the impact of PRL on the feature representations. Dashed circles and arrows highlight observable differences between baseline and PRL. PRL visually concentrates the distribution of features within classes, disperses the distribution of features between classes, and mitigates inter-class confusion.

Figure 5: The comparison of confusion matrix of fine-tuning, PASS, NAPA-VQ and our method on CIFAR-100 (10 phases).

**Plasticity and stability analysis.** An incremental learner should acquire new knowledge of the current task for the sake of plasticity and also preserve knowledge from previous tasks for the sake of stability [70; 71]. We present an analysis of the plasticity and stability of the different methods in Fig. 6. First, we observe a gradual decline in average performance on past tasks during incremental learning. This is rational because experiencing more tasks also results in heavier catastrophic forgetting. Nonetheless, our method exhibits better stability due to less degradation and consistently superior average performance on old tasks. Then we turn our attention to the current task and also found a performance degradation as more and more tasks are learned. This corresponds to a gradual reduction in plasticity since tasks are sampled uniformly from the set of possible tasks, which is consistent with observations from previous studies [72; 73]. PRAKA  starts with good performance, but its plasticity degrades as more tasks are learned. NAPA-VQ consistently performs poorly on the current task, which is also in line with the results in Fig. 5. Remarkably, PRL maintains a good performance on the current task and has yet to show a visible decline. In general, our method achieves a better trade-off between stability and plasticity.

## 5 Conclusion and Limitation

In this work, we consider the conflict between old and new classes in NECIL from a prospective view. In the base phase, we construct a preemptive embedding squeezing constraint to reserve space for future classes by enforcing intra-class concentration and inter-class reserved separation. In the incremental phase, we propose a prototype-guided representation update (PGRU) strategy, which reduces the impact on the old class during model update by keeping the new class embedding away from the old class prototype. In cases where exemplars cannot be saved, waiting until the conflict arrives could exacerbate the problem, and we offer a novel solution. Through extensive experiments on four public benchmarks, our method exhibits excellent average performance and can provide a good balance between stability and plasticity. However, since the number and distribution of unknown classes cannot be predicted, how to rationally allocate the space of base classes in prospective learning is open to further discussion.