# Is Distance Matrix Enough for Geometric Deep Learning?

Zian Li\({}^{1,2}\), Xiyuan Wang\({}^{1,2}\), Yinan Huang\({}^{1}\), Muhan Zhang\({}^{1,}\)

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)School of Intelligence Science and Technology, Peking University

Corresponding author: Muhan Zhang (muhan@pku.edu.cn).

###### Abstract

Graph Neural Networks (GNNs) are often used for tasks involving the 3D geometry of a given graph, such as molecular dynamics simulation. While incorporating Euclidean distance into Message Passing Neural Networks (referred to as Vanilla DisGNN) is a straightforward way to learn the geometry, it has been demonstrated that Vanilla DisGNN is geometrically incomplete. In this work, we first construct families of novel and symmetric geometric graphs that Vanilla DisGNN cannot distinguish even when considering all-pair distances, which greatly expands the existing counterexample families. Our counterexamples show the inherent limitation of Vanilla DisGNN to capture symmetric geometric structures. We then propose \(k\)-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of \(k\)-DisGNNs from three perspectives: 1. They can learn high-order geometric information that cannot be captured by Vanilla DisGNN. 2. They can unify some existing well-designed geometric models. 3. They are universal function approximators from geometric graphs to scalars (when \(k 2\)) and vectors (when \(k 3\)). Most importantly, we establish a connection between geometric deep learning (GDL) and traditional graph representation learning (GRL), showing that those highly expressive GNN models originally designed for GRL can also be applied to GDL with impressive performance, and that existing complicated, equivariant models are not the only solution. Experiments verify our theory. Our \(k\)-DisGNNs achieve many new state-of-the-art results on MD17.

## 1 Introduction

Many real-world tasks are relevant to learning the geometric structure of a given graph, such as molecular dynamics simulation, physical simulation, drug designing and protein structure prediction . Usually in these tasks, the coordinates of nodes and their individual properties, such as atomic numbers, are given. The goal is to accurately predict both invariant properties, such as the energy of the molecule, and equivariant properties, such as the force acting on each atom. This kind of graphs is also referred to as _geometric graphs_ by researchers .

In recent years, Graph Neural Networks (GNNs) have achieved outstanding performance on such tasks, as they can learn a representation for each graph or node in an end-to-end fashion, rather than relying on handcrafted features. In such setting, a straightforward idea is to incorporate Euclidean distance, the most basic geometric feature, into Message Passing Neural Network  (we call these models Vanilla DisGNN) . While Vanilla DisGNN is simple yet powerful when considering all-pair distances (we assume all-pairdistance is used by default to research a model's maximal expressive power), it has been proved to be geometrically incomplete (Zhang et al., 2021; Garg et al., 2020; Schutt et al., 2021; Pozdnyakov and Ceriotti, 2022), i.e., there exist pairs of geometric graphs which Vanilla DisGNN cannot distinguish. It has led to the development of various GNNs which go beyond simply using _distance_ as input. Instead, these models use complex group irreducible representations, first-order equivariant representations or manually-designed complex invariant geometric features such as angles or dihedral angles to learn better representations from geometric graphs. It is generally believed that pure distance information is insufficient for complex GDL tasks (Klicpera et al., 2020; Schutt et al., 2021).

On the other hand, it is well known that the _distance matrix_, which contains the distances between all pairs of nodes in a geometric graph, holds all of the geometric structure information (Satorras et al., 2021). This suggests that it is possible to obtain all of the desired geometric structure information from _distance graphs_, i.e., complete weighted graphs with Euclidean distance as edge weight. Therefore, the complex GDL task with node coordinates as input (i.e., 3D graph) may be transformed into an equivalent graph representation learning task with distance graphs as input (i.e., 2D graph).

Thus, a desired question is: Can we design theoretically and experimentally powerful geometric models, which can learn the rich geometric patterns purely from distance matrix?

In this work, we first revisit the counterexamples in previous work used to show the incompleteness of Vanilla DisGNN. These existing counterexamples either consider only finite pairs of distance (a cutoff distance is used to remove long edges) (Schutt et al., 2021; Zhang et al., 2021; Garg et al., 2020), which can limit the representation power of Vanilla DisGNN, or lack diversity thus may not effectively reveal the inherent limitation of Vanilla DisGNN (Pozdnyakov and Ceriotti, 2022). In this regard, we further constructed plenty of novel and symmetric counterexamples, as well as a novel method to construct families of counterexamples based on several basic units. These counterexamples significantly enrich the current set of counterexample families, and can reveal the inherent limitations of MPNNs in _capturing symmetric configurations_, which can explain the reason why they perform badly in real-world tasks where lots of symmetric (sub-)structures are included.

Given the limitations of Vanilla DisGNN, we propose \(k\)-DisGNNs, models that take pair-wise distance as input and aim for learning the rich geometric information contained in geometric graphs. \(k\)-DisGNNs are mainly based on the well known \(k\)-(F)WL test (Cai et al., 1992), and include three versions: \(k\)-DisGNN, \(k\)-F-DisGNN and \(k\)-E-DisGNN. We demonstrate the superior geometric learning ability of \(k\)-DisGNNs from three perspectives. We first show that \(k\)-DisGNNs can capture arbitrary \(k\)- or \((k+1)\)-order **geometric information** (multi-node features), which cannot be achieved by Vanilla DisGNN. Then, we demonstrate the high **generality** of our framework by showing that \(k\)-DisGNNs can implement DimeNet (Klicpera et al., 2020) and GemNet (Gasteiger et al., 2021), two well-designed state-of-the-art models. A key insight is that these two models are both augmented with manually-designed high-order geometric features, including angles (three-node features) and dihedral angles (four-node features), which correspond to the \(k\)-tuples in \(k\)-DisGNNs. However, \(k\)-DisGNNs can learn _more than_ these handcrafted features in an end-to-end fashion. Finally, we demonstrate that \(k\)-DisGNNs can act as **universal function approximators** from geometric graphs to **scalars** (i.e., E(3) invariant properties) when \(k 2\) and **vectors** (i.e., first-order O(3) equivariant and translation invariant properties) when \(k 3\). This essentially answers the question posed in our title: _distance matrix is sufficient for GDL_. We conduct experiments on benchmark datasets where our models achieve **state-of-the-art results** on a wide range of the targets in the MD17 dataset.

Our method reveals the high potential of the most basic geometric feature, _distance_. Highly expressive GNNs originally designed for traditional GRL can naturally leverage such information as edge weight, and achieve high theoretical and experimental performance in geometric settings. This **opens up a new door for GDL research** by transferring knowledge from traditional GRL, and suggests that existing complex, equivariant models may not be the only solution.

## 2 Related Work

**Equivariant neural networks.** Symmetry is a rather important design principle for GDL (Bronstein et al., 2021). In the context of geometric graphs, it is desirable for models to be equivariant or invariant under the group (such as E(3) and SO(3)) actions of the input graphs. These models include those using group irreducible representations (Thomas et al., 2018; Batzner et al., 2022; Anderson et al., 2019; Fuchs et al., 2020), complex invariant geometric features (Schutt et al., 2018; Klicperaet al., 2020; Gasteiger et al., 2021) and first-order equivariant representations (Satorras et al., 2021; Schutt et al., 2021; Tholke and De Fabritiis, 2021). Particularly, Dym and Maron (2020) proved that both Thomas et al. (2018) and Fuchs et al. (2020) are universal for SO(3)-equivariant functions. Besides, Villar et al. (2021) showed that one could construct powerful (first-order) equivariant outputs by leveraging invariances, highlighting the potential of invariant models to learn equivariant targets.

**GNNs with distance.** In geometric settings, incorporating 3D distance between nodes as edge weight is a simple yet efficient way to improve geometric learning. Previous work (Maron et al., 2019; Morris et al., 2019; Zhang and Li, 2021; Zhao et al., 2022) mostly treat distance as an auxiliary edge feature for better experimental performance and do not explore the expressiveness or performance of using pure distance for geometric learning. Schutt et al. (2018) proposes to expand distance using a radial basis function as a continuous-filter and perform convolutions on the geometric graph, which can be essentially unified into the Vanilla DisGNN category. Zhang et al. (2021); Garg et al. (2020); Schutt et al. (2021); Pozdnyakov and Ceriotti (2022) demonstrated the limitation of Vanilla DisGNN by constructing pairs of non-congruent geometric graphs which cannot be distinguished by it, thus explaining the poor performance of such models (Schutt et al., 2018; Kearnes et al., 2016). However, none of these studies proposed a complete purely distance-based geometric model. Recent work (Hordan et al., 2023) proposed theoretically complete GNNs for distinguishing geometric graphs, but they go beyond pair-wise distance and instead utilize gram matrices or coordinate projection.

**Expressive GNNs.** In traditional GRL (no 3D information), it has been proven that the expressiveness of MPNNs is limited by the Weisfeiler-Leman test (Weisfeiler and Leman, 1968; Xu et al., 2018; Morris et al., 2019), a classical algorithm for graph isomorphism test. While MPNNs can distinguish most graphs (Babai and Kucera, 1979), they are unable to count rings, triangles, or distinguish regular graphs, which are common in real-world data such as molecules (Huang et al., 2023). To increase the expressiveness and design space of GNNs, Morris et al. (2019, 2020) proposed high-order GNNs based on \(k\)-WL. Maron et al. (2019) designed GNNs based on the folklore WL (FWL) test (Cai et al., 1992) and Azizian and Lelarge (2020) showed that these GNNs are the most powerful GNNs for a given tensor order. Beyond these, there are subgraph GNNs (Zhang and Li, 2021; Bevilacqua et al., 2021; Frasca et al., 2022; Zhao et al., 2021), substructure-based GNNs (Bouritsas et al., 2022; Horn et al., 2021; Bodnar et al., 2021) and so on, which are also strictly more expressive than MPNNs. For a comprehensive review on expressive GNNs, we refer the readers to Zhang et al. (2023).

More related work can be referred to Appendix E.

## 3 Preliminaries

In this paper, we denote multiset with \(\{\!\}\). We use \([n]\) to represent the set \(\{1,2,...,n\}\). A complete weighted graph with \(n\) nodes is denoted by \(G=(V,)\), where \(V=[n]\) and \(=[e_{ij}]_{n n}^{n n}\). The neighborhoods of node \(i\) are denoted by \(N(i)\).

**Distance graph vs. geometric graph.** In many tasks, we need to deal with geometric graphs, where each node \(i\) is attached with its 3D coordinates \(_{i}^{3}\) in addition to other invariant features. Geometric graphs contain rich geometric information useful for learning chemical or physical properties. However, due to the significant variation of coordinates under E(3) transformations, they can be redundant when it comes to capturing geometric structure information.

Corresponding to geometric graphs are distance graphs, i.e., _complete_ weighted graph with Euclidean distance as edge weight. Unlike geometric graphs, distance graphs do not have explicit coordinates attached to each node, but instead, they possess distance features that are naturally _invariant_ under E(3) transformation and can be _readily utilized_ by most GNNs originally designed for traditional GRL. Distance also provides an inherent _inductive bias_ for effectively modeling the interaction/relationship between nodes. Moreover, a distance graph maintains all the essential _geometric structure information_, as stated in the following theorem:

**Theorem 3.1**.: _(_Satorras et al._,_ 2021_)_ _Two geometric graphs are congruent (i.e., they are equivalent by permutation of nodes and E(3) transformation of coordinates) \(\) their corresponding distance graphs are isomorphic._

By additionally incorporating _orientation_ information, we can learn equivariant features as well (Villar et al., 2021). Hence, distance graphs provide a way to **represent geometric graphs without referring to a canonical coordinate system**. This urges us to study their full potential for GDL.

Weisfeiler-Leman Algorithms.Weisfeiler-Lehman test (also called as 1-WL) (Weisfeiler and Leman, 1968) is a well-known efficient algorithm for graph isomorphism test (traditional setting, no 3D information). It iteratively updates the labels of nodes according to nodes' own labels and their neighbors' labels, and compares the histograms of the labels to distinguish two graphs. Specifically, we use \(l_{i}^{t}\) to denote the label of node \(i\) at iteration \(t\), then 1-WL updates the node label by

\[l_{i}^{t+1}=(l_{i}^{t},\{\!\!\{l_{j}^{t} j N(i)\}\!\!\}),\] (1)

where \(\) is an injective function that maps different inputs to different labels. However, 1-WL cannot distinguish all the graphs (Zhang and Li, 2021), and thus \(k\)-dimensional WL (\(k\)-WL) and \(k\)-dimensional Folklore WL (\(k\)-FWL), \(k 2\), are proposed to boost the expressiveness of WL test.

Instead of updating the label of nodes, \(k\)-WL and \(k\)-FWL update the label of \(k\)-tuples \(:=(v_{1},v_{2},...,v_{k}) V^{k}\), denoted by \(l_{}\). Both methods initialize \(k\)-tuples' labels according to their isomorphic types and update the labels iteratively according to their \(j\)-neighbors \(N_{j}()\). The difference between \(k\)-WL and \(k\)-FWL mainly lies in the definition of \(N_{j}()\). To be specific, tuple \(\)'s \(j\)-neighbors in \(k\)-WL and \(k\)-FWL are defined as follows respectively

\[N_{j}()=\{\!\!\{v_{1},...,v_{j-1},w,v_{j+1},...,v_{k}\} w  V\!\!\},\] (2) \[N_{j}^{F}()=(j,v_{2},...,v_{k}),(v_{1},j,...,v_{k} ),...,(v_{1},v_{2},...,j).\] (3)

To update the label of each tuple, \(k\)-WL and \(k\)-FWL iterates as follows

\[k:\;l_{}^{t+1}=l_{}^{t},\{\!\!\{l_ {}^{t} N_{j}()\}\!\!\} j[k],\] (4)

\[k:\;l_{}^{t+1}=l_{}^{t},\{\!\!\{l_ {}^{t} N_{j}^{F}()\} j[|V|]\!\!\}.\] (5)

According to Cai et al. (1992), there always exists a pair of non-isomorphic graphs that cannot be distinguished by \(k\)-(F)WL but can be distinguished by \((k+1)\)-(F)WL. This means that \(k\)-(F)WL forms a strict hierarchy, which, however, still cannot solve the graph isomorphism problem with a finite \(k\).

## 4 Revisiting the Incompleteness of Vanilla DisGNN

As mentioned in previous sections, Vanilla DisGNN is the edge-enhanced version of MPNN, which can unify many model frameworks (Schutt et al., 2018; Kearnes et al., 2016). Its message passing formula can be generalized from its discrete version, which we call 1-WL-E (Pozdnyakov and Ceriotti, 2022):

\[l_{i}^{t+1}=(l_{i}^{t},\{\!\!\{l_{j}^{t},e_{ij}\} j|V|\!\! \}).\] (6)

To provide valid counterexamples that Vanilla DisGNN cannot distinguish, Pozdnyakov and Ceriotti (2022) proposed both finite-size and periodic counterexamples and showed that they included real chemical structures. This work demonstrated for the first time the inherent limitations of Vanilla DisGNN. However, it should be noted that the essential change between these counterexamples is merely the distortion of \(C^{}\) atoms, which lacks diversity despite having high manifold dimensions. In this regard, constructing new families of counterexamples cannot only enrich the diversity of existing families, but also demonstrate Vanilla DisGNN's limitations from different angles.

In this paper, we give a **simple** valid counterexample which Vanilla DisGNN cannot distinguish even with an infinite cutoff, as shown in Figure 1. In both geometric graphs, all nodes have exactly the same unordered list of distances (infinite cutoff considered), which means that Vanilla DisGNN will always label them identically. Nevertheless, the two geometric graphs are obviously non-congruent, since there are two small equilateral triangles on the right, and zero on the left. Beyond this, we construct three kinds of counterexamples in Appendix A, which can significantly enrich the counterexamples found by Pozdnyakov and Ceriotti (2022), including:

Figure 1: A pair of geometric graphs that are non-congruent but cannot be distinguished by Vanilla DisGNN. Only red nodes belong to the two geometric graphs. The grey nodes and the “edges” are for visualization purpose only. Note that the nodes of the geometric graphs are sampled from regular icosahedrons.

1. Individual counterexamples sampled from regular polyhedrons that can be directly verified.
2. Small families of counterexamples that can be transformed in one dimension.
3. Families of counterexamples constructed by _arbitrary combinations_ of basic symmetric units.

These counterexamples further demonstrate the incompleteness of Vanilla DisGNN in learning the geometry: even quite simple geometric graphs as shown in Figure 1 cannot be distinguished by it. While the counterexamples we constructed may not correspond to real molecules, such symmetric structures are commonly seen in other geometry-relevant tasks, such as physical simulations and point clouds. The inability to distinguish these symmetric structures can have a significant impact on the geometric learning performance of models, even for non-degenerated configurations, as demonstrated by Pozdnyakov and Ceriotti (2022). This perspective can provide an additional explanation for the poor performance of models such as SchNet (Schutt et al., 2018) and inspire the design of more powerful and efficient geometric models.

## 5 \(k\)-DisGNNs: \(k\)-order Distance Graph Neural Networks

In this section, we propose the framework of \(k\)-DisGNNs, _complete and universal_ geometric models learning from _pure distance features_. \(k\)-DisGNNs consist of three versions: \(k\)-DisGNN, \(k\)-F-DisGNN, and \(k\)-E-DisGNN. Detailed implementation is referred to Appendix C.

**Initialization Block.** Given a geometric \(k\)-tuple \(\), high-order DisGNNs initialize it with an injective function by \(h_{}^{0}=f_{}z_{v_{i}} i[k], e_{v_{i}v_{j}} i,j[k],i<j^{K}\), where \(K\) is the hidden dimension. This function can injectively embed the _ordered_ distance matrix along with the \(z\) type of each node within the \(k\)-tuple, thus preserving all the geometric information within it.

**Message Passing Block.** The key difference between the three versions of high-order DisGNNs lies in their message passing blocks. The message passing blocks of \(k\)-DisGNN and \(k\)-F-DisGNN are based on the paradigms of \(k\)-WL and \(k\)-FWL, respectively. Their core message passing function \(f_{}^{t}\) and \(f_{}^{,t}\) are formulated simply by replacing the _discrete_ tuple labels \(l_{}^{t}\) in \(k\)-(F)WL (Equation (4, 5)) with _continuous_ geometric tuple representation \(h_{}^{t}^{K}\), see Equation (7, 8). Tuples containing local geometric information interact with each other in message passing blocks, thus allowing models to learn considerable global geometric information (as explained in Section 6.1).

\[k- :h_{}^{t+1}=f_{}^{t}h_{}^{t}, h_{}^{t} N_{j}()} j|k| ,\] (7) \[k-- :h_{}^{t+1}=f_{}^{,t}h_{ }^{t},h_{}^{t} N_{j}^{F}()} j [|V|}.\] (8)

However, during message passing in \(k\)-DisGNN, the information about distance is not explicitly used but is embedded implicitly in the initialization block when each geometric tuple is embedded according to its distance matrix and node types. This means that \(k\)-DisGNN is **unable to capture the relationship** between a \(k\)-tuple \(\) and its neighbor \(\) during message passing, which could be very helpful for learning the geometric structural information of the graph. For example, in a physical system, a tuple \(\) far from \(\) should have much less influence on \(\) than another tuple \(^{}\) near \(\).

Based on this observation, we propose \(k\)**-E-DisGNN**, which maintains a representation \(e_{ij}^{t}\) for each edge \(e_{ij}\) at every time step and explicitly incorporates it into the message passing procedure. The edge representation \(e_{ij}^{t}\) is defined as

\[e_{ij}^{t}=f_{}^{t}e_{ij},h_{}^{t}  V^{k},_{u}=i,_{v}=j} u,v[k],u<v .\] (9)

Note that \(e_{ij}^{t}\) not only contains the distance between nodes \(i\) and \(j\), but also pools all the tuples related to edge \(ij\), making it informative and general. For example, in the special case \(k=2\), Equation (9) is equivalent to \(e_{ij}^{t}=f_{}^{t}(e_{ij},h_{ij}^{t})\).

We realize the message passing function of \(k\)-E-DisGNN, \(f_{}^{,t}\), by replacing the neighbor representation, \(h_{}^{t}\) in \(f_{}^{t}\), with \(h_{}^{t},e_{,}^{t}\) where \(\) gives the only element in \(\) but not in \(\), see Equation 10. In other words, \(e_{,}^{t}\) gives the representation of the edge connecting \(,\). By this means, a tuple can be aware of **how far** it is to its neighbors and by what kind of edges each neighbor is connected. This can boost the ability of geometric structure learning (as explained in Section 6.1).

\[k--:h_{}^{t+1}=f_{}^{,t} h_{}^{t},\!\!}(h_{}^{t},e_{ ,}^{t}) N_{j}()} \ j|k|.\] (10)

**Output Block.** The output function \(t=f_{}\!}\!}h_{}^{T}  V^{k}}\), where \(T\) is the final iteration, injectively pools all the tuple representations, and generates the E(3) and permutation invariant geometric target \(t\).

Like the conclusion about \(k\)-(F)WL for unweighted graphs, the expressiveness (in terms of approximating functions) of DisGNNs does _not decrease_ as \(k\) increases. This is simply because all \(k\)-tuples are contained within some \((k+1)\)-tuples, and by designing message passing that ignores the last index, we can implement \(k\)-DisGNNs with \((k+1)\)-DisGNNs.

## 6 Rich Geometric Information Learned by \(k\)-DisGNNs

In this section, we aim to delve deeper into the geometry learning capability of \(k\)-DisGNNs from various perspectives. In Subsection 6.1, we will examine the **high-order geometric features** that the models can extract from geometric graphs. This analysis is more intuitive and aids in comprehending the high geometric expressiveness of \(k\)-DisGNNs. In Subsection 6.2, we will show that DimeNet and GemNet, two classical and widely-used GNNs for GDL employing invariant geometric representations, are **special cases** of \(k\)-DisGNNs, highlighting the generality of \(k\)-DisGNNs. In the final subsection, we will show the **completeness** (distinguishing geometric graphs) and **universality** (approximating functions over geometric graphs) of \(k\)-DisGNNs, thus answering the question posed in the title: distance matrix is enough for GDL.

### Ability of Learning High-Order Geometry

We first give the concept of _high-order geometric information_ for better understanding.

**Definition 6.1**.: \(k\)-order geometric information is the _E(3)-invariant_ features calculable from \(k\) nodes' 3D coordinates.

For example, in a 4-tuple, one can find various high-order geometric information, including distance (2-order), angles (3-order) and dihedral angles (4-order), as shown in Figure 2(A1). Note that high-order information is not limited to the common features listed above: we show some of other possible 3-order geometric features in Figure 2(A2).

We first note that \(k\)-DisGNNs can learn and process at least \(k\)-order geometric information. Theorem 3.1 states that we can reconstruct the whole geometry of the \(k\)-tuple from the embedding of its distance matrix. In other words, we can extract all the desired \(k\)-order geometric information solely from the embedding of the \(k\)-tuple's distance matrix, which is calculated at the initialization step of \(k\)-DisGNNs, with a learnable function.

Figure 2: **A**: High-order geometric information contained in the distance matrix of \(k\)-tuples. We mark different orders with different colors, with brown, green, blue for 2-,3-,4-order respectively. (A1) High-order geometric information contained in 4-tuples, including distances, angles and dihedral angles. (A2) More 3-order geometric features, such as vertical lines, middle lines and the area of triangles. **B**: Examples explaining that neighboring 3-tuples can form a 4-tuple. Blue represents the center tuple, the other colors represent neighbor tuples, and the red node is the one node of that neighbor tuple which is not in the center tuple. (B1) Example for 3-E-DisGNN. With the green edge, two 3-tuples can form a 4-tuple. (B2) Example for 3-F-DisGNN. Four 3-tuples form a 4-tuple.

Furthermore, both \(k\)-E-DisGNN and \(k\)-F-DisGNN can actually **learn \((k+1)\)-order geometric information** in their message passing layers. For \(k\)-E-DisGNN, information about \((h_{},h_{},e_{,})\), where \(\) is some \(j\)-neighbor of \(\), is included in the input of its update function. Since the distance matrices of tuples \(\) and \(\) can be reconstructed from \(h_{}\) and \(h_{}\), the all-pair distances of \((_{1},_{2},...,_{k},)\) can be reconstructed from \((h_{},h_{},e_{,})\), as shown in Figure 2(B1). Similarly, the distance matrix of \((_{1},_{2},...,_{k},j)\) can also be reconstructed from \(h_{},(h_{}^{t} N_{j}^{F}())\) in update function of \(k\)-F-DisGNN as shown in Figure 2(B2). These enable \(k\)-E-DisGNN and \(k\)-F-DisGNN to reconstruct all \((k+1)\)-tuples' distance matrices during message passing and thus learn all the \((k+1)\)-order geometric information contained in the graph.

With the ability to learn high-order geometric information, \(k\)-DisGNNs can learn geometric structures that cannot be captured by Vanilla DisGNNs. For example, consider the counterexample shown in Figure 1. The two geometric graphs have a different number of small equilateral triangles, which cannot be distinguished by Vanilla DisGNNs even with infinite message passing layers. However, \(3\)-DisGNN and \(2\)-E/F-DisGNN can easily distinguish the graphs by counting the number of small equilateral triangles, which is actually a kind of 3-order geometric information. This example also illustrates that high-order DisGNNs are _strictly more powerful_ than Vanilla DisGNNs.

### Unifying Existing Geometric Models with DisGNNs

There have been many attempts to improve GDL models by _manually_ designing and incorporating high-order geometric features such as angles (3-order) and dihedral angles (4-order). These features are all invariant geometric features that can be learned by some \(k\)-DisGNNs. It is therefore natural to ask whether these models can be implemented by \(k\)-DisGNNs. In this subsection, we show that two classical models, DimeNet  and GemNet , are just special cases of \(k\)-DisGNNs, thus unifying existing methods based on _hand-crafted_ features with a learning paradigm that learns _arbitrary_ high-order features from distance matrix.

DimeNet embeds atom \(i\) with a set of incoming messages \(m_{ji}\), i.e., \(h_{i}=_{j N_{i}}m_{ji}\), and updates the message \(m_{ji}\) by

\[m_{ji}^{t+1}=f_{}^{}m_{ji}^{t},_{k}f_{}^{}(m_{kj}^{t},d_{ji},_{kji}),\] (11)

where \(d_{ji}\) is the distance between node \(j\) and node \(i\), and \(_{kji}\) is the angle \(kji\). We simplify the subscript of \(\), same for that in Equation (12). The detailed ones are referred to Appendix B.2, B.1.

DimeNet is one early approach that uses geometric features to improve the geometry learning ability of GNNs, especially useful for learning the _angle-relevant_ energy or other chemical properties. Note that the angle information that DimeNet explicitly incorporates into its message passing function is actually a kind of 3-order geometric information, which can be exploited from distance matrices by our 2-E/F-DisGNN with a learning paradigm. This gives the key insight for the following proposition.

**Proposition 6.2**.: _2-E-DisGNN and 2-F-DisGNN can implement DimeNet._

GemNet is also a graph neural network designed to process graphs embedded in Euclidean space. Unlike DimeNet, GemNet incorporates both angle and _dihedral angle_ information into the message passing functions, with a 2-step message passing scheme. The core procedure is as follows:

\[m_{ca}^{t+1}=f_{}^{}m_{ca}^{t},_{b,d}f_{ }^{}(m_{db}^{t},d_{db},_{cab},_{abd},_{ cabd}),\] (12)

where \(_{cabd}\) is the dihedral angle of planes \(cab\) and \(abd\). The use of dihedral angle information allows GemNet to learn at least 4-order geometric information, making it much more powerful than models that only consider angle information. We now prove that GemNet is just a special case of 3-E/F-DisGNN, which can also learn 4-order geometric information during its message passing stage.

**Proposition 6.3**.: _3-E-DisGNN and 3-F-DisGNN can implement GemNet._

It is worth noting that while both \(k\)-DisGNNs and existing models can learn some \(k\)-order geometric information, \(k\)-DisGNNs have the advantage of **learning arbitrary \(k\)-order geometric information**. This means that we can learn different high-order geometric features according to specific tasks in a _data-driven_ manner, including but not limited to angles and dihedral angles.

### Completeness and Universality of \(k\)-DisGNNs

We have provided an illustrative example in Section 6.1 that demonstrates the ability of \(k\)-DisGNNs to distinguish certain geometric graphs, which is not achievable by Vanilla DisGNNs. Actually, all counterexamples presented in Appendix A can be distinguished by 3-DisGNNs or 2-E/F-DisGNNs. This leads to a natural question: Can \(k\)-DisGNNs distinguish all geometric graphs with a finite and small value of \(k\)? Furthermore, can they approximate arbitrary functions over geometric graphs?

We now present our main findings that elaborate on the **completeness** and **universality** of \(k\)-DisGNNs with finite and small \(k\), which essentially highlight the high theoretical expressiveness of \(k\)-DisGNNs.

**Theorem 6.4**.: _(informal) Let \(()=\{\)1-round 4-DisGNN, 2-round 3-E/F-DisGNN\(\}\) with parameters \(\). Denote \(h_{m}=f_{}\!\{h_{n}^{T}_{0}=m\}\!} ^{K^{}}\) as node \(m\)'s representation produced by \(\) where \(f_{}\) is an injective multiset function, and \(_{m}^{c}\) as node \(m\)'s coordinates w.r.t. the center. Denote \(:^{K^{}}\) as a multi-layer perceptron. Then we have:_

1. _(Completeness) There exists_ \(_{0}\) _such that_ \((_{0})\) _can distinguish all pairs of non-congruent geometric graphs._
2. _(Universality for scalars)_ \(()\) _is a universal function approximator for continuous,_ \(E(3)\) _and permutation invariant functions_ \(f:^{3 n}\) _over geometric graphs._
3. _(University for vectors)_ \(f_{}^{}=_{m=1}^{|V|}h_{m} _{m}^{c}\) _is a universal function approximator for continuous,_ \(O(3)\) _equivariant and translation and permutation invariant functions_ \(f:^{3 n}^{3}\) _over geometric graphs._

In fact, completeness directly implies universality for scalars . Since we can actually recover the whole geometry from arbitrary node representation \(h_{m}\), universality for vectors can be proved with conclusions from Villar et al. . Also note that as order \(k\) and round number go higher, the completeness and universality still hold. Formal theorems and detailed proof are provided in Appendix B.3.

There is a concurrent work by Delle Rose et al. , which also investigates the completeness of \(k\)-WL on distance graphs. In contrast to our approach of using only one tuple's final representation to reconstruct the geometry, they leverage all tuples' final representations. They prove that with the distance matrix, 3-round geometric 2-FWL and 1-round geometric 3-FWL are complete for geometry. This conclusion can also be extended to \(k\)-DisGNNs, leading to the following theorem:

**Theorem 6.5**.: _The completeness and universality for scalars stated in Theorem 6.4 hold for \(()_{}=\{\)3-round 2-E/F-DisGNN, 1-round 3-E/F-DisGNN\(\}\) with parameters \(\)._

This essentially lowers the required order \(k\) for achieving universality on scalars to 2. However, due to the lower order and fewer rounds, the expressiveness of a single node's representation may be limited, thus the universality for vectors stated in Theorems 6.4 may not be guaranteed.

## 7 Experiments

In this section, we evaluate the experimental performance of \(k\)-DisGNNs. Our main objectives are to answer the following questions:

**Q1** Does 2/3-E/F-DisGNN outperform their counterparts in experiments (corresponding to Section 6.2)?

**Q2** As universal models for scalars (when \(k 2\)), do \(k\)-DisGNNs also have good experimental performance (corresponding to Section 6.3)?

**Q3** Does incorporating well-designed edge representations in the \(k\)-E-DisGNN result in improved performance?

The best and the second best results are shown in **bold** and underline respectively in tables. Detailed experiment configuration and supplementary experiment information can be found in Appendix D. Our code is available at https://github.com/GraphPKU/DisGNN.

**MD17**.: MD17  is a dataset commonly used to evaluate the performance of machine learning models in the field of molecular dynamics. It contains a collection of moleculardynamics simulations of small organic molecules such as aspirin and ethanol. Given the atomic numbers and coordinates, the task is to predict the energy of the molecule and the atomic forces. We mainly focus on the comparison between 2-F-DisGNN/3-E-DisGNN and DimeNet/GemNet. At the same time, we also compare \(k\)-DisGNNs with the state-of-the-art models: FCHL [Christensen et al., 2020], PaiNN [Schutt et al., 2021], NequIP [Batzner et al., 2022], TorchMD [Tholke and De Fabritiis, 2021], GNN-LF [Wang and Zhang, 2022]. The results are shown in Table 1. 2-F-DisGNN and 3-E-DisGNN outperform their counterparts on **16/16** and 6/8 targets, respectively, with an average improvement of 43.9% and 12.23%, suggesting that data-driven models can subsume carefully designed manual features given high expressiveness. In addition, \(k\)-DisGNNs also achieve the **best performance** on 8/16 targets, and achieve the second-best performance on **all the other targets**, outperforming the best baselines GNN-LF and TorchMD by 10.19% and 12.32%, respectively. Note that GNN-LF and TorchMD are all complex equivariant models. Beyond these, we perform experiments on **revised MD17**, which has better data quality, and \(k\)-DisGNNs outperforms the SOTA models [Batatia et al., 2022, Musaelian et al., 2023] on a wide range of targets. The results are referred to Appendix D.2. The results firmly answer **Q1** and **Q2** posed at the beginning of this section and demonstrate the potential of pure distance-based methods for graph deep learning.

**QM9.** QM9 [Ramakrishnan et al., 2014, Wu et al., 2018] consists of 134k stable small organic molecules with 19 regression targets. The task is like that in MD17, but this time we want to predict the molecule's properties such as the dipole moment. We mainly compare 2-F-DisGNN with DimeNet on this dataset and the results are shown in Table 2. 2-F-DisGNN outperforms DimeNet on **11/12** targets, by 14.27% on average, especially on targets \(\) (65.0%) and \( R^{2}\) (90.4%), which also answers **Q1** well. A full comparison to other state-of-the-art models is included in Appendix D.2.

**Effectiveness of edge representations.** To answer **Q3**, we split the edge representation \(e_{ij}^{t}\) (Equation (9)) into two parts, namely the pure distance (the first element) and the tuple representations (the second element), and explore whether incorporating the two elements is beneficial. We conduct experiments on MD17 with three versions of 2-DisGNNs, including 2-DisGNN (no edge representation), 2-e-DisGNN (add only edge weight) and 2-E-DisGNN (add full edge representation). The results are shown in Table 3. Both 2-e-DisGNN and 2-E-DisGNN exhibit significant performance improvements over 2-DisGNN, highlighting the significance of edge representation. Furthermore, 2-E-DisGNN outperforms 2-DisGNN and 2-e-DisGNN on 16/16 and 12/16 targets, respectively, with average improvements of 39.8% and 3.8%. This verifies our theory that capturing the _full edge representation_ connecting two tuples can boost the model's representation power.

  Target & & FCHL & PaiNN & NequIP & TorchMD & GNN-LF & DimeNet & 2F-Dis. & GemNet & 3E-Dis. \\   & E & 0.182 & 0.167 & - & **0.124** & 0.1342 & 0.204 & 0.1305 & - & 0.1466 \\  & F & 0.478 & 0.338 & 0.348 & 0.255 & 0.2018 & 0.499 & **0.1393** & 0.2168 & 0.2060 \\  & E & - & - & - & **0.056** & 0.0686 & 0.078 & 0.0683 & - & 0.0795 \\  & F & - & - & 0.187 & 0.201 & 0.1506 & 0.187 & 0.1474 & **0.1453** & 0.1471 \\  & E & 0.054 & 0.064 & - & 0.054 & 0.0520 & 0.064 & **0.0502** & - & 0.0541 \\  & F & 0.136 & 0.224 & 0.208 & 0.116 & 0.0814 & 0.230 & **0.0478** & 0.0853 & 0.0617 \\  & E & 0.081 & 0.091 & - & 0.079 & 0.0764 & 0.104 & **0.0730** & - & 0.0739 \\  & F & 0.245 & 0.319 & 0.337 & 0.176 & 0.1259 & 0.383 & **0.0786** & 0.1545 & 0.0974 \\  & E & 0.117 & 0.166 & - & **0.085** & 0.1136 & 0.122 & 0.1146 & - & 0.1135 \\  & F & 0.151 & 0.077 & 0.097 & 0.06 & 0.0550 & 0.215 & 0.0518 & 0.0553 & **0.0478** \\  & E & 0.114 & 0.166 & - & **0.094** & 0.1081 & 0.134 & 0.1071 & - & 0.1084 \\  & F & 0.221 & 0.195 & 0.238 & 0.135 & 0.1005 & 0.374 & **0.0862** & 0.1048 & 0.1186 \\  & E & 0.098 & 0.095 & - & **0.074** & 0.0930 & 0.102 & 0.0922 & - & 0.1004 \\  & F & 0.203 & 0.094 & 0.101 & 0.066 & 0.0543 & 0.216 & **0.395** & 0.0600 & 0.0455 \\  & E & 0.104 & 0.106 & - & **0.096** & 0.1037 & 0.115 & 0.1036 & - & 0.1037 \\  & F & 0.105 & 0.139 & 0.173 & 0.094 & **0.0751** & 0.301 & 0.0876 & 0.0969 & 0.0921 \\   &  & 11.58\% & -2.09\% & - & **26.40\%** & 17.05\% & 0.00\% & 18.18\% & - & 13.515\% \\  & & 5 & 7 & - & **1** & 3 & 6 & 2 & - & 4 \\   &  & 31.85\% & 39.13\% & 29.86\% & 52.40\% & 63.53\% & 0.00\% & **69.68\%** & 60.96\% & 65.27\% \\  & & 7 & 6 & 8 & 5 & 3 & 9 & **1** & 4 & 2 \\   &  & 31.85\% & 39.13\% & 29.86\% & 52.40\% & 63.53\% & 0.00\% & **69.68\%** & 60.96\% & 65.27\% \\  & & 7 & 6 & 8 & 5 & 3 & 9 & **1** & 4 & 2 \\  

Table 1: MAE loss on MD17. Energy (E) in kcal/mol, force (F) in kcal/mol/Å. We color the cell if DisGNNs outperforms its counterpart. The improvement ratio is calculated relative to DimeNet. \(k\)-DisGNNs rank **top 2** on force prediction (which determines the accuracy of molecular dynamics [Gasteiger et al., 2021]) on average.

## 8 Conclusions and Limitations

**Conclusions.** In this work we have thoroughly studied the ability of GNNs to learn the geometry of a graph solely from its distance matrix. We expand on the families of counterexamples that Vanilla DisGNNs are unable to distinguish from their distance matrices by constructing families of symmetric and diverse geometric graphs, revealing the inherent limitation of Vanilla DisGNNs in capturing symmetric configurations. To better leverage the geometric structure information contained in distance graphs, we proposed \(k\)-DisGNNs, geometric models with high generality (ability to unify two classical and widely-used models, DimeNet and GemNet), provable completeness (ability to distinguish all pairs of non-congruent geometric graphs) and universality (ability to universally approximate scalar functions when \(k 2\) and vector functions when \(k 3\)). In experiments, \(k\)-DisGNNs outperformed previous state-of-the-art models on a wide range of targets of the MD17 dataset. Our work reveals the potential of using expressive GNN models, which were originally designed for traditional GRL, for the GDL tasks, and opens up new opportunities for this domain.

**Limitations.** Although DisGNNs can achieve universality (for scalars) with a small order of 2, the results are still generated under the assumption that the distance graph is complete. In practice, for large geometric graphs such as proteins, this assumption may lead to intractable computation due to the \(O(n^{3})\) time complexity and \(O(n^{2})\) space complexity. However, we can still balance theoretical universality and experimental tractability by using either an appropriate cutoff, which can furthest preserve the completeness of the distance matrix in local clusters while cutting long dependencies to improve efficiency (and generalization), or using sparse but expressive GNNs such as subgraph GNNs (Zhang and Li, 2021). We leave it for future work.