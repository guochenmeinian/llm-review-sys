# OPEL: Optimal Transport Guided Procedure Learning

Sayeed Shafayet Chowdhury, Soumyadeep Chandra, and Kaushik Roy

Elmore Family School of Electrical and Computer Engineering

Purdue University, West Lafayette, IN 47907, USA

{chowdh23, chand133, kaushik}@purdue.edu

###### Abstract

Procedure learning refers to the task of identifying the key-steps and determining their logical order, given several videos of the same task. For both third-person and first-person (egocentric) videos, state-of-the-art (SOTA) methods aim at finding correspondences across videos in time to accomplish procedure learning. However, to establish temporal relationships within the sequences, these methods often rely on frame-to-frame mapping, or assume monotonic alignment of video pairs, leading to sub-optimal results. To this end, we propose to treat the video frames as samples from an unknown distribution, enabling us to frame their distance calculation as an optimal transport (OT) problem. Notably, the OT-based formulation allows us to relax the previously mentioned assumptions. To further improve performance, we enhance the OT formulation by introducing two regularization terms. The first, inverse difference moment regularization, promotes transportation between instances that are homogeneous in the embedding space as well as being temporally closer. The second, regularization based on the KL-divergence with an exponentially decaying prior smooths the alignment while enforcing conformity to the optimality (alignment obtained from vanilla OT optimization) and temporal priors. The resultant optimal transport guided procedure learning framework ('OPEL') significantly outperforms the SOTA on benchmark datasets. Specifically, we achieve 22.4% (IoU) and 26.9% (F1) average improvement compared to the current SOTA on large scale egocentric benchmark, EgoProceL. Furthermore, for the third person benchmarks (ProCeL and CrossTask), the proposed approach obtains 46.2% (F1) average enhancement over SOTA.

## 1 Introduction

The development of autonomous agents capable of reliably replicating human actions to accomplish certain end goals presents significant challenges. Traditional approaches would necessitate hard-coding tedious explicit instructions for each sub-task of the process (thus difficult to scale and generalize). A more efficient solution would involve the agent learning directly from observing multiple demonstrations of the assembly, without the need of any label. This motivates us to explore unsupervised procedure learning from videos. In the context of this work, procedure learning (PL) is conceptualized as the process of determining the key-steps and their correct sequential order to accomplish an overall task, as demonstrated across multiple video demonstrations .

PL analyzes multiple videos of a task as illustrated in Figure 1, in contrast to action-based tasks , which focus on a single video. The single-video approach is inapplicable to the identification of repetitive key-steps across videos. Moreover, action-based tasks typically neglect the sequencing of events, crucial for discerning an overall expected procedure composed of the sub-tasks. For instance, they fail to capture variations in the sequence of key-steps between videos V1 and V2 (Fig. 1). Other research efforts in video understanding that employ instructional videos include procedure planning , verifying sequences of procedures , and summarizing instructional content . Additionally, unlike video alignment tasks , PL specifically aims to localize these essential steps within videos.

Much of the research on PL till now has been performed within the frameworks of supervised [9; 10; 11] and weakly supervised learning [12; 13; 14]. In a supervised setting, the reliance on per-frame annotations demands extensive manual labor. Conversely, weakly supervised methods involve using either ordered or unordered lists of key-steps. The generation of these lists requires either direct observation of the videos or specific heuristics, both of which pose significant scalability challenges . Consequently, recent studies [1; 8] have shifted focus towards self-supervised learning, which do not require frame-wise labeling. Such a learning paradigm leverages the structured nature of accomplishing a complex task, which typically unfold in a predictable sequence of steps. For example, the act of preparing a "brownie" might involve breaking an egg, adding water, oil, mixing the contents, and then baking in the oven. The alignment of video frames is commonly performed in a monotonic manner , which presupposes a consistent order of actions across sequences. However, real-world sequences frequently deviate from this pattern, exhibiting temporal non-uniformities as depicted in Figure 1. These deviations can be categorized as follows: (i) background frames: frames irrelevant to the primary activity and should thus be excluded from alignment; (ii) redundant frames: these frames appear only in one sequence but not in others and do not contribute to the task; (iii) non-monotonic frames: these frames are characterized by a non-monotonic sequence of actions. Such frames challenge the assumption of monotonic progression and highlight the complexity of real-world activities. State-of-the-art (SOTA) methods adopt custom approaches to counter these irregularities such as removing background frames from processing , using extra information (e.g. gaze, depth) , or simply ignore them leading to suboptimal results .

To address the limitations of previous approaches, we relax the strict assumptions about the temporal sequence of actions and introduce a novel PL framework designed to learn temporal correspondences across videos. By treating instances of the sequences as samples from an unknown distribution, we formulate the task of computing the distance between them as an optimal transport (OT)  problem. The differentiable OT loss facilitates the alignment of non-monotonic sequences through frame-wise matching based on individual frame features. However, it typically overlooks the temporal smoothness and the inherent ordering relationships within the videos. To overcome this deficiency, we integrate two priors into the transportation matrix. First, the optimality prior favors the positions dictated by the OT, whereas the temporal prior discourages transport between temporally distant frames. Both these priors are modeled using a Laplace distribution with exponentially decaying probability from the corresponding centers. We introduce an additional virtual frame into the OT matrix to address background and redundant frames. Furthermore, to avoid the common issue of converging to trivial solutions in temporal video alignment , we employ a novel inter-video contrastive loss, which acts as a regularizer. Finally, the sub-tasks of each video are clustered in the embedding space using graphcut segmentation . The overall framework, termed optimal transport guided procedure learning ('OPEL'), achieves SOTA results on both the ego and exocentric benchmarks. To summarize, our main contributions are-

* We propose a novel optimal transport based procedure learning framework that aligns frames with similar semantics together in an embedding space.

Figure 1: Key-steps required to prepare a brownie . The sequences showcase temporal variations and corresponding key-step alignment challenges, namely (i) background frames (depicted as gray blocks), (ii) non-monotonic frames. OPEL aims to learn an embedding space where corresponding key-steps have similar embeddings while tackling the above challenges.

* To enhance the OT-based learning, we integrate optimality and temporal priors, both modeled using the Laplace distribution. These two priors also serve as regularizers. Furthermore, OPEL incorporates a novel inter-video contrastive loss for additional improvement.
* OPEL demonstrates substantial performance gains, achieving an average improvement of 22.4% in IoU and 26.9% in F1-score compared to the current SOTA on the EgoProceL benchmark.

## 2 Related Works

**Representation Learning for Videos.** Recent studies have explored various pretext tasks to facilitate representation learning through self-supervised or unsupervised approaches. Examples include temporal coherence and sequence ordering [20; 21; 22; 23; 24], predicting frames [25; 26; 27; 28; 29], and determining the directionality of time . These methods typically derive signals from a constrained set of videos. In contrast, our objective is to discern and characterize key-steps of a certain task across multiple videos, expanding the scope and applicability of representation learning.

**Self-Supervised Representations for Procedure Learning.** Previous studies on PL have focused on developing methods for learning frame-level features [31; 3; 32; 33]. For instance, Kukleva et al.  enhance the representation space by utilizing relative timestamps of frames, while Vidal et al.  engage in predicting future frames along with their timestamps. Elhamifar et al.  apply attention mechanisms to individual frames to enhance feature learning. Similarly, Bansal et al.  leverage temporal correspondences across videos to generate signals and learn frame-level embeddings. The current SOTA model for egocentric PL  utilizes task-level graph representation to cluster semantically similar and temporally close frames. Despite these advancements, these methods often exhibit limitations in adequately modeling either temporal or spatial relationships within video sequences, especially in the presence of background and redundant frames. As a result, extra curated processing steps are required, resulting in additional layer of complexity and computation e.g.  depends on background frame removal to improve performance.

**Multi-modal Procedure Learning.** PL has also been used with multi-modal data such as (a) narrated text and videos [34; 35; 36; 37; 38; 39], (b) optical flow, depth and gaze information . These studies typically rely on the assumption of a reliable alignment between video content and corresponding supporting modalities [34; 38; 39], an assumption that often proves inaccurate [31; 3] due to lack of synchrony among the modalities. Additionally, the dependence on imperfect Automatic Speech Recognition (ASR) systems necessitates subsequent manual corrections. Moreover, multiple modalities require additional memory and compute. In contrast, our framework exclusively leverages visual data, thereby circumventing the inaccuracies associated with multimodal alignment and enhancing scalability by eliminating the need for extra data modalities.

**Video Alignment.** can be efficiently addressed in synchronized settings using established methods like Canonical Correlation Analysis (CCA)  and soft-Dynamic Time Warping (DTW) . A recent work  aligns videos by learning self-supervised representations from multiple viewpoints. However, the requirement for synchronized multi-view recordings limits its applicability. To tackle this challenge,  proposes a cycle consistency loss to establish frame correspondences, focusing primarily on local matches and not on the global temporal structure of the videos. Perhaps, works leveraging OT for visual analysis [43; 44; 45] are most related to our approach. But, such approaches do not address sequence alignment as we do. An exception is , which employs OT for videos, however their setup for evaluation is supervised fine-tuning for action segmentation, a fundamentally different task than unsupervised PL. As a result,  does not deal with temporal localization of the key-steps of a task nor their ordering, unlike us. Moreover, our modeling of priors using Laplace distribution and inter-video contrastive loss formulation are different from .

**Learning Key-step Ordering.** Most existing studies fail to account for variations in the ordering of key-steps required to complete a task, often assuming either a strict sequential order [31; 32; 33] or neglecting to model the sequence altogether [3; 47]. However, as illustrated in Figure 1, individuals frequently execute the same task in diverse manners, underscoring the need for a more flexible approach. To that end, OPEL is designed to identify and construct a unique key-step sequence for each video, thereby adapting to and inferring the specific ordering of the task.

## 3 OPEL Framework

**Optimal Transport Formulation.** OT provides a metric for assessing the dissimilarity between two probability distributions within a metric space . By using the feature vectors from each entity and a distance matrix between them, it establishes correspondences that minimize total distance, also ensuring optimality, separability, and completeness. Assume, the inputs are two sequences of video frames, \(=[_{1},_{2},,_{N}]\) and \(=[_{1},_{2},,_{M}]\). We pass these through a deep encoder network (as illustrated in Fig. 2(A)) to obtain their respective embeddings, \(=[_{1},_{2},,_{N}]\) and \(=[_{1},_{2},,_{M}]\). Let, \((,l)\) is a metric space, where \(l:\) denotes the distance in \(\), and \(P()\) represents all Borel probability measures on \(\). Considering the elements of \(\) and \(\) as independent samples, their probability measures can be written as, \(f=_{i=1}^{N}_{i}_{x_{i}}\;\;g=_{j=1}^{M}_{j} _{y_{j}},\) where \(_{x}\) denotes the Dirac mass at \(x\), and \(\) and \(\) are the weights for the distributions \(f\) and \(g\), respectively. Since there is no justification for assigning greater importance to one frame over another, initially we set \(_{i}=\) and \(_{j}=\) for all \(i\), \(j\), leading to a feasible set of weight matrices defined as the transportation polytope , \(U(,):=\{_{+}^{N M}: _{M}=,^{}_{N}=\}\). Here, \(t_{ij}\) can be interpreted to be proportional to the probability that \(_{i}\) will be aligned to \(_{j}\). We start by computing the pairwise Euclidean distances between embedding vectors, \(d(_{i},_{j})=\|_{i}-_{j}\|\) to form the \(N M\) distance matrix, \(\). The cost of transporting mass from \(f\) to \(g\) with a transport plan \(\) is quantified by the Frobenius inner product \(,\). Thus, the Wasserstein distance raised to the power \(p\) is: \(W_{p}^{p}(f,g)=l_{W}(,,)=_{ U(,)},.\) We only consider \(p=1\), and drop \(p\) henceforth. To simplify the above optimization and make training feasible, Cuturi  introduced an entropy regularization, leading to the Sinkhorn distance,

\[l_{}^{S}(,,)=_{}, \;_{}=_{ U(, )},-h(),\] (1)

where \(h()=-_{i=1}^{N}_{j=1}^{M}t_{ij} t_{ij}\) denotes the entropy of \(\), and \(\) is the regularization parameter. The optimal solution for Eqn. (1) has the form , \(_{}=(_{1})(-)( _{2})\), where \((-)\) is the element-wise exponential of the matrix \(-\), and \(_{1}^{N}\) and \(_{2}^{M}\) are the non-negative left and right scaling vectors to be obtained by the Sinkhorn fixed point iterations.

**Regularization with Priors.** The above formulation minimizes the cost of aligning two sequences, however it totally neglects the temporal ordering relationships inherent in videos, failing to leverage the temporal consistency. Typically, the alignment of multiple videos depicting the same activity should constrain the temporal position of one sequence to correspond closely with adjacent temporal positions of another sequence. Perfect alignment would render the transport matrix \(\) diagonal, but this strict requirement is impractical for real-world applications. As illustrated in Fig. 2(B), variations such as earlier commencement of activities, differing action speeds, or non-monotonic sequences complicate alignment. To address these challenges and achieve optimal alignment while accounting for temporal variations, we introduce two priors into the OT framework. Essentially, there are 2 factors in play - (i) optimality which tries to find the best match between frames irrespective of their temporal distance (which may result in temporally incoherent alignment), and (ii) the temporal factor which promotes transport between nearby frames only without considering their feature matching. We hypothesize that the optimal solution requires striking a balance between both, and thus propose to enhance the OT formulation by incorporating two specific priors addressing the above factors .

Figure 2: (A) The encoder generates frame-wise embeddings from videos, facilitating subsequent OT calculations. (B) Pair-wise scenarios captured through the assignment matrix- from strictly synchronized actions to temporal shifts and differing action speeds, to non-monotonicity. (C) 1-D depiction of alignment of a single frame (\(i\)-th) of Video 2 with its best match frame (\(j\)-th) of Video 1, based on the proposed priors. (D) 2-D representation of the optimal alignment of frame sequences.

The first prior, termed the 'Optimality Prior', is introduced to effectively manage non-monotonic sequences. This prior leverages the transport matrix \(\) as delineated in Eqn. (1), which provides a preliminary indication of alignment between two video sequences. This matrix adapts dynamically to reflect the temporal variations observed across the sequences. Our approach uses this dynamic behavior to establish the optimality prior. We want the point representing the most likely alignment according to \(\) to have the highest likelihood, while the assignment probability decays along any perpendicular direction from this center. Specifically, we model this as a Laplace distribution,

\[_{o}(i,j)=e^{-(i,j)|}{b}},d_{o}(i,j)=/N|+|j/M-j_{o}/M|}{2+1/M^{2}}},\] (2)

represents the average distance from \((i,j)\) to the frame locations \((i,j_{o})\) and \((i_{o},j)\) that correspond to the optimal alignment as indicated by the transport matrix, and \(b\) is a scale parameter. Motivated by , we incorporate a second prior, termed the 'Temporal Prior', which promotes alignment of one sequence with elements in proximal temporal positions of the other sequence, thereby preserving the overall temporal structure and maintaining consistency in action order. This prior results in an assignment matrix characterized by peak values along the diagonal, with values diminishing perpendicular to the diagonal. Again, this scenario is modeled using a two-dimensional Laplace distribution, where the distribution along any line perpendicular to the diagonal is exponentially decaying, centered along the diagonal itself:

\[_{t}(i,j)=e^{-(i,j)|}{b}},d_{t}(i,j)=+1/M^{2}}}\] (3)

is the distance from \((i,j)\) to the diagonal. Inspired by , we merge these priors as,

\[(i,j)=\ _{t}(i,j)+(1-)\ _{o}(i,j),\] (4)

where, \(\) serves as a dynamic weight, initially set to 1.0, and progressively reduced to 0.5 during training. This gradual adjustment of \(\) allows the model to adaptively improve its alignment based on the increasing fidelity of the OT predictions. Optimal alignment based on these priors is pictorially depicted in Fig.2(C, D). Note, the 1-dimensional alignment in Fig.2(C) is for demonstration only, our actual implementation is based on 2-dimensional distributional priors as shown in Fig.2(D).

**Background and Redundant Frames.** To effectively manage background and redundant frames, we integrate an additional 'virtual frame' within the transport matrix, following . This serves as a placeholder for aligning any frame that do not match with the primary sequence, and allows OPEL to explicitly assign these non-contributing frames to the virtual frame, as shown in Fig. 4(B). The augmented transport matrix, now denoted as \(}^{(N+1)(M+1)}\), includes an extra row and column to accommodate the virtual frame. Note, if the likelihood of a frame aligning with any salient frame falls below a predefined threshold, \(\), we assign that frame to the virtual frame.

**Training Methodology.** While the above formulation sounds promising, devising a differentiable framework to leverage these during training is pivotal. To that effect, following , we define 2 terms to effectively regularize \(}\). To capture the essence of \(_{o}\), \(}\) needs to be structured to highlight prominent values at locations corresponding to the most probable alignments,

\[M_{o}(})=_{i=1}^{N+1}_{j=1}^{M+1}}{ d_{m}+1},d_{m}=(}{N+1})^{2}+(}{M+1})^ {2}\] (5)

Similarly, to conform to \(_{t}\), \(}\) is expected to exhibit prominent values along its diagonal, reflecting temporally closely aligned frames; while off-diagonal elements should ideally possess diminished magnitudes. This sort of structural arrangement can be quantitatively assessed using:

\[M_{t}(})=_{i=1}^{N+1}_{j=1}^{M+1}}{(-)^{2}+1}\] (6)

Similar to Eqn. (4), the above 2 equations are combined as a regularizer on the transport matrix, \(M(})=\ M_{t}(})+(1-)\ M_{o}(}).\) Such a structure is known as the inverse difference moment (IDM) . To encourage optimal alignment, \(M(})\) of the learned \(}\) should be maximized. In order to facilitate this and to ensure the smooth assignment of such matches, we define a modified feasible set for \(}\) by incorporating two additional constraints into the set \(U(,)\),

\[U_{_{1},_{2}}(,)=\{} _{+}^{N+1 M+1}\ |\ }_{M+1}=,}^{}_{N+1}=,M(})_{1},(}}) _{2}\}\] (7)where \((}})=_{i=1}^{N+1}_{j=1}^{M+1}t_{ij} }{q_{ij}}\) is the Kullback- Leibler (KL) divergence between \(}\) and \(}\), and \(}\) is same as Eqn. (4) but augmented with the virtual frame. So, the regularized Wasserstein distance between \(\) and \(\) now becomes -

\[l_{_{1},_{2}}^{R}(,)=_{} U_{_{1},_{2} }(,)}},.\] (8)

The above optimization can be efficiently solved by considering its dual. As such, we incorporate two Lagrange multipliers, \(_{1}>0\) and \(_{2}>0\), to obtain the dual of Eqn. (8) as-

\[l_{_{1},_{2}}^{R}(,):=}_{_ {1},_{2}},,\,}_{_{1},_{2}}=_{} U (,)}}_{_{1},_{2}},-_{1}M(})+_{2}(} }).\] (9)

The optimal \(}_{_{1},_{2}}\) that optimizes Eqn. (9) is \(^{diag(--_{2}}{_{2}})}^{ diag(--}{_{2}})}\), where \(=[q_{ij}e^{}(s_{ij}^{_{1}}-d_{ij})}]_{ij}\), \(s_{ij}^{_{1}}=_{1}(-)^{2}+1}+d_{m}+1})\), \(d_{m}\) given by Eqn.(5), and \(\) and \(\) are the dual variables for the two equality constraints \(}_{M+1}=\), and \(}^{T}_{N+1}=\), respectively. The detailed derivation of this optimal \(}_{_{1},_{2}}\) is provided in appendix section A.1.

**Contrastive Regularization.** Incorporating temporal priors into the video alignment processes often leads to trivial solutions . So, following , we utilize the Contrastive-Inverse Difference Moment (C-IDM) loss to further regularize the training. This loss is characterized by,

\[I()=_{i=1}^{N+1}_{j=1}^{M+1}(1-(i,j)) (i,j)(0,_{3}-d(i,j))+(i,j),\] (10)

where \((i,j)=(i-j)^{2}+1\), \(d(i,j)=\|_{i}-_{j}\|\), \((i,j)\) is a neighborhood function defined as: \((i,j)=1\), if \(|i-j|\) and \(0\) otherwise, \(\) is a predefined window size, \(_{3}\) is a margin parameter. The preceding C-IDM loss is an intra-video loss. Additionally, we incorporate an inter-video contrastive loss guided by OT to further regularize the training process. Specifically, this novel loss component contrasts pairs of videos based on their similarity as quantified by the OT matrix. We find, \(x_{}(i)=_{j}}_{_{1},_{2}}\) and \(x_{}(i)=_{j}}_{_{1},_{2}}\). Likewise, \(y_{}(j)=_{i}}_{_{1},_{2}}\) and \(y_{}(j)=_{i}}_{_{1},_{2}}\) are calculated. Then, the best distance is computed as the average of squared differences between matched pairs, scaled by a temperature factor: \(=}(_{i=1 }^{N}\|_{i}-_{x_{}(i)}\|^{2}+_{j=1}^{M} \|_{j}-_{y_{}(j)}\|^{2})\). Similarly, the worst distance is: \(=}(_{i= 1}^{N}\|_{i}-_{x_{}(i)}\|^{2}+_{j=1}^{ M}\|_{j}-_{y_{}(j)}\|^{2})\). Finally, the inter-sequence loss is computed using the cross-entropy over the best and worst distances:

\[=F_{}([ \\ ],[0\\ 1]).\] (11)

Ideally, we want each frame embedding \(_{i}\), to align highly to its best match from \(\). So the best distance should be as close to \(0\) as possible, at the same time, we maximize its distance from the unmatched frame embeddings, and the same holds true for \(_{i}\)s. As a result, our proposed inter-video loss (Eqn. (11)) promotes learning disentangled representations. So, the overall loss for OPEL combines the regularized OT loss (Eqn. (9)) with the contrastive regularization terms,

\[L_{}(,)=c_{1}*l_{_{1},_{2}}^{R}(, {Y})+c_{2}*(I()+I())+c_{3}*.\] (12)

**Clustering and Key-step Ordering.** After learning the embeddings, our goal is to localize the key-steps required for PL. We frame this problem as multi-label graph-cut segmentation . The node set \(V\) of the graph includes \(k\) terminal nodes representing the key-steps and non-terminal nodes corresponding to the number of frames, which are derived from the embeddings produced by the embedder network. Upon constructing the graph, we apply \(\)-Expansion  to identify the minimum cost cut, utilizing the results to assign frames to \(k\) labels. To deduce the sequential order of key-steps, we first compute the normalized time for each frame in a video, following . Subsequently, the temporal instant for each cluster is determined by calculating the average normalized time for frames allocated to that cluster. Clusters are then sequenced in ascending order of their average time, thus outlining the sequence of key-steps of a video. Upon establishing the key-step order for all videos associated with the same task, we generate a ranked list based on the frequency at which subjects adhere to a specific sequence. The most commonly observed order is placed at the top of this list. This methodological approach allows us to discern various sequential orders of key-steps of a task.

Experiments and Results

**Datasets.** In contrast to previous research that predominantly utilized either \(1^{st}\) person or \(3^{rd}\) person viewpoints for PL, we incorporate datasets from both perspectives. For \(3^{rd}\) person view, we utilize established benchmark datasets, namely CrossTask  and ProceL . CrossTask features 213 hours of video footage spanning 18 primary tasks, totaling 2763 videos. ProceL includes 47.3 hours of video from 12 varied tasks, comprising 720 videos. To evaluate the effectiveness of our proposed OPEL framework, we apply it to the \(1^{st}\)-person EgoProceL benchmark , which contains 62 hours of egocentric video recordings from 130 subjects engaged in 16 tasks. Detailed information on individual datasets is provided in Table A2 of appendix.

**Evaluation.** Unless specified differently, we assess OPEL as per the current SOTA . We compute the framewise scores for each key-step separately and then take the mean of the scores over all the key-steps, reporting both the F1-score and the Intersection over Union (IoU). The F1-score is defined as the harmonic mean of precision and recall. Precision is calculated as the ratio of the number of frames correctly predicted as key-steps to the total number of frames labeled as key-steps. Recall is determined by the ratio of correctly predicted key-step frames to the total number of actual key-step frames. Following the methodology in , we employ the Hungarian algorithm  to derive a one-to-one mapping between the ground truth and the predictions.

**Experimental Setup.** We employ ResNet-50 (pretrained on ImageNet) as the embedder network. Inspired by , we train the embedder using pairs of training videos. Within these videos, we randomly select frames and optimize the proposed \(L_{}\) until convergence. The feature extraction is conducted from the Conv4c layer, and we subsequently create a stack of 2 context frames along the temporal dimension. Our video frames are resized to 224x224. The aggregated features are processed through two 3D convolutional layers, followed by a 3D global max pooling layer, two fully-connected layers, and a linear projection layer that outputs embeddings of 128 dimensions. All hyper-parameters are listed in Table A1. Our code is provided as part of the supplementary material.

**Results on Egocentric View.** Table 1 presents a comparative analysis between the SOTA techniques and OPEL applied to the large scale egocentric benchmark, EgoProceL. Results from tasks within CMU-MMAC and EGTEA G. have been aggregated and presented (detailed task-wise results are given in Table A4). It is important to highlight that EgoProceL represents a contemporary dataset specifically designed for egocentric procedure learning, thereby limiting the number of applicable approaches for fair comparison. Notably, OPEL outperforms the SOTA across most tasks. This superiority underscores the efficacy of the video representation learning through OT. Specifically, we achieve 22.4% (IoU) and 26.9% (F1) average improvement compared to current SOTA.

**Results on Third-person View.** We provide comparison between SOTA and OPEL on two distinct third-person datasets  in Table 2. To ensure consistency in evaluation, we follow the evaluation protocol outlined in the SOTA prior arts . Once again, we perform better in almost all cases, with

   &  &  \\   & P & R & F1 & P & R & F1 \\  Uniform & 12.4 & 9.4 & 10.3 & 8.7 & 9.8 & 9.0 \\ Alayre _et al._ & 12.3 & 3.7 & 5.5 & 6.8 & 3.4 & 4.5 \\ Kukleva _et al._ & 11.7 & 30.2 & 16.4 & 9.8 & 35.9 & 15.3 \\ Elhamifar _et al._ & 9.5 & 26.7 & 14.0 & 10.1 & **41.6** & 16.3 \\ Fried _et al._ & - & - & - & - & 28.8 & - \\ Shen _et al._ & 16.5 & 31.8 & 21.1 & 15.2 & 35.5 & 21.0 \\ CnC  & 20.7 & 22.6 & 21.6 & 22.8 & 22.5 & 22.6 \\ GPL-2D  & 21.7 & 23.8 & 22.7 & 24.1 & 23.6 & 23.8 \\ UG-3D  & 21.3 & 23.0 & 22.1 & 23.4 & 23.0 & 23.2 \\ GPL  & 22.4 & 24.5 & 23.4 & 24.9 & 24.1 & 24.5 \\ STEPS  & 23.5 & 26.7 & 24.9 & 26.2 & 25.8 & 25.9 \\ OPEL _(Ours)_ & **33.6** & **36.3** & **34.9** & **35.6** & 34.8 & **35.1** \\  

Table 2: PL results on third-person datasets . P, R, and F1 represent precision, recall, F1-score.

   &  &  &  &  &  &  \\   & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU \\  Random & 15.7 & 5.9 & 15.3 & 4.6 & 13.4 & 5.3 & 14.1 & 6.5 & 15.1 & 7.2 & 15.3 & 7.1 \\ Uniform & 18.4 & 6.1 & 20.1 & 6.6 & 16.2 & 6.7 & 16.2 & 7.9 & 17.4 & 8.9 & 18.1 & 9.1 \\ CnC  & 22.7 & 11.1 & 21.7 & 9.5 & 18.1 & 7.8 & 17.2 & 8.3 & 25.1 & 12.8 & 27.0 & 14.8 \\ GPL-2D  & 21.8 & 11.7 & 23.6 & 14.3 & 18.0 & 8.4 & 17.4 & 8.5 & 24.0 & 12.6 & 27.4 & 15.9 \\ UG-I3D  & 28.4 & 15.6 & 25.3 & 14.7 & 18.3 & 8.0 & 16.8 & 8.2 & 22.0 & 11.7 & 24.2 & 13.8 \\ GPL-w BG  & 30.2 & 16.7 & 23.6 & 14.9 & 20.6 & 9.8 & 18.3 & 8.5 & 27.6 & 14.4 & 26.9 & 15.0 \\ GPL-w/o BG  & 31.7 & 17.9 & 27.1 & **16.0** & 20.7 & 10.0 & 19.8 & 9.1 & 27.5 & 15.2 & 26.7 & 15.2 \\ OPEL _(Ours)_ & **36.5** & **18.8** & **29.5** & 13.2 & **39.2** & **20.2** & **20.7** & **10.6** & **33.7** & **17.9** & **32.2** & **16.9** \\  

Table 1: Results using EgoProceL  demonstrate the superior performance of OPEL. The results in bold and underline denote the highest and second-highest values in a column, respectively.

46.2% (F1) average enhancement over SOTA. Note, [32; 3] predominantly allocate frames to a single key-step, resulting in elevated recall rates but concomitantly diminishing precision, consequently impacting the overall F-score. Additional detailed third-person results from CMU-MMAC , ProceL  and CrossTask  are given in Table A3 and Table A5, respectively.

**Qualitative Results.** Fig. 3 illustrates the qualitative PL outcomes of the baselines and OPEL. Higher match with the ground truth in our case (the bottom row) depicts the usefulness of OPEL. Additionally, we depict the alignment of two sequences in Fig. 4(B), showcasing accurate alignment despite temporal variations, with correct matches indicating consistent action frame alignment and redundancy handling, affirming the reliability of our model.

**Comparison with Multimodal Models.** While we only use videos for training, our results are competitive with models using multiple modalities. On the egocentric EgoProcel dataset, we perform comparably or even better (4 out of 6 datasets) compared to the multimodal SOTA model, STEPS , as shown in Table 3. Note, STEPS uses gaze and depth data during training, thus enhancing its results on EPIC-Tents. We also outperform [47; 34] (Table 2), which use narrations with video.

## 5 Ablation Study

**Effectiveness of \(L_{}\).** We analyze the effectiveness of the proposed loss by replacing or combining with other SOTA losses used for PL - TCC , LAV , and CnC . Overall, the proposed \(L_{}\) outperforms previous approaches as shown in Table 4. This enhancement can be attributed to the flexibility in modeling sequences provided by OT. Furthermore, ablation results on all the loss components of \(L_{}\) are provided in Table 5, where we show the contribution of each factor individually to analyze their effect on the overall result. Comparing row 3 with row 9, we observe, the priors jointly play a critical role; as without them (row 3), the F1 and IoU scores drop by \(\)5 points.

Figure 4: (A) Impact of training data quantity on encoder training. (B) Example alignment of two videos with corresponding key-step clusters from the Brownie task .

   & CMU-MMAC & EGTEA-GAZE+ & MECCANO &  &  &  \\   & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU \\  STEPS  & 28.3 & 11.4 & **30.8** & 12.4 & 36.4 & 18.0 & **42.2** & **21.4** & 24.9 & 15.4 & 25.9 & 14.6 \\ OPEL & **36.5** & **18.8** & 29.5 & **13.2** & **39.2** & **20.2** & 20.7 & 10.6 & **34.9** & **21.3** & **35.1** & **21.5** \\  

Table 3: Comparison with models with multimodal input. Note, STEPS  uses additional data (optical flow, gaze, depth) for training, while we use just the visual modality.

Figure 3: Qualitative results from MECCANO  and PC Assembly  tasks. Each sub-task is color-coded to represent different key-steps, while gray areas signify background elements. Notably, OPELâ€™s performance surpasses that of the SOTA networks, attributed to its ability to handle unmatched frames through the integration of a virtual frame, thus enhancing alignment accuracy.

Specifically, the optimality prior has a significant impact (\(\)2 point), while the temporal prior affects the score by \(\)1 point. Similar to the combined priors, the intra and inter-video contrastive losses together (row 6 vs row 9) have a significant effect (\(\)3.5 points) on the overall performance. The individual effect of virtual frame is negligible as it only plays a role in case of excessive background frames - a scenario that is not prevalent in most datasets. Furthermore, due to the IDM structure of \(M(})\), the \(}\) and \(}\) are similar by formulation. This results in \((}})\) to be already small. As a consequence, adding the KL divergence as a standalone loss component in the proposed pipeline has a minimal impact. Overall, while some loss components may have a smaller individual impact, they do contribute to performance improvements, even if incrementally. Therefore, our proposed approach incorporates all of them to achieve the best possible results.

**Choice of clustering algorithm.** We replace the proposed clustering approach with K-means and subset selection (SS). The results in Table 6 show that OPEL performs the best, highlighting the effectiveness of OT with graphcut segmentation.

**Number of key-steps.** In Table 7, we present the results of OPEL alongside baseline models, with varying \(k\). Note, we obtain best results with \(k\)=7, and the performance drops sharply as \(k\) goes from 7 to 10 or higher. This observation is consistent with all the other SOTA methods on the same datasets . We hypothesize that \(k\)=7 works best as it is the optimal number of clusters considering the average number of distinct key-steps (subtasks) of the datasets. For example, for PC Disassembly, although the ground-truth (GT) number of steps is 9, 3 steps are quite similar (remove hard disk, remove motherboard, remove RAM), effectively making them quite close in the feature space. This results in \(k\)=7 being a better estimation of the cluster number with distinct steps. Note,

   &  &  &  &  &  &  \\   & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU & F1 & IoU \\  Random & 15.7 & 5.9 & 15.3 & 4.6 & 13.4 & 5.3 & 14.1 & 6.5 & 15.1 & 7.2 & 15.3 & 7.1 \\ OT + K-means & 34.2 & 13.5 & 23.9 & 8.8 & 31.8 & 19.6 & 16.2 & 7.9 & 24.8 & 12.5 & 27.4 & 14.4 \\ OT + SS & 34.8 & 13.2 & 23.7 & 8.7 & 31.6 & 19.5 & 17.2 & 8.3 & 25.1 & 12.8 & 28.0 & 14.8 \\ OPEL & **36.5** & **18.8** & **29.5** & **13.2** & **39.2** & **20.2** & **20.7** & **10.6** & **33.7** & **17.9** & **32.2** & **16.9** \\  

Table 6: Analysis of different clustering algorithms.

   &  &  &  &  \\   & P & F1 & IoU & P & F1 & IoU & P & F1 & IoU & P & F1 & IoU \\  TCC + PCM  & 18.5 & 19.7 & 9.5 & 15.1 & 17.9 & 8.7 & 17.5 & 19.7 & 8.8 & 19.9 & 21.7 & 11.6 \\ LAV + TCC + PCM  & 18.8 & 19.7 & 9.0 & 13.4 & 15.6 & 7.3 & 16.4 & 18.6 & 7.5 & 21.6 & 21.1 & 10.8 \\ LAV + PCM  & 20.6 & 21.1 & 9.4 & 14.6 & 17.4 & 7.1 & 17.4 & 19.1 & 8.0 & 21.5 & 22.7 & 11.7 \\ TC3J + PCM (CnC)  & 21.6 & 22.7 & 11.1 & 15.5 & 18.1 & 7.8 & 19.6 & 21.7 & 9.5 & 25.0 & 25.1 & 12.8 \\ OT + TCC & 28.8 & 32.6 & 15.6 & 25.2 & 34.5 & 17.5 & 22.6 & 26.7 & 11.2 & 27.8 & 28.2 & 15.6 \\ OT + LAV & 30.2 & 34.7 & 16.8 & 26.7 & 36.2 & 18.8 & 23.1 & 27.8 & 12.4 & 30.2 & 30.9 & 16.8 \\ OT + TCC + LAV & 27.6 & 31.2 & 15.3 & 23.8 & 33.6 & 16.1 & 21.8 & 25.4 & 10.5 & 28.1 & 28.4 & 14.7 \\ OPEL (_Ours_) & **32.8** & **36.5** & **18.8** & **28.9** & **39.2** & **20.2** & **24.3** & **29.5** & **13.2** & **32.5** & **33.7** & **17.9** \\  

Table 4: Comparison of effectiveness of \(L_{}\) with other losses.

   &  &  &  &  \\   & P & F1 & IoU & P & F1 & IoU & P & F1 & IoU & P & F1 & IoU \\  TCC + PCM  & 18.5 & 19.7 & 9.5 & 15.1 & 17.9 & 8.7 & 17.5 & 19.7 & 8.8 & 19.9 & 21.7 & 11.6 \\ LAV + TCC + PCM  & 18.8 & 19.7 & 9.0 & 13.4 & 15.6 & 7.3 & 16.4 & 18.6 & 7.5 & 21.6 & 21.1 & 10.8 \\ LAV + PCM  & 20.6 & 21.1 & 9.4 & 14.6 & 17.4 & 7.1 & 17.4 & 19.1 & 8.0 & 21.5 & 22.7 & 11.7 \\ TC3J + PCM (CnC)  & 21.6 & 22.7 & 11.1 & 15.5 & 18.1 & 7.8 & 19.6 & 21.7 & 9.5 & 25.0 & 25.1 & 12.8 \\ OT + TCC & 28.8 & 32.6 & 15.6 & 25.2 & 34.5 & 17.5 & 22.6 & 26.7 & 11.2 & 27.8 & 28.2 & 15.6 \\ OT + LAV & 30.2 & 34.7 & 16.8 & 26.7 & 36.2 & 18.8 & 23.1 & 27.8 & 12.4 & 30.2 & 30.9 & 16.8 \\ OT + TCC + LAV & 27.6 & 31.2 & 15.3 & 23.8 & 33.6 & 16.1 & 21.8 & 25.4 & 10.5 & 28.1 & 28.4 & 14.7 \\ OPEL (_Ours_) & **32.8** & **36.5** & **18.8** & **28.9** & **39.2** & **20.2** & **24.3** & **29.5** & **13.2** & **32.5** & **33.7** & **17.9** \\  

Table 5: Analysis of the impact of each term in \(L_{}\) on the overall performance.

this demarcation of subtasks (hence, number of clusters) is subjective and varies from dataset to dataset as well as from task to task; as some may consider semantically similar tasks (e.g. pouring oil vs water) to be one subtask, while others may consider it different. As \(k\) becomes larger than the actual distinctive number of clusters, each subtask gets split into multiple clusters with very similar embeddings, which upon comparison with GT leads to inferior results.

**Impact of Training data Quantity.** Fig. 4(A) presents the results from varying the number of training videos on MECCANO, aiming to evaluate OPEL's performance with respect to video count. We consistently outperform other SOTA methods. Overall, the performance improves with more training data, however, even with just few (2-5) videos of a task, we reach the upper-limit of other methods using full dataset, as shown in Fig. 4(A). Additional ablation results including choice of distribution as priors and hyperparameters \(_{1},_{2}\) are provided in appendix A.8.

**Comparison with AS methods.** PL and action segmentation (AS) are related but not the same. PL, when applied to a set of instructional videos depicting the same task, involves two primary steps: (i) assigning each video frame to one of the \(k\) key-steps (including background elements), and (ii) determining the logical sequence of these key-steps necessary to complete the task. As illustrated in Fig. 1, PL addresses multiple videos of a given task, enabling the identification of repetitive key-steps across these videos . In contrast, AS  focuses on a single video, thereby lacking the ability to discern repetitive key-steps across different videos.

Despite the differences between PL and AS, we compare our approach against existing SOTA unsupervised AS models and present the results in Table 8. Our model demonstrates a significant performance improvement compared to these works. In , authors report a high recall score for CrossTask as it assigns majority of the frames to a single key-step - a phenomenon also reported by . While achieving high recall is important for ensuring that most positive instances are correctly identified, it can result in a greater number of false positives, which in turn lowers precision and leads to undesirable results. Therefore, it is crucial to balance recall with precision to develop an effective model. This balance is reflected in the superior performance of our model, as evidenced by the F1-score results across various benchmarks. Note in the Table 8, our approach is compared with SOTA unsupervised AS methods for only third-person datasets, as these do not report any result on egocentric datasets.

## 6 Conclusion

In this study, we have introduced a novel approach for procedure learning leveraging optimal transport, enhanced by temporal and distributional regularizations to improve the alignment of key-steps across multiple video instances. Our method addresses inherent limitations in current SOTA techniques that primarily rely on frame-to-frame mappings and assumptions of monotonic alignment, which do not optimally utilize temporal information. We observe an improvement of 22.4% in IoU and 26.9% in F1 scores on the EgoProceL dataset, outperforming the current state-of-the-art methods. Similarly, in third-person video benchmarks, such as ProCeL and CrossTask, our framework achieves an average F1 score enhancement of 46.2% over existing methods. These advancements underscore the potential of OT guided learning in handling complex video procedure learning tasks. A limitation of the proposed OPEL framework is the assumption that subjects utilize similar objects for identical key-steps, which may introduce inaccuracies when dissimilar objects are employed in the execution of these steps. Future work will focus on exploring the integration of additional contextual and semantic features within the OT framework to further refine the procedure learning process. Moreover, extending this framework to other domains of video understanding could provide valuable insights into the general applicability of optimal transport in video analysis tasks.