# Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit

Jason D. Lee\({}^{1}\), Kazusato Oko\({}^{2,4}\), Taiji Suzuki\({}^{3,4}\), Denny Wu\({}^{5,6}\)

\({}^{1}\)Princeton University, \({}^{2}\)University of California, Berkeley, \({}^{3}\)University of Tokyo

\({}^{4}\)RIKEN AIP, \({}^{5}\)New York University, \({}^{6}\)Flatiron Institute

jasonlee@princeton.edu, oko@berkeley.edu,

taiji@mist.i.u-tokyo.ac.jp, dennywu@nyu.edu

###### Abstract

We study the problem of gradient descent learning of a single-index target function \(f_{*}()=_{*}(,)\) under isotropic Gaussian data in \(^{d}\), where the unknown link function \(_{*}:\) has information exponent \(p\) (defined as the lowest degree in the Hermite expansion). Prior works showed that gradient-based training of neural networks can learn this target with \(n d^{(p)}\) samples, and such complexity is predicted to be necessary by the correlational statistical query lower bound. Surprisingly, we prove that a two-layer neural network optimized by an SGD-based algorithm (on the squared loss) learns \(f_{*}\) with a complexity that is not governed by the information exponent. Specifically, for arbitrary polynomial single-index models, we establish a sample and runtime complexity of \(n T=(dd)\), where \(()\) hides a constant only depending on the degree of \(_{*}\); this dimension dependence matches the information theoretic limit up to polylogarithmic factors. More generally, we show that \(n d^{(p_{*}-1) 1}\) samples are sufficient to achieve low generalization error, where \(p_{*} p\) is the _generative exponent_ of the link function. Core to our analysis is the reuse of minibatch in the gradient computation, which gives rise to higher-order information beyond correlational queries.

## 1 Introduction

Single-index models are a classical class of functions that capture low-dimensional structure in the learning problem. To efficiently estimate such functions, the learning algorithm should extract the relevant (one-dimensional) subspace from high-dimensional observations; hence this problem setting has been extensively studied in deep learning theory , to examine the adaptivity to low-dimensional targets and benefit of representation learning in neural networks (NNs) optimized by gradient descent (GD). In this work we study the learning of a single-index target function under isotropic Gaussian data:

\[y_{i}=f_{*}(_{i})+_{i}, f_{*}(_{i})= _{*}(_{i},),_{i}}{}(0,_{d}),\] (1.1)

where \(_{i}\) is i.i.d. label noise, \(^{d}\) is the direction of index features, and we assume the link function \(_{*}:\) has information exponent \(p_{+}\) defined as the index of the first non-zero coefficient in the Hermite expansion (see Definition 1).

Equation (1.1) requires the estimation of the one-dimensional link function \(_{*}\) and the relevant direction \(\); it is known that learning is information theoretically possible with \(n d\) training examples . Indeed, when \(_{*}\) is polynomial, such statistical complexity can be achieved up to logarithmic factors by a tailored algorithm that exploit the structure of low-dimensional target . On the other hand, for gradient-based training of two-layer NNs, existing works established a sample complexity of \(n d^{(p)}\), which presents a gapbetween the information theoretic limit and what is computationally achievable by (S)GD. Such a gap is also predicted by the correlational statistical query (CSQ) lower bound , which roughly states that for a CSQ algorithm to learn (isotropic) Gaussian single-index models using less than exponential compute, a sample size of \(n d^{p/2}\) is necessary.

Although CSQ lower bounds are frequently cited to imply a fundamental barrier of learning via SGD (with the squared/correlation loss), strictly speaking, the CSQ model does not include empirical risk minimization with gradient descent, due to the non-adversarial noise and existence of non-correlational terms in the gradient computation. Very recently,  exploited higher-order terms in the gradient update arising from the reuse of the same training data, and showed that for certain link functions with high information exponent (\(p>2\)), two-layer NNs may still achieve weak recovery (i.e., nontrivial overlap with \(\)) after two GD steps with \((d)\) batch size. While this presents evidence that GD-trained NNs can learn \(f_{*}\) beyond the sample complexity suggested by the CSQ lower bound, the weak recovery statement in  may not translate to statistical guarantees; moreover, the class of functions where SGD can achieve vanishing generalization error is not fully characterized, as only a few specific examples of link functions are discussed.

Given the existence of (non-NN) algorithms that learn any single-index polynomials in \(n=(d)\) samples  regardless of the information exponent \(p\), and more generally, non-CSQ algorithms with a sample complexity surpassing the CSQ lower bound , it is natural to ask if gradient-based training of NNs can achieve similar statistical efficiency for this function class. Motivated by observations in  that SGD with reused data may break the "curse of information exponent", we aim to address the question:

_Can NN optimized by SGD with reused batch learn single-index \(f_{*}\) beyond the CSQ lower bound?_

_And for polynomial \(_{*}\), can learning succeed near the information-theoretic limit \(n d\)?_

Empirically, the separation between one-pass (online) and multi-pass SGD is clearly observed in Figure 1, where we trained the same two-layer ReLU neural network to learn a single-index polynomial with information exponent \(p=3\). We see that SGD with reused data (Figure 1(b)) reaches low test error using roughly \(n d\) samples, whereas online SGD fails to achieve even weak recovery with much larger sample size \(n=(d^{2})\). Our main contribution is to establish this improved statistical complexity for two-layer NNs trained by a variant of SGD with reused training data.

### Our Contributions

We answer the above question in the affirmative by showing that SGD training (with the squared loss) on a natural class of shallow NNs can achieve small generalization error using polynomial compute and a sample complexity that is not governed by the information exponent, if we employ a layer-wise optimization procedure (analogous to that in ) and reuse of the same minibatch. The core insight is that SGD can implement a full statistical query (SQ) algorithm that goes beyond CSQ, despite the correlational structure of the squared loss. Our main finding is summarized by the following theorem.

Figure 1: We train a ReLU NN (3.1) with \(N=1024\) neurons using SGD (squared loss) with step size \(=1/d\) to learn a single-index target \(f_{*}()=_{3}(,)\); heatmaps are values averaged over 10 runs. \((a)\) online SGD with batch size \(B=8\); \((b)\) GD on the same batch of size \(n\) for \(T=2^{14}\) steps. For online SGD we only report weak recovery (i.e., averaged overlap between neuron \(\) and target \(\)) since the test error does not drop.

**Theorem** (informal).: _A shallow NN with \(N=_{d}(1)\) neurons can learn arbitrary single-index models up to small population loss: \(_{}[(f_{}()-f_{*}())^{2}]=o_{d, }(1)\), if we employ an SGD-based algorithm (with reused training data) to minimize the squared loss objective, with a sample and runtime complexity of \(n,T=_{d}(d^{(p_{*}-1) 1})\), where \(p_{*}\) is the generative exponent of the link \(_{*}\)._

Note that the generative exponent  is defined as the _minimum_ information exponent of the link function \(_{*}\) after arbitrary \(L^{2}\) transformation, and hence by definition \(p_{*} p\) (equality is achieved by the identity transformation). We make the following remarks on our main result.

* We know that \(p_{*} 2\) for arbitrary _polynomial_ link functions. Therefore, the theorem suggests that NN + SGD with reused batch can learn single-index polynomials with a sample complexity \(n=_{d}(d)\) which is information theoretically optimal up to polylogarithmic factors, hence matching the efficiency of SQ algorithms tailored for low-dimensional polynomial regression .
* For non-polynomial \(_{*}\) with high generative exponent \(p_{*}>2\), our sample complexity \(n d^{p_{*}-1}\) can be interpreted as an SQ version of the online SGD result in . Since the information exponent \(p\) can be arbitrarily larger than the generative exponent \(p_{*}\), our main theorem disproves a conjecture in  stating that \(n d^{p/2}\) is the optimal sample complexity for empirical risk minimization with SGD on the squared loss / correlation loss.
* A key observation in our analysis is that with suitable activation function, SGD with reused batch can go beyond correlational queries and implement (a subclass of) SQ algorithms. This enables polynomial transformations to the labels that reduce the information exponent, and therefore optimization can escape the high-entropy "equator" at initialization in polylogarithmic time.

Upon completion of this work, we became aware of the preprint  showing weak recovery (for polynomial targets with \(p_{*} 2\)) with similar sample complexity, also by exploiting the reuse of training data. Our work was conducted independently and simultaneously.

## 2 Problem Setting and Prior Works

Notations.\(\|\,\,\|\) denotes the \(_{2}\) norm for vectors and the \(_{2}_{2}\) operator norm for matrices. \(O_{d}()\) and \(o_{d}()\) stand for the big-O and little-o notations, where the subscript highlights the asymptotic variable \(d\) and suppresses dependence on \(p,q\); we write \(()\) when (poly-)logarithmic factors are ignored. \(_{d,}()\) (resp. \(o_{d,}()\)) represents big-O (resp. little-o) in probability as \(d\). \((),()\) are defined analogously. \(\) is the standard Gaussian distribution in \(\). We denote the \(L^{2}\)-norm of a function \(f\) with respect to the data distribution (which will be specified) as \(\|f\|_{L^{2}}\). For \(g:\), we denote \(g^{i}\) as its \(i\)-th exponentiation, and \(g^{(i)}\) is the \(i\)-th derivative. We say an event happens _with high probability_ when the failure probability is bounded by \((-C d)\) for large constant \(C\).

### Complexity of Learning Single-index Models

We aim to learn a single-index model (1.1) where the link function \(_{*}:\) has information exponent \(p\) defined as follows .

**Definition 1** (Information exponent).: _Let \(\{_{j}\}_{j=0}^{}\) denote the normalized Hermite polynomials. The information exponent of \(g L^{2}()\), denoted by \((g):=p_{+}\), is the index of the first non-zero Hermite coefficient of \(g\), that is, given \(g(z)=_{i=0}^{}_{i}_{i}(z)\), \(p:=\{i\!>\!0:_{i}\!\!0\}\)._

By definition, when \(_{*}\) is a degree-\(q\) polynomial, we always have \(p q\). Note that \(f_{*}\) contains \((d)\) parameters to be estimated, and hence _information theoretically \(n d\)_ samples are both su

Figure 2: Complexity of learning single-index model where the link function \(_{*}\) is a degree-\(q\) polynomial with information exponent \(p\). For the CSQ lower bound, we translate the tolerance to sample complexity using the i.i.d. concentration heuristic \( n^{-1/2}\). We restrict ourselves to algorithms using polynomial compute; this excludes the sphere-covering procedure in  or exponential-width neural network in .

ficient and necessary for learning ; however, the sample complexity achieved by different (polynomial time) algorithms depends on structure of the link function.

* **Kernel Methods.** Rotationally invariant kernels cannot adapt to the low-dimensional structure of single-index \(f_{*}\) and hence suffer from the curse of dimensionality . By a standard dimension argument , we know that in the isotropic data setting, kernel methods (including neural networks in the lazy regime ) require \(n d^{q}\) samples to learn degree-\(q\) polynomials in \(^{d}\).
* **Gradient-based Training of NNs.** While NNs can easily approximate a single-index model , the sample complexity of gradient-based learning established in prior works typically scales as \(n d^{(p)}\): in the well-specified setting,  proved a sample complexity of \(n=(d^{p-1})\) for online SGD, which is later improved to \((d^{p/2})\) by a smoothed objective ; as for the misspecified setting,  showed that \(n d^{p}\) samples suffice, and in some cases a \((d^{p-1})\) complexity is achievable . Consequently, at the information-theoretic limit (\(n d\)), existing results can only cover the learning of low information exponent targets . This exponential dependence on \(p\) also appears in the CSQ lower bounds , which is often considered to be indicative of the performance of SGD learning with the squared loss (see Section 2.2).

Statistical Query Learners.If we do not restrict ourselves to correlational queries, the sample complexity of learning (1.1) can be drastically improved. Specifically, for polynomial \(_{*}\),  gave an SQ algorithm that achieves low generalization error in \(n=(d)\) samples, which is near the information-theoretic limit; the key ingredient is to construct nonlinear transformations to the labels that lowers the information exponent to \(2\); similar preprocessing also appeared in context of phase retrieval . Such transformations do not belong to CSQ, but can be utilized by a full SQ learner to enhance the statistical efficiency. Recently,  introduced the _generative exponent_ which governs the complexity of SQ algorithms.

**Definition 2** (Generative exponent).: _The generative exponent (GE) of \(g L^{2}()\) is defined as the lowest information exponent (IE) after arbitrary \(L^{2}\) transformation, that is,_

\[p_{*}=:(g)=_{ L^{2}(P_{y})}( g).\]

The generative exponent is the smallest information exponent obtained by all possible label transformations. By definition we always have \(p^{*} p\), and the gap between the two indices can be arbitrarily large; for example, for the Hermite polynomials we have \((_{k})=k\) whereas \((_{k}) 2\).

 established a sample complexity lower bound of \(n=(d^{p_{*}/2 1})\) for full SQ learners with polynomial compute (assuming \( n^{-1/2}\)), and obtained matching upper bound by a tensor partial-trace algorithm. Our goal is to show that SGD training of two-layer neural network can also achieve a sample and runtime complexity that scales with \(n d^{(p_{*})}\), where the dimension dependence is governed by the generative exponent \(p_{*}\) instead of the information exponent \(p\).

### Can Gradient Descent Go Beyond Correlational Queries?

Correlational statistical query.A statistical query (SQ) learner  accesses the target \(f_{*}\) through noisy queries \(\) with error tolerance \(\): \(|-_{,y}[(,y)]|\). Lower bound on the performance of SQ algorithm is a classical measure of computational hardness. In the context of gradient-based optimization, an often-studied subclass of SQ is the _correlational_ statistical query (CSQ)  where the query is restricted to (noisy version of) \(_{,y}[()y]\). To see the connection between CSQ and SGD, consider the gradient of expected squared loss for one neuron \(f_{}()\):

\[_{}_{,y}(f_{}()-y)^{2}-_{,y}[}f_{}()}_{}]+_{}[}()_{}f_{}( )}_{}].\]

One can see that information of the target function is encoded in the correlation term in the gradient. To infer the statistical efficiency of GD in the empirical risk minimization setting, we replace the population gradient with the empirical average \(_{}(_{i=1}^{n}(f_{}(_{i})-y_{i})^{2})\), and heuristically equate the CSQ tolerance \(\) with the scale of i.i.d. concentration error \(n^{-1/2}\).

For the Gaussian single-index model class with information exponent \(p\),  proved a lower bound stating that a CSQ learner either has access to queries with tolerance \( d^{-p/4}\), or exponentially many queries are needed to learn \(f_{*}\) with small population loss. Using the heuristic \( n^{-1/2}\), this suggests a sample complexity lower bound \(n d^{p/2}\) for polynomial time CSQ algorithm. This lower bound can be achieved by a landscape smoothing procedure  (in the well-specified setting), and is conjectured to be optimal for empirical risk minimization with SGD .

SGD with reused data.As previously discussed, the gap between SQ and CSQ algorithms primarily stems from the existence of label transformations that decrease the information exponent. While such transformation cannot be utilized by a CSQ learner,  argued that they may arise from two consecutive gradient updates using the same minibatch. For illustrative purposes, consider one neuron \(f_{}()=(,)\) updated by two GD steps using the same data point \((,y)\), starting from zero initialization \(^{0}=\) (we focus on the correlational term in the loss for simplicity):

\[^{2}=^{1}+ y^{}(,^{1} )=^{}(0)}_{}+ (^{}(0)\|\|^{2} y)}_{}.\] (2.1)

Under appropriate learning rate scaling \(\|\|^{2}=(1)\), one can see that in the second gradient step, the label \(y\) is transformed by the nonlinearity \(^{}\), even though the loss function itself is not modified. Based on this observation,  showed that if the non-CSQ term in (2.1) reduces the information exponent to \(1\), then _weak recovery_ (i.e., nontrivial overlap between the first-layer parameters \(\) and index features \(\)) can be achieved after two GD steps with \(n=(d)\) samples.

### Challenges in Establishing Statistical Guarantees

Importantly, the analysis in  does not lead to concrete learnability guarantees for the class of single-index polynomials for the following reasons: \((i)\) it is not clear if an appropriate nonlinear transformation that lowers the information exponent can always be extracted from SGD with reused data, and \((ii)\) the weak recovery guarantee may not translate to a sample complexity for the trained NN to achieve small generalization error. We elaborate these technical challenges below.

SGD decreases information exponent.To show weak recovery, , Definition 3.1] assumed that the student activation \(\) can reduce the information exponent of the labels to \(1\); while a few examples are given, the existence of such transformations in SGD is not guaranteed:

* The label transformation employed in prior SQ algorithms  is based on thresholding, which reduces the information exponent to \(2\) for any polynomial \(_{*}\); however, isolating such function from SGD updates on the squared loss is challenging. Instead, we make use of monomial transformation which can be extracted from SGD via Taylor expansion.
* If the link function satisfies \(p_{*} 2\), its information exponent after arbitrary nonlinear transformation is at least \(2\); such functions are predicted not be not learnable by SGD in the \(n d\) regime . To handle this setting, we analyze the SGD update up to \((d)\) time, at which a non-trivial overlap can be established by a Gronwall-type argument similar to . For \(p_{*}=2\), this recovers results on phase retrieval when \(_{*}(z)=z^{2}\) which requires \(n=(d d)\) samples.

From weak recovery to sample complexity.Note that weak recovery (i.e., \(|,|>\) for some small constant \(>0\)) is generally insufficient to establish low generalization error of the trained NN. Therefore, we need to show that starting from a nontrivial overlap, subsequent gradient steps can achieve _strong recovery_ of the index features (i.e., \(|,|>1-\)), despite the link misspecification. After the first-layer parameters align with the target function, we train the second-layer parameters with SGD to learn the link function \(_{*}\) with the aid of random bias units .

## 3 Learning Polynomial \(f_{*}\) in Linear Sample Complexity

We first consider the setting where \(_{*}\) is polynomial with degree \(q\) specified as follows.

**Assumption 1**.: _The target function is given as \(f_{*}()=_{*}(,)\), where the link function \(_{*}:\) admits the Hermite decomposition \(_{*}(z)=_{i=p}^{q}_{i}_{i}(z)\)._

For single-index polynomials, we do not expect a computational-to-statistical gap under the SQ class  -- indeed, we will establish learning guarantees near the information theoretic limit \(n d\)

### Training Algorithm

We train the following two-layer network with \(N\) neurons using SGD to minimize the squared loss:

\[f_{}()=_{j=1}^{N}a_{j}_{j}(, _{j}+b_{j}),\] (3.1)

where \(=(_{j},a_{j},b_{j})_{j=1}^{N}\) are trainable parameters, and \(_{j}:\) is the activation function defined as the sum of Hermite polynomials up to degree \(C_{}\): \(_{j}(z):=_{i=0}^{C_{}}_{j,i}_{i}(z)\), where \(C_{}\) only depends on the degree of link function \(_{*}\). Note that we allow each neuron to have a different nonlinearity as indicated by the subscript in \(_{j}\); this subscript is omitted when we focus on the dynamics of one single neuron. Our SGD training procedure is described in Algorithm 1, and below we outline the key ingredients of the algorithm.

* Algorithm 1 employs a layer-wise training strategy common in the recent feature learning theory literature , where in the first stage, we optimize the first-layer parameters \(\{_{j}\}_{j=1}^{N}\) with normalized SGD to learn the low-dimensional latent representation (index features \(\)), and in the second phase, we train the second-layer \(\{a_{j}\}_{j=1}^{N}\) to fit the unknown link function \(_{*}\).
* The most crucial part in Phase I of Algorithm 1 is the reuse of the same minibatch in the gradient computation. Specifically, we sample a fresh batch of training examples in _every two GD steps_; this enables us to extract non-CSQ terms from two consecutive gradient updates outlined in (2.1).
* We introduce an _interpolation step_ between the current and previous iterates with hyperparameter \(\) to stabilize the training dynamics; this resembles a negative momentum often seen in optimization algorithms ; the role of this interpolation is discussed in Section 4.2. We use a projected gradient update \(_{}()=(_{d}-^{2t}^{2t} \,^{})_{}()\) for steps \(2t\) and \(2t+1\), where \(_{}\) is the Euclidean gradient; similar use of projection also appeared in .

``` Input : Step sizes \(^{t}\); momentum parameters \(^{t}\); training time \(T_{1},T_{2}\); \(_{2}\) regularization \(\). Initialize \(_{j}^{0}^{d-1}(1)\), \(a_{j}\{ c_{}\}\). Phase I: normalized SGD on first-layer parameters for\(t=0\)to\(T_{1}\)do if\(t\) is eventhen \((0,_{d}),\ y=f_{*}()+\) ; // Draw i.i.d. data \((,y)\) \(_{j}^{t}_{j}^{t}-_{j}^{t}(_{j}^{t}-_{j}^ {t-2})\), (when \(t>0\)) ; // Interpolation step \(_{j}^{t}_{j}^{t}/\|_{j}^{t}\|\) ; // Normalization  end if \(_{j}^{t+1}_{j}^{t}-^{t}_{}(f_{ }()-y)^{2}\), (\(j=1,,N\)) ; // SGD step  end for Initialize \(b_{j}([-C_{b},C_{b}])\). Phase II: SGD on second-layer parameters \(}\!_{^{N}}}_{ i=1}^{T_{2}}(f_{}(_{i})-y_{i})^{2}+\|\|^{2}\) ; // Ridge regression Output: Prediction function \( f_{}}()\) with \(}=(_{j},_{j}^{T_{1}},b_{j})_{j=1}^{N}\). ```

**Algorithm 1**Gradient-based training of two-layer neural network

### Convergence and Sample Complexity

Weak Recovery Guarantee.We first consider the "search phase" of SGD, and show that after running Phase I of Algorithm 1 for \(T=(d)\) steps, a subset of parameters \(\) achieve non-trivial overlap with the target direction \(\). We denote \(H(g;j)\) as the \(j\)-th Hermite coefficient of some \(g L^{2}()\). Our main theorems handle polynomial activations satisfying the following condition.

**Assumption 2**.: _We require the activation function to be a polynomial \((z)=_{i=0}^{C_{}}_{i}_{i}(z)\) and its degree \(C_{}\) to be sufficiently large so that \(C_{} C_{}\) holds (\(C_{q}\) is defined in Proposition 6). For all \(2 C_{}\) and \(k=0,1\), we assume that \(H^{()}(^{(1)})^{-1};k>0\)._As discussed in Appendix B.1, for a given \(_{*}\), the above assumption only needs to be met for one pair of \((k,)\). Appendix B.1.3 states that \(H^{()}(^{(1)})^{-1};k 0\) also suffices if we set the momentum parameter \(\) differently. Now we verify this condition for a wide range of polynomial activations.

**Lemma 3**.: _Given \( 2\) and \(k 0\). For \(C_{}\), if we choose \(\{_{i}\}_{i=0}^{C_{}}\) where \(_{i}\) is randomly drawn from some non-empty interval \([a_{i},b_{i}]\), then \(H(^{()}(^{(1)})^{-1};k) 0\) with probability 1._

The next theorem states that \(n=(d)\) samples are sufficient for SGD to achieve weak recovery.

**Theorem 1**.: _Under Assumptions 1 and 2, for suitable choices of hyperparameters \(^{t}=_{d}(Nd^{-1})\) and \(1-^{t}=o_{d}(1)\), there exists constant \(C(q)\) such that after Phase I of Algorithm 1 is run for \(2T_{1,1}=C(q) d(d)\) steps, with high probability, there exists a subset of neurons \(_{j}^{2T_{1}}\) with \(||=(N)\) such that \(_{j}^{2T_{1}},>c\) for some \(c 1/(d)\)._

Recall that at random initialization we have \(, d^{-1/2}\) with high probability. The theorem hence implies that SGD "escapes from mediocrity" after seeing \(n=(d)\) samples, analogous to the information exponent \(p=2\) setting studied in . We remark that due to the small second-layer initialization, the squared loss is dominated by the correlation loss, which allows us to track the evolution of each neuron independently; similar use of vanishing initialization also appeared in .

Strong recovery and sample complexity.After weak recovery is achieved, we continue Phase I to amplify the alignment. Due to the nontrivial overlap between \(\) and \(\), the objective is no longer dominated by the lowest degree in the Hermite expansion. Therefore, to establish strong recovery (\(,>1-\)), we place an additional assumption on the activation function.

**Assumption 3**.: _Given the Hermite expansions \(_{*}(z)=_{i=0}^{q}_{i}_{i}(z)\), \(_{j}(z)=_{i=0}^{C_{}}_{j,i}_{i}(z)\), we assume the coefficients satisfy \(_{i}_{j,i} 0\) for \(p i q\)._

This assumption is easily verified in the well-specified setting \(_{*}=\) since \(_{i}=_{i}\), and under link misspecification, it has been directly assumed in prior work . We follow  and show that by randomizing the Hermite coefficients of the activation function, a subset of neurons satisfy the above assumption for any degree-\(q\) polynomial link function \(_{*}\).

**Lemma 4**.: _If we set \(_{j}(z)=_{i=0}^{C_{}}_{j,i}_{i}(z)\), where for each neuron we sample \(_{j,i}}}{{}}(\{  r_{i}\})\) with appropriate constant \(r_{i}\), then Assumption 2 and 3 are satisfied in \((-(q))\)-fraction of neurons._

Note that in our construction of activation functions for both assumptions, we do not exploit knowledge of the link function \(_{*}\) other than its degree \(q\) which decides the constant \(C_{}\); see Appendix B.1 for more discussion of Assumption 3 and Lemma 4. The next theorem shows that by running Phase I for \((d)\) more steps, a subset of neurons achieves sufficiently large overlap with the index features.

**Theorem 2**.: _For student neurons satisfying Assumptions 2, 3 and parameter \(_{j}\) starting from nontrivial overlap \(c>0\) specified in Theorem 1, if Phase I of Algorithm 1 continues for \(2T_{1,2}=_{d}(d^{-2})\) steps with hyperparameters \(^{t}=_{d}(Nd^{-1})\), \(^{t}=1\), we achieve \(_{j}^{2(T_{1,1}+T_{1,2})},>1-\) with high probability._

The following proposition shows that after strong recovery, training the second-layer parameters in Phase II is sufficient for the NN model (3.1) to achieve small generalization error.

**Proposition 5**.: _After Phase I terminates, for suitable \(>0\), the output of Phase II satisfies_

\[_{}[(f_{}()-f_{*}())^{2}] ^{2}.\]

_with probability 1 as \(d\), if we set \(T_{2}=C(q)N^{4}(d)^{-4}\), \(N=C(q)(d)^{-1}\) for some constant \(C(q)\) depending on the target degree \(q\)._

Putting things together.Combining the above theorems, we conclude that in order for two-layer NN (3.1) trained by Algorithm 1 to achieve \(\) population squared loss, it is sufficient to set

\[n=T_{1}+T_{2} C(q)(d^{-2}^{-8})(d), N C(q)^{-1}(d),\]

where constant \(C(q)\) only depends on the target degree \(q\) (although exponentially). Hence we may set \(^{-1}d\) to conclude an almost-linear sample and computational complexity for learning arbitrary single-index polynomials up to \(o_{d}(1)\) population error.

Proof Sketch

In this section we outline the high-level ideas and key steps in our derivation.

### Monomial Transformation Reduces Information Exponent

To prove the main theorem, we first establish the existence of nonlinear label transformation that \((i)\) reduces the information exponent, and \((ii)\) can be easily extracted from SGD updates. If we ignore desideratum \((ii)\), then for polynomial link functions, transformations that decrease the information exponent to at most \(2\) have been constructed in [10, Section 2.1]. However, prior results are based on the thresholding function, and it is not clear if such function naturally arises from SGD with batch reuse. The following proposition shows that the effect of thresholding can also be achieved by a simple monomial transformation where the required degree can be uniformly upper bounded.

**Proposition 6**.: _Let \(g:\) be any polynomial with degree up to \(p\) and \(\|g\|_{L^{2}()}^{2}=1\), then_

1. _There exists some_ \(i C_{q}_{+}\) _such that_ \((g^{i}) 2\)_, where constant_ \(C_{q}\) _only depends on_ \(q\)_._
2. _Let_ \(g^{}:\) _be the odd part of_ \(g\) _with_ \(\|g^{}\|_{L^{2}()}^{2}>0\)_. Then there exists some_ \(i C_{q,}_{+}\) _such that_ \((g^{i})=1\)_, where constant_ \(C_{q,}\) _only depends on_ \(q\) _and_ \(\)_._

The proof can be found in Appendix A. We make the following remarks.

* The proposition implies that for any polynomial link function that is not even, there exists some \(i_{+}\) only depending on the degree of \(_{*}\) such that raising the function to the \(i\)-th power reduces the information exponent to \(1\) (this implies the generative exponent \(p_{*}=1\)). For even \(_{*}\), the information exponent after arbitrary transformation is at least \(2\) (\(p_{*}=2\)), which can also be attained by monomial transformation. Furthermore, we provide a _uniform_ upper-bound on the required degree of transformation \(i\) via a compactness argument.
* The advantage of working with monomial transformations is that they can be obtained from two GD steps on the same training example, by Taylor expanding the activation \(^{}\). In Section 4.2, we build upon this observation to show that Phase I of Algorithm 1 achieves weak recovery using \(n d\,(d)\) samples.

Intuition behind the analysis.Our proof is inspired by  which introduced a (non-polynomial) label transformation that reduces the information exponent of any degree-\(q\) polynomial to at most \(2\). To prove the existence of monomial transformation for the same purpose, we first show that for a fixed link function \(_{*}\), there exists some \(i\) such that the \(i\)-th power of the link function has information exponent \(2\), which mirrors the transformation used in . Then, we make use of the compactness of the space of link functions to define a test function and obtain a uniform bound on \(i\). As for the polynomial transformation for non-even functions, we exploit the asymmetry of \(_{*}\) to further reduce the information exponent to 1.

### SGD with Batch Reuse Implements Polynomial Transformation

Now we present a more formal discussion of (2.1) to illustrate how polynomial transformation can be utilized in batch reuse SGD. We let \(^{t}\). When one neuron \(f_{}()=(,)\) is updated by two GD steps using the same sample \((,y)\), starting from \(^{0}:=\), the alignment with \(\) becomes

\[,^{2}=, ^{1}+ y^{}(,^{1}) =,+\] \[y^{}(,) ,+_{i=0}^{C_{q}-1} \|^{2})^{i}y^{i+1}(i!)^{-1}(^{}(,)) ^{i}^{(i+1)}(,),}_{=:_{i}}.\] (4.1)

We take \( c_{}d^{-1}\) with a small constant \(c_{}\) so that \(\|\|^{2} 1\) with high probability. Crucially, the strength of each term in (4.1) can vary depending on properties of the unknown link function \(_{*}\). Hence a careful analysis is required to ensure that a suitable monomial transformation is always singled out from the gradient. We establish the following lemma on the evolution of alignment.

**Lemma 7**.: _Under the assumptions per Theorem 1, the following holds for \(p_{*}=1,2\):_

\[,^{2(t+1)},^{2t} +c_{}^{I}c_{}c_{}d^{-}{2} 1}(^{2t})^{p_{*}-1}+c_{v }c_{}d^{-}{2} 1}^{2t}.\]See Lemma 16 for the formal version. For \(p_{*}=1\), taking expectation immediately yields that weak recovery within \(((1-))^{-1}=O(d)\) steps. For \(p_{*}=2\), \(,_{j}^{2t}=:^{t}\) can be approximated by a differential equation \(}}}}}}}}}}}}{4t}= (1-)^{t}\). Solving this yields \(^{t}=^{0}((1-) t) d^{-}( (1-) t)\), and weak recovery is obtained within \(t((1-))^{-1} d=O(d d)\) steps, similar to the analysis in .

Why interpolation is needed.In our setting, the signal strength may not dominate the error from discarding the effect of normalization. In prior analyses for online SGD, given the gradient \(-\) and projection \(P_{}=_{d}-^{}\), the spherical gradient changes the alignment as \(,^{t+1}=, ^{}+ P_{}}{\|^{}+ P_{}\|} ,^{t}+, -^{2}\|\|^{2},^{t} +,DNGL23]}}\). Here \(,\) corresponds to the signal, and \(-^{2}\|\|^{2},^{t}\) comes from normalization. Thus, taking \(\) sufficiently small, the normalization error shrinks faster than the signal. However, in our case the signal shrinks at the rate of \(c_{}^{I}\) (recall that \(=c_{}d^{-1}\)), and hence taking a smaller step may not improve the signal-to-noise ratio when the degree of transformation \(I\) is large. The interpolation step in Algorithm 1 reduces the effect of normalization without shrinking the signal too much. In particular, by setting \(=1-\), we see that the signal is affected by a factor of \(\) whereas the normalization error shrinks by \(^{2}\); this allows us to boost the signal-to-noise ratio by taking \(\) small.

### Analysis of Phase II and Statistical Guarantees

Once strong recovery is achieved for the first-layer parameters, we turn to Phase II and optimize the second-layer with \(_{2}\) regularization. Since the objective is strongly convex, gradient-based optimization can efficiently minimize the empirical loss. In Appendix B.6, the learnability guarantee follows from standard analysis analogous to that in , where we construct a "certificate" second-layer \(^{*}^{N}\) that achieves small loss and small norm:

\[_{}f_{*}()-_{j=1}^{N}a_{j}^{*} _{j}_{j}^{T_{1}},+b_{j}^{ 2}^{*},\|^{*}\| r^{*},\]

from which the population loss of the regularized empirical risk minimizer can be bounded via standard Rademacher complexity argument. To construct such a certificate, we make use of the random bias units \(\{b_{j}\}_{j=1}^{N}\) to approximate the link function \(_{*}\) as done in .

## 5 Beyond Polynomial Link Functions

Thus far we have shown that for polynomial single-index target functions (which satisfy \(p_{*} 2\)), SGD with data reuse can implement a polynomial transformation to the labels that reduces the information exponent to at most 2; consequently, the trained two-layer neural network can achieve small generalization error with \(n=d(d)\) samples. However, as shown in , there exists (non-polynomial) \(_{*}\) with generative exponent \(p_{*}>2\) (i.e., label transformations cannot lower the information exponent to 2) and thus not learnable by SQ algorithms in linear sample complexity.

Nevertheless, for a single-index model with generative exponent \(p_{*}\), we know there exists an "optimal" label transformation that reduces the information exponent to \(p_{*}\). If SGD can make use of such transformation, then from the arguments in , it is natural to conjecture that a sample size of \(n d^{p_{*}-1}\) is sufficient. In this section we confirm this intuition by proving that SGD with data reuse (Algorithm 1) indeed matches this complexity. The following lemma is an analogue of Proposition 6 stating that polynomial transformations are sufficient to lower the information exponent.

**Lemma 8**.: _Given \(_{*}\) with generative exponent \(p_{*}_{+}\). Suppose we can take an orthonormal polynomial basis \(\{_{k}\}_{k}\) for the space \(L^{2}(P_{y})\) with inner product \( f,g=_{y=_{*}(z)}[f(y)g(y)]\). Then there exists some degree of transformation \(I_{*}\) such that \((_{*}^{I})=p_{*}\)._

We outline the differences and additional technical challenges to handle the \((_{*})>2\) setting.

* For general \(L^{2}\) link functions \(_{*}\), we can no longer make use of the compactness argument (see proof of Proposition 6) to upper bound the degree of monomial transformation. Hence in Lemma 8 we do not state a uniform upper bound on the required degree \(I\), unlike the polynomial setting.
* Any link function with \(p_{*}>2\) cannot be polynomial, and hence we cannot achieve low generalization error using a neural network with polynomial nonlinearity. We therefore need to use an activation function with universal function approximation ability.

### Sample Complexity for Weak Recovery

We first show that Algorithm 1 achieves weak recovery (i.e., nontrivial overlap with the ground truth \(\)) with a complexity governed by the generative exponent of the link function \(p_{*}=(_{*})\). Similar to Section 3.2, we make use of randomized activation functions to ensure the desired label transformation is encoded -- we defer the conditions on the student activation to Appendix B.1.2.

**Proposition 9**.: _Suppose the link function \(_{*}\) has generative exponent \(p_{*}\), and let \(I_{+}\) be the smallest degree of monomial transformation that lowers the information exponent to \(p_{*}\) (i.e., \((_{*}^{})=p\)). We can find a student activation function \(\) depending only on \(p,p_{*}\) and \(I\), such that if we take \(^{2t},^{2t+1}=c_{}Nd^{-1}\), \(^{2(t+1)}=1-c_{}d^{-(p_{*}-2)+/2}\) for small \(c_{},c_{}=o_{d}(1)\), and set_

\[T_{1,1} c_{}^{-1}d&(p_{*}=1)\\ d( d)&(p_{*}=2)\\ d^{p_{*}-1}&(p_{*} 3),\]

_then if the initial alignment \(^{0}, 2c_{}^{-1}d^{-}{{2}}}\), there exists \(_{*} T_{1,1}\) such that for all \(_{*}\),_

\[^{2},(1),1-o_{d}(1).\]

Proposition 9 is a generalization of Theorem 1 beyond polynomial \(_{*}\) (the proof of both results are presented in Appendix B.3,B.4), and can be interpreted as an SQ counterpart to : we establish a sufficient sample size of \(n d^{(p_{*}-1) 1}\) for Algorithm 1 to exit the search phase, which is parallel to the \(n d^{(p-1) 1}\) rate for one-pass SGD (note that our rates are slightly sharper due to logarithmic factors removed, since \(c_{}^{-1}\) can grow arbitrarily slowly with \(d\)). For high generative exponent \(_{*}\) with \(p_{*}>2\), we no longer match the information theoretically optimal sample complexity \(n d\), which is consistent with the computational-to-statistical gap observed in .

### Generalization Error Guarantee

After Phase I of Algorithm 1, we learn the unknown link function \(_{*}\) via training the second-layer. To approximate non-polynomial functions, we introduce a ReLU component in the student nonlinearity \(\) (see Lemma 12 for discussions), and make use of the approximation result for the (univariate) ReLU kernel in , which handles general \(_{*}\) whose second derivative has bounded 4th moment. Combining the above, we arrive at the following end-to-end guarantee for learning single-index models with arbitrary generative exponent using SGD training of neural network.

**Proposition 10** (Informal).: _Suppose the link function \(_{*}\) has generative exponent \(p_{*}_{*}\) and satisfies \(_{*},_{*}^{} L^{4}()\). For appropriately chosen activation function \(\) (see Appendix B.1.2), a neural network (3.1) with \(N=(1)\) neurons optimized by Algorithm 1 achieves small population loss \(_{}[(f_{}}()-f_{*}())^{2}]=o_{d, }(1)\) with a sample complexity of \(n=(d^{(p_{*}-1) 1})\)._

See Appendix B.6 for the full statement with \(\) dependence. This proposition confirms that the sample complexity for weak recovery (Proposition 9) is the bottleneck in single-index learning, as the total sample size required for Algorithm 1 to achieve low test error also scales with \(d^{(p_{*}-1) 1}\).

## 6 Conclusion and Future Directions

We showed that a two-layer neural network (3.1) trained by SGD with reused batch can learn single-index model (with generative exponent \(p_{*}\)) using \(n d^{(p_{*}-1) 1}\) samples and compute; in particular, when the link function \(_{*}\) is polynomial, we established a sample complexity of \(n=(d^{-2})\) to achieve \(\) population loss, which is almost information theoretically optimal. Our analysis is based on the observation that by reusing the same training data twice in the gradient computation, a non-correlational term arises in the SGD update that transforms the labels (despite the loss function not modified). We proved that monomial transformations that lower the information exponent of \(_{*}\) can be extracted by Taylor-expanding the SGD update; then we showed via careful analysis of the trajectory that strong recovery and low population loss is achieved under suitable activation function.

Interesting future directions include extension to multi-index models , hierarchical target functions , and in-context learning . Also, the SGD algorithm that we employ requires a layer-wise training procedure and a specific batch reuse schedule; one may therefore ask if standard multi-pass SGD training of all parameters simultaneously  (as reported in Figure 1) also achieves the same statistical efficiency.