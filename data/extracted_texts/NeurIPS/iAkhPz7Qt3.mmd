# Scaling Retrieval-Based Language Models

with a Trillion-Token Datastore

 Rulin Shao1  Jacqueline He1  Akari Asai1  Weijia Shi1

Tim Dettmers1  Sewon Min1  Luke Zettlemoyer1  Pang Wei Koh1,2

1University of Washington 2Allen Institute for AI

{rulins,jyyh,akari,swj0419,dettmers,sewon,lsz,pangwei} @cs.washington.edu

###### Abstract

Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at _inference_ time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.

Figure 1: **Datastore scaling improves language modeling and downstream task performance.**_Left:_ Datastore scaling performance on language modeling and a downstream task (MMLU) with Llama-2 and Llama-3 models. _Right:_ Compute-optimal scaling of retrieval-based language models vs. LM-only models with Pythia models. By considering the size of the datastore as an additional dimension of scaling, we can improve model performance at lower training cost.

Introduction

The scaling of large language models (LMs) has driven tremendous performance gains across a variety of tasks (Brown et al., 2020; Kaplan et al., 2020; Muennighoff et al., 2023). Current scaling laws are primarily a function of the size of the pretraining data and the number of parameters (Hoffmann et al., 2022; Muennighoff et al., 2023; Gadre et al., 2024). In this paper, we consider another dimension of scaling: the amount of data in a datastore used at _inference_ time by retrieval-based LMs, which can directly retrieve information from the datastore to use in context when generating output (Karpukhin et al., 2020; Guu et al., 2020; Izacard & Grave, 2020; Asai et al., 2024b).

Retrieval-based LMs have a range of benefits such as improved factuality (Mallen et al., 2023), effective domain adaptation (Khandelwal et al., 2020), credit attribution (Gao et al., 2023), and parametric efficiency (Min et al., 2023b). However, most prior work in retrieval-based LMs use datastores constructed from a single data source (Karpukhin et al., 2020), such as Wikipedia, with sizes on the order of a few billion tokens. While there has been some work on larger datastores (Table 1), with the largest being RETRO (Borgeaud et al., 2022; Wang et al., 2024) in the trillion-token range, these studies use proprietary datastores and custom architectures with a limited evaluation suite. As such, it remains unknown how datastore scaling helps the currently dominant retrieval-in-context approaches on a broad categories of tasks.

We first construct MassiveDS, a massively multi-domain datastore comprising 1.4 trillion tokens of both general web data and domain specific data (SS3.1) that serves as the cornerstone for our scaling study. A key challenge in studying datastore scaling is the computational cost introduced by building datastores with all possible combinations of factors such as the datastore scale, data composition, random seed for subsampling, and different data preprocessing methods. To make our study accessible, we design an efficient datastore construction pipeline that reduces the compute needed by an order of magnitude while being equivalent to the standard pipeline (SS3.2).

Using the proposed pipeline, we systematically evaluate the effects of scaling MassiveDS on retrieval-based LMs with varying numbers of parameters and pretraining tokens (SS4). Beyond upstream language modeling, we also consider a suite of diverse downstream tasks, including general-knowledge question answering (QA), domain-specialized QA, and reasoning tasks. We find that, first, datastore scaling consistently improves both language modeling and some downstream tasks in a task-dependent manner (Figure 1_Left_), much like the widely observed data and parameter scaling trends. In fact, on knowledge-intensive tasks, a small retrieval-based LM can outperform its larger LM-only counterparts. Second, since indexing a datastore is cheaper than training on the same amount of data, retrieval-based LMs enable better compute-optimal scaling trends, where they achieve superior performance than LM-only models at the same training cost (Figure 1_Right_).

Through our analyses (SS5), we show that retrieval-based LMs are capable of automatically retrieving documents that are in-domain to the query, which allows them to reap the benefits of larger, broader datastores. In addition, data quality filters and improved retrieval methods can further enhance our observed scaling trends.

Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To spur future research, we open-source MassiveDS (including the raw passages, the embedding, and the index) and all code (including our evaluation suite and pipeline for efficiently studying datastore scaling) at https://github.com/RulinShao/retrieval-scaling.

## 2 Related Work

**Retrieval-based LMs.** Unlike parametric LMs that only use data during training, retrieval-based LMs can access data through a datastore during inference (see Asai et al. (2024b) for a review). We focus on retrieve-in-context language models (RIC-LMs), which retrieves a small set of documents from the datastore and feeds a concatenation of them as an input to the LM (Ram et al., 2023; Shi et al., 2023). The RIC-LM approach is simple and allows the use of off-the-shelf retrievers and LMs, even with only black-box access.

**Scaling the retrieval datastore.** Prior work on retrieval-based LMs often focused on specific aspects of LMs such as factuality and attribution. In addition, they typically use limited-size, single-domain

[MISSING_PAGE_FAIL:3]

### Studying Datastore Scaling with the MassiveDS Pipeline

Studying datastore scaling requires constructing datastores of varying sizes and varying compositions from the raw text corpus. This involves the following operations: **data filtering**, including deduplication, decontamination, and quality filters (Soldaini et al., 2024); **data subsampling**, which randomly subselects a \(p\)-fraction of the text corpus to achieve the specified size; **indexing**, which embeds the data using an auxiliary model and builds a searchable index; **document retrieval**, which uses the index to find the top-\(k\) relevant documents for each test query1; and **top-\(k\) evaluation**, which uses the top-\(k\) documents per test query to augment the retrieval-based model. A naive approach is to run these operations in the aforementioned order for each datastore, and build separate datastores for all combinations of subsampled datastore sizes, random seeds, and other experimental variations. However, this naive approach is prohibitively expensive2 at the trillion-token datastore scale because it repeats expensive operations, as shown in Figure 2 (top).

To make the datastore scaling study feasible, we develop the MassiveDS pipeline (Figure 2 bottom). The key idea is to reorder the above operations such that the most expensive ones--indexing and retrieval--are run only once at the start and then shared across all subsequent datastore variants. Other operations with many variants--such as subsampling, deduplication, and decontamination--are performed as late as possible to minimize repeating subsequent steps. To enable this, we first retrieve a relatively large number (\(K k\)) of documents for each query and then apply the subsequent operations to these sets of retrieved documents, rather than the entire datastore. Altogether, this pipeline reduces compute, I/O, and storage requirements by more than an order of magnitude, enabling us to conduct the datastore scaling study on a modest compute budget. In Appendix A.5, we show that the results from the MassiveDS pipeline are equivalent to the results from the naive pipeline with high probability, where the randomness comes from the subsampling procedure. We provide more details on the steps in the MassiveDS pipeline in Appendix A and detailed configuration in Appendix B.1.

Note on the number of tokens in the datastore.In our figures, we plot the datastore size (on the x-axis) by multiplying the total number of tokens in the raw data pool by the subsampling fraction \(p\). A more accurate representation would be the number of tokens in the filtered data pool; however, we do not know the size of the filtered data pool as we apply data filtering on the retrieved documents instead of the raw data for computational efficiency. As a result, the number of tokens we plot on our x-axis is proportionally larger, i.e., if a \(p_{f}\) fraction of the data is filtered out (\(0<p_{f} 1\)), then the

Figure 2: **Comparison between the MassiveDS pipeline and a naive pipeline for studying datastore scaling (§3.2).** The green and red arrows indicate repeated operations. In the naive pipeline, these operations are more expensive because they require repeating expensive steps such as rebuilding the datastore index. In the MassiveDS pipeline, these operations are cheaper because they repeat fewer steps and are only run on the retrieved top-K documents instead of the full datastore. Datastore-level operations are represented by purple arrows, while document-level operations, repeated for every query, are represented by black arrows.

actual number of tokens should also be multiplied by \(p_{f}\). Since we plot datastore size on a log axis, this corresponds to a constant shift and does not change the scaling trends.

## 4 Datastore Scaling with Retrieval-Based Language Models

### Experimental Setup

Model details.Following prior work (Izacard et al., 2023; Liu et al., 2023; Ram et al., 2023; Shi et al., 2024; Xu et al., 2023; Asai et al., 2024a), we use Contriever-MSMARCO (Izacard et al., 2022), which represents every document in the datastore as a dense vector, as our retriever. We ablate the choice of retriever in Appendix E.1; we found that Contriever-MSMARCO performs on par with, or even better than, more recent larger retrievers. We augment input examples with retrieved documents at the granularity of 256-word chunks. We study datastore scaling performance with the Llama-2, Llama-3 (Touvron et al., 2023), Pythia(Biderman et al., 2023), and OLMo (Groeneveld et al., 2024) model families.

Evaluation.We consider both language modeling and downstream tasks for evaluation. We evaluate language modeling perplexity on data from two domains: (1) general web data sampled from RedPajama(Computer, 2023); (2) scientific paper data sampled from S2ORC (Lo et al., 2020). For downstream tasks, our evaluation encompasses general-knowledge, medical, math, and science domains including the following tasks. TriviaQA (**TQA**; Joshi et al. 2017) comprises trivia questions with answers sourced from Wikipedia and the web. Natural Questions (**NQ**; Kwiatkowski et al. 2019; Lee et al. 2019) comprises search engine queries and human-annotated answers from Wikipedia. Massive Multitask Language Understanding (**MMLU**; Hendrycks et al. 2021) comprises general-purpose, multi-task reasoning questions. **MedQA**(Jin et al., 2020) comprises medical multiple-choice questions sourced from professional medical exams.

Implementation details.For evaluation with retrieval, we concatenate the top \(k=3\) documents in reverse order, so that higher-ranked documents are positioned closer to the query. For downstream tasks, we evaluate models via 5-shot prompting, and we prepend the retrieved documents before the few-shot examples, followed by the question. We do not apply reranking for our main experiments in Section 4; we study the effect of rerankers in Section 5.2. More details, including decontamination measures, are in Appendix B.

### Datastore Scaling Results on Language Modeling and Downstream Tasks

Finding 1: Datastore scaling significantly helps language modeling.Figures 3(a) and (b) show perplexity curves as a function of datastore size on general web and scientific papers, respectively. Retrieval is strictly beneficial for language modeling: the LM-only baselines (denoted by dashed lines) show the highest perplexity across all models and evaluation datasets. Scaling up the datastore reduces perplexity without signs of saturation, suggesting that further scaling is likely to yield additional

Figure 3: **Scaling performance on upstream and downstream tasks with MassiveDS, in comparison with LM-only performance.**_Left:_ Perplexity (PPL) scaling performance on RedPajama (multi-domain pretraining corpus) and S2ORC (scientific papers). _Right:_ Downstream scaling performance on TriviaQA (TQA), Natural Questions (NQ), MMLU, and MedQA.

improvements. Further, datastore scaling enables small models to outperform their larger LM-only counterparts: when retrieving from MassiveDS at the largest scale, Llama-2 7B outperforms the LM-only performance of its larger Llama-2-13B counterpart. Interestingly, we find Llama-3 8B underperforms Llama-2 7B on RedPajama. This aligns with the observations in Xiao et al. (2023) and we discuss potential reasons in Appendix D.

Finding 2: Datastore scaling improves performance on several downstream tasks, and the degree of improvement is task-dependent.Figure 3(c)-(f) show the performance on four downstream tasks as a function of datastore size. Datastore scaling brings major improvements to knowledge-intensive question answering tasks such as NQ and TQA, where retrieval-based LMs significantly outperform LM-only baselines across all scales, and performance monotonically increases with datastore scale. For instance, a Llama-2 7B model that retrieves from fewer than 100B tokens can outperform both its 13B LM-only counterpart and the more capable LM-only Llama-3 8B on TQA and NQ, indicating the effectiveness of storing knowledge in the datastore.

On MMLU, a multi-subject, reasoning-heavy benchmark, datastore scaling monotonically improves performance across all model scales. Results are more mixed for MedQA, where only the weaker Llama-2 7B benefits more from datastore scaling. For both tasks, datastore scaling does not help the smaller model do better than the larger model. We suspect that this is due to task difficulty and the lack of in-domain data sources: both MMLU and MedQA are more oriented toward reasoning rather than pure factual recall, which poses bigger challenges for both the retriever and the LM. Additionally, MassiveDS only contains a small subset of web data and medical papers which may not cover all necessary information to answer these questions. We defer to future work to explore better data sources for these tasks.

### Compute-Optimal Scaling with Retrieval-Based Language Models

Next, we study performance as a function of total training-time compute and show that retrieval-based LMs achieve superior compute-optimal performance compared to LM-only models.

Figure 4: **Compute-optimal scaling curves for retrieval-based and LM-only models of varying datastore sizes, model sizes, and pretraining corpus sizes (detailed setup in §B.4). Darker green or pink indicate larger model sizes for Pythia and OLMo respectively; crossmarks in matching colors represent the same model size trained with varying numbers of tokens; each crossmark corresponds to a datastore scaling curve of lined dots similar to the ones in Figure 3. The Pareto-optimal points are highlighted in red for retrieval-based LMs and blue for LM-only. Within a fixed computational budget (represented on the x-axis), retrieval-based LMs achieve superior performance, which remains unsaturated along the datastore scaling dimension. Pythia models do not exhibit meaningful scaling curves on MMLU and MedQA that require advanced reasoning abilities.**

Use of intermediate checkpoints.We use the intermediate checkpoints of Pythia and OLMo as an approximation of models trained on different numbers of tokens, as detailed in Appendix B.4. These intermediate checkpoints share the same learning rate scheduler, with a fixed maximum number of training steps that equals or exceeds the number of steps they have been actually trained for, and therefore the performance of these intermediate checkpoints (with or without retrieval) might be lower than otherwise attainable with the same amount of compute. However, pretraining LMs from scratch for all combinations of model sizes and numbers of pretraining tokens is prohibitively expensive for an academic budget.

FLOPs calculation.We detail the FLOPs computation for datastore construction and pretraining in Appendix B.4. Datastore construction is much cheaper than pretraining because it only requires one forward pass on all tokens in the datastore with a small retriever (177M parameters in our setup), while pretraining requires a forward pass and a backward pass on pretraining tokens with an LM that can be much larger than the retriever. As we use a flat index, no additional operations are required at the indexing step, so the number of FLOPs for datastore construction equals the number of FLOPs for embedding. We note that other types of indexing, e.g., inverted file indexing (IVFADC) (Jegou et al., 2011), may require additional FLOPs during construction and fewer FLOPs at inference. We first focus on training-time compute and discuss inference cost at the end of the section.

We show the scaling curves against computational cost of retrieval-based LMs and LM-only performance on downstream tasks in Figure 4. The Pareto-optimal points for retrieval-based and LM-only settings are highlighted in red and blue, respectively.

Finding 3: Retrieval-based LMs outperform LM-only models for the same compute budget.With the same training-time compute, retrieval-based LMs achieves superior performance than LM-only models, indicating offloading FLOPs from pretraining to datastore construction can result in better performance. Therefore, we conjecture that storing factual knowledge in a datastore is more computationally efficient than memorizing factual knowledge in model parameters at training time. We note this claim assumes the LM has enough capacity to reason with the retrieved knowledge. Otherwise, an LM may fail to utilize the retrieved knowledge, which we further discuss in Finding 5.

Finding 4: Even weak language models can benefit significantly from retrieval on knowledge-intensive tasks that measure factual recall.Surprisingly, we find that retrieval-based Pythia models (trained on up to 300B tokens) and OLMo-1.7 models (trained on up to 2T tokens3) have a similar compute-optimal scaling trajectory on TriviaQA and NQ (left columns in Figure 4), despite Pythia being trained on less and lower-quality data. Both TriviaQA and NQ evaluate factual recall without complex reasoning. When the right information is provided in context using retrieval, the LM only needs to extract the answer; therefore, these results suggest that the ability to extract factual knowledge for simple factual question answering is obtained early in training.

Finding 5: Retrieval shows benefits for reasoning-intensive tasks with capable OLMo models, but it does not help when the language model is not sufficiently advanced such as Pythia.As shown on the right side of Figure 4, datastore scaling gives marginal benefits on MMLU and MedQA for Pythia models where the performance stays around random even at the 12B model size. However, OLMo, which is trained on more and better data, consistently benefits from retrieval on both tasks. We thus conjecture that training on higher-quality data, as OLMo applied in pretraining, could help the model benefit more from retrieved documents for reasoning-heavy tasks. Beyond reasoning ability, access to the right data sources for the datastore might be critical. For example, we observe fewer benefits from retrieval on MMLU and MedQA in comparison with TriviaQA and NQ. This may indicate that MMLU and MedQA need more specific data, such as relevant textbooks for MMLU and biomedical literature for MedQA, which are currently not included in MassiveDS.

Discussion on inference cost.The compute-optimal scaling study described above focuses only on the cost of training. For inference, prepending retrieved documents in context increases inference cost due to the extended context length and additional computation required for the search. On the flip side, inference cost can be reduced by switching from a larger to a smaller LM, especially since a small LM augmented with a datastore can match or outperform its larger counterparts on some tasks. We also note that there is emerging work on accelerating retrieval search and designing efficient serving strategies for retrieval-based LMs, such as Cao et al. (2023). We leave a study of inference-compute-optimal scaling to future work.

## 5 Analysis

### Effects of Data Composition

Finding 6: MassiveDS matches or outperforms single-domain datastores.The default setting in prior work is to use a single-domain datastore that is in-distribution to the downstream task. In practice, however, it is often difficult to determine and curate a datastore that is perfectly in-distribution for a downstream task, and even if we can, it limits the generality of the retrieval-based model to that task.

In Table 3, we compare MassiveDS with single-domain datastores. MassiveDS significantly outperforms these in-domain datastores on language modeling, as well as TQA and MMLU, while matching performance on NQ and MedQA.4 In Figure 5, we show that the retriever tends to retrieve from the relevant domain even in the presence of out-of-domain data in the datastore: for NQ, it retrieves relatively more frequently from Wikipedia and web sources, whereas for MedQA, it retrieves more frequently from scientific papers from peS2o (Soldaini & Lo, 2023). Thus, the retriever can maintain robustness to out-of-domain data in the datastore; this aligns with similar findings on kNN-LM (Khandelwal et al., 2020), another type of retrieval-based LM, in Shao et al. (2023). Overall, these results show that retrieving from broad datastores like MassiveDS can simultaneously improve performance across multiple domains, paving the path towards general-purpose retrieval-based models.

### Effects of Reranking

Retrieving the most relevant documents from a large-scale datastore remains a challenging problem. To study how improving the retrieval process impacts datastore scaling trends, we first retrieve 500 documents from Contriever, rerank them using a more computationally expensive model (Ram

Figure 5: **Retrievers tend to retrieve from relevant domains.** We plot the domain composition of MassiveDS vs. the top-\(1\) retrieved documents for evaluation examples from MedQA and NQ. The retriever retrieves more frequently from domains that are relevant to the evaluation examples.

  
**Tasks** & **LM-Only** & **PubMed** & **MATH** & **peS2o** & **DPR** &  &  \\  TQA \(\) & 64.1 & 64.5 & 65.5 & 65.6 & 72.6 & 72.9 & 70.5 & 62.3 & 64.7 & 64.2 & **77.0** \\ NQ \(\) & 26.6 & 26.7 & 26.4 & 26.9 & **34.6** & 33.8 & 28.0 & 26.4 & 27.0 & 26.4 & **34.6** \\ MedQA \(\) & 36.6 & 37.8 & 36.5 & 38.1 & 38.5 & 38.4 & **39.8** & 36.9 & 35.4 & 36.1 & 39.4 \\ MMLU \(\) & 45.8 & 46.8 & 47.5 & 47.4 & 48.3 & 48.1 & 48.3 & 45.6 & 46.2 & 45.9 & **49.3** \\  RedPajama (PPL) \(\) & 4.09 & 4.06 & 4.08 & 4.08 & 4.06 & 3.99 & 4.01 & 3.87 & 4.01 & 3.95 & **3.50** \\ S2ORC (PPL) \(\) & 7.18 & 7.05 & 7.10 & 6.71 & 7.08 & 7.11 & 7.14 & 6.64 & 7.08 & 7.11 & **6.57** \\   

Table 3: **Downstream and upstream performance comparison between MassiveDS for retrieval versus single-domain datastores with Llama-27.b.** “SE” is short for StackExchange. The best performance is highlighted in **bold** and the second best is underlined. We show the diverse domain coverage in MassiveDS consistently improve the performance across tasks.

et al., 2023; Sachan et al., 2022), and take the final top-\(3\) reranked documents for evaluation. Specifically, we use a **cross-encoder reranker**, which encodes a concatenation of a query and document and returns a similarity score (Nogueira & Cho, 2019). We choose Mini-LM-L12 V2, a BERT-based cross-encoder5 that is trained for passage ranking, following Izacard et al. (2022). Additionally, we use a **lexical oracle reranker**, which uses the gold answer, as an upper bound on the potential benefit realizable by a better reranker or retriever for knowledge-intensive question-answering. The oracle reranker scores each document based on whether the gold answer is included in the document and if not, the fraction of unigram overlap between the document and the answer.

Figure 6 reports scaling trends on TQA and NQ of RIC-LM with Llama2-7B. While the cross-encoder-based reranker improves performance on TQA and NQ, a notable gap persists between the oracle reranker and the cross-encoder-based reranker. These suggest that improving either retrieval or reranking can further boost the scaling performance of retrieval datastores. Improving the retriever for more reasoning-heavy tasks such as MMLU and MedQA remains an open problem (BehnamGhader et al., 2022) that we leave to future work.

### Effects of Datastore Filtering

**Data decontamination.** Data decontamination is a crucial concern when evaluating LMs, especially in retrieval-based LMs that can retrieve the test data verbatim during inference (Borgeaud et al., 2022). By default (Section 4), we perform decontamination by filtering documents with 80+% 13-gram

Figure 6: **Scaling trends on TriviaQA and NaturalQuestions using different rerankers (Section 5.2).** “Lexical Oracle” represents the oracle reranker that reorders documents based on lexical overlap with the ground-truth answer. “Cross-encoder” represents a neural reranker which uses a cross-encoder model. Both the oracle lexical reranker and the neural reranker boost scaling trends, indicating the potential improvement space by enhancing the retrieval quality.

Figure 7: Ablation study on data decontamination. ‘Aggressive Decon.’ removes a document as long as it has an 8-gram (i.e., 1.5% of the answer length) continuous overlap with the answer. ‘Standard Decon.’—our default setup—removes a document when it either has a 32-gram (i.e., 6.2% of the answer length) continuous overlap or an 80%+ Jaccard similarity with the answer. We find decontamination impacts the language modeling performance a lot but not the downstream task.

Jaccard similarity for downstream tasks and 32-gram longest sequence overlap for perplexity, which we call **standard decontamination**. Prior work such as RETRO (Borgeaud et al., 2022) only used 80+% 13-gram Jaccard similarity for decontamination. However, we find the additional 32-gram longest sequence overlap decontamination is critical for removing near-identical documents.

To study the impact of varying levels of data decontamination, we compare standard decontamination with two additional methods: (1) **no decontamination** and (2) **aggressive decontamination**, which uses 8-gram longest sequence overlap for all tasks. This is a strict filter, as 8-gram overlap occurs frequently even when documents are not nearly identical.

Figure 7 reports the performance of the Llama-2 7B model on language modeling and the Natural Questions dataset using different decontamination methods. The scaling trend shows significantly better language modeling performance without decontamination, which worsens with more aggressive decontamination methods. This suggests that the benefits in language modeling primarily arise from lexical overlap. However, retrieval continues to benefit language modeling performance even after aggressive decontamination--where no more than 8 continuous words overlap--indicating that semantically similar retrieved documents with minimal lexical overlap can still enhance language modeling. Decontamination does not significantly affect NQ performance, likely because there is less contamination of NQ in the datastore. Interestingly, decontamination decreases performance with smaller datastores, but improves final performance at larger scales.

Data quality filtering.In Appendix E.2, we study the impact of data quality filtering on MassiveDS, where we consider global data deduplication and a combination of 3 filters adapted from Dolma(Soldaini et al., 2024): whitespace filter; language filter, and alphanumeric filter. We find deduplication is helpful to minimizing saturation as the datastore scales on NQ; intuitively, subsampling with higher \(p\) increases the chance of seeing more duplicates. In addition, we observed that Dolma quality filters have a relatively limited effect. We hypothesize this is because the data sources we used in MassiveDS, such as RedPajama, have already gone through similar quality filtering processes and may not benefit much from applying additional filtering.

## 6 Limitations and Discussion

We conclude by discussing limitations and future directions. First, while our pipeline allows us to study datastore scaling efficiently, our experiments are still limited by our available compute. In particular, our compute-optimal scaling studies are limited to model families like OLMo and Pythia that release intermediate model checkpoints, since full pretraining runs exceed our budget constraints. Similarly, we conduct the full scaling study with a single retriever, as changing the retriever necessitates re-indexing the entire datastore. It remains unclear how changes in the size and architecture of the retriever affect datastore scaling trends.

Second, although MassiveDS is large in size, it might still lack high-quality data for improving performance on more complex, reasoning-heavy tasks such as MMLU and MedQA. Future work could study the effect of extending MassiveDS to more varied and higher quality data sources.

Lastly, our downstream evaluations are mostly on question-answering tasks whose outputs are either predefined choices or short form generations. We defer the evaluation on more tasks such as long-form generation and mathematical reasoning to future work.

Despite these limitations, our research shows that increasing the scale of data available at inference time can improve model performance, at lower training cost, on language modeling and a variety of downstream tasks. We expect that future work on improving retrieval-based models with large-scale datastores will lead to even larger improvements: for example, our analysis suggests that further improving the retrieval process, either through better retrievers or rerankers, could have a significant impact. We also expect scaling up the datastore to be one way to achieve extremely long-context modeling (e.g., retrieving information from a trillion-token context) and test-time scaling (e.g., spending more test-time compute to retrieve helpful information from a large datastore).