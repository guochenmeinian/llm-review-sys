# Ensemble sampling for linear bandits:

small ensembles suffice

 David Janz

University of Oxford

david.janz@stats.ox.ac.uk

&Alexander E. Litvak

University of Alberta

alitvak@ualberta.ca

&Csaba Szepesvari

University of Alberta

szepesva@ualberta.ca

Work carried out while David Janz was a postdoc with Csaba Szepesvari at the University of Alberta.

###### Abstract

We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a \(d\)-dimensional stochastic linear bandit with an interaction horizon \(T\), ensemble sampling with an ensemble of size of order \(d T\) incurs regret at most of the order \((d T)^{5/2}\). Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with \(T\)--which defeats the purpose of ensemble sampling--while obtaining near \(\) order regret. Our result is also the first to allow for infinite action sets.

## 1 Introduction

Ensemble sampling (Lu and Van Roy, 2017) is a family of randomised algorithms for balancing exploration and exploitation within sequential decision-making tasks. These algorithms maintain an ensemble of perturbed models of the value of the available actions and, in each step, select an action that has the highest value according to a model chosen uniformly at random from the ensemble. The ensemble is then incrementally updated the new observations.

Ensemble sampling was introduced as an alternative to Thompson sampling (Thompson, 1933) that is tractable whenever incremental model updates are cheap (Osband et al., 2016; Lu and Van Roy, 2017). It is thus particularly popular in deep reinforcement learning, where the models (neural networks) are trained via gradient descent in an incremental fashion. Therein, ensemble sampling features directly in algorithms such as _bootstrapped DQN_ and _ensemble+_(Osband et al., 2016, 2018), as part of other methods (Dimakopoulou and Van Roy, 2018; Curi et al., 2020), and as the motivation for methods, including _random network distillation_(Burda et al., 2018) and _hypermodels for exploration_(Dwaracherla et al., 2020). Ensemble sampling has also been applied to online recommendation systems (Lu et al., 2018; Hao et al., 2020; Zhu and Van Roy, 2021), the behavioural sciences (Eckles and Kaptein, 2019) and to marketing (Yang et al., 2020).

Despite its practicality and simple nature, ensemble sampling has thus far resisted analysis. Indeed, Qin et al. (2022), who showed a bound of order \(\) on its _Bayesian regret_ under the impractical condition that the size of the ensemble scales at least linearly with the horizon \(T\), summarise the state-of-the-art in this regard as follows:

_'A lot of work has attempted to analyze ensemble sampling, but none of them has been successful.'_

Readers familiar with the literature know that the analysis of randomised exploration methods, such as Thompson sampling, or perturbed history exploration (Kveton et al., 2020; Janz et al., 2024), relies on showing that at each step there is a constant probability of choosing a model that overestimates the true value (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017). For these methods, the analysisis relatively simple, because the algorithms can be described as first fitting a model to the past data, and then using a fresh source of randomness to perturb the model, which is then used to derive the action to be used. Ensemble sampling does not fit this pattern: here, the distribution of models given the past is controlled only implicitly, making the analysis challenging.

Our contribution is a guarantee that (a symmetrised version of) ensemble sampling, given an ensemble size logarithmic in \(T\) and linear in the number of features \(d\), incurs regret no worse than order \((d T)^{5/2}\). This is the first successful analysis of ensemble sampling in the stochastic linear bandit setting, or indeed, any structured setting (see Remarks 6 and 7 for a discussion of this claim). Our result is based on (a slight extension of) the now-standard framework of Abeille and Lazaric (2017), and as such it ought to be possible to extend it to the usual settings: generalised linear bandits (Filippi et al., 2010), kernelised bandits (Srinivas et al., 2010), deep learning (via the neural tangent kernel, per Jacot et al., 2018) and reinforcement learning (following the work of Zanette et al., 2020).

## 2 Linear ensemble sampling

We now outline our problem setting, a version of the linear ensemble sampling algorithm and our main result, and discuss these in the context of prior literature. We encourage readers less familiar with the motivation around ensemble sampling to consult Lu and Van Roy (2017) or Osband et al. (2019); our focus will be on analysis. We will use the following notation:

**Sets and real numbers**: We write \(^{+}=\{1,2,\}\), \(=^{+}\{0\}\) and \([n]=\{1,2,,n\}\). We use the shorthand \((a_{t})_{t}\) for a sequence indexed by \(t\) in \(\) or \(^{+}\) when this index set can be deduced from the context. For \(a,b\), that is, for real numbers \(a\) and \(b\), \(a b\) denotes their minimum and \(a b\) their maximum.
**Vectors and matrices**: We identify vectors and linear operators with matrices. We write \(\|\|_{2}\) for the 2-norm of a vector, and \(\|\|\) for the operator norm of a matrix corresponding to the largest singular value, which we denote by \(s_{1}()\); we write \(s_{2}(),s_{3}(),\) for the remaining singular values, ordered descendingly. For vectors \(u,v\) of matching dimension, \( u,v=u^{}v\) denotes their usual inner product. We write \(_{2}^{d}\) for the 2-ball in \(^{d}\), \(_{2}^{d-1}=_{2}^{d}\) for its surface, the \((d-1)\)-sphere, and \(H_{u}\) for the closed half-space \(\{v^{d} u,v 1\}\).
**Probability**: We work on a probability space with probability measure \(\), and denote the corresponding expectation operator by \(\). We write \(()\) for the \(\)-algebra generated by the argument. For an event \(A\), we write \([A]\) for the indicator function of \(A\).
**Uniform distribution**: We write \((B)\) for the uniform distribution over a set \(B\) (when well-defined), and \((B)^{ m}\) for its \(m\)-fold outer product; that is, \((U_{1},,U_{m})(B)^{ m}\) are \(m\) i.i.d. random variables with common law \((B)\).

### Problem setting: stochastic linear bandits

We consider the standard stochastic linear bandit setting. At each step \(t^{+}\), a learner selects an action \(X_{t}\) from an action set \(\), a compact subset of \(_{2}^{d}\), and receives a random reward \(Y_{t}\) that obeys the following assumption:

**Assumption 1**.: _At each time step \(t^{+}\), the agent receives a reward of the form \(Y_{t}= X_{t},^{}+Z_{t}\) with \(^{}_{2}^{d}\) a fixed instance parameter and \(Z_{t}\) a random variable satisfying_

\[[\{sZ_{t}\}(_{t-1}_{t} (X_{t}))]\{s^{2}/2\}, s,\]

_almost surely, where \(_{t-1}=(X_{1},Y_{1},,X_{t-1},Y_{t-1})\) and \(_{t}\) is the \(\)-algebra generated by the random variables used by the learner to select its actions up to and including time \(t\)._

The learner is given a horizon \(T^{+}\) and its performance is evaluated by the (pseudo-)regret, \(R(T)\), incurred for the first \(T\) steps, defined as

\[R(T)=_{x}_{t=1}^{T} x-X_{t},^{} \,,\]

The smaller the regret, the better the learner. Under our assumptions, up to logarithmic factors, the regret of the best learners is of order \(d\)(Chapters 19-24, Lattimore and Szepesvari, 2020). Ourmain result will show that if a learner follows the upcoming ensemble sampling algorithm to select the actions \(X_{1},,X_{T}\), the regret it incurs will satisfy a similar high probability bound, with a slightly worse dependence on the dimension \(d\).

### Algorithm: linear ensemble sampling

Linear ensemble sampling, listed as Algorithm 1, proceeds as follows. At the outset, we fix an ensemble size \(m^{+}\), a regularisation parameter \(>0\) and a sequence of perturbation scale parameters \(r_{0},r_{1},r_{2},>0\). Then, before the start of each round \(t^{+}\), linear ensemble sampling computes \(m+1\)\(d\)-dimensional vectors. The first of these is the usual ridge regression estimate of \(^{}\),

\[_{t-1}=V_{t-1}^{-1}_{i=1}^{t-1}X_{i}Y_{i}  V_{t-1}=V_{0}+_{i=1}^{t-1}X_{i}X_{i}^{}  V_{0}= I\,.\] (1)

The remaining \(m\) parameter vectors, which shall serve as perturbations, are of the form

\[_{t-1}^{j}=V_{t-1}^{-1}S_{0}^{j}+_{i=1}^{t-1}X_{i}U _{i}^{j} j[m]\,,\]

where \(S_{0}^{j}^{d}\) is a _random initialisation_, and is taken to be uniform on \(_{2}^{d-1}\), and \(U_{1}^{j},U_{2}^{j},\) are _random targets_, uniform on the interval \([-1,1]\). All of these random variables are independent of the past at the time they are sampled and across the \(m\)-many replications. The algorithm then selects a random index \(J_{t}[m]\) and sign \(_{t}\{ 1\}\), both uniformly distributed over their respective ranges, computes the perturbed parameter

\[_{t}=_{t-1}+r_{t-1}_{t}_{t-1}^{J_{t}}\]

and selects an action

\[X_{t}*{arg\,max}_{x} x,_{t}\,,\]

with ties dealt with in a measurable way. All in all, the vectors

\[_{t-1} r_{t-1}_{t-1}^{1},,_{t- 1} r_{t-1}_{t-1}^{m}\]

serve as an ensemble of \(2m\)-many perturbed estimates of \(^{}\), and in each round the algorithm acts optimally with respect to one of these, selected at uniformly at random.

Our linear ensemble sampling algorithm deviates in two ways from that of Lu and Van Roy (2017):

1. The random sequence of targets used to fit the ensembles in our algorithm consists of uniform random variables rather than the Gaussians used in the previous literature. This choice simplifies the proofs, but our results hold (up to constant factors) for any suitable subgaussian targets, including Gaussian--see Remark 14 for a sketch of the argument.
2. We symmetrise our perturbations using Rademacher random variables, which does not feature in previous formulations of the ensemble sampling algorithm. This helps to create a more uniform distribution of the perturbations \((_{t}^{j})_{t}\) around zero with minimal computational overhead, and is important for our proof technique.

In Appendix A, we provide reformulation of our linear ensemble sampling algorithm that is more in line with the style of presentation of the algorithm given by Lu and Van Roy (2017).

### Regret bound for linear ensemble sampling

Our advertised result is captured in the following theorem. To state the theorem we need the sequence

\[_{t}^{}=+)/ ^{d})}\,, t\,.\]

Here and in related quantities, the superscripted \(\) expresses the dependence on a \((0,1]\).

**Theorem 1**.: _Fix \((0,1]\) and take \(r_{t}=7_{t}^{}\) for all \(t\), \( 5\) and \(m 400(NT/)\) for \(N=(134)^{d}\). Then there exists a universal constant \(C>0\) such that, with probability at least \(1-\), the regret incurred by a learner following linear ensemble sampling with these parameters in our stochastic linear bandit setting (formalised in Assumption 1) is bounded as_

\[R() Cm^{3/2}_{-1}^{}(+)[T]\,.\]

**Remark 1**.: _If \( 1/T^{}\) for some \(>0\) and we take \(=5\) and \(m\) to be as small as possible given the constraint of Theorem 1, then_

\[m C_{}d T R(T) C_{}^{}(d  T)^{5/2}\]

_for some constants \(C_{},C_{}^{}>0\) that depend on \(\) only and where the bound on the regret holds with probability \(1-\). That is, for polynomially sized confidence parameters, the ensemble size scales linearly with \(d(T)\), while the regret scales with \(d^{5/2}\) up to logarithmic-in-\(T\) factors. The latter scaling is slightly worse than that obtained for Thompson sampling (cf. Theorem 17), where the regret scales with \(d^{3/2}\), again, up to logarithmic-in-\(T\) factors._

**Remark 2**.: _Theorem 1 does not recover the expected behaviour for large ensemble sizes; that is, as \(m\). On one hand, this is not an issue: we are interested in the practical regime where the ensemble size is small. On the other, it suggests that better analysis might be possible. In Remark 12, we discuss a potential looseness in our analysis, which, if addressed, would result in the removal of a factor of \(m\) from the bound, thus bringing the regret in line with that of Thompson sampling. The technique that would be required to do so could then also be used to change the remaining \(\) dependence to an order \(\) dependence, thus recovering a bound with the right dependence on \(m\). However, as discussed in Remark 12, such improvements, if possible, may be hard to attain._

**Remark 3**.: _Our ensemble sampling algorithm requires the ensemble size \(m\) to be fixed in advance. As \(m\) depends on the horizon \(T\), the method only provides guarantees for a fixed, finite horizon \(T\), and our regret bound has a direct dependence on \(T\). The doubling trick would be a simple, but somewhat unsatisfactory way of removing this dependence. An alternative is to grow the ensemble online. However, a naive implementation of growing the ensemble size would require storing all past observations and computation per time step also growing with time, which is counter to the idea of a fast incremental method._

**Remark 4**.: _In light of Theorem 1, linear ensemble sampling might be seen as a less effective and less computationally efficient version of linear Thompson sampling. This is correct: in the linear setting, where Thompson sampling may be implemented in closed form, ensemble sampling has no clear advantages. With that said, variants of ensemble sampling and related bootstrapping methods are popular in more complex settings where closed forms are not available (say, deep reinforcement learning, per Osband et al., 2016, 2018), and the linear setting provides an important testing ground for the soundness of these algorithms._

**Remark 5**.: _Instead of using a randomly sampled sign and index \((_{t},J_{t})\), we could use the upper-confidence-bound strategy of selecting the actions using a maximum over the ensemble, that is_

\[X_{t}*{arg\,max}_{x}Q(x) Q(x )=\{ x,_{t-1}+r_{t-1}^{}_{t-1}^{ ^{}}(^{},J^{})\{ 1\}[m]\}\,.\]

_It is immediate from the proof of Theorem 1 that this would yield a regret bound scaling with \(d^{3/2}\). This raises the question of whether this max-over-ensemble approach is actually superior, or whether it's just that its much simpler analysis more readily yields a good bound._

### Comparison to related results

We now discuss the results of Lu and Van Roy (2017), Qin et al. (2022), and Lee and Oh (2024), showing that our result is the first to begin justifying the practical effectiveness of ensemble sampling.

**Remark 6**.: _The work of Lu and Van Roy (2017) makes strong claims on the frequentist regret of linear ensemble sampling. However, as pointed out by Qin et al. (2022), and confirmed by Lu and Van Roy (2017) in their updated arxiv manuscript, the analysis of Lu and Van Roy (2017) is flawed._

**Remark 7**.: _The only correct result on the regret of linear ensemble sampling is by Qin et al. (2022), which gives that for a \(d\)-dimensional linear bandit with an action set \(\) of cardinality \(K\), the Bayesian regret incurred--that is, average regret for \(^{}(0,I_{d})\)--is bounded as_

\[BR(T) C+CT}(d K)\]

_for some universal constant \(C>0\). Observe that this bound needs an ensemble size linear in \(T\) for Bayesian regret that scales as \(\) (up to constant and polylogarithmic factors), which largely defeats the purpose of ensemble sampling. Furthermore, the ensemble size \(m\) needs to scale linearly with \(K\) to get a \( K\) overall dependence on \(K\). Therefore, to tackle a bandit with \(=_{2}^{d}\) using the bound of Qin et al. (2022) and discretisation, since order \(2^{d-1}\)-many actions would be needed to guarantee a small discretisation error, the ensemble size \(m\) would need to scale exponentially in \(d\)._

**Remark 8**.: _Building on our result, Lee and Oh (2024) have shown that for a \(K\)-armed linear bandit, an ensemble of size on the order of \(K T\) suffices to ensure a bound on \(R(T)\) on the order of \(d^{3/2}\), ignoring logarithmic-in-\(T\) factors. This gives a regret bound tighter by a factor of \(d\) relative to ours, but at the cost of replacing the dimension \(d\) by the number of arms \(K\) in the ensemble size. Recalling that, \(K\) can be exponential in \(d\) (per Remark 12), their result may require rather large ensemble sizes. Lee and Oh (2024) also contribute some rather curious remarks about our proofs._

## 3 Analysis of linear ensemble sampling

Our analysis of ensemble sampling will be based on a _master theorem_, presented in Section 3.1 and proven in Appendix B. The master theorem provides a method for obtaining regret bound for Thompson-sampling-like randomised algorithms. Thereafter, in Section 3.2, we apply the said master theorem to linear ensemble sampling, using some intermediate results proven in Sections 3.3 and 3.4, and some routine calculations that have been deferred to the appendix. Additionally, in Appendix D, we validate our master theorem by demonstrating that it recovers a previous result for a more classical Thompson-sampling-type algorithm.

### Master Theorem: a regret bound for optimistic randomised algorithms

Our analysis of ensemble sampling will rely on the revered principle of _optimism_. To make this precise, consider a fixed instance parameter \(^{}^{d}\). Writing \(J()=_{x} x,\), we call

\[^{}=\{^{d} J() J(^ {})\}\]

the set of parameters _optimistic_ for \(^{}\). We now present a'master theorem' that bounds the regret of any algorithm that chooses actions based on a randomly chosen parameter in terms of the probabilities that, given the past, the algorithm samples a parameter that falls in the intersection of ellipsoidal confidence sets and the optimistic region \(^{}\). This result generalises a similar theorem stated for Thompson sampling by Abeille and Lazaric (2017), extending it to allow for finitely supported perturbations (critical for the proof of Theorem 1) and for a finer control over the way dependencies between time-steps are handled.

**Theorem 2** (Master regret bound).: _Fix \(T^{+}\{+\}\), \((0,1]\), \( 1\), and let \((V_{t},_{t})_{t}\) be defined per equation (1). Let Assumption 1 hold, recalling the filtrations \((_{t})_{t}\) and \((_{t})_{t}\) defined therein, and let \((_{t}^{})_{t}\) be any filtration satisfying_

\[_{t-1}_{t-1}^{}_{t}\,,  t^{+}\,.\]

_Let \((b_{t})_{t}\) be a sequence of \((_{t}_{t}^{})_{t}\)-adapted nonnegative random variables, and define the ellipsoids_

\[_{t}=_{t}+b_{t}V_{t}^{-1/2}_{2}^{d}\,, t \,.\]_Let \((_{t})_{t}\) be a \(((_{t}_{t}))_{t}\)-adapted \(^{d}\)-valued sequence and suppose that_

\[X_{t}_{x_{}} x,_{t}\,, t [T]\,.\]

_Suppose further that there exist events \(_{T}\) and \(_{T}^{}\) satisfying_

\[_{T}_{t=1}^{T}\{_{t}_{t-1}\}\,, _{T}^{}_{t=1}^{T}\{^{}_{t-1}\} (_{T})(_{ T}^{}) 1-\,.\] (2)

_Then, writing_

\[p_{t-1}=(_{t}^{}_{t-1} (_{t-1}_{t-1}^{}))\,, t ^{+}\,,\]

_we have that on a subset of \(_{T}_{T}^{}\) of probability at least \(1-3\), for all \([T]\),_

\[R() 2_{i[]}}{p_{i-1}}(2)}+}{})})\,.\]

We defer the proof of Theorem 2 to Appendix B.

**Remark 9**.: _To apply the theorem, we need to show that our algorithm generates \((_{t})_{t}\) such that the probability of each \(_{t}\) landing in \(^{}_{t-1}\), conditional on \((_{t-1}_{t-1}^{})\), is bounded away from zero for all \(t[T]\). We have two degrees of freedom in our analysis:_

1. _We can choose_ \((b_{t})_{t}\)_, the widths of the ellipsoids_ \((_{t})_{t}\)_. Larger widths may make it easier to bound_ \((p_{t})_{t}\) _away from zero, but at a linear cost in the regret bound._
2. _We can choose_ \(_{t-1}^{}\)_, the 'point' between observing_ \(Y_{t-1}\) _and selecting_ \(_{t}\) _with respect to which we consider the aforementioned conditional probabilities defined._

**Remark 10**.: _The introduction of \(_{t-1}^{}\) serves to model the case where \(_{t}\) is sampled from a distribution \(P_{t}\) that itself depends on \(X_{1},Y_{1},,X_{t-1},Y_{t-1}\) in a random manner. The \(\)-algebra \(_{t-1}^{}\) is then such that \(P_{t}\) is \((_{t-1}_{t-1}^{})\)-measurable._

The upcoming two lemmas will be helpful our applications of the Master Theorem; both are stated in terms of the random functions \(_{t}^{d}^{d}\) given by

\[_{t}^{}(u)=_{t}+_{t}^{}V_{t}^{-1/2}u, t \,.\]

The first is the classic concentration result of Abbasi-Yadkori et al. (2011), and the second, Proposition 5 of Abeille and Lazaric (2017).

**Lemma 3**.: _Under Assumption 1, for any \((0,1]\), \(( t,\ ^{}_{t}^{}(_{2}^{d}))  1-\)._

**Lemma 4**.: _If Assumption 1 holds and \(^{}_{t}^{}(_{2}^{d})\), then for any measure \(Q\) over \(^{d}\) and \(a>0\),_

\[Q(^{}_{t}^{}(a_{2}^{d}))_{u _{2}^{d-1}}Q(_{t}^{}(H_{u} a_{2}^{d}))\,,\]

_where we recall that \(H_{u}\) denotes the closed halfspace \(\{v^{d} v,u 1\}\)._

We provide a short, clean proof of Lemma 4 in Appendix C.

### Proof of Theorem 1: regret bound for linear ensemble sampling

Let \(_{0},_{1},\) be the sequence of random matrices in \(^{d m}\) with the \(j\)th column given by

\[_{t}^{j}=V_{t}^{-1/2}S_{t}^{j}\,,_{t-1}^{j}=V_{t-1}^{-1/2}_{t-1}^{j}\,.\]

Recall that \(S_{t}^{j}=S_{0}^{j}+_{i=1}^{t}U_{i}^{j}X_{i}\) and observe that each \((S^{j})_{t}\) is a vector-valued random walk with increments given by \((U_{t}X_{t})_{t}\), where \((X_{t})_{t}\) is a serially correlated vector-valued sequence and \((U_{t}^{j})_{t}\) is a sequence of independent \(([-1,1])\) variables, and \((_{t}^{j})_{t}\) is a normalised version of \((S_{t}^{j})_{t}\). Consider the extremal singular values of \(_{t}\),

\[s_{d}(_{t})=_{u_{2}^{d-1}}\|_{t}^{}u\|_{2 }=_{u_{2}^{d-1}}_{j=1}^{m}(_{t}^{j},u)^{2} ^{} s_{1}(_{t})=\|_{t}^{ }\|=_{u_{2}^{d-1}}\|_{t}^{}u\|_{2}\,.\]

The lower of these may be interpreted as capturing how well the columns \(_{t}^{1},_{t}^{m}\) cover all directions \(u_{2}^{d-1}\), and the upper, their maximal deviations from zero. The following theorem, proven in Section 3.3, shows that, for a sufficiently large \(m\), with high probability, for all \(t[T]\), all the singular values of \(_{t-1}\) are on the order of \(\).

**Theorem 5**.: _For \( 5\) and \(m 400(3+T) 10\)d, the event_

\[_{T}=\{ t[T],\;/7 s_{d}(_{t-1}) s_{1 }(_{t-1}) 10/7\}\]

_satisfies \((_{T}) 1-NTe^{-}\), where \(N=(134)^{d}\)._

Proof of Theorem 1.: We will use the master theorem, Theorem 2. We let the filtrations \((_{t})_{t}\) and \((^{}_{t})_{t}\) needed in this theorem be defined recursively via

\[^{}_{0}=(S^{1}_{0},,S^{m}_{0}),\;\;_{ t}=(^{}_{t-1}(J_{t},_{t}))\;\;\;\; ^{}_{t}=(_{t}(U^{1}_{t},,U ^{m}_{t}))\,,\;t^{+}.\]

With this, the conditions concerning these filtrations hold.

We let \(b_{t}=a_{t}^{}\) for \(a=10\), for each \(t\), which is \((_{t}^{}_{t})_{t}\)-adapted, as required. For the two required events, we take the event \(_{T}\) from Theorem 5 and the event \(_{T}^{}=_{t}\{_{}_{t}( ^{d}_{2})\}\). To see that these satisfy the requirements given in equation (2), observe the following:

**Conditions on \(_{T}\)**: By our assumptions on \(m\) and \(\), the conditions for Theorem 5 are satisfied. Hence, \((_{T}) 1-\). We now show that on \(_{T}\), for all \(t[T]\), \(_{t}_{t-1}\). Note this is equivalent to the statement that on \(_{T}\), for all \(t[T]\), \(\|_{t}_{t-1}^{J_{t}}\|_{2} b_{t-1}/r_{t-1}=10/7\). And indeed, on \(_{T}\),

\[\|_{t}_{t-1}^{J_{t}}\|_{2}_{j}\|_{t-1}^{j}\|_{2} s _{1}(_{t-1}) 10/7\,, t[T]\,.\] (3)
**Conditions on \(_{T}^{}\)**: Since \(a 1\), \(_{t}(^{d}_{2})_{t}(a^{d}_{2})=_{t}\), and thus, by Lemma 3, \(_{}_{t}\) for all \(t\) jointly with probability at least \(1-\).

It remains to lower-bound the sequence \((p_{t})_{t}\). For this, define \(^{}_{t-1}()=((_{t-1} ^{}_{t-1}))\). Fixing \(t^{+}\), writing \(Q(A)=^{}_{t-1}(_{t} A)\), we have that on \(_{T}^{}\),

\[p_{t-1} =Q(^{}_{t-1})\] \[_{u^{d-1}_{2}}Q(_{t-1}^{}(H_{u} a ^{d}_{2}))\] (Lemma 4) \[=_{u^{d-1}_{2}}^{}_{t-1}(_{t -1}^{}(7_{t}_{t-1}^{J_{t}})_{t-1}^{}(H_{u} a ^{d}_{2}))\] (definition of \[_{t}\], \[Q\] ) \[=_{u^{d-1}_{2}}^{}_{t-1}(7_{t }_{t-1}^{J_{t}} H_{u} a^{d}_{2})\] ( \[_{t-1}^{}\] is a bijection) \[=_{u^{d-1}_{2}}_{(s,j)\{ 1 \}[m]}[7s_{t-1}^{j} H_{u} a^{d}_{2}]\,,\]

where the last equality used the definitions of \(^{}_{t-1}\), \(_{t-1}\) and \(^{}_{t-1}\).

We now show that on \(_{T}\), regardless of the choice of \(u\), for at least one \((s,j)\{ 1\}[m]\), \(7s_{t-1}^{j} H_{u} a^{d}_{2}\). Assume thus that \(_{T}\) holds. First observe that for any \((s,j)\), by equation (3), \(7s_{t-1}^{j} a^{d}_{2}\). Hence, it remains to show that for any \(u\), for some \((s,j)\), \(7s_{t-1}^{j} H_{u}\); this holds because for any \(u\),

\[1^{2}(7_{t-1})}{m}_{j=1}^{m} 7 _{t-1}^{j},u^{2}_{j} 7_{t-1}^{j},u^{2}\,.\] (4)

Hence, on \(_{T}_{T}^{}\), \(p_{t-1} 1/(2m)\) and thus \(b_{t-1}/p_{t-1} 20m^{3/2}_{t-1}^{}\). Inserting this bound into the regret bound of Theorem 2 establishes the result. 

**Remark 11** (On symmetrisation).: _If the algorithm were run without symmetrisation, following the steps of the proof we see that we need to show that on \(_{T}\), regardless the choice of \(u\), for at least one of \(j[m]\), \(7_{t-1}^{j} H_{u}\). In the presence of symmetrisation, the equivalent statement is that regardless of the choice of \(u\), for at least one 'particle' \(j\), we have either \(7_{t-1}^{j} H_{u}\) or \(7_{t-1}^{j} H_{-u}\). Through equation (4), the latter reduces to studying quadratic forms, which lead to convenient algebra._

**Remark 12** (Can we improve our bound?).: _For any \(u_{2}^{d-1}\), we lower bound the maximum \(_{j}_{t-1}^{j},u^{2}\) by the average \(_{j=1}^{m}_{t-1}^{j},u^{2}\), which we show exceeds 1, thus establishing the existence of at least one element \(_{t-1}^{j}\) in \(H_{u}\) (out of \(2m\)). This gives a \(1/(2m)\) lower bound for \(p_{t-1}\). However, lower bounding a maximum by an average might be wasteful. If, instead, we managed to lower bound the (or any other fixed-proportion order statistic) of \(_{t-1}^{t},u^{2},,_ {t-1}^{m},u^{2}\), we would be able to conclude that \(p_{t}\) is lower bounded by a constant (since a constant proportion of 'particles' are then contained in \(H_{ u}\)), and thus conclude that linear ensemble sampling performs similarly to Thompson sampling (see Appendix D for an analysis of the latter). However, order statistics are considerably more technical to work with than averages, especially in a non-i.d.d. setting, and we do not currently know if such a result is attainable (for order statistics, see Gordon et al. (2012) and Livak and Tikhomirov (2018) and the references within)._

**Remark 13** (Upper versus lower bound).: _When it comes to singular values, the difficulty is usually in the lower bounds. This is so for our problem as well. Consider our use of the upper bound in the middle inequality of equation (3): all we really needed there is that \(\|_{t-1}^{j}\|_{2}\) is upper bounded for all \(j[m]\) and \(t[T]\). But, for any fixed \(j[m]\), we can apply the standard method-of-mixtures of bound of Abbasi-Yadkori et al. (2011) to obtain that \(\|_{t-1}^{j}\|_{2}=O()\) for all \(t[T]\). An application of the union bound over the \(j[m]\) then recovers what we need, up to logarithmic terms. We note that the \(\) factor is strictly necessary, per Lattimore (2023)._

### Setting up to prove Theorem 5: bound on singular values

Theorem 5 for \(t=0\) follows by standard methods (see Appendix E for proof):

**Lemma 6**.: _Whenever \(m 10d\),_

\[( s_{d}(_{0}) s_{1}(_{0}) ) 1-e^{-m/24}.\]

To extend the result to \(t>0\), we will consider the processes \((R_{t}^{j}(u))_{t}\) and \((R_{t}(u))_{t}\) defined for \(u^{d}\), \(u 0\) by

\[R_{t}^{j}(u)=^{j}^{2}}{\|u\|_{V_{t}}^{2}}  R_{t}(u)=_{j}R_{t}^{j}(u).\]

Note that for \(v=V_{t}^{1/2}u 0\) one has \(R_{t}^{j}(u)= v,_{t}^{j}^{2}/\|v\|^{2}\). Since \(V_{t}\) is positive-definite (and hence a bijection when viewed as a linear map) we observe the following relations:

**Claim 7**.: _For all \(t 0\), \(j[m]\),_

\[_{u 0}R_{t}^{j}(u)=_{v 0}^{j} ^{2}}{\|v\|_{2}^{2}}=\|_{t}^{j}\|_{2}^{2} _{u 0}R_{t}(u)=_{v 0}^{}v\|_{2}^{2}}{m \|v\|_{2}^{2}}=^{2}(_{t})}{m}\,,\]

_and likewise \(_{u 0}R_{t}(u)=s_{d}^{2}(_{t})/m\)._

In the upcoming section (Section 3.4) we establish the following bound on \(R_{t}(u)\) for a fixed \(u_{2}^{d-1}\).

**Lemma 8**.: _Fix \(u_{2}^{d-1}\) and \( 5\). Suppose that \(m 400(3+2T)\). Then, there exists an event \(_{T}^{}\) with \((_{T}^{}) 1-Te^{-}\) such that on \(_{T}^{}\{ R_{0}(u)\}\),_

\[ R_{t}(u)\,, t[T]\,.\]

Our proof of Theorem 5 employs the above pointwise bound together with a covering argument over \(_{2}^{d-1}\). For that, we need the following Lipschitzness result, proven in Appendix F (by simple algebra), and a standard bound on the size of \(\)-nets of \(_{2}^{d-1}\) (Lemma 4.10 in Pisier, 1999).

**Lemma 9**.: _On \(_{2}^{d-1}\), \(R_{t}\) is \(L\)-Lipschitz with \(L 4\|_{t}\|^{2}\|V_{t}^{1/2}\|/(m)\)._

**Lemma 10**.: _For all \(\) in \((0,1]\), there exists an \(\)-net \(\) of \(_{2}^{d-1}\) with \(||(1+)^{d}\)._

Proof of Theorem 5.: Let \(_{}\) be a minimal \(\)-net of \(_{2}^{d-1}\) and define the event

\[_{}=\{ v_{},\; t[ T],\; R_{t}(v)\}.\]We will now confirm that, for a suitable choice of \(\), \(_{}\) is a subset of the event in Theorem 5, and that \((_{}) 1-NTe^{-}\), which establishes the theorem.

Let \(u_{2}^{d-1}\) and \(v_{}\) be such that \(\|u-v\|_{2}\). From Lemma 9, and choosing \(=/(132)\), and noting that \(\|V_{t}^{1/2}\|\), we get

\[|R_{t}(u)-R_{t}(v)|\|^{2}\|V_{t}^{1/2}\|}{m}\|^{2}}{33m}.\]

Then on \(_{}\), for our choice of \(\),

\[\|_{t}\|^{2}=m_{u 0}R_{t}(u) m_{v_{ }}R_{t}(v)+\|^{2}}{33}m+\| ^{2}}{33}\,,\]

Solving for \(\|_{t}\|^{2}\), we have that \(\|_{t}\|^{2}=s_{1}^{2}(_{t})m\). The same argument also gives that, on \(_{}\),

\[s_{d}^{2}(_{t}) m_{v_{}}R_{t}(v)-\|^{2}}{33}m-\|^{2}}{33} m.\]

Loosening these bounds slightly, we have that on \(_{}\), \(/7 s_{d}(_{t}) s_{1}(_{t}) 10/7\).

The probability that \(_{}\) occurs is at least the probability that the event of Lemma 6 occurs and that the event of Lemma 8 occurs for each \(u_{}\), noting that the former ensures \( R_{0}(u)\) for all \(u_{}\). Taking a union bound over these events, we have that

\[(_{}) 1-e^{-}-T|_{ }|e^{-} 1-T(|_{}|+1)e^{-}\,.\]

We conclude by noting that, by Lemma 10, for our choice of \(\), \(N|_{}|+1\). 

### Proof of Lemma 8

Since we now consider a fixed \(u_{2}^{d-1}\), we will write \(R_{t}^{j}:=R_{t}^{j}(u)\) and \(R_{t}:=R_{t}(u)\). Let \((_{t}^{})_{t}\) be the filtration given by \(_{t}^{}=(_{t}(_{t+1},J_ {t+1}))\) for each \(t\), and let \(_{t}^{}\) denote the \((_{t}_{t}^{}(X_{t+1}))\)-conditional expectation, which will be used to integrate out the random targets \(U_{t+1}^{1},,U_{t+1}^{m}\).

With that, we define

\[D_{t}=_{t}^{}R_{t+1}-R_{t} W_{t+1}= R_{t+1}-_{t}^{}R_{t+1}\]

to be the drift and the noise of the process \((R_{t})_{t}\), respectively. Also let

\[Q_{t}= u,X_{t+1}^{2}/\|u\|_{V_{t+1}}^{2}\]

be the strength of the drift. These are related by the following two results:

**Claim 11**.: \(D_{t}=(-R_{t})Q_{t}\) _for all \(t\)._

**Lemma 12**.: _For any \(T^{+}\) and \(m 400(3+T)\), there exists an event \(\) with \(() 1-Te^{-}\), such that on \(\{R_{0} 2\}\), for all \(0 t<T\),_

\[|_{i=}^{t}W_{i+1}|(3+_{i=}^{t} Q_{i}R_{i})\,.\]

The lemma is proven in Appendix G, and the claim in Appendix H. The constant \(\) above is just the almost sure value of \(_{t}^{}[(U_{t+1}^{j})^{2}]\); see also Remark 14 for a discussion of the significance of the \((U_{t+1}^{j})^{2}\) terms and how we bound these in the proof of said lemma.

Proof of Lemma 8.: Fix \(0 t<T\). We can decompose \(R_{t+1}\) as

\[R_{t+1}=R_{t+1}-_{t}^{}R_{t+1}+_{t}^{ }R_{t+1}-R_{t}+R_{t}=W_{t+1}+D_{t}+R_{t},\]

which unrolled back to \(\) and combined with Claim 11 gives us that

\[R_{t+1}=R_{}+_{i=}^{t}-R_{i}Q_{i}+_{ i=}^{t}W_{i+1}\,.\] (5)Observe from the above that the process \(R_{0},R_{1},\) drifts towards \(\), with strength proportional the level of deviation, scaled by \(Q_{i}\). We will now show that on the event of Lemma 12, whenever \(R_{t}\) moves sufficiently far away from \(\), the drift will overwhelm the effect of the noises \((W_{t})_{t}\). Assume henceforth that the aforementioned event holds.

_Lower bound._ We consider the excursions of \((R_{t})_{t}\) where it goes and stays below \(1/2\). Let \(0<s T\) be endpoints of such a maximal excursion, in the sense that \(R_{} 1/2\), \(R_{+1},,R_{s}<1/2\) and if \(s+1 T\) then \(R_{s+1} 1/2\). Our goal is to show that for any \( t<s\), \(R_{t+1} 9/100\), which suffices to prove the lower bound. Fix \(t[,s)\). From equation (5) and since the event from Lemma 12 holds, defining \(=1/10\),

\[R_{t+1}  R_{}+_{i=}^{t}-R_{i} Q_{i}-3+_{i=}^{t}Q_{i}R_{i}\] \[=(1-(1+)Q_{})R_{}+Q_{}-3+ _{i=+1}^{t}-(1+)R_{i}Q_{i}\] \[(1-(1+)Q_{})R_{}+Q_{}-3+ _{i=+1}^{t}-Q_{i}\] \[(1-(1+)Q_{})R_{}-3\] ( \[Q_{i} 0,\,->0\] ) \[(1-)-\] ( \[Q_{i} 1/ 1/5\], \[R_{} 1/2\], \[\] ) \[=\,.\]

_Upper bound._ The upper bound follows near-verbatim, taking \(\) with \(R_{}<R_{+1}\). 

**Remark 14**.: _The proof of Lemma 8 was where we use that the targets \((U^{j}_{t})\) are uniform--or, in particular, that they are bounded random variables--for each \(W_{t+1}\) features \((U^{j}_{t})^{2}\) terms, and might otherwise be only sub-exponential. Of course, in that case, we would simply use a truncation argument: pick some truncation level \(a>0\), set \(W^{}_{t+1}=W_{t+1} a\) for each \(t^{+}\) and work with the process given by the recursion \(R^{}_{t+1}=W^{}_{t+1}+D_{t}+R^{}_{t}\). Then, \(R_{t} R^{}_{t}\) for all \(t\), and the truncated noises \((^{}_{t+1})_{t}\) are once again subgaussian, so our approach to lower bounding \(R_{t}\) would also work for \(R^{}_{t}\). We could then establish what we needed from the upper bound (that is, a bound for the quantity in equation (3)) as discussed in Remark 13, which does not require bounded targets._

## 4 Discussion

We showed that linear ensemble sampling can work with relatively small ensembles, providing the first useful theoretical grounding for the method (see Remarks 6 and 7 for comparisons). We do, however, believe our result to be loose (see Remarks 2 and 12 for discussion); resolving this would make for an important step forward. Moreover, our algorithm uses a certain symmetrisation not used within the work of Lu and Van Roy (2017) (see Section 2.2 and Remark 11). While this symmetrisation comes with no particular downsides, we would nonetheless be curious to see whether there is a clean way of making the analysis go through without it. A natural further question is whether the idea of adding noise to the rewards is the right approach to the explore-vs-exploit dilemma. On this, first, in Janz et al. (2024) we showed that beyond the linear setting, it is losses rather than rewards that should be perturbed--with the two approaches being equivalent in the linear-Gaussian setting. Second, the very recent work of Cassel et al. (2024) shows that in the multi-armed bandit setting, a simple bootstrap-based method, which takes a max over the ensemble (as in Remark 5), yields instance dependent bounds. This raises the question of the trade-offs, if any, between randomising over the ensemble and taking a maximum, and between bootstrapping and using perturbations.