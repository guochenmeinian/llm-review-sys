# A Unified Discretization Framework for Differential Equation Approach with Lyapunov Arguments for Convex Optimization

A Unified Discretization Framework for Differential Equation Approach with Lyapunov Arguments for Convex Optimization

Kansei Ushiyama

The University of Tokyo, Tokyo, Japan

ushiyama-kansei074@g.ecc.u-tokyo.ac.jp

&Shun Sato

The University of Tokyo, Tokyo, Japan

shun@mist.i.u-tokyo.ac.jp

Takayasu Matsuo

The University of Tokyo, Tokyo, Japan

matsuo@mist.i.u-tokyo.ac.jp

###### Abstract

The differential equation (DE) approach for convex optimization, which relates optimization methods to specific continuous DEs with rate-revealing Lyapunov functionals, has gained increasing interest since the seminal paper by Su-Boyd-Candes (2014). However, the approach still lacks a crucial component to make it truly useful: there is no general, consistent way to transition back to discrete optimization methods. Consequently, even if we derive insights from continuous DEs, we still need to perform individualized and tedious calculations for the analysis of each method. This paper aims to bridge this gap by introducing a new concept called "weak discrete gradient" (wDG), which consolidates the conditions required for discrete versions of gradients in the DE approach arguments. We then define abstract optimization methods using wDG and provide abstract convergence theories that parallel those in continuous DEs. We demonstrate that many typical optimization methods and their convergence rates can be derived as special cases of this abstract theory. The proposed unified discretization framework for the differential equation approach to convex optimization provides an easy environment for developing new optimization methods and achieving competitive convergence rates with state-of-the-art methods, such as Nesterov's accelerated gradient.

## 1 Introduction

In this paper, we consider unconstrained convex optimization problems:

\[_{x^{d}}f(x).\] (1)

Various optimization methods, such as standard gradient descent and Nesterov's accelerated gradient methods (Nesterov, 1983), are known for these problems. The convergence rates of these methods have been intensively investigated based on the classes of objective functions (\(L\)-smooth and/or \(\)-strongly convex). We focus on the convergence rate of function values \(fx^{(k)}-f^{}\), while the rates for \(\| fx^{(k)}\|\) or \(\|x^{(k)}-x^{}\|\) have also been discussed. Topics particularly relevant to this study include the lower bound of convergence rates for first-order methods (see Remark 4.3 for the relationship between our framework and first-order methods) for convex and strongly convex functions: \(1/k^{2}\) for \(L\)-smooth and convex functions (cf. Nesterov (2018)) and \((1-)^{2k}\) for \(L\)-smooth and \(\)-strongly convex functions (Drori and Taylor, 2022). These lower bounds are tight, as they are achieved by some optimization methods, such as Nesterov (1983) for convex functions and Van Scoy et al. (2018); Taylor and Drori (2022a) for strongly convex functions. In these studies, the discussion is typically conducted for each method, utilizing various techniques accumulated in the optimization research field.

Whereas, it has long been known that some optimization methods can be related to continuous differential equations (DEs). Early works on this aspect include the following: the continuous gradient flow \(=- f(x)\) as a continuous optimization method was discussed in Bruck (1975). Similar arguments were later applied to second-order differential equations (Alvarez, 2000; Alvarez et al., 2002; Cabot et al., 2009). An important milestone in this direction was Su et al. (2014), where it was shown that Nesterov's famous accelerated gradient method (Nesterov, 1983) could be related to a second-order system with a convergence rate-revealing "Lyapunov functional." The insights gained from this relationship have been useful in understanding the behavior of the Nesterov method and in considering its new variants. This success has followed by many studies, including Wilson (2018); Wilson et al. (2021). The advantage of the DEs with Lyapunov functional approach (which we simply call the "DE approach" hereafter) is that the continuous DEs are generally more intuitive, and convergence rate estimates are quite straightforward thanks to the Lyapunov functionals. However, the DE approach still lacks one important component; although we can draw useful insights from continuous DEs, there is no known general way to translate them into a discrete setting. Consequently, we still need to perform complex discrete arguments for each method. This limitation was already acknowledged in Su et al. (2014): "... The translation, however, involves parameter tuning and tedious calculations. This is the reason why a general theory mapping properties of ODEs into corresponding properties for discrete updates would be a welcome advance."

In this paper we attempt to provide this missing piece by incorporating the concept of "discrete gradients" (DGs) from numerical analysis, which is used to replicate some properties of continuous DEs in discrete settings. We demonstrate that a relaxed concept of DG, which we call "weak discrete gradient" (wDG), can serve a similar purpose in the optimization context. More precisely, we show that for known DEs in the DE approach, if we define abstract optimization methods using wDGs analogously to the DEs, their abstract convergence theories can be obtained by following the continuous arguments and replacing gradients with wDGs. The tedious parts of the case-specific discrete arguments are consolidated in the definition of wDG, which simplifies the overall arguments: _we can now consider "simple continuous DE arguments" and "case-specific discrete discussions summarized in wDG" separately._ We demonstrate that many typical existing optimization methods and their rate estimates, previously done separately for each method, can be recovered as special cases of the abstract methods/theories, providing a simpler view of them. Any untested combination of a known DE and wDG presents an obvious new method and its rate, further expanding the potential for innovation in the optimization field. Creating a new wDG leads to a series of optimization methods by applying it to known DEs. One simply needs to verify if the wDG satisfies the conditions for wDG (Theorem 4.2) and reveal the constants of the wDG. If, in the future, a new DE with a rate-revealing Lyapunov functional is discovered, it should be possible to achieve similar results. We suggest first defining an abstract wDG method analogous to the DE and then examining whether the continuous theory can be translated to a discrete setting, as demonstrated in this paper.

The aforementioned paper (Su et al., 2014) concludes in the following way (continued from the previous quote) "Indeed, this would allow researchers to only study the simpler and more user-friendly ODEs." Although there is still room for minor adjustments (see the discussion on limitations below), we believe the wDG framework substantially reduces the complexity of discussions in discrete methods, allowing researchers to focus on more accessible and intuitive aspects of optimization.

We consider the problems (1) on the \(d\)-dimensional Euclidean space \(^{d}\) (\(d\) is a positive integer) with the standard inner product \(,\) and the induced norm \(\), where \(f^{d}\) represents a differentiable convex objective function. We assume the existence of the optimal value \(f^{}\) and the optimal solution \(x^{}\). In the following discussion, we use the inequality

\[\|y-x\|^{2} f(y)-f(x)- f(x),y-x,\] (2)

which holds for any \(x,y^{d}\) when \(f\) is \(\)-strongly convex and differentiable.

_Remark 1.1_.: Although the scope of this paper is limitted due to the restriction of space, the framework can be naturally extended to more general cases. Extension to the objective functions satisfying the Polyak-Lojasiewicz (PL) condition is provided in Appendix H. The framework can be extended to constrained optimizations by the DE approach for mirror descent methods (cf. Krichene et al. (2015); Wilson et al. (2021)), which the authors have already confirmed. Stochastic methods such as the stochastic gradient descent can be handled by considering random compositions of wDGs, which is left as our future work.

## 2 Short summary of the differential equation approach

Let us first consider the gradient flow:

\[=- f(x), x(0)=x_{0}^{d}.\] (3)

It is easy to see that

\[}{t}f(x(t))= f(x(t)),(t) =-^{2} 0.\] (4)

This means the flow can be regarded as a continuous optimization method. Notice that the proof is quite simple, once we admit the _chain rule of differentiation_, and the _form of the flow_ itself (3); this will be quite important in the subsequent discussion.

Despite its simplicity, the convergence rate varies depending on the class of objective functions. Below we show some known results. The following rates are proven using the so-called Lyapunov argument, which introduces a "Lyapunov functional" that explicitly contains the convergence rate. The proof is left to Appendix B (we only note here that, in addition to the two key tools the chain rule and the form of the flow we need the _convexity inequality_ (2) to complete the proof.)

**Theorem 2.1** (Convex case).: _Suppose that \(f\) is convex. Let \(x[0,)^{d}\) be the solution of the gradient flow (3). Then the solution satisfies_

\[f(x(t))-f^{}-x^{}}^{2}}{2t}.\]

**Theorem 2.2** (Strongly convex case).: _Suppose that \(f\) is \(\)-strongly convex. Let \(x[0,)^{d}\) be the solution of the gradient flow (3). Then the solution satisfies_

\[f(x(t))-f^{}^{- t}-x^{}}^{2}.\]

An incomplete partial list of works using the Lyapunov approach includes, in addition to Su et al. (2014), Karimi and Vavasis (2016); Attouch et al. (2016); Attouch and Cabot (2017); Attouch et al. (2018); Franca et al. (2018); Defazio (2019); Shi et al. (2019); Wilson et al. (2021) (see also a comprehensive list in Suh et al. (2022)). A difficulty in this approach is that the Lyapunov functionals were found only heuristically. A remedy is provided in Suh et al. (2022); Du (2022), but its target is still limited.

Next, we consider DEs corresponding to accelerated gradient methods, including Nesterov's method. As is well known, the forms of accelerated gradient methods differ depending on the class of objective functions, and consequently, the DEs to be considered also change. In this paper, we call them _accelerated gradient flows_.

When the objective functions are convex, we consider the following DE proposed in Wilson et al. (2021): let \(A_{ 0}_{ 0}\) be a differentiable strictly monotonically increasing function with \(A(0)=0\), and

\[=}{A}(v-x),=-}{4} f(x),\] (5)

with \((x(0),v(0))=(x_{0},v_{0})^{d}^{d}\).

**Theorem 2.3** (Convex case (Wilson et al. (2021))).: _Suppose that \(f\) is convex. Let \((x,v)[0,)^{d}^{d}\) be the solution of the DE (5). Then it satisfies_

\[f(x(t))-f^{}-x^{}}^{2}}{A(t)}.\]_Remark 2.4_.: If we set \(A(t)=t^{2}\), this system coincides with a continuous limit DE of the accelerated gradient method for convex functions

\[++ f(x)=0,\]

which is derived in Su et al. (2016).

_Remark 2.5_.: From Theorem 2.3, it might seem that we can achieve an arbitrarily high order rate. Although it is surely true in the continuous context, it does not imply we can construct discrete optimization methods from the ODE. In fact, greedily demanding a higher rate is penalized at the timing of discretization from the numerical stability. See, for example, the discussion in Ushiyama et al. (2022b).

Next, for strongly convex objective functions, let us consider the DE (again in Wilson et al. (2021)):

\[=(v-x),=(x-v- f(x)/)\] (6)

with \((x(0),v(0))=(x_{0},v_{0})^{d}^{d}\). (Note that this system coincides with the continuous limit ODE of the accelerated gradient method for strongly convex functions by Polyak (1964): \(+2+ f(x)=0\).)

**Theorem 2.6** (Strongly convex case (Wilson et al. (2021); Luo and Chen (2022))).: _Suppose that \(f\) is \(\)-strongly convex. Let \((x,v)[0,)^{d}^{d}\) be the solution of (6). Then it satisfies_

\[f(x(t))-f^{}^{-t}f(x_{0})-f^{}+\|v_{0}-x^{}\|^{2}.\]

## 3 Discrete gradient method for gradient flows (from numerical analysis)

The remaining issue is how we discretize the above DEs. In the optimization context, it was done separately in each study. One tempting strategy for a more systematic discretization is to import the concept of "DG," which was invented in numerical analysis for designing structure-preserving numerical methods for gradient flows such as (3) (Gonzalez (1996); McLachlan et al. (1999)). Recall that the automatic decrease of objective function came from the two keys: (a) the chain rule, and (b) the gradient flow structure. The DG method respects and tries to imitate them in discrete settings.

**Definition 3.1** (Discrete gradient (Gonzalez (1996); Quispel and Capel (1996))).: A continuous map \(_{}f^{d}^{d}^{d}\) is said to be _discrete gradient of \(f\)_ if the following two conditions hold for all \(x,y^{d}\):

\[f(y)-f(x)=_{}f(y,x),y-x,_{}f(x,x)= f(x).\] (7)

In the definition provided above, the second condition simply requires that \(_{}f\) approximates \( f\). On the contrary, the first condition, referred to as the _discrete chain rule_, is a critical requirement for the key (a). The discrete chain rule is a scalar equality constraint on the vector-valued function, and for any given \(f\), there are generally infinitely many DGs. The following is a list of some popular choices of DGs. When it is necessary to differentiate them from wDGs, we call them _strict DGs_.

**Proposition 3.2** (Strict discrete gradients).: _The following functions are strict DGs._

_Gonzalez discrete gradient \(_{}f(y,x)\) (Gonzalez (1996)):_

\[ f+ ,y-x}{\|y-x\|^{2}}(y-x).\]

_Itoh-Abe discrete gradient \(_{}f(y,x)\) (Itoh and Abe (1988)):_

\[,x_{2},x_{3},,x_{d})-f(x_{1},x_{2},x_{3}, ,x_{d})}{y_{1}-x_{2}}\\ ,y_{2},x_{3},,x_{d})-f(y_{1},x_{2},x_{3},,x_{d})}{y_{ 2}-x_{2}}\\ \\ ,y_{2},y_{3},,y_{d})-f(y_{1},y_{2},y_{3},,x_{d})}{y_{ d}-x_{d}}.\]

_Average vector field (AVF) \(_{}f(y,x)\) (Quispel and McLaren (2008)):_

\[_{0}^{1} f( y+(1-)x).\]Suppose we have a DG for a given \(f\). Then we can define a discrete scheme for the gradient flow (3):

\[-x^{(k)}}{h}=-_{}fx^{(k+1)},x^{(k)},  x^{(0)}=x_{0},\]

where the positive real number \(h\) is referred to as the step size, and \(x^{(k)} x(kh)\) is the numerical solution. The left-hand side approximates \(\) and is denoted by \(^{+}x^{(k)}\) hereafter. Note that the definition conforms to the key point (b) mentioned earlier.

The scheme decreases \(f(x^{(k)})\) as expected:

\[fx^{(k+1)}-fx^{(k)}\,/h= _{}fx^{(k+1)},x^{(k)},^{+}x^{(k)} =-_{}fx^{(k+1)},x^{(k)}^{2} 0.\]

In the first equality we used the discrete chain rule, and in the second, the form of the scheme itself. Observe that the proof proceeds in the same manner as the continuous case (4). Due to the decreasing property, the scheme should work as an optimization method. Additionally, the above argument does not reply on the step size \(h\), and it can be changed in every step (which will not destroy the decreasing property).

In the numerical analysis community, the above approach has already been attempted for optimizations (Grimm et al. (2017); Celledoni et al. (2018); Ehrhardt et al. (2018); Miyatake et al. (2018); Ringholm et al. (2018); Benning et al. (2020); Riis et al. (2022)). Although they were successful on their own, this does not immediately provide the missing piece we seek for the following reasons. First, the DG framework does not include typical important optimization methods; it even does not include the steepest descent. Second, as noted above, the proofs of rate estimates in the continuous DEs (in Section 2) require the inequality of convexity (2). Unfortunately, however, existing DGs generally do not satisfy it; see Appendix C for a counterexample. Next, we show how to overcome these difficulties.

_Remark 3.3_.: Some members of the optimization community may find the use of DGs peculiar, since it involves referring to two solutions \(x^{(k+1)},x^{(k)}\). However, in some sense, it is quite natural because the decrease of \(f\) occurs in a single step \(x^{(k)} x^{(k+1)}\). There may also be concerns about the computational complexity of DGs because the method becomes "implicit" by referring to \(x^{(k+1)}\). In the field of structure-preserving numerical methods, however, it is widely known that in some highly unstable DEs, implicit methods are often advantageous, allowing larger time-stepping widths, while explicit methods require extremely small ones. In fact, it has been confirmed in Ehrhardt et al. (2018) that this also applies to the optimization context. The Itoh-Abe DG results in a system of \(d\) nonlinear equations, which is slightly less expensive. Moreover, note that the integral in the AVF can be evaluated analytically before implementation, when \(f\) is a polynomial.

## 4 Weak discrete gradients and abstract optimization methods

We introduce the concept of _a weak discrete gradient_ (wDG), which is a relaxed version of the DG introduced earlier.

**Definition 4.1** (Weak discrete gradient).: A gradient approximation1\(f^{d}^{d}^{d}\) is said to be _weak discrete gradient of \(f\)_ if there exists \( 0\) and \(,\) with \(+ 0\) such that the following two conditions hold for all \(x,y,z^{d}\):

\[f(y)-f(x)f(y,z),y-x+\|y -z\|^{2}-\|z-x\|^{2}-\|y-x\|^{2},f(x,x)=  f(x).\] (8)

Note that (8) can be regarded as a modification of the three points descent lemma, where the third variable z is utilized to give some estimates. The freedom in variable z in (8) is fully utilized also in this paper; see Theorems 5.4 and 5.5 and their proofs.

The condition (8) can be interpreted in two ways. First, it can be understood as a discrete chain rule in a weaker sense. By substituting \(x\) with \(z\), we obtain the inequality:

\[f(y)-f(x)f(y,x),y-x+(- )\|y-x\|^{2}.\] (9)Compared to the strict discrete chain rule (7), it is weaker because it is an inequality and allows an error term. Second, it can be interpreted as a weaker discrete convex inequality. By exchanging \(x\) and \(y\) and rearranging terms, we obtain another expression

\[f(y)-f(x)-f(x,z),y-x\|y-x\|^ {2}+\|y-z\|^{2}-\|x-z\|^{2}.\] (10)

Compared to the strongly convex inequality (2), the term \((/2)\|y-x\|^{2}\) is now replaced with \(\|y-x\|^{2}+\|y-z\|^{2}\), which can be interpreted as the squared distance between \(y\) and the point \((x,z)\) where the gradient is evaluated. The term \(-\|x-z\|^{2}\) is an error term.

We now list some examples of wDGs (proof is provided in Appendix D). Notice that these examples include various typical gradient approximations from the optimization and numerical analysis literature. Note that for ease of presentation, we simply write "\(\)-strongly convex function," which includes convex functions by setting \(=0\).

**Theorem 4.2**.: _Suppose that \(f:^{d}\) is a \(\)-strongly convex function. Let (L) and (SC) denote the additional assumptions: (L) \(f\) is \(L\)-smooth, and (SC) \(>0\). Then, the following functions are wDGs:_

1. _If_ \(f(y,x)= f(x)\) _and_ \(f\) _satisfies (L), then_ \((,,)=(L/2,/2,0)\)_._
2. _If_ \(f(y,x)= f(y)\)_, then_ \((,,)=(0,0,/2)\)_._
3. _If_ \(f(y,x)= f()\) _and_ \(f\) _satisfies (L), then_ \((,,)=((L+)/8,/4,/4)\)_._
4. _If_ \(f(y,x)=_{}f(y,x)\) _and_ \(f\) _satisfies (L), then_ \((,,)=(L/6+/12,/4,/4)\)_._
5. _If_ \(f(y,x)=_{}f(y,x)\) _and_ \(f\) _satisfies (L)(SC), then_ \((,,)=((L+)/8+(L-)^{2}/16,/4,0)\)_._
6. _If_ \(f(y,x)=_{}f(y,x)\) _and_ \(f\) _satisfies (L)(SC), then_ \((,,)=(dL^{2}/-/4,/2,-/4)\)_._

Although we assumed the smoothness of \(f\) to simplify the presentation, the case (ii) does not demand it (see the end of Appendix D). Thus, it can handle non-smooth convex optimization. While the wDGs (i), (iii), (iv) only require (L), the wDGs (v) and (vi) demand (SC) (\(>0\)). This implies that the latter wDGs might be fragile for small \(\)'s.

We now define an abstract method using wDGs:

\[-x^{(k)}}{h}=-fx^{(k+1)},x^{(k)}, x^{(0)}=x_{0},\] (11)

which is analogous to the gradient flow (3). By "abstract," we mean that it is a formal formula, and given a concrete wDG it reduces to a concrete method; see Table 1 which summarizes some typical choices. Observe that the abstract method covers many popular methods from both optimization and numerical analysis communities. The step size \(h\) may be selected using line search techniques, but for simplicity, we limit our presentation to the fixed step size in this paper (see Remark 5.1).

_Remark 4.3_.: Note that some wDGs are not directly connected to the original gradient \( f\)'s; the Itoh-Abe wDG (vi) does not even refer to the gradient. Thus, the concrete methods resulting from our framework do not necessarily fall into the so-called "first-order methods," which run in a linear space spanned by the past gradients (Nesterov (1983)). This is why we use the terminology "gradient-based methods" in this paper, instead of first-order methods.

Similar to the aforementioned, we can define abstract methods for (5) and (6). Details and theoretical results can be found in Theorems 5.4 and 5.5.

We also introduce the next lemma, which is useful in expanding the scope of our framework.

**Lemma 4.4**.: _Suppose \(f\) can be expressed as a sum of two functions \(f_{1},f_{2}\). If \(_{1}f_{1}\) and \(_{2}f_{2}\) are wDGs of \(f_{1}\) and \(f_{2}\) with parameters \((_{1},_{1},_{1})\) and \((_{2},_{2},_{2})\), respectively, then \(_{1}f_{1}+_{2}f_{2}\) is a weak discrete gradient of \(f\) with \((,,)=(_{1}+_{2},_{1}+_{2},_{1}+ _{2})\)._

This lemma allows us to consider the following discretization of the gradient flow:

\[-x^{(k)}}{h}=- f_{1}x^{(k)}- f_{2} x^{(k+1)}\] (12)within our framework. For instance, if \(f_{1}\) is \(L_{1}\)-smooth and \(_{1}\)-strongly convex, and \(f_{2}\) is \(_{2}\)-strongly convex, then the right-hand side of (12) is a wDG with \((,,)=(L_{1}/2,_{1}/2,_{2}/2)\). In this case, the method is known as the proximal gradient method or the forward-backward splitting algorithm in optimization (cf. Bauschke and Combettes (2017)). Discretizing the accelerated gradient flows allows for obtaining accelerated versions. (Acceleration of the proximal gradient method has been studied for some time (Beck and Teboulle, 2009b,a).)

## 5 Convergence rates of abstract optimization methods

We establish the discrete counterparts of Theorems 2.1 to 2.3 and 2.6. Although the proofs are left to Appendix E, we emphasize that they can be performed analogously to those of the continuous cases. The discrete theorems are established in four cases: the gradient flow (3) (for \(f\) convex and \(\)-strongly convex), and the accelerated flows (for \(f\) convex (5) and \(\)-strongly convex (6)). For ease of understanding, we summarize the results for \(\)-strongly convex cases in Table 1. A similar table for convex cases is included in Appendix A.

_Remark 5.1_.: The following theorems are presented under the assumption that the step size \(h\) is fixed for simplicity. However, if all varying step sizes satisfy the step size condition (with a finite number of violations allowed), the theorems still hold true. The step sizes must be bounded by a positive number from below to ensure the designated rates.)

### For the abstract method based on the gradient flow

The abstract method is given in (11).

**Theorem 5.2** (Convex case).: _Let \(f\) be a convex function. Let \(f\) be a wDG of \(f\), and suppose that \(f\) also satisfies the necessary conditions required by the wDG. Suppose that in the wDG \( 0, 0\). Let \(\{x^{(k)}\}\) be the sequence given by (11). Then, under the step size condition \(h 1/(2)\), the sequence satisfies_

\[fx^{(k)}-f^{}-x^{}\|^{2}}{ 2kh}.\]

Let us demonstrate how to use the theorem using the proximal gradient method (12) as an example. Suppose that \(f_{1}\) is \(L_{1}\)-smooth and convex, and \(f_{2}\) is convex. Then, \(f(y,x)= f_{1}(x)+ f_{2}(y)\) is a wDG with the parameter \((,,)=(L_{1}/2,0,0)\) due to Theorem 4.2 and Lemma 4.4. Therefore,

    & Opt. meth. &  \\ \(f\) & (for (3)) & Numer. meth. & Theorem 5.3 & Theorem 5.5 \\  (i) & steep. des. & exp. Euler & \(1-2^{k}\) & \(1-}^{k}\) \\ (ii) & prox. point & imp. Euler & \(0\) & \(0\) \\ (i)+(ii) & prox. grad. & (splitting) & \(1-2+_{2}}{L_{1}+_{1}+2_{2}} ^{k}\) & \(1-+_{2}}{L_{1}+_{2}}} ^{k}\) \\ (iii) & — & imp. midpoint & \(1-8^{k}\) & \(1-}^{k}\) \\ (iv) & — & AVF (DG) & \(1-6^{k}\) & \(1-}^{k}\) \\ (v) & — & Gonzalez (DG) & \(1-8}{L^{2}+4^{2}}^{k}\) & \(1-}{L^{2}+3^{2}}}^{k} \) \\ (vi) & — & Itoh–Abe (DG) & \(1-2}{4d^{2}L^{2}-^{2}}^{k} \) & \(1-}{4dL^{2}-2^{2}}}^{k} \) \\   

Table 1: Examples of wDGs and their corresponding convergence rates for a \(\)-strongly convex and \(L\)-smooth function \(f\) on \(^{d}\). The numbers in the \(f\) column correspond to the numbers in Theorem 4.2. The line in the figure corresponding to the proximal gradient method is described in the setting of (12). The notation (DG) represents a strict discrete gradient. The convergence rates shown in the table are the best possible for the step sizes chosen in Theorems 5.3 and 5.5.

the proximal gradient method (12) satisfies the assumption of Theorem 5.2 and thus the convergence rate is O\((1/k)\) under the step size condition \(h(1/L_{1})\).

**Theorem 5.3** (Strongly convex case).: _Let \(f\) be a strongly convex function. Let \(f\) be a wDG of \(f\), and suppose that \(f\) also satisfies the necessary conditions required by the wDG. Suppose that in the wDG \(+>0\). Let \(\{x^{(k)}\}\) be the sequence given by (11). Then, under the step size condition \(h 1/(+)\), the sequence satisfies_

\[fx^{(k)}-f^{}(1-)^{k}\|x_{0}-x^{}\|^{2}.\]

_In particular, the sequence satisfies_

\[fx^{(k)}-f^{}(1-)^{k}\|x_{0}-x^{}\|^{2},\]

_when the optimal step size \(h=1/(+)\) is employed._

### For the abstract methods based on the accelerated gradient flows

We consider abstract methods with wDGs based on the accelerated gradient flows (5) and (6), which will be embedded in the theorems below. We note one thing: when using (8) as an approximation of the chain rule, we can determine \(z\) independently of \(x\) and \(y\), which gives us some degrees of freedom (thus allowing for adjustment.) Below we show some choices of \(z^{(k)}\) that are easy to calculate from known values while keeping the decrease of the Lyapunov functional.

**Theorem 5.4** (Convex case).: _Let \(f\) be a convex function. Let \(f\) be a wDG of \(f\), and suppose that \(f\) also satisfies the necessary conditions required by the wDG. Suppose that in the wDG \( 0, 0\). Let \(\{(x^{(k)},v^{(k)})\}\) be the sequence given by_

\[\{ ^{+}x^{(k)}&=A_{k}}{A_{k}}v^{(k+1)}-x^{(k+1)},\\ ^{+}v^{(k)}&=-A_{k}}{4} {}fx^{(k+1)},z^{(k)},\\ -x^{(k)}}{h}&=A_{k}}{A _{k+1}}v^{(k)}-x^{(k)}.\]

_with \(x^{(0)},v^{(0)}=(x_{0},v_{0})\), where \(A_{k}:=A(kh)\). Then if \(A_{k}=(kh)^{2}\) and \(h 1/\), the sequence satisfies_

\[fx^{(k)}-f^{}-x^{}\|^{2}}{A_{k}}.\]

**Theorem 5.5** (Strongly convex case).: _Let \(f\) be a strongly convex function. Let \(f\) be a wDG of \(f\), and suppose that \(f\) also satisfies the necessary conditions required by the wDG. Suppose that in the wDG \(+>0\). Let \(\{(x^{(k)},v^{(k)})\}\) be the sequence given by_

\[\{ ^{+}x^{(k)}&=v^{(k+1)}-x^{(k+1)},\\ ^{+}v^{(k)}&=( {}{+}z^{(k)}+x^{(k+1)}-v^{(k+1)}- fx^{(k+1)},z^{(k)}}{2(+)} ),\\ -x^{(k)}}{h}&= x^{(k)}+v^{(k)}-2z^{(k)}.\]

_with \(x^{(0)},v^{(0)}=(x_{0},v_{0})\). Then if \(h(-)^{-1}\), the sequence satisfies_

\[fx^{(k)}-f^{}1+h^{ -k}f(x_{0})-f^{}+\|v_{0}-x^{}\|^{2}.\]

_In particular, the sequence satisfies_

\[fx^{(k)}-f^{}1-}^{k}f(x_{0})-f^{}+\|v_{0}-x^{}\|^{ 2},\]

_when the optimal step size \(h=\) is employed.__Remark 5.6_.: Time scaling can eliminate the factor \(\) from the scheme and simplify it, as shown in Luo and Chen (2022). However, we do not use time scaling here to match the time scale with the accelerated gradient method and to maintain correspondence with the continuous system.

## 6 Discussions including Limitations

Relation to some other systematic/unified frameworksA systematic approach to obtaining optimization methods with convergence estimates was developed using the "performance estimation problems" (PEPs) technique, as seen in works such as Taylor et al. (2018); Taylor and Drori (2022b). While our framework unifies discussions in both continuous and discrete settings, the design of methods is not automatic and requires finding a new wDG. In contrast, the PEP framework automates method design but separates discussions between the continuous and discrete settings. Combining these two approaches could be a promising research direction, such as applying our framework to the Lyapunov functionals obtained in Taylor and Drori (2022b).

Another unified convergence analysis is presented in Chen and Luo (2021), where the authors cite the same passage in the Introduction from Su et al. (2016). However, this seems to focus on unifying discussions in the continuous setting, and it is still necessary to individualize the discretization for each method in the discrete setting.

In Diakonikolas and Orecchia (2019), a unified method for deriving continuous DEs describing first-order optimization methods was proposed using the "approximate duality gap technique." While this work is capable of finding new DEs, it does not provide insight into how to discretize the DEs for obtaining discrete optimization methods.

Another closely related work is De Sa et al. (2022), which proposed a framework to construct Lyapunov functionals for continuous ODEs. This is strong in view of the fact that generally Lyapunov functionals can be found only in ad hoc ways. Instead, they considered only the simplest gradient descent (and its stochastic version), while the main focus of the present paper lies in the discretizations.

As said in Section 3, in the field of numerical analysis, the use of discrete gradients has been tried. Among them, Ehrhardt et al. (2018) is a pioneering work that comes with several theoretical results. Both this and the present work aim at convex, strongly convex, and the PL functions (in the Appendix, in the present paper). The scope of Ehrhardt et al. (2018) was limited in the sense that they considered only discretizations of gradient flows with strict discrete gradients. Our target ODEs and discretizations are not limited to that, but as its price, our rate is worse in some strict discrete gradient discretizations of gradient flow. This comes from the difference in proof techniques: they proved convergence rates directly and algebraically, while our analysis is via Lyapunov functionals. They also gave several theoretical results besides the convergence analysis, such as the (unique) existence of solutions, and step-size analysis which are important in actual implementations. Whether these two frameworks could be unified would be an interesting future research topic.

Limitations of the proposed framework.Although we believe that the current framework provides an easy environment for working on the DE approach to optimization, it still has some limitations.

First, methods that do not fall into the current framework exist, such as the following splitting method (cf. the Douglas-Rachford splitting method (Eckstein and Bertsekas, 1992)):

\[-x^{(k)}}{h}=-_{1}f_{1}x^{(k+1/2)}, x^{(k)},-x^{(k+1/2)}}{h}=-_{2}f_{2} x^{(k+1)},x^{(k+1/2)}.\]

The right-hand side cannot be written by a single wDG. Additionally, methods based on Runge-Kutta (RK) numerical methods (Zhang et al. (2018); Ushiyama et al. (2022)) appear difficult to be captured by wDG because RK methods cannot be expressed by DG in the first place. Investigating whether these methods can be captured by the concept of DG is an interesting future research topic.

Second, there is still some room for adjustment in wDG methods. A typical example is \(z^{(k)}\) in Section 5.2, which is chosen in the theorems to optimize efficiency and rates. Another example is the adjustment of time-stepping in the last phase of constructing a method to achieve a better rate or practical efficiency. Although these optimizations in the construction of optimization methods are standard in optimization studies, we feel that they are difficult to capture in the current framework, as they fall between the intuitive continuous argument and the discrete wDG arguments that aim to capture common structures.

Third, some rates in the theorems are not optimal. For example, on strongly convex functions, the scheme proposed in Theorem 5.5 with the choice (i) achieves the convergence rate of \((1-)^{k}\), which is not the optimal rate of \((1-)^{2k}\). This is because the choice of the DE and Lyapunov functional used in this work is not optimal. A DE and Lyapunov functional for obtaining the optimal rate are known (Sun et al., 2020), but the DE is a so-called high-resolution DE (known as a "modified equation" in numerical analysis), which involves the Hessian. Whether these DEs can be captured with the wDG perspective is an interesting future research topic.

## 7 Concluding remaks

In this paper, we proposed a new unified discretization framework for the DE approach to convex optimization. Our framework provides an easy environment for those working on the DE approach, and some new methods are immediate from the framework, both as methods and for their convergence estimates. For example, any combination of strict DGs with the accelerated gradient flows are new methods, and their rates are given by the theorems. Although we did not include numerical experiments in the main body owing to the space restrictions, some preliminary numerical tests confirming the theory can be found in Appendix I. These tests show that some new methods can be competitive with state-of-the-art methods, such as Nesterov's accelerated gradient.