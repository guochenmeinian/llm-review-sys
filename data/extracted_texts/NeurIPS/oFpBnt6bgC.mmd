# Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion

Zhengyi Yang  Jiancan Wu  Zhicai Wang  Yancheng Yuan  Xiang Wang  Xiangnan He

Jiancan Wu and Yancheng Yuan are corresponding authors.Xiang Wang and Xiangnan He are also affiliated with Institute of Artificial Intelligence, Institute of Dataspace, Hefei Comprehensive National Science Center.

###### Abstract

Sequential recommendation aims to recommend the next item that matches a user's interest, based on the sequence of items he/she interacted with before. Scrutinizing previous studies, we can summarize a common **learning-to-classify** paradigm -- given a positive item, a recommender model performs negative sampling to add negative items and learns to classify whether the user prefers them or not, based on his/her historical interaction sequence. Although effective, we reveal two inherent limitations: (1) it may differ from human behavior in that a user could imagine an oracle item in mind and select potential items matching the oracle; and (2) the classification is limited in the candidate pool with noisy or easy supervision from negative samples, which dilutes the preference signals towards the oracle item. Yet, generating the oracle item from the historical interaction sequence is mostly unexplored. To bridge the gap, we reshape sequential recommendation as a **learning-to-generate** paradigm, which is achieved via a guided diffusion model, termed **DreamRec**. Specifically, for a sequence of historical items, it applies a Transformer encoder to create guidance representations. Noising target items explores the underlying distribution of item space; then, with the guidance of historical interactions, the denoising process generates an oracle item to recover the positive item, so as to cast off negative sampling and depict the true preference of the user directly. We evaluate the effectiveness of DreamRec through extensive experiments and comparisons with existing methods. Codes and data are open-sourced at https://github.com/YangZhengyi98/DreamRec.

## 1 Introduction

Sequential recommendation has long been a fundamental and important topic in many online platforms, such as e-commerce, streaming media, and social networking [1; 2; 3]. Its core task is to recommend the next item that matches user preference, based on the sequence of items he/she interacted with before. Scrutinizing recent research on sequential recommendation [4; 5; 6; 7; 8; 9], we may discern a common **learning-to-classify** paradigm: given a sequence of historical items and a target (truly positive) item, a recommender model first performs negative sampling to append the historical interactions with some non-interacted (possibly negative) items, and then learns to classify the positive instance from the sampled negatives. Along with this line, extensive studies are conducted on evolving the model architecture (_e.g.,_ recurrent neural networks , convolutional neural networks[10; 11] and Transformers [5; 6]) and auxiliary tasks (_e.g.,_ causal inference [7; 12], contrastive learning [8; 13], robust optimization ), to enhance the classification ability and characterize the user preference better.

Clearly, this learning-to-classify paradigm seeks to predict whether an item in the candidate set (_i.e.,_ positive and sampled negative items) aligns with the user preference evidenced by the historical item sequence. Despite its success, we reveal two limitations inherent in this paradigm:

* It may simplify human behavior -- a user could imagine an **oracle item** in mind, and then compromise to a real item that best matches the oracle. By "imagine", we mean that the oracle item is the one that a user would ideally like to interact with next, softly absorbing and representing his/her preference from previous interactions. By "compromise", we mean that the oracle item, conceivably, is not a concrete and discrete instance limited in the candidate set, while the real item interacted with next exemplifies the oracle. Considering the example in Figure 1, the red triangle is the oracle item ingratiating with the interaction sequence, while the red circle is the real item close to the oracle item and satisfying the user.
* Classification upon the candidate set is a circuitous way to characterize the user preference, to address the one-class problem posed by the availability of only positive samples . Yet, as the selected negatives are confined in a small candidate set (_e.g.,_ the blue circles in Figure 1), their contrasts with the positive sample (_e.g.,_ the red circle in Figure 1) coarsen the decision boundary between what a user likes and dislikes, leaving the item space mostly unexplored. Worse still, simple negative samples may be far away from the positive item, thus failing to contribute sufficient supervision signals to the model learning ; whereas, the overly complex ones might be falsely negative and inject noises into the model [14; 16; 17]. Therefore, such classification signals are hard to control to depict the oracle item.

To resolve these limitations, we reshape sequential recommendation from the perspective of **learning-to-generate** instead. The basic idea is to describe the underlying data-generation distribution based on the historical item sequence, directly generate the oracle item that softly represents the user preference, and infer the real items most matching the oracle, as shown in Figure 1 (_right_ subfigure). Conceptually, generating the oracle item enables us to go beyond the scope of all concrete and discrete items, and encouraging its agreement with the positive items allows us to get rid of negative sampling. Towards this end, we are setting our sights on the diffusion generative models [18; 19; 20; 21; 22]. The key idea of diffusion generative models is to gradually convert the data into noise, and generate samples by the parameterized denoising process. Yet, generating oracle items in sequential recommendation via diffusion models is mostly unexplored.

Figure 1: Illustration of learning-to-classify and learning-to-generate paradigms of sequential recommenders _w.r.t._ the treatment of item embeddings. In the learning-to-classify paradigm, the negative sampling technique is typically employed to classify the positive instance from the sampled negatives (_left_ subfigure); Without negative sampling, all items are naively viewed as positive, undermining recommendation performance (_middle_ subfigure). In contrast, the learning-to-generate paradigm leverages observed interactions to capture the underlying data-generation distribution, enabling it to generate the oracle item beyond the candidate set (_right_ subfigure).

To bridge this gap, we propose a simple yet effective approach **DreamRec**, which uses the guided diffusion idea to directly generate the oracle item tailor-made for the user history. Specifically, given a sequence of historical items, the forward process of DreamRec progressively adds noise to the target item's representation and nearly leads to a complete noise, while the reverse process gradually denoises a Gaussian noise guided by history to generate the oracle item aligned with the historical interactions. Wherein, we establish the history guidance by applying a Transformer encoder on the historical item sequence, so as to make the oracle item specialize for each sequence. As a result, DreamRec enjoys the merit of directly modeling the user preference, without relying on classification and negative sampling. On three benchmark datasets, we evaluate the effectiveness of DreamRec through extensive experiments and comparisons with existing methods [4; 10; 5; 7; 12; 8].

## 2 Related Work

**Sequential recommendation**, which captures user preference and recommends the next items based on the historical interactions, has garnered increasing attention in academia and industry [5; 4; 23]. Most existing studies approach the task under a learning-to-classify paradigm, where the decision boundary separates the positive instance from the sampled negatives. To enhance the classification ability of sequential recommenders, existing efforts can be broadly categorized into two branches. The first branch centers around leveraging complex model architectures, such as recurrent neural networks , convolutional neural networks [10; 11], Transformer encoders [5; 6; 24], to encode the interaction sequences and better characterize user preferences. While the second research line is centered around the inclusion of diverse auxiliary learning tasks, including causal inference , contrastive learning [8; 13], and robust optimization .

**Diffusion Models** have emerged as a prevailing approach for various generative tasks, including image synthesis [25; 18], text generation , and molecule design . Their popularity stems from the ability to precisely approximate underlying data generation distribution and offer more stable training compared to other generative models like GANs [27; 28] and VAEs [29; 30]. Three primary formulations exist in the literature : denoising diffusion probabilistic models (DDPM) , score-based generative models (SGMs) [33; 20], and stochastic differential equations (SDEs) [21; 34]. DDPM represents the forward and reverse process as two Markov chains, leveraging the Markov property to factorize the joint distribution into the product of transition kernels. SGMs introduce a sequence of intensifying Gaussian noise to perturb data, jointly estimating the score function for all noisy data distributions. Samples are generated by chaining the score functions at decreasing noise levels with score-based sampling methods such as Monte Carlo and Langevin Dynamics[35; 20]. SDEs perturb data to noise with a stochastic differential equation , whose forward process can be viewed as the continuous version of DDPM and SGM. Moreover, recent advances [25; 36; 22] have also enabled control over the generation process for conditional diffusion to generate specific images and text.

To our knowledge, recent studies [37; 38; 39; 40] have explored integrating diffusion models into sequential recommendation. However, these approaches still adhere to the learning-to-classify paradigm, inevitably requiring negative sampling during training. For instance, Li et al.  and Du et al.  apply _softmax_ cross-entropy loss  on the predicted logits of candidates, treating all non-target items as negative samples. While Wang et al.  uses binary cross-entropy loss, taking the next item as positive and randomly sampling a non-interacted item as the negative counterpart. They also incorporate contrastive loss for better classification, which requires substantial negative samples. However, diffusion model is mostly used for adding noise in the training samples for robustness, and the learning objectives are largely categorized as classification instead of generation. DiffRec by Wang et al.  proposes to apply diffusion on user's interaction vectors (_i.e.,_ multi-hot vectors) for collaborative recommendation, where "1" indicates a positive interaction and "0" suggests a potential negative.

In contrast, our proposed DreamRec reshapes sequential recommendation as a learning-to-generate task. Specifically, DreamRec directly generates the oracle item tailored to user behavior sequence, transcending limitations of the concrete items in the candidate set and encouraging exploration of the underlying data distribution without the need of negative sampling.

Preliminary

We recap the basic notions of diffusion model, as established by the pioneering DDPM framework . In this paper, we define the lower-case letter in bold as a variable, whose superscript refers to the diffusion step. We keep other notations the same as DDPM .

The fundamental objective of a generative model parameterized by \(\) is to model the underlying data-generation distribution, denoted by \(p_{}(^{0})\), where \(^{0}\) is the target variable. DDPM, a representative formulation of diffusion model, formulates two Markov chains, leveraging the chain rule of probabilities and the Markov property, to model the underlying distribution.

In the _forward (noising) process_, DDPM gradually adds Gaussian noise to \(^{0}\) with a variance schedule \([_{1},_{2},,_{T}]\):

\[q(^{1:T}|^{0})=_{t=1}^{T}q(^{t}| ^{t-1}), q(^{t}|^{t-1})=(^{t}; }^{t-1},_{t}).\] (1)

Let \(_{t}=1-_{t}\), \(_{t}=_{s=1}^{t}_{s}\), \((,)\), we have \(^{t}=_{t}}^{0}+_{t }}\). DDPM jointly models the target variable \(^{0}\) alongside a set of latent variables denoted by \(^{1},,^{T}\) as a Markov chain with Gaussian transitions:

\[p_{}(^{0:T})=p(^{T})_{t=1}^{T}p_{}( ^{t-1}|^{t}), p_{}(^{t-1}|^{t})=(^{t-1};_{}(^{t}, t),_{}(^{t},t)),\] (2)

where the initial state is a Gaussian noise \(^{T}(,)\). This is called the _reverse (denoising) process_ of DDPM.

At the core of the generation task is to optimize the underlying data generating distribution \(p_{}(^{0})\), which is performed by optimizing the variational bound of negative log-likelihood. In DDPM, this equals minimizing the KL divergence between \(q(^{0:T})\) and \(p_{}(^{0:T})\):

\[[- p_{}(^{0})] D_{KL}(q(_{0},_{1},,_{T} )\|p_{}(_{0},_{1},,_{T} ))\] (3) \[= _{q(_{0},_{1},,_{T} )}[- p(_{T})-_{t=1}^{T} (_{t-1}_{t})}{q(_{t} _{t-1})}]+C_{1}\] (4) \[= _{t=1}^{T}(q(^{t-1}| ^{t},^{0})||p_{}(^{t-1}|^{t}) )}_{:=L_{t-1}}+C_{2}\] (5)

where \(C_{1}\) and \(C_{2}\) are constants that are independent of the model parameter \(\). Using Bayes' theorem, the posterior distribution \(q(^{t-1}|^{t},^{0})\) could be solved in closed form:

\[q(^{t-1}|^{t},^{0})=(^{t-1 };}_{t}(^{t},^{0}),_{ t}),\] (6)

where

\[}_{t}(^{t},^{0})=_{t-1}}_{t}}{1-_{t}}^{0}+_{t}}(1-_{t})}{1-_{t}}^{t}_{t}=_{t-1}}{1-_{t}}_{t}.\] (7)

Further reparameterizing \(_{}(^{t},t)\) as:

\[_{}(^{t},t)=_{t}}} (^{t}-}{_{t}}} _{}(^{t},t)),\] (8)

the \(t\)-th term of the training objective in Equation (5) is simplified to:

\[L_{t-1}=_{^{0},}[ ^{2}}{2_{t}_{t}(1-_{t})}||_{0}-_{}(_{t}}^{0}+ _{t}},t)||^{2}]+C.\] (9)

Following DDPM , \(_{}(^{t},t)\) is set to \(_{t}\) to match the variance of Equation (2) and Equation (6). The model architecture of \(_{}\) depends on specific tasks, such as U-Net for image generation  and Transformer for text generation .

Method

In this section, we elaborate on the proposed DreamRec, following the learning-to-generate paradigm with guided diffusion. We start by reformulating sequential recommendation as an oracle item generation task. Subsequently, we explain our approach to directly generate oracle item embeddings, taking inspiration from DDPM . To conclude, we introduce our approach to enable personalized generation using classifier-free guidance.

### Sequential Recommendation as Oracle Item Generation

Let \(\) be the set of discrete items in the dataset. A historical interaction sequence is represented as \(v_{1:n-1}=[v_{1},v_{2},,v_{n-1}]\), and \(v_{n}\) is the subsequent consumed item of the sequence. Let \(=\{[v_{1:n-1},v_{n}]_{m}\}_{m=1}^{||}\) stand for all the sequences within the training data, and \(_{t}=\{[v_{1:n-1}]_{m}\}_{m=1}^{|_{t}|}\) denotes test sequences. Typically, each item \(v\) is initially translated into its corresponding embedding vector \(\). Thus, the interaction sequence can be represented as \(_{1:n-1}=[_{1},_{2},,_{n-1}]\). The fundamental objective of sequential recommendation is to recommend the potential subsequent item that aligns with the user preference.

In this work, we propose DreamRec, a method that reshapes sequential recommendation as a learning-to-generate task. The principle behind it is that users tend to create an "oracle" item in their minds -- an idealized item that they then search for in tangible form from the candidate set, when making a purchase. We assume that these oracle items are drawn from the same underlying distribution that generates observed items. In DreamRec, we model the underlying distribution as \(p_{}(_{n}^{0}|_{1:n-1})\) (_i.e._, \(p_{}(_{n}|_{1:n-1})\)), since the generation of next item \(_{n}^{0}\) is highly related to historical interactions \(_{1:n-1}\) in sequential recommendation. If \(p_{}(_{n}^{0}|_{1:n-1})\) can be precisely learned, it becomes possible to generate the oracle item from the interaction sequence through \(p_{}(|_{1:n-1})\). In DreamRec, we learn \(p_{}(_{n}^{0}|_{1:n-1})\) by employing a guided diffusion model.

### Oracle Item Generation with Guided Diffusion

After framing sequential recommendation as an oracle item generation task, we proceed to introduce the learning and generation phases of DreamRec.

#### 4.2.1 Learning Phase of DreamRec

A straightforward application of DDPM, as outlined in Section 3, cannot sufficiently achieve the oracle item generation objective in sequential recommendation. This is primarily because the denoising process modeled in Equation (2) lacks guidance from historical interactions, resulting in non-personalized generated items. To resolve this, we propose to guide the denoising process with the corresponding historical interaction sequence. Specifically, we first encode the interaction sequence \(_{1:n-1}=[_{1},_{2},,_{n-1}]\) with a Transformer encoder:

\[_{n-1}=(_{1:n-1}).\] (10)

This encoded interaction sequence, \(_{n-1}\), conditions the denoising process as follows:

\[p_{}(_{n}^{t-1}|_{n}^{t},_{n-1})=(_{n}^{t-1};_{}(_{n}^{t},_{n-1},t), _{}(_{n}^{t},_{n-1},t)),\] (11)

where the architecture of \(_{}(_{n}^{t},_{n-1},t)\) is an MLP in DreamRec depicted in Figure 2.

Similar to Equation (1), the forward process is formulated as a Markov chain of Gaussian transitions: \(q(_{n}^{t}|_{n}^{t-1})=(_{n}^{t}; }_{n}^{t-1},_{t})\) with a variance schedule \([_{1},_{2},,_{T}]\). Both the forward and reverse processes of DreamRec are also illustrated in Figure 2.

Figure 2: Framework of DreamRec. To achieve personalized denoising, DreamRec encodes historical interactions \(_{1:n-1}\) to be guidance signal \(_{n-1}\) with a Transformer encoder **T-enc**, subsequently utilizing \(_{n-1}\) to guide the reverse process.

Thereafter, DreamRec improves upon \(L_{t-1}\) of Equation (5) with guidance signal, and achieves the learning objective:

\[L_{t-1}=D_{KL}(q(_{n}^{t-1}|_{n}^{t},_{n-1}^{0})||p _{}(_{n}^{t-1}|_{n}^{t},_{n-1})).\] (12)

We employ another reparameterization that predicts target sample \(_{n}^{0}\) rather than the added noise \(\) as in Equation (8)3:

\[_{}(_{n}^{t},_{n-1},t)=_{ t-1}}f_{}(_{n}^{t},_{n-1},t)+}(1- _{t-1})}{_{t}}},\] (13)

and Equation (12) converts to another format:

\[L_{t-1}=_{_{n}^{0},}[ _{t-1}}{2_{t}}||_{n}^{0}-f_{}(_{ t}}_{n}^{0}+_{t}},_{n-1},t)||^{2} ]+C.\] (14)

Besides the conditional diffusion model, guided diffusion typically necessitates an additional unconditional one , which can be jointly trained using a _classifier-free guidance_ scheme . Specifically, during training, we randomly replace guidance signal \(_{n-1}\) by a dummy token \(\) with probability \(p_{u}\) to achieve the training of unconditional diffusion model.

It is important to note that in Equation (14), \(_{n}^{0}\) denotes the observed target item in interaction sequence, and \(\) denotes the Gaussian noise. Therefore, DreamRec's training procedure does not require negative sampling. Instead, it concentrates on recovering the target item in the interaction sequence. Algorithm 1 shows the details of DreamRec's training phase.

```
1:repeat
2:\(_{n}^{0},_{1:n-1}\)\(\) Sample and embed a data from training set.
3:\(_{n-1}=(_{1:n-1})\)\(\) Encode interaction sequence.
4:\(_{n-1}=(_{1:n-1})\)\(\) Compute the target item with Gaussian noise.
5:\(=- g\|_{n}^{0}-f_{}(_{n}^{t}, _{n-1},t)\|^{2}\)\(\) Take gradient descent step, \(\) is the step size.
6:until converged ```

**Algorithm 1** Training phase of DreamRec

#### 4.2.2 Generation Phase of DreamRec

In the generation phase, we target to generate personalized oracle items for different users, given their historical interactions. To manipulate the influence of the guidance signal \(_{n-1}\), we would modify \(f_{}(_{n}^{t},_{n-1},t)\) to conform to the following format:

\[_{}(_{n}^{t},_{n-1},t)=(1+w)\;f_{}( _{n}^{t},_{n-1},t)-w\;f_{}(_{n}^{t},,t),\] (15)where \(w\) is a hyperparameter controlling the strength of \(_{n-1}\): A higher \(w\) value can enhance personalized guidance, but it could potentially undermine diffusion generalization, consequently deteriorating the quality of the generated oracle item.

Based on Equation (7), the one-step denoising process is straightforward as follows:

\[_{n}^{t-1}=_{t-1}}_{t}}{1-_{ t}}_{}(_{n}^{t},_{n-1},t)+}(1-_{t-1})}{1-_{t}}_{n}^{t}+ _{t}},(, ).\] (16)

During the inference stage, for a user characterized by historical interactions encoded as \(_{n-1}\), the oracle item \(_{n}^{0}\) can be generated by denoising a Gaussian sample \(_{n}^{T}(,)\) for \(T\) steps, in accordance with Equation (16). The generation phase of DreamRec is demonstrated in Algorithm 2.

#### 4.2.3 Retrieval of Recommendation List

After generating the oracle item, the subsequent step involves obtaining the recommendation list tailored to the specific user. To achieve this, we retrieve the K-nearest items to the oracle item within the candidate set using an inner product measurement, forming the recommendaiton list. It's crucial to emphasize that the selection of K-nearest items is exclusive to the retrieval of the recommendation list and does not figure into the training phase. Conceptually, DreamRec transcends the confines of a limited candidate set, venturing into the entire item space in its pursuit of the oracle item.

## 5 Experiments

In this section, we conduct experiments to demonstrate that: 1) DreamRec provides a powerful learning-to-generate framework for sequential recommendation; 2) DreamRec can better explore the item space without negative sampling; and 3) Guiding the diffusion process with historical interactions is important for personalized oracle item generation in DreamRec.

### Experimental Settings

**Datasets.** We use three datasets from real-world sequential recommendation scenarios: YooChoose, KuaiRec, and Zhihu (the statistics of datasets are illustrated in Appendix B):

* **YooChoose** dataset comes from RecSys Challenge 2015 4. We preserve the purchase sequences for a moderate size of data. We only retain items with at least 5 interactions to avoid cold-start issue. Additionally, we exclude sequences that are shorter than 3 interactions in length. * **KuaiRec** dataset is collected from the recommendation logs of a video-sharing mobile app. We also remove items that are interacted with less than 5 times and sequences shorter than 3. * **Zhihu** dataset is collected from a socialized knowledge-sharing community. Users are presented with a recommended Q&A list and they can read their preferred ones. We remove items that are read less than 5 times and sequences that are shorter than 3 in length.

For all datasets, we first sort all sequences in chronological order, and then split the data into training, validation and testing data at the ratio of 8:1:1.

**Baselines.** We compare DreamRec against several competitive models, including GRU4Rec , Caser , SASRec , S-IPS , AdaRanker , CL4SRec  and DiffRec . GRU4Rec, Caser, and SASRec adopt recurrent neural networks, convolutional neural networks, and Transformer encoders respectively to capture sequential patterns of user behaviors. S-IPS and AdaRanker leverage causal inference and neural process to address the issues of selection bias and temporal dynamics in sequential recommendation. CL4SRec designs three data augmentation methods and applies contrastive learning techniques to enhance the classification ability of sequential recommender. DiffRec proposes to incorporate the diffusion model in collaborative filtering, but it applies diffusion on user's interaction vectors -- multi-hot vectors whose element of "1" indicates a positive interaction while "0" suggests a potential negative.

**Training Protocol.** We implement all models with Python 3.7 and PyTorch 1.12.1 in Nvidia GeForce RTX 3090. We preserve the last 10 interactions as historical sequence. For sequences with less than 10 interactions, we would pad them to 10 with a padding token. We leverage AdamW as the optimizer. The embedding dimension of items is fixed as 64 across all models. The learning rate is tuned in the range of [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005]. Despite that DreamRec does not require L2 regularization, we tune the weight of L2 regularization for all baselines in the range of [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]. For all baselines, we conduct negative sampling from the uniform distribution at the ratio of 1: 1, which is not conducted in DreamRec. For our DreamRec, we fix the unconditional training probability \(p_{u}\) as 0.1 suggested by . We search the total diffusion step \(T\) in the range of , and the personalized guidance strength \(w\) in the range of .

Evaluation Protocol.We follow the widely used top-K protocol to evaluate the performance of sequential recommendation and adopt two widely used metrics: hit ratio (HR) and normalized discounted cumulative gain (NDCG) . Sequential recommenders within the learning-to-classify paradigm usually leverage the classification logits on candidate items to select top-K items. For DreamRec, we first generate the oracle item with Algorithm 2, and then we find the K-nearest items in the candidate set for top-K recommendation with the measurement of inner product. Note that the selection of K-nearest items is only conducted at the evaluation phase, and is not involved in the training phase.

### Main Results

In this section, we compare DreamRec against baseline models in terms of top-K recommendation performance. Table 1 shows the experimental results. Overall, DreamRec substantially and consistently outperforms compared models, which demonstrates the superiority of our proposed learn-to-generate paradigm in sequential recommendation.

Note that all baselines are within the learning-to-classify paradigm: GRU4Rec, Caser and SASRec are backbone models; IPS and AdaRanker enhance classification ability by solving data biasing issues; and CL4SRec further equips classification objectives with contrastive learning techniques. We do witness in Table 1 that these auxiliary tasks improve the performance of backbone models.

   &  &  &  \\   & HR@20(\%) & NDCG@20(\%) & HR@20(\%) & NDCG@20(\%) & HR@20(\%) & NDCG@20(\%) \\  GRU4Rec & 3.89\(\)0.11 & 1.62\(\)0.02 & 3.32\(\)0.11 & 1.23\(\)0.08 & 1.78\(\)0.12 & 0.67\(\)0.03 \\ Caser & 4.06\(\)0.12 & 1.88\(\)0.09 & 2.88\(\)0.19 & 1.07\(\)0.07 & 1.57\(\)0.05 & 0.59\(\)0.01 \\ SASRec & 3.68\(\)0.08 & 1.63\(\)0.002 & 3.92\(\)0.18 & 1.53\(\)0.11 & 1.62\(\)0.01 & 0.60\(\)0.03 \\ IPS & 3.81\(\)0.05 & 1.73\(\)0.03 & 3.73\(\)0.03 & 1.40\(\)0.05 & 1.66\(\)0.04 & 0.64\(\)0.02 \\ AdaRanker & 3.74\(\)0.06 & 1.67\(\)0.04 & 4.14\(\)0.09 & 1.89\(\)0.05 & 1.70\(\)0.04 & 0.61\(\)0.02 \\ CL4SRec & 4.45\(\)0.04 & 1.86\(\)0.02 & 4.25\(\)0.10 & 2.01\(\)0.09 & 2.03\(\)0.06 & 0.74\(\)0.03 \\ DiffRec & 4.33\(\)0.02 & 1.84\(\)0.01 & 3.74\(\)0.08 & 1.77\(\)0.05 & 1.82\(\)0.03 & 0.65\(\)0.09 \\  DreamRec & **4.78\(\)**0.06 & **2.23\(\)**0.02 & **5.16\(\)**0.05 & **4.11\(\)**0.02 & **2.26\(\)**0.07 & **0.79\(\)**0.01 \\  

Table 1: Overall performance comparison. The boldface denotes the best performance while the underline indicates the second best. The experiments are conducted 5 times and the average and standard deviation are reported.

Figure 3: Visualization of the learned item embeddings on Zhihu dataset using T-SNE, where red points represent items. Figure 2(a) shows that without negative sampling, the item embeddings of SASRec are crowded in limited discrete zones. Figure 2(b) shows that With negative sampling, the item embeddings of SASRec concentrate on only part of the item space. Figure 2(c) shows that DreamRec explores most of the item space without requiring negative sampling.

In terms of DreamRec, it reshapes sequential recommendation purely as an oracle item generation task without those auxiliary tasks, and it has already achieved better performance in our experiments. We believe designing auxiliary tasks can also boost the generation of oracle items in sequential recommendation, yet we leave this as future work since it is beyond the scope of this paper.

### Visualization

In this section, we visualize the learned item embeddings using T-SNE  to demonstrate that DreamRec can well explore the underlying distribution of item space without negative sampling.

Specifically, we first train three recommenders on Zhihu dataset (results on other datasets included in Appendix C): 1) SASRec without negative sampling; 2) SASRec with negative sampling; and 3) DreamRec that does not require negative sampling. Then we use T-SNE to visualize the learned item embeddings under the default setting of scikit-learn 5. Figure 3 demonstrates the visualization results, where each item is represented as a red point. From Figure 3 we can observe that: 1) Negative sampling is necessary for classification-based sequential recommenders: if no negative sampling is conducted, plenty of items may be crowded pathologically in limited discrete zones of the item space, making them indistinguishable; 2) Negative sampling makes classification easier since items are better shattered, but they are biased to concentrate more on part of the zone of item space; and 3) By reshaping sequential recommendation as a learning-to-generate framework, DreamRec explores most of the zones of item space without requiring negative sampling. These observations provide strong empirical support for our claims made in Section 1.

### Controlling the Personalized Guidance of DreamRec

As introduced in Section 4.2.2, achieving personalized oracle item generation requires guidance from the corresponding interaction sequence. To achieve controllable guidance in DreamRec, we adopt the classifier-free guidance technique. However, we should carefully adjust the value of guidance strength \(w\) in Equation (15), as a higher value may hurt the generalization of diffusion and lead to generating lower-quality oracle items. To shed light on this issue, we conduct experiments on the three datasets by adjusting the value of \(w\). As shown in Figure 4, we observed that increasing the value of \(w\) initially leads to better recommendation accuracy. This supports our intuition that stronger guidance leads to better personalization. However, as we continue to increase the value of \(w\), we observe a decline in performance. This is in line with our analysis that concentrating too much on the guidance signal may hurt the generation quality of oracle items.

## 6 Conclusion and Limitations

We propose DreamRec, reshaping sequential recommendation as a learning-to-generate task, instead of a learning-to-classify task as almost all previous methods do. DreamRec is based on the analysis of user behaviors that, after several interactions with the recommendation system, users tend to fantasize about an oracle item they would "ideally" consume. The oracle item does not have to be the next item in the dataset, and even should not be limited to the pre-defined candidate set. By

Figure 4: Ablation study of classifier-free guidance by adjusting the guidance strength \(w\) on YooChoose (subfigure 3(a)), KuaiRec (subfigure 3(b)), and Zhihu (subfigure 3(c)) datasets.

modeling the underlying data-generation distribution with diffusion model, DreamRec promises to generate unobserved oracle items. Moreover, targeting to model the data-generation process directly, DreamRec makes it possible to discard negative samples in sequential recommendation, which can hardly be achieved by previous classification-based models. Experiments show that DreamRec brings consistent improvements to sequential recommendation, implying its superiority in modeling user behaviors.

Meanwhile, DreamRec also has a few limitations: 1) the sampling process is quite slow with the iteration of total diffusion steps; and 2) the training process is more time-consuming. We believe these can be resolved in further research with more advanced generation models such as consistency model . Moreover, as an initial attempt of reshaping sequential recommendation as an oracle item generation task, DreamRec provides many research opportunities such as designing auxiliary tasks (_e.g.,_ contrastive learning or data augmentation) to enhance oracle item generation. Another research line could be designing other encoders for guidance representation, or other model architectures for the denoising process of DreamRec.

## 7 Broader Impact

The proposed DreamRec can significantly improve the performance of sequential recommendation. Therefore, it can be applied to real-world platforms to provide more satisfying recommendation results. One concern of generating oracle items is the potential for privacy disclosure. Despite that we encode the oracle item with vector representations, one may decode the representation and snoop on users' preference explicitly. Therefore, we kindly advise researchers to be cautious about the usage of generated oracle items.