# Online (Multinomial) Logistic Bandit:

Improved Regret and Constant Computation Cost

 Yu-Jie Zhang\({}^{1}\) Masashi Sugiyama\({}^{2,1}\)

\({}^{1}\) The University of Tokyo, Chiba, Japan

\({}^{2}\) RIKEN AIP, Tokyo, Japan

###### Abstract

This paper investigates the logistic bandit problem, a variant of the generalized linear bandit model that utilizes a logistic model to depict the feedback from an action. While most existing research focuses on the binary logistic bandit problem, the multinomial case, which considers more than two possible feedback values, offers increased practical relevance and adaptability for use in complex decision-making problems such as reinforcement learning. In this paper, we provide an algorithm that enjoys both statistical and computational efficiency for the logistic bandit problem. In the binary case, our method improves the state-of-the-art binary logistic bandit method by reducing the per-round computation cost from \(( T)\) to \((1)\) with respect to the time horizon \(T\), while still preserving the minimax optimal guarantee up to logarithmic factors. In the multinomial case, with \(K+1\) potential feedback values, our algorithm achieves an \(}(K)\) regret bound with \((1)\) computational cost per round. The result not only improves the \(}(K)\) bound for the best-known tractable algorithm--where the large constant \(\) increases exponentially with the diameter of the parameter domain--but also reduces the \((T)\) computational complexity demanded by the previous method.

## 1 Introduction

The stochastic linear bandit (SLB) [1; 2; 3] problem is a natural generalization of the classic stochastic multi-armed bandit problem  by incorporating side information into the decision-making process. In the SLB problem, a linear model is used to characterize the relationship between the reward \(r_{t}\) and the learner's action \(_{t}^{d}\), whereas such an assumption is not always satisfied in real-world applications. Consequently, various models have been developed to account for the non-linear reward, including the generalized linear bandit (GLB) model  and kernelized bandit model . The logistic bandit is a specific kind of GLB model by connecting the learner's \(d\)-dimensional action and the reward with a logistic model. Most existing work focuses on the binary case [7; 8; 9; 10]. The reward \(r_{t}\{0,1\}\) exhibits a binary value and the probability is modeled by \([r_{t}=1_{t}]=(_{*}^{}_ {t})\), where \((z)=1/(1+(-z))\) is a non-linear link function and \(_{*}^{d}\) is an unknown parameter. Compared to the SLB model, the logistic bandit model provides a more precise representation for a wide range of real-world application problems, where feedback exhibits discrete behavior. Moreover, from a theoretical perspective, it also serves as a basic setting for understanding the impact of non-linearity of the reward on the decision-making process. In this paper, we investigate a more general _multinomial logistic bandit_ (MLogB) problem , in which the learner's action \(_{t}\) results in feedback \(y_{t}\) that could have \(K+1\) possible outcome values. The probability of each outcome is characterized with a logistic model (the formal definition is provided in Section 2.1). The MLogB model is of more practical interest compared to the binary case. For example, in the real-world application such as online advertising, there could be multiple possible feedback from customers, including "buy now", "add to cart", "view related item", andjust leave without any click. Beyond addressing practical demands, studying the MLogB problem can shed light on other decision-making problems. For instance, in the theoretical reinforcement learning research [12; 13], the transition matrix is approximated using a linear model, which enables the application of the SLB method for balancing the exploration-exploitation trade-off. Since the transition matrix is inherently a probability matrix, the multinomial logistic model may be a more suitable structure for modeling transition probabilities between states.

As shown in Table 1, the logistic bandit problem has received much attention in the binary case. There are two main focuses arising from the non-linearity of the reward function: _statistical efficiency_ and _resource overhead_. Regarding the statistical efficiency, a main focus is the algorithms' dependence on \(=_{,}1/^{ }(^{})\), a crucial parameter capturing the non-linearity of the reward function. Since the binary logistic bandit is a special case of the generalized linear model, the result from  implies an \(}()\) bound. However, the parameter \(\) grows exponentially in terms of the diameter of the decision domain \(\) and action space \(\), making the linear dependence unfavorable. A pivotal advancement is made by , where the paper presents an algorithm that achieves the nearly minimax optimal bound \(}(})\). Here, \(_{*}=1/^{}(^{}_{*}_{*})\) is the non-linear parameter associated with the best action \(_{*}=_{}(^{}_{ *})\), suggesting that non-linearity might be advantageous for statistical efficiency, rather than a hindrance. Alongside statistical efficiency considerations, the non-linearity of the feedback raises concerns about the algorithms' computation and storage efficiency. The methods [8; 9] with improved dependence on \(\) usually require storing and optimizing over all historical data to estimate the unknown parameter, leading to an \((t)\) computation and storage cost at round \(t[T]\). The pioneering work  provided the first efficient solution for binary logistic bandit with constant computation and storage costs and  further proposed an efficient algorithm for generalized linear bandit, but their regret bounds still exhibit a linear dependence on \(\). Recently, a jointly efficient algorithm was proposed by , which achieves the optimal dependence on \(\) with a \(( t)\) computation cost and constant storage cost per round. However, it remains open whether the minimax optimal bound is achievable with constant computation cost independent of the time \(t\).

Regrading the multinomial logistic bandit, the best-known feasible algorithm was proposed by . This method achieves an \(}(K)\) regret bound, bearing an \(()\) dependence on the exponentially large constant. Moreover, it still demands \((t)\) computation and storage costs to optimize over all past data. The same study also introduced an \(}(K^{3/2})\) bound with improved dependence on \(\). Yet, this solution leans heavily towards theoretical insights and is intractable in implementation [11; Section 2.6]. Designing a practical or more efficient algorithm with improved dependence on \(\) is still an unsolved challenge. More discussions on the related work and topics can be found in Section 4.

**Our Results**. In this paper, we provide an algorithm with both statistical and computational efficiency.

* For the multinomial logistic bandit, we propose OFUL-MLogB, a jointly efficient method attaining an \(}(K)\) regret bound with \((1)\) in \(T\) computation and storage cost per round. The result improves the previous work on the dependence of the large constant \(\).
* For the binary case, our proposed OFUL-MLogB can achieve the \(}(})\) optimal bound up to logarithmic factors. Besides, our method reduces the computation cost of the state-of-the-art binary method  from \(( t)\) to \((1)\) per round.

  
**Setting** & **Algorithm** & **Regret** & **Comput. per Round** & **Storage Cost** & **Improved \(\)** & **Constant Cost** \\   & Logistic-UCB-1  & \(}()\) & \((t)\) & \((t)\) & \(\) & \(\) \\  & OFULog-t  & \(}(})\) & \((t)\) & \((t)\) & \(\) & \(\) \\  & (ada)-OFU-ECOLog  & \(}(})\) & \(( t)\) & \((1)\) & \(\) & \(\) \\  & OL2M ,GLOC  & \(}()\) & \((1)\) & \((1)\) & \(\) & \(\) \\  & OFUL-MLogB (Corollary 1) & \(}(})\) & \((1)\) & \((1)\) & \(\) & \(\) \\   & MNL-UCB  & \(}(K)\) & \((t)\) & \((t)\) & \(\) & \(\) \\  & Improved MNL-UCB  & \(}(K^{3/2})\) & \(\--\) & \((t)\) & \(\) & \(-\) \\   & OFUL-MLogB (Theorem 4) & \(}(K)\) & \((1)\) & \((1)\) & \(\) & \(\) \\   

Table 1: Comparison in terms of the regret bound, computation cost and storage cost. For the regret bound, the logarithmic dependence on the time horizon \(T\) is hidden by the \(}()\)-notation. As for the computational cost (abbreviated as “Comput.”) and storage cost, we only keep the dependence on the time step \(t\). Notation “\(\)” denotes that the algorithm is intractable for implementation.

## 2 Multinomial Logistic Bandit with Improved Regret

This section provides preliminaries on the multinomial logistic bandit problem and optimistic algorithms, beginning with the problem formulation and notations. Then, we revisit the optimistic algorithms for the MLogB problem. Specifically, we investigate the previously best-known feasible algorithm, MNL-UCB algorithm , and propose an improved version with better dependence on the exponentially large constant \(\) and the number of outcome values \(K\).

### Problem Formulation

The multinomial logistic bandit (MLogB) problem studies a \(T\) round decision-making process between the learner and the environment. At the beginning of the iteration \(t\), the learner will first select an action \(_{t}\) from the feasible action set \(^{d}\) and then submit it to the environment. After that, a response \(y_{t}\{0\}[K]\) with \(K+1\) possible outcomes (like "buy now", "add to chart", or "do nothing") is returned based on the learner's choice, where \(K\). Specifically, the MLogB problem assumes that each outcome \(k[K]\) is associated with a ground-truth parameter \(_{*}^{(k)}^{d}\) and the probability of the outcome \([y_{t}=k_{t}]\) follows the logistic model,

\[[y_{t}=k_{t}]=_{*}^{(k)})^{ }_{t})}{1+_{j=1}^{K}((_{*}^{(j)})^ {}_{t})}\ \ \ [y_{t}=0_{t}]=1-_{k=1}^{K}[y_{t}=k _{t}].\]

For notational simplicity, we denote by \(W_{*}=[_{*}^{(1)},,_{*}^{(K)}]^{}^{K  d}\) the matrix for the unknown parameter and define the softmax function \(:^{K}^{K}\) by

\[[()]_{k}=]_{k})}{1+_{ j=1}^{K}([]_{j})}\ k[K]\ \ \ [()]_{0}=^{K}([ ]_{j})},\] (1)

where \([]_{k}\) denotes the \(k\)-th entry of the input vector. Then, the probability of the outcome can also be written in a concise way as \([y_{t}=k_{t}]=[(W_{*}_{t })]_{k}\). Besides, each outcome is associated with a fixed and known reward. We denote by \(_{k}_{+}\) the reward for the outcome \(k[K]\), and let \(_{0}=0\) for the outcome \(y_{t}=0\). Therefore, the expected reward of the learner's action \(_{t}\) is defined as \(r(_{t})=_{k=0}^{K}[y_{t}=k_{t}] _{k}=^{}(W_{*}_{t})\). Let \(_{*}=_{}^{} (W_{*})\). The goal of the learner is to maximize the cumulative reward, which is equivalent to minimizing the regret

\[_{T}=_{t=1}^{T}^{}(W_{*}_{*})-(W_{*}_{t}),\] (2)

When \(K=1\) and \(_{1}=1\), MLogB recovers the binary logistic bandit by \(r()=(_{*}^{})=1/(1+(-_{*} ^{}))\), where \(_{*}^{d}\) is a unknown parameter.

**Exponentially Large Constant \(\).** In the logistic bandit problem, the non-linearity of the reward function is captured by the gradient of the link function \(:^{d}( ())-()()^{}\). The analysis typically that requires that the gradient term is bounded from below and thus one would define the constant \( 1/_{W,}_{}( (W))\) such that \(I_{d}(W)\) for any \(W\) and \(\), where \(_{}:^{K K}^{K}\) is the minimum eigenvalue of the input matrix. In the binary case (\(K=1\)), one can show that \(=_{,}\{1+(^{})+(-^{})\}=(e^{SX})\), where \(S\) and \(X\) are the diameters of the parameter space \(\) and action space \(\). In the multinomial case, the paper [11, Section 3] also shows that \(\) is an exponentially large constant with respect to \(S\) and \(X\). Thus, an algorithm with improved dependence on \(\) is demanded.

### Assumptions and Notations

Same as the previous work for multinomial logistic bandit , we use the following assumptions.

**Assumption 1**.: The norm of the action is bounded by 1, i.e., \(\|\|_{2} 1\) for any \(\).

**Assumption 2**.: The reward vector \(_{+}^{K}\) and its norm is bounded by \(R\), i.e., \(\|\|_{2} R\).

**Assumption 3**.: The norm of the parameter \(W_{*}^{K d}\) is bounded by \(S\), i.e., \(\|W_{*}\|_{} S\), where \(\|\|_{}\) denotes the Frobenius norm of a matrix.

**Assumption 4**.: Let \(():^{d}(())-()()^{}\). For all \(\) and \(W\), we have \(_{}((W)) 1/\) and \(_{}((W)) L\), where \(_{}:^{K K}^{K}\) and \(_{}:^{K K}^{K}\) take the maximum and minimum eigenvalues of the input, respectively.

**Other Notations.** The following notations are used in the paper. Given a \(K\)-by-\(d\) matrix \(W\), we denote by \(\) its \(Kd\)-dimensional vectorization. For any positive semi-definite \(H^{Kd Kd}\), we define the norm \(\|\|_{H}=,H }\). The notation \(\) is used for the standard Kronecker product. When the input is a vector \(^{Kd}\), we treat it as a \(Kd 1\) matrix. Moreover, for any symmetric matrix \(A,B^{Kd Kd}\), we denote by \(A B\) that \(A-B\) is a semi-positive definite matrix. We use \(_{t}=(_{1},y_{1},,_{t-1},_{t})\) to denote the filtration, which encodes the information collected so far before receiving \(y_{t}\). Finally, \(()\) is used to highlight the dependence on \(d\), \(K\), \(\), and \(T\). With \(}()\)-notation, we further hide the dependence on the dimension \(d\) and logarithmic factors.

### Optimistic Algorithm with Improved Bound

This part revisit the principle of optimism in the face of uncertainty (OFU)  and introduces an improved version of the MNL-UCB algorithm  with better dependence on \(\) and \(K\).

**Optimism in the Face of Uncertainty.** The OFU principle is a fundamental paradigm for addressing the exploration-exploitation dilemma in bandits. At each iteration \(t\), the algorithm selects the arm by the rule \(_{t}=_{}_{t}()\), where \(_{t}()\) is an optimistic estimate of the true reward \(r()\) satisfying \(_{t}() r()\) for all \(\). Based on the OFU rule, one can show that \(_{T}_{t=1}^{T}((_{t})-r(_{t}))\), indicating that a tighter optimistic estimate \(_{t}\) will lead to a tighter regret bound. Therefore, designing a tight optimistic estimate \(_{t}\) is essential for the OFU algorithm.

In the context of the logistic bandit, a common practice is to construct a confidence set \(_{t}^{K d}\), which is supposed to contain the true parameter \(W_{*}\) with high probability. As such, the learner can obtain the optimistic reward for each arm \(\) by \(_{t}()=_{W_{t}}^{} (W)\). A tighter confidence set will lead to a tighter optimistic reward, and thus resulting in a better regret bound.

**Improved Concentration Set.** Given the reward is generated by the multinomial logistic model, we can employ the maximum likelihood estimation (MLE) method to learn the unknown parameter \(W_{*}\). Specifically, after observing the action-feedback pairs \(\{(_{s},y_{s})\}_{s=1}^{t-1}\), one can train the model by

\[W_{t}^{}=*{arg\,min}_{W^{K d}} _{t}(W)_{s=1}^{t-1}_{s}(W)+\|W \|_{}^{2},\] (3)

where \(_{t}(W)=_{k=0}^{K}\{y_{t}=k\}(1/[( W_{t})]_{k})\) is the multiclass logistic loss established over \((_{t},y_{t})\) and \(>0\) is the regularization parameter. Let \(^{Kd}\) be the vectorized parameter. We also define the gradient \(_{t}()\) and the Fisher information matrix \(H_{t}(W)\) of the logistic loss by

\[_{t}()_{t}( ) H_{t}(W) I+_{s=1}^{t-1}(W _{s})_{s}_{s}^{},\]

where \(^{Kd}\) is the vectorized parameter and \(:(())- ()()^{}\) is the first order derivative of the reward vector \(()\). Then, we are ready to present our confidence set for the maximum likelihood estimator, which exhibits \(()\) improvement over that in .

**Theorem 1**.: _Set the parameter \(=(dK(t/))\) with a certain \((0,1]\). For each iteration \(t[T]\), we define the confidence set as_

\[_{t}()\{W\;|\;\| _{t}()-_{t}(_{t}^{ })\|_{H_{t}^{-1}(W)}_{t}()\},\] (4)

_where \(_{t}()=4= ()\) is the radius of the set and \(=\{W^{K d}\;|\;\|W\|_{} S\}\). Then, we have \( t 1,W_{*}_{t}() 1-\)._

One advantage of the confidence set (4) is that its radius \(_{t}()\) is independent of the exponentially large constant \(\) and thus is much tighter than the confident set constructed for the generalizedlinear bandit , whose radius exhibits a linear dependence on \(\). The \(\)-independent set are first established by  for the binary logistic bandit problem and then adapted to multinomial setting by  with refined analysis on the self-normalized martingale tail-inequality for multinomial noise. Our confidence set is tighter than that in  by improving the radius from \(_{t}()=(K)\) to \(_{t}()=()\). The improvement is based on a slightly refined self-normalized tail-inequality for the multinomial case, whose formal description is provided in Appendix C.1.

**Construction of Optimistic Reward.** Based on the confidence set (4), we can construct the optimistic reward and select the arm for each iteration by

\[_{t}()=_{W_{t}()} {}^{}(W)_{t}=_{} _{t}().\] (5)

We have following guarantee for the algorithm based on the MLE (3) and the selection rule specified in (4) and (5). We summarize the algorithmic procedure in Algorithm 1.

**Theorem 2**.: _Under the same conditions as Theorem 1. Let \((0,1]\). Algorithm 1 ensures_

\[_{T}(\{d T,dK  T+ d^{2}K^{2}T\})\]

_with probability at least \(1-\) when we set the parameter \(=(dK(T/))\)._

**Remark 1** (Improved dependence on \(K\) and \(\)).: Our method achieves the \(}()\) and \(}(K)\) regret bounds for \(\) problem simultaneously. The first bound slightly improves the \(}(K)\) guarantee of the MNL-UCB algorithm  by an \(()\) factor while the second one is independent of the exponentially large constant \(\) in its leading term. An \(}(K^{})\) bound is also attained by . However, their proposed method is intractable as its confidence set is established on all minimal elements of partially Loewner-ordered set \(C_{t}()\)[11, Appendix D]. The computation cost of identifying all minimal elements and projecting onto the proposed confidence set is prohibitive. Our solution is free from such demands by using a different rule to construct the optimistic reward.

```
0: regularization coefficient \(\), probability \(\).
1: Initialize \(H_{1}= I_{Kd}\) and \(_{1}^{}\) as any point in \(\)
2:for\(t=1,,T\)do
3: Construct \(_{t}\) and select the arm by (5). Then, the learner receives \(y_{t}\).
4: Train the estimator \(W_{t+1}^{}\) by (3) and construct the confidence set \(}_{t+1}()\) as (4).
5:endfor ```

**Algorithm 1** MNL-UCB+

## 3 Jointly Efficient Algorithm

In this section, we introduce OFUL-MLogB, an algorithm with jointly computational and statistical efficiency for the \(\) problem. We will first discuss the efficiency concern of the existing methods in the literature and then introduce our algorithm, followed by a technical highlight.

### Efficiency Concerns

The algorithms for logistic bandit crucially rely on two components to ensure the statistical efficiency: the MLE (3) and the optimistic rule (5) for constructing \(_{t}\). However, the implementation of both components could be inefficient by requiring \((t)\) computation cost per online iteration.

**Computation and Storage Cost of Maximum Likelihood Estimation.** For logistic bandit or even the generalized linear bandit problem [3; 8; 9; 11], MLE is a widely used tool to learn the unknown parameter. To solve the optimization problem, the gradient-based method, e.g. the projected gradient descent , are usually applied. However, as discussed in , the optimization of the MLE problem typically requires \((t(1/))\) gradient step to achieve \(\)-accuracy. Besides, the loss function \(_{t}\) is established on all historical data \(\{(_{s},y_{s})\}_{s=1}^{t-1}\), resulting in an \((t)\) gradient query complexity for each gradient step and \((t)\) storage cost, and thus is inefficient.

**Computation and Storage Cost of Optimistic Reward Construction.** The construction of the optimistic reward \(_{t}()\) requires to solve the optimization problem (5). However, the objective function \(^{}(W)\) is non-concave and the decision domain \(_{t}()\) is non-convex, making the maximization problem \(_{t}()=_{W_{t}()}^ {}(W)\) computationally challenging. In the binary case, the paper  proposed a convex relaxation of the confidence set \(_{t}()\), whereas the relaxed confidence set is still established on all historical data, resulting in \((t)\) time complexity and \((t)\) storage cost at iteration \(t\). The optimistic estimate construction in the previous work for multinomial logisticti bandits  is computationally efficient without involving any optimization problem solving. However, it will lead to an inferior regret bound of \(}()\).

In the binary case, the work  proposed a jointly efficient algorithm, which achieved a nearly minimax optimal bound with \(( t)\) computation cost per iteration and \((1)\) storage cost. However, as we will discuss in Section 3.3, it is hard to apply their analysis to the multinomial case.

### Efficient Algorithm

In this section, we proposed a novel algorithm which only requires \((1)\) computation cost per iteration and \((1)\) storage cost. The algorithm can achieve the best known results both for binary and multinomial logistic bandits. We have introduced new ingredients both on the algorithm design and regret analysis to achieve the jointly efficient algorithm.

**Efficient Online Estimation.** Instead of performing MLE, we run an online mirror descent algorithm to estimate parameter:

\[_{t+1}^{}}=*{arg\,min}_{W }_{t}(_{t}^{}}), +\|- {W}_{t}^{}}\|_{_{t}}^{2},\  t 1\] (6)

where \(>0\) is the step size to be specified later and the first iteration model \(_{1}^{}}\) can be initialized as any point in the domain \(=\{^{Kd}\| \|_{2} S\}\). We set the matrix as \(_{t}=H_{t}+(W_{t}^{}} _{t})_{t}_{t}^{}\), where \(H_{t}= I+_{s=1}^{t-1}(W_{s+1}^{}} _{s})_{s}_{s}^{}\). Both \(_{t}\) and \(H_{t}\) can be updated incrementally.

We show the online estimator (6) enjoys computational, storage, and statistical efficiency. Since (6) exhibits a standard online mirror descent formulation , it can be solved with a single projected gradient step with the following equivalent formulation by

\[_{t+1}=_{t}^{}}- _{t}^{-1}_{t}(_{t}^{}} )_{t+1}^{}}=_{ }\|-_{t+1} \|_{_{t}}.\]

For the gradient descent step above, the most time-consuming operation is maintaining the inverse of the matrix \(_{t}\). Since \((W_{t}^{}}_{t})_ {t}_{t}^{}\) is a rank-\(K\) matrix, it can be calculated by the Sherman-Morrison-Woodbury formula with \((d^{2}K^{3})\) cost per round. As for the projection step, since \(_{t}\) is positive semi-definite matrix, it can be solved in \((K^{3}d^{3})\)[18, Section 4].* As a consequence, our algorithm achieves a light update with \((1)\) cost per round. Regarding storage cost, our proposed estimator eliminates the need to store all historical data and updates in a one-pass fashion, requiring only \((1)\) storage cost throughout the learning process. Moreover, the estimator is also statistically efficient. We can construct the following \(\)-independent confidence set similar to that in Theorem 1.

**Theorem 3**.: _Let \((0,1]\) and \(=2(1+S)+(K+1)\). Set the parameter \(=/2\) and \(=\{28Kd,7 S\}\). For each iteration \(t[T]\), we define the confidence set as_

\[_{t}^{}()\{W\; |\;|\|_{t}^{}-\|_{H_{t} }_{t}^{}()\},\] (7)

_where \(_{t}^{}()= K t\). Then, we have \( t 1,W_{*}_{t}() 1-\). Moreover, the computation cost of solving (6) is \((1)\) per round._

**Remark 2** (Comparison with the online estimator ).: Our algorithm design is inspired by the online estimator  developed for the binary case, while achieving even lighter cost by novel algorithm ingredients and analysis. As discussed in Section 3.3, the analysis for the binary setting is hard to be applied to the multinomial case. Specifically, the paper  has proposed an intermediary decision \(_{t}\) in the analysis to prove the statistical efficiency of the proposed estimator. However, the favorable property of the intermediate decision only holds in the binary case. To this end, we have proposed a new intermediary decisions, which not only help to prove the statistically efficient of our estimator but also eliminate the requirement of the exploration step. Besides, we have also introduced a novel algorithm ingredient to further speed-up the algorithm. Instead of learning with original loss function as in , by a more refined exploitation of the negative term in the analysis, we show it is sufficient to learn with the first order approximate \(_{t}(_{t}^{}),\) of \(_{t}(W)\) with the adjusted local norm \(\|\|_{_{t}}\). Our new algorithm not only enjoys a computation efficiency improvement from \(( t)\) in  to \((1)\), but also is free from any exploration step required by the previous work. We provide a technical highlight in Section 3.3.

**Efficient Optimistic Reward Construction.** Although the confidence set \(_{t}^{}()\) is convex, the optimistic rule (5) by \(_{t}()=_{W_{t}^{}( )}^{}(W)\) still involves inefficient non-concave optimization problem solving. In this part, we propose a novel optimistic reward that can be solved in a constant time per round.

**Proposition 1**.: _For any \(\) and iteration \(t[T]\), the optimistic reward is constructed by_

\[_{t}^{}()=^{} (W_{t}^{})+_{t}^{}()+_{t}^{}().\] (8)

_In above, \(_{t}^{}()=_{t}^{}() \|H_{t}^{-}(I_{K})(W_{t} ^{})\|_{2}\) and \(_{t}^{}()=3R(_{t}^{}) ^{2}\|(I_{K}^{})H_{t}^{-1/2}\|_{2}^{2}\) are the bonus. Then, we have \(_{t}^{}()^{} (W_{*})\) for all \(t 1\) and \(\) with probability at least \(1-\)._

Proposition 1 constructs the optimistic reward by adding the "bonus" to the reward empirically estimated by \(W_{t}^{}\). Different from the term used in , our bonus terms are independent of the exponentially large constant and thus can lead to an improved \(}(K)\) bound. The optimistic rule (8) does not involve any optimization problem solving and can be calculated in an \((1)\) cost.

Overall Algorithm and Guarantees.We overall procedures in Algorithm 2. For the general multinomial setting, it ensures an \(}(K)\) regret guarantee and an \((1)\) computation cost.

**Theorem 4**.: _Under the same condition as Theorem 3, Algorithm 2 ensures_

\[_{T}=(Kd K( T)^{} + K^{}d^{2}( K)^{2}( T)^{3})= {}(K).\]

_The computation cost of Algorithm 2 is bounded by \((1)\) for each round \(t[T]\)._

**Remark 3** (On the \(}(})\) bound).: In the binary setting,  show that an \(}(})\) minimax optimal is achievable with \(_{*}=1/^{}(_{*}^{}_{*})\). However, due to the multinomial behavior of the feedback, it is unclear how to achieve such a rate in MLogB case (see the discussion in Appendix C.5). Besides, it also raises concerns about efficiency when applying the method developed for binary case to multinomial setting. In particular, the \(}(})\) is achieved by the rule \(_{,_{t}()} (^{})\) in . The optimization can be efficiently solved in the binary case since one can simply eliminate the non-linearity of the reward function by the relationship \((z_{1})>(z_{2})\) for any \(z_{1}>z_{2}\). Such a condition does not hold in MLogB problem.

When reduced to the binary case \(K=1\), our algorithm can also achieves the minimax regret bound.

**Corollary 1**.: _When \(K=1\), the multinomial logistic bandit reduces to the binary logistic bandit problem. Then, under the same conditions as Theorem 4, Algorithm 2 with the optimistic rule \(_{t}()=_{_{t}^{}()}^{}\) ensures \(_{T}}(})\) with probability at least \(1-\)._

### Analysis

This section presents the proof sketch for Theorem 3, which plays an key role in the analysis for Algorithm 2. For notational simplicity, we will drop the superscript \(\) in this section.

**Leveraging Negative Terms for Efficient Update.** If we update the model with the original loss function by \(_{t+1}=_{W}_{t}(W)+ \|-_{t}\|_{H_{t}}^{2}\), the arguments in  shows that the estimation error between \(_{t+1}\) and \(_{*}\) can be bounded by their gap on the loss function

\[\|_{t+1}-_{*}\|_{H_{t+1}}^{2} _{s=1}^{t}_{s}(W_{*})-_{s=1}^{t}_{s}(W_{s+1}).\] (9)

However, the update rule with original loss will lead to an \(( t)\) computation cost per iteration. To facilitate a more efficient algorithm, we introduce the update rule with the linearized loss \(_{t}(_{t}),\) and the adjusted norm \(_{t}\) as (6). As we have shown in the proof of Lemma 12 in Appendix C.1. Denote by \(_{t}(W)=_{t}(_{t}), +\|-_{t}\|_{^{2}_{t}(_{t})}^{2}\) the second-order surrogate of \(_{t}(W)\). The efficient update rule will introduce an additional term

\[_{s=1}^{t}_{s}(_{s+1})- _{s}(_{s+1}),_{s+1}- _{*}.\]

We handle the additional term by the self-concordant property of the logistic loss and exploiting a negative term ignored in the previous analysis. Since the logistic loss is a \(\)-self-concordant-like function [19, Lemma 4], then Theorem 3 of  indicates that the additional term can be bounded by

\[_{s=1}^{t}_{t}(_{s+1})- _{s}(_{s+1}),_{s+1}- _{*}_{s=1}^{t}S\|_{s+1}-_{s}\|_{^{2}_{s}(_{s})}^{2},\]

where \(_{s}^{Kd}\) is on the line connecting \(_{s+1}\) and \(_{s}\). Besides, by a refined analysis of the online mirror descent (OMD) update (6), we identify an additional _negative term_\(-_{s=1}^{t}\|_{s+1}-_{s}\|_{H_{s}}^{2}\) on the right hand side of (9). By properly choosing the coefficient \(\), one can cancel the additional term by the negative term and achieves (9) with the efficient update. We note that the negative term in the OMD analysis is also found crucial in the gradient-variation regret of non-stationary online learning  as well as its applications to game theory  and the SEA model .

**Novel construction of the intermediary prediction.** Then, we can further bound the right hand side of (9) by inserting an intermediary loss \(_{s}(_{s})\) as

\[_{s=1}^{t}_{s}(W_{*})-_{s=1}^{t}_{s}(W_{s+1})=^{t}_{s}(W_{*})-_{s=1}^{t}_{s}(_{s})}_{ \ (k)}+^{t}_{s}(_{s})-_{s=1}^{t} _{s}(W_{s+1})}_{\ (k)}.\]

In the binary case, inspired by the study  for the binary online logistic regression,  propose to construct \(}_{s}=_{_{s}}_{}(^{}_{s},+1)+_{}(^{} _{s},0)+\|-_{s}\|_{H_{s}}^{2}\), where \(_{}\) is the binary logistic loss and \(_{s}\) is a branch decision domain. Then, both term (A) and term (B) can be bounded without the dependence on \(\). However, the analysis for term (B) crucially relies on the condition \(|(}_{s}^{}_{s})-y_{s}||( }_{s}^{}_{s})-1+y_{s}|=^{}( }_{s}^{}_{s})\) to eliminate the dependence on \(\), where \(y_{s}\{0,1\}\) is the one-dimensional feedback. It is hard to show such an relationship in the multinomial case since the feedback \(y_{s}\) has multiple value. A similar challenge is also observed in the recent study on online multiclass logistic regression [25, Appendix F]. One might consider whether it is possible to construct \(_{t}\) with the update rule developed in . However, since the online update rule of  requires to perform over \(^{K d}\), the learned parameter would become unbounded, which makes it is hard to provide an upper bound for term (A).

To this end, we design a new intermediary term by \(_{}(}_{s},y_{s})\), where \(_{}\) is the multiclass logistic loss. The prediction is constructed by \(}_{s}=^{+}(_{W P_{s}}[ (W_{s})])\) with the Gaussian distribution \(P_{s}=(_{s}, H_{s}^{-1})\), where \(^{+}\) is a pseudo inverse function of the sigmoid function \(\). Suchan integral construction \(}_{s}=^{+}(_{W P_{s}}[ (W_{s})])\) is previously used in the online logistic regression literature [26; 27] but we tailored the tool to our analysis with a different construction of the distribution \(P_{s}\). Lemma 13 and Lemma 14 in Appendix C.1 shows that

\[()( K+ t) t\ \ \ \ \ ()_{s=1}^{t} _{s}-_{s+1}_{H_{s}}^{2}+Kd K  t,\]

Then, combining the upper bound of term (A) and term (B) and eliminating the additional term \(_{s=1}^{t}_{s}-_{s+1}_{H_ {s}}^{2}\) with the negative term obtained by a refined analysis of online mirror descent, we complete the proof of Theorem 3.

## 4 More Discussions and Related Work

This section begins with a discussion on the tightness of the proposed bounds.

**On the Tightness of Our Bounds.** In this paper, we introduced OFUL-MLogB, a jointly efficient algorithm that simultaneously achieves regret bounds of \(}()\) and \(}(K)\).+ The tightness of these bounds, with respect to \(\) and \(T\), was detailed in Remark 3. Regarding the number of feedback values \(K\),  claimed the optimality of a linear dependence on \(K\). However, our findings revealed a nuanced interplay between \(\) and \(K\). The \(}()\) bound does not conflict with the lower bound argument by , given that the non-linear constant \(\) is also associated with \(K\). Beyond \(\), the norm of the unknown parameter \(S\) and maximum norm \(R\) of the reward vector \(\) can also depend on \(K\) based on the problem's specifics. It is an interesting direction to understand the interrelation of these constants by establishing a lower bound. Additionally, our method has a linear dependence on \(d\). In the finite-arm case, an \(()\) dependence might be attainable with a SupLin-type algorithm .

Footnote †: Theorem 5 in Appendix C.3.2 shows that OFUL-MLogB also attains the \(}()\) regret bound.

Below, we introduce more related works on logistic bandit and the related topics.

**Logistic Bandit.** While the logistic bandit is a specific instance of the generalized linear bandit model [5; 29; 30; 31; 32; 33; 34], the algorithms proposed for GLB tend to exhibit a linear dependence on the nonlinear term \(\), which is exponentially large in the logistic bandit case. Therefore, addressing the non-linearity of the reward function warrants specialized consideration. Besides the UCB-type algorithms [8; 9] mentioned in Section 1, for the \(N\)-arm case,  proposed an experimental design-based algorithm providing an \(}(})\) regret bound with better dependence on \(d\). However, the previous methods were built upon the MLE estimator, whose optimization demands \((t)\) computation and storage complexity for the \(t\)-th iteration. To the best of our knowledge, the only known jointly computational and statistical efficient algorithm was proposed by , which achieves a nearly minimax optimal regret bound with computation cost of \(( t)\) per round. In addition to the frequentist bounds, there are also researches on logistic bandit from the Bayesian view.  showed the Bayesian regret of the Thompson sampling method is independent of \(\) (even in the lower order term) when the feasible domain is identical to the parameter domain, i.e., \(=\).  further proved the \(\)-independent bound with weaker conditions.

**Multinomial Logit (MNL) Bandit.** Another relevant line of research is the multinomial logit contextual bandit problem [38; 39; 40; 41; 42], which generalizes the binary logistic bandit by allowing the learner to submit a subset of arms \(S_{t}=\{_{t,i}\}_{i=1}^{K}\) to the users. The expected reward function is also modeled by the multinomial logit model: \([r_{t}|S_{t}]=_{_{t,i} S_{t}}_{t,i}(_{*}^{+}_{t,i})/(1+_{_{t,i} S_{t}}(_{*}^{}_{t,i}))\), where \(_{t,i}\) is the reward for arm \(_{t,i}\) and \(_{*}^{d}\) is an unknown parameter. There are also studies on the MNL bandit problem concerning the exponentially large constant \(\).  proposed an optimistic algorithm with \((d)\) regret bounds without \(\) in its leading term, which improves the \((d)\) bound with better dependence on \(\). Considering uniform reward, i.e., \(_{t,i}=1\) for all \(t[T]\) and \(i[N]\),  further showed an \(}(d})\) bound. To the best of our knowledge, all the existing methods with improved \(\) are established on the MLE estimator. It would be an interesting future direction to develop jointly efficient algorithm for the MNL bandit problem.

## 5 Experiments

This section validates the statistical and computation efficiency of the proposed method by experiments. We conduct a bandit learning for \(T=3000\) iterations. In each experiment, we run experiments on 6 random configurations, in which the arm set and the underlying parameter are randomly sampled. Specially, \(||=20\) actions are randomly sampled from a 2-dimensional sphere of radius 1. In the binary case, the norm of the unknown parameter \(_{*}\) is set as \(S=5\). In the multinomial case, we set \(K=4\) with \(S=1\). The reward vector is set as \(=[0.25,0.5,0.75,1]\). For each configuration, we report the averaged results over 10 trials. More details can be found in Appendix D.

**Experimental Results.** Figure 4 provides a comparison of performance and computation costs in the binary case. The algorithm O2LM, which has a constant computational cost per iteration, is used as a comparison baseline. Our algorithm demonstrates a time complexity akin to O2LM, affirming its computational efficiency. Compared to the state-of-the-art binary logistic bandit algorithm ada-OFU-ECOLOG, our OFU-MLogB method has lighter computational overheads while preserving similar empirical performance. Figure 4 illustrates the comparison in the multinomial setting. Our algorithm is around 50 times faster than MNL-UCB for running \(T=3000\) iterations and achieves better empirical performance. More experimental results on other configurations and the running time curve that increases along the iterations can be found in Appendix D.

## 6 Conclusion

This paper proposed a jointly efficient algorithm OFUL-MLogB for both binary and multinomial logistic bandit problems with constant computation cost per round and improved regret guarantees. For the multinomial setting, our method improves over the best-known feasible algorithm both on the dependence of \(\) and the computation cost. When reduced to the binary case, OFUL-MLogB also contributes to improve the computation cost of previous method from \(( t)\) to \((1)\) per round while still preserving the \(}(})\) minimax optimal bound up to logarithmic factors. A promising future direction is to consider the multinomial logit model in reinforcement learning. Besides, it is still open on how to achieve the \(}(})\) bound for the multinomial case.