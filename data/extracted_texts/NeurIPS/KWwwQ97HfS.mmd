# Transition Path Sampling with

Boltzmann Generator-based MCMC Moves

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Sampling all possible transition paths between two 3D states of a molecular system has various applications ranging from catalyst design to drug discovery. Current approaches to sample transition paths use Markov chain Monte Carlo and rely on time-intensive molecular dynamics simulations to find new paths. Our approach operates in the latent space of a normalizing flow that maps from the molecule's Boltzmann distribution to a Gaussian, where we propose new paths without requiring molecular simulations. Using alanine dipeptide, we explore Metropolis-Hastings acceptance criteria in the latent space for exact sampling and investigate different latent proposal mechanisms.

## 1 Introduction

Sampling the trajectories in which a molecular system changes from one 3D configuration to another--a task known as transition path sampling (TPS)--has many applications, such as designing catalysts (Crehuet and Field, 2007), materials (Selli et al., 2016), or drug discovery (Kirmizaltin et al., 2012, 2015). In fact, the transition path ensemble is the ideal description of a chemical reaction's mechanism. We explore how this problem can be solved using a Boltzmann generator (a normalizing flow trained to sample a molecule's Boltzmann distribution) (Noe et al., 2019) and its latent space to obtain or approximate the ensemble.

In the TPS problem, we are given a single molecular system and two 3D conformations of interest for it: states A and B, as seen in Figure 1. These could be the structure of reactants before a reaction and the structure of the product molecule after the reaction. With this, we aim to sample the transition paths between them with the likelihood at which they occur. To describe a transition path, we use a sequence of time-equidistant 3D atom configurations (i.e., frames) that starts in state A and ends in state B.

Existing approaches for this problem (Dellago et al., 1998, 2002; Bolhuis et al., 2002) use Markov chain Monte Carlo (MCMC) sampling to iteratively sample a new path given the current one. New paths are commonly proposed using shooting moves that require molecular dynamics simulation. Given a path, the proposal is generated by first randomly selecting a frame of the path and sampling a random velocity from a Gaussian. The selected frame with the new velocity is then simulated forward and backward in time. If the backward simulation reaches state A and the forward simulation ends in state B, this trajectory constitutes a new non-zero probability transition path, which is accepted or

Figure 1: Distribution of alanine dipeptide’s 3D configurations visualized via a histogram of its main dihedral angles \(,\). Two metastable states are highlighted, between which we aim to sample the ensemble of all possible transition paths.

rejected based on its probability and a Metropolis-Hastings (Metropolis et al., 1953; Hastings, 1970) acceptance criterion. All paths that do not transition between A and B will be rejected. Repeating this is guaranteed to eventually produce the exact transition path ensemble, but convergence is slow since many proposals will not fulfill the constraints, paths are correlated, and finding transitions requires expensive simulation.

In this work, we explore how the TPS problem can be addressed when having access to a trained Boltzmann generator, which solves the easier problem of sampling the molecular systems distribution of 3D conformers. Given this, we generate MCMC proposals by first moving every frame in a path into our latent space. We modify each frame of this path by adding independent Gaussian noise such that the overall likelihood can easily be evaluated. Then, we use the Boltzmann generator to bring the whole path back to configuration space, compute the probability of the path, and use it to accept or reject the proposed path. This procedure is depicted in Figure 2.

Our contributions are investigating this novel method for transition path sampling and highlighting its challenges. To that end, we describe the difficulty of calculating likelihoods for paths that were not generated with molecular dynamics and the obstacles for calculating path probabilities in parallel. Additionally, we provide insights into what configuration space paths are produced from simple paths in the latent space of a Boltzmann generator.

## 2 Background and Related Work

**Boltzmann generators.** Given a molecule, the probability of each 3D configuration is proportional to the exponential of its negative energy, i.e., they follow a Boltzmann distribution. Noe et al. (2019) train a normalizing flow (Tabak and Vanden-Eijnden, 2010; Tabak and Turner, 2013) to sample a molecule's Boltzmann distribution, known as Boltzmann generator. While recent innovations (Midgley et al., 2023; 2023) improved their training efficiency, training them for larger systems remains an open problem and a limitation of our Boltzmann generator-based approach.

**Deep learning for transition path sampling.** The TPS problem, with the goal to sample the whole transition path ensemble, is more challenging than finding a single low-energy transition path: A problem that also has been explored with deep learning (DL) approaches (Liu et al., 2022; Holdijk et al., 2023). For the harder TPS problem, DL methods require MCMC with shooting moves as proposed by Dellago et al. (1998, 1998). For instance, Falkner et al. (2023) replace the shooting point selection with DL and sample them with a Boltzmann generator. Similarly, Jung et al. (2023) increase the acceptance rate of shooting moves by selecting the frames to shoot from with a learned function. These approaches still require sequential MD simulation. In this work, we explore a novel molecular dynamics-free MCMC paradigm using DL.

## 3 Method

We assume access to a Boltzmann generator for the molecule of interest and two of its states, A and B, between which we wish to sample the transition path ensemble. In the following, we lay out the overall MCMC framework over latent space paths (see SS 3.1). This requires two components:

Figure 2: **MCMC proposals for latent space transition paths.** We move a transition path \(x\) into latent space using a Boltzmann generator \(F()\). With this path \(\) and our latent space proposal kernel \(q_{Z}(}|)\) we propose \(}\) and bring it back to configuration/atom space to obtain the transition path proposal \(\). The likelihood of all steps can be computed, and we use them in a Metropolis-Hastings acceptance criterion to sample the transition path ensemble with MCMC.

Calculating the path probability (see SS 3.2), and a proposal kernel for a path in latent space for which we lay out several options (see SS 3.3).

### MCMC Framework for Latent Paths

Let \(\) be our current path with frames \(_{i}^{n 3}\) and \(i\{1,...,l\}\), where \(n\) is the number of atoms of our molecule and \(l\) the number of frames which we keep constant (the spacing of the frames can change with changing path lengths). For our MCMC procedure, we further need a proposal kernel \(q(})\) that produces a new path proposal \(}\) from our current path \(^{}\). If we can additionally compute the probability of a path \(p_{AB}\), we can sample the transition path ensemble with the MCMC algorithm using Metropolis-Hastings acceptance criterion

\[=\{1,(})}{p_{AB}()} })}{q(} )}\}.\] (1)

In our work, the proposal consists of first using a Boltzmann generator \(F\) trained on the molecule to move the path \(\) into latent space to obtain the latent path \(=\{F^{-1}(_{1}),,F^{-1}(_{l} )\}\). Subsequently, we make a proposal in latent space to obtain a new latent path \(}\) using the latent proposal kernel \(q_{z}(})\) which we design in SS 3.3. Lastly, the latent path is projected back to configuration space using the Boltzmann generator \(=\{F(}_{1}),,F(}_ {l})\}\).

The proposal kernel thus takes the form \(q(})=p(|)q_{z}(})p( }|)\), where \(p(|)\) accounts for the change of density when using our Boltzmann generator to move the path \(\) into latent space and \(p(}|})\) arises from moving the new latent path back to configuration space. Since the Boltzmann generator processes all the frames independently, the change of density factors can be written as the product of the individual frames \(p(|)=_{i=1}^{l}p(_{i}|_{i})\). With this in mind, the ratio of the forward path proposal \(q(})\) and the backward proposal \(q(})\), as it is required in the acceptance criterion in Equation 1, takes the form

\[})}{q(})}=( })}{q_{Z}(})}_{i=1}^{l }}_{i}|}_{i})p(_{i}|_{i})}{p( _{i}|_{i})p(}_{i}|}_{i})}.\] (2)

Each term in the product can be simplified as follows, where we write \(,\) for an individual frame \(_{i},_{i}\) and use the change of variables formula \(p()=p()( J(F()))^{-1}\) in the third equality

\[}|})p(|)}{p(|)p( }|})}=},})} {p(})},)}{p()}}{p()}=)p(})}{p(})p()}=)( J(F()))^{-1}p( })}{p(})( J(F(})))^{-1}p()}= }))}{ J(F())}.\] (3)

Thus, the ratio of proposals we need to calculate is

\[})}{q(})}=( })}{q_{Z}(})}_{j=1}^{l }}_{j}))}{ J(F(_{j}^{(i-1)} ))},\] (4)

which we can readily use to calculate the acceptance ratio for the MCMC algorithm as laid out in Algorithm 1. The remaining challenges are the ability to compute the path probability \(p_{AB}\) and a concrete latent space proposal kernel \(q_{Z}(})\), which we will tackle next.

### Calculating the Path Probability

A path's probability is defined with respect to a molecular dynamics model. Here, we assume Langevin dynamics2 under which the transition from frame \(_{i}\) to the next frame \(_{i+1}\) can be calculated as

\[_{i+1}=_{i}+ t_{i +1}\\ _{i+1}=_{i}+(1-) U(_{ i})+T(1-^{2})},\] (5)given the velocity \(_{i}\), and the molecule's energy function \(U:^{d 3}\). \(=(- t)\) for a time step size \( t\)3 and \((0,1)\) corresponds to random motion that is scaled proportional to the Boltzmann constant \(k_{B}\) and temperature \(T\). Notice that in Langevin dynamics, the only randomness when obtaining \(_{i+1}\) from \(_{i}\) given the velocity \(_{i}\) stems from the Gaussian variable \(T(1-^{2})}\). Thus, the probability density \(p(_{i+1},_{i+1}|_{i},_{i})\) of moving from \(_{i}\) to \(_{i+1}\) is that of a Gaussian with mean \(=_{i}+ t(_{i}+(1-) U(_{i}))\) and standard deviation \(=k_{B}T(1-^{2})\).

Given this probability \(p(_{i+1},_{i+1}|_{i},_{i})\) of moving between individual frames with the auxiliary velocity variable, the probability of a whole path in configuration space is

\[p_{AB}()=p(_{1})_{i=1}^{l-1}p( {x}_{i+1},_{i+1}|_{i},_{i}),\] (6)

where \(p(_{i})\) follows the molecule's Boltzmann distribution, meaning that \(p(_{i}) exp(-U(_{i})/k_{B}T)\) with an unknown proportionality constant. However, this constant is unnecessary since it will cancel out with the same constant of the reverse path density \(p_{BA}\) in the acceptance ratio in Equation 1.

Thus, the last missing link to computing \(p_{AB}()\) is the initial velocity \(_{1}\). Since our path definition does not include an initial velocity (because we do not have a Boltzmann generator that operates over both velocities and positions), we opt to marginalize over all possible velocities and approximate the following expectation as our final path probability

\[p_{AB}()=_{_{1}(,(k_{B }T))}[p(_{1})_{i=1}^{l-1}p(_{i+1}, _{i+1}|_{i},_{i})].\] (7)

All subsequent velocities \(\{_{i}\}_{i\{2,,l\}}\) can then be inferred by solving the previous step, allowing us to compute \(p(_{i+1},_{i+1}|_{i},_{i})\) sequentially.

**Desirable properties.** In designing our MCMC procedure, we set out to avoid the time-consuming sequential molecular dynamics simulation. While the path probability can be computed easily for paths generated by MD (Jung et al., 2017), calculating the path probability \(p_{AB}()\) still requires sequential computation in our approach. However, this amounts to sequentially performing \(l\) vector additions, which is very cheap and can be done in parallel for all different initial velocities when approximating the expectation. The expensive, time-consuming computations stem from the evaluation of the energy function \(U(x_{i})\) for each frame. In our procedure, this can be done in parallel, while in molecular dynamics, it has to be performed sequentially.

### Latent Space Path Proposal Kernel

As for the concrete latent space path proposal kernel \(q_{Z}(})\), we propose three different options: 1) Gaussian noise added to each frame. 2) A Gaussian Process (GP) with the current path as its mean. 3) A GP that is adaptively fit to the history of all sampled transition paths and only weakly depends on the current path. All these proposals are symmetric and will not contribute to our acceptance ratio with \(q_{Z}(})/q_{Z}(})=1\).

**Gaussian proposal.** From a latent path \(\), we propose a new path \(}=\{_{1}+_{1},,_{l}+_{l}\}\) where \(_{1},,_{l}(,)\). While this independent noise for each frame makes it unlikely that all frames move coherently and produce high-probability paths, this operation can be performed efficiently and allows for fast exploration of the latent space.

**Conditional Gaussian process path proposals.** We employ a GP \(f(t)(m(t),k(t,t^{}))\), where \(f:^{3n-6}\) maps the time \(t[1,l]\) along the path to a Gaussian from which a frame at time \(t\) is sampled4. We fit the GP mean \(m()\) and kernel function \(k(,)\), which is not to be confused with the proposal kernel, to a set of \(s\) latent paths \(\{^{i}\}_{i\{1, s\}}\), where the index of each frame is used as the time \(t\). In the following, we first detail the set of latent paths before explaining how the GP is used to propose a new path.

Our set of latent paths \(\{^{i}\}_{i\{1, s\}}\) to fit the GP is either the history of all previously sampled paths or we obtain it via linear interpolation in latent space. Specifically, to obtain an interpolation, we sample a start \(_{1}\) and an end frame \(_{l}\) from states A and B, move them to latent space to generate \(_{1},_{l}=F^{-1}(_{1}),F^{-1}(_{l})\), and produce the latent path as the linear interpolation \(_{i}=_{1}+(1-)_{l}\) for \(i\{1,,l\}\). After moving it back to configuration space with the Boltzmann generator, this constitutes a coarse path. This produces a fixed proposal kernel, where the quality depends on the paths it was trained on.

When using the history of all previously sampled paths as \(\{^{i}\}_{i\{1, s\}}\), the proposal kernel \(_{s}\) changes over the course of MCMC steps \(s\), leading to an adaptive MCMC algorithm. For this to be correct, the proposal kernel has to converge and satisfy vanishing adaptation (Andrieu and Thoms, 2008) where, as the Markov chain progresses, the influence of its most recent states on the proposal kernel has to diminish. Intuitively, this is the case for our adaptive kernel since the influence of the most recent path on the fitted mean and covariance kernel vanishes as the size of the history (the Markov chain) increases.

We re-fit this adaptive GP proposal to the history of latent paths \(\{^{i}\}_{i\{1, s\}}\) at each step \(s\) when a new path has been accepted. To efficiently do so, we start optimization from the parameters of the previous GP proposal kernel that are optimal for \(\{^{i}\}_{i\{1, s-1\}}\). The new optimization's convergence is typically fast since the minimum under the new set of latent paths at step \(s\) is likely close to that at step \(s-1\), with the difference diminishing as the length of the Markov chain increases.

Given the fitted GP, a new latent path \(}\) is proposed conditioned on the current one \(\) by sampling \(_{s}\) at times \(t=1,,l\) (which correspond to the frame numbers of the paths) after setting the means of \(_{s}\) at those times to the frames of \(\), meaning that \(m(t)=_{t}\) for \(t\{1, l\}\). This amounts to sampling \(_{s}\) unconditionally at \(t=1,,l\), subtracting the means \(m(t)\), and adding the frames \(_{t}\) at each time.

**Unconditional Gaussian process path proposals.** Here, we use the adaptive Gaussian process \(_{s}\) and propose new paths \(}\) unconditionally, meaning that each proposal is a sample of \(_{s}\) and the only influence of \(\) is through its presence in the set of paths \(\{^{i}\}_{i\{1, s\}}\) that \(_{s}\) was fit on. This means that with a progressively increasing number of accepted paths, the influence of the current path will diminish. This would fit a Gaussian process that could be used to sample transition paths without any latent space, which is an interesting aspect on its own.

Further, since we will rely on the mean of the Gaussian process, we can also estimate it between the frames. This allows us to introduce more variance by evaluating the Gaussian process not at the fixed points \(1,,l\), but to uniformly draw \(l\) sorted samples from \(_{[0,5,l+0.5]}\). With this, the individual frames of the path can shift more easily towards and from each other.

## 4 Experiments

**Latent space analysis.** When moving configurations from the meta-stable states \(C_{5}\) and \(_{R}\) of alanine dipeptide (ALDP) into the latent space, we can linearly interpolate between them and map them back with the trained Boltzmann generator. For this, we train a Boltzmann generator by minimizing the forward KL-divergence loss (compare Appendix A.3). Figure 3 shows that linear paths in latent space produce non-linear paths in configuration space. While linearly interpolating atom positions of a molecule produces unrealistic paths, this naive latent space approach recovers two different modes of transitions between the meta-stable states.

**Ground truth ensemble.** We simulated 10 nanoseconds with a timestep of 1 femtosecond at 300K with the openMM MD engine (Eastman et al., 2017). From this data, we can determine for each conformation whether it belongs to a meta-stable state, allowing us to find paths by looking for sequences that start in A and transition to B (or vice versa). This approach finds variable-length transition paths. We rely on the two-way shooting scheme implemented by OPS (Swenson et al., 2018a,b) with the same MD setup to sample a fixed-length transition path ensemble. Transitions that only rarely occur (Figure 4 bottom) are particularly difficult to produce with classical MD, already for the small molecule alanine dipeptide.

**Results.** Figure 4 shows for all methods a histogram of the sampled transition paths between the states \(C_{5}_{R}\) and \(_{R} C_{7}\), respectively. _Unconditional GP Uni_ refers to the adaptive GP proposal with uniform timepoint sampling while _Unconditional GP_ always samples the index of the frame as timepoint. _Conditional GP_ uses the adaptive proposal.

The main finding is that due to the low acceptance rate of our MCMC steps, we are only able to produce a low amount of paths or a set of paths with low diversity. When increasing the variance, paths will be more diverse but are also less likely to be accepted. To overcome this, proposals that produce more physically likely paths are required.

Some proposal strategies, such as the Gaussian proposal, are computationally efficient, while fitting a high-dimensional Gaussian process is time-consuming. With an increasing number of paths, the proposals are more likely to be stuck in a local minimum. For the more computationally expensive proposals, this makes it challenging to produce the high number of transitions needed to overcome this threshold.

While training a fixed Gaussian process on simple paths in latent space is computationally favorable, the results do not indicate that it can capture the transition paths. Since the iterative Gaussian processes do not seem to fit the distribution either, our choice of kernel or formulation might be inappropriate. In general, we have seen in our experiments that the selection for a kernel of the Gaussian process (compare Appendix A.2) poses a difficult problem for this task because it must capture an adequate amount of noise without overfitting to the previous paths.

Overall, the results qualitatively show that the simplest proposal kernel, one that simply adds Gaussian noise in latent space, appears to be the most efficient and effective choice. Further, conditioning the Gaussian process on the current path appears to slightly increase the variance and leads to a more diverse set of paths.

Figure 4: **Comparison of sampling methods**. Each row shows the transitions between two different metastable states. _Left:_ “Ground truth” path ensemble from MD simulation of all paths (sub-left) and the 25% of paths with the highest probability (sub-right). _Right:_ Shooting move MCMC ensemble and the ensembles of our different latent space proposal kernels. Note that it is unclear what a meaningful ground truth ensemble is.

Figure 3: **Linear latent space interpolation.**_Left:_ A histogram of the two main dihedral angels \(,\) as they occur in the MD simulation. The meta-stable states \(C_{5}\) and \(_{R}\), and a linear interpolation in latent space (red line) are shown. _Right_: The resulting density of transition paths when linearly interpolating between those states in latent space.

Discussion and Conclusion

**Limitations** Our approach relies on a trained Boltzmann generator, of which high-quality ones for larger molecular systems do not exist yet. Furthermore, the latent space path proposal kernels we devise have too low acceptance rates to be useful. This limits them to a low-variance, slowing down mode-mixing. Better latent space proposals would be necessary. Lastly, an avenue toward a practical solution could be adaptively fine-tuning the Boltzmann generator to make simple paths in latent space correspond to physical paths that obey Langevin dynamics in configuration space.

**Conclusion** In this paper, we presented a novel way to propose transition paths in the latent space of a Boltzmann generator. Throughout this work, we have introduced multiple latent space path proposal kernels that perform (learned) operations. This enables a transition path sampling MCMC procedure without the need for molecular dynamics simulation. We believe that learned transition path sampling methods and, in general, simulation-free MCMC approaches are interesting research questions to explore and might lead to faster sampling methods.