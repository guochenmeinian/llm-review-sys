# Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models

Ling Yang\({}^{1*{}}\),  Zhaochen Yu\({}^{1}\),  Tianjun Zhang\({}^{2}\),  Shiyi Cao\({}^{2}\),  Minkai Xu\({}^{3}\),  Wentao Zhang\({}^{1}\),  Joseph E. Gonzalez\({}^{2}\),  Bin Cui\({}^{1}\)\({}^{1}\)Peking University, \({}^{2}\)UC Berkeley, \({}^{3}\)Stanford University

Project: https://github.com/YangLing0818/buffer-of-thought-llm

Extension: https://github.com/YangLing0818/SuperCorrect-llm

Equal Contribution. Contact: yangling0818@163.comCorresponding Authors.

###### Abstract

We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose _meta-buffer_ to store a series of informative high-level thoughts, namely _thought-template_, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose _buffer-manager_ to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, we find that our Llama3-8B + BoT has the potential to surpass Llama3-70B model. Our project is available at https://github.com/YangLing0818/buffer-of-thought-llm

## 1 Introduction

A series of Large Language Models (LLMs)  like GPT-4 , PaLM  and LLaMA  have showcased the impressive performance in various reasoning tasks. In addition to scaling up the model size to improve the reasoning performance, there are more effective prompting methods that further enhance the functionality and performance of LLMs. We divide these methods into two categories: (i) **single-query reasoning:** these methods  usually focus on prompt engineering and their reasoning process can be finished within a single query, such as CoT  that appends the input query with 'Let's think step by step' to produce rationales for increasing reasoning accuracy, and Few-shot Prompting  which provides task-relevant exemplars to assist the answer generation; (ii) **multi-query reasoning:** these methods  focus on leveraging multiple LLM queries to elicit different plausible reasoning paths, thus decomposing a complex problem into a series of simpler sub-problems, such as Least-to-Most , ToT  and GoT .

However, both kinds of methods face some limitations: (1) single-query reasoning usually requires prior assumption or relevant exemplars of reasoning process, which makes it impractical to manually design them task by task, thus lacking universality and generalization; (2) Due to the recursiveexpansion of reasoning paths, multi-query reasoning is usually computationally-intensive when finding a unique intrinsic structure underlying the reasoning process for each specific task; (3) Both single-query and multi-query reasoning processes are limited by their designed exemplars and reasoning structures, and they neglect to derive general and high-level guidelines or thoughts from previously-completed tasks, which are informative for improving efficiency and accuracy when solving similar problems.

To address these limitations, we propose Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning framework aimed at enhancing reasoning accuracy, efficiency and robustness of LLMs across various tasks. Specifically, we design _meta-buffer_, a lightweight library housing a series of universal high-level thoughts (_thought-template_), which are distilled from different problem-solving processes and can be shared across tasks. Then, for each problem, we retrieve a relevant thought-template and instantiate it with specific reasoning structure for efficient thought-augmented reasoning. In order to guarantee the scalability and stability of our BoT, we further propose _buffer-manager_ to dynamically update the meta-buffer, which effectively enhances the capacity of meta-buffer as more tasks are solved.

Our method has three critical advantages: (i) **Accuracy Improvement:** With the shared thought-templates, we can adaptively instantiate high-level thoughts for addressing different tasks, eliminating the need to build reasoning structures from scratch, thereby improving reasoning accuracy. (ii) **Reasoning Efficiency:** Our thought-augmented reasoning could directly leverage informative historical reasoning structures to conduct reasoning without complex multi-query processes, thus improving reasoning efficiency. (iii) **Model Robustness:** The procedure from thought retrieval to thought instantiation is just like the human thought process, enabling LLMs to address similar problems in a consistent way, thus significantly enhancing the model robustness of our method. Our empirical studies demonstrate that Buffer of Thoughts significantly improves precision, efficiency, and robustness over a diverse array of tasks. Here, we summarize our contributions as follows:

1. We propose a novel thought-augmented reasoning framework Buffer of Thoughts (BoT) for improving the accuracy, efficiency and robustness of LLM-based reasoning.
2. We propose meta-buffer for store informative high-level thoughts distilled from different problems, and adaptively instantiate each thought template to address each specific task.
3. We design buffer-manager to distill thought-templates from various solutions, and is continually improves the capacity of meta-buffer as more tasks are solved.
4. We conduct extensive experiments on 10 challenging reasoning-intensive tasks. Our BoT achieves significant performance improvements over previous SOTA methods: **11% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One**, while requiring **only 12% of the cost** of multi-query prompting methods on average.

Figure 1: Comparison between single-query , multi-query , and (c) our BoT methods.

Related Work and Discussions

Retrieval-Augmented Language ModelsThe retrieval-augmented (Large) Language Model is introduced as a solution to mitigate the phenomenon of hallucination and enhance the output quality of language models [18; 19; 20; 21; 22]. When presented with an input question, the retrieval-augmented LLM first queries an external database with billion-level tokens  for retrieving a subset of the text corpus to help generating the final answer. Notably, the retrieval-augmented LLM achieves superior question-answering performance using fewer parameters compared to conventional LLMs , and it has found application across various downstream tasks [24; 25; 26], including multi-modal generation [24; 22; 23; 25] and biomedical applications [26; 27]. In this paper, we construct a novel category of retrieval database, termed _meta-buffer_, which contains a series of high-level thoughts rather than specific instances, aiming to universally address various tasks for LLM-based reasoning.

Prompt-based Reasoning with Large Language ModelsPrompting techniques have significantly enahnced the arithmetic and commonsense reasoning capabilities of LLMs. Chain-of-Thought (CoT) prompting  and its variants [28; 29; 30], such as Least-to-Most , Decomposed Prompting , and Auto-CoT --prompt LLMs to break down complex questions into simpler subtasks and systematically solve them before summarizing a final answer. Numerous studies [32; 33; 34; 35; 36; 37] have demonstrated the effectiveness of these prompting methods across a wide range of tasks and benchmarks. Innovations like Tree-of-Thought  and Graph-of-Thought , have further advanced this field by exploring dynamic, non-linear reasoning pathways to expand heuristic capabilities of LLMs [38; 39]. However, they suffer from increased resource demands and greater time complexity, depend on manual prompt crafting, and are often tailored to specific task types. Recent meta prompting methods [15; 40] utilize a same task-agnostic form of prompting for various tasks and recursively guide a single LLM to adaptively addressing different input queries. Nevertheless, such a long meta prompt may require a considerable context window, and these methods fail to leverage historical informative guidelines or thoughts for potential similar tasks.

Analogical ReasoningAnalogical reasoning is a useful technique for natural language reasoning [41; 42; 43; 44; 45]. Recent works demonstrate that LLMs can perform analogical reasoning just like humans [46; 47; 12; 48; 49]. For example, Analogical Prompting  and Thought Propagation  prompt LLMs to self-generate a set of analogous problems, and then utilize the results of analogous problems to produce a solution for input problem. However, the specific solutions for self-explored problems may introduce additional noise and cause error accumulation. Recent Thought-Retriever  uses the intermediate thoughts generated when solving past user to address analogous queries, but it only focuses on textual comprehension/generation instead of general reasoning problems. Thus, a more high-level and general analogical approach for LLM complex reasoning is still lacking.

## 3 Buffer of Thoughts

Overview of Buffer of ThoughtsIn this section, we introduce our Buffer of Thoughts in detail and we also illustrate our core thought-augmented reasoning process in Figure 2. Given a specific task, we utilize our _problem-distiller_ (Section 3.1) to extract critical task-specific information along with relevant constraints. Based on the distilled information, we search in _meta-buffer_ (Section 3.2) that contains a series of high-level thoughts (_thought-template_) and retrieve a most relevant thought-template for the task. Subsequently, we instantiate the retrieved thought-template with more task-specific reasoning structures and conduct reasoning process. Finally, we employs a _buffer-manager_ (Section 3.3) for summarizing the whole problem-solving process and distilling high-level thoughts for increasing the capacity of meta-buffer.

### Problem Distiller

Most of complex tasks contain implicit constraints, complex object relationships, and intricate variables and parameters within their contexts. Consequently, during the reasoning stage, LLMs need to overcome three main challenges: extracting vital information, recognizing potential constraints, and performing accurate reasoning. These challenges would impose a significant burden on a single LLM. Therefore, we separate the extraction and comprehension stages of task information from the final reasoning stage, through prepending a _problem distiller_ to the reasoning process. Moreconcretely, we design a meta prompt \(\) to first distill and formalize the task information. The distilled task information could be denoted as:

\[x_{d}=LLM((x)),\] (1)

where \(x\) is the task statement. Due to the page limit, we put the detailed meta prompt for problem-distiller in Appendix B.2.

Problem Condensation and TranslationWe use the problem distiller to extract key elements from input tasks, focusing on: (1). Essential parameters and variables for problem-solving; (2). The objectives of the input tasks and their corresponding constraints. We then re-organize this distilled information into a clear, comprehensible format for the subsequent reasoning stage. We then translate the specific problems into high-level concepts and structures. This translation procedure decomposes complex real-world problems, like intricate mathematical application scenarios, into simpler, multi-step calculations, making it easier for later retrieval of high-level thought.

### Thought-Augmented Reasoning with Meta Buffer

MotivationHuman often summarize and induce higher-level guidelines when solving problems and then apply them to relevant problems. Motivated by this, we propose _meta-buffer_, a lightweight library that contains a series of high-level thoughts (_thought-template_) for addressing various types of problems. Unlike traditional methods [11; 46; 12; 36; 9] that require specific instructions or exemplars, our high-level thought-templates can be adaptively instantiated when solving different problems, thereby enhancing LLMs with superior precision and flexibility.

Thought TemplateAs a kind of high-level guideline, our thought-template is stored in meta-buffer, and is obtained from various problem-solving processes by our _buffer-manager_. The details about acquiring thought-templates would be introduced in Section 3.3. Since our BoT aims to provide a general reasoning approach for various tasks, we correspondingly classify the thought-templates into six categories: Text Comprehension, Creative Language Generation, Common Sense Reasoning, Mathematical Reasoning, Code Programming and Application Scheduling. We provide some example thought-templates in Appendix B.1. Such classification of thought-templates can

Figure 2: Illustration of different reasoning process. Buffer of Thoughts enables large language models to tackle complex reasoning tasks through our thought-augmented reasoning process. Thought template is marked in orange and instantiated thought is marked in blue.

facilitate the template retrieval for finding most suitable solutions to different problems. Here we denote thought template, template description and its corresponding category as \((T_{i},D_{T_{i}},C_{k})\), where \(i\) denotes the index of meta-template, \(k^{+}\) and \(1 k 6\), which means \(C_{k}\) is in one of the six categories, and \(D_{T_{i}}\) is the description of thought template.

Template RetrievalFor each task, our BoT retrieves a thought-template \(T_{i}\) that is highly similar to the distilled problem \(x_{d}\) by calculating the embedding similarity between the description \(D_{T_{i}}\) and \(x_{d}\). The retrieval process can be formulated as:

\[j=_{i}((f(x_{d}),\{f(D_{T_{i}})\}_{i=1}^{N})), (f(x_{d}),\{f(D_{T_{i}})\}_{i=0}^{n})>=,\] (2)

\(N\) is the size of the meta-buffer, \(f()\) is a normal text embedding model, and \(T_{j}\) denotes the retrieved thought template. We set a threshold \(\) (0.5\(\)0.7 is recommended) to determine whether the current task is new. Therefore, if \((f(x_{d}),\{f(D_{T_{i}})\}_{i=0}^{n})<\), we identify the task \(x\) as a new task.

Instantiated ReasoningFor each specific task, we discuss two situations for the instantiated reasoning, depending on whether the current task is new: The first situation is that we successfully retrieve a thought-template \(T_{j}\) for the task. In this case, as presented in Figure 2, our thought-augmented reasoning will be adaptively instantiated to suitable reasoning structures with our designed instantiation prompt (in Appendix B.3). For example, in a Checkmate-in-One problem, we instantiate the template of updating chess board state to solve the problem step by step. Thus we conduct the instantiated reasoning for task \(x\) using the distilled information \(x_{d}\) and the retrieved template \(T_{j}\), and produce its solution \(S_{x}\) as:

\[S_{x}=LLM_{}(x_{d},T_{j}),\] (3)

where \(LLM_{}\) denotes the instantiated reasoner with a LLM.

In the second situation, the task is identified as a new task. To enable proper instantiated reasoning, we prepare three general coarse-grained thought-templates for utilization. Based on the distilled task information \(x_{d}\), our BoT would automatically assign a suitable thought-template to the reasoning process. The detailed pre-defined thought-templates are included in Appendix B.3).

### Buffer Manager

We propose _buffer-manager_ to summarize the high-level guidelines and thoughts that are gained from each problem-solving process. It can generalize each specific solution to more problems, storing the critical distilled knowledge in the form of thought-templates within the meta buffer. In contrast to methods that **temporarily** generate exemplars or instructions for each problem, our buffer-manager can ensure **permanent** advancements in accuracy, efficiency, and robustness for LLM-based reasoning.

Template DistillationTo extract a general though-template, we propose a three-step approach: (1) Core task summarization: identifying and describing basic types and core challenges of problems; (2) Solution steps description: summarize the general steps for solving a problem; (3) General answering template: based on the above analysis, propose a solution template or approach that can be widely applied to similar problems. Additionally, to boost the generalization ability and stability of template distillation, we carefully design two types of in-context examples of how to generate thought-template--_in-task_ and _cross-task_ examples. _Cross-task_ means we choose the template distilled from one task to tackle the problem of other tasks, such as addressing a mathematical problem with a code-related thought-template. The new template distilled from input task \(x\) can be denoted as:

\[T_{new}=LLM_{}(x_{d},S_{x}),\] (4)

where \(LLM_{}\) is the LLM-based template distillized initialized with the following prompt:

**Prompt for Template Distillation:**

**User: [Problem Description] + [Solution Steps or Code]**

To extract and summarize the high-level paradigms and general approaches for solving such problems, please follow these steps in your response:

**1. Core task summarization:**

Identify and describe the basic type and core challenges of the problem, such as classifying it as a mathematical problem (e.g., solving a quadratic equation), a data structure problem (e.g., array sorting), an algorithm problem (e.g., search algorithms), etc. And analyze the most efficient way to solve the problem.

**2. Solution Steps Description:**

Outline the general solution steps, including how to define the problem, determine variables, list key equations or constraints, choose appropriate solving strategies and methods, and how to verify the correctness of the results.

**3. General Answer Template:**

Based on the above analysis, propose a template or approach that can be widely applied to this type of problem, including possible variables, functions, class definitions, etc. If it is a programming problem, provide a set of base classes and interfaces that can be used to construct solutions to specific problems.

Please ensure that your response is highly concise and structured, so that specific solutions can be transformed into generalizable methods.

**[Optional] Here are some exemplars of the thought-template:** (Choose cross-task or in-task exemplars based on the analysis of the Core task summarization.)

Dynamic Update of Meta-BufferAfter template distillation, we need to consider whether the distilled template should be updated into the meta-buffer. If we initialize an empty meta-buffer or encounter a problem without a proper thought-template, the distilled thought-templates will be directly stored in the meta-buffer. If we solve problem with a retrieved thought-template, new insights may arise during the instantiation of a certain thought-template. Therefore, to avoid the redundancy of the meta-buffer while maintaining newly-generated informative thoughts, we will calculate the similarity between the embedding vectors of \(D_{T_{new}}\) and \(\{D_{T_{i}}\}_{i=0}^{n}\) and update the meta-buffer with the following rule:

\[((f(D_{T_{new}}),\{f(D_{T_{i}})\}_{i=0}^{n}))<.\] (5)

Otherwise, it means the meta-buffer has already possessed the necessary knowledge to solve this task and does not need to perform the update. Our dynamic update strategy effectively reduces the computational burden of template retrieval while ensuring the lightweight property of our meta-buffer. We further conduct ablation study to analyze it in Section 4 and Appendix A.

## 4 Experiments

Datasets and TasksTo evaluate the efficacy of our proposed Buffer of Thoughts and compare with previous methods, we consider a diverse set of tasks and datasets that require varying degrees of mathematical and algorithmic reasoning, domain-specific knowledge, and literary creativity: (a). The **Game of 24** from ToT , where the objective is to form an arithmetic expression that equals 24 using each of four given numbers exactly once; (b). Three BIG-Bench Hard (BBH)  tasks: **Geometric Shapes, Multi-Step Arithmetic Two**, and **Word Sorting**; (c). Three reasoning tasks directly obtained from the BIG-Bench suite : **Checkmate-in-One**, **Penguins**--where the task is to answer questions about penguins' attributes based on a given table and additional natural language information, and **DateUnderstanding**--a task that involves inferring dates from natural language descriptions, performing arithmetic operations on dates, and utilizing global knowledge such as the number of days in February; (d). **Python Programming Puzzles** (P3) [51; 52], a collection of challenging programming puzzles written in Python with varying difficulty levels; (e). **Multilingual Grade School Math** (MGSM) , a multilingual version of the GSM8K dataset  featuring translations of a subset of examples into ten typologically diverse languages, including Bengali, Japanese, and Swahili; (f). **Shakespearean Sonnet Writing** from meta-prompting , a novel task where the goal is to write a sonnet following the strict rhyme scheme "ABAB CDCD EFFG" and incorporating three provided words verbatim.

Implementation and BaselinesFor the fair comparisons with previous methods, we use GPT-4 as the base model of our BoT, including the main experiment and the ablation study. We also use Llama3-8B and Llama3-70B in our analysis part on NVIDIA A100-PCIE-40GB GPU. We compare our Buffer of Thoughts with the following prompting methods: **1. Standard Prompting**: This is our most basic baseline, where an LLM is asked to generate a response directly from the input query, without any specific guiding input-output examples or additional instructions beyond the task description included in the query.

2. Single-query MethodThis includes Zero-shot CoT  and PAL , which use the LLM to analyze natural language problems and generate intermediate reasoning steps. We also include Expert Prompting , which creates an expert identity tailored to the specific context of the input query, and then integrates this expert profile into the input to generate a well-informed response.

3. Multi-query MethodThis includes ToT  and GoT , which enable LLMs to make deliberate decisions by considering multiple reasoning paths and self-evaluating choices to determine the next course of action. These methods also allow for looking ahead or backtracking when necessary to make global decisions. Additionally, we include Meta Prompting , which employs an effective scaffolding technique designed to enhance the functionality of LLMs.

### BoT Achieves Better Accuracy, Efficiency and Robustness

Reasoning AccuracyAs shown in Table 1, our BoT consistently outperforms all previous prompting methods across multiple kinds of challenging benchmarks, particularly demonstrated in complicated reasoning tasks such as Game of 24 and Checkmate-in-One. Taking GPT-4 as a baseline, our method achieves an astonishing 79.4% accuracy improvement in Game of 24, and compared to ToT, which has a good performance on this task, we also achieve an 8.4% accuracy improvement. What's more, compared to recent Meta-prompting method , we see **significant accuracy improvements: 23% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One**. Existing methods need complex, iterative, and heuristic search strategies to address these problems on a case-by-case basis. Conversely, our BoT leverages the historical insights and informative guidelines from thought-templates, and further adaptively instantiate a more optimal reasoning structure for addressing these complex problems.

Reasoning EfficiencyIn addition to significant improvements in accuracy, as a multi-query method, our BoT can achieve comparable reasoning time to single-query method across various tasks, while being considerably less than conventional multi-query method like ToT  as shown in Figure 3. For example, in Game of 24, both single-query and multi-query methods necessitate iterative and heuristic searches to identify feasible solutions. This process is particularly time-consuming and inefficient, especially for the multi-query method, which involves conducting multi-query search and backtrace phases. In contrast, our BoT directly retrieves a thought-template in code format, thus a program is instantiated to traverse combinations of numbers and symbols, thereby eliminating the need to build the reasoning structure from scratch. This allows for solving the problem with just one query after invoking the problem-distiller, significantly reducing the time required for complex reasoning. Notably, our **BoT requires only 12% of the cost of multi-query methods** (e.g., tree of thoughts and meta-prompting) on average.

   Task & Standard &  &  \\   & GPT4  & GPT4+CoT  & Expert  & PAL  & ToT  & GoT  & Meta Prompting  & **BoT** \\  Game of 24 & 3.0 & 11.0 & 3.0 & 64.0 & **74.0** & 73.2 & 67.0 & **82.4** \\ MOSM (avg) & 84.4 & 85.5 & 85.0 & 72.0 & 86.4 & **87.0** & 84.8 & **89.2** \\ Multi-Step Arithmetic & 84.0 & 83.2 & 83.2 & 87.4 & 88.2 & 89.2 & **90.0** & **99.8** \\ WordString & 80.4 & 83.6 & 85.2 & 93.2 & 96.4 & 98.4 & **99.6** & **100.0** \\ Python Puzzles & 31.1 & 36.3 & 33.8 & 47.3 & 43.5 & 41.9 & **45.8** & **52.4** \\ Geometric Shapes & 52.6 & 69.2 & 55.2 & 51.2 & 56.8 & 54.2 & **78.2** & **93.6** \\ Checkmate-in-One & 36.4 & 32.8 & 39.6 & 10.8 & 49.2 & 51.4 & **57.2** & **86.4** \\ Date Understanding & 68.4 & 69.6 & 68.4 & 76.2 & 78.6 & 77.4 & **79.2** & **88.2** \\ Penguins & 71.1 & 73.6 & 75.8 & **93.3** & 84.2 & 85.4 & 88.6 & **94.7** \\ Sonnet Writing & 62.0 & 71.2 & 74.0 & 36.2 & 68.4 & 62.8 & **79.6** & **80.0** \\   

Table 1: Comparing BoT with previous methods across various tasks. We denote the best score in **blue**, and the second-best score in **green**. Our BoT significantly outperforms other methods on all tasks, especially on general reasoning problems.

Reasoning RobustnessTo better evaluate our BoT, we devise a new evaluation metric: _success rate_, which is used to assess the reasoning robustness. We randomly sample 1000 examples from various benchmarks as a test subset and evaluate different methods on this subset. As shown in Figure 4, we repeat this evaluation process 10 times and take the average accuracy as the success rate of different methods on each benchmark. Compared with other methods, our BoT consistently maintains a higher success rate across various tasks, surpassing the second-best by 10% in average success rate. We attribute our outstanding robustness to the great generalization ability of our distilled thought-templates during reasoning across different tasks. By offering high-level thought from the suitable thought-templates, the stability of our method across different tasks is greatly enhanced.

## 5 Model Analysis

Distribution Analysis of Thought-TemplatesAs depicted in the left figure of Figure 5, we choose six different benchmarks, each sampled with 100 distinct tasks. We update the meta-buffer from scratch, and after completing all sampled tasks, we display the number of derived thought-templates. We can observe that our BoT generates a greater number of thought-templates in the MGSM tasks that contain more diverse scenarios. In tasks with relatively simple requirements, such as Checkmate-in-One and Penguins, BoT produces more fixed thought-templates tailored for those specific issues. The distribution of templates indicates that our BoT can effectively discover appropriate thought templates for different benchmarks.

Figure 4: Comparison of reasoning robustness between our Buffer of Thoughts and GPT4 , GPT4+CoT , Expert-prompting , PAL , ToT  across different benchmarks.

Figure 3: Comparison of **logarithmic inference time** between our Buffer of Thoughts and GPT4 , GPT4+CoT , Expert-prompting , PAL , ToT  across different benchmarks.

**Distribution Analysis of Time Cost** As illustrated in Figure 5, we measured the average time cost for each component of BoT's reasoning framework across different tasks. The time required for distilling task information and template retrieval is relatively short, whereas instantiated reasoning takes longer. Overall, considering the complexity of different components, our BoT achieves a relatively balanced distribution of time cost, demonstrating the efficiency of our BoT framework.

**Better Trade-off between Model Size and Performance** As depicted in Figure 6, on Game of 24, word list sorting and Checkmate-in-One, Llama3-8B and Llama-70B models  may result in poor outcomes. However, equipped with our BoT, both models demonstrate a substantial accuracy improvement. Notably, **BoT+Llama3-8B has the potential to surpass single Llama3-70B model**. Our BoT enables smaller models to exhibit the capabilities that approximate or even surpass larger models, significantly bridging the gap between their reasoning abilities. Furthermore, it greatly diminishes the inference cost required by large language models when tackling complex problems.

**Quality of Automatically-Induced Template** The success of the proposed approach critically depends on the quality of the automatically induced template. While this previous experiments has shown promising empirical performance on downstream tasks, it remains unclear how good the templates themselves are. Thus we make a comparison between the automatically generated task templates with manually prepared templates for more complex reasoning tasks on MATH dataset , with randomly sampled 500 problems. From the results, we can find that our automatic template boosts the reasoning ability of LLMs, demonstrating its generalization ability.

**Impact of Buffer-Manager** We further conduct ablation study on our buffer-manager, where we divide the entire process into four rounds. In each round, we randomly sample 50 questions from

Figure 5: Distribution Analysis of Thought-Templates and Time. _Left:_ Distribution Analysis of Thought-Templates. _Right_: Time Distribution of BoT.

Figure 6: We evaluate the trade-off between model size and performance with Llama3-8B and Llama3-70B models on three challenging benchmarks.

each benchmark and conduct reasoning. In the subsequent round, we continue to randomly sample another 50 questions from each benchmark. As depicted in Figure 7, with the increase of the number of rounds, the model with the buffer-manager continually expands the meta-buffer while also utilizing the thought-templates obtained from previously solved problems to help addressing subsequent similar problems. Therefore, we can observe that the accuracy of BoT steadily improves with each round. In contrast, the model without the buffer-manager fails to exhibit an upward trend.

Additionally, we also demonstrate the superiority of our buffer-manager on the reasoning efficiency as depicted in Figure 10. when the number of rounds increases, the model with the buffer-manager will experience a continual improvement in reasoning efficiency. This is because, with the continual expansion of the meta-buffer, the likelihood of retrieving suitable thought-templates also increases. Consequently, models can avoid constructing reasoning structures from scratch, thereby enhancing the inference efficiency accordingly. More ablation study can be found in Appendix A.

## 6 Discussion

In this work, we introduce Buffer of Thoughts, a novel buffered reasoning framework that employs LLMs to utilize pre-accumulated experiences and methodologies from prior tasks for progressively raising the LLM's reasoning capacity. our BoT brings out a set of future directions: (1). integrating external resources with BoT to build a open-domain system like agent models [55; 56]. (2). making the distillation of thought-templates optimizable, which may significantly enhance their template qualities for more complex tasks. (3). incorporating BoT into LLM training for eliciting more fine-grained and accurate reasoning process, like SuperCorrect .