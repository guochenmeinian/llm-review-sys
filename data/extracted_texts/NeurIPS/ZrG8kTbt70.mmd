# WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding

Yanchao Tan

College of Computer and

Data Science

Fuzhou University

Fuzhou, China

yctan@fzu.edu.cn

&Zihao Zhou

College of Computer and

Data Science

Fuzhou University

Fuzhou, China

reiverkey@gmail.com

&Hang Lv

College of Computer and

Data Science

Fuzhou University

Fuzhou, China

lvhangkenn@gmail.com

&Weiming Liu

College of Computer Science

Zhejiang University

Hangzhou, China

21831010@zju.edu.cn

&Carl Yang

Department of Computer Science

Emory University

Atlanta, United States

j.carlyang@emory.edu

Corresponding author

###### Abstract

Graphs are widely used to model interconnected entities and improve downstream predictions in various real-world applications. However, real-world graphs nowadays are often associated with complex attributes on multiple types of nodes and even links that are hard to model uniformly, while the widely used graph neural networks (GNNs) often require sufficient training toward specific downstream predictions to achieve strong performance. In this work, we take a fundamentally different approach than GNNs, to simultaneously achieve deep joint modeling of complex attributes and flexible structures of real-world graphs and obtain unsupervised generic graph representations that are not limited to specific downstream predictions. Our framework, built on a natural integration of language models (LMs) and random walks (RWs), is straightforward, powerful and data-efficient. Specifically, we first perform attributed RWs on the graph and design an automated program to compose roughly meaningful textual sequences directly from the attributed RWs; then we fine-tune an LM using the RW-based textual sequences and extract embedding vectors from the LM, which encapsulates both attribute semantics and graph structures. In our experiments, we evaluate the learned node embeddings towards different downstream prediction tasks on multiple real-world attributed graph datasets and observe significant improvements over a comprehensive set of state-of-the-art unsupervised node embedding methods. We believe this work opens a door for more sophisticated technical designs and empirical evaluations toward the leverage of LMs for the modeling of real-world graphs.

## 1 Introduction

Graphs are widely used to model interconnected entities, and they are critical in enhancing downstream predictions in various real-world applications. Nowadays, real-world graphs are often associated with complex attributes on multiple types of nodes and even links , and modeling such real-world graphs is non-trivial. For example, in the schema of a clinical attributed graph constructedfrom MIMIC-III2 data (shown in Figure 1(a)), there are multiple types of nodes such as patients, visits and diseases, each with their own attributes such as age, sex, time and name; there are also multiple types of links associated with different meanings. Such complex heterogeneous attributes of nodes and links attributes can hardly be modeled in a uniform space.

Although the widely used graph neural networks (GNNs) have shown remarkable successes in the modeling of attributed graphs for various downstream applications , the representation learning (a.k.a. embedding) of GNNs often requires sufficient training toward specific downstream predictions to achieve strong performance . While unsupervised training has also been explored for GNNs , the generic performances of unsupervised GNN embeddings across different downstream tasks are still unsatisfactory (as we will also show in our experiments). Consequently, it is important to devise a general-purpose graph embedding method to simultaneously understand the complex node/link attributes and incorporate the flexible graph structures in an unsupervised manner.

However, two obstacles stand in the way of achieving this goal. First, the nature of the attributes can be intricate and diverse, thus understanding their semantics in a uniform space is non-trivial. Second, the graph structures need to be accurately captured and incorporated into the embedding space, which is not straightforward due to the inherent flexibility and potential complexity of entity relations.

To address these issues, in this work, we take a fundamentally different approach than GNNs, named WalkLM, which is a uniform framework to obtain unsupervised generic graph representations that are not limited to specific downstream tasks. To this end, we first draw inspiration from the recent successes of language models (LMs), and propose to leverage LMs as uniform attribute models that can capture the intricate semantics of complex heterogeneous attributes of nodes and links. Secondly, we propose to leverage the classic tool of random walks (RWs) on graphs which have been shown effective in capturing flexible graph topological structures by various studies .

Specifically, we first generate attributed RWs on the graph (e.g., \(1}2}5\) in Figure 1(b)), and design an automated textualization program to compose roughly meaningful textual sequences directly from the attributed RWs. As shown in Figure 1(c), the composed text is a mapping from the attributed RW in Figure 1(b), where a uniform automatic program firstly textualize different types of nodes (in different colors) by properly concatenating the nodes with the names and values of different attributes, and then textualize the whole RWs by concatenating the nodes and links. Furthermore, we fine-tune an LM using the RW-based textual sequences and extract embedding vectors from the LM. The learned embeddings encapsulate both attribute semantics and graph structures, and can be flexibly utilized for arbitrary downstream tasks.

In our experiments, we take the node embeddings and evaluate them towards different downstream prediction tasks (e.g., node classification and link prediction) on multiple real-world attributed graph datasets and observe significant improvements over a comprehensive set of state-of-the-art unsupervised node embedding methods (e.g., WalkLM achieves an average of 88.98% improvement over the state-of-the-art baselines ranging from existing RW-based graph embedding methods to popular unsupervised GNN modes regarding both Macro-F1 and Micro-F1 metrics). We believe this

Figure 1: **A toy example of the transformation from a real-world attributed graph to the composed text**. (a) is a schema of a real-world attributed graph on MIMIC-III that delineates how nodes (e.g, patients), edges (e.g, _has_ between patients and visits), and the associated attributes (e.g., age) are organized and interconnected. (b) is attributed random walk for capturing structural information and can be composed to text in (c).

work paves the way for more sophisticated technical designs and empirical evaluations toward the leverage of LMs for the modeling of real-world graphs.

## 2 Related Work

**Graph Representation Learning.** In recent years, a plethora of representation learning techniques are proposed for graphs [3; 15; 55; 58; 74]. In this work, we focus on the objective of learning embedding vectors for each node that characterizes its topological (and semantic) information in the graph. Among existing node embedding methods, many have analyzed and utilized the great promise of random walks (RWs) in capturing the topological structures of graphs [8; 13; 21; 38]. However, the above methods ignore the abundant attribute information surrounding the nodes and edges . In recent studies, Graph neural networks (GNNs) for learning node representations through aggregating information from neighboring nodes on graphs [14; 24; 62]. However, most existing GNNs are established in a supervised learning setting, which requires abundant task-specific labeled data that may not be available in real-world applications [4; 76], and the embeddings they learn are not generalizable across different downstream tasks . Although some studies tried to reduce the labeling effort by pre-training an expressive GNN model on unlabeled data with self-supervision methods (e.g., contrastive learning) [19; 22; 75], their performances in specific downstream tasks still rely much on the properly chosen self-supervision tasks and attribute encoders [47; 73]- that is, there still lack a uniform framework for generic unsupervised attributed graph representation learning.

**Language Models (LMs).** With the massive corpora and powerful computation resources for pre-training, modern language models (LMs) derive various families . These LMs can be grouped into: (1) auto-regressive LMs (e.g., GPT  and GPT-2/3 [1; 41]), (2) masked LMs (e.g., BERT , RoBERTa , and XLNet ), and (3) encoder-decoder LMs (e.g., BART  and T5 ). LMs have been intensively studied by NLP researchers for various language-related tasks [16; 29; 43]. In our work, we innovatively utilize LMs as uniform attribute models for nodes and links in graphs for the first time. Note that, our work also readily generalizes to recent large language models (LLMs) (e.g., InstructGPT , ChatGPT and GPT-4 ) via appropriate parameter-efficient training approaches (e.g., LoRA  and prompt-tuning [25; 30]). However, those are orthogonal to the innovations in this work, for which we leave the exploration as immediate future works.

**LMs with Knowledge Graph (KG).** In recent studies, combining LMs with KG has been widely applied in various real-world applications . Among existing methods, many have proposed to enhance LMs with KG for significantly improve the performance of LMs in accessing domain-specific knowledge [46; 56; 67; 69], and the others proposed to harness the power of LMs for addressing KG-related tasks [65; 66]. However, the above methods fail to effectively combine the rich semantic information of graphs with global topological information. Furthermore, as closest to us,  proposed to combine random walks and neural network language models to produce new word representations. However, it ignores the rich relational information between nodes and thus fails to learn richer topological information. Moreover, compared with modern LMs with extensive prior knowledge, the text-based encoder used in [12; 51] fails to extract richer semantic information.

## 3 Preliminaries

### Problem Formulation

Given an attributed graph \(\) and multiple downstream tasks \(_{i}\) (e.g., node classification \(_{1}\) and link prediction \(_{2}\)), the goal of WalkLM is to sufficiently model information in \(\) and improve task performances on \(_{i}\). Specifically, we denote the graph as \(=(V,E,,)\), where each node \(v_{i} V\) is associated with attributes \((v_{i})\), and each edge \(e_{i} E\) is associated with attributes \((e_{i})\).

To fully exploit both the semantic information in \(\) and \(\), and the structural information in \(V\) and \(E\), we first design attributed random walks (RWs) based automated textualization program on \(\), where we can transform the attributed graph to the meaningful textual sequences \(=\{W_{i}\}_{i=1}^{N}\). Then, we fine-tune a graph-aware language model (LM) using the RW-based textual sequences \(\), find the optimized parameters \(\) of the LM, and extract embedding vectors from the LM. Finally, we apply these embeddings to \(_{1}\) and \(_{2}\) for performance evaluation.

### Masked Language Modeling

Masked Language Modeling (MLM) is a widely-used and highly effective technique for training large-scale language models . In the MLM task, 15% of the language tokens are randomly chosen for prediction. If the \(i\)-th token in a sequence \(W\) is chosen, it can be replaced by (1) the [MASK] token 80% of the time, (2) a random token 10% of the time, and (3) the unchanged \(i\)-th token 10% of the time. Then, token \(t_{i}\) will be used to predict the original token with following cross-entropy loss :

\[_{MLM}=-|}\ _{X}\ _{t_{i}} p(t_{i}|T_{i} ),\] (1)

where \(\) is a set of training examples, \(\) is the prediction set of the masked token, \(T_{ i}=\{t_{1},,t_{i-1},t_{i+1},,t_{L}\}\) is the surrounding token set of \(t_{i}\), and \(|T_{ i}|=L-1\).

## 4 Methodology

In this section, we present the proposed method WalkLM, which comprises two major components. The first component, the attributed random walk (RW) based textualization program, captures both topological structures and attribute information of the graph and composes corresponding text automatically. The second component, the graph-aware language model (LM) fine-tuning, leverages pre-trained LM to encode the complex semantics along with the graph structure. The overall model architecture is shown in Figure 2.

### Attributed RW-based Textualization Program

To model node/link attributes, traditional machine learning algorithms require a standard process of _vectorization_, which transforms different types of attributes into categorical or numerical features as model input. However, such a vectorization process removes the actual semantic meanings of the attributes, and it cannot unify different types of attributes (e.g., ages, sexes, time, etc.) in the same space. Inspired by the recent successes of LMs, we find it promising to leverage pre-trained LMs to understand the intricate semantics of complex heterogeneous attributes . The key idea is to perform _textualization_ instead of vectorization, that is, to transform different types of attributes into texts, which can then be modeled by the LMs in a uniform space. Therefore, we first design the following process for the textualization of individual entities in the graph (i.e., nodes and edges).

**Entity-level Textualization.** Inspired by a wide range of NLP tasks that leverage prompts to construct informative rule templates, we propose to textualize attributed graph entities via a rule-based program function \(()\) that automatically concatenates attribute values with the corresponding attribute names, as well as attributes and the corresponding entity id. For example, as shown in Figure 1, for a patient node \(v_{i}\) with attributes \((v_{i})=\{age:35,\ sex:,\ pid:\}\), the texualization program will convert it into \((v_{i})\) = < A \(35\)-year-old female patient P246 >. For edges, in this work, we only consider simple relational edges such as _has_ and _including_, which are already texts, but our framework is readily extensible to edges with more complex attributes.

For attributed graphs, after modeling the complex attributes of individual entities, the next challenge would be to model the flexible graph topological structures. To this end, we propose to utilize the

Figure 2: **The overall framework of WalkLM: (a) An attributed random walk (RW) based textualization program obtains attributed RWs and composes roughly meaningful textual sequences directly from the attributed RWs. (b) A pre-trained general LM is fine-tuned in a graph-aware fashion using the generated textual sequences to produce generic graph embeddings. (c) The learned graph embeddings can be applied to various downstream tasks.**powerful and efficient tool of RWs, as the foundation for the textualization of graphs. Specifically, we design the following process:

**Walk-level Textualization.** We first initiate an attributed RW \(W\) by randomly selecting a node \(v_{0}\) and attaching the textualized node information \((v_{0})\) to \(W\). Then, we extend \(W\) by randomly selecting an edge \(e_{1}\) starting from \(v_{0}\) as in a standard RW with a uniform probability as 1 divided by the number of out-edges of \(v_{0}\), with its terminating node \(v_{1}\), and appending the corresponding texts \((e_{1})\) and \((v_{1})\) to \(W\). We can keep adding edges and nodes until the random walk terminates, such as based on a termination probability \(\). The final textual sequence \(W=\{(v_{0}),(e_{1}),(v_{1}),,(v_{L-1}),(e_{L}),(v_{L})\}\), which corresponds to an actual attributed random walk with the length of \(2L+1\) on the graph, will be a roughly meaningful sentence, such as the one shown in Figure 1(c).

After performing the RW for \(N\) times, we can obtain \(N\) attributed RWs as \(=\{W_{i}\}_{i=1}^{N}\), which constitutes a graph-aware corpus for training LMs without any downstream task supervision.

**Discussion.** We believe that the above proposed attributed RW-based textualization program is effective in capturing both complex attribute information and flexible topological structures of the graph in an unsupervised manner. Such ability arises from two critical properties: (1) Random walks are known to be capable of providing characteristic graph traits and reconstructing network proximity of nodes . To be specific, it has been proven that the distribution of random walks starting at a particular node, which can be estimated with sufficient numbers of random walks, can sufficiently preserve the subgraph structure around the node. This means a sufficient amount of attributed RWs from different nodes can well reflect the topological structures of graphs. (2) Our textualization program completely preserves the attributes of nodes and edges, as well as the whole RWs, and it presents such information as meaningful texts, which is natural for LMs to comprehend. Moreover, RWs are known to be efficient and highly parallelizable, where numerous threads can run simultaneously to generate large amounts of RWs . Note that, we only need to perform the rule-based textualization once for every node during the pre-processing stage, which is also efficient and highly amenable to parallelization.

### Graph-Aware LM Fine-Tuning

Despite the robust generalizability of LMs, fine-tuning remains a necessary step , which allows the general LM to adapt its broad language capabilities to the specificities of the different attributed graphs.

As one of the mainstream language modeling techniques, masked language modeling (MLM) is proven to sufficiently utilize textual semantic information for further fine-tuning LMs . To achieve the balance between effectiveness and efficiency, we propose a graph-aware LM fine-tuning mechanism with knowledge distillation . Specifically, we adopt a general LM DistilRoBERTa (abbr. DRoBERTa)3 as our starting point for fine-tuning, where RoBERTa is a widely used successor of BERT . Note that, DRoBERTa can further reduce the size of the original RoBERTa model by 40% and achieve 60% faster training while retaining 97% capacity of RoBERTa's language understanding . Then, we feed the attributed RW \(W\) to the LM tokenizer and obtain the corresponding token list \(=\{t_{1},t_{2},,t_{K},<>_{1},<>_{2}, ,<>_{||}\}\), where \(t_{i}\) denotes the unmasked token, and \(<>_{i}\) denotes the token chosen for prediction. In this way, we create a training example \(X_{i}= W,\) for fine-tuning, where \(=\{X_{1},X_{2},,X_{N}\}\) is the whole training dataset. We adopt the cross-entropy loss as the fine-tuning objective , which is formulated as follows:

\[_{FT}()=-|}\ _{X_{i} }[_{t_{k}^{*}},t_{k}^ {*}))}{_{t}(Sim(t_{k},t))}],\] (2)

where \(\) is the learnable parameters of our graph-aware LM, \(\) is the ground-truth set of the masked token, \(\) is the token vocabulary, \(t_{k}\) is the prediction token, \(t_{k}^{*}\) is the ground-truth token, and \(Sim(t_{i},t_{j})\) is the similarity scoring function between \(t_{i}\) and \(t_{j}\).

After obtaining the fine-tuned LM based on MLM, we can extract generic graph embeddings (e.g., node embeddings based on node name). For example, we can access the representation of disease embedding in Figure 1(c) via extracting the embedding of Epistaxis.

**Complexity Analysis.** The time complexity of fine-tuning is \((Iter|N| l_{avg}^{2} d)\), where \(Iter\) is the number of iterations of training, \(N\) is the number of training examples, \(l_{avg}\) is the average length of input textual sequences for the LM and \(d\) is the dimension of embedding. We infuse global graph structure knowledge into the LM to distinguish similar positions instead of negative sampling, making it possible to fine-tune more efficiently .

**Model Extension.** Our framework is a fundamental approach to integrate LMs and RWs for generic attributed graph embedding, which can choose different LMs according to different tasks and domains (shown in our experiments in Sec. 5.4) and generalize to recent large language models (LLMs) (e.g., InstructGPT , ChatGPT and GPT-4 ) via appropriate parameter-efficient training approaches such as LoRA . A full exploration of different LMs is orthogonal to the main contributions in this work, which is left as future work.

**Datasets Extension.** Our method introduces the novel process of textualization, which converts general attributed graphs into text-like sequence data. This process allows us to leverage the capabilities of pre-trained language models for graph representation learning. Note that, our method only requires some meaningful attributes on the graphs, which are available in most real-world graphs such as biological networks, social networks, and knowledge graphs. Some preliminary experimental results of graph classification and KG-related datasets are shown in Appendix A.1 and Appendix A.2.

### Various Downstream Tasks

In this work, we focus on node embeddings since they are most commonly studied for graph representation learning, and it is straightforward to extract node embeddings from the fine-tuned LM based on node names (or node IDs if the node has no meaningful name). However, WalkLM can also easily generate edge embeddings, by adding edge names (e.g., relation names) or edge IDs to the textualization process, and even obtain subgraph/graph embeddings via appropriate embedding aggregation mechanisms. The extracted embeddings can be directly used as fixed feature vectors to train downstream prediction models for tasks such as node classification or link prediction. Alternatively, these embeddings can also serve as initialization for more learnable embeddings in complex neural network models, which can be further updated according to the specific requirements of the downstream task.

## 5 Experiments

### Experimental Setup

**Datasets.** We conduct extensive experiments on two real-world datasets, PubMed 4 and MIMIC-III 5. PubMed contains a graph of genes, diseases, chemicals, and species. The nodes and edges are extracted according to . A relatively small fraction of diseases are grouped into eight categories. MIMIC-III contains a graph of diseases, patients, and visits, where nodes and relations are extracted from clinical records. Diseases are classified into nineteen categories according to ICD-9-CM 6. The detailed statistics are shown in Table 1.

**Competitors.** We compare our proposed WalkLM with ten graph-oriented baselines that are designed for heterogeneous information networks (HINs) or knowledge graphs (KG), which can handle different types of nodes and edges. We divided them into four groups as follows:

(1) RW-based methods: **Metapath2Vec** (abbr. M2V)  proposes to use user-defined meta-paths as guidance, so as to learn node embeddings on HINs. **HIN2Vec** carries out multiple prediction

   Dataset & \#attribute type & \#node type & \#node & \#link type & \# link & \#label & \#label node \\  PubMed & 8 & 4 & 63,109 & 10 & 244,986 & 8 & 454 \\ MIMIC-III & 10 & 3 & 32,267 & 4 & 559,290 & 19 & 4880 \\   

Table 1: Statistics of the datasets.

training tasks jointly based on a target set of relations to learn node embeddings and meta-paths on HINs.

(2) Relation learning-based methods: **ConvE** proposes to use 2D convolution over embeddings and multiple layers of non-linear features to model KGs. **ComplEx** handles a large number of binary relations using complex-valued embeddings on KGs. **SimKGC** proposes to elicit the implicitly stored knowledge from BERT and designs a text-based contrastive learning mechanism for knowledge graph completion.

(3) Supervised heterogeneous graph neural networks (HGNNs): **RGCN** proposes to apply GCN to model HINs or KGs. **HAN** proposes to learn the importance between a node and its meta-path based neighbors on HINs. **HGT** proposes to use each edge's type to parameterize the transformer-based self-attention architecture on HINs. For the above supervised HGNNs, we use link prediction loss introduced in GraphSAGE  to achieve unsupervised learning (i.e., without any node labels), following existing studies on HINs .

(4) Unsupervised HGNNs: **HeCo** proposes a co-contrastive learning mechanism for HGNNs. **SHGP** designs a self-supervised pre-training method for HGNNs.

**Settings.** We mainly compare ten algorithms under the setting of unsupervised graph learning. The full code for this work is available7. All the models are optimized through the Adam optimizer and

   Task &  &  \\  Dataset &  &  &  &  \\  Metric & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 & AUC & MRR & AUC & MRR \\  M2V & 15.35 & 20.27 & 19.69 & 29.24 & 74.53 & 89.58 & 75.05 & 88.32 \\  & (\(\)1.27) & (\(\)3.01) & (\(\)0.62) & (\(\)1.57) & (\(\)3.79) & (\(\)2.05) & (\(\)0.41) & (\(\)0.23) \\ HIN2Vec & 11.57 & 18.92 & 19.12 & 28.05 & 74.21 & 90.56 & 73.46 & 88.10 \\  & (\(\)1.23) & (\(\)2.78) & (\(\)1.32) & (\(\)1.44) & (\(\)5.49) & (\(\)1.06) & (\(\)0.41) & (\(\)0.14) \\ ConvE & 16.06 & 19.16 & 24.44 & 32.89 & 76.48 & 92.27 & 69.56 & 84.88 \\  & (\(\)3.69) & (\(\)4.00) & (\(\)1.28) & (\(\)0.86) & (\(\)4.31) & (\(\)0.57) & (\(\)0.36) & (\(\)0.25) \\ ComplEx & 13.93 & 18.27 & 9.82 & 21.39 & 79.81 & 91.79 & 63.86 & 81.40 \\  & (\(\)2.59) & (\(\)4.12) & (\(\)0.56) & (\(\)3.12) & (\(\)0.97) & (\(\)0.48) & (\(\)0.42) & (\(\)0.40) \\ SimKGC & 21.97 & 30.83 & 51.62 & 85.50 & 79.62 & 91.43 & 67.73 & 84.86 \\  & (\(\)3.51) & (\(\)3.10) & (\(\)1.81) & (\(\)1.52) & (\(\)2.72) & (\(\)0.48) & (\(\)1.69) & (\(\)0.54) \\ RGCN & 12.50 & 18.50 & 7.19 & 14.55 & 72.08 & 88.20 & 57.31 & 73.91 \\  & (\(\)2.36) & (\(\)1.41) & (\(\)0.77) & (\(\)3.25) & (\(\)1.13) & (\(\)0.47) & (\(\)0.71) & (\(\)0.57) \\ HAN & 15.29 & 16.95 & 6.98 & 14.73 & 70.57 & 87.89 & - & - \\  & (\(\)2.87) & (\(\)2.71) & (\(\)0.58) & (\(\)1.69) & (\(\)1.58) & (\(\)0.62) & - & - \\ HGT & 11.98 & 20.12 & 8.03 & 17.79 & 77.24 & 89.63 & 64.01 & 81.54 \\  & (\(\)2.23) & (\(\)3.89) & (\(\)0.87) & (\(\)0.83) & (\(\)3.50) & (\(\)0.84) & (\(\)0.36) & (\(\)0.56) \\ HeCo & 10.32 & 18.01 & 10.78 & 15.26 & 65.04 & 83.29 & 53.13 & 71.81 \\  & (\(\)1.12) & (\(\)0.87) & (\(\)0.41) & (\(\)1.52) & (\(\)1.26) & (\(\)0.72) & (\(\)0.47) & (\(\)0.35) \\ SHGP & 10.80 & 19.28 & 11.34 & 17.44 & 68.22 & 85.34 & 54.49 & 72.58 \\  & (\(\)3.03) & (\(\)0.91) & (\(\)1.29) & (\(\)1.49) & (\(\)2.71) & (\(\)0.48) & (\(\)0.33) & (\(\)0.24) \\  LM & 40.10 & 44.71 & 54.51 & 61.27 & 60.20 & 84.23 & 51.21 & 74.22 \\ (XRoBERTa) & (\(\)4.62) & (\(\)3.68) & (\(\)1.50) & (\(\)1.22) & (\(\)2.78) & (\(\)1.71) & (\(\)0.17) & (\(\)0.26) \\ LM & 59.43 & 61.53 & 70.26 & 72.67 & 51.71 & 80.54 & 50.66 & 72.36 \\ (GPT-2) & (\(\)4.73) & (\(\)3.43) & (\(\)1.43) & (\(\)0.90) & (\(\)3.67) & (\(\)2.49) & (\(\)0.74) & (\(\)0.86) \\ LM & 58.29 & 60.57 & 66.25 & 70.14 & 60.97 & 83.00 & 51.44 & 75.09 \\ (DRoBERTa) & (\(\)2.44) & (\(\)2.11) & (\(\)1.60) & (\(\)1.52) & (\(\)2.98) & (\(\)0.40) & (\(\)0.14) & (\(\)0.29) \\ LM & 13.83 & 22.70 & 14.32 & 24.59 & 72.35 & 88.86 & 58.62 & 78.78 \\ +RGCN & (\(\)0.73) & (\(\)3.25) & (\(\)0.87) & (\(\)1.17) & (\(\)3.43) & (\(\)1.46) & (\(\)0.50) & (\(\)0.10) \\ LM & 12.81 & 21.79 & 10.49 & 20.57 & 82.97 & 89.98 & 65.01 & 82.28 \\ +HGT & (\(\)1.22) & (\(\)3.54) & (\(\)0.41) & (\(\)0.97) & (\(\)3.91) & (\(\)0.88) & (\(\)0.20) & (\(\)0.30) \\  WalkLM & **60.42*** & **62.33*** & **75.16*** & **77.89*** & **85.65*** & **94.16*** & **82.15*** & **92.78*** \\  & (\(\)2.62) & (\(\)3.13) & (\(\)0.93) & (\(\)0.70) & (\( learning rate is searched in [1e-4, 1e-2]. The hyper-parameters of baselines are chosen carefully based on either grid search or their official source codes. For all the methods, we use a five-fold cross-validation for a more reliable evaluation of the model's performance. All the experiments are performed with two NVIDIA GTX 3090 Ti GPUs. For link prediction, we deploy the Large Margin Nearest Neighbor (LMNN) technique based on the embeddings generated by our WalkLM. Then we construct feature vectors for edges.

### Node Classification

**Main Results.** For node classification, we train a separate one-layer MLP classifier based on the learned embeddings on 80% of the labeled nodes and predict the remaining 20%. All the methods are trained in an unsupervised manner without classification labels. We evaluate WalkLM with Macro-F1 (across all labels) and Micro-F1 (across all nodes).

As shown in Table 2, our proposed WalkLM has superior performance, indicating the importance of leveraging both semantic and structural information in attributed graphs. WalkLM achieves 138.59% performance gains on PubMed over the second-best performance on average while achieving 39.37% average performance gains on MIMIC-III. Specifically, SimKGC achieves second-best performance by effectively employing text-based contrastive learning, which leverages BERT to capture a rich set of semantic information. Compared with SimKGC, WalkLM can effectively combine the complex semantic and graph structure information of attributed graphs, so as to accurately model the complex attributes of nodes. Although the HGNNs can naturally model attributes, their unsupervised training mechanisms likely do not align well with the downstream prediction task of node classification.

**Results in the Few-shot Setting.** Since one key challenge of node classification lies in the generalizability and adaptability of models , we design a few-shot setting to evaluate models in extending knowledge to unseen scenarios and adapting to new tasks with limited training data.

As shown in Table 3, our framework can stay strong with a small size of training data, where we win a 275.45% performance gain over the second-best performance on average. Through the novel textualization process that converts general attributed graphs into text-like sequence data, our proposed WalkLM can leverage the capabilities of modern language models for graph representation learning. With the extensive pre-training of LMs on broad text corpora, WalkLM can easily understand meaningful node attributes given a new graph, while the random walk strategy further allows it to capture graph structures. Consequently, WalkLM maintains superior performance even in the few-shot setting. Similar to the main results in Table 2, the ranking of baselines is fluctuating across datasets, where ConvE and M2V continue to exhibit promising performance. Note that ComplEx and HIN2Vec exhibit notable improvements in this setting, likely because HIN2Vec can also learn node representations based on random walks and ComplEx can capture fine interactions through complex-valued vectors, thus being more capable of capturing comprehensive node information before supervision.

### Link Prediction

For link prediction, we train all models with the randomly selected 80% links and evaluate towards the 20% held-out links. We use the Hadamard function to construct feature vectors for node pairs and train a two-layer MLP classifier on the 80% training links. We evaluate WalkLM with AUC (area under the ROC curve) and MRR (mean reciprocal rank). Note that, HAN cannot predict links on MIMIC-III for its restriction to embed only one type of node at a time, and thus it cannot predict links between different types of nodes on MIMIC-III .

   Dataset &  &  \\  Setting &  &  &  &  &  &  \\  Metric & Ma-F1 & Mi-F1 & Ma-F1 & Mi-F1 & Ma-F1 & Mi-F1 & Ma-F1 & Ma-F1 & Mi-F1 & Ma-F1 & Ma-F1 & Ma-F1 & Mi-F1 \\  ComplEx & 9.31 & 12.51 & **10.32** & 18.26 & 10.12 & 15.94 & 2.82 & 5.29 & 2.00 & 3.03 & 3.87 & 9.26 \\ M2V & 9.86 & 13.42 & 10.27 & 12.56 & 12.97 & 14.98 & 5.83 & 8.72 & 3.91 & 5.11 & 3.40 & 4.49 \\ ConvE & 13.23 & 13.48 & 8.84 & 10.93 & 11.25 & 13.53 & 5.88 & 6.75 & 5.61 & 7.24 & 6.31 & 7.69 \\ RGCN & 9.34 & 11.02 & 8.57 & 10.58 & 10.84 & 13.43 & 4.97 & 5.82 & 5.43 & 6.28 & 5.22 & 5.73 \\ HN2Vec & 8.46 & 10.54 & 9.04 & 12.79 & 10.96 & 12.90 & 5.79 & 5.72 & 10.83 & 4.90 & 5.72 & 3.57 & 4.91 \\ SHGP & 8.94 & 12.79 & 9.12 & 11.73 & 10.53 & 15.14 & 4.12 & 6.35 & 5.36 & 6.58 & 4.47 & 5.34 \\ WalkLM & **28.09*** & **30.94*** & **32.11*** & **35.35*** & **35.41*** & **37.68*** & **23.33*** & **27.96*** & **34.19*** & **40.49*** & **41.12*** & **46.83*** \\   

Table 3: Node classification results (%) in the few-shot setting with Macro-F1 (abbr. Ma-F1) and Micro-F1 (abbr. Mi-F1) metrics.

As shown in Table 2, our fine-tuned WalkLM demonstrates outstanding performance in uncovering latent associations among nodes in attributed graphs. In general, WalkLM outperforms all ten baselines with an average of 5.97% performance gain over the second-best performance, showing that our proposed framework can learn accurate edge representation for link prediction. As a RW-based method, M2V can effectively employ meta-path-guided random walks to capture topological information and trace meta-path to understand relations between nodes. As relation-learning methods, ConvE and CompIEx design different deep neural models to evaluate triplets. ConvE can achieve good performance by using convolutions over embeddings to mine relations between entities. CompIEx can capture relations via complex-valued embeddings, so as to better represent the complex inherent relations among entities. Compare to M2V, ConvE, and ComplEX, WalkLM can effectively capture the complex relations by providing characteristic graph traits and reconstructing network proximity of nodes that inherit from RWs. On the other hand, the unsupervised HGNNs, especially HeCo and SHGP, perform rather poorly, again because their training mechanisms are not aligned with the link prediction task. Such observation is consistent with the results in the recent work , showing the heterogeneous approaches that only preserve certain-type entities fail to capture accurate representations for all kinds of nodes.

### Ablation Studies

To better understand our proposed techniques, we closely study our framework by selecting different LMs and varying the graph-aware LM fine-tuning mechanism.

Compared with the graph-based baselines, the LM-based models (e.g., LM (XRoBERTa8), LM (GPT-29), and LM (DRoBERTa10)) are able to learn accurate and rich node attributes. However, it is difficult for them to mine the relations between nodes in the attributed graph, where all of them perform worse than the RW-based and relation learning-based methods in the link prediction task. Considering the overall performance of the above three LMs on two different tasks and the goal of learning graph embedding, we choose LM (DROBERTa) as our starting point for fine-tuning.

Furthermore, we show that LM can further effectively integrate with existing heterogeneous graph algorithms, such as LM + RGCN and LM + HGT, resulting in a notable performance enhancement over their individual methods. Compared with LM + RGCN and LM + HGT, our proposed graph-aware LM fine-tuning can achieve the largest improvement gains based on the chosen LM (DRoBERTa) in both node classification and link prediction tasks, showing the effectiveness of capturing topological information together with semantics in modeling attributed graphs. The detailed analysis of the ablation studies is shown in Appendix A.3.

### Hyper-parameter Studies

In this subsection, we investigate the model sensitivity on the number of sampled walks \(N\) and the termination probability \(\), which are the major hyper-parameters in WalkLM. For the space limitation, we show results on PubMed in Figure 3 and the results on MIMIC-III in Appendix A.4. Overall, WalkLM is not sensitive to the two hyper-parameters, where its performance increases slowly with \(N\) and \(\). Note that, too small \(N\) or large \(\) can cause the textualization data \(|C|\) to lose

Figure 3: Analysis of the number of sampled walks \(N\) and the termination probability \(\).

sufficient information, while too large \(N\) or small \(\) lead to extensive \(|C|\) and increase computational costs for fine-tuning. Setting \(N\) around \(3 10^{5}\) and \(\) around \(0.05\) seems appropriate to generate sufficient textual sequences, which can achieve a good balance of performance and efficiency. We additionally investigate the model's sensitivity to the quantity of language model masking samples, aiming to elucidate the parameter's influence on the performance of downstream tasks. The detailed experimental results are shown in Appendix A.4.

### Visualization

For an intuitive comparison, we visualize the embedding space of different types of nodes which are learned by M2V, ConvE, HGT, and our WalkLM, respectively. Specifically, we select gene/disease nodes on PubMed and patient/visit/disease nodes on MIMIC-III. The embeddings are further transformed into the 2-dimensional Euclidean space via the t-SNE algorithm . The nodes and links are both colored according to their types.

As shown in Figure 4, M2V, ConvE, and HGT have blurred boundaries and even overlaps between different types of nodes, which are hard to distinguish. Our WalkLM shows the clearest boundaries between different types of nodes and the best within-type compactness, which indicate it can automatically organize heterogeneous nodes in a uniform space. Moreover, by connecting different types of nodes according to the relations in the data, we find that WalkLM can provide more discriminate distributions for different types of relations than others. The visualizations clearly demonstrate the advantages of WalkLM in capturing both attribute semantics and topological structures on graphs.

## 6 Conclusion

In this paper, we propose a novel uniform language model fine-tuning framework for attributed graph embedding. The proposed WalkLM consists of two key modules, which encapsulate both attribute semantics and graph structures and obtain unsupervised generic graph representations. We conduct extensive experiments to demonstrate the superior effectiveness of WalkLM against state-of-the-art baselines. For future work, it is intriguing to further design more sophisticated techniques and empirical evaluations toward the leverage of LMs and generalize our work to modern LLMs.

## 7 Acknowledgments

This work was supported in part by the National Natural Science Foundation of China (No. 6230071268); Fujian Provincial Youth Education and Scientific Research Project under Grant JAT220811. Carl Yang was not supported by any fund from China.

Figure 4: Visualization of different types of node embeddings on PubMed and MIMIC-III.