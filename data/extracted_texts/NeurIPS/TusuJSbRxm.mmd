# Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear \(q^{\pi}\)-Realizability and Concentrability

Trajectory Data Suffices for Statistically Efficient Learning in Offline RL with Linear \(q^{}\)-Realizability and Concentrability

Volodymyr Tkachuk

University of Alberta, Edmonton, Canada

&Gellert Weisz

Google DeepMind, London, UK

Csaba Szepesvari

Google DeepMind, Edmonton, Canada

University of Alberta, Edmonton, Canada

###### Abstract

We consider offline reinforcement learning (RL) in \(H\)-horizon Markov decision processes (MDPs) under the linear \(q^{}\)-realizability assumption, where the action-value function of every policy is linear with respect to a given \(d\)-dimensional feature function. The hope in this setting is that learning a good policy will be possible without requiring a sample size that scales with the number of states in the MDP. Foster et al. (2021) have shown this to be impossible even under _concentrability_, a data coverage assumption where a coefficient \(C_{}\) bounds the extent to which the state-action distribution of any policy can veer off the data distribution. However, the data in this previous work was in the form of a sequence of individual transitions. This leaves open the question of whether the negative result mentioned could be overcome if the data was composed of sequences of full trajectories. In this work we answer this question positively by proving that with trajectory data, a dataset of size \((d,H,C_{})/^{2}\) is sufficient for deriving an \(\)-optimal policy, regardless of the size of the state space. The main tool that makes this result possible is due to Weisz et al. (2023), who demonstrate that linear MDPs can be used to approximate linearly \(q^{}\)-realizable MDPs. The connection to trajectory data is that the linear MDP approximation relies on "skipping" over certain states. The associated estimation problems are thus easy when working with trajectory data, while they remain nontrivial when working with individual transitions. The question of computational efficiency under our assumptions remains open.

## 1 Introduction

We study the offline reinforcement learning (RL) setting, where the objective is to derive a near-optimal policy for an \(H\)-horizon Markov decision process (MDP) using _offline data_. This contrasts with the online RL paradigm, where learners interact directly with an MDP - or its simulator - to collect new data. Offline RL is especially relevant when acquiring new data guided by the learner is infeasible or ill-advised for safety reasons.

Deriving a near-optimal policy is only possible from offline data that covers the MDP well enough. One way to formalize this as an assumption, which we adopt for this work, is called _concentrability_. This assumption posits that the offline data sufficiently covers the distribution of state-action pairs that are accessible through running any policy. Challenges also arise in MDPs characterized by large or infinite state spaces. In such scenarios, an efficient learner's data requirements cannot scale with the state space size. An approach to remove state space dependence is to assume that the state-action value function of any policy can be linearly represented using a \(d\)-dimensional feature map, an assumption known as _linear \(q^{}\)-realizability_.

While linear \(q^{}\)-realizability facilitates efficient online RL (Weisz et al., 2023), its applicability has been limited in offline contexts. For instance, Foster et al. (2021) proves that no learning algorithm can derive an \(\)-optimal policy under linear \(q^{}\)-realizability and concentrability bounded by \(C_{}\), with a \((d,H,C_{},^{-1})\) number of samples. However, their result does not apply to _trajectory data_, where the offline data contains full sequences of state, action, and reward tuples obtained by following some policy from the initial state to the terminal state. The following problem is left open:

_"Does there exist an efficient learner that outputs an \(\)-optimal policy, under the assumptions of linear \(q^{}\)-realizability, concentrability, and trajectory data?"_

Our findings affirmatively answer this question in terms of sample complexity, highlighting a notable distinction in the requirements for trajectory data versus general offline data for effective learning. This underscores the practical value of accumulating trajectory data whenever feasible.

## 2 Related Works

In Table 1 we provide a comparison of our result to the other works in offline RL discussed below.

**Lower bounds:** As we have already discussed in Section 1, the work of Foster et al. (2021) shows a lower bound that depends on the size of the state space (in the same setting as ours), except they do not assume access to trajectory data. The work by Jia et al. (2024) is perhaps the most relevent to ours. They show an exponential lower bound in the horizon for policy evaluation, under the assumptions of trajectory data, concentrability, and a _restricted_ linear \(q^{}\)-realizability where the value function of only the target policy is linear. While we anticipate that evaluating policies (their focus) is no more difficult than optimizing policies (our focus), our \(q^{}\) realizability is for all memoryless policies (Assumption 1), while theirs is restricted to the target policy. Zanette (2021) shows an exponential lower bound in terms of the feature dimension \(d\), under linear \(q^{}\)-realizability, and various other structural assumptions; however, their setting would result in a concentrability coefficient larger than the size of the state space. Wang et al. (2020), Amortila et al. (2020) show a lower bound that is exponential in the horizon, under linear \(q^{}\)-realizability. However, they use a \(_{}\)_lower bound_ condition, which requires a lower bound on the minimum eigenvalue \(_{}\) of the expected covariance matrix used for least-squares estimation. This is seen as a weaker condition than ours, as it only posits good coverage in terms of the feature space, not the (possibly much richer) state-action space.

**Upper bounds:**Chen and Jiang (2019), Munos and Szepesvari (2008) present efficient algorithms under concentrability and _Bellman completeness_, an assumption that the Bellman optimality operator outputs a linearly realizable function when its input is linearly realizable. As linear \(q^{}\)-realizability does not imply Bellman completeness (Zanette et al., 2020), these results do not transfer

   & &  & \\ Work & Task & Data & Structural & Result \\   (Xiong et al., 2022) & \(\)-opt & \(_{}\) lower bound & Linear MDP & ✓ \\  (Chen and Jiang, 2019) & \(\)-opt & Conc & Bellman complete & ✓ \\  (Xie and Jiang, 2021) & \(\)-opt & Strong Conc & \(q^{}\) & ✓ \\  (**This work**) & \(\)-opt & Conc \& Traj\_data & \(q^{}\) & ✓ \\  (Foster et al., 2021) & \(\)-opt or \(\)-eval & Conc & \(q^{}\) & x \\  (Wang et al., 2020) & \(\)-eval & \(_{}\) lower bound & \(q^{}\) & x \\  (Jia et al., 2024) & \(\)-eval & Conc & Restricted \(q^{}\) & x \\  

Table 1: Notation is defined as: \(\)-opt = policy optimization, \(\)-eval = policy evaluation, Conc = Concentrability, \(q^{}\) = linear \(q^{}\)-realizability, Traj = Trajectory, ✓ = \((d,H,C_{},1/)\) sample complexity, x = exponential lower bound in terms of one of \(d,H,C_{}\).

to our setting. Xie and Jiang (2021) show an upper bound under linear \(q^{}\)-realizability, albeit, using a stronger notion of data coverage than concentrability, which we call _strong concentrability_. The work of Xie et al. (2021, 2022) give data-dependent sample complexity bounds that hold under both Bellman completeness and linear \(q^{}\)-realizability even in the absence of explicit data coverage assumptions. Jin et al. (2021) assume a general function approximation setting and also provide data-dependent bounds, while Duan et al. (2020), Xiong et al. (2022) show upper bounds for _linear MDPs_ (a stronger assumption than linear \(q^{}\)-realizability (Zanette et al., 2020)) with the \(_{}\) lower bound condition.

## 3 Setting

Throughout we fix the integer \(d 1\). Let \(^{d}\) be the \(d\)-dimensional, all zero vector. For \(L>0\), let \((L)=\{x^{d}:\|x\|_{2} L\}\) denote the \(d\)-dimensional Euclidean ball of radius \(L\) centered at the origin, where \(\|\|_{2}\) denotes the Euclidean norm. The inner product \( x,y\) for \(x,y^{d}\) is defined as the dot product \(x^{}y\). Let \(\{B\}\) be the indicator function of a boolean-valued (possibly random) variable \(B\), taking the value \(1\) if \(B\) is true and \(0\) if false. Let \(_{1}(X)\) denote the set of probability distributions over the set \(X\). Let \(_{B}\) denote the expectation of random variable \(B\) under distribution \(\). For integers \(i,j\), let \([i]=\{1,2,,i\}\) and \([i:j]=\{i,,j\}\). For a symmetric matrix \(M^{d d}\) we write \(_{}(M)\) and \(_{}(M)\) for its minimum and maximum eigenvalue.

The environment is modeled by a finite horizon Markov decision process (MDP). Fix the horizon to \(H\). This MDP is defined by a tuple \((,,P,)\). Here, the state space \(\) is finite1, and organized by stages: \(=_{h[H+1]}_{h}\), starting from a designated initial state \(s_{1}\) (\(_{1}=\{s_{1}\}\))2, and culminating in a designated terminal state \(s_{}\) (\(_{H+1}=\{s_{}\}\))3. Without loss of generality, we assume \(_{h}\) and \(_{h^{}}\) for \(h h^{}\) are disjoint sets. Define the function \(:[H+1]\), such that \((s)=h\) if \(s_{h}\). The action space \(\) is finite. The transition kernel is \(P:(_{h[H]}_{h})_{1}( )\), with the property that transitions occur between successive stages. Specifically, for any \(h[H]\), state \(s_{h}_{h}\), and action \(a\), \(P(s_{h},a)_{1}(_{h+1})\). The reward kernel is \(:_{1}()\). So that the terminal state \(s_{}\) has no influence on the learner we force the reward kernel to deterministically give zero reward for all actions \(a\) in \(s_{}\) (i.e. \((s_{},a)(r)=\{0=r\}\)). An agent interacts with this environment sequentially across an episode of \(H+1\) stages, by selecting an action \(a\) in the current state. The environment (except at stage \(H+1\)) then transitions to a subsequent state according to \(P\) and provides a reward in \(\) as specified by \(^{4}\).

We define an agent's interaction with the MDP through a _policy_\(\), which assigns a probability distribution over actions based on the history of interactions (including states, actions, and rewards). For this work, we restrict policies to be _memoryless_, that is, their action distribution depends solely on the most recent state in the history. The set of all memoryless policies is \(=\{::_{1}()\}\). For \(\), we write \((a|s)\) to denote the probability \((s)\) assigns to action \(a\). For deterministic policies only (i.e., those that for each state place a unit probability mass on some action) we sometimes abuse notation by writing \((s)\) to denote \(_{a}(a|s)\). Starting from any state \(s\) within the MDP and using a policy \(\) induces a probability distribution over trajectories, denoted as \(_{,s}\). For any \(a\), \(_{,s,a}\) is the distribution over the trajectories when first action \(a\) is used in state \(s\), after which policy \(\) is followed. Specifically, for some \(h[H+1]\) and \((s,a)_{h}\), we write \(_{,s,a}\) to denote that \(=(S_{h},A_{h},R_{h},,S_{H+1},A_{H+1},R_{H+1})\) for a random trajectory that follows the distribution specified by \(_{,s,a}\), that is, \(S_{h}=s\), \(A_{h}=a\), \(A_{i}(S_{i})\) for \(i[h+1:H+1]\), \(S_{i+1} P(S_{i},A_{i})\) for \(i[h:H]\), and \(R_{i}(S_{i},A_{i})\) for \(i[h:H+1]\). Writing \(_{,s}\) has an analogous meaning, with the only difference being that \(A_{h}\) is not fixed,and instead \(A_{h}(S_{h})\). For \(h[H+1]\), we write \(_{,s}^{h}\) (and \(_{,s,a}^{h}\)) for the marginal distribution of \((S_{h},A_{h})\) (i.e., the state-action pair of stage \(h\)) under the joint distribution of \(_{,s}\) (and \(_{,s,a}\)).

For \(1 t t^{} H+1\), we use the notation \(x_{t:t^{}}=(x_{u})_{u[t:t^{}]}\) throughout, except when \((x_{u})_{u[t:t^{}]}\) are a sequence of scalar rewards. In that case, for convenience, we write \(r_{t:t^{}}=_{u=t}^{t^{}}r_{u}\) and \(R_{t:t^{}}=_{u=t}^{t^{}}R_{u}\). The state-value and action-value functions \(v^{}\) and \(q^{}\) are defined as the expected total reward along the rest of the trajectory while \(\) is used:

\[v^{}(s)=_{,s,a}}{}R_{ {stage}(s):H}s q^{}(s,a)= _{,s,a}}{}R_{(s):H}(s,a) \,.\]

Let \(^{}\) be an optimal policy, satisfying \(q^{^{}}(s,a)=_{}q^{}(s,a)\) for all \((s,a)\). Let \(q^{}(s,a)=q^{^{}}(s,a)\) and \(v^{}(s)=_{a}q^{}(s,a)\) for all \((s,a)\). By definition, we have

\[v^{}(s_{})=v^{}(s_{})=0 q^{}(s_{},a)=q^{}(s_{},a)=0,a\,.\] (1)

### Assumptions and Problem Statement

A feature map is defined as \(:(L_{1})\) for some \(L_{1}>0\). The representative power of a feature map for an MDP is described by the following assumption:

**Assumption 1** (\((,L_{2})\)-Approximately Linear \(q^{}\)-Realizable MDP).: _For some \( 0,L_{2}>0\), assume that the MDP (together with a feature map \(\)) is such that_

\[_{}_{_{h}(L_{2})}_{(s_{h},a_{h}) _{h}}|q^{}(s_{h},a_{h})-(s_{h},a_{h} ),_{h}|h[H+1]\,.\]

_For any \(h[H+1]\), let \(_{h}:(L_{2})\) be a mapping from policies to parameter values \(_{h}\) that attain the \(\) in the above display. For \(h=H+1\), we restrict this mapping to \(_{H+1}()=\), which satisfies the above display by definition. We write \(_{h:t}()\) for \((_{h}(),,_{t}())\)._

We also make the following assumptions on the offline data (the relationship to non-trajectory data and the negative result by Foster et al. (2021) is discussed in Appendix B) :

**Assumption 2** (Full Length Trajectory Data).: _Assume the learner is given a dataset of full length trajectories5 and corresponding features of size \(n 1\):_

\[(^{1},,^{n})(( (s_{h}^{1},a))_{h[H],a},,((s_{h}^{n},a))_{h[ H],a}),\]

_where for some "data collection policy"\(^{0}\) unknown to the learner, \((^{j})_{j=1}^{n}\) are independent samples from \(_{^{0},s_{1}}\) where \(^{j}=(s_{i}^{j},a_{i}^{j},r_{i}^{j})_{t[H+1]}\). To simplify notation we write_

\[_{h}^{j}=(s_{h}^{j},a_{h}^{j})h[H],j[n]\,.\]

**Definition 1** (Admissible Distribution).: _A sequence of \(H\) state-action distributions \(=(_{h})_{h[H]}(_{1}(_{h})) ^{H}\) is admissible for an MDP if there exists a policy \(\) such that_

\[_{h}(s_{h},a_{h})=_{,s_{1}}^{h}(s_{h},a_{h})(s_{h},a_{h})_{h},\ h[H]\,.\]

Define the state-action occupancy measure of the data collection policy \(^{0}\) as \(=(_{h})_{h[H]}\) such that

\[_{h}(s_{h},a_{h})=_{^{0},s_{1}}^{h}(s_{h},a_{h})(s_{h},a_{h})_{h},\ h[H]\,.\]

**Assumption 3** (Concentability).: _Assume there exists a constant \(C_{} 1\), such that for all admissible distributions \(=(_{h})_{h[H]}\)_

\[_{h[H]}_{(s_{h},a_{h})_{h}}\{ (s_{h},a_{h})}{_{h}(s_{h},a_{h})}\} C_{}\,.\]

**Problem 1**.: _Let \(>0\). Under Assumptions 1 to 3, does there exist a learner, with access to only \(n=(1/,H,d,C_{},(1/),(1/L_{1}),(1/L_{2}))\) full length trajectories (as defined in Assumption 2), that outputs a policy \(\) such that, with probability at least \(1-\),_

\[v^{}(s_{1})-v^{}(s_{1})\,?\]Result

We resolve Problem 1 in the positive by defining a learner (Algorithm 1) that: selects parameters optimistically from modified MDPs that "skip over" certain states while preserving tight \(q\)-value estimation guarantees (achieved by solving Optimization Problem 1); then, outputs a greedy policy \(^{}\) (defined in line 3) over the selected parameters. This result is made formal in following theorem (proof in Section 5):

**Theorem 1**.: _Let \((0,H]\). Under Assumptions 1 to 3, if the number of full length trajectories \(n=(C^{4}_{}H^{7}d^{4}/^{2})\) and \(=}^{2}/(C^{2}_{}H^{5}d^{2}) \)6, then, with probability at least \(1-\), the policy \(^{}\) output by our learner (Algorithm 1) satisfies_

\[v^{}(s_{1})-v^{^{}}(s_{1})\,,\]

where \(,}\) and \(\) are the counterparts of \(,\) and \(\) from the big-Oh notation that hide polylogarithmic factors of the problem parameters \((1/,1/,H,d,C_{},L_{1},L_{2})\). The following subsections focus on introducing the theory needed to formally present our learner, giving intuition behind our learner, and presenting our learner.

### Background Theory

Our learner relies on the observation due to Weisz et al. (2023) that linearly \(q^{}\)-realizable MDPs are linear MDPs, as long as they contain no low-range states. The _range_ of a state is the largest possible regret from that state, that is, the largest difference in action-value that the choice of action in that state can make (up to misspecification):

\[(s)=_{}_{a,a^{}} (s,a,a^{}),_{(s)}()s\,,\] (2)

where \((s,a,a^{})=(s,a)-(s,a^{})\) is a notation we use to denote feature differences. Intuitively, the choice of actions in low-range states are unimportant, as

\[|v^{}(s)-q^{}(s,a)|(s)+2(s,a).\] (3)

As a warm-up, consider the example MDPs shown in Fig. 1. We will transform the linearly \(q^{}\)-realizable MDP on the left into a linear MDP on the right by "skipping over" the red low-range states. Let the features for both MDPs be \((s_{1},)=(1),(s_{3},)=(0.5),(,)=(0)\) otherwise. Then the left MDP is \((0,1)\)-approximately \(q^{}\)-realizable, with realizability parameter \(_{h}()=(1)\) for all \(h[H+1],\). However, it is not a linear MDP, since the rewards cannot be represented by a linear function of the features. To see this, notice that there exists no \(\) such that \((s_{1},a_{1}),=r(s_{1},a_{1})=1\) and \((s_{1},a_{2}),=r(s_{1},a_{2})=0.5\), since \((s_{1},a_{1})=(s_{1},a_{2})=(1)\). We modify this MDP on the left to "skip over" low-range red states, by automatically taking the first available action at such states, and summing up the rewards along skipped paths. This turns the MDP into the one on the right of Fig. 1, which is a linear MDP.

The key fact about linear MDPs that we will use is that for any function \(f:[0,H]\) (e.g., \(v\)-value approximators), and any \(h[H]\), there is some parameter \(_{h}^{d}\) so that for _any_\((s,a)_{h}\), \((s,a),_{h}\) gives the expectation of the reward plus \(f\)'s value on the next state. In our modified

Figure 1: The features for both MDPs are \((s_{1},)=(1),(s_{3},)=(0.5),(,)=(0)\) otherwise. **Left:** A \((0,1)\)-approximately \(q^{}\)-realizable MDP. **Right:** Linear MDP, obtained by skipping low range (red) states in the left MDP. Source: Figure 1 from (Weisz et al., 2023).

MDP this result transfers to the fact that the expected sum of rewards along a skipped path, plus \(f\)'s value on the next state after the skipped path, is linearly realizable. Before making this result formal in Lemma 4.2, we clarify the skipping behavior.

First, we address the fact that we need an approximate, parametric bound on \(()\) with a parameter count that is independent of \(||\). For \(h[2:H]\), let \(_{h}=\{_{h}()\,:\,\}(L_{2})\) be the (compact) set of parameter values corresponding to all policies. For all \(h[2:H]\), fix a subset \(_{h}_{h}\) of size \(|_{h}|=d_{0}:= 4d(d)+16\) that is the basis of a near-optimal design for \(_{h}\) (more precisely, satisfying Definition 3). The existence of such a near-optimal design follows from (Todd, 2016, Part (ii) of Lemma 3.9). Let \(=_{2:H}\), which we call the _true guess_. Now notice that \(\) where

\[=((L_{2}))^{[2:H][d_{0}]}\,.\] (4)

For \(G\) we will use the notation that \(G=G_{2:H}\), where \(G_{h}=(_{h}^{i})_{i[d_{0}]}(L_{2})^{d_{0}}\). Any \(G=G_{2:H}\) can be used to define an approximate, low parameter-count "version" of range that is completely specified by \((Hd^{2})\) parameters:

\[^{G}(s)=_{ G_{h}}_{a,a^{}} (s,a,a^{}),h[2:H],s _{h}\,.\] (5)

As shown in Proposition 4.5 of (Weisz et al., 2023), \(^{}\) can be used to bound the true range:

**Lemma 4.1**.: _For all \(h[2:H]\) and \(s_{h}\), \((s)^{}(s)\)._

Based on any \(G\), we are interested in simulating a modified MDP that "skips over" states \(s\) that have a low \(^{G}(s)\), by taking an action according to \(^{0}\), and presenting as the reward the summed up rewards along paths consisting of skipped states. This "modified MDP" only serves as intuition, and will not be formally defined or used in our formal arguments. Instead, we define the "skipping probability" parameter at state \(s\), with \(>0\) (set later in Eq. (35)), as

\[_{G}(s)=1&s_{1} _{H+1}^{G}(s)/\\ 2-^{G}(s)/&s_{1} _{H+1}/^{G}(s) 2/\\ 0&\] (6)

The skipping behavior is probabilistic7: it never skips for stages \(1\) and \(H+1\) (where \(^{G}\) is not defined); it always skips for ranges lower than some threshold, never skips for ranges higher than twice this threshold, and linearly interpolates between the two in between the thresholds. For \(h[H]\), and \(1 l h\), let \(=(s_{t},a_{t},r_{t})_{l t H+1}\) be any fixed trajectory that starts from some stage \(l\). Let \( F_{G,,h+1}_{1}([h+1:H+1])\) be the random stopping stage, when starting from state \(s_{h}\) and skipping subsequent states with probability \(_{G}()\). Formally, for \(t[h+1:H+1]\) let \(F_{G,,h+1}(=t)=(1-_{G}(s_{t}))_{u=h+1}^{t-1}_{G }(s_{u})\). We will often write \(F_{G,h+1}^{j}\) to denote \(F_{G,^{j},h+1}\) where \(^{j}=(s_{t}^{j},a_{t}^{j},r_{t}^{j})_{t[H+1]},j[n]\).

Next, we present a key tool derived from results of Weisz et al. (2023): as long as the skips are informed by the true guess, the resulting MDP is approximately linear (proof in Appendix C):

**Lemma 4.2** (Approximate Linear MDP under the true guess).: _Let \( 0,L_{2}>0\). Let \(M\) be an \((,L_{2})\)-approximately linear \(q^{}\)-realizable MDP (Assumption 1) with corresponding feature map \(\). Let \(_{2}=L_{2}(8H^{2}d_{0}/+1)\). Then, for each \(f:[0,H]\) with \(f(s_{})=0\), policy \(\), and stage \(h[H]\), there exists a parameter \(_{h}^{}(f)(})\) such that for all \((s,a)_{h}\),_

\[|*{}_{_{,s,a}} *{}_{ F_{G,,h+1}}[R_{h:-1}+f(S_{ })]-(s,a),_{h}^{}(f)|\,,\]

_where \(=(10H^{2}d_{0}/+1)\)._

### The Benefit of Trajectory Data

Our learner will heavily rely on the result presented in Lemma 4.2. We will need to learn good estimates of the parameters \(_{h}^{^{0}}(f)\), for any \(f:[0,H],h[H]\). However, to estimate a \(_{h}^{^{0}}(f)\)parameter well we will require least-squares targets that have bounded noise and expectation equal to \((s,a),_{h}^{}(f)\) for all \((s,a)_{h}\). Full trajectory data (Assumption 2) makes this possible. Each full length trajectory \(^{j}=(s_{i}^{j},a_{i}^{j},r_{i}^{j})_{t[H+1]}j[n]\) can be used to create the following least-squares target (which has the desired properties):

\[^{j}}{}[r_{h:-1}^{j}+fs_ {}^{j}].\]

Importantly, it is because we have full length trajectories that we can transform the data available to simulate arbitrary length skipping mechanisms.

### Intuition Behind our Learner

Next, we describe the high-level intuition and ideas behind our learner. Consider the "modified" MDP where low-range states are skipped. As the learner has access to trajectory data (Assumption 2), it can transform this data accordingly to simulate trajectories from the modified MDP. Any near-optimal policy for the modified MDP is also near-optimal for the original MDP (due to Eq. (3)). Thus, our previous linear realizability property (Lemma 4.2) allows for an offline RL version of the algorithm Eleanor(Zanette et al., 2020) to statistically efficiently derive a near-optimal policy for the modified MDP. Indeed, the optimization problem underlying Eleanor serves as a starting point for Optimization Problem 1, which is at the heart of our learner.

The challenge is that the true guess \(\) that Lemma 4.2 relies upon is not known to the learner. This means that the learner is not given any explicit information of what states to "skip over". To overcome this, we design a learner to output the policy \(^{}\) (defined in Algorithm 1) based on Optimization Problem 1, where the optimization problem considers all guesses for the possible values of \(\). For each \(G\), it considers the MDP that skips over low-range states when the range is calculated according to \(G\). It then calculates sets \(_{G,h}\) for each stage \(h\), that are guaranteed (with high probability) to include the parameter \(_{h}(^{}_{G})\) realizing \(q^{^{}_{G}}\) (where \(^{}_{G}\), defined in Eq. (15), is the optimal policy in the MDP with skipping based on \(G\)). We achieve this by defining \(_{G,h}\) backwards for \(h=H,H-1,,1\). By induction, if \(_{G,h+1},,_{G,H}\) all contain the desired parameter for their stage, then _some_ parameter sequence in the Cartesian product \(_{G,h+1}_{G,H}\) allows us to near-perfectly (up to some misspecification error) compute \(q^{^{}_{G}}\)-values of stages \(>h\). Therefore, the least-squares parameter based on this sequence will be near the true parameter for stage \(h\). Defining \(_{G,h}\) to be all least-squares predictors for sequences in the aforementioned Cartesian product, and \(_{G,h}\) to be unions of the confidence ellipsoids around these predictors ensures the true parameter \(_{h}(^{}_{G})\) realizing \(q^{^{}_{G}}\) for stage \(h\) is included in \(_{G,h}\). This argument is made precise in Lemma D.2.

There are two problems remaining. One is that some values of \(G\) considered by Optimization Problem 1 lead to skipping over important large-value states, degrading the performance of the best policy \(^{}_{G}\) available under that skipping. The other problem is that at the expense of making sure the true parameters are included in the sets \(_{G,h}\), these sets might become large, in the sense of containing parameters that lead to very different predictions. Avoiding the first problem would make \(v^{^{}_{G}}(s_{1})\) nearly as large as \(v^{}(s_{1})\). Avoiding the second problem would lead to tight \(q\)-value estimators, and therefore to \(v^{^{}_{G}}(s_{1})\) being nearly as large as \(v^{^{}_{G}}(s_{1})\), for a policy \(^{}_{G}\) that is greedy with respect to our hypothetically tight \(q\)-value estimator. A key idea is to reject from consideration any \(G\) that leads to \(q\)-estimations that are not sufficiently tight (Eq. (14)). The reason we can do this is because for \(G=\) we can show that this condition passes (with high probability), and therefore we do not reject \(\) (precise statement in Lemma D.3). We can show this since we have trajectory data (Assumption 2), allowing us to use least-squares targets of the form used in Eq. (12), which we know are linearly realizable when \(G=\) (discussed in Section 4.2). Finally, we resolve the first problem by selecting among these tight estimators the one that guarantees the highest policy value from \(s_{1}\), which can be no worse than the value guaranteed by the choice of \(G=\), which itself can be seen to be close to \(v^{}(s_{1})\).

### Learner

Next, we formally introduce our learner, at the heart of which lies Optimization Problem 1. We define various \(q\) and \(v\)-value estimators that we use. For \(x\), let \(_{[0,H]}x=\{0,\{H,x\}\}\).

[MISSING_PAGE_FAIL:8]

Proof.: \(v^{}(s_{1})-v^{^{}}(s_{1})\) can be decomposed into the following error terms.

\[v^{}(s_{1})-v^{^{}}(s_{1})=(s_{1})-v^{^{ }_{}}(s_{1})}_{}+_{}}(s_ {1})-v_{^{}_{1}}(s_{1})}_{}+_{1}}(s_{1})-v^{^{}}(s_{1})}_{}\,.\]

The remainder of the proof focuses on bounding these error terms. Following the intuition described in Section 4, showing that terms \(\) and \(\) are small can be seen as addressing the first problem of potentially skipping over large-value states, while showing that term \(\) is small can be seen as addressing the second problem of \(^{}\) being greedy w.r.t. to a potentially inaccurate estimates \(^{}_{1:H+1}\).

**Bounding \(=v^{}(s_{1})-v^{^{}_{}}(s_{1})\)**: This term cannot be too large since the range\({}^{}\) function is approximately correct (Lemma 4.1), and we only skip over states with low range\({}^{}\) (Eq. (6)), implying the action we take doesn't affect the value function much (Eq. (3)). In Appendix D.1 we formalize this intuition, and show the following result.

\[=v^{}(s_{1})-v^{^{}_{}}(s_{1}) H(2+2 )\,.\] (16)

**Bounding \(=v^{^{}_{}}(s_{1})-_{^{}_{1}}(s _{1})\)**: This term can be bounded by approximately zero due to Optimization Problem 1 being optimistic from the start state. First, note that \(v^{^{}_{}}\) is approximately equal to \(_{_{1}(^{}_{})}\) (Assumption 1). Then, in Lemma D.2 we show that \(_{1}(^{}_{})_{,1}\), and in Lemma D.3 we show that \(\) is a feasible solution to Optimization Problem 1. Since \((G^{},^{}_{1:H+1})\) is the solution to Optimization Problem 1, it holds that \(_{^{}_{1}}(s_{1})_{}(s_{1})\) for any \(_{G,1}\) where \(G\) is a feasible solution to Optimization Problem 1. Thus \(_{^{}_{1}}(s_{1}) v_{_{1}(^{}_{})}(s _{1})\). In Appendix D.2 we formalize this intuition, and show that with probability at least \(1-\),

\[=v^{^{}_{}}(s_{1})-_{^{}_{1}}(s_ {1})\,.\] (17)

**Bounding \(=_{^{}_{1}}(s_{1})-v^{^{}}(s_{1})\)**: To bound term \(\) we will first show in Lemma 5.1 that value estimates in terms of \(^{}_{h}\) and \(_{h}(^{}_{G^{}})\) are close with high probability for all \(h[H+1]\). This lemma allows us to relate \(_{^{}_{1}}\) to \(_{_{1}(^{}_{G^{}})}\) and then Assumption 1 relates \(_{_{1}(^{}_{G^{}})}\) to \(v^{^{}_{^{}}}\). We are then left with relating \(v^{^{}_{G^{}}}\) to \(v^{^{}}\). To do this we claim that \(^{}\) is an approximate policy improvement step w.r.t. \(v^{^{}_{G^{}}}\), which can be seen by recalling that \(^{}\) is greedy w.r.t. \(_{^{}_{1:H+1}}\), and as we mentioned a couple sentences ago, \(_{^{}_{h}}\) and \(_{_{h}(^{}_{G^{}})}\) are close for all \(h[H+1]\).

To formalize this intuition we begin by decomposing \(_{^{}_{1}}(s_{1})-v^{^{}}(s_{1})\) into the following error terms

\[_{^{}_{1}}(s_{1})-v^{^{}}(s_{1})=_{^{ }_{1}}(s_{1})-_{_{1}(^{}_{G^{}})}(s_{1},^{ }(s_{1}))+_{_{1}(^{}_{G^{}})}(s_{1},^{ }(s_{1}))-v^{^{}}(s_{1})\,.\] (18)

To bound \(_{^{}_{1}}(s_{1})-_{_{1}(^{}_{G^{}} )}(s_{1},^{}(s_{1}))\) we introduce a useful lemma (proof in Appendix F).

**Lemma 5.1**.: _There is an event \(_{2}\), that occurs with probability at least \(1-/3\), such that under event \(_{2}\), for all \(G\) that are feasible solutions to Optimization Problem 1, for all \(h[H]\), for all \((_{s,a})_{(s,a)_{h}}\) and \((_{s,a})_{(s,a)_{h}}_{G,h}^ {_{h}}\), and for all admissible distributions \(=(_{t})_{t[H]}\), it holds that_

\[*{}_{(S,A)_{h}}_{_{S,A}}(S,A )-_{_{S,A}}(S,A) _{_{1}(^{*}_{G^{}})}(s_{1},^{}(s_{1}))-v^{^{ }}(s_{1})\) in Eq. (18). To do this, first note that

\[_{_{1}(^{*}_{G^{}})}(s_{1},^{}(s_{1}))-v^{^{ }}(s_{1}) q^{^{*}_{G^{}}}(s_{1},^{}(s_{1}))-v^{ ^{}}(s_{1})+\,.\]

where the inequality holds since we have approximate linear \(q^{}\)-realizability (Assumption 1). To bound \(q^{^{*}_{G^{}}}(s_{1},^{}(s_{1}))-v^{^{}}(s_{1})\) notice that \(v^{^{}}(s_{1})=q^{^{}}(s_{1},^{}(s_{1}))\), which implies that

\[q^{^{*}_{G^{}}}(s_{1},^{}(s_{1}))-v^{^{}}(s_{1})=q^ {^{*}_{G^{}}}(s_{1},^{}(s_{1}))-q^{^{}}(s_{1},^ {}(s_{1}))=_{^{},s_{1}}}{ }v^{^{*}_{G^{}}}(S_{2})-v^{^{}}(S_{2})\,.\]

Next, we give a bound on \(_{_{^{},s_{1}}}v^{^{*}_{G ^{}}}(S_{2})-v^{^{}}(S_{2})\) (proof in Appendix D.3):

**Lemma 5.2**.: _Under event \(_{1}_{2}\), for any \(h[2:H+1]\), it holds that_

\[_{^{},s_{1}}}{}v ^{^{*}_{G^{}}}(S_{h})-v^{^{}}(S_{h}) 2(H-h+2)( +)\,.\]

Intuitively, the above lemma holds since \(^{}\) can be thought of as an approximate policy improvement step w.r.t. \(v^{^{*}_{G^{}}}\). To see this, recall that \(^{}\) is greedy w.r.t. \(_{^{*}_{1:H+1}}\) (line 3). Then, with Lemma 5.1, we can show \(_{^{*}_{h}}\) and \(_{_{h}(^{*}_{G^{}})}\)(which is close to \(v^{^{*}_{G^{}}}\) (Assumption 1)) are close for all \(h[H+1]\). The above bounds imply that under event \(_{1}_{2}\), which occurs with probability at least \(1-2/3\),

\[()=_{^{*}_{1}}(s_{1})-v^{^{}}(s_{1}) 2H( +)++\,.\] (19)

**Combining the Bounds:** To finish the proof we combine the bounds on all three terms (Eqs. (16), (17) and (19)), to get that under event \(_{1}_{2}_{3}\), which occurs with probability at least \(1-\),

\[v^{*}(s_{1})-v^{^{}}(s_{1}) H(2+2)++2H(+)++ 4(H+1)(++)\,.\]

To bound the above display by \(\) we set \(=/(12(H+1))<1\). If \(n=C^{4}_{}H^{7}d^{4}/^{2}\) and \(=}/\) (Eq. (26)), we show that \(=}C^{2}_{}H^{5/2}d^{2}/ \) (Eq. (44)). This implies that

\[4(H+1)(++)\,.\]

## 6 Limitations and Conclusions

In this work we resolved an open problem in the positive, by presenting the first statistically efficient learner (Section 4.4) that outputs a near optimal policy in the offline RL setting with approximate linear \(q^{}\)-realizability (Assumption 1), trajectory data (Assumption 2), and concentrability (Assumption 3). One limitation of this work is that we are not aware of any computationally efficient implementation of Optimization Problem 1, which is at the heart of our learner. As such, it is left as an open problem whether computationally efficient learning is possible in the setting we considered. Another limitation is that we are not sure if our statistical rate in Theorem 1 is optimal. Showing a matching lower bound or improving the rate is left for future work.

Another limitation of our work originates from our setting underpinning our result (Section 4), namely the three assumptions: approximate linear \(q^{}\)-realizability, trajectory data, and concentrability. Approximate linear \(q^{}\)-realizability requires the value function of all memoryless policies to be linear in a fixed and known \(d\)-dimensional feature map. While strictly weaker than the linear MDP assumption (Zanette et al., 2020), this assumption is still strong. Trajectory data requires full sequences of interactions with an environment to be collected by a single policy. For long horizon problems this can be practically challenging. Concentrability requires the state and action spaces to be well-covered. This can be challenging to guarantee since often the state and action spaces are unknown at the time of data collection. Further, since we require the trajectory data to be collected by a single policy, it may be the case that no single policy exists that covers the state and action spaces well, and a mixture of policies must be considered, which our current result does not immediately hold for. Although the assumptions appear strong, a justification for them is that under many variations of weaker assumptions (for instance: general data, or linear \(q^{}\)-realizability of only one policy, or only coverage of the feature space), polynomial statistical rates have been shown to be impossible to achieve by any learner (Table 1).

Since this work is focused on foundational theoretical research it is unlikely to have any direct and immediate societal impacts.