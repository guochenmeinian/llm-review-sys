# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

2022, Pan et al., 2023, Wan et al., 2024) and conflicts in embedded knowledge (Elazar et al., 2021, Lu et al., 2024). Retrieved conflicts arise during the inference stage when newly retrieved information contradicts the model's parametric memory, while embedded conflicts occur during the training stage due to discrepancies within the training text itself. To construct conflict-related datasets for controlled experiments, previous works primarily utilize word-level substitution (Longre et al., 2021, Zhou et al., 2023, Wang et al., 2023) or language model generation methods (Tan et al., 2024) to craft conflicts. Xie et al. (2024) combines these two approaches, eliciting parametric memory from LLMs and constructs more coherent and convincing conflict pairs. However, all these existing studies solely explored the conflicts between the embedded knowledge of LLMs and the retrieved contextual knowledge, leaving other conflict scenarios like the conflict within the models' encoded knowledge and the interplay between different conflict forms under-explored.

To fill in the existing research gap, we construct the ConflictBank, the first comprehensive benchmark for analyzing the models' behavior by simulating the knowledge conflicts encountered in the pre-training and inference stages. In the ConflictBank, we meticulously design three main conflict scenarios, i.e., inaccurate information (misinformation conflict) (Du et al., 2022, Pan et al., 2023, Zhou et al., 2024), knowledge changes over time (temporal conflict) (Lazaridou et al., 2021, Su et al., 2022), and the polysemic nature of language (semantic conflict) (Ansell et al., 2021, Sevgili et al., 2022). Specifically, we collect **2,863,205** claims from Wikidata and generate the evidence with the revised conflict claims to create a total of **7,453,853** claim-evidence pairs. Additionally, we construct **553,117** QA pairs for investigating the model behavior when facing conflicts. Unlike the previous datasets, ConflictBank can be employed to systematically evaluate the effects of knowledge conflict in retrieved knowledge, embedded knowledge, and their interactions. Based on the ConflictBank, we conduct pilot experiments on twelve LLMs across four model series and provide insights into their behaviors under different conflict scenarios. Our main contributions are summarized below:

* We present ConflictBank, the first comprehensive benchmark for knowledge conflicts, including 7M claim-evidence pairs and 553k QA pairs. Our benchmark covers three conflict causes in the real-world scenario, including misinformation, temporal, and semantic conflicts.
* ConflictBank can be utilized to conduct a series of experiments about knowledge conflicts, including conflicts in retrieved knowledge, embedded knowledge, and their interplay.
* We conduct in-depth pilot experiments on twelve LLMs across four model series and provide comprehensive analyses about model scales, conflict causes, and conflict types.
* To make the ConflictBank accessible for future research, we release a Python package that automates data loading, baseline evaluation, and training. Additionally, we have open-sourced all the models used in our analysis. We hope that ConflictBank could facilitate comprehensive studies on different conflict scenarios and contribute to the advancement of more reliable and trustworthy language models.

## 2 ConflictBank Benchmark

### Knowledge Conflicts Causes

Knowledge conflict within datasets can significantly diminish a model's accuracy, reliability, and trustworthiness (Longpre et al., 2021, Wang et al., 2023, Xie et al., 2023, Xu et al., 2024). In this paper, we identify and investigate three prevalent knowledge conflict causes:

* **Type 1: Misinformation Conflict** emerges from the presence of incorrect or misleading information in datasets, resulting in considerable confusion and misinterpretation (Schuster et al., 2021). This conflict usually occurs during data collection, introducing false narratives or misleading facts into the model and diminishing its factual accuracy.
* **Type 2: Temporal Conflict** occurs when knowledge changes or evolves over time (Lazaridou et al., 2021, Su et al., 2022, Huang et al., 2024, 2024). As new knowledge emerges, previous knowledge becomes outdated or obsolete, leading to inconsistencies regarding the same entity.
* **Type 3: Semantic Conflict** arises when words with multiple meanings cause ambiguity in interpretation (Sevgili et al., 2022). These conflicts stem from the polysemic nature of language, leading to misunderstandings as the same word conveys different meanings in different contexts.

### Extracting Facts from Wikidata

We utilize the Wikidata (Vrandei and Krotzsch, 2014) dump of April 02, 2024, as a massive and comprehensiveensive (Longpre et al., 2021; Pan et al., 2023; Xie et al., 2024) knowledge source to extract and construct facts. The information is structured by transforming knowledge triples and qualifiers into a quintuplet format: \((s,r,o,s_{d},o_{d})\), where \(s\) is the subject, \(r\) is the relation, \(o\) is the object, \(s_{d}\) is the description of \(s\), and \(o_{d}\) is the description of \(o\). To filter out overlapping and contradictory knowledge within Wikidata, knowledge triples with the same \((s,r)\) pair are selected only once. As relationship types and their representations are key factors for factual knowledge memorization (Mallen et al., 2023), we focus on the top 100 most frequent relations to ensure sufficient coverage, transforming \((s,r,o)\) into claims using templates for each relation. The used templates are shown in Table 4.

### Constructing Knowledge Conflict Claims

Based on the extracted knowledge triples, we substitute the entity with a same-type entity to construct conflict claims (Longpre et al., 2021). Specifically, we use the following strategies for three conflict types: (1) Misinformation conflicts simulate conflicts involving misinformation, such as fake news and rumors that contradict reality. We generate these conflicts by replacing \(o\) with \(o^{}\) in \((s,r,o,s_{d},o_{d})\), where \(o^{}\) is selected from other quintuplets sharing the same relation \(r\); (2) Temporal conflicts capture the discrepancies arising from changes in knowledge, highlighting the tension between outdated and newly updated information. To prevent conflicts with the LLMs updated parametric knowledge (Lazaridou et al., 2021), we incorporate a future time span into the claim, resulting in \((s,r,o^{},s_{d},o_{d},T_{s},T_{e})\), where \(T_{s}\) and \(T_{e}\) represent the start and the end timestamps, respectively; (3) Semantic conflicts depict situations where identical words convey entirely different meanings. To simulate such polysemous situations, we generate an additional description for the conflicting subject \(s^{}_{d}\) based on \((s,r,o^{},s_{d})\)3. The final modification is \((s,r,o^{},s^{}_{d},o_{d})\).

### Generating Diverse Evidence Texts

Previous works have proven the evidence generated from the powerful generative LLMs is more coherent compared to word-level editing methods (Xie et al., 2024). Therefore, we adopt LLMs to produce corresponding evidence for each claim. Since the description provides additional information that helps the model generate more accurate and relevant evidence (Shao et al., 2023), we utilize \((s_{d},o_{d})\) as part of the prompt to make up supporting evidence for the claim. To account

Figure 1: The pipeline of ConflictBank construction. (1) We extract facts from Wikidata and (2) transform them into conflict claims based on different causes, then (3) employ LLM to generate evidence in three text styles, and finally (4) apply three processes to control data quality: feature filtering, fact-evidence entailment checking, and conflict confirmation between evidence.

for the diversity of texts in each practical field in our taxonomy, we produce three types of textual styles: Wikipedia, book, and news. The textual styles of the generated evidence are finely controlled through corresponding prompts. We exhibit prompts and examples in Appendix E.

### Controlling Data Quality

In order to avoid ambiguity caused by the sparsity of knowledge bases and harvest a high-quality dataset, we perform the following steps to strictly clean the generated data. We provide detailed descriptions of each validation and training step, the running time for each processing step, and the resulting data quantities in Appendix B.

**Feature filtering:** Since LLMs proactively refuse to answer questions when they lack knowledge (Yang et al., 2023), the previous steps are inevitable to involve the response "refusal to answer" when fabricating the fictional information. Such responses cannot serve as credible evidence for specific conflict claims, thereby reducing the quality of the dataset. To address this, we manually identify and extract common features in these refusal responses, e.g., "I apologize" and "I can't." (Chen et al., 2021) and filter out all model-generated content containing these features.

**Fact-evidence entailment checking:** A gold piece of evidence should exhibit a strong correlation with its corresponding claim and effectively support it. To achieve this, we employ the state-of-the-art NLI model DeBERTa-V24 to rigorously assess the relationship between the generated evidence and the claims. We retain samples demonstrating strong fact-evidence entailment to enhance the authenticity of the conflicts within the dataset (Xie et al., 2024).

**Conflict confirmation between evidence:** We verify that each type of conflict evidence contradicts the default evidence. Specifically, we utilize SBERT5 to compute embeddings for all evidence and train a conflict confirmation classifier on the existing conflict dataset (Xie et al., 2024). This classifier evaluates whether two pieces of evidence conflict. To ensure reliability, we manually evaluate 200 random examples and observe over 95% accuracy. We filter out all evidence pairs identified as non-conflicting by the classifier to ensure dataset quality.

### Synthesizing Question-Answer Pairs

After the steps above, we manually sample 200 data pairs and employ 5 volunteers to evaluate them. The results indicate that our constructed dataset effectively reflects real-world conflict scenarios. Detailed information about this process is provided in Appendix B.5. Subsequently, we construct

Figure 2: Examples and data composition in ConflictBank. Evidence is styled as either Wikipedia, book, or news. Each question includes one default and three types of conflict evidence. ConflictBank contains 7,453,853 claim-evidence pairs and 553,117 QA pairs.

questions for the object by substituting the object in the claim. The model will be required to answer questions based on the provided evidence. Each question is constructed with four options. The first option is the default answer from the original claim. The second is the object we used for substitution when constructing the corresponding conflicting claim. The remaining two are similar but unrelated choices. We search for objects with the same quintuplet structure as o' but do not appear in the default claim or conflict claim to serve as these two options. We provide an example about _"Anne Hathaway"_ and our data composition in Figure 2.

## 3 Experiments

### Experimental Setup

ModelsTo explore the behavior of LLMs when encountering knowledge conflicts, we perform a comprehensive evaluation on 12 LLMs ranging from 0.5B to 70B. This evaluation covers the following four model series: Gemma (2B, 7B) [Team et al., 2024], LLaMA2 (7B, 13B, 70B) [Touvron et al., 2023], LLaMA3 (7B, 70B), Qwen1.5 (0.5B, 4B, 7B, 14B, 72B) [Bai et al., 2023a]. To investigate the impact of internal knowledge conflicts within the parametric memory, we continue pre-training three representative LLMs, including Qwen1.5-4B, Mistral-7B, and LLaMA3-8B.

Evaluation MetricsTo minimize randomness and facilitate evaluation [Hendrycks et al., 2021], we append the question to the input prompt and conclude with the "Answer:". The model then generates probabilities for the tokens "(A)", "(B)", "(C)" and "(D)". The option with the highest probability is selected as the predicted answer. Following Gekhman et al. , we identify QA pairs that models can correctly answer, both with and without additional default evidence, to represent the model's internal knowledge. We then examine how conflicts affect these QA pairs to understand their impact on model behavior. We measure the Original Answer Ratio (OAR) and the Counter Answer Ratio (CAR) to assess model performance [Longpre et al., 2021]. The Memorization Ratio (\(M_{R}\)) is then used to quantify how often LLMs rely on their parametric memory:

\[M_{R}=\] (1)

where higher memorization ratios indicate greater reliance on parametric memory, while lower ratios suggest more adoption of the constructed conflicting knowledge.

### Conflicts in Retrieved Knowledge

In this section, we examine LLMs' behavior in two retrieved knowledge conflict scenarios: (1) models are presented solely with external evidence that contradicts their parametric memory, and (2) models are given two pieces of external evidence, one matching their parametric knowledge and one conflicting with it6. We report the \(M_{R}\) to illustrate the performance of different models, varying in series and parameter size when faced with conflicting evidence pairs. The results of these two settings are shown in Figure 3 and Figure 4, respectively. We draw the following observations:

**LLMs are highly receptive to external evidence and often prefer evidence consistent with their internal beliefs.** As shown in Figure 3, all models exhibit memorization ratios below 50%, indicating that models are highly receptive to external evidence when it is the only evidence available, even when it conflicts with their parametric memory. However, as shown in Figure 4, all LLMs demonstrate significantly higher memorization ratios (over 50%) when parametric memory is also provided as evidence. These two above findings are consistent with previous work [Xie et al., 2024], confirming that LLMs are easily deceived by disinformation and indicate strong _confirmation bias_ when facing multiple pieces of conflicting evidence. We also conduct extensive experiments using one of the most advanced closed-source models (i.e., GPT-4o), by investigating the model's behavior in the same two scenarios. The results align with our findings above on open-source models, indicating that the representative closed-sourced model GPT-4 is also sensitive to semantic conflict. The results are shown in Table 6.

**LLMs are more sensitive to temporal and semantic conflicts.** In Figure 3, we observe that all models exhibit a lower \(M_{R}\) in temporal and semantic conflicts compared to misinformation conflicts,indicating higher sensitivity to these types of external conflicting knowledge. A similar trend is evident in models with larger parameter sizes when facing two pieces of conflicts in Figure 4. For example, in the LLAMA2-70B model, the \(M_{R}\) for temporal and semantic conflicts is lower than those for misinformation conflicts (i.e., 7.77% & 3.73% v.s. 9.45%). These findings suggest that implicit conflicts, which seem reasonable and closely related to the model's internal knowledge, cause more confusion than explicit factual errors.

**Larger models are more susceptible to conflicting knowledge.** In Figure 4, we observe a decrease in \(M_{R}\) as the parameter size increases within the same model series. For instance, in the LLAMA2 series, the \(M_{R}\) consistently decreases as the parameters increase from 7B to 13B to 70B, with reductions of 5.12% and 6.37% in an average of three conflict causes, respectively. These observations suggest that larger models exhibit increased susceptibility to conflicting knowledge, and this susceptibility becomes more pronounced as model size increases. Figure 5 further shows that models are susceptible to the order of evidence, with larger models tending to favor later pieces. For example, in the LLAMA2 series, when conflicting memory is placed later in the sequence, the 7B model has a \(M_{R}\) of 56.44%, while the 70B model's \(M_{R}\) drops to 38.97%. This phenomenon highlights the importance of considering evidence order to mitigate the impact of conflicting knowledge in future retrieval-augmented models.

### Conflicts in Embedded Knowledge

Benefiting from the extensive data in our benchmark, we provide the opportunity to investigate the impact of internal knowledge conflicts on model performance. We mix default evidence and conflict

Figure 4: Memorization ratio (\(M_{R}\)) of different LLMs under three types of conflict evidence when given two pieces of external evidence. Within the same model series, models with larger parameter sizes exhibit lower \(M_{R}\) compared to their smaller counterparts.

Figure 5: Memorization ratio (\(M_{R}\)) of LLMs with different evidence orders. The legend indicates the type of evidence closest to the question. We report the average \(M_{R}\) for the three conflict causes.

Figure 3: Memorization ratio (\(M_{R}\)) of different LLMs under three types of conflict evidence when presented with solely contradictory external evidence. All series consistently show the lowest \(M_{R}\) for semantic conflicts, with higher values for the other two conflict types across all model sizes.

evidence in 2:1, 1:1, and 2:3, along with a control setup with no conflict evidence (i.e., 1:0). We randomly select evidence from three different conflict types to ensure diversity in conflict sources.

We inject conflicting knowledge into the models by continually pre-training the foundational models and maintaining consistency by training each experimental setup on 1.06B tokens. We report the Original Answer Ratio (OAR) for each model to analyze the impact of internal knowledge conflicts on model behaviors. As shown in Figure 6, our findings highlight two key points:

**Internal knowledge can be easily impacted by the introduction of conflicting data.** LLMs demonstrate a noticeable decline in the ratio of default answers when internal conflicts are introduced. This degradation is particularly significant in LLAMA2-7B and Mistral-7B. Even with only one-third (2:1 ratio of default to conflicting data) of the data being conflicting, these models exhibit substantial declines in OAR, with decreases of 41.5% and 37.6%, respectively. Moreover, as the amount of conflicting knowledge increases, the models' performance further deteriorates. These observations indicate that the introduction of conflicting data negatively affects the models' internal knowledge, and the greater the amount of conflicting data, the worse the models' performance.

**Embedded knowledge conflicts do not affect the model's ability to follow external evidence.** However, when we prepend the original default evidence to the question, we find that models maintain their original performance, choosing the default answer regardless of the amount of conflicting data introduced. This indicates that injected conflicts affect only the model's internal knowledge but do not impact its ability to follow external evidence. This suggests that leveraging retrieved or tool-assisted methods to access correct and relevant knowledge can effectively mitigate the adverse effects of internal conflicts on model performance. Based on this finding, we further explore the model's performance when external conflicts are presented in Section 3.4.

Figure 6: Original Answer Ratio (OAR) of LLMs with varying proportions of conflicts embedded in the model’s parametric memory. “Without evidence” represents the model answering without any evidence, while “with default evidence” represents the model answering with the original evidence prepended to the question. We report the average OAR for the three conflict causes.

Figure 7: Counter Answer Ratio (CAR) of Qwen1.5-4B with internal conflicts and external conflicting evidence. The left picture shows the model with one conflict evidence. The middle and right pictures show the model with one conflict and one default evidence, with the middle picture having a randomized order and the right picture placing the conflict evidence at last. We report the average CAR for the three conflict causes.

### Interplay among the Conflicts

In this section, we aim to investigate the interaction between different types of knowledge conflicts. It is crucial to understand the relationship between the internal knowledge inconsistency of the model and its behavior in response to the context. Following the setup in Section 3.3, we conduct the experiments on the Qwen1.5-4B and use the Counter Answer Ratio (CAR) to measure the model's preference for answering the substituted object. The results are shown in Figure 7. Our observations are summarized as follows:

**Model relies more on retrieved knowledge for answering.** When conflicts are present in the embedded knowledge, the model increasingly relies on externally retrieved knowledge for answers. In the single conflict evidence scenario, the model's dependency on conflict evidence is evident, with the OAR increasing from 97.5% to 98.36% when the ratio of conflict data is 2:1. As the proportion of internal conflict data rises, the model consistently follows the external preference, maintaining an OAR around 98%, as shown in the left part of Figure 7.

In the multiple evidence scenario, the model similarly exhibits a firm reliance on external knowledge. Additionally, we observe that the model shows a higher preference for the evidence closer to the question than scenarios without internal conflicts. As shown in the right part of Figure 7, the model prefers the later-positioned conflict evidence more when internal conflicts are present (77.52% vs. 82.08%), this phenomenon we also observed in Section 3.2 when no internal conflicts were present. However, with internal conflicts, the model shows an even stronger preference for the external knowledge closer to the question. This indicates that the order of evidence during continued pre-training is crucial for conflicting knowledge handling due to the model's increased reliance on external information.

### Detailed Description Can Make the LLMs' Objectives More Explicit

In this section, we aim to explore whether refining questions can encourage the model to exhibit desired behavior when encountering conflicts. Specifically, we incorporate descriptions of temporal and semantic conflicts within the questions. For temporal conflict scenarios, we add specific years to the questions. For semantic conflict scenarios, we include detailed descriptions of the subjects. We conduct experiments to observe the model's behavior with and without internal conflicts when provided with two pieces of external conflicting evidence. We analyze the impact of including these descriptions on the model's performance in both scenarios.

The results are shown in Figure 8. We observe that LLMs, whether with or without internal conflicts, exhibit a significant decrease in \(M_{R}\) when provided with external knowledge containing descriptions compared to without them. Take LLaMA3-70B as an example, the \(M_{R}\) drops from 67.52% to 8.31% when descriptions are included. Furthermore, when internal conflicts are presented, adding descriptions also makes the LLMs' objectives more explicit. For example, when the internal conflict ratio is 2:1, the \(M_{R}\) of Qwen1.5-4B decreases from 55.37% to 41.80% with the inclusion of descriptions. This suggests that the more specific and detailed the text, the more likely the LLM

Figure 8: Memorization Ratio (\(M_{R}\)) of LLMs with or without a description in the question. “w/ des.” and “w/o des.” indicate the presence or absence of conflict context descriptions. The left image shows no internal conflicts, while the right shows the Qwen1.5-4B model with internal conflicts. We report the average \(M_{R}\) for the three causes of conflict.

is to trust the external knowledge, and designing detailed instructions can effectively improve the faithfulness of LLMs.

## 4 Related Work

**Taxonomy of knowledge conflicts** Knowledge conflicts are mainly divided into two types: retrieved knowledge conflicts and embedded knowledge conflicts. Retrieved conflicts occur when the model's internal knowledge conflicts with externally retrieved information, commonly in retrieval-augmented generation (RAG) and tool-augmented scenarios (Zhang and Choi, 2021; Li et al., 2023; Peng et al., 2023; Kasai et al., 2024). Embedded conflicts arise from conflicting parametric knowledge within LLMs, increasing uncertainty during knowledge-intensive tasks and undermining trustworthiness (Chang and Bergen, 2023; Chen et al., 2023; Raj et al., 2023; Rabinovich et al., 2023; Raj et al., 2023; Bartsch et al., 2023). Currently, most research focuses on retrieved conflicts. Our work extends this by investigating both types and their interactions.

**The Causes of Knowledge Conflicts** With the rapid expansion of diverse knowledge sources, the risk of misinformation generated by LLMs has increased, posing challenges for detection (Chen and Shu, 2023; Bengio et al., 2024; Wang et al., 2023; Solaiman et al., 2023; Goldstein et al., 2023; Ferrara, 2024). Therefore, misinformation is the main focus in previous work as a cause of knowledge conflicts (Hsu et al., 2021; Ko et al., 2022; Li et al., 2023). However, many factors contribute to knowledge conflicts in real-world scenarios, such as knowledge update (Lazaridou et al., 2021) and the multiple meanings of words (Sevgili et al., 2022). In ConflictBank, we construct conflicts from three causes to provide a more comprehensive analysis.

**Knowledge Conflicts Datasets** To construct conflict-related datasets, previous works have primarily adopted two methods, including entity-level substitution (Longpre et al., 2021; Chen et al., 2022; Si et al., 2023; Wang et al., 2023) and generative approaches using LLMs (Ying et al., 2024; Xu et al., 2024; Tan et al., 2024). Recent datasets combined these two methods to create more coherent conflicting pairs (Xie et al., 2024), providing insights into the causes and behaviors of LLMs when encountering conflicts (Aggarwal et al., 2021; Chen et al., 2021). However, these datasets primarily focus on conflicts in retrieved knowledge, with few addressing internal conflicts within parametric memory and more complex scenarios.

## 5 Conclusion

We develop ConflictBank, a novel and comprehensive dataset for studying the effect of knowledge conflicts from misinformation, temporal updates, and semantic variations. For each of the knowledge conflict source, we utilize LLMs to generate three styles of texts to maximize the dataset diversity. In summary, ConflictBank is a large diverse dataset consists of 553K QA pairs and 7M knowledge conflict evidence in high quality. The QA pairs could be used for model evaluations, and the evidence could be utilized for simulating the conflicts encountered in the LLM pre-training and the inference phases. With ConflictBank, we conduct pilot experiments to investigate LLMs' behaviors under three common conflict scenarios, including the embedded knowledge conflict in pre-training, the retrieved knowledge conflict when inference, and the interplay between the above two conflicts. We believe ConflictBank could be used in broad applications, and help analyze and build trustworthy large language models.