# Action Gaps and Advantages in Continuous-Time Distributional Reinforcement Learning

Harley Wiltzer

Mila-Quebec AI Institute

McGill University

&Marc G. Bellemare

Mila-Quebec AI Institute

McGill University

&David Meger

McGill University

&Patrick Shafto

Rutgers University-Newark

&Yash Jhaveri

Rutgers University-Newark

Equal contribution. Correspondence to harley.wiltzer@mail.mcgill.ca.

###### Abstract

When decisions are made at high frequency, traditional reinforcement learning (RL) methods struggle to accurately estimate action values. In turn, their performance is inconsistent and often poor. Whether the performance of distributional RL (DRL) agents suffers similarly, however, is unknown. In this work, we establish that DRL agents _are_ sensitive to the decision frequency. We prove that action-conditioned return distributions collapse to their underlying policy's return distribution as the decision frequency increases. We quantify the rate of collapse of these return distributions and exhibit that their statistics collapse at different rates. Moreover, we define distributional perspectives on action gaps and advantages. In particular, we introduce the _superiority_ as a probabilistic generalization of the advantage--the core object of approaches to mitigating performance issues in high-frequency value-based RL. In addition, we build a superiority-based DRL algorithm. Through simulations in an option-trading domain, we validate that proper modeling of the superiority distribution produces improved controllers at high decision frequencies.

## 1 Introduction

In many real-time deployments of reinforcement learning (RL)--quantitative finance, robotics, and autonomous driving, for instance--the state of the environment evolves continuously in time, but policies make decisions at discrete timesteps (\(h\) units of time apart) . In such systems, the performance of value-based agents is sensitive to the frequency \(:=}{{h}}\) with which actions are taken. In particular, action values become indistinguishable as the time between actions decreases. In turn, in high-frequency settings, Baird demonstrated that action value estimates are susceptible to noise and approximation error . Moreover, Tallec et al. exhibited that the performance of popular deep \(Q\)-learning agents is inconsistent and often poor .

In order to remedy this sensitivity, Baird proposed the advantage function and advantage-based variants of \(Q\)-learning, Advantage Updating (AU)  and Advantage Learning (AL) . Unlike action values, advantages (appropriately rescaled) do not become indistinguishable as decision frequency increases. As a result, Baird, in , demonstrated that advantage-based agents can learn faster and be more resilient to noise than their action value-based counterparts. Furthermore, Tallec et al., in , exhibited that their extension of AU, Deep Advantage Updating (DAU), works efficiently over a wide range of timesteps and environments, unlike standard deep \(Q\)-learning approaches.

While advantage-based approaches to RL have demonstrated robustness to decision frequency, in this work, we establish that they are nevertheless sensitive to the frequency with which actionsare taken. This discovery arises as we answer the question: to what extent is the performance of distributional RL (DRL) agents sensitive to decision frequency? To this end, we build theory within the formalism of continuous-time RL where environmental dynamics are governed by SDEs, as in . Additionally, we validate our theory empirically through simulations. Specifically, we make the following four contributions:

**Distributional Action Gap.** First, we extend notions of action gap to the realm of DRL. Precisely, we consider the minimal distance between pairs of action-conditioned distributions under metrics on the space of probability measures on \(\). We observe that some metrics are viable for this extension, while others are not. This formalism sets the stage for analyzing the influence of individual actions as well as decision frequency on, for example, an agent's return distributions.

**Collapse of Distributional Control at High Frequency.** Second, we establish tight bounds on the distributional action gaps of _\(h\)-dependent action-conditioned return distributions_--return distributions induced by applying a specific initial action for \(h\) units of time. We prove that these distributional action gaps not only collapse, as \(h\) tends to zero, but do so at a _slower rate_ than action-value gaps. On one hand, therefore, distributional \(Q\)-learning algorithms are susceptible to the same failures as \(Q\)-learning in continuous-time RL. On the other hand, however, remedies to these failures transliterated to distributional \(Q\)-learning algorithms are unlikely to succeed, because the means of these return distributions collapse _faster_ than their other statistics.

**Distributional Superiority.** Third, we propose an axiomatic construction of a distributional analogue of the advantage, which we call the _superiority_. Leveraging our analysis of \(h\)-dependent action-conditioned returns and their distributional action gaps, we present a frequency-scaled superiority distribution that enables greedy action selection at any fixed decision frequency.

**A Distributional Action Gap-Preserving Algorithm.** Fourth, we propose an algorithm that learns the superiority distribution from data. Empirically, we demonstrate that our algorithm maintains the ability to perform policy optimization at high frequencies more reliably than existing methods.

## 2 Setting

**Notation:** Spaces will either be subsets of Euclidean space or discrete. Measurability, in the former case, will be with respect to the Borel sigma algebra; in the latter case, it will be with respect to the power set. The set of probability measures over a space \(\) will be denoted by \(()\). Functions on spaces are assumed to be measurable. For \(f:\) and \(()\), the _push forward_ of \(\) through/by \(f\), \(f_{\#}()\), is defined by \(f_{\#}:= f^{-1}\). For a random variable \(X\), defined implicitly on some probability space \((,,)\), we write \((X):=X_{\#}\) to denote the law of \(X\); the notation \(X=Y\) is shorthand for \((X)=(Y)\). For any \(()\), the _quantile function of \(\)_, \(F_{}^{-1}\), is defined by \(F_{}^{-1}():=_{z}\{F_{}(z)\}\), where \(F_{}(z)\) is the CDF of \(\).

### Continuous-Time RL

Here we give a brief introduction to the technical aspects of continuous-time RL, a la . We provide additional exposition and references in Appendix A. For any reader looking to defer some of this technical introduction, we summarize the core objects of interest at the end of Section 2.1.1.

#### 2.1.1 MDPs

Continuous-time Markov Decision Processes (MDPs) are defined by three spaces and four measurable functions: a time interval \(:=[0,T]\) with \(T(0,)\) or \(:=[0,T)\) with \(T=\), a state space \(^{n}\), an action space \(\), a drift \(b:^{n}\), a diffusion \(:^{n n}\), a reward \(r:\), and a terminal reward \(f:\).3 The pair \((b,)\) govern the environment's dynamics by a family of SDEs parameterized by \(a\),

\[\!X_{t}^{a}=b(t,X_{t}^{a},a)\!t+(t,X_{t }^{a},a)\!B_{t}.\] (2.1)

Here \((B_{t})_{t 0}\) is an \(n\)-dimensional Brownian motion. In turn, any solution to (2.1) collects the state paths of an agent that chooses action \(a\) at every time, regardless of the state they are in.

As is done in discrete-time RL, an agent might consider the induced Markov Reward Process (MRP) derived from a policy \(:()\).4 The dynamics of a policy-induced MRP (with policy \(\))are governed by the SDE

\[X_{t}^{}=b^{}(t,X_{t}^{})\,t+^{}(t,X_{t}^{ })\,B_{t}.\] (2.2)

Here, following , the policy-averaged coefficients \(b^{}\) and \(^{}\) are defined by

\[b^{}(t,x):=_{}b(t,x,a)\,(a\,|\,t,x)^{}(t,x):=(_{}^{}(t,x,a)\, (a\,|\,t,x))^{}{{2}}}.\] (2.3)

Thus, solutions to (2.2) collect the paths of an agent following policy \(\).

A class of policies central to our study is those that fix an action \(a\) from some time \(t\) for a given _persistence horizon_\(h\).

**Definition 2.1**.: _Given \(h>0\) and \(a\), a policy \(\) is said to be \((h,a)\)-persistent at time \(t\) if \((\,|\,s,y)=_{a}\) for all \((s,y)[t,t+h)\)._

In particular, given a policy \(\), we will consider \((h,a)\)_-persistent modifications of \(\)_: for \(t\),

\[|_{h,a,t}(\,|\,s,y):=_{a}&s[t,t+h)\\ (\,|\,s,y)&s[t,t+h).\]

These policies will help us understand the influence of taking actions relative to others as well as to those taken by \(\). We assume \(h\) is small enough so that \(t+h\).

In order to guarantee the global-in-time existence and uniqueness of solutions to our SDEs (2.1) and (2.2), we make two sets of assumptions.

**Assumption 2.2**.: _The functions \(b\) and \(\) have linear growth and are Lipschitz in state, uniformly in time and action: a finite, positive constants \(C_{2.2}\) exists such that_

\[_{t,a}|b(t,x,a)|+_{t,a}|(t,x,a)| C_{2.2}(1+|x|)  x;\] \[_{t,a}|b(t,x,a)-b(t,y,a)|+_{t,a}|(t,x,a)-(t, y,a)| C_{2.2}|x-y| x,y.\]

**Assumption 2.3**.: _The averaged coefficient functions \(b^{}\) and \(^{}\) are Lipschitz in state, uniformly in time: a finite, positive constant \(C_{2.3}\) exists such that_

\[_{t}|b^{}(t,x)-b^{}(t,y)|+_{t}|^{}(t,x)-^{}( t,y)| C_{2.3}|x-y| x,y.\]

These assumptions are standard in the analysis of continuous-time RL, optimal control, and SDEs . Since \(\) is a function of state, we note that Assumption 2.3 is not a direct consequence of Assumption 2.2. The coefficients \(b^{}\) and \(^{}\) satisfy the conditions of Assumption 2.3 provided \(b\) and \(\) satisfy some (also standard) additional regularity conditions and \(\) satisfies some regularity conditions. These technical details are discussed in Appendix A.2.

In summary, in continuous-time RL, there are three stochastic processes of interest: \((X_{s}^{})_{s t}\) with \(\{a,,|_{h,a,t}\}\), all beginning at some time \(t\). These processes collect the state paths of an agent in one of three scenarios: 1. choosing action \(a\) at every state and time; 2. following a policy \(\); or 3. choosing \(a\) at every state and time for the first \(h\) units of time and following \(\) thereafter.

#### 2.1.2 Value Functions and their Distributions

Given a policy-induced state process \((X_{s}^{})_{s t}\), the (discounted) random _return_\(G^{}(t,x)\) earned by \(\) starting from state \(x\) at time \(t\) is defined  by

\[G^{}(t,x):=_{t}^{T}^{s-t}r(s,X_{s}^{})\,s+^{T- t}f(X_{T}^{}) X_{t}^{}=x,\] (2.4)

where \(f 0\) when \(=[0,)\). We distinguish returns earned by \((h,a)\)-persistent modifications of policies. We call these \(h\)_-dependent action-conditioned returns_ and denote them by \(Z_{h}^{}(t,x,a)\). Given \(\), they are defined by

\[Z_{h}^{}(t,x,a):=_{t}^{T}^{s-t}r(s,X_{s}^{|_{h,a,t}})\, s+^{T-t}f(X_{T}^{|_{h,a,t}}) X_{t}^{ |_{h,a,t}}=x.\] (2.5)Value-based approaches in RL estimate either the _value function_\(V^{}(t,x):=[G^{}(t,x)]\) or the _\(h\)-dependent action-value function_\(Q_{h}^{}(t,x,a):=[Z_{h}^{}(t,x,a)]\).5 As distributional approaches in RL estimate the laws of returns, following , we define

\[^{}(t,x):=(G^{}(t,x))_{h}^{} (t,x,a):=(Z_{h}^{}(t,x,a)).\]

It is important to note that only the laws of random returns (and not their representations as random variables) are observable and modeled in practice.

### \(Q\)-Learning in Continuous Time

The failure of action-value-based RL in continuous-time stems from the collapse of action values at a given state to the value of that state. Precisely, Tallec et al. and Jia and Zhou established that

\[Q_{h}^{}(t,x,a)-V^{}(t,x)=(H^{}(t,x,a)+()V^{}(t,x))h+o(h),\] (2.6)

where \(H^{}\) is independent of \(h\) (see  and  respectively).6 In a discrete action space, given a state \(x\) and time \(t\), a concise way to capture the asymptotic information of (2.6) is by considering the _action gap_[11; 5] of the associated \(h\)-dependent action values

\[(Q_{h}^{},t,x):=_{a_{1} a_{2}}|Q_{h}^{}(t,x,a_{1})-Q _{h}^{}(t,x,a_{2})|.\]

Anticipating (2.6), which implies that \((Q_{h}^{},t,x)=O(h)\), Baird proposed AU wherein he estimated the _rescaled advantage function_\(A_{h}^{}\) in place of \(Q_{h}^{}\):

\[A_{h}^{}(t,x,a):=^{}(t,x,a)-V^{}(t,x)}{h}(t,x,a).\] (2.7)

Note that \((A_{h}^{},t,x)=O(1)\). Tallec et al.  and Jia and Zhou , following Baird, also estimated \(A_{h}^{}\) to ameliorate \(Q\)-learning in continuous time.

## 3 The Distributional Action Gap

In this section, we define a distributional notion of action gap; we prove that \(h\)-dependent action-conditioned return distributions collapse to their underlying policy's return distribution as \(h\) vanishes; and we quantify the rate of collapse of these return distributions.

**Definition 3.1**.: _Consider an MDP with discrete action space and let \(:((),d)\) for a metric \(d\). The \(d\) action gap of \(\) at a state \(x\) and time \(t\) is given by_

\[_{d}(,t,x):=_{a_{1} a_{2}}d((t,x,a_{1}),(t,x, a_{2})).\]

While \(\) has a canonical metric, induced by \(||\), the space \(()\) does not. So a choice must be made, and some metrics are unsuitable. For example, in deterministic MDPs with deterministic policies, return distributions are identified by expected returns: \(_{h}^{}(t,x,a)\) is the delta at \(Q_{h}^{}(t,x,a)\), for all \((t,x,a)\). Thus, \(_{d}(_{h}^{},t,x)\) should vanish as \(h\) decreases to zero if \((Q_{h}^{},t,x)\) vanishes as \(h\) decreases to zero. With the total variation metric \(d=\), for instance, this is not the case, making \(\) unsuitable. Indeed, suppose we have a deterministic MDP with \(=\{a_{1},a_{2}\}\) and such that \(_{h}^{}(t,x,a_{1})=_{h}\) and \(_{h}^{}(t,x,a_{2})=_{0}\), for some state \(x\) and time \(t\) (see, e.g., ). Then \(_{}(_{h}^{},t,x)=1\), for all \(h>0\), yet \((Q_{h}^{},t,x)=h\).

The _\(W_{p}\) distances_ from the theory of Optimal Transportation (see ), however, are suitable. They are defined via _couplings_ of distributions.

**Definition 3.2**.: _Let \(,()\). A \((^{2})\) is a coupling of \(\) and \(\) if its first and second marginals are \(\) and \(\) respectively. We denote the set of these couplings by \((,)\)._

**Definition 3.3**.: _Let \(,()^{}\) and \(p[1,)\). The \(W_{p}\) distance between \(\) and \(\) is_

\[W_{p}(,):=_{(,)}(_{^{2}} |z-w|^{p}\;(zw))^{1/p}.\] (3.1)Any coupling attaining the infimum in (3.1) is called a _\(W_{p}\)-optimal coupling_. Henceforth, we write \(_{p}\) when considering \(W_{p}\) action gaps. If \(\) and \(\) are deltas at \(Q_{h}^{}(t,x,a_{1})\) and \(Q_{h}^{}(t,x,a_{2})\) respectively, then the right-hand side of (3.1) is equal to \(|Q_{h}^{}(t,x,a_{1})-Q_{h}^{}(t,x,a_{2})|\). Hence, in deterministic MDPs with deterministic policies, \(W_{p}\) action gaps of \(_{h}^{}\) are identical to action gaps of \(Q_{h}^{}\), making the \(W_{p}\) distances suitable in the above sense. In non-deterministic MDPs, the relationship between \(_{p}(_{h}^{},t,x)\) and \((Q_{h}^{},t,x)\) is opaque.

The following results study the \(W_{p}\) action gap of \(_{h}^{}\) as a function of \(h\), lending some color to the relationship between \(_{p}(_{h}^{},t,x)\) and \((Q_{h}^{},t,x)\). These results all hold under Assumptions 2.2 and 2.3. Henceforth, we suppress mention of these assumptions; we do not restate them explicitly. First, we observe that \(W_{p}\) action gaps of \(_{h}^{}\) are bounded from below by action gaps of \(Q_{h}^{}\).

**Proposition 3.4**.: _For all \((t,x)\), we have that \(_{p}(_{h}^{},t,x)(Q_{h}^{},t,x)\)._

For a proof of this statement and any other made in this work, see Appendix B. Our next result establishes that \(W_{p}\) action gaps of \(_{h}^{}\), like action gaps of \(Q_{h}^{}\), vanishes for a large class of MDPs.

**Theorem 3.5**.: _If \(r\) and \(f\) are bounded, then \(_{h 0}W_{p}(_{h}^{}(t,x,a),^{}(t,x))=0\), for all \((t,x,a)\); hence, \(_{h 0}_{p}(_{h}^{},t,x)=0\)._

While Theorem 3.5 shows that the \(W_{p}\) distance between \(_{h}^{}(t,x,a)\) and \(^{}(t,x)\) (and the \(W_{p}\) action gap of \(_{h}^{}\) at \((t,x)\)) does indeed vanish as \(h\) decreases, it does not identify the rate at which it does so. Our next two theorems establish this rate.

**Theorem 3.6**.: _MDPs and policies exist in and under which, for all \((t,x,a)\), we have that \(W_{p}(_{h}^{}(t,x,a),^{}(t,x))}h^{}{{2}}}\) and \(_{p}(_{h}^{},t,x)}h^{}{{2}}}\)._

Finally, we prove that for a large class of MDPs (different from but overlapping with the class of MDPs captured in Theorem 3.5), the lower bound found in Theorem 3.6 is an upper bound.

**Theorem 3.7**.: _If \(r\) is Lipschitz in state, uniformly in time, \(f\) is Lipschitz, and \(T<\), then \(W_{p}(_{h}^{}(t,x,a),^{}(t,x))}h^{}{{2}}}\), for all \((t,x,a)\); hence, \(_{p}(_{h}^{},t,x)}h^{}{{2}}}\)._

Theorems 3.6 and 3.7 demonstrate that the \(W_{p}\) distance between \(_{h}^{}(t,x,a)\) and \(^{}(t,x)\) and the distance between \(Q_{h}^{}(t,x,a)\) and \(V^{}(t,x)\) are of different orders in terms of \(h\). Thus, we see that \(_{p}(_{h}^{},t,x)\) and \((Q_{h}^{},t,x)\) in stochastic MDPs are fundamentally different.

## 4 Distributional Superiority

In this section, we introduce a probabilistic generalization of the advantage. We define this random variable--which we call the _superiority_ and denote by \(S_{h}^{}\)--via a pair of axioms.

A natural construction of the superiority at \((t,x,a)\) is given by \(Z_{h}^{}(t,x,a)-G^{}(t,x)\). The law of this difference, however, depends on the joint law of \((Z_{h}^{}(t,x,a),G^{}(t,x))\), which is unobservable in practice and ill-defined (cf. Section 2.1.2). Yet, the set of all possible laws of this difference is easily characterized; it is the set of _coupled difference representations of \(_{h}^{}(t,x,a)\) and \(^{}(t,x)\)_.

**Definition 4.1**.: _Let \(,()\). A coupled difference representation (CDR) \(()\) of \(\) and \(\) takes the form \(=_{\#}\) where \((,)\) and \(:^{2}\) is given by \((z,w):=z-w\). The set of all coupled difference representations of \(\) and \(\) will be denoted by \((,)\)._

Our first axiom places the superiority's law in this set, \((_{h}^{}(t,x,a),^{}(t,x))\).

**Axiom 1**.: _The law of \(S_{h}^{}(t,x,a)\) is a coupled difference representation of \(_{h}^{}(t,x,a)\) and \(^{}(t,x)\)._

Our second axiom encodes a type of consistency for deterministic policy behavior.

**Axiom 2**.: \(S_{h}^{}(t,x,a)\) _is deterministic whenever \(\) is \((h,a)\)-persistent at time \(t\)._

To see how Axiom 2 encodes a notion of deterministic consistency, first consider its discrete-time analogue: _the superiority at \((x,a)\) for a policy \(\) is deterministic if \(\) at \(x\) deterministically chooses \(a\)._

In this situation, our \(a\)-following agent makes the same choices as a \(\)-following agent--both take action \(a\) in state \(x\) initially and then follow \(\) thereafter--, and we posit that the superiority should not be random. The continuous-time analogue of the situation just described occurs precisely when a policy \(\) is \((h,a)\)-persistent at starting time \(t\). Given a starting time \(t\) and state \(x\), an agent that chooses action \(a\) between \(t\) and \(t+h\) and then follows \(\) is following the \((h,a)\)-persistent modification of \(\) at time \(t\). By definition, they make the same choices as a \(\)-following agent when \(\) is \((h,a)\)-persistent starting at time \(t\). Axiom 2 stipulates that, in this case, \(S_{h}^{}(t,x,a)\) should be deterministic.

By construction, if \(_{h}^{}(t,x,a)(_{h}^{}(t,x,a),^{}(t,x))\), then its mean is \(Q_{h}^{}(t,x,a)-V^{}(t,x)\) (see Appendix B.2 for a proof of this claim). Axiom 2 then says that any determining coupling \(_{h}^{}(t,x,a)\) when \(\) is \((h,a)\)-persistent at time \(t\) must be such that \(_{\#}_{h}^{}(t,x,a)=_{0}\).8 In particular, Axiom 2 nontrivially restricts \((_{h}^{}(t,x,a),^{}(t,x))\).

**Example 4.2**.: _Let \(_{h}^{}(t,x,a):=_{\#}_{h}^{}(t,x,a)\) for \(_{h}^{}(t,x,a)=_{h}^{}(t,x,a)^{}(t,x)\). If \(\) is \((h,a)\)-persistent at time \(t\), then \((_{h}^{}(t,x,a))=2(^{}(t,x))\). This variance is \(0\) only when \(\)'s return is deterministic. Hence, \(_{h}^{}(t,x,a)\) may be nontrivial even when conditioning on \(a\) reflects the policy's behavior exactly. We posit, via Axiom 2, that this should be prohibited._

In fact, Axiom 2 determines a single coupling, if we want a consistent choice across all time-state-action triplets and all MDPs.

**Theorem 4.3**.: _Let \((,)\) for some \(()\). The push-forward of \(\) by \(\) is the delta at zero, \(_{\#}=_{0}\), if and only if \(\) is a \(W_{p}\)-optimal coupling, for some \(p[1,)\). Moreover, there is only one such coupling. It is given by \(_{}:=(,)_{\#}\) or, equivalently, \(_{}:=(F_{}^{-1},F_{}^{-1})_{\#}(0,1)\). Here \((0,1)\) is the uniform distribution on \(\)._

The second definition of \(_{}\) corresponds, more generally, to the \(W_{p}\)-optimal coupling, for all \(p 1\), of \(\) and \(\) given by \(_{,}:=(F_{}^{-1},F_{}^{-1})_{\#}(0,1)\). As \(_{\#}_{,}\)'s quantile function is \(F_{}^{-1}-F_{}^{-1}\), we have in hand everything we need to define the superiority distribution (via its quantile function).

**Definition 4.4**.: _The superiority distribution \(_{h}^{}\) at \((t,x,a)\) is_

\[_{h}^{}(t,x,a):=(F_{_{h}^{-1}(t,x,a)}^{-1}-F_{^{}(t,x)}^{-1 })_{\#}(0,1).\]

As \(_{h}^{}(t,x,a)\) has the smallest possible central absolute \(p\)th moments among all CDRs of \(_{h}^{}(t,x,a)\) and \(^{}(t,x)\), heuristically, it captures more of the individual features of both return distributions than other such CDRs (like \(_{h}^{}(t,x,a)\) in Example 4.2). We illustrate this by example in Figure 4.1.

### The Rescaled Superiority Distribution

From Section 3, we know that the \(W_{p}\) distance between \(_{h}^{}(t,x,a)\) and \(^{}(t,x)\), for every \(p 1\), vanishes as \(h\) vanishes. Moreover, we know the rate at which this distance disappears. As a result, by construction, the central absolute \(p\)th moments of \(_{h}^{}(t,x,a)\) collapse as \(h\) collapses, and, for a large class of MDPs, we understand the rate at which these moments collapse. More generally and precisely, if we consider the \(q\)_-rescaled superiority distribution_ defined by

\[_{h;q}^{}:=(h^{-q}\,)_{\#}_{h}^{},\]

we see that Theorems 3.6 and 3.7 translate to the follow statements on \(W_{p}\) action gaps of \(_{h;q}^{}\):

**Theorem 4.5**.: _MDPs and policies exist satisfying Assumptions 2.2 and 2.3 in and under which, for all \((t,x)\), we have that \(_{p}(_{h;q}^{},t,x)}{{}}\;$}h^{}{{2}}-q}\)._

**Theorem 4.6**.: _Under Assumptions 2.2 and 2.3, if \(r\) is Lipschitz in state, uniformly in time, \(f\) is Lipschitz, and \(T<\), then \(_{p}(_{h;q}^{},t,x)}{{}}\;$}h^{}{{2}}-q}\), for all \((t,x)\)._

These two theorems tell us how to preserves the \(W_{p}\) action gaps of \(q\)-rescaled superiority distributions (as a function of \(h\)). They identify \(q=}{{2}}\). For \(q<}{{2}}\), \(W_{p}\) action gaps vanish as \(h\) vanishes. Whereas for \(q>}{{2}}\), \(W_{p}\) action gaps blow up as \(h\) vanishes. These behaviors are undesirable. When

Figure 4.1: PDFs of Return Distributions and Two Candidate CDRs.

\(q<}{{2}}\), the influence of an action on an agent's superiority becomes indistinguishable from any other action. For \(q>}{{2}}\), ever larger sample sizes are need to obtain any statistical estimate of an agent's superiority with the same level of accuracy. These scenarios are untenable.

Another consideration regarding rescalings of \(_{h}^{}\) is whether they upend rankings of actions determined by some given measure of utility. This would be counterproductive. In DRL, agents often use _distortion risk measures_ to rank actions ().

**Definition 4.7**.: _Given \(()\), the distortion risk measure \(_{}:()\) is defined by \(_{}():=,F_{}^{-1}\); on \(()\), its value is given by the integral of \(F_{}^{-1}\) with respect to \(\)._

A family of \(_{}\) is the \(\)-conditional value-at-risk measures (\(\)-CVaR) , where \(_{}=(0,)\) for \((0,1]\); \(=1\) is the expected-value utility measure. Crucially, \(_{h;q}^{}\) preserves \(_{}\)-valued utility.

**Theorem 4.8**.: _Let \(_{}\) be a distortion risk measure, \(q 0\), and \(h>0\). If \(_{}(^{}(t,x))<\), then \(_{a}_{}(_{h;q}^{}(t,x,a))=_{a }_{}(_{h}^{}(t,x,a))\)._

In turn, the \(}{{2}}\)-rescaled superiority distribution is not only \(W_{p}\) action gap preserving but matches \(_{h}^{}\) in its greedy choice of action as measured by a distortion risk measure.

### Algorithmic Considerations

We now turn to building DRL algorithms based on our theory. Our algorithms leverage the quantile TD-learning framework  to learn \(_{}\)-greedy policies, for a given distortion risk measure \(_{}\), just as DAU  leverages the \(Q\)-learning framework to learn greedy policies. Full pseudocode and implementation details are given in Appendix C.

At the heart of our algorithms is an equality of quantile functions, which holds by construction,

\[F_{_{h}^{}(t,x,a)}^{-1}=F_{^{}(t,x)}^{-1}+h^{q}F_{_{h;q}^{ }(t,x,a)}^{-1}.\] (4.1)

Indeed, given \(\) and \(_{h;q}\), as models of \(^{}\) and \(_{h;q}^{}\) respectively, equation (4.1) justifies the application of quantile TD-learning to \(_{h}\), as a model for \(_{h}^{}\), defined via the quantile function

\[F_{_{h}(t,x,a)}^{-1}:=F_{(t,x)}^{-1}+h^{q}F_{_{h;q}^{}(t,x,a)}^ {-1}.\] (4.2)

That said, we cannot realize quantile TD-learning without defining predictions and bootstrap targets in terms of \(m\)-quantile representations9 of \(_{h}\), via those of \(\) and \(_{h;q}\).

While we may freely parameterize the \(m\)-quantile representation of \(\) with a neural network (with interface) \(:^{m}\), we have to be careful when parameterizing the \(m\)-quantile representation of \(_{h;q}\). Given a neural network \(:^{m}\), we set

\[F_{_{h;q}(t,x,a)}^{-1}:=(t,x,a)-(t,x,a^{})  a^{}_{a}_{}((t,x,a)).\] (4.3)

This ensures we identify a \(_{}\)-greedy policy; it is \(0\) at the \(_{}\)-greedy action \(a^{}\) (cf. [34, Eq. 27]).

With appropriate parameterized \(m\)-quantile representations of \(\) and \(_{h;q}\) in hand, we derive our predictions and bootstrap targets. By (4.2), recalling \(\) and (4.3), we compute our predictions via

\[F_{_{h}(t,x,a)}^{-1}:=(t,x)+h^{q}((t,x,a)-(t,x,a^{})).\] (4.4)

By Wiltzer , as \(X_{t}^{|_{h,a,t}}=x\) and \(X_{t+h}^{|_{h,a,t}}=X_{t+h}^{a}\), observe that

\[Z_{h}^{}(t,x,a)= _{0}^{h}^{s}r(t+s,X_{t+s}^{|_{h,a,t}}) \,s+^{h}G^{}(t+h,X_{t+h}^{|_{h,a,t}})\] \[= \,hr(t,x)+^{h}G^{}(t+h,X_{t+h}^{a})+Y_{h},\]

where \([|Y_{h}|^{p}]=o(h)\)10 for all \(p\). So upon getting a sample state/realization \(x_{t+h}\) of \(X_{t+h}^{a}\), as in , we compute our bootstrap targets via

\[F_{_{h}(t,x,a)}^{-1}:=hr(t,x)+^{h}(t+h,x_{t+h}).\] (4.5)In summary, the predictions (4.4) and the bootstrap targets (4.5) together characterize a family of QR-DQN-based algorithms called DSUP(\(q\)), whose core update is outlined in Algorithm 1.

```
0:\(q\) (rescale factor), \(\) (step size), \(\) (replay buffer), \(_{}\) (distortion risk measure)
0:\(:^{m}\) (\(m\)-quantile approximator of \(F_{}^{-1}\)), \(:^{m}\) (see (4.3))  Sample \((t,x_{t},a_{t},r_{t},x_{t+h},_{t+h})\) from \(\) \(a^{}_{a}_{}(_{n=1}^{m}_{ (t,x_{t},a_{n})})\)\(\) Risk-sensitive greedy action \(F_{}^{-1}(,)(t,x_{t})+h^{q}((t,x_{t},a_{t} )-(t,x_{t},a^{}))\)\(\) Prediction (4.4) \(F_{}^{-1} hr_{t}+^{h}(1-_{t+h}) (t+h,x_{t+h})+^{h}_{t+h}f(x_{t+h})\)\(\) Target (4.5) \((,)(F_{}^{-1}(, ),F_{}^{-1})\)\(\) See (10, Eq. (10)) \(-_{}(,)\) and \(-_{}(,)\)\(\) Gradient updates ```

**Algorithm 1** DSUP(\(q\)) Update

One theoretical drawback of DSUP(\(q\)) for mean-return control is that the mean of the \(q\)-rescaled superiority distribution is \(O(1)\) only when \(q=1\), by (2.6) and (2.7). Thus, we propose modeling \(A_{h}^{}\) simultaneously. This yields a novel form of a _two-timescale_ approach to value-based RL (see, e.g., ). In particular, we estimate \(_{h;q}^{}\) defined by

\[F_{_{h;q}^{}(t,x,a)}^{-1}:=F_{_{h;q}^{}(t,x,a)}^{-1}+(1-h^{1-q}) A_{h}^{}(t,x,a).\]

We call \(_{h;q}^{}\) the _advantage-shifted \(q\)-rescaled superiority_. Note that its mean is \(A_{h}^{}\), which is \(O(1)\). To realize this, we approximate \(A_{h}^{}\) using DAU and employ parameter sharing between the approximators of \(A_{h}^{}\) and \(_{h;q}^{}\). We call this family of algorithms DAU+DSUP(\(q\)). We note that \(A_{h}^{}\) is used only for increasing action gaps; it does not change the training loss for \(\) and \(_{h;q}\).

## 5 Simulations

The empirical work herein is two-fold in nature: illustrative and comparative. First, we simulate an example that illustrates Theorems 3.6/4.5 and Theorem 3.7/4.6 and their consequences. Second, in an option-trading environment, we compare the performance of \(_{h}^{}\)-based agent(s) against QR-DQN  and DAU  in the risk-neutral setting and against QR-DQN in a risk-sensitive setting.

### The Rescaled Superiority Distribution Revisited

Consider an MDP with time horizon \(10\), a two element action space, \(0\) and \(1\)--when action \(1\) is executed, the system follows \(1\)-dimensional Brownian dynamics with a constant drift of \(10\), and otherwise, the state is fixed--, a reward that equals the agent's signed distance to \(0\), and a trivial terminal reward. We estimate four distributions at \((t,x,a)=(0,0,1)\) for the policy that always selects \(0\). Figure 5.1 shows these estimated distributions for a sample of frequencies (kHz), \(=}{{h}}\).

First (from the left), we see that \(_{h}^{}\) collapses to \(_{0}\), as \(h\) tends to \(0\). Thus, accurate action ranking distributional or otherwise becomes impossible in the vanishing \(h\) limit. Second, we see that rescaling by \(h\) produces distributions with \(O(1)\) mean but infinite non-mean statistics in the vanishing \(h\) limit. Here the \(O(1)\) means are imperceptible in face of the large variances. So while this rescaling permits ranking actions by action values, it does so at the expense of producing high-variance distributions.

Third, we see that rescaling by \(h^{}{{2}}}\) yields distributions with \(O(1)\) non-mean statistics but vanishingly small means, \(O(h^{}{{2}}})\). Hence, this rescaling permits ranking actions by non-mean statistics, even if action values again becomes indistinguishable in the vanishing \(h\) limit. That said, the vanishing rate of the means here is slower than when no rescaling is considered, \(O(h)\). Fourth, we see that rescaling by \(h^{}{{2}}}\) and then shifting it by \((1-h^{}{{2}}})A_{h}^{}\) produces distributions with \(O(1)\) mean and non-mean statistics. In turn, this two-timescale approach permits ranking actions by either action values or non-mean statistics (but not both by Theorem 4.8). However, the mean estimates here are inaccurate and imprecise--rather than uniformly being \(100\), they oscillate substantially.

In risk-neutral control, we are left with a number of questions. What effect do the high variance distributions in DAU/DSUP(\(1\)) have on performance? What effect do the \(O(h^{}{{2}}})\) means have on the performance of DSUP(\(}{{2}}\))? What effect does the instability of the mean estimates in DAU+DSUP(\(}{{2}}\)) have on performance? In Section 5.2, we begin to answer these questions and others by testing our superiority-based algorithms against appropriate benchmarks in an option-trading environment.

### High-Frequency Option Trading

The option-trading environment in which we run our comparative experiments is a commonly used benchmark (see, e.g., ). We use an Euler-Maruyama discretization scheme  at high resolution to simulate high-frequency trading. Returns are averaged over \(10\) seeds and \(10\) different dynamics models (corresponding to data from different stocks). Additionally, following , we use disjoint datasets to estimate the dynamics parameters for simulation during training and evaluation.11

First, we consider the risk-neutral setting. Here we compare QR-DQN, DAU, and three algorithms based on the \(q\)-rescaled superiority distribution with \(q=1,}{{2}}\): DSUP(\(1\)), DAU+DSUP(\(}{{2}}\)), and DSUP(\(}{{2}}\)). Figure 5.2 summarizes their performance at a sample of frequencies (Hz).

We see that DSUP(\(}{{2}}\)) is not only the most consistent performer, but outperforms every competitor at all but the two lowest frequencies. Even then, its performance is very close to the best performer. We also see that DAU+DSUP(\(}{{2}}\))'s preservation of both action gaps and \(W_{p}\) action gaps does not lead to the strongest performance. In particular, its performance is inconsistent and sometimes poor. We believe this is because the tested frequencies are low enough that DSUP(\(}{{2}}\)) maintains large enough action gaps to learn performant policies, but high enough that the variances of the distributions underlying \(A_{h}^{}\) cause estimation difficulty. Indeed, the three methods that estimate \(A_{h}^{}\) (explicitly in DAU and DAU+DSUP(\(}{{2}}\)) or implicitly in DSUP(\(1\))) exhibit almost identical behavior.

Our results highlight a dichotomy in existing (ours included) methods for value-based, high-frequency, risk-neutral control. They can either maintain \(O(1)\) expected return estimates or \(O(1)\) return variance estimates, but not both. We observe better performance in estimating small means from \(O(1)\) variance distributions than in estimating \(O(1)\) means from receipricolly large variance distributions.

To qualitatively illustrate the appeal of the \(}{{2}}\)-rescaled superiority, Figure 5.3 presents examples of learned action-conditioned distributions used by DSUP(\(}{{2}}\)) and QR-DQN agents to make decisions.

Figure 5.2: Risk-neutral algorithms on high-frequency option-trading as a function of \(\).

In this environment, action \(1\) taken in the start state terminates the episode, yielding the smallest return, \(0\), making this action inferior to its alternative action, \(1\). We see that \((}{{2}})\) infers this fact. QR-DQN, on the other hand, has difficulty distinguishing these actions. This is because the \(}{{2}}\)-rescaled superiority preserves \(W_{p}\) action gaps, while \(_{h}^{}\) does not.

Second, we consider a risk-sensitive setting. Here we compare QR-DQN and \((}{{2}})\) using \(\)-CVaR for greedy action selection. We do this because Theorem 4.8 does not hold with \(_{h;q}^{}\), and preserving means is less critical in risk-sensitive control than it is in risk-neutral control. Figure 5.4 depicts our results at \(=35\) (see Appendix D for results across a range of \(\)).

Again, we see that \((}{{2}})\) is conclusively the best performer.

## 6 Related Work

Notions of action gap and ranking have long been of interest in RL (see, e.g., ). Action gaps are related to sample complexity in RL--indeed, instance-dependent sample complexity rates are inversely proportional to the divergence between action-conditioned return distributions (). Bellemare et al.  argue for the consideration of alternatives to the Bellman operator that explicitly devalue suboptimal actions, and they show that Baird's AL  operator falls within this class of operators. On the other hand, Schaul et al.  implicitly question Bellemare et al.'s position. They demonstrate that stochastic gradient updates in deep value-based RL algorithms induce frequent changes in relative action values, which in turn is a mechanism for exploration.

The advantage function is commonplace in RL (see, e.g., ). In , Mesnard et al. employ a distributional critic that is closely related to our (unscaled) distributional superiority. Their choice of critic stems from a desire to minimize variance. We note that the distributional superiority is _a posteriori_ characterized as a minimal variance coupled difference representation of action-conditioned return distributions and policy-induced return distributions.

Lastly, DRL in continuous-time MDPs is in its infancy. There are only three works to mention. Wiltzer et al.  give a characterization of return distributions for policy evaluation, and Halperin  studies algorithms for control. That said, neither work considers distributional notions of action gaps or advantages. Moreover, Halperin does not consider any of the challenges of estimating the influence of actions in high decision frequency settings.

## 7 Conclusion

We establish that DRL agents are sensitive to decision frequency through analysis and simulation. In experiments, \((}{{2}})\) learns well-performing policies across a range of high decision frequencies, unlike prior approaches. \((1)\) and \(+(}{{2}})\) are less robust. Given our analysis, the performance of \((1)\) is expected. Building an alternate algorithm to \(+(}{{2}})\) that is both tailored to risk-neutral control and robust to \(h\) is an important avenue for future work.

Figure 5.4: Risk-sensitive algorithms on high-frequency option-trading at \(=35\)Hz.