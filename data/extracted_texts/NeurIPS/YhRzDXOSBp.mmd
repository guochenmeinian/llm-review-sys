# Training Spiking Neural Networks via Augmented Direct Feedback Alignment

Yongbo Zhang\({}^{1}\), Katsuma Inoue\({}^{1}\), Mitsumasa Nakajima\({}^{2}\),

**Toshikazu Hashimoto\({}^{2}\), Yasuo Kuniyoshi\({}^{1}\), and Kohei Nakajima\({}^{1}\)**

\({}^{1}\)Graduate School of Information Science and Technology, The University of Tokyo,

\({}^{2}\)NTT Device Technology Labs.

zhang@isi.imi.i.u-tokyo.ac.jp

###### Abstract

Spiking neural networks (SNNs), the models inspired by the mechanisms of real neurons in the brain, transmit and represent information by employing discrete action potentials or spikes. The sparse, asynchronous properties of information processing make SNNs highly energy efficient, leading to SNNs being promising solutions for implementing neural networks in neuromorphic devices. However, the nondifferentiable nature of SNN neurons makes it a challenge to train them. The current training methods of SNNs that are based on error backpropagation (BP) and precisely designing surrogate gradient are difficult to implement and biologically implausible, hindering the implementation of SNNs on neuromorphic devices. Thus, it is important to train SNNs with a method that is both physically implementatable and biologically plausible. In this paper, we propose using augmented direct feedback alignment (aDFA), a gradient-free approach based on random projection, to train SNNs. This method requires only partial information of the forward process during training, so it is easy to implement and biologically plausible. We systematically demonstrate the feasibility of the proposed aDFA-SNNs scheme, propose its effective working range, and analyze its well-performing settings by employing genetic algorithm. We also analyze the impact of crucial features of SNNs on the scheme, thus demonstrating its superiority and stability over BP and conventional direct feedback alignment. Our scheme can achieve competitive performance without accurate prior knowledge about the utilized system, thus providing a valuable reference for physically training SNNs.

## 1 Introduction

Neuromorphic computing refers to a series of devices and models inspired by the real brain . In the machine learning field, such biologically inspired technology is designed to simulate the learning and adaptability of the brain by utilizing hardware as accelerators to accomplish complex tasks with high accuracy and low energy consumption [1; 2; 3; 4]. With the convergence of Moore's law and the increasing need for large-scale, low-energy neural networks, neuromorphic computing has great potential. Currently, although artificial neural networks (ANNs) have already achieved impressive performance on various tasks, the high computational complexity and energy consumption of ANNs hinder their application on neuromorphic devices . Spiking neural networks (SNNs), which simulate the mechanism of real neurons in the brain, represent a solution to the application of neural networks in neuromorphic computing. Different from ANNs that use continuous scalars to represent and transfer information, SNNs communicate through streams of discrete action potentials or spikes, as shown in Fig.1a. This discrete spikes-based information processing mode makes SNN neurons consume energy only when they generate spikes, allowing SNN significantly reduce the activity times of neurons and energy demand for information transmission [6; 7; 8]. Taking advantageof low power consumption from simulated brain neurons, SNNs bridge the gap when it comes to the implementation of neural networks on neuromorphic devices.

As with ANNs, to obtain high-performance SNN models, effective optimization and training methods are essential. The most effective and representative training method in ANNs-gradient descent-based algorithm backpropagation (BP) -has achieved remarkable success in many fields. However, because the dynamics of SNN neurons are described by discontinuous equations, their inability to perform gradient solutions makes BP difficult to be directly applied to SNNs, posing a challenge for training them. To address this training challenge, two main categories are proposed and widely adopted for porting BP to SNNs version: ANN-SNN conversion and surrogate gradient learning. ANN-SNN conversion methods convert the activation function of ANNs, which are pretrained by BP, into spiking activation mechanisms while keeping the trained parameters constant or using a weight balancing technique, which has achieved very high accuracy on many complicated tasks, such like image classification and speech recognition tasks [10; 11; 12; 13]. However, because of the conversion mechanism, online learning and the physical implementation of these methods are not feasible. Surrogate gradient learning enables online learning of BP on SNNs by employing well-designed and accurately approximated differentiable functions to substitute the non-differentiable elements of SNN neurons during the backpropagation process. This training scheme uses flexible and efficient strategies to achieve excellent performance on several types of tasks [14; 15].

However, from the perspective of neuromorphic computing, BP-based methods are not the best choice. First, during the process of backward propagating errors, the networks need to fully record carefully orchestrated adjustments of all synaptic weights, leading to the physical implementation of these schemes being complex and unscalable [16; 17; 18; 19; 20; 21]. Second, such a mechanism for transmitting all precise information layer by layer is considered biologically implausible [22; 23; 24; 25; 4]. Considering the above problems, developing learning schemes for SNNs based on the neuromorphic computing idea is significant; that is, it is critical to develop easy-to-implement and biologically plausible algorithms.

Augmented direct feedback alignment (aDFA), a BP-free learning algorithm designed for physical neural networks, can be a promising candidate . In aDFA, instead of transmitting error information layer by layer to update weights, as done in BP, the global error is injected directly into each layer through fixed, randomly initialized synaptic weights. Additionally, the arbitrary functions \(g\) can be employed in the backward process rather than relying on \(f^{}\), the exact derivative of the activation function. This approach, which breaks the BP transmission chain and uses imprecise information, is more implementable for physical platforms and is biologically plausible. From the aspect of SNNs, aDFA also can address the challenge posed by the nondifferentiable dynamics of SNN neurons. In , aDFA is preliminary applied to SNNs by employing \(cos^{2}\) as the backward function. However, whether arbitrary functions can be used as backward functions for SNNs as in feedforward neural networks has not been systematically explored and confirmed, and the features of backward functions that can achieve better performance have not been analyzed. To use this

Figure 1: **The schematics of SNN neuron and dynamics of leaky integrate and fire (LIF) model.****(a)** SNN neurons transmit discrete signals. **(b)** Presynaptic spikes are transmitted to postsynaptic neurons, leading to the accumulation of membrane potential, and the postsynaptic spikes are generated when the membrane potential exceeds the firing threshold. After this, the membrane potential is placed to reset the value, and SNN neurons enter a refractory period.

approach more efficiently and inform the training of SNNs implemented in neuromorphic devices, in the present study, we systematically validate the feasibility of the aDFA-SNNs scheme by using random functions with universal properties, present its range of validity, and analyze the settings of backward functions with genetic algorithm (GA) to achieve good performance. We also investigate the impact of basic but crucial features-network scale and temporal dynamics-of SNNs on our scheme, thus demonstrating the stability and superiority of it (see Appendix G and H). Finally, we directly adjust the parameter settings of the entire backward process without using forward information on the aDFA-SNNs schemes with certain forms of \(g\), so that obtaining competitive performance (see Appendix I). Compared to existing studies, our scheme achieves a competitive performance while posing good hardware implementation feasibility (see Appendix K).

## 2 aDFA: BP, gradient-free training mechnism

Considering a standard multilayer network in Fig.2, the forward propagation is expressed as \(x_{n+1}=f(a_{n})\), where \(a_{n}=W_{n}x_{n}\). \(x_{n}^{N_{n}}\) is the input signal from \(n-1\)-th hidden layer \(H_{n-1}\) to \(n\)-th layer, \(N_{n}\) represents number of nodes in \(H_{n}\). \(W_{n}^{N_{n} N_{n-1}}\) stands for the weight for the \(n\)-th layer. \(f\) denotes the element-wise activation function, which is often ReLu or sigmoid function in conventional ANNs , while in SNNs, \(f\) is the non-continuous firing function , as shown in Appendix A. In general, to train such a network, the connection matrices \(W\) need to be optimized to minimize the loss function \(L\). The process of the BP algorithm is shown in Fig.2a, here using the optimization of \(W_{n}\) as an example, and the gradient \(e_{n}\) that transmitted to \(H_{n}\) through the chain-rule of BP can be expressed as:

\[e_{n}=[W_{n+1}^{T}e_{n+1}] f^{}(a_{n}),\] (1)

where the superscript \(T\) represents precise transposition, \(\) denotes Hadamard product, and \(f^{}\) is the exact derivative of activation function \(f\). With this information, we can compute the gradient of \(W_{n}\) as \( W_{n}=-e_{n} x_{n}^{T}\). From Eq.1, we can see that the injected error signal to current layer depends on the error information from the layers behind, and it also needs to engage several precise calculations. Thus, this scheme is not the best choice from the perspective of neuromorphic computing. Feedback alignment (FA) , which is one of the earliest backward path-based BP-free algorithms, replaces the calculation of precise transposition in the backward process by employing fixed randomly generated matrices \(B\), thus simplifying Eq.1 into:

\[e_{n}=[B_{n}e_{n+1}] f^{}(a_{n}),\] (2)

However, the sequential transmission still constrains the neuromorphic implementation of FA. Then, considering the solution of this mechanism, direct feedback alignment (DFA) , which can break the chain-rule of BP by injecting global error signal \(e\) from the final layer to each previous layer directly through \(B\), leads to a new mechanism:

\[e_{n}=[B_{n}e] f^{}(a_{n}),\] (3)

Nevertheless, the precise calculation of derivative \(f^{}\) still could not be avoided, which impedes the complete physical implementation of this method. Additionally, in the context of its application on

Figure 2: **Information flow of BP, DFA, and aDFA.****(a)** BP, transmits the error signal layer by layer, needs to calculate precise \(W^{T}\) and derivative \(f^{}\). **(b)** Orange represents FA, where \(W^{T}\) is replaced by fixed, randomly initialized matrices \(B\). Red stands for DFA, which injects the global error from the last layer directly to each previous layer through fixed and random matrices \(B\). Blue stands for aDFA, a drastic augmentation of DFA, substitutes for \(f^{}\) by arbitrary nonlinear functions \(g\).

SNNs, \(f^{}\) cannot be obtained directly. Instead, it needs to be derived after accurately approximating the dynamics of SNN neurons into differentiable functions. Although several studies have successfully realized DFA-SNNs schemes [37; 38; 39; 40], these accurate simulations and meticulous design processes are complex and require specialized expertise.

In aDFA, which is an impressive expansion of DFA, the \(f^{}\) in Eq.3 is substituted with arbitrary nonlinear functions \(g\), effectively addressing the derivative issue in DFA . The training rule is then updated as:

\[e_{n}=[B_{n}e] g(a_{n}).\] (4)

Compared with Eq.1 in BP, Eq.4 can mitigate all terms, resulting in minimal feedforward information during training process. Breaking the BP chain-rule and the precise gradient calculation makes aDFA easy to implement physically and biologically plausible. Therefore, in the context of neuromorphic computing, aDFA is extremely suitable for training SNNs.

## 3 Results

First, we demonstrate the feasibility of the aDFA-SNNs scheme, which employs a variety of arbitrary nonlinear functions as backward functions \(g\) in Eq.4, to check the effect of aDFA on the performance of SNNs. We use the benchmark task MNIST  with a simple three-layer fully connected SNN model. In this experiment, the model has dimensions \(784 1000 10\), which consist of two spiking layers with 1000 and 10 nodes, respectively. The spiking layers are composed of leaky-integrate-and-fire (LIF) neurons (see Appendix A). For making a comparison, a smoother, exact approximation of the derivative of the discontinuous functions in LIF neuron-namely an approximation of the Dirac delta function-is utilized as the derivative \(f^{}\) of the dynamics of LIF neurons during the backward process, thus constructing both standard BP and DFA schemes (see Appendix D).

For preparing nonlinear functions \(g\), we generate them from random Fourier series (RFS) \(g(a)=_{k=1}^{k}[p_{k}sin(k a)+q_{k}cos(k a)]\), where \(p_{k}\) and \(q_{k}\) are random coefficients that are uniformly sampled from the interval \([-1,1]\). \(k\) is set to 4 in our case, and \(p_{k}\) and \(q_{k}\) are normalized by the relationship of \(_{k=1}^{k}(|p_{k}|+|q_{k}|)=1\). As can be seen, RFS is the sum of a series of sine and cosine functions with introduced randomness, hence possessing the theoretical capability to indefinitely approximate any function. When generating RFS, we notice that neither the exact derivative of the dynamics of the LIF neuron, nor the smoother differentiable approximation \(f^{}\) yields a negative value. Therefore, we introduce a shift to the vertical axis of RFS to obtain positive random Fourier series (PRFS). In fact, numerous standard RFSs are tested in this experiment; however, almost all of them proved to be ineffective. We consider that this phenomenon arises because the negative values of standard RFSs in the backward process change the updating direction of \(W\), which affects the accumulation of membrane potential and firing of LIF neurons in the forward process, hence leading to training failure. We employ correlation coefficient \(\) (see Appendix E) to denote the degree of functional similarity between generated PRFS and \(f^{}\) so as to conduct classified investigation and analysis on the performance of many generated PRFS on the aDFA-SNNs scheme. When \(\) equals 1, \(g\) is the exact \(f^{}\), that is, the standard BP and DFA cases; when it is 0, it represents uncorrelated case; and when it equals -1, it denotes the negative correlated case. In our study, the shape of \(f^{}\), as indicated by the gray line in Fig.3c, is highly slender and distinctive, making it challenging to directly obtain \(\) with a higher value and broader range, which hinders a systematic classified analysis. Therefore, to achieve relatively higher value and wider range of degree of functional similarity with \(f^{}\), we incorporate the scaling factor \(\) into PRFS to adjust its fundamental frequency. The transformed PRFS is presented as:

\[g(a)=|m|+_{k=1}^{k}[p_{k}sin( k a)+q_{k}cos( k  a)].\] (5)

where \(|m|\) denotes a shift toward the positive field. To obtain proper \(\), that is, to achieve a higher value and wider range of \(\), we generate PRFSs with 10,000 random seeds, calculate their correlation coefficient \(\) with \(f^{}\) at different orders of \(\), and investigate the distribution of them. The results are shown in Fig.3a. When \(\) equals 0.01, the distribution of \(\) is approximately normal in the range [-0.6, 0.6], which represents the maximum value and widest range of \(\) that we can obtain. We divide this range into six intervals with a uniform size of 0.2 and randomly select five PRFSsas \(g\) within each interval to engage the aDFA-SNNs scheme. The experiment is also carried out on the BP-SNNs scheme for comparison, wherein randomly selected PRFSs are used instead of \(f^{}\) during training. These schemes and the test accuracy as a function of \(\) are illustrated in Fig.3b. The whiskers, the line in the middle of the box, and the filled area indicate the maximum and minimum values, the average value, and the data distribution, respectively. The black and red dashed lines indicate the best performances of the standard BP and DFA on SNNs in five trials, at 97.78% and 96.75%, respectively (BP and DFA work unstably). The corresponding PRFSs that are randomly selected within each interval and \(f^{}\) are illustrated in Fig.3c.

Figure 3: **Feasibility of the aDFA-SNNs scheme.****(a)** The distribution of the correlation coefficient \(\) between PRFSs and \(f^{}\) at different orders of scaling factor \(\). The \(x\) axis represents values of \(\), and the \(y\) axis denotes the probability density of distribution. **(b)** The test accuracy on MNIST task as a function of \(\) between \(f^{}\) and PRFSs. The whiskers, the line in the middle of the box, and the filled area indicate the maximum and minimum values, the average value, and the distribution density, respectively. The dashed lines indicate the best performances of standard BP and DFA in five trials, which are 97.78% and 96.75%, respectively. **(c)** Figures of the corresponding shapes of PRFSs in each interval. The blue and orange lines represent selected PRFSs for aDFA and BP, respectively, and the gray line represents \(f^{}\).

For BP with PRFSs, the average test accuracies are lower than 90%, regardless of the range of \(\), indicating the general failure of BP with randomly selected nonlinearities. On the other hand, when \(\) is greater than -0.2, aDFA can work stably and achieve good performance, even outperforming the best accuracy of standard BP, thus, demonstrating the effectiveness of aDFA on SNNs. The test accuracy of aDFA is significantly reduced and performs unstably when \(g\) and \(f^{}\) exhibit excessive negative correlation (i.e., \(<-0.2\)). This is somewhat different from the conclusion of aDFA on feedforward neural networks in , where aDFA can work effectively by employing arbitrary \(g\). We think this is caused by the fact that we shift \(g\) to positive functions to avoid the effect of negative values on the accumulation and firing processes of LIF neurons. The results presented herein demonstrate the feasibility of the aDFA-SNNs framework, elucidate its effective working conditions, that is, by using positive nonlinear functions that are not excessively negatively correlated with \(f^{}\) as backward function \(g\), and show that the BP-SNNs scheme fails to utilize the mechanism of employing relaxed nonlinear functions. In general, to effectively train SNNs by using BP-based methods, it is necessary to approximate the dynamics of neurons so that we can carefully design the nonlinear functions in the backward process. While aDFA allows training of SNNs with relaxed nonlinear function, which do not contain any hyperparameter used in the dynamics of SNN neurons. This relaxed mechanism significantly alleviates the difficulty of physical implementation and allows avoiding the complex process of approximating SNN dynamics as the differentiable function.

We employ the genetic algorithm (GA), an evolutionary computational technique for updating and optimizing parameters [42; 43; 44], to search for parameter combinations of PRFS that achieve good performance in the aDFA-SNNs scheme described above. In this experiment, we randomly generate 10 PRFSs and use the test accuracy on MNIST and F-MNIST  tasks after one epoch of training as the fitness scores to optimize the random parameters \(p_{k}\) and \(q_{k}\) in Eq.5. The number of generations is set to 20. The performance of aDFA with GA-selected PRFSs, along with the standard BP and DFA schemes, are summarized in Table.1. As can be seen, the \(f^{}\) based standard BP and DFA schemes exhibit unstable behavior and poor average performance on both tasks. Here aDFA scheme in our experiments demonstrates stable performance, and outperforms the standard BP and DFA schemes. Through GA, we also find that the initial irregular PRFSs always converge to shapes with a specific characteristic after evolution, that is, the "bell curve" near the peak of \(f^{}\), as shown in Fig.A.1. Detailed information can be seen in Appendix F. Furthermore, information about the scheme's stability to changes in key features-network scale and temporal dynamics-of SNNs, and the competitive performance achieved by directly tuning backward process parameters, with comparisons to existing studies, is also presented in appendix.

## 4 Conclusion

In the present study, we investigated the implementation of aDFA-a learning mechanism that is easy to implement physically and that is biologically plausible-on the SNNs platform. By using PRFS-random functions with universal properties-as the backward function \(g\) to replace the meticulous designed derivative \(f^{}\) of LIF neurons, we systematically showed the feasibility of the aDFA-SNNs scheme. We have presented the range of the validity of the approach and utilized GA to identify the PRFS settings that yield good performance. We also analyzed the impact of crucial features of SNNs on this scheme, so that showing the superiority and stability of it. Finally, we directly adjusted the \(B\) and \(g\) of schemes with determined forms of \(g\), thus achieving competitive performance. Compared with BP and DFA, in our experiments, the stable and competitive performance obtained by the aDFA-SNNs scheme, which leverages the simple, straightforward, and hardware-friendly learning mechanism, provides a valuable reference for training SNNs. In the future, we will continue to explore the application of aDFA methods on more complex SNN models, and focus on developing general and efficient methods for optimizing backward functions \(g\) to achieve competitive performance.

   &  &  \\  Mechanism & Backward function & Best & Average & Best & Average \\  BP & \(f^{}\) & 97.78\% & 87.46\% & 72.71\% & 66.17\% \\  DFA & \(f^{}\) & 96.75\% & 92.09\% & 84.48\% & 82.54\% \\  aDFA & PRFS & **98.01\%** & **97.91\%** & **87.43\%** & **87.20\%** \\  

Table 1: **Performances of BP, DFA and aDFA. Bold fonts indicate the best performances.**