# Image Captioners Are Scalable Vision Learners Too

Michael Tschannen\({}^{,}\) &Manoj Kumar\({}^{}\) &Andreas Steiner\({}^{}\)

Xiaohua Zhai &Neil Houlsby &Lucas Beyer\({}^{}\)

Google DeepMind

###### Abstract

Contrastive pretraining on image-text pairs from the web is one of the most popular large-scale pretraining strategies for vision backbones, especially in the context of large multimodal models. At the same time, image captioning on this type of data is commonly considered an inferior pretraining strategy. In this paper, we perform a fair comparison of these two pretraining strategies, carefully matching training data, compute, and model capacity. Using a standard encoder-decoder transformer, we find that captioning alone is surprisingly effective: on classification tasks, captioning produces vision encoders competitive with contrastively pretrained encoders, while surpassing them on vision & language tasks. We further analyze the effect of the model architecture and scale, as well as the pretraining data on the representation quality, and find that captioning exhibits the same or better scaling behavior along these axes. Overall our results show that plain image captioning is a more powerful pretraining strategy than was previously believed.

## 1 Introduction

Contrastive language image pretraining (CLIP)  has recently become a very popular pretraining strategy, enabling scalable vision-language pretraining on billions of image-text pairs collected from the web. CLIP not only enables zero-shot transfer to arbitrary image classification and retrieval tasks, but also produces high-quality vision backbones rivaling the best label-supervised ones . The corresponding checkpoints  are among the most powerful publicly available vision backbones, enjoying wide adoption in the context of multi-modal vision-language models, where often a pretrained vision backbone is combined with a pretrained language model .

Before contrastive image-text pretraining was popularized by CLIP  and ALIGN , a number of works explored generative image captioning  and n-gram prediction  approaches for representation learning at small scale. However,  showed that the zero-shot classification performance of contrastive pretraining scales much better than captioning as a function of the number of training examples seen (see [50, Fig. 2]). Since then, follow-up works of  which focus on pretraining from scratch do usually not rely on captioning alone, but augment it with a contrastive loss . Those follow-up works based on generative captioning alone typically rely on cross-modal encoders with early fusion of image and text, targeting visual question answering (VQA) and/or captioning tasks with the full encoder-decoder system , and do not study the properties of the vision backbone alone.

We revisit image captioning as a pretraining task for learning general vision encoders from web image-text pairs. We compare CLIP-style models with image captioning ones consisting of a Vision Transformer (ViT)  encoder and a standard Transformer decoder, henceforth simply referred to as Captioner (Cap). In our experiments, we carefully match pretraining compute and model capacity, and train both models for the same number of examples.

While our results confirm the findings of  that Cap models generally lag contrastive ones in zero-shot classification accuracy, the gap becomes smaller with increasing scale. More importantly, the Cap vision backbone matches or outperforms comparable CLIP models in few-shot classification and obtains competitive performance when transferring to classification tasks with large labeled data sets. When combining the vision encoder with a randomly initialized transformer decoder, the Cap pretrained encoder outperforms the CLIP one on captioning, OCR, and VQA tasks. This indicates that a Cap vision backbone might be better suited for multimodal downstream tasks. These benefits can be even more pronounced when combining the vision encoder with a pretrained, partially frozen language decoder similar to .

We further propose the CapPa pretraining procedure: the decoder training alternates between standard autoregressive prediction (Cap) and _parallel_ prediction (Pa), where the entire caption is predicted in a single decoder forward pass. This only changes the decoder text input (all input tokens are set to [MASK] tokens) and self-attention mask (no mask instead of a causal mask) while requiring no change in the loss or architecture. This mixed training, termed CapPa, significantly improves classification accuracy of the vision backbone.

We ablate how various training and architecture design choices impact performance, and discover promising scaling properties of captioners. Finally, we show that Cap achieves state-of-the-art performance in zero-shot classification tasks which require careful handling of word order and attributes, rather than treating the query as a bag-of-words, as measured by ARO  and SugarCrepe .

_Overall, our results show that pretraining a simple encoder-decoder architecture via image captioning alone can produce vision encoders competitive with CLIP and presents an interesting alternative to contrastive pretraining--in particular for multimodal vision-language models built form pretrained vision and language modeling components._

## 2 Related work

Large scale contrastive vision-language pretraining was popularized by CLIP  and ALIGN . Several works investigated scaling beyond these works along several relevant axes  or using pretrained vision and language backbones with contrastive training .

Recent works investigating plain image captioning for pretraining are ;  study n-gram prediction from images, which can be considered a precursor to captioning. However, image captioning as a pretraining task to learn general vision representations did not attract as much attention as contrastive training, possibly because of inferior and less efficient zero-shot transfer capabilities. Related to captioning, SimVLM  uses prefix language modeling to pretrain a multimodal encoder

Figure 1: Contrastive models **(left)** use two separate Transformer encoders to extract vector representations from image-text pairs, which are then matched across a potentially large batch . Cap **(middle)** uses a Transformer encoder-decoder architecture  and predicts text tokens autoregressively. During training, all tokens are predicted in parallel by shifting the expected output by one token and applying a causal self-attention mask (teacher forcing). In _parallel_ decoding **(right)** the Transformer decoder has to predict all tokens at once, conditioned only on the image. CapPa trains a single model switching between autoregressive and parallel decoding.

D: model width, M: number of image tokens, N: number of text tokens, V: vocabulary size.

decoder model with early vision-language fusion and hybrid convolutional/transformer vision encoder, targeting transfer to VQA and captioning. Further, encoder-decoder models for document, website, and UI understanding are often pretrained to generate captions from rendered websites/documents which trains the model to extract features from those data types and perform OCR [36; 29; 38]. Focusing on image captioning only, LEMON  investigates scaling of an encoder-decoder model.

Several works have combined contrastive and captioning losses [66; 40; 34], optionally using a separate text encoder in addition to the decoder. While obtaining excellent results on a range of vision and vision-language tasks, the impact of the loss terms are not well disentangled, nor are compute-matched comparisons of individual losses provided.

Image captioning is a popular ingredient to build large multimodal models from separately pretrained vision and language models [1; 6; 62; 23; 17; 5]. Some of these models freeze large parts of vision and language components, sometimes only learning a small adapter between the two [1; 17; 39]. It is interesting to see how the pretraining strategy for the vision model affects this type of adaptation.

Finally, masked image-text modeling, often combined with image-text matching, is a popular pretraining strategy for encoder-only vision-language models at small data scale [24; 30; 41; 64; 56; 16].

## 3 A simple recipe to pretrain vision encoders via image captioning

Our goal is to establish a pretraining recipe based on image captioning that is comparable to CLIP in terms of simplicity, scalability, and efficiency. Therefore, we adopt Vision Transformer (ViT)  backbones as vision encoders, and use a standard Transformer decoder architecture  to predict image captions, feeding the ViT-encoded sequence to the decoder via cross-attention. As is common in recent literature [52; 11], we remove biases from attention layers, MLP blocks, and LayerNorms and we replace ReLU by GELU. The decoder input and output embeddings are not shared. Ablations for these choices are in Section 4.4. The decoder has the same width, attention-heads, and MLP dimension as the encoder, but has half the depth. This leads to captioning models which have slightly lower parameter count than corresponding CLIP models, but require about the same pretraining compute in terms of accelerator hours per epoch (see Table 1). We rely on standard next-token-prediction language modeling and train our captioning models with causal attention mask and teacher forcing (Fig. 1, middle), henceforth referring to this variant simply as Captioner (Cap).

Parallel predictionWe also experiment with a slightly modified training recipe (Fig. 1, right): Instead of training the model only for autoregressively, we train it to predict all tokens in parallel for a fraction of the training examples instead (sampling the prediction type for every example at random throughout training). To this end, we replace the (shifted) decoder input sequence with a sequence of all [MASK] tokens, and drop the causal decoder attention mask. We emphasize that this kind of parallel prediction does not modify the training objective or decoder architecture, and does not incur any extra training cost, but simply modifies the decoder input sequence and attention mask for a fraction of training examples. Moreover, this is different from bag-of-word prediction as not only the presence but also the position of each token has to be predicted. We perform parallel prediction for 75% of the training examples by default and term this variant Cap with parallel prediction (CapPa).

Intuitively, captioning via next token prediction induces an implicit weighting on the supervisory signal of the caption tokens: To predict the first few tokens of a caption, the decoder can benefit a lot from using the image information, while to predict later tokens it can rely more and more on already predicted tokens. Consequently, early tokens might provide a stronger supervisory signal to the encoder than later tokens. By contrast, when predicting all the caption tokens independently in parallel, the decoder can only rely on the image information to predict each token.

Implementation aspectsWe emphasize that our approach closely follows a standard encoder/decoder transformer architecture , with the only fundamental difference being the input data format and patch embedding, as well as the modification of the decoder input sequence and attention mask for parallel prediction. This means that our approach is easy to implement in existing transformer code bases [52; 47; 57]. We do not rely on image-specific preprocessing operations other than resizing, see Section 4.1. As a result, existing infrastructure for model-parallel and distributed training can readily be used to scale our approach. In particular, our loss does not require computations across devices the way CLIP does.

Zero-shot classification via scoringImage captioning models allow for zero-shot classification simply by scoring the class name. Unlike with contrastive models, where the text (class) embeddings can be computed once and reused for new images, with captioning models all class names have to be scored again for every new image. Cap/CapPa are hence less efficient zero-shot classifiers than CLIP. We emphasize that we focus on learning vision encoders here and zero-shot transfer is not our focus.

## 4 Experiments

### Experiment setup

Pretraining dataWe use a subset of the WebLI data set  which contains 10B images and 12B multilingual alt-texts. Specifically, we rely on the WebLI subset corresponding to English websites and apply text-based filtering similar to [26, Sec. 3], [4, Sec 2.2] to obtain 1B image/alt-text pairs, not using any image-text similarity-based filtering. Importantly, WebLI was de-duplicated w.r.t. the images in the evaluation data sets we use in this paper. Please refer to [6, Sec 2.2] for more details on the WebLI data set and to [6, Appendix B] for a datasheet.

Pretraining details and baselines1We use a batch size of 8k for our captioning models (larger batch size did not lead to improved performance), and both 8k and 16k for our retrained CLIP baselines (henceforth denoted by CLIP\({}^{*}\) to avoid confusion with the model checkpoints released by , which we reference by CLIP in our results). We explicitly note the batch size for CLIP\({}^{*}\) models when relevant; CLIP\({}^{*}\) without modifier refers to the variant trained with batch size 16k. Models are trained on up to 9B image/alt-text pairs (corresponding to 9 epochs on our subset of WebLI). We use the AdaFactor variant from  with a cosine schedule (with 10k warmup steps), and set learning rate and decay factor to \(10^{-3}\) and \(10^{-4}\), respectively. Previous work  established these optimizer settings for contrastive pretraining and we adopt them here for captioning. Images are resized to a resolution of \(224 224\), and alt-texts are tokenized to a 32k-sized vocabulary with a sentence piece model trained on the English portion of C4 , with a maximum sequence length of 64. Following  for CLIP\({}^{*}\) we use identically sized image and text towers, and use global average pooling (GAP) to compute the image representation.

### Evaluation protocols and data sets

We focus on properties of the frozen representations and also present some fine-tuning results.

   Model & Params & TPU-hrs. \\  B/16 Cap & 192 M & 454 \\ B/16 CLIP* & 197 M & 444 \\ L/14 Cap & 570 M & 1570 \\ L/14 CLIP* & 640 M & 1596 \\   

Table 1: Parameter count and TPUv4-hrs. per bn. examples seen.

    &  &  &  &  \\   & i1k & sun & food & res & pet & COCO & Flickr & VQA & VQAv2 & GQA \\  Cap & 80.2\(\)0.1 & 82.3\(\)0.2 & 90.3\(\)0.1 & 93.6\(\)0.1 & 93.1\(\)0.1 & **117.5\(\)0.3** & **78.6\(\)1.1** & **62.2\(\)0.1** & **68.2\(\)0.1** & **55.0\(\)0.1 \\ CapPa & **81.3\(\)0.1** & 82.4\(\)0.1 & 90.9\(\)0.1 & 94.2\(\)0.2 & **94.4\(\)0.1** & **117.9\(\)0.6** & **80.5\(\)0.2** & **62.2\(\)0.0** & **68.3\(\)1.5** & **55.7\(\)0.2** \\ CLIP* (8k) & 81.1\(\)0.0 & **83.2\(\)0.1** & 91.2\(\)0.0 & **94.8\(\)0.0** & 93.4\(\)0.2 & 115.8\(\)0.2 & 74.5\(\)1.2 & 56.0\(\)0.1 & 66.5\(\)0.1 & 54.3\(\)0.3 \\ CLIP* (16k) & **81.4\(\)0.1** & **83.3\(\)0.1** & **92.0\(\)0.1** & **95.2\(\)0.2** & **93.6\(\)0.2** & **116.3\(\)0.7** & 77.1\(\)0.7 & 56.5\(\)0.1 & 66.7\(\)0.1 & **54.8\(\)0.6** \\ CLIP & 81.6\(\)0.1 & 82.5\(\)0.0 & 92.6\(\)0.1 & 94.9\(\)0.1 & 93.6\(\)0.1 & 118.4\(\)0.6 & 78.7\(\)0.9 & 60.0\(\)0.1 & 67.8\(\)0.1 & 57.5\(\)0.1 \\  CapPa L/14 & 84.4\(\)0.0 & 84.9\(\)0.1 & 93.8\(\)0.0 & 96.0\(\)0.1 & **95.6\(\)0.2** & **125.8\(\)0.1** & **89.3\(\)1.4** & **65.6\(\)0.1** & **70.9\(\)0.0** & **58.3\(\)0.2** \\ CLIP* L/14 & **84.7\(\)0.1** & **85.7\(\)0.1** & **94.6\(\)0.1** & **96.4\(\)0.0** & 95.2\(\)0.1 & 123.2\(\)0.6 & 85.5\(\)0.3 & 61.3\(\)0.2 & 68.5\(\)0.1 & 55.3\(\)0.1 \\ CLIP L/14 & 84.8\(\)0.0 & 84.8\(\)0.1 & 95.2\(\)0.1 & 96.3\(\)0.1 & 95.4\(\)0.3 & 124.4\(\)0.6 & 87.1\(\)0.7 & 64.1\(\)0.0 & 70.4\(\)0.1 & 58.7\(\)0.1 \\   

Table 2: Performance of frozen visual representations trained via image captioning (Cap/CapPa) and contrastively (CLIP\({}^{*}\)), when combined with a single transformer decoder trained from scratch for image classification, captioning and VQA (we use CIDEr for captioning, the VQAv2 weighted accuracy for VQAv2, and exact matching accuracy for all other tasks). Bold marks results where two standard-deviation interval overlaps with the two standard-deviation interval of the best result.

Probing the frozen representationAs an inexpensive way to assess classification performance we use the linear adaptation protocol (based on the pre-logits layer for CLIP\({}^{*}\) and GAP of the encoder output sequence for our captioning models) and eval sets from [68; 70], reporting the 10-shot classification accuracy. We also evaluate the classification accuracy when using the full ImageNet1k training set to learn a dense layer, an MLP, and a multihead attention pooling (MAP)-based  classifier. To assess the amenability of the different the CLIP\({}^{*}\) and CapPa vision encoders to fine-grained classification, we train MAP-based classifiers on a range of specialized data sets which can be divided in two groups. The first group requires fine-grained classification of animal or plant breed [45; 61; 48; 28], or product variant [32; 3], whereas data sets in the second group covers a range of distinct objects [18; 12; 8; 71; 46; 19].

Text encoder/decoder-based inferenceWe use LiT  to learn a text embedding matched to the embedding of our pretrained vision encoders. Generally, LiT is an efficient way to equip any pretrained vision backbone with zero-shot classification and retrieval capabilities, here particularly for Cap whose pretrained decoder does in principle have these capabilities but incurs significant inference cost. We follow the setup from  and assess the zero-shot classification accuracy on ImageNet1k  and retrieval recall@1 on COCO captions . We choose the LiT text encoder to mirror the architecture of the vision encoder at hand and attach a randomly initialized MAP head to the vision encoder to map into the shared image-text embedding space. For comparison, we also apply LiT to our CLIP\({}^{*}\) models; this is not necessarily meant to be practically useful (continuing training the CLIP\({}^{*}\) model might be a better investment of compute).

Motivated by recent work combining pretrained vision backbones and language models [1; 6; 62; 23; 17; 5], we investigate the amenability of the learned representations to interface with a text decoder. Concretely, we use the "LiT decoder" setup  which trains a transformer decoder from scratch on top of a frozen vision representation to solve captioning [7; 65], VQA [20; 25; 43] and classification [53; 71; 3; 8; 48] tasks in a multitask fashion (we use the default hyperparameters from  except for the data mixing strategy set to "concat image-question pairs" [2, Sec. 5.3] ).

In addition, we explore combining our representations with a pretrained T5 decoder . We rely on the previously described multitask LiT decoder setup and tune the most important hyper parameters (see supplementary material for details). For the T5 decoder we keep all the parameters frozen but reinitialize and train the cross-attention layers. Finally, we also leverage a frozen, pretrained 12-layer GPT-2 decoder  for image captioning by combining it with the frozen vision encoders via an adapter, similar to ClipCap .

Fine-tuningWe fine-tune our vision encoders on the full ImageNet1k data set, attaching a randomly initialized MAP head to the pretrained representation (see supplementary material for details).

Using the pretrained text decoderFinally, we also evaluate our models (with the pretrained text decoder) on the SugarCrepe  and the Attribute, Relation and Order (ARO)  benchmarks (which are derived from different captioning data sets [33; 65; 7]). Specifically, SugarCrepe and ARO shuffle the attributes, relations and order of captions and measures the sensitivity of vision-language

    &  &  &  \\  LiT pairs: & 3B & 12B & 3B & 12B & 3B & 12B \\  Cap & 67.8 & 69.0 & 37.5 & 39.1 & 53.9 & 54.8 \\ CapPa & 68.8 & **70.2** & 37.3 & 38.6 & 53.9 & 55.1 \\ CLIP* & 69.0 & 70.0 & 38.9 & **40.1** & 55.1 & **57.0** \\ CLIP & 68.3 & & 32.3 & & 52.8 \\  CapPa L/14 & 76.4 & **77.5** & 43.9 & 45.4 & 60.3 & **62.6** \\ CLIP* L/14 & 75.8 & 76.6 & 44.7 & **46.3** & 60.7 & 62.3 \\ CLIP L/14 & 75.1 & & 36.5 & & 56.6 \\   

Table 4: Frozen transfer for zero-shot classification and retrieval via LiT . Especially for the larger model, CapPa is competitve with CLIP\({}^{*}\) with comparable or fewer LiT examples seen.

    &  &  &  &  \\  Cap & 57.2\(\)0.1 & 58.6\(\)0.8 & 83.7\(\)0.6 & 84.2\(\)0.1 \\ CapPa & **59.1\(\)0.1** & 62.4\(\)0.4 & **86.5\(\)0.1** & **86.6\(\)0.2** \\ CLIP* (8k) & **58.5\(\)0.1** & 64.9\(\)0.4 & 77.7\(\)1.5 & 80.8\(\)0.4 \\ CLIP* (16k) & **59.7\(\)0.4** & **66.3\(\)0.2** & 80.6\(\)1 & 82.9\(\)0.1 \\ CLIP & 59.0\(\)0.2 & 68.6\(\)0.1 & 82.1\(\)0.7 & 70.2\(\)0.7 \\  CapPa L/14 & **70.6\(\)0.2** & **72.9\(\)0.4** & **92.6\(\)0.5** & **92.2\(\)0.2** \\ CLIP* L/14 & **69.8\(\)0.1** & **74.1\(\)0.8** & **87.7\(\)0.9** & **89.2\(\)0.2** \\ CLIP L/14 & 69.7\(\)0.1 & 79.4\(\)0.3 & 90.4\(\)0.38 & 81.1\(\)0.4 \\   

Table 3: 10-shot linear evaluation accuracy on the pre-logit representation. CapPa outperforms Cap and achieves overall comparable results with CLIP\({}^{*}\) trained with a batch size of 16k.

models to these manipulations. As shown by , contrastive models are not very sensitive to precise relational and attributional information and behave closer to bag-of-words models. Intuitively, since captioning-based models model the joint distribution over all tokens, it is interesting to see how they compare to contrastive models on ARO. For each example, we use log-likelihood to score both the true caption and the shuffled captions. The model prediction is the caption with the highest score.

### Main results

Tables 2, 3, 4, and Fig. 3 show the LiT decoder results, the 10-shot classification accuracy, the LiT tuning results, and the full linear probing accuracy for our models trained for 9B examples seen.

CapPa outperforms Cap across almost all evaluations, and often also outperforms CLIP\({}^{*}\) trained with the same batch size (8k), while being competitive with CLIP\({}^{*}\) trained with a batch size of 16k. This is remarkable since CLIP\({}^{*}\) benefits substantially from a large batch size . The trend becomes more pronounced when increasing the model size: CapPa clearly outperforms CLIP\({}^{*}\) in 10-shot classification accuracy for a ViT-L/14 encoder (Table 3).

When transferred to ImageNet1k classification with a linear probe (Fig. 3), the frozen Cap and CapPa encoders lag somewhat behind CLIP\({}^{*}\), but the gap almost vanishes when using a MAP head instead of a linear probe. This is not very surprising, given CLIP\({}^{*}\) models are trained with a linear head, which might induce linear separability in the average pooled encoder output representation. In contrast, the Cap models feed into a decoder via cross-attention which might not impose linear separability as strongly. For fine-tuning the full model on ImageNet1k (Table 5) we only observe a minor gap between CapPa and CLIP\({}^{*}\). As for text encoders learned via LiT (Table 4) CapPa outperforms CLIP\({}^{*}\) for long LiT training and large model size in zero-shot classification, but is outperformed by CLIP\({}^{*}\) in retrieval. Notably, for a short LiT tuning with 3B examples our models outperform CLIP  in zero-shot classification and retrieval while matching

Figure 3: Linear probing makes cap pre-trained image encoders seem worse, but when learning the pooling (MAP probe), the gap is essentially closed.

Figure 2: 10-shot classification accuracy on the frozen pre-logit representation (left two columns); captioning and VQA performance with a decoder (right two columns). **Top row:** Performance of vision backbones pretrained with captioning (Cap/CapPa) and contrastively (CLIP\({}^{*}\)) as a function of the model size/FLOPs (we compare ViT-S/16, M/16, B/16, and L/14). CapPa exhibits favorable scaling behavior on captioning and VQA tasks. **Bottom row:** Performance of CapPa and CLIP\({}^{*}\) as a function of the number of training examples seen. The behavior is similar as for model scale.

    & B/16 & B/16\({}_{834}\) & L/14\({}_{336}\) \\  CLIP\({}^{*}\) & 84.9 & 86.0 & 88.1 \\ CapPa & 84.4 & 85.7 & 87.7 \\ Cap & 83.9 & 85.3 & - \\   

Table 5: Finetuning on INet.

the number of examples seen by CLIP (12.8B) when summing over pretraining (9B) and LiT tuning (3B), despite the vision encoder being frozen during LiT tuning (which saves compute). Matching the number of examples seen by CLIP during LiT tuning (12B) leads to a clear additional improvement.

Combining our models with a fresh (LiT) decoder (Table 2), we observe that CapPa performs better than CLIP\({}^{*}\) trained with batch size 16k on captioning and VQA, while obtaining competitive accuracy on classification tasks. Again, this pattern becomes more pronounced with increased models size. Indeed, for a ViT-L/14 encoder CapPa even outperforms CLIP in the majority of LiT decoder tasks.

Scaling propertiesIn Fig. 2 we present an analysis of our encoders as a function of the model size and the number of training examples seen for a selection of 10-shot classification and vision & language evaluations using a fresh decoder (plots for all evaluations and Cap can be found in the supplementary material). It can be seen that CLIP\({}^{*}\) and CapPa models exhibit similar scaling behavior, with CapPa showing a somewhat steeper increase in captioning and VQA performance as a function of model size and examples seen, in particular for a ViT-L/14 backbone. This indicates that the benefits of CapPa models might become more pronounced with further model and data scaling.

Attribution, relation, orderingTable 6 presents our results on the ARO Benchmark . Cap and CapPa models achieve close to a perfect score on the ordering subsets. They outperform the best contrastive variants by around 30% and 40% on Flickr Order and COCO Order, respectively. We also compare with NegCLIP , which employs additional supervision to make contrastive models sensitive to word ordering and attribution. Cap and CapPa exceed NegCLIP by 8% and 13% out-of-the-box on COCO Order and Flickr Order. The same can be observed for the attribution, and relation subsets of ARO. So we are facing a clear trade-off: CLIP-style models outperform captioning-based models in terms of standard zero-shot classification accuracy, whereas the latter are much better at processing fine-grained descriptions of visual content. Finally, we also trained

    & Attrib. & Rel. & Order (F) & Order (C) \\  Blind dec. & 83.7 & 86.2 & 98.8 & 98.7 \\ Cap & **88.9** & 86.6 & 99.1 & **99.0** \\ CapPa & 85.7 & **86.7** & **99.2** & 98.8 \\ CLIP\({}^{*}\) & 53.2 & 39.7 & 45.5 & 37.0 \\  CLIP & 62.7 & 58.7 & 57.9 & 49.5 \\ ARO Best & 88.0 & 73.0 & 60.0 & 46.0 \\ NegCLIP & 71.0 & 81.0 & 91.0 & 86.0 \\   

Table 6: Results on the Attribute, Relation and Order (ARO) benchmark . Cap and CapPa clearly outperform all CLIP\({}^{*}\) and CLIP variants across all data sets. They also outperform NegCLIP  which was explicitly trained to be “Add”s plausible but wrong details to the caption.

Figure 4: Absolute improvement of CapPa over CLIP\({}^{*}\) in various settings. **Left:** CapPa pairs significantly better with decoders in image-language tasks, especially when the decoder is a pre-trained and frozen language model. **Right:** CapPa seems to be a noticeably better frozen feature extractor for fine-grained classification tasks (we show L/14 results, see appendix for B/16).

   Training & Arch & Repl. & Swap & Add \\  Cap & B/16 & **88.21** & **84.00** & 98.94 \\ CapPa & B/16 & 87.67 & 83.11 & **99.13** \\ CLIP\({}^{*}\) & B/16 & 81.95 & 63.22 & 81.91 \\  NegCLIP & B/32 & 85.36 & 75.33 & 87.29 \\ OpenCLIP & G/14 & 86.50 & 68.56 & 88.36 \\   

Table 7: Results on the SugarCrepe  benchmark, which fixes known issues in previous image-text benchmarks like ARO. Full results are in Table 18 in the appendix. Small Cap and CapPa models outperform even large or hard-negative trained CLIP models in all categories, and essentially solve the category of tests which “Add”s plausible but wrong details to the caption.

a Cap model with no image input (Blind dec.), which is just a language model for alt-text. This model overall performs only slightly worse than Cap and CapPa, which suggests that the sentence manipulation in ARO can to a large extent be detected by language modeling alone. This was also observed in concurrent work .

SugarCrepeThe SugarCrepe benchmark , introduced concurrently to our work, promises to fix the issues in ARO and similar benchmarks; for instance, a blind model is no better than chance. Still, even our small captioning pretrained B/16 model outperforms the largest G/14 contrastive model as well as the specifically trained NegCLIP. The "Swap" category is especially sensitive to the relation between multiple things in the image, something that is fundamentally hard for contrastive pretraining to learn. The "Add" category, where highly plausible but wrong things are added to the text is essentially solved by captioners. The full breakdown, more baselines, and qualitative examples are provided in Tables 18 and 22-24 in the appendix. This result is strong evidence that captioning as a pretraining objective imbues capabilities to the model that contrastively trained models are blind to.

Frozen T5 decoderWe also trained models with frozen encoder _and_ frozen decoder (initialized with a T5 checkpoint). For these experiments, only the cross attention weights were trained (28M trainable parameters, compared to the 248M trainable parameters when the decoder is trained from scratch). The relative improvements of CapPa over CLIP\({}^{*}\) are shown in Fig. 4 (left). Even though the absolute performance of the decoder trained from scratch (Table 2) is better than when only training the cross attention weights (Table 12), CapPa with a frozen decoder closes the gap on three classification tasks, reverts the trend on ImageNet, and improves the performance by large margins on Pets, as well as all captioning and VQA tasks. This result suggests that the captioning objective is better suited to train an encoder that is later combined with a pretrained language decoder.

Frozen GPT-2 decoderWe combined our frozen vision encoders with a frozen GPT-2 model from  using an adapter as proposed by ClipCap . We found that this setup is less well suited for VQA and multitask conditioning, so we only evaluate it on captioning in single-task fashion as done by ClipCap. We tried the MLP and transformer adapters from , but obtained better results for both CLIP\({}^{*}\) and CapPa with a "resampler", a LayerNorm followed by a single cross-attention layer with learnable query embeddings, generating a prefix of 32 soft tokens for GPT-2. This resampler has only 7.1M parameters, about \(4\) less than the MLP adapter from . CapPa still outperforms CLIP\({}^{*}\) in this setup, but the gains are more modest than for the T5 decoder and a decoder trained from scratch (both single-task, see Fig. 5). We emphasize that both encoder and GPT-2 decoder are frozen and we only use a small adapter.

Fine-grained classificationFig. 4 (right) shows the improvement when using CapPa instead of CLIP\({}^{*}\) for a range of specialized data sets. CapPa outperforms CLIP\({}^{*}\) on the majority of fine-grained tasks which suggests that captioning as a pretraining task leads to better features for fine-grained classification than contrastive pretraining.

### Ablations

All models discussed in this section are trained for 900M training examples seen.

Parallel predictionRecall that for parallel prediction we replace all text input tokens with [MASK] tokens. An alternative would be to only replace a random subset, as done e.g. in , to provide a partial context for the prediction. However, we did not observe improvements of the vision encoder when only masking a fraction of the tokens, so we focused on fully masked input sequences. For fully masked input sequence Fig. 6 (left) shows the improvement in 10-shot classification accuracy over training for pure autoregressive decoding as a function of the fraction of examples for which the decoder is trained for parallel prediction instead. A fraction of 0.75 leads to a balanced improvement across all considered data sets. Finally, alternating between parallel and autoregressive prediction for all examples in a batch, rather than performing parallel prediction with mixed batches, led to significantly worse results.

Figure 5: Absolute improvement (single-task) of CapPa over CLIP\({}^{*}\) for a decoder trained from scratch (LiT-d.), a frozen T5 decoder, and a frozen GPT-2 similar to .

Encoder architectureNext, we investigate the effect of the encoder architecture on the representation quality, comparing CLIP\({}^{*}\) with Cap when using a BiT ResNet50 , ViT-B/32, and a ViT-B/16 vision encoder. We use a B-sized text encoder and B-sized 6-layer decoder for CLIP\({}^{*}\) and Cap, respectively, for all encoders, except for ResNet50 for which we also train with a prefix decoder following [50, Sec. A2]. According to  the BiT ResNet50 obtains performance roughly comparable to ViT-B/32 when trained and evaluated on image classification. Further, ResNet50 and ViT-B/32 both produce a sequence of length 49 before pooling for \(224 224\) images, which we feed to the decoder. Fig. 6 (right) shows the improvement obtained when using Cap instead of CLIP\({}^{*}\) as a function of the encoder architecture. The improvement (regression) in ImageNet zero-shot accuracy is smallest (largest) for the ResNet50 architecture (this is the architecture used to create [50, Fig. 2] that compares contrastive with bag-of-words and captioning approaches) and is significantly improved when using a ViT architecture and when reducing the patch size (which does not increase model capacity but the sequence length and hence FLOPs). Also recall that these models were only trained on 900M examples. Interestingly, the difference between CLIP\({}^{*}\) and Cap on 10-shot metrics are generally smaller, and for a ViT-B/16 encoder the two approaches lead to similar performance.

Captioning task variantsWe train Cap while randomly reversing the caption with probability 0.5. This maintains model capacity and pretraining compute unchanged. We do not observe improved performance (see Table 8, left). While  ablates backwards captioning and shows improvements, they use a separate decoder for backwards captioning, so the ablated model has fewer parameters and FLOPs (here we control for both factors). Additionally, we train a CapPa variant with two parallel decoders, one for autoregressive prediction and another one for parallel prediction, each with 3 layers (instead of 6). This model matches the pretraining compute of the default CapPa but underperforms in the majority of 10-shot tasks.

Training with language-pretrained decoderFinally, we train Cap ViT-B/16 with a frozen pre-trained T5-Base decoder (which has 12 decoder layers). To obtain a stable training setup we adapt the optimizer hyper-parameters (learning rate \(0.0005\), \(_{2}=0.95\)) and unfreeze the cross attention. Optionally, we re-initialize the cross-attention parameters and unfreeze the decoder. None of these variants performs better than Cap overall (see Table 8, right), and the more we allow the decoder to deviate from its language-pretrained weights the better the vision performance gets.

    & ImageNet & CIFAR100 & Pets & Cars \\  Cap & 49.7 & 56.0 & 72.6 & 74.7 \\ Cap (fw.+bw.) & 49.2 & 56.1 & 71.7 & 73.0 \\ CapPa 2 dec. & 49.5 & 54.9 & 75.8 & 79.0 \\ CapPa & 50.4 & 57.4 & 76.2 & 78.5 \\   

Table 8: **Left:** Comparing Cap and CapPa with other captioner variants: Forward and backward captioning with the same decoder, autoregressive and parallel prediction with two 3-layer decoders. **Right:** Training Cap with a frozen T5 decoder does not help, even when unfreezing parts of it.

Figure 6: **Left:** Improvement in 10-shot classification accuracy as a function of the fraction of training examples for which parallel prediction is performed in CapPa. A fraction of 0.75 leads to a balanced improvement. **Right:** Improvement of 10-shot/zero-shot (without prompts) classification accuracy when using Cap instead of CLIP\({}^{*}\). For ResNet-50 (p) the decoder consumes 4 averaged image tokens as a prefix (no cross-attention). Cap is competitive in 10-shot accuracy for ViT-B/16.

Decoder architectureWhile following the original transformer decoder architecture  closely, we adopt the now common change of removing the biases in the decoder [52; 11] to improve stability without affecting performance, see Table 19 (left) in the appendix. We use separate embeddings for the decoder's input and output, Table 19 (left) shows that this works slightly better. We use six decoder layers (see Table 19, right) which simultaneously leads to overall good results while also matching the total parameter count of the corresponding CLIP\({}^{*}\) model.

Inspired by experiments from , where applying a stronger weight decay to the classification head of ViTs trained for image classification led to accuracy improvements, we also experimented with increased weight decay applied to the decoder or cross-attention layers, but we did not observe any benefits from this. Further, we explored using a tokenizer trained on 300M randomly sampled WebLI alt-texts instead of the one pretrained on C4, which did not improve accuracy.

Effect of pretraining dataSo far all our results were based on models pretrained on a variant of WebLI, and one might wonder whether our findings transfer to pretraining on other data sets. We therefore train some of our models and baselines on the smaller, publicly available LAION-400M dataset  which was collected and filtered following a different protocol. For instance, it was filtered using an existing CLIP model to score image-text pairs, which might induce a significantly different bias in the training data and our conclusions. However, the 10-shot linear classification results in Fig. 7 show that the conclusions remain the same: CapPa achieves accuracy comparable with CLIP\({}^{*}\) and outperforms Cap.

## 5 Discussion and conclusion

We presented an extensive comparison of vision encoders pretrained with a contrastive and generative (captioning) objective and found that the generatively pretrained encoders obtain better performance when used for captioning, VQA, fine-grained and few-shot classification tasks, while achieving competitive performance in classification overall. This is in contrast to previous work  which argued that predicting alt-text from images _explicitly and exactly_ is an overly challenging pretraining task and may lead to sub-par vision capabilities. Moreover, our results show that captioning as a pretraining task might exhibit favorable scaling properties with increasing model and data scale, and we hope future work will explore this avenue.

One downside of our approach is that the Cap/CapPa text decoder is of limited use. While it achieves excellent results when word order and object relationships matter--a scenario where CLIP-style models exhibit a strong bag-of-word bias--zero-shot classification and retrieval are computationally expensive. We showed that this can be mitigated relatively cheaply by training a text encoder matched to the frozen Cap/CapPa representation via LiT tuning. Note that such a procedure is cheaper than training the text encoder with the image encoder and text decoder throughout the whole pretraining procedure as done e.g. by CoCa .2 Another promising avenue is to explore parallel prediction for inference task, as it would allow for efficient scoring e.g. for zero-shot classification. Indeed, parallel prediction produces a joint distribution over all tokens, which can be used to score an arbitrary number of classes or queries.

In conclusion, we established plain image captioning as a competitive pretraining strategy for vision backbones from image-text data. We hope to inspire follow up work dedicating more attention the advantages of captioning as a pretraining task for vision encoders.

Figure 7: Pretraining on LAION-400M.

AcknowledgmentsWe would like to thank Jannis Bulian, Mostafa Dehghani, Alexey Dosovitskiy, Daniel Keysers, Mario Lucic, Basil Mustafa, and Xiao Wang for feedback on this paper and inspiring discussions on captioning-based pretraining. We would also like to thank Janko Ferlic from Unsplash for providing the photo used in Fig. 1.