# Implicit Regularization in Over-Parameterized Support Vector Machine

Yang Sui  Xin He Yang Bai

School of Statistics and Management

Shanghai University of Finance and Economics

suiyang1027@stu.sufe.edu.cn;he.xin17@mail.shufe.edu.cn;

statbyang@mail.shufe.edu.cn

Equal contributions.Corresponding author.

###### Abstract

In this paper, we design a regularization-free algorithm for high-dimensional support vector machines (SVMs) by integrating over-parameterization with Nesterov's smoothing method, and provide theoretical guarantees for the induced implicit regularization phenomenon. In particular, we construct an over-parameterized hinge loss function and estimate the true parameters by leveraging regularization-free gradient descent on this loss function. The utilization of Nesterov's method enhances the computational efficiency of our algorithm, especially in terms of determining the stopping criterion and reducing computational complexity. With appropriate choices of initialization, step size, and smoothness parameter, we demonstrate that unregularized gradient descent achieves a near-oracle statistical convergence rate. Additionally, we verify our theoretical findings through a variety of numerical experiments and compare the proposed method with explicit regularization. Our results illustrate the advantages of employing implicit regularization via gradient descent in conjunction with over-parameterization in sparse SVMs.

## 1 Introduction

In machine learning, over-parameterized models, such as deep learning models, are commonly used for regression and classification . The corresponding optimization tasks are primarily tackled using gradient-based methods. Despite challenges posed by the nonconvexity of the objective function and over-parameterization , empirical observations show that even simple algorithms, such as gradient descent, tend to converge to a global minimum. This phenomenon is known as the implicit regularization of variants of gradient descent, which acts as a form of regularization without an explicit regularization term .

Implicit regularization has been extensively studied in many classical statistical problems, such as linear regression  and matrix factorization . These studies have shown that unregularized gradient descent can yield optimal estimators under certain conditions. However, a deeper understanding of implicit regularization in classification problems, particularly in support vector machines (SVMs), remains limited. Existing studies have focused on specific cases and alternative regularization approaches . A comprehensive analysis of implicit regularization via direct gradient descent in SVMs is still lacking. We need further investigation to explore the implications and performance of implicit regularization in SVMs.

The practical significance of such exploration becomes evident when considering today's complex data landscapes and the challenges they present. In modern applications, we often face classificationchallenges due to redundant features. Data sparsity is notably evident in classification fields like finance, document classification, and gene expression analysis. For instance, in genomics, only a few genes out of thousands are used for disease diagnosis and drug discovery . Similarly, spam classifiers rely on a small selection of words from extensive dictionaries . These scenarios highlight limitations in standard SVMs and those with explicit regularization, like the \(_{2}\) norm. From an applied perspective, incorporating sparsity into SVMs is an intriguing research direction. While sparsity in regression has been deeply explored recently, sparse SVMs have received less attention. Discussions typically focus on generalization error and risk analysis, with limited mention of variable selection and error bounds . Our study delves into the implicit regularization in classification, complementing the ongoing research in sparse SVMs.

In this paper, we focus on the implicit regularization of gradient descent applied to high-dimensional sparse SVMs. Contrary to existing studies on implicit regularization in first-order iterative methods for nonconvex optimization that primarily address regression, we investigate this intriguing phenomenon of gradient descent applied to SVMs with hinge loss. By re-parameterizing the parameters using the Hadamard product, we introduce a novel approach to nonconvex optimization problems. With proper initialization and parameter tuning, our proposed method achieves the desired statistical convergence rate. Extensive simulation results reveal that our method outperforms the estimator under explicit regularization in terms of estimation error, prediction accuracy, and variable selection. Moreover, it rivals the performance of the gold standard oracle solution, assuming the knowledge of true support.

### Our Contributions

First, we reformulate the parameter \(\) as \(-\), where \(\) denotes the Hadamard product operator, resulting in a non-convex optimization problem. This re-parameterization technique has not been previously employed for classification problems. Although it introduces some theoretical complexities, it is not computationally demanding. Importantly, it allows for a detailed theoretical analysis of how signals change throughout iterations, offering a novel perspective on the dynamics of gradient descent not covered in prior works [13; 33]. With the help of re-parameterization, we provide a theoretical analysis showing that with appropriate choices of initialization size \(\), step size \(\), and smoothness parameter \(\), our method achieves a near-oracle rate of \(\), where \(s\) is the number of the signals, \(p\) is the dimension of \(\), and \(n\) is the sample size. Notice that the near-oracle rate is achievable via explicit regularization [32; 46; 20]. To the best of our knowledge, this is the first study that investigates implicit regularization via gradient descent and establishes the near-oracle rate specifically in classification.

Second, we employ a simple yet effective smoothing technique [28; 45] for the re-parameterized hinge loss, addressing the non-differentiability of the hinge loss. Additionally, our method introduces a convenient stopping criterion post-smoothing, which we discuss in detail in Section 2.2. Notably, the smoothed gradient descent algorithm is not computationally demanding, primarily involving vector multiplication. The variables introduced by the smoothing technique mostly take values of \(0\) or \(1\) as smoothness parameter \(\) decreases, further streamlining the computation. Incorporating Nesterov's smoothing is instrumental in our theoretical derivations. Directly analyzing the gradient algorithm with re-parameterization for the non-smooth hinge loss, while computationally feasible, introduces complexities in theoretical deductions. In essence, Nesterov's smoothing proves vital both computationally and theoretically.

Third, to support our theoretical results, we present finite sample performances of our method through extensive simulations, comparing it with both the \(_{1}\)-regularized estimator and the gold standard oracle solution. We demonstrate that the number of iterations \(t\) in gradient descent parallels the role of the \(_{1}\) regularization parameter \(\). When chosen appropriately, both can achieve the near-oracle statistical convergence rate. Further insights from our simulations illustrate that, firstly, in terms of estimation error, our method generalizes better than the \(_{1}\)-regularized estimator. Secondly, for variable selection, our method significantly reduces false positive rates. Lastly, due to the efficient transferability of gradient information among machines, our method is easier to be paralleled and generalized to large-scale applications. Notably, while our theory is primarily based on the Sub-Gaussian distribution assumption, our method is actually applicable to a much wider range. Additional simulations under heavy-tailed distributions still yield remarkably desired results. Extensive experimental results indicate that our method's performance, employing implicitly regularized gradient descent in SVMs,rivals that of algorithms using explicit regularization. In certain simple scenarios, it even matches the performance of the oracle solution.

### Related Work

Frequent empirical evidence shows that simple algorithms such as (stochastic) gradient descent tend to find the global minimum of the loss function despite nonconvexity. To understand this phenomenon, studies by [30; 29; 19; 38; 43] proposed that generalization arises from the implicit regularization of the optimization algorithm. Specifically, these studies observe that in over-parameterized statistical models, although optimization problems may contain bad local errors, optimization algorithms, typically variants of gradient descent, exhibit a tendency to avoid these bad local minima and converge towards better solutions. Without adding any regularization term in the optimization objective, the implicit preference of the optimization algorithm itself plays the role of regularization.

Implicit regularization has attracted significant attention in well-established statistical problems, including linear regression [13; 35; 39; 23; 44] and matrix factorization [14; 24; 3; 25; 26]. In high-dimensional sparse linear regression problems, [35; 44] introduced a re-parameterization technique and demonstrated that unregularized gradient descent yields an estimator with optimal statistical accuracy under the Restricted Isometric Property (RIP) assumption .  obtained similar results for the single index model without the RIP assumption. In low-rank matrix sensing, [14; 24] revealed that gradient descent biases towards the minimum nuclear norm solution when initiated close to the origin. Additionally,  demonstrated the same implicit bias towards the nuclear norm using a depth-\(N\) linear network. Nevertheless, research on the implicit regularization of gradient descent in classification problems remains limited.  found that the gradient descent estimator converges to the direction of the max-margin solution on unregularized logistic regression problems. In terms of the hinge loss in SVMs,  provided a diagonal descent approach and established its regularization properties. However, these investigations rely on the diagonal regularization process , and their algorithms' convergence rates depend on the number of iterations, and are not directly compared with those of explicitly regularized algorithms. Besides, Frank-Wolfe method and its variants have been used for classification [21; 34]. However, sub-linear convergence to the optimum requires the strict assumption that both the direction-finding step and line search step are performed exactly .

## 2 Model and Algorithms

### Notations

Throughout this work, we denote vectors with boldface letters and real numbers with normal font. Thus, \(\) denotes a vector and \(w_{i}\) denotes the \(i\)-th coordinate of \(\). We use \([n]\) to denote the set \(\{1,2,n\}\). For any subset \(S\) in \([n]\) and a vector \(\), we use \(_{S}\) to denote the vector whose \(i\)-th entry is \(w_{i}\) if \(i S\) and \(0\) otherwise. For any given vector \(\), let \(\|\|_{1}\), \(\|\|\) and \(\|\|_{}\) denote its \(_{1}\), \(_{2}\) and \(_{}\) norms. Moreover, for any two vectors \(,^{p}\), we define \(^{p}\) as the Hadamard product of vectors \(\) and \(\), whose components are \(w_{i}v_{i}\) for \(i[p]\). For any given matrix \(^{p_{1} p_{2}}\), we use \(\|\|_{F}\) and \(\|\|_{S}\) to represent the Frobenius norm and spectral norm of matrix \(\), respectively. In addition, we let \(\{a_{n},b_{n}\}_{n 1}\) be any two positive sequences. We write \(a_{n} b_{n}\) if there exists a universal constant \(C\) such that \(a_{n} C b_{n}\) and we write \(a_{n} b_{n}\) if we have \(a_{n} b_{n}\) and \(b_{n} a_{n}\). Moreover, \(a_{n}=(b_{n})\) shares the same meaning with \(a_{n} b_{n}\).

### Over-parameterization for \(_{1}\)-regularized SVM

Given a random sample \(^{n}=\{(_{i},y_{i})\}_{i=1}^{n}\) with \(_{i}^{p}\) denoting the covariates and \(y_{i}\{0,1\}\) denoting the corresponding label, we consider the following \(_{1}\)-regularized SVM:

\[_{^{p}}_{i=1}^{n}(1-y_{i} _{i}^{T})_{+}+\|\|_{1},\] (1)

where \(()_{+}=\{,0\}\) denotes the hinge loss and \(\) denotes the \(_{1}\) regularization parameter. Let \(L_{^{n}}()\) denote the first term of the right-hand side of (1). Previous works have shown that the \(_{1}\)-regularized estimator of the optimization problem (1) and its extensions achieve a near-oracle rate of convergence to the true parameter \(^{*}\)[32; 37; 43]. In contrast, rather than imposing the regularization term in (1), we minimize the hinge loss function \(L_{^{n}}()\) directly to obtain a sparse estimator. Specifically, we re-parameterize \(\) as \(=-\), using two vectors \(\) and \(\) in \(^{p}\). Consequently, \(L_{^{n}}()\) can be reformulated as \(_{^{n}}(,)\):

\[_{^{n}}(,)=_{i=1}^{n} 1-y_{i}_{i}^{T}(- )_{+}.\] (2)

Note that the dimensionality of \(\) in (1) is \(p\), but a \(2p\)-dimensional parameter is involved in (2). This indicates that we over-parameterize \(\) via \(=-\) in (2). We briefly describe our motivation on over-parameterizing \(\) this way. Following , \(\|\|_{1}=*{argmin}_{= }(\|\|^{2}+\|\|^{2})/2\). Thus, the optimization problem (1) translates to \(_{,}_{^{n}}(,)+(\|\|^{2}+\|\|^{2})/2\). For a better understanding of implicit regularization by over-parameterization, we set \(=+}{2}\) and \(=-}{2}\), leading to \(=-\). This incorporation of new parameters \(\) and \(\) effectively over-parameterizes the problem. Finally, we drop the explicit \(_{2}\) regularization term \((\|\|^{2}+\|\|^{2})/2\) and perform gradient descent to minimize the empirical loss \(_{^{n}}(,)\) in (2), in line with techniques seen in neural network training, high-dimensional regression [35; 23; 44], and high-dimensional single index models .

### Nesterov's smoothing

It is well-known that the hinge loss function is not differentiable. As a result, traditional first-order optimization methods, such as the sub-gradient and stochastic gradient methods, converge slowly and are not suitable for large-scale problems . Second-order methods, like the Newton and Quasi-Newton methods, can address this by replacing the hinge loss with a differentiable approximation . Although these second-order methods might achieve better convergence rates, the computational cost associated with computing the Hessian matrix in each iteration is prohibitively high. Clearly, optimizing (2) using gradient-based methods may not be the best choice.

To address the trade-off between convergence rate and computational cost, we incorporate Nesterov's method  to smooth the hinge loss and then update the parameters via gradient descent. By employing Nesterov's method, (2) can be reformulated as the following saddle point function:

\[_{^{n}}(,)_{ _{1}}_{i=1}^{n}1-y_{i}_{i}^{T}( -)_{i},\]

where \(_{1}=\{^{n}:0_{i} 1\}\). According to , the above saddle point function can be smoothed by subtracting a prox-function \(d_{}()\), where \(d_{}()\) is a strongly convex function of \(\) with a smoothness parameter \(>0\). Throughout this paper, we select the prox-function as \(d_{}()=\|\|^{2}\). Consequently, \(_{^{n}}(,)\) can be approximated by

\[_{^{n},}^{*}(,)_{ {}_{1}}\{_{i=1}^{n}1-y_{i}_{i}^{T}(-)_{i}-d_{ }()\}.\] (3)

Since \(d_{}()\) is strongly convex, \(_{i}\) can be obtained by setting the gradient of the objective function in (3) to zero and has the explicit form:

\[_{i}=(0,_{i}{}^{T}( -)}{ n},1).\] (4)

For each sample point \(^{i}=\{(_{i},y_{i})\}\), \(i[n]\), we use \(_{^{i}}(,)\) and \(_{^{i},}^{*}(,)\) to denote its hinge loss and the corresponding smoothed approximation, respectively. With different choices of \(_{i}\) for any \(i[n]\) in (4), the explicit form of \(_{^{i},}^{*}(,)\) can be written as

\[_{^{i},}^{*}(,)=0, &\ y_{i}_{i}^{T}(- )>1,\\ (1-y_{i}_{i}^{T}(- ))/n-/2,&\ y_{i}_{i}^{T}(- )<1- n,\\ (1-y_{i}_{i}^{T}(- ))^{2}/(2 n^{2}),&.\] (5)

Note that the explicit solution (5) indicates that a larger \(\) yields a smoother \(_{^{i},}^{*}(,)\) with larger approximation error, and can be considered as a parameter that controls the trade-off between smoothness and approximation accuracy. The following theorem provides the theoretical guarantee of the approximation error. The proof can be directly derived from (5), and thus is omitted here.

**Theorem 1**.: _For any random sample \(^{i}=\{(_{i},y_{i})\}\), \(i[n]\), the corresponding hinge loss \(_{^{i}}(,)\) is bounded by its smooth approximation \(_{^{i},}^{*}(,)\), and the approximation error is completely controlled by the smooth parameter \(\). For any \((,)\), we have_

\[_{^{i},}^{*}(,)_{^{i}}(,)_{^{i}, }^{*}(,)+.\]

### Implicit regularization via gradient descent

In this section, we apply gradient descent algorithm to \(_{^{n},}(,)\) in (3) by updating \(\) and \(\) to obtain the estimator of \(^{*}\). Specifically, the gradients of (3) with respect to \(\) and \(\) can be directly obtained, with the form of \(-2/n_{i=1}^{n}y_{i}_{i}_{i}\) and \(2/n_{i=1}^{n}y_{i}_{i}_{i}\), respectively. Thus, the updates for \(\) and \(\) are given as

\[_{t+1}=_{t}+2_{i=1}^{n}y_{i}_{t,i} _{i}_{t} and_{t+1}=_{t }-2_{i=1}^{n}y_{i}_{t,i}_{i}_{t},\] (6)

where \(\) denotes the step size. Once \((_{t+1},_{t+1})\) is obtained, we can update \(\) as \(_{t+1}=_{t+1}_{t+1}-_{t+1} _{t+1}\) via the over-parameterization of \(\). Note that we cannot initialize the values of \(_{0}\) and \(_{0}\) as zero vectors because these vectors are stationary points of the algorithm. Given the sparsity of the true parameter \(^{*}\) with the support \(S\), ideally, \(\) and \(\) should be initialized with the same sparsity pattern as \(^{*}\), with \(_{S}\) and \(_{S}\) being non-zero and the values outside the support \(S\) being zero. However, such initialization is infeasible as \(S\) is unknown. As an alternative, we initialize \(_{0}\) and \(_{0}\) as \(_{0}=_{0}=_{p 1}\), where \(>0\) is a small constant. This initialization approach strikes a balance: it aligns with the sparsity assumption by keeping the zero component close to zero, while ensuring that the non-zero component begins with a non-zero value .

We summarize the details of the proposed gradient descent method for high-dimensional sparse SVM in the following Algorithm 1.

**Given**: Training set \(^{n}\), initial value \(\), step size \(\), smoothness parameter \(\), maximum iteration number \(T_{1}\), validation set \(}^{n}\).

**Initialize**: \(_{0}=_{p 1}\), \(_{0}=_{1}p 1\), and set iteration index \(t=0\).

**While**\(t<T_{1}\)**, **do**

\[_{t+1} =_{t}+2_{i=1}^{n}y_{i}_{t,i} _{i}_{t};\] \[_{t+1} =_{t}-2_{i=1}^{n}y_{i}_{t,i} _{i}_{t};\] \[_{t+1} =_{t+1}_{t+1}-_{t+1} _{t+1};\] \[_{t+1,i} =(0,_{i}{}^{T} _{t+1}}{n},1);\] \[t =t+1.\]

**End if**\(t>T_{1}\) or \(_{t}=\).

**Return** Set \(}\) as \(_{t}\).

**Algorithm 1** Gradient Descent Algorithm for High-Dimensional Sparse SVM.

We highlight three key advantages of Algorithm 1. First, the stopping condition for Algorithm 1 can be determined based on the value of \(\) in addition to the preset maximum iteration number \(T_{1}\). Specifically, when the values of \(_{i}\) are \(0\) across all samples, the algorithm naturally stops as no further updates are made. Thus, \(\) serves as an intrinsic indicator for convergence, providing a more efficient stopping condition. Second, Algorithm 1 avoids heavy computational cost like the computation of the Hessian matrix. The main computational load comes from the vector multiplication in (6). Since a considerable portion of the elements in \(\) are either \(0\) or \(1\), and the proportion of these elements increases substantially as \(\) decreases, the computation in (6) can be further simplified. Lastly, the utilization of Nesterov's smoothing not only optimizes our approach but also aids in our theoretical derivations, as detailed in Appendix E.

## 3 Theoretical Analysis

In this section, we analyze the theoretical properties of Algorithm 1. The main result is the error bound \(\|_{t}-^{*}\|\), where \(^{*}\) is the minimizer of the population hinge loss function for \(\) without the \(_{1}\) norm: \(^{*}=_{^{p}}(1-y}^{T})_{+}\). We start by defining the \(\)-incoherence, a key assumption for our analysis.

**Definition 1**.: _Let \(^{n p}\) be a matrix with \(_{2}\)-normalized columns \(_{1},,_{p}\), i.e., \(\|_{i}\|=1\) for all \(i[n]\). The coherence \(=()\) of the matrix \(\) is defined as_

\[:=_{K[n],1 i j p}|_{i} _{K},_{j}_{K}|,\]

_where \(_{K}\) denotes the \(n\)-dimensional vector whose \(i\)-th entry is \(1\) if \(i K\) and \(0\) otherwise. Then, the matrix \(\) is said to be satisfying \(\)-incoherence._

Coherence measures the suitability of measurement matrices in compressed sensing . Several techniques exist for constructing matrices with low coherence. One such approach involves utilizing sub-Gaussian matrices that satisfy the low-incoherence property with high probability [9; 7]. The Restricted Isometry Property (RIP) is another key measure for ensuring reliable sparse recovery in various applications [35; 44], but verifying RIP for a designed matrix is NP-hard, making it computationally challenging . In contrast, coherence offers a computationally feasible metric for sparse regression [9; 23]. Hence, the assumptions required in our main theorems can be verified within polynomial time, distinguishing them from the RIP assumption.

Recall that \(^{*}^{p}\) is the \(s\)-sparse signal to be recovered. Let \(S\{1,,p\}\) denote the index set that corresponds to the nonzero components of \(^{*}\), and the size \(|S|\) of \(S\) is given by \(s\). Among the \(s\) nonzero signal components of \(^{*}\), we define the index set of strong signals as \(S_{1}=\{i S:|_{i}^{*}| C_{s} p\}\) and of weak signals as \(S_{2}=\{i S:|_{i}^{*}| C_{w}\}\) for some constants \(C_{s},C_{w}>0\). We denote \(s_{1}\) and \(s_{2}\) as the cardinalities of \(S_{1}\) and \(S_{2}\), respectively. Furthermore, we use \(m=_{i S_{1}}|_{i}^{*}|\) to represent the minimal strength for strong signals and \(\) to represent the condition number-the ratio of the largest absolute value of strong signals to the smallest. In this paper, we focus on the case that each nonzero signal in \(^{*}\) is either strong or weak, which means that \(s=s_{1}+s_{2}\). Regarding the input data and parameters in Algorithm 1, we introduce the following two structural assumptions.

**Assumption 1**.: _The design matrix \(/\) satisfies \(\)-incoherence with \(0< 1/( s p)\). In addition, every entry \(x\) of \(\) is \(i.i.d.\) zero-mean sub-Gaussian random variable with bounded sub-Gaussian norm \(\)._

**Assumption 2**.: _The initialization for gradient descent are \(_{0}=_{p 1}\), \(_{0}=_{p 1}\) where the initialization size \(\) satisfies \(0< 1/p\), the parameter of prox-function \(\) satisfies \(0< 1/n\), and the step size \(\) satisfies \(0< 1/( p)\)._

Assumption 1 characterizes the distribution of the input data, which can be easily satisfied across a wide range of distributions. Interestingly, although our proof relies on Assumption 1, numerical results provide compelling evidence that it isn't essential for the success of our method. This indicates that the constraints set by Assumption 1 can be relaxed in practical applications, as discussed in Section 4. The assumptions about the initialization size \(\), the smoothness parameter \(\), and the step size \(\) primarily stem from the theoretical induction of Algorithm 1. For instance, \(\) controls the strength of the estimated weak signals and error components, \(\) manages the approximation error in smoothing, and \(\) affects the accuracy of the estimation of strong signals. Our numerical simulations indicate that extremely small initialization size \(\), step size \(\), and smoothness parameter \(\) are not required to achieve the desired convergence results, highlighting the low computational burden of our method, with details found in Section 4. The primary theoretical result is summarized in the subsequent theorem.

**Theorem 2**.: _Suppose that Assumptions 1 and 2 hold, then there exist positive constants \(c_{1},c_{2},c_{3}\) and \(c_{4}\) such that there holds with probability at least \(1-c_{1}n^{-1}-c_{2}p^{-1}\) that, for every time \(t\) with \(c_{3}(m/^{2})/( m) t c_{4}(1/)/( n)\), the solution of the \(t\)-th iteration in Algorithm 1, \(_{t}=_{t}_{t}-_{t}_{t}\), satisfies_

\[\|_{t}-^{*}\|^{2}.\]Theorem 2 demonstrates that if \(^{*}\) contains \(s\) nonzero signals, then with high probability, for any \(t[c_{3}(m/^{2})/( m),c_{4}(1/)/( n)]\), the convergence rate of \(()\) in terms of the \(_{2}\) norm can be achieved. Such a convergence rate matches the near-oracle rate of sparse SVMs and can be attained through explicit regularization using the \(_{1}\) norm penalty [32; 46], as well as through concave penalties . Therefore, Theorem 2 indicates that with over-parameterization, the implicit regularization of gradient descent achieves the same effect as imposing an explicit regularization into the objective function in (1).

**Proof Sketch.** The ideas behind the proof of Theorem 2 are as follows. First, we can control the estimated strengths associated with the non-signal and weak signal components, denoted as \(\|_{t}_{S^{c}_{1}}\|_{}\), and \(\|_{t}_{S^{c}_{1}}\|_{}\), to the order of the square root of the initialization size \(\) for up to \(((1/)/( n))\) steps. This provides an upper boundary on the stopping time. Also, the magnitude of \(\) determines the size of coordinates outside the signal support \(S_{1}\) at the stopping time. The importance of choosing small initialization sizes and their role in achieving the desired statistical performance are further discussed in Section 4. On the other hand, each entry of the strong signal part, denoted as \(_{t}_{S_{1}}\), increases exponentially with an accuracy of around \(( p/n)\) near the true parameter \(^{*}_{S_{1}}\) within roughly \(((m/^{2})/( m))\) steps. This establishes the left boundary of the stopping time. The following two Propositions summarize these results.

**Proposition 1**.: _(Analyzing Weak Signals and Errors) Under Assumptions 1-2, with probability at least \(1-cn^{-1}\), we have_

\[\|_{t}_{S^{c}_{1}}\|_{} } and\|_{t}_{S^{c}_{1}}\| _{}},\]

_for all \(t T^{*}=((1/)/( n))\), where \(c\) is some positive constant._

**Proposition 2**.: _(Analyzing Strong Signals) Under Assumptions 1-2, with probability at least \(1-c_{1}n^{-1}-c_{2}p^{-1}\), we have_

\[\|_{t}_{S_{1}}-^{*}_{S_{1} }\|_{}},\]

_holds for all \(((m/^{2})/( m)) t((1/)/ ( n))\) where \(c_{1},c_{2}\) are two constants._

Consequently, by appropriately selecting the stopping time \(t\) within the interval specified in Theorem 2, we can ensure convergence of the signal components and effectively control the error components. The final convergence rate can be obtained by combining the results from Proposition 1 and Proposition 2.

## 4 Numerical Study

In our simulations, unless otherwise specified, we follow a default setup. We generate \(3n\) independent observations, divided equally for training, validation, and testing. The true parameters \(^{*}\) is set to \(m_{S}\) with a constant \(m\). Each entry of \(\) is sampled as \(i.i.d.\) zero-mean Gaussian random variable, and the labels \(y\) are determined by a binomial distribution with probability \(p=1/(1+(^{T}^{*}))\). Default parameters are: true signal strength \(m=10\), number of signals \(s=4\), sample size \(n=200\), dimension \(p=400\), step size \(=0.5\), smoothness parameter \(=10^{-4}\), and initialization size \(=10^{-8}\). For evaluation, we measure the estimation error using \(\|_{t}/\|_{t}\|-^{*}/\|^{*}\|\|\) (for comparison with oracle estimator) and the prediction accuracy on the testing set with \(P(=y_{test})\). Additionally, "False positive" and "True negative" metrics represent variable selection errors. Specifically, "False positive" means the true value is zero but detected as a signal, while "True negative" signifies a non-zero true value that isn't detected. Results are primarily visualized employing shaded plots and boxplots, where the solid line depicts the median of \(30\) runs and the shaded area marks the \(25\)-th and \(75\)-th percentiles over these runs.

**Effects of Small Initialization Size.** We investigate the power of small initialization size \(\) on the performance of our algorithm. We set the initialization size \(=\{10^{-4},10^{-6},10^{-8},10^{-10}\}\), and other parameters are set by default. Figure 1 shows the importance of small initialization size in inducing exponential paths for the coordinates. Our simulations reveal that small initialization size leads to lower estimation errors and more precise signal recovery, while effectively constraining the error term to a negligible magnitude. Remarkably, although small initialization size might slow the convergence rate slightly, this trade-off is acceptable given the enhanced estimation accuracy.

**Effects of Signal Strength and Sample Size.** We examine the influence of signal strength on the estimation accuracy of our algorithm. We set the true signal strength \(m=0.5*k,k=1,,20\) and keep other parameters at their default values. As depicted in Figure 2, we compare our method (denoted by **GD**) with \(_{1}\)-regularized SVM (denoted by **Lasso** method), and obtain the oracle solution (denoted by **Oracle**) using the true support information. We assess the advantages of our algorithm from three aspects. Firstly, in terms of estimation error, our method consistently outperforms the Lasso method across different signal strengths, approaching near-oracle performance. Secondly, all three methods achieve high prediction accuracy on the testing set. Lastly, when comparing variable selection performance, our method significantly surpasses the Lasso method in terms of false positive error. Since the true negative error of both methods is basically \(0\), we only present results for false positive error in Figure 2.

We further analyze the impact of sample size \(n\) on our proposed algorithm. Keeping the true signal strength fixed at \(m=5\), we vary the sample size as \(n=50*k\) for \(k=1,,8\). Other parameters remain at their default values. Consistently, our method outperforms the Lasso method in estimation, prediction, and variable selection, see Figure 3 for a summary of the results.

**Performance on Complex Signal Structure.** To examine the performance of our method under more complex signal structures, we select five signal structures: \(-(5,6,7,8)\), \(-(4,6,8,9)\), \(-(3,6,9,10)\), \(-(2,6,10,11)\), and \(-(1,6,11,12)\). Other parameters are set by default. The

Figure 1: Effects of small initialization size \(\). In Figure 1(a), the estimation error is calculated by \(\|_{t}-^{*}\|/\|^{*}\|\).

Figure 2: Effects of signal strength \(m\). The orange vertical line in Figure 2(a) show the threshold of strong signal \( p\).

results, summarized in Figure 4, highlight the consistent superiority of our method over the Lasso method in terms of prediction and variable selection performance, even approaching an oracle-like performance for complex signal structures. High prediction accuracy is achieved by the both methods.

**Performance on Heavy-tailed Distribution.** Although the sub-Gaussian distribution of input data is assumed in Assumption 1, we demonstrate that our method can be extended to a wider range of distributions. We conduct simulations under both uniform and heavy-tailed distributions. The simulation setup mirrors that of Figure 4, with the exception that we sample \(\) from a \([-1,1]\) uniform distribution and a \(t(3)\) distribution, respectively. Results corresponding to the \(t(3)\) distribution are presented in Figure 5, and we can see that our method maintains strong performance, suggesting that the constraints of Assumption 1 can be substantially relaxed. Additional simulation results can be found in the Appendix A.

**Sensitivity Analysis with respect to smoothness parameter \(\).** We analyze the impact of smoothness parameter \(\) on our proposed algorithm. Specifically, the detailed experimental setup follows the default configuration, and \(\) is set within the range \([2.5 10^{-5},1 10^{-3}]\). The simulations are replicated \(30\) times, and the numerical results of estimation error and four estimated signal strengths are presented in Figure 6. From Figure 6, it's evident that the choice of \(\) is relatively insensitive in the sense that the estimation errors and the estimated strengths of the signals under different \(\)s are very close. Furthermore, as \(\) increases, the estimation accuracy experiences a minor decline, but it remains within an acceptable range. See Appendix A for simulation results of Signal \(3\) and Signal \(4\).

Figure 4: Performance on complex signal structure. The boxplots are depicted based on \(30\) runs.

Figure 3: Effects of sample size \(n\).

## 5 Conclusion

In this paper, we leverage over-parameterization to design an unregularized gradient-based algorithm for SVM and provide theoretical guarantees for implicit regularization. We employ Nesterov's method to smooth the re-parameterized hinge loss function, which solves the difficulty of non-differentiability and improves computational efficiency. Note that our theory relies on the incoherence of the design matrix. It would be interesting to explore to what extent these assumptions can be relaxed, which is a topic of future work mentioned in other studies on implicit regularization. It is also promising to consider extending the current study to nonlinear SVMs, potentially incorporating the kernel technique to delve into the realm of implicit regularization in nonlinear classification. In summary, this paper not only provides novel theoretical results for over-parameterized SVMs but also enriches the literature on high-dimensional classification with implicit regularization.

## 6 Acknowledgements

The authors sincerely thank the anonymous reviewers, AC, and PCs for their valuable suggestions that have greatly improved the quality of our work. The authors also thank Professor Shaogao Lv for the fruitful and valuable discussions at the very beginning of this work. Dr. Xin He's and Dr. Yang Bai's work is supported by the Program for Innovative Research Team of Shanghai University of Finance and Economics and the Shanghai Research Center for Data Science and Decision Technology.

Figure 5: Performance on complex signal structure under \(t(3)\) distribution. The boxplots are depicted based on \(30\) runs.

Figure 6: Sensitivity analysis of smoothness parameter \(\). The boxplots are depicted based on \(30\) runs. The estimation error is calculated by \(\|_{t}-^{*}\|/\|^{*}\|\).