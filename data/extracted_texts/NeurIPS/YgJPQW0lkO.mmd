# Graph-based Uncertainty Metrics for

Long-form Language Model Outputs

 Mingjian Jiang

Stanford University

jiangm@stanford.edu

&Yangjun Ruan

Stanford University

ryoungj@stanford.edu

&Prasanna Sattigeri

IBM Research

psattig@us.ibm.com

&Salim Roukos

IBM Research

roukos@us.ibm.com

&Tatsunori Hashimoto

Stanford University

thashim@stanford.edu

###### Abstract

Recent advancements in Large Language Models (LLMs) have significantly improved text generation capabilities, but these systems are still known to hallucinate, and granular uncertainty estimation for long-form LLM generations remains challenging. In this work, we propose Graph Uncertainty - which represents the relationship between LLM generations and claims within them as a bipartite graph and estimates the claim-level uncertainty with a family of graph centrality metrics. Under this view, existing uncertainty estimation methods based on the concept of self-consistency can be viewed as using degree centrality as an uncertainty measure, and we show that more sophisticated alternatives such as closeness centrality provide consistent gains at claim-level uncertainty estimation. Moreover, we present uncertainty-aware decoding techniques that leverage both the graph structure and uncertainty estimates to improve the factuality of LLM generations by preserving only the most reliable claims. Compared to existing methods, our graph-based uncertainty metrics lead to an average of 6.8% relative gains on AUPRC across various long-form generation settings, and our end-to-end system provides consistent 2-4% gains in factuality over existing decoding techniques while significantly improving the informativeness of generated responses1.

## 1 Introduction

Large Language Models (LLMs)  have demonstrated remarkable capabilities and been widely used as an interactive chatbot to provide knowledge and answers to user queries. However, they still struggle with generating false information, often referred to as "hallucinations" , which hinders their ability to provide calibrated  and factual  responses and ultimately undermines the trust users place in their outputs. Improving uncertainty estimation techniques is crucial for building trust in LLMs and mitigating the risks associated with their deployment in real-world applications.

While many existing uncertainty estimation techniques for LLMs primarily focus on estimating the uncertainty of their answers to multiple-choice questions  or their entire generated responses (typically in a short form) , they are often not sufficiently informative in real-world applications where LLMs generate paragraphs of texts consisting of a mixture of true and false claims . In such scenarios, more granular uncertainty estimates are needed to help users distinguish the reliability of each individual claim within the generated text. Recent approaches  attempt to measure the uncertainty of each claim by its consistency with randomly sampled responses based on the concept of self-consistency . However, they do not fully leverage the semantic relationships between claims and responses that could support more granular uncertainty estimation.

We introduce Graph Uncertainty (Fig. 1), a framework for granular, claim-level uncertainty estimation for LLMs which utilizes the fine-grained semantic relationships over a claim-response entailment graph. Our key idea is motivated by the observation that given multiple responses sampled from LLMs and a set of claims decomposed from them, we can construct a bipartite graph that captures the semantic entailment relationships between each response and claim, which serves as a summary statistic that captures uncertainty information associated with each response and claim. On this graph, claim-level uncertainties correspond to the _importance_ of each node in the graph, and we extract such information with a family of graph centrality metrics [18; 19; 20; 21] that measure the 'importance' of each claim node within the graph. A notable example is the existing uncertainty estimates based on the concept of self-consistency [17; 11], which turns out to be a special instantiation of our framework with the degree centrality as the uncertainty measure. Our framework generalizes it to accommodate a broader family of well-studied graph centrality metrics that capture more granular graph information than degree centrality. Through a systematic benchmarking of various graph centrality metrics and different baselines, we demonstrate that closeness centrality  is a natural and high-performance uncertainty estimator and outperforms baselines by an average of 6.8% on AUPRC at claim-wise uncertainty estimation for models like GPT-4  on two challenging long-form generation datasets, FactScore  and PopQA .

Furthermore, we demonstrate that our granular, claim-level uncertainty estimates translate to gains in the factuality of LM outputs by integrating them with an uncertainty-aware decoding process that can leverage these graph metrics. The idea is straightforward and builds upon ideas explored in Mohri and Hashimoto , Wang et al. : after obtaining claim-level uncertainty estimates following our method, we filter the claim set using uncertainty scores and synthesize the remaining claims with low uncertainty to produce the final response. Our empirical analysis demonstrates that our framework generates responses with better factuality without compromising their informativeness, achieving a better tradeoff compared to existing methods. Notably, our approach provides consistent 2-4% gains in factuality and can generate 70% more true claims at the 95% precision level compared to baselines.

The main contributions of our work are as follows:

* We introduce Graph Uncertainty, a general framework for claim-level uncertainty estimation of long-form LLM generations through the use of semantic graphs and graph centrality metrics.
* We demonstrate that our method significantly outperforms existing methods with an average of 6.8% gains on AUPRC for claim-level uncertainty estimation, and provide a systematic benchmarking of different baselines and our method with various graph centrality metrics in these settings.
* We present a straightforward and effective framework that integrates granular uncertainty estimates into LLM decoding for both factual and informative generations. Our method provides consistent 2-4% factuality gains and generates 70% more true claims than baselines at 95% precision level.

## 2 Related Work

**Short-form uncertainty estimation in LLMs** Existing approaches for characterizing the uncertainty of LLMs have largely focused on multiple-choice classification or short-form generation setups and

Figure 1: **Graph Uncertainty for claim-level uncertainty estimation**. We first sample several responses from LLMs (a) and decompose each response into atomic claims (b) following Sec. 4.1. The key components are the construction of a bipartite graph that captures the relations between responses and claims (c) and the use of graph centrality metrics to estimate the uncertainty of each claim. We simplify the pipeline and prompt for presentation, see Appx. F for details.

can be categorized into likelihood-based [13; 7; 24; 9], consistency/ensemble-based [10; 25], and verbalizer-based [14; 15] methods. In this line of work, Kadavath et al.  transforms uncertainty estimation into a binary classification task to estimate the probability of whether a sample is true or not. Kuhn et al.  proposes to estimate the'semantic entropy' of the sample distribution given a prompt to address the surface-form competition  in the generated samples. Lin et al.  generalizes it to incorporate more fine-grained similarities between different samples and utilize degree or eigenvalue-related metrics for better discriminative performance. Xiong et al.  proposes a framework for systematically evaluating verbalizer and consistency strategies of eliciting confidence scores for black-box LLM, without access to model parameters or activations. Our work is distinct from these existing works in our focus on the long-form setting, which requires uncertainty estimation at a more granular level.

**Granular uncertainty estimation** Recent works have begun to extend uncertainty estimation to long-form outputs. Duan et al.  and Band et al.  obtain claim-level uncertainty scores from long-form outputs but operate in a white-box setting - requiring access to model internals - and therefore do not apply to API-based LLMs. Most relevant to our work, Manakul et al.  extends the concept of self-consistency  to assess uncertainty at the sentence level within long-form outputs, which is applicable to black-box LLM. Building upon this, Mohri and Hashimoto  performs uncertainty estimation at the claim level, combining a self-consistency style technique with conformal prediction. However, these works purely rely on the sample-and-count technique  that may not sufficiently capture the semantic relationships between claims and responses. Our work improves granular uncertainty estimation by considering more fine-grained semantic information contained in the entailment graph and its associated centrality measures.

**Factuality of LLMs** There have been several works on enhancing the factuality of LLMs at various stages, including retrieval [30; 31], pretraining [32; 33], and fine-tuning [34; 35; 29]. These methods typically require a reliable knowledge database for retrieval or extensive model training, which can be impractical and costly. Our work focuses on improving the factuality of LLMs at inference time and can be combined with other techniques. In this context, CoVe  proposes that LLMs can enhance their outputs through a series of planning and self-verification steps. DoLA  dynamically selects intermediate layers at each decoding step to minimize the generation of incorrect facts, though this method is only applicable to white-box LLMs. Recently, Wang et al.  and Mohri and Hashimoto  study methods that leverage claim-level uncertainty estimates for more factual LM outputs. We show that improved uncertainty estimators and decoders based on the entailment graph formalism lead to significant improvements in the factuality of LM outputs.

## 3 Preliminary

In this work, we focus on the problem of granular uncertainty estimation for LLMs, particularly in the context of distinguishing whether each claim in a long-form output is factual. Specifically, let \(\) denote the set of all characters and \(^{*}\) the space of all possible text strings. Given a text prompt \(x^{*}\), the generation process of a model \(M\) with a specified temperature \(T=t\) can be represented as a conditional probability distribution \(M_{T=t}(|x)\) over \(^{*}\).

**Uncertainty estimation for LLMs** In the context of LLMs, uncertainty estimation is concerned with the following: given a model \(M\), a prompt \(x^{*}\), and a response \(y^{*}\), we seek an uncertainty function \(U:^{*}^{*}\) that measures the uncertainty of LLMs about the response. In this work, we define the efficacy of \(U\) by how effectively it differentiates the true and false claims of \(y\), using classification metrics such as AUROC and AUPRC. We focus on classification metrics rather than calibration or coverage, as our main goal will be using \(U\) to identify and remove false claims as part of a decoding-time intervention.

**Claim-level uncertainty estimation** In many practical applications, the outputs from an LLM encompass a few paragraphs of text containing multiple _claims_[11; 16]. We consider a claim to be the smallest semantically distinct unit of information presented within the generated output. For example, in Fig. 1, "Snedden was elected to the Australian Parliament" is an example of a single claim. In this work, instead of assigning a single uncertainty score to the entire output, we assess uncertainty at the level of individual claims. Formally, we further define \(\) as a universal set of all unique, semantically distinct claims. The claim-level uncertainty function is then \(U:^{*}\), allowing for granular analysis of factuality at the claim level.

**Black-box LLM setup & Existing approaches** We focus on settings where we can only access the LLM outputs for each prompt and not its internal architecture or likelihood estimates. This reflects the real-world scenario for the most capable models, such as GPT-4  and Claude-2 , which are typically accessed through API calls. The black-box assumption restricts the applicability of certain uncertainty estimation methods, such as those that rely on activation layers or logits [24; 37; 29]. There are relatively few methods that apply to this black-box setting. A few examples of uncertainty quantification methods that are applicable include:

* **Verbalized Confidence (VC)**[14; 15] based approaches which involve prompting the LLM to express its confidence in a claim \(c\) directly, based on the prompt \(x^{*}\). The uncertainty is quantified by parsing the verbalized confidence expression (e.g.,"very confident","100%", etc.) and mapping it to a numerical value.
* **Self-consistency (SC)**[17; 10; 11] based approaches involve checking the claim \(c\) (typically decomposed from the greedily decoded output \(M_{T=0}(x)\)) against a set of sampled responses \(=\{r^{(i)}\}_{i[N]}\) where \(r^{(i)} M_{T=t}(|x)\) at a higher temperature \(t>0\). The uncertainty estimate of \(c\) is typically calculated by the proportion of responses \(r^{i}\) that entail (denoted by \(\)) the claim \(c\), where \(N\) is the total number of generated responses. Formally, the consistency score for a claim \(c\) can be expressed as \((c)=_{i=1}^{N}[r^{(i)} c]\). A higher consistency score indicates lower uncertainty, as the claim is more consistently entailed across the diverse responses.

Empirical evidence [10; 16] suggests that SC often surpasses VC in its effectiveness for uncertainty estimation.

## 4 Claim-Level Uncertainty Estimation with Semantic Entailment Graphs

Our motivation stems from the observation that given a set of generated responses \(\) and their entailed claims \(\), we can construct a bipartite graph \(G=((,),)\) between \(\) and \(\) with edges \(\) indicating the entailment relationship between each response and claim (Fig. 1). This graph captures the semantic entailment relationship between responses and claims, from which we may extract information that effectively captures the uncertainty of each claim. A motivating example is SC, which turns out to be a special case of calculating the degree centrality of each claim node as their uncertainty. This encourages the investigation of a broad family of graph-based metrics beyond the node degree, potentially offering more robust uncertainty estimates by exploiting intra-graph information.

In the following section, we will demonstrate how to construct the semantic entailment graph using an LLM in Sec. 4.1, and then describe the graph metrics we are exploring in Sec. 4.2.

### Semantic Entailment Graph Construction

Here we describe the procedure for constructing a bipartite graph \(G=((,),)\) that captures the generation-claim relationships for a given input \(x\) using an LLM, as illustrated in Fig. 1. The graph construction involves multiple LLM interactions, with detailed prompts provided in Appx. F.

  
**Metric** & **Formula** & **Brief Explanation** \\ 
**Degree** & \(C_{D}(v)=_{u V}A_{vu}\) & Number of edges incident to a node \(v\) \\ 
**Betweenness** & \(C_{B}(v)=_{s v t}(v)}{_{st}}\) & Fraction of shortest paths \(_{st}\) between other nodes \(s\), \(t\) that pass through a node \(v\) \\ 
**Eigenvector** & \(C_{E}(v)=_{u N(v)}A_{vu}C_{E}(u)\) & Importance of a node \(v\) measured by the importances of its neighboring nodes \(N(v)\) \\ 
**PageRank** & \(C_{PR}(v)=+d_{u N(v)}(u)}{N(u)}\) & Stationary distribution of a random walk within the graph with restart. \(d\) is the damping factor. \\ 
**Closeness** & \(C_{C}(v)=d(v,u)}|}\) & Reciprocal of the average shortest path distance to all nodes \\   

Table 1: **Graph centrality metrics with their formulas and brief explanations.** We utilize the centrality metric value for each claim node to measure the uncertainty of the claim. Self-consistency-based estimate corresponds to the specific case of using the degree centrality. \(V\) and \(A\) are the node set and adjacency matrix of the graph \(G\). A full definition of the notations is provided in Appx. E.

**Step 1: Response sampling (\(\))** Given an input prompt \(x\), we follow the same procedure as SC  to generate a set of \(||\) responses from the LLM, including one greedily decoded response \(M_{T=0}(x)\) and \(||-1\) responses from \(M_{T=t}(|x)\). This process produces a set of generated responses \(=\{M_{T=0}(x),M_{T=t}^{(1)}(x),,M_{T=t}^{(||-1)}(x)\}\).

**Step 2: Claim decomposition and merging (\(C\))** We select a subset \(_{N}\) of \(N\) responses from \(\). For each response \(r_{N}\), we first prompt the LLM to decompose \(r\) into a set of claims, denoted as \(_{r}\), following the procedure in . Since the claims from different responses may semantically overlap, we also utilize an LLM to merge all claims into a comprehensive set of all unique, semantically distinct claims. Specifically, we prompt an LLM to implement a union function \(:()() ()\), which takes two claim sets \(^{(1)},^{(2)}\) and merges them according to their semantic meaning. This is approximated by prompting the LLM to evaluate whether each claim in \(^{(2)}\) is entailed by any claim in \(^{(1)}\), and only those that are not are kept and appended to the original set. By sequentially prompting the LLM to merge the claim sets, we could get a union of all claims in \(_{N}\), i.e., \(^{(1)}=_{r_{1}},^{(i)}=(^{(i-1)},_{r_{i}})\) for \(i\{2,,N\}\) where \(r_{i}_{N}\). The final set forms our set of claim nodes \(=^{(N)}\).

**Step 3: Edge construction (\(\))** To construct the bipartite graph, we link the responses in \(\) to the claims in \(\). An edge \(e\) between a response \(r\) and a claim \(c\) is established if \(r\) entails \(c\). The entailment relation is determined by prompting the same LLM \(M\), following the procedure in .

### Uncertainty Estimation with Graph Centrality Metrics

Recall that SC corresponds to using the specific claim node degree as the uncertainty estimate. Intuitively, the effectiveness of SC demonstrates that the more 'connected' a claim node is to other nodes in the graph, the more likely the claim to hold true. Drawing on this premise, we explore a broader family of graph centrality metrics [18; 19; 20; 21] that measure the importance of a claim node within the graph from different angles, some of which may correlate with the factuality of the node to a greater extent. Specifically, denoting the graph \(G=(V,A)\) here by its node set \(V\) and adjacency matrix \(A\), we assess the graph centrality metrics detailed in Table 1. These include the betweenness \(C_{B}\), eigenvalue \(C_{E}\), PageRank \(C_{PR}\), and closeness centrality \(C_{C}\). A full definition of the notations is provided in Appendix E. Note that we use the Wasserman and Faust (WF) improved formula  for closeness centrality to ensure applicability to disconnected graphs.

These centrality metrics are pre-defined to measure the importance of a node in a graph in different ways, and it is not clear a priori which types of centrality are useful for uncertainty estimation. Therefore, we carefully study all of these centrality metrics in various settings in our experiments (Sec. 6.1). We use these centrality metric values of the claim nodes within the bipartite graph as their confidence scores (i.e., the negative values as their uncertainty estimates), and evaluate the correlation between these metric values and the claim factualities. This analysis helps us identify the most empirically effective centrality metric for uncertainty estimation at the granularity of claims.

## 5 Uncertainty-Aware Decoding

We have introduced a graph-based technique for estimating the uncertainty at the level of individual claims. To demonstrate that our uncertainty estimation method translates to more factual LLM outputs, we now present a framework that integrates these uncertainty estimates at decoding time to improve the factuality of LLM outputs (Fig. 2). Similar to contemporaneous work on factuality-enhancing decoding [16; 23], we filter claims by uncertainty score to retain only confident claims. We show in our experiments that our use of the entire claim set (vs claims associated with a single output,

Figure 2: **Our uncertainty-aware decoding framework. Based on our claim-wise uncertainty estimates obtained from Fig. 1, we keep low-uncertainty claims above a certain confidence threshold and use LLMs to synthesize them into a coherent response. Varying the threshold enables us to balance factuality and informativeness.**compared to other works) and our improved uncertainty metrics lead to improved factuality without compromising the informativeness of LLM outputs.

The intuition of our approach is to retain only the most confident claims and to synthesize them into a single coherent response. A detailed description of the steps is provided below.

1. **Create a candidate claim set with uncertainty estimates:** For a given prompt \(x\), generate a candidate claim set \(\) for the final output, along with their uncertainty estimates \(U(x,c), c\). Different approaches can apply here, and we follow the same procedure described in Sec. 4.2.
2. **Filter claims by uncertainty estimates**: Filter out claims from the candidate claim set with a high uncertainty estimate above a certain threshold \(\). This creates an operational subset of \(,^{o}=\{c|U(x,c)<\}\), containing claims with low uncertainties. The threshold \(\) can be selected either heuristically in an unsupervised manner or based on a percentile \(q\) over training data claims, where the latter approach provides correctness guarantees with factuality probability levels determined by \(q\). Following the supervised approach, we determine \(\) by selecting a percentile \(q\) and computing the corresponding threshold on a small set of training data.
3. **Integrate selected claims:** Integrate the selected claims in the operational subset (\(^{o}\)) into a single, coherent output using an LLM. The prompt details for this step can be found in Appx. F.

In subsequent sections, we show that our approach provides favorable factuality-informativeness tradeoffs, where factuality is defined as the precision of the generated claims, and informativeness is defined as the number of true claims included in the output, analogous to the recall of true claims.

## 6 Experiments

In Sec. 6.1, we benchmark our proposed graph-based metrics and existing methods adapted for claim-wise uncertainty on two long-form factuality datasets, demonstrating the effectiveness of closeness centrality as a reliable uncertainty measure. In Sec. 6.2, we show that applying the closeness centrality metric with our uncertainty-aware decoding framework demonstrates the best informativeness-factuality trade-off for long-form generation and empirically analyze the impact of each component. Additionally, Sec. 6.3 presents an ablation study to investigate the factors contributing to the performance of closeness centrality and provide insights for interpretation.

### Uncertainty Estimation

In this subsection, we empirically analyze different graph centrality metrics for uncertainty estimation and systematically benchmark existing methods adapted for claim-wise uncertainty estimation.

**Datasets and annotation** We evaluated the different uncertainty estimation methods on two challenging datasets, FActScore  and (long-form) PopQA , where even the most capable LLMs like GPT-4  demonstrate frequent factuality failures. For each dataset, we randomly sampled 100 entities and generated a set of claims about each entity with their uncertainty estimates using our pipeline described in Sec. 4.1. This process yielded over 2000 claims on average for each evaluation setting. We briefly describe each dataset and their annotation details here, and include additional details in Appx. A:

* **FActScore** is a widely used dataset for evaluating the factuality of long-form text generation for LLMs, containing entities sourced from Wikipedia. To assess the factuality of claims, we employed a similar pipeline to the one in their paper, classifying them as True, False, or Subjective using LLMs conditioned on the corresponding Wikipedia article. We specifically used GPT-4-Turbo due to its low classification error rate.
* **Long-form PopQA** comprises of entities covering a diverse range of subjects. The original PopQA was not designed for long-form generation, we adapted it by adjusting the prompt to "Provide me with a paragraph detailing some facts related to subject". To ensure data quality, we filtered out entities that either lacked a Wikipedia page or had pages shorter than 1500 characters. The factuality of claims was evaluated by GPT-4-Turbo using the associated Wikipedia pages as reference, where longer Wikipedia pages were preferred as they typically provide more comprehensive coverage of entity information, thus reducing the risk of false negative annotations.

We also evaluated different methods on the **Natural Question** dataset  and observed consistent gains. Due to a higher rate of false negatives in the auto-annotation pipeline compared to the aforementioned datasets, we have included these results in Appx. C.1.

**Baseline methods** Since existing approaches mostly focus on uncertainty estimation at the generation level, we adapted them to claim-level estimation for the purpose of baseline comparison. In particular, we conducted a systematic benchmarking of a wide range of methods including:

* **Post-hoc verbalized confidence (PH-VC)** This method is a variant of **Verbalized confidence (VC)** as introduced in Sec. 3, which elicits the verbalized confidence in a post-hoc manner after the entire claim set \(\) has been decomposed from generations, following Tian et al. .
* **In-line verbalized confidence (IL-VC)** This method directly elicits the verbalized confidence about each claim \(c\) in an in-line manner right after it is decomposed from the generations during Step 2 in Sec. 4.1 and incurs negligible inference overhead in contrast to PH-VC.
* **P(True)**: this method elicits the uncertainty estimate of a claim by prompting an LLM to answer whether the claim is true or false, using the likelihood of being true as the confidence score (which we estimated by sampling multiple times from the LLMs).
* **Self-Consistency (SC)**[17; 11]: as discussed in Sec. 3, this method utilizes the consistency score of one claim across different samples from the same LLM.
* **Self-Consistency + PH-VC (SC + VC)**: a straightforward variant of SC that integrates the PH-VC by summing their scores to break the frequent ties in confidence scores in SC.

For our graph-based uncertainty estimates, we evaluated four graph centrality metrics: **betweenness (\(C_{B}\))**, **eigenvalue (\(C_{E}\))**, **PageRank** (\(C_{PR}\)), and **closeness (\(C_{C}\))**. We provide more detailed information about our methods and baselines in Appx. B.

**Evaluation metrics** We assess how well various uncertainty estimation metrics distinguish factual claims from false ones using the Area Under the Receiver Operating Characteristic (AUROC) curve. Since the dataset may be imbalanced, we also compute the Area Under the Precision-Recall Curve for the Negative class (AUPRC-Negative). AUPRC-Negative focuses on the classifier's performance in identifying false claims that are more critical to the factuality compared to true claims. By using AUPRC-Negative, we can better assess the metrics' performance in identifying false claims, which complements the overall performance assessment provided by the AUROC metric. More detailed results are provided in Appx. C.

    & Setup &  &  &  &  &  &  \\  & Metric & ROC & PRC & ROC & PRC & ROC & PRC & ROC & PRC & ROC & PRC & ROC & PRC \\   & IL-VC & 0.537 & 0.437 & 0.537 & 0.437 & 0.540 & 0.394 & 0.540 & 0.394 & 0.536 & 0.486 & 0.536 & 0.486 \\  & PH-VC & 0.748 & 0.641 & 0.748 & 0.641 & 0.701 & 0.531 & 0.701 & 0.531 & 0.675 & 0.593 & 0.675 & 0.593 \\  & P(True) & 0.609 & 0.521 & 0.609 & 0.521 & 0.756 & 0.633 & 0.756 & 0.633 & 0.580 & 0.500 & 0.580 & 0.500 \\  & SC & 0.835 & 0.726 & 0.852 & 0.756 & 0.812 & 0.651 & 0.839 & 0.708 & 0.801 & 0.712 & 0.829 & 0.761 \\  & SC+VC & 0.870 & 0.781 & 0.879 & 0.801 & 0.827 & 0.670 & 0.838 & 0.701 & 0.817 & 0.739 & 0.833 & 0.771 \\   & \(C_{B}\) & 0.765 & 0.706 & 0.793 & 0.731 & 0.758 & 0.637 & 0.780 & 0.683 & 0.743 & 0.695 & 0.759 & 0.733 \\  & \(C_{E}\) & 0.794 & 0.726 & 0.809 & 0.735 & 0.775 & 0.623 & 0.797 & 0.673 & 0.732 & 0.690 & 0.757 & 0.722 \\  & \(C_{PR}\) & 0.852 & 0.776 & 0.853 & 0.777 & 0.791 & 0.644 & 0.801 & 0.675 & 0.757 & 0.683 & 0.764 & 0.700 \\  & \(C_{C}\) & **0.892** & **0.848** & **0.898** & **0.859** & **0.843** & **0.733** & **0.855** & **0.751** & **0.857** & **0.837** & **0.867** & **0.850** \\   & IL-VC & 0.508 & 0.449 & 0.508 & 0.449 & 0.499 & 0.325 & 0.499 & 0.325 & 0.568 & 0.473 & 0.568 & 0.473 \\  & PH-VC & 0.623 & 0.533 & 0.623 & 0.533 & 0.640 & 0.415 & 0.640 & 0.415 & 0.663 & 0.556 & 0.663 & 0.556 \\  & P(True) & 0.614 & 0.656 & 0.614 & 0.656 & 0.642 & 0.585 & 0.642 & 0.585 & 0.572 & 0.583 & 0.572 & 0.583 \\  & SC & 0.758 & 0.676 & 0.789 & 0.721 & 0.744 & 0.516 & 0.771 & 0.572 & 0.735 & 0.616 & 0.756 & 0.652 \\  & **SC+VC** & 0.778 & 0.693 & 0.794 & 0.716 & 0.752 & 0.548 & 0.762 & 0.581 & 0.761 & 0.666 & 0.775 & 0.692 \\   & \(C_{B}\) & 0.650 & 0.645 & 0.683 & 0.679 & 0.718 & 0.514 & 0.738 & 0.566 & 0.702 & 0.607 & 0.720 & 0.638 \\  & \(C_{E}\) & 0.716 & 0.666 & 0.728 & 0.697 & 0.703 & 0.564 & 0.729 & 0.604 & 0.700 & 0.591 & 0.725 & 0.630 \\  & \(C_{PR}\) & 0.734 & 0.687 & 0.740 & 0.713 & 0.703 & 0.473 & 0.723 & 0.511 & 0.718 & 0.617 & 0.734 & 0.645 \\  & \(C_{C}\) & **0.792** & **0.754** & **0.809** & **0.774** & **0.786** & **0.668** & **0.800** & **0.693** & **0.772** & **0.699** & **0.784** & **0.713** \\   

Table 2: **Our claim-level uncertainty estimate based on the closeness centrality metric consistently and significantly outperforms baselines.** We assess the AUROC (ROC) and AUPRC-Negative (PRC) to compare baselines and different centrality metrics (Table 1) with the number of samples \(||\{5,10\}\) on the FactScore and PopQA datasets. Results with statistically significant gains are bolded. All p-values are significantly less than 0.05 through a pairwise significance test detailed in Appx. C.2. Each setup is annotated as “model, \(||\)”. Abbreviations are used for baselines, as defined in the baseline discussion, and for centrality metrics, as defined in Sec. 4.2.

**Experimental details** Our experiments are conducted on the three most capable LLMs to date (as of June 2024): GPT-3.5-turbo, GPT-4 , and Llama-3-70B-Instruct . We used the same LLM to sample responses and construct a semantic entailment graph for uncertainty estimation as described in Sec. 4.1. The graph construction is set up as follows: To construct the set of claims \(\), we used a greedily decoded sample (temperature \(t=0\)) and 4 samples with temperature \(t=1\) as \(_{N}\). To construct the set of responses \(\) in the graph, we used \(||=5\) or \(||=10\) samples, where we included those for obtaining the claims and sampled additional ones with temperature \(t=1\) if needed. The impact of these hyperparameters is analyzed in Sec. 6.3. We collected all the claims in the entities we sampled and labeled their factuality using the method discussed in Sec. 6.1. Then, we only used those claims that are annotated as True or False (but not Subjective) to avoid ambiguity. The results are presented in Table 2.

**Closeness centrality consistently and significantly outperforms baselines** As illustrated in Table 2, our closeness centrality metric consistently outperforms other graph centrality metrics and baselines, including those with higher inference costs, such as SC + VC. In some settings, closeness centrality achieves significant gains up to over 6% at AUROC and 12% at AUPRC-Negative compared to the SC baseline. To better understand the effectiveness of closeness centrality for uncertainty estimation, we provide an ablation study in Sec. 6.3.

**Additional analysis** Our systematic benchmarking in Table 2 also provides some insights into the effectiveness of baseline methods for claim-level uncertainty estimation:

* **SC is a strong baseline**: We find that Self-Consistency (SC) score, which is essentially degree centrality \(C_{D}\), serves as a strong baseline, outperforming all other previous approaches. It sometimes underperforms our PageRank centrality metric \(C_{PR}\), but the comparison is sensitive to the specific setup or dataset.
* **IL-VC usually underperforms PH-VC**: Comparing the first two rows of each setup, we find that combining the process of claim decomposition and uncertainty elicitation of claims actually hurts the uncertainty estimation performance.
* **Integrating VC improves SC:** We observe that integrating VC into SC can improve its uncertainty estimation in most setups, even when a naive addition of their scores (SC + VC) is applied. This is probably because VC offers additional information to distinguish claims with tied scores in SC.

### Uncertainty-Aware Decoding

In this subsection, we empirically demonstrate how our improved claim-level uncertainty estimates contribute to a better tradeoff between precision (factuality) and recall (informativeness) of generated responses within our UAD framework, and analyze the impact of each component.

**Experimental setup** We tested UAD with our best closeness centrality uncertainty estimate \(C_{C}\) on the FActScore dataset used in Sec. 6.1. We experimented with GPT-3.5-Turbo that were used for all steps described in Fig. 1 and Fig. 2. We randomly sampled 180 entities and used \(||=5\) to construct candidate claim set \(\). Additional details can be found in Appx. B.2.

**Evaluated methods** We benchmarked the performance of UAD against several inference-time decoding methods. For a systematic comparison, we also included inference-time decoding methods that do

Figure 3: **UAD with better claim-level uncertainty estimates demonstrates a better trade-off between factuality and informativeness of the generated responses. We compare UAD across different thresholds \(\) and two non-uncertainty decoding baselines. We assume that random noise is applied to break ties for each uncertainty method, resulting in a horizontal line extending from the leftmost dot to the left. The shaded confidence interval is obtained by bootstrapping with a confidence level of 95%.**not rely on uncertainty estimation. For UAD, we evaluated its performance across various uncertainty estimation techniques \(U\) and claim candidate set choices \(\) to study their impact. Specifically, our evaluated methods include (Appx. B) :

* **Greedy Decoding:** the most naive baseline that generates a response with a temperature of \(t=0\) for a given input prompt \(x\).
* **CoVe **: an inference-time decoding method that improves the factuality of generations by self-verification without any uncertainty estimates being used.
* **UAD (SC, Greedy)**: as discussed in Sec. 5, this method corresponds to Conformal Factuality Decoding , which employs the SC uncertainty estimate to filter out high-uncertainty claims in the claim set obtained from the greedily decoded response.
* **UAD (SC, Multi-Sample)**: based on the previous method, it expands the claim set \(\) to include those decomposed from multiple sampled responses \(\), following our procedure in Fig. 1. Comparing this method with the previous one studies the impact of candidate claim set size \(||\).
* **UAD (SC + IL-VC, Multi-Sample)**: similar to the previous method, but it utilizes IL-VC to break ties for SC scores. We applied IL-VC instead of PH-VC here to ensure a fair comparison with similar inference costs across different methods (additional results with PH-VC are included in Appx. D.1).
* **UAD (\(C_{}\), Multi-Sample)**: it applies our best graph-based uncertainty estimate with the closeness centrality \(C_{C}\) as \(U\), from which we can study how better claim-wise uncertainty estimates may improve the performance of UAD.

**Evaluation Metrics** The efficacy of long-form text generation was assessed along two dimensions: factuality and informativeness of the generated content. The factuality score, ranging from 0 to 1, was measured using FactScore without length penalty , with higher being better. The informativeness score quantified the number of claims within the output. In cases where no claims were included in the output (e.g., "I don't know"), the factuality score is set to 1 but the informativeness score is set to 0, indicating the absence of potentially hallucinated content. These metrics were averaged across data, showing both the truthfulness and utility aspects of the generated text.

**Results and analysis** Our results are presented in Fig. 3, with factuality on the y-axis and informativeness on the x-axis. Methods positioned towards the upper right are preferred, as they demonstrate desirable performance for both factuality and informativeness. UAD-based methods trace a trajectory in the plot when varying the uncertainty estimation threshold \(\), while Greedy decoding and CoVe appear as single points. The results reveal several key findings:

* **UAD achieves a better tradeoff between factuality and informativeness:** We observe that UAD-based methods consistently outperform non-UAD methods, with all UAD variants achieving superior factuality-informativeness tradeoffs compared to Greedy Decoding and CoVe. Specifically, when generating the same number of claims as Greedy Decoding, UAD methods achieve up to

Figure 4: **Ablation study**: (a) The false claims have a greater average distance to other claims compared to true ones, indicating the effectiveness of the closeness centrality metric. (b) Performance improves consistently as we increase the number of responses \(|_{N}|\) used to construct the claim node set \(\) in our uncertainty estimation method. While all evaluations are conducted on the same fixed set of claims, varying \(|_{N}|\) alters the graph structure used to estimate these claims’ uncertainty values.

18% higher factuality. This indicates the effectiveness of utilizing uncertainty estimates to steer the response generation.
* **Expanding the claim candidate set \(\) trades-off factuality for informativeness**: Comparing UAD (SC, Multi-Sample) and UAD (SC, Greedy), we find that by expanding \(\) to include those decomposed from multiple samples, we can achieve a better trade-off in setups where more claims are desired, but a lower peak accuracy otherwise. This indicates that the choice of \(\) should be determined based on the desired balance between factuality and informativeness.
* **Better claim-level uncertainty estimates lead to a better tradeoff**: Comparing UAD with SC, SC + IL-VC, and \(C_{C}\) in Fig. 3, we find that applying our best metric, closeness centrality, also leads to a clearly dominating Pareto optimality. Our approach consistently achieves 2-4% gains in factuality and can generate 70% more true claims at the 95% precision level compared to the best baseline. Moreover, because closeness centrality provides a more fine-grained metric than degree centrality (used in SC), it offers a broader range of trade-off options compared to SC (even when combined with IL-VC to break ties), as evidenced by more points in the figure obtained by varying the threshold \(\).

### Ablation Study

In this section, we aim to analyze why closeness centrality is effective for discriminating between true and false claims, and how the performance of this method changes as we increase the number of claim nodes in the semantic bipartite graph. For all experiments in the ablation study, we used the FActScore dataset with the GPT-3.5-turbo model.

Why is the closeness centrality so effective?Closeness centrality is intrinsically related to the distances between nodes in a graph. To understand its effectiveness, we analyze how the distances between a pair of claims correlate with the factuality of these claims. Specifically, in Fig. 3(a), we visualize the distances between true-true claim pairs, true-false pairs, and false-false pairs in the semantic graph. We observe a clear shift in the distance distribution from true claims to false claims across all settings. False claims generally exhibit larger distances to other claims in the semantic entailment graph. This observation can be intuitively interpreted as false claims being less centered, i.e., less entailed by generations and having lower co-occurrences with the majority of claims. This insight provides a clear explanation for the strong performance of closeness centrality in claim-level uncertainty estimation.

How does the performance change with the number of responses?Fig. 3(b) explores how the performance changes as we increase the number of responses \(_{N}\) used to construct the claim set. We find that increasing the number of claim nodes consistently leads to improved performance (despite the increased inference cost). This also indicates that the closeness centrality effectively leverages additional graph information as more nodes are present in the semantic graph.

Is our end-to-end decoding pipeline computational intensive?While our pipeline increases the lower bound of computation by introducing the graph construction process, it attains Pareto optimality for the compute-quality tradeoff compared to existing methods across multiple quality metrics. Details are present in Appx. D.2.

## 7 Conclusion

In this work, we propose Graph Uncertainty, a family of graph-based methods for claim-wise uncertainty estimation in LLM generations. We also present an uncertainty-aware decoding framework that integrates these estimates to improve the trade-off between factuality and informativeness. Empirical results demonstrate the effectiveness of the proposed approach.

Despite these improvements, we note that the graph construction process increases inference-time compute and latency. Moreover, our claim decomposition assumes that claims can be decontextualized and separated, which may not always be the case in real-world applications. Future work may aim to optimize the graph construction process to reduce computational overhead and develop techniques to handle scenarios with dependent claims more effectively. These improvements will further improve the applicability and robustness of our uncertainty estimation framework in complex settings.