# Optimal Regret Is Achievable with Bounded Approximate Inference Error: An Enhanced Bayesian Upper Confidence Bound Framework

Optimal Regret Is Achievable with Bounded Approximate Inference Error: An Enhanced Bayesian Upper Confidence Bound Framework

Ziyi Huang, \({}^{1}\)Henry Lam, \({}^{2}\)Amirhossein Meisami, \({}^{1}\)Haofeng Zhang \({}^{*}\)

\({}^{1}\) Columbia University, New York, NY, USA

\({}^{2}\) Adobe Inc., San Jose, CA, USA

zh2354,khl2114,hz2553@columbia.edu, meisami@adobe.com

Authors are listed alphabetically.

###### Abstract

Bayesian bandit algorithms with approximate Bayesian inference have been widely used in real-world applications. However, there is a large discrepancy between the superior practical performance of these approaches and their theoretical justification. Previous research only indicates a negative theoretical result: Thompson sampling could have a worst-case linear regret \((T)\) with a constant threshold on the inference error measured by one \(\)-divergence. To bridge this gap, we propose an Enhanced Bayesian Upper Confidence Bound (EBUCB) framework that can efficiently accommodate bandit problems in the presence of approximate inference. Our theoretical analysis demonstrates that for Bernoulli multi-armed bandits, EBUCB can achieve the optimal regret order \(O( T)\) if the inference error measured by two different \(\)-divergences is less than a constant, regardless of how large this constant is. To our best knowledge, our study provides the first theoretical regret bound that is better than \(o(T)\) in the setting of constant approximate inference error. Furthermore, in concordance with the negative results in previous studies, we show that only one bounded \(\)-divergence is insufficient to guarantee a sub-linear regret.

## 1 Introduction

The stochastic bandit problem, dated back to , is an important sequential decision-making problem that aims to find optimal adaptive strategies to maximize cumulative reward. At each time step, the learning agent chooses an action among all possible actions and observes its corresponding reward (but not others), and thus requires a balance between exploration and exploitation. Previous theoretical studies mainly focus on _exact_ Bayesian bandit problems, requiring access to exact posterior distributions. However, their work cannot be easily applied to complex models such as deep neural networks, where maintaining exact posterior distributions tends to be intractable . In contrast, _approximate_ Bayesian bandit methods are widely employed in real-world applications with state-of-the-art performances [36; 41; 33; 45; 17; 50]. In comparison with exact algorithms, approximate Bayesian bandit algorithms are more challenging to analyze, as the inaccessibility of exact posterior sampling adds another level of discrepancy, and the resulting theory and solutions hence also differ substantially.

Few theoretical studies have been developed around Bayesian bandit approaches with approximate inference, despite their superior practical performance.  gave a theoretical analysis of an approximate sampling method called Ensemble Sampling, which possessed constant Kullback-Leibler divergence (KL divergence) error from the exact posterior and thus indicated a linear regret. showed that with a _constant_ threshold on the inference error in terms of \(\)-divergence (a generalization of KL divergence), regardless of how small the threshold is, Thompson sampling with general approximate inference could have a linear regret in the worst case. Their work also showed that Thompson sampling combined with a small amount of forced exploration could achieve a \(o(T)\) regret upper bound, but no better result than \(o(T)\) was shown. Moreover, this improvement was mostly credited to the forced exploration rather than the intrinsic property of Thompson sampling. It appears that  illustrated a paradox that approximate Bayesian bandit methods worked well empirically but failed theoretically. Thus, further study regarding the fundamental understanding of approximate Bayesian bandit methods is necessary.

Motivated by the negative results in ,  leveraged an efficient Markov chain Monte Carlo (MCMC) Langevin algorithm in multi-armed Thompson sampling so that the inference error would _vanish_ along with increased sample numbers.  extended  to the contextual bandit problems and integrated contextual Thompson sampling with the Langevin algorithm that allowed the approximate posterior distribution to be sufficiently close to the exact posterior distribution. Hence, both works [48; 30] had a similar feature in terms of vanishing inference error. However, in other inference approaches, such as variational inference , the approximate posteriors might incur a systematic computational bias. To accommodate the latter scenario, we consider a general Bayesian inference approach that allows bounded inference error:

_Is it possible to achieve the optimal regret order \(O( T)\) with a **constant** (non-vanishing) threshold on the inference error?_

This question is not well investigated in previous literature, even for Bernoulli bandits.  showed that the answer is _No_ for Thompson sampling when the inference error measured by only _one_\(\)-divergence is bounded. In this study, we will provide a novel theoretical framework and point out that the answer could be _Yes_ when the inference error measured by _two_ different \(\)-divergences is bounded where one \(\) is greater than 1, and the other \(\) is less than 0. This assumption guarantees that the approximate posterior is close to the exact posterior from two different "directions". Our finding implies that the problem of sub-optimal regret in the presence of approximate inference may not arise from the constant but from the design of the inference error. Our study takes the first step in deriving positive answers in the presence of constant approximate inference error, which provides some theoretical support for the superior performance of approximate Bayesian bandit methods.

In this study, we extend the work of the Bayesian Upper Confidence Bound (BUCB) [21; 20; 42; 17] to the setting of approximate inference and propose an enhanced Bayesian bandit algorithm that can efficiently accommodate approximate inference, termed as Enhanced Bayesian Upper Confidence Bound (EBUCB). In particular, we redesign the quantile choice in the algorithm to address the challenge of approximate inference: The original choice of \(t^{-1}\) provides the best regret bound without approximate inference, but in the presence of approximate inference, it leads to an undesirable quantile shift which degrades the performance. By adjusting the quantile choice, we theoretically demonstrate that EBUCB can achieve the optimal regret order \(O( T)\) if the inference error measured by two different \(\)-divergences (\(_{1}>1\) and \(_{2}<0\)) is bounded. We also provide insights in the other direction: Instead of two different \(\)-divergences, controlling one \(\)-divergence alone is not sufficient to guarantee a sub-linear regret for both Thompson sampling, BUCB, and EBUCB. This further suggests that naive approximate inference methods that only minimize one \(\)-divergence alone could perform poorly, and thus it is critical to design approaches with two different \(\)-divergences reduced.

Our main contributions are summarized as follows:

1) We propose a general Bayesian bandit framework, named EBUCB, to address the challenge of approximate inference. Our theoretical study shows that for Bernoulli bandits, EBUCB can achieve a \(O( T)\) regret upper bound when the inference error measured by two different \(\)-divergences is bounded. To the best of our knowledge, with constant approximate inference error, there is no existing result showing a regret upper bound that is better than \(o(T)\), even for Bernoulli bandits.

2) We develop a novel sensitivity analysis of quantile shift with respect to inference error. This provides a fundamental tool to analyze Bayesian quantiles in the presence of approximate inference, which holds promise for broader applications, e.g., when the inference error is time-dependent.

3) We demonstrate that one bounded \(\)-divergence alone is insufficient to guarantee a sub-linear regret. Worst-case examples are constructed and illustrated where Thompson sampling/BUCB/EBUCB has \((T)\) regret if only one \(\)-divergence is bounded. Hence, special consideration on reducing two different \(\)-divergences is necessary for real-world applications.

4) Our experimental evaluations corroborate our theory well, showing that our EBUCB is consistently superior to BUCB and Thompson sampling on multiple approximate inference settings.

**Related Work.** Bandit problems and their theoretical optimality have been extensively studied over decades [27; 10]. The seminal paper  (and subsequently ) established the first problem-dependent frequentist regret lower bound, showing that without any prior knowledge on the distributions, a regret of order \(( T)\) is unavoidable. Two popular lines of Bayesian bandit algorithms, Thompson sampling [1; 22; 16] and BUCB [21; 20], had been shown to match the lower bound, which indicated the theoretical optimality of those algorithms. Beyond Gaussian processes  and linear models [3; 39], exact computation of the posterior distribution is generally intractable, and thus, the approximate Bayesian inference techniques are necessary.

Some recent work focused on designing specialized methods to construct Bayesian indices since previous studies had demonstrated that Thompson sampling with constant inference error could exhibit linear regret in the worst-case scenario [28; 34].  constructed Langevin algorithms to generate approximate samples with decreasing inference error and showed an optimal problem-dependent frequentist regret.  proposed variational Bayesian optimistic sampling, suggesting solving a convex optimization problem over the simplex at every time step. Unlike these researches, our study presents general results that only depend on the error threshold of approximate inference, rather than some specific approximate inference approaches.

Beyond Bayesian, another mainstream of bandit algorithms to address the exploration-exploitation tradeoff is upper confidence bound (UCB)-type frequentist algorithms [5; 6; 7; 13; 15; 40; 51; 24; 29; 43].  revealed that Thompson sampling empirically outperformed UCB algorithms in practice, partly because UCB was typically conservative, as its configuration was data-independent which led to over-exploration . BUCB  could be viewed as a middle ground between Thompson sampling and UCB. On the other hand, empirical studies  showed that Thompson sampling and BUCB performed similarly well in general.

## 2 Methodology

The stochastic multi-armed bandit problem consists of a set of \(K\) actions (arms), each with a stochastic scalar reward following a probability distribution \(_{i}\) (\(i=1,...,K\)). At each time step \(t=1,...,T\), where \(T\) is the time horizon, the agent chooses an action \(A_{t}[K]\) and in return observes an independent reward \(X_{t}\) drawn from the associated probability distribution \(_{A_{t}}\). The goal is to devise a strategy \(=(A_{t})_{t[T]}\), to maximize the accumulated rewards through the observations from historical interactions.

In general, a wise strategy should be sequential, in the sense that the upcoming actions are determined and adjusted by the past observations: letting \(_{t}=(A_{1},X_{1},...,A_{t},X_{t})\) be the \(\)-field generated by the observations up to time \(t\), \(A_{t}\) is \((_{t-1},U_{t})\)-measurable, where \(U_{t}\) is a uniform random variable independent from \(_{t-1}\) (as algorithms may be randomized). More precisely, let \(_{j}\) (\(j[K]\)) denote the mean reward of the action \(j\) (i.e., the mean of the distribution \(_{i}\)), and without loss of generality, we assume that \(_{1}=_{j[K]}_{j}\). Then maximizing the rewards is equivalent to minimizing the (frequentist) regret, which is defined as the expected difference between the reward accumulated by an "ideal" strategy (a strategy that always playing the best action), and the reward accumulated by a strategy \(\):

\[R(T,):=[T_{1}-_{t=1}^{T}X_{t}]=[_{t=1}^{T}(_{1}-_{A_{t}})].\] (1)

The expectation is taken with respect to both the randomness in the sequence of successive rewards from each action \(j\), denoted by \((Y_{j,s})_{s}\), and the possible randomization of the algorithm, \((U_{t})_{t[T]}\). Let \(N_{j}(t)=_{s=1}^{t}(A_{s}=j)\) denote the number of draws from action \(j\) up to time \(t\), so that \(X_{t}=Y_{A_{t},N_{A_{t}}(t)}\). Moreover, let \(_{j,s}=_{k=1}^{s}Y_{j,k}\) be the empirical mean of the first \(s\) rewards from action \(j\) and let \(_{j}(t)\) be the empirical mean of action \(j\) after \(t\) rounds of the bandit algorithm. Therefore \(_{j}(t)=0\) if \(N_{j}(t)=0\), \(_{j}(t)=_{j,N_{j}(t)}\) otherwise.

Note that the true mean rewards \(=(_{1},...,_{K})\) are fixed and unknown to the agent. In order to perform Thompson sampling, or more generally, Bayesian approaches, we artificially define a prior distribution \(_{0}\) on \(\). Let \(_{t}\) be the exact posterior distribution of \(|_{t-1}\) with density function with marginal distributions \(_{t,1},...,_{t,K}\) for actions \(1,...,K\). Specifically, if at time step \(t\), the agent chooses action \(A_{t}=j\) and consequently observes \(X_{t}=Y_{A_{t},N_{A_{t}}(t)}\), the Bayesian update for action \(j\) is

\[_{t,j}()_{}(X_{t})_{t-1,j}(),\] (2)

whereas for \(i j\), \(_{t,i}=_{t-1,i}\). At each time step \(t\), we assume that the exact posterior computation in (2) cannot be obtained explicitly and an approximate inference method is able to give us an approximate distribution \(Q_{t}\) (instead of \(_{t}\)). We use \(q_{t}\) to denote the density function of \(Q_{t}\).

First, we consider a standard case where the exact posterior is accessible. In Thompson sampling [1; 44], a sample \(\) is drawn from the posterior distribution \(_{t-1}\) and then an action \(A_{t}\) is selected using the following strategy: \(A_{t}=i\) if \(_{i}=_{j}_{j}\). In BUCB , we compute the quantile of the posterior distribution \(qu_{j}(t)=Qu(1-},_{t-1,j})\) for each action \(j\), where \(Qu(,)\) is the quantile function associated to the distribution \(\), such that \(P_{}(X Qu(,))=\). Then we select action \(A_{t}\) as follows: \(A_{t}=i\) if \(qu_{i}(t)=_{j}qu_{j}(t)\).

Next, we move to a more concrete example, Bernoulli multi-armed bandit problems with a standard setting used in seminal papers [1; 2; 21; 22]. In these problems, each (stochastic) reward follows a Bernoulli distribution \(_{i}(_{i})\) and these distributions are independent of each other. The prior \(_{0,j}\) is typically chosen to be the independent and identically distributed (i.i.d.) \((1,1)\), or the uniform distribution for every action \(j\). Then the posterior distribution for action \(j\) is a Beta distribution \(_{t,j}=(1+S_{j}(t),1+N_{j}(t)-S_{j}(t))\), where \(S_{j}(t)=_{s=1}^{t}\{A_{s}=j\}X_{t}\) is the empirical cumulative reward from action \(j\) up to time \(t\). Then, Thompson sampling/BUCB chooses the samples/quantiles of the posterior \(_{t,j}=(1+S_{j}(t),1+N_{j}(t)-S_{j}(t))\) respectively at each time step.

In the presence of approximate inference, Thompson sampling draws the sample \(\) from \(Q_{t-1}\), as the exact \(_{t-1}\) is not accessible. Correspondingly, we modify the specific sequence of quantiles chosen by the BUCB algorithm with a general sequence of \(\{_{t}\}\)-quantiles and term it as Enhanced Bayesian Upper Confidence Bound (EBUCB) algorithm. The detailed pseudo algorithm of EBUCB is described in Algorithm 1. Note that the choice of \(_{t}\) should address the presence of inference error and should be trailed to the specific definition of inference error; See Remark 3.10 in Section 3.

``` Input:\(T\) (time horizon), \(_{0}=Q_{0}\) (initial prior on \(\)), \(c\) (parameters of the quantile), and a real-value increasing sequence \(\{_{t}\}\) such that \(_{t} 1\) as \(t\) for\(t=1\)to\(T\)do for each action \(j=1,...,K\)do  Compute \(qu_{j}(t)=Qu(_{t},Q_{t-1,j})\). endfor  Draw action \(A_{t}=\;qu_{j}(t)\)  Get reward \(X_{t}=Y_{A_{t},N_{A_{t}}(t)}\)  Obtain the approximate distribution \(Q_{t}\) endfor ```

**Algorithm 1** Enhanced Bayesian Upper Confidence Bound (EBUCB) with Approximate Inference

## 3 Theoretical Analysis

In this section, we present a theoretical analysis of EBUCB. In Section 3.1, we provide the necessary background of \(\)-divergence on approximate inference error measurement. Then in Section 3.2, we develop a novel sensitivity analysis of quantile shift with respect to inference error. This provides a fundamental tool to analyze Bayesian quantiles in the presence of approximate inference. The general results therein will be used for our derivation for the regret upper bound of EBUCB in Section 3.3, and are also potentially useful for broad applications, e.g., when the inference error is time-dependent. Lastly, in Section 3.4, we provide examples where Thompson sampling/BUCB/EBUCB has a linear regret with arbitrarily small inference error measured by one \(\)-divergence alone. All proofs are given in the Appendix.

### The Alpha Divergence for Inference Error Measurement

The \(\)-divergence, generalizing the KL divergence, is a common way to measure errors in inference methods.

**Definition 3.1**.: The \(\)-divergence between two distributions \(P_{1}\) and \(P_{2}\) with density functions \(p_{1}(x)\) and \(p_{2}(x)\) is defined as: \(D_{}(P_{1},P_{2})=( p_{1}(x)^{} p_{2}(x)^{1-}dx-1)\), where \(\) and the case of \(=0\) and \(1\) is defined as the limit.

Note that different studies use the \(\) parameter in different ways. Herein, our definition of \(\)-divergence does not follow Renyi's definition of \(\)-divergence ; Instead, we follow a generalized version of Tsallis's \(\)-divergence, which is adopted by [52; 31; 34]. Compared with Renyi's \(\)-divergence, Tsallis's \(\)-divergence does not involve a log function, and it has the following property:

**Proposition 3.2** (Positivity and symmetry).: _For any \(\), \(D_{}(P_{1},P_{2}) 0\) and \(D_{}(P_{1},P_{2})=D_{1-}(P_{2},P_{1})\)._

The \(\)-divergence contains many distances such as \(KL(P_{2},P_{1})( 0)\), \(KL(P_{1},P_{2})( 1)\), Hellinger distance \((=0.5)\), and \(^{2}\) divergence \((=2)\). \(\)-divergence is widely used in variational inference [8; 23; 26], which is one of the most popular approaches in Bayesian approximate inference. Moreover, it was also adopted in previous studies on Thompson sampling with approximate inference [34; 28]. In particular, the KL divergence (\((=1)\)-divergence) is: \(KL(P_{1},P_{2})= p_{1}(x)((x)}{p_{2}(x)})dx\). In approximate Bayesian inference, the exact posterior distribution \(_{t}\) and the approximate distribution \(Q_{t}\) may differ from each other. To provide a statistical analysis of approximate sampling methods, we use the \(\)-divergence as the measurement of inference error (statistical distance) between \(_{t}\) and \(Q_{t}\). Our starting point is the following:

**Assumption 3.3**.: Suppose that there exists a positive value \((0,+)\) and two different parameters \(_{1}>1\) and \(_{2}<0\) such that

\[& D_{_{1}}(Q_{t,j},_{t,j}),\;  t[T],j[K],\\ & D_{_{2}}(Q_{t,j},_{t,j}),\; t[T],j[K].\] (3)

This assumption is adapted from  but we enhance theirs with two bounded \(\)-divergences, as  showed one bounded \(\)-divergence was not sufficient to guarantee the sublinear regret. However, in the following, we show that the optimal regret order \(O( T)\) is indeed achievable under Assumption 3.3 with two bounded \(\)-divergences. Intuitively, \(P_{2}\) is flatten to cover \(P_{1}\)'s entire support when minimizing \(D_{}(P_{1},P_{2})\) with a large \(\) (greater than 1), while when \(\) is small (less than 0), \(P_{2}\) fits the \(P_{1}\)'s dominant mode; See  for the implication of \(\)-divergence. Therefore, Assumption 3.3 guarantees that the approximate posterior is close to the exact posterior from two different "directions". It is worth mentioning that when one \(\)-divergence is small, it does not necessarily imply that any other \(\)-divergences are large or infinite. In fact, as long as the two distributions have densities with the same support, then any \(\)-divergence between them is finite. Note that Assumption 3.1 does not require the threshold \(\) to be small; instead, \(\) can be any finite positive number. We pinpoint that this assumption, as well as our subsequent results, are very general in the sense that it does not depend on any specific methods of approximate inference. To enhance credibility on Assumption 3.3, we make several additional remarks in Section A.

### Quantile Shift with Inference Error

In this section, we develop a novel sensitivity analysis of quantile shift with respect to inference error, which implies that under Assumption 3.3, the \(\)-quantiles of \(_{t,j}\) and \(Q_{t,j}\) only differs from a bound depending on \(\). We provide a general result first, which is rigorously stated as follows:

**Theorem 3.4**.: _Consider any two distributions \(P_{1}\) and \(P_{2}\) with densities \(p_{1}(x)\) and \(p_{2}(x)\). Let \(R_{i}\) denote the quantile function of the distribution \(P_{i}\), i.e., \(R_{i}(p):=Qu(p,P_{i})\) (\(i=1,2\)). Let \(0<<1\). Let \(_{,}\) satisfy that \(R_{1}()=R_{2}(+_{,})\) where \(-_{,} 1-\). a) If \(D_{}(P_{1},P_{2})\) where \(>1\), then_

\[_{,} 1--((-1)+1)^{}(1-)^{}.\]

_Note that when \(>1\), \(((-1)+1)^{}<1\) and \((1-)^{}<1-\).__b) If \(D_{}(P_{1},P_{2})\) where \(<0\), then_

\[_{,} 1--((-1)+1)^{}(1-)^{}.\]

_Note that when \(<0\), \(((-1)+1)^{}>1\) and \((1-)^{}>1-\)._

_c) Suppose that \((0,1)\) and \(\). Then for any \(_{,}[-,1-]\), there exist two distributions \(P_{1}\) and \(P_{2}\) such that \(D_{}(P_{1},P_{2})\). This implies that the condition \(D_{}(P_{1},P_{2})\) cannot control the quantile shift between \(P_{1}\) and \(P_{2}\) in general when \((0,1)\)._

Theorem 3.4 states that \(\)-quantile of the distribution \(P_{1}\) is the \((+_{,})\)-quantile of the distribution \(P_{2}\) where the quantile shift \(_{,}\) has the following properties. a) The upper bound of \(_{,}\) is close to \(0\) if \(D_{}(P_{1},P_{2})\) with \(>1\) is bounded; b) The lower bound of \(_{,}\) is close to \(0\) if \(D_{}(P_{1},P_{2})\) with \(<0\) is bounded; c) A slightly large bound on \(D_{}(P_{1},P_{2})\) with \(0<<1\) cannot control the shift \(_{,}\) in general, which gives the intuition that \((0,1)\) is not implemented in Assumption 3.3.

This theorem is distribution-free, in the sense that the bound of \(_{,}\) does not depend on any specific distributions (noting that distribution changes as \(t\) evolves in bandit problems). In particular, a)+b) in Theorem 3.4 shows that \(C_{,_{1}}(1-)^{}{_{1}-1} }_{,}-(1-)-C_{,_{2} }(1-)^{}{_{2}-1}},\) with \(_{1}>1\) and \(_{2}<0\) where \(C_{,}=((-1)+1)^{}\) is independent of \(\) or distributions, and thus independent of the time step \(t\) in our EBUCB algorithm. This observation is important in the robustness of using quantiles in the EBUCB. The proof of Theorem 3.4 relies on the following lemma, which provides a quantile-based representation of \(\)-divergence.

**Lemma 3.5**.: _Under the same conditions in Theorem 3.4, we have that for any \(\)-divergence, \(D_{}(P_{1},P_{2})=^{I}(R_{2}^{-1}(R _{1}(u)))^{1-}du-1}{(-1)}.\)_

### Finite-Time Regret Bound for EBUCB

In this section, we derive a finite-time upper bound of problem-dependent frequentist regret for our EBUCB algorithm in Bernoulli multi-armed bandit problems. Without loss of generality, we assume action \(1\) is optimal in the subsequent theorems.

To begin with, we first express the frequentist regret as \(R(T,):=[_{t=1}^{T}(_{1}-_{A_{t}})]= _{j=1}^{K}(_{1}-_{j})[N_{j}(t)]\) by (1). Therefore, it is sufficient to study each term \([N_{j}(t)]\) in order to bound the regret \(R(T,)\). For \((p,q)^{2}\), we denote the Bernoulli KL divergence between two points by \(d(p,q)=p()+(1-p)()\), with \(0 0=0(0/0)=0\) and \(x(x/0)=+\) for \(x>0\) by convention. We also denote that \(d^{+}(p,q)=d(p,q)\{p<q\}\) for convenience.

Note that \(_{t,j}(x)\) is the density of \((1+S_{j}(t),1+N_{j}(t)-S_{j}(t))\) so its (closed) support is \(\). We put a basic assumption on the approximate distribution \(Q_{t,j}\).

**Assumption 3.6**.: \(Q_{t,j}\) has the density \(q_{t,j}(x)\) whose support is \(\) for any \(t[T],j[K]\).

The following is our main theorem, which establishes a finite-time regret bound for our EBUCB algorithm.

**Theorem 3.7**.: _Suppose Assumptions 3.3 and 3.6 hold. Let \(M_{,1}=(_{1}(_{1}-1)+1)^{}}<1\), \(_{1}=}{_{1}-1}>0\), \(M_{,2}=(_{2}(_{2}-1)+1)^{}}>1\), and \(_{2}=}{_{2}-1}>0\). For any \(>0\), choosing the parameter \(c\) such that \(c_{2} 5\) in the EBUCB algorithm and setting \(_{t}=1-( T)^{c}}(>0)\), the number of draws of any sub-optimal action \(j 2\) is upper-bounded by_

\[[N_{j}(T)]_{2}+c_{2 })M_{,2}eT^{1-_{2}}}{1-_{2 }}+o(T^{1-_{2}})\]

_if \(0<_{2}<1\), and_

\[[N_{j}(T)]_{1}}{d(_{j},_{1} )}(T)+o( T)\]

_if \(_{2} 1\)._Theorem 3.7 provides an exact _finite-time_ regret bound and the \(o()\) term in Theorem 3.7 has an exact finite-time closed-form expression that holds for any time horizon \(T\); See Step 4 in the proof of Theorem 3.7 in Appendix C. We only show the most dominant term of the regret bound and shrink the rest to the \(o()\) term to improve the readability of the main paper. Note that this bound has explicit dependence on \(\), which is \(M_{,1}^{-1}\) and \(M_{,2}\) in Step 4 in the proof of Theorem 3.7. Obviously, the error terms \(M_{,1}^{-1}\) and \(M_{,2}\) in the bound increase as \(\) increases. However, this dependence on \(\) does not impact the dominating term too much. The exact posterior will be more "concentrated" on the true mean with small variability as the time t increases, and the impact from the error will vanish; See Remark 3.9.

It is easy to see that to minimize the regret upper bound, we may choose \(=_{2}}\) in Theorem 3.7.

**Corollary 3.8**.: _Under the same conditions in Theorem 3.7, for any \(>0\), choosing the parameter \(c\) such that \(c_{2} 5\) in the EBUCB algorithm and setting \(_{t}=1-_{2}}( T)^{}}\), the number of draws of any sub-optimal action \(j 2\) is upper-bounded by \([N_{j}(T)]}{_{2}}}{d( _{j},_{1})}(T)+o( T).\)_

This result states that with the \(\) error threshold, the regret of the EBUCB algorithm is bounded above by \(O( T)\) regardless of how large \(\) is, which reaches the same order \(( T)\) of the problem-dependent frequentist regret lower bound . In comparison with the exact lower bound, there is a slight difference in the multiplier before the order \(( T)\): Our upper bound in Corollary 3.8 has the additional multiplier \(_{1}}{_{2}}>1\), which arises from the approximate inference when estimating the posterior distributions (Assumption 3.3). If in addition Assumption 3.3 holds for any \(_{1}>1\) and any \(_{2}<0\), then we can let \(_{1}}{_{2}} 1\) by taking \(_{1}+\) and \(_{2}-\) to match the exact lower bound.

In the absence of approximate inference,  showed that \([N_{j}(T)],_{1})}(T)+o( T)\) matching the exact lower bound. Prior to our work, it was unclear in the literature whether the optimal regret order \(O( T)\) could be achieved in the presence of bounded approximate inference error. Our result provides a positive answer to this question, despite the fact that the inference error may cause the multiplier \(_{1}}{_{2}}\) before the order \(( T)\). To the best of our knowledge, this is the first algorithm providing the regret upper bound that is better than \(o(T)\) in  with bounded approximate inference error.

As discussed in , the horizon-dependent term \(( T)^{c}\) in Corollary 3.8 is only an artifact of the theoretical analysis to obtain a finite-time regret upper bound. In practice, the model with choice \(c=0\) (i.e., without the horizon-dependent term) already achieves superior performance. This is confirmed by our experiments in Section 4. A similar observation in BUCB was indicated in .

_Remark 3.9_.: It might appear a little surprising that the result in Corollary 3.8 indicates a regret upper bound with the dominating term \(O( T)\) that does not depend on \(\), as one may expect that a large \(\) allows the "full swap" between the posterior of the optimal action and the posterior of a suboptimal action, making any Bayesian-based approaches unable to distinguish them. However, benefiting from historical observations, the exact posterior will be more "concentrated" on the true mean with small variability, which will keep enlarging the \(\)-divergence between two actions. This indicates that, for a fixed \(\), the \(\)-divergence between the exact posteriors of two actions can be sufficiently large as \(t\) increases and ultimately exceeds \(\). Hence, the "full swap" will not happen when \(t\) is large.

_Remark 3.10_.: The \(_{2}}}\) in EBUCB, instead of the original \(\) in BUCB, is a delicate choice to address the tradeoff between making the regret optimal without approximate inference and the presence of inference error. On a technical level, a power \(\) close to \(1\) in \(t^{}\) improves the regret bound without the presence of approximate inference but simultaneously leads to high-level quantile shift caused by approximate inference. Choosing \(=_{2}}\) is a subtle balance of these two.

The technical derivation of Theorem 3.7 depends on analyzing the quantiles of the approximate distributions used in the EBUCB algorithm. In particular, one of the major techniques in our analysis is Lemma C.1 in Appendix C. It provides explicit upper and lower bounds on the tails of approximate distributions to control the quantiles designed by the EBUCB algorithm. It is obtained by combining the quantile shift between the approximate and exact posterior distributions that developed in Theorem 3.4 (Section 3.2) with the tight bounds on the quantiles of the exact posterior distributions (the proof of Lemma 1 in ). This result is then used to bound the expectation of a decomposition of \(N_{j}(T)\) in Lemma C.2 that links \(N_{j}(T)\) to the over-estimation of the optimal arm.

### Negative Results

We show that one bounded \(\)-divergence alone cannot guarantee a sub-linear regret. We provide two worst-case examples, one where Thompson sampling has a linear regret, and the other where BUCB/EBUCB has a linear regret, even when the inference error measured by one \(\)-divergence is small. A similar study on Thompson sampling was conducted in  with a special focus on the inference error on the joint distribution of all actions. In our study, nevertheless, we focus on a setting where the inference error on the distribution of each action is assumed; See Remark A.3. Therefore, the examples in  cannot be directly applied in our setting. Moreover, our second example shows that BUCB/EBUCB could have a linear regret if only one \(\)-divergence is considered, which is new.

**Assumption 3.11**.: Suppose that there exists a positive value \((0,+)\) such that

\[D_{}(Q_{t,j},_{t,j}),\  t[T],j[K].\] (4)

We establish the following theorem for Thompson sampling:

**Theorem 3.12**.: _Consider a Bernoulli multi-armed bandit problem where the number of actions is \(K=2\) and \(_{1}>_{2}\). The prior \(_{0,j}\) is chosen to be the i.i.d. \((1,1)\), or the uniform distribution for every action \(j=1,2\). For any given \(<1\) and any error threshold \(>0\), there exists a sequence of distributions \(Q_{t-1}\) such that for all \(t 1\): 1) The probability of sampling from \(Q_{t-1}\) choosing action \(2\) is greater than a positive constant independent of \(t\). 2) \(Q_{t-1}\) satisfies Assumptions 3.6 and 3.11. Therefore Thompson sampling from the approximate distribution \(Q_{t-1}\) will cause a finite-time linear frequentist regret: \(R(T,)=(T)\)._

This theorem shows that making one \(\)-divergence a small constant alone, even for each action \(j\), is not sufficient to guarantee a sub-linear regret of Thompson sampling. Note that Theorem 3.12 is an enhancement of the results in  in the sense that the \(Q_{t}\) constructed by our theorem satisfies more restrictive assumptions. We can derive a similar observation for the BUCB/EBUCB algorithm as follows:

**Theorem 3.13**.: _Consider a Bernoulli multi-armed bandit problem where the number of actions is \(K=2\) and \(_{1}>_{2}\). The prior \(_{0,j}\) is chosen to be the i.i.d. \((1,1)\), or the uniform distribution for every action \(j=1,2\). Consider the general EBUCB algorithm described in Algorithm 1. For any given \(<1\) and any error threshold \(>0\), there exists a constant \(T_{0}\) (only depending on \(\), \(\), and the sequence \(\{_{t}\}\)) and a sequence of distributions \(Q_{t-1}\) such that for all \(t 1\): 1) The EBUCB algorithm always chooses action \(2\) when \(t T_{0}\). 2) \(Q_{t-1}\) satisfies Assumptions 3.6 and 3.11. Therefore the EBUCB algorithm from the approximate distribution \(Q_{t-1}\) will cause a finite-time linear frequentist regret: \(R(T,)=(T)\)._

This theorem shows that making one \(\)-divergence a small constant alone, even for each action \(j\), is insufficient to guarantee a sub-linear regret of BUCB/EBUCB. We emphasize that the examples in Theorems 3.12 and 3.13 are in the _worst-case_ sense, indicating that there exist worst-case examples where Thompson sampling/EBUCB exhibits a linear regret if only one \(\)-divergence is bounded. However, this does not imply that EBUCB and Thompson sampling would fail on average in the presence of approximate inference. In fact, Theorem 3.7 shows that a sub-linear regret can be achieved if the inference error measured by two different \(\)-divergences is bounded.

## 4 Experiments

In this section, we conduct numerical experiments to show the correctness of our theory.2 In Section 4.1, we compare the performance of EBUCB with the following baselines: BUCB (using its originally proposed quantile and \(Q_{t}\) as \(_{t}\) since the exact posterior distribution \(_{t}\) is unavailable) and Thompson sampling (using \(Q_{t}\) as \(_{t}\)). In Section 4.2, we construct worst-case examples showing that both EBUCB and Thompson sampling can degenerate to linear regret if only one \(\)-divergence is bounded. We consider the Bernoulli multi-armed bandit problem which has two actions with mean rewards\([0.7,0.3]\), and use \((1,1)\) as the prior distribution of mean reward for each action. At each time step \(t\), the exact posterior distribution for each action is \((1+S_{j}(t),1+N_{j}(t)-S_{j}(t))\), where \(S_{j}(t)\) is the empirical cumulative reward from action \(j\) up to time \(t\) and \(N_{j}(t)\) is the number of draws from action \(j\) up to time \(t\).

### Generally Misspecified Posteriors

Suppose the posterior distributions are misspecified to the following distributions:

\[(P1): (1-w)*(1+S_{j}(t),1+N_{j}(t)-S_{j}(t))+w*( (t)}{2},(t)-S_{j}(t)}{2})\] \[(P2): (1-w)*(1+S_{j}(t),1+N_{j}(t)-S_{j}(t))+w*( 2(1+S_{j}(t)),2(1+N_{j}(t)-S_{j}(t)))\]

where \(w=0.9,0.8,0.7\). Figure 1 presents the results of EBUCB and the baselines. Overall, EBUCB achieves consistently superior performance than the baselines, and it outperforms BUCB with considerable improvements. These results confirm the effectiveness of EBUCB across multiple settings. Moreover, EBUCB performs well without the horizon-dependent term (i.e., \(c=0\) in Corollary 3.8). This brings EBUCB practical advantages in real-world applications, as it does not require advanced knowledge of the horizon (i.e., _anytime_). A similar observation of BUCB was also noticed in .

Figure 1: Comparison of EBUCB and baselines with generally misspecified posteriors under different problem settings. Results are averaged over \(10\) runs with shaded standard errors.

Figure 2: Results of Thompson sampling, BUCB, and EBUCB with worst-case misspecified posteriors under different problem settings. Results are averaged over 10 runs with shaded standard errors.

### Worst-Case Misspecified Posteriors

We consider the worst-case examples, Equations (13) and (14), presented in the proof of Theorems 3.12 and 3.13, where the posterior distributions are misspecified using one \(\)-divergence. The results of Thompson sampling, BUCB, and EBUCB are displayed in Figure 2. From these worst-case examples, we observe that: 1) Thompson sampling exhibits a linear regret after \(t 1\). As shown in Theorem 3.12, the linear coefficient (i.e., the slope) of the regret depends on the level \(r\) that corresponds to the inference error. Specifically, the slope of the regret is increased along with the increased value of \(r\), as illustrated in both Figure 2 and the proof of Theorem 3.12. 2) BUCB/EBUCB exhibits a linear regret with constant slope \(_{1}-_{2}\) after \(t T_{0}\), where \(T_{0}\) is the time threshold introduced in Theorem 3.13 after which BUCB/EBUCB always chooses the sub-optimal action. The artificial choice of \(r\) is to make \(T_{0}=100,200,333\) where \(_{T_{0}}=\); See the proof of Theorem 3.13.

In summary, our experiments evidently demonstrate the superior performance of our proposed EBUCB on multi-armed bandit problems with generally misspecified posteriors. Our results also align closely with our theory that making one \(\)-divergence a small constant alone is insufficient to guarantee a sub-linear regret of Thompson sampling/BUCB/EBUCB. Hence, making two different \(\)-divergences bounded is necessary for the sub-linear regret upper bound.

## 5 Conclusions and Future Work

In this paper, we propose a general Bayesian bandit algorithm, Enhanced Bayesian Upper Confidence Bound (EBUCB), that achieves superior performance for Bernoulli bandit problems with approximate inference. We prove that, if the inference error measured by two different \(\)-divergences is less than a constant, EBUCB can achieve the optimal regret order \(O( T)\). Additionally, we construct worse-case examples to show the necessity of bounding two different \(\)-divergences, which is further validated by our experiments. We consider the study of other problem settings as meaningful future research that could be built upon our current framework, e.g., extending to the general exponential family bandit problems by leveraging the techniques in . We will also extend our current framework to contextual bandit problems and investigate the performance of contextual Bayesian bandit algorithms with approximate inference .