# Soft Superpixel Neighborhood Attention

Kent Gauen

Purdue University

gauenk@purdue.edu &Stanley Chan

Purdue University

stanchan@purdue.edu

###### Abstract

Images contain objects with deformable boundaries, such as the contours of a human face, yet attention operators act on square windows. This mixes features from perceptually unrelated regions, which can degrade the quality of a denoiser. One can exclude pixels using an estimate of perceptual groupings, such as superpixels, but the naive use of superpixels can be theoretically and empirically worse than standard attention. Using superpixel probabilities rather than superpixel assignments, this paper proposes soft superpixel neighborhood attention (SNA) which interpolates between the existing neighborhood attention and the naive superpixel neighborhood attention. This paper presents theoretical results showing SNA is the optimal denoiser under a latent superpixel model. SNA outperforms alternative local attention modules on image denoising, and we compare the superpixels learned from denoising with those learned with superpixel supervision.1

## 1 Introduction

The attention mechanism is attributed to meaningful benchmark improvements in deep neural networks . While the initial attention mechanism acted globally, recent methods operate on local neighborhoods, which requires less computation and reduces the chance of learning spurious correlations . Neighborhood attention (NA) transforms each pixel using its surrounding square neighborhood of pixels. In reality, this square neighborhood is merely an implementation convenience. Natural images contain objects with non-rigid boundaries, such as the contours of a human face or the outlines of text. To account for these deformations, this paper proposes a new local attention module that re-weights the attention map according to low-level perceptual groups, or "superpixels" , named soft superpixel neighborhood attention (SNA).

Theory from vision science posits that humans view scenes according to perceptual groups (e.g. Gestalt principles) . Superpixels are one such formulation of perceptual groupings, and they create a deformable, boundary-preserving segmentation of an image . Since deep learning models can learn spurious correlations, operations that utilize perceptual groups should principally help a network learn desirable correlations. Despite the deformable neighborhood shapes, applying attention with a superpixel estimate is straightforward. However, this naive approach is theoretically worse than NA for some superpixel shapes and sensitive to errors in the superpixel estimate. To address these issues, we propose thinking beyond using a single superpixel estimate.

While recent works use the maximum likelihood (ML) superpixel estimate [6; 7], many superpixel formations are equally likely. Figure 1 depicts three samples from a (slightly modified) recent BASS superpixel model . Many of the boundaries seem arbitrary and change among the samples, while some remain fixed. No single superpixel segmentation is the absolute "best". This is not a deficiency of the particular method to estimate superpixels, but rather a drawback of using a point estimate.

This paper presents a theoretically grounded approach to account for the ambiguity of superpixel assignments within an attention module. We consider this ambiguity in our derivation by re-weighting the empirical data distribution with superpixel probabilities. While standard superpixel models assume a fixed prior probability, this paper supposes each pixel has a separate prior superpixel probability. Under this new model, we find the optimal denoiser is the proposed SNA module.

**Contributions.** In summary, this paper presents a rigorous way to incorporate superpixels into an attention module. By modeling image pixels with distinct superpixel probabilities, we find the soft superpixel neighborhood attention (SNA) module is the optimal denoiser. We empirically show the SNA module outperforms NA within a single-layer denoising network by \(1-2\) dB PSNR. We compare the superpixels learned for denoising with superpixels learned using supervised training .

## 2 Related Works

**Non-Local Methods.** Classical image restoration methods transform a pixel using its neighborhood based on pairwise similarities [9; 10; 11]. Some early deep learning models incorporate non-local modules with deep learning models for image restoration [12; 13; 14; 15; 16; 17]. Once attention was introduced to the computer vision community, later methods drew analogies to between attention and non-local methods [18; 19; 20].

**Attention.** While attention modules followed the same form as non-local methods, they were originally presented as a disconnected idea [1; 21]. Attention's primary issue was originally computation, and subsequent research efforts proposed efficient alternatives [22; 23; 24; 25; 2]. These alternatives often compute attention across a smaller region, and use a deep network's depth to disperse information globally. Neighborhood attention (NA) is implemented with an efficient, custom CUDA kernel to search a square window and is among the fastest methods . The square window is theoretically more desirable than asymmetrical alternatives, such as SWIN .

**Superpixels.** Superpixels embody the concept of perceptual groupings, which suggests that humans first group smaller objects together and then aggregate this information to understand the whole image . There are many methods to model these superpixels [26; 27]. SLIC is a state-of-the-art, classical superpixel method and is explained in Section 3.1 . Superpixels have been applied extensively outside of deep learning literature [28; 29; 30; 31; 32]. Superpixels have not been employed as widely since the advent of deep learning, but recent methods have started incorporating them [6; 7]. A recent method, referred to as the superpixel sampling network (SSN), presents a supervised learning method to sample using segmentation labels . The loss function of SSN is related to the idea of superpixel pooling, which we use to evaluate superpixel probabilities in Section 5.3 [33; 34].

**Non-Parametric Density Estimation.** The derivation of the optimal denoiser using latent superpixels closely resembles the problem of mixture-based non-parametric density estimation . This paper does not address important statistical issues, such as identifiability.

Figure 1: **The Ambiguity of Superpixels.** This figure compares three superpixel estimates from a recent method named BASS. \({}^{2}\)While all three samples achieve a similar segmentation quality, some regions are different, and some are almost identical. Since no single segmentation is the “best”, this suggests that superpixel assignments are not as important as superpixel probabilities.

## 3 Preliminaries

### SLIC Superpixels

In this paper, an image (\(\)) has height (\(\)), width (\(\)), and features (\(F\)) with shape \( F\). Conceptually, superpixels are evenly spaced across the image according to the stride denoted \(_{}\). The superpixel stride (\(_{}\)) determines the number of superpixels, \(S=/_{}^{2}\) (see Fig 2). SLIC superpixels consist of superpixel means (\(}\) with shape \(S F\)), intra-superpixel precision (\(}=[}_{}\,}_{}]\) with shape \(S 2\)), the probability each pixel belongs to each superpixel (\(}\) with shape \( S\)), and the superpixel assigned to each pixel (\(}\) with shape \(\)). SLIC estimates these quantities using Lloyd's algorithm (similar to k-means) . Following recent works, each pixel is connected to at most nine superpixels .

Presently, we describe how SLIC superpixel probabilities are computed. Let the distances between pixels and superpixel means be written as a matrix \(\) of shape \( S\) and let \(^{(i,s)}=\|_{i}-_{s}\|_{2}^{2}\) if the two are connected and infinity otherwise. A difference between the indices is written \(}^{(i,s)}=\|[i_{x}\,i_{y}]-[s_{x}\,s_{y}]\|_{2}^{2}\). Superpixel probabilities are computed as,

\[}^{(i)}=(-_{i,}^{(i)}-_ {i,}}^{(i)})\] (1)

where \(_{i,},_{i,}>0\) enforce consistency across appearance (_app_) and _shape_. The estimated superpixel probabilities are the conditional probability pixel \(i\) is assigned to superpixel \(s\), \(p(s_{i}=s|_{i})=}^{(i,s)}\). A superpixel segmentation (\(}\)) is formed by assigning each pixel's cluster label to their most similar superpixel; \(_{i}=*{arg\,max}_{s}}^{(i,s)}\). These quantities are _estimates_ (note the \(}\)) of latent parameters, which is a modeling assumption discussed in Section 4.2.

### Superpixel Pooling

The attention module proposed in this paper will learn superpixel probabilities, but standard superpixel evaluation only considers a point estimate. To augment the standard superpixel benchmarks, we follow recent works and compare superpixel probabilities with _superpixel pooling_[8; 5; 34]. A matrix \(\) has size \( C\) for \(C\) segmentation classes, and superpixel probabilities (\(}\)) have size \( S\). Then, the probabilities are re-normalized, \(}^{(i,s)}=}^{(i,s)}/_{j=1}^{}}^{(j,s)}\). Finally, the superpixel pooling operation is written,

\[}_{}=}}^{}\] (2)

In this paper, the vector \(\) is either an image (\(C=F\) is the number of channels) or a segmentation label. A recent method, named the Superpixel Sampling Network (SSN), proposes learning task-specific superpixels with this superpixel pooling loss . Their loss function trains a network to estimate superpixel probabilities by comparing the original and pooled vectors. In Section 5.3, we compare the quality of superpixels learned within an SNA-denoising network and those learned using the SSN loss on the BSD500 dataset .

Figure 2: **Each Pixel is Connected to Nine Superpixels.** This figure illustrates the anatomy of the SLIC superpixels. The left-most figure illustrates how superpixels are conceptually distributed across a grid on the input image with stride \(_{}\). The right figure illustrates a single pixel is connected to (at most) nine centroids.

### Neighborhood Attention

Let a noisy input image be denoted \(\) with shape \( F\). The queries (\(\)), keys (\(\)), and values (\(\)) project the image \(\) from dimension \(F\) to \(D\), written as \(=_{q},=_{k},=_{v}\). Neighborhood attention (NA) computes the output of the attention operator using pixels within a square neighborhood around pixel \(i\), denoted \((i)\). A standard neighborhood of size \(7 7\) is among the fastest available attention methods. The attention scale controls the shape of the attention weights, \(_{} 0\). The output and attention weights are computed below,

\[f_{}^{(i)}()=_{j(i)}w_{i,j}_{j}, w _{i,j}=}d(_{i},_{j}))}}{_{j^{ }(i)}}d(_{i},_{j^{ }}))}}\] (3)

As an additional baseline, this paper augments the standard NA by learning the attention parameter for each pixel. Specifically, each pixel's attention scale is the output of a deep network, \(_{}^{(i)}=g_{,}^{(i)}()\). Further details are described in Section 4.5.

## 4 Approach

### Superpixel Neighborhood Attention

**Hard Superpixel Neighborhood Attention (H-SNA).** Perceptual groupings, such as superpixels, segment pixels into a boundary-preserving partition of the image. These boundaries account for sharp changes in pixel intensity, so using these segmentations as masks can remove perceptually unrelated information. Supplemental Section A.2 formalizes this intuition. H-SNA is a naive implementation of this concept. For a fixed query pixel (\(i\)), a neighboring pixel (\(j\)) is masked if its estimated superpixel does not match the query's superpixel, \(_{i}_{j}\). The function is written as,

\[f_{}^{(i)}(;})=_{j(i)}w_{i,j} _{j}, w_{i,j}=[_{i}=_{j}]}d(_{i},_{j}))}}{_{j^{}( i)}[_{i}=_{j^{}}]}d(_{i },_{j^{}}))}}\] (4)

**Soft Superpixel Neighborhood Attention (SNA).** There are two problems with H-SNA. First, H-SNA is unstable when the number of pixels sharing the same superpixel label within the local neighborhood is small. Examples include snake-like tendrils and superpixels that consist of small, disconnected regions (see Section A.2). The second problem with H-SNA is its high sensitivity to improper superpixel estimation. Erroneous ML estimates of superpixels have a dramatic effect on quality. To mitigate both issues, we propose soft superpixel neighborhood attention (SNA), which uses superpixel probabilities instead of superpixel assignments,

\[f_{}^{(i)}(;})=_{j(i)}w_{i,j} _{j}, w_{i,j}=}d(_{i},_{ j}))}_{s=1}^{S}}^{(i,s)}}^{(j,s)}}{_{j^{ }(i)}}d(_{i},_{j^{ }}))}_{s=1}^{S}}^{(i,s)}}^{(j^{},s)}}\] (5)

Figure 3: **Superpixel Neighborhood Attention.** The yellow region represents the attention window and the red contours are a superpixel boundary. NA considers all pixels, mixing the dissimilar orange and blue pixels. H-SNA considers only pixels within its own superpixel, which is too few pixels for denoising. SNA excludes the dissimilar blue pixels but retains the similar orange pixels.

Importantly, the sum over all superpixels contains only 9 non-zero terms (see Section 3.1). The shape of the superpixel probabilities allows SNA to interpolate between NA and H-SNA. To be specific, the superpixel probabilities may be flat to match NA, \(}^{(i,s)}\), or the superpixel may be sharp to match H-SNA, \(}^{(i,s)}[s=s^{*}]\). This control over the shape of the superpixel probabilities allows SNA to be better than both H-SNA and NA on average.

### The Latent Superpixel Model

Properly incorporating superpixels into attention requires rethinking the relationship between superpixels and images. While superpixels are usually presented with a generative model for images, this model is used only to estimate superpixel assignments. In contrast, this paper uses the data-generating process itself to derive the form of our proposed attention module. Our process is slightly different from standard literature. Ordinarily, all pixels share a single prior superpixel probability. In this paper, the superpixel probabilities are sampled for each image pixel.

For each image pixel, our generative model samples (1) superpixel probabilities \( p()\), (2) superpixel assignments \(s_{i} p(s_{i}|^{(i)})\), and (3) pixel values \(| p(|)\). Figure 4 illustrates this proposed latent variable model. Note that SLIC estimated these quantities (recall the hats: \(},}\)), but assumes a different data generating model. While the distribution of pixels given the superpixel assignments is unknown, the new model is useful for theoretical analysis. We use it to derive the form of our proposed attention module (Sec 4.3), and to compare the theoretical error between SNA, NA, and H-SNA (Sec A.2).

### SNA is the Optimal Denoiser under the Latent Superpixel Model

The optimal denoiser is the function that minimizes the mean-squared error between the denoised pixel and the original pixel value. Following similar derivations , minimizing this expectation with respect to an unknown denoiser (\(\)) is possible because of our chosen model for the unknown data density. Say we have a sample, \(,, p(,,)\). Then we approximate \(p(_{i}|_{i})\),

\[(_{i}|_{i})=_{m=1}^{M}p(m|_{i})p(_{i}|m)= _{m=1}^{M}p(_{i}|m)(_{i}-_{m})\] (6)

since \(p(_{i})=\), and \(p(m)=\). By re-weighting with superpixel probabilities, rather than superpixel assignments, our estimate does not depend on ambiguous superpixel labels. The expected loss of a denoiser is computed over the factorized joint density of the Gaussian-corrupted pixels, noise-free pixels, superpixel assignments, and superpixel probabilities: \(p(}_{i}|_{i})(_{i}|_{i})p(s_{i}| ^{(i)})p()\).

**Claim 1**: _The optimal denoiser to the following optimization problem is soft superpixel neighborhood attention (SNA) when the qkv-transforms are identity, the attention scale is fixed to \(_{}=}\), and the samples used to estimate the data density come from a neighborhood surrounding pixel \(i\),_

\[f^{(i)}_{}(};)=D^{*}(}_{i};,)=*{arg\,min}_{D}[\|D(}_{i};,)-_{i}\|^{2}]\] (7)

Figure 4: **The Latent Superpixel Model. The latent superpixel model assumes superpixel probabilities are sampled for each image pixel. This figure illustrates the data-generating process. The leftmost image allows the reader to visually compare the similarities among superpixels by representing each pixel by its most likely superpixel means. Informally, this looks like a “low-resolution image”. The superpixels and image pixels are sampled as usual.**

See Supplemental Section A.1 for the proof. In practice, the error of a denoiser depends on the sampled data, and in this paper, the data is limited to a neighborhood surrounding the query point. Section A.2 analyzes this limited neighborhood.

### Normalization for Restricted Connectivity

SLIC  and SNN  connect each pixel to (at most) nine superpixels. Therefore, some pixels within a neighborhood may only connect to a subset of superpixels that connect to the query index. This sharp cut-off in connectivity creates artifacts within the attention map since the number of non-zero superpixel probabilities for some neighbors is fewer than nine. These artifacts are due to our choice of superpixel algorithms [26; 8] rather than a limitation of our attention module. To correct these artifacts for the attention map associated with pixel \(i\), we re-normalize the adjacent superpixel probabilities to one; \(^{(j,s)}^{(j,s)}/_{s^{}(i )}^{(j,s^{})}\), where \((i)\) is the set of superpixels connected to pixel index \(i\). Figure 5 illustrates the attention maps with and without the normalization step.

### Estimating Superpixel Probabilities

SNA re-weights the attention map using superpixel probabilities, which are estimated from noisy images, \(}=g_{}()\). This paper forgoes considering theoretical concerns (such as identifiability) and uses ad-hoc estimates of these probabilities with two deep networks, depicted in Figure 6.

The first method estimates the marginal probabilities directly from a deep network, \(}=g_{,}()\). The second method estimates the superpixel parameters controlling the balance between appearance and shape with a deep network, which is then fed into differentiable SLIC iterations  (Section 3.1): \(}=g_{,}(,})\) where \([}_{}\,}_{}]=_{ ,}()\). This model is abbreviated as \(}=g_{,}()\). The number of SLIC iterations is fixed to \(5\), following a recent paper . These networks are two-layer UNets .

### Learnable Attention Scale

Using a network similar to the ones used to estimate superpixel probabilities, all attention modules are augmented to serve as another baseline. The augmentation replaces the fixed attention scale with learnable attention scales for each pixel, \(_{}=g_{,}()\). The network architecture is identical to \(g_{,}\) except for the final layer, which outputs one channel rather than nine. This baseline empirically demonstrates the theoretical argument from Supplemental Section A.3; even when modulating the attention scale, NA cannot robustly reject dissimilar pixels.

Figure 5: **Artifacts in Attention. This attention map visualizes the unnormalized superpixel weights for neighbors, \(j(i)\). The restricted connectivity leads to sharp cut-offs in the superpixel weights. To create this figure, \(^{(k,s)}=1/9\) if pixel \(k\) is connected to superpixels \(s\) and zero otherwise. White pixels indicate the region sums to \(1\), while grey regions sum to values less than \(1\).**

Figure 6: **Estimating Superpixel Probabilities. The superpixel probabilities are learned during training using one of these two methods. The left method uses a shallow UNet followed by a softmax layer with nine channels (\(g_{,}\)). The right method estimates hyperparameters to be used within SLIC iterations (\(g_{,}\)).**

Experiments

This section demonstrates the impressive benefit of superpixel neighborhood attention compared to standard neighborhood attention. To verify whether the improvement is due to the proposed method, we compare several variations of both attention methods. Section 5.2 compares different attention modules within a simple network architecture on Gaussian denoising, which empirically verifies the theoretical findings in Sections 4.3 and A.2. Section 5.3 compares the superpixel probabilities learned from the denoising loss function with superpixels learned through supervised training. Supplemental Section B.1 includes ablation experiments.

### Experimental Setup

Our experiments use a simple denoising network with larger auxiliary networks to highlight the role of the learned superpixel probabilities and attention scale parameters. The denoising network contains only projection layers and a single attention module. To keep the extracted features simple, the only interaction between neighboring pixels is through the attention layer. The project matrices project the input RGB image from three to six dimensions dimension. The full network is written,

\[}_{}=_{,}()=f_{ }(_{0},g_{}(_{0}))_{1 }+_{0}_{1}\] (8)

The primary network parameters are denoted \(=[_{0},_{1}]\), and \(g_{}()\) is the auxiliary deep network from Section 4.5. The primary network consists of only \(200\) parameters (\(\)), and the auxiliary networks contain about \(4.4\)k or \(8.8\)k parameters (\(\)). We train each network for 800 epochs using a batch size of \(2\) on the BSD500 dataset  using a learning rate of \(2 10^{-4}\) with a decay factor of \(1/2\) at epochs 300 and 600. The network is optimized with Adam . The code is implemented in Python using Pytorch, Numpy, Pandas, and CUDA and run using two NVIDIA Titan RTX GPUs and one RTX 3090 Ti GPU [40; 41; 42; 43]. Testing datasets are Set5 , BSD100 , Urban100 , and Manga109 .

### Gaussian Denoising

This subsection empirically verifies the utility of SNA on Gaussian denoising. The Charbonnier loss is used for denoiser training, \(L_{}=}_{}-\|^{2}+^{2 }}\;\;(=10^{-3})\;\;\). By default, the window size is \(15 15\), the fixed SLIC precision is ten, and the fixed attention scale is one.

Table 1 presents the quantitative results. The SNA module achieves far better denoising quality than NA, even when NA can learn its attention scale. As an extreme example, the PSNR at \(=30\) of the left-most SNA column is about **2.8 dB better than standard NA** (2nd NA column). If comparison is restricted to models with the same number of parameters, the PSNR at \(=30\) of the the fourth SNA column is about **1.8 dB better than the augmented NA** (1st NA column). For a fixed budget of network parameters, SNA yields a higher quality denoiser than NA.

Figure 7 shows SNA is qualitatively superior to NA. The first row shows that SNA produces less grainy images than NA. The bottom two rows show that NA mixes perceptually unrelated information. In the second row, NA outputs a red-orange semi-transparent mist surrounding the orange-red rock. In the third row of the NA results, the white zebra stripes are shaded darker than the clean image when the attention scale is learned, and contain white speckles if fixed. See Section B.2 for more examples.

Learning the attention scale improves standard NA and SNA+\(g_{,Deep}\) at higher noise intensities. However, it degrades model quality when superpixels are estimated with SLIC iterations (SNA+\(g_{,SLIC}\)). Perhaps this quality drop is simply due to network architecture and/or the flow of information. Since the network that learns the attention parameter is only given a noisy image, the simple model struggles to coordinate with the denoising network, which uses multiple SLIC iterations. SLIC iterations explicitly compute pairwise distances, which cannot be learned with the shallow model.

A limitation of the proposed SNA module is the additional computation compared to NA. SNA amounts to re-weighting the attention map produced by a neighborhood search, so SNA is necessarily more compute-intensive than NA. The bottom four rows of Table 1 show SNA is 15 - 22 times slower than NA. While this increase is significant, we believe the numbers reported in this paper are overly pessimistic. Our implementation of SNA has not been optimized, and future development can dramatically reduce the wall-clock time. One way to reduce wall-clock time is to read more efficiently from global CUDA memory . Anecdotally, the naive implementations of NA are 500% to 1500% slower than their recently optimized alternatives.

### Inspecting the Learned Superpixel Probabilities

This subsection investigates how the superpixels learned from denoising compare with superpixels learned from explicit supervised superpixel training . We observe a collaborative relationship between superpixel probabilities for denoising and superpixel pooling. We observe an adversarial relationship between the superpixels for denoising and boundary adherence. Generally, learning superpixels that improve superpixel benchmarks decreases their utility for denoising.

   &  &  & NA  \\  Learn \(_{}\) & & & & & & & \\ Sp. Model & \(g_{}\) & \(g_{}\) & \(g_{}\) & \(g_{}\) & \(g_{}\) & \\  \(\) & 31.96 & 32.08 & 32.07 & **32.19** & 30.88 & 30.87 & 31.10 \\
10 & 0.869 & 0.871 & 0.865 & **0.871** & 0.810 & 0.850 & 0.850 \\  & 29.01 & 28.72 & 28.77 & **29.08** & 25.56 & 27.12 & 26.96 \\  & **0.838** & 0.815 & 0.819 & 0.804 & 0.630 & 0.774 & 0.743 \\
30 & **27.70** & 26.94 & 27.25 & 27.51 & 22.37 & 25.69 & 24.91 \\  & **0.805** & 0.777 & 0.764 & 0.763 & 0.512 & 0.743 & 0.687 \\  Deno Params \(()\) & 195 & 195 & 195 & 195 & 195 & 195 & 195 \\ Aux Params \(()\) & 8.8k & 8.8k & 4.4k & 4.4k & 0 & 4.4k & 0 \\  Fwd Time (ms) & 30.20 & 45.05 & 27.06 & 40.58 & 28.86 & 4.64 & **2.08** \\ Bwd Time (ms) & 38.72 & 80.93 & 40.00 & 51.35 & 32.54 & 6.06 & **4.67** \\ Fwd Mem (GB) & 1.90 & 2.30 & 1.87 & 2.28 & 1.96 & 0.23 & **0.21** \\ Bwd Mem (GB) & 3.27 & 3.68 & 3.25 & 3.66 & 3.13 & 0.27 & **0.25** \\  

Table 1: **Image Denoising [PSNR†].** The denoising quality of each network is averaged across images from Set5 , BSD100 , Urban100 , and Manga109 . The SNA module is quantitatively superior to NA, even when NA’s attention scales are learned with a deep network. However, NA is over 20 times faster than SNA and consumes 13 times less memory. A major contribution of NA is efficiency, while the code for this paper’s proposed SNA module has not been optimized. Time and memory usage are reported for a single image of size \(128 128\).

Figure 7: **Denoised Examples [PSNR†].** This figure compares the quality of denoised images using the Simple Network and noise intensity \(=20\). The attention scale \((_{})\) is either fixed or learned with a deep network. In both cases, the NA module mixes perceptually dissimilar information, while the SNA module excludes dissimilar regions. See Section B.2 for more examples.

To compare the different training regimes for learning superpixel probabilities, an SNA network is fully trained with a combination of two loss functions. One loss is the denoising loss (\(L_{}\)). The second loss is the SSN loss function, which computes the difference between a target vector and its superpixel-pooled transformation (see Section 3.2 for super-pixel pooling) . Specifically, \(L_{}=(}_{},)\) where \(\) is the cross-entropy loss for a segmentation label and the mean-squared-error for image pixels. The superpixel stride for all methods is fixed to \(_{}=14\). Each network estimates superpixels using SLIC iterations, and the attention scale is fixed to one (if applicable). Evaluation is computed on the test set of BSD500 .

The Achievable Segmentation Accuracy (ASA) and Boundary Recall (BR) scores evaluate the quality of the maximum likelihood superpixel assignment. The ASA score measures the upper bound on the achievable accuracy of any segmentation method, and the BR measures the boundary alignment between the superpixels and a segmentation. The superpixels are processed with a connected components algorithm before computing ASA and BR scores. To qualitatively compare the learned superpixel probabilities, we follow recent literature and compare the superpixel pooled images . Images are compared against their superpixel pooled images using PSNR and SSIM.

Table 2 evaluates the denoising quality, Table 3 quantitatively evaluates the superpixel quality, and Figure 8 qualitatively evaluates the superpixel pooling quality. We observe a harmony between the denoising loss and pixel pooling loss terms. Combining the two terms yields slightly improved denoising, as reported in the first two rows of Table 2. The first, third, and fourth columns of Table 3 show the ML superpixel quality is similar, but the boundary recall is _slightly worse_ when combining the two. The superpixel pooling quality **increases by 2.33 dB PSNR** when combined compared to only denoising. The second and last columns of Figure 8 qualitatively compare the pooled images.

We observe a disharmony between the superpixel probabilities useful for denoising and those trained on segmentation labels. Table 2 reports a **2.6 dB PSNR drop in denoising quality**. The first two rows of Table 3 report training with segmentation labels yields the best ML superpixel estimates, matching related works . However, the superpixel pooling quality is poor. This suggests boundary detection is distinct from denoising and pixel pooling.

   SSN Label & PSNR/SSIM & Fwd/Bwd Time (ms) & Fwd/Bwd Mem (GB) \\  None & 32.77/0.879 & 33 & 0.5 \\ Pix & 32.78/0.889 & 62 & 6.3 \\ Seg & 30.14/0.798 & 86 & 6.3 \\  

Table 2: **Supervised Superpixel Training Impacts Denoiser Quality. This table compares the denoising quality of SNA networks trained with both a denoising loss term and an SSN loss term, \(L_{}=L_{}+L_{}\). The SSN Label “None” indicates only a denoising loss is used. Pixel labels marginally improve the denoising quality, suggesting a cooperative relationship between these optimization problems. Segmentation labels degrade the denoising, suggesting the best superpixels for boundary adherence are not the best superpixels for image denoising. Time and memory usage are reported for a single \(128 128\) image.**

   SSN Label &  &  \\  Loss & \(L_{}\) & \(L_{}\) & \(L_{}\) & \(L_{}+L_{}\) & \(L_{}\) & \(L_{}\) & \(L_{}+L_{}\) \\ \(\) & 10 & 0 & 10 & 10 & 0 & 10 & 10 \\  ASA & 0.954 & 0.952 & 0.946 & 0.947 & **0.961** & 0.958 & 0.951 \\ BR & 0.771 & 0.774 & 0.752 & 0.751 & **0.855** & 0.824 & 0.754 \\ PSNR & 27.08 & **31.02** & 29.89 & 29.41 & 21.56 & 22.20 & 25.31 \\ SSIM & 0.811 & **0.934** & 0.886 & 0.883 & 0.553 & 0.574 & 0.737 \\  

Table 3: **Evaluating Superpixel Quality. Training an SNA attention module on denoising learns superpixel probabilities with comparable quality to explicitly training superpixels. The ASA and BR metrics evaluate the ML superpixel estimate. The PSNR and SSIM metrics evaluate the quality of the superpixel pooled image.**

### Comparing Computational Complexity

Table 4 compares the FLOPs and memory consumption of SNA and NA from Equations 3 and 5. Let the neighborhood window size be written \(|(i)|=K^{2}\). In summary, the FLOPs estimate for SNA and NA is \(O([2F+10]^{2})\) and \(O([2F+1]^{2})\), respectively. The peak memory consumption for SNA and NA is \(O(+2^{2})\) and \(O(NF+^{2})\), respectively. Importantly, SNA's additional complexity does not scale with the number of features (F). Since the number of features is a significant factor in the computational cost of an attention operator, it is sensible to believe SNA can be used efficiently within large-scale deep neural networks. To be concrete, if the number of features is 128, then there is less than a 4% percent increase in FLOPs from NA to SNA.

## 6 Discussion

This paper presents the soft superpixel neighborhood attention (SNA) module as an alternative to neighborhood attention (NA). SNA accounts for deformable boundaries of objects. The key modeling assumption is that superpixel probabilities vary per pixel, and we find SNA is the optimal denoiser under this model. For a fixed budget of network parameters, SNA achieves a significantly better denoising quality than NA within a single-layer neural network. While SNA does require more computation than NA, SNA's computation does not scale with features which suggests SNA can be used efficiently within large-scale deep neural networks.

   &  &  \\  Expression & FLOPs & Memory & FLOPs & Memory \\  \((_{}^{*}_{})\) & \(O(F K^{2})\) & \(O( K^{2})\) & \(O(F K^{2})\) & \(O( K^{2})\) \\ \((_{}^{*}_{})_{i=1}^{3}(_{ }^{*}_{})_{i=1}^{3}(_{}^{*}_{ })\) & \(O( K^{2})\) & \(O( K^{2})\) & \\ \(w_{i,j}=}^{*}_{}(^{*}_{})_{i=1}^{3}(_{}^{*}_{})_{i=1}^{3} (_{}^{*}_{})_{i=1}^{3}(_{ {u}}^{*}_{})_{i=1}^{3}(_{}^{*}_{ {LPs}})}\) & \(O( K^{2})\) & \(O( K^{2})\) & \(O( K^{2})\) \\ \(_{i=1}^{3}(_{}^{*}_{})\) & \(O(F K^{2})\) & \(O(F)\) & \(O(F K^{2})\) & \(O(F K^{2})\) \\  Total FLOPs \& Peak Memory & \(O([2F+10] K^{2})\) & \(O(F K^{2}+3 K^{2})\) & \(O([2F+1] K^{2})\) & \(O(F+2 K^{2})\) \\  

Table 4: **Computational Complexity.** This table reports the FLOPs and memory consumption for Equations 3 and 5. Constant terms are retained to clarify the differences between SNA and NA. The second row re-weights the attention map using superpixel probabilities, and it is the extra step distinguishing SNA from NA. This incurs an extra \(O(9 K^{2})\) FLOPs and requires \(O(2 K^{2})\) memory. However, SNA’s additional complexity does not scale with the number of features (F).

Figure 8: **Comparing Superpixel Probabilites via Superpixel Pooling [PSNR\(\)].** This figure uses superpixel pooling to qualitatively compare superpixel probabilities learned with different loss functions. Learning superpixel probabilities with only a denoising loss yields better superpixel pooling than supervised learning with segmentation labels. However, jointly training superpixel probabilities for denoising and image superpixel pooling improves denoising and pooling quality, which suggests a useful relationship between the two tasks.

Acknowledgments

This work is supported, in part, by the National Science Foundation under the awards 2030570, 2134209, and 2133032. The authors thank Drs. Vinayak Rao and David Inouye for their valuable feedback and support.