# Infusing Spatial Knowledge into Deep Learning for Earth Science: A Hydrological Application

Zelin Xu

Department of CISE

University of Florida

Gainesville, Florida, USA

zelin.xu@ufl.edu

&Tingsong Xiao

Department of CISE

University of Florida

Gainesville, Florida, USA

xiaotingsong@ufl.edu

&Wenchong He

Department of CISE

University of Florida

Gainesville, Florida, USA

whe2@ufl.edu

&Yu Wang

Department of MAE

University of Florida

Gainesville, Florida, USA

yuwang1@ufl.edu

&Zhe Jiang

Department of CISE

University of Florida

Gainesville, Florida, USA

zhe.jiang@ufl.edu

###### Abstract

The integration of Artificial Intelligence (AI) into Earth science, including areas such as geology, ecology, and hydrology, brings potential for significant advancements. Despite this potential, applying deep learning techniques to spatial data in this field is often hindered by the lack of domain knowledge. This paper studies the integration of spatial domain knowledge and deep learning for Earth science. The problem is challenging due to the sparse and noisy input labels, spatial uncertainty, and high computational costs associated with a large number of sample locations. Existing works on neuro-symbolic models focus on integrating symbolic logic into neural networks (e.g., loss function, model architecture, and training label augmentation), but these methods do not fully address the specific spatial data challenges. To bridge this gap, we propose a Spatial Knowledge-Infused Hierarchical Learning (SKI-HL) framework1, which iteratively infers labels within a multi-resolution hierarchy, and trains the deep learning model with uncertainty-aware multi-instance learning. The evaluation of real-world hydrological datasets demonstrates the enhanced performance of the SKI-HL framework over several baseline methods. The code is available at https://github.com/ZelinXu2000/SKI-HL.

## 1 Introduction

In recent years, the potential of Artificial Intelligence (AI) to revolutionize a variety of scientific domains has been widely recognized . Earth science, including disciplines such as geology , ecology , and hydrology , stands as a prominent field where AI brought about transformative change. However, one major bottleneck is the lack of spatial domain knowledge in deep learning models, which is a crucial element in Earth science research. This paper studies the integration of spatial domain knowledge and deep learning for Earth science, focusing on flood mapping as a representative hydrological application.

Given Earth imagery, normally sparse training labels, a base deep neural network model, and a spatial knowledge base with label constraints, our problem is to infer the full labels while training the neural network. For the example of flood mapping on Earth imagery, data samples are Earth imagery pixelsin a raster grid, and explanatory feature layers are spectral bands. Initial noisy labels can be from volunteered geographical information (e.g., geo-tagged tweets). These labels are sparse and limited, as collecting complete high-quality labels through manual annotation is impractical (e.g., high time costs, obscured view due to tree canopies near flood boundary). On the other hand, there exists spatial domain knowledge related to topographical constraints on floodwater distribution, e.g., if location A is flooded and location B is at a nearby lower location, then B is flooded. Similar examples exist in crop type classification , tree crown delineation in forest ecology \({}^{*}\), and land use classification .

However, the problem poses unique challenges. First, the input labels are always spatially sparse and noisy, making it difficult to train a neural network on Earth imagery directly. For example, in flood mapping, in-situ water sensors are often located at a few locations. Second, spatial uncertainty is inherent in the knowledge-guided label inference process, which comes from the noise and sparsity of input labels, imperfect knowledge and rules, and grounding spatial rules on a coarse grid. Third, there are high computational costs associated with spatial logic inference on a large number of raster pixels and a trade-off has to be made between computational efficiency and spatial granularity.

The most closely related works are neural-symbolic systems, which integrate symbolic logical reasoning with deep neural networks . Existing methods focus on replacing the search process of symbolic reasoning to neural network [20; 32; 19], convert unstructured data, _e.g._, images into symbols for relational learning , representing symbolic knowledge as a loss regularization term [11; 4; 5; 29; 28; 33; 2], or the combination of logic inference of pseudo-labels and neural network training iteratively [18; 26; 3; 12; 17; 24]. However, these methods do not fully address the inherent challenges of spatial data, such as spatial uncertainty and the substantial computational burdens associated with logical inference over a massive number of samples (pixels).

To address the limitations of existing works, we propose **S**patial **K**nowledge-**I**nfused **H**ierarchical **L**earning (SKI-HL)  that integrates deep learning techniques with spatial knowledge-infused label inference [15; 1]. SKI-HL consists of two main modules: the uncertainty-guided hierarchical label inference module and the uncertainty-aware deep learning module. The uncertainty-guided hierarchical label inference module captures spatial relationships and dependencies based on a spatial knowledge base and infers labels with quantified uncertainty. To handle the continuous space issue, we design a multi-resolution hierarchy to iteratively refine labels with a trade-off between granularity and computational efficiency. The uncertainty-aware deep learning module leverages complete but uncertain labels from the label inference module, capturing information from the data features that cannot be obtained through logical reasoning alone. Both modules are trained iteratively to refine inferred labels, reduce uncertainty, and improve deep learning model performance. In summary, the contributions of this paper are as follows:

* We propose SKI-HL, a spatial knowledge-infused framework that integrates deep learning and logical reasoning to leverage both explanatory features and spatial knowledge derived from domain logic rules.
* Our approach is designed to handle uncertainty in both the original labels and the label inference process, making it more robust and reliable.
* We propose a strategy to balance the trade-off between spatial accuracy and computational efficiency when discretizing continuous spatial spaces for constructing logic rules and training deep learning models.
* Taking the flood mapping problem as an example, extensive experiments on real-world datasets demonstrate the superior performance of our model compared to baseline methods.

## 2 Problem statement

### Preliminaries

**Spatial Raster Framework:** A spatial raster framework is a tessellation of a two-dimensional plane into a regular grid of \(N\) cells. The framework can consist of \(m\) non-spatial explanatory _feature layers_ and _one class layer_. We denote the explanatory feature layers by \(=\{_{1},_{2},,_{N}\}\) and the class layer by \(=\{y_{1},y_{2},,y_{N}\}\), where \(_{i}^{m 1}\) and \(y_{i}\) are the explanatory features, and class at cell \(i\) respectively. Each cell in a raster framework is a spatial data sample, note as \(_{i}=(_{i},y_{i})\), where \(i,1 i N\). For example, in the flood mapping problem, the explanatory features are the spectral bands from remote sensing imagery, the target classes are flood and dry categories, and each pixel in the image is a spatial sample.

**Spatial Knowledge Base:** A spatial knowledge base \(\) is a set of logic rules: \(=\{r_{1},r_{2},,r_{||}\}\). Here, each \(r_{i}\) is a rule that represents a spatial relationship, dependency, or constraint between entities in the set of spatial samples \(\). The quantity \(||\) represents the number of rules in the spatial knowledge base \(\).

Table 1 provides an example of a spatial knowledge base used for a flood mapping on Earth imagery problem. Here the variable \(s_{i},s_{j}\) stands for a location in the study area or a pixel of Earth imagery. It is important to clarify that these rules are probabilistic in nature, reflecting the likelihood of a flood occurrence under certain conditions, rather than providing an absolute certainty. Please see Appendix A for more preliminaries about the logic rule.

### Problem definition

Formally, we define our problem as follows: given a large-scale spatial raster framework with spatial samples \(=\{_{1},_{2},,_{N}\}\), a set of explanatory feature layers \(\) in \(\), a limited set of labels \(_{1}=\{y_{1},y_{2},,y_{l}\}\), usually \(l N\), each label associated with a sample in \(\), a spatial knowledge base \(\), and a base neural network model (e.g., U-Net ), the output will be 1) Inferred labels \(}\) with quantified uncertainty \(\), and 2) A deep learning model \(DL:\). Our objective is to maximize the consistency between inferred labels \(}\) and \(\) and maximize the prediction accuracy of the deep learning model.

Specifically, we assume the raster framework \(\) contains a large number of pixel samples but only with a limited set of labels \(_{1}\). The main objective is to predict the class layer \(\) for all spatial samples. To illustrate, consider the case of flood mapping on Earth imagery. In this scenario, the set of spatial samples \(\) corresponds to Earth imagery pixels. The explanatory feature layers \(\) are the spectral bands. The label set \(\) corresponds to the flood status of each pixel (_i.e._, flooded or not). The spatial knowledge contains domain constraints on flood locations (e.g., terrains and topography), which is used to infer flood labels \(}\). Considering the errors in the inference process and the imperfect logic rules, uncertainty \(\) naturally exists in the inferred label.

## 3 The proposed approach

### Overview

Our task is to train the deep learning model and infer sample labels based on the spatial knowledge base. The task is non-trivial for several reasons. First, spatial knowledge inference on labels is computationally expensive due to the immense volume of spatial samples and complex spatial dependencies and interactions. Therefore, scalable grounding strategies are required that can effectively handle these issues by balancing computational efficiency and grounding granularity. In addition, the label inference is complicated due to incomplete and sparse initial labels compared with the large study area. Such a low proportion of known data makes logic inference difficult. Furthermore, the labels inferred come with uncertainty at different granularity levels, which is non-trivial for the training of a deep learning model.

 
**Spatial Rules** \\  \( s_{i},s_{j}(Flood(s_{i}) Adjacent(s_{i},s_{j}))  Flood(s_{j})\) \\ \( s_{i},s_{j}(River(s_{i}) Distance(s_{i},s_{j}) d)  Flood(s_{j})\) \\ \( s_{i},s_{j}(Flood(s_{i}) River(s_{i}) Downstream(s_{i},s_{ j})) Flood(s_{j})\) \\ \( s_{i}(LandCover(s_{i},) HeayRain(s_{i}))  Flood(s_{i})\) \\ \( s_{i}(Slope(s_{i})>s) Flood(s_{i})\) \\ \( s_{i}(Elevation(s_{i})>e) Flood(s_{i})\) \\  

Table 1: An example of a spatial knowledge base in flood mapping.

To address these challenges, we propose a Spatial **K**nowledge-**I**nfused **H**ierarchical **L**earning (SKI-HL) framework. Our SKI-HL framework, illustrated in Figure 1, consists of two interdependent modules. The hierarchical label inference module infers sample labels in the raster framework with a trade-off between computational efficiency and spatial granularity. We formulate the inference process as an optimization problem with an objective based on the distance loss from Probabilistic Soft Logic (PSL) [15; 1], and the spatial grounding configuration in a multi-resolution hierarchical grid structure. We design a greedy heuristic to iteratively refine the inferred labels based on inferred spatial uncertainty. The uncertainty-aware deep learning module trains neural network parameters from uncertain labels in multiple resolutions by an uncertainty-aware loss function and multi-instance learning. The two modules run in iterations: the outputs of the deep learning model will serve as the initialization of the hierarchical label inference module in the next iteration.

### Hierarchical label inference with spatial knowledge

The hierarchical label inference module utilizes spatial knowledge to enhance the label inference process within a raster framework. The module seeks spatial grounding within the framework to optimize sample class probabilities, considering the balance between granularity, computational efficiency, and inference accuracy in handling large-scale spatial data. This approach avoids excessive computational costs associated with high-resolution pixels and the coarseness of low-resolution pixel blocks. The challenge is framed as an optimization problem, focusing on generating accurate sample labels and ensuring a balance in the spatial grounding process.

#### 3.2.1 Optimization objective

We now formulate the spatial logic inference of sample labels in a raster framework as an optimization problem. First, we need to define the candidate feasible solution of spatial grounding. The process of spatial grounding refers to substituting the variables in the knowledge base rules with specific, concrete instances, which in our case are spatial samples such as pixels in Earth imagery. Given a set of spatial samples and rules from a spatial knowledge base, we substitute each possible sample into the rules to generate candidate feasible solutions for spatial grounding.

**Spatial hierarchical structure:** We exploit a hierarchical framework to address the hierarchical and fractal pattern of spatial relationships. A large-scale spatial raster framework of pixels is represented at multiple resolutions. At the coarse level, we can treat each cell as a condensed representation of many pixels. In the hierarchical structure of the spatial raster, each cell condenses many pixels, simplifying the initial logic inference stage by reducing the ratio of unlabeled data, making inference more feasible. Please see Appendix B for more details.

Second, we need to define the loss function based on the spatial grounding and inferred label probabilities. To make inferences that are consistent with spatial knowledge, we adopt t-norm fuzzy

Figure 1: Framework of SKI-HL.

logic (see Appendix A) to define the extent of a rule as satisfied, which relaxes binary truth values to a continuous value between \(\). Then, following the structure of Probabilistic Soft Logic (PSL), we can induce the distance \(d_{r}(I)=\{0,I(r_{body})-I(r_{head})\}\) to satisfaction for a rule \(r:r_{body} r_{head}\), where \(I\) is the soft truth value function that can map an atom \(a\) or a rule \(r\) to an interval between \(\), indicating the probability that the atoms or rule holds. PSL determines a rule \(r\) as satisfied when the truth value of \(I(r_{body})-I(r_{head}) 0\). To this end, we can convert logical sentences into convex combinations of individual differentiable loss functions, which not only improves the training robustness but also ensures monotonicity with respect to logical entailment, _i.e._, the smaller the loss, the higher the satisfaction. Therefore, given a set of ground rules in the ground knowledge base \(\), we can obtain the truth value for all ground atoms, which can serve as inferred labels for the spatial samples.

\[}=*{arg\,min}_{I}_{r}_{ r}d_{r}(I)\] (1)

where \(_{r}\) is the weight of rule \(r\). It is noted that here the inferred labels \(}\) are not binary values but soft truth values between \(\).

Therefore, in our hierarchical framework, our optimization problem can be summarized as searching for an optimal grounding strategy and minimizing the overall distance to satisfaction for the ground atoms. we formally define the objective to minimize in the hierarchical label inference module as:

\[L_{logic}=_{k=1}^{K}(_{r_{k}}_{r}d_{r}(I_{k })+|_{k}|))\] (2)

where \(_{k}\) stands for the ground knowledge base in the \(k\)-th layer, \(\) is a balancing coefficient. The summation over \(k\) stands for the overall objective of all layers in the hierarchical structure. The first term is the loss defined by PSL distance, which can drive accurate inference. The second term is used to decrease the ground atoms in each layer.

#### 3.2.2 A greedy algorithm

To make a balance between inference accuracy, efficiency, and granularity, we proposed a greedy heuristic grounding strategy. Intuitively, uncertain atom inference always causes a higher distance to the satisfaction of a rule, so here we choose uncertain cells in a coarse layer to refine. The quantified uncertainty \(u_{k,i}\) for each cell \(i\) at the \(k\)-th resolution level can be calculated using the entropy of the inferred label \(_{k,i}\) as follows:

\[u_{k,i}=-_{k,i}_{k,i}-(1-_{k,i})(1-_{k,i})\] (3)

We select a subset of cells with the highest uncertainty at each resolution level to refine the spatial partitioning. Let \(T_{k}\) be a threshold for selecting high-uncertainty cells at the \(k\)-th resolution level. We define a set of cells \(\{s_{k,i} u_{k,i} T_{k}\}\) that will be refined to the next finer resolution level \((k-1)\).

For the selected cells in \(_{k}\), we construct a new spatial partitioning with smaller cell size and update the grounding atoms set accordingly. We then perform PSL inference using the hierarchical label inference module at the \((k-1)\)-th resolution level with only the leaf node in the hierarchy. Since the distance-based loss is convex, we can use gradient descent to optimize it. To initialize \(I\) at different resolutions, in the first iteration, _i.e._, we pre-train the deep learning model with limited labels and use the output probabilities as the initialization. In the following iterations, the predicted probabilities of the corresponding deep learning model are regarded as the initial soft truth value of the ground atom in each \(r\). Starting from the coarsest resolution (\(k=K\)), the process continues iteratively until the finest resolution (\(k=0\)) is reached.

### Uncertainty-aware deep learning

The uncertainty-aware deep learning module is capable of capturing information from the explanatory features and plays a significant role in handling the uncertainty of inferred labels and variations in resolution. In traditional deep learning models, the model makes a prediction for each sample, but it doesn't utilize any information about how confident the model is about that prediction . This could lead to overconfident predictions in regions with scarce or noisy labels.

The module employs a modified version of the Binary Cross Entropy (BCE) loss to manage the uncertainty from the spatial knowledge and inferred label. In this module, all the deep learning predictions are at the finest resolution. We replace the binary ground truth labels in the BCE loss function with the inferred uncertain labels. The adjusted cross entropy quantifies the difference between the predicted and inferred label probability distributions, effectively incorporating uncertainty into the training process and enhancing performance in ambiguous scenarios. The module addresses multi-instance learning scenarios encountered in partitioned spatial domains by computing an aggregate probability output for each pixel at various resolution levels, capturing the overall event likelihood within corresponding coarser cells. The loss function thus becomes:

\[L_{DL}=-_{i=1}^{N_{k}}_{k,i} P_{k,i}+(1-_{k,i})(1-P_{k,i})\] (4)

The probability output \(P_{k,i}\) for each cell sample \(s_{k,i}\) is computed as:

\[P_{k,i}=|}_{s_{0,j} s_{k,i}}p_{j}\] (5)

where \(_{i}}\) is the inferred uncertain label, \(P_{k,i}\) is the average of the predicted probabilities \(p_{j}\) for all the samples \(s_{j}\) within the coarse cell \(s_{k,i}\). \(|s_{k,i}|\) represents the number of finest resolution pixels in the cell.

This modification effectively incorporates uncertainty information into the training process and can improve the model's performance when dealing with ambiguous cases. What's more, it allows the model to handle different levels of granularity in the spatial domain, making it flexible and adaptable to various spatial scales.

## 4 Evaluation

### Experiment setup

For the experiments, we use two real-world flood mapping datasets collected from North Carolina during Hurricane Matthew in 2016. We compare our proposed SKI-HL model with a variety of baselines that represent different approaches to handling spatial data and infusing knowledge into deep learning: **Pretrain**, **Self-training**, **DeepProbLog **, **Abductive Learning (ABL) **, and**SKI-HL-Base**. We used precision, recall, and F1 score on the flood mapping class to evaluate the pixel-level classification performance, and used \(AvU_{A}\), \(AvU_{I}\), and \(AvU\) to evaluate the uncertainty quantification performance. The spatial knowledge base for the flood mapping task is based on distance and topology relationships . Please see Appendix C for more experiment details.

### Comparison on classification performance

We evaluated each model using 4 labeled pixels, with results in Table 2 highlighting SKI-HL's superiority over baselines. See Appendix D for the results on dataset 2. The Pretrain model struggles the most, likely due to surface obstacles disrupting classifier generalization. While Self-training

   &  &  \\   & Class & P & R & F1 & Avg. F1 & Acc & Accuracy & \(AvU_{A}/AvU_{I}\) & \(AvU\) \\   & Dry & 0.79 & 0.62 & 0.70 &  & Accurate & 0.81 &  \\  & Flood & 0.73 & 0.86 & 0.79 & & & & & \\   & Dry & 0.60 & 0.83 & 0.70 &  & Accurate & 0.85 &  \\  & Flood & 0.93 & 0.81 & 0.86 & & & & & \\   & Dry & 0.73 & 0.78 & 0.75 &  &  & Accurate & 0.90 &  \\  & Flood & 0.88 & 0.85 & 0.87 & & & & & \\   & Dry & 0.66 & 0.78 & 0.72 &  &  & Accurate & 0.85 &  \\  & Flood & 0.90 & 0.83 & 0.86 & & & & & \\   & Dry & 0.95 & 0.93 & 0.94 &  &  & Accurate & 0.82 &  \\  & Flood & 0.96 & 0.97 & 0.96 & & & & & \\   & Dry & 0.96 & 0.92 & 0.94 &  &  & Accurate & 0.80 &  \\  & Flood & 0.95 & 0.98 & 0.96 & & & & & \\  

Table 2: Comparison on classification and uncertainty quantification for Dataset 1.

surpasses Pretrain, its predictions, though high confidence, can be erroneous, and its lack of spatial knowledge integration curtails performance. Both DeepProbLog and ABL underscore the value of integrating spatial knowledge. ABL, relying on first-order logic as rigid constraints for label revisions, fallers with intricate spatial rules possessing inherent uncertainties. DeepProbLog, while promising, requires patch-level inference given its design constraints, impacting its efficacy. SKI-HL consistently tops the baseline models across datasets. Even without grounding every pixel, it matches its base model on Dataset 1, credited to its uncertainty-driven hierarchical label inference. This structure negates the need for full dense labeling, a hurdle for other models. Overall, SKI-HL epitomizes the merits of merging spatial domain knowledge with deep learning, particularly for expansive spatial tasks with scant training labels.

### The effect of the number of initial labeled samples

To rigorously assess our SKI-HL method, we tested Dataset 1 with initial labeled data ranging from 4 to 256, doubling at each step. We used the classification accuracy over 5 runs as the metric, presented in Figure 2. All baseline models improve with more labels. However, the gains in Pretrain and Self-training are modest due to their reliance on labeled pixels to represent entire patches. ABL, despite improvements, faces a performance ceiling because of its rigid logic for label revisions. DeepProbLog, using a logic-based framework for gradients, shows steady improvement with minimal variance. Distinctly, SKI-HL's accuracy remains consistent at around 0.95, regardless of the initial label count. This can be attributed to its unique label inference, which starts coarsely and refines iteratively. Minor result variations arise from training dynamics and initialization.

### Comparison on uncertainty quantification performance

In Table 2, we notice a clear distinction in uncertainty estimation between our proposed SKI-HL model and the baselines. While Pretrain and Self-training models manifest a larger gap between \(AvU_{A}\) and \(AvU_{I}\), this discrepancy is mitigated in DeepProbLog and ABL, which effectively incorporate spatial knowledge into learning. However, they still struggle to achieve a balanced \(AvU_{A}\) and \(AvU_{I}\), particularly in situations of sparse and noisy labels. In stark contrast, our proposed SKI-HL model exhibits a superior performance on both datasets, signifying its robust ability to model complex spatial dependencies and adjust to areas of uncertainty dynamically. The integration of uncertainty-guided hierarchical label inference further mitigates the impact of sparse labeling, a bottleneck for other models. This finding emphasizes the pivotal role of efficiently integrating spatial domain knowledge with deep learning, especially under the constraints of limited training labels, in achieving reliable uncertainty estimation for large-scale spatial applications.

### Case study

In our case study, we visually analyze the effectiveness of our model across varying resolution levels. As depicted in Figure 2(a), we present the aerial Earth imagery, ground truth label, and digital elevation map from Dataset 1. It is noted that we don't use the ground truth to train our label, instead, it was

Figure 2: Accuracy comparison on different numbers of initial labels.

only used for testing. Figures 2(b) and 2(c) illustrate the evolution of inferred labels and deep learning predictions at different resolution levels. The resolution of the inferred labels refines progressively from a coarse resolution of 25 by 18 to the finest resolution of 2500 by 1800. This process allows for the accurate detection and refinement of uncertain areas, which often represent flood boundaries. Simultaneously, the granularity increase of the training labels results in an improved output from the deep learning model. A clear reduction in misclassified pixels can be observed, appearing as noise within each class of the area. This improvement can be attributed to the fact that multi-instance learning, used with coarse resolution labels, cannot provide supervision to every pixel. Hence, as our approach refines the label resolution, the deep learning model is able to generate more accurate predictions.

## 5 Conclusion and future works

In this paper, we proposed a novel Spatial Knowledge-Infused Hierarchical Learning (SKI-HL) framework that successfully addresses the limitations of existing deep learning models for Earth science through a system of iteratively inferring labels within a multi-resolution hierarchy. Our model outperformed several baseline methods on real-world flood mapping datasets.

In the future, the model can incorporate temporal dynamics features and capture changes in Earth imagery over time, which is critical for many applications such as deforestation tracking. Second, we can expand to other Earth science applications to further validate the generalizability and adaptability of the proposed SKI-HL framework.