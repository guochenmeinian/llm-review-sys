# Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts

Emanuele Marconato

DISI and DI

University of Trento and University of Pisa

Trento, Italy

emanuele.marconato@unitn.it

&Stefano Teso

CIMeC and DISI

University of Trento

Trento, Italy

stefano.teso@unitn.it

&Antonio Vergari

School of Informatics

University of Edinburgh

Edinburgh, UK

avergari@exseed.ed.ac.uk

&Andrea Passerini

DISI

University of Trento

Trento, Italy

andrea.passerini@unitn.it

###### Abstract

Neuro-Symbolic (NeSy) predictive models hold the promise of improved compliance with given constraints, systematic generalization, and interpretability, as they allow to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts extracted from sub-symbolic inputs. It was recently shown that NeSy predictors are affected by _reasoning shortcuts_: they can attain high accuracy but by leveraging concepts with _unintended semantics_, thus coming short of their promised advantages. Yet, a systematic characterization of reasoning shortcuts and of potential mitigation strategies is missing. This work fills this gap by characterizing them as unintended optima of the learning objective and identifying four key conditions behind their occurrence. Based on this, we derive several natural mitigation strategies, and analyze their efficacy both theoretically and empirically. Our analysis shows reasoning shortcuts are difficult to deal with, casting doubts on the trustworthiness and interpretability of existing NeSy solutions.

## 1 Introduction

Neuro-Symbolic (NeSy) AI aims at improving the _robustness_ and _trustworthiness_ of neural networks by integrating them with reasoning capabilities and prior knowledge . We focus on _NeSy predictors_, neural structured-output classifiers that infer one or more labels by reasoning over high-level _concepts_ extracted from sub-symbolic inputs, like images or text . They leverage reasoning techniques to encourage - or even _guarantee_ - that their predictions comply with domain-specific regulations and inference rules. As such, they hold the promise of improved _systematic generalization_, _modularity_, and _interpretability_, in that learned concepts can be readily reused in different NeSy tasks, as done for verification , and for explaining the model's inference process to stakeholders . On paper, this makes NeSy predictors ideal for high-stakes applications that require both transparency and fine-grained control over the model's (in- and out-of-distribution) behavior, such as medical diagnosis , robotics  and self-driving cars .

Much of the promise of these models relies on learned concepts being _high quality_. The general consensus is that the prior knowledge constrains learned concepts to behave as expected  and issues with them are often tackled heuristically . It was recently shown that, however, NeSy predictors can _attain high accuracy by leveraging concepts with unintended semantics_. FollowingMarconato et al. , we refer to these as _reasoning shortcuts_ (RSs). RSs are problematic, as concepts encoding unintended semantics compromise generalization across NeSy tasks, as shown in Fig. 1, as well as interpretability and verification of NeSy systems . Moreover, the only known mitigation strategies are based on heuristics [20; 11].

The issue is that RSs - and their root causes - are not well understood, making it difficult to design effective remedies. In this paper, we fill this gap. We introduce a formal definition of RSs and theoretically characterize their properties, highlighting how they are a general phenomenon affecting a variety of state-of-the-art NeSy predictors. Specifically, our results show that RSs can be shared across many NeSy architectures, and provide a way of _counting_ them for any given learning problem. They also show that RSs depend on _four key factors_, namely the structure of the prior knowledge and of the data, the learning objective, and the architecture of the neural concept extractor. This enables us to identify several supervised and unsupervised mitigation strategies, which we systematically analyze both theoretically and empirically. Finally, we experimentally validate our findings by testing a number of representative NeSy predictors and mitigation strategies on four NeSy data sets.

**Contributions.** Summarizing, we: (_i_) Formalize RSs and identify four key root causes. (_ii_) Show that RSs are a general issue impacting a variety of NeSy predictors. (_iii_) Identify a number of mitigation strategies and analyze their effectiveness, or lack thereof. (_iv_) Empirically show that RSs arise even when the data set is large and unbiased, and evaluate the efficacy of different mitigation strategies, highlighting the limits of unsupervised remedies and the lack of a widely applicable recipe.

## 2 The Family of Neuro-Symbolic Predictors

**Notation.** Throughout, we indicate scalar constants \(x\) in lower-case, random variables \(X\) in upper case, and ordered sets of constants \(\) and random variables \(\) in bold typeface. Also, \(_{i:j}\) denotes the subset \(\{x_{i},,x_{j}\}\), \([n]\) the set \(\{1,,n\}\), and \(\) indicates that \(\) satisfies a logical formula \(\).

**NeSy predictors.** A _NeSy predictor_ is a model that infers \(n\)_labels_\(\) (taking values in \(\)) by reasoning over a set of \(k\) discrete _concepts_\(\) (taking values in \(\)) extracted from a sub-symbolic continous _input_\(\) (taking values in \(\)). Reasoning can be implemented in different ways, but overall its role is to encourage the model's predictions to comply with given _prior knowledge_\(\), in the sense that predictions \(\) that do not satisfy the knowledge are generally avoided. The prior knowledge \(\) is assumed to be provided upfront and correct, as formalized in Section 3. Normally, only supervision on the labels \(\) is available for training, with the concepts \(\) treated as latent variables.

**Example 1**.: _In MNIST-Addition, given a pair of MNIST images , say \(=(},})\), the model has to infer the concepts \(=(C_{1},C_{2})\) encoding the digit classes, to predict their sum \(Y\), in this case \(8\). Reasoning drives the model towards complying with constraint \(=(Y=C_{1}+C_{2})\)._

The concepts are modeled by a conditional distribution \(p_{}()\) parameterized by \(\), typically implemented with a neural network. The predicted concepts can be viewed as "soft" or "neural" predicates with a truth value ranging in \(\). As for the reasoning step, the most popular strategies

Figure 1: **Reasoning shortcuts undermine trustworthiness. An autonomous vehicle has to decide whether to \(Y=\) or \(Y=\) based on three binary concepts extracted from an image \(\), namely \(C_{1}=\) light, \(C_{2}=\) light and \(C_{3}=\) of pedestrians (shown in pink). Left: In Task 1, the prior knowledge \(=()\) instructs the vehicle to stop whenever the light is red or there are pedestrians on the road. The model can perfectly classify an (even exhaustive) training set by acquiring a _reasoning shortcut that classifies pedestrians as red lights_. Right: The learned concepts are then reused to guide an autonomous ambulance with the additional rule that in emergency situations red lights can be ignored, with potentially dire consequences. Our work identifies the causes of RSs (Section 4) and several mitigation strategies (Section 5).**

involve _penalizing_ the model for producing concepts and/or labels inconsistent with the knowledge at training time [8; 23; 24] or introducing a _reasoning layer_ that infers labels from the predicted concepts and also operates at inference time [7; 9; 25; 10]. In either case, end-to-end training requires to differentiate through the knowledge. Mainstream options include softening the knowledge using fuzzy logic [26; 6; 27; 11] and casting reasoning in terms of probabilistic logics [28; 7; 10].

To investigate the scope and impact of RSs, we consider three representative NeSy predictors. The first one is **DeepProbLog** (DPL) , which implements a sound probabilistic-logic reasoning layer on top of the neural predicates. DPL is a discriminative predictor of the form :

\[p_{}(;)=_{}u_{}() p_{}()\] (1)

where the concept distribution is fully factorized and the label distribution is _uniform_1 over all label-concept combinations compatible with the knowledge, that is, \(u_{}()=\{ [/]\}/Z(;)\) where the indicator \(\{[/]\}\)_guarantees_ all labels inconsistent with \(\) have zero probability, and \(Z(;)=_{}\{ [/]\}\) is a normalizing constant. Inference amounts to computing a most likely label \(*{argmax}_{}\;p_{}(; )\), while learning is carried out via maximum (log-)likelihood estimation, that is given a training set \(=\{(,)\}\), maximizing

\[(p_{},,):=|}_{ (,)}\; p_{}(;).\] (2)

In general, it is intractable to evaluate Eq. (1) and solve inference exactly. DPL leverages knowledge compilation [29; 30] to make both steps practical.

Our analysis on DPL can be carried over to other NeSy predictors implementing analogous reasoning layers [31; 32; 33; 10; 34]. Furthermore, we show that certain RSs affect also alternative NeSy approaches such as the **Semantic Loss** (SL)  and **Logic Tensor Networks** (LTNs) , two state-of-the-art penalty-based approaches. Both reward a neural network for predicting labels \(\) consistent with the knowledge, but SL measures consistency in probabilistic terms, while LTNs use fuzzy logic to measure a fuzzy degree of knowledge satisfaction. See A for a full description.

## 3 Reasoning Shortcuts as Unintended Optima

It was recently shown that NeSy predictors are vulnerable to _reasoning shortcuts_ (RSs), whereby the model attains high accuracy by leveraging concepts with _unintended semantics_.

**Example 2**.: _To build intuition, consider \(\) and assume the model is trained on examples of only two sums: \(+=1\) and \(+=2\). Note that there exist two distinct maps from images to concepts that perfectly classify such examples: one is the intended solution (\( 0, 1, 2\)), while the other is (\( 1, 0, 1\)). The latter is unintended._

**The ground-truth data generating process.** In order to properly define what a RS is, we have to first define what _non_-shortcut solutions are. In line with work on causal representation learning [35; 36; 37], we do so by specifying the ground-truth data generation process \(p^{*}(,;)\). Specifically, we assume it takes the form illustrated in Fig. 2. In short, we assume there exist \(k\) unobserved ground-truth concepts \(\) (_e.g._, in \(\) these are the digits \(0\) to \(9\)) that determine _both_ the observations \(\) (the MNIST images) and the labels \(\) (the sum). We also allow for extra stylistic factors \(\), independent from \(\), that do influence the observed data (_e.g._, calligraphic style) but _not_ the labels. The ground-truth concepts are discrete and range in \(=[m_{1}][m_{k}]\), whereas the style \(^{q}\) is continuous. The training and test examples \((,)\) are then obtained by first sampling \(\) and \(\) and then \( p^{*}(,)\) and \( p^{*}(;)\). Later on, we will use \(()\) to denote the support of \(p^{*}()\). We also assume that the ground-truth process is consistent with the prior knowledge \(\), in the sense that invalid examples are never generated: \(p^{*}(;)=0\) for all \((,)\) that violate \(\).

**What is a reasoning shortcut?** By definition, a NeSy predictor \(p_{}(;)\) - shown in **blue** in Fig. 2 - acquires concepts with the correct semantics if it recovers the ground-truth concepts, _i.e._,

\[p_{}() p^{*}() \;\] (3)

Figure 2: The ground-truth data generation process (in **black**) and a NeSy predictor (in **blue**).

A model satisfying Eq. (3) easily generalizes to other NeSy prediction tasks that make use of the same ground-truth concepts \(\), as \(p_{}()\) can be reused for solving the new task . It is also interpretable, in the sense that as long as stakeholders understand the factors \(\), they can also interpret concept-based explanations of the inference process that rely on \(\). Naturally, there may exist many concept distributions that _violate_ Eq. (3) and that as such do _not_ capture the correct semantics. However, all those that achieve sub-par log-likelihoods can be ideally avoided simply by improving learning or supplying more examples. We define RSs as those concept distributions for which these strategies are not enough.

**Definition 1**.: _A reasoning shortcut is a distribution \(p_{}()\) that achieves maximal log-likelihood on the training set but does not match the ground-truth concept distribution,_

\[(p_{},,)=_{^{} }\ (p_{^{}},,) p_{ }() p^{*}()\] (4)

This makes RSs difficult to improve by regular means and also hard to spot based on predictions alone. Yet, since the concepts do not recover the correct semantics, RSs can compromise systematic generalization and interpretability. For instance, the shortcut concepts learned in Example 2 would fail completely at MNIST-Addition tasks involving digits other than \(0\), \(1\), and \(2\) and also at other arithmetic tasks, like multiplication, involving the same digits, as we show experimentally in Section 6. An examples of RSs in a high-stakes scenario is shown in Fig. 1.

## 4 Properties of Reasoning Shortcuts

**Counting deterministic RSs.** We begin by assessing how many RSs exist in an idealized setting in which the ground-truth generative process is simple, we have access to the true risk, and the concepts \(\) have been specified correctly, that is, \(=\), and then proceed to work out what this entails. Specifically, we work with these assumptions:

1. The distribution \(p^{*}(,)\) is induced by a map \(f:(,)\), _i.e._, \(p^{*}(,)=\{=f(, )\}\), where \(f\) is _invertible_, and _smooth_ over \(\).
2. The distribution \(p^{*}(;)\) is induced by a map \(_{}:\). This is what happens in MNIST-Addition, as there exists a unique value \(y\) that is the sum of any two digits \((g_{1},g_{2})\).

Our analysis builds on a link between the NeSy predictor \(p_{}(;)\) and the concept extractor \(p_{}()\), which depend on \(\), and their analogues \(p_{}(;)\) and \(p_{}()\) that depend directly on the ground-truth concepts \(\). This link is formalized by the following lemma:

**Lemma 1**.: _It holds that: (i) The true risk of \(p_{}\) can be upper bounded as follows:_

\[_{(,) p^{*}(,;)}[ p_{}(;)]_{  p()}-[p^{*}(;) p_{}(;)]- [p^{*}(;)]\] (5)

_where \(\) is the Kullback-Leibler divergence and \(\) is the Shannon entropy. Moreover, under \(\) and \(\), \(p_{}(;)\) is an optimum of the LHS of Eq. (5) if and only if \(p_{}(;)\) is an optimum of the RHS. (ii) Under \(\), there exists a bijection between the deterministic concept distributions \(p_{}()\) that are constant over the support of \(p()\), for each \(()\), and the deterministic distributions of the form \(p_{}()\)._

All proofs can be found in Appendix B. Lemma 1 implies that the deterministic concept distributions \(p_{}()\) of NeSy predictors \(p_{}(;)\) that maximize the LHS of Eq. (5), including those that are RSs, correspond one-to-one to the deterministic distributions \(p_{}()\) yielding label distributions \(p_{}(;)\) that maximize the RHS of Eq. (5). Hence, we can count the number of _deterministic_ RSs by counting the deterministic distributions \(p_{}()\):

**Theorem 2**.: _Let \(\) be the set of mappings \(:\) induced by all possible deterministic distributions \(p_{}()\), i.e., each \(p_{}()=\{=()\}\) for exactly one \(\). Under \(\) and \(\), the number of deterministic optima \(p_{}()\) of Eq. (5) is:_

\[_{}_{ ()}(_{})()= _{}()}\] (6)

Intuitively, this sum counts the deterministic concept distributions \(p_{}()\) - embodied here by the maps \(\) - that output concepts predicting a _correct_ label for each example in the training set. The ground-truth distribution \(p^{*}()\) is one such distribution, so the count is always at least one, but there may be more, and all of these are RSs. Eq.6 gives us their exact number. This formalizes the intuition of Marconato et al.  that, as long as the prior knowledge \(\) admits the correct label \(\) to be inferred from more than one concept vector \(\), there is room for RSs. So far, we have assumed Eq.2 is computed as in DPL. However, RSs are chiefly a property of the _prior knowledge_, and as such also affect NeSy predictors employing different reasoning procedures or different relaxations of the knowledge. We show this formally in AppendixA. Deterministic RS are also important because - in certain cases - they define a basis for _all_ reasoning shortcuts:

**Proposition 3**.: _For probabilistic logic approaches (including DPL and SL): (i) All convex combinations of two or more deterministic optima \(p_{}()\) of the likelihood are also (non-deterministic) optima. However, not all convex combinations can be expressed in DPL and SL. (ii) Under **AI** and **A2**, all optima of the likelihood can be expressed as a convex combination of deterministic optima. (iii) If **A2** does not hold, there may exist non-deterministic optima that are not convex combinations of deterministic ones. These may be the only optima._

Combining Proposition3 (_i_) with Theorem2 gives us a lower bound for the number of _non-deterministic_ RSs, in the sense that if there are at least two deterministic RS, then there exist infinitely many non-deterministic ones. An important consequence is that, if we can somehow control what deterministic RSs affect the model, then we may be able to implicitly lower the number of _non-deterministic_ RSs as well. However, Proposition3 implies that there may exist _non-deterministic_ RSs that are unrelated to the deterministic ones and that as such cannot be controlled this way.

## 5 Analysis of Mitigation Strategies

The key factors underlying the occurrence of deterministic RSs appear explicitly in Eq.6. These are: (_i_) the knowledge \(\), (_ii_) the structure of \(()\), (_iii_) the objective function \(\) used for training (via Lemma1), and (_iv_) the architecture of the concept extractor \(p_{}()\), embodied in the Theorem by \(p_{}()\). This gives us a starting point for identifying possible mitigation strategies and analyzing their impact on the number of _deterministic_ RSs. Our main results are summarized in Table1.

### Knowledge-based Mitigation

The _prior knowledge_\(\) is the main factor behind RSs and also a prime target for mitigation. The most direct way of eliminating unintended concepts is to edit \(\) directly, for instance by eliciting additional constraints from a domain expert. However, depending on the application, this may not be feasible: experts may not be available, or it may be impossible to constrain \(\) without also eliminating concepts with the intended semantics.

A more practical alternative is to employ _Multi-Task Learning_ (mtl). The idea is to train a NeSy predictor over \(T\) tasks sharing the same ground-truth concepts \(\) but differing prior knowledge \(^{(t)}\), for \(t[T]\). _E.g._, one could learn a model to predict both the sum and product of MNIST digits, as in our experiments (Section6). Intuitively, by constraining the concepts to work well across tasks, mtl leaves less room for unintended semantics. The following result confirms this intuition:

**Proposition 4**.: _Consider \(T\) NeSy prediction tasks with knowledge \(^{(t)}\), for \(t[T]\) and data sets \(^{(t)}\), all sharing the same \(p^{*}()\). Under **AI** and **A2**, any deterministic optimum \(p_{}()\) of the mtl loss (i.e., the average of per-task losses) is a deterministic optimum of a single task with prior

   Mitigation & Requires & Constraint on \(\) & Assumptions & Result \\  None & – & \(_{()}((_{})()=_{}())\) & **AI**, **A2** & Theorem2 \\ mtl & Tasks & \(_{()}_{t[T]} ((_{^{(t)}})()=_{^{(t )}}())\) & **AI**, **A2** & Proposition4 \\ c & Sup. on C & \(_{ S()}_{t  I}((_{i}()=g_{i}).\) & **AI** & Proposition5 \\ r & – & \(_{,^{}(): ^{}}(()( ^{}))\) & **AI**, **A3** & Proposition6 \\   

Table 1: **Impact of different mitigation strategies on the number of deterministic optima**: \(\) is reconstruction, \(\) supervision on \(\), mtl multi-task learning, and \(\) disentanglement. All strategies reduce the number of \(\)’s in Eq.6, sometimes substantially, but require different amounts of effort to be put in place. Actual counts for our data sets are reported in AppendixC.2.

knowledge \(_{t}^{(t)}\). The number of deterministic optima amounts to:_

\[_{}_{()}_{t=1}^{T}(_{^{(t)}})( )=_{^{(t)}}()}\] (7)

This means that, essentially, mtl behaves like a logical conjunction: any concept extractor \(p_{}()\) incompatible with the knowledge of _any_ task \(t\) is not optimal. This strategy can be very effective, and indeed it performs very well in our experiments, but it necessitates gathering or designing a _set_ of correlated learning tasks, which may be impractical in some situations.

### Data-based Mitigation

Another key factor is the support of \(p^{*}()\): if the support is not full, the conjunction in Eq. (6) becomes looser, and the number of \(\)'s satisfying it increases. This is what happens in MNIST-EvenOdd (Example 2): here, RSs arise precisely because the training set only includes a _subset_ of combinations of digits, leaving ample room for acquiring unintended concepts.

We stress, however, that RSs can also occur if the data set is _exhaustive_, as in the next example.

**Example 3** (XOR task).: _Consider a task with three binary ground-truth concepts \(=(G_{1},G_{2},G_{3})\) in which the label \(Y\) is the parity of these bits, that is \(=(Y=G_{1} G_{2} G_{3})\). Each label \(Y\{0,1\}\) can be inferred from four possible concept vectors \(\), meaning that knowing \(\) is not sufficient to identify the \(\) it was generated from. In this case, it is impossible to pin down the ground-truth distribution \(p^{*}(;)\) even if all possible combinations of inputs \(\) are observed._

One way of avoiding RSs is to explicitly guide the model towards satisfying the condition in Eq. (3) by supplying _supervision_ for a subset of concepts \(_{I}\), with \(I[k]\), and then augmenting the log-likelihood with a cross-entropy loss over the concepts of the form \(_{i I} p_{}(C_{i}=g_{i})\). Here, training examples \((,_{I},)\) come with annotations for the concepts indexed by \(I\). The impact of this strategy on the number of deterministic RSs is given by the following result:

**Proposition 5**.: _Assume that concept supervision is available for all \(\) in \(()\). Under **AI**, the number of deterministic optima \(p_{}()\) minimizing the cross-entropy over the concepts is:_

\[_{}_{ }_{i I}_{i}()=g_{i}}\] (8)

This strategy is very powerful: if \(I=[k]\), \(()\), and the support is complete, there exists only _one_ map \(\) that is consistent with the condition in Eq. (8) and it is the identity. Naturally, this comes at the cost of obtaining dense annotations for all examples, which is often impractical.

### Objective-based Mitigation

A natural alternative is to augment the log-likelihood with an _unsupervised_ penalty designed to improve concept quality. We focus on reconstruction penalties like those used in auto-encoders [40; 41; 42; 43; 44], which encourage the model to capture all information necessary to reconstruct the input \(\). To see why these might be useful, consider Example 2. Here, the model learns a RS mapping both and to the digit \(1\): this RS hinders reconstruction of the input images, and therefore could be avoided by introducing a reconstruction penalty.

In order to implement this, we introduce additional latent variables \(\) that capture the style \(\) of the input \(\) and modify the concept extractor to output both \(\) and \(\), that is: \(p_{}(,)=p_{}( ) p_{}()\). The auto-encoder reconstruction penalty is then given by:

\[()=-_{(,) p_{}( ,|)} p_{}( ,)\] (9)

where, \(p_{}(,)\) is the decoder network. We need to introduce an additional assumption **A3**: the encoder and the decoder separate content from style, that is, \(p_{}(,,):=_{  p^{*}(|,)}p_{}(, )\) factorizes as \(p_{}()p_{}()\) and \(p_{}(,,):=_{  p_{}(|,)}p^{*}(, )\) as \(p_{}()p_{}()\). In this case, we have the following result:

**Proposition 6**.: _Under **AI** and **A3**, the number of deterministic distributions \(p_{}()\) that minimize the reconstruction penalty in Eq. (9) is:_

\[_{}_{,^ {}():^{}^{ }}()(^{})}\] (10)In words, this shows that indeed optimizing for reconstruction facilitates disambiguating between different concepts, _i.e._, different ground-truth concepts cannot be mapped to the same concept. However, minimizing the reconstruction can be non-trivial in practice, especially for complex inputs.

### Architecture-based Mitigation

One last factor is the _architecture of the concept extractor_\(p_{}()\), as it implicitly controls the number of candidate deterministic maps \(\) and therefore the sum in Theorem2. If the architecture is unrestricted, \(p_{}()\) can in principle map any ground-truth concept \(\) that generated \(\) to any concept \(\), thus the cardinality of \(\) increases exponentially with \(k\).

A powerful strategy for reducing the size of \(\) is _disentanglement_. A model is disentangled if and only if \(p_{}()\) factorizes as \(_{j[]}p_{}(C_{j} G_{j})\)[45; 36]. In this case, the maps \(\) also factorize into per-concept maps \(_{j}:[m_{j}][m_{j}]\), dramatically reducing the cardinality of \(\), as shown by our first experiment. In applications where the \(k\) concepts are naturally independent from one another, _e.g._, digits in MNIST-Addition, one can implement disentanglement by predicting each concept using the same neural network, although more general techniques exist [46; 47].

### Other Heuristics based on Entropy Regularization

Besides these mitigation strategies, we investigate empirically the effect of the Shannon entropy loss, defined as \(1-_{i=1}^{k}_{m_{i}}[p_{}(c_{i})]\), which was shown to increase concept quality in DPL . Here, \(p_{}()\) is the marginal distribution over the concepts, and \(_{m_{i}}\) is the normalized Shannon entropy over \(m_{i}\) possible values for the distribution. Notice that this term goes to zero only when each distribution \(p_{}(C_{i})\) is uniform, which may conflict with the real objective of the NeSy prediction (especially when only few concepts are observed). Other similar heuristics that are suited for reducing over-confidence in label predictions are based on label smoothing , energy-based models , annealing , and many others [51; 52; 53]. In principle, when applied at the concept level, they help reduce the over-confidence in concepts that is typical in deterministic RSs. While they could also be beneficial to mildly reduce the number of RSs, we take the Shannon entropy regularization as a representative of this family for our experiments.

## 6 Case Studies

In this section, we evaluate the impact of RSs in synthetic and real-world NeSy prediction tasks and how the mitigation strategies discussed in Section5 fare in practice. More details about the models and data are reported in AppendixC. The **code** is available at github.com/reasoning-shortcuts.

**Q1: More data does not prevent RSs but disentanglement helps.** We start by evaluating the robustness of DPL, SL, and LTN to RSs in two settings where the data set is _exhaustive_. In XOR (cf. Example3), the goal is to predict the parity of three binary concepts \(\{0,1\}^{3}\) given prior knowledge \(=(y=g_{1} g_{2} g_{3})\). The predictor receives the ground-truth concepts \(\) as input and has to learn a distribution \(p_{}()\). In MNIST-Addition (cf. Example1) the goal is to correctly predict the sum of two MNIST digits, _e.g._, \(=(}}}}}}}}}}}})\) and \(y=3\). In both tasks, the training set contains examples of _all_ possible combinations of concepts, _i.e._, \(()=\).

Since the tasks are relatively simple, we can afford to study models that achieve near-optimal likelihood, for which Definition1 approximately applies. To this end, for each NeSy architecture, we train several models using different seeds and stop as soon as we obtain \(30\) models with likelihood \( 0.95\). Then, we measure the percentage of models that have acquired a RS. We do the same also for models modified to ensure they are disentangled (dis in the Table), see AppendixC for details. The results, reported in Table2, clearly show that RSs affect _all_ methods if disentanglement is not in place. This confirms that, without implicit architectural biases, _optimizing for label accuracy alone is not

    &  &  \\   & DPL & SL & LTN & DPL & SL & LTN \\  \)} & \(100\%\) & \(100\%\) & \(100\%\) & \(96.7\%\) & \(82.9\%\) & \(100\%\) \\  & \(0\%\) & \(0\%\) & \(0\%\) & \(0\%\) & \(0\%\) & \(0\%\) \\   

Table 2: **Q1: Disentanglement (dis) can be very powerful to lower the frequency of RSs on XOR and MNIST-Addition data sets (the lower the better). Results are averaged over \(30\)_optimal_ runs.**sufficient to rule out RSs, even when the data is exhaustive_. However, when forcing disentanglement, the percentage of models affected by RSs drops to _zero_ for all data sets and methods, indicating that mitigation strategies that go beyond the standard learning setup - and specifically, disentanglement - can be extremely effective.

**Q2: Disentanglement is not enough under selection bias.** Next, we look at two (non-exhaustive) data sets where RS occur due to _selection bias_. Label and concept quality is measured using the \(F_{1}\)-score on the macro average measured on the test set. From here onward, all models are disentangled by construction, and despite this are affected by RSs, as shown below.

We start by evaluating MNIST-EvenOdd, a variant of MNIST-Addition (inspired by ) where only \(16\) possible pairs of digits out of \(100\) are given for training: \(8\) comprise only even digits and \(8\) only odd digits. As shown in , this setup allows to exchange the semantics of the even and odd digits while ensuring all sums are correct (because, _e.g._, \(}}+}}=12=}}+}}\)). As commonly done in NeSy, hyperparameters were chosen to optimize for label prediction performance on a validation set, cf. Appendix C. The impact of reconstruction (r), concept supervision (c), and Shannon entropy loss (h) on all architectures are reported in Table 3. Roughly speaking, a concept \(F_{1}\) below \(95\%\) typically indicates a RSs, as shown by the concept confusion matrices in Appendix C. The main take away is that _no strategy alone can effectively mitigate RSs for any of the methods_. Additionally, for DPL and LTN, these also tend to interfere with label supervision, yielding degraded prediction performance. The SL is not affected by this, likely because it is the only method using a separate neural layer to predict the labels. Combining multiple strategies does improve concept quality on average, depending on the method. In particular, \(+\) and \(++\) help LTN identify good concepts, and similarly for DPL, although concept quality is slightly less stable. For the SL only concept supervision is relatively effective, but the other strategies are not, probably due to the extra flexibility granted by the top neural layer compared to DPL and LTN.

Next, we evaluate the impact of multi-task learning on an arithmetic task, denoted MNIST-AddMul. Here, the model observes inputs \(\{(}},}}),( }},}}),(}},}})\}\) and has to predict both their sum _and_ their product, either separately (no MTL) or jointly (MTL). The results in Table 3 show that, as in Example 2, all methods are dramatically affected by RSs when MTL is not in place. DPL and LTN also yield sub-par \(F_{1}\)-scores on the labels for the addition task. For SL and LTN, we also observe that, despite high concept \(F_{1}\) for the multiplication task, the digit \(}}\) is _never_ predicted correctly. This is clearly visible in the confusion matrices in Appendix C. However, solving addition and multiplication jointly via MTL ensures all methods acquire very high quality concepts.

**Q3: Reasoning Shortcuts are pervasive in real-world tasks.** Finally, we look at RSs occurring in BDD-OIA, an autonomous vehicle prediction task. The goal is to predict multiple possible actions \(=(,,,)\) from frames \(\) of real driving scenes. Each scene is described by \(21\) binary concepts \(\) and the knowledge \(\) ensures the predictions are consistent with safety constraints (_e.g._, \(\) -move_forward) and guarantees concepts are predicted consistently with one another (_e.g._, \(\) -obstacle). Since this is a high-stakes task, we solve it with DPL, as it is the only approach out of the ones we consider that _guarantees_ hard compliance with the knowledge . See Appendix C for the full experimental setup.

Table 4 lists the results for DPL paired with different mitigation strategies. In order to get a sense of the model's ability to acquire good concepts, we also include two baselines: CBM-AUC introduced by

    &  &  &  \\   & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\)(\(\)) \\   & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\)(\(\)) \\   & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) & \(F_{1}\) (\(\)) \\   & \(68.1 6.7\) & \(0.0 0.0\) & \(99.5 0.2\) & \(0.0 0.1\) & \(67.4 0.1\) & \(0.9 0.0\) \\   & \(100.0 0.0\) & \(37.6 0.2\) & \(100.0 0.0\) & \(76.1 11.7\) & \(98.1 0.5\) & \(78.1 0.4\) \\   & \(100.0 0.0\) & \(99.8 0.1\) & \(100.0 0.0\) & \(99.8 0.1\) & \(98.3 0.2\) & \(98.3 0.2\) \\   

Table 3: **Q2: Impact of mitigation strategies.** We report the \(F_{1}\)-score on the labels (\(\)) and concepts (\(\)). All tested methods incorporate DIS and are averaged over \(10\) runs. **Top**: NeSy methods combined with r, C, H on MNIST-EvenOdd. **Bottom**: evaluation on single tasks MTL on MNIST-AddMul.

Sawada and Nakamura  for this task, which learns jointly the concepts together with unsupervised ones, and processes both of them with a linear layer to obtain the labels; c-only, in which the concept extractor is trained with full concept supervision and frozen, and DPL is only used to perform inference. This avoids any interference between label and concept supervision. The \(F_{1}\)-scores are computed over the test set and averaged over all \(4\) labels and all \(21\) concepts. On the right, we also report the concept confusion matrices (CMs) for the training set: each row corresponds to a ground-truth concept _vector_\(\), and each column to a predicted concept _vector_\(\).

Overall, the results show that, unless supplied with concept supervision, DPL optimizes for label accuracy (\(F_{1}\)(Y)) by leveraging low quality concepts (\(F_{1}\)(C)). This occurs even when it is paired with a Shannon entropy penalty \(\). This is especially evident in the CMs, which show that, for certain labels, all ground-truth concepts \(\) are mapped a single \(\), not unlike what happens in our first experiment for entangled models. Conversely, the concept quality of DPL substantially improves when concept supervision is available, at the cost of a slight degradation in prediction accuracy. The CMs back up this observation as the learned concepts tend to align much closer to the diagonal. Only for the turn_left label concept supervision fails to prevent collapse. This occurs because annotations for the corresponding concepts are poor: for a large set of examples, the concepts necessary to predict turn_left \(=1\) are annotated as negatives, complicating learning. In practice, this means that all variants of DPL predict those concepts as negative, hindering concept quality.

## 7 Related Work

Shortcuts in ML.State-of-the-art ML predictors often achieve high performance by exploiting spurious correlations - or "shortcuts" - in the training data . Well known examples include watermarks , background pixels [58; 59], and textual meta-data in X-ray scans . Like RSs, regular shortcuts can compromise the classifier's reliability and out-of-distribution generalization and are hard to identify based on accuracy alone. Proposed solutions include dense annotations , out-of-domain data , and interaction with annotators . Shortcuts are often the result of confounding resulting from, _e.g._, selective sampling. RSs may also arise due to confounding, as is the case in MNIST-Addition (cf. Example 1), however - as discussed in Section 5, data is not the only factor underlying RSs. For instance, in XOR (cf. Example 3) RSs arise despite exhaustive data.

**Reasoning shortcuts.** The issue of RSs has so far been mostly neglected in the NeSy literature, and few remedies have been introduced but never theoretically motivated. Stammer et al.  have investigated shortcuts affecting NeSy architectures, but consider only _input-level_ shortcuts that occur even if concepts are high-quality and fix by injecting additional knowledge in the model. In contrast, we focus on RSs that impact the quality of learned concepts. Marconato et al.  introduce the concept of RSs in the context of NeSy but for continual learning, and proposes a combination of concept supervision and concept-level rehearsal to address them, without delving into a theoretical justification. Li et al.  propose a minimax objective that ensures the concepts learned by the model satisfy \(\). Like the entropy regularizer  we addressed in Section 5, this strategy ends up spreading probability across all concepts that satisfy the knowledge, including those that have unintended semantics. As such, it does not directly address RSs.

**Neuro-symbolic integration.** While RSs affect a number of NeSy predictors, NeSy encompasses a heterogeneous family of architectures integrating learning and reasoning [1; 2]. We conjecture that

    &  \\   & F1-mean (Y) & F1-mean(C) \\ CRM-AUC & \(70.8 0.1\) & \(62.1 0.1\) \\ c-only & \(64.8 0.2\) & \(60.3 0.1\) \\ DPL & \(71.4 0.1\) & \(39.4 6.2\) \\ DPL+H & \(\) & \(48.1 0.3\) \\ DPL+C & \(68.2 0.2\) & \(60.5 0.1\) \\ DPL+C+H & \(68.3 0.3\) & \(\) \\   

Table 4: **Q3. Left: Means and std. deviations over \(10\) runs for DPL paired with different mitigations. Right: Confusion matrices on the training set for DPL alone (_top_) and paired with mitigation strategies (_bottom_) for {move_forward,stop,turn_left,turn_right} concept vectors.**RSs do transfer to _all_ NeSy approaches that do not specifically address the factors we identified, but an in-depth analysis of RSs in NeSy is beyond the scope of this paper. In a concurrent work, Wang et al.  derive the sample complexity of NeSy predictors when no RSs are in place. It is unclear how these bounds may change when RSs are present.

**Relation to disentanglement.** Recovering the latent variables \(\) has been the center of several works in representation learning [65; 35; 66]. Among these, achieving identifiability of the latent components has been studied in non-linear independent component analysis [67; 68; 69] and recently in Causal Representation Learning [46; 70; 71; 72]. In this respect, several overlaps exist with the natural mitigation strategies that we investigated: (i) multi-task learning has been shown to increase disentanglement  and provably leads to identifiability [74; 75], and (ii) latent variables supervision also largely increases the amount of disentanglement [76; 77; 47]. Our work is the witness that the intersection with _disentanglement_ literature is beneficial for learning the intended concepts in NeSy and, vice-versa, that knowledge-guided learning can be a new way of acquiring identifiable representations, by avoiding RSs.

## 8 Conclusion

In this work, we provide the first in-depth analysis of RS affecting NeSy predictors. Our analysis highlights four key causes of RS and suggests several mitigation strategies, which we analyze both theoretically and empirically. Our experiments indicate that RSs do naturally appear in both synthetic and real-world NeSy prediction tasks, and that the effectiveness of mitigation strategies is model and task dependent, and that a general recipe for avoiding RSs is currently missing.

We foresee that reasoning shortcuts extend beyond the current scope of NeSy predictors with known prior knowledge. This includes approaches that learn jointly both the concepts and the knowledge, like ROAP  and DSL , as well as fully neural models, like CBMs  and their variants [39; 81], which are also likely to be affected by RSs when concept supervision is limited. We plan to investigate further this direction in the near future.

Ultimately, this work aims at jumpstarting research on the analysis and mitigation of RSs, with the hope of leading to more trustworthy and explainable NeSy architectures.

**Broader impact.** Our work brings the subtle but critical issue of RSs to the spotlight, and highlights benefits and limitations of a variety of mitigation strategies, but it is otherwise fundamental research that has no direct societal impact.