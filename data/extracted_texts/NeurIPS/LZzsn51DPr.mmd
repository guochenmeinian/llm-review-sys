# Echoes Beyond Points: Unleashing the Power of Raw Radar Data in Multi-modality Fusion

Yang Liu CASIA Feng Wang TuSimple Naiyan Wang TuSimple Zhaoxiang Zhang MAIS of CASIA & UCAS, HKISI_CAS

{liuyang2022, zhaoxiang.zhang}@ia.ac.cn {feng.wff, winsty}@gmail.com

###### Abstract

Radar is ubiquitous in autonomous driving systems due to its low cost and good adaptability to bad weather. Nevertheless, the radar detection performance is usually inferior because its point cloud is sparse and not accurate due to the poor azimuth and elevation resolution. Moreover, point cloud generation algorithms already drop weak signals to reduce the false targets which may be suboptimal for the use of deep fusion. In this paper, we propose a novel method named EchoFusion to skip the existing radar signal processing pipeline and then incorporate the radar raw data with other sensors. Specifically, we first generate the Bird's Eye View (BEV) queries and then take corresponding spectrum features from radar to fuse with other sensors. By this approach, our method could utilize both rich and lossless distance and speed clues from radar echoes and rich semantic clues from images, making our method surpass all existing methods on the RADIal dataset, and approach the performance of LiDAR. The code will be released on https://github.com/tusen-ai/EchoFusion.

## 1 Introduction

Robust and accurate perception is a long-standing challenge in Autonomous Driving Systems (ADS). The full perception system usually relies on multiple sensor fusion including camera, LiDAR, and radar. Camera provides rich semantic cues of objects and excellent resolution, while LiDAR is capable of capturing highly accurate spatial information. However, neither camera nor LiDAR can directly measure speed or survive in adverse weather conditions, such as fog, sandstorms, and snowstorms . Although radar can overcome the aforementioned problems, even the point cloud from the latest 4D millimeter wave (mmWave) radars suffers from severe sparsity and low angular resolution, which makes it hard to discriminate objects and backgrounds solely. Fortunately, radar and camera are highly complementary to each other. Fusing these two types of sensors becomes a promising solution.

Radar data are usually represented as point clouds. Technically, the radar point cloud stems from raw Analog-Digital-Converter (ADC) data received by antennas. Range, speed, and angle of arrival (AoA) information of perceivable objects can be explicitly extracted in the frequency domain by consecutively applying FFT along corresponding dimensions. Among these steps, side lobe suppression and constant false alarm rate (CFAR) detector  are usually adopted to reduce the noise and false alarms. As a result, all the derived points are equipped with spatial coordinates, speed, and reflection intensity. Though these noise suppression operations can significantly reduce the data size and further computation costs, the resulting radar point cloud is extremely sparse, as shown in Figure 1. Even worse, quite a lot of useful information is lost during CFAR, which is adverse to the accurate perception of environments and subsequent fusion with other sensors . Raw data serves as apotential solution to alleviate these problems. However, how to efficiently utilize it, especially fusing with other sensors remains an open problem.

Recently, Birds-Eye-View (BEV) based methods are prevalent for driving scenario perception. It integrates features from multiple views and multiple modalities into a unified 3D representation space [4; 17; 24]. Among the varieties of BEV-based methods, PolarFormer  divides the 3D space in polar coordinates, which matches the format of raw radar data better. As such, we chose PolarFormer as our baseline model.

In this work, we present EchoFusion, a method designated for fusing raw radar data with images. We observe that a set of 3D positions exactly corresponds to one column in the image and one row in raw radar data. Thereby we propose a novel polar-aligned attention (PAA) technique to efficiently fuse features from these two different modalities. With row-wise cross-attention on radar data and column-wise attention on image, PAA could precisely aggregate essential features from both modalities while maintaining simple and efficient implementation. Finally, a polar BEV decoder refines object queries for accurate bounding box predictions.

The contribution of our paper is three-folds:

1. We are the first to fuse raw radar data with images in BEV space. Specifically, we propose a novel Polar-Aligned Attention module to guide the network effectively learn radar and image features.
2. We relabel the RADIal dataset  with accurate 3D bounding box annotations. The new annotations is published with our codebase together.
3. Extensive experiments show that our method has outperformed all the existing methods and achieved promising BEV detection performance, even approaching the LiDAR-based method.

## 2 Related Work

### Deep Learning on Radar

#### 2.1.1 Radar Data Representation and Datasets

Traditional radar usually only outputs sparse points [2; 5; 42] for use. However, such points are subjective to heavy signal processing methods to reduce noisy observation and alleviate bandwidth limitation. Nevertheless, such hand-crafted pipelines make the subsequent fusion with other sensors intractable. Consequently, some attempts have been made to provide upstream data, such as range-azimuth-Doppler (RAD) tensor [34; 54], range-azimuth (RA) maps , range-Doppler (RD) spectrums  or even raw Analog Digital Converter (ADC) data [19; 29]. Note that the RAD, RA, and RD tensors are generated from ADC data using several times (almost) lossless FFT. Thus we call all these four types of data "radar raw data" in the sequel.

Since our motivation is to deeply fuse with other sensors, we only consider the datasets providing data beyond point cloud. Among them, both RADIal , Radatron  and KRadar  provide such data. But Radatron only covers a single scenario while lacking camera calibration, LiDAR modality,

Figure 1: Illustration of sparsity and false alarms of the radar point cloud. The red points and the blue points correspond to data from radar and LiDAR, respectively. We visualize the predicted bounding boxes of different variants of our proposed method. red denotes ground-truth, purple denotes predictions from image only modality, green denotes predictions from image and radar range-time data fusion, and blue denotes predictions from image and radar point cloud fusion. Best viewed on screen.

and height annotation, while KRadar only provides radar RA map and PCD formats. Taking the factors mentioned above into account, we carry out our experiments mainly on the RADIal dataset, and provide additional results on a subset of KRadar dataset. However, the annotations of RADIal only contain object-level points. For a more comprehensive and objective evaluation, we further annotate the bounding box size and heading of each object based on dense LiDAR points. The annotations can be found in our codebase.

#### 2.1.2 Object Detection on Radar Data

Traditional radar object detection methods that utilize point cloud face challenges of super sparsity due to the low angular resolution. Practical treatments include occupancy grid mapping  and separate processing for static and dynamic objects . 4D radar technology improves the azimuth resolution and provides additional elevation resolution, which enables the combination of static and dynamic object detection in a single network, using either PointPillars  or tailor-made self-attention modules [1; 51].

The pre-CFAR(Constant False Alarm Rate Detector) data provides rich information of both targets and backgrounds. RAD tensor is fully decoded but brings high demand of storage and computation. Hence, Zhang et al.  takes the Doppler dimension as channels and Major et al.  projects RAD tensor to multiple 2D views. RTCNet  divides RAD tensor into small cubes and applies 3D CNN to reduce computation burden. Besides, [39; 55] take complex RD spectrum as input and apply neural networks to automatically extract spatial information. Networks such as RODNet  adopt RA maps for detection, which avoid false alarms caused by extended Doppler profile. Despite the aforementioned research works, the utilization of ADC radar data has recently gained increasing attention within the community [7; 52]. However, their results remain unsatisfactory.

The fusion of different type of sensors provides complementary cues, leading to more robust performance. At input level, radar point clouds are usually projected as pseudo-images and concatenated with camera images [3; 33]. At Region of Interest (RoI) level, some approaches [31; 32] consecutively refine the RoIs by radar and other modalities, while others [12; 14] unify RoIs generated independently by different sensors. At feature level, [50; 56] integrate feature maps generated from different modalities, while [11; 13] use RoIs to crop and merge features across modalities. To the best of our knowledge, we are the first to deeply fuse radar using raw data with other modalities in a unified BEV perspective.

### BEV 3D Object Detector

BEV object detection bursts out a rising wave of research due to its vast success in 3D perception tasks. One thread is to adopt transformations to project 2D image features to 3D BEV space for further detection, such as OFT  and LSS . Another thread applies initialized BEV queries  or object queries [22; 23] to iteratively and automatically sample features from multi-view images. Based on these advanced techniques, BEVFusion  explores the advantages of BEV representation in multi-sensor fusion and achieves impressive performances. Despite that, how to make full use of other information sources also attracts the interest of researchers. BEVFormer  and its variant  utilize temporal information to enhance detection capability, while BEVStereo  and STS

   Dataset & Data & Other Sensors & Scenarios & Annotations & Classes & Size \\  Astyx  & PC & CL & SH & 3D & 7 & 500 \\ View-of-Delft  & PC & \(C_{s}\)LO & U & 3D,T & 13 & 8693 \\ RADIal  & ADC,RA,PC & CLO & USH & \(P_{o}\),M & 1 & 8252 \\ TJ4DRadSet  & PC & CLO & U & 3D,T & 5 & 7757 \\ Radatron  & ADC,RA & \(C_{s}\) & U & BEV & 1 & 16K \\ KRadar  & RA, PC & CLO & USH & 3D,T & 5 & 35K \\   

Table 1: 4D Radar Datasets Comparison. Data: PC, RA, ADC denote to point cloud, range azimuth map and raw ADC data; Sensors: C, \(C_{s}\), L, O denote to camera, stereo camera, LiDAR, and odometer; Scenarios: U, S, H denote to urban (city), suburban, highway; Annotations: 3D, BEV, T, \(P_{o}\), M denote to 3D bounding box, BEV bounding box, track ID, object-level point and segmentation mask; Classes and Size denote the number of classes and the number of annotated frames of each dataset.

 explore how the estimated depth can benefit BEV-based detection. Besides, PolarFormer  introduces the polar grid setting and proves its effectiveness in environment perception, which is closely related to our work.

## 3 Preliminary

In the automotive industry, multiple-input, multiple-output (MIMO) frequency-modulated continuous-wave (FMCW) radars are adopted  to balance cost, size, and performance. Each transmitting antenna (Tx) transmits a sequence of FMCW waveforms, also named chirps, which will be reflected by objects and returned to the receiving antennas (Rx). Then the echo signal will be first mixed with the corresponding emitted signal and then passed through a low pass filter, so as to obtain the intermediate frequency (IF) signal. The discretely sampled IF signal, which includes the phase difference of emitted and returned signal, is called **ADC data**. The ADC data has three dimensions, fast time, slow time, and channel, which have physical correspondence with range, speed, and angle respectively.

Because of the continuous-wave nature, the IF signal of a chirp contains a frequency shift caused by the time delay of traveling between radar and objects. Then the radial distance of obstacles can be extracted by applying Fast Fourier Transform (FFT) on each chirp, i.e. fast-time dimension. This FFT operation is also called range-FFT. The resulting complex signal is denoted as **range-time (RT) data**. Next, a second FFT (Doppler FFT) along different chirps, i.e. slow-time dimension, is conducted to extract the phase shift between two chirps caused by the motion of targets. The derived data is named **range-Doppler (RD) spectrum**. We denote \(N_{Tx}\) and \(N_{Rx}\) respectively as the number of transmitting and receiving antennas. And each grid on the RD spectrum has \(N_{Tx} N_{Rx}\) channels. The small distance between adjacent virtual antennas leads to phase shifts among different antennas, which is dependent on the AoA. Thus a third FFT (angle-FFT) can be applied along the channel axis to unwrap AoA. The AoA can be further decoded to azimuth and elevation based on the radar antenna calibration. The final 4D tensor is referred to as the **range-elevation-azimuth-Doppler (READ) tensor**.

If point cloud is desired, the RD spectrum will be first processed by the CFAR algorithm to filter out peaks as candidate points. Then the angle-FFT will only be executed on these points. Final **radar point cloud (PCD)** consists of 3D coordinates, speed, and reflective intensity. In traditional radar signal processing, there is another data format named **range-azimuth (RA) map**, which is obtained by decoding the azimuth of a single elevation on the RD spectrum and compressing the Doppler dimension to one channel. For comprehensive analyses, we include this modality in our experiments as well. The data formats mentioned above are illustrated in Figure 2, except for READ tensor because it is difficult to visualize.

Figure 2: Radar data formats used in this paper. All the axes that indicate spatial location are labeled with units. For ADC data, the vertical unit is the sampling cycle. And for ADC data, Range-Time data, the horizontal unit is the chirp cycle. Due to Doppler domain multiplexing (DDM), the real speed is periodic on the Doppler dimension, which is hard to label (More details of DDM can be found in the appendix). We use the index number of the Doppler dimension as horizontal unit instead in Range-Doppler and CFAR result.

## 4 Methods

In this section, we will introduce our network design. Firstly we will give a brief introduction of the whole pipeline in Section 4.1. Then our proposed Polar-Aligned Attention module will be elaborated in Section 4.2 and Section 4.3. Finally, the detection head and loss functions are introduced in Section 4.4.

### Overall Structure

The overall architecture of our model is shown in Figure 3. First, we use separate backbones and FPNs  to extract multi-level features from RT radar maps and images. Then the polar queries aggregate camera features and radar features by cross-attention. Finally, the multi-scale polar BEV map will be passed through a transformer-based decoder with object queries. The final score and bounding box predictions can be obtained by decoding the output object queries. The core of our EchoFusion is how to fuse the camera and radar slow-time data. We fuse them using a novel strategy named Polar Aligned Attention (PA-Attention). In the next two subsections, we will explain this strategy in detail.

### Column-wise Image Fusion

After obtaining image and radar features from corresponding encoders, we first initialize the \(l\)-th level polar queries \(}^{R_{l} A_{l} d}\) uniformly in polar BEV space. Here, \(R_{l}\), \(A_{l}\), and \(d\) respectively denote the shape of range, azimuth, and channel dimensions. For brevity, we omit \(l\) in the following two subsections. Given a query \(q^{d}\) located at \((r_{bev},_{bev})\), it corresponds to a pillar centered at \((r_{bev},_{bev})\) with infinite height. Taking the coordinate system defined in RADIial , \(x_{R}\), \(y_{R}\), and \(z_{R}\) axes respectively points to the front, left, and upright, while subscript \(R\) means radar coordinate. Similarly, the camera coordinates are denoted as \((x_{C},y_{C},z_{C})\), which points to the right, down, and front, respectively. Thus we have correspondence in Cartesian coordinate system as follows:

\[x_{R}=r_{bev}(_{bev}), y_{R}=r_{bev}(_{bev }).\] (1)

Using extrinsic matrix and intrinsic matrix, we have the following formulation:

\[-u_{0}}{f_{x}}=}{z_{C}},(x_{C }\\ y_{C}\\ z_{C})=R(x_{R}\\ y_{R}\\ z_{R})+T,T=(d_{1}\\ d_{2}\\ d_{3}),\] (2)

where \(u_{0}\), \(f_{x}\) are principal point offset and focal length on \(x\)-axis in the intrinsic matrix, \(x_{I}\) is the image column index that corresponds to the point \((x_{R},y_{R},z_{R})\), while \(R\) and \(T\) are rotation matrix and translation vector of the calibration matrix between camera and radar, respectively. Under mild distortion conditions and normally adopted sensor setting, \(x_{I}\) is determined by pillar position and calibration parameters as follows:

\[x_{I} u_{0}+f_{x}_{bev}+d_{1}}{r_{bev}_{ bev}+d_{3}}.\] (3)

Figure 3: Overall pipeline of the proposed EchoFusion. H and W are the height and weight of the image, while R, A, and T are the bin number of range, azimuth, and Doppler dimension, respectively. The Coord. Mapping in the figure is explained in section 4.2.

Limited by the number of pages, we leave the detailed proof in the appendix.

However, the height is not specified for a query, while for a pixel \((x_{I},y_{I})\) on the monocular image, its depth is unknown as well. Thus we cannot precisely associate a query to the image feature, but we can correspond queries of the same azimuth to the same column of images. Enlightened by this observation, we use a versatile cross-attention mechanism to flexibly aggregate required features from images. Namely, all the queries with the same azimuth form the query matrix, while all the features in the corresponding column in the image form the key and value matrix. Formally, these two steps can be expressed as follows:

\[ F_{I}(x_{I},)&=([f_{I}(x_{I},y_{I})])^{H_{l}  d}, y_{I}[0,H_{l}-1],\\ (r_{bev},_{bev})&= (q(r_{bev},_{bev}),F_{I}(x_{I},),F_{I}(x_{I},)),\] (4)

where \(f_{I}(x_{I},y_{I})\) is the image feature located at \(x_{I},y_{I}\), while details of cross attention function \(\) can be found in .

### Range-wise Radar Fusion

We denote updated BEV queries as \(}_{}^{R_{l} A_{l} d}\). After that, it will be used to sample features from radar. Since the radar feature map shares the same range partition with the BEV queries, the updated query \((r_{bev},_{bev})\) matches the same range index of the radar feature, i.e. \(r_{R}=r_{bev}\). To aggregate all the features from all chirps in one frame, we perform cross attention for a certain query \(\) with the row \(r_{R}\) in RT map as key and value. Formally,

\[ F_{R}(r_{R},)&=([f_{R}(r_{R},t_{R})])^{R_{l}  d}, t_{R}[0,T_{l}-1],\\ (r_{bev},_{bev})&= ((r_{bev},_{bev}),F_{R} (r_{R},),F_{R}(r_{R},)),\] (5)

where \(R_{l}\) and \(T_{l}\) are the height and width of the radar feature map, and \(f_{R}(r_{R},t_{R})^{d}\) is the radar feature of position \((r_{R},t_{R})\).

Note that though we don't explicitly decode AoA as in RA map, the AoA has been implicitly encoded in the phase difference of the response of different virtual antenna. Consequently, it can be learned from the features in RT map indirectly.

### Head and Loss

We follow polar BEV decoders  to decode multi-level BEV features and make predictions from object queries. Since we don't have velocity annotation on this dataset, we remove the corresponding branch. The regression targets include (\(d_{}\), \(d_{}\), \(d_{z}\), \(l\), \(w\), \(h\), \((_{ori}-)\), \((_{ori}-)\)), where \(d_{}\), \(d_{}\), \(d_{z}\) are relative offset to the reference point (\(\), \(\), \(z\)), \(l\), \(w\), \(h\), \(_{ori}\) are length, width, height, and orientation of bounding box. The classification and regression tasks are respectively supervised by Focal loss  and L1 loss.

## 5 Experiments

In this section, we conduct thorough experiments to validate the effectiveness of our approach. We first introduce the dataset and metrics used in our experiments, then followed by comparisons with other state-of-the-art methods. Lastly, we ablate some designs and potential input format of raw radar data in our method, and discuss some limitations.

### Experimental Settings

**The RADIaal Dataset** provides 8252 annotated frames with synchronized multi-sensor data. Training, validation, and test sets contain 6231, 986, and 1035 frames, respectively. Each object is annotated with a 3D center coordinate of the visible face of the vehicle. The size and orientation of the objects are pre-defined templates, i.e. they are the same for all objects. In the evaluation, precisions and recalls under ten thresholds are calculated. The average precision (AP), average recall (AR), and F1 score are obtained through the ten precisions and ten recalls as the evaluation metric. Range error (RE) and azimuth error (AE) are also reported to evaluate localization accuracy. The annotation method and metrics are quite different to those in common autonomous driving datasets[2; 45]. Nonetheless, to compare with state-of-the-art methods, we still list our results using the metrics defined in RADIal  as a reference.

**Re-annotation and more metrics.** Based on the provided LiDAR point cloud, we have refined the cluster-center-based annotation to 3D bounding box form, which includes center position, scale, and heading in 3D space. Capitalizing on these new annotations, we are able to apply metrics that are vastly used in 3D object detection tasks. Specifically, we take AP defined in Waymo dataset  as our main metrics. Since the LiDAR used in RADIal is only 16-beam, we cannot label accurate height for the objects. So we mainly utilize BEV IoU for evaluation. We use the threshold of 0.7 for normal IoU and longitudinal error-tolerant (LET) IoU computation . Considering the spatial distribution of ground-truth, the distance evaluation is further broken down into two categories: 0 to 50 meters, 50 to 100 meters. For a more comprehensive evaluation, we also introduce the error metrics defined in nuScenes dataset , including ATE, ASE, and AOE for measuring errors of translation, scale, and orientation.

**The KRadar Dataset** is a recently published 4D radar dataset, which contains 58 sequences, i.e. 35K frames of synchronized multi-modality data. However, we have limited time to obtain the full data via shipping, so we only provide results on the sub-dataset stored on Google Drive, which consists of 20 sequences. This so-called KRadar-20 dataset includes 6493 training samples and 6515 test samples, of which 50 % are nighttime data. The training samples are named trainval split, it is further split into train split of 5190 samples and val split of 1303 samples for ablation. Metrics follow KITTI  protocols with 40 recall positions. Different from RADIal, it only contains radar formats of range-azimuth map (RA map) and radar point cloud. But the RA map of KRadar contains an extra height dimension, and it enables us to test 3D prediction ability with 4D radar data.

**Radar Formats.** As introduced in Section 3, there are multiple formats of radar raw data, such as ADC, RT, RD, READ. These formats can be converted sequentially by FFT, which is a linear operation that can be absorbed into the linear layers. So from the perspective of neural network, these formats are equivalent. However, our proposed Polar-Aligned Attention requires a range dimension for indexing key and value in cross-attention, so we choose RT as a representative in the experiments. We also conduct experiments on RA to investigate the necessity of explicit azimuth.

**Implementation Details.** All our experiments are carried out on eight 3090 GPUs with the same batch size of 8. For the image backbone, we adopt ResNet-50 with pre-trained weights provided by BEVFormer . For LiDARAR and radar point cloud branch, we borrow the voxel encoder from PointPillar  and use ResNet-50 with modified strides as the backbone. Other representations of radar data share similar modified ResNet-50 as the backbone, but with an extra 1 \(\) 1 convolution at the beginning for channel number alignment. To prevent overfitting, the image-only variant is trained for only 12 epochs while others are trained for 24 epochs. AdamW  is adopted as the optimizer and a learning rate of 5e-5 is shared for all models. Due to the limited size of the dataset, we find that the results are unstable across different runs. To make a fair comparison, each experiment is repeated three times. In modality ablation, we show the mean and variance of each experiment result.

   Methods & Modality & AP(\(\%\))\(\) & AR(\(\%\))\(\) & F1(\(\%\))\(\) & RE(m)\(\) & AE(\({}^{}\))\(\) \\  FFTRadNet  & RD & 96.84 & 82.18 & 88.91 & **0.11** & 0.17 \\ T-FFTRadNet  & ADC & 89.60 & 89.50 & 89.50 & 0.15 & 0.12 \\ ADCNet  & ADC & 95.00 & 89.00 & 91.90 & 0.13 & **0.11** \\ CMS  & RD\&C & 96.90 & 83.50 & 89.70 & 0.45 & n/a \\ EchoFusion & RT\&C & **96.95** & **93.43** & **95.16** & 0.12 & 0.18 \\   

Table 2: Detection performances on RADIal Dataset test split with original protocol . C, RD, and RT respectively refer to camera, range-Doppler spectrum, and range-time data. Our method achieves the best performance in both average precision (AP), average recall (AR), and F1-score (F1). The best result of each metric is in **bold**.

### Comparison to State-of-the-art Methods

#### 5.2.1 Results on RADIial Dataset

We first compare our method with state-of-the-art detectors on AP, AR, range error, and azimuth error defined in RADIial , with original object-level point annotations1. The results are illustrated in Table 2. Our method significantly improves AR and F1 performance and achieves a remarkable improvement over all existing detectors including CMS , which also integrates both radar raw data and camera data. Note that RE and AE are not quite informative because these two metrics are calculated on all the recalled objects of a method, which is unfair to high-recall models like ours.

To evaluate in a more rigorous and informative way, we also conduct experiments on the refined annotations and vastly adopted metrics in 3D object detection. FFTRadNet  is lifted up with additional branches to predict the scale and orientation of targets. We can only compare with this FFTRadNet variant since codes of other algorithms [7; 10; 52] are not available yet. The results in Table 3 show that the proposed method outperforms FFTRadNet by a large margin. This significant improvement reveals the huge benefit of unleashing the power of radar data in multi-modality fusion.

#### 5.2.2 Results on KRadar Dataset

The baseline on KRadar dataset is the official RTNH , which requires radar point cloud. Here, the proposed EchoFusion is input with image and RA map. To align with the baseline, we take the radius and azimuth range respectively as  m and [-20, 20] degrees by the limit of camera field of view. The results are shown in Table 4. With RA map and image fusion, our EchoFusion improves over 10 points at each metric, except for BEV AP of IoU threshold 0.3. And since KRadar has higher vertical resolution and more accurate 3D groundtruth than RADIial, our method achieves more considerable improvements in 3D metrics.

### Ablation Studies

We ablate the input format to the radar feature extraction network in Table 5. It shows that the decomposition of complex input can improve overall detection performance, especially for long-range perception. The magnitude and phase representation encodes a non-linear transformation of the complex representation, which may relate to the final prediction target better. We also tried to remove the pre-training of the image branch. The result drops significantly, which confirms that in this relatively small dataset, a good pre-training is crucial for good performance.

    & &  &  \\  Training Set & Method & AP@0.3 & AP@0.5 & AP@0.7 & AP@0.3 & AP@0.5 & AP@0.7 \\  KRadar & RTNH & 58.04 & 42.60 & 10.69 & 49.65 & 17.87 & 0.45 \\ KRadar-20-trainval & RTNH & 61.38 & 46.47 & 10.47 & 53.05 & 17.98 & 3.03 \\ KRadar-20-trainval & EchoFusion & **69.95** & **57.28** & **33.07** & **68.35** & **43.87** & **14.00** \\   

Table 4: BEV detection performances on KRadar Dataset test split with KITTI protocol. 0.3, 0.5, and 0.7 are IoU thresholds. The inputs of EchoFusion are image and range-azimuth map. The best result of each metric is in **bold**.

    & &  &  \\  Methods & Modality & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\  FFTRadNet3D & RD & 57.26 & 68.66 & 52.28 & 62.81 & 75.55 & 57.98 \\ EchoFusion & RT\&C & **84.92** & **87.56** & **91.06** & **88.86** & **92.81** & **94.81** \\   

Table 3: BEV detection performances on RADIial Dataset test split with refined 3D ground-truth bounding box. C, RD, and RT respectively refer to camera, range-Doppler spectrum, and range-time data. The best result of each metric is in bold. Our method exceeds FFTRadNet by a large margin.

### Comparison of Different Modalities

To further study the effects of different modalities, we change the input format indicated in Figure 2 and corresponding backbones, and test with and without image modality. Table 6 presents the results, more results on KRadar can be found in our appendix, and our findings are listed as follows.

**Comparing single-modality results** It is not surprising that LiDAR ranks first while the camera is the worst in terms of AP. Though poor in depth estimation, the camera has excellent angular resolution, which is beneficial for LET-BEV-AP. It achieves comparable results with radar in LET-BEV-AP. Besides, without the guidance of image, the radar-only network gets better performance as more hand-crafted post-processing are involved. However, their metrics still lag far from those of LiDAR.

**Comparing multi-modality with single modality** The power of radar data is released by fusing with image. No matter in what data format, all the radar raw representations gain around 30 points improvement by fusing with images. By combining image and range-time data, we obtain BEV AP that is only 0.03 less than that of LiDAR only method. And the LET-BEV-AP and long-range detection ability are even better. We argue that it is mainly because images provide enough clues to decode the essential information from raw radar representation.

**Comparing multi-modality results** When integrating with images, the LiDAR-fused method still outperforms other radar-fused methods. In terms of different representations of radar data, both RT and RA outperform traditional cloud points, especially in the 50-100m range, in which the difference is as large as 4 points in AP. We owe this finding to the information loss and false alarm of radar point cloud as shown in Figure 1. The performance gap between RT and RA with camera modality is within the error bar, indicating that it is not necessary to explicitly solve the azimuth, nor necessary to permute the Doppler-angle dimensions as FFTRadNet  does.

### Discussion and Limitation

Firstly, it is worth noticing that although the overall performance of radar is improved by fusing with the camera, the performance in the short-range is lower than that in the long-range. We speculate the main reason is inaccurate annotation. The 16-beam LiDAR provided in RADIai is too sparse at

    &  &  &  \\  C & RT & RA & RP & L & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\  ✓ & & & & & 10.07\(\)1.10 & 21.68\(\)3.11 & 2.21\(\)0.59 & 56.92\(\)3.91 & **78.98\(\)1.41** & 36.20\(\)5.81 \\  & ✓ & & & & 48.26\(\)0.92 & 62.33\(\)4.00 & 45.05\(\)0.98 & 55.04\(\)1.25 & 68.56\(\)4.11 & 52.30\(\)1.82 \\  & & ✓ & & & 50.67\(\)1.03 & 65.05\(\)1.59 & 42.98\(\)0.51 & 58.53\(\)0.13 & 72.95\(\)0.78 & 51.41\(\)0.54 \\  & & ✓ & & & **53.55\(\)0.70** & **66.19\(\)1.72** & **47.41\(\)**1.06** & **62.59\(\)**1.14** & 76.45\(\)2.06 & **56.73\(\)1.33** \\  & & & & ✓ & 84.47\(\)0.15 & 86.92\(\)0.61 & 91.91\(\)0.13 & 86.07\(\)0.44 & 88.55\(\)0.79 & 93.17\(\)0.10 \\  ✓ & ✓ & & & & **84.92\(\)0.98** & 87.56\(\)1.58 & 91.06\(\)1.18 & 88.86\(\)0.62 & 92.81\(\)1.37 & 94.81\(\)0.52 \\ ✓ & & ✓ & & & 84.77\(\)0.65 & **87.93\(\)0.60** & **91.48\(\)0.28** & **89.54\(\)0.54** & **93.83\(\)0.56** & **94.87\(\)0.58** \\ ✓ & & & ✓ & & 82.35\(\)0.93 & 86.97\(\)1.01 & 86.39\(\)0.74 & 88.35\(\)0.78 & 93.07\(\)0.80 & 92.77\(\)0.69 \\ ✓ & & & & ✓ & & 86.35\(\)1.15 & **88.81\(\)1.40** & **94.25\(\)**1.68** & 88.44\(\)0.79 & 91.63\(\)1.34 & 95.31\(\)1.28 \\   

Table 6: BEV detection performances of different modality combinations of our algorithm on RADIai Dataset test split. Refined 3D bounding box annotations are applied. C, L, RD, RT, RA, RP respectively refer to the camera, LiDAR, range-Doppler spectrum, range-time data, range-azimuth map, and radar point cloud. The best result without LiDAR of each metric are in **bold**, and the results with LiDAR are on gray background.

    &  &  \\  Complex & Pretrain & Overall & 0 - 50m & 50 - 100m & Overall & 0 - 50m & 50 - 100m \\  MP & w & **84.92** & **87.56** & **91.06** & **88.86** & **92.81** & **94.81** \\ IQ & w & 82.64 & 87.09 & 87.81 & 88.06 & 92.25 & 93.24 \\ MP & w/o & 74.34 & 81.26 & 78.71 & 78.90 & 85.44 & 84.28 \\   

Table 5: Ablation studies on our EchoFusion. Complex means interpreting input features as real and imaginary (IQ) or magnitude and phase (MP). Pretrain means whether to use the pre-trained image backbone. The best result of each metric is in **bold**.

a farther distance for annotators to give an accurate size for the long-range objects. In such cases, they are required to assign a template with a fixed size to these objects. These annotations make the problem much easier for farther distances, which leads to higher performance than near distances.

Secondly, though the radial speed label is included in the original annotations of RADIaI, radial speed is not quite useful for downstream modules like planning since they need accurate longitudinal and lateral velocity to predict whether the vehicles will interfere with the ego vehicle. However, the synchronized frames are not consecutive in this dataset, it is hard to label velocities for the objects. Considering these factors, velocity prediction is not included in our task.

Finally, as shown in Table 7, our method still lags far from LiDAR-based methods. The primary limitation can be attributed to imperfect z localization, resulting in relatively high Average Localization Error (ATE). The inferior performance is mainly due to coarse LiDAR annotation and inferior radar elevation resolution. The low elevation distinguishing power of LiDAR makes annotation error much worse at long range. As a result, both LiDAR-based and radar-based methods experience a significant drop in 3D AP within the 50-100m interval. But when equipped with high elevation resolution and accurate 3D groundtruth provided by KRadar, our method shows excellent performance in 3D metrics, as shown in Table 4. This emphasizes the pressing need for a raw radar dataset of higher quality that enables better exploration of radar data usage.

## 6 Conclusion and Future Work

In this work, we have proposed a novel method for radar raw data fusion with other sensors in a BEV space. Our proposed EchoFusion is concise and effective, which outperforms previous work by a significant margin. We are the first to demonstrate the potential of radar as a low-cost alternative for LiDAR in autonomous driving systems through thorough analyses and experiments.

This work is only a starting point to study how raw radar data can be exploited. However, many attempts are limited by the available datasets. We urge a large-scale and high-quality dataset. However, the acquisition of high-quality multi-modality data with accurate annotation needs great effort and deliberate design for clock synchronization and high storage demand. We will try to build the dataset to facilitate further research.

**Societal Impacts** Our method can be deployed in the autonomous driving system. Performance loss caused by improper usage may increase security risks.