# Fully Dynamic \(k\)-Clustering in \(\tilde{O}(k)\) Update Time

# Fully Dynamic \(k\)-Clustering in \((k)\) Update Time

Sayan Bhattacharya

University of Warwick

s.bhattacharya@warwick.ac.uk &Martin Costa

University of Warwick

martin.costa@warwick.ac.uk &Silvio Lattanzi

Google Research

silviol@google.com &Nikos Parotsidis

Google Research

nikosp@google.com

###### Abstract

We present a \(O(1)\)-approximate fully dynamic algorithm for the \(k\)-median and \(k\)-means problems on metric spaces with amortized update time \((k)\) and worst-case query time \((k^{2})\). We complement our theoretical analysis with the first in-depth experimental study for the dynamic \(k\)-median problem on general metrics, focusing on comparing our dynamic algorithm to the current state-of-the-art by Henzinger and Kale . Finally, we also provide a lower bound for dynamic \(k\)-median which shows that any \(O(1)\)-approximate algorithm with \(((k))\) query time must have \((k)\) amortized update time, even in the incremental setting.

## 1 Introduction

Clustering is a fundamental problem in unsupervised learning with several practical applications. In clustering, one is interested in partitioning elements into different groups (i.e. _clusters_), so that elements in the same group are more similar to each other than to elements in other groups. One of the most studied formulations of clustering is the metric clustering formulation. In this setting, elements are represented by points in a metric space, and the distances between points represent how similar the corresponding elements are (the closer elements are, the more similar they are). More formally, the input to our problem consists of a set of points \(U\) in a metric space with distance function \(d:U U_{ 0}\), a real number \(p 1\), and an integer \(k 1\). The goal is to compute a subset \(S U\) of size \(|S| k\), so as to minimize \((S):=_{x U}d(x,S)^{p}\), where \(d(x,S):=_{y S}d(x,y)\). We refer to the points in \(S\) as _centers_. We note that this captures well-known problems such as \(k\)-median clustering (when \(p=1\)) or \(k\)-means clustering (when \(p=2\)).

Due to this simple and elegant formulation, metric clustering has been extensively studied throughout the years, across a range of computational models [14; 22; 1; 12; 15; 2; 31; 11; 7]. In this paper, we focus on the _dynamic setting_, where the goal is to efficiently maintain a good clustering when the input data keeps changing over time. Because of its immediate real-world applications, the dynamic clustering problem has received a lot of attention from the machine learning community in recent years [16; 6; 10; 24; 18]. Below, we formally describe the model [13; 20; 3] considered in this paper.

At _preprocessing_, the algorithm receives the initial input \(U\). Subsequently, the input keeps changing by means of a sequence of _update_ operations, where each update inserts/deletes a point in \(U\). Throughout this sequence of updates, the algorithm needs to implicitly maintain a solution \(S^{*} U\) to the current input \((U,d)\). The algorithm has an approximation ratio of \( 1\) if and only if we always have \((S^{*})(U)\), where \((U):=_{S U,|S| k}(S)\) denotes the optimal objective. Finally, whenever one _queries_ the algorithm, it has to return the list of centers in \(S^{*}\). The performance of a dynamic algorithm is captured by its approximation ratio, its update time, and its query time. Let\(U_{}\) be the state of the input \(U\) at preprocessing. A dynamic algorithm has (amortized) _update time_\(O()\) if for any sequence of \(t 0\) updates, the total time it spends on preprocessing and handling the updates is \(O((t+|U_{}|))\). The time it takes to answer a query is called its _query time_.

Our Contributions.Our primary contribution is to design an algorithm for this problem with a near-optimal update time of \((k)\), without significantly compromising on the approximation ratio and query time.1 Interestingly, our algorithm can be easily extended to dynamic \(k\)-clustering for any constant \(p\) (see Appendix A). In the theorem below, we summarize our main result for dynamic \(k\)-median and \(k\)-means.

**Theorem 1.1**.: _There exists a dynamic algorithm that, with high probability, maintains a \(O(1)\)-approximate solution to the \(k\)-median and \(k\)-means problems for general metric spaces under point insertions and deletions with \((k)\) amortized update time and \((k^{2})\) worst-case query time._

It is important to note that in practice an algorithm often receives the updates in _batches_ of variable sizes , so it is common for there to be a significant number of updates between cluster requests. As a consequence, optimizing the update time is of primary importance in practical applications. For example, if the size of each batch of updates is \((k)\), then our amortized query time becomes \((k^{2}/k)=(k)\), because the algorithm has to answer a query after processing at least one batch. Thus, our dynamic algorithm has near-optimal update and query times when the updates arrive in moderately large batches.

In addition, it is possible to observe that any dynamic \(k\)-clustering algorithm must have \((k)\) query time, since the solution it needs to return while answering a query can consist of \(k\) centers. We also show a similar lower bound on the _update time_ of any constant approximate dynamic algorithm for this problem. This lower bound holds even in an _incremental_ setting, where we only allow for point-insertions in \(U\). We defer the proof of Theorem 1.2 to Appendix D.

**Theorem 1.2**.: _Any \(O(1)\)-approximate incremental algorithm for the \(k\)-median problem with \(((k))\) query time must have \((k)\) amortized update time._

Theorem 1.2 implies that the ultimate goal in this line of research is to get a \(O(1)\)-approximate dynamic \(k\)-clustering algorithm with \((k)\) update time and \((k)\) query time. Prior to this work, however, the state-of-the-art result for dynamic \(k\)-median (and \(k\)-means) was due to Henzinger and Kale , who obtained \(O(1)\)-approximation ratio, \((k^{2})\) update time and \((k^{2})\) query time. In this context, our result is a meaningful step toward obtaining an asymptotically optimal algorithm for the problem.

We supplement the above theorem with experiments that compare our algorithm with that of . To the best of our knowledge, this is the first work in the dynamic clustering literature with a detailed experimental evaluation of dynamic \(k\)-median algorithms for general metrics. Interestingly, we observe that experimentally our algorithm is significantly more efficient than previous work without impacting the quality of the solution.

Our Techniques.We first summarize the approach used in the previous state-of-the-art result. In , the authors used a static algorithm for computing a coreset of size \((k)\) as a black box to get a fully dynamic algorithm for maintaining a coreset of size \((k)\) for general metric spaces in worst-case update time \((k^{2})\). Their algorithm works by maintaining a balanced binary tree of depth \(O( n)\), where each leaf in the tree corresponds to a point in the metric space . Each internal node of the tree then takes the union of the (weighted) sets of points maintained by its two children and computes its coreset, which is then fed to its parent. They maintain this tree dynamically by taking all the leaves affected by an update and recomputing all the coresets at nodes contained in their leaf-to-root paths. Using state-of-the-art static coreset constructions, this update procedure takes time \((k^{2})\). Unfortunately, if one wants to ensure that the final output is a valid coreset of the metric space after each update, then there is no natural way to bypass having to recompute the leaf-to-root path after the deletion of a point that is contained in the final coreset. Hence, it is not at all clear how to modify this algorithm in order to reduce the update time to \((k)\).

We circumvent this bottleneck by taking a completely different approach compared to . Our algorithm instead follows from the dynamization of a \((nk)\) time static algorithm for \(k\)-median by Mettu and Plaxton , where \(n=|U|\). We refer to this static algorithm as the MP algorithm. Informally, the MP algorithm works by constructing a set of \(O((n/k))\)_layers_ by iteratively sampling random points at each layer, defining a clustering of the points in the layer that are 'close' to these samples, and removing these clusters from the layer. The algorithm then obtains a solution to the \(k\)-clustering problem by running a static algorithm for _weighted_\(k\)-median (defined in Section 2), on an instance of size \(O(k(n/k))\) defined by the clusterings at each layer. In order to dynamize this algorithm, we design a data structure that maintains \(O((n/k))\) layers that are analogous to the layers in the MP algorithm. By allowing for some'slack' in the way that these layers are defined, we ensure that they can be periodically reconstructed in a way that leads to good amortized update time, while only incurring a small loss in the approximation ratio. The clustering at each layer is obtained by random sampling every time it is reconstructed and is maintained by arbitrarily reassigning a point in a cluster as the new center when the current center is deleted. We obtain a solution to the \(k\)-clustering problem from this data structure in the same way as the (static) MP algorithm--by running a static algorithm for weighted \(k\)-median on an instance defined by the clusterings maintained at each layer.

## 2 Preliminaries

In our computational model, the algorithm has access to the distance \(d(x,y)\) for any \(x,y U\) in \(O(1)\) time2. Given any two sets \(X,S U\), define _the cost of \(X\) w.r.t. \(S\)_ as \((S,X):=_{x X}_{s S}d(x,s)\). In addition, let \((S)=(S,U)\). Next, define an _assignment_ to be a function \(:U U\), and say that \(\) assigns \(x U\) to \((x)\). We refer to an assignment \(\) with \(|(U)| m\) as an _\(m\)-assignment_. The cost of \(X U\) w.r.t. assignment \(\) is \((,X):=_{x X}d(x,(x))\). We denote \((,U)\) by \(()\). For any \( 1\), say that \(\) is \(\)-approximate if \(()(U)\), where \((U)\) is the cost of the optimal solution.

Given any subset \(U^{} U\) and \(x U^{}\), we denote by \(B_{U^{}}(x,r)\) the set \(\{y U^{}\,|\,d(x,y) r\}\), i.e. the closed ball in \(U^{}\) of radius \(r 0\) centered at \(x\). When it is clear from the context that \(U^{}=U\), we omit the subscript \(U^{}\) and simply write \(B(x,r)\). For \(X U^{} U\), we define \(B_{U^{}}(X,r)\) to be the union of the balls \(B_{U^{}}(x,r)\) for \(x X\).

**Definition 2.1**.: Given \(0<<1\) and subsets \(X U^{} U\), we define the following real numbers:

\[_{}(X,U^{}):=\{r 0\,|\,|B_{U^{}}(X,r)| |U^{}|\},_{}(U^{}):=\{_{}(Y,U^{})\,| \,Y U^{},\ |Y|=k\}.\]

In words, the real number \(_{}(X,U^{})\) is the smallest radius \(r\) such that the closed ball of radius \(r\) around \(X\) captures at least a \(\)-proportion of the points in \(U^{}\). The real number \(_{}(U^{})\) is then defined to be the smallest such radius \(_{}(X,U^{})\) over all subsets \(X U^{}\) of size \(k\). Note that \(_{}(X,U^{})\) and \(_{}(U^{})\) are both increasing as functions of \(\).

Finally, we will sometimes refer to the _weighted \(k\)-median problem_. An instance of this problem is given by a triple \((U,d,w)\), where \(w:U_{ 0}\) assigns a nonnegative _weight_ to every point \(x U\), in a metric space with distance function \(d\). Here, the goal is to compute a subset of at most \(k\) centers \(S U\), so as to minimize \(_{w}(S):=_{x U}w(u) d(x,S)\). We let \(_{w}(U):=_{S U|S| k}_{w}(S)\) denote the optimal objective value of this weighted \(k\)-median instance. We will analogously use the symbol \(_{w}(S,X):=_{x X}w(x) d(x,S)\) to denote the cost of a set of points \(X\) w.r.t. \(S\) and the weight function \(w\).

## 3 Our Algorithm

Throughout this section, we fix the following parameters: \(\), \(\), \(\), \(\) and \(k^{}\); where \( 1\) is a sufficiently large constant, \(\) is any constant in the interval \((0,1)\), \(>0\) is a sufficiently small constant, \(:=\), and \(k^{}:=\{k,(|U|)\}\).

### The Static Algorithm of 

Our starting point is the static algorithm of  for computing a \((k)\)-assignment \(:U U\), with \(S=(U)\), such that \(() O(1)(U)\). The relevant pseudocode appears in Algorithm 1 and Algorithm 2.

```
1:\(i 1\) and \(U_{1} U\)
2:while\(|U_{i}|> k^{}\)do
3:\((S_{i},C_{i},_{i},_{i})(U_{i})\)
4:\(U_{i+1} U_{i} C_{i}\)
5:\(i i+1\)
6:endwhile
7:\(t i\)
8:\(S_{t} U_{t}\), \(C_{t} S_{t}\) and \(_{t} 0\)
9: Assign each \(x C_{t}\) to itself (i.e., \(_{t}(x):=x S_{t}\))
10:\(S_{j[t]}S_{j}\)
11: Let \(:U S\) be the assignment such that for all \(j[t]\) and \(x C_{j}\), we have \((x)=_{j}(x)\)
12:return\((S,,t)\) ```

**Algorithm 1**\((U)\)

The algorithm runs for \(t\)_iterations_. Let \(U_{i} U\) denote the set of unassigned points at the start of iteration \(i[t-1]\) (initially, we have \(U_{1}=U\)). During iteration \(i\), the algorithm samples a set of \( k^{}\) points \(S_{i}\) as centers, uniformly at random from \(U_{i}\). It then identifies the smallest radius \(_{i}\) such that the balls of radius \(_{i}\) around \(S_{i}\) cover at least \(\)-fraction of the points in \(U_{i}\). Let \(C_{i} U_{i}\) denote the set of points captured by these balls (note that \(C_{i} S_{i}\) and \(|C_{i}||U_{i}|\)). The algorithm assigns each \(x C_{i}\) to some center \(_{i}(x) S_{i}\) within a distance of \(_{i}\). It then sets \(U_{i+1} U_{i} C_{i}\), and proceeds to the next iteration. In the very last iteration \(t\), we have \(|U_{t}| k^{}\), and the algorithm sets \(S_{t}:=U_{t}\), \(C_{t}:=U_{t}\), and \(_{t}(x):=x\) for each \(x U_{t}\).

The algorithm returns the assignment \(\) with set of centers \((U)=S\), where \(\) is simply the union of \(_{i}\) for all \(i[t]\).

Fix any \(i[t-1]\). Since \(|C_{i}||U_{i}|\), we have \(|U_{i+1}|(1-)|U_{i}|\). As \(\) is a constant, it follows that the algorithm runs for \(t=(1)\) iterations. Since each iteration picks \(O(k)\) new centers, we get: \(|S|=(k)\), and hence \(\) is a \((k)\)-assignment. Furthermore,  showed that \(()=O(1)(U)\).

### Our Dynamic Algorithm

The main idea behind our dynamic algorithm is that it maintains the output \(S\) of the static algorithm from Section 3.1 in a lazy manner, by allowing for some small slack at appropriate places. Thus, it maintains \(t\)_layers_, where each layer \(i[t]\) corresponds to iteration \(i\) of the static algorithm. In the dynamic setting, the value of \(t\) changes over time. Specifically, layer \(i[t]\) consists of the tuple \((U_{i},S_{i},C_{i},_{i},_{i})\), as in Algorithm 1. Whenever a large fraction of points gets inserted or deleted from some layer \(j[t]\), the dynamic algorithm rebuild all the layers \(i[j,t]\) from scratch.

The dynamic algorithm maintains the sets \(U_{i},S_{i}\) and \(C_{i}\) explicitly, and the assignment \(_{i}\) implicitly, in a manner which ensures that for all \(x C_{i}\) it can return \(_{i}(x) S_{i}\) in \(O(1)\) time. Furthermore, for each layer \(i[t]\), it explicitly maintains the value \(n_{i}\) (resp. \(n_{i}^{*}\)), which denotes the size of the set \(U_{i}\) at (resp. the number of updates to \(U_{i}\) since) the time it was last rebuilt. We explain this in more detail below. The value \(_{i}\) is needed only for the sake of analysis.

**Preprocessing:** At preprocessing, we essentially run the static algorithm from Section 3.1 to set the value of \(t\) and construct the layers \(i[t]\). See Algorithm 3 and Algorithm 4. Note that at this stage \(n_{j}^{*}=0\) for all layers \(j[t]\).

```
1:for\(i=1,...,t\)do
2: Add \(x\) to \(U_{i}\)
3:\(n_{i}^{*} n_{i}^{*}+1\)
4:endfor
5: Add \(x\) to \(C_{t}\) and \(S_{t}\), and set \(_{t}(x) x\)
6:Rebuild ```

**Algorithm 5**Insert\((x)\)

**Handling the insertion of a point \(x\) in \(U\):** We simply add the point \(x\) to \(U_{i}\), for each layer \(i[t]\). Next, in the last layer \(t\), we create a new center at point \(x\) and assign the point \(x\) to itself. Finally, we call the Rebuild subroutine (to be described below). See Algorithm 5.

**Handling the deletion of a point \(x\) from \(U\):** Let \(j[t]\) be the last layer (with the largest index) containing the point \(x\). We remove \(x\) from each layer \(i[j]\). Next, if \(x\) was a center at layer \(j\), then we pick any arbitrary point \(x^{}_{j}^{-1}(x)\{x\}\) (if such a point exists), make \(x^{}\) a center, and assign every point \(y_{j}^{-1}(x)\{x\}\) to the newly created center \(x^{}\). Finally, we call the Rebuild subroutine (to be described below). See Algorithm 6.

**The rebuild subroutine:** We say that a layer \(i[t]\) is _rebuild_ whenever our dynamic algorithm calls ConstructFromLayer\((j)\) for some \(j i\), and that there is an _update_ in layer \(i\) whenever we add/remove a point in \(U_{i}\). It is easy to see that \(n_{i}^{*}\) denotes the number of updates in layer \(i[t]\) since the last time it was rebuilt (see Line 3 in Algorithm 4, Algorithm 5 and Algorithm 6). Next, we observe that Line 6 in Algorithm 5 and Line 16 in Algorithm 6, along with the pseudocode of Algorithm 7, imply the following invariant.

```
1:\(j i\)
2:while\(|U_{j}|> k^{}\)do
3:\(n_{j}|U_{j}|\) and \(n_{j}^{*} 0\)
4:\((S_{j},C_{j},_{j},_{j})(U_{j})\)
5:\(U_{j+1} U_{j} C_{j}\)
6:\(j j+1\)
7:endwhile
8:\(t j\)
9:\(S_{t} U_{t}\), \(C_{t} S_{t}\) and \(_{t} 0\)
10: Assign each \(x C_{t}\) to itself (i.e., \(_{t}(x):=x S_{t}\))
11:\(S_{j[t]}S_{i}\)
12: Let \(:U S\) be the assignment such that for all \(j[t]\) and \(x C_{j}\), we have \((x)=_{j}(x)\) ```

**Algorithm 6**Insert\((j)\)

**Invariant 3.1**.: \(n_{i}^{*} n_{i}\) for all \(i[t]\). Here, \(n_{i}\) is the size of \(U_{i}\) just after the last rebuild of layer \(i\), and \(n_{i}^{*}\) is the number of updates in layer \(i\) since that last rebuild.

Intuitively, the above invariant ensures that the layers maintained by our dynamic algorithm remain _close_ to the layers of the static algorithm in Section 3.1. This is crucially exploited in the proofs of Lemma 3.2 (which helps us bound the update and query times) and Lemma 3.3 (which leads to the desired bound on the approximation ratio). We defer the proofs of these two lemmas to Appendix B.

**Lemma 3.2**.: _We always have \(t=(1)\), where \(t\) denotes the number of layers maintained by our dynamic algorithm._```
1:for\(i=1,...,t\)do
2:if\(x U_{i}\)then
3: Remove \(x\) from \(U_{i}\), and set \(n_{i}^{*} n_{i}^{*}+1\)
4:if\(x C_{i}\)then
5: Remove \(x\) from \(C_{i}\)
6:if\(x S_{i}\)then
7: Remove \(x\) from \(S_{i}\)
8:if\(\,y_{i}^{-1}(x)\{x\}\)then
9: Pick any such \(y\) and place it into \(S_{i}\)
10: Set \(_{i}(z)\) to \(y\) for each \(z_{i}^{-1}(x)\)
11:endif
12:endif
13:endif
14:endif
15:endfor
16:Rebuild ```

**Algorithm 6**\((x)\)

**Lemma 3.3**.: _The assignment \(\) maintained by our dynamic algorithm always satisfies \(()=O(1)(U)\)._

**Answering a query:** Upon receiving a query, we consider a weighted \(k\)-median instance \((S,d,w)\), where each point \(x S\) receives a weight \(w(x):=|^{-1}(x)|\). Next, we compute a \(O(1)\)-approximate solution \(S^{*} S\), with \(|S^{*}| k\), to this weighted instance, so that \(_{w}(S^{*},S) O(1)_{w}(S)\). We then return the centers in \(S^{*}\).

**Corollary 3.4**.: \((S^{*})=O(1)(U)\)_._

Corollary 3.4 implies that our dynamic algorithm has \(O(1)\) approximation ratio. It holds because of Lemma 3.3, and its proof immediately follows from . We delegate this proof to Appendix C.

**Corollary 3.5**.: _Our algorithm has \((k^{2})\) query time._

Proof (Sketch).: By Lemma 3.2, we have \(t=(1)\). Since the dynamic algorithm maintains at most \((k)\) centers \(S_{i}\) in each layer \(i\) (follows from Invariant 3.1), we have \(|S|=_{i[t]}|S_{i}|=(k)\). Using appropriate data structures (see the proof of Claim 3.7), in \(O(1)\) time we can find the number of points assigned to any center \(x S\) under \(\) (given by \(|^{-1}(x)|=w(x)\)). Thus, the weighted \(k\)-median instance \((S,d,w)\) is of size \((k)\), and upon receiving a query we can construct the instance in \((k)\) time. We now run a static \(O(1)\)-approximation algorithm  on \((S,d,w)\), which returns the set \(S^{*}\) in \((k^{2})\) time. 

**Lemma 3.6**.: _Our algorithm has \((k)\) update time._

Corollaries 3.4, 3.5 and Lemma 3.6 imply Theorem 1.1. It now remains to prove Lemma 3.6.

### Proof of Lemma 3.6

We first bound the time taken to handle an update, excluding the time spent on rebuilding the layers.

**Claim 3.7**.: _Excluding the calls to_ Rebuild_, Algorithm 5 and Algorithm 6 both run in \((1)\) time._

Proof.: We first describe how our dynamic algorithm maintains the assignment \(\) implicitly. In each layer \(i[t]\), the assignment \(_{i}\) partitions the set \(C_{i}\) into \(|S_{i}|\)_clusters_\(\{_{i}^{-1}(x)\}_{x S_{i}}\). For each such cluster \(Z=_{i}^{-1}(x)\), we maintain: (1) a unique id, given by \((Z)\), (2) its center, given by \(((Z)):=x\), and (3) its size, given by \(((Z)):=|Z|\). For each point \(y Z\), we also maintain the id of the cluster it belongs to, given by \((y):=(Z)\). Using these data structures, for any \(y C_{i}\) we can report in \(O(1)\) time the center \(_{i}(y)\), and we can also report the size of a cluster in \(O(1)\) time.

Recall that \(t=(1)\) as per Lemma 3.2. Thus, Algorithm 5 takes \((1)\) time, excluding the call to Rebuild in Line 6. For Algorithm 6, the key thing to note is that using the data structures described above, Lines 8 - 10 can be implemented in \(O(1)\) time, by setting \(y\) as the new center of the cluster \(_{i}^{-1}(x)\) and by decreasing the size of the cluster by one. It follows that excluding the call to Rebuild in Line 16, Algorithm 6 also runs in \((1)\) time. 

**Claim 3.8**.: _A call to_ ConstructFromLayer\((i)\)_, as described in Algorithm 4, takes \((k|U_{i}|)\) time._

Proof (Sketch).: By Lemma 3.2, the **while** loop in Algorithm 4 runs for \((1)\) iterations. Hence, within a \((1)\) factor, the runtime of Algorithm 4 is dominated by the call to AlmostCover\((U_{j})\) in Line 4. Accordingly, we now consider Algorithm 2. As the ball \(B_{U^{}}(X,r)\) can be computed in \(O(|X||U^{}|)\) time, using a binary search the value \(^{}\) (see Line 2) can be found in \((k|U^{}|)\) time. The rest of the steps in Algorithm 2 also take \((k|U^{}|)\) time. Thus, it takes \((k|U_{j}|)\) time to implement Line 4 of Algorithm 4. Hence, the total runtime of Algorithm 4 is \(_{j[i,t]}(k|U_{j}|)=_{j[i,t]}(k|U_{i }|)=(k|U_{i}|)\). 

Define the potential \(:=_{i[t]}n_{i}^{*}\). For each update in \(U\), the potential \(\) increases by at most \(t\), excluding the calls to Rebuild (see Algorithm 5 and Algorithm 6). Thus, by Lemma 3.2, each update increases the value of \(\) by at most \((1)\). Now, consider any call to ReconstructFromLayer\((i)\). Algorithm 7 and Invariant 3.1 ensure that just before this call, we had \(n_{i}^{*} n_{i}=(|U_{i}|)\), since \(\) is a constant. Hence, because of Line 3 in Algorithm 4, during this call the value of \(\) decreases by at least \((|U_{i}|)\). Claim 3.8, in contrast, implies that the time taken to implement this call is \((k|U_{i}|)\).

To summarize, each update in \(U\) creates \((1)\) units of potential, and the total time spent on the calls to Rebuild is at most \((k)\) times the decrease in the potential. Since the potential \(\) is always nonegative, it follows that the amortized time spent on the calls to Rebuild is \((k)\). This observation, along with Claim 3.7, implies Lemma 3.6.

## 4 Experimental Evaluation

Datasets.We conduct the empirical evaluation of our algorithm on five datasets from the UCI repository : (1) Census with \(2,458,285\) points of dimension \(68\), (2) KDD-Cup containing \(311,029\) points of dimension \(74\), (3) Song with \(515,345\) points of dimension \(90\), (4) Drift with \(13,910\) points of dimension \(129\), and (5) SIFT10M with \(11,164,866\) points of dimension \(128\).3

We generate the dynamic instances that we use in our study as follows. We keep the first \(10,000\) points of each dataset, in the order that they appear in their file. This choice allows us to test against competing algorithms that are not as efficient, and at the same time captures the different practical aspects of the algorithms that we consider; as a sanity check, we perform an experiment on an instance that is an order of magnitude larger to confirm the scalability of our algorithm, and that the relative behavior in terms of the cost of the solutions remains the same among the tested algorithms.

We use the \(L_{2}\) distance of the embeddings of two points to compute their distance. To avoid zero distance without altering the structural properties of the instances, we add \(1/|U|\) to all distances, where \(U\) is the set of points of the instance.

Order of Updates.We obtain a dynamic sequence of updates following the sliding window approach. In this typical approach, one defines a parameter \(\) indicating the size of the window, and then "slides" the window over the static sequence of points in steps creating at each step \(i\) a point insertion of the \(i\)-th point (if there are more than \(i\) points) and a point deletion of the \((i-)\)-th point (if \(>i\)), until each point of the static instance is inserted once and deleted once. We set \(=2,000\), which results in our dynamic instances having at most \(2,000\) points at any point in time.

Algorithms.We compare our algorithm from Section 3 against the state-of-the-art algorithm for maintaining a solution to the \(k\)-median problem, which is obtained by dynamically maintaining a coreset for the same problem . In particular,  presented an algorithm that can dynamically maintain an \(\)-coreset of size \(O(^{-2}k( n,(1/)))\) for the \(k\)-median and the \(k\)-means problems. Their algorithm has a worst-case update time of \(O(^{-2}k^{2}( n,(1/)))\). To the best of our knowledge, this is the first implementation of the algorithm in . For brevity, we refer to our algorithm as OurAlg, and the algorithm from  as HK.

Since the exact constants and thresholds required to adhere to the theoretical guarantees of  are impractically large, there is no obvious way to use the thresholds provided by the algorithm in practice without making significant modifications. In order to give a fair comparison of the two algorithms, we implemented each of them to have a single parameter controlling a trade-off between update time, query time, and the cost of the solution. OurAlg has a parameter \(\) which we set to be the number of points sampled during the construction of a layer in Algorithm 2. In other words, we sample \(\) points in Line 1 of Algorithm 2 instead of \( k^{}\). We fix the constants \(\) and \(\) used by our algorithm (see Section 3) to \(0.2\) and \(0.5\) respectively. HK has a parameter \(\) which we set to be the number of points sampled during the static coreset construction from  which is used as a black box, replacing the large threshold required to adhere to the theoretical guarantees. Since this threshold is replaced by a single parameter, it also replaces the other parameters used by HK, which are only used in this thresholding process. For the bicriteria algorithm required by this static coreset construction, we give the same implementation as the empirical evaluations in  and use kmeans++ with 2 iterations of Lloyd's.

For each of \(\) and \(\), we experimented with the values \(250,500,1000\). As the values of \(\) and \(\) are increased, the asymptotic update times and query times of the corresponding algorithms increase, while the cost of the solution decreases.

Setup.All of our code is written in Java and is available online.4 We did not use parallelization. We used a machine with 8 cores, a 2.9Ghz processor, and 16 GiB of main memory.

Results.We compare OurAlg\((=500)\) against HK\((=1000)\). We selected these parameters such that they obtain a good solution quality, without these parameters being unreasonably high w.r.t. the size of our datasets. Our experiments suggest that the solution quality produced by OurAlg is robust against different values of \(\) (typically these versions of the algorithm differ by less than \(1\%\)), while that of HK is more sensitive (in several instances the differences are in the range of \(3-9\%\), while for KDD-Cup HK\((=250)\) produces much worse solutions). The update time among the different versions of each algorithm do not differ significantly. Lastly, the query time of OurAlg is less sensitive compared to the query time of HK. Specifically, the average query time of OurAlg\((=1000)\) is roughly \(3\) times slower than OurAlg\((=250)\), while the average query time of HK\((=1000)\) is \(11\) times slower compared to HK\((=250)\). For OurAlg we selected the middle value \(=500\). Since the solution quality of HK drops significantly when using smaller values of \(\), we selected \(=1000\) to prioritize for the quality of the solution produced by the algorithm. Since we did not observe significant difference in the behavior across the datasets, we tuned these parameters considering three or our datasets. We provide the relevant plots in Appendix E.4.

Update Time Evaluation.In Figure 1 (left) we plot the total update time over the sequence of updates for the dataset Song, and only for \(k=50\). OurAlg runs up to more than 2 orders of 

[MISSING_PAGE_FAIL:9]

a window size of \(=5,000\) and applied it to the first \(100,000\) points. As expected the relative performance in terms of solution cost and query time remains the same, while the gap in terms of update time grows significantly (HK\((=1000)\) performs more than 3 orders of magnitude worse than \((=500)\)). The corresponding plots and numbers are provided in the Appendix.

Conclusion of the Experiments.Our experimental evaluation shows the practicality of our algorithms on a set of five standard datasets used for the evaluation of dynamic algorithms in metric spaces. In particular, our experiments verify the theoretically superior update time of our algorithm w.r.t. to the state-of-the-art algorithm for the fully-dynamic \(k\)-median problem , where our algorithm performs orders of magnitude faster than HK. On top of that, the solution quality produced by our algorithm is better than HK in most settings, and the query times of the two algorithms are comparable. We believe that our algorithm is the best choice when it comes to efficiently maintaining a dynamic solution for the \(k\)-median problem in practice.